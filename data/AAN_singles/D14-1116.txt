Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1092?1103,October 25-29, 2014, Doha, Qatar.c?2014 Association for Computational LinguisticsQuestion Answering over Linked Data Using First-order Logic?Shizhu He, Kang Liu, Yuanzhe Zhang, Liheng Xu and Jun ZhaoNational Laboratory of Pattern RecognitionInstitute of Automation, Chinese Academy of Sciences, Beijing, 100190, China{shizhu.he, kliu, yzzhang, lhxu, jzhao}@nlpr.ia.ac.cnAbstractQuestion Answering over Linked Data(QALD) aims to evaluate a question an-swering system over structured data, thekey objective of which is to translatequestions posed using natural languageinto structured queries.
This techniquecan help common users to directly ac-cess open-structured knowledge on theWeb and, accordingly, has attracted muchattention.
To this end, we propose anovel method using first-order logic.
Weformulate the knowledge for resolvingthe ambiguities in the main three stepsof QALD (phrase detection, phrase-to-semantic-item mapping and semantic itemgrouping) as first-order logic clauses in aMarkov Logic Network.
All clauses canthen produce interacted effects in a unifiedframework and can jointly resolve all am-biguities.
Moreover, our method adopts apattern-learning strategy for semantic itemgrouping.
In this way, our method cancover more text expressions and answermore questions than previous methods us-ing manually designed patterns.
The ex-perimental results using open benchmarksdemonstrate the effectiveness of the pro-posed method.1 IntroductionWith the rapid development of the Web of Data,many RDF datasets have been published as LinkedData (Bizer et al., 2009), such as DBpedia (Aueret al., 2007), Freebase (Bollacker et al., 2008)and YAGO (Suchanek et al., 2007).
The grow-ing amount of Linked Data contains a wealth ofknowledge, including entities, classes and rela-tions.
Moreover, these linked data usually have?Shizhu He and Kang Liu have equal contribution to thiswork.complex structures and are highly heterogeneous.As a result, there are gaps for users regarding ac-cess.
Although a few experts can write queries us-ing structured languages (such as SPARQL) basedon their needs, this skill cannot be easily utilizedby common users (Christina and Freitas, 2014).Thus, providing user-friendly, simple interfacesto access these linked data becomes increasinglymore urgent.Because of this, question answering over linkeddata (QALD) (Walter et al., 2012) has recentlyreceived much interest, and most studies on thistopic have focused on translating natural lan-guage questions into structured queries (Freitasand Curry, 2014; Yahya et al., 2012; Unger et al.,2012; Shekarpour et al., 2013; Yahya et al., 2013;Bao et al., 2014; Zou et al., 2014).
For example,with respect to the question?Which software has been developed by organi-zations founded in California, USA?
?,the aim is to automatically convert this utteranceinto an SPARQL query that contains the follow-ing subject-property-object (SPO) triple format:?
?url rdf:type dbo:Software, ?url dbo:developer ?x1,?x1 rdf:type dbo:Company, ?x1 dbo:foundationPlacedbr:California?1.To fulfill this objective, existing systems (Lopezet al., 2006; Unger et al., 2012; Yahya et al., 2012;Zou et al., 2014) usually adopt a pipeline frame-work that contains four major steps: 1) decompos-ing the question and detecting phrases, 2) map-ping the detected phrases into semantic items ofLinked Data, 3) grouping the mapped semanticitems into semantic triples, and 4) generating thecorrect SPARQL query.However, completing these four steps and con-structing such a structured query is not easy.
Thefirst three steps mentioned above are subject to the1The prefixes in semantic items indicate the source oftheir vocabularies.1092problem of ambiguity, which is the major chal-lenge in QALD.
Using the question mentionedabove as an example, we can choose Califor-nia or California, USA when detecting phrases,the phrase California can be mapped to the en-tity California State or California Film, and the classSoftware (mapped from the phrase software) canbe matched with the first argument of the rela-tion producer or developer (these two relations canbe mapped from the phrase developed).
Previ-ous methods (Lopez et al., 2006; Lehmann etal., 2012; Freitas and Curry, 2014) have usu-ally performed disambiguation at each step only,and the subsequent step was performed based onthe disambiguation results in the previous step(s).However, we argue that the three steps men-tioned above have mutual effects.
In the previ-ous example, the phrase founded in (verb) canbe mapped to the entities (Founding of Rome andFounder (company)), classes (Company and Depart-ment) or relations (foundedBy and foundationPlace).If we know that the phrase California can referto the entity California State, and which can be thesecond argument of the relation foundationPlace,together with a verb phrase being more likelyto be mapped to Relation, we should map thephrase founded in to foundationPlace in this ques-tion.
Thus, we aim to determine if joint disam-biguation is better than individual disambigua-tion.
(Question One)In addition, previous systems usually employedmanually designed patterns to extract predicate-argument structures that are used to guide the dis-ambiguation process in the three steps mentionedabove (Yahya et al., 2012; Unger et al., 2012; Zouet al., 2014).
For example, (Yahya et al., 2012)used only three dependency patterns to group themapped semantic items into semantic triples.
Nev-ertheless, these three manually designed patternsmiss many cases because of the diversity of thequestion expressions.
We gathered statistics on144 questions and found that the macro-averageF1 and micro-average F1 of the three patterns2used in (Yahya et al., 2012) are only 62.8 and66.2%, respectively.
Furthermore, these speciallydesigned patterns may not be valid with variationsin domains or languages.
Therefore, another im-portant question arises: can we automaticallylearn rules or patterns to achieve the same ob-2They are 1) verbs and their arguments, 2) adjectives andtheir arguments and 3) propositionally modified tokens andobjects of prepositions.jective?
(Question Two)Focusing on the two problems mentionedabove, this paper proposes a novel algorithm basedon a learning framework, Markov Logic Networks(MLNs) (Richardson and Domingos, 2006), tolearn a joint model for constructing structuredqueries from natural language utterances.
MLNis a statistical relational learning framework thatcombines first-order logic and Markov networks.The appealing property of MLN is that it is read-ily interpretable by humans and that it is a naturalframework for performing joint learning.
We for-mulate the knowledge for resolving the ambigui-ties in the main three steps of QALD (phrase de-tection, phrase-to-semantic-item mapping and se-mantic item grouping) as first-order logic clausesin an MLN.
In the framework of MLN, all clauseswill produce interacted effects that jointly resolveall problems into a unified process.
In this way,the result in each step can be globally optimized.Moreover, in contrast to previous methods, weadopt a learning strategy to automatically learnthe patterns for semantic item grouping.
We de-sign several meta patterns as opposed to the spe-cific patterns.
In addition, these meta patterns areformulated as the first-order logic formulas in theMLN.
The specific patterns can be generated bythese meta patterns based on the training data.
Themodel will learn the weights of each clause to de-termine the most effective patterns for semantictriple construction.
In this way, with little effort,our approach can cover more semantic expressionsand answer more questions than previous meth-ods, which depend on manually designed patterns.We evaluate the proposed method using severalbenchmarks (QALD-1, QALD-3, QALD-4).
Theexperimental results demonstrate the advantage ofthe joint disambiguation process mentioned above.They also prove that our approach, employingMLN to automatically learn the patterns of seman-tic triple grouping, is effective.
Our system cananswer more questions and obtain better perfor-mance than the traditional methods based on man-ually designed heuristic rules.2 Background2.1 Linked Data SourcesLinked Data consist of many relational data,which are usually inter-linked as subject-property-object (SPO) triple statements (such as using theowl:sameAs relation).
In this paper, we mainly use1093Subject(Arg1) Relation(Property) Object(Arg2)ProgrammingLanguage subClassOf SoftwareJava_(programming_language) type SoftwareJava_(programming_language) developer Oracle_CorporationOracle_Corporation foundationPlace California_(State)foundationPlace domain OrganisationCalifornia_(State) label ?California?California_(1977_film) label ?California?Oracle_Corporation numEmployees 118119(xsd:integer)Figure 1: Sample knowledge base facts.DBpedia3and some classes from Yago4.
Theseknowledge bases (KBs) are composed of many on-tological and instance statements, and all state-ments are expressed by SPO triple facts.
Figure1 shows some triple fact samples from DBpedia.Each fact is composed of three semantic items.
Asemantic item can be an entity (California (State),Oracle Corporation, etc.
), a class (Software, Organ-isation, etc.)
or a relation (called a propertyor predicate in some occasions).
Some entitiesare literals including strings, numbers and dates(118119(xsd:integer), etc.).
Relations contain stan-dard Semantic Web relations (subClassOf, type, do-main and label) and ontological relations (developer,foundationPlace and numEmployees).2.2 Task StatementGiven a knowledge base (KB), our objective is totranslate a natural language question qNLinto aformal language query qFLthat targets the seman-tic vocabularies given by the KB, and the queryqFLshould capture the user information needs ex-pressed by qNL.Following (Yahya et al., 2012), we focus on thefactoid questions, and the answers to such ques-tions are an entity or a set of entities.
We ignorethe questions that need the aggregation5(max/min,etc.)
and negation operations.
That is, we generatequeries that consist of a plentiful number of triplepatterns, which are multiple conjunctions of SPOsearch conditions.3 FrameworkFigure 2 shows the entire framework of our systemfor translating a question into a formal SPARQLquery.
The first three steps address the input ques-tion through 1) Phrase Detection (detecting pos-sible phrases), 2) Phrase Mapping (mapping all3http://dbpedia.org/4http://www.mpi-inf.mpg.de/yago-naga/yago/5We can address the count query questions, which willbe explained in Section 3.phrase candidates to the corresponding seman-tic items), and 3) Feature Extraction (extractingthe linguistic features and semantic item featuresfrom the question and the Linked Data, respec-tively).
As a result, a space of candidates is con-structed, including possible phrases, mapped se-mantic items and the possible argument match re-lations among them.
Next, the fourth step (In-ference) formulates the joint disambiguation as ageneralized inference task.
We employ rich fea-tures and constraints (including hard and soft con-straints) to infer a joint decision through an MLN.Finally, with the inference results, we can con-struct a semantic item query graph and generatean executable SPARQL query.
In the followingsubsection, we demonstrate each step in detail.1) Phrase detection.
In this step, we detectphrases (sequences of tokens) that probably indi-cate semantic items in the KB.
We do not use anamed entity recognizer (NER) because of its lowcoverage.
We perform testing on two commonlyused question corpora, QALD-3 and free9176, us-ing the Stanford NER tool7.
The results demon-strate that only 51.5 and 23.8% of the NEs arecorrectly recognized, respectively.
To avoid miss-ing useful phrases, we retain all n-grams as phrasecandidates, and then use some rules to filter them.The rules include the following: the span lengthmust be less than 4 (accepting that all contiguoustokens are capitalizations), the POS tag of the starttoken must be jj, nn, rb and vb, all contiguouscapitalization tokens must not be split, etc.
Forinstance, software, developed by, organizations,founded in and California are detected in the ex-ample of the first section.2) Phrase mapping.
After the phrases are de-tected, each phrase can be mapped to the corre-sponding semantic item in KB (entity, class andrelation).
For example, software is mapped todbo:Software, dbo:developer, etc., and California ismapped to dbr:California, dbr:California (wine), etc.For different types of semantic items, we use dif-ferent techniques.
For mapping phrases to en-tities, considering that the entities in DBpediaand Wikipedia are consistent, we employ anchor,redirection and disambiguation information fromWikipedia.
For mapping phrases to classes, con-sidering that classes have lexical variation, espe-cially synonyms, e.g., dbo:Film can be mapped6http://www.cis.temple.edu/?yates/open-sem-parsing/index.html7http://nlp.stanford.edu/software/CRF-NER.shtml1094Which software has been developed byorganizations founded in California, USA?
software, developed, developed by, organizations,founded, founded in, California, USAsoftwaredeveloped by.........CaliforniaphraseIndexphrasePosTagresourceTypepriorMatchScorehasMeanWordphraseDepTaghasRelatedness...isTypeCompatiblehasPhrase hasResourcehasRelationFigure 2: Framework of our system.from film, movie and show, we compute the simi-larity between the phrase and the class in the KBwith the word2vec tool8.
The word2vec tool com-putes fixed-length vector representations of wordswith a recurrent-neural-network based languagemodel (Mikolov et al., 2010).
The similarity scor-ing methods are introduced in Section 4.2.
Then,the top-N most similar classes for each phrase arereturned.
For mapping phrases to relations, weemploy the resources from PATTY (Nakashole etal., 2012) and ReVerb (Fader et al., 2011).
Specif-ically, we first compute the associations betweenthe ontological relations in DBpedia and the re-lation patterns in PATTY and ReVerb through in-stance alignments as in (Berant et al., 2013).
Next,if a detected phrase is matched to some relationpattern, the corresponding ontological relations inDBpedia will be returned as a candidate.
This steponly generates candidates for every possible map-ping, and the decision of the best selection will beperformed in the next step.3) Feature extraction and joint inference.There exist ambiguities in phrase detection and inmapping phrases to semantic items.
This step fo-cuses on addressing these ambiguities and deter-8https://code.google.com/p/word2vec/mining the argument match relations among themapped semantic items.
This is the core compo-nent of our system, and it performs disambigua-tion in a unified manner.
First, feature extractionis performed to prepare a rich number of featuresfrom the input question and from the KB.
Next,the disambiguation is performed in a joint fashionwith a Markov Logic Network.
Detailed informa-tion will be presented in Section 4.4) Semantic item query graph construction.Based on the inference results, we construct aquery graph.
The vertices contain the following:the detected phrase, the token span indexes ofthe phrases, the mapped semantic items and theirtypes.
The edge indicates the argument match re-lation between two semantic items.
For example,we use 1 2 to indicate that the first argument ofan item matches the second argument of anotheritem9.
The right bottom in Figure 2 shows an ex-ample of this.5) Query generation.
The SPARQL queriesrequire the grouped triples of semantic items.Thus, in this step, we convert a query graphinto multiple joined semantic triples.
Three in-terconnected semantic items, whereby it must9The other marks will be introduced in Section 4.2.1095be ensured that the middle item is a rela-tion, are converted into a semantic triple (mul-tiple joined facts containing variables).
Forexample, the query graph Vdbo:Book[Class] 1 2?
?dbo:author[Relation] 1 1?
?dbr:Danielle Steel[Entity]W isconverted into ?
?x rdf:type dbo:Book, dbr:Danielledbo:author ?x?, and Vdbo:populationTotal[Relation]1 2?
?dbo:capital[Relation] 1 1?
?dbr:Australia[Entity]W10isconverted into ?
?x1 dbo:populationTotal ?answer, ?x1dbo:capital dbr:Australia?.
If the query graph onlycontains one vertex that indicates a class ClassURI,we generate ?
?x rdf:type ClassURI?.
If the querygraph only contains two connected vertexes, weappend a variable to bind the missing match argu-ment of the semantic item.The final SPARQL query is constructed by join-ing the semantic item triples based on the cor-responding SPARQL template.
We divide thequestions into three types: Yes/No, Normal andNumber.
Yes/No questions use the ASK WHEREtemplate.
Normal questions use the SELECT ?urlWHERE template.
Number questions first use thenormal question template, and if they cannot ob-tain a correct answer (a valid numeric value), weuse the SELECT COUNT(?url) WHERE template togenerate a query again.
For instance, we constructthe SPARQL query SELECT(?url) WHERE{ ?urlrdf:type dbo:Software.
?url dbo:developer ?x1.
?x1 rdf:typedbo:Company.
?x1 dbo:foundationPlace dbr:California.
}for this example.4 Joint Disambiguation with MLNIn this section, we present our method for ques-tion answering over linked data using a MarkovLogic Network (MLN).
In the following subsec-tions, we first briefly describe the MLN.
Then, wepresent the predicates and the first-order logic for-mulas used in the model.4.1 Markov Logic NetworksMarkov logic networks combine Markov networkswith first-order logic in a probabilistic framework(Richardson and Domingos, 2006).
An MLNMconsists of several weighted formulas {(?i, wi)}i,where ?iis a first order formula and wiis thepenalty (the formula?s weight).
In contrast tothe first-order logic, whereby a formula repre-sents a hard constraint, these logic formulas arerelaxed and can be violated with penalties in the10This corresponds to the question ?How many people livein the capital of Australia??MLN.
Each formula ?iconsists of a set of first-order predicates, logical connectors and variables.These weighted formulas define a probability dis-tribution over a possible world.
Let y denote a pos-sible world.
Then p(y) is defined as follows:p(y) =1Zexp???(?i,wi)?Mwi?c?Cn?if?ic(y)?
?,where each c is a binding of the free variables in?ito constants; f?icis a binary feature functionthat returns 1 if the ground formula that we ob-tain through replacing the free variables in ?iwiththe constants in c under the given possible worldy is true and is 0 otherwise; and Cn?iis the set ofall possible bindings for the free variables in ?i.Z is a normalized constant.
The Markov networkcorresponds to this distribution, where nodes rep-resent ground atoms and factors represent groundformulas.4.2 PredicatesIn the MLN, we design several predicates to re-solve the ambiguities in phrase detection, map-ping phrases to semantic items and semantic itemgrouping.
Specifically, we design a hidden pred-icate hasPhrase(i) to indicate that the i-th candi-date phrase has been chosen.
The predicate hasRe-source(i,j) indicates that the i-th phrase is mappedto the j-th semantic item.
The predicate hasRe-lation(j,k,rr) indicates that the j-th semantic itemand the k-th semantic item should be grouped to-gether with the argument-match-type rr.
Note thatwe define four argument match types between twosemantic items: 1 1, 1 2, 2 1 and 2 2.
Here, theargument match type t s denotes that the t-th argu-ment of the first semantic item corresponds to thes-th argument of the second semantic item11.
Thedetailed illustration is shown in Table 1.Type Example Question1 1 dbo:height 1 1 dbr:Michael Jordan How tall is Michael Jor-dan?1 2 dbo:River 1 2 dbo:crosses Which river does theBrooklyn Bridge cross?2 1 dbo:creator 2 1 dbr:Walt Disney Which television showswere created by WaltDisney?2 2 dbo:birthPlace 2 2 dbo:capital Which actors were born inthe capital of American?Table 1: Examples of the argument match types.11The 2-nd argument is corresponding to the object argu-ment of the relation, and the 1-st argument is correspondingwith the subject argument of the relation and the entity (in-cluding the class) itself.1096Describing the attributes of phrases and relation between two phrasesphraseIndex(p, i, j) The start and end position of phrase p in question.phrasePosTag(p, pt) The POS tag of the head word in phrase p.phraseDepTag(p, q, dt) The dependency path tags between phrase p and q.phraseDepOne (p, q) If there is only one tag in the dependency path, the predicate is true.hasMeanWord (p, q) If there is any one meaning word in the dependency path of two phrases, the predicate is true.Describing the attributes of semantic item and the mappings between phrases and semantic itemsresourceType(r, rt) The type of semantic item r. Types of semantic items include Entity, Class and RelationpriorMatchScore(p, r, s) The prior score of phrase p mapping to semantic item r.Describing the attributes of relation between two semantic items in a knowledge basehasRelatedness(p, q, s) The semantic coherence of semantic items.isTypeCompatible(p, q, rr) If the semantic items p are type-compatible with the semantic items q, the predicate is true.hasQueryResult(s, p, o, rr1, rr2) If the triple pattern consisting of semantic items s, p, o and argument-match-types rr1 and rr2 have queryresults, the predicate is true.Table 2: Descriptions of observed predicates.Moreover, we define a set of observed predi-cates to describe the properties of phrases, seman-tic items, relations between phrases and relationsbetween semantic items.
The observed predicatesand descriptions are shown in Table 2.Previous methods usually designed someheuristic patterns to group semantic items, whichusually employed a human-designed syntacticpath between two phrases to determine their re-lations.
In contrast, we collect all the tokens inthe dependency path between two phrases as pos-sible patterns.
The predicates phraseDepTag andhasMeanWord are designed to indicate the possi-ble patterns.
Note that if these tokens only containPOS tags dt|in|wdt|to|cc|ex|pos|wp or stop words,the value of the predicate hasMeanWord is false;otherwise, it is true.
In this way, our system is ex-pected to cover more question expressions.
More-over, the SPARQL endpoint is used to verify thetype compatibility of two semantic items and ifone triple pattern can obtain query results.The predicate hasRelatedness needs to computethe coherence score between two semantic items.Following (Yahya et al., 2012), we use the Jaccardcoefficient (Jaccard, 1908) based on the inlinks be-tween two semantic items.The predicate priorMatchScore assigns a priorscore when mapping a phrase to a semantic item.We use different methods to compute this scoreaccording to different semantic item types.
Forentities, we use a normalized score based on thefrequencies of a phrase referring to an entity.For classes and relations, we use different meth-ods.
We first define the following three similar-ity metrics: a) s1: The Levenshtein distance score(Navarro, 2001) between the labels of the seman-tic item and the phrase; b) s2: The word embed-ding (Mikolov et al., 2010) score, which measuresthe similarity between two phrases and is the max-imum cosine value of the words?
word embed-dings between two phrases; and c) s3: the instanceoverlap score, which is computed using the Jac-card coefficient of the instance overlap.
All scoresare normalized to produce a comparable scoresin the interval of (0, 1).
The final prior scoresfor mapping phrases to classes and relations are?s1+ (1?
?
)s2and ?s1+ ?s2+ (1?
??
?)s3,respectively.
The parameters are set to empiricalvalues12.4.3 FormulasAccording to these predicates, we design severalfirst-order logic formulas for joint disambiguation.As mentioned in the first section, these formulasrepresent the meta patterns.
The concrete pat-terns can be generated through these meta pat-terns with training data.
Specifically, we use twotypes of formulas for the joint decisions: Booleanand Weighted formulas.
Boolean formulas arehard constraints, which must be satisfied by allof the ground atoms in the final inference results.Weighted formulas are soft constraints, which canbe violated with some penalties.4.3.1 Boolean Formulas (Hard Constraints)Table 3 lists the Boolean formulas used in thiswork.
The ?
?
notation in the formulas indicatesan arbitrary constant.
The ?|f |?
notation expressesthe number of true grounded atoms in the formulaf .
These formulas express the following con-straints:hf1: If a phrase is chosen, then it must have amapped semantic item;hf2: If a semantic item is chosen, then its mappedphrase must be chosen;hf3: A phrase can be mapped to at most one se-mantic item;hf4: If the phrase is not chosen, then its mapped12Set ?
to 0.6 for Class and set ?
and ?
to 0.3 and 0.3 forRelation, respectively.1097hf1 hasPhrase(p)?
hasResource(p, )hf2 hasResource(p, )?
hasPhrase(p)hf3 |hasResource(p, )| ?
1hf4 !hasPhrase(p)?
!hasResource(p, r)hf5 hasResource( , r)?
hasRelation(r, , ) ?
hasRelation( , r, )hf6 |hasRelation(r1, r2, )| ?
1hf7 hasRelation(r1, r2, )?
hasResource( , r1) ?
hasResource( , r2)hf8 phraseIndex(p1, s1, e1) ?
phraseIndex(p2, s2, e2) ?
overlap(s1, e1, s2, e2) ?
hasPhrase(p1)?
!hasPhrase(p2)hf9 resourceType(r, ?Entity?)?
!hasRelation(r, , ?2 1?)
?
!hasRelation(r, , ?2 2?
)hf10 resourceType(r, ?Entity?)?
!hasRelation( , r, ?2 1?)
?
!hasRelation(r, , ?2 2?
)hf11 resourceType(r, ?Class?)?
!hasRelation(r, , ?2 1?)
?
!hasRelation(r, , ?2 2?
)hf12 resourceType(r, ?Class?)?
!hasRelation( , r, ?2 1?)
?
!hasRelation(r, , ?2 2?
)hf13 !isTypeCompatible(r1, r2, rr)?
!hasRelation(r1, r2, rr)Table 3: Descriptions of Boolean formulas.sf1 priorMatchScore(p, r, s)?
hasPhrase(p)sf2 priorMatchScore(p, r, s)?
hasResource(p)sf3 phrasePosTag(p, pt+) ?
resourceType(r, rt+)?
hasResource(p, r)sf4 phraseDepTag(p1, p2, dp+) ?
hasResource(p1, r1) ?
hasResource(p2, r2)?
hasRelation(r1, r2, rr+)sf5 phraseDepTag(p1, p2, dp+) ?
hasResource(p1, r1) ?
hasResource(p2, r2)?
!hasMeanWord(p1, p2) ?hasRelation(r1, r2, rr+)sf6 phraseDepTag(p1, p2, dp+) ?
hasResource(p1, r1) ?
hasResource(p2, r2) ?
phraseDepOne(p1, p2) ?hasRelation(r1, r2, rr+)sf7 hasRelatedness(r1, r2, s) ?
hasResource( , r1) ?
hasResource( , r2)?
hasRelation(r1, r2, )sf8 hasQueryResult(r1, r2, r3, rr1, rr2)?
hasRelation(r1, r2, rr1) ?
hasRelation(r2, r3, rr2)Table 4: Descriptions of weighted formulas.semantic item should not be chosen;hf5: If a semantic item is chosen, then it shouldhave at least one argument match relation withother semantic items;hf6: Two semantic items have at most one argu-ment match relation;hf7: If an argument match relation for two seman-tic items is chosen, then they must be chosen;hf8: Each of two chosen phrases must not overlap;hf9, hf10, hf11, hf12: The semantic item withtype Entity and Class should not have a second ar-gument that matches with others;hf13: The chosen argument match relation for twosematic items must be type compatible.4.3.2 Weighted Formulas (Soft Constraints)Table 4 lists the weighted formulas used in thiswork.
The ?+?
notation in the formulas indicatesthat each constant of the logic variable should beweighted separately.
Those formulas express thefollowing properties in joint decisions:sf1, sf2: The larger the score of the phrase map-ping to a semantic item, the more likely the cor-responding phrase and semantic item should beenchosen;sf3: There are some associations between the POStags of phase and the types of mapped semanticitems;sf4, sf5, sf6: There are some associations be-tween the dependency tags in the dependency pat-tern path of two phases and the types of argumentmatch relations of two mapped semantic items;sh7: The larger the relatedness of two seman-tic items, the more likely they have an argumentmatch relation;sf8: If the triple pattern has query results, these se-mantic items should have corresponding argumentmatch relations.5 Experiments5.1 Dataset & Evaluation MetricsWe use the following three collections of questionsfrom the QALD13task for question answeringover linked data: QALD-1, QALD-3 and QALD-4.
The generated SPARQL queries are evaluatedon Linked Data from DBpedia and YAGO usinga Virtuoso engine14.
A typical example questionfrom the QALD benchmark is ?Which books writ-ten by Kerouac were published by Viking Press?
?.As mentioned in Section 2.2, our system is not de-signed to answer questions that contain numbers,date comparisons and aggregation operations suchas group by or order by.
Therefore, we removethese types of questions and retain 110 questionsfrom the QALD-4 training set for generating thespecific formulas and for training their weights inMLN.
We test our system using 37, 75 and 26questions from the training set of QALD-115, andthe testing set of QALD-3 and QALD-4 respec-tively.
We use #T, #Q and #A to indicate the total13www.sc.cit-ec.uni-bielefeld.de/qald/14https://github.com/openlink/virtuoso-opensource15We use the training set because we try to make a faircomparison with (Yahya et al., 2012).1098number of questions in the testing set, the num-ber of questions we could address and the numberof questions answered correct, respectively.
Weselect Precision (P =#A#Q), Recall (R =#A#T),and F1-score (F1 =2?P ?RP+R) as the evaluation met-rics.
To assess the effectiveness of the disambigua-tion process in the MLN, we computed the overallquality measures by precision and recall with themanually obtained results.5.2 Experimental ConfigurationsThe Stanford dependency parser (De Marneffe etal., 2006) is used for extracting features from thedependency parse trees.
We use the toolkit the-beast16to learn the weights of the formulas andto perform the MAP inference.
The inference al-gorithm uses a cutting plane approach.
In addi-tion, for the parameter learning, we set all ini-tial weights to zero and use an online learningalgorithm with MIRA update rules to update theweights of the formulas.
The number of iterationsfor the training and testing are set to 10 and 200,respectively.5.3 Results and Discussion5.3.1 The Effect of Joint LearningTo demonstrate the advantages of our joint learn-ing, we design a pipeline system for compari-son, which independently performs phrase detec-tion, phrase mapping, and semantic item groupingby removing the unrelated formulas in MLN.
Forexample, the formulas17related to the predicateshasResource and hasRelation are removed whendetecting phrases in questions.Table 5 shows the results, where Joint de-notes the proposed method with joint inferenceand Pipeline denotes the compared method per-forming each step independently.
We perform acomparison with the question answering results ofQALD (QA), and comparisons at each of the fol-lowing steps: PD (phrase detection), PM (phrasemapping) and MG (mapped semantic items group-ing).
From the results, we observe that our methodanswers over half of the questions.
Moreover, ourjoint model based on MLN can obtain better per-formance in question answering compared to thepipeline system.
We also observe that Joint ex-hibits better performance than Pipeline in moststeps, except for MG in QALD-3.
We believe this16http://code.google.com/p/thebeast17including entire formulas, excluding hf8 and sf1is because the three tasks (phrase detection, phrasemapping, and semantic item grouping) are con-nected with each other.
Each step can provide use-ful information for the other two tasks.
Therefore,performing joint inference can effectively improvethe performance.
Finally, we observe that the for-mer task usually produces better results than thesubsequent tasks (phrase detection exhibits a bet-ter performance than phrase mapping, and phrasemapping exhibits a better performance than se-mantic item grouping).
The main reason is thatthe latter subtask is more complex than the formertask.
The decisions of the latter subtask stronglyrely on the former results even though they haveinteracted effects.5.3.2 The Effect of Pattern LearningTable 6 shows a comparison of our system withDEANNA (Yahya et al., 2012), which is basedon a joint disambiguation model but which em-ploys hand-written patterns in its system.
BecauseDEANNA only reports its results of the QALD-1dataset, we do not show the results for QALD-3and QALD-4 for equity.
From the results, we cansee that our system solved more questions and ex-hibited a better performance than did DEANNA.One of the greatest strengths of our system is thatthe learning system can address more questionsthan hand-written pattern rules.System #T #Q #A P R F1DEANNA (Yahya et al., 2012) 50 27 13 0.48 0.26 0.33Ours 50 37 20 0.54 0.4 0.46Table 6: Comparisons with DEANNA using theQALD-1 test questions.Compared to the ILP (Integer Linear Program-ming) used in (Yahya et al., 2012) for joint disam-biguation, we argue that there are two major dif-ferences to our method.
1) Our method is a data-driven approach that can learn effective patternsor rules for the task.
Therefore, it exhibits morerobustness and adaptability for various KBs.
2)We design several meta rules in MLN as opposedto specific ones.
The specific rules can be gen-erated by these meta rules based on the trainingdata.
By contrast, the traditional approach usingILP needs to set specific rules in advance, whichrequires more intensive labor than our approach.To further illustrate the effectiveness of ourpattern-learning strategy, we show the weights ofthe learned patterns corresponding to formula sf3in the MLN, as shown in Table 7.
From the table,1099BenchmarkPD PM MG QAP R F1 P R F1 P R F1 #T #Q #A P R F1QALD-1(Joint) 0.93 0.981 0.955 0.895 0.944 0.919 0.703 0.813 0.754 50 37 20 0.54 0.4 0.46QALD-1(Pipeine) 0.921 0.972 0.946 0.868 0.917 0.892 0.585 0.859 0.696 50 34 17 0.5 0.34 0.41QALD-3(Joint) 0.941 0.941 0.941 0.878 0.918 0.898 0.636 0.798 0.708 99 75 45 0.6 0.46 0.52QALD-3(Pipeline) 0.912 0.912 0.912 0.829 0.867 0.848 0.677 0.789 0.729 99 75 42 0.56 0.42 0.48QALD-4(Joint) 0.947 0.978 0.963 0.937 0.967 0.952 0.776 0.865 0.817 50 26 15 0.58 0.3 0.4QALD-4(Pipeline) 0.937 0.967 0.952 0.905 0.935 0.920 0.683 0.827 0.748 50 24 13 0.54 0.26 0.35Table 5: The performance of joint learning on three benchmark datasets.we can see that nn18is more likely mapped to En-tity19than to Class and Relation, and vb is mostlikely mapped to Relation.
This proves that ourmodel can learn effective and reasonable patternsfor QALD.POS tag of Phrase type of mapped Item Weightnn Entity 2.11nn Class 0.243nn Relation 0.335vb Relation 0.517wp Class 0.143wr Class 0.025Table 7: Sample weights of formulas, correspond-ing with formula sf3.5.3.3 Comparison to the state of the artTo illustrate the effectiveness of the proposedmethod, we perform comparisons to the state-of-the-art methods.
Table 8 shows the results usingQALD-3 and QALD-4.
These systems are theparticipants in the QALD evaluation campaigns.From the results, we can see that our system out-performs most systems at a competitive perfor-mance.
They further prove the effectiveness of theproposed method.Test set System #T #Q #A P R F1QALD-3CASIA (He et al.,2013)99 52 29 0.56 0.3 0.38Scalewelis (Jorisand Ferr?e, 2013)99 70 32 0.46 0.32 0.38RTV (Cristina etal., 2013)99 55 30 0.55 0.3 0.39Intui2 (Corina,2013)99 99 28 0.28 28 0.28SWIP (Pradel et al.,2013)99 21 15 0.71 0.15 0.25Ours 99 75 45 0.6 0.46 0.52QALD-420gAnswer 50 25 16 0.64 0.32 0.43Intui3 50 33 10 0.30 0.2 0.24ISOFT 50 50 10 0.2 0.2 0.2RO FII 50 50 6 0.12 0.12 0.12Ours 50 26 15 0.58 0.3 0.4Table 8: Comparisons with state-of-the-art sys-tems using the QALD benchmark.18The POS tag of the head word in the phrase19The type of semantic item20Because the QALD-4 conference does not start un-til after submission, we have no citation for the state-of-5.3.4 The Effect of Different FormulasTo determine which formulas are more useful forQALD, we evaluate the performance of the pro-posed method with different predicate sets.
Wesubtract one weighted formula from the originalsets at a time, except retaining the first two for-mulas sf1 and sf2 for basic inference.
Because ofspace limitations, only the results using QALD-3testing set are shown in Table 9.From the results, we can observe that remov-ing some formulas can boost the performance onsome single tasks, but employing all formulas canproduce the best performance.
This illustrates thatsolely resolving the steps in QALD (phrase detec-tion, phrase mapping, semantic items grouping)can obtain local results, and that making joint in-ference is necessary and useful.6 Related WorkOur proposed method is related to two lines ofwork: Question Answering over Knowledge basesand Markov Logic Networks.Question answering over knowledge baseshas attracted a substantial amount of interest overa long period of time.
The initial attempts in-cluded BaseBall (Green Jr et al., 1961) and Lu-nar (Woods, 1977).
However, these systems weremostly limited to closed domains due to a lack ofknowledge resources.
With the rapid developmentof structured data, such as DBpedia, Freebase andYago, the need for providing user-friendly inter-face to these data has become increasingly urgent.Keyword (Elbassuoni and Blanco, 2011) and se-mantic (Pound et al., 2010) searches are limitedto their ability to specify the relations among thedifferent keywords.The open topic progress has also been pushedby the QALD evaluation campaigns (Walter et al.,2012).
Lopez et al.
(2011) gave a comprehensivesurvey in this research area.
The authors devel-oped the PowerAqua system (Lopez et al., 2006) tothe-art systems in QALD-4.
The results can be found athttp://greententacle.techfak.uni-bielefeld.de/ cunger/qald.1100FormulasPD PM MG AvgP R F1 P R F1 P R F1 P R F1All Formulas 0.941 0.941 0.941 0.878 0.918 0.898 0.636 0.798 0.708 0.839 0.901 0.869-sf3 0.931 0.927 0.929 0.877 0.913 0.895 0.637 0.816 0.715 0.834 0.897 0.864-sf4 0.926 0.917 0.922 0.852 0.883 0.867 0.63 0.763 0.69 0.824 0.87 0.846-sf5 0.931 0.927 0.929 0.873 0.908 0.89 0.633 0.816 0.713 0.831 0.895 0.862-sf6 0.922 0.922 0.922 0.844 0.883 0.863 0.702 0.746 0.723 0.842 0.868 0.855-sf7 0.931 0.917 0.924 0.881 0.908 0.894 0.621 0.763 0.685 0.833 0.88 0.856-sf8 0.927 0.927 0.927 0.868 0.908 0.888 0.639 0.807 0.713 0.83 0.893 0.861Table 9: Performance comparisons of different weighted formulas evaluated using the QALD-3 questionset.answer questions on large, heterogeneous datasets.For questions containing quantifiers, comparativesor superlatives, Unger et al.
(2012) translatedNL to FL using several SPARQL templates andusing a set of heuristic rules mapping phrasesto semantic items.
The system most similar toours is DEANNA (Yahya et al., 2012).
However,DEANNA extracts predicate-argument structuresfrom the questions using three hand-written pat-terns.
Our system jointly learns these mappingsand extractions completely from scratch.Recently, the Semantic Parsing (SP) communitytargeted this problem from limited domains (Tangand Mooney, 2001; Liang et al., 2013) to open do-mains (Cai and Yates, 2013; Berant et al., 2013).The methods in semantic parsing answer questionsby first converting natural language utterances intomeaningful representations (e.g., the lambda cal-culus) and subsequently executing the formal log-ical forms over KBs.
Compared to deriving thecomplete logical representation, our method aimsto parse a question into a limited logic form withthe semantic item query, which we believe is moreappropriate for answering factoid questions.Markov Logic Networks have been widelyused in NLP tasks.
Huang (2012) applied MLNto compress sentences by formulating the task as aword/phrase deletion problem.
Fahrni and Strube(2012) jointly disambiguated and clustered con-cepts using MLN.
MLN has also been used incoreference resolution (Song et al., 2012).
Forthe task of identifying subjective text segmentsand of extracting their corresponding explanationsfrom product reviews, Zhang et al.
(2013) mod-eled these segments with MLN.
To discover log-ical knowledge for deep question answering, Liu(2012) used MLN to resolve the inconsistenciesof multiple knowledge bases.Meza-Ruiz and Riedel (2009) employed MLNfor Semantic Role Labeling (SRL).
They jointlyperformed the following tasks for a sentence:predicate identification, frame disambiguation, ar-gument identification and argument classification.The semantic analysis of SRL solely rested onthe lexical level, but our analysis focuses on theknowledge-base level and aims to obtain an exe-cutable query and to support natural language in-ference.7 Conclusions and Future WorkFor the task of QALD, we present a joint learn-ing framework for phrase detection, phrase map-ping and semantic item grouping.
The novelty ofour method lies in the fact that we perform jointinference and pattern learning for all subtasks inQALD using first-order logic.
Our experimentalresults demonstrate the effectiveness of the pro-posed method.In the future, we plan to address the follow-ing limitations that still exist in the current sys-tem: a) numerous hand-labeled data are requiredfor training the MLN, and we could use a la-tent form of semantic item query graphs (Liang etal., 2013); b) more robust solutions can be devel-oped to find the implicit relations in questions; c)our system can be scaled up to large-scale open-domain knowledge bases (Fader et al., 2013; Yaoand Van Durme, 2014); and d) the learning systemhas the advantage of being easily adapted to newsettings, and we plan to extend it to other domainsand languages (Liang and Potts, 2014).AcknowledgmentsThe authors are grateful to the anonymous re-viewers for their constructive comments.
Thiswork was sponsored by the National Basic Re-search Program of China (No.
2014CB340503)and the National Natural Science Foundation ofChina (No.
61202329, 61272332), CCF-TencentOpen Fund.
This work was also supported in partby Noahs Ark Lab of Huawei Tech.
Ltm.1101ReferencesS?oren Auer, Christian Bizer, Georgi Kobilarov, JensLehmann, Richard Cyganiak, and Zachary Ives.2007.
Dbpedia: A nucleus for a web of open data.In The semantic web, pages 722?735.
Springer.Junwei Bao, Nan Duan, Ming Zhou, and Tiejun Zhao.2014.
Knowledge-based question answering as ma-chine translation.
In ACL.Jonathan Berant, Andrew Chou, Roy Frostig, and PercyLiang.
2013.
Semantic parsing on freebase fromquestion-answer pairs.
In EMNLP.Christian Bizer, Tom Heath, and Tim Berners-Lee.2009.
Linked data-the story so far.
Internationaljournal on semantic web and information systems,5(3):1?22.Kurt Bollacker, Colin Evans, Praveen Paritosh, TimSturge, and Jamie Taylor.
2008.
Freebase: a col-laboratively created graph database for structuringhuman knowledge.
In SIGMOD.Qingqing Cai and Alexander Yates.
2013.
Large-scalesemantic parsing via schema matching and lexiconextension.
In ACL.Unger Christina and Andr Freitas.
2014.
Question an-swering over linked data: Challenges, approaches,trends.
In ESWC.Dima Corina.
2013.
Intui2: A prototype systemfor question answering over linked data.
In Work.Multilingual Question Answering over Linked Data(QALD-3).Giannone Cristina, Bellomaria Valentina, and BasiliRoberto.
2013.
A hmm-based approach to questionanswering against linked data.
In Work.
Multilin-gual Question Answering over Linked Data (QALD-3).Marie-Catherine De Marneffe, Bill MacCartney,Christopher D Manning, et al.
2006.
Generat-ing typed dependency parses from phrase structureparses.
In LREC.Shady Elbassuoni and Roi Blanco.
2011.
Keywordsearch over rdf graphs.
In CIKM.Anthony Fader, Stephen Soderland, and Oren Etzioni.2011.
Identifying relations for open information ex-traction.
In EMNLP.Anthony Fader, Luke Zettlemoyer, and Oren Etzioni.2013.
Paraphrase-driven learning for open questionanswering.
In ACL.Angela Fahrni and Michael Strube.
2012.
Jointlydisambiguating and clustering concepts and entitieswith markov logic.
In COLING.Andre Freitas and Edward Curry.
2014.
Naturallanguage queries over heterogeneous linked datagraphs: A distributional-compositional semanticsapproach.
In IUI.Bert F Green Jr, Alice K Wolf, Carol Chomsky, andKenneth Laughery.
1961.
Baseball: an automaticquestion-answerer.
In Papers presented at the May9-11, 1961, western joint IRE-AIEE-ACM computerconference, pages 219?224.
ACM.Shizhu He, Shulin Liu, Yubo Chen, Guangyou Zhou,Kang Liu, and Jun Zhao.
2013.
Casia@qald-3:A question answering system over linked data.
InWork.
Multilingual Question Answering over LinkedData (QALD-3).Minlie Huang, Xing Shi, Feng Jin, and Xiaoyan Zhu.2012.
Using first-order logic to compress sentences.In AAAI.Paul.
Jaccard.
1908.
Nouvelles recherches sur la dis-tribution florale.
Bulletin de la Soci`ete Vaudense desSciences Naturelles, 44:223?270.Guyonvarc?H Joris and S?ebastien Ferr?e.
2013.Scalewelis: a scalable query-based faceted searchsystem on top of sparql endpoints.
In Work.Multilingual Question Answering over Linked Data(QALD-3).Jens Lehmann, Tim Furche, Giovanni Grasso, Axel-Cyrille Ngonga Ngomo, Christian Schallhart, An-drew Sellers, Christina Unger, Lorenz B?uhmann,Daniel Gerber, Konrad H?offner, et al.
2012.
Deqa:deep web extraction for question answering.
InISWC.Percy Liang and Christopher Potts.
2014.
Bringingmachine learning and compositional semantics to-gether.
Annual Reviews of Linguistics (to appear).Percy Liang, Michael I Jordan, and Dan Klein.
2013.Learning dependency-based compositional seman-tics.
Computational Linguistics, 39(2):389?446.Zhao Liu, Xipeng Qiu, Ling Cao, and Xuanjing Huang.2012.
Discovering logical knowledge for deep ques-tion answering.
In CIKM.Vanessa Lopez, Enrico Motta, and Victoria Uren.2006.
Poweraqua: Fishing the semantic web.
InThe Semantic Web: research and applications, pages393?410.
Springer.Vanessa Lopez, Victoria Uren, Marta Sabou, and En-rico Motta.
2011.
Is question answering fit for thesemantic web?
: a survey.
Semantic Web, 2(2):125?155.Ivan Meza-Ruiz and Sebastian Riedel.
2009.
Jointlyidentifying predicates, arguments and senses usingmarkov logic.
In NAACL.Tomas Mikolov, Martin Karafi?at, Lukas Burget, JanCernock`y, and Sanjeev Khudanpur.
2010.
Recur-rent neural network based language model.
In IN-TERSPEECH, pages 1045?1048.Ndapandula Nakashole, Gerhard Weikum, and FabianSuchanek.
2012.
Patty: a taxonomy of relationalpatterns with semantic types.
In EMNLP.1102Gonzalo Navarro.
2001.
A guided tour to approximatestring matching.
ACM Comput.
Surv., 33(1):31?88.Jeffrey Pound, Ihab F Ilyas, and Grant Weddell.
2010.Expressive and flexible access to web-extracteddata: a keyword-based structured query language.In SIGMOD.C Pradel, G Peyet, O Haemmerl?e, and N Hernandez.2013.
Swip at qald-3: results, criticisms and les-son learned (working notes).
In Work.
MultilingualQuestion Answering over Linked Data (QALD-3).Matthew Richardson and Pedro Domingos.
2006.Markov logic networks.
Machine learning, 62(1-2):107?136.Saeedeh Shekarpour, Axel-Cyrille Ngonga Ngomo,and S?oren Auer.
2013.
Question answering on in-terlinked data.
In WWW.Yang Song, Jing Jiang, Wayne Xin Zhao, Sujian Li,and Houfeng Wang.
2012.
Joint learning for coref-erence resolution with markov logic.
In EMNLP.Fabian M Suchanek, Gjergji Kasneci, and GerhardWeikum.
2007.
Yago: a core of semantic knowl-edge.
In WWW.Lappoon R. Tang and Raymond J. Mooney.
2001.
Us-ing multiple clause constructors in inductive logicprogramming for semantic parsing.
In Proceedingsof the 12th European Conference on Machine Learn-ing, pages 466?477.Christina Unger, Lorenz B?uhmann, Jens Lehmann,Axel-Cyrille Ngonga Ngomo, Daniel Gerber, andPhilipp Cimiano.
2012.
Template-based questionanswering over rdf data.
In WWW.Sebastian Walter, Christina Unger, Philipp Cimiano,and Daniel B?ar.
2012.
Evaluation of a layeredapproach to question answering over linked data.In The Semantic Web?ISWC 2012, pages 362?374.Springer.William A Woods.
1977.
Lunar rocks in natural en-glish: Explorations in natural language question an-swering.
In Linguistic structures processing, pages521?569.Mohamed Yahya, Klaus Berberich, Shady Elbas-suoni, Maya Ramanath, Volker Tresp, and GerhardWeikum.
2012.
Natural language questions for theweb of data.
In EMNLP.Mohamed Yahya, Klaus Berberich, Shady Elbassuoni,and Gerhard Weikum.
2013.
Robust question an-swering over the web of linked data.
In CIKM.Xuchen Yao and Benjamin Van Durme.
2014.
Infor-mation extraction over structured data: Question an-swering with freebase.
In ACL.Qi Zhang, Jin Qian, Huan Chen, Jihua Kang, and Xu-anjing Huang.
2013.
Discourse level explanatoryrelation extraction from product reviews using first-order logic.
In ACL.Lei Zou, Ruizhe Huang, Haixun WangZou, Jeffrey XuYu, Wenqiang He, and Dongyan Zhao.
2014.
Natu-ral language question answering over rdf ?
a graphdata driven approach.
In SIGMOD.1103
