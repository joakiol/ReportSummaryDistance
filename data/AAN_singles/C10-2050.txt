Coling 2010: Poster Volume, pages 436?444,Beijing, August 2010ABSTRACTRe-ranking for Information Retrievalaims to elevate relevant feedbacks anddepress negative ones in initial retrievalresult list.
Compared to relevance feed-back-based re-ranking method widelyadopted in the literature, this paper pro-poses a new method to well use threefeatures in known negative feedbacks toidentify and depress unknown negativefeedbacks.
The features include: 1) theminor (lower-weighted) terms in negativefeedbacks; 2) hierarchical distance (HD)among feedbacks in a hierarchical clus-tering tree; 3) obstinateness strength ofnegative feedbacks.
We evaluate themethod on the TDT4 corpus, which ismade up of news topics and their relevantstories.
And experimental results showthat our new scheme substantially out-performs its counterparts.1.
INTRODUCTIONWhen we start out an information retrieval jour-ney on a search engine, the first step is to enter aquery in the search box.
The query seems to bethe most direct reflection of our informationneeds.
However, it is short and often out of stan-dardized syntax and terminology, resulting in alarge number of negative feedbacks.
Some re-searches focus on exploring long-term query logsto acquire query intent.
This may be helpful forobtaining information relevant to specific inter-ests but not to daily real-time query intents.
Es-pecially it is extremely difficult to determinewhether the interests and which of them shouldbe involved into certain queries.
Therefore, givena query, it is important to ?locally?
ascertain itsintent by using the real-time feedbacks.Intuitively it is feasible to expand the queryusing the most relevant feedbacks (Chum et al,2007).
Unfortunately search engines just offer?farraginous?
feedbacks (viz.
pseudo-feedback)which may involve a great number of negativefeedbacks.
And these negative feedbacks neverhonestly lag behind relevant ones in the retrievalresults, sometimes far ahead because of theirgreat literal similarity to query.
These noisyfeedbacks often mislead the process of learningquery intent.For so long, there had no effective approachesto confirm the relevance of feedbacks until theusage of the web click-through data (Joachims etal., 2003).
Although the data are sometimes in-credible due to different backgrounds and habitsof searchers, they are still the most effective wayto specify relevant feedbacks.
This arouses re-cent researches about learning to rank based onsupervised or semi-supervised machine learningmethods, where the click-through data, as thedirect reflection of query intent, offer reliabletraining data to learning the ranking functions.Although the learning methods achieve sub-stantial improvements in ranking, it can be foundthat lots of ?obstinate?
negative feedbacks stillpermeate retrieval results.
Thus an interestingquestion is why the relevant feedbacks are ableto describe what we really need, but weakly repelwhat we do not need.
This may attribute to theinherent characteristics of pseudo-feedback, i.e.their high literal similarity to queries.
Thus nomatter whether query expansion or learning torank, they may fall in the predicament that ?fa-voring?
relevant feedbacks may result in ?favor-ing?
negative ones, and that ?hurting?
negativefeedbacks may result in ?hurting?
relevant ones.However, there are indeed some subtle differ-ences between relevant and negative feedbacks,e.g.
the minor terms (viz.
low-weighted terms intexts).
Although these terms are often ignored inNegative Feedback: The Forsaken Nature Available for Re-rankingYu Hong, Qing-qing Cai, Song Hua, Jian-min Yao, Qiao-ming ZhuSchool of Computer Science and Technology, Soochow Universityjyao@suda.edu.cn436relevance measurement because their little effecton mining relevant feedbacks that have the sametopic or kernel, they are useful in distinguishingrelevant feedbacks from negative ones.
As a re-sult, these minor terms provides an opportunityto differentiate the true query intent from itscounterpart intents (called ?opposite intents?thereafter in this paper).
And the ?opposite in-tents?
are adopted to depress negative feedbackswithout ?hurting?
the ranks of relevant feedbacks.In addition, hierarchical clustering tree is helpfulto establish the natural similarity correlationamong information.
So this paper adopts the hi-erarchical distance among feedbacks in the treeto enhance the ?opposite intents?
based divisionof relevant and negative feedbacks.
Finally, anobstinateness factor is also computed to dealwith some obstinate negative feedbacks in thetop list of retrieval result list.
In fact, Teevan(Teevan et al, 2008) observed that most search-ers tend to browse only a few feedbacks in thefirst one or two result pages.
So our method fo-cuses on improving the precision of highlyranked retrieval results.The rest of the paper is organized as follows.Section 2 reviews the related work.
Section 3describes our new irrelevance feedback-basedre-ranking scheme and the HD measure.
Section4 introduces the experimental settings while Sec-tion 5 reports experimental results.
Finally, Sec-tion 6 draws the conclusion and indicates futurework.2.
RELATED WORKOur work is motivated by information searchbehaviors, such as eye-tracking and click through(Joachims, 2003).
Thereinto, the click-throughbehavior is most widely used for acquiring queryintent.
Up to  present, several interesting fea-tures, such as click frequency and hit time onclick graph (Craswell et al, 2007), have beenextracted from click-through data to improvesearch results.
However, although effective onquery learning, they fail to avoid the thornyproblem that even when the typed query and theclick-through data are the same, their intents maynot be the same for different searchers.A considerable number of studies have ex-plored pseudo-feedback to learn query intent,thus refining page ranking.
However, most ofthem focus on the relevant feedbacks.
It is untilrecently that negative ones begin to receive someattention.
Zhang (Zhang et al, 2009) utilize theirrelevance distribution to estimate the true rele-vance model.
Their work gives the evidence thatnegative feedbacks are useful in the rankingprocess.
However, their work focuses on gener-ating a better description of query intent to attractrelevant information, but ignoring that negativefeedbacks have the independent effect on repel-ling their own kind.
That is, if we have a king,we will not refuse a queen.
In contrast, Wang(Wang et al, 2008) benefit from the independenteffect from the negative feedbacks.
Their methodrepresents the opposite of query intent by usingnegative feedbacks and adopts that to discountthe relevance of each pseudo-feedback to a query.However, their work just gives a hybrid repre-sentation of opposite intent which may overlapmuch with the relevance model.
Although an-other work (Wang et al, 2007) of them filtersquery terms from the opposite intent, such filter-ing makes little effect because of the sparsity ofthe query terms in pseudo-feedback.Other related work includes query expansion,term extraction and text clustering.
In fact, queryexpansion techniques are often the chief benefi-ciary of click-through data (Chum et al, 2007).However, the query expansion techniques viaclicked feedbacks fail to effectively repel nega-tive ones.
This impels us to focus on un-clickedfeedbacks.
Cao (Cao et al, 2008) report the ef-fectiveness of selecting good expansion terms forpseudo-feedback.
Their work gives us a hintabout the shortcomings of the one-sided usage ofhigh-weighted terms.
Lee (Lee et al, 2008) adopta cluster-based re-sampling method to emphasizethe core topic of a query.
Their repeatedly feed-ing process reveals the hierarchical relevance ofpseudo-feedback.3.
RE-RANKING SCHEME3.1 Re-ranking SchemeThe re-ranking scheme, as shown in Figure 1,consists of three components: acquiring negativefeedbacks, measuring irrelevance feedbacks andre-ranking pseudo-feedback.Given a query and its search engine results, westart off the re-ranking process after a triggerpoint.
The point may occur at the time whensearchers click on ?next page?
or any hyperlink.437All feedbacks before the point are assumed tohave been seen by searchers.
Thus the un-clickedfeedbacks before the point will be treated as theknown negative feedbacks because they attractno attention of searchers.
This may be questionedbecause searchers often skip some hyperlinksthat have the same contents as before, even if thelinks are relevant to their interests.
However,such skip normally reflects the true searchingintent because novel relevant feedbacks alwayshave more attractions after all.Figure 1.
Re-ranking schemeAnother crucial step after the trigger point is togenerate the opposite intent by using the knownnegative feedbacks.
But now we temporarilyleave the issue to Section 3.2 and assume that wehave obtained a good representation of the oppo-site intent, and meanwhile that of query intenthas been composed of the highly weighted termsin the known relevant feedbacks and query terms.Thus, given an unseen pseudo-feedback, we cancalculate its overall ranking score predisposed tothe opposite intent as follows:scoreIscoreOscoreR ___ ?
?= ?
(1)where the O_score is the relevance score to theopposite intent, I_score is that to the query intentand ?
is a weighting factor.
On the basis, were-rank the unseen feedbacks in ascending order.That is, the feedback with the largest score ap-pears at the bottom of the ranked list.It is worthwhile to emphasize that although theoverall ranking score, i.e.
R_score, looks similarto Wang (Wang et al, 2008) who adopts the in-versely discounted value (i.e.
the relevance scoreis calculated as -scoreI _ scoreO _?? )
to re-rankfeedbacks in descending order, they are actuallyquite different because our overall ranking scoreas shown in Equation (1) is designed to depressnegative feedbacks, thereby achieving the similareffect to filtering.3.2 Representing Opposite IntentIt is necessary for the representation of oppositeintent to obey two basic rules: 1) the oppositeintent should be much different from the queryintent; and 2) it should reflect the independenteffect of negative feedbacks.Given a query, it seems easy to represent itsopposite intent by using a vector ofhigh-weighted terms of negative feedbacks.However, the vector is actually a ?close relative?of query intent because the terms often havemuch overlap with that of relevant feedbacks.And the overlapping terms are exactly the sourceof the highly ranked negative feedbacks.
Thuswe should throw off the overlapping terms andfocus on the rest instead.In this paper, we propose two simple facilitiesin representing opposite intent.
One is a vector ofthe weighted terms (except query terms) occur-ring in the known negative feedbacks, named as)( qO ?
, while another further filters out thehigh-weighted terms occurring in the knownrelevant feedbacks, named as .
Although )( rqO ??
)( qO ?
filters out query terms, the terms are sosparse that they contribute little to opposite intentlearning.
Thus, we will not explore  fur-ther in this paper (Our preliminary experimentsconfirm our reasoning).
In contrast,  notonly differs from the representation of query in-tent due to its exclusion of query terms but alsoemphasize the low-weighted terms occurring innegative feedbacks due to exclusion ofhigh-weighted terms occurring in the knownrelevant feedbacks.
)( qO ?
)( rqO ?
?3.3 Employing Opposite IntentAnother key issue in our re-ranking scheme ishow to measure the relevance of all the feed-backs to the opposite intent, i.e.
O_score, therebythe ranking score R_score.
For simplicity, weonly consider Boolean measures in employingopposite intent to calculate the ranking scoreR_score.Assume that given a query, there areknown relevant feedbacks andNN  known nega-tive ones.
First, we adopt query expansion to ac-quire the representation of query intent.
This isdone by pouring all terms of the  relevantfeedbacks and query terms into a bag of words,where all the occurring weights of each term areN438accumulated, and extracting n top-weightedterms to represent the query intent as )( rqI ++ .Then, we use the N  negative feedbacks to rep-resent the n-dimensional opposite intents.
For any unseen pseudo-feedback u, wealso represent it using an n-dimensional vectorwhich contains its n top-weighted terms.
Inall the representation processes, the TFIDFweighting is adopted.
)( rqO ??
)(uVThus, for an unseen pseudo-feedback u, therelevance scores to the query intent and the op-posite intent can be measured as:(2)}  )(  ),(  {)(_}  )(  ),(  {)(_rqOuVBuscoreOrqIuVBuscoreI?
?=++=where  indicates Boolean calculation: },{ ??B(3)????
?=?=?YxifYxifYxbXxYxbYXBiiiii,0,1},{},,{},{In particular, we simply set the factor ?
, asmentioned in Equation (1), to 1 so as to balancethe effect of query intent and its opposite intenton the overall ranking score.
The intuition is thatif an unseen pseudo-feedback has more overlap-ping terms with )( rqO ??
than , it willhas higher probability of being depressed as annegative feedback.
)( rqI ++Two alternatives to the above Boolean meas-ure are to employ the widely-adopted VSM co-sine measure and Kullback-Liebler (KL) diver-gence (Thollard et al, 2000).
However, suchterm-weighting alternatives will seriously elimi-nate the effect of low-weighted terms, which iscore of our negative feedback-based re-rankingscheme.3.4 Hierarchical Distance (HD) MeasureThe proposed method in Section 3.3 ignorestwo key issues.
First, given a query, althoughsearch engine has thrown away most oppositeintents, it is unavoidable that thepseudo-feedback still involves more than oneopposite intent.
However, the representationhas the difficulty in highlighting all theopposite intents because the feature fusion of therepresentation smoothes the independent charac-teristics of each opposite intent.
Second, givenseveral opposite intents, they have different lev-els of effects on the negative score .And the effects cannot be measured by the uni-lateral score.
)( rqO ??
)(_ uscoreOFigure 2.
Weighted distance calculationTo solve the issues, we propose a hierarchicaldistance based negative measure, abbr.
HD,which measures the distances among feedbacksin a hierarchical clustering tree, and involvesthem into hierarchical division of relevance score.Given two random leaves u and v in the tree,their HD score is calculated as:),(),(),(_vuWvurelvuscoreHD =           (4)where ),( ?
?rel  indicates textual similarity, ),( ?
?Windicates the weighted distance in the tree, whichis calculated as:?
?=mii vuwvuW ),(),(              (5)where m is the total number of the edges betweentwo leaves,  indicates the weight of thei-th edge.
In this paper, we adopt CLUTO togenerate the hierarchical binary tree, and simplylet each  equal 1.
Thus the),( ?
?iw),( ?
?iw ),( ?
?W  be-comes to be the number of edges m, for example,the  equals 5 in Figure 2.
),( kjWOn the basis, given an unseen feedback u, wecan acquire its modified re-ranking scorescoreR _ ?
by following steps.
First, we regardeach known negative feedback as an oppositeintent, following the two generative rules (men-tioned in section 3.2) to generate itsn-dimensional representation .
Addition-ally we represent both the known relevant feed-backs and the unseen feedback u asn-dimensional term vectors.
Second, we clusterthese feedbacks to generate a hierarchical binarytree and calculate the HD score for each pair of)( rqO ??
),( ?u , where ?
denotes a leaf in the tree except u.Thus the modified ranking score is calculated as:?
??
?
?=?Ni Njji vuscoreHDIvuscoreHDIscoreR ),(_),(__ (6)where iv  indicates the i-th known negativefeedback in the leaves, N  is the total number of439v , j  indicates the j-th known relevant feed-back,  is the total number ofvN v .
Besides, westill adopt Boolean value to measure the textualsimilarity  in both clustering process andranking score calculation, thus the HD score inthe formula (6) can be calculated as follows:),( ?
?rel),()(_),(_),(}  )(  ),(  {),(_vuWuscoreOvuscoreHDvuWvVuVBvuscoreHD==(7)3.5 Obstinateness FactorAdditionally we involve an interesting feature,i.e.
the obstinate degree, into our re-rankingscheme.
The degree is represented by the rank ofnegative feedbacks in the original retrieval re-sults.
That is, the more ?topping the list?
annegative feedback is, the more obstinate it is.Therefore we propose a hypothesis that if afeedback is close to the obstinate feedback, itshould be obstinate too.
Thus given an unseenfeedback u, its relevance to an opposite intent inHD can be modified as:)(_)1()(_ uscoreOrnkuscoreO ?+=?
?
(8)where  indicates the rank of the oppositeintent in original retrieval results (Note: in HD,every known negative feedback is an oppositeintent),rnk?
is a smoothing factor.
Because as-cending order is used in our re-ranking process,by the weighting coefficient, i.e.
)/1( rnk?+ , thefeedback close to the obstinate opposite intentswill be further depressed.
But the coefficient isnot commonly used.
In HD, we firstly ascertainthe feedback closest to u, and if the feedback isknown to be negative, set to maxv , we will usethe Equation (8) to punish the pair of (u, maxv )alone, otherwise without any punishment.4.
EXPERIMENTAL SETTING4.1 Data SetWe evaluate our methods with two TDT collec-tions: TDT 2002 and TDT 2003.
There are 3,085stories in the TDT 2002 collection are manuallylabeled as relevant to 40 news topics, 30,736ones irrelevant to any of the topics.
And 3,083news stories in the TDT 2003 collection are la-beled as relevant to another 40 news topics,15833 ones irrelevant to them.
In our evaluation,we adopt TDT 2002 as training set, and TDT2003 as test set.
Besides, only English stories areused, both Mandarin and Arabic ones are re-placed by their machine-translated versions (i.e.mttkn2 released by LDC).Corpus good fair poorTDT 2002 26 7 7TDT 2003 22 10 8Table 1.
Number of queries referring to differenttypes of feedbacks (Search engine: Lucene 2.3.2)In our experiments, we realize a simple searchengine based on Lucene 2.3.2 which appliesdocument length to relevance measure on thebasis of traditional literal term matching.
Toemulate the real retrieval process, we extract thetitle from the interpretation of news topic andregard it as a query, and then we run the searchengine on the TDT sets and acquire the first 1000pseudo-feedback for each query.
All feedbackswill be used as the input of our re-ranking proc-ess, where the hand-crafted relevant stories de-fault to the clicked feedbacks.
By the search en-gine, we mainly obtain three types ofpseudo-feedback: ?good?, ?fair?
and ?poor?,where ?good?
denotes that more than 5 clicked(viz.
relevant) feedbacks are in the top 10, ?fair?denotes more than 2 but less than 5, ?poor?
de-notes less than 2.
Table 1 shows the number ofqueries referring to different types of feedbacks.4.2 Evaluation MeasureWe use three evaluation measures in experiments,P@n, NDCG@n and MAP.
Thereinto, P@n de-notes the precision of top n feedbacks.
On thebasis, NDCG takes into account the influence ofposition to precision.
NDCG at position n is cal-culated as:nniurn ZiNDCGZnNDCGi?= +?=?= 1)()1log(12@1@    (9)where i is the position in the result list, Zn is anormalizing factor and chosen so that for theperfect list DCG at each position equals one, andr(ui) equals 1 when ui is relevant feedback, else 0.While MAP additionally takes into account recall,calculated as:?
?= = ?= mi kj ijii jpurRmMAP 1 1 ))@()((11    (10)where m is the total number of queries, so MAPgives the average measure of precision and recall440for multiple queries, Ri is the total number offeedbacks relevant to query i, and k is the num-ber of pseudo-feedback to the query.
Here k isindicated to be 1000, thus Map can give the av-erage measure for all positions of result list.4.3 SystemsWe conduct experiments using four main sys-tems, in which the search engine based on Lu-cene 2.3.2, regarded as the basic retrieval system,provides the pseudo-feedback for the followingthree re-ranking systems.Exp-sys: Query is expanded by the first N knownrelevant feedbacks and represented by ann-dimensional vector which consists of n distinctterms.
The standard TFIDF-weighted cosinemetric is used to measure the relevance of theunseen pseudo-feedback to query.
And the rele-vance-based descending order is in use.Wng-sys: A system realizes the work of Wang(Wang et al, 2008), where the known relevantfeedbacks are used to represent query intent, andthe negative feedbacks are used to generate op-posite intent.
Thus, the relevance score of a feed-back is calculated as I_scorewng- O_score?w?
wng,and the relevance-based descending order is usedin re-ranking.Our-sys: A system is approximately similar toWng-sys except that the relevance is measured byO_scoreour- ??
I_scoreour and the pseudo-feedbackis re-ranked in ascending order.Additionally both Wng-sys and Our-sys havethree versions.
We show them in Table 2, where?I?
corresponds to the generation rule of queryintent, ?O?
to that of opposite intent, Rel.
meansrelevance measure, u is an unseen feedback, v isa known relevant feedback, v  is a known nega-tive feedback.5.
RESULTS5.1 Main Training ResultWe evaluate the systems mainly in two circum-stances: when both  and N N  equal 1 andwhen they equal 5.
In the first case, we assumethat retrieval capability is measured under givenfew known feedbacks; in the second, we emulatethe first page turning after several feedbackshave been clicked by searchers.
Besides, the ap-proximately optimal value of n for the Exp-sys,which is trained to be 50, is adopted as the globalvalue for all other systems.
The training resultsare shown in Figure 3, where the Exp-sys nevergains much performance improvement when n isgreater than 50.
In fairness to effects of ?I?
and?O?
on relevance measure, we also make nequal 50.
In addition, all the discount factors(viz.?
, ?
w2 and ?
w3) initially equal 1, and thesmoothing factor ?
is trained to be 0.5.Table 2.
All versions of both Wngs and OursFigure 3.
Parameter training of Exp-sysFor each query we re-rank all thepseudo-feedback, including that defined asknown, so P@20 and NDCG@20 are in use toavoid over-fitting (such as P@10 andNDCG@10 given both  and N N  equal 5 ).We show the main training results in Table 3,where our methods achieve much better per-formances than the re-ranking methods based onrelevant feedback learning when N= N =5.Thereinto, our basic system, i.e.
Our-sys1, atleast achieves approximate 5% improvement onP@20, 3% on NDCG@20 and 1% on MAP thanthe optimal wng-sys (viz.
wng-sys1).
And obvi-?I?
n-dimensional vector for each v, Number of v in use is N?O?
NoneWng-sys1Rel.
NvuscoreRNiw /)),cos((_11 ?==?I?Number of v in use is N, all v combine into a n-dimensionalbag of words bw2?O?Number of v  in use is N , all v combine into an-dimensional words bag 2wbWng-sys2Rel.
),cos(),cos(_ 2222 wwww bubuscoreR ?
?= ??I?
?O?Similar generation rules to Wng-sys2 except that queryterms are removed from bag of words  and 3wb 3wb  Wng-sys3Rel.
),cos(),cos(_ 3333 wwww bubuscoreR ?
?= ??I?
)( rqI ++  in section 3.3?O?
)( rqO ??
in section 3.2 Our-sys1Rel.
scoreIscoreOscoreR ___ ?
?= ??I?
?O?The same generation rules to Our-sys1Our-sys2Rel.HD algorithm: ?
??
?
?=?Ni Njji vuscoreHDIvuscoreHDIscoreR ),(_),(__?I?
?O?The same generation rules to Our-sys1Our-sys3Rel.HD algorithm + obstinateness factor:)(_)1()(_ uscoreOrnkuscoreO ?+=?
?441ously the most substantial improvements arecontributed by the HD measure which even in-creases the P@20 of Our-sys1 by 8.5%,NDCG@20 by 13% and MAP by 9%.
But it isslightly disappointing that the obstinateness fac-tor only has little effectiveness on performanceimprovement, although Our-sys3 nearly winsthe best retrieval results.
This may stem from?soft?
punishment on obstinateness, that is, foran unseen feedback, only the obstinate com-panion closest to the feedback is punished inrelevance measure.Table 3.
Main training resultsIt is undeniable that all the re-ranking systemswork worse than the basic search engine whenthe known feedbacks are rare, such as =N N =1.This motivates an additional test on the highervalues of both  and N N ( =N N =9), as shownin Table 4.
Thus it can be found that most of there-ranking systems achieve much better per-formance than the basic search engine.
An im-portant reason for this is that more key terms canbe involved into representations of both queryintent and its opposite intent.
So it seems thatmore manual intervention is always reliable.However in practice, seldom searchers are will-ing to use an unresponsive search engine that canonly offer relatively satisfactory feedbacks afterlots of click-through and page turning.
And infact at least two pages (if one page includes 10pseudo-feedback) need to be turned in the train-ing corpus when both  and N N  equal 9.
Sowe just regard the improvements benefiting fromhigh click-through rate as an ideal status, andstill adopt the practical numerical value ofandNN , i.e.
=N N =5, to run following test.5.2 Constraint from QueryA surprising result is that Exp-sys alwaysachieves the worst MAP value, even worse thanthe basic search engine even if high value of N isin use, such as the performance when N equal 9in Table 4.
It seems to be difficult to question thereasonability of the system because it alwaysselects the most key terms to represent query in-tent by query expansion.
But an obvious differ-ence between Exp-sys and other re-ranking sys-tems could explain the result.
That is the queryterms consistently involved in query representa-tion by Exp-sys.Table 4.
Effects of  and N N  on re-rankingperformance (when =N N =9, n= n =50)In fact, Wng-sys1 never overly favor the queryterms because they are not always the main bodyof an independent feedback, and our systemseven remove the query terms from the oppositeintent directly.
Conversely Exp-sys continuouslyenhances the weights of query terms which resultin over-fitting and bias.
The visible evidence forthis is shown in Figure 4, where Exp-sysachieves better Precision and NDCG than thebasic search engine at the top of result list butworse at the subsequent parts.
The results illus-trate that too much emphasis placed on queryterms in query expansion is only of benefit toelevating the originally high-ranked relevantfeedback but powerless to pull the straggler outof the bottom of result list.Figure 4.
MAP comparison (basic vs Exp)5.3 Positive Discount LossObviously Wang (Wang et al, 2008) has noticedthe negative effects of query terms on re-ranking.Therefore his work (reproduced by Wng-sys1, 2,3 in this paper) avoids arbitrarily enhancing theterms in query representation, even removesthem as Wng-sys3.
This indeed contributes to the- Our-sys1 Our-sys2 Exp-sys Wng-sys1 BasicP@20 0.6603 0.8141 0.63125 0.7051 0.6588NDCG@20 0.7614 0.8587 0.8080 0.7797 0.6944MAP 0.6583 0.7928 0.5955 0.7010 0.6440systems N = N P@20 NDCG@20 MAP FactorBasic - 0.6588 0.6944 0.6440 -1 0.4388 0.4887 0.3683 - Exp-sys5 0.5613 0.6365 0.5259 -1 0.5653 0.6184 0.5253 - Wng-sys15 0.6564 0.7361 0.6506 -1 0.5436 0.6473 0.4970 2w?
=1Wng-sys25 0.5910 0.7214 0.5642 2w?
=11 0.5436 0.6162 0.4970 3w?
=1Wng-sys35 0.5910 0.6720 0.5642 3w?
=11 0.5628 0.6358 0.4812 ?
=1 Our-sys15 0.7031 0.7640 0.6603 ?
=11 0.6474 0.6761 0.5967 ?
=1 Our-sys25 0.7885 0.8381 0.7499 ?
=11 0.6026 0.6749 0.5272 ?
=0.5Our-sys35 0.7897 0.8388 0.7464 ?
=0.5442improvement of the re-ranking system, such asthe better performances of Wng-sys1, 2, 3 shownin Table 3, although Wng-sys3 has no furtherimprovement than Wng-sys2 because of the spar-sity of query terms.
On the basis, the work re-gards the terms in negative feedbacks as noisesand reduces their effects on relevance measure asmuch as possible.
This should be a reasonablescheme, but interestingly it does not work well inour experiments.
For example, althoughWng-sys2 and Wng-sys3 eliminate the relevancescore calculated by using the terms in negativefeedbacks, they perform worse than Wng-sys1which never make any discount.systems ??
=0.5 ??
=1 ??
=2Our-sys1 0.4751 0.6603 0.6901Wng-sys2 0.6030 0.5642 0.4739Wng-sys3 0.6084 0.5642 0.4739Table 5.
Effects on MAPAdditionally when we increase the discountfactor 2w?
and 3w?
, as shown in Table 5, theperformances (MAP) of Wng-sys2 and Wng-sys3further decrease.
This illustrates that thehigh-weighted terms of high-ranked negativefeedbacks are actually not noises.
Otherwise whydo the feedbacks have high textual similarity toquery and even to their neighbor relevant feed-backs?
Thus it actually hurts real relevance todiscount the effect of the terms.Conversely Our-sys1 can achieve further im-provement when the discount factor ?
in-creases, as shown in Table 5.
It is because thediscount contributes to highlighting minor termsof negative feedbacks, and these terms alwayshave little overlap with the kernel of relevantfeedbacks.
Additionally the minor terms are usedto generate the main body of opposite intent inour systems, thus the discount can effectivelyseparate opposite intent from positive query rep-resentation.
Thereby we can use relatively purerepresentation of opposite intent to detect andrepel subsequent negative feedbacks.5.4 Availability of Minor TermsIntuitively we can involve more terms into queryrepresentation to alleviate the positive discountloss.
But it does not work in practice.
For exam-ple, Wng-sys2 shown in Figure 5 has no obviousimprovement no matter how many terms are in-cluded in query representation.
ConverselyOur-sys1 can achieve much more improvementwhen it involves more terms into the oppositeintent.
For example, when the number of termsincreases to 150, Our-sys1 has approximately 5%better MAP than Wng-sys2, shown in Figure 5.Figure 5.
Effects on MAP in modifying the di-mensionality n (when N= N =5, ?
=1)This result illustrates that minor terms areavailable for repelling negative feedbacks, buttoo weak to recall relevant feedbacks.
In fact, theminor terms are just the low-weighted terms intext.
Current text representation techniques oftenignore them because of their marginality.
How-ever minor terms can reflect fine distinctionsamong feedbacks, even if they have the sametopic.
And the distinctions are of great impor-tance when we determine why searchers say?Yes?
to some feedbacks but ?No?
to others.Table 6.
Main test results5.5 Test ResultWe run all systems on test corpus, i.e.
TDT2003,but only report four main systems: Wng-sys1,Our-sys1, Our-sys2 and Our-sys3.
Other systemsare omitted because of their poor performances.The test results are shown in Table 6 which in-cludes not only global performances for all testqueries but also local ones on three distinct typesof queries, i.e.
?good?, ?fair?
and ?poor?.
There-into, Our-sys2 achieves the best performancearound all types of queries.
So it is believablesystems metric good fair poor global FactorP@20 0.7682 0.5450 0.2643 0.6205NDCG@20 0.8260 0.6437 0.4073 0.7041Wng-sys1MAP 0.6634 0.4541 0.9549 0.6620-P@20 0.8273 0.5700 0.2643 0.6603NDCG@20 0.8679 0.6620 0.4017 0.7314Our-sys1MAP 0.6740 0.4573 0.9184 0.6623?
=2,?
=0.5P@20 0.8523 0.7600 0.2714 0.7244NDCG@20 0.8937 0.8199 0.4180 0.7894Our-sys2MAP 0.7148 0.6313 0.9897 0.7427?
=2,?
=0.5P@20 0.8523 0.7600 0.2714 0.7244NDCG@20 0.8937 0.8200 0.4180 0.7894Our-sys3MAP 0.7145 0.6292 0.9897 0.7420?
=2,?
=0.5443that hierarchical distance of clustering tree al-ways plays an active role in distinguishing nega-tive feedbacks from relevant ones.
But it is sur-prising that Our-sys3 achieves little worse per-formance than Our-sys2.
This illustrates poorrobustness of obstinateness factor.Interestingly, the four systems all achieve veryhigh MAP scores but low P@20 and NDCG@20for ?poor?
queries.
This is because the querieshave inherently sparse relevant feedbacks: lessthan 6?
averagely.
Thus the highest p@20 isonly approximate 0.3, i.e.
6/20.
And the lowNDCG@20 is in the same way.
Besides, allMAP scores for ?fair?
queries are the worst.
Wefind that this type of query involves more mac-roscopic features which results in more kernelsof negative feedbacks.
Although we can solvethe issue by increasing the dimensionality of op-posite intent, it undoubtedly impairs the effi-ciency of re-ranking.6.
CONCLUSIONThis paper proposes a new re-ranking schemeto well explore the opposite intent.
In particular,a hierarchical distance-based (HD) measure isproposed to differentiate the opposite intent fromthe true query intent so as to repel negativefeedbacks.
Experiments show substantial out-performance of our methods.Although our scheme has been proven effec-tive in most cases, it fails on macroscopic queries.In fact, the key difficulty of this issue lies in howto ascertain the focal query intent given variouskernels in pseudo-feedback.
Fortunately,click-through data provide some useful informa-tion for learning real query intent.
Although itseems feasible to generate focal intent represen-tation by using overlapping terms in clickedfeedbacks, such representation is just a reproduc-tion of macroscopic query since the overlappingterms can only reflect common topic instead offocal intent.
Therefore, it is important to segmentclicked feedbacks into different blocks, and as-certain the block of greatest interest to searchers.ReferencesAllan, J., Lavrenko, V., and Nallapati, R. 2002.UMass at TDT 2002, Topic Detection andTracking: Workshop.Craswell, N., and Szummer, M. Random walks onthe click graph.
2007.
In Proceedings of theConference on Research and Development inInformation Retrieval.
SIGIR '30.
ACM Press,New York, NY, 239-246.Cao, G. H., Nie, J. Y., and Gao, J. F. 2008.
StephenRobertson.
Selecting Good Expansion Terms forPseudo-Relevance Feedback.
In Proceedings ofthe Conference on Research and Development inInformation Retrieval.
SIGIR '31.
ACM Press,New York, NY, 243-250.Chum, O., Philbin, J., Sivic, J., and Zisserman, A.2007.
Automatic query expansion with a genera-tive feature model for object retrieval.
In Pro-ceedings of the 11th International Conference onComputer Vision, Rio de Janeiro, Brazil, 1?8.Joachims, T., Granka, L., and Pan, B.
2003.
Accu-rately Interpreting Clickthrough Data as ImplicitFeedback.
In Proceedings of the Conference onResearch and Development in Information Re-trieval.
SIGIR '28.
New York, NY, 154-161.Lee, K. S., Croft, W. B., and Allan, J.
2008 A Clus-ter-Based Resampling Method forPseudo-Relevance Feedback.
In Proceedings ofthe Conference on Research and Development inInformation Retrieval.
SIGIR '31.
ACM Press,New York, NY, 235-242.Thollard, F., Dupont, P., and Higuera, L.2000.Probabilistic DFA Inference Using Kull-back-Leibler Divergence and Minimality.
InProceedings of the 17th Int'l Conf on MachineLearning.
San Francisco: Morgan Kaufmann,975-982.Teevan, J. T., Dumais, S. T., and Liebling, D. J.2008.
To Personalize or Not to Personalize:Modeling Queries with Variation in User Intent.In Proceedings of the Conference on Researchand Development in Information Retrieval.SIGIR '31.
New York, NY, 163-170.Wang, X. H., Fang, H., and Zhai, C. X.
2008.
AStudy of Methods for Negative Relevance Feed-back.
In Proceedings of the Conference on Re-search and Development in Information Re-trieval.
SIGIR '31.
ACM Press, New York, NY,219-226.Wang, X. H., Fang, H., and Zhai, C. X.
2007.
Im-prove retrieval accuracy for difficult queries us-ing negative feedback.
In Proceedings of thesixteenth ACM conference on Conference oninformation and knowledge management.
ACMpress, New York, NY, USA, 991-994.Zhang, P., Hou, Y. X., and Song, D. 2009.
Ap-proximating True Relevance Distribution from aMixture Model based on Irrelevance Data.
InProceedings of the Conference on Research andDevelopment in Information Retrieval.
SIGIR'31.
ACM Press, New York, NY, 107-114.444
