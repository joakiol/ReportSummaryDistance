Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 91?99,Suntec, Singapore, 2-7 August 2009. c?2009 ACL and AFNLPLearning Semantic Correspondences with Less SupervisionPercy LiangUC Berkeleypliang@cs.berkeley.eduMichael I. JordanUC Berkeleyjordan@cs.berkeley.eduDan KleinUC Berkeleyklein@cs.berkeley.eduAbstractA central problem in grounded language acqui-sition is learning the correspondences between arich world state and a stream of text which refer-ences that world state.
To deal with the high de-gree of ambiguity present in this setting, we presenta generative model that simultaneously segmentsthe text into utterances and maps each utteranceto a meaning representation grounded in the worldstate.
We show that our model generalizes acrossthree domains of increasing difficulty?Robocupsportscasting, weather forecasts (a new domain),and NFL recaps.1 IntroductionRecent work in learning semantics has focusedon mapping sentences to meaning representa-tions (e.g., some logical form) given aligned sen-tence/meaning pairs as training data (Ge andMooney, 2005; Zettlemoyer and Collins, 2005;Zettlemoyer and Collins, 2007; Lu et al, 2008).However, this degree of supervision is unrealisticfor modeling human language acquisition and canbe costly to obtain for building large-scale, broad-coverage language understanding systems.A more flexible direction is grounded languageacquisition: learning the meaning of sentencesin the context of an observed world state.
Thegrounded approach has gained interest in variousdisciplines (Siskind, 1996; Yu and Ballard, 2004;Feldman and Narayanan, 2004; Gorniak and Roy,2007).
Some recent work in the NLP commu-nity has also moved in this direction by relaxingthe amount of supervision to the setting whereeach sentence is paired with a small set of can-didate meanings (Kate and Mooney, 2007; Chenand Mooney, 2008).The goal of this paper is to reduce the amountof supervision even further.
We assume that we aregiven a world state represented by a set of recordsalong with a text, an unsegmented sequence ofwords.
For example, in the weather forecast do-main (Section 2.2), the text is the weather report,and the records provide a structured representationof the temperature, sky conditions, etc.In this less restricted data setting, we must re-solve multiple ambiguities: (1) the segmentationof the text into utterances; (2) the identification ofrelevant facts, i.e., the choice of records and as-pects of those records; and (3) the alignment of ut-terances to facts (facts are the meaning represen-tations of the utterances).
Furthermore, in someof our examples, much of the world state is notreferenced at all in the text, and, conversely, thetext references things which are not represented inour world state.
This increased amount of ambigu-ity and noise presents serious challenges for learn-ing.
To cope with these challenges, we propose aprobabilistic generative model that treats text seg-mentation, fact identification, and alignment in asingle unified framework.
The parameters of thishierarchical hidden semi-Markov model can be es-timated efficiently using EM.We tested our model on the task of aligningtext to records in three different domains.
Thefirst domain is Robocup sportscasting (Chen andMooney, 2008).
Their best approach (KRISPER)obtains 67% F1; our method achieves 76.5%.
Thisdomain is simplified in that the segmentation isknown.
The second domain is weather forecasts,for which we created a new dataset.
Here, thefull complexity of joint segmentation and align-ment arises.
Nonetheless, we were able to obtainreasonable results on this task.
The third domainwe considered is NFL recaps (Barzilay and Lap-ata, 2005; Snyder and Barzilay, 2007).
The lan-guage used in this domain is richer by orders ofmagnitude, and much of it does not reference theworld state.
Nonetheless, taking the first unsuper-vised approach to this problem, we were able tomake substantial progress: We achieve an F1 of53.2%, which closes over half of the gap betweena heuristic baseline (26%) and supervised systems(68%?80%).91Dataset # scenarios |w| |T | |s| |A|Robocup 1919 5.7 9 2.4 0.8Weather 22146 28.7 12 36.0 5.8NFL 78 969.0 44 329.0 24.3Table 1: Statistics for the three datasets.
We report averagevalues across all scenarios in the dataset: |w| is the number ofwords in the text, |T | is the number of record types, |s| is thenumber of records, and |A| is the number of gold alignments.2 Domains and DatasetsOur goal is to learn the correspondence between atext w and the world state s it describes.
We usethe term scenario to refer to such a (w, s) pair.The text is simply a sequence of words w =(w1, .
.
.
, w|w|).
We represent the world state s asa set of records, where each record r ?
s is de-scribed by a record type r.t ?
T and a tuple offield values r.v = (r.v1, .
.
.
, r.vm).1 For exam-ple, temperature is a record type in the weatherdomain, and it has four fields: time, min, mean,and max.The record type r.t ?
T specifies the field typer.tf ?
{INT, STR, CAT} of each field value r.vf ,f = 1, .
.
.
,m. There are three possible fieldtypes?integer (INT), string (STR), and categori-cal (CAT)?which are assumed to be known andfixed.
Integer fields represent numeric propertiesof the world such as temperature, string fields rep-resent surface-level identifiers such as names ofpeople, and categorical fields represent discreteconcepts such as score types in football (touch-down, field goal, and safety).
The field type de-termines the way we expect the field value to berendered in words: integer fields can be numeri-cally perturbed, string fields can be spliced, andcategorical fields are represented by open-endedword distributions, which are to be learned.
SeeSection 3.3 for details.2.1 Robocup SportscastingIn this domain, a Robocup simulator generates thestate of a soccer game, which is represented bya set of event records.
For example, the recordpass(arg1=pink1,arg2=pink5) denotes a pass-ing event; this type of record has two fields: arg1(the actor) and arg2 (the recipient).
As the game isprogressing, humans interject commentaries aboutnotable events in the game, e.g., pink1 passes backto pink5 near the middle of the field.
All of the1To simplify notation, we assume that each record has mfields, though in practice, m depends on the record type r.t.fields in this domain are categorical, which meansthere is no a priori association between the fieldvalue pink1 and the word pink1.
This degree offlexibility is desirable because pink1 is sometimesreferred to as pink goalie, a mapping which doesnot arise from string operations but must insteadbe learned.We used the dataset created by Chen andMooney (2008), which contains 1919 scenariosfrom the 2001?2004 Robocup finals.
Each sce-nario consists of a single sentence representing afragment of a commentary on the game, pairedwith a set of candidate records.
In the annotation,each sentence corresponds to at most one record(possibly one not in the candidate set, in whichcase we automatically get that sentence wrong).See Figure 1(a) for an example and Table 1 forsummary statistics on the dataset.2.2 Weather ForecastsIn this domain, the world state contains de-tailed information about a local weather forecastand the text is a short forecast report (see Fig-ure 1(b) for an example).
To create the dataset,we collected local weather forecasts for 3,753cities in the US (those with population at least10,000) over three days (February 7?9, 2009) fromwww.weather.gov.
For each city and date, wecreated two scenarios, one for the day forecast andone for the night forecast.
The forecasts consist ofhour-by-hour measurements of temperature, windspeed, sky cover, chance of rain, etc., which rep-resent the underlying world state.This world state is summarized by recordswhich aggregate measurements over selected timeintervals.
For example, one of the records statesthe minimum, average, and maximum tempera-ture from 5pm to 6am.
This aggregation pro-cess produced 22,146 scenarios, each containing|s| = 36 multi-field records.
There are 12 recordtypes, each consisting of only integer and categor-ical fields.To annotate the data, we split the text by punc-tuation into lines and labeled each line with therecords to which the line refers.
These lines areused only for evaluation and are not part of themodel (see Section 5.1 for further discussion).The weather domain is more complex than theRobocup domain in several ways: The text w islonger, there are more candidate records, and mostnotably, w references multiple records (5.8 on av-92xbadPass(arg1=pink11,arg2=purple3)ballstopped()ballstopped()kick(arg1=pink11)turnover(arg1=pink11,arg2=purple3)sw:pink11 makes a bad pass and was picked off by purple3(a) Robocup sportscasting.
.
.rainChance(time=26-30,mode=Def)temperature(time=17-30,min=43,mean=44,max=47)windDir(time=17-30,mode=SE)windSpeed(time=17-30,min=11,mean=12,max=14,mode=10-20)precipPotential(time=17-30,min=5,mean=26,max=75)rainChance(time=17-30,mode=--)windChill(time=17-30,min=37,mean=38,max=42)skyCover(time=17-30,mode=50-75)rainChance(time=21-30,mode=--).
.
.sw:Occasional rain after 3am .Low around 43 .South wind between 11 and 14 mph .Chance of precipitation is 80 % .New rainfall amounts between aquarter and half of an inch possible .
(b) Weather forecasts.
.
.rushing(entity=richie anderson,att=5,yds=37,avg=7.4,lg=16,td=0)receiving(entity=richie anderson,rec=4,yds=46,avg=11.5,lg=20,td=0)play(quarter=1,description=richie anderson ( dal ) rushed left side for 13 yards .
)defense(entity=eric ogbogu,tot=4,solo=3,ast=1,sck=0,yds=0).
.
.s w:.
.
.Former Jets player Richie Andersonfinished with 37 yards on 5 carriesplus 4 receptions for 46 yards .. .
.
(c) NFL recapsFigure 1: An example of a scenario for each of the three domains.
Each scenario consists of a candidate set of records s and atext w. Each record is specified by a record type (e.g., badPass) and a set of field values.
Integer values are in Roman, stringvalues are in italics, and categorical values are in typewriter.
The gold alignments are shown.erage), so the segmentation of w is unknown.
SeeTable 1 for a comparison of the two datasets.2.3 NFL RecapsIn this domain, each scenario represents a singleNFL football game (see Figure 1(c) for an exam-ple).
The world state (the things that happenedduring the game) is represented by database tables,e.g., scoring summary, team comparison, drivechart, play-by-play, etc.
Each record is a databaseentry, for instance, the receiving statistics for a cer-tain player.
The text is the recap of the game?an article summarizing the game highlights.
Thedataset we used was collected by Barzilay and La-pata (2005).
The data includes 466 games duringthe 2003?2004 NFL season.
78 of these gameswere annotated by Snyder and Barzilay (2007),who aligned each sentence to a set of records.This domain is by far the most complicated ofthe three.
Many records corresponding to inconse-quential game statistics are not mentioned.
Con-versely, the text contains many general remarks(e.g., it was just that type of game) which arenot present in any of the records.
Furthermore,the complexity of the language used in the re-cap is far greater than what we can represent us-ing our simple model.
Fortunately, most of thefields are integer fields or string fields (generallynames or brief descriptions), which provide im-portant anchor points for learning the correspon-dences.
Nonetheless, the same names and num-bers occur in multiple records, so there is still un-certainty about which record is referenced by agiven sentence.3 Generative ModelTo learn the correspondence between a text w anda world state s, we propose a generative modelp(w | s) with latent variables specifying this cor-respondence.Our model combines segmentation with align-ment.
The segmentation aspect of our model issimilar to that of Grenager et al (2005) and Eisen-stein and Barzilay (2008), but in those two models,the segments are clustered into topics rather thangrounded to a world state.
The alignment aspectof our model is similar to the HMM model forword alignment (Ney and Vogel, 1996).
DeNeroet al (2008) perform joint segmentation and wordalignment for machine translation, but the natureof that task is different from ours.The model is defined by a generative process,93which proceeds in three stages (Figure 2 shows thecorresponding graphical model):1.
Record choice: choose a sequence of recordsr = (r1, .
.
.
, r|r|) to describe, where eachri ?
s.2.
Field choice: for each chosen record ri, se-lect a sequence of fields fi = (fi1, .
.
.
, fi|fi|),where each fij ?
{1, .
.
.
,m}.3.
Word choice: for each chosen field fij ,choose a number cij > 0 and generate a se-quence of cij words.The observed text w is the terminal yield formedby concatenating the sequences of words of allfields generated; note that the segmentation of wprovided by c = {cij} is latent.
Think of thewords spanned by a record as constituting an ut-terance with a meaning representation given by therecord and subset of fields chosen.Formally, our probabilistic model places a dis-tribution over (r, f , c,w) and factorizes accordingto the three stages as follows:p(r, f , c,w | s) = p(r | s)p(f | r)p(c,w | r, f , s)The following three sections describe each ofthese stages in more detail.3.1 Record Choice ModelThe record choice model specifies a distribu-tion over an ordered sequence of records r =(r1, .
.
.
, r|r|), where each record ri ?
s. Thismodel is intended to capture two types of regu-larities in the discourse structure of language.
Thefirst is salience, that is, some record types are sim-ply more prominent than others.
For example, inthe NFL domain, 70% of scoring records are men-tioned whereas only 1% of punting records arementioned.
The second is the idea of local co-herence, that is, the order in which one mentionsrecords tend to follow certain patterns.
For ex-ample, in the weather domain, the sky conditionsare generally mentioned first, followed by temper-ature, and then wind speed.To capture these two phenomena, we define aMarkov model on the record types (and given therecord type, a record is chosen uniformly from theset of records with that type):p(r | s) =|r|?i=1p(ri.t | ri?1.t)1|s(ri.t)|, (1)where s(t)def= {r ?
s : r.t = t} and r0.t isa dedicated START record type.2 We also modelthe transition of the final record type to a desig-nated STOP record type in order to capture regu-larities about the types of records which are de-scribed last.
More sophisticated models of coher-ence could also be employed here (Barzilay andLapata, 2008).We assume that s includes a special null recordwhose type is NULL, responsible for generatingparts of our text which do not refer to any realrecords.3.2 Field Choice ModelEach record type t ?
T has a separate field choicemodel, which specifies a distribution over a se-quence of fields.
We want to capture salienceand coherence at the field level like we did at therecord level.
For instance, in the weather domain,the minimum and maximum fields of a tempera-ture record are mentioned whereas the average isnot.
In the Robocup domain, the actor typicallyprecedes the recipient in passing event records.Formally, we have a Markov model over thefields:3p(f | r) =|r|?i=1|fj |?j=1p(fij | fi(j?1)).
(2)Each record type has a dedicated null field withits own multinomial distribution over words, in-tended to model words which refer to that recordtype in general (e.g., the word passes for passingrecords).
We also model transitions into the firstfield and transitions out of the final field with spe-cial START and STOP fields.
This Markov structureallows us to capture a few elements of rudimentarysyntax.3.3 Word Choice ModelWe arrive at the final component of our model,which governs how the information about a par-ticular field of a record is rendered into words.
Foreach field fij , we generate the number of words cijfrom a uniform distribution over {1, 2, .
.
.
, Cmax},where Cmax is set larger than the length of thelongest text we expect to see.
Conditioned on2We constrain our inference to only consider record typest that occur in s, i.e., s(t) 6= ?.3During inference, we prohibit consecutive fields from re-peating.94srfc,wsr1f11w1 ?
?
?
wc11?
?
??
?
?
rifi1w ?
?
?
wci1?
?
?
fi|fi|w ?
?
?
wci|fi|?
?
?
rn?
?
?
fn|fn|w ?
?
?
w|w|cn|fn|Record choiceField choiceWord choiceFigure 2: Graphical model representing the generative model.
First, records are chosen and ordered from the set s. Then fieldsare chosen for each record.
Finally, words are chosen for each field.
The world state s and the words w are observed, while(r, f , c) are latent variables to be inferred (note that the number of latent variables itself is unknown).the fields f , the words w are generated indepen-dently:4p(w | r, f , c, s) =|w|?k=1pw(wk | r(k).tf(k), r(k).vf(k)),where r(k) and f(k) are the record and field re-sponsible for generating word wk, as determinedby the segmentation c. The word choice modelpw(w | t, v) specifies a distribution over wordsgiven the field type t and field value v. This distri-bution is a mixture of a global backoff distributionover words and a field-specific distribution whichdepends on the field type t.Although we designed our word choice modelto be relatively general, it is undoubtedly influ-enced by the three domains.
However, we canreadily extend or replace it with an alternative ifdesired; this modularity is one principal benefit ofprobabilistic modeling.Integer Fields (t = INT) For integer fields, wewant to capture the intuition that a numeric quan-tity v is rendered in the text as a word whichis possibly some other numerical value w due tostylistic factors.
Sometimes the exact value v isused (e.g., in reporting football statistics).
Othertimes, it might be customary to round v (e.g., windspeeds are typically rounded to a multiple of 5).In other cases, there might just be some unex-plained error, where w deviates from v by somenoise + = w ?
v > 0 or ?
= v ?
w > 0.
Wemodel + and ?
as geometric distributions.5 In4While a more sophisticated model of words would beuseful if we intended to use this model for natural languagegeneration, the false independence assumptions present herematter less for the task of learning the semantic correspon-dences because we always condition on w.5Specifically, p(+;?+) = (1 ?
?+)+?1?+, where?+ is a field-specific parameter; p(?;??)
is defined analo-gously.8 9 10 11 12 13 14 15 16 17 18w0.10.20.30.40.5p w(w|v=13)8 9 10 11 12 13 14 15 16 17 18w0.10.20.30.40.6p w(w|v=13)(a) temperature.min (b) windSpeed.minFigure 3: Two integer field types in the weather domain forwhich we learn different distributions over the ways in whicha value v might appear in the text as a word w. Suppose therecord field value is v = 13.
Both distributions are centeredaround v, as is to be expected, but the two distributions havedifferent shapes: For temperature.min, almost all the massis to the left, suggesting that forecasters tend to report con-servative lower bounds.
For the wind speed, the mass is con-centrated on 13 and 15, suggesting that forecasters frequentlyround wind speeds to multiples of 5.summary, we allow six possible ways of generat-ing the word w given v:v dve5 bvc5 round5(v) v ?
?
v + +Separate probabilities for choosing among thesepossibilities are learned for each field type (seeFigure 3 for an example).String Fields (t = STR) Strings fields are in-tended to represent values which we expect to berealized in the text via a simple surface-level trans-formation.
For example, a name field with valuev = Moe Williams is sometimes referenced in thetext by just Williams.
We used a simple genericmodel of rendering string fields: Let w be a wordchosen uniformly from those in v.Categorical Fields (t = CAT) Unlike stringfields, categorical fields are not tied down to anylexical representation; in fact, the identities of thecategorical field values are irrelevant.
For eachcategorical field f and possible value v, we have a95v pw(w | t, v)0-25 , clear mostly sunny25-50 partly , cloudy increasing50-75 mostly cloudy , partly75-100 of inch an possible new a rainfallTable 2: Highest probability words for the categorical fieldskyCover.mode in the weather domain.
It is interesting tonote that skyCover=75-100 is so highly correlated with rainthat the model learns to connect an overcast sky in the worldto the indication of rain in the text.separate multinomial distribution over words fromwhich w is drawn.
An example of a categori-cal field is skyCover.mode in the weather domain,which has four values: 0-25, 25-50, 50-75,and 75-100.
Table 2 shows the top words foreach of these field values learned by our model.4 Learning and InferenceOur learning and inference methodology is a fairlyconventional application of Expectation Maxi-mization (EM) and dynamic programming.
Theinput is a set of scenarios D, each of which is atext w paired with a world state s. We maximizethe marginal likelihood of our data, summing outthe latent variables (r, f , c):max??
(w,s)?D?r,f ,cp(r, f , c,w | s; ?
), (3)where ?
are the parameters of the model (all themultinomial probabilities).
We use the EM algo-rithm to maximize (3), which alternates betweenthe E-step and the M-step.
In the E-step, wecompute expected counts according to the poste-rior p(r, f , c | w, s; ?).
In the M-step, we op-timize the parameters ?
by normalizing the ex-pected counts computed in the E-step.
In our ex-periments, we initialized EM with a uniform dis-tribution for each multinomial and applied add-0.1smoothing to each multinomial in the M-step.As with most complex discrete models, the bulkof the work is in computing expected counts underp(r, f , c | w, s; ?).
Formally, our model is a hier-archical hidden semi-Markov model conditionedon s. Inference in the E-step can be done using adynamic program similar to the inside-outside al-gorithm.5 ExperimentsTwo important aspects of our model are the seg-mentation of the text and the modeling of the co-herence structure at both the record and field lev-els.
To quantify the benefits of incorporating thesetwo aspects, we compare our full model with twosimpler variants.?
Model 1 (no model of segmentation or co-herence): Each record is chosen indepen-dently; each record generates one field, andeach field generates one word.
This model issimilar in spirit to IBM model 1 (Brown etal., 1993).?
Model 2 (models segmentation but not coher-ence): Records and fields are still generatedindependently, but each field can now gener-ate multiple words.?
Model 3 (our full model of segmentation andcoherence): Records and fields are generatedaccording to the Markov chains described inSection 3.5.1 EvaluationIn the annotated data, each text w has been di-vided into a set of lines.
These lines correspondto clauses in the weather domain and sentences inthe Robocup and NFL domains.
Each line is an-notated with a (possibly empty) set of records.
LetA be the gold set of these line-record alignmentpairs.To evaluate a learned model, we com-pute the Viterbi segmentation and alignment(argmaxr,f ,c p(r, f , c | w, s)).
We produce a pre-dicted set of line-record pairsA?
by aligning a lineto a record ri if the span of (the utterance corre-sponding to) ri overlaps the line.
The reason weevaluate indirectly using lines rather than using ut-terances is that it is difficult to annotate the seg-mentation of text into utterances in a simple andconsistent manner.We compute standard precision, recall, and F1of A?
with respect to A.
Unless otherwise spec-ified, performance is reported on all scenarios,which were also used for training.
However, wedid not tune any hyperparameters, but rather usedgeneric values which worked well enough acrossall three domains.5.2 Robocup SportscastingWe ran 10 iterations of EM on Models 1?3.
Ta-ble 3 shows that performance improves with in-creased model sophistication.
We also compare96Method Precision Recall F1Model 1 78.6 61.9 69.3Model 2 74.1 84.1 78.8Model 3 77.3 84.0 80.5Table 3: Alignment results on the Robocup sportscastingdataset.Method F1Random baseline 48.0Chen and Mooney (2008) 67.0Model 3 75.7Table 4: F1 scores based on the 4-fold cross-validationscheme in Chen and Mooney (2008).our model to the results of Chen and Mooney(2008) in Table 4.Figure 4 provides a closer look at the predic-tions made by each of our three models for a par-ticular example.
Model 1 easily mistakes pink10for the recipient of a pass record because decisionsare made independently for each word.
Model 2chooses the correct record, but having no modelof the field structure inside a record, it proposesan incorrect field segmentation (although our eval-uation is insensitive to this).
Equipped with theability to prefer a coherent field sequence, Model3 fixes these errors.Many of the remaining errors are due to thegarbage collection phenomenon familiar fromword alignment models (Moore, 2004; Liang etal., 2006).
For example, the ballstopped recordoccurs frequently but is never mentioned in thetext.
At the same time, there is a correlation be-tween ballstopped and utterances such as pink2holds onto the ball, which are not aligned to anyrecord in the annotation.
As a result, our modelincorrectly chooses to align the two.5.3 Weather ForecastsFor the weather domain, staged training was nec-essary to get good results.
For Model 1, we ran15 iterations of EM.
For Model 2, we ran 5 it-erations of EM on Model 1, followed by 10 it-erations on Model 2.
For Model 3, we ran 5 it-erations of Model 1, 5 iterations of a simplifiedvariant of Model 3 where records were chosen in-dependently, and finally, 5 iterations of Model 3.When going from one model to another, we usedthe final posterior distributions of the former to ini-Method Precision Recall F1Model 1 49.9 75.1 60.0Model 2 67.3 70.4 68.8Model 3 76.3 73.8 75.0Table 5: Alignment results on the weather forecast dataset.
[Model 1] r:f :w:passarg2=pink10pink10 turns the ball over to purple5[Model 2] r:f :w:turnoverxpink10 turns the ball over arg2=purple5to purple5[Model 3] r:f :w:turnoverarg1=pink10pink10 xturns the ball over to arg2=purple5purple5Figure 4: An example of predictions made by each of thethree models on the Robocup dataset.tialize the parameters of the latter.6 We also pro-hibited utterances in Models 2 and 3 from crossingpunctuation during inference.Table 5 shows that performance improves sub-stantially in the more sophisticated models, thegains being greater than in the Robocup domain.Figure 5 shows the predictions of the three modelson an example.
Model 1 is only able to form iso-lated (but not completely inaccurate) associations.By modeling segmentation, Model 2 accounts forthe intermediate words, but errors are still madedue to the lack of Markov structure.
Model 3remedies this.
However, unexpected structuresare sometimes learned.
For example, the temper-ature.time=6-21 field indicates daytime, whichhappens to be perfectly correlated with the wordhigh, although high intuitively should be associ-ated with the temperature.max field.
In these casesof high correlation (Table 2 provides another ex-ample), it is very difficult to recover the properalignment without additional supervision.5.4 NFL RecapsIn order to scale up our models to the NFL do-main, we first pruned for each sentence the recordswhich have either no numerical values (e.g., 23,23-10, 2/4) nor name-like words (e.g., those thatappear only capitalized in the text) in common.This eliminated all but 1.5% of the record can-didates per sentence, while maintaining an ora-6It is interesting to note that this type of staged trainingis evocative of language acquisition in children: lexical asso-ciations are formed (Model 1) before higher-level discoursestructure is learned (Model 3).97[Model 1] r:f :w: cloudy , with awindDirtime=6-21high neartemperaturemax=6363 .windDirmode=SEeast southeast wind betweenwindSpeedmin=55 andwindSpeedmean=911 mph .
[Model 2] r:f :w:rainChancemode=?cloudy ,temperaturexwith a time=6-21high near max=6363 .windDirmode=SEeast southeast wind xbetween 5 andwindSpeedmean=911 mph .
[Model 3] r:f :w:skyCoverxcloudy ,temperaturexwith a time=6-21high near max=6363 mean=56.windDirmode=SEeast southeast xwind betweenwindSpeedmin=55 max=13and 11 xmph .Figure 5: An example of predictions made by each of the three models on the weather dataset.cle alignment F1 score of 88.7.
Guessing a singlerandom record for each sentence yields an F1 of12.0.
A reasonable heuristic which uses weightednumber- and string-matching achieves 26.7.Due to the much greater complexity of this do-main, Model 2 was easily misled as it tried with-out success to find a coherent segmentation of thefields.
We therefore created a variant, Model 2?,where we constrained each field to generate ex-actly one word.
To train Model 2?, we ran 5 it-erations of EM where each sentence is assumedto have exactly one record, followed by 5 itera-tions where the constraint was relaxed to also al-low record boundaries at punctuation and the wordand.
We did not experiment with Model 3 sincethe discourse structure on records in this domain isnot at all governed by a simple Markov model onrecord types?indeed, most regions do not refer toany records at all.
We also fixed the backoff prob-ability to 0.1 instead of learning it and enforcedzero numerical deviation on integer field values.Model 2?
achieved an F1 of 39.9, an improve-ment over Model 1, which attained 32.8.
Inspec-tion of the errors revealed the following problem:The alignment task requires us to sometimes aligna sentence to multiple redundant records (e.g.,play and score) referenced by the same part of thetext.
However, our model generates each part oftext from only one record, and thus it can only al-low an alignment to one record.7 To cope with thisincompatibility between the data and our notion ofsemantics, we used the following solution: We di-vided the records into three groups by type: play,score, and other.
Each group has a copy of themodel, but we enforce that they share the samesegmentation.
We also introduce a potential thatcouples the presence or absence of records across7The model can align a sentence to multiple records pro-vided that the records are referenced by non-overlappingparts of the text.Method Precision Recall F1Random (with pruning) 13.1 11.0 12.0Baseline 29.2 24.6 26.7Model 1 25.2 46.9 32.8Model 2?
43.4 37.0 39.9Model 2?
(with groups) 46.5 62.1 53.2Graph matching (sup.)
73.4 64.5 68.6Multilabel global (sup.)
87.3 74.5 80.3Table 6: Alignment results on the NFL dataset.
Graph match-ing and multilabel are supervised results reported in Snyderand Barzilay (2007).9groups on the same segment to capture regular co-occurrences between redundant records.Table 6 shows our results.
With groups, weachieve an F1 of 53.2.
Though we still trail su-pervised techniques, which attain numbers in the68?80 range, we have made substantial progressover our baseline using an unsupervised method.Furthermore, our model provides a more detailedanalysis of the correspondence between the worldstate and text, rather than just producing a singlealignment decision.
Most of the remaining errorsmade by our model are due to a lack of calibra-tion.
Sometimes, our false positives are close callswhere a sentence indirectly references a record,and our model predicts the alignment whereas theannotation standard does not.
We believe that fur-ther progress is possible with a richer model.6 ConclusionWe have presented a generative model of corre-spondences between a world state and an unseg-mented stream of text.
By having a joint modelof salience, coherence, and segmentation, as wellas a detailed rendering of the values in the worldstate into words in the text, we are able to copewith the increased ambiguity that arises in this newdata setting, successfully pushing the limits of un-supervision.98ReferencesR.
Barzilay and M. Lapata.
2005.
Collective content selec-tion for concept-to-text generation.
In Human LanguageTechnology and Empirical Methods in Natural LanguageProcessing (HLT/EMNLP), pages 331?338, Vancouver,B.C.R.
Barzilay and M. Lapata.
2008.
Modeling local coher-ence: An entity-based approach.
Computational Linguis-tics, 34:1?34.P.
F. Brown, S. A. D. Pietra, V. J. D. Pietra, and R. L. Mer-cer.
1993.
The mathematics of statistical machine trans-lation: Parameter estimation.
Computational Linguistics,19:263?311.D.
L. Chen and R. J. Mooney.
2008.
Learning to sportscast:A test of grounded language acquisition.
In InternationalConference on Machine Learning (ICML), pages 128?135.
Omnipress.J.
DeNero, A.
Bouchard-Co?te?, and D. Klein.
2008.
Samplingalignment structure under a Bayesian translation model.In Empirical Methods in Natural Language Processing(EMNLP), pages 314?323, Honolulu, HI.J.
Eisenstein and R. Barzilay.
2008.
Bayesian unsupervisedtopic segmentation.
In Empirical Methods in Natural Lan-guage Processing (EMNLP), pages 334?343.J.
Feldman and S. Narayanan.
2004.
Embodied meaning in aneural theory of language.
Brain and Language, 89:385?392.R.
Ge and R. J. Mooney.
2005.
A statistical semantic parserthat integrates syntax and semantics.
In ComputationalNatural Language Learning (CoNLL), pages 9?16, AnnArbor, Michigan.P.
Gorniak and D. Roy.
2007.
Situated language understand-ing as filtering perceived affordances.
Cognitive Science,31:197?231.T.
Grenager, D. Klein, and C. D. Manning.
2005.
Unsu-pervised learning of field segmentation models for infor-mation extraction.
In Association for Computational Lin-guistics (ACL), pages 371?378, Ann Arbor, Michigan.
As-sociation for Computational Linguistics.R.
J. Kate and R. J. Mooney.
2007.
Learning language se-mantics from ambiguous supervision.
In Association forthe Advancement of Artificial Intelligence (AAAI), pages895?900, Cambridge, MA.
MIT Press.P.
Liang, B. Taskar, and D. Klein.
2006.
Alignment by agree-ment.
In North American Association for ComputationalLinguistics (NAACL), pages 104?111, New York City.
As-sociation for Computational Linguistics.W.
Lu, H. T. Ng, W. S. Lee, and L. S. Zettlemoyer.
2008.
Agenerative model for parsing natural language to meaningrepresentations.
In Empirical Methods in Natural Lan-guage Processing (EMNLP), pages 783?792.R.
C. Moore.
2004.
Improving IBM word alignment model1.
In Association for Computational Linguistics (ACL),pages 518?525, Barcelona, Spain.
Association for Com-putational Linguistics.H.
Ney and S. Vogel.
1996.
HMM-based word align-ment in statistical translation.
In International Conferenceon Computational Linguistics (COLING), pages 836?841.Association for Computational Linguistics.J.
M. Siskind.
1996.
A computational study of cross-situational techniques for learning word-to-meaning map-pings.
Cognition, 61:1?38.B.
Snyder and R. Barzilay.
2007.
Database-text alignmentvia structured multilabel classification.
In InternationalJoint Conference on Artificial Intelligence (IJCAI), pages1713?1718, Hyderabad, India.C.
Yu and D. H. Ballard.
2004.
On the integration of ground-ing language and learning objects.
In Association for theAdvancement of Artificial Intelligence (AAAI), pages 488?493, Cambridge, MA.
MIT Press.L.
S. Zettlemoyer and M. Collins.
2005.
Learning to mapsentences to logical form: Structured classification withprobabilistic categorial grammars.
In Uncertainty in Arti-ficial Intelligence (UAI), pages 658?666.L.
S. Zettlemoyer and M. Collins.
2007.
Online learn-ing of relaxed CCG grammars for parsing to logicalform.
In Empirical Methods in Natural Language Pro-cessing and Computational Natural Language Learning(EMNLP/CoNLL), pages 678?687.99
