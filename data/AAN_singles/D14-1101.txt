Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 938?950,October 25-29, 2014, Doha, Qatar.c?2014 Association for Computational LinguisticsNeural Networks Leverage Corpus-wide Informationfor Part-of-speech TaggingYuta TsuboiIBM Research - Tokyoyutat@jp.ibm.comAbstractWe propose a neural network approach tobenefit from the non-linearity of corpus-wide statistics for part-of-speech (POS)tagging.
We investigated several typesof corpus-wide information for the words,such as word embeddings and POS tag dis-tributions.
Since these statistics are en-coded as dense continuous features, it isnot trivial to combine these features com-paring with sparse discrete features.
Ourtagger is designed as a combination ofa linear model for discrete features anda feed-forward neural network that cap-tures the non-linear interactions among thecontinuous features.
By using several re-cent advances in the activation functionsfor neural networks, the proposed methodmarks new state-of-the-art accuracies forEnglish POS tagging tasks.1 IntroductionAlmost all of the approaches to NLP tasks suchas part-of-speech tagging and syntactic parsingmainly use sparse discrete features to represent lo-cal information such as word surfaces in a size-limited window.
The non-linearity of those dis-crete features is often used in many NLP taskssince the simple conjunction (AND) of discretefeatures represents the co-occurrence of the fea-tures and is intuitively understandable.
In addi-tion, the thresholding of these combinatorial fea-tures by simple counts effectively suppresses thecombinatorial increase of the parameters.
At thesame time, although global information had alsobeen used in several reports (Nakagawa and Mat-sumoto, 2006; Huang and Yates, 2009; Turian etal., 2010; Schnabel and Sch?utze, 2014), the non-linear interactions of these features were not wellinvestigated since these features are often densecontinuous features and the explicit non-linear ex-pansions are counterintuitive and drastically in-crease the number of the model parameters.
In ourwork, we investigate neural networks used to rep-resent the non-linearity of global information forPOS tagging in a compact way.We focus on four kinds of corpus-wide infor-mation: (1) word embeddings, (2) POS tag dis-tributions, (3) supertag distributions, and (4) con-text word distributions.
All of them are continuousdense features and we use a feed-forward neuralnetwork to exploit the non-linearity of these fea-tures.
Although all of them except (3) have beenused for POS tagging in previous work (Nakamuraet al., 1990; Schmid, 1994; Schnabel and Sch?utze,2014; Huang and Yates, 2009), we propose a neu-ral network approach to capture the non-linear in-teractions of these features.
By feeding these fea-tures into neural networks as an input vector, wecan expect our tagger can handle not only the non-linearity of the N-grams of the same kinds of fea-tures but also the non-linear interactions amongthe different kind of features.Our tagger combines a linear model usingsparse high-dimensional features and a neural net-work using continuous dense features.
AlthoughCollobert et al.
(2011) seeks to solve NLP taskswithout depending on the feature engineering ofconventional NLP methods, our architecture ismore practical because it integrates the neuralnetworks into a well-tuned conventional method.Thus, our tagger enjoys both the manually ex-plored combinations of discrete features and theautomatically learned non-linearity of the contin-uous features.
We also studied some of the neweractivation functions: Rectified Linear Units (Nairand Hinton, 2010), Maxout networks (Goodfel-low et al., 2013), and Lp-pooling (Gulcehre et al.,2014; Zhang et al., 2014).Deep neural networks have been a hot topicin many application areas such as computer vi-938sion and voice recognition.
However, althoughneural networks show state-of-the-art results ona few semantic tasks (Zhila et al., 2013; Socheret al., 2013; Socher et al., 2011), neural net-work approaches have not performed better thanthe state-of-the-art systems for traditional syn-tactic tasks.
Our neural tagger shows state-of-the-art results: 97.51% accuracy in the standardbenchmark on the Penn Treebank (Marcus et al.,1993) and 98.02% accuracy in POS tagging onCoNLL2009 (Haji?c et al., 2009).
In our experi-ments, we found that the selection of the activationfunctions led to large differences in the tagging ac-curacies.
We also observed that the POS tags ofthe words are effectively clustered by the hiddenactivations of the intermediate layer.
This obser-vation is evidence that the neural network can findgood representations for POS tagging.The remainder of this paper is organized as fol-lows.
Section 2 introduces our deterministic tag-ger and its learning algorithm.
Section 3 describesthe continuous features that represent corpus-wideinformation and Section 4 is about the neural net-work we used.
Section 5 presents our empiri-cal study of the effects of corpus-wide informa-tion and neural networks on English POS taggingtasks.
Section 6 describes related work, and Sec-tion 7 concludes and suggests items for futurework.2 Transition-based taggingOur tagging model is a deterministic tagger basedon Choi and Palmer (2012), which is a one-pass,left-to-right tagging algorithm that uses well-tunedbinary features.Let x = (x1, x2, .
.
.
, xT) ?
XTbe aninput token sequence of length T and y =(y1, y2, .
.
.
, yT) ?
YTbe a corresponding POStag sequence of x.
We denote the predicted tagsby a tagger as?y and the subsequence from r to tas ytr.
The prediction of the t-th tag is determinis-tically done by the classifier:y?t= argmaxy?Yf?
(zt, y), (1)where f?
is a scoring function with arbitrary pa-rameters, ?
?
Rd, that are to be learned and ztisan arbitrary feature representation of the t-th po-sition using x and?yt?11which is the predictionhistory of the previous tokens.We extend Choi and Palmer (2012) in threeways: (1) an online SVM learning algorithm withL1and L2regularization, (2) continuous featuresfor corpus-wide information, and (3) the compos-ite function of a linear model for discrete featuresand a non-linear model for continuous features.Since (2) and (3) are the main topics of this pa-per, they are explained in detail in Sections 3 and4 and we describe only (1) here.First, our learning algorithm trains a multi-classSVM with L1and L2regularization based on Fol-low the Proximally Regularized Leader (FTRL-Proximal) (McMahan, 2011).
In the k-th iteration,the parameter update is done by?k=argmin?k?l=1(gl?
?+12?l?????????l??????22)+R(?
),where gk?
Rdis a subgradient of the hinge lossfunction and R(?)
= ?1||?||1+?22||?||22is thecomposite function of the L1and L2regulariza-tion terms with hyper-parameters ?1?
0 and?2?
0.
To incorporate an adaptive learning ratescheduling, Adagrad (Duchi et al., 2010), we useper-coordinate learning rates for {i|1 ?
i < d}:?ki=?i(?i+?
?kl=1(gli)2),where ?
?
0 and ?
?
0.
Although thenaive implementation may require O(k) compu-tation in the k-th iteration, FTRL-Proximal canbe implemented efficiently by maintaining twolength-d vectors, m =?klgl?12?l?land n =?kl(gli)2(McMahan et al., 2013).Second, to overcome the error propagationproblem, we train the classifier with a simple vari-ant of the on-the-fly example generation algorithmfrom Goldberg and Nivre (2012).
Since the scor-ing function refers to the prediction history, Choiand Palmer (2012) uses the gold POS tags, yt?11,to generate training examples, which means theyassume all of the past decisions are correct.
How-ever, this causes error propagation problems, sinceeach state depends on the history of the past deci-sions.
Therefore, at the k-th iteration and the t-thposition of the input sequence, we simply use thepredictions of the previously learned classifiers togenerate training examples, i.e.,y?t?r= argmaxy?Yf?k?r(zt?r, y)for all {r|1 ?
r < t ?
1}.
Although it isnot theoretically justified, it empirically runs as a939stochastic version of DAGGER (Ross et al., 2011)or SEARN (Daum?e III et al., 2009) with the speedbenefit of online learning.Algorithm 1 Learning algorithmfunction LEARN(?,?,?1,?2,m,n,?k)while ?
stop doSelect a random sentence (x,y)for t = 1 to T dou=UPDATE(?,?,?1,?2,m,n,?k)y?t= argmaxy?Yfu(zt, y)y?
= argmaxy ?=ytfu(zt, , y)if fu(zt, yt)?
fu(zt, y?)
< 1 theng=?u?
(zt, yt, y?)
?
SubgradientFor all i ?
I compute?i=(?ni+ g2i??ni)/?imi?
mi+ gi?
?iuini?
ni+ g2iend ifk ?
k + 1end forend whilereturn ?kend functionfunction UPDATE(?,?,?1,?2,m,n,?k)for i ?
I do?ki=??
?0 if |mi| ?
?1?mi+sgn(mi)?1(?i?2+?ni)/?i+?2otherwiseui?
?kiif acceleration thenui?
?ki+kk+3(?ki?
?k?1i)end ifend forfor i ??
I doui?
?ki?
?k?1i?
Leaving all ?
for inactive i unchangedend forreturn uend functionAlgorithm 1 summarizes our training processwhere ?
(zt, yt, y?)
:= max(0, 1 ?
f?
(zt, y) +f?
(zt, y?))
is the multi-class hinge loss (Crammerand Singer, 2001).
I in Algorithm 1 is a set ofparameter indexes that correspond to the non-zerofeatures, so the update is sparse for sparse fea-tures.
In addition, for the parameter update of theneural networks, we also use an accelerated prox-imal method (Parikh and Boyd, 2013), which isconsidered as a variant of the momentum meth-ods (Sutskever et al., 2013).
Although u and ?
arethe same when the acceleration is not used, u inAlgorithm 1 is an extrapolation step in the accel-erated method.
Although we do not focus on thelearning algorithm in this work, the algorithm con-verges quite quickly and the speed is important be-cause the neural network extension described laterrequires a hyper-parameter search which is com-putationally demanding.3 Corpus-wide InformationSince typical discrete features indicate only theoccurrence in a local context and do not conveycorpus-wide statistics, we studied four kinds ofcontinuous features for POS tagging to representthe corpus-wide information.3.1 Word embeddingsWord embeddings, or distributed word represen-tations, embed the words into a low-dimensionalcontinuous space.
Most of the neural network ap-plications for NLP use word embeddings (Col-lobert et al., 2011; Socher et al., 2011; Zhila etal., 2013; Socher et al., 2013), and even for linearmodels, Turian et al.
(2010) highlights the benefitof word embeddings on sequential labeling tasks.In particular, in our experiments, we used tworecently proposed algorithms, word2vec (Mikolovet al., 2013) and glove (Pennington et al.,2014), which are simple and scalable, althoughour method could use other word embeddings.Word2vec trains the word embeddings to pre-dict the words surrounding each word, and glovetrains the word embeddings to predict the loga-rithmic count of the surrounding words of eachword.
Thus, these embeddings can be seen asthe distributed versions of the distributional fea-tures since the word vectors compactly representthe distribution of the context in which a word ap-pears.
We normalized the word embeddings tounit length and used the average vector of trainingvocabulary for the unknown tokens.3.2 POS tag distributionIn a way similar to Schmid (1994), we use POS tagdistribution over a training corpus.
Each word isrepresented by a vector of length |Y | in which they-th element is the conditional probabilities withwhich that word gets the y-th POS tag.
We alsouse the POS tag distributions of the affixes and940spelling binary features used in Choi and Palmer(2012).
We cite the definitions of these features.1.
Affix: c:1, c:2, c:3, cn:, cn?1:, cn?2:, cn?3:where c?is a character string in a word.
Forexample c:2is the prefix of length two of aword and cn?1:is the suffix of length two ofa word.2.
Spelling: initial uppercase, all upper-case, all lowercase, contains 1/2+ capi-tal(s) not at the beginning, contains a (pe-riod/number/hyphen).The probabilities for a feature b is estimated withadditive smoothing asP (y|b) =C(b, y) + 1C(b) + |Y |, (2)where C(b) and C(b, y) are the counts of b andco-occurrences of b and y, respectively.
In addi-tion, an extra dimension for sentence boundariesis added to the vector for word-forms.
In total, thePOS tag distributions for each word are encodedby a vector of dimension |Y |+1+|Y |?14 (|Y | forlowercase simplified word-forms, 1 for sentenceboundaries, |Y | ?
7 for affixes, and |Y | ?
7 forspellings).3.3 Supertag distributionWe also use the distribution of supertags for de-pendency parsing.
Supertags are lexical templateswhich are extracted from the syntactic dependencystructures and suppertagging is often used for thepre-processing of a parsing task.
Since the su-pertags encode rich syntactic information, we ex-pect the supertag distribution of a word to alsoprovide clues for the POS tagging.
We used twotypes of supertags: One is the dependency rela-tion label of the head of the word and the otheris that of the dependents of the word.
FollowingOuchi et al.
(2014), we added the relative posi-tion, left (L) or right (R), to the supertags.
Forexample, a word has its dependents in the left di-rection with a label ?nn?
and in the right direc-tion with a label ?amod?, so its supertag set fordependents is {?nn/L?, ?amod/R?}.
A special su-pertag ?NO-CHILD?
is used for a word that hasno dependent.
Note that, although the Model 2 su-pertag set of Ouchi et al.
(2014) is defined as thecombination of head and dependent tags, we usedthem separately.
The feature values for each wordare defined in the same way as Equation 2 in Sec-tion 3.2.
Since a word can have more than onedependent, the dependent supertag features are nolonger multinomial distributions but we used themin that way.
Note that, since the feature values arecalculated using the tree annotations from trainingset, our tagger does not require any dependencyparser at runtime.3.4 Context word distributionThis is the simplest distributional features inwhich each word is represented by the distribu-tions of its left and right neighbors.
Although thecontext word distribution is similar to word em-beddings, we believe they complement each other,as reported by Levy and Goldberg (2014).
Fol-lowing Schnabel and Sch?utze (2014), we restrictedthe set of indicator words to the 500 most frequentwords in the corpus, and used two special featureentries: One is the marginal probability of the non-indicator words and the other is the probabilitiesof neighboring sentence boundaries.
The condi-tional probabilities for left and right neighbors areestimated in the same way as Equation 2 in Sec-tion 3.2, and there are a total of 1, 004 dimensionsof this feature for a word.4 Neural NetworksThe non-linearity of the discrete features has beenexploited in many NLP tasks, since the simpleconjunction of the discrete features is intuitive andthe thresholding of these combinatorial featuresby their feature counts effectively suppresses thecombinatorial increase of the parameters.In contrast, it is not easy to manually tune thenon-linearity of the continuous features.
For ex-ample, it is not intuitive to design the conjunc-tion features of two kinds of word embeddings,word2vec and glove.
Although kernel methodshave been used to incorporate non-linearity inprior research, they are rarely used now becausetheir tagging speed is too slow (Gim?enez andM`arquez, 2003).
Our solution is to introducefeed-forward neural networks to capture the non-linearity of the corpus-wide information.4.1 Hybrid modelWe designed our tagger as a hybrid of a linearmodel and a non-linear model.
Wang and Man-ning (2013) reported that a neural network us-ing both sparse discrete features and dense (low-941Figure 1: A hybrid architecture of a linear modeland a neural network with a pooling activationfunctiondimensional) continuous features was worse thana linear model using the same two features.
Atthe same time, they also reported that a neural net-work using only the dense continuous features out-performed a linear model using the same features.Based on their results, we applied neural networksonly for the continuous features and used a linearmodel for the discrete features.Formally, the scoring function (1) in Section 2is defined as the composite function of two terms:f(z, y) := flinear(z, y)+fnn(z, y).
The first flinearis the linear model and the second fnnis a neu-ral network.
Since this is a linear combination oftwo functions, the subgradient of the loss functionrequired for Algorithm 1 is also the linear com-bination of subgradients of two functions, whichmeans???
(zt, yt, y?)
= ?
?flinear(zt, y?)
+ ?
?fnn(zt, y?)?
?
?flinear(zt, yt)?
?
?fnn(zt, yt)if f?
(zt, yt)?
f?
(zt, y?)
< 0.First, the linear model can be defined asflinear(z, y) := ?d?
?d(z, y),where ?d(z, y) is a feature mapping for the dis-crete part of z and a POS tag, and ?dis the cor-responding parameter vector.
Since this is a lin-ear model, the gradient of this function is simply?
?flinear(z, y) = ?d(z, y).Second, each hidden layer of our neural net-works non-linearly transforms an input vector h?into an output vector h and we can say h?is thecontinuous part of z at the first layer.
Let hLbea hidden activation of the top layer, which is thenon-linear transformation of the continuous partof z.
The output layer of the neural network isdefined asfnn(z, y) := ?o?
?o(hL, y),where?o(h, y) is a feature mapping for the hiddenvariables and a POS tag, and ?ois the correspond-ing parameter vector.4.2 Activation functionsThe hidden variables h are computed by the re-cursive application of a non-linear activation func-tion.
Since new styles of the activation functionswere recently proposed, we review several acti-vation functions here.
Let v ?
R|V |be the in-put of an activation function and each element isvj= ?nn,j?
h?+ ?bias,j, where ?nn,jis the param-eter vector for vjand ?bias,jis the bias parameterfor vj.
We also assume v is divided into groupsof size G, and denote the j-th element of the i-thgroup as {vij|1 ?
i ?
|V |/G ?
1 ?
j ?
G}.
Westudied three activation functions:1.
Rectified linear units (ReLUs) (Nair and Hin-ton, 2010):hj= max(0, vj) for all {j|1 ?
j ?
|V |}.Note that a subgradient of ReLUs is?hj??={?vj??
if vj > 00 otherwise.2.
Maxout networks (MAXOUT) (Goodfellowet al., 2013):hi= max1?j?Gvijfor all {i|1 ?
i ?|V |G}.Note that a subgradient of MAXOUT is?hi??=?vi?j?
?, where?j = argmax1?j?Gvij3.
Normalized Lp-pooling (Lp) (Gulcehre et al.,2014):hi=??1GG?j=1|vij|p?
?1pfor all {i|1 ?
i ?|V |G}.Note that a subgradient of Lpis?hi??=G?j=1?vij??vij|vij|p?2G??1GG?j=1|vi,j|p?
?1p?1.942The activation inputs for each predefined group,{v1j, .
.
.
, vGj}, are aggregated by a non-linearfunction in MAXOUT or Lpactivation functions,while each input is transformed into a correspond-ing hidden variable in the ReLUs.
When thenumber of parameters required for these activationfunctions is the same, the number of output vari-ables h for MAXOUT and Lpis one-G-th smallerthan that for ReLUs.
Boureau et al.
(2010) showpooling operations theoretically reduce the vari-ance of hidden activations, and our experimentalresults also show MAXOUT and Lpperform bet-ter than the ReLUs with the same number of pa-rameters.
Note that MAXOUT is a special caseof unnormalized Lppooling when p = ?
andvj> 0 for all j (Zhang et al., 2014).
Figure 1summarizes the proposed architecture with a sin-gle hidden layer and a pooling activation function.4.3 Hyper-parameter searchFinally, the subgradients of the neural network,fnn(z, y), can be computed through standardback-propagation algorithms and we can applythem in Algorithm 1.
However, many of the hyper-parameters have to be determined for the trainingof the neural networks, and two stages of randomhyper-parameter searches (Bergstra and Bengio,2012) are used in our experiments.
Note that theparameters are grouped into three sets, ?d,?o,?nn,and the same values for ?1, ?2, ?, ?
are used foreach parameter set.In the first stage, we randomly select 32 combi-nations of ?2for fnn, ?1, ?2for flinear, the epochto start the L1/L2 regularizations, and the on andoff the acceleration in Algorithm 1.
Here are thecandidates of three hyper-parameters:1.
?1: 0 for the update of fnnand{0, 10?8, 10?6, 10?4, 10?2, 1} for theupdate of flinear;2.
?2: {0.1, 0.5, 1, 5, 10} for the update offnnand {1, 5, 10, 50, 100} for the update offlinear; and3.
Epoch to start the regularizations: {0, 1, 2}.In the second stage with each hyper-parametercombination above, we select 8 random combina-tions of ?, ?
for both flinearand fnnand initial pa-rameter ranges R for fnn.
Here are the candidatesof the three hyper-parameters:1.
?
: {0.01, 0.05, 0.1, 0.5, 1, 5};Data Set #Sent.
#Tokens #UnknownTraining 38,219 912,344 0Development 5,527 131,768 4,467Testing 5,462 129,654 3,649Table 1: Data set splits for PTB.2.
?
: {0.5, 1, 5};3.
R: {[?0.1, 0.1], [?0.05, 0.05], [?0.01, 0.01],[?0.005, 0.005]}.The values of ?
for fnnare uniformly sampled inthe range of the randomly selected R. Note that,according to Goodfellow et al.
(2014), the biases?biasare initialized as 0 for MAXOUT and Lp, anduniformly sampled from a range R + max(R),i.e., always initialized with non-negative values.The best combination for the development set ischosen after training that uses random 20% of thetraining set at the second stage, and Algorithm 1is terminated when the all token accuracy of thedevelopment data has been declining for 5 epochsat the first stage.
In other words, 32 ?
8 randomcombinations of ?, ?, and ?
for fnnwere tested.5 Experiments5.1 SetupOur experiments were mainly performed usingthe Wall Street Journal from Penn Treebank(PTB) (Marcus et al., 1993).
We used tagged sen-tences from the parse trees (Toutanova et al., 2003)and followed the standard approach of splitting thePTB, using sections 0?18 for training, section 19?21 for development, and section 22?24 for testing(Table 1).
In addition, we used the CoNLL2009data sets with the training, development, andtest splits used in the shared task (Haji?c et al.,2009) for better comparison with a joint model ofPOS tagging and dependency parsing (Bohnet andNivre, 2012).Our baseline tagger was trained by Algorithm 1.As discrete features for our tagger, we used thesame binary feature set as Choi and Palmer (2012)which is composed of (a) 1, 2, 3-grams of thesurface word-forms and their predicted/dominatedPOS tags, (b) the prefixes and suffixes of thewords, and (c) the spelling types of the words.
Inthe same way as Choi and Palmer (2012), we usedlowercase simplified word-forms which appearedat least 3 times.943In addition to their binary features, we used con-tinuous features which are the concatenation of thecorpus-wide features in a context window.
Thewindow of size w = 2s + 1 is the local contextcentered around xt: xt?s, ?
?
?
, xt, ?
?
?
, xt+s.
Theexperimental settings of each feature described inSection 3 are as follows.Word embeddingsWe used two word vectors: 300-dimensionalvectors that were learned by word2vec usinga part of the Google News dataset (around100 billion tokens)1, and 300-dimensionalvectors that were learned by glove using apart of the Common Crawl dataset (840 bil-lion tokens)2.
For sentence boundaries, weuse the vector of the special entry ?</s>?
forthe word2vec embeddings and the zero vec-tor for the glove embeddings.POS tag distributionThe counts are calculated using training data.Supertag distributionIn the experiments on PTB, we used the Stan-ford parser v2.0.43to convert from phrasestructures to dependency structures so thatthe dependency relation labels of the Stan-ford dependencies are used.
The size of thesupertag set is 85 for both heads and depen-dents in our experiments.
In the experimentson CoNLL2009, the dependency structuresand labels defined in CoNLL2009 are usedand the size of supertag set is 99 for bothheads and dependents.Context word distributionTo count the neighboring words in our exper-iments, we used sections 0?18 of the WallStreet Journal and all of the Brown corpusfrom Penn Treebank (Marcus et al., 1993).Since the training of the neural networks is com-putationally demanding, first, we trained the lin-ear classifiers using Algorithm 1 to select the bestwindow sizes for each corpus-wide information ofSection 3.
Then the best window size setting forthe development set of PTB was used for train-ing the neural networks described in Section 4.1The pre-trained vectors are available at https://code.google.com/p/word2vec2The pre-trained vectors are available at http://nlp.stanford.edu/projects/glove/3http://nlp.stanford.edu/software/lex-parser.shtmlWindow size Accuracy (%)# w2v glv pos stg cw All Unk.1 - - - - - 97.15 86.812 3 - - - - 97.36 88.963 - 3 - - - 97.34 89.554 3 3 - - - 97.40 90.445 3 3 3 - 1 97.44 90.176 3 3 3 1 1 97.44 90.537 3 3 3 3 1 97.45 90.228 3 3 6 - 1 97.41 90.519 3 3 6 3 1 97.44 90.15Table 2: Feature and window size selection: de-velopment accuracies of all tokens (All) and un-known tokens (Unk.)
of linear models trained onPTB (w2v: word2vec; glv: glove; pos: POS tagdistribution; stg: supertag distribution; cw: con-text word distribution).We fixed the group size at 8 for MAXOUT andLp, and the number of hidden variables was cho-sen from {32, 48} for MAXOUT and Lpand from{32, 64, 128, 256, 384} for ReLUs according to alltoken accuracy on the development data of PTB.We report the POS tagging accuracy for both allof the tokens and only for the unknown tokens thatdo not appear in the training set.5.2 ResultsTable 2 shows the accuracies of the linear modelson PTB with different window sizes for the con-tinuous features.
The window sizes of the wordembeddings (word2vec and glove) in Section 3.1,POS tag distributions in Section 3.2, supertag dis-tributions in Section 3.3, and context word distri-butions in Section 3.4 are shown in the columnsof w2v, glv, pos, stg, and cw, respectively.
Notethat ?-?
denotes the corresponding feature was notused at all and the first row with all ?-?
denotes theresults only using the original binary features fromChoi and Palmer (2012).
The window sizes in Ta-ble 2 are chosen mainly to investigate the effectof the word2vec embeddings, glove embeddings,and supertag distributions, since they had not pre-viously been used for POS tagging.The additions of the word embeddings improveall token accuracy by about 0.2 points accord-ing to the results shown in Nos.
1, 2, 3.
Al-though both word embeddings improved the ac-curacy of the unknown tokens, the gain of theglove embeddings (No.
3) is larger than that of the944Neural Network Settings Development Set Test Set# Activation functions #Hidden Group size (G) All Unk.
All Unk.1 Linear model - - 97.45 90.22 97.46 91.392 ReLUs 384 1 97.45 90.87 97.42 91.043 Lp(p = 2) 48 8 97.52 90.91 97.51 91.644 Lp(p = 3) 32 8 97.51 90.91 97.51 91.535 MAXOUT 48 8 97.50 90.89 97.50 91.676 Lp(p = 2) (w/o linear part) 48 8 97.39 91.18 97.40 91.23Table 3: Development and test accuracies of all tokens and unknown tokens (%) on PTB.Tagger All Unk.Manning (2011) 97.32 90.79S?gaard (2011) 97.50 N/ALp(p = 2) 97.51 91.64(a) Test accuracies on PTBTagger All Unk.Bohnet and Nivre (2012) 97.84 N/ALp(p = 2) 98.02 92.01(b) Test accuracies on CoNLL2009Table 4: Test accuracies of all tokens and unknown tokens (%) comparing with the previously reportedresultsword2vec (No.
2).
The reason for this differencein the two embeddings may be because the train-ing data for the glove vectors is 8 times larger thanthat for the word2vec vectors.
The usage of thetwo word embeddings shows further improvementin the tagging accuracy over single word embed-dings (No.
4).The addition of the POS tag distributions andthe context word distributions improves all tokenaccuracy (Nos.
5, 8).
The comparison between theresults with stag=?-?
(Nos.
5, 8) and stag = {1, 3}(Nos.
6, 7, 9) indicates the minor but consistentimprovement by using the supertag distributionfeatures in Section 3.3.
Finally, the 7th window-size setting in Table 2 achieves the best all tokenaccuracy among the linear models, so we chosethis setting for the experiments with the neural net-works.In Table 3, we compare the different settings ofthe neural networks with a single hidden layer4on the development set and test set from PTB.Neural networks with the MAXOUT and Lp(Nos.
3, 4, 5) significantly outperform the best lin-ear model (No.
1)5, but the accuracy of the Re-LUs (No.
2) was similar to that of the best lin-ear model.
According to these results, we argue4We leave the investigation of deeper neural networks asfuture work.5For significance tests, we have used the Wilcoxonmatched-pairs signed-rank test at the 95% confidence leveldividing the data into 100 data pairs.that the activation function selection is important,although conventional research in NLP has usedonly a single activation function.
It took roughly7 times as long to learn the hybrid models thanthe linear model (No.
1).
?Lp(p = 2) (w/o linearpart)?
(No.
6) shows the result for a Lp(p = 2)model which does not include the linear modelflinearfor the binary features.
Comparing the testresults of No.
6 with that of No.
3, the proposedhybrid architecture of a linear model and a neuralnetwork enjoys the benefits of both models.
Notethat No.
6?s accuracies of the unknown tokens arerelatively competitive, and this may be because thecontinuous features for the neural network do notinclude word surfaces.Since it shows the best accuracy for all tokenson the development set, we refer to Lp(p = 2)with 48 hidden variables and the group size of 8(No.
3 in Table 3) as our representative tagger anddenote it as Lp(p = 2) in the rest of discussion.In Table 4a, we compare our result with the pre-viously reported results and we see that our taggeroutperforms the current state-of-the-art systems onPTB for the accuracies of all tokens and unknowntokens.In addition, since our tagger was trained us-ing the dependency tree annotations as describedin Section 3.3, we compare it with the results ofBohnet and Nivre (2012) which is also trainedusing both POS tag and dependency annotations.Although their focus is on the dependency pars-945PC1?1.0 0.0 1.0 ?1.5 0.0 1.0?4?20?1.00.01.0PC2PC3?2.5?1.00.5?4 ?2 0?1.5?0.50.5?2.5 ?1.0 0.5PC4VBVBDVBGVBNVBPVBZ(a) PCA of the raw featuresPC1?0.5 0.5 ?0.8 ?0.2?0.8?0.20.4?0.50.00.5 PC2PC3?1.5?0.5?0.8 ?0.2 0.4?0.8?0.40.0?1.5 ?0.5PC4VBVBDVBGVBNVBPVBZ(b) PCA of the hidden activations of Lp(p = 2)Figure 2: Scatter plots of verbs for all combinations between the first four principal components of theraw features and the activation of hidden variables.ing, they report state-of-the art POS accuraciesfor many languages.
Note that Bohnet and Nivre(2012) also used external resources.
Table 4bgives the results for CoNLL2009 data set6.
Ourtagger outperform Bohnet and Nivre (2012), so webelieve this is the highest POS accuracy ever re-ported for a tagger trained on this data set.Finally, to visualize the learned representations,we applied principal components analysis (PCA)to the hidden activations hLof the first 10, 000 to-kens of the development set from PTB.
We alsoperformed PCA to the raw continuous inputs ofthe same data set.
Figure 2 shows the data plotsfor all the combinations among the first four prin-cipal components.
We plots only the verb tokensto make the plots easier to see.
Figures 2a and2b show the PCA results of the raw features andthe hidden activations of Lp(p = 2), respectively.Compared to Figure 2a, the tokens with the samePOS tag are more clearly clustered in Figure 2b.This suggests the neural network learned the goodrepresentations for POS tagging and these hiddenactivations can be used as the input of the succeed-ing processes, such as parsing.6The accuracies of our tagger on the development set ofCoNLL2009 data are 97.76% for all tokens and 93.42% forunknown tokens.6 Related WorkThere is some old work on the POS tagging byneural networks.
Nakamura et al.
(1990) proposeda neural tagger that predicts the POS tag using aprevious POS predictions.
Schmid (1994) is mostsimilar to our work.
The inputs of his neural net-work are the POS tag distributions of a word andits suffix in a context window, and he reports a2% improvement over a regular hidden Markovmodel.
However, his tagger did not use the otherkinds of corpus-wide information as we used.Most of the recent studies on POS tagging uselinear models (Suzuki and Isozaki, 2008; Spous-tov?a et al., 2009) or other non-linear models, suchas k-nearest neighbor (kNN) (S?gaard, 2011).One trend in these studies is model combinations.Suzuki and Isozaki (2008) combined generativeand discriminative models, Spoustov?a et al.
(2009)used the combination of three taggers to gener-ate automatically annotated corpus, and S?gaard(2011) used the outputs of a supervised tagger andan unsupervised tagger as the feature space of thekNN.
Our work also follows this trend since neuralnetworks can be considered as non-linear integra-tion of several linear classifiers.Apart from POS tagging, some previous studiesin parsing used the discretization method to handlethe combination of continuous features.
Bohnetand Nivre (2012) binned the difference of two con-946tinuous features in discrete steps of a predefinedsmall interval.
Bansal et al.
(2014) used the con-junction of discretized features and studied twodiscretization methods: One is the binning of realvalues into discrete steps and the other is a hardclustering of continuous feature vectors.
It is noteasy to determine the optimal intervals for the bin-ning method, and the clustering method is unsu-pervised so that the clusters are not guaranteed forgood representations of the target tasks.To capture rich syntactic information for Chi-nese POS tagging, Sun and Uszkoreit (2012) usedthe ensemble model of both a POS tagger and aconstituency parser.
Sun et al.
(2013) improvedthe efficiency of Sun and Uszkoreit (2012) inwhich a single tagging model is trained using au-tomatically annotated corpus generated by the en-semble tagger.
Although the supertag distributionfeature in Section 3.3 is a simple way to incor-porate syntactic information, automatically parsedlarge corpora may make the estimate of the su-pertag distributions more accurate.7 Conclusion and Future WorkWe are studying a neural network approach to han-dle the non-linear interaction among corpus-widestatistics.
For POS tagging, we used word em-beddings, POS tag distributions, supertag distribu-tions, and context word distributions in a contextwindow.
These features are beneficial, even forlinear classifiers, but the neural networks leveragethese features for improving tagging accuracies.Our tagger with Maxout networks (Goodfellow etal., 2013) or Lp-pooling (Zhang et al., 2014; Gul-cehre et al., 2014) show the state-of-the-art resultson two English benchmark sets.Our empirical results suggest further opportu-nities to investigate continuous features not onlyfor POS tagging but also for other NLP tasks.An obvious use case for continuous features isthe N-best outputs with confidence values, whichwere predicted by the previous process in a NLPpipeline, such as the POS tags used for syntacticparsing.
Another interesting extension is the use ofon-the-fly features which reflect previous networkstates, although the neural networks in our currentwork do not refer to the prediction history.
Recur-rent neural networks (RNNs) may be a solution torepresent the prediction history in a compact way,and Mesnil et al.
(2013) reported that RNNs out-perform conditional random fields (CRFs) on a se-quential labeling task.
They also show the superi-ority of bi-directional RNNs on their task, so thebi-directional RNNs may also be effective on thePOS tagging, since bi-directional inferences werealso used in earlier work (Tsuruoka and Tsujii,2005).It has a clear benefit over kernel methods inthat the test-time computational cost of neural net-works is independent from training data.
How-ever, although the test-time speed of original ker-nel methods is proportional to the number of train-ing data, recent development of kernel approxima-tion techniques achieve significant speed improve-ments (Le et al., 2013; Pham and Pagh, 2013).Since this work shows the non-linearity of contin-uous features should be exploited, those approxi-mated kernel methods may also improve the tag-ging accuracies without sacrifice tagging speed.Independent from our work, Ma et al.
(2014)and Santos and Zadrozny (2014) also recently pro-posed neural network approaches for POS tagging.Ma et al.
(2014)?s approach is similar to our ap-proach, with a combination of a linear model anda neural network, although a direct comparison isnot easy since their focus is the Web domain adap-tation of POS tagging.
Remarkably, they report n-gram embeddings are better than single word em-beddings.
Santos and Zadrozny (2014) proposedcharacter-level embedding to capture the morpho-logical and shape information for POS tagging.Although the reported accuracy (97.32%) on PTBdata is lower than state of the art results, their ap-proach is promising for morphologically rich lan-guages.
We may study the integration of these em-beddings into our approach as future work.ReferencesMohit Bansal, Kevin Gimpel, and Karen Livescu.2014.
Tailoring continuous word representations fordependency parsing.
In Proceedings of the AnnualMeeting of the Association for Computational Lin-guistics, Proceedings of the Conference (ACL).
TheAssociation for Computer Linguistics.James Bergstra and Yoshua Bengio.
2012.
Randomsearch for hyper-parameter optimization.
Journal ofMachine Learning Research, 13:281?305.Bernd Bohnet and Joakim Nivre.
2012.
A transition-based system for joint part-of-speech tagging andlabeled non-projective dependency parsing.
In Pro-ceedings of the Joint Conference on Empirical Meth-ods in Natural Language Processing and Com-947putational Natural Language Learning (EMNLP-CoNLL), pages 1455?1465.Y-Lan Boureau, Jean Ponce, and Yann LeCun.
2010.A theoretical analysis of feature pooling in visualrecognition.
In Proceedings of the InternationalConference on Machine Learning (ICML), pages111?118.Jinho D. Choi and Martha Palmer.
2012.
Fast androbust part-of-speech tagging using dynamic modelselection.
In Proceedings of the Annual Meeting ofthe Association for Computational Linguistics, Pro-ceedings of the Conference (ACL), pages 363?367.Ronan Collobert, Jason Weston, L?eon Bottou, MichaelKarlen, Koray Kavukcuoglu, and Pavel P. Kuksa.2011.
Natural language processing (almost) fromscratch.
Journal of Machine Learning Research,12:2493?2537.Koby Crammer and Yoram Singer.
2001.
On the algo-rithmic implementation of multiclass kernel-basedvector machines.
Journal of Machine Learning Re-search, 2:265?292.Hal Daum?e III, John Langford, and Daniel Marcu.2009.
Search-based structured prediction.
MachineLearning Journal, 75(3):297?325, June.John C. Duchi, Elad Hazan, and Yoram Singer.
2010.Adaptive subgradient methods for online learningand stochastic optimization.
In Proceedings ofthe Conference on Learning Theory (COLT), pages257?269.Jes?us Gim?enez and Llu?
?s M`arquez.
2003.
Fast and ac-curate part-of-speech tagging: The svm approach re-visited.
In Proceedings of Recent Advances in Natu-ral Language Processing (RANLP), pages 153?163.Yoav Goldberg and Joakim Nivre.
2012.
A dynamicoracle for arc-eager dependency parsing.
In Pro-ceedings of International Conference on Computa-tional Linguistics (COLING), pages 959?976.Ian J. Goodfellow, David Warde-Farley, Mehdi Mirza,Aaron C. Courville, and Yoshua Bengio.
2013.Maxout networks.
In Proceedings of the Interna-tional Conference on Machine Learning (ICML),pages 1319?1327.Ian J. Goodfellow, Mehdi Mirza, Xia Da, Aaron C.Courville, and Yoshua Bengio.
2014.
An empiricalinvestigation of catastrophic forgeting in gradient-based neural networks.
In Proceedings of Inter-national Conference on Learning Representations(ICLR).Caglar Gulcehre, Kyunghyun Cho, Razvan Pascanu,and Yoshua Bengio.
2014.
Learned-norm pool-ing for deep feedforward and recurrent neural net-works.
In Proceedings of the European Con-ference on Machine Learning and Principles andPractice of Knowledge Discovery in Databases(ECML/PKDD).Jan Haji?c, Massimiliano Ciaramita, Richard Johans-son, Daisuke Kawahara, Maria Ant`onia Mart?
?, Llu?
?sM`arquez, Adam Meyers, Joakim Nivre, SebastianPad?o, Jan?St?ep?anek, Pavel Stra?n?ak, Mihai Surdeanu,Nianwen Xue, and Yi Zhang.
2009.
The CoNLL-2009 shared task: Syntactic and semantic dependen-cies in multiple languages.
In Proceedings of theConference on Computational Natural LanguageLearning (CoNLL), pages 1?18.Fei Huang and Alexander Yates.
2009.
Distributionalrepresentations for handling sparsity in supervisedsequence-labeling.
In Proceedings of the AnnualMeeting of the Association for Computational Lin-guistics and the International Joint Conference onNatural Language Processing (ACL/IJCNLP), pages495?503.Quoc V. Le, Tam?as Sarl?os, and Alexander J. Smola.2013.
Fastfood - computing Hilbert space expan-sions in loglinear time.
In Proceedings of the Inter-national Conference on Machine Learning (ICML),pages 244?252.Omer Levy and Yoav Goldberg.
2014.
Linguisticregularities in sparse and explicit word represen-tations.
In Proceedings of the Thirteenth Confer-ence on Computational Natural Language Learning(CoNLL).Ji Ma, Yue Zhang, Tong Xiao, and Jingbo Zhu.
2014.Tagging the Web: Building a robust web tagger withneural network.
In Proceedings of the Annual Meet-ing of the Association for Computational Linguis-tics, Proceedings of the Conference (ACL).
The As-sociation for Computer Linguistics.Christopher D. Manning.
2011.
Part-of-speech tag-ging from 97% to 100%: Is it time for some linguis-tics?
In Proceedings of Conference on IntelligentText Processing and Computational Linguistics (CI-CLing), pages 171?189.Mitchell P. Marcus, Beatrice Santorini, and Mary AnnMarcinkiewicz.
1993.
Building a large annotatedcorpus of english: The Penn treebank.
Computa-tional Linguistics, 19(2):313?330.H.
Brendan McMahan, Gary Holt, David Sculley,Michael Young, Dietmar Ebner, Julian Grady,Lan Nie, Todd Phillips, Eugene Davydov, DanielGolovin, Sharat Chikkerur, Dan Liu, Martin Wat-tenberg, Arnar Mar Hrafnkelsson, Tom Boulos, andJeremy Kubica.
2013.
Ad click prediction: aview from the trenches.
In Proceedings of the ACMSIGKDD International Conference on KnowledgeDiscovery and Data Mining (KDD), pages 1222?1230.H.
Brendan McMahan.
2011.
Follow-the-regularized-leader and mirror descent: Equivalence theoremsand L1 regularization.
In Proceedings of the Four-teenth International Conference on Artificial Intelli-gence and Statistics (AISTATS), pages 525?533.948Gr?egoire Mesnil, Xiaodong He, Li Deng, and YoshuaBengio.
2013.
Investigation of recurrent-neural-network architectures and learning methods for spo-ken language understanding.
In Proceedings of An-nual Conference of the International Speech Com-munication Association (INTERSPEECH), pages3771?3775.Tomas Mikolov, Ilya Sutskever, Kai Chen, Gregory S.Corrado, and Jeffrey Dean.
2013.
Distributed repre-sentations of words and phrases and their composi-tionality.
In Proceedings of Advances in Neural In-formation Processing Systems (NIPS), pages 3111?3119.Vinod Nair and Geoffrey E. Hinton.
2010.
Rectifiedlinear units improve restricted Boltzmann machines.In Proceedings of the International Conference onMachine Learning (ICML), pages 807?814.Tetsuji Nakagawa and Yuji Matsumoto.
2006.
Guess-ing parts-of-speech of unknown words using globalinformation.
In Proceedings of the Annual Meet-ing of the Association for Computational Linguis-tics, Proceedings of the Conference (ACL).Masami Nakamura, Katsuteru Maruyama, TakeshiKawabata, and Kiyohiro Shikano.
1990.
Neuralnetwork approach to word category prediction forEnglish texts.
In Proceedings of International Con-ference on Computational Linguistics (COLING),pages 213?218.Hiroki Ouchi, Kevin Duh, and Yuji Matsumoto.
2014.Improving dependency parsers with supertags.
InProceedings of Conference of the European Chap-ter of the Association for Computational Linguistics(EACL), pages 154?158.Neal Parikh and Stephen P. Boyd.
2013.
Proximal al-gorithms.
Foundations and Trends in Optimization,1(3):123?231.Jeffrey Pennington, Richard Socher, and Christo-pher D. Manning.
2014.
GloVe: global vectorsfor word representation.
In Proceedings of the Con-ference on Empirical Methods on Natural LanguageProcessing (EMNLP).Ninh Pham and Rasmus Pagh.
2013.
Fast and scal-able polynomial kernels via explicit feature maps.In Proceedings of the ACM SIGKDD InternationalConference on Knowledge Discovery and Data Min-ing (KDD), pages 239?247.St?ephane Ross, Geoffrey J. Gordon, and Drew Bagnell.2011.
A reduction of imitation learning and struc-tured prediction to no-regret online learning.
In Pro-ceedings of the Fourteenth International Conferenceon Artificial Intelligence and Statistics (AISTATS),pages 627?635.Cicero Dos Santos and Bianca Zadrozny.
2014.Learning character-level representations for part-of-speech tagging.
In Proceedings of the InternationalConference on Machine Learning (ICML), pages1818?1826.Helmut Schmid.
1994.
Part-of-speech tagging withneural networks.
In Proceedings of InternationalConference on Computational Linguistics (COL-ING), pages 172?176.Tobias Schnabel and Hinrich Sch?utze.
2014.
FLORS:Fast and simple domain adaptation for part-of-speech tagging.
Transactions of the Association forComputational Linguistics, 2:15?26, February.Richard Socher, Eric H. Huang, Jeffrey Pennington,Andrew Y. Ng, and Christopher D. Manning.
2011.Dynamic pooling and unfolding recursive autoen-coders for paraphrase detection.
In Proceedings ofAdvances in Neural Information Processing Systems(NIPS), pages 801?809.Richard Socher, Alex Perelygin, Jean Wu, JasonChuang, Chris Manning, Andrew Ng, and ChrisPotts.
2013.
Recursive deep models for semanticcompositionality over a sentiment treebank.
In Pro-ceedings of the Conference on Empirical Methodson Natural Language Processing (EMNLP), pages1631?1642.Anders S?gaard.
2011.
Semi-supervised condensednearest neighbor for part-of-speech tagging.
In Pro-ceedings of the Annual Meeting of the Associationfor Computational Linguistics, Proceedings of theConference (ACL), pages 48?52.Drahom?
?ra johanka Spoustov?a, Jan Haji?c, Jan Raab,and Miroslav Spousta.
2009.
Semi-supervisedtraining for the averaged perceptron pos tagger.
InProceedings of Conference of the European Chap-ter of the Association for Computational Linguistics(EACL), pages 763?771.Weiwei Sun and Hans Uszkoreit.
2012.
Capturingparadigmatic and syntagmatic lexical relations: To-wards accurate Chinese part-of-speech tagging.
InProceedings of the Annual Meeting of the Associa-tion for Computational Linguistics, Proceedings ofthe Conference (ACL), pages 242?252.Weiwei Sun, Xiaochang Peng, and Xiaojun Wan.2013.
Capturing long-distance dependencies in se-quence models: A case study of Chinese part-of-speech tagging.
In Proceedings of the InternationalJoint Conference on Natural Language Processing(IJCNLP), pages 180?188.Ilya Sutskever, James Martens, George E. Dahl, andGeoffrey E. Hinton.
2013.
On the importance ofinitialization and momentum in deep learning.
InProceedings of the International Conference on Ma-chine Learning (ICML), pages 1139?1147.Jun Suzuki and Hideki Isozaki.
2008.
Semi-supervisedsequential labeling and segmentation using giga-word scale unlabeled data.
In Proceedings of the An-nual Meeting of the Association for ComputationalLinguistics, Proceedings of the Conference (ACL),pages 665?673.949Kristina Toutanova, Dan Klein, Christopher D. Man-ning, and Yoram Singer.
2003.
Feature-rich part-of-speech tagging with a cyclic dependency net-work.
In Proceedings of the Conference of the NorthAmerican Chapter of the Association for Computa-tional Linguistics: Human Language Technologies(NAACL-HLT), pages 252?259.Yoshimasa Tsuruoka and Jun?ichi Tsujii.
2005.
Bidi-rectional inference with the easiest-first strategy fortagging sequence data.
In Proceedings of HumanLanguage Technology Conference and Conferenceon Empirical Methods in Natural Language Pro-cessing (HLT-EMNLP), pages 467?474.Joseph P. Turian, Lev-Arie Ratinov, and Yoshua Ben-gio.
2010.
Word representations: A simple and gen-eral method for semi-supervised learning.
In Pro-ceedings of the Annual Meeting of the Associationfor Computational Linguistics, Proceedings of theConference (ACL), pages 384?394.Mengqiu Wang and Christopher D. Manning.
2013.Effect of non-linear deep architecture in sequencelabeling.
In Proceedings of the International JointConference on Natural Language Processing (IJC-NLP).Xiaohui Zhang, Jan Trmal, Daniel Povey, and San-jeev Khudanpur.
2014.
Improving deep neuralnetwork acoustic models using generalized maxoutnetworks.
In Proceedings of International Confer-ence on Acoustics, Speech and Signal Processing(ICASSP).Alisa Zhila, Wen tau Yih, Christopher Meek, GeoffreyZweig, and Tomas Mikolov.
2013.
Combining het-erogeneous models for measuring relational similar-ity.
In Proceedings of the Conference of the NorthAmerican Chapter of the Association for Computa-tional Linguistics: Human Language Technologies(NAACL-HLT), pages 1000?1009.950
