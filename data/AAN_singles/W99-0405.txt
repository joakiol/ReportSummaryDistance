Modeling the language assessment process and result:Proposed architecture for automatic oral proficiency assessmentGina-Anne  Levow and Mar i  B roman OlsenUniversity of Maryland Inst i tute for Advanced Computer  StudiesCollege Park, MD 20742{gina,molsen}@umiacs.umd.eduAbst rac tWe outline challenges for modeling human lan-guage assessment in automatic systems, both interms of the process and the reliability of the re-sult.
We propose an architecture for a system toevaluate learners of Spanish via the Computer-ized Oral Proficiency Instrument, o determinewhether they have 'reached' or 'not reached' theIntermediate Low level of proficiency, accordingto the American Council on the Teaching of For-eign Languages (ACTFL) Speaking ProficiencyGuidelines.
Our system divides the acousticand non-acoustic features, incorporating humanprocess modeling where permitted by the tech-nology and required by the domain.
We suggestmachine learning techniques applied to this typeof system permit insight into yet unarticulatedaspects of the human rating process.1 I n t roduct ionComputer-mediated language assessment ap-peals to educators and language valuators be-cause it has the potential for making languageassessment widely available with minimal hu-man effort and limited expense.
Fairly robustresults (n '~ 0.8) have been achieved in the com-mercial domain modeling the human rater re-sults, with both the Electronic Essay Rater (e-rater) system for written essay scoring (Bursteinet al, 1998), and the PhonePass pronunciationassessment (Ordinate, 1998).There are at least three reasons why it isnot possible to model the human rating pro-cess.
First, there is a mismatch between whatthe technology is able to handle and what peo-ple manipulate, especially in the assessmentof speech features.
Second, we lack a well-articulated model of the human process, oftencharacterized as holistic.
Certain assessmentfeatures have been identified, but their rela-tive importance is not clear.
Furthermore, un-like automatic assessments, human raters of oralproficiency exams are trained to focus on com-petencies, which are difficult to enumerate.
Incontrast, automatic assessments of spoken lan-guage fluency typically use some type of errorcounting, comparing duration, silence, speakingrate and pronunciation mismatches with nativespeaker models.There is, therefore, a basic tension withinthe field of computer-mediated language assess-ment, between modeling the assessment pro-cess of human raters or achieving comparable,consistent assessments, perhaps through differ-ent means.
Neither extreme is entirely satisfac-tory.
A spoken assessment system that achieveshuman-comparable performance based only, forexample, on the proportion of silence in an ut-terance would seem not to be capturing a num-ber of critical elements of language competence,regardless of how accurate the assessments are.Such a system would also be severely limitedin its ability to provide constructive feedbackto language learners or teachers.
The e-ratersystem has received similar criticism for basingessay assessments on a number of largely lexicalfeatures, rather than on a deeper, more human-style rating process.Thirdly, however, even if we could articulateand model human performance, it is not clearthat we want to model all aspects of the hu-man rating process.
For example, human per-formance varies due to fatigue.
Transcribers of-ten inadvertently correct examinees' errors ofomitted or incorrect articles, conjugations, oraffixes.
These mistakes are a natural effect ofa cooperative listener; however, they result inan over-optimistic assessment of the speaker'sactual proficiency.
We arguably do not wish tobuild this sort of cooperation i to an automated24assessment system, though it is likely desirablefor other sorts of human-computer interactionsystems.Furthermore, if we focus on modeling hu-man processes we may end up underutillzing thetechnology.
Balancing human-derived featureswith machine learning techniques may actuallyallow us to discuss more about the human rat-ing process by making the entire process avail-able for inspection and evaluation.
For exam-ple, if we are able to articulate human ratingfeatures, machine learning techniques may al-low us to 'learn' the relative weighting of thesefeatures for a particular assessment value.2 Mode l ing  the  ra ter2.1 Inference gz Inductive BiasResearch in machine learning has demonstratedthe need for some form of inductive bias, to limitthe space of possible hypotheses the learningsystem can infer.
In simple example-based con-cept learning, concepts are often restricted tocertain classes of Boolean combinations, uch asconjuncts of disjuncts, in order to make learningtractable.
Recent research in automatic induc-tion of context-free grammars, a topic of moredirect interest o language learning and assess-ment, also attests to the importance of structur-ing the class of grammars that can be inducedfrom a data set.
For instance Pereira and Sch-abes (1992) demonstrate hat a grammar learn-ing algorithm with a simple constraint on binarybranching (CNF) achieves less than 40% accu-racy after training on an unbracketed corpus.Two alternatives achieve comparable in-creases in grammatical accuracy.
Training onpartially bracketed corpora - providing more su-pervision and a restriction on allowable gram-mars - improves to better than 90%.
(DeMar-cken, 1995) finds that requiring binary branch-ing, as well as headedness and head projectionrestrictions on the acquirable grammar, leadsto similar improvements.
These results arguestrongly that simply presenting raw text or fea-ture sequences to a machine learning program tobuild an automatic rating system for languageassessment is of limited utility.
Results will bepoorer and require substantially more trainingdata than if some knowledge of the task or clas-sifter end-state based in human knowledge andlinguistic theory is applied to guide the searchfor classifiers.2.2 Encoding Linguistic KnowledgeWhy, then, if it is necessary to encode humanknowledge in order to make machine learningpractical, do we not simply encode ach piece ofthe relevant assessment knowledge from the per-son to the machine?
Here again parallels withother areas of Natural Language Processing(NLP) and Artificial Intelligence (AI) provideguidance.
While both rule-based, hand-craftedgrammars and expert systems have played auseful role, they require substantial labor toconstruct and become progressively more diffi-cult to maintain as the number of rules and ruleinteractions increases.
Furthermore, this laboris not transferable to a new (sub-)language ortopic and is difficult to encode in a way thatallows for graceful degradation.Another challenge for primarily hand-craftedapproaches i identifying relevant features andtheir relative importance.
As is often noted,human assessment of language proficiency islargely holistic.
Even skilled raters have diffi-culty identifying and quantifying those featuresused and their weights in determining an as-sessment.
Finally, even when identifiable, thesefeatures may not be directly available to a com-puter system.
For instance, in phonology, hu-man listeners perceive categorical distinctionsbetween phonemes (Eimas et al, 1971; Thi-bodeau and Sussman, 1979) whereas acousticmeasures vary continuously.We appeal to machine learning techniques inthe acoustic module, as well as in the pool-ing of information from both acoustic and non-acoustic features.3 Domain :  The  Computer i zed  Ora lProficiency InstrumentThe Center for Applied Linguistics in Washing-ton, D.C. (CAL) has developed or assisted indeveloping simulated oral proficiency interview(SOPI) tests for a variety of languages, recentlyadapting them to a computer-administered for-mat, the COPI.
Scoring at present is done en-tirely by human raters.
The Spanish versionof the COPI is in the beta-test phase; Chineseand Arabic versions are under development.
Allfocus on assessing proficiency at the Intermedi-ate Low level, defined by the American Councilon the Teaching of Foreign Languages (ACTFL)25Speaking Proficiency Guidelines (ACT, 1986),a common standard for passing at many highschools.
We focus on Spanish, since we will haveaccess to reed data.
Our goal is to develop a sys-tem with a high interannotator agreement withhuman raters, such that it can replace one of thetwo or three raters required for oral proficiencyinterview scoring.With respect to non-acoustic features, ourdomain is tractable, for current natured lan-guage processing techniques, since the inputis expected to be (at best) sentences, perhapsonly phrases and words at Intermediate Low.Although the tasks and the language at thislevel are relatively simple, the domain variesenough to be interesting from a research stand-point: enumerating items in a picture, leav-ing a answering machine message, requesting acar rental, giving a sequence of directions, anddescribing one's family, among others.
Thesetasks elicit a more varied, though still topicallyconstrained, vocabulary.
They also allow theassessment of the speaker's grasp of target lan-guage syntax, and, in the more advanced tasks,discourse structure and transitions.
The COPI,therefore, provides a natured omain for ratingnon-native speech on both acoustic and non-acoustic features.
These subsystems differ interms of how amenable they are to machinemodeling of the human process, as outlined be-low.4 Acoustic Features: The SpeechRecognition ProcessIn the last two decades ignificant advances havebeen made in the field of automatic speechrecognition (SR), both in commercial and re-search domains.
Recently, research interestin recognizing non-native speech has increased,providing direct comparisons of recognition ac-curacy for non-native speakers at different lev-els of proficiency (Byrne et ed., 1998).
Tomokiyo(p.c.
), in experiments with the JANUS (Waibelet ed., 1992) speech system developed byCarnegie Mellon University, reports that sys-tems with recognition accuracies of 85% fornative speech perform at 40-50% for high flu-ency L2 learners (German, Tomokiyo, p.c.
)and 30% for medium fluency speech (Japanese,Tomokiyo, p.c.
).However, the current speech recognition tech-nology makes little or no effort to model thehuman auditory or speech understanding pro-cess.
Furthermore, standard SR approachesto speaker adaptation rely on relatively largeamounts (20-30 minutes) of fixed, recordedspeech (Jecker, 1998) to modify the underly-ing model, say in the case of accented speech,again unlike human listeners.While a complete reengineering of speechrecognition is beyond the scope of our currentproject, we do attempt o model the human as-sessor's approach to understanding non-nativespeech.
The SR system allows us two points ofaccess through which linguistic knowledge of L2phonology and grammar can be applied to im-prove recognition: the lexicon and the speechrecognizer grammar.4.1 Lexicon: Transfer -model -basedPhonological-AdaptationSince we have too little data for conventionalspeaker adaptation (less than 5 minutes ofspeech per examinee), we require a principledway of adapting an L1 or L2 recognizer modelto non-native learner's peech that places lessreliance upon recorded training data.
We knowthat the pronunciation reflects native languageinfluence, most notably at early stages (Noviceand Intermediate), with which we are primar-ily concerned.
Following the L2 transfer acqui-sition model, we assume that the L2 speaker,in attempting to produce target language ut-terances, will be influenced by L1 phonologyand phonotactics.
Thus, rather than being ran-dom divergences from TL pronunciation, errorsshould be closer to L1 phonetic realizations.To model these constraints, we will employtwo distinct speech recognizers that can be em-ployed to recognize L2 speech, produced byadaptations pecific to Target Language (TL)and Source Language (SL).
We propose touse language identification technology to arbi-trate between the two sets of recognizer results,based on a sample of speech, either countingto 20, or a short read text.
Since we need tochoose between an underlying TL phonologi-ced model and one based on the SL, we willmake the selection based on the language iden-tification decision as to the apparent phonolog-ical identity of the sample as SL or TL, basedon the sample's phonological and acoustic fea-tures (Berkling and Barnard, 1994; Hazen and26Zue, 1994; Kadambe and Hieronymus, 1994;Muthusamy, 1993).
Parameterizing phoneticexpectation based on a short sample of speech(Ladefoged and Broadbent, 1957) or expecta-tions in context (Ohala and Feder, 1994) mirrorswhat people do in speech processing enerally,independent of the rating context.4.2 An  acoust ic  g rammar :  Modelingthe processModeling the grammar of a Novice or Inter-mediate level L2 speaker for use by a speechrecognizer is a challenging task.
As noted inthe ACTFL guidelines, these speakers are fre-quently inaccurate.
However, to use the con-tent of the speech in the assessment, we needto model human raters, who recognize ven er-rorful speech as accurately and completely aspossible.
Speech recognizers work most effec-tively when perplexity is low, as is the casewhen the grammar and vocabulary are highlyconstrained.
However, speech recognizers alsorecognize what they are told to expect, of-ten accepting and misrecognizing utteranceswhen presented with out-of-vocabulary or out-of-grammar input.
We must balance these con-flicting demands.We will take advantage of the fact that thistask is being performed off-line and thus can tol-erate recognizer speeds everal times real-time.We propose a multi-pass recognition processwith step-wise relaxation of grammatical con-straints.
The relaxed grammar specifies a nounphrase with determiner and optional adjectivephrase but relaxes the target language restric-tions on gender and number agreement amongdeterminer, noun, and adjective and on posi-tion of adjective.
Similar relaxations can be ap-plied to other major constructions, such as verbsand verbal conjugations, to pass, without "cor-recting", utterances with small target languageinaccuracies.
For those who would not reachsuch a level, and for tasks in which sentence-level structure is not expected, we must relaxthe grammar still further, relying on rejection atthe first pass grammar to choose grammars ap-propriately.
Successive relaxation of the gram-mar model will allow us to balance the need toreduce perplexity as much as possible with theneed to avoid over-predicting and thereby cor-recting the learner's peech.4.3 Acoust ic  features: Modeling theresultResearch in the area of pronunciation scoring(Rypa, 1996; Franco et al, 1997; Ehsani andKnodt, 1998) has developed both direct andindirect measures of speech quality and pro-nunciation accuracy, none of which seem tomodel human raters at any level.
The directmeasures include calculations of phoneme r-ror rate, computed as divergence from nativespeaker model standards, and number of incor-rectly pronounced phonemes.
The indirect mea-sures attempt to capture some notion of flu-ency and include speaking rate, number andlength of pauses or silences, and total utterancelength.
Analogous measures should prove usefulin the current assessment of spoken proficiency.In addition, one could include, as a baseline,a human-scored measure of perceived accent orlack of fluency.
A final measure of acoustic qual-ity could be taken from the language identifica-tion process used in the arbitration phase, as towhether the utterance was more characteristicof the source or target language.
In our samplesof Intermediate Low passing speech we iden-tify, for example, large proportions of silenceto speech both between and within sentences.Some sentences are more than 50% silence.5 Natura l  Language Understanding:Linguistic Features AssessmentIn the non-acoustic features, we have a fairlyexplicit notion of generative competence and areasonable way of encoding syntax in terms ofContext-Free Grammars (CFGs) and semanticsvia Lexical Conceptual Structures (LCSs).
Wedo not know, however, the relative importanceof different aspects of this competence in deter-mining reached/not reached for particular lev-els in an assessment task.
Therefore, we applymachine learning techniques to pool the human-identified features, generating a machine-basedmodel of process which is fully explicit andamenable to evaluation.The e-rater system, deployed by the Educa-tional Testing Service (ETS) incorporates morethan 60 variables based on properties used byhuman raters and divided into syntactic, rhetor-ical and topical content categories.
Althoughthe features deal with suprasentential structure,the reported variables (Burstein et al, 1998)27are identified via lexical information and shal-low constituent parsing, arguably not modelingthe human process.We attempt o model the features based ona deeper analysis of the structure of the text atvarious levels.
We propose to parallel the archi-tecture of the Military Language Tutoring sys-tem (MILT), developed jointly by the Univer-sity of Maryland and Micro Analysis and Designcorporation under army sponsorship.
MILTprovides a robust model of errors from Englishspeakers learning Spanish and Arabic, identify-ing lexical and syntactic haracteristics of shorttexts, as well as low-level semantic features, aprerequisite for more sophisticated inferencing(Dorr et al, 1995; Weinberg et al, 1995).
At aminimum, the system will provide linguisticallyprincipled feedback on errors of various types,rather than providing system error messages, orcrashing on imperfect input.Our work with MILT and the COPI beta-test data suggests that relevant features maybe found in each of four main areas of spokenlanguage processing: acoustic, lexical, syntac-tic/semantic ' and discourse.
In order to au-tomate the assessment s age of the oral profi-ciency exam, we must identify features of theL2 examinees' utterances that are correlatedwith different ratings and that can be extractedautomatically.
If we divide language up intoseparate components, we can describe a widerange of variation within a bounded set of pa-rameters within these components.
We cantherefore build a cross-linguistically valid meta-interpreter with the properties we desire (com-pactness, robustness and extensibility).
Thismakes both engineering and linguistic sense.Our system treats the constraints as submod-ules, able to be turned on or off, at the instruc-tor's choice, made based on, e.g., what is learnedearly, and the level of correction desired.
TheMILT-style architecture allows us to make use ofthe University of Maryland's other parsing andlexicon resources, including large scale lexica inSpanish and English.5.1 Lexical featuresOne would expect command of vocabulary tobe a natural component of a language learner'sproficiency in the taxget language.
A varietyof automatically extractable measures providecandidate features for assessing the examinee'slexical proficiency.
In addition, the structure ofthe tasks in the examination allows for testingof extent of lexical knowledge in restricted com-mon topics.
For instance, the student may beasked to count to twenty in the target languageor to enumerate he items in a pictured context,such as a classroom scene.
Within these tasksone can test for the presence and number of spe-cific desired vocabulary items, yielding anothermeasure of lexical knowledge.Simple measures with numerical values in-clude number of words in the speech sample andnumber of distinct words.
In addition, exami-nees at this level frequently rely on vocabularyitems from English in their answers.A deeper type of knowledge may be capturedby the lexicon in Lexical Conceptual Structure(LCS) (Dorr, 1993b; Dorr, 1993a; Jackendoff,1983).
The LCS is an interlingual framework forrepresenting semantic elements that have syn-tactic reflexes3 LCSs have been ported fromEnglish into a variety of languages, includingSpanish, requiring a minimum of adaptation ineven unrelated languages (e.g.
Chinese (Olsenet al, 1998)).
The representation i dicatesthe argument-taking properties of verbs (hit re-quires an object; smile does not), selectionalconstraints (the subject of fear and the object offrighten are animate), thematic information ofarguments (the subject of frighten is an agent;the object is a patient) and classification in-formation of verbs (motion verbs like go areconceptually distinct from psychological verbslike fear/frighten; run is a more specific type ofmotion verb than go).
Each information typeis modularly represented and therefore may beseparately analyzed and scored.5.2 Syntact ic  featuresWe adopt a generative approach to grammar(Government and Binding, Principles and Pa-rameter, Minimalism) principles.
In these mod-els, differences in the surface structure of lan-guages can be reduced to a small number ofmodules and parameters.
For example, al-though Spanish and English both have subject-verb-object (SVO) word order, the relative or-dering of many nouns and adjectives differs (the1That is, the LCS does not represent non-syntacticaspects of 'meaning', including metaphor and pragmat-iC$.28"head parameter").
2 In English the adjectiveprecedes the noun, whereas Spanish adjectivesof nationality, color and shape regularly fol-low nouns (Whitley, 1986)\[pp.
241-2\].
TheMILT architecture allows us both to enumer-ate errors of these types, and parse data thatincludes uch errors.
We will also consider mea-sures of number and form of distinct construc-tion types, both attempted and correctly com-pleted.
Such constructs could include simpledeclaratives ( ubject, verb, and one argument),noun phrases with both determiner and adjec-tive, with correct agreement and word order,questions, and multi-clause sentences.5.3 Semant ic  featuresLike the syntactic information, lexical informa-tion can be used modularly to assess a varietyof properties of examinees' peech.
The LexicalConceptual Structure (LCS) allows principles ofrobustness, flexibility and modularity to applyto the semantic omponent of the proposed sys-tem.
The LCS serves as the basis of severaldifferent applications, including machine trans-lation and information retrieval as well as for-eign language tutoring.
The LCS is considereda subset of mental representation, that is, thelanguage of mental representation as realized inlanguage (Dorr et al, 1995).
Event types suchas event and state, are represented in primitivessuch as GO, STAY, BE, GO-EXT and ORI-ENT, used in spatial and other 'fields'.
As such,it allows potential modeling of human rater pro-cesses.The LCS allows various syntactic forms tohave the same semantic representation, e.g.Walk to the table and pick up the book, Go tothe table and remove the book, or Retrieve thebook from the table.
COPI examinees are alsoexpected to express similar information in dif-ferent ways.
We propose to use the LCS struc-ture to handle and potentially enumerate com-petence in this type of variation.Stored LCS representations may also handlehierarchical relations among verbs, and diver-gences in the expression of elements of meaning,sometimes reflecting native language word or-der.
The modularity of the system allows us totease the semantic and syntactic features apart,2Other parameters deal with case, theta-role assign-ment, binding, and bounding.giving credit for the semantic expression, butidentifying divergences from the target L2.5.4 Discourse featuresSince the ability to productively combine wordsin phrase and sentence structures eparates theIntermediate l arner from the Novice, featuresthat capture this capability should prove use-ful in semi-automatic assessment of Intermedi-ate Low level proficiency, our target level.
Ac-cording to the ACTFL Speaking ProficiencyGuidelines, Intermediate Low examinees beginto compose sentences; full discourses do notemerge until later levels of competence.
Nev-ertheless, we want both to give credit for anydiscourse-level features that surface, as well asto provide a slot for such features, to allow scal-ability to more complex tasks and higher levelsof competence.
We will therefore develop dis-course and dialog models, with the appropriateand measurable characteristics.
Many of thesecan be lexically or syntactically identified, asthe ETS GMAT research shows (Burstein et al,1998).
Our data might include uses of discourseconnectives (entonces 'then; in that case' pero'but'; es que 'the fact is'; cuando 'when'), othersubordinating structures (Yo creo que 'I thinkthat') and use of pronouns instead of repeatednouns.
The discourse measures can easily beexpanded to cover additional, more advancedconstructions that are lexically signaled, suchas the use of subordination or paragraph-levelstructures.6 Mach ine  learn ingWhile the above features capture some measureof target language speaking proficiency, it is dif-ficult to determine a priori which features orgroups of features will be most useful in makingan accurate assessment.
In this work, humanassessor ratings for those trained on the Speak-ing Proficiency Guidelines will be used as the"Gold Standard" for determining accuracy ofautomatic assessment.
We plan to apply ma-chine learning techniques to determine the rel-ative importance of different feature values inrating a speech sample.The assessment phase goes beyond the cur-rent work in test scoring, combining recogni-tion of acoustic features, such as the Auto-matic Spoken Language Assessment by Tele-phone (ASLAT) or PhonePass (Ordinate, 1998)29with aspects of the syntactic, discourse, and se-mantic factors, as in e-rater.
Our goal is tohave the automatic scoring system mirror theoutcome of raters trained in the ACTFL Guide-lines (ACT, 1986), to determine whether exam-inees did or did not reach the Intermediate Lowlevel.
We also aim to make the process of featureweighting transparent, sothat we can determinewhether the system provides an adequate modelof the human rating process.
We will evalu-ate quantitatively the extent to which machineclassification agrees with human raters on bothacoustic and non-acoustic properties alone andseparately.
We will also evaluate the processqualitatively with human raters.We plan to exploit the natural structuringof the data features through decision trees ora small hierarchical "mixture-of-experts"- typemodel (Quinlan, 1993; Jacobs et al, 1991; Jor-dan and Jacobs, 1992).
Intuitively, the lat-ter approach creates experts (machine-learningtrained classifiers) for each group of features(acoustic, lexical, and so on).
The correctway of combining these experts is then ac-quired though similar machine learning tech-niques.
The organization of the classifier allowsthe machine learning technique at each stageof the hierarchy to consider fewer features, andthus, due to the branching structure of treeclassifier, dramatically fewer classifier configu-rations.Decision tree type classifiers have an addi-tional advantage: unlike neural network or near-est neighbor classifiers, they are easily inter-pretable by humans.
The trees can be rewrittentrivially as sequences of if-then rules leading toa certain classification.
For instance, in the as-sessment task, one might hypothesize a rule ofthe form: IF silence > 20% of utterance, THENIntermediate Low NOT REACHED.
It is thuspossible to have human raters analyze how wellthe rules agree with their own intuitions aboutscoring and to determine which automatic fea-tures play the most important role in assess-ment.7 Conc lus ionsWe have outlined challenges for modeling thehuman rating task, both in terms of processand result.
In the domain of acoustic featuresand speech recognition, we suggest the technol-ogy currently does not permit complete mod-eling of the rating process.
Nevertheless, thepaucity of data in our domain requires us toadopt the transfer model of speech, which per-mits automatic adaptation to errorful speech.In addition, our recognizer incorporates a re-laxed grammar, permitting input to vary fromthe target language at the lexical and syntacticlevels.
These adaptations allow us to model hu-man perception and processing of (non-native)speech, as required by our task.
In the non-acoustic domain, we also adopt machine learn-ing techniques to pool the relevant human-identified features.
As a result, we can learnmore about the feature-weighting i  the processof tuning to an appropriate level of reliabilitywith the results of human raters.Re ferences1986.
Proficiency guidelines.
American Councilfor the Teaching of Foreign Languages.Kay M. Berkling and Etienne Barnard.
1994.Language identification of six languagesbased on a common set of broad phonemes.In Proceedings ofICSLP \[ICS9~\], pages 1891-1894.Jill Burstein, Karen Kukich, Susanne Wolff, ChiLu, Martin Chodorow, Lisa Braden-Harder,and Mary Dee Harris.
1998.
Automated scor-ing using a hybrid feature identification tech-nique.
In ACL/COLING 98, pages 206-210,Montreal, Canada, August 10-14.William Byrne, Eva Knodt, Sanjeev Khudan-pur, and Jared Bernstein.
1998.
Is auto-matic speech recognition ready for non-nativespeech?
a data collection effort and ini-tim experiments in modeling conversationalhispanic english.
In Proceedings of STILL(Speech Technology in Language Learning),Marholmen, Sweden.
European Speech Com-munication Association, May.Carl DeMarcken.
1995.
Lexical heads, phrasestructure, and the induction of grammar.
InProceedings ofThird Workshop on Very LargeCorpora, Cambridge, MA.Bonnie J. Dorr, Jim Hendler, Scott Blanksteen,and Barrie MigdMoff.
1995.
Use of LCS andDiscourse for Intelligent Tutoring: On Be-yond Syntax.
In Melissa Holland, JondthanKaplan, and Michelle Sams, editors, Intelli-gent Language Tutors: Balancing Theory and30Technology, pages 289-309.
Lawrence Erl-baum Associates, Hillsdale, NJ.Bonnie J. Dorr.
1993a.
Interlingual MachineTranslation: a Parameterized Approach.
Ar-tificial Intelligence, 63(1&2):429-492.Bonnie J. Dorr.
1993b.
Machine Translation:A View from the Lexicon.
The MIT Press,Cambridge, MA.Farzad Ehsani and Eva Knodt.
1998.
Speechtechnology incomputer-aided language l arn-ing: Strengths and limitations of a new CALLparadigm.
Language Learning ~ Technology,2(1), July.Peter D. Eimas, Einar R. Siqueland, Pe-ter Jusczyk, and James Vigorito.
1971.Speech perception in infants.
Science,171(3968):303-306, January.H.
Franco, L. Neumeyer, Y. Kim, and O. Ro-hen.
1997.
Automatic pronunciation scoring?
for language instruction.
In Proceedings ofICASSP, pages 1471-1474, April.Timothy J. Hazen and Victor W. Zue.
1994.Recent improvements in an approach tosegment-based automatic language identifica-tion.
In Proceedings of ICSLP \[ICS94\], pages1883-1886.Ray Jackendoff.
1983.
Semantics and Cogni-tion.
The MIT Press, Cambridge, MA.R.
A. Jacobs, M. I. Jordan, S. J. Nowlan, andG.
E. Hinton.
1991.
Adaptive mixtures of lo-cal experts.
Neural Computation, 3(1):79-87.D.
Jecker.
1998.
Speech recognition - perfor-mance tests.
PC Magazine, 17, March.M.
I. Jordan and R. A. Jacobs.
1992.
Hierar-chies of adaptive xperts.
In Nips4.Shubha Kadambe and James L. Hieronymus.1994.
Spontaneous speech language identifi-cation with a knowledge oflinguistics.
In Pro-ceedings of ICSLP \[ICS94\], pages 1879-1882.P.
Ladefoged and D.E.
Broadbent.
1957.
Infor-mation conveyed by vowels.
Journal of theAcoustical Society of America, 29:98-104.Yeshwant K. Muthusamy.
1993.
A Segmen-tal Approach to Automatic Language Identi-fication.
Ph.D. thesis, Oregon Graduate In-stitute of Science & Technology, P.O.
Box91000, Portland, OR 97291-1000.J.J.
Ohala and D. Feder.
1994.
Listeners'normalization of vowel quality is influencedby restored consonantal context.
Phonetica,51:111-118.Marl Broman Olsen, Bonnie J. Dorr, andScott C. Thomas.
1998.
Enhancing Auto-matic Acquisition of Thematic Structure ina Large-Scale Lexicon for Mandarin Chinese.In Proceedings of the Third Conference ofthe Association for Machine Translation inthe Americas, AMTA-98, in Lecture Notesin Artificial Intelligence, 1529, pages 41-50,Langhorne, PA, October 28-31.Ordinate.
1998.
The PhonePass Test.
Tech-nical report, Ordinate Corporation, MenloPark, CA, January.Fernando Pereira and Yves Schabes.
1992.Inside-outside reestimation from partiallybracket corpora.
In Proceedings of the30th Annual Meeting of the Association forComputational Linguistics, pages 128-135,Newark, DE.J.
R. Quinlan.
1993.
C4.
5: Programs for Ma-chine Learning.
Morgan Kaufmann, San Ma-teo, CA.M.
Rypa.
1996.
VILTS: The voice interactivelanguage training system.
In Proceedings ofCALICO, July.Linda M. Thibodeau and Harvey M. Sussman.1979.
Performance on a test of categoricalperception of speech in normal and com-munication disordered children.
Phonetics,7(4):375-391, October.A.
Waibel, A.N.
Jain, A. McNair, J. Tebel-sis, L. Osterholtz, H. Salto, O. Schmid-bauer, T. Sloboda, and M. Woszczyna.
1992.JANUS: Speech-to-Speech Translation UsingConnectionist and Non-Connectionist Tech-niques.
In J.E.
Moody, S.J.
Hanson, and R.P.Lippman, editors, Advances in Neural Infor-mation Processing Systems 4.
Morgan Kauf-mann.Amy Weinberg, Joseph Garman, Jeffery Mar-tin, and Paola Merlo.
1995.
Principle-BasedParser for Foreign Language Training inGerman and Arabic.
In Melissa Holland,Jonathan Kaplan, and Michelle Sams, ed-itors, Intelligent Language Tutors: TheoryShaping Technology, pages 23-44.
LawrenceErlbaum Associates, Hillsdale, NJ.Stanley M. Whitley.
1986.
Spanish/EnglishContrasts: A Course in Spanish Linguistics.Georgetown University Press, Washington,D.C.31
