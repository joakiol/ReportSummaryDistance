Proceedings of the Fourth Workshop on Statistical Machine Translation , pages 233?241,Athens, Greece, 30 March ?
31 March 2009. c?2009 Association for Computational LinguisticsA Deep Learning Approach to Machine TransliterationThomas Deselaers and Sas?a Hasan and Oliver Bender and Hermann NeyHuman Language Technology and Pattern Recognition Group ?
RWTH Aachen University<surname>@cs.rwth-aachen.deAbstractIn this paper we present a novel translit-eration technique which is based on deepbelief networks.
Common approachesuse finite state machines or other meth-ods similar to conventional machine trans-lation.
Instead of using conventional NLPtechniques, the approach presented herebuilds on deep belief networks, a tech-nique which was shown to work well forother machine learning problems.
Weshow that deep belief networks have cer-tain properties which are very interestingfor transliteration and possibly also fortranslation and that a combination withconventional techniques leads to an im-provement over both components on anArabic-English transliteration task.1 IntroductionTransliteration, i.e.
the transcription of words suchas proper nouns from one language into another or,more commonly from one alphabet into another, isan important subtask of machine translation (MT)in order to obtain high quality output.We present a new technique for transliterationwhich is based on deep belief networks (DBNs),a well studied approach in machine learning.Transliteration can in principle be considered to bea small-scale translation problem and, thus, someideas presented here can be transferred to the ma-chine translation domain as well.Transliteration has been in use in machine trans-lation systems, e.g.
Russian-English, since the ex-istence of the field of machine translation.
How-ever, to our knowledge it was first studied as amachine learning problem by Knight and Graehl(1998) using probabilistic finite-state transducers.Subsequently, the performance of this system wasgreatly improved by combining different spellingand phonetic models (Al-Onaizan and Knight,2002).
Huang et al (2004) construct a proba-bilistic Chinese-English edit model as part of alarger alignment solution using a heuristic boot-strapped procedure.
Freitag and Khadivi (2007)propose a technique which combines conventionalMT methods with a single layer perceptron.In contrast to these methods which stronglybuild on top of well-established natural languageprocessing (NLP) techniques, we propose an al-ternative model.
Our new model is based on deepbelief networks which have been shown to workwell in other machine learning and pattern recog-nition areas (cf.
Section 2).
Since translation andtransliteration are closely related and translitera-tion can be considered a translation problem on thecharacter level, we discuss various methods fromboth domains which are related to the proposedapproach in the following.Neural networks have been used in NLP inthe past, e.g.
for machine translation (Asuncio?nCastan?o et al, 1997) and constituent parsing(Titov and Henderson, 2007).
However, it mightnot be straight-forward to obtain good results us-ing neural networks in this domain.
In general,when training a neural network, one has to choosethe structure of the neural network which involvescertain trade-offs.
If a small network with no hid-den layer is chosen, it can be efficiently trainedbut has very limited representational power, andmay be unable to learn the relationships betweenthe source and the target language.
The DBN ap-proach alleviates some of the problems that com-monly occur when working with neural networks:1. they allow for efficient training due to a goodinitialisation of the individual layers.
2.
Overfit-ting problems are addressed by creating generativemodels which are later refined discriminatively.
3.The network structure is clearly defined and only afew structure parameters have to be set.
4.
DBNscan be interpreted as Bayesian probabilistic gener-ative models.Recently, Collobert and Weston (2008) pro-posed a technique which applies a convolutionalDBN to a multi-task learning NLP problem.
Theirapproach is able to address POS tagging, chunk-ing, named entity tagging, semantic role and simi-lar word identification in one model.
Our model issimilar to this approach in that it uses the same ma-chine learning techniques but the encoding and the233processing is done differently.
First, we learn twoindependent generative models, one for the sourceinput and one for the target output.
Then, thesetwo models are combined into a source-to-targetencoding/decoding system (cf.
Section 2).Regarding that the target is generated and notsearched in a space of hypotheses (e.g.
in a wordgraph), our approach is similar to the approachpresented by Bangalore et al (2007) who presentan MT system where the set of words of the tar-get sentence is generated based on the full sourcesentence and then a finite-state approach is used toreorder the words.
Opposed to this approach wedo not only generate the letters/words in the targetsentence but we generate the full sentence with or-dering.We evaluate the proposed methods on anArabic-English transliteration task where Arabiccity names have to be transcribed into the equiva-lent English spelling.2 Deep Belief Networks forTransliterationAlthough DBNs are thoroughly described in theliterature, e.g.
(Hinton et al, 2006), we give a shortoverview on the ideas and techniques and intro-duce our notation.Deep architectures in machine learning and ar-tificial intelligence are becoming more and morepopular after an efficient training algorithm hasbeen proposed (Hinton et al, 2006), although theidea is known for some years (Ackley et al, 1985).Deep belief networks consist of multiple layers ofrestricted Boltzmann machines (RBMs).
It wasshown that DBNs can be used for dimensionalityreduction of images and text documents (Hintonand Salakhutdinow, 2006) and for language mod-elling (Mnih and Hinton, 2007).
Recently, DBNswere also used successfully in image retrieval tocreate very compact but meaningful representa-tions of a huge set of images (nearly 13 million)for retrieval (Torralba et al, 2008).DBNs are built from RBMs by first training anRBM on the input data.
A second RBM is builton the output of the first one and so on until asufficiently deep architecture is created.
RBMsare stochastic generative artificial neural networkswith restricted connectivity.
From a theoreticalviewpoint, RBMs are interesting because they areable to discover complex regularities and find no-table features in data (Ackley et al, 1985).Figure 1: A schematic representation of our DBNfor transliteration.Hinton and Salakhutdinow (2006) present adeep belief network to learn a tiny representationof its inputs and to reconstruct the input with highaccuracy which is demonstrated for images andtextual documents.
Here, we use DBNs similarly:first, we learn encoders for the source and tar-get words respectively and then connect these twothrough a joint layer to map between the two lan-guages.
This joint layer is trained in the same wayas the top-level neurons in the deep belief classi-fier from (Hinton et al, 2006).In Figure 1, a schematic view of our DBN fortransliteration is shown.
On the left and on theright are encoders for the source and target wordsrespectively.
To transliterate a source word, it ispassed through the layers of the network.
First, ittraverses through the source encoder on the left,then it passes into the joint layer, finally travers-ing down through the target encoder.
Each layerconsists of a set of neurons receiving the outputof the preceding layer as input.
The first layers inthe source and target encoders consist of S1 andT1 neurons, respectively; the second layers haveS2 and T2 nodes, and the third layers have S3 andT3 nodes, respectively.
A joint layer with J nodesconnects the source and the target encoders.Here, the number of nodes in the individual lay-ers are the most important parameters.
The more234nodes a layer has, the more information can beconveyed through it, but the harder the training:the amount of data needed for training and thusthe computation time required is exponential in thesize of the network (Ackley et al, 1985).To transliterate a source word, it is first encodedas a DF -dimensional binary vector SF (cf.
Sec-tion 2.1) and then fed into the first layer of thesource encoder.
The S1-dimensional output vec-tor OS1 of the first layer is computed asOS1 ?
1/ exp (1 + wS1SF + bS1) , (1)where wS1 is a S1 ?DF -dimensional weight ma-trix and bS1 is an S1-dimensional bias vector.The output of each layer is used as input to thenext layer as follows:OS2 ?
1/ exp (1 + wS2OS1 + bS2) , (2)OS3 ?
1/ exp (1 + wS3OS2 + bS3) .
(3)After the source encoder has been traversed, thejoint layer is reached which processes the datatwice: once using the input from the source en-coder to get a state of the hidden neurons OSJ andthen to infer an output state OJT as input to thetopmost level of the output encoderOSJ ?
1/ exp (1 + wSJOS3 + bSJ) , (4)OJT ?
1/ exp (1 + wJTOSJ + bJT ) .
(5)This output vector is decoded by traversing down-wards through the output encoder:OT3 ?
1/ exp (1 + wT3OJT + bT3) , (6)OT2 ?
1/ exp (1 + wT2OT3 + bT2) , (7)OT1 ?
wT1OT2 + bT1, (8)where OT1 is a vector encoding a word in the tar-get language.Note that this model is intrinsically bidirec-tional since the individual RBMs are bidirectionalmodels and thus it is possible to transliterate fromsource to target and vice versa.2.1 Source and Target EncodingA problem with DBNs and transliteration is thedata representation.
The input and output data arecommonly sequences of varying length but a DBNexpects input data of constant length.
To repre-sent a source or target language word, it is con-verted into a sparse binary vector of dimensional-ity DF = |F | ?
J or DE = |E| ?
I , respectively,where |F | and |E| are the sizes of the alphabetsand I and J are the lengths of the longest words.If a word is shorter than this, a padding letter w0is used to fill the spaces.
This encoding is depictedin the bottom part of Figure 1.Since the output vector of the DBN is not bi-nary, we infer the maximum a posterior hypothe-sis by selecting the letter with the highest outputvalue for each position.2.2 Training MethodFor the training, we follow the method proposedin (Hinton et al, 2006).
To find a good startingpoint for backpropagation on the whole network,each of the RBMs is trained individually.
First, welearn the generative encoders for the source andtarget words, i.e.
the weights wS1 and wT1, respec-tively.
Therefore, each of the layers is trained as arestricted Boltzmann machine, such that it learnsto generate the input vectors with high probability,i.e.
the weights are learned such that the data val-ues have low values of the trained cost function.After learning a layer, the activity vectors of thehidden units, as obtained from the real trainingdata, are used as input data for the next layer.
Thiscan be repeated to learn as many hidden layers asdesired.
After learning multiple hidden layers inthis way, the whole network can be viewed as asingle, multi-layer generative model and each ad-ditional hidden layer improves a lower bound onthe probability that the multi-layer model wouldgenerate the training data (Hinton et al, 2006).For each language, the output of the first layer isused as input to learn the weights of the next lay-ers wS2 and wT2.
The same procedure is repeatedto learn wS3 and wT3.
Note that so far no con-nection between the individual letters in the twoalphabets is created but each encoder only learnsfeature functions to represent the space of possi-ble source and target words.
Then, the weightsfor the joined layer are learned using concatenatedoutputs of the top layers of the source and targetencoders to find an initial set of weights wSJ andwJT .After each of the layers has been trained in-dividually, backpropagation is performed on thewhole network to tune the weights and to learn theconnections between both languages.
We use theaverage squared error over the output vectors be-tween reference and inferred words as the trainingcriterion.
For the training, we split the training235data into batches of 100 randomly selected wordsand allow for 10 training iterations of the individ-ual layers and up to 200 training iterations for thebackpropagation.
Currently, we only optimise theparameters for the source to target direction andthus do not retain the bidirectionality1.Thus, the whole training procedure consists of4 phases.
First, an autoencoder for the sourcewords is learnt.
Second, an autoencoder forthe target words is learnt.
Third, these autoen-coders are connected by a top connecting layer,and finally backpropagation is performed over thewhole network for fine-tuning of the weights.2.3 Creation of n-Best ListsN -best lists are a common means for combinationof several systems in natural language processingand for rescoring.
In this section, we describe howa set of hypotheses can be created for a given in-put.
Although these hypotheses are not n-best listsbecause they have not been obtained from a searchprocess, they can be used similarly and can bet-ter be compared to randomly sampled ?good?
hy-potheses from a full word-graph.Since the values of the nodes in the individ-ual layers are probabilities for this particular nodeto be activated, it is possible to sample a set ofstates from the distribution for the individual lay-ers, which is called Gibbs sampling (Geman andGeman, 1984).
This sampling can be used to cre-ate several hypotheses for a given input sentence,and this set of hypotheses can be used similar toan n-best list.The layer in which the Gibbs sampling is donecan in principle be chosen arbitrarily.
However,we believe it is natural to sample in either the firstlayer, the joint layer, or the last layer.
Sampling inthe first layer leads to different features traversingthe full network.
Sampling in the joint layer onlyaffects the generation of the target sentence, andsampling in the last layer is equal to directly sam-pling from the distribution of target hypotheses.Conventional Gibbs sampling has a very strongimpact on the outcome of the network because thesmoothness of the distributions and the encodingof similar matches is entirely lost.
Therefore, weuse a weak variant of Gibbs sampling.
Instead ofreplacing the states?
probabilities with fully dis-cretely sampled states, we keep the probabilities1Note that it is easily possible to extend the backpropaga-tion to include both directions, but to keep the computationaldemands lower we decided to start with only one direction.and add a fraction of a sampled state, effectivelymodifying the probabilities to give a slightly bet-ter score to the last sampled state.
Let p be theD-dimensional vector of probabilities for D nodesin an RBM to be on.
Normal Gibbs samplingwould sample a D-dimensional vector S contain-ing a state for each node from this distribution.Instead of replacing the vector p with S, we usep?
?
p + ?S, leading to smoother changes thanconventional Gibbs sampling.
This process caneasily be repeated to obtain multiple hypotheses.3 Experimental EvaluationIn this section we present experimental results foran Arabic-English transliteration task.
For evalu-ation we use the character error rate (CER) whichis the commonly used word error rate (WER) oncharacter level.We use a corpus of 10,084 personal names inArabic and their transliterated English ASCII rep-resentation (LDC corpus LDC2005G02).
TheArabic names are written in the usual way, i.e.lacking vowels and diacritics.
1,000 names wererandomly sampled for development and evalua-tion, respectively (Freitag and Khadivi, 2007).The vocabulary of the source language is 33 andthe target language has 30 different characters(including the padding character).
The longestword on both sides consists of 14 characters,thus the feature vector on the source side is 462-dimensional and the feature vector on the targetside is 420-dimensional.3.1 Network StructureFirst, we evaluate how the structure of the networkshould be chosen.
For these experiments, we fixedthe numbers of layers and the size of the bottomlayers in the target and source encoder and evalu-ate different network structures and the size of thejoint layer.The experiments we performed are described inTable 1.
The top part of the table gives the resultsfor different network structures.
We compare net-works with increasing layer sizes, identical layersizes, and decreasing layer sizes.
It can be seenthat decreasing layer sizes leads to the best results.In these experiments, we choose the number ofnodes in the joint layer to be three times as largeas the topmost encoder layers.In the bottom part, we kept most of the networkstructure fixed and only vary the number of nodes236Table 1: Transliteration experiments using differ-ent network structures.number of nodes CER [%]S1,T1 S2,T2 S3,T3 J train dev eval400 500 600 1800 0.3 27.2 28.1400 400 400 1200 0.7 26.1 25.2400 350 300 900 1.8 25.1 24.3400 350 300 1000 1.7 24.8 24.0400 350 300 1500 1.3 24.1 22.7400 350 300 2000 0.2 24.2 23.5in the joint layer.
Here, a small number of nodesleads to suboptimal performance and a very highnumber of nodes leads to overfitting which can beseen in nearly perfect performance on the trainingdata and an increasing CER on the developmentand eval data.3.2 Network SizeNext, we evaluate systems with different numbersof nodes.
Therefore, we start from the best param-eters (400-350-300-1500) from the previous sec-tion and scale the number of nodes in the individ-ual layers by a certain factor, i.e.
factor 1.5 leadsto (600-525-450-2250).In Figure 2 and Table 2, the results from theexperimental evaluation on the transliteration taskare given.
The network size denotes the numberof nodes in the bottom layers of the source and thetarget encoder (i.e.
S1 and T1) and the other layersare chosen according to the results from the exper-iments presented in the previous section.The results show that small networks performbadly, the optimal performance is reached withmedium sized networks of 400-600 nodes in thebottom layers, and larger networks perform worse,which is probably due to overfitting.For comparison, we give results for a state-of-the-art phrase-based MT system applied on thecharacter level with default system parameters (la-belled as ?PBT untuned?
), and the same system,where all scaling factors were tuned on dev data(labelled as ?PBT tuned?).
The tuned phrase-basedMT system clearly outperforms our approach.Additionally, we perform an experiment witha standard multi-layer perceptron.
Therefore, wechoose the network structure with 400-350-300-1500 nodes, initialised these randomly and trainedthe entire network with backpropagation training.00.050.10.150.20.250.30.350.40.4550  200  300  400  500  600  1000CER [%]network sizetraindevtestFigure 2: Results for the Arabic-English translit-eration task depending on the network size.The results (line ?MLP-400?
in Table 2) of this ex-periment are far worse than any of the other re-sults, which shows that, apart from the convenienttheoretical interpretation, the creation of the DBNas described is a suitable method to train the sys-tem.
The reason for the large difference is likelythe bad initialisation of the network and the factthat the backpropagation algorithm gets stuck in alocal optimum at this point.3.3 Reordering capabilitiesAlthough reordering is not an issue in transliter-ation, the proposed model has certain propertieswhich we investigated and where interesting prop-erties can be observed.To investigate the performance under adversereordering conditions, we also perform an exper-iment with reversed ordering of the target letters(i.e.
a word w = c1, c2, .
.
.
, cJ is now writtencJ , cJ?1, .
.
.
, c1).
Since the DBN is fully sym-metric, i.e.
each input node is connected with eachoutput node in the same way and vice versa, theDBN result is not changed except for some minornumerical differences due to random initialisation.Indeed, the DBN obtained is nearly identical ex-cept for a changed ordering of the weights in thejoint layer, and if desired it is possible to constructa DBN for reverse-order target language from afully trained DBN by permuting the weights.On the same setup an experiment with ourphrase-based decoder has been performed andhere the performance is strongly decreased (bot-tom line of Table 2).
The phrase-based MT sys-tem for this experiment used a reordering withIBM block-limit constraints with distortion lim-its and all default parameters were reasonablytuned.
We observed that the position-independent237Table 2: Results for the Arabic-English translit-eration task depending on the network size and acomparison with state of the art results using con-ventional phrase-based machine translation tech-niquesnetwork CER [%]size train dev eval50 35.8 43.7 43.6100 26.4 36.3 35.8200 5.8 25.2 24.3300 3.9 24.3 24.4400 1.3 24.1 22.7500 1.2 22.9 22.8600 1.0 24.1 22.61000 0.2 26.6 24.4MLP-400 22.0 64.1 63.2untuned PBT 4.9 23.3 23.6tuned PBT 2.2 12.9 13.3(Freitag and Khadivi, 2007) n/a 11.1 11.1reversed task: PBT 13.0 35.2 35.7error rate of the phrase-based MT system is hardlychanged which also underlines that, in principle,the phrase-based MT system is currently better butthat under adverse reordering conditions the DBNsystem has some advantages.3.4 N-Best ListsAs described above, different possibilities to cre-ate n-best lists exists.
Starting from the systemwith 400-350-300-1500 nodes, we evaluate thecreation of n-best lists in the first source layer, thejoint layer, and the last target layer.
Therefore,we create n best lists with up to 10 hypotheses(sometimes, we have less due to duplicates aftersampling, on the average we have 8.3 hypothesesper sequence), and evaluate the oracle error rate.In Table 3 it can be observed that sampling in thefirst layer leads to the best oracle error rates.
Thebaseline performance (first best) for this system is24.1% CER on the development data, and 22.7%CER on the eval data, which can be improved bynearly 10% absolute using the oracle from a 10-best list.3.5 RescoringUsing the n-best list sampled in the first sourcelayer, we also perform rescoring experiments.Therefore, we rescore the transliteration hypothe-Table 3: Oracle character error rates on 10-bestlists.sampling layer oracle CER [%]dev evalS1 15.8 14.8joint layer 17.5 16.4T1 18.7 18.2CER [%]System dev evalDBN w/o rescoring 24.1 22.7w/ rescoring 21.3 20.1Table 4: Results from the rescoring experimentsand fusion with the phrase-based MT system.ses (after truncating the padding letters w0) withadditional models, which are commonly used inMT, and which we have trained on the trainingdata:?
IBM model 1 lexical probabilities modellingthe probability for a target sequence given asource sequencehIBM1(fJ1 , eI1)=?
log?
?1(I + 1)JJ?j=1I?i=0p(fj |ei)???
m-gram language model over the letter se-quenceshLM(eI1) = ?
logI?i=1p(ei|ei?1i?m+1),with m being the size of the m-gram, wechoose m = 9.?
sequence length model (commonly referredto as word penalty).Then, these models are fused in a log-linear model(Och and Ney, 2002), and we tune the model scal-ing factors discriminatively on the development n-best list using the downhill simplex algorithm.
Re-sults from the rescoring experiments are given inTable 4.The performance of the DBN system is im-proved on the dev data from 24.1% to 21.3% CERand on the eval data from 22.7% to 20.1% CER.2383.6 Application Within a SystemCombination FrameworkAlthough being clearly outperformed by thephrase-based MT system, we applied the translit-eration candidates generated by the DBN ap-proach within a system combination framework.Motivated by the fact that the DBN approachdiffers decisively from the other statistical ap-proaches we applied to the machine transliterationtask, we wanted to investigate the potential ben-efit of the diverse nature of the DBN transliter-ations.
Taking the transliteration candidates ob-tained from another study which was intended toperform a comparison of various statistical ap-proaches to the transliteration task, we performedthe system combination as is customary in speechrecognition, i.e.
following the Recognizer OutputVoting Error Reduction (ROVER) approach (Fis-cus, 1997).The following methods were investigated:(Monotone) Phrase-based MT on character level:A state-of-the-art phrase-based SMT system(Zens and Ney, 2004) was used for nametransliteration, i.e.
translation of charactersinstead of words.
No reordering modelwas employed due to the monotonicityof the transliteration task, and the modelscaling factors were tuned on maximumtransliteration accuracy.Data-driven grapheme-to-phoneme conversion:In Grapheme-to-Phoneme conversion (G2P),or phonetic transcription, we seek the mostlikely pronunciation (phoneme sequence)for a given orthographic form (sequence ofletters).
Then, a grapheme-phoneme jointmulti-gram, or graphone for short, is a pairof a letter sequence and a phoneme sequenceof possibly different length (Bisani and Ney,2008).
The model training is done in twosteps: First, maximum likelihood is used toinfer the graphones.
Second, the input issegmented into a stream of graphones andabsolute discounting with leaving-one-outis applied to estimate the actual M -grammodel.
Interpreting the characters of theEnglish target names as phonemes, we usedthe G2P toolkit of (Bisani and Ney, 2008) totransliterate the Arabic names.Position-wise maximum entropy models / CRFs:The segmentation as provided by the G2Pmodel is used and ?null words?
are insertedsuch that the transliteration task can beinterpreted as a classical tagging task (e.g.POS, conceptual tagging, etc.).
This meansthat we seek for a one-to-one mapping anddefine feature functions to model the pos-terior probability.
Maximum entropy (ME)models are defined position-wise, whereasconditional random fields (CRFs) considerfull sequences.
Both models were trainedaccording to the maximum class posteriorcriterion.
We used an ME tagger (Bender etal., 2003) and the freely available CRF++toolkit.2Results for each of the individual systems anddifferent combinations are given in Table 5.
Asexpected, the DBN transliterations cannot keep upwith the other approaches.
The additional models(G2P, CRF and ME) perform slightly better thanthe PBT method.
If we look at combinations ofsystems without the DBN approach, we observeonly marginal improvements of around 0.1-0.2%CER.
Interestingly, a combination of all 4 models(PBT, G2P, ME, CRF) works as good as individual3-way combinations (the same 11.9% on dev areobtained).
This can be interpreted as a potential?similarity?
of the approaches.
Adding e.g.
ME toa combination of PBT, G2P and CRF does not im-prove results because the transliteration hypothe-ses are too similar.
If we simply put together all5 systems including DBN with equal weights, wehave a similar trend.
Since all systems are equallyweighted and at least 3 of the systems are similarin individual performance (G2P, ME, CRF have allaround 12% CER on the tested data sets), the DBNapproach does not get a large impact on overallperformance.If we drop similar systems and tune for 3-waycombinations, we observe a large reduction inCER if DBN comes into play.
Compared to thebest individual system of 12% CER, we now ar-rive at a CER of 10.9% for a combination of PBT,CRF and DBN which is significantly better thaneach of the individual methods.
Our interpreta-tion of this is that the DBN system has differenthypotheses compared to all other systems and thatthe hypotheses from the other systems are too sim-ilar to be apt for combination.
So, although DBNis much worse than the other approaches, it obvi-ously helps in the system combination.
Using therescored variant of the DBN transliterations from2http://crfpp.sourceforge.net/239CER [%]System dev evalDBN 24.1 22.7PBT 12.9 13.3G2P 12.2 12.1ME 12.3 12.4CRF 12.0 12.0ROVERbest setting w/o DBN 11.9 11.85-way equal weights 11.7 11.9best setting w/ DBN 10.9 10.9Table 5: Results from the individual methods in-vestigated versus ROVER combination.Section 3.5, performance is similar to the one ob-tained for the DBN baseline.4 Discussion and ConclusionWe have presented a novel method for machinetransliteration based on DBNs, which despite nothaving competitive results can be an important ad-ditional cue for system combination setups.
TheDBN model has some immediate advantages: themodel is in principle fully bidirectional and isbased on sound and valid theories from machinelearning.
Instead of common techniques whichare based on finite-state machines or phrase-basedmachine translation, the proposed system does notrely on word alignments and beam-search decod-ing and has interesting properties regarding the re-ordering of sequences.
We have experimentallyevaluated the network structure and size, reorder-ing capabilities, the creation of multiple hypothe-ses, and rescoring and combination with othertransliteration approaches.
It was shown that, al-beit the approach cannot compete with the cur-rent state of the art, deep belief networks mightbe a learning framework with some potential fortransliteration.
It was also shown that the pro-posed method is suited for combination with dif-ferent state-of-the-art systems and that improve-ments over the single models can be obtainedin a ROVER-like setting.
Furthermore, addingDBN-based transliterations, although individuallyfar behind the other approaches, significantly im-proves the overall results by 1% absolute.OutlookIn the future we plan to investigate several detailsof the proposed model: we will exploit the inher-ent bidirectionality, further investigate the struc-ture of the model, such as the number of layersand the numbers of nodes in the individual lay-ers.
Also, it is important to improve the efficiencyof our implementation to allow for working onlarger datasets and obtain more competitive re-sults.
Furthermore, we are planning to investigateconvolutional input layers for transliteration anduse a translation approach analogous to the oneproposed by Collobert and Weston (2008) in or-der to allow for the incorporation of reorderings,language models, and to be able to work on largertasks.Acknowledgement.
We would like to thank Ge-offrey Hinton for providing the Matlab Code ac-companying (Hinton and Salakhutdinow, 2006).ReferencesD.
Ackley, G. Hinton, and T. Sejnowski.
1985.
Alearning algorithm for Boltzmann machines.
Cog-nitive Science, 9(1):147?169.Y.
Al-Onaizan and K. Knight.
2002.
Machinetransliteration of names in Arabic text.
In ACL2002 Workshop on Computationaal Approaches toSemitic Languages.M.
Asuncio?n Castan?o, F. Casacuberta, and E. Vidal.1997.
Machine translation using neural networksand finite-state models.
In Theoretical and Method-ological Issues in Machine Translation (TMI), pages160?167, Santa Fe, NM, USA, July.S.
Bangalore, P. Haffner, and S. Kanthak.
2007.
Sta-tistical machine translation through global lexicalselection and sentence reconstruction.
In AnnualMeeting of the Association for Computational Lin-gustic (ACL), pages 152?159, Prague, Czech Repub-lic.O.
Bender, F. J. Och, and H. Ney.
2003.
Maxi-mum entropy models for named entity recognition.In Proc.
7th Conf.
on Computational Natural Lan-guage Learning (CoNLL), pages 148?151, Edmon-ton, Canada, May.M.
Bisani and H. Ney.
2008.
Joint-sequence modelsfor grapheme-to-phoneme conversion.
Speech Com-munication, 50(5):434?451, May.R.
Collobert and J. Weston.
2008.
A unified architec-ture for natural language processing: Deep neuralnetworks with multitask learning.
In InternationalConference on Machine Learning, Helsinki, Finn-land, July.J.
Fiscus.
1997.
A post-processing system to yield re-duced word error rates: Recognizer output voting er-ror reduction (ROVER).
In IEEE Automatic SpeechRecognition and Understanding Workshop (ASRU),pages 347?354, Santa Barbara, CA, USA, Decem-ber.240D.
Freitag and S. Khadivi.
2007.
A sequence align-ment model based on the averaged perceptron.
InConference on Empirical methods in Natural Lan-guage Processing, pages 238?247, Prague, CzechRepublic, June.S.
Geman and D. Geman.
1984.
Stochastic relaxation,Gibbs distributions, and the Bayesian restoration ofimages.
IEEE Transaction on Pattern Analysis andMachine Intelligence, 6(6):721?741, November.G.
Hinton and R. R. Salakhutdinow.
2006.
Reduc-ing the dimensionality of data with neural networks.Science, 313:504?507, July.G.
Hinton, S. Osindero, and Y.-W. Teh.
2006.
Afast learning algorithm for deep belief nets.
NeuralComputation, 18:1527?1554.F.
Huang, S. Vogel, and A. Waibel.
2004.
Improvingnamed entity translation combining phonetic and se-mantic similarities.
In HLT-NAACL.K.
Knight and J. Graehl.
1998.
Machine translitera-tion.
Computational Linguistics, 24(2).A.
Mnih and G. Hinton.
2007.
Three new graphicalmodels for statistical language modelling.
In ICML?07: International Conference on Machine Learn-ing, pages 641?648, New York, NY, USA.
ACM.F.
Och and H. Ney.
2002.
Discriminative trainingand maximum entropy models for statistical ma-chine translation.
In Annual Meeting of the As-soc.
for Computational Linguistics, pages 295?302,Philadelphia, PA, USA, July.I.
Titov and J. Henderson.
2007.
Constituent parsingwith incremental sigmoid belief networks.
In Pro-ceedings of the 45th Annual Meeting of the Associ-ation of Computational Linguistics, pages 632?639,Prague, Czech Republic, June.A.
Torralba, R. Fergus, and Y. Weiss.
2008.
Smallcodes and large image databases for recognition.
InIEEE Conference on Computer Vision and PatternRecognition, Anchorage, AK, USA, June.R.
Zens and H. Ney.
2004.
Improvements in phrase-based statistical machine translation.
In Proceed-ings of the Human Language Technology Confer-ence of the North American Chapter of the Associ-ation for Computational Linguistics: HLT-NAACL2004, pages 257?264, Boston, MA, May.241
