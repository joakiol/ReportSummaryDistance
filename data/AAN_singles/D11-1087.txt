Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 941?948,Edinburgh, Scotland, UK, July 27?31, 2011. c?2011 Association for Computational LinguisticsEntire Relaxation Path for Maximum Entropy ProblemsMoshe DubinerGooglemoshe@google.comYoram SingerGooglesinger@google.comAbstractWe discuss and analyze the problem of find-ing a distribution that minimizes the relativeentropy to a prior distribution while satisfyingmax-norm constraints with respect to an ob-served distribution.
This setting generalizesthe classical maximum entropy problems asit relaxes the standard constraints on the ob-served values.
We tackle the problem by in-troducing a re-parametrization in which theunknown distribution is distilled to a singlescalar.
We then describe a homotopy betweenthe relaxation parameter and the distributioncharacterizing parameter.
The homotopy alsoreveals an aesthetic symmetry between theprior distribution and the observed distribu-tion.
We then use the reformulated problem todescribe a space and time efficient algorithmfor tracking the entire relaxation path.
Ourderivations are based on a compact geomet-ric view of the relaxation path as a piecewiselinear function in a two dimensional spaceof the relaxation-characterization parameters.We demonstrate the usability of our approachby applying the problem to Zipfian distribu-tions over a large alphabet.1 IntroductionMaximum entropy (max-ent) models and its dualcounterpart, logistic regression, is a popular and ef-fective tool in numerous natural language process-ing tasks.
The principle of maximum entropy wasspelled out explicitly by E.T.
Jaynes (1968).
Ap-plications of maximum entropy approach to naturallanguage processing are numerous.
A notable ex-ample and probably one of the earliest usages andgeneralizations of the maximum entropy principleto language processing is the work of Berger, DellaPietra?2, and Lafferty (Berger et al, 1996, DellaPietra et al, 1997).
The original formulation ofmax-ent cast the problem as the task of finding thedistribution attaining the highest entropy subject toequality constraints.
While this formalism is aes-thetic and paves the way to a simple dual in the formof a unique Gibbs distribution (Della Pietra et al,1997), it does not provide sufficient tools to dealwith input noise and sparse representation of thetarget Gibbs distribution.
To mitigate these issues,numerous relaxation schemes of the equality con-straints have been proposed.
A notable recent workby Dudik, Phillips, and Schapire (2007) provided ageneral constraint-relaxation framework.
See alsothe references therein for an in depth overview ofother approaches and generalizations of max-ent.The constraint relaxation surfaces a natural param-eter, namely, a relaxation value.
The dual form ofthis free parameter is the regularization value of pe-nalized logistic regression problems.
Typically thisparameter is set by experimentation using cross val-idation technique.
The relaxed maximum-entropyproblem setting is the starting point of this paper.In this paper we describe and analyze a frame-work for efficiently tracking the entire relaxationpath of constrained max-ent problems.
We start inSec.
2 with a generalization in which we discuss theproblem of finding a distribution that minimizes therelative entropy to a given prior distribution whilesatisfying max-norm constraints with respect to anobserved distribution.
In Sec.
3 we tackle the prob-lem by introducing a re-parametrization in which the941unknown distribution is distilled to a single scalar.We next describe in Sec.
4 a homotopy between therelaxation parameter and the distribution character-izing parameter.
This formulation also reveals anaesthetic symmetry between the prior distributionand the observed distribution.
We use the reformu-lated problem to describe in Secs.
5-6 space and timeefficient algorithms for tracking the entire relaxationpath.
Our derivations are based on a compact ge-ometric view of the relaxation path as a piecewiselinear function in a two dimensional space of therelaxation-characterization parameters.
In contrastto common homotopy methods for the Lasso Os-borne et al (2000), our procedure for tracking themax-ent homotopy results in an uncharacteristicallylow complexity bounds thus renders the approachapplicable for large alphabets.
We provide prelim-inary experimental results with Zipf distributions inSec.
8 that demonstrate the merits of our approach.Finally, we conclude in Sec.
9 with a brief discus-sion of future directions.2 Notations and Problem SettingWe denote vectors with bold face letters, e.g.
v.Sums are denoted by calligraphic letters, e.g.
M =?j mj .
We use the shorthand [n] to denote the setof integers {1, .
.
.
, n}.
The n?th dimensional sim-plex, denoted ?, consists of all vectors p such that,?nj=1 pj = 1 and for all j ?
[n], pj ?
0.
We gen-eralize this notion to multiplicity weighted vectors.Formally, we say that a vector p with multiplicity mis in the simplex, (p,m) ?
?, if ?nj=1 mjpj = 1,and for all j ?
[n], pj ?
0, and mj ?
0.The generalized relaxed maximum-entropy prob-lem is concerned with obtaining an estimate p, givena prior distribution u and an observed distribution qsuch that the relative entropy between p and u is assmall as possible while p and q are within a givenmax-norm tolerance.
Formally, we cast the follow-ing constrained optimization problem,minpn?j=1mjpj log(pjuj), (1)such that (p,m) ?
?
; ?p ?
q??
?
1/?.
Thevectors u and q are dimensionally compatible withp, namely, (q,m) ?
?
and (u,m) ?
?.
The scalar?
is a relaxation parameter.
We use 1/?
rather than?
itself for reasons that become clear in the sequel.We next describe the dual form of (1).
We derivethe dual by introducing Lagrange-Legendre multi-pliers for each of the constraints appearing in (1).Let ?+j ?
0 denote the multiplier for the constraintqj ?
pj ?
1/?
and ?
?j ?
0 the multiplier for theconstraint qj ?
pj ?
?1/?.
In addition, we use ?
asthe multiplier for the constraint?j mjpj = 1. ftersome routine algebraic manipulations we get that theLagrangian is,?nj=1 mi(pj log(pjuj)+ ?j(qj ?
pj) + |?j |?
)+ ?
(?nj=1 mjpj ?
1).
(2)To find the dual form we take the partial derivativeof the Lagrangian with respect to each pj , equate tozero, and get that log(pjuj)+1?
?j +?
= 0, whichimplies that pj ?
uje?j .
We now employ the factthat (p,m) ?
?
to get that the exact form for pj ispj =uje?j?ni=1 miuie?i.
(3)Using (3) in the compact form of the Lagrangian weobtain the following dual problemmax????
?log (Z)?n?j=1mjqj?j +n?j=1mj?
|?j |???
,(4)where Z = ?nj=1mjuje?j .
We make rather littleuse of the dual form of the problem.
However, thecomplementary slackness conditions that are neces-sary for optimality to hold play an important role inthe next section in which we present a reformulationof the relaxed maximum entropy problem.3 Problem ReformulationFirst note that the primal problem is a strictly con-vex function over a compact convex domain.
Thus,its optimum exists and is unique.
Let us now charac-terize the form of the solution.
We partition the setof indices in [n] into three disjoint sets depending onwhether the constraint |pj ?
qj| ?
1/?
is active andits form.
Concretely, we defineI?
= {1 ?
j ?
n | pj = qj ?
1/?
}I0 = {1 ?
j ?
n | |pj ?
qj| < 1/?}
(5)I+ = {1 ?
j ?
n | pj = qj + 1/?}
.942F(1,1)(-1,-1)Figure 1: The capping function F .Recall that Z =?nj=1 mjuje?j .
Thus, from(3) we can rewrite pj = uje?j/Z .
We next usethe complementary slackness conditions (see for in-stance (Boyd and Vandenberghe, 2004)) to furthercharacterize the solution.
For any j ?
I?
we musthave ?
?j = 0 and ?+j ?
0 therefore ?j ?
0, whichimmediately implies that pj ?
uj/Z .
By definitionwe have that pj = qj ?
1/?
for j ?
I?.
Combin-ing these two facts we get that uj/Z ?
qj ?
1/?
forj ?
I?.
Analogous derivation yields that uj/Z ?qj + 1/?
for j ?
I+.
Last, if the set I0 is not emptythen for each j in I0 we must have ?+j = 0 and?
?j = 0 thus ?j = 0.
Resorting again to the def-inition of p from (3) we get that pj = uj/Z forj ?
I0.
Since |pj ?
qj| < 1/?
for j ?
I0 weget that |uj/Z ?
qj| < 1/?.
To recap, there ex-ists Z > 0 such that the optimal solution takes thefollowing form,pj =??
?qj ?
1/?
uj/Z ?
qj ?
1/?uj/Z |uj/Z ?
qj| < 1/?qj + 1/?
uj/Z ?
qj + 1/?.
(6)We next introduce an key re-parametrization,defining ?
= ?/Z .
We also denote by F (?)
thecapping function F (x) = max {?1,min {1, x}}.
Asimple illustration of the capping function is givenin Fig.
1.
Equipped with these definition we canrewrite (6) as follows,pj = qj +1?
F (?uj ?
?qj) .
(7)Given u, q, and ?, the value of ?
can be found byusing?j mjpj =?j mjqj = 1, which impliesG(?, ?)
def=n?j=1mjF (?uj ?
?qj) = 0 .
(8)We defer the derivation of the actual algorithm forcomputing ?
(and in turn p) to the next section.
Inthe meanwhile let us continue to explore the richstructure of the general solution.
Note that ?,u areinterchangeable with ?,q.
We can thus swap theroles of the prior distribution with the observed dis-tribution and obtain an analogous characterization.In the next section we further explore the depen-dence of ?
on ?.
The structure we reveal shortlyserves as our infrastructure for deriving efficient al-gorithms for following the regularization path.4 The function ?(?
)In order to explore the dependency of ?
on ?
let usintroduce the following sumsM =?j?I+mj ?
?j?I?mjU =?j?I0mj ujQ =?j?I0mj qj .
(9)Fixing ?
and using (9), we can rewrite (8) as follows?U ?
?Q+M = 0 .
(10)Clearly, so long as the partition of [n] into the setsI+, I?, I0 is intact, there is a simple linear relationbetween ?
and ?.
The number of possible subsetsI?, I0, I+ is finite.
Thus, the range 0 < ?
< ?decomposes into a finite number of intervals eachof which corresponds to a fixed partition of [n] intoI+, I?, I0.
In each interval ?
is a linear function of?, unless I0 is empty.
Let ??
be the smallest ?
valuefor which I0 is empty.
Let ??
be its corresponding?
value.
If I0 is never empty for any finite value of ?we define ??
= ??
=?.
Clearly, replacing (?, ?
)with (?
?, ??)
for any ?
?
1 and ?
?
??
yieldsthe same feasible solution as I+(??)
= I+(?),I?(??)
= I?(?).
Hence, as far as the original prob-lem is concerned there is no reason to go past ?
?during the process of characterizing the solution.
Werecap our derivation so far in the following lemma.Lemma 4.1 For 0 ?
?
?
?
?, the value of ?
asdefined by (7) is a unique.
Further, the function ?(?
)is a piecewise linear continuous function in ?.
When?
?
??
letting ?
= ???/??
keeps (7) valid.We established the fact that ?(?)
is a piecewise lin-ear function.
The lingering question is how many943linear sub-intervals the function can attain.
To studythis property, we take a geometric view of the planedefined by (?, ?).
Our combinatorial characteriza-tion of the number of sub-intervals makes use of thefollowing definitions of lines in R2,?+j = {(?, ?)
| uj??
qj?
= +1} (11)?
?j = {(?, ?)
| uj??
qj?
= ?1} (12)?0 = {(?, ?)
| ?U ?
?Q+M = 0} , (13)where??
< ?
<?
and j ?
[n].
The next theoremgives an upper bound on the number of linear seg-ments the function ?
() may attain.
While the boundis quadratic in the dimension, for both artificial dataand real data the bound is way too pessimistic.Theorem 4.2 The piecewise linear function ?(?
)consists of at most n2 linear segments for ?
?
R+.Proof Since we showed that that ?(?)
is a piece-wise linear function, it remains to show that ithas at most n2 linear segments.
Consider thetwo dimensional function G(?, ?)
from (8).
The(?, ?)
plane is divided by the 2n straight lines?1, ?2, .
.
.
, ?n, ?
?1, ?
?2, .
.
.
, ?
?n into at most 2n2+1polygons.
The latter property is proved by induc-tion.
It clearly holds for n = 0.
Assume that it holdsfor n ?
1.
Line ?n intersects the previous 2n ?
2lines at no more than 2n ?
2 points, thus splittingat most 2n ?
1 polygons into two separate polygo-nal parts.
Line ?
?n is parallel to ?n, again addingat most 2n ?
1 polygons.
Recapping, we obtain atmost 2(n ?
1)2 + 1 + 2(2n ?
1) = 2n2 + 1 poly-gons, as required per induction.
Recall that ?(?)
islinear inside each polygon.
The two extreme poly-gons where G(?, ?)
= ?
?nj=1 mj clearly disallowG(?, ?)
= 0, hence ?(?)
can have at most 2n2 ?
1segments for ??
< ?
< ?.
Lastly, we use thesymmetry G(??,??)
= ?G(?, ?)
which impliesthat for ?
?
R+ there are at most n2 segments.This result stands in contrast to the Lasso homotopytracking procedure (Osborne et al, 2000), where theworst case number of segments seems to be expo-nential in n. Moreover, when the prior u is uniform,uj = 1/?nj=1 mj for all j ?
[n], the number ofsegments is at most n + 1.
We defer the analysis ofthe uniform case to a later section as the proof stemsfrom the algorithm we describe in the sequel.0 20 40 60 80 10001020304050?
?Figure 2: An illustration of the function ?(?)
for a syn-thetic 3 dimensional example.10 20 30 40 50 60?1.5?1?0.500.511.5?GFigure 3: An illustration of the function G(?)
for a syn-thetic 4 dimensional example and a ?
= 17.5 Algorithm for a Single Relaxation ValueSuppose we are given u,q,m and a specific relax-ation value ??.
How can we find p?
The obviousapproach is to solve the one dimensional monotoni-cally nondecreasing equation G(?)
def= G(?
?, ?)
= 0by bisection.
In this section we present a more effi-cient and direct procedure that is guaranteed to findthe optimal solution p in a finite number of steps.Clearly G(?)
is a piecewise linear function withat most 2n easily computable change points of theslope.
See also Fig.
(5) for an illustration of G(?
).In order to find the slope change points we need tocalculate the point (?, ?j) for all the lines ?
?j where1 ?
j ?
n. Concretely, these values are?j =?q|j| + sign(j)u|j|.
(14)We next sort the above values of ?j and denote theresulting sorted list as ?pi1 ?
?pi2 ?
?
?
?
?
?pi2n .
Forany 0 ?
j ?
2n letMj ,Uj ,Qj be the sums, defined944in (9), for the line segment ?pij?1 < ?
< ?pij (de-noting ?pi0 = ?
?, ?pi2n+1 = ?).
We computethe sums Mj,Uj ,Qj incrementally, starting fromM0 = ?
?ni=1 mi, U0 = Q0 = 0.
Once thevalues of j?1?th sums are known, we can computethe next sums in the sequence as follows,Mj = Mj?1 + m|pij|Uj = Uj?1 ?
sign(?j)m|pij | u|pij |Qj = Qj?1 ?
sign(?j)m|pij | q|pij| .From the above sums we can compute the value ofthe function G(?, ?)
at the end point of the line seg-ment (?pij?1 , ?pij), which is the same as the startpoint of the line segment (?pij , ?pij+1),Gj = Mj?1 + Uj?1 ?j ?Qj?1 ?= Mj + Uj ?j ?Qj ?
.The optimal value of ?
resides in the line segmentfor which G(?)
attains 0.
Such a segment must existsince G0 = M0 = ?
?ni=1 mi < 0 and G2n =?M0 > 0.
Therefore, there exists an index 1 ?j < 2n, where Gj ?
0 ?
Gj+1.
Once we bracketedthe feasible segment for ?, the optimal value of ?
isfound by solving the linear equation (10),?
= (Qj ?
?
Mj) /Uj .
(15)From the optimal value of ?
it is straightforward toconstruct p using (7).
Due to the sorting step, the al-gorithm?s run time is O(n log(n)) and it takes linearspace.
The number of operations can be reduced toO(n) using a randomized search procedure.6 Homotopy TrackingWe now shift gears and focus on the main thrustof this paper, namely, an efficient characterizationof the entire regularization path for the maximumentropy problem.
Since we have shown that theoptimal solution p can be straightforwardly ob-tained from the variable ?, it suffices to efficientlytrack the function ?(?)
as we traverse the plane(?, ?)
from ?
= 0 through the last change pointwhich we denoted as (?
?, ??).
In this sectionwe give an algorithm that traverses ?(?)
by lo-cating the intersections of ?0 with the fixed lines?
?n, ?
?n+1, .
.
.
, ?
?1, ?1, .
.
.
, ?n and updating ?0 af-ter each intersection.More formally, the local homotopy tracking fol-lows the piecewise linear function ?(?
), segment bysegment.
Each segment corresponds to a subset ofthe line ?0 for a given triplet (M,U ,Q).
It is simpleto show that ?
(0) = 0, hence we start with?
= 0, M = 0, U = Q = 1 .
(16)We now track the value of ?
as ?
increases, and therelaxation parameter 1/?
decreases.
The character-ization of ?0 remains intact until ?0 hits one of thelines ?j for 1 ?
|j| ?
n. To find the line intersect-ing ?0 we need to compute the potential intersectionpoints (?j, ?j) = ?0 ?
?j which amounts to calculat-ing ?
?n, ?
?n+1, .
.
.
, ?
?1, ?1, ?2, ?
?
?
, ?n where?j =Mu|j| + Usign(j)Qu|j| ?
Uq|j|.
(17)The lines for which the denominator is zero cor-respond to infeasible intersection and can be dis-carded.
The smallest value ?j which is larger thanthe current traced value of ?
corresponds to the nextline intersecting ?0.While the above description is mathematicallysound, we devised an equivalent intersection in-spection scheme which is more numerically stableand efficient.
We keep track of partition I?, I0, I1through the vector,sj =???
?1 j ?
I?0 j ?
I0+1 j ?
I+.Initially s1 = s2 = ?
?
?
= sn = 0.
What kind ofintersection does ?0 have with ?j?
Recall that QU isthe slope of ?0 whileq|j|u|j| is the slope of ?j .
ThusQU >q|j|u|j| means that the |j|?th constraint is moving?up?
from I?
to I0 or from I0 to I+.
When QU <q|j|u|j|the |j|?th constraint is moving ?down?
from I+ to I0or from I0 to I?.
See also Fig.
4 for an illustrationof the possible transitions between the sets.
For in-stance, the slope of ?(?)
on the bottom left part ofthe figure is larger than the slope the line it inter-sects.
Since this line defines the boundary betweenI?
and I0, we transition from I?
to I0.
We needonly consider 1 ?
|j| ?
n of the following types.Moving ?up?
from I?
to I0 requiress|j| = ?1 j < 0 Qu|j| ?
Uq|j| > 0 .945Figure 4: Illustration of the possible intersections be-tween ?(?)
and ?j and the corresponding transition be-tween the sets I?, I0.Similarly, moving ?down?
from I+ to I0 requiress|j| = 1 j > 0 Qu|j| ?
Uq|j| < 0 .Finally, moving ?up?
or ?down?
from I0 entailss|j| = 0 j(Qu|j| ?
Uq|j|) > 0 .If there are no eligible ?j?s, we have finished travers-ing ?().
Otherwise let index j belong to the thesmallest eligible ?j .
Infinite accuracy guaranteesthat ?j ?
?.
In practice we perform the update?
?
max(?, ?j)M ?
M+ sign(Qu|j| ?
Uq|j|)m|j|U ?
U +(2??s|j|???
1)m|j| u|j|Q ?
Q+(2??s|j|???
1)m|j| q|j|sj ?
sj + sign(Qu|j| ?
Uq|j|) .We are done with the tracking process when I0 isempty, i.e.
for all j sj 6= 0.The local homotopy algorithm takes O(n) mem-ory and O(nk) operations where k is the number ofchange points in the function ?(?).
This algorithmis simple to implement, and when k is relativelysmall it is efficient.
An illustration of the trackingresult, ?(?
), along with the lines ?
?j , that provide ageometrical description of the problem, is given inFig.
5.7 Uniform PriorWe chose to denote the prior distribution as u to un-derscore the fact that in the case of no prior knowl-0 0.2 0.4 0.6 0.8 1 1.2 1.4 1.6 1.8?4?20246810?
?Figure 5: The result of the homotopy tracking for a 4dimensional problem.
The lines ?j for j < 0 are drawn inblue and for j > 0 in red.
The function ?(?)
is drawn ingreen and its change points in black.
Note that althoughthe dimension is 4 the number of change points is rathersmall and does not exceed 4 either in this simple example.edge u is the uniform distribution,u def= uj =( n?i=1mi)?1.In this case the objective function amounts to thenegative entropy and by flipping the sign of the ob-jective we obtain the classical maximum entropyproblem.
The fact that the prior probability is thesame for all possible observations infuses the prob-lem with further structure which we show how toexploit in this section.
Needless to say though thatall the results we obtained thus far are still valid.Let us consider a point (?, ?)
on the boundary be-tween I0 and I+, namely, there exist a line ?+i suchthat,?ui ?
?qi = ?u?
?qi = 1 .By definition, for any j ?
I0 we have?uj ?
?qj = ?u?
?qj < 1 = ?u?
?qi .Thus, qi < qj for all j ?
I0 which implies thatmj u qj > mj u qi .
(18)Summing over j ?
I0 we get thatQu =?j?I0mj qj u >?j?I0mj u qi = Uqi ,hence,qiui= qiu <QU946and we must be moving ?up?
from I0 to I+ when theline ?0 hits ?i.
Similarly we must be moving ?down?from when ?0 hits on the boundary between I0 andI?.
We summarize these properties in the followingtheorem.Theorem 7.1 When the prior distribution u is uni-form, I?(?)
and I+(?)
are monotonically nonde-creasing and I0(?)
is monotonically nonincreasingin ?
> 0 .
Further, the piecewise linear function?(?)
consists of at most n + 1 line segments.The homotopy tracking procedure when the prioris uniform is particularly simple and efficient.
In-tuitively, there is a sole condition which controlsthe order in which indices would enter I?
from I0,which is simply how ?far?
each qi is from u, the sin-gle prior value.
Therefore, the algorithm starts bysorting q.
Let qpi1 > qpi2 > ?
?
?
> qpin denote thesorted vector.
Instead of maintaining the vector ofset indicators s, we merely maintain two indices j?and j+ which designate the size of I?
and I+ thatwere constructed thus far.
Due to the monotonic-ity property of the sets I?
as ?
grows, the two setscan be written as, I?
= {?j | 1 ?
j < j?}
andI+ = {?j | j+ < j ?
n}.
The homotopy track-ing procedure starts as before with ?
= 0,M = 0,U = Q = 1.
We also set j?
= 1 and j+ = n whichby definition imply that I?
are empty and I0 = [n].In each tracking iteration we need to compare onlytwo values which we compactly denote as,??
=Mu ?
UQu ?
Uqpij?.When ??
?
?+ we just encountered a transitionfrom I0 to I?
and as we encroach I?
we performthe updates, ?
?
?
?, M ?M ?
mpij?
, U ?U ?mpij?u, Q?
Q?mpij?qpij?
, j?
?
j?
+ 1.Similarly when ??
> ?+ we perform the updates?
?
?+, M?M + mpij+ , U ?
U ?
mpij+u,Q?
Q ?
mpij+ qpij+ , j+ ?
j+ ?
1.The tracking process stops when j?
> j+ as weexhausted the transitions out of the set I0 which be-comes empty.
Homotopy tracking for a uniformprior takes O(n) memory and O(n log(n)) opera-tions and is very simple to implement.We also devised a global homotopy tracking algo-rithms that requires a priority queue which facilitatesinsertions, deletions, and finding the largest element1000.10.20.30.40.50.60.70.80.91Sample Size / Dimensions#ChangePoints/ DimensionsFigure 6: The number of line-segments in the homotopyas a function of the number of samples used to build theobserved distribution q.in the queue in O(log(n)) time.
The algorithm re-quires O(n) memory and O(n2 log(n)) operations.Clearly, if the number of line segments constituting?(?)
is greater than n log(n) (recall that the upperbound is O(n2)) then the global homotopy proce-dure is faster than the local one.
However, as weshow in Sec.
8, in practice the number of line seg-ments is merely linear and it thus suffices to use thelocal homotopy tracking algorithm.8 Number of line segments in practiceThe focus of the paper is the design and analysisof a novel homotopy method for maximum entropyproblems.
We thus left with relatively little spaceto discuss the empirical aspects of our approach.
Inthis section we focus on one particular experimentalfacet that underscores the usability of our apparatus.We briefly discuss current natural language applica-tions that we currently work on in the next section.The practicality of our approach hinges on thenumber of line segments that occur in practice.
Ourbounds indicate that this number can scale quadrat-ically with the dimension, which would render thehomotopy algorithm impractical when the size of thealphabet is larger than a few thousands.
We there-fore extensively tested the actual number of line seg-ments in the resulting homotopy when u and q areZipf (1949) distributions.
We used an alphabet ofsize 50, 000 in our experiments.
The distribution uwas set to be the Zipf distribution with an offset pa-rameter of 2, that is, ui ?
1/(i + 2).
We defineda ?mother?
distribution for q, denoted q?, which is947a plain Zipf distribution without an offset, namelyq?i ?
1/i.
We then sampled n/2l letters accordingto the distribution q?
where l ?
?3, .
.
.
, 3.
Thus thesmallest sample was n/23 = 6, 250 and the largestsample was n/3?3 = 40, 000.
Based on the samplewe defined the observed distribution q such that qiis proportional to the number of times the i?th let-ter appeared in the sample.
We repeated the process100 times for each sample size and report averageresults.
Note that when the sample is substantiallysmaller than the dimension the observed distributionq tends to be ?simple?
as it consists of many zerocomponents.
In Fig.
6 we depict the average num-ber line segments for each sample size.
When thesample size is one eighth of the dimension we aver-age st most 0.1n line segments.
More importantly,even when the size of the sample is fairly large, thenumber of lines segments is linear in the dimensionwith a constant close to one.
We also performedexperiments with large sample sizes for which theempirical distribution q is very close to the motherdistribution q?.
We seldom found that the number ofline segments exceeds 4n and the mode is around2n.
These findings render our approach usable evenin the very large natural language applications.9 ConclusionsWe presented a novel efficient apparatus for trackingthe entire relaxation path of maximum entropy prob-lems.
We currently study natural language process-ing applications.
In particular, we are in the processof devising homotopy methods for domain adapta-tion Blitzer (2008) and language modeling basedon context tree weighting (Willems et al, 1995).We also examine generalization of our approach inwhich the relative entropy objective is replaced witha separable Bregman (Censor and Zenios, 1997)function.
Such a generalization is likely to distillfurther connections to the other homotopy methods,in particular the least angle regression algorithm ofEfron et al (2004) and homotopy methods for theLasso in general (Osborne et al, 2000).
We also planto study separable Bregman functions in order to de-rive entire path solutions for less explored objectivessuch as the Itakura-Saito spectral distance (Rabinerand Juang, 1993) and distances especially suited fornatural language processing.ReferencesA.L.
Berger, S.A. Della Pietra, and V. J. Della Pietra.A maximum entropy approach to natural lan-guage processing.
Computational Linguistics, 22(1):39?71, 1996.John Blitzer.
Domain Adaptation of Natural Lan-guage Processing Systems.
PhD thesis, Universityof Pennsylvania, 2008.S.
Boyd and L. Vandenberghe.
Convex Optimiza-tion.
Cambridge University Press, 2004.Y.
Censor and S.A. Zenios.
Parallel Optimization:Theory, Algorithms, and Applications.
OxfordUniversity Press, New York, NY, USA, 1997.S.
Della Pietra, V. Della Pietra, and J. Lafferty.
In-ducing features of random fields.
IEEE Trans-actions on Pattern Analysis and Machine Intelli-gence, 5:179?190, 1997.M.
Dud?
?k, S. J. Phillips, and R. E. Schapire.
Maxi-mum entropy density estimation with generalizedregularization and an application to species distri-bution modeling.
Journal of Machine LearningResearch, 8:1217?1260, June 2007.Bradley Efron, Trevor Hastie, Iain Johnstone, andRobert Tibshirani.
Least angle regression.
Annalsof Statistics, 32(2):407?499, 2004.Edwin T. Jaynes.
Prior probabilities.
IEEE Transac-tions on Systems Science and Cybernetics, SSC-4(3):227?241, September 1968.Michael R. Osborne, Brett Presnell, and Berwin A.Turlach.
On the lasso and its dual.
Journalof Computational and Graphical Statistics, 9(2):319?337, 2000.L.
Rabiner and B.H.
Juang.
Fundamentals of SpeechRecognition.
Prentice Hall, 1993.F.
M. J. Willems, Y. M. Shtarkov, and T. J. Tjalkens.The context tree weighting method: basic proper-ties.
IEEE Transactions on Information Theory,41(3):653?664, 1995.George K. Zipf.
Human Behavior and the Principleof Least Effort.
Addison-Wesley, 1949.948
