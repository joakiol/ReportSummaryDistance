The Effects of Language Relatedness on Multilingual Information Re-trieval: A Case Study With Indo-European and Semitic LanguagesPeter A. ChewSandia National LaboratoriesP.
O.
Box 5800, MS 1012Albuquerque, NM 87185-1012, USApchew@sandia.govAhmed AbdelaliNew Mexico State UniversityP.O.
Box 30002, Mail Stop 3CRLLas Cruces, NM 88003-8001, USAahmed@crl.nmsu.eduAbstractWe explore the effects of language related-ness within a multilingual information re-trieval (IR) framework which can be de-ployed to virtually any language, focusingspecifically on Indo-European versus Se-mitic languages.
The Semitic languagespresent unique challenges to IR for a num-ber of reasons, so we set out to answer thequestion of whether cross-language IR forSemitic languages can be boosted by ma-nipulation of the training data (which, inour framework, includes multilingual paral-lel text, some of which is morphologicallyanalyzed).
We attempted three measures toachieve this: first, the inclusion of geneti-cally related (i.e., other Semitic) languagesin the training data; second, the inclusionof non-related languages sharing the samescript, and third, the inclusion of morpho-logical analysis for Semitic languages.
Wefind that language relatedness is a definitefactor in boosting IR precision; script simi-larity can probably be ruled out as a factor;and morphological analysis can be helpful,but ?
perhaps paradoxically ?
not necessar-ily to the languages which are subjected tomorphological analysis.1 IntroductionIn this paper, we consider how related languagesfit into a general framework developed formultilingual cross-language information retrieval(CLIR).
Although this framework can deal withvirtually any language, there are some specialconsiderations which make related languages moreinteresting for exploration.
Taking one example,Semitic languages are distinguished by theircomplex morphology, a characteristic whichpresents challenges to an information retrievalmodel in which terms (usually, separated by whitespace or punctuation) are implicitly treated asindividual units of meaning.
We consider threepossible methods for investigating the phenomena.In all cases, we keep the overall framework thesame but simply make changes to the training data.One method we consider is to augment the train-ing data with text from related languages; we com-pare results obtained from using Semitic languageswith those obtained when non-Semitic languagesare used.
The other two relate to morphologicalanalysis: the second is to replace inflected forms(in just one language, Arabic) with just the root inthe training data; and the third is to remove vowels(again in just one language, Hebrew).The paper is organized as follows.
Section 2 de-scribes our general framework, which is a standardone used for CLIR.
At a high level, section 3 out-lines some of the challenges Semitic languagespresent within the context of our approach.
In sec-tion 4, we compare results from using a number ofdifferent combinations of training data with thesame test data.
Finally, we conclude on our find-ings in section 5.2 The Framework2.1 General descriptionThe framework that we use for IR is multilingualLatent Semantic Analysis (LSA) as described byBerry et al (1994:21, and used by Landauer andLittman (1990) and Young (1994).
A number ofdifferent approaches to CLIR have been proposed;generally, they rely either on the use of a parallelcorpus for training, or translation of the IR query.Either or both of these methods can be based onthe use of dictionaries, although that is not the ap-proach that we use.In the standard multilingual LSA framework, aterm-by-document matrix is formed from a parallelaligned corpus.
Each ?document?
consists of theconcatenation of all the languages, so terms fromall languages will appear in any given document.Thus, if there are K languages, N documents (eachof which is translated into each of the K lan-guages), and T distinct linguistic terms across alllanguages, then the term-by-document matrix is ofdimensions T by N. Each cell in the matrix repre-sents a weighted frequency of a particular term t(in any language) in a particular document n. Theweighting scheme we use is a standard log-entropyscheme in which the weighted frequency xt,n of aparticular term t in a particular document n is givenby:W = log2 (F + 1) ?
(1 + Ht / log2 (N))where F is the raw frequency of t in n, and Ht is ameasure of the entropy of the term across alldocuments.
The last term in the expression above,log2 (N), is the maximum entropy that any termcan have in the corpus, and therefore (1 + Ht / log2(N)) is 1 for the most distinctive terms in the cor-pus, 0 for those which are least distinctive.
Thelog-entropy weighting scheme has been shown tooutperform other schemes such as tf-idf in LSA-based retrieval (see for example Dumais 1991).The sparse term-by-document matrix is sub-jected to singular value decomposition (SVD), anda reduced non-sparse matrix is output.
Generally,we used the output corresponding to the top 300singular values in our experiments.To evaluate the similarity of unseen queries ordocuments (those not in the training set) to oneanother, these documents are tokenized, theweighted frequencies are calculated in the sameway as they were for the training set, and the re-sults are multiplied by the matrices output by theSVD to project the unseen queries/documents intoa ?semantic space?, assigning (in our case) 300-dimensional vectors to each document.
Again, ourapproach to measuring the similarity of one docu-ment to another is a standard one: we calculate thecosine between the respective vectors.For CLIR, the main advantages of an approachlike LSA are that it is by now quite well-understood; the underlying algorithms remain con-stant regardless of which languages are beingcompared; and there is wide scope to use differentsets of training data, providing they exist in paral-lel corpora.
LSA is thus a highly generic approachto CLIR: since it relies only on the ability to token-ize text at the boundaries between words, or moregenerally semantic units, it can be generalized tovirtually all languages.2.2 Training and test dataFor our experiments, the training and test datawere taken from the Bible and Quran respectively.As training data, the Bible lends itself extremelywell to multilingual LSA.
It is highly available inmultiple languages1 (over 80 parallel translationsin 50 languages, mostly public-domain, are avail-able from a single website,www.unboundbible.org); and a very fine-grainedalignment is possible (by verse) (Resnik et al1999,Chew and Abdelali 2007).
Many purpose-builtparallel corpora are biased towards particular lan-guage groups (for example, the European Unionfunds work in CLIR, but it tends to be biased to-wards European languages ?
for example, see Pe-ters 2001).
This is not as true of the Bible, and thefact that it covers a wider range of languages is areflection of the reasons it was translated in thefirst place.The question which is most commonly raisedabout use of the Bible in this way is whether itscoverage of vocabulary from other domains is suf-ficient to allow it to be used as training data formost applications.
Based on a variety of experi-ments we have carried out (see for example Chewet al forthcoming), we believe this need not al-ways be a drawback ?
it depends largely on theintended application.
However, it is beyond ourscope to address this in detail here; it is sufficientto note that for the experiments we describe in thispaper, we were able to achieve perfectly respect-able CLIR results using the Bible as the trainingdata.1 It has proved hard to come by reliable statistics to al-low direct comparison, but the Bible is generally be-lieved to be the world?s most widely translated book.
Atthe end of 2006, it is estimated that there were full trans-lations into 429 languages and partial translations into2,426 languages (Bible Society 2007).As test data, we used the 114 suras (chapters) ofthe Quran, which has also been translated into awide variety of languages.
Clearly, both trainingand  test data have to be available in multiple lan-guages to allow the effectiveness of CLIR to bemeasured in a meaningful way.
For the experi-ments reported in this paper, we limited the testinglanguages to Arabic, English, French, Russian andSpanish (the respective abbreviations AR, EN, FR,RU and ES are used hereafter).
The test data thusamounted to 570 (114 ?
5) documents: a relativelysmall set, but large enough to achieve statisticallysignificant results for our purposes, as will beshown.
In all tests described in this paper, we usethe same test set: thus, although the test documentsall come from a single domain, it is reasonable tosuppose that the comparative results can be gener-alized to other domains.The complete list of languages used for bothtesting and training is given in Table 1.Language Bible -training- Quran -test- Language Family Sub-FamilyAfrikaans Yes No Indo-European Germanic-WestAmharic Yes No Afro-Asiatic Semitic-SouthArabic Yes Yes Afro-Asiatic Semitic-CentralAramaic Yes No Afro-Asiatic Semitic-NorthCzech Yes No Indo-European Slavic-WestDanish Yes No Indo-European Germanic-NorthDutch Yes No Indo-European Germanic-WestEnglish Yes Yes Indo-European Germanic-WestFrench Yes Yes Indo-European ItalicHebrew Yes No Afro-Asiatic Semitic-CentralHungarian Yes No Uralic Finno-UgricJapanese Yes No AltaicLatin Yes No Indo-European ItalicPersian Yes No Indo-European Indo-IranianRussian Yes Yes Indo-European Slavic-EastSpanish Yes Yes Indo-European ItalicTable 1.
Languages used for training and testing2.3 Test methodWe tokenized each of the 570 test documents, ap-plying the weighting scheme described above toobtain a vector of weighted frequencies of eachterm in the document, then multiplying that vectorby U ?
S-1, also as described above.
The result wasa set of projected document vectors in the 300-dimensional LSA space.For some of our experiments, we used a lightstemmer for Arabic (Darwish 2002) to replace in-flected forms in the training data with citationforms.
It is commonly accepted that morphologyimproves IR (Abdou et al 2005, Lavie et al 2004,Larkey et al 2002, Oard and Gey 2002), and it willbe seen that our results generally confirm this.For Hebrew, we used the Westminster Lenin-grad Codex in the training data.
Since this is avail-able for download either with vowels or withoutvowels, no morphological pre-processing was re-quired in this case; we simply substituted one ver-sion for the other in the training data when neces-sary.Various measurements are used for evaluatingIR systems performance (Van Rijsbergen 1979).However, since the aim of our experiments is toassess whether we could identify the correct trans-lation for a given document among a set of possi-bilities in another language (i.e., given the lan-guage of the query and the language of the results),we selected ?precision at 1 document?
as our pre-ferred metric.
This metric represents the proportionof cases, on average, where the translation was re-trieved first.3 Challenges of Semitic languagesThe features which make Semitic languages chal-lenging for information retrieval are generallyfairly well understood: it is probably fair to saythat chief among them is their complex morphol-ogy (for example, ambiguity resulting from diacri-tization, root-and-pattern alternations, and the useof infix morphemes as described in Habash 2004).These challenges can be illustrated by means of astatistical comparison of a portion of our trainingdata (the Gospel of Matthew) as shown in Table 2.Types TokensAfrikaans 2,112 24,729French 2,840 24,438English 2,074 23,503Dutch 2,613 23,099Danish 2,649 21,816Spanish 3,075 21,279Persian 3,587 21,190Hungarian 4,730 18,787Czech 4,236 18,000Russian 4,196 16,826Latin 3,936 16,543Hebrew (Modern) 4,337 14,153Arabic 4,607 13,930Japanese 5,741 13,130Amharic 5,161 12,940TOTAL 55,894 284,363Table 2.
Statistics of parallel texts by languageFrom Table 2, it should be clear that there isgenerally an inverse relationship between the num-ber of types and tokens.
Modern Indo-European(IE) (and particularly Germanic or Italic lan-guages) are at one end of the spectrum, while theSemitic languages (along with Japanese) are at theother.
The statistics separate ?analytic?
languagesfrom ?synthetic?
ones, and essentially illustrate thefact that, thanks to the richness of their morphol-ogy, the Semitic languages pack more information(in the information-theoretic sense) into each termthan the other languages.
Because this results inhigher average entropy per word (in the informa-tion theoretic sense), a challenge is presented toinformation retrieval techniques such as LSAwhich rely on tokenization at word boundaries: it isharder to isolate each ?unit?
of meaning in a syn-thetic language.
The actual effect this has on in-formation retrieval precision will be shown in thenext section.4 Results with LSAThe series of experiments described in this sectionhave the aims of:?
clarifying what effect morphological analysisof the training data has on CLIR precision;?
highlighting the effect on CLIR precision ofadding more languages in training;?
illustrating what the impact is of adding a par-tial translation (text in one language which isonly partially parallel with the texts in the oth-er languages)We choose Arabic as the language of focus inour experiment; specifically for these experiments,we intended to reveal the effect of adding lan-guages from the same group (Semitic) comparedwith that of adding languages of different groups.First, we present results in Table 3 which con-firm that morphological analysis of the trainingdata improves CLIR performance.ES RU FR EN ARwithout morphological analysis of ArabicES 1.0000 0.5614 0.8333 0.7368 0.2895RU 0.4211 1.0000 0.5263 0.7632 0.2632FR 0.7807 0.7018 1.0000 0.8158 0.4035EN 0.7193 0.8158 0.8596 1.0000 0.4825AR 0.5000 0.2807 0.6228 0.5526 1.0000Average precision:Overall 0.677, within IE 0.783, IE-Semitic 0.488with morphological analysis of ArabicES 1.0000 0.6579 0.8772 0.7807 0.4123RU 0.4912 1.0000 0.7193 0.8158 0.3947FR 0.8421 0.7719 1.0000 0.8421 0.3772EN 0.8070 0.8684 0.8947 1.0000 0.3684AR 0.3947 0.3509 0.5614 0.4561 1.0000Average precision:Overall 0.707, within IE 0.836, IE-Semitic 0.480Table 3.
Effect of morphological analysis2An important point to note first is that CLIRprecision is generally much lower for pairs includ-ing Arabic than it is elsewhere, lending support toour assertion above that Arabic and other Semiticlanguages present special challenges in informa-tion retrieval.It also emerges from Table 3 that when morpho-logical analysis of Arabic was added, the overallaverage precisions increased from 0.677 to 0.707, ahighly significant increase (p?
6.7 ?
10-8).
(Hereand below, a chi-squared test is used to measurestatistical significance.
)Given that the ability of morphological analysisto improve IR precision has been documented, thisresult in itself is not surprising.
However, it is in-teresting that the net benefit of adding morphologi-cal analysis ?
and just to Arabic within the trainingdata ?
was more or less confined to pairs of non-Semitic languages.
We believe that the explanationis that by adding morphology more relations (liai-2 In this and the following tables, the metric used is pre-cision at 1 document (discussed in section 2.3).sons) are defined in LSA between the words fromdifferent languages.
For language pairs includingArabic, the average precision actually decreasedfrom 0.488 to 0.480 when morphology was added(although this decrease is insignificant).With the same five training languages as used inTable 3, we added Persian.
The results are shownin Table 4.ES RU FR EN ARES 1.0000 0.6140 0.8246 0.7632 0.3246RU 0.5088 1.0000 0.6667 0.7982 0.2281FR 0.8772 0.7368 1.0000 0.8158 0.3947EN 0.8246 0.8333 0.8947 1.0000 0.4035AR 0.4474 0.4386 0.6140 0.5526 1.0000Average precision:Overall 0.702, within IE 0.822, IE-Semitic 0.489Table 4.
Effect on CLIR of adding PersianFirst to note is that the addition of Persian (an IElanguage) led to a general increase in precision forpairs of IE languages (Spanish, Russian, Frenchand English) from 0.783 to 0.822 but no significantchange for pairs including Arabic (0.488 to 0.489).Although Persian and Arabic share the same script,these results confirm that genetic relatedness is amuch more important factor in affecting precision.Chew and Abdelali (2007) show that the resultsof multilingual LSA generally improve as thenumber of parallel translations used in training in-creases.
Our next step here, therefore, is to analyzewhether it makes any difference whether the addi-tional languages are from the same or differentlanguage groups.
In Table 5 we compare the re-sults of adding an IE language (Latin), an Altaiclanguage (Japanese), and another Semitic language(Hebrew) to the training data.
In all three cases, nomorphological analysis of the training data wasperformed.Based on these results, cross-language precisionyielded only very slightly improved results overallby adding Latin or Japanese.
With Japanese, thenet improvement (0.677 to 0.680) was not statisti-cally significant overall, neither was the changesignificant for pairs either including or excludingArabic (0.488 to 0.485 and 0.783 to 0.789 respec-tively).
Note that this is even though Japaneseshares some statistical (although of course not lin-guistic) properties with the Semitic languages, asshown in Table 2.
With Latin, the net overall im-provement (0.677 to 0.699) was barely significant(p ?
0.01) and was insignificant for pairs includingArabic (0.488 to 0.496).
With Hebrew, however,the net improvement was highly significant in allcases (0.677 to 0.718, p ?
3.36 ?
10-6 overall,0.783 to 0.819, p ?
2.20 ?
10-4 for non-Semiticpairs, and 0.488 to 0.538, p ?
1.45 ?
10-3 for pairsincluding Arabic).
We believe that these resultsindicate that there is more value overall in ensuringthat languages are paired with at least one otherrelated language in the training data; our least im-pressive results (with Japanese) were when twolanguages in training (one Semitic and one Altaiclanguage) were ?isolated?.ES RU FR EN ARLatin included in training dataES 1.0000 0.6140 0.8333 0.7456 0.2544RU 0.4737 1.0000 0.6316 0.8246 0.3333FR 0.8596 0.7368 1.0000 0.8333 0.4474EN 0.7719 0.7982 0.8860 1.0000 0.4474AR 0.5088 0.3509 0.6140 0.5088 1.0000Average precision:Overall 0.699, within IE 0.813, IE-Semitic 0.496Japanese included in training dataES 1.0000 0.5789 0.8333 0.7456 0.2895RU 0.4298 1.0000 0.5526 0.7807 0.2719FR 0.7719 0.7368 1.0000 0.8070 0.4035EN 0.7193 0.807 0.8596 1.0000 0.4123AR 0.5088 0.2982 0.614 0.5702 1.0000Average precision:Overall 0.680, within IE 0.789, IE-Semitic 0.485Modern Hebrew (no vowels) in training dataES 1.0000 0.6140 0.8596 0.7807 0.3509RU 0.4561 1.0000 0.6667 0.7719 0.3684FR 0.8509 0.7193 1.0000 0.8684 0.4298EN 0.7632 0.8509 0.9035 1.0000 0.4298AR 0.5263 0.4474 0.6491 0.6404 1.0000Average precision:Overall 0.718, within IE 0.819, IE-Semitic 0.538Table 5.
Effect of language relatedness on CLIRThe next set of results are for a repetition of theprevious three experiments, but this time withmorphological analysis of the Arabic data.
Theseresults are shown in Table 6.As was the case without the additional lan-guages, the overall effect of adding morphologicalanalysis of Arabic is still to increase precision.
Inall three cases, the net improvement for pairs ex-cluding Arabic is highly significant (0.813 to 0.844with Latin, 0.789 to 0.852 with Japanese, and0.819 to 0.850 with Hebrew).
For pairs includingArabic, however, the change is again insignificant.This was a consistent but surprising feature of ourresults, that morphological analysis of Arabic infact appears to benefit non-Semitic languages morethan it benefits Arabic itself, at least with this data-set.
The results might possibly have been differentif we had included other Semitic languages in thetest data, although this appears unlikely as wefound the same phenomenon consistently occur-ring across a wide variety of tests, and regardlessof which languages we used in training.ES RU FR EN ARLatin included in training dataES 1.0000 0.6579 0.8684 0.7456 0.4211RU 0.5614 1.0000 0.7456 0.8509 0.4386FR 0.8421 0.8158 1.0000 0.8509 0.4211EN 0.8421 0.8333 0.8947 1.0000 0.4123AR 0.4123 0.3947 0.5351 0.4825 1.0000Average precision:Overall 0.721, within IE 0.844, IE-Semitic 0.502Japanese included in training dataES 1.0000 0.7544 0.8684 0.8070 0.4211RU 0.4737 1.0000 0.7193 0.8509 0.4123FR 0.8246 0.8596 1.0000 0.8772 0.4211EN 0.8421 0.8596 0.8947 1.0000 0.4035AR 0.3333 0.3509 0.5614 0.4649 1.0000Average precision:Overall 0.720, within IE 0.852, IE-Semitic 0.485Modern Hebrew (no vowels) in training dataES 1.0000 0.7018 0.9035 0.7982 0.4561RU 0.5614 1.0000 0.7105 0.8070 0.4035FR 0.8421 0.8246 1.0000 0.8596 0.4825EN 0.8509 0.8509 0.8947 1.0000 0.4123AR 0.3947 0.4298 0.5351 0.5175 1.0000Average precision:Overall 0.729, within IE 0.850, IE-Semitic 0.514Table 6.
Effect of language relatedness andmorphology on CLIRFor further verification, we explored what wouldhappen if only the Arabic root were included inmorphological analysis.
As already mentioned, forlanguages that combine affixes with the stem, thereis a higher token-to-type ratio.
Omitting the affixfrom the morphological analysis of these languagesreveals the importance of considering the affixesand their contribution to the semantics of a givensentence.
Although LSA is not sentence-structure-aware (as it uses a bag-of-words approach), theimportance of considering the affixes as part of thesentence is very crucial.
The results in Table 7demonstrate clearly that ignoring or over-lookingthe word affixes has a negative effect on the over-all performance of the CLIR system.
When includ-ing only the Arabic stem, a performance degrada-tion is noticeable across all languages, with a lar-ger impact on IE languages.
The results which il-lustrate can be seen by comparing Table 7 withTable 3.ES RU FR EN ARmorphological analysis of Arabic ?Stem only-ES 1.0000 0.5789 0.8070 0.7807 0.3421RU 0.4912 1.0000 0.6842 0.8246 0.1842FR 0.8421 0.7018 1.0000 0.8333 0.4211EN 0.8333 0.8333 0.9211 1.0000 0.4211AR 0.4561 0.4386 0.5702 0.4912 1.0000Average precision:Overall 0.698, within IE 0.821, IE-Semitic 0.481Table 7.
Effect of Using Stem onlyNext, we turn specifically to a comparison of theeffect that different Semitic languages have onCLIR precision.
Here, we compare the resultswhen the sixth language used in training is He-brew, Amharic, or Aramaic.
However, since ourAmharic and Aramaic training data were only par-tially parallel (we have only the New Testament inAmharic, and only portions of the New Testamentin Aramaic), we first considered the effect that par-tial translations have on precision.
Table 8 showsthe results we obtained when only the Hebrew OldTestament (with vowels) was used as the sixth par-allel version.
No morphological analysis was per-formed.ES RU FR EN ARwithout morphological analysis of ArabicES 1.0000 0.6842 0.8421 0.8158 0.3947RU 0.4211 1.0000 0.6228 0.7982 0.4737FR 0.8509 0.7719 1.0000 0.8509 0.4737EN 0.7895 0.8333 0.8684 1.0000 0.4649AR 0.4561 0.3333 0.6404 0.4561 1.0000Average precision:Overall 0.714, within IE 0.822, IE-Semitic 0.521with morphological analysis of ArabicES 1.0000 0.7105 0.9035 0.8333 0.4737RU 0.4649 1.0000 0.7456 0.8333 0.4912FR 0.8421 0.8070 1.0000 0.8860 0.4474EN 0.8772 0.8421 0.9298 1.0000 0.4298AR 0.2719 0.3684 0.5088 0.5000 1.0000Average precision:Overall 0.727, within IE 0.855, IE-Semitic 0.499Table 8.
Effect of partial translation on CLIRAlthough two or more parameters differ fromthose used for Hebrew in Table 5 (a fully-paralleltext in modern Hebrew without vowels, versus apartial text in Ancient Hebrew with vowels), it isworth comparing the two sets of results.
In particu-lar, the reductions in average precision from 0.718to 0.714 and from 0.729 to 0.727 respectively areinsignificant.
Likewise, the changes for pairs withand without Arabic were insignificant.
This ap-pears to show that, at least up to a certain point,even only partially parallel corpora can success-fully be used under our LSA-based approach.
Wenow turn to the results we obtained using Aramaic,with the intention of comparing these to our previ-ous results with Hebrew.ES RU FR EN ARno morphological analysis of ArabicES 1.0000 0.4035 0.8070 0.7368 0.2632RU 0.3509 1.0000 0.5965 0.6579 0.2281FR 0.8421 0.6754 1.0000 0.8246 0.2719EN 0.7018 0.6754 0.8947 1.0000 0.2719AR 0.4825 0.2807 0.4649 0.3947 1.0000Average precision:Overall 0.633, within IE 0.760, IE-Semitic 0.406morphological analysis of ArabicES 1.0000 0.5351 0.8684 0.7719 0.2895RU 0.5175 1.0000 0.6930 0.7807 0.3421FR 0.8947 0.7807 1.0000 0.8684 0.2807EN 0.8070 0.8158 0.9035 1.0000 0.2982AR 0.3509 0.2193 0.3772 0.2895 1.0000Average precision:Overall 0.667, within IE 0.827, IE-Semitic 0.383Table 9.
Effect of Aramaic on CLIRHere, there is a noticeable across-the-board de-crease in precision from the previous results.
Webelieve that this may have more to do with the factthat the Aramaic training data we have is fairlysparse (2,957 verses of the Bible out of a total of31,226, compared with 23,269 out of 31,226 forAncient Hebrew).
It is likely that at some point asthe parallel translation?s coverage drops (some-where between the coverage of the Hebrew and theAramaic), there is a severe hit to the performanceof CLIR.
Accordingly, we discarded Aramaic forfurther tests.Next, we considered the addition of two Semiticlanguages other than Arabic, Modern Hebrew andAmharic, to the training data.
In this case, we per-formed morphological analysis of Arabic.The results appear to show a significant increasein precision for pairs of IE languages and a signifi-cant decrease for cross-language-group cases(those where an IE language is paired with Ara-bic), compared to when just Modern Hebrew wasused in the training data (see the relevant part ofTable 6).
It is not clear why this is the case, but inthis case we believe that it is quite possible that theresults would have been different if more than oneSemitic language had been included in the testdata.ES RU FR EN ARES 1.0000 0.6930 0.8860 0.7719 0.4649RU 0.5000 1.0000 0.7456 0.8684 0.5175FR 0.8772 0.7982 1.0000 0.8772 0.4649EN 0.8684 0.8596 0.9298 1.0000 0.4386AR 0.2632 0.2982 0.4386 0.3947 1.0000Average precision:Overall 0.718, within IE 0.855, IE-Semitic 0.476Table 10.
CLIR with 7 languages (includingModern Hebrew and Amharic)We now come to a rare example where weachieved a boost in precision specifically for Ara-bic.
In this case, we repeated the last experimentbut removed the vowels from the Hebrew text.
Theresults are shown in Table 11.ES RU FR EN ARES 1.0000 0.7018 0.8772 0.8158 0.5088RU 0.5175 1.0000 0.7632 0.8421 0.4825FR 0.8596 0.8246 1.0000 0.8860 0.5351EN 0.8947 0.8158 0.9298 1.0000 0.5088AR 0.2895 0.3772 0.5526 0.5000 1.0000Average precision:Overall 0.739, within IE 0.858, IE-Semitic 0.528Table 11.
Effect of removing Hebrew vowelsAverage precision for pairs including Arabic in-creased from 0.476 to 0.528, an increase whichwas significant (p ?
7.33 ?
10-4), but for other pairsthe change was insignificant.
Since the Arabic textin training did not include vowels, we believe thatthe exclusion of vowels from Hebrew placed thetwo languages on a more common footing, allow-ing LSA, for example, to make associations be-tween Hebrew and Arabic roots which otherwisemight not have been made.
Although Hebrew andArabic do not always share common stems, it canbe seen from Table 2 that the type/token statisticsof Hebrew (without vowels) and Arabic are verysimilar.
The inclusion of Hebrew vowels wouldchange the statistics for Hebrew considerably, in-creasing the number of types (since previously in-distinguishable wordforms would now be listedseparately).
Thus, with the exclusion of Hebrewvowels, there should be more instances where Ara-bic tokens can be paired one-to-one with Hebrewtokens.Finally, in order to confirm our conclusions andto eliminate any doubts about the results obtainedso far, we experimented with more languages.
Weadded Japanese, Afrikaans, Czech, Danish, Dutch,Hungarian and Hebrew in addition to our 5 originallanguages.
Morphological analysis of the Arabictext in training was performed, as in some of theprevious experiments.
The results of these tests areshown in Table 12.ES RU FR EN AR11 languages (original 5 + Japanese, Afrikaans,Czech, Danish, Dutch, and Hungarian)ES 1.0000 0.6754 0.9035 0.7719 0.5526RU 0.4737 1.0000 0.7632 0.8772 0.5175FR 0.8596 0.8070 1.0000 0.8947 0.5088EN 0.8421 0.8684 0.9035 1.0000 0.4912AR 0.3772 0.2632 0.6316 0.4912 1.0000Average precision:Overall 0.739, within IE 0.853, IE-Semitic 0.53712 languages (as above plus Hebrew)ES 1.0000 0.7018 0.8947 0.7719 0.6404RU 0.6667 1.0000 0.7105 0.9123 0.6228FR 0.8772 0.8333 1.0000 0.8421 0.6404EN 0.6667 0.8684 0.9035 1.0000 0.6316AR 0.5877 0.4386 0.5965 0.6491 1.0000Average precision:Overall 0.778, within IE 0.853, IE-Semitic 0.645Table 12.
Effect of further languages on CLIRGenerally, these results confirm the finding ofChew and Abdelali (2007) about adding more lan-guages; doing so enhances the ability to identifytranslations across language boundaries.
Across theboard (for Arabic and other languages), the in-crease in precision gained by adding Afrikaans,Czech, Danish, Dutch and Hungarian is highly sig-nificant (compared to the part of Table 5 whichdeals with Japanese, overall average precision in-creased from 0.680 to 0.739, with p ?
1.17 ?
10-11;for cross-language-group retrieval, from 0.485 to0.537, with p ?
9.31 ?
10-4; for pairs within IE,from 0.789 to 0.853 with p ?
2.81 ?
10-11).
In con-trast with most previous results, however, with thefurther addition of Hebrew, precision was boostedprimarily for Arabic (0.537 to 0.645 with p ?
4.39?
10-13).
From this and previous results, it appearsthat there is no clear pattern to when the additionof a Semitic language in training was beneficial tothe Semitic language in testing.5 Conclusion and future workBased on our results, it appears that althoughclear genetic relationships exist between certainlanguages in our training data, it was less possiblethan we had anticipated to leverage this to our ad-vantage.
We had expected, for example, that byincluding multiple Semitic languages in the train-ing data within an LSA framework, we would havebeen able to improve cross-language informationretrieval results specifically for Arabic.
Perhapssurprisingly, the greatest benefit of including addi-tional Semitic languages in the training data ismost consistently to non-Semitic languages.
Aclear observation is that any additional languagesin training are generally beneficial, and the benefitof additional languages can be considerably greaterthan the benefits of linguistic pre-processing (suchas morphological analysis).
Secondly, it is not nec-essarily the case that cross-language retrieval withArabic is helped most by including other Semiticlanguages, despite the genetic relationship.
Finally,as we expected, we were able to rule out scriptsimilarity (e.g.
between Persian and Arabic) as afactor which might improve precision.
Our resultsappear to demonstrate clearly that language relat-edness is much more important in the training datathan use of the same script.Finally, to improve cross-language retrieval withArabic ?
the most difficult case in the languageswe tested ?
we attempted to ?prime?
the trainingdata by including Arabic morphological analysis.This did lead to a statistically significant improve-ment overall in CLIR, but ?
perhaps paradoxically?
the improvement specifically for cross-languageretrieval with Arabic was negligible in most cases.The only two measures which were successful inboosting precision for Arabic significantly were (1)the inclusion of Modern Hebrew in the trainingdata; and (2) the elimination of vowels in the An-cient Hebrew training data ?
both measures whichwould have placed the training data for the twoSemitic languages (Arabic and Hebrew) on a morecommon statistical footing.
These results appear toconfirm our hypothesis that there is value, withinthe current framework, of ?pairing?
genetically re-lated languages in the training data.
In short, lan-guage relatedness does matter in cross-languageinformation retrieval.6 AcknowledgementSandia is a multiprogram laboratory operated bySandia Corporation, a Lockheed Martin Company,for the United States Department of Energy?s Na-tional Nuclear Security Administration under con-tract DE-AC04-94AL85000.7 ReferencesAbdou, S., Ruck, P., and Savoy, J.
2005.
Evaluation ofStemming, Query Expansion and Manual IndexingApproaches for the Genomic Task.
In Proceedings ofTREC 2005.Berry, M. W., Dumais, S. T., and O?Brien, G. W. 1994.Using Linear Algebra for Intelligent Information Re-trieval.
SIAM: Review, 37, 573-595.Biola University.
2005-2006.
The Unbound Bible.
Ac-cessed at http://www.unboundbible.com/ on February27, 2007.Chew, P. A., and Abdelali, A.
2007.
Benefits of the?Massively Parallel Rosetta Stone?
: Cross-LanguageInformation Retrieval with over 30 Languages, Pro-ceedings of the 45th Annual Meeting of the Associa-tion for Computational Linguistics, ACL 2007.
Pra-gue, Czech Republic, June 23?30, 2007. pp.
872-879.Chew, P. A., Kegelmeyer, W. P., Bader, B. W. and Ab-delali, A.
Forthcoming.
The Knowledge of Good andEvil: Multilingual Ideology Classification withPARAFAC2 and Maching Learning.Chew, P. A., Verzi, S. J., Bauer, T. L., and McClain, J.T.
2006.
Evaluation of the Bible as a Resource forCross-Language Information Retrieval.
Proceedingsof the Workshop on Multilingual Language Re-sources and Interoperability, 68?74.Darwish, K. 2002.
Building a shallow Arabic morpho-logical analyzer in one day.
In Proceedings of theAssociation for Computational Linguistics (ACL-02),40th Anniversary Meeting.
pp.
47-54.Dumais, S. T. 1991.
Improving the Retrieval of Infor-mation from External Sources.
Behavior ResearchMethods, Instruments, and Computers 23 (2), 229-236.Dumais, S. T., Furnas, G. W., Landauer, T. K., Deer-wester, S. and Harshman, R. 1998.
Using Latent Se-mantic Analysis to Improve Access to Textual In-formation.
In CHI?88: Proceedings of the SIGCHIConference on Human Factors in Computing Sys-tems, 281-285.
ACM Press.Frakes, W. B. and Baeza-Yates, R. 1992.
InformationRetrieval: Data Structures and Algorithms.
Prentice-Hall: New Jersey.Habash, N. 2004.
Large Scale Lexeme Based ArabicMorphological Generation.
In Proc.
of TraitementAutomatique du Langage Naturel.Larkey, L., Ballesteros, L. and Connell, M. 2002.
Im-proving Stemming for Arabic Information Retrieval:Light Stemming and Co-Occurrence Analysis.
SIGIR2002, Finland, pp.
275-282.Larkey, L. and Connell, M. 2002.
Arabic InformationRetrieval at Umass in TREC-10.
In Voorhees, E.M.and Harman, D.K.
(eds.
): The Tenth Text RetrievalConference, TREC 2001 NIST Special Publication500-250, pp.
562-570.Lavie, A., Peterson, E., Probst, K., Wintner, S., and Ey-tani, Y.
2004.
Rapid Prototyping of a Transfer-BasedHebrew-to-English Machine Translation System.
InProceedings of the TMI-04.Mathieu, B., Besan?on, R. and Fluhr, C. 2004.
Multilin-gual Document Clusters Discovery.
Recherched?Information Assist?e par Ordinateur (RIAO) Pro-ceedings, 1-10.Oard, D. and Gey, F. 2002.
The TREC 2002 Ara-bic/English CLIR Track, NIST TREC 2002 Proceed-ings, pp.
16-26.Peters, C.
(ed.).
2001.
Cross-Language InformationRetrieval and Evaluation: Workshop of the Cross-Language Evaluation Forum, CLEF 2000.
Berlin:Springer-Verlag.Resnik, P., Olsen, M. B., and Diab, M. 1999.
The Bibleas a Parallel Corpus: Annotating the "Book of 2000Tongues".
Computers and the Humanities, 33, 129-153.Van Rijsbergen, C. 1979.
Information Retrieval (2ndedition).
Butterworth: London.
