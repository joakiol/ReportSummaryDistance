Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 646?655,MIT, Massachusetts, USA, 9-11 October 2010. c?2010 Association for Computational LinguisticsSCFG Decoding Without BinarizationMark Hopkins and Greg LangmeadSDL Language Weaver, Inc.6060 Center Drive, Suite 150Los Angeles, CA 90045{mhopkins,glangmead}@languageweaver.comAbstractConventional wisdom dictates that syn-chronous context-free grammars (SCFGs)must be converted to Chomsky Normal Form(CNF) to ensure cubic time decoding.
For ar-bitrary SCFGs, this is typically accomplishedvia the synchronous binarization technique of(Zhang et al, 2006).
A drawback to this ap-proach is that it inflates the constant factors as-sociated with decoding, and thus the practicalrunning time.
(DeNero et al, 2009) tackle thisproblem by defining a superset of CNF calledLexical Normal Form (LNF), which also sup-ports cubic time decoding under certain im-plicit assumptions.
In this paper, we makethese assumptions explicit, and in doing so,show that LNF can be further expanded toa broader class of grammars (called ?scope-3?)
that also supports cubic-time decoding.By simply pruning non-scope-3 rules from aGHKM-extracted grammar, we obtain bettertranslation performance than synchronous bi-narization.1 IntroductionAt the heart of bottom-up chart parsing (Younger,1967) is the following combinatorial problem.
Wehave a context-free grammar (CFG) rule (for in-stance, S ?
NP VP PP) and an input sentence oflength n (for instance, ?on the fast jet ski of mrsmith?).
During chart parsing, we need to apply therule to all relevant subspans of the input sentence.See Figure 1.
For this particular rule, there are(n+14)application contexts, i.e.
ways to choose the sub-spans.
Since the asymptotic running time of chartparsing is at least linear in this quantity, it will takeon the fast jet ski of mr smithNP VP PPNP VP PPNP VP PPNP VP PPNP VP PPchoice point choice point choice point choice point?
?Figure 1: A demonstration of application contexts.
Thereare(n+14)application contexts for the CFG rule ?S?
NPVP PP?, where n is the length of the input sentence.at least O((n+14)) = O(n4) time if we include thisrule in our grammar.Fortunately, we can take advantage of the fact thatany CFG has an equivalent representation in Chom-sky Normal Form (CNF).
In CNF, all rules havethe form X ?
Y Z or X ?
x, where x is a termi-nal and X, Y, Z are nonterminals.
If a rule has theform X ?
Y Z, then there are only(n+13)applica-tion contexts, thus the running time of chart parsingis O((n+13)) = O(n3) when applied to CNF gram-mars.A disadvantage to CNF conversion is that it in-creases both the overall number of rules and theoverall number of nonterminals.
This inflation ofthe ?grammar constant?
does not affect the asymp-totic runtime, but can have a significant impact onthe performance in practice.
For this reason, (DeN-646the NPB of NNPon the fast jet ski of mr smiththe NPB of NNPon the fast jet ski of mr smiththe JJ NPB of NNPthe JJ NPB of NNPthe JJ NPB of NNPthe JJ NPB of NNPchoice pointchoice point choice pointFigure 2: A demonstration of application contexts forrules with lexical anchors.
There are O(n) applicationcontexts for CFG rule ?S ?
the NPB of NNP?, andO(n2) application contexts for CFG rule ?S ?
the JJNPB of NNP?, if we assume that the input sentence haslength n and contains no repeated words.ero et al, 2009) provide a relaxation of CNF calledLexical Normal Form (LNF).
LNF is a superclass ofCNF that also allows rules whose right-hand sideshave no consecutive nonterminals.
The intuition isthat the terminals provide anchors that limit the ap-plicability of a given rule.
For instance, consider therule NP?
the NPB of NNP.
See Figure 2.
Becausethe terminals constrain our choices, there are onlytwo different application contexts.
The implicit as-sumption is that input sentences will not repeat thesame word more than a small constant number oftimes.
If we make the explicit assumption that allwords of an input sentence are unique, then thereare O(n2) application contexts for a ?no consecu-tive nonterminals?
rule.
Thus under this assumption,the running time of chart parsing is stillO(n3) whenapplied to LNF grammars.But once we make this assumption explicit, it be-comes clear that we can go even further than LNFand still maintain the cubic bound on the runtime.Consider the rule NP ?
the JJ NPB of NNP.
Thisrule is not LNF, but there are still only O(n2) ap-plication contexts, due to the anchoring effect of theterminals.
In general, for a rule of the form X?
?,there are at most O(np) application contexts, wherep is the number of consecutive nonterminal pairs inthe string X ???
X (where X is an arbitrary nontermi-nal).
We refer to p as the scope of a rule.
Thus chartparsing runs in time O(nscope(G)), where scope(G)is the maximum scope of any of the rules in CFG G.Specifically, any scope-3 grammar can be decodedin cubic time.Like (DeNero et al, 2009), the target of our in-terest is synchronous context-free grammar (SCFG)decoding with rules extracted using the GHKM al-gorithm (Galley et al, 2004).
In practice, it turns outthat only a small percentage of the lexical rules inour system have scope greater than 3.
By simply re-moving these rules from the grammar, we can main-tain the cubic running time of chart parsing withoutany kind of binarization.
This has three advantages.First, we do not inflate the grammar constant.
Sec-ond, unlike (DeNero et al, 2009), we maintain thesynchronous property of the grammar, and thus canintegrate language model scoring into chart parsing.Finally, a system without binarized rules is consid-erably simpler to build and maintain.
We show thatthis approach gives us better practical performancethan a mature system that binarizes using the tech-nique of (Zhang et al, 2006).2 PreliminariesAssume we have a global vocabulary of symbols,containing the reserved substitution symbol ?.
De-fine a sentence as a sequence of symbols.
We willtypically use space-delimited quotations to representexample sentences, e.g.
?the fast jet ski?
rather than?the, fast, jet, ski?.
We will use the dot operator torepresent the concatenation of sentences, e.g.
?thefast?
?
?jet ski?
= ?the fast jet ski?.Define the rank of a sentence as the countof its ?
symbols.
We will use the no-tation SUB(s, s1, ..., sk) to denote the substitu-tion of k sentences s1, ..., sk into a k-rank sen-tence s. For instance, if s = ?the ?
?
of?
?, then SUB(s, ?fast?, ?jet ski?, ?mr smith?)
=?the fast jet ski of mr smith?.To refer to a subsentence, define a span as a pair[a, b] of nonnegative integers such that a < b. Fora sentence s = ?s1, s2, ..., sn?
and a span [a, b] suchthat b ?
n, define s[a,b] = ?sa+1, ..., sb?.647NP -> the JJ NN of NNPPP -> on NPJJ -> fast NN -> jet ski NNP -> mr smithNP -> < the JJ1 NN2 of NNP3, le NN2 JJ1 de NNP3 >PP -> < on NP1, sur NP1>JJ -> < fast, vite > NN -> < jet ski, jet ski > NNP -> < mr smith, m smith >Figure 3: An example CFG derivation (above) and an ex-ample SCFG derivation (below).
Both derive the sen-tence SUB(?on ?
?, SUB( ?the ?
?
of ?
?, ?fast?, ?jetski?, ?mr smith?)
) = ?on the fast jet ski of mr smith?.The SCFG derivation simultaneously derives the auxil-iary sentence ?sur le jet ski vite de m smith?.3 Minimum Derivation CostChart parsing solves a problem which we will re-fer to as Minimum Derivation Cost.
Because wewant our results to be applicable to both CFG decod-ing and SCFG decoding with an integrated languagemodel, we will provide a somewhat more abstractformulation of chart parsing than usual.In Figure 3, we show an example of a CFG deriva-tion.
A derivation is a tree of CFG rules, constructedso that the preconditions (the RHS nonterminals) ofany rule match the postconditions (the LHS nonter-minal) of its child rules.
The purpose of a derivationis to derive a sentence, which is obtained throughrecursive substitution.
In the example, we substitute?fast?, ?jet ski?, and ?mr smith?
into the lexical pat-tern ?the ?
?
of ??
to obtain ?the fast jet ski of mrsmith?.
Then we substitute this result into the lexi-cal pattern ?on ??
to obtain ?on the fast jet ski of mrsmith?.The cost of a derivation is simply the sum of thebase costs of its rules.
Thus the cost of the CFGderivation in Figure 3 is C1 + C2 + C3 + C4 + C5,where C1 is the base cost of rule ?PP?
on NP?, etc.Notice that this cost can be distributed locally to thenodes of the derivation (Figure 4).An SCFG derivation is similar to a CFG deriva-NP -> the JJ NN of NNPPP -> on NPJJ -> fast NN -> jet ski NNP -> mr smithC 3C 4C 5C 2C 1Figure 4: The cost of the CFG derivation in Figure 3 isC1 + C2 + C3 + C4 + C5, where C1 is the base costof rule ?PP ?
on NP?, etc.
Notice that this cost can bedistributed locally to the nodes of the derivation.tion, except that it simultaneously derives two sen-tences.
For instance, the SCFG derivation in Fig-ure 3 derives the sentence pair ?
?on the fast jet skiof mr smith?, ?sur le jet ski vite de m smith?
?.
Inmachine translation, often we want the cost of theSCFG derivation to include a language model costfor this second sentence.
For example, the cost of theSCFG derivation in Figure 3 might beC1+C2+C3+C4+C5+LM(sur le)+LM(le jet)+LM(jet ski)+LM(ski de) + LM(de m) + LM(m smith), whereLM is the negative log of a 2-gram language model.This new cost function can also be distributed lo-cally to the nodes of the derivation, as shown in Fig-ure 5.
However, in order to perform the local com-putations, we need to pass information (in this case,the LM boundary words) up the tree.
We refer tothis extra information as carries.
Formally, define acarry as a sentence of rank 0.In order to provide a chart parsing formulationthat applies to both CFG decoding and SCFG de-coding with an integrated language model, we needabstract definitions of rule and derivation that cap-ture the above concepts of pattern, postcondition,preconditions, cost, and carries.3.1 RulesDefine a rule as a tuple ?k, s?, X, pi,?, c?, where k isa nonnegative integer called the rank, s?
is a rank-k648NP -> < the JJ1 NN2 of NNP3, le NN2 JJ1 de NNP3 >PP -> < on NP1, sur NP1>JJ -> < fast, vite > NN -> < jet ski, jet ski > NNP -> < mr smith, m smith >m * smithC 5 + LM(m smith)C 4 + LM(jet ski)C 3vite * vite jet * skile * smithC 2 + LM(le jet) + LM(ski vite) + LM( vite de) + LM(de m)C 1 + LM ( sur le)Figure 5: The cost of the SCFG derivation in Figure 3(with an integrated language model score) can also be dis-tributed to the nodes of the derivation, but to perform thelocal computations, information must be passed up thetree.
We refer to this extra information as a carry.sentence called the pattern 1, X is a symbol calledthe postcondition, pi is a k-length sentence called thepreconditions, ?
is a function (called the carry func-tion) that maps a k-length list of carries to a carry,and c is a function (called the cost function) thatmaps a k-length list of carries to a real number.
Fig-ure 6 shows a CFG and an SCFG rule, deconstructedaccording to this definition.
2 Note that the CFG rulehas trivial cost and carry functions that map every-thing to a constant.
We refer to such rules as simple.We will use post(r) to refer to the postconditionof rule r, and pre(r, i) to refer to the ith preconditionof rule r.Finally, define a grammar as a finite set of rules.A grammar is simple if all its rules are simple.3.2 DerivationsFor a grammarR, define deriv(R) as the smallest setthat contains every tuple ?r, ?1, ..., ?k?
satisfying thefollowing conditions:1For simplicity, we also impose the condition that ???
is nota valid pattern.
This is tantamount to disallowing unary rules.2One possible point of confusion is why the pattern of theSCFG rule refers only to the primary sentence, and not the aux-iliary sentence.
To reconstruct the auxiliary sentence from anSCFG derivation in practice, one would need to augment theabstract definition of rule with an auxiliary pattern.
Howeverthis is not required for our theoretical results.NP -> < the JJ1 NN2 of NNP3, le NN2 JJ1 de NNP3 >postcondition preconditions rankpattern the ?
?of ?carry function ?
( ?u*v?
, ?w*x?
, ?y*z?
)= ?????
?cost function c( ?u*v?
, ?w*x?
, ?y*z? )
= C + LM( w|le ) + LM( u|x ) + LM( de|v ) + LM( y |de )NP -> the JJ NN of NNPpostcondition preconditionspattern the ?
?of ?carry function ?
( ??
, ??
, ?? )
= ?
?cost function c( ??
, ??
, ?? )
= Crank = 3Figure 6: Deconstruction of a CFG rule (left) and SCFGrule (right) according to the definition of rule in Sec-tion 3.1.
The carry function of the SCFG rule computesboundary words for a 2-gram language model.
In the costfunctions, C is a real number and LM returns the negativelog of a language model query.?
r ?
R is a k-rank rule?
?i ?
deriv(R) for all 1 ?
i ?
k?
pre(r, i) = post(ri) for all 1 ?
i ?
k, where riis the first element of tuple ?i.An R?derivation is an element of deriv(R).
Con-sider a derivation ?
= ?r, ?1, ..., ?k?, where ruler = ?k, s?, X, pi,?, c?.
Define the following prop-erties:post(?)
= post(r)sent(?)
= SUB(s?, sent(?1), ..., sent(?k))carry(?)
= ?
(carry(?1), ..., carry(?k))cost(?)
= c(carry(?1), ..., carry(?k)) +k?j=1cost(?j)In words, we say that derivation ?
derives sen-tence sent(?).
If for some span ?
of a particular sen-tence s, it holds that sent(?)
= s?, then we will saythat ?
is a derivation over span ?.3.3 Problem StatementThe Minimum Derivation Cost problem is the fol-lowing.
Given a set R of rules and an input sentence649on the fast jet ski of mr smiththe ?
?
of ?0 1 2 3 4 5 6 7 8Figure 7: An application context for the pattern ?the ?
?of ??
and the sentence ?on the fast jet ski of mr smith?.s, find the minimum cost of any R?derivation thatderives s. In other words, compute:MinDCost(R, s) , min??deriv(R)|sent(?)=scost(?
)4 Application ContextsChart parsing solves Minimum Derivation Cost viadynamic programming.
It works by building deriva-tions over increasingly larger spans of the input sen-tence s. Consider just one of these spans ?.
How dowe build a derivation over that span?Recall that a derivation takes the form?r, ?1, ..., ?k?.
Given the rule r and its patterns?, we need to choose the subderivations ?i suchthat SUB(s?, sent(?1), ..., sent(?k)) = s?.
To doso, we must match the pattern to the span, so thatwe know which subspans we need to build thesubderivations over.
Figure 7 shows a matchingof the pattern ?the ?
?
of ??
to span [1, 8] of thesentence ?on the fast jet ski of mr smith?.
It tellsus that we can build a derivation over span [1, 8] bychoosing this rule and subderivations over subspans[2, 3], [3, 5], and [6, 8].We refer to these matchings as application con-texts.
Formally, given two sentences s?
and sof respective lengths m and n, define an ?s?, s?
?context as an monotonically increasing sequence?x0, x1, ..., xm?
of integers between 0 and n suchthat for all i:s?
[i?1,i] 6= ?
implies that s?
[i?1,i] = s[xi?1,xi]The context shown in Figure 7 is ?1, 2, 3, 5, 6, 8?.Use cxt(s?, s) to denote the set of all ?s?, s?
?contexts.An ?s?, s?
?context x = ?x0, x1, ..., xm?
has thefollowing properties:span(x; s?, s) = [x0, xm]subspans(x; s?, s) = ?
[x0, x1], ..., [xm?1, xm]?Moreover, define varspans(x; s?, s) as the sub-sequence of subspans(x; s?, s) including only[xi?1, xi] such that s?
[i?1,i] = ?.
For the contextx shown in Figure 7:span(x; s?, s) = [1, 8]subspans(x; s?, s) = ?
[1, 2], [2, 3], [3, 5], [5, 6], [6, 8]?varspans(x; s?, s) = ?
[2, 3], [3, 5], [6, 8]?An application context x ?
cxt(s?, s) tells us thatwe can build a derivation over span(x) by choosinga rule with pattern s?
and subderivations over eachspan in varspans(x; s?, s).5 Chart Parsing AlgorithmWe are now ready to describe the chart parsing al-gorithm.
Consider a span ?
of our input sentences and assume that we have computed and stored allderivations over any subspan of ?.
A naive way tocompute the minimum cost derivation over span ?
isto consider every possible derivation:1.
Choose a rule r = ?k, s?, X, pi,?, c?.2.
Choose an application context x ?
cxt(s?, s)such that span(x; s?, s) = ?.3.
For each subspan ?i ?
varspans(x; s?, s),choose a subderivation ?i such that post(?i) =pre(r, i).The key observation here is the following.
In or-der to score such a derivation, we did not actuallyneed to know each subderivation in its entirety.
Wemerely needed to know the following informationabout it: (a) the subspan that it derives, (b) its post-condition, (c) its carry.650Chart parsing takes advantage of the above obser-vation to avoid building all possible derivations.
In-stead it groups together derivations that share a com-mon subspan, postcondition, and carry, and recordsonly the minimum cost for each equivalence class.It records this cost in an associative map referred toas the chart.Specifically, assume that we have computed andstored the minimum cost of every derivation class??
?, X ?, ??
?, where X ?
is a postcondition, ??
is acarry, and ??
is a proper subspan of ?.
Chart pars-ing computes the minimum cost of every derivationclass ?
?,X, ??
by adapting the above naive methodas follows:1.
Choose a rule r = ?k, s?, X, pi,?, c?.2.
Choose an application context x ?
cxt(s?, s)such that span(x; s?, s) = ?.3.
For each subspan ?i ?
varspans(x; s?, s),choose a derivation class ?
?i, Xi, ?i?
from thechart such that Xi = pre(r, i).4.
Update3 the cost of derivation class?
?, post(r),?
(?1, ..., ?k)?
with:c(?1, ..., ?k) +k?i=1chart[?i, Xi, ?i]where chart[?i, Xi, ?i] refers to the storedcost of derivation class ?
?i, Xi, ?i?.By iteratively applying the above method to all sub-spans of size 1, 2, etc., chart parsing provides anefficient solution for the Minimum Derivation Costproblem.6 Runtime AnalysisAt the heart of chart parsing is a single operation:the updating of a value in the chart.
The runningtime is linear in the number of these chart updates.4 The typical analysis counts the number of chartupdates per span.
Here we provide an alternative3Here, update means ?replace the cost associated with theclass if the new cost is lower.
?4This assumes that you can linearly enumerate the relevantupdates.
One convenient way to do this is to frame the enumer-ation problem as a search space, e.g.
(Hopkins and Langmead,2009)analysis that counts the number of chart updates perrule.
This provides us with a finer bound with prac-tical implications.Let r be a rule with rank k and pattern s?.
Con-sider the chart updates involving rule r. There is(potentially) an update for every choice of (a) span,(b) application context, and (c) list of k derivationclasses.
If we let C be the set of possible carries,then this means there are at most |cxt(s?, s)| ?
|C|kupdates involving rule r. 5 If we are doing beam de-coding (i.e.
after processing a span, the chart keepsonly the B items of lowest cost), then there are atmost |cxt(s?, s)| ?Bk updates.We can simplify the above by providing an upperbound for |cxt(s?, s)|.
Define an ambiguity as thesentence ??
?
?, and define scope(s?)
as the numberof ambiguities in the sentence ???
?s??
???.
Thefollowing bound holds:Lemma 1.
Assume that a zero-rank sentence s doesnot contain the same symbol more than once.
Then|cxt(s?, s)| ?
|s|scope(s?).Proof.
Suppose s?
and s have respective lengths mand n. Consider ?x0, x1, ..., xm?
?
cxt(s?, s).
LetI be the set of integers i between 1 and m such thats?i 6= ?
and let I+ be the set of integers i between0 and m ?
1 such that s?i+1 6= ?.
If i ?
I , then weknow the value of xi, namely it is the unique integerj such that sj = s?i .
Similarly, if i ?
I+, then thevalue of xi must be the unique integer j such thatsj = s?i+1.
Thus the only nondetermined elementsof context xi are those for which i 6?
I ?
I+.
Hence|cxt(s?, s)| ?
|s|{0,1,...,m}?I?I+= |s|scope(s?
).Hence, under the assumption that the input sen-tence s does not contain the same symbol more thanonce, then there are at most |s|scope(s?)
?
|C|k chartupdates involving a rule with pattern s?.For a rule r with pattern s?, define scope(r) =scope(s?).
For a grammar R, define scope(R) =maxr?R scope(r) and rank(R) = maxr?R rank(r).Given a grammar R and an input sentence s,the above lemma tells us that chart parsing makes5For instance, in SCFG decoding with an integrated j-gramlanguage model, a carry consists of 2(j ?
1) boundary words.Generally it is assumed that there are O(n) possible choices fora boundary word, and hence O(n2(j?1)) possible carries.651O(|s|scope(R) ?
|C|rank(R)) chart updates.
If we re-strict ourselves to beam search, than chart parsingmakes O(|s|scope(R)) chart updates.
66.1 On the Uniqueness AssumptionIn practice, it will not be true that each input sen-tence contains only unique symbols, but it is not toofar removed from the practical reality of many usecases, for which relatively few symbols repeat them-selves in a given sentence.
The above lemma canalso be relaxed to assume only that there is a con-stant upper bound on the multiplicity of a symbolin the input sentence.
This does not affect the O-bound on the number of chart updates, as long as wefurther assume a constant limit on the length of rulepatterns.7 Scope ReductionFrom this point of view, CNF binarization can beviewed as a specific example of scope reduction.Suppose we have a grammar R of scope p. See Fig-ure 8.
If we can find a grammar R?
of scope p?
< pwhich is ?similar?
to grammar R, then we can de-code in O(np?)
rather than O(np) time.We can frame the problem by assuming the fol-lowing parameters:?
a grammar R?
a desired scope p?
a loss function ?
that returns a (non-negativereal-valued) score for any two grammars R andR?
; if ?
(R, R?)
= 0, then the grammars are con-sidered to be equivalentA scope reduction method with loss ?
finds a gram-mar R?
such that scope(R?)
?
p and ?
(R, R?)
= ?.A scope reduction method is lossless when its lossis 0.In the following sections, we will use the lossfunction:?
(R, R?)
= |MinDCost(R, s)?MinDCost(R?, s)|where s is a fixed input sentence.
Observe that if?
(R, R?)
= 0, then the solution to the Minimum6Assuming rank(R) is bounded by a constant.CNF LNFScope 3All GrammarsFigure 8: The ?scope reduction?
problem.
Given a gram-mar of large scope, find a similar grammar of reducedscope.Derivation Cost problem is the same for both R andR?.
77.1 CNF BinarizationA rule r is CNF if its pattern is ????
or ?x?, where xis any non-substitution symbol.
A grammar is CNFif all of its rules are CNF.
Note that the maximumscope of a CNF grammar is 3.CNF binarization is a deterministic process thatmaps a simple grammar to a CNF grammar.
Sincebinarization takes subcubic time, we can decodewith any grammar R in O(n3) time by convertingR to CNF grammar R?, and then decoding with R?.This is a lossless scope reduction method.What if grammar R is not simple?
For SCFGgrammars, (Zhang et al, 2006) provide a scopereduction method called synchronous binarizationwith quantifiable loss.
Synchronous binarization se-lects a ?binarizable?
subgrammar R?
of grammar R,and then converts R?
into a CNF grammar R?.
Thecost and carry functions of these new rules are con-structed such that the conversion from R?
to R?
isa lossless scope reduction.
Thus the total loss ofthe method is |MinDCost(R, s)?MinDCost(R?, s)|.Fortunately, they find in practice thatR?
usually con-tains the great majority of the rules of R, thus they7Note that if we want the actual derivation and not just itscost, then we need to specify a more finely grained loss func-tion.
This is omitted for clarity and left as an exercise.652a ???
?aa ?
?b?a ???
?a ???
?aa ??
?a b ???
?a ba ?
?b ?a ?
?b ca b ?
?c?a ?
?ba ?b ???
?a ?b??
?a ba ??
?ba ?
?b ?ca ???
?ba ?
?b c ?
?da ???
?b ?c ?da ?
?b ?
?c ?
?dFigure 9: A selection of rule patterns that are scope ?
3but not LNF or CNF.assert that this loss is negligable.A drawback of their technique is that the resultingCNF grammar contains many more rules and post-conditions than the original grammar.
These con-stant factors do not impact asymptotic performance,but do impact practical performance.7.2 Lexical Normal FormConcerned about this inflation of the grammar con-stant, (DeNero et al, 2009) consider a superset ofCNF called Lexical Normal Form (LNF).
A rule isLNF if its pattern does not contain an ambiguity asa proper subsentence (recall that an ambiguity wasdefined to be the sentence ??
??).
Like CNF, themaximum scope of an LNF grammar is 3.
In theworst case, the pattern s?
is ??
?
?, in which casethere are three ambiguities in the sentence ???
?s?????.
(DeNero et al, 2009) provide a lossless scopereduction method that maps a simple grammar toan LNF grammar, thus enabling cubic-time decod-ing.
Their principal objective is to provide a scopereduction method for SCFG that introduces fewerpostconditions than (Zhang et al, 2006).
Howeverunlike (Zhang et al, 2006), their method only ad-dresses simple grammars.
Thus they cannot inte-grate LM scoring into their decoding, requiring themto rescore the decoder output with a variant of cubegrowing (Huang and Chiang, 2007).00.10.20.30.40.50.60.70.80.910 1 2 3 4 5 6 7 8% of ruleswith scope<= PPAE Lexical CE Lexical AE Nonlexical CE NonlexicalFigure 10: Breakdown of rules by scope (average per sen-tence in our test sets).
In practice, most of the lexical rulesapplicable to a given sentence (95% for Arabic-Englishand 85% for Chinese-English) are scope 3 or less.7.3 Scope PruningTo exercise the power of the ideas presented in thispaper, we experimented with a third (and very easy)scope reduction method called scope pruning.
If weconsider the entire space of scope-3 grammars, wesee that it contains a much richer set of rules thanthose permitted by CNF or LNF.
See Figure 9 forexamples.
Scope pruning is a lossy scope reduc-tion method that simply takes an arbitrary grammarand prunes all rules with scope greater than 3.
Bynot modifying any rules, we preserve their cost andcarry functions (enabling integrated LM decoding),without increasing the grammar constant.
The prac-tical question is: how many rules are we typicallypruning from the original grammar?We experimented with two pretrained syntax-based machine translation systems with rules ex-tracted via the GHKM algorithm (Galley et al,2004).
The first was an Arabic-English system, withrules extracted from 200 million words of paralleldata from the NIST 2008 data collection, and witha 4-gram language model trained on 1 billion wordsof monolingual English data from the LDC Giga-word corpus.
We evaluated this system?s perfor-mance on the NIST 2008 test corpus, which con-sists of 1357 Arabic sentences from a mixture ofnewswire and web domains, with four English refer-ence translations.
The second system was a Chinese-653Arabic -English Chinese -English33343536373839400 2000 4000 6000 8000BLEU-4Words per minute27282930313233343536370 2000 4000 6000 8000BLEU-4Words per minuteFigure 11: Speed-quality tradeoff curves comparing the baseline scope reduction method of synchronous binarization(dark gray diamonds) with scope-3 pruning (light gray squares).English system, with rules extracted from 16 millionwords of parallel data from the mainland-news do-main of the LDC corpora, and with a 4-gram lan-guage model trained on monolingual English datafrom the AFP and Xinhua portions of the LDC Gi-gaword corpus.
We evaluated this system?s perfor-mance on the NIST 2003 test corpus, which con-sists of 919 Chinese sentences, with four Englishreference translations.
For both systems, we reportBLEU scores (Papineni et al, 2002) on untokenized,recapitalized output.In practice, how many rules have scope greaterthan 3?
To answer this question, it is useful to dis-tinguish between lexical rules (i.e.
rules whose pat-terns contain at least one non-substitution symbol)and non-lexical rules.
Only a subset of lexical rulesare potentially applicable to a given input sentence.Figure 10 shows the scope profile of these applicablerules (averaged over all sentences in our test sets).Most of the lexical rules applicable to a given sen-tence (95% for Arabic-English, 85% for Chinese-English) are scope 3 or less.
8 Note, however, thatscope pruning also prunes a large percentage of non-lexical rules.Figure 11 compares scope pruning with the base-line technique of synchronous binarization.
To gen-erate these speed-quality tradeoff curves, we de-coded the test sets with 380 different beam settings.We then plotted the hull of these 380 points, by elim-inating any points that were dominated by another(i.e.
had better speed and quality).
We found thatthis simple approach to scope reduction produceda better speed-quality tradeoff than the much morecomplex synchronous binarization.
98For contrast, the corresponding numbers for LNF are 64%and 53%, respectively.9We also tried a hybrid approach in which we scope-pruned6548 ConclusionIn this paper, we made the following contributions:?
We provided an abstract formulation of chartparsing that generalizes CFG decoding andSCFG decoding with an integrated LM.?
We framed scope reduction as a first-class ab-stract problem, and showed that CNF binariza-tion and LNF binarization are two specific solu-tions to this problem, each with their respectiveadvantages and disadvantages.?
We proposed a third scope reduction techniquecalled scope pruning, and we showed that it canoutperform synchronous CNF binarization forparticular use cases.Moreover, this work gives formal expression to theextraction heuristics of hierarchical phrase-basedtranslation (Chiang, 2007), whose directive not toextract SCFG rules with adjacent nonterminals canbe viewed as a preemptive pruning of rules withscope greater than 2 (more specifically, the prun-ing of non-LNF lexical rules).
In general, this workprovides a framework in which different approachesto tractability-focused grammar construction can becompared and discussed.ReferencesDavid Chiang.
2007.
Hierarchical phrase-based transla-tion.
Computational Linguistics, 33(2):201?228.John DeNero, Mohit Bansal, Adam Pauls, and Dan Klein.2009.
Efficient parsing for transducer grammars.
InProceedings of the Human Language Technology Con-ference of the NAACL, Main Conference.Michel Galley, Mark Hopkins, Kevin Knight, and DanielMarcu.
2004.
What?s in a translation rule?
In Pro-ceedings of HLT/NAACL.Mark Hopkins and Greg Langmead.
2009.
Cube pruningas heuristic search.
In Proceedings of EMNLP.Liang Huang and David Chiang.
2007.
Forest rescoring:Faster decoding with integrated language models.
InProceedings of ACL.the lexical rules and synchronously binarized the non-lexicalrules.
This had a similar performance to scope-pruning allrules.
The opposite approach of scope-pruning the lexical rulesand synchronously binarizing the non-lexical rules had a similarperformance to synchronous binarization.Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu.
2002.
Bleu: a method for automatic eval-uation of machine translation.
In Proceedings of 40thAnnual Meeting of the Association for ComputationalLinguistics, pages 311?318.Daniel Younger.
1967.
Recognition and parsing ofcontext-free languages in time n3.
Information andControl, 10(2):189?208.Hao Zhang, Liang Huang, Daniel Gildea, and KevinKnight.
2006.
Synchronous binarization for machinetranslation.
In Proceedings of the Human LanguageTechnology Conference of the NAACL, Main Confer-ence, pages 256?263.655
