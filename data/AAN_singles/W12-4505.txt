Proceedings of the Joint Conference on EMNLP and CoNLL: Shared Task, pages 64?70,Jeju Island, Korea, July 13, 2012. c?2012 Association for Computational LinguisticsUsing Syntactic Dependencies to Solve CoreferencesMarcus Stamborg Dennis Medved Peter Exner Pierre NuguesLund UniversityLund, Swedencid03mst@student.lu.se, dt07dm0@student.lth.sePeter.Exner@cs.lth.se, Pierre.Nugues@cs.lth.seAbstractThis paper describes the structure of the LTHcoreference solver used in the closed track ofthe CoNLL 2012 shared task (Pradhan et al,2012).
The solver core is a mention classifierthat uses Soon et al (2001)?s algorithm andfeatures extracted from the dependency graphsof the sentences.This system builds on Bjo?rkelund and Nugues(2011)?s solver that we extended so that it canbe applied to the three languages of the task:English, Chinese, and Arabic.
We designeda new mention detection module that removespleonastic pronouns, prunes constituents, andrecovers mentions when they do not match ex-actly a noun phrase.
We carefully redesignedthe features so that they reflect more com-plex linguistic phenomena as well as discourseproperties.
Finally, we introduced a minimalcluster model grounded in the first mention ofan entity.We optimized the feature sets for the three lan-guages: We carried out an extensive evalua-tion of pairs of features and we complementedthe single features with associations that im-proved the CoNLL score.
We obtained the re-spective scores of 59.57, 56.62, and 48.25 onEnglish, Chinese, and Arabic on the develop-ment set, 59.36, 56.85, and 49.43 on the testset, and the combined official score of 55.21.1 IntroductionIn this paper, we present the LTH coreference solverused in the closed track of the CoNLL 2012 sharedtask (Pradhan et al, 2012).
We started from anearlier version of the system by Bjo?rkelund andNugues (2011), to which we added substantial im-provements.
As base learning and decoding algo-rithm, our solver extracts noun phrases and posses-sive pronouns and uses Soon et al (2001)?s pairwiseclassifier to decide if a pair corefers or not.
Similarlyto the earlier LTH system, we constructed a primaryfeature set from properties extracted from the depen-dency graphs of the sentences.2 System ArchitectureThe training and decoding modules consist of amention detector, a pair generator, and a feature ex-tractor.
The training module extracts a set of positiveand negative pairs of mentions and uses logistic re-gression and the LIBLINEAR package (Fan et al,2008) to generate a binary classifier.
The solver ex-tracts pairs of mentions and uses the classifier and itsprobability output, Pcoref (Antecedent,Anaphor), todetermine if a pair corefers or not.
The solver hasalso a post processing step to recover some mentionsthat do not match a noun phrase constituent.3 Converting Constituents to DependencyTreesAlthough the input to coreference solvers are pairsor sets of constituents, many systems use conceptsfrom dependency grammars to decide if a pair iscoreferent.
The most frequent one is the con-stituent?s head that solvers need then to extract us-ing ad-hoc rules; see the CoNLL 2011 shared task(Pradhan et al, 2011), for instance.
This can be te-dious as we may have to write new rules for eachnew feature to incorporate in the classifier.
That is64why, instead of writing sets of rules applicable tospecific types of dependencies, we converted all theconstituents in the three corpora to generic depen-dency graphs before starting the training and solvingsteps.
We used the LTH converter (Johansson andNugues, 2007) for English, the Penn2Malt converter(Nivre, 2006) with the Chinese rules for Chinese1,and the CATiB converter (Habash and Roth, 2009)for Arabic.The CATiB converter (Habash and Roth, 2009)uses the Penn Arabic part-of-speech tagset, whilethe automatically tagged version of the CoNLL Ara-bic corpus uses a simplified tagset inspired by theEnglish version of the Penn treebank.
We translatedthese simplified POS tags to run the CATiB con-verter.
We created a lookup table to map the simpli-fied POS tags in the automatically annotated corpusto the Penn Arabic POS tags in the gold annotation.We took the most frequent association in the lookuptable to carry out the translation.
We then used theresult to convert the constituents into dependencies.We translated the POS tags in the development setusing a dictionary extracted from the gold trainingfile and we translated the tags in the training file by a5-fold cross-validation.
We used this dictionary dur-ing both training and classifying since our featureshad a better performance with the Arabic tagset.4 Mention Extraction4.1 Base ExtractionAs first step of the mention selection stage, we ex-tracted all the noun phrases (NP), pronouns (PRP),and possessive pronouns (PRP$) for English andArabic, with the addition of PN pronouns for Chi-nese.
This stage is aimed at reaching a high recall ofthe mentions involved in the coreference chains andresults in an overinclusive set of candidates.
Table 1shows the precision and recall figures for the respec-tive languages when extracting mentions from thetraining set.
The precision is significantly lower forArabic than for English and Chinese.4.2 Removal of the Pleonastic itIn the English corpus, the pronoun it in the first stepof the mention extraction stage creates a high num-ber of false positive mentions.
We built a classifier1http://stp.lingfil.uu.se/ nivre/research/Penn2Malt.htmlLanguage Recall PrecisionEnglish 92.17 32.82English with named entities 94.47 31.61Chinese 87.32 32.29Arabic 87.22 17.64Table 1: Precision and recall for the mention detectionstage on the training set.Feature nameHeadLexHeadRightSiblingPOSHeadPOSTable 2: Features used by the pleonastic it classifier.to discard as many of these pleonastic it as possiblefrom the mention list.Table 2 shows the features we used to train theclassifier and Table 3 shows the impact on the finalsystem.
We optimized the feature set using greedyforward and backward selections.
We explored var-ious ways of using the classifier: before, after, andduring coreference resolving.
We obtained the bestresults when we applied the pleonastic classifier dur-ing coreference solving and we multiplied the prob-ability outputs from the two classifiers.
We used theinequality:Pcoref (Antecedent, it)?
(1?
Ppleo(it)) > 0.4,where we found the optimal threshold of 0.4 using a5-fold cross-validation.4.3 Named EntitiesThe simple rule to approximate entities to nounphrases and pronouns leaves out between ?8% and?13 % of the entities in the corpora (Table 1).
As thenamed entities sometimes do not match constituents,we tried to add them to increase the recall.
Wecarried out extensive experiments for the three lan-English CoNLL scoreWithout removal 59.15With removal 59.57Table 3: Score on the English development set with andwithout removal of the pleonastic it pronouns.65English Total scoreWithout named entities 58.85With named entities 59.57Table 4: Impact on the overall score on the English devel-opment set by addition of named entities extracted fromthe corpus.Language Without pruning With pruningEnglish 56.42 59.57Chinese 50.94 56.62Arabic 48.25 47.10Table 5: Results on running the system on the develop-ment set with and without pruning for all the languages.guages.
While the named entities increased the scorefor the English corpus, we found that it loweredthe results for Chinese and Arabic.
We added allsingle and multiword named entities of the Englishcorpus except the CARDINAL, ORDINAL, PER-CENT, and QUANTITY tags.
Table 1 shows the re-call and precision for English and Table 4 shows thenamed entity impact on the overall CoNLL score onthe development set.4.4 PruningWhen constituents shared the same head in the listof mentions, we pruned the smaller ones.
This in-creased the scores for English and Chinese, but low-ered that of Arabic (Table 5).
The results for thelatter language are somewhat paradoxical; they arepossibly due to errors in the dependency conversion.5 DecodingDepending on the languages, we applied differentdecoding strategies: For Chinese and Arabic, weused a closest-first clustering method as describedby Soon et al (2001) for pronominal anaphors anda best-first clustering otherwise as in Ng and CardieEnglish Total scoreWithout extensions 57.22With extensions 59.57Table 6: Total impact of the extensions to the mentionextraction stage on the English development set.(2002).
For English, we applied a closest-first clus-tering for pronominal anaphors.
For nonpronomi-nal anaphors, we used an averaged best-first cluster-ing: We considered all the chains before the currentanaphor and we computed the geometric mean of thepair probabilities using all the mentions in a chain.We linked the anaphor to the maximal scoring chainor we created a new chain if the score was less than0.5.
We discarded all the remaining singletons.As in Bjo?rkelund and Nugues (2011), we recov-ered some mentions using a post processing stage,where we clustered named entities to chains havingstrict matching heads.6 FeaturesWe started with the feature set described inBjo?rkelund and Nugues (2011) for our baseline sys-tem for English and with the feature set in Soon et al(2001) for Chinese and Arabic.
Due to space limita-tions, we omit the description of these features andrefer to the respective papers.6.1 Naming ConventionWe denoted HD, the head word of a mention in a de-pendency tree, HDLMC and HDRMC, the left-mostchild and the right-most child of the head, HDLS andHDRS, the left and right siblings of the head word,and HDGOV, the governor of the head word.From these tokens, we can extract the surfaceform, FORM, the part-of-speech tag, POS, and thegrammatical function of the token, FUN, i.e.
the la-bel of the dependency edge of the token to its parent.We used a naming nomenclature consisting of therole in the anaphora, where J- stands for the anaphor,I-, for the antecedent, F-, for the mention in the chainpreceding the antecedent (previous antecedent), andA- for the first mention of the entity in the chain;the token we selected from the dependency graph,e.g.
HD or HDLMC; and the value extracted fromthe token e.g.
POS or FUN.
For instance, the part-of-speech tag of the governor of the head word ofthe anaphor is denoted J-HDGOVPOS.6.2 Combination of FeaturesIn addition to the single features, we combined themto create bigram, trigram, and four-gram features.Table 7 shows the features we used, either single orin combination, e.g.
I-HDFORM+J-HDFORM.66We emulated a simple cluster model by uti-lizing the first mention in the chain and/or theprevious antecedent, e.g.
A-EDITDISTANCE+F-EDITDISTANCE+EDITDISTANCE, where the editdistance of the anaphor is calculated for the firstmention in the chain, previous antecedent, and an-tecedent.6.3 Notable New FeaturesEdit Distance Features.
We created edit distance-based features between pairs of potentiallycoreferring mentions: EDITDISTANCE is thecharacter-based edit distance between twostrings; EDITDISTANCEWORD is a word-leveledit distance, where the symbols are the com-plete words; and PROPERNAMESIMILARITYis a character-based edit distance betweenproper nouns only.Discourse Features.
We created features to reflectthe speaker agreement, i.e.
when the pair ofmentions corresponds to the same speaker, of-ten in combination with the fact that both men-tions are pronouns.
For example, references tothe first person pronoun I from a same speakerrefer probably to a same entity; in this case, thespeaker himself.Document Type Feature.
We created the I-HDFORM+J-HDFORM+DOCUMENTTYPE fea-ture to capture the genre of different documenttypes, as texts from e.g.
the New Testament arelikely to differ from internet blogs.6.4 Feature SelectionWe carried out a greedy forward selection of the fea-tures starting from Bjo?rkelund and Nugues (2011)?sfeature set for English, and Soon et al (2001)?s forChinese and Arabic.
The feature selection used a 5-fold cross-validation over the training set, where weevaluated the features using the arithmetic mean ofMUC, BCUB, and CEAFE.After reaching a maximal score using forward se-lection, we reversed the process using a backwardelimination, leaving out each feature and removingthe one that had the worst impact on performance.This backwards procedure was carried out until thescore no longer increased.
We repeated this forward-backward procedure until there was no increase inperformance.7 EvaluationTable 7 shows the final feature set for each languagecombined with the impact each feature has on thescore on the development set when being left out.
Adash (?)
means that the feature is not part of thefeature set used in the respective language.
As wecan see, some features increase the score.
This isdue to the fact that the feature selection was carriedout in a cross-validated manner over the training set.Table 8 shows the results on the development andtest sets as well as on the test set with gold mentions.For each language, the figures are overall consistentbetween the development and test sets across all themetrics.
The scores improve very significantly withthe gold mentions: up to more than 10 points forChinese.8 ConclusionsThe LTH coreference solver used in the CoNLL2012 shared task uses Soon et al (2001)?s algorithmand a set of lexical and nonlexical features.
To alarge extent, we extracted these features from the de-pendency graphs of the sentences.
The results weobtained seem to hint that this approach is robustacross the three languages of the task.Our system builds on an earlier system that weevaluated in the CoNLL 2011 shared task (Pradhanet al, 2011), where we optimized significantly thesolver code, most notably the mention detection stepand the feature design.
Although not exactly compa-rable, we could improve the CoNLL score by 4.83from 54.53 to 59.36 on the English corpus.
Themention extraction stage plays a significant role inthe overall performance.
By improving the qual-ity of the mentions extracted, we obtained a perfor-mance increase of 2.35 (Table 6).Using more complex feature structures alsoproved instrumental.
Scores of additional featurevariants could be tested in the future and possiblyincrease the system?s performance.
Due to limitedcomputing resources and time, we had to confine thesearch to a handful of features that we deemed mostpromising.67All features En (+/-) Zh (+/-) Ar (+/-)STRINGMATCH -0.003 -0.58 -1.79A-STRINGMATCH+STRINGMATCH -0.11 ?
?DISTANCE -0.19 -0.57 -0.24DISTANCE+J-PRONOUN 0.03 ?
?I-PRONOUN 0.02 ?
?J-PRONOUN 0.02 ?
?J-DEMOSTRATIVE -0.02 0.01 ?BOTHPROPERNAME ?
0.03 ?NUMBERAGREEMENT -0.23 ?
?GENDERAGREEMENT 0.003 ?
?NUMBERBIGRAM ?
0.06 ?GENDERBIGRAM -0.03 0.01 ?I-HDFORM -0.16 ?
-0.67I-HDFUN 0.05 ?
?I-HDPOS -0.02 ?
-0.52I-HDRMCFUN 0.003 ?
?I-HDLMCFORM ?
?
-0.05I-HDLMCPOS 0.01 ?
?I-HDLSFORM -0.08 ?
-0.18I-HDGOVFUN 0.06 ?
?I-HDGOVPOS ?
-0.003 -0.19J-HDFUN 0.003 ?
?J-HDGOVFUN 0.03 ?
?J-HDGOVPOS -0.05 ?
?J-HDRSPOS ?
?
-0.2A-HDCHILDSETPOS ?
0.06 ?I-HDFORM+J-HDFORM 0.08 ?
-0.57A-HDFORM+J-HDFORM ?
?
-0.46I-HDGOVFORM+J-HDFORM ?
-0.14 0.04I-LMCFORM+J-LMCFORM -0.07 -0.15 ?A-HDFORM+I-HDFORM+J-HDFORM 0.11 ?
?F-HDFORM+I-HDFORM+J-HDFORM ?
-0.1 ?I-HDPOS+J-HDPOS+I-HDFUN+J-HDFUN ?
-0.09 ?I-HDPOS+J-HDPOS+I-HDFORM+J-HDFORM ?
?
-0.05I-HDFORM+J-HDFORM+SPEAKAGREE ?
-0.55 ?I-HDFORM+J-HDFORM+BOTHPRN+SPEAKAGREE -0.11 ?
?I-HDGOVFORM+J-HDFORM+BOTHPRN+SPEAKAGREE -0.23 ?
?A-HDFORM+J-HDFORM+SPEAKAGREE 0.04 ?
?I-HDFORM+J-HDFORM+DOCUMENTTYPE -0.4 -0.18 ?SSPATHBERGSMALIN -0.07 ?
?SSPATHFORM ?
?
-0.19SSPATHFUN -0.08 ?
-0.14SSPATHPOS -0.1 -0.11 -0.53DSPATHBERGSMALIN ?
?
0DSPATHFORM 0.07 ?
?DSPATHFORM+DOCUMENTTYPE 0.03 ?
?DSPATHPOS 0.07 -0.06 0.05EDITDISTANCE -0.05 -0.16 0EDITDISTANCEWORD ?
?
-0.25A-EDITDISTANCE+EDITDISTANCE ?
?
-0.02A-EDITDISTANCE+F-EDITDISTANCE ?
-0.01 -0.01A-EDITDISTANCE+F-EDITDISTANCE+EDITDISTANCE ?
?
-0.09EDITDISTANCEWORD+BOTHPROPERNAME 0.02 ?
?PROPERNAMESIMILARITY -0.03 ?
?SEMROLEPROPJHD 0.01 ?
?Table 7: The feature sets for English, Chinese and Arabic, and for each feature, the degradation in performance whenleaving out this feature from the set; the more negative, the better the feature contribution.
We carried out all theevaluations on the development set.
The table shows the difference with the official CoNLL score.68Metric/Corpus Development set Test set Test set (Gold mentions)English R P F1 R P F1 R P F1Mention detection 74.21 72.81 73.5 75.51 72.39 73.92 78.17 100 87.74MUC 65.27 64.25 64.76 66.26 63.98 65.10 71.22 88.12 78.77BCUB 69.1 70.94 70.01 69.09 69.54 69.31 64.75 83.16 72.8CEAFM 57.56 57.56 57.56 56.76 56.76 56.76 66.74 66.74 66.74CEAFE 43.44 44.47 43.95 42.53 44.89 43.68 71.94 43.74 54.41BLANC 75.36 77.41 76.34 74.03 77.28 75.52 78.68 81.47 79.99CoNLL score 59.57 59.36 68.66Chinese R P F1 R P F1 R P F1Mention detection 60.55 68.73 64.38 57.65 71.93 64.01 68.97 100 81.63MUC 54.63 60.96 57.62 52.56 64.13 57.77 63.52 88.23 73.86BCUB 66.91 74.4 70.46 64.43 77.55 70.38 63.54 88.12 73.84CEAFM 55.09 55.09 55.09 55.57 55.57 55.57 65.60 65.60 65.60CEAFE 44.65 39.25 41.78 47.90 38.04 42.41 72.56 42.01 53.21BLANC 73.23 72.95 73.09 72.74 77.84 75.00 76.96 83.70 79.89CoNLL score 56.62 56.85 66.97Arabic R P F1 R P F1 R P F1Mention detection 55.54 61.7 58.46 56.1 63.28 59.47 56.13 100 71.9MUC 39.18 43.76 41.34 39.11 43.49 41.18 41.99 69.78 52.43BCUB 59.16 67.94 63.25 61.57 67.95 64.61 50.45 81.30 62.26CEAFM 47.8 47.8 47.8 50.16 50.16 50.16 54.00 54.00 54.00CEAFE 42.57 38.01 40.16 44.86 40.36 42.49 66.16 34.52 45.37BLANC 62.44 67.18 64.36 66.80 66.94 66.87 67.37 73.46 69.87CoNLL score 48.25 49.43 53.35Table 8: Scores on the development set, test set, and test set with gold mentions for English, Chinese, and Arabic:recall R, precision P, and harmonic mean F1.
The official CoNLL score is computed as the arithmetic mean of MUC,BCUB, and CEAFE.69AcknowledgmentsThis research was supported by Vetenskapsra?det, theSwedish research council, under grant 621-2010-4800, and the European Union?s seventh frameworkprogram (FP7/2007-2013) under grant agreementno.
230902.ReferencesAnders Bjo?rkelund and Pierre Nugues.
2011.
Explor-ing lexicalized features for coreference resolution.
InProceedings of the Fifteenth Conference on Compu-tational Natural Language Learning: Shared Task,pages 45?50, Portland, Oregon, USA, June.
Associ-ation for Computational Linguistics.Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, Xiang-RuiWang, and Chih-Jen Lin.
2008.
LIBLINEAR: A li-brary for large linear classification.
Journal of Ma-chine Learning Research, 9:1871?1874.Nizar Habash and Ryan Roth.
2009.
CATiB: TheColumbia Arabic treebank.
In Proceedings of theACL-IJCNLP 2009 Conference Short Papers, pages221?224, Suntec, Singapore, August.
Association forComputational Linguistics.Richard Johansson and Pierre Nugues.
2007.
Extendedconstituent-to-dependency conversion for English.
InJoakim Nivre, Heiki-Jaan Kaalep, Kadri Muischnek,and Mare Koit, editors, NODALIDA 2007 ConferenceProceedings, pages 105?112, Tartu, May 25-26.Vincent Ng and Claire Cardie.
2002.
Improving machinelaerning approaches to coreference resolution.
In Pro-ceedings of the 40th Annual Meeting of the Associationfor Computational Linguistics, pages 104?111.Joakim Nivre.
2006.
Inductive Dependency Parsing.Springer, Dordrecht, The Netherlands.Sameer Pradhan, Lance Ramshaw, Mitchell Marcus,Martha Palmer, Ralph Weischedel, and Nianwen Xue.2011.
CoNLL-2011 shared task: Modeling unre-stricted coreference in OntoNotes.
In Proceedings ofthe Fifteenth Conference on Computational NaturalLanguage Learning: Shared Task, pages 1?27, Port-land, Oregon, USA, June.
Association for Computa-tional Linguistics.Sameer Pradhan, Alessandro Moschitti, Nianwen Xue,Olga Uryupina, and Yuchen Zhang.
2012.
CoNLL-2012 shared task: Modeling multilingual unrestrictedcoreference in OntoNotes.
In Proceedings of theSixteenth Conference on Computational Natural Lan-guage Learning (CoNLL 2012), Jeju, Korea.Wee Meng Soon, Hwee Tou Ng, and Daniel Chung YongLim.
2001.
A machine learning approach to corefer-ence resolution of noun phrases.
Computational Lin-guistics, 27(4):521?544.70
