Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 864?872,Jeju, Republic of Korea, 8-14 July 2012. c?2012 Association for Computational LinguisticsModeling Sentences in the Latent SpaceWeiwei GuoDepartment of Computer Science,Columbia University,weiwei@cs.columbia.eduMona DiabCenter for Computational Learning Systems,Columbia University,mdiab@ccls.columbia.eduAbstractSentence Similarity is the process of comput-ing a similarity score between two sentences.Previous sentence similarity work finds thatlatent semantics approaches to the problem donot perform well due to insufficient informa-tion in single sentences.
In this paper, weshow that by carefully handling words thatare not in the sentences (missing words), wecan train a reliable latent variable model onsentences.
In the process, we propose a newevaluation framework for sentence similarity:Concept Definition Retrieval.
The new frame-work allows for large scale tuning and test-ing of Sentence Similarity models.
Experi-ments on the new task and previous data setsshow significant improvement of our modelover baselines and other traditional latent vari-able models.
Our results indicate comparableand even better performance than current stateof the art systems addressing the problem ofsentence similarity.1 IntroductionIdentifying the degree of semantic similarity [SS]between two sentences is at the core of many NLPapplications that focus on sentence level semanticssuch as Machine Translation (Kauchak and Barzi-lay, 2006), Summarization (Zhou et al, 2006), TextCoherence Detection (Lapata and Barzilay, 2005),etc.To date, almost all Sentence Similarity [SS] ap-proaches work in the high-dimensional word spaceand rely mainly on word similarity.
There are twomain (not unrelated) disadvantages to word similar-ity based approaches: 1. lexical ambiguity as thepairwise word similarity ignores the semantic inter-action between the word and its sentential context;2. word co-occurrence information is not sufficientlyexploited.Latent variable models, such as Latent SemanticAnalysis [LSA] (Landauer et al, 1998), Probabilis-tic Latent Semantic Analysis [PLSA] (Hofmann,1999), Latent Dirichlet Allocation [LDA] (Blei etal., 2003) can solve the two issues naturally by mod-eling the semantics of words and sentences simulta-neously in the low-dimensional latent space.
How-ever, attempts at addressing SS using LSA performsignificantly below high dimensional word similar-ity based models (Mihalcea et al, 2006; O?Shea etal., 2008).We believe that the latent semantics approachesapplied to date to the SS problem have not yieldedpositive results due to the deficient modeling of thesparsity in the semantic space.
SS operates in a verylimited contextual setting where the sentences aretypically very short to derive robust latent semantics.Apart from the SS setting, robust modeling of thelatent semantics of short sentences/texts is becom-ing a pressing need due to the pervasive presence ofmore bursty data sets such as Twitter feeds and SMSwhere short contexts are an inherent characteristic ofthe data.In this paper, we propose to model the missingwords (words that are not in the sentence), a fea-ture that is typically overlooked in the text model-ing literature, to address the sparseness issue for theSS task.
We define the missing words of a sentenceas the whole vocabulary in a corpus minus the ob-served words in the sentence.
Our intuition is sinceobserved words in a sentence are too few to tell uswhat the sentence is about, missing words can beused to tell us what the sentence is not about.
Weassume that the semantic space of both the observed864and missing words make up the complete semanticsprofile of a sentence.After analyzing the way traditional latent variablemodels (LSA, PLSA/LDA) handle missing words,we decide to model sentences using a weighted ma-trix factorization approach (Srebro and Jaakkola,2003), which allows us to treat observed words andmissing words differently.
We handle missing wordsusing a weighting scheme that distinguishes missingwords from observed words yielding robust latentvectors for sentences.Since we use a feature that is already implied bythe text itself, our approach is very general (similarto LSA/LDA) in that it can be applied to any formatof short texts.
In contrast, existing work on model-ing short texts focuses on exploiting additional data,e.g., Ramage et al (2010) model tweets using theirmetadata (author, hashtag, etc.
).Moreover in this paper, we introduce a new eval-uation framework for SS: Concept Definition Re-trieval (CDR).
Compared to existing data sets, theCDR data set alows for large scale tuning and test-ing of SS modules without further human annota-tion.2 Limitations of Topic Models and LSAfor Modeling SentencesUsually latent variable models aim to find a latentsemantic profile for a sentence that is most relevantto the observed words.
By explicitly modeling miss-ing words, we set another criterion to the latent se-mantics profile: it should not be related to the miss-ing words in the sentence.
However, missing wordsare not as informative as observed words, hence theneed for a model that does a good job of modelingmissing words at the right level of emphasis/impactis central to completing the semantic picture for asentence.LSA and PLSA/LDA work on a word-sentenceco-occurrence matrix.
Given a corpus, the row en-tries of the matrix are the unique M words in thecorpus, and theN columns are the sentence ids.
Theyielded M ?N co-occurrence matrix X comprisesthe TF-IDF values in each Xij cell, namely that TF-IDF value of word wi in sentence sj .
For ease ofexposition, we will illustrate the problem using aspecial case of the SS framework where the sen-tences are concept definitions in a dictionary suchas WordNet (Fellbaum, 1998) (WN).
Therefore, thesentence corresponding to the concept definition ofbank#n#1 is a sparse vector in X containing thefollowing observed words where Xij 6= 0:the 0.1, financial 5.5, institution 4, that 0.2,accept 2.1, deposit 3, and 0.1, channel 6, the 0.1,money 5, into 0.3, lend 3.5, activity 3All the other words (girl, car,..., check, loan, busi-ness,...) in matrix X that do not occur in the conceptdefinition are considered missing words for the con-cept entry bank#n#1, thereby their Xij = 0 .Topic models (PLSA/LDA) do not explicitlymodel missing words.
PLSA assumes each docu-ment has a distribution over K topics P (zk|dj), andeach topic has a distribution over all vocabulariesP (wi|zk).
Therefore, PLSA finds a topic distribu-tion for each concept definition that maximizes thelog likelihood of the corpus X (LDA has a similarform):?i?jXij log?kP (zk|dj)P (wi|zk) (1)In this formulation, missing words do not contributeto the estimation of sentence semantics, i.e., exclud-ing missing words (Xij = 0) in equation 1 does notmake a difference.However, empirical results show that given asmall number of observed words, usually topic mod-els can only find one topic (most evident topic)for a sentence, e.g., the concept definitions ofbank#n#1 and stock#n#1 are assigned the fi-nancial topic only without any further discernabil-ity.
This results in many sentences are assigned ex-actly the same semantics profile as long as they arepertaining/mentioned within the same domain/topic.The reason is topic models try to learn a 100-dimension latent vector (assume dimension K =100) from very few data points (10 observed wordson average).
It would be desirable if topic modelscan exploit missing words (a lot more data than ob-served words) to render more nuanced latent seman-tics, so that pairs of sentences in the same domaincan be differentiable.On the other hand, LSA explicitly models missingwords but not at the right level of emphasis.
LSAfinds another matrix X?
(latent vectors) with rank Kto approximate X using Singular Vector Decompo-sition (X ?
X?
= UK?KV >K ), such that the Frobe-865financial sport institution Ro Rm Ro ?Rm Ro ?
0.01Rmv1 1 0 0 20 600 -580 14v2 0.6 0 0.1 18 300 -282 15v3 0.2 0.3 0.2 5 100 -95 4Table 1: Three possible latent vectors hypotheses for the definition of bank#n#1nius norm of difference between the two matrices isminimized: ????
?i?j(X?ij ?Xij)2(2)In effect, LSA allows missing and observed wordsto equally impact the objective function.
Given theinherent short length of the sentences, LSA (equa-tion 2) allows for much more potential influencefrom missing words rather than observed words(99.9% cells are 0 in X).
Hence the contributionof the observed words is significantly diminished.Moreover, the true semantics of the concept defini-tions is actually related to some missing words, butsuch true semantics will not be favored by the objec-tive function, since equation 2 allows for too strongan impact by X?ij = 0 for any missing word.
There-fore the LSA model, in the context of short texts,is allowing missing words to have a significant ?un-controlled?
impact on the model.2.1 An ExampleThe three latent semantics profiles in table 1 il-lustrate our analysis for topic models and LSA.
As-sume there are three dimensions: financial, sports,institution.
We use Rvo to denote the sum of related-ness between latent vector v and all observed words;similarly, Rvm is the sum of relatedness between thevector v and all missing words.
The first latent vec-tor (generated by topic models) is chosen by maxi-mizing Robs = 600.
It suggests bank#n#1 is onlyrelated to the financial dimension.
The second la-tent vector (found by LSA) has the maximum valueof Robs ?Rmiss = 95, but obviously the latent vec-tor is not related to bank#n#1 at all.
This is be-cause LSA treats observed words and missing wordsequally the same, and due to the large number ofmissing words, the information of observed wordsis lost: Robs?Rmiss ?
?Rmiss.
The third vector isthe ideal semantics profile, since it is also related tothe institution dimension.
It has a slightly smallerRobs in comparison to the first vector, yet it has asubstantially smaller Rmiss.In order to favor the ideal vector over other vec-tors, we simply need to adjust the objective func-tion by assigning a smaller weight to Rmiss such as:Robs?0.01?Rmiss.
Accordingly, we use weightedmatrix factorization (Srebro and Jaakkola, 2003) tomodel missing words.3 The Proposed Approach3.1 Weighted Matrix FactorizationThe weighted matrix factorization [WMF] ap-proach is very similar to SVD, except that it allowsfor direct control on each matrix cellXij .
The modelfactorizes the original matrix X into two matricessuch that X ?
P>Q, where P is a K ?M matrix,and Q is a K ?N matrix (figure 1).The model parameters (vectors in P and Q) areoptimized by minimizing the objective function:?i?jWij (P?,i ?Q?,j ?Xij)2 + ?||P ||22 + ?||Q||22 (3)where ?
is a free regularization factor, and theweight matrix W defines a weight for each cell inX .Accordingly, P?,i is a K-dimension latent seman-tics vector profile for word wi; similarly, Q?,j is theK-dimension vector profile that represents the sen-tence sj .
Operations on these K-dimensional vec-tors have very intuitive semantic meanings:(1) the inner product of P?,i and Q?,j is used to ap-proximate semantic relatedness of word wi and sen-tence sj : P?,i ?
Q?,j ?
Xij , as the shaded parts inFigure 1;(2) equation 3 explicitly requires a sentence shouldnot be related to its missing words by forcing P?,i ?Q?,j = 0 for missing words Xij = 0.
(3) we can compute the similarity of two sentencessj and sj?
using the cosine similarity between Q?,j ,Q?,j?
.The latent vectors in P and Q are first randomlyinitialized, then can be computed iteratively by thefollowing equations (derivation is omitted due tolimited space, which can be found in (Srebro andJaakkola, 2003)):P?,i =(QW?
(i)Q> + ?I)?1QW?
(i)X>i,?Q?,j =(PW?
(j)P> + ?I)?1PW?
(i)X?,j(4)866Figure 1: Matrix Factorizationwhere W?
(i) = diag(W?,i) is an M ?
M diagonalmatrix containing ith row of weight matrixW .
Sim-ilarly, W?
(j) = diag(W?,j) is an N ?
N diagonalmatrix containing jth column of W .3.2 Modeling Missing WordsIt is straightforward to implement the idea in Sec-tion 2.1 (choosing a latent vector that maximizesRobs ?
0.01 ?
Rmiss) in the WMF framework, byassigning a small weight for all the missing wordsand minimizing equation 3:Wi,j ={1, if Xij 6= 0wm, if Xij = 0(5)We refer to our model as Weighted Textual MatrixFactorization [WTMF].
1This solution is quite elegant: 1. it explicitly tellsthe model that in general all missing words shouldnot be related to the sentence; 2. meanwhile latentsemantics are mainly generalized based on observedwords, and the model is not penalized too much(wm is very small) when it is very confident thatthe sentence is highly related to a small subset ofmissing words based on their latent semantics pro-files (bank#n#1 definition sentence is related to itsmissing words check loan).We adopt the same approach (assigning a smallweight for some cells in WMF) proposed for rec-ommender systems [RS] (Steck, 2010).
In RS, anincomplete rating matrix R is proposed, where rowsare users and columns are items.
Typically, a userrates only some of the items, hence, the RS systemneeds to predict the missing ratings.
Steck (2010)guesses a value for all the missing cells, and sets asmall weight for those cells.Compared to (Steck, 2010), we are facing a differ-ent problem and targeting a different goal.
We havea full matrix X where missing words have a 0 value,while the missing ratings in RS are unavailable ?
thevalues are unknown, henceR is not complete.
In theRS setting, they are interested in predicting individ-ual ratings, while we are interested in the sentence1An efficient way to compute equation 4 is proposed in(Steck, 2010).semantics.
More importantly, they do not have thesparsity issue (each user has rated over 100 items inthe movie lens data2) and robust predictions can bemade based on the observed ratings alone.4 Evaluation for SSWe need to show the impact of our proposed modelWTMF on the SS task.
However we are faced witha problem, the lack of a suitable large evaluation setfrom which we can derive robust observations.
Thetwo data sets we know of for SS are: 1. human-ratedsentence pair similarity data set (Li et al, 2006)[LI06]; 2. the Microsoft Research Paraphrase Cor-pus (Dolan et al, 2004) [MSR04].
The LI06 dataset consists of 65 pairs of noun definitions selectedfrom the Collin Cobuild Dictionary.
A subset of 30pairs is further selected by LI06 to render the sim-ilarity scores evenly distributed.
While this is theideal data set for SS, the small size makes it impos-sible for tuning SS algorithms or deriving significantperformance conclusions.On the other hand, the MSR04 data set comprisesa much larger set of sentence pairs: 4,076 trainingand 1,725 test pairs.
The ratings on the pairs arebinary labels: similar/not similar.
This is not a prob-lem per se, however the issue is that it is very strictin its assignment of a positive label, for examplethe following sentence pair as cited in (Islam andInkpen, 2008) is rated not semantically similar:Ballmer has been vocal in the past warning thatLinux is a threat to Microsoft.In the memo, Ballmer reiterated the open-sourcethreat to Microsoft.We believe that the ratings on a data set for SSshould accommodate variable degrees of similaritywith various ratings, however such a large scale setdoes not exist yet.
Therefore for purposes of evaluat-ing our proposed approach we devise a new frame-work inspired by the LI06 data set in that it com-prises concept definitions but on a large scale.4.1 Concept Definition RetrievalWe define a new framework for evaluating SS andproject it as a Concept Definition Retrieval (CDR)task where the data points are dictionary definitions.The intuition is that two definitions in different dic-2http://www.grouplens.org/node/73, with 1M data set beingthe most widely used.867tionaries referring to the same concept should be as-signed large similarity.
In this setting, we design theCDR task in a search engine style.
The SS algorithmhas access to all the definitions in WordNet (WN).Given an OntoNotes (ON) definition (Hovy et al,2006), the SS algorithm should rank the equivalentWN definition as high as possible based on sentencesimilarity.The manual mapping already exists for ON toWN.
One ON definition can be mapped to sev-eral WN definitions.
After preprocessing we obtain13669 ON definitions mapped to 19655 WN defini-tions.
The data set has the advantage of being verylarge and it doesn?t require further human scrutiny.After the SS model learns the co-occurrence ofwords from WN definitions, in the testing phase,given an ON definition d, the SS algorithm needs toidentify the equivalent WN definitions by comput-ing the similarity values between all WN definitionsand the ON definition d, then sorting the values indecreasing order.
Clearly, it is very difficult to rankthe one correct definition as highest out of all WNdefinitions (110,000 in total), hence we use ATOPd,area under the TOPKd(k) recall curve for an ONdefinition d, to measure the performance.
Basically,it is the ranking of the correct WN definition amongall WN definitions.
The higher a model is able torank the correct WN definition, the better its perfor-mance.Let Nd be the number of aligned WN definitionsfor the ON definition d, and Nkd be the number ofaligned WN definitions in the top-k list.
Then witha normalized k ?
[0,1], TOPKd(k) and ATOPd isdefined as:TOPKd(k) = Nkd /NdATOPd =?
10TOPKd(k)dk(6)ATOPd computes the normalized rank (in the rangeof [0, 1]) of aligned WN definitions among all WNdefinitions, with value 0.5 being the random case,and 1 being ranked as most similar.5 Experiments and ResultsWe evaluate WTMF on three data sets: 1.
CDRdata set using ATOP metric; 2.
Human-rated Sen-tence Similarity data set [LI06] using Pearson andSpearman Correlation; 3.
MSR Paraphrase corpus[MSR04] using accuracy.The performance of WTMF on CDR is com-pared with (a) an Information Retrieval model (IR)that is based on surface word matching, (b) an n-gram model (N-gram) that captures phrase overlapsby returning the number of overlapping ngrams asthe similarity score of two sentences, (c) LSA thatuses svds() function in Matlab, and (d) LDA thatuses Gibbs Sampling for inference (Griffiths andSteyvers, 2004).
WTMF is also compared with allexisting reported SS results on LI06 and MSR04data sets, as well as LDA that is trained on thesame data as WTMF.
The similarity of two sentencesis computed by cosine similarity (except N-gram).More details on each task will be explained in thesubsections.To eliminate randomness in statistical models(WTMF and LDA), all the reported results are aver-aged over 10 runs.
We run 20 iterations for WTMF.And we run 5000 iterations for LDA; each LDAmodel is averaged over the last 10 Gibbs Samplingiterations to get more robust predictions.The latent vector of a sentence is computed by:(1) using equation 4 in WTMF, or (2) summingup the latent vectors of all the constituent wordsweighted by Xij in LSA and LDA, similar to thework reported in (Mihalcea et al, 2006).
For LDAthe latent vector of a word is computed by P (z|w).It is worth noting that we could directly use the es-timated topic distribution ?j to represent a sentence,however, as discussed the topic distribution has onlynon-zero values on one or two topics, leading to alow ATOP value around 0.8.5.1 CorpusThe corpus we use comprises three dictionariesWN, ON, Wiktionary [Wik],3 Brown corpus.
Forall dictionaries, we only keep the definitions withoutexamples, and discard the mapping between senseids and definitions.
All definitions are simply treatedas individual documents.
We crawl Wik and removethe entries that are not tagged as noun, verb, adjec-tive, or adverb, resulting in 220, 000 entries.
For theBrown corpus, each sentence is treated as a docu-ment in order to create more coherent co-occurrencevalues.
All data is tokenized, pos-tagged4, and lem-3http://en.wiktionary.org/wiki/Wiktionary:Main Page4http://nlp.stanford.edu/software/tagger.shtml868Models Parameters Dev Test1.
IR - 0.8578 0.85152.
N-gram - 0.8238 0.81713.
LSA - 0.8218 0.81434a.
LDA ?
= 0.1, ?
= 0.01 0.9466?
0.0020 0.9427?
0.00064b.
LDA ?
= 0.05, ?
= 0.05 0.9506?
0.0017 0.9470?
0.00055.
WTMF wm = 1, ?
= 0 0.8273?
0.0028 0.8273?
0.00146.
WTMF wm = 0, ?
= 20 0.8745?
0.0058 0.8645?
0.00317a.
WTMF wm = 0.01, ?
= 20 0.9555?
0.0015 0.9511?
0.00037b.
WTMF wm = 0.0005, ?
= 20 0.9610?
0.0011 0.9558?
0.0004Table 2: ATOP Values of Models (K = 100 for LSA/LDA/WTMF)matized5.
The importance of words in a sentence isestimated by the TF-IDF schema.All the latent variable models (LSA, LDA,WTMF) are built on the same set of cor-pus: WN+Wik+Brown (393, 666 sentences and4, 262, 026 words).
Words that appear only once areremoved.
The test data is never used during trainingphrase.5.2 Concept Definition RetrievalAmong the 13669 ON definitions, 1000 defini-tions are randomly selected as a development set(dev) for picking best parameters in the models, andthe rest is used as a test set (test).
The performanceof each model is evaluated by the average ATOPdvalue over the 12669 definitions (test).
We use thesubscript set in ATOPset to denote the average ofATOPd of a set of ON definitions, where d ?
{set}.If all the words in an ON definition are not coveredin the training data (WN+Wik+Br), then ATOPd forthis instance is set to 0.5.To compute ATOPd for an ON definition effi-ciently, we use the rank of the aligned WN definitionamong a random sample (size=1000) of WN defini-tions, to approximate its rank among all WN defini-tions.
In practice, the difference between using 1000samples and all data is tiny for ATOPtest (?0.0001),due to the large number of data points in CDR.We mainly compare the performance of IR, N-gram, LSA, LDA, and WTMF models.
Generallyresults are reported based on the last iteration.
How-ever, we observe that for model 6 in table 2, the bestperformance occurs at the first few iterations.
Hencefor that model we use the ATOPdev to indicate whento stop.5http://wn-similarity.sourceforge.net, WordNet::QueryData5.2.1 ResultsTable 2 summarizes the ATOP values on the devand test sets.
All parameters are tuned based on thedev set.
In LDA, we choose an optimal combinationof ?
and ?
from {0.01, 0.05, 0.1, 0.5}.In WTMF, wechoose the best parameters of weight wm for miss-ing words and ?
for regularization.
We fix the di-mension K = 100.
Later in section 5.2.2, we willsee that a larger value of K can further improve theperformance.WTMF that models missing words using a smallweight (model 7b with wm = 0.0005) outperformsthe second best model LDA by a large margin.
Thisis because LDA only uses 10 observed words to infera 100 dimension vector for a sentence, while WTMFtakes advantage of much more missing words tolearn more robust latent semantics vectors.The IR model that works in word space achievesbetter ATOP scores than N-gram, although the ideaof N-gram is commonly used in detecting para-phrases as well as machine translation.
ApplyingTF-IDF for N-gram is better, but still the ATOPtest isnot higher: 0.8467.
The reason is words are enoughto capture semantics for SS, while n-grams/phrasesare used for a more fine-grained level of semantics.We also present model 5 and 6 (both are WTMF),to show the impact of: 1. modeling missing wordswith equal weights as observed words (wm = 1)(LSA manner), and 2. not modeling missing wordsat all (wm = 0) (LDA manner) in the context ofWTMF model.
As expected, both model 5 andmodel 6 generate much worse results.Both LDA and model 6 ignore missing words,with better ATOPtest scores achieved by LDA.
Thismay be due to the different inference algorithms.Model 5 and LSA are comparable, where missingwords are used with a large weight.
Both of themyield low results.
This confirms our assumption8690.0001 0.0005 0.001 0.005 0.01 0.050.940.9450.950.955wmATOPWTMFFigure 2: missing words weight wm in WTMF50 100 1500.940.9450.950.955KATOPWTMFLDAFigure 3: dimension K in WTMF and LDAthat allowing for equal impact of both observed andmissing words is not the correct characterization ofthe semantic space.5.2.2 AnalysisIn these latent variable models, there are severalessential parameters: weight of missing words wm,and dimensionK.
Figure 2 and 3 analyze the impactof these parameters on ATOPtest.Figure 2 shows the influence of wm on ATOPtestvalues.
The peak ATOPtest is around wm = 0.0005,while other values of wm (except wm = 0.05) alsoyield high ATOP values (better than LDA).We also measure the influence of the dimensionK = {50, 75, 100, 125, 150} on LDA and WTMFin Figure 3, where parameters for WTMF are wm =0.0005, ?
= 20, and for LDA are ?
= 0.05, ?
=0.05.
We can see WTMF consistently outperformsLDA by an ATOP value of 0.01 in each dimension.Although a larger K yields a better result, we stilluse a 100 due to computational complexity.5.3 LI06: Human-rated Sentence SimilarityWe also assess WTMF and LDA model on LI06data set.
We still use K = 100.
As we can seein Figure 2, choosing the appropriate parameter wmcould boost the performance significantly.
Since wedo not have any tuning data for this task, we presentPearson?s correlation r for different values of wm inTable 3.
In addition, to demonstrate that wm doesnot overfit the 30 data points, we also evaluate on30 pairs 35 pairswm r ?
r ?0.0005 0.8247 0.8440 0.4200 0.60060.001 0.8470 0.8636 0.4308 0.59850.005 0.8876 0.8966 0.4638 0.58090.01 0.8984 0.9091 0.4564 0.54500.05 0.8804 0.8812 0.4087 0.4766Table 3: Different wm of WTMF on LI06 (K = 100)the other 35 pairs in LI06.
Same as in (Tsatsaroniset al, 2010), we also include Spearman?s rank ordercorrelation ?, which is correlation of ranks of simi-larity values .
Note that r and ?
are much lower for35 pairs set, since most of the sentence pairs havea very low similarity (the average similarity valueis 0.065 in 35 pairs set and 0.367 in 30 pairs set)and SS models need to identify the tiny differenceamong them, thereby rendering this set much harderto predict.Using wm = 0.01 gives the best results on 30pairs while on 35 pairs the peak values of r and ?happens when wm = 0.005.
In general, the cor-relations in 30 pairs and in 35 pairs are consistent,which indicates wm = 0.01 or wm = 0.005 doesnot overfit the 30 pairs set.Compared to CDR, LI06 data set has a strongpreference for a larger wm.
This could be caused bydifferent goals of the two tasks: CDR is evaluatedby the rank of the most similar ones among all can-didates, while the LI06 data set treats similar pairsand dissimilar pairs as equally important.
Using asmaller wm means the similarity score is computedmainly from semantics of the observed words.
Thisbenefits CDR, since it gives more accurate similarityscores for those similar pairs, but not so accurate fordissimilar pairs.
In fact, from Figure 2 and Table 2we see that wm = 0.01 also produces a very highATOPtest value in CDR.Table 4 shows the results of all current SS modelswith respect to the LI06 data set (30 pairs set).
Wecite their best performance for all reported results.Once the correct wm = 0.01 is chosen, WTMFresults in the best Pearson?s r and best Spearman?s?
(wm = 0.005 yields the second best r and ?
).Same as in CDR task, WTMF outperforms LDA bya large margin in both r and ?.
It indicates that thelatent vectors induced by WTMF are able to not onlyidentify same/similar sentences, but also identify the?correct?
degree of dissimilar sentences.870Model r ?STASIS (Li et al, 2006) 0.8162 0.8126(Liu et al, 2007) 0.841 0.8538(Feng et al, 2008) 0.756 0.608STS (Islam and Inkpen, 2008) 0.853 0.838LSA (O?Shea et al, 2008) 0.8384 0.8714Omiotis (Tsatsaronis et al, 2010) 0.856 0.8905WSD-STS (Ho et al, 2010) 0.864 0.8341SPD-STS (Ho et al, 2010) 0.895 0.9034LDA (?
= 0.05, ?
= 0.05) 0.8422 0.8663WTMF (wm = 0.005, ?
= 20) 0.8876 0.8966WTMF (wm = 0.01, ?
= 20) 0.8984 0.9091Table 4: Pearson?s correlation r and Spearman?s corre-lation ?
on LI06 30 pairsModel AccuracyRandom 51.3LSA (Mihalcea et al, 2006) 68.4full model (Mihalcea et al, 2006) 70.3STS (Islam and Inkpen, 2008) 72.6Omiotis (Tsatsaronis et al, 2010) 69.97LDA (?
= 0.05, ?
= 0.05) 68.6WTMF (wm = 0.01, ?
= 20) 71.51Table 5: Performance on MSR04 test set5.4 MSR04: MSR Paraphrase CorpusFinally, we briefly discuss results of applyingWTMF on MSR04 data.
We use the same pa-rameter setting used for the LI06 evaluation set-ting since both sets are human-rated sentence pairs(?
= 20, wm = 0.01,K = 100).
We use the train-ing set of MSR04 data to select a threshold of sen-tence similarity for the binary label.
Table 5 sum-marizes the accuracy of other SS models noted inthe literature and evaluated on MSR04 test set.Compared to previous SS work and LDA, WTMFhas the second best accuracy.
It suggests that WTMFis quite competitive in the paraphrase recognitiontask.It is worth noting that the best system on MSR04,STS (Islam and Inkpen, 2008), has much lower cor-relations on LI06 data set.
The second best systemamong previous work on LI06 uses Spearman cor-relation, Omiotis (Tsatsaronis et al, 2010), and ityields a much worse accuracy on MSR04.
The otherworks do not evaluate on both data sets.6 Related WorkAlmost all current SS methods work in the high-dimensional word space, and rely heavily onword/sense similarity measures, which is knowledgebased (Li et al, 2006; Feng et al, 2008; Ho et al,2010; Tsatsaronis et al, 2010), corpus-based (Islamand Inkpen, 2008) or hybrid (Mihalcea et al, 2006).Almost all of them are evaluated on LI06 data set.
Itis interesting to see that most works find word sim-ilarity measures, especially knowledge based ones,to be the most effective component, while other fea-tures do not work well (such as word order or syn-tactic information).
Mihalcea et al (2006) use LSAas a baseline, and O?Shea et al (2008) train LSAon regular length documents.
Both results are con-siderably lower than word similarity based methods.Hence, our work is the first to successfully approachSS in the latent space.Although there has been work modeling latent se-mantics for short texts (tweets) in LDA, the focushas been on exploiting additional features in Twit-ter, hence restricted to Twitter data.
Ramage et al(2010) use tweet metadata (author, hashtag) as somesupervised information to model tweets.
Jin et al(2011) use long similar documents (the article thatis referred by a url in tweets) to help understand thetweet.
In contrast, our approach relies solely on theinformation in the texts by modeling local missingwords, and does not need any additional data, whichrenders our approach much more widely applicable.7 ConclusionsWe explicitly model missing words to alleviate thesparsity problem in modeling short texts.
We alsopropose a new evaluation framework for sentencesimilarity that allows large scale tuning and test-ing.
Experiment results on three data sets show thatour model WTMF significantly outperforms existingmethods.
For future work, we would like to comparethe text modeling performance of WTMF with LSAand LDA on regular length documents.AcknowledgmentsWe would like to thank the anonymous reviewers fortheir valuable comments and suggestions to improvethe quality of the paper.This research was funded by the Office of the Di-rector of National Intelligence (ODNI), IntelligenceAdvanced Research Projects Activity (IARPA),through the U.S. Army Research Lab.
All state-ments of fact, opinion or conclusions containedherein are those of the authors and should not beconstrued as representing the official views or poli-cies of IARPA, the ODNI or the U.S. Government.871ReferencesDavid M. Blei, Andrew Y. Ng, and Michael I. Jordan.2003.
Latent dirichlet alocation.
Journal of MachineLearning Research, 3.William Dolan, Chris Quirk, and Chris Brockett.
2004.Unsupervised construction of large paraphrase cor-pora: Exploiting massively parallel news sources.
InProceedings of the 20th International Conference onComputational Linguistics.Christiane Fellbaum.
1998.
WordNet: An ElectronicLexical Database.
MIT Press.Jin Feng, Yi-Ming Zhou, and Trevor Martin.
2008.
Sen-tence similarity based on relevance.
In Proceedings ofIPMU.Thomas L. Griffiths and Mark Steyvers.
2004.
Find-ing scientific topics.
Proceedings of the NationalAcademy of Sciences, 101.Chukfong Ho, Masrah Azrifah Azmi Murad, Rabiah Ab-dul Kadir, and Shyamala C. Doraisamy.
2010.
Wordsense disambiguation-based sentence similarity.
InProceedings of the 23rd International Conference onComputational Linguistics.Thomas Hofmann.
1999.
Probabilistic latent semanticindexing.
In Proceedings of the 22nd annual interna-tional ACM SIGIR conference on Research and devel-opment in information retrieval.Eduard Hovy, Mitchell Marcus, Martha Palmer, LanceRamshaw, and Ralph Weischedel.
2006.
Ontonotes:The 90% solution.
In Proceedings of the Human Lan-guage Technology Conference of the North AmericanChapter of the ACL.Aminul Islam and Diana Inkpen.
2008.
Semantictext similarity using corpus-based word similarity andstring similarity.
ACM Transactions on KnowledgeDiscovery from Data, 2.Ou Jin, Nathan N. Liu, Kai Zhao, Yong Yu, and QiangYang.
2011.
Transferring topical knowledge fromauxiliary long texts for short text clustering.
In Pro-ceedings of the 20th ACM international conference onInformation and knowledge management.David Kauchak and Regina Barzilay.
2006.
Paraphras-ing for automatic evaluation.
In Proceedings of theHuman Language Technology Conference of the NorthAmerican Chapter of the ACL.Thomas K Landauer, Peter W. Foltz, and Darrell Laham.1998.
An introduction to latent semantic analysis.Discourse Processes, 25.Mirella Lapata and Regina Barzilay.
2005.
Automaticevaluation of text coherence: Models and representa-tions.
In Proceedings of the 19th International JointConference on Artificial Intelligence.Yuhua Li, Davi d McLean, Zuhair A. Bandar, James D. OShea, and Keeley Crockett.
2006.
Sentence similar-ity based on semantic nets and corpus statistics.
IEEETransaction on Knowledge and Data Engineering, 18.Xiao-Ying Liu, Yi-Ming Zhou, and Ruo-Shi Zheng.2007.
Sentence similarity based on dynamic timewarping.
In The International Conference on Seman-tic Computing.Rada Mihalcea, Courtney Corley, and Carlo Strapparava.2006.
Corpus-based and knowledge-based measuresof text semantic similarity.
In Proceedings of the 21stNational Conference on Articial Intelligence.James O?Shea, Zuhair Bandar, Keeley Crockett, andDavid McLean.
2008.
A comparative study of twoshort text semantic similarity measures.
In Proceed-ings of the Agent and Multi-Agent Systems: Technolo-gies and Applications, Second KES International Sym-posium (KES-AMSTA).Daniel Ramage, Susan Dumais, and Dan Liebling.
2010.Characterizing microblogs with topic models.
In Pro-ceedings of the Fourth International AAAI Conferenceon Weblogs and Social Media.Nathan Srebro and Tommi Jaakkola.
2003.
Weightedlow-rank approximations.
In Proceedings of the Twen-tieth International Conference on Machine Learning.Harald Steck.
2010.
Training and testing of recom-mender systems on data missing not at random.
InProceedings of the 16th ACM SIGKDD InternationalConference on Knowledge Discovery and Data Min-ing.George Tsatsaronis, Iraklis Varlamis, and Michalis Vazir-giannis.
2010.
Text relatedness based on a word the-saurus.
Journal of Articial Intelligence Research, 37.Liang Zhou, Chin-Yew Lin, Dragos Stefan Munteanu,and Eduard Hovy.
2006.
Paraeval: Using paraphrasesto evaluate summaries automatically.
In Proceedingsof Human Language Tech-nology Conference of theNorth American Chapter of the ACL,.872
