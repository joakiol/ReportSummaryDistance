Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and ComputationalNatural Language Learning, pp.
791?800, Prague, June 2007. c?2007 Association for Computational LinguisticsSemi-Supervised Structured Output Learningbased on a Hybrid Generative and Discriminative ApproachJun Suzuki, Akinori Fujino and Hideki IsozakiNTT Communication Science Laboratories, NTT Corp.2-4 Hikaridai, Seika-cho, Soraku-gun, Kyoto, 619-0237 Japan{jun, a.fujino, isozaki}@cslab.kecl.ntt.co.jpAbstractThis paper proposes a framework forsemi-supervised structured output learning(SOL), specifically for sequence labeling,based on a hybrid generative and discrim-inative approach.
We define the objectivefunction of our hybrid model, which is writ-ten in log-linear form, by discriminativelycombining discriminative structured predic-tor(s) with generative model(s) that incor-porate unlabeled data.
Then, unlabeleddata is used in a generative manner to in-crease the sum of the discriminant functionsfor all outputs during the parameter estima-tion.
Experiments on named entity recogni-tion (CoNLL-2003) and syntactic chunking(CoNLL-2000) data show that our hybridmodel significantly outperforms the state-of-the-art performance obtained with super-vised SOL methods, such as conditional ran-dom fields (CRFs).1 IntroductionStructured output learning (SOL) methods, whichattempt to optimize an interdependent output spaceglobally, are important methodologies for certainnatural language processing (NLP) tasks such aspart-of-speech tagging, syntactic chunking (Chunk-ing) and named entity recognition (NER), which arealso referred to as sequence labeling tasks.
When weconsider the nature of these sequence labeling tasks,a semi-supervised approach appears to be more nat-ural and appropriate.
This is because the number offeatures and parameters typically become extremelylarge, and labeled examples can only sparsely coverthe parameter space, even if thousands of labeled ex-amples are available.
In fact, many attempts have re-cently been made to develop semi-supervised SOLmethods (Zhu et al, 2003; Li and McCallum, 2005;Altun et al, 2005; Jiao et al, 2006; Brefeld andScheffer, 2006).With the generative approach, we can easily in-corporate unlabeled data into probabilistic modelswith the help of expectation-maximization (EM) al-gorithms (Dempster et al, 1977).
For example, theBaum-Welch algorithm is a well-known algorithmfor training a hidden Markov model (HMM) of se-quence learning.
Generally, with sequence learningtasks such as NER and Chunking, we cannot expectto obtain better performance than that obtained us-ing discriminative approaches in supervised learningsettings.In contrast to the generative approach, with thediscriminative approach, it is not obvious how un-labeled training data can be naturally incorporatedinto a discriminative training criterion.
For ex-ample, the effect of unlabeled data will be elimi-nated from the objective function if the unlabeleddata is directly used in traditional i.i.d.
conditional-probability models.
Nevertheless, several attemptshave recently been made to incorporate unlabeleddata in the discriminative approach.
An approachbased on pairwise similarities, which encouragenearby data points to have the same class label, hasbeen proposed as a way of incorporating unlabeleddata discriminatively (Zhu et al, 2003; Altun et al,2005; Brefeld and Scheffer, 2006).
However, thisapproach generally requires joint inference over thewhole data set for prediction, which is not practi-cal as regards the large data sets used for standardsequence labeling tasks in NLP.
Another discrim-inative approach to semi-supervised SOL involvesthe incorporation of an entropy regularizer (Grand-791valet and Bengio, 2004).
Semi-supervised condi-tional random fields (CRFs) based on a minimumentropy regularizer (SS-CRF-MER) have been pro-posed in (Jiao et al, 2006).
With this approach, theparameter is estimated to maximize the likelihood oflabeled data and the negative conditional entropy ofunlabeled data.
Therefore, the structured predictoris trained to separate unlabeled data well under theentropy criterion by parameter estimation.In contrast to these previous studies, this paperproposes a semi-supervised SOL framework basedon a hybrid generative and discriminative approach.A hybrid approach was first proposed in a super-vised learning setting (Raina et al, 2003) for textclassification.
(Fujino et al, 2005) have developed asemi-supervised approach by discriminatively com-bining a supervised classifier with generative mod-els that incorporate unlabeled data.
We extend thisframework to the structured output domain, specifi-cally for sequence labeling tasks.
Moreover, we re-formalize the objective function to allow the incor-poration of discriminative models (structured pre-dictors) trained from labeled data, since the originalframework only considers the combination of gen-erative classifiers.
As a result, our hybrid model cansignificantly improve on the state-of-the-art perfor-mance obtained with supervised SOL methods, suchas CRFs, even if a large amount of labeled data isavailable, as shown in our experiments on CoNLL-2003 NER and CoNLL-2000 Chunking data.
Inaddition, compared with SS-CRF-MER, our hybridmodel has several good characteristics including alow calculation cost and a robust optimization interms of a sensitiveness of hyper-parameters.
Thisis described in detail in Section 5.3.2 Supervised SOL: CRFsThis paper focuses solely on sequence labelingtasks, such as named entity recognition (NER) andsyntactic chunking (Chunking), as SOL problems.Thus, let x=(x1, .
.
.
, xS)?X be an input sequence,and y=(y0, .
.
.
, yS+1)?Y be a particular output se-quence, where y0 and yS+1 are special fixed labelsthat represent the beginning and end of a sequence.As regards supervised sequence learning, CRFsare recently introduced methods that constitute flex-ible and powerful models for structured predictorsbased on undirected graphical models that have beenglobally conditioned on a set of inputs (Laffertyet al, 2001).
Let ?
be a parameter vector andf(ys?1, ys,x) be a (local) feature vector obtainedfrom the corresponding position s given x. CRFsdefine the conditional probability, p(y|x), as beingproportional to a product of potential functions onthe cliques.
That is, p(y|x) on a (linear-chain) CRFcan be defined as follows:p(y|x;?)
= 1Z(x)S+1?s=1exp(?
?
f (ys?1, ys,x)).Z(x) =?y?S+1s=1 exp(?
?
f(ys?1, ys,x)) is a nor-malization factor over all output values, Y , and isalso known as the partition function.For parameter estimation (training), given labeleddata Dl = {(xk,yk)}Kk=1, the Maximum a Posteri-ori (MAP) parameter estimation, namely maximiz-ing log p(?|Dl), is now the most widely used CRFtraining criterion.
Thus, we maximize the followingobjective function to obtain optimal ?:LCRF(?)
=?k[?
?
?sf s ?
logZ(xk)]+ log p(?
), (1)where f s is an abbreviation of f(ys?1, ys,x) andp(?)
is a prior probability distribution of ?.
Agradient-based optimization algorithm such as L-BFGS (Liu and Nocedal, 1989) is widely used formaximizing Equation (1).
The gradient of Equation(1) can be written as follows:?LCRF(?)
=?kEp?(yk,xk;?
)[?sf s]??kEp(Y|xk;?
)[?sf s]+?
log p(?
).Calculating Ep(Y|x,?)
as well as the partition func-tion Z(x) is not always tractable.
However, forlinear-chain CRFs, a dynamic programming algo-rithm similar in nature to the forward-backward al-gorithm in HMMs has already been developed foran efficient calculation (Lafferty et al, 2001).For prediction, the most probable output, that is,y?
= argmaxy?Y p(y|x;?
), can be efficiently ob-tained by using the Viterbi algorithm.3 Hybrid Generative and DiscriminativeApproach to Semi-Supervised SOLIn this section, we describe our formulation of ahybrid approach to SOL and a parameter estima-tion method for sequence predictors.
We assume792that we have a set of labeled and unlabeled data,D = {Dl,Du}, where Dl = {(xn,yn)}Nn=1 andDu = {xm}Mm=1.Let us assume that we have I-units of discrimina-tive models, pDi , and J-units of generative models,pGj .
Our hybrid model for a structured predictor isdesigned by the discriminative combination of sev-eral joint probability densities of x and y, p(x,y).That is, the posterior probability of our hybrid modelis defined by providing the log-values of p(x,y) asthe features of a log-linear model, such that:R(y|x;?,?,?
)=?i pDi (x,y;?i)?i?j pGj (x,y; ?j)?j?y?i pDi (x,y;?i)?i?j pGj (x,y; ?j)?j=?i pDi (y|x;?i)?i?j pGj (x,y; ?j)?j?y?i pDi (y|x;?i)?i?j pGj (x,y; ?j)?j.
(2)Here, ?
= {{?i}Ii=1, {?j}I+Jj=I+1} represents thediscriminative combination weight of each modelwhere ?i,?j?
[0, 1].
Moreover, ?={?i}Ii=1 and ?={?j}Jj=1 represent model parameters of individualmodels estimated from labeled and unlabeled data,respectively.
Using pD(x,y) = pD(y|x)pD(x), wecan derive the third line from the second line, wherepDi (x;?i)?i for all i are canceled out.
Thus, our hy-brid model is constructed by combining discrimina-tive models, pDi (y|x;?i), with generative models,pGj (x,y;?j).Hereafter, let us assume that our hybrid modelconsists of CRFs for discriminative models, pDi , andHMMs for generative models, pGj , shown in Equa-tion (2), since this paper focuses solely on sequencemodeling.
For HMMs, we consider a first orderHMM defined in the following equation:p(x,y|?)
=S+1?s=1?ys?1,ys?ys,xs ,where ?ys?1,ys and ?ys,xs represent the transitionprobability between states ys?1 and ys and the sym-bol emission probability of the s-th position of thecorresponding input sequence, respectively, where?yS+1,xS+1 = 1.It can be seen that the formalization in the log-linear combination of our hybrid model is very sim-ilar to that of LOP-CRFs (Smith et al, 2005).
Infact, if we only use a combination of discriminativemodels (CRFs), which is equivalent to ?j = 0 forall j, we obtain essentially the same objective func-tion as that of the LOP-CRFs.
Thus, our frameworkcan also be seen as an extension of LOP-CRFs thatenables us to incorporate unlabeled data.3.1 Discriminative CombinationFor estimating the parameter ?, let us assume thatwe already have discriminatively trained models onlabeled data, pDi (y|x;?i).
We maximize the fol-lowing objective function for estimating parameter?
under a fixed ?:LHySOL(?|?)
=?nlogR(yn|xn;?,?,?
)+log p(?).
(3)where p(?)
is a prior probability distribution of ?.The value of ?
providing a global maximum ofLHySOL(?|?)
is guaranteed under an arbitrary fixedvalue in the ?
domain, since LHySOL(?|?)
is a con-cave function of ?.
Thus, we can easily maximizeEquation (3) by using a gradient-based optimizationalgorithm such as (bound constrained) L-BFGS (Liuand Nocedal, 1989).3.2 Incorporating Unlabeled DataWe cannot directly incorporate unlabeled data fordiscriminative training such as Equation (3) sincethe correct outputs y for unlabeled data are un-known.
On the other hand, generative approachescan easily deal with unlabeled data as incompletedata (data with missing variable y) by using a mix-ture model.
A well-known way to achieve this in-corporation is to maximize the log likelihood of un-labeled data with respect to the marginal distributionof generative models asL(?)
=?mlog?yp(xm,y; ?
).In fact, (Nigam et al, 2000) have reported that usingunlabeled data with a mixture model can improvethe text classification performance.According to Bayes?
rule, p(y|x;?)
?p(x,y;?
), the discriminant functions of gener-ative classifiers are provided by generative modelsp(x,y;?).
Therefore, we can regard L(?)
as thelogarithm of the sum of discriminant functions forall missing variables y of unlabeled data.
Followingthis view, we can directly incorporate unlabeleddata into our hybrid model by maximizing the793discriminant functions g of our hybrid model inthe same way as for a mixture model as explainedabove.
Thus, we maximize the following objectivefunction for estimating the model parameters ?
forgenerative models of unlabeled data:G(?|?)
=?mlog?yg(xm,y;?)
+ log p(?).
(4)where p(?)
is a prior probability distribution of ?.Here, the discriminant function g of output y giveninput x in our hybrid model can be obtained by thenumerator on the third line of Equation (2), since thedenominator does not affect the determination of y,that is,g(x,y;?)
=?ipDi (y|x;?i)?i?jpGj (x,y; ?j)?j .Under a fixed ?, we can estimate the local max-imum of G(?|?)
around the initialized value of ?by an iterative computation such as the EM algo-rithm (Dempster et al, 1977).
Let ???
and ??
beestimates of ?
in the next and current steps, respec-tively.
Using Jensen?s inequality, log a ?
a ?
1,we obtain a Q-function that satisfies the inequalityG(???|?)?G(??|?)?Q(???,??;?)?Q(??,??;?
),such thatQ(???,??;?)=?j?j?m?yR(y|xm;?,??,?)
log pGj (xm,y;???
)+ log p(???).
(5)Since Q(??,??;?)
is independent of ??
?, we canimprove the value of G(?|?)
by computing ???
tomaximize Q(???,??;?).
We can obtain a ?
es-timate by iteratively performing this update whileG(?|?)
is hill climbing.As shown in Equation (5), R is used for estimat-ing the parameter ?.
The intuitive effect of maxi-mizing Equation (4) is similar to performing ?soft-clustering?.
That is, unlabeled data is clustered withrespect to the R distribution, which also includes in-formation about labeled data, under the constraint ofgenerative model structures.3.3 Parameter Estimation ProcedureAccording to our definition, the ?
and ?
estima-tions are mutually dependent.
That is, the param-eters of the hybrid model, ?, should be estimated1.Given training set: Du = {xm}Mm=1 andDl = {D?l = {(xk,yk)}Kk=1, D?
?l = {(xn,yn)}Nn=1}2.Compute ?, using D?l.3.Initialize ?
(0), ?
(0) and t ?
0.4.Perform the following until |?(t+1)??(t)||?
(t)| < ?.4.1.
Compute ?
(t+1) to maximize Equation (4)under fixed ?
(t) and ?
using Du.4.2.
Compute ?
(t+1) to maximize Equation (3)under fixed ?
(t+1) and ?
using D?
?l .4.3. t ?
t + 1.5.Output a structured predictor R(y|x,?,?(t),?
(t)).Figure 1: Algorithm of learning model parametersused in our hybrid model.using Equation (3) with a fixed ?, while the param-eters of the generative models, ?, should be esti-mated using Equation (4) with a fixed ?.
As a solu-tion to our parameter estimation, we search for the?
and ?
that maximize LHySOL(?|?)
and G(?|?)simultaneously.
For this search, we compute ?
and?
by maximizing the objective functions shown inEquations (4) and (3) iteratively and alternately.
Wesummarize the algorithm for estimating these modelparameters in Figure 1.Note that during the ?
estimation (procedure 4.2in Figure 1), ?
can be over-fitted to the labeled train-ing data if we use the same labeled training data asused for the?
estimation.
There are several possibleways to reduce this over-fit.
In this paper, we selectone of the simplest; we divide the labeled trainingdata Dl into two distinct sets D?l and D?
?l .
Then, D?land D?
?l are individually used for estimating ?
and?, respectively.
In our experiments, we divide thelabeled training data Dl so that 4/5 is used for D?land the remaining 1/5 for D?
?l .3.4 Efficient Parameter Estimation AlgorithmLet NR(x) represent the denominator of Equation(2), that is the normalization factor of R. We canrearrange Equation (2) as follows:R(y|x;?,?,?)
=?s?i[V Di,s]?i ?j[V Gj,s]?jNR(x)?i[Zi(x)]?i, (6)where V Di,s represents the potential function of thes-th position of the sequence in the i-th CRF andV Gj,s represents the probability of the s-th positionin the j-th HMM, that is, V Di,s = exp(?i ?
f s) andV Gj,s = ?ys?1,ys?ys,xs , respectively.
See the Ap-pendix for the derivation of Equation (6) from Equa-tion (2).794To estimate ?
(t+1), namely procedure 4.2 in Fig-ure 1, we employ the derivatives with respect to ?iand ?j shown in Equation (6), which are the parame-ters of the discriminative and generative models, re-spectively.
Thus, we obtain the following derivativeswith respect to ?i:?LHySOL(?|?)?
?i=?nlog pDi (yn|xn) +?nlogZDi (xn)??nER(Y|xn;?,?,?
)[?slog V Di,s].The first and second terms are constant during it-erative procedure 4 in our optimization algorithmshown in Figure 1.
Thus, we only need to calcu-late these values once at the beginning of proce-dure 4.
Let ?s(y) and ?s(y) represent the forwardand backward state costs at position s with outputy for corresponding input x.
Let Vs(y, y?)
repre-sent the products of the total value of the transitioncost between s?1 and s with labels y and y?
in thecorresponding input sequence, that is, Vs(y, y?)
=?i[V Di,s(y, y?
)]?i?j [V Gj,s(y, y?
)]?j .
The third term,which indicates the expectation of potential func-tions, can be rewritten in the form of a forward-backward algorithm, that is,ER(Y|x;?,?,?
)[?slog V Di,s]= 1ZR(x)?s?y,y?
?s?1(y)Vs(y, y?)?s(y?)
log V Di,s(y, y?
),(7)where ZR(x) represents the partition function of ourhybrid model, that is, ZR(x)=NR(x)?i[Zi(x)]?i .Hence, the calculation of derivatives with respect to?i is tractable since we can incorporate the sameforward-backward algorithm as that used in a stan-dard CRF.Then, the derivatives with respect to ?j , which arethe parameters of generative models, can be writtenas follows:?LHySOL(?|?)?
?j=?nlog pGj (xn,yn)??nER(Y|xn;?,?,?
)[?slog V Gj,s].Again, the second term, which indicates the expec-tation of transition probabilities and symbol emis-sion probabilities, can be rewritten in the form of aforward-backward algorithm in the same manner as?i, where the only difference is that V Di,s is substi-tuted by V Gj,s in Equation (7).To estimate?
(t+1), which is procedure 4.1 in Fig-ure 1, the same forward-backward algorithm as usedin standard HMMs is available since the form of ourQ-function shown in Equation (5) is the same as thatof standard HMMs.
The only difference is that ourmethod uses marginal probabilities given by R in-stead of the p(x,y;?)
of standard HMMs.Therefore, only a forward-backward algorithm isrequired for the efficient calculation of our param-eter estimation process.
Note that even though ourhybrid model supports the use of a combination ofseveral generative and discriminative models, weonly need to calculate the forward-backward algo-rithm once for each sample during optimization pro-cedures 4.1 and 4.2.
This means that the requirednumber of executions of the forward-backward al-gorithm for our parameter estimation is independentof the number of models used in the hybrid model.In addition, after training, we can easily merge allthe parameter values in a single parameter vector.This means that we can simply employ the Viterbi-algorithm for evaluating unseen samples, as well asthat of standard CRFs, without any additional cost.4 ExperimentsWe examined our hybrid model (HySOL) by ap-plying it to two sequence labeling tasks, namedentity recognition (NER) and syntactic chunking(Chunking).
We used the same Chunking and?English?
NER data as those used for the sharedtasks of CoNLL-2000 (Tjong Kim Sang and Buch-holz, 2000) and CoNLL-2003 (Tjong Kim Sang andMeulder, 2003), respectively.For the baseline method, we performed a condi-tional random field (CRF), which is exactly the sametraining procedure described in (Sha and Pereira,2003) with L-BFGS.
Moreover, LOP-CRF (Smith etal., 2005) is also compared with our hybrid model,since the formalism of our hybrid model can be seenas an extension of LOP-CRFs as described in Sec-tion 3.
For CRF, we used the Gaussian prior asthe second term on the RHS in Equation (1), where?2 represents the hyper-parameter in the Gaussianprior.
In contrast, for LOP-CRF and HySOL, weused the Dirichlet priors as the second term on the795?1 f(words), f(lwords), f(poss), f(wtypes),f(poss?1, poss), f(wtypes?1, wtypes),f(poss, poss+1), f(wtypes, wtypes+1),f(pref1s), f(pref2s), f(pref3s), f(pref4s),f(suf1s), f(suf2s), f(suf3s), f(suf4s)?2 f(words), f(lwords), f(poss), f(wtypes),f(words?1), f(lwords?1), f(poss?1), f(wtypes?1),f(words?2), f(lwords?2), f(poss?2), f(wtypes?2),f(poss?2, poss?1), f(wtypes?2, wtypes?1)?3 f(words), f(lwords), f(poss), f(wtypes),f(words+1), f(lwords+1), f(poss+1), f(wtypes+1),f(words+2), f(lwords+2), f(poss+2), f(wtypes+2),f(poss+1, poss+2), f(wtypes+1, wtypes+2)?4 all of the abovelword : lowercase of word, wtype : ?word type?pref1-4: 1-4 character prefix of wordsuf1-4 : 1-4 character suffix of wordTable 1: Features used in NER experimentsRHS in Equations (3), and (4), where ?
and ?
are thehyper-parameters in each Dirichlet prior.4.1 Named Entity Recognition ExperimentsThe English NER data consists of 203,621, 51,362and 46,435 words from 14,987, 3,466 and 3,684 sen-tences in training, development and test data, re-spectively, with four named entity tags, PERSON,LOCATION, ORGANIZATION and MISC, plus the?O?
tag.
The unlabeled data consists of 17,003,926words from 1,029,122 sentences.
These data sets areexactly the same as those provided for the sharedtask of CoNLL-2003.We slightly extended the feature set of the sup-plied data by adding feature types such as ?wordtype?, and word prefix and suffix.
Examples of?word type?
include whether the word is capitalized,contains digit or contains punctuation, which basi-cally follows the baseline features of (Sutton et al,2006) without regular expressions.
Note that, unlikeseveral previous studies, we did not employ addi-tional information from external resources such asgazetteers.
All our features can be automatically ex-tracted from the supplied data.For LOP-CRF and HySOL, we used four base dis-criminative models trained by CRFs with differentfeature sets.
Table 1 shows the feature sets we usedfor training these models.
The design of these fea-ture sets was derived from a suggestion in (Smith etal., 2005), which exhibited the best performance inthe several feature division.
Note that the CRF forthe comparison method was trained by using all fea-?1 f(words), (poss),f(words?1, words), f(poss?1, poss),f(words, words+1), f(poss, poss+1)?2 f(words), (poss),f(words?1), f(poss?1), f(words?2), f(poss?2),f(words?2, words?1), f(poss?2, poss?1)?3 f(words), (poss),f(words+1), f(poss+1), f(words+2), f(poss+2),f(words+1, words+2), f(poss+1, poss+2)?4 all of the aboveTable 2: Features used in Chunking experimentsture types, namely the same as ?4.As we explained in Section 3.3, for trainingHySOL, the parameters of four discriminative mod-els, ?, were trained from 4/5 of the labeled trainingdata, and ?
were trained from remaining 1/5.
Forthe features of the generative models, we used all ofthe feature types shown in Figure 1.
Note that onefeature type corresponds to one HMM.
Thus, eachHMM maintains to consist of a non-overlapping fea-ture set since each feature type only generates onesymbol per state.4.2 Syntactic Chunking ExperimentsCoNLL-2000 Chunking data was obtained from theWall Street Journal (WSJ) corpus: sections 15-18 astraining data (8,936 sentences and 211,727 words),and section 20 as test data (2,012 sentences and47,377 words), with 11 different chunk-tags, suchas NP and VP plus the ?O?
tag, which represents theregion outside any target chunk.For LOP-CRF and HySOL, we also used fourbase discriminative models trained by CRFs withdifferent feature sets.
Table 2 shows the feature setwe used in the Chunking experiments.
We used thefeature set of the supplied data without any exten-sion of additional feature types.To train HySOL, we used the same unlabeled dataas used for our NER experiments (17,003,926 wordsfrom the Reuters corpus).
Moreover, the division ofthe labeled training data and the feature set of thegenerative models were derived in the same man-ner as our NER experiments (see Section 4.1).
Thatis, we divided the labeled training data into 4/5 forestimating ?
and 1/5 for estimating ?
; one featuretype shown in Table 2 is assigned in one generativemodel.796methods (hyper-params) F?=1 (gain) Sent (gain)CRF (?2=100.0) 84.70 - 78.30 -(4/5 labeled data, ?2=100.0) 83.74 (-0.96) 77.06 (-1.24)LOP-CRF (?
?=0.1) 84.90 (+0.20) 79.02 (+0.72)HySOL (??=0.1,?
?=0.0001) 87.20 (+2.50) 81.19 (+2.89)(w/o prior) 86.86 (+2.16) 80.75 (+2.45)w/o pGj ?j ( ?
?=1.0) 84.56 (-0.14) 78.23 (-0.07)Table 3: NER performance (CoNLL-2003)methods (hyper-params) F?=1 (gain) Sent (gain)CRF (?2=10.0) 93.87 - 59.84 -(4/5 labeled data, ?2=10.0) 93.70 (-0.17) 58.85 (-0.99)LOP-CRF (?
?=0.1) 93.91 (+0.04) 60.34 (+0.50)HySOL (??=1.0,?
?=0.0001) 94.30 (+0.43) 61.73 (+1.89)(w/o prior) 94.17 (+0.30) 61.23 (+1.39)w/o pGj ?j (?
?=1.0) 93.84 (-0.03) 59.74 (-0.10)Table 4: Chunking performance (CoNLL-2000)5 Results and DiscussionWe evaluated the performance in terms of the F?=1score, which is the evaluation measure used inCoNLL-2000 and 2003, and sentence accuracy,since all the methods in our experiments optimizesequence loss.
Tables 3 and 4 show the results ofthe NER and Chunking experiments, respectively.The F?=1 and ?Sent?
columns show the performanceevaluated using the F?=1 score and sentence accu-racy, respectively.
?2, ?
and ?, which are the hyper-parameters in Gaussian or Dirichlet priors, are se-lected from a certain value set by using a develop-ment set1, that is, ?2 ?
{0.01, 0.1, 1, 10, 100, 1000},?
?
1 = ??
?
{0.01, 0.1, 1, 10} and ?
?
1 = ??
?
{0.00001, 0.0001, 0.001, 0.01}.
The second rows ofCRF in Tables 3 and 4 represent the performance ofbase discriminative models used in HySOL with allthe features, which are trained with 4/5 of the la-beled training data.
The third rows of HySOL showthe performance obtained without using generativemodels (unlabeled data).
The model itself is essen-tially the same as LOP-CRFs.
However the perfor-mance in the third HySOL rows was consistentlylower than that of LOP-CRF since the discrimina-tive models in HySOL are trained with 4/5 labeleddata.As shown in Tables 3 and 4, HySOL signifi-1Chunking (CoNLL-2000) data has no common develop-ment set.
Thus, our preliminary examination employed by using4/5 labeled training data with the remaining 1/5 as developmentdata to determine the hyper-parameter values.                                                       (a) NER (b) ChunkingFigure 2: Changes in the performance and the con-vergence condition value (procedure 4 in Figure 1)of HySOL.cantly improved the performance of supervised set-ting, CRF and LOP-CRF, as regards both NER andChunking experiments.5.1 Impact of Incorporating Unlabeled DataThe contributions provided by incorporating unla-beled data in our hybrid model can be seen by com-parison with the performance of the first and thirdrows in HySOL, namely a 2.64 point F-score and a2.96 point sentence accuracy gain in the NER exper-iments and a 0.46 point F-score and a 1.99 point sen-tence accuracy gain in the Chunking experiments.We believe there are two key ideas that enablethe unlabeled data in our approach to exhibit thisimprovement compared with the the state-of-the-artperformance provided by discriminative models insupervised settings.
First, unlabeled data is onlyused for optimizing Equation (4) to obtain a similareffect to ?soft-clustering?, which can be calculatedwithout information about the correct output.
Sec-ond, by using a combination of generative models,we can enhance the flexibility of the feature designfor unlabeled data.
For example, we can handle ar-bitrary overlapping features, similar to those used indiscriminative models, for unlabeled data by assign-ing one feature type for one generative model as inour experiments.5.2 Impact of Iterative Parameter EstimationFigure 2 shows the changes in the performance andthe convergence condition value of HySOL dur-ing parameter estimation iteration in our NER andChunking experiments, respectively.
As shown inthe figure, HySOL was able to reach the conver-797gence condition in a small number of iterations inour experiments.
Moreover, the change in the per-formance remains quite stable during the iteration.However, theoretically, our optimization procedureis not guaranteed to converge in the ?
and ?
space,since the optimization of ?
has local maxima.
Evenif we were unable to meet the convergence condi-tion, we were easily able to obtain model parame-ters by performing a sufficient fixed number of itera-tions, and then select the parameters when Equation(4) obtained the maximum objective value.5.3 Comparison with SS-CRF-MERWhen we consider semi-supervised SOL methods,SS-CRF-MER (Jiao et al, 2006) is the most compet-itive with HySOL, since both methods are definedbased on CRFs.
We planned to compare the perfor-mance with that of SS-CRF-MER in our NER andChunking experiments.
Unfortunately, we failed toimplement SS-CRF-MER since it requires the use ofa slightly complicated algorithm, called the ?nested?forward-backward algorithm.Although, we cannot compare the performance,our hybrid approach has several good characteris-tics compared with SS-CRF-MER.
First, it requiresa higher order algorithm, namely a ?nested?
forward-backward algorithm, for the parameter estimation ofunlabeled data whose time complexity is O(L3S2)for each unlabeled data, where L and S represent theoutput label size and unlabeled sample length, re-spectively.
Thus, our hybrid approach is more scal-able for the size of unlabeled data, since HySOLonly needs a standard forward-backward algorithmwhose time complexity is O(L2S).
In fact, westill have a question as to whether SS-CRF-MERis really scalable in practical time for such a largeamount of unlabeled data as used in our experi-ments, which is about 680 times larger than that of(Jiao et al, 2006).
Scalability for unlabeled datawill become really important in the future, as it willbe natural to use millions or billions of unlabeleddata for further improvement.
Second, SS-CRF-MER has a sensitive hyper-parameter in the objec-tive function, which controls the influence of the un-labeled data.
In contrast, our objective function onlyhas a hyper-parameter of prior distribution, which iswidely used for standard MAP estimation.
More-over, the experimental results shown in Tables 3 andF?=1 additional resourcesASO-semi 89.31 unlabeled data (27M words)(Ando and Zhang, 2005)(Florian et al, 2003) 88.76 their own large gazetteers,2M-word labeled data(Chieu and Ng, 2003) 88.31 their own large gazetteers,very elaborated featuresHySOL 88.14 unlabeled data (17M words)supplied gazettersHySOL 87.20 unlabeled data (17M words)Table 5: Previous top systems in NER (CoNLL-2003) experimentsF?=1 additional resourcesASO-semi 94.39 unlabeled data(Ando and Zhang, 2005) (15M words: WSJ)HySOL 94.30 unlabeled data(17M words: Reuters)(Zhang et al, 2002) 94.17 full parser output(Kudo and Matsumoto, 2001) 93.91 ?Table 6: Previous top systems in Chunking(CoNLL-2000) experiments4 indicate that HySOL is rather robust with respectto the hyper-parameter since we can obtain fairlygood performance without a prior distribution.5.4 Comparison with Previous Top SystemsWith respect to the performance of NER and Chunk-ing tasks, the current best performance is reportedin (Ando and Zhang, 2005), which we refer to as?ASO-semi?, as shown in Figures 5 and 6.
ASO-semi also incorporates unlabeled data solely forthe additional information in the same way as ourmethod.
Unfortunately, our results could not reachtheir level of performance, although the size andsource of the unlabeled data are not the same for cer-tain reasons.
First, (Ando and Zhang, 2005) does notdescribe the unlabeled data used in their NER ex-periments in detail, and second, we are not licensedto use the TREC corpus including WSJ unlabeleddata that they used for their Chunking experiments(training and test data for Chunking is derived fromWSJ).
Therefore, we simply used the supplied unla-beled data of the CoNLL-2003 shared task for bothNER and Chunking.
If we consider the advantage ofour approach, our hybrid model incorporating gener-ative models seems rather intuitive, since it is some-times difficult to find out a design of effective auxil-iary problems for the target problem.Interestingly, the additional information obtained798F?=1 (gain)HySOL (??=0.1,?
?=0.0001) 87.20 -+ w/ F-score opt.
(Suzuki et al, 2006) 88.02 (+0.82)+ unlabeled data (17M ?
27M words) 88.41 (+0.39)+ supplied gazetters 88.90 (+0.49)+ add dev.
set for estimating ?
89.27 (+0.37)Table 7: The HySOL performance with the F-score optimization technique and some additionalresources in NER (CoNLL-2003) experimentsF?=1 (gain)HySOL (??=0.1,?
?=0.0001) 94.30 -+ w/ F-score opt.
(Suzuki et al, 2006) 94.36 (+0.06)Table 8: The HySOL performance with the F-scoreoptimization technique on Chunking (CoNLL-2000)experimentsfrom unlabeled data appear different from eachother.
ASO-semi uses unlabeled data for construct-ing auxiliary problems to find the ?shared structures?of auxiliary problems that are expected to improvethe performance of the main problem.
Moreover,it is possible to combine both methods, for exam-ple, by incorporating the features obtained with theirmethod in our base discriminative models, and thenconstruct a hybrid model using our method.
There-fore, there may be a possibility of further improvingthe performance by this simple combination.In NER, most of the top systems other thanASO-semi boost performance by employing exter-nal hand-crafted resources such as large gazetteers.This is why their results are superior to those ob-tained with HySOL.
In fact, if we simply add thegazetteers included in CoNLL-2003 supplied data asfeatures, HySOL achieves 88.14.5.5 Applying F-score Optimization TechniqueIn addition, we can simply apply the F-score opti-mization technique for the sequence labeling tasksproposed in (Suzuki et al, 2006) to boost theHySOL performance since the base discriminativemodels pD(y|x) and discriminative combination,namely Equation (3), in our hybrid model basicallyuses the same optimization procedure as CRFs.
Ta-bles 7 and 8 show the F-score gain when we applythe F-score optimization technique.
As shown in theTables, the F-score optimization technique can eas-ily improve the (F-score) performance without anyadditional resources or feature engineering.In NER, we also examined HySOL with addi-tional resources to observe the performance gain.The third row represents the performance when weadd approximately 10M words of unlabeled data (to-tal 27M words)2 that are derived from 1996/11/15-30 articles in Reuters corpus.
Then, the fourth andfifth rows represent the performance when we addthe supplied gazetters in the CoNLL-2003 data asfeatures, and adding development data as trainingdata of ?.
In this case, HySOL achieved a com-parable performance to that of the current best sys-tem, ASO-semi, in both NER and Chunking exper-iments even though the NER experiment is not afair comparison since we added additional resources(gazetters and dev.
set) that ASO-semi does not usein training.6 Conclusion and Future WorkWe proposed a framework for semi-supervised SOLbased on a hybrid generative and discriminative ap-proach.
Experimental results showed that incorpo-rating unlabeled data in a generative manner hasthe power to further improve on the state-of-the-artperformance provided by supervised SOL methodssuch as CRFs, with the help of our hybrid approach,which discriminatively combines with discrimina-tive models.
In future we intend to investigate moreappropriate model and feature design for unlabeleddata, which may further improve the performanceachieved in our experiments.AppendixLet V Di,s = exp(?
?
f s) and V Gj,s = ?ys?1,ys?ys,xs .Equation (6) can be obtained by the following rear-rangement of Equation (2) :R(y|x;?,?,?
)=?i pDi (y|x,?i)?i?j pGj (x,y, ?j)?j?y?i pDi (y|x,?i)?i?j pGj (x,y, ?j)?j= 1NR(x)?i[?s VDi,sZi(x)]?i?j[?sV Gj,s]?j= 1NR(x)?i[Zi(x)]?i?i[?sV Di,s]?i?j[?sV Gj,s]?j= 1NR(x)?i[Zi(x)]?i?s?i[V Di,s]?i ?j[V Gj,s]?j .2In order to keep the consistency of POS tags, we re-attached POS tags of the supplied data set and new 10M wordsof unlabeled data using a POS tagger trained from WSJ corpus.799ReferencesY.
Altun, D. McAllester, and M. Belkin.
2005.
Max-imum Margin Semi-Supervised Learning for Struc-tured Variables.
In Proc.
of NIPS*2005.R.
Ando and T. Zhang.
2005.
A High-PerformanceSemi-Supervised Learning Method for Text Chunking.In Proc.
of ACL-2005, pages 1?9.U.
Brefeld and T. Scheffer.
2006.
Semi-SupervisedLearning for Structured Output Variables.
In Proc.
ofICML-2006.H.
L. Chieu and Hwee T. Ng.
2003.
Named EntityRecognition with a Maximum Entropy Approach.
InProc.
of CoNLL-2003, pages 160?163.A.
P. Dempster, N. M. Laird, and D. B. Rubin.
1977.Maximum Likelihood from Incomplete Data via theEM Algorithm.
Journal of the Royal Statistical Soci-ety, Series B, 39:1?38.R.
Florian, A. Ittycheriah, H. Jing, and T. Zhang.
2003.Named Entity Recognition through Classifier Combi-nation.
In Proc.
of CoNLL-2003, pages 168?171.A.
Fujino, N. Ueda, and K. Saito.
2005.
A Hybrid Gen-erative/Discriminative Approach to Semi-SupervisedClassifier Design.
In Proc.
of AAAI-05, pages 764?769.Y.
Grandvalet and Y. Bengio.
2004.
Semi-SupervisedLearning by Entropy Minimization.
In Proc.
ofNIPS*2004, pages 529?536.F.
Jiao, S. Wang, C.-H. Lee, R. Greiner, and D. Schuur-mans.
2006.
Semi-Supervised Conditional RandomFields for Improved Sequence Segmentation and La-beling.
In Proc.
of COLING/ACL-2006, pages 209?216.T.
Kudo and Y. Matsumoto.
2001.
Chunking with Sup-port Vector Machines.
In Proc.
of NAACL 2001, pages192?199.J.
Lafferty, A. McCallum, and F. Pereira.
2001.
Condi-tional Random Fields: Probabilistic Models for Seg-menting and Labeling Sequence Data.
In Proc.
ofICML-2001, pages 282?289.W.
Li and A. McCallum.
2005.
Semi-Supervised Se-quence Modeling with Syntactic Topic Models.
InProc.
of AAAI-2005, pages 813?818.D.
C. Liu and J. Nocedal.
1989.
On the Limited MemoryBFGS Method for Large Scale Optimization.
Math.Programming, Ser.
B, 45(3):503?528.K.
Nigam, A. McCallum, S. Thrun, and T. Mitchell.2000.
Text Classification from Labeled and UnlabeledDocuments using EM.
Machine Learning, 39:103?134.R.
Raina, Y. Shen, A. Y. Ng, and A. McCallum.
2003.Classification with Hybrid Generative/DiscriminativeModels.
In Proc.
of NIPS*2003.F.
Sha and F. Pereira.
2003.
Shallow Parsing with Condi-tional Random Fields.
In Proc.
of HLT/NAACL-2003,pages 213?220.A.
Smith, T. Cohn, and M. Osborne.
2005.
Logarith-mic Opinion Pools for Conditional Random Fields.
InProc.
of ACL-2005, pages 10?17.C.
Sutton, M. Sindelar, and A. McCallum.
2006.
Reduc-ing Weight Undertraining in Structured DiscriminativeLearning.
In Proc.
of HTL-NAACL 2006, pages 89?95.J.
Suzuki, E. McDermott, and H. Isozoki.
2006.
TrainingConditional Random Fields with Multivariate Evalua-tion Measure.
In Proc.
of COLING/ACL-2006, pages217?224.E.
F. Tjong Kim Sang and S. Buchholz.
2000.
Introduc-tion to the CoNLL-2000 Shared Task: Chunking.
InProc.
of CoNLL-2000 and LLL-2000, pages 127?132.E.
T. Tjong Kim Sang and F. De Meulder.
2003.
Intro-duction to the CoNLL-2003 Shared Task: Language-Independent Named Entity Recognition.
In Proc.
ofCoNLL-2003, pages 142?147.T.
Zhang, F. Damerau, and D. Johnson.
2002.
TextChunking based on a Generalization of Winnow.
Ma-chine Learning Research, 2:615?637.X.
Zhu, Z. Ghahramani, and J. Lafferty.
2003.
Semi-Supervised Learning using Gaussian Fields and Har-monic Functions.
In Proc.of ICML-2003, pages 912?919.800
