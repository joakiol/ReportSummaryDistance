Proceedings of the COLING/ACL 2006 Main Conference Poster Sessions, pages 57?64,Sydney, July 2006. c?2006 Association for Computational LinguisticsTowards Conversational QA: Automatic Identification of ProblematicSituations and User Intent ?Joyce Y. Chai Chen Zhang Tyler BaldwinDepartment of Computer Science and EngineeringMichigan State UniversityEast Lansing, MI 48824{jchai, zhangch6, baldwi96}@cse.msu.eduAbstractTo enable conversational QA, it is impor-tant to examine key issues addressed inconversational systems in the context ofquestion answering.
In conversational sys-tems, understanding user intent is criti-cal to the success of interaction.
Recentstudies have also shown that the capabil-ity to automatically identify problematicsituations during interaction can signifi-cantly improve the system performance.Therefore, this paper investigates the newimplications of user intent and problem-atic situations in the context of questionanswering.
Our studies indicate that, inbasic interactive QA, there are differenttypes of user intent that are tied to dif-ferent kinds of system performance (e.g.,problematic/error free situations).
Onceusers are motivated to find specific infor-mation related to their information goals,the interaction context can provide usefulcues for the system to automatically iden-tify problematic situations and user intent.1 IntroductionInteractive question answering (QA) has beenidentified as one of the important directions in QAresearch (Burger et al, 2001).
One ultimate goal isto support intelligent conversation between a userand a QA system to better facilitate user informa-tion needs.
However, except for a few systems thatuse dialog to address complex questions (Small etal., 2003; Harabagiu et al, 2005), the general di-alog capabilities have been lacking in most ques-?This work was partially supported by IIS-0347548 fromthe National Science Foundation.tion answering systems.
To move towards conver-sational QA, it is important to examine key issuesrelevant to conversational systems in the contextof interactive question answering.This paper focuses on two issues related to con-versational QA.
The first issue is concerned withuser intent.
In conversational systems, understand-ing user intent is the key to the success of the inter-action.
In the context of interactive QA, one ques-tion is what type of user intent should be captured.Unlike most dialog systems where user intent canbe characterized by dialog acts such as question,reply, and statement, in interactive QA, user in-puts are already in the form of question.
Thenthe problems become whether there are differenttypes of intent behind these questions that shouldbe handled differently by a QA system and how toautomatically identify them.The second issue is concerned with problem-atic situations during interaction.
In spoken di-alog systems, many problematic situations couldarise from insufficient speech recognition and lan-guage understanding performance.
Recent workhas shown that the capability to automaticallyidentify problematic situations (e.g., speech recog-nition errors) can help control and adapt dialogstrategies to improve performance (Litman andPan, 2000).
Similarly, QA systems also face chal-lenges of technology limitation from language un-derstanding and information retrieval.
Thus onequestion is, in the context of interactive QA, howto characterize problematic situations and auto-matically identify them when they occur.In interactive QA, these two issues are inter-twined.
Questions formed by a user not only de-pend on his/her information goals, but are also in-fluenced by the answers from the system.
Prob-lematic situations will impact user intent in the57follow-up questions, which will further influencesystem performance.
Both the awareness of prob-lematic situations and understanding of user in-tent will allow QA systems to adapt better strate-gies during interaction and move towards intelli-gent conversational QA.To address these two questions, we conducteda user study where users interacted with a con-trolled QA system to find information of inter-est.
These controlled studies allowed us to fo-cus on the interaction aspect rather than informa-tion retrieval or answer extraction aspects.
Ourstudies indicate that in basic interactive QA whereusers always ask questions and the system alwaysprovides some kind of answers, there are differ-ent types of user intent that are tied to differ-ent kinds of system performance (e.g., problem-atic/error free situations).
Once users are moti-vated to find specific information related to theirinformation goals, the interaction context can pro-vide useful cues for the system to automaticallyidentify problematic situations and user intent.2 Related WorkOpen domain question answering (QA) systemsare designed to automatically locate answers fromlarge collections of documents to users?
naturallanguage questions.
In the past few years, au-tomated question answering techniques have ad-vanced tremendously, partly motivated by a se-ries of evaluations conducted at the Text RetrievalConference (TREC) (Voorhees, 2001; Voorhees,2004).
To better facilitate user information needs,recent trends in QA research have shifted towardscomplex, context-based, and interactive questionanswering (Voorhees, 2001; Small et al, 2003;Harabagiu et al, 2005).
For example, NIST initi-ated a special task on context question answeringin TREC 10 (Voorhees, 2001), which later becamea regular task in TREC 2004 (Voorhees, 2004) and2005.
The motivation is that users tend to ask asequence of related questions rather than isolatedsingle questions to satisfy their information needs.Therefore, the context QA task was designed toinvestigate the system capability to track contextthrough a series of questions.
Based on contextQA, some work has been done to identify clarifica-tion relations between questions (Boni and Man-andhar, 2003).
However context QA is differentfrom interactive QA in that context questions arespecified ahead of time rather than incrementallyas in an interactive setting.Interactive QA has been applied to process com-plex questions.
For analytical and non-factualquestions, it is hard to anticipate answers.
Clari-fication dialogues can be applied to negotiate withusers about the intent of their questions (Small etal., 2003).
Recently, an architecture for interactivequestion answering has been proposed based on anotion of predictive questioning (Harabagiu et al,2005).
The idea is that, given a complex ques-tion, the system can automatically identify a set ofpotential follow-up questions from a large collec-tion of question-answer pairs.
The empirical re-sults have shown the system with predictive ques-tioning is more efficient and effective for users toaccomplish information seeking tasks in a partic-ular domain (Harabagiu et al, 2005).The work reported in this paper addresses adifferent aspect of interactive question answering.Both issues raised earlier (Section 1) are inspiredby earlier work on intelligent conversational sys-tems.
Automated identification of user intent hasplayed an important role in conversational sys-tems.
Tremendous amounts of work has focusedon this aspect (Stolcke et al, 2000).
To improvedialog performance, much effort has also been puton techniques to automatically detect errors duringinteraction.
It has shown that during human ma-chine dialog, there are sufficient cues for machinesto automatically identify error conditions (Levow,1998; Litman et al, 1999; Hirschberg et al, 2001;Walker et al, 2002).
The awareness of erroneoussituations can help systems make intelligent de-cisions about how to best guide human partnersthrough the conversation and accomplish the tasks.Motivated by these earlier studies, the goal of thispaper is to investigate whether these two issues canbe applied in question answering to facilitate intel-ligent conversational QA.3 User StudiesWe conducted a user study to collect data concern-ing user behavior in a basic interactive QA set-ting.
We are particularly interested in how usersrespond to different system performance and itsimplication in identifying problematic situationsand user intent.
As a starting point, we charac-terize system performance as either problematic,which indicates the answer has some problem, orerror-free, which indicates the answer is correct.In this section, we first describe the methodology58and the system used in this effort and then discussthe observed user behavior and its relation to prob-lematic situations and user intent.3.1 Methodology and SystemThe system used in our experiments has a user in-terface that takes a natural language question andpresents an answer passage.
Currently, our inter-face only presents to the user the top one retrievedresult.
This simplification on one hand helps usfocus on the investigation of user responses to dif-ferent system performances and on the other handrepresents a possible situation where a list of po-tential answers may not be practical (e.g., throughPDA or telephone line).We implemented a Wizard-of-Oz (WOZ) mech-anism in the interaction loop to control and simu-late problematic situations.
Users were not awareof the existence of this human wizard and wereled to believe they were interacting with a realQA system.
This controlled setting allowed usto focus on the interaction aspect rather than in-formation retrieval or answer extraction aspect ofquestion answering.
More specifically, during in-teraction after each question was issued, a ran-dom number generator was used to decide if aproblematic situation should be introduced.
Ifthe number indicated no, the wizard would re-trieve a passage from a database with correct ques-tion/answer pairs.
Note that in our experimentswe used specific task scenarios (described later),so it was possible to anticipate user informationneeds and create this database.
If the number in-dicated that a problematic situation should be in-troduced, then the Lemur retrieval engine 1 wasused on the AQUAINT collection to retrieve theanswer.
Our assumption is that AQUAINT dataare not likely to provide an exact answer given ourspecific scenarios, but they can provide a passagethat is most related to the question.
The use of therandom number generator was to control the ratiobetween the occurrence of problematic situationsand error-free situations.
In our initial investiga-tion, since we are interested in observing user be-havior in problematic situations, we set the ratio as50/50.
In our future work, we will vary this ratio(e.g., 70/30) to reflect the performance of state-of-the-art factoid QA and investigate the implicationof this ratio in automated performance assessment.1http://www-2.cs.cmu.edu/ lemur/3.2 ExperimentsEleven users participated in our study.
Each userwas asked to interact with our system to com-plete information seeking tasks related to fourspecific scenarios: the 2004 presidential debates,Tom Cruise, Hawaii, and Pompeii.
The exper-imental scenarios were further divided into twotypes: structured and unstructured.
In the struc-tured task scenarios (for topics Tom Cruise andPompeii), users had to fill in blanks on a dia-gram pertaining to the given topic.
Using the dia-gram was to avoid the influence of these scenarioson the language formation of the relevant ques-tions.
Because users must find certain informa-tion, they were constrained in the range of ques-tions in which they could ask, but not the way theyask those questions.
The task was completed whenall of the blanks on the diagram were filled.
Thestructured scenarios were designed to mimic thereal information seeking practice in which usershave real motivation to find specific informationrelated to their information goals.
In the unstruc-tured scenarios (for topics the 2004 presidentialdebates and Hawaii), users were given a generaltopic to investigate, but were not required to findspecific information.
This gave the user the abil-ity to ask a much wider range of questions thanthe structured scenarios.
Users were generally inan exploration mode when performing these un-structured tasks.
They were not motivated to findspecific information and were content with any in-formation provided by the system.
In our view,the unstructured scenarios are less representativeof the true information seeking situations.3.3 Observations and AnalysisFrom our studies, a total of 44 interaction sessionswith 456 questions were collected.
Figure 1 showsan example of a fragment of interaction related toTom Cruise.
In this example, both problematic sit-uations applied to answers (e.g., Problematic andError-Free) and user intent (described later) ap-plied to questions are annotated.There are several observations from this data.First, questions formed during interactive QA tendto be self-contained and free of definite nounphrases, pronouns, or ellipsis.
Only one questionin the entire data set has a pronoun (i.e., Whatare the best movies with Tom Cruise in them?
).Even in this case, the pronoun them did not re-fer to any entities that occurred previously in the59# Question/Answer AnnotationQ1 When was Tom Cruise born?Q2 What year was Tom Cruise born?
Re-tryQ3 What does Tom Cruise do for a living?
ContinueQ4 What is Tom Cruise's profession?
Re-tryQ5 What are the best movies with Tom Cruisein them?SwitchA1 ?
Based on the memoir of combat veteranRon Kovic, the film stars Tom Cruise asKovic, whose gunshot wound in Vietnamleft him paralyzed from the chest down.
?.a powerfully intimate portrait thatunfolds on an epic scale, Born on theFourth of July is arguably Stone's best film(if you can forgive its often strident tone),?..Proble-maticA2 ?Thomas Cruise Mapother IV was bornon the 3rd of July, 1962 (eerily similar tohis film Born on the 4th of July), inSyracuse, New York.
He was the only boyof four children?.Error-FreeA3 ?you get a very nice role that shows youdifferently, not the heavy, and you'reworking with a George Clooney or a TomCruise or a Nicolas Cage or a MartinScorsese.
I can live with that ?Proble-maticA4 ?So we may agree that Cruise?sprofessional standing accounts for somemeasure of his fame?.Proble-maticA5 ?Cruise proved his dramatic talents in the1988 drama Rain Man, where he co-starred with Oscar-winner Dustin Hoffman.Oliver Stone's Born on the Fourth of July(1989) earned him a Best Actor Oscarnomination for his hard-hitting portrayal ofanti-war activist Ron Kovic?.Error-FreeFigure 1: An example fragment of interactionQA process.
This phenomenon could be caused byhow the answers are presented.
Unlike specific an-swer entities, the answer passages provided by oursystem do not support the natural use of referringexpressions in the follow-up questions.
Anotherpossible explanation could be that in an interac-tive environment, users seem to be more aware ofthe potential limitation of a computer system andthus tend to specify self-contained questions in ahope to reduce the system?s inference load.The second observation is about user behaviorin response to different system performances (i.e.,problematic or error-free situations).
We werehoping to see different strategies users might ap-ply to deal with the problematic situations.
How-ever, based on the data, we found that when a prob-lem occurred, users either rephrased their ques-tions (i.e., the same question expressed in a dif-ferent way) or gave up the question and went onspecifying a new question.
(Here we use Rephraseand New to denote these two kinds of behaviors.
)We have not observed any sub-dialogs initiated byProblematic Error-free TotalNew Switch Continueunstruct.
29 90 119struct.
29 133 162entire 58 223 281Rephrase Re-try Negotiateunstruct.
19 4 23struct.
102 6 108entire 121 10 131Total-unst 48 94 142Total-st 131 139 270Total-ent 179 233 412Table 1: Categorization of user intent with the cor-responding number of occurrences from the un-structured scenarios, the structured scenarios, andthe entire dataset.the user to clarify a previous question or answer.One possible explanation is that the current inves-tigation was conducted in a basic interactive modewhere the system was only capable of providingsome sort of answers.
This may limit users?
expec-tation in the kind of questions that can be handledby the system.
Our assumption is that, once theQA system becomes more intelligent and able tocarry on conversation, different types of questions(i.e., other than rephrase or new) will be observed.This hypothesis certainly needs to be validated ina conversational setting.The third observation is that the rephrased ques-tions seem to strongly correlate with problematicsituations, although not always.
New questionscannot distinguish a problematic situation froman error-free situation.
Table 1 shows the statis-tics from our data about different combinationsof new/rephrase questions and performance situ-ations2.
What is interesting is that these differentcombinations can reflect different types of user in-tent behind the questions.
More specifically, givena question, four types of user intent can be cap-tured with respect to the context (e.g., the previousquestion and answer)Continue indicates that the user is satisfied withthe previous answer and now moves on to thisnew question.Switch indicates that the user has given up on theprevious question and now moves on to this2The last question from each interaction session is not in-cluded in these statistics because there is no follow-up ques-tion after that.60new question.Re-try indicates that the user is not satisfied withthe previous answer and now tries to get abetter answer.Negotiate indicates that the user is not satisfiedwith the previous answer (although it ap-pears to be correct from the system?s pointof view) and now tries to get a better answerfor his/her own needs.Table 1 summarizes these different types ofintent together with the number of correspond-ing occurrences from both structured and unstruc-tured scenarios.
Since in the unstructured sce-narios it was hard to anticipate user?s questionsand therefore take a correct action to respond to aproblematic/error-free situation, the distribution ofthese two situations is much more skewed than thedistribution for the structured scenarios.
Also asmentioned earlier, in unstructured scenarios, userslacked the motivation to pursue specific informa-tion, so the ratio between switch and re-try is muchlarger than that observed in the structured scenar-ios.
Nevertheless, we did observe different userbehavior in response to different situations.
Asdiscussed later in Section 5, identifying these fine-grained intents will allow QA systems to be moreproactive in helping users find satisfying answers.4 Automatic Identification ofProblematic Situations and User IntentGiven the discussion above, the next question ishow to automatically identify problematic situa-tions and user intent.
We formulate this as a classi-fication problem.
Given a question Qi, its answerAi, and the follow-up question Qi+1:(1) Automatic identification of problematic situa-tions is to decide whether Ai is problematic (i.e.,correct or incorrect) based on the follow-up ques-tion Qi+1 and the interaction context.
This is abinary classification problem.
(2) Automatic identification of user intent is toidentify the intent of Qi+1 given the interactioncontext.
Because we only have very limited in-stances of Negotiate (see Table 1), we currentlymerge Negotiate with Re-try since both of themrepresent a situation where a better answer is re-quested.
Thus, this problem becomes a trinaryclassification problem.To build these classifiers, we identified a set offeatures, which are illustrated next.4.1 FeaturesGiven a question Qi, its answer Ai, and the follow-up question Qi+1, the following set of features areused:Target matching(TM): a binary feature indicat-ing whether the target type of Qi+1 is the same asthe target type of Qi.
Our data shows that the rep-etition of the target type may indicate a rephrase,which could signal a problematic situation has justhappened.Named entity matching (NEM): a binary featureindicating whether all the named entities in Qi+1also appear in Qi.
If no new named entity is in-troduced in Qi+1, it is likely Qi+1 is a rephrase ofQi.Similarity between questions (SQ): a numericfeature measuring the similarity between Qi+1 andQi.
Our assumption is that the higher the simi-larity is, the more likely the current question is arephrase to the previous one.Similarity between content words of questions(SQC): this feature is similar to the previous fea-ture (i.e., SQ) except that the similarity measure-ment is based on the content words excludingnamed entities.
This is to prevent the similaritymeasurement from being dominated by the namedentities.Similarity between Qi and Ai (SA): this featuremeasures how close the retrieved passage matchesthe question.
Our assumption is that although a re-trieved passage is the most relevant passage com-pared to others, it still may not contain the answer(e.g., when an answer does not even exist in thedata collection).Similarity between Qi and Ai based on the con-tent words (SAC): this feature is essentially thesame as the previous feature (SA) except that thesimilarity is calculated after named entities are re-moved from the questions and answers.Note that since our data is currently collectedfrom simulation studies, we do not have the confi-dence score from the retrieval engine associatedwith every answer.
In practice, the confidencescore can be used as an additional feature.Since our focus is not on the similarity measure-ment but rather the use of the measurement in theclassification models, our current similarity mea-surement is based on a simple approach that mea-sures commonality and difference between twoobjects as proposed by Lin (1998).
More specifi-cally, the following equation is applied to measure61the similarity between two chunks of text T1andT2:sim1(T1, T2) =?
logP (T1?
T2)?
logP (T1?
T2)Assume the occurrence of each word is indepen-dent, then:sim1(T1, T2) =?
?w?T1?T2log P (w)?
?w?T1?T2log P (w)where P (w) was calculated based on the data usedin the previous TREC evaluations.4.2 Identification of Problematic SituationsTo identify problematic situations, we experi-mented with three different classifiers: Maxi-mum Entropy Model (MEM) from MALLET3,SVM from SVM-Light4, and Decision Trees fromWEKA5.
A leave-one-out validation was appliedwhere one interaction session was used for testingand the remaining interaction sessions were usedfor training.Table 2 shows the performance of the threemodels based on different combinations of fea-tures in terms of classification accuracy.
The base-line result is the performance achieved by sim-ply assigning the most frequently occurred class.For the unstructured scenarios, the performanceof the classifiers is rather poor, which indicatesthat it is quite difficult to make any generaliza-tion based on the current feature sets when usersare less motivated in finding specific information.For the structured scenarios, the best performancefor each model is highlighted in bold in Table 2.The Decision Tree model achieves the best per-formance of 77.8% in identifying problematic sit-uations, which is more than 25% better than thebaseline performance.4.3 Identification of User IntentTo identify user intent, we formulate the problemas follows: given an observation feature vector fwhere each element of the vector corresponds toa feature described earlier, the goal is to identifyan intent c?
from a set of intents I ={Continue,Switch, Re-try/Negotiate} that satisfies the follow-ing equation:c?
= argmaxc?IP (c|f)3http://mallet.cs.umass.edu/index.php/4http://svmlight.joachims.org/5http://www.cs.waikato.ac.nz/ml/weka/Our assumption is that user intent for a ques-tion can be potentially influenced by the intentfrom a preceding question.
For example, Switchis likely to follow Re-try.
Therefore, we have im-plemented a Maximum Entropy Markov Model(MEMM) (McCallum et al, 2000) to take the se-quence of interactions into account.Given a sequence of questions Q1, Q2, up to Qt,there is an observation feature vector fiassociatedwith each Qi.
In MEMM, the prediction of userintent ct for Qt not only depends on the observa-tion ft, but also the intent ct?1 from the precedingquestion Qt?1.
In fact, this approach finds the bestsequence of user intent C?
for Q1up to Qt basedon a sequence of observations f1, f2, ..., ft as fol-lows:C?
= argmaxC?ItP (C|f1, f2, ..., ft)where C is a sequence of intent and It is the set ofall possible sequences of intent with length t.To find this sequence of intent C?, MEMMkeeps a variable ?t(i) which is defined to be themaximum probability of seeing a particular se-quence of intent ending at intent i (i ?
I) forquestion Qt, given the observation sequence forquestions Q1up to Qt:?t(i) = maxc1,...,ct?1P (c1, .
.
.
, ct?1, ct = i|f1, .
.
.
, ft)This variable can be calculated by a dynamicoptimization procedure similar to the Viterbi algo-rithm in the Hidden Markov Model:?t(i) = maxj ?t?1(j) ?
P (ct = i|ct?1 = j, ft)where P (ct = i|ct?1 = j, ft) is estimated by theMaximum Entropy Model.Table 3 shows the best results of identifyinguser intent based on the Maximum Entropy Modeland MEMM using the leave-one-out approach.The results have shown that both models did notwork for the data collected from unstructured sce-narios (i.e., the baseline accuracy for intent iden-tification is 63.4%).
For structured scenarios, interms of the overall accuracy, both models per-formed significantly better than the baseline (i.e.,49.3%).
The MEMM worked only slightly betterthan the MEM.
Given our limited data, it is notconclusive whether the transitions between ques-tions will help identify user intent in a basic inter-active mode.
However, we expect to see more in-fluence from the transitions in fully conversationalQA.62MEM SVM DTreeFeatures un s ent un s ent un s entBaseline 66.2 51.5 56.3 66.2 51.5 56.3 66.2 51.5 56.3TM, SQC 50.0 57.4 54.9 53.5 60.0 57.8 53.5 55.9 55.1NEM, SQC 37.3 74.4 61.7 37.3 74.4 61.7 37.3 74.4 61.7TM, SQ 61.3 64.8 63.6 57.0 64.1 61.7 59.9 64.4 62.9NEM, SQC, SAC 40.8 76.7 64.3 38.0 74.4 61.9 49.3 77.8 68.0TM, SQ, SAC 59.2 67.4 64.6 61.3 66.3 64.6 62.7 65.6 64.6TM, NEM, SQC 54.2 75.2 68.0 54.2 75.2 68.0 53.5 74.4 67.2TM, SQ, SA 63.4 71.9 68.9 58.5 71.5 67.0 67.6 75.6 72.8TM, NEM, SQC, SAC 54.9 75.6 68.4 54.2 75.2 68.0 55.6 74.4 68.0* un - unstructured, s - structured, ent - entireTable 2: Performance of automatic identification of problematic situationsMEM MEMMun s un sCONTINUE P 64.4 69.7 67.3 70.8R 96.7 85.8 80.0 88.8F 77.3 76.8 73.1 78.7RE-TRY P 28.6 76.2 37.1 79.0/NEGOTIATE R 8.7 74.1 56.5 73.1F 13.3 75.1 44.8 75.9SWITCH P - - - 50.0R 0 0 0 3.6F - - - 6.7Overall accuracy 62.7 72.2 59.9 73.7* un - unstructured, s - structuredTable 3: Performance of automatic identificationof user intent5 Implications of Problematic Situationsand User IntentAutomated identification of problematic situationsand user intent have potential implications in thedesign of conversational QA systems.
Identifica-tion of problematic situations can be considered asimplicit feedback.
The system can use this feed-back to improve its answer retrieval performanceand proactively adapt its strategy to cope withproblematic situations.
One might think that analternative way is to explicitly ask users for feed-back.
However, this explicit approach will defeatthe purpose of intelligent conversational systems.Soliciting feedback after each question not onlywill frustrate users and lengthen the interaction,but also will interrupt the flow of user thoughts andconversation.
Therefore, our focus here is to inves-tigate the more challenging end of implicit feed-back.
In practice, the explicit feedback and im-plicit feedback should be intelligently combined.For example, if the confidence for automaticallyidentifying a problematic situation or an error-freesituation is low, then perhaps explicit feedback canbe solicited.Automatic identification of user intent also hasimportant implications in building intelligent con-versational QA systems.
For example, if Con-tinue is identified during interaction, then the sys-tem can automatically collect the question answerpairs for potential future use.
If Switch is identi-fied, the system may put aside the question that hasnot been correctly answered and proactively comeback to that question later after more informationis gathered.
If Re-try is identified, the system mayavoid repeating the same answer and at the sametime may take the initiative to guide users on howto rephrase a question.
If Negotiate is identified,the system may want to investigate the user?s par-ticular needs that may be different from the gen-eral needs.
Overall, different strategies can be de-veloped to address problematic situations and dif-ferent intents.
We will investigate these strategiesin our future work.This paper reports our initial effort in investi-gating interactive QA from a conversational pointof view.
The current investigation has severalsimplifications.
First, our current work has fo-cused on factoid questions where it is relativelyeasy to judge a problematic or error-free situation.However, as discussed in earlier work (Small etal., 2003), sometimes it is very hard to judge thetruthfulness of an answer, especially for analyti-cal questions.
Therefore, our future work will ex-amine the new implications of problematic situa-tions and user intent for analytical questions.
Sec-63ond, our current investigation is based on a ba-sic interactive mode.
As mentioned earlier, oncethe QA systems become more intelligent and con-versational, more varieties of user intent are an-ticipated.
How to characterize and automaticallyidentify more complex user intent under these dif-ferent situations is another direction of our futurework.6 ConclusionThis paper presents our initial investigation onautomatic identification of problematic situationsand user intent in interactive QA.
Our results haveshown that, once users are motivated in findingspecific information related to their informationgoals, user behavior and interaction context canhelp automatically identify problematic situationsand user intent.
Although our current investigationis based on the data collected from a controlledstudy, the same approaches can be applied dur-ing online processing as the question answeringproceeds.
The identified problematic situationsand/or user intent will provide immediate feed-back for a QA system to adjust its behavior andadapt better strategies to cope with different situa-tions.
This is an important step toward intelligentconversational question answering.ReferencesMarco De Boni and Suresh Manandhar.
2003.
Ananalysis of clarification dialogues for question an-swering.
In Proceedings of HLT-NAACL 2003,pages 48?55.John Burger, Claire Cardie, Vinay Chaudhri, RobertGaizauskas, Sanda Harabagiu, David Israel, Chris-tian Jacquemin, Chin-Yew Lin, Steve Maiorano,George Miller, Dan Moldovan, Bill Ogden, JohnPrager, Ellen Riloff, Amit Singhal, Rohini Shrihari,Tomek Strzalkowski, Ellen Voorhees, and RalphWeishedel.
2001.
Issues, tasks and program struc-tures to roadmap research in question & answering.In NIST Roadmap Document.Sanda Harabagiu, Andrew Hickl, John Lehmann, andDan Moldovan.
2005.
Experiments with interactivequestion-answering.
In Proceedings of the 43rd An-nual Meeting of the Association for ComputationalLinguistics (ACL?05), pages 205?214, Ann Arbor,Michigan, June.
Association for Computational Lin-guistics.Julia Hirschberg, Diane J. Litman, and Marc Swerts.2001.
Identifying user corrections automaticallyin spoken dialogue systems.
In Proceedings ofthe Second Meeting of the North American Chap-ter of the Association for Computational Linguistics(NAACL?01).Gina-Anne Levow.
1998.
Characterizeing and recog-nizing spoken corrections in human-computer dia-logue.
In Proceedings of the 36th Annual Meet-ing of the Association of Computational Linguistics(COLING/ACL-98), pages 736?742.Dekang Lin.
1998.
An information-theoretic defini-tion of similarity.
In Proceedings of InternationalConference on Machine Learning, Madison, Wis-consin, July.Diane J. Litman and Shimei Pan.
2000.
Predictingand adapting to poor speech recognition in a spo-ken dialogue system.
In Proceedings of the Seven-teenth National Conference on Artificial Intelligence(AAAI-2000), pages 722?728.Diane J. Litman, Marilyn A. Walker, and Michael S.Kearns.
1999.
Automatic detection of poor speechrecognition at the dialogue level.
In Proceedings ofthe 37th Annual meeting of the Association of Com-putational Linguistics (ACL-99), pages 309?316.Andrew McCallum, Dayne Freitag, and FernandoPereira.
2000.
Maximum entropy markov mod-els for information extraction and segmentation.
InProceedings of Internatioanl Conference on Ma-chine Learning (ICML 2000), pages 591?598.Sharon Small, Ting Liu, Nobuyuki Shimizu, andTomek Strzalkowski.
2003.
HITIQA: An interac-tive question answering system: A preliminary re-port.
In Proceedings of the ACL 2003 Workshop onMultilingual Summarization and Question Answer-ing.Andreas Stolcke, Klaus Ries, Noah Coccaro, ElizabethShriberg, Rebecca Bates, Daniel Jurafsky, Paul Tay-lor, Rachel Martin, Marie Meteer, and Carol VanEss-Dykema.
2000.
Dialogue act modeling for au-tomatic tagging and recognition of conversationalspeech.
In Computational Linguistics, volume 26.Ellen Voorhees.
2001.
Overview of TREC 2001 ques-tion answering track.
In Proceedings of TREC.Ellen Voorhees.
2004.
Overview of TREC 2004.
InProceedings of TREC.Marilyn Walker, Irene Langkilde-Geary, Helen WrightHastie, Jerry Wright, and Allen Gorin.
2002.
Auto-matically training a problematic dialogue predictorfor the HMIHY spoken dialog system.
In Journal ofArtificial Intelligence Research.64
