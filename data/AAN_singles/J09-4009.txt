Binarization of SynchronousContext-Free GrammarsLiang Huang?USC/Information Science InstituteHao Zhang?
?Google Inc.Daniel Gildea?University of RochesterKevin Knight?USC/Information Science InstituteSystems based on synchronous grammars and tree transducers promise to improve the qualityof statistical machine translation output, but are often very computationally intensive.
Thecomplexity is exponential in the size of individual grammar rules due to arbitrary re-orderingsbetween the two languages.
We develop a theory of binarization for synchronous context-freegrammars and present a linear-time algorithm for binarizing synchronous rules when possible.In our large-scale experiments, we found that almost all rules are binarizable and the resultingbinarized rule set significantly improves the speed and accuracy of a state-of-the-art syntax-based machine translation system.
We also discuss the more general, and computationally moredifficult, problem of finding good parsing strategies for non-binarizable rules, and present anapproximate polynomial-time algorithm for this problem.1.
IntroductionSeveral recent syntax-based models for machine translation (Chiang 2005; Galley et al2004) can be seen as instances of the general framework of synchronous grammarsand tree transducers.
In this framework, both alignment (synchronous parsing) anddecoding can be thought of as parsing problems, whose complexity is in general ex-ponential in the number of nonterminals on the right-hand side of a grammar rule.To alleviate this problem, we investigate bilingual binarization as a technique to fac-tor each synchronous grammar rule into a series of binary rules.
Although mono-lingual context-free grammars (CFGs) can always be binarized, this is not the case?
Information Science Institute, 4676 Admiralty Way, Marina del Rey, CA 90292.
E-mail: lhuang@isi.edu,liang.huang.sh@gmail.com.??
1600 Amphitheatre Parkway, Mountain View, CA 94303.
E-mail: haozhang@google.com.?
Computer Science Dept., University of Rochester, Rochester NY 14627.
E-mail: gildea@cs.rochester.edu.?
Information Science Institute, 4676 Admiralty Way, Marina del Rey, CA 90292.
E-mail: knight@isi.edu.Submission received: 14 March 2007; revised submission received: 8 January 2009; accepted for publication:25 March 2009.?
2009 Association for Computational LinguisticsComputational Linguistics Volume 35, Number 4for all synchronous rules; we investigate algorithms for non-binarizable rules as well.In particular:r We develop a technique called synchronous binarization and devise alinear-time binarization algorithm such that the resulting rule set alowsefficient algorithms for both synchronous parsing and decoding withintegrated n-gram language models.r We examine the effect of this binarization method on end-to-endtranslation quality on a large-scale Chinese-to-English syntax-basedsystem, compared to a more typical baseline method, and a state-of-the-artphrase-based system.r We examine the ratio of binarizability in large, empirically derived rulesets, and show that the vast majority is binarizable.
However, we alsoprovide, for the first time, real examples of non-binarizable cases verifiedby native speakers.r In the final, theoretical, sections of this article, we investigate the generalproblem of finding the most efficient synchronous parsing or decodingstrategy for arbitrary synchronous context-free grammar (SCFG) rules,including non-binarizable cases.
Although this problem is believed to beNP-complete, we prove two results that substantially reduce the searchspace over strategies.
We also present an optimal algorithm that runstractably in practice and a polynomial-time algorithm that is a goodapproximation of the former.Melamed (2003) discusses binarization of multi-text grammars on a theoreticallevel, showing the importance and difficulty of binarization for efficient synchronousparsing.
One way around this difficulty is to stipulate that all rules must be binaryfrom the outset, as in Inversion Transduction Grammar (ITG) (Wu 1997) and the binarySCFG employed by the Hiero system (Chiang 2005) to model the hierarchical phrases.In contrast, the rule extraction method of Galley et al (2004) aims to incorporate moresyntactic information by providing parse trees for the target language and extractingtree transducer rules that apply to the parses.
This approach results in rules with manynonterminals, making good binarization techniques critical.We explain how synchronous rule binarization interacts with n-gram languagemodels and affects decoding for machine translation in Section 2.
We define binarizationformally in Section 3, and present an efficient algorithm for the problem in Section 4.Experiments described in Section 51 show that binarization improves machine trans-lation speed and quality.
Some rules cannot be binarized, and we present a decodingstrategy for these rules in Section 6.
Section 7 gives a solution to the general theo-retical problem of finding optimal decoding and synchronous parsing strategies forarbitrary SCFGs, and presents complexity results on the nonbinarizable rules from ourChinese?English data.
These final two sections are of primarily theoretical interest, asnonbinarizable rules have not been shown to benefit real-world machine translation sys-tems.
However, the algorithms presented may become relevant as machine translationsystems improve.1 A preliminary version of Section 1?5 appeared in Zhang et al (2006).560Huang et al Binarization of Synchronous Context-Free Grammars2.
MotivationConsider the following Chinese sentence and its English translation:(1) ?Ba`owe?ierPowellyu?with??Sha?lo?ngSharon>Lju?x??nghold?le[past.
]hu?`ta?nmeeting?Powell held a meeting with Sharon?Suppose we have the following SCFG, where superscripts indicate reorderings (formaldefinitions of SCFGs with a more flexible notation can be found in Section 3):(2)S ?
NP 1 PP 2 VP 3 , NP 1 VP 3 PP 2NP ?
? / Ba`owe?ier, PowellVP ?
>L? / ju?x?
?ng le hu?`ta?n, held a meetingPP ?
??
/ yu?
Sha?lo?ng, with SharonDecoding can be cast as a (monolingual) parsing problem because we only need to parsethe source-language side of the SCFG, as if we were constructing a CFG by projectingthe SCFG onto its Chinese side:(3)S ?
NP PP VPNP ?
? / Ba`owe?ierVP ?
>L? / ju?x?
?ng le hu?`ta?nPP ?
??
/ yu?
Sha?lo?ngThe only extra work we need to do for decoding is to build corresponding target-language (English) subtrees in parallel.
In other words, we build synchronous treeswhen parsing the source-language input, as shown in Figure 1.For efficient parsing, we need to binarize the projected CFG either explicitly intoChomsky Normal Form as required by the CKY algorithm, or implicitly into a dottedrepresentation as in the Earley algorithm.
To simplify the presentation, we will focus onthe former, but the following discussion can be easily adapted to the latter.
Rules can bebinarized in different ways.
For example, we could binarize the first rule left to right orright to left (see Figure 2):S ?
NP-PP VPNP-PP ?
NP PP orS ?
NP PP-VPPP-VP ?
PP VPWe call these intermediate symbols (e.g., PP-VP) virtual nonterminals and correspond-ing rules virtual rules, whose probabilities are all set to 1.Figure 1A pair of synchronous parse trees in the SCFG (2).
The superscript symbols (????)
indicate pairsof synchronous nonterminals (and subtrees).561Computational Linguistics Volume 35, Number 4Figure 2The alignment matrix and two binarization schemes, with virtual nonterminals in gray.
(a) Atwo-dimensional matrix representation of the first SCFG rule in grammar 2.
Rows are positionsin Chinese: columns are positions in English, and black cells indicate positions linked by theSCFG rule.
(b) This scheme groups NP and PP into an intermediate state which contains a gapon the English side.
(c) This scheme groups PP and VP into an intermediate state which iscontiguous on both sides.These two binarizations are no different in the translation-model-only decodingdescribed previously, just as in monolingual parsing.
However, in the source-channelapproach to machine translation, we need to combine probabilities from the translationmodel (TM) (an SCFG) with the language model (an n-gram), which has been shownto be very important for translation quality (Chiang 2005).
To do bigram-integrateddecoding (Wu 1996), we need to augment each chart item (X, i, j) with two target-language boundary words u and v to produce a bigram-item which we denote(u ???
vXi j).2Now the two binarizations have very different effects.
In the first case, we first com-bine NP with PP.
This step is written as follows in the weighted deduction notation ofNederhof (2003):( Powell ???
PowellNP1 2): p(with ???
SharonPP2 4): q( Powell ???
Powell ???
with ???
SharonNP-PP1 4): pqwhere p and q are the scores of antecedent items.This situation is unpleasant because in the target language NP and PP are notcontiguous so we cannot apply language model scoring when we build the NP-PP item.Instead, we have to maintain all four boundary words (rather than two) and postponethe language model scoring till the next step where NP-PP is combined with(held ???
meetingVP2 4)to form an S item.
We call this binarization method monolingual binarization becauseit works only on the source-language projection of the rule without respecting theconstraints from the other side.This scheme generalizes to the case where we have n nonterminals in a SCFGrule, and the decoder conservatively assumes nothing can be done on language modelscoring (because target-language spans are non-contiguous in general) until the realnonterminal has been recognized.
In other words, target-language boundary words2 An alternative to integrated decoding is rescoring, where one first computes the k-best translationsaccording to the TM only, and then reranks the k-best list with the language model costs.
This methodruns very fast in practice (Huang and Chiang 2005), but often produces a considerable number of searcherrors because the true best translation is often outside of the k-best list, especially for longer sentences.562Huang et al Binarization of Synchronous Context-Free Grammarsfrom each child nonterminal of the rule will be cached in all virtual nonterminalsderived from this rule.
In the case of m-gram integrated decoding, we have to maintain2(m ?
1) boundary words for each child nonterminal, which leads to a prohibitive over-all complexity of O(|w|3+2n(m?1)), which is exponential in rule size (Huang, Zhang, andGildea 2005).
Aggressive pruning must be used to make it tractable in practice, whichin general introduces many search errors and adversely affects translation quality.In the second case, however, we have:(with ???
SharonPP2 4): r( held ???
meetingVP4 7): s( held ???
SharonPP-VP2 7): rs ?
Pr(with | meeting)Here, because PP and VP are contiguous (but swapped) in the target language, we caninclude the language model score by multiplying in Pr(with | meeting), and the resultingitem again has two boundary words.
Later we multiply in Pr(held | Powell) when theresulting item is combined with (Powell ???
PowellNP1 2) to form an S item.
As illustrated in Figure 2,PP-VP has contiguous spans on both source- and target-sides, so that we can generate abinary-branching SCFG:(4) S ?
NP1 PP-VP 2 , NP 1 PP-VP 2PP-VP ?
VP 1 PP 2 , PP 2 VP 1In this case m-gram integrated decoding can be done in O(|w|3+4(m?1)) time, which is amuch lower-order polynomial and no longer depends on rule size (Wu 1996), allowingthe search to be much faster and more accurate, as is evidenced in the Hiero system ofChiang (2005), which restricts the hierarchical phrases to form binary-branching SCFGrules.Some recent syntax-based MT systems (Galley et al 2004) have adopted the for-malism of tree transducers (Rounds 1970), modeling translation as a set of rules for atransducer that takes a syntax tree in one language as input and transforms it into a tree(or string) in the other language.
The same decoding algorithms are used for machinetranslation in this formalism, and the following example shows that the same issues ofbinarization arise.Suppose we have the following transducer rules:(5)S(x1:NP x2:PP x3:VP) ?
S(x1 VP(x3 x2))NP(? / Ba`owe?ier) ?
NP(NNP(Powell))VP(>L? / ju?x?
?ng le hu?`ta?n) ?
VP(VBD(held) NP(DT(a) NPS(meeting)))PP(??
/ yu?
Sha?lo?ng) ?
PP(TO(with) NP(NNP(Sharon)))where the reorderings of nonterminals are denoted by variables xi.
In the tree-transducer formalism of Rounds (1970), the right-hand (target) side subtree can havemultiple levels, as in the first rule above.
This system can model non-isomorphictransformations on English parse trees to ?fit?
another language, learning, for example,that the (V S O) structure in Arabic should be transformed into a (S (V O)) structurein English, by looking at two-level tree fragments (Knight and Graehl 2005).
From asynchronous rewriting point of view, this is more akin to synchronous tree substitutiongrammar (STSG) (Eisner 2003; Shieber 2004) (see Figure 3).
This larger locality capturesmore linguistic phenomena and leads to better parameter estimation.
By creating a563Computational Linguistics Volume 35, Number 4Figure 3Two equivalent representations of the first rule in Example (5): (a) tree transducer; (b)Synchronous Tree Subsitution Grammar (STSG).
The ?
arrows denote substitution sites,which correspond to variables in tree transducers.nonterminal for each right-hand-side tree, we can convert the transducer representationto an SCFG with the same generative capacity.
We can again create a projected CFGwhich will be exactly the same as in Example (3), and build English subtrees whileparsing the Chinese input.
In this sense we can neglect the tree structures whenbinarizing a tree-transducer rule, and consider only the alignment (or permutation) ofthe nonterminal variables.
Again, rightmost binarization is preferable for the first rule.In SCFG-based frameworks, the problem of finding a word-level alignment be-tween two sentences is an instance of the synchronous parsing problem: Given twostrings and a synchronous grammar, find a parse tree that generates both input strings.The benefit of binary grammars also applies in this case.
Wu (1997) shows that parsinga binary-branching SCFG is in O(|w|6), while parsing SCFG with arbitrary rules isNP-hard (Satta and Peserico 2005).
For example, in Figure 2, the complexity of syn-chronous parsing for the original grammar (a) is O(|w|8), because we have to maintainfour indices on either side, giving a total of eight; parsing the monolingually binarizedgrammar (b) involves seven indices, three on the Chinese side and four on the En-glish side.
In contrast, the synchronously binarized version (c) requires only 3 + 3 =6 indices, which can be thought of as ?CKY in two dimensions.?
An efficient alignmentalgorithm is guaranteed if a binarization is found, and the same binarization can beused for decoding and alignment.
We show how to find optimal alignment algorithmsfor non-binarizable rules in Section 7; in this case different grammar factorizations maybe optimal for alignment and for decoding with n-gram models of various orders.Handling difficult rules may in fact be more important for alignment than for decoding,because although we may be able to find good translations during decoding within thespace permitted by computationally friendly rules, during alignment we must handlethe broader spectrum of phenomena found in real bitext data.In general, if we are given an arbitrary synchronous rule with many nonterminals,what are the good decompositions that lead to a binary grammar?
Figure 2 suggeststhat a binarization is good if every virtual nonterminal has contiguous spans on bothsides.
We formalize this idea in the next section.3.
Synchronous BinarizationDefinition 1A synchronous CFG (SCFG) is a context-free rewriting system for generating stringpairs.
Each rule (synchronous production)A ?
?, B ?
?564Huang et al Binarization of Synchronous Context-Free Grammarsrewrites a pair of synchronous nonterminals (A, B) in two dimensions subject to theconstraint that there is a one-to-one mapping between the nonterminal occurrences in ?and the nonterminal occurrences in ?.
Each co-indexed child nonterminal pair is a pairof synchronous nonterminals and will be further rewritten as a unit.Note that this notation, due to Satta and Peserico (2005), is more flexible than thosein the previous section, in the sense that we can allow different symbols to be synchro-nized, which is essential to capture the syntactic divergences between languages.
Forexample, the following rule from Chinese to English(6) VP ?
VB 1 NN 2 , VP ?
VBZ 1 NNS 2illustrates the fact that Chinese does not have a plural noun (NNS) or third-person-singular verb (VBZ), although we can always convert this form back into the othernotation by creating a compound nonterminal alphabet:(VP, VP) ?
(VB, VBZ) 1 (NN, NNS) 2 , (VB, VBZ) 1 (NN, NNS) 2 .We define the language L(G) produced by an SCFG G as the pairs of terminal stringsproduced by rewriting exhaustively from the start nonterminal pair.As shown in Section 4.2, terminals do not play an important role in binarization.
Sowe now write rules in the following notation:X ?
X 11 ...X nn , Y ?
Ypi(1)pi(1) ...Ypi(n)pi(n)where Xi and Yi are variables ranging over nonterminals in the source and target pro-jections of the synchronous grammar, respectively, and pi is the permutation of the rule.For example, in rule (6), we have n = 2, X = Y = VP, X1 = VB, X2 = NN, Y1 = VBZ,Y2 = NNS, and pi is the identity permutation.
Note that this general notation includescases where a nonterminal occurs more than once in the right-hand side, for example,when n = 2, X = Y = A, and X1 = X2 = Y1 = Y2 = B, we can have the following tworules:A ?
B 1 B 2 , A ?
B 2 B 1 ;A ?
B 1 B 2 , A ?
B 1 B 2 .We also define an SCFG rule as n-ary if its permutation is of n and call an SCFGn-ary if its longest rule is n-ary.
Our goal is to produce an equivalent binary SCFG for aninput n-ary SCFG.However, not every SCFG can be binarized.
In fact, the binarizability of an n-ary rule is determined by the structure of its permutation, which can sometimes beresistant to factorization (Aho and Ullman 1972).
We now turn to rigorously definingthe binarizability of permutations.Definition 2A permuted sequence is a permutation of consecutive integers.
If a permuted sequencea can be split into the concatenation of two permuted sequences b and c, then (b; c)is called a proper split of a.
We say b < c if each element in b is smaller than anyelement in c.565Computational Linguistics Volume 35, Number 4For example, (3, 5, 4) is a permuted sequence whereas (2, 5) is not.
As special cases,single numbers are permuted sequences as well.
(3; 5, 4) is a proper split of (3, 5, 4)whereas (3, 5; 4) is not.
A proper split has the following property:Lemma 1For a permuted sequence a, a = (b; c) is a proper split if and only if b < c or c < b.ProofThe ?
direction is trivial by the definition of proper split.We prove the ?
direction by contradiction.
If b < c but b is not a permuted se-quence, i.e., the set of b?s elements is not consecutive, then there must be some x ?
csuch that min b < x < max b, which contradicts the fact that b < c. We have a similarcontradiction if c is not a permuted sequence.
Now that both b and c are permutedsequences, (b; c) is a proper split.
The case when b > c is similar.
?Definition 3A permuted sequence a is said to be binarizable if either1.
a is a singleton, that is, a = (a), or2.
there is a proper split a = (b; c) where b and c are both binarizable.
We callsuch split a binarizable split.This is a recursive definition, and it implies that there is a hierarchical binarizationpattern associated with each binarizable sequence, which we now rigorously define.Definition 4A binarization tree bi(a) of a binarizable sequence a is either1.
a if a = (a), or2.
[bi(b), bi(c)] if b < c, or ?bi(b), bi(c)?
otherwise, where a = (b; c) is abinarizable split, and bi(b) is a binarization tree of b and bi(c) abinarization tree of c.Here we use [] and ??
for straight (b < c) and inverted (b > c) combinations, respec-tively, following the ITG notation of Wu (1997).
Note that a binarizable sequence mighthave multiple binarization trees.
See Figure 4 for a binarizable sequence (1, 2, 4, 3) withits two possible binarization trees and a non-binarizable sequence (2, 4, 1, 3).We are now able to define the binarizability of SCFGs:Definition 5An SCFG is said to be binarizable if the permutation of each synchronous productionis binarizable.
We denote the class of binarizable SCFGs as bSCFG.This set, bSCFG, represents an important subclass of SCFG that is easy to handle(for example, parsable in O(|w|6)) and covers many interesting longer-than-two rules.The goal of synchronous binarization, then, is to convert a binarizable grammar G inbSCFG, which might be n-ary with n ?
2, into an equivalent binary grammar G?
thatgenerates the same string pairs (see Figure 5).
This is always possible because for eachrule in G with its permutation pi, there is a binarization tree bi(pi) which essentially566Huang et al Binarization of Synchronous Context-Free GrammarsFigure 4(a)?
(c) Alignment matrix and two binarization trees for the permutation sequence (1, 2, 4, 3).
(d) Alignment matrix for the non-binarizable sequence (2, 4, 1, 3).Figure 5Subclasses of SCFG.
The thick arrow denotes the direction of synchronous binarization andindicates bSCFG can collapse to binary SCFG.decomposes the original permutation into a set of binary ones.
All that remains is todecorate the skeleton binarization tree with nonterminal symbols and attach terminalsto the skeleton appropriately (see the next section for details).
We state this result as thefollowing theorem:Theorem 1For each grammar G in bSCFG, there exists a binary SCFG G?, such that L(G?)
= L(G).4.
Binarization AlgorithmsWe have reduced the problem of binarizing an SCFG rule into the problem of binarizingits permutation.
The simplest algorithm for this problem is to try all bracketings of apermutation and pick one that corresponds to a binarization tree.
The number of allpossible bracketings of a sequence of length n is known to be the Catalan Number(Catalan 1844)Cn = 1n + 1(2nn)which grows exponentially with n. A better approach is to reduce this problem to aninstance of synchronous ITG parsing (Wu 1997).
Here the parallel string pair that weare parsing is the integer sequence (1...n) and its permutation (pi(1)...pi(n)).
The goal ofthe ITG parsing is to find a synchronous tree that agrees with the alignment indicatedby the permutation.
Synchronous ITG parsing runs in time O(n6) but can be improvedto O(n4) because there is no insertion or deletion in a permutation.567Computational Linguistics Volume 35, Number 4Another problem besides efficiency is that there are possibly multiple binarizationtrees for many permutations whereas we just need one.
We would prefer a consistentpattern of binarization trees across different permutations so that sub-binarizations (vir-tual nonterminals) can be shared.
For example, permutations (1, 3, 2, 5, 4) and (1, 3, 2, 4)can share the common sub-binarization tree [1, ?3, 2?].
To this end, we can borrow thenon-ambiguous ITG of Wu (1997, Section 7) that prefers left-heavy binarization trees sothat for each permutation there is a unique synchronous derivation.3 We now refine thedefinition of binarization trees accordingly.Definition 6A binarization tree bi(a) is said to be canonical if the split at each non-leaf node of thetree is the rightmost binarizable split.For example, for sequence (1, 2, 4, 3), the binarization tree [[1, 2], ?4, 3?]
is canonical,whereas [1, [2, ?4, 3?]]
is not, because its top-level split is not at the rightmost binarizablesplit (1, 2; 4, 3).
By definition, there is a unique canonical binarization tree for eachbinarizable sequence.We next present an algorithm that is both fast and consistent.4.1 The Linear-Time Skeleton AlgorithmShapiro and Stephens (1991, page 277) informally present an iterative procedure that,in each pass, scans the permuted sequence from left to right and combines two adja-cent subsequences whenever possible.
This procedure produces canonical binarizationtrees and runs in O(n2) time because we need n passes in the worst case.
Inspired bythe Graham Scan Algorithm (Graham 1972) for computing convex hulls from computa-tional geometry, we modify this procedure and improve it into a linear-time algorithmthat only needs one pass through the sequence.The skeleton binarization algorithm is an instance of the widely used left-to-rightshift-reduce algorithm.
It maintains a stack for contiguous subsequences discovered sofar; for example: 2?5, 1.
In each iteration, it shifts the next number from the input andrepeatedly tries to reduce the top two elements on the stack if they are consecutive.
SeeAlgorithm 1 for the pseudo-code and Figures 6 and 7 for example runs on binarizableand non-binarizable permutations, respectively.We need the following lemma to prove the properties of the algorithm:Lemma 2If c is a permuted sequence (properly) within a binarizable permuted sequence a, thenc is also binarizable.ProofWe prove by induction on the length of a.
Base case: |a| = 2, a (proper) subsequence ofa, has length 1, so it is binarizable.
For |a| > 2, because a has a binarization tree, there3 We are not aiming at optimal sharing, that is, a strategy that produces the smallest binarized grammar for agiven ruleset, which would require a global optimization problem over the whole set.
In practice, we canonly use online algorithms that binarize rules one by one.
The left-heavy (or its symmetric variant,right-heavy) preference we choose here is one of the obvious candidates for consistency.568Huang et al Binarization of Synchronous Context-Free GrammarsAlgorithm 1 The linear-time binarization algorithm.1: function SYNCHRONOUSBINARIZER(pi)2: top ?
0 .
stack top pointer3: PUSH(stack, (pi(1),pi(1))) .
initial shift4: for i ?
2 to |pi| do .
for each remaining element5: PUSH(stack, (pi(i),pi(i))) .
shift6: while top > 1 and CONSECUTIVE(stack[top], stack[top ?
1]) do7: .
keep reducing if possible8: (p, q) ?
COMBINE(stack[top], stack[top ?
1])9: top ?
top ?
210: PUSH(stack, (p, q))11: return (top = 1) .
returns true iff.
the input is reduced to a single element12:13: function CONSECUTIVE((a, b), (c, d))14: return (b = c ?
1) or (d = a ?
1) .
either straight or inverted15: function COMBINE((a, b), (c, d))16: A = {min(a, c)...max(b, d)}17: B = {a...b}18: C = {c...d}19: rule[A] = A?
B C20: return (min(a, c), max(b, d))Figure 6Example of Algorithm 1 on the binarizable input (1, 5, 3, 4, 2).
The rightmost column shows thebinarization-trees generated at each reduction step.exists a (binarizable) split which is nearest to the root and splits c into two parts.
Let thesplit be (b1, c1; c2, b2), where c = (c1; c2), and either b1 or b2 can be empty.
By Lemma 1,we have c1 < c2 or c1 > c2.
By Lemma 1 again, we have that (c1; c2) is a proper split of c,i.e., both c1 and c2 are themselves permuted sequences.
We also know both (b1, c1) and(c2, b2) are binarizable.
By the induction hypothesis, c1 and c2 are both binarizable.
Sowe conclude that c = (c1; c2) is binarizable (See figure 8).
?We now state the central result of this work.Theorem 2Algorithm 1 runs in time linear to the length of the input, and succeeds (i.e., it reducesthe input into one single element) if and only if the input permuted sequence a isbinarizable, in which case the binarization tree recovered is canonical.569Computational Linguistics Volume 35, Number 4Figure 7Example of Algorithm 1 on the non-binarizable input (2, 5, 4, 1, 3).Figure 8Illustration of the proof of Lemma 2.
The arrangement of (b1, c1; c2, b2) must be either all straightas in (a) or all inverted as in (b).ProofWe prove the following three parts of this theorem:1.
If Algorithm 1 succeeds, then a is binarizable because we can recover abinarization tree from the algorithm.2.
If a is binarizable, then Algorithm 1 must succeed and the binarization treerecovered must be canonical:We prove by a complete induction on n, the length of a.Base case: n = 1, trivial.
Assume it holds for all n?
< n.If a is binarizable, then let a = (b; c) be its rightmost binarizable split.
Bydefinition, both b and c are binarizable.
By the induction hypothesis, thealgorithm succeeds on the partial input b, reducing it to the single elementstack[0] on the stack and recovering its canonical binarization tree bi(b).If c is a singleton, the algorithm will combine it with the element stack[0]and succeed.
The final binarization tree is canonical because the top-levelsplit is at the rightmost binarizable split, and both subtrees are canonical.If c is not a singleton, we want to show by contradiction that the algorithmwill never combine b with a proper prefix of c. Because a = (b; c) is aproper split, we know that either b < c or c < b.
Now if the algorithmmakes a combination of (b; c1) for some proper prefix c1 where c = (c1; c2),we have either c1 < c2 or c1 > c2.
By Lemma 1, (c1; c2) is a proper split.Because c is binarizable, by Lemma 2, c2 is also binarizable.
So (b, c1; c2) isa binarizable split to the right of (b; c), which contradicts the assumptionthat the latter is the rightmost binarizable split (see Figure 9).570Huang et al Binarization of Synchronous Context-Free GrammarsFigure 9Illustration of the proof of Theorem 2.
The combination of (b; c1) (in dashed squares) contradictsthe assumption that (b; c) is the rightmost binarizable split of a.Therefore, the algorithm will scan through the whole c as if from the emptystack.
By the induction hypothesis again, it will reduce c into stack[1] onthe stack and recover its canonical binarization tree bi(c).
Because b and care combinable, the algorithm reduces stack[0] and stack[1] in the last step,forming the canonical binarization tree for a, which is either [bi(b), bi(c)] or?bi(b), bi(c)?.3.
The running time of Algorithm 1 (regardless of success or failure) is linearin n:By amortized analysis (Cormen, Leiserson, and Rivest 1990), there areexactly n shifts and at most n ?
1 reductions, and each shift or reductiontakes O(1) time.
So the total time complexity is O(n).
?4.2 Dealing with Terminals and Adapting to Tree TransducersThus far we have discussed how to binarize synchronous productions involving onlynonterminals through binarizing the corresponding skeleton permutations.
We nowturn to technical details for the implementation of a synchronous binarizer in real MTsystems.
We will first show how to deal with the terminal symbols, and then describehow to adapt it to tree transducers.Consider the following SCFG rule:(7) ADJP ?
RB 1 # / fu`ze?
PP 2 ?
/ de NN 3 ,ADJP ?
RB 1 responsible for the NN 3 PP 2whose permutation is (1, 3, 2).
We run the skeleton binarization algorithm and getthe (canonical) binarization tree [1, ?3, 2?
], which corresponds to [RB, ?NN, PP?]
(seeFigure 10(a)).
The alignment matrix is shown in Figure 11.We will then do a post-order traversal of the skeleton tree, and attach the terminalsfrom both languages when appropriate.
It turns out we can do this quite freely as longas we can uniquely reconstruct the original rule from its binary parse tree.
We use thefollowing rules for this step:1.
Attach source-language terminals to the leaf nodes of the skeleton tree.Consecutive terminals are attached to the nonterminal on their left571Computational Linguistics Volume 35, Number 4Figure 10Attaching terminals in SCFG binarization.
(a) The skeleton binarization tree, (b) attachingChinese words at leaf nodes, (c) attaching English words at internal nodes.Figure 11Alignment matrix of the SCFG rule (7).
Areas shaded in gray and light gray denote virtualnonterminals (see rules in Example (8)).
(except for the initial ones which are attached to the nonterminal ontheir right).2.
Attach target-language terminals to the internal nodes (virtualnonterminals) of the skeleton tree.
These terminals are attached greedily:When combining two nonterminals, all target-side terminal stringsneighboring either nonterminal will be included.
This greedy merging ismotivated by the idea that the language model score helps to guide thedecoder and should be computed as early as possible.For example, at the leaf nodes, the Chinese word # / fu`ze?
is attached to RB 1 , and?
/ de to PP 1 (Figure 10(b)).
Next, when combining NN 3 and the virtual nonterminalPP-?
/ de, we also include the English-side string responsible for the (Figure 10(c)).
Inorder to do this rigorously we need to keep track of sub-alignments including bothaligned nonterminals and incorporated terminals.
A pre-order traversal of the fullydecorated binarization tree gives us the following binary SCFG rules:(8)ADJP ?
V1 1 V2 2 , ADJP ?
V1 1 V2 2V1 ?
RB 1 # / fu`ze?, V1 ?
RB 1V2 ?
V 13 NN 2 , V2 ?
responsible for the NN 2 V 13V3 ?
PP 1 ?
/ de, V3 ?
PP 1572Huang et al Binarization of Synchronous Context-Free Grammarswhere the virtual nonterminals (illustrated in Figure 11) are:V1: RB-# / fu`ze?V2: ?
resp.
for the NN, PP-?
/ de?V3: PP-?
/ deAnalogous to the ?dotted rules?
in Earley parsing for monolingual CFGs, the nameswe create for the virtual nonterminals reflect the underlying sub-alignments, ensuringintermediate states can be shared across different string-to-tree rules without causingambiguity.The whole binarization algorithm still runs in time linear in the number of symbolsin the rule (including both terminals and nonterminals).We now turn to tree transducer rules.
We view each left-hand side subtree as amonolithic nonterminal symbol and factor each transducer rule into two SCFG rules:one from the root nonterminal to the subtree, and the other from the subtree to theleaves.
In this way we can uniquely reconstruct the transducer derivation using thetwo-step SCFG derivation.
For example, consider the following tree transducer rule:We create a specific nonterminal, say, T859, which uniquely identifies the left-handside subtree.
This gives the following two SCFG rules:ADJP ?
T859 1 , ADJP ?
T859 1T859 ?
RB 1 # / fu`ze?
PP 2 ?
/ de NN 3 , T859 ?
RB 1 resp.
for the NN 3 PP 2The newly created nonterminals ensure that the newly created rules can only combinewith one another to reconstruct the original rule, leaving the output of the transducer,as well as the probabilities it assigns to transductions, unchanged.
The problem ofbinarizing tree transducers is now reduced to the binarization of SCFG rules, whichwe solved previously.5.
ExperimentsIn this section, we investigate two empirical questions with regard to synchronousbinarization.5.1 How Many Rules are (Synchronously) Binarizable?Shapiro and Stephens (1991) and Wu (1997, Section 4) show that the percentage ofbinarizable cases over all permutations of length n quickly approaches 0 as n grows573Computational Linguistics Volume 35, Number 4(see Figure 12).
However, for machine translation, the percentage of synchronous rulesthat are binarizable is what we care about.
We answer this question in both large-scaleautomatically aligned data and small-scale hand-aligned data.Automatically Aligned Data.
Our rule set here is obtained by first doing word alignmentusing GIZA++ on a Chinese?English parallel corpus containing 50 million words inEnglish, then parsing the English sentences using a variant of the Collins parser, andfinally extracting rules using the graph-theoretic algorithm of Galley et al (2004).
Wedid a spectrum analysis on the resulting rule set with 50,879,242 rules.
Figure 12 showshow the rules are distributed against their lengths (number of nonterminals).
We cansee that the percentage of non-binarizable rules in each bucket of the same length doesnot exceed 25%.
Overall, 99.7% of the rules are binarizable.
Even for the 0.3% of rulesthat are not binarizable, human evaluations show that the majority are due to alignmenterrors.
Because the rule extraction process looks for rules that are consistent with boththe automatic parses of the English sentences, and automatic word level alignmentsfrom GIZA++, errors in either parsing or word-level alignment can lead to noisy rulesbeing input to the binarizer.
It is also interesting to know that 86.8% of the rules havemonotonic permutations, i.e., either taking identical or totally inverted order.5.2 Hand-Aligned DataOne might wonder whether automatic alignments computed by GIZA++ are system-atically biased toward or against binarizability.
If syntactic constraints not taken intoaccount by GIZA++ enforce binarizability, automatic alignments could tend to containspurious non-binarizable cases.
On the other hand, simply by preferring monotonicalignments, GIZA++ might tend to miss complex non-binarizable patterns.
To test this,we carried out experiments on hand-aligned sentence pairs with three language pairs:Chinese?English, French?English, and German?English.Chinese?English Data.
For Chinese?English, we used the data of Liu, Liu, and Lin (2005)which contains 935 pairs of parallel sentences.
Of the 13,713 rules extracted using thesame method described herein, 0.3% (44) are non-binarizable, which is exactly theFigure 12The solid-line curve represents the distribution of all rules against permutation lengths.
Thedashed-line stairs indicate the percentage of non-binarizable rules in our initial rule set, and thedotted line denotes that percentage among all permutations.574Huang et al Binarization of Synchronous Context-Free Grammarssame ratio as the GIZA-aligned data.
The following is an interesting example of non-binarizable rules:where ... in with ... Mishira is the long phrase in shadow modifying Mishira.
Here thenon-binarizable permutation is (3, 2, 5, 1, 4), which is reducible to (2, 4, 1, 3).
The SCFGversion of the tree-transducer rule is as follows:where we indicate dependency links in solid arcs and permutation in dashed lines.It is interesting to examine dependency structures, as some authors have argued thatthey are more likely to generalize across languages than phrase structures.
The ChineseADVP 1 S) / da?ngtia?n (lit., that day) is translated into an English PP 1 (on the same day),but the dependency structures on both sides are isomorphic (i.e., this is an extremelyliteral translation).A simpler but slightly non-literal example is the following:(10) .........? e1j?`ny??bu`further[1jiu`on-fizho?ngdo?ngMideastq:we?ij?
?crisis]2 >L3ju?x??nghold4hu?`ta?ntalk...
hold3 further1 talks4 [on the Mideast crisis]2where the SCFG version of the tree-transducer rule (in the same format as the previousexample) is:Note that the Chinese ADVP 1 ? e / j?`ny?
?bu` modifying the verb VB 3 becomesa JJ 1 ( further) in the English translation modifying the object of the verb, NNS 4 , andthis change also happens to PP 2 .
This is an example of syntactic divergence, where thedependency structures are not isomorphic between the two languages (Eisner 2003).Wu (1997, page 158) has ?been unable to find real examples?
of non-binarizablecases, at least in ?fixed-word-order languages that are lightly inflected, such as English575Computational Linguistics Volume 35, Number 4and Chinese.?
Our empirical results not only confirm that this is largely correct (99.7% inour data sets), but also provide, for the first time, ?real examples?
between English andChinese, verified by native speakers.
It is interesting to note that our non-binarizableexamples include both cases of isomorphic and non-isomorphic dependency structures,indicating that it is difficult to find any general linguistic explanation that covers allsuch examples.
Wellington, Waxmonsky, and Melamed (2006) used a different measureof non-binarizability, which is on the sentence-level permutations, as opposed to rule-level permutation as in our case, and reported 5% non-binarizable cases for a differenthand-aligned English?Chinese data set, but they did not provide real examples.French?English Data.
We analyzed 447 hand-aligned French?English sentences from theNAACL 2003 alignment workshop (Mihalcea and Pederson 2003).
We found only 2out of 2,659 rules to be non-binarizable, or 0.1%.
One of these two is an instance oftopicalization:The second instance is due to movement of an adverbial:German?English Data.
We analyzed 220 sentences from the Europarl corpus, hand-aligned by a native German speaker (Callison-Burch, personal communication).
Of2,328 rules extracted, 13 were non-binarizable, or 0.6%.
Some cases are due to separableGerman verb prefixes:Here the German prefix auf is separated from the verb auffordern (request).
Anothercause of non-binarizability is verb-final word order in German in embedded clauses:Although fewer than 1% of the rules were non-binarizable in each language pairwe studied, German?English had the highest percentage with 0.6%.
The fact that theGerman?English examples are due to syntactic phenomena such as separable prefixesand verb-final word order may indicate that an MT system would have less freedomto choose an equivalent binarizable reordering than in the case of the examples dueto adverbial placement, heavy NP shift, and topicalization that we see in the Chinese?576Huang et al Binarization of Synchronous Context-Free GrammarsEnglish and French?English data.
The results on binarizability of hand-aligned data forthe three language pairs are summarized in Table 1.It is worth noting that for most of these non-binarizable examples, there do existalternative translations that only involve binarizable permutations.
For example, inChinese?English Example (9), we can move the English PP on the same day to the firstposition (before will), which results in a binarizable permutation (1, 3, 2, 5, 4).
Simi-larly, we can avoid non-binarizability in French?English Example (12) by moving theEnglish adverbial still under private ownership to the third position.
German?EnglishExample (13) would also become binarizable by replacing call on with a single wordrequest on the English side.
However, the point of this experiment is to test the ITGhypothesis by attempting to explain existing real data (the hand-aligned parallel text),rather than to generate fresh translations for a given source sentence, which is the topic ofthe subsequent decoding experiment.
This subsection not only provides the first solidconfirmation of the existence of linguistically-motivated non-binarizable reorderings,but also motivates further theoretical studies on parsing and decoding with these non-binarizable synchronous grammars, which is the topic of Sections 6 and 7.5.3 Does Synchronous Binarization Help Decoding?We did experiments on our CKY-based decoder with two binarization methods.
It isthe responsibility of the binarizer to instruct the decoder how to compute the languagemodel scores from children nonterminals in each rule.
The baseline method is mono-lingual left-to-right binarization.
As shown in Section 2, decoding complexity with thismethod is exponential in the size of the longest rule, and because we postpone all thelanguage model scorings, pruning in this case is also biased.To move on to synchronous binarization, we first did an experiment using thisbaseline system without the 0.3% of rules that are non-binarizable and did not observeany difference in BLEU scores.
This indicates that we can safely focus on the binarizablerules, discarding the rest.The decoder now works on the binary translation rules supplied by an externalsynchronous binarizer.
As shown in Section 1, this results in a simplified decoder with apolynomial time complexity, allowing less aggressive and more effective pruning basedon both translation model and language model scores.We compare the two binarization schemes in terms of translation quality withvarious pruning thresholds.
The rule set is that of the previous section.
The test sethas 116 Chinese sentences of no longer than 15 words, taken from the NIST 2002 testset.
Both systems use trigram as the integrated language model.
Figure 13 demonstratesthat decoding accuracy is significantly improved after synchronous binarization.
Thenumber of edges (or items, in the deductive parsing terminology) proposed duringTable 1Summary of non-binarizable ratios from hand-aligned data.Sentence Non-BinarizableLanguage Pair Pairs Rules Rules Major Causes for Non-BinarizationChinese?English 935 13,713 44 (0.3%) Adverbials, Heavy NP ShiftFrench?English 447 2,659 2 (0.1%) Adverbials, TopicalizationGerman?English 220 2,328 13 (0.6%) V-final, separable prefix, etc.577Computational Linguistics Volume 35, Number 4Figure 13Comparing the two binarization methods in terms of translation quality against search effort.Table 2Machine translation results for syntax-based systems vs. the phrase-based Alignment TemplateSystem.System BLEUmonolingual binarization 36.25synchronous binarization 38.44alignment-template system 37.00decoding is used as a measure of the size of search space, or time efficiency.
Our systemis consistently faster and more accurate than the baseline system.We also compare the top result of our synchronous binarization system with thestate-of-the-art alignment-template system (ATS) (Och and Ney 2004).
The results areshown in Table 2.
Our system has a promising improvement over the ATS system,which is trained on a larger data set but tuned independently.
A larger-scale systembased on our best result performs very well in the 2006 NIST MT Evaluation (ISIMachine Translation Team 2006), achieving the best overall BLEU scores in the Chinese-to-English track among all participants.4 The readers are referred to Galley et al (2004)for details of the decoder and the overall system.6.
One-Sided BinarizationIn this section and the following section, we discuss techniques for handling rulesthat are not binarizable.
This is primarily of theoretical interest, as we found that theyconstitute a small fraction of all rules, and removing these did not affect our Chinese-to-English translation results.
However, non-binarizable rules are shown to be importantin explaining existing hand-aligned data, especially for other language pairs such asGerman?English (see Section 5.2, as well as Wellington, Waxmonsky, and Melamed[2006]).
Non-binarizable rules may also become more important as machine translation4 NIST 2006 Machine Translation Evaluation Official Results: seehttp://www.nist.gov/speech/tests/mt/2006/doc/mt06eval official results.html.578Huang et al Binarization of Synchronous Context-Free GrammarsTable 3A summary of the four factorization algorithms, and the ?incremental relaxation?
theme of thewhole paper.
Algorithms 2?4 are for non-binarizable SCFGs, and are mainly of theoreticalinterest.
Algorithms 1?3 make fewer and fewer assumptions on the strategy space, and produceparsing strategies closer and closer to the optimal.
Algorithm 4 further improves Algorithm 3.Section Algorithm Assumptions of Strategy Space Complexity3?4 Alg.
1 (synchronous) Contiguous on both sides O(n)6 Alg.
2 (one-sided, CKY) Contiguous on one side O(n3)7.2 Alg.
3 (optimal) No assumptions O(3n)?
Alg.
4 (best-first) O(9kn2k)systems improve.
Synchronous grammars that go beyond the power of SCFG (andtherefore binary SCFG) have been defined by Shieber and Schabes (1990) and Rambowand Satta (1999), and motivated for machine translation by Melamed (2003), althoughprevious work has not given algorithms for finding efficient and optimal parsing strate-gies for general SCFGs, which we believe is an important problem.In the remainder of this section and the next section, we will present a series ofalgorithms that produce increasingly faster parsing strategies, by gradually relaxingthe strong ?continuity?
constraint made by the synchronous binarization technique.
Asthat technique requires continuity on both languages, we will first study a relaxationwhere binarized rules are always continuous in one of the two languages, but maybe discontinuous in the other.
We will present a CKY-style algorithm (Section 6.2) forfinding the best parsing strategy under this new constraint, which we call one-sidedbinarization.
In practice, this factorization has the advantage that we need to maintainonly one set of language model boundary words for each partial hypothesis.We will see, however, that it is not always possible to achieve the best asymptoticcomplexity within this constraint.
But most importantly, as the synchronous binariza-tion algorithm covers most of the SCFG rules in real data, the one-sided binarizationwe discuss in this section is able to achieve optimal parsing complexity for most ofthe non-binarizable rules in real data.
So this section can be viewed as a middle stepbetween the synchronous binarization we focus on in the previous sections and theoptimal factorization coming in Section 7, and also a trade-off point between simplicityand asymptotic complexity for parsing strategies of SCFGs.
Table 3 summarizes thisincremental structure of the whole paper.6.1 Formalizing the ProblemThe complexity for decoding given a grammar factorization can be expressed in terms ofthe number of spans of the items being combined at each step.
As an example, Figure 14shows the three combination steps for one factorization of the non-binarizable rule:X ?
A 1 B 2 C 3 D 4 , X ?
B 2 D 4 A 1 C 3 (15)At each step, we consider all positions in the Chinese string as possible end-points forthe rule?s child nonterminals.
Each step combines two dynamic programming itemscovering disjoint spans of the Chinese input, and creates a new item covering the unionof the spans.
For example, in the first combination step shown in Figure 14, wherenonterminals A and B are combined, A has one span in Chinese, from position y1 toy2 in the string, and B has one span from y3 to y4.
The chart entry for the nonterminal579Computational Linguistics Volume 35, Number 4Figure 14The tree at the top of the figure defines a three-step decoding strategy for rule (15), building anEnglish output string on the horizontal axis as we process the Chinese input on the vertical axis.In each step, the two subsets of nonterminals in the inner marked spans are combined into a newchart item with the outer spans.
The intersection of the outer spans, shaded, has now beenprocessed.pair {A, B} must record a total of four string indices: positions y1, y2, y3, and y4 in theChinese string.Any combination of two subsets of the rule?s nonterminals involves the indices forthe spans of each subset.
However, some of the indices are tied together: If we are joiningtwo spans into one span in the new item, one of the original spans?
end-points must beequal to another span?s beginning point.
For example, the index y2 is the end-point ofA in Chinese, as well as the beginning position of D. In general, if we are combining asubset B of nonterminals having b spans with a subset C having c spans, to produce aspans for a combined subset A = B ?
C, the number of linked indices is b + c ?
a.
In theexample of the first step of Figure 14, subset {A} has two spans (one in each language)so b = 1, and {B} also has two spans, so c = 1.
The combined subset {A, B} has twospans, so a = 2.
The total number of indices involved in a combination of two subsets is2(b + c) ?
(b + c ?
a) = a + b + c (16)where 2(b + c) represents the original beginning and end points, and b + c ?
a thenumber of shared indices.
In the first step of Figure 14, a + b + c = 1 + 1 + 2 = 4 totalindices, and therefore the complexity of this step is O(|w|4) where |w| is the length of theinput Chinese strings, and we ignore the language model for the moment.
Applyingthis formula to the second and third step, we see that the second is O(|w|5), and thethird is again O(|w|4).In order to find a good decoding strategy for a given grammar rule, we need tosearch over possible orders in which partial translation hypotheses can be built by suc-cessively combining nonterminals.
Any strategy we find can be used for synchronousparsing as well as decoding.
For example, the strategy shown in Figure 14 can be used toparse an input Chinese/English string pair.
The complexity of each step is determinedby the total number of indices into both the Chinese and English strings.
Each step in580Huang et al Binarization of Synchronous Context-Free GrammarsAlgorithm 2 An O(n3) CKY-style algorithm for parsing strategies, keeping continuousspans in one language.
Takes an SCFG rule?s permutation as input and returns thecomplexity of the best parsing strategy found.1: function BESTCONTINUOUSPARSER(pi)2: n = |pi|3: for span ?
1 to n ?
1 do4: for i ?
1 to n ?
span do5: A = {i...i + span}6: best[A] = ?7: for j ?
i + 1 to i + span do8: B = {i...j ?
1}9: C = { j...i + span}10: Let api, bpi, and cpi denote the number of pi(A), pi(B), and pi(C)?s spans11: compl[A?
B C] = max {api + bpi + cpi, best[B], best[C]}12: if compl[A?
B C] < best[A] then13: best[A] = compl[A?
B C]14: rule[A] = A?
B C15: return best[{1...n}]the diagram has three indices into the English string, so the complexity of the first stepis O(|w|4+3) = O(|w|7), the second step is O(|w|8), and the third is again O(|w|7).6.2 A CKY-Style Algorithm for Parsing StrategiesThe O(n3) algorithm we present in this section can find good factorizations for mostnon-binarizable rules; we discuss optimal factorization in the next section.
Thisalgorithm, shown in Algorithm 2, considers only factorizations that have only onespan in one of the two languages, and efficiently searches over all such factorizationsby combining adjacent spans with CKY-style parsing.5 The input is an SCFG grammarrule in its abstract form, which is a permutation, and best is a dynamic programmingtable used to store the lowest complexity with which we can parse a given subset of theinput rule?s child nonterminals.Although this CKY-style algorithm finds the best grammar factorization maintain-ing continuous spans in one of the two dimensions, in general the best factorizationmay require discontinuous spans in both dimensions.
As an example, the followingpattern causes problems for the algorithm regardless of which of the dimensions itparses across:1 ?
1 n/2 + 1 ?
22 ?
n/2 + 1 n/2 + 2 ?
n/2 + 23 ?
3 n/2 + 3 ?
44 ?
n/2 + 3 n/2 + 4 ?
n/2 + 4...
...This pattern, shown graphically in Figure 15 for n = 16, can be parsed in time O(|w|10)by maintaining a partially completed item with two spans in each dimension, one5 A special case of this algorithm, target-side binarization, is discussed in Huang (2007).
It binarizesleft-to-right on the target side while leaving gaps on the source side, and is shown to be preferable tosource-side monolingual binarization in m-gram integrated decoding.581Computational Linguistics Volume 35, Number 4Figure 15Left, a general pattern of non-binarizable permutations.
Center, a partially completed chart itemwith two spans in each dimension; the intersection of the completed spans is shaded.
Right, thecombination of the item from the center panel with a singleton item.
The two subsets ofnonterminals in the inner marked spans are combined into a new chart item with the outerspans.beginning at position 1 and one beginning at position n2 + 1, and adding one non-terminal at a time to the partially completed item, as shown in Figure 15 (right).
How-ever, our CKY factorization algorithm will give a factorization with n/2 discontinuousspans in one dimension.
Thus in the worst case, the number of spans found by thecubic-time algorithm grows with n, even when a constant number of spans is possible,implying that there is no approximation ratio on how close the algorithm will get to theoptimal solution.7.
Optimal FactorizationThe method presented in the previous section is not optimal for all permutations,because in some cases it is better to maintain multiple spans in the output language(despite the extra language model state that is needed) in order to maintain continuousspans in the input language.
In this section we give a method for finding decodingstrategies that are guaranteed to be optimal in their asymptotic complexity.This method can also be used to find the optimal strategy for synchronous parsing(alignment) using complex rules.
This answers a question left open by earlier work insynchronous grammars: Although Satta and Peserico (2005) show that tabular parsingof a worst-case SCFG can be NP-hard, they do not give a procedure for finding the com-plexity of an arbitrary input grammar.
Similarly, Melamed (2003) defines the cardinalityof a grammar, and discusses the interaction of this property with parsing complexity,but does not show how to find a normal form for a grammar with the lowest possiblecardinality.We show below how to analyze parsing and decoding strategies for a given SCFGrule in Section 7.1, and then present an exponential-time dynamic programming algo-rithm for finding the best strategy in Section 7.2.
We prove that factorizing an SCFGrule into smaller SCFG rules is a safe preprocessing step for finding the best strategyin Section 7.4, which leads to much faster computation in many cases.
First, however,we take a brief detour to discuss modifying our a + b + c formula from the previoussection in order to take the state from an m-gram language model into account duringMT decoding.582Huang et al Binarization of Synchronous Context-Free Grammars7.1 Taking the Language Model into AccountFirst, how do we analyze algorithms that create discontinuous spans in both the sourceand target language?
It turns out that the analysis in Section 6.1 for counting stringindices in terms of spans in fact applies in the same way to both of our languages.For synchronous parsing, if we are combining item B with be target spans and bfsource spans with item C having ce target spans and cf source spans to form newitem A having ae target spans and af source spans, the complexity of the operation isO(|w|ae+be+ce+af+bf+cf ).The a + b + c formula can also be generalized for decoding with an integratedm-gram language model.
At first glance, because we need to maintain (m ?
1) boundarywords at both the left and right edges of each target span, the total number of interactingvariables is:2(m ?
1)(be + ce) + af + bf + cfHowever, by using the ?hook trick?
suggested by Huang, Zhang, and Gildea (2005),we can optimize the decoding algorithm by factorizing the dynamic programmingcombination rule into two steps.
One step incorporates the language model probability,and the other step combines adjacent spans in the input language and incorporates theSCFG rule probability.
The hook trick for a bigram language model and binary SCFGis shown in Figure 16.
In the equations of Figure 16, i,j,k range over 1 to |w|, the lengthof the input foreign sentence, and u,v,v1,u2 (or u,v,v2,u1) range over possible Englishwords, which we assume to take O(|w|) possible values.
There are seven free variablesrelated to input size for doing the maximization computation, hence the algorithmiccomplexity is O(|w|7).The two terms in Figure 16 (top) within the first level of the max operator corre-spond to straight and inverted ITG rules.
Figure 16 (bottom) shows how to decom-pose the first term; the same method applies to the second term.
Counting the freevariables enclosed in the innermost max operator, we get five: i, k, u, v1, and u2.
The?
(X[i, j, u, v]) = max???????maxk,v1,u2,Y,Z[?
(Y[i, k, u, v1]) ?
?
(Z[k, j, u2, v])?
P(X ?
[YZ]) ?
bigram(v1, u2)],maxk,v2,u1,Y,Z[?
(Y[i, k, u1, v]) ?
?
(Z[k, j, u, v2])?
P(X ?
?YZ?)
?
bigram(v2, u1)]???????wheremaxk,v1,u2,Y,Z[?
(Y[i, k, u, v1]) ?
?
(Z[k, j, u2, v]) ?
P(X ?
[YZ]) ?
bigram(v1, u2)]= maxk,u2,Z[maxv1,Y[?
(Y[i, k, u, v1]) ?
P(X ?
[YZ]) ?
bigram(v1, u2)]?
?
(Z[k, j, u2, v])]Figure 16The hook trick for machine translation decoding with a binary SCFG (equivalent to InversionTransduction Grammar).
The fundamental dynamic programming equation is shown at the top,with an efficient factorization shown below.583Computational Linguistics Volume 35, Number 4decomposition eliminates one free variable, v1.
In the outermost level, there are six freevariables left.
The maximum number of interacting variables is six overall.
So, we havereduced the complexity of ITG decoding using the bigram language model from O(|w|7)to O(|w|6).When we generalize the hook trick for any m-gram language model and morecomplex SCFGs, each left boundary for a substring of an output language hypothesiscontains m ?
1 words of language model state, and each right boundary contains a?hook?
specifying what the next m ?
1 words must be.
This yields a complexity analysissimilar to that for synchronous parsing, based on the total number of boundaries, butnow multiplied by a factor of m ?
1:(m ?
1)(ae + be + ce) + af + bf + cf (17)for translation from source to target.Definition 7The number of m-gram weighted spans of a constituent, denoted am, is defined as thenumber of source spans plus the number target spans weighted by the language modelfactor (m ?
1):am = af + (m ?
1)ae (18)Using this notation, we can rewrite the expression for the complexity of decodingin Equation 17 as a simple sum of the numbers of weighted spans of constituent subsetsA, B, and C:am + bm + cm (19)and more generally when k ?
2 constituents are combined together:(af + (m ?
1)ae) +k?i=1(bfi + (m ?
1)bei) = am +k?i=1bmi (20)It can be seen that, as m grows, the parsing/decoding strategies that favor contiguity onthe output side will prevail.
This effect is demonstrated by the experimental results inSection 7.5.This analysis applies to one combination of two subsets of a rule?s children duringparsing or decoding.
A strategy for parsing (or decoding) the entire rule must build upthe complete set of the rule?s children through a sequence of such combinations.
Thusa parsing strategy corresponds to a recursive partitioning of the rule?s children, thatis, an unordered rooted tree having the child nonterminals as leaves.
Each node in thepartition tree represents a subset of nonterminals used as a partial result in the chartfor parsing, built by combining the subsets corresponding to the node?s children.
Thiscombination step at each node has complexity determined by the number of spans,and the overall complexity of a parsing strategy is the complexity of the strategy?sworst combination step.
We wish to find the recursive partition with the lowest overallcomplexity.
Unfortunately, the number of recursive partitions of n items grows super-584Huang et al Binarization of Synchronous Context-Free Grammarsexponentially, as 0.175n!n?3/22.59n = ?(?
(n + 1)2.59n) (Schro?der 1870, Problem IV).6More formally, the optimization over the space of all recursive partitions is expressed as:best(A) = minB:A=?ki=1 Bimax{compl(A?
B1...Bk), maxki=1 best(Bi)} (21)where compl(A?
B1...Bk) is given by Equation (20).
This recursive equation impliesthat we can solve the optimization problem using dynamic programming techniques.7.2 Combining Two Subsets at a Time Is OptimalIn this section we show that a branching factor of more than two is not necessaryin our recursive partitions, by showing that any ternary combination can be factoredinto two binary combinations with no increase in complexity.
This fact leads to a moreefficient, but still exponential, algorithm for finding the best parsing strategy for a givenSCFG rule.Theorem 3For any SCFG rule, if there exists a recursive partition of child nonterminals whichenables tabular parsing of an input sentence w in time O(|w|k), then there exists arecursive binary partition whose corresponding parser is also O(|w|k).ProofWe use the notion of number of weighted spans (Equation (20)) to concisely analyze thecomplexity of synchronous parsing/decoding.
For any fixed m, we count a constituent?snumber of spans using the weighted span value from Equation (18), and we drop boththe adjective ?weighted?
and the m subscript from this point forward.If we are combining k subsets Bi (i = 1, ..., k, k ?
2) together to produce a new subsetA = ?i Bi, the generalized formula for counting the total number of indices isa +k?i=1bi (22)where bi is the number of spans for Bi and a is the number of spans for the resultingitem A.Consider a ternary rule X ?
AB C where X has x spans, A has a spans, B has bspans, and C has c spans.
In the example shown in Figure 17, x = 2, a = 2, b = 1, and c =2.
The complexity for parsing this is O(|w|a+b+c+x).
Now factor this rule into two rules:X ?
Y CY ?
AB6 We use ?
to indicate an asymptotic bound that is tight in both directions.585Computational Linguistics Volume 35, Number 4and refer to the number of spans in partial constituent Y as y. Parsing Y ?
AB takestime O(|w|y+a+b), so we need to show thaty + a + b ?
a + b + c + xto show that we can parse this new rule in no more time than the original ternary rule.Subtracting a + b from both sides, we need to prove thaty ?
c + xEach of the y spans in Y corresponds to a left edge.
(In the case of decoding, each edgehas a multiplicity of (m ?
1) on the output language side.)
The left edge in each spanof Y corresponds to the left edge of a span in X or to the right edge of a span in C.Therefore, Y has at most one span for each span in C ?
X , so y ?
c + x.Returning to the first rule in our factorization, the time to parse X ?
Y C isO(|w|x+y+c).
We know thaty ?
a + bsince Y was formed from A and B. Thereforex + y + c ?
x + (a + b) + cso parsing X ?
Y C also takes no more time than the original rule X ?
AB C. Byinduction over the number of subsets, a rule having any number of subsets on the right-hand side can be converted into a series of binary rules.
?Our finding that combining no more than two subsets of children at a time is opti-mal implies that we need consider only binary recursive partitions, which correspond tounordered binary rooted trees having the SCFG rule?s child nonterminals as leaves.
Thetotal number of binary recursive partitions of n nodes is (2n?3)!2n?2(n?2)!
= ?(?
(n ?
12 )2n?1)(Schro?der 1870, Problem III).
Note that this number grows much faster than the CatalanNumber, which characterizes the number of bracketings representing the search space ofsynchronous binarization (Section 4).Although the total number of binary recursive partitions is still superexponential,the binary branching property also enables a straightforward dynamic programmingalgorithm, shown in Algorithm 3.
The same algorithm can be used to find optimalstrategies for synchronous parsing or for m-gram decoding: for parsing, the variablesa, b, and c in Line 9 refer to the total number of spans of A, B, and C (Equation (16)),Figure 17Left, example spans for a ternary rule decomposition X ?
AB C. Each symbol represents asubset of nonterminals from the original SCFG rule, and the subsets may cover discontinuousspans in either language.
Line segments represent the projection of each set of child nonterminalsinto a single language, as in Figure 15.
Right, factorization into X ?
Y C and Y ?
AB.586Huang et al Binarization of Synchronous Context-Free GrammarsAlgorithm 3 An O(3n) search algorithm for the optimal parsing strategy that maycontain discontinuous spans.1: function BESTDISCONTINUOUSPARSER(pi)2: n = |pi|3: for i ?
2 to n do4: for A ?
{1...n} s.t.
|A| = i do5: best[A] ?
?6: for B, C s.t.
A = B ?
C ?
B ?
C = ?
do7: Let a, b, and c denote the number of8: (A,pi(A)), (B,pi(B)), and (C,pi(C))?s spans9: compl[A?
B C] = max {a + b + c, best[B], best[C]}10: if compl[A?
B C] < best[A] then11: best[A] ?
compl[A?
B C]12: rule[A] ?
A?
B C13: return best[{1...n}]while for decoding, a, b, and c refer to weighted spans (Equation (19)).
The dynamicprogramming states correspond to subsets of the input rule?s children, for which anoptimal strategy has already been computed.
In each iteration of the algorithm?s innerloop, each of the child nonterminals is identified as belonging to B, C, or neither B norC, making the total running time of the algorithm O(3n).
Although this is exponential inn, it is a significant improvement over considering all recursive partitions.The algorithm can be improved by adopting a best-first exploration strategy (Knuth1977), in which dynamic programming items are placed on a priority queue sortedaccording to their complexity, and only used to build further items after all items oflower complexity have been exhausted.
This technique, shown in Algorithm 4, guar-antees polynomial-time processing on input permutations of bounded complexity.
Tosee why this is, observe that each rule of the form A?
B C that has complexity nogreater than k can be written using a string of ke < k indices into the target nonterminalstring to represent the spans?
boundaries.
For each index we must specify whether thecorresponding nonterminal either starts a span of subset B, starts a span of subset C, orends a span of B ?
C. Therefore there are O((3n)k) rules of complexity no greater thank.
If there exists a parsing strategy for the entire rule with complexity k, the best-firstalgorithm will find it after, in the worst case, popping all O((3n)k) rules of complexityless than or equal to k off of the heap in the outer loop, and combining each one with allother O((3n)k) such rules in the inner loop, for a total running time of O(9kn2k).
Althoughthe algorithm is still exponential in the rule length n in the worst case (when k is linearlycorrelated to n), the best-first behavior makes it much more practical for our empiricallyobserved rules.7.3 Adding One Nonterminal at a Time Is Not OptimalOne might wonder whether it is necessary to consider all combinations of all subsetsof nonterminals, or whether an optimal parsing strategy can be found by adding onenonterminal at a time to an existing subset of nonterminals until the entire permutationhas been covered.
Were such an assumption warranted, this would enable an O(n2n)dynamic programming algorithm.
It turns out that one-at-a-time parsing strategiesare sometimes not optimal.
For example, the permutation (4, 7, 3, 8, 1, 6, 2, 5), shownin Figure 18, can be parsed in time O(|w|8) using unconstrained subsets, but only in587Computational Linguistics Volume 35, Number 4Figure 18The permutation (4, 7, 3, 8, 1, 6, 2, 5) cannot be efficiently parsed by adding one nonterminal at atime.
The optimal grouping of nonterminals is shown on the right.time O(|w|10) by adding one nonterminal at a time.
All permutations of less than eightelements can be optimally parsed by adding one element at a time.7.4 Discontinuous Parsing Is Necessary Only for Non-Decomposable PermutationsIn this subsection, we show that an optimal parsing strategy can be found by firstfactoring an SCFG rule into a sequence of shorter SCFG rules, if possible, and thenconsidering each of the new rules independently.
The first step can be done efficientlyusing the algorithms of Zhang and Gildea (2007).
The second step can be done in timeO(9kc ?
n2kc) using Algorithm 4, where kc is the complexity of the longest SCFG ruleafter factorizations, implying that kc ?
(n + 4).
We show that this two-step process isoptimal, by proving that the optimal parsing strategy for the initial rule will not needto build subsets of children that cross the boundaries of the factorization into shorterSCFG rules.Figure 19 shows a permutation that contains permutations of fewer numbers withinitself so that the entire permutation can be decomposed hierarchically.
We prove thatif there is a contiguous block of numbers that are permuted within a permutation, theAlgorithm 4 Best-first search for the optimal parsing strategy.1: function BESTDISCONTINUOUSPARSER(pi)2: n = |pi|3: for A ?
{1...n} do4: chart[A] = ?5: for i ?
1...n do6: push[heap, 0, {i}] .
Priority queue of good subsets7: while chart[{1 .
.
.
n}] = ?
do8: B ?
pop(heap)9: chart[B] ?
best[B] .
guaranteed to have found optimal analysis for subset B10: for C s.t.
B ?
C = ?
?
chart[C] < ?
do11: A?
B ?
C12: Let a, b, and c denote the number of13: (A,pi(A)), (B,pi(B)), and (C,pi(C))?s spans14: compl[A?
B C] = max {a + b + c, best[B], best[C]}15: if compl[A?
B C] < best[A] then16: best[A] ?
compl[A?
B C]17: rule[A] ?
A?
B C18: push(heap, best[A],A)19: return best[{1...n}]588Huang et al Binarization of Synchronous Context-Free GrammarsFigure 19A permutation that can be decomposed into smaller permutations hierarchically.
We prove thatthis decomposition corresponds to the optimal parsing strategy for an SCFG rule with thispermutation.optimal parsing strategy for the entire permutation does not have to involve interactionsbetween subsets of numbers inside and outside the block.
We call filled entries in thepermutation matrix pebbles; the contiguous blocks are shaded in Figure 19, and formsubmatrices with a pebble in each row and column.
We can first decompose a givenpermutation into a hierarchy of smaller permutations as the tree shown in Figure 19and then apply the discontinuous strategy to the non-decomposable permutations inthe tree.
So, in this example, we just need to focus on the optimal parsing strategy for(2, 4, 1, 3), which is applied to permute (4, 5, 6, 7) into (5, 7, 4, 6).
By doing this kind ofminimization, we can effectively reduce the search space without losing optimality ofthe parsing strategy for the original permutation.Theorem 4For any SCFG rule, if there exists a recursive partition of child nonterminals whichenables tabular parsing of an input sentence w in time O(|w|k), and if S is a subsetof child nonterminals forming a single continuous span in each language, then thereexists a recursive partition containing S as a member whose corresponding parser isalso O(|w|k).See the Appendix for the proof.
See Table 3 for a summary of the four factorizationalgorithms presented in this article (Algorithms 3 and 4 can be improved by firstfactorizing the permutation into smaller permutations [Section 7.4]).7.5 ExperimentsThe combination of minimizing SCFG rule length as a preprocessing step and thenapplying the best-first version of Algorithm 3 makes it possible to find optimal parsingstrategies for all of the rules in the large Chinese?English rule set used for our decodingexperiments.
For the 157,212 non-binarizable rules (0.3% of the total), the complexityof the optimal parsing strategies is shown in Table 4.
Although the worst parsingcomplexity is O(|w|12), this is only achieved by a single rule.
The best-first analyzertakes approximately five minutes of CPU time to analyze this single rule, but processesall others in less than one second.We tested the CKY-based factorization algorithm on our set of non-binarizablerules extracted from the Chinese?English data.
The CKY-on-English method found anoptimal parsing strategy for 98% of the rules, and its worst-case complexity over theentire ruleset was O(|w|15), rather than the optimal O(|w|12).
If we run CKY factorizationfrom two directions (one for the permutation pi and the other for the permutation pi?1)and take the minimum of both, we can get an even better approximation.
In Table 4,we compare the approximate strategy which takes the minimum of CKY runs for589Computational Linguistics Volume 35, Number 4Table 4The distribution of parsing complexities of non-binarizable rules extracted from theGIZA-aligned Chinese?English data in Section 5.
The first column denotes the exponent of thetime complexity?for example, 10 means O(|w|10).
opt denotes the optimal parsing strategy andcky-min denotes the approximation strategy that takes the better of the CKY results on both sides.Complexities Synchronous Parsing Trigram Decoding Trigram Decoding/Bigram Decoding (Chinese to English) (English to Chinese)opt cky-min opt cky-min opt cky-min19 118 117 1 11615 1 7 10 3 414 4 3 6 613 2240 2238 1080 107912 1 1 610 610 548 54811 154,350 154,350 155,574 155,57410 68 1569 101 3078 157,042 156,747two languages, which we call CKY-min, with the optimal strategy.
For synchronousparsing, for 99.77% of the rules, the CKY-min method found an optimal strategy.
Whengeneralized for m-gram integrated decoding, CKY maintains continuous spans on theoutput language and allows for discontinuous parsing on the input sentence.
The differ-ence between CKY-on-output and the optimal decoding strategy was negligible in thesituation of trigram-integrated decoding for the given rules.
The worst-case complexityfor decoding into English by CKY-on-English was O(|w|18), versus O(|w|17) from theoptimal strategy.
The CKY-on-English approach found an optimal decoding strategyfor 99.97% of the non-binarizable rules.
The CKY-min strategy was even better, onlyfinding sub-optimal results for six rules out of all rules, which translates to 99.996%.
InTable 4, we have also included the comparison for translating into Chinese, in whichcase the inverted permutations are used and the language model weight is put on theChinese side.
A similar approximation accuracy was achieved.7.6 Bounds on Complexity of FactorizationGiven that our algorithms for optimal factorization are exponential, it is natural toask whether the problem is provably NP-complete.
Gildea and S?tefankovic?
(2007)relate the problem of finding the optimal parsing strategy for a rule to computingthe treewidth of a graph derived from the rule?s permutation.
Computing treewidth ofarbitrary graphs is NP-complete (Arnborg, Corneil, and Proskurowski 1987), but thegraphs derived from SCFG permutations have a restricted structure that it might bepossible to exploit.
In particular, the graphs have degree no greater than six.
Whilecomputing treewidth for graphs of bounded degree nine was shown to be NP-completeby Bodlaender and Thilikos (1997), whether the treewidth problem for graphs of degreebetween three and eight is NP-complete is not known.
Thus, whether computing theoptimal parsing strategy for an SCFG rule is NP-complete remains an interesting openproblem.590Huang et al Binarization of Synchronous Context-Free Grammars8.
ConclusionThis work develops a theory of binarization for synchronous context-free grammars.
Wepresent a technique called synchronous binarization along with an efficient binariza-tion algorithm.
Empirical study shows that the vast majority of syntactic reorderings, atleast between languages like English and Chinese, can be efficiently decomposed intohierarchical binary reorderings.
As a result, decoding with n-gram models can be fastand accurate, making it possible for our syntax-based system to overtake a comparablephrase-based system in BLEU score.There are, however, some interesting rules that are not binarizable, and we pro-vide, for the first time, real examples verified by native speakers.
For these remainingrules, we have shown an exponential time algorithm for finding optimal parsing strate-gies, which runs quite fast with the help of two optimality-maintaining operations andthe A* search strategy.
We also provide an efficient approximation, which usually findsoptimal parsing strategies in practice.
As non-binarizable rules did not improve ourtranslation system, these parsing strategies are primarily of theoretical interest, thoughthey may become more important in future systems.AcknowledgmentsMuch of this work was done while the first two authors were visiting USC/ISI.
This work waspartially funded by NSF grants IIS-0428020 and EIA-0205456.Appendix A.
Proof of Theorem 4We prove by contradiction.
Let us suppose that the optimal parsing strategy for apermutation P splits a contiguous block S of P into two subtrees TL and TR, as shownat the top of Figure 20, in either or both of which there are some pebbles from outsideS.
As in Section 7, we count spans in this section using the weighted span value ofEquation (18) to account for m-gram language model state.
We use dLO to denote thenumber of spans of the pebbles outside of S in TL.
dLI is the number of spans of thepebbles inside S for TL.
We use rL to denote the reduction in the number of spansachieved by merging the pebbles inside and outside of S for TL.
So, the number of spansof the root of TL is dLO + dLI ?
rL.
We have symmetric notions for TR.
We use d to denotethe number of spans of the root of the subtree T after merging TL and TR.
The numberof spans is annotated for each node in Figure 20.Notice that (rR + rL) ?
2m because there are at most two boundaries shared byinside pebbles and outside pebbles in each language.
Each boundary in the sourcecorresponds to the reduction of one span.
Each boundary in the target corresponds tothe reduction of one weighted span of (m ?
1).
In total, we can reduce the number of(weighted) spans by no more than 2(m ?
1) + 2 = 2m.
This inequality implies eitherrR ?
m or rL ?
m. Without loss of generality, we assumerR ?
m (A.1)Given the decomposition into TL and TR, the yield of the best strategy throughoutT isbest(T) = max { best(TL), best(TR), ((dLO + dLI ?
rL) + (dRI + dRO ?
rR) + d) } (A.2)591Computational Linguistics Volume 35, Number 4Figure 20Reorganization of a parsing strategy to build a continuous span S first.Figure 21 gives a concrete example.
The permutation is (5, 7, 4, 6, 1, 2, 3).
The block Swe focus on is (5, 7, 4, 6).
The original strategy at the top of the figure splits the block intoTL and TR.
The improved strategy on the bottom merges the pebbles inside S togetherbefore making combinations with pebbles outside S.Figure 21An actual example of reorganization of a parsing strategy to build a continuous span S first.Before, the overall strategy cost is (7m ?
3).
After, the cost is (5m ?
2).
Note that (m ?
2).
We useblack to represent pebbles in the right branch of the root node and white for the left branch.
Grayareas are continuous blocks within the permutation.
The reorganized strategy can be furtherimproved by making another such transformation to allow for the lower right corner pebbles togroup before interacting with the upper left corner.592Huang et al Binarization of Synchronous Context-Free GrammarsIn general, we argue that we can have an equally good or better strategy by separat-ing each of TL and TR into two trees involving pebbles purely inside or outside of S, asshown at the bottom of Figure 20.
The separation works by simply ignoring the pebblesthat are not inside when creating the inside half of the tree or outside when doing theoutside half throughout TL and TR.
Then we have four elementary subtrees TLO, TLI,TRI, and TRO.
In our new strategy, we recombine the four elementary trees by mergingTLI and TRI to create a pebble first and merging the resulting pebble back into TLO tomake a T?LO, and finally merging T?LO with TRO.The elementary trees yield better strategies because the number of spans of eachnode in these trees is reduced or not changed as compared to that before separation.Using the a + b + c formula with reduced a, b, and c will produce lower complexity.Roughly speaking, the reason is the inside pebbles and outside pebbles are positionedside by side instead of mixed together.
Mathematically, the reduction of spans bycombining both sides is upper-bounded by 2m, considering there are two boundariesin each language.
At the same time, the number of spans of either the inside pebblesor the outside pebbles is lower-bounded by 2m because both TL and TR only partiallycover S. Hence, we have the following set of inequalities:best(TLI ) ?
best(TL) (A.3)best(TRI ) ?
best(TR) (A.4)best(TRO) ?
best(TR) (A.5)Now we consider what happens when the pebble of S joins TLO.
Because TLO iscreated from TL by pruning away the pebbles that are inside S, the pebble of S can joinTLO by taking the place of any trace of the pruned leaves and making the number ofspans from the bottom up to the root no greater than in the counterpart nodes in TL.So the fragment of the new left subtree T?LO with S being its leaf has a better yieldthan the original TL:best(T?LO/S) ?
best(TL) (A.6)where we use T?LO/S to denote the tree fragment excluding the nodes under S. Thenumber of spans for each node in the reorganized tree is shown in Figure 20 (bottom),where r (?
2m) is the reduction in spans after combining the new pebble S with TLO.
rsums up the reductions achievable on the four boundaries of S with TLO, while rL sumsup the reductions on some of the four boundaries.
Thus,rL ?
r (A.7)The final yield of the updated strategy isbest(T?)
= max??
?best(T?LO/S), best(TLI ), best(TRI ), best(TRO),(dLI + dRI + m),((dLO + m ?
r) + dRO + d)???
(A.8)We have shown the first four terms inside the maximization are bounded by theyield of the old strategy (Equations [A.3?A.6]).
We need to bound the remaining terms.593Computational Linguistics Volume 35, Number 4Both of them can be bounded by the third term inside the maximization of Equa-tion (A.2).
The first inequality isdLI + dRI + m ?
(dLO + dLI ?
rL) + (dRI + dRO ?
rR) + d (A.9)which is equivalent tom ?
(dLO + dRO) ?
(rL + rR) + dwhich is true because d ?
m (since T has at least one pebble), and dLO ?
rL and dRO ?
rR(since the number of reducible spans is less than the total number of outside spans).Our final bound relates the last terms of Equations (A.2) and (A.8)((dLO + m ?
r) + dRO + d) ?
((dLO + dLI ?
rL) + (dRI + dRO ?
rR) + d) (A.10)This simplifies tom ?
r ?
dLI + dRI ?
rR ?
rLThis inequality is true because m ?
dLI, since there is at least one inside pebble in TL, anddRI ?
rR because dRI ?
m ?
rR, referring to Equation (A.1), and finally r ?
rL, as shownin Equation (A.7).Figure 20 also demonstrates the re-distribution of numbers of spans after the reor-ganization.
In the example, the updated parsing/decoding complexity is O(|w|5m?2),better than before (O(|w|7m?3)).Therefore, any synchronous parsing/decoding strategy that crosses decompositionboundaries cannot be better than an optimized strategy that respects such boundaries.
?ReferencesAho, Albert V. and Jeffery D. Ullman.
1972.The Theory of Parsing, Translation, andCompiling, volume 1.
Prentice-Hall,Englewood Cliffs, NJ.Arnborg, Stefen, Derek G. Corneil, andAndrzej Proskurowski.
1987.
Complexityof finding embeddings in a k-tree.
SIAMJournal of Algebraic and Discrete Methods,8:277?284.Bodlaender, H. L. and D. M. Thilikos.
1997.Treewidth for graphs with smallchordality.
Discrete Applied Mathematics,79:45?61.Catalan, Euge`ne.
1844.
Note extraite d?unelettre adresse?e.
Journal fu?rdie reine undangewandte Mathematik, 27:192.Chiang, David.
2005.
A hierarchicalphrase-based model for statisticalmachine translation.
In Proceedingsof the 43rd Annual Conference of theAssociation for ComputationalLinguistics (ACL-05), pages 263?270,Ann Arbor, MI.Cormen, Thomas H., Charles E. Leiserson,and Ronald L. Rivest.
1990.
Introductionto Algorithms.
MIT Press, Cambridge, MA.Eisner, Jason.
2003.
Learning non-isomorphictree mappings for machine translation.
InProceedings of the 41st Meeting of theAssociation for Computational Linguistics,pages 205?208, Sapporo, Japan.Galley, Michel, Mark Hopkins, Kevin Knight,and Daniel Marcu.
2004.
What?s in atranslation rule?
In Proceedings of the 2004Meeting of the North American chapter of theAssociation for Computational Linguistics(NAACL-04), pages 273?280, Boston, MA.Gildea, Daniel and Daniel S?tefankovic?.
2007.Worst-case synchronous grammar rules.
InProceedings of the 2007 Meeting of the NorthAmerican Chapter of the Association forComputational Linguistics (NAACL-07),pages 147?154, Rochester, NY.Graham, Ronald.
1972.
An efficientalgorithm for determining the convex hullof a finite planar set.
Information ProcessingLetters, 1:132?133.594Huang et al Binarization of Synchronous Context-Free GrammarsHuang, Liang.
2007.
Binarization,synchronous binarization, and target-sidebinarization.
In Proceedings of theNAACL/AMTA Workshop on Syntax andStructure in Statistical Translation (SSST),pages 33?40, Rochester, NY.Huang, Liang and David Chiang.
2005.Better k-best parsing.
In InternationalWorkshop on Parsing Technologies (IWPT05),pages 53?64, Vancouver.Huang, Liang, Hao Zhang, and DanielGildea.
2005.
Machine translation aslexicalized parsing with hooks.
InInternational Workshop on ParsingTechnologies (IWPT05), pages 65?73,Vancouver.ISI Machine Translation Team.
2006.
ISI atNIST-06.
Working Notes of the NIST MTEvaluation Workshop, September.Washington, D.C.Knight, Kevin and Jonathan Graehl.
2005.
Anoverview of probabilistic tree transducersfor natural language processing.
InConference on Intelligent Text Processing andComputational Linguistics (CICLing),pages 1?24.
Mexico City.Knuth, D. 1977.
A generalization of Dijkstra?salgorithm.
Information Processing Letters,6(1):1?5.Liu, Yang, Qun Liu, and Shouxun Lin.
2005.Log-linear models for word alignment.
InProceedings of the 43rd Annual Conferenceof the Association for ComputationalLinguistics (ACL-05), pages 459?466,Ann Arbor, MI.Melamed, I. Dan.
2003.
Multitext grammarsand synchronous parsers.
In Proceedings ofthe 2003 Meeting of the North AmericanChapter of the Association for ComputationalLinguistics (NAACL-03), pages 158?165,Edmonton.Mihalcea, Rada and Ted Pederson.
2003.
Anevaluation exercise for word alignment.
InHLT-NAACL 2003 Workshop on Building andUsing Parallel Texts: Data Driven MachineTranslation and Beyond, pages 1?10,Edmonton.Nederhof, M.-J.
2003.
Weighted deductiveparsing and knuth?s algorithm.Computational Linguistics, 29(1):135?144.Och, Franz Josef and Hermann Ney.
2004.The alignment template approach tostatistical machine translation.Computational Linguistics, 30(4):417?449.Rambow, Owen and Giorgio Satta.
1999.Independent parallelism in finite copyingparallel rewriting systems.
TheoreticalComputer Science, 223(1-2):87?120.Rounds, William C. 1970.
Mappings andgrammars on trees.
Mathematical SystemsTheory, 4(3):257?287.Satta, Giorgio and Enoch Peserico.
2005.Some computational complexity results forsynchronous context-free grammars.
InProceedings of Human Language TechnologyConference and Conference on EmpiricalMethods in Natural Language Processing(HLT/EMNLP), pages 803?810, Vancouver.Schro?der, E. 1870.
Vier combinatorischeProbleme.
Zeitschrift fu?r Mathematik undPhysik, 15:361?376.Shapiro, L. and A.
B. Stephens.
1991.Bootstrap percolation, the Schro?dernumbers, and the n-kings problem.SIAM Journal on Discrete Mathematics,4(2):275?280.Shieber, Stuart M. 2004.
Synchronousgrammars as tree transducers.
InProceedings of the Seventh InternationalWorkshop on Tree Adjoining Grammar andRelated Formalisms (TAG+ 7), pages 88?95,Vancouver.Shieber, Stuart and Yves Schabes.
1990.Synchronous tree-adjoining grammars.In Proceedings of the 13th InternationalConference on Computational Linguistics(COLING-90), volume III, pages 253?258,Helsinki.Wellington, Benjamin, Sonjia Waxmonsky,and I. Dan Melamed.
2006.
Empiricallower bounds on the complexity oftranslational equivalence.
In Proceedingsof the International Conference onComputational Linguistics/Associationfor Computational Linguistics(COLING/ACL-06), pages 977?984, Sydney.Wu, Dekai.
1996.
A polynomial?timealgorithm for statistical machinetranslation.
In 34th Annual Meeting of theAssociation for Computational Linguistics,pages 152?158, Santa Cruz, CA.Wu, Dekai.
1997.
Stochastic inversiontransduction grammars and bilingualparsing of parallel corpora.
ComputationalLinguistics, 23(3):377?403.Zhang, Hao and Daniel Gildea.
2007.Factorization of synchronous context-freegrammars in linear time.
In NAACLWorkshop on Syntax and Structure inStatistical Translation (SSST), pages 25?32,Rochester, NY.Zhang, Hao, Liang Huang, Daniel Gildea,and Kevin Knight.
2006.
Synchronousbinarization for machine translation.In Proceedings of the NAACL,pages 256?263, New York.595
