Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 862?870,Singapore, 6-7 August 2009. c?2009 ACL and AFNLPBilingual dictionary generation for low-resourced language pairsVarga Istv?nYamagata University,Graduate School of Science and Engineeringdyn36150@dip.yz.yamagata-u.ac.jpYokoyama ShoichiYamagata University,Graduate School of Science and Engineeringyokoyama@yz.yamagata-u.ac.jpAbstractBilingual dictionaries are vital resources inmany areas of natural language processing.Numerous methods of machine translation re-quire bilingual dictionaries with large cover-age, but less-frequent language pairs rarelyhave any digitalized resources.
Since the needfor these resources is increasing, but the hu-man resources are scarce for less representedlanguages, efficient automatized methods areneeded.
This paper introduces a fully auto-mated, robust pivot language based bilingualdictionary generation method that uses theWordNet of the pivot language to build a newbilingual dictionary.
We propose the usage ofWordNet in order to increase accuracy; wealso introduce a bidirectional selection methodwith a flexible threshold to maximize recall.Our evaluations showed 79% accuracy and51% weighted recall, outperforming represen-tative pivot language based methods.
A dic-tionary generated with this method will stillneed manual post-editing, but the improvedrecall and precision decrease the work of hu-man correctors.1 IntroductionIn recent decades automatic and semi-automaticmachine translation systems gradually managedto take over costly human tasks.
This much wel-comed change can be attributed not only to majordevelopments in techniques regarding translationmethods, but also to important translation re-sources, such as monolingual or bilingual dic-tionaries and corpora, thesauri, and so on.
How-ever, while widely used language pairs can fullytake advantage of state-of-the-art developmentsin machine translation, certain low-frequency, orless common language pairs lack some or evenmost of the above mentioned translation re-sources.
In that case, the key to a highly accuratemachine translation system switches from thechoice and adaptation of the translation methodto the problem of available translation resourcesbetween the chosen languages.One possible solution is bilingual corpus ac-quisition for statistical machine translation(SMT).
However, for highly accurate SMT sys-tems large bilingual corpora are required, whichare rarely available for less represented lan-guages.
Rule or sentence pattern based systemsare an attractive alternative, for these systems theneed for a bilingual dictionary is essential.Our paper targets bilingual dictionary genera-tion, a resource which can be used within theframeworks of a rule or pattern based machinetranslation system.
Our goal is to provide a low-cost, robust and accurate dictionary generationmethod.
Low cost and robustness are essential inorder to be re-implementable with any arbitrarylanguage pair.
We also believe that besides highprecision, high recall is also crucial in order tofacilitate post-editing which has to be performedby human correctors.
For improved precision, wepropose the usage of WordNet, while for goodrecall we introduce a bidirectional selectionmethod with local thresholds.Our paper is structured as follows: first weoverview the most significant related works, af-ter which we analyze the problems of currentdictionary generation methods.
We present thedetails of our proposal, exemplified with theJapanese-Hungarian language pair.
We evaluatethe generated dictionary, performing also a com-parative evaluation with two other pivot-language based methods.
Finally we present ourconclusions.2 Related works2.1 Bilingual dictionary generationVarious corpus based, statistical methods withvery good recall and precision were developedstarting from the 1980?s, most notably using the862Dice-coefficient (Kay & R?scheisen, 1993), cor-respondence-tables (Brown, 1997), or mutualinformation (Brown et al, 1998).As an answer to the corpus-based method?sbiggest disadvantage, namely the need for a largebilingual corpus, in the 1990?s Tanaka andUmemura (1994) presented a new approach.
As aresource, they only use dictionaries to and from apivot language to generate a new dictionary.These so-called pivot language based methodsrely on the idea that the lookup of a word in anuncommon language through a third, intermedi-ated language can be automated.
Tanaka andUmemura?s method uses bidirectional source-pivot and pivot-target dictionaries (harmonizeddictionaries).
Correct translation pairs are se-lected by means of inverse consultation, amethod that relies on counting the number ofpivot language definitions of the source word,through which the target language definitions canbe identified (Tanaka and Umemura, 1994).Sj?bergh (2005) also presented an approach topivot language based dictionary generation.When generating his English pivoted Swedish-Japanese dictionary, each Japanese-to-Englishdescription is compared with each Swedish-to-English description.
Scoring is based on wordoverlap, weighted with inverse document fre-quency; the best matches being selected as trans-lation pairs.These two approaches described above are thebest performing ones that are general enough tobe applicable with other language pairs as well.In our research we used these two methods asbaselines for comparative evaluation.There are numerous refinements of the abovemethods, but for various reasons they cannot beimplemented with any arbitrary language pair.Shirai and Yamamoto (2001) used English todesign a Korean-Japanese dictionary, but be-cause the usage of language-specific information,they conclude that their method ?can be consid-ered to be applicable to cases of generatingamong languages similar to Japanese or Koreanthrough English?.
In other cases, only a smallportion of the lexical inventory of the language ischosen to be translated: Paik et al (2001) pro-posed a method with multiple pivots (Englishand Kanji/Hanzi characters) to translate Sino-Korean entries.
Bond and Ogura describe a Japa-nese-Malay dictionary that uses a novel tech-nique in its improved matching through normali-zation of the pivot language, by means of seman-tic classes, but only for nouns (2007).
BesidesEnglish, they also use Chinese as a second pivot.2.2 Lexical database in lexical acquisitionLarge lexical databases are vital for many areasin natural language processing (NLP), wherelarge amount of structured linguistic data isneeded.
The appearance of WordNet (Miller etal., 1990) had a big impact in NLP, since notonly did it provide one of the first wide-rangecollections of linguistic data in electronic format,but it also offered a relatively simple structurethat can be implemented with other languages aswell.
In the last decades since the first, EnglishWordNet, numerous languages adopted theWordNet structure, thus creating a potential largemultilingual network.
The Japanese language isone of the most recent ones added to the Word-Net family (Isahara et al 2008), but the Hungar-ian WordNet is still under development(Pr?sz?ky et al 2001; Mih?ltz and Pr?sz?ky2004).Multilingual projects, such as EuroWordNet(Vossen 1998; Peters et al 1998), Balkanet(Stamou et al 2002) or Multilingual Central Re-pository (Agirre et al 2007) aim to solve numer-ous problems in natural language processing.EuroWordNet was specifically designed forword disambiguation purposes in cross-languageinformation retrieval (Vossen 1998).
The internalstructure of the multilingual WordNets itself canbe a good starting point for bilingual dictionarygeneration.
In case of EuroWordNet, besides theinternal design of the initial WordNet for eachlanguage, an Inter-Lingual-Index interlinks wordmeaning across languages is implemented (Pe-ters et al 1998).
However, there are two limita-tions: first of all, the size of each individual lan-guage database is relatively small (Vossen 1998),covering only the most frequent words in eachlanguage, thus not being sufficient for creating adictionary with a large coverage.
Secondly, thesemultilingual databases cover only a handful oflanguages, with Hungarian or Japanese not beingpart of them.
Adding a new language would re-quire the existence of a WordNet of that lan-guage.3 Problems of current pivot languagebased methods3.1 Selection method shortcomingsPrevious pivot language based methods generateand score a number of translation candidates, andthe candidate?s scores that exceed a certain pre-defined global threshold are selected as viabletranslation pairs.
However, the scores highly de-863pend on the entry itself or the number of transla-tions in the pivot language, therefore there is avariance in what that score represents.
For thisreason, a large number of good entries are en-tirely left out from the dictionary, because all oftheir translation candidates scored low, whilefaulty translation candidates are selected, be-cause they exceed the global threshold.
Due tothis effect the recall value drops significantly.3.2 Dictionaries not enough as resourceRegardless of the language pair, in most casesthe meanings of the corresponding words are notidentical; they only overlap to a certain extent.Therefore, the pivot language based dictionarygeneration problem can be defined as the identi-fication of the common elements or the extent ofthe relevant overlapping in the source-to-pivotand target-to-pivot definitions.Current methods perform a strictly lexicaloverlap of the source-pivot and target-pivot en-tries.
Even if the meanings of the source and tar-get head words are transferred to the pivot lan-guage, this is rarely done with the same set ofwords or definitions.
Thus, due to the differentword-usage or paraphrases, even semanticallyidentical or very similar head words can havedifferent definitions in different dictionaries.
Asa result, performing only lexical overlap, currentmethods cannot identify the differences betweentotally different definitions resulted by unrelatedconcepts, and differences in only nuances re-sulted by lexicographers describing the sameconcept, but with different words.4 Proposed method4.1 Specifics of our proposalFor higher precision, instead of the familiar lexi-cal overlap of the current methods we calculatethe semantically expanded lexical overlap of thesource-to-pivot and target-to-pivot translations.In order to do that, we use semantic informationextracted from the WordNet of the pivot lan-guage.To improve recall, we introduce bidirectionalselection.
As we stated above, the global thresh-old eliminates a large number of good translationpairs, resulting in a low recall.
As a solution, wecan group the translations that share the samesource or target entry, and set local thresholdsfor each head word.
For example, for a sourcelanguage head word entry_source there could bemultiple target language candidates:  en-try_target1, ?
,entry_targetn.
If the top scoringentry_targetk candidates are selected, we ensurethat at least one translation will be available forentry_source, maintaining a high recall.
Since wecan group the entries in the source language andtarget language as well, we perform this selectiontwice, once in each direction.
Local thresholdsdepend on the top scoring entry_target, being setto maxscore?c.
Constant c varies between 0 and 1,allowing a small window not only for the maxi-mum, but high scoring candidates as well.
It islanguage and selection method dependent (see?5.1 for details).4.2 Translation resourcesAs an example of a less-common language pair,we have chosen Japanese and Hungarian.
Fortranslation candidate generation, we have chosentwo freely available dictionaries with English asthe pivot language.
The Japanese-English dic-tionary had 197282, while the Hungarian-Englishcontained 189331 1-to-1 entry pairs.
The Japa-nese-English dictionary had part-of-speech(POS) information as well, but to ensure robust-ness, our method does not use this information.To select from the translation candidates, wemainly use WordNet (Miller et.
al., 1990).
FromWordNet we consider four types of information:sense categorization, synonymy, antonymy andsemantic categories provided by the tree struc-ture of nouns and verbs.4.3 Dictionary generation methodOur proposed method consists of two steps.
Instep 1 we generate a number of translation paircandidates, while in step 2 we score and selectfrom them based on semantic information ex-tracted from WordNet.Step 1: translation candidate generationUsing the source-pivot and pivot-target diction-aries, we connect the source and target entriesthat share at least one common translation in thepivot language.
We consider each source-targetpair a translation candidate.
With our Japanese-English and English-Hungarian dictionaries weaccumulated 436966 Japanese-Hungarian trans-lation candidates.Step 2: translation pair selectionWe examine the translation candidates one byone, looking up the source-pivot and target-pivotdictionaries, comparing the translations in thepivot language.
There are six types of transla-tions that we label A-F and explain below.
First,864we perform a strictly lexical match based only onthe dictionaries.
Next, using information ex-tracted from WordNet we attempt to identify thecorrect translation pairs.
(a) Lexically unambiguous translation pairsSome of the translation candidates have exactlythe same translations into in the pivot language;we consider these pairs as being correct by de-fault.
Also among the translation candidates weidentified a number of source entries that hadonly one target translation; and a number of tar-get entries that had only one source translation.Being the sole candidates for the given entries,we consider these pairs too as being correct.37391 Japanese-Hungarian translation pairs wereretrieved with this method (type A pairs).
(b) Using sense descriptionFor most polysemous words WordNet has de-tailed descriptions with synonyms for each sense.We use these synonyms of WordNet?s sense de-scriptions to disambiguate the meanings of thecommon translations.
For a given source-targettranslation candidate (s,t) we look up the source-pivot and target-pivot translations(s?I={s?i1,?,s?in} andt?I={t?i1,?,t?im}).
We select the elementsthat are common in the two definitions(I?=(s?I)?
(t?I)) and we look up their respec-tive senses from WordNet (sns(I?)).
We identifythe words?
senses comparing each synonym inthe WordNet?s synonym description with eachword from the dictionary definition.
As a result,for each common word we arrive at a certain setof senses from the source-pivot definitions(sns((s?I?))
and a certain set of senses from thetarget-pivot definitions (sns((t?I?)).
We markscoreB(s,t) the maximum ratio of the identicaland total number of identified senses (Jaccardcoefficient).
The higher the scoreB(s,t) is, themore probable is candidate (s,t) a valid transla-tion.
( ) ( ) ( )( ) ( )( ) ( )''''max,' itsnsissnsitsnsissnstsscoreItIsiB??????=????
(1)For example, ??
(seikai: correct, right, cor-rect interpretation) and helyes (correct, proper,right, appropriate) have two common transla-tions (I?={right, correct}), thus scoreB(s,t) can beperformed with these two words.
The adjectiveright has 13 senses according to WordNet,among them 4 were identified from the Japaneseto English definition (sns(right)={#1, #3, #5,#10}, all identified through correct) and 5 fromthe Hungarian to English definition(sns(right)={#1, #3, #5, #6, #10}, through cor-rect or proper).
As a result, 4 senses are com-mon, and 1 is different.
Thus the adjective right?sscore is 0.8 (scoreB(s,t)[right](??,helyes)).
Theadjective correct has 4 senses, all of them arerecognized by both definitions through right,therefore the score through correct is 1(scoreB(s,t)[correct](??
,helyes)).
The maxi-mum of the above scores is the final score:scoreB(s,t)(?
?,helyes)=1.All translation candidates are verified basedon all four POS available from WordNet.
Sincesynonymy information is available for nouns (N),verbs (V), adjectives (A) and adverbs (R), fourseparate scores are calculated for each POS.Scores that pass a global threshold are consid-ered correct.
33971 Japanese-Hungarian candi-dates (type B translations) were selected, withthese two languages the global threshold was setto 0.1.
Even this low value ensures that at leastone of ten meanings is shared by the two entriesof the pair, thus being suitable as translation pair.
(c) Using synonymy, antonymy and semanticcategoriesWe expand the source-to-pivot and target-to-pivot definitions with information from WordNet(synonymy, antonymy and semantic category,respectively).
Thus the similarity of the two ex-panded pivot language descriptions gives a betterindication on the suitability of the translationcandidate.
Using the three relations, the commonversus total number of translations (Jaccard coef-ficient) will define the appropriateness of thetranslation candidate.
( ) ( ) ( )( ) ( )itextisextitextisexttsscore EDC?????
?=,,,(2)Since the same word or concept?s translationsinto the pivot language also share the same se-mantic value, the extension with synonyms(ext(l?i)=(l?i)?syn(l?i), where l={s,t}) theextended translation should share more commonelements.In case of antonymy, we expand the initialdefinitions with the antonyms of the antonyms(ext(l?i)=(l?i)?ant(ant(l?i)), where l={s,t}).This extension is different from the synonymyextension, in most cases the resulting set ofwords being considerably larger.Along with synonymy, antonymy is also avail-able for nouns, verbs, adjectives and adverbs,four separate scores are calculated for each POS.865Semantic categories are provided by the treestructure (hypernymy/hyponymy) of nouns andverbs of WordNet.
We transpose each entry fromthe pivot translations to its semantic categories(ext(l?i)=?semcat(l?i), where l={s,t}).
We as-sume that the correct translation pairs share ahigh percentage of semantic categories.
Accord-ingly, the translations of semantically similar oridentical entries should share a high number ofcommon semantic categories.The scores based on these relations highly de-pend on the number of pivot language transla-tions; therefore we use the bidirectional selectionmethod with local thresholds for each source andtarget head word.
Local thresholds are set basedon the best scoring candidate for a given entry.The thresholds were maxscore?0.9 for synonymyand antonymy; and maxscore?0.8 for the seman-tic categories (see ?5.1 for details).Using synonymy, 196775 candidate pairs(type C), with antonymy 99614 pairs (type D);while with semantic categories 195480 pairs(type E) were selected.
(d) Combined semantic informationThe three separate lists of type C, D and E selec-tion methods resulted in slightly different results,proving that they cannot be used as standaloneselection methods (see ?5.2 for details).Because of the multiple POS labelling of nu-merous words in WordNet, many translationpairs can be selected up to four times based onseparate POS information (noun, verb, adjective,adverb), all within one single semantic informa-tion based methods.
Since we use a bidirectionalselection method, experiments showed that trans-lation pairs that were selected during both direc-tions, in most cases were the correct translations.Similarly, translation pairs selected during onlyone direction were less accurate.
In other words,translation pairs whose target language transla-tion was selected as a good translation for thesource language entry; and whose source lan-guage translation was also selected as a goodtranslation for the target language entry, shouldbe awarded with a higher score.
In the same way,entries selected only during one direction shouldreceive a penalty.
For every translation candidatewe select the maximum score from the severalPOS (noun, verb, adjective and adverb for syn-onymy and antonymy relations; noun and verbfor semantic category) based scores, multipliedby a multiplication factor (mfactor).
The multi-plication factor varies between 0 and 1, awardingthe candidates that were selected both times dur-ing the double directional selection; and punish-ing when selection was made only in a singledirection.
The product gives the combined score(scoreF), c1, c2 and c3 are constants.
In case ofJapanese and Hungarian, these method scoredbest with the constants set to 1, 0.5 and 0.8, re-spectively.
The combined score also highly de-pends on the word entry, therefore local thresh-olds are used in this selection method as well,which were empirically set to maxscore?0.85 (see?5.1 for details).
( ) ( )( )( )( )( )?
????????
?+?+=rel relrelF tsmfactorcctsscorectsscore,,max,321(3)As an example, for the Japanese entry ??(k?ny?
: buy, purchase) there are 10 possibleHungarian translations; using the above methods5 of them (#1, #7, #8, #9, #10) are selected ascorrect ones.
Among these, only 1 of them (#1)is a correct translation, the rest have similar ortotally different meanings.
However, with thecombined scores the faulty translations wereeliminated and a new, correct, but previouslyaverage scoring translation (#2) was selected(Table 1).scoreC scoreD scoreE # translation candidate scoreF N V A R N V A R N V1 v?tel (purchase) 2.012 0.193 0.096 0 0 0 0.500 0 0 0.154 0.5002 ?zlet (business transaction) 1.387 0.026 0.030 0 0 0 0.250 0 0 0.020 0.0773 hozam (output, yield) 1.348 0.095 0.071 0 0 0 0 0 0 0.231 0.0624 emel?r?d (lever, purchase) 1.200 0.052 0.079 0 0 0 0 0 0 0.111 0.0675 el?ny (advantage, virtue) 1.078 0.021 0.020 0 0 0 0 0 0 0.054 0.0566 t?masz (purchase, support) 1.053 0.014 0.015 0 0 0 0 0 0 0.037 0.0317 v?s?rl?s (shopping) 0.818 0.153 0.285 0 0 0 0 0 0 0.273 0.2008 szerzem?ny (attainment) 0.771 0.071 0.285 0 0 0 0 0 0 0.136 0.2009 k?nny?t?s (facilitation) 0.771 0.064 0.285 0 0 0 0 0 0 0.136 0.20010 emel?szerkezet (lever) 0.459 0.285 0.285 0 0 0 0 0 0 0.429 0.200Table 1: Translation candidate scoring for ??
: buy, purchase (above thresholds in bold)866161202 translation pairs were retrieved withthis method (type F).During pre-evaluation type A and type B trans-lations received a score of above 75%, while typeC, type D and type E scored low (see ?5.2 fordetails).
However, type F translations scoredclose to 80%, therefore from the six translationmethods presented above we chose only three(type A, B and F) to construct the dictionary,while the remaining three methods (type C, Dand E) are used only indirectly for type F selec-tion.With the described selection methods 187761translation pairs, with 48973 Japanese and 44664Hungarian unique entries was generated.5 Threshold settings and pre-evaluation5.1 Local threshold settingsAs development set we considered all translationcandidates whose Hungarian entry starts with?zs?
(IPA: ?).
We assume that the behaviour ofthis subset of words reflects the behaviour of theentire vocabulary.
133 unique entries totalling515 translation candidates comprise this devel-opment set.
After this, we manually scored the515 translation candidates as correct (the transla-tion conveys the same meaning, or the meaningsare slightly different, but in a certain context thetranslation is possible) or wrong (the translationpair?s two entries convey a different meaning).The scoring was performed by one of the authorswho is a native Hungarian and fluent in Japanese.273 entries were marked as correct.
Next, weexperimented with a number of thresholds to de-termine which ones provide with the best F-scores (Table 2).
The F-scores were determinedas follows: for example using synonymy infor-mation (type C) in case of threshold=0.85%, 343of the 515 translation pairs were above thethreshold.
Among these, 221 were marked ascorrect by our manual evaluator, thus the preci-sion being 221/343?100=64.43 and the recall be-ing 221/273?100=80.95.
F-score is the harmonicmean of precision and recall (71.75 in this case).threshold value (%) selectiontype 0.75 0.80 0.85 0.90 0.95C 70.27 70.86 71.75 72.81 66.95D 69.92 70.30 70.32 70.69 66.66E 73.71 74.90 72.52 71.62 65.09F 78.78 79.07 79.34 78.50 76.94Table 2: Selection type F-scores with varying thresh-olds (best threshold values in bold)5.2 Selection method evaluationAs a pre-evaluation of the above selection meth-ods, we randomly selected 200 1-to-1 source-target entries resulted by each method.
The sameevaluator scored the translation pairs as correct(the translation conveys the same meaning, or themeanings are slightly different, but in a certaincontext the translation is possible), undecided(the translation pair?s semantic value is similar,but a translation based on them would be faulty)or wrong (the translation pair?s two entries con-vey a different meaning).evaluation score (%) selectiontype correct undecided wrongA 75.5 6.5 18B 83 7 10C 68 5.5 26.5D 60 9 31E 71 5.5 23.5F 79 5 16Table 3: Selection type evaluationThe results showed that type A and type B selec-tions scored higher than all order-based selec-tions, with type C, type D and type E selectionsfailing to deliver the desired accuracy (Table 3).6 EvaluationWe performed three types of evaluation:(1) frequency-weighted recall evaluation(2) 1-to-1 entry precision evaluation(3) 1-to-multiple entry evaluationFor comparative purposes we also performedeach type of evaluation for two other pivot lan-guage based methods whose characteristics per-mit to be implementable with virtually any lan-guage pair.
In order to do so, we constructed twoother Hungarian-Japanese dictionaries using themethods proposed by Tanaka & Umemura andSj?bergh, using the same source dictionaries.6.1 Recall evaluationIt is well known that one of the most challengingaspects of dictionary generation is word ambigu-ity.
It is relatively easy to automatically generatethe translations of low-frequency keywords, be-cause they tend to be less ambiguous.
On thecontrary, the ambiguity of the high frequencywords is much higher than their low-frequencycounterparts, and as a result conventional meth-ods fail to translate a considerable number ofthem.
However, this discrepancy is not reflectedin the traditional recall evaluation, since each867word has an equal weight, regardless of its fre-quency of use.
As a result, we performed a fre-quency weighted recall evaluation.
We used aJapanese frequency dictionary (FD) generatedfrom the Japanese EDR corpus (Isahara, 2007) toweight each Japanese entry.
Setting the standardto the frequency dictionary (its recall value being100), we automatically search for each entry (w)from the frequency dictionary, looking whetheror not it is included in the bilingual dictionary(WD).
If it is recalled, we weight it with its fre-quency from the frequency dictionary.
( )( ) 100?= ???
?DDFwWwwwfrequencywfrequencyrecall  (4)method recallour method 51.68Sj?bergh method 37.03Tanaka method 30.76initial candidates 51.68Japanese-English(*) 73.23Table 4: Recall evaluation results (* marks a manu-ally created dictionary)The frequency weighted recall value resultsshow that our method?s dictionary (51.68) out-scores every other automatically generatedmethod?s dictionary (37.03, 30.76) with a sig-nificant advantage.
Moreover, it maintains thescore of the initial translation candidates, there-fore managing to maximize the recall value, ow-ing to the bidirectional selection method withlocal thresholds.
However, the recall value of amanually created Japanese-English dictionary ishigher than any automatically generated diction-ary?s value (Table 4).6.2 1-to-1 precision evaluationWith 1-to-1 precision evaluation we determinethe translation accuracy of our method, com-pared with the two baseline methods.
200 ran-dom pairs were selected from each of the threeHungarian-Japanese dictionaries, scoring themmanually the same way as with selection typeevaluation (correct, undecided, wrong) (Table 5).The manual scoring was performed by one of theauthors, who is a native Hungarian and fluent inJapanese.
Since no independent evaluator wasavailable for these two languages, after a randomidentification code being assigned to each of the600 selected translation pairs (200 from eachdictionary), they were mixed.
Therefore theevaluator did not know the origin of the transla-tion pairs, only after manual scoring the totalscore for each dictionary was available, after re-grouping based on the initial identification codes.The process was repeated 10 times, 2000 pairswere manually checked from each dictionary.code JapaneseentryHungarianentry classificationk9g6n5d8??
(h?koku:information, re-port)h?r (report, infor-mation, news) correctj8h0k1x5?
(ubu: innocent,naive)z?ld (green, ver-dant) undecideda5b6n8i3????
(entori:entry <a contest>)bej?rat (entry,entrance) wrongTable 5: 1-to-1 precision evaluation examplesevaluation score (%)methodcorrect undecided wrongour method 79.15% 6.15% 14.70%Sj?bergh method 54.05% 9.80% 36.15%Tanaka method 62.50% 7.95% 29.55%Table 6: 1-to-1 precision evaluation resultsTo rank the methods we only consider the cor-rect translations.
Our method performed bestwith an average of 79.15%, outscoring Tanakamethod?s 62.50% and Sj?bergh method?s54.05% (Table 6).
The maximum deviance of thecorrect translations during the 10 repetitions wasless than 3% from the average.6.3 1-to-multiple evaluationWhile with 1-to-1 precision evaluation we esti-mated the accuracy of the translation pairs, with1-to-multiple we calculate the true reliability ofthe dictionary, with the initial translation candi-dates set as recall benchmark.
When looking upthe meanings or translations of a certain headword, the user, whether he?s a human or a ma-chine, expects all translations to be accurate.Therefore we evaluated 200 randomly selectedJapanese entries from the initial translation can-didates, together with all of their Hungariantranslations, scoring them as correct (all transla-tions are correct), acceptable (the good transla-tions are predominant, but there are up to 2 erro-neous translations), wrong (the number or wrongtranslations exceeds 2) or missing (the translationis missing) (Table 7).The same type of mixed, manual evaluationwas performed by the same author on samples of200 entries from each Japanese-Hungarian dic-tionary.
This evaluation was also repeated 10times.To rank the methods, we only consider thecorrect translations.
Our method scored best with86871.45%, outperforming Sj?bergh method?s61.65% and Tanaka method?s 46.95% (Table 8).code JapaneseentryHungariantranslations classificationj4h8m9x5??
(asshuku:compres-sion,squeeze)?sszenyom?s (com-pression, crush,squeeze: correct)?sszeszor?t?s (com-pression, confinement:correct)zsugor?t?s (shrinkage:correct)correcth9j9l3v1??
(teimen:base)alap (base, bottom,foundation: correct)alapzat (base, bed,bottom: correct)l?g (alkali, base: unde-cided)t?mpont (base: correct)acceptablel0k6m3n7???
(narasu: tosound, toring, to beat)beker?t (to encircle, toenclose, to ring:wrong)cseng (to clang, toclank, to ring, to tinkle:correct)hangzik (to ring, tosound: correct)horkan (to snort:wrong)?t (to bang, to knock,to ring: wrong)wrongTable 7: 1-to-multiple entry evaluation examplesevaluation score (%)methodcorrectaccept-able wrong missingour method 71.45 13.85 14.70 0Sj?bergh method 61.65 11.30 15.00 12.05Tanaka method 46.95 3.35 9.10 40.60Table 8: 1-to-many evaluation results7 DiscussionBased on the recall evaluations, the traditionalmethods showed their major weakness by losingsubstantially from the initial recall values, scoredby the initial translation candidates.
Our methodmaintains the same value with the translationcandidates, but we cannot say that the recall isperfect.
When compared with a manually createddictionary, our method also lost significantly.Precision evaluation also showed an im-provement compared with the traditional meth-ods, our method outscoring the other two meth-ods with the 1-to-1 precision evaluation.
1-to-multiple evaluation was also the highest, provingthat WordNet based methods outperform dic-tionary based methods.
Discussing the weak-nesses of our system, we have to divide the prob-lems into two categories: recall problems dealwith the difficulty in connecting the target andsource entries through the pivot language, whileprecision problems discuss the reasons why erro-neous pairs are produced.7.1 Recall problemsWe managed to maximize the recall of our initialtranslation candidates, but in many cases certaintranslation pairs still could not be generated be-cause the link from the source language to thetarget language through the pivot language sim-ply doesn?t exist.
The main reasons are: the entryis missing from at least one of the dictionaries;translations in the pivot language are expressionsor explanations; or there is no direct translationor link between the source and target entries.
Theentries that could not be recalled are mostly ex-pressions, rare entries, words specific to a lan-guage (ex: tatami: floor-mat, or guly?s: goulash).Moreover, a number of head words don?t haveany synonym, antonym and/or hy-pernymy/hyponymy information in WordNet,and as a result these words could not participatein the type B, C, D, E and F scoring.7.2 Precision problemsWe identified two types of precision problems.The most obvious reasons for erroneous transla-tions are the polysemous nature of words and themeaning-range differences across languages.With words whose senses are clear and mostlypreserved even through the pivot language, mostof the correct senses were identified and cor-rectly translated.
Nouns, adjectives and adverbshad a relatively high degree of accuracy.
How-ever, verbs proved to be the most difficult POSto handle.
Because semantically they are moreflexible than other POS categories, and themeaning range is also highly flexible across lan-guages, the identification of the correct transla-tion is increasingly difficult.
For this reason, thenumber of faulty translations and the number ofmeanings that are not translated was relativelyhigh.One other source of erroneous translations isthe quality of the initial dictionaries.
Even theunambiguous type A translations fail to producethe desired accuracy, although they are theunique candidate for a given word entry.
Themain reason for this is the deficiency of the ini-tial dictionaries, which contain a great number ofirrelevant or low usage translations, shadowingthe main, important senses of some words.
Inother cases the resource dictionaries don?t con-tain translations of all meanings; homonyms are869present as pivot entries with different meanings,sometimes creating unique, but faulty links.8 ConclusionsWe proposed a new pivot language basedmethod to create bilingual dictionaries that canbe used as translation resource for machine trans-lation.
In contrast to conventional methods thatuse dictionaries only, our method uses WordNetas a main resource of the pivot language to selectthe suitable translation pairs.
As a result, weeliminate most of the weaknesses caused by thestructural differences of dictionaries, while prof-iting from the semantic relations provided byWordNet.
We believe that because of the natureof our method it can be re-implemented withmost language pairs.In addition, owing to features such as the bidi-rectional selection method with local thresholdswe managed to maximize recall, while maintain-ing a precision which is better than any othercompared method?s score.
During exemplifica-tion, we generated a mid-large sized Japanese-Hungarian dictionary with relatively good recalland promising precision.The dictionary is freely available online(http://mj-nlp.homeip.net/mjszotar), being alsodownloadable at request.ReferencesAgirre, E., Alegria, I., Rigau, G, Vossen, P. 2007.MCR for CLIR, Procesamiento del lenguaje natu-ral 38, pp 3-15.Bond, F., Ogura, K. 2007.
Combining linguistic re-sources to create a machine-tractable Japanese-Malay dictionary, Language Resources andEvaluation, 42(2), pp.
127-136.Breen, J.W.
1995.
Building an Electric Japanese-English Dictionary, Japanese Studies Associationof Australia Conference, Brisbane, Queensland,Australia.Brown, P., Cocke, J., Della Pietra, S., Della Pietra, V.,Jelinek, F., Mercer, R., Roossin, P. 1998.
A Statis-tical Approach to Language Translation, Proceed-ings of COLING-88, pp.
71-76.Brown, R.D.
1997.
Automated Dictionary Extractionfor Knowledge-Free Example-Based Translation,Proceedings of the 7th International Conference onTheoretical and Methodological Issues in MachineTranslation, pp.
111-118.Isahara, H., Bond, F., Uchimoto, K., Uchiyama, M.,Kanzaki, K. 2008.
Development of JapaneseWordNet, Proceedings of LREC-2008.Isahara, H. 2007.
EDR Electronic Dictionary ?
pre-sent status (EDR ????????
), NICT-EDRsymposium, pp.
1-14.
(in Japanese)Kay, M., R?scheisen, M. 1993.
Text-TranslationAlignment, Computational Linguistics, 19(1), pp.121-142.Mih?ltz, M., Pr?sz?ky, G. 2004.
Results and Evalua-tion of Hungarian Nominal WordNet v1.0, Pro-ceedings of the Second Global WordNet Confer-ence, pp.
175-180.Miller G.A., Beckwith R., Fellbaum C., Gross D.,Miller K.J.
(1990).
Introduction to WordNet: AnOnline Lexical Database, Int J Lexicography 3(4),pp.
235-244.Paik, K., Bond, F., Shirai, S. 2001.
Using MultiplePivots to align Korean and Japanese Lexical Re-sources, NLPRS-2001, pp.
63-70, Tokyo, Japan.Peters, W., Vossen, P., D?ez-Orzas, P., Adriaens, G.1998.
Cross-linguistic Alignment of Wordnets withan Inter-Lingual-Index, Computers and the Hu-manities 32, pp.
221?251.Pr?sz?ky, G., Mih?ltz, M., Nagy, D. 2001.
Toward aHungarian WordNet, Proceedings of the NAACL2001 Workshop on WordNet and Other Lexical Re-sources, Pittsburgh, June 2001.Sj?bergh, J.
2005.
Creating a free Japanese-Englishlexicon, Proceedings of PACLING, pp.
296-300.Shirai, S., Yamamoto, K. 2001.
Linking Englishwords in two bilingual dictionaries to generate an-other pair dictionary, ICCPOL-2001, pp.
174-179.Stamou, S., Oflazer, K., Pala, K., Christoudoulakis,D., Cristea, D., Tufi?, D., Koeva, S.,  Totkov, G.,Dutoit, D., Grigoriadou, M. 1997.
BalkaNet: AMultilingual Semantic Network for the BalkanLanguages, In Proceedings of the InternationalWordnet Conference, Mysore, India.Tanaka, K., Umemura, K. 1994.
Construction of abilingual dictionary intermediated by a third lan-guage, Proceedings of COLING-94, pp.
297-303.Vossen, P. 1998.
Introduction to EuroWordNet.
Com-puters and the Humanities 32: 73-89 Special Issueon EuroWordNet.870
