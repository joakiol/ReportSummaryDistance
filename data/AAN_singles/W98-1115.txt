Edge-Based Best-First Chart Parsing *Eugene Charn iak  and Sharon  Go ldwater  and Mark  Johnson(ec/sjg/mj@cs.brown.edu)Departments of Computer  Science/Cognitive and Linguistic SciencesBrown UniversityProvidence RI 02912Abst rac tBest-first probabilistic hart parsing attemptsto parse efficiently by working on edges thatare judged ~'best" by some probabilistic figureof merit (FOM).
Recent work has used proba-bilistic context-free grammars (PCFGs) to as-sign probabilities to constituents, and to usethese probabilities as the starting point for theFOM.
This paper extends this approach to us-ing a probabilistic FOM to judge edges (incom-plete constituents), thereby giving a much finer-grained control over parsing effort.
We showhow this can be accomplished in a particularlysimple way using the common idea of binarizingthe PCFG.
The results obtained are about a fac-tor of twenty improvement over the best priorresults m that is, our parser achieves equivalentresults using one twentieth the number of edges.Furthermore we show that this improvement isobtained with parsing precision and recall levelssuperior to those achieved by exhaustive pars-ing.1 In t roduct ionFinding one (or all) parses for a sentence accord-ing to a context-free grammar equires earch.Fortunately, there are well known O(n 3) algo-rithms for parsing, where n is the length of thesentence.
Unfortunately, for large grammars(such as the PCFG induced from the Penn IIWSJ corpus, which contains around 1.6. i04rules) and Iongish sentences (say, 40 words andpunctuation), even O(n 3) looks pretty bleak.One well-known O(n 3) parsing method (Kay,1980) is chart parsing.
In this approach onemaintains an agenda of items remaining to be" This material is based on work supported in past byNSF grants IRI-9319516 and SBR-9720368.
and by ONRgrant N0014-96.- 1-0549.processed, one of which is processed uring eachiteration.
As each item is pulled off the agenda,it is added to the chart (unless it is alreadythere, in which case it can be discarded) andused to extend and create additional items.
In"exhaustive" chart parsing one removes itemsfrom the agenda in some relatively simple way(last-in, first-out is common), and continues todo so until nothing remains.A commonly discussed alternative is to re-move the constituents from the agenda ccord-ing to a figure of merit (FOM).
The idea is thatthe FOM selects "good" items to be processed,leaving the ~'bad" ones-- the ones that are not,in fact, part of the correct parse---- sitting onthe agenda.
When one has a completed parse,or perhaps everal possible parses, one simplystops parsing, leaving items remaining on theagenda.
The time that would have been spentprocessing these remaining items is time saved,and thus time earned.In our work we have found that exhaustivelyparsing maximum-40-word sentences from thePenn II treebank requires an average of about1.2 million edges per sentence.
Numbers likethis suggest hat any approach that offers thepossibility of reducing the work load is wellworth pursuing, a fact that has been notedby several researchers.
Early on, Kay (1980)suggested the use of the chart agenda for thispurpose.
More recently, the statistical ap-proach to language processing and the use ofprobabilistic ontext-free grammars (PCFGs)has suggested using the PCFG probabilitiesto create a FOM.
Bobrow (1990) and Chi-trao and Grishman (1990) introduced best-firstPCFG parsing, the approach taken here.
Subse-quent work has suggested ifferent FOMs builtfrom PCFG probabilities (Miller and Fox.
1994:Kochman and Kupin.
1991: Magerman and127Marcus, 1991).Probably the most extensive comparison ofpossible metrics for best-first PCFG parsingis that of Caraballo and Charniak (henceforthC&C) (Forthcoming).
They consider a largenumber of FOMs, and view them as approxi-mations of some "ideal" (but only computableafter the fact) FOM.
Of these they recommendone as the best of the lot.
In this paper webasically adopt both their framework and theirrecommended FOM.
The next section describestheir work in more detail,Besides C&C the work that is most directlycomparable to ours is that of Goodman (1997)and Ratnaparki (1997).
Goodman uses an FOMthat is similar to that of C&C but one thatshould, in general, be somewhat more accu-rate.
However, both Goodman's and Ratna-parki's work assumes that one is doing a beamsearch of some sort, rather than a best-firstsearch, and their FOM are unfortunately tiedto their frameworks and thus cannot be adoptedhere.
We briefly compare our results to theirsin Section 5.As noted, our paper takes off from that ofC&C and uses the same FOM.
The major differ-ence is simply that our parser uses the FOM torank edges (including incomplete dges), ratherthan simply completed constituents, as wasdone by C&C.
What is interesting about our ap-proach is that such a seemingly simple changecan produce rather dramatic results.
Ratherthan the thousands of edges required by C&C,the parser presented here requires hundreds, oreven, if one is willing to pay a small price inaccuracy, tens.2 Const i tuent -Based  Best -F i r s tChar t  Pars ingIn the approach taken in C&C, only completededges, i.e., constituents, are entered into theagenda; incomplete dges are always processedas soon as they are constructed.
At each it-eration the constituent with the highest figureof merit is removed from the agenda, added tothe chart, and used to extend current partiallycompleted constituents.
Thus we characterizetheir work as constituent-based best-first chartparsing.C&C take as an "ideal" FOM the quantityp(N~,~ \[ to,n).
Here N~,k is aconstituent of type i128(e.g., NP, VP, etc.)
that spans the constituentsfrom j up to but not including k, and t0,n are then parts-of-speech (tags) of the sentence.
Notethat C&C simplify parsing by assuming that theinput is a sequence of tags, not words.
We makethe same assumption i this paper.
Thus takingp(Nj, k \[ t0,n) as an FOM says that one shouldwork on the constituent that is most likely tobe correct given the tags of the sentence.As p(N~, k \[ to,n) can only be computed pre-cisely after a full parse of the sentence, C&Cderive several approximations, in each casestarting from the well known equation forP(Nj,k \] to,n) in terms of the inside and outsideprobabilities,/3(Nj,k) and a(N~,k).i i ~6(Nj.k)a(Nj.~)P(Nj'k l t?'")
= p(to,,) (1)where /3(Nj,k) and a(N~,k) are defined as fol-lows:B(N~, k) = p(tj,k l gj.k) (2 )~(Nj, k) = p(toj, N;, k,tk,n) (3)C&Cs best approximation is based upon theequation:p(Nj,klto,.)
~ p(Nj,~ I t./-i)~(N~,k)p(ttlP(ti,k l tj-~)P(tk l tk-~)(4)Informally, this can be obtained by approximat-ing the outside probability ot(Nj,k) in Equation1 with a bitag estimate.Of the five terms in Equation 4, two canbe directly estimated from training data: the"boundary statistics" p(Nj, k I tj) (the probabil-ity of a constituent of type Nj,kstarting just af-ter the tag tj) and p(tk I N~, k) (the probabil-ity of tk appearing just after the end of a con-stituent of type Nj k)- The tag sequence proba-bilitiy in the denominator is approximated usinga bi-tag approximation:kp(tj,k) = Hp(ti l (5)i= jThe basic algorithm then is quite simple.
Oneuses the standard chart-parsing algorithm, ex-cept at each iteration one takes from the agendathe constituent that maximizes the FOM de-scribed in Equation 4.There are, however, two minor complexitiesthat need to be noted.
The first relates to theinside probability ~(N;,k).
C&C approximateit with the sum of the probabilities of all theparses for N~, k found at that point in the parse.This in turn requires a somewhat complicatedscheme to avoid repeatedly re-evaluating Equa-tion 4 whenever a new parse is found.
In thispaper we adopt a slightly simpler method.
Weapproximate fl(N~,k) by the most probable parsefor N~ , rather than the sum of all the parses.
j~kWe justify this on the grounds that our parsereventually returns the most probable parse, so itseems reasonable to base our metric on its value.This also simplifies updating i fl(Nj,k) when newparses are found for N~ k- Our algorithm com-pares the probability of the new parse to thebest already found for Nj, k. If the old one ishigher, nothing need be done.
If the new one ishigher, it is simply added to the agenda.The second complexity has to do with thefact that in Equation 4 the probability of thetags tj,k are approximated using two differentdistributions, once in the numerator where weuse the PCFG probabilities, and once in thedenominator, where we use the bi-tag proba-bilities.
One fact noted by C&C, but not dis-cussed in their paper, is that typically the bi-tag model gives higher probabilities for a tagsequence than does the PCFG distribution.
Forany single tag tj, the difference is not much,but as we use Equation 4 to compute our FOMfor larger constituents, the numerator becomessmaller and smaller with respect o the denom-inator, effectively favoring smaller constituents.To avoid this one needs to normalize the twodistributions to produce more similar results.We have empirically measured the normal-ization factor and found that the bi-tag distri-bution produces probabilities that are approxi-mately 1.3 times those produced by the PCFGdistribution, on a per-word basis.
We correctfor this by making the PCFG probability of aknown tag r/ > 1.
This has the effect of mul-tiplying the inside probability ~(Ni,k ) by rl k- j3In Section 4 we show how the behavior of ouralgorithm changes for r/s between 1.0 and 2.4.3 Char t  pars ing  and  b inar i za t ionInformally, our algorithm differs from the onepresented in C&C primarily in that we rankall edges, incomplete as well as complete, withrespect to the FOM.
A straight-forward wayto extend C&C in this fashion is to transformthe grammar so that all productions are eitherunary or binary.
Once this has been done thereis no need for incomplete dges at all in bottom-up parsing, and parsing can be performed usingthe CKY algorithm, suitably extended to han-dle unary productions.One way to convert a PCFG into thisform is left-factoring (Hopcroft and Ullman,1979).
Left-factoring replaces each productionA ~ /3 : p, where p is the production proba-bility and Jill = n > 2, with the following set ofbinary productions:A ~ '~1,n-l 'f ln :P'fll,i' ~ '~l, i - l '  ~i : 1.0'/~1,2' ~ /~1 ~2:1.0for i e \[3, n\]In these productions j3i is the ith element of~3 and '~3i,j' is the subsequence /3i...flj of fl,but treated as a 'new' single non-terminal inthe left-factored grammar (the quote marks in-dicate that this subsequence is to be considereda single symbol).For example, the productionVP-+VNPNPPP  :0.7left-factors to the following productions:VP ~ 'VNPNP '  PP : 0.7'VNPNP '  ~ 'VNP 'PP  :1.0'VNP' --~ VNP :1.0It is not difficult to show that the left-factoredgrammar defines the same probability distribu-tion over strings as the original grammar, andto devise a tree transformation that maps eachparse tree of the original grammar into a uniqueparse tree of the left-factored grammar of thesame probability.In fact, the assumption that all productionsare at most binary is not extraordinary, sincetabular parsers that construct complete parseforests in worst-case O(n 3) time explicitly orimplicitly convert their grammars into binarybranching form (Lang, 1974; Lang, 1991).Sikkel and Nijholt (1997) describe in detailthe close relationship between the CKY algo-rithm, the Earley algorithm and a bottom-up129variant of the Earley algorithm.
The key obser-vation is that the 'new' non-terminals 'fll,i' in aCKY parse using a left-factored grammar corre-spond to the set of non-empty incomplete dgesA ~ fll,i "fli+l,n in the bottom-up variant of timEarley algorithm, where A ~ fll,n is a produc-tion of the original grammar.
Specifically, thefundamental rule of chart parsing (Kay, 1980),which combines an incomplete dge A ~ a .
B f lwith a complete dge B ~ '7- to yield the edgeA ~ aB.
fl, corresponds to the left-factoredproductions ' aB '  ~ a B if fl is non-empty orA ~ 'a 'B  if fl is empty.
Thus in general asingle 'new' non-terminal in a CKY parse us-ing the left-factored grammar abbreviates sev-eral incomplete dges in the Earley algorithm.4 The  Exper imentFor our experiment, we used a tree-bank gram-mar induced from sections 2-21 of the PennWall Street Journal text (Marcus et al, 1993),with section 22 reserved for testing.
All sen-tences of length greater than 40 were ignoredfor testing purposes as done in both C&C andGoodman (1997).
We applied the binarizationtechnique described above to the grammar.We chose to measure the amount of work doneby the parser in terms of the average number ofedges popped off the agenda before finding aparse.
This method has the advantage of be-ing platform independent, as well as providinga measure of "perfection".
Here, perfection isthe minimum number of edges we would needto pop off the agenda in order to create the cor-rect parse.
For the binarized grammar, whereeach popped edge is a completed constituent,this number is simply the number of terminalsplus nonterminals in the sentence--- on average,47.5.Our algorithm includes some measures to re-duce the number of items on the agenda, andthus (presumably) the number of popped edges.Each time we add a constituent to the chart, wecombine it with the constituents on either sideof it, potentially creating several new edges.
Foreach of these new edges, we check to see if amatching constituent (i.e.
a constituent withthe same head, start, and end points) alreadyexists in either the agenda or the chart.
If thereis no match, we simply add the new edge to theagenda.
If there is a match but the old parse4o03oo2oolO0Figure h r/vs.
Popped Edges, !
.
.
.
.
!1.0 1.5 2.0Normal izat ion constantJFigure 2: r\] vs.
Precision and Recall767472"~ ~ 70~ m 6866 i~ precision.
.
.
.
.
?
.. .
.
.
| .
.
.
.
I , , , ,1.0 1.5 2.0Normal izat ion constantof Nj, k is better than the new one, we discardthe new parse.
Finally, if we have found a bet-ter parse of N~,k, we add the new edge to theagenda, removing the old one if it has not al-ready been popped.We tested the parser on section section 22 ofthe WSJ text with various normalization con-stants r/, working on each sentence only untilwe reached the first full parse.
For each sen-tence we recorded the number of popped edgesneeded to reach the first parse, and the precisionand recall of that parse.
The average numberof popped edges to first parse as a function ofis shown in Figure 1, and the average precisionand recall are shown in Figure 2.The number of popped edges decreases as r/increases from 1.0 to 1.7, then begins to increaseagain.
See Section 5 for discussion of these re-sults.
The precision and recall also decrease asr/increases.
Note that, because we used a bina-rized grammer for parsing, the trees producedby the parser contain binarized labels ratherthan the labels in the treebank.
In order tocalculate precision and recall, we "debinarized"1307574.73-Figure 3: Popped Edges vs. Accuracy~n=l .0  .
.
?~o~ ""-=.~..~.~ .
.
.
.
.
.
n= l.l,'.." "~/~"- " - .
.~ .
.
.
.
n=l.2~"~'~ / " .
"-.
~ - - - - - -n=13/ t  : ' ,  z.?~ Y ' .
: ' ' .
"""..~ .
.
.
.
.
n=l .4/ W' '  "%  \  " .  "
' - .
.
.
~ .
.
.
.
.
n= 1.5I "',. "
'~" : - : ?~ " ' -l72 !o 5o0?
?
!10O0Average Number of Popped Edges!
?
!1500 2000the parser's output and then calculated the fig-ures as usual.These results suggest wo further questions:Is the higher accuracy with lower r/due in partto the higher number of edges popped?
If so,can we gain accuracy with higher r /by lettingthe parser continue past the first parse (i.e.
popmore edges)?
To answer these questions, we ranthe parser again, this time allowing it to con-tinue parsing until it had popped 20 times asmany edges as needed to reach the first parse.The results of this experiment are shown inFigure 3, where we plot (precision + recall)/2(henceforth "accuracy") as a function of edges.Note that regardless of r/ the accuracy of theparse increases given extra time, but that all ofthe increase is achieved with only 1.5 to 2 timesas many edges as needed for the first parse.
For77 between 1.0 and 1.2, the highest accuracy isalmost the same, about 75.2, but this value isreached with an average of slightly under 400edges when r/ = 1.2, compared to about 650when r /= 1.0.5 Resu l tsTo better understand the experimental resultsit first behooves us to compare them to thoseachieved previously.
Goodman's results (1997)are hard to compare against ours because hisparser eturns more than a singe best parse andbecause he measures processing time, not edges.However he does give edges/second for one of hist~Figure 4: Edges vs. Sentences Parsed1008070 "6050 ;40 ":.
.
.
.
.
.
.
.
.
.
?
.
.
.
.
.
.
.?
?o????
-???
?
?tt - Our parser.
.
.
.
.
.
C&C's parserI | $ L I2000 4000 6000 8000 1O000Number of Popped Edgesparsers and this plus his parsing times suggeststhat for him edges/sentence will measure in thetens of thousands - -  a far cry from our hun-dreds.
Ratnaparki's (1997) beam search pars-ing procedure produces higher accuracy resultsthan our PCFG model, and achieves this witha beam width of 20.
Unfortunately his paperdoes not give statistics which can be directlycompared with ours.The work by C&C is easier to compare.
InFigure 4 we reproduce C&C's results on thepercentage of sentences (length 18-26) parsedas a function of number of edges used.
We per-formed the same experiment, and our resultsare incliaded there as well.
This figure makesdramatic the order of magnitude improvementprovided by our new scheme, but it is not tooeasy to read numbers off of it.
Such numbersare provided in Table 1.131Table 1: Edges vs. Sentences Parsed% Sents Parsed Our Edges C&C Edges40 90 200071 150 300082 220 400091 320 600095 490 900096 520 10000100 1760Our figures were obtained using rl = 1.2.
Ascan be seen, our parser equires about one twen-tieth the number of edges required by C&C.Indeed, the low average number of edges tofirst parse is probably the most striking thingabout our results.
Even allowing for the factthat considerably more edges must be pushedthan are popped, the total number of edges re-quired to first parse is quite small.
Since theaverage number of edges required to constructjust the (left-factored) test corpus trees is 47.5,our parsing system considers as few as 3 timesas many edges as are required to actually pro-duce the output tree.Almost as interesting, if r I is below 1.4, theprecision and recall scores of the first parseare better than those obtained by running theparser to exhaustion, even though the proba-bility of the first parses our algorithm returnscannot be higher than that found by the ex-haustive version.
Furthermore, as seen in Fig-ure 3, running our parser past the first parseby a small amount (150% of the edges requiredfor the first parse) produces till more accurateparses.
At 150% of the minimum number ofedges and r I = 1.2 the precision/recall figuresare about 2% above those for the maximum like-lihood parse.We have two (possibly related) theories ofthese phenomona.
It may be that the FOMmetric used to select constituents forces ourparser to concentrate on edges which are plausi-ble given their surrounding preterminals; infor-mation which is ignored by the exhaustive maxi-mum likelihood parser.
Alternatively, it may bethat because our FOM causes our parser to pre-fer edges with a high inside times (estimated)outside probability, it is in fact partially mim-132icking Goodman's (Goodman, 1996) 'LabelledRecall' parsing algorithm, which does not re-turn the highest probability parse but attemptsto maximize labelled bracket recall with the testset.Finally, it is interesting to note that the mini-mum number of edges per parse is reached whenr /~ 1.65, which is considerably larger than thetheoretical estimate of 1.3 given earlier.
Noticethat one effect of increasing rl is to raise theFOM for longer constituents.
It may be that onaverage a partial parse is completed fastest iflarger constituents receive more attention sincethey are more likely to lead quickly to a com-plete analysis, which would be one consequenceof the larger than expected r/.This last hypothesis is also consistent withthe observation that average precision and re-call sharply falls off when r/ is increased be-yond its theoretically optimal value, since thenthe parser is presumably focusing on relativelylarger constituents and ignoring other, strictlymore plausible, smaller ones.6 Conc lus ionIt is worth noting that while we have presentedthe use of edge-based best-first chart parsingin the service of a rather pure form of PCFGparsing, there is no particular eason to assumethat the technique is so limited in its domain ofapplicability.
One can imagine the same tech-niques coupled with more informative proba-bility distributions, such as lexicalized PCFGs(Charniak, 1997), or even grammars not basedupon literal rules, but probability distributionsthat describe how rules are built up from smallercomponents (Magerman, 1995; Collins, 1997).Clearly further research is warranted.Be this as it may, the take-home lesson fromthis paper is simple: combining an edge-basedagenda with the figure of merit from C&C?
is easy to do by simply binarizing the gram-mar?
provides a factor of 20 or so reduction inthe number of edges required to find a firstparse, and?
improves parsing precision and recall overexhaustive parsing.To the best of our knowledge this is currentlythe most effecient parsing technique for PCFGgrammars induced from large tree-banks.
Assuch we strongly recommend this technique toothers interested in PCFG parsing.Re ferencesRobert J. Bobrow.
1990.
Statistical agendaparsing.
In DARPA Speech and LanguageWorkshop, pages 222-224.Sharon Caraballo and Eugene Charniak.
Forth-coming.
New figures of merit for best-firstprobabilistic hart parsing.
ComputationalLinguistics.Eugene Charniak.
1997.
Statistical pars-ing with a context-free grammar and wordstatistics.
In Proceedings of the FourteenthNational Conference on Artificial Intelli-gence, pages 598-603, Menlo Park.
AAAIPress/MIT Press.Mahesh V. Chitrao and Ralph Grishman.
1990.Statistical parsing of messages.
In DARPASpeech and Language Workshop, pages 263-266.Michael John Collins.
1997.
Three generativelexicalised models for statistical parsing.
InProceedings of the 35th Annual Meeting ofthe Association for Computational Linguis-tics, pages 16-23.Joshua Goodman.
1996.
Parsing algorithmsand metrics.
In Proceedings of the 34th An-nual Meeting of the Association for Compu-tational Linguistics, pages 177-183.Joshua Goodman.
1997.
Global thresholdingand multiple-pass parsing.
In Proceedings ofthe Second Conference on Empirical Methodsin Natural Language Processing, pages 11-25.John E. Hopcroft and Jeffrey D. Ullman.
1979.Introduction to Automata Theory, Languagesand Computation.
Addison-Wesley.Martin Kay.
1980.
Algorithm schemata nddata structures in syntactic processing.
InBarbara J. Grosz, Karen Sparck Jones, andBonnie Lynn Weber, editors, Readings inNatural Language Processing, pages 35-70.Morgan Kaufmann, Los Altos, California.Fred Kochman and Joseph Kupin.
1991.
Cal-culating the probability of a partial parse ofa sentence.
In DARPA Speech and LanguageWorkshop, pages 273-240.Bernard Lang.
1974.
Deterministic techniquesfor efficient non-deterministic parsers.
In2nd Colloquium on Automata, Languages and133Programming, Lecture Notes in ComputerScience 14, pages 225-269.
Springer Verlag,Berlin.Bernard Lang.
1991.
Towards a uniform formalframework for parsing.
In Masaru Tomita,editor, Current Issues in Parsing Technology,pages 153-172.
Kluwer Academic Publishers,Dordrecht.David M. Magerman and Mitchell P. Mar-cus.
1991.
Parsing the voyager domain us-ing pearl.
In DARPA Speech and LanguageWorkshop, pages 231-236.David Magerman.
1995.
Statistical decision-tree models for parsing.
In Proceedings of the33rd Annual Meeting off the Association forComputational Linguistics, pages 276-283.Mitchell P. Marcus, Beatrice Santorini, andMary Ann Marcinkiewicz.
1993.
Buildinga large annotated corpus of english: Thepenn treebank.
Computational Linguistics,19:313-330.Scott Miller and Heidi Fox.
1994.
Auto-matic grammar acquisition.
In Proceedings ofthe Human Language Technology Workshop,pages 268-271.Adwait Ratnaparki.
1997.
A linear observedtime statistical parser based on maximum en-tropy models.
In Proceedings of the SecondConference on Empirical Methods in Natural?
Language Processing, pages 1-10.Klaas Sikkel and Anton Nijholt.
1997.
Pars-ing of Context-Free languages.
In GrzegorzRozenberg and Arto Salomaa, editors, Hand-book of Formal Languages, volume 2: Lin-ear Modelling: Background and Application,chapter 2, pages 61-100.
Springer, Berlin.
