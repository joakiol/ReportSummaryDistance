Proceedings of the 53rd Annual Meeting of the Association for Computational Linguisticsand the 7th International Joint Conference on Natural Language Processing, pages 836?845,Beijing, China, July 26-31, 2015.c?2015 Association for Computational LinguisticsUnifying Bayesian Inference and Vector Space Models for ImprovedDeciphermentQing Dou?, Ashish Vaswani?, Kevin KnightInformation Sciences InstituteDepartment of Computer ScienceUniversity of Southern California{qdou,avaswani,knight}@isi.eduChris DyerSchool of Computer ScienceCarnegie Mellon Universitycdyer@cs.cmu.eduAbstractWe introduce into Bayesian deciphermenta base distribution derived from similari-ties of word embeddings.
We use Dirich-let multinomial regression (Mimno andMcCallum, 2012) to learn a mapping be-tween ciphertext and plaintext word em-beddings from non-parallel data.
Exper-imental results show that the base dis-tribution is highly beneficial to decipher-ment, improving state-of-the-art decipher-ment accuracy from 45.8% to 67.4% forSpanish/English, and from 5.1% to 11.2%for Malagasy/English.1 IntroductionTremendous advances in Machine Translation(MT) have been made since we began applyingautomatic learning techniques to learn translationrules automatically from parallel data.
However,reliance on parallel data also limits the develop-ment and application of high-quality MT systems,as the amount of parallel data is far from adequatein low-density languages and domains.In general, it is easier to obtain non-parallelmonolingual data.
The ability to learn transla-tions from monolingual data can alleviate obsta-cles caused by insufficient parallel data.
Motivatedby this idea, researchers have proposed differentapproaches to tackle this problem.
They can belargely divided into two groups.The first group is based on the idea proposedby Rapp (1995), in which words are representedas context vectors, and two words are likely tobe translations if their context vectors are simi-lar.
Initially, the vectors contained only context?Equal contributionwords.
Later extensions introduced more fea-tures (Haghighi et al, 2008; Garera et al, 2009;Bergsma and Van Durme, 2011; Daum?e and Jagar-lamudi, 2011; Irvine and Callison-Burch, 2013b;Irvine and Callison-Burch, 2013a), and used moreabstract representation such as word embeddings(Klementiev et al, 2012).Another promising approach to solve this prob-lem is decipherment.
It has drawn significantamounts of interest in the past few years (Ravi andKnight, 2011; Nuhn et al, 2012; Dou and Knight,2013; Ravi, 2013) and has been shown to improveend-to-end translation.
Decipherment views a for-eign language as a cipher for English and findsa translation table that converts foreign texts intosensible English.Both approaches have been shown to improvequality of MT systems for domain adaptation(Daum?e and Jagarlamudi, 2011; Dou and Knight,2012; Irvine et al, 2013) and low density lan-guages (Irvine and Callison-Burch, 2013a; Dou etal., 2014).
Meanwhile, they have their own ad-vantages and disadvantages.
While context vec-tors can take larger context into account, it re-quires high quality seed lexicons to learn a map-ping between two vector spaces.
In contrast, de-cipherment does not depend on any seed lexicon,but only looks at a limited n-gram context.In this work, we take advantage of both ap-proaches and combine them in a joint inferenceprocess.
More specifically, we extend previouswork in large scale Bayesian decipherment by in-troducing a better base distribution derived fromsimilarities of word embedding vectors.
The maincontributions of this work are:?
We propose a new framework that combinesthe two main approaches to finding transla-tions from monolingual data only.836?
We develop a new base-distribution tech-nique that improves state-of-the art decipher-ment accuracy by a factor of two for Span-ish/English and Malagasy/English.?
We make our software available for futureresearch, functioning as a kind of GIZA fornon-parallel data.2 Decipherment ModelIn this section, we describe the previous decipher-ment framework that we build on.
This frameworkfollows Ravi and Knight (2011), who built an MTsystem using only non-parallel data for translat-ing movie subtitles; Dou and Knight (2012) andNuhn et al (2012), who scaled decipherment tolarger vocabularies; and Dou and Knight (2013),who improved decipherment accuracy with depen-dency relations between words.Throughout this paper, we use f to denote tar-get language or ciphertext tokens, and e to denotesource language or plaintext tokens.
Given cipher-text f : f1...fn, the task of decipherment is to finda set of parameters P (fi|ei) that convert f to sen-sible plaintext.
The ciphertext f can either be fullsentences (Ravi and Knight, 2011; Nuhn et al,2012) or simply bigrams (Dou and Knight, 2013).Since using bigrams and their counts speeds up de-cipherment, in this work, we treat f as bigrams,where f = {fn}Nn=1= {fn1, fn2}Nn=1.Motivated by the idea from Weaver (1955), wemodel an observed cipher bigram fnwith the fol-lowing generative story:?
First, a language model P (e) generates a se-quence of two plaintext tokens en1, en2withprobability P (en1, en2).?
Then, substitute en1with fn1and en2with fn2with probability P (fn1| en1) ?
P (fn2| en2).Based on the above generative story, the proba-bility of any cipher bigram fnis:P (fn) =?e1e2P (e1e2)2?i=1P (fni| ei)The probability of the ciphertext corpus,P ({fn}Nn=1) =N?n=1P (fn)There are two sets of parameters in the model:the channel probabilities {P (f | e)} and the bi-gram language model probabilities {P (e?| e)},where f ranges over the ciphertext vocabulary ande, e?range over the plaintext vocabulary.
Givena plaintext bigram language model, the trainingobjective is to learn P (f | e) that maximizeP ({fn}Nn=1).
When formulated like this, one candirectly apply EM to solve the problem (Knightet al, 2006).
However, EM has time complexityO(N ?V2e) and space complexityO(Vf?Ve), whereVf, Veare the sizes of ciphertext and plaintext vo-cabularies respectively, andN is the number of ci-pher bigrams.
This makes the EM approach un-able to handle long ciphertexts with large vocabu-lary size.An alternative approach is Bayesian decipher-ment (Ravi and Knight, 2011).
We assume thatP (f | e) and P (e?| e) are drawn from a Dirichetdistribution with hyper-parameters ?f,eand ?e,e?,that is:P (f | e) ?
Dirichlet(?f,e)P (e | e?)
?
Dirichlet(?e,e?
).The remainder of the generative story is thesame as the noisy channel model for decipher-ment.
In the next section, we describe how welearn the hyper parameters of the Dirichlet prior.Given ?f,eand ?e,e?, The joint likelihood of thecomplete data and the parameters,P ({fn, en}Nn=1, {P (f | e)}, {P (e | e?
)})= P ({fn| en}Nn=1, {P (f | e)})P ({en}Nn=1, P (e | e?))=?e?(?f?f,e)?f?
(?e,f)?fP (f | e)#(e,f)+?e,f?1?e?(?e??e,e?)?e??(?e,e?
)?fP (e | e?)#(e,e?)+?e,e?
?1,(1)where #(e, f) and #(e, e?)
are the counts of thetranslated word pairs and plaintext bigram pairs inthe complete data, and ?
(?)
is the Gamma func-tion.
Unlike EM, in Bayesian decipherment, weno longer search for parameters P (f | e) thatmaximize the likelihood of the observed cipher-text.
Instead, we draw samples from posterior dis-tribution of the plaintext sequences given the ci-phertext.
Under the above Bayesian deciphermentmodel, it turns out that the probability of a par-ticular cipher word fjhaving a value k, given thecurrent plaintext word ej, and the samples for all837the other ciphertext and plaintext words, f?jande?j, is:P (fj= k | ej, f?j, e?j) =#(k, ej)?j+ ?ej,k#(ej)?j+?f?ej,f.Where, #(k, ej)?jand #(ej)?jare the countsof the ciphertext, plaintext word pair and plaintextword in the samples excluding fjand ej.
Simi-larly, the probability of a plaintext word ejtaking avalue l given samples for all other plaintext words,P (ej= l | e?j) =#(l, ej?1)?j+ ?l,ej?1#(ej?1)?j+?e?e,ej?1.
(2)Since we have large amounts of plaintext data,we can train a high-quality dependency-bigramlanguage model, PLM(e | e?)
and use it to guideour samples and learn a better posterior distribu-tion.
For that, we define ?e,e?= ?PLM(e | e?
),and set ?
to be very high.
The probability of aplaintext word (Equation 2) is nowP (ej= l | e?j) ?
PLM(l | ej?1).
(3)To sample from the posterior, we iterate over theobserved ciphertext bigram tokens and use equa-tions 2 and 3 to sample a plaintext token with prob-abilityP (ej| e?j, f) ?
PLM(ej| ej?1)PLM(ej+1| ej)P (fj| ej, f?j, e?j).
(4)In previous work (Dou and Knight, 2012), theauthors use symmetric priors over the channelprobabilities, where ?e,f= ?1Vf, and they set ?
to1.
Symmetric priors over word translation prob-abilities are a poor choice, as one would not a-priori expect plaintext words and ciphertext wordsto cooccur with equal frequency.
Bayesian infer-ence is a powerful framework that allows us toinject useful prior information into the samplingprocess, a feature that we would like to use.
In thenext section, we will describe how we model andlearn better priors using distributional propertiesof words.
In subsequent sections, we show signif-icant improvements over the baseline by learningbetter priors.3 Base Distribution with Cross-LingualWord SimilaritiesAs shown in the previous section, the base dis-tribution in Bayesian decipherment is given inde-pendent of the inference process.
A better basedistribution can improve decipherment accuracy.Ideally, we should assign higher base distributionprobabilities to word pairs that are similar.One straightforward way is to consider ortho-graphic similarities.
This works for closely relatedlanguages, e.g., the English word ?new?
is trans-lated as ?neu?
in German and ?nueva?
in Span-ish.
However, this fails when two languages arenot closely related, e.g., Chinese/English.
Previ-ous work aims to discover translations from com-parable data based on word context similarities.This is based on the assumption that words appear-ing in similar contexts have similar meanings.
Theapproach straightforwardly discovers monolingualsynonyms.
However, when it comes to findingtranslations, one challenge is to draw a mappingbetween the different context spaces of the twolanguages.
In previous work, the mapping is usu-ally learned from a seed lexicon.There has been much recent work in learn-ing distributional vectors (embeddings) for words.The most popular approaches are the skip-gramand continuous-bag-of-words models (Mikolov etal., 2013a).
In Mikolov et al (2013b), the au-thors are able to successfully learn word trans-lations using linear transformations between thesource and target word vector-spaces.
However,unlike our learning setting, their approach re-lied on large amounts of translation pairs learnedfrom parallel data to train their linear transforma-tions.
Inspired by these approaches, we aim to ex-ploit high-quality monolingual word embeddingsto help learn better posterior distributions in unsu-pervised decipherment, without any parallel data.In the previous section, we incorporated ourpre-trained language model in ?e,e?to steer oursampling.
In the same vein, we model ?e,fus-ing pre-trained word embeddings, enabling us toimprove our estimate of the posterior distribution.In Mimno and McCallum (2012), the authors de-velop topic models where the base distributionover topics is a log-linear model of observed docu-ment features, which permits learning better priorsover topic distributions for each document.
Sim-ilarly, we introduce a latent cross-lingual linearmapping M and define:838?f,e= exp{vTeMvf}, (5)where veand vfare the pre-trained plaintextword and ciphertext word embeddings.
M isthe similarity matrix between the two embeddingspaces.
?f,ecan be thought of as the affinity of aplaintext word to be mapped to a ciphertext word.Rewriting the channel part of the joint likelihoodin equation 1,P ({fn| en}Nn=1, {P (f | e)})=?e?(?fexp{vTeMvf})?f?
(exp{vTeMvf})?fP (f | e)#(e,f)+exp{vTeMvf}?1Integrating out the channel probabilities, thecomplete data log-likelihood of the observed ci-phertext bigrams and the sampled plaintext bi-grams,P ({fn| en})=?e?(?fexp{vTeMvf})?f?
(exp{vTeMvf})?e?f?
(exp{vTeMvf}+ #(e, f))?
(?fexp{vTeMvf}+ #(e)).We also add a L2 regularization penalty on theelements of M .
The derivative of logP ({fn|en} ?
?2?i,jM2i,j, where ?
is the regularizationweight, with respect to M ,?
logP ({fn| en} ??2?i,jM2i,j?M=?e?fexp{vTeMvf}vevTf(????f?exp{vTeMvf?}???????f?exp{vTeMvf?
}+ #(e)?
?++ ?
(exp{vTeMvf}+ #(e, f))??(exp{vTeMvf})?
?M,where we use?
exp{vTeMvf}?M= exp{vTeMvf}?vTeMvf?M= exp{vTeMvf}vevTf.?
(?)
is the Digamma function, the derivative oflog ?
(?).
Again, following Mimno and McCal-lum (2012), we train the similarity matrix M withstochastic EM.
In the E-step, we sample plaintextwords for the observed ciphertext using equation 4and in the M-step, we learn M that maximizeslogP ({fn| en}) with stochastic gradient descent.The time complexity of computing the gradientis O(VeVf).
However, significant speedups canbe achieved by precomputing vevTfand exploitingGPUs for Matrix operations.After learning M , we can set?e,f=?f?exp{vTeMvf?}exp{vTeMvf}?f??exp{vTeMvf??
}= ?eme,f, (6)where ?e=?f?exp{vTeMvf?}
is the concentra-tion parameter andme,f=exp{vTeMvf}?f??exp{vTeMvf??
}is anelement of the base measure mefor plaintext worde.
In practice, we find that ?ecan be very large,overwhelming the counts from sampling when weonly have a few ciphertext bigrams.
Therefore, weuse meand set ?eproportional to the data size.4 Deciphering Spanish GigawordIn this section, we describe our data and exper-imental conditions for deciphering Spanish intoEnglish.4.1 DataIn our Spanish/English decipherment experiments,we use half of the Gigaword corpus as monolin-gual data, and a small amount of parallel data fromEuroparl for evaluation.
We keep only the 10kmost frequent word types for both languages andreplace all other word types with ?UNK?.
We alsoexclude sentences longer than 40 tokens, whichsignificantly slow down our parser.
After pre-processing, the size of data for each language isshown in Table 1.
While we use all the mono-lingual data shown in Table 1 to learn word em-beddings, we only parse the AFP (Agence France-Presse) section of the Gigaword corpus to extract839Spanish EnglishTraining992 million 940 million(Gigaword) (Gigaword)Evaluation1.1 million 1.0 million(Europarl) (Europarl)Table 1: Size of data in tokens used in Span-ish/English decipherment experimentcipher dependency bigrams and build a plaintextlanguage model.
We also use GIZA (Och and Ney,2003) to align Europarl parallel data to build a dic-tionary for evaluating our decipherment.4.2 SystemsWe implement a baseline system based on thework described in Dou and Knight (2013).
Thebaseline system carries out decipherment on de-pendency bigrams.
Therefore, we use the Bohnetparser (Bohnet, 2010) to parse the AFP section ofboth Spanish and English versions of the Giga-word corpus.
Since not all dependency relationsare shared across the two languages, we do not ex-tract all dependency bigrams.
Instead, we only usebigrams with dependency relations from the fol-lowing list:?
Verb / Subject?
Verb / Object?
Preposition / Object?
Noun / Noun-ModifierWe denote the system that uses our new methodas DMRE (Dirichlet Multinomial Regression withEmbedings).
The system is the same as the base-line except that it uses a base distribution derivedfrom word embeddings similarities.
Word embed-dings are learned using word2vec (Mikolov et al,2013a).For all the systems, language models are builtusing the SRILM toolkit (Stolcke, 2002).
We usethe modified Kneser-Ney (Kneser and Ney, 1995)algorithm for smoothing.4.3 Sampling ProcedureMotivated by the previous work, we use multiplerandom restarts and an iterative sampling processto improve decipherment (Dou and Knight, 2012).As shown in Figure 1, we start a few sampling pro-cesses each with a different random sample.
Thenresults from different runs are combined to initi-ate the next sampling iteration.
The details of thesampling procedure are listed below:Figure 1: Iterative sampling procedures1.
Extract dependency bigrams from parsingoutputs and collect their counts.2.
Keep bigrams whose counts are greater thana threshold t. Then start N different ran-domly seeded and initialized sampling pro-cesses.
Perform sampling.3.
At the end of sampling, extract word transla-tion pairs (f, e) from the final sample.
Es-timate translation probabilities P (e|f) foreach pair.
Then construct a translation ta-ble by keeping translation pairs (f, e) seenin more than one decipherment and use theaverage P (e|f) as the new translation proba-bility.4.
Start N different sampling processes again.Initialize the first samples with the transla-tion pairs obtained from the previous step(for each dependency bigram f1, f2, find anEnglish sequence e1, e2, whose P (e1|f1) ?P (e2|f2) ?
P (e1, e2)is the highest).
Initial-ize similarity matrix M with one learned byprevious sampling process whose posteriorprobability is highest.
Go to the third step,repeat until it converges.5.
Lower the threshold t to include more bi-grams into the sampling process.
Go to thesecond step, and repeat until t = 1.840The sampling process consists of sampling andlearning of similarity matrix M .
The samplingprocess creates training examples for learning M ,and the new M is used to update the base distri-bution for sampling.
In our Spanish/English de-cipherment experiments, we use 10 different ran-dom starts.
As pointed out in section 3, setting?eto it?s theoretical value (equation 6) gives poorresults as it can be quite large.
In experiments,we set ?eto a small value for the smaller datasets and increase it as more ciphtertext becomesavailable.
We find that using the learned base dis-tribution always improves decipherment accuracy,however, certain ranges are better for a given datasize.
We use ?evalues of 1, 2, and 5 for cipher-texts with 100k, 1 million, and 10 million tokensrespectively.
We leave automatic learning of ?efor future work.5 Deciphering MalagasyDespite spoken in Africa, Malagasy has its rootin Asia, and belongs to the Malayo-Polynesianbranch of the Austronesian language family.Malagasy and English have very different wordorder (VOS versus SVO).
Generally, Malagasy isa typical head-initial language: Determiners pre-cede nouns, while other modifiers and relativeclauses follow nouns (e.g.
ny ?the?
ankizilahy?boy?
kely ?little?).
The significant differences inword order pose great challenges for both parsingand decipherment.5.1 DataTable 2 lists the sizes of monolingual and paralleldata used in this experiment, released by Dou et al(2014).
The monolingual data in Malagasy con-tains news text collected from Madagascar web-sites.
The English monolingual data contains Gi-gaword and an additional 300 million tokens ofAfrican news.
Parallel data (used for evaluation) iscollected from GlobalVoices, a multilingual newswebsite, where volunteers translate news into dif-ferent languages.5.2 SystemsThe baseline system is the same as the base-line used in Spanish/English decipherment exper-iments.
We use data provided in previous work(Dou et al, 2014) to build a Malagasy depen-dency parser.
For English, we use the Turboparser, trained on the Penn Treebank (Martins etMalagasy EnglishTraining16 million 1.2 billion(Web)(Gigawordand Web)Evaluation2.0 million 1.8 million(GlobalVoices) (GlobalVoices)Table 2: Size of data in tokens used in Mala-gasy/English decipherment experiment.
Glob-alVoices is a parallel corpus.al., 2013).Because the Malagasy parser does not predictdependency relation types, we use the followinghead-child part-of-speech (POS) tag patterns to se-lect a subset of dependency bigrams for decipher-ment:?
Verb / Noun?
Verb / Proper Noun?
Verb / Personal Pronoun?
Preposition / Noun?
Preposision / Proper Noun?
Noun / Adjective?
Noun / Determiner?
Noun / Verb Particle?
Noun / Verb Noun?
Noun / Cardinal?
Noun / Noun5.3 Sampling ProcedureWe use the same sampling protocol designed forSpanish/English decipherment.
We double thenumber of random starts to 20.
Further more,compared with Spanish/English decipherment, wefind the base distribution plays a more importantrole in achieving higher decipherment accuracyfor Malagasy/English.
Therefore, we set ?eto 10,50, and 200 when deciphering 100k, 1 million, and20 million token ciphtertexts, respectively.6 ResultsIn this section, we first compare decipherment ac-curacy of the baseline with our new approach.Then, we evaluate the quality of the base distri-bution through visualization.We use top-5 type accuracy as our evaluationmetric for decipherment.
Given a word type fin Spanish, we find top-5 translation pairs (f, e)ranked by P (e|f) from the learned decipherenttranslation table.
If any pair (f, e) can also befound in a gold translation lexicon Tgold, we treat841Spanish/English Malagasy/EnglishTop 5k 10k 5k 10kSystem Baseline DMRE Baseline DMRE Baseline DMRE Baseline DMRE100k 1.9 12.4 1.1 7.1 1.2 2.7 0.6 1.41 million 7.3 37.7 4.2 23.6 2.5 5.8 1.3 3.210 million 29.0 64.7 23.4 43.7 5.4 11.2 3.0 6.9100 million 45.8 67.4 39.4 58.1 N/A N/A N/A N/ATable 3: Spanish/English, Malagasy/English decipherment top-5 accuracy (%) of 5k and 10k most fre-quent word typesthe word type f as correctly deciphered.
Let |C|be the number of word types correctly deciphered,and |V | be the total number of word types evalu-ated.
We define type accuracy as|C||V |.To create Tgold, we use GIZA to align a smallamount of Spanish/English parallel text (1 mil-lion tokens for each language), and use the lexi-con derived from the alignment as our gold trans-lation lexicon.
Tgoldcontains a subset of 4233word types in the 5k most frequent word types,and 7479 word types in the top 10k frequent wordtypes.
We decipher the 10k most frequent Span-ish word types to the 10k most frequent Englishword types, and evaluate decipherment accuracyon both the 5k most frequent word types as wellas the full 10k word types.We evaluate accuracy for the 5k and 10k mostfrequent word types for each language pair, andpresent them in Table 3.Figure 2: Learning curves of top-5 accuracy eval-uated on 5k most frequent word types for Span-ish/English decipherment.We also present the learning curves of de-cipherment accuracy for the 5k most frequentword types.
Figure 2 compares the baseline withDMRE in deciphering Spanish into English.
Per-formance of the baseline is in line with previouswork (Dou and Knight, 2013).
(The accuracy re-ported here is higher as we evaluate top-5 accu-racy for each word type.)
With 100k tokens ofSpanish text, the baseline achieves 1.9% accuracy,while DMRE reaches 12.4% accuracy, improvingthe baseline by over 6 times.
Although the gainsattenuate as we increase the number of ciphertexttokens, they are still large.
With 100 million ci-pher tokens, the baseline achieves 45.8% accuracy,while DMRE reaches 67.4% accuracy.Figure 3: Learning curves of top-5 accuracy eval-uated on 5k most frequent word types for Mala-gasy/English decipherment.Figure 3 compares the baseline with our newapproach in deciphering Malagasy into English.With 100k tokens of data, the baseline achieves1.2% accuracy, and DMRE improves it to 2.4%.We observe consistent improvement throughoutthe experiment.
In the end, the baseline accuracyobtains 5.8% accuracy, and DMRE improves it to11.2%.Low accuracy in Malagasy-English decipher-ment is attributed to the following factors: First,842compared with the Spanish parser, the Mala-gasy parser has lower parsing accuracy.
Second,word alignment between Malagasy and English ismore challenging, producing less correct transla-tion pairs.
Last but not least, the domain of theEnglish language model is much closer to the do-main of the Spanish monolingual text comparedwith that of Malagasy.Overall, we achieve large consistent gainsacross both language pairs.
We hypothesize thegain comes from a better base distribution thatconsiders larger context information.
This helpsprevent the language model driving deicphermentto a wrong direction.Since our learned transformation matrix M sig-nificantly improves decipherment accuracy, it?slikely that it is translation preserving, that is,plaintext words are transformed from their nativevector space to points in the ciphertext such thattranslations are close to each other.
To visualizethis effect, we take the 5k most frequent plaintextwords and transform them into new embeddingsin the ciphertext embedding space ve?= vTeM ,where M is learned from 10 million Spanish bi-gram data.
We then project the 5k most fre-quent ciphertext words and the projected plain-text words from the joint embedding space into a2?dimensional space using t-sne (?
).In Figure 4, we see an instance of a recur-ring phenomenon, where translation pairs are veryclose and sometimes even overlap each other, forexample (judge, jueces), (secret, secretos).
Theword ?magistrado?
does not appear in our evalu-ation set.
However, it is placed close to its possi-ble translations.
Thus, our approach is capable oflearning word translations that cannot be discov-ered from limited parallel data.We often also see translation clusters, wheretranslations of groups of words are close to eachother.
For example, in Figure 5, we can see thattime expressions in Spanish are quite close to theirtranslations in English.
Although better qualitytranslation visualizations (Mikolov et al, 2013b)have been presented in previous work, they exploitlarge amounts of parallel data to learn the mappingbetween source and target words, while our trans-formation is learned on non-parallel data.These results show that our approach canachieve high decipherment accuracy and discovernovel word translations from non-parallel data.Figure 4: Translation pairs are often close andsometimes overlap each other.
Words in spanishhave been appended with spanishFigure 5: Semantic groups of word-translationsappear close to each other.7 Conclusion and Future WorkWe proposed a new framework that simultane-ously performs decipherment and learns a cross-lingual mapping of word embeddings.
Ourmethod is both theoretically appealing and prac-tically powerful.
The mapping is used to give de-cipherment a better base distribution.Experimental results show that our new algo-rithm improved state-of-the-art decipherment ac-curacy significantly: from 45.8% to 67.4% forSpanish/English, and 5.1% to 11.2% for Mala-gasy/English.
This improvement could lead to fur-ther advances in using monolingual data to im-prove end-to-end MT.In the future, we will work on making the ourapproach scale to much larger vocabulary sizes us-ing noise contrastive estimation (?
), and apply it toimprove MT systems.843AcknowledgmentsThis work was supported by ARL/ARO(W911NF-10-1-0533) and DARPA (HR0011-12-C-0014).ReferencesShane Bergsma and Benjamin Van Durme.
2011.Learning bilingual lexicons using the visual similar-ity of labeled web images.
In Proceedings of theTwenty-Second international joint conference on Ar-tificial Intelligence - Volume Volume Three.
AAAIPress.Bernd Bohnet.
2010.
Top accuracy and fast depen-dency parsing is not a contradiction.
In Proceedingsof the 23rd International Conference on Computa-tional Linguistics.
Coling.Hal Daum?e, III and Jagadeesh Jagarlamudi.
2011.
Do-main adaptation for machine translation by miningunseen words.
In Proceedings of the 49th AnnualMeeting of the Association for Computational Lin-guistics: Human Language Technologies.
Associa-tion for Computational Linguistics.Qing Dou and Kevin Knight.
2012.
Large scale deci-pherment for out-of-domain machine translation.
InProceedings of the 2012 Joint Conference on Empir-ical Methods in Natural Language Processing andComputational Natural Language Learning.
Asso-ciation for Computational Linguistics.Qing Dou and Kevin Knight.
2013.
Dependency-based decipherment for resource-limited machinetranslation.
In Proceedings of the 2013 Conferenceon Empirical Methods in Natural Language Pro-cessing.
Association for Computational Linguistics.Qing Dou, Ashish Vaswani, and Kevin Knight.
2014.Beyond parallel data: Joint word alignment and de-cipherment improves machine translation.
In Pro-ceedings of the 2014 Conference on Empirical Meth-ods in Natural Language Processing (EMNLP).
As-sociation for Computational Linguistics.Nikesh Garera, Chris Callison-Burch, and DavidYarowsky.
2009.
Improving translation lexicon in-duction from monolingual corpora via dependencycontexts and part-of-speech equivalences.
In Pro-ceedings of the Thirteenth Conference on Computa-tional Natural Language Learning.
Association forComputational Linguistics.Aria Haghighi, Percy Liang, Taylor Berg-Kirkpatrick,and Dan Klein.
2008.
Learning bilingual lexiconsfrom monolingual corpora.
In Proceedings of ACL-08: HLT.
Association for Computational Linguis-tics.Ann Irvine and Chris Callison-Burch.
2013a.
Com-bining bilingual and comparable corpora for lowresource machine translation.
In Proceedings ofthe Eighth Workshop on Statistical Machine Trans-lation.
Association for Computational Linguistics,August.Ann Irvine and Chris Callison-Burch.
2013b.
Su-pervised bilingual lexicon induction with multiplemonolingual signals.
In Proceedings of the 2013Conference of the North American Chapter of theAssociation for Computational Linguistics: HumanLanguage Technologies.
Association for Computa-tional Linguistics.Ann Irvine, Chris Quirk, and Hal Daume III.
2013.Monolingual marginal matching for translationmodel adaptation.
In Proceedings of the Conferenceon Empirical Methods in Natural Language Pro-cessing (EMNLP).
Association for ComputationalLinguistics.Alexandre Klementiev, Ivan Titov, and Binod Bhat-tarai.
2012.
Inducing crosslingual distributed rep-resentations of words.
In Proceedings of COLING2012.
The COLING 2012 Organizing Committee.Reinhard Kneser and Hermann Ney.
1995.
Improvedbacking-off for m-gram language modeling.
IEEEInternational Conference on Acoustics, Speech, andSignal Processing (ICASSP).Kevin Knight, Anish Nair, Nishit Rathod, and KenjiYamada.
2006.
Unsupervised analysis for deci-pherment problems.
In Proceedings of the COL-ING/ACL 2006 Main Conference Poster Sessions.Association for Computational Linguistics.Andre Martins, Miguel Almeida, and Noah A. Smith.2013.
Turning on the turbo: Fast third-order non-projective turbo parsers.
In Proceedings of the 51stAnnual Meeting of the Association for Computa-tional Linguistics (Volume 2: Short Papers).
Asso-ciation for Computational Linguistics.Tomas Mikolov, Kai Chen, Greg Corrado, and Jef-frey Dean.
2013a.
Efficient estimation of wordrepresentations in vector space.
arXiv preprintarXiv:1301.3781.Tomas Mikolov, Quoc V Le, and Ilya Sutskever.2013b.
Exploiting similarities among lan-guages for machine translation.
arXiv preprintarXiv:1309.4168.David Mimno and Andrew McCallum.
2012.
Topicmodels conditioned on arbitrary features withdirichlet-multinomial regression.
arXiv preprintarXiv:1206.3278.Malte Nuhn, Arne Mauser, and Hermann Ney.
2012.Deciphering foreign language by combining lan-guage models and context vectors.
In Proceedingsof the 50th Annual Meeting of the Association forComputational Linguistics: Long Papers - Volume1.
Association for Computational Linguistics.Franz Josef Och and Hermann Ney.
2003.
A sys-tematic comparison of various statistical alignmentmodels.
Computational Linguistics.844Reinhard Rapp.
1995.
Identifying word translations innon-parallel texts.
In Proceedings of the 33rd an-nual meeting on Association for Computational Lin-guistics.
Association for Computational Linguistics.Sujith Ravi and Kevin Knight.
2011.
Deciphering for-eign language.
In Proceedings of the 49th AnnualMeeting of the Association for Computational Lin-guistics: Human Language Technologies.
Associa-tion for Computational Linguistics.Sujith Ravi.
2013.
Scalable decipherment for machinetranslation via hash sampling.
In Proceedings of the51th Annual Meeting of the Association for Compu-tational Linguistics.
Association for ComputationalLinguistics.Andreas Stolcke.
2002.
SRILM - an extensible lan-guage modeling toolkit.
In Proceedings of the Inter-national Conference on Spoken Language Process-ing.Warren Weaver, 1955.
Translation (1949).
Reproducedin W.N.
Locke, A.D. Booth (eds.).
MIT Press.845
