Automatic Detection Of New Words In A Large Vocabulary ContinuousSpeech Recognition SystemAyman Asad i tR ichard Schwartz$John Makhoul~t Northeastern University,  Boston,  MA 02115BBN Systems and Technolog ies  Corporat ion,  Cambr idge,  MA 02138ABSTRACTIn practical large vocabulary speech recognition systems,it is nearly impossible for a speaker to remember whichwords are in the vocabulary.
The probability of thespeaker using words outside the vocabulary can be quitehigh.
For the case when a speaker uses a new word, cur-rent systems will always" recognize other words withinthe vocabulary in place of the new word, and the speakerwouldn't know what the problem is.In this paper, we describe a preliminary investigationof techniques that automatically detect when the speakerhas used a word that is not in the vocabulary.
We de-veloped a technique that uses a general model for theacoustics of any word to recognize the existence of newwords.
Using this general word model, we measure thecorrect detection of new words versus the false alarmrate.Experiments were run using the DARPA 1000-wordResource Management Database for continuous peechrecognition.
The recognition system used is the BBNBYBLOS continuous peech recognition system (Chowet al.
1987).
The preliminary results indicate a detectionrate of 74% with a false alarm rate of 3.4%.I THE NEW WORD PROBLEMThe current continuous peech recognition systems aredesigned to recognize words within the vocabulary ofthe system.
When a new word is spoken they recognizeother words that are in the vocabulary in place of thenew word.
When this happens, the user does not knowthat the problem is that one of the words spoken is not inthe vocabulary.
He assumes that the system simply mis-recognized the word, and therefore he says the sentenceagain and again.
The current systems do not tell the userwhat the problem is, which could be very frustrating.Adding the ability to detect new words automaticlycan be very efficient and will improve the performanceof the system.
Once a new word is detected it is possibleto add the word to the vocabulary with some extra in-formation from the user such as repeating the word witha carrier phrase and typing in the spellmg of the word.2 APPROACHThe obvious zero-order solution for this problem is toapply some rejection threshold on the word score.
If thescore reaches a level higher than the threshold then anew word is detected.
However, when we examined thescores of words in a sentence, we found that the scoreof correct words varies widely, making it impossible totell whether a word is correct or not.
Therefore, thisapproach for detecting new words did not work well.Our proposed solution is to develop an explicit modelof new words that will be detected whenever a new wordoccurs.
The word model should be general enough torepresent any new word.
It should score better than otherwords in the vocabulary in place of new words only.
Itshould not appear in place of already existing words inthe vocabulary.
We tried two acoustic models of newwords which are described below.The first word model we tried was a new word modelwith a minimum 'of four phonemes long.
It is a linearword model of 5 states and 4 identical phonemes with flatspectral distribution.
The results were not encouragingdue to the high false alarm rate and low detection rate.The second word model that we tried was a wordmodel that allows for any sequence of phonemes of atleast two phonemes long.
The model has 3 states, all263phonemes in parallel from the first state to the secondstate, all phonemes in parallel from the second state tothe third state and all phonemes in parallel ooping onthe second state.
All phonemes are context independentphonemes.
Note that this is in contrast o the normalvocabulary of the system, which uses context dependentphoneme models.We used a statistical class grammar to make the de-tection process more useful, and created a new wordmodel for each open class.
Open classes are the classesthat accept new words (e.g.
ship names, port names) asopposed to closed classes that do not accept new words(e.g.
months, week-days, digits).
By using separate newword models for the open classes we can make the dis-traction whether the new word was a ship name or a portname, etc.
Also, it is easy to add the open class wordsto statistical class grammars and to Natural Languagesyntax and semantics.3 EXPERIMENTS AND RESULTSWe presented new words to the system, simply by re-moving words, that occur m the test sentences, from thevocabulary.
We give results for experiments that usedthe three state acoustic model for new words.
The ex-periments were run on 7 speakers, 25 test sentences perspeaker, from the May 88 test which are BEF, CMR,DMS, DTB, DTD, JWS and PGH.
We varied the per-plexity of the statistical class grammars imply by chang-ing the number of training sentences.
A bias against newwords was implemented to reduce the false alarm rate.Our first experiment was detecting new ship namesor new ship names possessive.
The perplexity of thegrammar was 100.
We had a detection rate of 83% anda false alarm rate of 1.7%.
In the second experiment wechanged the perplexity of rite grammar to 60 to measurethe effect of the perplexity on the detection rate and thefalse alarm rate.
There was no significant difference inthe detection rate (84%) but the false alarm rate droppedto 1.1%.Our third experiment was detecting new port nameswith grammar of perplexity 100.
We had a detectionrate of 64% and a false alarm rate of 0.6%.In the fourth experiment we tried to detect new wordsfrom 7 different classes which are ship name, ship namepossessive, port name, water name, land name, capabil-ity and track name.
The grammar had a perplexity of100.
The detection rate was 74% and the false alarmrate was 3.4%.We measured the detection rate versus the false alarmrate for each experiment.
The results are tabulated intable I.
The columns in the table are described thenfollowed by an example as an illustration:?
new words: the new words were allowed in thefollowing classes.?
perp: the peqalexity of the grammar.?
cr: exact detect ionram as a pementa~ o fnumbero fnew wo~s.SENTENCE (1237)REF: how many LAMPSHYP: how many NEW-CAPABIL ITYREF: c ru i sers  are inHYP: c ru i sers  are inREF: MOZAMBIQUE channe lHYP: NEW-WATER-NAME channe l?
cc: close call or close detection rate.
That is, thenew word was detected but there was an insertionor deletion in its vicinity.SENTENCE (0464)REF: when+s HAWKBILL  DUE inHYP: when+s NEW-SHIP -NAME *** inREF: portHYP: port?
sw: switch between classes, i.e.
the new word wasdetected, but was assigned to the wrong class.SENTENCE (I006)REF: when was PEORIA  lastHYP: when was NEW-SHIP -NAME+S lastREF: in  the  a t lant i c  oceanHYP: in  the  a t lant i c  ocean?
dr: total detection rate.
Sum of cr, cc and sw.?
far: false alarm rate, percentage of number of  falsealarms to the total number of test sentences.
A falsealarm is a new word detected where there was nonew word in that part of the test sentence.264SENTENCE (1025)REF: WHEN DID sherman lastHYP: WHEN+S THE sherman lastREF: downgrade  for  asuwI-IYP: downgrade  for  asuwREF: miss ion  AREAHYP: miss ion  NEW-TRACK-NAMEReferencesY.L.
Chow, M.O.
Dunham, O.A.
Kimball, M.A.
Kras-ner, G.F. Kubala, J. Makhoul, P.J.
Price, S. Roucos,and R.M.
Schwartz.
"BYBLOS: The BBN Continu-ous Speech Recognition System".
In IEEE Int.
Conf.Acoust..
Speech, Signal Processing, pages 89-92.
Dal-las, TX, April 1987.
Paper No.
3.7.new words l perpl cr cc I sw I d r l  far lshipnarne(+s) 100 42 36 5 83 1.7shipname(+s') 60 49 30 5 84 1.1pormame 100 27 37 64 0.6\[ 7 classes 100 44 6 24 74 3.4Table 1: Detection of new words resultsWhile we would like the system to detect he exactlocation and the class of the new words, it is also usefulto simply detect hat a new word occured.
Thus we saythat 74% of the time the system was able to inform theuser that a new word had been used.4 CONCLUSIONFrom the above results we conclude that the problemof detecting new words can be solved by selecting anappropriate word model for new words.
In these exper-iments we have proved that this approach is viable andresults in a detection rate of 74% and a false alarm rateof 3.4%.
These results also suggest hat a better wordmodel can be used to enhance the detection rate.
Theuse of a bias helps reduce the false alarm rate withoutaftecting the detection rate significantly.
Changing theperplexity of the class grammar did not affect the de-tection rate significantly but it reduced 35% of the falsealarm rate.AcknowledgementsThe work reported here was supported by the AdvancedResearch Projects Agency and was monitored by the Of-fice of Naval Research under Contract No.
00014.--89-C-0008.265
