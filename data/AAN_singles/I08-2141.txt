A Mechanism to Provide Language-Encoding Support and an NLP FriendlyEditorAnil Kumar SinghLanguage Technologies Research CentreIIIT, Hyderabad, Indiaanil@research.iiit.ac.inAbstractMany languages of the world (some withvery large numbers of native speakers) arenot yet supported on computers.
In this pa-per we first present a simple method to pro-vide an extra layer of easily customizablelanguage-encoding support for less comput-erized languages.
We then describe an ed-itor called Sanchay Editor, which uses thismethod and also has many other facilitiesuseful for those using less computerized lan-guages for simple text editing or for Nat-ural Language Processing purposes, espe-cially for annotation.1 IntroductionA large number of languages of the world are stillnot supported on computers.
Some of them are spo-ken by a tens or hundreds of millions of people, sothey will probably be supported in the near future.However, many languages may not be, simply be-cause the number of people using them on comput-ers, for whatever reason, is not large.
Those whowant to use these languages on computers, includ-ing the researchers working on those languages, willneed support for these languages.
A related problemis that of support for encodings, as many of theseless computerized languages do not have one stan-dard encoding that is used by all.
Therefore, there isa need of a simple and easily customizable methodof adding support for a new language or encoding.Such a method should require minimum technicalknowledge from the user.
In this paper, we willpresent a method of providing language and encod-ing support for less computerized languages.Another need which we address in this paper is ofan editor that not only makes use of the above men-tioned method of language and encoding support,but also has many facilities useful for Natural Lan-guage Processing (NLP) researchers and linguists.2 Language-Encoding SupportThere is no exhaustive, commonly agreed upon listof encodings for many languages.
Even the list oflanguages is not without dispute (e.g., whether Bho-jpuri is a language or not).
This implies that theconventional deterministic approach to language-encoding support based on the assumption that thepossible languages and encodings are known in ad-vance is not enough if we do not want to prevent thepossibility of using any language or encoding usedby a significant number of people, or even a rarelyused endangered language.Even though with the increasing use of Unicodebased encodings, the problem has reduced for manylanguages, we still require a facility that can allowconvenient use of new languages which are not cov-ered in Unicode.Therefore, what we need is a more customizablelanguage-encoding support where it is very easy forthe user or the developer to add support for somelanguage-encoding.
For this purpose, as many ofthe tasks should be automated as possible.
This canbe done by using NLP techniques.
Even thoughmany of the encodings used for less computerizedlanguages are based on just font mappings, i.e., sup-porting them basically means providing an appropri-957Figure 1: A customizable design for language-encoding supportate font.
This seems to be very simple, but the prob-lem is that the user may not know which font to use.Moreover, providing basic support so that you cantype once you have selected the font is not enough.The user might not even know what encoding someexisting text is in.
Then, the user might want to savethe text in some other encoding.
To provide userfriendly support for language-encodings in a situa-tion like this requires a more intelligent design.Figure-1 shows a design for language-encodingsupport which addresses these problems.
The mainelements of this design are:?
Properties files listing languages, encodings,fonts, and their connections?
Language-encoding identification for text?
Language-encoding identification for fonts?
A language-encoding API?
Encoding convertersCurrently, 15 languages and 10 encoding are sup-ported.
These are mostly all South Asian languages,apart from English, since the users till now weremostly from South Asia.
A large number of freelyavailable fonts have also been included in the distri-bution, but the user would probably like to add morefonts, which can be done easily just by adding thepaths of the new fonts in a properties file.
There isno need to install these fonts, irrespective of the op-erating systems.
Also, more languages and encod-ings can be added quite easily.
In most cases, to adda new language-encoding, the user just has to followthese steps:1.
Make a new entry in the properties files foreach of these three: languages, encodings andlanguage-encodings.2.
Specify the paths of all the fonts for thatlanguage-encoding in the properties file forfonts.
These fonts just have to be on the systemand their paths have to specified in the prop-erties file.
However, it may be preferable (forconvenience) that they be stored in fonts direc-tory of Sanchay.3.
Specify the default font in the properties file fordefault fonts.4.
If the new language uses a Unicode encoding,make an entry for the Unicode block corre-958Figure 2: A font chooser listing fonts relevant to a specific language encoding pairsponding to that language.
This is not compul-sory, but it will allow language specific listingof fonts for language-encoding pairs involvingUnicode encodings.In future, we will make this even more easy byproviding a graphic user interface based wizard togo through these steps.The editor can also use any input methods avail-able from the operating system.
New input meth-ods can also be added as Java libraries.
Such exter-nal Java libraries have just to be copied to the ext-lib directory of Sanchay.
It is also possible to eas-ily switch among input methods (Figure-4), whetherprovided by the operating system or included into(or add to) Sanchay.
So, it is possible to enter text inmultiple languages.Note that, right now, this support for language-encodings is in the form of an extra platform inde-pendent layer on top of the support provided by op-erating systems.
Such a layer could possibly be inte-grated into operating systems in future.
This might,of course, require porting of the code for differentoperating systems and can be in-built into the oper-ating system.2.1 A More Intelligent Listing of FontsIn the design used on all operating systems so far,when you want to view the list of fonts, what youget is a list of all the fonts installed on the sys-tem or at least all the fonts found by the operatingsystem or the user program.
This is not very userfriendly for less computerized languages, becausemost of the fonts listed may not be meant for thelanguage-encoding the user is interested in.
Whatthe user needs is the list of fonts relevant to thespecific language-encoding she is interested in.
Inour design, this is what the user will see (Figure-2),when the user views the list of fonts.
Of course, wecan also give the user the option to see all the fontsinstalled on the system.2.2 Language-Encoding IdentificationAnother important element of the design is alanguage-encoding identification tool that is inte-grated into the language-encoding support moduleso that if the user opens a file and does not know thelanguage or encoding of the text, the tool can auto-matically identify the language-encoding of the text.The language-encoding identification tool is basedon byte based n-gram models using a distributionalsimilarity measures (Singh, 2006a).
This tools iscomputationally quite a light one as the amount of959data required for training is very small and it hasbeen found to be one of the most accurate language-encoding systems currently available.
The user canmake it even faster by removing those language-encodings which she may not be interested in.
Thiswill require only a change in the relevant propertiesfile.2.3 Encoding ConversionThere is also a wrapper module for calling any in-stalled or built in encoding converter for languageswhich use more than one encodings.
The user caneasily convert the encoding of the text depending onher needs and the availability of a relevant encod-ing converter.
It is also possible to easily add newencoding converters.3 Sanchay EditorAlthough numerous text editors, even free and opensource ones, are available, the simple open sourceeditor that we are going to describe in this section(Figure-3) is based on the language-encoding sup-port mentioned earlier and is also closely integratedwith Sanchay1, a collection of tools and APIs forNLP.
The editor is implemented as a customizableGUI component that can be easily included in anyJava application.
The notable features of this editorare:- Uses customizable language-encoding support asdescribed earlier.- Can automatically identify language-encoding ofthe text using a byte based n-grammodeling (Singh, 2006a).- The font chooser (Figure-2) shows only the fontsapplicable for the language-encoding.- Text can be preprocessed for NLP or annotationpurposes from this editor.- The formats used for annotation can be detectedand validated from the editor.- Specialized annotation interfaces can be launchedto edit the annotated files (in text format) opened inthis editor.- Since the editor is implemented in Java, it can beused on any platform on which Java (JRE or JDKversion 1.5 or higher) is installed.1http://ltrc.iiit.ac.in/anil/Sanchay-EILMT and http://sourceforge.net/projects/nlp-sanchaySome of the facilities are described in the follow-ing sub-sections.3.1 Validation of Annotation FormatsIf the user is directly editing a document which isannotated with POS tags, chunks or is syntacticallyannotated, it is possible to automatically validate theannotation format of the document.
A text box be-low the main editing panel shows the errors in for-mat, if any.
Usually, annotation is performed by us-ing some annotation interface, but since the anno-tated data is stored as simple text, the document canbe edited or annotated directly from a text editor.The format validation facility has been included toensure that after any such editing or annotation, thedocument is still in the correct format, as it is easyfor users to make format related mistakes.3.2 Format ConversionSanchay annotation interfaces allow annotation atvarious levels like POS tagging, chunking, syntac-tic (treebank) annotation etc.
Currently four dif-ferent formats are recognized by the system: rawtext without annotation, POS tagged format whereeach sentence is simply a sequence of word and POStag pairs separated by some symbol like underscore,?bracket form?
which allows POS tagged and chun-ked data to be represented (including recursion), andShakti Standard Format (SSF)2.
The editor allowsthe user to convert the data from one format to an-other.3.3 Document StatisticsThe user can also get a statistics about the document,such as the number of words, the number of sen-tences, the number of characters, and their respec-tive frequencies etc.
These statistics are according tothe format of the document, i.e., if the document isin SSF format, then the document will be parsed andthe statistics will be about the annotated documentand the elements of the format, e.g.
<Sentence>tag will not be counted: only actual words (or POStags etc.)
in the annotated document will be counted.Such statistics can also be obtained for a number ofdocuments, i.e., a corpus, not just the current docu-ment.
This can be a very useful facility for workingon annotated corpus.2www.elda.org/en/proj/scalla/SCALLA2004/sangalsharma.pdf960Figure 3: A multipurpose editor for NLP for South Asian languagesFigure 4: Input methods currently supported9613.4 Integration with Annotations InterfacesThe editor is built into Sanchay in such a way thatit is possible to open different views of a document,depending on the annotation format.
For example,if the currently opened document is in SSF format,then the same document can be opened in the San-chay Syntactic Annotation Interface just by clickingon a button or a context menu item.
The opposite isalso possible, i.e., if a document is open in the Syn-tactic Annotation Interface, then it can be directlyopened into the Sanchay Editor as a simple text file.3.5 Some Other FacilitiesApart from the above mentioned facilities, SanchayEditor also has the usual facilities available in texteditors such as find and replace (with regular ex-pressions and also in the batch mode), reverting tothe saved version, automatic periodic backup etc.4 Facilities Being IntegratedSome other facilities that have already been imple-mented and are going to be integrated into the San-chay Editor include a better spell checker for SouthAsian languages based on a Computational PhoneticModel of Scripts or CPMS (Singh, 2006b).
Thismodel provides a method to calculate the phoneticand orthographic similarity (surface similarity) ofwords or strings.
Another facility is the identifi-cation of languages and encoding in a multilingualdocument (Singh and Gorla, 2007a).
This is an ex-tension of the language-encoding identification toolsdescribed earlier and is the first systematic workon the problem of identification of languages andencoding in a multilingual document.
When thistool is integrated into the editor, the user will beable to open a multilingual document and the sys-tem will automatically identify the sections in dif-ferent languages and display them accordingly, evenif the document has not been encoded using Uni-code.
Of course, identification is not 100% accu-rate at present, but we are working on improvingit.
Another already implemented facility that is go-ing to be added is fuzzy text search (Singh et al,2007c).
It is also mainly based on the idea of cal-culating surface similarity using the CPMS.
Fuzzytext search based on this method performs betterthan the traditional methods.
Yet another facilityto be added is a more discerning mechanism fortransliteration (Surana and Singh, 2008).
The firstimportant idea in this mechanism is to use differentmethods for transliteration based on the word origin(identified using a modified version of the language-encoding tool).
The second major idea is to usefuzzy text matching for selecting the best match.This method also has outperformed other methods.There is a plan to extend the editor to allow directannotation.
We will begin by providing support fordiscourse annotation and other similar annotations.5 ConclusionsIn this paper we presented a simple but effectivemethod of providing an easily customizable extralayer of language-encoding support for less comput-erized languages.
We also described Sanchay Ed-itor, which uses this method of language-encodingsupport and has many other facilities that may beuseful for NLP researchers as well as those who justneed a simple text editor for language-encodings notusually supported on computers.
Sanchay Editor isclosely integrated with a collection of NLP tools andAPIs called Sanchay.ReferencesAnil Kumar Singh and Jagadeesh Gorla.
2007a.
Identi-fication of languages and encodings in a multilingualdocument.
In Proceedings of the 3rd ACL SIGWACWorkshop on Web As Corpus, Louvain-la-Neuve, Bel-gium.Anil Kumar Singh, Harshit Surana, and Karthik Gali.2007c.
More accurate fuzzy text search for languagesusing abugida scripts.
In Proceedings of ACM SI-GIR Workshop on Improving Web Retrieval for Non-English Queries, Amsterdam, Netherlands.Anil Kumar Singh.
2006a.
Study of some distance mea-sures for language and encoding identification.
In Pro-ceedings of ACL 2006 Workshop on Linguistic Dis-tance, Sydney, Australia.Anil Kumar Singh.
2006b.
A computational phoneticmodel for indian language scripts.
In Constraints onSpelling Changes: Fifth International Workshop onWriting Systems, Nijmegen, The Netherlands.Harshit Surana and Anil Kumar Singh.
2008.
A morediscerning and adaptable multilingual transliterationmechanism for indian languages.
In Proceedings ofthe Third International Joint Conference on NaturalLanguage Processing (To appear), Hyderabad, India.962
