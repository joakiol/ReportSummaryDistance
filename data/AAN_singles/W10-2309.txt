Proceedings of the 2010 Workshop on Graph-based Methods for Natural Language Processing, ACL 2010, pages 55?59,Uppsala, Sweden, 16 July 2010. c?2010 Association for Computational LinguisticsCo-occurrence Cluster Features for Lexical Substitutions in ContextChris BiemannPowerset (a Microsoft company)475 Brannan St Ste.
330San Francisco, CA 94107, USAcbiemann@microsoft.comAbstractThis paper examines the influence of fea-tures based on clusters of co-occurrencesfor supervised Word Sense Disambigua-tion and Lexical Substitution.
Co-occurrence cluster features are derivedfrom clustering the local neighborhood ofa target word in a co-occurrence graphbased on a corpus in a completely un-supervised fashion.
Clusters can be as-signed in context and are used as featuresin a supervised WSD system.
Experi-ments fitting a strong baseline system withthese additional features are conducted ontwo datasets, showing improvements.
Co-occurrence features are a simple way tomimic Topic Signatures (Mart?
?nez et al,2008) without needing to construct re-sources manually.
Further, a system is de-scribed that produces lexical substitutionsin context with very high precision.1 IntroductionWord Sense Disambiguation (WSD, see (Agirreand Edmonds, 2006) for an extensive overview)is commonly seen as an enabling technology forapplications like semantic parsing, semantic rolelabeling and semantic retrieval.
Throughout re-cent years, the Senseval and Semeval competitionshave shown that a) WordNet as-is is not an ade-quate semantic resource for reaching high preci-sion and b) supervised WSD approaches outper-form unsupervised (i.e.
not using sense-annotatedexamples) approaches.
Due to the manual effortinvolved in creating more adequate word sense in-ventories and sense-annotated training data, WSDhas yet to see its prime-time in real world applica-tions.Since WordNet?s sense distinctions are often toofine-grained for allowing reliable distinctions bymachines and humans, the OntoNotes project(Hovy et al, 2006) conflated similar WordNetsenses until 90% inter-annotator agreement onsense-labelling was reached.
The SemEval 2007lexical sample task employs this ?coarse-grained?inventory, which allows for higher system perfor-mance.To alleviate the bottleneck of sense-labelled sen-tences, (Biemann and Nygaard, 2010) presentan approach for acquiring a sense inventoryalong with sense-annotated example usages usingcrowdsourcing, which makes the acquisition pro-cess cheaper and potentially quicker.Trying to do away with manual resources entirely,the field of Word Sense Induction aims at induc-ing the inventory from text corpora by clusteringoccurrences or senses according to distributionalsimilarity, e.g.
(Veronis, 2004).
While such unsu-pervised and knowledge-free systems are capableof discriminating well between different usages, itis not trivial to link their distinctions to existing se-mantic resources, which is often necessary in ap-plications.Topic Signatures (Mart?
?nez et al, 2008) is an at-tempt to account for differences in relevant topicsper target word.
Here, a large number of contextsfor a given sense inventory are collected automat-ically using relations from a semantic resource,sense by sense.
The most discriminating contentwords per sense are used to identify a sense inan unseen context.
This approach is amongst themost successful methods in the field.
It requires,however, a semantic resource of sufficient detailand size and a sense-labeled corpus to estimatepriors from the sense distribution.
Here, a sim-ilar approach is described that uses an unlabeled55corpus alone for unsupervised topic signature ac-quisition using graph clustering, not relying on theexistence of a WordNet.
Unlike in previous eval-uations like (Agirre et al, 2006), parameters forword sense induction are not optimized globally,but instead several parameter settings are offeredas features to a Machine Learning setup.Experimental results are provided for two datasets:the Semeval-2007 lexical sample task (Pradhan etal., 2007) and the Turk bootstrap Word Sense In-ventory (TWSI1, (Biemann and Nygaard, 2010) ).2 Cluster Co-occurrence Features2.1 Graph Preperation and ParameterizationSimilar to the approach in (Widdows and Dorow,2002), a word graph around each target wordis constructed.
In this work, sentence-basedco-occurrence statistics from a large corpus areused as a basis to to construct several wordgraphs for different parameterizations.
Significantco-occurrences between all content words (nouns,verbs, adjectives as identified by POS tagging) arecomputed from a large corpus using the tinyCC2tool.
The full word graph for a target word is de-fined as all words significantly co-occurring withthe target as nodes, with edge weights set to thelog-likelihood significance of the co-occurrencebetween the words corresponding to nodes.
Edgesbetween words that co-occur only once or withsignificance smaller than 6.63 (1% confidencelevel) are omitted.Aiming at different granularities of usage clusters,the graph is parameterized by a size parametert and a density parameter n: Only the mostsignificant t co-occurrences of the target enter thegraph as nodes, and an edge between nodes isdrawn only if one of the corresponding words iscontained in the most significant n co-occurrencesof the other.2.2 Graph Clustering ParameterizationAs described in (Biemann, 2006), the neighbor-hood graph is clustered with Chinese Whispers.This efficient graph clustering algorithm finds thenumbers of clusters automatically and returns apartition of the nodes.
It is initialized by assigningdifferent classes to all nodes in the graph.
Then,1full dataset available for download athttp://aclweb.org/aclwiki/index.php?title=Image:TWSI397.zip2http://beam.to/biem/software/TinyCC2.htmla number of local update steps are performed, inwhich a node inherits the predominant class in itsneighborhood.
At this, classes of adjacent nodesare weighted by edge weight and downweightedby the degree (number of adjacent nodes) of theneighboring node.
This results in hard clusters ofwords per target, which represent different targetusages.Downweighting nodes by degree is done accord-ing to the following intuition: nodes with highdegrees are probably very universally used wordsand should be less influential for clustering.
Threeways of node weighting are used: (a) dividing theinfluence of a node in the update step by the degreeof the node, (b) dividing by the natural logarithmof the degree + 1 and (c) not doing node weight-ing.
The more aggressive the downweighting, thehigher granularity is expected for the clustering.It is emphasized that no tuning techniques are ap-plied to arrive at the ?best?
clustering.
Rather, sev-eral clusterings of different granularities as fea-tures are made available to a supervised system.Note that this is different from (Agirre et al,2006), where a single global clustering was useddirectly in a greedy mapping to senses.2.3 Feature Assignment in ContextFor a given occurrence of a target word, theoverlap in words between the textual contextand all clusters from the neighborhood graph ismeasured.
The cluster ID of the cluster with thehighest overlap is assigned as a feature.
This canbe viewed as a word sense induction system in itsown right.At this, several clusterings from different param-eterizations are used to form distinct features,which enables the machine learning algorithm topick the most suitable cluster features per targetword when building the classification model.2.4 Corpora for Cluster FeaturesWhen incorporating features that are induced us-ing large unlabeled corpora, it is important to en-sure that the corpus for feature induction and theword sense labeled corpus are from the same do-main, ideally from the same source.Since TWSI has been created from Wikipedia, anEnglish Wikipedia dump from January 2008 isused for feature induction, comprising a total of 60million sentences.
The source for the lexical sam-ple task is the Wall Street Journal, and since the5676,400 sentences from the WSJ Penn Treebank arerather small for co-occurrence analysis, a 20 Mil-lion sentence New York Times corpus was usedinstead.For each corpus, a total of 45 different clus-terings were prepared for all combinations oft={50,100,150,200,250}, n={50,100,200} andnode degree weighting options (a), (b) and (c).3 Experimental Setup3.1 Machine Learning SetupThe classification algorithm used throughout thiswork is the AODE (Webb et al, 2005) classifieras provided by the WEKA Machine Learningsoftware (Hall et al, 2009).
This algorithm issimilar to a Na?
?ve Bayes classifier.
As opposedto the latter, AODE does not assume mutualindependence of features but models correlationsbetween them explicitly, which is highly desirablehere since both baseline and co-occurrence clusterfeatures are expected to be highly correlated.Further, AODE handles nominal features, so it isdirectly possible to use lexical features and clusterIDs in the classifier.
AODE showed superiorperformance to other classifiers handling nominalfeatures in preliminary experiments.3.2 Baseline SystemThe baseline system relies on 15 lexical and POS-based nominal features: word forms left and rightfrom target, POS sequences left and right bigramaround target, POS tags of left and right word fromtarget, and POS tag of target, two left and two rightnouns from target, left and right verbs from targetand left and right adjectives from target.3.3 Feature SelectionTo determine the most useful cluster co-occurrence features, they were added to thebaseline features one at the time, measuring thecontribution using 10-fold cross validation onthe training set.
Then, the best k single clusterfeatures for k={2,3,5,10} were added togetherto account for a range of different granularities.The best performing system on the lexical sampletraining data resulted in a 10-fold accuracy of88.5% (baseline: 87.1%) for k=3.
On the 204ambiguous words (595 total senses with 46sentences per sense on average) of the TWSIonly, the best system was found at k=5 with aSystem F1NUS-ML 88.7% ?
1.2top3 cluster, optimal F1 88.0% ?
1.2top3 cluster, max recall 87.8% ?
1.2baseline, optimal F1 87.5% ?
1.2baseline, max recall 87.3% ?
1.2UBC-ALM 86.9% ?
1.2Table 1: Cluster co-occurrence features and base-line in comparison to the best two systems in theSemEval 2007 Task 17 Lexical Sample evaluation(Pradhan et al, 2007).
Error margins provided bythe task organizers.10-fold accuracy of 83.0% (baseline: 80.7%,MFS: 71.5%).
Across the board, all singleco-occurrence features improve over the baseline,most of them significantly.4 Results4.1 SemEval 2007 lexical sample taskThe system in the configuration determined abovewas trained on the full training set and applied itto the test data provided bt the task organizers.Since the AODE classifier reports a confidencescore (corresponding to the class probability forthe winning class at classification time), it is possi-ble to investigate a tradeoff between precision andrecall to optimize the F1-value3 used for scoringin the lexical sample task.It is surprising that the baseline system outper-forms the second-best system in the 2007 evalua-tion, see Table 1.
This might be attributed to theAODE classifier used, but also hints at the powerof nominal lexical features in general.The co-occurrence cluster system outperforms thebaseline, but does not reach the performance of thewinning system.
However, all reported systemsfall into each other?s error margins, unlike whenevaluating on training data splits.
In conclusion,the WSD setup is competitive to other WSD sys-tems in the literature, while using only minimallinguistic preprocessing and no word sense inven-tory information beyond what is provided by train-ing examples.3F1 = (2 ?
precision ?
recall)/(precision + recall)57SubstitutionsGold System RandomYES 469 (93.8%) 456 (91.2%) 12 (2.4%)NO 14 (2.8%) 27 (5.4%) 485 (97.0%)SOMEWHAT 17 (3.4%) 17 (3.4%) 3 (0.6%)Table 2: Substitution acceptability as measured bycrowdsourcing for TWSI gold assignments, sys-tem assignments and random assignments.4.2 Substitution AcceptabilityFor evaluating substitution acceptability, 500labeled sentences from the overall data (for all397 nouns, not just the ambiguous nouns used inthe experiments above) were randomly selected.The 10-fold test classifications as described abovewere used for system word sense assignment.
Thethree highest ranked substitutions per sense fromthe TWSI are supplied as substitutions.In a crowdsourcing task, workers had to statewhether the substitutions provided for a targetword in context do not change the meaning ofthe sentence.
Each assignment was given to threeworkers.Since this measures both substitution quality ofthe TWSI and the system?s capability of assigningthe right sense, workers were also asked to scorethe substitutions for the gold standard assignmentsof this data set.
For control, random substitutionquality for all sentences is measured.Table 2 shows the results for averaging overthe worker?s responses.
For being counted asbelonging to the YES or NO class, the majority ofworkers had to choose this option, otherwise theitem was counted into the SOMEWHAT class.The substitution quality of the gold standard issomewhat noisy, containing 2.8% errors and 3.4%questionable cases.
Despite this, the system is ableto assign acceptable substitutions in over 91% ofcases, questionable substitutions for 3.4% at anerror rate of only 5.4%.
Checking the positivelyjudged random assignments, an acceptable substi-tution was found in about half of the cases by theauthor, which allows to estimate the worker noiseat about 1%.When using confidence values of the AODE clas-sifier to control recall as reported in Table 3, it ispossible to further reduce error rates, which mighte.g.
improve retrieval applications.coverage YES NO100% 91.2% 5.4%95% 91.8% 3.4%90% 93.8% 2.9%80% 94.8% 2.0%70% 95.7% 0.9%Table 3: Substitution acceptability in reduced cov-erage settings.
SOMEWHAT class accounts forpercentage points missing to 100%.5 ConclusionA way to improve WSD accuracy using a family ofco-occurrence cluster features was demonstratedon two data sets.
Instead of optimizing parametersglobally, features corresponding to different gran-ularities of induced word usages are made avail-able in parallel as features in a supervised MachineLearning setting.Whereas the contribution of co-occurrence fea-tures is significant on the TWSI, it is not signif-icantly improving results on the SemEval 2007data.
This might be attributed to a larger numberof average training examples in the latter, makingsmoothing over clusters less necessary due to lesslexical sparsity.We measured performance of our lexical substi-tution system by having the acceptability of thesystem-provided substitutions in context manuallyjudged.
With error rates in the single figures andthe possibility to reduce error further by sacrific-ing recall, we provide a firm enabling technologyfor semantic search.For future work, it would be interesting to evaluatethe full substitution system based on the TWSI ina semantic retrieval application.ReferencesEneko Agirre and Philip Edmonds, editors.
2006.Word Sense Disambiguation: Algorithms and Appli-cations, volume 33 of Text, Speech and LanguageTechnology.
Springer, July.Eneko Agirre, David Mart?
?nez, Oier L. de Lacalle, andAitor Soroa.
2006.
Evaluating and optimizing theparameters of an unsupervised graph-based wsd al-gorithm.
In Proceedings of TextGraphs: the Sec-ond Workshop on Graph Based Methods for NaturalLanguage Processing, pages 89?96, New York City.Association for Computational Linguistics.58Chris Biemann and Valerie Nygaard.
2010.
Crowd-sourcing WordNet.
In Proceedings of the 5th GlobalWordNet conference, Mumbai, India.
ACL Data andCode Repository, ADCR2010T005.Chris Biemann.
2006.
Chinese whispers - an efficientgraph clustering algorithm and its application to nat-ural language processing problems.
In Proceedingsof the HLT-NAACL-06 Workshop on Textgraphs-06,New York, USA.Mark Hall, Eibe Frank, Geoffrey Holmes, BernhardPfahringer, Peter Reutemann, and Ian H. Witten.2009.
The WEKA data mining software: An up-date.
SIGKDD Explorations, 11(1).Eduard Hovy, Mitchell Marcus, Martha Palmer,Lance Ramshaw, and Ralph Weischedel.
2006.OntoNotes: The 90% solution.
In Proceedings ofHLT-NAACL 2006, pages 57?60.David Mart?
?nez, Oier Lopez de Lacalle, and EnekoAgirre.
2008.
On the use of automatically acquiredexamples for all-nouns word sense disambiguation.J.
Artif.
Intell.
Res.
(JAIR), 33:79?107.Sameer Pradhan, Edward Loper, Dmitriy Dligach, andMartha Palmer.
2007.
SemEval-2007 Task-17: En-glish Lexical Sample, SRL and All Words.
In Pro-ceedings of the Fourth International Workshop onSemantic Evaluations (SemEval-2007), pages 87?92, Prague, Czech Republic, June.
Association forComputational Linguistics.Jean Veronis.
2004.
Hyperlex: lexical cartographyfor information retrieval.
Computer Speech & Lan-guage, 18(3):223?252.G.
Webb, J. Boughton, and Z. Wang.
2005.
Not soNaive Bayes: Aggregating one-dependence estima-tors.
Machine Learning, 58(1):5?24.Dominic Widdows and Beate Dorow.
2002.
A graphmodel for unsupervised lexical acquisition.
In Pro-ceedings of the 19th international conference onComputational linguistics, pages 1?7, Morristown,NJ, USA.
Association for Computational Linguis-tics.59
