Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 508?513,Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational LinguisticsThey Can Help: Using Crowdsourcing to Improve the Evaluation ofGrammatical Error Detection SystemsNitin Madnania Joel Tetreaulta Martin Chodorowb Alla RozovskayacaEducational Testing ServicePrinceton, NJ{nmadnani,jtetreault}@ets.orgbHunter College of CUNYmartin.chodorow@hunter.cuny.educUniversity of Illinois at Urbana-Champaignrozovska@illinois.eduAbstractDespite the rising interest in developing gram-matical error detection systems for non-nativespeakers of English, progress in the field hasbeen hampered by a lack of informative met-rics and an inability to directly compare theperformance of systems developed by differ-ent researchers.
In this paper we addressthese problems by presenting two evaluationmethodologies, both based on a novel use ofcrowdsourcing.1 Motivation and ContributionsOne of the fastest growing areas in need of NLPtools is the field of grammatical error detection forlearners of English as a Second Language (ESL).According to Guo and Beckett (2007), ?over a bil-lion people speak English as their second or for-eign language.?
This high demand has resulted inmany NLP research papers on the topic, a SynthesisSeries book (Leacock et al, 2010) and a recurringworkshop (Tetreault et al, 2010a), all in the last fiveyears.
In this year?s ACL conference, there are fourlong papers devoted to this topic.Despite the growing interest, two major factorsencumber the growth of this subfield.
First, the lackof consistent and appropriate score reporting is anissue.
Most work reports results in the form of pre-cision and recall as measured against the judgmentof a single human rater.
This is problematic becausemost usage errors (such as those in article and prepo-sition usage) are a matter of degree rather than sim-ple rule violations such as number agreement.
As aconsequence, it is common for two native speakersto have different judgments of usage.
Therefore, anappropriate evaluation should take this into accountby not only enlisting multiple human judges but alsoaggregating these judgments in a graded manner.Second, systems are hardly ever compared to eachother.
In fact, to our knowledge, no two systemsdeveloped by different groups have been compareddirectly within the field primarily because there isno common corpus or shared task?both commonlyfound in other NLP areas such as machine transla-tion.1 For example, Tetreault and Chodorow (2008),Gamon et al (2008) and Felice and Pulman (2008)developed preposition error detection systems, butevaluated on three different corpora using differentevaluation measures.The goal of this paper is to address the aboveissues by using crowdsourcing, which has beenproven effective for collecting multiple, reliablejudgments in other NLP tasks: machine transla-tion (Callison-Burch, 2009; Zaidan and Callison-Burch, 2010), speech recognition (Evanini et al,2010; Novotney and Callison-Burch, 2010), au-tomated paraphrase generation (Madnani, 2010),anaphora resolution (Chamberlain et al, 2009),word sense disambiguation (Akkaya et al, 2010),lexicon construction for less commonly taught lan-guages (Irvine and Klementiev, 2010), fact min-ing (Wang and Callison-Burch, 2010) and namedentity recognition (Finin et al, 2010) among severalothers.In particular, we make a significant contributionto the field by showing how to leverage crowdsourc-1There has been a recent proposal for a related sharedtask (Dale and Kilgarriff, 2010) that shows promise.508ing to both address the lack of appropriate evaluationmetrics and to make system comparison easier.
Oursolution is general enough for, in the simplest case,intrinsically evaluating a single system on a singledataset and, more realistically, comparing two dif-ferent systems (from same or different groups).2 A Case Study: Extraneous PrepositionsWe consider the problem of detecting an extraneouspreposition error, i.e., incorrectly using a preposi-tion where none is licensed.
In the sentence ?Theycame to outside?, the preposition to is an extrane-ous error whereas in the sentence ?They arrivedto the town?
the preposition to is a confusion er-ror (cf.
arrived in the town).
Most work on au-tomated correction of preposition errors, with theexception of Gamon (2010), addresses prepositionconfusion errors e.g., (Felice and Pulman, 2008;Tetreault and Chodorow, 2008; Rozovskaya andRoth, 2010b).
One reason is that in addition to thestandard context-based features used to detect con-fusion errors, identifying extraneous prepositionsalso requires actual knowledge of when a preposi-tion can and cannot be used.
Despite this lack ofattention, extraneous prepositions account for a sig-nificant proportion?as much as 18% in essays byadvanced English learners (Rozovskaya and Roth,2010a)?of all preposition usage errors.2.1 Data and SystemsFor the experiments in this paper, we chose a propri-etary corpus of about 500,000 essays written by ESLstudents for Test of English as a Foreign Language(TOEFL R?).
Despite being common ESL errors,preposition errors are still infrequent overall, withover 90% of prepositions being used correctly (Lea-cock et al, 2010; Rozovskaya and Roth, 2010a).Given this fact about error sparsity, we needed an ef-ficient method to extract a good number of error in-stances (for statistical reliability) from the large es-say corpus.
We found all trigrams in our essays con-taining prepositions as the middle word (e.g., marrywith her) and then looked up the counts of each tri-gram and the corresponding bigram with the prepo-sition removed (marry her) in the Google Web1T5-gram Corpus.
If the trigram was unattested or hada count much lower than expected based on the bi-gram count, then we manually inspected the trigramto see whether it was actually an error.
If it was,we extracted a sentence from the large essay corpuscontaining this erroneous trigram.
Once we had ex-tracted 500 sentences containing extraneous prepo-sition error instances, we added 500 sentences con-taining correct instances of preposition usage.
Thisyielded a corpus of 1000 sentences with a 50% errorrate.These sentences, with the target preposition high-lighted, were presented to 3 expert annotators whoare native English speakers.
They were asked toannotate the preposition usage instance as one ofthe following: extraneous (Error), not extraneous(OK) or too hard to decide (Unknown); the last cat-egory was needed for cases where the context wastoo messy to make a decision about the highlightedpreposition.
On average, the three experts had anagreement of 0.87 and a kappa of 0.75.
For subse-quent analysis, we only use the classes Error andOK since Unknown was used extremely rarely andnever by all 3 experts for the same sentence.We used two different error detection systems toillustrate our evaluation methodology:2?
LM: A 4-gram language model trained onthe Google Web1T 5-gram Corpus withSRILM (Stolcke, 2002).?
PERC: An averaged Perceptron (Freund andSchapire, 1999) classifier?
as implemented inthe Learning by Java toolkit (Rizzolo and Roth,2007)?trained on 7 million examples and us-ing the same features employed by Tetreaultand Chodorow (2008).3 CrowdsourcingRecently,we showed that Amazon Mechanical Turk(AMT) is a cheap and effective alternative to expertraters for annotating preposition errors (Tetreault etal., 2010b).
In other current work, we have extendedthis pilot study to show that CrowdFlower, a crowd-sourcing service that allows for stronger quality con-trol on untrained human raters (henceforth, Turkers),is more reliable than AMT on three different errordetection tasks (article errors, confused prepositions2Any conclusions drawn in this paper pertain only to thesespecific instantiations of the two systems.509& extraneous prepositions).
To impose such qualitycontrol, one has to provide ?gold?
instances, i.e., ex-amples with known correct judgments that are thenused to root out any Turkers with low performanceon these instances.
For all three tasks, we obtained20 Turkers?
judgments via CrowdFlower for each in-stance and found that, on average, only 3 Turkerswere required to match the experts.More specifically, for the extraneous prepositionerror task, we used 75 sentences as gold and ob-tained judgments for the remaining 923 non-goldsentences.3 We found that if we used 3 Turker judg-ments in a majority vote, the agreement with any oneof the three expert raters is, on average, 0.87 with akappa of 0.76.
This is on par with the inter-expertagreement and kappa found earlier (0.87 and 0.75respectively).The extraneous preposition annotation cost only$325 (923 judgments ?
20 Turkers) and was com-pleted in a single day.
The only restriction on theTurkers was that they be physically located in theUSA.
For the analysis in subsequent sections, weuse these 923 sentences and the respective 20 judg-ments obtained via CrowdFlower.
The 3 expertjudgments are not used any further in this analysis.4 Revamping System EvaluationIn this section, we provide details on how crowd-sourcing can help revamp the evaluation of error de-tection systems: (a) by providing more informativemeasures for the intrinsic evaluation of a single sys-tem (?
4.1), and (b) by easily enabling system com-parison (?
4.2).4.1 Crowd-informed Evaluation MeasuresWhen evaluating the performance of grammaticalerror detection systems against human judgments,the judgments for each instance are generally re-duced to the single most frequent category: Erroror OK.
This reduction is not an accurate reflectionof a complex phenomenon.
It discards valuable in-formation about the acceptability of usage becauseit treats all ?bad?
uses as equal (and all good onesas equal), when they are not.
Arguably, it wouldbe fairer to use a continuous scale, such as the pro-portion of raters who judge an instance as correct or3We found 2 duplicate sentences and removed them.incorrect.
For example, if 90% of raters agree on arating of Error for an instance of preposition usage,then that is stronger evidence that the usage is an er-ror than if 56% of Turkers classified it as Error and44% classified it as OK (the sentence ?In additionclassmates play with some game and enjoy?
is an ex-ample).
The regular measures of precision and recallwould be fairer if they reflected this reality.
Besidesfairness, another reason to use a continuous scale isthat of stability, particularly with a small number ofinstances in the evaluation set (quite common in thefield).
By relying on majority judgments, precisionand recall measures tend to be unstable (see below).We modify the measures of precision and re-call to incorporate distributions of correctness, ob-tained via crowdsourcing, in order to make themfairer and more stable indicators of system perfor-mance.
Given an error detection system that classi-fies a sentence containing a specific preposition asError (class 1) if the preposition is extraneous andOK (class 0) otherwise, we propose the followingweighted versions of hits (Hw), misses (Mw) andfalse positives (FPw):Hw =N?i(cisys ?
picrowd) (1)Mw =N?i((1?
cisys) ?
picrowd) (2)FPw =N?i(cisys ?
(1?
picrowd)) (3)In the above equations, N is the total number ofinstances, cisys is the class (1 or 0) , and picrowdindicates the proportion of the crowd that classi-fied instance i as Error.
Note that if we were torevert to the majority crowd judgment as the solejudgment for each instance, instead of proportions,picrowd would always be either 1 or 0 and the aboveformulae would simply compute the normal hits,misses and false positives.
Given these definitions,weighted precision can be defined as Precisionw =Hw/(Hw + FPw) and weighted recall as Recallw =Hw/(Hw + Mw).510agreementcount010020030040050050 60 70 80 90 100Figure 1: Histogram of Turker agreements for all 923 in-stances on whether a preposition is extraneous.Precision RecallUnweighted 0.957 0.384Weighted 0.900 0.371Table 1: Comparing commonly used (unweighted) andproposed (weighted) precision/recall measures for LM.To illustrate the utility of these weighted mea-sures, we evaluated the LM and PERC systemson the dataset containing 923 preposition instances,against all 20 Turker judgments.
Figure 1 shows ahistogram of the Turker agreement for the major-ity rating over the set.
Table 1 shows both the un-weighted (discrete majority judgment) and weighted(continuous Turker proportion) versions of precisionand recall for this system.The numbers clearly show that in the unweightedcase, the performance of the system is overesti-mated simply because the system is getting as muchcredit for each contentious case (low agreement)as for each clear one (high agreement).
In theweighted measure we propose, the contentious casesare weighted lower and therefore their contributionto the overall performance is reduced.
This is afairer representation since the system should not beexpected to perform as well on the less reliable in-stances as it does on the clear-cut instances.
Essen-tially, if humans cannot consistently decide whether0.00.20.40.60.81.0Precision/Recall50?75%[n=93] 75?90%[n=114] 90?100%[n=716]Agreement BinLM PrecisionPERC PrecisionLM RecallPERC RecallFigure 2: Unweighted precision/recall by agreement binsfor LM & PERC.a case is an error then a system?s output cannot beconsidered entirely right or entirely wrong.4As an added advantage, the weighted measuresare more stable.
Consider a contentious instance ina small dataset where 7 out of 15 Turkers (a minor-ity) classified it as Error.
However, it might easilyhave happened that 8 Turkers (a majority) classifiedit as Error instead of 7.
In that case, the change inunweighted precision would have been much largerthan is warranted by such a small change in thedata.
However, weighted precision is guaranteed tobe more stable.
Note that the instability decreasesas the size of the dataset increases but still remains aproblem.4.2 Enabling System ComparisonIn this section, we show how to easily compare dif-ferent systems both on the same data (in the idealcase of a shared dataset being available) and, morerealistically, on different datasets.
Figure 2 shows(unweighted) precision and recall of LM and PERC(computed against the majority Turker judgment)for three agreement bins, where each bin is definedas containing only the instances with Turker agree-ment in a specific range.
We chose the bins shown4The difference between unweighted and weighted mea-sures can vary depending on the distribution of agreement.511since they are sufficiently large and represent a rea-sonable stratification of the agreement space.
Notethat we are not weighting the precision and recall inthis case since we have already used the agreementproportions to create the bins.This curve enables us to compare the two sys-tems easily on different levels of item contentious-ness and, therefore, conveys much more informationthan what is usually reported (a single number forunweighted precision/recall over the whole corpus).For example, from this graph, PERC is seen to havesimilar performance as LM for the 75-90% agree-ment bin.
In addition, even though LM precision isperfect (1.0) for the most contentious instances (the50-75% bin), this turns out to be an artifact of theLM classifier?s decision process.
When it must de-cide between what it views as two equally likely pos-sibilities, it defaults to OK.
Therefore, even thoughLM has higher unweighted precision (0.957) thanPERC (0.813), it is only really better on the mostclear-cut cases (the 90-100% bin).
If one were to re-port unweighted precision and recall without usingany bins?as is the norm?this important qualifica-tion would have been harder to discover.While this example uses the same dataset for eval-uating two systems, the procedure is general enoughto allow two systems to be compared on two dif-ferent datasets by simply examining the two plots.However, two potential issues arise in that case.
Thefirst is that the bin sizes will likely vary across thetwo plots.
However, this should not be a significantproblem as long as the bins are sufficiently large.
Asecond, more serious, issue is that the error rates (theproportion of instances that are actually erroneous)in each bin may be different across the two plots.
Tohandle this, we recommend that a kappa-agreementplot be used instead of the precision-agreement plotshown here.5 ConclusionsOur goal is to propose best practices to address thetwo primary problems in evaluating grammatical er-ror detection systems and we do so by leveragingcrowdsourcing.
For system development, we rec-ommend that rather than compressing multiple judg-ments down to the majority, it is better to use agree-ment proportions to weight precision and recall toyield fairer and more stable indicators of perfor-mance.For system comparison, we argue that the bestsolution is to use a shared dataset and present theprecision-agreement plot using a set of agreed-uponbins (possibly in conjunction with the weighted pre-cision and recall measures) for a more informativecomparison.
However, we recognize that shareddatasets are harder to create in this field (as most ofthe data is proprietary).
Therefore, we also providea way to compare multiple systems across differ-ent datasets by using kappa-agreement plots.
As foragreement bins, we posit that the agreement valuesused to define them depend on the task and, there-fore, should be determined by the community.Note that both of these practices can also be im-plemented by using 20 experts instead of 20 Turkers.However, we show that crowdsourcing yields judg-ments that are as good but without the cost.
To fa-cilitate the adoption of these practices, we make allour evaluation code and data available to the com-munity.5AcknowledgmentsWe would first like to thank our expert annotatorsSarah Ohls and Waverely VanWinkle for their hoursof hard work.
We would also like to acknowledgeLei Chen, Keelan Evanini, Jennifer Foster, DerrickHiggins and the three anonymous reviewers for theirhelpful comments and feedback.ReferencesCem Akkaya, Alexander Conrad, Janyce Wiebe, andRada Mihalcea.
2010.
Amazon Mechanical Turkfor Subjectivity Word Sense Disambiguation.
In Pro-ceedings of the NAACL Workshop on Creating Speechand Language Data with Amazon?s Mechanical Turk,pages 195?203.Chris Callison-Burch.
2009.
Fast, Cheap, and Creative:Evaluating Translation Quality Using Amazon?s Me-chanical Turk.
In Proceedings of EMNLP, pages 286?295.Jon Chamberlain, Massimo Poesio, and Udo Kruschwitz.2009.
A Demonstration of Human Computation Us-ing the Phrase Detectives Annotation Game.
In ACMSIGKDD Workshop on Human Computation, pages23?24.5http://bit.ly/crowdgrammar512Robert Dale and Adam Kilgarriff.
2010.
Helping OurOwn: Text Massaging for Computational Linguisticsas a New Shared Task.
In Proceedings of INLG.Keelan Evanini, Derrick Higgins, and Klaus Zechner.2010.
Using Amazon Mechanical Turk for Transcrip-tion of Non-Native Speech.
In Proceedings of theNAACL Workshop on Creating Speech and LanguageData with Amazon?s Mechanical Turk, pages 53?56.Rachele De Felice and Stephen Pulman.
2008.
AClassifier-Based Approach to Preposition and Deter-miner Error Correction in L2 English.
In Proceedingsof COLING, pages 169?176.Tim Finin, William Murnane, Anand Karandikar,Nicholas Keller, Justin Martineau, and Mark Dredze.2010.
Annotating Named Entities in Twitter Data withCrowdsourcing.
In Proceedings of the NAACL Work-shop on Creating Speech and Language Data withAmazon?s Mechanical Turk, pages 80?88.Yoav Freund and Robert E. Schapire.
1999.
Large Mar-gin Classification Using the Perceptron Algorithm.Machine Learning, 37(3):277?296.Michael Gamon, Jianfeng Gao, Chris Brockett, Alexan-der Klementiev, William Dolan, Dmitriy Belenko, andLucy Vanderwende.
2008.
Using Contextual SpellerTechniques and Language Modeling for ESL ErrorCorrection.
In Proceedings of IJCNLP.Michael Gamon.
2010.
Using Mostly Native Data toCorrect Errors in Learners?
Writing.
In Proceedingsof NAACL, pages 163?171.Y.
Guo and Gulbahar Beckett.
2007.
The Hegemonyof English as a Global Language: Reclaiming LocalKnowledge and Culture in China.
Convergence: In-ternational Journal of Adult Education, 1.Ann Irvine and Alexandre Klementiev.
2010.
UsingMechanical Turk to Annotate Lexicons for Less Com-monly Used Languages.
In Proceedings of the NAACLWorkshop on Creating Speech and Language Datawith Amazon?s Mechanical Turk, pages 108?113.Claudia Leacock, Martin Chodorow, Michael Gamon,and Joel Tetreault.
2010.
Automated GrammaticalError Detection for Language Learners.
SynthesisLectures on Human Language Technologies.
MorganClaypool.Nitin Madnani.
2010.
The Circle of Meaning: FromTranslation to Paraphrasing and Back.
Ph.D. thesis,Department of Computer Science, University of Mary-land College Park.Scott Novotney and Chris Callison-Burch.
2010.
Cheap,Fast and Good Enough: Automatic Speech Recogni-tion with Non-Expert Transcription.
In Proceedingsof NAACL, pages 207?215.Nicholas Rizzolo and Dan Roth.
2007.
ModelingDiscriminative Global Inference.
In Proceedings ofthe First IEEE International Conference on SemanticComputing (ICSC), pages 597?604, Irvine, California,September.Alla Rozovskaya and D. Roth.
2010a.
Annotating ESLerrors: Challenges and rewards.
In Proceedings of theNAACLWorkshop on Innovative Use of NLP for Build-ing Educational Applications.Alla Rozovskaya and D. Roth.
2010b.
Generating Con-fusion Sets for Context-Sensitive Error Correction.
InProceedings of EMNLP.Andreas Stolcke.
2002.
SRILM: An Extensible Lan-guage Modeling Toolkit.
In Proceedings of the Inter-national Conference on Spoken Language Processing,pages 257?286.Joel Tetreault and Martin Chodorow.
2008.
The Ups andDowns of Preposition Error Detection in ESL Writing.In Proceedings of COLING, pages 865?872.Joel Tetreault, Jill Burstein, and Claudia Leacock, edi-tors.
2010a.
Proceedings of the NAACL Workshop onInnovative Use of NLP for Building Educational Ap-plications.Joel Tetreault, Elena Filatova, and Martin Chodorow.2010b.
Rethinking Grammatical Error Annotation andEvaluation with the Amazon Mechanical Turk.
In Pro-ceedings of the NAACL Workshop on Innovative Useof NLP for Building Educational Applications, pages45?48.Rui Wang and Chris Callison-Burch.
2010.
Cheap Factsand Counter-Facts.
In Proceedings of the NAACLWorkshop on Creating Speech and Language Datawith Amazon?s Mechanical Turk, pages 163?167.Omar F. Zaidan and Chris Callison-Burch.
2010.
Pre-dicting Human-Targeted Translation Edit Rate via Un-trained Human Annotators.
In Proceedings of NAACL,pages 369?372.513
