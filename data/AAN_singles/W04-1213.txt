Introduction to the Bio-Entity Recognition Task at JNLPBAJin-Dong KIM, Tomoko OHTA, Yoshimasa TSURUOKA, Yuka TATEISICREST, Japan Science and Technology Agency, andDepartment of Computer Science, University of Tokyo,7-3-1 Hongo, Bunkyo-ku, Tokyo 113-0033, Japan?Nigel COLLIERNational Institute of Informatics,2-1-2 Hitotsubashi, Chiyoda-ku, Tokyo 101-8430, Japan?AbstractWe describe here the JNLPBA shared task ofbio-entity recognition using an extended versionof the GENIA version 3 named entity corpus ofMEDLINE abstracts.
We provide backgroundinformation on the task and present a generaldiscussion of the approaches taken by partici-pating systems.1 IntroductionBio-entity recognition aims to identify and clas-sify technical terms in the domain of molecu-lar biology that correspond to instances of con-cepts that are of interest to biologists.
Exam-ples of such entities include the names of pro-teins, genes and their locations of activity suchas cells or organism names as shown in Figure 1.Entity recognition is a core component tech-nology in several higher level information accesstasks such as information extraction (templatefilling), summarization and question answering.These tasks aim to help users find structure inunstructured text data and aid in finding rele-vant factual information.
This is becoming in-creasingly important with the massive increasein reported results due to high throughput ex-perimental methods.Bio-entity recognition by computers remainsa significantly challenging task.
Despite goodprogress in newswire entity recognition (e.g.
(MUC, 1995; Tjong Kim Sang and De Meul-der, 2003)) that has led to ?near human?
levelsof performance, measured in the high 90s for F-score (van Rijsbergen, 1979), similar methodshave not performed so well in the bio-domainleaving an accuracy gap of some 30 points of F-score.
Challenges occur for example due to am-biguity in the left boundary of entities causedby descriptive naming, shortened forms due toabbreviation and aliasing, the difficulty of creat-?
{jdkim,yucca,okap,tsuruoka}@is.s.u-tokyo.ac.jp?
collier@nii.ac.jpWe have shown that <conssem=?G#protein?>interleukin-1</cons>(<cons sem=?G#protein?>IL-1</cons>)and <cons sem=?G#protein?>IL-2</cons>control <cons sem=?G#DNA?>IL-2 receptoralpha (IL-2R alpha) gene</cons> transcriptionin <cons sem=?G#cell line?>CD4-CD8-murine T lymphocyte precursors</cons>.Figure 1: Example MEDLINE sentence markedup in XML for molecular biology named-entities.ing consistently annotated human training datawith a large number of classes, etc.
In or-der to make progress it is becoming clear thatseveral points need to be considered: (1) ex-tension of feature sets beyond the lexical level(part of speech, orthography etc.)
and use ofhigher-levels of linguistic knowledge such as de-pendency relations, (2) potential for re-use ofexternal domain knowledge resources such asgazetteers and ontologies, (3) improved qualitycontrol methods for building annotation collec-tions, (4) fine grained error analysis beyond theF-score statistics.The JNLPBA shared task 1 is an open chal-lenge task and as such we allowed participantsto use whatever methodology and knowledgesources they liked in the bio-entity task.
Thesystems were evaluated on a common bench-mark data set using a common evaluationmethod.
Although it is not directly possibleto compare systems due to the diversity of re-sources used the F-score results provide an ap-proximate indication of how useful each methodis.2 DataThe training data used in the task came fromthe GENIA version 3.02 corpus (Kim et al,1http://research.nii.ac.jp/?collier/workshops/JNLPBA04st.htm702003).
This was formed from a controlled searchon MEDLINE using the MeSH terms ?human?,?blood cells?
and ?transcription factors?.
Fromthis search 2,000 abstracts were selected andhand annotated according to a small taxon-omy of 48 classes based on a chemical classi-fication.
Among the classes, 36 terminal classeswere used to annotate the GENIA corpus.The GENIA corpus is important for two ma-jor reasons: the first is that it provides thelargest single source of annotated training datafor the NE task in molecular biology and thesecond is in the breadth of classification.
Al-though 36 classes is a fraction of the classes con-tained in major taxonomies it is still the largestclass set that has been attempted so far for theNE task.
In this respect it is an important testof the limits of human and machine annotationcapability.
For the shared task we decided how-ever to simplify the 36 classes and used only theclasses protein, DNA, RNA, cell line and celltype.
The first three incorporate several sub-classes from the original taxonomy while thelast two are interesting in order to make thetask realistic for post-processing by a potentialtemplate filling application.For testing purposes we used a newly anno-tated collection of MEDLINE abstracts fromthe GENIA project.
404 abstracts were usedthat were annotated for the same classes of en-tities: Half of them were from the same domainas the training data and the other half of themwere from the super-domain of ?blood cells?
and?transcription factors?.
Our hope was that thisshould provide an important test of generaliz-ability in the methods used.3 EvaluationThe 2,000 abstracts of the GENIA corpus ver-sion 3.02 which had already been made publiclyavailable were formatted for IOB2 notation andmade available as training materials.
For test-ing, additional 404 abstracts were randomly se-lected from an unpublished set of the GENIAcorpus and the annotations were re-checked by abiologist.
The training set consists of abstractsretrieved from the MEDLINE database withMeSH terms ?human?, ?blood cells?
and ?tran-scription factors?, and their publication yearranges over 1990?1999.
Most parts of the testset include abstracts retrieved with the sameset of MeSH terms, and their publication yearranges over 1978?2001.
To see the effect of pub-lication year, the test set was roughly dividedinto four subsets: 1978-1989 set (which rep-resents an old age from the viewpoint of themodels that will be trained using the trainingset), 1990-1999 set (which represents the sameage as the training set), 2000-2001 set (whichrepresents a new age compared to the trainingset) and S/1998-2001 set (which representsroughly a new age in a super domain).
The lastsubset represents a super domain and the ab-stracts was retrieved with MeSH terms, ?bloodcells?
and ?transcription factors?
(without ?hu-man?)2.
Table 1 illustrates the size of the datasetsTable 2 shows the number of entities anno-tated in each data set3.
As seen in the ta-ble, the annotation density of proteins increasesover the ages significantly, whereas the anno-tation density of DNAs and RNAs increases inthe 1990-1999 set and slightly decreases in the2000-2001 set.
This tendency roughly corre-sponds to the expansion in the subject area as awhole that can be estimated from statistics onthe MeSH terms introduced in each age shownin Table 3.
This observation suggests that thedensity of mention of a class of entities in aca-demic papers is affected by the amount of inter-est the entity receives in each age.Figure 2 shows the ratio of annotated struc-tures in each set.
In accordance with our expec-tation, the 1990-1999 set has the most simi-lar annotation trait with the training set.
The2000-2001 set is also similar to the trainingset, but the 1978-1989 set had quite a differ-ent distribution of entity classes.
The variationof domain does not seem to make any signif-icant difference to the distribution of entitiesmentioned.
One reason may be the large frac-tion of abstracts from the same domain in thesuper domain set.
In fact, among 206 abstractsin the super domain set, 140 abstracts (69%)are also from the same domain.
It also corre-sponds to the fraction in the whole MEDLINEdatabase: among 9,362 abstracts that can beretrieved with MeSH terms, ?blood cells?
and?transcription factors?, 6,297 abstracts (67%)can also be retrieved with MeSH terms ?human?,?blood cells?
and ?transcription factors?.To simplify the annotation task to a simplelinear sequential analysis problem, embeddedstructures have been removed leaving only the2The S/1998-2001 set includes the whole 2000-2001 set.3The figures in the parenthesis are the average num-ber of entities per an abstract in each set.71# abs # sentences # wordsTraining Set 2,000 20,546 (10.27/abs) 472,006 (236.00/abs) (22.97/sen)Test Set 404 4,260 (10.54/abs) 96,780 (239.55/abs) (22.72/sen)1978-1989 104 991 ( 9.53/abs) 22,320 (214.62/abs) (22.52/sen)1990-1999 106 1,115 (10.52/abs) 25,080 (236.60/abs) (22.49/sen)2000-2001 130 1,452 (11.17/abs) 33,380 (256.77/abs) (22.99/sen)S/1998-2001 206 2,270 (11.02/abs) 51,957 (252.22/abs) (22.89/sen)Table 1: Basic statistics for the data setsprotein DNA RNA cell type cell line ALLTraining Set 30,269 (15.1) 9,533 (4.8) 951 (0.5) 6,718 (3.4) 3,830 (1.9) 51,301 (25.7)Test Set 5,067 (12.5) 1,056 (2.6) 118 (0.3) 1,921 (4.8) 500 (1.2) 8,662 (21.4)1978-1989 609 ( 5.9) 112 (1.1) 1 (0.0) 392 (3.8) 176 (1.7) 1,290 (12.4)1990-1999 1,420 (13.4) 385 (3.6) 49 (0.5) 459 (4.3) 168 (1.6) 2,481 (23.4)2000-2001 2,180 (16.8) 411 (3.2) 52 (0.4) 714 (5.5) 144 (1.1) 3,501 (26.9)S/1998-2001 3,186 (15.5) 588 (2.9) 70 (0.3) 1,138 (5.5) 170 (0.8) 5,152 (25.0)Table 2: Absolute (and relative) frequencies for NEs in each data set.
Figures for the test set arebroken down according to the age of the data.Figure 2: Ratio of annotated NEs                                                            !
          outermost structures (i.e.
the longest tag se-quence).
Consequently, a group of coordinatedentities involving ellipsis are annotated as onestructure like in the following example:... in [lymphocytes] and [T- and B-lymphocyte] count in ...In the example, ?T- and B-lymphocyte?
is an-notated as one structure but involves two entitynames, ?T-lymphocyte?
and ?B-lymphocyte?,whereas ?lymphocytes?
is annotated as one andinvolves as many entity names.prot.
DNA RNA ctype cline1978-1989 3.3 2.8 1.5 0.1 01990-1999 16.6 4.6 7.4 0.2 1.52000-2001 40.0 4.0 3.5 0 0Table 3: MeSH terms in each age (#/year)4 Evaluation MethodologyResults are given as F-scores using a modifiedversion of the CoNLL evaluation script and aredefined as F = (2PR)/(P + R), where P de-notes Precision and R Recall.
P is the ratio ofthe number of correctly found NE chunks to thenumber of found NE chunks, and R is the ra-tio of the number of correctly found NE chunksto the number of true NE chunks.
The scriptoutputs three sets of F-scores according to ex-act boundary match, right and left boundarymatching.
In the right boundary matching onlyright boundaries of entities are considered with-out matching left boundaries and vice versa.5 Participating Systems5.1 Classification ModelsRoughly four types of classification models wereapplied by the eight participating systems; Sup-port Vector Machines (SVMs), Hidden MarkovModels (HMMs), Maximum Entropy MarkovModels (MEMMs) and Conditional RandomFields (CRFs).
The most frequently applied72models were SVMs with totally five systemsadopting SVMs as the classification models ei-ther in isolation (Park et al, 2004; Lee etal., 2004) or in combination with other models(Zhou and Su, 2004; Song et al, 2004; Ro?ssler,2004).
HMMs were employed by one system inisolation (Zhao, 2004) and by two systems incombination with SVMs (Zhou and Su, 2004;Ro?ssler, 2004).
Similarly, CRFs were employedby one system in isolation (Settles, 2004) andby another system in combination with SVMs(Song et al, 2004).
It is somewhat surprisingthat Maximum Entropy Models were appliedby only one system (Finkel et al, 2004), whileit was the most successfully applied model inthe CoNLL-2003 Shared Task of Named EntityRecognition, and at this time also the MEMMsystem yields quite good performance.
One in-terpretation on this may be the CRF is oftenregarded as a kind of version-upped model ofthe MEMM (in the sense that both are condi-tional, exponential models) and thus is replac-ing MEMM.5.2 Features and External ResourcesIt has been found that utilizing various sourcesof information is crucial to get good perfor-mance in this kind of task.
Table 4 outlinessome of the features exploited by the systemsparticipating in the JNLPBA 2004 shared task(the table also lists the classification models em-ployed and external resources exploited by thesystems to provide the outline of the systems ata glance).Lexical features (words) were widely ex-ploited by three systems that didn?t employSVMs.
It seems that this may be due to SVMs?high time complexity and actually other twoSVM systems also employed lexical featuresonly in a limited way.
Instead, affixes, ortho-graphic features or word shapes that are all gen-eralized forms of lexical features were activelyexploited by most of the systems.
The ATCGsequence feature is an example of domain spe-cific orthographic features and was incorporatedin three systems.
Park et al (2004) suggestedthe use of word variation features, a unique wayof selecting substrings from words, but the ef-fectiveness was not reported.Part-of-speech information was incorporatedin five systems: four of them utilized domain-specialized part-of-speech taggers (Zhou andSu, 2004; Finkel et al, 2004; Song et al, 2004;Park et al, 2004) and the other utilized general-purpose taggers (Lee et al, 2004).
BeseNP tagsand deep syntactic features were also exploitedby several systems but the effectiveness was notclearly examined.The top-ranked two systems incorporated in-formation from gazetteers and employed abbre-viation handling mechanisms, which were re-ported to give good effect.
However, one par-ticipant (Settles, 2004) reported that their at-tempt to utilize gazetteers (together with otherresources) had failed in gaining better overallperformance.To overcome the shortage of training materi-als, several systems attempted to use externalresources.
Gazetteers are also examples of suchresources.
MEDLINE database was explored asa source of a large corpus that is similar to thetraining corpus, but one participant (Ro?ssler,2004) reported the attempt was not success-ful.
Finkel et al (2004) exploited BNC cor-pus and World Wide Web as knowledge sourcesand achieved good performance., but the effec-tiveness of the use of such resources was notclearly examined.
Song et al (2004) exploitedautomatically generated virtual examples andreported good effect on both recall and preci-sion.
Lee et al (2004) utilized external proteinand gene taggers instead of using gazetteers butthe effectiveness was not reported.5.3 Performances4Table 5 lists entity recognition performance ofeach system on each test set.
The baselinemodel (BL) utilizes lists of entities of each classcollected from the training set, and performslongest match search for entities through thetest set.
Frequency of each entity with eachclass is referred to break ties.It may be notable that SVMs worked muchbetter in combination with other models, whileother models showed reasonable performanceeven in isolation.
This fact suggests that globaloptimization over whole sequence (e.g, Viterbioptimization) is crucial in this type of tasks.
Asis well known, the outputs of SVMs are noteasy to use in global optimization.
It seems(Zhou and Su, 2004) overcomes the drawback ofSVMs by mapping the SVM output into proba-bility, and complementing it with Markov mod-els.
Their remarkable performance seems dueto the well designed classification model and the4A comprehensive report of systems perfor-mance is available at http://www-tsujii.is.s.u-tokyo.ac.jp/GENIA/ERtask/report.html.73CM lx af or sh gn wv ln gz po np sy tr ab ca do pa pr ext.Zho SH - + + - + - - + + - - + + + - - + -Fin M + + - + - - - + + - + - + - + + + B, WSet C + + + + - - - (+) - - - (+) - - - - + (W)Son SC * + + - - - - - + + - - - - - - + VZha H + - - - - - - - - - - - - - - - + MRo?s SH - + + - + - + - - - - - - - - - + (M)Par S - + + + + + - - + + - + - - - - - M, PLee S * + - - - - - - + - - - - - - - - Y, GTable 4: Overview of participating systems in terms of classification models, main features andexternal resources, sorted by performance.
Classification Model (CM): S: SVM; H: HMM; M:MEMM; C: CRF; lx: lexical features; af: affix information (character n-grams); or: orthographicinformation; sh: word shapes; gn: gene sequences (ATCG sequences); wv: word variations; ln:word length; gz: gazetteers; po: part-of-speech tags; np: noun phrase tags; sy: syntactic tags;tr: word triggers; ab: abbreviations; ca: cascaded entities; do: global document information; pa:parentheses handling; pr: previously predicted entity tags; External resources (ext): B: BritishNational Corpus; M: MEDLINE corpus; P: Penn Treebank II corpus; W: world wide web; V:virtually generated corpus; Y: Yapex; G: GAPSCORE.1978-1989 set 1990-1999 set 2000-2001 set S/1998-2001 set TotalZho 75.3 / 69.5 / 72.3 77.1 / 69.2 / 72.9 75.6 / 71.3 / 73.8 75.8 / 69.5 / 72.5 76.0 / 69.4 / 72.6Fin 66.9 / 70.4 / 68.6 73.8 / 69.4 / 71.5 72.6 / 69.3 / 70.9 71.8 / 67.5 / 69.6 71.6 / 68.6 / 70.1Set 63.6 / 71.4 / 67.3 72.2 / 68.7 / 70.4 71.3 / 69.6 / 70.5 71.3 / 68.8 / 70.1 70.3 / 69.3 / 69.8Son 60.3 / 66.2 / 63.1 71.2 / 65.6 / 68.2 69.5 / 65.8 / 67.6 68.3 / 64.0 / 66.1 67.8 / 64.8 / 66.3Zha 63.2 / 60.4 / 61.8 72.5 / 62.6 / 67.2 69.1 / 60.2 / 64.7 69.2 / 60.3 / 64.4 69.1 / 61.0 / 64.8Ro?s 59.2 / 60.3 / 59.8 70.3 / 61.8 / 65.8 68.4 / 61.5 / 64.8 68.3 / 60.4 / 64.1 67.4 / 61.0 / 64.0Par 62.8 / 55.9 / 59.2 70.3 / 61.4 / 65.6 65.1 / 60.4 / 62.7 65.9 / 59.7 / 62.7 66.5 / 59.8 / 63.0Lee 42.5 / 42.0 / 42.2 52.5 / 49.1 / 50.8 53.8 / 50.9 / 52.3 52.3 / 48.1 / 50.1 50.8 / 47.6 / 49.1BL 47.1 / 33.9 / 39.4 56.8 / 45.5 / 50.5 51.7 / 46.3 / 48.8 52.6 / 46.0 / 49.1 52.6 / 43.6 / 47.7Table 5: Performance of each participating system and a baseline model (BL) (recall / precision /F-score)rich set of features.As is naturally expected, most systems (5 outof 8) show their best performance on the 1990-1999 set which is believed to have the mostsimilar annotation trait.
The same tendency isobserved more clearly with recall (7 out of 8show their best performance on the 1990-1999set) while no such tendency is observed withprecision.
If we accept such tendency of showingbest performance on the most similar test set asnatural, one interpretation on the observationmight be that positive information has been wellexploited while negative information has not.Clearly, a such case is the baseline model whichutilizes only positive information and no nega-tive information.
Finkel et al (2004) explic-itly pointed out the problem of ?abusing?
posi-tive information with regard to using gazetteers,and utilized frequency information from BNCcorpus to prevent such ?abusement?.
Settles(2004)?s CRF system deserves special note inthe sense that it achieved comparable perfor-mance to top ranked systems with a rather sim-ple feature set.
This fact may suggest that inte-gration of information is as much important asdevelopment of useful features.As the resulting performance may not seemvery successful, other systems suggest interest-ing approaches: Song et al (2004) reportsabout the effectiveness of using virtual exam-ples.
Zhao (2004) reports about the usefulnessof unlabeled MEDLINE corpus as a complementto expensive and limited size of labeled corpus.Ro?ssler (2004) reports their experience to adaptan NER system for German to biomedical do-main.
Park et al (2004) reports their effortsto find out useful information by corpus com-parison.
Lee et al (2004) suggests the use ofexternal protein/gene taggers instead of usinggazetteers.6 ConclusionWhile it is not entirely meaningful to rank sys-tems performance according to simple F-scores,74the accuracy results do nevertheless show someimportant trends that may help guide futuresystem developers in the bio-entity task.
It isclear that we have to move beyond simple lexicalfeatures if we want to obtain high levels of per-formance in molecular biology and the top per-forming systems were seen to be those that em-ployed strong learning models (SVM, MEMMand CRF), rich feature sets, support for ?dif-ficult?
constructions such as parenthesized ex-pressions and a sophisticated mix of externalresources such as gazette lists and ontologieswhich provide terminological resources.
It isalso interesting to observe that we have seen thebeginning of a trend in the use of the Web whichcan provide online access to dynamically up-dated resources or sophisticated search for setsof similar terms.7 AcknowledgementsWe gratefully acknowledge Prof. Jun?ichi Tsu-jii, University of Tokyo, for his generous sup-port of the shared task.
The GENIA corpusis a product of the GENIA project which issupported by the Information Mobility Project(CREST, JST) and the Genome InformationScience Project (MEXT).ReferencesJin-Dong Kim, Tomoko Ohta, Yuka Tateishi,and Jun?ichi Tsujii.
2003.
GENIA cor-pus - a semantically annotated cor-pus for bio-textmining.
Bioinformatics,19(Suppl.1):180?182.DARPA.
1995.
Proceedings of the SixthMessage Understanding Conference(MUC-6),Columbia, MD, USA, November.
MorganKaufmann.Erik F. Tjong Kim Sang and Fien De Meul-der.
2003.
Introduction to the CoNLL-2003shared task: Language-independent namedentity recognition.
In Proceedings of the Sev-enth Conference on Natural Language Learn-ing (CoNLL-2003), pages 142?147.
Edmon-ton, Canada.C.
J. van Rijsbergen.
1979.
Information Re-trieval.
Butterworths, London.Jin-Dong Kim, Tomoko Ohta, Yuka Tateisiand Jun?ichi Tsujii.
2002.
Corpus-Based Ap-proach to Biological Entity Recognition.
inProceedings of the Second Meeting of the Spe-cial Interest Group on Test Data Mining ofISMB (BioLink-2002), Edmonton, Canada.GuoDong Zhou and Jian Su.
2004.
Explor-ing Deep Knowledge Resources in Biomedi-cal Name Recognition.
in Proceedings of theJoint Workshop on Natural Language Pro-cessing in Biomedicine and its Applications(JNLPBA-2004), Geneva, Switzerland.Jenny Finkel, Shipra Dingare, Huy Nguyen,Malvina Nissim, Gail Sinclair and Christo-pher Manning.
2004.
Exploiting Context forBiomedical Entity Recognition: From Syn-tax to the Web.
in Proceedings of the JointWorkshop on Natural Language Processing inBiomedicine and its Applications (JNLPBA-2004), Geneva, Switzerland.Burr Settles.
2004.
Biomedical Named En-tity Recognition Using Conditional RandomFields and Novel Feature Sets.
in Proceedingsof the Joint Workshop on Natural LanguageProcessing in Biomedicine and its Applica-tions (JNLPBA-2004), Geneva, Switzerland.Yu Song, Eunju Kim, Gary Geunbae Lee andByoung-kee Yi.
2004.
POSBIOTM-NER inthe shared task of BioNLP/NLPBA 2004. inProceedings of the Joint Workshop on Natu-ral Language Processing in Biomedicine andits Applications (JNLPBA-2004), Geneva,Switzerland.Shaojun Zhao.
2004.
Name Entity Recognitionin Biomedical Text using a HMM model inProceedings of the Joint Workshop on Natu-ral Language Processing in Biomedicine andits Applications (JNLPBA-2004), Geneva,Switzerland.Marc Ro?ssler.
2004.
Adapting an NER-Systemfor German to the Biomedical Domain.
inProceedings of the Joint Workshop on Nat-ural Language Processing in Biomedicine andits Applications (JNLPBA-2004), Geneva,Switzerland.Kyung-Mi Park, Seon-Ho Kim, Do-Gil Leeand Hae-Chang Rim.
2004.
Boosting Lex-ical Knowledge for Biomedical Named En-tity Recognition.
in Proceedings of the JointWorkshop on Natural Language Processing inBiomedicine and its Applications (JNLPBA-2004), Geneva, Switzerland.Chih Lee, Wen-Juan Hou and Hsin-Hsi Chen.2004.
Annotating Multiple Types of Biomed-ical Entities: A Single Word Classificica-tion Approach.
in Proceedings of the JointWorkshop on Natural Language Processing inBiomedicine and its Applications (JNLPBA-2004), Geneva, Switzerland.75
