Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 46?54,Suntec, Singapore, 2-7 August 2009. c?2009 ACL and AFNLPExploiting Heterogeneous Treebanks for ParsingZheng-Yu Niu, Haifeng Wang, Hua WuToshiba (China) Research and Development Center5/F., Tower W2, Oriental Plaza, Beijing, 100738, China{niuzhengyu,wanghaifeng,wuhua}@rdc.toshiba.com.cnAbstractWe address the issue of using heteroge-neous treebanks for parsing by breakingit down into two sub-problems, convert-ing grammar formalisms of the treebanksto the same one, and parsing on thesehomogeneous treebanks.
First we pro-pose to employ an iteratively trained tar-get grammar parser to perform grammarformalism conversion, eliminating prede-fined heuristic rules as required in previ-ous methods.
Then we provide two strate-gies to refine conversion results, and adopta corpus weighting technique for parsingon homogeneous treebanks.
Results on thePenn Treebank show that our conversionmethod achieves 42% error reduction overthe previous best result.
Evaluation onthe Penn Chinese Treebank indicates that aconverted dependency treebank helps con-stituency parsing and the use of unlabeleddata by self-training further increases pars-ing f-score to 85.2%, resulting in 6% errorreduction over the previous best result.1 IntroductionThe last few decades have seen the emergence ofmultiple treebanks annotated with different gram-mar formalisms, motivated by the diversity of lan-guages and linguistic theories, which is crucial tothe success of statistical parsing (Abeille et al,2000; Brants et al, 1999; Bohmova et al, 2003;Han et al, 2002; Kurohashi and Nagao, 1998;Marcus et al, 1993; Moreno et al, 2003; Xue etal., 2005).
Availability of multiple treebanks cre-ates a scenario where we have a treebank anno-tated with one grammar formalism, and anothertreebank annotated with another grammar formal-ism that we are interested in.
We call the firsta source treebank, and the second a target tree-bank.
We thus encounter a problem of how touse these heterogeneous treebanks for target gram-mar parsing.
Here heterogeneous treebanks referto two or more treebanks with different grammarformalisms, e.g., one treebank annotated with de-pendency structure (DS) and the other annotatedwith phrase structure (PS).It is important to acquire additional labeled datafor the target grammar parsing through exploita-tion of existing source treebanks since there is of-ten a shortage of labeled data.
However, to ourknowledge, there is no previous study on this is-sue.Recently there have been some works on us-ing multiple treebanks for domain adaptation ofparsers, where these treebanks have the samegrammar formalism (McClosky et al, 2006b;Roark and Bacchiani, 2003).
Other related worksfocus on converting one grammar formalism of atreebank to another and then conducting studies onthe converted treebank (Collins et al, 1999; Forst,2003; Wang et al, 1994; Watkinson and Manand-har, 2001).
These works were done either on mul-tiple treebanks with the same grammar formalismor on only one converted treebank.
We see thattheir scenarios are different from ours as we workwith multiple heterogeneous treebanks.For the use of heterogeneous treebanks1, wepropose a two-step solution: (1) converting thegrammar formalism of the source treebank to thetarget one, (2) refining converted trees and usingthem as additional training data to build a targetgrammar parser.For grammar formalism conversion, we choosethe DS to PS direction for the convenience of thecomparison with existing works (Xia and Palmer,2001; Xia et al, 2008).
Specifically, we assumethat the source grammar formalism is dependency1Here we assume the existence of two treebanks.46grammar, and the target grammar formalism isphrase structure grammar.Previous methods for DS to PS conversion(Collins et al, 1999; Covington, 1994; Xia andPalmer, 2001; Xia et al, 2008) often rely on pre-defined heuristic rules to eliminate converison am-biguity, e.g., minimal projection for dependents,lowest attachment position for dependents, and theselection of conversion rules that add fewer num-ber of nodes to the converted tree.
In addition, thevalidity of these heuristic rules often depends ontheir target grammars.
To eliminate the heuristicrules as required in previous methods, we proposeto use an existing target grammar parser (trainedon the target treebank) to generate N-best parsesfor each sentence in the source treebank as conver-sion candidates, and then select the parse consis-tent with the structure of the source tree as the con-verted tree.
Furthermore, we attempt to use con-verted trees as additional training data to retrainthe parser for better conversion candidates.
Theprocedure of tree conversion and parser retrainingwill be run iteratively until a stopping condition issatisfied.Since some converted trees might be imper-fect from the perspective of the target grammar,we provide two strategies to refine conversion re-sults: (1) pruning low-quality trees from the con-verted treebank, (2) interpolating the scores fromthe source grammar and the target grammar to se-lect better converted trees.
Finally we adopt a cor-pus weighting technique to get an optimal combi-nation of the converted treebank and the existingtarget treebank for parser training.We have evaluated our conversion algorithm ona dependency structure treebank (produced fromthe Penn Treebank) for comparison with previouswork (Xia et al, 2008).
We also have investi-gated our two-step solution on two existing tree-banks, the Penn Chinese Treebank (CTB) (Xue etal., 2005) and the Chinese Dependency Treebank(CDT)2 (Liu et al, 2006).
Evaluation on WSJ datademonstrates that it is feasible to use a parser forgrammar formalism conversion and the conversionbenefits from converted trees used for parser re-training.
Our conversion method achieves 93.8%f-score on dependency trees produced from WSJsection 22, resulting in 42% error reduction overthe previous best result for DS to PS conversion.Results on CTB show that score interpolation is2Available at http://ir.hit.edu.cn/.more effective than instance pruning for the useof converted treebanks for parsing and convertedCDT helps parsing on CTB.
When coupled withself-training technique, a reranking parser withCTB and converted CDT as labeled data achieves85.2% f-score on CTB test set, an absolute 1.0%improvement (6% error reduction) over the previ-ous best result for Chinese parsing.The rest of this paper is organized as follows.
InSection 2, we first describe a parser based methodfor DS to PS conversion, and then we discuss pos-sible strategies to refine conversion results, andfinally we adopt the corpus weighting techniquefor parsing on homogeneous treebanks.
Section3 provides experimental results of grammar for-malism conversion on a dependency treebank pro-duced from the Penn Treebank.
In Section 4, weevaluate our two-step solution on two existing het-erogeneous Chinese treebanks.
Section 5 reviewsrelated work and Section 6 concludes this work.2 Our Two-Step Solution2.1 Grammar Formalism ConversionPrevious DS to PS conversion methods built aconverted tree by iteratively attaching nodes andedges to the tree with the help of conversionrules and heuristic rules, based on current head-dependent pair from a source dependency tree andthe structure of the built tree (Collins et al, 1999;Covington, 1994; Xia and Palmer, 2001; Xia etal., 2008).
Some observations can be made onthese methods: (1) for each head-dependent pair,only one locally optimal conversion was kept dur-ing tree-building process, at the risk of pruningglobally optimal conversions, (2) heuristic rulesare required to deal with the problem that onehead-dependent pair might have multiple conver-sion candidates, and these heuristic rules are usu-ally hand-crafted to reflect the structural prefer-ence in their target grammars.
To overcome theselimitations, we propose to employ a parser to gen-erate N-best parses as conversion candidates andthen use the structural information of source treesto select the best parse as a converted tree.We formulate our conversion method as fol-lows.Let CDS be a source treebank annotated withDS and CPS be a target treebank annotated withPS.
Our goal is to convert the grammar formalismof CDS to that of CPS .We first train a constituency parser on CPS47Input: CPS , CDS , Q, and a constituency parser Output: Converted trees CDSPS1.
Initialize:?
Set CDS,0PS as null, DevScore=0, q=0;?
Split CPS into training set CPS,train and development set CPS,dev;?
Train the parser on CPS,train and denote it by Pq?1;2.
Repeat:?
Use Pq?1 to generate N-best PS parses for each sentence in CDS , and convert PS to DS for each parse;?
For each sentence in CDS Do?
t?=argmaxtScore(xi,t), and select the t?-th parse as a converted tree for this sentence;?
Let CDS,qPS represent these converted trees, and let Ctrain=CPS,train?CDS,qPS ;?
Train the parser on Ctrain, and denote the updated parser by Pq;?
Let DevScoreq be the f-score of Pq on CPS,dev;?
If DevScoreq > DevScore Then DevScore=DevScoreq, and CDSPS =CDS,qPS ;?
Else break;?
q++;Until q > QTable 1: Our algorithm for DS to PS conversion.
(90% trees in CPS as training set CPS,train, andother trees as development set CPS,dev) and thenlet the parser generate N-best parses for each sen-tence in CDS .Let n be the number of sentences (or trees) inCDS and ni be the number of N-best parses gen-erated by the parser for the i-th (1 ?
i ?
n) sen-tence in CDS .
Let xi,t be the t-th (1 ?
t ?
ni)parse for the i-th sentence.
Let yi be the tree of thei-th (1 ?
i ?
n) sentence in CDS .To evaluate the quality of xi,t as a conversioncandidate for yi, we convert xi,t to a dependencytree (denoted as xDSi,t ) and then use unlabeled de-pendency f-score to measure the similarity be-tween xDSi,t and yi.
Let Score(xi,t) denote theunlabeled dependency f-score of xDSi,t against yi.Then we determine the converted tree for yi bymaximizing Score(xi,t) over the N-best parses.The conversion from PS to DS works as fol-lows:Step 1.
Use a head percolation table to find thehead of each constituent in xi,t.Step 2.
Make the head of each non-head childdepend on the head of the head child for each con-stituent.Unlabeled dependency f-score is a harmonicmean of unlabeled dependency precision and unla-beled dependency recall.
Precision measures howmany head-dependent word pairs found in xDSi,tare correct and recall is the percentage of head-dependent word pairs defined in the gold-standardtree that are found in xDSi,t .
Here we do not takedependency tags into consideration for evaluationsince they cannot be obtained without more so-phisticated rules.To improve the quality of N-best parses, we at-tempt to use the converted trees as additional train-ing data to retrain the parser.
The procedure oftree conversion and parser retraining can be run it-eratively until a termination condition is satisfied.Here we use the parser?s f-score on CPS,dev as atermination criterion.
If the update of training datahurts the performance on CPS,dev, then we stopthe iteration.Table 1 shows this DS to PS conversion algo-rithm.
Q is an upper limit of the number of loops,and Q ?
0.2.2 Target Grammar ParsingThrough grammar formalism conversion, we havesuccessfully turned the problem of using hetero-geneous treebanks for parsing into the problem ofparsing on homogeneous treebanks.
Before usingconverted source treebank for parsing, we presenttwo strategies to refine conversion results.Instance Pruning For some sentences inCDS , the parser might fail to generate high qual-ity N-best parses, resulting in inferior convertedtrees.
To clean the converted treebank, we can re-move the converted trees with low unlabeled de-pendency f-scores (defined in Section 2.1) beforeusing the converted treebank for parser training48Figure 1: A parse tree in CTB for a sentence of/?.<world> ?<every> I<country> <?<people> ?<all> r<with> 81<eyes>?
?<cast> ?
l<Hong Kong>0with/People from all over the world are cast-ing their eyes on Hong Kong0as its Englishtranslation.because these trees are/misleading0training in-stances.
The number of removed trees will be de-termined by cross validation on development set.Score Interpolation Unlabeled dependencyf-scores used in Section 2.1 measure the quality ofconverted trees from the perspective of the sourcegrammar only.
In extreme cases, the top bestparses in the N-best list are good conversion can-didates but we might select a parse ranked quitelow in the N-best list since there might be con-flicts of syntactic structure definition between thesource grammar and the target grammar.Figure 1 shows an example for illustration ofa conflict between the grammar of CDT andthat of CTB.
According to Chinese head percola-tion tables used in the PS to DS conversion tool/Penn2Malt03 and Charniak?s parser4, the headof VP-2 is the word /r0(a preposition, with/BA0as its POS tag in CTB), and the head ofIP-OBJ is ??0.
Therefore the word /?
?0depends on the word/r0.
But accordingto the annotation scheme in CDT (Liu et al, 2006),the word/r0is a dependent of the word/??0.
The conflicts between the two grammarsmay lead to the problem that the selected parsesbased on the information of the source grammarmight not be preferred from the perspective of the3Available at http://w3.msi.vxu.se/?nivre/.4Available at http://www.cs.brown.edu/?ec/.target grammar.Therefore we modified the selection metric inSection 2.1 by interpolating two scores, the prob-ability of a conversion candidate from the parserand its unlabeled dependency f-score, shown asfollows:S?core(xi,t) = ??Prob(xi,t)+(1??)?Score(xi,t).
(1)The intuition behind this equation is that convertedtrees should be preferred from the perspective ofboth the source grammar and the target grammar.Here 0 ?
?
?
1.
Prob(xi,t) is a probability pro-duced by the parser for xi,t (0 ?
Prob(xi,t) ?
1).The value of ?
will be tuned by cross validation ondevelopment set.After grammar formalism conversion, the prob-lem now we face has been limited to how to buildparsing models on multiple homogeneous tree-bank.
A possible solution is to simply concate-nate the two treebanks as training data.
Howeverthis method may lead to a problem that if the sizeof CPS is significantly less than that of convertedCDS , converted CDS may weaken the effect CPSmight have.
One possible solution is to reduce theweight of examples from converted CDS in parsertraining.
Corpus weighting is exactly such an ap-proach, with the weight tuned on development set,that will be used for parsing on homogeneous tree-banks in this paper.3 Experiments of Grammar FormalismConversion3.1 Evaluation on WSJ section 22Xia et al (2008) used WSJ section 19 from thePenn Treebank to extract DS to PS conversionrules and then produced dependency trees fromWSJ section 22 for evaluation of their DS to PSconversion algorithm.
They showed that theirconversion algorithm outperformed existing meth-ods on the WSJ data.
For comparison with theirwork, we conducted experiments in the same set-ting as theirs: using WSJ section 19 (1844 sen-tences) as CPS , producing dependency trees fromWSJ section 22 (1700 sentences) as CDS5, andusing labeled bracketing f-scores from the tool/EVALB0on WSJ section 22 for performanceevaluation.5We used the tool/Penn2Malt0to produce dependencystructures from the Penn Treebank, which was also used forPS to DS conversion in our conversion algorithm.49All the sentencesDevScore LR LP FModels (%) (%) (%) (%)The best result ofXia et al (2008) - 90.7 88.1 89.4Q-0-method 86.8 92.2 92.8 92.5Q-10-method 88.0 93.4 94.1 93.8Table 2: Comparison with the work of Xia et al(2008) on WSJ section 22.All the sentencesDevScore LR LP FModels (%) (%) (%) (%)Q-0-method 91.0 91.6 92.5 92.1Q-10-method 91.6 93.1 94.1 93.6Table 3: Results of our algorithm on WSJ section2?18 and 20?22.We employed Charniak?s maximum entropy in-spired parser (Charniak, 2000) to generate N-best(N=200) parses.
Xia et al (2008) used POStag information, dependency structures and depen-dency tags in test set for conversion.
Similarly, weused POS tag information in the test set to restrictsearch space of the parser for generation of betterN-best parses.We evaluated two variants of our DS to PS con-version algorithm:Q-0-method: We set the value of Q as 0 for abaseline method.Q-10-method: We set the value of Q as 10 tosee whether it is helpful for conversion to retrainthe parser on converted trees.Table 2 shows the results of our conversion al-gorithm on WSJ section 22.
In the experimentof Q-10-method, DevScore reached the highestvalue of 88.0% when q was 1.
Then we usedCDS,1PS as the conversion result.
Finally Q-10-method achieved an f-score of 93.8% on WSJ sec-tion 22, an absolute 4.4% improvement (42% er-ror reduction) over the best result of Xia et al(2008).
Moreover, Q-10-method outperformed Q-0-method on the same test set.
These results indi-cate that it is feasible to use a parser for DS to PSconversion and the conversion benefits from theuse of converted trees for parser retraining.3.2 Evaluation on WSJ section 2?18 and20?22In this experiment we evaluated our conversion al-gorithm on a larger test set, WSJ section 2?18 and20?22 (totally 39688 sentences).
Here we alsoused WSJ section 19 as CPS .
Other settings forAll the sentencesLR LP FTraining data (%) (%) (%)1?
CTB + CDTPS 84.7 85.1 84.92?
CTB + CDTPS 85.1 85.6 85.35?
CTB + CDTPS 85.0 85.5 85.310?
CTB + CDTPS 85.3 85.8 85.620?
CTB + CDTPS 85.1 85.3 85.250?
CTB + CDTPS 84.9 85.3 85.1Table 4: Results of the generative parser on the de-velopment set, when trained with various weight-ing of CTB training set and CDTPS .this experiment are as same as that in Section 3.1,except that here we used a larger test set.Table 3 provides the f-scores of our method withQ equal to 0 or 10 on WSJ section 2?18 and20?22.With Q-10-method, DevScore reached the high-est value of 91.6% when q was 1.
Finally Q-10-method achieved an f-score of 93.6% on WSJsection 2?18 and 20?22, better than that of Q-0-method and comparable with that of Q-10-methodin Section 3.1.
It confirms our previous findingthat the conversion benefits from the use of con-verted trees for parser retraining.4 Experiments of ParsingWe investigated our two-step solution on two ex-isting treebanks, CDT and CTB, and we used CDTas the source treebank and CTB as the target tree-bank.CDT consists of 60k Chinese sentences, anno-tated with POS tag information and dependencystructure information (including 28 POS tags, and24 dependency tags) (Liu et al, 2006).
We did notuse POS tag information as inputs to the parser inour conversion method due to the difficulty of con-version from CDT POS tags to CTB POS tags.We used a standard split of CTB for perfor-mance evaluation, articles 1-270 and 400-1151 astraining set, articles 301-325 as development set,and articles 271-300 as test set.We used Charniak?s maximum entropy inspiredparser and their reranker (Charniak and Johnson,2005) for target grammar parsing, called a gener-ative parser (GP) and a reranking parser (RP) re-spectively.
We reported ParseVal measures fromthe EVALB tool.50All the sentencesLR LP FModels Training data (%) (%) (%)GP CTB 79.9 82.2 81.0RP CTB 82.0 84.6 83.3GP 10?
CTB + CDTPS 80.4 82.7 81.5RP 10?
CTB + CDTPS 82.8 84.7 83.8Table 5: Results of the generative parser (GP) andthe reranking parser (RP) on the test set, whentrained on only CTB training set or an optimalcombination of CTB training set and CDTPS .4.1 Results of a Baseline Method to Use CDTWe used our conversion algorithm6 to convert thegrammar formalism of CDT to that of CTB.
LetCDTPS denote the converted CDT by our method.The average unlabeled dependency f-score of treesin CDTPS was 74.4%, and their average index in200-best list was 48.We tried the corpus weighting method whencombining CDTPS with CTB training set (abbre-viated as CTB for simplicity) as training data, bygradually increasing the weight (including 1, 2, 5,10, 20, 50) of CTB to optimize parsing perfor-mance on the development set.
Table 4 presentsthe results of the generative parser with variousweights of CTB on the development set.
Consid-ering the performance on the development set, wedecided to give CTB a relative weight of 10.Finally we evaluated two parsing models, thegenerative parser and the reranking parser, on thetest set, with results shown in Table 5.
Whentrained on CTB only, the generative parser and thereranking parser achieved f-scores of 81.0% and83.3%.
The use of CDTPS as additional trainingdata increased f-scores of the two models to 81.5%and 83.8%.4.2 Results of Two Strategies for a Better Useof CDT4.2.1 Instance PruningWe used unlabeled dependency f-score of eachconverted tree as the criterion to rank trees inCDTPS and then kept only the top M treeswith high f-scores as training data for pars-ing, resulting in a corpus CDTPSM .
M var-ied from 100%?|CDTPS | to 10%?|CDTPS |with 10%?|CDTPS | as the interval.
|CDTPS |6The setting for our conversion algorithm in this experi-ment was as same as that in Section 3.1.
In addition, we usedCTB training set as CPS,train, and CTB development set asCPS,dev .All the sentencesLR LP FModels Training data (%) (%) (%)GP CTB + CDTPS?
81.4 82.8 82.1RP CTB + CDTPS?
83.0 85.4 84.2Table 6: Results of the generative parser and thereranking parser on the test set, when trained onan optimal combination of CTB training set andconverted CDT.is the number of trees in CDTPS .
Thenwe tuned the value of M by optimizing theparser?s performance on the development set with10?CTB+CDTPSM as training data.
Finally the op-timal value of M was 100%?|CDT|.
It indicatesthat even removing very few converted trees hurtsthe parsing performance.
A possible reason is thatmost of non-perfect parses can provide useful syn-tactic structure information for building parsingmodels.4.2.2 Score InterpolationWe used ?Score(xi,t)7 to replace Score(xi,t) inour conversion algorithm and then ran the updatedalgorithm on CDT.
Let CDTPS?
denote the con-verted CDT by this updated conversion algorithm.The values of ?
(varying from 0.0 to 1.0 with 0.1as the interval) and the CTB weight (including 1,2, 5, 10, 20, 50) were simultaneously tuned on thedevelopment set8.
Finally we decided that the op-timal value of ?
was 0.4 and the optimal weight ofCTB was 1, which brought the best performanceon the development set (an f-score of 86.1%).
Incomparison with the results in Section 4.1, theaverage index of converted trees in 200-best listincreased to 2, and their average unlabeled depen-dency f-score dropped to 65.4%.
It indicates thatstructures of converted trees become more consis-tent with the target grammar, as indicated by theincrease of average index of converted trees, fur-ther away from the source grammar.Table 6 provides f-scores of the generativeparser and the reranker on the test set, whentrained on CTB and CDTPS?
.
We see that theperformance of the reranking parser increased to7Before calculating S?core(xi,t), we normal-ized the values of Prob(xi,t) for each N-best listby (1) Prob(xi,t)=Prob(xi,t)-Min(Prob(xi,?)),(2)Prob(xi,t)=Prob(xi,t)/Max(Prob(xi,?
)), resultingin that their maximum value was 1 and their minimum valuewas 0.8Due to space constraint, we do not show f-scores of theparser with different values of ?
and the CTB weight.51All the sentencesLR LP FModels Training data (%) (%) (%)Self-trained GP 10?T+10?D+P 83.0 84.5 83.7Updated RP CTB+CDTPS?
84.3 86.1 85.2Table 7: Results of the self-trained gen-erative parser and updated reranking parseron the test set.
10?T+10?D+P stands for10?CTB+10?CDTPS?
+PDC.84.2% f-score, better than the result of the rerank-ing parser with CTB and CDTPS as training data(shown in Table 5).
It indicates that the use ofprobability information from the parser for treeconversion helps target grammar parsing.4.3 Using Unlabeled Data for ParsingRecent studies on parsing indicate that the use ofunlabeled data by self-training can help parsingon the WSJ data, even when labeled data is rel-atively large (McClosky et al, 2006a; Reichartand Rappoport, 2007).
It motivates us to em-ploy self-training technique for Chinese parsing.We used the POS tagged People Daily corpus9(Jan. 1998?Jun.
1998, and Jan. 2000?Dec.2000) (PDC) as unlabeled data for parsing.
Firstwe removed the sentences with less than 3 wordsor more than 40 words from PDC to ease pars-ing, resulting in 820k sentences.
Then we ran thereranking parser in Section 4.2.2 on PDC and usedthe parses on PDC as additional training data forthe generative parser.
Here we tried the corpusweighting technique for an optimal combinationof CTB, CDTPS?
and parsed PDC, and chose therelative weight of both CTB and CDTPS?
as 10by cross validation on the development set.
Fi-nally we retrained the generative parser on CTB,CDTPS?
and parsed PDC.
Furthermore, we usedthis self-trained generative parser as a base parserto retrain the reranker on CTB and CDTPS?
.Table 7 shows the performance of self-trainedgenerative parser and updated reranker on the testset, with CTB and CDTPS?
as labeled data.
We seethat the use of unlabeled data by self-training fur-ther increased the reranking parser?s performancefrom 84.2% to 85.2%.
Our results on Chinese dataconfirm previous findings on English data shownin (McClosky et al, 2006a; Reichart and Rap-poport, 2007).9Available at http://icl.pku.edu.cn/.4.4 Comparison with Previous Studies forChinese ParsingTable 8 and 9 present the results of previous stud-ies on CTB.
All the works in Table 8 used CTBarticles 1-270 as labeled data.
In Table 9, Petrovand Klein (2007) trained their model on CTB ar-ticles 1-270 and 400-1151, and Burkett and Klein(2008) used the same CTB articles and parse treesof their English translation (from the English Chi-nese Translation Treebank) as training data.
Com-paring our result in Table 6 with that of Petrovand Klein (2007), we see that CDTPS?
helps pars-ing on CTB, which brought 0.9% f-score improve-ment.
Moreover, the use of unlabeled data furtherboosted the parsing performance to 85.2%, an ab-solute 1.0% improvement over the previous bestresult presented in Burkett and Klein (2008).5 Related WorkRecently there have been some studies address-ing how to use treebanks with same grammar for-malism for domain adaptation of parsers.
Roarkand Bachiani (2003) presented count merging andmodel interpolation techniques for domain adap-tation of parsers.
They showed that their sys-tem with count merging achieved a higher perfor-mance when in-domain data was weighted moreheavily than out-of-domain data.
McClosky et al(2006b) used self-training and corpus weighting toadapt their parser trained on WSJ corpus to Browncorpus.
Their results indicated that both unla-beled in-domain data and labeled out-of-domaindata can help domain adaptation.
In comparisonwith these works, we conduct our study in a dif-ferent setting where we work with multiple het-erogeneous treebanks.Grammar formalism conversion makes it possi-ble to reuse existing source treebanks for the studyof target grammar parsing.
Wang et al (1994)employed a parser to help conversion of a tree-bank from a simple phrase structure to a more in-formative phrase structure and then used this con-verted treebank to train their parser.
Collins et al(1999) performed statistical constituency parsingof Czech on a treebank that was converted fromthe Prague Dependency Treebank under the guid-ance of conversion rules and heuristic rules, e.g.,one level of projection for any category, minimalprojection for any dependents, and fixed positionof attachment.
Xia and Palmer (2001) adopted bet-ter heuristic rules to build converted trees, which52?
40 words All the sentencesLR LP F LR LP FModels (%) (%) (%) (%) (%) (%)Bikel & Chiang (2000) 76.8 77.8 77.3 - - -Chiang & Bikel (2002) 78.8 81.1 79.9 - - -Levy & Manning (2003) 79.2 78.4 78.8 - - -Bikel?s thesis (2004) 78.0 81.2 79.6 - - -Xiong et.
al.
(2005) 78.7 80.1 79.4 - - -Chen et.
al.
(2005) 81.0 81.7 81.2 76.3 79.2 77.7Wang et.
al.
(2006) 79.2 81.1 80.1 76.2 78.0 77.1Table 8: Results of previous studies on CTB with CTB articles 1-270 as labeled data.?
40 words All the sentencesLR LP F LR LP FModels (%) (%) (%) (%) (%) (%)Petrov & Klein (2007) 85.7 86.9 86.3 81.9 84.8 83.3Burkett & Klein (2008) - - - - - 84.2Table 9: Results of previous studies on CTB with more labeled data.reflected the structural preference in their targetgrammar.
For acquisition of better conversionrules, Xia et al (2008) proposed to automati-cally extract conversion rules from a target tree-bank.
Moreover, they presented two strategies tosolve the problem that there might be multipleconversion rules matching the same input depen-dency tree pattern: (1) choosing the most frequentrules, (2) preferring rules that add fewer numberof nodes and attach the subtree lower.In comparison with the works of Wang et al(1994) and Collins et al (1999), we went fur-ther by combining the converted treebank with theexisting target treebank for parsing.
In compar-ison with previous conversion methods (Collinset al, 1999; Covington, 1994; Xia and Palmer,2001; Xia et al, 2008) in which for each head-dependent pair, only one locally optimal conver-sion was kept during tree-building process, weemployed a parser to generate globally optimalsyntactic structures, eliminating heuristic rules forconversion.
In addition, we used converted trees toretrain the parser for better conversion candidates,while Wang et al (1994) did not exploit the use ofconverted trees for parser retraining.6 ConclusionWe have proposed a two-step solution to deal withthe issue of using heterogeneous treebanks forparsing.
First we present a parser based methodto convert grammar formalisms of the treebanks tothe same one, without applying predefined heuris-tic rules, thus turning the original problem into theproblem of parsing on homogeneous treebanks.Then we present two strategies, instance pruningand score interpolation, to refine conversion re-sults.
Finally we adopt the corpus weighting tech-nique to combine the converted source treebankwith the existing target treebank for parser train-ing.The study on the WSJ data shows the benefits ofour parser based approach for grammar formalismconversion.
Moreover, experimental results on thePenn Chinese Treebank indicate that a converteddependency treebank helps constituency parsing,and it is better to exploit probability informationproduced by the parser through score interpolationthan to prune low quality trees for the use of theconverted treebank.Future work includes further investigation ofour conversion method for other pairs of grammarformalisms, e.g., from the grammar formalism ofthe Penn Treebank to more deep linguistic formal-ism like CCG, HPSG, or LFG.ReferencesAnne Abeille, Lionel Clement and Francois Toussenel.
2000.Building a Treebank for French.
In Proceedings of LREC2000, pages 87-94.Daniel Bikel and David Chiang.
2000.
Two Statistical Pars-ing Models Applied to the Chinese Treebank.
In Proceed-ings of the Second SIGHAN workshop, pages 1-6.Daniel Bikel.
2004.
On the Parameter Space of GenerativeLexicalized Statistical Parsing Models.
Ph.D. thesis, Uni-versity of Pennsylvania.Alena Bohmova, Jan Hajic, Eva Hajicova and BarboraVidova-Hladka.
2003.
The Prague Dependency Tree-bank: A Three-Level Annotation Scenario.
Treebanks:53Building and Using Annotated Corpora.
Kluwer Aca-demic Publishers, pages 103-127.Thorsten Brants, Wojciech Skut and Hans Uszkoreit.
1999.Syntactic Annotation of a German Newspaper Corpus.
InProceedings of the ATALA Treebank Workshop, pages 69-76.David Burkett and Dan Klein.
2008.
Two Languages areBetter than One (for Syntactic Parsing).
In Proceedings ofEMNLP 2008, pages 877-886.Eugene Charniak.
2000.
A Maximum Entropy InspiredParser.
In Proceedings of NAACL 2000, pages 132-139.Eugene Charniak and Mark Johnson.
2005.
Coarse-to-FineN-Best Parsing and MaxEnt Discriminative Reranking.
InProceedings of ACL 2005, pages 173-180.Ying Chen, Hongling Sun and Dan Jurafsky.
2005.
A Cor-rigendum to Sun and Jurafsky (2004) Shallow SemanticParsing of Chinese.
University of Colorado at BoulderCSLR Tech Report TR-CSLR-2005-01.David Chiang and Daniel M. Bikel.
2002.
Recovering La-tent Information in Treebanks.
In Proceedings of COL-ING 2002, pages 1-7.Micheal Collins, Lance Ramshaw, Jan Hajic and ChristophTillmann.
1999.
A Statistical Parser for Czech.
In Pro-ceedings of ACL 1999, pages 505-512.Micheal Covington.
1994.
GB Theory as DependencyGrammar.
Research Report AI-1992-03.Martin Forst.
2003.
Treebank Conversion - Establishinga Testsuite for a Broad-Coverage LFG from the TIGERTreebank.
In Proceedings of LINC at EACL 2003, pages25-32.Chunghye Han, Narae Han, Eonsuk Ko and Martha Palmer.2002.
Development and Evaluation of a Korean Treebankand its Application to NLP.
In Proceedings of LREC 2002,pages 1635-1642.Sadao Kurohashi and Makato Nagao.
1998.
Building aJapanese Parsed Corpus While Improving the Parsing Sys-tem.
In Proceedings of LREC 1998, pages 719-724.Roger Levy and Christopher Manning.
2003.
Is It Harder toParse Chinese, or the Chinese Treebank?
In Proceedingsof ACL 2003, pages 439-446.Ting Liu, Jinshan Ma and Sheng Li.
2006.
Building a Depen-dency Treebank for Improving Chinese Parser.
Journal ofChinese Language and Computing, 16(4):207-224.Mitchell P. Marcus, Beatrice Santorini and Mary AnnMarcinkiewicz.
1993.
Building a Large Annotated Cor-pus of English: The Penn Treebank.
Computational Lin-guistics, 19(2):313-330.David McClosky, Eugene Charniak and Mark Johnson.2006a.
Effective Self-Training for Parsing.
In Proceed-ings of NAACL 2006, pages 152-159.David McClosky, Eugene Charniak and Mark Johnson.2006b.
Reranking and Self-Training for Parser Adapta-tion.
In Proceedings of COLING/ACL 2006, pages 337-344.Antonio Moreno, Susana Lopez, Fernando Sanchez andRalph Grishman.
2003.
Developing a Syntactic Anno-tation Scheme and Tools for a Spanish Treebank.
Tree-banks: Building and Using Annotated Corpora.
KluwerAcademic Publishers, pages 149-163.Slav Petrov and Dan Klein.
2007.
Improved Inference forUnlexicalized Parsing.
In Proceedings of HLT/NAACL2007, pages 404-411.Roi Reichart and Ari Rappoport.
2007.
Self-Training for En-hancement and Domain Adaptation of Statistical ParsersTrained on Small Datasets.
In Proceedings of ACL 2007,pages 616-623.Brian Roark and Michiel Bacchiani.
2003.
Supervised andUnsupervised PCFG Adaptation to Novel Domains.
InProceedings of HLT/NAACL 2003, pages 126-133.Jong-Nae Wang, Jing-Shin Chang and Keh-Yih Su.
1994.An Automatic Treebank Conversion Algorithm for CorpusSharing.
In Proceedings of ACL 1994, pages 248-254.Mengqiu Wang, Kenji Sagae and Teruko Mitamura.
2006.
AFast, Accurate Deterministic Parser for Chinese.
In Pro-ceedings of COLING/ACL 2006, pages 425-432.Stephen Watkinson and Suresh Manandhar.
2001.
Translat-ing Treebank Annotation for Evaluation.
In Proceedingsof ACL Workshop on Evaluation Methodologies for Lan-guage and Dialogue Systems, pages 1-8.Fei Xia and Martha Palmer.
2001.
Converting DependencyStructures to Phrase Structures.
In Proceedings of HLT2001, pages 1-5.Fei Xia, Rajesh Bhatt, Owen Rambow, Martha Palmerand Dipti Misra.
Sharma.
2008.
Towards a Multi-Representational Treebank.
In Proceedings of the 7th In-ternational Workshop on Treebanks and Linguistic Theo-ries, pages 159-170.Deyi Xiong, Shuanglong Li, Qun Liu, Shouxun Lin andYueliang Qian.
2005.
Parsing the Penn Chinese Tree-bank with Semantic Knowledge.
In Proceedings of IJC-NLP 2005, pages 70-81.Nianwen Xue, Fei Xia, Fu-Dong Chiou and Martha Palmer.2005.
The Penn Chinese TreeBank: Phrase Structure An-notation of a Large Corpus.
Natural Language Engineer-ing, 11(2):207-238.54
