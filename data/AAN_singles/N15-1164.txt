Human Language Technologies: The 2015 Annual Conference of the North American Chapter of the ACL, pages 1428?1433,Denver, Colorado, May 31 ?
June 5, 2015.c?2015 Association for Computational LinguisticsEmbedding a Semantic Network in a Word SpaceRichard Johansson and Luis Nieto Pi?naSpr?akbanken, Department of Swedish, University of GothenburgBox 200, SE-40530 Gothenburg, Sweden{richard.johansson, luis.nieto.pina}@svenska.gu.seAbstractWe present a framework for using continuous-space vector representations of word meaningto derive new vectors representing the mean-ing of senses listed in a semantic network.
Itis a post-processing approach that can be ap-plied to several types of word vector represen-tations.
It uses two ideas: first, that vectors forpolysemous words can be decomposed intoa convex combination of sense vectors; sec-ondly, that the vector for a sense is kept sim-ilar to those of its neighbors in the network.This leads to a constrained optimization prob-lem, and we present an approximation for thecase when the distance function is the squaredEuclidean.We applied this algorithm on a Swedish se-mantic network, and we evaluate the qualityof the resulting sense representations extrinsi-cally by showing that they give large improve-ments when used in a classifier that createslexical units for FrameNet frames.1 IntroductionRepresenting word meaning computationally is cen-tral in natural language processing.
Manual,knowledge-based approaches to meaning represen-tation maps word strings to symbolic concepts,which can be described using any knowledge rep-resentation framework; using the relations betweenconcepts defined in the knowledge base, we can in-fer implicit facts from the information stated in atext: a mouse is a rodent, so it has prominent teeth.Conversely, data-driven meaning representationapproaches rely on cooccurrence patterns to derivea vector representation (Turney and Pantel, 2010).There are two classes of methods that compute wordvectors: context-counting and context-predicting;while the latter has seen much interest lately, theirrespective strengths and weaknesses are still beingdebated (Baroni et al, 2014; Levy and Goldberg,2014).
The most important relation defined in a vec-tor space between the meaning of two words is sim-ilarity: a mouse is something quite similar to a rat.Similarity of meaning is operationalized in terms ofgeometry, by defining a distance metric.Symbolic representations seem to have an advan-tage in describing word sense ambiguity: when asurface form corresponds to more than one concept.For instance, the word mouse can refer to a rodentor an electronic device.
Vector-space representationstypically represent surface forms only, which makesit hard to search e.g.
for a group of words similarto the rodent sense of mouse or to reliably use thevectors in classifiers that rely on the semantics ofthe word.
There have been several attempts to createvectors representing senses, most of them based onsome variant of the idea first proposed by Sch?utze(1998): that senses can be seen as clusters of similarcontexts.
Recent examples in this tradition includethe work by Huang et al (2012) and Neelakantan etal.
(2014).
However, because sense distributions areoften highly imbalanced, it is not clear that contextclusters can be reliably created for senses that occurrarely.
These approaches also lack interpretability:if we are interested in the rodent sense of mouse,which of the vectors should we use?In this work, we instead derive sense vectors byembedding the graph structure of a semantic net-work in the word space.
By combining two com-plementary sources of information ?
corpus statis-tics and network structure ?
we derive useful vec-tors also for concepts that occur rarely.
The method,which can be applied to context-counting as wellas context-predicting spaces, works by decompos-1428ing word vectors as linear combinations of sensevectors, and by pushing the sense vectors towardstheir neighbors in the semantic network.
This in-tuition leads to a constrained optimization problem,for which we present an approximate algorithm.We applied the algorithm to derive vectors forthe senses in a Swedish semantic network, and weevaluated their quality extrinsically by using themas features in a semantic classification task ?
map-ping senses to their corresponding FrameNet frames.When using the sense vectors in this task, we saw alarge improvement over using word vectors.2 Embedding a Semantic NetworkThe goal of the algorithm is to embed the seman-tic network in a geometric space: that is, to asso-ciate each sense sijwith a sense embedding, a vec-tor E(sij) of real numbers, in a way that reflects thetopology of the semantic network but also that thevectors representing the lemmas are related to thosecorresponding to the senses.
We now formalize thisintuition, and we start by introducing some notation.For each lemma li, there is a set of possible sensessi1, .
.
.
, simifor which liis a surface realization.Furthermore, for each sense sij, there is a neighbor-hood consisting of senses semantically related to sij.Each neighbor nijkof sijis associated with a weightwijkrepresenting the degree of semantic relatednessbetween sijand nijk.
How we define the neighbor-hood, i.e.
our notion of semantical relatedness, willobviously have an impact on the result.
In this work,we simply assume that it can be computed from thenetwork, e.g.
by picking a number of hypernymsand hyponyms in a lexicon such as WordNet.
Wethen assume that for each lemma li, we have a D-dimensional vector F (li) of real numbers; this canbe computed using any method described in Section1.
Finally, we assume a distance function ?
(x, y)that returns a non-negative real number for each pairof vectors in RD.The algorithm maps each sense sijto a sense em-bedding, a real-valued vector E(sij) in the samevector space as the lemma embeddings.
The lemmaand sense embeddings are related through a mix con-straint: F (li) is decomposed as a convex combi-nation?jpijE(sij), where the {pij} are pickedfrom the probability simplex.
Intuitively, the mixvariables correspond to the occurrence probabilitiesof the senses, but strictly speaking this is only thecase when the vectors are built using simple contextcounting.
Since the mix gives an estimate of whichsense is the most frequent in the corpus, we get astrong baseline for word sense disambiguation (Mc-Carthy et al, 2007) as a bonus; see our followuppaper (Johansson and Nieto Pi?na, 2015) for a dis-cussion of this.We can now formalize the intuition above: theweighted sum of distances between each sense andits neighbors is minimized, while satisfying the mixconstraint for each lemma.
We get the followingconstrained optimization program:minimizeE,p?i,j,kwijk?
(E(sij), E(nijk))subject to?jpijE(sij) = F (li) ?i?jpij= 1 ?ipij?
0 ?i, j(1)The mix constraints make sure that the solutionis nontrivial.
In particular, a very large numberof words are monosemous, and the procedure willleave the embeddings of these words unchanged.2.1 An Approximate AlgorithmThe difficulty of solving the problem stated in Equa-tion (1) obviously depends on the distance function?.
Henceforth, we focus on the case where ?
isthe squared Euclidean distance.
This is an impor-tant special case that is related to a number of otherdistances or similarities, e.g.
cosine similarity andHellinger distance.
In this case, (1) is a quadraticallyconstrained quadratic problem, which is NP-hard ingeneral and difficult to handle with off-the-shelf op-timization tools.
We therefore resort to an approx-imation; we show empirically in Sections 3 and 4that it works well in practice.The approximate algorithm works in an onlinefashion by considering one lemma at a time.
It ad-justs the embeddings of the senses as well as theirmix in order to minimize the loss functionLi=?jkwijk?E(sij)?
E(nijk)?2.
(2)1429The embeddings of the neighbors nijkof the senseare kept fixed at each such step.
We iterate throughthe whole set of lemmas for a fixed number ofepochs or until the objective is unchanged.Furthermore, instead of directly optimizing withrespect to the sense embeddings (which involvesmi?
D scalars), the sense embeddings (and there-fore also the loss Li) can be computed analytically ifthe mix variables pi1, .
.
.
, pimiare given, so we havereduced the optimization problem to one involvingmi?
1 scalars, i.e.
it is univariate in most cases.Given a sense sijof a lemma li, we define theweighted centroid of the set of neighbors of sijascij=?kwijkE(nijk)?kwijk.
(3)If the mix constraints were removed, cijwould bethe solution to the optimization problem: they min-imize the weighted sum of squared Euclidean dis-tances to the neighbors.
Then, given the mix, theresidual is definedri=1?jp2ij?kwijk???jpijcij?
F (li)??.
(4)The vector rirepresents the difference between thelinear combination of the weighted centroids and thelemma embedding.
Finally, we the sense embed-dings for the lemma libecomeE(sij) = cij?pij?kwijkri.
(5)Equations (4) and (5) show the role of the mix vari-ables: if pij= 0, then the sense embedding E(sij) iscompletely determined by the neighbors of the sense(that is, it is equal to the weighted centroid).
On theother hand, if pij= 1, then the sense embedding be-comes equal to the embedding of the lemma, F (li).To optimize the mix variables pi1, .
.
.
, pimiwith re-spect to the loss Li, basically any search procedurecould be used; we found it easiest to use a variantof a simple randomized gradient-free search method(Matyas, 1965).3 Application to Swedish DataThe algorithm described in Section 2 was applied toSwedish data: we started with lemma embeddingscomputed from a corpus, and then created sense em-beddings by using the SALDO semantic network(Borin et al, 2013).
The algorithm was run for afew epochs, which seemed to be enough for reach-ing a plateau in performance; the total runtime of thealgorithm was a few minutes.3.1 Creating Lemma EmbeddingsWe created a corpus of 1 billion words downloadedfrom Spr?akbanken, the Swedish language bank.1The corpora are distributed in a format where thetext has been tokenized, part-of-speech-tagged andlemmatized.
Compounds have been segmented au-tomatically and when a lemma was not listed inSALDO, we used the parts of the compounds in-stead.
The input to the software computing thelemma embedding consisted of lemma forms withconcatenated part-of-speech tags, e.g.
dricka..vbfor the verb ?to drink?
and dricka..nn for the noun?drink?.
We used the word2vec tool2to build thelemma embeddings.
All the default settings wereused, except the vector space dimensionality whichwas set to 512.3.2 SALDO, a Swedish Semantic NetworkSALDO (Borin et al, 2013) is the largest freelyavailable lexical resource for Swedish.
We used aversion from May 2014, which contains 125,781entries organized into a single semantic network.Compared to WordNet (Fellbaum, 1998), there aresimilarities but also significant differences.
Mostsignificantly, SALDO is organized according to thelexical-semantic relation of association, not is-a asin WordNet.
In SALDO, when we go up in thehierarchy we move from specialized vocabulary tothe most central vocabulary of the language (e.g.
?move?, ?want?, ?who?
); in WordNet we move fromspecific to abstract (e.g.
?entity?).
Another differ-ence is that sense distinctions in SALDO tend to bemore coarse-grained than in WordNet.Each entry except a special root is connected toother entries, its semantic descriptors.
One of the1http://spraakbanken.gu.se2https://code.google.com/p/word2vec1430semantic descriptors is called the primary descrip-tor: this is the semantic neighbor that is conceptu-ally most central.
Primary descriptors are most of-ten hypernyms or synonyms, but they can also bee.g.
antonyms or meronyms, or be in an argument?predicate relationship with the entry.
To exemplify,we consider the word rock, which has two senses:a long coat and rock music, respectively.
Its firstsense has the primary descriptor kappa ?coat?, whilefor the second sense it is musik ?music?.When embedding the SALDO network using thealgorithm in Section 2, the neighbors nijkof aSALDO sense sijare its primary descriptor andinverse primaries (the senses for which sijis theprimary descriptor).
We did not use any descrip-tors beside the primary.
The neighborhood weightswere set so that the primary descriptor and the setof inverse primaries were balanced, and so that allweights sum to 1.
If there were N inverse pri-maries, we set the weight of the primary descriptorto12and of each inverse primary to12N.
We did notsee any measurable effect of using a larger set ofneighbors (e.g.
adding connections to second-orderneighbors).3.3 Inspection of Sense EmbeddingsTable 1 shows a list of the nearest neighbors of thetwo senses of rock; as we can see, both lists makesense semantically.
(A similar query among thelemma embeddings returns a list almost identical tothat of the second sense.
)rock-1 ?coat?
rock-2 ?rock music?syrtut-1 ?overcoat?
punk-1 ?punk music?k?apa-2 ?cloak?
pop-1 ?pop music?kappa-1 ?coat?
soul-1 ?soul music?kavaj-1 ?blazer?
h?ardrock-1 ?hard rock?jacka-1 ?jacket?
hot-2 ?hot jazz?Table 1: The senses closest to the two senses of rock.A more difficult and interesting use case, wherecorpus-based methods clearly have an advantageover knowledge-based methods, is to search amongthe lemmas that have occurred in the corpus butwhich are not listed in SALDO.
Table 2 shows theresult of this search, and again the result is very use-ful.
We stress that the list for the first sense wouldhave been hard to derive in a standard word vectorspace, due to the dominance of the music sense.rock-1 ?coat?
rock-2 ?rock music?midjekort ?doublet?
indie ?indie?trekvarts?arm ?3/4 sleeve?
indierock ?indie rock?spetsbh ?lace bra?
doo-wop ?doo-wop?bl?ajeans ?blue jeans?
psykedelia ?psychedelia?treggings ?treggings?
R&B ?R&B?Table 2: The unlisted lemmas closest to the twosenses of rock.4 EvaluationEvaluating intrinsically using e.g.
a correlation be-tween a graph-based similarity measure and geo-metric similarity would be problematic, since this isin some sense what our algorithm optimizes.
Wetherefore evaluate extrinsically, by using the sensevectors in a classifier that maps senses to semanticclasses defined by FrameNet (Fillmore and Baker,2009).
FrameNet is a semantic database consistingof two parts: first, an ontology of semantic frames ?the classes ?
and secondly a lexicon that maps wordsenses to frames.
In standard FrameNet terminol-ogy, the senses assigned to a frame are called itslexical units (LUs).
Coverage is often a problem inframe-semantic lexicons, and this has a negative im-pact on the quality of NLP systems using FrameNet(Palmer and Sporleder, 2010).
The task of findingLUs for frames is thus a useful testbed for evaluat-ing lemma and sense vectors.To evaluate, we used 567 frames fromthe Swedish FrameNet (Friberg Heppin andToporowska Gronostaj, 2012); in total we had28,842 verb, noun, adjective, and adverb LUs,which we split into training (67%) and test sets(33%).
For each frame, we trained a SVM withLIBLINEAR (Fan et al, 2008), using the LUs in thatframe as positive training instances, and all otherLUs as negative instances.
Each LU was repre-sented as a vector: its lemma or sense embeddingnormalized to unit length.Table 3 shows the precision, recall, and F -measure for the classifiers for the five frames withmost LUs, and finally the micro-average over allframes.
In the overall evaluation as well as in four1431out of the five largest frames, the classifiers usingsense vectors clearly outperform those using lemmavectors.
The frame where we do not see any im-provement by introducing sense distinctions, PEO-PLE BY VOCATION, contains terms for professionssuch as painter and builder; since SALDO derivessuch terms from their corresponding verbs ratherthan from a common hypernym (e.g.
worker), theydo not form a coherent subnetwork in SALDO orsubregion in the embedding space.Frame P R FANIMALS 0.741 0.643 0.689FOOD 0.684 0.679 0.682PEOPLE BY VOCATION 0.595 0.651 0.622ORIGIN 0.789 0.691 0.737PEOPLE BY ORIGIN 0.693 0.481 0.568Overall 0.569 0.292 0.386(a) Using lemma embeddings.Frame P R FANIMALS 0.826 0.663 0.736FOOD 0.726 0.743 0.735PEOPLE BY VOCATION 0.605 0.637 0.621ORIGIN 0.813 0.684 0.742PEOPLE BY ORIGIN 0.756 0.508 0.608Overall 0.667 0.332 0.443(b) Using sense embeddings.Table 3: FrameNet lexical unit classification.5 ConclusionWe have presented a new method to embed a se-mantic network consisting of linked word sensesinto a continuous-vector word space; the methodis agnostic about whether the original word spacewas produced using a context-counting or context-predicting method.
Unlike previous approaches forcreating sense vectors, since we rely on the networkstructure, we can create representations for sensesthat occur rarely in corpora.
While the experimentsdescribed in this paper have been carried out usinga Swedish corpus and semantic network, the algo-rithm we have described is generally applicable andthe software3can be applied to other languages andsemantic networks.3http://demo.spraakdata.gu.se/richard/scouseThe algorithm takes word vectors and uses themand the network structure to induce the sense vec-tors.
It is based on two separate ideas: first, senseembeddings should preserve the structure of the se-mantic network as much as possible, so two sensesshould be close if they are neighbors in the graph;secondly, the word vectors are a probablistic mix ofsense vectors.
These two ideas are stated as an opti-mization problem where the first becomes the objec-tive and the second a constraint.
While this is hardto solve in the general case, we presented an approx-imation that can be applied when using the squaredEuclidean distance.We implemented the algorithm and used it to em-bed the senses of a Swedish semantic network intoa word space produced using the skip-gram model.While a qualitative inspection of nearest-neighborlists of a few senses gives very appealing results, ourmain evaluation was extrinsic: a FrameNet lexicalunit classifier saw a large performance boost whenusing sense vectors instead of word vectors.In a followup paper (Johansson and Nieto Pi?na,2015), we have shown that sense embeddings can beused to build an efficient word sense disambiguationsystem that is much faster than graph-based systemswith a similar accuracy, and that the mix variablescan be used to predict the predominant sense of aword.
In future work, we plan to investigate whetherthe sense vectors are useful for retrieving rarely oc-curring senses in corpora.
Furthermore, since wenow evaluated extrinsically, it would be useful to de-vise intrinsic sense-based evaluation schemes, e.g.
asense analogy task similar to the word analogy taskused by Mikolov et al (2013).AcknowledgmentsThis research was funded by the Swedish Re-search Council under grant 2013?4944, Distribu-tional methods to represent the meaning of framesand constructions, and grant 2012?5738, Towards aknowledge-based culturomics.
The evaluation mate-rial used in this work was developed in the projectSwedish FrameNet++, grant 2010?6013.
We alsoacknowledge the University of Gothenburg for itssupport of the Centre for Language Technology andSpr?akbanken.1432ReferencesMarco Baroni, Georgiana Dinu, and Germ?an Kruszewski.2014.
Don?t count, predict!
a systematic compari-son of context-counting vs. context-predicting seman-tic vectors.
In Proceedings of the 52nd Annual Meet-ing of the Association for Computational Linguistics(Volume 1: Long Papers), pages 238?247, Baltimore,United States.Lars Borin, Markus Forsberg, and Lennart L?onngren.2013.
SALDO: a touch of yin to WordNet?s yang.Language Resources and Evaluation, 47(4):1191?1211.Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, Xiang-RuiWang, and Chih-Jen Lin.
2008.
LIBLINEAR: A li-brary for large linear classification.
Journal of Ma-chine Learning Research, 9:1871?1874.Christiane Fellbaum, editor.
1998.
WordNet: An elec-tronic lexical database.
MIT Press.Charles J. Fillmore and Collin Baker.
2009.
A frames ap-proach to semantic analysis.
In B. Heine and H. Nar-rog, editors, The Oxford Handbook of Linguistic Anal-ysis, pages 313?340.
Oxford: OUP.Karin Friberg Heppin and Maria Toporowska Gronostaj.2012.
The rocky road towards a Swedish FrameNet ?creating SweFN.
In Proceedings of the Eighth confer-ence on International Language Resources and Evalu-ation (LREC-2012), pages 256?261, Istanbul, Turkey.Eric H. Huang, Richard Socher, Christopher D. Manning,and Andrew Y. Ng.
2012.
Improving word repre-sentations via global context and multiple word pro-totypes.
In Association for Computational Linguistics2012 Conference (ACL 2012), pages 41?48, Boston,United States.Richard Johansson and Luis Nieto Pi?na.
2015.
Combin-ing relational and distributional knowledge for wordsense disambiguation.
In Proceedings of the 20thNordic Conference of Computational Linguistics, Vil-nius, Lithuania.Omer Levy and Yoav Goldberg.
2014.
Linguistic regu-larities in sparse and explicit word representations.
InProceedings of the Eighteenth Conference on Compu-tational Natural Language Learning, pages 171?180,Ann Arbor, United States.J.
Matyas.
1965.
Random optimization.
Automation andRemote Control, 26(2):246?253.Diana McCarthy, Rob Koeling, Julie Weeds, and JohnCarroll.
2007.
Unsupervised acquisition of pre-dominant word senses.
Computational Linguistics,33(4):553?590.Tom?a?s Mikolov, Wen-tau Yih, and Geoffrey Zweig.2013.
Linguistic regularities in continuous space wordrepresentations.
In Proceedings of the 2013 Confer-ence of the North American Chapter of the Associa-tion for Computational Linguistics: Human LanguageTechnologies, pages 746?751, Atlanta, USA.Arvind Neelakantan, Jeevan Shankar, Alexandre Pas-sos, and Andrew McCallum.
2014.
Efficient non-parametric estimation of multiple embeddings perword in vector space.
In Proceedings of the 2014 Con-ference on Empirical Methods in Natural LanguageProcessing (EMNLP), pages 1059?1069, Doha, Qatar,October.Alexis Palmer and Caroline Sporleder.
2010.
EvaluatingFrameNet-style semantic parsing: the role of coveragegaps in FrameNet.
In Coling 2010: Posters, pages928?936, Beijing, China.Hinrich Sch?utze.
1998.
Automatic word sense discrimi-nation.
Computational Linguistics, 24(1):97?123.Peter D. Turney and Patrick Pantel.
2010.
From fre-quency to meaning: Vector space models of semantics.Journal of Artificial Intelligence Research, 37:141?188.1433
