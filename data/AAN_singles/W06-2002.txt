A Framework for Incorporating Alignment Information in ParsingMark HopkinsDept.
of Computational LinguisticsSaarland UniversitySaarbru?cken, Germanymhopkins@coli.uni-sb.deJonas KuhnDept.
of Computational LinguisticsSaarland UniversitySaarbru?cken, Germanyjonask@coli.uni-sb.deAbstractThe standard PCFG approach to parsing isquite successful on certain domains, but isrelatively inflexible in the type of featureinformation we can include in its prob-abilistic model.
In this work, we dis-cuss preliminary work in developing a newprobabilistic parsing model that allows usto easily incorporate many different typesof features, including crosslingual infor-mation.
We show how this model canbe used to build a successful parser for asmall handmade gold-standard corpus of188 sentences (in 3 languages) from theEuroparl corpus.1 IntroductionMuch of the current research into probabilis-tic parsing is founded on probabilistic context-free grammars (PCFGs) (Collins, 1999; Charniak,2000; Charniak, 2001).
For instance, considerthe parse tree in Figure 1.
One way to decom-pose this parse tree is to view it as a sequenceof applications of CFG rules.
For this particulartree, we could view it as the application of rule?NP?
NP PP,?
followed by rule ?NP?
DT NN,?followed by rule ?DT?
that,?
and so forth.
Henceinstead of analyzing P (tree), we deal with themore modular:P(NP ?
NP PP, NP ?
DT NN,DT?
that, NN?money, PP?
IN NP,IN ?
in, NP ?
DT NN, DT ?
the,NN?
market)Obviously this joint distribution is just as diffi-cult to assess and compute with as P (tree).
How-ever there exist cubic time algorithms to find themost likely parse if we assume that all CFG ruleapplications are marginally independent of one an-other.
In other words, we need to assume that theabove expression is equivalent to the following:P(NP ?
NP PP) ?
P(NP ?
DT NN) ?P(DT ?
that) ?
P(NN ?
money) ?P(PP ?
IN NP) ?
P(IN ?
in) ?P(NP ?
DT NN) ?
P(DT ?
the) ?P(NN?
market)It is straightforward to assess the probability ofthe factors of this expression from a corpus us-ing relative frequency.
Then using these learnedprobabilities, we can find the most likely parse ofa given sentence using the aforementioned cubicalgorithms.The problem, of course, with this simplifica-tion is that although it is computationally attrac-tive, it is usually too strong of an independenceassumption.
To mitigate this loss of context, with-out sacrificing algorithmic tractability, typicallyresearchers annotate the nodes of the parse treewith contextual information.
For instance, it hasbeen found to be useful to annotate nodes withtheir parent labels (Johnson, 1998), as shown inFigure 2.
In this case, we would be learning prob-abilities like: P(PP-NP?
IN-PP NP-PP).The choice of which annotations to use isone of the main features that distinguish parsersbased on this approach.
Generally, this approachhas proven quite effective in producing Englishphrase-structure grammar parsers that performwell on the Penn Treebank.One drawback of this approach is that it issomewhat inflexible.
Because we are adding prob-abilistic context by changing the data itself, wemake our data increasingly sparse as we add fea-tures.
Thus we are constrained from adding too9NPNPDTthatNNmoneyPPINinNPDTtheNNmarketFigure 1: Example parse tree.NP-TOPNP-NPDT-NPthatNN-NPmoneyPP-NPIN-PPinNP-PPDT-NPtheNN-NPmarketFigure 2: Example parse tree with parent annota-tions.many features, because at some point we will nothave enough data to sustain them.
Hence in thisapproach, feature selection is not merely a matterof including good features.
Rather, we must strikea delicate balance between how much context wewant to include versus how much we dare to par-tition our data set.This poses a problem when we have spent timeand energy to find a good set of features that workwell for a given parsing task on a given domain.For a different parsing task or domain, our parsermay work poorly out-of-the-box, and it is no triv-ial matter to evaluate how we might adapt our fea-ture set for this new task.
Furthermore, if we gainaccess to a new source of feature information, thenit is unclear how to incorporate such informationinto such a parser.Namely, in this paper, we are interested in see-ing how the cross-lingual information containedby sentence alignments can help the performanceof a parser.
We have a small gold-standard cor-pus of shallow-parsed parallel sentences (in En-glish, French, and German) from the Europarl cor-pus.
Because of the difficulty of testing new fea-tures using PCFG-based parsers, we propose anew probabilistic parsing framework that allowsus to flexibly add features.
The closest relative1 2 3 4 51 true true false false true2 - true false false false3 - - true false true4 - - - true true5 - - - - trueFigure 3: Span chart for example parse tree.
Chartentry (i, j) = true iff span (i, j) is a constituentin the tree.of our framework is the maximum-entropy parserof Ratnaparkhi(Ratnaparkhi, 1997).
Both frame-works are bottom-up, but while Ratnaparkhi?sviews parse trees as the sequence of applicationsof four different types of tree construction rules,our framework strives to be somewhat simpler andmore general.2 The Probability ModelThe example parse tree in Figure 1 can also be de-composed in the following manner.
First, we canrepresent the unlabeled tree with a boolean-valuedchart (which we will call the span chart) that as-signs the value of true to a span if it is a con-stituent in the tree, and false otherwise.
The spanchart for Figure 1 is shown in Figure 3.To represent the labels, we simply add similarcharts for each labeling scheme present in the tree.For a parse tree, there are typically three typesof labels: words, preterminal tags, and nontermi-nals.
Thus we need three labeling charts.
Labelingcharts for our example parse tree are depicted inFigure 4.
Note that for words and preterminals, itis not really necessary to have a two-dimensionalchart, but we do so here to motivate the generalmodel.The general model is as follows.
Define a la-beling scheme as a set of symbols including aspecial symbol null (this will designate that agiven span is unlabeled).
For instance, we mightdefine LNT = {null, NP, PP, IN, DT} to bea labeling scheme for non-terminals.
Let L ={L1, L2, ...Lm} be a set of labeling schemes.
De-fine a model variable of L as a symbol of the formSij or Lkij , for positive integers i, j, k, such thatj ?
i and k ?
m. The domain of model vari-able Sij is {true, false} (these variables indicatewhether a given span is a tree constituent).
The do-main of model variable Lkij is Lk (these variablesindicate which label from Lk is assigned to span101 2 3 4 51 that null null null null2 - money null null null3 - - in null null4 - - - the null5 - - - - market1 2 3 4 51 DT null null null null2 - NN null null null3 - - IN null null4 - - - DT null5 - - - - NN1 2 3 4 51 null NP null null NP2 - null null null null3 - - null null PP4 - - - null NP5 - - - - nullFigure 4: Labeling charts for example parse tree:the top chart is for word labels, the middle chart isfor preterminal tag labels, and the bottom chart isfor nonterminal labels.
null denotes an unlabeledspan.i, j).
Define a model order of L as a total order-ing ?
of the model variables of L such that for alli, j, k: ?
(Sij) < ?
(Lkij) (i.e.
we decide whether aspan is a constituent before attempting to label it).Let ?n denote the finite subset of ?
that includesprecisely the model variables of the form Sij orLkij , where j ?
n.Given a set L of labeling schemes and a modelorder ?
of L, a preliminary generative story mightlook like the following:1.
Choose a positive integer n.2.
In the order defined by ?n, assign a valueto every model variable of ?n from its do-main, conditioned on any previous assign-ments made.Thus some model order ?
for our examplemight instruct us to first choose whether span (4,5) is a constituent, for which we might say ?true,?then instruct us to choose a label for that con-stituent, for which we might say ?NP,?
and soforth.There are a couple of problems with this genera-tive story.
One problem is that it allows us to makestructural decisions that do not result in a well-formed tree.
For instance, we should not be per-mitted to assign the value true to both variable S13and S24.
Generally, we cannot allow two modelvariables Sij and Skl to both be assigned true ifthey properly overlap, i.e.
their spans overlap andone is not a subspan of the other.
We should alsoensure that the leaves and the root are consideredconstituents.
Another problem is that it allows usto make labeling decisions that do not correspondwith our chosen structure.
It should not be possi-ble to label a span which is not a constituent.With this in mind, we revise our generativestory.1.
Choose a positive integer n from distributionP0.2.
In the order defined by ?n, process modelvariable x of ?n:(a) If x = Sij , then:i.
Automatically assign the valuefalse if there exists a properlyoverlapping model variable Skl suchthat Skl has already been assignedthe value true.ii.
Automatically assign the value trueif i = j or if i = 1 and j = n.iii.
Otherwise assign a value sij to Sijfrom its domain, drawn from someprobability distribution PS condi-tioned on all previous variable as-signments.
(b) If x = Lkij , then:i.
Automatically assign the value nullto Lkij if Sij was assigned the valuefalse (note that this is well-definedbecause of way we defined modelorder).ii.
Otherwise assign a value lkij to Lkijfrom its domain, drawn from someprobability distribution Pk condi-tioned on all previous variable as-signments.Defining ?<n (x) = {y ?
?n|?
(y) < ?
(x)}for x ?
?n, we can decompose P (tree) into thefollowing expression:11P0(n) ??Sij?
?nPS(sij |n,?<n (Sij))??Lkij?
?nPk(lkij |n,?<n (Lkij))where PS and Pk obey the constraints given inthe generative story above (e.g.
PS(Sii = true) =1, etc.
)Obviously it is impractical to learn conditionaldistributions over every conceivable history, so in-stead we choose a small set F of feature variables,and provide a set of functions Fn that map everypartial history of ?n to some feature vector f ?
F(later we will see examples of such feature func-tions).
Then we make the assumption that:PS(sij |n,?<n (Sij) = PS(sij|f)where f = Fn(?<n (Sij)) and thatPk(lkij |n,?<n (Sij) = Pk(lkij |f)where f = Fn(?<n (Lkij)).In this way, our learning task is simplified tolearn functions P0(n), PS(sij |f), and Pk(lkij |f).Given a corpus of labeled trees, it is straightfor-ward to extract the training instances for these dis-tributions and then use these instances to learn dis-tributions using one?s preferred learning method(e.g., maximum entropy models or decision trees).For this paper, we are interested in parse treeswhich have three labeling schemes.
Let L ={Lword, LPT , LNT }, where Lword is a labelingscheme for words, LPT is a labeling scheme forpreterminals, and LNT is a labeling scheme fornonterminals.
We will define model order ?
suchthat:1.
?
(Sij) < ?
(Lwordij ) < ?
(LPTij ) < ?
(LNTij ).2.
?
(LNTij ) < ?
(Skl) iff j?i < l?k or (j?i =l ?
k and i < k).In this work, we are not as much interested inlearning a marginal distribution over parse trees,but rather a conditional distribution for parse trees,given a tagged sentence (from which n is alsoknown).
We will assume that Pword is condition-ally independent of all the other model variables,given n and the Lwordij variables.
We will also as-sume that Ppt is conditionally independent of theother model variables, given n, the Lwordij vari-ables, and the Lptij variables.
These assumptionsallow us to express P (tree|n, Lwordij , Lptij ) as thefollowing:?Sij?
?nPS(sij |fS) ??Lntij?
?nPnt(lntij |fnt)where fS = Fn(?<n (Sij)) and fnt =Fn(?<n (Lntij )).
Hence our learning task in this pa-per will be to learn the probability distributionsPS(sij|fS) and Pnt(lntij |fnt), for some choice offeature functions Fn.3 DecodingFor the PCFG parsing model, we can findargmaxtreeP (tree|sentence) using a cubic-timedynamic programming-based algorithm.
Byadopting a more flexible probabilistic model, wesacrifice polynomial-time guarantees.
Neverthe-less, we can still devise search algorithms thatwork efficiently in practice.
For the decoding ofthe probabilistic model of the previous section, wechoose a depth-first branch-and-bound approach,specifically because of two advantages.
First, thisapproach is linear space.
Second, it is anytime, i.e.it finds a (typically good) solution early and im-proves this solution as the search progresses.
Thusif one does not wish the spend the time to run thesearch to completion (and ensure optimality), onecan use this algorithm easily as a heuristic.The search space is simple to define.
Given aset L of labeling schemes and a model order ?
ofL, the search algorithm simply makes assignmentsto the model variables (depth-first) in the order de-fined by ?.This search space can clearly grow to be quitelarge, however in practice the search speed isimproved drastically by using branch-and-boundbacktracking.
Namely, at any choice point in thesearch space, we first choose the least cost child toexpand.
In this way, we quickly obtain a greedysolution.
After that point, we can continue to keeptrack of the best solution we have found so far,and if at any point we reach an internal node ofour search tree with partial cost greater than thetotal cost of our best solution, we can discard thisnode and discontinue exploration of that subtree.This technique can result in a significant aggre-grate savings of computation time, depending on12EN: [1[2On behalf of the European People ?s Party , ] [3I] call [5for a vote [6in favour of that motion ] ] ]FR: [1[2Au nom du Parti populaire europe?en ,] [3je] demande [5l?
adoption [6de cette re?solution] ] ]DE: [1[2Im Namen der Europa?ischen Volkspartei ] rufe [3ich] [4Sie] auf , [5[6diesem Entschlie?ungsantrag] zuzustimmen] ]ES: [1[2En nombre del Grupo del Partido Popular Europeo ,] solicito [5la aprobacio?n [6de la resolucio?n] ] ]Figure 5: Annotated sentence tuplethe nature of the cost function.
For our limitedparsing domain, it appears to perform quite well,taking fractions of a second to parse each sentence(which are short, with a maximum of 20 words persentence).4 ExperimentsOur parsing domain is based on a ?lean?
phrasecorrespondence representation for multitexts fromparallel corpora (i.e., tuples of sentences that aretranslations of each other).
We defined an anno-tation scheme that focuses on translational corre-spondence of phrasal units that have a distinct,language-independent semantic status.
It is a hy-pothesis of our longer-term project that such a se-mantically motivated, relatively coarse phrase cor-respondence relation is most suitable for weaklysupervised approaches to parsing of large amountsof parallel corpus data.
Based on this lean phrasestructure format, we intend to explore an alter-native to the annotation projection approach tocross-linguistic bootstrapping of parsers by (Hwaet al, 2005).
They depart from a standard treebankparser for English, ?projecting?
its analyses to an-other language using word alignments over a par-allel corpus.
Our planned bootstrapping approachwill not start out with a given parser for English (orany other language), but use a small set of manu-ally annotated seed data following the lean phrasecorrespondence scheme, and then bootstrap con-sensus representations on large amounts of unan-notated multitext data.
At the present stage, weonly present experiments for training an initialsystem on a set of seed data.The annotation scheme underlying in the goldstandard annotation consists of (A) a bracketingfor each language and (B) a correspondence rela-tion of the constituents across languages.
Neitherthe constituents nor the embedding or correspon-dent relations were labelled.The guiding principle for bracketing (A) is verysimple: all and only the units that clearly playthe role of a semantic argument or modifier in alarger unit are bracketed.
This means that functionwords, light verbs, ?bleeched?
PPs like in spiteof etc.
are included with the content-bearing el-ements.
This leads to a relatively flat bracketingstructure.
Referring or quantified expressions thatmay include adjectives and possessive NPs or PPsare also bracketed as single constituents (e.g., [ thepresident of France ]), unless the semantic rela-tions reflected by the internal embedding are partof the predication of the sentence.
A few morespecific annotation rules were specified for caseslike coordination and discontinuous constituents.The correspondence relation (B) is guided bysemantic correspondence of the bracketed units;the mapping need not preserve the tree structure.Neither does a constituent need to have a corre-spondent in all (or any) of the other languages(since the content of this constituent may be im-plicit in other languages, or subsumed by the con-tent of another constituent).
?Semantic correspon-dence?
is not restricted to truth-conditional equiv-alence, but is generalized to situations where twounits just serve the same rhetorical function in theoriginal text and the translation.Figure 5 is an annotation example.
Note thatindex 4 (the audience addressed by the speaker)is realized overtly only in German (Sie ?you?
); inSpanish, index 3 is realized only in the verbal in-flection (which is not annotated).
A more detaileddiscussion of the annotation scheme is presentedin (Kuhn and Jellinghaus, to appear).For the current parsing experiments, only thebracketing within each of three languages (En-glish, French, German) is used; the cross-linguistic phrase correspondences are ignored (al-though we intend to include them in future ex-periments).
We automatically tagged the train-ing and test data in English, French, and Germanwith Schmid?s decision-tree part-of-speech tagger(Schmid, 1994).The training data were taken from the sentence-aligned Europarl corpus and consisted of 188 sen-tences for each of the three languages, with max-13Feature Notation Descriptionp(language) the preterminal tag of word x ?
1 (null if does not exist)f(language) the preterminal tag of word xl(language) the preterminal tag of word yn(language) the preterminal tag of word y ?
1 (null if does not exist)lng the length of the span (i.e.
y ?
x + 1)Figure 6: Features for span (x, y).
E = English, F = French, G = GermanEnglish Crosslingual Rec.
Prec.
F- Nofeatures features score crossp(E), f(E), l(E) none 40.3 63.6 49.4 (?3.9%) 57.1p(F), f(F), l(F) 43.1 67.6 52.6 (?4.0%) 61.2p(G), f(G), l(G) 45.9 66.8 54.4 (?4.0%) 69.4p(F), f(F), l(F), 44.5 65.5 53.0 (?3.9%) 65.3p(G), f(G), l(G)p(E), f(E), l(E), n(E) none 57.2 68.6 62.4 (?4.0%) 65.3p(F), f(F), l(F), n(F) 56.6 71.9 63.3 (?4.0%) 75.5p(G), f(G), l(G), n(G) 57.9 67.7 62.5 (?3.9%) 67.3p(F), f(F), l(F), n(F), 57.9 72.1 64.2 (?4.0%) 77.6p(G), f(G), l(G), n(G)p(E), f(E), l(E), n(E), lng none 64.8 71.2 67.9 (?4.0%) 79.6p(F), f(F), l(F), n(F), lng 62.1 74.4 67.7 (?4.0%) 83.7p(G), f(G), l(G), n(G), lng 61.4 78.8 69.0 (?4.1%) 83.7p(F), f(F), l(F), n(F), 63.1 76.9 69.3 (?4.1%) 81.6p(G), f(G), l(G), n(G), lngBIKEL 57.9 60.2 59.1 (?3.8%) 57.1Figure 7: Parsing results for various feature sets, and the Bikel baseline.
The F-scores are annotated with95% confidence intervals.imal length of 21 words in English (French: 38;German: 24) and an average length of 14.0 wordsin English (French 16.8; German 13.6).
The testdata were 50 sentences for each language, pickedarbitrarily with the same length restrictions.
Thetraining and test data were manually aligned fol-lowing the guidelines.1For the word alignments used as learning fea-tures, we used GIZA++, relying on the default pa-rameters.
We trained the alignments on the fullEuroparl corpus for both directions of each lan-guage pair.As a baseline system we trained Bikel?s reim-plementation (Bikel, 2004) of Collins?
parser(Collins, 1999) on the gold standard (En-1A subset of 39 sentences was annotated by two peopleindependently, leading to an F-Score in bracketing agreementbetween 84 and 90 for the three languages.
Since finding anannotation scheme that works well in the bootstrapping set-up is an issue on our research agenda, we postpone a moredetailed analysis of the annotation process until it becomesclear that a particular scheme is indeed useful.glish) training data, applying a simple additionalsmoothing procedure for the modifier events in or-der to counteract some obvious data sparseness is-sues.2Since we were attempting to learn unlabeledtrees, in this experiment we only needed to learnthe probabilistic model of Section 3 with no la-beling schemes.
Hence we need only to learn theprobability distribution:PS(sij|fS)In other words, we need to learn the probabil-ity that a given span is a tree constituent, givensome set of features of the words and preterminaltags of the sentences, as well as the previous spandecisions we have made.
The main decision that2For the nonterminal labels, we defined the left-most lex-ical daughter in each local subtree of depth 1 to project itspart-of-speech category to the phrase level and introduceda special nonterminal label for the rare case of nonterminalnodes dominating no preterminal node.14remains, then, is which feature set to use.
The fea-tures we employ are very simple.
Namely, for span(i, j) we consider the preterminal tags of wordsi ?
1, i, j, and j + 1, as well as the French andGerman preterminal tags of the words to whichthese English words align.
Finally, we also usethe length of the span as a feature.
The featuresconsidered are summarized in Figure 6.To learn the conditional probability distributu-tions, we choose to use maximum entropy mod-els because of their popularity and the availabil-ity of software packages.
Specifically, we usethe MEGAM package (Daume?
III, 2004) fromUSC/ISI.We did experiments for a number of differentfeature sets, with and without alignment features.The results (precision, recall, F-score, and the per-centage of sentences with no cross-bracketing) aresummarized in Figure 7.
Note that with a verysimple set of features (the previous, first, last, andnext preterminal tags of the sequence), our parserperforms on par with the Bikel baseline.
Addingthe length of the sequence as a feature increasesthe quality of the parser to a statistically signif-icant difference over the baseline.
The crosslin-gual information provided (which is admittedlynaive) does not provide a statistically significantimprovement over the vanilla set of features.
Theconclusion to be drawn is not that crosslingual in-formation does not help (such a conclusion shouldnot be drawn from the meager set of crosslingualfeatures we have used here for demonstration pur-poses).
Rather, the take-away point is that suchinformation can be easily incorporated using thisframework.5 DiscussionOne of the primary concerns about this frameworkis speed, since the decoding algorithm for ourprobabilistic model is not polynomial-time like thedecoding algorithms for PCFG parsing.
Neverthe-less, in our experiments with shallow parsed 20-word sentences, time was not a factor.
Further-more, in our ongoing research applying this prob-abilistic framework to the task of Penn Treebank-style parsing, this approach appears to also be vi-able for the 40-word sentences of Sections 22 and23 of theWSJ treebank.
A strong mitigating factorof the theoretical intractibility is the fact that wehave an anytime decoding algorithm, hence evenin cases when we cannot run the algorithm to com-pletion (for a guaranteed optimal solution), the al-gorithm always returns some solution, the qualityof which increases over time.
Hence we can tellthe algorithm how much time it has to compute,and it will return the best solution it can computein that time frame.This work suggests that one can get a good qual-ity parser for a new parsing domain with relativelylittle effort (the features we chose are extremelysimple and certainly could be improved on).
Thecross-lingual information that we used (namely,the foreign preterminal tags of the words to whichour span was aligned by GIZA) did not give a sig-nificant improvement to our parser.
However thegoal of this work was not to make definitive state-ments about the value of crosslingual features inparsing, but rather to show a framework in whichsuch crosslingual information could be easily in-corporated and exploited.
We believe we have pro-vided the beginnings of one in this work, and workcontinues on finding more complex features thatwill improve performance well beyond the base-line.AcknowledgementThe work reported in this paper was supported bytheDeutsche Forschungsgemeinschaft (DFG; Ger-man Research Foundation) in the Emmy Noetherproject PTOLEMAIOS on grammar learning fromparallel corpora.ReferencesDaniel M. Bikel.
2004.
Intricacies of collins?
parsingmodel.
Computational Linguistics, 30(4):479?511.Eugene Charniak.
2000.
A maximum entropy-inspiredparser.
In NAACL.Eugene Charniak.
2001.
Immediate-head parsing forlanguage models.
In ACL.Michael Collins.
1999.
Head-driven statistical modelsfor natural language parsing.
Ph.D. thesis, Univer-sity of Pennsylvania.Hal Daume?
III.
2004.
Notes on CG and LM-BFGS op-timization of logistic regression.
Paper available athttp://www.isi.edu/ hdaume/docs/daume04cg-bfgs.ps, implementation available athttp://www.isi.edu/ hdaume/megam/, August.Rebecca Hwa, Philip Resnik, Amy Weinberg, ClaraCabezas, and Okan Kolak.
2005.
Bootstrappingparsers via syntactic projection across parallel texts.Natural Language Engineering, 11(3):311?325.15Mark Johnson.
1998.
PCFG models of linguistic treerepresentation.
Computational Linguistics, 24:613?632.Jonas Kuhn and Michael Jellinghaus.
to appear.
Mul-tilingual parallel treebanking: a lean and flexible ap-proach.
In Proceedings of the Fifth InternationalConference on Language Resources and Evaluation,Genoa, Italy.Adwait Ratnaparkhi.
1997.
A linear observed time sta-tistical parser based on maximum entropy models.In EMNLP.Helmut Schmid.
1994.
Probabilistic part-of-speechtagging using decision trees.
In International Con-ference on New Methods in Language Processing.16
