Proceedings of the Workshop on Statistical Machine Translation, pages 39?46,New York City, June 2006. c?2006 Association for Computational LinguisticsPhrase-Based SMT with Shallow Tree-PhrasesPhilippe Langlais and Fabrizio GottiRALI ?
DIROUniversite?
de Montre?al,C.P.
6128 Succ.
Centre-VilleH3C 3J7, Montre?al, Canada{felipe,gottif}@iro.umontreal.caAbstractIn this article, we present a translationsystem which builds translations by glu-ing together Tree-Phrases, i.e.
associ-ations between simple syntactic depen-dency treelets in a source language andtheir corresponding phrases in a targetlanguage.
The Tree-Phrases we use inthis study are syntactically informed andpresent the advantage of gathering sourceand target material whose words do nothave to be adjacent.
We show that thephrase-based translation engine we imple-mented benefits from Tree-Phrases.1 IntroductionPhrase-based machine translation is now a popularparadigm.
It has the advantage of naturally cap-turing local reorderings and is shown to outper-form word-based machine translation (Koehn et al,2003).
The underlying unit (a pair of phrases), how-ever, does not handle well languages with very dif-ferent word orders and fails to derive generalizationsfrom the training corpus.Several alternatives have been recently proposedto tackle some of these weaknesses.
(Matusov etal., 2005) propose to reorder the source text in or-der to mimic the target word order, and then let aphrase-based model do what it is good at.
(Simardet al, 2005) detail an approach where the standardphrases are extended to account for ?gaps?
either onthe target or source side.
They show that this repre-sentation has the potential to better exploit the train-ing corpus and to nicely handle differences such asnegations in French and English that are poorly han-dled by standard phrase-based models.Others are considering translation as a syn-chronous parsing process e.g.
(Melamed, 2004;Ding and Palmer, 2005)) and several algorithmshave been proposed to learn the underlying produc-tion rule probabilities (Graehl and Knight, 2004;Ding and Palmer, 2004).
(Chiang, 2005) proposesan heuristic way of acquiring context free transferrules that significantly improves upon a standardphrase-based model.As mentioned in (Ding and Palmer, 2005), mostof these approaches require some assumptions onthe level of isomorphism (lexical and/or structural)between two languages.
In this work, we considera simple kind of unit: a Tree-Phrase (TP), a com-bination of a fully lexicalized treelet (TL) and anelastic phrase (EP), the tokens of which may be innon-contiguous positions.
TPs capture some syntac-tic information between two languages and can eas-ily be merged with standard phrase-based engines.A TP can be seen as a simplification of the treeletpairs manipulated in (Quirk et al, 2005).
In particu-lar, we do not address the issue of projecting a sourcetreelet into a target one, but take the bet that collect-ing (without structure) the target words associatedwith the words encoded in the nodes of a treelet willsuffice to allow translation.
This set of target wordsis what we call an elastic phrase.We show that these units lead to (modest) im-provements in translation quality as measured by au-tomatic metrics.
We conducted all our experiments39on an in-house version of the French-English Cana-dian Hansards.This paper is organized as follows.
We first definea Tree-Phrase in Section 2, the unit with which webuilt our system.
Then, we describe in Section 3the phrase-based MT decoder that we designed tohandle TPs.
We report in Section 4 the experimentswe conducted combining standard phrase pairs andTPs.
We discuss this work in Section 5 and thenconclude in Section 6.2 Tree-PhrasesWe call tree-phrase (TP) a bilingual unit consistingof a source, fully-lexicalized treelet (TL) and a tar-get phrase (EP), that is, the target words associatedwith the nodes of the treelet, in order.
A treelet canbe an arbitrary, fully-lexicalized subtree of the parsetree associated with a source sentence.
A phrase canbe an arbitrary sequence of words.
This includesthe standard notion of phrase, popular with phrased-based SMT (Koehn et al, 2003; Vogel et al, 2003)as well as sequences of words that contain gaps (pos-sibly of arbitrary size).In this study, we collected a repository of tree-phrases using a robust syntactic parser called SYN-TEX (Bourigault and Fabre, 2000).
SYNTEX identi-fies syntactic dependency relations between words.It takes as input a text processed by the TREETAG-GER part-of-speech tagger.1 An example of the out-put SYNTEX produces for the source (French) sen-tence ?on a demande?
des cre?dits fe?de?raux?
(requestfor federal funding) is presented in Figure 1.We parsed with SYNTEX the source (French) partof our training bitext (see Section 4.1).
From thismaterial, we extracted all dependency subtrees ofdepth 1 from the complete dependency trees foundby SYNTEX.
An elastic phrase is simply the list oftokens aligned with the words of the correspondingtreelet as well as the respective offsets at which theywere found in the target sentence (the first token ofan elastic phrase always has an offset of 0).For instance, the two treelets in Figure 2 will becollected out of the parse tree in Figure 1, yielding2 tree-phrases.
Note that the TLs as well as the EPsmight not be contiguous as is for instance the case1www.ims.uni-stuttgart.de/projekte/corplex/.a demande?SUBllllllllll OBJYYYYYYYYYYYYYYYYYYon cre?ditsDETllllllllll ADJRRRRRRRRRRdes fe?de?rauxFigure 1: Parse of the sentence ?on a demande?
descre?dits fe?de?raux?
(request for federal funding).
Notethat the 2 words ?a?
and ?demande??
(literally ?have?and ?asked?)
from the original sentence have beenmerged together by SYNTEX to form a single token.These tokens are the ones we use in this study.with the first pair of structures listed in the example.3 The Translation EngineWe built a translation engine very similar to the sta-tistical phrase-based engine PHARAOH described in(Koehn, 2004) that we extended to use tree-phrases.Not only does our decoder differ from PHARAOH byusing TPs, it also uses direct translation models.
Weknow from (Och and Ney, 2002) that not using thenoisy-channel approach does not impact the qualityof the translation produced.3.1 The maximization settingFor a source sentence f , our engine incrementallygenerates a set of translation hypotheses H by com-bining tree-phrase (TP) units and phrase-phrase (PP)units.2 We define a hypothesis in this set as h ={Ui ?
(Fi, Ei)}i?
[1,u], a set of u pairs of source(Fi) and target sequences (Ei) of ni and mi wordsrespectively:Fi ?
{fjin : jin ?
[1, |f |]}n?
[1,ni]Ei ?
{elim : lim ?
[1, |e|]}m?
[1,mi]under the constraints that for all i ?
[1, u], jin <jin+1 ,?n ?
[1, ni[ for a source treelet (similar con-straints apply on the target side), and jin+1 = jin +1 ,?n ?
[1, ni[ for a source phrase.
The way thehypotheses are built imposes additional constraintsbetween units that will be described in Section 3.3.Note that, at decoding time, |e|, the number of words2What we call here a phrase-phrase unit is simply a pair ofsource/target sequences of words.40alignment:a demande?
?
request for, fe?de?raux ?
federal,cre?dits ?
fundingtreelets:a demande?qqqqqqqMMMMMMMon cre?ditscre?ditsqqqqqqqMMMMMMMdes fe?de?rauxtree-phrases:TL?
{{on@-1} a_demande?
{cre?dits@2}}EP?
|request@0||for@1||funding@3|TL {{des@-1} cre?dits {fe?de?raux@1}}EP |federal@0||funding@1|Figure 2: The Tree-Phrases collected out of theSYNTEX parse for the sentence pair of Figure 1.Non-contiguous structures are marked with a star.Each dependent node of a given governor token isdisplayed as a list surrounding the governor node,e.g.
{governor {right-dependent}}.
Along with thetokens of each node, we present their respective off-set (the governor/root node has the offset 0 by defi-nition).
The format we use to represent the treeletsis similar to the one proposed in (Quirk et al, 2005).of the translation is unknown, but is bounded accord-ing to |f | (in our case, |e|max = 2?
|f |+ 5).We define the source and target projection of ahypothesis h by the proj operator which collects inorder the words of a hypothesis along one language:projF (h) ={fp : p ??ui=1{jin}n?
[1,ni]}projE(h) ={ep : p ??ui=1{lim}m?
[1,mi]}If we denote by Hf the set of hypotheses thathave f as a source projection (that is, Hf = {h :projF (h) ?
f}), then our translation engine seekse?
= projE(h?)
where:h?
= argmaxh?Hfs(h)The function we seek to maximize s(h) is a log-linear combination of 9 components, and might bebetter understood as the numerator of a maximumentropy model popular in several statistical MT sys-tems (Och and Ney, 2002; Bertoldi et al, 2004; Zensand Ney, 2004; Simard et al, 2005; Quirk et al,2005).
The components are the so-called featurefunctions (described below) and the weighting co-efficients (?)
are the parameters of the model:s(h) = ?pprf log ppprf (h) + ?p|h|+?tprf log ptprf (h) + ?t|h|+?ppibm log pppibm(h)+?tpibm log ptpibm(h)+?lm log plm(projE(h))+?d d(h) + ?w|projE(h)|3.2 The components of the scoring functionWe briefly enumerate the features used in this study.Translation models Even if a tree-phrase is a gen-eralization of a standard phrase-phrase unit, for in-vestigation purposes, we differentiate in our MTsystem between two kinds of models: a TP-basedmodel ptp and a phrase-phrase model ppp.
Both relyon conditional distributions whose parameters arelearned over a corpus.
Thus, each model is assignedits own weighting coefficient, allowing the tuningprocess to bias the engine toward a special kind ofunit (TP or PP).We have, for k ?
{rf, ibm}:pppk(h) =?ui=1 ppp(Ei|Fi)ptpk(h) =?ui=1 ptp(Ei|Fi)with p?rf standing for a model trained by rel-ative frequency, whereas p?ibm designates a non-normalized score computed by an IBM model-1translation model p, where f0 designates the so-called NULL word:p?ibm(Ei|Fi) =mi?m=1ni?n=1p(elim |fjin) + p(ekim |f0)Note that by setting ?tprf and ?tpibm to zero, werevert back to a standard phrase-based translationengine.
This will serve as a reference system in theexperiments reported (see Section 4).The language model Following a standard prac-tice, we use a trigram target language modelplm(projE(h)) to control the fluency of the trans-lation produced.
See Section 3.3 for technical sub-tleties related to their use in our engine.41Distortion model d This feature is very similar tothe one described in (Koehn, 2004) and only de-pends on the offsets of the source units.
The onlydifference here arises when TPs are used to build atranslation hypothesis:d(h) = ?n?i=1abs(1 + F i?1 ?
F i)where:F i ={ ?n?
[1,ni] jin/ni if Fi is a treeletjini otherwiseF i = ji1This score encourages the decoder to produce amonotonous translation, unless the language modelstrongly privileges the opposite.Global bias features Finally, three simple fea-tures help control the translation produced.
EachTP (resp.
PP) unit used to produce a hypothesisreceives a fixed weight ?t (resp.
?p).
This allowsthe introduction of an artificial bias favoring eitherPPs or TPs during decoding.
Each target word pro-duced is furthermore given a so-called word penalty?w which provides a weak way of controlling thepreference of the decoder for long or short transla-tions.3.3 The search procedureThe search procedure is described by the algorithmin Figure 3.
The first stage of the search consists incollecting all the units (TPs or PPs) whose sourcepart matches the source sentence f .
We call U theset of those matching units.In this study, we apply a simple match policy thatwe call exact match policy.
A TL t matches a sourcesentence f if its root matches f at a source positiondenoted r and if all the other words w of t satisfy:fow+r = wwhere ow designates the offset of w in t.Hypotheses are built synchronously along withthe target side (by appending the target material tothe right of the translation being produced) by pro-gressively covering the positions of the source sen-tence f being translated.Require: a source sentence fU ?
{u : s-match(u, f)}FUTURECOST(U)for s?
1 to |f | doS[s]?
?S[0]?
{(?, , 0)}for s?
0 to |f | ?
1 doPRUNE(S[s], ?
)for all hypotheses alive h ?
S[s] dofor all u ?
U doif EXTENDS(u, h) thenh?
?
UPDATE(u, h)k ?
|projF (h?)|S[k]?
S[k] ?
{h?
}return argmaxh?S[|f |] ?
: h?
(ps, t, ?
)Figure 3: The search algorithm.
The symbol ?
isused in place of assignments, while?
denotes uni-fication (as in languages such as Prolog).The search space is organized into a set S of |f |stacks, where a stack S[s] (s ?
[1, |f |]) contains allthe hypotheses covering exactly s source words.
Ahypothesis h = (ps, t, ?)
is composed of its targetmaterial t, the source positions covered ps as well asits score ?.
The search space is initialized with anempty hypothesis: S[0] = {(?, , 0)}.The search procedure consists in extending eachpartial hypothesis h with every unit that can con-tinue it.
This process ends when all partial hypothe-ses have been expanded.
The translation returned isthe best one contained in S[|f |]:e?
= projE(argmaxh?S[|f |]?
: h?
(ps, t, ?
))PRUNE ?
In order to make the search tractable,each stack S[s] is pruned before being expanded.Only the hypotheses whose scores are within a frac-tion (controlled by a meta-parameter ?
which typi-cally is 0.0001 in our experiments) of the score ofthe best hypothesis in that stack are considered forexpansion.
We also limit the number of hypothesesmaintained in a given stack to the top maxStackones (maxStack is typically set to 500).Because beam-pruning tends to promote in a stackpartial hypotheses that translate easy parts (i.e.
parts42that are highly scored by the translation and lan-guage models), the score considered while pruningnot only involves the cost of a partial hypothesis sofar, but also an estimation of the future cost that willbe incurred by fully expanding it.FUTURECOST ?
We followed the heuristic de-scribed in (Koehn, 2004), which consists in comput-ing for each source range [i, j] the minimum costc(i, j) with which we can translate the source se-quence f ji .
This is pre-computed efficiently at anearly stage of the decoding (second line of the algo-rithm in Figure 3) by a bottom-up dynamic program-ming scheme relying on the following recursion:c(i, j) = min{mink?
[i,j[c(i, k) + c(k, j)minu?U/us?fji =usscore(us)where us stands for the projection of u on the tar-get side (us ?
projE(u)), and score(u) is com-puted by considering the language model and thetranslation components ppp of the s(h) score.
Thefuture cost of h is then computed by summing thecost c(i, j) of all its empty source ranges [i, j].EXTENDS ?
When we simply deal with standard(contiguous) phrases, extending a hypothesis h by aunit u basically requires that the source positions ofu be empty in h. Then, the target material of u isappended to the current hypothesis h.Because we work with treelets here, things area little more intricate.
Conceptually, we are con-fronted with the construction of a (partial) sourcedependency tree while collecting the target mate-rial in order.
Therefore, the decoder needs to checkwhether a given TL (the source part of u) is compati-ble with the TLs belonging to h. Since we decided inthis study to use depth-one treelets, we consider thattwo TLs are compatible if either they do not shareany source word, or, if they do, this shared wordmust be the governor of one TL and a dependent inthe other TL.So, for instance, in the case of Figure 2, thetwo treelets are deemed compatible (they obviouslyshould be since they both belong to the same orig-inal parse tree) because cre?dit is the governorin the right-hand treelet while being the depen-dent in the left-hand one.
On the other hand, thetwo treelets in Figure 4 are not, since pre?sidentis the governor of both treelets, even though mr.le pre?sident supple?ant would be a validsource phrase.
Note that it might be the case thatthe treelet {{mr.@-2} {le@-1} pre?sident{supple?ant@1}} has been observed duringtraining, in which case it will compete with thetreelets in Figure 2.pre?sidentmr.pre?sidentqqqqqqqMMMMMMMle supple?antFigure 4: Example of two incompatible treelets.mr.
speaker and the acting speakerare their respective English translations.Therefore, extending a hypothesis containing atreelet with a new treelet consists in merging the twotreelets (if they are compatible) and combining thetarget material accordingly.
This operation is morecomplicated than in a standard phrase-based decodersince we allow gaps on the target side as well.
More-over, the target material of two compatible treeletsmay intersect.
This is for instance the case for thetwo TPs in Figure 2 where the word funding iscommon to both phrases.UPDATE ?
Whenever u extends h, we add anew hypothesis h?
in the corresponding stackS[|projF (h?)|].
Its score is computed by adding tothat of h the score of each component involved ins(h).
For all but the one language model compo-nent, this is straightforward.
However, care must betaken to update the language model score since thetarget material of u does not come necessarily rightafter that of h as would be the case if we only ma-nipulated PP units.Figure 5 illustrates the kind of bookkeepingrequired.
In practice, the target material ofa hypothesis is encoded as a vector of triplets{?wi, log plm(wi|ci), li?}i?
[1,|e|max] where wi is theword at position i in the translation, log plm(wi|ci)is its score as given by the language model, ci de-notes the largest conditioning context possible, andli indicates the length (in words) of ci (0 means aunigram probability, 1 a bigram probability and 2 atrigram probability).
This vector is updated at eachextension.43udes f?d?rauxon a_demand?
cr?ditsTL: {on@?1}  a_demand?
{cr?dits@2}EP: request@0  for@1  funding@3TL: {des@?1}  cr?dits  {f?d?raux@1}EP: federal@0  funding@1U B F Urequest for fundingcr?ditson a_demand?
des f?d?rauxforrequest fundingU B T Tfederalhh?S[3]S[4]uFigure 5: Illustration of the language model up-dates that must be made when a new target unit(circles with arrows represent dependency links) ex-tends an existing hypothesis (rectangles).
The taginside each occupied target position shows whetherthis word has been scored by a Unigram, a Bigramor a Trigram probability.4 Experimental Setting4.1 CorporaWe conducted our experiments on an in-house ver-sion of the Canadian Hansards focussing on thetranslation of French into English.
The split of thismaterial into train, development and test corpora isdetailed in Table 1.
The TEST corpus is subdividedin 16 (disjoints) slices of 500 sentences each thatwe translated separately.
The vocabulary is atypi-cally large since some tokens are being merged bySYNTEX, such as e?taient#finance?es (werefinanced in English).The training corpus has been aligned at theword level by two Viterbi word-alignments(French2English and English2French) that wecombined in a heuristic way similar to the refinedmethod described in (Och and Ney, 2003).
Theparameters of the word models (IBM model 2) weretrained with the GIZA++ package (Och and Ney,2000).TRAIN DEV TESTsentences 1 699 592 500 8000e-toks 27 717 389 8 160 130 192f-toks 30 425 066 8 946 143 089e-toks/sent 16.3 (?
9.0) 16.3 (?
9.1) 16.3 (?
9.0)f-toks/sent 17.9 (?
9.5) 17.9 (?
9.5) 17.9 (?
9.5)e-types 164 255 2 224 12 591f-types 210 085 2 481 15 008e-hapax 68 506 1 469 6 887f-hapax 90 747 1 704 8 612Table 1: Main characteristics of the corpora used inthis study.
For each language l, l-toks is the numberof tokens, l-toks/sent is the average number of to-kens per sentence (?
the standard deviation), l-typesis the number of different token forms and l-hapaxis the number of tokens that appear only once in thecorpus.4.2 ModelsTree-phrases Out of 1.7 million pairs of sen-tences, we collected more than 3 million differentkinds of TLs from which we projected 6.5 milliondifferent kinds of EPs.
Slightly less than half ofthe treelets are contiguous ones (i.e.
involving a se-quence of adjacent words); 40% of the EPs are con-tiguous.
When the respective frequency of each TLor EP is factored in, we have approximately 11 mil-lion TLs and 10 million EPs.
Roughly half of thetreelets collected have exactly two dependents (threeword long treelets).Since the word alignment of non-contiguousphrases is likely to be less accurate than the align-ment of adjacent word sequences, we further filterthe repository of TPs by keeping the most likely EPsfor each TL according to an estimate of p(EP |TL)that do not take into account the offsets of the EP orthe TL.PP-model We collected the PP parameters by sim-ply reading the alignment matrices resulting fromthe word alignment, in a way similar to the onedescribed in (Koehn et al, 2003).
We use an in-house tool to collect pairs of phrases of up to 8words.
Freely available packages such as THOT(Ortiz-Mart?
?nez et al, 2005) could be used as wellfor that purpose.44Language model We trained a Kneser-Ney tri-gram language model using the SRILM toolkit (Stol-cke, 2002).4.3 ProtocolWe compared the performances of two versions ofour engine: one which employs TPs ans PPs (TP-ENGINE hereafter), and one which only uses PPs(PP-ENGINE).
We translated the 16 disjoint sub-corpora of the TEST corpus with and without TPs.We measure the quality of the translation pro-duced with three automatic metrics.
Two errorrates: the sentence error rate (SER) and the worderror rate (WER) that we seek to minimize, andBLEU (Papineni et al, 2002), that we seek tomaximize.
This last metric was computed withthe multi-bleu.perl script available at www.statmt.org/wmt06/shared-task/.We separately tuned both systems on the DEV cor-pus by applying a brute force strategy, i.e.
by sam-pling uniformly the range of each parameter (?)
andpicking the configuration which led to the best BLEUscore.
This strategy is inelegant, but in early experi-ments we conducted, we found better configurationsthis way than by applying the Simplex method withmultiple starting points.
The tuning roughly takes24 hours of computation on a cluster of 16 comput-ers clocked at 3 GHz, but, in practice, we found thatone hour of computation is sufficient to get a con-figuration whose performances, while subobptimal,are close enough to the best one reachable by an ex-haustive search.Both configurations were set up to avoid distor-tions exceeding 3 (maxDist = 3).
Stacks wereallowed to contain no more than 500 hypotheses(maxStack = 500) and we further restrained thenumber of hypotheses considered by keeping foreach matching unit (treelet or phrase) the 5 bestranked target associations.
This setting has beenfixed experimentally on the DEV corpus.4.4 ResultsThe scores for the 16 slices of the test corpus are re-ported in Table 2.
TP-ENGINE shows slightly betterfigures for all metrics.For each system and for each metric, we had16 scores (from each of the 16 slices of the test cor-pus) and were therefore able to test the statistical sig-nicance of the difference between the TP-ENGINEand PP-ENGINE using a Wilcoxon signed-rank testfor paired samples.
This test showed that the dif-ference observed between the two systems is signif-icant at the 95% probability level for BLEU and sig-nificant at the 99% level for WER and SER.Engine WER% SER% BLEU%PP 52.80 ?
1.2 94.32 ?
0.9 29.95 ?
1.2TP 51.98 ?
1.2 92.83 ?
1.3 30.47 ?
1.4Table 2: Median WER, SER and BLEU scores(?
value range) of the translations produced by thetwo engines on a test set of 16 disjoint corpora of500 sentences each.
The figures reported are per-centages.On the DEV corpus, we measured that, on aver-age, each source sentence is covered by 39 TPs (theirsource part, naturally), yielding a source coverage ofapproximately 70%.
In contrast, the average numberof covering PPs per sentence is 233.5 DiscussionOn a comparable test set (Canadian Hansard texts),(Simard et al, 2005) report improvements by addingnon-contiguous bi-phrases to their engine withoutrequiring a parser at all.
At the same time, they alsoreport negative results when adding non-contiguousphrases computed from the refined alignment tech-nique that we used here.Although the results are not directly comparable,(Quirk et al, 2005) report much larger improve-ments over a phrase-based statistical engine withtheir translation engine that employs a source parser.The fact that we consider only depth-one treelets inthis work, coupled with the absence of any particulartreelet projection algorithm (which prevents us fromtraining a syntactically motivated reordering modelas they do) are other possible explanations for themodest yet significant improvements we observe inthis study.6 ConclusionWe presented a pilot study aimed at appreciating thepotential of Tree-Phrases as base units for example-based machine translation.45We developed a translation engine which makesuse of tree-phrases on top of pairs of source/targetsequences of words.
The experiments we conductedsuggest that TPs have the potential to improve trans-lation quality, although the improvements we mea-sured are modest, yet statistically significant.We considered only one simple form of tree in thisstudy: depth-one subtrees.
We plan to test our en-gine on a repository of treelets of arbitrary depth.
Intheory, there is not much to change in our engineto account for such units and it would offer an al-ternative to the system proposed recently by (Liu etal., 2005), which performs translations by recyclinga collection of tree-string-correspondence (TSC) ex-amples.ReferencesNicola Bertoldi, Roldano Cattoni, Mauro Cettolo, andMarcello Federico.
2004.
The ITC-irst statistical ma-chine translation system for IWSLT-2004.
In IWSLT,pages 51?58, Kyoto, Japan.Didier Bourigault and Ce?cile Fabre.
2000.
Ap-proche linguistique pour l?analyse syntaxique de cor-pus.
Cahiers de Grammaire, (25):131?151.
Toulousele Mirail.David Chiang.
2005.
A hierarchical phrase-based modelfor statistical machine translation.
In 43rd ACL, pages263?270, Ann Arbor, Michigan, USA.Yuang Ding and Martha Palmer.
2004.
Automatic learn-ing of parallel dependency treelet pairs.
In Proceed-ings of the first International Joint Conference on NLP.Yuan Ding and Martha Palmer.
2005.
Machine trans-lation using probabilistic synchronous dependency in-sertion grammars.
In 43rd ACL, pages 541?548, AnnArbor, Michigan, June.Jonathan Graehl and Kevin Knight.
2004.
Training treetransducers.
In HLT-NAACL 2004, pages 105?112,Boston, Massachusetts, USA, May 2 - May 7.
Asso-ciation for Computational Linguistics.Philipp Koehn, Franz Joseph Och, and Daniel Marcu.2003.
Statistical Phrase-Based Translation.
In Pro-ceedings of HLT, pages 127?133.Philipp Koehn.
2004.
Pharaoh: a Beam Search Decoderfor Phrase-Based SMT.
In Proceedings of AMTA,pages 115?124.Zhanyi Liu, Haifeng Wang, and Hua Wu.
2005.Example-based machine translation based on tsc andstatistical generation.
In Proceedings of MT SummitX, pages 25?32, Phuket, Thailand.Evgeny Matusov, Stephan Kanthak, and Hermann Ney.2005.
Efficient statistical machine translation withconstraint reordering.
In 10th EAMT, pages 181?188,Budapest, Hongary, May 30-31.I.
Dan Melamed.
2004.
Statistical machine translationby parsing.
In 42nd ACL, pages 653?660, Barcelona,Spain.Franz Joseph Och and Hermann Ney.
2000.
ImprovedStatistical Alignment Models.
In Proceedings of ACL,pages 440?447, Hongkong, China.Franz Joseph Och and Hermann Ney.
2002.
Discrimina-tive training and maximum entropy models for statis-tical machine translation.
In Proceedings of the ACL,pages 295?302.Franz Joseph Och and Hermann Ney.
2003.
A Sys-tematic Comparison of Various Statistical AlignmentModels.
Computational Linguistics, 29:19?51.Daniel Ortiz-Mart?
?nez, Ismael Garcia?-Varea, and Fran-cisco Casacuberta.
2005.
Thot: a toolkit to trainphrase-based statistical translation models.
In Pro-ceedings of MT Summit X, pages 141?148, Phuket,Thailand, Sep.Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu.
2002.
Bleu: a method for automatic evalua-tion of machine translation.
In 40th ACL, pages 311?318, Philadelphia, Pennsylvania.Chris Quirk, Arul Menezes, and Colin Cherry.
2005.
De-pendency treelet translation: Syntactically informedphrasal SMT.
In 43rd ACL, pages 271?279, Ann Ar-bor, Michigan, June.Michel Simard, Nicola Cancedda, Bruno Cavestro,Marc Dymetman, Eric Gaussier, Cyril Goutte,Kenji Yamada, Philippe Langlais, and Arne Mauser.2005.
Translating with non-contiguous phrases.
InHLT/EMNLP, pages 755?762, Vancouver, BritishColumbia, Canada, Oct.Andreas Stolcke.
2002.
Srilm - an Extensible LanguageModeling Toolkit.
In Proceedings of ICSLP, Denver,Colorado, Sept.Stephan Vogel, Ying Zhang, Fei Huang, Alicai Trib-ble, Ashish Venugopal, Bing Zao, and Alex Waibel.2003.
The CMU Statistical Machine Translation Sys-tem.
InMachine Translation Summit IX, New Orleans,Louisina, USA, Sep.Richard Zens and Hermann Ney.
2004.
Improvements inphrase-based statistical machine translation.
In Pro-ceedings of the HLT/NAACL, pages 257?264, Boston,MA, May.46
