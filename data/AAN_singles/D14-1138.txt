Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1319?1328,October 25-29, 2014, Doha, Qatar.c?2014 Association for Computational LinguisticsLow-dimensional Embeddings for InterpretableAnchor-based Topic InferenceMoontae LeeDept.
of Computer ScienceCornell UniversityIthaca, NY, 14853moontae@cs.cornell.eduDavid MimnoDept.
of Information ScienceCornell UniversityIthaca, NY, 14853mimno@cornell.eduAbstractThe anchor words algorithm performsprovably efficient topic model inferenceby finding an approximate convex hullin a high-dimensional word co-occurrencespace.
However, the existing greedy al-gorithm often selects poor anchor words,reducing topic quality and interpretability.Rather than finding an approximate con-vex hull in a high-dimensional space, wepropose to find an exact convex hull ina visualizable 2- or 3-dimensional space.Such low-dimensional embeddings bothimprove topics and clearly show users whythe algorithm selects certain words.1 IntroductionStatistical topic modeling is useful in exploratorydata analysis (Blei et al., 2003), but model infer-ence is known to be NP-hard even for the sim-plest models with only two topics (Sontag andRoy, 2011), and training often remains a blackbox to users.
Likelihood-based training requiresexpensive approximate inference such as varia-tional methods (Blei et al., 2003), which are deter-ministic but sensitive to initialization, or Markovchain Monte Carlo (MCMC) methods (Griffithsand Steyvers, 2004), which have no finite conver-gence guarantees.
Recently Arora et al.
proposedthe Anchor Words algorithm (Arora et al., 2013),which casts topic inference as statistical recoveryusing a separability assumption: each topic hasa specific anchor word that appears only in thecontext of that single topic.
Each anchor wordcan be used as a unique pivot to disambiguate thecorresponding topic distribution.
We then recon-struct the word co-occurrence pattern of each non-anchor words as a convex combination of the co-occurrence patterns of the anchor words.burgersaladpizzachickengoodtoldpopcornstadiumviewstiremoviesscreensashimicarcalledhotel yogabagelsshoppingdog movieFigure 1: 2D t-SNE projection of a Yelp reviewcorpus and its convex hull.
The words corre-sponding to vertices are anchor words for topics,whereas non-anchor words correspond to the inte-rior points.This algorithm is fast, requiring only one passthrough the training documents, and providesprovable guarantees, but results depend entirely onselecting good anchor words.
(Arora et al., 2013)propose a greedy method that finds an approxi-mate convex hull around a set of vectors corre-sponding to the word co-occurrence patterns foreach vocabulary word.
Although this method isan improvement over previous work that used im-practical linear programming methods (Arora etal., 2012), serious problems remain.
The methodgreedily chooses the farthest point from the cur-rent subspace until the given number of anchorshave been found.
Particularly at the early stages1319of the algorithm, the words associated with thefarthest points are likely to be infrequent and id-iosyncratic, and thus form poor bases for humaninterpretation and topic recovery.
This poor choiceof anchors noticeably affects topic quality: the an-chor words algorithm tends to produce large num-bers of nearly identical topics.Besides providing a separability criterion, an-chor words also have the potential to improve topicinterpretability.
After learning topics for given textcollections, users often request a label that sum-marizes each topic.
Manually labeling topics is ar-duous, and labels often do not carry over betweenrandom initializations and models with differingnumbers of topics.
Moreover, it is hard to con-trol the subjectivity in labelings between annota-tors, which is open to interpretive errors.
Therehas been considerable interest in automating thelabeling process (Mei et al., 2007; Lau et al., 2011;Chuang et al., 2012).
(Chuang et al., 2012) pro-pose a measure of saliency: a good summary termshould be both distinctive specifically to one topicand probable in that topic.
Anchor words are bydefinition optimally distinct, and therefore mayseem to be good candidates for topic labels, butgreedily selecting extreme words often results inanchor words that have low probability.In this work we explore the opposite of Arora etal.
?s method: rather than finding an approximateconvex hull for an exact set of vectors, we find anexact convex hull for an approximate set of vec-tors.
We project the V ?
V word co-occurrencematrix to visualizable 2- and 3-dimensional spacesusing methods such as t-SNE (van der Maaten andHinton, 2008), resulting in an input matrix up to3600 times narrower than the original input forour training corpora.
Despite this radically low-dimensional projection, the method not only findstopics that are as good or better than the greedyanchor method, it also finds highly salient, in-terpretable anchor words and provides users witha clear visual explanation for why the algorithmchooses particular words, all while maintainingthe original algorithm?s computational benefits.2 Related WorkLatent Dirichlet allocation (LDA) (Blei et al.,2003) models D documents with a vocabulary Vusing a predefined number of topics by K. LDAviews both {Ak}Kk=1, a set of K topic-word distri-butions for each topic k, and {Wd}Dd=1, a set of Ddocument-topic distributions for each document d,and {zd}Dd=1, a set of topic-assignment vectors forword tokens in the document d, as randomly gen-erated from known stochastic processes.
Merging{Ak} as k-th column vector of V ?K matrix A,{Wd} as d-th column vector of K ?D matrix W ,the learning task is to estimate the posterior dis-tribution of latent variables A, W , and {zd} givenV ?
D word-document matrix?M , which is theonly observed variable where d-th column corre-sponds to the empirical word frequencies in thetraining documents d.(Arora et al., 2013) recover word-topic matrixA and topic-topic matrix R = E[WWT] insteadof W in the spirit of nonnegative matrix factoriza-tion.
Though the true underlying word distribu-tion for each document is unknown and could befar from the sample observation?M , the empiricalword-word matrix?Q converges to its expectationAE[WWT]AT= ARATas the number of docu-ments increases.
Thus the learning task is to ap-proximately recover A and R pretending that theempirical?Q is close to the true second-order mo-ment matrix Q.The critical assumption for this method is tosuppose that every topic k has a specific anchorword skthat occurs with non-negligible probabil-ity (> 0) only in that topic.
The anchor word skneed not always appear in every document aboutthe topic k, but we can be confident that the doc-ument is at least to some degree about the topic kif it contains sk.
This assumption drastically im-proves inference by guaranteeing the presence ofa diagonal sub-matrix inside the word-topic ma-trix A.
After constructing an estimate?Q, the al-gorithm in (Arora et al., 2013) first finds a setS = {s1, ..., sK} of K anchor words (K is user-specified), and recovers A and R subsequentlybased on S. Due to this structure, overall perfor-mance depends heavily on the quality of anchorwords.In the matrix algebra literature this greedyanchor finding method is called QR with row-pivoting.
Previous work classifies a matrix intotwo sets of row (or column) vectors where the vec-tors in one set can effectively reconstruct the vec-tors in another set, called subset-selection algo-rithms.
(Gu and Eisenstat, 1996) suggest one im-portant deterministic algorithm.
A randomized al-gorithm provided by (Boutsidis et al., 2009) is thestate-of-the art using a pre-stage that selects the1320candidates in addition to (Gu and Eisenstat, 1996).We found no change in anchor selection usingthese algorithms, verifying the difficulty of the an-chor finding process.
This difficulty is mostly be-cause anchors must be nonnegative convex bases,whereas the classified vectors from the subset se-lection algorithms yield unconstrained bases.The t-SNE model has previously been used todisplay high-dimensional embeddings of words in2D space by Turian.1Low-dimensional embed-dings of topic spaces have also been used to sup-port user interaction with models: (Eisenstein etal., 2011) use a visual display of a topic embed-ding to create a navigator interface.
Althought-SNE has been used to visualize the results oftopic models, for example by (Lacoste-Julien etal., 2008) and (Zhu et al., 2009), we are not awareof any use of the method as a fundamental compo-nent of topic inference.3 Low-dimensional EmbeddingsReal text corpora typically involve vocabularies inthe tens of thousands of distinct words.
As theinput matrix?Q scales quadratically with V , theAnchor Words algorithm must depend on a low-dimensional projection of?Q in order to be practi-cal.
Previous work (Arora et al., 2013) uses ran-dom projections via either Gaussian random ma-trices (Johnson and Lindenstrauss, 1984) or sparserandom matrices (Achlioptas, 2001), reducing therepresentation of each word to around 1,000 di-mensions.
Since the dimensionality of the com-pressed word co-occurrence space is an order ofmagnitude larger than K, we must still approxi-mate the convex hull by choosing extreme pointsas before.In this work we explore two projection meth-ods: PCA and t-SNE (van der Maaten and Hinton,2008).
Principle Component Analysis (PCA) is acommonly-used dimensionality reduction schemethat linearly transforms the data to new coordi-nates where the largest variances are orthogonallycaptured for each dimension.
By choosing only afew such principle axes, we can represent the datain a lower dimensional space.
In contrast, t-SNEembedding performs a non-linear dimensionalityreduction preserving the local structures.
Given aset of points {xi} in a high-dimensional space X ,t-SNE allocates probability mass for each pair ofpoints so that pairs of similar (closer) points be-1http://metaoptimize.com/projects/wordreprs/goodplacegreatlovechickenstorericeshowerresponseto-gobroccoliteriyakiyogasalonlettuceFigure 2: 2D PCA projections of a Yelp reviewcorpus and its convex hulls.come more likely to co-occur than dissimilar (dis-tant) points.pj|i=exp(?d(xi,xj)2/2?2i)?k 6=iexp(?d(xi,xk)2/2?2i)(1)pij=pj|i+ pi|j2N(symmetrized) (2)Then it generates a set of new points {yi} inlow-dimensional space Y so that probability dis-tribution over points in Y behaves similarly to thedistribution over points in X by minimizing KL-divergence between two distributions:qij=(1 + ?yi?
yj?2)?1?k 6=l(1 + ?yk?
yl?2)?1(3)min KL(P ||Q) =?i 6=jpijlogpijqij(4)Instead of approximating a convex hull in sucha high-dimensional space, we select the exactvertices of the convex hull formed in a low-dimensional projected space, which can be calcu-lated efficiently.
Figures 1 and 2 show the con-vex hulls for 2D projections of?Q using t-SNE andPCA for a corpus of Yelp reviews.
Figure 3 il-lustrates the convex hulls for 3D t-SNE projectionfor the same corpus.
Anchor words correspond tothe vertices of these convex hulls.
Note that wepresent the 2D projections as illustrative examplesonly; we find that three dimensional projectionsperform better in practice.1321staffatmospherelovebarbeerhourchickenlocationfoodhighlywinemexicanyearskidsmusic screenshopriceroomsmanagercalledbbq dimshowergroundgroupcheesecaketiresbagelssashimiwaiterenchiladaspecialshotelbeersdonutsglassyogapeakscupcakesdivemoviedogcookiechorizostarbucksshoppinghummusplay hairbottlepromptFigure 3: 3D t-SNE projection of a Yelp reviewcorpus and its convex hull.
Vertices on the convexhull correspond to anchor words.In addition to the computational advantages,this approach benefits anchor-based topic model-ing in two aspects.
First, as we now compute theexact convex hull, the number of topics dependson the dimensionality of the embedding, v. Forexample in the figures, 2D projection has 21 ver-tices, whereas 3D projection supports 69 vertices.This implies users can easily tune the granularityof topic clusters by varying v = 2, 3, 4, ... with-out increasing the number of topics by one eachtime.
Second, we can effectively visualize the the-matic relationships between topic anchors and therest of words in the vocabulary, enhancing bothinterpretability and options for further vocabularycuration.4 Experimental ResultsWe find that radically low-dimensional t-SNE pro-jections are effective at finding anchor words thatare much more salient than the greedy method, andtopics that are more distinctive, while maintain-ing comparable held-out likelihood and semanticcoherence.
As noted in Section 1, the previousgreedy anchor words algorithm tends to producemany nearly identical topics.
For example, 37 outof 100 topics trained on a 2008 political blog cor-pus have obama, mccain, bush, iraq or palin astheir most probable word, including 17 just forobama.
Only 66% of topics have a unique topword.
In contrast, the t-SNE model on the samedataset has only one topic whose most probableword is obama, and 86% of topics have a uniquetop word (mccain is the most frequent top word,with five topics).We use three real datasets: business reviewsfrom the Yelp Academic Dataset,2political blogsfrom the 2008 US presidential election (Eisen-stein and Xing, 2010), and New York Times ar-ticles from 2007.3Details are shown in Table1.
Documents with fewer than 10 word tokensare discarded due to possible instability in con-structing?Q.
We perform minimal vocabulary cu-ration, eliminating a standard list of English stop-words4and terms that occur below frequency cut-offs: 100 times (Yelp, Blog) and 150 times (NYT).We further restrict possible anchor words to wordsthat occur in more than three documents.
As ourdatasets are not artificially synthesized, we reserve5% from each set of documents for held-out like-lihood computation.Name Documents Vocab.
Avg.
lengthYelp 20,000 1,606 40.6Blog 13,000 4,447 161.3NYT 41,000 10,713 277.8Table 1: Statistics for datasets used in experimentsUnlike (Arora et al., 2013), which presentsresults on synthetic datasets to compare perfor-mance across different recovery methods given in-creasing numbers of documents, we are are inter-ested in comparing anchor finding methods, andare mainly concerned with semantic quality.
Asa result, although we have conducted experimentson synthetic document collections,5we focus onreal datasets for this work.
We also choose to com-pare only anchor finding algorithms, so we do notreport comparisons to likelihood-based methods,which can be found in (Arora et al., 2013).For both PCA and t-SNE, we use three-dimensional embeddings across all experiments.This projection results in matrices that are 0.03%as wide as the original V ?
V matrix for theNYT dataset.
Without low-dimensional embed-ding, each word is represented by a V-dimensionalvector where only several terms are non-zero dueto the sparse co-occurrence patterns.
Thus a ver-2https://www.yelp.com/academic dataset3http://catalog.ldc.upenn.edu/LDC2008T194We used the list of 524 stop words included in the Malletlibrary.5None of the algorithms are particularly effective at find-ing synthetically introduced anchor words possibly becausethere are other candidates around anchor vertices that approx-imate the convex hull to almost the same degree.1322tex captured by the greedy anchor-finding methodis likely to be one of many eccentric vertices invery high-dimensional space.
In contrast, t-SNEcreates an effective dense representation where asmall number of pivotal vertices are more clearlyvisible, improving both performance and inter-pretability.Note that since we can find an exact convex hullin these spaces,6there is an upper bound to thenumber of topics that can be found given a partic-ular projection.
If more topics are desired, one cansimply increase the dimensionality of the projec-tion.
For the greedy algorithm we use sparse ran-dom projections with 1,000 dimensions with 5%negative entries and 5% positive entries.
PCA andt-SNE choose (49, 32, 47) and (69, 77, 107) an-chors, respectively for each of three Yelp, Blog,and NYTimes datasets.4.1 Anchor-word SelectionWe begin by comparing the behavior of low-dimensional embeddings to the behavior of thestandard greedy algorithm.
Table 2 shows orderedlists of the first 12 anchor words selected by threealgorithms: t-SNE embedding, PCA embedding,and the original greedy algorithm.
Anchor wordsselected by t-SNE (police, business, court) aremore general than anchor words selected by thegreedy algorithm (cavalry, al-sadr, yiddish).
Ad-ditional examples of anchor words and their asso-ciated topics are shown in Table 3 and discussedin Section 4.2.# t-SNE PCA Greedy1 police beloved cavalry2 bonds york biodiesel3 business family h/w4 day loving kingsley5 initial late mourners6 million president pcl7 article people carlisle8 wife article al-sadr9 site funeral kaye10 mother million abc?s11 court board yiddish12 percent percent great-grandmotherTable 2: The first 12 anchor words selected bythree algorithms for the NYT corpus.The Gram-Schimdt process used by Arora etal.
greedily selects anchors in high-dimensionalspace.
As each word is represented within V -6In order to efficiently find an exact convex hull, we usethe Quickhull algorithm.Type # HR Top Words (Yelp)t-SNE 16 0 mexican good service great eat restaurant authentic deliciousPCA 15 0 mexican authentic eat chinese don?t restaurant fast salsaGreedy 34 35 good great food place service restaurant it?s mexicant-SNE 6 0 beer selection good pizza great wings tap nicePCA 39 6 wine beer selection nice list glass wines barGreedy 99 11 beer selection great happy place wine good bart-SNE 3 0 prices great good service selection price nice qualityPCA 12 0 atmosphere prices drinks friendly selection nice beer ambianceGreedy 34 35 good great food place service restaurant it?s mexicant-SNE 10 0 chicken salad good lunch sauce ordered fried soupPCA 10 0 chicken salad lunch fried pita time back sauceGreedy 69 12 chicken rice sauce fried ordered i?m spicy soupType # HR Top Words (Blog)t-SNE 10 0 hillary clinton campaign democratic bill party win racePCA 4 0 hillary clinton campaign democratic party bill democrats voteGreedy 45 19 obama hillary campaign clinton obama?s barack it?s democratict-SNE 3 0 iraq war troops iraqi mccain surge security americanPCA 9 1 iraq iraqi war troops military forces security americanGreedy 91 8 iraq mccain war bush troops withdrawal obama iraqit-SNE 9 0 allah muhammad qur verses unbelievers ibn muslims worldPCA 18 14 allah muhammad qur verses unbelievers story time updateGreedy 4 5 allah muhammad people qur verses unbelievers ibn surat-SNE 19 0 catholic abortion catholics life hagee time biden humanPCA 2 0 people it?s time don?t good make years palinGreedy 40 1 abortion parenthood planned people time state life governmentType # HR Top Words (NYT)t-SNE 0 0 police man yesterday officers shot officer year-old chargedPCA 6 0 people it?s police way those three back don?tGreedy 50 198 police man yesterday officers officer people street cityt-SNE 19 0 senator republican senate democratic democrat state billPCA 33 2 state republican republicans senate senator house bill partyGreedy 85 33 senator republican president state campaign presidential peoplet-SNE 2 0 business chief companies executive group yesterday billionPCA 21 0 billion companies business deal group chief states unitedGreedy 55 10 radio business companies percent day music article satellitet-SNE 14 0 market sales stock companies prices billion investors pricePCA 11 0 percent market rate week state those increase highGreedy 77 44 companies percent billion million group business chrysler peopleTable 3: Example t-SNE topics and their mostsimilar topics across algorithms.
The Greedy algo-rithm can find similar topics, but the anchor wordsare much less salient.dimensions, finding the word that has the nextmost distinctive co-occurrence pattern tends toprefer overly eccentric words with only short, in-tense bursts of co-occurring words.
While thebases corresponding to these anchor words couldbe theoretically relevant for the original space inhigh-dimension, they are less likely to be equallyimportant in low-dimensional space.
Thus project-ing down to low-dimensional space can rearrangethe points emphasizing not only uniqueness, butalso longevity, achieving the ability to form mea-surably more specific topics.Concretely, neither cavalry, al-sadr, yiddish norpolice, business, court are full representations ofNew York Times articles, but the latter is a muchbetter basis than the former due to its greater gen-erality.
We see the effect of this difference in thespecificity of the resulting topics (for example in17 obama topics).
Most words in the vocabularyhave little connection to the word cavalry, so theprobability p(z|w) does not change much acrossdifferent w. When we convert these distributionsinto P (w|z) using the Bayes?
rule, the resultingtopics are very close to the corpus distribution, a1323unigram distribution p(w).p(w|z = kcavalry) ?
p(z = kcavalry|w)p(w)?
p(w)This lack of specificity results in the observed sim-ilarity of topics.4.2 Quantitative ResultsIn this section we compare PCA and t-SNE pro-jections to the greedy algorithm along severalquantitative metrics.
To show the effect of dif-ferent values of K, we report results for varyingnumbers of topics.
As the anchor finding algo-rithms are deterministic, the anchor words in a K-dimensional model are identical to the first K an-chor words in a (K + 1)-dimensional model.
Forthe greedy algorithm we select anchor words inthe order they are chosen.
For the PCA and t-SNE methods, which find anchors jointly, we sortwords in descending order by their distance fromtheir centroid.Recovery Error.
Each non-anchor word is ap-proximated by a convex combination of the Kanchor words.
The projected gradient algorithm(Arora et al., 2013) determines these convex coef-ficients so that the gap between the original wordvector and the approximation becomes minimized.As choosing a good basis of K anchor words de-creases this gap, Recovery Error (RE) is definedby the average `2-residuals across all words.RE =1VV?i=1?
?Qi?K?k=1p(z1= k|w1= i)?QSk?2(5)Recovery error decreases with the number of top-Yelp Blog NYTimes0.000.010.020.030.040.050 30 60 90 0 30 60 90 0 30 60 90TopicsRecovery AlgorithmGreedyPCAtSNEFigure 4: Recovery error is similar across algo-rithms.ics, and improves substantially after the first 10?15anchor words for all methods.
The t-SNE methodhas slightly better performance than the greedy al-gorithm, but they are similar.
Results for recoverywith the original, unprojected matrix (not shown)are much worse than the other algorithms, sug-gesting that the initial anchor words chosen are es-pecially likely to be uninformative.Normalized Entropy.
As shown previously, ifthe probability of topics given a word is close touniform, the probability of that word in topics willbe close to the corpus distribution.
NormalizedEntropy (NE) measures the entropy of this distri-bution relative to the entropy of a K-dimensionaluniform distribution:NE =1VV?i=1H(z|w = i)logK.
(6)The normalized entropy of topics given word dis-Yelp Blog NYTimes0.250.500.751.000 30 60 90 0 30 60 90 0 30 60 90TopicsNormalizedEntropyAlgorithmGreedyPCAtSNEFigure 5: Words have higher topic entropy in thegreedy model, especially in NYT, resulting in lessspecific topics.tributions usually decreases as we add more top-ics, although both t-SNE and PCA show a dip inentropy for low numbers of topics.
This result in-dicates that words become more closely associatedwith particular topics as we increase the number oftopics.
The low-dimensional embedding methods(t-SNE and PCA) have consistently lower entropy.Topic Specificity and Topic Dissimilarity.
Wewant topics to be both specific (that is, not overlygeneral) and different from each other.
When thereare insufficient number of topics, p(w|z) often re-sembles the corpus distribution p(w), where highfrequency terms become the top words contribut-ing to most topics.
Topic Specificity (TS) is de-fined by the average KL divergence from eachtopic?s conditional distribution to the corpus dis-tribution.7TS =1KK?k=1KL(p(w|z = k) || p(w))(7)7We prefer specificity to (AlSumait et al., 2009)?s termvacuousness because the metric increases as we move awayfrom the corpus distribution.1324One way to define the distance between multiplepoints is the minimum radius of a ball that cov-ers every point.
Whereas this is simply the dis-tance form the centroid to the farthest point inthe Euclidean space, it is an itself difficult opti-mization problem to find such centroid of distri-butions under metrics such as KL-divergence andJensen-Shannon divergence.
To avoid this prob-lem, we measure Topic Dissimilarity (TD) view-ing each conditional distribution p(w|z) as a sim-ple V -dimensional vector in RV.
Recall aik=p(w = i|z = k),TD = max1?k?K?1KK?k?=1a?k??
a?k?2.
(8)Specificity and dissimilarity increase with theYelp Blog NYTimes0.00.51.01.52.00 30 60 90 0 30 60 90 0 30 60 90TopicsSpecificity AlgorithmGreedyPCAtSNEYelp Blog NYTimes0.00.20.40.60 30 60 90 0 30 60 90 0 30 60 90TopicsDissimilarity AlgorithmGreedyPCAtSNEFigure 6: Greedy topics look more like the corpusdistribution and more like each other.number of topics, suggesting that with few anchorwords, the topic distributions are close to the over-all corpus distribution and very similar to one an-other.
The t-SNE and PCA algorithms produceconsistently better specificity and dissimilarity, in-dicating that they produce more useful topics earlywith small numbers of topics.
The greedy algo-rithm produces topics that are closer to the corpusdistribution and less distinct from each other (17obama topics).Topic Coherence is known to correlate with thesemantic quality of topic judged by human anno-tators (Mimno et al., 2011).
LetW(T )kbe T mostprobable words (i.e., top words) for the topic k.TC =?w16=w2?W(T )klogD(w1, w2) + D(w1)(9)Here D(w1, w2) is the co-document frequency,which is the number of documents inD consistingof two words w1and w2simultaneously.
D(w)is the simple document frequency with the wordw.
The numerator contains smoothing count in order to avoid taking the logarithm of zero.Coherence scores for t-SNE and PCA are worseYelp Blog NYTimes?600?550?500?450?4000 30 60 90 0 30 60 90 0 30 60 90TopicsCoherence AlgorithmGreedyPCAtSNEFigure 7: The greedy algorithm creates more co-herent topics (higher is better), but at the cost ofmany overly general or repetitive topics.than those for the greedy method, but this resultmust be understood in combination with the Speci-ficity and Dissimilarity metrics.
The most frequentterms in the overall corpus distribution p(w) oftenappear together in documents.
Thus a model creat-ing many topics similar to the corpus distributionis likely to achieve high Coherence, but low Speci-ficity by definition.Saliency.
(Chuang et al., 2012) define saliencyfor topic words as a combination of distinctive-ness and probability within a topic.
Anchor wordsare distinctive by construction, so we can increasesaliency by selecting more probable anchor words.We measure the probability of anchor words intwo ways.
First, we report the zero-based rank ofanchor words within their topics.
Examples of thismetric, which we call ?hard?
rank are shown in Ta-ble 3.
The hard rank of the anchors in the PCA andt-SNE models are close to zero, while the anchorwords for the greedy algorithm are much lowerranked, well below the range usually displayed tousers.
Second, while hard rank measures the per-ceived difference in rank of contributing words,position may not fully capture the relative impor-tance of the anchor word.
?Soft?
rank quantifiesthe average log ratio between probabilities of the1325prominent word w?kand the anchor word sk.SR =1KK?k=1logp(w = w?k|z = k)p(w = sk|z = k)(10)Yelp Blog NYTimes012340 30 60 90 0 30 60 90 0 30 60 90TopicsSoftAnchorRank AlgorithmGreedyPCAtSNEFigure 8: Anchor words have higher probability,and therefore greater salience, in t-SNE and PCAmodels (1 ?
one third the probability of the topranked word).Lower values of soft rank (Fig.
8 indicate thatthe anchor word has greater relative probability tooccur within a topic.
As we increase the num-ber of topics, anchor words become more promi-nent in topics learned by the greedy method, butt-SNE anchor words remain relatively more prob-able within their topics as measured by soft rank.Held-out Probability.
Given an estimate ofthe topic-word matrix A, we can compute themarginal probability of held-out documents underthat model.
We use the left-to-right estimator in-troduced by (Wallach et al., 2009), which uses asequential algorithm similar to a Gibbs sampler.This method requires a smoothing parameter fordocument-topic Dirichlet distributions, which weset to ?k= 0.1.
We note that the greedy algo-Yelp Blog NYTimes?6.65?6.60?6.55?7.70?7.65?7.60?7.55?7.50?7.45?8.4?8.3?8.2?8.10 25 50 75 100 0 25 50 75 100 0 30 60 90TopicsHeldOutLL AlgorithmGreedyPCAtSNEFigure 9: t-SNE topics have better held-out prob-ability than greedy topics.rithm run on the original, unprojected matrix hasbetter held-out probability values than t-SNE forthe Yelp corpus, but as this method does not scaleto realistic vocabularies we compare here to thesparse random projection method used in (Aroraet al., 2013).
The t-SNE method appears to dobest, particularly in the NYT corpus, which has alarger vocabulary and longer training documents.The length of individual held-out documents hasno correlation with held-out probability.We emphasize that Held-out Probability is sen-sitive to smoothing parameters and should only beused in combination with a range of other topic-quality metrics.
In initial experiments, we ob-served significantly worse held-out performancefor the t-SNE algorithm.
This phenomenon wasbecause setting the probability of anchor words tozero for all but their own topics led to large neg-ative values in held-out log probability for thosewords.
As t-SNE tends to choose more frequentterms as anchor words, these ?spikes?
significantlyaffected overall probability estimates.
To make thecalculation more fair, we added 10?5to any zeroentries for anchor words in the topic-word matrixA across all models and renormalized.Because t-SNE is a stochastic model, differentinitializations can result in different embeddings.To evaluate how steady anchor word selection is,we ran five random initializations for each dataset.For the Yelp dataset, the number of anchor wordsvaries from 59 to 69, and 43 out of those are sharedacross at least four trials.
For the Blog dataset, thenumber of anchor words varies from 80 to 95, with56 shared across at least four trials.
For the NYTdataset, this number varies between 83 and 107,with 51 shared across at least four models.4.3 Qualitative ResultsTable 3 shows topics trained by three methods (t-SNE, PCA, and greedy) for all three datasets.
Foreach model, we select five topics at random fromthe t-SNE model, and then find the closest topicfrom each of the other models.
If anchor wordspresent in the top eight words, they are shown inboldface.A fundamental difference between anchor-based inference and traditional likelihood-basedinference is that we can give an order to top-ics according to their contribution to word co-occurrence convex hull.
This order is intrinsic tothe original algorithm, and we heuristically giveorders to t-SNE and PCA based on their contri-butions.
This order is listed as # in the previoustable.
For all but one topic, the closest topic fromthe greedy model has a higher order number than1326the associated t-SNE topic.
As shown above, thestandard algorithm tends to pick less useful anchorwords at the initial stage; only the later, higher or-dered topics are specific.The most clear distinction between models isthe rank of anchor words represented by HardRank for each topic.
Only one topic correspond-ing to (initial) has the anchor word which doesnot coincide with the top-ranked word.
For thegreedy algorithm, anchor words are often tens ofwords down the list in rank, indicating that theyare unlikely to find a connection to the topic?s se-mantic core.
In cases where the anchor word ishighly ranked (unbelievers, parenthood) the wordis a good indicator of the topic, but still less deci-sive.t-SNE and PCA are often consistent in their se-lection of anchor words, which provides usefulvalidation that low-dimensional embeddings dis-cern more relevant anchor words regardless of lin-ear vs non-linear projections.
Note that we areonly varying the anchor selection part of the An-chor Words algorithm in these experiments, recov-ering topic-word distributions in the same mannergiven anchor words.
As a result, any differencesbetween topics with the same anchor word (for ex-ample chicken) are due to the difference in eitherthe number of topics or the rest of anchor words.Since PCA suffers from a crowding problem inlower-dimensional projection (see Figure 2) andthe problem could be severe in a dataset with alarge vocabulary, t-SNE is more likely to find theproper number of anchors given a specified granu-larity.5 ConclusionOne of the main advantages of the anchor wordsalgorithm is that the running time is largely inde-pendent of corpus size.
Adding more documentswould not affect the size of the co-occurrence ma-trix, requiring more times to construct the co-occurrence matrix at the beginning.
While theinference is scalable depending only on the sizeof the vocabulary, finding quality anchor words iscrucial for the performance of the inference.
(Arora et al., 2013) presents a greedy anchorfinding algorithm that improves over previous lin-ear programming methods, but finding quality an-chor words remains an open problem in spec-tral topic inference.
We have shown that previ-ous approaches have several limitations.
Exhaus-tively finding anchor words by eliminating wordsthat are reproducible by other words (Arora etal., 2012) is impractical.
The anchor words se-lected by the greedy algorithm are overly eccen-tric, particularly at the early stages of the algo-rithm, causing topics to be poorly differentiated.We find that using low-dimensional embeddingsof word co-occurrence statistics allows us to ap-proximate a better convex hull.
The resultinganchor words are highly salient, being both dis-tinctive and probable.
The models trained withthese words have better quantitative and qualita-tive properties along various metrics.
Most im-portantly, using radically low-dimensional projec-tions allows us to provide users with clear visualexplanations for the model?s anchor word selec-tions.An interesting property of using low-dimensional embeddings is that the numberof topics depends only on the projecting dimen-sion.
Since we can efficiently find an exact convexhull in low-dimensional space, users can achievetopics with their preferred level of granularitiesby changing the projection dimension.
We donot insist this is the ?correct?
number of topicsfor a corpus, but this method, along with therange of metrics described in this paper, providesusers with additional perspective when choosing adimensionality that is appropriate for their needs.We find that the t-SNE method, besides itswell-known ability to produce high quality lay-outs, provides the best overall anchor selectionperformance.
This method consistently selectshigher-frequency terms as anchor words, resultingin greater clarity and interpretability.
Embeddingswith PCA are also effective, but they result in lesswell-formed spaces, being less effective in held-out probability for sufficiently large corpora.Anchor word finding methods based on low-dimensional projections offer several importantadvantages for topic model users.
In addition toproducing more salient anchor words that can beused effectively as topic labels, the relationship ofanchor words to a visualizable word co-occurrencespace offers significant potential.
Users who cansee why the algorithm chose a particular modelwill have greater confidence in the model and inany findings that result from topic-based analy-sis.
Finally, visualizable spaces offer the poten-tial to produce interactive environments for semi-supervised topic reconstruction.1327AcknowledgmentsWe thank David Bindel and the anonymous re-viewers for their valuable comments and sugges-tions, and Laurens van der Maaten for providinghis t-SNE implementation.ReferencesDimitris Achlioptas.
2001.
Database-friendly randomprojections.
In SIGMOD, pages 274?281.Loulwah AlSumait, Daniel Barbar, James Gentle, andCarlotta Domeniconi.
2009.
Topic significanceranking of lda generative models.
In ECML.S.
Arora, R. Ge, and A. Moitra.
2012.
Learning topicmodels ?
going beyond svd.
In FOCS.Sanjeev Arora, Rong Ge, Yonatan Halpern, DavidMimno, Ankur Moitra, David Sontag, Yichen Wu,and Michael Zhu.
2013.
A practical algorithm fortopic modeling with provable guarantees.
In ICML.D.
Blei, A. Ng, and M. Jordan.
2003.
Latent dirichletallocation.
Journal of Machine Learning Research,pages 993?1022.
Preliminary version in NIPS 2001.Christos Boutsidis, Michael W. Mahoney, and PetrosDrineas.
2009.
An improved approximation algo-rithm for the column subset selection problem.
InSODA, pages 968?977.Jason Chuang, Christopher D. Manning, and JeffreyHeer.
2012.
Termite: Visualization techniquesfor assessing textual topic models.
In InternationalWorking Conference on Advanced Visual Interfaces(AVI), pages 74?77.Jacob Eisenstein and Eric Xing.
2010.
The CMU2008 political blog corpus.
Technical report, CMU,March.Jacob Eisenstein, Duen Horng Chau, Aniket Kittur, andEric P. Xing.
2011.
Topicviz: Semantic navigationof document collections.
CoRR, abs/1110.6200.T.
L. Griffiths and M. Steyvers.
2004.
Finding scien-tific topics.
Proceedings of the National Academy ofSciences, 101:5228?5235.Ming Gu and Stanley C. Eisenstat.
1996.
Efficientalgorithms for computing a strong rank-revealing qrfactorization.
In SIAM J. Sci Comput, pages 848?869.William B. Johnson and Joram Lindenstrauss.
1984.Extensions of lipschitz mappings into a hilbertspace.
Contemporary Mathematics, 26:189?206.Simon Lacoste-Julien, Fei Sha, and Michael I. Jordan.2008.
DiscLDA: Discriminative learning for dimen-sionality reduction and classification.
In NIPS.Jey Han Lau, Karl Grieser, David Newman, and Tim-othy Baldwin.
2011.
Automatic labelling of topicmodels.
In HLT, pages 1536?1545.Qiaozhu Mei, Xuehua Shen, and ChengXiang Zhai.2007.
Automatic labeling of multinomial topicmodels.
In KDD, pages 490?499.David Mimno, Hanna Wallach, Edmund Talley,Miriam Leenders, and Andrew McCallum.
2011.Optimizing semantic coherence in topic models.
InEMNLP.D.
Sontag and D. Roy.
2011.
Complexity of inferencein latent dirichlet allocation.
In NIPS, pages 1008?1016.L.J.P.
van der Maaten and G.E.
Hinton.
2008.
Visu-alizing high-dimensional data using t-SNE.
JMLR,9:2579?2605, Nov.Hanna Wallach, Iain Murray, Ruslan Salakhutdinov,and David Mimno.
2009.
Evaluation methods fortopic models.
In ICML.Jun Zhu, Amr Ahmed, and Eric P. Xing.
2009.MedLDA: Maximum margin supervised topic mod-els for regression and classication.
In ICML.1328
