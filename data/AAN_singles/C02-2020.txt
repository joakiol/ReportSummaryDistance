Looking for candidate translational equivalents in specialized, comparablecorporaYun-Chuang Chiao and Pierre ZweigenbaumSTIM/DSI, Assistance Publique ?
H?pitaux de Paris &D?partement de Biomath?matiques, Universit?
Paris 6AbstractPrevious attempts at identifying translational equiv-alents in comparable corpora have dealt with verylarge ?general language?
corpora and words.
We ad-dress this task in a specialized domain, medicine,starting from smaller non-parallel, comparable cor-pora and an initial bilingual medical lexicon.
Wecompare the distributional contexts of source andtarget words, testing several weighting factors andsimilarity measures.
On a test set of frequently oc-curring words, for the best combination (the Jaccardsimilarity measure with or without tf:idf weight-ing), the correct translation is ranked first for 20% ofour test words, and is found in the top 10 candidatesfor 50% of them.
An additional reverse-translationfiltering step improves the precision of the top can-didate translation up to 74%, with a 33% recall.1 IntroductionOne of the issues that have to be addressedin cross-language information retrieval (CLIR,Grefenstette (1998b)) is that of query transla-tion, which relies on some form of bilinguallexicon.
Methods have been proposed to ac-quire a lexicon from corpora when such a lex-icon does not exist or is not complete enough(Fung and McKeown, 1997; Fung and Yee, 1998;Picchi and Peters, 1998; Rapp, 1999).
The presentwork addresses this issue in a specialized domain:medicine.
We aim at identifying French-Englishtranslation candidates from comparable medicalcorpora, extending an existing specialized bilinguallexicon.
These translational equivalents may thenbe used, e.g., for query expansion and translation.We first recall previous work on this topic, thenpresent the corpora and initial bilingual lexicon westart with, and the method we use to build, trans-fer and compare context vectors.
We finally pro-vide and discuss experimental results on a test setof French medical words.2 BackgroundSalton (1970) first demonstrated that with carefullyconstructed thesauri, cross-language retrieval canperform as well as monolingual retrieval.
In manyexperiments, parallel corpora have been used fortraining statistical models for bilingual lexicon com-pilation and disambiguation of query translation(Hiemstra et al, 1997; Littman et al, 1998).
A lim-iting factor in these experiments was an expensiveinvestment of human effort for collecting large-sizeparallel corpora, although Chen and Nie (2000)?sexperiments show a potential solution by automati-cally collecting parallel Web pages.Comparable corpora are ?texts which, thoughcomposed independently in the respective lan-guage communities, have the same communica-tive function?
(Laffling, 1992).
Such non-paralleltexts can become prevalent in the developmentof bilingual lexicons and in cross-language infor-mation research as they may be easier to col-lect than parallel corpora (Fung and Yee, 1998;Rapp, 1999; Picchi and Peters, 1998).
Amongthese, Rapp (1999) proposed that in any languagethere is a correlation between the cooccurrencesof words which are translations of each other.Fung and Yee (1998) demonstrated that the asso-ciations between a word and its context seedwords are preserved in comparable texts of dif-ferent languages.
By designing procedures toretrieve crosslingual lexical equivalents together,Picchi and Peters (1998) proposed that their systemcould have applications such as retrieving docu-ments containing terms or contexts which are se-mantically equivalent in more than one language.3 Collecting comparable medical corporaThe material for the present experiments con-sists of comparable medical corpora in Frenchand English and a French-English medical lexicon(Fung and Yee (1998) call its words ?seed words?
).3.1 ?Signs and Symptoms?
CorporaWe selected two medical corpora from Inter-net catalogs of medical web sites.
Some ofthese catalogs index web pages with controlledvocabulary keywords taken from the MeSHthesaurus (www.nlm.nih.gov/mesh/meshhome),among which CISMeF (French language med-ical web sites, www.chu-rouen.fr/cismef) andCliniWeb (English language medical web sites,www.ohsu.edu/cliniweb).
The MeSH thesaurusis hierarchically structured, so that it is easy toselect a subfield of medicine.
We chose the subtreeunder the MeSH concept ?Pathological Conditions,Signs and Symptoms?
(?C23?
), which is the bestrepresented in CISMeF.We compiled the 2,338 URLs indexed by CIS-MeF under that concept, and downloaded the cor-responding pages, plus the pages directly linked tothem, so that framesets or tables of contents be ex-panded.
9,787 pages were converted into plain textfrom HTML or PDF, yielding a 602,484-word cor-pus (41,295 unique words).
The initial pages shouldall be in French; the additional pages sometimeshappen to be foreign language versions of the ini-tial ones.
In the same line, we collected 2,019pages under 921 URLs indexed by CliniWeb, andobtained a 608,320-word English medical corpus(32,919 unique words).3.2 Base bilingual medical lexiconA base French-English lexicon of simple wordswas compiled from several sources.
On the onehand, an online French medical dictionary (Diction-naire M?dical Masson, www.atmedica.com) whichincludes English translations of most of its en-tries.
On the other hand, some international medicalterminologies which are available in both Englishand French.
We obtained these from the UMLSmetathesaurus, which includes French versions ofMeSH, WHOART, ICPC and their English coun-terparts (www.nlm.nih.gov/research/umls).
The re-sulting lexicon (see excerpt in table 1) contains18,437 entries, mainly specialized medical terms.When several translations of the same term areavailable, they are all listed.4 MethodsThe basis of the method is to find the target wordsthat have the most similar distributions with a givensource word.
We explain how distributional behav-ior is approximated through context vectors, howabarognosie abarognosisabarthrose abarthrosisabarticulaire abarticularabasie abasiaabattement prostrationabaxial abaxialabc?d?
abscessedabc?s abscessabdomen abdomen, bellyabdominal abdominalabdomino-g?nital abdominogenitalabdomino-thoracique abdominothoracicabdomino-v?sical abdominovesicalabducteur abducens, abducentTable 1: Lexicon excerptcontext vectors are transferred into target contextvectors, and how context vectors are compared.4.1 Computing context vectorsEach input corpus is segmented at non-alphanumeric characters.
Stop words are thenremoved, and a simple lemmatization is per-formed.
For English, we used a list of stopwords that we had from a former project.
ForFrench, we merged Savoy?s online stop words list(www.unine.ch/info/clef) with a list of our own.The S-stemmer algorithm (Harman, 1991) wasapplied to the English words.
Another simplestemmer was used for French; it handles some -sand -x endings.The context of occurrence of each word is thenapproximated by the bag of words that occur withina window of N words around any occurrence of that?pivot?
word.
In the experiments reported here, Nwas set to 3 (i.e., a seven-word window) to approxi-mate syntactic dependencies.
The context vector ofa pivot word j is the vector of all words in the cor-pus,1 where each word i is represented by its num-ber of occurrences occjiin that bag of words.A context vector is similar to a document (thedocument that would be produced by concatenatingthe windows around all the occurrences of the givenpivot word).
Therefore, weights that are used forwords in documents can be tested here in order toeliminate word-frequency effects and to emphasizesignificant word pairs.
Besides simple context fre-quency occji, two additional, alternative weights arecomputed: tf:idf and log likelihood.1We shall see below that actually, only a subset of the corpuswords will be kept in each vector.The formulas we used to compute tf:idf are thefollowing: the normalized frequency of a word iin a context j is tf ji=occjimax occwhere occjiis thenumber of occurrences of word i in the context of jand maxocc= maxijoccjiis the maximum numberof cooccurrences of any two words in the corpus;idfi= 1 + logmaxoccocci(Sparck Jones, 1979) whereocciis the total number of contexts in which i oc-curs in the corpus.For the computation of the log likelihood ratio,we used the following formula from Dunning:2loglike(a; b) =PijlogkijNCiRj= k11logk11NC1R1+k12logk12NC1R2+ k21logk21NC2R1+ k22logk22NC2R2;C1= k11+k12, C2= k21+k22, R1= k11+k21,R2= k12+ k22, N = k11+ k12+ k21+ k22;k11= # cooccurrences of word a and word b,k12= occa  k11, k21= occb  k11,k22= corpus size ?
k12?
k21+ k11.At the end of this step, each non-stop word inboth corpora has a weighted context vector.4.2 Transferring context vectorsWhen a translation is sought for a source word, itscontext vector is transferred into a target languagecontext vector, relying on the existing bilingual lex-icon.
Only the words in the bilingual lexicon canbe used in the transfer.
When several translationsare listed, only the first one is added to the targetcontext vector.
The result is a target-language con-text vector which is comparable to ?native?
contextvectors directly obtained from the target corpus.Let us now be more precise about the context-word space.
Since we want to compare contextvectors obtained through transfer with native con-text vectors, these two sorts of vectors should be-long to the same space, i.e., range over the sameset of context words.
A (target) word belongs tothis set iff (i) it occurs in the target corpus, (ii) itis listed in the bilingual lexicon, and (iii) (one of)its source counterpart(s) occurs in the source cor-pus.
This set corresponds to the ?seed words?
ofFung and Yee (1998).
Therefore, the dimension ofthe target context vectors is reduced to this set of?cross-language pivot words?.
In our experimentalsetting, 4,963 pivot words are used.4.3 Computing vector similarityGiven a transferred context vector, for each nativetarget vector, a similarity score is computed; a rank-2Posted on the ?corpora?
mailing list on 22/7/1997(helmer.hit.uib.no/corpora/1997-2/0148.html).ing list is built according to this score.
The tar-get words that ?own?
the best-ranked target vectorsare the words in the target corpus whose distribu-tions with respect to the bilingual pivot words arethe most similar to that of the source word; they areconsidered candidate translational equivalents.We used several similarity metrics for compar-ing pairs of vectors V and W (of length n): Jac-card (Romesburg, 1990) and cosine (Losee, 1998),each combined with the three different weightingschemes.
With k; l;m ranging from 1 to n:Jaccard(V;W ) =PkvkwkPkv2k+Plw2l Pmvmwmcos(V;W ) =PkvkwkpPkv2kpPlw2l4.4 ExperimentsThe present work performs a first evaluation of thismethod in a favorable, controlled setting.
It tests, ina ?leave-one-out?
style, whether the correct transla-tion of one of the source (French) words in the bilin-gual lexicon can be found among the target (En-glish) words of this lexicon, based on context vectorsimilarity.
To make similarity measures more re-liable, we selected the most frequent words in theEnglish corpus (Nocc> 100) whose French trans-lations were known in our lexicon.
Among these,we chose the most frequent ones (Nocc> 60) inthe French corpus.
This provides us with a test setof 95 French words (i) which are frequent in theFrench corpus, (ii) of which we know the correcttranslation, and (iii) such that this translation oc-curs often in the English corpus.
For each of theFrench test words, we computed a weighted con-text vector for each of the different weighting mea-sures (occji, tf:idf , log likelihood).
Then, using theabove-mentioned similarity measures (cosine, Jac-card), we compared this weighted vector with theset of cross-language pivot words?s context vectorscomputed from the English corpus.
We then pro-duced a ranked list of the top translational equiv-alents and tested whether the expected translationcan be differentiated from other well-known domainwords.
For the evaluation, we computed the rank ofthe expected translation of each test word and syn-thesized them as a percentile rank distribution.5 Initial ResultsTable 2 shows example results for the French wordsanxi?t?
and infection with different weightings andsimilarity measures.
For reasons of space, we onlyMeas.
Weight Fr word En word R Top 5 ranked candidate translationsCos.
occjianxi?t?
anxiety 1 anxiety .55, depression .45, medication .36, insomnia .36, memory .34Cos.
tf:idf anxi?t?
anxiety 1 anxiety .54, depression .41, eclipse .33, medication .29, psychiatrist .29Cos.
loglike anxi?t?
anxiety 1 anxiety .56, depression .43, eclipse .37, psychiatrist .36, dysthymia .33Jac.
occjianxi?t?
anxiety 2 memory .21, anxiety .21, insomnia .19, confusion .19, psychiatrist .18Jac.
tf:idf anxi?t?
anxiety 1 anxiety .21, psychiatrist .17, confusion .15, memory .14, phobia .14Jac.
loglike anxi?t?
anxiety 1 anxiety .26, psychiatrist .19, memory .15, phobia .14, depressed .14Cos.
occjiinfection infection 2 infected .55, infection .52, neurotropic .47, homosexual .43Cos.
tf:idf infection infection 3 infected .56, neurotropic .49, infection .48, aids .45, homosexual .41Cos.
loglike infection infection 2 infected .67, infection .55, neurotropic .53, aids .48, homosexual .48Jac.
occjiinfection infection 1 infection .33, aids .21, tract .17, positive .16, prevention .15Jac.
tf:idf infection infection 1 infection .27, aids .24, positive .17, hiv .15, virus .15Jac.
loglike infection infection 1 infection .38, aids .27, tract .18, infected .18, positive .17Table 2: Example results; R = rank of expected target English word for source French wordprint out the top 5 ranked words.
Rank refers to theperformance of our program, with a 1 meaning thatthe correct translation of the input French word wasfound as the first candidate.01020304050600 5 10 15 20cosine/tf.idfJaccard/tf.idfcosine/loglikeJaccard/loglikecosine/occJaccard/occFigure 1: Percentile rank of the measures.A percentile rank (figure 1) showed that using thecombination of occjiand Jaccard, about 20% of theFrench test words have their correct translation asthe first ranked word.
If we look at the best rankedwords, we find that they have a strong thematic rela-tion: e.g., anxiety, depression, psychiatrist, phobia,or infection, infected, aids, homosexual.6 Discussion and Improvement DirectionsAs the percentile rank figure showed, the combi-nation of context frequency weighting (occji) andJaccard gives an accuracy of about 20% for cor-rect translation which is followed by tf:idf /Jaccardmeasures.
However, if we look among the top 20ranked words, we can find that the tf:idf /Jaccardand tf:idf /cosine have better performance: morethan 60% of the words find their correct transla-tions within the top 20 words, which is much betterthan occji/Jaccard and occji/cosine.
It seems that theloglike weighting factor did not help to improve thetranslation performance; this is true when we com-bined it with the cosine measure, but with Jaccard,we can see an improvement at the 20th percentile.In some cases where the correct translation wasbadly ranked, the French test words have differentusages, which induces an important context diver-sity.
For instance, for the French word chirurgiewhose expected translation is surgery, we have astop ranked words pain, breast, desmoplasia, pro-cedure, metastatic..., and for m?decine (medicine),we have information, clinician, article, medical....For common words like, e.g., analyse/analysis andsang/blood, we have girdle, sample, statistic... foranalysis and output, collection, calorimetry... forblood as best ranked translations.As an attempt to improve the precision of theFrench-English translation method, the same modelwas applied in the reverse direction to find theFrench counterparts of the 10 top-scoring Englishcandidates.
We then kept only those English candi-dates that had the initial French source word amongtheir top 10 reverse translation candidates.
In thepresent settings, only 42 of the 95 French sourcewords remained, 38 of which kept exactly one En-glish candidate; among these, 27 are the expectedtranslation, and 1 is an adjective derived from theexpected translation (estomac/gastric).
The other4 words still have multiple translation candidates,which can be ordered according to their combinedsimilarity scores: for 2 of them, the top ranked can-didate is then correct, and 1 is a derived adjective(th?rapie/therapeutic).Altogether, if we propose the top ranked re-maining candidate according to this scheme, re-call/precision reach .31/.69, or .33/.74 if derived ad-jectives are considered acceptable.
This result is re-ally encouraging as it shows that the reverse appli-cation of the translation method to the English can-didate words improves its effectiveness.As a comparison, on a ?general language?
cor-pus, Rapp (1999) reports an accuracy of 65% atthe first percentile by using loglike weighting andcity-block metric.3 This difference in accuracymay be accounted for by the larger size of thecorpora (135 and 163 Mwords), the use of ageneral English-German lexicon (16,380 entries),and the consideration of word order within con-texts.
In Fung and McKeown (1997), a transla-tion model applied to a pair of unrelated languages(English/Japanese) with a random selection of testwords, many of them multi-word terms, gives a pre-cision around 30% when only the top candidate isproposed.Our bilingual lexicon does not include generalFrench and English words.
This implies that somecontexts are ignored: all cooccurrences of a special-ized word with a general word are lost in our case.We therefore plan to explore the effectiveness of in-corporating a general lexicon, as well as applyingPOS-tagging to the corpus.
An additional differ-ence with Fung and Yee (1998) is that they look fortranslational equivalents only among words that areunknown in both corpora.
This additional conditionmight also help to improve our current results.7 AcknowledgementsWe thank Jean-David Sta, Julien Quint and Beno?tHabert for their help during this work.ReferencesJiang Chen and J-Y.
Nie.
2000.
Parallel web textmining for cross-language IR.
In Proceedingsof RIAO 2000: Content-Based Multimedia In-formation Access, volume 1, pages 62?78, Paris,France, April.
C.I.D.3The city-block metric is computed as the sum of the abso-lute differences of corresponding vectors positions.Pascale Fung and Kathleen McKeown.
1997.
Find-ing terminology translations from non-parallecorpora.
In Proceedings of the 5th Annual Work-shop on Very Large Copora, volume 1, pages192?202, Hong Kong.Pascale Fung and L. Y. Yee.
1998.
An IR ap-proach for translating new words from non-parallel, comparable texts.
In Proceedings of the36th ACL, pages 414?420, Montr?al, August.Gregory Grefenstette.
1998a.
Cross-Language In-formation Retrieval.
Kluwer Academic Publish-ers, London.Gregory Grefenstette.
1998b.
The problemof cross-language information retrieval.In Cross-Language Information Retrieval(Grefenstette, 1998a), pages 1?9.D.
Harman.
1991.
How effective is suffixing.
Jour-nal of the American Society for Information Sci-ence, 42:7?15.D.
Hiemstra, F. de Jong, and W. Kraaij.
1997.
Adomain specific lexicon acquisition tool for cross-linguage information retrieval.
In Proceedings ofRIAO97, pages 217?232, Montreal, Canada.J.
Laffling.
1992.
On constructiong a transfer dic-tionary for man and machine.
Target, 4(1):17?31.M.L.
Littman, S.T.
Dumais, and T.K.
Landauer.1998.
Automatic cross-language information re-trieval using latent semantic indexing.
In Grefen-stette (Grefenstette, 1998a), chapter 5, pages 51?62.Robert M. Losee.
1998.
Text Retrieval and Filter-ing: Analytic Models of Performance, volume 3of Information Retrieval.
Kluwer Academic Pub-lishers, Dordrecht & Boston.E.
Picchi and C. Peters.
1998.
Cross-languageinformation retrieval: A system for com-parable corpus querying.
In Grefenstette(Grefenstette, 1998a), chapter 7, pages 81?90.Reinhard Rapp.
1999.
Automatic identificationof word translations from unrelated English andGerman corpora.
In Proceedings of the 37thACL, College Park, Maryland, June.H.
Charles Romesburg.
1990.
Cluster Analysis forResearchers.
Krieger, Malabar, FL.Gerald Salton.
1970.
Automatic processing of for-eign language documents.
Journal of the Ameri-can Society for Information Science, 21(3):187?194.Karen Sparck Jones.
1979.
Experiments in rel-evance weighting of search terms.
InformationProcessing and Management, 15:133?144.
