Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 300?304,Dublin, Ireland, August 23-24, 2014.haLF: Comparing a Pure CDSM Approach with a StandardMachine Learning System for RTELorenzo FerroneUniversity of Rome ?Tor Vergata?Via del Politecnico 100133 Roma, Italylorenzo.ferrone@gmail.comFabio Massimo ZanzottoUniversity of Rome ?Tor Vergata?Via del Politecnico 100133 Roma, Italyfabio.massimo.zanzotto@uniroma2.itAbstractIn this paper, we describe our sub-mission to the Shared Task #1.
Wetried to follow the underlying idea ofthe task, that is, evaluating the gapof full-fledged recognizing textual en-tailment systems with respect to com-positional distributional semantic mod-els (CDSMs) applied to this task.
Wethus submitted two runs: 1) a sys-tem obtained with a machine learningapproach based on the feature spacesof rules with variables and 2) a sys-tem completely based on a CDSM thatmixes structural and syntactic infor-mation by using distributed tree ker-nels.
Our analysis shows that, underthe same conditions, the fully CDSMsystem is still far from being competi-tive with more complex methods.1 IntroductionRecognizing Textual Entailment is a largelyexplored problem (Dagan et al., 2013).
Pastchallenges (Dagan et al., 2006; Bar-Haim etal., 2006; Giampiccolo et al., 2007) exploredmethods and models applied in complex andnatural texts.
In this context, machine learn-ing solutions show interesting results.
TheShared Task #1 of SemEval instead wants toexplore systems in a more controlled textualenvironment where the phenomena to modelare clearer.
The aim of the Shared Task is tostudy how RTE systems built upon composi-tional distributional semantic models behaveThis work is licenced under a Creative Commons At-tribution 4.0 International License.
Page numbers andproceedings footer are added by the organizers.
Licensedetails: http://creativecommons.org/licenses/by/4.0/with respect to the above tradition.
We triedto capture this underlying idea of the task.In this paper, we describe our submissionto the Shared Task #1.
We tried to fol-low the underlying idea of the task, that is,evaluating the gap of full-fledged recognizingtextual entailment systems with respect tocompositional distributional semantic models(CDSMs) applied to this task.
We thus sub-mitted two runs: 1) a system obtained with amachine learning approach based on the fea-ture spaces of rules with variables (Zanzottoet al., 2009) and 2) a system completely basedon a CDSM that mixes structural and syntac-tic information by using distributed tree ker-nels (Zanzotto and Dell?Arciprete, 2012).
Ouranalysis shows that, under the same condi-tions, the fully CDSM system is still far frombeing competitive with more complete meth-ods.The rest of the paper is organized as follows.Section 2 describes the full-fledged recognizingtextual entailment system that is used for com-parison.
Section 3 introduces a novel composi-tional distributional semantic model, namely,the distributed smoothed tree kernels, and theway this model is applied to the task of RTE.Section 4 describes the results in the challengeand it draws some preliminary conclusions.2 A Standard full-fledged MachineLearning Approach for RTEFor now on, the task of recognizing textual en-tailment (RTE) is defined as the task to decideif a pair p = (a, b) like:(?Two children are lying in the snow and aremaking snow angels?, ?Two angels aremaking snow on the lying children?
)is in entailment, in contradiction, or neutral.As in the tradition of applied machine learn-300ing models, the task is framed as a multi-classification problem.
The difficulty is to de-termine the best feature space on which totrain the classifier.A full-fledged RTE systems based on ma-chine learning that has to deal with naturaloccurring text is generally based on:?
some within-pair features that model thesimilarity between the sentence a and thesentence b?
some features representing more complexinformation of the pair (a, b) such as ruleswith variables that fire (Zanzotto andMoschitti, 2006)In the following, we describe the within-pairfeature and the syntactic rules with variablefeatures used in the full-fledged RTE system.As the second space of features is generallyhuge, the full feature space is generally used inkernel machines where the final kernel betweentwo instances p1= (a1, b1) and p2= (a2, b2) is:K(p1, p2) = FR(p1, p2) ++ (WTS(a1, b1) ?WTS(a2, b2) + 1)2where FR counts how many rules are in com-mon between p1and p2and WTS computes alexical similarity between a and b.
In the fol-lowing sections we describe the nature ofWTSand of FR2.1 Weighted Token Similarity (WTS)This similarity model was first defined bt Cor-ley and Mihalcea (2005) and since then hasbeen used by many RTE systems.
The modelextends a classical bag-of-word model to aWeighted-Bag-of-Word (wbow) by measuringsimilarity between the two sentences of thepair at the semantic level, instead of the lexicallevel.For example, consider the pair: ?Os-cars forgot Farrah Fawcett?, ?Farrah Fawcettsnubbed at Academy Awards?.
This pair isredundant, and, hence, should be assigneda very high similarity.
Yet, a bag-of-wordmodel would assign a low score, since manywords are not shared across the two sen-tences.
wbow fixes this problem by match-ing ?Oscar?-?Academy Awards?
and ?forgot?-?snubbed?
at the semantic level.
To providethese matches, wbow relies on specific wordsimilarity measures over WordNet (Miller,1995), that allow synonymy and hyperonymymatches: in our experiments we specificallyuse Jiang&Conrath similarity (Jiang and Con-rath, 1997).2.2 Rules with Variables as FeaturesThe above model alone is not sufficient tocapture all interesting entailment features asthe relation of entailment is not only relatedto the notion of similarity between a and b.In the tradition of RTE, an interesting featurespace is the one where each feature representsa rule with variables, i.e.
a first order rulethat is activated by the pairs if the variablesare unified.
This feature space has beenintroduced in (Zanzotto and Moschitti, 2006)and shown to improve over the one above.Each feature ?fr1, fr2?
is a pair of syntactictree fragments augmented with variables.The feature is active for a pair (t1, t2) if thesyntactic interpretations of t1and t2canbe unified with < fr1, fr2>.
For example,consider the following feature:?SPPPNPXVPHHVBPboughtNPY,SPPNPXVPHHVBPownsNPY?This feature is active for the pair (?GM boughtOpel?,?GM owns Opel?
), with the variableunificationX= ?GM ?
andY= ?Opel?.
Onthe contrary, this feature is not active for thepair (?GM bought Opel?,?Opel owns GM ?)
asthere is no possibility of unifying the two vari-ables.FR(p1, p2) is a kernel function that countsthe number of common rules with variablesbetween p1and p2.
Efficient algorithms forthe computation of the related kernel func-tions can be found in (Moschitti and Zanzotto,2007; Zanzotto and Dell?Arciprete, 2009; Zan-zotto et al., 2011).301S(t) = {S:booked::vQNP VP,VP:booked::vZV NP,NP:we::pPRP,S:booked::vZNPPRPVP, .
.
.
,VP:booked::vHHVbookedNPDT NN, .
.
.
}Figure 1: Subtrees of the tree t in Figure 2 (a non-exhaustive list.
)3 Distributed Smoothed TreeKernel: a CompositionalDistributional Semantic Modelfor RTEThe above full-fledged RTE system, althoughit may use distributional semantics, is not amodel that applies a compositional distribu-tional semantic model as it does not explic-itly transform sentences in vectors, matrices,or tensors that represent their meaning.We here propose a model that can be con-sidered a compositional distributional seman-tic model as it transforms sentences into ma-trices that are then used by the learner as fea-ture vectors.
Our model is called DistributedSmoothed Tree Kernel (Ferrone and Zanzotto,2014) as it mixes the distributed trees (Zan-zotto and Dell?Arciprete, 2012) representingsyntactic information with distributional se-mantic vectors representing semantic informa-tion.
The computation of the final matrix foreach sentence is done compositionally.S:booked::v````NP:we::pPRP:we::pWeVP:booked::vXXXXV:booked::vbookedNP:flight::nPPPDT:the::dtheNN:flight::nflightFigure 2: A lexicalized tree.3.1 NotationBefore describing the distributed smoothedtrees (DST) we introduce a formal way to de-note constituency-based lexicalized parse trees,as DSTs exploit this kind of data structures.Lexicalized trees are denoted with the letter tand N(t) denotes the set of non terminal nodesof tree t. Each non-terminal node n ?
N(t)has a label lncomposed of two parts ln=(sn, wn): snis the syntactic label, while wnisthe semantic headword of the tree headed byn, along with its part-of-speech tag.
Termi-nal nodes of trees are treated differently, thesenodes represent only words wnwithout anyadditional information, and their labels thusonly consist of the word itself (see Fig.
2).The structure of a DST is represented as fol-lows: Given a tree t, h(t) is its root node ands(t) is the tree formed from t but consideringonly the syntactic structure (that is, only thesnpart of the labels), ci(n) denotes i-th childof a node n. As usual for constituency-basedparse trees, pre-terminal nodes are nodes thathave a single terminal node as child.Finally, we use?wn?
Rkto denote the distri-butional vector for word wn, whereas T repre-sents the matrix of a tree t encoding structureand distributional meaning.3.2 The Method in a GlanceWe describe here the approach in a few sen-tences.
In line with tree kernels over struc-tures (Collins and Duffy, 2002), we introducethe set S(t) of the subtrees tiof a given lexi-calized tree t. A subtree tiis in the set S(t) ifs(ti) is a subtree of s(t) and, if n is a node inti, all the siblings of n in t are in ti.
For eachnode of tiwe only consider its syntactic labelsn, except for the head h(ti) for which we alsoconsider its semantic component wn(see Fig.1).
The functions DSTs we define compute thefollowing:DST (t) = T =?ti?S(t)Tiwhere Tiis the matrix associated to each sub-tree ti.
The similarity between two text frag-ments a and b represented as lexicalized treestaand tbcan be computed using the Frobeniusproduct between the two matrices Taand Tb,that is:?Ta,Tb?F=?tai?S(ta)tbj?S(tb)?Tai,Tbj?F(1)302We want to obtain that the product ?Tai,Tbj?Fapproximates the dot product between thedistributional vectors of the head words(?Tai,Tbj?F?
??h(tai),?h(tbj)?)
whenever the syn-tactic structure of the subtrees is the same(that is s(tai) = s(tbj)), and ?Tai,Tbj?F?
0 oth-erwise.
This property is expressed as:?Tai,Tbj?F?
?
(s(tai), s(tbj)) ?
??h(tai),?h(tbj)?
(2)To obtain the above property, we defineTi=?s(ti)?wh(ti)>where?s(ti) are distributed tree fragment(Zanzotto and Dell?Arciprete, 2012) for thesubtree t and?wh(ti)is the distributionalvector of the head of the subtree t. Dis-tributed tree fragments have the propertythat?s(ti)?s(tj) ?
?
(ti, tj).
Thus, given theimportant property of the outer productthat applies in the Frobenius product:?
?a?w>,?b?v>?F= ?
?a ,?b ?
?
?
?w,?v ?.
we have thatEquation 2 is satisfied as:?Ti,Tj?F= ??s(ti),?s(tj)?
?
??wh(ti),?wh(tj)??
?
(s(ti), s(tj)) ?
?
?wh(ti),?wh(tj)?It is possible to show that the overall com-positional distributional model DST (t) can beobtained with a recursive algorithm that ex-ploit vectors of the nodes of the tree.The compositional distributional model isthen used in the same learning machine usedfor the traditional RTE system with the fol-lowing kernel function:K(p1, p2) =?DST (a1), DST (a2)?+ ?DST (b1), DST (b2)?++(WTS(a1, b1) ?WTS(a2, b2) + 1)24 Results and ConclusionsFor the submission we used the java ver-sion of LIBSVM (Chang and Lin, 2011).Distributional vectors are derived withDISSECT (Dinu et al., 2013) from acorpus obtained by the concatenation ofukWaC (wacky.sslmit.unibo.it), a mid-2009 dump of the English WikipediaModel Accuracy (3-ways)DST 69.42full-fledged RTE System 75.66Max 84.57Min 48.73Average 75.35Table 1: Accuracies of the two systems on thetest set, together with the maximum, mini-mum and average score for the challenge.
(en.wikipedia.org) and the British Na-tional Corpus (www.natcorp.ox.ac.uk), for atotal of about 2.8 billion words.
The raw co-occurrences count vectors were transformedinto positive Pointwise Mutual Informationscores and reduced to 300 dimensions bySingular Value Decomposition.
This setupwas picked without tuning, as we found iteffective in previous, unrelated experiments.We parsed the sentence with the StanfordParser (Klein and Manning, 2003) and ex-tracted the heads for use in the lexicalizedtrees with Collins?
rules (Collins, 2003).Table 1 reports our results on the textual en-tailment classification task, together with themaximum, minimum and average score for thechallenge.
The first observation is that thefull-fledged RTE system is still definitely bet-ter than our CDSM system.
We believe thatthe main reason is that the DST cannot en-code variables which is an important aspectto capture when dealing with textual entail-ment recognition.
This is particularly truefor this dataset as it focuses on word order-ing and on specific and recurrent entailmentrules.
Our full-fledged system scored amongthe first 10 systems, slightly above the over-all average score, but our pure CDSM systemis instead ranked within the last 3.
We thinkthat a more in-depth comparison with otherfully CDSM systems will give us a better in-sight on our model and will also assess morerealistically the quality of our system.ReferencesRoy Bar-Haim, Ido Dagan, Bill Dolan, Lisa Ferro,Danilo Giampiccolo, Bernardo Magnini, andIdan Szpektor.
2006.
The second pascal recog-nising textual entailment challenge.
In Proceed-ings of the Second PASCAL Challenges Work-303shop on Recognising Textual Entailment.
Venice,Italy.Chih-Chung Chang and Chih-Jen Lin.
2011.LIBSVM: A library for support vector ma-chines.
ACM Transactions on Intelligent Sys-tems and Technology, 2:27:1?27:27.
Soft-ware available at http://www.csie.ntu.edu.tw/~cjlin/libsvm.Michael Collins and Nigel Duffy.
2002.
New rank-ing algorithms for parsing and tagging: Kernelsover discrete structures, and the voted percep-tron.
In Proceedings of ACL02.Michael Collins.
2003.
Head-driven statisticalmodels for natural language parsing.
Comput.Linguist., 29(4):589?637.Courtney Corley and Rada Mihalcea.
2005.
Mea-suring the semantic similarity of texts.
In Proc.of the ACL Workshop on Empirical Modelingof Semantic Equivalence and Entailment, pages13?18.
Association for Computational Linguis-tics, Ann Arbor, Michigan, June.Ido Dagan, Oren Glickman, and BernardoMagnini.
2006.
The pascal recognising tex-tual entailment challenge.
In Quionero-Candelaet al., editor, LNAI 3944: MLCW 2005, pages177?190.
Springer-Verlag, Milan, Italy.Ido Dagan, Dan Roth, Mark Sammons, andFabio Massimo Zanzotto.
2013.
RecognizingTextual Entailment: Models and Applications.Synthesis Lectures on Human Language Tech-nologies.
Morgan & Claypool Publishers.Georgiana Dinu, Nghia The Pham, and MarcoBaroni.
2013.
DISSECT: DIStributional SE-mantics Composition Toolkit.
In Proceedingsof ACL (System Demonstrations), pages 31?36,Sofia, Bulgaria.Lorenzo Ferrone and Fabio Massimo Zanzotto.2014.
Towards syntax-aware compositional dis-tributional semantic models.
In Proceedings ofColing 2014.
COLING, Dublin, Ireland, Aug 23?Aug 29.Danilo Giampiccolo, Bernardo Magnini, Ido Da-gan, and Bill Dolan.
2007.
The third pas-cal recognizing textual entailment challenge.
InProceedings of the ACL-PASCAL Workshop onTextual Entailment and Paraphrasing, pages 1?9.
Association for Computational Linguistics,Prague, June.Jay J. Jiang and David W. Conrath.
1997.
Seman-tic similarity based on corpus statistics and lex-ical taxonomy.
In Proc.
of the 10th ROCLING,pages 132?139.
Tapei, Taiwan.Dan Klein and Christopher D. Manning.
2003.Accurate unlexicalized parsing.
In Proceedingsof the 41st Annual Meeting on Association forComputational Linguistics - Volume 1, ACL ?03,pages 423?430, Stroudsburg, PA, USA.
Associ-ation for Computational Linguistics.George A. Miller.
1995.
WordNet: A lexicaldatabase for English.
Communications of theACM, 38(11):39?41, November.Alessandro Moschitti and Fabio Massimo Zan-zotto.
2007.
Fast and effective kernels for re-lational learning from texts.
In Proceedings ofthe International Conference of Machine Learn-ing (ICML).
Corvallis, Oregon.Fabio Massimo Zanzotto and LorenzoDell?Arciprete.
2009.
Efficient kernels forsentence pair classification.
In Conferenceon Empirical Methods on Natural LanguageProcessing, pages 91?100, 6-7 August.F.M.
Zanzotto and L. Dell?Arciprete.
2012.
Dis-tributed tree kernels.
In Proceedings of Interna-tional Conference on Machine Learning, pages193?200.Fabio Massimo Zanzotto and Alessandro Mos-chitti.
2006.
Automatic learning of textual en-tailments with cross-pair similarities.
In Pro-ceedings of the 21st Coling and 44th ACL, pages401?408.
Sydney, Australia, July.Fabio Massimo Zanzotto, Marco Pennacchiotti,and Alessandro Moschitti.
2009.
A machinelearning approach to textual entailment recog-nition.
NATURAL LANGUAGE ENGINEER-ING, 15-04:551?582.Fabio Massimo Zanzotto, Lorenzo Dell?Arciprete,and Alessandro Moschitti.
2011.
Efficient graphkernels for textual entailment recognition.
Fun-damenta Informaticae, 107(2-3):199 ?
222.304
