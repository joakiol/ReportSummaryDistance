Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 282?292,Baltimore, Maryland, USA, June 23-25 2014.c?2014 Association for Computational LinguisticsModelling function words improves unsupervised word segmentationMark Johnson1,2, Anne Christophe3,4, Katherine Demuth2,6and Emmanuel Dupoux3,51Department of Computing, Macquarie University, Sydney, Australia2Santa Fe Institute, Santa Fe, New Mexico, USA3Ecole Normale Sup?erieure, Paris, France4Centre National de la Recherche Scientifique, Paris, France5Ecole des Hautes Etudes en Sciences Sociales, Paris, France6Department of Linguistics, Macquarie University, Sydney, AustraliaAbstractInspired by experimental psychologicalfindings suggesting that function wordsplay a special role in word learning, wemake a simple modification to an AdaptorGrammar based Bayesian word segmenta-tion model to allow it to learn sequencesof monosyllabic ?function words?
at thebeginnings and endings of collocationsof (possibly multi-syllabic) words.
Thismodification improves unsupervised wordsegmentation on the standard Bernstein-Ratner (1987) corpus of child-directed En-glish by more than 4% token f-score com-pared to a model identical except that itdoes not special-case ?function words?,setting a new state-of-the-art of 92.4% to-ken f-score.
Our function word model as-sumes that function words appear at theleft periphery, and while this is true oflanguages such as English, it is not trueuniversally.
We show that a learner canuse Bayesian model selection to determinethe location of function words in their lan-guage, even though the input to the modelonly consists of unsegmented sequences ofphones.
Thus our computational modelssupport the hypothesis that function wordsplay a special role in word learning.1 IntroductionOver the past two decades psychologists have in-vestigated the role that function words might playin human language acquisition.
Their experimentssuggest that function words play a special role inthe acquisition process: children learn functionwords before they learn the vast bulk of the asso-ciated content words, and they use function wordsto help identify context words.The goal of this paper is to determine whethercomputational models of human language acqui-sition can provide support for the hypothesis thatfunction words are treated specially in humanlanguage acquisition.
We do this by comparingtwo computational models of word segmentationwhich differ solely in the way that they modelfunction words.
Following Elman et al (1996)and Brent (1999) our word segmentation modelsidentify word boundaries from unsegmented se-quences of phonemes corresponding to utterances,effectively performing unsupervised learning of alexicon.
For example, given input consisting ofunsegmented utterances such as the following:j u w ?
n t t u s i ?
?
b ?
ka word segmentation model should segment this asju w?nt tu si ??
b?k, which is the IPA representationof ?you want to see the book?.We show that a model equipped with the abil-ity to learn some rudimentary properties of thetarget language?s function words is able to learnthe vocabulary of that language more accuratelythan a model that is identical except that it is inca-pable of learning these generalisations about func-tion words.
This suggests that there are acqui-sition advantages to treating function words spe-cially that human learners could take advantage of(at least to the extent that they are learning similargeneralisations as our models), and thus supportsthe hypothesis that function words are treated spe-cially in human lexical acquisition.
As a reviewerpoints out, we present no evidence that childrenuse function words in the way that our model does,and we want to emphasise we make no such claim.While absolute accuracy is not directly relevantto the main point of the paper, we note that themodels that learn generalisations about functionwords perform unsupervised word segmentationat 92.5% token f-score on the standard Bernstein-Ratner (1987) corpus, which improves the previ-ous state-of-the-art by more than 4%.As a reviewer points out, the changes we maketo our models to incorporate function words canbe viewed as ?building in?
substantive informa-tion about possible human languages.
The model282that achieves the best token f-score expects func-tion words to appear at the left edge of phrases.While this is true for languages such as English,it is not true universally.
By comparing the pos-terior probability of two models ?
one in whichfunction words appear at the left edges of phrases,and another in which function words appear at theright edges of phrases ?
we show that a learnercould use Bayesian posterior probabilities to deter-mine that function words appear at the left edgesof phrases in English, even though they are nottold the locations of word boundaries or whichwords are function words.This paper is structured as follows.
Section 2describes the specific word segmentation mod-els studied in this paper, and the way we ex-tended them to capture certain properties of func-tion words.
The word segmentation experimentsare presented in section 3, and section 4 discusseshow a learner could determine whether functionwords occur on the left-periphery or the right-periphery in the language they are learning.
Sec-tion 5 concludes and describes possible futurework.
The rest of this introduction provides back-ground on function words, the Adaptor Grammarmodels we use to describe lexical acquisition andthe Bayesian inference procedures we use to inferthese models.1.1 Psychological evidence for the role offunction words in word learningTraditional descriptive linguistics distinguishesfunction words, such as determiners and prepo-sitions, from content words, such as nouns andverbs, corresponding roughly to the distinction be-tween functional categories and lexical categoriesof modern generative linguistics (Fromkin, 2001).Function words differ from content words in atleast the following ways:1. there are usually far fewer function wordtypes than content word types in a language2.
function word types typically have muchhigher token frequency than content wordtypes3.
function words are typically morphologicallyand phonologically simple (e.g., they are typ-ically monosyllabic)4. function words typically appear in peripheralpositions of phrases (e.g., prepositions typi-cally appear at the beginning of prepositionalphrases)5. each function word class is associated withspecific content word classes (e.g., deter-miners and prepositions are associated withnouns, auxiliary verbs and complementisersare associated with main verbs)6. semantically, content words denote sets ofobjects or events, while function words de-note more complex relationships over the en-tities denoted by content words7.
historically, the rate of innovation of functionwords is much lower than the rate of innova-tion of content words (i.e., function words aretypically ?closed class?, while content wordsare ?open class?
)Properties 1?4 suggest that function wordsmight play a special role in language acquisitionbecause they are especially easy to identify, whileproperty 5 suggests that they might be useful foridentifying lexical categories.
The models westudy here focus on properties 3 and 4, in thatthey are capable of learning specific sequences ofmonosyllabic words in peripheral (i.e., initial orfinal) positions of phrase-like units.A number of psychological experiments haveshown that infants are sensitive to the functionwords of their language within their first year oflife (Shi et al, 2006; Hall?e et al, 2008; Shaferet al, 1998), often before they have experiencedthe ?word learning spurt?.
Crucially for our pur-pose, infants of this age were shown to exploitfrequent function words to segment neighboringcontent words (Shi and Lepage, 2008; Hall?e etal., 2008).
In addition, 14 to 18-month-oldchildren were shown to exploit function words toconstrain lexical access to known words - for in-stance, they expect a noun after a determiner (Cau-vet et al, 2014; Kedar et al, 2006; Zangl andFernald, 2007).
In addition, it is plausible thatfunction words play a crucial role in children?sacquisition of more complex syntactic phenom-ena (Christophe et al, 2008; Demuth and McCul-lough, 2009), so it is interesting to investigate theroles they might play in computational models oflanguage acquisition.1.2 Adaptor grammarsAdaptor grammars are a framework for Bayesianinference of a certain class of hierarchical non-parametric models (Johnson et al, 2007b).
Theydefine distributions over the trees specified bya context-free grammar, but unlike probabilisticcontext-free grammars, they ?learn?
distributionsover the possible subtrees of a user-specified set of?adapted?
nonterminals.
(Adaptor grammars arenon-parametric, i.e., not characterisable by a finite283set of parameters, if the set of possible subtreesof the adapted nonterminals is infinite).
Adaptorgrammars are useful when the goal is to learn apotentially unbounded set of entities that need tosatisfy hierarchical constraints.
As section 2 ex-plains in more detail, word segmentation is sucha case: words are composed of syllables and be-long to phrases or collocations, and modelling thisstructure improves word segmentation accuracy.Adaptor Grammars are formally defined inJohnson et al (2007b), which should be consultedfor technical details.
Adaptor Grammars (AGs)are an extension of Probabilistic Context-FreeGrammars (PCFGs), which we describe first.
AContext-Free Grammar (CFG) G = (N,W,R, S)consists of disjoint finite sets of nonterminal sym-bols N and terminal symbols W , a finite set ofrules R of the form A??
where A ?
N and?
?
(N ?
W )?, and a start symbol S ?
N .
(Weassume there are no ??-rules?
inR, i.e., we requirethat |?| ?
1 for each A??
?
R).A Probabilistic Context-Free Grammar (PCFG)is a quintuple (N,W,R, S,?)
where N , W , Rand S are the nonterminals, terminals, rules andstart symbol of a CFG respectively, and ?
is a vec-tor of non-negative reals indexed by R that sat-isfy???RA?A?
?= 1 for each A ?
N , whereRA= {A??
: A??
?
R} is the set of rulesexpanding A.Informally, ?A?
?is the probability of a nodelabelled A expanding to a sequence of nodes la-belled ?, and the probability of a tree is the prod-uct of the probabilities of the rules used to con-struct each non-leaf node in it.
More precisely, foreach X ?
N ?W a PCFG associates distributionsGXover the set of trees TXgenerated by X asfollows:If X ?
W (i.e., if X is a terminal) then GXis the distribution that puts probability 1 on thesingle-node tree labelled X .If X ?
N (i.e., if X is a nonterminal) then:GX=?X?B1...Bn?RX?X?B1...BnTDX(GB1, .
.
.
, GBn) (1)where RXis the subset of rules in R expandingnonterminal X ?
N , and:TDX(G1, .
.
.
, Gn)(.Xt1tn.
.
.
)=n?i=1Gi(ti).That is, TDX(G1, .
.
.
, Gn) is a distribution overthe set of trees TXgenerated by nonterminal X ,where each subtree tiis generated independentlyfrom Gi.
The PCFG generates the distribution GSover the set of trees TSgenerated by the start sym-bol S; the distribution over the strings it generatesis obtained by marginalising over the trees.In a Bayesian PCFG one puts Dirichlet priorsDir(?)
on the rule probability vector ?, such thatthere is one Dirichlet parameter ?A?
?for eachrule A??
?
R. There are Markov Chain MonteCarlo (MCMC) and Variational Bayes proceduresfor estimating the posterior distribution over ruleprobabilities ?
and parse trees given data consist-ing of terminal strings alone (Kurihara and Sato,2006; Johnson et al, 2007a).PCFGs can be viewed as recursive mixturemodels over trees.
While PCFGs are expres-sive enough to describe a range of linguistically-interesting phenomena, PCFGs are parametricmodels, which limits their ability to describe phe-nomena where the set of basic units, as well astheir properties, are the target of learning.
Lexi-cal acqusition is an example of a phenomenon thatis naturally viewed as non-parametric inference,where the number of lexical entries (i.e., words)as well as their properties must be learnt from thedata.It turns out there is a straight-forward modifica-tion to the PCFG distribution (1) that makes it suit-ably non-parametric.
As Johnson et al (2007b)explain, by inserting a Dirichlet Process (DP)or Pitman-Yor Process (PYP) into the generativemechanism (1) the model ?concentrates?
mass ona subset of trees (Teh et al, 2006).
Specifically,an Adaptor Grammar identifies a subset A ?
Nof adapted nonterminals.
In an Adaptor Gram-mar the unadapted nonterminals N \ A expandvia (1), just as in a PCFG, but the distributions ofthe adapted nonterminals A are ?concentrated?
bypassing them through a DP or PYP:HX=?X?B1...Bn?RX?X?B1...BnTDX(GB1, .
.
.
, GBn)GX= PYP(HX, aX, bX)Here aXand bXare parameters of the PYP asso-ciated with the adapted nonterminal X .
As Gold-water et al (2011) explain, such Pitman-Yor Pro-cesses naturally generate power-law distributeddata.Informally, Adaptor Grammars can be viewedas caching entire subtrees of the adapted nonter-minals.
Roughly speaking, the probability of gen-erating a particular subtree of an adapted nonter-minal is proportional to the number of times thatsubtree has been generated before.
This ?rich get284richer?
behaviour causes the distribution of sub-trees to follow a power-law (the power is speci-fied by the aXparameter of the PYP).
The PCFGrules expanding an adapted nonterminal X de-fine the ?base distribution?
of the associated DPor PYP, and the aXand bXparameters determinehow much mass is reserved for ?new?
trees.There are several different procedures for infer-ring the parse trees and the rule probabilities givena corpus of strings: Johnson et al (2007b) describeaMCMC sampler and Cohen et al (2010) describea Variational Bayes procedure.
We use the MCMCprocedure here since this has been successfully ap-plied to word segmentation problems in previouswork (Johnson, 2008).2 Word segmentation with AdaptorGrammarsPerhaps the simplest word segmentation model isthe unigram model, where utterances are modeledas sequences of words, and where each word isa sequence of segments (Brent, 1999; Goldwateret al, 2009).
A unigram model can be expressedas an Adaptor Grammar with one adapted non-terminal Word (we indicate adapted nonterminalsby underlining them in grammars here; regular ex-pressions are expanded into right-branching pro-ductions).Sentence?Word+(2)Word?Phone+(3)The first rule (2) says that a sentence consists ofone or more Words, while the second rule (3)states that a Word consists of a sequence of one ormore Phones; we assume that there are rules ex-panding Phone into all possible phones.
BecauseWord is an adapted nonterminal, the adaptor gram-mar memoises Word subtrees, which correspondsto learning the phone sequences for the words ofthe language.The more sophisticated Adaptor Grammars dis-cussed below can be understood as specialis-ing either the first or the second of the rulesin (2?3).
The next two subsections review theAdaptor Grammar word segmentation models pre-sented in Johnson (2008) and Johnson and Gold-water (2009): section 2.1 reviews how phonotac-tic syllable-structure constraints can be expressedwith Adaptor Grammars, while section 2.2 re-views how phrase-like units called ?collocations?capture inter-word dependencies.
Section 2.3presents the major novel contribution of this paperby explaining how we modify these adaptor gram-mars to capture some of the special properties offunction words.2.1 Syllable structure and phonotacticsThe rule (3) models words as sequences of inde-pendently generated phones: this is what Gold-water et al (2009) called the ?monkey model?
ofword generation (it instantiates the metaphor thatword types are generated by a monkey randomlybanging on the keys of a typewriter).
However, thewords of a language are typically composed of oneor more syllables, and explicitly modelling the in-ternal structure of words typically improves wordsegmentation considerably.Johnson (2008) suggested replacing (3) with thefollowing model of word structure:Word?
Syllable1:4(4)Syllable?
(Onset)Rhyme (5)Onset?Consonant+(6)Rhyme?Nucleus (Coda) (7)Nucleus?Vowel+(8)Coda?Consonant+(9)Here and below superscripts indicate iteration(e.g., a Word consists of 1 to 4 Syllables), whilean Onset consists of an unbounded number ofConsonants), while parentheses indicate option-ality (e.g., a Rhyme consists of an obligatoryNucleus followed by an optional Coda).
We as-sume that there are rules expanding Consonantand Vowel to the set of all consonants and vow-els respectively (this amounts to assuming that thelearner can distinguish consonants from vowels).Because Onset, Nucleus and Coda are adapted,this model learns the possible syllable onsets, nu-cleii and coda of the language, even though neithersyllable structure nor word boundaries are explic-itly indicated in the input to the model.The model just described assumes that word-internal syllables have the same structure as word-peripheral syllables, but in languages such asEnglish word-peripheral onsets and codas canbe more complex than the corresponding word-internal onsets and codas.
For example, theword ?string?
begins with the onset cluster str,which is relatively rare word-internally.
Johnson(2008) showed that word segmentation accuracyimproves if the model can learn different conso-nant sequences for word-inital onsets and word-final codas.
It is easy to express this as an Adaptor285Grammar: (4) is replaced with (10?11) and (12?17) are added to the grammar.Word?
SyllableIF (10)Word?
SyllableI Syllable0:2SyllableF (11)SyllableIF?
(OnsetI)RhymeF (12)SyllableI?
(OnsetI)Rhyme (13)SyllableF?
(Onset)RhymeF (14)OnsetI?Consonant+(15)RhymeF?Nucleus (CodaF) (16)CodaF?Consonant+(17)In this grammar the suffix ?I?
indicates a word-initial element, and ?F?
indicates a word-final el-ement.
Note that the model simply has the abil-ity to learn that different clusters can occur word-peripherally and word-internally; it is not givenany information about the relative complexity ofthese clusters.2.2 Collocation models of inter-worddependenciesGoldwater et al (2009) point out the detrimentaleffect that inter-word dependencies can have onword segmentation models that assume that thewords of an utterance are independently gener-ated.
Informally, a model that generates words in-dependently is likely to incorrectly segment multi-word expressions such as ?the doggie?
as singlewords because the model has no way to captureword-to-word dependencies, e.g., that ?doggie?
istypically preceded by ?the?.
Goldwater et alshowthat word segmentation accuracy improves whenthe model is extended to capture bigram depen-dencies.Adaptor grammar models cannot express bi-gram dependencies, but they can capture similiarinter-word dependencies using phrase-like unitsthat Johnson (2008) calls collocations.
John-son and Goldwater (2009) showed that word seg-mentation accuracy improves further if the modellearns a nested hierarchy of collocations.
This canbe achieved by replacing (2) with (18?21).Sentence?Colloc3+(18)Colloc3?Colloc2+(19)Colloc2?Colloc1+(20)Colloc1?Word+(21)Informally, Colloc1, Colloc2 and Colloc3 define anested hierarchy of phrase-like units.
While notdesigned to correspond to syntactic phrases, by ex-amining the sample parses induced by the AdaptorGrammar we noticed that the collocations oftencorrespond to noun phrases, prepositional phrasesor verb phrases.
This motivates the extension tothe Adaptor Grammar discussed below.2.3 Incorporating ?function words?
intocollocation modelsThe starting point and baseline for our extensionis the adaptor grammar with syllable structurephonotactic constraints and three levels of collo-cational structure (5-21), as prior work has foundthat this yields the highest word segmentation to-ken f-score (Johnson and Goldwater, 2009).Our extension assumes that the Colloc1 ?Colloc3 constituents are in fact phrase-like, so weextend the rules (19?21) to permit an optional se-quence of monosyllabic words at the left edgeof each of these constituents.
Our model thuscaptures two of the properties of function wordsdiscussed in section 1.1: they are monosyllabic(and thus phonologically simple), and they appearon the periphery of phrases.
(We put ?functionwords?
in scare quotes below because our modelonly approximately captures the linguistic proper-ties of function words).Specifically, we replace rules (19?21) with thefollowing sequence of rules:Colloc3?(FuncWords3)Colloc2+(22)Colloc2?(FuncWords2)Colloc1+(23)Colloc1?(FuncWords1)Word+(24)FuncWords3?FuncWord3+(25)FuncWord3?
SyllableIF (26)FuncWords2?FuncWord2+(27)FuncWord2?
SyllableIF (28)FuncWords1?FuncWord1+(29)FuncWord1?
SyllableIF (30)This model memoises (i.e., learns) both the in-dividual ?function words?
and the sequences of?function words?
that modify the Colloc1 ?Colloc3 constituents.
Note also that ?functionwords?
expand directly to SyllableIF, which inturn expands to a monosyllable with a word-initialonset and word-final coda.
This means that ?func-tion words?
are memoised independently of the?content words?
that Word expands to; i.e., themodel learns distinct ?function word?
and ?con-tent word?
vocabularies.
Figure 1 depicts a sampleparse generated by this grammar.286.SentenceColloc3FuncWords3FuncWord3youFuncWord3wantFuncWord3toColloc2Colloc1WordseeColloc1FuncWords1FuncWord1theWordbookFigure 1: A sample parse generated by the ?func-tion word?
Adaptor Grammar with rules (10?18)and (22?30).
To simplify the parse we only showthe root node and the adapted nonterminals, andreplace word-internal structure by the word?s or-thographic form.This grammar builds in the fact that functionwords appear on the left periphery of phrases.
Thisis true of languages such as English, but is not truecross-linguistically.
For comparison purposes wealso include results for a mirror-image model thatpermits ?function words?
on the right periphery,a model which permits ?function words?
on boththe left and right periphery (achieved by changingrules 22?24), as well as a model that analyses allwords as monosyllabic.Section 4 explains how a learner could useBayesian model selection to determine that func-tion words appear on the left periphery in Englishby comparing the posterior probability of the dataunder our ?function word?
Adaptor Grammar tothat obtained using a grammar which is identi-cal except that rules (22?24) are replaced with themirror-image rules in which ?function words?
areattached to the right periphery.3 Word segmentation resultsThis section presents results of running our Adap-tor Grammar models on subsets of the Bernstein-Ratner (1987) corpus of child-directed English.We use the Adaptor Grammar software availablefrom http://web.science.mq.edu.au/?mjohnson/with the same settings as described in Johnsonand Goldwater (2009), i.e., we perform Bayesianinference with ?vague?
priors for all hyperpa-rameters (so there are no adjustable parametersin our models), and perform 8 different MCMCruns of each condition with table-label resamplingfor 2,000 sweeps of the training data.
At every10th sweep of the last 1,000 sweeps we use themodel to segment the entire corpus (even if itis only trained on a subset of it), so we collectModelTokenf-scoreBoundaryprecisionBoundaryrecallBaseline 0.872 0.918 0.956+ left FWs 0.924 0.935 0.990+ left + right FWs 0.912 0.957 0.953Table 1: Mean token f-scores and boundary preci-sion and recall results averaged over 8 trials, eachconsisting of 8 MCMC runs of models trainedand tested on the full Bernstein-Ratner (1987) cor-pus (the standard deviations of all values are lessthan 0.006; Wilcox sign tests show the means ofall token f-scores differ p < 2e-4).800 sample segmentations of each utterance.The most frequent segmentation in these 800sample segmentations is the one we score in theevaluations below.3.1 Word segmentation with ?function word?modelsHere we evaluate the word segmentations foundby the ?function word?
Adaptor Grammar modeldescribed in section 2.3 and compare it to the base-line grammar with collocations and phonotacticsfrom Johnson and Goldwater (2009).
Figure 2presents the standard token and lexicon (i.e., type)f-score evaluations for word segmentations pro-posed by these models (Brent, 1999), and Table 1summarises the token and lexicon f-scores for themajor models discussed in this paper.
It is interest-ing to note that adding ?function words?
improvestoken f-score by more than 4%, corresponding toa 40% reduction in overall error rate.When the training data is very small the Mono-syllabic grammar produces the highest accuracyresults, presumably because a large proportion ofthe words in child-directed speech are monosyl-labic.
However, at around 25 sentences the morecomplex models that are capable of finding multi-syllabic words start to become more accurate.It?s interesting that after about 1,000 sentencesthe model that allows ?function words?
only onthe right periphery is considerably less accuratethan the baseline model.
Presumably this is be-cause it tends to misanalyse multi-syllabic wordson the right periphery as sequences of monosyl-labic words.The model that allows ?function words?
only onthe left periphery is more accurate than the modelthat allows them on both the left and right periph-ery when the input data ranges from about 100 toabout 1,000 sentences, but when the training data2870.000.250.500.751.001 10 100 1000 10000NumberWofWtrainingWsentencesTokenWf-scoreModelMonosyllablesBaseline+WleftWFWs+WrightWFWs+WleftW+WFWs0.000.250.500.751.001 10 100 1000 10000NumberWofWtrainingWsentencesLexiconWf-scoreModelMonosyllablesBaseline+WleftWFWs+WrightWFWs+WleftW+WrightWFWsFigure 2: Token and lexicon (i.e., type) f-score on the Bernstein-Ratner (1987) corpus as a function oftraining data size for the baseline model, the model where ?function words?
can appear on the left pe-riphery, a model where ?function words?
can appear on the right periphery, and a model where ?functionwords?
can appear on both the left and the right periphery.
For comparison purposes we also includeresults for a model that assumes that all words are monosyllabic.is larger than about 1,000 sentences both modelsare equally accurate.3.2 Content and function words found by?function word?
modelAs noted earlier, the ?function word?
model gen-erates function words via adapted nonterminalsother than the Word category.
In order to bet-ter understand just how the model works, we givethe 5 most frequent words in each word categoryfound during 8 MCMC runs of the left-peripheral?function word?
grammar above:Word : book, doggy, house, want, IFuncWord1 : a, the, your, little1, inFuncWord2 : to, in, you, what, putFuncWord3 : you, a, what, no, canInterestingly, these categories seem fairly rea-sonable.
The Word category includes open-classnouns and verbs, the FuncWord1 category in-cludes noun modifiers such as determiners, whilethe FuncWord2 and FuncWord3 categories in-clude prepositions, pronouns and auxiliary verbs.1The phone ?l?
is generated by both Consonant andVowel, so ?little?
can be (incorrectly) analysed as one syl-lable.Thus, the present model, initially aimed at seg-menting words from continuous speech, showsthree interesting characteristics that are also ex-hibited by human infants: it distinguishes be-tween function words and content words (Shi andWerker, 2001), it allows learners to acquire at leastsome of the function words of their language (e.g.
(Shi et al, 2006)); and furthermore, it may also al-low them to start grouping together function wordsaccording to their category (Cauvet et al, 2014;Shi and Melanc?on, 2010).4 Are ?function words?
on the left orright periphery?We have shown that a model that expects functionwords on the left periphery performs more accu-rate word segmentation on English, where func-tion words do indeed typically occur on the leftperiphery, leaving open the question: how coulda learner determine whether function words gen-erally appear on the left or the right periphery ofphrases in the language they are learning?
Thisquestion is important because knowing the sidewhere function words preferentially occur is re-288lated to the question of the direction of syntac-tic headedness in the language, and an accuratemethod for identifying the location of functionwords might be useful for initialising a syntac-tic learner.
Experimental evidence suggests thatinfants as young as 8 months of age already ex-pect function words on the correct side for theirlanguage ?
left-periphery for Italian infants andright-periphery for Japanese infants (Gervain etal., 2008) ?
so it is interesting to see whetherpurely distributional learners such as the onesstudied here can identify the correct location offunction words in phrases.We experimented with a variety of approachesthat use a single adaptor grammar inference pro-cess, but none of these were successful.
For ex-ample, we hoped that given an Adaptor Gram-mar that permits ?function words?
on both theleft and right periphery, the inference procedurewould decide that the right-periphery rules simplyare not used in a language like English.
Unfortu-nately we did not find this in our experiments; theright-periphery rules were used almost as often asthe left-periphery rules (recall that a large fractionof the words in English child-directed speech aremonosyllabic).In this section, we show that learners could useBayesian model selection to determine that func-tion words appear on the left periphery in Englishby comparing the marginal probability of the datafor the left-periphery and the right-periphery mod-els.Instead, we used Bayesian model selectiontechniques to determine whether left-peripheralor a right-peripheral model better fits the un-segmented utterances that constitute the trainingdata.2While Bayesian model selection is in prin-ciple straight-forward, it turns out to require the ra-tio of two integrals (for the ?evidence?
or marginallikelihood) that are often intractable to compute.Specifically, given a training corpusD of unseg-mented sentences and model families G1and G2(here the ?function word?
adaptor grammars withleft-peripheral and right-peripheral attachment re-spectively), the Bayes factor K is the ratio of themarginal likelihoods of the data:K =P(D | G1)P(D | G2)2Note that neither the left-peripheral nor the right-peripheral model is correct: even strongly left-headed lan-guages like English typically contain a few right-headed con-structions.
For example, ?ago?
is arguably the head of thephrase ?ten years ago?.02000400060001 10 100 1000 10000Number of training sentenceslog BayesfactorFigure 3: Bayes factor in favour of left-peripheral?function word?
attachment as a function of thenumber of sentences in the training corpus, cal-culated using the Harmonic Mean estimator (seewarning in text).where the marginal likelihood or ?evidence?
for amodel G is obtained by integrating over all of thehidden or latent structure and parameters ?
:P(D | G) =??P(D,?
| G) d?
(31)Here the variable ?
ranges over the space ?
of allpossible parses for the utterances inD and all pos-sible configurations of the Pitman-Yor processesand their parameters that constitute the ?state?
ofthe Adaptor Grammar G. While the probability ofany specific Adaptor Grammar configuration ?
isnot too hard to calculate (the MCMC sampler forAdaptor Grammars can print this after each sweepthrough D), the integral in (31) is in general in-tractable.Textbooks such as Murphy (2012) describe anumber of methods for calculating P(D | G), butmost of them assume that the parameter space ?is continuous and so cannot be directly appliedhere.
The Harmonic Mean estimator (32) for (31),which we used here, is a popular estimator for(31) because it only requires the ability to calcu-late P(D,?
| G) for samples from P(?
| D,G):P(D | G) ?
(1nn?i=11P(D,?i| G))?1where ?i, .
.
.
,?nare n samples from P(?
|289D,G), which can be generated by the MCMC pro-cedure.Figure 3 depicts how the Bayes factor in favourof left-peripheral attachment of ?function words?varies as a function of the number of utter-ances in the training data D (calculated from thelast 1000 sweeps of 8 MCMC runs of the cor-responding adaptor grammars).
As that figureshows, once the training data contains more thanabout 1,000 sentences the evidence for the left-peripheral grammar becomes very strong.
On thefull training data the estimated log Bayes factor isover 6,000, which would constitute overwhelmingevidence in favour of left-peripheral attachment.Unfortunately, as Murphy and others warn, theHarmonic Mean estimator is extremely unstable(Radford Neal calls it ?the worst MCMC methodever?
in his blog), so we think it is important toconfirm these results using a more stable estima-tor.
However, given the magnitude of the differ-ences and the fact that the two models being com-pared are of similar complexity, we believe thatthese results suggest that Bayesian model selec-tion can be used to determine properties of the lan-guage being learned.5 Conclusions and future workThis paper showed that the word segmentationaccuracy of a state-of-the-art Adaptor Grammarmodel is significantly improved by extending itso that it explicitly models some properties offunction words.
We also showed how Bayesianmodel selection can be used to identify that func-tion words appear on the left periphery of phrasesin English, even though the input to the model onlyconsists of an unsegmented sequence of phones.Of course this work only scratches the surfacein terms of investigating the role of function wordsin language acquisition.
It would clearly be veryinteresting to examine the performance of thesemodels on other corpora of child-directed English,as well as on corpora of child-directed speech inother languages.
Our evaluation focused on word-segmentation, but we could also evaluate the ef-fect that modelling ?function words?
has on otheraspects of the model, such as its ability to learnsyllable structure.The models of ?function words?
we investi-gated here only capture two of the 7 linguisticproperties of function words identified in section 1(i.e., that function words tend to be monosyllabic,and that they tend to appear phrase-peripherally),so it would be interesting to develop and exploremodels that capture other linguistic properties offunction words.
For example, following the sug-gestion by Hochmann et al (2010) that humanlearners use frequency cues to identify functionwords, it might be interesting to develop computa-tional models that do the same thing.
In an Adap-tor Grammar the frequency distribution of func-tion words might be modelled by specifying theprior for the Pitman-Yor Process parameters asso-ciated with the function words?
adapted nontermi-nals so that it prefers to generate a small numberof high-frequency items.It should also be possible to develop modelswhich capture the fact that function words tend notto be topic-specific.
Johnson et al (2010) andJohnson et al (2012) show how Adaptor Gram-mars can model the association between wordsand non-linguistic ?topics?
; perhaps these modelscould be extended to capture some of the semanticproperties of function words.It would also be interesting to further explorethe extent to which Bayesian model selection is auseful approach to linguistic ?parameter setting?.In order to do this it is imperative to develop bettermethods than the problematic ?Harmonic Mean?estimator used here for calculating the evidence(i.e., the marginal probability of the data) that canhandle the combination of discrete and continuoushidden structure that occur in computational lin-guistic models.As well as substantially improving the accuracyof unsupervised word segmentation, this work isinteresting because it suggests a connection be-tween unsupervised word segmentation and the in-duction of syntactic structure.
It is reasonable toexpect that hierarchical non-parametric Bayesianmodels such as Adaptor Grammars may be usefultools for exploring such a connection.AcknowledgmentsThis work was supported in part by the Aus-tralian Research Council?s Discovery Projectsfunding scheme (project numbers DP110102506and DP110102593), the European Research Coun-cil (ERC-2011-AdG-295810 BOOTPHON), theAgence Nationale pour la Recherche (ANR-10-LABX-0087 IEC, and ANR-10-IDEX-0001-02PSL*), and the Mairie de Paris, Ecole des HautesEtudes en Sciences Sociales, the Ecole NormaleSup?erieure, and the Fondation Pierre Gilles deGennes.290ReferencesN.
Bernstein-Ratner.
1987.
The phonology of parent-child speech.
In K. Nelson and A. van Kleeck, ed-itors, Children?s Language, volume 6, pages 159?174.
Erlbaum, Hillsdale, NJ.M.
Brent.
1999.
An efficient, probabilistically soundalgorithm for segmentation and word discovery.Machine Learning, 34:71?105.E.
Cauvet, R. Limissuri, S. Millotte, K. Skoruppa,D.
Cabrol, and A. Christophe.
2014.
Functionwords constrain on-line recognition of verbs andnouns in French 18-month-olds.
Language Learn-ing and Development, pages 1?18.A.
Christophe, S. Millotte, S. Bernal, and J. Lidz.2008.
Bootstrapping lexical and syntactic acquisi-tion.
Language and Speech, 51(1-2):61?75.S.
B. Cohen, D. M. Blei, and N. A. Smith.
2010.
Vari-ational inference for adaptor grammars.
In HumanLanguage Technologies: The 2010 Annual Confer-ence of the North American Chapter of the Associa-tion for Computational Linguistics, pages 564?572,Los Angeles, California, June.
Association for Com-putational Linguistics.K.
Demuth and E. McCullough.
2009.
The prosodic(re-)organization of childrens early English articles.Journal of Child Language, 36(1):173?200.J.
Elman, E. Bates, M. H. Johnson, A. Karmiloff-Smith, D. Parisi, and K. Plunkett.
1996.
Rethink-ing Innateness: A Connectionist Perspective on De-velopment.
MIT Press/Bradford Books, Cambridge,MA.V.
Fromkin, editor.
2001.
Linguistics: An Introductionto Linguistic Theory.
Blackwell, Oxford, UK.J.
Gervain, M. Nespor, R. Mazuka, R. Horie, andJ.
Mehler.
2008.
Bootstrapping word order inprelexical infants: A japaneseitalian cross-linguisticstudy.
Cognitive Psychology, 57(1):56 ?
74.S.
Goldwater, T. L. Griffiths, and M. Johnson.
2009.A Bayesian framework for word segmentation: Ex-ploring the effects of context.
Cognition, 112(1):21?54.S.
Goldwater, T. L. Griffiths, and M. Johnson.
2011.Producing power-law distributions and dampingword frequencies with two-stage language models.Journal of Machine Learning Research, 12:2335?2382.P.
A.
Hall?e, C. Durand, and B. de Boysson-Bardies.2008.
Do 11-month-old French infants process ar-ticles?
Language and Speech, 51(1-2):23?44.J.-R. Hochmann, A. D. Endress, and J. Mehler.
2010.Word frequency as a cue for identifying functionwords in infancy.
Cognition, 115(3):444 ?
457.M.
Johnson and S. Goldwater.
2009.
Improving non-parameteric Bayesian inference: experiments on un-supervised word segmentation with adaptor gram-mars.
In Proceedings of Human Language Tech-nologies: The 2009 Annual Conference of the NorthAmerican Chapter of the Association for Compu-tational Linguistics, pages 317?325, Boulder, Col-orado, June.
Association for Computational Linguis-tics.M.
Johnson, T. Griffiths, and S. Goldwater.
2007a.Bayesian inference for PCFGs via Markov chainMonte Carlo.
In Human Language Technologies2007: The Conference of the North American Chap-ter of the Association for Computational Linguistics;Proceedings of the Main Conference, pages 139?146, Rochester, New York.
Association for Compu-tational Linguistics.M.
Johnson, T. L. Griffiths, and S. Goldwater.
2007b.Adaptor Grammars: A framework for specifyingcompositional nonparametric Bayesian models.
InB.
Sch?olkopf, J. Platt, and T. Hoffman, editors, Ad-vances in Neural Information Processing Systems19, pages 641?648.
MIT Press, Cambridge, MA.M.
Johnson, K. Demuth, M. Frank, and B. Jones.2010.
Synergies in learning words and their refer-ents.
In J. Lafferty, C. K. I. Williams, J. Shawe-Taylor, R. Zemel, and A. Culotta, editors, Advancesin Neural Information Processing Systems 23, pages1018?1026.M.
Johnson, K. Demuth, and M. Frank.
2012.
Exploit-ing social information in grounded language learn-ing via grammatical reduction.
In Proceedings ofthe 50th Annual Meeting of the Association for Com-putational Linguistics, pages 883?891, Jeju Island,Korea, July.
Association for Computational Linguis-tics.M.
Johnson.
2008.
Using Adaptor Grammars to iden-tify synergies in the unsupervised acquisition of lin-guistic structure.
In Proceedings of the 46th AnnualMeeting of the Association of Computational Lin-guistics, pages 398?406, Columbus, Ohio.
Associ-ation for Computational Linguistics.Y.
Kedar, M. Casasola, and B.
Lust.
2006.
Gettingthere faster: 18- and 24-month-old infants?
use offunction words to determine reference.
Child De-velopment, 77(2):325?338.K.
Kurihara and T. Sato.
2006.
VariationalBayesian grammar induction for natural language.In Y. Sakakibara, S. Kobayashi, K. Sato, T. Nishino,and E. Tomita, editors, Grammatical Inference: Al-gorithms and Applications, pages 84?96.
Springer.K.
P. Murphy.
2012.
Machine learning: a probabilisticperspective.
The MIT Press.V.
L. Shafer, D. W. Shucard, J. L. Shucard, andL.
Gerken.
1998.
An electrophysiological study ofinfants?
sensitivity to the sound patterns of English291speech.
Journal of Speech, Language and HearingResearch, 41(4):874.R.
Shi and M. Lepage.
2008.
The effect of functionalmorphemes on word segmentation in preverbal in-fants.
Developmental Science, 11(3):407?413.R.
Shi and A. Melanc?on.
2010.
Syntactic categoriza-tion in French-learning infants.
Infancy, 15(517?533).R.
Shi and J. Werker.
2001.
Six-months old infants?preference for lexical words.
Psychological Science,12:71?76.R.
Shi, A. Cutler, J. Werker, and M. Cruickshank.2006.
Frequency and form as determinants of func-tor sensitivity in English-acquiring infants.
TheJournal of the Acoustical Society of America,119(6):EL61?EL67.Y.
W. Teh, M. Jordan, M. Beal, and D. Blei.
2006.
Hi-erarchical Dirichlet processes.
Journal of the Amer-ican Statistical Association, 101:1566?1581.R.
Zangl and A. Fernald.
2007.
Increasing flexibil-ity in children?s online processing of grammaticaland nonce determiners in fluent speech.
LanguageLearning and Development, 3(3):199?231.292
