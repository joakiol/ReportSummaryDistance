Proceedings of the 53rd Annual Meeting of the Association for Computational Linguisticsand the 7th International Joint Conference on Natural Language Processing, pages 730?740,Beijing, China, July 26-31, 2015.c?2015 Association for Computational LinguisticsUnsupervised Cross-Domain Word Representation LearningDanushka Bollegala Takanori Maehara Ken-ichi Kawarabayashidanushka.bollegala@ maehara.takanori@ k keniti@liverpool.ac.uk shizuoka.ac.jp nii.ac.jpUniversity of Liverpool Shizuoka University National Institute of InformaticsJST, ERATO, Kawarabayashi Large Graph Project.AbstractMeaning of a word varies from one do-main to another.
Despite this impor-tant domain dependence in word seman-tics, existing word representation learningmethods are bound to a single domain.Given a pair of source-target domains,we propose an unsupervised method forlearning domain-specific word representa-tions that accurately capture the domain-specific aspects of word semantics.
First,we select a subset of frequent words thatoccur in both domains as pivots.
Next,we optimize an objective function thatenforces two constraints: (a) for bothsource and target domain documents, piv-ots that appear in a document must accu-rately predict the co-occurring non-pivots,and (b) word representations learnt forpivots must be similar in the two do-mains.
Moreover, we propose a methodto perform domain adaptation using thelearnt word representations.
Our proposedmethod significantly outperforms compet-itive baselines including the state-of-the-art domain-insensitive word representa-tions, and reports best sentiment classifi-cation accuracies for all domain-pairs in abenchmark dataset.1 IntroductionLearning semantic representations for words is afundamental task in NLP that is required in nu-merous higher-level NLP applications (Collobertet al, 2011).
Distributed word representationshave gained much popularity lately because oftheir accuracy as semantic representations forwords (Mikolov et al, 2013a; Pennington et al,2014).
However, the meaning of a word oftenvaries from one domain to another.
For exam-ple, the phrase lightweight is often used in a posi-tive sentiment in the portable electronics domainbecause a lightweight device is easier to carryaround, which is a positive attribute for a portableelectronic device.
However, the same phrase has anegative sentiment assocition in the movie domainbecause movies that do not invoke deep thoughtsin viewers are considered to be lightweight (Bol-legala et al, 2014).
However, existing word rep-resentation learning methods are agnostic to suchdomain-specific semantic variations of words, andcapture semantics of words only within a singledomain.
To overcome this problem and capturedomain-specific semantic orientations of words,we propose a method that learns separate dis-tributed representations for each domain in whicha word occurs.Despite the successful applications of dis-tributed word representation learning meth-ods (Pennington et al, 2014; Collobert et al,2011; Mikolov et al, 2013a) most existing ap-proaches are limited to learning only a singlerepresentation for a given word (Reisinger andMooney, 2010).
Although there have been somework on learning multiple prototype representa-tions (Huang et al, 2012; Neelakantan et al, 2014)for a word considering its multiple senses, suchmethods do not consider the semantics of the do-main in which the word is being used.If we can learn separate representations for aword for each domain in which it occurs, we canuse the learnt representations for domain adapta-tion tasks such as cross-domain sentiment clas-sification (Bollegala et al, 2011b), cross-domainPOS tagging (Schnabel and Sch?utze, 2013), cross-domain dependency parsing (McClosky et al,2010), and domain adaptation of relation extrac-tors (Bollegala et al, 2013a; Bollegala et al,2013b; Bollegala et al, 2011a; Jiang and Zhai,2007a; Jiang and Zhai, 2007b).We introduce the cross-domain word represen-730tation learning task, where given two domains,(referred to as the source (S) and the target (T ))the goal is to learn two separate representationswSand wTfor a word w respectively from thesource and the target domain that capture domain-specific semantic variations of w. In this paper,we use the term domain to represent a collectionof documents related to a particular topic such asuser-reviews in Amazon for a product category(e.g.
books, dvds, movies, etc.).
However, a do-main in general can be a field of study (e.g.
biol-ogy, computer science, law, etc.)
or even an entiresource of information (e.g.
twitter, blogs, newsarticles, etc.).
In particular, we do not assume theavailability of any labeled data for learning wordrepresentations.This problem setting is closely related to unsu-pervised domain adaptation (Blitzer et al, 2006),which has found numerous useful applicationssuch as, sentiment classification and POS tagging.For example, in unsupervised cross-domain sen-timent classification (Blitzer et al, 2006; Blitzeret al, 2007), we train a binary sentiment classifierusing positive and negative labeled user reviewsin the source domain, and apply the trained clas-sifier to predict sentiment of the target domain?suser reviews.
Although the distinction between thesource and the target domains is not important forthe word representation learning step, it is impor-tant for the domain adaptation tasks in which wesubsequently evaluate the learnt word representa-tions.
Following prior work on domain adapta-tion (Blitzer et al, 2006), high-frequent features(unigrams/bigrams) common to both domains arereferred to as domain-independent features or piv-ots.
In contrast, we use non-pivots to refer to fea-tures that are specific to a single domain.We propose an unsupervised cross-domainword representation learning method that jointlyoptimizes two criteria: (a) given a document dfrom the source or the target domain, we must ac-curately predict the non-pivots that occur in d us-ing the pivots that occur in d, and (b) the sourceand target domain representations we learn for piv-ots must be similar.
The main challenge in domainadaptation is feature mismatch, where the featuresthat we use for training a classifier in the sourcedomain do not necessarily occur in the target do-main.
Consequently, prior work on domain adap-tation (Blitzer et al, 2006; Pan et al, 2010) learnlower-dimensional mappings from non-pivots topivots, thereby overcoming the feature mismatchproblem.
Criteria (a) ensures that word represen-tations for domain-specific non-pivots in each do-main are related to the word representations fordomain-independent pivots.
This relationship en-ables us to discover pivots that are similar to tar-get domain-specific non-pivots, thereby overcom-ing the feature mismatch problem.On the other hand, criteria (b) captures the priorknowledge that high-frequent words common totwo domains often represent domain-independentsemantics.
For example, in sentiment classifica-tion, words such as excellent or terrible would ex-press similar sentiment about a product irrespec-tive of the domain.
However, if a pivot expressesdifferent semantics in source and the target do-mains, then it will be surrounded by dissimilarsets of non-pivots, and reflected in the first crite-ria.
Criteria (b) can also be seen as a regulariza-tion constraint imposed on word representations toprevent overfitting by reducing the number of freeparameters in the model.Our contributions in this paper can be summa-rized as follows.?
We propose a distributed word representa-tion learning method that learns separaterepresentations for a word for each do-main in which it occurs.
To the bestof our knowledge, ours is the first-everdomain-sensitive distributed word represen-tation learning method.?
Given domain-specific word representations,we propose a method to learn a cross-domainsentiment classifier.Although word representation learning meth-ods have been used for various relatedtasks in NLP such as similarity measure-ment (Mikolov et al, 2013c), POS tag-ging (Collobert et al, 2011), dependencyparsing (Socher et al, 2011a), machine trans-lation (Zou et al, 2013), sentiment classifica-tion (Socher et al, 2011b), and semantic rolelabeling (Roth and Woodsend, 2014), to thebest of our knowledge, word representationsmethods have not yet been used for cross-domain sentiment classification.Experimental results for cross-domain senti-ment classification on a benchmark dataset showthat the word representations learnt using the pro-posed method statistically significantly outper-731form a state-of-the-art domain-insensitive wordrepresentation learning method (Pennington et al,2014), and several competitive baselines.
In par-ticular, our proposed cross-domain word represen-tation learning method is not specific to a par-ticular task such as sentiment classification, andin principle, can be in applied to a wide-rangeof domain adaptation tasks.
Despite this task-independent nature of the proposed method, itachieves the best sentiment classification accu-racies on all domain-pairs, reporting statisticallycomparable results to the current state-of-the-artunsupervised cross-domain sentiment classifica-tion methods (Pan et al, 2010; Blitzer et al, 2006).2 Related WorkRepresenting the semantics of a word using somealgebraic structure such as a vector (more gener-ally a tensor) is a common first step in many NLPtasks (Turney and Pantel, 2010).
By applying al-gebraic operations on the word representations, wecan perform numerous tasks in NLP, such as com-posing representations for larger textual units be-yond individual words such as phrases (Mitchelland Lapata, 2008).
Moreover, word representa-tions are found to be useful for measuring se-mantic similarity, and for solving proportionalanalogies (Mikolov et al, 2013c).
Two main ap-proaches for computing word representations canbe identified in prior work (Baroni et al, 2014):counting-based and prediction-based.In counting-based approaches (Baroni andLenci, 2010), a word w is represented by a vec-torw that contains other words that co-occur withw in a corpus.
Numerous methods for selectingco-occurrence contexts such as proximity or de-pendency relations have been proposed (Turneyand Pantel, 2010).
Despite the numerous suc-cessful applications of co-occurrence counting-based distributional word representations, theirhigh dimensionality and sparsity are often prob-lematic in practice.
Consequently, further post-processing steps such as dimensionality reduction,and feature selection are often required when us-ing counting-based word representations.On the other hand, prediction-based approachesfirst assign each word, for example, with a d-dimensional real-vector, and learn the elements ofthose vectors by applying them in an auxiliary tasksuch as language modeling, where the goal is topredict the next word in a given sequence.
Thedimensionality d is fixed for all the words in thevocabulary, and, unlike counting-based word rep-resentations, is much smaller (e.g.
d ?
[10, 1000]in practice) compared to the vocabulary size.
Theneural network language model (NNLM) (Bengioet al, 2003) uses a multi-layer feed-forward neu-ral network to predict the next word in a sequence,and uses backpropagation to update the word vec-tors such that the prediction error is minimized.Although NNLMs learn word representationsas a by-product, the main focus on languagemodeling is to predict the next word in a sen-tence given the previous words, and not learn-ing word representations that capture semantics.Moreover, training multi-layer neural networksusing large text corpora is time consuming.
Toovercome those limitations, methods that specif-ically focus on learning word representations thatmodel word co-occurrences in large corpora havebeen proposed (Mikolov et al, 2013a; Mnih andKavukcuoglu, 2013; Huang et al, 2012; Pen-nington et al, 2014).
Unlike the NNLM, thesemethods use all the words in a contextual win-dow in the prediction task.
Methods that useone or no hidden layers are proposed to improvethe scalability of the learning algorithms.
Forexample, the skip-gram model (Mikolov et al,2013b) predicts the words c that appear in thelocal context of a word w, whereas the continu-ous bag-of-words model (CBOW) predicts a wordw conditioned on all the words c that appear inw?s local context (Mikolov et al, 2013a).
Meth-ods that use global co-occurrences in the entirecorpus to learn word representations have shownto outperform methods that use only local co-occurrences (Huang et al, 2012; Pennington etal., 2014).
Overall, prediction-based methodshave shown to outperform counting-based meth-ods (Baroni et al, 2014).Despite their impressive performance, existingmethods for word representation learning do notconsider the semantic variation of words acrossdifferent domains.
However, as described in Sec-tion 1, the meaning of a word vary from one do-main to another, and must be considered.
To thebest of our knowledge, the only prior work study-ing the problem of word representation variationacross domains is due to Bollegala et al (2014).Given a source and a target domain, they first se-lect a set of pivots using pointwise mutual infor-mation, and create two distributional representa-732tions for each pivot using their co-occurrence con-texts in a particular domain.
Next, a projectionmatrix from the source to the target domain featurespaces is learnt using partial least squares regres-sion.
Finally, the learnt projection matrix is usedto find the nearest neighbors in the source domainfor each target domain-specific features.
However,unlike our proposed method, their method doesnot learn domain-specific word representations,but simply uses co-occurrence counting when cre-ating in-domain word representations.Faralli et al (2012) proposed a domain-drivenword sense disambiguation (WSD) method wherethey construct glossaries for several domain us-ing a pattern-based bootstrapping technique.
Thiswork demonstrates the importance of consideringthe domain specificity of word senses.
However,the focus of their work is not to learn representa-tions for words or their senses in a domain, but toconstruct glossaries.
It would be an interesting fu-ture research direction to explore the possibility ofusing such domain-specific glossaries for learningdomain-specific word representations.Neelakantan et al (2014) proposed a methodthat jointly performs WSD and word embeddinglearning, thereby learning multiple embeddingsper word type.
In particular, the number of sensesper word type is automatically estimated.
How-ever, their method is limited to a single domain,and does not consider how the representations varyacross domains.
On the other hand, our proposedmethod learns a single representation for a partic-ular word for each domain in which it occurs.Although in this paper we focus on the mono-lingual setting where source and target domainsbelong to the same language, the related settingwhere learning representations for words that aretranslational pairs across languages has been stud-ied (Hermann and Blunsom, 2014; Klementiev etal., 2012; Gouws et al, 2015).
Such representa-tions are particularly useful for cross-lingual in-formation retrieval (Duc et al, 2010).
It will be aninteresting future research direction to extend ourproposed method to learn such cross-lingual wordrepresentations.3 Cross-Domain RepresentationLearningWe propose a method for learning word represen-tations that are sensitive to the semantic variationsof words across domains.
We call this problemcross-domain word representation learning, andprovide a definition in Section 3.1.
Next, in Sec-tion 3.2, given a set of pivots that occurs in both asource and a target domain, we propose a methodfor learning cross-domain word representations.We defer the discussion of pivot selection meth-ods to Section 3.4.
In Section 3.5, we propose amethod for using the learnt word representationsto train a cross-domain sentiment classifier.3.1 Problem DefinitionLet us assume that we are given two sets of docu-ments DSand DTrespectively for a source (S)and a target (T ) domain.
We do not considerthe problem of retrieving documents for a domain,and assume such a collection of documents to begiven.
Then, given a particular word w, we definecross-domain representation learning as the task oflearning two separate representations wSand wTcapturing w?s semantics in respectively the sourceS and the target T domains.Unlike in domain adaptation, where there is aclear distinction between the source (i.e.
the do-main on which we train) vs. the target (i.e.
thedomain on which we test) domains, for represen-tation learning purposes we do not make a distinc-tion between the two domains.
In the unsupervisedsetting of the cross-domain representation learn-ing that we study in this paper, we do not assumethe availability of labeled data for any domain forthe purpose of learning word representations.
Asan extrinsic evaluation task, we apply the trainedword representations for classifying sentiment re-lated to user-reviews (Section 3.5).
However, forthis evaluation task we require sentiment-labeleduser-reviews from the source domain.Decoupling of the word representation learn-ing from any tasks in which those representationsare subsequently used, simplifies the problem aswell as enables us to learn task-independent wordrepresentations with potential generic applicabil-ity.
Although we limit the discussion to a pair ofdomains for simplicity, the proposed method canbe easily extended to jointly learn word represen-tations for more than two domains.
In fact, priorwork on cross-domain sentiment analysis showthat incorporating multiple source domains im-proves sentiment classification accuracy on a tar-get domain (Bollegala et al, 2011b; Glorot et al,2011).7333.2 Proposed MethodTo describe our proposed method, let us denote apivot and a non-pivot feature respectively by c andw.
Our proposed method does not depend on aspecific pivot selection method, and can be usedwith all previously proposed methods for selectingpivots as explained later in Section 3.4.
A pivotc is represented in the source and target domainsrespectively by vectors cS?
Rnand cT?
Rn.Likewise, a source specific non-pivot w is repre-sented bywSin the source domain, whereas a tar-get specific non-pivot w is represented by wTinthe target domain.
By definition, a non-pivot oc-curs only in a single domain.
For notational conve-nience we use w to denote non-pivots in both do-mains when the domain is clear from the context.We use CS, WS, CT, and WTto denote the setsof word representation vectors respectively for thesource pivots, source non-pivots, target pivots, andtarget non-pivots.Let us denote the set of documents in the sourceand the target domains respectively by DSandDT.
Following the bag-of-features model, we as-sume that a document D is represented by the setof pivots and non-pivots that occur in D (w ?
dand c ?
d).
We consider the co-occurrencesof a pivot c and a non-pivot w within a fixed-size contextual window in a document.
Followingprior work on representation learning (Mikolov etal., 2013a), in our experiments, we set the win-dow size to 10 tokens, without crossing sentenceboundaries.
The notation (c, w) ?
d denotes theco-occurrence of a pivot c and a non-pivot w in adocument d.We learn domain-specific word representationsby maximizing the prediction accuracy of the non-pivots w that occur in the local context of a pivotc.
The hinge loss, L(CS,WS), associated withpredicting a non-pivot w in a source documentd ?
DSthat co-occurs with pivots c is given by:?d?DS?(c,w)?d?w?
?p(w)max(0, 1?
cS>wS+ cS>w?S)(1)Here, w?Sis the source domain representation ofa non-pivot w?that does not occur in d. The lossfunction given by Eq.
1 requires that a non-pivotw that co-occurs with a pivot c in the document dis assigned a higher ranking score as measured bythe inner-product between cSand wSthan a non-pivot w?that does not occur in d. We randomlysample k non-pivots from the set of all source do-main non-pivots that do not occur in d as w?.Specifically, we use the marginal distributionof non-pivots p(w), estimated from the corpuscounts, as the sampling distribution.
We raisep(w) to the 3/4-th power as proposed by Mikolovet al (2013a), and normalize it to unit probabil-ity mass prior to sampling k non-pivots w?pereach co-occurrence of (c, w) ?
d. Because non-occurring non-pivots w?are randomly sampled,prior work on noise contrastive estimation hasfound that it requires more negative samples thanpositive samples to accurately learn a predictionmodel (Mnih and Kavukcuoglu, 2013).
We exper-imentally found k = 5 to be an acceptable trade-off between the prediction accuracy and the num-ber of training instances.Likewise, the loss function L(CT,WT) for pre-dicting non-pivots using pivots in the target do-main is given by:?d?DT?(c,w)?d?w?
?p(w)max(0, 1?
cT>wT+ cT>w?T)(2)Here, w?denotes target domain non-pivots thatdo not occur in d, and are randomly sampledfrom p(w) following the same procedure as in thesource domain.The source and target loss functions given re-spectively by Eqs.
1 and 2 can be used on their ownto independently learn source and target domainword representations.
However, by definition, piv-ots are common to both domains.
We use thisproperty to relate the source and target word repre-sentations via a pivot-regularizer, R(CS, CT), de-fined as:R(CS, CT) =12K?i=1||c(i)S?
c(i)T||2(3)Here, ||x|| represents the l2norm of a vector x,and c(i)is the i-th pivot in a total collection of Kpivots.
Word representations for non-pivots in thesource and target domains are linked via the pivotregularizer because, the non-pivots in each domainare predicted using the word representations forthe pivots in each domain, which in turn are reg-ularized by Eq.
3.
The overall objective function,L(CS,WS, CT,WT), we minimize is the sum1of1Weighting the source and target loss functions by the re-spective dataset sizes did not result in any significant increasein performance.
We believe that this is because the bench-mark dataset contains approximately equal numbers of docu-ments for each domain.734the source and target loss functions, regularizedvia Eq.
3 with coefficient ?, and is given by:L(CS,WS, ) + L(CT,WT) + ?R(CS, CT) (4)3.3 TrainingWord representations of pivots c and non-pivots win the source (cS,wS) and the target (cT,wT) do-mains are parameters to be learnt in the proposedmethod.
To derive parameter updates, we computethe gradients of the overall loss function in Eq.
4w.r.t.
to each parameter as follows:?L?wS={0 if cS>(wS?w?S) ?
1?cSotherwise(5)?L?w?S={0 if cS>(wS?w?S) ?
1cSotheriwse(6)?L?wT={0 if cT>(wT?w?T) ?
1?cTotherwise(7)?L?w?T={0 if cT>(wT?w?T) ?
1cTotherwise(8)?L?cS={?(cS?
cT) if cS>(wS?w?S) ?
1w?S?wS+ ?(cS?
cT) otherwise(9)?L?cT={?(cT?
cS) if cT>(wT?w?T) ?
1w?T?wT+ ?(cT?
cS) otherwise(10)Here, for simplicity, we drop the arguments insidethe loss function and write it as L. We use minibatch stochastic gradient descent with a batch sizeof 50 instances.
AdaGrad (Duchi et al, 2011) isused to schedule the learning rate.
All word repre-sentations are initialized with n dimensional ran-dom vectors sampled from a zero mean and unitvariance Gaussian.
Although the objective in Eq.
4is not jointly convex in all four representations,it is convex w.r.t.
the representation of a partic-ular feature (pivot or non-pivot) when the repre-sentations for all the other features are held fixed.In our experiments, the training converged in allcases with less than 100 epochs over the dataset.The rank-based predictive hinge loss (Eq.
1)is inspired by the prior work on word represen-tation learning for a single domain (Collobert etal., 2011).
However, unlike the multilayer neu-ral network in Collobert et al (2011), the pro-posed method uses a computationally efficient sin-gle layer to reduce the number of parameters thatmust be learnt, thereby scaling to large datasets.Similar to the skip-gram model (Mikolov et al,2013a), the proposed method predicts occurrencesof contexts (non-pivots) w within a fixed-size con-textual window of a target word (pivot) c.Scoring the co-occurrences of two words c andw by the bilinear form given by the inner-productis similar to prior work on domain-insensitiveword-representation learning (Mnih and Hinton,2008; Mikolov et al, 2013a).
However, unlikethose methods that use the softmax function toconvert inner-products to probabilities, we directlyuse the inner-products without any further trans-formations, thereby avoiding computationally ex-pensive distribution normalizations over the entirevocabulary.3.4 Pivot SelectionGiven two sets of documents DS, DTrespec-tively for the source and the target domains, weuse the following procedure to select pivots andnon-pivots.
First, we tokenize and lemmatize eachdocument using the Stanford CoreNLP toolkit2.Next, we extract unigrams and bigrams as featuresfor representing a document.
We remove featureslisted as stop words using a standard stop wordslist.
Stop word removal increases the effective co-occurrence window size for a pivot.
Finally, weremove features that occur less than 50 times inthe entire set of documents.Several methods have been proposed in theprior work on domain adaptation for selecting aset of pivots from a given pair of domains suchas the minimum frequency of occurrence of a fea-ture in the two domains, mutual information (MI),and the entropy of the feature distribution over thedocuments (Pan et al, 2010).
In our preliminaryexperiments, we discovered that a normalized ver-sion of the PMI (NPMI) (Bouma, 2009) to workconsistently well for selecting pivots from differ-ent pairs of domains.
NPMI between two featuresx and y is given by:NPMI(x, y) = log(p(x, y)p(x)p(y))1?
log(p(x, y))(11)Here, the joint probability p(x, y), and themarginal probabilities p(x) and p(y) are estimatedusing the number of co-occurrences of x and y inthe sentences in the documents.
Eq.
11 normalizesboth the upper and lower bounds of the PMI.2http://nlp.stanford.edu/software/corenlp.shtml735We measure the appropriateness of a feature asa pivot according to the score given by:score(x) = min (NPMI(x,S),NPMI(x, T )) .
(12)We rank features that are common to both domainsin the descending order of their scores as given byEq.
12, and select the top NPfeatures as pivots.We rank features x that occur only in the sourcedomain by NPMI(x,S), and select the top rankedNSfeatures as source-specific non-pivots.
Like-wise, we rank the features x that occur only in thetarget domain by NPMI(x, T ), and select the topranked NTfeatures as target-specific non-pivots.The pivot selection criterion described here dif-fers from that of Blitzer et al (2006; 2007), wherepivots are defined as features that behave similarlyboth in the source and the target domains.
Theycompute the mutual information between a feature(i.e.
unigrams or bigrams) and the sentiment labelsusing source domain labeled reviews.
This methodis useful when selecting pivots that are closely as-sociated with positive or negative sentiment in thesource domain.
However, in unsupervised domainadaptation we do not have labeled data for the tar-get domain.
Therefore, the pivots selected usingthis approach are not guaranteed to demonstratethe same sentiment in the target domain as in thesource domain.
On the other hand, the pivot se-lection method proposed in this paper focuses onidentifying a subset of features that are closely as-sociated with both domains.It is noteworthy that our proposed cross-domainword representation learning method (Section 3.2)does not assume any specific pivot/non-pivot se-lection method.
Therefore, in principle, our pro-posed word representation learning method couldbe used with any of the previously proposed pivotselection methods.
We defer a comprehensiveevaluation of possible combinations of pivot selec-tion methods and their effect on the proposed wordrepresentation learning method to future work.3.5 Cross-Domain Sentiment ClassificationAs a concrete application of cross-domain wordrepresentations, we describe a method for learninga cross-domain sentiment classifier using the wordrepresentations learnt by the proposed method.Existing word representation learning methodsthat learn from only a single domain are typi-cally evaluated for their accuracy in measuring se-mantic similarity between words, or by solvingword analogy problems.
Unfortunately, such goldstandard datasets capturing cross-domain seman-tic variations of words are unavailable.
Therefore,by applying the learnt word representations in across-domain sentiment classification task, we canconduct an indirect extrinsic evaluation.The train data available for unsupervised cross-domain sentiment classification consists of unla-beled data for both the source and the target do-mains as well as labeled data for the source do-main.
We train a binary sentiment classifier usingthose train data, and apply it to classify sentimentof the target test data.Unsupervised cross-domain sentiment classifi-cation is challenging due to two reasons: feature-mismatch, and semantic variation.
First, the setsof features that occur in source and target domaindocuments are different.
Therefore, a sentimentclassifier trained using source domain labeled datais likely to encounter unseen features during testtime.
We refer to this as the feature-mismatchproblem.
Second, some of the features that occurin both domains will have different sentiments as-sociated with them (e.g.
lightweight).
Therefore,a sentiment classifier trained using source domainlabeled data is likely to incorrectly predict simi-lar sentiment (as in the source) for such features.We call this the semantic variation problem.
Next,we propose a method to overcome both problemsusing cross-domain word representations.Let us assume that we are given a set{(x(i)S, y(i))}ni=1of n labeled reviews x(i)Sfor thesource domain S. For simplicity, let us considerbinary sentiment classification where each reviewx(i)is labeled either as positive (i.e.
y(i)= 1) ornegative (i.e.
y(i)= ?1).
Our cross-domain bi-nary sentiment classification method can be eas-ily extended to multi-class classification.
First, welemmatize each word in a source domain labeledreview x(i)S, and extract unigrams and bigrams asfeatures to represent x(i)Sby a binary-valued fea-ture vector.
Next, we train a binary linear clas-sifier, ?, using those feature vectors.
Any binaryclassification algorithm can be used for this pur-pose.
We use ?
(z) to denote the weight learnt bythe classifier for a feature z.
In our experiments,we used l2regularized logistic regression.At test time, we represent a test target reviewby a binary-valued vector h using a the set of un-igrams and bigrams extracted from that review.Then, the activation score, ?
(h), of h is defined736by:?
(h) =?c?h?c????(c?
)f(c?S, cS)+?w?h?w????(w?
)f(w?S,wT)(13)Here, f is a similarity measure between two vec-tors.
If ?
(h) > 0, we classify h as positive, andnegative otherwise.
Eq.
13 measures the similaritybetween each feature in h against the features inthe classification model ?.
For pivots c ?
h, weuse the the source domain representations to mea-sure similarity, whereas for the (target-specific)non-pivots w ?
h, we use their target domain rep-resentations.
We experimented with several pop-ular similarity measures for f and found cosinesimilarity to perform consistently well.
We can in-terpret Eq.
13 as a method for expanding a test tar-get document using nearest neighbor features fromthe source domain labeled data.
It is analogous toquery expansion used in information retrieval toimprove document recall (Fang, 2008).
Alterna-tively, Eq.
13 can be seen as a linearly-weightedadditive kernel function over two feature spaces.4 Experiments and ResultsFor train and evaluation purposes, we use theAmazon product reviews collected by Blitzer etal.
(2007) for the four product categories: books(B), DVDs (D), electronic items (E), and kitchenappliances (K).
There are 1000 positive and 1000negative sentiment labeled reviews for each do-main.
Moreover, each domain has on average17, 547 unlabeled reviews.
We use the standardsplit of 800 positive and 800 negative labeled re-views from each domain as training data, and therest (200+200) for testing.
For validation purposeswe use movie (source) and computer (target) do-mains, which were also collected by Blitzer et al(2007), but not part of the train/test domains.Experiments conducted using this validationdataset revealed that the performance of the pro-posed method is relatively insensitive to the valueof the regularization parameter ?
?
[10?3, 103].For the non-pivot prediction task we generate pos-itive and negative instances using the proceduredescribed in Section 3.2.
As a typical example,we have 88, 494 train instances from the bookssource domain and 141, 756 train instances fromthe target domain (1:5 ratio between positive andnegative instances in each domain).
The numberof pivots and non-pivots are set to NP= NS=NT= 500.In Figure 1, we compare the proposed methodagainst two baselines (NA, InDomain), currentstate-of-the-art methods for unsupervised cross-domain sentiment classification (SFA, SCL),word representation learning (GloVe), and cross-domain similarity prediction (CS).
The NA (no-adapt) lower baseline uses a classifier trained onsource labeled data to classify target test data with-out any domain adaptation.
The InDomain base-line is trained using the labeled data for the targetdomain, and simulates the performance we can ex-pect to obtain if target domain labeled data wereavailable.
Spectral Feature Alignment (SFA) (Panet al, 2010) and Structural Correspondence Learn-ing (SCL) (Blitzer et al, 2007) are the state-of-the-art methods for cross-domain sentiment clas-sification.
However, those methods do not learnword representations.We use Global Vector Prediction (GloVe) (Pen-nington et al, 2014), the current state-of-the-art word representation learning method, to learnword representations separately from the sourceand target domain unlabeled data, and use thelearnt representations in Eq.
13 for sentiment clas-sification.
In contrast to the joint word representa-tions learnt by the proposed method, GloVe sim-ulates the level of performance we would obtainby learning representations independently.
CS de-notes the cross-domain vector prediction methodproposed by Bollegala et al (2014).
AlthoughCS can be used to learn a vector-space transla-tion matrix, it does not learn word representations.Vertical bars represent the classification accuracies(i.e.
percentage of the correctly classified test in-stances) obtained by a particular method on targetdomain?s test data, and Clopper-Pearson 95% bi-nomial confidence intervals are superimposed.Differences in data pre-processing (tokeniza-tion/lemmatization), selection (train/test splits),feature representation (unigram/bigram), pivot se-lection (MI/frequency), and the binary classifica-tion algorithms used to train the final classifiermake it difficult to directly compare results pub-lished in prior work.
Therefore, we re-run the orig-inal algorithms on the same processed dataset un-der the same conditions such that any differencesreported in Figure 1 can be directly attributableto the domain adaptation, or word-representationlearning methods compared.All methods use l2regularized logistic regres-sion as the binary sentiment classifier, and the reg-737E?>B D?>B K?>B556065707580AccuracyB?>E D?>E K?>E5055606570758085AccuracyB?>D E?>D K?>D556065707580AccuracyNA GloVe SFA SCL CS ProposedB?>K E?>K D?>K5060708090AccuracyFigure 1: Accuracies obtained by different methods for each source-target pair in cross-domain sentiment classification.ularization coefficients are set to their optimal val-ues on the validation dataset.
SFA, SCL, and CSuse the same set of 500 pivots as used by the pro-posed method selected using NPMI (Section 3.4).Dimensionality n of the representation is set to300 for both GloVe and the proposed method.From Fig.
1 we see that the proposed methodreports the highest classification accuracies in all12 domain pairs.
Overall, the improvements of theproposed method over NA, GloVe, and CS are sta-tistically significant, and is comparable with SFA,and SCL.
The proposed method?s improvementover CS shows the importance of predicting wordrepresentations instead of counting.
The improve-ment over GloVe shows that it is inadequate tosimply apply existing word representation learn-ing methods to learn independent word represen-tations for the source and target domains.We must consider the correspondences betweenthe two domains as expressed by the pivots tojointly learn word representations.
As shown inFig.
2, the proposed method reports superior ac-curacies over GloVe across different dimension-alities.
Moreover, we see that when the dimen-sionality of the representations increases, initiallyaccuracies increase in both methods and saturatesafter 200 ?
600 dimensions.
However, furtherincreasing the dimensionality results in unstableand some what poor accuracies due to overfit-ting when training high-dimensional representa-tions.
Although our word representations learntby the proposed method are not specific to senti-ment classification, the fact that it clearly outper-forms SFA and SCL in all domain pairs is encour-aging, and implies the wider-applicability of the0 200 400 600 800 1000Dimensions6062646668707274AccuracyProposedGloVeNAFigure 2: Accuracy vs. dimensionality of the representation.proposed method for domain adaptation tasks be-yond sentiment classification.5 ConclusionWe proposed an unsupervised method for learningcross-domain word representations using a givenset of pivots and non-pivots selected from a sourceand a target domain.
Moreover, we proposed a do-main adaptation method using the learnt word rep-resentations.Experimental results on a cross-domain senti-ment classification task showed that the proposedmethod outperforms several competitive baselinesand achieves best sentiment classification accura-cies for all domain pairs.
In future, we plan toapply the proposed method to other types of do-main adaptation tasks such as cross-domain part-of-speech tagging, named entity recognition, andrelation extraction.Source code and pre-processed data etc.
for thispublication are publicly available3.3www.csc.liv.ac.uk/?danushka/prj/darep738ReferencesMarco Baroni and Alessandro Lenci.
2010.
Dis-tributional memory: A general framework forcorpus-based semantics.
Computational Linguis-tics, 36(4):673 ?
721.Marco Baroni, Georgiana Dinu, and Germ?anKruszewski.
2014.
Don?t count, predict!
asystematic comparison of context-counting vs.context-predicting semantic vectors.
In Proc.
ofACL, pages 238?247.Yoshua Bengio, R?ejean Ducharme, Pascal Vincent, andChristian Jauvin.
2003.
A neural probabilistic lan-guage model.
Journal of Machine Learning Re-search, 3:1137 ?
1155.John Blitzer, Ryan McDonald, and Fernando Pereira.2006.
Domain adaptation with structural correspon-dence learning.
In Proc.
of EMNLP, pages 120 ?128.John Blitzer, Mark Dredze, and Fernando Pereira.2007.
Biographies, bollywood, boom-boxes andblenders: Domain adaptation for sentiment classi-fication.
In Proc.
of ACL, pages 440 ?
447.Danushka Bollegala, Yutaka Matsuo, and MitsuruIshizuka.
2011a.
Relation adaptation: Learning toextract novel relations with minimum supervision.In Proc.
of IJCAI, pages 2205 ?
2210.Danushka Bollegala, David Weir, and John Carroll.2011b.
Using multiple sources to construct a senti-ment sensitive thesaurus for cross-domain sentimentclassification.
In ACL/HLT, pages 132 ?
141.Danushka Bollegala, Mitsuru Kusumoto, YuichiYoshida, and Ken ichi Kawarabayashi.
2013a.Mining for analogous tuples from an entity-relationgraph.
In Proc.
of IJCAI, pages 2064 ?
2070.Danushka Bollegala, Yutaka Matsuo, and MitsuruIshizuka.
2013b.
Minimally supervised novel re-lation extraction using latent relational mapping.IEEE Transactions on Knowledge and Data Engi-neering, 25(2):419 ?
432.Danushka Bollegala, David Weir, and John Carroll.2014.
Learning to predict distributions of wordsacross domains.
In Proc.
of ACL, pages 613 ?
623.Gerlof Bouma.
2009.
Normalized (pointwsie) mutualinformation in collocation extraction.
In Proc.
ofGSCL, pages 31 ?
40.Ronan Collobert, Jason Weston, Leon Bottou, MichaelKarlen, Koray Kavukcuoglu, and Pavel Kuska.2011.
Natural language processing (almost) fromscratch.
Journal of Machine Learning Research,12:2493 ?
2537.Nguyen Tuan Duc, Danushka Bollegala, and MitsuruIshizuka.
2010.
Using relational similarity betweenword pairs for latent relational search on the web.
InIEEE/WIC/ACM International Conference on WebIntelligence and Intelligent Agent Technology, pages196 ?
199.John Duchi, Elad Hazan, and Yoram Singer.
2011.Adaptive subgradient methods for online learningand stochastic optimization.
Journal of MachineLearning Research, 12:2121 ?
2159, July.Hui Fang.
2008.
A re-examination of query expansionusing lexical resources.
In Proc.
of ACL, pages 139?147.Stefano Faralli and Roberto Navigli.
2012.
A newminimally-supervised framework for domain wordsense disambiguation.
In EMNLP, pages 1411 ?1422.Xavier Glorot, Antoine Bordes, and Yoshua Bengio.2011.
Domain adaptation for large-scale sentimentclassification: A deep learning approach.
In Proc.
ofICML.Stephan Gouws, Yoshua Bengio, and Greg Corrado.2015.
Bilbowa: Fast bilingual distributed represen-tations without word alignments.
In Proc.
of ICML.Karl Moritz Hermann and Phil Blunsom.
2014.
Mul-tilingual distributed representations without wordalignment.
In Proc.
of ICLR.Eric H. Huang, Richard Socher, Christopher D. Man-ning, and Andrew Y. Ng.
2012.
Improving wordrepresentations via global context and multiple wordprototypes.
In Proc.
of ACL, pages 873 ?
882.Jing Jiang and ChengXiang Zhai.
2007a.
Instanceweighting for domain adaptation in nlp.
In ACL2007, pages 264 ?
271.Jing Jiang and ChengXiang Zhai.
2007b.
A two-stageapproach to domain adaptation for statistical classi-fiers.
In CIKM 2007, pages 401?410.Alexandre Klementiev, Ivan Titov, and Binod Bhat-tarai.
2012.
Inducing crosslingual distributed rep-resentations of words.
In Proc.
of COLING, pages1459 ?
1474.David McClosky, Eugene Charniak, and Mark John-son.
2010.
Automatic domain adaptation for pars-ing.
In Proc.
of NAACL/HLT, pages 28 ?
36.Tomas Mikolov, Kai Chen, and Jeffrey Dean.
2013a.Efficient estimation of word representation in vectorspace.
CoRR.Tomas Mikolov, Ilya Sutskever, Kai Chen, Gregory S.Corrado, and Jeffrey Dean.
2013b.
Distributed rep-resentations of words and phrases and their compo-sitionality.
In Proc.
of NIPS, pages 3111 ?
3119.Tomas Mikolov, Wen tau Yih, and Geoffrey Zweig.2013c.
Linguistic regularities in continous spaceword representations.
In NAACL?13, pages 746 ?751.739Jeff Mitchell and Mirella Lapata.
2008.
Vector-basedmodels of semantic composition.
In Proc.
of ACL-HLT, pages 236 ?
244.Andriy Mnih and Geoffrey E. Hinton.
2008.
A scal-able hierarchical distributed language model.
InProc.
of NIPS, pages 1081?1088.Andriy Mnih and Koray Kavukcuoglu.
2013.
Learningword embeddings efficiently with noise-contrastiveestimation.
In Proc.
of NIPS.Arvind Neelakantan, Jeevan Shankar, Alexandre Pas-sos, and Andrew McCallum.
2014.
Efficient non-parametric estimation of multiple embeddings perword in vector space.
In Proc.
of EMNLP, pages1059?1069.Sinno Jialin Pan, Xiaochuan Ni, Jian-Tao Sun, QiangYang, and Zheng Chen.
2010.
Cross-domain sen-timent classification via spectral feature alignment.In Proc.
of WWW, pages 751 ?
760.Jeffery Pennington, Richard Socher, and Christo-pher D. Manning.
2014.
Glove: global vectors forword representation.
In Proc.
of EMNLP.Joseph Reisinger and Raymond J. Mooney.
2010.Multi-prototype vector-space models of word mean-ing.
In Proc.
of HLT-NAACL, pages 109 ?
117.Michael Roth and Kristian Woodsend.
2014.
Com-position of word representations improves semanticrole labelling.
In Proc.
of EMNLP, pages 407?413.Tobias Schnabel and Hinrich Sch?utze.
2013.
Towardsrobust cross-domain domain adaptation for part-of-speech tagging.
In Proc.
of IJCNLP, pages 198 ?206.Richard Socher, Cliff Chiung-Yu Lin, Andrew Ng, andChris Manning.
2011a.
Parsing natural scenes andnatural language with recursive neural networks.
InICML?11.Richard Socher, Jeffrey Pennington, Eric H. Huang,Andrew Y. Ng, and Christopher D. Manning.
2011b.Semi-supervised recursive autoencoders for predict-ing sentiment distributions.
In Proc.
of EMNLP,pages 151?161.Peter D. Turney and Patrick Pantel.
2010.
From fre-quency to meaning: Vector space models of seman-tics.
Journal of Aritificial Intelligence Research,37:141 ?
188.Will Y. Zou, Richard Socher, Daniel Cer, and Christo-pher D. Manning.
2013.
Bilingual word embed-dings for phrase-based machine translation.
In Proc.of EMNLP?13, pages 1393 ?
1398.740
