Proceedings of the 4th International Workshop on Semantic Evaluations (SemEval-2007), pages 366?369,Prague, June 2007. c?2007 Association for Computational LinguisticsUCB: System Description for SemEval Task #4Preslav I. NakovEECS, CS divisionUniversity of California at BerkeleyBerkeley, CA 94720nakov@cs.berkeley.eduMarti A. HearstSchool of InformationUniversity of California at BerkeleyBerkeley, CA 94720hearst@ischool.berkeley.eduAbstractThe UC Berkeley team participated in theSemEval 2007 Task #4, with an approachthat leverages the vast size of the Web in or-der to build lexically-specific features.
Theidea is to determine which verbs, preposi-tions, and conjunctions are used in sentencescontaining a target word pair, and to com-pare those to features extracted for otherword pairs in order to determine which aremost similar.
By combining these Web fea-tures with words from the sentence context,our team was able to achieve the best resultsfor systems of category C and third best forsystems of category A.1 IntroductionSemantic relation classification is an important butunderstudied language problem arising in manyNLP applications, including question answering, in-formation retrieval, machine translation, word sensedisambiguation, information extraction, etc.
Thisyear?s SemEval (previously SensEval) competitionhas included a task targeting the important specialcase of Classification of Semantic Relations betweenNominals.
In the present paper we describe the UCBsystem which took part in that competition.The SemEval dataset contains a total of 7 se-mantic relations (not exhaustive and possibly over-lapping), with 140 training and about 70 testingsentences per relation.
Sentence classes are ap-proximately 50% negative and 50% positive (?nearmisses?).
Table 1 lists the 7 relations together withsome examples.# Relation Name Examples1 Cause-Effect hormone-growth, laugh-wrinkle2 Instrument-Agency laser-printer, ax-murderer3 Product-Producer honey-bee, philosopher-theory4 Origin-Entity grain-alcohol, desert-storm5 Theme-Tool work-force, copyright-law6 Part-Whole leg-table, door-car7 Content-Container apple-basket, plane-cargoTable 1: SemEval dataset: Relations with examples(context sentences are not shown).Each example consists of a sentence, two nomi-nals to be judged on whether they are in the targetsemantic relation, manually annotated WordNet 3.0sense keys for these nominals, and the Web queryused to obtain that example:"Among the contents of the <e1>vessel</e1>were a set of carpenters <e2>tools</e2>,several large storage jars, ceramicutensils, ropes and remnants of food, aswell as a heavy load of ballast stones.
"WordNet(e1) = "vessel%1:06:00::",WordNet(e2) = "tool%1:06:00::",Content-Container(e2, e1) = "true",Query = "contents of the * were a"2 Related WorkLauer (1995) proposes that eight prepositions areenough to characterize the relation between nounsin a noun-noun compound: of, for, in, at, on, from,with or about.
Lapata and Keller (2005) improveon his results by using Web statistics.
Rosario et al(2002) use a ?descent of hierarchy?, which charac-terizes the relation based on the semantic category ofthe two nouns.
Girju et al (2005) apply SVM, deci-sion trees, semantic scattering and iterative seman-366tic specialization, using WordNet, word sense dis-ambiguation, and linguistic features.
Barker and Sz-pakowicz (1998) propose a two-level hierarchy with5 classes at the upper level and 30 at the lower level.Turney (2005) introduces latent relational analysis,which uses the Web, synonyms, patterns like ?X forY ?, ?X such as Y ?, etc., and singular value decom-position to smooth the frequencies.
Turney (2006)induces patterns from the Web, e.g.
CAUSE is bestcharacterized by ?Y * causes X?, and ?Y in * earlyX?
is the best pattern for TEMPORAL.
Kim and Bald-win (2006) propose to use a predefined set of seedverbs and multiple resources: WordNet, CoreLex,and Moby?s thesaurus.
Finally, in a previous publi-cation (Nakov and Hearst, 2006), we make the claimthat the relation between the nouns in a noun-nouncompound can be characterized by the set of inter-vening verbs extracted from the Web.3 MethodGiven an entity-annotated example sentence, we re-duce the target entities e1 and e2 to single nounsnoun1 and noun2, by keeping their last nounsonly, which we assume to be the heads.
We thenmine the Web for sentences containing both noun1and noun2, from which we extract features, con-sisting of word(s), part of speech (verb, preposi-tion, verb+preposition, coordinating conjunction),and whether noun1 precedes noun2.
Table 2 showssome example features and their frequencies.We start with a set of exact phrase queriesagainst Google: ?infl1 THAT * infl2?, ?infl2THAT * infl1?, ?infl1 * infl2?, and ?infl2 *infl1?, where infl1 and infl2 are inflectional vari-ants of noun1 and noun2, generated using WordNet(Fellbaum, 1998); THAT can be that, which, or who;and * stands for 0 or more (up to 8) stars separatedby spaces, representing the Google * single-wordwildcard match operator.
For each query, we collectthe text snippets from the result set (up to 1000 perquery), split them into sentences, assign POS tagsusing the OpenNLP tagger1, and extract features:Verb: If one of the nouns is the subject, and theother one is a direct or indirect object of that verb,we extract it and we lemmatize it using WordNet(Fellbaum, 1998).
We ignore modals and auxil-1OpenNLP: http://opennlp.sourceforge.netFreq.
Feature POS Direction2205 of P 2?
11923 be V 1?
2771 include V 1?
2382 serve on V 2?
1189 chair V 2?
1189 have V 1?
2169 consist of V 1?
2148 comprise V 1?
2106 sit on V 2?
181 be chaired by V 1?
278 appoint V 1?
277 on P 2?
166 and C 1?
2. .
.
.
.
.
.
.
.
.
.
.Table 2: Most frequent features for committeemember.
V stands for verb, P for preposition, andC for coordinating conjunction.iaries, but retain the passive be, verb particles andprepositions (in case of indirect object).Preposition: If one of the nouns is the head ofan NP which contains a PP, inside which there is anNP headed by the other noun (or an inflectional formthereof), we extract the preposition heading that PP.Coordination: If the two nouns are the heads oftwo coordinated NPs, we extract the coordinatingconjunction.In addition, we include some non-Web features2:Sentence word: We use as features the wordsfrom the context sentence, after stop words removaland stemming with the Porter stemmer.Entity word: We also use the lemmas of thewords that are part of ei (i = 1, 2).Query word: Finally, we use the individualwords that are part of the query string.
This featureis used for category C runs only (see below).Once extracted, the features are used to calculatethe similarity between two noun pairs.
Each featuretriplet is assigned a weight.
We wish to downweightvery common features, such as ?of?
used as a prepo-sition in the 2 ?
1 direction, so we apply tf.idfweighting to each feature.
We then use the followingvariant of the Dice coefficient to compare the weightvectors A = (a1, .
.
.
, an) and B = (b1, .
.
.
, bn):Dice(A,B) =2?
?ni=1 min(ai, bi)?ni=1 ai +?ni=1 bi(1)This vector representation is similar to that of2Features have type prefix to prevent them from mixing.367System Relation P R F AccUCB-A1 Cause-Effect 58.2 78.0 66.7 60.0Instrument-Agency 62.5 78.9 69.8 66.7Product-Producer 77.3 54.8 64.2 59.1Origin-Entity 67.9 52.8 59.4 67.9Theme-Tool 50.0 31.0 38.3 59.2Part-Whole 51.9 53.8 52.8 65.3Content-Container 62.2 60.5 61.3 60.8average 61.4 58.6 58.9 62.7UCB-A2 Cause-Effect 58.0 70.7 63.7 58.8Instrument-Agency 65.9 71.1 68.4 67.9Product-Producer 80.0 77.4 78.7 72.0Origin-Entity 60.6 55.6 58.0 64.2Theme-Tool 45.0 31.0 36.7 56.3Part-Whole 41.7 38.5 40.0 58.3Content-Container 56.4 57.9 57.1 55.4average 58.2 57.5 57.5 61.9UCB-A3 Cause-Effect 62.5 73.2 67.4 63.8Instrument-Agency 65.9 76.3 70.7 69.2Product-Producer 75.0 67.7 71.2 63.4Origin-Entity 48.4 41.7 44.8 54.3Theme-Tool 62.5 51.7 56.6 67.6Part-Whole 50.0 46.2 48.0 63.9Content-Container 64.9 63.2 64.0 63.5average 61.3 60.0 60.4 63.7UCB-A4 Cause-Effect 63.5 80.5 71.0 66.2Instrument-Agency 70.0 73.7 71.8 71.8Product-Producer 76.3 72.6 74.4 66.7Origin-Entity 50.0 47.2 48.6 55.6Theme-Tool 61.5 55.2 58.2 67.6Part-Whole 52.2 46.2 49.0 65.3Content-Container 65.8 65.8 65.8 64.9average 62.7 63.0 62.7 65.4Baseline (majority) 81.3 42.9 30.8 57.0Table 3: Task 4 results.
UCB systems A1-A4.Lin (1998), who measures word similarity by usingtriples extracted from a dependency parser.
In par-ticular, given a noun, he finds all verbs that have itas a subject or object, and all adjectives that modifyit, together with the corresponding frequencies.4 Experiments and ResultsParticipants were asked to classify their systemsinto categories depending on whether they used theWordNet sense (WN) and/or the Google query (GC).Our team submitted runs for categories A (WN=no,QC=no) and C (WN=no, QC=yes) only, since webelieve that having the target entities annotated withthe correct WordNet senses is an unrealistic assump-tion for a real-world application.Following Turney and Littman (2005) and Barkerand Szpakowicz (1998), we used a 1-nearest-neighbor classifier.
Given a test example, we calcu-lated the Dice coefficient between its feature vectorSystem Relation P R F AccUCB-C1 Cause-Effect 58.5 75.6 66.0 60.0Instrument-Agency 65.2 78.9 71.4 69.2Product-Producer 81.4 56.5 66.7 62.4Origin-Entity 67.9 52.8 59.4 67.9Theme-Tool 50.0 31.0 38.3 59.2Part-Whole 51.9 53.8 52.8 65.3Content-Container 62.2 60.5 61.3 60.8Average 62.4 58.5 59.4 63.5UCB-C2 Cause-Effect 58.0 70.7 63.7 58.8Instrument-Agency 67.5 71.1 69.2 69.2Product-Producer 80.3 79.0 79.7 73.1Origin-Entity 60.6 55.6 58.0 64.2Theme-Tool 50.0 37.9 43.1 59.2Part-Whole 43.5 38.5 40.8 59.7Content-Container 56.4 57.9 57.1 55.4Average 59.5 58.7 58.8 62.8UCB-C3 Cause-Effect 62.5 73.2 67.4 63.8Instrument-Agency 68.2 78.9 73.2 71.8Product-Producer 74.1 69.4 71.7 63.4Origin-Entity 56.8 58.3 57.5 61.7Theme-Tool 62.5 51.7 56.6 67.6Part-Whole 50.0 42.3 45.8 63.9Content-Container 64.9 63.2 64.0 63.5Average 62.7 62.4 62.3 65.1UCB-C4 Cause-Effect 63.5 80.5 71.0 66.2Instrument-Agency 70.7 76.3 73.4 73.1Product-Producer 76.7 74.2 75.4 67.7Origin-Entity 59.0 63.9 61.3 64.2Theme-Tool 63.0 58.6 60.7 69.0Part-Whole 52.2 46.2 49.0 65.3Content-Container 64.1 65.8 64.9 63.5Average 64.2 66.5 65.1 67.0Baseline (majority) 81.3 42.9 30.8 57.0Table 4: Task 4 results.
UCB systems C1-C4.and the vector of each of the training examples.
Ifthere was a single highest-scoring training example,we predicted its class for that test example.
Oth-erwise, if there were ties for first, we assumed theclass predicted by the majority of the tied examples.If there was no majority, we predicted the class thatwas most likely on the training data.
Regardless ofthe classifier?s prediction, if the head words of thetwo entities e1 and e2 had the same lemma, we clas-sified that example as negative.Table 3 and 4 show the results for our A and Cruns for different amounts of training data: 45 (A1,C1), 90 (A2, C2), 105 (A3, C3) and 140 (A4, C4).All results are above the baseline: always proposethe majority label (?true?/?false?)
in the test set.
Infact, our category C system is the best-performing(in terms of F and Acc) among the participatingsystems, and we achieved the third best results forcategory A.
Our category C results are slightly but368consistently better than forA for all measures (P ,R,F , Acc), which suggests that knowing the query ishelpful.
Interestingly, systems UCB-A2 and UCB-C2 performed worse than UCB-A1 and UCB-C1,which means that having more training data does notnecessarily help with a 1NN classifier.Table 5 shows additional analysis for A4 and C4.We study the effect of adding extra Google contexts(using up to 10 stars, rather than 8), and using differ-ent subsets of features.
We show the results for: (a)leave-one-out cross-validation on the training data,(b) on the test data, and (c) our official UCB runs.Acknowledgements: This work is supported inpart by NSF DBI-0317510.ReferencesKen Barker and Stan Szpakowicz.
1998.
Semi-automaticrecognition of noun modifier relationships.
In Proceedingsof COLING-ACL?98, pages 96?102.Christiane Fellbaum, editor.
1998.
WordNet: An ElectronicLexical Database.
MIT Press.Roxana Girju, Dan Moldovan, Marta Tatu, and Daniel Antohe.2005.
On the semantics of noun compounds.
ComputerSpeech and Language, 19(4):479?496.Su Nam Kim and Timothy Baldwin.
2006.
Interpreting seman-tic relations in noun compounds via verb semantics.
In Pro-ceedings of COLING/ACL 2006.
(poster), pages 491?498.Mirella Lapata and Frank Keller.
2005.
Web-based models fornatural language processing.
ACM Transactions on Speechand Language Processing, 2:1?31.Mark Lauer.
1995.
Designing Statistical Language Learners:Experiments on Noun Compounds.
Ph.D. thesis, Departmentof Computing Macquarie University NSW 2109 Australia.Dekang Lin.
1998.
An information-theoretic definition of sim-ilarity.
In Proceedings of International Conference on Ma-chine Learning, pages 296?304.Preslav Nakov and Marti Hearst.
2006.
Using verbs to charac-terize noun-noun relations.
In Proceedings of AIMSA, pages233?244.Barbara Rosario, Marti Hearst, and Charles Fillmore.
2002.The descent of hierarchy, and selection in relational seman-tics.
In ACL, pages 247?254.Peter Turney and Michael Littman.
2005.
Corpus-based learn-ing of analogies and semantic relations.
Machine LearningJournal, 60(1-3):251?278.Peter Turney.
2005.
Measuring semantic similarity by latentrelational analysis.
In Proceedings IJCAI, pages 1136?1141.Peter Turney.
2006.
Expressing implicit semantic relationswithout supervision.
In Proceedings of COLING-ACL,pages 313?320.Features Used Leave-1-out Test UCBCause-Effectsent 45.7 50.0p 55.0 53.8v 59.3 68.8v + p 57.1 63.7v + p + c 70.5 67.5v + p + c + sent 58.5 66.2 66.2v + p + c + sent + query 59.3 66.2 66.2Instrument-Agencysent 63.6 59.0p 62.1 70.5v 71.4 69.2v + p 70.7 70.5v + p + c 70.0 70.5v + p + c + sent 68.6 71.8 71.8v + p + c + sent + query 70.0 73.1 73.1Product-Producersent 47.9 59.1p 55.7 58.1v 70.0 61.3v + p 66.4 65.6v + p + c 67.1 65.6v + p + c + sent 66.4 69.9 66.7v + p + c + sent + query 67.9 69.9 67.7Origin-Entitysent 64.3 72.8p 63.6 56.8v 69.3 71.6v + p 67.9 69.1v + p + c 66.4 70.4v + p + c + sent 68.6 72.8 55.6v + p + c + sent + query 67.9 72.8 64.2Theme-Toolsent 66.4 69.0p 56.4 56.3v 61.4 70.4v + p 56.4 67.6v + p + c 57.1 69.0v + p + c + sent 52.1 62.0 67.6v + p + c + sent + query 52.9 62.0 69.0Part-Wholesent 47.1 51.4p 57.1 54.1v 60.0 66.7v + p 62.1 63.9v + p + c 61.4 63.9v + p + c + sent 60.0 61.1 65.3v + p + c + sent + query 60.0 61.1 65.3Content-Containersent 56.4 54.1p 57.9 59.5v 71.4 67.6v + p 72.1 67.6v + p + c 72.9 67.6v + p + c + sent 69.3 67.6 64.9v + p + c + sent + query 71.4 71.6 63.5Average A4 67.3 65.4Average C4 68.1 67.0Table 5: Accuracy for different features and extraWeb contexts: on leave-one-out cross-validation,on testing data, and in the official UCB runs.369
