A Decision Tree Method for Finding and Classifying Namesin Japanese TextsSatosh i  Sek ine  Ra lph  Gr i shmanComputer  Science DepartmentNew York University715 Broadway, 7th floorNew York, NY 10003, USA\ [ sek ine  \] g r i  shman\] ?cs.
nyu.
eduH i royuk i  Sh innouDepartment of Systems EngineeringIbaraki UniversityNakanarusawa, 4-12-1Hitachi, Ibaraki, 316, Japanshinnou~lily.dse.ibaraki.ac.jpAbst ractThis paper describes a system which uses a deci-sion tree to find and classify names in Japanesetexts.
The decision tree uses part-of-speech,character type, and special dictionary informa-tion to determine the probability that a particu-lar type of name opens or closes at a given po-sition in the text.
The output is generated fromthe consistent sequence of name opens and namecloses with the highest probability.
This systemdoes not require any human adjustment.
Ex-periments indicate good accuracy with a smallamount of training data, and demonstrate thesystem's portability.
The issues of training datasize and domain dependency are discussed.1 In t roduct ionFor some NLP applications, it is important oidentify, "named entities" (NE), such as personnames, organization ames, time, date, or moneyexpressions in the text.
For example, in informa-tion extraction systems, it is crucial to identifythem in order to provide the knowledge to beextracted, and in machine translation systems,they are useful for creating translations of un-known words or for disambiguation.
However, itis not easy to identify these names, because theyinvolve unknown words, and hence the strategyof listing candidates won't work.
Also, it is some-times hard to determine the category of propernouns, like distinguishing a person name froma company name.
These phenomena re oftendifferent from domain to domain.
One domainmay use a special pattern which is not found inother domains.In this paper, we will present a supervisedlearning system whicil finds and classifies namedentities in Japanese newspaper texts.
Recently,several systems have been proposed for this task,but many of them use hand-coded patterns.
Cre-171ating these patterns is laborious work, and whenwe adapt these systems to a new domain or anew definition of named entities, it is likely toneed a large amount of additional work.
Onthe other hand, in a supervised learning system,what is needed to adapt the system is to makenew training data..
While this is also not a veryeasy task, it would be easier than creating com-plicated rules.
For example, based on our expe-rience, 100 training articles can be created in aday.There also have been several machine learningsystems applied to this task.
However, these ei-ther 1) partially need hand-made rules, 2) haveparameters which must be adjusted by hand, or3) do not perform well by fully automatic means.Our system does not work fully automaticallyand also needs special dictionaries, but performswelt and does not have parameters to be adjustedby hand.
We will discuss one of the related sys-tems in a later section.The issue of training data size will be dis-cussed based on experiments using differentsizes of training data.
In order to demonstratethe portability of our system, we ran the systemon a new domain with a new type of named en-tity.
The experiment shows that the portabilityof the system is quite good and the performanceis satisfactory.2 TaskThe task is to find and classify several types ofnamed entity items in texts, shown in Table 1.We use the task definition provided in the MET-2 guidelines (Multilingual Entity Task; the for-mal definition will be published in May 1998).
"Executive position" is a new category which isused in the portability experiment only and isnot part of the MET definition.There are some idiosyncratic definitions.
ForNamed Entity ExamplesOrganizationPersonLocationPositionDateTimeMoneyPercentMatsushita, M atsushitaElectric Industrial Co.Ltd.Mr.
Matsushita, MikeU.S.A., MatsushitaPresident, ProfessorMarch 5, 21st century12:09, noon, morning100,000 yen, 1 ECU10%, a quarterTable h Named Entities (NE)example, a sub-organization expression like "Ex-ecutive Staff" should be identified only when itfollows an organization in proper expression.
So,in the expression "Defense Ministry's ExecutiveStaff", "Executive Staff" should be identified;however, it should not be identified if it appearsalone in a sentence.
Also, a country expressionat the head of an organization should be iden-tified if it is expressed by one Chinese charac-ter, but it should not when it is expressed byKatakana characters.
Although we find some id-iosyncratic definitions in the guidelines, we willuse them, because there are such difficulties innature and we can't easily find another reason-able definition.3 A lgor i thmIn this section, the algorithm of the system willbe presented.
There are two phases, one forcreating the decision tree from training data(training phase) and the other for generating thetagged output based on the decision tree (testingphase).
We use a Japanese morphological ana-lyzer, JUMAN (JUMAN, 1997) and a programpackage for decision trees, C4.5 (Quinlan, 1993).We use three kinds of feature sets in the decisiontree:?
Part-of-speech tagged by JUMANWe define the set of our categoriefi based onits major category and minor category.?
Character type informationCharacter type, like Kanji, Hiragana,Katakana, alphabet, number or symbol, etc.and some combinations of these.172?
Special DictionariesLists of entities created based on JUMANdictionary entries, lists found on the Web orbased on human knowledge.
Table 2 showsthe number of entities in each dictionary.Organization ame has two types of dictio-nary; one for proper names and tile otherfor general nouns.
An example of the lattercase is "Executive Staff", mentioned before.name- name name-Entity prefix suffixOrg.
9 7018/49 96Person 0 17851 7Loc.
0 14863 61Position 0 75 0Date 24 198 29Time 2 25 5Money 22 0 39Percent 0 99 3Table 2: Special Dictionary EntryCreating the special dictionaries is not very easy,but it is not very laborious work.
The initial dic-tionary was built in about a week.
In tile courseof the system development, in particular whilecreating the training corpus, we added some en-tities to the dictionaries.The decision tree gives an output for the be-ginning and tile ending position of each token.
Itis one of the 4 possible combinations of opening,continuation and closing for each named entitytype, or having no named entity, shown in Table3.
When we have 8 named entity types, thereare 33 kinds of output.
For example, if an or-outputOP-CLOP-CNCN-CNCN-CLnonebeginning ending tokenof token of token isopeningopeningcont.cont.noneclosingcont.cont.closingnoneNE itselfstarting NEmiddle of NEending NEnot NETable 3: Five types of Outputganization name covers three words, h. B and C~/ /and the next word D has no named entity, thenwe will have the following data:A : org-OP-CNB : org-CN-CNC : org-CN-CLD : noneNote that there is no overlapping or embeddingof named entities.
An example of real data isshown in Appendix A.There could be a problem, in the testing phase,if we just use the deterministic decision createdby the tree.
Because the decisions are made lo-cally, the system could make an inconsistent se-quence of decisions overall.
For example, onetoken could be tagged as the opening of an orga-nization, while the next token might be taggedas the closing of person name.
We can think ofseveral strategies to solve this problem (for ex-ample, the method adopted by (Bennett et al1997) will be described in a later section), butwe used a probabilistic method.There will usually be more than one tag in theleaf of a decision tree.
At a leaf we don't justrecord the most probable tag; rather, we keep theprobabilities of tile all possible tags for that leaf.In this way we can salvage cases where ~ tagis part of the most probable globally-consistenttagging of the text, even though it is not themost probable tag for this token, and so would bediscarded if we made a deterministic decision ateach token.
Note that.
we did not apply smooth-ing technique, which might be able to avoid thedata sparseness problem.
More about the proba-bilistic method will be explained in the next sec-tion.Training PhaseFirst, the training sentences are segmentedand part-of-speech tagged by JUMAN.
Theneach token is analyzed by its character type andis matched against entries in the special dictio-naries.
One token can match entries in severaldictionaries.
For example, "Matsushita" couldmatch the organization, person anfflocation dic-tionaries.Using the training data, a decision tree is built.It learns about the opening and closing of namedentities based on the three kinds of informationof the previous, current and following tokens.173The three types of information are tile part-of-speech, character type and special dictionary in-formation described above.Test ing PhaseIn the testing phase, the first three steps, to-ken segmentation and part-of-speech tagging byJUMAN, analysis of character type, and specialdictionary look-up, are identical to that in thetraining phase.
Then, in order to find the proba-bilities of opening and closing a named entity foreach token, the properties of the previous, cur-rent and following tokens are examined againstthe decision tree.
Appendix 13 shows two exam-ple paths in the decision tree.
For each token,the probabilities of 'none' and the four combina-tions of answer pairs for each named entity typeare assigned.
For instance, if we have 7 namedentity types, then 29 probabilities are generated.Once the probabilities for all the tokens ina sentence are assigned, the remaining task isto discover the most probable consistent paththrough the sentence.
Here, a consistent pathmeans that for example, a path can't haveorg-0P-CN and date-0P-CL in a row, but callhave loc-0P-CN and loc-CN-CL.
The outputis generated from the consistent sequence withthe highest probability for each sentence.
TheViterbi algorithm is used in the search; this canbe run in time linear in the length of the input.4 ExampleAppendix A shows an example sentence alongwith three types of information, part-of-speech.character type and special dictionary informa-tion, and information of opening and closing ofnamed entities.
Appendix 13 shows two examplepaths in the decision tree.
For the purpose ofdemonstration, we used the seventh and eighthtoken of the example sentence in Appendix A.Each line corresponds to a question asked bythe tree nodes along the path.
The last lineshows the probabilities of named entity informa-tion which have none-zero probability.
This in-stance demonstrates how the probability methodworks.
As we can see, the probability of none forthe seventh token ( I suraeru  = Israel) is higherthan that for the opening of organization (0.67to 0.33), but in the eighth token (Ke isatsu =Police), the probability of closing organization ismuch higher than none (0.86 to 0.14).
The com-bined probabilities of the two consistent pw:hsare calculated.
One of these paths makes thetwo tokens an organization entity while along theother path, neither token is part of a named en-tity.
The probabilities are higher in the first case(0.28) than that in the latter case (0.09), So thetwo tokens are tagged as an organization entity.5 Exper imentsIn this section, the experiments will be de-scribed.
We chose two domains for the exper-iments.
One is the vehicle accident report do-main.
Newspaper articles in the domain reportaccidents of vehicles, like car, train or airplane.The other is the executive succession domain,articles in this domain report succession eventsof executives, like president, vice president orCEO.
We have 103 training articles in the acci-dent domain, which contain 2.368 NE's and 11evaluation articles which were hidden from thedeveloper, In the evaluation articles, there are258 NE items (58 organization, 30 person, 100location, 47 date, 21 time and 2 money expres-sions).
Also, we have 70 training articles, whichcontain 2,406 NE's and 17 evaluation articles inthe succession domain.
In the evaluation arti-cles, there are 566 NE items (113 organization,114 person, 67 location.
183 position.
77 date.
1time.
9 money and 2 percent expressions).5.1 Acc ident  Repor t  DomainFirst.
we will report on the experiment on the ac-cident domain.
Basically, this is the initial targetdomain of the system.The result is shown in Table 4.
The F-scoresbased on recall and precision are shown.
'Re-call' is the percentage of the correct answersamong the answers in the key provided by hu-man.
'Precision' is the percentage of the correctanswers among the answers proposed by the sys-tem.
'F-score' is a measurement combining thetwo figures.
See (Tipster2, 1996) for more "de-tail" definition of F-score, recall and precision.They are compared with the results producedby JUMAN's part-of-speech information and theaverage scores in MET1, reported in (Tipster2,1996).
The result from JUMAN is created basedon JUMAN version 3.3's output alone 1.
WhenI Latest version may have better performance than theresults reported here.
Also remember that the definitionsit identifies a sequence of locations, persons or.other proper nouns, then we tag the sequencewith location, person or organization, respec-tively.
The MET1 evaluation was conducted oncompletely different exts and on a. different do-main, so it is not directly comparable, but sincethe task definitions are almost the same, we be-lieve it gives a rough point of comparison.
Notethat for the MET1 evaluation, there were about300 training articles compared to our 100 train-ing articles.
Also, they did not report the scoresby each individual participant.Entity OurscoreOrg.
86Person 91Loc.
87Date 96Time ; I 91Money 100PercentOverall 85JUMAN MET1only ave. score56635173778294939596Table 4: Result in Accident Report DomainWe believe these results are quite good andindicate the capability of our system.
In terms ofexecution time, the training phase takes about 5minutes, of which JUMAN and the decision treecreation take most of the time.
It takes less thana minute to create the named entity output, andagain JUMAN takes the bulk of the time.5.2 Issue of  Tra in ing SizeIt is quite nice that we can get this level of perfor-mance with only about 100 training articles.
Itis interesting to investigate how much trainingdata is needed to achieve a good performance.We created 8 small training sets of different size,and ran the system using these training data.Note that we used the same dictionaries for allthe experiments, which were generated by sev-eral means including the items in the entire train-ing data.
Table 5 shows the results.
The size ofthe training set is indicated by the number of ar-ticles and the number of NE in the training data.It is amazing that the performance is not greatlydegraded even with 9 articles.
Also, even withare different.174only one article, our system can achieve 68 F-score.
Actually, the three sets of 1-article train-ing data were created from each article in the3-article training data, and we can see that theperformance using tlle 3-article training data.
ismainly derived from the high performance sin-gle article.
So, we believe that once you have agood coverage dictionaries and some amount ofstandard patterns in the training data, the sys-tem can achieve fairly good performance.
Weobserved that tile article which gives high perfor-mance contains a good variety of many namedentities.Size ofTraining103 (2368)69 (1586)35 (721)18 (384)9 (216)3 (59)1 (23/13/23)score85868081797168/21/41Table 5: Result for Training Data Sizezation, location dictionary, etc.
We believethat these dictionaries can be relatively do-main independent.2.
Modify the programAssign a new ID number for the position en-tity in the decision tree program and modifythe input/output routine accordingly.
Thisalso took less than an hour.In less than two hours for the system modifica-tion, and about a day's work for the preparationof the training data, the new system becomesrunnable, Table 6 shows the result of the experi-ment.
The result is quite satisfactory.
However,Entity scoreOrg.
: 72Person 88Loc.
67Position 93Date 89Time 100Money 90Percent 100Overall 845.3 Execut ive Success ion Domain- Por tab i l i ty -In general, one of the advantages of automaticlearning systems is their portability.
In this sub--section, we will report an experiment of movingtile system to a new domain, the executive suc-cession domain.
Also, in order to see the porta-bility of the system, we add a new kind of namedentity.
In this domain, executive positions ap-pear very often and it is an important entitytype for understanding those articles.
So, we adda new entity class, 'position'.
When porting thesystem, only the following two changes are re-quired.1.
Add a new dictionaryCreate a new dictionary for positions.
Inpractice, many of them were listed in theperson prefix in the previous-experiment.So we separate them and add several po-sition names which appeared in or could beinferred from the training data.
This tookless than an hour.
Note that we did notchange any other dictionaries, i.e.
organi-175Table 6: Result in Executive Succession Domainit "is not as good as the result in the previousdomain, in particular, for organization and lo-cation.
Observing the output, we noticed do-main idiosyncrasies which we had not thoughtof before.
For example, in the new domain,there are many Chinese company names, whichhave the suffix "Yuugenkoushi' .
This is neverused for Japanese company names and we don'thave the suffix in our organization suffix dic-tionary.
Another interesting example is a Chi-nese character "Shou".
In Japanese, the char-acter is used as a suffix of official organizations,like "Monbu-Shou" (Department of Education),but in Chinese it is used as a suffix of locationnames, like "Kanton-Shou" (Canton District).In the accident domain, we did not encountersuch Chinese location names, so we just had thetoken in the organization suffix dictionary.
Thisled to many errors in location names in the newdomain.
Also, we find many unfamiliar foreignlocation names and company names.
We believethese make the result relatively worse.5.4 Domain  DependencyAs we have training and evaluation data on twodifferent domains, it is interesting to observe thedomain dependency of the system.
Namely, wewill see how the performance differs if we usethe knowledge (decision tree) created from a dif-ferent domain.
We conducted two new exper-iments, tagging named entities for texts in 1:hesuccession domain based on the decision tree cre-ated for the accident domain, and vice versa.Table 7 shows the comparison of these re-suits.
The performance in the accident domaindecreased from 85 to 71 using the decision treeof the other domain.
Also, the performance de-creased from 82 to 59 in the succession domain.Test \ Train Acc.
Suc.Accident 85 71Succession , 59 82Table 7: Result on Domain DependencyThe result demonstrates the domain depen-dency of the method used, at least for the twodomains.
Obviously, making a general commentbased on these small experiments i dangerous,but it suggests that we should consider the do-main dependency when we port the system to anew domain.6 Re la ted  WorkThere have been several efforts to apply machinelearning techniques to the same task (Cowie,1995) (Bikel et al 1997) (Gallippi, 1996) (Ben-nett et al 1997) (Borthwick et al 1997).
In thissection, we will discuss a system which is one ofthe most advanced and which closely resemblesour own (Bennett et al 1997).
A good review ofmost of the other systems can be found in theirpaper.Their system uses the decision tree algorithmand almost the same features.
However, thereare significant differences between the systems.The main difference is that they have-more thanone decision tree, each of which decides if a par-ticular named entity starts/ends at the currenttoken.
In contrast, our system has only one de-cision tree which produces probabilities of infor-mation about the named entity.
In this regard,we are similar to (Bikel et al 1997), which alsouses a probabilistic method in their HMM basedsystem.
This is a crucial difference which alsohas important consequences.
Because the sys-tem of (Bennett et al 1997) makes multiple de-cisions at each token, they could assign multiple,possibly inconsistent tags.
They solved the prob-lem by introducing two somewhat idiosyncraticmethods.
One of them is the distance score,which is used to find an opening and closing pairfor each named entity mainly based on distanceinformation.
The other is a tag priority scheme,which chooses a named entity among differenttypes of overlapping candidates based on the pri-ority order of named entities.
These methods re-quire parameters which must be adjusted whenthey are applied to a new domain.
In contrast,our system does not require such methods, as themultiple possibilities are resolved bv the proba-bilistic method.
This is a strong advantage, be-cause we don't need manual adjustments.The result they reported is not comparable toour result, because the text and definition aredifferent.
But the total F-score of our systemis similar to theirs, even though the size of ourtraining data is much smaller.7 D iscuss ionThis paper has described a system which usesa .decision tree to find and classify names inJapanese texts.
Experiments indicate good ac-curacy with a small amount of training data,and demonstrate the system's portability.
Theissues of training data size and domain depen-dency were discussed.We would like to discuss the issue of the handcreated ictionaries.
People might think that thehand made dictionaries play the mQor role in thesystem.
It may be true, but we should remem-ber that the experiment in the Executive Succes-sion Domain use the same pre-exist dictionar-ies used in the Accident Domain.
We did notmodify any dictionaries used in the previous do-main, we only added the dictionary for the posi-tion.
Although we found some dictionary entitieswhich should be added, the fact that we achievedgood performance in the new domain by usingthe same dictionaries hows that dictionaries arenot so domain dependent.
Once we prepared thedictionaries, we might not need to modify themto a great degree.
Also, Table 7 suggests that the176decision tree rules are more domain dependentrather the dictionaries.We have several ideas in order to improve oursystem.The most crucial and most elaborate step inbuilding up the system is creating the dictionar-ies.
It was done by hand, because 100 trainingarticles are not enough to acquire even prefixesand suffixes.
One possibility is to use a boot-strapping method.
Starting with core dictionar-ies, we can run the system on untagged texts,and increase the entities in the dictionaries.Another issue is aliases.
In newspaper articles,aliases are often used.
The full name is usedonly the first time the company is mentioned(Matsushita Denki Sangyou gabushiki Kaisya= Matsushita Electric Industrial Co. Ltd.)and then aliases (Matsushita or Matsushi taDensan = Matsushita E.I.)
are used in the latersections of the article.
Our system cannot handlethese aliases, unless the aliases are registered intile dictionaries.Also.
lexical information should help the accu-racy.
For example, a name, possibly a person oran organization, in a particular argument slot ofa verb can be disambiguated by the verb.
Forexample, a name in the object slot of the verb'hire' might be a person, while a name in thesubject slot of verb 'manufacture' might be anorganization.8 AcknowledgmentWe would like to thank our colleague at NYU, inparticular Mr.Andrew Borthwick and Mr.JohnSterling.
There comments and discussion wereuseful for the research.Re ferencesDefense Advanced Research Projects Agency1996 Proceedings of Workshop on TipsterProgram Phase II Morgan Kaufmann Pub-lishersScott Bennett, Chinatsu Aone and Craig Lovell1997 Learning to Tag Multilingual TextsThrough Observation Conference on Empiri-cal Methods in Natural Language ProcessingDaniel Bikel, Scott Miller, Richard Schwartz andRalph Weischedel 1997 Nymble: a High-Performance Learning Name-finder Proceed-ings of the Fifth Conference on Applied Nat-ural Language ProcessingAndrew Borthwick, John Sterling, EugeneAgichtein and Ralph Grishman 1998 Exploit-ing Diverse Knowledge Sources via MaximumEntropy in Named Entity Recognition Pro-ceedings of the Sizth Workshop on Very LargeCorporaAnthony Gallippi 1996 Learning to RecognizeNames Across Languages Proceedings of the16th International Conference on Computa-tional Linguistics (COLING-96)Jim Cowie 1995 Description of the CRL/NMSUSystems Used for MUC-6 Proceedings of SixthMessage Understanding Conference (31UC-6}Ross J. Quinlan 1993 C4.5: Program for Ma-chine Learning Morgan Kaufmann PublishersYuuji Matsumoto, Sadao Kurohashi, Osamu Ya-maji, Yuu Taeki and Makoto Nagao 1997Japanese morphological analyzing System:JUMAN Kyoto University and Nara Instituteof Science and Technology177Appendix A: Example training dataToken POS St r ing  Spec ia l  Named ent i tytype Dict .
answer\[ \[ Sym - -ERUSAREMU PN- loc  Kata  loc  ioc -OP-CL26 number  Num - date -OP-CNN ICHI  N-su f  Kan j i  date -S  date -CN-CLKYOD0 PN Kan j i  o rg  org-OP--CL\] \] Sym - -ISURAERU PN- loc  Kata  loc  org-0P-.CNKE ISATSU N Kan j i  o rg -S  org -CN-CLNI postpos  M i ra  - -YORU V H i ra  - -TO postpos  H i ra  - -, comma Comma - -ERUSAREMU PM- loc  Kata  loc  ioc-OP--CNSHI N-su f  Kan j i  l oc -S  ioc -CN-CLHOKUBU N Kan j i  - -DE  postpos  Mira  - -26 number  Num - date -OP-CNN ICMI  N-su f  Kan j i  date -S  date -CN-CLGOGO N Kan j i  t ime,  t ime-OP-CLt ime-P, comma Comma - -Appendix B: Example paths in the treeI SURAERU (seventh  token)if cur rent  token  is a locat ion  -> yesif next  token  is a loc -su f f ix  -> noif next  token  is a person-su f f ix  -> noif next  token  is a o rg -su f f ix  -> yesif p rev ious  token  is a locat ion  -> nothen  none = 0.67,  o rg -OP-CN = 0 .33KE ISATSU (e ighthif cur rent  tokenif cur rent  tokenif cur rent  tokenif cur rent  tokenif next  tokenif cur rent  tokenif next  tokenif cur rent  tokenif cur rent  tokenif next  tokenif cur rent  tokentoken)Is a locat ionIs a o rgan izat ionis a t imeIs a lo t - su f f i xIs a t ime-su f f ixIs a t ime-su f f ixis a date -su f f ixis a date -su f f ixIs a dateIs a locat ionIs a o rg -su f f ixif p rev ious  token  Is a locat ionthen  none  = 0.14,  o rg -CN-CL  = 0 .86->  no-> no-> no->  no->  no-> no-> no-> no-> no-> yes->  yes178
