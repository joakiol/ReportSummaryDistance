Text-Translation AlignmentMart in  Kay*Xerox Palo Alto Research CenterandStanford UniversityMar t in  R6scheisen*Xerox Palo Alto Research CenterandTechnical University of MunichWe present an algorithm for aligning texts with their translations that is based only on internalevidence.
The relaxation process rests on a notion of which word in one text corresponds to whichword in the other text that is essentially based on the similarity of their distributions.
It exploitsa partial alignment of the word level to induce a maximum likelihood alignment of the sentencelevel, which is in turn used, in the next iteration, to refine the word level estimate.
The algorithmappears to converge to the correct sentence alignment in only a few iterations.1.
The ProblemTo align a text with a translation of it in another language is, in the terminology ofthis paper, to show which of its parts are translated by what parts of the second text.The result takes the form of a list of pairs of items--words, sentences, paragraphs, orwhatever--from the two texts.
A pair (a~ b> is on the list if a is translated, in whole orin part, by b.
If (a, b> and (a, c) are on the list, it is because a is translated partly by b,and partly by c. We say that the alignment is partial if only some of the items of thechosen kind from one or other of the texts are represented in the pairs.
Otherwise, itis complete.It is notoriously difficult to align good translations on the basis of words, becauseit is often difficult to decide just which words in an original are responsible for a givenone in a translation and, in any case, some words apparently translate morphologicalor syntactic phenomena rather than other words.
However, it is relatively easy toestablish correspondences between such words as proper nouns and technical terms,so that partial alignment on the word level is often possible.
On the other hand, itis also easy to align texts and translations on the sentence or paragraph levels, forthere is rarely much doubt as to which sentences in a translation contain the materialcontributed by a given one in the original.The growing interest in the possibility of automatically aligning large texts is at-tested to by independent work that has been done on it since the first description ofour methods was made available (Kay and R6scheisen 1988).
In recent years it hasbeen possible for the first time to obtain machine-readable v rsions of large corporaof text with accompanying translations.
The most striking example is the Canadian"Hansard," the transcript of the proceedings of the Canadian parliament.
Such bilin-gual corpora make it possible to undertake statistical, and other kinds of empirical,studies of translation on a scale that was previously unthinkable.Alignment makes possible approaches to partially, or completely, automatic trans-lation based on a large corpus of previous translations that have been deemed accept-* Xerox PARC, 3333 Coyote Hill Road, Palo Alto, CA 94306.t Department of Computer Science, Technical University of Munich, 8000 Munich 40, Germany.
(~) 1993 Association for Computational LinguisticsComputational Linguisti4s Volume 19, Number 1able.
Perhaps the best-known example of this approach is to be found in Sato andNagao (1990).
The method proposed there requires a database to be maintained ofthe syntactic structures of sentences together with the structures of the correspondingtranslations.
This database is searched in the course of making a new translation forexamples of previous entences that are like the current one in ways that are relevantfor the method.
Another example is the completely automatic, statistical approach totranslation taken by the research group at IBM (Brown et al 1990), which takes a largecorpus of text with aligned translations as its point of departure.It is widely recognized that one of the most important sources of information towhich a translator can have access is a large body of previous translations.
No dic-tionary or terminology bank can provide information of comparable value on topicalmatters of possibly intense though only transitory interest, or on recently coined termsin the target language, or on matters relating to house style.
But such a body of datais useful only if, once a relevant example has been found in the source language, thecorresponding passage can be quickly located in the translation.
This is simple only ifthe texts have been previously aligned.
Clearly, what is true of the translator is equallytrue of others for whom translations are a source of primary data, such as studentsof translation, the designers of translations systems, and lexicographers.
Alignmentwould also facilitate the job of checking for consistency in technical and legal textswhere consistency constitutes a large part of accuracy.In this paper, we provide a method for aligning texts and translations based onlyon internal evidence.
In other words, the method depends on no information aboutthe languages involved beyond what can be derived from the texts themselves.
Fur-thermore, the computations on which it is based are straightforward and robust.
Theplan rests on a relationship between word and sentence alignments arising from theobservation that a pair of sentences containing an aligned pair of words must them-selves be aligned.
It follows that a partial alignment on the word level could induce amuch more complete alignment on the sentence l vel.A solution to the alignment problem consists of a subset of the Cartesian productof the sets of source and target sentences.
The process starts from an initial subsetexcluding pairs whose relative positions in their respective texts is so different hatthe chance of their being aligned is extremely low.
This potentially alignable set ofsentences forms the basis for a relaxation process that proceeds as follows.
An initialset of candidate word alignments i produced by choosing pairs of words that tendto occur in possibly aligned sentences.
The idea is to propose a pair of words foralignment if they have similar distributions in their respective texts.
The distributionsof a pair of words are similar if most of the sentences in which the first word occursare alignable with sentences in which the second occurs, and vice versa.
The mostapparently reliable of these word alignments are then used to induce a set of sentencealignments hat will be a subset of the eventual result.
A new estimate is now made ofwhat sentences are alignable based on the fact that we are now committed to aligningcertain pairs.
Because sentence pairs are never removed from the set of alignments,the process converges to the point when no new ones can be found; then it stops.In the next section, we describe the algorithm.
In Section 3 we describe addi-tions to the basic technique required to provide for morphology, that is, relativelysuperficial variations in the forms of words.
In Section 4 we show the results of ap-plying a program that embodies these techniques to an article from Scientific Americanand its German translation i  Spektrum der Wissenschaft.
In Section 5 we discuss otherapproaches tothe alignment problem that were subsequently undertaken by other re-searchers (Gale and Church 1991; Brown, Lai, and Mercer 1991).
Finally, in Section 6,we consider ways in which our present methods might be extended and improved.122Martin Kay and Martin R6scheisen Text-Translation Alignment2.
The Alignment Algorithm2.1 Data StructuresThe principal data structures used in the algorithm are the following:Word-Sentence Index (WSI).
One of these is prepared for each of the texts.
Itis a table with an entry for each different word in the text showing thesentences in which that word occurs.
For the moment, we may take aword as being simply a distinct sequence of letters.
If a word occurs morethan once in a sentence, that sentence occurs on the list once for eachoccurrence.Alignable Sentence Table (AST).
This is a table of pairs of sentences, one fromeach text.
A pair is included in the table at the beginning of a pass if thatpair is a candidate for association by the algorithm in that pass.Word Alignment Table (WAT).
This is a list of pairs of words, together with sim-ilarities and frequencies in their respective texts, that have been alignedby comparing their distributions in the texts.Sentence Alignment Table (SAT).
This is a table that records for each pair ofsentences how many times the two sentences were set in correspondenceby the algorithm.Some additional data structures were used to improve performance in our im-plementation of the algorithm, but they are not essential to an understanding of themethod as a whole.2.2 Outline of the AlgorithmAt the beginning of each cycle, an AST is produced that is expected to contain theeventual set of alignments, generally amongst others.
It pairs the first and last sentencesof the two texts with a small number of sentences from the beginning and end of theother text.
Generally speaking, the closer a sentence is to the middle of the text, thelarger the set of sentences in the other text that are possible correspondents for it.The next step is to hypothesize a set of pairs of words that are assumed to cor-respond based on similarities between their distributions in the two texts.
For thispurpose, a word in the first text is deemed to occur at a position corresponding to aword in the second text if they occur in a pair of sentences that is a member of theAST.
Similarity of distribution is a function of the number of corresponding sentencesin which they occur and the total number of occurrences of each.
Pairs of words areentered in the WAT if the association between them is so close that it is not likely tobe the result of a random event.
In our algorithm, the closeness of the association isestimated on the basis of the similarity of their distributions and the total number ofoccurrences.The next step is to construct he SAT, which, in the last pass, will essentiallybecome the output of the program as a whole.
The idea here is to associate sentencesthat contain words paired in the WAT, giving preference to those word pairs thatappear to be more reliable.
Multiple associations are recorded.If there are to be further passes of the main body of the algorithm, a new ASTis then constructed in light of the associations in the SAT.
Associations that are sup-ported some minimum number of times are treated just as the first and last sentencesof the texts were initially; that is, as places at which there is known to be a corre-spondence.
Possible correspondences are provided for the intervening sentences by123Computational Linguistics Volume 19, Number 1the same interpolation method initially used for all sentences in the middle of thetexts.In preparation for the next pass, a new set of corresponding words is now hy-pothesized using distributions based on the new AST, and the cycle repeats.2.3 The AlgorithmThe main algorithm is a relaxation process that leaves at the end of each pass a newWAT and SAT, each presumably more refined than the one left at the end of thepreceding pass.
The input to the whole process consists only of the WSIs of the twotexts.
Before the first pass of the relaxation process, an initial AST is computed simplyfrom the lengths of the two texts:Construct Initial AST.
If the texts contain m and n sentences respectively, thenthe table can be thought of as an m x n array of ones and zeros.
Theaverage number  of sentences in the second text corresponding to a givenone in the first text is n/m, and the average position of the sentence in thesecond text corresponding to the i-th sentence in the first text is thereforei.
n/m.
In other words, the expectation is that the true correspondenceswill lie close to the diagonal.
Empirically, sentences typically correspondone for one; correspondences of one sentence to two are much rarer, andcorrespondences of one to three or more, though they doubtless occur,are very rare and were unattested in our data.
The max imum deviationcan be stochastically modeled as O(v~),  the factor by which the standarddeviation of a sum of n independent and identically distributed randomvariables multiplies.
1We construct he initial AST using a function that pairs single sen-tences near the middle of the text with as many as O(v~ff) sentences in theother text; it is generously designed to admit all but the most improbableassociations.
Experience shows that because of this policy the results arehighly insensitive to the particular function used to build this initial table.
2The main body of the relaxation process consists of the following steps:Build the WAT.
For all sentences s a in the first text, each word in s a is comparedwith each word in those sentences s B of the second text that are consideredas candidates for correspondence, i.e., for which (s A, s B) EAST.
A pair ofwords is entered into the WAT if the distributions of the two words intheir texts are sufficiently similar and if the total number  of occurrencesindicates that this pair is unlikely to be the result of a spurious match.
Notethat the number  of comparisons of the words in two sentences i quadraticonly in the number  of words in a sentence, which can be assumed to be nota function of the length of the text.
Because of the.constraint on the max-imum deviation from the diagonal as outlined above, the computationalcomplexity of the algorithm is bound by O(nx/n) in each pass.1 In such a model, each random variable would correspond to a translator's choice to move away fromthe diagonal in the AST by a certain distance (which is assumed to be zero mean, Gaussiandistributed).
However, the specific assumptions about he maximum deviation are not crucial in thatthe algorithm was observed to be insensitive to such modifications.2 The final results howed that no sentence alignment is at a distance greater than ten from the diagonalin texts of 255 and 300 sentences.
Clearly, any such prior knowledge could be used for a significantspeed-up of the algorithm, but it was our goal to adopt as few prior assumptions a  possible.124Martin Kay and Martin R6scheisen Text-Translation AlignmentOur definition of the similarity between a pair of words is complicatedby the fact that the two texts have unequal lengths and that the AST allowsmore than one correspondence, which means that we cannot simply takethe inner product of the vector representations of the word's occurrences.Instead, we use as a measure of similarity: 32cNA (v) + N~ (w)where c is the number of corresponding positions, and Nv(x) is the num-ber of occurrences of the word x in the text T. This is essentially Dice'scoefficient (Rijsbergen 1979).
Technically, the value of c is the cardinalityof the largest set of pairs (i, j) such that1.
(s~(v),s~(w)) c AST, where szr(x) is the sentence in text T thatcontains the z-th occurrence of word x.2.
Pairs are non-overlapping in the sense that, if (a, b) and (c, d) aredistinct members of the set then they are distinct in bothcomponents, that is, a ~ c and b ~ d.Suppose that the word "dog" occurs in sentences 50, 52, 75, and 200 of theEnglish text, and "Hund" in sentences 40 and 180 of the German, and thatthe AST contains the pairs (50, 40), (52, 40), and (200,180), among others,but not (75, 40).
There are two sets that meet the requirements, namely~(1, 1), (4,2)} and {(2, 1), (4,2)}.
The set {(1, 1), (2, 1), (4,2)} is excludedon the grounds that (1, 1) and (2, 1) overlap in the above sense--the firstoccurrence of "Hund" is represented twice.
In the example, the similarity2 _ 1 regardless of the ambiguity between would be computed as 4+2-2  - -  2'(1, 1) and (2, 1).The result of the comparisons of the words in all of the sentences ofone text with those in the other text is that the word pairs with the highestsimilarity are located.
Comparing the words in a sentence of one text withthose in a sentence of the other text carries with it an amortized costof constant computational complexity, 4 if the usual memory-processingtradeoff on serial machines is exploited by maintaining redundant datastructures uch as multiple hash tables and ordered indexed trees.
5The next task is to determine for each word pair, whether it will ac-tually be entered into the WAT: the WAT is a sorted table where the morereliable pairs are put before less reliable ones.
For this purpose, each en-try contains, as well as the pair of words themselves, the frequencies ofthose words in their respective texts and the similarity between them.
Thecloseness of the association between two words, and thus their rank inthe WAT, is evaluated with respect o their similarity and the total num ~ber of their occurrences.
To understand why similarity cannot be used3 Throughout his paper, we use the word similarity to denote this similarity measure, which does notnecessarily have to be an indicator of what one would intuitively describe as "similar" words.
Inparticular, we will later see that similarity alone, without consideration of the total frequency, is not agood indicator for "similarity.
"4 The basic idea is this: more processing has to be done to compute the similarity of a high-frequencyword to another frequent word, but there are also more places at which this comparison can later besaved.
Recall also that we assume sentence length to be independent of text length.5 For very large corpora, this might not be feasible.
However, large texts can almost invariably be brokeninto smaller pieces at natural and reliable places, such as chapter and section headings.125Computational Linguistics Volume 19, Number 1alone, note that there are far more one-frequency words than words ofhigher frequency.
Thus, a pair of words with a similarity of 1, each ofthem occurring only once, may well be the result of a random event.
Ifsuch a pair was proposed for entry into the WAT, it should only be addedwith a low priority.The exact stochastic relation is depicted in Figure 1, where the proba-bility is shown that a word of a frequency k that was aligned with a wordin the other text with a certain similarity s is just the result of a randomprocess.
6 Note that, for a high-frequency word that has a high similaritywith some other word (right front corner), it is very unlikely (negligibleplateau height) that this association has to be attributed to chance.
On theother hand, low similarities (back) can easily be attained by just associat-ing arbitrary words.
Low-frequency words--because there are so many ofthem in a text--can also achieve a high similarity with some other wordswithout having to be related in an interesting way.
This can be intuitivelyexplained by the fact that the similarity of a high-frequency word is basedon a pattern made up of a large number of instances.
It is therefore a pat-tern that is unlikely to be replicated by chance.
Furthermore, since thereare relatively few high-frequency words, and they can only contract highsimilarities with other high-frequency words, the number of possible cor-respondents for them is lower, and the chance of spurious associationsis therefore less on these grounds also.
Note that low-frequency wordswith low similarity (back left corner) have also a low probability of beingspuriously associated to some other word.
This is because low-frequencywords can achieve a low similarity only with words of a high frequency,which in turn are rare in a text, and are therefore unlikely to be associatedspuriously.
7Our algorithm does not use all the detail in Figure 1, but only a simplediscrete heuristic: a word pair whose similarity exceeds ome threshold isassigned to one of two or three segments of the WAT, depending on theword frequency.
A segment with words of higher frequency is preferredto lower-frequency segments.
Within each segment, he entries are sortedin order of decreasing similarity and, in case of equal similarities, in orderof decreasing frequency.
In terms of Figure 1, we take a rectangle fromthe right front.
We place the left boundary as far to the left as possible,because this is where most of the words are.Build the SAT.
In this step, the correspondences in the WAT are used to estab-lish a mapping between sentences of the two texts.
In general, these new6 The basis for this graph is an analytic derivation of the probability that a word with a certainfrequency in a 300-sentence text matches ome random pattern with a particular similarity.
The analyticformula relies on word-frequency data derived from a large corpus instead of on a stochastic model forword frequency distribution (such as Zipf's law, which states that the frequency with which wordsoccur in a text is indirectly proportional to the number of words with this frequency; for a recentdiscussion of more accurate models, see also Baayen \[1991\]).
Clearly, the figure is dependent on thestate of the AST (e.g.
lower similarities become more acceptable as the AST becomes more and morenarrow), but the thresholds relevant to our algorithm can be precomputed at compile-time.
The figureshown would be appropriate to pass 3 in our experiment.
In the formula used, there are a fewreasonable simplifications concerning the nature of the AST; however, a Monte-Carlo simulation that isexactly in accordance with our algorithm confirmed the depicted figure in every essential detail.7 This discussion could also be cast in an information theoretic framework using the notion of "mutualinformation" (Fano 1961), estimating the variance of the degree of match in order to find afrequency-threshold (see Church and Hanks 1990).126Martin Kay and Martin R6scheisen Text-Translation Alignment/ / / / / J I I f J i" ,~1 / \[ / \[ / \[ I \] I I J t I f5.... i iiiii i ,, ii i i i i i i i i l ,  i i,~,kkNk~,\:,:,\k,t r i P i i i i i i i i i i i i \ \ \ \ \ \ \  ?8t !
!
!
!
!
!
!
!
\ [ !
!~\ \ \ \ \ \ \ \ \ \10 15 20FrequencyFigure 1Likelihood that a word pair is a spurious match as a function of a word's frequency and itssimilarity with a word in the other text (maximum 0.94).associations are added to the ones inherited from the preceding pass.
Itis an obvious requirement of the mapping that lines of association shouldnot cross.
At the beginning of the relaxation process, the SAT is initializedsuch that the first sentences of the two texts, and the last sentences, areset in correspondence with one another, regardless of any words they maycontain.
The process that adds the remaining associations scans the WATin order and applies a three-part process to each pair Iv, w/.1.
Construct he correspondence set for/v~ w / using essentially thesame procedure as in the calculation of the denominator, c ofword similarities above.
Now, however, we are concerned toavoid ambiguous pairs as characterized above.
The set containsa sentence pair IsiA(v),s~(w)l if (1) IsiA(v)~ s~(w)l EAST, and (2) voccurs in no other sentence h (resp.
w in no g) such thatIs~(v), h I (resp.
Ig~ s~(w)l) is also in the AST.2.
If any sentence pair in the correspondence s t crosses any of theassociations that have already been added to the SAT, the wordpair is rejected as a whole.
In other words, if a given pair ofsentences correspond, then sentences preceding the first of themcan be associated only with sentences preceding the second.3.
Add each sentence pair in the correspondence s t of the wordpair Iv, w / to the SAT.
A count is recorded of the number oftimes a particular association is supported.
These counts arelater thresholded when a new AST is computed or when theprocess terminates.Build a New AST.
If there is to be another pass of the relaxation algorithm, a newAST must be constructed as input to it.
This is based on the current SATand is derived from it by supplying associations for sentences for which it127Computational Linguistics Volume 19, Number 1provides none.
The idea is to fill gaps between associated pairs of sentencesin the same manner that the gap between the first and the last sentencewas filled before the first pass.
However, only sentence associations thatare represented more than some minimum number of times in the SATare transferred to the AST.
In what follows, we will refer to these sentencepairs as anchors.As before, it is convenient to think of the AST as a rectangular array,even though it is represented more economically in the program.
Considera maximal sequence of empty AST entries, that is, a sequence of sentencesin one text for which there are no associated sentences in the other, butwhich is bounded above and below by an anchor.
The new associationsthat are added lie on and adjacent to the diagonal joining the two anchors.The distance from the diagonal is a function of the distance of the currentcandidate sentence pair and the nearest anchor.
The function is the sameone used in the construction of the initial AST.Repeat.
Build a new WAT and continue.3.
MorphologyAs we said earlier, the basic alignment algorithm treats words as atoms; that is, it treatsstrings as instances of the same word if they consist of identical sequences of letters,and otherwise as totally different.
The effect of this is that morphological variants of aword are not seen as related to one another.
This might not be seen as a disadvantagein all circumstances.
For example, nouns and verbs in one text might be expectedto map onto nouns with the same number and verbs with the same tense muchof the time.
But this is not always the case and, more importantly, some languagesmake morphological distinctions that are absent in the other.
German, for example,makes a number of case distinctions, especially in adjectives, that are not reflected inthe morphology of English.
For these reasons, it seems desirable to allow words tocontract associations with other words both in the form in which they actually occur,and in a more normalized form that will throw them together with morphologicallyrelated other words in the text.3.1 The Basic IdeaThe strategy we adopted was to make entries in the WSI, not only for maximal stringsof alphabetic haracters occurring in the texts, but also for other strings that couldusefully be regarded as normalized forms of these.Clearly, one way to obtain normalized forms of words is to employ a fully fledgedmorphological analyzer for each of the languages.
However, we were concerned thatour methods hould be as independent as possible of any specific facts about the lan-guages being treated, since this would make them more readily usable.
Furthermore,since our methods attend only to very gross features of the texts, it seemed unreason-able that their success hould turn on a very fine analysis at any level.
We argue that,by adding a guess as to how a word should be normalized to the WSI, we removeno associations that could have been formed on the basis of the original word, butonly introduce the possibility of some additional associations.
Also, it is unlikely thatan incorrect normalization will contract any associations at all, especially in view ofthe fact that these forms, because they normalize several original forms, tend to occurmore often.
They will therefore rarely be misleading.128Martin Kay and Martin R6scheisen Text-Translation AlignmentFor us, a normalized form of a word is always an initial or a final substring of thatword- -no  attention is paid to morphographemic or word-internal changes.
A word isbroken into two parts, one of which becomes the normalized form, if there is evidencethat the resulting prefix and suffix belong to a paradigm.
In particular, both must  occuras prefixes and suffixes of other forms.3.2 The AlgorithmThe algorithm proceeds in two stages.
First a data structure, called the trie, is con-structed in which information about the occurrences of potential prefixes and suffixesin the text is stored.
Second, words are split, where the trie provides evidence fordoing so, and one of the resulting parts is chosen as the normalization...A trie (Knuth 1973; pp.
481--490) is a data structure for associatinginformation with strings of characters.
It is particularly economical insituations where many of the strings of interest are substrings of othersin the set.
A trie is in fact a tree, with a branch at the root node for everycharacter that begins a string in the set.
To look up a string, one starts atthe root, and follows the branch corresponding to its first character toanother node.
From there, the branch for the second character isfollowed to a third node, and so on, until either the whole string hasbeen matched, or it has been discovered not to be in the set.
If it is in theset, then the node reached after matching its last character containswhatever information the structure contains for it.
The economy of thescheme lies in the fact that a node containing information about a stringalso serves as a point on the way to longer strings of which the givenone is a prefix.
In this application, two items of information are storedwith a string, namely the number  of textual words in which it occurs asa prefix and as a suffix.Consider the possibility of breaking an n-letter word before the i-thcharacter of the word (1 < i _< n).
The conditions for a break are: Thenumber  of other words starting with characters 1 ?.
?
i - 1 of the currentword must be greater than the number  of words starting with characters1 ?
.. i because, if the characters 1 .. ?
i - 1 constitute a useful prefix, thenthis prefix must be followed, in different words, by other suffixes thancharacters i .
.
.
n. So, consider the word "wanting," and suppose that weare considering the possibility of breaking it before the 5th character, "i.
"For this to be desirable, there must be other words in the text, such as"wants," and "wanted," that share the first i - 1 = 4 characters.Conversely, there must be more words ending with characters i .
.
.
n ofthe word than with i - 1 -.
?
n. So, there must be more words with thesuffix "ing" than with the suffix "ting"; for example "seeing" and"believing.
"There is a function from potential break points in words to numberswhose value is maximized to choose the best point at which to break.
Ifp and s are the potential prefix and suffix, respectively, and P(p) and S(s)are the number  of words in the text in which they occur as such, thevalue of the function is kP(p)S(s).
The quantity k is introduced to enableus to prefer certain kinds of breaks over others.
For the English andGerman texts used in our experiments, k = length(p) so as to favor longprefixes on the grounds that both languages are primari ly suffixing.
If129Computational Linguistics Volume 19, Number 1the function has the same value for more than one potential break point,the one farthest o the right is preferred, also for the reason that weprefer to maximize the lengths of prefixes.Once it has been decided to divide a word, and at what place, one ofthe two parts is selected as the putative canonical form of the word,namely, whichever is longer, and the prefix if both are of equal length.Finally, any other words in the same text that share the chosen prefix(suffix) are split at the corresponding place, and so assigned to the samecanonical form.The morphological lgorithm treats words that appear hyphenatedin the text specially.
The hyphenated word is treated as a unit, just as itappears, and so are the strings that result from breaking the word at thehyphens.
In addition, the analysis procedure described above is appliedto these components, and any putative normal forms found are alsoused.
It is worth pointing out that we received more help from hyphensthan one might normally expect in our analysis of the German textsbecause of a tendency on the part of the Spektrum der Wissenschafttranslators, following standard practice for technical writing, ofhyphenating compounds.4.
Experimental ResultsIn this section, we show some of the results of our experiments with these algorithms,and also data produced at some of the intermediate stages.
We applied the meth-ods described here to two pairs of articles from Scientific American and their Germantranslations in Spektrum der Wissenschaft (see references).
The English and German ar-ticles about human-powered flight had 214 and 162 sentences, respectively; the onesabout cosmic rays contained 255 and 300 sentences, respectively.
The first pair wasprimarily used to develop the algorithm and to determine the various parameters ofthe program.
The performance of the algorithm was finally tested on the latter pair ofarticles.
We chose these journals because of a general impression that the translationswere of very high quality and were sufficiently "free" to be a substantial challenge forthe algorithm.
Furthermore, we expected technical translators to adhere to a narrowview of semantic accuracy in their work, and to rate the importance of this abovestylistic considerations.
Later we also give results for another application of our algo-rithm to a larger text of 1257 sentences that was put together from two days from theFrench-English Hansard corpus.Table 1 shows the first 50 entries of the WAT after pass 1 of the algorithm.
Itshows part of the first section of the WAT (lines 1-23) and the beginning of the second(lines 24-50).
The first segment contains words or normalized forms with more than 7occurrences and a similarity not less than 0.8.
Strings shown with a following hyphenare prefixes arising from the morphological procedure; strings with an initial hyphenare suffixes.
Naturally, some of the word divisions are made in places that do notaccurately reflect linguistic facts.
For example, English "proto-" (1) comes from "pro-ton" and "protons"; German "-eilchen" (17) is the normalization for words ending in"-teilchen" and, in the same way, "-eistung" (47) comes from "-leistung.
"Of these 50 word pairs, 42 have essentially the same meanings.
We take it that"erg" and "Joule," in line 4, mean the same, modulo a change in units.
Also, it is not un-reasonable to associate pairs like "primary"/"sekundaren" (26) and "electric"/"Feld"(43), on the grounds that they tend to be used together.
The pair "rapid-"/"Pulsare-"(49) is made because a pulsar is a rapidly spinning neutron star and some such phrase130Martin Kay and Martin R6scheisen Text-Translation AlignmentTable 1The WAT after pass 1.English German Eng.
Freq.
Similarity1 proto- Proto- 14 12 proton- , Proton- 13 13 interstellar interstellare- 12 14 ergs Joule 10 15 electric- elektrisch- 9 16 pulsar- Pulsar- 17 16/177 photo- Photo- 14 14/158 and und 69 11/129 per pro 12 11/1210 relativ- relativ- 11 10/1111 atmospher- Atmosph~ire- 10 10/1112 Cygnus Cygnus 63 59/6513 cosmic- kosmische- 81 39/4314 volts Elektronenvolt 19 19/2115 telescope- Teleskop- 9 8/916 univers- Univers- 8 7/817 particle- -eilchen 53 51/5918 shower- Luftschauer- 20 19/2219 X-ray- R6ntgen- 19 19/2220 electrons Elektronen 12 11/1321 source- Quelle- 40 37/4522 magnetic Magnetfeld 11 9/1123 ray-- Strahlung- 141 135/16724 Obs diesem 6 1 ervatory25 shower Gammaquant 6 126 primary sekund~iren 6 127 percent Prozent 6 128 ~a!axies Galaxien 5 129 ~nmean Krim 5 130 ultrahigh- ultraho- 5 131 density Dichte 5 132 synchrotron Synchrotronstrahlung 5 133 activ- aktiv- 5 134 supernova Supernova-Explosion- 5 135 composition Zusammensetzung 5 136 detectors l~rim~ire- 5 137 data Daten- 7 7/838 University Universit- 7 6/739 element- -usammensetzung 7 6/740 neutron Neutronenstern 7 6/741 Cerenkov Cerenkov-Licht- 7 6/742 spinning rotier- 6 6/743 electric Feld 6 5/644 lines -inien 6 5/645 medium Medium 6 5/646 estimate- absch~itz- 6 5/647 output -eistung 6 5/648 bright- Astronom- 5 5/649 rapid- Pulsare- 5 5/650 proposed vorgeschlagen 6 5/6occurs with it five out of six times.
Notice, however, that the association "pulsar-""Pulsar-" is also in table (6).
Furthermore, the German strings "Pulsar" and "Pulsar-"are both given correct associations in the next pass (lines 17 and 20 of Table 2).The table shows two interesting effects of the morphological analysis procedure.The word "shower" is wrongly associated with the word "Gammaquant"  (25) witha frequency of 6, but the prefix "shower-" is correctly associated with "Luftschauer-"131Computational Linguistics Volume 19, Number 1~Jgi.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
~ .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
: .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
_ .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
~__t..1.~, .. .
.
.3 3~32 1. .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.1t.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
t .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.11 1 .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.. .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
"1. .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
~ .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.. .
.
.
-3----~ .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.150 160 170 180 190 200Eng l i sh  Sentence  NoFigure 2The SAT after pass 1.
(18) with a frequency of 20.
On the other hand, the incorrect association of "ele-ment" with "-usammensetzung" (39) is on the basis of a normalized form (for wordsending in "Zusarnmensetzung'), whereas "Zusammensetzung," unnormalized, is cor-rectly associated with "composition" (35).
Totally unrelated words are associated ina few instances, as in "Observatory"/"diesem" (24), "detectors"/"prim~ire-" (36), and"bright-"/"Astronom-" (48).
Of these only the second remains at the end of the thirdpass.
The English "Observatory" is then properly associated with the German word"Observatorium-."
At that stage, "bright-" has no association.Figure 2 shows part of the SAT at the end of pass I of the relaxation cycle.
Sentencesin the English text and in the German text are identified by numbers on the abscissaand the ordinate respectively.
Entries in the array indicate that the sentences are con-sidered to correspond.
The numbers how how often a particular association is sup-ported, which is essentially equivalent to how many word pairs in the WAT supportsuch an association.
If there are no such numbers, then no associations have beenfound for it at this stage.
For example, the association of English sentence 148 withGerman sentence 170 is supported by three different word pairs.
It is already verystriking how strongly occupied entries in this table constrain the possible entries inthe unoccupied slots.Figure 3 shows part of the AST before pass 2.
This is derived directly from thematerial illustrated in Figure 2.
The abscissa gives the English sentence number andin direction of the ordinate the associated German sentences are shown (bullet).
Thosesentence pairs in Figure 2 supported by at least hree word pairs, namely those shownon lines 148, 192, 194, and 196, are assumed to be reliable, and they are the onlyassociations shown for these sentences in Figure 3.
Candidate associations have beenprovided for the intervening sentences by the interpolation method described above.Notice that the greatest number of candidates are shown against sentences occurringmidway between a pair assumed to have been reliably connected (English sentencenumbers 169 to 171).Table 2 shows the first 100 entries of the WAT after pass 3, where the threshold132Martin Kay and Martin R6scheisen Text-Translation AlignmentTable 2The WAT after pass 3.English German Eng.
Freq.
Similarity1 interstellar interstellare- 12 12 ergs Joule 10 13 per plro 12 11/124 univers- Univers- 8 7/85 proto- Proto- 14 13/156 X-ray- R6ntgen- 19 19/227 proton- Proton- 13 6/78 volts Elektronenvolt 19 9 / 119 photo- Photo- 14 13/1610 light- Licht- 23 21/2611 earth Erde 9 4/512 accelerate- beschleunigt 9 7/913 object Objekt 9 7/914 Cygnus Cygnus 63 27/3515 accelerat- beschleunig- 18 16/2116 model- Modell- 17 16/2117 pulsars Pulsare- 8 3/418 cosmic- kosmische- 81 35/4719 galaxy Milchstrat~e 19 17/2320 pulsar- Pulsar- 17 14/1921 electrons Elektronen 12 5/722 magnetic Magnetfeld- 11 5/723 shower- Luftschauer- 20 17/2424 telescope- Teleskop- 9 7/1025 source- Quelle- 40 33/4926 Second- Sekund- 20 2/327 low- nied- 9 2/328 part- Teil- 59 49/7629 and und 69 9/1430 electric- elektrisch- 9 7/1131 gamma- Gammastrahl- 61 27/4332 gas- Gas- 16 5/833 relativ- relativ- 11 8/1334 atmospher- Atmosphere- 10 8/1335 direction -ichtung 10 3/536 years Jahre- 11 10/1737 object- Objekt- 14 10/1738 period- Stunden- 11 7/1239 electro- elektr- 83 63 / 10940 only Nur 18 15/2641 source -uelle 26 4/742 photon- Photon- 10 4/743 high-energy hochenerg- 13 9/1644 directions -ichtungen 8 5/945 thousand- Tausend- 8 5/946 stars Sterne- 11 6/1147 number Anzahl 8 6/1148 interact- wechselwirk- 9 6/1149 signal Signal- 12 7/1350 the die- 496 313/58251 energy Energie 28 22/4152 wave- Wellen- 13 8 / 1553 star- Stern- 29 9/1754 sources Quellen 14 11/2155 nude- Atom- 19 12/2356 of ein- 304 1/257 not nicht 30 1/258 ray Gammaquant- 14 1/259 arrival Ankunfts- 9 1/2133Computational Linguistics Volume 19, Number 1Table 2(continued) The WAT after pass 3.60 percent Prozent 6 161 ultrahigh- ultraho- 5 162 galaxies Galaxien 5 163 composition Zusammensetzung 5 164 Crimean Krim 5 165 supernova Supernova-Explosion- 5 166 activ- aktiv- 5 167 synchrotron Sy.nchrotronstrahlung 5 168 detectors "" 5 1 p rlmare-69 muons lvlyonen 4 170 massive Masse- 4 171 meteorite- Meteorit- 4 172 Low-energy niederenergetische- 4 173 Fermi Fermi- 4 174 decay- Zerfall- 4 175 discovery Entdeckung 4 176 limit Grenze 4 177 ground Erdboden 4 178 ?
aa - Ta - 3 179 R~ert  Rto~ert 3 180 mirrors Spiegel- 3 181 absorption Absorptionslinie- 3 182 David David 3 183 average Mittel- 3 184 !i~ht-years Lichtjahre 3 185 Neutrons Neutronen 3 186 Gregory- Gregory- 3 187 explosions Supernova-Explosionen 3 188 electrically elektrisch 3 189 electromagnetic elektromagnetische- 3 190 candidates Kandidaten 3 191 data Daten- 7 7/892 University Universit- 7 6 / 793 spinning rotier- 6 6 / 794 neutron Neutronenstern 7 6 / 795 p.roposed vorgeschlagen 6 5/696 nnes -inien 6 5/697 colleague- Kollegen 4 4/598 interactions Wechselwirkungen 5 4/599 Physic- Physik- 5 4/5100 models Modelle- 4 4/5for the similarity was lowered to 0.5.
As we pointed out earlier, most of the incor-rect associations in Table 1 have been eliminated.
German "Milchstrafge" (19) is nota translation of the English "galaxy," but the Milky Way is indeed a galaxy and "thegalaxy" is sometimes used in place of "Milky Way" where the reference is clear.
Theassociation between "period-" and "Stunden-" (38) is of a similar kind.
The words arestrongly associated because of recurring phrases of the form "in a 4.8-hour period.
"Figure 4 gives the SAT after pass 3.
It is immediately apparent, first, that themajority of the sentences have been associated with probable translations and, second,that many of these associations are very strongly supported.
For example, note that thecorrespondence b tween English sentence 190 and German sentence 219 is supported21 times.
Using this table, it is in fact possible to locate the translation of a given Englishsentence to within two or three sentences in the German text, and usually more closelythan that.
However,  some ambiguities remain.
Some of the apparent anomalies come134Martin Kay and Martin R6scheisen Text-Translation Alignment8GO~a........................................................................................ i ... i ................... i ..... ~ ....i i i i .t =........................................................................................ i ................... : .
.
: _ i _LL i  ...................................i i !
i .=!
t t t t  i ' i  ii E i .
: t t t t !
t t t "  i............ \] .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
\] .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
r ...... ~ ~ l ;7 - i l - l ' " J l  !
i  i~  .
.
.
.
.
.
.
.
.
.
.
.
.
i .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
i ..................i =: t t  t l i  ~ i. .
.
.
.
.
.
.
.
.
.
.
i .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
~ .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
, .
t - , - t .
, , - i i .
L i - '  .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
i .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
i .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.i i !
, , i !
l l l i !
!
i i ' " i  i i. .
.
.
.
.
.
.
.
.
.
.
i ......... ; ; i ; i i i !
- l i l l l , , ' -T '  ...... ................. i ............................................. i ... ..................... i - -mt i !
i=  ..................... i ........................ i ........................ i ........................ i .................., i i l i i~ ' "  i i i i i.... i .
.
.
.
i ........................ l .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.150 160 170 180 190 200English Sentence NoFigure 3The AST before pass 2.from stylistic differences in the way the texts were presented in the two journals.
Thepractice of Scientific American is to collect sequences of paragraphs into a logical unitby beginning the first of them with an oversized letter.
This is not done in Spektrumder Wissenschafl, which instead provides a subheading at these points.
This thereforeappears as an insertion in the translation.
Two such are sentences number 179 and233, but our procedure has not created incorrect associations for them.Recall that the alignment problem derives its interest from the fact that singlesentences are sometimes translated as sequences of sentences and conversely.
Thesecases generally stand out strongly in the output hat our method delivers.
For example,the English sentence pair (5, 6):Yet whereas many of the most exciting advances in astronomy havecome from the detailed analysis of X-ray and radio sources, until re-cently the source of cosmic rays was largely a matter of speculation.They seem to come from everywhere, raining down on the earth fromall directions at a uniform rate.is rendered in German by the single sentence (5):Dennoch blieben die Quellen der kosmischen Strahlung, die aus allenRichtungen gleichm~t~ig auf die Erde zu treffen scheint, bis vor kurzemreine Spekulation, w~ihrend einige der aufregendsten Fortschritte inder Astronomie aus dem detaillierten Studium von R6ntgen- und Ra-diowellen herriihrten.The second English sentence becomes a relative clause in the German.More complex associations also show up clearly in the results.
For example, En-glish sentences 218 and 219 are translated by German sentences 253, 254, and 255,135Computational Linguistics Volume 19, Number 1o = oE23 t l............................................................... ~ .................................................................. ~-~ !
............839~ 13. .
.
.
.
.
.
.
.
.
.
.
i .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
i .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
i .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
............. ~ .................................................................. 6 i;~..~.z~---~.Y.
.
.. .................................................3855............................................................ ~~-~--~ .......................................................................................3533 ,.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
~-~-~ .
.
.
.
.
.
.
.
.
.
.
.
.
.
~ .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
, .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.s~ 243 84 .... 4-s~_i ................................................................................................................................................150 160 170 180 190 200English Sentence NoF igure  4The SAT after pass 3.where 254 is a translation of the latter part of 218 and the early part of 219:When a proton strikes a gas nucleus, it produces three kinds of pion, ofwhich one kind decays into two gamma rays.
The gamma rays travelclose to the original trajectory of the proton, and the model predictsthey will be beamed toward the earth at just two points on the pulsarsorbit around the companion star.Trifft ein Proton auf einen Atomkern in dieser Gash/.ille, werden dreiArten von Pionen erzeugt.
Die neutralen Pionen zerfallen in jeweilszwei Gammaquanten, die sich beinahe in dieselbe Richtung wie dasursprfingliche Proton bewegen.
Nach der Modellvorstellung gibt esgerade zwei Positionen im Umlauf des Pulsars um seinen Begleitstern,bei denen die Strahlung in Richtung zum Beobachter auf der Erdeausgesandt wird.Another example is provided by English sentences 19 and 20, which appear in Ger-man as sentences 21 and 22.
However the latter part of English sentence 19 is in facttransferred tosentence 22 in the German.
This is also unmistakable in the final results.Notice also, in this example, that the definition of "photon" has become a parenthet-ical expression at the beginning of the second German sentence, a fact which is notreflected.The other end of the cosmic-ray energy spectrum is defined some-what arbitrarily: any quantum greater than 108 electron volts arrivingfrom space is considered a cosmic ray.
The definition encompassesnot only particles but also gamma-ray photons, which are quanta ofelectromagnetic radiation.136Martin Kay and Martin R6scheisen Text-Translation AlignmentTable 3Correctness ofsentence alignment in the various passes of thealgorithm.Pass  Correctness Coverage Constraintin SAT of SAT by AST1 100 % 12 % 4 %2 100 % 47 % 17 %3 100 % 89 % 38 %4 99.7 % 96 % 41 %Das untere Ende des Spektrums der kosmischen Strahlen ist verh~iltnism~it~igunscharf definiert.
Jedes Photon (Quant der elektromagnetischen Strahlung)oder Teilchen mit einer Energie von mehr als 10 s Elektronenvolt, dasaus dem Weltraum eintrifft, bezeichnet man als kosmischen Strahl.It frequently occurred in our data that sentences that were separated by colons orsemicolons in the original appeared as completely distinct sentences in the Germantranslation.
Indeed, the common usage in the two languages would probably havebeen better epresented if we had treated colons and semicolons as sentence separa-tors, along with periods, question marks, and the like.
There are, of course, situationsin English in which these punctuation marks are used in other ways, but they areconsiderably ess frequent and, in any case, it seems that our program would almostalways make the right associations.
An example involving the colon is to be found insentence 142 of the original, translated as sentences 163 and 164:The absorption lines established a lower limit on the distance of CygnusX-3: it must be more distant han the farthest hydrogen cloud, whichis believed to lie about 37,000 light-years away, near the edge of thegalaxy.Aus dieser Absorptionslinie kann man eine untere Grenze der Ent-fernung von Cygnus X bestimmen.
Die Quelle mud jenseits der amweitesten entfernten Wasserstoff-Wolke s in, also weiter als ungefahr37000 Lichtjahre ntfernt, am Rande der Milchstrat~e.English sentence 197, containing a semicolon, is translated by German sentences 228and 229:The estimate is conservative; because it is based on the gamma raysobserved arriving at the earth, it does not take into account he like-lihood that Cygnus X emits cosmic rays in all directions.Dies ist eine vorsichtige Abschatzung.
Sie ist nur aus den Gamma-strahlen-Daten abgeleitet, die auf der Erde gemessen werden; dat~Cygnus X-3 wahrscheinlich kosmische Strahlung in alle Richtungenaussendet, ist dabei noch nicht ber~icksichtigt.137Computational Linguistics Volume 19, Number 18 cE $ooX ee??|.??
"oX ?ooX ?
?
"X @x".~?X Ix?X ?"
xX ?X ?.?
?0 10 20 30 40English Sentence No50Figure 5Sentence alignment of the first 50 sentences ofthe test texts: true alignment (dots) andhypothesis of the SAT after the first pass (circles) and after the second pass (crosses).Table 3 summarizes the accuracy of the algorithm as a function of the numberof passes.
The (thresholded) SAT is evaluated by two criteria: the number of correctalignments divided by the total number of alignments, and--since the SAT does notnecessarily give an alignment for every sentence---the coverage, i.e., the number ofsentences with at least one entry relative to the total number of sentences.
An align-ment is said to be correct if the SAT contains exactly the numbers of the sentencesthat are complete or partial translations of the original sentence.
The coverage of 96%of the SAT in pass 4 is as much as one would expect, since the remaining nonalignedsentences are one-zero alignments, most of them due to the German subheadings thatare not part of the English version.
The table also shows that the AST always providesa significant number of candidates for alignment with each sentence before a pass:the fourth column gives the number of true sentence alignments relative to the totalnumber of candidates in the AST.
Recall that the final alignment is always a subset ofthe hypotheses in the AST in every preceding pass.Figure 5 shows the true sentence alignment for the first 50 sentences (dots), andhow the algorithm discovered them: in the first pass, only a few sentences are setinto correspondence (circles); after the second pass (crosses) already almost half of thecorrespondences are found.
Note that there are no wrong alignments in the first twopasses.
In the third pass, almost all of the remaining alignments are found (for thefirst 50 sentences in the figure: all), and a final pass usually completes the alignment.Our algorithm produces very favorable results when allowed to converge gradu-ally.
Processing time in the original LISP implementation was high, typically severalhours for each pass.
By trading CPU time for memory massively, the time needed bya C++ implementation a Sun 4/75 was reduced to 1.7 min for the first pass, 0.8min for the second, and 0.5 min for the third pass in an application to this pair ofarticles.
(Initialization, i.e., reading the files and building up the data structures, takesanother 0.6 min in the beginning.)
It should be noted that a naive implementation f138Martin Kay and Martin R6scheisen Text-Translation Alignmentthe algorithm without using the appropriate data structures can easily lead to timesthat are a factor of 30 higher and do not scale up to larger texts.The application of our method to a text that we put together from the Hansardcorpus had essentially no problem in identifying the correct sentence alignment ina process of five passes.
The alignments for the first 1000 sentences of the Englishtext were checked by hand, and seven errors were found; five of them occurred insentences where sentence boundaries were not correctly identified by the programbecause of periods that did not mark a sentence boundary and were not identifiedas such by a very simple preprocessing program.
The other two errors involved twoshort sentences for which the SAT did not give an alignment.
Processing time increasedessentially linearly (per pass): the first pass took 8.3 min, the second 3.2 min, and itfurther decreased until the last pass, which took 2.1 min.
(Initialization took 4.2 min.
)Note that the error rate depends crucially on the kind of "annealing schedule" used:if the thresholds that allow a word pair in the WAT to influence the SAT are loweredtoo fast, only a few passes are needed, but accuracy deteriorates.
For example, in anapplication where the process terminated after only three passes, the accuracy wasonly in the eighties (estimated on the basis of the first 120 sentences of the EnglishHansard text checked by hand).
Since processing time after the first pass is usuallyalready considerably ower, we have found that a high accuracy can safely be attainedwhen more passes are allowed than are actually necessary.In order to evaluate the sensitivity of the algorithm to the lengths of the textsthat are to be aligned, we applied it to text samples that ranged in length from 10to 1000 sentences, and examined the accuracy of the WAT after the first pass; thatis, more precisely, the number of word pairs in the WAT that are valid translationsrelative to the total number of word pairs with a similarity of not less than 0.7 (themeasurements are cross-validated over different texts).
The result is that this accuracyincreases asymptotically to 1 with the text length, and is already higher than 80% fora text length of 100 sentences (which is sufficient to reach an almost perfect alignmentin the end).
Roughly speaking, the accuracy is almost 1 for texts longer than 150sentences, and around 0.5 for text length in the lower range from 20 to 60.
In otherwords, texts of a length of more than 150 sentences are suitable to be processed in thisway; text fragments shorter than 80 sentences do not have a high proportion of correctword pairs in the first WAT, but further experiments showed that the final alignmentfor texts of this length is, on average, again almost perfect: the drawback of a lessaccurate initial WAT is apparently largely compensated for by the fact that the ASTis also narrower for these texts; however, the variance in the alignment accuracies isignificantly higher.5.
Related WorkSince we addressed the text translation alignment problem in 1988, a number of re-searchers, among them Gale and Church (1991) and Brown, Lai, and Mercer (1991),have worked on the problem.
Both methods are based on the observation that thelength of text unit is highly correlated to the length of the translation of this unit, nomatter whether length is measured in number of words or in number of characters(see Figure 6).
Consequently, they are both easier to implement than ours, though notnecessarily more efficient.
The method of Brown, Lai, and Mercer (1991) is based ona hidden Markov model for the generation of aligned pairs of corpora, whose param-eters are estimated from a large text.
For an application of this method to the Cana-dian Hansard, good results are reported.
However, the problem was also considerablyfacilitated by the way the implementation made use of Hansard-specific comments139Computational Linguistics Volume 19, Number 1?
?~ ?
Re ?S.
d .S40 60 80 120English: Length in words"i?f, ?
% o200 400 600 800English: Length in charsFigure 6Lengths of Aligned Paragraphs are Correlated: Robust regression between lengths of alignedparagraphs.
Left: length measured in words.
Right: length measured in characters.and annotations: these are used in a preprocessing step to find anchors for sentencealignment such that, on average, there are only ten sentences in between.
Moreover,this particular corpus is well known for the near literalness of its translations, andit is therefore unclear to what extent he good results are due to the relative easeof the problem.
This would be an important consideration when comparing variousalgorithms; when the algorithms are actually applied, it is clearly very desirable to in-corporate as much prior knowledge (say, on potential anchors) as possible.
Moreover,long texts can almost always be expected to contain natural anchors, such as chapterand section headings, at which to make an a priori segmentation.Gale and Church (1991) note that their method performed considerably betterwhen lengths of sentences were measured in number of characters instead of in num-ber of words.
Their method is based on a probabilistic model of the distance betweentwo sentences, and a dynamic programming algorithm is used to minimize the totaldistance between aligned units.
Their implementation assumes that each character inone language gives rise to, on average, one character in the other language.
8 In ourtexts, one character in English on average gives rise to somewhat more than 1.2 char-acters in German, and the correlation between the lengths (in characters) of alignedparagraphs in the two languages was with 0.952 lower than the 0.991 that are men-tioned in Gale and Church (1991), which supports our impression that the ScientificAmerican texts we used are hard texts to align, but it is not clear to what extent hiswould deteriorate he results.
In applications to economic reports from the Union Bankof Switzerland, the method performs very well on simple alignments (one-to-one, one-to-two), but has at the moment problems with complex matches.
The method has the8 Recall that, in a similar way, we assumed in our implementation that one sentence in one languagegives rise to, on average, n/m sentences in the other language (see first footnote in Section 2.3).140Martin Kay and Martin R6scheisen Text-Translation Alignmentadvantage of associating a score with pairs of sentences so that it is easy to extract asubset for which there is a high likelihood that the alignments are correct.Given the simplicity of the methods proposed by Brown, Lai, and Mercer andGale and Church, either of them could be used as a heuristic in the constructionof the initial AST in our algorithm.
In the current version, the number of candidatesentence pairs that are considered in the first pass near the middle of a text contributesdisproportionally to the cost of the computation.
In fact, as we remarked earlier, thecomplexity of this step is O(nvFff).
The proposed modification would effectively makeit linear.6.
Future WorkFor most practical purposes, the alignment algorithm we have described producesvery satisfactory esults, even when applied to relatively free translations.
There aredoubtless many places in which the algorithm itself could be improved.
For example,it is clear that the present method of building the SAT favors associations betweenlong sentences, and this is not surprising, because there is more information in longsentences.
But we have not investigated the extent of this bias and we do not thereforeknow it as appropriate.The present algorithm rests on being able to identify one-to-one associations be-tween certain words, notably technical terms and proper names.
It is clear from abrief inspection of Table 2 that very few correspondences are noticed among everydaywords and, when they are, it is usually because those words also have precise tech-nical uses.
The very few exceptions include "only ' / "nur"  and "the" / "die-."
The pair"per" / "pro" might also qualify, but if the languages afford any example of a scientificpreposition, this is surely it.
The most interesting further developments would be inthe direction of loosening up this dependence on one-to-one associations both becausethis would present a very significant challenge and also because we are convinced thatour present method identifies essentially all the significant one-to-one associations.There are two obvious kinds of looser associations that could be investigated.One would consist of connections between a single vocabulary item in one languageand two or more in the other, or even between several items in one language andseveral in the other.
The other would involve connections--one-one, one-many, ormany-many--between phrases or recurring sequences.We have investigated the first of these enough to satisfy ourselves that there islatent information on one-to-many associations in the text, and that it can be revealedby suitable xtensions ofour methods.
However, it is clear that the combinatorial prob-lems associated with this approach are severe, and pursuing it would require much finetuning of the program and designing much more effective ways of indexing the mostimportant data structures.
The key to reducing the combinatorial explosion probablylies in using tables of similarities uch as those the present algorithm uses to suggestcombinations of items that would be worth considering.
If such an approach couldbe made efficient enough, it is even possible that it would provide a superior way ofsolving the problem for which our heuristic methods of morphological nalysis wereintroduced.
Its superiority would come from the fact that it would not depend onwords being formed by concatenation, but would also accommodate such phenom-ena as umlaut, ablaut, vowel harmony, and the nonconcatenative process of Semiticmorphology.The problems of treating recurring sequences are less severe.
Data structures, uchas the Patricia tree (Knuth 1973; pp.
490-493) provide efficient means of identifyingall such sequences and, once identified, the data they provide could be added to141Computational Linguistics Volume 19, Number 1the WAT much as we now add the results of morphological analysis.
Needless tosay, this would only allow for uninterrupted sequences.
Any attempt o deal withdiscontinuous sequences would doubtless also involve great combinatorial problems.These avenues for further development are intriguing and would surely lead tointeresting results.
But it is unlikely that they would lead to much better sets of asso-ciations among sentences than are to be found in the SATs that our present programproduces, and it was mainly these results that we were interested in from the outset.The other avenues we have mentioned concern improvements in the WAT which, forus, was always a secondary interest.ReferencesBaayen, H. (1991).
"A stochastic process forword frequency distributions."
InProceedings, 29th Annual Meeting of theAssociation for Computational Linguistics,Berkeley, CA.Brown, P.; Lai, J. C.; and Mercer, R. L.(1991).
"Aligning sentences in parallelcorpora."
In Proceedings, 29th AnnualMeeting of the Association for ComputationalLinguistics, Berkeley, CA.Brown, P.; Cocke, J.; Della Pietra, S.; DellaPietra, V.; Jelinek, F.; Lafferty, J.; Mercer,R.
; and Roossin P. (1990).
"A statisticalapproach to machine translation.
"Computational Linguistics, 16, 79-85.Church, K. W., and Hanks, P. (1990).
"Wordassociation orms, mutual information,and lexicography."
ComputationalLinguistics, 16(1), 22-29.Drela, M., and Langford, J. S.
(1985).
"Human-powered flight."
ScientificAmerican, 253(5).Drela, M., and Langford, J. S.
(1986).
"Fliegen mit Muskelkraft."
Spektrum derWissenschaft.Fano, R. (1961).
Transmission of Information.
AStatistical Theory of Communications.
MITPress.Gale, W. A., and Church, K. W. (1991).
"Aprogram for aligning sentences inbilingual corpora."
In Proceedings, 29thAnnual Meeting of the Association forComputational Linguistics.
Berkeley, CA.Kay, M., and R6scheisen, M.
(1988).
"Text-translation alignment."
TechnicalReport, Xerox Palo Alto Research Center.Knuth, D. E. (1973).
The Art of ComputerProgramming.
Vol.
3, Sorting andSearching.
Addison-Wesley.MacKeown, P. K., and Weekes, T. C.
(1985).
"Cosmic rays from Cygnus X-3."
ScientificAmerican, 253(5).MacKeown, P. K., and Weekes, T. C.
(1986).
"Kosmische Strahlen von Cygnus X-3.
"Spektrum der Wissenschafl.van Rijsbergen, C. J.
(1979).
InformationRetrieval.
Butterworths.Sato, S., and Nagao, M. (1990).
"Towardmemory-based translation."
InProceedings, I5th International Conference onComputational Linguistics (COLING-90).Helsinki, Finland.142
