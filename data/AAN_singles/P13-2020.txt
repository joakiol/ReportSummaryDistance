Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 110?114,Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational LinguisticsEasy-First POS Tagging and Dependency Parsing with Beam SearchJi Ma?
JingboZhu?
Tong Xiao?
Nan Yang?
?Natrual Language Processing Lab., Northeastern University, Shenyang, China?MOE-MS Key Lab of MCC, University of Science and Technology of China,Hefei, Chinamajineu@outlook.com{zhujingbo, xiaotong}@mail.neu.edu.cnnyang.ustc@gmail.comAbstractIn this paper, we combine easy-first de-pendency parsing and POS tagging algo-rithms with beam search and structuredperceptron.
We propose a simple variantof ?early-update?
to ensure valid updatein the training process.
The proposed so-lution can also be applied to combinebeam search and structured perceptronwith other systems that exhibit spuriousambiguity.
On CTB, we achieve 94.01%tagging accuracy and 86.33% unlabeledattachment score with a relatively smallbeam width.
On PTB, we also achievestate-of-the-art performance.1 IntroductionThe easy-first dependency parsing algorithm(Goldberg and Elhadad, 2010) is attractive due toits good accuracy, fast speed and simplicity.
Theeasy-first parser has been applied to many appli-cations (Seeker et al, 2012; S?ggard and Wulff,2012).
By processing the input tokens in an easy-to-hard order, the algorithm could make use ofstructured information on both sides of the hardtoken thus making more indicative predictions.However, rich structured information also causesexhaustive inference intractable.
As an alterna-tive, greedy search which only explores a tinyfraction of the search space is adopted (Goldbergand Elhadad, 2010).To enlarge the search space, a natural exten-sion to greedy search is beam search.
Recentwork also shows that beam search together withperceptron-based global learning (Collins, 2002)enable the use of non-local features that are help-ful to improve parsing performance withoutoverfitting (Zhang and Nivre, 2012).
Due to the-se advantages, beam search and global learninghas been applied to many NLP tasks (Collins andRoark 2004; Zhang and Clark, 2007).
However,to the best of our knowledge, no work in the lit-erature has ever applied the two techniques toeasy-first dependency parsing.While applying beam-search is relativelystraightforward, the main difficulty comes fromcombining easy-first dependency parsing withperceptron-based global learning.
In particular,one needs to guarantee that each parameter up-date is valid, i.e., the correct action sequence haslower model score than the predicted one1.
Thedifficulty in ensuring validity of parameter up-date for the easy-first algorithm is caused by itsspurious ambiguity, i.e., the same result might bederived by more than one action sequences.For algorithms which do not exhibit spuriousambiguity, ?early update?
(Collins and Roark2004) is always valid: at the k-th step when thesingle correct action sequence falls off the beam,1 As shown by (Huang et al, 2012), only valid update guar-antees the convergence of any perceptron-based training.Invalid update may lead to bad learning or even make thelearning not converge at all.Figure 1: Example of cases without/with spuriousambiguity.
The 3 ?
1 table denotes a beam.
?C/P?denotes correct/predicted action sequence.
Thenumbers following C/P are model scores.110its model score must be lower than those still inthe beam (as illustrated in figure 1, also see theproof in (Huang et al, 2012)).
While for easy-first dependency parsing, there could be multipleaction sequences that yield the gold result (C1 andC2 in figure 1).
When all correct sequences falloff the beam, some may indeed have highermodel score than those still in the beam (C2 infigure 1), causing invalid update.For the purpose of valid update, we present asimple solution which is based on early update.The basic idea is to use one of the correct actionsequences that were pruned right at the k-th step(C1 in figure 1) for parameter update.The proposed solution is general and can alsobe applied to other algorithms that exhibit spuri-ous ambiguity, such as easy-first POS tagging(Ma et al, 2012) and transition-based dependen-cy parsing with dynamic oracle (Goldberg andNivre, 2012).
In this paper, we report experi-mental results on both easy-first dependencyparsing and POS tagging (Ma et al, 2012).
Weshow that both easy-first POS tagging and de-pendency parsing can be improved significantlyfrom beam search and global learning.
Specifi-cally, on CTB we achieve 94.01% tagging accu-racy which is the best result to date2 for a singletagging model.
With a relatively small beam, weachieve 86.33% unlabeled score (assume goldtags), better than state-of-the-art transition-basedparsers (Huang and Sagae, 2010; Zhang andNivre, 2011).
On PTB, we also achieve goodresults that are comparable to the state-of-the-art.2 Easy-first dependency parsingThe easy-first dependency parsing algorithm(Goldberg and Elhadad, 2010) builds a depend-ency tree by performing two types of actionsLEFT(i) and RIGHT(i) to a list of sub-tree struc-tures p1,?, pr.
pi is initialized with the i-th word2 Joint tagging-parsing models achieve higher accuracy, butthose models are not directly comparable to ours.Algorithm 1: Easy-first with beam searchInput:     sentence   of n words,  beam width sOutput:  one best dependency tree(     )( )(  )// top s extensions from the beam1                     // initially, empty beam2 for    1   1 do3             (        )4 return        ( )   // tree built by the best sequenceof the input sentence.
Action LEFT(i)/RIGHT(i)attaches pi to its left/right neighbor and then re-moves pi from the sub-tree list.
The algorithmproceeds until only one sub-tree left which is thedependency tree of the input sentence (see theexample in figure 2).
Each step, the algorithmchooses the highest score action to perform ac-cording to the linear model:( )     ( )Here,  is the weight vector and  is the featurerepresentation.
In particular,  (    ( )( )) denotes features extracted from pi.The parsing algorithm is greedy which ex-plores a tiny fraction of the search space.
Oncean incorrect action is selected, it can never yieldthe correct dependency tree.
To enlarge thesearch space, we introduce the beam-search ex-tension in the next section.3 Easy-first with beam searchIn this section, we introduce easy-first with beamsearch in our own notations that will be usedthroughout the rest of this paper.For a sentence x of n words, let   be the action(sub-)sequence that can be applied, in sequence,to x and the result sub-tree list is denoted by( )  For example, suppose x is ?I am valid?
andy is [RIGHT(1)], then y(x) yields figure 2(b).
Letto be LEFT(i)/RIGHT(i) actions where    1   .Thus, the set of all possible one-action extensionof   is:( )            ( )Here, ?
?
means insert   to the end of  .
Follow-ing (Huang et al, 2012), in order to formalizebeam search, we also use the( )operation which returns the top s action sequenc-es in   according to   ( ).
Here,  denotes aset of action sequences,   ( ) denotes the sum offeature vectors of each action inPseudo-code of easy-first with beam search isshown in algorithm 1.
Beam search grows s(beam width) action sequences in parallel using aFigure 2: An example of parsing ?I am valid?.
Spu-rious ambiguity: (d) can be derived by both[RIGHT(1), LEFT(2)] and [LEFT(3), RIGHT(1)].111Algorithm 2: Perceptron-based training over onetraining sample (   )Input:    (   ), s, parameterOutput: new parameter(       )(      ( ))(  )// top correct extension from the beam12 for    1   1 do3     ?
(          )4            (        )5    if           // all correct seq.
falls off the beam6             ( ?)
(     )7         break8 if        ( )      // full update9         ( ?)
(       )10 returnbeam  , (sequences in   are sorted in terms ofmodel score, i.e.,   (    )     (  1 ) ).At each step, the sequences in   are expanded inall possible ways and then   is filled up with thetop s newly expanded sequences (line 2 ~ line 3).Finally, it returns the dependency tree built bythe top action sequence in      .4 TrainingTo learn the weight vector , we use the percep-tron-based global learning3 (Collins, 2002) whichupdates  by rewarding the feature weights firedin the correct action sequence and punish thosefired in the predicted incorrect action sequence.Current work (Huang et al, 2012) rigorouslyexplained that only valid update ensures conver-gence of any perceptron variants.
They also justi-fied that the popular ?early update?
(Collins andRoark, 2004) is valid for the systems that do notexhibit spurious ambiguity4.However, for the easy-first algorithm or moregenerally, systems that exhibit spurious ambigui-ty, even ?early update?
could fail to ensure valid-ity of update (see the example in figure 1).
Forvalidity of update, we propose a simple solutionwhich is based on ?early update?
and which canaccommodate spurious ambiguity.
The basic ideais to use the correct action sequence which was3 Following (Zhang and Nivre, 2012), we say the trainingalgorithm is global if it optimizes the score of an entire ac-tion sequence.
A local learner trains a classifier which dis-tinguishes between single actions.4 As shown in (Goldberg and Nivre 2012), most transition-based dependency parsers (Nivre et al, 2003; Huang andSagae 2010;Zhang and Clark 2008) ignores spurious ambi-guity by using a static oracle which maps a dependency treeto a single action sequence.Features of (Goldberg and Elhadad, 2010)for p in pi-1, pi, pi+1 wp-vlp, wp-vrp, tp-vlp,tp-vrp, tlcp, trcp, wlcp, wlcpfor p in pi-2, pi-1, pi, pi+1, pi+2 tp-tlcp,  tp-trcp, tp-tlcp-trcpfor p, q, r in (pi-2, pi-1, pi), (pi-1, pi+1, pi), (pi+1, pi+2 ,pi)tp-tq-tr, tp-tq-wrfor p, q in (pi-1, pi) tp-tlcp-tq,   tp-trcp-tq,   ,tp-tlcp-wq,,tp-trcp-wq,   tp-wq-tlcq,  tp-wq-trcqTable 1: Feature templates for English dependencyparsing.
wp denotes the head word of p, tp denotes thePOS tag of wp.
vlp/vrp denotes the number p?s ofleft/right child.
lcp/rcp denotes p?s leftmost/rightmostchild.
pi denotes partial tree being considered.pruned right at the step when all correct sequencefalls off the beam (as C1 in figure 1).Algorithm 2 shows the pseudo-code of thetraining procedure over one training sample(   ), a sentence-tree pair.
Here we assume   tobe the set of all correct action sequences/sub-sequences.
At step k, the algorithm constructs acorrect action sequence  ?
of length k by extend-ing those in      (line 3).
It also checks whetherno longer contains any correct sequence.
If so,?
together with       are used for parameter up-date (line 5 ~ line 6).
It can be easily verified thateach update in line 6 is valid.
Note that both?TOPC?
and the operation in line 5 use   to checkwhether an action sequence y is correct or not.This  can  be  efficiently  implemented   (withoutexplicitly enumerating  ) by checking if eachLEFT(i)/RIGHT(i) in y are compatible with (   ):pi already collected all its dependents accordingto t; pi is attached to the correct neighbor sug-gested by t.5 ExperimentsFor English, we use PTB as our data set.
We usethe standard split for dependency parsing and thesplit used by (Ratnaparkhi, 1996) for POS tag-ging.
Penn2Malt5 is used to convert the bracket-ed structure into dependencies.
For dependencyparsing, POS tags of the training set are generat-ed using 10-fold jack-knifing.For Chinese, we use CTB 5.1 and the splitsuggested by (Duan et al, 2007) for both taggingand dependency parsing.
We also use Penn2Maltand the head-finding rules of (Zhang and Clark2008) to convert constituency trees into depend-encies.
For dependency parsing, we assume goldsegmentation and POS tags for the input.5 http://w3.msi.vxu.se/~nivre/research/Penn2Malt.html112Features used in English dependency parsingare listed in table 1.
Besides the features in(Goldberg and Elhadad, 2010), we also includesome trigram features and valency featureswhich are useful for transition-based dependencyparsing (Zhang and Nivre, 2011).
For EnglishPOS tagging, we use the same features as in(Shen et al, 2007).
For Chinese POS tagging anddependency parsing, we use the same features as(Ma et al, 2012).
All of our experiments areconducted on a Core i7 (2.93GHz) machine, boththe tagger and parser are implemented using C++.5.1 Effect of beam widthTagging/parsing performances with differentbeam widths on the development set are listed intable 2 and table 3.
We can see that Chinese POStagging, dependency parsing as well as Englishdependency parsing greatly benefit from beamsearch.
While tagging accuracy on English onlyslightly improved.
This may because that theaccuracy of the greedy baseline tagger is alreadyvery high and it is hard to get further improve-ment.
Table 2 and table 3 also show that thespeed of both tagging and dependency parsingdrops linearly with the growth of beam width.5.2 Final resultsTagging results on the test set together with someprevious results are listed in table 4.
Dependencyparsing results on CTB and PTB are listed in ta-ble 5 and table 6, respectively.On CTB, tagging accuracy of our greedy base-line is already comparable to the state-of-the-art.As the beam size grows to 5, tagging accuracyincreases to 94.01% which is 2.3% error reduc-tion.
This is also the best tagging accuracy com-paring with previous single tagging models (Forlimited space, we do not list the performance ofjoint tagging-parsing models).Parsing performances on both PTB and CTBare significantly improved with a relatively smallbeam width (s = 8).
In particular, we achieve86.33% uas on CTB which is 1.54% uas im-provement over the greedy baseline parser.Moreover, the performance is better than the besttransition-based parser (Zhang and Nivre, 2011)which adopts a much larger beam width (s = 64).6 Conclusion and related workThis work directly extends (Goldberg and El-hadad, 2010) with beam search and global learn-ing.
We show that both the easy-first POS taggerand dependency parser can be significantly impr-s PTB CTB speed1 97.17 93.91 13503 97.20 94.15 5605 97.22 94.17 385Table 2: Tagging accuracy vs beam width vs.
Speed isevaluated using the number of sentences that can beprocessed in one secondsPTB CTBspeeduas compl uas compl1 91.77 45.29 84.54 33.75 2212 92.29 46.28 85.11 34.62 1244 92.50 46.82 85.62 37.11 718 92.74 48.12 86.00 35.87 39Table 3: Parsing accuracy vs beam width.
?uas?
and?compl?
denote unlabeled score and complete matchrate respectively (all excluding punctuations).PTB CTB(Collins, 2002) 97.11 (Hatori et al, 2012) 93.82(Shen et al, 2007) 97.33 (Li et al, 2012) 93.88(Huang et al, 2012) 97.35 (Ma et al, 2012) 93.84this work   1 97.22 this work   1 93.87this work     97.28 this work     94.01?Table 4: Tagging results on the test set.
???
denotesstatistically significant over the greedy baseline byMcNemar?s test (      )Systems s uas compl(Huang and Sagae, 2010) 8 85.20 33.72(Zhang and Nivre, 2011) 64 86.00 36.90(Li et al, 2012) ?
86.55 ?this work 1 84.79 32.98this work 8 86.33?36.13Table 5: Parsing results on CTB test set.Systems s uas compl(Huang and Sagae, 2010) 8 92.10 ?
(Zhang and Nivre, 2011) 64 92.90 48.50(Koo and Collins, 2010) ?
93.04 ?this work 1 91.72 44.04this work 8 92.47?46.07Table 6: Parsing results on PTB test set.oved using beam search and global learning.This work can also be considered as applying(Huang et al, 2012) to the systems that exhibitspurious ambiguity.
One future direction mightbe to apply the training method to transition-based parsers with dynamic oracle (Goldberg andNivre, 2012) and potentially further advance per-formances of state-of-the-art transition-basedparsers.113Shen et al, (2007) and (Shen and Joshi, 2008)also proposed bi-directional sequential classifica-tion with beam search for POS tagging andLTAG dependency parsing, respectively.
Themain difference is that their training method aimsto learn a classifier which distinguishes betweeneach local action while our training method aimsto distinguish between action sequences.
Ourmethod can also be applied to their framework.AcknowledgmentsWe would like to thank Yue Zhang, Yoav Gold-berg and Zhenghua Li for discussions and sug-gestions on earlier drift of this paper.
We wouldalso like to thank the three anonymous reviewersfor their suggestions.
This work was supported inpart by the National Science Foundation of Chi-na (61073140; 61272376), Specialized ResearchFund for the Doctoral Program of Higher Educa-tion (20100042110031) and the FundamentalResearch Funds for the Central Universities(N100204002).ReferencesCollins, M. 2002.
Discriminative training methods forhidden markov models: Theory and experimentswith perceptron algorithms.
In Proceedings ofEMNLP.Duan, X., Zhao, J., , and Xu, B.
2007.
Probabilisticmodels for action-based Chinese dependency pars-ing.
In Proceedings of ECML/ECPPKDD.Goldberg, Y. and Elhadad, M. 2010 An Efficient Al-gorithm for Eash-First Non-Directional Dependen-cy Parsing.
In Proceedings of NAACLHuang, L. and Sagae, K. 2010.
Dynamic program-ming for linear-time incremental parsing.
In Pro-ceedings of ACL.Huang, L. Fayong, S. and Guo, Y.
2012.
StructuredPerceptron with Inexact Search.
In Proceedings ofNAACL.Koo, T. and Collins, M. 2010.
Efficient third-orderdependency parsers.
In Proceedings of ACL.Li, Z., Zhang, M., Che, W., Liu, T. and Chen, W.2012.
A Separately Passive-Aggressive TrainingAlgorithm for Joint POS Tagging and DependencyParsing.
In Proceedings of COLINGMa, J., Xiao, T., Zhu, J. and Ren, F. 2012.
Easy-FirstChinese POS Tagging and Dependency Parsing.
InProceedings of COLINGRataparkhi, A.
(1996) A Maximum Entropy Part-Of-Speech Tagger.
In Proceedings of EMNLPShen, L., Satt, G. and Joshi, A. K. (2007) GuidedLearning for Bidirectional Sequence Classification.In Proceedings of ACL.Shen, L. and  Josh, A. K. 2008.
LTAG DependencyParsing with Bidirectional Incremental Construc-tion.
In Proceedings of  EMNLP.Seeker, W., Farkas, R. and Bohnet, B.
2012 Data-driven Dependency Parsing With Empty Heads.
InProceedings of COLINGS?ggard, A. and Wulff, J.
2012.
An Empirical Studyof Non-lexical Extensions to Delexicalized Trans-fer.
In Proceedings of COLINGYue Zhang and Stephen Clark.
2007 Chinese Seg-mentation Using a Word-based Perceptron Algo-rithm.
In Proceedings of ACL.Zhang, Y. and Clark, S. 2008.
Joint word segmenta-tion and POS tagging using a single perceptron.
InProceedings of ACL.Zhang, Y. and Nivre, J.
2011.
Transition-based de-pendency parsing with rich non-local features.
InProceedings of ACL.Zhang, Y. and Nivre, J.
2012.
Analyzing the Effect ofGlobal Learning and Beam-Search for Transition-Based Dependency Parsing.
In Proceedings ofCOLING.114
