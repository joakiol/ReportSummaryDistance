Improving Chronological Sentence Orderingby Precedence RelationNaoaki OKAZAKIThe University of Tokyo7-3-1 Hongo, Bunkyo-ku,Tokyo 113-8656,Japanokazaki@miv.t.u-tokyo.ac.jpYutaka MATSUOAIST2-41-6 Aomi, Koto-ku,Tokyo 135-0064,Japany.matsuo@carc.aist.go.jpMitsuru ISHIZUKAThe University of Tokyo7-3-1 Hongo, Bunkyo-ku,Tokyo 113-8656,Japanishizuka@miv.t.u-tokyo.ac.jpAbstractIt is necessary to find a proper arrangement of sen-tences in order to generate a well-organized sum-mary from multiple documents.
In this paper we de-scribe an approach to coherent sentence ordering forsummarizing newspaper articles.
Since there is noguarantee that chronological ordering of extractedsentences, which is widely used by conventional sum-marization system, arranges each sentence behindpresupposed information of the sentence, we improvechronological ordering by resolving antecedent sen-tences of arranged sentences.
Combining the re-finement algorithm with topical segmentation andchronological ordering, we address our experiment totest the effectiveness of the proposed method.
Theresults reveal that the proposed method improveschronological sentence ordering.1 IntroductionThe growth of computerized documents enablesus to find relevant information easily owing totechnological advances in Information Retrieval.Although it is convenient that we can obtain agreat number of documents with a search en-gine, this situation also presents the informationpollution problem: ?Who is willing to take thetedious burden of reading all those text docu-ments??
Automatic text summarization (Mani,2001), is one solution to the problem, providingusers with a condensed version of the originaltext.Most existing summarization systems makeuse of sentence or paragraph extraction, whichfinds significant textual segments in source doc-uments, and compile them in a summary.
Afterwe select significant sentences as a material fora summary, we must find a proper arrangementof the sentences and edit each sentence by delet-ing unnecessary parts or inserting necessary ex-pressions.
Although there has been a great dealof research on extraction since the early stageof natural language processing (Luhn, 1958),research on post-processing of automatic sum-marization is relatively small in number.
It isessential to pay attention to sentence orderingin case of multi-document summarization.
Sen-tence position in the original document, whichyields a good clue to sentence arrangement forsingle-document summarization, is not enoughfor multi-document summarization because wemust consider inter-document order at the sametime.In this paper we propose an approach to co-herent text structuring for summarizing news-paper articles.
We improve chronological order-ing, which is widely used by conventional sum-marization system, complementing presupposedinformation of each sentence.
The rest of thispaper is organized as follows.
We first reviewthe sentence ordering problem and present ourapproach to generate an acceptable ordering inthe light of coherence relation.
The subsequentsection (Section 3) addresses evaluation metricsand experiment results.
In Section 4 we discussfuture work and conclude this paper.2 Sentence Ordering2.1 Sentence ordering problemOur goal is to determine the most probable per-mutation of given sentences and to generate awell-structured text.
When a human is askedto make an arrangement of sentences, he or shemay perform this task without difficulty justas we write out thoughts in a text.
However,we must consider what accomplishes this tasksince computers are unaware of order of thingsby nature.
Discourse coherence as typified byrhetorical relation (Mann and Thompson, 1988)and coherence relation (Hobbs, 1990) is of helpto this question.
Hume (Hume, 1748) claimedthat qualities from which association arises andby which the mind is conveyed from one ideato another are three: resemblance; contiguityin time or place; and cause and effect.
Thatis to say we should organize a text from frag-c) Dolly gave birth to two children in her life.b) The father is of a different kind and Dollyhad been pregnant for about five months.a) Dolly the clone sheep was born in 1996.Sentences Preferred ordering[a-c-b]RefinementChronological orderFigure 1: A chronological ordering is not enough.mented information on the basis of topical rele-vancy, chronological sequence, and cause-effectrelation.
It is especially true in sentence order-ing of newspaper articles because we must ar-range a large number of time-series events con-cerning several topics.Barzilay et al (Barzilay et al, 2002) addressthe problem of sentence ordering in the contextof multi-document summarization and the im-pact of sentence ordering on readability of asummary.
They proposed two naive sentence-ordering techniques such as majority ordering(examines most frequent orders in the originaldocuments) and chronological ordering (orderssentence by the publication date).
Showing thatusing naive ordering algorithms does not pro-duce satisfactory orderings, Barzilay et al alsoinvestigates through experiments with humans,how to identify patterns of orderings that canimprove the algorithm.
Based on the exper-iments, they propose another algorithm thatutilizes chronological ordering with topical seg-mentation to separate sentences referring to atopic from ones referring to another.Lapata (Lapata, 2003) proposes another ap-proach to information ordering based on a prob-abilistic model that assumes the probability ofany given sentence is determined by its adja-cent sentence and learns constraints on sen-tence order from a corpus of domain specifictexts.
Lapata estimates transitional probabil-ity between sentences by some attributes suchas verbs (precedence relationships of verbs inthe corpus), nouns (entity-based coherence bykeeping track of the nouns) and dependencies(structure of sentences).2.2 Improving chronological orderingAgainst the background of these studies, wepropose the use of antecedence sentences to ar-range sentences.
Let us consider an exampleshown in Figure 1.
There are three sentences a,b, and c from which we get an order [a-b-c]by chronological ordering.
When we read thesesentences in this order, we find sentence b tobe incorrectly positioned.
This is because sen-tence b is written on the presupposition thatthe reader may know that Dolly had a child.
Inother words, it is more fitting to assume sen-tence b to be an elaboration of sentence c. Asone may easily imagine, there are some prece-dent sentences prior to sentence b in the origi-nal document.
Lack of presupposition obscureswhat a sentence is saying and confuses the read-ers.
Hence, we should refine the chronologicalorder and revise the order to [a-c-b], puttingsentence c before sentence b.We show a block diagram of our ordering al-gorithm shown in Figure 2.
Given nine sen-tences denoted by [a b ... i], for example,the algorithm eventually produces an order-ing, [a-b-f-c-i-g-d-h-e].
We consider top-ical segmentation and chronological ordering tobe fundamental to sentence ordering as well asconventional ordering techniques (Barzilay etal., 2002) and make an attempt to refine theordering.
We firstly recognize topics in sourcedocuments to separate sentences referring to atopic from ones referring to another.
In Fig-ure 2 example we obtain two topical segments(clusters) as an output from the topical cluster-ing.
In the second phase we order sentences ofeach segment by the chronological order.
If twosentences have the same chronological order, weelaborate the order on the basis of sentence po-sition and resemblance relation.
Finally, we re-fine each ordering by resolving antecedent sen-tences and output the final ordering.
In the restof this section we give a detailed description ofeach phase.2.3 Topical clusteringThe first task is to categorize sentences by theirtopics.
We assume a newspaper article to bewritten about one topic.
Hence, to classify top-ics in sentences, we have only to classify articlesa ab bc cddeeffggh hiiabcheigdfabfdhciegTopical clustering by documentsChronological orderingwith resemblance relationOrdering refinementby precedence relationCluster #1Unorderedsentences OrderedsentencesCluster #2Figure 2: The outline of the ordering algorithm.by their topics.
Given l articles and we foundm kinds of terms in the articles.
Let D be adocument-term matrix (l ?m), whose elementDij represents frequency of a term #j in doc-ument #i, We use Di to denote a term vector(i-component row vector) of document #i. Af-ter measuring distance or dissimilarity betweentwo articles #x and #y:distance(Dx, Dy) = 1?
Dx ?Dy|Dx||Dy| , (1)we apply the nearest neighbor method (Coverand Hart, 1967) to merge a pair of clusterswhen their minimum distance is lower than agiven parameter ?
= 0.3 (determined empiri-cally).
At last we classify sentences accordingto topical clusters, assuming that a sentence ina document belonging to a cluster also belongsto the same cluster.2.4 Chronological orderingIt is difficult for computers to find a resemblanceor cause-effect relation between two phenom-ena while we do not have conclusive evidencewhether a pair of sentences gathered arbitrarilyfrom multiple documents has some relation.
Anewspaper usually deals with novel events thathave occurred since the last publication.
Hence,publication date (time) of each article turns outto be a good estimator of resemblance relation(i.e., we observe a trend or series of relevantevents in a time period), contiguity in time, andcause-effect relation (i.e., an event occurs as aresult of previous events).
Although resolvingtemporal expressions in sentences (e.g., yester-day, the next year, etc.)
(Mani and Wilson,2000; Mani et al, 2003) may give a more pre-cise estimation of these relations, it is not aneasy task.
For this reason we order sentencesof each segment (cluster) by the chronologicala..c'.bc.Article #1 Article #2 Article #3chronological orderFigure 3: Background idea of ordering refine-ment by precedence relation.order, assigning a time stamp for each sentenceby its publication date (i.e., the date when thearticle was written).When there are sentences having the sametime stamp, we elaborate the order on the ba-sis of sentence position and sentence connectiv-ity.
We restore an original ordering if two sen-tences have the same time stamp and belongto the same article.
If sentences have the sametime stamp and are not from the same article,we arrange a sentence which is more similar topreviously ordered sentences to assure sentenceconnectivity.2.5 Ordering refinement by precedencerelationAfter we obtain an ordering of a topical seg-ment by chronological ordering, we improve itas shown in Figure 1 based on antecedence sen-tences.
Figure 3 shows the background ideaof ordering refinement by precedence relation.Just as in the example in Figure 1, we havethree sentences a, b, and c in chronological or-der.
At first we get sentence a out of the sen-tences and check its antecedent sentences.
See-ing that there are no sentences prior to sentencea in article #1, we accept to put sentence ahere.
Then we get sentence b out of remainingsentences and check its antecedent sentences.We find several sentences before sentence b inarticle #2 this time.
Grasping what the an-tecedent sentences are saying, we confirm firstof all whether what they are saying is mentionedby previously arranged sentences (i.e., sentencea).
If it is mentioned, we put sentence b hereand extend the ordering to [a-b].
Otherwise,we search a substitution for what the precedencesentences are saying from the remaining sen-tences (i.e., sentence c in this example).
In theFigure 3 example, we find out that sentence a isnot referring to what sentence c?
is saying butsentence c is approximately referring to that.StartEndabefe fcdffddfd feded efNo precedent sentences beforesentence a.
Choose a.Choose the rest, sentence f.The refined ordering isa-b-e-c-d-f.No precedent sentences beforesentence b.
Choose b.There are precedent sentencesbefore sentence c.Search a shortest path from cto b and a.
We found sentencee to be the closest to theprecedent sentences of c.Search a shortest path from eto b and a.
No precedentsentences before e. Choose e.We find a path from c to b anda via e is the shortest.There are precedent sentencesbefore sentence d.Search a shortest path from dto c, e, b and a.
We find thedirect path from d to c is theshortest.00.2.7.6 100.4.8(1)(1)(2)(3)(3)(4)(5)(6)(7)(8)(8)(6)(7)(5)(4)(2)Figure 4: Ordering refinement by precedence relation as a shortest path problem.Putting sentence c before b, we finally get therefined ordering [a-c-b].Supposing that sentence c mentions similarinformation as c?
but expresses more than c?,it is nothing unusual that an extraction methoddoes not choose sentence c?
but sentence c.Because a method for multi-document summa-rization (e.g., MMR (Carbonell and Goldstein,1998)) makes effort to acquire information cov-erage and refuse redundant information at thesame time, it is quite natural that the methoddoes not choose both sentence c?
and c in termsof redundancy and prefers sentence c as c?
interms of information coverage.Figure 4 illustrates how the algorithm refinesa given chronological ordering [a-b-c-d-e-f].We define distance as a dissimilarity value ofprecedent information of a sentence.
Whena sentence has antecedent sentences and theircontent is not mentioned by previously arrangedsentences, this distance will be high.
When asentence has no precedent sentences, we definethe distance to be 0.
In the example shownin Figure 4 example we do not change posi-tion of sentences a and b because they do nothave precedent sentences (i.e., they are lead sen-tences).
On the other hand, sentence c hassome precedent sentences in its original docu-ment.
Preparing a term vector of the precedentsentences, we calculate how much the precedentcontent is covered by other sentences using dis-tance defined above.
In Figure 4 example thedistance from sentence a and b to c is high(distance = 0.7).
We search a shortest pathfrom sentence c to sentences a and b by best-first search in order to find suitable sentencesbefore sentence c. Given that sentence e in Fig-ure 4 describes similar content as the precedentsentences of sentence c and is a lead sentence,we trace the shortest path from sentence c tosentences a and b via sentence e. We extendthe resultant ordering to [a-b-e-c], insertingsentence e before sentence c. Then we considersentence d, which is not a lead sentence again(distance = 0.4).
Preparing a term vector of theprecedent sentences of sentence d, we search ashortest path from sentence d to sentences a,b, c, and e. The search result shows that weshould leave sentence d this time because theprecedent content seems to be described in sen-tences a, b, c, and e better than f. In this waywe get the final ordering, [a-b-e-c-d-f].3 EvaluationIn this section we describe our experiment totest the effectiveness of the proposed method.3.1 Experiment and evaluation metricsWe conducted an experiment of sentence order-ing through multi-document summarization totest the effectiveness of the proposed method.We utilized the TSC-3 (Hirao et al, to appear in2004) test collection, which consists of 30 sets ofmulti-document summarization tasks.
For moreinformation about TSC-3 task, see the work-shop proceedings.
Performing an importantsentence extraction (Okazaki et al, to appearin 2004) up to the specified number of sentences(approximately 10% of summarization rate), wemade a material for a summary (i.e., extractedsentences) for each task.
We order the sentencesby six methods: human-made ordering (HO) asthe highest anchor; random ordering (RO) asthe lowest anchor; chronological ordering (CO)(i.e., phase 2 only); chronological ordering withtopical segmentation (COT) (i.e., phases 1 and2); proposed method without topical segmenta-tion (PO) (i.e., phases 2 and 3); and proposedmethod with topical segmentation (POT)).
Weasked human judges to evaluate sentence order-ing of these summaries.The first evaluation task is a subjective grad-ing where a human judge marks an ordering ofsummary sentences on a scale of 4: 4 (perfect), 3(acceptable), 2 (poor), and 1 (unacceptable).
Wegive a clear criterion of scoring to the judges asfollows.
A perfect summary is a text that wecannot improve any further by re-ordering.
Anacceptable summary is a one that makes senseand is unnecessary to be revised even thoughthere may be some room for improvement interms of readability.
A poor summary is a onethat loses a thread of the story at some placesand requires minor amendment to bring it up tothe acceptable level.
An unacceptable summaryis a one that leaves much to be improved andrequires overall restructuring rather than par-tial revision.
Additionally, we inform the judgesthat summaries were made of the same set ofextracted sentences and only sentence orderingmade differences between the summaries in or-der to avoid any disturbance in rating.In addition to the rating, it is useful that weexamine how close an ordering is to an accept-able one when the ordering is regarded as poor.Considering that several sentence-ordering pat-terns are acceptable for a given summary, weAn ordering to evaluate:The corrected ordering:s5, s6, s7, s8, s1, s2, s9, s3, s4s5, s6, s7, s9, s2, s8, s1, s3, s4( ))(Correction by move operationA judge is supposed to show how to improve an ordering.The judge's reading is interupted before the points marked with black circles.Figure 5: Correction of an ordering.think that it is valuable to measure the degree ofcorrection because this metric virtually requiresa human corrector to prepare a correct answerfor each ordering in his or her mind.
Therefore,a human judge is supposed to illustrate how toimprove an ordering of a summary when he orshe marks the summary with poor in the rat-ing task.
We restrict applicable operations ofcorrection to move operation so as to keep theminimum correction of the ordering.
We definea move operation here as removing a sentenceand inserting the sentence into an appropriateplace (see Figure 5).Supposing a sentence ordering to be a rank,we can calculate rank correlation coefficient of apermutation of an ordering pi and a permutationof the reference ordering ?.
Let {s1, ..., sn} be aset of summary sentences identified with indexnumbers from 1 to n. We define a permutationpi ?
Sn to denote an ordering of sentences wherepi(i) represents an order of sentence si.
Simi-larly, we define a permutation ?
?
Sn to denotethe corrected ordering.
For example, the pi and?
in Figure 5 will be:pi =( 1 2 3 4 5 6 7 8 95 6 8 9 1 2 3 4 7), (2)?
=( 1 2 3 4 5 6 7 8 97 5 8 9 1 2 3 6 4).
(3)Spearman?s rank correlation ?s(pi, ?)
andKendall?s rank correlation ?k(pi, ?)
are knownas famous rank correlation metrics.
?s(pi, ?)
= 1?
6n(n+ 1)(n?
1)n?i=1(pi(i)?
?
(i))2(4)?k(pi, ?)
= 1n(n?
1)/2 ?n?1?i=1n?j=i+1sgn(pi(j)?
pi(i)) ?
sgn(?(j)?
?
(i)), (5)4 3 2 1RO 0.0 0.0 6.0 94.0CO 13.1 22.6 63.1 1.2COT 10.7 22.6 61.9 4.8PO 16.7 38.1 45.2 0.0POT 15.5 36.9 44.0 3.6HO 52.4 21.4 26.2 0.0Table 1: Distribution of rating score of order-ings in percent figures.where sgn(x) = 1 for x > 0 and ?1 otherwise.These metrics range from ?1 (an inverse rank)to 1 (an identical rank) via 0 (a non-correlatedrank).
In the example shown in Equations 2 and3 we obtain ?s(pi, ?)
= 0.85 and ?k(pi, ?)
= 0.72.We propose another metric to assess the de-gree of sentence continuity in reading, ?c(pi, ?
):?c(pi, ?)
= 1nn?i=1eq(pi?
?1(i), pi??1(i?
1) + 1),(6)where: pi(0) = ?
(0) = 0; eq(x, y) = 1 when xequals y and 0 otherwise.
This metric rangesfrom 0 (no continuity) to 1 (identical).
Thesummary in Figure 5 may interrupt judge?sreading after sentence S7, S1, S2 and S9 as heor she searches a next sentence to read.
Hence,we observe four discontinuities in the order-ing and calculate sentence continuity ?c(pi, ?)
=(9?
4)/9 = 0.56.3.2 ResultsTable 1 shows distribution of rating score ofeach method in percent figures.
Judges markedabout 75% of human-made ordering (HO) as ei-ther perfect or acceptable while they rejected asmany as 95% of random ordering (RO).
Chrono-logical ordering (CO) did not yield satisfactoryresult losing a thread of 63% summaries al-though CO performed much better than RO.Topical segmentation could not contribute toordering improvement of CO as well: COT isslightly worse than CO. After taking an in-depth look at the failure orderings, we foundthe topical clustering did not perform well dur-ing this test.
We suppose the topical clusteringcould not prove the merits with this test collec-tion because the collection consists of relevantarticles retrieved by some query and polishedwell by a human so as not to include unrelatedarticles to a topic.On the other hand, the proposed method(PO) improved chronological ordering muchbetter than topical segmentation.
Note that thesum of perfect and acceptable ratio jumped upfrom 36% (CO) to 55% (PO).
This shows theordering refinement by precedence relation im-proves chronological ordering by pushing poorordering to an acceptable level.Table 2 reports closeness of orderings to thecorrected ones with average scores (AVG) andthe standard deviations (SD) of the three met-rics ?s, ?k and ?c.
It appears that average figuresshows similar tendency to the rating task withthree measures: HO is the best; PO is betterthan CO; and RO is definitely the worst.
Weapplied one-way analysis of variance (ANOVA)to test the effect of four different methods (RO,CO, PO and HO).
ANOVA proved the effect ofthe different methods (p < 0.01) for three met-rics.
We also applied Tukey test to compare thedifference between these methods.
Tukey testrevealed that RO was definitely the worst withall metrics.
However, Spearman?s rank correla-tion ?S and Kendall?s rank correlation ?k failedto prove the significant difference between CO,PO and HO.
Only sentence continuity ?c provedPO is better than CO; and HO is better thanCO (?
= 0.05).
The Tukey test proved thatsentence continuity has better conformity to therating results and higher discrimination to makea comparison.Table 3 shows closeness of orderings to onesmade by human (all results of HO should be 1by necessity).
Although we found RO is clearlythe worst as well as other results, we cannot findthe significant difference between CO, PO, andHO with all metrics.
This result presents to thedifficulty of automatic evaluation by preparingone correct ordering.4 ConclusionsIn this paper we described our approach to co-herent sentence ordering for summarizing news-paper articles.
We conducted an experimentof sentence ordering through multi-documentsummarization.
The proposed method whichutilizes precedence relation of sentence archivedgood results, raising poor chronological order-ings to an acceptable level by 20%.
We also pro-posed an evaluation metric that measures sen-tence continuity and a amendment-based eval-uation task.
The amendment-based evalua-tion outperformed the evaluation that comparesan ordering with an answer made by a hu-man.
The sentence continuity metric applied tothe amendment-based task showed more agree-Spearman Kendall ContinuityMethod AVG SD AVG SD AVG SDRO 0.041 0.170 0.035 0.152 0.018 0.091CO 0.838 0.185 0.870 0.270 0.775 0.210COT 0.847 0.164 0.791 0.440 0.741 0.252PO 0.843 0.180 0.921 0.144 0.856 0.180POT 0.851 0.158 0.842 0.387 0.820 0.240HO 0.949 0.157 0.947 0.138 0.922 0.138Table 2: Comparison with corrected ordering.Spearman Kendall ContinuityMethod AVG SD AVG SD AVG SDRO -0.117 0.265 -0.073 0.202 0.054 0.064CO 0.838 0.185 0.778 0.198 0.578 0.218COT 0.847 0.164 0.782 0.186 0.571 0.229PO 0.843 0.180 0.792 0.184 0.606 0.225POT 0.851 0.158 0.797 0.171 0.599 0.237HO 1.000 0.000 1.000 0.000 1.000 0.000Table 3: Comparison with human-made ordering.ments with the rating result.We plan to do further study on the sentenceordering problem in future work, exploring howto apply our algorithm to documents other thannewspaper or integrate ordering problem withextraction problem to improve each other.
Wealso recognize the necessity to establish an auto-matic evaluation method of sentence ordering.AcknowledgmentsWe made use of Mainichi Newspaper and Yomi-uri Newspaper articles and summarization testcollection of TSC-3.ReferencesR.
Barzilay, E. Elhadad, and K. McKeown.2002.
Inferring strategies for sentence order-ing in multidocument summarization.
Jour-nal of Artifical Intelligence Research (JAIR),17:35?55.J.
Carbonell and J. Goldstein.
1998.
The use ofMMR, diversity-based reranking for reorder-ing documents and producing summaries.In Proceedings of the 21st Annual Interna-tional ACM-SIGIR Conference on Researchand Development in Information Retrieval,pages 335?336.T.
M. Cover and P. E. Hart.
1967.
Nearestneighbor pattern classification.
IEEE Trans-actions on Information Theory, IT-13:21?27.T.
Hirao, T. Fukusima, M. Okumura, andH.
Nanba.
to appear in 2004.
Text summa-rization challenge 3: text summarization eval-uation at ntcir workshop4.
In Working noteof the 4th NTCIR Workshop Meeting.J.
Hobbs.
1990.
Literature and Cognition, CSLILecture Notes 21.
CSLI.D.
Hume.
1748.
Philosophical Essays concern-ing Human Understanding.M.
Lapata.
2003.
Probabilistic text structur-ing: experiments with sentence ordering.
InProceedings of the 41st Meeting of the Asso-ciation of Computational Linguistics, pages545?552.H.
P. Luhn.
1958.
The automatic creation ofliterature abstracts.
IBM Journal of Researchand Development, 2(2):159?165.I.
Mani and G. Wilson.
2000.
Robust temporalprocessing of news.
In Proceedings of the 38thAnnual Meeting of ACL?2000, pages 69?76.I.
Mani, B. Schiffman, and J. Zhang.
2003.
In-ferring temporal ordering of events in news.Proceedings of the Human Language Technol-ogy Conference (HLT-NAACL) ?03.I.
Mani.
2001.
Audomatic Summarization.John Benjamins.W.
Mann and S. Thompson.
1988.
Rhetoricalstructure theory: Toward a functional theoryof text organization.
Text, 8:243?281.N.
Okazaki, Y. Matsuo, and M. Ishizuka.
toappear in 2004.
TISS: An integrated summa-rization system for TSC-3.
In Working noteof the 4th NTCIR Workshop Meeting.
