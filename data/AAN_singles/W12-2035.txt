The 7th Workshop on the Innovative Use of NLP for Building Educational Applications, pages 295?301,Montre?al, Canada, June 3-8, 2012. c?2012 Association for Computational LinguisticsHelping Our Own: NTHU NLPLAB System DescriptionJian-Cheng Wu+, Joseph Z. Chang*, Yi-Chun Chen+, Shih-Ting Huang+, Mei-Hua Chen*,Jason S. Chang+* Institute of Information Systems and Applications, NTHU, HsinChu, Taiwan, R.O.C.
30013+ Department of Computer Science, NTHU, HsinChu, Taiwan, R.O.C.
30013{wujc86, bizkit.tw, pieyaaa, koromiko1104, chen.meihua,jason.jschang}@gmail.comAbstractGrammatical error correction has been an activeresearch area in the field of Natural LanguageProcessing.
In this paper, we integrated fourdistinct learning-based modules to correctdeterminer and preposition errors in leaners?writing.
Each module focuses on a particulartype of error.
Our modules were tested inwell-formed data and learners?
writing.
Theresults show that our system achieves highrecall while preserves satisfactory precision.1.
IntroductionResearchers have demonstrated that prepositionsand determiners are the two most frequent errortypes for language learners (Leacock et al 2010).According to Swan and Smith (2001), prepositionerrors might result from L1 interference.
Chen andLin (2011) also reveal that prepositions are themost perplexing problem for Chinese-speakingEFL learners mainly because there are no clearpreposition counterparts in Chinese for learners torefer to.
On the other hand, Swan and Smith (2001)predict that the possibility of determiner errorsdepends on learners?
native language.
TheCambridge Learners Corpus illustrates thatlearners of Chinese, Japanese, Korean, and Russianmight have a poor command of determiners.In view of the fact that a large number ofgrammatical errors appear in non-native speakers?writing, more and more research has been directedtowards the automated detection and correction ofsuch errors to help improve the quality of thatwriting (Dale and Kilgarriff, 2010).
In recent years,preposition error detection and correction hasespecially been an area of increasingly activeresearch (Leacock et al 2010).
The HOO 2012shared task also focuses on error detection andcorrection in the use of prepositions anddeterminers (Dale et al, 2012).Many studies have been done at correctingerrors using hybrid modules: implementing distinctmodules to correct errors of different types.
Inother word, instead of using a general module tocorrect any kind of errors, using different modulesto deal with different error types seems to be moreeffective and promising.
In this paper, we proposefour distinct modules to deal with four kinds ofdeterminer and preposition errors (insertingmissing determiner, replacing erroneousdeterminer, inserting missing preposition, andreplacing erroneous prepositions).
Fourlearning-based approaches are used to detect andcorrect the errors of prepositions and determiners.In this paper, we describe our methods in thenext section.
Section 3 reports the evaluationresults.
Then we conclude this paper in Section 4.2.
System Description2.1 OverviewIn this sub-section, we give a general view of oursystem.
Figure 1 shows the architecture of theintegrated error detection and error correctionsystem.
The input of the system is a sentence in alearner?s writing.
First, the data is pre-processedusing the GeniaTagger tool (Tsuruoka et al, 2005),which provides the base forms, part-of-speech tags,chunk tags and named entity tags.
The tag result of295the sample sentence ?This virus affects the defensesystem.?
is shown in Table 1.
The determiner errordetection module then directly inserts the missingdeterminers and deletes the unnecessarydeterminers.
Meanwhile, the error determiners arereplaced with predicted answers by the determinererror correction module.
After finishing thedeterminer error correction, the preposition errordetection and correction module detects andcorrects the preposition errors of the modifiedinput sentence.In the following subsections, we first introducethe training and testing of the determiner errordetection and correction modules (Section 3.2).Then in section 3.3 we focus on the training andtesting of the preposition error detection andcorrection modules.Figure 1.
System Architecture (Run-Time)Word Base form POS Chunk NEThis This DT B-NP Ovirus virus NN I-NP Oaffects affect VBZ B-VP Othe the DT B-NP Odefence defence NN I-NP Osystem system NN I-NP O. .
.
O OTable 1.
The tag result of sample sentence.2.2 DeterminersIn this section, we investigate the performance oftwo maximum entropy classifiers (Ratnaparkhi,1997), one for determining whether a noun phrasehas a determiner or not and the other for selectingthe appropriate determiner if one is needed.From the British National Corpus (BNC), weextract 22,552,979 noun phrases (NPs).
Fordetermining which features are useful for this task,all NPs are divided into two sets, 20 million casesas a training set and the others as a validation set.For the classifier (named the DetClassifierhereafter) trained for predicting whether a NP has adeterminer or not, the label set contains two labels:?Zero?
and ?DET.?
On the other hand, for theclassifier (named the SelClassifier hereafter) whichpredicts appropriate determiners, the label setcontains 9 labels: the, a, an, my, your, our, one,this, their.
(In the training data, there are 7,249,218cases with those labels.
)Both of the classifiers use contextual andsyntactic information as features to predict thelabels.
The features include single features such asthe headword of the NP, the part of speech (PoS)of the headword, the words and  PoSs in thechunks before or after the NP (pre-NP, post-NP),and all words and PoSs in the NP (excluding thedeterminer if there was one), etc.
We also combinethe single features to form more specific featuresfor better performance.At run time, the given data are also tagged andall features for each NP in the data are extractedfor classification.
For testing, all determiners at thebeginning of the NPs are ignored if they exist.
Atfirst, the DetClassifier is used to determinewhether a NP needs a determiner or not.
If theclassifier predicts that the NP should not have adeterminer but it does, there is an ?UD?
(Unnecessary determiner) type mistake.
In contrast,Preposition ErrorChoiceDeterminer ErrorDetectionDeterminerChoicePreposition ErrorDetectionInputsentenceTagger & ParserDeterminerPrepositionOutput296if the classifier predicts that the NP should have adeterminer but it does not, there is a ?MD?
typemistake.
For both ?MD?
(Missing determiner) and?RD?
(Replace determiner) mistake types, wewould use the SelClassifier to predict whichdeterminer is more appropriate for the given NP.2.3 Prepositions2.3.1 Preposition Error DetectionIn solving other problems in natural languageprocessing, supervised training methods suffersfrom the difficulty of acquiring manually labeleddata.
This may not be the case with grammaticallanguage error correction.
Although high qualityerror learner?s corpora are not currently availableto the public to provide negative cases, anyordinary corpus can used as positive cases attraining time.In our method, we use an ordinary corpus totrain a Conditional Random Field (CRF) tagger toidentify the presence of a targeted lexical category.The input of the tagger is a sentence with all wordsin the targeting lexical category removed.
Thetagger will tag every word with a positive ornegative tag, predicting the presence of a word inthe targeted lexical category.
In this paper, wechoose the top 13 most frequent prepositions: of, to,in, for, on, with, as, at, by, from, about, like, since.Conditional Random FieldThe sequence labeling is the task of assigninglabels from a finite set of categories sequentially toa set of observation sequences.
This problem isencountered not only in the field of computationallinguistics, but also many others, includingbioinformatics, speech recognition, and patternrecognition.Traditionally sequence labeling problems aresolved using the Hidden Markov Model (HMM).HMM is a directed graph model in which everyoutcome is conditioned on the correspondingobservation node and only the previous outcomes.Conditional Random Field (CRF) is consideredthe state-of-the-art sequence labeling algorithm.One of the major differences of CRF is that it ismodeled as a undirected graph.
CRF also obeys theMarkov property, with respect to the undirectedgraph, every outcome is conditioned on itsneighboring outcomes and potentially the entireobservation sequence.Figure 2.
Simplified view of HMM and CRFSupervised TrainingObtaining labeled training data is relatively easyfor this task, that is, it requires no human labeler.For this task, we will use this method to target thelexical category preposition.
To produce trainingdata, we simply use an ordinary English corpusand use the presence of prepositions as theoutcome, and remove all prepositions.
For example,the sentence?Miss Hardbroom ?s eyes bored into Mildredlike    a    laser-beam    the    momentthey    came into view .
?will produce?Miss _Hardbroom _?s _eyes _bored +Mildred_like _a _laser-beam _the _moment _they_came  +view .
?where the underscores indicate no prepositionpresence and the plus signs indicate otherwise.Combined with additional features described infollowing sections, we use the CRF model to traina preposition presence detection tagger.
Featuresadditional to the words in the sentence are theircorresponding lemmas, part-of-speech tags, upperor lower case, and word suffix.At runtime, we first remove all prepositionalwords in the user input sentence, generateadditional features, and use the trained tagger topredict the presence of prepositions in the alteredsentence.
By comparing the tagged result with theoriginal sentence, the system can output insertionand/or deletion of preposition suggestions.The process of generating features is identical toproducing the training set.
To generate297part-of-speech tag features at runtime, one simpleapproach is to use an ordinary POS tagger togenerate POS tags to the tokens in the alteredsentences, i.e.
English sentences without anyprepositions.
A more sophisticated approach is totrain a specialized POS tagger to tag Englishsentences with their prepositions removed.
Astate-of-the-art part-of-speech tagger can achievearound 95% precision.
In our implementation, wefind that using an ordinary POS tagger to tagaltered sentences yield near 94% precision,whereas a specialized POS tagger performedaround 1% higher precision.We used a small portion of the British NationalCorpus (BNC) to train and evaluate our tagger (1Mand 10M tokens, i.e.
words and punctuation marks).The British National Corpus contains over 100million words of both written (90%) and spoken(10%) British English.
The written part of the BNCis sampled from a wide variety of sources,including newspapers, journals, academic books,fictions, letter, school and university essays.
Aseparate portion of the BNC is selected to evaluatethe performance of the taggers.
The test setcontains 322,997 tokens (31,916 sentences).2.3.2 Preposition Error CorrectionRecently, the problem of preposition errorcorrection has been viewed as a word sensedisambiguation problem and all prepositions areconsidered as candidates of the intended senses.
Inprevious studies, well-formed corpora and learnercorpora are both used in training the classifiers.However, due to the limited size of learner corpora,it is difficult to use the learner corpora to train aclassifier.
A more feasible approach is to use alarge well-formed corpus to train a model inchoosing prepositions.
Similar to the determinererror correction, we choose the maximum entropymodel as our classifier to choose appropriateprepositions underlying certain contexts.
In orderto cover a large variety of genres in learners?writing, we use a balanced well-formed corpus, theBNC, to train a maximum entropy model.Our context features include four featurecategories which are introduced as follows.?
Word feature (f1): Word features include awindow of five content words to the left andright with their positions.?
Head feature (f2): We select two head wordsin the left and right of prepositions with theirrelative orders as head features.
For example,in Table 2, we select the first head word, face,with its relative order, Rh1, as one of thehead features of preposition, to.
Morespecifically, ?Rh1=face?
denotes first headword, face, right of the preposition, to.?
Head combine feature (f3): Combine anytwo head features described above to get sixfeatures.
For example, L1R2 denotes twohead words surrounding the preposition.?
Phrase combine feature (f4): Combine thehead words of noun phrase and verb phrasewhere the preposition is between the phrases.For example, V_N feature denotes the headwords of verb phrase and noun phrase wherethe preposition is followed by noun phraseand is preceded by verb phrase.Word Feature(f1)Lw1=leaving, Rw1=face,Rw2= chronic, Rw3= conditionHead Feature(f2)Lh1=them, Lh2=leaving,Rh1=face, Rh2=conditionHead CombineFeature (f3)L1L2= them_leaving,L1R1= them_face,L1R2= them_condition, ?Phrase CombineFeature (f4)N_N= them_condition,V_N= leaving_condition,N_V= them_face,V_V= leaving_faceTable 2.
Features example for leaving them to face thischronic conditionAt run time, we extract the features of eachpreposition in learners?
writings and ask the modelto predict the preposition.
The preposition errordetection model described in section 2.3.1 firstremoves all prepositions from test sentences andthen marks the ?presence?
and ?absence?
labels inevery blank of a sentence.
For each blank labeled?presence?, the correction model predicts thepreposition which best fits the blank underlying thecontexts.
The correction model does not predictwhen the blanks are labeled ?absence?.
Althoughsome blanks labeled ?absence?
may stillcorrespond to prepositions, we decide to reducesome recall score to ensure the accuracy of theresults.2983.
Experimental ResultsIn this section, we present the experimental resultsof the determiner and preposition modulesrespectively.3.1 DeterminersTable 3 shows the performance of theDetClassifier of individual feature and Table 4shows the performance of the SelClassifier.
Wealso wonder how the size of training datainfluences the performance of the models.
Table 5and 6 show the precision of modes of differentsizes of training data with the best feature ?wholewords in NP and last word of pre-NP.?
Because theperformance converges while using more than 5million training cases, we use only 1 milliontraining cases to investigate the performance ofusing multiple features.
When using all features,the precision increases from 84.8% to 85.8% forDetClassifier, and from 39.8% to 56.0% forSelClassifier.We also implement another data-driven modelfor determiner selection (including zero) by usingthe 5gram of Web 1T corpus.
The basic concept ofthe model is to use the frequency of determinerswhich fit the context of the given test data tochoose the determiner candidates.
If the frequencyof the determiner using in the given NP is lowerthan other candidate determiners, we would use themost frequent one as the suggestion.
However,according to our observation during testing, wefind that the model tends to cause false alarms.
Toreduce the probability of false alarm, we set a highthreshold for the ratio f1/f2 where f1 is the frequencyof the used determiner and f2 is the frequency ofthe most frequent determiner.
The suggestion isaccepted only when the ratio exceeds the threshold.The major limitation of the proposed method isthat some errors are ignored due to parsing errors.For example, the given data ?the them?
should beconsidered as one NP with the ?UD?
type error.However, the parser would give the chunk result?the [B-NP] them [B-NP]?
and the error would notbe recognized.
It might need some rules to handlethese exceptions.
Another weakness of theproposed methods is that the less frequently useddeterminers are usually considered as errors andsuggested to be replaced with more frequently usedones.
For example, possessives such as ?my?and ?your?, are usually replaced with ?the.?
Weneed to integrate more informative features toimprove performance.Features Precisionhead/PoS 79.1%word/PoS of pre-NP 70.0%word/PoS of all words in NP 85.9%PoS of all words in NP 77.8%word/PoS of post-NP 71.8%whole words in NP 87.2%last word/PoS of pre-NP and head/PoS 92.3%whole words in NP and last word ofpre-NP96.8%Table 3.
Precision of features used in the DetClassifierFeatures Precisionhead/PoS 55.2%word/PoS of pre-NP 49.5%word/PoS of all words in NP 53.9%PoS of all words in NP 45.3%word/PoS of post-NP 46.1%whole words in NP 60.4%last word/PoS of pre-NP and head/PoS 65.3%whole words in NP and last word ofpre-NP70.8%Table 4.
Precision of features used in the SelClassifierSize Precision1,000,000 84.8%5,000,000 96.8%10,000,000 96.8%15,000,000 96.8%20,000,000 96.8%Table 5.
Precision of different training size for theDetClassifierSize Precision1,000,000 39.8%3,000,000 43.2%5,000,000 44.5%7,000,000 61.6%7,249,218 70.8%Table 6.
Precision of different training size for theSelClassifier2993.2 PrepositionsTwo sets of evaluation were carried out fordetection.
First, we use a randomly-selectedportion of the BNC containing 1 million tokens totrain our tokenizer targeting the 34 highestfrequency prepositions.
Second, we use a largertraining corpus containing 10 million tokens, alsorandomly selected from the BNC, and target asmaller set of the 13 highest frequencyprepositions, due to the fact that these 13prepositions can cover over 90% of the prepositionerrors found in the development set.We evaluate the trained taggers using twodifferent metrics.
First we evaluate the overalltagging precision, which is defined asPoverall   =  # of correctly tagged words  / # ofall wordsPpresence =  # correctly tagged PRESENCE / #all words labeled with PRESENCESince most answer tags are Non-presence,Poverall is not informative, we therefore focus onPpresense, and further evaluate the recall of presence,defined as:Rpresence = # correctly tagged PRESENCE  / #word should be tagged with PRESENCEWe then evaluate on Precision and Recall of thePRESENCE tag using different probabilities tothreshold the CRF tagging results.
Then we showthe result of two evaluation sets.
On the left is thetagger train with 1 million tokens, targeting 34prepositions.
On the right is the tagger trained with10 million tokens, targeting 13 prepositions.
Onlythe latter tagger is used for producing thesubmitted runs.We used the development data released as partof HOO 2012 Shared Task as the gold standard forthe evaluation of our preposition correction module.In order to observe the effect of different featuresets in training, we first extracted the MT and RTinstances marked by the gold standard and then askthe correction module to correct these prepositionsdirectly.
Table 7 shows the precision of the modelstrained on different feature sets.
The definition ofprecision is the same as the definition in the HOO2012 Shared Task.
The results shows that themodel trained using four feature sets achievedhigher precision.Features PrecisionMT RT MT+RTf1 43.62% 39.15% 40.48%f1+f2 52.58% 43.47% 46.18%f1+f2+f3 55.20% 46.77% 49.27%f1+f2+f3+f4 55.11% 47% 49.41%Table 7.
The feature selection and accuracy of thepreposition correction module.In addition to the evaluation on the effect ofdifferent feature sets, we also conducted anevaluation done on the development data of HOO2012 Shared Task to observe the performance ofthe correction model when combined with thedetection model.
The correction model correctedthree different types of preposition errors, MT, RTand MT+RT simultaneously (Table 8).MT RT MT+RTPrecision 1.16% 3.80% 4.96%Recall 29.86% 41.14% 37.79%Table 8.
Precision and recall scores of the correctionmodules when combined with the detection module.Note that when we only corrected thepreposition errors marked MT by preposition errordetection module, the precision and recall are bothlower than that of RT.
The amount of false alarminstances of detection module in MT seems to betoo high, thus in this paper, we won?t correct theinstance marked MT to insure the higher precisionof overall preposition correction.4.
ConclusionIn this paper, we integrate four learning-basedmethods in determiner and preposition errordetection and correction.
The integrated systemsimply parses and tags the test sentences and thencorrects determiners and prepositions step by step.The training of our system relies on well-formedcorpora and thus seems to be easier tore-implement it.
The large well-formed corpusmight also insure higher recall.In the future, we plan to integrate the system ina more flexible way.
The detection modules could300pass probabilities to the correction modules.
Thecorrection modules thus could decide whether tocorrect the instances or not.
In addition, we plan toreduce the false alarm rate of the detection module.Besides, a more considerable evaluation would beconducted in the near future.AcknowledgementsWe would acknowledge the funding supportfrom the Project (NSC 100-2627-E-007-001)and the help of the participants.
Thanks also go tothe comments of anonymous reviewers on thispaper.ReferencesMei-Hua Chen and Maosung Lin, 2011.
Factors andAnalyses of Common Miscollocations of CollegeStudents in Taiwan.
Studies in English Language andLiterature, 28, pp.
57-72.Martin Chodorow, Joel R. Tetreault, and Na-Rae Han.2007.
Detection of grammatical errors involvingprepositions.
In Proceedings of the FourthACL-SIGSEM Workshop on Prepositions, pp.25-30.Robert Dale and Adam Kilgarriff.
2010.
Helping OurOwn: Text massaging for computational linguisticsas a new shared task.
In Proceedings of the 6thInternational Natural Language GenerationConference, pp.
261?266.Robert Dale, Ilya Anisimoff and George Narroway(2012) HOO 2012: A Report on the Preposition andDeterminer Error Correction Shared Task.
InProceedings of the Seventh Workshop on InnovativeUse of NLP for Building Educational Applications.Rachele De Felice and Stephen G. Pulman.
2007.Automatically acquiring models of preposition use.In Proceedings of the Fourth ACL-SIGSEMWorkshop on Prepositions, pp.
45-50.Claudia Leacock, Martin Chodorow, Michael Gamon,and Joel Tetreault.
2010.
Automated GrammaticalError Detection for Language Learners.
SynthesisLectures on Human Language Technologies.
Morganand Claypool.Adwait Ratnaparkhi.
1997.
A linear observed timestatistical parser based on maximum entropy models.In Proceedings of the Second Conference onEmpirical Methods in Natural Language Processing,Brown University, Providence, Rhode Island.Michael Swan and Bernard Smith, editors.
LearnerEnglish: A teacher?s guide to interference and otherproblems.
Cambridge University Press, 2 edition,2001.
DOI: 10.1017/CBO9780511667121 19, 23, 91Tsuruoka Y, Tateishi Y, Kim JD, Ohta T, McNaught J,Ananiadou S, Tsujii J.
2005.
Developing a robustpart-of-speech tagger for biomedical text.
InAdvances in Informatics, 10th PanhellenicConference on Informatics; 11-13 November 2005Volos, Greece.
Springer; pp.
382-392.301
