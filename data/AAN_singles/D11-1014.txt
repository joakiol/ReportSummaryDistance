Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 151?161,Edinburgh, Scotland, UK, July 27?31, 2011. c?2011 Association for Computational LinguisticsSemi-Supervised Recursive Autoencodersfor Predicting Sentiment DistributionsRichard Socher Jeffrey Pennington?
Eric H. Huang Andrew Y. Ng Christopher D. ManningComputer Science Department, Stanford University, Stanford, CA 94305, USA?SLAC National Accelerator Laboratory, Stanford University, Stanford, CA 94309, USArichard@socher.org {jpennin,ehhuang,ang,manning}@stanford.edu ang@cs.stanford.eduAbstractWe introduce a novel machine learning frame-work based on recursive autoencoders forsentence-level prediction of sentiment labeldistributions.
Our method learns vector spacerepresentations for multi-word phrases.
Insentiment prediction tasks these represen-tations outperform other state-of-the-art ap-proaches on commonly used datasets, such asmovie reviews, without using any pre-definedsentiment lexica or polarity shifting rules.
Wealso evaluate the model?s ability to predictsentiment distributions on a new dataset basedon confessions from the experience project.The dataset consists of personal user storiesannotated with multiple labels which, whenaggregated, form a multinomial distributionthat captures emotional reactions.
Our al-gorithm can more accurately predict distri-butions over such labels compared to severalcompetitive baselines.1 IntroductionThe ability to identify sentiments about personal ex-periences, products, movies etc.
is crucial to un-derstand user generated content in social networks,blogs or product reviews.
Detecting sentiment inthese data is a challenging task which has recentlyspawned a lot of interest (Pang and Lee, 2008).Current baseline methods often use bag-of-wordsrepresentations which cannot properly capture morecomplex linguistic phenomena in sentiment analy-sis (Pang et al, 2002).
For instance, while the twophrases ?white blood cells destroying an infection?and ?an infection destroying white blood cells?
havethe same bag-of-words representation, the former isa positive reaction while the later is very negative.More advanced methods such as (Nakagawa et al,IndicesWordsSemantic RepresentationsRecursive Autoencoderi         walked      into         a        parked     carSorry, Hugs      You Rock       Teehee    I Understand    Wow, Just WowPredicted Sentiment DistributionFigure 1: Illustration of our recursive autoencoder archi-tecture which learns semantic vector representations ofphrases.
Word indices (orange) are first mapped into asemantic vector space (blue).
Then they are recursivelymerged by the same autoencoder network into a fixedlength sentence representation.
The vectors at each nodeare used as features to predict a distribution over senti-ment labels.2010) that can capture such phenomena use manymanually constructed resources (sentiment lexica,parsers, polarity-shifting rules).
This limits the ap-plicability of these methods to a broader range oftasks and languages.
Lastly, almost all previouswork is based on single, positive/negative categoriesor scales such as star ratings.
Examples are moviereviews (Pang and Lee, 2005), opinions (Wiebe etal., 2005), customer reviews (Ding et al, 2008) ormultiple aspects of restaurants (Snyder and Barzilay,2007).
Such a one-dimensional scale does not accu-rately reflect the complexity of human emotions andsentiments.In this work, we seek to address three issues.
(i)Instead of using a bag-of-words representation, ourmodel exploits hierarchical structure and uses com-positional semantics to understand sentiment.
(ii)Our system can be trained both on unlabeled do-main data and on supervised sentiment data and doesnot require any language-specific sentiment lexica,151parsers, etc.
(iii) Rather than limiting sentiment toa positive/negative scale, we predict a multidimen-sional distribution over several complex, intercon-nected sentiments.We introduce an approach based on semi-supervised, recursive autoencoders (RAE) whichuse as input continuous word vectors.
Fig.
1 showsan illustration of the model which learns vector rep-resentations of phrases and full sentences as well astheir hierarchical structure from unsupervised text.We extend our model to also learn a distribution oversentiment labels at each node of the hierarchy.We evaluate our approach on several standarddatasets where we achieve state-of-the art perfor-mance.
Furthermore, we show results on the re-cently introduced experience project (EP) dataset(Potts, 2010) that captures a broader spectrum ofhuman sentiments and emotions.
The dataset con-sists of very personal confessions anonymouslymade by people on the experience project websitewww.experienceproject.com.
Confessions are la-beled with a set of five reactions by other users.
Re-action labels are you rock (expressing approvement),tehee (amusement), I understand, Sorry, hugs andWow, just wow (displaying shock).
For evaluation onthis dataset we predict both the label with the mostvotes as well as the full distribution over the senti-ment categories.
On both tasks our model outper-forms competitive baselines.
A set of over 31,000confessions as well as the code of our model areavailable at www.socher.org.After describing the model in detail, we evalu-ate it qualitatively by analyzing the learned n-gramvector representations and compare quantitativelyagainst other methods on standard datasets and theEP dataset.2 Semi-Supervised RecursiveAutoencodersOur model aims to find vector representations forvariable-sized phrases in either unsupervised orsemi-supervised training regimes.
These representa-tions can then be used for subsequent tasks.
We firstdescribe neural word representations and then pro-ceed to review a related recursive model based onautoencoders, introduce our recursive autoencoder(RAE) and describe how it can be modified to jointlylearn phrase representations, phrase structure andsentiment distributions.2.1 Neural Word RepresentationsWe represent words as continuous vectors of param-eters.
We explore two settings.
In the first settingwe simply initialize each word vector x ?
Rn bysampling it from a zero mean Gaussian distribution:x ?
N (0, ?2).
These word vectors are then stackedinto a word embedding matrix L ?
Rn?|V |, where|V | is the size of the vocabulary.
This initializationworks well in supervised settings where a networkcan subsequently modify these vectors to capturecertain label distributions.In the second setting, we pre-train the word vec-tors with an unsupervised neural language model(Bengio et al, 2003; Collobert and Weston, 2008).These models jointly learn an embedding of wordsinto a vector space and use these vectors to predicthow likely a word occurs given its context.
Afterlearning via gradient ascent the word vectors cap-ture syntactic and semantic information from theirco-occurrence statistics.In both cases we can use the resulting matrix ofword vectors L for subsequent tasks as follows.
As-sume we are given a sentence as an ordered list ofm words.
Each word has an associated vocabularyindex k into the embedding matrix which we use toretrieve the word?s vector representation.
Mathemat-ically, this look-up operation can be seen as a sim-ple projection layer where we use a binary vector bwhich is zero in all positions except at the kth index,xi = Lbk ?
Rn.
(1)In the remainder of this paper, we represent a sen-tence (or any n-gram) as an ordered list of thesevectors (x1, .
.
.
, xm).
This word representation isbetter suited to autoencoders than the binary numberrepresentations used in previous related autoencodermodels such as the recursive autoassociative mem-ory (RAAM) model (Pollack, 1990; Voegtlin andDominey, 2005) or recurrent neural networks (El-man, 1991) since sigmoid units are inherently con-tinuous.
Pollack circumvented this problem by hav-ing vocabularies with only a handful of words andby manually defining a threshold to binarize the re-sulting vectors.152x1 x3 x4x2y1=f(W(1)[x3;x4] + b)y2=f(W(1)[x2;y1] + b)y3=f(W(1)[x1;y2] + b)Figure 2: Illustration of an application of a recursive au-toencoder to a binary tree.
The nodes which are not filledare only used to compute reconstruction errors.
A stan-dard autoencoder (in box) is re-used at each node of thetree.2.2 Traditional Recursive AutoencodersThe goal of autoencoders is to learn a representationof their inputs.
In this section we describe how toobtain a reduced dimensional vector representationfor sentences.In the past autoencoders have only been used insetting where the tree structure was given a-priori.We review this setting before continuing with ourmodel which does not require a given tree structure.Fig.
2 shows an instance of a recursive autoencoder(RAE) applied to a given tree.
Assume we are givena list of word vectors x = (x1, .
.
.
, xm) as describedin the previous section as well as a binary tree struc-ture for this input in the form of branching tripletsof parents with children: (p ?
c1c2).
Each childcan be either an input word vector xi or a nontermi-nal node in the tree.
For the example in Fig.
2, wehave the following triplets: ((y1 ?
x3x4), (y2 ?x2y1), (y1 ?
x1y2)).
In order to be able to applythe same neural network to each pair of children, thehidden representations yi have to have the same di-mensionality as the xi?s.Given this tree structure, we can now compute theparent representations.
The first parent vector y1 iscomputed from the children (c1, c2) = (x3, x4):p = f(W (1)[c1; c2] + b(1)), (2)where we multiplied a matrix of parameters W (1) ?Rn?2n by the concatenation of the two children.After adding a bias term we applied an element-wise activation function such as tanh to the result-ing vector.
One way of assessing how well this n-dimensional vector represents its children is to try toreconstruct the children in a reconstruction layer:[c?1; c?2]= W (2)p+ b(2).
(3)During training, the goal is to minimize the recon-struction errors of this input pair.
For each pair, wecompute the Euclidean distance between the originalinput and its reconstruction:Erec([c1; c2]) =12????
[c1; c2]?
[c?1; c?2]???
?2 .
(4)This model of a standard autoencoder is boxed inFig.
2.
Now that we have defined how an autoen-coder can be used to compute an n-dimensional vec-tor representation (p) of two n-dimensional children(c1, c2), we can describe how such a network can beused for the rest of the tree.Essentially, the same steps repeat.
Now that y1is given, we can use Eq.
2 to compute y2 by settingthe children to be (c1, c2) = (x2, y1).
Again, aftercomputing the intermediate parent vector y2, we canassess how well this vector capture the content ofthe children by computing the reconstruction erroras in Eq.
4.
The process repeat until the full treeis constructed and we have a reconstruction error ateach nonterminal node.
This model is similar to theRAAM model (Pollack, 1990) which also requires afixed tree structure.2.3 Unsupervised Recursive Autoencoder forStructure PredictionNow, assume there is no tree structure given forthe input vectors in x.
The goal of our structure-prediction RAE is to minimize the reconstruction er-ror of all vector pairs of children in a tree.
We de-fine A(x) as the set of all possible trees that can bebuilt from an input sentence x.
Further, let T (y) bea function that returns the triplets of a tree indexedby s of all the non-terminal nodes in a tree.
Usingthe reconstruction error of Eq.
4, we computeRAE?
(x) = argminy?A(x)?s?T (y)Erec([c1; c2]s) (5)We now describe a greedy approximation that con-structs such a tree.153Greedy Unsupervised RAE.
For a sentence withm words, we apply the autoencoder recursively.
Ittakes the first pair of neighboring vectors, definesthem as potential children of a phrase (c1; c2) =(x1;x2), concatenates them and gives them as in-put to the autoencoder.
For each word pair, we savethe potential parent node p and the resulting recon-struction error.After computing the score for the first pair, thenetwork is shifted by one position and takes as inputvectors (c1, c2) = (x2, x3) and again computes a po-tential parent node and a score.
This process repeatsuntil it hits the last pair of words in the sentence:(c1, c2) = (xm?1, xm).
Next, it selects the pairwhich had the lowest reconstruction error (Erec) andits parent representation p will represent this phraseand replace both children in the sentence word list.For instance, consider the sequence (x1, x2, x3, x4)and assume the lowestErec was obtained by the pair(x3, x4).
After the first pass, the new sequence thenconsists of (x1, x2, p(3,4)).
The process repeats andtreats the new vector p(3,4) like any other input vec-tor.
For instance, subsequent states could be either:(x1, p(2,(3,4))) or (p(1,2), p(3,4)).
Both states wouldthen finish with a deterministic choice of collapsingthe remaining two states into one parent to obtain(p(1,(2,(3,4)))) or (p((1,2),(3,4))) respectively.
The treeis then recovered by unfolding the collapsing deci-sions.The resulting tree structure captures as much ofthe single-word information as possible (in orderto allow reconstructing the word vectors) but doesnot necessarily follow standard syntactic constraints.We also experimented with a method that finds bet-ter solutions to Eq.
5 based on CKY-like beamsearch algorithms (Socher et al, 2010; Socher et al,2011) but the performance is similar and the greedyversion is much faster.Weighted Reconstruction.
One problem withsimply using the reconstruction error of both chil-dren equally as describe in Eq.
4 is that each childcould represent a different number of previouslycollapsed words and is hence of bigger importancefor the overall meaning reconstruction of the sen-tence.
For instance in the case of (x1, p(2,(3,4)))one would like to give more importance to recon-structing p than x1.
We capture this desideratumby adjusting the reconstruction error.
Let n1, n2 bethe number of words underneath a current poten-tial child, we re-define the reconstruction error to beErec([c1; c2]; ?)
=n1n1 + n2???
?c1 ?
c?1???
?2 + n2n1 + n2???
?c2 ?
c?2???
?2 (6)Length Normalization.
One of the goals ofRAEs is to induce semantic vector representationsthat allow us to compare n-grams of differentlengths.
The RAE tries to lower reconstruction errorof not only the bigrams but also of nodes higher inthe tree.
Unfortunately, since the RAE computes thehidden representations it then tries to reconstruct, itcan just lower reconstruction error by making thehidden layer very small in magnitude.
To preventsuch undesirable behavior, we modify the hiddenlayer such that the resulting parent representation al-ways has length one, after computing p as in Eq.
2,we simply set: p = p||p|| .2.4 Semi-Supervised Recursive AutoencodersSo far, the RAE was completely unsupervised andinduced general representations that capture the se-mantics of multi-word phrases.In this section, weextend RAEs to a semi-supervised setting in orderto predict a sentence- or phrase-level target distribu-tion t.1One of the main advantages of the RAE is thateach node of the tree built by the RAE has associ-ated with it a distributed vector representation (theparent vector p) which could also be seen as fea-tures describing that phrase.
We can leverage thisrepresentation by adding on top of each parent nodea simple softmax layer to predict class distributions:d(p; ?)
= softmax(W labelp).
(7)Assuming there are K labels, d ?
RK isa K-dimensional multinomial distribution and?k=1 dk = 1.
Fig.
3 shows such a semi-supervisedRAE unit.
Let tk be the kth element of the multino-mial target label distribution t for one entry.
Thesoftmax layer?s outputs are interpreted as condi-tional probabilities dk = p(k|[c1; c2]), hence thecross-entropy error isEcE(p, t; ?)
= ?K?k=1tk log dk(p; ?).
(8)1For the binary label classification case, the distribution isof the form [1, 0] for class 1 and [0, 1] for class 2.154R e c o n s t r u c t i o n  e r r o r            C r o s s - e n t r o p y e r r o rW(1)W(2) W(l a be l )Figure 3: Illustration of an RAE unit at a nonterminal treenode.
Red nodes show the supervised softmax layer forlabel distribution prediction.Using this cross-entropy error for the label and thereconstruction error from Eq.
6, the final semi-supervised RAE objective over (sentences,label)pairs (x, t) in a corpus becomesJ = 1N?
(x,t)E(x, t; ?)
+ ?2 ||?||2, (9)where we have an error for each entry in the trainingset that is the sum over the error at the nodes of thetree that is constructed by the greedy RAE:E(x, t; ?)
=?s?T (RAE?
(x))E([c1; c2]s, ps, t, ?
).The error at each nonterminal node is the weightedsum of reconstruction and cross-entropy errors,E([c1; c2]s, ps, t, ?)
=?Erec([c1; c2]s; ?)
+ (1?
?
)EcE(ps, t; ?
).The hyperparameter ?
weighs reconstruction andcross-entropy error.
When minimizing the cross-entropy error of this softmax layer, the error willbackpropagate and influence both the RAE param-eters and the word representations.
Initially, wordssuch as good and bad have very similar representa-tions.
This is also the case for Brown clusters andother methods that use only cooccurrence statisticsin a small window around each word.
When learn-ing with positive/negative sentiment, the word em-beddings get modified and capture less syntactic andmore sentiment information.In order to predict the sentiment distribution of asentence with this model, we use the learned vectorrepresentation of the top tree node and train a simplelogistic regression classifier.3 LearningLet ?
= (W (1), b(1),W (2), b(1),W label, L) be the setof our model parameters, then the gradient becomes:?J??
=1N?
(x,t)?E(x, t; ?)??
+ ??.
(10)To compute this gradient, we first greedily constructall trees and then derivatives for these trees are com-puted efficiently via backpropagation through struc-ture (Goller and Ku?chler, 1996).
Because the algo-rithm is greedy and the derivatives of the supervisedcross-entropy error also modify the matrix W (1),this objective is not necessarily continuous and astep in the gradient descent direction may not nec-essarily decrease the objective.
However, we foundthat L-BFGS run over the complete training data(batch mode) to minimize the objective works wellin practice, and that convergence is smooth, with thealgorithm typically finding a good solution quickly.4 ExperimentsWe first describe the new experience project (EP)dataset, results of standard classification tasks onthis dataset and how to predict its sentiment labeldistributions.
We then show results on other com-monly used datasets and conclude with an analysisof the important parameters of the model.In all experiments involving our model, we repre-sent words using 100-dimensional word vectors.
Weexplore the two settings mentioned in Sec.
2.1.
Wecompare performance on standard datasets when us-ing randomly initialized word vectors (random wordinit.)
or word vectors trained by the model of Col-lobert and Weston (2008) and provided by Turianet al (2010).2 These vectors were trained on anunlabeled corpus of the English Wikipedia.
Notethat alternatives such as Brown clusters are not suit-able since they do not capture sentiment information(good and bad are usually in the same cluster) andcannot be modified via backpropagation.2http://metaoptimize.com/projects/wordreprs/155Corpus K Instances Distr.
(+/-) Avg|W |MPQA 2 10,624 0.31/0.69 3MR 2 10,662 0.5/0.5 22EP 5 31,675 .2/.2/.1/.4/.1 113EP?
4 5 6,129 .2/.2/.1/.4/.1 129Table 1: Statistics on the different datasets.
K is the num-ber of classes.
Distr.
is the distribution of the differentclasses (in the case of 2, the positive/negative classes, forEP the rounded distribution of total votes in each class).|W | is the average number of words per instance.
We useEP?
4, a subset of entries with at least 4 votes.4.1 EP Dataset: The Experience ProjectThe confessions section of the experience projectwebsite3 lets people anonymously write short per-sonal stories or ?confessions?.
Once a story is onthe site, each user can give a single vote to one offive label categories (with our interpretation):1 Sorry, Hugs: User offers condolences to author.2.
You Rock: Indicating approval, congratulations.3.
Teehee: User found the anecdote amusing.4.
I Understand: Show of empathy.5.
Wow, Just Wow: Expression of surprise,shock.The EP dataset has 31,676 confession entries, a to-tal number of 74,859 votes for the 5 labels above, theaverage number of votes per entry is 2.4 (with a vari-ance of 33).
For the five categories, the numbers ofvotes are [14, 816; 13, 325; 10, 073; 30, 844; 5, 801].Since an entry with less than 4 votes is not very wellidentified, we train and test only on entries with atleast 4 total votes.
There are 6,129 total such entries.The distribution over total votes in the 5 classesis similar: [0.22; 0.2; 0.11; 0.37; 0.1].
The averagelength of entries is 129 words.
Some entries con-tain multiple sentences.
In these cases, we averagethe predicted label distributions from the sentences.Table 1 shows statistics of this and other commonlyused sentiment datasets (which we compare on inlater experiments).
Table 2 shows example entriesas well as gold and predicted label distributions asdescribed in the next sections.Compared to other datasets, the EP dataset con-tains a wider range of human emotions that goes farbeyond positive/negative product or movie reviews.Each item is labeled with a multinomial distribu-3http://www.experienceproject.com/confessions.phption over interconnected response categories.
Thisis in contrast to most other datasets (including multi-aspect rating) where several distinct aspects are ratedindependently but on the same scale.
The topicsrange from generic happy statements, daily clumsi-ness reports, love, loneliness, to relationship abuseand suicidal notes.
As is evident from the total num-ber of label votes, the most common user reactionis one of empathy and an ability to relate to the au-thors experience.
However, some stories describehorrible scenarios that are not common and hencereceive more offers of condolence.
In the followingsections we show some examples of stories with pre-dicted and true distributions but refrain from listingthe most horrible experiences.For all experiments on the EP dataset, we split thedata into train (49%), development (21%) and testdata (30%).4.2 EP: Predicting the Label with Most VotesThe first task for our evaluation on the EP dataset isto simply predict the single class that receives themost votes.
In order to compare our novel jointphrase representation and classifier learning frame-work to traditional methods, we use the followingbaselines:Random Since there are five classes, this gives 20%accuracy.Most Frequent Selecting the class which most fre-quently has the most votes (the class I under-stand).Baseline 1: Binary BoW This baseline uses logis-tic regression on binary bag-of-word represen-tations that are 1 if a word is present and 0 oth-erwise.Baseline 2: Features This model is similar to tra-ditional approaches to sentiment classificationin that it uses many hand-engineered resources.We first used a spell-checker and Wordnet tomap words and their misspellings to synsets toreduce the total number of words.
We then re-placed sentiment words with a sentiment cat-egory identifier using the sentiment lexica ofthe Harvard Inquirer (Stone, 1966) and LIWC(Pennebaker et al, 2007).
Lastly, we used tf-idfweighting on the bag-of-word representationsand trained an SVM.156KL Predicted&Gold V. Entry (Shortened if it ends with ...).03.16 .16 .16 .33 .166 I reguarly shoplift.
I got caught once and went to jail, but I?ve found that this was not a deterrent.
I don?t buygroceries, I don?t buy school supplies for my kids, I don?t buy gifts for my kids, we don?t pay for movies, and Idont buy most incidentals for the house (cleaning supplies, toothpaste, etc.
)....03.38 .04 .06 .35 .14165 i am a very succesfull buissnes man.i make good money but i have been addicted to crack for 13 years.i moved 1hour away from my dealers 10 years ago to stop using now i dont use daily but once a week usally friday nights.i used to use 1 or 2 hundred a day now i use 4 or 5 hundred on a friday.my problem is i am a funcational addict....05.14 .28 .14 .28 .147 Hi there, Im a guy that loves a girl, the same old bloody story...
I met her a while ago, while studying, she Is soperfect, so mature and yet so lonely, I get to know her and she get ahold of me, by opening her life to me and sodid I with her, she has been the first person, male or female that has ever made that bond with me,....07.27 .18 .00 .45 .0911 be kissing you right now.
i should be wrapped in your arms in the dark, but instead i?ve ruined everything.
i?vepiled bricks to make a wall where there never should have been one.
i feel an ache that i shouldn?t feel becausei?ve never had you close enough.
we?ve never touched, but i still feel as though a part of me is missing.
....05 23 Dear Love, I just want to say that I am looking for you.
Tonight I felt the urge to write, and I am becoming moreand more frustrated that I have not found you yet.
I?m also tired of spending so much heart on an old dream.
....05 5 I wish I knew somone to talk to here..06 24 I loved her but I screwed it up.
Now she?s moved on.
I?ll never have her again.
I don?t know if I?ll ever stopthinking about her..06 5 i am 13 years old and i hate my father he is alwas geting drunk and do?s not care about how it affects me or mysisters i want to care but the truthis i dont care if he dies.13 6 well i think hairy women are attractive.35 5 As soon as I put clothings on I will go down to DQ and get a thin mint blizzard.
I need it.
It?ll make my soulfeel a bit better :).36 6 I am a 45 year old divoced woman, and I havent been on a date or had any significant relationship in 12years...yes, 12 yrs.
the sad thing is, Im not some dried up old granny who is no longer interested in men, I justcan?t meet men.
(before you judge, no Im not terribly picky!)
What is wrong with me?.63 6 When i was in kindergarden i used to lock myself in the closet and eat all the candy.
Then the teacher found outit was one of us and made us go two days without freetime.
It might be a little late now, but sorry guys it wasme haha.92 4 My paper is due in less than 24 hours and I?m still dancing round my room!Table 2: Example EP confessions from the test data with KL divergence between our predicted distribution (light blue,left bar on each of the 5 classes) and ground truth distribution (red bar and numbers underneath), number of votes.
The5 classes are [Sorry, Hugs ;You Rock; Teehee;I Understand;Wow, Just Wow].
Even when the KL divergence is higher,our model makes reasonable alternative label choices.
Some entries are shortened.Baseline 3: Word Vectors We can ignore the RAEtree structure and only train softmax layers di-rectly on the pre-trained words in order to influ-ence the word vectors.
This is followed by anSVM trained on the average of the word vec-tors.We also experimented with latent Dirichlet aloca-tion (Blei et al, 2003) but performance was verylow.Table 3 shows the results for predicting the classwith the most votes.
Even the approach that is basedon sentiment lexica and other resources is outper-formed by our model by almost 3%, showing thatfor tasks involving complex broad-range human sen-timent, the often used sentiment lexica lack in cover-age and traditional bag-of-words representations arenot powerful enough.4.3 EP: Predicting Sentiment DistributionsWe now turn to evaluating our distribution-prediction approach.
In both this and the previousMethod AccuracyRandom 20.0Most Frequent 38.1Baseline 1: Binary BoW 46.4Baseline 2: Features 47.0Baseline 3: Word Vectors 45.5RAE (our method) 50.1Table 3: Accuracy of predicting the class with most votes.maximum label task, we backprop using the goldmultinomial distribution as a target.
Since we max-imize likelihood and because we want to predict adistribution that is closest to the distribution of labelsthat people would assign to a story, we evaluate us-ing KL divergence: KL(g||p) = ?i gi log(gi/pi),where g is the gold distribution and p is the predictedone.
We report the average KL divergence, where asmaller value indicates better predictive power.
Toget an idea of the values of KL divergence, predict-157Avg.Distr.
BoW Features Word Vec.
RAE0.60.70.80.83 0.81 0.72 0.73 0.70Figure 4: Average KL-divergence between gold and pre-dicted sentiment distributions (lower is better).ing random distributions gives a an average of 1.2 inKL divergence, predicting simply the average distri-bution in the training data give 0.83.
Fig.
4 showsthat our RAE-based model outperforms the otherbaselines.
Table 2 shows EP example entries withpredicted and gold distributions, as well as numbersof votes.4.4 Binary Polarity ClassificationIn order to compare our approach to other meth-ods we also show results on commonly used sen-timent datasets: movie reviews4 (MR) (Pang andLee, 2005) and opinions5 (MPQA) (Wiebe et al,2005).We give statistical information on these andthe EP corpus in Table 1.We compare to the state-of-the-art system of(Nakagawa et al, 2010), a dependency tree basedclassification method that uses CRFs with hiddenvariables.
We use the same training and testing regi-men (10-fold cross validation) as well as their base-lines: majority phrase voting using sentiment andreversal lexica; rule-based reversal using a depen-dency tree; Bag-of-Features and their full Tree-CRFmodel.
As shown in Table 4, our algorithm outper-forms their approach on both datasets.
For the moviereview (MR) data set, we do not use any hand-designed lexica.
An error analysis on the MPQAdataset showed several cases of single words whichnever occurred in the training set.
Correctly classify-ing these instances can only be the result of havingthem in the original sentiment lexicon.
Hence, forthe experiment on MPQA we added the same sen-timent lexicon that (Nakagawa et al, 2010) used intheir system to our training set.
This improved ac-curacy from 86.0 to 86.4.Using the pre-trained wordvectors boosts performance by less than 1% com-4www.cs.cornell.edu/people/pabo/movie-review-data/5www.cs.pitt.edu/mpqa/Method MR MPQAVoting with two lexica 63.1 81.7Rule-based reversal on trees 62.9 82.8Bag of features with reversal 76.4 84.1Tree-CRF (Nakagawa et al?10) 77.3 86.1RAE (random word init.)
76.8 85.7RAE (our method) 77.7 86.4Table 4: Accuracy of sentiment classification on moviereview polarity (MR) and the MPQA dataset.0 0.2 0.4 0.6 0.8 10.830.840.850.860.87Figure 5: Accuracy on the development split of the MRpolarity dataset for different weightings of reconstructionerror and supervised cross-entropy error: err = ?Erec+(1?
?
)EcE .pared to randomly initialized word vectors (setting:random word init).
This shows that our method canwork well even in settings with little training data.We visualize the semantic vectors that the recursiveautoencoder learns by listing n-grams that give thehighest probability for each polarity.
Table 5 showssuch n-grams for different lengths when the RAE istrained on the movie review polarity dataset.On a 4-core machine, training time for the smallercorpora such as the movie reviews takes around 3hours and for the larger EP corpus around 12 hoursuntil convergence.
Testing of hundreds of movie re-views takes only a few seconds.4.5 Reconstruction vs.
Classification ErrorIn this experiment, we show how the hyperparame-ter ?
influences accuracy on the development set ofone of the cross-validation splits of the MR dataset.This parameter essentially trade-off the supervisedand unsupervised parts of the objective.
Fig.
5 showsthat a larger focus on the supervised objective is im-portant but that a weight of ?
= 0.2 for the recon-struction error prevents overfitting and achieves thehighest performance.158n Most negative n-grams Most positive n-grams1 bad; boring; dull; flat; pointless; tv; neither; pretentious; badly;worst; lame; mediocre; lack; routine; loud; bore; barely; stupid;tired; poorly; suffers; heavy;nor; choppy; superficialtouching; enjoyable; powerful; warm; moving; culture; flaws;provides; engrossing; wonderful; beautiful; quiet; socio-political;thoughtful; portrait; refreshingly; chilling; rich; beautifully; solid;2 how bad; by bad; dull .
; for bad; to bad; boring .
; , dull; are bad;that bad; boring ,; , flat; pointless .
; badly by; on tv; so routine; lackthe; mediocre .
; a generic; stupid ,; abysmally patheticthe beautiful; moving,; thoughtful and; , inventive; solid and; abeautiful; a beautifully; and hilarious; with dazzling; provides the;provides.
; and inventive; as powerful; moving and; a moving; apowerful3 .
too bad; exactly how bad; and never dull; shot but dull; is moreboring; to the dull; dull, UNK; it is bad; or just plain; by turnspretentious; manipulative and contrived; bag of stale; is a bad; thewhole mildly; contrived pastiche of; from this choppy; stale mate-rial.funny and touching; a small gem; with a moving; cuts, fast; , finemusic; smart and taut; culture into a; romantic , riveting; ... a solid;beautifully acted .
; , gradually reveals; with the chilling; cast ofsolid; has a solid; spare yet audacious; ... a polished; both thebeauty;5 boring than anything else.
; a major waste ... generic; nothing ihadn?t already; ,UNK plotting;superficial; problem ?
no laughs.
;,just horribly mediocre .
; dull, UNK feel.
; there?s nothing exactlywrong; movie is about a boring; essentially a collection of bitsreminded us that a feel-good; engrossing, seldom UNK,; betweenrealistic characters showing honest; a solid piece of journalistic;easily the most thoughtful fictional; cute, funny, heartwarming;with wry humor and genuine; engrossing and ultimately tragic.
;8 loud, silly, stupid and pointless.
; dull, dumb and derivative horrorfilm.
; UNK?s film, a boring, pretentious; this film biggest problem?
no laughs.
; film in the series looks and feels tired; do draw easychuckles but lead nowhere.
; stupid, infantile, redundant, sloppyshot in rich , shadowy black-and-white , devils an escapist con-fection that ?s pure entertainment .
; , deeply absorbing piece thatworks as a; ... one of the most ingenious and entertaining; film is ariveting , brisk delight .
; bringing richer meaning to the story ?s;Table 5: Examples of n-grams (n = 1, 2, 3, 5, 8) from the test data of the movie polarity dataset for which our modelpredicts the most positive and most negative responses.5 Related Work5.1 Autoencoders and Deep LearningAutoencoders are neural networks that learn a re-duced dimensional representation of fixed-size in-puts such as image patches or bag-of-word repre-sentations of text documents.
They can be used toefficiently learn feature encodings which are usefulfor classification.
Recently, Mirowski et al (2010)learn dynamic autoencoders for documents in a bag-of-words format which, like ours, combine super-vised and reconstruction objectives.The idea of applying an autoencoder in a recursivesetting was introduced by Pollack (1990).
Pollack?srecursive auto-associative memories (RAAMs) aresimilar to ours in that they are a connectionst, feed-forward model.
However, RAAMs learn vectorrepresentations only for fixed recursive data struc-tures, whereas our RAE builds this recursive datastructure.
More recently, (Voegtlin and Dominey,2005) introduced a linear modification to RAAMsthat is able to better generalize to novel combina-tions of previously seen constituents.
One of themajor shortcomings of previous applications of re-cursive autoencoders to natural language sentenceswas their binary word representation as discussed inSec.
2.1.Recently, (Socher et al, 2010; Socher et al, 2011)introduced a max-margin framework based on recur-sive neural networks (RNNs) for labeled structureprediction.
Their models are applicable to naturallanguage and computer vision tasks such as parsingor object detection.
The current work is related inthat it uses a recursive deep learning model.
How-ever, RNNs require labeled tree structures and use asupervised score at each node.
Instead, RAEs learnhierarchical structures that are trying to capture asmuch of the the original word vectors as possible.The learned structures are not necessarily syntacti-cally plausible but can capture more of the semanticcontent of the word vectors.
Other recent deep learn-ing methods for sentiment analysis include (Maas etal., 2011).5.2 Sentiment AnalysisPang et al (2002) were one of the first to experimentwith sentiment classification.
They show that sim-ple bag-of-words approaches based on Naive Bayes,MaxEnt models or SVMs are often insufficient forpredicting sentiment of documents even though theywork well for general topic-based document classi-fication.
Even adding specific negation words, bi-grams or part-of-speech information to these mod-els did not add significant improvements.
Otherdocument-level sentiment work includes (Turney,2002; Dave et al, 2003; Beineke et al, 2004; Pangand Lee, 2004).
For further references, see (Pangand Lee, 2008).Instead of document level sentiment classifica-tion, (Wilson et al, 2005) analyze the contextualpolarity of phrases and incorporate many well de-signed features including dependency trees.
Theyalso show improvements by first distinguishing be-159tween neutral and polar sentences.
Our model natu-rally incorporates the recursive interaction betweencontext and polarity words in sentences in a unifiedframework while simultaneously learning the neces-sary features to make accurate predictions.
Other ap-proaches for sentence-level sentiment detection in-clude (Yu and Hatzivassiloglou, 2003; Grefenstetteet al, 2004; Ikeda et al, 2008).Most previous work is centered around a givensentiment lexicon or building one via heuristics(Kim and Hovy, 2007; Esuli and Sebastiani, 2007),manual annotation (Das and Chen, 2001) or machinelearning techniques (Turney, 2002).
In contrast, wedo not require an initial or constructed sentiment lex-icon of positive and negative words.
In fact, whentraining our approach on documents or sentences, itjointly learns such lexica for both single words andn-grams (see Table 5).
(Mao and Lebanon, 2007)propose isotonic conditional random fields and dif-ferentiate between local, sentence-level and global,document-level sentiment.The work of (Polanyi and Zaenen, 2006; Choi andCardie, 2008) focuses on manually constructing sev-eral lexica and rules for both polar words and re-lated content-word negators, such as ?prevent can-cer?, where prevent reverses the negative polarity ofcancer.
Like our approach they capture composi-tional semantics.
However, our model does so with-out manually constructing any rules or lexica.Recently, (Velikovich et al, 2010) showed how touse a seed lexicon and a graph propagation frame-work to learn a larger sentiment lexicon that also in-cludes polar multi-word phrases such as ?once in alife time?.
While our method can also learn multi-word phrases it does not require a seed set or a largeweb graph.
(Nakagawa et al, 2010) introduced anapproach based on CRFs with hidden variables withvery good performance.
We compare to their state-of-the-art system.
We outperform them on the stan-dard corpora that we tested on without requiringexternal systems such as POS taggers, dependencyparsers and sentiment lexica.
Our approach jointlylearns the necessary features and tree structure.In multi-aspect rating (Snyder and Barzilay, 2007)one finds several distinct aspects such as food or ser-vice in a restaurant and then rates them on a fixedlinear scale such as 1-5 stars, where all aspects couldobtain just 1 star or all aspects could obtain 5 starsindependently.
In contrast, in our method a singleaspect (a complex reaction to a human experience)is predicted not in terms of a fixed scale but in termsof a multinomial distribution over several intercon-nected, sometimes mutually exclusive emotions.
Asingle story cannot simultaneously obtain a strongreaction in different emotional responses (by virtueof having to sum to one).6 ConclusionWe presented a novel algorithm that can accuratelypredict sentence-level sentiment distributions.
With-out using any hand-engineered resources such assentiment lexica, parsers or sentiment shifting rules,our model achieves state-of-the-art performance oncommonly used sentiment datasets.
Furthermore,we introduce a new dataset that contains distribu-tions over a broad range of human emotions.
Ourevaluation shows that our model can more accu-rately predict these distributions than other models.AcknowledgmentsWe gratefully acknowledge the support of the DefenseAdvanced Research Projects Agency (DARPA) MachineReading Program under Air Force Research Laboratory(AFRL) prime contract no.
FA8750-09-C-0181.
Anyopinions, findings, and conclusion or recommendationsexpressed in this material are those of the author(s) anddo not necessarily reflect the view of DARPA, AFRL, orthe US government.
This work was also supported in partby the DARPA Deep Learning program under contractnumber FA8650-10-C-7020.We thank Chris Potts for help with the EP data set, Ray-mond Hsu, Bozhi See, and Alan Wu for letting us usetheir system as a baseline and Jiquan Ngiam, Quoc Le,Gabor Angeli and Andrew Maas for their feedback.ReferencesP.
Beineke, T. Hastie, C. D. Manning, andS.
Vaithyanathan.
2004.
Exploring sentimentsummarization.
In Proceedings of the AAAI SpringSymposium on Exploring Attitude and Affect in Text:Theories and Applications.Y.
Bengio, R. Ducharme, P. Vincent, and C. Janvin.
2003.A neural probabilistic language model.
Journal of Ma-chine Learning Research, 3:1137?1155.D.
M. Blei, A. Y. Ng, and M. I. Jordan.
2003.
Latentdirichlet alocation.
Journal of Machine Learning Re-search., 3:993?1022.160Y.
Choi and C. Cardie.
2008.
Learning with composi-tional semantics as structural inference for subsenten-tial sentiment analysis.
In EMNLP.R.
Collobert and J. Weston.
2008.
A unified archi-tecture for natural language processing: deep neuralnetworks with multitask learning.
In Proceedings ofICML, pages 160?167.S.
Das and M. Chen.
2001.
Yahoo!
for Amazon: Ex-tracting market sentiment from stock message boards.In Proceedings of the Asia Pacific Finance AssociationAnnual Conference (APFA).K.
Dave, S. Lawrence, and D. M. Pennock.
2003.
Min-ing the peanut gallery: Opinion extraction and seman-tic classification of product reviews.
In Proceedings ofWWW, pages 519?528.X.
Ding, B. Liu, and P. S. Yu.
2008.
A holistic lexicon-based approach to opinion mining.
In Proceedings ofthe Conference on Web Search and Web Data Mining(WSDM).J.
L. Elman.
1991.
Distributed representations, simplerecurrent networks, and grammatical structure.
Ma-chine Learning, 7(2-3):195?225.A.
Esuli and F. Sebastiani.
2007.
Pageranking word-net synsets: An application to opinion mining.
InProceedings of the Association for Computational Lin-guistics (ACL).C.
Goller and A. Ku?chler.
1996.
Learning task-dependent distributed representations by backpropaga-tion through structure.
In Proceedings of the Interna-tional Conference on Neural Networks (ICNN-96).G.
Grefenstette, Y. Qu, J. G. Shanahan, and D. A. Evans.2004.
Coupling niche browsers and affect analysisfor an opinion mining application.
In Proceedingsof Recherche d?Information Assiste?e par Ordinateur(RIAO).D.
Ikeda, H. Takamura, L. Ratinov, and M. Okumura.2008.
Learning to shift the polarity of words for senti-ment classification.
In IJCNLP.S.
Kim and E. Hovy.
2007.
Crystal: Analyzing predic-tive opinions on the web.
In EMNLP-CoNLL.A.
L. Maas, R. E. Daly, P. T. Pham, D. Huang, A. Y. Ng,and C. Potts.
2011.
Learning accurate, compact, andinterpretable tree annotation.
In Proceedings of ACL.Y.
Mao and G. Lebanon.
2007.
Isotonic ConditionalRandom Fields and Local Sentiment Flow.
In NIPS.P.
Mirowski, M. Ranzato, and Y. LeCun.
2010.
Dynamicauto-encoders for semantic indexing.
In Proceedingsof the NIPS 2010 Workshop on Deep Learning.T.
Nakagawa, K. Inui, and S. Kurohashi.
2010.
Depen-dency tree-based sentiment classification using CRFswith hidden variables.
In NAACL, HLT.B.
Pang and L. Lee.
2004.
A sentimental education:Sentiment analysis using subjectivity summarizationbased on minimum cuts.
In ACL.B.
Pang and L. Lee.
2005.
Seeing stars: Exploiting classrelationships for sentiment categorization with respectto rating scales.
In ACL, pages 115?124.B.
Pang and L. Lee.
2008.
Opinion mining and senti-ment analysis.
Foundations and Trends in InformationRetrieval, 2(1-2):1?135.B.
Pang, L. Lee, and S. Vaithyanathan.
2002.
Thumbsup?
Sentiment classification using machine learningtechniques.
In EMNLP.J.
W. Pennebaker, R.J. Booth, and M. E. Francis.
2007.Linguistic inquiry and word count: Liwc2007 opera-tors manual.
University of Texas.L.
Polanyi and A. Zaenen.
2006.
Contextual valenceshifters.J.
B. Pollack.
1990.
Recursive distributed representa-tions.
Artificial Intelligence, 46:77?105, November.C.
Potts.
2010.
On the negativity of negation.
In DavidLutz and Nan Li, editors, Proceedings of Semanticsand Linguistic Theory 20.
CLC Publications, Ithaca,NY.B.
Snyder and R. Barzilay.
2007.
Multiple aspect rank-ing using the Good Grief algorithm.
In HLT-NAACL.R.
Socher, C. D. Manning, and A. Y. Ng.
2010.
Learningcontinuous phrase representations and syntactic pars-ing with recursive neural networks.
In Proceedings ofthe NIPS-2010 Deep Learning and Unsupervised Fea-ture Learning Workshop.R.
Socher, C. C. Lin, A. Y. Ng, and C. D. Manning.2011.
Parsing Natural Scenes and Natural Languagewith Recursive Neural Networks.
In ICML.P.
J.
Stone.
1966.
The General Inquirer: A ComputerApproach to Content Analysis.
The MIT Press.J.
Turian, L. Ratinov, and Y. Bengio.
2010.
Word rep-resentations: a simple and general method for semi-supervised learning.
In Proceedings of ACL, pages384?394.P.
Turney.
2002.
Thumbs up or thumbs down?
Seman-tic orientation applied to unsupervised classification ofreviews.
In ACL.L.
Velikovich, S. Blair-Goldensohn, K. Hannan, andR.
McDonald.
2010.
The viability of web-derived po-larity lexicons.
In NAACL, HLT.T.
Voegtlin and P. Dominey.
2005.
Linear Recursive Dis-tributed Representations.
Neural Networks, 18(7).J.
Wiebe, T. Wilson, and C. Cardie.
2005.
Annotating ex-pressions of opinions and emotions in language.
Lan-guage Resources and Evaluation, 39.T.
Wilson, J. Wiebe, and P. Hoffmann.
2005.
Recogniz-ing contextual polarity in phrase-level sentiment anal-ysis.
In HLT/EMNLP.H.
Yu and V. Hatzivassiloglou.
2003.
Towards answer-ing opinion questions: Separating facts from opinionsand identifying the polarity of opinion sentences.
InEMNLP.161
