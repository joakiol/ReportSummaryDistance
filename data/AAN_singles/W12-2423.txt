Proceedings of the 2012 Workshop on Biomedical Natural Language Processing (BioNLP 2012), pages 185?192,Montre?al, Canada, June 8, 2012. c?2012 Association for Computational LinguisticsClassifying Gene Sentences in Biomedical Literature byCombining High-Precision Gene IdentifiersSun Kim, Won Kim, Don Comeau, and W. John WilburNational Center for Biotechnology InformationNational Library of Medicine, National Institutes of HealthBethesda, MD 20894, USA{sun.kim,won.kim,donald.comeau,john.wilbur}@nih.govAbstractGene name identification is a fundamentalstep to solve more complicated text miningproblems such as gene normalization and pro-tein-protein interactions.
However, state-of-the-art name identification methods are notyet sufficient for use in a fully automated sys-tem.
In this regard, a relaxed task,gene/protein sentence identification, mayserve more effectively for manually searchingand browsing biomedical literature.
In this pa-per, we set up a new task, gene/protein sen-tence classification and propose an ensembleapproach for addressing this problem.
Well-known named entity tools use similar gold-standard sets for training and testing, whichresults in relatively poor performance for un-known sets.
We here explore how to combinediverse high-precision gene identifiers formore robust performance.
The experimentalresults show that the proposed approach out-performs BANNER as a stand-alone classifierfor newly annotated sets as well as previousgold-standard sets.1 IntroductionWith the rapidly increasing biomedical literature,text mining has become popular for finding bio-medical information in text.
Among others, namedentity recognition (NER) for bio-entities such asgenes and proteins is a fundamental task becauseextracting biological relationships begins with enti-ty identification.
However, NER in biomedicalliterature is challenging due to the irregularitiesand ambiguities in bio-entities nomenclature (Yanget al, 2008).
In particular, compound entity namesmake this problem difficult because it also requiresdeciding word boundaries.Recent bio-text competitions such as JNLPBA(Kim et al, 2004) and BioCreative (Lu et al, 2011;Smith et al, 2008) have evaluated NER systemsfor gene mentions.
Even though progress has beenmade in several areas, gene identification methodsare not yet sufficient for real-world use withouthuman interaction (Arighi et al, 2011).
Thus, atthe present, a realistic suggestion is to use thesealgorithms as an aid to human curation and infor-mation retrieval (Altman et al, 2008).In this paper, we define a new task, gene/proteinsentence classification.
A gene or protein sentencemeans a sentence including at least one specificgene or protein name.
This new task has ad-vantages over gene mention identification.
First,gene name boundaries are not important at the sen-tence level and human judges will agree more intheir judgments.
Second, highlighting gene sen-tences may be more useful in manual search andbrowsing environments since this can be donemore accurately and with less distraction from in-correct annotations.To classify gene/protein sentences, we here pro-pose an ensemble approach to combine differentNER identifiers.
Previous NER approaches aremostly developed on a small number of gold-185standard sets including GENIA (Kim et al, 2003)and BioCreative (Smith et al, 2008) corpora.
The-se sets help to find regular name patterns in a lim-ited set of articles, but also limit the NERperformance for real-world use.
In the proposedapproach, we use a Semantic Model and a PriorityModel along with BANNER (Leaman andGonzalez, 2008).
The Semantic and Priority Mod-els are used to provide more robust performance ongene/protein sentence classification because theyutilize larger resources such as SemCat and Pub-Med?R  to detect gene names.For experiments, we created three new gold-standard sets to include cases appearing in the mostrecent publications.
The experimental results showthat our approach outperforms machine learningclassifiers using unigrams and substring features aswell as stand-alone BANNER classification on fivegold-standard datasets.The paper is organized as follows.
In Section 2,the ensemble approach for gene/protein sentenceclassification is described.
Section 3 explains thegold-standard sets used for our experiments.
Sec-tion 4 presents and discusses the experimental re-sults.
Conclusions are drawn in Section 5.2 MethodsFigure 1.
Method Overview.Figure 1 shows the overall framework for our pro-posed approach.
We basically assume that a mainNER module works as a strong predictor, i.e., themajority of outputs obtained from this module arecorrect.
We here use BANNER (Leaman andGonzalez, 2008) as the main NER method becauseit adopts features and methods which are generallyknown to be effective for gene name recognition.While BANNER shows good performance onwell-known gold-standard sets, it suffers from rela-tively poor performance on unknown examples.
Toovercome this problem, we combine BANNERwith two other predictors, a Sematic Model and aPriority Model.
First, the Semantic Model and thePriority Model do not use previous gold-standardsets for training.
Second, these two models learnname patterns in different ways, i.e., semantic rela-tionships for the Semantic Model and positionaland lexical information for the Priority Model.This combination of a strong predictor and twoweaker but more general predictors can respondbetter to unknown name patterns.As described above, the proposed method main-ly relies on outputs from different NER methods,whereas word features can still provide useful evi-dence for discriminating gene and non-gene sen-tences.
Hence, we alternatively utilize wordfeatures such as unigrams and substrings alongwith NER features.
For NER features only, theoutput is the sum of binary decisions from threeNER modules.
For word and NER features, theHuber classifier (Kim and Wilbur, 2011) is trainedto combine the features.
The parameter set in theHuber classifier is optimized to show the best clas-sification performance on test sets.
The followingsubsections describe each feature type used forgene sentence classification.2.1 Word FeaturesUnigrams are a set of words obtained from to-kenizing sentences on white space.
All letters inunigrams are converted to lower case.Substrings are all contiguous substrings of a sen-tence, sized three to six characters.
This substringfeature may help reduce the difference betweendistributions on training and test sets (Huang et al,2008).
Substrings encode the roots and morpholo-gy of words without identifying syllables or stems.They also capture neighboring patterns betweenwords.2.2 BANNERBANNER is a freely available tool for identifyinggene mentions.
Due to its open-source policy andJava implementation, it has become a popular tool.BANNER uses conditional random fields (CRF)as a discriminative method and utilizes a set of fea-ture types that are known to be good for identify-ing gene names.
The feature sets used are186orthographic, morphological and shallow syntaxfeatures (Leaman and Gonzalez, 2008):(1) The part of speech (POS) of a token in a sen-tence.
(2) The lemma of a word.
(3) 2, 3 and 4-character prefixes and suffixes.
(4) 2 and 3 character n-grams including start-of-token and end-of-token indicators.
(5) Word patterns by converting upper-case letters,lower-case letters and digits to their correspondingrepresentative characters (A, a, 0).
(6) Numeric normalization by converting digits to?0?s.
(7) Roman numerals.
(8) Names of Greek letters.Even though BANNER covers most popularfeature types, it does not apply semantic features orother post-processing procedures such as abbrevia-tion processing.
However, these features may nothave much impact for reducing performance sinceour goal is to classify gene sentences, not genementions.2.3 Semantic ModelThe distributional approach to semantics (Harris,1954) has become more useful as computationalpower has increased, and we have found this ap-proach helpful in the attempt to categorize entitiesfound in text.
We use a vector space approach tomodeling semantics (Turney and Pantel, 2010) andcompute our vectors as described in (Pantel andLin, 2002) except we ignore the actual mutual in-formation and just include a component of 1 if thedependency relation occurs at all for a word, elsethe component is set to 0.
We constructed our vec-tor space from all single tokens (a token must havean alphabetic character) throughout the titles andabstracts of the records in the whole of the Pub-Med database based on a snapshot of the databasetaken in January 2012.
We included only tokensthat occurred in the data sufficient to accumulate10 or more dependency relations.
There were justover 750 thousand token types that satisfied thiscondition and are represented in the space.
We de-note this space by h. We then took all the singletokens and all head words from multi-token stringsin the categories ?chemical?, ?disease?, and?gene/protein?
from an updated version of theSemCat database (Tanabe et al, 2006) and placedall the other SemCat categories similarly processedinto a category we called ?other?.
We consider on-ly the tokens in these categories that also occur inour semantic vector space h and refer to these setsas Chemicalh , Diseaseh , inGene/Proteh , Otherh .
Table 1 showsthe size of overlaps between sets.Chemicalh Diseaseh  inGene/Proteh  OtherhChemicalh 54478 209 4605 5495Diseaseh  8801 1139 169inGene/Proteh   76440 9466Otherh    127337Table 1.
Pairwise overlap between sets representing thedifferent categories.Class 'Chemicalh 'Diseaseh  ' inGene/Proteh  'OtherhStrings 49800 7589 70832 113815Ave.
Prec.
0.8680 0.7060 0.9140 0.9120Table 2.
Row two contains the number of unique stringsin the four different semantic classes studied.
The lastrow shows the mean average precisions from a 10-foldcross validation to learn how to distinguish each classfrom the union of the other three.In order to remove noise or ambiguity in thetraining set, we removed the tokens that appearedin more than one semantic class as follows.?
??
??
??
?inGene/ProteDiseaseChemicalOther'OtherDiseaseChemicalinGene/Prote'inGene/ProteinGene/ProteChemicalDisease'DiseaseinGene/ProteDiseaseChemical'Chemicalhhhhhhhhhhhhhhhhh?????????????
(1)We then applied Support Vector Machine learn-ing to the four resulting disjoint semantic classes ina one-against-all strategy to learn how to classifyinto the different classes.
We used 31064.1 ?
?Cbased upon the size of the training set.
As a test ofthis process we applied this same learning with 10-fold cross validation on the training data and theresults are given in the last row of Table 2.This Semantic Model is an efficient and generalway to identify words indicating gene names.
Un-like other NER approaches, this model decides atarget class solely based on a single word.
Howev-er, evaluating all tokens from sentences may in-crease incorrect predictions.
A dependency parseranalyzes a sentence as a set of head- and depend-187ent-word combinations.
Since gene names likelyappear in describing a relationship with other enti-ties, a name indicating a gene mention will bemostly placed in a dependent position.
Thus, wefirst apply the C&C CCG parser (Curran et al,2007), and evaluate words in dependent positionsonly.2.4 Priority ModelThe Semantic Model detects four different catego-ries for a single word.
However, the Priority Modelcaptures gene name patterns by analyzing the orderof words and the character strings making upwords.
Since gene names are noun phrases in gen-eral, we parse sentences and identify noun phrasesfirst.
These phrases are then evaluated using thePriority Model.The Priority Model is a statistical languagemodel for named entity recognition (Tanabe andWilbur, 2006).
For named entities, a word to theright is more likely to be the word determining thenature of the entity than a word to the left in gen-eral.Let T1 be the set of training data for class C1 andT2 for class C2.
Let ?
?
At ???
denote the set of all to-kens used in names contained in 21 TT ?
.
For eachtoken t?, A??
, it is assumed that there are associ-ated two probabilities p?
and q?, where p?
is theprobability that the appearance of the token t?
in aname indicates that name belongs to class C1 andq?
is the probability that t?
is a more reliable indi-cator of the class of a name than any token to itsleft.
Let )()2()1( ktttn ???
??
be composed of thetokens on the right in the given order.
Then theprobability of n belonging to class C1 can be com-puted as follows.?
?
?
?
?
??
???
??????
?kikijjiikjj qpqqpnCp2 1)()()(2)()1(1 11| ?????
(2)A limited memory BFGS method (Nash andNocedal, 1991) and a variable order Markov model(Tanabe and Wilbur, 2006) are used to obtain p?and q?.
An updated version of SemCat (Tanabe andWilbur, 2006) was used to learn gene names.2.5 Semantic and Priority Models for High-Precision ScoresThe Semantic and Priority Models learn genenames and other necessary information from theSemCat database, where names are semanticallycategorized based on UMLS?R  (Unified MedicalLanguage System) Semantic Network.
Eventhough the Semantic and Priority Models showgood performance on names in SemCat, they can-not avoid noise obtained from incorrect pre-processing, e.g., parsing errors.
The use of a gen-eral category for training may also limit perfor-mance.
To obtain high-precision scores for ourensemble approach, it is important to reduce thenumber of false positives from predictions.
Hence,we apply the Semantic and Priority Models ontraining sets, and mark false positive cases.
Thesefalse positives are automatically removed frompredictions on test sets.
These false positive casestend to be terms for entities too general to warrantannotation.Table 3 shows the classification performancewith and without false positive corrections ontraining data.
For both Semantic and Priority Mod-els, precision rates are increased by removing falsepositives.
Even though recall drops drastically, thisdoes not cause a big problem in our setup sincethese models try to detect gene names which arenot identified by BANNER.SEM SEMFP PM PMFPAccuracy 0.7907 0.7773 0.7805 0.8390Precision 0.7755 0.8510 0.7405 1.0000Recall 0.8323 0.6852 0.8799 0.6856F1 0.8029 0.7592 0.8042 0.8135Table 3.
Performance changes on training set for theSemantic Model (SEM) and the Priority Model (PM).FP indicates that learned false positives were removedfrom predictions.3 DatasetsFor experiments, we rigorously tested the proposedmethod on gene mention gold-standard sets andnewly annotated sets.
GENETAG (Smith et al,2008) is the dataset released for BioCreative I andBioCreative II workshops.
Since it is well-knownfor a gene mention gold-standard set, we usedGENETAG as training data.For test data, two previous gold-standard setswere selected and new test sets were also built forgene sentence classification.
YAPEX (Franzen etal., 2002) and JNLPBA (Kim et al, 2004) are con-sidered of moderate difficulty because they are188both related to GENIA corpus, a well-known gold-standard set.
However, Disease, Cell Line andReptiles are considered as more difficult tasks be-cause they represent new areas and contain recent-ly published articles.
The annotation guideline fornew test sets basically followed those used inGENETAG (Tanabe et al, 2005), however do-mains, complexes, subunits and promoters werenot included in new sets.
(1) ?Disease?
Set: This set of 60 PubMed docu-ments was obtained from two sources.
Fifty of thedocuments were obtained from the 793 PubMeddocuments used to construct the AZDC (Leaman etal., 2009).
They are the fifty most recent amongthese records.
In addition to these fifty documents,ten documents were selected from PubMed on thetopic of maize to add variety to the set and becauseone of the curators who worked with the set hadexperience studying the maize genome.
These tenwere chosen as recent documents as of early March2012 and which contained the text word maize anddiscussed genetics.
The whole set of 60 docu-ments were annotated by WJW to produce a goldstandard.
(2) ?CellLine?
Set: This set comprised the mostrecent 50 documents satisfying the query ?cellline[MeSH]?
in PubMed on March 15, 2012.
Thisquery was used to obtain documents which discusscell lines, but most of these documents also discussgenes and for this reason the set was expected to bechallenging.
The set was annotated by WJW andDC and after independently annotating the set theyreconciled differences to produce a final goldstandard.
(3) ?Reptiles?
Set: This set comprised the mostrecent 50 documents satisfying the query ?reptilesAND genes [text]?
in PubMed on March 15, 2012.This set was chosen because it would have littleabout human or model organisms and for this rea-son it was expected to be challenging.
The set wasannotated by WJW and DC and after independent-ly annotating the set they reconciled differences toproduce a final gold standard.For both ?CellLine?
and ?Reptiles?
Sets, themost recent data was chosen in an effort to makethe task more challenging.
Presumably such docu-ments will contain more recently created namesand phrases that do not appear in the older trainingdata.
This will then pose a more difficult test forNER systems.Table 4 shows all datasets used for training andtesting.
The new sets, ?Disease?, ?CellLine?
and?Reptiles?
are also freely available athttp://www.ncbi.nlm.nih.gov/CBBresearch/Wilbur/IRET/bionlp.zipPositives Negatives TotalGENETAG 10245 9755 20000YAPEX 1298 378 1676JNLPBA 17761 4641 22402Disease 345 251 596CellLine 211 217 428Reptiles 179 328 507Table 4.
Datasets.
?GENETAG?
was used for trainingdata and others were used for test data.
?YAPEX?
and?JNLPBA?
were selected from previous gold-standardcorpora.
?Disease?, ?Cell Line?
and ?Reptiles?
are new-ly created from recent publications and considered asdifficult sets.4 Results and DiscussionIn this paper, our goal is to achieve higher-prediction performance on a wide range of genesentences by combining multiple gene mentionidentifiers.
The basic assumption here is that thereis a strong predictor that performs well for previ-ously known gold-standard datasets.
For thisstrong predictor, we selected BANNER since itincludes basic features that are known to give goodperformance.Accuracy Precision Recall F1GENETAG 0.9794 0.9817 0.9779 0.9799YAPEX 0.9051 0.9304 0.9483 0.9392JNLPBA 0.8693 0.9349 0.8976 0.9159Disease 0.8591 0.9223 0.8261 0.8716Cell Line 0.8925 0.9146 0.8626 0.8878Reptiles 0.8994 0.8478 0.8715 0.8595Table 5.
Performance of BANNER on training and testdatasets.Table 5 presents the gene sentence classificationperformance of BANNER on training and test sets.We emphasize that performance here means that ifBANNER annotates a gene/protein name in a sen-tence, that sentence is classified as positive, other-wise it is classified as negative.
BANNER usedGENETAG as training data, hence it shows excel-lent classification performance on the same set.189Unigrams Substrings BANNER Ensemble Uni+Ensemble Sub+EnsembleYAPEX 0.9414 0.9491 0.9685 0.9704 0.9624 0.9678JNLPBA 0.9512 0.9504 0.9584 0.9651 0.9625 0.9619Disease 0.8255 0.8852 0.9238 0.9501 0.9573 0.9610CellLine 0.8174 0.9004 0.9281 0.9539 0.9429 0.9496Reptiles 0.6684 0.7360 0.8696 0.9049 0.9001 0.8937Table 6.
Average precision results on test sets for different feature combinations.Unigrams Substrings BANNER Ensemble Uni+Ensemble Sub+EnsembleYAPEX 0.8735 0.8819 0.9321 0.9196 0.9298 0.9336JNLPBA 0.8902 0.8938 0.9111 0.9197 0.9262 0.9264Disease 0.7449 0.7884 0.8479 0.8894 0.8957 0.9043CellLine 0.7346 0.8057 0.8698 0.9017 0.9052 0.8957Reptiles 0.6257 0.6816 0.8499 0.8199 0.8547 0.8547Table 7.
Breakeven results on test sets for different feature combinations.?
Just one fiber gene was revealed in this strain.?
This transcription factor family is characterized bya DNA-binding alpha-subunit harboring the Runtdomain and a secondary subunit, beta, which bindsto the Runt domain and enhances its interactionwith DNA.Figure 2.
False positive examples including misleadingwords.YAPEX and JNLPBA are gold-standard sets thatpartially overlap the GENIA corpus.
SinceBANNER utilizes features from previous researchon GENETAG, YAPEX and JNLPBA, we expectgood performance on these data sets.
For that rea-son, we created the three additional gold-standardsets to use in this study, and we believe the per-formance on these sets is more representative ofwhat could be expected when our method is ap-plied to cases recently appearing in the literature.Table 6 show average precision results for thedifferent methods and all the test sets.
GENETAGis left out because BANNER is trained onGENETAG.
We observe improved performance ofthe ensemble methods over unigrams, substringsand BANNER.
The improvement is small onYAPEX and JNLPBA, but larger for Disease,CellLine and Reptiles.
We see that unigrams andsubstrings tend to add little to the plain ensemble.The MAP (Mean Average Precision) values inTable 6 are in contrast to the breakeven results inTable 7, where we see that unigrams and sub-strings included with the ensemble generally giveimproved results.
Some of the unigrams and sub-strings are specific enough to detect gene/proteinnames with high accuracy, and improve precisionin top ranks in a way that cannot be duplicated bythe annotations coming from Semantic or PriorityModels or BANNER.
In addition, substrings maycapture more information than unigrams becauseof their greater generality.Some of our errors are due to false positive NERidentifications.
By this we mean a token was clas-sified as a gene/protein by BANNER or the Se-mantic or Priority Models.
This often happenswhen the name indeed represents a gene/proteinclass, which is too general to be marked positive(Figure 2).
A general way in which this problemcould be approached is to process a large amountof literature discussing genes or proteins and lookfor names that are marked as positives by one ofthe NER identifiers, and which appear frequentlyin plural form as well as in the singular.
Suchnames are likely general class names, and have ahigh probability to be false positives.Another type of error will arise when unseen to-kens are encountered.
If such tokens have stringsimilarity to gene/protein names already encoun-tered in the SemCat data, they may be recognizedby the Priority Model.
But there will be completelynew strings.
Then one must rely on context andthis may not be adequate.
We think there is littlethat can be done to solve this short of better lan-guage understanding by computers.There is a benefit in considering whole sentenc-es as opposed to named entities.
By consideringwhole sentences, name boundaries become a non-issue.
For this reason, one can expect training datato be more accurate, i.e., human judges will tend toagree more in their judgments.
This may allow forimproved training and testing performance of ma-190chine learning methods.
We believe it beneficialthat human users are directed to sentences that con-tain the entities they seek without necessity ofviewing the less accurate entity specific taggingwhich they may then have to correct.5 ConclusionsWe defined a new task for classifying gene/proteinsentences as an aid to human curation and infor-mation retrieval.
An ensemble approach was usedto combine three different NER identifiers for im-proved gene/protein sentence recognition.
Our ex-periments show that one can indeed find improvedperformance over a single NER identifier for thistask.
An additional advantage is that performanceat this task is significantly more accurate thangene/protein NER.
We believe this improved accu-racy may benefit human users of this technology.We also make available to the research communitythree gold-standard gene mention sets, and two ofthese are taken from the most recent literature ap-pearing in PubMed.AcknowledgmentsThis work was supported by the Intramural Re-search Program of the National Institutes ofHealth, National Library of Medicine.ReferencesR.
B. Altman, C. M. Bergman, J. Blake, C. Blaschke, A.Cohen, F. Gannon, L. Grivell, U. Hahn, W. Hersh, L.Hirschman, L. J. Jensen, M. Krallinger, B. Mons, S.I.
O'donoghue, M. C. Peitsch, D. Rebholz-Schuhmann, H. Shatkay, and A. Valencia.
2008.
Textmining for biology - the way forward: opinions fromleading scientists.
Genome Biol, 9 Suppl 2:S7.C.
N. Arighi, Z. Lu, M. Krallinger, K. B. Cohen, W. J.Wilbur, A. Valencia, L. Hirschman, and C. H. Wu.2011.
Overview of the BioCreative III workshop.BMC Bioinformatics, 12 Suppl 8:S1.J.
R. Curran, S. Clark, and J. Bos.
2007.
Linguisticallymotivated large-scale NLP with C&C and boxer.
InProceedings of the 45th Annual Meeting of the ACLon Interactive Poster and Demonstration Sessions,pages 33-36.K.
Franzen, G. Eriksson, F. Olsson, L. Asker, P. Liden,and J. Coster.
2002.
Protein names and how to findthem.
Int J Med Inform, 67:49-61.Z.
S. Harris.
1954.
Distributional structure.
Word,10:146-162.M.
Huang, S. Ding, H. Wang, and X. Zhu.
2008.Mining physical protein-protein interactions from theliterature.
Genome Biol, 9 Suppl 2:S12.J.-D. Kim, T. Ohta, Y. Tateisi, and J. Tsujii.
2003.GENIA corpus - semantically annotated corpus forbio-textmining.
Bioinformatics, 19 Suppl 1:i180-i182.J.-D. Kim, T. Ohta, Y. Tsuruoka, Y. Tateisi, and N.Collier.
2004.
Introduction to the bio-entityrecognition task at JNLPBA.
In Proceedings of theInternational Joint Workshop on Natural LanguageProcessing in Biomedicine and its Applications,pages 70-75.S.
Kim and W. J. Wilbur.
2011.
Classifying protein-protein interaction articles using word and syntacticfeatures.
BMC Bioinformatics, 12 Suppl 8:S9.R.
Leaman and G. Gonzalez.
2008.
BANNER: anexecutable survey of advances in biomedical namedentity recognition.
In Proceedings of the PacificSymposium on Biocomputing, pages 652-663.R.
Leaman, C. Miller, and G. Gonzalez.
2009.
Enablingrecognition of diseases in biomedical text withmachine learning: corpus and benchmark.
In 2009Symposium on Languages in Biology and Medicine.Z.
Lu, H. Y. Kao, C. H. Wei, M. Huang, J. Liu, C. J.Kuo, C. N. Hsu, R. T. Tsai, H. J. Dai, N. Okazaki, H.C. Cho, M. Gerner, I. Solt, S. Agarwal, F. Liu, D.Vishnyakova, P. Ruch, M. Romacker, F. Rinaldi, S.Bhattacharya, P. Srinivasan, H. Liu, M. Torii, S.Matos, D. Campos, K. Verspoor, K. M. Livingston,and W. J. Wilbur.
2011.
The gene normalization taskin BioCreative III.
BMC Bioinformatics, 12 Suppl8:S2.S.
G. Nash and J. Nocedal.
1991.
A numerical study ofthe limited memory BFGS method and the truncated-Newton method for large scale optimization.
SIAMJournal on Optimization, 1:358-372.P.
Pantel and D. Lin.
2002.
Discovering word sensesfrom text.
In Proceedings of the Eighth ACMSIGKDD International Conference on KnowledgeDiscovery and Data Mining, pages 613-619.L.
Smith, L. K. Tanabe, R. J. Ando, C. J. Kuo, I. F.Chung, C. N. Hsu, Y. S. Lin, R. Klinger, C. M.Friedrich, K. Ganchev, M. Torii, H. Liu, B. Haddow,C.
A. Struble, R. J. Povinelli, A. Vlachos, W. A.Baumgartner, Jr., L. Hunter, B. Carpenter, R. T. Tsai,H.
J. Dai, F. Liu, Y. Chen, C. Sun, S. Katrenko, P.Adriaans, C. Blaschke, R. Torres, M. Neves, P.Nakov, A. Divoli, M. Mana-Lopez, J. Mata, and W.J.
Wilbur.
2008.
Overview of BioCreative II genemention recognition.
Genome Biol, 9 Suppl 2:S2.L.
Tanabe, L. H. Thom, W. Matten, D. C. Comeau, andW.
J. Wilbur.
2006.
SemCat: semanticallycategorized entities for genomics.
In AMIA AnnuSymp Proc, pages 754-758.191L.
Tanabe and W. J. Wilbur.
2006.
A priority model fornamed entities.
In Proceedings of the Workshop onLinking Natural Language Processing and Biology:Towards Deeper Biological Literature Analysis,pages 33-40.L.
Tanabe, N. Xie, L. H. Thom, W. Matten, and W. J.Wilbur.
2005.
GENETAG: a tagged corpus forgene/protein named entity recognition.
BMCBioinformatics, 6 Suppl 1:S3.P.
D. Turney and P. Pantel.
2010.
From frequency tomeaning: vector space models of semantics.
Journalof Artificial Intelligence Research, 37:141-188.Z.
Yang, H. Lin, and Y. Li.
2008.
Exploiting thecontextual cues for bio-entity name recognition inbiomedical literature.
J Biomed Inform, 41:580-587.192
