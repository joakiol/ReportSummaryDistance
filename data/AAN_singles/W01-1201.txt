Looking Under the Hood: Tools for Diagnosing Your QuestionAnswering EngineEric Breck?, Marc Light?, Gideon S.
Mann?, Ellen Riloff?,Brianne Brown?, Pranav Anand?, Mats Rooth?, Michael Thelen??
The MITRE Corporation, 202 Burlington Rd.,Bedford, MA 01730, {ebreck,light}@mitre.org?
Department of Computer Science, Johns Hopkins University, Baltimore, MD 21218, gsm@cs.jhu.edu?
School of Computing, University of Utah, Salt Lake City, UT 84112, {riloff,thelenm}@cs.utah.edu?
Bryn Mawr College, Bryn Mawr, PA 19010, bbrown@brynmawr.edu?
Department of Mathematics, Harvard University, Cambridge, MA 02138, anand@fas.harvard.edu?
Department of Linguistics, Cornell University, Ithaca, NY 14853, mr249@cornell.eduAbstractIn this paper we analyze two questionanswering tasks : the TREC-8 ques-tion answering task and a set of readingcomprehension exams.
First, we showthat Q/A systems perform better whenthere are multiple answer opportunitiesper question.
Next, we analyze com-mon approaches to two subproblems:term overlap for answer sentence iden-tification, and answer typing for shortanswer extraction.
We present generaltools for analyzing the strengths andlimitations of techniques for these sub-problems.
Our results quantify the limi-tations of both term overlap and answertyping to distinguish between compet-ing answer candidates.1 IntroductionWhen building a system to perform a task, themost important statistic is the performance onan end-to-end evaluation.
For the task of open-domain question answering against text collec-tions, there have been two large-scale end-to-end evaluations: (TREC-8 Proceedings, 1999)and (TREC-9 Proceedings, 2000).
In addition, anumber of researchers have built systems to takereading comprehension examinations designed toevaluate children?s reading levels (Charniak et al,2000; Hirschman et al, 1999; Ng et al, 2000;Riloff and Thelen, 2000; Wang et al, 2000).
Theperformance statistics have been useful for deter-mining how well techniques work.However, raw performance statistics are notenough.
If the score is low, we need to under-stand what went wrong and how to fix it.
If thescore is high, it is important to understand why.For example, performance may be dependent oncharacteristics of the current test set and wouldnot carry over to a new domain.
It would also beuseful to know if there is a particular character-istic of the system that is central.
If so, then thesystem can be streamlined and simplified.In this paper, we explore ways of gaininginsight into question answering system perfor-mance.
First, we analyze the impact of havingmultiple answer opportunities for a question.
Wefound that TREC-8 Q/A systems performed bet-ter on questions that had multiple answer oppor-tunities in the document collection.
Second, wepresent a variety of graphs to visualize and ana-lyze functions for ranking sentences.
The graphsrevealed that relative score instead of absolutescore is paramount.
Third, we introduce boundson functions that use term overlap1 to rank sen-tences.
Fourth, we compute the expected score ofa hypothetical Q/A system that correctly identifiesthe answer type for a question and correctly iden-tifies all entities of that type in answer sentences.We found that a surprising amount of ambiguityremains because sentences often contain multipleentities of the same type.1Throughout the text, we use ?overlap?
to refer to theintersection of sets of words, most often the words in thequestion and the words in a sentence.2 The dataThe experiments in Sections 3, 4, and 5 were per-formed on two question answering data sets: (1)the TREC-8 Question Answering Track data setand (2) the CBC reading comprehension data set.We will briefly describe each of these data setsand their corresponding tasks.The task of the TREC-8 Question Answeringtrack was to find the answer to 198 questions us-ing a document collection consisting of roughly500,000 newswire documents.
For each question,systems were allowed to return a ranked list of5 short (either 50-character or 250-character) re-sponses.
As a service to track participants, AT&Tprovided top documents returned by their retrievalengine for each of the TREC questions.
Sec-tions 4 and 5 present analyses that use all sen-tences in the top 10 of these documents.
Eachsentence is classified as correct or incorrect auto-matically.
This automatic classification judges asentence to be correct if it contains at least halfof the stemmed, content-words in the answer key.We have compared this automatic evaluation tothe TREC-8 QA track assessors and found it toagree 93-95% of the time (Breck et al, 2000).The CBC data set was created for the JohnsHopkins Summer 2000 Workshop on ReadingComprehension.
Texts were collected from theCanadian Broadcasting Corporation web page forkids (http://cbc4kids.ca/).
They are an averageof 24 sentences long.
The stories were adaptedfrom newswire texts to be appropriate for ado-lescent children, and most fall into the follow-ing domains: politics, health, education, science,human interest, disaster, sports, business, crime,war, entertainment, and environment.
For eachCBC story, 8-12 questions and an answer keywere generated.2 We used a 650 question sub-set of the data and their corresponding 75 stories.The answer candidates for each question in thisdata set were all sentences in the document.
Thesentences were scored against the answer key bythe automatic method described previously.2This work was performed by Lisa Ferro and Tim Bevinsof the MITRE Corporation.
Dr. Ferro has professional expe-rience writing questions for reading comprehension examsand led the question writing effort.3 Analyzing the number of answeropportunities per questionIn this section we explore the impact of multipleanswer opportunities on end-to-end system per-formance.
A question may have multiple answersfor two reasons: (1) there is more than one differ-ent answer to the question, and (2) there may bemultiple instances of each answer.
For example,?What does the Peugeot company manufacture?
?can be answered by trucks, cars, or motors andeach of these answers may occur in many sen-tences that provide enough context to answer thequestion.
The table insert in Figure 1 shows that,on average, there are 7 answer occurrences perquestion in the TREC-8 collection.3 In contrast,there are only 1.25 answer occurrences in a CBCdocument.
The number of answer occurrencesvaries widely, as illustrated by the standard devia-tions.
The median shows an answer frequency of3 for TREC and 1 for CBC, which perhaps givesa more realistic sense of the degree of answer fre-quency for most questions.00.10.20.30.40.50.60.70.80.91 2 3 4 5 6 7 9 1 2 1 4 1 8 2 7 2 8 6 1 6 7# Answers%QuestionsTREC-85 03 5 27.04312.94CBC2 1 92 7 41.2510.61# Questions# AnswersMeanMedianStandard Dev.Figure 1: Frequency of answers in the TREC-8(black bars) and CBC (white bars) data setsTo gather this data we manually reviewed 50randomly chosen TREC-8 questions and identi-fied all answers to these questions in our text col-lection.
We defined an ?answer?
as a text frag-ment that contains the answer string in a contextsufficient to answer the question.
Figure 1 showsthe resulting graph.
The x-axis displays the num-ber of answer occurrences found in the text col-lection per question and the y-axis shows the per-3We would like to thank John Burger and John Aberdeenfor help preparing Figure 1.centage of questions that had x answers.
For ex-ample, 26% of the TREC-8 questions had only1 answer occurrence, and 20% of the TREC-8questions had exactly 2 answer occurrences (theblack bars).
The most prolific question had 67answer occurrences (the Peugeot example men-tioned above).
Figure 1 also shows the analysisof 219 CBC questions.
In contrast, 80% of theCBC questions had only 1 answer occurrence inthe targeted document, and 16% had exactly 2 an-swer occurrences.00.10.20.30.40.50.60.70.80.90 1 0 2 0 3 0 4 0 5 0 6 0 7 0# answers occurences per question%of systemswithat leastonecorrectresponsePoint per questionMean correct per occurrence #Figure 2: Answer repetition vs. system responsecorrectness for TREC-8Figure 2 shows the effect that multiple answeropportunities had on the performance of TREC-8systems.
Each solid dot in the scatter plot repre-sents one of the 50 questions we examined.4 Thex-axis shows the number of answer opportunitiesfor the question, and the y-axis represents the per-centage of systems that generated a correct an-swer5 for the question.
E.g., for the question with67 answer occurrences, 80% of the systems pro-duced a correct answer.
In contrast, many ques-tions had a single answer occurrence and the per-centage of systems that got those correct variedfrom about 2% to 60%.The circles in Figure 2 represent the averagepercentage of systems that answered questionscorrectly for all questions with the same numberof answer occurrences.
For example, on averageabout 27% of the systems produced a correct an-swer for questions that had exactly one answer oc-4We would like to thank Lynette Hirschman for suggest-ing the analysis behind Figure 2 and John Burger for helpwith the analysis and presentation.5For this analysis, we say that a system generated a cor-rect answer if a correct answer was in its response set.currence, but about 50% of the systems produceda correct answer for questions with 7 answer op-portunities.
Overall, a clear pattern emerges: theperformance of TREC-8 systems was stronglycorrelated with the number of answer opportuni-ties present in the document collection.4 Graphs for analyzing scoringfunctions of answer candidatesMost question answering systems generate sev-eral answer candidates and rank them by defin-ing a scoring function that maps answer candi-dates to a range of numbers.
In this section,we analyze one particular scoring function: termoverlap between the question and answer can-didate.
The techniques we use can be easilyapplied to other scoring functions as well (e.g.,weighted term overlap, partial unification of sen-tence parses, weighted abduction score, etc.).
Theanswer candidates we consider are the sentencesfrom the documents.The expected performance of a system thatranks all sentences using term overlap is 35% forthe TREC-8 data.
This number is an expectedscore because of ties: correct and incorrect can-didates may have the same term overlap score.
Ifties are broken optimally, the best possible score(maximum) would be 54%.
If ties are brokenmaximally suboptimally, the worst possible score(minimum) would be 24%.
The correspondingscores on the CBC data are 58% expected, 69%maximum, and 51% minimum.
We would like tounderstand why the term overlap scoring functionworks as well as it does and what can be done toimprove it.Figures 3 and 4 compare correct candidates andincorrect candidates with respect to the scoringfunction.
The x-axis plots the range of the scor-ing function, i.e., the amount of overlap.
They-axis represents Pr(overlap=x | correct) andPr(overlap=x | incorrect), where separate curvesare plotted for correct and incorrect candidates.The probabilities are generated by normalizingthe number of correct/incorrect answer candidateswith a particular overlap score by the total numberof correct/incorrect candidates, respectively.Figure 3 illustrates that the correct candidatesfor TREC-8 have term overlap scores distributedbetween 0 and 10 with a peak of 24% at an over-00.050.10.150.20.250.30.350 2 4 6 8 10 12 14 16 18 20Normalized(+/3087,-/57073) CountoverlapincorrectcorrectFigure 3: Pr(overlap=x|[in]correct) for TREC-800.050.10.150.20.250.30.350.40 5 10 15 20 25 30Normalized(+/1311,-/14610) CountoverlapincorrectcorrectFigure 4: Pr(overlap=x|[in]correct) for CBClap of 2.
However, the incorrect candidates havea similar distribution between 0 and 8 with a peakof 32% at an overlap of 0.
The similarity of thecurves illustrates that it is unclear how to use thescore to decide if a candidate is correct or not.Certainly no static threshold above which a can-didate is deemed correct will work.
Yet the ex-pected score of our TREC term overlap systemwas 35%, which is much higher than a randombaseline which would get an expected score ofless than 3% because there are over 40 sentenceson average in newswire documents.6After inspecting some of the data directly, weposited that it was not the absolute term overlapthat was important for judging candidate but howthe overlap score compares to the scores of othercandidates.
To visualize this, we generated newgraphs by plotting the rank of a candidate?s score6We also tried dividing the term overlap score by thelength of the question to normalize for query length but didnot find that the graph was any more helpful.on the x-axis.
For example, the candidate withthe highest score would be ranked first, the can-didate with the second highest score would beranked second, etc.
Figures 5 and 6 show thesegraphs, which display Pr(rank=x | correct) andPr(rank=x | incorrect) on the y-axis.
The top-ranked candidate has rank=0.00.0020.0040.0060.0080.010.0120.0140.0160.0180.02-1000-900-800-700-600-500-400-300-200-100 0Normalized(+/3087,-/57073) Countranked overlapincorrectcorrectFigure 5: Pr(rank=x | [in]correct) for TREC-800.050.10.150.20.250.3-45 -40 -35 -30 -25 -20 -15 -10 -5 0Normalized(+/1311,-/14610) Countranked overlapincorrectcorrectFigure 6: Pr(rank=x | [in]correct) for CBCThe ranked graphs are more revealing than thegraphs of absolute scores: the probability of ahigh rank is greater for correct answers than in-correct ones.
Now we can begin to understandwhy the term overlap scoring function worked aswell as it did.
We see that, unlike classificationtasks, there is no good threshold for our scor-ing function.
Instead relative score is paramount.Systems such as (Ng et al, 2000) make explicituse of relative rank in their algorithms and nowwe understand why this is effective.Before we leave the topic of graphing scoringfunctions, we want to introduce one other view ofthe data.
Figure 7 plots term overlap scores on-4-3.5-3-2.5-2-1.5-1-0.500.510 2 4 6 8 10 12 1402000400060008000100001200014000160001800020000log-oddsof correctnessmassoverlaplog-oddsmass curveFigure 7: TREC-8 log odds correct given overlapthe x-axis and the log odds of being correct givena score on the y-axis.
The log odds formula is:log Pr(correct|overlap)Pr(incorrect|overlap)Intuitively, this graph shows how much morelikely a sentence is to be correct versus incorrectgiven a particular score.
A second curve, labeled?mass,?
plots the number of answer candidateswith each score.
Figure 7 shows that the odds ofbeing correct are negative until an overlap of 10,but the mass curve reveals that few answer candi-dates have an overlap score greater than 6.5 Bounds on scoring functions that useterm overlapThe scoring function used in the previous sec-tion simply counts the number of terms sharedby a question and a sentence.
One obvious mod-ification is to weight some terms more heavilythan others.
We tried using inverse document fre-quence based (IDF) term weighting on the CBCdata but found that it did not improve perfor-mance.
The graph analogous to Figure 6 but withIDF term weighting was virtually identical.Could another weighting scheme perform bet-ter?
How well could an optimal weightingscheme do?
How poorly would the maximallysuboptimal scheme do?
The analysis in this sec-tion addresses these questions.
In essence the an-swer is the following: the question and the can-didate answers are typically short and thus thenumber of overlapping terms is small ?
conse-quently, many candidate answers have exactly thesame overlapping terms and no weighting schemecould differentiate them.
In addition, subset rela-tions often hold between overlaps.
A candidatewhose overlap is a subset of a second candidatecannot score higher regardless of the weightingscheme.7 We formalize these overlap set relationsand then calculate statistics based on them for theCBC and TREC data.Question: How much was Babe Belanger paid to playamateur basketball?S1: She was a member of the winningestbasketball team Canada ever had.S2: Babe Belanger never made a cent for herskills.S3: They were just a group of young womenfrom the same school who liked toplay amateur basketball.S4: Babe Belanger played with the Grads from1929 to 1937.S5: Babe never talked about her fabulous career.MaxOsets : ( {S2, S4}, {S3} )Figure 8: Example of Overlap Sets from CBCFigure 8 presents an example from the CBCdata.
The four overlap sets are (i) Babe Belanger,(ii) basketball, (iii) play amateur basketball, and(iv) Babe.
In any term-weighting scheme withpositive weights, a sentence containing the wordsBabe Belanger will have a higher score than sen-tences containing just Babe, and sentences withplay amateur basketball will have a higher scorethan those with just basketball.
However, we can-not generalize with respect to the relative scoresof sentences containing Babe Belanger and thosecontaining play amateur basketball because someterms may have higher weights than others.The most we can say is that the highest scor-ing candidate must be a member of {S2, S4} or{S3}.
S5 and S1 cannot be ranked highest be-cause their overlap sets are a proper subset ofcompeting overlap sets.
The correct answer isS2 so an optimal weighting scheme would havea 50% chance of ranking S2 first, assuming thatit identified the correct overlap set {S2, S4} andthen randomly chose between S2 and S4.
A max-imally suboptimal weighting scheme could rankS2 no lower than third.We will formalize these concepts using the fol-lowing variables:7Assuming that all term weights are positive.q: a question (a set of words)s: a sentence (a set of words)w,v: sets of intersecting wordsWe define an overlap set (ow,q) to be a set ofsentences (answer candidates) that have the samewords overlapping with the question.
We define amaximal overlap set (Mq) as an overlap set that isnot a subset of any other overlap set for the ques-tion.
For simplicity, we will refer to a maximaloverlap set as a MaxOset.ow,q = {s|s ?
q = w}?q = all unique overlap sets for qmaximal(ow,q) if ?ov,q ?
?q, w 6?
vMq = {ow,q ?
?q | maximal(ow,q)}Cq = {s|s correctly answers q}We can use these definitions to give upperand lower bounds on the performance of term-weighting functions on our two data sets.
Table 1shows the results.
The max statistic is the per-centage of questions for which at least one mem-ber of its MaxOsets is correct.
The min statis-tic is the percentage of questions for which allcandidates of all of its MaxOsets are correct (i.e.,there is no way to pick a wrong answer).
Finallythe expectedmax is a slightly more realistic up-per bound.
It is equivalent to randomly choosingamong members of the ?best?
maximal overlapset, i.e., the MaxOset that has the highest percent-age of correct members.
Formally, the statisticsfor a set of questions Q are computed as:max = |{q|?o ?
Mq,?s ?
o s.t.
s ?
Cq}||Q|min = |{q|?o ?
Mq,?s ?
o s ?
Cq}||Q|exp.
max = 1|Q| ?
?q?Qmaxo?Mq|{s ?
o and s ?
Cq}||o|The results for the TREC data are considerablylower than the results for the CBC data.
One ex-planation may be that in the CBC data, only sen-tences from one document containing the answerare considered.
In the TREC data, as in the TRECtask, it is not known beforehand which docu-ments contain answers, so irrelevant documentsexp.
max max minCBC training 72.7% 79.0% 24.4%TREC-8 48.8% 64.7% 10.1%Table 1: Maximum overlap analysis of scoresmay contain high-scoring sentences that distractfrom the correct sentences.In Table 2, we present a detailed breakdownof the MaxOset results for the CBC data.
(Notethat the classifications overlap, e.g., questions thatare in ?there is always a chance to get it right?are also in the class ?there may be a chance toget it right.?)
21% of the questions are literallyimpossible to get right using only term weight-ing because none of the correct sentences are inthe MaxOsets.
This result illustrates that maxi-mal overlap sets can identify the limitations of ascoring function by recognizing that some candi-dates will always be ranked higher than others.Although our analysis only considered term over-lap as a scoring function, maximal overlap setscould be used to evaluate other scoring functionsas well, for example overlap sets based on seman-tic classes rather than lexical items.In sum, the upper bound for term weightingschemes is quite low and the lower bound isquite high.
These results suggest that methodssuch as query expansion are essential to increasethe feature sets used to score answer candidates.Richer feature sets could distinguish candidatesthat would otherwise be represented by the samefeatures and therefore would inevitably receivethe same score.6 Analyzing the effect of multipleanswer type occurrences in a sentenceIn this section, we analyze the problem of extract-ing short answers from a sentence.
Many Q/Asystems first decide what answer type a questionexpects and then identify instances of that type insentences.
A scoring function ranks the possibleanswers using additional criteria, which may in-clude features of the surrounding sentence suchas term overlap with the question.For our analysis, we will assume that two shortanswers that have the same answer type and comefrom the same sentence are indistinguishable tothe system.
This assumption is made by manynumber of percentagequestions of questionsImpossible to get it wrong 159 24%(?ow ?
Mq, ?s ?
ow, s ?
Cq)There is always a chance to get it right 45 7%(?ow ?
Mq, ?s ?
ow s.t.
s ?
Cq)There may be a chance to get it right 310 48%(?ow ?
Mq s.t.
?s ?
ow s.t.
s ?
Cq)The wrong answers will always be weighted too highly 137 21%(?ow ?
Mq, ?s ?
ow, s 6?
Cq)There are no correct answers with any overlap with Q 66 10%(?s ?
d, s is incorrect or s has 0 overlap)There are no correct answers (auto scoring error) 12 2%(?s ?
d, s is incorrect)Table 2: Maximal Overlap Set Analysis for CBC dataQ/A systems: they do not have features that canprefer one entity over another of the same type inthe same sentence.We manually annotated data for 165 TREC-9 questions and 186 CBC questions to indicateperfect question typing, perfect answer sentenceidentification, and perfect semantic tagging.
Us-ing these annotations, we measured how much?answer confusion?
remains if an oracle gives youthe correct question type, a sentence containingthe answer, and correctly tags all entities in thesentence that match the question type.
For exam-ple, the oracle tells you that the question expectsa person, gives you a sentence containing the cor-rect person, and tags all person entities in that sen-tence.
The one thing the oracle does not tell youis which person is the correct one.Table 3 shows the answer types that we used.Most of the types are fairly standard, except forthe Defaultnp and Defaultvp which are defaulttags for questions that desire a noun phrase orverb phrase but cannot be more precisely typed.We computed an expected score for this hy-pothetical system as follows: for each question,we divided the number of correct candidates (usu-ally one) by the total number of candidates of thesame answer type in the sentence.
For example,if a question expects a Location as an answer andthe sentence contains three locations, then the ex-pected accuracy of the system would be 1/3 be-cause the system must choose among the loca-tions randomly.
When multiple sentences containa correct answer, we aggregated the sentences.
Fi-nally, we averaged this expected accuracy acrossall questions for each answer type.TREC CBCAnswer Type Score Freq Score Freqdefaultnp .33 47 .25 28organization .50 1 .72 3length .50 1 .75 2thingname .58 14 .50 1quantity .58 13 .77 14agent .63 19 .40 23location .70 24 .68 29personname .72 11 .83 13city .73 3 n/a 0defaultvp .75 2 .42 15temporal .78 16 .75 26personnoun .79 7 .53 5duration 1.0 3 .67 4province 1.0 2 1.0 2area 1.0 1 n/a 0day 1.0 1 n/a 0title n/a 0 .50 1person n/a 0 .67 3money n/a 0 .88 8ambigbig n/a 0 .88 4age n/a 0 1.0 2comparison n/a 0 1.0 1mass n/a 0 1.0 1measure n/a 0 1.0 1Overall .59 165 .61 186Overall-dflts .69 116 .70 143Table 3: Expected scores and frequencies for eachanswer typeTable 3 shows that a system with perfect ques-tion typing, perfect answer sentence identifica-tion, and perfect semantic tagging would stillachieve only 59% accuracy on the TREC-9 data.These results reveal that there are often multi-ple candidates of the same type in a sentence.For example, Temporal questions received an ex-pected score of 78% because there was usuallyonly one date expression per sentence (the correctone), while Default NP questions yielded an ex-pected score of 25% because there were four nounphrases per question on average.
Some commontypes were particularly problematic.
Agent ques-tions (most Who questions) had an answer con-fusability of 0.63, while Quantity questions had aconfusability of 0.58.The CBC data showed a similar level of an-swer confusion, with an expected score of 61%,although the confusability of individual answertypes varied from TREC.
For example, Agentquestions were even more difficult, receiving ascore of 40%, but Quantity questions were easierreceiving a score of 77%.Perhaps a better question analyzer could assignmore specific types to the Default NP and De-fault VP questions, which skew the results.
TheOverall-dflts row of Table 3 shows the expectedscores without these types, which is still about70% so a great deal of answer confusion remainseven without those questions.
The confusabilityanalysis provides insight into the limitations ofthe answer type set, and may be useful for com-paring the effectiveness of different answer typesets (somewhat analogous to the use of grammarperplexity in speech research).Q1: What city is Massachusetts General Hospital locatedin?A1: It was conducted by a cooperative group of on-cologists from Hoag, Massachusetts General Hospitalin Boston, Dartmouth College in New Hampshire, UCSan Diego Medical Center, McGill University in Montrealand the University of Missouri in Columbia.Q2: When was Nostradamus born?A2: Mosley said followers of Nostradamus, who livedfrom 1503 to 1566, have claimed ...Figure 9: Sentences with Multiple Items of theSame TypeHowever, Figure 9 shows the fundamentalproblem behind answer confusability.
Many sen-tences contain multiple instances of the sametype, such as lists and ranges.
In Q1, recognizingthat the question expects a city rather than a gen-eral location is still not enough because severalcities are in the answer sentence.
To achieve bet-ter performance, Q/A systems need use featuresthat can more precisely target an answer.7 ConclusionIn this paper we have presented four analyses ofquestion answering system performance involv-ing: multiple answer occurence, relative score forcandidate ranking, bounds on term overlap perfor-mance, and limitations of answer typing for shortanswer extraction.
We hope that both the resultsand the tools we describe will be useful to others.In general, we feel that analysis of good perfor-mance is nearly as important as the performanceitself and that the analysis of bad performance canbe equally important.ReferencesE.J.
Breck, J.D.
Burger, L. Ferro, L. Hirschman, D. House,M.
Light, and I. Mani.
2000.
How to Evaluate yourQuestion Answering System Every Day and Still GetReal Work Done.
In Proceedings of the Second Con-ference on Language Resources and Evaluation (LREC-2000).E.
Charniak, Y. Altun, R. de Salvo Braz, B. Garrett, M. Kos-mala, T. Moscovich, L. Pang, C. Pyo, Y.
Sun, W. Wy,Z.
Yang, S. Zeller, and L. Zorn.
2000.
Reading Compre-hension Programs in a Statistical-Language-ProcessingClass.
In ANLP/NAACL Workshop on Reading Com-prehension Tests as Evaluation for Computer-Based Lan-guage Understanding Systems.L.
Hirschman, M. Light, E. Breck, and J. Burger.
1999.Deep Read: A Reading Comprehension System.
In Pro-ceedings of the 37th Annual Meeting of the Associationfor Computational Linguistics.H.T.
Ng, L.H.
Teo, and J.L.P.
Kwan.
2000.
A MachineLearning Approach to Answering Questions for ReadingComprehension Tests.
In Proceedings of EMNLP/VLC-2000 at ACL-2000.E.
Riloff and M. Thelen.
2000.
A Rule-based QuestionAnswering System for Reading Comprehension Tests.In ANLP/NAACL Workshop on Reading ComprehensionTests as Evaluation for Computer-Based Language Un-derstanding Systems.TREC-8 Proceedings.
1999.
Proceedings of the EighthText Retrieval Conference (TREC8).
National Institute ofStandards and Technology, Special Publication 500-246,Gaithersburg, MD.TREC-9 Proceedings.
2000.
Proceedings of the Ninth TextRetrieval Conference (forthcoming).
National Instituteof Standards and Technology, Special Publication 500-XXX, Gaithersburg, MD.W.
Wang, Auer J., R. Parasuraman, I. Zubarev, D. Brandy-berry, and M.P.
Harper.
2000.
A Question AnsweringSystem Developed as a Project in a Natural LanguageProcessing Course.
In ANLP/NAACL Workshop on Read-ing Comprehension Tests as Evaluation for Computer-Based Language Understanding Systems.
