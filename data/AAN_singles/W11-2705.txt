Proceedings of the UCNLG+Eval: Language Generation and Evaluation Workshop, pages 33?38,Edinburgh, Scotland, UK, July 31, 2011. c?2011 Association for Computational LinguisticsExploring linguistically-rich patterns for question generationSe?rgio CurtoL2F/INESC-ID Lisbonsslc@l2f.inesc-id.ptAna Cristina MendesL2F/INESC-ID LisbonIST, Tech.
Univ.
Lisbonacbm@l2f.inesc-id.ptLu?
?sa CoheurL2F/INESC-ID LisbonIST, Tech.
Univ.
Lisbonlcoheur@inesc-id.ptAbstractLinguistic patterns reflect the regularities ofNatural Language and their applicability isacknowledged in several Natural LanguageProcessing tasks.
Particularly, in the task ofQuestion Generation, many systems dependon patterns to generate questions from text.The approach we follow relies on patternsthat convey lexical, syntactic and semantic in-formation, automatically learned from large-scale corpora.In this paper we discuss the impact of varyingseveral parameters during pattern learning andmatching in the Question Generation task.
Inparticular, we introduce semantics (by meansof named entities) in our lexico-syntactic pat-terns.
We evaluate and compare the numberand quality of the learned patterns and thematched text segments.
Also, we detail theinfluence of the patterns in the generation ofnatural language questions.1 IntroductionNatural Language (NL) is known for its variabilityand expressiveness.
There are hundreds of ways toexpress an idea, to describe a fact.
But language alsocomprises several regularities, or patterns, that de-note the presence of certain information.
For exam-ple, Paris is located in France is a common way tosay that Paris is in France, indicated by the wordslocated in.The use of patterns is a widely accepted as an ef-fective approach in the field of Natural LanguageProcessing (NLP), in tasks like Question-Answering(QA) (Soubbotin, 2001; Ravichandran and Hovy,2002) or Question Generation (QG) (Wyse and Pi-wek, 2009; Mendes et al, 2011).Particularly, QG aims at generating questionsfrom text and has became a vibrant line of re-search.
Generating questions (and answers), on onehand, allows QA or Dialogue Systems to be easilyported to different domains, by quickly providingnew questions to train the systems.
On the otherhand, it is useful for knowledge assessment-relatedtasks, by reducing the amount of time allocated forthe creation of tests by teachers (a time consumingand tedious task if done manually), or by allowingthe self evaluation of knowledge acquired by learn-ers.Most systems dedicated to QG are based on hand-crafted rules and rely on pattern matching to gener-ate questions.
For example, in (Chen et al, 2009),after the identification of key points, a situationmodel is built and question templates are used togenerate questions.
The Ceist system (Wyse and Pi-wek, 2009) uses syntactic patterns and the Tregextool (Levy and Andrew, 2006) that receives a setof hand-crafted rules and matches the rules againstparsed text, generating, in this way, questions (andanswers).
Kalady et al(2010) bases the QG taskin Up-keys (significant phrases in documents), parsetree manipulation and named entity recognition.Our approach to QG also relies on linguistic pat-terns, defined as a sequence of symbols that conveylexical, syntactic and semantic information, reflect-ing and expressing a regularity of the language.
Thepatterns associate a question to its answer and areautomatically learned from a set of seeds, based onlarge-scale information corpora, shallow parsing andnamed entities recognition.
The generation of ques-tions uses the learned patterns, as questions are cre-ated from text segments found in free text after beingmatched against the patterns.33This paper studies the impact on QG of vary-ing linguistic parameters during pattern learning andmatching.
It is organized as follows: in Sec.
2we introduce our pattern-based approach to QG; inSec.
3 we show the experiments and discuss results;in Sec.
4 we conclude and point to future work.2 Linguistically-Rich Patterns forQuestion GenerationThe generation of questions involves two phases: afirst offline phase ?
pattern learning ?
where pat-terns are learned from a set of seeds; and a sec-ond online phase ?
pattern matching and questiongeneration ?
where the learned patterns are matchedagainst a target document and the questions are gen-erated.
Next we describe these phases.Pattern Learning Our approach to pattern learn-ing is inspired by the work of Ravichandran andHovy (2002), who propose a method to learn pat-terns based on a two-step technique: the first ac-quires patterns from the Web given a set of seeds andthe second validates the patterns.
Despite the sim-ilarities, ours and Ravichandran and Hovy?s workhave some differences: our patterns also containsyntactic and semantic information and are not vali-dated.
Moreover, our seeds are well formulated NLquestions and their respective correct answers (in-stead of two entities), which allows to directly takeadvantage of the test sets already built and madeavailable in evaluation campaigns for QA systems(like Text REtrieval Conference (TREC) or CrossLanguage Evaluation Forum (CLEF)).We use a set of seeds, each composed by a NLquestion and its correct answer.
We start by classi-fying each seed question into a semantic category,in order to discover the type of information theseare seeking after: for example, the question ?Whopainted the Birth of Venus ??
asks for a person?sname.
Afterwards, we extract the phrase nodes ofeach seed question (excluding the Wh-phrase), en-close each in double quotes and submit them as aquery to a search engine.
For instance, given theseed ?Who painted the Birth of Venus ?
?/Botticelliand the syntactic structure of its question [WHNPWho] [VBD painted] [NP the Birth of Venus]1, we1The Penn Treebank II Tags (Bies et al, 1995) are used.build the query: "painted" "the Birth ofVenus" "Botticelli".We build patterns that associate the entitiesin the question to the answer from the top re-trieved documents.
From the sentence The Birthof Venus was painted around 1486 by Botti-celli, retrieved as result to the above query, welearn the pattern ?NP VBD[was] VBN PP[around1486]:[Date] IN:[by] NP{ANSWER}?2.
Thesyntactic labels without lexical information are re-lated with the constituents of the question, whilethose with ?{ANSWER}?
mark the answer.By creating queries with the inflected forms of themain verb of the question, we learn patterns wherethe surface form of the verb is different to that ofthe verb in the seed question (e.g., ?NP{ANSWER}VBD[began] VBG NP?
is learned from the sen-tence Botticelli began painting the Birth of Venus).The patterns generated by verb inflection are IN-FLECTED; the others are STRONG patterns.Our patterns convey linguistic information ex-tracted from the sentences in the documents whereall the constituents of the query exist.
The pat-tern is built with the words, their syntactic and se-mantic classes, that constitute the segments wherethose constituents are found.
For that, we per-form syntactic analysis and named entity recog-nition in each sentence.
In this paper, we ad-dress the impact of adding semantic informationto the patterns, that is, the difference in hav-ing a pattern ?NP VBD[was] VBN PP[around1486]:[Date] IN:[by] NP{ANSWER}?
with orwithout the named entity of type DATE, for instance.Pattern Matching and Question GenerationThe match of the patterns against a given free textis done at the lexical, syntactic and semantic lev-els.
We have implemented a (recursive) algorithmthat explores the parsed tree of the text sentences ina top-down, left-to-right, depth-first search, unifyingthe text with the linguistic information in the pattern.Also, we discard all matched segments in whichthe answer does not agree with the semantic cate-gory expected by the question.The generation of questions from the matched text2The patterns are more complex than the ones presented:they are linked to the seed question by indexes, mapping the po-sition of each of its components into the question constituents.34segments is straightforward, since we keep track ofthe syntactic structure of the questions and the sen-tences on the origin of the patterns.
There is a di-rect unification of all components of the text seg-ment with the constituents of the pattern.
In theINFLECTED patterns, the verb is inflected with thetense and person of the seed question and the auxil-iary verb is also used.3 Experiments3.1 Experimental SetupWe used the 6 seeds shown in Table 1, chosen be-cause the questions contain regular verbs and theyfocus on known entities ?
being so, it is probablethat there will be several texts in the Web referring tothem.
However, understanding the characteristics ofa pair that makes it a good seed is an important andpertinent question and a direction for future work.GId: 1Syntactic Structure: WHNP VBD NPSemantic Category: HUMAN:INDIVIDUAL?Who wrote Hamlet?
?/Shakespeare?Who painted Guernica?
?/Picasso?Who painted The Starry Night?
?/Van GoghGId: 2Syntactic Structure: WHADVP VBD NP VBNSemantic Category: NUMERIC:DATE?When was Hamlet written?
?/1601?When was Guernica painted?
?/1937?When was The Starry Night painted?
?/1889Table 1: Seeds used in the experiments.The syntactic analysis of the questions was doneby the Berkeley Parser (Petrov and Klein, 2007)trained on the QuestionBank (Judge et al, 2006).For question classification, we used Li and Roth(2002) taxonomy and a machine learning-basedclassifier fed with features derived from a rule-basedclassifier (Silva et al, 2011).For the learning of patterns we used the top64 documents retrieved by Google and to recog-nize the named entities in the pattern we applyseveral strategies, namely: 1) the Stanford?s Con-ditional Random-Field-based named entity recog-nizer (Finkel et al, 2005) to detect entities of typeHUMAN; 2) regular expressions to detect NUMERICand DATE type entities; 3) gazetteers to detect enti-ties of type LOCATION.For the generation of questions we used the top 16documents retrieved by the Google for 9 personali-ties from several domains, like literature (e.g., JaneAusten) and politics (e.g., Adolf Hitler).
We do nothave influence on the content of the retrieved doc-uments, nor perform any pre-processing (like textsimplification or anaphora resolution).
The BerkeleyParser (Petrov and Klein, 2007) was used to parsethe sentences, trained with the Wall Street Journal.3.2 Pattern Learning ResultsA total of 272 patterns was learned, from which 212are INFLECTED and the remaining are STRONG.
Onaverage, each seed led to 46 patterns.Table 2 shows the number of learned patterns oftypes INFLECTED and STRONG according to eachgroup of seed questions.
It indicates the number ofpatterns in which at least one named entity was rec-ognized (W) and the number of patterns which do notcontain any named entity (WO).
Three main resultsof the pattern learning phase are shown: 1) the num-ber of learned INFLECTED patterns is much higherthan the number of learned STRONG patterns: nearly80% of the patterns are INFLECTED; 2) most of thepatterns do not have named entities; and 3) the num-ber of patterns learned from the questions of group1 are nearly 70% of the total number of patterns.INFLECTED STRONGGId WO W WO W TOTAL1 127 19 36 8146 44 1902 40 26 10 666 16 82All 167 45 46 14212 60 272Table 2: Number of learned patterns.The following are examples of patterns and theactual sentences from where they were learned:?
?NP{ANSWER} VBZ NP?
: an INFLECTED patternlearned from group 1, from the sentence 1601William Shakespeare writes Hamlet in London.,without named entities;?
?NP VBD VBN IN[in] NP{ANSWER}?
: a35STRONG pattern learned from group 2, from thesentence (Guernica was painted in 1937.
), withoutnamed entities;?
?NNP VBZ[is] NP[a tragedy] ,[,]VBN[believed] VBN IN[between]NP[1599]:[NUMERIC COUNT,NUMERIC DATE]CC[and] NP{ANSWER}?
: an INFLECTED patternlearned from group 2, from the sentence WilliamShakespeare?s Hamlet is a tragedy , believed writtenbetween 1599 and 1601, with 1599 being recog-nized as named entity of type NUMERIC COUNTand NUMERIC DATE.3.3 Pattern Matching and QuestionGeneration ResultsRegarding the number of text segments matched inthe texts retrieved for the 9 personalities, Table 3shows that, from the 272 learned patterns, only 30(11%) were in fact effective (an effective patternmatches at least one text segment).
The most effec-tive patterns were those from group 2, as 12 from 82(14.6%) matched at least one instance in the text.GId INFLECTED STRONG TOTAL1 13 5 182 9 3 (2 W) 12All 22 8 30Table 3: Matched patterns.Regarding the patterns with named entities, onlythose from group 2 matched instances in the texts.The pattern that matched the most instances was?NP{ANSWER} VBD NP?, learned from group 1.In the evaluation of the questions, we use theguidelines of Chen et al (2009), who classify ques-tions as plausible ?
if they are grammatically correctand if they make sense regarding the text from wherethey were extracted ?
and implausible (otherwise).However, we split plausible questions in three cat-egories: 1) Pa for plausible, anaphoric questions,e.g., When was she awarded the Nobel Peace Prize?
;2) Pc for plausible questions that need a context tobe answered, e.g., When was the manuscript pub-lished?
; and 3) Pp, a plausible perfect question.
Ifa question can be marked both as PLa and PLc, wemark it as PLa.
Also, we split implausible questionsin: 1) IPi: for implausible questions due to incom-pleteness, e.g., When was Bob Marley invited?
; and2) IP: for questions that make no sense, e.g., Whenwas December 1926 Agatha identified?.A total of 447 questions was generated: 31 bySTRONG patterns, 269 by INFLECTED patterns and147 by both STRONG and INFLECTED patterns.
Wemanually evaluated 100 questions, randomly se-lected.
Results are in Table 4, shown accordingto the type (INFLECTED/STRONG) and presence ofnamed entities (W/WO) in the pattern that generatedthem.Pa Pc Pp IPi IP TotalINFLECTED 57WO 2 0 27 23 5STRONG 13W 1 0 1 0 1WO 1 2 3 3 1INFL/STR 30WO 0 0 9 18 3All 4 2 40 44 10 100Table 4: Evaluation of the generated questions.46 of the evaluated questions were consideredplausible and, from these, 40 can be used withoutmodifications.
From the 54 implausible questions,44 were due to lack of information in the question.69% (9 in 13) of the questions originated in STRONGpatterns were plausible.
This value is smaller forquestions generated by INFLECTED patterns: 50.8%(29 in 57).
Questions that had in their origin both aSTRONG and a INFLECTED pattern were mostly im-plausible, only 9 in 30 were plausible (30%).
Thepresence of named entities led to an increase ofquestions of only 3 (2 plausible and 1 implausible).3.4 DiscussionThe results concerning the transition from lexico-syntactic to lexico-syntactic-semantic patterns werenot conclusive.
There were 59 patterns with namedentities, but only 2 matched new text segments.Only 3 questions were generated from patternswith semantics.
We think that this happened due totwo reasons: 1) not all of the named entities in thepatterns were detected; and 2) the patterns containedlexical information that did not allow a match withthe text (e.g., ?NP{ANSWER} VBD[responded]36PP[in 1937]:textit[Date] WHADVP[when]NP[he] VBD NP?
requires the words responded,when and he.
)From a small set of seeds, our approach learnedpatterns that were later used to generate 447 ques-tions from previously unseen text.
In a sample of100 questions, 46% were judged as plausible.
Twoplausible questions are: ?Who had no real interestin the former German African colonies?
?, ?Whenwas The Road to Resurgence published??
and ?Wholaunched a massive naval and land campaign de-signed to seize New York?
?.The presence of syntactic information (a differ-ence between ours and Ravichandran and Hovy?swork) allows to relax the patterns and to gener-ate questions of various topics: e.g., the questions?Who invented the telegraph??
and ?Who di-rected the Titanic??
can be generated from match-ing the pattern ?NP VBD[was] VBN IN:[by]NP{ANSWER}?
with the sentences The telegraph wasinvented by Samuel Morse and The Titanic was di-rected by James Cameron, respectively.4 Conclusions and Future WorkWe presented an approach to generating questionsbased on linguistic patterns, automatically learnedfrom the Web from a set of seeds.
We addressed theimpact of adding semantics to patterns in matchingtext segments and generating new NL questions.We did not detect any improvement when addingsemantics to the patterns, mostly because the pat-terns with named entities did not match too manytext segments.
Nevertheless, from a small set of 6seeds, we generated 447 NL questions.
From these,we evaluated 100 and 46% were considered correctat the lexical, syntactic and semantic levels.In the future, we intend to pre-process the textsagainst which the patterns are matched and fromwhich the questions are generated.
Also, we areexperimenting this approach in another language.We aim at using more complex questions as seeds,studying its influence on the generation of questions.AcknowledgementsThis work was supported by FCT (INESC-ID mul-tiannual funding) through the PIDDAC Programfunds, and also through the project FALACOMIGO(ProjectoVII em co-promoc?a?o, QREN n 13449).Ana Cristina Mendes is supported by a PhD fel-lowship from Fundac?a?o para a Cie?ncia e a Tecnolo-gia (SFRH/BD/43487/2008).ReferencesAnn Bies, Mark Ferguson, Karen Katz, and Robert Mac-intyre.
1995.
Bracketing Guidelines for Treebank IIStyle Penn Treebank Project.Wei Chen, Gregory Aist, , and Jack Mostow.
2009.
Gen-erating questions automatically from informationaltext.
In The 2nd Workshop on Question Generation.Jenny Rose Finkel, Trond Grenager, and ChristopherManning.
2005.
Incorporating non-local informa-tion into information extraction systems by gibbs sam-pling.
In Proc.
43rd Annual Meeting on Associationfor Computational Linguistics, ACL ?05, pages 363?370.
ACL.John Judge, Aoife Cahill, and Josef van Genabith.
2006.Questionbank: creating a corpus of parse-annotatedquestions.
In ACL-44: Proc.
21st Int.
Conf.
on Com-putational Linguistics and the 44th Annual Meeting ofthe Association for Computational Linguistics, pages497?504.
ACL.Saidalavi Kalady, Ajeesh Elikkottil, and Rajarshi Das.2010.
Natural language question generation usingsyntax and keywords.
In The 3rd Workshop on Ques-tion Generation.Roger Levy and Galen Andrew.
2006.
Tregex and tsur-geon: tools for querying and manipulating tree datastructures.
In LREC 2006.Xin Li and Dan Roth.
2002.
Learning question classi-fiers.
In Proc.
19th Int.
Conf.
on Computational Lin-guistics, pages 1?7.
ACL.Ana Cristina Mendes, Se?rgio Curto, and Lu?
?sa Coheur.2011.
Bootstrapping multiple-choice tests with the-mentor.
In CICLing, 12th International Conferenceon Intelligent Text Processing and Computational Lin-guistics.Slav Petrov and Dan Klein.
2007.
Improved inferencefor unlexicalized parsing.
In Human Language Tech-nologies 2007: The Conference of the North AmericanChapter of the Association for Computational Linguis-tics; Proc.
Main Conference, pages 404?411.
ACL.Deepak Ravichandran and Eduard Hovy.
2002.
Learn-ing surface text patterns for a question answering sys-tem.
In ACL ?02: Proc.
40th Annual Meeting on As-sociation for Computational Linguistics, pages 41?47.ACL.Joa?o Silva, Lu?
?sa Coheur, Ana Mendes, and AndreasWichert.
2011.
From symbolic to sub-symbolic in-37formation in question classification.
Artificial Intelli-gence Review, 35:137?154.M.
M. Soubbotin.
2001.
Patterns of potential answerexpressions as clues to the right answers.
In Proc.
10thText REtrieval Conference (TREC), pages 293?302.Brendan Wyse and Paul Piwek.
2009.
Generating ques-tions from openlearn study units.
In The 2nd Workshopon Question Generation.38
