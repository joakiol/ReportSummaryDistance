Proceedings of the 53rd Annual Meeting of the Association for Computational Linguisticsand the 7th International Joint Conference on Natural Language Processing, pages 419?429,Beijing, China, July 26-31, 2015.c?2015 Association for Computational LinguisticsLearning to Adapt Credible Knowledge in Cross-lingual SentimentAnalysisQiang Chen?,?, Wenjie Li?,?, Yu Lei?, Xule Liu?, Yanxiang He?,?
?School of Computer Science, Wuhan University, China?Department of Computing, The Hong Kong Polytechnic University, Hong Kong?Hong Kong Polytechnic University Shenzhen Research Institute, China?The State Key Lab of Software Engineering, Wuhan University, China?
{qchen, xuleliu, yxhe}@whu.edu.cn?
{csqchen, cswjli, csylei}@comp.polyu.edu.hkAbstractCross-lingual sentiment analysis is a taskof identifying sentiment polarities of textsin a low-resource language by using sen-timent knowledge in a resource-abundantlanguage.
While most existing approachesare driven by transfer learning, theirperformance does not reach to a promisinglevel due to the transferred errors.
In thispaper, we propose to integrate into knowl-edge transfer a knowledge validation mod-el, which aims to prevent the negativeinfluence from the wrong knowledge bydistinguishing highly credible knowledge.Experiment results demonstrate the neces-sity and effectiveness of the model.1 IntroductionWith the wide range of business value, sentimentanalysis has drawn increasing attention in the pastyears.
The extensive research and developmentefforts produce a variety of reliable sentimentresources for English, one of the most popularlanguage in the world.
These available richresources become the treasure of knowledge tohelp conduct or enhance sentiment analysis inthe other languages, which is a task known ascross-lingual sentiment analysis (CLSA).
In theliterature of CSLA, the language with abundantreliable resources is called the source language(e.g., English), while the low-resource language isreferred to as the target language (e.g., Chinese).However, in this paper, the situation is a lowresource language scenario, where the sourcelanguage is English, and the target language isChinese.The main idea of existing CLSA researches isto first build up the connection between the sourceand target languages to overcome the languagebarrier, and then develop an appropriate knowl-edge transfer approach to leverage the annotateddata from the source language to train a sentimentclassification model in the target language, eithersupervised or semi-supervised.
In particular, theseapproaches exploit and convert the knowledgelearned from the source language to automaticallygenerate and expand the pseudo-training data forthe target language.The machine translation (MT) service is oneof the most common ways used to build thelanguage connection (Wan, 2008; Banea et al,2008; Wan, 2009; Wei and Pal, 2010; Gui etal., 2014).
Although it is claimed in Duh et al(2011) that the MT service is ripe for CLSA,the imperfect MT quality hinders existing MT-based CLSA approaches from the further advance.In our preliminary study, we find that even theGoogle translator1(i.e., one of the most widelyused online MT service (Shankland 2013)) mayunavoidably changes the sentiment polarity ofthe translated text, as illustrated below, with apercentage of around 10%.
[Original English Text]: I am at home on bedrest and desperate for something good to read.
[Sentiment Label: Negative][Translated Chinese Text]: ?3[?K>E??"???
?w" {Meaning: I am in bedto rest at home and feel that desperate things arealso good to read.
}[Sentiment Label: Positive]The noisy data generated by MT errors for surewill weaken the contribution of the transferredknowledge and even worse may create conflictingknowledge.
While it is a critical step in CLSA tolocalize the sentiment knowledge learned from thesource language in the target language, to the bestof our knowledge, hardly any previous researchhas focused on knowledge validation to filter outthe noisy knowledge having sentiment changescaused by wrong translations during knowledgetransfer.1http://translate.google.com419To reduce the noisy sentiment knowledge intro-duced into the target language, we are motivated tovalidate the knowledge transferred from the sourcelanguage by checking its linguistic distributionsand sentiment polarity consistency with the knownknowledge in the target language.
Differentfrom previous co-training based approaches wheretwo language views recommend knowledge toeach other in the same manner, we considerthe source language as the ?supervisor?
and thetarget language as the ?learner?.
The ?supervisor?boosts itself with its own accumulated labeled data(called knowledge) and meanwhile recommendsits confident knowledge to the ?learner?.
The?learner?
tries to select trustworthy knowledgebased on the recommendation to update andexpand its training data.
Adding a process toefficiently filter out noisy knowledge and retain theself-adaptive and interested new knowledge makesthe subsequent boosting process more credible.This is why our approach can outperform state-of-the-art CLSA approaches.The rest of this paper is organized as follows.Section 2 summarizes the related work.
Section 3explains the proposed model.
Section 4 presentsexperimental results.
Finally, Section 5 concludesthe paper and suggests future work.2 Related Work2.1 Sentiment AnalysisSentiment has been analyzed in different languagegranularity, e.g., entity, aspect, sentence anddocument.
This paper focuses on sentimentanalysis of online product reviews in the documentlevel.Existing approaches are generally categorizedinto lexicon-based and machine learning basedapproaches (Liu, 2012).
Lexicon-based approach-es highly depend on sentiment lexicons.
Turney(2002) derives the overall phrase and documentsentiment scores by averaging the sentimentscores provided in a lexicon over the wordsincluded.
Similar idea is adopted in (Hiroshi etal., 2004; Kennedy and Inkpen, 2006).
Machinelearning based approaches, on the other hand,apply classification models.
The task-specificfeatures are designed to train sentiment polarityclassifiers.
Pang et al (2002) compare theperformance of NB, SVM and ME on moviereviews.
SVM is found more effective.
Gamon(2004) shows that SVM with deep linguisticfeatures can further improve the performance.
Avariety of other machine learning approaches arealso proposed to sentiment classification (Mullenand Collier, 2004; Read, 2005; Hassan and Radev,2010; Socher et al, 2013).Cross-domain sentiment classification (CDSC)shares certain common characteristics with cross-lingual sentiment classification (CLSC) (Tan et al,2007; Li et al, 2009; Pan and Yang, 2010; He etal., 2011a; Glorot et al, 2011).
Notice that the gapbetween source domain and target domain is themain difference between CDSC and CLSC.
CLSCcopes with two different datasets in two differentlanguages.
This difference makes CLSC a newchallenge, drawing specific attention to researcherrecently.2.2 Cross-lingual Sentiment AnalysisThere are two alternative solutions to cross-lingualsentiment analysis.
One is ensemble learningthat combines multiple classifiers.
The other istransfer learning that develops strategies to adaptthe knowledge from one language to the other.Wan (2008) is among the pioneers to developthe ensemble learning solutions, where multipleclassifiers learned from different training datasetsincluding those in original languages and trans-lated languages are combined by voting.
Mostresearches, on the other hand, explore transferlearning and focus on knowledge adaptation.
Forexample, Wan (2009) applies a supervised co-training framework to iteratively adapt knowledgelearned from the two languages by transferringtranslated texts to each other.
Other similar workincludes (Wei and Pal, 2010) and (He, 2011b).
Allthese approaches rely on MT to build languageconnection.Meanwhile, the unlabeled parallel data is alsoemployed to fill the gap between two languages.To solve the feature coverage problem with theEM algorithm, Meng et al (2012) leverage theunlabeled parallel data to learn unseen sentimentwords.
Similarly, Popat et al (2013) use theunlabeled parallel data to cluster features in orderto reduce the data sparsity problem.
Meng etal.
(2012) and Popat et al (2013) also usethe unlabeled parallel data to reduce the negativeinfluence of the noisy and incorrect sentimentlabels introduced by machine translation andknowledge transfer.
However, the parallel data isalso a scarce resource.420Some existing transfer learning based CLSAmethods have attempted to address the noisyknowledge problem caused by wrong labels bychecking label consistency.
For example, tofilter out the unconfident labels in Chinese, thesupervised learning method proposed by (Xu etal., 2011) runs boosting in Chinese by checkingconsistency between the labels manually annotat-ed in English and predicted by Chinese classifierson translated Chinese.
The work in (Gui et al,2014) follows the same line although it considersknowledge transferring between two languages.On the contrary, the main focus of our work isto filter out the noisy knowledge having sentimentchanges by wrong translations.
Actually, bothlabel consistency checking and linguistic distribu-tion checking are important.
Any one alone cannotwork well.
In fact, both of them are considered asthe knowledge validation in our work, though thelater is our focus.3 Credible Boosting ModelIn this paper, we propose a knowledge validationapproach to improve the effectiveness of knowl-edge transfer without directly using extra paralleldata.
Our target is to filter out the noisy senti-ment labels introduced by MT and the incorrectsentiment labels generated by imperfect classifierin the source language.
Here, the knowledge isreferred to as a collection of distributed documentpresentations with sentiment labels that have beenverified to be robust in sentiment classification (Leand Mikolov, 2014).
A novel credible boostingmodel, namely CredBoost is proposed to applytransfer-supervised learning with an added self-validation mechanism to guarantee the knowledgetransferred highly credible and self-adaptive.3.1 Problem DescriptionIn a standard cross-lingual sentiment analysissetting, the training data includes labeled Englishreviews LEN= {(xleni, yi)}Mi=1and unlabeledChinese reviews UCN= {xucnj}Nj=1, where xki(k = lenor ucn) represents review i and yi?
{?1, 1} is the sentiment label of review xli.
Thetest data is Chinese reviews TCN= {xtcns}Ss=1.We now introduce the unlabeled data intocredBoost?s setting.
LENis divided into twodisjoint partsLTENandLBEN, whereLTENfor basictraining and LBENfor self-boosting.
We translateLENinto Chinese to obtain extra labeled Chinesepseudo-reviews LTrCN= {(xlcnTri, yi)}Mi=1andUCNinto English to obtain extra unlabeledEnglish pseudo-reviews UTrEN= {xlenTrj}Nj=1.Thereby, we obtain a pair of pseudo-parallel data(UCN, UTrEN).The task is to use LENand UCNto train aChinese classifier to predict sentiment polarity forthe test data TCN.
It is a standard transfer learningproblem.
We consider two language views, i.e.,source language view DSand target languageviewD?.
DSboosts itself with the labeled Englishdata and recommend translated knowledge to D?,while Dtselects self-adaptive ones to boost itself.3.2 Framework of CredBoostThe CredBoost model involves two synchronouslyboosting views for two languages respectively.During training, one view acts as a ?supervisor?that recommends and passes the knowledge to theother view.
The same knowledge is also addedinto its own view for boosting by automaticallyupdating the weights of the labeled data.
Theother view acts as a ?learner?
that receives therecommended knowledge and selects the best-suited new knowledge to learn.As mentioned before, the knowledge trans-ferred through MT is not reliable.
The sourcelanguage view may also make wrong predictionsand thus transfer the wrong knowledge to thetarget language even the translations are correct.Whether or not the ?learner?
can benefit fromits ?supervisor?
and how much it benefits highlydepends on the credibility and adaptiveness ofthe recommended knowledge accepted by the?learner?.
Knowledge validation is necessary toensure the quality of learning.
The objectiveof knowledge validation is to identify the newand acquired knowledge from recommendations.Both language views are iteratively trained untillearning converges or reaches the iteration upperbound.In the source language view, at iteration (t),the CredBoost model first uses LT (t)ENto train abasic classifier C(t)ENand then uses C(t)ENto predictLB(t)ENand U(t)TrEN.
Top m and top n instances aresampled from LB(t)ENand U(t)TrENrespectively, byFormula (1) :O(t)EN= {(xLBi?
, y?LBi?
)}meni?=1TR(t)EN= {(xUTri, y?UTri)}neni=1(1)where O(t)ENdenotes the candidates to be added421into the training data, and TR(t)ENthe knowledgeto be recommended to the target language view.We use the source knowledge validation functionVS(O(t)EN) to identify the acquired knowledgeK(t)?Aclearned in the previous learning process andthe new knowledge K(t)?Nwfresh to the currentknowledge system from O(t)EN.
The importance ofeach training instance is updated according to theperformance of prediction by Formula (2) :??Aci?
=???e(t)???(t)i??
c(t)i?if y?
?Aci?6= y?Aci???(t)i??
c(t)i?otherwise;??Nwj?
={e(t)?
log (1 +?e ?
c(t)j?)
if y?
?Acj?6= y?Acj?log (1 +?e ?
c(t)j?)
otherwise.
(2)where c(t)j?is the confidence of an instancegiven by C(t)EN, thus log (1 +?e ?
c(t)j?)
> 1 is toenhance the weight of new knowledge because ofthe higher significance contributing to the laterlearning.
?(t)i?
(< 1) is the adaptiveness scoregiven by the source knowledge validation functionVS(O(t)EN).
(t)(> 1) is the error rate of C(t)EN,thus e(t)> 1 is to reward the wrongly predicteddata in the next iteration.
y?
?Aci?is the labelgiven by C(t)ENand y?Aci?is the manually annotatedlabel.
For the incorrectly predicted instance, theweight is boosted inversely to the performanceof the current classifier.
The instance identifiedas the new knowledge which contributes moreto performance improvement is given a rewardparameter to enhance its significant in the nexttraining iteration.
Data sets update by Formula (3).The training starts with iteration (1), the trainingdata is initially set as LT (1)EN= LTEN.LT (t+1)EN= LT (t)EN?K(t)?Ac?K(t)?NwLB(t+1)EN= LB(t)EN?
(K(t)?Ac?K(t)?Nw)(3)In the target language view, at iteration (t),the CredBoost model receives the recommendedknowledge TR(t)ENand projects it to O(t)CNfromthe unlabeled Chinese data U(t)CNwith the pseudo-parallel data (U(t)CN, U(t)TrEN).
OCN(t)is validat-ed by the target knowledge validation functionV?
(O(t)CN) to identify the acquired knowledgeK(t)Acand the new knowledge K(t)Nw.
K(t)AcandK(t)Nware projected to K(t)?Acand K(t)?Nwfromthe unlabeled English pseudo-data U(t)TrEN.
Theweight of an instance is updated by Formula (4),and the parameter setting is similar to that inthe source language view.
The confidence c(t)iis directly transferred from Ds.
We reward thevalidated knowledge to raise their significance inthe training data considering they are originallyChinese.?Aci=?c(t)i?
log(1 +?e ?
v(t)i)?Nwj= elog (1+?e?c(t)j)= 1 +?e ?
c(t)j(4)We update the data setting by Formula (5).
Thetraining data is initially set as UT (1)CN= UTCN.
TheCredBoost model is illustrated in Algorithm 1.L(t+1)TrCN= L(t)TrCN?K(t)Ac?K(t)NwU(t+1)CN= U(t)CN?
(K(t)Ac?K(t)Nw)U(t+1)TrEN= U(t)TrEN?
(K(t)?Ac?K(t)?Nw)(5)Algorithm 1 CredBoost ModelInput: English labeled data LTENand LBEN, translatedEnglish unlabeled data UTrEN, translated Chinese dataLTrCNand unlabeled Chinese data UCN;Initialize: Weights W(1)EN= {1}Mfor LTENandW(1)TrCN= {1}Mfor LTrCN;For t = 1, ?
?
?
, T :1.
Use LT (t)ENto learn English classifier CEN(t);2.
Use C(t)ENto predict LB(t)ENand U(t)TrENsample topm and top n instances from LB(t)ENand U(t)TrEN, O(t)ENandTR(t)EN;3.
Validate O(t)ENby knowledge validation functionVS(O(t)EN) to identify acquired knowledge K(t)?Acand newknowledge K(t)?Nw, generate the weights for them byFormula (2), then recommend TR(t)ENto D?;4.
Project TR(t)ENto O(t)CNwith pseudo-parallel data(U(t)CN, U(t)TrEN), and use knowledge validation functionV?
(O(t)CN) to identify acquired knowledge K(t)Acand newknowledge K(t)Nw, then generate weights for them byFormula (4);5.
UpdateDSby Formula (2) andD?by Formula (5);End For.Output: Chinese classifier C(T )CN.3.3 Knowledge ValidationKnowledge is familiarity, awareness or under-standing of someone or something, such asfacts, information or skills, which is acquiredthrough experience or education by perceiving,discovering or learning2.
It can be implicit orexplicit.In machine learning, natural language knowl-edge is a continuously improving hypothesis thatconsists of both semantic and significant domain2Definition from Oxford Dictionary of English, avail-able at: http://oxforddictionaries.com/view/entry/m_en_us126.422characters.
While language is the expression ofsemantic, semantic is the carrier of sentiment.Using another word, two texts with more smallersemantic distance have higher probability to sharethe same sentiment polarity.
Choi and Cardie(2008) assert that the sentiment polarity of naturallanguage can be better inferred by compositionalsemantics.
They also suggest that incorporatingcompositional semantics into learning can im-prove the performance of sentiment classifiers.Saif et al (2012) also demonstrate that theaddition of extra semantic features can furtherimprove performance.In order to filter out noisy and incorrect senti-ment labels, we propose a knowledge validationapproach to reduce these noisy data that hinder theimprovement of learning performance.
Knowl-edge validation is a way to identify the acquiredknowledge implied in current knowledge systemand also the new knowledge fresh to currentknowledge system.
The knowledge can be repre-sented in the semantic space.
(Le and Mikolov,2014) project documents into a low-dimensionsemantic space with a deep learning approach,known as document-to-vector (Doc2Vec3).
Con-sidering that Dov2Vec has been verified to beefficient in many NLP tasks including sentimentanalysis, we follow previous research to representknowledge embedded in product reviews with thevectors generated by Doc2Vec.Suppose distributed representations (i.e., low-dimensional vectors) of the all reviews including{LTEN, LBEN, UTrEN} and {LTrCN, UCN}are {V(LTEN),V(LBEN),V(UTrEN)} and{V(LTrCN),V(UCN)} respectively.
At iteration(t), V(LT (t)EN) is the current knowledge systemof the English view and V(L(t)TrCN) is that ofthe Chinese.
The knowledge validation runsseparately in the source and target views.In the target language view, at iteration (t),suppose the prediction confidence of the candidate(xUi, y?Ui) ?
O(t)CNis c(t)i.
We define theadaptiveness score as the average distance of top?+semantic distances between the instance xLBiand the positive cluster of L(t)TrCN, denoted asL(t)+TrCN, and top ?
(t)?= ?+?L(t)+L(t)?semantic distancesbetween xUiand the negative cluster, denoted as3Doc2Vec is one of the models implemented in the freepython library Gensim which can be freely downloaded at:https://pypi.python.org/pypi/gensim.L(t)?TrCN, where L(t)+and L(t)?are the numbers ofthe elements in L(t)+TrCNand L(t)?TrCNrespectively.The validation parameters are defined by Formula(6), ?ris the weight of training instance V(r), ?
(t)iis the adaptiveness score, and Vlabel??
{1,?1} isthe validated label which denotes the knowledgebelonging to the positive cluster L(t)+TrCNor thenegative cluster L(t)?TrCN.
The validation processis illustrated in Algorithm 2, where the acquiredknowledge is k(t)Ac, and the new knowledge is k(t)Nw.D(V(xLBi),V(r)) =V(xLBi)T?
V(r)?
V(xLBi) ?
?
?
V(r) ????????(t)+i=1?+?r?L(t)+EN?rD(V(xLBi),V(r))?(t)?i=1?(t)??r??L(t)?EN?r?D(V(xLBi),V(r?))?
?(?
(t)i) = ?(t)+i?
?(t)?i?
?(t)i=1e1+?(?(t)i)?
Vlabel?={1 if ?
(t)i> 0.5,?1 if ?(t)i?
0.5.?
?(t)i={?
(t)+iif Vlabel?= 1,?
(t)?iif Vlabel?= ?1.
(6)where D(V(xLBi),V(r)) is the Cosine distancebetween the distributed representations of the tworeviews.
?
(t)+iand ?
(t)?iare the weighted averagesof the semantic distances.
?
(t)iis the Sigmoidfunction which computes the probability that thedata is distributed in the positive cluster L(t)+TrCN.In the source language view, at iteration(t), let?s suppose the prediction confidence ofcandidate (xLBi?, y?LBi?)
?
O(t)ENto be c(t)i?.
Thedefinitions of validation parameters are similarto those in the target language view.
Thevalidation process is illustrated in Algorithm 3.The validation is looser, because the training dataand candidates are both in English.
This differsfrom it in the target view.4 Experiments4.1 Experimental SetupWe evaluate the proposed CredBoost model onan open cross-lingual sentiment analysis task inNLP&CC 20134.
The data set provided is a4NLP&CC is an annual conference of Chinese infor-mation technology professional committee organized byChinese computer Federation (CCF).
It mainly focuseson the study and application novelty of natural languageprocessing and Chinese computation.
CLSA task is thetask 3 of NLP&CC 2013.
For more details and open423Algorithm 2 Knowledge Validation V?(D?
)Input: Labeled Chinese training data L(t)TrCN, weightsof labeled data W(t)CNand semantics vectors of allEnglish data for iteration (t): {V(L(t)TrCN),V(U(t)CN)};Initialize: K(1)?Ac= ?, K(1)?Nw= ?
;For xUiin O(t)CN:1.
Use L(t)TrCNto train a classifier C(t)CN,then use C(t)CNpredict xUi, giving la-bel yCNi;2.
Get validated label Vlabel?, positive andnegative average distances ?
(t)+i, ?
(t)?iof xUiby fomula (6);3.
If ?
(t)+i< ?
and ?
(t)?i< ?
:If y?LBi= Vlabel?
:Then K(t)Nw?
K(t)Nw+ xUi;Else:If y?LBi= Vlabel?= yCNi:Then K(t)Ac?
K(t)Ac+ xUi;End For.Output: K(t)Nw, K(t)Ac.Algorithm 3 Knowledge Validation VS(DS)Input: Weights of labeled data W(1)ENand semanticsvectors of all English data for iteration (t):{V(LT (t)EN),V(LB(t)EN),V(U(t)TrEN)};Initialize: K(1)?Ac= ?, K(1)?Nw= ?
;For xLBi?in O(t)EN:1.
Get validated label Vlabel?, positive andnegative average distances ?
(t)+i?, ?
(t)?i?of xLBi?by fomula (6);2.
If ?
(t)+i?< ?
and ?
(t)?i?< ?
:If y?LBi?= Vlabel?
:Then K(t)?Nw?
K(t)?Nw+ xLBi?
;Else:If y?LBi?= Vlabel?
:Then K(t)?Ac?
K(t)?Ac+ xLBi?
;End For.Output: K(t)?Nw, K(t)?Ac.collection of bilingual Amazon product reviewsin Books, DVD and Music domains.
It contains4,000 labeled English reviews, 4,000 Chinese testreviews, and 17,814, 47,071, 29,677 unlabeledChinese reviews in three different domains.
Werandomly select 2,000 unlabeled Chinese reviewsin each domain to train classifiers.
Besides, thepseudo-data sets described in CredBoost modelare translated with Google translator.
The data setis summarized in Table 1.To better illustrate the significance of knowl-edge validation during knowledge transfer, wecompare the proposed method with the followingbaseline methods:Lexicon-based (LB): The standard EnglishMPQA sentiment lexicons are translated intoresource, you can available at: http://tcci.ccf.org.cn/conference/2013/index.html.DomainEnglish ChineseL U L UBooksTrain 4,000 - - 2,000Test - - 4,000 -DVDTrain 4,000 - - 2,000Test - - 4,000 -MusicTrain 4,000 - - 2,000Test - - 4,000 -Table 1: Experimental data sets.
All data setsare balanced, L represents labeled data and Urepresents unlabeled data.Chinese and then utilized together with a smallnumber of Chinese turning words, negations andintensifiers to predict the sentiment polarities ofthe Chinese test reviews.Basic SVM (BSVM-CN): The labeled Englishreviews are translated into Chinese, which are thenused as the pseudo-training data to train a ChineseSVM classifier.Primarily boost transfer learning (BTL-1):The labeled English reviews are used to trainthe English classifier, which is applied to labelthe English translations of the unlabeled Chinesereviews.
These labeled Chinese reviews obtainedvia MT together with the Chinese translations ofthe labeled English reviews are then used as thepseudo-training data to train a Chinese sentimentclassifier.Best result in NLP&CC 2013 (BR2013): Thisis the best result reported in NLP&CC 2013.Unfortunately, the specification of the method isnot available.Self-boost (SB-CN) in Chinese: The labeledEnglish reviews are translated into Chinese, whichare used as the pseudo-training data to train a basicChinese classifier.
This classifier is iterativelyrefined by choosing the most confidently predictedEnglish reviews to add into the Chinese trainingdata until a predefined iteration number reaches.
Itcan be also considered as a self-adaptive boostingapproach.Iteratively boost transfer learning (BTL-2):This is an enhanced transfer learning method shar-ing the same learning framework with CredBoostbut it ignores knowledge validation.
It iterativelytransfers the knowledge from English to Chinese.The learning in both languages iteratively booststhemselves separately.
The transfer size is 16,comparable to that in CredBoost.Basic co-training (CoTr): The co-trainingmethod proposed in (Wan, 2009) is implemented.It is bidirectional transfer learning.
In each424iteration, 10 positive and 10 negative reviews aretransferred from one language to the other.Doc2vec feature CredBoost (dCredB): Thismethod is similar to CredBoost except thatdocument-to-vector is used to generate featureswhen training basic classifiers.
The vectorsare obtained from both original and translatedreviews.
The dimension of doc2vec is 300, whilethe other parameters are set as default.The baseline methods described above arecategorized into three classes: the first fourwhich are preliminary methods, the middle threewhich are several state-of-the-art models beingcomparable to our proposed model, and the lastone which is a comparison to suggest that theknowledge representation is not the answer to theperformance improvement.
For all the methodsexcluding LB and BR2013, we use support vectormachines (SVMs) as basic classifiers.
We usethe Liblinear package (Fan et al, 2008) with thelinear kernel5.
All methods use Unigram+Bigramfeatures to train the basic classifiers, except fordCredB.4.2 Experimental ResultIn this work, there are two main parameters thatmay significantly influence the performance of ourproposed model.
They are the new knowledgevalidation boundary ?
and the validation scale?+in the training data.
We set the values ofparameters with the grid search strategy.
Wefirst fix initial ?+= 14 to search the bestnew knowledge validation boundary ?
from anempirical value set {0.30, 0.35, 0.40, 0.45, 0.50}.We then fix the best ?
= 0.40 to check thesuitable validation scale ?+from the initial valueset {6, 8, 9, 10, 11, 12, 14, 16} in which values arecomparable with the knowledge transfer scaleof CoTr in the training data.
Besides, therecommendation size m for English is set to 20and the recommendation size n for Chinese is setto 40.
The final settings are listed in Table 2.The performance is evaluated in terms of accuracy(Ac) defined by Formula (7).Ac(f) =pfPf, Avg Ac =13??f??FAc(f?)
(7)where pfis the number of correct predictionsand Pfis the total number of the test data; F ?
{Books,DV D,Music} is the domain set.5The parameter setting used in this paper is ?-s 7?.Domain ?
?+m nBooks 0.45 12 20 40DVD 0.40 12 20 40Music 0.40 9 20 40Table 2: Parameter settings of three domains inthis paper.ApproachesDomainAvg AcBooks DVD MusicLB 0.7770 0.7832 0.7595 0.7709BSVM-CN 0.7940 0.7995 0.7778 0.7904BTL-1 0.8010 0.8058 0.7605 0.7891BR2013 0.7850 0.7773 0.7513 0.7712SB-CN 0.8400 0.8428 0.8012 0.8280BTL-2 0.8105 0.8265 0.7980 0.8117CoTr 0.8025 0.8508 0.7812 0.8115dCredB 0.6485 0.6753 0.6700 0.6646CredBoost 0.8465 0.8518 0.8093 0.8359Table 3: Macro performance of all approachesin three domains.
All values are accuracies andAvg-Ac represents the average accuracy in threedomains.The performances are reported in Tables 3 and4.
As shown, CredBoost outperforms all the othercomparison methods.
The first four baselineshave poor performances compared to others.
Thissuggests that the CLSA problem cannot be wellsolved by directly learning from the labeledtranslated data without any knowledge adaption orknowledge validation.
SB-CN, BTL-2 and CoTremploy iterative boosting to adapt knowledgefrom the source English to the target Chinese with-out validating the transferred knowledge.
Theyinevitably mis-recommend the massive noisy datainto Chinese.
CredBoost, in contrast, introducesknowledge validation into transfer learning withiterative boosting.
It better adapts knowledge fromEnglish to Chinese and thus ensures the credibilityof the accepted knowledge.
Its best result justifiesour assumption.Specifically, SB-CN leverages both the Chinesetraining data translated from the labeled Englishdata and the unlabeled Chinese data used forboosting.
The boosting in Chinese iterativelyselects the trustworthy data with the labels as-signed by the Chinese classifier.
Our proposedmethod, however, exploits two different languagessimultaneously with an additional boosting step,i.e., it transfers knowledge from English toChinese during boosting.
We then use knowledgevalidation model to validate the unlabeled Chinesedata whose labels are assigned by the English425Model (Books)Positive NegativeAcP R F1 P R F1LB 0.7368 0.8400 0.7850 0.8140 0.7000 0.7527 0.7700BSVM-CN 0.8249 0.7465 0.7837 0.7685 0.8415 0.8033 0.7940BTL-1 0.8537 0.7265 0.7850 0.7620 0.8755 0.8148 0.8010BR2013 - - - - - - 0.7850SB-CN 0.8716 0.7975 0.8329 0.8134 0.8825 0.8465 0.8400BTL-2 0.7105 0.8881 0.7894 0.9105 0.7588 0.8278 0.8105CoTr 0.8339 0.7555 0.7928 0.7765 0.8495 0.8114 0.8025dCredB 0.5310 0.6941 0.6017 0.7660 0.6202 0.6854 0.6485CredBoost 0.8225 0.8640 0.8427 0.8705 0.8306 0.8501 0.8465Model (DVD)Positive NegativeAcP R F1 P R F1LB 0.7648 0.8180 0.7905 0.8044 0.7485 0.7754 0.7832BSVM-CN 0.7745 0.8450 0.8082 0.8295 0.7540 0.7900 0.7995BTL-1 0.8282 0.7715 0.7988 0.7861 0.8400 0.8122 0.8058BR2013 - - - - - - 0.7773SB-CN 0.8853 0.7875 0.8335 0.8086 0.8980 0.8510 0.8428BTL-2 0.8525 0.8104 0.8309 0.8005 0.8444 0.8219 0.8265CoTr 0.8374 0.8705 0.8536 0.8652 0.8310 0.8478 0.8508dCredB 0.6070 0.7030 0.6515 0.7435 0.6542 0.6960 0.6753CredBoost 0.8440 0.8572 0.8508 0.8595 0.8465 0.8530 0.8518Model (Music)Positive NegativeAcP R F1 P R F1LB 0.7387 0.8030 0.7695 0.7842 0.7160 0.7485 0.7595BSVM-CN 0.8492 0.6755 0.7525 0.7306 0.8800 0.7984 0.7778BTL-1 0.8437 0.6395 0.7275 0.7097 0.8815 0.7863 0.7605BR2013 - - - - - - 0.7513SB-CN 0.8787 0.6990 0.7786 0.7501 0.9035 0.8197 0.8012BTL-2 0.7285 0.8461 0.7829 0.8675 0.7616 0.8111 0.7980CoTr 0.8536 0.6790 0.7564 0.7335 0.8835 0.8015 0.7812dCredB 0.5860 0.7043 0.6397 0.7540 0.6455 0.6955 0.6700CredBoost 0.7258 0.8708 0.7917 0.8928 0.7653 0.8241 0.8093Table 4: Micro performance of all approaches in three domains.
P: Precision, R: Recall, F1: micro-Fmeasure, Ac: Accuracy, and - represents unknown.
The model in BR2013 is unknown, thus its microperformance is unavailable.classifier.
It is reasonable that a Chinese classifierperforms better on Chinese text than an Englishclassifier performs on the translated English textdue to the different language distributions and MTerrors.
However, as shown in Tables 3 and 4,the better performance of our proposed methodcompared with that of the self-boosting methodfurther suggests the effectiveness of our proposedknowledge validation model.Figure 1 illustrates the continuous changes ofperformances vs. the corresponding growth sizesof the training data sets for SB-CN, BTL-2,CoTr, and CredBoost.
According to our commonsense, noisy data have negative influence onperformance improvement.
Compared to the otherthree methods, CredBoost accepts less number oftraining instances during learning while it achievesmore improvement.
This verifies the abilityof CredBoost that can filter out the noisy datarecommended by the English sentiment classifier.In Figure 1(a), the curves of BTL-2 and CoTrsuggest that directly transferring the knowledgerecommended from English imports many noisydata into Chinese.
It is also obvious that theperformance curve of CredBoost implies a stableimprovement trend while the other three decreaseafter certain iterations because of the accumulatednegative influence from the noisy data.
Figure1(b) shows CredBoost accepts decreased traininginstances after certain iterations because thenumber of ?high-quality?
instances decrease whenlearning proceeds.
This finding suggests thatknowledge validation would rather abandon ?less-credible?
knowledge with higher probability thaneasily accept it.
Knowledge validation in theproposed model guarantees highly-credible learn-ing when transferring knowledge from English toChinese.
The results also show that CredBoosthas great potential to achieve better performanceapproaching to supervised approaches if moreunlabeled Chinese data are available.Another interesting finding is also observed.426(a) Performances comparison in three domains(b) Growth sizes comparison in three domains0 20 40 60 80 100 1200.650.70.750.80.85Performance in Books domain.Iteration NumberAccuracyselfBoostCoTrBTL-2CredBoost0 20 40 60 80 100 1200.760.770.780.790.80.810.820.830.840.850.86Performance in DVD domain.Iteration NumberAccuracyselfBoostCoTrBTL-2CredBoost0 20 40 60 80 100 1200.60.650.70.750.8Performance in Music domain.Iteration NumberAccuracyselfBoostCoTrBTL-2CredBoost0 20 40 60 80 100 120051015202530Growth Sizes in Books domain.Iteration NumberNO.ofInstancesselfBoostCoTrBTL-2CredBoost0 20 40 60 80 100 120051015202530Growth Sizes in DVD domain.Iteration NumberNO.ofInstanceselfBoostCoTrBTL-2CredBoost0 20 40 60 80 100 1200510152025Growth Sizes in Music domain.Iteration NumberNO.ofInstanceselfBoostCoTrBTL-2CredBoostFigure 1: Performances vs. Growth Sizes for SB-CN, CoTr, BTL-2, and CredBoost in three domains.The similar performance curves of CoTr is also reported in (Gui et al, 2014).Although document-to-vector represents contentsemantic well, it cannot determine the sentimentpolarity of text well, even when the document-to-vectors that are used to train basic classifiersare learned on the mixture of the translated andoriginal reviews.
The superior performance ofCredBoost to dCredB suggests that the semanticrepresentation is effective to identify highly-credible acquired knowledge and new knowledgebut it alone may not be sufficient enough to modelthe sentiment information.We also conduct some other experiments tostudy the sensitivity of the new knowledge valida-tion boundary ?
and the validation scale ?+in thetraining data.
The experimental results show thatthe performances with different parameter settingsfluctuate around the best result reported in Tables3 and 4 in a small range.
Our model is basicallyquite stable.5 ConclusionIn this paper, we propose a semi-supervised learn-ing model, called CredBoost, to address cross-lingual (English vs Chinese) sentiment analysiswithout direct labeled Chinese data nor directparallel data.
We propose to introduce knowledgevalidation during transfer learning to reduce thenoisy data caused by machine translation errors orinevitable mistakes made by the source languagesentiment classifier.
The experimental resultdemonstrates the effectiveness of the proposedmodel.
In the future, we will explore more suitableknowledge representations and knowledge valida-tion in the CredBoost framework.AcknowledgementsWe thank all the anonymous reviewers for theirdetailed and insightful comments on this paper.The work described in this paper was supportedby the Research Grants Council of Hong Kongproject (PolyU 5202/12E and PolyU 152094/14E)and the grants from the National Natural Sci-ence Foundation of China (61272291, 61472290,61472291 and 61303115).ReferencesCarmen Banea and Rada Mihalcea, Janyce Wiebe,Samer Hassan.
2008.
Multilingual SubjectivityAnalysis Using Machine Translation.
In Proceed-ings of the 2008 Conference on Empirical Methodsin Natual Language Processing, pages 127-135,Honolulu, October.Carmen Banea, Yoonjung Choi, Lingjia Deng, SamerHassan, Michael Mohler, Bishan Yang, Claire427Cardie, Rada Mihalcea, Janyce Wiebe.
2013.
CPN-CORE: A Text Semantic Similarity System Infusedwith Opinion Knowledge.
In Proceedings of theMain Conference and the SHared Task in *SEM2013, pages 221-228, Atlanta, Georgia, June 13-14,2013.Yejin Choi and Claire Cardie.
2008.
Learning withCompositional Semantics as Structural Inference forSubsentential Sentiment Analysis.
In Proceedingsof the 2008 Conference on Empirical Methodsin Natural Language Processing, pages 792-801,Honolulu, October 2008.Kevin Duh and Akinori Fujino and Masaaki Nagata.2011.
Is Machine Translation Ripe for Cross-lingual Sentiment Classification?
In Proceedingsof the 49th Annual Meeting of the Association forComputational Linguistics: shortpapers, pages 429-433, Portland, Oregon, June 19-24, 2011.Rong-En Fan, Kai-Wei Chang, Cho-Jui Ksieh, Xiang-Rui Wang, Chih-Jen Lin.
2008.
LIBLINEAR: ALibrary for Large Linear Classification.
In Journalof Machine Learning Research, 9 (2008) 1871-1874.Micheal Gamon.
2004.
Sentiment Classificationon Customer Feedback Data: Noisy Data, LargeFeature Vectors and the Role of Linguistic Analysis.In Proceedings of the 20th International Conferenceon Computational Linguistics, pages 841-847, CH.Xavier Glorot, Antoine Bordes, and Yoshua Bengio.2011.
Domain Adaptation for Large-scale Senti-ment Classification: A Deep Learning Approach.In Proceedings of the 28th International Conferenceon Machine Learning, pages 513-520, Bellevue,Washington, USA.Lin Gui, Ruifeng Xu, Qin Lu, Jun Xu, Jian Xu,Bin Liu, Xiaolong Wang.
2014.
Cross-lingualOpinion Analysis via Negative Transfer Detection.In Proceedings of the 52th Annual Meeting of theAssociation for Computational Linguistics (shortpaper), pages 860-865, Baltimore, Maryland, USA,June 23-25 2014.Ahmed Hassan and Dragomir Radev.
2010.
I-dentifying Text Polarity Using Random Walks.In Proceedings of the 48th Annual Meeting onAssociation for Computational Linguistics, pages395-403, Uppsala, Sweden, 11-16 July 2010.Yulan He, Chenghua Lin, Harith Alani.
2011a.Automatically Extracting Polarity-bearing Topicsfor Cross Domain Sentiment Classification.
InProceedings of the 49th Annual Meeting of theAssociation for Computational Linguistics: HuamnLanguage Technologies, pages 123-131, Portland,Oregon, USA.Yulan He.
2011b.
Latent Sentiment Model for Weakly-Supervised Cross-Lingual Sentiment Classification.In Proceedings of the 33th European Conference onInformation Retrieval(ECIR 2011), 18-21 Apr 2011,Dublin, Ireland.KANAYAMA Hiroshi, NASUKAWAA Tetsuya,WATANABE Hideo.
2004.
Deeper SentimentAnalysis Using Machine Translation Technology.In Proceedings of the 20th International Conferenceon Computational Linguistics, pages 494-500.Alistair Kennedy and Diana Inkpen.
2006.
Senti-ment Classification of Movie and Product ReviewsUsing Contextual Valence Shifters.
ComputationalIntelligence,22(2):110-125.Quoc Le, Tomas Mikolov.
2014.
DistributedRepresentations of Sentences and Documents.
InProceedings of the 31th International Conference onMachine Learning, Beijing, China, 2014.
JMLR:W&CP volume 32.Tao Li, Vikas Sindhwani, Chris Ding, and Yi Zhang.2009.
Knowledge Transformation for Cross-Domain Sentiment Classification.
In Proceedingsof the 32th International ACM SIGIR Conferenceon Research and Development in Information Re-trieval, pages 716-717, Boston, MA, USA.Bing Liu.
May 2012.
Sentiment Analysis and OpinionMining.
Morgan & Claypool Publisher.Xinfan Meng, Furu Wei, Xiaohua Liu, Ming Zhou, GeXu, Houfeng Wang.
2012.
Cross-Lingual MixtureModel for Sentiment Classification.
In Proceedingsof the 50th Annual Meeting of the Association forComputational Linguistics, pages 572-581, Jeju,Republic of Korea, 8-14 July 2012.Tony Mullen and Nigel Collier.
2004.
Sentimentanalysis using support vector machines with di-verse inoformation sources.
In Proceedings ofthe Conference on Empirical Methods in NaturalLanguage Processing, pages 412-418, (July 2004)poster paper.Sinno Jialin Pan and Qiang Yang, Fellow, IEEE.
2010.A Survey on Transfer Learning.
In Journal of IEEETransactions on Knowledge and Data Engineering,Vol.22, NO.10, October 2010.Bo Pang and Lillian Lee, Shivakumar Vaithyanathan.2002.
Thumps Up?
Sentiment Classification usingMachine Learning Techniques.
In Proceedingsof the 2002 Conference on Empirical Methodsin Natural Language Processing, pages 79-86,Philadelphia, July 2002.Kashyap Popat, Balamurali A R, Pushpak Bhat-tacharyya and Gholamreza Haffari.
2013.
TheHaves and the Have-Nots: Leverage UnlabeledCorpora for Sentiment Analysis.
In Proceedingsof the 51th Annual Meeting of the Association forComputational Linguistics, pages 412-422, Sofia,Bulgaria, 4-9 August 2013.Jonathon Read.
2005.
Using Emotions to reduceDependency in Machine Learning Techniques forSentiment Classification.
In Proceedings of the 43thAnnual Meeting on Association for ComputationalLinguistics Student Research Workshop, pages 43-48.428Hassan Saif, Yulan He and Harith Alani.
2012.Semantic Sentiment Analysis of Twitter.
InProceedings of the 11th International SemanticsWeb Conference ISWC 2012, Boston, USA.Stephen Shankland.
2013.
Google Translate nowserves 200 millon people daily.
In CNET.
CBSInteractive Inc. May 18, 2013.Richard Socher, Alex Perelygin, Jean Y. Wu, JasonChuang, Chiristopher D. Manning, Andrew Y. Ngand Christopher Potts.
2013.
Recursive DeepModels for Semantics Computationality Over aSentiment Treebank.
In Proceedings of the Con-ference on Empirical Methods in Natural LanguageProcessing.Songbo Tan, Gaowei Wu, Huifeng Tang and XueqiCheng.
2007.
A Novel Scheme for Domain-transferProblem in the context of Sentiment Analysis.
InCIKM 2007, November 6-8, 2007, Lisboa, Portugal.Peter D. Turney.
2002.
Thumps Up or ThumpsDown?
Semantic Orientation Applied to Unsuper-vised Classification of Reviews.
In Proceedings ofthe 40th Annual Meeting of the Association for Com-putational Linguistics, pages 417-424, Philadelphia,July 2002.Xiaojun Wan.
2008.
Using Bilingual Knowledgeand Ensemble Technics for Unsupervised Chi-nese Sentiment Analysis.
In Proceedings of the2008 Conference on Empirical Methods in NatualLanguage Processing, pages 553-561, Honolulu,October 2008.Xiaojun Wan.
2009.
Co-Training for Cross-LingualSentiment Classification.
In Proceedings of the 47thAnnual Meeting of the ACL and the 4th IJCNLP ofthe AFNLP, pages 235-243, Suntec, Singapore, 2-7August 2009.Bin Wei and Christopher Pal.
2010.
Cross LingualAdaptation: An Experiment on Sentiment Classifi-cations.
In Proceedings of the 48 Annual Meeting ofthe Association for Computational Linguistics (shortpaper), pages 258-262, Uppsala, Sweden, 11-16July 2010.Ruifeng Xu, Jun Xu and Xiaolong Wang.
2011.Instance Level Transfer Learning for Cross LingualOpinion Analysis.
In Proceedings of the 2nd Work-shop on Computational Approaches to Subjectivityand Sentiment Analysis, ACL-HLT 2011, pages 182-188, 24 June, 2011, Portland, Oregon, USA.429
