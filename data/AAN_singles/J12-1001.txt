Affirmative Cue Words in Task-Oriented DialogueAgust?n Gravano?Universidad de Buenos AiresJulia Hirschberg?
?Columbia University?tefan Ben?u?
?Constantine the Philosopher Universityand Institute of Informatics,Slovak Academy of SciencesWe present a series of studies of affirmative cue words?a family of cue words such as ?okay?
or?alright" that speakers use frequently in conversation.
These words pose a challenge for spokendialogue systems because of their ambiguity: They may be used for agreeing with what the in-terlocutor has said, indicating continued attention, or for cueing the start of a new topic, amongother meanings.
We describe differences in the acoustic/prosodic realization of such functions ina corpus of spontaneous, task-oriented dialogues in Standard American English.
These resultsare important both for interpretation and for production in spoken language applications.
Wealso assess the predictive power of computational methods for the automatic disambiguation ofthese words.
We find that contextual information and final intonation figure as the most salientcues to automatic disambiguation.1.
IntroductionCUE PHRASES are linguistic expressions that may be used to convey explicit informationabout the discourse or dialogue, or to convey a more literal, semantic contribution.They aid speakers and writers in organizing the discourse, and listeners and readers inprocessing it.
In previous literature, these constructions have also been termed discoursemarkers, pragmatic connectives, discourse operators, and clue words.
Examples of cuephrases include now, well, so, and, but, then, after all, furthermore, however, in consequence,as a matter of fact, in fact, actually, okay, alright, for example, and incidentally.The ability to correctly determine the function of cue phrases is critical for importantnatural language processing tasks, including anaphora resolution (Grosz and Sidner1986), argument understanding (Cohen 1984), plan recognition (Grosz and Sidner 1986;Litman and Allen 1987), and discourse segmentation (Litman and Passonneau 1995).?
Departamento de Computaci?n, FCEyN, Universidad de Buenos Aires, Pabell?n I, Ciudad Universitaria,(C1428EGA) Buenos Aires, ARGENTINA.
E-mail: gravano@dc.uba.ar.??
E-mail: julia@cs.columbia.edu.?
E-mail: sbenus@ukf.sk.Submission received: 9 December 2009; revised submission received: 2 February 2011; accepted forpublication: 13 March 2011.?
2012 Association for Computational LinguisticsComputational Linguistics Volume 38, Number 1Furthermore, correctly determining the function of cue phrases using features of thesurrounding text can be used to improve the naturalness of synthetic speech in text-to-speech systems (Hirschberg 1990).In this study, we focus on a subclass of cue phrases that we term affirmativecue words (hereafter, ACWs), and that include alright, mm-hm, okay, right, anduh-huh, inter alia.
These words are frequent in spontaneous conversation, especially intask-oriented dialogue, and are heavily overloaded: Their possible discourse/pragmaticfunctions include agreeing with what the interlocutor has said, displaying interest andcontinued attention, and cueing the start of a new topic.
Some ACWs (e.g., alright, okay)are capable of conveying as many as ten different functions, as described in Section 3.Whereas ACWs thus form a subset of more general classes of utterances which havebeen studied in more general studies of cue words, cue phrases, discourse markers, feedbackutterances, linguistic feedback, acknowledgments, grounding acts, our focus is on this par-ticular subset of lexical items which may convey an affirmative response?but whichmay also convey many different meanings.
The disambiguation of these meanings webelieve is critical to the success of spoken dialogue systems.In the studies presented here, our goal is to extend our understanding of ACWs,in particular by finding descriptions of the acoustic/prosodic characteristics of theirdifferent functions, and by assessing the predictive power of computational methods fortheir automatic disambiguation.
This knowledge should be helpful in spoken languagegeneration and understanding tasks, including interactive spoken dialogue systems andapplications doing off-line analyses of conversational data, such as meeting segmenta-tion and summarization.
For example, spoken dialogue systems lacking a model of theappropriate realization of different uses of these words are likely to have difficulty inunderstanding and communicating with their users, either by producing cue phrasesin a way that does not convey the intended meaning or by misunderstanding users?productions.This article is organized as follows.
Section 2 reviews previous literature.
In Sec-tion 3 we describe the materials used in the present study from the Columbia GamesCorpus.
Section 4 presents a statistical description of the acoustic, prosodic, and con-textual characteristics of the functions of ACWs in this corpus.
In Section 5 we describeresults from a number of machine learning experiments aimed at investigating howaccurately ACWs may be automatically classified into their various functions.
Finally,in Section 6 we summarize and discuss our main findings.2.
Previous WorkCue phrases have received extensive attention in the computational linguistics litera-ture.
Early work by Cohen (1984) presents a computational justification for the impor-tance of cue phrases in discourse processing.
Using a simple propositional frameworkfor analyzing discourse, Cohen claims that, in some cases, cue phrases decrease thenumber of operations required by the listener to process ?coherent transmissions?
; inother cases, cue phrases are necessary to allow the recognition of ?transmissions whichwould be incoherent (too complex to reconstruct) in the absence of clues?
(page 251).Reichman (1985) proposes a model of discourse structure in which discourse com-prises a collection of basic constituents called context spaces, organized hierarchicallyaccording to semantic and logical relations called conversational moves.
In Reichman?smodel, cue phrases are portrayed as mechanisms that signal context space boundaries,specifying the kind of conversational move about to take place.
Grosz and Sidner (1986)introduce an alternative model of discourse structure formed by three interrelated2Gravano, Hirschberg, and Ben?u?
Affirmative Cue Words in Task-Oriented Dialoguecomponents: a linguistic structure, an intentional structure, and an attentional state.
Inthis model, cue phrases play a central role, allowing the speaker to provide informationabout all of the following to the listener:1) that a change of attention is imminent; 2) whether the change returns to a previousfocus space or creates a new one; 3) how the intention is related to other intentions;4) what precedence relationships, if any, are relevant (page 196).In a corpus study of spontaneous conversations, Schiffrin (1987) describes cue phrasesas syntactically detachable from a sentence, commonly used in initial position withinutterances, capable of operating at both local and global levels of discourse, and havinga range of prosodic contours.
As other authors, Schiffrin observes that cue phrasesprovide contextual coordinates for an utterance in the discourse?that is, they indicatethe discourse segment to which an utterance belongs.
However, she suggests that cuephrases only display discourse structure relations; they do not create them.
In a critiqueof Schiffrin?s work, Redeker (1991) proposes defining cue phrases as phrases ?utteredwith the primary function of bringing to the listener?s attention a particular kind oflinkage of the upcoming utterance with the immediate discourse context?
(page 1169).Prior work on the automatic classification of cue phrases includes a series of studiesperformed by Hirschberg and Litman (Hirschberg and Litman 1987, 1993; Litman andHirschberg 1990), which focus on differentiating between the discourse and sententialsenses of single-word cue phrases such as now, well, okay, say, and so in AmericanEnglish.
When used in a discourse sense, a cue phrase explicitly conveys informationabout the discourse structure; when used in a sentential sense, a cue phrase insteadconveys semantic information.
Hirschberg and Litman present two manually devel-oped classification models, one based on prosodic features, and one based on textualfeatures.
This line of research is further pursued by Litman (1994, 1996), who incorpo-rates machine learning techniques to derive classification models automatically.
Litmanuses different combinations of prosodic and text-based features to train decision-treeand rule learners, and shows that machine learning constitutes a powerful tool fordeveloping automatic classifiers of cue phrases into their sentential and discourse uses.Zufferey and Popescu-Belis (2004) present a similar study on the automatic classificationof like and well into discourse and sentential senses, achieving a performance close tothat of human annotators.Besides the binary division of cue phrases into discourse vs. sentential meanings,the Conversational Analysis (CA) literature describes items it terms linguistic feedbackor acknowledgments.
These include not only the computational linguists?
cue phrasesbut also expressions such as I see or oh wow, which CA research describes in terms ofattention, understanding, and acceptance by the speaker of a proposition uttered byanother conversation participant (Kendon 1967; Yngve 1970; Duncan 1972; Schegloff1982; Jefferson 1984).
Such items typically occur at the second position in commonadjacency pairs and include backchannels (also referred to as continuers), which?exhibit on the part of [their] producer an understanding that an extended unit of talkis underway by another, and that it is not yet, or may not be (even ought not yet be)complete; [they take] the stance that the speaker of that extended unit should continuetalking?
(Schegloff 1982, page 81), and agreements, which indicate the speaker?sagreement with a statement or opinion expressed by another speaker.
Allwood,Nivre, and Ahlsen (1992) distinguish four basic communicative functions of linguisticfeedback which enable conversational partners to exchange information: contact,perception, understanding, and attitudinal reactions.
These correspond respectively3Computational Linguistics Volume 38, Number 1to whether the interlocutor is willing and able to continue the interaction, perceive themessage, understand the message, and react and respond to the message.
Allwood,Nivre, and Ahlsen posit that ?simple feedback words, like yes, [...] involve a highdegree of context dependence?
(page 5), and suggest that their basic communicativefunction strongly depends on the type of speech act, factual polarity, and informationstatus of the immediately preceding communicative act.
Novick and Sutton (1994)propose an alternative categorization of linguistic feedback in task-oriented dialogue,which is based on the structural context of exchanges rather than on the characteristicsof the preceding utterance.
The three main classes in Novick and Sutton?s catalogueare: (i) other ?
ackn, where an acknowledgment immediately follows a contribution byother speaker; (ii) self ?
other ?
ackn, where self initiates an exchange, other eventuallycompletes it, and self utters an acknowledgment; and (iii) self + ackn, where self includesan acknowledgment in an utterance independently of other?s previous contribution.Substantial attention has been paid to subsets and supersets of words we includein our class of ACWs in the psycholinguistic literature in studies of grounding?the process by which conversants obtain and maintain a common ground of mutualknowledge, mutual beliefs, and mutual assumptions over the course of a conversation(Clark and Schaefer 1989; Clark and Brennan 1991).
Computational work on groundinghas been pursued for a number of years by Traum and colleagues (e.g., Traum and Allen1992; Traum 1994), who recently have described a corpus-based study of lexical andsemantic evidence supporting different degrees of grounding (Roque and Traum 2009).Our ACWs often occur in the process of establishing such common ground.Prosodic characteristics of the responses involved in grounding have been studied inthe Australian English Map Task corpus by Mushin et al (2003), who find that theseutterances often consist of acknowledgment contributions such as okay or yeh producedwith a ?non-final?
intonational contour, and followed by speech by the same speakerwhich appears to continue the intonational phrase.
Studies by Walker of informa-tionally redundant utterances (IRUs) (Walker 1992, 1996), utterances which express?a proposition already entailed, presupposed or implicated by a previous utterancein the same discourse situation?
(Walker 1993a, page 12), also include some of ourACWs, such as IRU prompts (e.g., uh-huh), which, according to Walker, ?add no newpropositional content to the common ground?
(Walker 1993a, page 32).
Walker adoptsthe term ?continuer?
from the Conversational Analysis school to further describe theseprompts (Walker 1993a).
Walker describes some intonational contours which are usedto realize IRUs in generation in Walker (1993a) and in Walker (1993b), examining 63 IRUtokens and finding five different types of contour used among them.As part of a larger project on automatically detecting discourse structure for speechrecognition and understanding tasks in American English, Jurafsky et al (1998) presenta study of four particular discourse/pragmatic functions, or dialog acts (Bunt 1989;Core and Allen 1997), closely related to ACWs: backchannel, agreement, incipientspeakership (indicating an intention to take the floor), and yes-answer (affirmativeanswer to a yes?no question).
The authors examine 1,155 conversations from the Switch-board database (Godfrey, Holliman, and McDaniel 1992), and report that the vast ma-jority of these four dialogue acts are realized with words like yeah, okay, or uh-huh.
Theyfind that the lexical realization of the dialogue act is the strongest cue to its identity (e.g.,backchannel is the preferred function for uh-huh and mm-hm), and report preliminaryresults on some prosodic differences across dialogue acts: Backchannels are shorter induration, have lower pitch and intensity, and are more likely to end in a rising intonationthan agreements.
Two related studies, part of the same project, address the automaticclassification of dialogue acts in conversational speech (Shriberg et al 1998; Stolcke4Gravano, Hirschberg, and Ben?u?
Affirmative Cue Words in Task-Oriented Dialogueet al 2000).
The results of their machine learning experiments, conducted on the samesubset of Switchboard used previously, indicate a high degree of confusion betweenagreements and backchannels, because both classes share words such as yeah and right.They also show that prosodic features (including duration, pause, and intensity) can aidthe automatic disambiguation between these two classes: A classifier trained using bothlexical and prosodic features slightly yet significantly outperforms one trained usingjust lexical features.There is also considerable evidence that linguistic feedback does not take placeat arbitrary locations in conversation; rather, it mostly occurs at or near transition-relevance places for turn-taking (Sacks, Schegloff, and Jefferson 1974; Goodwin 1981).Ward and Tsukahara (2000) describe, in both Japanese and American English, a regionof low pitch lasting at least 110 msec which may function as a prosodic cue invitingthe realization of a backchannel response from the interlocutor.
In a corpus studyof Japanese dialogues, Koiso et al (1998) find that both syntax and prosody play acentral role in predicting the occurrence of backchannels.
Cathcart, Carletta, and Klein(2003) propose a method for automatically predicting the placement of backchannelsin Scottish English conversation, based on pause durations and part-of-speech tags,that outperforms a random baseline model.
Recently, Gravano and Hirschberg (2009a,2009b, 2011) describe six distinct prosodic, acoustic, and lexical events in AmericanEnglish speech that tend to precede the occurrence of a backchannel by the interlocutor.Despite their high frequency in spontaneous conversation, the set of ACWs weexamine here have seldom, if ever, been an object of study in themselves, as a separatesubclass of cue phrases or dialogue acts.
Some have attempted to model other typesof cue phrases (e.g., well, like) or cue phrases in general; others discuss discourse/pragmatic functions that may be conveyed through ACWs, but which may also beconveyed through other types of expressions (e.g., agreements may be communicatedby single words such as yes or longer cue phrases such as that?s correct).
Subsets of ACWshave been studied in very small corpora, with some proposals about their prosodicand functional variations.
For example, Hockey (1993) examines the prosodic variationof two ACWs, okay and uh-huh (66 and 77 data points, respectively) produced as fullintonational phrases in two spontaneous task-oriented dialogues.
She groups the F0contours visually and auditorily, and shows that instances of okay produced with ahigh-rise contour are significantly more likely to be followed by speech from the otherspeaker than from the same speaker.
The results of a perception experiment conductedby Gravano et al (2007) suggest that, in task-oriented American English dialogue,contextual information (e.g., duration of surrounding silence, number of surroundingwords) as well as word-final intonation figure as the most salient cues to disambiguationof the function of the word okay by human listeners.
Also, in a study of the function ofintonation in Scottish English task-oriented dialogue, Kowtko (1996) examines a corpusof 273 instances of single-word utterances, including affirmative cue words such as mm-hm, okay, right, uh-huh, and yes.
Kowtko finds a significant correlation between discoursefunction and intonational contour: The align function (which checks that the listener?sunderstanding aligns with that of the speaker) is shown to correlate with rising in-tonational contours; the ready function (which cues the speaker?s intention to begina new task) and the reply-y function (which ?has an affirmative surface and usuallyindicates agreement?
; Kowtko 1996, page 59) correlate with a non-rising intonation;and the acknowledge function (which indicates having heard and understood) presentsall types of final intonation.
It is important to note, however, that different dialectsand different languages have distinct ways of realizing different discourse/pragmaticfunctions, so it is unclear how useful these results are for American English.5Computational Linguistics Volume 38, Number 1Although broader studies focusing on the pragmatic function of cue phrases, dis-course markers, linguistic feedback, and dialogue acts do shed light on the particularsubset of utterances we are studying, and although there is some information on par-ticular lexical items we include here in our study, the class of ACWs itself has receivedlittle attention.
Particularly given the frequency of ACWs in dialogue, it is importantto identify reliable and automatically extractable cues to their disambiguation, so thatspoken dialogue systems can recognize the pragmatic function of ACWs in user inputand can produce ACWs that are less likely to be misinterpreted in system output.3.
MaterialsThe materials for all experiments in this study were taken from the Columbia GamesCorpus, a collection of 12 spontaneous task-oriented dyadic conversations elicited from13 native speakers (6 female, 7 male) of Standard American English (SAE).
A detaileddescription of this corpus is given in Appendix A.
In each session, two subjects werepaid to play a series of computer games requiring verbal communication to achievejoint goals of identifying and moving images on the screen.
Each subject used a separatelaptop computer; they sat facing each other in a soundproof booth, with an opaquecurtain hanging between to allow only verbal communication.Each session contains an average of 45 minutes of dialogue, totaling roughly 9 hoursof dialogue in the corpus.
Trained annotators orthographically transcribed the re-cordings and manually aligned the words to the speech signal, yielding a total of70,259 words and 2,037 unique words in the corpus.
Additionally, self repairs andcertain non-word vocalizations were marked, including laughs, coughs, and breaths.For roughly two thirds of the corpus, intonational patterns and other aspects of theprosody were identified by trained annotators using the ToBI transcription framework(Beckman and Hirschberg 1994; Pitrelli, Beckman, and Hirschberg 1994).3.1 Affirmative Cue Words in the Games CorpusThroughout the Games Corpus, subjects made frequent use of affirmative cue words:The 5,456 instances of affirmative cue words alright, gotcha, huh, mm-hm, okay, right,uh-huh, yeah, yep, yes, and yup account for 7.8% of the total words in the corpus.
Becausethe usage of these words seems to vary significantly in meaning, we asked three labelersto independently classify all occurrences of these 11 words in the entire corpus into theten discourse/pragmatic functions listed in Table 1.Among the distinctions we make in these pragmatic functions, we note particularlythat our categories of Agr and BC differ primarily in that Agr is defined as indicatingbelief in or agreement with the interlocutor (e.g., a response to a yes?no question),whereas BC indicates only continued attention.11 Our definition of BC is similar to definitions of backchannel and continuer as discussed by a numberof authors in the Conversational Analysis and spoken language processing communities (e.g.,Stolcke et al?s [2000] ?a short utterance that plays discourse structuring roles, e.g., indicating that thespeaker should go on talking?
[page 345]; and Cathcart et al?s [2003] ?utterances, with minimal content,used to clearly signal that the speaker should continue with her current turn?
[page 51]).
Although somedefinitions of BC also include the notion that the speaker is indicating understanding, we did not askannotators to make this distinction.
We note further that, although it is also possible (cf.
Clark andSchaefer 1989) to signal understanding without agreement, in the process of designing the labelingscheme we did not find instances of ACWs that seemed to us to have this function in our corpus; nordid our labelers find such cases.
Hence we did not include this distinction among our classes.6Gravano, Hirschberg, and Ben?u?
Affirmative Cue Words in Task-Oriented DialogueTable 1Labeled discourse/pragmatic functions of affirmative cue words.Agr Agreement.
Indicates I believe what you said, and/or I agree with what you say.BC Backchannel.
Indicates only I hear you and please continue, in response to anotherspeaker?s utterance.CBeg Cue beginning discourse segment.
Marks a new segment of a discourse or a newtopic.CEnd Cue ending discourse segment.
Marks the end of a current segment of a discourseor a current topic.PBeg Pivot beginning (Agr+CBeg).
Functions both to agree and to cue a beginningsegment.PEnd Pivot ending (Agr+CEnd).
Functions both to agree and to cue the end of thecurrent segment.Mod Literal modifier.
Examples: I think that?s okay; to the right of the lion.BTsk Back from a task.
Indicates I?ve just finished what I was doing and I?m back.Chk Check.
Used with the meaning Is that okay?Stl Stall.
Used to stall for time while keeping the floor.?
Cannot decide.Labelers were given examples of each category, and annotated with access to bothtranscript and speech source.
The guidelines used by the annotators are presented inAppendix B.
Appendix C includes some examples of each class of ACWs, as labeledby our annotators.
Inter-labeler reliability was measured by Fleiss?s ?
(Fleiss 1971) asSubstantial at 0.745.2 We define the majority label of a token as the label chosen for thattoken by at least two of the three labelers; we assign the ???
label to a token either whenits majority label is ??
?, or when it was assigned a different label by each labeler.
Of the5,456 affirmative cue words in the corpus, 5,185 (95%) have a majority label other than??.?
Table 2 shows the distribution of discourse/pragmatic functions over ACWs in thewhole corpus.3.2 Data DownsamplingSome of the word/function pairs in Table 2 are skewed to contributions from a fewspeakers.
For example, for backchannel (BC) uh-huh, as many as 65 instances (44%) arefrom one single speaker, and the remaining 83 are from seven other speakers.
In caseslike this, using the whole sample would pose the risk of drawing false conclusions onthe usage of ACWs, possibly influenced by stylistic properties of individual speakers.Therefore, we downsampled the tokens of ACWs in the Games Corpus to obtain abalanced data set, with instances of each word and function coming in similar propor-tions from as many speakers as possible.
Specifically, we downsampled our data usingthe following procedure: First, we discarded all word/function pairs with tokens fromfewer than four different speakers; second, for each of the remaining word/functionpairs, we discarded tokens (at random) from speakers who contributed more than 25%of its tokens.
In other words, the resulting data set meets two conditions: For each word/2 The ?
measure of agreement above chance is interpreted as follows: 0 = None, 0?0.2 = Small,0.2?0.4 = Fair, 0.4?0.6 = Moderate, 0.6?0.8 = Substantial, 0.8?1 = Almost perfect.7Computational Linguistics Volume 38, Number 1Table 2Distribution of function over ACW.
Rest = {gotcha, huh, yep, yes, yup}.alright mm-hm okay right uh-huh yeah Rest TotalAgr 76 58 1,092 111 18 754 116 2,225BC 6 395 120 14 148 69 5 757CBeg 83 0 543 2 0 2 0 630CEnd 6 0 6 0 0 0 0 12PBeg 4 0 65 0 0 0 0 69PEnd 11 12 218 2 0 20 15 278Mod 5 0 18 1,069 0 0 0 1,092BTsk 7 1 32 0 0 0 0 40Chk 1 0 6 49 0 1 6 63Stl 1 0 15 1 0 2 0 19?
36 12 150 10 3 55 5 271Total 236 478 2,265 1,258 169 903 147 5,456function pair, (a) tokens come from at least four different speakers, and (b) no singlespeaker contributes more than 25% of the tokens.
The two thresholds were found viaa grid search, and were chosen as a trade-off between size and representativeness ofthe data set.
With this procedure we discarded 506 tokens of ACWs, or 9.3% of suchwords in the corpus.
Table 3 shows the resulting distribution of discourse/pragmaticfunctions over ACWs in the whole corpus after downsampling the data.
The ?
measureof inter-labeler reliability was practically identical for the downsampled data, at 0.751.3.3 Feature ExtractionWe extracted a number of lexical, discourse, timing, phonetic, acoustic, and prosodicfeatures for each target ACW, which we use in the statistical analysis and machinelearning experiments presented in the following sections.
Tables 4 through 8 summarizethe full feature set.
For simplicity, in those tables each line may describe one or morefeatures.
Features that may be extracted by on-line applications are marked with letterO; this is further explained later in this section.Table 3Distribution of function over ACW, after downsampling.
Rest = {gotcha, huh, yep, yes, yup}.alright mm-hm okay right uh-huh yeah Rest TotalAgr 76 58 1,092 74 16 754 87 2,157BC 0 395 120 0 101 58 0 674CBeg 61 0 543 0 0 0 0 604CEnd 0 0 4 0 0 0 0 4PBeg 0 0 64 0 0 0 0 64PEnd 10 4 218 0 0 18 0 250Mod 4 0 18 1,069 0 0 0 1,091BTsk 5 0 28 0 0 0 0 33Chk 0 0 5 49 0 0 4 58Stl 0 0 15 0 0 0 0 15Total 156 457 2,107 1,192 117 830 91 4,9508Gravano, Hirschberg, and Ben?u?
Affirmative Cue Words in Task-Oriented DialogueTable 4Lexical and discourse features.
Each line may describe one or more features.
Features marked Omay be available in on-line conditions.Lexical featuresO Lexical identity of the target word (w).O Part-of-speech tag of w, original and simplified.O Word immediately preceding w, and its original and simplified POS tags.
If w is precededby silence, this feature takes value ?#?.O Word immediately following w, and its original and simplified POS tags.
If w is followedby silence, this feature takes value ?#?.Discourse featuresO Number of words in w?s IPU.O Number and proportion of words in w?s IPU before and after w.O Number of words uttered by the other speaker during w?s IPU.O Number of words in the previous turn by the other speaker.Number of words in w?s turn.Number and proportion of words and IPUs in w?s turn before and after w.Number and proportion of turns in w?s task before and after w.Number of words uttered by the other speaker during w?s turn.Number of words in the following turn by the other speaker.Number of ACWs in w?s turn other than w.Our lexical features consist of the lexical identity and the part-of-speech (POS) tagof the target word (w), the word immediately preceding w, and the word immediatelyfollowing w (see Table 4).
POS tags were labeled automatically for the whole corpususing Ratnaparkhi, Brill, and Church?s (1996) maxent tagger trained on a subset of theSwitchboard corpus (Charniak and Johnson 2001) in lower-case with all punctuationremoved, to simulate spoken language transcripts.
Each word had an associated POStag from the full Penn Treebank tag set (Marcus, Marcinkiewicz, and Santorini 1993),and one of the following simplified tags: noun, verb, adjective, adverb, contraction,or other.For our discourse features, listed in Table 4, we define an inter-pausal unit (IPU) asa maximal sequence of words surrounded by silence longer than 50 msec.
A turn is amaximal sequence of IPUs from one speaker, such that between any two adjacent IPUsthere is no speech from the interlocutor.3,4 Boundaries of IPUs and turns are computedautomatically from the time-aligned transcriptions.
A task in the Games Corpus cor-responds to a simple game played by the subjects, requiring verbal communication toachieve a joint goal of identifying and moving images on the screen (see Appendix A fora description of these game tasks).
Task boundaries are extracted from the logs collectedautomatically during the sessions, and subsequently checked by hand.
Our discoursefeatures are intended to capture discrete positional information of the target word, inrelation to its containing IPU, turn, and task.3 Here ?between?
refers strictly to the time after the end point of the former IPU and before the start pointof the latter.4 Note that our operational definition of ?turn?
here includes all speaker utterances, includingbackchannels, which are typically not counted as turn-taking behaviors.
We use this more inclusivedefinition of ?turn?
here to avoid inventing a new term to encompass ?turns and backchannels?.9Computational Linguistics Volume 38, Number 1Our timing features (Table 5) are intended to capture positional information of atemporal nature, such as the duration (in milliseconds) of w and its containing IPUand turn, or the duration of any silence before and after w. These features also containinformation about the target word relative to the other speaker?s speech, including theduration of any overlapping speech, and the latencies between w?s conversational turnand the other speaker?s preceding and subsequent turns.Prosody was annotated following the ToBI system (Beckman and Hirschberg 1994;Pitrelli, Beckman, and Hirschberg 1994), which consists of annotations at four time-linked levels of analysis: an orthographic tier of time-aligned words; a tonal tier describingtargets in the fundamental frequency (F0) contour; a break index tier indicating degreesof juncture between words; and a miscellaneous tier, in which phenomena such asdisfluencies may be optionally marked.
The tonal tier describes events such as pitchaccents, which make words intonationally prominent and are realized by increased F0height, loudness, and duration of accented syllables.
A given word may be accentedor not and, if accented, may bear different tones, or different degrees of prominence,with respect to other words.
Five types of pitch accent are distinguished in the ToBIsystem for American English: two simple accents H* and L*, and three complex ones,L*+H, L+H*, and H+!H*.
An L indicates a low tone and an H, a high tone; theasterisk indicates which tone of the accent is aligned with the stressable syllable ofthe lexical item bearing the accent.
Some pitch accents may be downstepped, such thatthe pitch range of the accent is compressed in comparison to a non-downstepped accent.Downsteps are indicated by the ?!?
diacritic (e.g., !H*, L+!H*).
Break indices define twolevels of phrasing: Level 3 corresponds to Pierrehumbert?s (1980) intermediate phraseand level 4 to Pierrehumbert?s intonational phrase.
Level 4 phrases consist of one ormore level 3 phrases, plus a high or low boundary tone (H% or L%) indicated in thetonal tier at the right edge of the phrase.
Level 3 phrases consist of one or more pitchaccents, aligned with the stressed syllable of lexical items, plus a phrase accent, whichalso may be high (H-) or low (L-).
For example, a standard declarative contour consistsTable 5Timing features.
Each line may describe one or more features.
Features marked O may beavailable in on-line conditions.Timing featuresO Duration (in msec) of w (raw, normalized with respect to all occurrences of the same wordby the same speaker, and normalized with respect to all words with the same number ofsyllables and phonemes uttered by the same speaker).O Flag indicating whether there was any overlapping speech from the other speaker.O Duration of w?s IPU.O Latency (in msec) between w?s turn and the previous turn by the other speaker.O Duration of the silence before w (or 0 if the w is not preceded by silence), its IPU, and itsturn.O Duration and proportion of w?s IPU elapsed before and after w.O Duration of w?s turn before w.O Duration of any overlapping speech from the other speaker during w?s IPU.O Duration of the previous turn by the other speaker.Duration of the silence after w (or 0 if w is not followed by silence), its IPU, and its turn.Latency between w?s turn and the following turn by the other speaker.Duration of w?s turn, as a whole and after w.Duration of any overlapping speech from the other speaker during w?s turn.Duration of the following turn by the other speaker.10Gravano, Hirschberg, and Ben?u?
Affirmative Cue Words in Task-Oriented DialogueFigure 1A standard declarative contour (left), and a standard yes?no question contour.
The top panesshow the waveform and the fundamental frequency (F0) track.of a sequence of H* pitch accents ending in a low phrase accent and low boundarytone (L-L%); likewise, a standard yes?no question contour consists of a sequence of L*pitch accents ending in H-H%.
These are illustrated in Figure 1.
In our study, prosodicfeatures include the ToBI labels as specified by the annotators, and also a simplifiedversion of the labels, considering only high and low pitch targets (i.e., H* vs. L* forpitch accents, H- vs. L- for phrase accents, and H% vs. L% for boundary tones), andsimplified break indices (0?4).
These are listed in Table 6.All acoustic features were extracted automatically for the whole corpus using thePraat toolkit (Boersma and Weenink 2001).
These include pitch, intensity, stylized pitch,ratio of voiced frames to total frames, jitter, shimmer, and noise-to-harmonics ratio(NHR) (see Table 7).
Pitch features capture how high the speaker?s voice sounds or howlow.
Intensity is correlated with how loud the speaker sounds to a hearer.
The voiced-frames ratio roughly approximates the speaking rate.
Jitter and shimmer correspond tovariability in the frequency and amplitude of vocal-fold vibration, respectively.
NHRis the energy ratio of noise to harmonic components in the voiced speech signal.
Jitter,shimmer, and NHR correlate with perceptual evaluations of voice quality, such as harsh,whispery, creaky, and nasalized, inter alia.
Pitch slope features capture elements ofthe intonational contour, and were computed by fitting least-squares linear regressionmodels to the F0 data points extracted from given portions of the signal, such as afull word or its last 200 msec.
This procedure is illustrated in Figure 2, which showsthe pitch track of a sample utterance (blue dots) with three linear regressions, com-puted over the whole utterance (solid black line), and over the final 300 and 200 msec(?A?
and ?B?
dashed lines, respectively).
We used a similar procedure to computeTable 6Prosodic features.
In all cases, both original and simplified ToBI labels were considered.
Each linemay describe one or more features.
Features marked O may be available in on-line conditions.ToBI prosodic features?
Phrase accent, boundary tone, break index, and pitch accent on w.?
Phrase accent, boundary tone, break index, and final pitch accent on the final intonationalphrase of the previous turn by the other speaker (these features are defined only when wis turn initial).11Computational Linguistics Volume 38, Number 1Table 7Acoustic features.
Each line may describe one or more features.
Features marked O may beavailable in on-line conditions.Acoustic featuresO w?s mean, maximum, and minimum pitch and intensity (raw and speaker normalized).O Jitter and shimmer, computed over the whole word and over the first and secondsyllables, computed over just the voiced frames (raw and speaker normalized).O Noise-to-harmonics ratio (NHR), computed over the whole word and over the first andsecond syllables (raw and speaker normalized).O w?s ratio of voiced frames to total frames (raw and speaker normalized).O Pitch slope, intensity slope, and stylized pitch slope, computed over the whole word, itsfirst and second halves, its first and second syllables, the first and second halves of eachsyllable, and the word?s final 100, 200, and 300 msec (raw and normalized with respect toall other occurrences of the same word by the same speaker).O w?s mean, maximum, and minimum pitch and intensity, normalized with respect to threetypes of context: w?s IPU, w?s immediately preceding word by the same speaker, and w?simmediately following word by the same speaker.O Voiced-frames ratio, jitter, and shimmer, normalized with respect to the same three typesof context.O Mean, maximum, and minimum pitch and intensity, ratio of voiced frames, (all raw andspeaker normalized), jitter, and shimmer, calculated over the final 500, 1,000, 1,500 and2,000 msec of the previous turn by the other speaker (only defined when w is turn initialbut not task initial).O Pitch slope, intensity slope, and stylized pitch slope, calculated over the final 100, 200,300, 500, 1,000, 1,500, and 2,000 msec of the previous turn by the other speaker (onlydefined when w is turn initial but not task initial).intensity slopes (which capture changes in perceived loudness) and stylized pitchslopes (which capture more coarse-grained characteristics of the intonational contour).Stylized pitch curves were obtained using the algorithm provided in Praat: Look upthe pitch point p that is closest to the straight line L that connects its two neighboringpoints; if p is further than four semitones away from L, end; otherwise, remove p andstart over.Figure 2Sample pitch track with three linear regressions: computed over the whole IPU (bold line), andover the final 300 msec (A) and 200 msec (B).12Gravano, Hirschberg, and Ben?u?
Affirmative Cue Words in Task-Oriented DialogueAll features related to absolute (i.e., unnormalized) pitch values, such as maximumpitch or final pitch slope, are not comparable across genders because of the differentpitch ranges of female and male speakers?roughly 75?500 kHz and 50?300 kHz, respec-tively.
Therefore, before computing those features we applied a linear transformationto the pitch track values, thus making the pitch range of speakers of both gendersapproximately equivalent.
We refer to this process as gender normalization.
All othernormalizations were calculated using z-scores: z = (x ?
?
)/?, where x is a raw mea-surement to be normalized (e.g., the duration of a particular word), and ?
and ?
arethe mean and standard deviation of a certain population (e.g., all instances of the sameword by the same speaker in the whole conversation).For our phonetic features (listed in Table 8), we trained an automatic phone rec-ognizer based on the Hidden Markov Model Toolkit (HTK) (Young et al 2006), usingthree corpora as training data: the TIMIT Acoustic-Phonetic Continuous Speech Corpus(Garofolo et al 1993), the Boston Directions Corpus (Hirschberg and Nakatani 1996),and the Columbia Games Corpus.
With this recognizer, we obtained automatic time-aligned phonetic transcriptions of each instance of alright, mm-hm, okay, right, uh-huh,and yeah in the corpus.
To improve accuracy, we restricted the recognizer?s grammar toaccept only the most frequent variations of each word, as shown in Table 9.
We extractedour phonetic features, such as phone and syllable durations, from the resulting time-aligned phonetic transcriptions.
The remaining five ACWs in our corpus (gotcha, huh,yep, yes, and yup) had too low counts to contain meaningful phonetic variation; thus, wedid not compute phonetic features for those words.Finally, our session-specific features include the session of the Games Corpus inwhich the target word was produced, along with the identity and gender of bothTable 8Phonetic and session-specific features.
Each line may describe one or more features.
Featuresmarked O may be available in on-line conditions.Phonetic featuresO Identity of each of w?s phones.O Absolute and relative duration of each phone.O Absolute and relative duration of each syllable.Session-specific features?
Session number.?
Identity and gender of both speakers.Table 9Restricted grammars for the automatic speech recognizer.
Phones in square brackets are optional.ACW ARPAbet Grammaralright (aa|ao|ax) r (ay|eh) [t]mm-hm m hh mokay [aa|ao|ax|m|ow] k (ax|eh|ey)right r (ay|eh) [t]uh-huh (aa|ax) hh (aa|ax)yeah y (aa|ae|ah|ax|ea|eh)13Computational Linguistics Volume 38, Number 1speakers (Table 8).
These features were solely intended for searching for speaker ordialogue dependencies.Also, to simulate the conditions of on-line applications, which process speech asit is produced by the user, we distinguish a subset of features that may typically beextracted from the speech signal only up to the IPU containing the target ACW.
InTables 4 through 8 these features are marked with letter O (for on-line).
All on-line fea-tures can be computed automatically in real time by state-of-the-art speech processingapplications, although it should be noted that all of our lexical and discourse featuresstrongly rely on a speech recognizer output, which typically has a high error rate forspontaneous productions.
All on-line features are also available in off-line conditions;the remaining features (those not tagged O in Tables 4 through 8) are normally availableonly in offline conditions.
We distinguish online features for the machine learning exper-iments described in Section 5, in which we assess, among other things, the usefulnessof information contained in different feature sets, simulating the conditions of actualon-line and off-line applications.In the following sections, we use the features described here in several ways.
Wefirst perform a series of statistical tests to find differences across the various func-tions of ACWs.
Subsequently, we experiment with machine learning techniques for theautomatic classification of the function of ACWs, training the models with differentcombinations of features.4.
Characterizing Affirmative Cue WordsIn this section we present results of a series of statistical tests aimed at identifying con-textual, acoustic, and prosodic differences in the production of the various discourse/pragmatic functions of affirmative cue words.
This kind of characterization is importantboth for interpretation and for production in spoken language applications: If we canfind reliable features that effectively distinguish the various uses of these words, we canhope to interpret them automatically and generate them appropriately.4.1 Position in IPU and TurnWe begin this analysis by looking at the discourse position of the various discourse/pragmatic functions of ACWs.
Because these words help shape, or at least reflect,the structure of conversations, we expect to find positional differences between theirfunctions.
Figure 3 shows the distribution of the six most frequent ACWs in the corpus(alright, okay, yeah, mm-hm, uh-huh, and right) with respect to their position in their IPU.An IPU-initial word is one that occurs in the first position in its corresponding IPU; thatis, it is preceded by at least 50 msec of silence and followed by another word.
An IPU-final word occurs last in its IPU.
An IPU-medial word is both immediately precededand followed by other words.
Lastly, a single-word IPU is an individual word bothpreceded and followed by silence.
Figure 3 also depicts the distribution of discourse/pragmatic functions within each of these four categories.
For example, roughly 40% ofall tokens of alright in the corpus occur as IPU initial; of those, about half are agreements(Agr), half are cues to beginning discourse segments (CBeg), and a marginal numberconvey other functions.Similarly, Figure 4 shows the distribution of the same six ACWs with respect to theirposition in the corresponding conversational turn.
Turn-initial, turn-medial, and turn-final words, and single-word turns are defined analogously to the four IPU-relatedcategories defined previously, but considering conversational turns instead of IPUs.14Gravano, Hirschberg, and Ben?u?
Affirmative Cue Words in Task-Oriented DialogueFigure 3Position of the target word in its IPU.Figure 4Position of the target word in its turn.From these figures we observe several interesting aspects of the discourse positionof ACWs in the Games Corpus.
Only a minority of these words occur as IPU medialor IPU final.
The only exception appears to be right, for which a high proportion ofinstances do occur in such positions?mainly tokens with the literal modifier (Mod)meaning, but also tokens used to check with the interlocutor (Chk), which take place atthe end of a turn (and thus, of an IPU).The default function of ACWs, agreement (Agr), occurs for alright, okay, yeah, andright in all possible positions within the IPU and the turn; for mm-hm and uh-huh,agreements occur mostly as full conversational turns.
Nearly all backchannels (BC)occur as separate turns, with only a handful of exceptions: In four cases, the backchannelis followed by a pause in which the interlocutor chooses not to continue speaking, andthe utterer of the backchannel takes the turn; in two other cases, two backchannels areuttered in fast repetition (e.g., uh-huh uh-huh).From the six lexical items analyzed in Tables 3 and 4, two pairs of words seem topattern similarly.
The first such pair consists of mm-hm and uh-huh, which show verysimilar distributions and are realized almost always as single-word turns, as either Agror BC.
The second pair of words with comparable patterns of IPU and turn positionare alright and okay.
These are precisely the only two ACWs used to convey all tendiscourse/pragmatic functions in the Games Corpus (recall from Table 2).
This resultsuggests that the lexical items in these two pairs may be used interchangeably inconversation.
The word yeah presents a pattern analogous to that of alright and okay,albeit with fewer meanings.In all, these findings confirm the existence of large differences in the discourseposition of ACWs between their functional types, as well as between their lexicaltypes.
We will revisit this topic in Section 5, where we discuss the predictive power15Computational Linguistics Volume 38, Number 1Figure 5ToBI phrase accents and boundary tones.
The ?other?
category consists of cases with no phraseaccent and/or boundary tone present at the target word.of discourse features in the automatic classification of the function of ACWs.
Given theobserved positional differences, we expect these features to play a prominent role insuch a task.4.2 Word-Final IntonationShifting our attention to acoustic/prosodic characteristics of ACWs, we examine nextthe manner in which word-final intonation varies across ACW functions.
First we lookat two categorical variables in the ToBI framework which capture the final pitch incur-sion: phrase accent and boundary tone.
Figure 5 shows the distribution of ToBI labels foreach of the six most frequent ACWs and their corresponding functions (see Section 3.3for a description of the ToBI labeling conventions).
The distributions for alright, okay,right, and yeah depart significantly from random (alright: Fisher?s Exact test, p = 0.0483;okay: Pearson?s Chi-squared test, ?2(24) = 261, p ?
0; right: Pearson, ?2(8) = 220, p ?
0;yeah: Fisher, p ?
0).5,6 For right, considering just its discourse/pragmatic functions (i.e.,excluding its Mod instances), the distribution also significantly differs from random(Fisher, p ?
0).
On the other hand, the distributions for mm-hm and uh-huh do not departsignificantly from random.5 Fisher?s Exact test was used whenever the accuracy of Pearson?s Chi-squared test was compromised bydata sparsity.6 We performed statistical tests for approximately 35 variables on the same data set.
Applying theBonferroni correction, the alpha value should be lowered from the standard 0.05 to 0.05/35 ?
0.0014 tomaintain the familywise error rate.
Thus, a result would be significant when p < 0.0014.
According tothis, most tests are still significant in the current section; however, the Tukey post hoc tests following ourANOVA tests are not: most of these have a confidence level of 95%, and significant differences begin todisappear when considering a confidence level of 99%.16Gravano, Hirschberg, and Ben?u?
Affirmative Cue Words in Task-Oriented DialogueThe first clear pattern we find is that the backchannel function (BC) shows a markedpreference for a high-rising (H-H% in the ToBI conventions) or low-rising (L-H%) pitchcontour towards the end of the word.
Those two contours account for more than 60%of the backchannel instances of mm-hm, okay, uh-huh, and yeah.
For the other ACWsthere are not enough instances labeled BC in the corpus for statistical comparison.The predominance of H% found for backchannels is consistent with the openness thatsuch boundary tone has been hypothesized to indicate (Hobbs 1990; Pierrehumbertand Hirschberg 1990).
The utterer of a backchannel understands that (i) there ismore to be said, and (ii) it is the speaker holding the conversational turn who mustsay it.The default function of ACWs, agreement (Agr) is produced most often with falling(L-L%) or plateau final intonation ([!
]H-L%) in the case of alright, okay, right, and yeah.The L% boundary tone is believed to indicate the opposite of H%, a sense of closure,separating the current phrase from a subsequent one (Pierrehumbert and Hirschberg1990).
In our case, by agreeing with what the speaker has said, the listener indicates thatenough information has been provided and that any subsequent phrases may refer to adifferent topic.
In other words, such closure might mean that the proposition precedingthe ACW has been added to the current context space (Reichman 1985), or that a newfocus space is about to be created (Grosz and Sidner 1986).Notably, Agr instances of mm-hm and uh-huh present a very different behavior fromthe other lexical items, with a distribution of final intonations that closely resemblesthat of backchannels.
In particular, over 60% of the Agr tokens of mm-hm and uh-huhare produced with final rising intonation (either L-H% or H-H%).
As we will see in thefollowing sections, the realization of mm-hm and uh-huh as Agr or BC seems to be verysimilar along several dimensions besides intonation.Alright and okay are the only two ACWs in the corpus that are used to cue thebeginning of a new discourse segment, either combined with an agreement function(PBeg) or in its pure form (CBeg).
These two functions typically have a falling (L-L%) orsustained ([!
]H-L%) final pitch contour.
Additionally, the instances of okay and yeah usedto cue a discourse segment ending (PEnd) tend to be produced with a L-L% contour, andalso with [!
]H-L% in the case of okay.
This predominance of L% for ACWs conveying adiscourse boundary function is consistent with the previously mentioned closure thatsuch boundary tone is believed to indicate.Lastly, the only ACW used frequently in the corpus for checking with the interlocu-tor (the Chk function) is right, as illustrated in the following exchange:A: and the top?s not either, right?B: noA: okaySuch instances of right in the corpus normally end in a high-rising pitch contour,or H-H%.
This fact is probably explained by the close semantic resemblance ofthis construction to yes?no questions, which typically end in the same contour type(Pierrehumbert and Hirschberg 1990).In addition to the categorical prosodic variables described previously, word finalintonation may also be studied by measuring the slope of the word-final pitch track(see Section 3.3 for a description of how pitch slopes are calculated).
A high positivevalue of pitch slope corresponds to a rising intonation; a value close to zero, to a flatintonation; a high negative value, to a falling intonation.
Final pitch slope has the ad-vantage of being automatically computable; ToBI labels, on the other hand, still must be17Computational Linguistics Volume 38, Number 1Figure 6Final pitch slope, computed over the second half and over the final 100 and 200 msec of thetarget word.
In all cases, the vertical axis represents the change in Hertz per second.
Significantdifferences: For okay: BC>all; CBeg<Agr, BC, PEnd.
For right: Chk>Agr.
For yeah: BC>Agr.manually annotated?although ongoing research may change this fact in the near future(Rosenberg and Hirschberg 2009; Rosenberg 2010a, 2010b).
Therefore, it is important toverify that the results obtained using ToBI labels?if they are to be of practical use?are also observable when considering numeric measures such as pitch slope.
Figure 6shows, for the same ACWs and functions discussed earlier,7 the mean pitch slopecomputed over the second half of the word and over its final 100 and 200 msec, andgender-normalized as described in Section 3.The comparison of these numeric acoustic features across discourse/pragmaticfunctions confirms that the observations made previously for categorical prosodic fea-tures also hold when considering numeric features such as pitch slope, thus making thelikelihood that such observations will be of practical use in actual systems.
For okay,the three measures of word-final pitch slope are significantly higher for backchannels(BC) than for all other functions, and significantly lower for CBeg than for Agr, BC,and PEnd (RMANOVA for each of the three variables: between-subjects p > 0.3, withinsubjects p ?
0; Tukey test confidence: 95%).8 BC tokens of yeah are also significantlyhigher than Agr, with similar p-values.
Figure 6 shows that BC instances of mm-hm7 For PEnd instances of yeah and Agr instances of uh-huh, the number of tokens with no errors in the pitchtrack and pitch slope computations is too low for statistical consideration.8 Repeated-measures analysis of variance (RMANOVA) tests estimate the existence of both within-subjectseffects (i.e., differences between discourse/pragmatic functions) and between-subjects effects (i.e.,differences between speakers).
When the between-subjects effects are negligible, we may safely drawconclusions across multiple speakers in the corpus, with low risk of a bias from the behavior of aparticular subset of speakers.18Gravano, Hirschberg, and Ben?u?
Affirmative Cue Words in Task-Oriented Dialogueand uh-huh also have comparably high final pitch slopes.
Again, for mm-hm we find nosignificant difference in final pitch slope between agreements and backchannels.Although Figure 6 shows that Chk tokens of right tend to end in a steeply risingpitch, the RMANOVA tests yield between-subjects p-values of 0.01 or lower, indicatingsubstantial speaker effects.
In other words, even though the general tendency for thesetokens, as indicated by both the numeric and categorical variables, seems to be to endin a high-rising intonation, there is evidence of different behavior for some individualspeakers, which keeps us from drawing general conclusions about this pragmatic func-tion of right.4.3 IntensityThe next feature we find to vary significantly with the discourse/pragmatic functionof ACWs is word intensity.
Commonly referred to as loudness or volume, intensitygenerally functions to make words more salient or prominent.
Figure 7 shows themaximum and mean intensity for the most frequent ACWs and functions, computedover the whole word and speaker normalized using z-scores.The two types of differences we find are related to the discourse functions ofACWs.
For okay and yeah, both maximum and mean intensity are significantly lowerfor instances cueing the end of a discourse segment (PEnd) than instances of all otherfunctions (for both variables and both words, RMANOVA tests report between-subjectsp > 0.4 and within-subjects p ?
0; Tukey 95%).
For ACWs cueing a beginning discoursesegment, the opposite is true.
Instances of alright and okay labeled CBeg or PBeg havea maximum and mean intensity significantly higher than all other functions (for alright,Figure 7Word maximum and mean intensity, speaker normalized using z-scores.
All vertical axesrepresent z-scores.
Significant differences: For alright: Agr<CBeg.
For okay: PEnd<all;Agr<CBeg, PBeg, BC; BC<CBeg.
For yeah: PEnd<Agr, BC.19Computational Linguistics Volume 38, Number 1a RMANOVA test reports between-subjects p > 0.12 and within-subjects p ?
0).
Theseresults are consistent with previous studies of prosodic variation relative to discoursestructure, which find intensity to increase at the start of a new topic and decrease at theend (Brown, Currie, and Kenworthy 1980; Hirschberg and Nakatani 1996).
Because bydefinition CBeg/PBeg ACWs begin a new topic and CEnd/PEnd end one, it is then notsurprising to find that the former tend to be produced with higher intensity, and thelatter with lower.Finally, for mm-hm and uh-huh we find no significant differences in intensity be-tween their unique functions, agreement (Agr) and backchannel (BC).
Recall from theprevious section that we find no differences in final intonation either.
This furthersuggests that these two lexical types tend to be produced with similar acoustic/prosodicfeatures, independently of their function.4.4 Other FeaturesFor the remaining acoustic/prosodic features analyzed, we find a small number of sig-nificant or approaching-significance differences between the functions of ACWs.
Thesedifferences are related to duration, mean pitch, and voice quality.
The first set of findingscorresponds to the duration of ACWs, normalized with respect to all words with thesame number of syllables and phonemes uttered by the same speaker.
For alright andokay, instances cueing a beginning (CBeg) tend to be shorter than the other functions(for both words, RMANOVA: between-subjects p > 0.5, within-subjects p < 0.05, Tukey95%).
We also find tokens of right used to check with the interlocutor (Chk) to beon average shorter than the other two functions of right (RMANOVA, between-subjectsp > 0.7, within-subjects p = 0.001; Tukey 95%).
Note that these two functions are rel-atively simple: CBeg calls for the listener?s attention, and is frequently conveyed witha filled pause (uh, um); Chk asks the interlocutor for confirmation, which may alter-natively be achieved via a high-rising intonation.
Thus, it is not surprising that thesefunctions take less time to be realized than other more pragmatically loaded functions,such as agreement.Speaker-normalized mean pitch over the whole word also presents significantdifferences for okay and yeah.
Instances labeled PEnd (agreement and cue endingdiscourse segment) present a higher mean pitch than the other functions (for bothwords, RMANOVA: between-subjects p > 0.6, within-subjects p < 0.01; Tukey 95%).
Thisis rather unexpected, because as noted in Section 4.2 around 70% of PEnd ACWs in thecorpus end in a L% boundary tone, and thus they would plausibly be uttered with alow pitch level.
What our data indicate, however, is that speakers tend to reset and raisetheir pitch range when producing PEnd instances of ACWs.Finally, we find some evidence of differences in voice quality.
Both alright andokay show a lower shimmer over voiced portions when starting a new segment (CBeg)(RMANOVA: between-subjects p > 0.9 for alright, p = 0.09 for okay; within-subjectsp < 0.001 for both words).
Also, okay and yeah present a lower noise-to-harmonics ratio(NHR) for backchannels (RMANOVA: between-subjects p > 0.3 for okay, p = 0.04 foryeah; within-subjects p < 0.005 for both words).
A lower value of shimmer and NHRhas been associated with the perception of a better voice quality (Eskenazi, Childers,and Hicks 1990; Bhuta, Patrick, and Garnett 2004).
Our results suggest, then, that voicequality may constitute another dimension along which speakers vary their productionsto convey the intended discourse/pragmatic meaning.
Notice though that for these twovariables some of the between-subjects p-values are low enough to suggest significant20Gravano, Hirschberg, and Ben?u?
Affirmative Cue Words in Task-Oriented Dialoguespeaker effects.
Therefore, our results related to differences in voice quality should beconsidered preliminary.5.
Automatic Classification of Affirmative Cue WordsIn this section we present results from machine learning (ML) experiments aimed atinvestigating how accurately affirmative cue words may be classified automaticallyinto their various discourse/pragmatic functions.
If spoken dialogue systems are tointerpret and generate ACWs reliably, we must identify reliable cues.
With this goalin mind, we explore several dimensions of the problem: We consider three classificationtasks, simulating the conditions of different speech applications, and study the perfor-mance of different ML algorithms and feature sets on each task.
We note that previousstudies have attempted to disambiguate between the sentential and discourse uses ofcue phrases such as now, well, and like, in corpora containing comparable numbers ofinstances of each class.
For ACWs in the Games Corpus dialogues, sentential uses arerare, with the sole exception of right.
Therefore, disambiguating between discourse andsentential uses appears to be less useful than distinguishing among different discoursefunctions.The first ML task we consider consists in the general classification of any ACW(alright, gotcha, huh, mm-hm, okay, right, uh-huh, yeah, yep, yes, yup) into any function(Agr, BC, CBeg, PBeg, CEnd, PEnd, Mod, BTsk, Chk, Stl; see Table 1), a critical taskfor spoken dialogue systems seeking to interpret user input in general.
The second taskinvolves identifying instances of these words used to signal the beginning (CBeg, PBegin our labeling scheme) or ending (CEnd, PEnd) of a discourse segment, which is im-portant for applications that must segment speech into coherent units, such as meetingbrowsing systems and turn-taking components of spoken dialogue systems.
The thirdtask consists in identifying tokens conveying some degree of acknowledgment: (Agr,BC, PBeg, and PEnd), a function especially important to spoken dialogue systems, forwhich it is critical to know that a user has heard the system?s output.Speech processing applications operate in disparate conditions.
On-line applica-tions such as spoken dialogue systems process information as it is generated, havingaccess to a limited amount of context, normally up to the last IPU uttered by the user.On the other hand, off-line applications, such as meeting transcription and browsingsystems, have the whole audio file available for processing.
We simulate these twoconditions in our experiments, assessing how the limitations of online systems af-fect performance.
We also group the features described in Section 3.3 into five sets?lexical (LX), discourse (DS), timing (TM), acoustic (AC), and phonetic (PH); see Tables 4through 8?to determine the relative importance of each feature set in the variousclassification tasks.
For example, this approach allows us to simulate the conditionsof the understanding component of a spoken dialogue system, which can use only theinformation up through the current IPU to detect the function of a user?s ACW.
Sucha system may have access only to ASR transcription or it may have access to acousticand prosodic information; we note that our analysis does not take into account the pos-sibility that transcriptions are likely to contain some errors.
Our approach also allowsus to simulate a text-to-speech (TTS) system which might be used to produce a spokenversion of an on-line chat room.
In order to choose the appropriate acoustic/prosodicrealization of each ACW, the TTS system will first need to determine its function basedon features extracted solely from the input text (in our taxonomy, LX and DS).We conduct our ML experiments using three well-known algorithms with verydifferent characteristics: the decision tree learner C4.5 (Quinlan 1993), the propositional21Computational Linguistics Volume 38, Number 1Table 10Error rate of each classifier on the general task using different feature sets; F-measures ofthe SVM classifier; and error rate and F-measures of two baselines and human labelers.For the classifier error rates: ?
Significantly different from full model.
?
Significantly differentfrom SVM.
(Wilcoxon signed rank sum test, p < 0.05.)
Significance was not tested for theclassifier F-measures.Error Rate SVM F-MeasureFeature Set C4.5 Ripper SVM Agr BC CBeg PEnd Mod ChkLX DS TM AC PH 16.6% ?
16.3% ?
14.3% .86 .81 .89 .50 .97 .39DS TM AC PH 21.3% ??
17.2% ?
16.5% ?
.84 .82 .87 .44 .94 0LX TM AC PH 20.3% ??
20.1% ?
17.0% ?
.84 .80 .83 .16 .97 .21LX DS AC PH 17.1% ?
18.1% ??
14.8% ?
.86 .81 .89 .38 .97 .35LX DS TM PH 15.2% ?
16.3% 16.2% ?
.85 .80 .86 .16 .97 .33LX DS TM AC 17.0% ?
16.9% ?
14.7% .86 .80 .89 .48 .97 .35LX 23.7% ??
22.7% ?
22.3% ?
.79 .80 .65 0 .96 .03DS 22.8% ??
24.0% ??
25.3% ?
.76 .67 .82 0 .87 0TM 29.5% ??
27.3% ??
36.2% ?
.70 0 .57 0 .83 0AC 44.8% ??
29.8% ??
41.3% ?
.67 .66 .14 0 .58 0PH 56.4% ??
26.5% ??
45.4% ?
.65 .08 .13 0 .64 0Majority class baseline ER 56.4% .61 0 0 0 0 0Word-based baseline ER 27.7% .75 .79 0 0 .94 .13Human labelers ER (estimate 1) 9.3% .92 .91 .94 .51 .99 .67Human labelers ER (estimate 2) 11.0% .90 .89 .93 ?
.99 ?rule learner RIPPER (Cohen 1995), and support vector machines (SVM) (Cortes andVapnik 1995; Vapnik 1995).
We use the implementation of these algorithms providedin the WEKA machine learning toolkit (Witten and Frank 2000), known respectively asJ48, JRIP, and SMO.
We also use 10-fold cross-validation in all experiments.95.1 Classifiers and Feature TypesTo assess the predictive power of the five feature types (LX, DS, TM, AC, and PH) weexclude one type at a time and compare the performance of the resulting set to thatof the full model.
Table 10 displays the error rate of each ML classifier on the generaltask, classifying any ACW into any of the most frequent discourse/pragmatic functions(Agr, BC, CBeg, PEnd, Mod, Chk).
Table 11 shows the same results for the other twotasks: the detection of a discourse boundary function?cue beginning (CBeg PBeg),cue ending (CEnd, PEnd), or no-boundary (all other labels); and the detection of anacknowledgment function?Agr, BC, PBeg, or PEnd, vs. all other labels.109 In the case of SVM, prior to the actual tests we experimented with two kernel types: polynomial(K(x, y) = (x + y)d) and Gaussian radial basis function (RBF) (K(x, y) = exp(?
?||x ?
y||2) for ?
> 0).
Weperformed a grid search for the optimal arguments for either kernel using the data portion left out afterdownsampling the corpus (see Section 3.2).
The best results were obtained using a polynomial kernelwith exponent d = 1.0 (i.e., a linear kernel) and model complexity C = 1.0.10 We note that performance on new data may be somewhat worse than the results reported here, becausewe did exclude approximately 5% of tokens in our corpus due to lack of annotator agreement on labels.22Gravano, Hirschberg, and Ben?u?
Affirmative Cue Words in Task-Oriented DialogueTable 11Error rate of each classifier on the detection of discourse boundary functions andacknowledgment functions, using different feature sets.
?
Significantly different fromfull model.
?
Significantly different from SVM.
(Wilcoxon signed rank sum test, p < 0.05.)Disc.
Boundary Acknowledgment{CBeg, PBeg} vs. {CEnd, PEnd} vs. Rest {Agr, BC, PBeg, PEnd} vs. RestFeature Set C4.5 Ripper SVM C4.5 Ripper SVMLX DS TM AC PH 6.9% 8.1% ?
6.9% 5.8% 5.9% ?
4.5%DS TM AC PH 7.6% ?
8.0% 7.6% ?
8.5% ??
5.5% ?
6.4% ?LX TM AC PH 10.4% ?
10.1% ?
9.5% ?
8.7% ??
8.7% ??
6.5% ?LX DS AC PH 8.0% ?
8.7% ?
7.5% ?
5.3% 5.7% ?
4.9%LX DS TM PH 6.6% ?
7.9% 8.9% ?
5.4% 5.4% 5.1%LX DS TM AC 7.1% 8.3% ?
7.0% 5.8% ?
5.6% ?
4.6%LX 14.2% ?
14.5% ??
13.9% ?
11.4% ?
11.4% ?
11.7% ?DS 7.8% ?
8.6% ?
10.9% ?
8.4% ??
8.9% ?
9.4% ?TM 12.2% ??
11.2% ??
14.7% ?
12.8% ??
13.5% ?
14.5% ?AC 17.3% ??
14.3% ??
18.5% ?
26.7% ?
16.6% ??
28.4% ?PH 18.6% ?
17.6% ?
18.6% ?
36.5% ??
14.1% ??
25.4% ?Majority class baseline ER 18.6% 36.5%Word-based baseline ER 18.6% 15.3%Human labelers ER (est.
1) 5.3% 2.9%Human labelers ER (est.
2) 5.6% 3.0%In both tables, the first line corresponds to the full model, with all five featuretypes.
The subsequent five lines show the performance of models with just four fea-ture types, excluding one feature type at a time, and the following five lines showthe performance of models with exactly one feature type?these are two methods forassessing the predictive power of each feature set.
For the error rates of our classifiers,the ?
symbol indicates that the given classifier performs significantly worse whentrained on a particular feature set than when trained on the full set.11 The ?
symbolindicates that the difference between SVM and the given classifier, either C4.5 or Ripper,is significant.
For example, the second line (DS TM AC PH) in Table 10 indicates that,for the general classification task, the three models trained on all but lexical featuresperform significantly worse than the respective full models; also, the performance ofC4.5 is significantly worse than SVM, and the difference between Ripper and SVM isnot significant.The bottom parts of Tables 10 and 11 show the error rate of two baselines, aswell as two estimates of the error rate of human labelers.
We consider two types ofbaseline: one a majority-class baseline, and one that employs a simple rule based11 All accuracy comparisons discussed in this section are tested for significance with the Wilcoxon signedrank sum test (a non-parametric alternative to Student?s t-test) at the p < 0.05 level, computed overthe error rates of the classifiers on the ten cross-validation folds.
These tests provide evidence thatthe observed differences in mean accuracy over cross-validation folds across two models are notattributable to chance.23Computational Linguistics Volume 38, Number 1on word identity.
In the general classification task, the majority class is Agr, and thebest performing word-based rule is huh?Chk, mm-hm?Mod, uh-huh?BC, right?Mod, others?Agr.
For the identification of a discourse boundary function, themajority class is no-boundary, and the word-based rule also assigns no-boundary toall tokens.
For the detection of an acknowledgment function, the majority classis acknowledgment, and the word-based rule is right, huh?no-acknowledgment;others?acknowledgment.The error rates of human labelers are estimated using two different approaches.Our first estimate compares the labels assigned by each labeler and the majority labelsas defined in Section 3.1.
Because each labeler?s labels are used for calculating both theerror rate and the gold standard, this estimate is likely to be over-optimistic.
Our secondestimate considers the subset of cases in which two annotators agree, and comparesthose labels with the third labeler?s.
Tables 10 and 11 show that these two estimatesyield similar results; for PEnd and Chk, there are not enough counts for computing theF-measure of estimate 2.The right half of Table 10 shows the F-measure of the SVM classifier for eachindividual ACW function, for the general task.
The highest F-measures correspond toAgr, BC, CBeg, and Mod, precisely the four functions with the highest counts in theGames Corpus.
For PBeg and Chk the F-measures are much lower (and equal to zerofor the four remaining functions, not included in the table) due very likely to their lowcounts, which prevent a better generalization during the learning stage.
Future researchcould investigate incorporating boosting and bootstrapping techniques to reduce thenegative effect on classification of low counts for some of the discourse/pragmaticfunctions of ACWs.For the three classification tasks, SVM outperforms, or performs at least comparablyto, the other two classifiers whenever acoustic features (AC) are taken into accounttogether with other feature types.
When used alone, though, acoustic features per-form poorly in all three tasks.
Moreover, when acoustic features are excluded, SVM?saccuracy is comparable to, or worse than, C4.5 and Ripper.
This is probably due tothe fact that SVM?s mathematical model is better suited to exploit larger amounts ofcontinuous numerical variables, and thus makes a difference when including acousticfeatures.For the first two tasks, the SVM classifier seems to take advantage of all but onefeature type, as shown by the significantly lower performance resulting from removingany of the feature types from the full model?the sole exception is the phonetic type(PH), whose removal in no case negatively affects the accuracy of any classifier.
C4.5and Ripper, on the other hand, appear to take more advantage of some feature typesthan others.
For the third task, lexical (LX) and discourse (DS) features apparently havemore predictive power for both C4.5 and SVM than the other types.
Note also thatfor the second and third tasks, the error rates of our full-model SVM classifiers closelyapproximate the estimated error rates of human labelers.For the general task of classifying any ACW into any discourse/pragmatic function,our full-model SVM classifier achieves the best overall results.
To take a closer look atthe performance of this model, we compute its F-measure for the discourse/pragmaticfunctions of each individual lexical item, as shown in Table 12.
We observe that theclassifier achieves better results for word?function pairs with higher counts in theGames Corpus, such as yeah-Agr or right-Mod (cf.
Table 2).
Again, the low countsfor the remaining word?function pairs may prevent a better generalization during thelearning stage, a problem that could be attenuated in future work with boosting andbootstrapping techniques.24Gravano, Hirschberg, and Ben?u?
Affirmative Cue Words in Task-Oriented Dialogue5.2 Session-Specific and ToBI Prosodic FeaturesWhen including session-specific features in the full model, such as identity and genderof both speakers (see Table 8), the error rate of the SVM classifier is significantly reducedfor the general task (13.3%) and for the discourse boundary function identification task(6.4%) (Wilcoxon, p < 0.05).
For the detection of an acknowledgment function, the errorrate is not modified when including those features (4.5%).
This suggests the existenceof speaker differences in the production of at least some functions of ACWs that may beexploited by ML classifiers.
Finally, the inclusion of categorical prosodic features basedon the ToBI framework, such as type of pitch accent and break index on the target word(see Table 5), does not improve the performance of the SVM-based full models in any ofthe classification tasks.5.3 Individual FeaturesTo estimate the importance of individual features in our classification tasks, we rankthem according to an information-gain metric.
We find that for the three tasks, lexical(LX), discourse (DS), and timing (TM) features dominate.
The highest ranked features arethe ones capturing the position of the target word in its IPU and in its turn.
Lexicalidentity and POS tags of the previous, target, and following words, and duration ofthe target word, are also ranked high.
Acoustic features appear lower in the ranking;the best performing ones are word intensity (range, mean, and standard deviation),pitch (maximum and mean), pitch slope over the final part of the word (200 msec andsecond half), voiced-frames ratio, and noise-to-harmonics ratio.
All phonetic featuresare ranked very low.
Note that, whereas durational features at the word level areranked high, durational features at the phonetic level are not, because the latter onlycapture the duration of each phone relative to the word duration?apparently not aninformative attribute for these classification tasks.
These results confirm the existence oflarge positional differences across functions of ACWs, as seen in Section 4.
Additionally,whereas several acoustic/prosodic features extracted from the target word contain use-ful information for the automatic disambiguation of ACWs, it is positional informationthat provides the most predictive power.5.4 Online and Offline TasksTo simulate the conditions of online applications, which process speech as it is producedby the user, we consider a subset of features that may typically be extracted from theTable 12F-measure achieved by our full-model SVM classifier for the different discourse/pragmaticfunctions of each lexical item.Agr BC CBeg PBeg PEnd Mod BTsk Chk Stlalright .88 ?
.93 ?
.33 ?
?
?
?mm-hm .35 .94 ?
?
?
?
?
?
?okay .82 .51 .88 .27 .63 .53 0 ?
.18right .84 ?
?
?
?
.98 ?
.53 ?uh-huh .35 .93 ?
?
?
?
?
?
?yeah .96 .54 ?
?
.17 ?
?
?
?25Computational Linguistics Volume 38, Number 1speech signal only up to the IPU containing the target ACW.
These features are markedin Tables 4 through 8 with letter O.
With these features, we train and evaluate an SVMclassifier for the three tasks described previously.
Table 13 shows the results, comparingthe performance of each classifier to that of the models trained on the full feature set,which simulate the conditions of off-line applications.
In all three cases the on-linemodel performs significantly worse than its offline correlate, but also significantly betterthan the baseline (Wilcoxon, p < 0.05).Table 13 also shows the error rates of on-line and off-line classifiers trained usingsolely text-based features?that is, only features of lexical (LX) or discourse (DS) types.Text-based models simulate the conditions of spoken dialogue systems with no accessto acoustic and prosodic information, or generation systems attempting to realize text-based exchanges in speech.
They reflect the importance of text information alone intraining such systems to recognize the function of ACWs on-line and off-line and toproduce appropriate realizations from limited or full transcription.Our on-line and off-line text-based models perform significantly worse than thecorresponding models that use the whole feature set, but they still outperform thebaseline models in all cases (Wilcoxon, p < 0.05).
Finally, the off-line text-based modelsalso outperform their on-line correlates in all three tasks (Wilcoxon, p < 0.05).
Theseresults indicate the important role that other classes of cues play in recognition, whileindicating the level of performance we can expect from TTS systems which have onlytext available.5.5 Backchannel DetectionThe correct identification of backchannels is a desirable capability for speech processingsystems, as it would allow us to distinguish between two quite distinct speaker inten-tions: the intention to take the conversational floor, and the intention to backchannel.We first consider an off-line binary classification task?namely, classifying allACWs in the corpus into backchannels vs. the rest, using information from the wholeconversation.
In such a task, an SVM classifier achieves a 4.91% error rate, slightlyyet significantly outperforming a word-based baseline (mm-hm, uh-huh?BC; others?no-BC), with 5.17% (Wilcoxon, p < 0.05).On-line applications such as spoken dialogue systems need to classify every newspeaker contribution immediately after (or even while) it is uttered, and certainlywithout access to any subsequent context.
The Games Corpus contains approximately6,700 turns following speech from the other speaker, all of which begin as potentialbackchannels and need to be disambiguated by the listener.
Most of these candidatescan be trivially discarded using a simple observation about backchannels: By definitionTable 13Error rate of the SVM classifier on online and offline tasks.All Functions Disc.
Boundary AcknowledgmentFeature Set Online Offline Online Offline Online OfflineLX DS TM AC PH (Full model) 17.4% 14.3% 10.1% 6.9% 6.7% 4.5%LX DS (Text-based) 21.4% 16.8% 13.5% 9.1% 10.0% 5.9%Word-based baseline 27.7% 18.6% 15.3%26Gravano, Hirschberg, and Ben?u?
Affirmative Cue Words in Task-Oriented Dialoguethey are short, isolated utterances, and consist normally in just one ACW.
Of the 6,700candidate turns in the corpus, only 2,351 (35%) begin with an isolated ACW, including753 of the 757 backchannels in the corpus.12 Thus, an on-line classification procedurewould only need to identify backchannels in those 2,351 turns.
At this point, we ex-plore using a binary classifier for this task.
The same word-based majority baselinedescribed earlier achieves an error rate of 11.56%.
An SVM classifier trained on featuresextracted from up to the current IPU (to simulate the on-line condition of a spokendialogue system) fails to improve over this baseline, achieving an error rate of 11.51%,not significantly different from the baseline.
A possible explanation for this might bethat backchannels seem to be difficult to distinguish from agreements in many cases,leading to an increase in the error rate.
Recall, from the statistical analyses in theprevious section, the positional and acoustic/prosodic similarities of tokens with thesetwo functions for mm-hm and uh-huh, for example.
Shriberg et al (1998) report the samedifficulty in distinguishing these two word functions.
We conclude that further researchis needed to develop novel approaches to this crucial problem of spoken dialoguesystems.5.6 Comparison with Previous WorkIn an effort to provide a general frame of reference for our results, we discuss herewhat we believe to be the most relevant results from related studies.
Note, however,that comparing these results directly to the results of our classification experimentsis difficult because the type of corpora, definitions used, features examined, and/ormethodology employed vary greatly among the studies.
The current study focusesexclusively on the discourse/pragmatic functions of ACWs whereas other studies haveeither a broader or narrower scope.Among the cue words tested in Litman (1996) is okay, one of the ACWs we alsoinvestigate.
Litman describes the automatic classification of cue words in general (in-cluding, e.g., now, well, say, and so), classifying these into discourse and sentential usesusing a corpus of monologue.
In this classification task, which is not performed in ourstudy, the best results are reached by decision-tree learners trained on prosodic andtext-based features, with an error rate of 13.8%.The most relevant study to ours is that of Stolcke et al (2000), which presentsexperiments on the automatic disambiguation of dialogue acts (DA) on 1,155 sponta-neous telephone conversations from the Switchboard corpus, labeled using the DAMSL(Dialogue Act Markup in Several Layers) annotation scheme (Core and Allen 1997).
Forthe subtask of identifying the Agreement and Backchannels tags collapsed together, theauthors report an error rate of 27.1% when using prosodic features, 19.0% when usingfeatures extracted from the text, and 15.3% when using all features.
Other DA classi-fications also include some of the functions of ACWs discussed in our current study.For instance, Reithinger and Klesen (1997) employ a Bayesian approach for classifying18 classes of DAs in transcripts of 437 German dialogues from the VERBMOBIL corpus(Jekat et al 1995).
The DA tags examined include Accept, Confirm, and Feedback, allof which are related to the functions of ACWs discussed here.
For the Accept DA tag,the authors report an F-measure of 0.69; for Feedback, 0.48; and for Confirm, 0.40.
These12 The four remaining backchannels correspond to a rare phenomenon in which the speaker overlaps theinterlocutor?s last phrase with a short agreement, followed by an optional short pause and a backchannel.Example: A: but it doesn?t overlap *them.
B: right* yeah yeah # okay.27Computational Linguistics Volume 38, Number 1experiments are repeated on transcripts of 163 English dialogues from the same corpus,yielding an F-measure of 0.78 for the Accept DA tag, and 0 for the other two tags due todata sparsity.As part of a study aimed at assessing the effectiveness of machine learning for thistype of task, Core (1998) experiments with hand-coded decision trees for classifyingfive high-level dialogue act classes, including AGREEMENT and UNDERSTANDING,following the DAMSL annotation scheme.
On 19 dialogues from the TRAINs corpus(discussions related to solving transportation problems), Core reports an accuracy of70% for both the Agreement and the Understanding DA classes, using only the previousutterance?s DAMSL tag as a feature in the decision trees.
This use of DA context inclassifying ACWs would appear to be promising, assuming an accurate automaticclassification of all DAs in the corpus.Finally, Lampert, Dale, and Paris (2006) describe a statistical classifier trained ontext-based features for automatically predicting eight different speech acts derived froma taxonomy called Verbal Response Modes (VRM).
The experiments are conductedon transcripts of 1,368 utterances from 14 dialogues in English.
For the Acknowledg-ment speech act (which ?conveys receipt of or receptiveness to other?s communication;simple acceptance, salutations; e.g., yes?
[page 37]), the classifier yields an F-measureof 0.75.Again, all of these studies differ significantly from our own, in their task definition,in their methodology, and in the domain they examine.
However, we expect this briefsummary to serve as a general frame of reference for our own classification results.6.
DiscussionIn this work we have undertaken a comprehensive study of affirmative cue words, asubset of cue phrases such as okay, yeah, or alright that may be utilized to convey asmany as ten different discourse/pragmatic functions, such as indicating continuedattention to the interlocutor or cueing the beginning of a new topic.
Consideringthe high frequency of ACWs in task-oriented dialogue, it is critical for some spokenlanguage processing applications such as spoken dialogue systems to model the usageof these words correctly, from both an understanding and a generation perspective.Section 4 presents statistical evidence of a number of differences in the productionof the various discourse/pragmatic functions of ACWs.
The most notable contrasts inacoustic/prosodic features relate to word final intonation and word intensity.
Backchan-nels typically end in a rising intonation; agreements and cue beginnings, in a fallingintonation.
Cue beginnings tend to be produced with a high intensity, and cue endingswith a very low one.
Other acoustic/prosodic features?duration, mean pitch, andvoice quality?also seem to vary with the word usage.
Our findings related to finalintonation are consistent with previous results obtained by Hockey (1993) and Jurafskyet al (1998) for American English.
For Scottish English, Kowtko (1996) reports a non-rising intonation for cue beginnings and for her ?reply-y?
function, a subclass of ouragreement function.
Kowtko also reports observing all types of final intonation in her?acknowledge?
function, whose definition overlaps both our agreements and backchan-nels.
Thus, we find no apparent contradictions between Kowtko?s results for ScottishEnglish and ours for American English.The word okay is the most heavily overloaded ACW in our corpus.
Our corpusincludes instances conveying each of the ten identified meanings, and this item showsthe highest degree of variation along the acoustic/prosodic features we have examined.28Gravano, Hirschberg, and Ben?u?
Affirmative Cue Words in Task-Oriented DialogueWe speculate from this finding that the more ambiguous an ACW, the more a speakerneeds to vary acoustic/prosodic features to differentiate its meaning.Our statistical analysis of ACWs also shows that these words display substantialpositional differences across functions, such as the position of the word in its con-versational turn, or whether the word is preceded and/or followed by silence.
Suchlarge differences bring support to Novick and Sutton?s (1994) claim that the discourse/pragmatic role of these expressions strongly depends on their basic structural context.For example, in Novick and Sutton?s words, an ACW in turn-initial position is ?clearlynot serving as a prompt for the other speaker to continue?
(page 97).Previous studies on the automatic disambiguation of other types of cue words, suchas now, well, or like, present the problem as a binary classification task: Each cue word haseither a discourse or a sentential sense (e.g., Litman 1996; Zufferey and Popescu-Belis2004).
In the study of automatic classification of ACWs presented in Section 5 we showthat for spoken task-oriented dialogue, the simple discourse/sentential distinction isinsufficient.
In consequence, we define two new classification tasks besides the generaltask of classifying any ACW into any function.
Our first task, the detection of anacknowledgment function, has important implications for the language managementcomponent in spoken dialogue systems, which must keep track of which material hasreached mutual belief in a conversation (Bunt, Morante, and Keizer 2007; Roque andTraum 2009).
Our second task, the detection of a discourse segment boundary func-tion, should help in discourse segmentation and meeting processing tasks (Litman andPassonneau 1995).
Our SVM models based on lexical, discourse, timing, and acousticfeatures approach the error rate of trained human labelers in all tasks, while our auto-matically computed phonetic features offer no improvement.
Previous studies indicatethat the pragmatic function of ambiguous expressions may be effectively predicted bymodels that combine information extracted from various sources, including lexical andprosodic (e.g., Litman 1996; Stolcke et al 2000).
Our results support this, and extend thelist of useful information sources to include discourse and timing features that may beeasily extracted from the time-aligned transcripts.Additionally, our machine learning study includes experiments with several combi-nations of feature sets, in an attempt to simulate the conditions of different applications.Models that are trained using features extracted only from the speech signal up to theIPU containing the target word simulate on-line applications such as spoken dialoguesystems with access to acoustic/prosodic features.
Although such models performworse than ?off-line?
models, which make use of left and right context, they still sig-nificantly outperform our baseline classifiers.
Models that simulate the conditions ofcurrent spoken dialogue systems with access only to lexical features (although perhapserrorful) and TTS systems synthesizing spoken conversations, which have access onlyto features extracted from the input text, also significantly outperform our baselineclassifiers.Interactions between state-of-the-art spoken dialogue systems and their users ap-pear to contain very few instances of backchannel responses from either conversationalpartner.
On the system?s side, the absence of this important element of spoken com-munication may be due to the difficulty of detecting appropriate moments where abackchannel response would be welcome by the user.
Recent advances on that researchtopic (Ward and Tsukahara 2000; Cathcart, Carletta, and Klein 2003; Gravano andHirschberg 2009a) have encouraged research on ways to equip systems with the abilityto signal to the user that the system is still listening (Maatman, Gratch, and Marsella2005; Bevacqua, Mancini, and Pelachaud 2008; Morency, de Kok, and Gratch 2008)?for example, when the user is asked to enter large amounts of information.
On the29Computational Linguistics Volume 38, Number 1user?s side, an important reason for not backchanneling may lie in the unnaturalnessof such systems, often described as ?confusing?
or even ?intimidating?
by users, aswell as their inability to recognize backchannels as such.
Nonetheless, recent Wizard-of-Oz experiments conducted by Hjalmarsson (2009, 2011) show that humans appearto react to turn-management cues produced by a synthetic voice in the same way thatthey react to cues produced by another human.
This important finding suggests thatusers of spoken dialogue systems could be cued to produce backchannel responses, forexample to determine if they are still paying attention.
In that case, it will be crucial forsystems to be able to distinguish backchannels from other pragmatic functions (Shriberget al 1998).
In Section 5.5 we present results on the task of automatically identifyingbackchannel ACWs from the other possible functions.
Our models improve over thebaseline in an off-line condition (e.g., for meeting processing tasks), but fail to do soin an on-line setting (e.g., for spoken dialogue systems).
Practically all of the confusionof this on-line model comes from misclassifying agreements (Agr) as backchannels (BC)and vice versa.
The reliability of our human labelers for distinguishing these two classeswas measured by Fleiss?s ?
at 0.570, a level considerably lower than the 0.745 achievedfor the general labeling task, which indicates that the backchannel identification task isdifficult for humans as well, at least when they are not engaged in the conversation itselfbut only listening to it after the fact.
Although we asked our annotators to distinguishthe agreement function of ACWs from ?continued attention,?
there are clearly caseswhere people disagree about whether speakers are indicating agreement or not.
Infuture research we will investigate this issue in more detail, given the relevance ofon-line identification of backchannels in spoken dialogue systems.In summary, in this study we have identified a number of characterizations ofaffirmative cue words in a large corpus of SAE task-oriented dialogue.
The corpuson which our experiments were conducted, rich in ACWs conveying a wide rangeof discourse/pragmatic functions, has allowed us to systematically investigate manydimensions of these words, including their production and automatic disambiguation.Besides the value of our findings from a linguistic modeling perspective, we believethat incorporating these results into the production and understanding components ofspoken dialogue systems should improve their performance and increase user satisfac-tion levels accordingly, getting us one step closer to the long-term goal of effectivelyemulating human behavior in dialogue systems.Appendix A: The COLUMBIA GAMES CORPUSThe COLUMBIA GAMES CORPUS is a collection of 12 spontaneous task-oriented dyadicconversations elicited from native speakers of Standard American English.
The cor-pus was collected and annotated jointly by the Spoken Language Group at ColumbiaUniversity and the Department of Linguistics at Northwestern University.
In each ofthe 12 sessions, two subjects were paid to play a series of computer games requiringverbal communication to achieve joint goals of identifying and moving images on thescreen.
Each subject used a separate laptop computer and could not see the screen of theother subject.
They sat facing each other in a soundproof booth, with an opaque curtainhanging between them, so that all communication was verbal.
The subjects?
speech wasnot restricted in any way, and it was emphasized at the session beginning that the gamewas not timed.
Subjects were told that their goal was to accumulate as many points aspossible over the entire session, since they would be paid additional money for eachpoint they earned.30Gravano, Hirschberg, and Ben?u?
Affirmative Cue Words in Task-Oriented DialogueA.1 Game TasksSubjects were first asked to play three instances of the CARDS game, where they wereshown cards with one to four images on them.
Images were of two sizes (small or large)and various colors, and were selected to contain primarily voiced consonants, whichfacilitates pitch track computation (e.g., yellow lion, blue mermaid).
There were two partsto each Cards game, designed to vary genre from primarily monologue to dialogue.In the first part of the Cards game, each player?s screen displayed a stack of 9 or10 cards (Figure A1a).
Player A was asked to describe the top card on her pile, whilePlayer B was asked to search through his pile to find the same card, clicking a buttonwhen he found it.
This process was repeated until all cards in Player A?s deck werematched.
In all cases, Player B?s deck contained one additional card that had no matchin Player A?s deck, to ensure that she would need to describe all cards.In the second part of the Cards game, each player saw a board of 12 cards on thescreen (Figure A1b), all initially face down.
As the game began, the first card on oneplayer?s (the DESCRIBER?s) board was automatically turned face up.
The Describerwas told to describe this card to the other player (the SEARCHER), who was to find amatching card from the cards on his board.
If the Searcher could not find a card exactlymatching the Describer?s card, but could find a card depicting one or more of the objectson that card, the players could decide whether to declare a partial match and receivepoints proportional to the numbers of objects matched on the cards.
At most three cardswere visible to each player at any time, with cards seen earlier being automaticallyturned face down as the game progressed.
Players switched roles after each card wasdescribed and the process continued until all cards had been described.
The playerswere given additional opportunities to earn points, based on other characteristics of thematched cards, to make the game more interesting and to encourage discussion.After completing all three instances of the Cards game, subjects were asked to playa final game, the OBJECTS game.
As in the Cards game, all images were selected tohave likely descriptions which were as voiced and sonorant as possible.
In the Objectsgame, each player?s laptop displayed a game board with 5 to 7 objects (Figure A1c).Both players saw the same set of objects at the same position on the screen, except forone (the TARGET).
For the DESCRIBER, the target object appeared in a random locationamong other objects on the screen; for the FOLLOWER, the target object appeared at thebottom of the screen.
The Describer was instructed to describe the position of the targetobject on her screen so that the Follower could move his representation to the samelocation on his own screen.
After players negotiated what they believed to be their bestFigure A1Sample screens from the Cards games (a, b) and Objects games (c).31Computational Linguistics Volume 38, Number 1location match, they were awarded 1 to 100 points based on how well the Follower?starget location matched the Describer?s.The Objects game proceeded through 14 tasks.
In the initial four tasks, one ofthe subjects always acted as the Describer, and the other one as the Follower.
In thefollowing four tasks their roles were inverted: The subject who played the Describerrole in the initial four tasks was now the Follower, and vice versa.
In the final six tasks,they alternated the roles with each new task.A.2 Subjects and SessionsThirteen subjects (six women, seven men) participated in the study, which took place inOctober 2004 in the Speech Lab at Columbia University.
Eleven of the subjects partici-pated in two sessions on different days, each time with a different partner.
All subjectsreported being native speakers of Standard American English and having no hearingimpairments.
Their ages ranged from 20 to 50 years (mean, 30.0; standard deviation,10.9), and all subjects lived in the New York City area at the time of the study.
Theywere contacted through the classified advertisements Web site craigslist.org.We recorded twelve sessions, each containing an average of 45 minutes of dialogue,totaling roughly 9 hours of dialogue in the corpus.
Of those, 70 minutes correspond tothe first part of the Cards game, 207 minutes to the second part of the Cards game, and258 minutes to the Objects game.
Each subject was recorded on a separate channel ofa DAT recorder, at a sample rate of 48 kHz with 16-bit precision, using a Crown head-mounted close-talking microphone.
Each session was later downsampled to 16 kHz,16-bit precision, and saved as one stereo .wav file with one player per channel, and alsoas two separate mono .wav files, one for each player.Trained annotators orthographically transcribed the recordings of the Games Cor-pus and manually aligned the words to the speech signal, yielding a total of 70,259words and 2,037 unique words in the corpus.
Additionally, self repairs and certain non-word vocalizations were marked, including laughs, coughs, and breaths.
Intonationalpatterns and other aspects of the prosody were identified using the ToBI transcriptionframework (Beckman and Hirschberg 1994; Pitrelli, Beckman, and Hirschberg 1994):Trained annotators intonationally transcribed all of the Objects portion of the corpus(258 minutes of dialogue) and roughly one third of the Cards portion (90 minutes).Appendix B: ACW Labeling GuidelinesThese guidelines for labeling the discourse/pragmatic functions of affirmative cuewords were developed by Julia Hirschberg, ?tefan Ben?u?, Agust?n Gravano, andMichael Mulley at Columbia University.Classification SchemeMost of the labels are defined using okay, but the definitions hold for all of these words:alright, gotcha, huh, mm-hm, okay, right, uh-huh, yeah, yep, yes, yup.
If you really have noclue about the function of a word, label it as ?.
[Mod] Literal Modifiers: In this case the words are used as modifiers.
Examples:?I think that?s okay.?
?It?s right between the mermaid and the car.?
?Yeah, that?s right.
?32Gravano, Hirschberg, and Ben?u?
Affirmative Cue Words in Task-Oriented Dialogue[Agr] Acknowledge/Agreement: The function of okay that indicates ?I believe what yousaid?, and/or ?I agree with what you say.?
This label should also be used for okay afteranother okay or after an evaluative comment like ?Great?
or ?Fine?
in its role as anacknowledgment.13 Examples:A: Do you have a blue moon?B: Yeah.A: Then move it to the left of the yellow mermaid.B: Okay, gotcha.
Let?s see... (Here, both okay and gotcha are labeled Agr.
)[CBeg] Cue Beginning: The function of okay that marks a new segment of a discourseor a new topic.
Test: could this use of okay be replaced by ?Now??
[PBeg] Pivot Beginning: (Agr+CBeg) When okay functions as both a cue word and as anAcknowledge/Agreement.
Test: Can okay be replaced by ?Okay now?
with the samepragmatic meaning?
[CEnd] Cue Ending: The function of okay that marks the end of a current segment of adiscourse or a current topic.
Example: ?So that?s done.
Okay.?
[PEnd] Pivot Ending: (Agr+CEnd) When okay functions as both a cue word and as anAcknowledge/Agreement, but ends a discourse segment.
[BC] Backchannel: The function of okay in response to another speaker?s utterance thatindicates only ?I?m still here / I hear you and please continue.?
[Stl] Stall: Okay used to stall for time while keeping the floor.
Test: Can okay be replacedby an elongated ?Um?
or ?Uh?
with the same pragmatic meaning?
?So I yeah I thinkwe should go together.?
[Chk] Check: Okay used with the meaning ?Is that okay??
or ?Is everything okay??
Forexample, ?I?m stopping now, okay??
[BTsk] Back from a task: ?I?ve just finished what I was doing and I?m back.?
Typicalcase: One subject spends some time thinking, and then signals s/he is ready to continuethe discourse.Special Cases(1) ?okay so?
/ ?okay now?
/ ?okay then?
/ and so forth, where both words are utteredtogether, okay seems to convey Agr, and so / now / then seems to convey CBeg.
Becausewe do not label words like so, now, or then, we label okay as PBeg.
(2) If you encounter a rapid sequence of the same word several times in a row, all of themuttered in one ?burst?
of breath, mark only the first one the corresponding label, andlabel the others with ???.
Example: ?okay yeah yeah yeah?
should be labeled as: ?okay:Agryeah:Agr yeah:?
yeah:?
?.13 Throughout this article we have used the term ?agreement?
to avoid confusion with other definitions of?acknowledgment?
found in the literature.33Computational Linguistics Volume 38, Number 1Appendix C: ACW Labeling ExamplesThis appendix lists a number of examples of each type of ACWs from the ColumbiaGames Corpus, as labeled by our annotators.
Each ACW is highlighted and annotatedwith its majority label.
Overlapping speech segments are embraced by square brackets,and additional notes are given in parentheses.A: it?s aligned to the f- to the foot of the M&M guy like to the bottom of the ironB: okayAgr lines upA: yeahAgr it?s it?s almost it?s just barely like overB: okayAgrA: the tailB: mm-hmBCA: of the ironB: mm-hmBCA: is past the it?s a little bit past the mermaid?s bodyA: when you look at the lower left corner of the ironB: [okayBC]A: [where] the turquoise stuff is [and you]B: [mm-hmBC]A: know the bottom point out to the farthest left for that regionA: the blinking image is a lawnmowerB: okayBCA: and it?s gonna go below the yellow lion and above the bl- blue lionB: mm-hmBCA: the bottom black part is almost aligned to the white feet of the M&M guyB: [okayAgr]A: [yeahPEnd] (end-of-task)A: okayCBeg um the blinking image is the ironA: okayCBeg it?s uh the l- I guess the lime that?s blinkingA: nothing lined up real wellB: yeahAgr that?s rightModA: that was good okayCEndA: that?s awesomeB: you?re still the ace alrightCEndA: his beak?s kinda orange rightChkB: uh-huhAgrA: you can?t see any of that34Gravano, Hirschberg, and Ben?u?
Affirmative Cue Words in Task-Oriented DialogueA: that?s like a smaller amount than it is on the rightMod side to the ear [rightChk]B: [rightAgr]A: okayAgrA: the lower rightMod cornerB: yeahAgr the lower rightMod cornerA: let?s start overB: okayAgrA: okayPBeg so you have your crescent moonA: but not any of the yellow [part]B: [okayPBeg] so would the top of the ear be aligned to like whereA: the like head of the lion to like the where the grass shoots out there?s that?s a significantdifferenceB: okayPBeg so there?s definitely a bigger space from the blue lion to the lawnmower than thereis from the handle to the feet of the yellowA: alright?
I?ll try it (7.81 sec) okayBTskB: okayCBeg the owl is blinkingA: that thing is gonna be like (0.99 sec) okayStl (0.61 sec) one pixel to the rightMod of the edgeAcknowledgmentsThis work was supported in part byNSF IIS-0307905, NSF IIS-0803148,ANPCYT PICT-2009-0026, UBACYT20020090300087, CONICET, and the SlovakAgency for Science and Research Support(APVV-0369-07).
We thank Fadi Biadsy,H?ctor Ch?vez, Enrique Henestroza,Jackson Liscombe, Shira Mitchell, MichaelMulley, Andrew Rosenberg, Elisa SneedGerman, Ilia Vovsha, Gregory Ward, andLauren Wilcox for valuable discussionsand for their help in collecting, labeling,and processing the data.ReferencesAllwood, J., J. Nivre, and E. Ahlsen.
1992.
Onthe semantics and pragmatics of linguisticfeedback.
Journal of Semantics, 9(1):1?30.Beckman, Mary E. and Julia Hirschberg.1994.
The ToBI annotation conventions.Available on-line at http://www.ling.ohio-state.edu/?tobi/ame_tobi/annotation_conventions.html.Bevacqua, E., M. Mancini, and C. Pelachaud.2008.
A listening agent exhibiting variablebehavior.
In B. H. Prendinger, J. Lester, andM.
Ishizuka, editors, Intelligent VirtualAgents, pages 262?269.
Springer, Berlin.Bhuta, T., L. Patrick, and J. D. Garnett.2004.
Perceptual evaluation of voicequality and its correlation withacoustic measurements.
Journal ofVoice, 18(3):299?304.Boersma, Paul and David Weenink.
2001.Praat: Doing phonetics by computer.Available at http://www.praat.org.Brown, G., K. L. Currie, and J. Kenworthy.1980.
Questions of Intonation.
UniversityPark Press, Baltimore, MD.Bunt, H. C. 1989.
Information dialogues ascommunicative actions in relation to usermodelling and information processing.
InM.
M. Taylor, F. Neel, and D. G. Bouwhuis,editors, The Structure of MultimodalDialogue, pages 47?73.
Elsevier,Amsterdam.Bunt, H. C., R. Morante, and S. Keizer.
2007.An empirically based computationalmodel of grounding in dialogue.
In35Computational Linguistics Volume 38, Number 1Proceedings of the 8th SIGdial Workshop onDiscourse and Dialogue, pages 283?290,Antwerp.Cathcart, N., J. Carletta, and E. Klein.
2003.
Ashallow model of backchannel continuersin spoken dialogue.
In Proceedings of the10th Conference of the European Chapter of theAssociation for Computational Linguistics(EACL), pages 51?58, Budapest.Charniak, Eugene and Mark Johnson.
2001.Edit detection and parsing for transcribedspeech.
In Proceedings of the 2nd Meeting ofthe North American Chapter of the Associationfor Computational Linguistics (NAACL),pages 118?126, Pittsburgh, PA.Clark, H. H. and Susan Brennan.
1991.Grounding in communication.
In L.Resnick, J. Levine, and S. Teasley, editors,Perspectives on Socially Shared Cognition,pages 127?149.
American PsychologicalAssociation (APA), Hyattsville, MD.Clark, H. H. and E. F. Schaefer.
1989.Contributing to discourse.
CognitiveScience, 13:259?294.Cohen, Robin.
1984.
A computational theoryof the function of clue words in argumentunderstanding.
In Proceedings of the22nd Annual Meeting Association forComputational Linguistics (ACL),pages 251?258, Stanford, CA.Cohen, William C. 1995.
Fast effective ruleinduction.
In Proceedings of the 12thInternational Conference on MachineLearning, pages 115?123, Tahoe City, CA.Core, Mark G. 1998.
Analyzing andpredicting patterns of DAMSL utterancetags.
In Working Notes of the AAAI SpringSymposium on Applying Machine Learningto Discourse Processing, pages 18?24,Stanford, CA.Core, M. G. and J. Allen.
1997.
Codingdialogs with the damsl annotation scheme.In Proceedings of the AAAI Fall Symposiumon Communicative Action in Humans andMachines, pages 28?35, Cambridge, MA.Cortes, Corinna and Vladimir Vapnik.
1995.Support vector networks.
MachineLearning, 20(3):273?297.Duncan, Starkey.
1972.
Some signals andrules for taking speaking turns inconversations.
Journal of Personalityand Social Psychology, 23(2):283?292.Eskenazi, L., D. G. Childers, and D. M.Hicks.
1990.
Acoustic correlates of vocalquality.
Journal of Speech, Language andHearing Research, 33(2):298?306.Fleiss, J. L. 1971.
Measuring nominalscale agreement among many raters.Psychological Bulletin, 76(5):378?382.Garofolo, John S., Lori F. Lamel, William M.Fisher, Jonathan G. Fiscus, David S. Pallett,Nancy L. Dahlgren, and Victor Zue.1993.
Ldc93s1: Timit acoustic-phoneticcontinuous speech corpus.
Linguistic DataConsortium, University of Pennsylvania,Philadelphia.Godfrey, J. J., E. C. Holliman, andJ.
McDaniel.
1992.
SWITCHBOARD:Telephone speech corpus for research anddevelopment.
In Proceedings of the IEEEInternational Conference on Acoustics, Speech,and Signal Processing, pages 517?520,San Francisco, CA.Goodwin, C. 1981.
ConversationalOrganization: Interaction Between Speakersand Hearers.
Academic Press, New York.Gravano, Agust?n, Stefan Benus, H?ctorCh?vez, Julia Hirschberg, and LaurenWilcox.
2007.
On the role of contextand prosody in the interpretation of?okay?.
In Proceedings of the 45th AnnualMeeting of the Association for ComputationalLinguistics (ACL), pages 800?807,Prague.Gravano, Agust?n and Julia Hirschberg.2009a.
Backchannel-inviting cues intask-oriented dialogue.
In Proceedings ofInterspeech, pages 1019?1022, Brighton.Gravano, Agust?n and Julia Hirschberg.2009b.
Turn-yielding cues in task-orienteddialogue.
In Proceedings of the 10th SIGdialWorkshop on Discourse and Dialogue,pages 253?261, London.Gravano, Agust?n and Julia Hirschberg.2011.
Turn-taking cues in task-orienteddialogue.
Computer Speech and Language,25(3):601?634.Grosz, Barbara and Candace Sidner.
1986.Attention, intention, and the structureof discourse.
Computational Linguistics,12(3):175?204.Hirschberg, J.
1990.
Accent and discoursecontext: Assigning pitch accent insynthetic speech.
In Proceedings of the 8thNational Conference on Artificial Intelligence,volume 2, pages 952?957, Boston, MA.Hirschberg, Julia and Diane Litman.
1987.Now let?s talk about now: Identifyingcue phrases intonationally.
In Proceedingsof the 25th Annual Meeting of the Associationfor Computational Linguistics (ACL),pages 163?171, Stanford, CA.Hirschberg, Julia and Diane Litman.
1993.Empirical studies on the disambiguationof cue phrases.
Computational Linguistics,19(3):501?530.Hirschberg, Julia and Christine Nakatani.1996.
A prosodic analysis of discourse36Gravano, Hirschberg, and Ben?u?
Affirmative Cue Words in Task-Oriented Dialoguesegments in direction-giving monologues.In Proceedings of the 34th Annual MeetingAssociation for Computational Linguistics(ACL), pages 286?293, Santa Cruz, CA.Hjalmarsson, Anna.
2009.
On cue?Additiveeffects of turn-regulating phenomena indialogue.
In Proceedings of Diaholmia?13th Workshop on the Semantics andPragmatics of Dialogue, pages 27?34,Stockholm.Hjalmarsson, Anna.
2011.
The additiveeffect of turn-taking cues in human andsynthetic voice.
Speech Communication,53(1):23?35.Hobbs, Jerry R. 1990.
ThePierrehumbert-Hirschberg theory ofintonational meaning made simple:Comments on Pierrehumbert andHirschberg.
In P. R. Cohen, J. Morgan,and M. E. Pollack, editors, Intentions inCommunication.
MIT Press, Cambridge,MA, pages 313?323.Hockey, B.
A.
1993.
Prosody and the roleof ?okay?
and ?uh-huh?
in discourse.In Proceedings of the Eastern StatesConference on Linguistics, pages 128?136,Columbus, OH.Jefferson, G. 1984.
Notes on a systematicdeployment of the acknowledgementtokens ?yeah?
; and ?mm hm?.
Researchon Language & Social Interaction,17(2):197?216.Jekat, S., A. Klein, E. Maier, I. Maleck,M.
Mast, and J. J. Quantz.
1995.
Dialogueacts in VERBMOBIL.
Technical reportVerbmobil-Report 65, UniversitaetErlangen, Berlin.Jurafsky, Daniel, Elizabeth Shriberg,Barbara Fox, and Traci Curl.
1998.
Lexical,prosodic, and syntactic cues for dialogacts.
In Proceedings of ACL/COLING,Workshop on Discourse Relations andDiscourse Markers, pages 114?120,Montreal.Kendon, A.
1967.
Some functions ofgaze-direction in social interaction.Acta Psychologica, 26:22?63.Koiso, H., Y. Horiuchi, S. Tutiya, A. Ichikawa,and Y. Den.
1998.
An analysis ofturn-taking and backchannels based onprosodic and syntactic features in JapaneseMap Task dialogs.
Language and Speech:Special Issue on Prosody and Conversation,41(3-4):295?321.Kowtko, Jacqueline C. 1996.
The Functionof Intonation in Task-Oriented Dialogue.Ph.D.
thesis, University of Edinburgh.Lampert, A., R. Dale, and C. Paris.
2006.Classifying speech acts using verbalresponse modes.
In Proceedings of theAustralasian Language TechnologyWorkshop, pages 34?41, Sydney.Litman, Diane.
1994.
Classifying cuephrases in text and speech using machinelearning.
In Proceedings of the 12th NationalConference on Artificial Intelligence - AAAI,pages 806?813, Seattle, WA.Litman, Diane.
1996.
Cue phraseclassification using machine learning.Journal of Artificial Intelligence, 5:53?94.Litman, Diane and Julia Hirschberg.
1990.Disambiguating cue phrases in text andspeech.
In Proceedings of the 13thInternational Conference on ComputationalLinguistics, pages 251?256, Helsinki.Litman, D. J. and J. F. Allen.
1987.
A planrecognition model for subdialoguesin conversations.
Cognitive Science,11(2):163?200.Litman, D. J. and R. J. Passonneau.
1995.Combining multiple knowledgesources for discourse segmentation.In Proceedings of the 33rd Annual Meetingof the Association for ComputationalLinguistics (ACL), pages 108?115,Cambridge, MA.Maatman, R. M., J. Gratch, and S. Marsella.2005.
Natural behavior of a listeningagent.
In 5th International Conference onIntelligent Virtual Agents, pages 25?36, Kos.Marcus, M. P., M. A. Marcinkiewicz, andB.
Santorini.
1993.
Building a largeannotated corpus of English: The PennTreebank.
Computational Linguistics,19(2):313?330.Morency, L. P., I. de Kok, and J. Gratch.2008.
Predicting listener backchannels:A probabilistic multimodal approach.In Proceedings of the 8th InternationalConference on Intelligent Virtual Agents,pages 176?190, Tokyo.Mushin, I., L. Stirling, J. Fletcher, andR.
Wales.
2003.
Discourse structure,grounding, and prosody in task-orienteddialogue.
Discourse Processes, 35(1):1?31.Novick, D. G. and S. Sutton.
1994.
Anempirical model of acknowledgment forspoken-language systems.
In Proceedingsof the 32nd Annual Meeting of the Associationfor Computational Linguistics (ACL),pages 96?101, Morristown, NJ.Pierrehumbert, Janet and Julia Hirschberg.1990.
The meaning of intonationalcontours in the interpretation ofdiscourse.
In P. R. Cohen, J. Morgan,and M. E. Pollack, editors, Intentions inCommunication.
MIT Press, Cambridge,MA, pages 271?311.37Computational Linguistics Volume 38, Number 1Pierrehumbert, J.
B.
1980.
The Phonology andPhonetics of English Intonation.
Ph.D. thesis,Massachusetts Institute of Technology,Cambridge, MA.Pitrelli, John F., Mary E. Beckman, and JuliaHirschberg.
1994.
Evaluation of prosodictranscription labeling reliability in theToBI framework.
In Proceedings of theInternational Conference of Spoken LanguageProcessing (ICSLP), pages 123?126,Yokohama.Quinlan, John Ross.
1993.
C4.5: Programs forMachine Learning.
Morgan Kaufmann,Waltham, MA.Ratnaparkhi, A., E. Brill, and K. Church.1996.
A maximum entropy model forpart-of-speech tagging.
In Proceedings of theConference on Empirical Methods in NaturalLanguage Processing, pages 133?142,Philadelphia, PA.Redeker, G. 1991. Review article: Linguisticmarkers of linguistic structure.
Linguistics,29(6):1139?1172.Reichman, Rachel.
1985.
Getting Computersto Talk like You and Me.
MIT Press,Cambridge, MA.Reithinger, N. and M. Klesen.
1997.
Dialogueact classification using language models.In Proceedings of the 5th European Conferenceon Speech Communication and Technology,pages 2235?2238, Rhodes.Roque, A. and D. Traum.
2009.
Improving avirtual human using a model of degreesof grounding.
In Proceedings of the 21stInternational Joint Conferences on ArtificialIntelligence (IJCAI), pages 1537?1542,Pasadena, CA.Rosenberg, A. and J. Hirschberg.
2009.Detecting pitch accent at the word,syllable and vowel level.
In Proceedingsof the North American Chapter ofthe Association for ComputationalLinguistics ?
Human LanguageTechnologies (NAACL-HLT) Conference,pages 81?84, Boulder, CO.Rosenberg, Andrew.
2010a.
AuToBI ?
Atool for automatic ToBI annotation.
InProceedings of Interspeech, pages 146?149,Makuhari.Rosenberg, Andrew.
2010b.
Classificationof prosodic events using quantizedcontour modeling.
In Proceedings of theNorth American Chapter of the Associationfor Computational Linguistics ?
HumanLanguage Technologies (NAACL-HLT) Conference, pages 721?724,Los Angeles, CA.Sacks, H., E. A. Schegloff, and G. Jefferson.1974.
A simplest systematics for theorganization of turn-taking forconversation.
Language, 50:696?735.Schegloff, E. A.
1982.
Discourse as aninteractional achievement: Someuses of ?uh huh?
and other thingsthat come between sentences.
InTannen D, editor, Analyzing Discourse:Text and Talk, pages 71?93.
APA,Hyattsville, MD.Schiffrin, Deborah.
1987.
DiscourseMarkers.
Cambridge University Press,Cambridge, UK.Shriberg, E., R. Bates, A. Stolcke, P. Taylor,D.
Jurafsky, K. Ries, N. Coccaro, R. Martin,M.
Meteer, and C. Van Ess-Dykema.1998.
Can prosody aid the automaticclassification of dialog acts inconversational speech?
Languageand Speech, 41(3-4):443?492.Stolcke, A., K. Ries, N. Coccaro, E. Shriberg,R.
Bates, D. Jurafsky, P. Taylor, R. Martin,C.
V. Ess-Dykema, and M. Meteer.
2000.Dialogue act modeling for automatictagging and recognition of conversationalspeech.
Computational Linguistics,26(3):339?373.Traum, David.
1994.
A ComputationalTheory of Grounding in Natural LanguageConversation.
Ph.D. thesis, RochesterUniversity, Rochester, NY.Traum, David and James Allen.
1992.A speech acts approach to groundingin conversation.
In Proceedings of theInternational Conference on SpokenLanguage Processing (ICSLP),pages 137?140, Banff.Vapnik, Vladimir N. 1995.
The Nature ofStatistical Learning Theory.
Springer-Verlag,New York.Walker, M. A.
1992.
Redundancy incollaborative dialogue.
In Proceedingsof the 14th Conference on ComputationalLinguistics, pages 345?351, Morristown, NJ.Walker, M. A.
1993a.
InformationalRedundancy and Resource Bounds inDialogue.
Ph.D. thesis, University ofPennsylvania, Philadelphia, PA.Walker, M. A.
1993b.
When giveninformation is accented: Repetition,paraphrase and inference in dialogue.In LSA Annual Meeting, pages 231?240,Los Angeles, CA.Walker, M. A.
1996.
Inferring acceptance andrejection in dialogue.
Language and Speech,39(2-3).Ward, N. and W. Tsukahara.
2000.
Prosodicfeatures which cue back-channel responsesin English and Japanese.
Journal ofPragmatics, 32(8):1177?1207.38Gravano, Hirschberg, and Ben?u?
Affirmative Cue Words in Task-Oriented DialogueWitten, I. H. and E. Frank.
2000.
Data Mining:Practical Machine Learning Tools andTechniques with Java Implementations.Morgan Kaufmann, Waltham, MA.Yngve, V. H. 1970.
On getting a word inedgewise.
In Proceedings of the 6th RegionalMeeting of the Chicago Linguistic Society,volume 6, pages 657?677, Chicago, IL.Young, S., G. Evermann, M. Gales,D.
Kershaw, G. Moore, J. Odell,D.
Ollason, D. Povey, V. Valtchev, andP.
Woodland.
2006.
The HTK Book,version 3.4.
Available on-line athttp://htk.eng.cam.ac.uk.Zufferey, S. and A. Popescu-Belis.
2004.Towards automatic identification ofdiscourse markers in dialogs: The case of?like?.
In Proceedings of the 5th SIGdialWorkshop on Discourse and Dialogue,pages 63?71, Boston, MA.39
