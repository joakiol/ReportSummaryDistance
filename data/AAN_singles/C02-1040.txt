Learning Verb Argument Structurefrom Minimally Annotated Corpora?Anoop Sarkar and Woottiporn TripasaiDept.
of Computer and Information ScienceUniversity of Pennsylvania200 South 33rd Street,Philadelphia, PA 19104-6389 USA{anoop,tripasai}@linc.cis.upenn.eduAbstractIn this paper we investigate the task of automaticallyidentifying the correct argument structure for a setof verbs.
The argument structure of a verb allowsus to predict the relationship between the syntac-tic arguments of a verb and their role in the under-lying lexical semantics of the verb.
Following themethod described in (Merlo and Stevenson, 2001),we exploit the distributions of some selected fea-tures from the local context of a verb.
These fea-tures were extracted from a 23M word WSJ cor-pus based on part-of-speech tags and phrasal chunksalone.
We constructed several decision tree classi-fiers trained on this data.
The best performing clas-sifier achieved an error rate of 33.4%.
This workshows that a subcategorization frame (SF) learningalgorithm previously applied to Czech (Sarkar andZeman, 2000) is used to extract SFs in English.
Theextracted SFs are evaluated by classifying verbs intoverb alternation classes.1 IntroductionThe classification of verbs based on their underlyingthematic structure involves distinguishing verbs thattake the same number and category of argumentsbut assign different thematic roles to these argu-ments.
This is often termed as the classification ofverb diathesis roles or the lexical semantics of pred-icates in natural language (see (Levin, 1993; Mc-Carthy and Korhonen, 1998; Stevenson and Merlo,1999; Stevenson et al, 1999; Lapata, 1999; Lapataand Brew, 1999; Schulte im Walde, 2000)).
Fol-lowing the method described in (Merlo and Steven-son, 2001; Stevenson and Merlo, 1999; Stevenson et?
This research was supported in part by NSF grant SBR-89-20230.
Thanks to Paola Merlo, Dan Gildea, David Chiang, Ar-avind Joshi and the anonymous reviewers for their comments.Also thanks to Virginie Nanta for an earlier collaboration withthe first author on an unsupervised version of this work.al., 1999), we exploit the distributions of some se-lected features from the local context of a verb butwe differ from these previous studies in the use ofminimally annotated data to construct our classifier.The data we use is only passed through a part-of-speech tagger and a chunker which is used to iden-tify base phrasal categories such as noun-phrase andverb-phrase chunks to identify potential argumentsof each verb.Lexical knowledge acquisition plays an impor-tant role in corpus-based NLP.
Knowledge of verbselectional preferences and verb subcategorizationframes (SFs) can be extracted from corpora for usein various NLP tasks.
However, knowledge of SFsis often not fine-grained enough to distinguish vari-ous verbs and the kinds of arguments that they canselect.
We consider a difficult task in lexical knowl-edge acquisition: that of finding the underlying ar-gument structure which can be used to relate the ob-served list of SFs of a particular verb.
The task in-volves identifying the roles assigned by the verb toits arguments.
Consider the following verbs, eachoccuring with intransitive and transitive SFs1.Unergative(1) a.
The horse raced past the barn.b.
The jockey raced the horse past thebarn.Unaccusative(2) a.
The butter melted in the pan.b.
The cook melted the butter in the pan.1The examples are taken from (Merlo and Stevenson,2001).
See (Levin, 1993) for more information.
The partic-ular categorization that we use here is motivated in (Stevensonand Merlo, 1997)Object-Drop(3) a.
The boy washed.b.
The boy washed the hall.Each of the verbs above occurs with both the in-transitive and transitive SFs.
However, the verbsdiffer in their underlying argument structure.
Eachverb assigns a different role to their arguments in thetwo subcategorization possibilities.
For each verbabove, the following lists the roles assigned to eachof the noun phrase arguments in the SFs permittedfor the verb.
This information can be used for ex-tracting appropriate information about the relation-ships between the verb and its arguments.UnergativeINTRAN: NPagent racedTRAN: NPcauser raced NPagentUnaccusativeINTRAN: NPtheme meltedTRAN: NPcauser melted NPthemeObject-DropINTRAN: NPagent washedTRAN: NPagent washed NPthemeOur task is to identify the transitive and intransi-tive usage of a particular verb as being related viathis notion of argument structure.
This is called theargument structure classification of the verb.
In theremainder of this paper we will look at the problemof placing verbs into such classes automatically.Our results in this paper serve as a replicationand extension of the results in (Merlo and Steven-son, 2001).
Our main contribution in this paper isto show that a subcategorization frame (SF) learn-ing algorithm previously applied to Czech (Sarkarand Zeman, 2000) can be applied to English andevaluated by classifying verbs into verb alternationclasses.
We perform this task using only taggedand chunked data as input to our subcategorizationframe learning stage.
Our result can be compared toprevious work (Merlo and Stevenson, 2001) whichdid not use SF learning but used a 65M word WSJcorpus which was tagged as well as automaticallyparsed with a Treebank trained statistical parser.It is important to note that (Merlo and Stevenson,2001) extract some features using the tagged infor-mation (in fact, those features that we use SF learn-ing to extract) and other features using parse trees.2 The HypothesisWe create a probabilistic classifier that can automat-ically classify a set of verbs into argument structureclasses with a reasonable error rate.
We use the hy-pothesis introduced by (Stevenson and Merlo, 1999)that although a verb in a particular class can occurin all of the syntactic contexts as verbs from otherclasses the statistical distributions can be distin-guished.
In other words, verbs from certain classeswill be more likely to occur in some syntactic con-texts than others.
We identify features that pickout the verb occurences in these contexts.
By us-ing these features, we will attempt to determine theclassification of those verbs.
In the previous sec-tion we saw that we sometimes have noun-phrasearguments (NPcauser) as being a causer of the actiondenoted by the verb.
For example, (Stevenson andMerlo, 1999) show that a classifier can exploit thesecausativity facts to improve classifiction.We use some new features in addition to the onesproposed and used in (Merlo and Stevenson, 2001)for this task.
In addition, we include as a feature theprobabilistic classification of the verb as a transitiveor intransitive verb.
Thus the classifier is simula-neously placing each verb into the appropriate sub-categorization frame as well as identifying the un-derlying thematic roles of the verb arguments.In our experiment, we will consider the follow-ing set of classes (each of these were explained inthe previous section): unergative, unaccusative, andobject-drop.
We test 76 verbs taken from (Levin,1993) that are in one of these three classes.
The par-ticular verbs were chosen to include high frequencyas well as low frequency verb tokens in our partic-ular corpus of 23M words of WSJ text.2 We usedall instances of these verbs from the WSJ corpus.The data was annotated with the right classificationfor each verb and the classifier was trained on 90%of the verbs taken from the 23M word corpus andtested on 10% of the data using 10-fold cross vali-dation.
We describe the experiment in greater detail2The particular verbs selected were looked up in (Levin,1993) and the class for each verb in the classification systemdefined in (Stevenson and Merlo, 1997) was selected with somediscussion with linguists.in Section 4.3 Identifying subcategorization framesAn important part of identifying the argument struc-ture of the verb is to find the verb?s subcategoriza-tion frame (SF).
For this paper, we are interested inwhether the verb takes an intransitive SF or a tran-sitive SF.In general, the problem of identifying subcatego-rization frames is to distinguish between argumentsand adjuncts among the constituents modifying averb.
For example, in ?John saw Mary yesterday atthe station?, only ?John?
and ?Mary?
are requiredarguments while the other constituents are optional(adjuncts).3The problem of SF identification using statisti-cal methods has had a rich discussion in the lit-erature (Ushioda et al, 1993; Manning, 1993;Briscoe and Carroll, 1997; Brent, 1994) (also seethe refences cited in (Sarkar and Zeman, 2000)).
Inthis paper, we use the method of hypothesis testingto discover the SF for a given verb (Brent, 1994).Along with the techniques given in these papers,(Sarkar and Zeman, 2000; Korhonen et al, 2000)also discuss other methods for hypothesis testingsuch the use of the t-score statistic and the likeli-hood ratio test.
After experimenting with all threeof these methods we selected the likelihood ratiotest because it performed with higher accuracy ona small set of hand-annotated instances.
We use thedetermination of the verb?s SF as an input to our ar-gument structure classifier (see Section 4).The method works as follows: for each verb, weneed to associate a score to the hypothesis that a par-ticular set of dependents of the verb are argumentsof that verb.
In other words, we need to assign avalue to the hypothesis that the observed frame un-der consideration is the verb?s SF.
Intuitively, we ei-ther want to test for independence of the observedframe and verb distributions in the data, or we wantto test how likely is a frame to be observed with aparticular verb without being a valid SF.
We developthese intuitions by using the method of hypothe-sis testing using the likelihood ratio test.
For fur-3There is some controversy as to the correct subcategoriza-tion of a given verb and linguists often disagree as to what is theright set of SFs for a given verb.
A machine learning approachsuch as the one followed in this paper sidesteps this issue al-together, since it is left to the algorithm to learn what is anappropriate SF for a verb.
The stance taken in this paper is thatthe efficacy of SF learning is evaluated on some domain, as isdone here on learning verb alternations.ther background on this method of hypothesis test-ing the reader is referred to (Bickel and Doksum,1977; Dunning, 1993).3.1 Likelihood ratio testLet us take the hypothesis that the distribution of anobserved frame f in the training data is independentof the distribution of a verb v. We can phrase thishypothesis as p( f | v) = p( f | !v) = p( f ), thatis distribution of a frame f given that a verb v ispresent is the same as the distribution of f giventhat v is not present (written as !v).
We use the loglikelihood test statistic (Bickel and Doksum, 1977,209) as a measure to discover particular frames andverbs that are highly associated in the training data.k1 = c( f , v)n1 = c(v) = c( f , v) + c(!
f , v)k2 = c( f , !v)n2 = c(!v) = c( f , !v) + c(!
f , !v)where c(?)
are counts in the training data.
Using thevalues computed above:p1 =k1n1p2 =k2n2p = k1 + k2n1 + n2Taking these probabilities to be binomially dis-tributed, the log likelihood statistic (Dunning, 1993)is given by:?2 log ?
=2[log L(p1, k1, n1) + log L(p2, k2, n2) ?log L(p, k1, n2) ?
log L(p, k2, n2)]where,log L(p, n, k) = k log p + (n ?
k) log(1 ?
p)According to this statistic, the greater the value of?2 log ?
for a particular pair of observed frame andverb, the more likely that frame is to be valid SF ofthe verb.
If this value is above a certain threshold itis taken to be a positive value for the binary featureTRAN, else it is a positive feature for the binary fea-ture INTRAN in the construction of the classifier.44 Steps in Constructing the ClassifierTo construct the classifier, we will identify featuresthat can be used to accurately distinguish verbs intodifferent classes.
The features are computed to bethe probability of observing a particular feature witheach verb to be classified.
We use C5.0 (Quinlan,1992) to generate the decision tree classifier.
Thefeatures are extracted from a 23M word corpus ofWSJ text (LDC WSJ 1988 collection).
Note that thetraining and test data constructed from this set areproduced by the classification of individual verbsinto their respective classes taken from (Merlo andStevenson, 2001).We prepare the corpus by passing it throughAdwait Ratnaparkhi?s part-of-speech tagger (Rat-naparkhi, 1996) (trained on the Penn TreebankWSJ corpus) and then running Steve Abney?s chun-ker (Abney, 1997) over the entire text.
The outputof this stage and the input to our feature extractor isshown below.Pierre NNP nx 2Vinken NNP, ,61 CD ax 3years NNSold JJ, ,will MD vx 2join VBthe DT nx 2board NNas INa DT nx 3nonexecutive JJdirector NNNov.
NNP29 CD.
.We use the following features to construct theclassifier.
The first four features were discussed andmotivated in (Stevenson and Merlo, 1999; Merloand Stevenson, 2001).
In some cases, we havemodified the features to include information aboutpart-of-speech tags.
The discussion below clarifies4See (Sarkar and Zeman, 2000) for information on how thethreshold is selected.the similarities and changes.
The features we usedin addition are the last two in the following list,the part-of-speech features and the subcategoriza-tion frame features.
51. simple past (VBD), and past participle(VBN)2. active (ACT) and passive (PASS)3. causative (CAUS)4. animacy (ANIM)5.
Part of Speech of the subject noun-phrase andobject noun-phrase6.
transitive (TRAN) and intransitive (INTRAN)To calculate all the probability values of each fea-tures, we perform the following steps.4.1 Finding the main verb of the sentencesTo find the main verb, we constructed a determin-istic finite-state automaton that finds the main verbwithin the verb phrase chunks.
This DFA is usedin two steps.
First, to select a set of main verbsfrom which we select the final set of 76 verbs usedin our experiment.
Secondly, the actual set of verbsis incorporated into the DFA in the feature selectionstep.4.2 Obtaining the frequency distribution of thefeaturesThe general form of the equation we use to find thefrequency distribution of each feature of the verb isthe following:P(V j) =C(V j)?1?x?N C(Vx)where P(V j) is the distribution of feature j of theverb, N is the total number of features of the partic-ular type (e.g., the total number of CAUS featuresor ANIM features as described below) and C(Vj)is the number of times this feature of the verb wasobserved in the corpus.
The features computed us-ing this formula are: ACT, PASS, TRAN, INTRAN,VBD, and VBN.5Note that while (Stevenson and Merlo, 1999; Merlo andStevenson, 2001) used a TRAN/INTRAN feature, in their caseit was estimated in a completely different way using taggeddata.
Hence, while we use the same name for the feature here,it is not the same kind of feature as the one used in the citedwork.4.3 The causative feature: CAUSTo correctly obtain the causative values of the test-ing verbs, we needed to know the meaning ofthe sentences.
In this paper, we approximate thevalue by using the following approach.
Also, thecausative value is not a probability but a weightwhich is subsequently normalized.We extract the subjects and objects of verbs andput them into two sets.
We use the last noun of thesubject noun phrase and object noun phrase (taggedby NN, NNS, NNP, or NNPS), as the subject andobject of the sentences.
Then the causative value isCAUS = overlapsum of all subject and objects in multisetwhere the overlap is defined as the largest multisetof elements belonging to both subjects and objectsmultisets.If subject is in the set {a, a, b, c} and object is inset {a, d}, the intersection between both set will be{a, a}, and the causative value will be 2(4+2) =13 .If subject is in the set {a, a, b, c} and object isin the set {a, b, d}, the intersection between bothset will be {a, a, b}, and the causative value will be(2+1)(4+3) =37 .Note that using this measure, we expect to gethigher weights for tokens that occur frequently inthe object position and sometimes in the subject po-sition.
For example, CAUS({a, b}, {a, b}) = 24 whileCAUS({a, b}, {a, a, a}) = 35 .
This difference in theweight given by the CAUS feature is exploited inthe classifier.4.4 The animate feature: ANIMSimilar to CAUS, we can only approximate thevalue of animacy.
We use the following formula tofind the value:ANIM = number of occurrence of pronoun insubject/number of occurrence of verbsThe set of pronouns used are I, we, you, she, he,and they.
In addition we use the set of part-of-speech tags which are associated with animacy inPenn Treebank tagset as part of set of features de-scribed in the next section.4.5 Part of Speech of object and subjectThe part-of-speech feature picks up several subtlecues about the differences in the types of argumentsselected by the verb in its subject or object position.We count the occurrence of the head nouns ofthe subject noun phrase and the object noun phrase.Then, we find the frequency distribution by usingthe same formula as before:P(V j) =C(V j)?1?x?N C(Vx)where P(V j) is the distribution of part of speech j,N is the total number of relevant POS features andC(V j) is the number of occurrences of part of speechj.
Also, we limit the part of speech to only the fol-lowing tags of speech: NNP, NNPS, EX, PRP, andSUCH, where NNP is singular noun phrase, NNPSis plural noun phrase, EX is ?there?, PRP is personalpronoun, and SUCH is ?such?.4.6 Transitive and intransitive SF of the verbTo find values for this feature we use the techniquedescribed in Section 3.
For each verb in our listwe extract all the subsequent NP and PP chunksand their heads from the chunker output.
We thenperform subcategorization frame learning with allsubsets of these extracted potential arguments.
Thecounts are appropriately assigned to these subsets toprovide a well-defined model.
Using these countsand the methods in Section 3 we categorize a verbas either transitive or intransitive.
For simplicity,any number of arguments above zero is consideredto be a candidate for transitivity.4.7 Constructing the ClassifierAfter we obtain all the probabilistic distributionsof the features of our testing verbs, we then useC5.0 (Quinlan, 1992) to construct the classifier.
Thedata was annotated with the right classification foreach verb and the classifier was run on 10% of thedata using 10-fold cross validation.5 ResultsWe tried all possible feature combinations (individ-ual features and all possible conjunctions of thosefeatures) to explore the contributions of each fea-ture to the reduction of the error rate.
The followingare the results of the best performing feature combi-nations.With our base features, ACT, PASS, VBD, VBN,TRAN, and INTRAN we get the average error rateof 49.4% for 10 fold cross validation.
We can seethat when we add the CAUS feature, the average er-ror decreases to 41.1%.
The CAUS feature helpsin decreasing the error rate.
Also, when we addthe ANIM feature, we get a much better perfor-mance.
Our average error rate decreases to 37.5%.Features Average error rate SE Average error rate SEfrom Decision Tree from Rule SetTRAN, INTRAN, VBD, 49.4% 1.1% 67.7% 0.9%VBN, PASS, ACTTRAN, INTRAN, VBD, 41.1% 0.8% 40.8% 0.6%VBN, PASS, ACT, CAUSTRAN, INTRAN, VBD, 37.5% 0.8% 36.9% 1.0%VBN, PASS, ACT, ANIMTRAN, INTRAN, VBD, 39.2% 0.8% 38.1% 1.1%VBN, PASS, ACT, PARTOF SPEECHTRAN, INTRAN, VBD, 33.4% 0.7% 33.9% 0.8%VBN, PASS, ACT, CAUS,ANIMTRAN, INTRAN, VBD, 39.0% 0.7% 37.1% 0.9%VBN, PASS, ACT, CAUS,PART OF SPEECHTRAN, INTRAN, VBD, 35.8% 1.3% 35.9% 1.7%VBN, PASS, ACT, ANIM,PART OF SPEECHTRAN, INTRAN, VBD, 39.5% 1.0% 38.3% 1.0%VBN, PASS, ACT, CAUS,ANIM, PART OF SPEECHFigure 1: Results of the verb classification.
Bold face results are for the best performing set of features inthe classifier.This is the lowest error rate we can achieve byadding one extra feature in addition to the base fea-tures.
The ANIM feature is an important featurethat we can use to construct the classifier.
When weadd the PART OF SPEECH feature, the error ratealso decreases to 39.2%.
Therefore, the PART OFSPEECH also helps reduce the error rate as well.When we put together the CAUS feature and ANIMfeature, we achieve the lowest error rate, which is33.4%.
When we put the PART OF SPEECH andCAUS features together, the error rate does not re-ally decrease (39.0%), comparing to the result withonly the PART OF SPEECH feature.
The reason ofthis result should be that there are some parts of thePART OF SPEECH feature and CAUS feature thatoverlap.
When we add the ANIM and PART OFSPEECH features together, the error rate does de-crease to 35.8%.
Although the result is not as goodas result of using ANIM and CAUS features, thecombination of the ANIM and PART OF SPEECHfeatures could be considered effective features thatwe can use to construct the classifier.
We then com-bine all the features together.
The result as expectedis not very good.
The error rate is 39.5%.
The rea-son should be the same reason as the lower perfor-mance when combining the CAUS and PART OFSPEECH features.Note that the features TRAN/INTRAN areneeded for computing a large subset of the featuresused.
Hence we did not conduct any experimentswithout these features.
These experiments show thatthe use of SF learning can be useful to the perfor-mance of the verb alternation classifier.
The errorrate of the baseline classifier (picking the right ar-gument structure at chance) was 65.5%.
(Merlo andStevenson, 2001) calculate the expert-based upperbound at this task to be an error rate of 13.5%.Our best performing classifier achieves a 33.4%error rate.
In comparison, (Merlo and Stevenson,2001) obtain an error rate of 30.2% using a taggedand automatically parsed data set of 65M words ofWSJ text.
Thus, while we obtain a slightly worseerror rate, this is obtained using a much smaller setof training data.6 ConclusionIn this paper, we discussed a technique which auto-matically identified the correct argument structureof a set of verbs.
Our results in this paper serve asa replication and extension of the results in (Merloand Stevenson, 2001).
Our main contribution inthis paper is to show that with reasonable accuracy,this task can be accomplished using only tagged andchunked data.
In addition, we incorporate some ad-ditional features such as part-of-speech tags and theuse of subcategorization frame learning as part ofour classification algorithm.We exploited the distributions of selected featuresfrom the local context of the verb which was ex-tracted from a 23M word WSJ corpus.
We usedC5.0 to construct a decision tree classifier using thevalues of those features.
We were able to constructa classifier that has an error rate of 33.4%.
Thiswork shows that a subcategorization frame learningalgorithm (Sarkar and Zeman, 2000) can be appliedto the task of classifying verbs into verb alternationclasses.In future work, we would like to classify verbsinto alternation classes on a per-token basis (as isdone in the approach taken by Gildea (2002)) ratherthan the per-type we currently employ and also in-corporate information about word senses in orderto feasibly include verb alternation information ina statistical parser.ReferencesSteve Abney.
1997.
Part of speech tagging and par-tial parsing.
In S. Young and G. Bloothooft, editors,Corpus based methods in language and speech, pages118?136.
Dordrecht: Kluwer.Peter Bickel and Kjell Doksum.
1977.
MathematicalStatistics.
Holden-Day Inc.Michael Brent.
1994.
Acquisition of subcategorizationframes using aggregated evidence from local syntac-tic cues.
Lingua, 92:433?470.
Reprinted in Acquisi-tion of the Lexicon, L. Gleitman and B. Landau (Eds.
).MIT Press, Cambridge, MA.Ted Briscoe and John Carroll.
1997.
Automatic extrac-tion of subcategorization from corpora.
In Proceed-ings of the 5th ANLP Conference, pages 356?363,Washington, D.C. ACL.Ted Dunning.
1993.
Accurate methods for the statisticsof surprise and coincidence.
Computational Linguis-tics, 19(1):61?74, March.Daniel Gildea.
2002.
Probabilistic models of verb-argument structure.
In Proc.
of COLING-2002.A.
Korhonen, G. Gorrell, and D. McCarthy.
2000.
Sta-tistical filtering and subcategorization frame acquisi-tion.
In Proceedings of EMNLP 2000.Maria Lapata and Chris Brew.
1999.
Using subcate-gorization to resolve verb class ambiguity.
In Pas-cale Fung and Joe Zhou, editors, Proceedings ofWVLC/EMNLP, pages 266?274, 21-22 June.Maria Lapata.
1999.
Acquiring lexical generalizationsfrom corpora: A case study for diathesis alternations.In Proceedings of 37th Meeting of ACL, pages 397?404.Beth Levin.
1993.
English Verb Classes and Alterna-tions.
Chicago University Press, Chicago, IL.Christopher D. Manning.
1993.
Automatic acquisitionof a large subcategorization dictionary from corpora.In Proceedings of the 31st Meeting of the ACL, pages235?242, Columbus, Ohio.Diana McCarthy and Anna Korhonen.
1998.
Detect-ing verbal participation in diathesis alternations.
InProceedings of COLING/ACL-1998.
Student Session,pages 1493?1495.Paola Merlo and Suzanne Stevenson.
2001.
Auto-matic verb classification based on statistical distribu-tion of argument structure.
Computational Linguis-tics, 27(3):373?408.J.
Ross Quinlan.
1992.
C4.5: Programs for MachineLearning.
Series in Machine Learning.
Morgan Kauf-mann, San Mateo, CA.A.
Ratnaparkhi.
1996.
A Maximum Entropy Part-Of-Speech Tagger.
In Proc.
of the Empirical Methods inNatural Language Processing Conference, Universityof Pennsylvania.Anoop Sarkar and Daniel Zeman.
2000.
Automatic ex-traction of subcategorization frames for czech.
In Pro-ceedings of COLING-2000.Sabine Schulte im Walde.
2000.
Clustering verbs se-mantically according to their alternation behaviour.
InProceedings of the 18th International Conference onComputational Linguistics (COLING-2000), Saarbr-cken, Germany, August.Suzanne Stevenson and Paola Merlo.
1997.
Lexicalstructure and parsing complexity.
Language and Cog-nitive Processes, 12(2).Suzanne Stevenson and Paola Merlo.
1999.
Automaticverb classification using distributions of grammaticalfeatures.
In Proceedings of EACL ?99, pages 45?52,Bergen, Norway, 8?12 June.Suzanne Stevenson, Paola Merlo, Natalia Kariaeva, andKamin Whitehouse.
1999.
Supervised learning oflexical semantic verb classes using frequency distri-butions.
In Proceedings of SIGLEX99: StandardizingLexical Resources, College Park, Maryland.Akira Ushioda, David A. Evans, Ted Gibson, and AlexWaibel.
1993.
The automatic acquisition of frequen-cies of verb subcategorization frames from tagged cor-pora.
In Proc.
of the Workshop on Acquisition of Lex-ical Knowledge from Text, Columbus, OH.
