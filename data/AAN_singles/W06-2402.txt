Grouping Multi-word Expressions According to Part-Of-Speech inStatistical Machine TranslationPatrik LambertTALP Research CenterJordi Girona Salgado, 1-308034 Barcelona, Spainlambert@gps.tsc.upc.eduRafael BanchsTALP Research CenterJordi Girona Salgado, 1-308034 Barcelona, Spainrbanchs@gps.tsc.upc.eduAbstractThis paper studies a strategy for identify-ing and using multi-word expressions inStatistical Machine Translation.
The per-formance of the proposed strategy for var-ious types of multi-word expressions (likenouns or verbs) is evaluated in terms ofalignment quality as well as translation ac-curacy.
Evaluations are performed by us-ing real-life data, namely the EuropeanParliament corpus.
Results from trans-lation tasks from English-to-Spanish andfrom Spanish-to-English are presented anddiscussed.1 IntroductionStatistical machine translation (SMT) was origi-nally focused on word to word translation and wasbased on the noisy channel approach (Brown etal., 1993).
Present SMT systems have evolvedfrom the original ones in such a way that mainlydiffer from them in two issues: first, word-basedtranslation models have been replaced by phrase-based translation models (Zens et al, 2002) and(Koehn et al, 2003); and second, the noisy chan-nel approach has been expanded to a more generalmaximum entropy approach in which a log-linearcombination of multiple feature functions is im-plemented (Och and Ney, 2002).Nevertheless, it is interesting to call the atten-tion about one important fact.
Despite the changefrom a word-based to a phrase-based translationapproach, word to word approaches for inferringalignment models from bilingual data (Vogel et al,1996; Och and Ney, 2003) continue to be widelyused.On the other hand, from observing bilingualdata sets, it becomes evident that in some cases itis just impossible to perform a word to word align-ment between two phrases that are translations ofeach other.
For example, certain combination ofwords might convey a meaning which is somehowindependent from the words it contains.
This isthe case of bilingual pairs such as ?fire engine?and ?camio?n de bomberos?.Notice that a word-to-word alignment strategywould most probably1 provide the follow-ing Viterbi alignments for words containedin the previous example: ?camio?n:truck?,?bomberos:firefighters?, ?fuego:fire?, and?ma?quina:engine?.Of course, it cannot be concluded from theseexamples that a SMT system which uses a wordto word alignment strategy will not be able to han-dle properly the kind of word expression describedabove.
This is because there are other models andfeature functions involved which can actually helpthe SMT system to get the right translation.However these ideas motivate for exploringalternatives for using multi-word expression in-formation in order to improve alignment qualityand consequently translation accuracy.
In thissense, our idea of a multi-word expression (here-after MWE) refers in principle to word sequenceswhich cannot be translated literally word-to-word.However, the automatic technique studied in thiswork for extracting and identifying MWEs doesnot necessarily follow this definition rigorously.In a preliminary study (Lambert and Banchs,2005), we presented a technique for extractingbilingual multi-word expressions (BMWE) fromparallel corpora.
In that study, BMWEs identifiedin a small corpus2 were grouped as a unique to-1Of course, alignment results strongly depends on corpusstatistics.2VERBMOBIL (Arranz et al, 2003)9ken before training alignment models.
As a re-sult, both alignment quality and translation accu-racy were slightly improved.In this paper we applied the same BMWE ex-traction technique, with various improvements, toa large corpus (EPPS, described in section 4.1).Since this is a statistical technique, and frequen-cies of multi-word expressions are low (Baldwinand Villavicencio, 2002), the size of the corpus isan important factor.
A few very basic rules basedon part-of-speech have also been added to filter outnoisy entries in the dictionary.
Finally, BMWEshave been classified into three categories (nouns,verbs and others).
In addition to the impact of thewhole set, the impact of each category has beenevaluated separately.The technique will be explained in section 3, af-ter presenting the baseline translation system used(section 2).
Experimental results are presented insection 4.
Finally some conclusions are presentedand further work in this area is depicted.2 Baseline Translation SystemThis section describes the SMT approach that wasused in this work.
A more detailed descriptionof the presented translation system is available inMarin?o et al (2005).
This approach implementsa translation model which is based on bilingualn-grams, and was developed by de Gispert andMarin?o (2002).The bilingual n-gram translation model actu-ally constitutes a language model of bilingual unitswhich are referred to as tuples.
This model ap-proximates the joint probability between sourceand target languages by using 3-grams as it is de-scribed in the following equation:p(T, S) ?N?n=1p((t, s)n|(t, s)n?2, (t, s)n?1) (1)where t refers to target, s to source and (t, s)n tothe nth tuple of a given bilingual sentence pair.Tuples are extracted from a word-to-wordaligned corpus.
More specifically, word-to-word alignments are performed in both direc-tions, source-to-target and target-to-source, by us-ing GIZA++ (Och and Ney, 2003), and tuples areextracted from the union set of alignments accord-ing to the following constraints (de Gispert andMarin?o, 2004):?
a monotonous segmentation of each bilingualsentence pairs is produced,?
no word inside the tuple is aligned to wordsoutside the tuple, and?
no smaller tuples can be extracted without vi-olating the previous constraints.As a consequence of these constraints, only onesegmentation is possible for a given sentence pair.Figure 1 presents a simple example illustrating thetuple extraction process.Figure 1: Example of tuple extraction from analigned bilingual sentence pair.A tuple set is extracted for each transla-tion direction, Spanish-to-English and English-to-Spanish.
Then the tuple 3-gram models are trainedby using the SRI Language Modelling toolkit(Stolcke, 2002).The search engine for this translation systemwas developed by Crego et al (2005).
It imple-ments a beam-search strategy based on dynamicprogramming.
The decoder?s monotonic searchmodality was used.This decoder was designed to take into accountvarious different models simultaneously, so trans-lation hypotheses are evaluated by considering alog-linear combination of feature functions.
Thesefeature functions are the translation model, a tar-get language model, a word bonus model, a lexicalmodel and an inverse lexical model.3 Experimental ProcedureIn this section we describe the technique used tosee the effect of multi-words information on thetranslation model described in section 2.3.1 Bilingual Multi-words ExtractionFirst, BMWEs were automatically extracted fromthe parallel training corpus and the most relevantones were stored in a dictionary.3.1.1 Asymmetry Based ExtractionFor BMWE extraction, the method proposedby Lambert and Castell (2004) was used.
This10verdad .
.
.
.
.
.
+es .
.
.
.
.
+ .esto .
.
.
.
+ .
.
; .
.
.
+ .
.
.siento .
?+ .
.
.lo ?
?
.
.
.
.I ?m sorry, thisis trueFigure 2: There is an asymmetry in the word-to-word alignments of the idiomatic expression ?losiento ?
I ?m sorry?.
Source-target and target-source links are represented respectively by hor-izontal and vertical dashes.method is based on word-to-word alignmentswhich are different in the source-target and target-source directions, such as the alignments trainedto extract tuples (section 2).
Multi-words like id-iomatic expressions or collocations can typicallynot be aligned word-to-word, and cause a (source-target and target-source) asymmetry in the align-ment matrix.
An asymmetry in the alignment ma-trix is a sub-matrix where source-target and target-source links are different.
If a word is part of anasymmetry, all words linked to it are also part ofthis asymmetry.
An example is depicted in figure2.In this method, asymmetries in the training cor-pus are detected and stored as possible BMWEs.Accurate statistics are needed to score eachBMWE entry.
In the identification phase (sec-tion 3.3), these scores permit to prioritise forthe selection of some entries with respect to oth-ers.
Previous experiments (Lambert and Banchs,2005) have shown than the large set of bilingualphrases described in the following section pro-vides better statistics than the set of asymmetry-based BMWEs.3.1.2 Scoring Based on Bilingual PhrasesHere we refer to Bilingual Phrase (BP) as thebilingual phrases used by Och and Ney (2004).The BP are pairs of word groups which are sup-posed to be the translation of each other.
The setof BP is consistent with the alignment and con-sists of all phrase pairs in which all words withinthe target language are only aligned to the wordsof the source language and vice versa.
At leastone word of the target language phrase has to bealigned with at least one word of the source lan-guage phrase.
Finally, the algorithm takes into ac-count possibly unaligned words at the boundariesof the target or source language phrases.We extracted all BP of length up to four words,with the algorithm described by Och and Ney.Then we estimated the phrase translation proba-bility distribution by relative frequency:p(t|s) =N(t, s)N(s)(2)In equation 2, s and t stand for the source andtarget side of the BP, respectively.
N(t, s) is thenumber of times the phrase s is translated by t,and N(s) is the number of times s occurs in thecorpus.
We took the minimum of both direct andinverse relative frequencies as probability of a BP.If this minimum was below some threshold, theBP was pruned.
Otherwise, this probability wasmultiplied by the number of occurrences N(t, s)of this phrase pair in the whole corpus.
A weight?
was introduced to balance the respective impor-tance of relative frequency and number of occur-rences, as shown in equation 3:score = min(p(t|s), p(s|t)) N(t, s)?= min(N(t, s)1+?N(s),N(t, s)1+?N(t))(3)We performed the intersection between the en-tire BP set and the entire asymmetry based multi-words set, keeping BP scores.
Notice that the en-tire set of BP is not adequate for our dictionary be-cause BP are extracted from all parts of the align-ment (and not in asymmetries only), so most BPare not BMWEs but word sequences that can bedecomposed and translated word to word.3.2 Lexical and Morpho-syntactic FiltersIn English and Spanish, a list of stop words3(respectively 19 and 26) was established.
TheBMWE dictionary was also processed by a Part-Of-Speech (POS) tagger and eight rules were writ-ten to filter out noisy entries.
These rules dependon the tag set used.
Examples of criteria to rejecta BMWE include:?
Its source or target side only contains stopwords?
Its source or target side ends with a coordina-tion conjunction3frequently occurring, semantically insignificant wordslike ?in?, ?of?, ?on?.11?
Its source or target side begins with a coordi-nation conjunction (except ?nor?, in English)?
Its source or target side ends with an indefi-nite determinerEnglish data have been POS-tagged using the TnTtagger (Brants, 2000), after the lemmas have beenextracted with wnmorph, included in the Wordnetpackage (Miller et al, 1991).
POS-tagging forSpanish has been performed using the FreeLinganalysis tool (Carreras et al, 2004).Finally, the BMWE set has been divided in threesubsets, according to the following criteria, ap-plied in this order:?
If source AND target sides of a BMWE con-tain at least a verb, it is assigned to the ?verb?class.?
If source AND target sides of a BMWE con-tain at least a noun, it is assigned to the?noun?
class.?
Otherwise, it is assigned to the ?misc?
class(miscellaneous).
Note that this class ismainly composed of adverbial phrases.3.3 Multi-Words IdentificationIdentification consists, first, of the detection of allpossible BMWE(s) in the corpus, and second, ofthe selection of the relevant candidates.The detection part simply means matching theentries of the dictionaries described in the previ-ous subsections.
In the example of figure 2, thefollowing BMWEs would have been detected (thenumber on the right is the score):i am sorry ||| lo siento ||| 1566am sorry ||| siento ||| 890it is ||| es ||| 1004407it is ||| esto es ||| 269true ||| es verdad ||| 63Then, selection in a sentence pair runs as fol-lows.
First, the BMWE with highest score amongthe possible candidates is considered and its cor-responding positions are set as covered.
If thisBMWE satisfies the selection criterion, the corre-sponding words in the source and target sentencesare grouped as a unique token.
This process is re-peated until all word positions are covered in thesentence pair, or until no BMWE matches the po-sitions remaining to cover.The selection criterion rejects candidates whosewords are linked to exactly one word.
Thus in theexample, ?esto ?
this is?
would not be selected.This is correct, because the subject ?esto?
(this)of the verb ?es?
(is) in Spanish is not omitted, sothat ?this is ?
es?
does not act as BMWE (?esto?should be translated to ?this?
and ?is?
to ?es?
).At the end of the identification process the sen-tence pair of figure 2 would be the following:?lo siento ; esto es verdad ?
I ?m sorry , this istrue?.In order to increase the recall, BMWE detec-tion was insensitive to the case of the first letterof each multi-word.
The detection engine also al-lows a search based on lemmas.
Two strategiesare possible.
In the first one, search is first carriedout with full forms, so that lemmas are resorted toonly if no match is found with full forms.
In thesecond strategy, only lemmas are considered.3.4 Re-alignmentThe modified training corpus, with identifiedBMWEs grouped in a unique ?super-token?
wasaligned again in the same way as explained in sec-tion 2.
By grouping multi-words, we increased thesize of the vocabulary and thus the sparseness ofdata.
However, we expect that if the meaning ofthe multi-words expressions we grouped is effec-tively different from the meaning of the words theycontain, the individual word probabilities shouldbe improved.After re-aligning, we unjoined the super-tokensthat had been grouped in the previous stage, cor-recting the alignment set accordingly.
More pre-cisely, if two super-tokens A and B were linkedtogether, after ungrouping them into various to-kens, every word of A was linked to every wordof B.
Translation units were extracted from thiscorrected alignment, with the unjoined sentencepairs (i.e.the same as in the baseline).
So the onlydifference with respect to the baseline lied in thealignment, and thus in the distribution of transla-tion units and in lexical model probabilities.4 Experimental Results4.1 Training and Test DataOur task was word alignment and translation ofparliamentary session transcriptions of the Eu-ropean Parliament (EPPS).
These data are cur-rently available at the Parliament?s website.4 Theywere distributed through the TC-STAR consor-tium.5 The training and translation test data used4http://www.euro parl.eu.int/5http: //www.tc-star.org/12included session transcriptions from April 1996until September 2004, and from November 15thuntil November 18th, 2004, respectively.
Transla-tion test data include two reference sets.
Align-ment test data was a subset of the training data(Lambert et al, 2006).Table 1 presents some statistics of the variousdata sets for each considered language: English(eng) and Spanish (spa).
More specifically, thestatistics presented in Table 1 are, the total num-ber of sentences, the total number of words, thevocabulary size (or total number of distinct words)and the average number of words per sentence.1.a.- Training data setLang.
Sentences Words Vocab.
Aver.Eng 1.22 M 33.4 M 105 k 27.3Spa 1.22 M 35.0 M 151 k 28.61.b.- Test data set for translationLang.
Sentences Words Vocab.
Aver.Eng 1094 26.8 k 3.9 k 24.5Spa 840 22.7 k 4.0 k 27.01.c.- Word alignment referenceLang.
Sentences Words Vocab.
Aver.Eng 400 11.7 k 2.7 k 29.1Spa 400 12.3 k 3.1 k 30.4Table 1: Basic statistics for the considered training(a) translation test (b) and alignment test (c) datasets (M and k stands for millions and thousands,respectively).4.2 Evaluation measuresDetails about alignment evaluation can be foundin Lambert et al (2006).
The alignment test datacontain unambiguous links (called S or Sure) andambiguous links (called P or Possible).
If thereis a P link between two words in the reference, acomputed link (i.e.
to be evaluated) between thesewords is acceptable, but not compulsory.
On thecontrary, if there would be an S link between thesewords in the reference, a computed link wouldbe compulsory.
In this paper, precision refers tothe proportion of computed links that are presentin the reference.
Recall refers to the proportionof reference Sure links that were computed.
Thealignment error rate (AER) is given by the follow-ing formula:AER = 1?|A ?
GS |+ |A ?
G||A|+ |GS |(4)where A is the set of computed links, GS is the setof Sure reference links and G is the entire set ofreference links.As for translation evaluation, we used the fol-lowing measures:WER (word error rate) or mWER (multi-reference word error rate) The WER is theminimum number of substitution, insertionand deletion operations that must be per-formed to convert the generated sentence intothe reference target sentence.
For the mWER,a whole set of reference translations is used.In this case, for each translation hypothesis,the edit distance to the most similar sentenceis calculated.BLEU score This score measures the precision ofunigrams, bigrams, trigrams, and fourgramswith respect to a whole set of reference trans-lations, and with a penalty for too short sen-tences (Papineni et al, 2001).
BLEU mea-sures accuracy, thus larger scores are better.4.3 Multi-words in Training DataIn this section we describe the results of theBMWE extraction and detection techniques ap-plied to the training data.4.3.1 Description of the BMWE dictionariesParameters of the extraction process have beenoptimised with the alignment development corpusavailable with the alignment test corpus.
Withthese parameters, a dictionary of 60k entries wasextracted.
After applying the lexical and morpho-syntactic filters, 45k entries were left.
The best30k entries (hereinafter referred to as all) havebeen selected for the experiments and divided inthe three groups mentioned in section 3.2. verb,noun and misc (miscellaneous) dictionaries con-tained respectively 11797, 9709 and 8494 entries.Table 2 shows recall and precision for theBMWEs identified with each dictionary.
Thefirst line is the evaluation of the MWEs obtainedwith the best 30k entries of the dictionary beforefiltering.
Alignments evaluated in table 2 con-tained only links corresponding to the identifiedBMWEs.
For an identified BMWE, a link was in-troduced between each word of the source side andeach word of the target side.
Nevertheless, the testdata contained the whole set of links.From table 2 we see the dramatic effect of thefilters.
The precision for nouns is lower than for13Recall PrecisionBest 30k (no filters) 13.6 53.6Best 30k (filters) 11.4 79.3VERB (filters) 3.7 81.8NOUN (filters) 4.0 72.8MISC (filters) 4.1 80.8Table 2: Quality of the BMWEs identified fromthe various dictionaries.the other categories because many word groupswhich were identified, like ?European Parliament -Parlamento europeo?, are not aligned as a group inthe alignment reference.
Notice also that the datain table 2 reflects the precision of bilingual MWE,which is a lower bound of the precision of ?super-tokens?
formed in each sentence, the quantity thatmatters in our experiment.Identification of BMWE based on lemmas hasalso been experimented.
However, with lemmas,the selection phase is more delicate.
With our ba-sic selection criterion (see section 3.3), the qualityof MWEs identified was worse so we based iden-tification on full forms.Figure 3 shows the first 10 entries in the miscdictionary, along with their renormalised score.Notice that ?the EU - la UE?, ?young people -jo?venes?
and ?the WTO - la OMC?
have been in-correctly classified due to POS-tagging errors.the EU ||| la UE ||| 770731secondly ||| en segundo lugar ||| 610599however ||| sin embargo ||| 443042finally ||| por u?ltimo ||| 421879firstly ||| en primer lugar ||| 324396thirdly ||| en tercer lugar ||| 286924young people ||| jo?venes ||| 178571the WTO ||| la OMC ||| 174496once again ||| una vez ma?s ||| 169317once ||| una vez ||| 150139Figure 3: Examples of BMWEs of the misc cate-gory.4.3.2 BMWE Identification StatisticsTable 3 shows, for each language, the MWE vo-cabulary size after the identification process, andhow many times a MWE has been grouped as aunique token (instances).
The different numberof instances between Spanish and English corre-spond to one-to-many BMWEs.
In general moreMWEs are grouped in the Spanish side, becauseEnglish is a denser language.
However, the omis-sion of the subject in Spanish causes the inversesituation for verbs.Vocabulary InstancesENG SPA ENG SPAALL 12.2k 12.6k 1.28M 1.56MVERB 6.0k 3.3k 738k 237kNOUN 3.9k 5.9k 288k 827kMISC 3.1k 4.3k 336k 557kTable 3: Statistics for the BMWEs identified fromthe various dictionaries.
ALL refers to the 30k bestentries with filters.4.4 Alignment and Translation ResultsTables 4 and 5 show the effect of aligning the cor-pus when the various categories of multi-wordshave been previously grouped.IBM1 lexical probabilities baseline Allp(in other words|es decir) - 0.94p(words|decir) 0.23 0.0013p(other|decir) 0.026 6 10?5p(say|decir) 0.45 0.49Table 4: Single word lexical probabilities of thealignment model in the baseline and after group-ing MWE with all dictionary entries.
The multi-word tokens ?in other words?
and ?es decir?
donot exist in the baseline.In table 4 we see how word-to-word lexi-cal probabilities of the alignment model can befavourably modified.
In the baseline, due to pres-ence of the fixed expression ?in other words -es decir?, the probability of ?words?
given ?de-cir?
(?say?
in English) is high.
With this ex-pression grouped, probabilities p(words|decir) andp(other|decir) vanish, while p(say|decir) is rein-forced.
These observations allowed to expect thatwith many individual probabilities improved, aglobal improvement of the alignment would occur.However, table 5 shows that alignment is notbetter when trained with BMWEs grouped as aunique token.A closer insight into alignments confirms thatthey have not been improved globally.
Changeswith respect to the baseline are very localised andcorrespond directly to the grouping of the BMWEspresent in each sentence pair.Table 6 presents the automatic translation eval-14Recall Precision AERBaseline 76.3 85.0 19.4All 78.0 82.0 19.9Verb 77.0 84.5 19.3Noun 76.8 83.0 20.0Misc 77.0 84.1 19.4Table 5: Alignment resultsuation results.
In the Spanish to English direc-tion, BMWEs seem to have a negative influence.In the English to Spanish direction, no significantimprovement or worsening is observed.S?E E?SmWER BLEU mWER BLEUBaseline 34.4 0.547 40.2 0.472All 36.4 0.517 40.7 0.470Verb 35.1 0.537 40.2 0.472Noun 35.1 0.537 40.7 0.469Misc 35.8 0.527 41.1 0.466Table 6: Translation results in Spanish-to-English(S?E) and English-to-Spanish (E?S) directions.In order to understand these results better, weperformed a manual error analysis for the first 50sentences of the test corpus.
We analysed, for theexperiment with all dictionary entries (?All?
lineof table 6), the changes in translation with respectto the baseline.
We counted how many changeshad a neutral, positive or negative effect on trans-lation quality.
Results are shown in table 7.
Noticethat approximatively half of these changes weredirectly related to the presence some BMWE.This study permitted to see interesting qualita-tive features.
First, BMWEs have a clear influenceon translation, sometimes positive and sometimesnegative, with a balance which appears to be nullin this experiment.
In many examples BMWEsallowed a group translation instead of an incor-rect word to word literal translation.
For instance,?Red Crescent?
was translated by ?Media LunaRoja?
instead of ?Cruz Luna?
(cross moon).Two main types of error were observed.
Thefirst ones are related to the quality of BMWEs.
De-terminers, or particles like ?of?, which are presentin BMWEs are mistakenly inserted in the trans-lations.
Some errors are caused by inadequateBMWEs.
For example ?looking at ?
si anal-izamos?
(?if we analyse?)
cannot be used in thesense of looking with the eyes.
The second type oferror is related to the rigidity and data sparsenessintroduced in the bilingual n-gram model.
For ex-ample, when inflected forms are encapsulated ina BMWE, the model looses flexibility to trans-late the correct inflection.
Another typical erroris caused by the use of back-off (n-1)-grams inthe bilingual language model, when the n-gram isnot any more available because of increased datasparseness.The error analysis did not give explanation forwhy the effect of BMWEs is so different for dif-ferent translation directions.
A possible hypothe-sis would be that BMWEs help in translating froma denser language.
However, in this case, verbswould be expected to help relatively more in theSpanish to English direction, since there are moreverb group instances in the English side.Neutral Positive NegativeS?E 43 20 22E?S 49 19 17Table 7: Effect on quality of differences in thetranslations between the baseline and the BMWEexperiment with ?ALL?
dictionary.
S and E standfor Spanish and English, respectively.5 Conclusions and Further workWe applied a technique for extracting and usingBMWEs in Statistical Machine Translation.
Thistechnique is based on grouping BMWEs beforeperforming statistical alignment.
On a large cor-pus with real-life data, this technique failed toclearly improve alignment quality or translationaccuracy.After performing a detailed error analysis, webelieve that when the considered MWEs are fixedexpressions, grouping them before training helpsfor their correct translation in test.
However,grouping MWEs which could in fact be translatedword to word, doesn?t help and introduces unnec-essary rigidity and data sparseness in the models.The main strength of the n-gram translationmodel (its history capability) is reduced when tu-ples become longer.
So we plan to run this experi-ment with a phrase-based translation model.
Sincethese models use unigrams, they are more flexibleand less sensitive to data sparseness.Some errors were also caused by noise in theautomatic generation of BMWEs.
Thus filter-15ing techniques should be improved, and differ-ent methods for extracting and identifying MWEsmust be developed and evaluated.
Resources buildmanually, like Wordnet multi-word expressions,should also be considered.The proposed method considers the bilingualmulti-words as units ; the use of each side of theBMWEs as independent monolingual multi-wordsmust be considered and evaluated.AcknowledgementsThis work has been partially funded by the Eu-ropean Union under the integrated project TC-STAR - Technology and Corpora for Speechto Speech Translation -(IST-2002-FP6-506738,http://www.tc-star.org).ReferencesV.
Arranz, N. Castell, and J. Gime?nez.
2003.
Devel-opment of language resources for speech-to-speechtranslation.
In Proc.
of the International Conferenceon Recent Advances in Natural Language Process-ing (RANLP), Borovets, Bulgary, September, 10-12.T.
Baldwin and A. Villavicencio.
2002.
Extracting theunextractable: A case study on verb-particles.
InComputational Natural Language Learning Work-shop (CoNLL).T.
Brants.
2000.
Tnt ?
a statistical part-of-speechtagger.
In Proc.
of Applied Natural Language Pro-cessing (ANLP), Seattle, WA.P.
Brown, S. Della Pietra, V. Della Pietra, and R. Mer-cer.
1993.
The mathematics of statistical machinetranslation: Parameter estimation.
ComputationalLinguistics, 19(2):263?311.Xavier Carreras, I. Chao, L.
Padro?, and M. Padro?.2004.
Freeling: An open-source suite of lan-guage analyzers.
In Proc.
of the 4th InternationalConference on Linguistic Resources and Evaluation(LREC), Lisbon, Portugal, May.J.
M. Crego, J. Marin?o, and A. de Gispert.
2005.
Angram-based statistical machine translation decoder.In Proc.
of the 9th European Conf.
on Speech Com-munication and Technology (Interspeech), pages3185?88, Lisbon, Portugal.A.
de Gispert and J. Marin?o.
2002.
Using X-grams forspeech-to-speech translation.
Proc.
of the 7th Int.Conf.
on Spoken Language Processing, ICSLP?02,September.A.
de Gispert and J. Marin?o.
2004.
Talp: Xgram-based spoken language translation system.
Proc.
ofthe Int.
Workshop on Spoken Language Translation,IWSLT?04, pages 85?90, October.P.
Koehn, F.J. Och, and D. Marcu.
2003.
Statisticalphrase-based translation.
In Proc.
of the 41th An-nual Meeting of the Association for ComputationalLinguistics.P.
Lambert and R. Banchs.
2005.
Data inferred multi-word expressions for statistical machine translation.In Proc.
of Machine Translation Summit X, pages396?403, Phuket, Tailandia.P.
Lambert and N. Castell.
2004.
Alignment of parallelcorpora exploiting asymmetrically aligned phrases.In Proc.
of the LREC 2004 Workshop on the Amaz-ing Utility of Parallel and Comparable Corpora,Lisbon, Portugal, May 25.P.
Lambert, A. de Gispert, R. Banchs, and J. Marin?o.2006.
Guidelines for word alignment and manualalignment.
Accepted for publication in LanguageResources and Evaluation.J.
Marin?o, R. Banchs, J. M. Crego, A. de Gispert,P.
Lambert, J.A.
Fonollosa, and M. Ruiz.
2005.Bilingual n-gram statistical machine translation.
InProc.
of Machine Translation Summit X, pages 275?82, Phuket, Thailand.G.A.
Miller, R. Beckwith, C. Fellbaum, D. Gross,K.
Miller, and R. Tengi.
1991.
Five papers on word-net.
Special Issue of International Journal of Lexi-cography, 3(4):235?312.F.J.
Och and H. Ney.
2002.
Dicriminative trainingand maximum entropy models for statistical ma-chine translation.
In Proc.
of the 40th Annual Meet-ing of the Association for Computational Linguis-tics, pages 295?302, Philadelphia, PA, July.F.J.
Och and H. Ney.
2003.
A systematic comparisonof various statistical alignment models.
Computa-tional Linguistics, 29(1):19?51, March.F.J.
Och and H. Ney.
2004.
The alignment templateapproach to statistical machine translation.
Compu-tational Linguistics, 30(4):417?449, December.Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu.
2001.
Bleu: a method for automatic eval-uation of machine translation.
IBM Research Re-port, RC22176, September.A.
Stolcke.
2002.
SRILM: an extensible languagemodeling toolkit.
In Proc.
of the Int.
Conf.
on Spo-ken Language Processing, pages 901?904, Denver,CO.S.
Vogel, H. Ney, and C. Tillmann.
1996.
HMM-based word alignment in statistical translation.
InCOLING?96: The 16thInt.
Conf.
on ComputationalLinguistics, pages 836?841, Copenhagen, Denmark,August.R.
Zens, F.J. Och, and H. Ney.
2002.
Phrase-basedstatistical machine translation.
In Springer Verlag,editor, Proc.
German Conference on Artificial Intel-ligence (KI), september.16
