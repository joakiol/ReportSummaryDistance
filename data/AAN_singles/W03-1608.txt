Extracting Structural Paraphrases from Aligned Monolingual CorporaAli Ibrahim Boris Katz Jimmy LinMIT Artificial Intelligence Laboratory200 Technology SquareCambridge, MA 02139{aibrahim,boris,jimmylin}@ai.mit.eduAbstractWe present an approach for automaticallylearning paraphrases from aligned mono-lingual corpora.
Our algorithm worksby generalizing the syntactic paths be-tween corresponding anchors in alignedsentence pairs.
Compared to previouswork, structural paraphrases generated byour algorithm tend to be much longeron average, and are capable of captur-ing long-distance dependencies.
In ad-dition to a standalone evaluation of ourparaphrases, we also describe a questionanswering application currently under de-velopment that could immensely bene-fit from automatically-learned structuralparaphrases.1 IntroductionThe richness of human language allows people toexpress the same idea in many different ways; theymay use different words to refer to the same entity oremploy different phrases to describe the same con-cept.
Acquisition of paraphrases, or alternative waysto convey the same information, is critical to manynatural language applications.
For example, an ef-fective question answering system must be equippedto handle these variations, because it should be ableto respond to differently phrased natural languagequestions.While there are many resources that help sys-tems deal with single-word synonyms, e.g., Word-Net, there are few resources for multiple-word ordomain-specific paraphrases.
Because manuallycollecting paraphrases is time-consuming and im-practical for large-scale applications, attention hasrecently focused on techniques for automatically ac-quiring paraphrases.We present an unsupervised method for acquir-ing structural paraphrases, or fragments of syntactictrees that are roughly semantically equivalent, fromaligned monolingual corpora.
The structural para-phrases produced by our algorithm are similar to theS-rules advocated by Katz and Levin for questionanswering (1988), except that our paraphrases areautomatically generated.
Because there is disagree-ment regarding the exact definition of paraphrases(Dras, 1999), we employ that operating definitionthat structural paraphrases are roughly interchange-able within the specific configuration of syntacticstructures that they specify.Our approach is a synthesis of techniques devel-oped by Barzilay and McKeown (2001) and Linand Pantel (2001), designed to overcome the limi-tations of both.
In addition to the evaluation of para-phrases generated by our method, we also describea novel information retrieval system under develop-ment that is designed to take advantage of structuralparaphrases.2 Previous WorkThere has been a rich body of research on automati-cally deriving paraphrases, including equating mor-phological and syntactic variants of technical terms(Jacquemin et al, 1997), and identifying equiva-lent adjective-noun phrases (Lapata, 2001).
Unfor-tunately, both are limited in types of paraphrases thatthey can extract.
Other researchers have exploreddistributional clustering of similar words (Pereira etal., 1993; Lin, 1998), but it is unclear to what extentsuch techniques produce paraphrases.1Most relevant to this paper is the work of Barzi-lay and McKeown and the work of Lin and Pan-tel.
Barzilay and McKeown (2001) extractedboth single- and multiple-word paraphrases from asentence-aligned corpus for use in multi-documentsummarization.
They constructed an aligned corpusfrom multiple translations of foreign novels.
Fromthis, they co-trained a classifier that decided whetheror not two phrases were paraphrases of each otherbased on their surrounding context.
Barzilay andMcKeown collected 9483 paraphrases with an av-erage precision of 85.5%.
However, 70.8% of theparaphrases were single words.
In addition, theparaphrases were required to be contiguous.Lin and Pantel (2001) used a general text corpusto extract what they called inference rules, whichwe can take to be paraphrases.
In their algorithm,rules are represented as dependency tree paths be-tween two words.
The words at the ends of a pathare considered to be features of that path.
For eachpath, they recorded the different features (words)that were associated with the path and their respec-tive frequencies.
Lin and Pantel calculated the sim-ilarity of two paths by looking at the similarity oftheir features.
This method allowed them to extractinference rules of moderate length from general cor-pora.
However, the technique is computationallyexpensive, and furthermore can give misleading re-sults, i.e., paths having the opposite meaning oftenshare similar features.3 ApproachOur approach, like Barzilay and McKeown?s, is builton the application of sentence-alignment techniquesused in machine translation to generate paraphrases.The insight is simple: if we have pairs of sentenceswith the same semantic content, then the differencein lexical content can be attributed to variations inthe surface form.
By generalizing these differenceswe can automatically derive paraphrases.
Barzilayand McKeown perform this learning process by only1For example, ?dog?
and ?cat?
are recognized to be similar,but they are obviously not paraphrases of one another.considering the local context of words and their fre-quencies; as a result, paraphrases must be contigu-ous, and in the majority of cases, are only one wordlong.
We believe that disregarding the rich syntacticstructure of language is an oversimplification, andthat structural paraphrases offer several distinct ad-vantages over lexical paraphrases.
Long distance re-lations can be captured by syntactic trees, so thatwords in the paraphrases do not need to be contigu-ous.
Use of syntactic trees also buffers against mor-phological variants (e.g., different inflections) andsome syntactic variants (e.g., active vs. passive).Finally, because paraphrases are context-dependent,we believe that syntactic structures can encapsulatea richer context than lexical phrases.Based on aligned monolingual corpora, our tech-nique for extracting paraphrases builds on Lin andPantel?s insight of using dependency paths (derivedfrom parsing) as the fundamental unit of learningand using parts of those paths as features.
Basedon the hypothesis that paths between identical wordsin aligned sentences are semantically equivalent, wecan extract paraphrases by scoring the path fre-quency and context.
Our approach addresses thelimitations of both Barzilay and McKeown?s andLin and Pantel?s work: using syntactic structures al-lows us to generate structural paraphrases, and usingaligned corpora renders the process more computa-tionally tractable.
The following sections describeour approach in greater detail.3.1 Corpus AlignmentMultiple English translations of foreign novels, e.g.,Twenty Thousand Leagues Under the Sea by JulesVerne, were used for extraction of paraphrases.Although translations by different authors differslightly in their literary interpretation of the origi-nal text, it was usually possible to find correspond-ing sentences that have the same semantic content.Sentence alignment was performed using the Galeand Church algorithm (1991) with the following costfunction:cost of substitution = 1?
ncwanwncw: number of common wordsanw: average number of words in two stringsHere is a sample from two different translationsof Twenty Thousand Leagues Under the Sea:Ned Land tried the soil with his feet, as if to takepossession of it.Ned Land tested the soil with his foot, as if he werelaying claim to it.To test the accuracy of our alignment, we man-ually aligned 454 sentences from two different ver-sions of Chapter 21 from Twenty Thousand LeaguesUnder the Sea and compared the results of our au-tomatic alignment algorithm against the manuallygenerated ?gold standard.?
We obtained a precisionof 0.93 and recall of 0.88, which is comparable tothe numbers (P.94/R.85) reported by Barzilay andMcKeown, who used a different cost function for thealignment process.3.2 Parsing and PostprocessingThe sentence pairs produced by the alignment al-gorithm are then parsed by the Link Parser (Sleatorand Temperly, 1993), a dependency-based parser de-veloped at CMU.
The resulting parse structures arepost-processed to render the links more consistent:Because the Link Parser does not directly identifythe subject of a passive sentence, our postprocessortakes the object of the by-phrase as the subject bydefault.
For our purposes, auxiliary verbs are ig-nored; the postprocessor connects verbs directly totheir subjects, discarding links through any auxiliaryverbs.
In addition, subjects and objects within rela-tive clauses are appropriately modified so that thelinkages remained consistent with subject and objectlinkages in the matrix clause.
For sentences involv-ing verbs that have particles, the Link Parser con-nects the object of the verb directly to the verb it-self, attaching the particle separately.
Our postpro-cessor modifies the link structure so that the objectis connected to the particle in order to form a contin-uous path.
Predicate adjectives are converted into anadjective-noun modification link instead of a com-plete verb-argument structure.
Also, common nounsdenoting places and people are marked by consult-ing WordNet.3.3 Paraphrase ExtractionThe paraphrase extraction process starts by findinganchors within the aligned sentence pairs.
In our ap-proach, only nouns and pronouns serve as possibleanchors.
The anchor words from the sentence pairsare brought into alignment and scored by a simpleset of ordered heuristics:?
Exact string matches denote correspondence.?
Noun and matching pronoun (same gender andnumber) denote correspondence.
Such a matchpenalizes the score by 50%.?
Unique semantic class (e.g., places and people)denotes correspondence.
Such a match penal-izes the score by 50%.?
Unique part of speech (i.e., the only nounpair in the sentences) denotes correspondence.Such a match penalizes the score by 50%.?
Otherwise, attempt to find correspondence byfinding longest common substrings.
Such amatch penalizes the score by 50%.?
If a word occurs more than once in the alignedsentence pairs, all possible combinations areconsidered, but the score for such a correspond-ing anchor pair is further penalized by 50%.For each pair of anchors, a breadth-first search isused to find the shortest path between the anchorwords.
The search algorithm explicitly rejects pathsthat contain conjunctions and punctuation.
If validpaths are found between anchor pairs in both of thealigned sentences, the resulting paths are consideredcandidate paraphrases, with a default score of one(subjected to penalties imposed by imperfect anchormatching).Scores of candidate paraphrases take into accounttwo factors: the frequency of anchors with respectto a particular candidate paraphrase and the varietyof different anchors from which the paraphrase wasproduced.
The initial default score of any paraphraseis one (assuming perfect anchor matches), but foreach additional occurrence the score is incrementedby 12n, where n is the number of times the currentset of anchors has been seen.
Therefore, the effect ofseeing new sets of anchors has a big initial impact onthe score, but the additional increase in score is sub-jected to diminishing returns as more occurrences ofthe same anchor are encountered.countaligned sentences 27479parsed aligned sentences 25292anchor pairs 43974paraphrases 5925unique paraphrases 5502gathered paraphrases (score ?
1.0) 2886Table 1: Summary of the paraphrase generation pro-cessFigure 1: Distribution of paraphrase length4 ResultsUsing the approach described in previous sections,we were able to extract nearly six thousand differentparaphrases (see Table 1) from our corpus, whichconsisted of two translations of 20,000 Leagues Un-der the Sea, two translations of The Kreutzer Sonata,and three translations of Madame Bouvary.Our corpus was essentially the same as the oneused by Barzilay and McKeown, with the exceptionof some short fairy tale translations that we found tobe unsuitable.
Due to the length of sentences (sometranslations were noted for their paragraph-lengthsentences), the Link Parser was unable to producea parse for approximately eight percent of the sen-tences.
Although the Link Parser is capable of pro-ducing partial linkages, accuracy deteriorated signif-icantly as the length of the input string increased.The distribution of paraphrase length is shown inFigure 1.
The length of paraphrases is measured bythe number of words that it contains (discounting theanchors on both sides).To evaluate the accuracy of our results, 130Evaluator PrecisionEvaluator 1 36.2%Evaluator 2 40.0%Evaluator 3 44.6%Average 41.2%Table 2: Summary of judgments by human evalua-tors for 130 unique paraphrasesunique paraphrases were randomly chosen to beassessed by human judges.
The human assessorswere specifically asked whether they thought theparaphrases were roughly interchangeable with eachother, given the context of the genre.
We believe thatthe genre constraint was important because someparaphrases captured literary or archaic uses of par-ticular words that were not generally useful.
Thisshould not be viewed as a shortcoming of our ap-proach, but rather an artifact of our corpus.
In ad-dition, sample sentences containing the structuralparaphrases were presented as context to the judges;structural paraphrases are difficult to comprehendwithout this information.A summary of the judgments provided by humanevaluators is shown in Table 2.
The average preci-sion of our approach stands at just over forty per-cent; the average length of the paraphrases learnedwas 3.26 words long.
Our results also show thatjudging structural paraphrases is a difficult task andinter-assessor agreement is rather low.
All of theevaluators agreed on the judgments (either positiveor negative) only 75.4% of the time.
The averagecorrelation constant of the judgments is only 0.66.The highest scoring paraphrase was the equiva-lence of the possessive morpheme ?s with the prepo-sition of.
We found it encouraging that our algo-rithm was able to induce this structural paraphrase,complete with co-indexed anchors on the ends of thepaths, i.e., A?s B ??
B of A.
Some other interest-ing examples include:2A1???
liked O??
A2??A1???
fond OF??
of J??
A2Example: The clerk liked Monsieur Bovary.
?
?2Brief description of link labels: S: subject to verb; O: objectto verb; OF: certain verbs to of; K: verbs to particles; MV: verbsto certain modifying phrases.
See Link Parser documentationfor full descriptions.Score Threshold Avg.
Precision Avg.
Length Count?
1.0 40.2% 3.24 130?
1.25 46.0% 2.88 58?
1.5 47.8% 2.22 23?
1.75 38.9% 1.67 12Table 3: Breakdown of our evaluation resultsThe clerk was fond of Monsieur Bovary.A1s??
rush K??
over MV??
to J??
A2??A1s??
runMV??
toJ??
A2Example: And he rushed over to his son, who hadjust jumped into a heap of lime to whiten his shoes.??
And he ran to his son, who had just precipi-tated himself into a heap of lime in order to whitenhis boots.A1s??
put K??
on O??
A2??A1s??
wearO??
A2Example: That is why he puts on his best waistcoatand risks spoiling it in the rain.
??
That?s why hewears his new waistcoat, even in the rain!A1???
fit MV??
to I??
give O??
A2??A1???
appropriate MV??
to I??
supply O??
A2Example: He thought fit, after the first few mouth-fuls, to give some details as to the catastrophe.
?
?After the first few mouthfuls he considered it appro-priate to supply a few details concerning the catas-trophe.A more detailed breakdown of the evaluation re-sults can be seen in Table 3.
Increasing the thresh-old for generating paraphrases tends to increase theirprecision, up to a certain point.
In general, the high-est ranking structural paraphrases consisted of sin-gle word paraphrases of prepositions, e.g., at ??in.
Our algorithm noticed that different prepositionswere often interchangeable, which is something thatour human assessors disagreed widely on.
Beyond acertain threshold, the accuracy of our approach ac-tually decreases.5 DiscussionAn obvious first observation about our algorithm isthe dependence on parse quality; bad parses lead tomany bogus paraphrases.
Although the parse resultsfrom the Link Parser are far from perfect, it is un-clear whether other purely statistical parsers wouldfare any better, since they are generally trained oncorpora containing a totally different genre of text.However, future work will most likely include acomparison of different parsers.Examination of our results show that a better no-tion of constituency would increase the accuracyof our results.
Our algorithm occasionally gener-ates non-sensical paraphrases that cross constituentboundaries, for example, including the verb of asubordinate clause with elements from the matrixclause.
Other problems arise because our current al-gorithm has no notion of verb phrases; it often gen-erates near misses such as fail??
succeed, neglect-ing to include not as part of the paraphrase.However, there are problems inherent in para-phrase generation that simple knowledge of con-stituency alone cannot solve.
Consider the followingtwo sentences:John made out gold at the bottom of the well.John discovered gold near the bottom of the well.Which structural paraphrases should we be able toextract?made out X at Y??
discovered X near Ymade out X??
discovered Xat X??
near XArguably, all three paraphrases are valid, althoughopinions vary more regarding the last paraphrase.What is the optimal level of structure for para-phrases?
Obviously, this represents a tradeoff be-tween specificity and accuracy, but the ability ofstructural paraphrases to capture long-distance re-lationships across large numbers of lexical itemscomplicates the problem.
Due to the sparseness ofour data, our algorithm cannot make a good deci-sion on what constituents to generalize as variables;naturally, greater amounts of data would alleviatethis problem.
This current inability to decide on agood ?scope?
for paraphrasing was a primary rea-son why we were unable to perform a strict eval-uation of recall.
Our initial attempts at generatinga gold standard for estimating recall failed becausehuman judges could not agree on the boundaries ofparaphrases.The accuracy of our structural paraphrases ishighly dependent on the corpus size.
As can be seenfrom the numbers in Table 1, paraphrases are rathersparse?nearly 93% of them are unique.
Withoutadequate statistical evidence, validating candidateparaphrases can be very difficult.
Although our dataspareness problem can be alleviated simply by gath-ering a larger corpus, the type of parallel text ouralgorithm requires is rather hard to obtain, i.e., thereare only so many translations of so many foreignnovels.
Furthermore, since our paraphrases are ar-guably genre-specific, different applications may re-quire different training corpora.
Similar to the workof Barzilay and Lee (2003), who have applied para-phrase generation techniques to comparable corporaconsisting of different newspaper articles about thesame event, we are currently attempting to solve thedata sparseness problem by extending our approachto non-parallel corpora.We believe that generating paraphrases at thestructural level holds several key advantages overlexical paraphrases, from the capturing of long-distance relationships to the more accurate modelingof context.
The paraphrases generated by our ap-proach could prove to be useful in any natural lan-guage application where understanding of linguis-tic variations is important.
In particular, we are at-tempting to apply our results to improve the perfor-mance of question answering system, which we willdescribe in the following section.6 Paraphrases and Question AnsweringThe ultimate goal of our work on paraphrases isto enable the development of high-precision ques-tion answering system (cf.
(Katz and Levin, 1988;Soubbotin and Soubbotin, 2001; Hermjakob et al,2002)).
We believe that a knowledge base of para-phrases is the key to overcoming challenges pre-sented by the expressiveness of natural languages.Because the same semantic content can be expressedin many different ways, a question answering sys-tem must be able to cope with a variety of alternativephrasings.
In particular, an answer stated in a formthat differs from the form of the question presentssignificant problems:When did Colorado become a state?
(1a) Colorado became a state in 1876.
(1b) Colorado was admitted to the Union in 1876.Who killed Abraham Lincoln?
(2a) John Wilkes Booth killed Abraham Lincoln.
(2b) John Wilkes Booth ended Abraham Lincoln?slife with a bullet.In the above examples, question answering sys-tems have little difficulty extracting answers if theanswers are stated in a form directly derived fromthe question, e.g., (1a) and (2a); simple keywordmatching techniques with primitive named-entitydetection technology will suffice.
However, ques-tion answering systems will have a much harder timeextracting answers from sentences where they arenot obviously stated, e.g., (1b) and (2b).
To re-late question to answers in those examples, a systemwould need access to rules like the following:X became a state in Y?
?X was admitted to the Union in YX killed Y??
X ended Y?s lifeWe believe that such rules are best formulated atthe syntactic level: structural paraphrases representa good level of generality and provide much moreaccurate results than keyword-based approaches.The simplest approach to overcoming the ?para-phrase problem?
in question answering is via key-word query expansion when searching for candidateanswers:(AND X became state)??
(AND X admitted Union)(AND X killed)??
(AND X ended life)The major drawback of such techniques is over-generation of bogus answer candidates.
For ex-ample, it is a well-known result that query expan-sion based on synonymy, hyponymy, etc.
may actu-ally degrade performance if done in an uncontrolledmanner (Voorhees, 1994).
Typically, keyword-based query expansion techniques sacrifice signifi-cant amounts of precision for little (if any) increasein recall.The problems associated with keyword query ex-pansion techniques stem from the fundamental de-ficiencies of ?bag-of-words?
approaches; in short,they simply cannot accurately model the semanticcontent of text, as illustrated by the following pairsof sentences and phrases that have the same wordcontent, but dramatically different meaning:(3a) The bird ate the snake.
(3b) The snake ate the bird.
(4a) the largest planet?s volcanoes(4b) the planet?s largest volcanoes(5a) the house by the river(5b) the river by the house(6a) The Germans defeated the French.
(6b) The Germans were defeated by the French.The above examples are nearly indistinguishablein terms of lexical content, yet their meanings arevastly different.
Naturally, because one text frag-ment might be an appropriate answer to a questionwhile the other fragment may not be, a question an-swering system seeking to achieve high precisionmust provide mechanisms for differentiating the se-mantic content of the pairs.While paraphrase techniques at the keyword-levelvastly overgenerate, paraphrase techniques at thephrase-level undergenerate, that is, they are oftentoo specific.
Although paraphrase rules can eas-ily be formulated at the string-level, e.g., usingregular expression matching and substitution tech-niques (Soubbotin and Soubbotin, 2001; Hermjakobet al, 2002), such a treatment fails to capture im-portant linguistic generalizations.
For example, theaddition of an adverb typically does not alter the va-lidity of a paraphrase; thus, a phrase-level rule ?Xkilled Y?
??
?X ended Y?s life?
would not be ableto match an answer like ?John Wilkes Booth sud-denly ended Abraham Lincoln?s life with a bullet?.String-level paraphrases are also unable to handlesyntactic phenomenona like passivization, which areeasily captured at the syntactic level.We believe that answering questions at level ofsyntactic relations, that is, matching parsed rep-resentations of questions with parsed representa-tions of candidates, addresses the issues presentedabove.
Syntactic relations, basically simplified ver-sions of dependency structures derived from theLink Parser, can capture significant portions of themeaning present in text documents, while providinga flexible foundation on which to build machineryfor paraphrases.Our position is that question answering should beperformed at the level of ?key relations?
in additionto keywords.
We have begun to experiment with re-lations indexing and matching techniques describedabove using an electronic encyclopedia as the testcorpus.
We identified a particular set of linguisticphenomena where relation-based indexing can dra-matically boost the precision of a question answer-ing system (Katz and Lin, 2003).
As an example,consider a sample output from a baseline keyword-based IR system:What do frogs eat?
(R1) Alligators eat many kinds of small animalsthat live in or near the water, including fish, snakes,frogs, turtles, small mammals, and birds.
(R2) Some bats catch fish with their claws, and afew species eat lizards, rodents, birds, and frogs.
(R3) Bowfins eat mainly other fish, frogs, and cray-fish.
(R4) Adult frogs eat mainly insects and other smallanimals, including earthworms, minnows, and spi-ders.. .
.
(R32) Kookaburras eat caterpillars, fish, frogs, in-sects, small mammals, snakes, worms, and evensmall birds.Of the 32 sentences returned, only (R4) correctlyanswers the user query; the other results answer adifferent question?
?What eats frogs??
A bag-of-words approach fundamentally cannot differentiatebetween a query in which the frog is in the subjectposition and a query in which the frog is in the objectposition.
Compare this to the results produced byour relations matcher:What do frogs eat?
(R4) Adult frogs eat mainly insects and other smallanimals, including earthworms, minnows, and spi-ders.By examining subject-verb-object relations, oursystem can filter out irrelevant results and returnonly the correct responses.We are currently working on combining thisrelations-indexing technology with the automaticparaphrase generation technology described earlier.For example, our approach would be capable of au-tomatically learning a paraphrase like X eat Y??
Yis a prey of X; a large collection of such paraphraseswould go a long way in overcoming the brittlenessassociated with a relations-based indexing scheme.7 ContributionsWe have presented a method for automatically learn-ing structural paraphrases from aligned monolingualcorpora that overcomes the limitation of previousapproaches.
In addition, we have sketched how thistechnology can be applied to enhance the perfor-mance of a question answering system based on in-dexing relations.
Although we have not completeda task-based evaluation, we believe that the abilityto handle variations in language is key to buildingbetter question answering systems.8 AcknowledgementsThis research was funded by DARPA under contractnumber F30602-00-1-0545 and administered by theAir Force Research Laboratory.ReferencesRegina Barzilay and Lillian Lee.
2003.
Learning toparaphrase: An unsupervised approach using multiple-sequence alignment.
In Proceedings of HLT-NAACL2003.Regina Barzilay and Kathleen McKeown.
2001.
Extract-ing paraphrases from a parallel corpus.
In Proceed-ings of the 39th Annual Meeting of the Association forComputational Linguistics (ACL-2001).Mark Dras.
1999.
Tree Adjoining Grammar and the Re-luctant Paraphrasing of Text.
Ph.D. thesis, MacquarieUniversity, Australia.William A. Gale and Kenneth Ward Church.
1991.
Aprogram for aligning sentences in bilingual corpora.In Proceedings of the 29th Annual Meeting of the As-sociation for Computational Linguistics (ACL-1991).Ulf Hermjakob, Abdessamad Echihabi, and DanielMarcu.
2002.
Natural language based reformulationresource andWeb exploitation for question answering.In Proceedings of the Eleventh Text REtrieval Confer-ence (TREC 2002).Christian Jacquemin, Judith L. Klavans, and EvelyneTzoukermann.
1997.
Expansion of multi-word termsfor indexing and retrieval using morphology and syn-tax.
In Proceedings of the 35th Annual Meeting ofthe Association for Computational Linguistics (ACL-1997).Boris Katz and Beth Levin.
1988.
Exploiting lexicalregularities in designing natural language systems.
InProceedings of the 12th International Conference onComputational Linguistics (COLING-1988).Boris Katz and Jimmy Lin.
2003.
Selectively using re-lations to improve precision in question answering.
InProceedings of the EACL-2003 Workshop on NaturalLanguage Processing for Question Answering.Maria Lapata.
2001.
A corpus-based account of reg-ular polysemy: The case of context-sensitive adjec-tives.
In Proceedings of the Second Meeting of theNorth American Chapter of the Association for Com-putational Linguistics (NAACL-2001).Dekang Lin and Patrick Pantel.
2001.
DIRT?discoveryof inference rules from text.
In Proceedings of theACM SIGKDD Conference on Knowledge Discoveryand Data Mining.Dekang Lin.
1998.
Extracting collocations from text cor-pora.
In Proceedings of the First Workshop on Com-putational Terminology.Fernando Pereira, Naftali Tishby, and Lillian Lee.
1993.Distributional clustering of English words.
In Pro-ceedings of the 30th AnnualMeeting of the Associationfor Computational Linguistics (ACL-1991).Daniel Sleator and Davy Temperly.
1993.
Parsing En-glish with a link grammar.
In Proceedings of the ThirdInternational Workshop on Parsing Technology.Martin M. Soubbotin and Sergei M. Soubbotin.
2001.Patterns of potential answer expressions as clues to theright answers.
In Proceedings of the Tenth Text RE-trieval Conference (TREC 2001).Ellen M. Voorhees.
1994.
Query expansion usinglexical-semantic relations.
In Proceedings of the17th Annual International ACM SIGIR Conference onResearch and Development in Information Retrieval(SIGIR-1994).
