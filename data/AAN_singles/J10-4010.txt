Book ReviewStatistical Machine TranslationPhilipp Koehn(University of Edinburgh)Cambridge University Press, 2010, xii+433 pp; ISBN 978-0-521-87415-1, $60.00Reviewed byColin CherryNational Research Council CanadaStatistical Machine Translation provides a comprehensive and clear introduction to themost prominent techniques employed in the field of the same name (SMT).
This text-book is aimed at students or researchers interested in a thorough entry-point to thefield, and it does an excellent job of providing basic understanding for each of the manypieces of a statistical translation system.
I consider this book to be an essential additionto any advanced undergraduate course or graduate course on SMT.The book is divided into three parts: Foundations, Core Methods, and Advanced Topics.Foundations (75 pages) covers an introduction to translation, working with text, andprobability theory.
Core Methods (170 pages) covers the main components of a standardphrase-based SMT system.
Advanced Topics (125 pages) covers discriminative trainingand linguistics in SMT, including an in-depth discussion of syntactic SMT.
The text asa whole assumes a certain familiarity with natural language processing; though theFoundations section provides an effort to fill in the gaps, the book?s focus is decidedlytranslation.
As such, students unfamiliar with NLP may sometimes need to consult ageneral NLP text.The book aims to provide a thorough introduction to each component of a statisticaltranslation system, and it definitely succeeds in doing so.
Supplementing this corematerial for each chapter is a highly inclusive Further Reading section.
These sectionsprovide brief narratives highlighting many relevant papers and alternative techniquesfor each topic addressed in the chapter.
I suspect many readers will find these literaturepointers to be quite valuable, from students wishing to dive deeper, to experienced SMTresearchers wishing to get started in a new sub-field.
Each chapter also closes with ashort list of exercises.
Many of these are very challenging (accurately indicated by astar-rating system), and involve getting your hands dirty with tools downloaded fromthe Web.
The usefulness of these exercises will depend largely on the instructor?s tastes;I view them as a bonus rather than a core feature of the book.1.
Chapters 1?3: FoundationsThe first three chapters provide foundational knowledge for the rest of the book.
In-troduction provides an overview of the book and a brief history of machine transla-tion, along with a discussion of applications and an expansive list of resources.
Theoverview?s structure takes the form of a summary of each chapter.
This structure pro-vides an effective preview of what will be covered and in what order, but it does notfocus on typical introductionmaterial; for example, there is no one place set aside to con-vince the reader that SMT is a good idea, or to introduce concisely themain philosophiesbehind the field.
The history section is enjoyable, and I was glad to see a cautionaryComputational Linguistics Volume 36, Number 4note regarding machine translation?s history of high hopes and disappointments.
Theapplications section provides an excellent overview of where SMT sees actual use, andhelps the reader understand why translations do not always need to be prefect.Words, Sentences, Corpora provides a whirlwind tour of NLP basics, briefly touchingon a broad set of topics including Zipf?s law, parts-of-speech, morphology, and a num-ber of grammar formalisms.
To give an idea of just how brief coverage can be, the sectionon grammar covers four formalisms in five pages.
Nonetheless, these descriptionsshould be helpful when the concepts re-appear later in the book.
This chapter closeswith a discussion of parallel corpora and sentence alignment.
As these are central tothe business of SMT, I feel they might have been better placed in a translation-focusedchapter.Probability Theory covers the basic statistics needed to understand the ideas through-out the book.
This chapter is clear, and provides strong intuitions on important issuessuch as conditional probability.
There is a surprisingly large emphasis on binomial andnormal distributions, considering SMT?s heavy reliance on categorical distributions;however, these are needed to discuss significance testing and some language modelingtechniques covered later.2.
Chapters 4?8: Core MethodsThe next five chapters provide detailed descriptions of each of the major componentsof a phrase-based SMT system.
Word-based Methods discusses the five IBM translationmodels, with a brief detour to discuss the noisy channel model that motivates theIBM approach.
This chapter is best taken as a complement to Brown et al (1993)and Kevin Knight?s (1999) tutorial on the same subject, rather than a replacement.
Itprovides strong intuitions on what each IBMmodel covers and how each model works,including the clearest descriptions I have seen of IBM:3?5.
However, it does sometimesmake them seem a little mysterious.
For example, there is no attempt to explain whyIBM:1 always arrives at a global maximum, or to generalize when one can apply themathematical simplification that reduces IBM:1?s exponential sum over products to apolynomial product over sums.
One glaring omission from this chapter is a discussionof the alignment HMM (Vogel, Ney, and Tillmann 1996).
This elegant model is widelyused and widely extended, and I had expected to see it covered in detail.Chapters 5 and 6 on Phrase-based Models andDecoding cover the major algorithms inthe popular phrase-based SMT paradigm.
They are clear and fairly complete; this bookcould easily serve as an effective reference for these topics.
Phrase-basedModelsmotivatesthe use of phrases, and then covers phrase extraction along with the calculation ofphrase features, such as lexical weighting and lexicalized re-orderingmodels.
This chap-ter also marks the beginning of a careful dance, where log-linear models are introducedwithout having yet covered SMT evaluation or discriminative training.
These topicsare covered in Chapters 8 and 9, respectively.
This division of modeling and trainingis a reasonable strategy, given the amount of material required to understand the fullpipeline, but a student may need some extra guidance to understand the completepicture.
The Decoding chapter focuses on stack decoding, and it is extremely well-written, with great explanations of search and pruning strategies.
Alternative decodingformalisms, such as A* or finite-state decoding, are given short but effective summaries.This chapter makes phrasal SMT decoding feel easy.The next chapter covers Language Models.
Considering that this topic is given in-depth coverage in other NLP texts, I was surprised to see it covered quite thoroughlyhere as well.
This chapter covers a number of smoothing techniques, as well as some774Book Reviewpractical tips for handling large models.
As usual, the exposition is exceptionally clear,and each newmethod?s advantages are demonstrated with predicted counts or perplex-ity scores on Europarl data, which I found to be very useful.
For many SMT courses,this chapter will be sufficient to stand alone as both an introduction and a reference forlanguage modeling.Finally, the Core Methods section closes with a discussion of Evaluation.
This chapterdiscusses human evaluation, motivates automatic evaluation, and then covers themajorcontenders: word error rate, BLEU, and METEOR.
The discussion of BLEU?s shortcom-ings is very even-handed, perhaps a little pessimistic, and acknowledges all of the majorconcerns regarding the metric.3.
Chapters 9?11: Advanced TopicsThe final three chapters cover advanced topics, which include recent or not universallyadopted advances.
So at this point, one might expect that all of the major components ofa baseline phrase-based SMT system have been covered, but the final piece of the puzzledoes not come until Discriminative Training, which includes a discussion of minimumerror rate training (MERT) for the log-linear models introduced in Chapter 5.
Thischapter also covers n-best list extraction, n-best list re-ranking, and posterior methodssuch as Minimum Bayes Risk Decoding.
It also devotes a surprisingly large amount oftime to large-scale discriminative training, where thousands of parameter values canbe learned.
There is a lot of ground to cover here; consequently, much of the materialwill need to be supplemented with research papers or other texts if the instructor wantsto cover any one topic in depth.
The sections covering the learning methods used inparameter tuning (maximum entropy, MERT) did not feel as clear as the rest of thebook.
I suspect that a newcomer to the field will require some guidance to pick out theessential parts.Chapter 10 is on Integrating Linguistic Information, which is kind of a grab bag,covering linguistic pre-processing, syntactic features, and factored translation models.The pre-processing discussion includes transliteration, morphological normalization,compound splitting, and even syntactically motivated re-ordering of the input sen-tence.
The syntactic features section mostly covers n-best list re-ranking as done in theSmorgasbord paper (Och et al 2004).
Each of these topics is well motivated, and the textprovides a clear description of a prominent, recent solution.Finally, the book closes with Tree-based Models.
This chapter covers a lot of ground:first describing synchronous context-free grammars, and then describing both for-mally syntactic hierarchical grammars and linguistically syntactic synchronous-tree-substitution grammars in terms of this common formalism.
This is a very nicelypresented chapter.
It draws a lot of interesting connections between formalisms; forexample, tree-to-tree rule extraction and tree-to-string rule extraction are presented assimple constraints on hierarchical phrase extraction.
The description of chart parsingfor decoding is also very clear, and it draws many useful analogies to the materialpresented earlier for phrasal decoding.
I get the impression that many insights gainedwhile adding syntactic SMT into the Moses translation system have found their wayinto this chapter.4.
SummaryThis book?s existence indicates that the field of SMT has reached a point of maturitywhere it makes sense to discuss core and foundational techniques.
This book provides775Computational Linguistics Volume 36, Number 4a clear and comprehensive introduction to word, phrase, and tree-based translationmodeling, along with the decoding, training, and evaluation algorithms that make thesemodels work.
The text?s stated goal is to provide a thorough introduction, but I wouldalso recommend it as an effective reference for anyone interested in writing their ownSMT decoder, be it phrasal or syntactic.
Most importantly, this book makes the prospectof teaching a course devoted to SMT much less daunting, and it should provide avaluable resource to researchers or students looking to teach themselves.ReferencesBrown, Peter F., Stephen A. Della Pietra,Vincent J. Della Pietra, and Robert L.Mercer.
1993.
The mathematics ofstatistical machine translation:Parameter estimation.
ComputationalLinguistics, 19(2):263?312.Knight, Kevin 1999.
A statistical MTtutorial workbook.
Available at:http://www.isi.edu/?knight/.Och, Franz Josef, Daniel Gildea, SanjeevKhudanpur, Anoop Sarkar, Kenki Yamada,Alex Fraser, Shankar Kumar, Libin Shen,David Smith, Katherine Eng, Viren Jain,Zheng Jin, and Dragomir Radev.
2004.A smorgasbord of features for statisticalmachine translation.
In Proceedings of theHuman Language Technology Conferenceof the North American Chapter of theAssociation for Computational Linguistics:HLT-NAACL 2004, pages 161?168,Boston, MA.Vogel, Stephan, Hermann Ney, andCristoph Tillmann.
1996.
HMM-basedword alignment in statistical translation.In Proceedings, 16th International Conferenceon Computational Linguistics (COLING),pages 836?841, Copenhagen.Colin Cherry is a research officer at the National Research Council Canada.
His research interestsinclude structure prediction and induction, with application to parsing, morphology, pronuncia-tion, andmachine translation.
Cherry?s address is NRC Institute for Information Technology, 1200Montreal Road, M50:C-318, Ottawa, Ontario, Canada K1A 0R6; e-mail: Colin.Cherry@nrc-cnrc.gc.ca; URL: https://sites.google.com/site/colinacherry/.776
