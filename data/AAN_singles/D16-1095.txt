Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 950?954,Austin, Texas, November 1-5, 2016. c?2016 Association for Computational LinguisticsA General Regularization Framework for Domain AdaptationWei Lu1 and Hai Leong Chieu2 and Jonathan Lo?fgren31Singapore University of Technology and Design2DSO National Laboratories3Uppsala Universityluwei@sutd.edu.sg, chaileon@dso.org.sg, lofgren021@gmail.comAbstractWe propose a domain adaptation framework,and formally prove that it generalizes the fea-ture augmentation technique in (Daume?
III,2007) and the multi-task regularization frame-work in (Evgeniou and Pontil, 2004).
Weshow that our framework is strictly more gen-eral than these approaches and allows practi-tioners to tune hyper-parameters to encouragetransfer between close domains and avoid neg-ative transfer between distant ones.1 IntroductionDomain adaptation (DA) is an important problemthat has received substantial attention in natural lan-guage processing (Blitzer et al, 2006; Daume?
III,2007; Finkel and Manning, 2009; Daume?
III et al,2010).
In this paper, we propose a novel regular-ization framework which allows DA practitionersto tune hyper-parameters to encourage transfer be-tween close domains, and avoid negative transfer(Rosenstein et al, 2005) between distant ones.
Inour framework, model parameters in multiple do-mains are learned jointly and constrained to remainclose to one another.
In the transfer learning tax-onomy (Pan and Yang, 2010), our framework fallsunder the parameter-transfer category for multi-taskinductive learning.
We show that our frameworkgeneralizes the frustratingly easy domain adapta-tion (FEDA) in Daume?
III (2007), Finkel and Man-ning (2009), and the regularised multi-task learningof Evgeniou and Pontil (2004).
At the same time,it provides us with hyper-parameters to control theamount of transfer between domains.2 Domain Adaptation FrameworkGiven labeled data from N domains, D1, .
.
.
,DN ,traditional machine learning maximizes the follow-ing objective function for each domain Di:O(Di;wi) = Li(Di;wi)?
?i||wi||2, (1)and we maximize Li by tuning the parameter vectorwi.
For example, Li can be the log-likelihood or thenegative hinge loss.
The term ?i||wi||2 is the L2-regularization term where ?i is a positive scalar.
Inour framework, we propose to maximizeN?i=1Li(Di;wi)?N?i=1?0,i||wi||2?
?1?j<k?N?j,k||wj ?wk||2, (2)where ?j,k are parameters controlling the transferbetween domains.
In the next sections, we show howour framework generalizes existing works.2.1 Frustratingly Easy DAThe FEDA approach was introduced by Daume?III (2007) and later formalized by Finkel and Man-ning (2009) within a hierarchical Bayesian DAframework.
While simple, the approach has oftenbeen shown to be effective.
In this section, we showthat our framework generalizes the FEDA approach.The FEDA approach defines a new augmentedfeature space by duplicating each feature in Di to a?general?
domain.
Therefore each parameter in wihas a corresponding parameter in w0, and:L?i(Di;wi,w0) = Li(Di;wi + w0) (3)This directly leads to the following remark:950Remark For all i, for any wi,w0,d ?
Rm:L?i(Di;wi + d,w0 ?
d) = L?i(Di;wi,w0)The complete objective function involving N(N ?
2) domains is defined as follows:O?
(D; ,w0,w1, .
.
.
,wN )=N?i=1L?i(Di;wi,w0)?N?i=0?i||wi||2We first prove the following relation:Lemma 2.1 Assume(w?0, ...,w?N ) = arg maxw1,...,wN ,w0[ N?i=1L?i(Di;wi,w0)?
(?0||w0||2 +N?i=1?i||wi||2)],where ?0, ?1, .
.
.
, ?N > 0, then:?0w?0 =N?i=1?iw?i (4)Proof Let?s introduce the vector d as follows:d = 1?Ni=0 ?i(?0w?0 ?N?i=1?iw?i)(5)Denote (w?0, .
.
.
,w?N ) such that ?
0 ?
i ?
N ,w?i = w?i + d, and w?0 = w?0 ?
d.Based on the remark, L?i(Di;w?i,w?0) =L?i(Di;w?i ,w?0).
Let ?
= O?
(D;w?0, .
.
.
,w?N ) ?O?
(D;w?0, .
.
.
,w?N ).
Since (w?0, .
.
.
,w?N ) isoptimal, ?
?
0.
Moreover,?
=N?i=1L?i(Di;w?i,w?0)?N?i=0?i||w?i||2?N?i=1L?i(Di;w?i ,w?0) +N?i=0?i||w?i ||2= ?0||w?0||2 ?
?0||w?0 ?
d||2+N?i=1?i||w?i ||2 ?N?i=1?i||w?i + d||2= ?
( N?i=0?i)||d||2 +2d ?
(?0w?0 ?N?i=1?iw?i)= ?
( N?i=0?i)||d||2 + 2d ?
( N?i=0?i)d=( N?i=0?i)||d||2 ?
0Hence, ?
= 0 implying ||d|| = 0 and so d = 0.From the definition of d, Equation 4 holds.Next we state the following lemma (see supple-mentary material for the proof).Lemma 2.2 For any vectors v1,v2, .
.
.
,vN ?
Rm,any scalars ?0, ?1, .
.
.
, ?N ?
R+, let v0 =(?Ni=1 ?ivi)/?0, then the following always holds:?0||v0||2 +N?i=1?i||vi||2=N?i=1?0,i||vi + v0||2 +?1?j<k?N?j,k||vj ?
vk||2,where ?i,j = ?i?j?Nl=0 ?l, ?
0 ?
i < j ?
N.Now we state and prove the following theorem,which shows our framework generalizes FEDA.Theorem 2.3 For ?0, ?1, .
.
.
, ?N ?
R+, define?0 ?
i < j ?
N, ?i,j =?i?j?Nl=0 ?l,951the following holds:maxw1,w2,...,wN ,w0[ N?i=1L?i(Di;wi,w0)?
(?0||w0||2 +N?i=1?i||wi||2)]= maxw1,w2,...,wN[ N?i=1Li(Di;wi)??
?N?i=1?0,i||wi||2 +?1?j<k?N?j,k||wj ?wk||2???
?Proof Let (w?0, .
.
.
,w?N ) be a solution to the firstoptimization problem.
We have:LHS =N?i=1L?i(Di;w?i ,w?0)?
(?0||w?0||2 +N?i=1?i||w?i ||2)(6)Lemma 2.1 gives w?0 =(?Ni=1 ?iw?i)/?0.
In-troduce w?i = w?i + w?0.
Using Lemma 2.2, wehave:LHS =N?i=1L?i(Di;w?i ,w?0)??
?N?i=1?0,i||w?i + w?0||2 +?1?j<k?N?j,k||w?j ?w?k||2??=N?i=1Li(Di;w?i)??
?N?i=1?0,i||w?i||2 +?1?j<k?N?j,k||w?j ?w?k||2???
RHSNow, let (w?1,w?2, .
.
.
,w?N ) be an optimal so-lution to the second problem.
Given the rela-tion between ?i,j and ?0, ?1, .
.
.
, ?N , let w?0 =(?Ni=1 ?iw?i)/(?Nl=0 ?l), and w?i = w?i ?
w?0.We show in the supplementary material thatw?0 =1?0( N?i=1?iw?i)(7)Based on these and Lemma 2.2, we have:RHS =N?i=1Li(Di;w?i )??
?N?i=1?0,i||w?i ||2 +?1?j<k?N?j,k||w?j ?w?k||2?
?=N?i=1Li(Di;w?i + w?0)??
?N?i=1?0,i||w?i + w?0||2 +?1?j<k?N?j,k||w?j ?w?k||2??=N?i=1L?i(Di;w?i,w?0)?
(?0||w?0||2 +N?i=1?i||w?i||2)?
LHSTherefore we must have LHS = RHS.This formally shows that FEDA is equivalent tosolving the objective function given in Equation 2.In this new optimization problem, if we drop theterms involving ?j,k for j 6= 0, we have:N?i=1(Li(Di;wi)?
?0,i||wi||2)(8)This is learning without domain adaptation.
The ad-ditional regularization terms allow us keep the pa-rameters from different domains close to one other.In the special case with two domains, if we use thesame ?
for all regularization terms, we have the fol-lowing corollary:Corollary 2.4 For any ?
> 0:maxw1,w2,w0[L?1(D1;w1,w0) + L?2(D2;w2,w0)??
(||w1||2 + ||w2||2 + ||w0||2)]= maxw1,w2[L1(D1;w1) + L2(D2;w2)?13?
(||w1||2 + ||w2||2 + ||w1 ?w2||2)]Hence, the FEDA feature augmentation tech-nique indirectly introduces a regularization term thatpushes the source and target parameters as close952as possible.
This is related to the technique ofChelba and Acero (2006) where they regularize themodel parameters for the target domain using theterm ?||w ?
ws||, where ws is the parameter vec-tor learned from the source domain.
The differencehere is, in their work the parameters for the sourcedomain are learned first and then fixed.
The rela-tion between their work and the feature augmenta-tion technique was also briefly discussed in the paperof Daume?
III (2007).
We formally showed a preciserelation here in this paper.2.2 Regularized Multi-task LearningEvgeniou and Pontil (2004) proposed multi-taskregularized learning using support vector machines(SVM).
They decomposed the model weight vectoras a sum of domain-specific vectors and a generalvector, in much the same way as FEDA1.
Hence,both Lemma 2.1 and Theorem 2.3 of this paper ap-ply, and our framework also generalizes multi-taskregularized learning.3 Experimental ResultsIn this section we apply our framework to both struc-tured and un-structured tasks.
For structured pre-diction, we use the named-entity recognition (NER)ACE-2005 dataset with 7 classes and 6 domains.We apply the linear chain CRF (Lafferty et al,2001), and show results using standard and softmax-margin CRF (SM-CRF) (Gimpel and Smith, 2010),with features consisting of word shape features,neighboring words, previous prediction and pre-fixes/suffixes.
The second task is sentiment classi-fication on the Amazon review data set (Blitzer etal., 2007) from 4 domains, labeled positive or neg-ative.
We apply logistic regression (LR) and SVMusing unigram and bigram features.
All the mod-els used in this section are implemented on top ofa common framework, which was also used to im-plement various structured prediction models previ-ously (Lu, 2015; Lu and Roth, 2015; Muis and Lu,2016).
For each task we compare:TGT Trained only on the specific domain data,ALL Trained on the data from all domains,1They proved in Lemma 2.1 in their paper a similar relation-ship to Equation 4, but their proof assumes a SVM framework,and that ?1=?2=.
.
.
=?N .Model Dom.
TGT ALL AUG RFCRFbc 71.85 75.56 75.30 76.48bn 72.06 75.02 75.17 75.15cts 85.49 85.98 86.44 86.70nw 72.55 76.52 76.27 76.61un 67.09 72.99 72.90 73.12wl 64.38 69.66 69.46 69.90avg 72.24 75.96 75.92 76.33SM-CRFbc 72.33 75.54 75.04 76.50bn 72.18 74.86 75.10 75.44cts 85.68 85.96 86.15 86.89nw 72.70 76.19 75.92 76.50un 66.83 72.94 72.91 72.93wl 64.57 69.90 69.76 70.30avg 72.38 75.90 75.81 76.43Table 1: F-score on the ACE NER task.
The domains arebroadcast conversations (bc), broadcast news (bn), conversa-tional telephone speech (cts), newswire (nw), usenet (un) andweblog (wl).
The macro-average (avg) over the 6 domains isalso shown in the table.Model Dom.
TGT ALL AUG RFLRbook 75.83 79.33 79.00 80.67dvd 82.17 82.83 83.83 83.83elec.
84.67 84.67 84.83 84.83kit.
83.83 86.33 86.17 87.33avg 81.63 83.29 83.46 84.17SVMbook 76.83 80.67 80.33 81.00dvd 83.17 83.17 82.50 84.00elec.
85.00 86.50 85.83 85.67kit.
86.33 85.83 88.33 87.83avg 82.83 84.04 84.25 84.63Table 2: Accuracies on the sentiment classification task.
Thedomains are books (book), dvds (dvd), electronics (elec.)
andkitchen (kit.).
The macro-average (avg) over the four domainsare also shown in the table.AUG The FEDA approach, andRF Our proposed regularization framework.We use a 40/30/30 train-development-test split andreport the results on the test set.
The regularizationparameters were tuned on the development set overa logarithmic scale between 10?3 to 103.
For ourframework, we used random search to tune the pa-rameters, since an exhaustive search is too expen-sive (21 parameters for 6 domains).
We choose thewithin-domain ?0,i to be close to those used for theALL and AUG model, while choosing the other ?j,kto be 1-2 orders of magnitude higher.
A good modelcould quickly be found that generally beats the base-lines on the development set and also generalizeswell to the test set.
We show the results for NERin Table 1 and the sentiment task in Table 2.9534 DiscussionOur proof did not require any assumption about L,as long as L2 regularization is used.
This meansour result is applicable to a variety of models suchas SVM, LR, and CRF (where L2 regularizationis used for the latter two models).
Theoretically,we have shown the equivalence of DA optimiza-tion problems.
Empirically, for non-convex objec-tives, different approaches may arrive at differentsolutions.
However, for convex loss functions, ourobjective (Equation 2) is also convex, and all ap-proaches should share the same solution.We have shown that we can map the FEDA opti-mization problem to our framework.
The converseis false: for any problem in this family (with arbi-trary choices of ?
), we can only solve it using FEDAif there are only 2 domains, or if all regularizationhyper-parameters are equal.
Some parameter con-figurations in this family are ?unreachable?
by thefeature augmentation technique.
This is because inTheorem 2.3, the values of ?
?s are defined based on?
?s and therefore possess certain properties.
For ex-ample, they must at least satisfy such constraints as?i,k?k,j = ?i,l?l,j for any i ?
k, l ?
j.
We have seenthat some of those unreachable problems could giveus better empirical results.
Can we find an alterna-tive simple adaptation method such that all problemsin this family are ?reachable??
This is a questionthat needs to be addressed in future research.5 ConclusionIn this paper, we presented a framework for do-main adaptation that generalizes several previousworks (Daume?
III, 2007; Finkel and Manning, 2009;Evgeniou and Pontil, 2004).
Our approach allowspractitioners to specify the amount of transfer be-tween domains via regularization hyper-parameters.These parameters could be tuned based on intu-ition or using held-out data.
In future work wecould also seek to find methods that can auto-matically optimize these parameters.
The sup-plementary material of this paper is available athttp://statnlp.org/research/ml/.AcknowledgementsWe would like to thank the anonymous reviewers fortheir helpful comments, and Zhanming Jie for hishelp on this work.
The experiments of this workwere done when Jonathan Lo?fgren was a visitingstudent at Singapore University of Technology andDesign (SUTD).
This work is supported by MOETier 1 grant SUTDT12015008.ReferencesJohn Blitzer, Ryan McDonald, and Fernando Pereira.2006.
Domain adaptation with structural correspon-dence learning.
In Proceedings of EMNLP.John Blitzer, Mark Dredze, and Fernando Pereira.
2007.Biographies, bollywood, boom-boxes and blenders:Domain adaptation for sentiment classification.
InProceedings of ACL.Ciprian Chelba and Alex Acero.
2006.
Adaptation ofmaximum entropy capitalizer: Little data can help alot.
Computer Speech & Language, 20(4):382?399.Hal Daume?
III, Abhishek Kumar, and Avishek Saha.2010.
Frustratingly easy semi-supervised domainadaptation.
In Proceedings of 2010 Workshop on Do-main Adaptation for Natural Language Processing.Hal Daume?
III.
2007.
Frustratingly easy domain adapta-tion.
In Proceedings of ACL.Theodoros Evgeniou and Massimiliano Pontil.
2004.Regularized multi?task learning.
In Proceedings ofKDD.J.
R. Finkel and C.D.
Manning.
2009.
Hierarchicalbayesian domain adaptation.
In Proceedings of ACL.Kevin Gimpel and Noah A. Smith.
2010.
Softmax-margin crfs: Training log-linear models with costfunctions.
In Proceedings of HLT-NAACL.John D. Lafferty, Andrew McCallum, and Fernando C. N.Pereira.
2001.
Conditional random fields: Probabilis-tic models for segmenting and labeling sequence data.In Proceedings of ICML.Wei Lu and Dan Roth.
2015.
Joint mention extractionand classification with mention hypergraphs.
In Pro-ceedings of EMNLP.Wei Lu.
2015.
Constrained semantic forests for im-proved discriminative semantic parsing.
In Proceed-ings of ACL/IJCNLP.Aldrian Obaja Muis and Wei Lu.
2016.
Weak semi-markov crfs for noun phrase chunking in informal text.In Proceedings of NAACL.Sinno Jialin Pan and Qiang Yang.
2010.
A survey ontransfer learning.
IEEE Transactions on Knowledgeand Data Engineering, 22(10):1345?1359, October.Michael T. Rosenstein, Zvika Marx, Leslie Pack Kael-bling, and Thomas G. Dietterich.
2005.
To transferor not to transfer.
In In NIPS?05 Workshop, InductiveTransfer: 10 Years Later.954
