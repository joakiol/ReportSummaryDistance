Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the ACL, pages 638?646,Boulder, Colorado, June 2009. c?2009 Association for Computational LinguisticsExtracting Social Meaning: Identifying Interactional Style in SpokenConversationDan JurafskyLinguistics DepartmentStanford Universityjurafsky@stanford.eduRajesh RanganathComputer Science DepartmentStanford Universityrajeshr@cs.stanford.eduDan McFarlandSchool of EducationStanford Universitydmcfarla@stanford.eduAbstractAutomatically extracting social meaning andintention from spoken dialogue is an impor-tant task for dialogue systems and social com-puting.
We describe a system for detectingelements of interactional style: whether aspeaker is awkward, friendly, or flirtatious.We create and use a new spoken corpus of 9914-minute speed-dates.
Participants rated theirinterlocutors for these elements of style.
Us-ing rich dialogue, lexical, and prosodic fea-tures, we are able to detect flirtatious, awk-ward, and friendly styles in noisy natural con-versational data with up to 75% accuracy,compared to a 50% baseline.
We describe sim-ple ways to extract relatively rich dialogue fea-tures, and analyze which features performedsimilarly for men and women and which weregender-specific.1 IntroductionHow can we extract social meaning from speech, de-ciding if a speaker is particularly engaged in the con-versation, is uncomfortable or awkward, or is partic-ularly friendly and flirtatious?
Understanding thesemeanings and how they are signaled in language isan important sociolinguistic task in itself.
Extractingthem automatically from dialogue speech and textis crucial for developing socially aware computingsystems for tasks such as detection of interactionalproblems or matching conversational style, and willplay an important role in creating more natural dia-logue agents (Pentland, 2005; Nass and Brave, 2005;Brave et al, 2005).Cues for social meaning permeate speech at everylevel of linguistic structure.
Acoustic cues such aslow and high F0 or energy and spectral tilt are impor-tant in detecting emotions such as annoyance, anger,sadness, or boredom (Ang et al, 2002; Lee andNarayanan, 2002; Liscombe et al, 2003), speakercharacteristics such as charisma (Rosenberg andHirschberg, 2005), or personality features like extro-version (Mairesse et al, 2007; Mairesse and Walker,2008).
Lexical cues to social meaning abound.Speakers with links to depression or speakers whoare under stress use more first person singular pro-nouns (Rude et al, 2004; Pennebaker and Lay, 2002;Cohn et al, 2004), positive emotion words are cuesto agreeableness (Mairesse et al, 2007), and neg-ative emotion words are useful cues to deceptivespeech (Newman et al, 2003).
The number of wordsin a sentence can be a useful feature for extrovertedpersonality (Mairesse et al, 2007).
Finally, dia-log features such as the presence of disfluenciescan inform listeners about speakers?
problems in ut-terance planning or about confidence (Brennan andWilliams, 1995; Brennan and Schober, 2001).Our goal is to see whether cues of this sort areuseful in detecting particular elements of conversa-tional style and social intention; whether a speakerin a speed-dating conversation is judged by the in-terlocutor as friendly, awkward, or flirtatious.2 The CorpusOur experiments make use of a new corpus we havecollected, the SpeedDate Corpus.
The corpus isbased on three speed-dating sessions run at an elite638private American university in 2005 and inspiredby prior speed-dating research (Madan et al, 2005;Pentland, 2005).
The graduate student participantsvolunteered to be in the study and were promisedemails of persons with whom they reported mutualliking.
Each date was conducted in an open settingwhere there was substantial background noise.
Allparticipants wore audio recorders on a shoulder sash,thus resulting in two audio recordings of the approx-imately 1100 4-minute dates.
In addition to the au-dio, we collected pre-test surveys, event scorecards,and post-test surveys.
This is the largest sample weknow of where audio data and detailed survey infor-mation were combined in a natural experiment.The rich survey information included date per-ceptions and follow-up interest, as well as gen-eral attitudes, preferences, and demographic infor-mation.
Participants were also asked about theconversational style and intention of the interlocu-tor.
Each speaker was asked to report how of-ten their date?s speech reflected different conversa-tional styles (awkward, friendly, flirtatious, funny,assertive) on a scale of 1-10 (1=never, 10=con-stantly): ?How often did the other person behave inthe following ways on this ?date???.
We chose threeof these five to focus on in this paper.We acquired acoustic information by taking theacoustic wave file from each recorder and manuallysegmenting it into a sequence of wavefiles, each cor-responding to one 4-minute date.
Since both speak-ers wore microphones, most dates had two record-ings, one from the male recorder and one from thefemale recorder.
Because of mechanical, opera-tor, and experimenter errors, some recordings werelost, and thus some dates had only one recording.Transcribers at a professional transcription serviceused the two recordings to create a transcript foreach date, and time-stamped the start and end timeof each speaker turn.
Transcribers were instructedto mark various disfluencies as well as some non-verbal elements of the conversation such as laughter.Because of noise, participants who accidentallyturned off their mikes, and some segmentation andtranscription errors, a number of dates were not pos-sible to analyze.
19 dates were lost completely, andfor an additional 130 we lost one of the two audiotracks and had to use the remaining track to extractfeatures for both interlocutors.
The current study fo-cuses on the 991 remaining clean dates for whichwe had usable audio, transcripts, and survey infor-mation.3 The ExperimentsOur goal is to detect three of the style variables, inparticular awkward, friendly, or flirtatious speakers,via a machine learning classifier.
Recall that eachspeaker in a date (each conversation side) was la-beled by his or her interlocutor with a rating from1-10 for awkward, friendly, or flirtatious behavior.For the experiments, the 1-10 Likert scale ratingswere first mean-centered within each respondent sothat the average was 0.
Then the top ten percent ofthe respondent-centered meaned Likert ratings weremarked as positive for the trait, and the bottom tenpercent were marked as negative for a trait.
Thuseach respondent labels the other speaker as eitherpositive, negative, or NA for each of the three traits.We run our binary classification experiments topredict this output variable.For each speaker side of each 4-minute conversa-tion, we extracted features from the wavefiles andthe transcript, as described in the next section.
Wethen trained six separate binary classifiers (for eachgender for the 3 tasks), as described in Section 5.4 Feature ExtractionIn selecting features we drew on previous researchon the use of relatively simple surface features thatcue social meaning, described in the next sections.Each date was represented by the two 4-minutewavefiles, one from the recorder worn by eachspeaker, and a single transcription.
Because of thevery high level of noise, the speaker wearing therecorder was much clearer on his/her own recording,and so we extracted the acoustic features for eachspeaker from their own microphone (except for the130 dates for which we only had one audio file).
Alllexical and discourse features were extracted fromthe transcripts.All features describe the speaker of the conversa-tion side being labeled for style.
The features fora conversation side thus indicate whether a speakerwho talks a lot, laughs, is more disfluent, has higherF0, etc., is more or less likely to be considered flir-tatious, friendly, or awkward by the interlocutor.
We639also computed the same features for the alter inter-locutor.
Alter features thus indicate the conversa-tional behavior of the speaker talking with an inter-locutor they considered to be flirtatious, friendly, orawkward.4.1 Prosodic FeaturesF0 and RMS amplitude features were extracted us-ing Praat scripts (Boersma and Weenink, 2005).Since the start and end of each turn were time-marked by hand, each feature was easily extractedover a turn, and then averages and standard devia-tions were taken over the turns in an entire conversa-tion side.
Thus the feature F0 MIN for a conversationside was computed by taking the F0 min of each turnin that conversation side (not counting zero values ofF0), and then averaging these values over all turns inthe side.
F0 MIN SD is the standard deviation acrossturns of this same measure.Note that we coded four measures of f0 varia-tion, not knowing in advance which one was likelyto be the most useful: F0 MEAN SD is the deviationacross turns from the global F0 mean for the con-versation side, measuring how variable the speakersmean f0 is across turns.
F0 SD is the standard devia-tion within a turn for the f0 mean, and then averagedover turns, hence measures how variable the speak-ers f0 is within a turn.
F0 SD SD measures how muchthe within-turn f0 variance varies from turn to turn,and hence is another measure of cross-turn f0 vari-ation.
PITCH RANGE SD measures how much thespeakers pitch range varies from turn to turn, andhence is another measure of cross-turn f0 variation.4.2 Lexical FeaturesLexical features have been widely explored in thepsychological and computational literature.
Forthese features we drew mainly on the LIWC lexiconsof Pennebaker et al (2007), the standard for socialpsychological analysis of lexical features.
From thelarge variety of lexical categories in LIWC we se-lected ten that the previous work of Mairesse et al(2007) had found to be very significant in detect-ing personality-related features.
The 10 LIWC fea-tures we used were Anger, Assent, Ingest, Insight,Negemotion, Sexual, Swear, I, We, and You.
We alsoadded two new lexical features, ?past tense auxil-iary?, a heuristic for automatically detecting narra-F0 MIN minimum (non-zero) F0 per turn, av-eraged over turnsF0 MIN SD standard deviation from F0 minF0 MAX maximum F0 per turn, averaged overturnsF0 MAX SD standard deviation from F0 maxF0 MEAN mean F0 per turn, averaged over turnsF0 MEAN SD standard deviation (across turns) fromF0 meanF0 SD standard deviation (within a turn)from F0 mean, averaged over turnsF0 SD SD standard deviation from the f0 sdPITCH RANGE f0 max - f0 min per turn, averagedover turnsPITCH RANGESDstandard deviation from mean pitchrangeRMS MIN minimum amplitude per turn, aver-aged over turnsRMS MIN SD standard deviation from RMS minRMS MAX maximum amplitude per turn, aver-aged over turnsRMS MAX SD standard deviation from RMS maxRMS MEAN mean amplitude per turn, averagedover turnsRMS MEAN SD standard deviation from RMS meanTURN DUR duration of turn in seconds, averagedover turnsTIME total time for a speaker for a conversa-tion side, in secondsRATE OFSPEECHnumber of words in turn divided byduration of turn in seconds, averagedover turnsTable 1: Prosodic features for each conversation side,extracted using Praat from the hand-segmented turns ofeach side.tive or story-telling behavior, and Metadate, for dis-cussion about the speed-date itself.
The features aresummarized in Table 2.4.3 Dialogue Act and Adjacency Pair FeaturesA number of discourse features were extracted,drawing from the conversation analysis, disfluencyand dialog act literature (Sacks et al, 1974; Juraf-sky et al, 1998; Jurafsky, 2001).
While discoursefeatures are clearly important for extracting socialmeaning, previous work on social meaning has metwith less success in use of such features (with theexception of the ?critical segments?
work of (Enoset al, 2007)), presumably because discourse fea-640TOTAL WORDS total number of wordsPAST TENSE uses of past tense auxiliaries was, were, hadMETADATE horn, date, bell, survey, speed, form, questionnaire, rushed, study, researchYOU you, you?d, you?ll, your, you?re, yours, you?ve (not counting you know)WE lets, let?s, our, ours, ourselves, us, we, we?d, we?ll, we?re, we?veI I?d, I?ll, I?m, I?ve, me, mine, my, myself (not counting I mean)ASSENT yeah, okay, cool, yes, awesome, absolutely, agreeSWEAR hell, sucks, damn, crap, shit, screw, heck, fuck*INSIGHT think*/thought, feel*/felt, find/found, understand*, figure*, idea*, imagine, wonderANGER hate/hated, hell, ridiculous*, stupid, kill*, screwed, blame, sucks, mad, bother, shitNEGEMOTION bad, weird, hate, crazy, problem*, difficult, tough, awkward, boring, wrong, sad, worry,SEXUAL love*, passion*, loves, virgin, sex, screwINGEST food, eat*, water, bar/bars, drink*, cook*, dinner, coffee, wine, beer, restaurant, lunch, dishTable 2: Lexical features.
Each feature value is a total count of the words in that class for each conversation side;asterisks indicate that suffixed forms were included (e.g., love, loves, loving).
All except the first three are from LIWC(Pennebaker et al, 2007) (modified slightly, for example by removing you know and I mean).
The last five classesinclude more words in addition to those shown.tures are expensive to hand-label and hard to auto-matically extract.
We chose a suggestive discoursefeatures that we felt might still be automatically ex-tracted.Four particular dialog acts were chosen as shownin Table 3.
Backchannels (or continuers) and ap-preciations (a continuer expressing positive affect)were coded by hand-built regular expressions.
Theregular expressions were based on analysis of thebackchannels and appreciations in the hand-labeledSwitchboard corpus of dialog acts (Jurafsky et al,1997).
Questions were coded simply by the pres-ence of question marks.Finally, repair questions (also called NTRIs; nextturn repair indicators) are turns in which a speakersignals lack of hearing or understanding (Schegloffet al, 1977).
To detect these, we used a simpleheuristic: the presence of ?Excuse me?
or ?Wait?, asin the following example:FEMALE: Okay.
Are you excited about that?MALE: Excuse me?A collaborative completion is a turn where aspeaker completes the utterance begun by the alter(Lerner, 1991; Lerner, 1996).
Our heuristic for iden-tifying collaborative completions was to select sen-tences for which the first word of the speaker wasextremely predictable from the last two words of theprevious speaker.
We trained a word trigram model11interpolated, with Good Turing smoothing, trained on theTreebank 3 Switchboard transcripts after stripping punctuation.and used it to compute the probability p of the firstword of a speaker?s turn given the last two wordsof the interlocutor?s turn.
We arbitrarily chose thethreshold .01, labeling all turns for which p > .01 ascollaborative completions and used the total numberof collaborative completions in a conversation sideas our variable.
This simple heuristic was errorful,but did tend to find completions beginning with andor or (1 below) and wh-questions followed by an NPor PP phrase that is grammatically coherent with theend of the question (2 and 3):(1) FEMALE: The driving range.
(1) MALE: And the tennis court, too.
(2) MALE: What year did you graduate?
(2) FEMALE: From high school?
(3) FEMALE: What department are you in?
(3) MALE: The business school.We also marked aspects of the preference struc-ture of language.
A dispreferred action is one inwhich a speaker avoids the face-threat to the inter-locutor that would be caused by, e.g., refusing arequest or not answering a question, by using spe-cific strategies such as the use of well, hesitations, orrestarts (Schegloff et al, 1977; Pomerantz, 1984).Finally, we included the number of instances oflaughter for the side, as well as the total number ofturns a speaker took.4.4 Disfluency FeaturesA second group of discourse features relating to re-pair, disfluency, and speaker overlap are summarized641BACKCHANNELS number of backchannel utterances in side (Uh-huh., Yeah., Right., Oh, okay.
)APPRECIATIONS number of appreciations in side (Wow, That?s true, Oh, great)QUESTIONS number of questions in sideNTRI repair question (Next Turn Repair Indicator) (Wait, Excuse me)COMPLETION (an approximation to) utterances that were ?collaborative completions?DISPREFERRED (an approximation to) dispreferred responses, beginning with discourse marker wellLAUGH number of instances of laughter in sideTURNS total number of turns in sideTable 3: Dialog act/adjacency pair features.in Table 4.
Filled pauses (um, uh) were coded byUH/UM total number of filled pauses (uh orum) in conversation sideRESTART total number of disfluent restarts inconversation sideOVERLAP number of turns in side which the twospeakers overlappedTable 4: Disfluency featuresregular expressions (the transcribers had been in-structed to transcribe all filled pauses).
Restarts area type of repair in which speakers begin a phrase,break off, and then restart the syntactic phrase.
Thefollowing example shows a restart; the speaker startsa sentence Uh, I and then restarts, There?s a group...:Uh, I?there?s a group of us that came in?Overlaps are cases in which both speakers weretalking at the same time, and were marked by thetranscribers in the transcripts:MALE: But-and also obviously?FEMALE: It sounds bigger.MALE: ?people in the CS school are notquite as social in general as other?5 Classifier TrainingBefore performing the classification task, we prepro-cessed the data in two ways.
First, we standardizedall the variables to have zero mean and unit variance.We did this to avoid imposing a prior on any of thefeatures based on their numerical values.2 Second,2Consider a feature A with mean 100 and a feature B withmean .1 where A and B are correlated with the output.
Sinceregularization favors small weights there is a bias to put weighton feature A because intuitively the weight on feature B wouldwe removed features correlated greater than .7.
Onegoal of removing correlated features was to removeas much colinearity as possible from the regressionso that the regression weights could be ranked fortheir importance in the classification.
In addition,we hoped to improve classification because a largenumber of features require more training examples(Ng, 2004).
For example for male flirt we removedf0 range (highly correlated with f0 max), f0 min sd(highly correlated with f0 min), and Swear (highlycorrelated with Anger).For each classification task due to the smallamounts of data we performed k-fold cross vali-dation to learn and evaluate our models.
We useda variant of k-fold cross validation with five foldswhere three folds are used for training, one fold isused for validation, and one fold is used as a test set.This test fold is not used in any training step.
Thisyields a datasplit of 60% for training, 20% for val-idation, and 20% for testing, or 120 training exam-ples, 40 validation examples, and 40 test examples.To ensure that we were not learning something spe-cific to our data split, we randomized our data order-ing and repeated the k-fold cross validation variant25 times.We used regularized logistic regression for clas-sification.
Recall that in logistic regression we traina vector of feature weights ?
?
Rn so as to makethe following classification of some output variabley for an input observation x:3p(y|x; ?)
= 11 + exp(?
?Tx) (1)In regularized logistic regression we find theneed to be 1000 times larger to carry the same effect.
This ar-gument holds similarly for the reduction to unit variance.3Where n is the number of features plus 1 for the intercept.642weights ?
which maximize the following optimiza-tion problem:argmax?
?ilog p(yi|xi; ?)?
?
?R(?)
(2)R(?)
is a regularization term used to penalizelarge weights.
We chose R(?
), the regularizationfunction, to be the L1 norm of ?.
That is, R(?)
=||?||1 =?ni=1 |?i|.In our case, given the training set Strain, test setStest, and validation set Sval, we trained the weights?
as follows:argmax?accuracy(?
?, Sval) (3)where for a given sparsity parameter ???
= argmax?
?ilog p(yi|xi; ?)?
?
?R(?)
(4)We chose L1-regularization because the number oftraining examples to learn well grows logarithmi-cally with the number of input variables (Ng, 2004),and to achieve a sparse activation of our featuresto find only the most salient explanatory variables.This choice of regularization was made to avoid theproblems that often plague supervised learning insituations with large number of features but only asmall number of examples.
The search space overthe sparsity parameter ?
is bounded around an ex-pected sparsity to prevent overfitting.Finally, to evaluate our model on the learned ?and ??
we used the features X of the test set Stest tocompute the predicted outputs Y using the logisticregression model.
Accuracy is simply computed asthe percent of correct predictions.To avoid any data ordering bias, we calculateda ??
for each randomized run.
The output of theruns was a vector of weights for each feature.
Wekept any feature if the median of its weight vectorwas nonzero.4 A sample boxplot for the highestweighted ego features for predicting male flirt canbe found in Figure 1.4We also performed a t-test to find salient feature valuessignificantly different than zero; the non-zero median methodturned out to be a more conservative measure in practice (intu-itively, because L1 normed regression pushes weights to 0).-1   -0.8  -0.6  -0.4  -0.2    0    0.2    0.4   0.6   0.8    1questionf0 mean stdyourateintensity minbackchannelappreciationrepair questintensity maxlaughIFigure 1: An illustrative boxplot for flirtation in menshowing the 10 most significant features and one notsignificant (?I?).
Shown are median values (central redline), first quartile, third quartile, outliers (red X?s) andinterquartile range (filled box).6 ResultsResults for the 6 binary classifiers are presented inTable 5.Awk Flirt FriendlyM F M F M FSpeaker 63% 51% 67% 60% 72% 68%+other 64% 64% 71% 60% 73% 75%Table 5: Accuracy of binary classification of each con-versation side, where chance is 50%.
The first row usesfeatures only from the single speaker; the second adds allthe features from the interlocutor as well.
These accu-racy results were aggregated from 25 randomized runs of5-fold cross validation.The first row shows results using features ex-tracted from the speaker being labeled.
Here, allconversational styles are easiest to detect in men.The second row of table 5 shows the accuracywhen using features from both speakers.
Not sur-prisingly, adding information about the interlocutortends to improve classification, and especially forwomen, suggesting that male speaking has greatersway over perceptions of conversational style.
Wediscuss below the role of these features.We first considered the features that helped clas-sification when considering only the ego (i.e., the re-sults in the first row of Table 5).
Table 6 shows fea-ture weights for the features (features were normedso weights are comparable), and is summarized inthe following paragraphs:?
Men who are labeled as friendly use you, col-643MALE FRIENDLY MALE FLIRTbackchannel -0.737 question 0.376you 0.631 f0 mean sd 0.288intensity min sd 0.552 you 0.214f0 sd sd -0.446 rate 0.190intensity min -0.445 intensity min -0.163completion 0.337 backchannel -0.142time -0.270 appreciation -0.136Insight -0.249 repair question 0.128f0 min -0.226 intensity max -0.121intensity max -0.221 laugh 0.107overlap 0.213 time -0.092laugh 0.192 overlap -0.090turn dur -0.059 f0 min 0.089Sexual 0.059 Sexual 0.082appreciation -0.054 Negemo 0.075Anger -0.051 metadate -0.041FEMALE FRIENDLY FEMALE FLIRTintensity min sd 0.420 f0 max 0.475intensity max sd -0.367 rate 0.346completion 0.276 intensity min sd 0.269repair question 0.255 f0 mean sd 0.21appreciation 0.253 Swear 0.156f0 max 0.233 question -0.153Swear -0.194 Assent -0.127wordcount 0.165 f0 min -0.111restart 0.172 intensity max 0.092uh 0.241 I 0.073I 0.111 metadate -0.071past -0.060 wordcount 0.065laugh 0.048 laugh 0.054Negemotion -0.021 restart 0.046intensity min -0.02 overlap -0.036Ingest -0.017 f0 sd sd -0.025Assent 0.0087 Ingest -0.024f0 max sd 0.0089MALE AWKrestart 0.502 completion -0.141f0 sd sd 0.371 intensity max -0.135appreciation -0.354 f0 mean sd -0.091turns -0.292 Ingest -0.079uh 0.270 Anger 0.075you -0.210 repair question -0.067overlap -0.190 Insight -0.056past -0.175 rate 0.049intensity min sd -0.173Table 6: Feature weights (median weights of the random-ized runs) for the non-zero predictors for each classifier.Since our accuracy for detecting awkwardness in womenbased solely on ego features is so close to chance, wedidn?t analyze the awkwardness features for women here.laborative completions, laugh, overlap, but don?tbackchannel or use appreciations.
Their utterancesare shorter (in seconds and words) and they are qui-eter and their (minimum) pitch is lower and some-what less variable.?
Women labeled as friendly have more collab-orative completions, repair questions, laughter, andappreciations.
They use more words overall, and useI more often.
They are more disfluent (both restartsand uh) but less likely to swear.
Prosodically their f0is higher, and there seems to be some pattern involv-ing quiet speech; more variation in intensity mini-mum than intensity max.?
Men who are labeled as flirting ask more ques-tions, including repair questions, and use you.
Theydon?t use backchannels or appreciations, or overlapas much.
They laugh more, and use more sexual andnegative emotional words.
Prosodically they speakfaster, with higher and more variable pitch, but qui-eter (lower intensity max).?
The strongest features for women who are la-beled as flirting are prosodic; they speak faster andlouder with higher and more variable pitch.
Theyalso use more words in general, swear more, don?task questions or use Assent, use more I, laugh more,and are somewhat more disfluent (restarts).
?Men who are labeled as awkward are more dis-fluent, with increased restarts and filled pauses (uhand um).
They are also not ?collaborative?
conversa-tionalists; they don?t use appreciations, repair ques-tions, collaborative completions, past-tense, or you,take fewer turns overall, and don?t overlap.
Prosod-ically the awkward labels are hard to characterize;there is both an increase in pitch variation (f0 sd sd)and a decrease (f0 mean sd).
They don?t seem to getquite as loud (intensity max).The previous analysis showed what features of theego help in classification.
We next asked about fea-tures of the alter, based on the results using bothego and alter features in the second row of Table 5.Here we are asking about the linguistic behaviors ofa speaker who describes the interlocutor as flirting,friendly, or awkward.While we don?t show these values in a table, weoffer here an overview of their tendencies.
Forexample for women who labeled their male in-terlocutors as friendly, the women got much qui-eter, used ?well?
much more, laughed, asked more644repair questions, used collaborative completions,and backchanneled more.
When a man labeled awoman as friendly, he used an expanded intensityrange (quieter intensity min, louder intensity max).laughed more, used more sexual terms, used lessnegative emotional terms, and overlapped more.When women labeled their male interlocutor asflirting, the women used many more repair ques-tions, laughed more, and got quieter (lower intensitymin).
By contrast, when a man said his female inter-locutor was flirting, he used more Insight and Angerwords, and raised his pitch.When women labeled their male interlocutor asawkward, the women asked a lot of questions, usedwell, were disfluent (restarts), had a diminishedpitch range, and didn?t use I.
In listening to someof these conversations, it was clear that the conver-sation lagged repeatedly, and the women used ques-tions at these points to restart the conversations.7 DiscussionThe results presented here should be regarded withsome caution.
The sample is not a random sample ofEnglish speakers or American adults, and speed dat-ing is not a natural context for expressing every con-versational style.
Therefore, a wider array of studiesacross populations and genres would be required be-fore a more general theory of conversational styles isestablished.On the other hand, the presented results mayunder-reflect the relations being captured.
The qual-ity of recordings and coarse granularity (1 second)of the time-stamps likely cloud the relations, and asthe data is cleaned and improved, we expect the as-sociations to only grow stronger.Caveats aside, we believe the evidence indicatesthat the perception of several types of conversationalstyle have relatively clear signals across genders, butwith some additional gender contextualization.Both genders convey flirtation by laughing more,speaking faster, and using higher and more variablepitch.
Both genders convey friendliness by laughingmore, and using collaborative completions.However, we do find gender differences; men aslmore questions when (labeled as) flirting, womenask fewer.
Men labeled as flirting are softer, butwomen labeled as flirting are louder.
Women flirt-ing swear more, while men are more likely to usesexual vocabulary.
Gender differences exist as wellfor the other variables.
Men labeled as friendly useyou while women labeled as friendly use I. Friendlywomen are very disfluent; friendly men are not.While the features for friendly and flirtatiousspeech overlap, there are clear differences.
Menspeaker faster and with higher f0 (min) in flirtatiousspeech, but not faster and with lower f0 (min) infriendly speech.
For men, flirtatious speech involvesmore questions and repair questions, while friendlyspeech does not.
For women, friendly speech ismore disfluent than flirtatious speech, and has morecollaborative style (completions, repair questions,appreciations).We also seem to see a model of collaborative con-versational style (probably related to the collabo-rative floor of Edelsky (1981) and Coates (1996)),cued by the use of more collaborative completions,repair questions and other questions, you, and laugh-ter.
These collaborative techniques were used byboth women and men who were labeled as friendly,and occurred less with men labeled as awkward.Women themselves displayed more of this collab-orative conversational style when they labeled themen as friendly.
For women only, collaborative styleincluded appreciations; while for men only, collabo-rative style included overlaps.In addition to these implications for social sci-ence, our work has implications for the extraction ofmeaning in general.
A key focus of our work was onways to extract useful dialog act and disfluency fea-tures (repair questions, backchannels, appreciations,restarts, dispreferreds) with very shallow methods.These features were indeed extractable and provedto be useful features in classification.We are currently extending these results to predictdate outcomes including ?liking?, extending worksuch as Madan and Pentland (2006).AcknowledgmentsThanks to three anonymous reviewers, Sonal Nalkur andTanzeem Choudhury for assistance and advice on datacollection, Sandy Pentland for a helpful discussion aboutfeature extraction, and to Google for gift funding.645ReferencesJ.
Ang, R. Dhillon, A. Krupski, E. Shriberg, and A. Stol-cke.
2002.
Prosody-Based Automatic Detection ofAnnoyance and Frustration in Human-Computer Dia-log.
In INTERSPEECH-02.P.
Boersma and D. Weenink.
2005.
Praat: doing pho-netics by computer (version 4.3.14).
[Computer pro-gram].
Retrieved May 26, 2005, from http://www.praat.org/.S.
Brave, C. Nass, and K. Hutchinson.
2005.
Comput-ers that care: Investigating the effects of orientationof emotion exhibited by an embodied conversationalagent.
International Journal of Human-ComputerStudies, 62(2):161?178.S.
E. Brennan and M. F. Schober.
2001.
How listen-ers compensate for disfluencies in spontaneous speech.Journal of Memory and Language, 44:274?296.S.
E. Brennan and M. Williams.
1995.
The feeling ofanother?s knowing: Prosody and filled pauses as cuesto listeners about the metacognitive states of speakers.Journal of Memory and Language, 34:383?398.J.
Coates.
1996.
Women Talk.
Blackwell.M.
A. Cohn, M. R. Mehl, and J. W. Pennebaker.
2004.Linguistic markers of psychological change surround-ing September 11, 2001.
Psychological Science,15:687?693.C.
Edelsky.
1981. Who?s got the floor?
Language inSociety, 10:383?421.F.
Enos, E. Shriberg, M. Graciarena, J. Hirschberg, andA.
Stolcke.
2007.
Detecting Deception Using CriticalSegments.
In INTERSPEECH-07.D.
Jurafsky, E. Shriberg, and D. Biasca.
1997.
Switch-board SWBD-DAMSL Labeling Project Coder?s Man-ual, Draft 13.
Technical Report 97-02, University ofColorado Institute of Cognitive Science.D.
Jurafsky, E. Shriberg, B.
Fox, and T. Curl.
1998.
Lex-ical, prosodic, and syntactic cues for dialog acts.
InProceedings, COLING-ACL Workshop on DiscourseRelations and Discourse Markers, pages 114?120.D.
Jurafsky.
2001.
Pragmatics and computational lin-guistics.
In L. R. Horn and G. Ward, editors, Hand-book of Pragmatics.
Blackwell.C.
M. Lee and S. S. Narayanan.
2002.
Combining acous-tic and language information for emotion recognition.In ICSLP-02, pages 873?876, Denver, CO.G.
H. Lerner.
1991.
On the syntax of sentences-in-progress.
Language in Society, 20(3):441?458.G.
H. Lerner.
1996.
On the ?semi-permeable?
characterof grammatical units in conversation: Conditional en-try into the turn space of another speaker.
In E. Ochs,E.
A. Schegloff, and S. A. Thompson, editors, Interac-tion and Grammar, pages 238?276.
Cambridge Uni-versity Press.J.
Liscombe, J. Venditti, and J. Hirschberg.
2003.
Clas-sifying Subject Ratings of Emotional Speech UsingAcoustic Features.
In INTERSPEECH-03.A.
Madan and A. Pentland.
2006.
Vibefones: Sociallyaware mobile phones.
In Tenth IEEE InternationalSymposium on Wearable Computers.A.
Madan, R. Caneel, and A. Pentland.
2005.
Voicesof attraction.
Presented at Augmented Cognition, HCI2005, Las Vegas.F.
Mairesse and M. Walker.
2008.
Trainable generationof big-five personality styles through data-driven pa-rameter estimation.
In ACL-08, Columbus.F.
Mairesse, M. Walker, M. Mehl, and R. Moore.
2007.Using linguistic cues for the automatic recognition ofpersonality in conversation and text.
Journal of Artifi-cial Intelligence Research (JAIR), 30:457?500.C.
Nass and S. Brave.
2005.
Wired for speech: Howvoice activates and advances the human-computer re-lationship.
MIT Press, Cambridge, MA.M.
L. Newman, J. W. Pennebaker, D. S. Berry, and J. M.Richards.
2003.
Lying words: Predicting deceptionfrom linguistic style.
Personality and Social Psychol-ogy Bulletin, 29:665?675.A.
Y. Ng.
2004.
Feature selection, L1 vs. L2 regulariza-tion, and rotational invariance.
In ICML 2004.J.
W. Pennebaker and T. C. Lay.
2002.
Language use andpersonality during crises: Analyses of Mayor RudolphGiuliani?s press conferences.
Journal of Research inPersonality, 36:271?282.J.
W. Pennebaker, R. Booth, and M. Francis.
2007.
Lin-guistic inquiry and word count: LIWC2007 operator?smanual.
Technical report, University of Texas.A.
Pentland.
2005.
Socially aware computation andcommunication.
Computer, pages 63?70.A.
M. Pomerantz.
1984.
Agreeing and disagreeing withassessment: Some features of preferred/dispreferredturn shapes.
In J. M. Atkinson and J.
Heritage, edi-tors, Structure of Social Action: Studies in Conversa-tion Analysis.
Cambridge University Press.A.
Rosenberg and J. Hirschberg.
2005.
Acous-tic/prosodic and lexical correlates of charismaticspeech.
In EUROSPEECH-05, pages 513?516, Lis-bon, Portugal.S.
S. Rude, E. M. Gortner, and J. W. Pennebaker.
2004.Language use of depressed and depression-vulnerablecollege students.
Cognition and Emotion, 18:1121?1133.H.
Sacks, E. A. Schegloff, and G. Jefferson.
1974.A simplest systematics for the organization of turn-taking for conversation.
Language, 50(4):696?735.E.
A. Schegloff, G. Jefferson, and H. Sacks.
1977.
Thepreference for self-correction in the organization of re-pair in conversation.
Language, 53:361?382.646
