Proceedings of the EACL 2009 Student Research Workshop, pages 54?60,Athens, Greece, 2 April 2009. c?2009 Association for Computational LinguisticsSpeech emotion recognition with TGI+.2 classifierJulia SidorovaUniversitat Pompeu FabraBarcelona, Spainjulia.sidorova@upf.eduAbstractWe have adapted a classification approachcoming from optical character recognitionresearch to the task of speech emotionrecognition.
The classification approachenjoys the representational power of a syn-tactic method and efficiency of statisti-cal classification.
The syntactic part im-plements a tree grammar inference algo-rithm.
We have extended this part of thealgorithm with various edit costs to pe-nalise more important features with higheredit costs for being outside the interval,which tree automata learned at the infer-ence stage.
The statistical part implementsan entropy based decision tree (C4.5).
Wedid the testing on the Berlin database ofemotional speech.
Our classifier outper-forms the state of the art classifier (Multi-layer Perceptron) by 4.68% and a baseline(C4.5) by 26.58%, which proves validityof the approach.1 IntroductionIn a number of applications such as human-computer interfaces, smart call centres, etc.
it isimportant to be able to recognise people?s emo-tional state.
An aim of a speech emotion recogni-tion (SER) engine is to produce an estimate of theemotional state of the speaker given a speech frag-ment as an input.
The standard way to do SER isthrough a supervised machine learning procedure(Sidorova et al, 2008).
It also should be notedthat a number of alternative classification strate-gies has been offered recently, such as unsuper-vised learning (Liu et al, 2007) and numeric re-gression (Grimm et al, 2007) etc, and which arepreferable under certain conditions.Our contribution is a new algorithm of amixed design with syntactic and statistical learn-ing, which we borrowed from optical characterrecognition (Sempere, Lopez, 2003), extended,and adapted for SER.
The syntactic part imple-ments tree grammar inference (Sakakibara, 1997),and the statistical part implements C4.5 (Quinlan,1993).
The intuitive reasons underlying this solu-tion are as follows.
We would like to have a clas-sification approach that enjoys the representationalpower of a syntactic method and efficiency of sta-tistical classification.
First we model the objectsby means of a syntactic method, i.e.
we map sam-ples into their representations.
A representation ofa sample is a set of seven numeric values, signi-fying to which degree a given sample resemblesthe averaged pattern of each of seven classes.
Sec-ond, we learn to classify the mappings of samples,rather than feature vectors of samples, with a pow-erful statistical method.
We called the classifierTGI+, which stands for Tree Grammar Inferenceand the plus is for the statistical learning enhance-ment.
In this paper we present the second versionof TGI+, which extends TGI+.1 (Sidorova et al,2008) and the difference is that we have added var-ious edit costs to penalise more important featureswith higher edit costs for being outside the inter-val, which tree automata learned at the inferencestage.
We evaluated TGI+ against a state of the artclassifier.
To obtain a state of the art performance,we constructed a speech emotion recogniser, fol-lowing the classical supervised learning approachwith a top performer out of more than 20 classi-fiers from the weka package, which turned out tobe multilayer perceptron (MLP) (Witten, Frank,2005).
Experimental results showed that TGI+outperforms MLP by 4.68%.The structure of this paper is as follows: in thissection below we explain construction of a clas-sical speech emotion recognizer, in Section 2 weexplain TGI+; Section 3 reports testing results forboth, the state of the art recogniser and TGI+.
Sec-tion 4 and 5 is discussion and conclusions.541.1 Classical Speech Emotion RecogniserA classical speech emotion recognizer is com-prised of three modules: Feature Extraction, Fea-ture Selection, and Classification.
Their perfor-mance will serve as a baseline for TGI+ recog-nizer.1.1.1 Feature Extraction and SelectionIn the literature there is a consensus that globalstatistics features lead to higher accuracies com-pared to the dynamic classification of multivariatetime-series (Schuller et al, 2003).
The feature ex-traction module extracts 116 global statistical fea-tures, both prosodic and segmental, a full list andexplanations for which can be found in (Sidorova,2007).The feature selection module implements awrapper approach with forward selection (Witten,Frank, 2005) in order to automatically select themost relevant features extracted by the previousmodule.1.1.2 ClassificationThe classification module takes an input as a fea-ture vector created by the feature selector, and ap-plies the Multilayer Perceptron classifier (MLP)(Witten, Frank, 2005), in order to assign a classlabel to it.
The labels are the emotional states todiscriminate among.
For our data, MLP turnedout to be the top performer among more than 20other different classifiers; details of this compara-tive testing can be found in (Sidorova, 2007).2 TGI+ classifierThe organisation of this section is as follows.
Inparagraph 2.1 we explain the TGI+.1 classifier andshow how its parts work together.
TGI+.2 is anextension of TGI+.1 and we explain it right af-terwards.
In paragraph 2.2 we briefly remind theC4.5 algorithm.
Further in the paper in paragraph4.1 we show that our TGI+ algorithm was cor-rectly constructed and that we arrived to a mean-ingful combination of methods from different pat-tern recognition paradigms.2.1 TGI+TGI+.1 is comprised of four major steps we ex-plain below.
Fig 1 graphically depicts the proce-dure.Step 1: In order to perform tree grammarinference we represent samples by tree structures.Divide the training set into two subsets T1 (39%of training data) and T2 (the rest of trainingdata).
Utterances from T1 are converted into treestructures, the skeleton of which is defined by thegrammar below.
S denotes a start symbol of theformal grammar (in the sense of a term-rewritingsystem):{S??
ProsodicFeatures SegmentalFeatures;ProsodicFeatures ??
Pitch Intensity JitterShimer;SegmentalFeatures ??
Energy Formants;Pitch ??
Min Max Quantile Mean Std MeanAb-soluteSlope;etc.
}The etc.
stands for further terminating produc-tions, i.e.
the productions which have low levelfeatures on their right hand side.
All trees have116 leaves, each corresponding to one of the 116features from the sample feature vector.
We puttrees from one class into one set.
In our datasetwe have the following seven classes to recogniseamong: fear, disgust, happiness, boredom, neutral,sadness and anger.
Therefore, we have seven setsof trees.
We put trees from one class into one set.Step 2: Apply tree grammar inference to learnseven automata accepting a different type of emo-tional utterance each.
Grammar inference is amethod to learn a grammar from examples.
In ourcase, it is tree grammar inference, because we dealwith trees representing utterances.
The result ofthis step is seven automata, one for each of sevenemotions to be recognised.Step 3: Calculate edit distances between ob-tained tree automata and trees in the training set.Edit distances are then calculated between eachautomaton obtained at step two and each tree rep-resenting utterances from the training set (T1?T2).The calculated edit distances are put into a matrixof size: (cardinality of the training set) ?
7 (thenumber of classes).Step 4: Run C4.5 over the matrix to obtain adecision tree.
The C4.5 algorithm is run over thismatrix in order to obtain a decision tree, classify-ing each utterance into one of the seven emotions,according to edit distances between a given utter-ance and the seven tree automata.
The accuraciesobtained from testing this decision tree are the ac-curacies of TGI+.1.TGI+.2 Our extension of the algorithm as pro-posed in (Sempere, Lopez, 2003) has to do withStep 3.
In TGI+.1 all edit costs equated to 1.
In55Figure 1: TGI+ steps.
Step 1: In order to perform tree grammar inference, represent samples by treestructures.
Step 2: Apply tree grammar inference to learn seven automata accepting a different type ofemotional utterance each.
Step 3: Calculate edit distances between obtained tree automata and trees inthe training set.
While calculating edit distances, penalise more important features with higher costs forbeing outside its interval.
The set of such features is determined exclusively for every class through afeature selection procedure.
Step 4: Run C4.5 over the matrix to obtain a decision tree.56other words, if a feature value fits the interval atree automaton has learned for it, the acceptancecost of the sample is not altered.
If a feature valueis outside the interval the automaton has learnt forit, the acceptance cost of the sample processed isincremented by one.
In TGI+.2 some edit costshave a coefficient greater than 1 (1.5 in the cur-rent version).
In other words, more important fea-tures are penalised with higher costs for being out-side its interval.
The set of these more importantfeatures is determined exclusively for every class(anger, neutral, etc.)
through a feature selectionprocedure.
The feature selection procedure imple-ments a wrapper approach with forward selection.Concluding the algorithm description, let us ex-plain how TGI+ classifiers an input sample, whichis fed to the automata in the form a 116 dimen-sional feature vector.
Firstly TGI+ calculates dis-tances from the sample to seven tree automata (theautomata learnt 116 feature intervals at the infer-ence step).
Secondly TGI+ uses the C 4.5 deci-sion tree to classify the sample (the decision treewas learnt from the table, where distances to sevenautomata to all the training samples had been put).2.2 C4.5 Learning algorithmC4.5 belongs to the family of algorithms that em-ploy a topdown greedy search through the spaceof possible decision trees.
A decision tree is a rep-resentation of a finite set of if-then-else rules.
Themain characteristics of decision trees are the fol-lowing:1.
The examples can be defined as a set of nu-merical and symbolic attributes.2.
The examples can be incomplete or containnoisy data.3.
The main learning algorithms work underMinimum Description Length approaches.The main learning algorithms for decision treeswere proposed by Quinlan (Quinlan, 1993).
First,he defined ID3 algorithm based on the informationgain principle.
This criterion is performed by cal-culating the entropy that produces every attributeof the examples and by selecting the attributes thatsave more decisions in information terms.
C4.5algorithm is an evolution of ID3 algorithm.
Themain characteristics of C4.5 are the following:1.
The algorithm can work with continuous at-tributes.2.
Information gain is not the only learning cri-terion.3.
The trees can be post-pruned in order to re-fine the desired output.3 Experimental workWe did the testing on acted emotional speech fromthe Berlin database (Burkhardt el al., 2005).
Al-though acted material has a number of well knowndrawbacks, it was used to establish a proof of con-cept for the methodology proposed and is a bench-mark database for SER.
In the future work we planto do the testing on real emotions.
The BerlinEmotional Database (EMO-DB) contains the setof emotions from the MPEG-4 standard (anger,joy, disgust, fear, sadness, surprise and neutral).Ten German sentences of emotionally undefinedcontent have been acted in these emotions by tenprofessional actors, five of them female.
Through-out perception tests by twenty human listeners 488phrases have been chosen that were classified asmore than 60% natural and more than 80% clearlyassignable.
The database is recorded in 16 bit, 16kHz under studio noise conditions.As for the testing protocol, 10-fold cross-validation was used.
Recall, precision and F mea-sure per class are given in Tables 3, 4.1 and 4.2 forC4.5, MLP and TGI+, respectively.
The overallaccuracy of MLP, the state of the art recogniser, is73.9% and the overall accuracy of the TGI+ basedrecogniser is 78.58%, which is a 4.68% ?
3.45%in favour of TGI+.
The confidence interval wascalculated as follows: Z?p(1?
p)n , where p isaccuracy, n is cardinality of the data set, and Zis a constant for the confidence level of 95%, i.e.Z = 1.96.
The proposed TGI+ has also been eval-uated against C4.5 to find out which is the con-tribution of moving from the feature vector repre-sentation of samples to the distance to automataone.
C4.5 performs with 52.9% of acuracy, whichis 25.68% less than TGI+.
The positive outcomeof such contrastive testing in favour of TGI+ wasexpected, because TGI+ was designed to enjoystrengths of two paradigms: syntactic and statis-tical, while MLP (or C4.5) is a powerful singleparadigm statistical method.57class precision recall F measurefear 0.49 0.44 0.46disgust 0.26 0.24 0.26happiness 0.35 0.36 0.35boredom 0.49 0.55 0.52neutral 0.51 0.46 0.49sadness 0.71 0.82 0.76anger 0.69 0.7 0.7Table 1: Baseline recognition with C4.5 on theBerlin emotional database.
The overall accuracy is52.9%, which is 25.68% less accurate than TGI+.class precision recall F measurefear 0.82 0.74 0.77disgust 0.72 0.74 0.73happiness 0.52 0.49 0.51boredom 0.73 0.75 0.74neutral 0.71 0.78 0.75sadness 0.88 0.94 0.91anger 0.75 0.76 0.75Table 2: State of the art recognition with MLP onthe Berlin emotional database.
The overall accu-racy is 73.9%, which is 4.68% less accurate thanTGI+.4 Discussion4.1 Correctness of algorithm constructionWhile constructing TGI+, it is of critical impor-tance that the following condition holds: The ac-curacy of TGI+ is better than that of tree accep-tors and C4.5.
If this condition holds, then TGI+is well constructed.
We tested TGI+, tree automataas acceptors and C4.5 on the same Berlin databaseunder the same experimental settings.
The treeautomata and C4.5 perform with 43% and 52.9%of accuracy respectively, which is 35.58% and25.68% worse than the accuracy of TGI+.
There-fore the condition is met and we can state that wearrived to a meaningful combination of methodsfrom different pattern recognition paradigms.4.2 A combination of statistical and syntacticrecognitionSyntactic recognition is a form of pattern recogni-tion, where items are presented as pattern struc-tures, which take account of more complex in-terrelationships between features than simple nu-meric feature vectors used in statistical classifica-tion.
One way to represent such structure is stringsclass precision recall F measurefear 0.66 0.66 0.66disgust 0.6 0.6 0.6happiness 0.86 0.73 0.81boredom 0.81 0.72 0.77neutral 0.64 0.79 0.71sadness 0.83 0.83 0.83anger 0.89 0.93 0.91Table 3: Performance of the TGI+ based emotionrecognizer on the Berlin emotional database.
Theoverall accuracy is 78.58%.
(or trees) of a formal language.
In this case differ-ences in the structures of the classes are encodedas different grammars.
In our case, we have nu-meric data in place of a finite alphabet, which ismore traditional for syntactic learning.
The syn-tactic method does the mapping of objects intotheir models, which can be classified more accu-rately than objects themselves.4.3 Why tree structures?Looking at the algorithm, it might seem redundantto have tree acceptors, when the same would bepossible to handle with a finite state automaton(that accepts the class of regular string languages).Yet tree structures will serve well to add differentweights to tree branches.
The motivation behindis that acoustically some emotions are transmittedwith segmental features and others with prosodic,e.g.
prosody can be prioritised over segmental fea-tures or vice versa (see also Section 4.5).4.4 Selection of C4.5 as a base classifier inTGI+A natural question is: given that MLP outperformsC4.5, which are the reasons for having C4.5 asa base classifier in TGI+ and not the top statisti-cal classifier?
We followed the idea of (Sempere,Lopez, 2003), where C4.5 was the base classifier.We also considered the possibility of having MLPin place of C4.5.
The accuracies dramatically wentdown and we abandoned this alternative.4.5 Future workI.
Tuning parameters.
There are two tuning pa-rameters.
To exclude the possibility of overfitting,the testing settings should be changed to the pro-tocol with disjoint training, validation and testingsets.
We have not done the experiments under the58new training/testing settings, yet we can use theold 10-f cross validation mode to see the trends.Tuning parameter 1 is the point of division of thetraining set into the two subsets T1 and T2, i.e.
adivision of the training data to train the statisticaland syntactic classifier.
The division point shouldbe shifted from 5% for syntactic and 100% for sta-tistical to 100% to train both syntactic and statis-tical models.
The point of division of the trainingdata is an accuracy sensitive parameter.
Our roughanalysis showed that the resulting function (pointof division for abscissa and accuracy for ordinate)has a hill shape with one absolute maximum, andwe made a division roughly at this point: 39% ofthe training data for the syntactic model.
Findingthe best division in fair experimental settings re-mains for future work.Tuning parameter 2 is a set of edit costs as-signed to different branches of the tree acceptors.A linguistic approach is an alternative to the fea-ture selection we followed so far.
This is the pointat which finite state automata cease to be an alter-native modelling device.
The motivation behindis that acoustically some emotions are transmittedwith segmental features and others with prosodic(Barra, et al, 1993).
A coefficient of 1.5 on theprosodic branches brought 2% of improvement ofrecognition for boredom, neutral and sadness.II.
Testing TGI+ on authentic emotions.
Ithas been shown that authentic corpora have verydifferent distributions compared to acted speechemotions (Vogt, Andre, 2005).
We must checkwhether TGI+ is also a top performer, when con-fronted with authentic corpora.III.
Complexity and computational time.
Anumber of classifiers, like MLP (but not C4.5) re-quire a prior feature selection step, while TGI+always uses a complete set of features, thereforebetter accuracies come at the cost of higher com-putational complexity.
We must analyse such ad-vantages and disadvantages of TGI+ compared toother popular classifiers.5 ConclusionsWe have adapted a classification approach com-ing from optical character recognition research tothe task of speech emotion recognition.
The gen-eral idea was that we would like a classificationapproach to enjoy the representational power ofa syntactic method and the efficiency of statisti-cal classification.
The syntactic part implementsa tree grammar inference algorithm.
The statisti-cal part implements an entropy based decision tree(C4.5).
We did the testing on the Berlin databaseof emotional speech.
Our classifier outperformsstate of the art classifier (Multilayer Perceptron)by 4.68% and a baseline (C4.5) by 26.58%, whichproves validity of the approach.6 AcknowledgementsThis research was supported by AGAUR, the Re-search Agency of Catalonia, under the BE-DRG2007 mobility grant.
We would like to thank Labof Spoken Language Systems, Saarland Univer-sity, where much of this work was completed.ReferencesBarra R., Montero J.M., Macias-Guarasa, DHaro, L.F.,San-Segundo R., Cordoba R. 2005.
Prosodic andsegmental rubrics in emotion identification.
Proc.ICASSP 2005, Philadelphia, PA, March 2005.Burkhardt, F., Paeschke, A., Rolfes, M., Sendlmeier,W., Weiss, B.
2002.
A database of German Emo-tional Speech.
Proc.
Interspeech 2005, ISCA, pp1517-1520, Lisbon, Portugal, 2005.Grimm M., Kroschel K., Narayanan S. 2007.
Sup-port vector regression for automatic recognition ofspontaneous emotions in speech, Proc.
of ICASSP,Honolulu, Hawaii, April 2007.Liu, J., Chen, C., Bu, J., You, M, Tao, J.
2007.
Speechemotion recognition using an enhanced co-trainingalgorithm, in Proc.
of ICME, Bejing, China, July,2007.Lopez D., Espana, S. 2002.
Error-correcting tree-language inference.
Pattern Recognition Letters 23,pp.
1-12.
2002Sakakibara, Y.
1997.
Recent advances of grammaticalinference.
Theoretical Computer Science 185, pp.15-45.
Elsevier.
1997.Schuller B., Rigoll G. Lang M. 2003.
Hidden MarkovModel-Based Speech Emotion Recognition, Proc.
ofICASSP 2003, Vol.
II, pp.
1-4, Hong Kong, China,2003.Sempere J. M., Lopez D. 2003.
Learning deci-sion trees and tree automata for a syntactic patternrecognition task.
Pattern Recognition and ImageAnalysis.
Lecture notes in CS.
Berlin.
Volume 2652.pp.
943-950, 2003.Sidorova J.
2007.
DEA report: Speech Emo-tion Recognition.
Appendix 1 (for the fea-ture list) and Section 3.3.
(for a compar-ative testing of various weka classifiers) .http://www.glicom.upf.edu/tesis/sidorova.pdfUniversitat Pompeu Fabra59Sidorova J., McDonough J., Badia T. 2008.
AutomaticRecognition of Emotive Voice and Speech, in (Eds.)K.
Izdebski.
Emotions in The Human Voice, Vol.
3,Chap.
12, Plural Publishing, San Diego, CA, 2008.Quinlan, J.R. 1993.
C4.5: Programs For MachineLearning.
Morgan Kaufmann, Los Altos.
1993.Vogt, T. Andre, E. 2005.
Comparing feature sets foracted and spontaneous speech in view of automaticemotion recognition.
Proc.
ICME 2005, Amster-dam, Netherlands, 2005.Witten I.H., Frank E. 2005.
Sec.
7.1 (for feature se-lection) and Sec.
10.4 (for multilayer perceptron) inData Mining.
Practical Machine Learning Tools andTechniques.
Elsevier.
2005.60
