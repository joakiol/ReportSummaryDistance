Two-Phase Biomedical NE Recognition based on SVMsKi-Joong Lee Young-Sook Hwang and Hae-Chang RimDepartment of Computer Science & EngineeringKorea University1, 5-ka, Anam-dong, SEOUL, 136-701, KOREA{kjlee, yshwang, rim}@nlp.korea.ac.krAbstractUsing SVMs for named entity recogni-tion, we are often confronted with themulti-class problem.
Larger as the num-ber of classes is, more severe the multi-class problem is.
Especially, one-vs-restmethod is apt to drop the performance bygenerating severe unbalanced class distri-bution.
In this study, to tackle the prob-lem, we take a two-phase named entityrecognition method based on SVMs anddictionary; at the first phase, we try toidentify each entity by a SVM classifierand post-process the identified entities bya simple dictionary look-up; at the sec-ond phase, we try to classify the seman-tic class of the identified entity by SVMs.By dividing the task into two subtasks, i.e.the entity identification and the semanticclassification, the unbalanced class distri-bution problem can be alleviated.
Further-more, we can select the features relevantto each task and take an alternative classi-fication method according to the task.
Theexperimental results on the GENIA cor-pus show that the proposed method is ef-fective not only in the reduction of train-ing cost but also in performance improve-ment: the identification performance isabout 79.9(F?
= 1), the semantic clas-sification accuracy is about 66.5(F?
= 1).1 IntroductionKnowledge discovery in the rapidly growing area ofbiomedicine is very important.
While most knowl-edge are provided in a vast amount of texts, it is im-possible to grasp all of the huge amount of knowl-edge provided in the form of natural language.
Re-cently, computational text analysis techniques basedon NLP have received a spotlight in bioinformat-ics.
Recognizing the named entities such as proteins,DNAs, RNAs, cells etc.
has become one of the mostfundamental tasks in the biomedical knowledge dis-covery.Conceptually, named entity recognition consistsof two tasks: identification, which finds the bound-aries of a named entity in a text, and classifi-cation, which determines the semantic class ofthat named entity.
Many machine learning ap-proaches have been applied to biomedical namedentity recognition(Nobata, 1999)(Hatzivalssiloglou,2001)(Kazama, 2002).
However, no work hasachieved sufficient recognition accuracy.
One rea-son is the lack of annotated corpora.
This is some-what appeased with announcement of the GENIAcorpus v3.0(GENIA, 2003).
Another reason is thatit is difficult to recognize biomedical named entitiesby using general features compared with the namedentities in newswire articles.
In addition, since non-entity words are much more than entity words inbiomedical documents, class distribution in the classrepresentation combining a B/I/O tag with a seman-tic class C is so severely unbalanced that it costs toomuch time and huge resources, especially in SVMstraining(Hsu, 2001).Therefore, Kazama and his colleagues tackled theproblems by tuning SVMs(Kazama, 2002).
Theysplitted the class with unbalanced class distributioninto several subclasses to reduce the training cost.In order to solve the data sparseness problem, theyexplored various features such as word cache fea-tures and HMM state features.
According to their re-port, the word cache and HMM state features madea positive effect on the performance improvement.But, not separating the identification task from thesemantic classification, they tried to classify thenamed entities in the integrated process.By the way, the features for identifying thebiomedical entity are different from those for se-mantically classifying the entity.
For example, whileorthographical characteristics and a part-of-speechtag sequence of an entity are strongly related to theidentification, those are weakly related to the seman-tic classification.
On the other hand, context wordsseem to provide useful clues to the semantic classifi-cation of a given entity.
Therefore, we will separatethe identification task from the semantic classifica-tion task.
We try to select different features accord-ing to the task.
This approach enables us to solve theunbalanced class distribution problem which oftenoccurs in a single complicated approach.
Besides, toimprove the performance, we will post-process theresults of SVM classifiers by utilizing the dictionary.That is, we adopt a simple dictionary lookup methodto correct the errors by SVMs in the identificationphase.Through some experiments, we will show howseparating the entity recognition task into two sub-tasks contributes to improving the performance ofbiomedical named entity recognition.
And we willshow the effect the hybrid approach of the SVMsand the dictionary-lookup.2 Definition of Named EntityClassification ProblemWe divide the named entity recognition into twosubtasks, the identification task which finds the re-gions of the named entities in a text and the semanticclassification which determines the semantic classesof them.
Figure 1 illustrates the proposed method,which is called two-phase named entity recognitionmethod.Figure 1: Examples of Biomedical Named EntityRecognitionThe identification task is formulated as classifica-tion of each word into one of two classes, T or Othat represent region information.
The region infor-mation is encoded by using simple T/O representa-tion: T means that current word is a part of a namedentity, and O means that the word is not in a namedentity.
With the representation, we need only onebinary SVM classifier of two classes, T, O.The semantic classification task is to assign oneof semantic classes to the identified entity.
At thesemantic classification phase, we need to classifyonly the identified entities into one of the N seman-tic classes because the entities were already identi-fied.
Non-entity words are ignored at this phase.
Theclasses needed to be classified are just only the N se-mantic classes.
Note that the number of total classes,N + 1 is remarkably small compared with the num-ber, 2N + 1 required in the complicated recognitionapproaches in which a class is represented by com-bining a region information B/I/O with a semanticclass C. It can considerably reduce workload in thenamed entity recognition.Especially when using SVMs, the number ofclasses is very critical to the training in the as-pect of training time and required resources.
LetL be the number of training samples and let N bethe number of classes.
Then one-vs-rest methodtakes N ?
O(L) in the training step.
The com-plicated approach with the B/I/O notation requires(2N + 1)?O(Lwords) (L is number of total wordsin a training corpus).
In contrast, the proposed ap-proach requires (N ?
O(Lentities)) + O(Lwords).Here, O(Lwords) stands for the number of words ina training corpus and O(Lentities) for the number ofentities.
It is a considerable reduction in the trainingcost.
Ultimately, it affects the performance of theentity recognizer.To achieve a high performance of the definedtasks, we use SVM(Joachims, 2002) as a machinelearning approach which has showed the best perfor-mance in various NLP tasks.
And we post-processthe classification results of SVMs by utilizing a dic-tionary.
Figure 2 outlines the proposed two-phasenamed entity recognition system.
At each phase,each classifier with SVMs outputs the class of thebest score.
For classifying multi-classes based on abinary classifier SVM, we use the one-vs-rest clas-sification method and the linear kernel in both tasks.Furthermore, for correcting the errors by SVMs,the entity-word dictionary constructed from a train-ing corpus is utilized in the identification phase.
Thedictionary is searched to check whether the bound-ary words of an identified entity were excluded ornot because the boundary words of an entity mightbe excluded during the entity identification.
If aboundary word was excluded, then we concatenatethe left or the right side word adjacent to the iden-tified entity.
This post-processing may enhance thecapability of the entity identifier.3 Biomedical Named Entity IdentificationThe named entity identification is defined as theclassification of each word to one of the classes thatrepresent the region information.
The region infor-mation is encoded by using simple T/O representa-tion: T means that the current word is a part of anamed entity, and O means that the current word isnot in a named entity.The above representation yields two classes of thetask and we build just one binary SVM classifiers forthem.
By accepting the results of the SVM classifier,we determine the boundaries of an entity.
To correctboundary errors, we post-process the identified enti-ties with the entity-word dictionary.3.1 Features for Entity IdentificationAn input x to a SVM classifier is a feature represen-tation of a target word to be classified and its context.We use a bit-vector representation.
The features ofthe designated word are composed of orthographi-cal characteristic features, prefix, suffix, and lexicalof the word.Table 1 shows all of the 24 orthographical fea-tures.
Each feature may be a discriminative fea-ture appeared in biomedical named entites such asprotein, DNA and RNA etc.
Actually, the name ofprotein, DNA or RNA is composed by combiningalpha-numeric string with several characters such asGreek or special symbols and so on.Table 1: Orthographical characteristic features ofthe designated wordOrthographic Feature examplesDIGITS 1 , 39SINGLE CAP A , MCOMMA ,PERIOD .HYPHON -SLASH /QUESTION MARK ?OPEN SQUARE [CLOSE SQUARE ]OPEN PAREN (CLOSE PAREN )COLON :SEMICOLON ;PERCENT %APOSTROPHE ?ETC SYMBOL +, *, etc.TWO CAPS alphaCD28ALL UPPER AIDSINCLUDE CAPS c-JunGREEK LETTER NF-kappaALPHA NUMERIC p65ALL LOWER motifCAPS DIGIT CD40INIT CAP RelAnd the suffix/prefix, the designated word and thecontext word features are as follows:wi =????
?1 if the word is the ith wordin the vocabulary V0 otherwiseFigure 2: System Configuration of Two Phase Biomedical NE Recognition Systemposi =????
?1 if the word is assigned the ithPOS tag in the POS tag list0 otherwisesufi =????
?1 if the word contains theith suffix in the suffix list0 otherwiseprei =????
?1 if the word contains theith prefix in the prefix list0 otherwisewki =????
?1 if a word at k is the ith wordin the vocabulary V0 otherwiseposki =????
?1 if a word at k is assigned theith POS tag in the POS tag list0 otherwiseIn the definition, k is the relative word positionfrom the target word.
A negative value representsa preceeding word and a positive value representsa following word.
Among them, the part-of-speechtag sequence of the word and the context words is akind of a syntactic rule to compose an entity.
Andlexical information is a sort of filter to identify anentity which is as possible as semantically cohesive.3.2 Post-Processing by Dictionary Look-UpAfter classifying the given instances, we do post-processing of the identified entities.
During the post-processing, we scan the identified entities and exam-ine the adjacent words to those.
If the part-of-speechof an adjacent word belongs to one of the group, ad-jective, noun, or cardinal, then we look up the dic-tionary to check whether the word is in it or not.
If itexists in the dictionary, we include the word into theentity region.
The dictionary is constructed of wordsconsisting of the named entities in a training corporaand stopwords are ignored.Figure 3 illustrates the post-processing algorithm.In Figure 3, the word cell adjacent to the left of theidentified entity cycle-dependent transcription, hasthe part-of-speech NN and exists in the dictionary.The word factor adjacent to the right of the entityhas the part-of-speech NN.
It exists in the dictionary,too.
Therefore, we include the words cell and factorinto the entity region and change the position tags ofthe words in the entity.By taking the post-processing method, we cancorrect the errors by a SVM classifier.
It also givesus a great effect of overcoming the low coverageproblem of the small-sized entity dictionary.4 Semantic Classification of BiomedicalNamed EntityThe objects of the semantic tagging are the entitiesidentified in the identification phase.
Each entity isassigned to a proper semantic class by voting theSVM classifiers.Figure 3: An example of the post-processing of an entity identification4.1 Features for Semantic ClassificationFor semantically tagging an entity, an input x to aSVM classifier is represented by a feature vector.The vector is composed of following features:fwi =????
?1 if a given entity contains oneof the functional words0 otherwiseinwi =????
?1 if one of the words in theentity is in the inside word list0 otherwiselcwi =????????
?1 if noun or verb word in theleft context is the ith wordin the left context word list0 otherwisercwi =????????
?1 if noun or verb word in theright context is the ith wordin the right context word list0 otherwiseOf the above features, fwi checks whether theentity contains one of functional words.
The func-tional words are similar to the feature terms used by(Fukuda, 1998).
For example, the functional wordssuch as factor, receptor and protein are very help-ful to classifying named entities into protein and thefunctional words such as gene, promoter and motifare very useful for classifying DNA.In case of the context features of a given entity, wedivide them into two kinds of context features, insidecontext features and outside context features.
As in-side context features, we take at most three wordsfrom the backend of the entity 1.
We make a list ofthe inside context words by collecting words in the1The average length of entities is about 2.2 in GENIA cor-pus.range of the inside context.
If one of the three wordsis the ith word in the inside context word list, we setthe inwi bit to 1.
The outside context features aregrouped in the left ones and the right ones.
For theleft and the right context features, we restrict themto noun or verb words in a sentence, whose positionis not specified.
This grouping make an effect of al-leviating the data sparseness problem when using aword as a feature.For example, given a sentence with the entity,RNA polymerase II as follows:General transcription factor are requiredfor accurate initiation of transcription byRNA polymerase II PROTEIN .The nouns transcription, factor, initiation and theverbs are, required are selected as left context fea-tures, and the words RNA, polymerase, II are se-lected as inside context features.
The bit field cor-responding to each of the selected word is set to 1.In this case, there is no right context features.
Andsince the entity contains the functional word RNA,the bit field of RNA is set to 1.For classifying a given entity, we build SVM clas-sifiers as many as the number of semantic classes.We take linear kernel and one-vs-rest classificationmethod.5 Experiments5.1 Experimental EnvironmentsExperiments have been conducted on the GENIAcorpus(v3.0p)(GENIA, 2003), which consists of2000 MEDLINE abstracts annotated with PennTreebank (PTB) POS tags.
There exist 36 distinctsemantic classes in the corpus.
However, we used22 semantic classes which are all but protein, DNAand RNA?s subclasses on the GENIA ontology 2.The corpus was transformed into a B/I/O annotatedcorpus to represent entity boundaries and a semanticclass.We divided 2000 abstracts into 10 collections for10-fold cross validation.
Each collection containsnot only abstracts but also paper titles.
The vo-cabularies for lexical features and prefix/suffix listswere constructed by taking the most frequent 10,000words from the training part only.Also, we made another experimental environ-ment to compare with the previous work by(Kazama, 2002).
From the GENIA corpus, 590abstracts(4,808 sentences; 20,203 entities; 128,463words) were taken as a training part and 80 ab-stracts(761 sentences; 3,327 entities; 19,622 words)were selected as a test part.
Because we couldn?tmake the experimental environment such as thesame as that of Kazama?s, we tried to make a com-parable environment.We implemented our method using the SVM-lightpackage(Joachims, 2002).
Though various learningparameters can significantly affect the performanceof the resulting classifiers, we used the SVM systemwith linear kernel and default options.The performance was evaluated by precision, re-call and F?=1.
The overall F?=1 for two models andten collections, were calculated using 10-fold crossvalidation on total test collection.5.2 Effect of Training Data SizeIn this experiment, varying the size of training set,we observed the change of F?=1 in the entity identi-fication and the semantic classification.
We fixed thetest data with 200 abstracts(1,921 sentences; 50,568words).
Figure 4 shows that the performance wasimproved by increasing the training set size.
As theperformance of the identification increases, the gapbetween the performance of the identification andthat of the semantic classification is gradually de-creased.5.3 Computational EfficiencyWhen using one-vs-rest method, the number ofnegative samples is very critical to the training in2That is, All of the protein?s subclass such as pro-tein molecule, protein family or group were regarded as pro-tein.Figure 4: Perfomance shift according to the increaseof training data size w/o post-processingthe aspect of training time and required resources.The SVM classifier for entity identifiation deter-mines whether each word is included in an entityor not.
Figure 5 shows there are much more nega-tive samples than positive samples in the identifica-tion phase.
Once entities are identified, non-entitywords are not considered in next semantic classifi-cation phase.
Therefore, the proposed method caneffectively remove the unnecessary samples.
It en-ables us effectively save the training costs.Furthermore, the proposed method could effec-tively decrease the degree of the unbalance amongclasses by simplifying the classes.
Figure 6 showshow much the proposed method can alleviate the un-balanced class distribution problem compared with1-phase complicated classification model.
However,even though the unbalanced class distribution prob-lem could be alleviated in the identification phase,we are still suffering from the problem in the seman-tic classification as long as we take the one-vs-restmethod.
It indicates that we need to take anotherclassification method such as a pairwise method inthe semantic classification(Krebel, 1999).5.4 Discriminative Feature SelectionWe subsequently examined several alternatives forthe feature sets described in section 3.1 and section4.1.The column (A) in Table 2 shows the identifica-tion cases.
The base feature set consisted of only thedesignated word and the context words in the rangefrom the left 2 to the right 2.
Several alternatives forfeature sets were constructed by adding a differentcombination of features to the base feature set.
FromFigure 5: training size vs. positive and negative sam-ple size in identification phase and semantic classi-fication phaseFigure 6: 2-phase model vs. 1-phase model : changeof the negative and the positive sample size accord-ing to the training data size( A ) ( B )FeatSet F-score FeatSet F-scorebase 74.6 base(inw) 65.8pos 77.4 (+2.8) fw 67.9 (+2.1)pre 75.0 (+0.4) lcw 67.9 (+2.1)suf 75.2 (+0.6) rcw 67.0 (+1.2)pre+suf 75.6 (+1.0) lcw+rcw 66.4 (+0.6)pos+pre 77.9 (+3.3) fw+lcw 68.1(+2.3)pos+suf 77.9 (+3.3) fw+rcw 67.1 (+1.3)all 77.9 (+3.3) all 66.9 (+1.1)Table 2: Effect of each feature set(training with 900abstracts, test with 100 abstracts): (A) identificationphase, (B) semantic classification phaseTable 2, we can see that part-of-speech informationcertainly improves the identification accuracy(about+2.8).
Prefix and suffix features made a positive ef-fect, but only modestly(about +1.2 on average).The column (B) in Table 2 shows semantic clas-sification cases with the identification phase of thebest performance.
We took the feature set composedof the inside words of an entity as a base feature set.And we made several alternatives by adding anotherfeatures.
The experimental results show that func-tional words and left context features are useful, butright context features are not.
Furthermore, part-of-speech information was not effective in the seman-tic classification while it was useful for the entityidentification.
That is, when we took the part-of-speech tags of inside context words instead of theinside context words, the performance of the seman-tic classification was very low(F?=1.0 was 25.1).5.5 Effect of PostProcessing by DictionaryLookupOur two-phase model has the problem that identifi-cation errors are propagated to the semantic classi-fication.
For this reason, it is necessary to ensurea high accuracy of the boundary identification byadopting a method such as post processing of theidentified entities.
Table 3 shows that the post pro-cessing by dictionary lookup is effective to improv-ing the performance of not only the boundary identi-fication accurary(79.2 vs. 79.9) but also the seman-tic classification accuracy(66.1 vs. 66.5).When comparing with the (Kazama, 2002) eventhough the environments is not the same, the pro-posed two-phase model showed much better per-formance in both the entity identification (73.6 vs.81.4) and the entity classification (54.4 vs. 68.0).One of the reason of the performance improvementis that we could take discriminative features for eachsubtask by separating the task into two subtasks.6 ConclusionIn this paper, we proposed a new method of two-phase biomedical named entity recognition based onSVMs and dictionary-lookup.
At the first phase, wetried to identify each entity with one SVM classifierand to post-process with a simple dictionary look-upfor correcting the errors by the SVM.
At the secondTable 3: Performance comparison with or w/o post-processing(F?=1): (A)10-fold cross validation(1800abstracts, test with 200 abstracts), (B)training with 590 abstracts, test with 80 abstractsA B (Kazama, 2002)No.
of W/O PostProc with PostProc No.
of W/O PostProc with PostProc No.
ofInst Inst InstIdentification 76.2/82.4/79.2 76.8/83.1/79.9 78.4/80.8/79.6 80.2/82.6/81.4 75.9/71.4/73.6Classification 63.6/68.8/66.1 64.0/69.2/66.5 65.8/67.9/66.8 67.0/69.0/68.0 56.2/52.8/54.4protein 25,276 60.9/79.8/69.1 61.7/78.8/69.2 1,056 61.3/81.3/69.9 62.8/80.7/70.6 709 49.2/66.4/56.5DNA 8,858 65.1/63.9/64.5 65.0/63.8/64.4 474 71.4/61.0/65.8 72.1/61.6/66.4 460 49.6/37.0/42.3RNA 683 72.2/71.7/72.0 73.8/72.5/73.1 36 74.4/88.9/81.0 75.6/86.1/80.5cell line 3,783 71.6/54.2/61.7 72.3/72.3/72.3 201 73.2/44.8/55.6 73.2/44.8/55.6 121 60.2/46.3/52.3cell type 6,423 67.2/77.5/72.0 67.5/67.5/67.5 252 64.9/82.1/72.5 65.4/81.7/72.7 199 70.0/75.4/72.6phase, we tried to classify the identified entity intoits semantic class by voting the SVMs.
By dividingthe task into two subtasks, the identification and thesemantic classification task, we could select morerelevant features for each task and take an alternativeclassification method according to the task.
This isresulted into the mitigation effect of the unbalancedclass distribution problem but also improvement ofthe performance of the overall tasks.ReferencesN.
Collier, C. Nobata, and J. Tsujii 2000.
Extractingthe Names of Genes and Gene Products with a HiddenMarkov Model.
In Proc.
of Coling2000, pages 201-207.K.
Fukuda, T. Tsunoda, A. Tamura, and T. Takagi.
1998.Information extraction: identifying protein nmes frombiological papers.
In Proc.
of the Pacific Symposiumon Biocomputing ?98(PSB?98).GENIA Corpus 3.0p.
2003. availableat http://www-tsujii.is.s.u-tokyo.ac.jp/ ge-nia/topics/Corpus/3.0/GENIA3.0p.intro.htmlV.
Hatzivalssiloglou, P. A. Duboue, and A. Rzhetsky.2001.
Disambiguating proteins, genes, and RNA intext: a machine learning approach.
Bioinformatics.
17Supple 1.C.
Hsu and C. Lin.
2001.
A comparison on methods formulti-class support vector machines.
Technical report,National Taiwan University, Taiwan.T.
Joachims.
1998.
Making Large-Scale SVM LearningPractical.
LS8-Report, 24, Universitat Dortmund, LSVIII-Report.T.
Joachims.
2000.
Estimating the generalization per-formance of a SVM efficiently.
In Proc.
of the Seven-teenth International Conference on Machine Learning.Morgan Kaufmann, pages 431-438.SVM Light.
2002. available athttp://svmlight.joachims.org/Jun?ichi Kazama, Takaki Makino, Yoshihiro Ohta andJun?ichi Tsujii.
2002.
Tuning support vector machinesfor biomedical named entity recognition.
In Proc.
ofACL-02 Workshop on Natural Language Processing inthe Biomedical Domain, pages 1-8.U.
H.-G Krebel 1999.
Pairwise Classification and Sup-port Vector machines.
In B. Scholkopf, C.J.C.
Burges,Advances in Kernel Methods: Support Vector Learn-ing, pp.
255-268, The MIT Press, Cambridge, MA.C.
Nobata, N. Collier, and J. Tsujii.
1999.
Automaticterm identification and classification in biology texts.In Proc.
of the 5th NLPRS, pages 369-374.B.J.
Stapley, L.A. Kelley, and M.J.E.
Sternberg.
2002.Predicting the Sub-Cellular Location of Proteins fromText Using Support Vector Machines.
In Proc.
of Pa-cific Symposium on Biocomputing 7, pages 374-385.Vladimir Vapnik.
1998.
Statistical Learning Theory Wi-ley, New York.
