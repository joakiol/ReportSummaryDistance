Hybrid Statistical and Structural Semantic Modeling for Thai Multi-Stage Spoken Language UnderstandingChai Wutiwiwatchai   and   Sadaoki FuruiDepartment of Computer Science, Tokyo Institute of Technology2-12-1 Ookayama, Meguro-ku, Tokyo, 152-8552 Japan.
{chai, furui}@furui.cs.titech.ac.jpAbstractThis article proposes a hybrid statistical andstructural semantic model for multi-stagespoken language understanding (SLU).
Thefirst stage of this SLU utilizes a weighted fi-nite-state transducer (WFST)-based parser,which encodes the regular grammar of con-cepts to be extracted.
The proposed methodimproves the regular grammar model by in-corporating a well-known n-gram semantictagger.
This hybrid model thus enhances thesyntax of n-gram outputs while providingrobustness against speech-recognition errors.With applications to a Thai hotel reservationdomain, it is shown to outperform both indi-vidual models at every stage of the SLU sys-tem.
Under the probabilistic WFSTframework, the use of N-best hypothesesfrom the speech recognizer instead of the 1-best can further improve performance requir-ing only a small additional processing time.1 IntroductionAutomatic speech recognition (ASR) for Thai lan-guage is still in the first stage, where Thai researchersin related fields have worked towards creating funda-mental tools for language processing such as phono-logical and morphological analyzers.
Although Thaiwriting is an alphabetic system, a problem of writingwithout sentence markers or spaces between words hasobstructed initiation of development of ASR.
Pioneer-ing a Thai spoken dialogue system has therefore be-come a challenging task, where several uniquecomponents need to be developed specifically for aThai system.Our prototype dialogue system, namely Thai Inter-active Hotel Reservation Agent (TIRA), was createdmainly by handcrafted rules.
The first user evaluation(Wutiwiwatchai and Furui, 2003a) showed that thespoken language understanding (SLU) part of the sys-tem proved the most problematic as it could not coverthe variety of contents supplied by the users, especiallywhen they talked in a mixed-initiative style.To rapidly improve performance, a trainable SLUmodel is preferable and it needs to be able to learnfrom a partially annotated corpus, where only essentialkeywords are given.
This is particularly important forThai where no large corpus is available.Recently, a novel multi-stage SLU model has beendeveloped (Wutiwiwatchai and Furui, 2003b), whichcombines two different practices used for SLU-relatedtasks, robust semantic parsing and topic classification.The former paradigm was implemented in the conceptextraction and concept-value recognition component,whereas the latter was applied for the goal identifica-tion component.
The concept extraction utilizes a setof weighted finite-state transducers (WFST) to encodepossible word-syntax (or regular grammar) expressedfor each concept.
The concept WFST not only deter-mines the existence of a concept in an input utterance,but also labels keywords used to construct its value inthe concept-value recognition component.
Given theextracted concepts, the goal of the utterance can beidentified in the goal identification component using ageneralized pattern classifier.This article reports an improvement of the conceptextraction and concept-value recognition parts by con-ducting a well-known statistical n-gram parser to com-pensate for the concept expressions, which cannot berecognized by the ordinary concept WFST.
The n-gram modeling alone lacks structural information as itcaptures only up to n-word dependencies.
Combiningthe statistical and structural model for SLU hence be-comes a better alternative.
Motivated by B?chet et al(2002), we propose a strategic way called logical n-gram modeling, which combines the statistical n-gramwith the existing regular grammar.
In contrast to theregular-grammar approach, the probabilistic modelallows the SLU to deal with ASR N-best hypotheses,resulting in an increment of the overall performance.Some related works are reviewed in the next sec-tion, followed by a description of our multi-stage SLUmodel.
Section 4 explains the proposed hybrid model.Section 5 shows the evaluation results with a conclu-sion in section 6.2 Related WorksIn the technology of trainable or data-driven SLU, twodifferent practices for different applications have beenwidely investigated.
The first practice aims to tag thewords (or group of words) in the utterance with se-mantic labels, which are later converted to a certainformat of semantic representation.
To generate such asemantic frame, words in the utterance are usuallyaligned to a semantic tree by a parsing algorithm suchas a probabilistic context free grammar or a recursivenetwork whose nodes represent semantic symbols ofthe words and arcs consist of transition probabilities.During parsing, these probabilities are summed up,and used to determine the most likely parsed tree.Many understanding engines have been successfullyimplemented based on this paradigm (Seneff, 1992;Potamianos et al, 2000; Miller et al, 1994).
A draw-back of this method is, however, the requirement of alarge, fully annotated corpus, i.e.
a corpus with seman-tic tags on every word, to ensure training reliability.The second practice has been utilized in applica-tions such as call classification (Gorin et al, 1997).
Inthis application, the understanding module aims toclassify an input utterance to one of predefined usergoals (if an utterance is supposed to have one goal)directly from the words contained in the utterance.This problem can be considered a simple pattern clas-sification task.
An advantage of this method is theneed for training utterances tagged only with theirgoals, one for each utterance.
However, another proc-ess is required if one needs to obtain more detailedinformation.
Our motivation for combining the twopractices described above is that this allows the use ofan only partially annotated corpus, while still allowingthe system to capture sufficient information.
The ideaof combination has also been investigated in otherworks such as Wang et al (2002).Another issue related to this article is the combina-tion of a statistical and rule-based approach for SLU, asystem which is expected to improve the overall per-formance over both individual approaches.
The closestapproach to our work was proposed by B?chet et al(2002), aiming to extract named-entities (NEs) froman input utterance.
NE extraction is performed in twosteps, detecting the NEs by a statistical tagger and ex-tracting NE values using local models.
Est?ve et al(2003) proposed a tighter coupling method that em-beds conceptual structures into the ASR decodingnetwork.
Wang et al (2000), and Hacioglu and Ward(2001) proposed similar ideas for unified models thatincorporated domain-specific context-free grammars(CFGs) into domain-independent n-gram models.
Thehybrid models thus improved the generalized ability ofthe CFG and specificity of the n-gram.
With the exist-ing regular grammar model in a weighted finite-statetransducer (WFST) framework, we propose anotherstrategy to incorporate the statistical n-gram modelinto the concept extraction and concept-value recogni-tion components of our multi-stage SLU.3 Multi-Stage SLUIn the design of our spoken dialogue system, the dia-logue manager decides to respond to the user afterperceiving the user goal.
In some types of goal, infor-mation items contained in the utterance are requiredfor communication.
For example the goal ?request forfacilities?
must come with the facilities the user is ask-ing for, and the goal ?request for prerequisite keys?aims to have the user state the reserved date and thenumber of participants.
Hence, the SLU module mustbe able to identify the goal and extract the requiredinformation items.We proposed a novel SLU model (Wutiwiwatchaiand Furui, 2003b) that processes an input utterance inthree stages, concept extraction, goal identification,and concept-value recognition.
Figure 1 illustrates theoverall architecture of the SLU model, in which itscomponents are described in detail as follows:Figure 1.
Overall architecture of the multi-stage SLU.3.1 Concept extractionThe function of concept extraction is similar to that ofother works, aiming to extract a set of concepts froman input utterance.
However, our way to define a con-cept is rather different.?
A concept has a unique semantic meaning.?
The order of concepts is not important.?
Each type of concept occurs only once in an ut-terance.?
The semantic meaning of a concept can be inter-preted from a sequence of words arbitrarilyplaced in the utterance (the sequence can overlapor cross each other).Examples of utterances and concepts contained in theutterances are shown in Table 1.
A word sequence orConcept-valuerecognitionAcceptedsubstringsGoal Concept-valuesConcept extractionConceptsWord stringGoalidentificationsubstring corresponding to the concept is presented inthe form of a label sequence.
The ???
and two-alphabetsymbols such as ?fd?
denote the words required to in-dicate the concept.
The two-alphabet symbols addi-tionally specify keywords used for concept-valuerecognition.
The ?-?
is for other words not related tothe concept.
As defined above, a concept such as?reqprovide?
(asking whether something is provided) isexpressed by the substring ?there is ?
right?, whichcontains two separated strings, ?there is?
and ?right?.In the same utterance, another concept ?yesnoq?
(ask-ing by a yes-no question) also possesses the word?right?.
We considered this method of definition tohave more impact for presenting the meaning of con-cepts, compared to what has been defined in otherworks.
It must be noted that some concepts containvalues such as the concept ?numperson?
(the number ofpeople), whereas some do not, such as the concept?yesnoq?.Figure 2.
A portion of regular grammar WFST for theconcept ?numperson?
(the number of people).We implemented the concept extraction componentby using weighted finite-state transducers (WFSTs).Similar to the implementation of salient grammarfragments in Gorin et al (1997), the possible wordsequences expressed for a concept are encoded in aWFST, one for each type of concept.
Figure 2 demon-strates a portion of WFST for the concept ?numperson?.Each arc or transition of the WFST is labeled with aninput word (or word class) followed after a colon byan output semantic label, and enclosed after a slash bya weight.
A special symbol ?NIL?
represents any wordnot included in the concept.
The transitions, linkingbetween the start and end node, characterize the ac-ceptable word syntax.
Weights of these transitions,except those containing ?NIL?, are assigned to be -1.The rest are assigned to have zero weights.
The outputlabels indicate keywords as shown in Table 1.
Theselabels will be used later by the concept-value recogni-tion component.In the training step, each concept WFST was cre-ated separately.
The training utterances were tagged bymarking just the words required by the concept.
Thenthe WFST was constructed by:1. replacing the unmarked words in each trainingutterance by the symbols ?NIL?,2.
making an individual FST for the preprocessedutterance,3.
performing the union operation of all FSTs anddeterminizing the resulting FST,4.
attaching the recursive-arcs of every word tothe start and end node as illustrated in Fig.
2,5. assigning the weights to the transitions asdescribed previously.In the parsing step, an input utterance is fed toevery concept WFST in parallel.
For each WFST, thewords in the utterance that are not included in theWFST are replaced by the symbols ?NIL?
and the pre-processed word string is parsed by the WFST using thecomposition operation.
By minimizing the cumulativeweight, the longest accepted substring is chosen.
Aconcept is considered to exist if at least one substringis accepted.
Since this model is a kind of word-grammar representation for a particular concept, wehave called it the concept regular grammar or ?Reg?model in short.
?two  nights  from  the  sixth  of  July?Concept Keyword labels of accepted substring(1) reservedate-        -          ?
?
fd     ?
fm(2) numnightnn      ?
-       -       -       -     -Goal inform_prerequisite-keysLabel sequence2:nn    2:?
1:?
1:?
1:fd  1:?
1:fm?there   is    a   pool,  right?
?Concept Keyword labels of accepted substring(1) reqprovide?
?
-      -        ?
(2) facility-       -     ?
fc       -(3) yesnoq-       -      -      -        ?Goal request_facilityLabel sequence1:?
1:?
2:?
2:fc  1:?,3:?Table 1.
Examples of defined goals, concepts and theircorresponding substrings presented by keyword labels.3.2 Goal identificationHaving extracted the concepts, the goal of the utter-ance can be identified.
The goal in our case can beconsidered as a derivative of the dialogue act coupledwith additional information.
As the examples show inTable 1, the goal ?request_facility?
means a request(dialogue act) for some facilities (additional informa-tion).
Since we observed in our largest corpus thatonly 1.1% were multiple-goal utterances, an utterancecould be supposed to have only one goal.The goal identification task can be viewed as asimple pattern classification problem, where a goal isidentified given an input vector of binary values indi-cating the existence of predefined concepts.
Our previ-ous work (Wutiwiwatchai and Furui, 2003b) showedthat this task could be efficiently achieved by the sim-ple multi-layer perceptron type of artificial neural net-work (ANN).DGT: np /-1I: ac /-1 DGT: nc /-1person: ?
/-1NIL: ?
/0S Efriend: ac /-1I: ?
/0NIL: ?
/0DGT: ?
/0?I :?
/0NIL: ?
/0DGT: ?
/0?3.3 Concept-value recognitionRecall again that some concepts contain values such asthe concept ?numperson?, whose value is the numberof people, whereas some concepts do not, such as theconcept ?yesnoq?.
Given an input utterance, the SLUmodule must be able to identify the goal and extractinformation items such as the reserved date, the num-ber of people, the name of facility, etc.
The conceptsextracted in the first stage are not only used to identifythe goal, but also strongly related to the described in-formation items, that is, the values of concepts areactually the required information items.
Hence, ex-tracting the information items is to recognize the con-cept values.Since the keywords within a concept have alreadybeen labeled by WFST composition in the conceptextraction step, recognizing the concept-value is just amatter of converting the labeled keywords to a certainformat.
For sake of explanation, let?s consider the ut-terance ?two nights from the sixth of July?
in Table 1.After parsing by the ?reservedate?
(the reserved date)concept WFST, the substring ?from the sixth of July?is accepted with the words ?sixth?
and ?July?
labeledby the symbols ?fd?
and ?fm?
respectively.
These labelsymbols are specifically defined for each type of con-cept and have their unique meanings, e.g.
?fd?
for thecheck-in date, ?fm?
for the check-in month, etc.
Thelabeled keywords are then converted to a predefinedformat for the concept value.
The value of ?reserve-date?
concept is in a form of <fy-fm-fd_ty-tm-td>, andthus the labeled keywords ?sixth(fd) July(fm)?
is con-verted to <04-07-06_ty-tm-td>.
It must be noted thatalthough the check-in year is not stated in the utterance,the concept-value recognition process under its knowl-edge-base inherently assigns the value ?04?
(the year2004) to the ?fy?.
This process can greatly help in solv-ing anaphoric expressions in natural conversation.
Ta-ble 2 gives more examples of substrings accepted andlabeled by ?reservedate?
WFST, and their correspond-ing values.
Currently, this conversion task is per-formed by simple rules.Accepted substring Concept-value?sixth(fd) to eighth(td) of July(tm)?
?check-in tomorrow(fd)?
?until next Tuesday(td)?<04-07-06_04-07-08><04-06-10_ty-tm-td><fy-fm-fd_04-06-18>Table 2.
Examples of substrings accepted by the ?re-servedate?
WFST with their corresponding values.4 Hybrid Statistical and Structural Se-mantic ModelingAlthough the Reg model described in Sect.
3.1 has anability to capture long-distant dependencies for seengrammar, it certainly fails to parse an unseen-grammarutterance, especially when it is distorted by speechrecognition errors.
This article thus presents an effortto improve concept extraction and concept-valuerecognition by incorporating a statistical approach.4.1 N-gram modelingWe can view the concept extraction process as a se-quence labeling task, where a label sequence L = (l1 ?lT) as shown in the ?Label sequence?
lines of Table 1is determined given a word string W = (w1?wT).
Eachlabel, in the form of {c:l}, refers to the cth-conceptwith keyword label l. A word is allowed to be in mul-tiple concepts, hence having multiple keyword labelssuch as {1:?,3: ?}
as shown in the last line of Table 1.Finding the most probable sequence L is equivalent tomaximizing the joint probability P(W,L), which can besimplified using n-gram modeling (n = 2 for bigram)as follows:?=?
?==TtttttLLlwlwPLWPL111 ),|,(maxarg),(maxarg~(1)The described n-gram model, called ?Ngram?hereafter, can be implemented also by a WFST, whoseweights are the smoothed n-gram probabilities.
Parsingan utterance by the Ngram WFST is performed simplyby applying the WFST composition in the same wayas operated with the Reg model.4.2 Logical n-gram modelingAlthough the n-gram model can assign a likelihoodscore to any input utterance, it cannot distinguish be-tween valid and invalid grammar structure.
On theother hand, the regular grammar model can give se-mantic tags to an utterance that is permitted by thegrammar, but always rejects an ungrammatical utter-ance.
Thus, another probabilistic approach that inte-grates the advantages of both models is optimum.Our proposed model, motivated mainly by (B?chetet al 2002), combines the statistical and structuralmodels in two-pass processing.
Firstly, the conven-tional n-gram model is used to generate M-best hy-potheses of label sequences given an input word string.The likelihood score of each hypothesis is then en-hanced once its word-and-label syntax is permitted bythe regular grammar model.
By rescoring the M-bestlist using the modified scores, the syntactically validsequence that has the highest n-gram probability isreordered to the top.
Even if no label sequence is per-mitted by the regular grammar, the hybrid model isstill able to output the best sequence based on theoriginal n-gram scores.
Since the proposed model aimsto enhance the logic of n-gram outputs, it is named thelogical n-gram model.This idea can be implemented efficiently in theframework of WFST as depicted in Fig.
3.
At first, theconcept-specific Reg WFST is modified from the oneshown in Fig.
2 by replacing the weight -1 by a vari-able -?, which can be empirically adjusted to gain thebest result.
An unknown word string in the form of afinite state machine is parsed by the Ngram WFST,producing a WFST of M-best label-sequence hypothe-ses.
Concepts are detected in the top hypothesis.
Then,the concept-value recognition process is applied foreach detected concept separately.
In the concept-valuerecognition process, the M-best WFST is intersectedby the concept-specific Reg WFST.
Rescoring theresult offers a new WFST of P-best (P < M) hypothe-ses with a score in logarithmic domain for each hy-pothesis assigned by?=?
?+=Ttttttt lwlwPScore111 )),|,((log ?
, (2)where }0,{??
?t .
If ?
is set to 0, the intersection op-eration is just to filter out the hypotheses that violatethe regular grammar, while the original scores from n-gram model are left unaltered.
If a larger ?
is used, thehypothesis that contains a longer valid syntax is givena higher score.
When no hypothesis in the M-best listis permitted by the grammar (P = 0), the top hypothe-sis of the M-best list is outputted.
It is noted that thestrategy of eliminating unacceptable paths of n-gramdue to syntactical violation has also successfully beenused in a WFST-based speech recognition system(Szarvas and Furui, 2003).
Hereafter, we will refer tothe logical n-gram modeling as ?LNgram?.4.3 The use of ASR N-best hypothesesThe probabilistic model allows the use of N-best hy-potheses from the automatic speech recognition (ASR)engine.
As described in Sect.
4.1, our Ngram semanticmodel produces a joint probability P(W,L), which in-dicates the chance that the semantic-label sequence Loccurs with the word hypothesis W. When the N-bestword hypotheses generated from the ASR are fed intothe Ngram semantic parser, the parsed scores arecombined with the ASR likelihood scores in a log-linear interpolation fashion (Klakow, 1998) as shownin Eq.
3.??
???
?1,),(),(maxarg~ LWPWAPLNWL(3)where A is an acoustic speech signal, and P(A,W) is aproduct of an acoustic score P(A|W) and a languagescore P(W).
?N denotes the N-best list and ?
is an in-terpolation weight, which can be adjusted experimen-tally to give the best result.
This interpolation methodcan be easily implemented in a WFST frameworkcompared to normal linear interpolation.An N-best list can be used in the LNgram usingthe same criterion as well.
The only necessary precau-tion is an appropriate size of M in the M-best seman-tic-label list, which is rescored in the second pass toimprove the concept-value result.Figure 5.
Logical n-gram modeling.5 Evaluation and Discussion5.1 CorporaCollecting and annotating a corpus is an especiallyserious problem for language like Thai, where onlyfew databases are available.
To shorten the collectiontime, we created a specific web page simulating ourexpected conversational dialogues, and asked Thainative users to answer the dialogue questions by typ-ing.
As we asked the users to try answering the ques-tions using spoken language, we could obtain a fairlygood corpus for training the SLU.Currently, 5,869 typed-in utterances from 150 us-ers have been completely annotated.
To reduce theeffort of manual annotation, we conducted a semi-automatic annotation method.
The prototype rule-based SLU was used to roughly tag each utterancewith a goal and concepts, which were then manuallycorrected.
Words or phases that were relevant to theconcept were marked automatically based on theirfrequencies and information mutual to the concept.Finally the tags were manually checked and the key-words within each concept were additionally markedby the defined label symbols.All 5,869 utterances described above were used asa training set (TR) for the SLU system.
We also col-lected a set of speech utterances during an evaluationof our prototype dialogue system.
It contained 1,101speech utterances from 96 dialogues.
By balancing theSemantic-label taggingby the Ngram modelThe top hypothesisM-best hypothesesRescoring by eachconcept Reg modelExtractedconceptsConverting keyword seq.to concept valuesConcept valuesConcept-valuerecognitionWord stringConceptextractionConceptReg modelsoccurrence of goals, we reserved 500 utterances for adevelopment set (DS), which was used for tuning pa-rameters.
The remaining 601 utterances were used foran evaluation set (ES).
Table 3 shows the characteris-tics of each data set.
From the TR set, 75 types of con-cepts and 42 types of goals were defined.
The out-of-goal and out-of-concept denote goals and concepts thatare not defined in the TR set, and thus cannot be rec-ognized by the trained SLU.
Since concepts that con-tain no value are not counted for concept-valueevaluation, Table 3 also shows the number of conceptsthat contain values in the line ?# Concept-values?.Characteristic TR DS ES# Utterances 5,869 500 601# Words / utterance 7.3 6.2 5.8# Goal types 42 40 40# Concept types 75 58 57# Concept-value types 20 18 18# Concepts 10,041 791 949# Concept-values 6,365 366 439% Out-of-goal  5.2 5.3% Out-of-concept  2.8 3.3% Word accuracy  77.2 79.0Table 3.
Characteristics of data sets5.2 Evaluation measuresFour measures were used for evaluation:1.
Word accuracy (WAcc) ?
the standard measurefor evaluating the ASR,2.
Concept F-measure (ConF) ?
the F-measure ofdetected concepts,3.
Goal accuracy (GAcc) ?
the number ofutterances with correctly identified goals,divided by the total number of test utterances,4.
Concept-value accuracy (CAcc) ?
the numberof concepts, whose values are correctlymatched to their references, divided by the totalnumber of concepts that contain values.5.3 The use of logical n-gram modelingThe first experiment was to inspect improvementgained after conducting the statistical approaches forconcept extraction and concept-value recognition.Only the 1-best word hypothesis from the ASR wasexperimented in this section.
The AT&T generalizedFSM library (Mohri et al, 1997) was used to constructand operate all WFSTs, and the SNNS toolkit (Zell etal., 1994) was used to create the ANN classifiers forthe goal identification task.The baseline system utilized the Reg model forconcept extraction and concept-value recognition, andthe multi-layer perceptron ANN for goal identification.75 WFSTs corresponding to the number of definedconcepts were created from the TR set.
The ANN con-sisted of a 75-node input layer, a 100-node hiddenlayer (Wutiwiwatchai and Furui, 2003b), and a 42-node output layer equal to the number of goals to beidentified.66687072747610 20 30 40 50 60 70 80 90 100M -bestCAcc(%)Figure 4.
CAcc results with respect to values of M inan oracle test for the DS set.585960616263640.00.10.20.30.40.50.60.70.80.91.01.11.2CAcc(%)?Figure 5.
CAcc results with variation of ?
for the DSset when M is set to 80.Recognition Orthography MeasureReg Ngram LNgram Reg LNgramConF 76.5 88.6 78.9 91.4GAcc 71.4 76.0 81.2 83.5CAcc 65.1 52.4 67.2 75.7 76.8Table 4.
Evaluation results for the ES set using theReg, Ngram, and LNgram models.Another WFST was constructed for the n-gramsemantic parser (n = 2 in our experiment), which wasused for the Ngram model and the first pass of theLNgram model.
Two parameters, M and ?, in theLNgram approach need to be adjusted.
To determinean appropriate value of M, we plotted in an oraclemode the CAcc of the DS set with respect to M, asshown in Figure 4.
According to the graph, an M of 80was considered optimum and set for the rest of theexperiments.
Figure 5 then shows the CAcc obtainedfor rescored M-best hypotheses when the weight ?
asdefined in Eq.
2 is varied.
Here, the larger value of ?means to assign a higher score to the hypothesis thatcontains longer valid word-and-label syntax.
Hence,we concluded by Fig.
5 that reordering the hypotheses,which contain longer valid syntaxes, could improvethe CAcc significantly.
Since the CAcc results becomesteady when the value of ?
is greater than 0.7, a ?
of1.0 is used henceforth to ensure the best performance.The overall evaluation results on the ES set areshown in Table 4, where M and ?
in the LNgrammodel are set to 80 and 1.0 respectively.
?Recognition?denotes the experiments on automatic speech-recognized utterances (at 79% WAcc), whereas ?Or-thography?
means their exact manual transcriptions.
Itis noted that the LNgram approach utilizes the sameprocess of Ngram in its first pass, where the conceptsare determined.
Therefore, the ConF and GAcc resultsof both approaches are the same.According to the results, the Ngram tagger workedwell for the concept extraction task as it increased theConF by over 10%.
The improvement mainly camefrom reduction of redundant concepts often acceptedby the Reg model.
The better extraction of conceptscould give better goal identification accuracy reasona-bly.
However, as we expected, the conventionalNgram model itself had no syntactic information andthus often produced a confusing label sequence, espe-cially for ill-formed utterances.
A typical error oc-curred for words that could be tagged with one ofseveral semantic labels, such as the word ?MNT?
(re-ferring to the name of the month), which could beidentified as ?check-in month?
or ?check-out month?.These two alternatives could only be clarified by acontext word, which sometimes located far from theword ?MNT?.
This problem could be solved by usingthe Reg model.
The Reg model, however, could notprovide a label sequence to any out-of-syntax sentence.The LNgram as an integration of both models thusobviously outperformed the others.In conclusion, the LNgram model could improvethe ConF, GAcc, and CAcc by 15.8%, 6.4%, and 3.2%relative to the baseline Reg model.
Moreover, if weconsidered the orthography result an upperbound ofthe underlying model, the GAcc and CAcc results pro-duced by the LNgram model are relatively closer totheir upperbounds compared to the Reg model.
Thisverifies robustness improvement of the proposedmodel against speech-recognition errors.5.4 The use of ASR N-best hypothesesTo incorporate N-best hypotheses from the ASR to theLNgram model, we need to firstly determine an ap-propriate value of N. An oracle test that measuresWAcc and ConF for the DS set with variation of N isshown in Fig.
6.
Although we can select a proper valueof N by considering only the WAcc, we also examinethe ConF to ensure that the selected N provides possi-bility to improve the understanding performance aswell.
According to Fig.
6, the ConF highly correlatesto the WAcc, and an N of 50 is considered optimumfor our task.
At this operating point, we plot anothercurve of ConF for the DS set with a variation of ?, theinterpolation weight in Eq.
3, as shown in Fig.
7.
Theappropriate value of ?
is 0.6, as the highest ConF isobtained at this point.
The last parameter we need toadjust is the value of M. Although we have tuned thevalue of M for the case of 1-best word hypothesis, theappropriate value of M may change when the N-besthypotheses are used instead.
However, in our trial, wefound that the optimum value of M is again in thesame range as that operated for the 1-best case.
Aprobable reason is that rescoring the N-best word hy-potheses by the Ngram model can reorder the goodhypotheses to a certain upper portion of the N-best list,and thus rescoring in the second pass of the LNgramis independent to the value of N. Consequently, an Mof 80 as that selected for the 1-best hypothesis is alsoused for the N-best case.75808590951001 11 21 31 41 51 61 71 81 91N -best%WAccConFFigure 6.
WAcc and ConF results with respect to val-ues of N in an oracle test for the DS set.8586878889900.00.10.20.30.40.50.60.70.80.91.0ConF(%)50-best1-best?Figure 7.
ConF results with variation of ?
for the DSset when N is set to 50.Given all tuned parameters, an evaluation on theES set is carried out as shown in Fig.
8.
With the Regmodel as a baseline system, the use of N-best hypothe-ses further improves the the ConF, GAcc, and CAccby 0.9%, 0.6%, and 3.9% from the only 1-best, andhence reduces the gap between the speech-recognizedtest set and the orthography test set by 25%, 5.3%, and26% respectively.Finally, we would like to note that the proposedLNgram approach provided the significant advantageof a much smaller computational time compared to theoriginal Reg approach.
While the Reg model requiresC times (C denotes the number of defined concepts) ofWFST operations to determine concepts, the LNgramneeds only D+1 times (D << C), where D is the num-ber of concepts appearing in the top hypothesis pro-duced by the n-gram semantic model.
Moreover, underthe framework of WFST, incorporating ASR N-besthypotheses required only a small increment of addi-tional processing time compared to the use of 1-best.6065707580859095ConF GAcc CAcc%RegLNgram (1-best)LNgram (50-best)OrthgraphyFigure 8.
Comparative results for the ES set betweenthe use of ASR 1-best and N-best (N = 50) hypotheses.6 Conclusion and Future WorksRecently, a multi-stage spoken language understanding(SLU) approach has been proposed for the first Thaispoken dialogue system.
This article reported an im-provement on the SLU system by replacing the regulargrammar-based semantic model by a hybrid n-gramand regular grammar approach, which not only cap-tures long-distant dependencies of word syntax, butalso provides robustness against speech-recognitionerrors.
The proposed model, called logical n-grammodeling, obviously improved the performance inevery SLU stage, while reducing the computationaltime compared to the original regular-grammar ap-proach.
Under the probabilistic WFST framework, thesystem was improved further by using N-best word-hypotheses from the ASR, requiring only a small addi-tional processing time compared to the use of 1-best.Further improvement of overall speech understand-ing as well as a spoken dialogue system in the futurecan be expected by introducing dialogue-state depend-ent modeling in the ASR and/or the SLU.
A better wayto utilize the first P-best goal hypotheses produced bythe goal identifier instead of 1-best would also en-hance the understanding performance.ReferencesB?chet, F., Gorin, A., Wright, J., and Tur, D. H. 2002.Named entity extraction from spontaneous speech in HowMay I Help You.
Proc.
ICSLP 2002, 597-600.Est?ve, Y., Raymond, C., B?chet, F., and De Mori, R. 2003.Conceptual decoding for spoken dialogue systems.
Proc.Eurospeech 2003, 617-620.Gorin, A. L., Riccardi, G., and Wright, J. H. 1997.
How MayI Help You.
Speech Communication, 23, 113-127.Hacioglu, K., and Ward, W. 2001.
Dialog-context dependentlanguage modeling combining n-grams and stochasticcontext-free grammars.
Proc.
ICASSP 2001, 537-540.Klakow, D. 1998.
Log-linear interpolation of language mod-els.
Proc.
ICSLP 1998, 1695-1699.Miller, S., Bobrow, R., Ingria, R., and Schwartz, R. 1994.Hidden understanding models of natural language.
Proc.ACL 1994, 25-32.Mohri, M., Pereira, F., and Riley, M. 1997.
General-purposefinite-state machine software tools.http://www.research.att.com/sw/tools/fsm, AT&T Labs ?Research.Potamianos, A., Kwang, H., and Kuo, J.
2000.
Statisticalrecursive finite state machine parsing for speech under-standing.
Proc.
ICSLP 2000, vol.3, 510-513.Seneff, S. 1992.
TINA: A natural language system for spokenlanguage applications.
Computational Linguistics, 18(1),61-86.Szarvas, M. and Furui, S. Finite-state transducer basedmodeling of morphosyntax with applications to Hungar-ian LVCSR.
Proc.
ICASSP 2003, 368-371.Wang, Y. Y., Mahajan, M., and Huang, X.
2000.
A unifiedcontext-free grammar and n-gram model for spoken lan-guage processing.
Proc.
ICASSP 2000, 1639-1642.Wang, Y. Y., Acero, A., Chelba, C., Frey, B., and Wong, L.2002.
Combination of statistical and rule-based ap-proaches for spoken language understanding.
Proc.ICSLP 2002, 609-612.Wutiwiwatchai, C. and Furui, S. 2003a.
Pioneering a ThaiLanguage Spoken Dialogue System.
Spring Meeting ofAcoustic Society of Japan, 2-4-15, 87-88.Wutiwiwatchai, C., and Furui, S. 2003b.
Combination offinite state automata and neural network for spoken lan-guage understanding.
Proc.
EuroSpeech 2003, 2761-2764.Zell, A., Mamier, G., Vogt, M., Mach, N., Huebner, R.,Herrmann, K. U., Doering, S., and Posselt, D. SNNSStuttgart neural network simulator, user manual.
Univer-sity of Stuttgart.
