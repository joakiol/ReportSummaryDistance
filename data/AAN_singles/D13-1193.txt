Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1880?1891,Seattle, Washington, USA, 18-21 October 2013. c?2013 Association for Computational LinguisticsAuthorship Attribution of Micro-MessagesRoy Schwartz Oren Tsur Ari RappoportInstitute of Computer ScienceHebrew University of Jerusalem{roys02|oren|arir}@cs.huji.ac.ilMoshe KoppelDepartment of Computer ScienceBar Ilan Universitykoppel@macs.biu.ac.ilAbstractWork on authorship attribution has tradition-ally focused on long texts.
In this work, wetackle the question of whether the author ofa very short text can be successfully iden-tified.
We use Twitter as an experimentaltestbed.
We introduce the concept of an au-thor?s unique ?signature?, and show that suchsignatures are typical of many authors whenwriting very short texts.
We also present a newauthorship attribution feature (?flexible pat-terns?)
and demonstrate a significant improve-ment over our baselines.
Our results show thatthe author of a single tweet can be identifiedwith good accuracy in an array of flavors ofthe authorship attribution task.1 IntroductionResearch in authorship attribution has developedsubstantially over the last decade (Stamatatos,2009).
The vast majority of such research has beendedicated towards finding the author of long texts,ranging from single passages to book chapters.
Inrecent years, the growing popularity of social me-dia has created special interest, both theoretical andcomputational, in short texts.
This has led to manyrecent authorship attribution projects that experi-mented with web data such as emails (Abbasi andChen, 2008), web forum messages (Solorio et al2011) and blogs (Koppel et al 2011b).
This paperaddresses the question to what extent the authors ofvery short texts can be identified.
To answer thisquestion, we experiment with Twitter tweets.Twitter messages (tweets) are limited to 140 char-acters.
This restriction imposes major difficulties onauthorship attribution systems, since authorship at-tribution methods that work well on long texts areoften not as useful when applied to short texts (Bur-rows, 2002; Sanderson and Guenter, 2006).Nonetheless, tweets are relatively self-containedand have smaller sentence length variance com-pared to excerpts from longer texts (see Section 3).These characteristics make Twitter data appealing asa testbed when focusing on short texts.
Moreover,an authorship attribution system of tweets may havevarious applications.
Specifically, a range of cyber-crimes can be addressed using such a system, includ-ing identity fraud and phishing.In this paper, we introduce the concept of k-signatures.
We denote the k-signatures of an authora as the features that appear in at least k% of a?straining samples, while not appearing in the trainingset of any other author.
When k is large, such signa-tures capture a unique style used by a.
An analysisof our training set reveals that unique k-signaturesare typical of many authors.
Moreover, a substantialportion of the tweets in our training set contain atleast one such signature.
These findings suggest thata single tweet, although short and sparse, often con-tains sufficient information for identifying its author.Our results show that this is indeed the case.We train an SVM classifier with a set of featuresthat include character n-grams and word n-grams.We use a rigorous experimental setup, with varyingnumber of authors (values between 50-1,000) andvarious sizes of the training set, ranging from 50 to1,000 tweets per author.
In all our experiments, asingle tweet is used as test document.
We also usea setting in which the system is allowed to responddon?t know in cases of uncertainty.
Applying thisoption results in higher precision, at the expense of1880lower recall.Our results show that the author of a tweet can besuccessfully identified.
For example, when using adataset of as many as 1,000 authors with 200 train-ing tweets per author, we are able to obtain 30.3%accuracy (as opposed to a random baseline of only0.1%).
Using a dataset of 50 authors with as fewas 50 training tweets per author, we obtain 50.7%accuracy.
Using a dataset of 50 authors with 1,000training tweets per author, our results reach as highas 71.2% in the standard classification setting, andexceed 91% accuracy with 60% recall in the don?tknow setting.We also apply a new set of features, never previ-ously used for this task ?
flexible patterns.
Flexi-ble patterns essentially capture the context in whichfunction words are used.
The effectiveness of func-tion words as authorship attribution features (Koppelet al 2009) suggests using flexible pattern features.The fact that flexible patterns are learned from plaintext in a fully unsupervised manner makes themdomain and language independent.
We demon-strate that using flexible patterns gives significantimprovement over our baseline system.
Further-more, using flexible patterns, our system obtains a6.1% improvement over current state-of-the-art re-sults in authorship attribution on Twitter.To summarize, the contribution of this paper isthreefold.?
We provide the most extensive research to dateon authorship attribution of micro-messages,and show that authors of very short texts canbe successfully identified.?
We introduce the concept of an author?s uniquek-signature, and demonstrate that such signa-tures are used by many authors in their writingof micro-messages.?
We present a new feature for authorship attri-bution ?
flexible patterns ?
and show its sig-nificant added value over other methods.
Us-ing this feature, our system obtains a 6.1% im-provement over the current state-of-the-art.The rest of the paper is organized as follows.
Sec-tions 2 and 3 describe our methods and our experi-mental testbed (Twitter).
Section 4 presents the con-cept of k-signatures.
Sections 5 and 6 present ourexperiments and results.
Flexible patterns are pre-sented in Section 7 and related work is presented inSection 8.2 MethodologyIn the following we briefly describe the main fea-tures employed by our system.
The features beloware binary features.Character n-grams.
Character n-gram featuresare especially useful for authorship attribution onmicro-messages since they are relatively tolerantto typos and non-standard use of punctuation (Sta-matatos, 2009).
These are common in the non-formal style generally applied in social media ser-vices.
Consider the example of misspelling ?Brit-ney?
as ?Brittney?.
The misspelled name shares the4-grams ?Brit?
and ?tney?
with the correct name.
Asa result, these features provide information about theauthor?s style (or at least her topic of interest), whichis not available through lexical features.Following standard practice, we use 4-grams(Sanderson and Guenter, 2006; Layton et al 2010;Koppel et al 2011b).
White spaces are consideredcharacters (i.e., a character n-gram may be com-posed of letters from two different words).
A sin-gle white-space is appended to the beginning andthe end of each tweet.
For efficiency, we consideronly character n-gram features that appear at leasttcng times in the training set of at least one author(see Section 5).Word n-grams.
We hypothesize that word n-gramfeatures would be useful for authorship attributionon micro-messages.
We assume that under a strictlength restriction, many authors would prefer usingshort, repeating phrases (word n-grams).In our experiments, we consider 2 ?
n ?
5.1We regard sequences of punctuation marks as words.Two special words are added to each tweet to indi-cate the beginning and the end of the tweet.
For effi-ciency, we consider only word n-gram features thatappear at least twng times in the training set of atleast one author (see Section 5).Model.
We use libsvm?s Matlab implementationof a multi-class SVM classifier with a linear kernel1We skip unigrams as they are generally captured by thecharacter n-gram features.1881(Chang and Lin, 2011).
We use ten-fold cross vali-dation on the training set to select the best regular-ization factor between 0.5 and 0.005.23 Experimental TestbedOur main research question in this paper is to deter-mine the extent to which authors of very short textscan be identified.
A major issue in working withshort texts is selecting the right dataset.
One ap-proach is breaking longer texts into shorter chunks(Sanderson and Guenter, 2006).
We take a differ-ent approach and experiment with micro-messages(specifically, tweets).Tweets have several properties making them anideal testbed for authorship attribution of short texts.First, tweets are posted as single units and do notnecessarily refer to each other.
As a result, they tendto be self contained.
Second, tweets have more stan-dardized length distribution compared to other typesof web data.
We compared the mean and standarddeviation of sentence length in our Twitter datasetand in a corpus of English web data (Ferraresi et al2008).3 We found that (a) tweets are shorter thanstandard web data (14.2 words compared to 20.9),and (b) the standard deviation of the length of tweetsis much smaller (6.4 vs. 21.4).Pre-Processing.
We use a Twitter corpus that in-cludes approximately 5 ?
108 tweets.4 All non-English tweets and tweets that contain fewer than3 words are removed from the dataset.
We also re-move tweets marked as retweets (using the RT sign,a standard Twitter symbol to indicate that this tweetwas written by a different user).
As some usersretweet without using the RT sign, we also removetweets that are an exact copy of an existing tweetposted in the previous seven days.Apart from plain text, some tweets contain ref-erences to other Twitter users (in the format of@<user>).
Since using reference informationmakes this task substantially easier (Layton et al2010), we replace each user reference with the spe-cial meta tag REF.
For sparsity reasons, we also re-place web addresses with the meta tag URL, num-2In practice, 0.05 or 0.1 are selected in almost all cases.3http://wacky.sslmit.unibo.it4These comprise ?15% of all public tweets created fromMay 2009 to March 2010.0 5 10 15 20 25 30 35 40 45 >500102030405060708090Number of k?signatures per userNumber of Usersk = 2%k = 5%k = 10%k = 20%k = 50%Figure 1: Number of users with at least x k-signatures(100 authors, 180 training tweets per author).bers with the meta tag NUM, time of day with themeta tag TIME and dates with the meta tag DATE.4 k-SignaturesIn this section, we show that many authors adopta unique style when writing micro-messages.
Thisstyle can be detected by a strong classification algo-rithm (such as SVM), and be sufficient to correctlyidentify the author of a single tweet.We define the concept of the k-signature of an au-thor a to be a feature that appears in at least k% ofa?s training set, while not appearing in the trainingset of any other user.
Such signatures can be usefulfor identifying future (unlabeled) tweets written bya.To validate our hypothesis, we use a dataset of100 authors with 180 tweets per author.
We com-pute the number of k-signatures used by each ofthe authors in our dataset.
Figure 1 shows our re-sults for a range of k values (2%, 5%, 10%, 20%and 50%).
Results demonstrate that 81 users useat least one 2%-signature, 43 users use at least one5%-signature, and 17 users use at least one 10%-signature.
These results indicate that a large portionof the users adopt a unique signature (or set of sig-natures) when writing short texts.
Table 1 providesexamples of 10%-signatures.1882Signature Type 10%-signature ExamplesCharacter n-grams?
?
?
?REF oh ok ?
?
Glad you found it!Hope everyone is having a good afternoon ?
?REF Smirnoff lol keeping the goose in the freezer ?
?
?yew ?gurl yew serving me tea noochREF about wen yew and ronnie see each otherREF lol so yew goin to check out tini?s tonight huh??
?Word n-grams.. lalREF aww those are cool where u get those.. how do ppl react.. lalLudas album is gone be hott.. lalDayum refs don?t get injury timeouts.. lal.. get him off the field..smoochies , e3I?m just back after takin?
a very long, icy coldshower........Shivering smoochies,E3 http://bit.ly/4CzzP9A blue stout or two would be nice as well, Purr!Blue smoothsmoochies,E3 http://bit.ly/75D4fOThat is sooooooooooooooooooo unfair!Double smoochies,E3http://bit.ly/07sXRGXTable 1: Examples of 10%-signatures.Results also show that seven users use one ormore 20%-signatures, and five users even use oneor more 50%-signatures.
Looking carefully at theseusers, we find that they write very structured mes-sages, and are probably bots, such as news feeds,bidding systems, etc.
Table 2 provides examples oftweets posted by such users.5Another interesting question is how many tweetscontain at least one k-signature.
Figure 2 showsfor each user the number of tweets in her trainingset for which at least one k-signature is found.
Re-sults demonstrate that a total of 18.6% of the train-ing tweets contain at least one 2%-signature, 10.3%the training tweets contain at least one 5%-signatureand 6.5% of the training tweets contain at least one10%-signature.
These findings validate our assump-tion that many users use k-signatures in short texts.These findings also have direct implications onauthorship attribution of micro-messages, since k-signatures are reliable classification features.
Asa result, texts written by authors that tend to usek-signatures are likely to be easily identified by areasonable classification algorithm.
Consequently,k-signatures provide a possible explanation for thehigh quality results presented in this paper.In the broader context, the presence (and contri-5Our k-signature method can actually be useful for automat-ically identifying such users.
We defer this to future work.0 20 40 60 80 100 120 140 160 1800102030405060708090Number of Tweets with at least one k?SignatureNumber of Usersk = 2%k = 5%k = 10%k = 20%k = 50%Figure 2: Number of users with at least x training tweetsthat contain at least one k-signature (100 authors, 180training tweets per author).bution) of k-signatures is in line with the hypothesisproposed by (Davidov et al 2010a): while still us-ing an informal and unstructured (grammatical) lan-guage, authors tend to use typical and unique struc-tures in order to allow a short message to stand alonewithout a clear conversational context.1883User 20%-signature Examples1 I?m listening to :I?m listening to: Sigur R?s ?
Intro:http://www.last.fm/music/Sigur+R%C3%B3s http://bit.ly/3XJHybI?m listening to: Tina Arena ?
In Command:http://www.last.fm/music/Tina+Arena http://bit.ly/7q9E25I?m listening to: Midnight Oil ?
Under the Overpass:http://www.last.fm/music/Midnight+Oil http://bit.ly/7IH4cg2 news now ( str )#Hotel News Now(STR) 5 things to know: 27 May 2009: From the desks ofthe HotelNewsNow.com editor... http://bit.ly/aZTZOq #Tourism #Lodging#Hotel News Now(STR) Five sales renegotiating tactics: As bookings rep-resentatives press to reneg... http://bit.ly/bHPn2L#Hotel News Now(STR) Risk of hotel recession retreats: The Hotel Indus-try?s Pulse Index increases... http://bit.ly/a8EKrm #Tourism #Lodging3( NUM bids )end date :NEW PINK NINTENDO DS LITE CONSOLE WITH 21 GIFTS +CASE: &#163;66.50 (13 Bids) End Date: Tuesday Dec-08-2009 17:..http://bit.ly/7uPt6VMicrosoft Xbox 360 Game System - Console Only - Working: US $51.99(25 Bids) End Date: Saturday Dec-12-2009 13:.. http://bit.ly/8VgdTvMicrosoft Sony Playstation 3 (80 GB) Console 6 Months Old:&#163;190.00 (25 Bids) End Date: Sunday Dec-13-2009 21:21:39 G..http://bit.ly/7kwtDSTable 2: Examples of tweets published by very structured users, suspected to be bots, along with one of their 20%-signatures.5 ExperimentsWe report of three different experimental configu-rations.
In the experiments described below, eachdataset is divided into training and test sets usingten-fold cross validation.
On the test phase, eachdocument contains a single tweet.Experimenting with varying Training Set Sizes.In order to test the affect of the training set size,we experiment with an increasingly larger numberof tweets per author.
Experimenting with a range oftraining set sizes serves two purposes: (a) to checkwhether the author of a tweet can be identified us-ing a very small number of (short) training samples,and (b) check howmuch our system can benefit fromtraining on a larger corpus.In our experiments we only consider users whoposted between 1,000?2,000 tweets6 (a total of6This range is selected since on one hand we want at least1,000 tweets per author for our experiments, and on the otherhand we noticed that users with a larger number of tweets incorpus tend to be spammers or bots that are very easy to identify,so we limit this number to 2,000.10,183 users), and randomly select 1,000 tweets peruser.
From these users, we select 10 groups of 50users each.7 We perform a set of classification ex-periments, selecting for each author an increasinglylarger subset of her 1,000 tweets as training set.
Sub-set sizes are (50, 100, 200, 500, 1,000).
Thresh-old values for our features in each setting (see Sec-tion 2) are (2, 2, 4, 10, 20) for tcng and (2, 2, 2, 3, 5)for twng, respectively.Experimenting with varying Numbers of Au-thors.
In a second set of experiments, we use anincreasingly larger number of authors (values be-tween 100-1,000), in order to check whether the au-thor of a very short text can be identified in a ?needlein a haystack?
type of setting.Due to complexity issues, we only experimentwith 200 tweets per author as training set.
We se-lect groups of size 100, 200, 500 and 1,000 users(one group per size).
We use the same threshold val-ues as the 200 tweets per author setting previouslydescribed (tcng = 4, twng = 2).7An eleventh group is selected as development set.18840 100 200 300 400 500 600 700 800 900 1000455055606570Training Set SizeAccuracy(%)Char.
N?grams + Word N?gramsChar.
N?gramsFigure 3: Authorship attribution accuracy for 50 authorswith various training set sizes.
The values are averagedover 10 groups.
The random baseline is 2%.Recall-Precision Tradeoff.
Another aspect of ourresearch question is the level of certainty our systemhas when suggesting an author for a given tweet.In cases of uncertainty, many real life applicationswould prefer not to get any response instead of get-ting a response with low certainty.
Moreover, in reallife applications we are often not even sure that thereal author is part of our training set.
Consequently,we allow our system to respond ?don?t know?
incases of low confidence (Koppel et al 2006; Kop-pel et al 2011b).
This allows our system to obtainhigher precision, at the expense of lower recall.To implement this feature, we use SVM?s proba-bility estimates, as implemented in libsvm.
Theseestimates give a score to each potential author.These scores reflect the probability that this authoris the correct author, as decided by the predictionmodel.
The selected author is always the one withthe highest probability estimate.As selection criterion, we use a set of increasinglylarger thresholds (0.05-0.9) for the probability of theselected author.
This means that we do not select testsamples for which the selected author has a proba-bility estimate value lower than the threshold.0 100 200 300 400 500 600 700 800 900 10002530354045505560Number of Candidate AuthorsAccuracy(%)Char.
N?grams + Word N?gramsChar.
N?gramsFigure 4: Authorship attribution accuracy with varyingnumber of candidate authors, using 200 training tweetsper author.
The random baselines for 509, 100, 200, 500and 1,000 authors are 2%, 1%, 0.5%, 0.2% and 0.1%,respectively.6 Basic ResultsExperimenting with varying Training Set Sizes.Figure 3 shows results for our experiments with50 authors and various training set sizes.
Resultsdemonstrate that authors of very short texts can besuccessfully identified, even with as few as 50 tweetsper author (49.5%).
When given more training sam-ples, authors are identified much more accurately(up to 69.7%).
Results also show that, according toour hypothesis, word n-gram features substantiallyimprove over character n-grams features only (3%averaged improvement over all settings).Experimenting with varying Numbers of Au-thors.
Figure 4 shows our results for various num-bers of authors, using 200 tweets per author as train-ing set.
Results demonstrate that authors of anunknown tweet can be identified to a large extenteven when there are as many as 1,000 candidate au-thors (30.3%, as opposed to a random baseline ofonly 0.1%).
Results further validate that word n-gram features substantially improve over character9Results for 50 authors with 200 tweets per author are takenfrom Figure 3.18850 10 20 30 40 50 60 70 80 90 100405060708090100Recall (%)Precision(%)1,000 tweets/author500 tweets/author200 tweets/author100 tweets/author50 tweets/authorFigure 5: Recall-precision curves for 50 authors withvarying training set sizes.n-grams features (2.6% averaged improvement).Recall-Precision Tradeoff.
Figure 5 shows therecall-precision curves for our experiments with 50authors and varying training set sizes.
Resultsdemonstrate that we are able to obtain very high pre-cision (over 90%) while still maintaining a relativelyhigh recall (from ?35% recall for 50 tweets per au-thor up to> 60% recall for 1,000 tweets per author).Figure 6 shows the recall-precision curves for ourexperiments with varying number of authors.
Re-sults demonstrate that even in the 1,000 authors set-ting, we are able to obtain high precision values(90% and 70%) with reasonable recall values (18%and ?30%, respectively).7 Flexible PatternsIn previous sections we provided strong evidencethat authors of micro-messages can be successfullyidentified using standard methods.
In this section wepresent a new feature, never previously used for thistask ?
flexible patterns.
We show that flexible pat-terns can be used to improve classification results.Flexible patterns are a generalization of word n-grams, in the sense that they capture potentially un-seen word n-grams.
As a result, flexible patternscan pick up fine-grained differences between au-thors?
styles.
Unlike other types of pattern features,0 10 20 30 40 50 60 70 80 90 10030405060708090100Recall (%)Precision(%)50 authors100 authors200 authors500 authors1,000 authorsFigure 6: Recall-precision curves for varying number ofauthors.flexible patterns are computed automatically fromplain text.
As such, they can be applied to varioustasks, independently of domain and language.
Wedescribe them in detail.Word Frequency.
Flexible patterns are composedof high frequency words (HFW) and content words(CW).
Every word in the corpus is defined as eitherHFW or CW.
This clustering is performed by count-ing the number of times each word appears in thecorpus of size s. A word that appears more than10?4?s times in a corpus is considered HFW.
Aword that appears less than 10?3?s times in a cor-pus is considered CW.
Some words may serve bothas HFWs and CWs (see Davidov and Rappoport(2008b) for discussion).Structure of a Flexible Pattern.
Flexible patternsstart and end with an HFW.
A sequence of zero ormore CWs separates consecutive HFWs.
At leastone CW must appear in every pattern.10 For effi-ciency, at most six HFWs (and as a result, five CWsequences) may appear in a flexible pattern.
Exam-ples of flexible patterns include1.
?theHFW CW ofHFW theHFW?10Omitting this treats word n-grams as flexible patterns.1886Flexible Pattern Features.
Flexible patterns canserve as binary classification features; a tweetmatches a given flexible pattern if it contains theflexible pattern sequence.
For example, (1) ismatched by (2).2.
?Go to theHFW houseCW ofHFW theHFW rising sun?Partial Flexible Patterns.
A flexible pattern mayappear in a given tweet with additional words notoriginally found in the flexible pattern, and/or withonly a subset of the HFWs (Davidov et al 2010a).For example, (3) is a partial match of (1), since theword ?great?
is not part of the original flexible pat-tern.
Similarly, (4) is another partial match of (1),since (a) the word ?good?
is not part of the originalflexible pattern and (b) the second occurrence of theword ?the?
does not appear in (4) (missing word ismarked by ).3.
?TheHFW greatHFW kingCW ofHFW theHFW ring?4.
?TheHFW goodHFW kingCW ofHFW Spain?We use such cases as features with lower weight,proportional to the number of found HFWs in thetweet (w = 0.5?nfoundnexpected ).
For example, (1) receives aweight of 1 (complete match) against (2).
Against(3), it receives a weight of 0.5 (= 0.5?33 , partialmatch with no missing HFWs).
Against (4) it re-ceives a weight of 1/3 (= 0.5?23 , partial match withonly 2/3 HFWs found).Experimenting with Flexible Pattern Features.We repeat our experiments with varying training setsizes (see Section 5) with two more systems: onethat uses character n-grams and flexible pattern fea-tures, and another that uses character n-grams, wordn-grams and flexible patterns.
High frequency wordcounts are computed separately for each author us-ing her training set.
We only consider flexible pat-tern features that appear at least tfp times in thetraining set of at least one author.
Values of tfp fortraining set sizes (50, 100, 200, 500, 1,000) are (2,3, 7, 7, 8), respectively.Results.
Figure 7 shows our results.
Resultsdemonstrate that flexible pattern features have anadded value over both character n-grams alone (av-eraged 2.9% improvement) and over character n-grams and word n-grams together (averaged 1.5%0 100 200 300 400 500 600 700 800 900 1000354045505560657075Training Set SizeAccuracy(%)Char.
N?grams, Word N?grams &Flex.
Patt.
Feats.Char.
N?grams + Flex.
Patt.
Feats.Char.
N?grams + Word N?gramsChar.
N?gramsSCAPNaive BayesFigure 7: Authorship attribution accuracy for 50 authorswith various training set sizes and various feature sets.The values are averaged over 10 groups.
The randombaseline is 2%.Comparison to previous work: SCAP ?
SCAP algo-rithm results, as reported by (Layton et al 2010), NaiveBayes ?
Naive Bayes algorithm results, as reported by(Boutwell, 2011).improvement).
We perform t-tests on each of ourtraining set sizes to check whether the latter im-provement is significant.
Results demonstrate thatit is highly significant in all settings, with p-valuessmaller than values between 10?3 (for 50 tweets perauthor) and 10?8 (1,000 tweets per author).Comparison to Previous Works.
Figure 7 alsoshows results for the only two works that experi-mented in some of the settings we experimented in:Layton et al(2010) and Boutwell (2011) (see Sec-tion 8).
Our system substantially outperforms thesetwo systems, by margins of 5.9% to 19%.
Thesemargins are explained by the choice of algorithm(SVM and not SCAP/naive Bayes) and our set offeatures (character n-grams + word n-grams + flex-ible patterns compared to character n-grams only).In order to rule out the possibility that these mar-gins stem from using different datasets, we testedour system on the dataset used in (Layton et al2010).
Our system obtains even higher results onthis dataset than on our datasets (61.6%, a total im-1887provement of 6.1% over (Layton et al 2010)).Discussion.
To illustrate the additional contribu-tion of flexible patterns over word n-grams, considerthe following tweets, written by the same author.5.
?.
.
.
theHFW wayCW IHFW treatedCW herHFW?6.
?.
.
.
half of theHFW thingsCW IHFW have seen?7.
?.
.
.
theHFW friendsCW IHFW have had for years?8.
?.
.
.
in theHFW neighborhoodCW IHFW grew up in?Consider a case where (5) is part of the test set,while (6-8) appear in the training set.
As (5) sharesno sequence of words with (6-8), no word n-gramfeature is able to identify the author?s style in (5).However, this style can be successfully identified us-ing the flexible pattern (9), shared by (5-8).9. theHFW CW IHFWThis demonstrates the added value flexible patternfeatures have over word n-gram features.8 Related WorkAuthorship attribution dates back to the end of 19thcentury, when (Mendenhall, 1887) applied sentencelength and word length features to plays of Shake-speare.
Ever since, many methods have been devel-oped for this task.
For recent surveys, see (Koppelet al 2009; Stamatatos, 2009; Juola, 2012).Authorship attribution methods can be generallydivided into two categories (Stamatatos, 2009).
Insimilarity-based methods, an anonymous text is at-tributed to some author whose writing style is mostsimilar (by some distance metric).
In machine learn-ing methods, which we follow in this paper, anony-mous texts are classified, using machine learning al-gorithms, into different categories (in this case, dif-ferent authors).Machine learning papers differ from each other bythe features and machine learning algorithm.
Exam-ples of features include HFWs (Mosteller and Wal-lace, 1964; Argamon et al 2007), character n-gram(Kjell, 1994; Hoorn et al 1999; Stamatatos, 2008),word n-grams (Peng et al 2004), part-of-speechn-grams (Koppel and Schler, 2003; Koppel et al2005) and vocabulary richness (Abbasi and Chen,2005).The various machine learning algorithms used in-clude naive Bayes (Mosteller and Wallace, 1964;Kjell, 1994), neural networks (Matthews and Mer-riam, 1993; Kjell, 1994), K-nearest neighbors (Kjellet al 1995; Hoorn et al 1999) and SVM (De Vel etal., 2001; Diederich et al 2003; Koppel and Schler,2003).Traditionally, authorship attribution systems havemainly been evaluated against long texts such astheater plays (Mendenhall, 1887), essays (Yule,1939; Mosteller and Wallace, 1964), biblical books(Mealand, 1995; Koppel et al 2011a) and bookchapters (Argamon et al 2007; Koppel et al 2007).In recent year, many works focused on web datasuch as emails (De Vel et al 2001; Koppel andSchler, 2003; Abbasi and Chen, 2008), web forummessages (Abbasi and Chen, 2005; Solorio et al2011), blogs (Koppel et al 2006; Koppel et al2011b) and chat messages (Abbasi and Chen, 2008).Some works focused on SMS messages (Mohan etal., 2010; Ishihara, 2011).Authorship Attribution on Twitter.
The perfor-mance of authorship attribution systems on shorttexts is affected by several factors (Stamatatos,2009).
These factors include the number of candi-date authors, the training set size and the size of thetest document.Very few authorship attribution works experi-mented with Twitter.
Unlike our work, all used asingle group of authors (group sizes varied between3-50).
Layton et al(2010) used the SCAP method-ology (Frantzeskou et al 2007) with character n-gram features.
They experimented with 50 authorsand compared different numbers of tweets per au-thor (values between 20-200).
Surprisingly, theyshowed that their system does not improve whengiven more training tweets.
In our work, we no-ticed a different trend, and showed that more datacan be extremely valuable for authorship attributionsystems on micro-messages (see Section 6).
Silvaet al(2011) trained an SVM classifier with variousfeatures (e.g., punctuation and vocabulary features)on a small dataset of three authors only, with vary-ing training set size.
Although their work used aset of Twitter-specific features that we do not explic-itly use, our features implicitly cover a large portionof their features (such as punctuation and emoticon1888features, which are largely covered by character n-grams).Boutwell (2011) used a naive Bayes classifierwith character n-gram features.
She experimentedwith 50 authors and two training size values (120and 230).
She also provided a set of experiments thatstudied the effect of joining several tweets into a sin-gle document.
Mikros and Perifanos (2013) trainedan SVM classifier with character n-gram and wordn-grams.
They experimented with 10 authors ofGreek text, and also joined several tweets into a sin-gle document.
Joining several tweets into a longerdocument is appealing since it can lead to substantialimprovement of the classification results, as demon-strated by the works above.
However, this approachrequires the test data to contain several tweets thatare known a-priori to be written by the same author.This assumption is not always realistic.
In our paper,we intentionally focus on a single tweet as documentsize.Flexible Patterns.
Patterns were introduced by(Hearst, 1992), who used hand crafted patternsto discover hyponyms.
Hard coded patternswere used for many tasks, such as discoveringmeronymy (Berland and Charniak, 1999), noun cat-egories (Widdows and Dorow, 2002), verb relations(Chklovski and Pantel, 2004) and semantic classlearning (Kozareva et al 2008).Patterns were first extracted in a fully unsuper-vised manner (?flexible patterns?)
by (Davidov andRappoport, 2006), who used flexible patterns in or-der to establish noun categories, and (Bicic?i andYuret, 2006) who used them for analogy questionanswering.
Ever since, flexible patterns were usedas features for various tasks such as extraction ofsemantic relationships (Davidov et al 2007; Tur-ney, 2008b; Bollegala et al 2009), detection ofsynonyms (Turney, 2008a), disambiguation of nom-inal compound relations (Davidov and Rappoport,2008a), sentiment analysis (Davidov et al 2010b)and detection of sarcasm (Tsur et al 2010).9 ConclusionThe main goal of this paper is to measure to whatextent authors of micro-messages can be identified.We have shown that authors of very short textscan be successfully identified in an array of au-thorship attribution settings reported for long doc-uments.
This is the first work on micro-messagesto address some of these settings.
We introducedthe concept of k-signature.
Using this concept, weproposed an interpretation of our results.
Last, wepresented the first authorship attribution system thatuses flexible patterns, and demonstrated that usingthese features significantly improves over other sys-tems.
Our system obtains 6.1% improvement overthe current state-of-the-art.AcknowledgmentsWe would like to thank Elad Eban and Susan Good-man for their helpful advice, as well as Robert Lay-ton for providing us with his dataset.
This researchwas funded (in part) by the Harry and Sylvia Hoff-man leadership and responsibility program (for thefirst author) and the Intel Collaborative Research In-stitute for Computational Intelligence (ICRI-CI).ReferencesAhmed Abbasi and Hsinchun Chen.
2005.
Applying au-thorship analysis to extremist-group web forum mes-sages.
IEEE Intelligent Systems, 20:67?75.Ahmed Abbasi and Hsinchun Chen.
2008.
Writeprints:A stylometric approach to identity-level identificationand similarity detection in cyberspace.
ACM Transac-tions on Information Systems, 26(2):7:1?7:29.Shlomo Argamon, Casey Whitelaw, Paul Chase, Sob-han Raj Hota, Navendu Garg, and Shlomo Levitan.2007.
Stylistic text classification using functional lex-ical features: Research articles.
J.
Am.
Soc.
Inf.
Sci.Technol., 58(6):802?822.Matthew Berland and Eugene Charniak.
1999.
Findingparts in very large corpora.
In Proc.
of ACL, pages57?64, College Park, Maryland, USA.Ergun Bicic?i and Deniz Yuret.
2006.
Clustering wordpairs to answer analogy questions.
In Proc.
of TAINN,pages 1?8.Danushka T. Bollegala, Yutaka Matsuo, and MitsuruIshizuka.
2009.
Measuring the similarity betweenimplicit semantic relations from the web.
In Proc.
ofWWW, New York, New York, USA.
ACM Press.Sarah R. Boutwell.
2011.
Authorship Attribution ofShort Messages Using Multimodal Features.
Master?sthesis, Naval Postgraduate School.John Burrows.
2002.
?Delta?
: a Measure of StylisticDifference and a Guide to Likely Authorship.
Literaryand Linguistic Computing, 17(3):267?287.1889Chih-Chung Chang and Chih-Jen Lin.
2011.
LIBSVM:A library for support vector machines.
ACM Transac-tions on Intelligent Systems and Technology, 2:27:1?27:27.
Software available at http://www.csie.ntu.edu.tw/?cjlin/libsvm.Timothy Chklovski and Patrick Pantel.
2004.
Verbo-cean: Mining the web for fine-grained semantic verbrelations.
In Dekang Lin and Dekai Wu, editors, Proc.of EMNLP, pages 33?40, Barcelona, Spain.Dmitry Davidov and Ari Rappoport.
2006.
Efficient un-supervised discovery of word categories using sym-metric patterns and high frequency words.
In Proc.of ACL-Coling, pages 297?304, Sydney, Australia.Dmitry Davidov and Ari Rappoport.
2008a.
Classifi-cation of semantic relationships between nominals us-ing pattern clusters.
In Proceedings of ACL-08: HLT,pages 227?235, Columbus, Ohio, June.
Associationfor Computational Linguistics.Dmitry Davidov and Ari Rappoport.
2008b.
Unsuper-vised discovery of generic relationships using patternclusters and its evaluation by automatically generatedSAT analogy questions.
In Proc.
of ACL-HLT, pages692?700, Columbus, Ohio.Dmitry Davidov, Ari Rappoport, and Moshe Koppel.2007.
Fully unsupervised discovery of concept-specific relationships by web mining.
In Proc.
of ACL,pages 232?239, Prague, Czech Republic.Dmitry Davidov, Oren Tsur, and Ari Rappoport.
2010a.Semi-supervised recognition of sarcastic sentences intwitter and amazon.
In Proc.
of CoNLL, pages 107?116, Uppsala, Sweden.Dmitry Davidov, Oren Tsur, and Ari Rappoport.
2010b.Enhanced sentiment learning using twitter hashtagsand smileys.
In Proc.
of Coling, pages 241?249, Bei-jing, China.Olivier De Vel, Alison Anderson, Malcolm Corney, andGeorge Mohay.
2001.
Mining e-mail content for au-thor identification forensics.
ACM Sigmod Record,30(4):55?64.JoachimDiederich, Jo?rg Kindermann, Edda Leopold, andGerhard Paass.
2003.
Authorship attribution withsupport vector machines.
Applied intelligence, 19(1-2):109?123.Adriano Ferraresi, Eros Zanchetta, Marco Baroni, andSilvia Bernardini.
2008.
Introducing and evaluatingukwac, a very large web-derived corpus of english.
InProc.
of the 4th Web as Corpus Workshop, WAC-4.Georgia Frantzeskou, Efstathios Stamatatos, StefanosGritzalis, and Carole E Chaski.
2007.
Identifying au-thorship by byte-level n-grams: The source code au-thor profile (scap) method.
Int Journal of Digital Evi-dence, 6(1):1?18.Marti A. Hearst.
1992.
Automatic acquisition of hy-ponyms from large text corpora.
In Proc.
of Coling?
Volume 2, pages 539?545, Stroudsburg, PA, USA.Johan F Hoorn, Stefan L Frank, Wojtek Kowalczyk, andFloor van der Ham.
1999.
Neural network identifi-cation of poets using letter sequences.
Literary andLinguistic Computing, 14(3):311?338.Shunichi Ishihara.
2011.
A forensic authorship clas-sification in sms messages: A likelihood ratio basedapproach using n-gram.
In Proc.
of the AustralasianLanguage Technology Association Workshop 2011,pages 47?56, Canberra, Australia.Patrick Juola.
2012.
Large-scale experiments in author-ship attribution.
English Studies, 93(3):275?283.Bradley Kjell, W Addison Woods, and Ophir Frieder.1995.
Information retrieval using letter tuples withneural network and nearest neighbor classifiers.
InIEEE International Conference on Systems, Man andCybernetics, volume 2, pages 1222?1226.
IEEE.Bradley Kjell.
1994.
Authorship determination using let-ter pair frequency features with neural network classi-fiers.
Literary and Linguistic Computing, 9(2):119?124.Moshe Koppel and Jonathan Schler.
2003.
Exploitingstylistic idiosyncrasies for authorship attribution.
InProc.
of IJCAI?03 Workshop on Computational Ap-proaches to Style Analysis and Synthesis, volume 69,page 72.Moshe Koppel, Jonathan Schler, and Kfir Zigdon.
2005.Determining an author?s native language by mining atext for errors.
In Proc.
of the eleventh ACM SIGKDDinternational conference on Knowledge discovery indata mining, KDD ?05, pages 624?628, New York,NY, USA.Moshe Koppel, Jonathan Schler, Shlomo Argamon, andEranMesseri.
2006.
Authorship attribution with thou-sands of candidate authors.
In SIGIR, pages 659?660.Moshe Koppel, Jonathan Schler, and Elisheva Bonchek-Dokow.
2007.
Measuring differentiability: Unmask-ing pseudonymous authors.
JMLR, 8:1261?1276.Moshe Koppel, Jonathan Schler, and Shlomo Argamon.2009.
Computational methods in authorship attribu-tion.
J.
Am.
Soc.
Inf.
Sci.
Technol., 60(1):9?26.Moshe Koppel, Navot Akiva, Idan Dershowitz, andNachum Dershowitz.
2011a.
Unsupervised decom-position of a document into authorial components.
InProc.
of ACL-HLT, pages 1356?1364, Portland, Ore-gon, USA.Moshe Koppel, Jonathan Schler, and Shlomo Argamon.2011b.
Authorship attribution in the wild.
LanguageResources and Evaluation, 45(1):83?94.Zornitsa Kozareva, Ellen Riloff, and Eduard Hovy.
2008.Semantic class learning from the web with hyponym1890pattern linkage graphs.
In Proc.
of ACL-HLT, pages1048?1056, Columbus, Ohio.Robert Layton, PaulWatters, and Richard Dazeley.
2010.Authorship attribution for twitter in 140 characters orless.
In Proc.
of the 2010 Second Cybercrime andTrustworthy Computing Workshop, CTC ?10, pages 1?8, Washington, DC, USA.
IEEE Computer Society.Robert AJ Matthews and Thomas VN Merriam.
1993.Neural computation in stylometry i: An application tothe works of shakespeare and fletcher.
Literary andLinguistic Computing, 8(4):203?209.DL Mealand.
1995.
Correspondence analysis of luke.Literary and linguistic computing, 10(3):171?182.Thomas Corwin Mendenhall.
1887.
The characteristiccurves of composition.
Science, ns-9(214S):237?246.George K Mikros and Kostas Perifanos.
2013.
Author-ship attribution in greek tweets using authors multi-level n-gram profiles.
In 2013 AAAI Spring Sympo-sium Series.Ashwin Mohan, Ibrahim M Baggili, and Marcus KRogers.
2010.
Authorship attribution of sms mes-sages using an n-grams approach.
Technical report,CERIAS Tech Report 2011.Frederick Mosteller and David Lee Wallace.
1964.Inference and disputed authorship: The Federalist.Addison-Wesley.Fuchun Peng, Dale Schuurmans, and Shaojun Wang.2004.
Augmenting naive bayes classifiers with sta-tistical language models.
Information Retrieval, 7(3-4):317?345.Conrad Sanderson and Simon Guenter.
2006.
Short textauthorship attribution via sequence kernels, markovchains and author unmasking: An investigation.
InProc.
of EMNLP, pages 482?491, Sydney, Australia.Rui Sousa Silva, Gustavo Laboreiro, Lu?
?s Sarmento, TimGrant, Euge?nio Oliveira, and Belinda Maia.
2011.?twazn me!!!
;(?
automatic authorship analysis ofmicro-blogging messages.
In Proc.
of the 16th inter-national conference on Natural language processingand information systems, NLDB?11, pages 161?168,Berlin, Heidelberg.
Springer-Verlag.Thamar Solorio, Sangita Pillay, Sindhu Raghavan, andManuel Montes-Gomez.
2011.
Modality specificmeta features for authorship attribution in web forumposts.
In Proc.
of IJCNLP, pages 156?164, ChiangMai, Thailand, November.Efstathios Stamatatos.
2008.
Author identification: Us-ing text sampling to handle the class imbalance prob-lem.
Inf.
Process.
Manage., 44(2):790?799.Efstathios Stamatatos.
2009.
A survey of modern au-thorship attribution methods.
Journal of the Ameri-can Society for Information Science and Technology,60(3):538?556.Oren Tsur, Dmitry Davidov, and Ari Rappoport.
2010.Icwsm?a great catchy name: Semi-supervised recog-nition of sarcastic sentences in online product reviews.In Proc.
of ICWSM.Peter Turney.
2008a.
A uniform approach to analogies,synonyms, antonyms, and associations.
In Proc.
ofColing, pages 905?912,Manchester, UK, August.
Col-ing 2008 Organizing Committee.Peter D. Turney.
2008b.
The latent relation mapping en-gine: Algorithm and experiments.
Journal of ArtificialIntelligence Research, 33:615?655.Dominic Widdows and Beate Dorow.
2002.
A graphmodel for unsupervised lexical acquisition.
In Proc.of Coling, pages 1?7, Stroudsburg, PA, USA.George Udny Yule.
1939.
On sentence-length as a statis-tical characteristic of style in prose: with applicationto two cases of disputed authorship.
Biometrika, 30(3-4):363?390.1891
