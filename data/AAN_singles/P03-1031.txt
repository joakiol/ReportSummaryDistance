Corpus-based Discourse Understanding in Spoken Dialogue SystemsRyuichiro Higashinaka and Mikio Nakano and Kiyoaki Aikawa?NTT Communication Science LaboratoriesNippon Telegraph and Telephone Corporation3-1 Morinosato WakamiyaAtsugi, Kanagawa 243-0198, Japan{rh,nakano}@atom.brl.ntt.co.jp, aik@idea.brl.ntt.co.jpAbstractThis paper concerns the discourse under-standing process in spoken dialogue sys-tems.
This process enables the system tounderstand user utterances based on thecontext of a dialogue.
Since multiple can-didates for the understanding result canbe obtained for a user utterance due tothe ambiguity of speech understanding, itis not appropriate to decide on a singleunderstanding result after each user ut-terance.
By holding multiple candidatesfor understanding results and resolving theambiguity as the dialogue progresses, thediscourse understanding accuracy can beimproved.
This paper proposes a methodfor resolving this ambiguity based on sta-tistical information obtained from dia-logue corpora.
Unlike conventional meth-ods that use hand-crafted rules, the pro-posed method enables easy design of thediscourse understanding process.
Experi-ment results have shown that a system thatexploits the proposed method performssufficiently and that holding multiple can-didates for understanding results is effec-tive.
?Currently with the School of Media Science, Tokyo Uni-versity of Technology, 1404-1 Katakuracho, Hachioji, Tokyo192-0982, Japan.1 IntroductionFor spoken dialogue systems to correctly understanduser intentions to achieve certain tasks while con-versing with users, the dialogue state has to be ap-propriately updated (Zue and Glass, 2000) after eachuser utterance.
Here, a dialogue state means allthe information that the system possesses concern-ing the dialogue.
For example, a dialogue state in-cludes intention recognition results after each userutterance, the user utterance history, the system ut-terance history, and so forth.
Obtaining the user in-tention and the content of an utterance using only thesingle utterance is called speech understanding, andupdating the dialogue state based on both the previ-ous utterance and the current dialogue state is calleddiscourse understanding.
In general, the result ofspeech understanding can be ambiguous, because itis currently difficult to uniquely decide on a singlespeech recognition result out of the many recogni-tion candidates available, and because the syntac-tic and semantic analysis process normally producemultiple hypotheses.
The system, however, has to beable to uniquely determine the understanding resultafter each user utterance in order to respond to theuser.
The system therefore must be able to choosethe appropriate speech understanding result by re-ferring to the dialogue state.Most conventional systems uniquely determinethe result of the discourse understanding, i.e., thedialogue state, after each user utterance.
However,multiple dialogue states are created from the currentdialogue state and the speech understanding resultscorresponding to the user utterance, which leads toambiguity.
When this ambiguity is ignored, the dis-course understanding accuracy is likely to decrease.Our idea for improving the discourse understandingaccuracy is to make the system hold multiple dia-logue states after a user utterance and use succeed-ing utterances to resolve the ambiguity among di-alogue states.
Although the concept of combiningmultiple dialogue states and speech understandingresults has already been reported (Miyazaki et al,2002), they use intuition-based hand-crafted rulesfor the disambiguation of dialogue states, which arecostly and sometimes lead to inaccuracy.
To resolvethe ambiguity of dialogue states and reduce the costof rule making, we propose using statistical infor-mation obtained from dialogue corpora, which com-prise dialogues conducted between the system andusers.The next section briefly illustrates the basic ar-chitecture of a spoken dialogue system.
Section 3describes the problem to be solved in detail.
Thenafter introducing related work, our approach is de-scribed with an example dialogue.
After that, wedescribe the experiments we performed to verify ourapproach, and discuss the results.
The last sectionsummarizes the main points and mentions futurework.2 Discourse UnderstandingHere, we describe the basic architecture of a spokendialogue system (Figure 1).
When receiving a userutterance, the system behaves as follows.1.
The speech recognizer receives a user utteranceand outputs a speech recognition hypothesis.2.
The language understanding component re-ceives the speech recognition hypothesis.
Thesyntactic and semantic analysis is performedto convert it into a form called a dialogueact.
Table 1 shows an example of a dialogueact.
In the example, ?refer-start-and-end-time?is called the dialogue act type, which brieflydescribes the meaning of a dialogue act, and?start=14:00?
and ?end=15:00?
are add-on in-formation.11In general, a dialogue act corresponds to one sentence.However, in dialogues where user utterances are unrestricted,smaller units, such as phrases, can be regarded as dialogue acts.SpeechRecognizerLanguageUnderstandingComponentDiscourseUnderstandingComponentDialogueStateDialogueManagerSpeechSynthesizerUpdateUpdateReferReferSpeech RecognitionHypothesis Dialogue ActFigure 1: Architecture of a spoken dialogue system.3.
The discourse understanding component re-ceives the dialogue act, refers to the current di-alogue state, and updates the dialogue state.4.
The dialogue manager receives the current dia-logue state, decides the next utterance, and out-puts the next words to speak.
The dialogue stateis updated at the same time so that it containsthe content of system utterances.5.
The speech synthesizer receives the output ofthe dialogue manager and responds to the userby speech.This paper deals with the discourse understand-ing component.
Since we are resolving the ambi-guity of speech understanding from the discoursepoint of view and not within the speech understand-ing candidates, we assume that a dialogue state isuniquely determined given a dialogue state and thenext dialogue act, which means that a dialogue actis a command to change a dialogue state.
We alsoassume that the relationship between the dialogueact and the way to update the dialogue state can beeasily described without expertise in dialogue sys-tem research.
We found that these assumptions arereasonable from our experience in system develop-ment.
Note also that this paper does not separatelydeal with reference resolution; we assume that it isperformed by a command.
A speech understandingresult is considered to be equal to a dialogue act inthis article.In this paper, we consider frames as representa-tions of dialogue states.
To represent dialogue states,plans have often been used (Allen and Perrault,1980; Carberry, 1990).
Traditionally, plan-baseddiscourse understanding methods have been imple-mented mostly in keyboard-based dialogue systems,User Utterance ?from two p.m. to three p.m.?Dialogue Act [act-type=refer-start-and-end-time, start=14:00, end=15:00]Table 1: A user utterance and the corresponding di-alogue act.although there are some recent attempts to applythem to spoken dialogue systems as well (Allen etal., 2001; Rich et al, 2001); however, consideringthe current performance of speech recognizers andthe limitations in task domains, we believe frame-based discourse understanding and dialogue man-agement are sufficient (Chu-Carroll, 2000; Seneff,2002; Bobrow et al, 1977).3 ProblemMost conventional spoken dialogue systemsuniquely determine the dialogue state after a userutterance.
Normally, however, there are multiplecandidates for the result of speech understanding,which leads to the creation of multiple dialoguestate candidates.
We believe that there are caseswhere it is better to hold more than one dialoguestate and resolve the ambiguity as the dialogueprogresses rather than to decide on a single dialoguestate after each user utterance.As an example, consider a piece of dialogue inwhich the user utterance ?from two p.m.?
has beenmisrecognized as ?uh two p.m.?
(Figure 2).
Fig-ure 3 shows the description of the example dia-logue in detail including the system?s inner states,such as dialogue acts corresponding to the speechrecognition hypotheses2 and the intention recogni-tion results.3 After receiving the speech recogni-tion hypothesis ?uh two p.m.,?
the system cannottell whether the user utterance corresponds to a dia-logue act specifying the start time or the end time(da1,da2).
Therefore, the system tries to obtainfurther information about the time.
In this case,the system utters a backchannel to prompt the nextuser utterance to resolve the ambiguity from the dis-course.4 At this stage, the system holds two dialogue2In this example, for convenience of explanation, the n-bestspeech recognition input is not considered.3An intention recognition result is one of the elements of adialogue state.4A yes/no question may be an appropriate choice as well. S1 : what time would you like to reserve ameeting room?U1 : from two p.m. [uh two p.m.]S2 : uh-huhU2 : to three p.m. [to three p.m.]S3 : from two p.m. to three p.m.?U3 : yes [yes] Figure 2: Example dialogue.
(S means a system utterance and U a user utterance.Recognition results are enclosed in square brackets.
)states having different intention recognition results(ds1,ds2).
The next utterance, ?to three p.m.,?
isone that uniquely corresponds to a dialogue act spec-ifying the end time (da3), and thus updates the twocurrent dialogue states.
As a result, two dialoguestates still remain (ds3,ds4).
If the system can tellthat the previous dialogue act was about the starttime at this moment, it can understand the user in-tention correctly.
The correct understanding result,ds3, is derived from the combination of ds1 andda3, where ds1 is induced by ds0 and da1.
Asshown here, holding multiple understanding resultscan be better than just deciding on the best speechunderstanding hypothesis and discarding other pos-sibilities.In this paper, we consider a discourse understand-ing component that deals with multiple dialoguestates.
Such a component must choose the best com-bination of a dialogue state and a dialogue act out ofall possibilities.
An appropriate scoring method forthe dialogue states is therefore required.4 Related WorkNakano et al (1999) proposed a method that holdsmultiple dialogue states ordered by priority to dealwith the problem that some utterances convey mean-ing over several speech intervals and that the under-standing result cannot be determined at each inter-val end.
Miyazaki et al (2002) proposed a methodcombining Nakano et al?s (1999) method and n-bestrecognition hypotheses, and reported improvementin discourse understanding accuracy.
They used ametric similar to the concept error rate for the evalu-[System utterance (S1)]?What time would you like to reserve a meetingroom??
[Dialogue act] [act-type=ask-time][Intention recognition result candidates]1.
[room=nil, start=nil, end=nil] (ds0)?
[User utterance (U1)]?From two p.m.?
[Speech recognition hypotheses]1.
?uh two p.m.?
[Dialogue act candidates]1.
[act-type=refer-start-time,time=14:00] (da1)2.
[act-type=refer-end-time,time=15:00] (da2)[Intention recognition result candidates]1.
[room=nil, start=14:00, end=nil](ds1, induced from ds0 and da1)2.
[room=nil, start=nil, end=14:00](ds2, induced from ds0 and da2)?
[System utterance (S2)] ?uh-huh?
[Dialogue act] [act-type=backchannel]?
[User utterance (U2)]?To three p.m.?
[Speech recognition hypotheses]1.
?to three p.m.?
[Dialogue act candidates]1.
[act-type=refer-end-time, time=15:00] (da3)[Intention recognition result candidates]1.
[room=nil, start=14:00, end=15:00](ds3, induced from ds1 and da3)2.
[room=nil, start=nil, end=15:00](ds4, induced from ds2 and da3)?
[System utterance (S3)]?from two p.m. to three p.m.??
[Dialogue act][act-type=confirm-time,start=14:00, end=15:00]?
[User utterance (U3)] ?yes?
[Speech recognition hypotheses]1.
?yes?
[Dialogue act candidates]1.
[act-type=acknowledge][Intention recognition result candidates]1.
[room=nil, start=14:00, end=15:00]2.
[room=nil, start=nil, end=15:00]Figure 3: Detailed description of the understandingof the example dialogue.ation of discourse accuracy, comparing reference di-alogue states with hypothesis dialogue states.
Boththese methods employ hand-crafted rules to scorethe dialogue states to decide the best dialogue state.Creating such rules requires expert knowledge, andis also time consuming.There are approaches that propose statistically es-timating the dialogue act type from several previousdialogue act types using N-gram probability (Nagataand Morimoto, 1994; Reithinger and Maier, 1995).Although their approaches can be used for disam-biguating user utterance using discourse informa-tion, they do not consider holding multiple dialoguestates.In the context of plan-based utterance understand-ing (Allen and Perrault, 1980; Carberry, 1990),when there is ambiguity in the understanding re-sult of a user utterance, an interpretation best suitedto the estimated plan should be selected.
In ad-dition, the system must choose the most plausibleplans from multiple possible candidates.
Althoughwe do not adopt plan-based representation of dia-logue states as noted before, this problem is close towhat we are dealing with.
Unfortunately, however,it seems that no systematic ways to score the candi-dates for disambiguation have been proposed.5 ApproachThe discourse understanding method that we pro-pose takes the same approach as Miyazaki et al(2002).
However, our method is different in that,when ordering the multiple dialogue states, the sta-tistical information derived from the dialogue cor-pora is used.
We propose using two kinds of statisti-cal information:1. the probability of a dialogue act type sequence,and2.
the collocation probability of a dialogue stateand the next dialogue act.5.1 Statistical InformationProbability of a dialogue act type sequenceBased on the same idea as Nagata and Morimoto(1994) and Reithinger and Maier (1995), we use theprobability of a dialogue act type sequence, namely,the N-gram probability of dialogue act types.
Sys-tem utterances and the transcription of user utter-ances are both converted to dialogue acts using a di-alogue act conversion parser, then the N-gram prob-ability of the dialogue act types is calculated.# explanation1.
whether slots asked previously by the systemare changed2.
whether slots being confirmed are changed3.
whether slots already confirmed are changed4.
whether the dialogue act fills slots that do nothave values5.
whether the dialogue act tries changing slotsthat have values6.
when 5 is true, whether slot values are notchanged as a result7.
whether the dialogue act updates the initialdialogue state 5Table 2: Seven binary attributes to classify collo-cation patterns of a dialogue state and the next dia-logue act.Collocation probability of a dialogue state andthe next dialogue act From the dialogue corpora,dialogue states and the succeeding user utterancesare extracted.
Then, pairs comprising a dialoguestate and a dialogue act are created after convert-ing user utterances into dialogue acts.
Contrary tothe probability of sequential patterns of dialogue acttypes that represents a brief flow of a dialogue, thiscollocation information expresses a local detailedflow of a dialogue, such as dialogue state changescaused by the dialogue act.
The simple bigram ofdialogue states and dialogue acts is not sufficientdue to the complexity of the data that a dialoguestate possesses, which can cause data sparsenessproblems.
Therefore, we classify the ways that di-alogue states are changed by dialogue acts into 64classes characterized by seven binary attributes (Ta-ble 2) and compute the occurrence probability ofeach class in the corpora.
We assume that the un-derstanding result of the user intention contained ina dialogue state is expressed as a frame, which iscommon in many systems (Bobrow et al, 1977).
Aframe is a bundle of slots that consist of attribute-value pairs concerning a certain domain.5The first user utterance should be treated separately, be-cause the system?s initial utterance is an open question leadingto an unrestricted utterance of a user.5.2 Scoring of Dialogue ActsEach speech recognition hypothesis is converted toa dialogue act or acts.
When there are several di-alogue acts corresponding to a speech recognitionhypothesis, all possible dialogue acts are created asin Figure 3, where the utterance ?uh two p.m.?
pro-duces two dialogue act candidates.
Each dialogueact is given a score using its linguistic and acous-tic scores.
The linguistic score represents the gram-matical adequacy of a speech recognition hypothe-sis from which the dialogue act originates, and theacoustic score the acoustic reliability of a dialogueact.
Sometimes, there is a case that a dialogue acthas such a low acoustic or linguistic score and thatit is better to ignore the act.
We therefore create adialogue act called null act, and add this null act toour list of dialogue acts.
A null act is a dialogue actthat does not change the dialogue state at all.5.3 Scoring of Dialogue StatesSince the dialogue state is uniquely updated by a di-alogue act, if there are l dialogue acts derived fromspeech understanding and m dialogue states, m ?
lnew dialogue states are created.
In this case, we de-fine the score of a dialogue state St+1asSt+1= St+ ?
?
sact+ ?
?
sngram+ ?
?
scolwhere Stis the score of a dialogue state just beforethe update, sactthe score of a dialogue act, sngramthe score concerning the probability of a dialogueact type sequence, scolthe score concerning the col-location probability of dialogue states and dialogueacts, and ?, ?, and ?
are the weighting factors.5.4 Ordering of Dialogue StatesThe newly created dialogue states are ordered basedon the score.
The dialogue state that has the bestscore is regarded as the most probable one, and thesystem responds to the user by referring to it.
Themaximum number of dialogue states is needed inorder to drop low-score dialogue states and therebyperform the operation in real time.
This droppingprocess can be considered as a beam search in viewof the entire discourse process, thus we name themaximum number of dialogue states the dialoguestate beam width.6 Experiment6.1 Extracting Statistical Information from Di-alogue CorpusDialogue Corpus We analyzed a corpus of dia-logues between naive users and a Japanese spokendialogue system, which were collected in acousti-cally insulated booths.
The task domain was meet-ing room reservation.
Subjects were instructed toreserve a meeting room on a certain date from a cer-tain time to a certain time.
As a speech recognitionengine, Julius3.1p1 (Lee et al, 2001) was used withits attached acoustic model.
For the language model,we used a trigram trained from randomly generatedtexts of acceptable phrases.
For system response,NTT?s speech synthesis engine FinalFluet (Takanoet al, 2001) was used.
The system had a vocabularyof 168 words, each registered with a category anda semantic feature in its lexicon.
The system usedhand-crafted rules for discourse understanding.
Thecorpus consists of 240 dialogues from 15 subjects(10 males and 5 females), each one performing 16dialogues.
Dialogues that took more than three min-utes were regarded as failures.
The task completionrate was 78.3% (188/240).Extraction of Statistical Information From thetranscription, we created a trigram of dialogue acttypes using the CMU-Cambridge Toolkit (Clarksonand Rosenfeld, 1997).
Figure 3 shows an exampleof the trigram information starting from {refer-start-time backchannel}.
The bigram information usedfor smoothing is also shown.
The collocation proba-bility was obtained from the recorded dialogue statesand the transcription following them.
Out of 64 pos-sible patterns, we found 17 in the corpus as shown inFigure 4.
Taking the case of the example dialogue inFigure 3, it happened that the sequence {refer-start-time backchannel refer-end-time} does not appear inthe corpus; thus, the probability is calculated basedon the bigram probability using the backoff weight,which is 0.006.
The trigram probability for {refer-end-time backchannel refer-end-time} is 0.031.The collocation probability of the sequence ds1+ da3 ?
ds3 fits collocation pattern 12, where aslot having no value was changed.
The sequenceds2 + da3 ?
ds4 fits collocation pattern 17, wherea slot having a value was changed to have a differ-ent value.
The probabilities were 0.155 and 0.009,dialogue act type sequence (trigram) probabilityscorerefer-start-time backchannel backchannel -1.0852refer-start-time backchannel ask-date -2.0445refer-start-time backchannel ask-start-time -0.8633refer-start-time backchannel request -2.0445refer-start-time backchannel refer-day -1.7790refer-start-time backchannel refer-month -0.4009refer-start-time backchannel refer-room -0.8633refer-start-time backchannel refer-start-time -0.7172dialogue act type sequence(bigram)backoffweightprobabilityscorerefer-start-time backchannel -1.1337 -0.7928refer-end-time backchannel 0.4570 -0.6450backchannel refer-end-time -0.5567 -1.0716Table 3: An example of bigram and trigram of dia-logue act types with their probability score in com-mon logarithm.collocation occurrence# pattern probability1.
0 1 1 1 0 0 1 0.0012.
0 1 1 0 0 1 0 0.0533.
0 0 0 0 0 0 0 0.2734.
1 0 0 0 1 0 0 0.0015.
1 0 1 1 0 0 0 0.0056.
0 0 1 1 0 0 0 0.0367.
0 0 0 0 1 0 0 0.0478.
0 1 1 0 1 0 0 0.0419.
0 0 1 1 0 0 1 0.01010.
0 0 1 0 0 1 0 0.01611.
0 0 0 0 0 0 1 0.06412.
0 0 0 1 0 0 0 0.15513.
1 0 0 1 0 0 0 0.04314.
0 0 1 0 1 0 0 0.06115.
1 0 0 1 0 0 1 0.00116.
0 0 0 1 0 0 1 0.18617.
0 0 0 0 0 1 0 0.009Table 4: The 17 collocation patterns and their oc-currence probabilities.
See Figure 2 for the detailof binary attributes.
Attributes 1-7 are ordered fromleft to right.respectively.
By the simple adding of the two proba-bilities in common logarithms in each case, ds3 hasthe probability score -3.015 and ds4 -3.549, sug-gesting that the sequence ds3 is the most probablediscourse understanding result after U2.6.2 Verification of our approachTo verify the effectiveness of the proposed ap-proach, we built a Japanese spoken dialogue systemin the meeting reservation domain that employs theproposed discourse understanding method and per-formed dialogue experiments.The speech recognition engine was Julius3.3p1(Lee et al, 2001) with its attached acoustic models.For the language model, we made a trigram fromthe transcription obtained from the corpora.
Thesystem had a vocabulary of 243.
The recognitionengine outputs 5-best recognition hypotheses.
Thistime, values for sact, sngram, scolare the logarithmof the inverse number of n-best ranks,6 the log like-lihood of dialogue act type trigram probability, andthe common logarithm of the collocation probabil-ity, respectively.
For the experiment, weighting fac-tors are all set to one (?
= ?
= ?
= 1).
The di-alogue state beam width was 15.
We collected 256dialogues from 16 subjects (7 males and 9 females).The speech recognition accuracy (word error rate)was 65.18%.
Dialogues that took more than fiveminutes were regarded as failures.
The task com-pletion rate was 88.3% (226/256).7From all user speech intervals, the number oftimes that dialogue states below second place be-came first place was 120 (7.68%), showing a relativefrequency of shuffling within the dialogue states.6.3 Effectiveness of Holding Multiple DialogueStatesThe main reason that we developed the proposedcorpus-based discourse understanding method wasthat it is difficult to manually create rules to dealwith multiple dialogue states.
It is yet to be exam-ined, however, whether holding multiple dialoguestates is really effective for accurate discourse un-derstanding.To verify that holding multiple dialogue states iseffective, we fixed the speech recognizer?s output to1-best, and studied the system performance changeswhen the dialogue state beam width was changedfrom 1 to 30.
When the dialogue state beam width istoo large, the computational cost becomes high andthe system cannot respond in real time.
We thereforeselected 30 for empirical reasons.The task domain and other settings were the same6In this experiment, only the acoustic score of a dialogue actwas considered.7It should be noted that due to the creation of an enormousnumber of dialogue states in discourse understanding, the pro-posed system takes a few seconds to respond after the user in-put.as in the previous experiment except for the dialoguestate beam width changes.
We collected 448 dia-logues from 28 subjects (4 males and 24 females),each one performing 16 dialogues.
Each subject wasinstructed to reserve the same meeting room twice,once with the 1-beam-width system and again with30-beam-width system.
The order of what room toreserve and what system to use was randomized.The speech recognition accuracy was 69.17%.
Di-alogues that took more than five minutes were re-garded as failures.
The task completion rates for the1-beam-width system and the 30-beam-width sys-tem were 88.3% and 91.0%, and the average taskcompletion times were 107.66 seconds and 95.86seconds, respectively.
A statistical hypothesis testshowed that times taken to carry out a task with the30-beam-width system are significantly shorter thanthose with the 1-beam-width system (Z = ?2.01,p < .05).
In this test, we used a kind of censoredmean computed by taking the mean of the timesonly for subjects that completed the tasks with bothsystems.
The population distribution was estimatedby the bootstrap method (Cohen, 1995).
It may bepossible to evaluate the discourse understanding bycomparing the best dialogue state with the referencedialogue state, and calculate a metric such as theCER (concept error rate) as Miyazaki et al (2002)do; however it is not clear whether the discourseunderstanding can be evaluated this way, since it isnot certain whether the CER correlates closely withthe system?s performance (Higashinaka et al, 2002).Therefore, this time, we used the task completiontime and the task completion rate for comparison.7 DiscussionCost of creating the discourse understandingcomponent The best task completion rate in the ex-periments was 91.0% (the case of 1-best recognitioninput and a 30 dialogue state beam width).
This highrate suggests that the proposed approach is effectivein reducing the cost of creating the discourse un-derstanding component in that no hand-crafted rulesare necessary.
For statistical discourse understand-ing, an initial system, e.g., a system that employsthe proposed approach with only sactfor scoring thedialogue states, is needed in order to create the di-alogue corpus; however, once it has been made, thecreation of the discourse understanding componentrequires no expert knowledge.Effectiveness of holding multiple dialogue statesThe result of the examination of dialogue state beamwidth changes suggests that holding multiple dia-logue states shortens the task completion time.
Asfar as task-oriented spoken dialogue systems areconcerned, holding multiple dialogue states con-tributes to the accuracy of discourse understanding.8 Summary and Future WorkWe proposed a new discourse understanding methodthat orders multiple dialogue states created frommultiple dialogue states and the succeeding speechunderstanding results based on statistical informa-tion obtained from dialogue corpora.
The results ofthe experiments show that our approach is effectivein reducing the cost of creating the discourse under-standing component, and the advantage of keepingmultiple dialogue states was also shown.There still remain several issues that we need toexplore.
These include the use of statistical informa-tion other than the probability of a dialogue act typesequence and the collocation probability of dialoguestates and dialogue acts, the optimization of weight-ing factors ?, ?, ?, other default parameters that weused in the experiments, and more experiments inlarger domains.
Despite these issues, the present re-sults have shown that our approach is promising.AcknowledgementsWe thank Dr. Hiroshi Murase and all members of theDialogue Understanding Research Group for usefuldiscussions.
Thanks also go to the anonymous re-viewers for their helpful comments.ReferencesJames F. Allen and C. Raymond Perrault.
1980.
Analyz-ing intention in utterances.
Artif.
Intel., 15:143?178.James Allen, George Ferguson, and Amanda Stent.
2001.An architecture for more realistic conversational sys-tems.
In Proc.
IUI, pages 1?8.Daniel G. Bobrow, Ronald M. Kaplan, Martin Kay, Don-ald A. Norman, Henry Thompson, and Terry Wino-grad.
1977.
GUS, a frame driven dialog system.
Artif.Intel., 8:155?173.Sandra Carberry.
1990.
Plan Recognition in NaturalLanguage Dialogue.
MIT Press, Cambridge, Mass.Junnifer Chu-Carroll.
2000.
MIMIC: An adaptivemixed initiative spoken dialogue system for informa-tion queries.
In Proc.
6th Applied NLP, pages 97?104.P.R.
Clarkson and R. Rosenfeld.
1997.
Statistical lan-guagemodeling using the CMU-Cambridge toolkit.
InProc.
Eurospeech, pages 2707?2710.Paul R. Cohen.
1995.
Empirical Methods for ArtificialIntelligence.
MIT Press.Ryuichiro Higashinaka, Noboru Miyazaki, MikioNakano, and Kiyoaki Aikawa.
2002.
A methodfor evaluating incremental utterance understandingin spoken dialogue systems.
In Proc.
ICSLP, pages829?832.Akinobu Lee, Tatsuya Kawahara, and Kiyohiro Shikano.2001.
Julius ?
an open source real-time large vocab-ulary recognition engine.
In Proc.
Eurospeech, pages1691?1694.Noboru Miyazaki, Mikio Nakano, and Kiyoaki Aikawa.2002.
Robust speech understanding using incremen-tal understanding with n-best recognition hypothe-ses.
In SIG-SLP-40, Information Processing Societyof Japan., pages 121?126.
(in Japanese).Masaaki Nagata and Tsuyoshi Morimoto.
1994.
Firststeps toward statistical modeling of dialogue to predictthe speech act type of the next utterance.
Speech Com-munication, 15:193?203.Mikio Nakano, Noboru Miyazaki, Jun-ichi Hirasawa,Kohji Dohsaka, and Takeshi Kawabata.
1999.
Un-derstanding unsegmented user utterances in real-timespoken dialogue systems.
In Proc.
37th ACL, pages200?207.Norbert Reithinger and Elisabeth Maier.
1995.
Utiliz-ing statistical dialogue act processing in Verbmobil.
InProc.
33th ACL, pages 116?121.Charles Rich, Candace Sidner, and Neal Lesh.
2001.COLLAGEN: Applying collaborative discourse the-ory.
AI Magazine, 22(4):15?25.Stephanie Seneff.
2002.
Response planning and genera-tion in the MERCURY flight reservation system.
Com-puter Speech and Language, 16(3?4):283?312.Satoshi Takano, Kimihito Tanaka, Hideyuki Mizuno,Masanobu Abe, and ShiN?ya Nakajima.
2001.
AJapanese TTS system based on multi-form units and aspeech modification algorithm with harmonics recon-struction.
IEEE Transactions on Speech and Process-ing, 9(1):3?10.Victor W. Zue and James R. Glass.
2000.
Conversationalinterfaces: Advances and challenges.
Proceedings ofIEEE, 88(8):1166?1180.
