Proceedings of the NAACL HLT Workshop on Integer Linear Programming for Natural Language Processing, pages 19?27,Boulder, Colorado, June 2009. c?2009 Association for Computational LinguisticsBounding and Comparing Methods for Correlation Clustering Beyond ILPMicha Elsner and Warren SchudyDepartment of Computer ScienceBrown UniversityProvidence, RI 02912{melsner,ws}@cs.brown.eduAbstractWe evaluate several heuristic solvers for corre-lation clustering, the NP-hard problem of par-titioning a dataset given pairwise affinities be-tween all points.
We experiment on two prac-tical tasks, document clustering and chat dis-entanglement, to which ILP does not scale.On these datasets, we show that the cluster-ing objective often, but not always, correlateswith external metrics, and that local search al-ways improves over greedy solutions.
We usesemi-definite programming (SDP) to provide atighter bound, showing that simple algorithmsare already close to optimality.1 IntroductionCorrelation clustering is a powerful technique fordiscovering structure in data.
It operates on thepairwise relationships between datapoints, partition-ing the graph to minimize the number of unrelatedpairs that are clustered together, plus the numberof related pairs that are separated.
Unfortunately,this minimization problem is NP-hard (Ailon et al,2008).
Practical work has adopted one of threestrategies for solving it.
For a few specific tasks, onecan restrict the problem so that it is efficiently solv-able.
In most cases, however, this is impossible.
In-teger linear programming (ILP) can be used to solvethe general problem optimally, but only when thenumber of data points is small.
Beyond a few hun-dred points, the only available solutions are heuristicor approximate.In this paper, we evaluate a variety of solu-tions for correlation clustering on two realistic NLPtasks, text topic clustering and chat disentangle-ment, where typical datasets are too large for ILPto find a solution.
We show, as in previous workon consensus clustering (Goder and Filkov, 2008),that local search can improve the solutions found bycommonly-used methods.
We investigate the rela-tionship between the clustering objective and exter-nal evaluation metrics such as F-score and one-to-one overlap, showing that optimizing the objectiveis usually a reasonable aim, but that other measure-ments like number of clusters found should some-times be used to reject pathological solutions.
Weprove that the best heuristics are quite close to op-timal, using the first implementation of the semi-definite programming (SDP) relaxation to providetighter bounds.The specific algorithms we investigate are, ofcourse, only a subset of the large number of pos-sible solutions, or even of those proposed in the lit-erature.
We chose to test a few common, efficientalgorithms that are easily implemented.
Our use ofa good bounding strategy means that we do not needto perform an exhaustive comparison; we will showthat, though the methods we describe are not per-fect, the remaining improvements possible with anyalgorithm are relatively small.2 Previous WorkCorrelation clustering was first introduced by Ben-Dor et al (1999) to cluster gene expression pat-terns.
The correlation clustering approach has sev-eral strengths.
It does not require users to specifya parametric form for the clusters, nor to pick thenumber of clusters.
Unlike fully unsupervised clus-19tering methods, it can use training data to optimizethe pairwise classifier, but unlike classification, itdoes not require samples from the specific clustersfound in the test data.
For instance, it can use mes-sages about cars to learn a similarity function thatcan then be applied to messages about atheism.Correlation clustering is a standard method forcoreference resolution.
It was introduced to thearea by Soon et al (2001), who describe the first-link heuristic method for solving it.
Ng and Cardie(2002) extend this work with better features, and de-velop the best-link heuristic, which finds better solu-tions.
McCallum and Wellner (2004) explicitly de-scribe the problem as correlation clustering and usean approximate technique (Bansal et al, 2004) toenforce transitivity.
Recently Finkel and Manning(2008) show that the optimal ILP solution outper-forms the first and best-link methods.
Cohen andRichman (2002) experiment with various heuristicsolutions for the cross-document coreference task ofgrouping references to named entities.Finally, correlation clustering has proven useful inseveral discourse tasks.
Barzilay and Lapata (2006)use it for content aggregation in a generation system.In Malioutov and Barzilay (2006), it is used for topicsegmentation?since segments must be contiguous,the problem can be solved in polynomial time.
El-sner and Charniak (2008) address the related prob-lem of disentanglement (which we explore in Sec-tion 5.3), doing inference with the voting greedy al-gorithm.Bertolacci and Wirth (2007), Goder and Filkov(2008) and Gionis et al (2007) conduct experimentson the closely related problem of consensus cluster-ing, often solved by reduction to correlation cluster-ing.
The input to this problem is a set of clusterings;the output is a ?median?
clustering which minimizesthe sum of (Rand) distance to the inputs.
Althoughthese papers investigate some of the same algorithmswe use, they use an unrealistic lower bound, and socannot convincingly evaluate absolute performance.Gionis et al (2007) give an external evaluation onsome UCI datasets, but this is somewhat unconvinc-ing since their metric, the impurity index, which isessentially precision ignoring recall, gives a perfectscore to the all-singletons clustering.
The other twopapers are based on objective values, not externalmetrics.1A variety of approximation algorithms for corre-lation clustering with worst-case theoretical guar-antees have been proposed: (Bansal et al, 2004;Ailon et al, 2008; Demaine et al, 2006; Charikaret al, 2005; Giotis and Guruswami, 2006).
Re-searchers including (Ben-Dor et al, 1999; Joachimsand Hopcroft, 2005; Mathieu and Schudy, 2008)study correlation clustering theoretically when theinput is generated by randomly perturbing an un-known ground truth clustering.3 AlgorithmsWe begin with some notation and a formal definitionof the problem.
Our input is a complete, undirectedgraph G with n nodes; each edge in the graph hasa probability pij reflecting our belief as to whethernodes i and j come from the same cluster.
Our goalis to find a clustering, defined as a new graph G?with edges xij ?
{0, 1}, where if xij = 1, nodesi and j are assigned to the same cluster.
To makethis consistent, the edges must define an equivalencerelationship: xii = 1 and xij = xjk = 1 impliesxij = xik.Our objective is to find a clustering as consistentas possible with our beliefs?edges with high proba-bility should not cross cluster boundaries, and edgeswith low probability should.
We define w+ij as thecost of cutting an edge whose probability is pij andw?ij as the cost of keeping it.
Mathematically, thisobjective can be written (Ailon et al, 2008; Finkeland Manning, 2008) as:min ?ij:i<jxijw?ij + (1?
xij)w+ij .
(1)There are two plausible definitions for the costs w+and w?, both of which have gained some support inthe literature.
We can take w+ij = pij and w?ij =1 ?
pij (additive weights) as in (Ailon et al, 2008)and others, or w+ij = log(pij), w?ij = log(1 ?
pij)(logarithmic weights) as in (Finkel and Manning,2008).
The logarithmic scheme has a tenuous math-ematical justification, since it selects a maximum-likelihood clustering under the assumption that the1Bertolacci and Wirth (2007) gave normalized mutual infor-mation for one algorithm and data set, but almost all of theirresults study objective value only.20pij are independent and identically distributed giventhe status of the edge ij in the true clustering.
Ifwe obtain the pij using a classifier, however, this as-sumption is obviously untrue?some nodes will beeasy to link, while others will be hard?so we eval-uate the different weighting schemes empirically.3.1 Greedy MethodsWe use four greedy methods drawn from the lit-erature; they are both fast and easy to implement.All of them make decisions based on the net weightw?ij = w+ij ?w?ij .These algorithms step through the nodes of thegraph according to a permutation pi.
We try 100 ran-dom permutations for each algorithm and report therun which attains the best objective value (typicallythis is slightly better than the average run; we dis-cuss this more in the experimental sections).
To sim-plify the pseudocode we label the vertices 1, 2, .
.
.
nin the order specified by pi.
After this relabelingpi(i) = i so pi need not appear explicitly in the al-gorithms.Three of the algorithms are given in Figure 1.
Allthree algorithms start with the empty clustering andadd the vertices one by one.
The BEST algorithmadds each vertex i to the cluster with the strongestw?
connecting to i, or to a new singleton if none ofthe w?
are positive.
The FIRST algorithm adds eachvertex i to the cluster containing the most recentlyconsidered vertex j with w?ij > 0.
The VOTE algo-rithm adds each vertex to the cluster that minimizesthe correlation clustering objective, i.e.
to the clustermaximizing the total net weight or to a singleton ifno total is positive.Ailon et al (2008) introduced the PIVOT algo-rithm, given in Figure 2, and proved that it is a 5-approximation if w+ij + w?ij = 1 for all i, j andpi is chosen randomly.
Unlike BEST, VOTE andFIRST, which build clusters vertex by vertex, thePIVOT algorithm creates each new cluster in its fi-nal form.
This algorithm repeatedly takes an unclus-tered pivot vertex and creates a new cluster contain-ing that vertex and all unclustered neighbors withpositive weight.3.2 Local SearchWe use the straightforward local search previouslyused by Gionis et al (2007) and Goder and Filkovk ?
0 // number of clusters created so farfor i = 1 .
.
.
n dofor c = 1 .
.
.
k doif BEST thenQualityc ?
maxj?C[c] w?ijelse if FIRST thenQualityc ?
maxj?C[c]:w?ij>0 jelse if VOTE thenQualityc ?
?j?C[c] w?ijc?
?
argmax1?c?k Qualitycif Qualityc?
> 0 thenC[c?]?
C[c?]
?
{i}elseC[k++]?
{i} // form a new clusterFigure 1: BEST/FIRST/VOTE algorithmsk ?
0 // number of clusters created so farfor i = 1 .
.
.
n doP ?
?1?c?k C[c] // Vertices already placedif i 6?
P thenC[k++] ?
{i} ?
{ i < j ?
n :j 6?
P and w?ij > 0 }Figure 2: PIVOT algorithm by Ailon et al (2008)(2008).
The allowed one element moves consistof removing one vertex from a cluster and eithermoving it to another cluster or to a new singletoncluster.
The best one element move (BOEM) al-gorithm repeatedly makes the most profitable bestone element move until a local optimum is reached.Simulated Annealing (SA) makes a random single-element move, with probability related to the dif-ference in objective it causes and the current tem-perature.
Our annealing schedule is exponential anddesigned to attempt 2000n moves for n nodes.
Weinitialize the local search either with all nodes clus-tered together, or at the clustering produced by oneof our greedy algorithms (in our tables, the latter iswritten, eg.
PIVOT/BOEM, if the greedy algorithmis PIVOT).4 Bounding with SDPAlthough comparing different algorithms to one an-other gives a good picture of relative performance, itis natural to wonder how well they do in an absolutesense?how they compare to the optimal solution.21For very small instances, we can actually find theoptimum using ILP, but since this does not scale be-yond a few hundred points (see Section 5.1), for re-alistic instances we must instead bound the optimalvalue.
Bounds are usually obtained by solving a re-laxation of the original problem: a simpler problemwith the same objective but fewer constraints.The bound used in previous work (Goder andFilkov, 2008; Gionis et al, 2007; Bertolacci andWirth, 2007), which we call the trivial bound, isobtained by ignoring the transitivity constraints en-tirely.
To optimize, we link (xij = 1) all the pairswhere w+ij is larger than w?ij ; since this solution isquite far from being a clustering, the bound tendsnot to be very tight.To get a better idea of how good a real clusteringcan be, we use a semi-definite programming (SDP)relaxation to provide a better bound.
Here we moti-vate and define this relaxation.One can picture a clustering geometrically by as-sociating cluster c with the standard basis vectorec = (0, 0, .
.
.
, 0,?
??
?c?11, 0, .
.
.
, 0?
??
?n?c) ?
Rn.
If object i isin cluster c then it is natural to associate i with thevector ri = ec.
This gives a nice geometric pictureof a clustering, with objects i and j in the same clus-ter if and only if ri = rj .
Note that the dot productri ?
rj is 1 if i and j are in the same cluster and 0otherwise.
These ideas yield a simple reformulationof the correlation clustering problem:minr?i,j:i<j(ri ?
rj)w?ij + (1?
rj ?
rj)w+ijs.t.
?i ?c : ri = ecTo get an efficiently computable lower-bound werelax the constraints that the ris are standard basisvectors, replacing them with two sets of constraints:ri ?
ri = 1 for all i and ri ?
rj ?
0 for all i, j.Since the ri only appear as dot products, we canrewrite in terms of xij = ri ?
rj .
However, wemust now constrain the xij to be the dot productsof some set of vectors in Rn.
This is true if andonly if the symmetric matrix X = {xij}ij is posi-tive semi-definite.
We now have the standard semi-definite programming (SDP) relaxation of correla-tion clustering (e.g.
(Charikar et al, 2005; Mathieuand Schudy, 2008)):minx?i,j:i<j xijw?ij + (1?
xij)w+ijs.t.??
?xii = 1 ?ixij ?
0 ?i, jX = {xij}ij PSD.This SDP has been studied theoretically by anumber of authors; we mention just two here.Charikar et al (2005) give an approximation al-gorithm based on rounding the SDP which is a0.7664 approximation for the problem of maximiz-ing agreements.
Mathieu and Schudy (2008) showthat if the input is generated by corrupting theedges of a ground truth clustering B independently,then the SDP relaxation value is within an additiveO(n?n) of the optimum clustering.
They furthershow that using the PIVOT algorithm to round theSDP yields a clustering with value at most O(n?n)more than optimal.5 Experiments5.1 ScalabilityUsing synthetic data, we investigate the scalabilityof the linear programming solver and SDP bound.To find optimal solutions, we pass the complete ILP2to CPLEX.
This is reasonable for 100 points andsolvable for 200; beyond this point it cannot besolved due to memory exhaustion.
As noted below,despite our inability to compute the LP bound onlarge instances, we can sometimes prove that theymust be worse than SDP bounds, so we do not in-vestigate LP-solving techniques further.The SDP has fewer constraints than the ILP(O(n2) vs O(n3)), but this is still more than manySDP solvers can handle.
For our experiments weused one of the few SDP solvers that can handle sucha large number of constraints: Christoph Helmberg?sConicBundle library (Helmberg, 2009; Helmberg,2000).
This solver can handle several thousand data-points.
It produces loose lower-bounds (off by a fewpercent) quickly but converges to optimality quiteslowly; we err on the side of inefficiency by run-ning for up to 60 hours.
Of course, the SDP solveris only necessary to bound algorithm performance;our solvers themselves scale much better.2Consisting of the objective plus constraints 0 ?
xij ?
1and triangle inequality (Ailon et al, 2008).225.2 Twenty NewsgroupsIn this section, we test our approach on a typi-cal benchmark clustering dataset, 20 Newsgroups,which contains posts from a variety of Usenetnewsgroups such as rec.motorcycles andalt.atheism.
Since our bounding techniquedoes not scale to the full dataset, we restrict our at-tention to a subsample of 100 messages3 from eachnewsgroup for a total of 2000?still a realisticallylarge-scale problem.
Our goal is to cluster messagesby their newsgroup of origin.
We conduct exper-iments by holding out four newsgroups as a train-ing set, learning a pairwise classifier, and applying itto the remaining 16 newsgroups to form our affinitymatrix.4Our pairwise classifier uses three types of fea-tures previously found useful in document cluster-ing.
First, we bucket al words5 by their log doc-ument frequency (for an overview of TF-IDF see(Joachims, 1997)).
For a pair of messages, we createa feature for each bucket whose value is the propor-tion of shared words in that bucket.
Secondly, werun LSA (Deerwester et al, 1990) on the TF-IDFmatrix for the dataset, and use the cosine distancebetween each message pair as a feature.
Finally, weuse the same type of shared words features for termsin message subjects.
We make a training instance foreach pair of documents in the training set and learnvia logistic regression.The classifier has an average F-score of 29% andan accuracy of 88%?not particularly good.
Weshould emphasize that the clustering task for 20newsgroups is much harder than the more com-mon classification task?since our training set is en-tirely disjoint with the testing set, we can only learnweights on feature categories, not term weights.
Ouraim is to create realistic-looking data on which totest our clustering methods, not to motivate correla-tion clustering as a solution to this specific problem.In fact, Zhong and Ghosh (2003) report better resultsusing generative models.We evaluate our clusterings using three different3Available as mini newsgroups.tar.gz from the UCImachine learning repository.4The experiments below are averaged over four disjointtraining sets.5We omit the message header, except the subject line, andalso discard word types with fewer than 3 occurrences.Logarithmic WeightsObj Rand F 1-1SDP bound 51.1% - - -VOTE/BOEM 55.8% 93.80 33 41SA 56.3% 93.56 31 36PIVOT/BOEM 56.6% 93.63 32 39BEST/BOEM 57.6% 93.57 31 38FIRST/BOEM 57.9% 93.65 30 36VOTE 59.0% 93.41 29 35BOEM 60.1% 93.51 30 35PIVOT 100% 90.85 17 27BEST 138% 87.11 20 29FIRST 619% 40.97 11 8Additive WeightsObj Rand F 1-1SDP bound 59.0% - - -SA 63.5% 93.75 32 39VOTE/BOEM 63.5% 93.75 32 39PIVOT/BOEM 63.7% 93.70 32 39BEST/BOEM 63.8% 93.73 31 39FIRST/BOEM 63.9% 93.58 31 37BOEM 64.6% 93.65 31 37VOTE 67.3% 93.35 28 34PIVOT 109% 90.63 17 26BEST 165% 87.06 20 29FIRST 761% 40.46 11 8Table 1: Score of the solution with best objective for eachsolver, averaged over newsgroups training sets, sorted byobjective.metrics (see Meila (2007) for an overview of cluster-ing metrics).
The Rand measure counts the numberof pairs of points for which the proposed clusteringagrees with ground truth.
This is the metric whichis mathematically closest to the objective.
However,since most points are in different clusters, any so-lution with small clusters tends to get a high score.Therefore we also report the more sensitive F-scorewith respect to the minority (?same cluster?)
class.We also report the one-to-one score, which mea-sures accuracy over single points.
For this metric,we calculate a maximum-weight matching betweenproposed clusters and ground-truth clusters, then re-port the overlap between the two.When presenting objective values, we locate themwithin the range between the trivial lower bound dis-23cussed in Section 4 and the objective value of thesingletons clustering (xij = 0, i 6= j).
On this scale,lower is better; 0% corresponds to the trivial boundand 100% corresponds to the singletons clustering.It is possible to find values greater than 100%, sincesome particularly bad clusterings have objectivesworse than the singletons clustering.
Plainly, how-ever, real clusterings will not have values as low as0%, since the trivial bound is so unrealistic.Our results are shown in Table 1.
The best re-sults are obtained using logarithmic weights withVOTE followed by BOEM; reasonable results arealso found using additive weights, and annealing,VOTE or PIVOT followed by BOEM.
On its own,the best greedy scheme is VOTE, but all of them aresubstantially improved by BOEM.
First-link is byfar the worst.
Our use of the SDP lower bound ratherthan the trivial lower-bound of 0% reduces the gapbetween the best clustering and the lower bound byover a factor of ten.
It is easy to show that the LPrelaxation can obtain a bound of at most 50%6?theSDP beats the LP in both runtime and quality!We analyze the correlation between objective val-ues and metric values, averaging Kendall?s tau7 overthe four datasets (Table 2).
Over the entire dataset,correlations are generally good (large and negative),showing that optimizing the objective is indeed auseful way to find good results.
We also examinecorrelations for the solutions with objective valueswithin the top 10%.
Here the correlation is muchpoorer; selecting the solution with the best objectivevalue will not necessarily optimize the metric, al-though the correspondence is slightly better for thelog-weights scheme.
The correlations do exist, how-ever, and so the solution with the best objective valueis typically slightly better than the median.In Figure 3, we show the distribution of one-to-one scores obtained (for one specific dataset) by thebest solvers.
From this diagram, it is clear that log-weights and VOTE/BOEM usually obtain the bestscores for this metric, since the median is higherthan other solvers?
upper quartile scores.
All solvershave quite high variance, with a range of about 2%between quartiles and 4% overall.
We omit the F-6The solution xij = 121`w?ij > w+ij?for i < j is feasiblein the LP.7The standard Pearson correlation coefficient is less robustto outliers, which causes problems for this data.BOEMbest/BLbest/Bfirst/Bpivot/BLpivot/BSAL-SAvote/BLvote/B0.320.340.360.380.400.420.44Figure 3: Box-and-whisker diagram (outliers as +) forone-to-one scores obtained by the best few solvers on aparticular newsgroup dataset.
L means using log weights.B means improved with BOEM.Rand F 1-1Log-wt -.60 -.73 -.71Top 10 % -.14 -.22 -.24Add-wt -.60 -.67 -.65Top 10 % -.13 -.15 -.14Table 2: Kendall?s tau correlation between objective andmetric values, averaged over newsgroup datasets, for allsolutions and top 10% of solutions.score plot, which is similar, for space reasons.5.3 Chat DisentanglementIn the disentanglement task, we examine data from ashared discussion group where many conversationsare occurring simultaneously.
The task is to partitionthe utterances into a set of conversations.
This taskdiffers from newsgroup clustering in that data points(utterances) have an inherent linear order.
Orderingis typical in discourse tasks including topic segmen-tation and coreference resolution.We use the annotated dataset and pairwise classi-24fier made available by Elsner and Charniak (2008);8this study represents a competitive baseline, al-though more recently Wang and Oard (2009) haveimproved it.
Since this classifier is ineffective atlinking utterances more than 129 seconds apart, wetreat all decisions for such utterances as abstentions,p = .5.
For utterance pairs on which it does makea decision, the classifier has a reported accuracy of75% with an F-score of 71%.As in previous work, we run experiments on the800-utterance test set and average metrics over 6 testannotations.
We evaluate using the three metrics re-ported by previous work.
Two node-counting met-rics measure global accuracy: one-to-one match asexplained above, and Shen?s F (Shen et al, 2006):F = ?i nin maxj(F (i, j)).
Here i is a gold con-versation with size ni and j is a proposed conver-sation with size nj , sharing nij utterances; F (i, j)is the harmonic mean of precision (nijnj ) and recall(nijni ).
A third metric, the local agreement, countsedgewise agreement for pairs of nearby utterances,where nearby means ?within three utterances.
?In this dataset, the SDP is a more moderate im-provement over the trivial lower bound, reducingthe gap between the best clustering and best lowerbound by a factor of about 3 (Table 3).Optimization of the objective does not correspondto improvements in the global metrics (Table 3);for instance, the best objectives are attained withFIRST/BOEM, but VOTE/BOEM yields better one-to-one and F scores.
Correlation between the ob-jective and these global metrics is extremely weak(Table 5).
The local metric is somewhat correlated.Local search does improve metric results for eachparticular greedy algorithm.
For instance, whenBOEM is added to VOTE (with log weights), one-to-one increases from 44% to 46%, local from 72%to 73% and F from 48% to 50%.
This represents amoderate improvement on the inference scheme de-scribed in Elsner and Charniak (2008).
They usevoting with additive weights, but rather than per-forming multiple runs over random permutations,they process utterances in the order they occur.
(Weexperimented with processing in order; the resultsare unclear, but there is a slight trend toward worseperformance, as in this case.)
Their results (also8Downloaded from cs.brown.edu/?melsnershown in the table) are 41% one-to-one, 73% localand .44% F-score.9 Our improvement on the globalmetrics (12% relative improvement in one-to-one,13% in F-score) is modest, but was achieved withbetter inference on exactly the same input.Since the objective function fails to distinguishgood solutions from bad ones, we examine the typesof solutions found by different methods in the hopeof explaining why some perform better than others.In this setting, some methods (notably local searchrun on its own or from a poor starting point) find farfewer clusters than others (Table 4; log weights notshown but similar to additive).
Since the classifierabstains for utterances more than 129 seconds apart,the objective is unaffected if very distant utterancesare linked on the basis of little or no evidence; thisis presumably how such large clusters form.
(Thisraises the question of whether abstentions shouldbe given weaker links with p < .5.
We leave thisfor future work.)
Algorithms which find reasonablenumbers of clusters (VOTE, PIVOT, BEST and lo-cal searches based on these) all achieve good metricscores, although there is still no reliable way to findthe best solution among this set of methods.6 ConclusionsIt is clear from these results that heuristic methodscan provide good correlation clustering solutions ondatasets far too large for ILP to scale.
The particularsolver chosen10 has a substantial impact on the qual-ity of results obtained, in terms of external metricsas well as objective value.For general problems, our recommendation is touse log weights and run VOTE/BOEM.
This algo-rithm is fast, achieves good objective values, andyields good metric scores on our datasets.
Althoughobjective values are usually only weakly correlatedwith metrics, our results suggest that slightly bet-ter scores can be obtained by running the algorithmmany times and returning the solution with the bestobjective.
This may be worth trying even when thedatapoints are inherently ordered, as in chat.9The F-score metric is not used in Elsner and Charniak(2008); we compute it ourselves on the result produced by theirsoftware.10Our C++ correlation clustering software and SDPbounding package are available for download fromcs.brown.edu/?melsner.25Log WeightsObj 1-1 Loc3 Shen FSDP bound 13.0% - - -FIRST/BOEM 19.3% 41 74 44VOTE/BOEM 20.0% 46 73 50SA 20.3% 42 73 45BEST/BOEM 21.3% 43 73 47BOEM 21.5% 22 72 21PIVOT/BOEM 22.0% 45 72 50VOTE 26.3% 44 72 48BEST 37.1% 40 67 44PIVOT 44.4% 39 66 44FIRST 58.3% 39 62 41Additive WeightsObj 1-1 Loc3 Shen FSDP bound 16.2% - - -FIRST/BOEM 21.7% 40 73 44BOEM 22.3% 22 73 20BEST/BOEM 22.7% 44 74 49VOTE/BOEM 23.3% 46 73 50SA 23.8% 41 72 46PIVOT/BOEM 24.8% 46 73 50VOTE 30.5% 44 71 49EC ?08 - 41 73 44BEST 42.1% 43 69 47PIVOT 48.4% 38 67 44FIRST 69.0% 40 59 41Table 3: Score of the solution with best objective foundby each solver on the chat test dataset, averaged over 6annotations, sorted by objective.Whatever algorithm is used to provide an initialsolution, we advise the use of local search as a post-process.
BOEM always improves both objectiveand metric values over its starting point.The objective value is not always sufficient to se-lect a good solution (as in the chat dataset).
If pos-sible, experimenters should check statistics like thenumber of clusters found to make sure they conformroughly to expectations.
Algorithms that find fartoo many or too few clusters, regardless of objec-tive, are unlikely to be useful.
This type of problemcan be especially dangerous if the pairwise classifierabstains for many pairs of points.SDP provides much tighter bounds than the trivialbound used in previous work, although how muchNum clustersMax human annotator 128PIVOT 122VOTE 99PIVOT/BOEM 89VOTE/BOEM 86Mean human annotator 81BEST 70FIRST 70Elsner and Charniak (2008) 63BEST/BOEM 62SA 57FIRST/BOEM 54Min human annotator 50BOEM 7Table 4: Average number of clusters found (using addi-tive weights) for chat test data.1-1 Loc3 Shen FLog-wt -.40 -.68 -.35Top 10 % .14 -.15 .15Add-wt -.31 -.67 -.25Top 10 % -.07 -.22 .13Table 5: Kendall?s tau correlation between objective andmetric values for the chat test set, for all solutions and top10% of solutions.tighter varies with dataset (about 12 times smallerfor newsgroups, 3 times for chat).
This bound canbe used to evaluate the absolute performance of oursolvers; the VOTE/BOEM solver whose use we rec-ommend is within about 5% of optimality.
Some ofthis 5% represents the difference between the boundand optimality; the rest is the difference between theoptimum and the solution found.
If the bound wereexactly optimal, we could expect a significant im-provement on our best results, but not a very largeone?especially since correlation between objectiveand metric values grows weaker for the best solu-tions.
While it might be useful to investigate moresophisticated local searches in an attempt to closethe gap, we do not view this as a priority.AcknowledgementsWe thank Christoph Helmberg, Claire Mathieu andthree reviewers.26ReferencesNir Ailon, Moses Charikar, and Alantha Newman.
2008.Aggregating inconsistent information: Ranking andclustering.
Journal of the ACM, 55(5):Article No.
23.Nikhil Bansal, Avrim Blum, and Shuchi Chawla.
2004.Correlation clustering.
Machine Learning, 56(1-3):89?113.Regina Barzilay and Mirella Lapata.
2006.
Aggregationvia set partitioning for natural language generation.
InHLT-NAACL.Amir Ben-Dor, Ron Shamir, and Zohar Yakhini.
1999.Clustering gene expression patterns.
Journal of Com-putational Biology, 6(3-4):281?297.Michael Bertolacci and Anthony Wirth.
2007.
Areapproximation algorithms for consensus clusteringworthwhile?
In SDM ?07: Procs.
7th SIAM Inter-national Conference on Data Mining.Moses Charikar, Venkatesan Guruswami, and AnthonyWirth.
2005.
Clustering with qualitative information.J.
Comput.
Syst.
Sci., 71(3):360?383.William W. Cohen and Jacob Richman.
2002.
Learn-ing to match and cluster large high-dimensional datasets for data integration.
In KDD ?02, pages 475?480.ACM.Scott Deerwester, Susan T. Dumais, George W. Furnas,Thomas K. Landauer, and Richard Harshman.
1990.Indexing by latent semantic analysis.
Journal of theAmerican Society for Information Science, 41:391?407.Erik D. Demaine, Dotan Emanuel, Amos Fiat, and NicoleImmorlica.
2006.
Correlation clustering in generalweighted graphs.
Theor.
Comput.
Sci., 361(2):172?187.Micha Elsner and Eugene Charniak.
2008.
You talk-ing to me?
a corpus and algorithm for conversationdisentanglement.
In Proceedings of ACL-08: HLT,pages 834?842, Columbus, Ohio, June.
Associationfor Computational Linguistics.Jenny Rose Finkel and Christopher D. Manning.
2008.Enforcing transitivity in coreference resolution.
InProceedings of ACL-08: HLT, Short Papers, pages 45?48, Columbus, Ohio, June.
Association for Computa-tional Linguistics.Aristides Gionis, Heikki Mannila, and PanayiotisTsaparas.
2007.
Clustering aggregation.
ACM Trans.on Knowledge Discovery from Data, 1(1):Article 4.Ioannis Giotis and Venkatesan Guruswami.
2006.
Corre-lation clustering with a fixed number of clusters.
The-ory of Computing, 2(1):249?266.Andrey Goder and Vladimir Filkov.
2008.
Consensusclustering algorithms: Comparison and refinement.
InALENEX ?08: Procs.
10th Workshop on Algorithm En-ginering and Experiments, pages 109?117.
SIAM.Cristoph Helmberg.
2000.
Semidefinite programmingfor combinatorial optimization.
Technical Report ZR-00-34, Konrad-Zuse-Zentrum fu?r InformationstechnikBerlin.Cristoph Helmberg, 2009.
The ConicBundle Li-brary for Convex Optimization.
Ver.
0.2i fromhttp://www-user.tu-chemnitz.de/?helmberg/ConicBundle/.Thorsten Joachims and John Hopcroft.
2005.
Errorbounds for correlation clustering.
In ICML ?05, pages385?392, New York, NY, USA.
ACM.Thorsten Joachims.
1997.
A probabilistic analysis ofthe Rocchio algorithm with TFIDF for text categoriza-tion.
In International Conference on Machine Learn-ing (ICML), pages 143?151.Igor Malioutov and Regina Barzilay.
2006.
Minimumcut model for spoken lecture segmentation.
In ACL.The Association for Computer Linguistics.Claire Mathieu and Warren Schudy.
2008.Correlation clustering with noisy input.Unpublished manuscript available fromhttp://www.cs.brown.edu/?ws/papers/clustering.pdf.Andrew McCallum and Ben Wellner.
2004.
Condi-tional models of identity uncertainty with applicationto noun coreference.
In Proceedings of the 18th An-nual Conference on Neural Information ProcessingSystems (NIPS), pages 905?912.
MIT Press.Marina Meila.
2007.
Comparing clusterings?an infor-mation based distance.
Journal of Multivariate Analy-sis, 98(5):873?895, May.Vincent Ng and Claire Cardie.
2002.
Improving machinelearning approaches to coreference resolution.
In Pro-ceedings of the 40th Annual Meeting of the Associationfor Computational Linguistics, pages 104?111.Dou Shen, Qiang Yang, Jian-Tao Sun, and Zheng Chen.2006.
Thread detection in dynamic text messagestreams.
In SIGIR ?06, pages 35?42, New York, NY,USA.
ACM.Wee Meng Soon, Hwee Tou Ng, and Daniel Chung YongLim.
2001.
A machine learning approach to corefer-ence resolution of noun phrases.
Computational Lin-guistics, 27(4):521?544.Lidan Wang and Douglas W. Oard.
2009.
Context-basedmessage expansion for disentanglement of interleavedtext conversations.
In Proceedings of NAACL-09 (toappear).Shi Zhong and Joydeep Ghosh.
2003.
Model-based clus-tering with soft balancing.
In ICDM ?03: Proceedingsof the Third IEEE International Conference on DataMining, page 459, Washington, DC, USA.
IEEE Com-puter Society.27
