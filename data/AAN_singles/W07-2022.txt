Proceedings of the 4th International Workshop on Semantic Evaluations (SemEval-2007), pages 117?120,Prague, June 2007. c?2007 Association for Computational LinguisticsCLaC and CLaC-NB: Knowledge-based and corpus-based approachesto sentiment taggingAlina AndreevskaiaConcordia University1455 de Maisonneuve Blvd.Montreal, Canadaandreev@cs.concordia.caSabine BerglerConcordia University1455 de Maisonneuve Blvd.Montreal, Canadabergler@cs.concordia.caAbstractFor the Affective Text task at Semeval-1/Senseval-4, the CLaC team compared aknowledge-based, domain-independent ap-proach and a standard, statistical machinelearning approach to ternary sentiment an-notation of news headlines.
In this paperwe describe the two systems submitted tothe competition and evaluate their results.We show that the knowledge-based unsu-pervised method achieves high accuracy andprecision but low recall, while supervisedstatistical approach trained on small amountof in-domain data provides relatively highrecall at the cost of low precision.1 IntroductionSentiment tagging of short text spans ?
sentences,headlines, or clauses ?
poses considerable chal-lenges for automatic systems due to the scarcity ofsentiment clues in these units: sometimes, the deci-sion about the text span sentiment has to be basedon just a single sentiment clue and the cost of everyerror is high.
This is particularly true for headlines,which are typically very short.
Therefore, an idealsystem for sentiment tagging of headlines has to usea large set of features with dependable sentiment an-notations and to be able to reliably deduce the senti-ment of the headline from the sentiment of its com-ponents.The valence labeling subtask of the Affective Texttask requires ternary ?
positive vs. negative vs.neutral ?
classification of headlines.
While suchcategorization at the sentence level remains rela-tively unexplored1 , the two related sentence-level,binary classification tasks ?
positive vs. negativeand subjective vs. objective ?
have attracted con-siderable attention in the recent years (Hu and Liu,2004; Kim and Hovy, 2005; Riloff et al, 2006; Tur-ney and Littman, 2003; Yu and Hatzivassiloglou,2003).
Unsupervised knowledge-based methods arethe preferred approach to classification of sentencesinto positive and negative, mostly due to the lack ofadequate amounts of labeled training data (Gamonand Aue, 2005).
These approaches rely on presenceand scores of sentiment-bearing words that havebeen acquired from dictionaries (Kim and Hovy,2005) or corpora (Yu and Hatzivassiloglou, 2003).Their accuracy on news sentences is between 65 and68%.Sentence-level subjectivity detection, where train-ing data is easier to obtain than for positive vs. neg-ative classification, has been successfully performedusing supervised statistical methods alone (Pang andLee, 2004) or in combination with a knowledge-based approach (Riloff et al, 2006).Since the extant literature does not provide clearevidence for the choice between supervised machinelearning methods and unsupervised knowledge-based approaches for the task of ternary sentimentclassification of sentences or headlines, we devel-oped two systems for the Affective Text task atSemEval-2007.
The first system (CLaC) relies onthe knowledge-rich approach that takes into consid-1To our knowledge, the only work that attempted such clas-sification at the sentence level is (Gamon and Aue, 2005) thatclassified product reviews.117eration multiple clues, such as a list of sentiment-bearing unigrams and valence shifters, and makesuse of sentence structure in order to combine theseclues into an overall sentiment of the headline.
Thesecond system (CLaC-NB) explores the potential ofa statistical method trained on a small amount ofmanually labeled news headlines and sentences.2 CLaC System: Syntax-AwareDictionary-Based ApproachThe CLaC system relies on a knowledge-based,domain-independent, unsupervised approach toheadline sentiment detection and scoring.
Thesystem uses three main knowledge inputs: a listof sentiment-bearing unigrams, a list of valenceshifters (Polanyi and Zaenen, 2006), and a set ofrules that define the scope and results of com-bination of sentiment-bearing words with valenceshifters.2.1 List of sentiment-bearing wordsThe unigrams used for sentence/headline classifica-tion were learned from WordNet (Fellbaum, 1998)dictionary entries using the STEP system describedin (Andreevskaia and Bergler, 2006b).
In order totake advantage of the special properties of WordNetglosses and relations, we developed a system thatused the human-annotated adjectives from (Hatzi-vassiloglou and McKeown, 1997) as a seed list andlearned additional unigrams from WordNet synsetsand glosses.
The STEP algorithm starts with asmall set of manually annotated seed words thatis expanded using synonymy and antonymy rela-tions in WordNet.
Then the system searches allWordNet glosses and selects the synsets that containsentiment-bearing words from the expanded seedlist in their glosses.
In order to eliminate errorsproduced by part-of-speech ambiguity of some ofthe seed words, the glosses are processed by Brill?spart-of-speech tagger (Brill, 1995) and only the seedwords with matching part-of-speech tags are consid-ered.
Headwords with sentiment-bearing seed wordsin their definitions are then added to the positive ornegative categories depending on the seed-word sen-timent.
Finally, words that were assigned contra-dicting ?
positive and negative ?
sentiment withinthe same run were eliminated.
The average accu-racy of 60 runs with non-intersecting seed lists whencompared to General Inquirer (Stone et al, 1966)was 74%.
In order to improve the list coverage,the words annotated as ?Positiv?
or ?Negativ?
in theGeneral Inquirer that were not picked up by STEPwere added to the final list.Since sentiment-bearing words in English havedifferent degree of centrality to the category of sen-timent, we have constructed a measure of word cen-trality to the category of positive or negative sen-timent described in our earlier work (Andreevskaiaand Bergler, 2006a).
The measure, termed Net Over-lap Score (NOS), is based on the number of ties thatconnect a given word to other words in the category.The number of such ties is reflected in the num-ber of times each word was retrieved from Word-Net by multiple independent STEP runs with non-intersecting seed lists.
This approach allowed usto assign NOSs to each unigram captured by mul-tiple STEP runs.
Only words with fuzzy member-ship score not equal to zero were retained in thelist.
The resulting list contained 10,809 sentiment-bearing words of different parts of speech.2.2 Valence ShiftersThe brevity of the headlines compared to typicalnews sentences2 requires that the system is able tomake a correct decision based on very few sentimentclues.
Due to the scarcity of sentiment clues, the ad-ditional factors, such as presence of valence shifters,have a greater impact on the system performance onheadlines than on sentences or texts, where impactof a single error can often be compensated by a num-ber of other, correctly identified sentiment clues.
Forthis reason, we complemented the system based onfuzzy score counts with the capability to discern andtake into account some relevant elements of syntac-tic structure of sentences.
We added to the systemtwo components in order to enable this capability:(1) valence shifter handling rules and (2) parse treeanalysis.Valence shifters can be defined as words that mod-ify the sentiment expressed by a sentiment-bearingword (Polanyi and Zaenen, 2006).
The list of va-lence shifters used in our experiments was a com-2An average length of a sentence in a news corpus is over 20words, while the average length of headlines in the test corpuswas only 7 words.118bination of (1) a list of common English nega-tions, (2) a subset of the list of automatically ob-tained words with increase/decrease semantics, and(3) words picked up in manual annotation conductedfor other research projects by two trained linguists.The full list consists of 490 words and expressions.Each entry in the list of valence shifters has an actionand scope associated with it.
The action and scopetags are used by special handling rules that enableour system to identify such words and phrases in thetext and take them into account in sentence senti-ment determination.
In order to correctly determinethe scope of valence shifters in a sentence, we intro-duced into the system the analysis of the parse treesproduced by MiniPar (Lin, 1998).As a result of this processing, every headline re-ceived a score according to the combined fuzzy NOSof its constituents.
We then mapped this score,which ranged between -1.2 and 0.99, into the[-100, 100] scale as required by the competition or-ganizers.3 CLaC-NB System: Na?
?ve BayesSupervised statistical methods have been very suc-cessful in sentiment tagging of texts and in subjec-tivity detection at sentence level: on movie reviewtexts they reach an accuracy of 85-90% (Aue andGamon, 2005; Pang and Lee, 2004) and up to 92%accuracy on classifying movie review snippets intosubjective and objective using both Nave Bayes andSVM (Pang and Lee, 2004).
These methods per-form particularly well when a large volume of la-beled data from the same domain as the test set isavailable for training (Aue and Gamon, 2005).
Thelack of sufficient data for training appears to be themain reason for the virtual absence of experimentswith statistical classifiers in sentiment tagging at thesentence level.In order to explore the potential of statistical ap-proaches on sentiment classification of headlines,we implemented a basic Na?
?ve Bayes classifier withsmoothing using Lidstone?s law of succession (with?=0.1).
No feature selection was performed.The development set for the Affective Text taskconsisted of only 250 headlines, which is not suf-ficient for training of a statistical classifier.
In or-der to increase the size of the training corpus, weaugmented it with a balanced set of 900 manuallyannotated news sentences on a variety of topics ex-tracted from the Canadian NewsStand database3 and200 headlines from different domains collected fromGoogle News in January 20074.The probabilities assigned by the classifier weremapped to [-100, 100] as follows: all negative head-lines received a score of -100, all positive headlines+100, and neutral headlines 0.4 Results and DiscussionTable 1 shows the results of the two CLaC systemsfor valence labeling subtask of Affective Text taskcompared to all participating systems average.
Thebest subtask scores are highlighted in bold.System Pearson Acc.
Prec.
Rec.
F1correl.CLaC 47.7 55.1 61.4 9.2 16CLaC-NB 25.4 31.2 31.2 66.4 42Task average 33.2 44.7 44.85 29.6 23.7Table 1: System resultsThe comparison between the two CLaC systemsclearly demonstrates the relative advantages of thetwo approaches.
The knowledge-based unsuper-vised system performed well above average on threemain measures: the Pearson correlation betweenfine-grained sentiment assigned by CLaC systemand the human annotation; the accuracy for ternaryclassification; and the precision of binary (positivevs.
negative) classification.
These results demon-strate that an accurately annotated list of sentiment-bearing words combined with sophisticated valenceshifter handling produces acceptably accurate senti-ment labels even for such difficult data as news head-lines.
This system, however, was not able to providegood recall.On the contrary, supervised machine learning hasvery good recall, but low accuracy relative to theresults of the unsupervised knowledge-based ap-proach.
This shortcoming could be in part reducedif more uniformly labeled headlines were available3http://www.il.proquest.com/products pq/descriptions/Canadian newsstand.shtml4The interannotator agreement for this data, as measured byKappa, was 0.74.119for training.
However, we can hardly expect largeamounts of such manually annotated data to behandy in real-life situations.5 ConclusionsThe two CLaC systems that we submitted to theAffective Text task have tested the applicability oftwo main sentiment tagging approaches to newsheadlines annotation.
The results of the two sys-tems indicate that the knowledge-based unsuper-vised approach that relies on an automatically ac-quired list of sentiment-bearing unigrams and takesinto account the combinatorial properties of valenceshifters, can produce high quality sentiment annota-tions, but may miss many sentiment-laden headlines.On the other hand, supervised machine learning hasgood recall even with a relatively small training set,but its precision and accuracy are low.
In our futurework we will explore the potential of combining thetwo approaches in a single system in order to im-prove both recall and precision of sentiment annota-tion.ReferencesAlina Andreevskaia and Sabine Bergler.
2006a.
Miningwordnet for a fuzzy sentiment: Sentiment tag extrac-tion from wordnet glosses.
In Proceedings EACL-06,the 11rd Conference of the European Chapter of theAssociation for Computational Linguistics, Trento, IT.Alina Andreevskaia and Sabine Bergler.
2006b.
Seman-tic tag extraction from wordnet glosses.
In Proceed-ings of LREC-06, the 5th Conference on Language Re-sources and Evaluation, Genova, IT.Anthony Aue and Michael Gamon.
2005.
Customiz-ing sentiment classifiers to new domains: a case study.In RANLP-05, the International Conference on RecentAdvances in Natural Language Processing, Borovets,Bulgaria.Eric Brill.
1995.
Transformation-Based Error-DrivenLearning and Natural Language Processing: A CaseStudy in Part-of-Speech Tagging.
Computational Lin-guistics, 21(4).Christiane Fellbaum, editor.
1998.
WordNet: An Elec-tronic Lexical Database.
MIT Press, Cambridge, MA.Michael Gamon and Anthony Aue.
2005.
Automaticidentification of sentiment vocabulary: exploiting lowassociation with known sentiment terms.
In Proceed-ings of the ACL-05 Workshop on Feature Engineeringfor Machine Learning in Natural Language Process-ing, Ann Arbor, MI.Vasileios Hatzivassiloglou and Kathleen B. McKeown.1997.
Predicting the Semantic Orientation of Ad-jectives.
In Proceedings of ACL-97, 35nd Meeting ofthe Association for Computational Linguistics, pages174?181, Madrid, Spain.
ACL.Minqing Hu and Bing Liu.
2004.
Mining and sum-marizing customer reviews.
In Tenth ACM SIGKDDConference on Knowledge Discovery and Data Min-ing (KDD-04), pages 168?177.Soo-Min Kim and Eduard Hovy.
2005.
Automatic de-tection of opinion bearing words and sentences.
InCompanion Volume to the Proceedings of IJCNLP-05,the Second International Joint Conference on NaturalLanguage Processing, pages 61?66, Jeju Island, KR.Dekang Lin.
1998.
Dependency-based Evaluationof MINIPAR.
In Proceedings of the Workshop onthe Evaluation of Parsing Systems, pages 768?774,Granada, Spain.Bo Pang and Lilian Lee.
2004.
A sentiment education:Sentiment analysis using subjectivity summarizationbased on minimum cuts.
In Proceedings of ACL-04,42nd Meeting of the Association for ComputationalLinguistics, pages 271?278.Livia Polanyi and Annie Zaenen.
2006.
Contextual Va-lence Shifters.
In James G. Shanahan, Yan Qu, andJanyce Wiebe, editors, Computing Attitude and Affectin Text: Theory and Application.
Springer Verlag.Ellen Riloff, Siddharth Patwardhan, and Janyce Wiebe.2006.
Feature subsumption for opinion analysis.
InProceedings of EMNLP-06, the Conference on Empir-ical Methods in Natural Language Processing, pages440?448, Sydney, AUS.P.
J.
Stone, D.C. Dumphy, M.S.
Smith, and D.M.
Ogilvie.1966.
The General Inquirer: a computer approach tocontent analysis.
M.I.T.
studies in comparative poli-tics.
M.I.T.
Press, Cambridge, MA.Peter Turney and Michael Littman.
2003.
Measuringpraise and criticism: inference of semantic orientationfrom association.
ACM Transactions on InformationSystems (TOIS), 21:315?346.Hong Yu and Vasileios Hatzivassiloglou.
2003.
Towardsanswering opinion questions: Separating facts fromopinions and identifying the polarity of opinion sen-tences.
In Michael Collins and Mark Steedman, ed-itors, Proceedings of EMNLP-03, 8th Conference onEmpirical Methods in Natural Language Processing,pages 129?136, Sapporo, Japan.120
