Proceedings of the ACL-08: HLT Demo Session (Companion Volume), pages 32?35,Columbus, June 2008. c?2008 Association for Computational LinguisticsThe QuALiM Question Answering Demo:Supplementing Answers with Paragraphs drawn from WikipediaMichael KaisserSchool of InformaticsUniversity of EdinburghM.Kaisser@sms.ed.ac.ukAbstractThis paper describes the online demo of theQuALiM Question Answering system.
Whilethe system actually gets answers from the webby querying major search engines, during pre-sentation answers are supplemented with rel-evant passages from Wikipedia.
We believethat this additional information improves auser?s search experience.1 IntroductionThis paper describes the online demo ofthe QuALiM1 Question Answering system(http://demos.inf.ed.ac.uk:8080/qualim/).
Wewill refrain from describing QuALiM?s answerfinding strategies?our work on QuALiM has beendescribed in several papers in the last few years,especially Kaisser and Becker (2004) and Kaisser etal.
(2006) are suitable to get an overview over thesystem?but concentrate on one new feature that wasdeveloped especially for this web demo: In orderto improve user benefit, answers are supplementedwith relevant passages from the online encyclopediaWikipedia.
We see two main benefits:1.
Users are presented with additional informationclosely related to their actual information needand thus of potential high interest.2.
The returned text passages present the answerin context and thus help users to validate theanswer?there always will be the odd case wherea system returns a wrong result.1for Question Answering with Linguistic MethodsHistorically, our system is web-based, receivingits answers by querying major search engines andpost processing their results.
In order to satisfyTREC requirements?which require participants toreturn the ID of one document from the AQUAINTcorpus that supports the answer itself (Voorhees,2004)?we already experimented with answer projec-tion strategies in our TREC participations in recentyears.
For this web demo we use Wikipedia insteadof the AQUAINT corpus for several reasons:1.
QuALiM is an open domain Question Answer-ing system and Wikipedia is an ?open domain?Encyclopedia; it aims to cover all areas of inter-est as long as they are of some general interest.2.
Wikipedia is a free online encyclopedia.
Otherthan the AQUAINT corpus, there are no legalproblems when using it for a public demo.3.
Wikipedia is frequently updated, whereas theAQUAINT corpus remains static and thus con-tains a lot of outdated information.Another advantage of Wikipedia is that the in-formation contained is much more structured.
Aswe will see, this structure can be exploited to im-prove performance when finding answers or?as inour case?projecting answers.2 How Best to Present Answers?In the fields of Question Answering and WebSearch, the issue how answers/results should be pre-sented is a vital one.
Nevertheless, as of today, themajority of QA system?which a few notable excep-tions, e.g.
MIT?s START (Katz et al, 2002)?are32Figure 1: Screenshot of QuALiM?s response to the question ?How many Munros are there in Scotland??
The greenbar to the left indicates that the system is confident to have found the right answer, which is shown in bold: ?284?.Furthermore, one Wikipedia paragraph which contains additional information of potential interest to the user is dis-played.
In this paragraph the sentence containing the answer is highlighted.
This display of context also allows theuser to validate the answer.still experimental and research-oriented and typi-cally only return the answer itself.
Yet it is highlydoubtful that this is the best strategy.Lin et al (2003) performed a study with32 computer science students comparing fourtypes of answer context: exact answer, answer-in-sentence, answer-in-paragraph, and answer-in-document.
Since they were interested in interfacedesign, they worked with a system that answeredall questions correctly.
They found that 53% of allparticipants preferred paragraph-sized chunks, 23%preferred full documents, 20% preferred sentences,and one participant preferred exact answer.Web search engines typically show results as alist of titles and short snippets that summarize howthe retrieved document is related to the query terms,often called query-biased summaries (Tombros andSanderson, 1998).
Recently, Kaisser et al (2008)conducted a study to test whether users would pre-fer search engine results of different lengths (phrase,sentence, paragraph, section or article) and whetherthe optimal response length could be predicted byhuman judges.
They find that judges indeed pre-fer different response lengths for different types ofqueries and that these can be predicted by otherjudges.In this demo, we opted for a slightly different, yetrelated approach: The system does not decide onone answer length, but always presents a combina-tion of three different lengths to the user (see Figure1): The answer itself (usually a phrase), is presentedin bold.
Additionally, a paragraph relating the an-swer to the question is shown, and in this paragraphone sentence containing the answer is highlighted.Note also, that each paragraph contains a link thattakes the user to the Wikipedia article, should he/shewant to know more about the subject.
The intentionbehind this mode of presentation is to prominentlydisplay the piece of information the user is most in-terested in, but also to present context informationand to furthermore provide options for the user tofind out more about the topic, should he/she want to.3 Finding Supportive WikipediaParagraphsWe use Lucene (Hatcher and Gospodnetic?, 2004) toindex the publically available Wikipedia dumps (seehttp://download.wikimedia.org/).
The text inside thedump is broken down into paragraphs and each para-graph functions as a Lucene document.
The data ofeach paragraph is stored in three fields: Title, whichcontains the title of the Wikipedia article the para-graph is from, Headers, which lists the title and allsection and subsection headings indicating the posi-tion of the paragraph in the article and Text, whichstores the text of the article.
An example can be seen33in Table 1.Title ?Tom Cruise?Headers ?Tom Cruise/Relationships and personallife/Katie Holmes?Text ?In April 2005, Cruise began datingKatie Holmes ... the couple married inBracciano, Italy on November 18, 2006.?Table 1: Example of Lucene index fields used.As mentioned, QuALiM finds answers by query-ing major search engines.
After post processing, alist of answer candidates, each one associated with aconfidence value, is output.
For the question ?Whois Tom Cruise married to?
?, for example, we get:81.0: "Katie Holmes"35.0: "Nicole Kidman"The way we find supporting paragraphs for theseanswers is probably best explained by giving anexample.
Figure 3 shows the Lucene query weuse for the mentioned question and answer can-didates.
(The numbers behind the terms indicatequery weights.)
As can be seen, we initially buildtwo separate queries for the Headers and the Textfields (compare Table 1).
In a later processing step,both queries are combined into a single query us-ing Lucene?s MultipleFieldQueryCreatorclass.
Note also that both answer candidates (?KatieHolmes?
and ?Nicole Kidman?)
are included in thisone query.
This is done because of speed issues: Inour setup, each query takes up roughly two secondsof processing time.
The complexity and length ofa query on the other hand has very little impact onspeed.The type of question influences the query buildingprocess in a fundamental manner.
For the question?When was Franz Kafka born??
and the correct an-swer ?July 3, 1883?, for example, it is reasonableto search for an article with title ?Franz Kafka?
andto expect the answer in the text on that page.
Forthe question ?Who invented the automobile??
onthe other hand, it is more reasonable to search theinformation on a page called ?Karl Benz?
(the an-swer to the question).
In order to capture this be-haviour we developed a set of rules that for differ-ent type of questions, increases or decreases con-stituents?
weights in either the Headers or the Textfield.Additionally, during question analysis, certainquestion constituents are marked as either Topic orFocus (see Moldovan et al, (1999)).
For the earlierexample question ?Tom Cruise?
becomes the Topicwhile ?married?
is marked Focus2.
These also influ-ence constituents?
weights in the different fields:?
Constituents marked as Topic are generally ex-pected to be found in the Headers field.
Afterall, the topic marks what the question is about.In a similar manner, titles and subtitles help tostructure an article, assisting the user to navi-gate to the place where the relevant informa-tion is most likely to be found: A paragraph?stitles and subtitles indicate what the paragraphis about.?
Constituents marked as Focus are generally ex-pected to be found in the text, especially if theyare verbs.
The focus indicates what the ques-tion asks for, and such information can usuallyrather be expected in the text than in titles orsubtitles.Figure 3 also shows that, if we recognize namedentities (especially person names) in the question oranswer strings, we once include each named entityas a quoted string and additionally add the wordsit contains separately.
This is to boost documentswhich contain the complete name as used in thequestion or the answer, but also to allow documentswhich contain variants of these names, e.g.
?ThomasCruise Mapother IV?.The formula to determine the exact boost factorfor each query term is complex and a matter of on-going development.
It additionally depends on thefollowing criteria:?
Named entities receive a higher weight.?
Capitalized words or constituents receive ahigher weight.?
The confidence value associated with the an-swer candidate influences the boost factor.?
Whether a term originates from the question oran answer candidate influences its weight in adifferent manner for the header and text fields.2With allowing verbs to be the Focus, we slightly departfrom the traditional definition of the term.34Header query:"Tom Cruise"?10 Tom?5 Cruise?5 "Katie Holmes"?5 Katie?2.5 Holmes2.
?5"Nicole Kidman"?4.3 Nicole?2.2 Kidman?2.2Text query:married?10 "Tom Cruise"?1.5 Tom?4.5 Cruise?4.5 "Katie Holmes"?3 Katie?9 Holmes?9"Nicole Kidman"?2.2 Nicole?6.6 Kidman?6.6Figure 2: Lucene Queries used to find supporting documents for the ?Who is Tom Cruise married to?
?and the two answers ?Katie Holmes?
and ?Nicole Kidman?.
Both queries are combined using Lucene?sMultipleFieldQueryCreator class.4 Future WorkAlthough QuALiM performed well in recent TRECevaluations, improving precision and recall will ofcourse always be on our agenda.
Beside this we cur-rently focus on increasing processing speed.
At thetime of writing, the web demo runs on a server witha single 3GHz Intel Pentium D dual core processorand 2Gb SDRAM.
At times, the machine is sharedwith other demos and applications.
This makes re-liable figures about speed difficult to produce, butfrom our log files we can see that users usually waitbetween three and twelve seconds for the system?sresults.
While this is okay for a research demo, itdefinitely would not be fast enough for a commer-cial product.
Three factors contribute with roughlyequal weight to the speed issue:1.
Search engine?s APIs usually do not return re-sults as fast as their web interfaces built for hu-man use do.
Google for example has a built-inone second delay for each query asked.
Thedemo usually sends out between one and fourqueries per question, thus getting results fromGoogle alone takes between one and four sec-onds.2.
All received results need to be post-processed,the most computing heavy step here is parsing.3.
Finally, the local (8.3 GB big) Wikipedia indexneeds to be queried, which roughly takes twoseconds per query.We are currently looking into possibilities to im-prove all of the above issues.AcknowledgementsThis work was supported by Microsoft Researchthrough the European PhD Scholarship Programme.ReferencesErik Hatcher and Otis Gospodnetic?.
2004.
Lucene inAction.
Manning Publications Co.Michael Kaisser and Tilman Becker.
2004.
Question An-swering by Searching Large Corpora with LinguisticMethods.
In The Proceedings of the 2004 Edition ofthe Text REtrieval Conference, TREC 2004.Michael Kaisser, Silke Scheible, and Bonnie Webber.2006.
Experiments at the University of Edinburgh forthe TREC 2006 QA track.
In The Proceedings of the2006 Edition of the Text REtrieval Conference, TREC2006.Michael Kaisser, Marti Hearst, and John Lowe.
2008.Improving Search Result Quality by CustomizingSummary Lengths.
In Proceedings of the 46th AnnualMeeting of the Association for Computational Linguis-tics.Boris Katz, Jimmy Lin, and Sue Felshin.
2002.
TheSTART multimedia information system: Current tech-nology and future directions.
In Proceedings of the In-ternational Workshop on Multimedia Information Sys-tems (MIS 2002).Jimmy Lin, Dennis Quan, Vineet Sinha, Karun Bakshi,David Huynh, Boris Katz, and David R. Karger.
2003.What Makes a Good Answer?
The Role of Context inQuestion Answering.
Human-Computer Interaction(INTERACT 2003).Dan Moldovan, Sanda Harabagiu, Marius Pasca, RadaMihalcea, Richard Goodrum, Roxana Girju, andVasile Rus.
1999.
LASSO: A tool for surfing the an-swer net.
In Proceedings of the Eighth Text RetrievalConference (TREC-8).A.
Tombros and M. Sanderson.
1998.
Advantages ofquery biased summaries in information retrieval.
Pro-ceedings of the 21st annual international ACM SIGIRconference on Research and development in informa-tion retrieval, pages 2?10.Ellen M. Voorhees.
2004.
Overview of the TREC 2003Question Answering Track.
In The Proceedings of the2003 Edition of the Text REtrieval Conference, TREC2003.35
