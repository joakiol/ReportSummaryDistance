Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008), pages 1121?1128Manchester, August 2008Diagnostic Evaluation of Machine Translation Systems Using Auto-matically Constructed Linguistic Check-PointsMing Zhou1, Bo Wang2, Shujie Liu2, Mu Li1, Dongdong Zhang1, Tiejun Zhao21Microsoft Research AsiaBeijing, China{mingzhou,muli,dozhang}@microsoft.com2Harbin Institute of TechnologyHarbin, China{bowang,Shujieliu,tjzhao}@mtlab.hit.edu.cn?AbstractWe present a diagnostic evaluation plat-form which provides multi-factored eval-uation based on automatically con-structed check-points.
A check-point is alinguistically motivated unit (e.g.
an am-biguous word, a noun phrase, a verb~objcollocation, a prepositional phrase etc.
),which are pre-defined in a linguistic tax-onomy.
We present a method that auto-matically extracts check-points from pa-rallel sentences.
By means of check-points, our method can monitor a MTsystem in translating important linguisticphenomena to provide diagnostic evalua-tion.
The effectiveness of our approachfor diagnostic evaluation is verifiedthrough experiments on various types ofMT systems.1 IntroductionAutomatic MT evaluation is a crucial issue forMT system developers.
The state-of-the-art me-thods for automatic MT evaluation are using ann-gram based metric represented by BLEU (Pa-pineni et al, 2002) and its variants.
Ever since itsinvention, the BLEU score has been a widelyaccepted benchmark for MT system evaluation.Nevertheless, the research community has beenaware of the deficiencies of the BLEU metric(Callison-Burch et al, 2006).
For instance,BLEU fails to sufficiently capture the vitality ofnatural languages: all grams of a sentence are?
2008.
Licensed under the Creative Commons Attribution-Noncommercial-Share Alike 3.0 Unported license(http://creativecommons.org/licenses/by-nc-sa/3.0/).
Somerights reserved.treated equally ignoring their linguistic signific-ance; only consecutive grams are considered ig-noring the skipped grams of certain linguisticrelations; candidate translation gets acknowl-edged only if it uses exactly the same lexicon asthe reference ignoring the variation in lexicalchoice.
Furthermore, BLEU is useful for opti-mizing and improving statistical MT systems butit has shown to be ineffective in comparing sys-tems with different architectures (e.g., rule-basedvs.
phrase-based) (Callison-Burch et al,  2006).Another common deficiency of the state-of-the-art evaluation approaches is that they cannotclearly inform MT developers on the detailedstrengths and flaws of an MT system, and there-fore there is no way for us to understand the ca-pability of certain modules of an MT system, andthe capability of translating certain kinds of lan-guage phenomena.
For the purpose of systemdevelopment, MT developers need a diagnosticevaluation approach to provide the feedback onthe translation ability of an MT system with re-gard to various important linguistic phenomena.We propose a novel diagnostic evaluation ap-proach.
Instead of assigning a general score to anMT system we evaluate the capability of the sys-tem in handling various important linguistic testcases called Check-Points.
A check-point is alinguistically motivated unit, (e.g.
an ambiguousword, a noun phrase, a verb~obj collocation, aprepositional phrase etc.)
which are pre-definedin a linguistic taxonomy for diagnostic evalua-tion.
The reference of a check-point is its corres-ponding part in the target sentence.
The evalua-tion is performed by matching the candidatetranslation corresponding to the references of thecheck-points.
The extraction of the check-pointsis an automatic process using word aligner andparsers.
We control the noise of the word alignerand parsers within tolerable scope by selecting1121reliable subset of the check-points and weightingthe references with confidence.The check-points of various kinds extracted inthis way have shown to be effective in perform-ing diagnostic evaluation of MT systems.
In ad-dition, scores of check-points are also approvedto be useful to improve the ranking of MT sys-tems as additional features at sentence level anddocument level.The rest of the paper is structured in the fol-lowing way:  Section 2 gives the overview of theprocess of the diagnostic evaluation.
Section 3introduces the design of check-point taxonomy.Section 4 explains the details of construction ofcheck-point database and the methods of reduc-ing the noise of aligner and parsers.
Section 5explains the matching approach.
In Section 6, weintroduce the experiments on different MT sys-tems to demonstrate the capability of the diag-nostic evaluation.
In Section 7, we show that thecheck-points can be used to improve the currentranking methods of MT systems.
Section 8 com-pares our approach with related evaluation ap-proaches.
We conclude this work in Section 9.2 Overview of Diagnostic EvaluationIn our implementation, we first build a check-point database encoded in XML by associating atest sentence with qualified check-points it con-tains.
This process can be described as followingsteps:?
Collect a large amount of parallel sen-tences from the web or book collections.?
Parse the sentences of source languageand target language.?
Perform the word alignments betweeneach sentence pair.?
For each category of check-points, extractthe check-points from the parsed sentencepairs.?
Determine the references of each check-point in source language based on theword alignment.With the extracted check-point database, thediagnostic evaluation of an MT system is per-formed with the following steps:?
The test sentences are selected from thedatabase based on the selected categoriesof check-points to be evaluated.?
For each check-point, we calculate thenumber of matched n-grams of the refer-ences against the translated sentence ofthe MT system.
The credit of the MT sys-tem in translating this check-point is ob-tained after necessary normalization.?
The credit of a category can be obtainedby summing up the credits of all check-points of this category.
Then the credit ofan MT system can be obtained by sum-ming up the credits of all categories.?
Finally, scores of system, category groups(e.g.
Words), single category (e.g.
Noun),and detail information of n-gram matchingof each check-point are all provided to thedevelopers to diagnose the MT system.3 Linguistic Check-Point TaxonomyThe taxonomy of automatic diagnostic evaluationshould be widely accepted so that the diagnosticresults can be explained and shared with eachother.
We will also need to remove the sophisti-cated categories that are out of the capability ofcurrent NLP tools to recognize.In light of this consideration, for Chinese-English machine translation, we adopted the ma-nual taxonomy introduced by (Lv, 2000; Liu,2002) and removed items that are beyond thecapability of our parsers.
The taxonomy includestypical check-pints at word, phrase and sentencelevels.
Some examples of the representativecheck-points at different levels are provided be-low:?
Word level check-points:a. Preposition word e.g., ?
(in), ?(at)b.
Ambiguous word e.g., ?(play)c.
New word1 e.g., ??(Punk)?
Phrase level check-points:a. Collocation.
e.g., ??-??
(fired ?
food)b.
Repetitive word combination.
e.g., ??
(have a look)c. Subjective-predicate phrase e.g., ?*?,(he*said)?
Sentence level check-points:a.
?BA?
sentence 2 : ??
(BA)????
.
(He took away the book.)b.
?BEI?
sentence3????(BEI)???.
(The vase was broken.
)1 New words are the terms extracted from web which can bea name entity or popular words emerging recently.2 In a ?BA?
sentence, the object which normally follows theverb occurs preverbally, marked by word ?BA?.3 ?BEI?
sentence is a kind of passive voice in Chinesemarked by word ?BEI?.1122Our implementation of Chinese-Englishcheck-point taxonomy contains 22 categories andEnglish-Chinese check-point taxonomy contains20 categories.
Table 1 and 2 show the twotaxonomies.
In practice, any tag in parsers (e.g.NP) can be easily added as new category.Word levelAmbiguous word New word IdiomNoun Verb AdjectivePronoun Adverb PrepositionQuantifier Repetitive word CollocationPhrase levelSubject-predicatephrasePredicate-objectphrasePreposition-object phraseMeasure phrase Location phraseSentence levelBA sentence BEI sentence SHI sentenceYOU sentence Compound sentenceTable 1:  Chinese check-point taxonomyWord levelNoun Verb (with Tense) Modal verbAdjective Adverb PronounPreposition Ambiguous word PluralityPossessive Comparative & Superlative  degreePhrase levelNoun phrase Verb phrase AdjectivephraseAdverb phrase Preposition phraseSentence levelAttribute clause Adverbial clause Noun clauseHyperbatonTable 2: English check-point taxonomy4 Construction of Check-Point Data-baseGiven a bilingual corpus with word alignment,the construction of check-point database consistsof following two steps.
First, the information ofpos-tag, dependency structure and constituentstructure can be identified with parsers.
Thencheck-points of different categories are identified.Check-points of word-level categories such asChinese idiom and English ambiguous words areextracted with human-made dictionaries, and thecheck-points of New-Word are extracted with anew word list mined from the web.
A set of hu-man-made rules are employed to extract certaincategories involving sentence types such as com-pound sentence.Second, for a check-point, with the wordalignment information, the corresponding targetlanguage portion is identified as the reference ofthis check-point.
The following example illu-strates the process of extracting check-pointsfrom a parallel sentence pair.?
A Chinese-English sentence pair:????????
?.They opposed the building of reserve funds.?
Word segmentation and pos-tagging:?
?/R ?
?/V ?
?/V ??
?/N ./P?
Parsing result (e.g.
a dependency result):(SUB, 1/?
?, 0/??)
(OBJ, 1/?
?, 2/??)
(OBJ, 2/?
?, 3/???)?
Word alignment:(1; 1); (2; 2); (3; 4); (4; 6,7);?
The check-points in table 3 are extracted:Table 3: Example of check-point extractionTo extract the categories of check-points ofdifferent schema of syntactic analysis such asconstitute structure and dependency structure,three parsers including a Chinese skeleton parser(a kind of dependency parser) (Zhou, 2000),Stanford statistical parser and Berkeley statisticalparser (Klein 2003) are used to parse the Chineseand English sentences.
As explained in next sec-tion, these multiple parsers are also used to selecthigh confident check-points.
To get word align-ment, an existing tool GIZA++ (Och 2003) isused.4.1 Reducing the Noise of the ParserThe reliability of the check-points mainlydepends on the accuracy of the parsers.
We canachieve high quality word level check-pointswith the state-of-the-art POS tagger (94%precision) and dictionaries of various purposes.For sentence level categories, the parser tags andmanually compiled rules can also achieve 95%accuracy.
For some kinds of categories at phraselevel which parsers cannot produce highaccuracy, we only select the check-points whichcan be identified by multiple parsers, that is,adopt the intersection of the parsers results.Table 4 shows the improvement brought by thisapproach.
Column 1 and 2 shows the precision of6 major types of phrases in Stanford andBerkeley parser.
Column 3 shows the precisionof intersection and column 4 shows the reductionof the number of check-points when adopting theintersection results.
The test corpus is a part ofCategory Check-point ReferenceNew word ???
reserve fundsAmbiguous word ??
buildingPredicate ?
objectphrase?????
the building ofreserve fundsSubject-predicatephrase????
They opposed1123Penn Chinese Treebank which is not contained inthe training corpus of two statistical parsers.
(Klein 2003).Stf% Brk% Inter% Tpts redu%NP 87.37 86.03 95.83 17.06VP 87.34 82.87 95.23 19.68PP 90.60 88.56 96.00 11.50QP 98.12 92.90 99.21 6.31ADJP 91.95 90.87 96.41 10.20ADVP 95.21 94.25 92.64 3.92Table 4:  Precision of parsers and their intersec-tion (Stf is Stanford, Brk is Berkelry)4.2 Alleviating the Impact of AlignmentNoiseExcept for sentence level check-points whosereferences are the whole sentences and NewWord, Idiom check-points whose references areextracted from dictionary, the quality of the ref-erences are impacted by the alignment accuracy.To alleviate the noise of aligner we use the lexi-cal dictionary to check the reliability of refer-ences.
Suppose c is a check-point, for each refer-ence c.r of c we calculate the dictionary match-ing degree DM(c.r) with the source side c.s of c:)1()).()).(,.(,1.0().
( rcWordCntscDicrcCoCntMaxrcDM ?Where Dic(x) is a word bag contains all wordsin the dictionary translations of each source wordin x. CoCnt(x, y) is the count of the commonwords in x and y. WordCnt(x) is the count ofwords in x.
Specially, if c.r is not obtained basedon alignment DM(c.r) will be 1.
Because the li-mitation of dictionary, a zero DM score not al-ways means the reference is completely wrong,so we force the DM score to be not less than aminimum value (e.g.
0.1).
DM score will furtherbe used in evaluation in section 5.To better understand the reliability of the ref-erences and explore whether increasing the num-ber of check-points could also alleviate the im-pact of noise, we built two check-point databasesfrom a human-aligned corpus (with 60,000 sen-tence pairs) and an automatically aligned corpus(using GIZA++) respectively and tested 10 dif-ferent SMT systems4 with them.
The Spearmancorrelation is calculated between two ranked listsof the 10 evaluation results against the two data-4 These systems are derived from an in-house phrase basedSMT engine with different parameter sets.bases.
A higher correlation score means that theimpact of the mistakes in word alignment isweaker.
The experiment is repeated on 6 subsetsof the database with the size from 500 sentencesto 16K sentences to check the impact of the cor-pus size.At system level, high correlations are found atdifferent corpus sizes.
At category level, correla-tions are found to be low for some categories atsmall size and become higher at larger corpussize.
The results indicate that the impact of thealignment quality can be ignored if the corpussize is at large scale.
As the check-points can beextracted fully automatically, increasing the sizeof check-point database will not bring extra costand efforts.
Empirically, the proper scale is set tobe 2000 or more sentences according to the Ta-ble 6.Table 6: Impact of word alignment at differentsizes of test corpus.5 Matching Check-Points for Evalua-tionEvaluation can be carried out at multiple options:for certain linguistic category, a group of catego-ries or entire taxonomy.
For instance, in Chinese-English translation task, if a MT developerwould like to know the ability to translate idiom,then a number of parallel sentences containingidiom check-points are selected from the data-base.
Then the system translation sentences arematched to the references of the check-points ofidioms.500 1K 2K 4K 8K 16KAmbiguousword0.98 0.98 0.98 0.98 0.96 0.98Noun 0.93 0.99 0.99 0.89 0.8 0.86Verb 0.97 0.97 0.99 0.99 0.95 0.92Adjective 0.16 0.19 0.57 0.75 0.77 0.97Pronoun 0.96 1 0.93 0.99 0.97 0.99Adverb 0.38 0.77 0.8 0.96 0.72 0.84Preposition 0.56 0.86 0.9 0.9 0.97 0.96Quantifier 1 0.46 0.46 0.98 0.85 0.96RepetitiveWord0.99 0.99 0.97 0.89 0.73 0.95Collocation 0.42 0.77 0.77 0.77 0.73 0.88Subject-predicatephrase0.06 0.8 0.95 1 0.96 0.84Predicate-object phrase0.84 0.96 0.78 0.7 0.78 0.88Preposition-object phrase0.51 0.5 0.93 0.95 0.87 0.99Measurephrase0.91 0.67 0.95 0.95 0.87 0.97Locationphrase0.62 0.54 0.55 0.55 0.85 0.89SYSTEM 0.95 0.95 0.98 0.99 0.97 0.981124To calculate the credit at different occasions ofmatching, similar to BLEU, we split each refer-ence of a check-point into a set of n-grams andsum up the gains over all grams as the credit ofthis check-point.
Especially, if the check-point isnot consecutive we use a special token (e.g.
?*?
)to represent a component which can be wildcardmatched by any word sequence.
We use the fol-lowing examples to demonstrate the splitting andmatching of grams.?
Consecutive check-point:Check- point: ??
?Reference: playing a drumCandidate translation:  He is playing a drum.Matched n-grams: playing; a; drum; playing a;a drum; playing a drum?
Not consecutive check-point:Check- point: ??
*?Reference: They*playingCandidate translation: They are playing copper drum.Matched n-grams: They; playing; They * play-ingAdditionally, to match word inflections, 3 dif-ferent options of matching granularity are de-fined as follows.?
Normal: matching with exact form.?
Lower-case: matching with lowercase.?
Stem: matching with the stem of the word.For a check-point c and references set R of c,we select the r* in R which matches the transla-tion best based on formula (2).When we calculate the recall of a set of check-points C (C can be a single check-point, a cate-gory or a category group), r* of each check-pointc in C are merged into one reference set R* andthe recall of C is obtained using formula (3) onR*.A penalty is also introduced to punish the re-dundancy of candidate sentences, where length(T)is the average length of all translation sentencesand length(R) is the average length of all refer-ence sentences.Then, the final score of C will be:)5()(Re)( PenaltyCCScore ?
?6 Experiments on MT System Diagnos-esIn this section, to demonstrate the ability of ourapproach in the diagnoses of MT systems, weapply diagnostic evaluation to 3 statistical MT(SMT) systems and a rule-based MT (RMT) sys-tem respectively.
We compare two SMT systemsto understand the strength and shortcoming ofeach of them, and also compare a SMT systemwith the RMT system.
The test corpus is NIST05test data with 54852 check-points.First SMT system (system A) is an implemen-tation of classical phrase based SMT.
The secondSMT system (system B) shares the same decoderwith system A and introduces a preprocess toreorder the long phrases in source sentences ac-cording to the syntax structure before decoding(Chiho Li et al, 2007).
The third SMT system(system C) is a popular internet service and theRMT system (system D) is a popular commercialsystem.In the first experiment, we diagnose the sys-tem A and B and compare the results as shown intable 7.
When evaluated using BLEU, system Bachieved a 0.005 points increase on top of systemA which is not a very significant difference.
Thediagnostic results in table 7 provide much richerinformation.
The results indicate that two sys-tems perform similar at the word level categorieswhile at all phrase level categories, system Bperforms better.
This result reflects the benefitfrom the reordering of complex phrases in sys-tem B. Paired t-statistic score for each pair ofcategory scores is also calculated by repeatingthe evaluation on a random copy of the test setwith replacement (Koehn 2004).
An absolutescore beyond 2.17 of paired t-statistic means thedifference of the samples is statistically signifi-cant (above 95%).
Table 8 and 9 show an in-stance of the check-point and its evaluation inthis experiment.)2())'()()((maxarg'*???????
???
?rgramnrgramnRr gramnCountgramnMatchrDMr)4(1)()()()(?????
?
?OtherwiseRlengthTlengthifTlengthRlengthPenalty)3())'()(())()(()Re(**' '''?
??
??
???
??????
?Rr rgranRr rgramngramnCountrDMgramnMatchrDMC1125System A System B T scoreWORDsIdiom 0.1933 0.2370 13.38Adjective 0.5836 0.5577 -17.43Pronoun 0.7566 0.7344 -13.49Adverb 0.5365 0.5433 7.11Preposition 0.6529 0.6456 -6.21Repetitive word 0.3363 0.3958 9.86PHRASEsSubject-predicate 0.5117 0.5206 7.36Predicate-object 0.4041 0.4180 15.52Predicate-complement 0.4409 0.5125 9.51Measure phrase 0.5030 0.5092 3.56Location phrase 0.5245 0.5338 2.83GROUPsWORDs 0.4839 0.4855 2.03PHRASEs 0.4744 0.4964 13.97SYSTEM (Linguistic) 0.4263 0.4370 16.50SYSTEM (BLEU) 0.3564 0.3614 7.91Table 7: Diagnose of SMT systemsSource Sentence ??????????????????????
?Category Preposition_Object_PhraseCheck-Point ?
?
?
?Reference 1 in this country  DM = 0.5Reference 2 in his country  DM = 0.5System A Translation but the prime minister of thailand Dex-in vowed to continue in domestic thesearch.System B Translation but the prime minister of thailand Dex-in vowed to continue the search in hiscountry.Table 8: An instance of the check-point.System A System BRef 1: Match/Total 1/6 2/6Ref 2: Match/Total 1/6 6/6Score 0.17 1Table 9: N-gram matching rate and scores.Table 10: Diagnose of SMT and RMT.In the second experiment, we diagnose systemC and D and compare the results.
The BLEUscore of system C is 0.3005 and system D is0.2606.
Table 10 shows the diagnostic results oncategories with significant differences.
Scorescalculated with 3 matching options described insection 5 are given (?Lower?
means Lowercase.The scores are listed in the form ?SMTscore/RMT score?).
The diagnostic results indi-cate that system C performs better on most cate-gories than system D, but system D performsbetter on categories like idiom, pronoun and pre-position.
This result reveals a key difference be-tween two types of MT systems: the SMT workswell on the open categories that can be handledby context, while the RMT works well on closedcategories which are easily translated by linguis-tic rules.As the results of two experiments demonstrate,the diagnostic evaluation provides rich informa-tion of the capability of translating various im-portant linguistic categories beyond a single sys-tem score.
It successfully distinguishes the spe-cific difference between the MT systems whosesystem level performance is similar.
It can alsodiagnose the MT system with different architec-tures.
Diagnostic evaluation tells the developersabout the direction to improve the system.
Alongwith the scores of categories, the diagnosticevaluation provides the system translation andreferences at every check-point so that the devel-opers can trace and understand about how theMT system works on every single instance.7 Experiments on Ranking MT SystemsOffering a general evaluation at system level isthe major goal of state-of-the-art evaluation me-thods including widely accepted n-gram metrics.The absence of linguistic knowledge in BLEUmotivated many work to integrate linguistic fea-tures into evaluation metric.
In (Yang 2007), theevaluation of SMT systems is alternately formu-lated as a ranking problem.
Different linguisticfeatures are combined with BLEU such asmatching rate of dependency relations of transla-tion candidates against the reference sentences.The experiments demonstrate that the dependen-cy matching rate feature can increase the rankingaccuracy in some cases.
Compared to dependen-cy structure, the linguistic categories in our ap-proach showcase more extensive features.
Itwould be interesting to see whether the linguisticcategories can be used to further improve theranking of SMT systems.In experiments, we use the scores of linguisticcategories, dependency matching rate, scores ofBLEU and other popular metrics as ranking fea-tures of MT systems and trained by RankingSVM of SVMlight (Joachims, 1998).
We per-formed the ranking experiments on ACL 2005workshop data, ranking 7 MT translations withthree-fold cross-validation both on sentence leveland document level.
The Spearman score is usedType Normal Lower StemAmbiguous word 0.49/0.42 0.50/0.42 0.53/0.46New word 0.13/0.13 0.37/0.32 0.42/0.35Idiom 0.43/0.66 0.46/0.67 0.51/0.71Pronoun 0.60/0.68 0.69/0.75 0.66/0.75Preposition 0.38/0.42 0.42/0.45 0.43/0.46Collocation 0.66/0.54 0.66/0.55 0.70/0.56Subject-predicatephrase0.46/0.30 0.51/0.36 0.58/0.42Predicate-objectphrase0.37/0.25 0.37/0.26 0.47/0.29Compound sentence 0.22/0.16 0.23/0.16 0.23/0.171126to calculate the correlation with human assess-ments.
Table 11 and 12 show the results of thedifferent feature sets on sentence level and doc-ument level respectively.As shown in experiment results linguistic cat-egories (LC), when used alone, are better relatedwith human assessments than BLEU and GTM.When combined with the baseline metrics(BLEU & NIST), LC scores further improve thecorrelation score, better than dependence match-ing rate (DP).
LC scores are obtained by match-ing the exact form of the words as ME-TEOR(exact) does.
NIST+LC combination scoreis better than METEOR(exact) at sentence anddocument level, and also better than ME-TEOR(exact&syn) (syn means wn_synonymymodule in METEOR) at document level.
Thisresults indicate the ability of linguistic features inimproving the performance of ranking task.Mean CorrelationBLEU 4 0.245NIST 5 0.307GTM (e=2) 0.251METEOR(exact) 0.306METEOR(exact&syn) 0.327DP 0.246LC 0.263BLEU+DP 0.270BLEU+ LC 0.288BLEU+ DP +LC 0.307NIST+ LC 0.322NIST+ DP +LC 0.333Table11: Sentence level ranking (DP meansdependency and LC means linguistic categories)Mean CorrelationBLEU 4 0.305NIST 5 0.373GTM (e=2) 0.327METEOR(exact) 0.363METEOR(exact&syn) 0.394DP 0.323LC 0.369BLEU+DP 0.325BLEU+ LC 0.387BLEU+ DP +LC 0.332NIST+ LC 0.409NIST+ DP +LC 0.359Table 12: Document level ranking8 Comparison with Related WorkThis work is inspired by (Yu, 1993) with manyextensions.
(Yu, 1993) proposed MTE evaluationsystem based on check-points for English-Chinese machine translation systems with humancraft linguistic taxonomy including 3,200 pairs ofsentences containing 6 classes of check-points.Their check-points were manually constructed byhuman experts, therefore it will be costly to buildnew test corpus while the check-points in ourapproach are constructed automatically.
Anotherlimitation of their work is that only binary scoreis used for credits while we use n-gram matchingrate which provides a broader coverage of differ-ent levels of matching.There are many recent work motivated by n-gram based approach.
(Callison-Burch et al,2006) criticized the inadequate accuracy of eval-uation at the sentence level.
(Lin and Och, 2004)used longest common subsequence and skip-bigram statistics.
(Banerjee and Lavie, 2005) cal-culated the scores by matching the unigrams onthe surface forms, stemmed forms and senses.
(Liu et al, 2005) used syntactic features and un-labeled head-modifier dependencies to evaluateMT quality, outperforming BLEU on sentencelevel correlations with human judgment.
(Gime-nez and Marquez, 2007) showed that linguisticfeatures at more abstract levels such as depen-dency relation may provide more reliable systemrankings.
(Yang et al, 2007) formulates MTevaluation as a ranking problems leading togreater correlation with human assessment at thesentence level.There are many differences between these n-gram based methods and our approach.
In n-gram approach, a sentence is viewed as a collec-tion of n-grams with different length without dif-ferentiating the specific linguistic phenomena.
Inour approach, a sentence is viewed as a collec-tion of check-points with different types anddepth, conforming to a clear linguistic taxonomy.Furthermore, in n-gram approach, only one gen-eral score at the system level is provided whichmake it not suitable for system diagnoses, whilein our approach we can give scores of linguisticcategories and provide much richer informationto help developers to find the concrete strengthand flaws of the system, in addition to the gener-al score.
The n-gram based metric is not veryeffective when comparing the systems with dif-ferent architectures or systems with similar gen-eral score, while our approach is more effectivein both cases by digging into the multiple lin-guistic levels and disclosing the latent differenc-es of the systems.9 Conclusion and Future WorkThis paper presents an automatically diagnosticevaluation methods on MT based on linguisticcheck-points automatically constructed.
In con-trast with the metrics which only give a generalscore, our evaluation system can give developers1127feedback about the faults and strength of an MTsystem regarding specific linguistic category orcategory group.
Different with the existing workbased on check-points, our work presents an ap-proach to automatically generate the check-pointdatabase.
We show that although there is somenoise brought from word alignment and parsing,we can effectively alleviate the problem by refin-ing the parser results, weighting the referencewith confidence score and providing large quan-tity of check-points.The experiments demonstrate that this methodcan uncover the specific difference between MTsystems with similar architectures and differentarchitectures.
It is also demonstrated that the lin-guistic check-points can be used as new featuresto improve the ranking task of MT systems.Although we present the diagnostic evaluationmethod with Chinese-English language pair, ourapproach can be applied to other language pair ifsyntax parser and word aligner are available.The taxonomy used in current proposal isbased on the human-made linguistic system.
Aninteresting problem to be explored in the future iswhether the taxonomy could be constructed au-tomatically from the parsing results.ReferencesStatanjeev Banerjee, Alon Lavie.
2005.
METEOR: AnAutomatic Metric for MT Evaluation with Im-proved Correlation with Human Judgements.
InProceedings of the ACL Workshop on Intrinsic andExtrinsic Evaluation Measures for Machine Trans-lation and/or Summarization 2005.Chris Callison-Burch, Miles Osborne, Philipp Koehn.2006.
Re-evaluating the Role of Bleu in MachineTranslation Research.
In Proceedings of the Euro-pean Chapter of the ACL 2006.Martin Chodorow, Claudia Leacock.
2000.
An unsu-pervised method for detecting grammatical errors,In 1st Meeting of the North America Chapter of theACL, pp.140?147, 2000.Thorsten Joachims.
1998.
Making Large-scale Sup-port Vector Machine Learning Practical, In B.Scholkopf, C. Burges, A. Smola.
Advances in Ker-nel Methods: Support VectorMachines, MIT Press,Cambridge, MA, December.Jesus Gimenez and Llis Marquez.
2007.
Linguisticfeatures for automatic evaluation of heterogeneousMT systems, Workshop of statistical machine trans-lation in conjunction with 45th ACL, 2007.Dan Klein, Christopher Manning.
2003.
AccurateUnlexicalized Parsing, Proceedings of the 41thMeeting of the ACL, pp.
423-430.Philipp Koehn.
2004.
Statistical Significance Tests forMachine Translation Evaluation.
In Proc.
of theEMNLP, Barcelona, Spain.Chiho Li, Minghui Li, Dongdong Zhang, Mu Li,Ming Zhou, Yi Guan.
2007.
A Probabilistic Ap-proach to Syntax-based Reor-dering for SMT.
InProceedings of the 45th  ACL, 2007.Chin-Yew Lin and Franz Josef Och.
2004.
Automaticevaluation of machine translation quality usinglongest common subsequence and skip-bigram sta-tistics.
In Proceedings of the 42th ACL 2004.Ding Liu, Daniel Gildea.
2005.
Syntactic Features forEvaluation of Machine Translation, ACL Work-shop on Intrinsic and Extrinsic Evaluation Meas-ures for Machine Translation and/or Summariza-tion.Shuxin Liu.
2002.
Linguistics of Contemporary Chi-nese Language (in Chinese), Advanced EducationPublisher.Jiping Lv.
2000.
Foundation of Mandarin Grammar(in Chinese), Shangwu Publisher.Franz Josef Och, Hermann Ney.
2003.
A SystematicComparison of Various Statistical Alignment Mod-els, Computational Linguistics, volume 29, number1, pp.
19-51 March 2003.Kishore Papieni, Salim Roukos, Todd Ward, Wei-JingZhu.
2002.
BLEU: a method for automatic evalua-tion of machine translation, In Proceedings of theACL 2002.Shiwen Yu.
1993.
Automatic evaluation of outputquality for machine translation systems, In Pro-ceedings of the evaluators?
forum, April 21-24,1991, Les Rasses, Vaud, 1993.Yang Ye, Ming Zhou, Chinyew Lin.
2007.
Sentencelevel machine translation evaluation as a rankingproblem: one step aside from BLEU, In Workshopof statistical machine translation, in conjunctionwith 45th ACL, 2007.Ming Zhou.
2000, A Block-Based Robust DependencyParser for Unrestricted Chinese Text.
Proceedingsof Second Chinese Language Processing Workshop,2000, held in conjunction with ACL, 2000.1128
