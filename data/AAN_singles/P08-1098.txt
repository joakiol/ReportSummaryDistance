Proceedings of ACL-08: HLT, pages 861?869,Columbus, Ohio, USA, June 2008. c?2008 Association for Computational LinguisticsMulti-Task Active Learning for Linguistic AnnotationsRoi Reichart1?
Katrin Tomanek2?
Udo Hahn2 Ari Rappoport11Institute of Computer ScienceHebrew University of Jerusalem, Israel{roiri|arir}@cs.huji.ac.il2Jena University Language & Information Engineering (JULIE) LabFriedrich-Schiller-Universita?t Jena, Germany{katrin.tomanek|udo.hahn}@uni-jena.deAbstractWe extend the classical single-task activelearning (AL) approach.
In the multi-task ac-tive learning (MTAL) paradigm, we select ex-amples for several annotation tasks rather thanfor a single one as usually done in the con-text of AL.
We introduce two MTAL meta-protocols, alternating selection and rank com-bination, and propose a method to implementthem in practice.
We experiment with a two-task annotation scenario that includes namedentity and syntactic parse tree annotations onthree different corpora.
MTAL outperformsrandom selection and a stronger baseline, one-sided example selection, in which one task ispursued using AL and the selected examplesare provided also to the other task.1 IntroductionSupervised machine learning methods have success-fully been applied to many NLP tasks in the last fewdecades.
These techniques have demonstrated theirsuperiority over both hand-crafted rules and unsu-pervised learning approaches.
However, they re-quire large amounts of labeled training data for everylevel of linguistic processing (e.g., POS tags, parsetrees, or named entities).
When, when domainsand text genres change (e.g., moving from common-sense newspapers to scientific biology journal arti-cles), extensive retraining on newly supplied train-ing material is often required, since different do-mains may use different syntactic structures as wellas different semantic classes (entities and relations).?
Both authors contributed equally to this work.Consequently, with an increasing coverage of awide variety of domains in human language tech-nology (HLT) systems, we can expect a growingneed for manual annotations to support many kindsof application-specific training data.Creating annotated data is extremely labor-intensive.
The Active Learning (AL) paradigm(Cohn et al, 1996) offers a promising solution todeal with this bottleneck, by allowing the learningalgorithm to control the selection of examples tobe manually annotated such that the human label-ing effort be minimized.
AL has been successfullyapplied already for a wide range of NLP tasks, in-cluding POS tagging (Engelson and Dagan, 1996),chunking (Ngai and Yarowsky, 2000), statisticalparsing (Hwa, 2004), and named entity recognition(Tomanek et al, 2007).However, AL is designed in such a way that it se-lects examples for manual annotation with respect toa single learning algorithm or classifier.
Under thisAL annotation policy, one has to perform a separateannotation cycle for each classifier to be trained.
Inthe following, we will refer to the annotations sup-plied for a classifier as the annotations for a singleannotation task.Modern HLT systems often utilize annotations re-sulting from different tasks.
For example, a machinetranslation system might use features extracted fromparse trees and named entity annotations.
For suchan application, we obviously need the different an-notations to reside in the same text corpus.
It is notclear how to apply the single-task AL approach here,since a training example that is beneficial for onetask might not be so for others.
We could annotate861the same corpus independently by the two tasks andmerge the resulting annotations, but that (as we showin this paper) would possibly yield sub-optimal us-age of human annotation efforts.There are two reasons why multi-task AL, andby this, a combined corpus annotated for varioustasks, could be of immediate benefit.
First, annota-tors working on similar annotation tasks (e.g., con-sidering named entities and relations between them),might exploit annotation data from one subtask forthe benefit of the other.
If for each subtask a sepa-rate corpus is sampled by means of AL, annotatorswill definitely lack synergy effects and, therefore,annotation will be more laborious and is likely tosuffer in terms of quality and accuracy.
Second, fordissimilar annotation tasks ?
take, e.g., a compre-hensive HLT pipeline incorporating morphological,syntactic and semantic data ?
a classifier might re-quire features as input which constitute the outputof another preceding classifier.
As a consequence,training such a classifier which takes into accountseveral annotation tasks will best be performed ona rich corpus annotated with respect to all input-relevant tasks.
Both kinds of annotation tasks, simi-lar and dissimilar ones, constitute examples of whatwe refer to as multi-task annotation problems.Indeed, there have been efforts in creating re-sources annotated with respect to various annotationtasks though each of them was carried out indepen-dently of the other.
In the general language UPennannotation efforts for the WSJ sections of the PennTreebank (Marcus et al, 1993), sentences are anno-tated with POS tags, parse trees, as well as discourseannotation from the Penn Discourse Treebank (Milt-sakaki et al, 2008), while verbs and verb argumentsare annotated with Propbank rolesets (Palmer et al,2005).
In the biomedical GENIA corpus (Ohta etal., 2002), scientific text is annotated with POS tags,parse trees, and named entities.In this paper, we introduce multi-task activelearning (MTAL), an active learning paradigm formultiple annotation tasks.
We propose a new ALframework where the examples to be annotated areselected so that they are as informative as possiblefor a set of classifiers instead of a single classifieronly.
This enables the creation of a single combinedcorpus annotated with respect to various annotationtasks, while preserving the advantages of AL withrespect to the minimization of annotation efforts.In a proof-of-concept scenario, we focus on twohighly dissimilar tasks, syntactic parsing and namedentity recognition, study the effects of multi-task ALunder rather extreme conditions.
We propose twoMTAL meta-protocols and a method to implementthem for these tasks.
We run experiments on threecorpora for domains and genres that are very differ-ent (WSJ: newspapers, Brown: mixed genres, andGENIA: biomedical abstracts).
Our protocols out-perform two baselines (random and a stronger one-sided selection baseline).In Section 2 we introduce our MTAL frameworkand present two MTAL protocols.
In Section 3 wediscuss the evaluation of these protocols.
Section4 describes the experimental setup, and results arepresented in Section 5.
We discuss related work inSection 6.
Finally, we point to open research issuesfor this new approach in Section 7.2 A Framework for Multi-Task ALIn this section we introduce a sample selectionframework that aims at reducing the human anno-tation effort in a multiple annotation scenario.2.1 Task DefinitionTo measure the efficiency of selection methods, wedefine the training quality TQ of annotated mate-rial S as the performance p yielded with a referencelearner X trained on that material: TQ(X, S) = p.A selection method can be considered better than an-other one if a higher TQ is yielded with the sameamount of examples being annotated.Our framework is an extension of the ActiveLearning (AL) framework (Cohn et al, 1996)).
Theoriginal AL framework is based on querying in an it-erative manner those examples to be manually anno-tated that are most useful for the learner at hand.
TheTQ of an annotated corpus selected by means of ALis much higher than random selection.
This AL ap-proach can be considered as single-task AL becauseit focuses on a single learner for which the examplesare to be selected.
In a multiple annotation scenario,however, there are several annotation tasks to be ac-complished at once and for each task typically a sep-arate statistical model will then be trained.
Thus, thegoal of multi-task AL is to query those examples for862human annotation that are most informative for alllearners involved.2.2 One-Sided Selection vs. Multi-Task ALThe naive approach to select examples in a multipleannotation scenario would be to perform a single-task AL selection, i.e., the examples to be annotatedare selected with respect to one of the learners only.1In a multiple annotation scenario we call such an ap-proach one-sided selection.
It is an intrinsic selec-tion for the reference learner, and an extrinsic selec-tion for all the other learners also trained on the an-notated material.
Obviously, a corpus compiled withthe help of one-sided selection will have a good TQfor that learner for which the intrinsic selection hastaken place.
For all the other learners, however, wehave no guarantee that their TQ will not be inferiorthan the TQ of a random selection process.In scenarios where the different annotation tasksare highly dissimilar we can expect extrinsic selec-tion to be rather poor.
This intuition is demonstratedby experiments we conducted for named entity (NE)and parse annotation tasks2 (Figure 1).
In this sce-nario, extrinsic selection for the NE annotation taskmeans that examples where selected with respectto the parsing task.
Extrinsic selection performedabout the same as random selection for the NE task,while for the parsing task extrinsic selection per-formed markedly worse.
This shows that examplesthat were very informative for the NE learner werenot that informative for the parse learner.2.3 Protocols for Multi-Task ALObviously, we can expect one-sided selection to per-form better for the reference learner (the one forwhich an intrinsic selection took place) than multi-task AL selection, because the latter would be acompromise for all learners involved in the multi-ple annotation scenario.
However, the goal of multi-task AL is to minimize the annotation effort over allannotation tasks and not just the effort for a singleannotation task.For a multi-task AL protocol to be valuable in aspecific multiple annotation scenario, the TQ for allconsidered learners should be1Of course, all selected examples would be annotated w.r.t.all annotation tasks.2See Section 4 for our experimental setup.1.
better than the TQ of random selection,2.
and better than the TQ of any extrinsic selec-tion.In the following, we introduce two protocols formulti-task AL.
Multi-task AL protocols can be con-sidered meta-protocols because they basically spec-ify how task-specific, single-task AL approaches canbe combined into one selection decision.
By this,the protocols are independent of the underlying task-specific AL approaches.2.3.1 Alternating SelectionThe alternating selection protocol alternates one-sided AL selection.
In sj consecutive AL iterations,the selection is performed as one-sided selectionwith respect to learning algorithm Xj .
After that,another learning algorithm is considered for selec-tion for sk consecutive iterations and so on.
Depend-ing on the specific scenario, this enables to weightthe different annotation tasks by allowing them toguide the selection in more or less AL iterations.This protocol is a straight-forward compromise be-tween the different single-task selection approaches.In this paper we experiment with the special caseof si = 1, where in every AL iteration the selectionleadership is changed.
More sophisticated calibra-tion of the parameters si is beyond the scope of thispaper and will be dealt with in future work.2.3.2 Rank CombinationThe rank combination protocol is more directlybased on the idea to combine single-task AL selec-tion decisions.
In each AL iteration, the usefulnessscore sXj (e) of each unlabeled example e from thepool of examples is calculated with respect to eachlearner Xj and then translated into a rank rXj (e)where higher usefulness means lower rank number(examples with identical scores get the same ranknumber).
Then, for each example, we sum the ranknumbers of each annotation task to get the overallrank r(e) = ?nj=1 rXj (e).
All examples are sortedby this combined rank and b examples with lowestrank numbers are selected for manual annotation.33As the number of ranks might differ between the single an-notation tasks, we normalize them to the coarsest scale.
Thenwe can sum up the ranks as explained above.86310000 20000 30000 40000 500000.650.700.750.80tokensf?scorerandom selectionextrinsic selection (PARSE?AL)10000 20000 30000 400000.760.780.800.820.84constituentsf?scorerandom selectionextrinsic selection (NE?AL)Figure 1: Learning curves for random and extrinsic selection on both tasks: named entity annotation (left) and syntacticparse annotation (right), using the WSJ corpus scenarioThis protocol favors examples which are good forall learning algorithms.
Examples that are highly in-formative for one task but rather uninformative foranother task will not be selected.3 Evaluation of Multi-Task ALThe notion of training quality (TQ) can be used toquantify the effectiveness of a protocol, and by this,annotation costs in a single-task AL scenario.
To ac-tually quantify the overall training quality in a multi-ple annotation scenario one would have to sum overall the single task?s TQs.
Of course, depending onthe specific annotation task, one would not want toquantify the number of examples being annotatedbut different task-specific units of annotation.
Whilefor entity annotations one does typically count thenumber of tokens being annotated, in the parsingscenario the number of constituents being annotatedis a generally accepted measure.
As, however, theactual time needed for the annotation of one exam-ple usually differs for different annotation tasks, nor-malizing exchange rates have to be specified whichcan then be used as weighting factors.
In this paper,we do not define such weighting factors4, and leavethis challenging question to be discussed in the con-text of psycholinguistic research.We could quantify the overall efficiency score Eof a MTAL protocol P byE(P ) =n?j=1?j ?
TQ(Xj , uj)where uj denotes the individual annotation task?s4Such weighting factors not only depend on the annotationlevel or task but also on the domain, and especially on the cog-nitive load of the annotation task.number of units being annotated (e.g., constituentsfor parsing) and the task-specific weights are definedby ?j .
Given weights are properly defined, such ascore can be applied to directly compare differentprotocols and quantify their differences.In practice, such task-specific weights might alsobe considered in the MTAL protocols.
In the alter-nating selection protocol, the numbers of consecu-tive iterations si each single task protocol can betuned according to the ?
parameters.
As for therank combination protocol, the weights can be con-sidered when calculating the overall rank: r(e) =?nj=1 ?j ?
rXj (e) where the parameters ?1 .
.
.
?n re-flect the values of ?1 .
.
.
?n (though they need notnecessarily be the same).In our experiments, we assumed the same weightfor all annotation schemata, thus simply setting si =1, ?i = 1.
This was done for the sake of a clearframework presentation.
Finding proper weights forthe single tasks and tuning the protocols accordinglyis a subject for further research.4 Experiments4.1 Scenario and Task-Specific SelectionProtocolsThe tasks in our scenario comprise one semantictask (annotation with named entities (NE)) and onesyntactic task (annotation with PCFG parse trees).The tasks are highly dissimilar, thus increasing thepotential value of MTAL.
Both tasks are subject tointensive research by the NLP community.The MTAL protocols proposed are meta-protocols that combine the selection decisions ofthe underlying, task-specific AL protocols.
Inour scenario, the task-specific AL protocols are864committee-based (Freund et al, 1997) selectionprotocols.
In committee-based AL, a committeeconsists of k classifiers of the same type trainedon different subsets of the training data.5 Eachcommittee member then makes its predictions onthe unlabeled examples, and those examples onwhich the committee members disagree most areconsidered most informative for learning and arethus selected for manual annotation.
In our scenariothe example grain-size is the sentence level.For the NE task, we apply the AL approach ofTomanek et al (2007).
The committee consists ofk1 = 3 classifiers and the vote entropy (VE) (Engel-son and Dagan, 1996) is employed as disagreementmetric.
It is calculated on the token-level asV Etok(t) = ?1log kc?i=0V (li, t)k logV (li, t)k (1)where V (li,t)k is the ratio of k classifiers where thelabel li is assigned to a token t. The sentence levelvote entropy V Esent is then the average over all to-kens tj of sentence s.For the parsing task, the disagreement score isbased on a committee of k2 = 10 instances of DanBikel?s reimplementation of Collins?
parser (Bickel,2005; Collins, 1999).
For each sentence in the un-labeled pool, the agreement between the committeemembers was calculated using the function reportedby Reichart and Rappoport (2007):AF (s) = 1N?i,l?
[1...N ],i6=lfscore(mi, ml) (2)Where mi and ml are the committee members andN = k2?
(k2?1)2 is the number of pairs of differentcommittee members.
This function calculates theagreement between the members of each pair by cal-culating their relative f-score and then averages thepairs?
scores.
The disagreement of the committee ona sentence is simply 1 ?
AF (s).4.2 Experimental settingsFor the NE task we employed the classifier describedby Tomanek et al (2007): The NE tagger is based onConditional Random Fields (Lafferty et al, 2001)5We randomly sampled L = 34 of the training data to createeach committee member.and has a rich feature set including orthographical,lexical, morphological, POS, and contextual fea-tures.
For parsing, Dan Bikel?s reimplementation ofCollins?
parser is employed, using gold POS tags.In each AL iteration we select 100 sentences formanual annotation.6 We start with a randomly cho-sen seed set of 200 sentences.
Within a corpus weused the same seed set in all selection scenarios.
Wecompare the following five selection scenarios: Ran-dom selection (RS), which serves as our baseline;one-sided AL selection for both tasks (called NE-ALand PARSE-AL); and multi-task AL selection withthe alternating selection protocol (alter-MTAL) andthe rank combination protocol (ranks-MTAL).We performed our experiments on three dif-ferent corpora, namely one from the newspapergenre (WSJ), a mixed-genre corpus (Brown), and abiomedical corpus (Bio).
Our simulation corporacontain both entity annotations and (constituent)parse annotations.
For each corpus we have a poolset (from which we select the examples for annota-tion) and an evaluation set (used for generating thelearning curves).
The WSJ corpus is based on theWSJ part of the PENN TREEBANK (Marcus et al,1993); we used the first 10,000 sentences of section2-21 as the pool set, and section 00 as evaluation set(1,921 sentences).
The Brown corpus is also basedon the respective part of the PENN TREEBANK.
Wecreated a sample consisting of 8 of any 10 consec-utive sentences in the corpus.
This was done asBrown contains text from various English text gen-res, and we did that to create a representative sampleof the corpus domains.
We finally selected the first10,000 sentences from this sample as pool set.
Every9th from every 10 consecutive sentences packagewent into the evaluation set which consists of 2,424sentences.
For both WSJ and Brown only parse an-notations though no entity annotations were avail-able.
Thus, we enriched both corpora with entityannotations (three entities: person, location, and or-ganization) by means of a tagger trained on the En-glish data set of the CoNLL-2003 shared task (TjongKim Sang and De Meulder, 2003).7 The Bio corpus6Manual annotation is simulated by just unveiling the anno-tations already contained in our corpora.7We employed a tagger similar to the one presented by Set-tles (2004).
Our tagger has a performance of ?
84% f-score onthe CoNLL-2003 data; inspection of the predicted entities on865is based on the parsed section of the GENIA corpus(Ohta et al, 2002).
We performed the same divi-sions as for Brown, resulting in 2,213 sentences inour pool set and 276 sentences for the evaluation set.This part of the GENIA corpus comes with entity an-notations.
We have collapsed the entity classes an-notated in GENIA (cell line, cell type, DNA, RNA,protein) into a single, biological entity class.5 ResultsIn this section we present and discuss our resultswhen applying the five selection strategies (RS, NE-AL, PARSE-AL, alter-MTAL, and ranks-MTAL) toour scenario on the three corpora.
We refrain fromcalculating the overall efficiency score (Section 3)here due to the lack of generally accepted weightsfor the considered annotation tasks.
However, werequire from a good selection protocol to exceed theperformance of random selection and extrinsic se-lection.
In addition, recall from Section 3 that weset the alternate selection and rank combination pa-rameters to si = 1, ?i = 1, respectively to reflect atradeoff between the annotation efforts of both tasks.Figures 2 and 3 depict the learning curves forthe NE tagger and the parser on WSJ and Brown,respectively.
Each figure shows the five selectionstrategies.
As expected, on both corpora and bothtasks intrinsic selection performs best, i.e., for theNE tagger NE-AL and for the parser PARSE-AL.Further, random selection and extrinsic selectionperform worst.
Most importantly, both MTAL pro-tocols clearly outperform extrinsic and random se-lection in all our experiments.
This is in contrastto NE-AL which performs worse than random se-lection for all corpora when used as extrinsic selec-tion, and for PARSE-AL that outperforms the ran-dom baseline only for Brown when used as extrin-sic selection.
That is, the MTAL protocols suggest atradeoff between the annotation efforts of the differ-ent tasks, here.On WSJ, both for the NE and the parse annotationtasks, the performance of the MTAL protocols isvery similar, though ranks-MTAL performs slightlybetter.
For the parser task, up to 30,000 constituentsMTAL performs almost as good as does PARSE-AL.
This is different for the NE task where NE-ALWSJ and Brown revealed a good tagging performance.clearly outperforms MTAL.
On Brown, in generalwe see the same results, with some minor differ-ences.
On the NE task, extrinsic selection (PARSE-AL) performs better than random selection, but it isstill much worse than intrinsic AL or MTAL.
Here,ranks-MTAL significantly outperforms alter-MTALand almost performs as good as intrinsic selection.For the parser task, we see that extrinsic and ran-dom selection are equally bad.
Both MTAL proto-cols perform equally well, again being quite similarto the intrinsic selection.
On the BIO corpus8 we ob-served the same tendencies as in the other two cor-pora, i.e., MTAL clearly outperforms extrinsic andrandom selection and supplies a better tradeoff be-tween annotation efforts of the task at hand than one-sided selection.Overall, we can say that in all scenarios MTALperforms much better than random selection and ex-trinsic selection, and in most cases the performanceof MTAL (especially but not exclusively, ranks-MTAL) is even close to intrinsic selection.
This ispromising evidence that MTAL selection can be abetter choice than one-sided selection in multiple an-notation scenarios.
Thus, considering all annotationtasks in the selection process (even if the selectionprotocol is as simple as the alternating selection pro-tocol) is better than selecting only with respect toone task.
Further, it should be noted that overall themore sophisticated rank combination protocol doesnot perform much better than the simpler alternatingselection protocol in all scenarios.Finally, Figure 4 shows the disagreement curvesfor the two tasks on the WSJ corpus.
As has alreadybeen discussed by Tomanek and Hahn (2008), dis-agreement curves can be used as a stopping crite-rion and to monitor the progress of AL-driven an-notation.
This is especially valuable when no anno-tated validation set is available (which is needed forplotting learning curves).
We can see that the dis-agreement curves significantly flatten approximatelyat the same time as the learning curves do.
In thecontext of MTAL, disagreement curves might notonly be interesting as a stopping criterion but ratheras a switching criterion, i.e., to identify when MTALcould be turned into one-sided selection.
This wouldbe the case if in an MTAL scenario, the disagree-8The plots for the Bio are omitted due to space restrictions.86610000 20000 30000 40000 500000.650.700.750.800.85tokensf?scoreRSNE?ALPARSE?ALalter?MTALranks?MTAL5000 10000 15000 20000 25000 300000.550.600.650.700.750.80tokensf?scoreRSNE?ALPARSE?ALalter?MTALranks?MTALFigure 2: Learning curves for NE task on WSJ (left) and Brown (right)10000 20000 30000 400000.760.780.800.820.84constituentsf?scoreRSNE?ALPARSE?ALalter?MTALranks?MTAL5000 10000 15000 20000 25000 30000 350000.650.700.750.80constituentsf?scoreRSNE?ALPARSE?ALalter?MTALranks?MTALFigure 3: Learning curves for parse task on WSJ (left) and Brown (right)ment curve of one task has a slope of (close to) zero.Future work will focus on issues related to this.6 Related WorkThere is a large body of work on single-task AL ap-proaches for many NLP tasks where the focus ismainly on better, task-specific selection protocolsand methods to quantify the usefulness score in dif-ferent scenarios.
As to the tasks involved in ourscenario, several papers address AL for NER (Shenet al, 2004; Hachey et al, 2005; Tomanek et al,2007) and syntactic parsing (Tang et al, 2001; Hwa,2004; Baldridge and Osborne, 2004; Becker and Os-borne, 2005).
Further, there is some work on ques-tions arising when AL is to be used in real-life anno-tation scenarios, including impaired inter-annotatoragreement, stopping criteria for AL-driven annota-tion, and issues of reusability (Baldridge and Os-borne, 2004; Hachey et al, 2005; Zhu and Hovy,2007; Tomanek et al, 2007).Multi-task AL is methodologically related to ap-proaches of decision combination, especially in thecontext of classifier combination (Ho et al, 1994)and ensemble methods (Breiman, 1996).
Those ap-proaches focus on the combination of classifiers inorder to improve the classification error rate for onespecific classification task.
In contrast, the focus ofmulti-task AL is on strategies to select training ma-terial for multi classifier systems where all classifierscover different classification tasks.7 DiscussionOur treatment of MTAL within the context of theorthogonal two-task scenario leads to further inter-esting research questions.
First, future investiga-tions will have to focus on the question whetherthe positive results observed in our orthogonal (i.e.,highly dissimilar) two-task scenario will also holdfor a more realistic (and maybe more complex) mul-tiple annotation scenario where tasks are more sim-ilar and more than two annotation tasks might beinvolved.
Furthermore, several forms of interde-pendencies may arise between the single annotationtasks.
As a first example, consider the (functional)interdependencies (i.e., task similarity) in higher-level semantic NLP tasks of relation or event recog-nition.
In such a scenario, several tasks includingentity annotations and relation/event annotations, aswell as syntactic parse data, have to be incorporatedat the same time.
Another type of (data flow) inter-86710000 20000 30000 400000.0100.0140.018tokensdisagreementRSNE?ALPARSE?ALalter?MTALranks?MTAL10000 20000 30000 40000510152025303540constituentsdisagreementRSNE?ALPARSE?ALalter?MTALranks?MTALFigure 4: Disagreement curves for NE task (left) and parse task (right) on WSJdependency occurs in a second scenario where ma-terial for several classifiers that are data-dependenton each other ?
one takes the output of another clas-sifier as input features ?
has to be efficiently anno-tated.
Whether the proposed protocols are beneficialin the context of such highly interdependent tasks isan open issue.
Even more challenging is the ideato provide methodologies helping to predict in anarbitrary application scenario whether the choice ofMTAL is truly advantageous.Another open question is how to measure andquantify the overall annotation costs in multiple an-notation scenarios.
Exchange rates are inherentlytied to the specific task and domain.
In practice, onemight just want to measure the time needed for theannotations.
However, in a simulation scenario, acommon metric is necessary to compare the perfor-mance of different selection strategies with respectto the overall annotation costs.
This requires stud-ies on how to quantify, with a comparable cost func-tion, the efforts needed for the annotation of a textualunit of choice (e.g., tokens, sentences) with respectto different annotation tasks.Finally, the question of reusability of the anno-tated material is an important issue.
Reusability inthe context of AL means to which degree corporaassembled with the help of any AL technique can be(re)used as a general resource, i.e., whether they arewell suited for the training of classifiers other thanthe ones used during the selection process.This isespecially interesting as the details of the classifiersthat should be trained in a later stage are typicallynot known at the resource building time.
Thus, wewant to select samples valuable to a family of clas-sifiers using the various annotation layers.
This, ofcourse, is only possible if data annotated with thehelp of AL is reusable by modified though similarclassifiers (e.g., with respect to the features beingused) ?
compared to the classifiers employed for theselection procedure.The issue of reusability has already been raisedbut not yet conclusively answered in the context ofsingle-task AL (see Section 6).
Evidence was foundthat reusability up to a certain, though not well-specified, level is possible.
Of course, reusabilityhas to be analyzed separately in the context of var-ious MTAL scenarios.
We feel that these scenariosmight both be more challenging and more relevantto the reusability issue than the single-task AL sce-nario, since resources annotated with multiple lay-ers can be used to the design of a larger number of a(possibly more complex) learning algorithms.8 ConclusionsWe proposed an extension to the single-task AL ap-proach such that it can be used to select examples forannotation with respect to several annotation tasks.To the best of our knowledge this is the first paper onthis issue, with a focus on NLP tasks.
We outlineda problem definition and described a framework formulti-task AL.
We presented and tested two proto-cols for multi-task AL.
Our results are promising asthey give evidence that in a multiple annotation sce-nario, multi-task AL outperforms naive one-sidedand random selection.AcknowledgmentsThe work of the second author was funded by theGerman Ministry of Education and Research withinthe STEMNET project (01DS001A-C), while thework of the third author was funded by the ECwithin the BOOTSTREP project (FP6-028099).868ReferencesJason Baldridge and Miles Osborne.
2004.
Active learn-ing and the total cost of annotation.
In Proceedings ofEMNLP?04, pages 9?16.Markus Becker and Miles Osborne.
2005.
A two-stagemethod for active learning of statistical grammars.
InProceedings of IJCAI?05, pages 991?996.Daniel M. Bickel.
2005.
Code developed at the Univer-sity of Pennsylvania, http://www.cis.upenn.edu/?dbikel/software.html.Leo Breiman.
1996.
Bagging predictors.
MachineLearning, 24(2):123?140.David A. Cohn, Zoubin Ghahramani, and Michael I. Jor-dan.
1996.
Active learning with statistical models.Journal of Artificial Intelligence Research, 4:129?145.Michael Collins.
1999.
Head-driven statistical modelsfor natural language parsing.
Ph.D. thesis, Universityof Pennsylvania.Sean Engelson and Ido Dagan.
1996.
Minimizing man-ual annotation cost in supervised training from cor-pora.
In Proceedings of ACL?96, pages 319?326.Yoav Freund, Sebastian Seung, Eli Shamir, and NaftaliTishby.
1997.
Selective sampling using the queryby committee algorithm.
Machine Learning, 28(2-3):133?168.Ben Hachey, Beatrice Alex, and Markus Becker.
2005.Investigating the effects of selective sampling on theannotation task.
In Proceedings of CoNLL?05, pages144?151.Tin Kam Ho, Jonathan J.
Hull, and Sargur N. Srihari.1994.
Decision combination in multiple classifier sys-tems.
IEEE Transactions on Pattern Analysis and Ma-chine Intelligence, 16(1):66?75.Rebecca Hwa.
2004.
Sample selection for statisticalparsing.
Computational Linguistics, 30(3):253?276.John D. Lafferty, Andrew McCallum, and FernandoPereira.
2001.
Conditional Random Fields: Proba-bilistic models for segmenting and labeling sequencedata.
In Proceedings of ICML?01, pages 282?289.Mitchell P. Marcus, Beatrice Santorini, and Mary AnnMarcinkiewicz.
1993.
Building a large annotated cor-pus of English: The Penn Treebank.
ComputationalLinguistics, 19(2):313?330.Eleni Miltsakaki, Livio Robaldo, Alan Lee, and Ar-avind K. Joshi.
2008.
Sense annotation in the penndiscourse treebank.
In Proceedings of CICLing?08,pages 275?286.Grace Ngai and David Yarowsky.
2000.
Rule writingor annotation: Cost-efficient resource usage for basenoun phrase chunking.
In Proceedings of ACL?00,pages 117?125.Tomoko Ohta, Yuka Tateisi, and Jin-Dong Kim.
2002.The GENIA corpus: An annotated research abstractcorpus in molecular biology domain.
In Proceedingsof HLT?02, pages 82?86.Martha Palmer, Paul Kingsbury, and Daniel Gildea.2005.
The Proposition Bank: An annotated corpus ofsemantic roles.
Computational Linguistics, 31(1):71?106.Roi Reichart and Ari Rappoport.
2007.
An ensemblemethod for selection of high quality parses.
In Pro-ceedings of ACL?07, pages 408?415, June.Burr Settles.
2004.
Biomedical named entity recognitionusing conditional random fields and rich feature sets.In Proceedings of JNLPBA?04, pages 107?110.Dan Shen, Jie Zhang, Jian Su, Guodong Zhou, andChew Lim Tan.
2004.
Multi-criteria-based activelearning for named entity recognition.
In Proceedingsof ACL?04, pages 589?596.Min Tang, Xiaoqiang Luo, and Salim Roukos.
2001.
Ac-tive learning for statistical natural language parsing.
InProceedings of ACL?02, pages 120?127.Erik F. Tjong Kim Sang and Fien De Meulder.2003.
Introduction to the CONLL-2003 shared task:Language-independent named entity recognition.
InProceedings of CoNLL?03, pages 142?147.Katrin Tomanek and Udo Hahn.
2008.
Approximatinglearning curves for active-learning-driven annotation.In Proceedings of LREC?08.Katrin Tomanek, Joachim Wermter, and Udo Hahn.2007.
An approach to text corpus construction whichcuts annotation costs and maintains corpus reusabil-ity of annotated data.
In Proceedings of EMNLP-CoNLL?07, pages 486?495.Jingbo Zhu and Eduard Hovy.
2007.
Active learning forword sense disambiguation with methods for address-ing the class imbalance problem.
In Proceedings ofEMNLP-CoNLL?07, pages 783?790.869
