Yet Another Chart-Based Technique for Parsing Ill-Formed InputTsuneaki KatoNTT Information and Communication Systems Laboratories1-2356 Take, Yokosuka-shi, Kanagawa, 238-03 JAPANkato@nttnly.ntt.jpAbstractA new chart-based technique for parsing ill-formedinput is proposed.
This can process sentenceswith unknown/misspelled words, omitted wordsor extraneous words.
This generalized parsingstrategy is, similar to Mellish's, based on anactive chart parser, and shares the manyadvantages of Mellish's technique.
It is based onpure syntactic knowledge, it is independent ofallgrammars, and it does not slow down the originalparsing operation if there is no ill-formedness.However, unlike Mellish's technique, it doesn'temploy any complicated heuristic parameters.There are two key points.
First, instead of usinga unified or interleaved process for finding errorsand correcting them, we separate the initial errordetection stage from the other stages and adopt aversion of bi-directional parsing.
This effectivelyprunes the search space.
Second, it employsnormal top-down parsing, in which each parsingstate reflects the global context, instead of top-down chart parsing.
This enables the technique todetermine the global plausibility of candidateseasily, based on an admissible A* search.
Theproposed strategy could enumerate all possibleminimal-penalty solutions in just 4 times thetime taken to parse the correct sentences.1 IntroductionIt is important that natural anguage interface systemshave the capability of composing the globally mostplausible explanation if a given input can not besyntactically parsed.
This would be useful for handlingerroneous inputs from the user and for offsettinggrammar and lexicon insufficiency.
Also, such acapability could be applied to deal with theungrammatical sentences and sentence fragments thatfrequently appear in spoken dialogs (Bear, Dowding andShriberg, 1992).
Several efforts have been conducted toachieve this objective ((Lang, 1988; Saito and Tomita,1988), for example.)
One major decision to be made indesigning this capability is whether knowledge otherthan purely syntactic knowledge is to be used.
Other-than syntactic knowledge includes grammar specificrecovery rules such as recta-rules (Weishedel andSondheimer, 1983), semantic or pragmatic knowledgewhich may depend on a particular domain (Carbonell andHayes, 1983) or the characteristics of the ill-formedutterances observed in human discourse (Hindle, 1983).Although it is obvious that the utilizing such knowledgeallows us to devise more powerful strategies, we shouldfirst determine the effectiveness of using only syntacticknowledge.
Moreover, the result can be applied widely,as using syntactic knowledge is a base of the most ofstrategies.One significant advance in the usage of syntacticknowledge was contained in the technique proposed byMellish (1989).
It can handle not onlyunknown/misspelled words, but also omitted words andextraneous words in sentences.
It can deal with suchproblems, and develop plausible explanations quicklysince it utilizes the full syntactic ontext by using anactive chart parser (Kay, 1980; Gazdar and Mellish,1989).
One problem with his technique is that itsperformance heavily depends on how the searchheuristics, which is implemented as a score calculatedfrom six parameters, i  set.
The heuristics complicatesthe algorithm significantly.
This must be one of reasonswhy the performance of the method, as Mellish himselfnoted, dropped dramatically when the input containsmultiple rrors.This paper proposes a new technique for parsinginputs that contain simple kinds of ill-formedness.
Thisgeneralized parsing strategy is, similar to Mellish's,based on an active chart parser, and so shares the manyadvantages of Mellish's technique.
It is based on puresyntactics, it is independent of all grammars, and it doesnot slow down the original parsing operation if there isno iU-formedness.
However, unlike Mellish's technique,it doesn't employ any complicated heuristic parameters.There are two key points.
First, instead of using aunified or interleaved process for finding errors andcorrecting them, we separate the initial error detectionstage from the other stages and adopt a version of bi-directional parsing, which has been pointed out to be auseful strategy for fragment parsing by itself (Satta andStock, 1989).
This effectively prunes the search spaceand allows the new technique to take full account of theright-side context.
Second, it employs normal top-downparsing, in which each parsing state reflects the globalcontext, instead of top-down chart parsing.
This enablesthe technique to determine the global plausibility ofcandidates asily.
The results of preliminary experimentsare encouraging.
The proposed strategy could enumerate107all possible minimal-penalty solutions in just 4 timesthe time taken to parse the correct sentences.
That is, itis almost wice as fast as Mellish's strategy.2 Mellish's Technique And Its ProblemsThe basic strategy of Mellish's technique is to run abottom-up arser over the input and then, if this fails tofind a complete parse, to run a generalized top-downparser over the resulting chart to hypothesize completeparse candidates.
When the input is well-formed, thebottom-up arser, precisely speaking, a left corner parserwithout top-down filtering, would generate the parsewithout any overhead.
Even if it failed, it is guaranteedto find all complete constituents of all possible parses.Reference to these constituents, enables us to avoidrepeating existing works and to exploit he full syntacticcontext rather just the left-side context of errorcandidates.
The generalized top-down parser attempts tofind out minimal errors by refining the set of "needs"that originates with bottom-up arsing.
Each needindicates the absence of an expected constituent.
Thegeneralized parser hypothesizes, and so remedies an error,when it was sufficiently focused on.
Next, the parsertries to construct a complete parse by taking account ofthe hypothesis.
In the case of multiple errors, thelocation and recovery phases are repeated until a completeparse is obtained.The data structure introduced for representinginformation about local needs is called the generalizededge.
It is an extension of active and inactive dges, andis described as< C from S to E needs CSl from Sl to el,cs2 from s 2 to e 2 .
.
.
.
.
cs n from s n to en >where C is category, csi are sequences of categories(which will be shown inside square brackets), S, E, si,and ei are positions in the input.
The special symbol"*" denotes the position that remains to be determined.The presence of an edge of this kind in the chart indicatesthat the parser is attempting to find a phrase of categoryC that covers the input from position S to E but that inorder to succeed it must still satisfy all the needs listed.Each need satisfies a sequence of categories cs i that mustbe found contiguously to occupy the portion from s i toe i.
An edge with an empty need, which corresponds toan inactive dge is represented as< C from S to E needs nothing>.The generalized top-down parser that uses thegeneralized edge as the data structure is governed by sixrules: three for finding out errors and the other three forrecovering from the three kinds of error.
The three errorlocating rules are the top-down rule, the fundamental ruleand the simplification rule.
The first one is also used inthe ordinary top-down chart parser, and the third one isjust for house keeping.
The second rule, the fundamentalrule, directs to combine an active edge with a inactiveedge.
It was extended from the ordinary rule so thatfound constituents could be incorporated from eitherdirection.
However, the constituents that can beabsorbed are limited to those in the first categorysequence; that is, one of the categories belonging to CSl.The application of the six rules is mainly controlled bythe scores given to edges, that is, agenda control isemployed.
The score of a particular edge reflects itsglobal plausibility and is calculated from six parameters,one of which, for example, says that edges that arisefrom the fundamental rule are preferable to those thatarise from the top-down rule.Although Mellish's technique has a lot of advantagessuch as the ability to utilize the right-side context oferrors and independence of a specific grammar, it cancreate a huge number of edges, as it mainly uses the top-down rule for finding errors.
That is, refining a set oferror candidates toward a pre-terminal category byapplying only the top-down rule may create too manyalternatives.
In addition, since the generalized edgesrepresent just local needs and don't reflect the globalneeds that created them, it is hard to decide if they shouldbe expanded.
In particular, these problems becomecritical when parsing ill-formed inputs, since the top-down rule may be applied without any anchoring; pre-terminals can not be considered as anchors, as pre-terminals may be freely created by error recovery rules.This argument also applies to the start symbol, as thatsymbol may be created depending the constituenthypothesized by error recovery rules and the fundamentalrule.
Mellish uses agenda control to prevent thegeneration of potentially useless edges.
For thispurpose, the agenda control needs complicated heuristicscoring, which complicates the whole algorithm.Moreover, so that the scoring reflects global plausibility,it must employs a sort of dependency analysis, amechanism for the propagation of changes and an easilyreordered agenda, which clearly contradicts his originalidea in which edges must be reflected only local needs.3 Proposed AlgorithmThe technique proposed here resolves the above problemsas follows.
First, some portion of the error locationprocess is separated from and precedes the processes thatare governed by agenda control, and is archived by usinga version of bi-directional parsing.
Second, so that thesearch process can be anchored by the start symbol, adata structure is created that can represent globalplausibility.
Third, in order to reduce the dependency onthe top-down rule, a rule is developed that uses twoactive edges to locate errors.
This process is closer toordinary top-down parsing than chart parsing and globalplausibility scoring is accurate and easily calculated.
Forsimplicity of explanation, simple CF-PSG grammarformalism is assumed throughout this paper, althoughthere are obvious generalizations to other formalism suchas DCG (Pereira nd Warren, 1980) or unification basedgrammars (Shieber, 1986).108Bottom-up rule:<C from S to E needs nothing>C1 --> ...Csl C Cs2... where Csl is not empty (in the grammar)<C1 from * to E 2 needs Csl from * to S ,  Cs2 from E to E2>where if Cs 2 is empty then E2 ffi E else E2 =*.Fundamental rule:<C from S to E needs .... \[...Csl 1, C1, Csl2...\] from Sl to el .... ><C1 from S1 to E1 needs nothing><C from S to E needs .... Csl 1 from Sl to S1 ,Csl2 from E1 to e 1 .... >where s 1 < S1 or Sl-*, E1 < el or el -*.Simplification rule:<C from S to E needs.... Csi-1 from si_ 1 to s, \[\] from s to s, Csi+ 1 from s to ei+l .... >.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.<C from S to E needs .... Csi-1 from si-1 to s, Csi+l from s to ei+l .... >FigureThe first phase of the process is invoked after thefailure of left comer parsing.
The bottom-up arsingleaves behind all complete constituents of every possibleparse and unsatisfied active edges for all error points thatare to the immediate fight of sequences of constituentscorresponding to the RHS.
Since parsing proceeds leftto fight, an active edge is generated only when an errorpoint exists to the fight of the found constituents.
In thefirst phase, bi-directional bottom-up arsing generates allgeneralized edges that represent unsatisfied expectationsto the right and left of constituents.
From someperspectives, the role this phase plays is similar to thatof the covered bi-directional phase of the Picky parser(Magerman and Weir, 1992), though the methodproposed herein does not employ stochastic informationat all.
This process can be described in three rules asshown in Figure 1.
As can be seen, this is bi-directionalbottom-up arsing that uses generalized edges as the datastructure.
For simplicity, the details for avoidingduplicated edge generation have been omitted.
It is worthnoting that after this process, the needs listed in eachgeneralized edge indicate that the expected constituentsdid not exist, while, before this process, a need may existjust because an expectation has not been checked.The second phase finds out errors and corrects them.The location operation proceeds by refining a need intomore precise one, and it starts from the global need thatrefers to the start symbol, S, from 0 to n, where n is thelength of the given input.
In the notion of generalizededges, that need can be represented as,<GOAL from 0 to n needs \[S\] from 0 to n>.The data structure reflecting lobal needs directly is usedin this phase, so the left part of each generalized edge isredundant and can be omitted.
In addition, two values, gand h, are introduced, g denotes how much cost has been1.
The Bi-Directional Parsing Rulesexpended for the recovery so far, and h is the estimationof how much cost will be needed to reach a solution.Cost involves solution plausibility; solutions with lowplausibility have high costs.
Thus, the data structureused in this phase is,109<needs CSl from Sl to el ,  cs2 from s2 to e2 .
.
.
.
.CSn from Sn to e n, g, h>.Here, the number of errors corrected so far is taken as g,and the total number of categories in the needs is used ash.
As mentioned above, since the needs listed indicateonly the existence of errors as detected by the precedingprocess and to be refined, the value of h is always lessthan or equal to the number of the errors that must becorrected to get a solution.
That is, the best first searchusing g+h as the cost functions is an admissible A*search (Rich and Knight, 1991).
Needless to say, moresophisticated cost functions can also be used, in which,for example, the cost depends on the kind of error.The rules governing the second phase, whichcorrespond to the search state transition operators in thecontext of search problems, are shown in Figure 2.
Thetop-down rule and the refining rule locate errors and theother three rules are for correcting them.
Most importantis the refining rule, which tries to find out errors byusing generalized edges in a top-down manner toward pre-terminals.
This reduces the frequency of using the top-down rule and prevents an explosion in the number ofaltematives.This process tarts from<needs \[S\] from 0 to n, g: 0, h: 1>.To reach the following need means to get one solution.<needs nothing, g: _, h: 0>.Top-down rule:<needs \[C1...Csl\] from Sl to E1 ..... g: G, h: H>C1 --> ...RHS (in grammar)<needs \[...RHS ...Csl\] form Sl to E1 ..... g: G, h: H+(length of RHS)-I>Refining rule:<needs \[...Csl 1, C1, Csl2...\] from s 1 to e 1 .
.
.
.
.
g: G, h: H><C1 from S to E needs Cs 1 from S 1 to E1 .
.
.
.
.
Csn from Sn to En ><needs Csl l  from s I to S,  Csl from S1 to E1 .
.
.
.
.
Csn from Sn to En ,Csl2 from E to el ..... g: G, h: H+~(length of Csn)-l>The result must be well-formed, that is sl < S1 or sl--* or SI=* and so on.Garbage rule:<needs \[Cl...Csl\] from Sl to el ..... g: G, h: H> where C1 is a pre-terminal<C1 from S1 to El needs nothing> where Sl < SI<needs Csl from E1 to el ..... g: G+(SI-Sl), h: H-l>Unknown word rule:<needs \[CI...Csl\] from Sl to el ..... g: G, h: H> where C1 is a pre-terminal<needs Csl from sl+l to el ..... g: G+I, h: H-l>where the edge, <C1 from s 1 to sl+l needs nothing> does not exist in the chartEmpty category rule:<needs Csl from s to s ,  Cs2 from s2 to e2 ..... g: G, h: H><needs Cs2 from s2 to e2 ..... g: G+(length of Csl), h: H-(length of Csl)>Figure 2.
The Error Locating and Recovery RulesThe need with the smallest value of g+h is processedfirst.
If two needs have the same value of g+h, the onewith the smaller h values dominates.
This controlstrategy guarantees to find the solution with minimalcost first; that is, the solution with the minimumnumber of recoveries.Figure 3 shows an example of this technique inoperation.
(a) shows the sample grammar adopted, (b)shows the input to be processed, and (c) shows some ofthe edges left behind after the failure of the originalbottom-up arsing.
As shown in (d), the first phasegenerates everal edges that indicate unsatisfiedexpectations to the left of found constituents.
Thesecond phase begins with need (e-1).
Among the others,(e-2) and (e-3) are realized by applying the refining ruleand the top-down rule, respectively.
Since (e-2) has thesmallest value of g+h, it takes precedence to beexpanded.
The refining rule processes (e-2) and generates(e-4) and (e-7), among others.
The solution indicated by(e-6), which says that the fifth word of the input must bea preposition, is generated from (e-4).
Another solutionindicated by (e-9), which says that the fifth word of theinput must be a conjunctive is derived from (e-7).
Thatthe top-down rule played no role in this example was notincidental.
In reality, application of the top-down rulemay be meaningful only when all the constituents li tedin the RHS of a grammar rule contain errors.
In everyother case, generalized edges derived from that rule musthave been generated already by the first phase.
Theapplication of the top-down rule can be restricted to casesinvolving unary rules, if one assumes at most one errormay exist.4 P re l iminary  Exper imentsIn order to evaluate the technique described above, somepreliminary experiments were conducted.
Theexperiments employed the same framework as used byMellish, and used a similar sized grammar, the small e-free CF-PSG for a fragment of English with 141 rulesand 72 categories.
Random sentences (10 for each lengthconsidered) were generated from the grammar, and thenrandom occurrences of specific types of errors wereintroduced into these sentences.
The errors consideredwere none, deletion of one word, adding one known orunknown word, and substituting one unknown or knownword for one word of the sentence.
The amount of workdone by the parser was calculated using the concept of"cycle".
The parser consumes one cycle for processingeach edge.
The results are shown in Table 1.
Thei i0(a) The grammar:S --> NP VP ...(a-l)NP ---> NP C NPVP ---~ VPPPNP --~ NVP --?
Vt NPPP ---> P NP ...(a-2)NP ---> Det NVP ---> Vi(b) The input:The lady bought cakes an the0 1 2 3 4 5 6shop7(c) Examples of the edges left behind:<NP from 0 to 2 needs nothing><Vi from 2 to 3 needs nothing><NP from 3 to 4 needs nothing><NP from 5 to 7 needs nothing> ...(c-l)<S from 0 to * needs \[VP\] from 2 to *> ...(c-2)<VP from 2 to * needs \[PP\] from 4 to *> ...(c-3)<VP from 2 to * needs \[NP\] from 3 to *> ...(c-4)<NP from 3 to * needs \[C NP\] from 4 to *> ...(c-5)(d) Examples edges generated in the bi-directional parsing:<PP from * to 7 needs \[P\] from * to 5> ...(d-l) Bottom up rule, (c-l), (a-2)<NP from 3 to 7 needs \[C\] from 4 to 5> ...(d-2) Fundamental rule, (c-l), (c-5)(e) Focusing on and recovering from errors:<needs \[S\] from 0 to 7, g:0, h: 1> ...(e-l) Initial needs<needs \[VP\] from 2 to 7, g:0, h:l> ...(e-2) Refining rule, (e-l), (c-2)<needs \[NP VP\] from 0 to 7, g:0, h:2> ...(e-3) Top-down rule, (e-l), (a-l)<needs \[PP\] from 4 to 7, g:0, h: 1> ...(e-4) Refining rule, (e-2), (c-3)<needs \[P\] from 4 to 5, g:0, h:l> ...(e-5) Refining rule, (e-4), (d-l)<needs nothing, g: 1, h:0> ...(e-6) Unknown word rule, (e-5)The fifth word, "an", is hypothesized tobe an unknown preposition (P)<needs \[NP\] from 3 to 7, g:0, h: 1> ...(e-7) Refining rule, (e-2), (c-4)<needs \[C\] from 4 to 5, g:0, h: 1> ...(e-8) Refining rule, (e-7), (d-2)<needs nothing, g: 1, h:0> ...(e-9) Unknown word rule, (e-8)The fifth word, "an", hypothesized tobe an unknown conjunctive (C)Figure 3.
An Example ofstatistics in the table are described as follows.
BU cyclesis the number of cycles taken to exhaust the chart in theinitial bottom-up arsing.
BD cycles is the number ofcycles required for bi-directional bottom-up arsing in thefirst phase.
#solns is the number of different solutionsand represents descriptions of possible rrors.
First~Lastis the number of cycles required for error location andrecovery to find the first / last solution.
LR cycles is thenumber of cycles in the error locating and recovery phaserequired to exhaust all possibilities of sets of errors withthe same penalty as the first solution.The preliminary results how that, for short sentenceswith one error, enumerating all possible minimum-penalty errors takes about 4 times as long as parsing thecorrect sentences.
This is almost twice the speed ofMellish's strategy.
As 75% of the process are occupiedby the first bi-directional parsing operation, more cyclesare needed to get the first solution with the proposedtechnique than with Mellish's strategy.i i ithe Error Recovery Process5 D iscuss ionThe second phase of the proposed technique is based onordinary top-down parsing or tree search rather than chartparsing.
As a consequence, some error locationoperations may be redundant, as Mellish pointed out.For example, suppose a new grammar rule, N ~ N PPis added to the grammar given in Figure 3.
In that case,the following edge, located in the first phase, may causea redundant error locating process, as the same search istriggered by (e-4).<N from 3 to * needs \[PP\] from 4 to *>.One way for avoiding such redundancies is to use a datastructure that reflects just local needs.
However, it istrue that an effective rror location process must takeinto account global needs.
There is a tradeoff betweensimplicity and the avoidance of duplicated efforts.
Thetechnique proposed here employs a data structure thatTable 1.
Prel iminary Exper imenta l  ResultsEITOrNoneDeleteonewt~xlAddunknownwordAddknownwordSubstituteunknownwc~lSubstituteknownwtxdLength oforiginal6126BU cycles7011417042BD cycles1329 79 25511160126378191#solns1.31.42.06.04.56.23.0First Last LR cycles8 24 3119 32 434320251457289 99 322 3.7 25 37 4612 147 534 2.6 46 61 726 69 221 5.7 14 25 33i9 94 4.7 24 38 555196.92.715950701429257815312680209 76 239 4.2 21 34 4312 109 363 3.2 34 46 586 51 151 3.8 9 18 279 82 256 3.2 15 25 3312 116 384 3.8 33 58 71directly reflects the global needs.
Mellish, on the otherhand, utilized a structure that reflected just local needsand tried to put global needs into the heuristic function.The result, at least so far as confirmed by tests, was thatpruning allowed the simple method to overcome thedrawback of duplicated effort.
Moreover, Mellish'sdependency ontrol mechanism, .introduced to maintainthe plausibility scores, means that edges are no longerlocal.
In addition, it can be expected that a standardgraph search strategy for avoiding duplicated search isapplicable to the technique proposed.Theoretical investigation is needed to confirm howthe number of grammar rules and the length of input willaffect he amount of computation needed.
Furthermore,the algorithm has to be extended in order to incorporatethe high level knowledge that comes from semantics andpragmatics.
Stochastic information such as statistics oncategory trigrams must be useful for effective control.ReferencesBear, John, Dowding, John and Shriberg, Elizabeth(1992).
Integrating Multiple Knowledge Sourcesfor Detection and Correction of Repairs inHuman-Computer Dialog.
Proceedings of 30thACL, 56 - 63.Carbonell, Jaime G. and Hayes, Philip J.
(1983).Recovery Strategies for Parsing Extragrammatical Language.
JACL, 9 (3-4), 123 - 146.Gazdar, Gerald and Mellish, Chris (1989).
NaturalLanguage Processing in LISP.
Workingham:Addison-Wesley.Hindle, Donald (1983).
Deterministic Parsing ofSyntactic Non-fluencies.
Proceedings of 21stACL, 123 - 128.Kay, Martin (1980).
Algorithm Schemata nd DataStructures in Syntactic Processing.
ResearchReport CSL-80-12 Xerox PARC.Lang, Bernard (1988).
Parsing Incomplete Sentences.Proceedings of COLING 88, 365 - 371.Magerman, David M. and Weir, Carl (1992).
Efficiency,Robustness and Accuracy in Picky Chart Parsing.Proceedings of 3Oth ACL, 40 - 47.Mellish, Chris S. (1989).
Some Chart-Based Techniquesfor Parsing Ill-Formed Input.
Proceedings of 27thACL, 102- 109.Pereira, Fernando C.N.
and Warren, David, H.D.
(1980).Definite Clause Grammars for Language Analysis- A Survey of the Formalism and a Comparisonwith Augmented Transition Networks.
ArtificialIntelligence, 13 (3), 231 - 278.Rich, Elaine and Knight, Kevin (1991).
ArtificialIntelligence (2nd ed.).
New York: McGraw-Hill.Saito, Hiroaki and Tomita, Masaru (1988).
ParsingNoisy Sentences.
Proceedings of COLING 88,561 - 566.Satta, Giorgio and Stock, Oliviero (1989).
FormalProperties and Implementation f BidirectionalCharts.
Proceedings oflJCAl 89, 1480 - 1485.Shieber, Stuart M. (1986).
An Introduction toUnification-Based Approaches to Grammar.Stanford: CSLI Lecture Notes 4.Weishedel, Ralph M. and Sondheimer, Norman K.(1983).
Meta-Rules as a Basis for Processing Ill-Formed Input.
JACL, 9 (3-4), 161 - 177.112
