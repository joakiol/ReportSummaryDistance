Proceedings of the Eighth Workshop on Statistical Machine Translation, pages 429?434,Sofia, Bulgaria, August 8-9, 2013 c?2013 Association for Computational LinguisticsAn approach using style classification features for Quality EstimationErwan MoreauCNGL and Computational Linguistics GroupCentre for Computing and Language StudiesSchool of Computer Science and StatisticsTrinity College DublinDublin 2, Irelandmoreaue@cs.tcd.ieRaphael RubinoNCLTDublin City UniversityDublin 9, Irelandrrubino@computing.dcu.ieAbstractIn this paper we describe our participationto the WMT13 Shared Task on Quality Es-timation.
The main originality of our ap-proach is to include features originally de-signed to classify text according to someauthor?s style.
This implies the use of ref-erence categories, which are meant to rep-resent the quality of the MT output.PreambleThis paper describes the approach followed in thetwo systems that we submitted to subtask 1.3 ofthe WMT13 Shared Task on Quality Estimation,identified as TCD-DCU-CNGL 1-3 SVM1 andTCD-DCU-CNGL 1-3 SVM2.
This approachwas also used by the first author in his submissionsto subtask 1.1, identified as TCD-CNGL OPENand TCD-CNGL RESTRICTED1.
In the remain-ing of this paper we focus on subtask 1.3, but thereis very little difference in the application of the ap-proach to task 1.1.1 IntroductionQuality Estimation (QE) aims to provide a qualityindicator for machine translated sentences.
Thereare many cases where such an indicator would beuseful in a translation process: to compare differ-ent Machine Translation (MT) models on a givenset of sentences, to tune automatically the param-eters of a MT model, to select the bad sentencesfor human translation or post-editing, to select thegood sentences for immediate publication and tryto apply automatic post-editing to the others, orsimply to provide users who are not fluent in thesource language information about the fluency of1The second author?s submission to subtask 1.1 is inde-pendent from this approach and is described in a differentpaper in this volume.the translated text they are reading.
As long as ma-chine translated text cannot be of reasonably con-sistent quality, QE is helpful in indicating linguis-tic quality variability.2After focusing on automatic prediction of ad-hoc quality scores (as estimated by professionalannotators) in the previous edition (Callison-Burch et al 2012), the WMT Shared Task onQuality Estimation 2013 proposes several variantsof the task.
We participated in task 1.1 which aimsto predict HTER scores (edit distance between theMT output and its manually post-edited version),and in task 1.3 which aims to predict the expectedtime needed to post-edit the MT output.The originality of our participation lies in thefact that we intended to test ?style classification?features for the task of QE: the idea is to select aset of n-grams which are particularly representa-tive of a given level of quality.
In practice we useonly two levels which simply represent low andhigh quality.
We explore various ways to buildthese two reference categories and to select the n-grams, as described in ?2.
The goal was to seeif such features can contribute to the task of pre-dicting quality of MT.
As explained in ?3, how-ever, various constraints forced us to somehow cutcorners in some parts of the features selection andtraining process; therefore we think that the mod-est results presented and discussed in ?4 might notnecessarily reflect the real contribution of thesefeatures.2 Features2.1 Classical featuresWe extract a set of features inspired by the onesprovided by the shared task organisers in their 17baseline feature set.
Using the corpora providedfor the task, we extract for each source and target2We focus on translation fluency rather than target lan-guage faithfulness to sources.429segments pair:?
24 surface features, such as the segmentlength, the number of punctuation marks anduppercased letters, words with mixed case,etc.?
30 language Model (LM) features, n-gramlog-probability and perplexity (with andwithout start and end of sentence tags) withn ?
[1; 5].?
30 backward LM features, n-gram log-probability and perplexity (with and withoutstart and end of sentence tags) with n ?
[1; 5].?
44 n-gram frequency features, with n ?
[1; 5], extracted from frequency quartiles.?
24 word-alignment features according to thealignment probability thresholds: 0.01, 0.1,0.25, 0.5, 0.75 and 1.0, with or without wordsfrequency weighting.For all these features, except the ones with binaryvalues, we compute the ratio between the sourceand target feature values and add them to our fea-ture set, which contains 223 classical features.2.2 Style classification featuresWe call the features described below ?styleclassification?
features because they have beenused recently in the context of author identifica-tion/profiling (Moreau and Vogel, 2013a; Moreauand Vogel, 2013b) (quite sucessfully in somecases).
The idea consists in representing the n-grams which are very specific to a given ?cate-gory?, a category being a level of quality in thecontext of QE, and more precisely we use only the?good?
and ?bad?
categories here.Thus this approach requires the following pa-rameters:?
At least two datasets used as reference for thecategories;?
Various n-grams patterns, from which com-parisons based on frequency can be done;?
One or several methods to compare a sen-tence to a category.2.2.1 Reference categoriesAs reference categories we use both the trainingdatasets provided for task 1.1 and 1.3: both areused in each task, that is, categories are extractedfrom subtasks 1.1 dataset and 1.3 dataset and usedin task 1.1 and 1.3 as well.
However we use onlyhalf of the sentences of task 1.1 in 1.1 and sim-ilarly in 1.3, in order to keep the other half forthe classical training process.
This is necessary toavoid using (even indirectly) a sentence as both afixed parameter from which features are extracted(the category data) and an actual instance on whichfeatures are computed.
In other words this simplyfollows the principle of keeping the training andtest data independent, but in this case there are twostages of training (comparing sentences to a refer-ence category is also a supervised process).The two datasets are used in three differentways, leading to three distinct pairs of categories?good/bad?:3?
The sentences for which the quality is belowthe median form the ?bad?
category, the oneabove form the ?good?
category;?
The sentences for which the quality is belowthe first quartile form the ?bad?
category, theone above the third quartile form the ?good?category;?
The complete set of MT output sentencesform the ?bad?
category, their manuallypost-edited counterpart form the ?good?
cat-egory.We use these three different ways to build cate-gories because there is no way to determine a pri-ori the optimal choice.
For instance, on the onehand the opposite quartiles probably provide morediscriminative power than the medians, but on theother hand the latter contains more data and there-fore possibly more useful cases.4 In the last ver-sion the idea is to consider that, in average, themachine translated sentences are of poor qualitycompared to the manually post-edited sentences;in this case the categories contain more data, but itmight be a problem that (1) some of the machine-translated sentences are actually good and (2) the3Below we call ?quality?
the value given by the HTERscore (1.1) or post-editing time (1.3), the level of quality be-ing of course conversely proportional to these values.4The datasets are not very big: only 803 sentences in task1.3 and 2,254 sentences in task 1.1 (and we can only use halfof these for categories, as explained above).430right translation of some difficult phrases in thepost-edited sentences might never be found in MToutput.
We think that the availability of differ-ent categories built in various ways is potentially agood thing, because it lets the learning algorithmdecide which features (based on a particular cate-gory) are useful and which are not, thus tuning themodel automatically while possibly using severalpossibilities together, rather than relying on somepredefined categories.It is important to notice that the correspondencebetween an MT output and its post-edited versionis not used5: in all categories the sentences areonly considered as an unordered set.
For instanceit would be possible to use a third-party corpus aswell (provided it shares at least a common domainwith the data).We use only the target language (Spanish) of thetranslation and not the source language in ordernot to generate too many categories, and becauseit has been shown that there is a high correlationbetween the complexity of the source sentence andthe fluency of the translation (Moreau and Vogel,2012).
However it is possible to do so for the cat-egories based on quantiles.2.2.2 n-grams patterns, thresholds anddistance measuresWe use a large set of 30 n-grams patterns based ontokens and POS tags.
POS tagging has been per-formed with TreeTagger (Schmid, 1995).
Variouscombinations of n-grams are considered, includ-ing standard sequential n-grams, skip-grams, andcombinations of tokens and POS tags.Since the goal is to compare a sentence to acategory, we consider the frequency in terms ofnumber of sentences in which the n-gram appears,rather than the global frequency or the local fre-quency by sentence.6Different frequency thresholds are considered,from 1 to 25.
Additionally we can also filter outn-grams for which the relative frequency is too5in the categories used as reference data; but it is used inthe final features during the (supervised) training stage (see?3).6The frequency by sentence is actually also taken into ac-count in the following way: instead of considering only then-gram, we consider a pair (n-gram, local frequency) as anobservation.
This way if a particular frequency is observedmore often in a given category, it can be interpreted as a cluein favor of this category.
However in most cases (long n-grams sequences) the frequency by sentence is almost alwaysone, sometimes two.
Thus this is only marginally a relevantcriterion to categorize a sentence.similar between the ?good?
and ?bad?
categories.For instance it is possible to keep only the n-gramsfor which 80% of the occurrencies belong to the?bad?
category, thus making it a strong markerfor low quality.
Once again different thresholdsare considered, in order to tradeoff between theamount of cases and their discriminative power.We use only three simple distance/similaritymeasures when comparing a sentence to a cate-gory:?
Binary match: for each n-gram in the sen-tence, count 1 if it belongs to the category, 0otherwise, then divide by the number of n-grams in the sentence;?
Weighted match: same as above but sum theproportion of occurrences belonging to thecategory instead of 1 (this way an n-gramwhich is more discriminative is given moreweight);?
Cosine similarity.Finally for every tuple formed by the combina-tion of?
a category,?
a quality level (?good/bad?),?
an n-gram pattern,?
a frequency threshold,?
a threshold for the proportion of the occur-rences in the given category,?
and a distance measurea feature is created.
For every sentence the valueof the feature is the score computed using the pa-rameters defined in the tuple.
From our set ofparameters we obtain approximately 35,000 fea-tures.7 It is worth noticing that these featuresare not meant to represent the sentence entirely,but rather particularly noticeable parts (in terms ofquality) of the sentence.7The number of features depends on the data in the cate-gory, because if no n-gram at all in the category satisfies theconditions given by the parameters (which can happen withvery high thresholds), then the feature does not exist.4312.3 Features specific to the datasetIn task 1.3 we are provided with a translator idand a document id for each sentence.
The distribu-tion of the time spent to post-edit the sentence de-pending on these parameters shows some signifi-cant differences among translators and documents.This is why we add several features intended to ac-count for these parameters: the id itself, the meanand the median for both the translator and the doc-ument.3 Design and training processThe main difficulty with so many features (around35,000) is of course to select a subset of reason-able size, in order to train a model which is notoverfitted.
This requires an efficient optimizationmethod, since it is clearly impossible to explorethe search space exhaustively in this case.Initially it was planned to use an ad-hoc geneticalgorithm to select an optimal subset of features.But unfortunately the system designed in this goaldid not work as well as expected8, this is why wehad to switch to a different strategy: the two fi-nal sets of features were obtained through severalstages of selection, mixing several different kindsof correlation-based features selection methods.The different steps described below were car-ried out using the Weka Machine Learning toolkit9(Hall et al 2009).
Since we have used half of thetraining data as a reference corpus for some of thecategories (see ?2), we use the other half as train-ing instances in the selection and learning process,with 10 folds cross-validation for the latter.3.1 Iterative selection of featuresBecause of the failure of the initial strategy, in or-der to meet the time constraints of the Shared Taskwe had to favor speed over performance in the pro-cess of selecting features and training a model.This probably had a negative impact on the finalresults, as discussed in section ?4.In particular the amount of features was toobig to be processed in the remaining time by asubset selection method.
This is why the fea-tures were first ranked individually using the Re-lief attribute estimation method (Robnik-Sikonja8At the time of writing it is still unclear if this was due toa design flaw or a bug in the implementation.9Weka 3.6.9, http://www.cs.waikato.ac.nz/ml/weka.and Kononenko, 1997).
Only the 20,00010 top fea-tures were extracted from this ranking and usedfurther in the selection process.From this initial subset of features, the follow-ing heuristic search algorithms combined with acorrelation-based method11 to evaluate subsets offeatures (Hall, 1998) are applied iteratively to agiven input set of features:?
Best-first search (forward, backward, bi-directional);?
Hill-climbing search (forward and back-ward);?
Genetic search with Bayes Networks.Each of these algorithms was used with differ-ent predefined parameters in order to trade off be-tween time and performance.
This selection pro-cess is iterated as long as the number of featuresleft is (approximately) higher than 200.3.2 Training the modelsWhen less than 200 features are obtained, the it-erative selection process is still applied but a 10folds cross-validated evaluation is also performedwith the following regression algorithms:?
Support Vector Machines (SVM) (Smola andScho?lkopf, 2004; Shevade et al 2000);?
Decision trees (Quinlan, 1992; Wang andWitten, 1996);?
Pace regression (Wang and Witten, 2002).These learning algorithms are also run withseveral possible sets of parameters.
Eventuallythe submitted models are chosen among thosefor which the set of features can not be reducedanymore without decreasing seriously the perfor-mance.
Most of the best models were obtainedwith SVM, although the decision trees regressionalgorithm performed almost as well.
It was notpossible to decrease the number of features below60 for task 1.3 (80 for task 1.1) without causing aloss in performance.10For subtask 1.3.
Only the 8,000 top features for subtask1.1.11Weka classweka.attributeSelection.CfsSubsetEval.4324 Results and discussionThe systems are evaluated based on the Mean Av-erage Error, and every team was allowed to submittwo systems.
Our systems ranked 10th and 11thamong 14 for task 1.1, and 13th and 15th among17 for task 1.1.4.1 Possible causes of loss in performanceWe plan to investigate why our approach does notperform as well as others, and in particular tostudy more exhaustively the different possibilitiesin the features selection process.12 It is indeedvery probable that the method can perform betterwith an appropriate selection of features and opti-mization of the parameters, in particular:?
The final number of features is too large,which can cause overfitting.
Most QE systemdo not need so many features (only 15 for thebest system in the WMT12 Shared Task onQE (Soricut et al 2012)).?
We had to perform a first selection to discardsome of the initial features based on their in-dividual contribution.
This is likely to be aflaw, since some features can be very usefulin conjuction with other even if poorly infor-mative by themselves.?
We also probably made a mistake in apply-ing the selection process to the whole set offeatures, including both classical features andstyle classification features: it might be rel-evant to run two independent selection pro-cesses at first and then gather the resultingfeatures together only for a more fine-grainedfinal selection.
Indeed, the final models thatwe submitted include very few classical fea-tures; we believe that this might have madethese models less reliable, since our initialassumption was rather that the style classifi-cation features would act as secondary cluesin a model primarily relying on the classicalfeatures.4.2 Selected featuresThe following observations can be made on the fi-nal models obtained for task 1.3, keeping in mindthat the models might not be optimal for the rea-sons explained above:12Unfortunately the results of this study are not ready yetat the time of writing.?
Only 5% of the selected features are classicalfeatures;?
The amount of data used in the categoryseems to play an important role: most fea-tures correspond to categories built from the1.1 dataset (which is bigger), and the pro-portions between the different kinds of cate-gories are: 13% for first quartile vs. fourthquartile (smallest dataset), 25% for belowmedian vs. above median, and 61% forMT output vs. postedited sentence (largestdataset);?
It seems more interesting to identify the lowquality n-grams (i.e.
errors) rather than thehigh quality ones: 76% of the selected fea-tures represent the ?bad?
category;?
81% of the selected features represent ann-grams containing at least one POS tag,whereas only 40% contain a token;?
Most features correspond to selecting n-grams which are very predictive of the?good/bad?
category (high difference of therelative proportion between the two cate-gories), although a significant number of lesspredictive n-grams are also selected;?
The cosine distance is selected about threetimes more often than the two other distancemethods.5 Conclusion and future workIn conclusion, the approach performed decently onthe Shared Task test data, but was outperformedby most other participants systems.
Thus cur-rently it is not proved that style classification fea-tures help assessing the quality of MT.
Howeverthe approach, and especially the contribution ofthese features, have yet to be evaluated in a lessconstrained environment in order to give a well-argued answer to this question.AcknowledgmentsThis research is supported by the Science Foun-dation Ireland (Grant 12/CE/I2267) as part of theCentre for Next Generation Localisation (www.cngl.ie) funding at Trinity College, Universityof Dublin.433ReferencesChris Callison-Burch, Philipp Koehn, Christof Monz,Matt Post, Radu Soricut, and Lucia Specia.
2012.Findings of the 2012 workshop on statistical ma-chine translation.
In Proceedings of the SeventhWorkshop on Statistical Machine Translation, Mon-treal, Canada, June.
Association for ComputationalLinguistics.M.
Hall, E. Frank, G. Holmes, B. Pfahringer, P. Reute-mann, and I.H.
Witten.
2009.
The weka data miningsoftware: an update.
ACM SIGKDD ExplorationsNewsletter, 11(1):10?18.M.
A.
Hall.
1998.
Correlation-based Feature SubsetSelection for Machine Learning.
Ph.D. thesis, Uni-versity of Waikato, Hamilton, New Zealand.Erwan Moreau and Carl Vogel.
2012.
Quality esti-mation: an experimental study using unsupervisedsimilarity measures.
In Proceedings of the SeventhWorkshop on Statistical Machine Translation, pages120?126, Montre?al, Canada, June.
Association forComputational Linguistics.Erwan Moreau and Carl Vogel.
2013a.
Participationto the pan author identification task.
In to appear inthe proceeding of CLEF 2013.Erwan Moreau and Carl Vogel.
2013b.
Participationto the pan author profiling task.
In to appear in theproceeding of CLEF 2013.J.R.
Quinlan.
1992.
Learning with continuous classes.In Proceedings of the 5th Australian joint Confer-ence on Artificial Intelligence, pages 343?348.
Sin-gapore.Marko Robnik-Sikonja and Igor Kononenko.
1997.An adaptation of relief for attribute estimation inregression.
In Douglas H. Fisher, editor, Four-teenth International Conference on Machine Learn-ing, pages 296?304.
Morgan Kaufmann.Helmut Schmid.
1995.
Improvements in part-of-speech tagging with an application to german.
InProceedings of the ACL SIGDAT-Workshop, pages47?50.S.K.
Shevade, SS Keerthi, C. Bhattacharyya, andK.R.K.
Murthy.
2000.
Improvements to the SMOalgorithm for SVM regression.
Neural Networks,IEEE Transactions on, 11(5):1188?1193.A.J.
Smola and B. Scho?lkopf.
2004.
A tutorial onsupport vector regression.
Statistics and computing,14(3):199?222.Radu Soricut, Nguyen Bach, and Ziyuan Wang.
2012.The SDL Language Weaver systems in the WMT12Quality Estimation shared task.
In Proceedings ofthe Seventh Workshop on Statistical Machine Trans-lation, pages 145?151, Montre?al, Canada, June.
As-sociation for Computational Linguistics.Y.
Wang and I.H.
Witten.
1996.
Induction of modeltrees for predicting continuous classes.Y.
Wang and I.H.
Witten.
2002.
Modeling for optimalprobability prediction.
In Proceedings of the Nine-teenth International Conference on Machine Learn-ing, pages 650?657.
Morgan Kaufmann PublishersInc.434
