Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 779?784,Dublin, Ireland, August 23-24, 2014.UoW: Multi-task Learning Gaussian Processfor Semantic Textual SimilarityMiguel RiosResearch Group in Computational LinguisticsUniversity of WolverhamptonStafford Street, Wolverhampton,WV1 1SB, UKM.Rios@wlv.ac.ukLucia SpeciaDepartment of Computer ScienceUniversity of SheffieldRegent Court, 211 Portobello,Sheffield, S1 4DP, UKL.Specia@sheffield.ac.ukAbstractWe report results obtained by the UoWmethod in SemEval-2014?s Task 10 ?
Mul-tilingual Semantic Textual Similarity.
Wepropose to model Semantic Textual Simi-larity in the context of Multi-task Learningin order to deal with inherent challenges ofthe task such as unbalanced performanceacross domains and the lack of trainingdata for some domains (i.e.
unknowndomains).
We show that the Multi-taskLearning approach outperforms previouswork on the 2012 dataset, achieves a ro-bust performance on the 2013 dataset andcompetitive results on the 2014 dataset.We highlight the importance of the chal-lenge of unknown domains, as it affectsoverall performance substantially.1 IntroductionThe task of Semantic Textual Similarity (STS)(Agirre et al., 2012) is aimed at measuring thedegree of semantic equivalence between a pair oftexts.
Natural Language Processing (NLP) ap-plications such as Question Answering (Lin andPantel, 2001), Text Summarisation (Lin and Hovy,2003) and Information Retrieval (Park et al., 2005)rely heavily on the ability to measure semanticsimilarity between pairs of texts.
The STS eval-uation campaign provides datasets that consist ofpairs of sentences from different NLP domainssuch as paraphrasing, video paraphrasing, and ma-chine translation (MT) evaluation.
The participat-ing systems are required to predict a graded simi-larity score from 0 to 5, where a score of 0 meansthat the two sentences are on different topics andThis work is licensed under a Creative Commons At-tribution 4.0 International Licence.
Page numbers and pro-ceedings footer are added by the organisers.
Licence details:http://creativecommons.org/licenses/by/4.0/a score of 5 means that the two sentences have ex-actly the same meaning.Methods for STS are commonly based on com-puting various types of similarity metrics betweenthe pair of sentences, where the similarity scoresare used as features to train regression algorithms.B?ar et al.
(2012) use similarity metrics of vary-ing complexity.
The range of features goes fromsimple string similarity metrics to complex vectorspace models.
The method yielded the best av-erage results based on the official evaluation met-rics, despite not having achieved the best resultsin all individual domains.
?Sari?c et al.
(2012) use asimilar set up, extracting features from similaritymetrics, where these features are based on word-overlap and syntax similarity.
The method wasamong the best for domains related to paraphras-ing.
It also achieved a high correlation betweenthe training and test data.
In contrast, for the ma-chine translation data the performance in the testset was lower than the one over the training data.A possible reason for the poor results on this do-main is the difference in length between the train-ing and test sentences, as in the test data the pairstend to be short and share similar words.
?Sari?c etal.
(2012) claim that these differences show thatthe MT training data is not representative of thetest set given their choice of features.Most of the participating systems in the STSchallenges achieve good results on certain do-mains (i.e.
STS datasets), but poor results on oth-ers.
Even the most robust methods still show a biggap in performances for different datasets.
In thesecond evaluation campaign of STS a new chal-lenge was proposed: domains for which no train-ing sets are provided, but only test sets.
Heilmanand Madnani (2013) propose to incorporate do-main adaptation techniques (Daum?e et al., 2010)for STS to generalise models to new domains.They add new features into the model, where thefeature set contains domain specific features plus779general task features.
The machine learning al-gorithm infers the extra weights of each specificdomain and of the general domain.
When an in-stance of a specific domain is to be predicted, onlythe copy of the features of that domain will be ac-tive; if the domain is unknown, the general fea-tures will be active.
Severyn et al.
(2013) pro-pose to use meta-classification to cope with do-main adaptation.
They merge each pair into a sin-gle text and extract meta-features such as bag-of-words and syntactic similarity scores.
The meta-classification model predicts, for each instance, itsmost likely domain based on these features.A possible solution to alleviate unbalanced per-formances on different domains is to model STSin the context of Multi-task Learning (MTL).
Themotivation behind MTL is that by learning multi-ple related tasks simultaneously the model perfor-mance may improve compared to the case wherethe tasks are learnt separately.
MTL is based onthe assumption that related tasks can be clusteredand inter-task correlations between tasks withinthe same cluster can be transferred.We propose to model STS using MTL basedon a state-of-the-art STS feature set (?Sari?c et al.,2012).
As algorithm we use a non-parametricBayesian approach, namely Gaussian Processes(GP) (Rasmussen, 2006).
We show that the MTLmodel outperforms previous work on the 2012datasets and leads to robust performance on the2013 datasets.
On the STS 2014 challenge, ourmethod shows competitive results.2 Experimental SettingWe apply MTL to cope with the challenge of un-balanced performances across domains and un-known domains present in the STS datasets.2.1 TakeLab FeaturesWe use the features from one the top perform-ing system in STS 2012: the TakeLab1system,which is publicly available.
It extracts the follow-ing types of features:N-gram overlap is the harmonic mean of the de-gree of matching between the first and secondtexts, and vice-versa.
The overlap is com-puted for unigrams, bigrams, and trigrams.WordNet-augmented word overlap is the par-tial WordNet path length similarity score as-1http://takelab.fer.hr/sts/signed to words that are not common to bothtexts.Vector space sentence similarity is the repre-sentation of each text as a distributional vec-tor by summing the distributional (i.e., LSA)vectors of each word in the text and taking thecosine distance between these texts vectors.Shallow NE similarity is the matching betweenNamed Entities (NE) that indicates whetherthey were found in both texts.Numbers overlap is an heuristic that penalisesdifferences between numbers in texts.Altogether, these features make up a vector of 21similarity scores.2.2 Multi-task Gaussian ProcessesGaussian Processes (Rasmussen, 2006) is aBayesian non-parametric machine learning frame-work based on kernels for regression and classifi-cation.
In GP regression, for the inputs x we wantto learn a function f that is inferred from a GPprior:f(x) ?
GP (m(x), k(x, x?
)), (1)where m(x) defines a 0 mean and k(x, x?)
definesthe covariance or kernel functions.
In the singleoutput case, the random variables are associatedto a process f evaluated at different values of theinput x.
In the multiple output case, the randomvariables are associated to different processes andevaluated at different values of x.We are interested in the intrinsic coregionaliza-tion model for GP.
A coregionalization model isa heterotopic MTL model in which each output isassociated with a different set of inputs.
In ourcase the different set of inputs are the STS do-mains (i.e.
datasets).
The intrinsic coregionaliza-tion model (i.e.
MTL-GP) is based on a separablemulti-task kernel (?Alvarez et al., 2012) of the formK(X,X) = B ?
k(X,X), (2)where k(X,X) is a standard kernel over the in-put points and B is a positive semi-definite ma-trix encoding task covariances, called coregion-alization matrix.
B is built from other matricesB = WW>+ diag(k), where W is a matrix thatdetermines the correlations between the differentoutputs and k is a matrix which allows the outputs780(i.e.
tasks) to behave independently.
The repre-sentation of data points is augmented with task idsand given the id of a pair of data points the co-variance from the standard kernel between themis multiplied by a corresponding covariance fromB, which modifies the data points?
covariance de-pending on whether they belong to the same taskor different tasks.The coregionalization matrix B allows us tocontrol the amount of inter and intra task transferof learning among tasks.
Cohn and Specia (2013)propose different types of B matrices to modelthe problem of predicting the quality of machinetranslations.
They developed B matrices that rep-resent an explicit intra-task transfer to be a part ofthe parameterised kernel function.
We use a de-fault B where the weights of the matrix are learntalong with the hyper-parameters by the GP tool.For training our method we use the GPy toolkit2with a combination of RBF and coregionalizationkernels.
The parameters used to build the core-gionalization matrix are the number of outputs tocoregionalize and the rank of W .
For example,in the 2012 training set, the number of outputsto coregionalize is 3, given that we have threetasks/domains.
The B matrix and the RBF kernelhyper-parameters are jointly optimised.
Each in-stance of the training data is then augmented withthe id of their corresponding task.
During test-ing a new instance has to be matched to a specifictask/domain id from the training data.
In the caseof an unknown test domain, we match it to a train-ing domain which is similar, given the descriptionof the test dataset.For the STS 2014 dataset, given the large num-ber of training instances, we train a sparse GPmodel within GPy.
The main limitation of the GPmodel is the that memory demands grow O(n2),and the computational demands grow O(n3), withn equals the number of training instances.
Sparsemethods (e.g.
(Titsias, 2009)) try to overcome thislimitation by constructing an approximation of thefull model on a smaller set of m support or induc-ing instances that allow the reduction of compu-tational demands to O(nm2).
For the sparse GPwe use the same combination of kernels as the fullmodel, where we chose empirically the number ofinducing instances m and the GP tool randomlyselects the instances from the training data.2https://github.com/SheffieldML/GPy3 Results and DiscussionIn what follows we show a comparison with previ-ous work on the STS 2012 and 2013 datasets, andthe official results for English and Spanish STS2014 datasets.3.1 STS 2012 and STS 2013For training we use the STS 2012 training datasetsand we compare the results on the STS 2012 withpublicly available systems and with the officialBaseline, which is based on the cosine metric com-puted over word overlaps.
The official evaluationmetric is Pearson?s correlation.
We match the un-known domain OnWN to MSRpar given that thedomain of paraphrasing is that of news from theweb, which potentially contains a broad enoughvocabulary to cover OnWN.Table 3.1 shows a comparison of the MTL-GPwith previous work on the STS 2012 data, whereour method outperforms them for most of the do-mains.
Our method improves the results of Take-Lab with the same feature set.
In other words,the transfer learning improves over (?Sari?c et al.,2012), which is trained with a separate SupportVector Regression model for each domain.
Wenote that we can only compare our method againstthe simpler version of TakeLab that is available.A different version using syntactic features wasalso proposed, where most results do not show asignificant variation, except for an improvementof r=0.4683 in the SMTnews dataset.
For thecomplete alternative results we refer the reader to(?Sari?c et al., 2012).On the STS 2013 dataset, we compare ourmethod with work based on domain adaptationand the official baseline.
We use the 2012 data fortraining as no additional training data is providedin 2013.
Table 3.1 shows all the possible match-ing combinations between the STS 2013 test setsand STS 2012 training sets.
The best results aregiven by matching the STS 2013 test sets with theMSRvid domain, where all 2013 sets achieve theirbest results.In Table 3.1, we show the comparison withprevious work on the 2013 datasets, where weuse the best matching result from Table 3.1(MSRvid).
Our method shows very competitiveresults but only with the correct matching of do-mains, whereas the worst performed matching(SMTeuroparl, Table 3.1) shows results that arecloser to the official Baseline.
In previous work781Method MSRpar MSRvid SMTeuroparl SMTnews OnWN?Sari?c et al.
(2012) 0.7343 0.8803 0.4771 0.3989 0.6797B?ar et al.
(2012) 0.68 0.8739 0.5280 0.4937 0.6641MTL-GP 0.7324 0.8877 0.5615 0.6053 0.7256Baseline 0.4334 0.2996 0.4542 0.3908 0.5864Table 1: Comparison with previous work on the STS 2012 test datasets.
(Heilman and Madnani, 2013), domain adaptationis performed with the addition of extra featuresand the subsequent extra parameters to the model,where in the MTL-GP the transfer learning is donewith the coregionalization matrix and does not de-pend on large amounts of data.3.2 English STS 2014The training dataset consists of the combination ofeach English training and test STS datasets from2012 and 2013, which results in 7 domains.
Fortesting, in our first run we matched similar do-mains with each other and the unknown domainwith MSRpar.
For our second run, we matchedthe unknown domains with a similar one.
Thedomain matching (test/training) was done as fol-lows: deft-forum/MSRpar, deft-news/SMTnews,tweet-news/SMTnews and images/MSRvid.
Forour third run, the difference in matching is for deft-news/headlines and tweet-news/headlines, wherethe other domains remain with the same match-ing.
Table 3.2 shows the official STS 2014 resultswhere our best method (i.e.
run3) achieves rank10.In Table 3.2, we show the comparison of theMTL-GP and the sparse MTL-GP with the best2014 system (DLSCU run2).
For both MTL meth-ods we match the 2014 domains with the train-ing domain headlines.
For the sparse MTL-GP,we chose empirically a number m of 500 ran-domly induced points.
For reference, the corre-lation of sparse MTL-GP with 50 points on deft-forum is r=0.4691 obtained in 0.23 hours, with100 points, r=0.4895, with 500 points, r=0.4912,and with 1000 points, r=0.4911.
The sparse MTL-TestTrainMSRvid MSRpar SMTeuroparlHeadlines0.6666 0.6595 0.5693OnWN0.6516 0.4635 0.4113FNWN0.4062 0.3217 0.2344Table 2: Matching of new 2013 domains with2012 training data.GP with 500 points runs in 1.38 hours, comparedto 2.39 hours for the full MTL-GP3.
Addition-ally, the sparse version achieves similar results tothe full model and very competitive performancecompared to the best STS 2014 system.
However,the result for OnWN is substantially lower than thebest system.
This result can be highly improved(r=0.7990) if the test set is matched with the cor-respondent training domain.3.3 Spanish STS 2014For the Spanish STS subtask we use both sim-ple and state-of-the-art (SoA) features to train theMTL-GP.
The simple features are similarity scoresfrom string metrics such as Levenshtein, Gotoh,Jaro, etc.4The SoA similarity features come againfrom TakeLab.
The training dataset consists of thecombination of each English STS domains from2012 and 2013 and the Spanish trial dataset withtask-id matching each instance to a given domain.We represent the feature vectors with sparse fea-tures for the English and Spanish training datasets,where in English the pairs have simple and SoAfeatures, and for Spanish, only the simple features.In other words, the feature vectors have the samenumber of features (34): 13 simple features and 21SoA features.
However, for Spanish the SoA fea-tures are set to 0 in training and testing.
The moti-vation to use SoA and simple features in English isthat the extra information will improve the transferlearning on the English domains and discriminatebetween the English domains and the Spanish do-main, which only contains simple features.
Fortesting we only extracted the simple features; theSoA features were set to 0.
For the coregionaliza-tion matrix we set the number of domains to be theEnglish STS domains from 2012 and 2013, plusthe Spanish trial, where the Spanish is treated as anadditional domain, which results in 8 domains.
Inthe first run of testing, we matched the test datasetsto the Spanish domain, and in the second run wematched the datasets to the English MSRpar do-3Intel Xeon(R) at 2.67GHz with 24 cores4https://github.com/Simmetrics/simmetrics782Method Headlines OnWN FNWNHeilman and Madnani (2013) 0.7601 0.4631 0.3516Severyn et al.
(2013) 0.7465 0.5572 0.3875MTL-GP 0.6666 0.6516 0.4062Baseline 0.5399 0.2828 0.2146Table 3: Comparison between best matching MTL-GP (MSRvid) and previous work on the STS 2013test datasets.Run deft-forum deft-news headlines images OnWN tweet-news Weighted mean rankUoW run1 0.3419 0.7512 0.7535 0.7763 0.7990 0.7368 0.7143 11UoW run2 0.3419 0.5875 0.7535 0.7877 0.7990 0.6281 0.6817 17UoW run3 0.3419 0.7634 0.7535 0.7877 0.7990 0.7529 0.7207 10Table 4: Official English STS 2014 results.main.
Table 3.3 shows the official results for theSpanish subtask, where our method achieves com-petitive performance, placed 7 in the systems rank-ing.
We only show the results for the first run asboth runs achieved the same performance.Run Wikipedia News WeightedmeanrankUoW 0.7483 0.8001 0.7792 7Table 6: Official Spanish STS 2014 results.Table 3.3 shows the comparison of the bestSpanish STS 2014 system (UMCC DLSI run2)against two different sparse MTL-GP matchedwith the Spanish trial with 500 inducing points.Sparse MTL-GP run1 uses the sparse features de-scribed above, while run2 uses a modification ofthe feature set consisting in specific features foreach type of domain.
For the English domainsthe simple features are set to 0, and for Spanishthe SoA are still set to 0.
The difference betweensparse MTL-GP models is very small, where theuse of all the features on the English domains im-proves the results.
However, the performance ofboth models is still substantially lower than that ofthe best system.Run Wikipedia NewsUMCC DLSI run2 0.7802 0.8254Sparse MTL-GP run1 0.7468 0.7959Sparse MTL-GP run2 0.7380 0.7878Table 7: Comparison of best system against sparseMTL-GP STS 2014 results.4 ConclusionsWe propose the use of MTL for STS.
We showthat MTL improves the results of one of the bestSTS systems, TakeLab.
However, the match-ing of an unknown domain during testing proveda key challenge that affects performance signifi-cantly.
Given the results of STS 2013 and 2014,our method tends to achieve best results whenknown/unknown domains are matched to the sametraining domains (i.e.
MSRpar for 2013 and head-lines for 2014).
The sparse MTL-GP shows sim-ilar performance to the full GP model, but takeshalf the time to be trained.
In the Spanish subtask,we train our method with English datasets and theSpanish trial data as an additional domain.
Forthis subtask our method also shows competitive re-sults.
Future work involves the automatic match-ing of unknown domains at test time via meta-classification (Severyn et al., 2013).AcknowledgmentsThis work was supported by the Mexican NationalCouncil for Science and Technology (CONA-CYT), scholarship reference 309261, and by theQTLaunchPad (EU FP7 CSA No.
296347)project.ReferencesEneko Agirre, Mona Diab, Daniel Cer, and AitorGonzalez-Agirre.
2012.
Semeval-2012 task 6: Apilot on semantic textual similarity.
In Proceedingsof the First Joint Conference on Lexical and Com-putational Semantics, SemEval ?12, pages 385?393,Stroudsburg, PA, USA.Mauricio A.?Alvarez, Lorenzo Rosasco, and Neil D.Lawrence.
2012.
Kernels for vector-valued func-tions: A review.
Found.
Trends Mach.
Learn.,4(3):195?266, March.Daniel B?ar, Chris Biemann, Iryna Gurevych, andTorsten Zesch.
2012.
Ukp: Computing seman-tic textual similarity by combining multiple contentsimilarity measures.
In Proceedings of the First783Run deft-forum deft-news headlines images OnWN tweet-newsDLSCU run2 0.4828 0.7657 0.7646 0.8214 0.8589 0.7639Best matching MTL-GP 0.4903 0.7633 0.7535 0.8063 0.7222 0.7528Sparse MTL-GP 0.4910 0.7642 0.7540 0.8057 0.7276 0.7539Table 5: Comparison between best matching MTL-GP (headlines), Sparse MTL-GP and best STS 2014system.Joint Conference on Lexical and Computational Se-mantics, SemEval ?12, pages 435?440, Stroudsburg,PA, USA.Trevor Cohn and Lucia Specia.
2013.
Modelling an-notator bias with multi-task gaussian processes: Anapplication to machine translation quality estima-tion.
In 51st Annual Meeting of the Association forComputational Linguistics, ACL-2013, pages 32?42, Sofia, Bulgaria.Hal Daum?e, III, Abhishek Kumar, and Avishek Saha.2010.
Frustratingly easy semi-supervised domainadaptation.
In Proceedings of the 2010 Workshopon Domain Adaptation for Natural Language Pro-cessing, DANLP 2010, pages 53?59, Stroudsburg,PA, USA.Michael Heilman and Nitin Madnani.
2013.
Henry-core: Domain adaptation and stacking for text simi-larity.
In Second Joint Conference on Lexical andComputational Semantics (*SEM), pages 96?102,Atlanta, Georgia, USA, June.Chin-Yew Lin and Eduard Hovy.
2003.
Auto-matic evaluation of summaries using n-gram co-occurrence statistics.
In Proceedings of the 2003Conference of the North American Chapter of theAssociation for Computational Linguistics on Hu-man Language Technology - Volume 1, NAACL ?03,pages 71?78, Stroudsburg, PA, USA.Dekang Lin and Patrick Pantel.
2001.
Discovery ofinference rules for question-answering.
Nat.
Lang.Eng., 7(4):343?360.Eui-Kyu Park, Dong-Yul Ra, and Myung-Gil Jang.2005.
Techniques for improving web retrieval effec-tiveness.
Inf.
Process.
Manage., 41(5):1207?1223.Carl Edward Rasmussen.
2006.
Gaussian processesfor machine learning.
MIT Press.Aliaksei Severyn, Massimo Nicosia, and AlessandroMoschitti.
2013. ikernels-core: Tree kernel learningfor textual similarity.
In Second Joint Conferenceon Lexical and Computational Semantics (*SEM),pages 53?58, Atlanta, Georgia, USA, June.Michalis Titsias.
2009.
Variational Learning of Induc-ing Variables in Sparse Gaussian Processes.
In the12th International Conference on Artificial Intelli-gence and Statistics (AISTATS).Frane?Sari?c, Goran Glava?s, Mladen Karan, Jan?Snajder,and Bojana Dalbelo Ba?si?c.
2012.
Takelab: Systemsfor measuring semantic text similarity.
In Proceed-ings of the First Joint Conference on Lexical andComputational Semantics, SemEval ?12, pages 441?448, Stroudsburg, PA, USA.784
