Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 682?691,Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational LinguisticsMultilingual Affect Polarity and Valence Prediction in Metaphor-RichTextsZornitsa KozarevaUSC Information Sciences Institute4676 Admiralty WayMarina del Rey, CA 90292-6695kozareva@isi.eduAbstractMetaphor is an important way of convey-ing the affect of people, hence understand-ing how people use metaphors to conveyaffect is important for the communicationbetween individuals and increases cohe-sion if the perceived affect of the con-crete example is the same for the two in-dividuals.
Therefore, building computa-tional models that can automatically iden-tify the affect in metaphor-rich texts like?The team captain is a rock.
?, ?Time ismoney.
?, ?My lawyer is a shark.?
is animportant challenging problem, which hasbeen of great interest to the research com-munity.To solve this task, we have collectedand manually annotated the affect ofmetaphor-rich texts for four languages.We present novel algorithms that integratetriggers for cognitive, affective, perceptualand social processes with stylistic and lex-ical information.
By running evaluationson datasets in English, Spanish, Russianand Farsi, we show that the developed af-fect polarity and valence prediction tech-nology of metaphor-rich texts is portableand works equally well for different lan-guages.1 IntroductionMetaphor is a figure of speech in which a wordor phrase that ordinarily designates one thing isused to designate another, thus making an implicitcomparison (Lakoff and Johnson, 1980; Martin,1988; Wilks, 2007).
For instance, in?My lawyer is a shark?the speaker may want to communicate that his/herlawyer is strong and aggressive, and that he willattack in court and persist until the goals areachieved.
By using the metaphor, the speaker ac-tually conveys positive affect because having anaggressive lawyer is good if one is being sued.There has been a substantial body of work onmetaphor identification and interpretation (Wilks,2007; Shutova et al, 2010).
However, in thispaper we focus on an equally interesting, chal-lenging and important problem, which concernsthe automatic identification of affect carried bymetaphors.
Building such computational mod-els is important to understand how people usemetaphors to convey affect and how affect is ex-pressed using metaphors.
The existence of suchmodels can be also used to improve the communi-cation between individuals and to make sure thatthe speakers perceived the affect of the concretemetaphor example in the same way.The questions we address in this paper are:?How can we build computational models that canidentify the polarity and valence associated withmetaphor-rich texts??
and ?Is it possible to buildsuch automatic models for multiple languages?
?.Our main contributions are:?
We have developed multilingual metaphor-rich datasets in English, Spanish, Russian andFarsi that contain annotations of the Positiveand Negative polarity and the valence (from?3 to +3 scale) corresponding to the inten-sity of the affect conveyed in the metaphor.?
We have proposed and developed automatedmethods for solving the polarity and valencetasks for all four languages.
We modelthe polarity task as a classification problem,while the valence task as a regression prob-lem.?
We have studied the influence of different in-formation sources like the metaphor itself,the context in which it resides, the source and682target domains of the metaphor, in addition tocontextual features and trigger word lists de-veloped by psychologists (Tausczik and Pen-nebaker, 2010).?
We have conducted in depth experimentalevaluation and showed that the developedmethods significantly outperform baselinemethods.The rest of the paper is organized as follows.Section 2 describes related work, Section 3 brieflytalks about metaphors.
Sections 4 and 5 describethe polarity classification and valence predictiontasks for affect of metaphor-rich texts.
Both sec-tions have information on the collected data forEnglish, Spanish, Russian and Farsi, the con-ducted experiments and obtained results.
Finally,we conclude in Section 6.2 Related WorkA substantial body of work has been done on de-termining the affect (sentiment analysis) of texts(Kim and Hovy, 2004; Strapparava and Mihalcea,2007; Wiebe and Cardie, 2005; Yessenalina andCardie, 2011; Breck et al, 2007).
Various taskshave been solved among which polarity and va-lence identification are the most common.
Whilepolarity identification aims at finding the Positiveand Negative affect, valence is more challengingas it has to map the affect on a [?3,+3] scaledepending on its intensity (Polanyi and Zaenen,2004; Strapparava and Mihalcea, 2007).Over the years researchers have developed vari-ous approaches to identify polarity of words (Esuliand Sebastiani, 2006), phrases (Turney, 2002; Wil-son et al, 2005), sentences (Choi and Cardie,2009) even documents (Pang and Lee, 2008).Multiple techniques have been employed, fromvarious machine learning classifiers, to clusteringand topic models.
Various domains and textualsources have been analyzed such as Twitter, Blogs,Web documents, movie and product reviews (Tur-ney, 2002; Kennedy and Inkpen, 2005; Niu et al,2005; Pang and Lee, 2008), but yet what is miss-ing is affect analyzer for metaphor-rich texts.While the affect of metaphors is well stud-ied from its linguistic and psychological aspects(Blanchette et al, 2001; Tomlinson and Love,2006; Crawdord, 2009), to our knowledge thebuilding of computational models for polarity andvalence identification in metaphor-rich texts is stilla novel task (Smith et al, 2007; Veale, 2012; Vealeand Li, 2012; Reyes and Rosso, 2012; Reyes etal., 2013).
Little (almost no) effort has been putinto multilingual computational affect models ofmetaphor-rich texts.
Our research specifically tar-gets the resolution of these problems and showsthat it is possible to build such computational mod-els.
The experimental result provide valuable con-tributions and fundings, which could be used bythe research community to build upon.3 MetaphorsAlthough there are different views on metaphor inlinguistics and philosophy (Black, 1962; Lakoffand Johnson, 1980; Gentner, 1983; Wilks, 2007),the common among all approaches is the idea ofan interconceptual mapping that underlies the pro-duction of metaphorical expressions.
There aretwo concepts or conceptual domains: the target(also called topic in the linguistics literature) andthe source (or vehicle), and the existence of a linkbetween them gives rise to metaphors.The texts ?Your claims are indefensible.?
and?He attacked every weak point in my argument.
?do not directly talk about argument as a war, how-ever the winning or losing of arguments, the attackor defense of positions are structured by the con-cept of war.
There is no physical battle, but thereis a verbal battle and the structure of an argument(attack, defense) reflects this (Lakoff and Johnson,1980).As we mentioned before, there has been a lot ofwork on the automatic identification of metaphors(Wilks, 2007; Shutova et al, 2010) and theirmapping into conceptual space (Shutova, 2010a;Shutova, 2010b), however these are beyond thescope of this paper.
Instead we focus on an equallyinteresting, challenging and important problem,which concerns the automatic identification of af-fect carried by metaphors.
To conduct our study,we use human annotators to collect metaphor-richtexts (Shutova and Teufel, 2010) and tag eachmetaphor with its corresponding polarity (Posi-tive/Negative) and valence [?3,+3] scores.
Thenext sections describe the affect polarity and va-lence tasks we have defined, the collected and an-notated metaphor-rich data for each one of the En-glish, Spanish, Russian and Farsi languages, theconducted experiments and obtained results.6834 Task A: Polarity Classification4.1 Problem FormulationTask Definition: Given metaphor-rich texts annotated withPositive and Negative polarity labels, the goal is to build anautomated computational affect model, which can assign topreviously unseen metaphors one of the two polarity classes.a tough pill to swallowvalues that gave our nation birthClinton also came into office hoping to bridge Washington?s partisan divide.Thirty percent of our mortgages are underwater.The administration, in fact, could go further with the budget knife by eliminating the V-22 Osprey aircraftthe 'things' are going to make sure their ox doesn't get goredFigure 1: Polarity ClassificationFigure 1 illustrates the polarity task in which themetaphors were classified into Positive or Nega-tive.
For instance, the metaphor ?tough pill toswallow?
has Negative polarity as it stands forsomething being hard to digest or comprehend,while the metaphor ?values that gave our nationbirth?
has a Positive polarity as giving birth is likestarting a new beginning.4.2 Classification AlgorithmsWe model the metaphor polarity task as a classifi-cation problem in which, for a given collection ofN training examples, where mi is a metaphor andci is the polarity of mi, the objective is to learna classification function f : mi ?
ci in which 1stands for positive polarity and 0 stands for neg-ative polarity.
We tested five different machinelearning algorithms such as Nave Bayes, SVMwith polynomial kernel, SVM with RBF kernel,AdaBoost and Stacking, out of which AdaBoostperformed the best.
In our experimental study, weuse the freely available implementations in Weka(Witten and Frank, 2005).Evaluation Measures: To evaluate the goodnessof the polarity classification algorithms, we cal-culate the f-score and accuracy on 10-fold crossvalidation.4.3 Data AnnotationTo conduct our experimental study, we have usedannotated data provided by the Language Com-puter Corporation (LCC)1, which developed anno-1http://www.languagecomputer.com/tation toolkit specifically for the task of metaphordetection, interpretation and affect assignment.They hired annotators to collect and annotate datafor the English, Spanish, Russian and Farsi lan-guages.
The domain for which the metaphors werecollected was Governance.
It encompasses elec-toral politics, the setting of economic policy, thecreation, application and enforcement of rules andlaws.
The metaphors were collected from polit-ical speeches, political websites, online newspa-pers among others (Mohler et al, 2013).The annotation toolkit allowed annotators toprovide for each metaphor the following infor-mation: the metaphor, the context in which themetaphor was found, the meaning of the metaphorin the source and target domains from the per-spective of a native speaker.
For example, in theContext: And to all nations, we will speak for thevalues that gave our nation birth.
; the annotatorstagged the Metaphor: values that gave our nationbirth; and listed as Source: mother gave birth tobaby; and Target: values of freedom and equal-ity motivated the creation of America.
The sameannotators also provided the affect associated withthe metaphor.
The agreements of the annotators asmeasured by LCC are: .83, .87, .80 and .61 for theEnglish, Spanish, Russian and Farsi languages.In our study, the maximum length of a metaphoris a sentence, but typically it has the span of aphrase.
The maximum length of a context is threesentences before and after the metaphor, but typ-ically it has the span of one sentence before andafter.
In our study, the source and target domainsare provided by the human annotators who agreeon these definitions, however the source and targetcan be also automatically generated by an inter-pretation system or a concept mapper.
The gen-eration of source and target information is beyondthe scope of this paper, but studying their impacton affect is important.
At the same time, we wantto show that if the technology for source/target de-tection and interpretation is not yet available, thenhow far can one reach by using the metaphor itselfand the context around it.
Later depending on theavailability of the information sources and toolkitsone can decide whether to integrate such informa-tion or to ignore it.
In the experimental sections,we show how the individual information sourcesand their combination affects the resolution of themetaphor polarity and valence prediction tasks.Table 1 shows the positive and negative class684distribution for each one of the four languages.Negative PositiveENGLISH 2086 1443SPANISH 196 434RUSSIAN 468 418FARSI 384 252Table 1: Polarity Class Distribution for Four Lan-guagesThe majority of the the annotated examples arefor English.
However, given the difficulty of find-ing bilingual speakers, we still managed to collectaround 600 examples for Spanish and Farsi, and886 examples for Russian.4.4 N-gram Evaluation and ResultsN-gram features are widely used in a variety ofclassification tasks, therefore we also use them inour polarity classification task.
We studied the in-fluence of unigrams, bigrams and a combinationof the two, and saw that the best performing fea-ture set consists of the combination of unigramsand bigrams.
In this paper, we will refer from nowon to n-grams as the combination of unigrams andbigrams.Figure 2 shows a study of the influence of thedifferent information sources and their combina-tion with n-gram features for English.!"#$!%#$!&#$!'#$!(#$!)#$!
*#$!+#$!,#$%$-./01234$53647.$ /048./$53647.9/048./$ 73:/.;/$-./01234953647.9/048./$73:/.
;/953647.9/048./$Figure 2: Influence of Information Sources forMetaphor Polarity Classification of English TextsFor each information source (metaphor, context,source, target and their combinations), we built aseparate n-gram feature set and model, which wasevaluated on 10-fold cross validation.
The resultsfrom this study show that for English, the moreinformation sources one combines, the higher theclassification accuracy becomes.Table 2 shows the influence of the informationsources for Spanish, Russian and Farsi with the n-gram features.
The best f-scores for each languageare shown in bold.
For Farsi and Russian high per-formances are obtained both with the context andwith the combination of the context, source andtarget information.
While for Spanish they reachsimilar performance.SPANISH RUSSIAN FARSIMetaphor 71.6 71.0 62.4Source 67.1 62.4 55.4Target 68.9 67.2 62.4Context 73.5 77.1 67.4S+T 76.6 68.7 62.4M+S+T 76.0 75.4 64.2C+S+T 76.5 76.5 68.4Table 2: N-gram features, F-scores on 10-fold val-idation for Spanish, Russian and Farsi4.5 LIWC as a Proxy for Metaphor PolarityLIWC Repository: In addition to the n-gramfeatures, we also used the Linguistic Inquiry andWord Count (LIWC) repository (Tausczik andPennebaker, 2010), which has 64 word categoriescorresponding to different classes like emotionalstates, psychological processes, personal concernsamong other.
Each category contains a list ofwords characterizing it.
For instance, the LIWCcategory discrepancy contains words like should,could among others, while the LIWC category in-hibition contains words like block, stop, constrain.Previously LIWC was successfully used to ana-lyze the emotional state of bloggers and tweeters(Quercia et al, 2011) and to identify deception andsarcasm in texts (Ott et al, 2011; Gonza?lez-Iba?n?ezet al, 2011).
When LIWC analyzes texts it gener-ates statistics like number of words found in cat-egory Ci divided by the total number of words inthe text.
For our metaphor polarity task, we useLIWC?s statistics of all 64 categories and feed thisinformation as features for the machine learningclassifiers.
LIWC repository contains conceptualcategories (dictionaries) both for the English andSpanish languages.LIWC Evaluation and Results: In our experi-ments LIWC is applied to English and Spanishmetaphor-rich texts since the LIWC category dic-tionaries are available for both languages.
Table 3shows the obtained accuracy and f-score results inEnglish and Spanish for each one of the informa-tion sources.685ENGLISH SPANISHAcc Fscore Acc FscoreMetaphor 98.8 98.8 87.9 87.2Source 98.6 98.6 97.3 97.3Target 98.2 98.2 97.9 97.9Context 91.4 91.4 93.3 93.2S+T 98.0 98.0 76.3 75.5M+S+T 95.8 95.7 86.8 86.0C+S+T 87.9 88.0 79.2 78.5Table 3: LIWC features, Accuracy and F-scoreson 10-fold validation for English and SpanishThe best performances are reached with indi-vidual information sources like metaphor, context,source or target instead of their combinations.
Theclassifiers obtain similar performance for both lan-guages.LIWC Category Relevance to Metaphor Polar-ity: We also study the importance and relevanceof the LIWC categories for the metaphor polar-ity task.
We use information gain (IG) to mea-sure the amount of information in bits about thepolarity class prediction, if the only informationavailable is the presence of a given LIWC cate-gory (feature) and the corresponding polarity classdistribution.
IG measures the expected reductionin entropy (uncertainty associated with a randomfeature) (Mitchell, 1997).Figure 3 illustrates how certain categories occurmore with the positive (in red color) vs negative(in green color) class.
With the positive metaphorswe observe the LIWC categories for present tense,social, affect and family, while for the negativemetaphors we see LIWC categories for past tense,inhibition and anger.!"#$%$&'#&%()*%!&)#+'%"',&)%-"$&,+).%!)&#&'$%$&'#&%!&)#+'"/%!)+'+0'#%1"23/.%-"$&,+).%#+-3"/%!
)&#&'-&%45%6%5%Figure 3: LIWC category relevance to MetaphorPolarityIn addition, we show in Figure 4 examples ofthe top LIWC categories according to IG rankingfor each one of the information sources.!
"#$%&'() *'+#",#) -'.
(/") 0$(1"#)2) 2) !
""#$ /'+34)/'+34) %&'$ 5+1"6#) $+1"()$+1"() ()"*)"$ ("7$895#:) $;"/#)+,(-.
"/01-%$ /0(2$2"1("$ &'<") ='(>)6="$()='(?6) 6="$()='(?6) 6="$()='(?6)5+&5@58'+) !.,"1+$ .
"#,3,&1$4&+%$ $;"/#) (/0-"$("7$895#:) (0+$&'<") 5+&5@58'+)5+1"6#) 5+1"6#)='(>) ='(>)@7'/>)/'+6#($5+)6#'%)6&'.7?)/'.7?)='.7?)&$#")>577)$++':"?
)Figure 4: Example of LIWC Categories andWordsFor metaphor texts, these categories are I, con-juntion, anger, discrepancy, swear words amongothers; for contexts the categories are pronounslike I, you, past tense, friends, affect and so on.Our study shows that some of the LIWC categoriesare important across all information sources, butoverall different triggers activate depending on theinformation source and the length of the text used.4.6 Comparative studyFigure 5 shows a comparison of the accuracy ofour best performing approach for each language.For English and Spanish these are the LIWC mod-els, while for Russian and Farsi these are the n-gram models.
We compare the performance of thealgorithms with a majority baseline, which assignsthe majority class to each example.
For instance,in English there are 3529 annotated examples, ofwhich 2086 are positive and 1443 are negative.Since the positive class is the predominant onefor this language and dataset, a majority classifierwould have .59 accuracy in returning the positiveclass as an answer.
Similarly, we compute the ma-jority baseline for the rest of the languages.!
""#$%"&' (%)*$+,&'-%./0+1/' 2+3/$/1"/'''4150+.6' 7898:' ;79<<' =>79?7'@A%1+.6' 7B97:' ?8988' =C79:C'D#..+%1' BB9::' ;C98C' =CE9<8'F%$.+' BC9C:' ?
:9>:' =<<97:'Figure 5: Best Accuracy Model and Comparisonagainst a Majority Baseline for Metaphor PolarityClassificationAs we can see from Figure 5 that all classi-fiers significantly outperform the majority base-686line.
For Farsi the increment is +11.90, while forEnglish the increment is +39.69.
This means thatthe built classifiers perform much better than a ran-dom classifier.4.7 Lessons LearnedTo summarize, in this section we have defined thetask of polarity classification and we have pre-sented a machine learning solution.
We have useddifferent feature sets and information sources tosolve the task.
We have conducted exhaustiveevaluations for four different languages namelyEnglish, Spanish, Russian and Farsi.
The learnedlessons from this study are: (1) for n-gram us-age, the larger the context of the metaphor, thebetter the classification accuracy becomes; (2) ifpresent source and target information can furtherboost the performance of the classifiers; (3) LIWCis a useful resource for polarity identification inmetaphor-rich texts; (4) analyzing the usages oftense like past vs. present and pronouns are im-portant triggers for positive and negative polarityof metaphors; (5) some categories like family, so-cial presence indicate positive polarity, while oth-ers like inhibition, anger and swear words are in-dicative of negative affect; (6) the built models sig-nificantly outperform majority baselines.5 Task B: Valence Prediction5.1 Problem FormulationTask Definition: Given metaphor-rich texts annotatedwith valence score (from ?3 to +3), where ?3 indicatesstrong negativity, +3 indicates strong positivity, 0 indi-cates neural, the goal is to build a model that can predictwithout human supervision the valence scores of new pre-viously unseen metaphors.The administration, in fact, could go further with the budget knife by eliminating the V-22 Osprey aircraftClinton also came into office hoping to bridge Washington?s partisan divide.values that gave our nation birth  !
"#!$#!%#a tough pill to swallow  &"#the 'things' are going to make sure their ox doesn't get gored  &$#Thirty percent of our mortgages are underwater.
&%#Figure 6: Valence PredictionFigure 6 shows an example of the valence pre-diction task in which the metaphor-rich texts mustbe arranged by the intensity of the emotionalstate provoked by the texts.
For instance, ?3corresponds to very strong negativity, ?2 strongnegativity, ?1 weak negativity (similarly for thepositive classes).
In this task we also considermetaphors with neutral affect.
They are annotatedwith the 0 label and the prediction model should beable to predict such intensity as well.
For instance,the metaphor ?values that gave our nation birth?,is considered by American people that giving birthsets new beginning and has a positive score +1,but ?budget knife?
is more positive +3 since taxcut is more important.
As any sentiment analysistask, affect assignment of metaphors is also a sub-jective task and the produced annotations expressthe values, believes and understanding of the an-notators.5.2 Regression ModelWe model the valence task a regression prob-lem, in which for a given metaphor m, we seekto predict the valence v of m. We do this viaa parametrized function f :v?
= f(m;w), wherew ?
Rd are the weights.
The objective is tolearn w from a collection of N training examples{< mi, vi >}Ni=1, where mi are the metaphor ex-amples and vi ?
R is the valence score of mi.Support vector regression (Drucker et al, 1996)is a well-known method for training a regressionmodel by solving the following optimization prob-lem:minw?Rs12 ||w||2 + CNN?i=1max(0, |vi ?
f(mi;w)| ?
)?
??
?-insensitive loss functionwhere C is a regularization constant and  controlsthe training error.
The training algorithm findsweights w that define a function f minimizing theempirical risk.
Let h be a function from seeds intosome vector-space representation ?
Rd, then thefunction f takes the form: f(m;w) = h(m)Tw =?Ni=1 ?iK(m,mi), where f is re-parameterizedin terms of a polynomial kernel function K withdual weights ?i.
K measures the similarity be-tween two metaphoric texts.
Full details of theregression model and its implementation are be-yond the scope of this paper; for more details see(Scho?lkopf and Smola, 2001; Smola et al, 2003).In our experimental study, we use the freely avail-able implementation of SVM in Weka (Witten andFrank, 2005).687Evaluation Measures: To evaluate the quality ofthe valence prediction model, we compare the ac-tual valence score of the metaphor given by humanannotators denoted with y against those valencescores predicted by the regression model denotedwith x.
We estimate the goodness of the regres-sion model calculating both the correlation coef-ficient ccx,y = n?xiyi??xi?yi?n?x2i?(?xi)2?n?y2i?
(?yi)2and the mean squared error msex,y =?ni=i(x?x?
)n .The two evaluation measures should be interpretedin the following manner.
Intuitively the higher thecorrelation score is, the better the correlation be-tween the actual and the predicted valence scoreswill be.
Similarly the smaller the mean squarederror rate, the better the regression model fits thevalence predictions to the actual score.5.3 Data AnnotationTo conduct our valence prediction study, we usedthe same human annotators from the polarity clas-sification task for each one of the English, Span-ish, Russian and Farsi languages.
We asked theannotators to map each metaphor on a [?3,+3]scale depending on the intensity of the affect asso-ciated with the metaphor.Table 4 shows the distribution (number of ex-amples) for each valence class and for each lan-guage.-3 -2 -1 0 +1 +2 +3ENGLISH 1057 817 212 582 157 746 540SPANISH 106 65 27 17 40 132 262RUSSIAN 118 42 308 13 202 149 67FARSI 147 117 120 49 91 63 98Table 4: Valence Score Distribution for Each Lan-guage5.4 Empirical Evaluation and ResultsFor each language and information source we builtseparate valence prediction regression models.
Weused the same features for the regression task aswe have used in the classification task.
Those in-clude n-grams (unigrams, bigrams and combina-tion of the two), LIWC scores.
Table 5 showsthe obtained correlation coefficient (CC) and meansquared error (MSE) results for each one of thefour languages (English, Spanish, Russian andFarsi) using the dataset described in Table 4.The Farsi and Russian regression models arebased only on n-gram features, while the Englishand Spanish regression models have both n-gramand LIWC features.
Overall, the CC for Englishand Spanish is higher when LIWC features areused.
This means that the LIWC based valence re-gression model approximates the predicted valuesbetter to those of the human annotators.
The bettervalence prediction happens when the metaphor it-self is used by LIWC.
The MSE for English andSpanish is the lowest, meaning that the predic-tion is the closest to those of the human annota-tors.
In Russian and Farsi the lowest MSE is whenthe combined metaphor, source and target infor-mation sources are used.
For English and Spanishthe smallest MSE or so called prediction error is1.52 and 1.30 respectively, while for Russian andFarsi is 1.62 and 2.13 respectively.5.5 Lessons LearnedTo summarize, in this section we have definedthe task of valence prediction of metaphor-richtexts and we have described a regression modelfor its solution.
We have studied different fea-ture sets and information sources to solve the task.We have conducted exhaustive evaluations in allfour languages namely English, Spanish, Russianand Farsi.
The learned lessons from this studyare: (1) valence prediction is a much harder taskthan polarity classification both for human annota-tion and for the machine learning algorithms; (2)the obtained results showed that despite its dif-ficulty this is still a plausible problem; (3) sim-ilarly to the polarity classification task, valenceprediction with LIWC is improved when shortercontexts (the metaphor/source/target informationsource) are considered.6 ConclusionPeople use metaphor-rich language to express af-fect and often affect is expressed through the usageof metaphors.
Therefore, understanding that themetaphor ?I was boiling inside when I saw him.
?has Negative polarity as it conveys feeling of angeris very important for interpersonal or multiculturalcommunications.In this paper, we have introduced a novel corpusof metaphor-rich texts for the English, Spanish,Russian and Farsi languages, which was manu-ally annotated with the polarity and valence scoresof the affect conveyed by the metaphors.
Wehave studied the impact of different informationsources such as the metaphor in isolation, the con-text in which the metaphor was used, the sourceand target domain meanings of the metaphor and688RUSSIAN N-gram FARSI N-gram ENGLISH N-gram SPANISH N-gram ENGLISH LIWC SPANISH LIWCCC MSE CC MSE CC MSE CC MSE CC MSE CC MSEMetaphor .45 1.71 .25 2.25 .36 2.50 .37 2.54 .74 1.52 .87 1.20Source .22 1.89 .11 2.42 .40 2.27 .22 2.43 .81 1.30 .85 1.28Target .25 1.91 .15 2.47 .37 2.41 .32 2.36 .72 1.56 .85 1.29Context .43 1.83 .32 2.38 .37 2.59 .40 2.37 .40 2.16 .67 1.92S+T .29 1.83 .18 2.38 .40 2.40 .41 2.19 .70 1.60 .78 1.53M+S+T .45 1.62 .29 2.13 .43 2.34 .43 2.14 .67 1.67 .78 1.53C+S+T .42 1.85 .26 2.61 .43 2.52 .39 2.41 .44 2.08 .64 1.96Table 5: Valence Prediction, Correlation Coefficient and Mean Squared Error for English, Spanish, Rus-sian and Farsitheir combination in order to understand how suchinformation helps and impacts the interpretationof the affect associated with the metaphor.
Wehave conducted exhaustive evaluation with multi-ple machine learning classifiers and different fea-tures sets spanning from lexical information topsychological categories developed by (Tausczikand Pennebaker, 2010).
Through experiments car-ried out on the developed datasets, we showed thatthe proposed polarity classification and valenceregression models significantly improve baselines(from 11.90% to 39.69% depending on the lan-guage) and work well for all four languages.
Fromthe two tasks, the valence prediction problem wasmore challenging both for the human annotatorsand the automated system.
The mean squared er-ror in valence prediction in the range [?3,+3],where ?3 indicates strong negative and +3 indi-cates strong positive affect for English, Spanishand Russian was around 1.5, while for Farsi wasaround 2.The current findings and learned lessons reflectthe properties of the collected data and its anno-tations.
In the future we are interested in study-ing the affect of metaphors for domains differ-ent than Governance.
We want to conduct stud-ies with the help of social sciences who would re-search whether the tagging of affect in metaphorsdepends on the political affiliation, age, gender orculture of the annotators.
Not on a last place, wewould like to improve the built valence predictionmodels and to collect more data for Spanish, Rus-sian and Farsi.AcknowledgmentsThe author would like to thank the reviewers fortheir helpful comments as well as the LCC anno-tators who have prepared the data and made thiswork possible.
This research is supported by theIntelligence Advanced Research Projects Activ-ity (IARPA) via Department of Defense US ArmyResearch Laboratory contract number W911NF-12-C-0025.
The U.S. Government is authorized toreproduce and distribute reprints for Governmen-tal purposes notwithstanding any copyright anno-tation thereon.
Disclaimer: The views and con-clusions contained herein are those of the authorsand should not be interpreted as necessarily rep-resenting the official policies or endorsements, ei-ther expressed or implied, of IARPA, DoD/ARL,or the U.S. Government.ReferencesMax Black.
1962.
Models and Metaphors.Isabelle Blanchette, Kevin Dunbar, John Hummel, andRichard Marsh.
2001.
Analogy use in naturalis-tic settings: The influence of audience, emotion andgoals.
Memory and Cognition, pages 730?735.Eric Breck, Yejin Choi, and Claire Cardie.
2007.
Iden-tifying expressions of opinion in context.
In Pro-ceedings of the 20th international joint conferenceon Artifical intelligence, IJCAI?07, pages 2683?2688.
Morgan Kaufmann Publishers Inc.Yejin Choi and Claire Cardie.
2009.
Adapting a po-larity lexicon using integer linear programming fordomain-specific sentiment classification.
In Pro-ceedings of the 2009 Conference on Empirical Meth-ods in Natural Language Processing: Volume 2 -Volume 2, EMNLP ?09, pages 590?598.Elizabeth Crawdord.
2009.
Conceptual metaphors ofaffect.
Emotion Review, pages 129?139.Harris Drucker, Chris J.C. Burges, Linda Kaufman,Alex Smola, and Vladimir Vapnik.
1996.
Supportvector regression machines.
In Advances in NIPS,pages 155?161.Andrea Esuli and Fabrizio Sebastiani.
2006.
Sen-tiwordnet: A publicly available lexical resourcefor opinion mining.
In In Proceedings of the 5thConference on Language Resources and Evaluation(LREC06, pages 417?422.Dedre Gentner.
1983.
Structure-mapping: A theo-retical framework for analogy.
Cognitive Science,7(2):155?170.689Roberto Gonza?lez-Iba?n?ez, Smaranda Muresa n, andNina Wacholder.
2011.
Identifying sarcasm in twit-ter: a closer look.
In Proceedings of the 49th An-nual Meeting of the Association for ComputationalLinguistics: Human Language Technologies: shortpapers - Volume 2, HLT ?11, pages 581?586.Alistair Kennedy and Diana Inkpen.
2005.
Sentimentclassification of movie and product reviews usingcontextual valence shifters.
Computational Intelli-gence, pages 110?125.Soo-Min Kim and Eduard Hovy.
2004.
Determin-ing the sentiment of opinions.
In Proceedings ofthe 20th international conference on ComputationalLinguistics, COLING ?04.George Lakoff and Mark Johnson.
1980.
MetaphorsWe Live By.
University of Chicago Press, Chicago.James H. Martin.
1988.
Representing regularities inthe metaphoric lexicon.
In Proceedings of the 12thconference on Computational linguistics - Volume 1,COLING ?88, pages 396?401.Thomas M. Mitchell.
1997.
Machine Learning.McGraw-Hill, Inc., 1 edition.Michael Mohler, David Bracewell, David Hinote, andMarc Tomlinson.
2013.
Semantic signatures forexample-based linguistic metaphor detection.
InThe Proceedings of the First Workshop on Metaphorin NLP, (NAACL), pages 46?54.Yun Niu, Xiaodan Zhu, Jianhua Li, and Graeme Hirst.2005.
Analysis of polarity information in medicaltext.
In In: Proceedings of the American MedicalInformatics Association 2005 Annual Symposium,pages 570?574.Myle Ott, Yejin Choi, Claire Cardie, and Jeffrey T.Hancock.
2011.
Finding deceptive opinion spamby any stretch of the imagination.
In Proceedingsof the 49th Annual Meeting of the Association forComputational Linguistics: Human Language Tech-nologies - Volume 1, HLT ?11, pages 309?319.Bo Pang and Lillian Lee.
2008.
Opinion mining andsentiment analysis.
Found.
Trends Inf.
Retr., 2(1-2):1?135, January.Livia Polanyi and Annie Zaenen.
2004.
Contextuallexical valence shifters.
In Yan Qu, James Shana-han, and Janyce Wiebe, editors, Proceedings of theAAAI Spring Symposium on Exploring Attitude andAffect in Text: Theories and Applications.
AAAIPress.
AAAI technical report SS-04-07.Daniele Quercia, Jonathan Ellis, Licia Capra, and JonCrowcroft.
2011.
In the mood for being influentialon twitter.
In the 3rd IEEE International Conferenceon Social Computing.Antonio Reyes and Paolo Rosso.
2012.
Making ob-jective decisions from subjective data: Detectingirony in customer reviews.
Decis.
Support Syst.,53(4):754?760, November.Antonio Reyes, Paolo Rosso, and Tony Veale.
2013.A multidimensional approach for detecting irony intwitter.
Lang.
Resour.
Eval., 47(1):239?268, March.Bernhard Scho?lkopf and Alexander J. Smola.
2001.Learning with Kernels: Support Vector Machines,Regularization, Optimization, and Beyond (Adap-tive Computation and Machine Learning).
The MITPress.Ekaterina Shutova and Simone Teufel.
2010.Metaphor corpus annotated for source - target do-main mappings.
In International Conference onLanguage Resources and Evaluation.Ekaterina Shutova, Lin Sun, and Anna Korhonen.2010.
Metaphor identification using verb and nounclustering.
In Proceedings of the 23rd InternationalConference on Computational Linguistics, COLING?10, pages 1002?1010.Ekaterina Shutova.
2010a.
Automatic metaphor in-terpretation as a paraphrasing task.
In Human Lan-guage Technologies: The 2010 Annual Conferenceof the North American Chapter of the Associa-tion for Computational Linguistics, HLT ?10, pages1029?1037.Ekaterina Shutova.
2010b.
Models of metaphor in nlp.In Proceedings of the 48th Annual Meeting of the As-sociation for Computational Linguistics, ACL ?10,pages 688?697.Catherine Smith, Tim Rumbell, John Barnden, BobHendley, Mark Lee, and Alan Wallington.
2007.Don?t worry about metaphor: affect extraction forconversational agents.
In Proceedings of the 45thAnnual Meeting of the ACL on Interactive Posterand Demonstration Sessions, ACL ?07, pages 37?40.
Association for Computational Linguistics.Alex J. Smola, Bernhard Schlkopf, and Bernhard SchOlkopf.
2003.
A tutorial on support vector regres-sion.
Technical report, Statistics and Computing.Carlo Strapparava and Rada Mihalcea.
2007.
Semeval-2007 task 14: Affective text.
In Proceedings of theFourth International Workshop on Semantic Evalua-tions (SemEval-2007), pages 70?74.
Association forComputational Linguistics, June.Yla R. Tausczik and James W. Pennebaker.
2010.
ThePsychological Meaning of Words: LIWC and Com-puterized Text Analysis Methods.
Journal of Lan-guage and Social Psychology, 29(1):24?54, March.Marc T. Tomlinson and Bradley C. Love.
2006.
Frompigeons to humans: grounding relational learning inconcrete examples.
In Proceedings of the 21st na-tional conference on Artificial intelligence - Volume1, AAAI?06, pages 199?204.
AAAI Press.Peter D. Turney.
2002.
Thumbs up or thumbs down?
:semantic orientation applied to unsupervised classi-fication of reviews.
In Proceedings of the 40th An-nual Meeting on Association for Computational Lin-guistics, ACL ?02, pages 417?424.690Tony Veale and Guofu Li.
2012.
Specifying viewpointand information need with affective metaphors: asystem demonstration of the metaphor magnet webapp/service.
In Proceedings of the ACL 2012 SystemDemonstrations, ACL ?12, pages 7?12.Tony Veale.
2012.
A context-sensitive, multi-facetedmodel of lexico-conceptual affect.
In The 50th An-nual Meeting of the Association for ComputationalLinguistics, Proceedings of the Conference, pages75?79.Janyce Wiebe and Claire Cardie.
2005.
Annotatingexpressions of opinions and emotions in language.language resources and evaluation.
In LanguageResources and Evaluation (formerly Computers andthe Humanities.Yorick Wilks.
2007.
A preferential, pattern-seeking,semantics for natural language inference.
In Wordsand Intelligence I, volume 35 of Text, Speechand Language Technology, pages 83?102.
SpringerNetherlands.Theresa Wilson, Janyce Wiebe, and Paul Hoffmann.2005.
Recognizing contextual polarity in phrase-level sentiment analysis.
In Proceedings of the con-ference on Human Language Technology and Em-pirical Methods in Natural Language Processing,HLT ?05, pages 347?354.Ian H. Witten and Eibe Frank.
2005.
Data Mining:Practical Machine Learning Tools and Techniques.Morgan Kaufmann, second edition.Ainur Yessenalina and Claire Cardie.
2011.
Com-positional matrix-space models for sentiment analy-sis.
In Proceedings of the Conference on EmpiricalMethods in Natural Language Processing, EMNLP?11, pages 172?182.691
