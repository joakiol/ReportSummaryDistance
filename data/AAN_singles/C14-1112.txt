Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers,pages 1186?1196, Dublin, Ireland, August 23-29 2014.Japanese Word Reordering Integrated with Dependency ParsingKazushi Yoshida1,a)Tomohiro Ohno2,b)Yoshihide Kato3,c)Shigeki Matsubara1,d)1Graduate School of Information Science, Nagoya University, Japan2Information Technology Center, Nagoya University, Japan3Information & Communications, Nagoya University, Japana)yoshida@db.ss.is.nagoya-u.ac.jpb)ohno@nagoya-u.jpc)yoshihide@icts.nagoya-u.ac.jpd)matubara@nagoya-u.jpAbstractAlthough Japanese has relatively free word order, Japanese word order is not completely arbitraryand has some sort of preference.
Since such preference is incompletely understood, even nativeJapanese writers often write Japanese sentences which are grammatically well-formed but noteasy to read.
This paper proposes a method for reordering words in a Japanese sentence sothat the sentence becomes more readable.
Our method can identify more suitable word orderthan conventional word reordering methods by concurrently performing dependency parsing andword reordering instead of sequentially performing the two processing steps.
As the result of anexperiment on word reordering using newspaper articles, we confirmed the effectiveness of ourmethod.1 IntroductionJapanese has relatively free word order, and thus Japanese sentences which make sense can be writtenwithout having a strong awareness of word order.
However, Japanese word order is not completelyarbitrary and has some sort of preference.
Since such preference is incompletely understood, even nativeJapanese writers often write Japanese sentences which are grammatically well-formed but not easy toread.
The word reordering of such sentences enables the readability to be improved.There have been proposed some methods for reordering words in a Japanese sentence so that thesentence becomes easier to read (Uchimoto et al., 2000; Yokobayashi et al., 2004).
In addition, thereexist a lot of researches for estimating appropriate word order in various languages (Filippova and Strube,2007; Harbusch et al., 2006; Kruijff et al., 2001; Ringger et al., 2004; Shaw and Hatzivassiloglou, 1999).Although most of these previous researches used syntactic information, the sentences they used therewere what had been previously parsed.
It is a problem that word reordering suffers the influence ofparsing errors.
Furthermore, as the related works, there are various researches on word reordering forimproving the performance of statistical machine translation (Goto et al., 2012; Elming, 2008; Ge, 2010;Christoph and Hermann, 2003; Nizar, 2007).
These researches consider information as to both a sourcelanguage and a target language to handle word order differences between them.
Therefore, their problemsetting is different from that for improving the readability of a single language.This paper proposes a method for reordering words in a Japanese sentence so that the sentence becomeseasier to read for revision support.
Our proposed method concurrently performs dependency parsingand word reordering for an input sentence of which the dependency structure is still unknown.
Ourmethod can identify more suitable word order than conventional word reordering methods because itcan concurrently consider the preference of both word order and dependency.
An experiment usingnewspaper articles showed the effectiveness of our method.2 Word Order and Dependency in Japanese SentencesThere have been a lot of researches on Japanese word order in linguistics (for example, Nihongo KijutsuBunpo Kenkyukai, 2009; Saeki, 1998), which have marshalled fundamental contributing factors whichThis work is licenced under a Creative Commons Attribution 4.0 International License.
Page numbers and proceedings footerare added by the organizers.
License details: http://creativecommons.org/licenses/by/4.0/1186naganen(for years)sekaiju-no(all over the world)hitobito-ga(people)torikun-de-ki-ta(have tackled)kare-ga(He)mondai-wo(theproblem)tsuini(finally)S1 (inappropriate word order)kaiketsu-shi-ta(resolved)?He finally resolved the problem that people all over the world have tackled for years.
?S2 (appropriate word order)naganen(for years)sekaiju-no(all over the world)hitobito-ga(people)torikun-de-ki-ta(have tackled)kare-ga(He)mondai-wo(theproblem)tsuini(finally) kaiketsu-shi-ta(resolved)Figure 1: Example of inappropriate/appropriate word orderdecide the appropriate word order in detail.
In a Japanese sentence, a predicate of the main clause isfundamentally placed in last position, and thus, case elements, adverbial elements, or subordinate clausesare located before it.
In addition, case elements are basically placed in the order of a nominative, a dativeand an accusative.
However, the basic order of case elements is often changed by being influenced fromgrammatical and discourse factors.
For example, it is pointed out that a long case element has strongpreference to be located at the beginning of a sentence even if the element is not nominative, as shownin Figure 1.In Figure 1, a box and an arrow express a bunsetsu1and a dependency relation respectively.
Both thesentences S1 and S2 have the same meaning which is translated as ?He finally resolved the problem thatpeople all over the world have tackled for years?
in English.
The difference between S1 and S2 is just intheir word orders in Japanese.The word order of S1 is more difficult to read than that of S2 because the distance between the bun-setsu ?kare-ga (He)?
and its modified bunsetsu ?kaiketsu-shi-ta (resolved)?
is large and thus the loads onworking memory become large.
This example suggests that if the dependency structure of S1 is iden-tified, that information is useful to reorder the word order of S1 to that of S2 so that it becomes easierto read.
In fact, most of the conventional word reordering methods have reordered words using the pre-viously parsed dependency structure.
However, the word order of S1 is thought to be more difficult toparse than that of S2 because dependency parsers are usually trained on syntactically annotated corporain which sentences have the appropriate word order such as that in S2.
This is why it is highly possiblethat dependency parsing can achieve a higher accuracy by changing the word order of S1 to that of S2 inadvance.The above observations indicate that word reordering and dependency parsing depend on each other.Therefore, we consider it is more desirable to concurrently perform the two processings than to sequen-tially perform them.3 Word Reordering MethodIn our method, a sentence, on which morphological analysis and bunsetsu segmentation have been per-formed, is considered as the input2.
We assume that the input sentence might have unsuitable word order,1Bunsetsu is a linguistic unit in Japanese that roughly corresponds to a basic phrase in English.
A bunsetsu consists of oneindependent word and zero or more ancillary words.
A dependency relation in Japanese is a modification relation in which amodifier bunsetsu depends on a modified bunsetsu.
That is, the modifier bunsetsu and the modified bunsetsu work as modifierand modifyee, respectively.2In order to focus attention on the comparison between our method and the conventional method, we assumed the input onwhich the lower layer processings than dependency parsing have been performed.
Even if morphological analysis and bunsetsusegmentation are automatically performed on input sentences which have unsuitable word order, we can expect the accuracies1187which is not easy to read but grammatically well-formed.
Our method identifies the suitable word orderwhich is easy to read by concurrently performing dependency parsing.The simultaneous performing of dependency parsing and word reordering is realized by searching forthe maximum-likelihood pattern of word order and dependency structure for the input sentence.
Note ourmethod reorders bunsetsus in a sentence without paraphrasing and does not reorder morphemes within abunsetsu.3.1 Probabilistic Model for Word ReorderingWhen a sequence of bunsetsus in an input sentence B = b1?
?
?bnis provided, our method identifies thestructure S which maximizes P (S|B).
The structure S is defined as a tuple S = ?O,D?
where O ={o1,2, o1,3, ?
?
?
, o1,n, ?
?
?
, oi,j, ?
?
?
, on?2,n?1, on?2,n, on?1,n} is the word order pattern after reorderingand D = {d1, ?
?
?
, dn?1} is dependency structure.
Here, oi,j(1 ?
i < j ?
n) expresses the orderbetween biand bjafter reordering.
oi,jis 1 if biis located before bj, and is 0 otherwise.
In addition, diexpresses the dependency relation whose modifier bunsetsu is bi.P (S|B) for a S = ?O,D?
is calculated as follows.P (S|B) = P (O,D|B)=?P (O|B) ?
P (D|O,B) ?
P (D|B) ?
P (O|D,B) (1)Formula (1) is obtained for the product of the following two formulas.
According to the probabilitytheory, the calculated result of Formula (1) is equal to those of Formulas (2) and (3).
However, in practice,since each factor in the formulas is estimated based on the corpus used for training, the calculated resultsof these formulas are different from each other.
We use Formula (1) to estimate P (S|B) by using bothvalues of P (D|O,B) and P (O|D,B).
In fact, we pre-experimentally confirmed that the calculatedresult of Formula (1) was better than those of the others.P (O,D|B) = P (O|B) ?
P (D|O,B) (2)P (O,D|B) = P (D|B) ?
P (O|D,B) (3)Assuming that order oi,jbetween two bunsetsus is independent of that between other two bunsetsusand that each dependency relation diis independent of the others, each factor in Formula (1) can beapproximated as follows:P (O|B)?=n?1?i=1n?j=i+1P (oi,j|B) (4)P (D|O,B)?=n?1?i=1P (di|O,B) (5)P (D|B)?=n?1?i=1P (di|B) (6)P (O|D,B)?=n?1?i=1n?j=i+1P (oi,j|D,B) (7)where P (oi,j|B) is the probability that the order between biand bjis oi,jwhenB is provided, P (di|O,B)is the probability that the dependency relation whose modifier bunsetsu is biis diwhen the sentencegenerated by reordering B according to O is provided, P (di|B) is the probability that the dependencyrelation whose modifier bunsetsu is biis diwhen B is provided, and P (oi,j|D,B) is the probabilitythat the order between biand bjis oi,jwhen B where the dependency relation is D is provided.
Theseprobabilities are estimated by the maximum entropy method.remain comparatively high.
This is because their processings use mainly local information.1188To estimate P (di|O,B), we used the features used in Uchimoto et al.
(1999) except when eliminatingfeatures about Japanese commas (called toten, which is a kind of punctuation) and quotation marks.To estimate P (di|B), we used the features which can be obtained without information about the orderof input bunsetsus among the features used in estimating P (di|O,B).
To estimate P (oi,j|D,B), if biand bjmodifies the same bunsetsu, we used the features used in Uchimoto et al.
(2000), except wheneliminating features about parallel relations and semantic features.
Otherwise, we used the features leftafter eliminating features about modified bunsetsus from those used in the above-mentioned case.
Toestimate P (oi,j|B), we used the features which can be obtained without dependency information amongthe features used to estimate P (Oi,j|D,B).3.2 Search AlgorithmSince there are a huge number of the structures S = ?O,D?
which are theoretically possible for an inputsentence B, an efficient algorithm is desired.
However, since O and D are dependent on each other,it is difficult to find the optimal structure efficiently.
In our research, we extend CYK algorithm usedin conventional dependency parsing to efficiently find the suboptimal S = ?O,D?
which maximizesP (S|B) efficiently.Our research assumes that an input sentence, which is grammatically well-formed, is reordered withoutchanging the meaning so that the sentence becomes much easier to read.
From this assumption, we canuse following conditions for efficient search:1.
The dependency structure of an input sentence should satisfy the following Japanese syntactic con-straints under the input word order:?
No dependency is directed from right to left.?
Dependencies don?t cross each other.?
Each bunsetsu, except the last one, depends on only one bunsetsu.2.
Even after the words are reordered, the dependency structure should satisfy the above-mentionedJapanese syntactic constraints under the changed word order.3.
The dependency structures of a sentence before and after reordering should be identical.Using the condition 1 and the condition 3, we can narrow down the search space of D to dependencystructures that satisfy Japanese syntactic constraints under the input word order.
Furthermore, the searchspace of O can be narrowed down to the word order patterns derived from the above narrowed depen-dency structures based on the conditions 2 and 3.
That is, after dependency structures possible for aninput sentence are narrowed down, we just have to find the word order patterns after reordering so thateach of the dependency structures is maintained and satisfies the Japanese syntactic constraints evenunder the changed word order.On the other hand, it is well known that CYK algorithm can efficiently find the optimal dependencystructure which satisfies Japanese syntactic constraints.
Therefore, in our research, we have extended theCYK algorithm for the conventional dependency parsing so that it can find the suboptimal D and O fromamong the dependency structures and word order patterns which satisfy the conditions 1, 2 and 3.3.2.1 Word Reordering AlgorithmAlgorithm 1 shows our word reordering algorithm.
In our algorithm, the n?n triangular matrixMi,j(1 ?i ?
j ?
n) such as the left-side figure in Figure 2 is prepared for an input sentence consisting of nnumbers of bunsetsus.
Mi,j, the element of the triangular matrix M in the i-th row and j-th column, isfilled by argmaxSi,jP (Si,j|Bi,j), which is the maximum-likelihood structure for an input subsequenceBi,j= bi?
?
?
bj.
In this section, for convenience of explanation, we represent Si,jas a sequence ofdependency relations dx(i ?
x ?
j).
For example, Si,j= didi+1?
?
?
d0jmeans that the first bunsetsuis bi, the second is bi+1, ?
?
?
, the last is bj, and the dependency structure is {di, di+1, ?
?
?
, dj?1}.
Here,if we need to clearly specify the modified bunsetsu, we represent the dependency relation that bunsetsu1189Algorithm 1 word reordering algorithm1: input B1,n= b1?
?
?
bn// input sentence2: set Mi,j(1 ?
i ?
j ?
n) // triangular matrix3: set Ci,j(1 ?
i ?
j ?
n) // set of structure candidates4: for i = 1 to n do5: Mi,i= d0i6: end for7: for d = 1 to n?
1 do8: for i = 1 to n?
d do9: j = i+ d10: for k = i to j ?
1 do11: Ci,j= Ci,j?
ConcatReorder(Mi,k,Mk+1,j)12: end for13: Mi,j= argmaxSi,j?Ci,jP (Si,j|Bi,j)14: end for15: end for16: return M1,nCandidates generated by?By the concatenating process?By the reordering processM1,1= M1,2= M1,3= M1,4M2,2= M2,3= M2,4M3,3= M3,4=M4,4=2 2 33 44Candidates generated by?By the concatenating process2 3 423 4?
By the reordering process2 3 4is filled by the structure which maximizes among the following candidates.Candidate 1:Candidate 3:Candidate 2:?
means that is located  before and depends on .
For example, i j?is filled by the maximum-likelihoodstructure for a subsequence from to .31 1 2 2 31No candidate is generated because has no child in .2 31 means .by moving after , which is the first child of inFigure 2: Execution example of our search algorithmbxmodifies byas dyx.
In addition, d0jmeans that the last bunsetsu of the subsequence don?t modify anybunsetsu.First, the statements of the lines 4 to 6 fill each of diagonal elements Mi,i(1 ?
i ?
n) with d0i.
Next,the statements of the lines 7 to 15 fill Mi,jin turn toward the upper right M1,nalong the diagonal line,starting from the diagonal elements Mi,i.
The maximum-likelihood structure which should fill an Mi,jis found as follows:The statements of the lines 10 to 12 repeat the process of generating candidates of the maximum-likelihood structure from Mi,kand Mk+1,jby the function ConcatReorder, and adding them to the setof structure candidates Ci,j.
The function ConcatReorder takes two arguments ofMi,kandMk+1,jandreturns the set of candidates of the maximum-likelihood structure which should fill Mi,j.
The functionConcatReorder is composed of two processes: concatenating process and reordering process.
First,the concatenating process generates a candidate by simply concatenating Mi,kand Mk+1,jin turn aboutthe word order and connecting Mi,kand Mk+1,jby the dependency relation between the last bunsetsusof them about the dependency structure, without changing the internal structure of each of them.
Forexample, whenMi,k= didi+1?
?
?
dk?1d0kandMk+1,j= dk+1dk+2?
?
?
dj?1d0jare given as the argument,the concatenating process generates ?didi+1?
?
?
dk?1djkdk+1dk+2?
?
?
dj?1d0j.
?1190Second, the reordering process generates candidates by reordering words in the candidate generatedby the concatenating process.
The reordering is executed on the following conditions.
The first conditionis that the dependency structure is maintained and satisfies the Japanese syntactic constraints even underthe changed word order.
The second condition is that the order of any two words within each of Mi,kand Mk+1,jis maintained.
Concretely, the first reordered candidate is generated by moving Mi,kafterthe first (leftmost) child3of the last bunsetsu of Mk+1,jamong the children in Mk+1,j.
Then, the sec-ond reordered candidate is generated by moving Mi,kafter the second child.
The reordering process iscontinued until the last reordered candidate is generated by moving Mi,kafter the last child.
That is, thenumber of candidates generated by the reordering process is equal to the number of children of the lastbunsetsu in Mk+1,j.
For example, when Mi,k= didi+1?
?
?
dk?1d0kand Mk+1,j= djk+1djk+2?
?
?
djj?1d0j,which means all bunsetsus except the last one depend on the last one, are given, the reorderingprocess generates the following j ?
k?
1 candidates: ?djk+1didi+1?
?
?
dk?1djkdjk+2djk+3?
?
?
djj?1d0j,??djk+1djk+2didi+1?
?
?
dk?1djkdjk+3djk+4?
?
?
djj?1d0j,?
.
.
., and ?djk+1djk+2?
?
?
djj?1didi+1?
?
?
dk?1djkd0j.
?Therefore, in this case, the function ConcatReorder finally returns the set of candidates of which sizeis j?k, which includes the candidates generated by the reordering process and a candidate generated bythe concatenating process.
Next, in the line 13, our algorithm fills in argmaxSi,j?Ci,jP (Si,j|Bi,j) whichis the maximum-likelihood structure for a subsequence Bi,jon Mi,j.Finally, our algorithm outputs M1,nas the maximum-likelihood structure of word order and depen-dency structure for the input sentence.Note that if the function ConcatReorder is changed to the function Concat in the line 11, our algorithmbecomes the same as CYK algorithm used in the conventional dependency parsing.
The functionConcattakes two arguments of Mi,kand Mk+1,jand generates a candidate of the maximum-likelihood structurewhich should fill Mi,jby the same way as the concatenating process in the function ConcatReorder.Then, the function Concat returns the set which has the generated candidate as a element, of which sizeis 1.3.2.2 Execution Example of Word Reordering AlgorithmFigure 2 represents an example of execution of our word reordering algorithm in n = 4.
The left sideof Figure 2 represents the triangle diagram which has 4 ?
4 dimensions.
The elements of the trianglediagram M1,1,M2,2,M3,3,M4,4,M1,2,M2,3,M3,4, and M1,3have already been filled in turn, and M2,4is being filled.
The right side of Figure 2 shows the process of calculating the maximum-likelihoodstructure which should fill M2,4.
First, in the loop from the line 10 to the line 12 in Algorithm 1,two structure candidates are generated by ConcatReorder(M2,2,M3,4).
The candidate 1 is generatedby the concatenating process, that is, by simply concatenating M2,2and M3,4and connecting the lastbunsetsu of M2,2and that of M3,4.
The candidate 2 is generated by the reordering process, that is, bymoving M2,2after b3, which is the first child of b4in M3,4.
Second, the candidate 3 is generated bythe concatenating process in ConcatReorder(M2,3,M4,4).
On the other hand, the reordering process inConcatReorder(M2,3,M4,4) generates no candidates because b4has no child inM4,4.
Among the threestructures generated in the above way, the structure which maximizes P (S2,4|B) = P (O2,4, D2,4|B2,4)fills M2,4.4 ExperimentTo evaluate the effectiveness of our method, we conducted an experiment on word reordering by usingJapanese newspaper articles.4.1 Outline of ExperimentIn the experiment, as the test data, we used sentences generated by only changing the word order ofnewspaper article sentences in Kyoto Text Corpus (Kurohashi and Nagao, 1998), maintaining the depen-dency structure.
That is, we artificially generated sentences which made sense but were not easy to read,3When bidepends on bj, we call bias a child of bj.
Furthermore, if bjhas more than or equal to one child, the children arenumbered from left to right based on their positions.1191kokkai-wo(the Diet)toot-ta(passed)ato-demo(Even after)seron-chosa-de-wa(according to opinion polls)shohi-ze-zoze-ga(the consumption tax hike bill)zoze-hantai-ga(opposing views to the bill)Original sentence in newspaper articles (correct word order)taise-da(are in majority)?Even after the Diet passed the consumption tax hike bill,according to opinion polls opposing views to the bill are in majority.
?Test data (input sentence)test data generationkokkai-wo(the Diet)toot-ta(passed)ato-demo(Even after)seron-chosa-de-wa(according to opinion polls)shohi-ze-zoze-ga(the consumption tax hike bill)zoze-hantai-ga(opposing views to the bill)taise-da(are in majority)Figure 3: Example of test data generationin order to focus solely on problems caused by unsuitable word order.
Figure 3 shows an example of thetest data generation.
The generation procedure is as follows:1.
Find a bunsetsu modified by multiple bunsetsus from the sentence end.2.
Change randomly the order of the sub-trees which modify such bunsetsu.3.
Iterate 1 and 2 until reaching the beginning of the sentence.In Figure 3, the bunsetsus ?taise-da (are in the majority)?
and ?toot-ta (passed)?
are found as bunsetsusmodified by multiple bunsetsus.
For example, when ?toot-ta (passed)?
is found, the order of ?shohi-ze-zoze-ga (the consumption tax hike bill)?
and ?kokkai-wo (the Diet)?
is randomly changed.
In thisexperiment, all Japanese commas (toten) in a sentence, and sentences which have quotation marks wereremoved.In this way, we artificially generated 865 sentences (7,620 bunsetsus) from newspaper articles of Jan.9 in Kyoto Text Corpus and used them as the test data.
As the training data, we used 7,976 sentences in 7days?
newspaper articles (Jan. 1, 3-8).
Here, we used the maximum entropy method tool (Zhang, 2008)with the default options except ?-i 1000.?In the evaluation of word reordering, we obtained the following two measurements, which are definedby Uchimoto et al.
(2000):?
complete agreement: the percentage of the sentences in which all words?
order completely agreeswith that of the original sentence.?
pair agreement: the percentage of the pairs of bunsetsus whose word order agrees with that in theoriginal sentence.
(For example, in Figure 3, if the word order of the input sentence is not changedafter reordering, the pair agreement is 52.4% (= 11/7C2) because the 11 pairs out of the7C2pairsare the same as those in the original sentence.
)In the evaluation of dependency parsing, we obtained the dependency accuracy (the percentage ofcorrectly analyzed dependencies out of all dependencies) and sentence accuracy (the percentage ofthe sentences in which all the dependencies are analyzed correctly), which are defined by Sekine et al.
(2000).For comparison, we established two baselines.
Both of the baselines execute the dependency pars-ing primarily, and then, perform the word reordering by using the conventional word reordering method1192Table 1: Experimental results (word reordering)pair agreement complete agreementour method 77.3% (30,190/38,838) 25.7% (222/865)baseline 1 75.4% (29,279/38,838)*23.8% (206/865)baseline 2 74.8% (29,067/38,838)*23.5% (203/865)no reordering 61.5% (23,886/38,838)*8.0% (69/865)*Note that the agreements followed by * differ signifi-cantly from those of our method (p < 0.05).Table 2: Experimental results (dependency parsing)dependency accuracy sentence accuracyour method 78.4% (5,293/6,755) 35.3% (305/865)baseline 1 79.2% (5,350/6,755) 31.6% (273/865)*baseline 2 81.2% (5,487/6,755)*32.1% (278/865)*Note that the accuracies followed by * differ sig-nificantly from those of our method (p < 0.05).
(Uchimoto et al., 1999).
The difference between the two is the method of dependency parsing.
Thebaselines 1 and 2 use the dependency parsing method proposed by Uchimoto et al.
(2000) and the de-pendency parsing tool CaboCha (Kudo and Matsumoto, 2002), respectively.
The features used for theword reordering in both the baselines are the same as those used to estimate P (oi,j|D,B) in our method.Additionally, the features used for the dependency parsing in the baseline 1 are the same as those used toestimate P (di|O,B) in our method.4.2 Experimental ResultsTable 1 shows the experimental results on word reordering of our method and the baselines.
Here, thelast row shows the agreements measured by comparing the input word order with the correct word order.The agreements mean the values which can be achieved with no reordering4.
The pair and completeagreements of our method were highest among all.
The pair agreement of our method is significantlydifferent from those of both the baselines (p < 0.05) although there is no significant difference betweenthe complete agreements of them.Next, Table 2 shows the experimental results on dependency parsing.
The sentence accuracy of ourmethod is significantly higher than those of both the baselines (p < 0.05).
On the other hand, thedependency accuracy of our method is significantly lower than that of the baseline 2 although there is nosignificant difference between the dependency accuracies of our method and the baseline 1 (p > 0.05).Here, if the input sentences had the correct word order, the dependency accuracies of the baselines 1 and2 were 86.4% (5,835/6,755) and 88.1% (5,950/6,755), respectively.
We can see that the unsuitable wordorder caused a large decrease of the accuracies of the conventional dependency parsing methods.
This iswhy the word order agreements of the baselines were decreased.Figure 4 shows an example of sentences of which all bunsetsus were correctly reordered and the de-pendency structure was correctly parsed only by our method.
We can see that our method can achievethe complicated word reordering.
On the other hand, Figure 5 shows an example of sentences incorrectlyreordered and parsed by our method.
In this example, our method could not identify the correct modifiedbunsetsu and the appropriate position of the bunsetsu ?arikata-wo (whole concept).?
This is because thedependency probability between the bunsetsu ?arikata-wo (whole concept)?
and the bunsetsu ?fukume4Some input sentences were in complete agreement with the original ordering.
There were some cases that the randomlyreordered sentences accidentally have the same word order as the original ones.
In addition, there were some sentences inwhich all bunsetsus except the last one depend on the next bunsetsu.
The word order of such sentences is not changed by thetest data generation procedure because the procedure is executed on condition of maintaining the dependency structure.1193Input sentence(inappropriate word order)?Although I myself do not have an experience with a war, I think any generation should not glorify war.
?Output sentence(correct word order and dependency structure) itsu-no(any) sedai-mo(generation) bika-su-beki-de-nai-to(should not glorify)senso-wo(with a war)senso-wo(war)taiken-shi-ta(have an  experience)koto-wa(?
)watashi-jishin(I myself)omou(think)itsu-no(any)sedai-mo(generation)bika-su-beki-de-nai-to(should not glorify)senso-wo(with a war)senso-wo(war)taiken-shi-ta(have an  experience)koto-wa(?
)watashi-jishin(I myself)nai-ga(although do not)omou(think)nai-ga(although do not):  shows an alignment of a bunsetsu before and after reordering.
:  shows a correct dependency relation.?
:  means there is no English word corresponding to the Japanese word.Figure 4: Example of sentences correctly reordered and parsed by our method?Whole concept of the examination of rice should be fundamentally revised including the transfer of control to a private sector or prefectural and city governments.
?Input sentence(inappropriate word order)Output sentence(incorrect word order and dependency structure)kensa-no(of the exami-nation)arikata-wo(whole concept)todofuken-ya(or prefectural and city governments)minkan-e-no(to a private sector)kome-no(of rice)ikan-mo(the transferof control)fukume(including)konpon-teki-ni(fundamen-tally)minaosu-beki-daro(should be revised)Original sentence(correct word order and dependency structure)kensa-no(of the exami-nation)arikata-wo(whole concept)todofuken-ya(or prefectural and city governments)minkan-e-no(to a private sector)kome-no(of rice)ikan-mo(the transferof control)fukume(including)konpon-teki-ni(fundamen-tally)minaosu-beki-daro(should be revised)kensa-no(of the exami-nation)arikata-wo(whole concept)todofuken-ya(or prefectural and city governments)minkan-e-no(to a private sector)kome-no(of rice)ikan-mo(the transferof control)fukume(including)konpon-teki-ni(fundamen-tally)minaosu-beki-daro(should be revised): shows an alignment of a bunsetsu before and after reordering.
:  shows a correct dependency relation.
:  shows an incorrect dependency relation.Figure 5: Example of sentences incorrectly reordered and parsed by our method(including)?
is higher than the one between the bunsetsu ?arikata-wo (whole concept)?
and the bunsetsu?minaosu-beki-daro (should be revised)?, and the probability that the bunsetsu ?arikata-wo (whole con-cept)?
is located at the left side of ?fukume (including)?
is higher than that of the right side.
Since theword order of the output sentence has a strong probability of causing a wrong interpretation like ?Thetransfer of control to a private sector or prefectural and city governments should be fundamentally re-vised including whole concept of the examination of rice.
?, this reordering has a harmful influence onthe comprehension.
We need to study techniques for avoiding the word order which causes the changeof meanings in an input sentence.From the above, we confirmed the effectiveness of our method on word reordering and dependencyparsing of a sentence of which the word order is not easy to read.5 ConclusionThis paper proposed the method for reordering bunsetsus in a Japanese sentence.
Our method can identifysuitable word order by concurrently performing word reordering and dependency parsing.
Based on the1194idea of limiting the search space using the Japanese syntactic constraints, we made the search algorithmby extending the CYK algorithm used in the conventional dependency parsing, and found the optimalstructure efficiently.
The result of the experiment using newspaper articles showed the effectiveness ofour method.In our future works, we would like to collect sentences written by Japanese subjects who do not havemuch writing skills, to conduct an experiment using those sentences.
In addition, we would like toconduct a subjective evaluation to investigate whether the output sentences are indeed more readablethan the input ones.AcknowledgmentsThis research was partially supported by the Grant-in-Aid for Young Scientists (B) (No.25730134) andChallenging Exploratory Research (No.24650066) of JSPS.ReferencesTillmann Christoph and Ney Hermann.
2003.
Word reordering and a dynamic programming beam searchalgorithm for statistical machine translation.
Computational Linguistics, 29(1):97?133.Jakob Elming.
2008.
Syntactic reordering integrated with phrase-based SMT.
In Proceedings of the22nd International Conference on Computational Linguistics (COLING2008), pages 209?216.Katja Filippova and Michael Strube.
2007.
Generating constituent order in German clauses.
In Proceed-ings of the 45th Annual Meeting of the Association for Computational Linguistics (ACL2007), pages320?327.Niyu Ge.
2010.
A direct syntax-driven reordering model for phrase-based machine translation.
InProceedings of Human Language Technologies: The 11th Annual Conference of the North AmericanChapter of the Association for Computational Linguistics (NAACL-HLT2010), pages 849?857.Geert-Jan M. Kruijff, Ivana Kruijff-Korbayov?a, John Bateman, and Elke Teich.
2001.
Linear order ashigher-level decision: Information structure in strategic and tactical generation.
In Proceedings of the8th European Workshop on Natural Language Generation (ENLG2001), pages 74?83.Isao Goto, Masao Utiyama, and Eiichiro Sumita.
2012.
Post-ordering by parsing for Japanese-Englishstatistical machine translation.
In Proceedings of the 50th Annual Meeting of the Association forComputational Linguistics (ACL2012), pages 311?316.Karin Harbusch, Gerard Kempen, Camiel van Breugel, and Ulrich Koch.
2006.
A generation-orientedworkbench for performance grammar: Capturing linear order variability in German and Dutch.
InProceedings of the 4th International Natural Language Generation Conference (INLG2006), pages9?11.Nihongo Kijutsu Bunpo Kenkyukai, editor.
2009.
Gendai nihongo bunpo 7 (Contemporary JapaneseGrammar 7), pages 165?182.
Kuroshio Shuppan.
(In Japanese).Taku Kudo and Yuji Matsumoto.
2002.
Japanese dependency analysis using cascaded chunking.
In Pro-ceedings of the 6th Conference on Computational Natural Language Learning (CoNLL2002), pages63?69.Sadao Kurohashi and Makoto Nagao.
1998.
Building a Japanese parsed corpus while improving theparsing system.
In Proceedings of the 1st International Conference on Language Resources andEvaluation (LREC ?98), pages 719?724.Habash Nizar.
2007.
Syntactic preprocessing for statistical machine translation.
In Proceedings of the11th Machine Translation Summit (MT SUMMIT XI), pages 215?222.Eric Ringger, Michael Gamon, Robert C. Moore, David Rojas, Martine Smets, and Simon Corston-Oliver.
2004.
Linguistically informed statistical models of constituent structure for ordering in sen-tence realization.
In Proceedings of the 20th International Conference on Computational Linguis-tics (COLING2004), pages 673?679.1195Tetsuo Saeki.
1998.
Yosetsu nihonbun no gojun (Survey: Word Order in Japanese Sentences).
KuroshioShuppan.
(In Japanese).Satoshi Sekine, Kiyotaka Uchimoto, and Hitoshi Isahara.
2000.
Backward beam search algorithm for de-pendency analysis of Japanese.
In Proceedings of the 18th International Conference on ComputationalLinguistics (COLING2000), volume 2, pages 754?760.James Shaw and Vasileios Hatzivassiloglou.
1999.
Ordering among premodifiers.
In Proceedings of the37th Annual Meeting of the Association for Computational Linguistics (ACL ?99), pages 135?143.Kiyotaka Uchimoto, Satoshi Sekine, and Hitoshi Isahara.
1999.
Japanese dependency structure analysisbased on maximum entropy models.
In Proceedings of the 9th Conference of the European Chapterof the Association for Computational Linguistics (EACL ?99), pages 196?203.Kiyotaka Uchimoto, Masaki Murata, Qing Ma, Satoshi Sekine, and Hitoshi Isahara.
2000.
Word orderacquisition from corpora.
In Proceedings of the 18th International Conference on ComputationalLinguistics (COLING2000), volume 2, pages 871?877.Hiroshi Yokobayashi, Akira Suganuma, and Rin-ichiro Taniguchi.
2004.
Generating candidates forrewriting based on an indicator of complex dependency and it?s application to a writing tool.
Journalof Information Processing Society of Japan, 45(5):1451?1459.
(In Japanese).Le Zhang.
2008.
Maximum entropy modeling toolkit for Python and C++.
http://homepages.inf.ed.ac.uk/s0450736/maxent_toolkit.html.
[Online; accessed 1-March-2008].1196
