Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 758?768,Seattle, Washington, USA, 18-21 October 2013. c?2013 Association for Computational LinguisticsOptimal Incremental Parsing via Best-First Dynamic Programming?Kai Zhao1 James Cross11Graduate CenterCity University of New York365 Fifth Avenue, New York, NY 10016{kzhao,jcross}@gc.cuny.eduLiang Huang1,22Queens CollegeCity University of New York6530 Kissena Blvd, Queens, NY 11367huang@cs.qc.cuny.eduAbstractWe present the first provably optimal polyno-mial time dynamic programming (DP) algo-rithm for best-first shift-reduce parsing, whichapplies the DP idea of Huang and Sagae(2010) to the best-first parser of Sagae andLavie (2006) in a non-trivial way, reducingthe complexity of the latter from exponentialto polynomial.
We prove the correctness ofour algorithm rigorously.
Experiments con-firm that DP leads to a significant speedupon a probablistic best-first shift-reduce parser,and makes exact search under such a modeltractable for the first time.1 IntroductionBest-first parsing, such as A* parsing, makes con-stituent parsing efficient, especially for bottom-upCKY style parsing (Caraballo and Charniak, 1998;Klein and Manning, 2003; Pauls and Klein, 2009).Traditional CKY parsing performs cubic time exactsearch over an exponentially large space.
Best-firstparsing significantly speeds up by always preferringto explore states with higher probabilities.In terms of incremental parsing, Sagae and Lavie(2006) is the first work to extend best-first search toshift-reduce constituent parsing.
Unlike other veryfast greedy parsers that produce suboptimal results,this best-first parser still guarantees optimality butrequires exponential time for very long sentencesin the worst case, which is intractable in practice.Because it needs to explore an exponentially largespace in the worst case, a bounded priority queuebecomes necessary to ensure limited parsing time.
?This work is mainly supported by DARPA FA8750-13-2-0041 (DEFT), a Google Faculty Research Award, and a PSC-CUNY Award.
In addition, we thank Kenji Sagae and theanonymous reviewers for their constructive comments.On the other hand, Huang and Sagae (2010) ex-plore the idea of dynamic programming, which isoriginated in bottom-up constituent parsing algo-rithms like Earley (1970), but in a beam-based nonbest-first parser.
In each beam step, they enablestate merging in a style similar to the dynamic pro-gramming in bottom-up constituent parsing, basedon an equivalence relation defined upon feature val-ues.
Although in theory they successfully reducedthe underlying deductive system to polynomial timecomplexity, their merging method is limited in thatthe state merging is only between two states in thesame beam step.
This significantly reduces the num-ber of possible merges, because: 1) there are onlya very limited number of states in the beam at thesame time; 2) a lot of states in the beam with differ-ent steps cannot be merged.We instead propose to combine the idea of dy-namic programming with the best-first search frame-work, and apply it in shift-reduce dependency pars-ing.
We merge states with the same features setglobally to further reduce the number of possiblestates in the search graph.
Thus, our DP best-first al-gorithm is significantly faster than non-DP best-firstparsing, and, more importantly, it has a polynomialtime complexity even in the worst case.We make the following contributions:?
theoretically, we formally prove that our DPbest-first parsing reaches optimality with poly-nomial time complexity.
This is the first timethat exact search under such a probabilisticmodel becomes tractable.?
more interestingly, we reveal that our dynamicprogramming over shift-reduce parsing is inparallel with the bottom-up parsers, except thatwe have an extra order constraint given by theshift action to enforce left to right generation of758input w0 .
.
.
wn?1axiom 0 : ?0, ?
: 0sh` : ?j, S?
: c`+ 1 : ?j + 1, S|wj?
: c+ scsh(j, S)j < nrex` : ?j, S|s1|s0?
: c`+ 1 : ?j, S|s1xs0?
: c+ screx(j, S|s1|s0)rey` : ?j, S|s1|s0?
: c`+ 1 : ?j, S|s1ys0?
: c+ screy(j, S|s1|s0)Figure 1: Deductive system of basic non-DP shift-reduceparsing.
Here ` is the step index (for beam search), S isthe stack, c is the score of the precedent, and sca(x) isthe score of action a from derivation x.
See Figure 2 forthe DP version.partial trees, which is analogous to Earley.?
practically, our DP best-first parser is only ?2times slower than a pure greedy parser, but isguaranteed to reach optimality.
In particular,it is ?20 times faster than a non-DP best-firstparser.
With inexact search of bounded prior-ity queue size, DP best-first search can reachoptimality with a significantly smaller priorityqueue size bound, compared to non-DP best-first parser.Our system is based on a MaxEnt model to meetthe requirement from best-first search.
We observethat this locally trained model is not as strong asglobal models like structured perceptron.
With thatbeing said, our algorithm shows its own merits inboth theory and practice.
To find a better model forbest-first search would be an interesting topic for fu-ture work.2 Shift-Reduce and Best-First ParsingIn this section we review the basics of shift-reduceparsing, beam search, and the best-first shift-reduceparsing algorithm of Sagae and Lavie (2006).2.1 Shift-Reduce Parsing and Beam SearchDue to space constraints we will assume some ba-sic familiarity with shift-reduce parsing; see Nivre(2008) for details.
Basically, shift-reduce parsing(Aho and Ullman, 1972) performs a left-to-rightscan of the input sentence, and at each step, chooseseither to shift the next word onto the stack, or to re-duce, i.e., combine the top two trees on stack, ei-ther with left as the root or right as the root.
Thisscheme is often called ?arc-standard?
in the litera-ture (Nivre, 2008), and is the basis of several state-of-the-art parsers, e.g.
Huang and Sagae (2010).
SeeFigure 1 for the deductive system of shift-reduce de-pendency parsing.To improve on strictly greedy search, shift-reduceparsing is often enhanced with beam search (Zhangand Clark, 2008), where b derivations develop inparallel.
At each step we extend the derivations inthe current beam by applying each of the three ac-tions, and then choose the best b resulting deriva-tions for the next step.2.2 Best-First Shift-Reduce ParsingSagae and Lavie (2006) present the parsing prob-lem as a search problem over a DAG, in which eachparser derivation is denoted as a node, and an edgefrom node x to node y exists if and only if the corre-sponding derivation y can be generated from deriva-tion x by applying one action.The best-first parsing algorithm is an applica-tion of the Dijkstra algorithm over the DAG above,where the score of each derivation is the priority.Dijkstra algorithm requires the priority to satisfythe superiority property, which means a descendantderivation should never have a higher score than itsancestors.
This requirement can be easily satisfied ifwe use a generative scoring model like PCFG.
How-ever, in practice we use a MaxEnt model.
And weuse the negative log probability as the score to sat-isfy the superiority:x ?
y ?
x.score < y.score,where the order x ?
y means derivation x has ahigher priority than y.1The vanilla best-first parsing algorithm inher-its the optimality directly from Dijkstra algorithm.However, it explores exponentially many derivationsto reach the goal configuration in the worst case.We propose a new method that has polynomial timecomplexity even in the worst case.1For simplicity we ignore the case when two derivationshave the same score.
In practice we can choose either one ofthe two derivations when they have the same score.7593 Dynamic Programming for Best-FirstShift-Reduce Parsing3.1 Dynamic Programming NotationsThe key innovation of this paper is to extend best-first parsing with the ?state-merging?
method of dy-namic programming described in Huang and Sagae(2010).
We start with describing a parsing configu-ration as a non-DP derivation:?i, j, ...s2s1s0?,where ...s2s1s0 is the stack of partial trees, [i..j] isthe span of the top tree s0, and s1s2... are the re-mainder of the trees on the stack.The notation fk(sk) is used to indicate the featuresused by the parser from the tree sk on the stack.
Notethat the parser only extracts features from the topd+1 trees on the stack.Following Huang and Sagae (2010), f?
(x) of aderivation x is called atomic features, defined as thesmallest set of features s.t.f?
(i, j, ...s2s1s0) = f?
(i, j, ...s?2s?1s?0)?
fk(sk) = fk(s?k), ?k ?
[0, d].The atomic feature function f?(?)
defines an equiv-alence relation ?
in the space of derivations D:?i, j, ...s2s1s0?
?
?i, j, ...s?2s?1s?0??
f?
(i, j, ...s2s1s0) = f?
(i, j, ...s?2s?1s?0)This implies that any derivations with the sameatomic features are in the same equivalence class,and their behaviors are similar in shift and reduce.We call each equivalence class a DP state.
Moreformally we define the space of all states S as:S?= D/?.Since only the top d+1 trees on the stack are usedin atomic features, we only need to remember thenecessary information and write the state as:?i, j, sd...s0?.We denote a derivation x?s state as [x]?.
In the restof this paper, we always denote derivations with let-ters x, y, and z, and denote states with letters p, q,and r.The deductive system for dynamic programmingbest-first parsing is adapted from Huang and Sagae(2010).
(See the left of Figure 2.)
The difference isthat we do not distinguish the step index of a state.This deductive system describes transitions be-tween states.
However, in practice we use one state?sbest derivation found so far to represent the state.For each state p, we calculate the prefix score, p.pre,which is the score of the derivation to reach thisstate, and the inside score, p.ins , which is the scoreof p?s top tree p.s0.
In addition we denote the shiftscore of state p as p.sh?= scsh(p), and the reducescore of state p as p.re?= scre(p).
Similarly wehave the prefix score, inside score, shift score, andreduce score for a derivation.With this deductive system we extend the conceptof reducible states with the following definitions:The set of all states with which a state p canlegally reduce from the right is denoted L(p), or leftstates.
(see Figure 3 (a)) We call any state q ?
L(p)a left state of p. Thus each element of this set wouldhave the following form:L(?i, j, sd...s0?
)?={?h, i, s?d...s?0?
|fk(s?k?1)=fk(sk), ?k ?
[1, d]} (1)in which the span of the ?left?
state?s top tree endswhere that of the ?right?
state?s top tree begins, andfk(sk) = fk(s?k?1) for all k ?
[1, d].Similarly, the set of all states with which a state pcan legally reduce from the left is denoted R(p), orright states.
(see Figure 3 (a)) For two states p, q,p ?
L(q)?
q ?
R(p)3.2 Algorithm 1We constrain the searching time with a polynomialbound by transforming the original search graphwith exponentially many derivations into a graphwith polynomial number of states.In Algorithm 1, we maintain a chart C and a prior-ity queue Q , both of which are based on hash tables.Chart C can be formally defined as a functionmapping from the space of states to the space ofderivations:C : S ?
D.In practice, we use the atomic features f?
(p) as thesignature of state p, since all derivations in the samestate share the same atomic features.760shstate p:?
, j, sd...s0?
: (c, )?j, j + 1, sd?1...s0, wj?
: (c+ ?, 0)j < n PRED?
, j, A?
?.B??
: (c, )?j, j, B ?
.??
: (c+s, s)(B ?
?)
?
Grexstate q:?k, i, s?d...s?0?
: (c?, v?
)state p:?i, j, sd...s0?
: ( , v)?k, j, s?d...s?1, s?0xs0?
: (c?+v+?, v?+v+?
)q ?
L(p) COMP?k, i, A??.B??
: (c?, v?)
?i, j, B?
: ( , v)?k, j, A?
?B.??
: (c?+v, v?+v)Figure 2: Deductive systems for dynamic programming shift-reduce parsing (Huang and Sagae, 2010) (left, omittingrey case), compared to weighted Earley parsing (Stolcke, 1995) (right).
Here ?
= scsh(p), ?
= scsh(q) + screx(p),s = sc(B ?
?
), G is the set of CFG rules, ?i, j, B?
is a surrogate for any ?i, j, B ?
?.
?, and is a wildcard thatmatches anything.. .
.L(p)sh sh.
.
.R(p)p. .
.L(p)sh.
.
.T (p)p(a) L(p) andR(p) (b) T (p) = R(L(p))Figure 3: Illustrations of left states L(p), right states R(p), and left corner states T (p).
(a) Left states L(p) is the setof states that can be reduced with p so that p.s0 will be the right child of the top tree of the result state.
Right statesR(p) is the set of states that can be reduced with p so that p.s0 will be the left child of the top tree of the result state.
(b) Left corner states T (p) is the set of states that have the same reducibility as shifted state p, i.e., ?p?
?
L(p), wehave ?q ?
T (p), q ?
R(p?).
In both (a) and (b), thick sh arrow means shifts from multiple states; thin sh arrow meansshift from a single state.We use C [p] to retrieve the derivation in C thatis associated with state p. We sometimes abuse thisnotation to say C [x] to retrieve the derivation asso-ciated with signature f?
(x) for derivation x.
This isfine since we know derivation x?s state immediatelyfrom the signature.
We say state p ?
C if f?
(p) isassociated with some derivation in C .
A derivationx ?
C if C [x] = x.
Chart C supports operationPUSH, denoted as C [x]?
x, which associate a sig-nature f?
(x) with derivation x.Priority queue Q is defined similarly as C , exceptthat it supports the operation POP that pops the high-est priority item.Following Stolcke (1995) and Nederhof (2003),we use the prefix score and the inside score as thepriority in Q :x ?
y ?
x.pre < y.pre or(x.pre = y.pre and x.ins < y.ins), (2)Note that, for simplicity, we again ignore the spe-cial case when two derivations have the same prefixscore and inside score.
In practice for this case wecan pick either one of them.
This will not affect thecorrectness of our optimality proof in Section 5.1.In the DP best-first parsing algorithm, once aderivation x is popped from the priority queue Q ,as usual we try to expand it with shift and reduce.Note that both left and right reduces are betweenthe derivation x of state p = [x]?
and an in-chartderivation y of left state q = [y]?
?
L(p) (Line 10of Algorithm 1), as shown in the deductive system(Figure 2).
We call this kind of reduction left expan-sion.We further expand derivation x of state p withsome in-chart derivation z of state r s.t.
p ?
L(r),i.e., r ?
R(p) as in Figure 3 (a).
(see Line 11 ofAlgorithm 1.)
Derivation z is in the chart because itis the descendant of some other derivation that hasbeen explored before x.
We call this kind of reduc-tion right expansion.Our reduction with L andR is inspired by Neder-hof (2003) and Knuth (1977) algorithm, which willbe discussed in Section 4.761Algorithm 1 Best-First DP Shift-Reduce Parsing.Let LC (x)?= C [L([x]?)]
be in-chart derivations of[x]?
?s left statesLet RC (x)?= C [R(p)] be in-chart derivations of[x]?
?s right states1: function PARSE(w0 .
.
.
wn?1)2: C ?
?
.
empty chart3: Q ?
{INIT} .
initial priority queue4: while Q 6= ?
do5: x?
POP(Q)6: if GOAL(x) then return x .
found best parse7: if [x]?
6?
C then8: C [x]?
x .
add x to chart9: SHIFT(x,Q)10: REDUCE(LC (x), {x},Q) .
left expansion11: REDUCE({x},RC (x),Q) .
right expansion12: procedure SHIFT(x,Q)13: TRYADD(sh(x),Q) .
shift14: procedure REDUCE(A,B,Q)15: for (x, y) ?
A?B do .
try all possible pairs16: TRYADD(rex(x, y),Q) .
left reduce17: TRYADD(rey(x, y),Q) .
right reduce18: function TRYADD(x, Q)19: if [x]?
6?
Q or x ?
Q[x] then20: Q[x]?
x .
insert x into Q or update Q[x]3.3 Algorithm 2: Lazy ExpansionWe further improve DP best-first parsing with lazyexpansion.In Algorithm 2 we only show the parts that aredifferent from Algorithm 1.Assume a shifted derivation x of state p is a directdescendant from derivation x?
of state p?, then p ?R(p?
), and we have:?ys.t .
[y]?
= q ?
REDUCE({p?},R(p?
)), x ?
ywhich is proved in Section 5.1.More formally, we can conclude that?ys.t .
[y]?
= q ?
REDUCE(L(p), T (p)), x ?
ywhere T (p) is the left corner states of shifted statep, defined asT (?i, i+1, sd...s0?
)?={?i, h, s?d...s?0?
|fk(s?k)=fk(sk), ?k ?
[1, d]}which represents the set of all states that have thesame reducibility as a shifted state p. In other words,T (p) = R(L(p)),Algorithm 2 Lazy Expansion of Algorithm 1.Let TC (x)?= C [T ([x]?)]
be in-chart derivations of[x]?
?s left-corner states1: function PARSE(w0 .
.
.
wn?1)2: C ?
?
.
empty chart3: Q ?
{INIT} .
initial priority queue4: while Q 6= ?
do5: x?
POP(Q)6: if GOAL(x) then return x .
found best parse7: if [x]?
6?
C then8: C [x]?
x .
add x to chart9: SHIFT(x,Q)10: REDUCE(x.lefts, {x},Q) .
left expansion11: else if x.action is sh then12: REDUCE(x.lefts, TC (x),Q) .
right expan.13: procedure SHIFT(x,Q)14: y ?
sh(x)15: y.lefts ?
{x} .
initialize lefts16: TRYADD(y,Q)17: function TRYADD(x, Q)18: if [x]?
?
Q then19: if x.action is sh then .
maintain lefts20: y ?
Q[x]21: if x ?
y then Q[x]?
x22: Q[x].lefts ?
y.lefts ?
x.lefts23: else if x ?
Q[x] then24: Q[x]?
x25: else .
x 6?
Q26: Q[x]?
xwhich is illustrated in Figure 3 (a).
Intuitively, T (p)is the set of states that have p?s top tree, p.s0, whichcontains only one node, as the left corner.Based on this observation, we can safely delay theREDUCE({x},RC (x)) operation (Line 11 in Algo-rithm 1), until the derivation x of a shifted state ispopped out from Q .
This helps us eliminate unnec-essary right expansion.We can delay even more derivations by extendingthe concept of left corner states to reduced states.Note that for any two states p, q, if q?s top tree q.s0has p?s top tree p.s0 as left corner, and p, q share thesame left states, then derivations of p should alwayshave higher priority than derivations of q.
We canfurther delay the generation of q?s derivations untilp?s derivations are popped out.22We did not implement this idea in experiments due to itscomplexity.7624 Comparison with Best-First CKY andBest-First Earley4.1 Best-First CKY and Knuth AlgorithmVanilla CKY parsing can be viewed as searchingover a hypergraph(Klein and Manning, 2005), wherea hyperedge points from two nodes x, y to one nodez, if x, y can form a new partial tree represented byz.
Best-first CKY performs best-first search overthe hypergraph, which is a special application of theKnuth Algorithm (Knuth, 1977).Non-DP best-first shift-reduce parsing can beviewed as searching over a graph.
In this graph, anode represents a derivation.
A node points to all itspossible descendants generated from shift and leftand right reduces.
This graph is actually a tree withexponentially many nodes.DP best-first parsing enables state merging onthe previous graph.
Now the nodes in the hyper-graph are not derivations, but equivalence classes ofderivations, i.e., states.
The number of nodes in thehypergraph is no longer always exponentially many,but depends on the equivalence function, which isthe atomic feature function f?(?)
in our algorithms.DP best-first shift-reduce parsing is still a specialcase of the Knuth algorithm.
However, it is more dif-ficult than best-first CKY parsing, because of the ex-tra topological order constraints from shift actions.4.2 Best-First EarleyDP best-first shift-reduce parsing is analogous toweighted Earley (Earley, 1970; Stolcke, 1995), be-cause: 1) in Earley the PRED rule generates statessimilar to shifted states in shift-reduce parsing; and,2) a newly completed state also needs to check allpossible left expansions and right expansions, simi-lar to a state popped from the priority queue in Al-gorithm 1.
(see Figure 2)Our Algorithm 2 exploits lazy expansion, whichreduces unnecessary expansions, and should bemore efficient than pure Earley.5 Optimality and Polynomial Complexity5.1 Proof of OptimalityWe define a best derivation of state [x]?
as a deriva-tion x such that ?y ?
[x]?, x  y.Note that each state has a unique feature signa-ture.
We want to prove that Algorithm 1 actually fillsthe chart by assigning a best derivation to its state.Without loss of generality, we assume Algorithm 1fills C with derivations in the following order:x0, x1, x2, .
.
.
, xmwhere x0 is the initial derivation, xm is the first goalderivation in the sequence, and C [xi] = xi, 0 ?
i ?m.
Denote the status of chart right after xk beingfilled as Ck.
Specially, we define C?1 = ?However, we do not have superiority as in non-DPbest-first parsing.
Because we use a pair of prefixscore and inside score, (pre, ins), as priority (Equa-tion 2) in the deductive system (Figure 2).
We havethe following property as an alternative for superior-ity:Lemma 1.
After derivation xk has been filled intochart, ?x s.t.
x ?
Q , and x is a best derivationof state [x]?, then x?s descendants can not have ahigher priority than xk.Proof.
Note that when xk pops out, x is still in Q ,so xk  x.
Assume z is x?s direct descendant.?
If z = sh(x) or z = re(x, ), based on the de-ductive system, x ?
z, so xk  x ?
z.?
If z = re(y, x), y ?
L(x), assume z ?
xk.z.pre = y.pre + y.sh + x.ins + x.reWe can construct a new derivation x?
?
x byappending x?s top tree, x.s0 to y?s stack, andx?.pre = y.pre + y.sh + x.ins < z.preSo x?
?
z ?
xk  x, which contradicts that xis a best derivation of its state.With induction we can easily show that any descen-dants of x can not have a higher priority than xk.We can now derive:Theorem 1 (Stepwise Completeness and Optimal-ity).
For any k, 0 ?
k ?
m, we have the followingtwo properties:?x ?
xk, [x]?
?
Ck?1 (Stepwise Completeness)?x ?
xk, xk  x (Stepwise Optimality)763Proof.
We prove by induction on k.1.
For k = 0, these two properties trivially hold.2.
Assume this theorem holds for k = 2, ..., i?1.For k = i, we have:a) [Proof for Stepwise Completeness](Proof by Contradiction) Assume ?x ?
xis.t.
[x]?
6?
Ci?1.Without loss of generality wetake a best derivation of state [x]?
as x. x mustbe derived from other best derivations only.Consider this derivation transition hypergraph,which starts at initial derivation x0 ?
Ci?1, andends at x 6?
Ci?1.There must be a best derivation x?
in this tran-sition hypergraph, s.t.
all best parent deriva-tion(s) of x?
are in Ci?1, but not x?.If x?
is a reduced derivation, assume x?
?s bestparent derivations are y ?
Ci?1, z ?
Ci?1.Because y and z are best derivations, and theyare in Ci?1, from Stepwise Optimality on k =1, ..., i?
1, y, z ?
{x0, x1, .
.
.
, xi?1}.
FromLine 7-11 in Algorithm 1, x?
must have beenpushed into Q when the latter of y, z is popped.If x?
is a shifted derivation, similarly x?
musthave been pushed into Q .As x?
6?
Ci?1, x?
must still be in Q when xi ispopped.
However, from Lemma 1, none of x?
?sdescendants can have a higher priority than xi,which contradicts x ?
xi.b) [Proof for Stepwise Optimality](Proof by Contradiction) Assume ?x ?
xis.t.
x ?
xi.
From Stepwise Completeness onk = 1, ..., i, x ?
Ci?1, which means the state[xi]?
has already been assigned to x, contra-dicting the premise that xi is pushed into chart.Both of the two properties have very intuitivemeanings.
Stepwise Optimality means Algorithm 1only fills chart with a best derivation for each state.Stepwise Completeness means every state that hasits best derivation better than best derivation pi musthave been filled before pi, this guarantees that therex?h?
?, h?k...i?
: (c?, v?)
?h?, hi...j?
: ( , v)?h?
?, hk...j?
: (c?
+ v + ?, v?
+ v + ?
)Figure 4: Example of shift-reduce with dynamic pro-gramming: simulating an edge-factored model.
GSSis implicit here, and rey case omitted.
Here ?
=scsh(h?
?, h?)
+ screx(h?, h).global best goal derivation is captured by Algo-rithm 1.More formally we have:Theorem 2 (Optimality of Algorithm 1).
The firstgoal derivation popped off the priority queue is theoptimal parse.Proof.
(Proof by Contradiction.)
Assume ?x, x isthe a goal derivation and x ?
xm.
Based on Step-wise Completeness of Theorem 1, x ?
Cm?1, thus xhas already been popped out, which contradicts thatxm is the first popped out goal derivation.Furthermore, we can see our lazy expansion ver-sion, i.e., Algorithm 2, is also optimal.
The key ob-servation is that we delay the reduction of derivationx?
and a derivation of right states R([x?]?)
(Line 11of Algorithm 1), until shifted derivation, x = sh(x?
),is popped out (Line 11 of Algorithm 2).
However,this delayed reduction will not generate any deriva-tion y, s.t.
y ?
x, because, based on our deduc-tive system (Figure 2), for any such kind of reducedderivations y, y.pre = x?.pre+x?.sh+y.re+y.ins ,while x.pre = x?.pre + x?.sh .5.2 Analysis of Time and Space ComplexityFollowing Huang and Sagae (2010) we present thecomplexity analysis for our DP best-first parsing.Theorem 3.
Dynamic programming best-first pars-ing runs in worst-case polynomial time and space,as long as the atomic features function satisfies:?
bounded: ?
derivation x, |?f(x)| is bounded bya constant.?
monotonic:764?
horizontal: ?k, fk(s) = fk(t) ?fk+1(s) = fk+1(t), for all possible treess, t.?
vertical: ?k, fk(sys?)
= fk(tyt?)
?fk(s) = fk(t) and fk(sxs?)
= fk(txt?)?fk(s?)
= fk(t?
), for all possible trees s, s?,t, t?.In the above theorem, boundness means we canonly extract finite information from a derivation, sothat the atomic feature function f?(?)
can only dis-tinguish a finite number of different states.
Mono-tonicity requires the feature representation fk sub-sumes fk+1.
This is necessary because we use thefeatures as signature to match all possible left statesand right states (Equation 1).
Note that we add thevertical monotonicity condition following the sug-gestion from Kuhlmann et al(2011), which fixesa flaw in the original theorem of Huang and Sagae(2010).We use the edge-factored model (Eisner, 1996;McDonald et al 2005) with dynamic programmingdescribed in Figure 4 as a concrete example for com-plexity analysis.
In the edge-factored model the fea-ture set consists of only combinations of informa-tion from the roots of the two top trees s1, s0, andthe queue.
So the atomic feature function isf?
(p) = (i, j, h(p.s1), h(p.s0))where h(s) returns the head word index of tree s.The deductive system for the edge-factored modelis in Figure 4.
The time complexity for this deduc-tive system is O(n6), because we have three headindexes and three span indexes as free variables inthe exploration.
Compared to the work of Huangand Sagae (2010), we reduce the time complexityfrom O(n7) to O(n6) because we do not need tokeep track of the number of the steps for a state.6 ExperimentsIn experiments we compare our DP best-first parsingwith non-DP best-first parsing, pure greedy parsing,and beam parser of Huang and Sagae (2010).Our underlying MaxEnt model is trained on thePenn Treebank (PTB) following the standard split:Sections 02-21 as the training set and Section 22 asthe held-out set.
We collect gold actions at differ-ent parsing configurations as positive examples frommodel score accuracy # states timegreedy ?1.4303 90.08% 125.8 0.0055beam?
?1.3302 90.60% 869.6 0.0331non-DP ?1.3269 90.70% 4, 194.4 0.2622DP ?1.3269 90.70% 243.2 0.0132Table 1: Dynamic programming best-first parsing reachoptimality faster.
*: for beam search we use beam size of8.
(All above results are averaged over the held-out set.
)gold parses in PTB to train the MaxEnt model.
Weuse the feature set of Huang and Sagae (2010).Furthermore, we reimplemented the beam parserwith DP of Huang and Sagae (2010) for compari-son.
The result of our implementation is consistentwith theirs.
We reach 92.39% accuracy with struc-tured perceptron.
However, in experiments we stilluse MaxEnt to make the comparison fair.To compare the performance we measure two setsof criteria: 1) the internal criteria consist of themodel score of the parsing result, and the numberof states explored; 2) the external criteria consist ofthe unlabeled accuracy of the parsing result, and theparsing time.We perform our experiments on a computer withtwo 3.1GHz 8-core CPUs (16 processors in total)and 64GB RAM.
Our implementation is in Python.6.1 Search Quality & SpeedWe first compare DP best-first parsing algorithmwith pure greedy parsing and non-DP best-first pars-ing without any extra constraints.The results are shown in Table 1.
Best-first pars-ing reaches an accuracy of 90.70% in the held-outset.
Since that the MaxEnt model is locally trained,this accuracy is not as high as the best shift-reduceparsers available now.
However, this is sufficient forour comparison, because we aim at improving thesearch quality and efficiency of parsing.Compared to greedy parsing, DP best-first pars-ing reaches a significantly higher accuracy, with ?2times more parsing time.
Given the extra time inmaintaining priority queue, this is consistent withthe internal criteria: DP best-first parsing reaches asignificantly higher model score, which is actuallyoptimal, exploring twice as many as states.On the other hand, non-DP best-first parsing alsoachieves the optimal model score and accuracy.76500.020.040.060.080.10.120  10  20  30  40  50  60  70avg.
parsingtime(secs)sentence lengthnon-DPDPbeamFigure 5: DP best-first significantly reduces parsing time.Beam parser (beam size 8) guarantees linear parsing time.Non-DP best-first parser is fast for short sentences, butthe time grows exponentially with sentence length.
DPbest-first parser is as fast as non-DP for short sentences,but the time grows significantly slower.However, it explores?17 times more states than DP,with an unbearable average time.Furthermore, on average our DP best-first parsingis significantly faster than the beam parser, becausemost sentences are short.Figure 5 explains the inefficiency of non-DP best-first parsing.
As the time complexity grows expo-nentially with the sentence length, non-DP best-firstparsing takes an extremely long time for long sen-tences.
DP best-first search has a polynomial timebound, which grows significantly slower.In general DP best-first parsing manages to reachoptimality in tractable time with exact search.
Tofurther investigate the potential of this DP best-first parsing, we perform inexact search experimentswith bounded priority queue.6.2 Parsing with Bounded Priority QueueBounded priority queue is a very practical choicewhen we want to parse with only limited memory.We bound the priority queue size at 1, 2, 5, 10,20, 50, 100, 500, and 1000, and once the priorityqueue size exceeds the bound, we discard the worstone in the priority queue.
The performances of non-DP best-first parsing and DP best-first parsing areillustrated in Figure 6 (a) (b).Firstly, in Figure 6 (a), our DP best-first pars-ing reaches the optimal model score with bound50, while non-DP best-first parsing fails even withbound 1000.
Also, in average with bound 1000,compared to non-DP, DP best-first only needs to ex-plore less than half of the number of states.Secondly, for external criteria in Figure 6 (b), bothalgorithms reach accuracy of 90.70% in the end.
Inspeed, with bound 1000, DP best-first takes ?1/3time of non-DP to parse a sentence in average.Lastly, we also compare to beam parser with beamsize 1, 2, 4, 8.
Figure 6 (a) shows that beam parserfails to reach the optimality, while exploring signif-icantly more states.
On the other hand, beam parseralso fails to reach an accuracy as high as best-firstparsers.
(see Figure 6 (b))6.3 Simulating the Edge-Factored ModelWe further explore the potential of DP best-firstparsing with the edge-factored model.The simplified feature set of the edge-factoredmodel reduces the number of possible states, whichmeans more state-merging in the search graph.
Weexpect more significant improvement from our DPbest-first parsing in speed and number of exploredstates.Experiment results confirms this.
In Figure 6 (c)(d), curves of DP best-first diverge from non-DPfaster than standard model (Figure 6 (a) (b)).7 Conclusions and Future WorkWe have presented a dynamic programming algo-rithm for best-first shift-reduce parsing which isguaranteed to return the optimal solution in poly-nomial time.
This algorithm is related to best-firstEarley parsing, and is more sophisticated than best-first CKY.
Experiments have shown convincinglythat our algorithm leads to significant speedup overthe non-dynamic programming baseline, and makesexact search tractable for the first-time under thismodel.For future work we would like to improve the per-formance of the probabilistic models that is requiredby the best-first search.
We are also interested inexploring A* heuristics to further speed up our DPbest-first parsing.766-1.45-1.4-1.35-1.30  100 200 300 400 500 600 700 800 900avg.modelscoreonheld-out# of statesbound=50 bound=1000beam=8non-DPDPbeam-1.32699090.290.490.690.80  0.01  0.02  0.03  0.04  0.05avg.accuracy(%) onheld-outparsing time (secs)bound=50bound=1000beam=8non-DPDPbeam90.70(a) search quality vs. # of states (b) parsing accuracy vs. time-1.4-1.36-1.32-1.28-1.240  100 200 300 400 500 600 700 800 900avg.modelscoreonheld-out# of statesbound=20 bound=500 beam=8non-DPDPbeam-1.256589.89090.20  0.01  0.02  0.03  0.04  0.05avg.accuracy(%) onheld-outparsing time (secs)bound=20 bound=500beam=8non-DPDPbeam90.25(c) search quality vs. # of states (edge-factored) (d) parsing accuracy vs. time (edge-factored)Figure 6: Parsing performance comparison between DP and non-DP.
(a) (b) Standard model with features of Huangand Sagae (2010).
(c) (d) Simulating edge-factored model with reduced feature set based on McDonald et al(2005).Note that to implement bounded priority queue we use two priority queues to keep track of the worst elements, whichintroduces extra overhead, so that our bounded parser is slower than the unbounded version for large priority queuesize bound.767ReferencesAlfred V. Aho and Jeffrey D. Ullman.
1972.
The The-ory of Parsing, Translation, and Compiling, volume I:Parsing of Series in Automatic Computation.
PrenticeHall, Englewood Cliffs, New Jersey.Sharon A Caraballo and Eugene Charniak.
1998.
Newfigures of merit for best-first probabilistic chart pars-ing.
Computational Linguistics, 24(2):275?298.Jay Earley.
1970.
An efficient context-free parsing algo-rithm.
Communications of the ACM, 13(2):94?102.Jason Eisner.
1996.
Three new probabilistic models fordependency parsing: An exploration.
In Proceedingsof COLING.Liang Huang and Kenji Sagae.
2010.
Dynamic program-ming for linear-time incremental parsing.
In Proceed-ings of ACL 2010.Dan Klein and Christopher D Manning.
2003.
A* pars-ing: Fast exact Viterbi parse selection.
In Proceedingsof HLT-NAACL.Dan Klein and Christopher D Manning.
2005.
Pars-ing and hypergraphs.
In New developments in parsingtechnology, pages 351?372.
Springer.Donald Knuth.
1977.
A generalization of Dijkstra?s al-gorithm.
Information Processing Letters, 6(1).Marco Kuhlmann, Carlos Go?mez-Rodr?
?guez, and Gior-gio Satta.
2011.
Dynamic programming algorithmsfor transition-based dependency parsers.
In Proceed-ings of ACL.Ryan McDonald, Koby Crammer, and Fernando Pereira.2005.
Online large-margin training of dependencyparsers.
In Proceedings of the 43rd ACL.Mark-Jan Nederhof.
2003.
Weighted deductive pars-ing and Knuth?s algorithm.
Computational Linguis-tics, pages 135?143.Joakim Nivre.
2008.
Algorithms for deterministic incre-mental dependency parsing.
Computational Linguis-tics, 34(4):513?553.Adam Pauls and Dan Klein.
2009.
Hierarchical searchfor parsing.
In Proceedings of Human LanguageTechnologies: The 2009 Annual Conference of theNorth American Chapter of the Association for Com-putational Linguistics, pages 557?565.
Association forComputational Linguistics.Kenji Sagae and Alon Lavie.
2006.
A best-first proba-bilistic shift-reduce parser.
In Proceedings of ACL.Andreas Stolcke.
1995.
An efficient probabilisticcontext-free parsing algorithm that computes prefixprobabilities.
Computational Linguistics, 21(2):165?201.Yue Zhang and Stephen Clark.
2008.
A tale oftwo parsers: investigating and combining graph-basedand transition-based dependency parsing using beam-search.
In Proceedings of EMNLP.768
