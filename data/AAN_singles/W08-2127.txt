CoNLL 2008: Proceedings of the 12th Conference on Computational Natural Language Learning, pages 203?207Manchester, August 2008Parsing Syntactic and Semantic Dependencies withTwo Single-Stage Maximum Entropy Models?Hai Zhao and Chunyu KitDepartment of Chinese, Translation and LinguisticsCity University of Hong Kong83 Tat Chee Avenue, Kowloon, Hong Kong, Chinahaizhao@cityu.edu.hk, ctckit@cityu.edu.hkAbstractThis paper describes our system to carryout the joint parsing of syntactic and se-mantic dependencies for our participationin the shared task of CoNLL-2008.
We il-lustrate that both syntactic parsing and se-mantic parsing can be transformed into aword-pair classification problem and im-plemented as a single-stage system withthe aid of maximum entropy modeling.Our system ranks the fourth in the closedtrack for the task with the following per-formance on the WSJ+Brown test set:81.44% labeled macro F1 for the overalltask, 86.66% labeled attachment for syn-tactic dependencies, and 76.16% labeledF1 for semantic dependencies.1 IntroductionThe joint parsing of syntactic and semantic depen-dencies introduced by the shared task of CoNLL-08 is more complicated than syntactic dependencyparsing or semantic role labeling alone (Surdeanuet al, 2008).
For semantic parsing, in particu-lar, a dependency-based representation is given butthe predicates involved are unknown, and we alsohave nominal predicates besides the verbal ones.All these bring about more difficulties for learning.This paper presents our research for participationin the CoNLL-2008 shared task, with a highlighton our strategy to select learning framework andfeatures for maximum entropy learning.
?This study is supported by CERG grant 9040861 (CityU1318/03H) and CityU Strategic Research Grant 7002037.?c?
2008.
Licensed under the Creative CommonsAttribution-Noncommercial-Share Alike 3.0 Unported li-cense (http://creativecommons.org/licenses/by-nc-sa/3.0/).Some rights reserved.The rest of the paper is organized as follows.The next section presents the technical details ofour system and Section 3 its evaluation results.Section 4 looks into a few issues concerning ourforthcoming work for this shared task, and Section5 concludes the paper.2 System DescriptionFor the sake of efficiency, we opt for the maximumentropy model with Gaussian prior as our learningmodel for both the syntactic and semantic depen-dency parsing.
Our implementation of the modeladopts L-BFGS algorithm for parameter optimiza-tion as usual (Liu and Nocedal, 1989).
No addi-tional feature selection techniques are applied.Our system consists of three components to dealwith syntactic and semantic dependency parsingand word sense determination, respectively.
Bothparsing is formulated as a single-stage word-pairclassification problem, and the latter is carried outby a search through the NomBank (Meyers et al,2004) or the PropBank (Palmer et al, 2005)1.2.1 Syntactic Dependency ParsingWe use a shift-reduce scheme to implement syn-tactic dependency parsing as in (Nivre, 2003).
Ittakes a step-wised, history- or transition-based ap-proach.
It is basically a word-by-word methodwith a projective constraint.
In each step, the clas-sifier checks a word pair, e.g., TOP, the top of astack for processed words, and, NEXT, the firstword in the unprocessed word sequence, in orderto determine if a dependent label should be as-signed to them.
Besides two arc-building actions,a shift action and a reduce action are also definedto meet the projective constraint, as follows.1These two dictionaries that we used are downloaded fromCoNLL-2008 official website.203Notation Meanings Clique in the top of stacks?1,...
The first clique below the top of stack, etc.i, i+1,...
The first (second) clique in the unprocessedsequence, etc.dprel Dependent labelh Headlm Leftmost childrm Rightmost childrn Right nearest childform Word formlemma Word lemmapos Predicted PoS tagsp Y Split Y , which may be form, lemma or pos.. ?s, e.g., ?s.dprel?
means dependent labelof the clique in the top of stack/ Feature combination, i.e., ?s.pos/i.pos?means s.pos and i.pos together as afeature function.p The current predicate candidatea The current argument candidateTable 1: Feature Notations1.
Left-arc: Add an arc from NEXT to TOP andpop the stack.2.
Right-arc: Add an arc from TOP to NEXTand push NEXT onto the stack.3.
Reduce: Pop TOP from the stack.4.
Shift: Push NEXT onto the stack.We implement a left-to-right arc-eager parsingmodel in a way that the parser scan through an in-put sequence from left to right and the right depen-dents are attached to their heads as soon as possible(Hall et al, 2007).
To construct a single-stage sys-tem, we extend the left-/right-arc actions to theircorrespondent multi-label actions as necessary.
In-cluding 32 left-arc and 66 right-arc actions, alto-gether a 100-class problem is yielded for the pars-ing action classification for this shared task.Since only projective sequences can be handledby the shift-reduce scheme, we apply the pseudo-projective transformation introduced by (Nivre andNilsson, 2005) to projectivize those non-projectivesequences.
Our statistics show that only 7.6% se-quences and less than 1% dependencies in the cor-pus provided for training are non-projective.
Thus,we use a simplified strategy to projectivize an inputsequence.
Firstly, we simply replace the head of anon-projective dependency by its original head?shead but without any additional dependent labelencoding for the purpose of deprojectivizing theoutput during decoding.
Secondly, if the abovestandard projectivization step cannot eliminate allBasic Extensionx.sp Y itself, its previous two and next two Y s, andall bigrams within the five-clique window,(x is s or i, and Y is form, lemma or pos.
)x.Y (x is s or i, and Y is form, lemma or pos.
)x.Y /i.Y (x is s or s?1and Y is pos, sp lemmaor sp pos)s.h.sp forms.dprels.lm.dprels.rn.dpreli.lm.sp poss.lm.dprel/s.dprels.lm.sp pos/s.sp poss.h.sp pos/s.sp posx.sp pos|rootscore (x is s or i.
)s.sp pos/i.sp pos|pairscores.curroot.sp pos/i.sp posTable 2: Features for Syntactic Parsingnon-projective dependencies in a sequence, thenthe word with the shortest sequence (rather thandependent tree) distance to the original head willbe chosen as the head of a non-projective depen-dency.
In practice, the above two-step projectiviza-tion procedure can eliminate all non-projective de-pendencies in all sequences.
Our purpose here is toprovide as much data as possible for training, andonly projective sequences are input for training andoutput for decoding.While memory-based and margin-based learn-ing approaches such as support vector machinesare popularly applied to shift-reduce parsing, ourwork provides evidence that the maximum en-tropy model can achieve a comparative perfor-mance with the aid of a suitable feature set.
Withfeature notations in Table 1, we use a feature set asshown in Table 2 for syntactic parsing.Here, we explain ?rootscore?, ?pairscore?
andcurroot in Table 2.
Both rootscore and pairscorereturn the log frequency for an event in the trainingcorpus.
The former counts a given split PoS occur-ring as ROOT, and the latter two split PoS?s com-bination associated with a dependency label.
Thefeature curroot returns the root of a partial parsingtree that includes a specified node.2.2 Semantic Dependency ParsingAssuming no predicates overtly known, we keepusing a word-pair classifier to perform semanticparsing through a single-stage processing.
Specif-ically, we specify the first word in a word pair asa predicate candidate (i.e., a semantic head, andnoted as p in our feature representation) and thenext as an argument candidate (i.e., a semantic de-204Basic Extensionx.sp Y itself, its previous and next cliques, andall bigrams within the three-clique window.
(Y is form or lemma.
)ax.sp pos itself, its previous and next two cliques, andall bigrams within the five-clique window.x.Y (Y is form, lemma or pos.
)p.Y /i.Y (Y is sp lemma or sp pos.
)a is the same as px.is Verb or NounbankAdviceba.h.sp formx.dprelx.lm.dprelp.rm.dprelp.lm.sp posa.lm.dprel/a.dprela.lm.sp pos/a.sp posa.sp Y/a.dprel (Y is lemma or pos.
)x.sp lemma/x.h.sp formp.sp lemma/p.h.sp posp.sp pos/p.dprela.preddircp.voice/a.preddirdx.posSeqex.dprelSeqfa.dpTreeLevelga/p|dpRelationa/p|SharedPosPatha/p|SharedDprelPatha/p|x.posPatha/p|x.dprelPatha/p|dprelPathax is p or a throughout the whole table.bThis and the following features are all concerned with aknown syntactic dependency tree.cpreddir: the direction to the current predicate candidate.dvoice: if the syntactic head of p is be and p is not endedwith -ing, then p is passive.eposSeq: PoS tag sequence of all syntactic childrenfdprelSeq: syntactic dependent label sequence of all syn-tactic childrengdpTreeLevel: the level in the syntactic parse tree, countedfrom the leaf node.Table 3: Features for Semantic Parsingpendent, and noted as a).
We do not differenti-ate between nominal and verbal predicates and oursystem handles them in in exactly the same way.If decoding outputs show that no arguments canbe found for a predicate candidate in the decodingoutput, then this candidate will be naturally dis-carded from the output predicate list.When no constraint available, however, all wordpairs in the an input sequence must be considered,leading to very poor efficiency in computation forno gain in effectiveness.
Thus, the training sampleneeds to be pruned properly.For predicate, only nouns and verbs are consid-ered possible candidates.
That is, all words with-out a split PoS in these two categories are filteredout.
Many prepositions are also marked as pred-icate in the training corpus, but their arguments?roles are ?SU?, which are not counted the officialevaluation.For argument, a dependency version of the prun-ing algorithm in (Xue and Palmer, 2004) is used tofind, in an iterative way, the current syntactic headand its siblings in a parse tree in a constituent-based representation.
In this representation, thehead of a phrase governs all its sisters in the tree,as illustrated in the conversion of constituents todependencies in (Lin, 1995).
In our implementa-tion, the following equivalent algorithm is appliedto select argument candidates from a syntactic de-pendency parse tree.Initialization: Set the given predicate candi-date as the current node;(1) The current node and all of its syntactic chil-dren are selected as argument candidates.
(2) Reset the current node to its syntactic headand repeat step (1) until the root is reached.This algorithm can cover 98.5% arguments whilereducing about 60% of the training samples, ac-cording to our statistics.
However, this is achievedat the price of including a syntactic parse tree aspart of the input for semantic parsing.The feature set listed in Table 3 is adopted forour semantic parsing, some of which are borrowedfrom (Hacioglu, 2004).
Among them, dpTreeRela-tion returns the relationship of a and p in a syntac-tic parse tree.
Its possible values include parent,sibling, child, uncle, grand parentetc.
Note that there is always a path to the ROOT inthe syntactic parse tree for either a or p. Along thecommon part of these two paths, SharedDprelPathreturns the sequence of dependent labels collectedfrom each node, and SharedPosPath returns thecorresponding sequence of PoS tags.
x.dprelPathand x.posPath return the PoS tag sequence from xto the beginnings of SharedDprelPath and Shared-PosPath, respectively.
a/p|dprelPath returns theconcatenation of a.dprelPath and p.dprelPath.We may have an example to show how the fea-ture bankAdvice works.
Firstly, the current pro-cessed semantic role labels and argument candi-date direction are checked.
Specifically, they arethe arguments A0 and A1 that have been markedbefore the predicate candidate p and the current ar-gument identification direction after p. Secondly,205UAS LAS Label-Acc.Development 88.78 85.85 91.14WSJ 89.86 87.52 92.47Brown 85.03 79.83 86.71WSJ+Brown 89.32 86.66 91.83Table 4: The Results of Syntactic Parsing (%)Data Precision Recall F-scoreDevelopment 79.76 72.25 75.82Label.
WSJ 80.57 74.97 77.67Brown 66.28 61.29 63.69WSJ+Brown 79.03 73.49 76.16Development 89.58 81.15 85.16Unlab.
WSJ 89.48 83.26 86.26Brown 83.14 76.88 79.89WSJ+Brown 88.79 82.57 85.57Table 5: The Results of Semantic Parsing (%)each example2of p in NomBank or PropBank thatdepends on the split PoS tag of p is checked ifit partially matches the current processed role la-bels.
If a unique example exists in this form, e.g.,Before:A0-A1; After:A3, then this featurereturns A3 as feature value.
If no matched or mul-tiple matched examples exist, then this feature re-turns a default value.2.3 Word Sense DeterminationThe shared task of CoNLL-2008 for word sensedisambiguation task is to determine the sense of anoutput predicate.
Our system carries out this taskby searching for a right example in the given Nom-Bank or PropBank.
The semantic role set schemeof each example for an output predicate is checked.If a scheme is found to match the output seman-tic role set of a predicate, then the correspondingsense for the first match is chosen; otherwise thesystem outputs ?01?
as the default sense.3 Evaluation ResultsOur evaluation is carried out on a 64-bit ubuntuLinux installed server with double dual-core AMDOpteron processors of 2.8GHz and 8GB memory.The full training set for CoNLL-2008 is used totrain the maximum entropy model.
The trainingfor the syntactic parser costs about 200 hours and2The term ?example?
means a chunk in NomBankor PropBank, which demonstrates how semantic rolesoccur around a specified predicate.
For example, fora sense item of the predicate access in PropBank,we first have <arg n="0">a computer</arg><rel>access</rel> <arg n="1">itsmemory</arg>, and then a role set scheme for thissense as Before:A0;After:A1.Data Precision Recall F-scoreDevelopment 82.80 79.05 80.88Label.
WSJ 84.05 81.25 82.62Macro Brown 73.05 70.56 71.78WSJ+Brown 82.85 80.08 81.44Development 89.18 84.97 87.02Unlab.
WSJ 89.67 86.56 88.09Macro Brown 84.08 80.96 82.49WSJ+Brown 89.06 85.94 87.47Development 83.69 80.71 82.17Label.
WSJ 85.07 82.88 83.96Micro Brown 75.14 73.09 74.10WSJ+Brown 83.98 81.80 82.88Development 89.06 85.90 87.45Unlab.
WSJ 89.72 87.42 88.56Micro Brown 84.38 82.07 83.21WSJ+Brown 89.14 86.83 87.97Table 6: Overall Scores (%)4.1GB memory and that for the semantic parsercosts about 170 hours and 4.9GB memory.
Therunning time in each case is the sum of all runningtime for all threads involved.
When a parallel opti-mization technique is applied to speedup the train-ing, the time can be reduced to about 1/3.5 of theabove.The official evaluation results for our system arepresented in Tables 4, 5 and 6.
Following theofficial guideline of CoNLL-2008, we use unla-beled attachment score (UAS), labeled attachmentscore (LAS) and label accuracy to assess the per-formance of syntactic dependency parsing.
Forsemantic parsing, the unlabeled scores metric theidentification performance and the labeled scoresthe overall performance of semantic labeling.4 To DoAlthough we are unable to follow our plan to domore than what we have done for this shared task,because of the inadequate computational resourceand limited time, we have a number of techniquesin our anticipation to bring in further performanceimprovement.While expecting to accomplish the joint infer-ence of syntactic and semantic parsing, we onlyhave time to complete a system with the former toenhance the latter.
But we did have experiments inthe early stage of our work to show that a syntacticdependency parser can make use of available se-mantic dependency information to enhance its per-formance by 0.5-1%3.Most errors in our syntactic parsing are related3We used the outputs of a semantic parser, either predictedor gold-standard, as features for syntactic parsing.206to the dependencies of comma and prepositions.We need to take care of them, for PP attachmentis also crucial to the success of semantic parsing.Extra effort is paid, as illustrated in previous worksuch as (Xue and Palmer, 2004), to handle suchcases, especially when a PP is involved.
We find inour data that about 1% arguments occur as a grand-child of a predicate through PP attachment.Syntactic parsing contributes crucially to theoverall performance of the joint parsing by pro-viding a solid basis for further semantic parsing.Thus there is reason to believe that improvementof syntactic dependency parsing can be more in-fluential than that of semantic parsing to the overallimprovement.
Only one model was used for syn-tactic parsing in our system, in contrast to the exist-ing work using an ensemble technique for furtherperformance enhancement, e.g., (Hall et al, 2007).Again, the latter means much more computationalcost should be taken.Though it was not done before submission dead-line, we also tried to enhance the semantic parsingwith some more sophisticated inputs from the syn-tactic parsing.
One is predicted syntactic parsedtree input that may be created by cross-validationrather than the gold-standard syntactic input thatour submitted semantic parser was actually trainedon.
Another is the n-best outputs of the syntacticparser.
However, only the single-best output of thesyntactic parser was actually used.5 ConclusionAs presented in the above sections, our system toparticipate in the CoNLL-2008 shared task is im-plemented as two single-stage maximum entropylearning.
We have tackled both syntactic and se-mantic parsing as a word-pair classification prob-lem.
Despite the simplicity of this approach, oursystem has produced promising results.AcknowledgementsWe wish to thank Dr. Wenliang Chen of NICT,Japan for helpful discussions on dependency pars-ing, and two anonymous reviewers for their valu-able comments.ReferencesHacioglu, Kadri.
2004.
Semantic role labeling us-ing dependency trees.
In Proceedings of the 20thinternational conference on Computational Linguis-tics (COLING-2004), pages 1273?1276, Geneva,Switzerland, August 23rd-27th.Hall, Johan, Jens Nilsson, Joakim Nivre,G?ulsen Eryi?git, Be?ata Megyesi, Mattias Nils-son, and Markus Saers.
2007.
Single malt orblended?
a study in multilingual parser optimiza-tion.
In Proceedings of the CoNLL Shared TaskSession of EMNLP-CoNLL 2007, pages 933?939,Prague, Czech, June.Lin, Dekang.
1995.
A dependency-based method forevaluating broad-coverage parser.
In Proceedingsof the Fourteenth International Joint Conference onArtificial Intelligence (IJCAI-95), pages 1420?1425,Montr?eal, Qu?ebec, Canada, August 20-25.Liu, Dong C. and Jorge Nocedal.
1989.
On the lim-ited memory bfgs method for large scale optimiza-tion.
Mathematical Programming, 45:503?528.Meyers, Adam, Ruth Reeves, Catherine Macleod,Rachel Szekely, Veronika Zielinska, Brian Young,and Ralph Grishman.
2004.
The nombank project:An interim report.
In Proceedings of HLT/NAACLWorkshop on Frontiers in Corpus Annotation, pages24?31, Boston, Massachusetts, USA, May 6.Nivre, Joakim and Jens Nilsson.
2005.
Pseudo-projective dependency parsing.
In Proceedings ofthe 43rd Annual Meeting on Association for Compu-tational Linguistics (ACL-2005), pages 99?106, AnnArbor, Michigan, USA, June 25-30.Nivre, Joakim.
2003.
An efficient algorithm for pro-jective dependency parsing.
In Proceedings of the8th International Workshop on Parsing Technologies(IWPT 03), pages 149?160, Nancy, France, April 23-25.Palmer, Martha, Daniel Gildea, and Paul Kingsbury.2005.
The proposition bank: An annotated cor-pus of semantic roles.
Computational Linguistics,31(1):71?106.Surdeanu, Mihai, Richard Johansson, Adam Meyers,Llu?
?s M`arquez, and Joakim Nivre.
2008.
TheCoNLL-2008 shared task on joint parsing of syntac-tic and semantic dependencies.
In Proceedings ofthe 12th Conference on Computational Natural Lan-guage Learning (CoNLL-2008).Xue, Nianwen and Martha Palmer.
2004.
Cal-ibrating features for semantic role labeling.
In2004 Conference on Empirical Methods in NaturalLanguage Processing (EMNLP-2004), pages 88?94,Barcelona, Spain, July 25-26.207
