Proceedings of the ACL-IJCNLP 2009 Conference Short Papers, pages 93?96,Suntec, Singapore, 4 August 2009.c?2009 ACL and AFNLPAutomatic Generation of Information-seeking Questions Using ConceptClustersShuguang LiDepartment of Computer ScienceUniversity of York, YO10 5DD, UKsgli@cs.york.ac.ukSuresh ManandharDepartment of Computer ScienceUniversity of York, YO10 5DD, UKsuresh@cs.york.ac.ukAbstractOne of the basic problems of efficientlygenerating information-seeking dialoguein interactive question answering is to findthe topic of an information-seeking ques-tion with respect to the answer documents.In this paper we propose an approach tosolving this problem using concept clus-ters.
Our empirical results on TREC col-lections and our ambiguous question col-lection shows that this approach can besuccessfully employed to handle ambigu-ous and list questions.1 IntroductionQuestion Answering systems have received a lotof interest from NLP researchers during the pastyears.
But it is often the case that traditional QAsystems cannot satisfy the information needs ofthe users as the question processing part may failto properly classify the question or the informa-tion needed for extracting and generating the an-swer is either implicit or not present in the ques-tion.
In such cases, interactive dialogue is neededto clarify the information needs and reformulatethe question in a way that will help the system tofind the correct answer.Due to the fact that casual users often ask ques-tions with ambiguity and vagueness, and most ofthe questions have multiple answers, current QAsystems return a list of answers for most questions.The answers for one question usually belong todifferent topics.
In order to satisfy the informationneeds of the user, information-seeking dialogueshould take advantage of the inherent grouping ofthe answers.Several methods have been investigated for gen-erating topics for questions in information-seekingdialogue.
Hori et al (2003) proposed a methodfor generating the topics for disambiguation ques-tions.
The scores are computed purely based onthe syntactic ambiguity present in the question.Phrases that are not modified by other phrases areconsidered to be highly ambiguous while phrasesthat are modified are considered less ambiguous.Small et al (2004) utilizes clarification dialogueto reduce the misunderstanding of the questionsbetween the HITIQA system and the user.
Thetopics for such clarification questions are basedon manually constructed topic frames.
Similarlyin (Hickl et al, 2006), suggestions are made tousers in the form of predictive question and answerpairs (known as QUABs) which are either gener-ated automatically from the set of documents re-turned for a query (using techniques first describedin (Harabagiu et al, 2005), or are selected from alarge database of questions-answer pairs createdoffline (prior to a dialogue) by human annotators.In Curtis et al (2005), query expansion of thequestion based on Cyc Knowledge is used to gen-erate topics for clarification questions.
In Duan etal.
(2008), the tree-cutting model is used to selecttopics from a set of relevant questions from YahooAnswers.None of the above methods consider the con-texts of the list of answers in the documents re-turned by QA systems.
The topic of a goodinformation-seeking question should not only berelevant to the original question but also should beable to distinguish each answer from the others sothat the new information can reduce the ambiguityand vagueness in the original question.
Instead ofusing traditional clustering methods on categoriza-tion of web results, we present a new topic gener-ation approach using concept clusters and a sepa-rability scoring mechanism for ranking the topics.2 Topic Generation Based on ConceptClusteringText categorization and clustering especially hier-archical clustering are predominant approaches toorganizing large amounts of information into top-93ics or categories.
But the main issue of catego-rization is that it is still difficult to automaticallyconstruct a good category structure, and manu-ally formed hierarchies are usually small.
And themain challenge of clustering algorithms is that theautomatically formed cluster hierarchy may be un-readable or meaningless for human users.
In orderto overcome the limits of the above methods, wepropose a concept clusters method and choose thelabels of the clusters as topics.Recent research on automatically extractingconcepts and clusters of words from large databasemakes it feasible to grow a big set of concept clus-ters.
Clustering by Committee (CBC) in Pantelet al (2002) made use of the fact that words inthe same cluster tend to appear in similar con-texts.
Pasca et al (2008) utilized Google logs andlexico-syntactic patterns to get clusters with labelssimultaneously.
Google also released Google Setswhich can be used to grow concept clusters withdifferent sizes.Currently our clusters are the union of the setsgenerated by the above three approaches, andwe label them using the method described inPasca et al (2008).
We define the conceptclusters in our collection as {C1, C2, ..., Cn}.Ci={ei1, ei2, ..., eim}, eijis jthsubtopic of clus-ter Ciand m is the size of Ci.We designed our system to take a questionand its corresponding list of answers as inputand then retrieve Google snippet documents foreach of the answers with respect to the ques-tion.
In a vectorspace model, a document isrepresented by a vector of keywords extractedfrom the document, with associated weights rep-resenting the importance of the keywords in thedocument and within the whole document col-lection.
A document Djin the collection isrepresented as {W0j,W1j, ...,Wnj}, and Wijisthe weight of word i in document j.
Here weuse our concept clusters to create concept clus-ter vectors.
A document Djnow is representedas <WC1j,WC2j, ...,WCnj>, and WCijis thescore vector of document Djfor concept clusterCi:WCij= <Scorej(ei1), Scorej(ei2), ...Scorej(eim)>Scorej(eip) is the weight of subtopic eipof cluster Ciindocument Dj.Currently we use tf-idf scheme (Yang et al, 1999)to calculate the weight of subtopics.3 Concept Cluster Separability MeasureWe view different concept clusters from the con-texts of the answers as different groups of fea-tures that can be used to classify the answers docu-ments.
We rank different context features by theirseparability on the answers.
Currently our systemretrieves the answers from Google search snippets,and each snippet is quite short.
So we combine thetop 50 snippets for one answer into one document.One answer is associated with one such big doc-ument.
We propose the following interclass mea-sure to compare the separability of different clus-ters:Score(Ci) =DNN?p<qDis(Dp, Dq),D is the Dimension Penalty score, D =1M,M is the size of cluster Ci,N is the combined total number of classes from all the answersDis(Dp, Dq) =?n?m=0(Scorep(eim)?
Scoreq(eim))2We introduce D, the ?Dimension Penalty?
scorewhich gives higher penalty to bigger clusters.
Cur-rently we use the reciprocal of the size of the clus-ter.
The second part is the average pairwise dis-tance between answers.
N is the total number ofclasses of the answers.
Next we describe in detailhow to use the concept cluster vectors and separa-bility measure to rank clusters.4 Cluster Ranking AlgorithmInput:Answer set A = {A1, A2, ..., Ap};Documents set D = {D1, D2, ..., Dp} associated with answer set A;Concept cluster set CS = {Ci| some of the subtopics from Cioccurs in D};Threshold ?1, ?2; The question Q;Concept cluster set QS = {Ci| some of the subtopics from Cioccurs in Q}Output:T = {< Ci, Score >}, a set of pairs of a concept cluster and its rankingscore;QS;Variables: X , Y ;Steps:1.
CS = CS ?QS2.
For each cluster Ciin CS3.
X = No.
of answers in which context subtopics from Ciare present;4.
Y = No.
of subtopics from Cithat occurs in the answers?
contexts;5.
If X < ?1or Y < ?26.
delete Cifrom CS7.
continue8.
Represent every document as a concept cluster vector on Ci(seesection 2)9.
Calculate the Score(Ci) using our separability measure10.
Store < Ci, Score > in T11.
return T the medoid.Figure 1: Concept Cluster Ranking AlgorithmFigure 1 describes the algorithm for rank-ing concept clusters based on their separabil-ity score.
This algorithm starts by deleting all94the clusters which are in QS from CS so thatwe only focus on the context clusters whosesubtopics are present in the answers.
Howeverin some cases this assumption is incorrect1.
Tak-ing the question shown in Table 2 for example,there are 6 answers for question LQ1, and inStep 1 CS = {C41American State, C1522Times,C414Tournament, C10004Y ear, ...} and QS ={C4545Event}.
Using cluster C414(see Table 2),D = {D1{Daytona 500, 24 Hours of Daytona,24 Hours of Le Mans, ...}, D2{3M Performance400, Cummins 200, ...}, D3{Indy 500, Truck se-ries, ...}, ...}, and hence the vector representa-tion for a given document Djusing C414willbe <Scorej(indy 500), Scorej(Cummins 200),Scorej(daytona 500), ...>.In Step 2 through 11 from Figure 1, for eachcontext cluster Ciin CS we calculate X (the num-ber of answers in which context subtopics from Ciare present), and Y (the number of subtopics fromCithat occurs in the answers?
contexts).
We wouldlike the clusters to hold two characteristics: (a) atleast occur in ?1answers as we want to have acluster whose subtopics are widely distributed inthe answers.
Currently we set ?1as half the num-ber of the answers; (b) at least have ?2subtopicsoccurring in the answers?
documents.
We set ?2as the number of the answers.
For example, forcluster C414, X = 6, Y = 10, ?1= 3 and ?2=6, so this cluster has the above two characteris-tics.
If a cluster has the above two characteris-tics, we use our separability measure described insection 3 to calculate a score for this cluster.
Thesize of C414is 11, so Score(C414) =111?6?Np<qDis(Dp, Dq).
Ranking the clusters based on thisseparability score means we will select a clus-ter which has several subtopics occurring in theanswers and the answers are distinguished fromeach other because they belong to these differentsubtopics.
The top three clusters for question LQ1is shown in Table 2.5 Experiment5.1 Data Set and Baseline MethodTo the best of our knowledge, the only availabletest data of multiple answer questions are list ques-tions from TREC 2004-2007 Data.
For our first1For the question ?In which movies did ChristopherReeve acted?
?, cluster Actor{Christopher Reeve, michaelcaine, anthony hopkins, ...} is quite useful.
While for ?Whichcountry won the football world cup??
cluster Sports{football,hockey, ...} is useless.list question collection we randomly selected 200questions which have at least 3 answers.
Wechanged the list questions to factoid ones withadditional words from their context questions toeliminate ellipsis and reference.
For the ambigu-ous questions, we manually choose 200 questionsfrom TREC 1999-2007 data and some questionsdiscussed as examples in Hori et al (2003) andBurger et al (2001).We compare our approach with a baselinemethod.
Our baseline system does not rank theclusters by the above separability score instead itprefers the cluster which occurs in more answersand have more subtopics distributed in the answerdocuments.
If we still use X to represent the num-ber of answers in which context subtopics fromone cluster are present and Y to represent the num-ber of subtopics from this cluster that occurs in theanswers?
contexts, for the baseline system, we willuse X ?
Y to rank all the concept clusters foundin the contexts.5.2 Results and Error AnalysisWe applied our algorithm on the two collectionsof questions.
Two assessors were involved in themanual judgments with an inter-rater agreementof 97%.
For each approach, we obtained the top20 clusters based on their scores.
Given a clus-ter with its subtopics in the contexts of the an-swers, an assessor manually labeled each cluster?good?
or ?bad?.
If it is labeled ?good?, the clusteris deemed relevant to the question and the clus-ter?s label could be used as dialogue seeking ques-tion?s topic to distinguish one answer from the oth-ers.
Otherwise, the assessor will label a cluster as?bad?.
We use the above two ranking approachesto rank the clusters for each question.
Table 1 pro-vides the statistics of the performance on the thetwo question collection.
List B means the base-line method on the list question set while Am-biguous S means our separability method on theambiguous questions.
The ?MAP?
column is themean of average precisions over the set of clusters.The ?P@1?
column is the precision of the top onecluster while the ?P@3?
column is the precisionof the top three clusters2.
The ?Err@3?
column isthe percentage of questions whose top three clus-ters are all labeled ?bad?.
One example associatedwith the manually constructed desirable questions2?P@3?
is the number of ?good?
clusters out of the topthree clusters95Table 1: Experiment resultsMethods MAP P@1 P@3 Err@3List B 41.3% 42.1% 27.7% 33.0%List S 60.3% 90.0% 81.3% 11.0%Ambiguous B 31.1% 33.2% 21.8% 47.1%Ambiguous S 53.6% 71.1% 64.2% 29.7%Table 2: TREC Question ExamplesLQ1: Who is the winners of the NASCAR races?1stC414(Tournament):{indy 500, Cummins 200, day-tona 500, ...}Q1 Which Tournament are you interested in?2ndC41(American State):{houston, baltimore, los an-geles, ...}Q2 Which American State were the races held?3rdC1522(Times):{once, twice, three times, ...}Q3 How many times did the winner win?is shown in Table 2.From Table 1, we can see that our approachoutperforms the baseline approach in terms of allthe measures.
We can see that 11% of the ques-tions have no ?good?
clusters.
Further analysisof the answer documents shows that the ?bad?clusters fall into four categories.
First, there arenoisy subtopics in some clusters.
Second, somequestions?
clusters are all labeled ?bad?
becausethe contexts for different answers are too simi-lar.
Third, unstructured web document soften con-tain multiple subtopics.
This means that differentsubtopics are in the context of the same answer.Currently we only look for context words whilenot using any scheme to specify whether there is arelationship between the answer and the subtopics.Finally, for other ?bad?
cases and the questionswith no good clusters all of the separability scoresare quite low.
This is because the answers fallinto different topics which do not share a commontopic in our cluster collection.6 Conclusion and DiscussionThis paper proposes a new approach to solvethe problem of generating an information-seekingquestion?s topic using concept clusters that can beused in a clarification dialogue to handle ambigu-ous questions.
Our empirical results show that thisapproach leads to good performance on TREC col-lections and our ambiguous question collections.The contribution of this paper are: (1) a new con-cept cluster method that maps a document into avector of subtopics; (2) a new ranking scheme torank the context clusters according to their sepa-rability.
The labels of the chosen clusters can beused as topics in an information-seeking question.Finally our approach shows significant improve-ment (nearly 48% points) over comparable base-line system.But currently we only consider the context clus-ters while ignoring the clusters associated with thequestions.
In the future, we will further investigatethe relationships between the concept clusters inthe question and the answers.ReferencesTiphaine Dalmas, Bonnie L. Webber: Answer com-parison in automated question answering.
J. AppliedLogic (JAPLL) 5(1):104-120, (2007).Chiori Hori, Sadaoki Furui: A new approach to auto-matic speech summarization.
IEEE Transactions onMultimedia (TMM) 5(3):368-378, (2003).Sharon Small and Tomek Strzalkowski, HITIQA:A Data Driven Approach to Interactive Analyti-cal Question Answering, in Proceedings of HLT-NAACL 2004: Short Papers, (2004).Andrew Hickl, Patrick Wang, John Lehmann, SandaM.
Harabagiu: FERRET: Interactive Question-Answering for Real-World Environments.
ACL,(2006).Sanda M. Harabagiu, Andrew Hickl, John Lehmann,Dan I. Moldovan: Experiments with InteractiveQuestion-Answering.
ACL, (2005).John Burger et al: Issues, Tasks and Program Struc-tures to Roadmap Research in Question and An-swering (Q&A),DARPA/NSF committee publica-tion, (2001).Patrick Pantel, Dekang Lin: Document clustering withcommittees.
SIGIR 2002:199-206, (2002).Marius Pasca and Benjamin Van Durme: Weakly-Supervised Acquisition of Open-Domain Classesand Class Attributes from Web Documents andQuery Logs.
ACL, (2008).Sanda M. Harabagiu, Andrew Hickl, V. Finley La-catusu: Satisfying information needs with multi-document summaries.
Inf.
Process.
Manage.
(IPM)43(6):1619-1642, (2007).Huizhong Duan, Yunbo Cao, Chin-Yew Lin and YongYu: Searching Questions by Identifying QuestionTopic and Question Focus.
ACL, (2008).Jon Curtis, G. Matthews and D. Baxter: On the Effec-tive Use of Cyc in a Question Answering System.IJCAI Workshop on Knowledge and Reasoning forAnswering Questions, Edinburgh, (2005).96
