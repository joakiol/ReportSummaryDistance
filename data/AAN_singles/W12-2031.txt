The 7th Workshop on the Innovative Use of NLP for Building Educational Applications, pages 263?271,Montre?al, Canada, June 3-8, 2012. c?2012 Association for Computational LinguisticsKU Leuven at HOO-2012: A Hybrid Approach to Detection and Correctionof Determiner and Preposition Errors in Non-native English TextLi Quan, Oleksandr Kolomiyets, Marie-Francine MoensDepartment of Computer ScienceKU LeuvenCelestijnenlaan 200A3001 Heverlee, Belgiumli.quan@student.kuleuven.be{oleksandr.kolomiyets, sien.moens}@cs.kuleuven.beAbstractIn this paper we describe the technical im-plementation of our system that participatedin the Helping Our Own 2012 Shared Task(HOO-2012).
The system employs a num-ber of preprocessing steps and machine learn-ing classifiers for correction of determiner andpreposition errors in non-native English texts.We use maximum entropy classifiers trainedon the provided HOO-2012 development dataand a large high-quality English text collec-tion.
The system proposes a number of highly-probable corrections, which are evaluated by alanguage model and compared with the origi-nal text.
A number of deterministic rules areused to increase the precision and recall of thesystem.
Our system is ranked among the threebest performing HOO-2012 systems with aprecision of 31.15%, recall of 22.08% and F1-score of 25.84% for correction of determinerand preposition errors combined.1 IntroductionThe Helping Our Own Challenge (Dale and Kilgar-riff, 2010) is a shared task that was proposed to ad-dress automated error correction of non-native En-glish texts.
In particular, the Helping Our Own 2012Shared Task (HOO-2012) (Dale et al, 2012) focuseson determiners and prepositions as they are well-known sources for errors produced by non-nativeEnglish writers.
For instance, Bitchener et al (2005)reported error rates of respectively 20% and 29%.Determiners are in particular challenging becausethey depend on a large discourse context and worldknowledge, and moreover, they simply do not existin many languages, such as Slavic and South-EastAsian languages (Ghomeshi et al, 2009).
The useof prepositions in English is idiomatic and thus verydifficult for learners of English.
On the one hand,prepositions connect noun phrases to other words ina sentence (e.g.
.
.
.
by bus), on the other hand, theycan also be part of phrasal verbs such as carry on,hold on, etc.In this paper we describe our system implemen-tation and results in HOO-2012.
The paper is struc-tured as follows.
Section 2 gives the task definition,errors addressed, data resources and evaluation cri-teria and metrics.
Section 3 shows some backgroundand related work.
Section 4 gives the full system de-scription, while Section 5 reports and discusses theresults of the experiments.
Section 6 concludes withan error analysis and possible further improvements.2 HOO-2012 Tasks and Resources2.1 TasksIn the scope of HOO-2012 the following six possibleerror types1 are targeted:?
Replace determiner (RD):Have the nice day.
?
Have a nice day.?
Missing determiner (MD):That is great idea.
?
That is a great idea.?
Unnecessary determiner (UD):I like the pop music.
?
I like pop music.1The set of error tags is based on the Cambridge UniversityPress Error Coding System, fully described in (Nicholls, 2003).263?
Replace preposition (RT):In the other hand.
.
.
?
On the other hand.
.
.?
Missing preposition (MT):She woke up 6 o?clock.
?
She woke up at 6o?clock.?
Unnecessary preposition (UT):He must go to home.
?
He must go home.2.2 DataThe HOO development dataset consists of 1000exam scripts drawn from a subset of the CLC FCEDataset (Yannakoudakis et al, 2011).
This corpuscontains texts written by students who attended theCambridge ESOL First Certificate in English exam-ination in 2000 and 2001.
The entire developmentdataset comprises 374680 words, with an averageof 375 words per file.
The test data consists of afurther 100 files provided by Cambridge UniversityPress (CUP), with 18013 words, and an average of180 words per file.Type # Dev # Test A # Test BRD 609 38 37MD 2230 125 131UD 1048 53 62Det 3887 217 230RT 2618 136 148MT 1104 57 56UT 822 43 39Prep 4545 236 243Total 8432 453 473Words/Error 44.18 39.77 38.08Table 1: Data error statistics.Counts of the different error types are provided inTable 1.
The table shows counts for the developmentdataset (?Dev?)
and two versions of the gold stan-dard test data: the original version as derived fromthe CUP-provided dataset (?Test A?
), and a revisedversion (?Test B?)
which was compiled in responseto requests for corrections from participating teams.The datasets and the revision process are further ex-plained in (Dale et al, 2012).2.3 Evaluation Criteria and MetricsFor evaluation in the HOO framework, a distinctionis made between scores and measures.
The com-plete evaluation mechanism is described in detail in(Dale and Narroway, 2012) and on the HOO-2012website.2Scores Three different scores are used:1.
Detection: does the system determine that anedit of the specified type is required at somepoint in the text?2.
Recognition: does the system correctly deter-mine the extent of the source text that requiresediting?3.
Correction: does the system offer a correctionthat is identical to that provided in the goldstandard?Measures For each score, three measures are cal-culated: precision (1), recall (2) and F -score (3).precision =tptp+ fp(1)recall =tptp+ fn(2)where tp is the number of true positives (the num-ber of instances that are correctly found by the sys-tem), fp the number of false positives (the numberof instances that are incorrectly found), and fn thenumber of false negatives (missing results).F?
= (1 + ?2)precision ?
recall?2 ?
precision+ recall(3)where ?
is used as a weight factor regulating thetrade-off between recall and precision.
We use thebalanced F -score, i.e.
?
= 1, such that recall andprecision are equally weighted.Combined We provide results on prepositions anddeterminers combined, and for each of these twosubcategories separately.
We also report on each ofthe different error types separately.2See http://www.correcttext.org/hoo2012.2643 Related WorkHOO-2012 follows on from the HOO-2011 SharedTask Pilot Round (Dale and Kilgarriff, 2011).
Thattask targeted a broader range of error types, and useda much smaller dataset.Most work on models for determiner and preposi-tion generation has been developed in the context ofmachine translation output (e.g.
(Knight and Chan-der, 1994), (Minnen et al, 2000), (De Felice andPulman, 2007) and (Toutanova and Suzuki, 2007)).Some of these methods depend on full parsing oftext, which is not reliable in the context of noisynon-native English texts.Only more recently, models for automated errordetection and correction of non-native texts havebeen explicitly developed and studied.
Most of thesemethods use large corpora of well-formed native En-glish text to train statistical models, e.g.
(Han et al,2004), (Gamon et al, 2008) and (De Felice and Pul-man, 2008).
Yi et al (2008) used web counts to de-termine correct article usage, while Han et al (2010)trained a classifier solely on a large error-taggedlearner corpus for preposition error correction.4 System Description4.1 Global System WorkflowThe system utilizes a hybrid approach that combinesstatistical machine learning classifiers and a rule-based system.
The global system architecture is pre-sented in Figure 1.
This section describes the globalsystem workflow.
The subsequent sections elabo-rate on the machine learning classifiers and heuris-tics implemented in the system.The system workflow is divided in the followingprocessing steps:1.
Text Preprocessing: The system performs apreliminary text analysis by automated spellingcorrection and subsequent syntactic analysis,such as tokenization and part-of-speech (POS)tagging.2.
Error Detection, Recognition and Correction:The system identifies if a correction is needed,and the type and extent of that correction.
Twofamilies of error correction tasks that separatelyaddress determiners and prepositions are per-formed in parallel.3.
Correction validation: Once a correction hasbeen proposed, it is validated by a languagemodel derived from a large corpus of high-quality English text.4.1.1 Text PreprocessingIn HOO-2012, texts submitted for automated cor-rections are written by learners of English.
Besidesthe error types that are addressed in HOO-2012, mis-spellings are another type of highly-frequent errors.For example, one student writes the following: In mypoint of vue, Internet is the most important discoverof the 2000 centery.When using automated natural language process-ing tools, incorrect spelling (and grammar) can in-troduce an additional bias.
To reduce the bias propa-gated from the preprocessing steps, the text is firstautomatically corrected by the open-source spellchecker GNU Aspell.3At the next step, the text undergoes a shallow syn-tactic analysis that includes sentence boundary de-tection, tokenization, part-of-speech tagging, chunk-ing, lemmatization, relation finding and preposi-tional phrase attachment.
These tasks are performedby MBSP (De Smedt et al, 2010).44.1.2 Error Detection, Recognition andCorrectionIn general, the task of automated error correctionis addressed by a number of subtasks of finding theposition in text, recognizing the type of error, andthe proposal for a correction.
In our implementationwe approach these tasks in a two-step approach asproposed in (Gamon et al, 2008).
With two familiesof errors, the system therefore employs four classi-fiers in total.For determiner error corrections, a classifier (C1in Figure 1) first predicts whether a determiner isrequired in the observed context.
If it is required,another classifier (C2 in Figure 1) estimates whichone.
The same approach is employed for the prepo-sition error correction task (classifiers C3 and C4 inFigure 1).
The details on how the classifiers wereimplemented are highlighted in Section 4.2.3http://aspell.net/4MBSP is a text analysis system based on the TiMBL andMBT memory based learning applications developed at CLiPSand ILK (Daelemans and van den Bosch, 2005).265Figure 1: System architecture.4.1.3 Correction ValidationOur error correction system implements a correc-tion validation mechanism as proposed in (Gamon etal., 2008).
The validation mechanism makes use ofa language model that is derived from a large corpusof English.
We use a trigram language model trainedon the English Gigaword corpus with a 64K-wordvocabulary (using interpolated Kneser-Ney smooth-ing with a bigram cutoff of 3 and trigram cutoff of5).The language model serves to increase the pre-cision at the cost of recall as false positives can beconfusing for learners for English.
The original sen-tence and the error-corrected version are passed tothe language model.
Only if the difference in proba-bility of being generated by the language model ex-ceeds a heuristic threshold (estimated using a tuningset) is the correction finally accepted.4.2 Machine Learning ClassifiersAs already mentioned, the system employs four ma-chine learning classifiers in total (C1?C4 ?
two foreach family of errors).
Classifiers C1 and C3 re-spectively estimate the presence of determiners andprepositions in the observed context.
If one is ex-pected, the second set of classifiers estimates whichone is the most likely.For the determiner choice classifier (C2), we re-strict the determiner choice class values to the indef-inite and definite articles: a/an and the.
The prepo-sition choice class values for the preposition choiceclassifier (C4) are restricted to set of the following10 common prepositions: on, in, at, for, of, about,from, to, by, with and (other).All the classifiers are implemented by discrimina-tive maximum entropy classification models (ME)(Ratnaparkhi, 1998).
Such models have been proveneffective for a number of natural language process-ing tasks by combining heterogeneous forms of evi-dence (Ratnaparkhi, 2010).Training Classifiers and Inference As traininginstances we consider each noun phrase (NP) in ev-ery sentence of the training data.
For the binary clas-sifiers (C1 and C3), a positive example is a nounphrase that follows a determiner/preposition, and anegative example is one that does not.
The multi-class classifiers (C2 and C4) are trained respectivelyto distinguish specific instances of determiners (defi-nite and indefinite for C2) and the set of prepositionsmentioned above.
For each classifier, a training in-stance is represented by the following features:?
Tokens in NP.?
Tokens?
POS tags in NP.?
Tokens?
lemmas in NP.?
Tokens in a contextual window of 3 tokens tothe left and to the right from the potential cor-rection position.?
Tokens?
POS tags in a contextual window of 3tokens from the potential correction position.?
Tokens?
lemmas in a contextual window of 3tokens from the potential correction position.?
Trigrams of concatenated tokens before and af-ter NP.266?
Trigrams of concatenated tokens?
POS tags be-fore and after NP.?
Trigrams of concatenated tokens?
lemmas be-fore and after NP.?
Head noun in NP.?
POS tag of head noun in NP.?
Lemma of head noun in NP.Once the classification models have been derived,the classifiers are ready to be employed in the sys-tem.
For the text correction task, each sentenceundergoes the same preprocessing analysis as de-scribed in Section 4.1.1.
Then, for each noun phrasein the input sentence, we extract the feature con-text, and use the models to predict the need forthe presence of a determiner or preposition, and ifso, which one.
Our system only accepts classifierpredictions if they are obtained with a high confi-dence.
The confidence thresholds were empiricallyestimated from pre-evaluation experiments with atuning dataset (Section 5.1).4.3 Rule-based ModulesOur system also has a number of rule-based mod-ules.
The first rule-based module is in charge ofmaking the choice between a and an if the deter-miner type classifier (C2) predicts the presence ofan indefinite determiner.
The choice is determinedby a lookup in the CMU pronouncing dictionary5(a/an CMU Dictionary in Figure 1).
In this dictio-nary each word entry is mapped to one or a numberof pronunciations in the phonetic transcription codesystem Arpabet.
If the pronunciation of the wordthat follows the estimated correction position startswith a consonant, a is used; if it starts with a vowel,an is selected.The second rule-based module corrects confusionerrors of determiner-noun agreement, e.g.
this/theseand that/those (Definite Determiner in Figure 1).
Itis implemented by introducing rules with patternsbased on whether the noun was tagged as singularor plural.The third rule-based module is used to filter outunnecessary corrections proposed by the classifiers5http://www.speech.cs.cmu.edu/cgi-bin/cmudict(C1-C4) and augmented by the already describedrule-based modules.
Each correction is examinedagainst the input text and if it yields a different textthan the original input text, such a correction is con-sidered as a necessary correction.However, sometimes automatically proposed cor-rections have to be rejected because they are out ofscope of the addressed errors.
We do not replacepossessive determiners such as my, your, his, our,their by the definite article the.
Similarly, someprepositions can be grouped in opposite pairs, forexample from and to, for which we do not proposeany correction as it requires a deep semantic analysisof text.5 Experiments and ResultsIn this section we describe the pre-evaluation exper-iments and the results of the final evaluation on theHOO-2012 test set.
Table 2 shows the characteris-tics of the datasets used in the experiments.Dataset Sentences TokensHOO training 21925 340693HOO tuning 2560 40966HOO held-out 2749 42325Reuters 207083 5487021Wikipedia 53370 1430428HOO test 1376 20606Table 2: Datasets used.5.1 Pre-Evaluation ExperimentsIn the course of system development, we split thefiles in the HOO development dataset into a train-ing set (80%), a tuning set (10%) and a held-out testset (10%).
From the beginning it was clear that theprovided development dataset alne was too small toaddress the automated error correction tasks by em-ploying machine learning classification techniques.Additionally to that dataset, we used a set of Reutersnews data and the Wikipedia corpus for training theclassifiers.Once the classification models had been derived,the system was evaluated on the tuning data and ad-justed in order to increase the overall performance.267After that, the system was evaluated on the held-outtest set for which the results are shown in Table 3.Type Precision Recall F1-scoreDet 64.11 14.89 24.17Prep 52.32 16.38 25.32All 60.19 15.38 24.50Table 3: Correction results on held-out test set.5.2 Final System Configuration and EvaluationResultsFor the final evaluation, we retrained the models us-ing the complete HOO development data (again, inaddition to the Reuters and Wikipedia corpus men-tioned above).
The number of training instances areshown in Table 4.Classifier # Training instancesC1 1746128C2 530885C3 1763784C4 706775Table 4: Number of training instances used for theME models.In the HOO framework, precision and recall areweighted equally.
However, in the domain of errorcorrection for non-native writers, precision is prob-ably more important because false positives can bevery confusing and demotivating for learners of En-glish.
For this reason, we submitted two differentruns which also gave us insights into the impact ofthe language model.
?Run 0?
denotes the system ex-cluding the language model and using lower thresh-olds, such that neither precision nor recall is favoredin particular, while ?Run 1?
focuses on precisionby using the language model as a filter, and havinghigher thresholds.
Thus, we present the results fortwo different runs on the final HOO test set, bothbefore and after manual revision (see Section 2.2).Table 5 presents the results for recognition and Ta-ble 6 those for correction.The difficulty of the HOO 2012 Shared Task isreflected by rather low system performance levels(Dale et al, 2012).
Nonetheless, we observed someinteresting patterns.
In terms of the overall systemperformance, our system achieved better results fordeterminer errors than for preposition errors.With respect to determiners, missing determinersare handled best by our system, while unnecessarydeterminers and replacement errors are more diffi-cult.
Concerning prepositions, missing prepositionsare found to be the most challenging.
This confirmsthe difficulty of choosing the right preposition due tothe large number of possible alternatives, and theirsometimes subtle differences in usage and meaning.While ?Run 1?
achieved a higher precision (at thecost of recall), ?Run 0?
performed better in terms ofoverall performance (F1-score).
This result can beexplained by the relative small size and limited tun-ing of the language model.
Moreover, it also showsthat the use of the F1-score might not be the mostinformative evaluation metric in this context.6 ConclusionsDeterminers and prepositions present real chal-lenges for non-native English writers.
For auto-mated determiner and preposition error correctionin HOO-2012, we implemented a hybrid systemthat combines statistical machine learning classifiersand a rule-based system.
By employing a languagemodel for correction validation, the system achieveda precision of 42.16%, recall of 9.49% and F1-scoreof 15.50%.
Without the language model, a preci-sion of 31.15%, recall of 22.08% and F1-score of25.84% were reached, and our system was rankedthird in terms of F1-score.Three major bottlenecks were identified in the im-plementation: (i) spelling errors should first be cor-rected due to the noisy input texts; (ii) classifierthresholds must be carefully adjusted to minimizefalse positives; and (iii) overall, preposition errorsare handled worse than determiner errors, althoughthere is also a large difference among the various er-ror types.For future work, we will focus on models that ex-plicitly utilize the writer?s background.
Also, a fullevaluation of the system should include a thoroughuser-centric study with evaluation criteria and met-rics beyond the traditional precision, recall and F -score.268Type Precision Recall F1-scoreRD 17.95 17.95 17.95MD 60.76 38.40 47.06UD 22.67 32.08 26.56Det 37.31 33.18 35.12RT 55.88 13.97 22.35MT 50.00 5.26 9.52UT 14.77 30.23 19.85Prep 27.34 14.83 19.23All 33.33 23.62 27.65(a) Run 0 (before revision)Type Precision Recall F1-scoreRD 19.44 17.95 18.67MD 65.82 39.69 49.52UD 26.67 32.26 29.20Det 40.93 34.50 37.44RT 61.76 14.09 22.95MT 50.00 5.36 9.68UT 15.91 35.90 22.05Prep 29.69 15.57 20.43All 29.47 24.74 29.47(b) Run 0 (after revision).Type Precision Recall F1-scoreRD 37.50 7.69 12.77MD 66.67 12.80 21.48UD 16.67 1.89 3.39Det 52.63 9.22 15.69RT 51.61 11.76 19.16MT 40.00 3.51 6.45UT 32.14 20.93 25.35Prep 42.19 11.44 18.00All 46.08 10.38 16.94(c) Run 1 (before revision).Type Precision Recall F1-scoreRD 37.50 8.33 13.64MD 79.17 14.50 24.52UD 33.33 3.23 5.88Det 63.16 10.48 17.98RT 54.84 11.41 18.89MT 40.00 3.57 6.56UT 35.71 25.64 29.85Prep 45.31 11.89 18.83All 51.96 11.21 18.43(d) Run 1 (after revision).Table 5: Recognition results of the runs on the test set.269Type Precision Recall F1-scoreRD 17.95 17.95 17.95MD 54.43 34.40 42.16UD 22.67 32.08 26.56Det 34.72 30.88 32.68RT 50.00 12.50 20.00MT 50.00 5.26 9.52UT 14.77 30.23 19.85Prep 25.78 13.98 18.13All 31.15 22.08 25.84(a) Run 0 (before revision)Type Precision Recall F1-scoreRD 17.95 19.44 18.67MD 59.49 35.88 44.76UD 26.67 32.26 29.20Det 38.34 32.31 35.07RT 55.88 12.75 20.77MT 50.00 5.36 9.68UT 15.91 35.90 22.05Prep 28.13 14.81 19.41All 34.27 23.26 27.71(b) Run 0 (after revision).Type Precision Recall F1-scoreRD 37.50 7.69 12.77MD 62.50 12.00 20.13UD 16.67 1.89 3.39Det 50.00 8.76 14.90RT 41.94 9.56 15.57MT 40.00 3.51 6.45UT 32.14 20.93 25.35Prep 37.50 10.17 16.00All 42.16 9.49 15.50(c) Run 1 (before revision).Type Precision Recall F1-scoreRD 37.50 8.33 13.64MD 75.00 13.74 23.23UD 33.33 3.23 5.88Det 60.05 10.04 17.23RT 45.16 9.40 15.56MT 40.00 3.57 6.56UT 35.71 25.64 29.85Prep 40.63 10.66 16.88All 48.04 10.36 17.04(d) Run 1 (after revision).Table 6: Correction results of the runs on the test set.270ReferencesJohn Bitchener, Stuart Young, and Denise Cameron.2005.
The effect of different types of corrective feed-back on ESL student writing.
Journal of Second Lan-guage Writing, 14:191?205.Walter Daelemans and Antal van den Bosch.
2005.Memory-Based Language Processing.
Studies inNatural Language Processing.
Cambridge UniversityPress.Robert Dale and Adam Kilgarriff.
2010.
Helping OurOwn: Text massaging for computational linguisticsas a new shared task.
In Proceedings of the 6th In-ternational Natural Language Generation Conference,pages 261?266, Dublin, Ireland, 7?9 July 2010.Robert Dale and Adam Kilgarriff.
2011.
Helping OurOwn: The HOO 2011 pilot shared task.
In Pro-ceedings of the 13th European Workshop on NaturalLanguage Generation, pages 242?249, Nancy, France,28?30 September 2011.Robert Dale and George Narroway.
2012.
A frame-work for evaluating text correction.
In Proceedings ofthe Eight International Conference on Language Re-sources and Evaluation, Istanbul, Turkey, 21?27 May2012.Robert Dale, Ilya Anisimoff, and George Narroway.2012.
HOO 2012: A report on the preposition and de-terminer error correction shared task.
In Proceedingsof the Seventh Workshop on Innovative Use of NLP forBuilding Educational Applications, Montreal, Canada,3?8 June 2012.Rachele De Felice and Stephen G. Pulman.
2007.
Au-tomatically acquiring models of preposition use.
InProceedings of the Fourth ACL-SIGSEM Workshop onPrepositions, pages 45?50, Prague, Czech Republic,28 June 2007.Rachele De Felice and Stephen G. Pulman.
2008.A classifier-based approach to preposition and deter-miner error correction in L2 English.
In Proceedingsof the 22nd International Conference on Computa-tional Linguistics, pages 169?176, Manchester, UnitedKingdom, 18?22 August 2008.Tom De Smedt, Vincent Van Asch, and Walter Daele-mans.
2010.
Memory-based shallow parser forPython.
CLiPS Technical Report Series (CTRS), 2.Michael Gamon, Lucy Vanderwende, Jianfeng Gao,Chris Brockett, Alexandre Klementiev, William B.Dolan, and Dmitriy Belenko.
2008.
Using contex-tual speller techniques and language modeling for ESLerror correction.
In Proceedings of the InternationalJoint Conference on Natural Language Processing,pages 449?456, Hyderabad, India, 7?12 January 2008.Jila Ghomeshi, Paul Ileana, and Martina Wiltschko.2009.
Determiners: Universals and Variation.
Lin-guistik Aktuell/Linguistics Today.
John BenjaminsPublishing Company.Na-Rae Han, Martin Chodorow, and Claudia Leacock.2004.
Detecting errors in English article usage witha maximum entropy classifier trained on a large, di-verse corpus.
In Proceedings of the 4th InternationalConference on Language Resources and Evaluation,Lisbon, Portugal, 26?28 May 2004.Na-Rae Han, Joel Tetreault, Soo-Hwa Lee, and Jin-Young Ha.
2010.
Using an error-annotated learnercorpus to develop an ESL/EFL error correction sys-tem.
In Proceedings of the Seventh International Con-ference on Language Resources and Evaluation, Val-letta, Malta, 19?21 May 2010.Kevin Knight and Ishwar Chander.
1994.
Automaticpostediting of documents.
In Proceedings of the 12thNational Conference on Artificial Intelligence, pages779?784, Seattle, Washington, USA, 31 July?4 Au-gust 1994.Guido Minnen, Francis Bond, and Ann Copestake.
2000.Memory-based learning for article generation.
In Pro-ceedings of the 4th Conference on Computational Nat-ural Language Learning and the Second LearningLanguage in Logic Workshop, pages 43?48, Lisbon,Portugal, 13?14 September 2000.Diane Nicholls.
2003.
The Cambridge LearnerCorpus?error coding and analysis for lexicographyand ELT.
In Proceedings of the Corpus Linguis-tics 2003 Conference, pages 572?581, Lancaster, UK,29 March?2 April 2003.Adwait Ratnaparkhi.
1998.
Maximum entropy modelsfor natural language ambiguity resolution.
Ph.D. the-sis, Philadelphia, PA, USA.
AAI9840230.Adwait Ratnaparkhi.
2010.
Maximum entropy modelsfor natural language processing.
In Encyclopedia ofMachine Learning, pages 647?651.Kristina Toutanova and Hisami Suzuki.
2007.
Gener-ating case markers in machine translation.
In HumanLanguage Technology Conference of the North Ameri-can Chapter of the Association of Computational Lin-guistics, pages 49?56, Rochester, New York, USA,22?27 April 2007.Helen Yannakoudakis, Ted Briscoe, and Ben Medlock.2011.
A new dataset and method for automaticallygrading ESOL texts.
In Proceedings of the 49th An-nual Meeting of the Association for ComputationalLinguistics: Human Language Technologies, pages180?189, Portland, Oregon, USA, 19?24 June 2011.Xing Yi, Jianfeng Gao, and William B. Dolan.
2008.
Aweb-based English proofing system for English as asecond language users.
In Proceedings of the ThirdInternational Join Conference on Natural LanguageProcessing, Hyderabad, India, 7?12 January 2008.271
