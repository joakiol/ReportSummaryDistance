Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 253?258,Baltimore, Maryland, USA, June 23-25 2014. c?2014 Association for Computational LinguisticsChinese Morphological Analysis with Character-level POS TaggingMo Shen?, Hongxiao Liu?, Daisuke Kawahara?, and Sadao Kurohashi?
?Graduate School of Informatics, Kyoto University, Japan?School of Computer Science, Fudan University, Chinashen@nlp.ist.i.kyoto-u.ac.jp {dk,kuro}@i.kyoto-u.ac.jp12210240027@fudan.edu.cnAbstractThe focus of recent studies on Chinese wordsegmentation, part-of-speech (POS) taggingand parsing has been shifting from words tocharacters.
However, existing methods havenot yet fully utilized the potentials of Chinesecharacters.
In this paper, we investigate theusefulness of character-level part-of-speechin the task of Chinese morphological analysis.We propose the first tagset designed for thetask of character-level POS tagging.
We pro-pose a method that performs character-levelPOS tagging jointly with word segmentationand word-level POS tagging.
Through exper-iments, we demonstrate that by introducingcharacter-level POS information, the perfor-mance of a baseline morphological analyzercan be significantly improved.1 IntroductionIn recent years, the focus of research on Chineseword segmentation, part-of-speech (POS) tag-ging and parsing has been shifting from wordstoward characters.
Character-based methodshave shown superior performance in these taskscompared to traditional word-based methods (Ngand Low, 2004; Nakagawa, 2004; Zhao et al,2006; Kruengkrai et al, 2009; Xue, 2003; Sun,2010).
Studies investigating the morphological-level and character-level internal structures ofwords, which treat character as the true atom ofmorphological and syntactic processing, havedemonstrated encouraging results (Li, 2011; Liand Zhou, 2012; Zhang et al, 2013).
This line ofresearch has provided great insight in revealingthe roles of characters in word formation andsyntax of Chinese language.However, existing methods have not yet fullyutilized the potentials of Chinese characters.While Li (2011) pointed out that some charactersCharacter-levelPart-of-SpeechExamples of Verbverb + noun ??
(invest : throw + wealth)noun + verb ??
(feel sorry : heart + hurt)verb + adjective??
(realize : recognize +clear)adjective + verb ??
(hate : pain + hate)verb + verb??
(inspect : examine + re-view)Table 1.
Character-level POS sequence as amore specified version of word-level POS: anexample of verb.can productively form new words by attaching toexisting words, these characters consist only aportion of all Chinese characters and appear in35% of the words in Chinese Treebank 5.0(CTB5) (Xue et al, 2005).
Zhang (2013) tookone step further by investigating the character-level structures of words; however, the machinelearning of inferring these internal structures re-lies on the character forms, which still suffersfrom data sparseness.In our view, since each Chinese character is infact created as a word in origin with completeand independent meaning, it should be treated asthe actual minimal morphological unit in Chineselanguage, and therefore should carry specificpart-of-speech.
For example, the character ???
(beat) is a verb and the character ???
(broken) isan adjective.
A word on the other hand, is eithersingle-character, or a compound formed by sin-gle-character words.
For example, the verb ????
(break) can be seen as a compound formedby the two single-character words with the con-struction ?verb + adjective?.Under this treatment, we observe that wordswith the same construction in terms of character-level POS tend to also have similar syntacticroles.
For example, the words having the con-253struction ?verb + adjective?
are typically verbs,and those having the construction ?adjective +noun?
are typically nouns, as shown in the fol-lowing examples:(a) verb : verb + adjective????
(break) : ???
(beat) + ???(broken)????
(update) : ???
(replace) + ???(new)????
(bleach) : ???
(wash) + ???
(white)(b) noun : adjective + noun????
(theme) : ???
(main) + ???(topic)????
(newcomer) : ???
(new) + ???(person)????
(express) : ???
(fast) + ???
(car)This suggests that character-level POS can beused as cues in predicting the part-of-speech ofunknown words.Another advantage of character-level POS isthat, the sequence of character-level POS in aword can be seen as a more fine-grained versionof word-level POS.
An example is shown in Ta-ble 1.
The five words in this table are very likelyto be tagged with the same word-level POS asverb in any available annotated corpora, while itcan be commonly agreed among native speakersof Chinese that the syntactic behaviors of thesewords are different from each other, due to theirdistinctions in word constructions.
For example,verbs having the construction ?verb + noun?
(e.g.??)
or ?verb + verb?
(e.g.
??)
can also benouns in some context, while others cannot; Andverbs having the constructions ?verb + adjective?(e.g.
??)
require exact one object argument,while others generally do not.
Therefore, com-pared to word-level POS, the character-levelPOS can produce information for more expres-sive features during the learning process of amorphological analyzer.In this paper, we investigate the usefulness ofcharacter-level POS in the task of Chinese mor-phological analysis.
We propose the first tagsetdesigned for the task of character-level POS tag-ging, based on which we manually annotate theentire CTB5.
We propose a method that performscharacter-level POS tagging jointly with wordsegmentation and word-level POS tagging.Through experiments, we demonstrate that byintroducing character-level POS information, theperformance of a baseline morphological analyz-er can be significantly improved.Tag Part-of-Speech Examplen noun ?
?/NN (bill)v verb ?
?/VV (publish)j adj./adv.
?
?/VA (vast)t numerical ???
?/CD (3.14)m quantifier ?/CD ?/M (a piece of)d date ??
?/NT (1995)k proper noun ?
?/NR (sino-US)b prefix ??
?/NN (vice mayor)e suffix??
?/NN (constructioninductry)r transliteration ???
?/NR (?rp?d)u punctuation??????
?/NR(Charles Dickens)f foreign chars X?
?/NN (X-ray)o onomatopoeia ?
?/AD (rumble)s surname??
?/NR (WangXinmin)p pronoun ?
?/PN (they)c other functional ?
?/VV (be used for)Table 2.
Tagset for character-level part-of-speech tagging.
The underlined characters inthe examples correspond to the tags on theleft-most column.
The CTB-style word-levelPOS are also shown for the examples.2 Character-level POS TagsetWe propose a tagset for the task of character-level POS tagging.
This tagset contains 16 tags,as illustrated in Table 2.
The tagset is designedby treating each Chinese character as a single-character word, and each (multi-character) wordas a phrase of single-character words.
Some ofthese tags are directly derived from the common-ly accepted word-level part-of-speech, such asnoun, verb, adjective and adverb.
It should benoted that, for single-character words, the differ-ence between adjective and adverb can almost beignored, because for any of such words that canbe used as an adjective, it usually can also beused as an adverb.
Therefore, we have mergedthese two tags into one.On the other hand, some other tags are de-signed specifically for characters, such as trans-literation, surname, prefix and suffix.
Unlikesome Asian languages such as Japanese, there isno explicit character set in Chinese that are usedexclusively for expressing names of foreign per-sons, places or organizations.
However, somecharacters are used much more frequently thanothers in these situations.
For example, in theperson?s name ??????
(?rp?d), all the fourcharacters can be frequently observed in words254Figure 1.
A Word-character hybrid lattice of a Chinese sentence.
Correct path is represented by bluebold lines.Word Length 1 2 3 4 5 6 7 or moreTags S BE BB2E BB2B3E BB2B3ME BB2B3MME BB2B3M...METable 3.
Word representation with a 6-tag tagset: S, B, B2, B3, M, Eof transliterations.
Similarly, surnames in Chi-nese are also drawn from a set of limited numberof characters.
We therefore assign specific tagsfor this kind of character sets.
The tags for pre-fixes and suffixes are motivated by the previousstudies (Li, 2011; Li and Zhou, 2012).We have annotated character-level POS for allwords in CTB5 1 .
Fortunately, character-levelPOS in most words are independent of context,which means it is sufficient to annotate wordforms unless there is an ambiguity.
The annota-tion was conducted by two persons, where eachone of them was responsible for about 70% ofthe documents in the corpus.
The redundancywas set for the purposes of style unification andquality control, on which we find that the inter-annotator agreement is 96.2%.
Although the an-notation also includes the test set, we blind thisportion in all the experiments.1 http://nlp.ist.i.kyoto-u.ac.jp/EN/index.php?CharPosCN3 Chinese Morphological Analysis withCharacter-level POS3.1 System DescriptionPrevious studies have shown that jointly pro-cessing word segmentation and POS tagging ispreferable to pipeline processing, which canpropagate errors (Nakagawa and Uchimoto, 2007;Kruengkrai et al, 2009).
Based on these studies,we propose a word-character hybrid modelwhich can also utilize the character-level POSinformation.
This hybrid model constructs a lat-tice that consists of word-level and character-level nodes from a given input sentence.
Word-level nodes correspond to words found in thesystem?s lexicon, which has been compiled fromtraining data.
Character-level nodes have specialtags called position-of-character (POC) that indi-cate the word-internal position (Asahara, 2003;Nakagawa, 2004).
We have adopted the 6-tagtagset, which (Zhao et al, 2006) reported to beoptimal.
This tagset is illustrated in Table 3.Figure 2 shows an example of a lattice for theChinese sentence: ?????????
(ChenDeming answers to journalists?
questions).
Thecorrect path is marked with blue bold lines.
The255Category Template ConditionBaseline-unigram ?
?
?
?
?
?
?
?
?
?
?
?
?
??
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?Baseline-bigram ?
?
?
?
?
?
?
?
?
?
?
??
?
?
?
?
?
?
?
?
??
?
?
?
?
?
?
??
?
?
?
?
??
?
?
?
?
?
?
?
?
?
?
?
?
??
?
OtherwiseProposed-unigram ?
?Proposed-bigram ?
?
?
??
?
?
??
?
?
??
?
?
??
?
?
??
?
?
??
?
?
?
?
?Table 4.
Feature templates.
The ?Condition?
column describes when to apply the templates:and   denote the previous and the current word-level node;     and    denote the previous andthe current character-level node;     and    denote the previous and the current node of anytypes.
Word-level nodes represent known words that can be found in the system?s lexicon.upper part of the lattice (word-level nodes) rep-resents known words, where each node carriesinformation such as character form, character-level POS , and word-level POS.
A word thatcontains multiple characters is represented by asub-lattice (the dashed rectangle in the figure),where a path stands for a possible sequence ofcharacter-level POS for this word.
For example,the word ????
(journalist) has two possiblepaths of character-level POS: ?verb + suffix?
and?noun + suffix?.
Nodes that are inside a sub-lattice cannot be linked to nodes that are outside,except from the boundaries.
The lower part ofthe lattice (character-level nodes) represents un-known words, where each node carries a posi-tion-of-character tag, in addition to other types ofinformation that can also be found on a word-level node.
A sequence of character-level nodesare considered as an unknown word if and only ifthe sequence of POC tags forms one of the caseslisted in Table 3.
This table also illustrates thepermitted transitions between adjacent character-level nodes.
We use the standard dynamic pro-gramming technique to search for the best path inthe lattice.
We use the averaged perceptron (Col-lins, 2002), an efficient online learning algorithm,to train the model.3.2 FeaturesWe show the feature templates of our model inTable 4.
The features consist of two categories:baseline features, which are modified from thetemplates proposed in (Kruengkrai et al, 2009);and proposed features, which encode character-level POS information.Baseline features: For word-level nodes thatrepresent known words, we use the symbols  ,and   to denote the word form, POS tag andlength of the word, respectively.
The functionsand        return the first and lastcharacter of  .
If   has only one character, weomit the templates that contain          or.
We use the subscript indices 0 and -1 toindicate the current node and the previous nodeduring a Viterbi search, respectively.
For charac-ter-level nodes,   denotes the surface character,and   denotes the combination of POS and POC(position-of-character) tags.Proposed features: For word-level nodes, thefunction           returns the pair of the char-acter-level POS tags of the first and last charac-ters of  , and          returns the sequence ofcharacter-level POS tags of .
If either the pairor the sequence of character-level POS is ambig-uous, which means there are multiple paths in thesub-lattice of the word-level node, then the val-ues on the current best path (with local context)during the Viterbi search will be returned.
Ifhas only one character, we omit the templatesthat contain          .
For character-level nodes,the function       returns its character-levelPOS.
The subscript indices 0 and -1 as well as256other symbols stand for the same meaning asthey are in the baseline features.4 Evaluation4.1 SettingsTo evaluate our proposed method, we have con-ducted two sets of experiments on CTB5: wordsegmentation, and joint word segmentation andword-level POS tagging.
We have adopted thesame data division as in (Jiang et al, 2008a;Jiang et al, 2008b; Kruengkrai et al, 2009;Zhang and Clark, 2010; Sun, 2011): the trainingset, dev set and test set have 18,089, 350 and 348sentences, respectively.
The models applied onall test sets are those that result in the best per-formance on the CTB5 dev set.We have annotated character-level POS in-formation for all 508,768 word tokens in CTB5.As mentioned in section 2, we blind the annota-tion in the test set in all the experiments.
To learnthe characteristics of unknown words, we builtthe system?s lexicon using only the words in thetraining data that appear at least 3 times.
We ap-plied a similar strategy in building the lexicon forcharacter-level POS, where the threshold wechoose is 2.
These thresholds were tuned usingthe development data.We have used precision, recall and the F-scoreto measure the performance of the systems.
Pre-cision ( ) is defined as the percentage of outputtokens that are consistent with the gold standardtest data, and recall ( ) is the percentage of to-kens in the gold standard test data that are recog-nized in the output.
The balanced F-score ( ) isdefined as.4.2 Experimental ResultsWe compare the performance between a baselinemodel and our proposed approach.
The results ofthe word segmentation experiment and the jointexperiment of segmentation and POS tagging areshown in Table 5(a) and Table 5(b), respectively.Each row in these tables shows the performanceof the corresponding system.
?CharPos?
standsfor our proposed model which has been de-scribed in section 3.
?Baseline?
stands for thesame model except it only enables features fromthe baseline templates.The results show that, while the differencesbetween the baseline model and the proposedmodel in word segmentation accuracies are small,the proposed model achieves significant im-provement in the experiment of joint segmentati-(a) Word Segmentation ResultsSystem P R FBaseline 97.48 98.44 97.96CharPOS 97.55 98.51 98.03(b) Joint Segmentation and POS Tagging ResultsSystem P R FBaseline 93.01 93.95 93.48CharPOS 93.42 94.18 93.80Table 5.
Experimental results on CTB5.System Segmentation JointBaseline 97.96 93.48CharPOS 98.03 93.80Jiang2008a 97.85 93.41Jiang2008b 97.74 93.37Kruengkrai2009 97.87 93.67Zhang2010 97.78 93.67Sun2011 98.17 94.02Table 6.
Comparison with previous studies onCTB5.on and POS tagging2.
This suggests that our pro-posed method is particularly effective in predict-ing the word-level POS, which is consistent withour observations mentioned in section 1.In Table 6 we compare our approach withmorphological analyzers in previous studies.
Theaccuracies of the systems in previous work aredirectly taken from the original paper.
As theresults show, despite the fact that the perfor-mance of our baseline model is relatively weakin the joint segmentation and POS tagging task,our proposed model achieves the second-bestperformance in both segmentation and joint tasks.5 ConclusionWe believe that by treating characters as the trueatoms of Chinese morphological and syntacticanalysis, it is possible to address the out-of-vocabulary problem that word-based methodshave been long suffered from.
In our error analy-sis, we believe that by exploring the character-level POS and the internal word structure (Zhanget al, 2013) at the same time, it is possible tofurther improve the performance of morphologi-cal analysis and parsing.
We will address theseissues in our future work.2        in McNemar?s test.257ReferenceMasayuki Asahara.
2003.
Corpus-based JapaneseMorphological Analysis.
Nara Institute of Scienceand Technology, Doctor?s Thesis.Michael Collins.
2002.
Discriminative TrainingMethods for Hidden Markov Models: Theory andExperiments with Perceptron Algorithms.
In Pro-ceedings of EMNLP, pages 1?8.Wenbin Jiang, Liang Huang, Qun Liu, and Yajuan L?.2008a.
A Cascaded Linear Model for Joint ChineseWord Segmentation and Part-of-speech Tagging.In Proceedings of ACL.Wenbin Jiang, Haitao Mi, and Qun Liu.
2008b.
WordLattice Reranking for Chinese Word Segmentationand Part-of-speech Tagging.
In Proceedings of COL-ING.Canasai Kruengkrai, Kiyotaka Uchimoto, Jun?ichiKazama, YiouWang, Kentaro Torisawa, and Hi-toshi Isahara.
2009.
An Error-Driven Word-Character Hybird Model for Joint Chinese WordSegmentation and POS Tagging.
In Proceedings ofACL-IJCNLP, pages 513-521.Zhongguo Li.
2011.
Parsing the Internal Structure ofWords: A New Paradigm for Chinese Word Seg-mentation.
In Proceedings of ACL-HLT, pages1405?1414.Zhongguo Li and Guodong Zhou.
2012.
Unified De-pendency Parsing of Chinese Morphological andSyntactic Structures.
In Proceedings of EMNLP,pages 1445?1454.Hwee Tou Ng and Jin Kiat Low.
2004.
Chinese Part-of-speech Tagging: One-at-a-time or All-at-once?Word-based or Character-based?
In Proceedings ofEMNLP, pages 277?284.Tetsuji Nakagawa.
2004.
Chinese and japanese wordsegmentation using word-level and character-levelinformation.
In Proceedings of COLING, pages466?472.Tetsuji Nakagawa and Kiyotaka Uchimoto.
2007.Hybrid Approach to Word Segmentation and PosTagging.
In Proceedings of ACL Demo and PosterSessions, pages 217-220.Weiwei Sun.
2010.
Word-based and Character-basedWord Segmentation Models: Comparison andCombination.
In Proceedings of COLING PosterSessions, pages 1211?1219.Weiwei Sun.
2011.
A Stacked Sub-word Model forJoint Chinese Word Segmentation and Part-of-speech Tagging.
In Proceedings of ACL-HLT,pages 1385?1394.Nianwen Xue.
2003.
Chinese Word Segmentation asCharacter Tagging.
In International Journal ofComputational Linguistics and Chinese LanguageProcessing.Nianwen Xue, Fei Xia, Fu-Dong Chiou, and MarthaPalmer.
2005.
The Penn Chinese Treebank: PhraseStructure Annotation of a Large Corpus.
NaturalLanguage Engineering, 11(2):207?238.Hai Zhao, Chang-Ning Huang, Mu Li, and Bao-LiangLu.
2006.
Effective Tag Set Selection in ChineseWord Segmentation via Conditional Random FieldModeling.
In Proceedings of PACLIC, pages 87-94.Meishan Zhang, Yue Zhang, Wanxiang Che, and TingLiu.
2013.
Chinese Parsing Exploiting Characters.In Proceedings of ACL, page 125-134.Yue Zhang and Stephen Clark.
2010.
A Fast Decoderfor Joint Word Segmentation and POS-tagging Us-ing a Single Discriminative Model.
In Proceedingsof EMNLP, pages 843?852.258
