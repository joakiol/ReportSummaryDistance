Proceedings of the 12th Conference of the European Chapter of the ACL, pages 1?9,Athens, Greece, 30 March ?
3 April 2009. c?2009 Association for Computational LinguisticsSlacker semantics: why superficiality, dependency and avoidance ofcommitment can be the right way to goAnn CopestakeComputer Laboratory, University of Cambridge15 JJ Thomson Avenue, Cambridge, UKaac@cl.cam.ac.ukAbstractThis paper discusses computational com-positional semantics from the perspectiveof grammar engineering, in the light of ex-perience with the use of Minimal Recur-sion Semantics in DELPH-IN grammars.The relationship between argument index-ation and semantic role labelling is ex-plored and a semantic dependency nota-tion (DMRS) is introduced.1 IntroductionThe aim of this paper is to discuss work on com-positional semantics from the perspective of gram-mar engineering, which I will take here as the de-velopment of (explicitly) linguistically-motivatedcomputational grammars.
The paper was writtento accompany an invited talk: it is intended to pro-vide background and further details for those partsof the talk which are not covered in previous pub-lications.
It consists of an brief introduction to ourapproach to computational compositional seman-tics, followed by details of two contrasting topicswhich illustrate the grammar engineering perspec-tive.
The first of these is argument indexing and itsrelationship to semantic role labelling, the secondis semantic dependency structure.Standard linguistic approaches to compositionalsemantics require adaptation for use in broad-coverage computational processing.
Althoughsome of the adaptations are relatively trivial, oth-ers have involved considerable experimentation byvarious groups of computational linguists.
Per-haps the most important principle is that semanticrepresentations should be a good match for syn-tax, in the sense of capturing all and only the in-formation available from syntax and productivemorphology, while nevertheless abstracting oversemantically-irrelevant idiosyncratic detail.
Com-pared to much of the linguistics literature, ouranalyses are relatively superficial, but this is essen-tially because the broad-coverage computationalapproach prevents us from over-committing on thebasis of the information available from the syntax.One reflection of this are the formal techniquesfor scope underspecification which have been de-veloped in computational linguistics.
The im-plementational perspective, especially when com-bined with a requirement that grammars can beused for generation as well as parsing, also forcesattention to details which are routinely ignored intheoretical linguistic studies.
This is particularlytrue when there are interactions between phenom-ena which are generally studied separately.
Fi-nally, our need to produce usable systems disal-lows some appeals to pragmatics, especially thosewhere analyses are radically underspecified to al-low for syntactic and morphological effects foundonly in highly marked contexts.1In a less high-minded vein, sometimes it is rightto be a slacker: life (or at least, project funding) istoo short to implement all ideas within a grammarin their full theoretical glory.
Often there is an easyalternative which conveys the necessary informa-tion to a consumer of the semantic representations.Without this, grammars would never stabilise.Here I will concentrate on discussing workwhich has used Minimal Recursion Semantics(MRS: Copestake et al (2005)) or Robust Min-imal Recursion Semantics (RMRS: Copestake(2003)).
The (R)MRS approach has been adoptedas a common framework for the DELPH-IN ini-tiative (Deep Linguistic Processing with HPSG:http://www.delph-in.net) and the work dis-cussed here has been done by and in collaborationwith researchers involved in DELPH-IN.The programme of developing computationalcompositional semantics has a large number ofaspects.
It is important that the semanticshas a logically-sound interpretation (e.g., Kollerand Lascarides (2009), Thater (2007)), is cross-1For instance, we cannot afford to underspecify numberon nouns because of examples such as The hash browns isgetting angry (from Pollard and Sag (1994) p.85).1linguistically adequate (e.g., Bender (2008)) andis compatible with generation (e.g., Carroll et al(1999), Carroll and Oepen (2005)).
Ideally, wewant support for shallow as well as deep syn-tactic analysis (which was the reason for devel-oping RMRS), enrichment by deeper analysis (in-cluding lexical semantics and anaphora resolution,both the subject of ongoing work), and (robust) in-ference.
The motivation for the development ofdependency-style representations (including De-pendency MRS (DMRS) discussed in ?4) has beento improve ease of use for consumers of the repre-sentation and human annotators, as well as use instatistical ranking of analyses/realisations (Fujitaet al (2007), Oepen and L?nning (2006)).
Inte-gration with distributional semantic techniques isalso of interest.The belated ?introduction?
to MRS in Copestakeet al (2005) primarily covered formal represen-tation of complete utterances.
Copestake (2007a)described uses of (R)MRS in applications.
Copes-take et al (2001) and Copestake (2007b) concernthe algebra for composition.
What I want to dohere is to concentrate on less abstract issues inthe syntax-semantics interface.
I will discuss twocases where the grammar engineering perspectiveis important and where there are some conclusionsabout compositional semantics which are relevantbeyond DELPH-IN.
The first, argument indexing(?3), is a relatively clear case in which the con-straints imposed by grammar engineering have asignificant effect on choice between plausible al-ternatives.
I have chosen to talk about this bothbecause of its relationship with the currently pop-ular task of semantic role labelling and becausethe DELPH-IN approach is now fairly stable af-ter a quite considerable degree of experimentation.What I am reporting is thus a perspective on workdone primarily by Flickinger within the EnglishResource Grammar (ERG: Flickinger (2000)) andby Bender in the context of the Grammar Matrix(Bender et al, 2002), though I?ve been involved inmany of the discussions.
The second main topic(?4) is new work on a semantic dependency rep-resentation which can be derived from MRS, ex-tending the previous work by Oepen (Oepen andL?nning, 2006).
Here, the motivation came froman engineering perspective, but the nature of therepresentation, and indeed the fact that it is possi-ble at all, reveals some interesting aspects of se-mantic composition in the grammars.2 The MRS and RMRS languagesThis paper concerns only representations whichare output by deep grammars, which use MRS, butit will be convenient to talk in terms of RMRS andto describe the RMRSs that are constructed underthose assumptions.
Such RMRSs are interconvert-ible with MRSs.2 The description is necessarilyterse and contains the minimal detail necessary tofollow the remainder of the paper.An RMRS is a description of a set of trees cor-responding to scoped logical forms.
Fig 1 showsan example of an RMRS and its correspondingscoped form (only one for this example).
RMRSis a ?flat?
representation, consisting of a bag of el-ementary predications (EP), a set of argumentrelations, and a set of constraints on the possi-ble linkages of the EPs when the RMRS is resolvedto scoped form.
Each EP has a predicate, a la-bel and a unique anchor and may have a distin-guished (ARG0) argument (EPs are written here aslabel:anchor:pred(arg0)).
Label sharing betweenEPs indicates conjunction (e.g., in Fig 1, big, an-gry and dog share the label l2).
Argument relationsrelate non-arg0 arguments to the corresponding EPvia the anchor.
Argument names are taken from afixed set (discussed in ?3).
Argument values maybe variables (e.g., e8, x4: variables are the onlypossibility for values of ARG0), constants (stringssuch as ?London?
), or holes (e.g.
h5), which in-dicate scopal relationships.
Variables have sortalproperties, indicating tense, number and so on, butthese are not relevant for this paper.
Variables cor-responding to unfilled (syntactically optional) ar-guments are unique in the RMRS, but otherwisevariables must correspond to the ARG0 of an EP(since I am only considering RMRSs from deepgrammars here).Constraints on possible scopal relationships be-tween EPs may be explicitly specified in the gram-mar via relationships between holes and labels.
Inparticular qeq constraints (the only type consid-ered here) indicate that, in the scoped forms, alabel must either plug a hole directly or be con-nected to it via a chain of quantifiers.
Hole argu-ments (other than the BODY of a quantifier) are al-ways linked to a label via a qeq or other constraint(in a deep grammar RMRS).
Variables survive inthe models of RMRSs (i.e., the fully scoped trees)whereas holes and labels do not.2See Flickinger and Bender (2003) and Flickinger et al(2003) for the use of MRS in DELPH-IN grammars.2l1:a1: some q, BV(a1,x4), RSTR(a1,h5), BODY(a1,h6), h5 qeq l2,l2:a2: big a 1(e8), ARG1(a2,x4), l2:a3: angry a 1(e9), ARG1(a3,x4), l2:a4: dog n 1(x4),l4:a5: bark v 1(e2), ARG1(a5,x4), l4:a6: loud a 1(e10), ARG1(a6,e2)some q(x4, big a 1(e8,x4) ?
angry a 1(e9, x4) ?
dog n 1(x4), bark v 1(e2,x4) ?
loud a 1(e10,e2))Figure 1: RMRS and scoped form for ?Some big angry dogs bark loudly?.
Tense and number are omitted.The naming convention for predicates corre-sponding to lexemes is: stem major sense tag,optionally followed by and minor sense tag (e.g.,loud a 1).
Major sense tags correspond roughlyto traditional parts of speech.
There are also non-lexical predicates such as ?poss?
(though none oc-cur in Fig 1).3 MRS varies from RMRS in that thearguments are all directly associated with the EPand thus no anchors are necessary.I have modified the definition of RMRS givenin Copestake (2007b) to make the ARG0 argumentoptional.
Here I want to add the additional con-straint that the ARG0 of an EP is unique to it (i.e.,not the ARG0 of any other EP).
I will term thisthe characteristic variable property.
This meansthat, for every variable, there is a unique EP whichhas that variable as its ARG0.
I will assume for thispaper that all EPs, apart from quantifier EPs, havesuch an ARG0.4 The characteristic variable prop-erty is one that has emerged from working withlarge-scale constraint-based grammars.A few concepts from the MRS algebra are alsonecessary to the discussion.
Composition canbe formalised as functor-argument combinationwhere the argument phrase?s hook fills a slot inthe functor phrase, thus instantiating an RMRS ar-gument relation.
The hook consists of an index(a variable), an external argument (also a vari-able) and an ltop (local top: the label correspond-ing to the topmost node in the current partial tree,ignoring quantifiers).
The syntax-semantics inter-face requires that the appropriate hook and slots beset up (mostly lexically in a DELPH-IN grammar)and that each application of a rule specifies the slotto be used (e.g., MOD for modification).
In a lex-ical entry, the ARG0 of the EP provides the hook3In fact, most of the choices about semantics made bygrammar writers concern the behaviour of constructions andthus these non-lexical predicates, but this would require an-other paper to discuss.4I am simplifying for expository convenience.
In currentDELPH-IN grammars, quantifiers have an ARG0 which corre-sponds to the bound variable.
This should not be the charac-teristic variable of the quantifier (it is the characteristic vari-able of a nominal EP), since its role in the scoped forms is asa notational convenience to avoid lambda expressions.
I willcall it the BV argument here.index, and, apart from quantifiers, the hook ltopis the EP?s label.
In intersective combination, theltops of the hooks will be equated.
In scopal com-bination, a hole argument in a slot is specified tobe qeq to the ltop of the argument phrase and theltop of the functor phrase supplies the new hook?sltop.By thinking of qeqs as links in an RMRS graph(rather than in terms of their logical behaviouras constraints on the possible scoped forms), anRMRS can be treated as consisting of a set of treeswith nodes consisting of EPs grouped via intersec-tive relationships: there will be a backbone tree(headed by the overall ltop and including the mainverb if there is one), plus a separate tree for eachquantified NP.
For instance, in Fig 1, the thirdline contains the EPs corresponding to the (singlenode) backbone tree and the first two lines showthe EPs comprising the tree for the quantified NP(one node for the quantifier and one for the N?which it connects to via the RSTR and its qeq).3 Arguments and rolesI will now turn to the representation of argumentsin MRS and their relationship to semantic roles.
Iwant to discuss the approach to argument labellingin some detail, because it is a reasonably clearcase where the desiderata for broad-coverage se-mantics which were discussed in ?1 led us to asyntactically-driven approach, as opposed to usingsemantically richer roles such as AGENT, GOALand INSTRUMENT.An MRS can, in fact, be written using a conven-tional predicate-argument representation.
A repre-sentation which uses ordered argument labels canbe recovered from this in the obvious way.
E.g.,l:like v 1(e,x,y) is equivalent to l:a:like v 1(e),ARG1(a,x), ARG2(a,y).
A fairly large inventory ofargument labels is actually used in the DELPH-INgrammars (e.g., RSTR, BODY).
To recover thesefrom the conventional predicate-argument nota-tion requires a look up in a semantic interfacecomponent (the SEM-I, Flickinger et al (2005)).But open-class predicates use the ARGn conven-tion, where n is 0,1,2,3 or 4 and the discussion here3only concerns these.5Arguably, the DELPH-IN approach is Davidso-nian rather than neo-Davidsonian in that, even inthe RMRS form, the arguments are related to thepredicate via the anchor which plays no other rolein the semantics.
Unlike the neo-Davidsonian useof the event variable to attach arguments, this al-lows the same style of representation to be useduniformly, including quantifiers, for instance.
Ar-guments can omitted completely without syntacticill-formedness of the RMRS, but this is primarilyrelevant to shallower grammars.
A semantic pred-icate, such as like v 1, is a logical predicate and assuch is expected to have the same arity wherever itoccurs in the DELPH-IN grammars.
Thus modelsfor an MRS may be defined in a language with orwithout argument labels.The ordering of arguments for open class lex-emes is lexically specified on the basis of thesyntactic obliqueness hierarchy (Pollard and Sag,1994).
ARG1 corresponds to the subject in thebase (non-passivised) form (?deep subject?).
Ar-gument numbering is consecutive in the base form,so no predicate with an ARG3 is lexically missingan ARG2, for instance.
An ARG3 may occur with-out an instantiated ARG2 when a syntactically op-tional argument is missing (e.g., Kim gave to thelibrary), but this is explicit in the linearised form(e.g., give v(e,x,u,y)).The full statement of how the obliqueness hi-erarchy (and thus the labelling) is determined forlexemes has to be made carefully and takes us toofar into discussion of syntax to explain in detailhere.
While the majority of cases are straightfor-ward, a few are not (e.g., because they dependon decisions about which form is taken as thebase in an alternation).
However, all decisions aremade at the level of lexical types: adding an en-try for a lexeme for a DELPH-IN grammar onlyrequires working out its lexical type(s) (from syn-tactic behaviour and very constrained semantic no-tions, e.g., control).
The actual assignment of ar-guments to an utterance is just a consequence ofparsing.
Argument labelling is thus quite differentfrom PropBank (Palmer et al, 2005) role labellingdespite the unfortunate similarity of the PropBanknaming scheme.It follows from the fixed arity of predicatesthat lexemes with different numbers of argu-5ARG4 occurs very rarely, at least in English (the verb betbeing perhaps the clearest case).ments should be given different predicate symbols.There is usually a clear sense distinction when thisoccurs.
For instance, we should distinguish be-tween the ?depart?
and ?bequeath?
senses of leavebecause the first takes an ARG1 and an ARG2 (op-tional) and the second ARG1, ARG2 (optional),ARG3.
We do not draw sense distinctions wherethere is no usage which the grammar could disam-biguate.Of course, there are obvious engineering rea-sons for preferring a scheme that requires mini-mal additional information in order to assign argu-ment labels.
Not only does this simplify the job ofthe grammar writer, but it makes it easier to con-struct lexical entries automatically and to integrateRMRSs derived from shallower systems.
However,grammar engineers respond to consumers: if moredetailed role labelling had a clear utility and re-quired an analysis at the syntax level, we wouldwant to do it in the grammar.
The question iswhether it is practically possible.Detailed discussion of the linguistics literaturewould be out of place here.
I will assume thatDowty (1991) is right in the assertion that thereis no small (say, less than 10) set of role labelswhich can also be used to link the predicate to itsarguments in compositionally constructed seman-tics (i.e., argument-indexing in Dowty?s terminol-ogy) such that each role label can be given a con-sistent individual semantic interpretation.
For ourpurposes, a consistent semantic interpretation in-volves entailment of one or more useful real worldpropositions (allowing for exceptions to the entail-ment for unusual individual sentences).This is not a general argument against rich rolelabels in semantics, just their use as the meansof argument-indexation.
It leaves open uses forgrammar-internal purposes, e.g., for defining andcontrolling alternations.
The earliest versions ofthe ERG experimented with a version of Davis?s(2001) approach to roles for such reasons: thiswas not continued, but for reasons irrelevant here.Roles are still routinely used for argument index-ation in linguistics papers (without semantic inter-pretation).
The case is sometimes made that moremnemonic argument labelling helps human inter-pretation of the notation.
This may be true of se-mantics papers in linguistics, which tend to con-cern groups of similar lexemes.
It is not true of acollaborative computational linguistics project inwhich broad coverage is being attempted: names4can only be mnemonic if they carry some meaningand if the meaning cannot be consistently appliedthis leads to endless trouble.What I want to show here is how problemsarise even when very limited semantic generalisa-tions are attempted about the nature of just one ortwo argument labels, when used in broad-coveragegrammars.
Take the quite reasonable idea that asemantically consistent labelling for intransitivesand related causatives is possible (cf PropBank).For instance, water might be associated with thesame argument label in the following examples:(1) Kim boiled the water.
(2) The water boiled.Using (simplified) RMRS representations, thismight amount to:(3) l:a:boil v(e), a:ARG1(k), a:ARG2(x), water(x)(4) l:a:boil v(e), a:ARG2(x), water(x)Such an approach was used for a time in the ERGwith unaccusatives.
However, it turns out to be im-possible to carry through consistently for causativealternations.Consider the following examples of gallop: 6(5) Michaela galloped the horse to the far end ofthe meadow, .
.
.
(6) With that Michaela nudged the horse with herheels and off the horse galloped.
(7) Michaela declared, ?I shall call him Lightningbecause he runs as fast as lightning.?
And withthat, off she galloped.If only a single predicate is involved, e.g., gal-lop v, and the causative has an ARG1 and anARG2, then what about the two intransitive cases?If the causative is treated as obligatorily transi-tive syntactically, then (6) and (7) presumably bothhave an ARG2 subject.
This leads to Michaelahaving a different role label in (5) and (7), de-spite the evident similarity of the real world situ-ation.
Furthermore, the role labels for intransitivemovement verbs could only be predicted by a con-sumer of the semantics who knew whether or nota causative form existed.
The causative may berare, as with gallop, where the intransitive use isclearly the base case.
Alternatively, if (7) is treated6http://www.thewestcoast.net/bobsnook/kid/horses.htm.as a causative intransitive, and thus has a subjectlabelled ARG1, there is a systematic unresolvableambiguity and the generalisation that the subjectsin both intransitive sentences are moving is lost.Gallop is an not isolated case in having a vo-litional intransitive use: it applies to most (if notall) motion verbs which undergo the causative al-ternation.
To rescue this account, we would needto apply it only to true lexical anti-causatives.
It isnot clear whether this is doable (even the standardexample sink can be used intransitively of deliber-ate movement) but from a slacker perspective, atthis point we should decide to look for an easierapproach.The current ERG captures the causative relation-ship by using systematic sense labelling:(8) Kim boiled the water.l:a:boil v cause(e), a:ARG1(k), a:ARG2(x),water(x)(9) The water boiled.l:a:boil v 1(e), a:ARG1(x), water(x)This is not perfect, but it has clear advantages.It allows inferences to be made about ARG1 andARG2 of cause verbs.
In general, inferences aboutarguments may be made with respect to particularverb classes.
This lends itself to successive refine-ment in the grammars: the decision to add a stan-dardised sense label, such as cause, does not re-quire changes to the type system, for instance.
Ifwe decide that we can identify true anti-causatives,we can easily make them a distinguished class viathis convention.
Conversely, in the situation wherecausation has not been recognised, and the verbhas been treated as a single lexeme having an op-tional ARG2, the semantics is imperfect but at leastthe imperfection is local.In fact, determining argument labelling by theobliqueness hierarchy still allows generalisationsto be made for all verbs.
Dowty (1991) arguesfor the notion of proto-agent (p-agt) and proto-patient (p-pat) as cluster concepts.
Proto-agentproperties include volitionality, sentience, causa-tion of an event and movement relative to anotherparticipant.
Proto-patient properties include be-ing causally affected and being stationary relativeto another participant.
Dowty claims that gener-alisations about which arguments are lexicalisedas subject, object and indirect object/oblique canbe expressed in terms of relative numbers of p-agtand p-pat properties.
If this is correct, then we can,5for example, predict that the ARG1 of any predi-cate in a DELPH-IN grammar will not have fewerp-agt properties than the ARG2 of that predicate.7As an extreme alternative, we could use la-bels which were individual to each predicate,such as LIKER and LIKED (e.g., Pollard and Sag(1994)).
For such role labels to have a consistentmeaning, they would have to be lexeme-specific:e.g., LEAVER1 (?departer?)
versus LEAVER2 (?be-queather?).
However this does nothing for seman-tic generalisation, blocks the use of argument la-bels in syntactic generalisations and leads to anextreme proliferation of lexical types when us-ing typed feature structure formalisms (one typewould be required per lexeme).
The labels addno additional information and could trivially beadded automatically to an RMRS if this were use-ful for human readers.
Much more interesting isthe use of richer lexical semantic generalisations,such as those employed in FrameNet (Baker et al,1998).
In principle, at least, we could (and should)systematically link the ERG to FrameNet, but thiswould be a form of semantic enrichment mediatedvia the SEM-I (cf Roa et al (2008)), and not analternative technique for argument indexation.4 Dependency MRSThe second main topic I want to address is aform of semantic dependency structure (DMRS:see wiki.delph-in.net for the evolving details).There are good engineering reasons for producinga dependency style representation with links be-tween predicates and no variables: ease of read-ability for consumers of the representation and forhuman annotators, parser comparison and integra-tion with distributional lexical semantics being theimmediate goals.
Oepen has previously producedelementary dependencies from MRSs but the pro-cedure (partially sketched in Oepen and L?nning(2006)) was not intended to produce complete rep-resentations.
It turns out that a DMRS can be con-structed which can be demonstrated to be inter-convertible with RMRS, has a simple graph struc-ture and minimises redundancy in the representa-tion.
What is surprising is that this can be donefor a particular class of grammars without mak-7Sanfilippo (1990) originally introduced Dowty?s ideasinto computational linguistics, but this relative behaviourcannot be correctly expressed simply by using p-agt and p-pat directly for argument indexation as he suggested.
It isincorrect for examples like (2) to be labelled as p-agt, sincethey have no agentive properties.ing use of the evident clues to syntax in the pred-icate names.
The characteristic variable propertydiscussed in ?2 is crucial: its availability allowsa partial replication of composition, with DMRSlinks being relatable to functor-argument combi-nations in the MRS algebra.
I should emphasizethat, unlike MRS and RMRS, DMRS is not intendedto have a direct logical interpretation.An example of a DMRS is given in Fig 2.
Linksrelate nodes corresponding to RMRS predicates.Nodes have unique identifiers, not shown here.
Di-rected link labels are of the form ARG/H, ARG/EQor ARG/NEQ, where ARG corresponds to an RMRSargument label.
H indicates a qeq relationship,EQ label equality and NEQ label inequality, as ex-plained more fully below.
Undirected /EQ arcsalso sometimes occur (see ?4.3).
The ltop is in-dicated with a *.4.1 RMRS-to-DMRSIn order to transform an RMRS into a DMRS, wewill treat the RMRS as made up of three subgraphs:Label equality graph.
Each EP in an RMRShas a label, which may be shared with any numberof other EPs.
This can be captured in DMRS viaa graph linking EPs: if this is done exhaustively,there would be n(n?
1)/2 binary non-directionallinks.
E.g., for the RMRS in Fig 1, we need to linkbig a 1, angry a 1 and dog n 1 and this takes3 links.
Obviously the effect of equality could becaptured by a smaller number of links, assumingtransitivity: but to make the RMRS-to-DMRS con-version deterministic, we need a method for se-lecting canonical links.Hole-to-label qeq graph.
A qeq in RMRS linksa hole to a label which labels a set of EPs.
Thereis thus a 1 : 1 mapping between holes and la-bels which can be converted to a 1 : n mappingbetween holes and the EPs which share the la-bel.
By taking the EP with the hole as the origin,we can construct an EP-to-EP graph, using the ar-gument name as a label for the link: of course,such links are asymmetric and thus the graph isdirected.
e.g., some q has RSTR links to each ofbig a 1, angry a 1 and dog n 1.
Reducing thisto a 1 : 1 mapping between EPs, which we wouldideally like for DMRS, requires a canonical methodof selecting a head EP from the set of target EPs (asdoes the selection of the ltop).Variable graph.
For the conversion to DMRS,we will rely on the characteristic variable prop-6some q big a 1 angry a at dog n 1 bark v 1* loud a 1-ARG1/EQffARG1/EQffARG1/NEQ-ARG1/EQ-RSTR/HFigure 2: DMRS for ?Some big angry dogs bark loudly.
?erty, that every variable has a unique EP associatedwith it via its ARG0.
Any non-hole argument of anEP will have a value which is the ARG0 of someother EP, or which is unbound (i.e., not found else-where in the RMRS) in which case we ignore it.Thus we can derive a graph between EPs, suchthat each link is labelled with an argument posi-tion and points to a unique EP.
I will talk about anEP?s ?argument EPs?, to refer to the set of EPs itsarguments point to in this graph.The three EP graphs can be combined to forma dependency structure.
But this has an excessivenumber of links due to the label equality and qeqcomponents.
We need deterministic techniques forremoving the redundancy.
These can utilise thevariable graph, since this is already minimal.The first strategy is to combine the label equal-ity and variable links when they connect the sametwo EPs.
For instance, we combine the ARG1link between big a 1, and dog n 1 with the la-bel equality link to give a link labelled ARG1/EQ.We then test the connectivity of the ARG/EQ linkson the assumption of transitivity and remove anyredundant links from the label graph.
This usuallyremoves all label equality links: one case whereit does not is discussed in ?4.3.
Variable graphlinks with no corresponding label equality are an-notated ARG/NEQ, while links arising from theqeq graph are labelled ARG/H.
This retains suf-ficient information to allow the reconstruction ofthe three graphs in DMRS-to-RMRS conversion.In order to reduce the number of links arisingfrom the qeq graph, we make use of the variablegraph to select a head from a set of EPs sharinga label.
It is not essential that there should be aunique head, but it is desirable.
The next sectionoutlines how head selection works: despite not us-ing any directly syntactic properties, it generallyrecovers the syntactic head.4.2 Head selection in the qeq graphHead selection uses one principle and one heuris-tic, both of which are motivated by the composi-tional properties of the grammar.
The principle isthat qeq links from an EP should parallel any com-parable variable links.
If an EP has two arguments,one of which is a variable argument which linksto EP?
and the other a hole argument which has avalue corresponding to a set of EPs including EP?,EP?
is chosen as the head of that set.This essentially follows from the compositionrules: in an algebra operation giving rise to a qeq,the argument phrase supplies a hook consistingof an index (normally, the ARG0 of the head EP)and an ltop (normally, the label of the head EP).Thus if a variable argument corresponds to EP?,EP?
will have been the head of the correspondingphrase and is thus the choice of head in the DMRS.This most frequently arises with quantifiers, whichhave both a BV and a RSTR argument: the RSTRargument can be taken as linking to the EP whichhas an ARG0 equal to the BV (i.e., the head of theN?).
If this principle applies, it will select a uniquehead.
In fact, in this special case, we drop the BVlink from the final DMRS because it is entirely pre-dictable from the RSTR link.In the case where there is no variable argu-ment, we use the heuristic which generally holdsin DELPH-IN grammars that the EPs which wewish to distinguish as heads in the DMRS do notshare labels with their DMRS argument EPs (incontrast to intersective modifiers, which alwaysshare labels with their argument EPs).
Heads mayshare labels with PPs which are syntactically ar-guments, but these have a semantics like PP mod-ifiers, where the head is the preposition?s EP ar-gument.
NP arguments are generally quantifiedand quantifiers scope freely.
AP, VP and S syn-tactic arguments are always scopal.
PPs which arenot modifier-like are either scopal (small clauses)or NP-like (case marking Ps) and free-scoping.Thus, somewhat counter-intuitively, we can selectthe head EP from the set of EPs which share a labelby looking for an EP which has no argument EPsin that set.4.3 Some properties of DMRSThe MRS-to-DMRS procedure deterministicallycreates a unique DMRS.
A converse DMRS-to-MRSprocedure recreates the MRS (up to label, anchor7the q dog n 1 def explicit q poss toy n 1 the q cat n 1 bite v 1 bark v 1*ffARG2/EQffARG1/NEQ-RSTR/H-RSTR/H-ARG1/NEQffARG2/NEQ-RSTR/H/EQffARG1/NEQFigure 3: DMRS for ?The dog whose toy the cat bit barked.
?and variable renaming), though requiring the SEM-I to add the uninstantiated optional arguments.I claimed above that DMRSs are an idealisa-tion of semantic composition.
A pure functor-argument application scheme would produce a treewhich could be transformed into a structure whereno dependent had more than one head.
But inDMRS the notion of functor/head is more complexas determiners and modifiers provide slots in theRMRS algebra but not the index of the result.
Com-position of a verb (or any other functor) with anNP argument gives rise to a dependency betweenthe verb and the head noun in the N?.
The headnoun provides the index of the NP?s hook in com-position, though it does not provide the ltop, whichcomes from the quantifier.
However, because thisltop is not equated with any label, there is no directlink between the verb and the determiner.
Thus thenoun will have a link from the determiner and fromthe verb.Similarly, if the constituents in compositionwere continuous, the adjacency condition wouldhold, but this does not apply because of the mech-anisms for long-distance dependencies and theavailability of the external argument in the hook.8DMRS indirectly preserves the informationabout constituent structure which is essential forsemantic interpretation, unlike some syntactic de-pendency schemes.
In particular, it retains infor-mation about a quantifier?s N?, since this forms therestrictor of the generalised quantifier (for instanceMost white cats are deaf has different truth condi-tions from Most deaf cats are white).
An inter-esting example of nominal modification is shownin Fig 3.
Notice that whose has a decomposedsemantics combining two non-lexeme predicatesdef explicit q and poss.
Unusually, the relativeclause has a gap which is not an argument of itssemantic head (it?s an argument of poss rather thanbite v 1).
This means that when the relative clause8Given that non-local effects are relatively circumscribed,it is possible to require adjacency in some parts of the DMRS.This leads to a technique for recording underspecification ofnoun compound bracketing, for instance.is combined with the gap filler, the label equalityand the argument instantiation correspond to dif-ferent EPs.
Thus there is a label equality whichcannot be combined with an argument link and hasto be represented by an undirected /EQ arc.5 Related work and conclusionHobbs (1985) described a philosophy of computa-tional compositional semantics that is in some re-spects similar to that presented here.
But, as far asI am aware, the Core Language Engine book (Al-shawi, 1992) provided the first detailed descrip-tion of a truly computational approach to com-positional semantics: in any case, Steve Pulmanprovided my own introduction to the idea.
Cur-rently, the ParGram project also undertakes large-scale multilingual grammar engineering work: seeCrouch and King (2006) and Crouch (2006) for anaccount of the semantic composition techniquesnow being used.
I am not aware of any othercurrent grammar engineering activities on the Par-Gram or DELPH-IN scale which build bidirectionalgrammars for multiple languages.Overall, what I have tried to do here is to give aflavour of how compositional semantics and syn-tax interact in computational grammars.
Analy-ses which look simple have often taken consider-able experimentation to arrive at when working ona large-scale, especially when attempting cross-linguistic generalisations.
The toy examples thatcan be given in papers like this one do no justice tothis, and I would urge readers to try out the gram-mars and software and, perhaps, to join in.AcknowledgementsParticular thanks to Emily Bender, Dan Flickingerand Alex Lascarides for detailed comments atvery short notice!
I am also grateful to manyother colleagues, especially from DELPH-IN andin the Cambridge NLIP research group.
Thiswork was supported by the Engineering and Phys-ical Sciences Research Council [grant numbersEP/C010035/1, EP/F012950/1].8ReferencesHiyan Alshawi, editor.
1992.
The Core Language En-gine.
MIT Press.Collin F. Baker, Charles J. Fillmore, and John B. Lowe.1998.
The Berkeley FrameNet project.
In Proc.ACL-98, pages 86?90, Montreal, Quebec, Canada.Association for Computational Linguistics.Emily Bender, Dan Flickinger, and Stephan Oepen.2002.
The Grammar Matrix: An open-sourcestarter-kit for the rapid development of cross-linguistically consistent broad-coverage precisiongrammars.
In Proc.
Workshop on Grammar Engi-neering and Evaluation, Coling 2002, pages 8?14,Taipei, Taiwan.Emily Bender.
2008.
Evaluating a crosslinguisticgrammar resource: A case study of Wambaya.
InProc.
ACL-08, pages 977?985, Columbus, Ohio,USA.John Carroll and Stephan Oepen.
2005.
High ef-ficiency realization for a wide-coverage unificationgrammar.
In Proc.
IJCNLP05, Springer LectureNotes in Artificial Intelligence, Volume 3651, pages165?176, Jeju Island, Korea.John Carroll, Ann Copestake, Dan Flickinger, and Vic-tor Poznanski.
1999.
An efficient chart generatorfor (semi-)lexicalist grammars.
In Proc.
7th Eu-ropean Workshop on Natural Language Generation(EWNLG?99), pages 86?95, Toulouse.Ann Copestake, Alex Lascarides, and Dan Flickinger.2001.
An algebra for semantic construction inconstraint-based grammars.
In Proc.
ACL-01,Toulouse.Ann Copestake, Dan Flickinger, Ivan A.
Sag, and CarlPollard.
2005.
Minimal Recursion Semantics: anintroduction.
Research on Language and Computa-tion, 3(2-3):281?332.Ann Copestake.
2003.
Report on the design of RMRS.DeepThought project deliverable.Ann Copestake.
2007a.
Applying robust semantics.In Proc.
PACLING 2007 ?
10th Conference of thePacific Association for Computational Linguistics,pages 1?12, Melbourne.Ann Copestake.
2007b.
Semantic composition with(Robust) Minimal Recursion Semantics.
In Proc.Workshop on Deep Linguistic Processing, ACL2007, Prague.Dick Crouch and Tracy Holloway King.
2006.
Seman-tics via F-structure rewriting.
In Miriam Butt andTracy Holloway King, editors, Proc.
LFG06 Con-ference, Universitat Konstanz.
CSLI Publications.Dick Crouch.
2006.
Packed rewriting for mapping se-mantics and KR.
In Intelligent Linguistic Architec-tures Variations on Themes by Ronald M. Kaplan,pages 389?416.
CSLI Publications.Anthony Davis.
2001.
Linking by Types in the Hierar-chical Lexicon.
CSLI Publications.David Dowty.
1991.
Thematic proto-roles and argu-ment selection.
Language, 67(3):547?619.Dan Flickinger and Emily Bender.
2003.
Compo-sitional semantics in a multilingual grammar re-source.
In Proc.
Workshop on Ideas and Strate-gies for Multilingual Grammar Development, ESS-LLI 2003, pages 33?42, Vienna.Dan Flickinger, Emily Bender, and Stephan Oepen.2003.
MRS in the LinGO Grammar Matrix: A prac-tical user?s guide.
http://tinyurl.com/crf5z7.Dan Flickinger, Jan Tore L?nning, Helge Dyvik,Stephan Oepen, and Francis Bond.
2005.
SEM-Irational MT ?
enriching deep grammars with a se-mantic interface for scalable machine translation.
InProc.
MT Summit X, Phuket, Thailand.Dan Flickinger.
2000.
On building a more efficientgrammar by exploiting types.
Natural LanguageEngineering, 6(1):15?28.Sanae Fujita, Francis Bond, Stephan Oepen, andTakaaki Tanaka.
2007.
Exploiting semantic infor-mation for HPSG parse selection.
In Proc.
Work-shop on Deep Linguistic Processing, ACL 2007,Prague.Jerry Hobbs.
1985.
Ontological promiscuity.
In Proc.ACL-85, pages 61?69, Chicago, IL.Alexander Koller and Alex Lascarides.
2009.
A logicof semantic representations for shallow parsing.
InProc.
EACL-2009, Athens.Stephan Oepen and Jan Tore L?nning.
2006.Discriminant-based MRS banking.
In Proc.
LREC-2006, Genoa, Italy.Martha Palmer, Dan Gildea, and Paul Kingsbury.
2005.The Proposition Bank: A corpus annotated with se-mantic roles.
Computational Linguistics, 31(1).Carl Pollard and Ivan Sag.
1994.
Head-driven PhraseStructure Grammar.
University of Chicago Press,Chicago.Sergio Roa, Valia Kordoni, and Yi Zhang.
2008.
Map-ping between compositional semantic representa-tions and lexical semantic resources: Towards accu-rate deep semantic parsing.
In Proc.
ACL-08, pages189?192, Columbus, Ohio.
Association for Compu-tational Linguistics.Antonio Sanfilippo.
1990.
Grammatical Relations,Thematic Roles and Verb Semantics.
Ph.D. thesis,Centre for Cognitive Science, University of Edin-burgh.Stefan Thater.
2007.
Minimal Recursion Semanticsas Dominance Constraints: Graph-Theoretic Foun-dation and Application to Grammar Engineering.Ph.D.
thesis, Universita?t des Saarlandes.9
