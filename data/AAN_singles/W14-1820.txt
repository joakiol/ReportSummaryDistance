Proceedings of the Ninth Workshop on Innovative Use of NLP for Building Educational Applications , pages 163?173,Baltimore, Maryland USA, June 26, 2014.c?2014 Association for Computational LinguisticsAssessing the Readability of Sentences: Which Corpora and Features?Felice Dell?Orletta, Martijn Wieling?
?, Andrea Cimino, Giulia Venturiand Simonetta MontemagniIstituto di Linguistica Computazionale ?Antonio Zampolli?
(ILC?CNR)ItaliaNLP Lab - www.italianlp.it{name.surname}@ilc.cnr.it?Department of Humanities Computing, University of Groningen, The Netherlands?Department of Quantitative Linguistics, University of T?ubingen, Germanywieling@gmail.comAbstractThe paper investigates the problem ofsentence readability assessment, which ismodelled as a classification task, with aspecific view to text simplification.
In par-ticular, it addresses two open issues con-nected with it, i.e.
the corpora to be usedfor training, and the identification of themost effective features to determine sen-tence readability.
An existing readabil-ity assessment tool developed for Italianwas specialized at the level of training cor-pus and learning algorithm.
A maximumentropy?based feature selection and rank-ing algorithm (grafting) was used to iden-tify to the most relevant features: it turnedout that assessing the readability of sen-tences is a complex task, requiring a highnumber of features, mainly syntactic ones.1 IntroductionOver the last ten years, work on automatic read-ability assessment employed sophisticated NLPtechniques (such as syntactic parsing and statisti-cal language modeling) to capture highly complexlinguistic features, and used statistical machinelearning to build readability assessment tools.
Avariety of different NLP?based approaches hasbeen proposed so far in the literature, differingat the level of the number of identified readabil-ity classes, the typology of features taken into ac-count, the intended audience of the texts underevaluation, or the application within which read-ability assessment is carried out, etc.Research focused so far on readability assess-ment at the document level.
However, as pointedout by Skory and Eskenazi (2010), methods devel-oped perform well when the task is characterizingthe readability level of an entire document, whilethey are unreliable for short texts, including singlesentences.
Yet, for specific applications, assessingthe readability level of individual sentences wouldbe desirable.
This is the case, for instance, for textsimplification: in current approaches, text read-ability is typically assessed with respect to the en-tire document, while text simplification is carriedout at the sentence level, as e.g.
done in Alu?
?sioet al.
(2010), Bott and Saggion (2011) and Inui etal.
(2003).
By decoupling the readability assess-ment and simplification processes, the impact ofsimplification operations on the overall readabil-ity level of a given text may not always be clear.With sentence?based readability assessment, thisis expected to be no longer a problem.
Sentencereadability assessment thus represents an open is-sue in the literature which is worth being furtherexplored.
To our knowledge, the only attemptsin this direction are represented by Dell?Orletta etal.
(2011) and Sj?oholm (2012) for the Italian andSwedish languages respectively, followed morerecently by Vajjala and Meurers (2014) dealingwith English.In this paper, we tackle the challenge of assess-ing the readability of individual sentences as a firststep towards text simplification.
The task is mod-elled as a classification task, with the final aimof shedding light on two open issues connectedwith it, namely the reference corpora to be usedfor training (i.e.
collections of sentences classifiedaccording to their readability level), and the iden-tification of the most effective features to deter-mine sentence readability.
For what concerns theformer, sentence readability assessment poses theremarkable issue of classifying sentences accord-ing to their difficulty: if all sentences occurring insimplified texts can be assumed to be easy?to?readsentences, the reverse does not necessarily holdsince not all sentences occurring in complex textsare to be assumed difficult?to?read.
This fact hasimportant implications at the level of the composi-tion of the corpora to be used for training.
The sec-163ond issue is concerned with whether and to whatextent the features playing a significant role in theassessment of readability at the sentence level co-incide with those exploited at the level of docu-ment.
In particular, the following research ques-tions are addressed:1. in assessing sentence readability, is it bet-ter to use a small gold standard training cor-pus of manually classified sentences or amuch bigger training corpus automaticallyconstructed from readability?tagged docu-ments possibly containing misclassified sen-tences?2.
which are the features maximizing sentencereadability assessment?3.
to what extent do important features for sen-tence readability classification match thoseplaying a role in the document readabilityclassification?We will try to answer these questions by work-ing on Italian, which is a less?resourced languageas far as readability is concerned.
To this end,READ?IT (Dell?Orletta et al., 2011; Dell?Orlettaet al., 2014), which represents the first NLP?basedreadability assessment tool for Italian, was spe-cialized in different respects, namely at the level ofthe training corpus and of the learning algorithm;to investigate questions 2. and 3. above, a maxi-mum entropy?based feature selection and rankingalgorithm (i.e.
grafting) was selected.
The specifictarget audience of readers addressed in this studyis represented by people characterised by low lit-eracy skills and/or by mild cognitive impairment.The paper is organized as follows: Section 2 de-scribes the background literature, Section 3 intro-duces our approach to the task, in terms of usedcorpora, features and learning algorithm.
Finally,Sections 4 and 5 describe the experimental settingand discuss achieved results.2 BackgroundIn spite of the acknowledged need of perform-ing readability assessment at the sentence level,so far very few attempts have been made to sys-tematically investigate the issues and challengesconcerned with the readability assessment of sen-tences (as opposed to documents).
The first twostudies in this direction focused on languagesother than English, namely Italian (Dell?Orlettaet al., 2011) and Swedish (Sj?oholm, 2012).
Inboth cases, the authors start from the assump-tion that while all sentences occurring in simpli-fied texts can be assumed to be easy?to?read sen-tences, the reverse is not true, since not all sen-tences occurring in complex texts are difficult?to?read.
This has important consequences at the levelof the evaluation of sentence classification results:i.e.
erroneous readability assessments within theclass of difficult?to?read texts may either corre-spond to those easy?to?read sentences occurringwithin complex texts or represent real classifi-cation errors.
To overcome this problem in thereadability assessment of individual sentences, anotion of distance with respect to easy-to-readsentences was introduced by Dell?Orletta et al.(2011).
Focusing on English, a similar issue isaddressed more recently by Vajjala and Meur-ers (2014) who developed a binary sentence clas-sifier trained on Wikipedia and Simple EnglishWikipedia: they showed that the low accuracy ob-tained by their classifier stems from the incorrectassumption that all Wikipedia sentences are morecomplex than the Simple Wikipedia ones.Besides readability, sentence?based analysesare reported in the literature for related tasks: forinstance, in a text simplification scenario by Drn-darevi?c et al.
(2013), Alu?
?sio et al.
(2008),?Stajnerand Saggion (2013) and Barlacchi and Tonelli(2013); or to predict writing quality level by Louisand Nenkova (2013).
Sheikha and Inkpen (2012)report the results of both document?
and sentence?based classification in the different but related taskof assessing formal vs. informal style of a docu-ment/sentence.
For students learning English, An-dersen et al.
(2013) made a self?assessment andtutoring system available which was able to assigna quality score for each individual sentence theywrite: this provides automated feedback on learn-ers?
writing.A further important issue, largely investigatedin previous readability assessment studies, is theidentification of linguistic factors playing a rolein assessing the readability of documents.
If tra-ditional readability metrics (see e.g., Kincaid etal.
(1975)) typically rely on raw text characteris-tics, such as word and sentence length, the newNLP?based readability indices exploit wider setsof features ranging across different linguistic lev-els.
Starting from Schwarm and Ostendorf (2005)and Heilman et al.
(2007), the role of syntactic164features in this task was considered, and more re-cently, the role of discourse features (e.g., dis-course topic, discourse cohesion and coherence)has also been taken into account (see e.g., Barzi-lay and Lapata (2008), Pitler and Nenkova (2008),Kate et al.
(2010), Feng et al.
(2010) and Tonelliet al.
(2012)).
Many of these studies also exploredthe usefulness of features belonging to individuallevels of linguistic description in predicting textreadability.
For example, Feng et al.
(2010) sys-tematically evaluated a wide range of features andcompared the results of different statistical classi-fiers trained on different classes of features.
Sim-ilarly, the correlation between level?specific fea-tures has been calculated by Pitler and Nenkova(2008) with respect to human readability judg-ments, and by Franc?ois and Fairon (2012) withrespect to readability levels.
In both cases, theclasses of features which turned out to be highlycorrelated with readability judgments were usedin a readability assessment tool to test their effi-cacy.
Note, however, that in all cases the predic-tive power of the selected features was evaluatedat the document level only.3 Our ApproachIn this section, we introduce the main ingredi-ents of our approach to sentence readability as-sessment, corpora used for training and testing,selected features and the learning and feature se-lection algorithm.3.1 CorporaWe relied on two different corpora: a newspapercorpus, La Repubblica (henceforth, Rep), and aneasy?to?read newspaper, Due Parole (henceforth,2Par).
2Par includes articles specifically writtenby Italian linguists experts in text simplificationfor an audience of adults with a rudimentary lit-eracy level or with mild intellectual disabilities(Piemontese, 1996), which represents the targetaudience of this study.
The two corpora ?
selectedas representative of complex vs. simplified textswithin the journalistic genre ?
differ significantlywith respect to the distribution of features typi-cally correlated with text complexity (Dell?Orlettaet al., 2011) and thus represent reliable trainingdatasets.
However, whereas such a distinction isvalid as far as documents are concerned, it appearsto be a simplistic generalization when the focus ison sentences.
In other words, whereas we can con-sider all sentences of 2Par as easy?to?read, not allRep sentences are expected to be difficult?to?read.From this it follows that whereas the internal com-position of 2Par is homogeneous at the sentencelevel, this is not the case for Rep.To overcome this asymmetry and in particularto assess the impact of the noise in the Rep train-ing corpus, we constructed different training setsdiffering in size and internal composition, goingfrom a noisy set which assumes all Rep sentencesto be difficult?to?read to a clean but smaller setin which the easy?to?read sentences occurring inRep were manually filtered out.
These trainingsets were used in different experiments whose re-sults are reported in Section 4.2.The corpus containing only difficult?to?readsentences was manually built by annotating Repsentences according to their readability (i.e.
easyvs.
difficult).
The annotation process was car-ried out by two annotators with a background incomputational linguistics.
In order to assess thereliability of their judgements, we started with asmall annotation experiment: the two annotatorswere provided with the same 5 articles from theRep corpus (for a total of 107 sentences) and wereasked to extract the difficult?to?read sentences (asopposed to both easy?to?read and not?easy?to?classify sentences).
The first annotator carried outthe task in 5 minutes and 46 seconds, while thesecond annotator took 9 minutes and 8 seconds.The two annotators agreed on the classification of81 difficult?to?read sentences out of 107 consid-ered ones (in particular, the first annotator iden-tified 90 difficult?to?read?sentences and the sec-ond one 93 sentences).
The agreement betweenthe two annotators was calculated in terms of pre-cision, by taking one of the annotation sets as thegold standard and the other as response: on aver-age, we obtained a precision of 0.88 in the retrievalof sentences definitely classified as difficult?to?read.
Given the high level of agreement, the twoannotators were asked to select difficult sentencesfrom two sets of distinct Rep articles.
This re-sulted in a set of 1,745 difficult?to?read sentenceswhich were used together with a random selectionof easy?to?read sentences from 2Par for trainingand testing.11The collection can be downloaded fromwww.italianlp.it/?page id=22.165Feature Ranking position Feature Ranking positionSent.
class.
Doc.
class.
Sent.
class.
Doc.
class.Raw text features:[1] Sentence length 1 1 [2] Word length 2 2Lexical features:[3] Word types in the Basic Italian Vocabu-lary14 42 [6] ?High availability words?
21 22[4] ?Fundamental words?
10 9 [7] TTR (form) 7[5] ?High usage words?
22 38 [8] TTR (lemma) 53Morpho?syntactic features:[9] Adjective 46 [26] Aux.
verb ?
inf.
mood 64[10] Adverb 29 59 [27] Aux.
verb ?
part.
mood 51[11] Article 49 25 [28] Aux.
verb ?
subj.
mood 55[12] Conjunction 40 [29] Main verb ?
cond.
mood 40 43[13] Determiner 43 54 [30] Main verb ?
ger.
mood 48 48[14] Interjection [31] Main verb ?
imp.
mood 37 57[15] Noun 12 19 [32] Main verb ?
indic.
mood 16 11[16] Number 65 44 [33] Main verb ?
inf.
mood 13 13[17] Predeterminer [34] Main verb ?
part.
mood 26 28[18] Preposition 61 [35] Main verb ?
subj.
mood 46 32[19] Pronoun 27 30 [36] Modal verb - inf.
mood 54 56[20] Punctuation 35 [37] Modal verb ?
cond.
mood 41 36[21] Residual [38] Modal verb ?
imp.
mood[22] Verb 63 34 [39] Modal verb ?
indic.
mood 18 23[23] Lexical density 34 33 [40] Modal verb ?
part.
mood[24] Aux.
verb ?
cond.
mood 59 60 [41] Modal verb ?
subj.
mood 60 58[25] Aux.
verb ?
indic.
mood 17 17Syntactic features:[42] Argument 62 [65] Sentence root 35 62[43] Auxiliary 70 [66] Subject 39 52[44] Clitic 63 [67] Subordinate clause 64[45] Complement 28 29 [68] Temporal complement 45 55[46] Concatenation 66 [69] Temporal modifier[47] Conjunct in a disjunctive compound 58 67 [70] Temporal predicate[48] Conjunct linked by a copulative con-junction38 37 [71] Parse tree depth 5 4[49] Copulative conjunction 31 39 [72] Embedded complement ?chains?
8 24[50] Determiner 50 26 [73] Verbal Root 6 3[51] Direct object 44 27 [74] Arity of verbal predicates 3 15[52] Disjunctive conjunction 57 68 [75] Pre?verbal subject 4 12[53] Indirect complement/object 66 [76] Post?verbal subject 25 16[54] Locative complement 52 51 [77] Pre?verbal object 36 41[55] Locative modifier [78] Post?verbal object 9 21[56] Locative predicate [79] Main clauses 23 14[57] Modal verb 61 [80] Subordinate clauses 42 45[58] Modifier 20 47 [81] Subordinate clauses in pre?verbal posi-tion32 10[59] Negative 56 69 [82] Subordinate clauses in post?verbal po-sition19 20[60] Passive subject [83] ?Chains?
of embedded subordinateclauses11 5[61] Predicative complement 49 [84] Finite complement clauses 30 18[62] Preposition [85] Infinitive clauses 53 50[63] Punctuation 24 31 [86] Length of dependency links 15 8[64] Relative modifier 47 65 [87] Maximum length of dependency links 7 6Table 1: Typology of features and ranking position in sentence and document readability assessmentexperiments.
Only about 14 features are needed for an adequate model of document readability, whereasthis number increases to 30 for sentence readability (marked in boldface).
Features which were notselected during ranking have no rank.3.2 Linguistic FeaturesThe set of features used in the experiments re-ported in this paper is wide, spanning across dif-ferent levels of linguistic analysis.
They canbe broadly classified into four main classes, asreported in Table 1: raw text features, lexicalfeatures, morpho?syntactic features and syntacticfeatures, shortly described below.22For an exhaustive discussion including the motivationsunderlying this selection of features, the interested reader isRaw text features (Features [1?2] in Table 1)refer to those features typically used within tra-ditional readability metrics and include sentencelength, calculated as the average number of wordsper sentence, and word length, calculated as theaverage number of characters per words.The cover category of lexical features (Features[3?8] in Table 1) includes features referring toreferred to Dell?Orletta et al.
(2011, 2014) where these fea-tures were successfully used for assessing the readability ofItalian texts.166both the internal composition of the vocabularyand the lexical richness of the text.
For what con-cerns the former, the Basic Italian Vocabulary byDe Mauro (2000) was taken as a reference re-source, including a list of 7000 words highly fa-miliar to native speakers of Italian.
In particular,we consider: a) the percentage of all unique words(types) on this reference list occurring in the text,and b) the internal distribution of the occurring ba-sic Italian vocabulary words into the usage classi-fication classes of ?fundamental words?
(very fre-quent words), ?high usage words?
(frequent words)and ?high availability words?
(relatively lower fre-quency words referring to everyday life).
Lexicalrichness of texts is monitored by computing theType/Token Ratio (TTR), which refers to the ratiobetween the number of lexical types and the num-ber of tokens within a text.
Due to its sensitivityto sample size, this feature is computed for textsamples of equivalent length.The set of morpho?syntactic features (Features[9?41] in Table 1) is aimed at capturing differ-ent aspects of the linguistic structure affecting inone way or another the readability of a text.
Theyrange from the probability distribution of part?of?speech (POS) types, to the lexical density ofthe text, calculated as the ratio of content words(verbs, nouns, adjectives and adverbs) to the to-tal number of lexical tokens in a text.
This classalso includes features referring to the distributionof verbs by mood and/or tense, which can be seenas a language?specific feature exploiting the pre-dictive power of the Italian rich morphology.The set of syntactic features (Features [42?87]in Table 1) captures different aspects of the syntac-tic structure which are taken as reliable indicatorsfor automatic readability assessment, namely:?
the unconditional probability of syntactic de-pendency types, e.g.
subject, direct object,modifier, etc.
(Features 42?70);?
parse tree depth features (71?72), going fromthe depth of the whole parse tree, calculatedin terms of the longest path from the rootof the dependency tree to some leaf, to amore specific feature referring to the aver-age depth of embedded complement ?chains?governed by a nominal head and includingeither prepositional complements or nominaland adjectival modifiers;?
verbal predicate features (73?78) aimed atcapturing different aspects of the behaviourof verbal predicates: they range from thenumber of verbal roots with respect to num-ber of all sentence roots occurring in a text,to more specific features such as the arityof verbs, meant as the number of instanti-ated dependency links sharing the same ver-bal head (covering both arguments and modi-fiers) and the relative ordering of subject andobject with respect to the verbal head;?
as subordination is widely acknowledgedto be an index of structural complexityin language, subordination features (79?85) include: the distribution of subordinatevs.
main clauses; for subordinates, the dis-tribution of infinitives vs finite complementclauses, their relative ordering with respectto the main clause and the average depth of?chains?
of embedded subordinate clauses;?
the length of dependency links is anothercharacteristic connected with the syntacticcomplexity of sentences.
Features 86?87measure dependency length in terms of thewords occurring between the syntactic headand the dependent: they focus on all depen-dency links vs. maximum dependency linksonly.3.3 Model Training and Feature RankingGiven the twofold goal of this study, i.e.
re-liably assessing sentence readability and findingthe most predictive features undelying it, we usedGRAFTING (Perkins et al., 2003), as this approachallows to train a maximum entropy model while si-multaneously including incremental feature selec-tion.
The method uses a gradient?based heuristicto select the most promising feature (to add to theset of selected features S), and then performs a fullweight optimization over all features in S. Thisprocess is repeated until a certain stopping crite-rion is reached.
As the grafting approach we useintegrates the l1regularization (preventing overfit-ting), features are only included (i.e.
have a non-zero weight) when the reduction of the objectivefunction is greater than a certain treshold.
In ourcase, the l1prior we use was selected on the basisof evaluating maximum entropy models with vary-ing l1values (range: 1e-11, 1e-10, ..., 0.1, 1) via10?fold cross validation.
We used TINYEST3, a3http://github.com/danieldk/tinyest167grafting-capable maximum entropy parameter es-timator for ranking tasks (de Kok, 2011; de Kok,2013), to select the features and estimate theirweights.
Whereas our task is not a ranking task,but rather a binary classification problem, we wereable to model it as a ranking task by assigning ahigh score (1) to difficult?to?read sentences and alow score (0) to easy?to?read sentences.
Conse-quently, a sentence having a score < 0.5 was in-terpreted as an easy?to?read sentence, whereas asentence which was assigned a score ?
0.5 wasinterpreted to be a difficult?to?read sentence.4 Experiments and Results4.1 Experimental SetupIn all experiments, the corpora were automaticallytagged by the part?of?speech tagger describedin Dell?Orletta (2009) and dependency?parsed bythe DeSR parser (Attardi, 2006) using SupportVector Machines as learning algorithm.
We de-vised two different experiments, aimed at explor-ing the research questions investigated in this pa-per.
To this end, READ?IT was adapted by inte-grating a specialized training corpus and a maxi-mum entropy?based feature selection and rankingalgorithm (i.e.
grafting).Experiment 1This experiment, investigating the first researchquestion, is aimed at identifying what is the mosteffective training data for sentence readability as-sessment.
In particular, the goal is to comparethe results on the basis of using a small set ofgold standard data with respect to a (potentiallylarger, but) noisy data set (i.e.
without manual re-vision) where every Rep sentence was assumed tobe difficult?to?read.
In particular, the comparisoninvolved four datasets:?
a collection of gold standard data consistingof 1,310 easy?to?read sentences randomlyextracted from the 2Par corpus and 1,310manually selected difficult?to-read sentencesfrom the Rep corpus;?
a large and unbalanced collection of uncor-rected data consisting of the whole 2Par cor-pus (3,910 easy?to?read sentences) and thewhole Rep corpus (8,452 sentences, classi-fied a priori as difficult?to?read);?
a balanced collection of uncorrected sen-tences, consisting of 3,910 sentences from2Par and 3,910 sentences from Rep;?
a balanced collection of uncorrected sen-tences having the same size as the gold stan-dard dataset, namely 1,310 sentences from2Par and 1,310 sentences from Rep.To assess similarities and differences at the levelof the different corpora used for training in thisexperiment, in Table 2 we report a selection oflinguistic features (see Section 3.2) characterizingthe four datasets with respect to the whole 2Parcorpus.We can observe that 2Par differs from allfour Rep corpora for all reported features, and thatthe four Rep corpora show similar trends.
Inter-estingly, however, the Rep Gold corpus is almostalways the most distant one from 2Par (i.e.
at thelevel of sentence length, word length, distributionof adjectives and subjects, average length of de-pendency links and parse tree depth).On the basis of the four Rep datasets, four mod-els were built which we evaluated using a held?out test set consisting of 435 sentences from 2Parand 435 manually classified difficult?to?read sen-tences from Rep.
Using the grafting method, wecalculated the classification score for each sen-tence in our test set on the basis of an increasingnumber of features (ranging from 1 to all non-zeroweighted features for the specific dataset): sen-tences with a score below 0.5 were classified aseasy?to?read, whereas sentences having a scoregreater or equal to 0.5 were classified as difficult?to?read.
This procedure was repeated for each ofthe four models.Experiment 2The second experiment is aimed at answering oursecond and third research questions, focusing onthe features relevant for sentence readability, andthe relationship of those features with documentreadability classification.
For this purpose, wecompared sentence?
and document?based read-ability classification results.
In particular, we com-pared the features used by the sentence?basedreadability model trained on the gold standarddata and the features used by the document?basedmodel trained on Rep and 2Par.
With respectto the document classification, we used a cor-pus of 638 documents (319 extracted from 2Parrepresentating easy?to?read texts, and 319 ex-tracted from Rep representing difficult?to?readtexts) with 20% of the documents constituting theheld?out test set.168Features Rep Unbalan.
large Rep Balan.
small Rep Balan.
large Rep Gold 2ParSentence length 24.98 26.03 25.26 28.14 18.66Word length 5.14 5.24 5.14 5.28 5?Fundamental words?
75.05% 75.08% 74.83% 74.99% 76.38Adjective 6.19% 6.25% 6.36% 6.42% 6.03%Noun 25.65% 27.09% 25.74% 26.10% 29.13%Subject 4.62% 4.75% 4.64% 4.42% 6%Max.
length of dependency links 9.73 10.13 9.85 10.98 7.67Parse tree depth 6.18 6.57 6.30 6.83 5.2Table 2: Distribution of some linguistic features in Rep and 2Par training dataAccuracy Precision (all ft)Training data 2 ft 10 ft 30 ft 50 ft all ft Easy DifficultUnbalanced large 50 63.7 74.9 78.4 78.9 (85 ft) 69.2 88.5Balanced small 64 67.9 79.2 80.8 82.5 (82 ft) 82.5 82.5Balanced large 63.9 70.6 79.7 81.0 82.3 (85 ft) 83.0 81.6Gold data 65.6 69.8 79.9 81.3 83.7 (66 ft) 84.8 82.5Table 3: Sentence classification results using four training datasets and a varying number of features4.2 Which Training Corpus for SentenceClassification?Table 3 reports the results for the sentence classi-fication task using the four training datasets de-scribed above.
Results are reported in terms ofboth overall accuracy (calculated as the proportionof correct answers against all answers) and preci-sion within each readability class (when using allfeatures), defined as the number of easy or diffi-cult sentences correctly identified as such (in theirrespective columns).Accuracy was computed for all training modelstested using an increasing number of features (2,10, 30, 50 and all features) as resulting from theGRAFTING?based ranking and detailed in Table 1.Note that the first two features correspond in allcases to the traditional readability features of sen-tence length and word length.
The classificationmodel trained on the small gold standard datasetturned out to almost always outperform all othermodels: it achieved the best accuracy (83.7%) us-ing a relatively small number of features (66), andalso for a fixed number of features (i.e.
2, 30and 50).
Only when using the top?10 features,the uncorrected balanced large dataset slightly out-performed the gold standard dataset.
The accu-racy when using the unbalanced dataset for train-ing was always significantly (p < 0.05) worse (us-ing McNemar?s test) than the accuracy based onthe other training data.
The only other significantdifference existed between the balanced small andlarge dataset for 10 features.
All other differencesare non?significant.It is also interesting to note that in the resultsreported in column 2 ft of Table 3 a significantdifference is observed when comparing the accu-racy achieved using the unbalanced large data setwith that achieved with the gold standard data: i.e.about 15.5 percentage points of difference for the2 ft model against 3 ?
6% using higher numbersof features.
This result originates from the fact thatthe unbalanced corpus contains to a larger extentsentences which are short and complex at the sametime whose correct readability assessment requireslinguistically?grounded features (see below).The last two columns of Table 3 report preci-sion results for easy?
vs. difficult?to?read sen-tences for each of the four training datasets (allfeatures).
It is clear that for the class of difficult?to?read sentences the highest precision (88.5%) isobtained when using the whole 2Par and Rep cor-pora for training (i.e.
unbalanced large), whereasfor the class of easy?to?read sentences the bestprecision results (84.8%) are obtained with thesystem trained on the gold standard dataset.
In-terestingly, the worst precision results (69.2%) arereported for the class of easy?to?read sentenceswith the unbalanced large training data set.These results suggest that the advantages of us-ing the gold standard data over the uncorrectedtraining data sets are limited.
From this it fol-lows that treating the whole Rep corpus as a col-lection of difficult?to?read sentences is not com-pletely unjustified: this is in line with the satisfac-tory results reported by Dell?Orletta et al.
(2011)where Rep was used for training a sentence read-169ability classifier without any manual filtering ofsentences.
Nevertheless, the results of this ex-periment demonstrate that readability assessmentaccuracy and in particular the precision in identi-fying easy?to?read sentences can be improved byusing a manually selected training dataset.
Bal-ancing the size of larger but potentially noisy (i.e.without manual revision) data sets appears to cre-ate a positive trade?off between accuracy and pre-cision for both classes, thus representing a viablealternative to the construction of a gold standarddataset.4.3 Sentence vs.
Document Classification:which and how many features?To identify the typology of features needed forsentence readability assessment and compare themto those needed for assessing document read-ability, we compared the results obtained by thegrafting?based feature selection in the sentenceclassification task (using the gold standard datasetfor training, see Table 3) to those obtained in thedocument classification task whose accuracy onthe test set is reported in Table 4 for increasingnumbers of features selected via GRAFTING.Train.
data 2 ft 10 ft 30 ft 50 ft 70 ft (all)Rep - 2Par 80 93.3 96.6 96.6 95Table 4: Accuracy of document classification fora varying number of featuresBy comparing the document classification re-sults with respect to those obtained for sentences,it can be noticed that the best accuracy is achievedusing a set of 30 features: in contrast to sentenceclassification where adding features keeps increas-ing the performance, more features do not appearto help for document classification.
Sentence read-ability classification thus seems to be a more com-plex task, requiring a higher amount of features.This trend emerges more clearly in Figures 1(a)and 1(b), where the classification results on thetraining set (using 10?fold cross?validation) andthe held?out test set are visualized for increas-ing amounts of features selected via GRAFTING.As Figure 1(a) shows, the document classifica-tion task requires about 14 features after whichthe performance appears to stabilize (97.4% accu-racy for the ten?fold cross-validation and 96.7%for the held?out test set).
In contrast, Figure1(b) shows that sentence classification requires atleast 30 features (83.4% accuracy for the ten?foldcross-validation and 79.9% for the test set).Noticeable differences can also be observed inthe typology of features playing a prominent rolein the two tasks.
For each feature taken into ac-count, Table 1 reports its ranking as resulting fromsentence?
and document?based classification ex-periments (columns ?Sent.
class.?
and ?Doc.class.?
respectively).
Note that in interpretingthe rank associated with each feature it shouldbe considered that in sentence?
and document?classification the number of required features issignificantly different, i.e.
30 and 14 respectively:this is to say that approximately the same rank as-sociated to the same feature does not entail a com-parable role across the two classification tasks.As already pointed out, for both sentences anddocuments raw text features (i.e.
Sentence lengthand Word length) turned out to be the top features,leading however to significantly different results:i.e.
80% accuracy for documents vs. 65% forsentences.
Among the remaining features, graft-ing results show that syntactic features do playa central role in both sentence?
and document?based readability assessment: many of these arehighly ranked, with some differences.
Syntacticfeatures playing a similar role in both readabil-ity classification tasks include: Verbal root [73],Parse tree depth [71], ?Chains?
of embedded sub-ordinate clauses [83] and Max.
length of depen-dency links [87], covering important aspects ofsyntactic complexity such as depth of the syntacticdependency (sub?
)tree and length of dependencylinks.
Features that are mainly useful for sentencereadability turned out to be Arity of verbal pred-icates [74], Pre?verbal subject [75], Post?verbalobject [78] and Embedded complement ?chains?
[72], which can all be seen as representing localfeatures referring to sentence parts.
The featureSubordinate clauses in pre?verbal position [81],focusing on the global distribution of pre?verbalsubordinate clauses within the document, is rele-vant for document classification only.
It is interest-ing to note that features capturing different facetsof the same phenomenon can play quite a differentrole for assessing the readability of sentences vs.documents: this is the case of dependency length,measured in terms of the words occurring betweenthe syntactic head and the dependent, where fea-ture [86] refers to the average length of all de-pendency links and [87] to the average length of170(a) Document classification (b) Sentence classificationFigure 1: Document vs Sentence classification resultsmaximum dependency links from each sentence.Whereas [86] plays a similar role for sentencesand documents (in both cases it is a middle rankfeature), [87] is a global feature playing a moreprominent role in document classification.At the morpho?syntactic level, the feature rank-ing is more comparable.
However, it is interest-ing to note that very few morpho?syntactic fea-tures were selected by the feature selection pro-cess: this is particularly true for document classi-fication.
This can follow from the fact that thesefeatures can be considered as proxies of the syn-tactic structure which in these experiments wasrepresented through specific features: in this situ-ation, the grafting process preferred syntactic fea-tures over morpho?syntactic ones, in spite of thelower accuracy of the dependency parser with re-spect to the part?of?speech tagger.
Interestingly,this result is in contrast with what reported byFalkenjack and J?onsson (2014) for what concernsdocument readability assessment, who claim thatan optimal subset of text features for readabilitybased document classification does not need fea-tures induced via parsing.
Among the morpho?syntactic features, it appears that verbal featuresplay an important role: this can follow both by thelanguage dealt with which is a a morphologicallyrich language, and by the fact that these featuresdo not have a counterpart at the syntactic level.Lexical features show a much more mixed re-sult.
Type?Token Ratio (TTR) is only importantfor document classification, whereas most of theother features are important for sentence readabil-ity, but not for document readability (with the ex-ception of the presence of ?fundamental words?
ofthe Basic Italian Vocabulary).5 DiscussionIn this study we have focused on three researchquestions.
First, we asked which type of train-ing corpus is best to assess sentence readability.Whereas we found that using a set of manuallyselected complex sentences was better than usinga simple corpus?based distinction, the extra ef-fort needed to construct the training corpus mightnot be worthwhile as observed improvements werequite modest.
However, we did not consider amore sensitive measure of the difficulty of a sen-tence (such as a number ranging between 0 and1), and this might be able to offer a more sub-stantial improvement (at the cost of needing moretime to create the training material).
Of course,when the goal is to identify the best features forassessing sentence readability, it does make senseto have high?quality training data to prevent se-lecting inadequate features.
The second researchquestion involved identifying which features weremost useful for assessing sentence readability.
Be-sides raw text features, syntactic but also morpho?syntactic features turned out to play a central roleto achieve adequate performance.
The third re-search question investigated the overlap betweenthe features needed for document and sentencereadability classification.
Whereas there certainlywas overlap between the top features (with dif-ferent levels of performance), most of the fea-tures had a different rank across the two tasks,with local features being more predictive for sen-tence classification and global ones for documents.This suggests that the sentence readability task ismore complex than assessing document readabil-ity, given that there is much less information avail-able for a sentence than for a document.171AcknowledgmentsThe research reported in this paper was carried outin the framework of the Short Term Mobility pro-gram of international exchanges funded by CNR(Italy).
We thank Dani?el de Kok for his help inapplying TINYEST to our data and Giulia Benottofor her help in manual revision of training data.ReferencesSandra M.
Alu?
?sio, Lucia Specia, Thiago A.S. Pardo,Erick G. Maziero, and Renata P.M. Fortes.
2008.Towards brazilian portuguese automatic text simpli-fication systems.
In Proceedings of the Eighth ACMSymposium on Document Engineering, pages 240?248.Sandra Alu?
?sio, Lucia Specia, Caroline Gasperin, andCarolina Scarton.
2010.
Readability assessment fortext simplification.
In Proceedings of the NAACLHLT 2010 Fifth Workshop on Innovative Use of NLPfor Building Educational Applications, pages 1?9.
?istein E. Andersen, Helen Yannakoudakis, FionaBarker, and Tim Parish.
2013.
Developing andtesting a self-assessment and tutoring system.
InProceedings of the Eighth Workshop on InnovativeUse of NLP for Building Educational Applications,pages 32?41.Giuseppe Attardi.
2006.
Experiments with a multi-language non-projective dependency parser.
In Pro-ceedings of the Tenth Conference on ComputationalNatural Language Learning (CoNLL-X ?06), NewYork City, New York, pages 166?170.Gianni Barlacchi and Sara Tonelli.
2013.
Ernesta: Asentence simplification tool for children?s stories initalian.
In Proceedings of the 14th Conferences onComputational Linguistics and Natural LanguageProcessing (CICLing 2013), pages 476?487.Regina Barzilay and Mirella Lapata.
2008.
Model-ing local coherence: An entity-based approach.
vol-ume 34.Stefan Bott and Horacio Saggion.
2011.
An un-supervised alignment algorithm for text simplifica-tion corpus construction.
In Proceedings of theWorkshop on Monolingual Text-To-Text Generation,pages 20?26.Dani?el de Kok.
2011.
Discriminative features inreversible stochastic attribute-value grammars.
InProceedings of the EMNLP Workshop on LanguageGeneration and Evaluation, pages 54?63.
Associa-tion for Computational Linguistics.Dani?el de Kok.
2013.
Reversible Stochastic Attribute-Value Grammars.
Ph.D. thesis, RijksuniversiteitGroningen.Tullio De Mauro.
2000.
Il dizionario della lingua ital-iana.
Paravia, Torino.Felice Dell?Orletta, Simonetta Montemagni, and Giu-lia Venturi.
2011.
Read-it: Assessing readabilityof italian texts with a view to text simplification.
InProceedings of the Workshop on Speech and Lan-guage Processing for Assistive Technologies (SLPAT2011), pages 73?83.Felice Dell?Orletta, Simonetta Montemagni, and GiuliaVenturi.
2014.
Assessing document and sentencereadability in less resourced languages and acrosstextual genres.
In International Journal of AppliedLinguistics (ITL).
Special Issue on Readability andText Simplification.
To appear.Felice Dell?Orletta.
2009.
Ensemble system for part-of-speech tagging.
In Proceedings of Evalita?09,Evaluation of NLP and Speech Tools for Italian,Reggio Emilia, December.Biljana Drndarevi?c, Sanja?Stajner, Stefan Bott, SusanaBautista, and Horacio Saggion.
2013.
Automatictext simplification in spanish: A comparative evalu-ation of complementing modules.
In ComputationalLinguistics and Intelligent Text Processing, pages488?500.
Springer Berlin Heidelberg.Johan Falkenjack and Arne J?onsson.
2014.
Classify-ing easy-to-read texts without parsing.
In Proceed-ings of the Proceedings of the 3rd Workshop on Pre-dicting and Improving Text Readability for TargetReader Populations (PITR), Gothenburg, Sweden.Association for Computational Linguistics.Lijun Feng, Martin Jansche, Matt Huenerfauth, andNo?emie Elhadad.
2010.
A comparison of featuresfor automatic readability assessment.
In Proceed-ings of the 23rd International Conference on Com-putational Linguistics (COLING 2010), pages 276?284.Thomas Franc?ois and C?edrick Fairon.
2012.
An ?AIreadability?
formula for french as a foreign lan-guage.
In Proceedings of the 2012 Joint Confer-ence on Empirical Methods in Natural LanguageProcessing and Computational Natural LanguageLearning, Jeju Island, Korea, pages 466?477.Michael J. Heilman, Kevyn Collins, and Jamie Callan.2007.
Combining lexical and grammatical featuresto improve readability measures for first and secondlanguage texts.
In Proceedings of the Human Lan-guage Technology Conference, pages 460?467.Kentaro Inui, Atsushi Fujita, Tetsuro Takahashi, RyuIida, and Tomoya Iwakura.
2003.
Text simplifica-tion for reading assistance: A project note.
In Pro-ceedings of the Second International Workshop onParaphrasing, pages 9?16.Rohit J. Kate, Xiaoqiang Luo, Siddharth Patwardhan,Martin Franz, Radu Florian, Raymond J. Mooney,Salim Roukos, and Chris Welty.
2010.
Learning to172predict readability using diverse linguistic features.In oceedings of the 23rd International Conferenceon Computational Linguistics, pages 546?554.J.
Peter Kincaid, Lieutenant Robert P. Fishburne,Richard L. Rogers, and Brad S. Chissom.
1975.Derivation of new readability formulas for navyenlisted personnel.
In Research Branch Report,Millington, TN: Chief of Naval Training, pages 8?75.Annie Louis and Ani Nenkova.
2013.
A corpus of sci-ence journalism for analysing writing quality.
vol-ume 4.Simon Perkins, Kevin Lacker, and James Theiler.2003.
Grafting: Fast, incremental feature selectionby gradient descent in function space.
The Journalof Machine Learning Research, 3:1333?1356.Maria Emanuela Piemontese.
1996.
Capire e farsicapire.
Teorie e tecniche della scrittura controllata.Tecnodid, Napoli.Emily Pitler and Ani Nenkova.
2008.
Revisitingreadability: A unified framework for predicting textquality.
In Proceedings of the 2008 Conference onEmpirical Methods in Natural Language Process-ing, pages 186?195.Sarah E. Schwarm and Mari Ostendorf.
2005.
Readinglevel assessment using support vector machines andstatistical language models.
In Proceedings of the43rd Annual Meeting on Association for Computa-tional Linguistics (ACL 05), pages 523?530.Fadi Abu Sheikha and Diana Inkpen.
2012.
Learningto classify documents according to formal and infor-mal style.
volume 8.Johan Sj?oholm.
2012.
Probability as readability: Anew machine learning approach to readability as-sessment for written Swedish.
LiU Electronic Press,Master thesis.Adam Skory and Maxine Eskenazi.
2010.
Predictingcloze task quality for vocabulary training.
In Pro-ceedings of the NAACL HLT 2010 Fifth Workshopon Innovative Use of NLP for Building EducationalApplications, pages 49?56.Sara Tonelli, Ke Tran Manh, and Emanuele Pianta.2012.
Making readability indices readable.
In Pro-ceedings of the First Workshop on Predicting andImproving Text Readability for Target Reader Popu-lations, pages 40?48.Sowmya Vajjala and Detmar Meurers.
2014.
On as-sessing the reading level of individual sentences fortext simplification.
In Proceedings of the 14th Con-ference of the European Chapter of the Associationfor Computational Linguistics (EACL-14), Gothen-burg, Sweden.
Association for Computational Lin-guistics.Sanja?Stajner and Horacio Saggion.
2013.
Readabil-ity indices for automatic evaluation of text simplifi-cation systems: A feasibility study for spanish.
InProceedings of the International Joint Conferenceon Natural Language Processing.173
