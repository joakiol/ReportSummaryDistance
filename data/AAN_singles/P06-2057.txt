Proceedings of the COLING/ACL 2006 Main Conference Poster Sessions, pages 436?443,Sydney, July 2006. c?2006 Association for Computational LinguisticsA FrameNet-based Semantic Role Labeler for SwedishRichard Johansson and Pierre NuguesDepartment of Computer Science, LTHLund University, Sweden{richard, pierre}@cs.lth.seAbstractWe present a FrameNet-based semanticrole labeling system for Swedish text.
Astraining data for the system, we used anannotated corpus that we produced bytransferring FrameNet annotation from theEnglish side to the Swedish side in a par-allel corpus.
In addition, we describe twoframe element bracketing algorithms thatare suitable when no robust constituentparsers are available.We evaluated the system on a part of theFrameNet example corpus that we trans-lated manually, and obtained an accuracyscore of 0.75 on the classification of pre-segmented frame elements, and precisionand recall scores of 0.67 and 0.47 for thecomplete task.1 IntroductionSemantic role labeling (SRL), the process of auto-matically identifying arguments of a predicate ina sentence and assigning them semantic roles, hasreceived much attention during the recent years.SRL systems have been used in a number ofprojects in Information Extraction and QuestionAnswering, and are believed to be applicable inother domains as well.Building SRL systems for English has beenstudied widely (Gildea and Jurafsky, 2002;Litkowski, 2004), inter alia.
However, all theseworks rely on corpora that have been produced atthe cost of a large effort by human annotators.
Forinstance, the current FrameNet corpus (Baker etal., 1998) consists of 130,000 manually annotatedsentences.
For smaller languages such as Swedish,such corpora are not available.In this work, we describe a FrameNet-based se-mantic role labeler for Swedish text.
Since therewas no existing training corpus available ?
noFrameNet-annotated Swedish corpus of substan-tial size exists ?
we used an English-Swedishparallel corpus whose English part was annotatedwith semantic roles using the FrameNet annota-tion scheme.
We then applied a cross-languagetransfer to derive an annotated Swedish part.
Toevaluate the performance of the Swedish SRLsystem, we applied it to a small portion of theFrameNet example corpus that we translated man-ually.1.1 FrameNet: an IntroductionFrameNet (Baker et al, 1998) is a lexical databasethat describes English words using Frame Seman-tics (Fillmore, 1976).
In this framework, predi-cates (or in FrameNet terminology, target words)and their arguments are linked by means of seman-tic frames.
A frame can intuitively be thought ofas a template that defines a set of slots, frame ele-ments (FEs), that represent parts of the conceptualstructure and typically correspond to prototypicalparticipants or properties.Figure 1 shows an example sentence annotatedwith FrameNet information.
In this example, thetarget word statements belongs to (?evokes?)
theframe STATEMENT.
Two constituents that fill slotsof the frame (SPEAKER and TOPIC) are annotatedas well.As usual in these cases, [both parties]SPEAKERagreed to make no further statements [on thematter]TOPIC.Figure 1: A sentence from the FrameNet examplecorpus.436The initial versions of FrameNet were focusedon describing situations and events, i.e.
typicallyverbs and their nominalizations.
Currently, how-ever, FrameNet defines frames for a wider range ofsemantic relations that can be thought of as predi-cate/argument structures, including descriptions ofevents, states, properties, and objects.FrameNet consists of the following main parts:?
An ontology consisting of a set of frames,frame elements for each frame, and rela-tions (such as inheritance and causative-of)between frames.?
A list of lexical units, that is word formspaired with their corresponding frames.
Theframe is used to distinguish between differ-ent senses of the word, although the treatmentof polysemy in FrameNet is relatively coarse-grained.?
A collection of example sentences that pro-vide lexical evidence for the frames and thecorresponding lexical units.
Although thiscorpus is not intended to be representative, itis typically used as a training corpus whencontructing automatic FrameNet labelers.1.2 Related WorkSince training data is often a scarce resource formost languages other than English, a wide rangeof methods have been proposed to reduce the needfor manual annotation.
Many of these have reliedon existing resources for English and a transfermethod based on word alignment in a parallel cor-pus to automatically create an annotated corpus ina new language.
Although these data are typicallyquite noisy, they have been used to train automaticsystems.For the particular case of transfer of FrameNetannotation, there have been a few projects thathave studied transfer methods and evaluated thequality of the automatically produced corpus.
Jo-hansson and Nugues (2005) applied the word-based methods of Yarowsky et al (2001) and ob-tained promising results.
Another recent effort(Pad?
and Lapata, 2005) demonstrates that deeperlinguistic information, such as parse trees in thesource and target language, is very beneficial forthe process of FrameNet annotation transfer.A rather different method to construct bilingualsemantic role annotation is the approach taken byBiFrameNet (Fung and Chen, 2004).
In that work,annotated structures in a new language (in thatcase Chinese) are produced by mining for similarstructures rather than projecting them via parallelcorpora.2 Automatic Annotation of a SwedishTraining Corpus2.1 Training an English Semantic RoleLabelerWe selected the 150 most frequent frames inFrameNet and applied the Collins parser (Collins,1999) to the example sentences for these frames.We built a conventional FrameNet parser for En-glish using 100,000 of these sentences as a train-ing set and 8,000 as a development set.
The classi-fiers were based on Support Vector Machines thatwe trained using LIBSVM (Chang and Lin, 2001)with the Gaussian kernel.
When testing the sys-tem, we did not assume that the frame was knowna priori.
We used the available semantic roles forall senses of the target word as features for theclassifier.On a test set from FrameNet, we estimated thatthe system had a precision of 0.71 and a recall of0.65 using a strict scoring method.
The result isslightly lower than the best systems at Senseval-3 (Litkowski, 2004), possibly because we used alarger set of frames, and we did not assume thatthe frame was known a priori.2.2 Transferring the AnnotationWe produced a Swedish-language corpus anno-tated with FrameNet information by applyingthe SRL system to the English side of Europarl(Koehn, 2005), which is a parallel corpus that isderived from the proceedings of the European Par-liament.
We projected the bracketing of the targetwords and the frame elements onto the Swedishside of the corpus by using the Giza++ wordaligner (Och and Ney, 2003).
Each word on theEnglish side was mapped by the aligner onto a(possibly empty) set of words on the Swedish side.We used the maximal span method to infer thebracketing on the Swedish side, which means thatthe span of a projected entity was set to the rangefrom the leftmost projected token to the rightmost.Figure 2 shows an example of this process.To make the brackets conform to the FrameNetannotation practices, we applied a small set ofheuristics.
The FrameNet conventions specify thatlinking words such as prepositions and subordinat-437SPEAKER express MESSAGE[We]             wanted to               [our perplexity as regards these points]             [by abstaining in committee]MEANSMEANS SPEAKER[Genom att avst?
fr?n att r?sta i utskottet]           har [vi]            velat                [denna v?r tveksamhet]uttrycka MESSAGEFigure 2: Example of projection of FrameNet annotation.ing conjunctions should be included in the brack-eting.
However, since constructions are not iso-morphic in the sentence pair, a linking word onthe target side may be missed by the projectionmethod since it is not present on the source side.For example, the sentence the doctor was answer-ing an emergency phone call is translated intoSwedish as doktorn svarade p?
ett larmsamtal,which uses a construction with a preposition p??to/at/on?
that has no counterpart in the Englishsentence.
The heuristics that we used are spe-cific for Swedish, although they would probablybe very similar for any other language that usesa similar set of prepositions and connectives, i.e.most European languages.We used the following heuristics:?
When there was only a linking word (preposi-tion, subordinating conjunction, or infinitivemarker) between the FE and the target word,it was merged with the FE.?
When a Swedish FE was preceded by a link-ing word, and the English FE starts with sucha word, it was merged with the FE.?
We used a chunker and adjusted the FEbrackets to include only complete chunks.?
When a Swedish FE crossed the target word,we used only the part of the FE that was onthe right side of the target.In addition, some bad annotation was discardedbecause we obviously could not use sentenceswhere no counterpart for the target word could befound.
Additionally, we used only the sentenceswhere the target word was mapped to a noun, verb,or an adjective on the Swedish side.Because of homonymy and polysemy problems,applying a SRL system without knowing targetwords and frames a priori necessarily introducesnoise into the automatically created training cor-pus.
There are two kinds of word sense ambigu-ity that are problematic in this case: the ?internal?ambiguity, or the fact that there may be more thanone frame for a given target word; and the ?exter-nal?
ambiguity, where frequently occurring wordsenses are not listed in FrameNet.
To sidestep theproblem of internal ambiguity, we used the avail-able semantic roles for all senses of the target wordas features for the classifier (as described above).Solving the problem of external ambiguity wasoutside the scope of this work.Some potential target words had to be ignoredsince their sense ambiguity was too difficult toovercome.
This category includes auxiliaries suchas be and have, as well as verbs such as take andmake, which frequently appear as support verbsfor nominal predicates.2.3 MotivationAlthough the meaning of the two sentences ina sentence pair in a parallel corpus should beroughly the same, a fundamental question iswhether it is meaningful to project semanticmarkup of text across languages.
Equivalentwords in two different languages sometimes ex-hibit subtle but significant semantic differences.However, we believe that a transfer makes sense,since the nature of FrameNet is rather coarse-grained.
Even though the words that evoke a framemay not have exact counterparts, it is probable thatthe frame itself has.For the projection method to be meaningful, wemust make the following assumptions:?
The complete frame ontology in the EnglishFrameNet is meaningful in Swedish as well,and each frame has the same set of semanticroles and the same relations to other frames.?
When a target word evokes a certain frame inEnglish, it has a counterpart in Swedish thatevokes the same frame.?
Some of the FEs on the English side havecounterparts with the same semantic roles onthe Swedish side.438In addition, we made the (obviously simplistic)assumption that the contiguous entities we projectare also contiguous on the target side.These assumptions may all be put into ques-tion.
Above all, the second assumption will failin many cases because the translations are not lit-eral, which means that the sentences in the pairmay express slightly different information.
Thethird assumption may be invalid if the informationexpressed is realized by radically different con-structions, which means that an argument may be-long to another predicate or change its semanticrole on the Swedish side.
Pad?
and Lapata (2005)avoid this problem by using heuristics based on atarget-language FrameNet to select sentences thatare close in meaning.
Since we have no such re-source to rely on, we are forced to accept that thisproblem introduces a certain amount of noise intothe automatically annotated corpus.3 Training a Swedish SRL SystemUsing the transferred FrameNet annotation, wetrained a SRL system for Swedish text.
Like mostprevious systems, it consists of two parts: a FEbracketer and a classifier that assigns semanticroles to FEs.
Both parts are implemented as SVMclassifiers trained using LIBSVM.
The semanticrole classifier is rather conventional and is not de-scribed in this paper.To construct the features used by the classifiers,we used the following tools:?
An HMM-based POS tagger,?
A rule-based chunker,?
A rule-based time expression detector,?
Two clause identifiers, of which one is rule-based and one is statistical,?
The MALTPARSER dependency parser(Nivre et al, 2004), trained on a 100,000-word Swedish treebank.We constructed shallow parse trees using theclause trees and the chunks.
Dependency and shal-low parse trees for a fragment of a sentence fromour test corpus are shown in Figures 3 and 4, re-spectively.
This sentence, which was translatedfrom an English sentence that read the doctor wasanswering an emergency phone call, comes fromthe English FrameNet example corpus.doktorn svarade p?
ett larmsamtalSUB ADVPRDETFigure 3: Example dependency parse tree.
[ doktorn ] svarade[ ] larmsamtal[[ ett ]NG_nomPPp?
]VG_finNG_nom Clause[ ]Figure 4: Example shallow parse tree.3.1 Frame Element Bracketing MethodsWe created two redundancy-based FE bracket-ing algorithms based on binary classification ofchunks as starting or ending the FE.
This is some-what similar to the chunk-based system describedby Pradhan et al (2005a), which uses a segmenta-tion strategy based on IOB2 bracketing.
However,our system still exploits the dependency parse treeduring classification.We first tried the conventional approach to theproblem of FE bracketing: applying a parser to thesentence, and classifying each node in the parsetree as being an FE or not.
We used a dependencyparser since there is no constituent-based parseravailable for Swedish.
This proved unsuccessfulbecause the spans of the dependency subtrees fre-quently were incompatible with the spans definedby the FrameNet annotations.
This was especiallythe case for non-verbal target words and when thehead of the argument was above the target word inthe dependency tree.
To be usable, this approachwould require some sort of transformation, possi-bly a conversion into a phrase-structure tree, to beapplied to the dependency trees to align the spanswith the FEs.
Preliminary investigations were un-successful, and we left this to future work.We believe that the methods we developed aremore suitable in our case, since they base theirdecisions on several parse trees (in our case, twoclause-chunk trees and one dependency tree).
Thisredundancy is valuable because the dependencyparsing model was trained on a treebank of just100,000 words, which makes it less robust thanCollins?
or Charniak?s parsers for English.
In ad-dition, the methods do not implicitly rely on thecommon assumption that every FE has a counter-part in a parse tree.
Recent work in semantic rolelabeling, see for example Pradhan et al (2005b),has focused on combining the results of SRL sys-tems based on different types of syntax.
Still, all439systems exploiting recursive parse trees are basedon binary classification of nodes as being an argu-ment or not.The training sets used to train the final classi-fiers consisted of one million training instances forthe start classifier, 500,000 for the end classifier,and 272,000 for the role classifier.
The featuresused by the classifiers are described in Subsec-tion 3.2, and the performance of the two FE brack-eting algorithms compared in Subsection 4.2.3.1.1 Greedy start-endThe first FE bracketing algorithm, the greedystart-end method, proceeds through the sequenceof chunks in one pass from left to right.
For eachchunk opening bracket, a binary classifier decidesif an FE starts there or not.
Similarly, another bi-nary classifier tests chunk end brackets for endsof FEs.
To ensure compliance to the FrameNetannotation standard (bracket matching, and no FEcrossing the target word), the algorithm inserts ad-ditional end brackets where appropriate.
Pseu-docode is given in Algorithm 1.Algorithm 1 Greedy BracketingInput: A list L of chunks and a target word tBinary classifiers starts and endsOutput: The sets S and E of start and end bracketsSplit L into the sublists Lbefore , Ltarget , and Lafter , which correspondto the parts of the list that is before, at, and after the target word, respectively.Initialize chunk-open to FALSEfor Lsub in {Lbefore, Ltarget, Lafter} dofor c in Lsub doif starts(c) thenif chunk-open thenAdd an end bracket before c to Eend ifchunk-open?
TRUEAdd a start bracket before c to Send ifif chunk-open ?
(ends(c) ?
c is final in Lsub) thenchunk-open?
FALSEAdd an end bracket after c to Eend ifend forend forFigure 5 shows an example of this algorithm,applied to the example fragment.
The small brack-ets correspond to chunk boundaries, and the largebrackets to FE boundaries that the algorithm in-serts.
In the example, the algorithm inserts an endbracket after the word doktorn ?the doctor?, sinceno end bracket was found before the target wordsvarade ?was answering?.3.1.2 Globally optimized start-endThe second algorithm, the globally optimizedstart-end method, maximizes a global probabilityscore over each sentence.
For each chunk open-ing and closing bracket, probability models assignSTART[ ] svarade [...  [doktorn]                    [p?]
[ett larmsamtal]   ...]Additional END inserted ENDSTARTFigure 5: Illustration of the greedy start-endmethod.the probability of an FE starting (or ending, re-spectively) at that chunk.
The probabilities areestimated using the built-in sigmoid fitting meth-ods of LIBSVM.
Making the somewhat unrealis-tic assumption of independence of the brackets,the global probability score to maximize is de-fined as the product of all start and end proba-bilities.
We added a set of constraints to ensurethat the segmentation conforms to the FrameNetannotation standard.
The constrained optimiza-tion problem is then solved using the JACOP fi-nite domain constraint solver (Kuchcinski, 2003).We believe that an n-best beam search methodwould produce similar results.
The pseudocodefor the method can be seen in Algorithm 2.
Thedefinitions of the predicates no-nesting andno-crossing, which should be obvious, areomitted.Algorithm 2 Globally Optimized BracketingInput: A list L of chunks and a target word tProbability models P?starts and P?endsOutput: The sets Smax and Emax of start and end bracketslegal(S, E) ?
|S| = |E|?
max(E) > max(S) ?min(S) < min(E)?
no-nesting(S, E) ?
no-crossing(t, S, E)score(S, E) ?
?c?S P?starts(c) ??c?L\S(1?
P?starts(c))?
?c?E P?ends(c) ?
?c?L\E(1 ?
P?ends(c))(Smax, Emax)?
argmax{legal(S,E)}score(S, E)Figure 6 shows an example of the globally op-timized start-end method.
In the example, theglobal probability score is maximized by a brack-eting that is illegal because the FE starting at dok-torn is not closed before the target (0.8 ?
0.6 ?
0.6 ?0.7 ?
0.8 ?
0.7 = 0.11).
The solution of the con-strained problem is a bracketing that contains anend bracket before the target (0.8 ?
0.4 ?
0.6 ?
0.7 ?0.8 ?
0.7 = 0.075)3.2 Features Used by the ClassifiersTable 1 summarizes the feature sets used bythe greedy start-end (GSE), optimized start-end(OSE), and semantic role classification (SRC).440[ ] svarade [...  [doktorn]                    [p?]
[ett larmsamtal]   ...]P^starts1?
P^starts1?
=0.4P^startsP^starts P^startsP^starts1?Pends^Pends^ Pends^Pends^Pends^Pends^1?
1?
1?=0.4=0.6=0.3=0.7=0.7=0.3=0.8=0.2=0.6 =0.2=0.8Figure 6: Illustration of the globally optimizedstart-end method.GSE OSE SRCTarget lemma + + +Target POS + + +Voice + + +Allowed role labels + + +Position + + +Head word (HW) + + +Head POS + + +Phrase/chunk type (PT) + + +HW/POS/PT,?2 chunk window + + -Dep-tree & shallow path ?target + + +Starting paths ?target + + -Ending paths ?target + + -Path?start + - -Table 1: Features used by the classifiers.3.2.1 Conventional FeaturesMost of the features that we use have been usedby almost every system since the first well-knowndescription (Gildea and Jurafsky, 2002).
The fol-lowing of them are used by all classifiers:?
Target word (predicate) lemma and POS?
Voice (when the target word is a verb)?
Position (before or after the target)?
Head word and POS?
Phrase or chunk typeIn addition, all classifiers use the set of allowedsemantic role labels as a set of boolean features.This is needed to constrain the output to a la-bel that is allowed by FrameNet for the currentframe.
In addition, this feature has proven use-ful for the FE bracketing classifiers to distinguishbetween event-type and object-type frames.
Forevent-type frames, dependencies are often long-distance, while for object-type frames, they aretypically restricted to chunks very near the targetword.
The part of speech of the target word aloneis not enough to distinguish these two classes,since many nouns belong to event-type frames.For the phrase/chunk type feature, we useslightly different values for the bracketing caseand the role assignment case: for bracketing, thevalue of this feature is simply the type of the cur-rent chunk; for classification, it is the type of thelargest chunk or clause that starts at the leftmosttoken of the FE.
For prepositional phrases, thepreposition is attached to the phrase type (for ex-ample, the second FE in the example fragmentstarts with the preposition p?
?at/on?, which causesthe value of the phrase type feature to be PP-p?
).3.2.2 Chunk Context FeaturesSimilarly to the chunk-based PropBank ar-gument bracketer described by Pradhan et al(2005a), the start-end methods use the head word,head POS, and chunk type of chunks in a windowof size 2 on both sides of the current chunk to clas-sify it as being the start or end of an FE.3.2.3 Parse Tree Path FeaturesParse tree path features have been shown to bevery important for argument bracketing in severalstudies.
All classifiers used here use a set of suchfeatures:?
Dependency tree path from the head to thetarget word.
In the example text, the firstchunk (consisting of the word doktorn), hasthe value SUB-?
for this feature.
This meansthat to go from the head of the chunk to thetarget in the dependency graph (Figure 3),you traverse a SUB (subject) link upwards.Similarly, the last chunk (ett larmsamtal) hasthe value PR-?-ADV-?.?
Shallow path from the chunk containing thehead to the target word.
For the same chunksas above, these values are both NG_nom-?-Clause-?-VG_fin, which means that to tra-verse the shallow parse tree (Figure 4) fromthe chunk to the target, you start with aNG_nom node, go upwards to a Clausenode, and finally down to the VG_fin node.The start-end classifiers additionally use the fullset of paths (dependency and shallow paths) to thetarget word from each node starting (or ending, re-spectively) at the current chunk, and the greedyend classifier also uses the path from the currentchunk to the start chunk.4414 Evaluation of the System4.1 Evaluation CorpusTo evaluate the system, we manually translated150 sentences from the FrameNet example corpus.These sentences were selected randomly from theEnglish development set.
Some sentences were re-moved, typically because we found the annotationdubious or the meaning of the sentence difficult tocomprehend precisely.
The translation was mostlystraightforward.
Because of the extensive use ofcompounding in Swedish, some frame elementswere merged with target words.4.2 Comparison of FE Bracketing MethodsWe compared the performance of the two methodsfor FE bracketing on the test set.
Because of lim-ited time, we used smaller training sets than for thefull evaluation below (100,000 training instancesfor all classifiers).
Table 2 shows the result of thiscomparison.Greedy OptimizedPrecision 0.70 0.76Recall 0.50 0.44F?=1 0.58 0.55Table 2: Comparison of FE bracketing methods.As we can see from the Table 2, the globally op-timized start-end method increased the precisionsomewhat, but decreased the recall and made theoverall F-measure lower.
We therefore used thegreedy start-end method for our final evaluationthat is described in the next section.4.3 Final System PerformanceWe applied the Swedish semantic role labeler tothe translated sentences and evaluated the result.We used the conventional experimental settingwhere the frame and the target word were givenin advance.
The results, with approximate 95%confidence intervals included, are presented in Ta-ble 3.
The figures are precision and recall for thefull task, classification accuracy of pre-segmentedarguments, precision and recall for the bracket-ing task, full task precision and recall using theSenseval-3 scoring metrics, and finally the propor-tion of full sentences whose FEs were correctlybracketed and classified.
The Senseval-3 methoduses a more lenient scoring scheme that counts aFE as correctly identified if it overlaps with thegold standard FE and has the correct label.
Al-though the strict measures are more interesting,we include these figures for comparison with thesystems participating in the Senseval-3 Restrictedtask (Litkowski, 2004).We include baseline scores for the argumentbracketing and classification tasks, respectively.The bracketing baseline method considers non-punctuation subtrees dependent of the target word.When the target word is a verb, the baseline putsFE brackets around the words included in each ofthese subtrees1.
When the target is a noun, we alsobracket the target word token itself, and when it isan adjective, we additionally bracket its parent to-ken.
As a baseline for the argument classificationtask, every argument is assigned the most frequentsemantic role in the frame.
As can be seen fromthe table, all scores except the argument bracket-ing recall are well above the baselines.Precision (Strict scoring method) 0.67 ?
0.064Recall 0.47 ?
0.057Argument Classification Accuracy 0.75 ?
0.050Baseline 0.41 ?
0.056Argument Bracketing Precision 0.80 ?
0.055Baseline Precision 0.50 ?
0.055Argument Bracketing Recall 0.57 ?
0.057Baseline Recall 0.55 ?
0.057Precision (Senseval-3 scoring method) 0.77 ?
0.057Overlap 0.75 ?
0.039Recall 0.55 ?
0.057Complete Sentence Accuracy 0.29 ?
0.073Table 3: Results on the Swedish test set with ap-proximate 95% confidence intervals.Although the performance figures are betterthan the baselines, they are still lower than formost English systems (although higher than someof the systems at Senseval-3).
We believe thatthe main reason for the performance is the qual-ity of the data that were used to train the system,since the results are consistent with the hypoth-esis that the quality of the transferred data wasroughly equal to the performance of the Englishsystem multiplied by the figures for the transfermethod (Johansson and Nugues, 2005).
In thatexperiment, the transfer method had a precisionof 0.84, a recall of 0.81, and an F-measure of0.82.
If we assume that the transfer performanceis similar for Swedish, we arrive at a precision of0.71 ?
0.84 = 0.60, a recall of 0.65 ?
0.81 = 0.53,1This is possible because MALTPARSER produces projec-tive trees, i.e.
the words in each subtree form a contiguoussubstring of the sentence.442and an F-measure of 0.56.
For the F-measure,0.55 for the system and 0.56 for the product, thefigures match closely.
For the precision, the sys-tem performance (0.67) is significantly higher thanthe product (0.60), which suggests that the SVMlearning method handles the noisy training setrather well for this task.
The recall (0.47) is lowerthan the corresponding product (0.53), but the dif-ference is not statistically significant at the 95%level.
These figures suggest that the main efforttowards improving the system should be spent onimproving the training data.5 ConclusionWe have described the design and implementa-tion of a Swedish FrameNet-based SRL systemthat was trained using a corpus that was anno-tated using cross-language transfer from Englishto Swedish.
With no manual effort except fortranslating sentences for evaluation, we were ableto reach promising results.
To our knowledge, thesystem is the first SRL system for Swedish in liter-ature.
We believe that the methods described couldbe applied to any language, as long as there ex-ists a parallel corpus where one of the languagesis English.
However, the relatively close relation-ship between English and Swedish probably madethe task comparatively easy in our case.As we can see, the figures (especially the FEbracketing recall) leave room for improvement forthe system to be useful in a fully automatic set-ting.
Apart from the noisy training set, proba-ble reasons for this include the lower robustnessof the Swedish parsers compared to those avail-able for English.
In addition, we have noticedthat the European Parliament corpus is somewhatbiased.
For instance, a very large proportion ofthe target words evoke the STATEMENT or DIS-CUSSION frames, but there are very few instancesof the BEING_WET and MAKING_FACES frames.While training, we tried to balance the selectionsomewhat, but applying the projection methodson other types of parallel corpora (such as novelsavailable in both languages) may produce a bettertraining corpus.ReferencesCollin F. Baker, Charles J. Fillmore, and John B. Lowe.1998.
The Berkeley FrameNet Project.
In Proceed-ings of COLING-ACL?98, pages 86?90, Montr?al,Canada.Chih-Chung Chang and Chih-Jen Lin, 2001.
LIBSVM:a library for support vector machines.Michael J. Collins.
1999.
Head-driven statistical mod-els for natural language parsing.
Ph.D. thesis, Uni-versity of Pennsylvania, Philadelphia.Charles J. Fillmore.
1976.
Frame semantics andthe nature of language.
Annals of the New YorkAcademy of Sciences: Conference on the Origin andDevelopment of Language, 280:20?32.Pascale Fung and Benfeng Chen.
2004.
BiFrameNet:Bilingual frame semantics resource constructionby cross-lingual induction.
In Proceedings ofCOLING-2004.Daniel Gildea and Daniel Jurafsky.
2002.
Automaticlabeling of semantic roles.
Computational Linguis-tics, 28(3):245?288.Richard Johansson and Pierre Nugues.
2005.
Usingparallel corpora for automatic transfer of FrameNetannotation.
In Proceedings of the 1st ROMANCEFrameNet Workshop, Cluj-Napoca, Romania, 26-28July.Philipp Koehn.
2005.
Europarl: A parallel corpus forstatistical machine translation.
InMT Summit 2005.Krzysztof Kuchcinski.
2003.
Constraints-drivenscheduling and resource assignment.
ACM Transac-tions on Design Automation of Electronic Systems,8(3):355?383.Ken Litkowski.
2004.
Senseval-3 task: Automatic la-beling of semantic roles.
In Senseval-3: Third Inter-national Workshop on the Evaluation of Systems forthe Semantic Analysis of Text, pages 9?12.Joakim Nivre, Johan Hall, and Jens Nilsson.
2004.Memory-based dependency parsing.
In Proceedingsof CoNLL-2004, pages 49?56.Franz Josef Och and Hermann Ney.
2003.
A sys-tematic comparison of various statistical alignmentmodels.
Computational Linguistics, 29(1):19?51.Sebastian Pad?
and Mirella Lapata.
2005.
Cross-lingual projection of role-semantic information.
InProceedings of HLT/EMNLP 2005.Sameer Pradhan, Kadri Hacioglu, Valerie Krugler,Wayne Ward, James Martin, and Dan Jurafsky.2005a.
Support vector learning for semantic argu-ment classification.
Machine Learning, 60(1):11?39.Sameer Pradhan, Wayne Ward, Kadri Hacioglu, JamesMartin, and Daniel Jurafsky.
2005b.
Semantic rolelabeling using different syntactic views.
In Proceed-ings of ACL-2005.David Yarowsky, Grace Ngai, and Richard Wicen-towski.
2001.
Inducing multilingual text analysistools via robust projection across aligned corpora.In Proceedings of HLT 2001.443
