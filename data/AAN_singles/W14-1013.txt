Proceedings of the 3rd Workshop on Hybrid Approaches to Translation (HyTra) @ EACL 2014, pages 70?74,Gothenburg, Sweden, April 27, 2014. c?2014 Association for Computational LinguisticsA Principled Approach to Context-Aware Machine TranslationRafael E. BanchsInstitute for Infocomm Research1 Fusionopolis Way, #21-01, Singapore 138632rembanchs@i2r.a-star.edu.sgAbstractThis paper presents a new principled approach tocontext-aware machine translation.
The proposedapproach reformulates the posterior probability of atranslation hypothesis given the source input byincorporating the source-context information as anadditional conditioning variable.
As a result, a newmodel component, which is referred to as thecontext-awareness model, is added into the originalnoisy channel framework.
A specific computation-al implementation for the new model component isalso described along with its main properties andlimitations.1 IntroductionIt is well known that source-context informationplays a significant role in human-based languagetranslation (Padilla and Bajo, 1998).
A similarclaim can be supported for the case of MachineTranslation on the grounds of the DistributionalHypothesis (Firth, 1957).
According to the Dis-tributional Hypothesis, much of the meaning of agiven word is implied by its context rather thanby the word itself.In this work, we first focus our attention on thefact that the classical formulation of the statis-tical machine translation framework, implicitlydisregards the role of source-context informationwithin the translation generation process.
Basedon this, we propose a principled reformulationthat allows for introducing context-awarenessinto the statistical machine translation frame-work.
Then, a specific computational implement-ation for the newly proposed model is derivedand described, along with its main properties andlimitations.The remainder of the paper is structured asfollows.
First, in section 2, the theoretical back-ground and motivation for this work are present-ed.
Then, in section 3, the proposed modelderivation is described.
In section 4, a specificcomputational implementation for the model isprovided.
And, finally in section 5, main conclu-sions and future research work are presented.2 Theoretical BackgroundAccording to the original formulation of thetranslation problem within the statistical frame-work, the decoding process is implemented bymeans of a probability maximization mechanism:??
=  argmax?
?(?|?)
(1)which means that the most likely translation ?
?for a source sentence ?
is provided by thehypothesis ?
that maximizes the conditionalprobability of ?
given ?.Furthermore, by considering the noisy channelapproach introduced in communications theory,the formulation in (1) can be rewritten as:??
=  argmax?
?(?|?)
?(?)
(2)where the likelihood ?(?|?)
is referred to as thetranslation model and the prior ?(?)
is referredto as the language model.Notice from the resulting formulation in (2)that, as the maximization runs over the trans-lation hypothesis space {?
}, the evidence ?(?)
isnot accounted for.This particular consequence of the mathema-tical representation in (2) is counterintuitive tothe notion of source-context information beinguseful for selecting appropriate translations.This problem becomes more relevant when theprobability models in (2) are decomposed intosub-sentence level probabilities for operationalpurposes.
Indeed, the computational implement-ation of (2) requires the decomposition of senten-ce-level probabilities ?(?|?)
and ?(?)
into sub-sentence level probabilities ?(?|?)
and ?(?)
,were ?
and ?
refer to sub-sentence units, such aswords or groups of words.In the original problem formulation (Brown etal., 1993), the sentence-level translation model?(?|?)
in (2) is approximated by means of word-level probabilities, and the sentence-level langua-ge model ?(?)
is approximated by means ofword n-gram probabilities.70Within this framework, translation probabilitiesat the sentence-level are estimated from word-level probabilities as follows1?(?|?)
=  ?
?
?(??|??)??
(3):where ??
and ??
refer to individual wordsoccurring in ?
and ?
, respectively.
The proba-bilities ?(??|??)
are referred to as lexical modelsand they represent the probability of an indi-vidual source word ??
to be the translation of agiven target word ??
.
These lexical models areestimated by using word alignment probabilities.In statistical phrase-based translation (Koehn etal., 2003), the translation model is approximatedby means of phrase-level probabilities (a phraseis a bilingual pair of sub-sentence units that isconsistent with the word alignments).Within this framework, translation probabilitiesat the sentence-level are computed from phrase-level probabilities as follows:?(?|?)
=  ?
?(??|??)?
(4)where ??
and ??
refer to phrases (i.e.
groups ofwords) occurring in ?
and ?
, respectively.
Theprobabilities ?(??|??)
are estimated by means ofrelative frequencies and, accordingly, they arereferred to as relative frequency models.Finally, in (Och and Ney, 2002), the maximumentropy framework was introduced into machinetranslation and the two-model formulation in thenoisy channel approach (2) was extended to thelog-linear combination of as many relevantmodels as can be reasonably derived from thetraining data.
In addition, the maximum entropyframework also allows for tuning the weights inthe log-linear combinations of models by meansof discriminative training.Within this framework, translation probabilitiesat the sentence-level are estimated from phrase-level probabilities as follows:?(?|?)
=1?exp{?
?
??
??(?
?, ??)?? }
(5)where ??(?
?, ??)
are referred to as feature modelsor functions, ??
are the feature weights of thelog-linear combination, and ?
is a normalizationfactor.
Notice from (5) that in the maximumentropy framework the posterior probability?(?|?)
is modeled rather than the likelihood.1 For the sake of clarity additional model componentssuch as fertility, reordering and distortion are omittedin both (3) and (4).From (3) and (4), it is clear that source-contextinformation is not taken into account duringtranslation hypothesis generation.
In such cases,the individual sub-sentence unit probabilitiesdepend only on the restricted context providedby the same sub-sentence unit level as observedfrom the training data.In the case of (5), on the other hand, someroom is left for incorporating source-contextinformation in the hypothesis generation processby means of context-aware feature models.
Thisis basically done by using features that relate theoccurrences of sub-sentence units with relevantsource-context information of lager extension.Several research works have already addressedthe problem of incorporating source contextinformation into the translation process withinthe maximum entropy framework (Carpuat andWu, 2007; Carpuat and Wu 2008; Haque et al.2009; Espa?a-Bonet et al.
2009; Costa-juss?
andBanchs 2010; Haque et al.
2010; Banchs andCosta-juss?
2011).In the following section, we will reformulatethe translation problem, as originally describedin (1), in order to provide a principled approachto context-aware machine translation for both thenoisy channel and the phrase-based approaches.As seen later, this will result in the incorporationof a new model component, which can be alsoused as a feature function within the context ofthe maximum entropy framework.3 Model DerivationIn our proposed formulation for context-awaremachine translation, we assume that the mostlikely translation ??
for a source sentence ?
doesnot depends on ?
only, but also on the context ?in which ?
occurs.
While this information mightbe not too relevant when estimating probabilitiesat the sentence level, it certainly becomes a veryuseful evidence support at the sub-sentence level.Based on this simple idea, we can reformulatethe mathematical representation of the translationproblem presented in (1) as follows:??
=  argmax?
?(?|?,?)
(6)where ?(?|?,?)
is the conditional probability ofa translation hypothesis ?
given the sourcesentence ?
and the context ?
in which ?
occurs.This means that the most likely translation ??
fora source sentence ?
is provided by the hypothesis?
that maximizes the conditional probability of ?given ?
and ?.71For now, let us just consider the context to beany unit of source language information withlarger span than the one of the units used torepresent ?
.
For instance, if ?
is a sentence, ?can be either a paragraph or a full document; if ?is a sub-sentence unit, ?
can be a sentence; andso on.From the theoretical point of view, the formula-tion in (6) is supported by the assumptions of theDistributional Hypothesis, which states thatmeaning is mainly derived from context ratherthan from individual language units.
Accordingto this, the formulation in (6) allows for incor-porating context information into the translationgeneration process, in a similar way humans takesource-context information into account whenproducing a translation.After some mathematical manipulations, theconditional probability in (6) can be rewritten asfollows:?(?|?,?)
=?(?|?,?)
?(?|?)
?(?)?(?|?)
?(?
)(7)where ?(?|?)
and ?(?)
are the same translationand language model probabilities as in (2), and?(?|?,?)
is the conditional probability of thesource-context ?
given the translation pair ??,?
?.Notice that if the translation pair is independentof the context, i.e.
??,??
?
?, then (7) reduces to:?(?|?,?)
=?(?|?)
?(?)?(?
)(8)and the context-aware formulation in (6) reducesto the noisy channel formulation presentedearlier in (2).If we assume, on the other hand, that thetranslation pair is not independent of the context,the formulation in (6) can be rewritten in termsof (7) as follows:??
=  argmax?
?(?|?,?)
?(?|?)
?(?)
(9)As seen from (2) and (9), the proposed context-aware machine translation formulation is similarto the noisy channel approach formulation withthe difference that a new probability model hasbeen introduced: ?(?|?,?).
This new model willbe referred to as the context-awareness model,and it acts as a complementary model, whichfavors those translation hypotheses ?
for whichthe current source context ?
is highly probablegiven the translation pair ??,?
?.In the same way translation probabilities?(?|?)
at the sentence-level can be estimatedfrom lower-level unit probabilities, such as wordor phrases, context-awareness probabilities at thesentence-level can be also estimated from lower-level unit probabilities.
For instance, ?(?|?,?
)can be approximated by means of phrase-levelprobabilities according to the following equation:?(?|?,?)
=  ?
?(?|?
?, ??)?
(10)where ??
and ??
refer to phrase pairs occurring in?
and ?, respectively, and ?
is the source-contextfor the translation under consideration.In the following section we develop a specificcomputational implementation for estimating theprobabilities of the context-awareness model.4 Model ImplementationBefore developing a specific implementation forthe context-awareness model in (10), we need todefine what type of units ??
and ??
will be usedand what kind of source-context information ?will be taken into account.Here, we will consider the phrase-basedmachine translation scenario, where phrase pairs<?
?, ?
?> are used as the building blocks of thetranslation generation process.
Accordingly, andin order to be relevant, the span of the contextinformation to be used must be larger than theone implicitly accounted for by the phrases.Typically, phrases span vary from one toseveral words, but most of the time they remainwithin the sub-sentence level.
Then, a contextdefinition at the sentence-level should be appro-priate for the purpose of estimating context-awareness probabilities at the phrase-level.
Inthis way, we can consider the context evidence ?to be the same sentence being translated ?.With these definitions on place, we can nowpropose a maximum likelihood approach forestimating context-awareness probabilities at thephrase-level.
According to this, the probabilitiescan be computed by using relative frequencies asfollows:?(?|?
?, ??)
=?????
(?,??,??)?????
(??,??
)(11)where the numerator accounts for the number oftimes the phrase pair <?
?, ?
?> has been seen alongwith context ?
in the training data, and thedenominator accounts for the number of timesthe phrase pair <?
?, ?
?> has been seen along withany context in the training data.While the computation of the denominator in(11) is trivial, i.e.
it just needs to count the72number of times <?
?, ?
?> occurs in the paralleltext, the computation of the numerator requirescertain consideration.Indeed, if we consider the context to be thesource sentence being translated ?, counting thenumber of times a phrase pair <?
?, ?
?> has beenseen along with context ?
implies that ?
isexpected to appear several times in the trainingdata.
In practice, this rarely occurs!
According tothis, the counts for the numerator in (11) will bezero most of the time (when the sentence beingtranslated is not contained in the training data)or, eventually, one (when the sentence beingtranslated is contained in the training data).Moreover, if the sentence being translated iscontained in the training data, then its translationis already known!
So, why do we need togenerate any translation at all?To circumvent this apparent inconsistency ofthe model, and to compute proper estimates forthe values of ?????
(?, ?
?, ??)
, our proposedmodel implementation uses fractional counts.This means that, instead of considering integercounts of exact occurrences of the context ?within the training data, we will consider frac-tional counts to account for the occurrences ofcontexts that are similar to ?.
In order to servethis purpose, a similarity metric within the rangefrom zero (no similarity at all) to one (maximumsimilarity) is required.In this way, for each source sentence ??,?
in thetraining data that is associated to the phrase pair<?
?, ?
?>, its corresponding fractional count wouldbe given by the similarity between ??,?
and theinput sentence being translated ?.??????(??,?)
=  ???
(?, ??,?)
(12)According to this, the numerator in (11) can beexpressed in terms of (12) as:?????
(?, ?
?, ??)
=  ?
???
(?, ??,?)?
(13)and the context-awareness probability estimatescan be computed as:?(?|?
?, ??)
=?
???(?,??,?)??
???(??,?
,??,?)?
(14)Notice that in (14) it is assumed that thenumber of times the phrase pair <?
?, ?
?> occurs inthe parallel text, i.e.
?????
(?
?, ??
), is equal to thenumber of sentence pairs containing <?
?, ??>.
Inother words, multiple occurrences of the samephrase pair within a bilingual sentence pair areaccounted for only once.Finally, two important differences between thecontext-awareness model presented here andother conventional models used in statisticalmachine translation must be highlighted.First, notice that the context-awareness modelis a dynamic model, in the sense that it has to beestimated at run-time.
In fact, as the modelprobabilities depend on the input sentence to betranslated, such probabilities cannot be computedbeforehand as in the case of other models.Second, different from the lexical models andrelative frequencies that can be computed onboth directions (source-to-target and target-to-source), a symmetric version of the context-awareness model cannot be implemented fordecoding.
This is basically because estimatingprobabilities of the form ?(?|?
?, ??)
requires theknowledge of the translation output ?, which isnot known until decoding is completed.However, the symmetric version of the context-awareness model can be certainly used at a post-processing stage, such as in n-best rescoring; or,alternatively, an incremental implementation canbe devised for its use during decoding.5 Conclusions and Future WorkWe have presented a new principled approach tocontext-aware machine translation.
The proposedapproach reformulates the posterior probabilityof a translation hypothesis given the source inputby incorporating the source-context informationas and additional conditioning variable.
As aresult, a new probability model component, thecontext-awareness model, has been introducedinto the noisy channel approach formulation.We also presented a specific computationalimplementation of the context-awareness model,in which likelihoods are estimated for the contextevidence at the phrase-level based on the use offractional counts, which can be computed bymeans of a similarity metric.Future work in this area includes efficient run-time implementations and comparative evalua-tions of different similarity metrics to be used forcomputing the fractional counts.
Similarly, acomparative evaluation between an incrementalimplementation of the symmetric version of thecontext-awareness model and its use in a post-processing stage should be also conducted.AcknowledgmentsThe author wants to thank I2R for its support andpermission to publish this work, as well as thereviewers for their insightful comments.73ReferencesBanchs, R.E., Costa-juss?, M. R. 2011.
A SemanticFeature for Statistical Machine Translation.
InProceedings of the Fifth Workshop on Syntax,Semantics and Structure in Statistical Translation,ACL HLT 2011, pp.
126-134.Brown, P., Della-Pietra, S., Della-Pietra, V., Mercer,R.
1993.
The Mathematics of Statistical MachineTranslation: Computational Linguistics 19(2), 263-- 311Carpuat, M., Wu, D. 2007.
How Phrase SenseDisambiguation Outperforms Word Sense Disam-biguation for Statistical Machine Translation.
In:11th International Conference on Theoretical andMethodological Issues in Machine Translation.SkovdeCarpuat, M., Wu, D. 2008.
Evaluation of Context-Dependent Phrasal Translation Lexicons forStatistical Machine Translation.
In: 6th Interna-tional Conference on Language Resources andEvaluation (LREC).
MarrakechCosta-juss?, M. R., Banchs, R.E.
2010.
A Vector-Space Dynamic Feature for Phrase-BasedStatistical Machine Translation.
Journal ofIntelligent Information SystemsEspa?a-Bonet, C., Gimenez, J., Marquez, L. 2009.Discriminative Phrase-Based Models for ArabicMachine Translation.
ACM Transactions on AsianLanguage Information Processing Journal (SpecialIssue on Arabic Natural Language Processing)Firth, J.R. 1957.
A synopsis of linguistic theory 1930-1955.
Studies in linguistic analysis, 51: 1-31Haque, R., Naskar, S. K., Ma, Y., Way, A.
2009.Using Supertags as Source Language Context inSMT.
In: 13th Annual Conference of the EuropeanAssociation for Machine Translation, pp.
234--241.BarcelonaHaque, R., Naskar, S. K., van den Bosh, A., Way, A.2010.
Supertags as Source Language Context inHierarchical Phrase-Based SMT.
In: 9th Con-ference of the Association for Machine Translationin the Americas (AMTA)Koehn, P., Och, F. J., Marcu, D. 2003.
StatisticalPhrase-Based Translation.
In: Human LanguageTechnology Conference and Conference onEmpirical Methods in Natural Language Proces-sing (HLTEMNLP), pp.
48--54.
EdmontonOch, F. J., Ney, H. (2002) Discriminative Trainingand Maximum Entropy Models for StatisticalMachine Translation.
In: 40th Annual Meeting ofthe Association for Computational Linguistics, pp.295--302Padilla, P., Bajo, T. (1998) Hacia un Modelo deMemoria y Atenci?n en la Interpretaci?n Simul-t?nea.
Quaderns: Revista de Traducci?
2, 107--11774
