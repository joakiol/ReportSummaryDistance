Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the ACL, pages 1?9,Boulder, Colorado, June 2009. c?2009 Association for Computational LinguisticsSubjectivity Recognition on Word Senses via Semi-supervised MincutsFangzhong SuSchool of ComputingUniversity of Leedsfzsu@comp.leeds.ac.ukKatja MarkertSchool of ComputingUniversity of Leedsmarkert@comp.leeds.ac.ukAbstractWe supplement WordNet entries with infor-mation on the subjectivity of its word senses.Supervised classifiers that operate on wordsense definitions in the same way that textclassifiers operate on web or newspaper textsneed large amounts of training data.
The re-sulting data sparseness problem is aggravatedby the fact that dictionary definitions are veryshort.
We propose a semi-supervised mini-mum cut framework that makes use of bothWordNet definitions and its relation structure.The experimental results show that it outper-forms supervised minimum cut as well as stan-dard supervised, non-graph classification, re-ducing the error rate by 40%.
In addition, thesemi-supervised approach achieves the sameresults as the supervised framework with lessthan 20% of the training data.1 IntroductionThere is considerable academic and commercial in-terest in processing subjective content in text, wheresubjective content refers to any expression of a pri-vate state such as an opinion or belief (Wiebe etal., 2005).
Important strands of work include theidentification of subjective content and the determi-nation of its polarity, i.e.
whether a favourable orunfavourable opinion is expressed.Automatic identification of subjective content of-ten relies on word indicators, such as unigrams(Pang et al, 2002) or predetermined sentiment lex-ica (Wilson et al, 2005).
Thus, the word positivein the sentence ?This deal is a positive developmentfor our company.?
gives a strong indication thatthe sentence contains a favourable opinion.
How-ever, such word-based indicators can be misleadingfor two reasons.
First, contextual indicators such asirony and negation can reverse subjectivity or po-larity indications (Polanyi and Zaenen, 2004).
Sec-ond, different word senses of a single word can ac-tually be of different subjectivity or polarity.
A typ-ical subjectivity-ambiguous word, i.e.
a word thathas at least one subjective and at least one objec-tive sense, is positive, as shown by the two examplesenses given below.1(1) positive, electropositive?having a positive electriccharge;?protons are positive?
(objective)(2) plus, positive?involving advantage or good; ?aplus (or positive) factor?
(subjective)We concentrate on this latter problem by automat-ically creating lists of subjective senses, insteadof subjective words, via adding subjectivity labelsfor senses to electronic lexica, using the exam-ple of WordNet.
This is important as the prob-lem of subjectivity-ambiguity is frequent: We (Suand Markert, 2008) find that over 30% of wordsin our dataset are subjectivity-ambiguous.
Informa-tion on subjectivity of senses can also improve othertasks such as word sense disambiguation (Wiebeand Mihalcea, 2006).
Moreover, Andreevskaia andBergler (2006) show that the performance of auto-matic annotation of subjectivity at the word level canbe hurt by the presence of subjectivity-ambiguouswords in the training sets they use.1All examples in this paper are from WordNet 2.0.1We propose a semi-supervised approach based onminimum cut in a lexical relation graph to assignsubjectivity (subjective/objective) labels to wordsenses.2 Our algorithm outperforms supervised min-imum cuts and standard supervised, non-graph clas-sification algorithms (like SVM), reducing the errorrate by up to 40%.
In addition, the semi-supervisedapproach achieves the same results as the supervisedframework with less than 20% of the training data.Our approach also outperforms prior approaches tothe subjectivity recognition of word senses and per-forms well across two different data sets.The remainder of this paper is organized as fol-lows.
Section 2 discusses previous work.
Section 3describes our proposed semi-supervised minimumcut framework in detail.
Section 4 presents the ex-perimental results and evaluation, followed by con-clusions and future work in Section 5.2 Related WorkThere has been a large and diverse body of researchin opinion mining, with most research at the text(Pang et al, 2002; Pang and Lee, 2004; Popescu andEtzioni, 2005; Ounis et al, 2006), sentence (Kimand Hovy, 2005; Kudo and Matsumoto, 2004; Riloffet al, 2003; Yu and Hatzivassiloglou, 2003) or word(Hatzivassiloglou and McKeown, 1997; Turney andLittman, 2003; Kim and Hovy, 2004; Takamura etal., 2005; Andreevskaia and Bergler, 2006; Kaji andKitsuregawa, 2007) level.
An up-to-date overview isgiven in Pang and Lee (2008).Graph-based algorithms for classification intosubjective/objective or positive/negative languageunits have been mostly used at the sentence anddocument level (Pang and Lee, 2004; Agarwal andBhattacharyya, 2005; Thomas et al, 2006), insteadof aiming at dictionary annotation as we do.
Wealso cannot use prior graph construction methodsfor the document level (such as physical proxim-ity of sentences, used in Pang and Lee (2004)) atthe word sense level.
At the word level Taka-mura et al (2005) use a semi-supervised spin modelfor word polarity determination, where the graph2It can be argued that subjectivity labels are maybe rathermore graded than the clear-cut binary distinction we assign.However, in Su and Markert (2008a) as well as Wiebe and Mi-halcea (2006) we find that human can assign the binary distinc-tion to word senses with a high level of reliability.is constructed using a variety of information suchas gloss co-occurrences and WordNet links.
Apartfrom using a different graph-based model from ours,they assume that subjectivity recognition has alreadybeen achieved prior to polarity recognition and testagainst word lists containing subjective words only.However, Kim and Hovy (2004) and Andreevskaiaand Bergler (2006) show that subjectivity recogni-tion might be the harder problem with lower humanagreement and automatic performance.
In addition,we deal with classification at the word sense level,treating also subjectivity-ambiguous words, whichgoes beyond the work in Takamura et al (2005).Word Sense Level: There are three prior ap-proaches addressing word sense subjectivity or po-larity classification.
Esuli and Sebastiani (2006) de-termine the polarity (positive/negative/objective) ofword senses in WordNet.
However, there is no eval-uation as to the accuracy of their approach.
Theythen extend their work (Esuli and Sebastiani, 2007)by applying the Page Rank algorithm to rank theWordNet senses in terms of how strongly a sensepossesses a given semantic property (e.g., positiveor negative).
Apart from us tackling subjectivityinstead of polarity, their Page Rank graph is alsoconstructed focusing on WordNet glosses (linkingglosses containing the same words), whereas weconcentrate on the use of WordNet relations.Both Wiebe and Mihalcea (2006) and our priorwork (Su and Markert, 2008) present an annota-tion scheme for word sense subjectivity and algo-rithms for automatic classification.
Wiebe and Mi-halcea (2006) use an algorithm relying on distribu-tional similarity and an independent, large manuallyannotated opinion corpus (MPQA) (Wiebe et al,2005).
One of the disadvantages of their algorithm isthat it is restricted to senses that have distributionallysimilar words in the MPQA corpus, excluding 23%of their test data from automatic classification.
Suand Markert (2008) present supervised classifiers,which rely mostly on WordNet glosses and do noteffectively exploit WordNet?s relation structure.3 Semi-Supervised Mincuts3.1 Minimum Cuts: The Main IdeaBinary classification with minimum cuts (Mincuts)in graphs is based on the idea that similar items2should be grouped in the same cut.
All items in thetraining/test data are seen as vertices in a graph withundirected weighted edges between them specifyinghow strong the similarity/association between twovertices is.
We use minimum s-t cuts: the graph con-tains two particular vertices s (source, correspondsto subjective) and t (sink, corresponds to objective)and each vertex u is connected to s and t via aweighted edge that can express how likely u is tobe classified as s or t in isolation.Binary classification of the vertices is equivalentto splitting the graph into two disconnected subsetsof all vertices, S and T with s ?
S and t ?
T .This corresponds to removing a set of edges fromthe graph.
As similar items should be in the samepart of the split, the best split is one which removesedges with low weights.
In other words, a minimumcut problem is to find a partition of the graph whichminimizes the following formula, where w(u, v) ex-presses the weight of an edge between two vertices.W (S, T ) = ?u?S,v?Tw(u, v)Globally optimal minimum cuts can be found inpolynomial time and near-linear running time inpractice, using the maximum flow algorithm (Pangand Lee, 2004; Cormen et al, 2002).3.2 Why might Semi-supervised MinimumCuts Work?We propose semi-supervised mincuts for subjectiv-ity recognition on senses for several reasons.First, our problem satisfies two major conditionsnecessary for using minimum cuts.
It is a bi-nary classification problem (subjective vs. objectivesenses) as is needed to divide the graph into twocomponents.
Our dataset alo lends itself naturallyto s-t Mincuts as we have two different views on thedata.
Thus, the edges of a vertex (=sense) to thesource/sink can be seen as the probability of a sensebeing subjective or objective without taking similar-ity to other senses into account, for example via con-sidering only the sense gloss.
In contrast, the edgesbetween two senses can incorporate the WordNet re-lation hierarchy, which is a good source of similar-ity for our problem as many WordNet relations aresubjectivity-preserving, i.e.
if two senses are con-nected via such a relation they are likely to be bothsubjective or both objective.3 An example here isthe antonym relation, where two antonyms such asgood?morally admirable and evil, wicked?morallybad or wrong are both subjective.Second, Mincuts can be easily expanded intoa semi-supervised framework (Blum and Chawla,2001).
This is essential as the existing labeleddatasets for our problem are small.
In addition,glosses are short, leading to sparse high dimensionalvectors in standard feature representations.
Also,WordNet connections between different parts of theWordNet hierarchy can also be sparse, leading torelatively isolated senses in a graph in a supervisedframework.
Semi-supervised Mincuts allow us toimport unlabeled data that can serve as bridges toisolated components.
More importantly, as the unla-beled data can be chosen to be related to the labeledand test data, they might help pull test data to theright cuts (categories).3.3 Formulation of Semi-supervised MincutsThe formulation of our semi-supervised Mincut forsense subjectivity classification involves the follow-ing steps, which we later describe in more detail.1.
We define two vertices s (source) and t (sink),which correspond to the ?subjective?
and ?ob-jective?
category, respectively.
Following thedefinition in Blum and Chawla (2001), we callthe vertices s and t classification vertices, andall other vertices (labeled, test, and unlabeleddata) example vertices.
Each example vertexcorresponds to one WordNet sense and is con-nected to both s and t via a weighted edge.
Thelatter guarantees that the graph is connected.2.
For the test and unlabeled examples, we seethe edges to the classification vertices as theprobability of them being subjective/objectivedisregarding other example vertices.
We use asupervised classifier to set these edge weights.For the labeled training examples, they are con-nected by edges with a high constant weight tothe classification vertices that they belong to.3.
WordNet relations are used to construct theedges between two example vertices.
Such3See Kamps et al (2004) for an early indication of suchproperties for some WordNet relations.3edges can exist between any pair of examplevertices, for example between two unlabeledexamples.4.
After graph construction we then employ amaximum-flow algorithm to find the minimums-t cuts of the graph.
The cut in which thesource vertex s lies is classified as ?subjective?,and the cut in which the sink vertex t lies is ?ob-jective?.We now describe the above steps in more detail.Selection of unlabeled data: Random selectionof unlabeled data might hurt the performance ofMincuts, as they might not be related to any sense inour training/test data (denoted by A).
Thus a basicprinciple is that the selected unlabeled senses shouldbe related to the training/test data by WordNet rela-tions.
We therefore simply scan each sense inA, andcollect all senses related to it via one of the WordNetrelations in Table 1.
All such senses that are not inA are collected in the unlabeled data set.Weighting of edges to the classification ver-tices: The edge weight to s and t represents howlikely it is that an example vertex is initially put inthe cut in which s (subjective) or t (objective) lies.For unlabeled and test vertices, we use a supervisedclassifier (SVM4) with the labeled data as trainingdata to assign the edge weights.
The SVM is alsoused as a baseline and its features are described inSection 4.3.
As we do not wish the Mincut to re-verse labels of the labeled training data, we assign ahigh constant weight of 5 to the edge between a la-beled vertex and its corresponding classification ver-tex, and a low weight of 0.01 to the edge to the otherclassification vertex.Assigning weights to WordNet relations: Weconnect two vertices that are linked by one ofthe ten WordNet relations in Table 1 via an edge.Not all WordNet relations we use are subjectivity-preserving to the same degree: for example, hy-ponyms (such as simpleton) of objective senses(such as person) do not have to be objective.
How-ever, we aim for high graph connectivity and wecan assign different weights to different relations4We employ LIBSVM, available at http://www.csie.ntu.edu.tw/?cjlin/libsvm/.
Linear kernel and prob-ability estimates are used in this work.to reflect the degree to which they are subjectivity-preserving.
Therefore, we experiment with twomethods of weight assignment.
Method 1 (NoSL)assigns the same constant weight of 1.0 to all Word-Net relations.Method 2 (SL) reflects different degrees of pre-serving subjectivity.
To do this, we adapt an un-supervised method of generating a large noisy setof subjective and objective senses from our previ-ous work (Su and Markert, 2008).
This methoduses a list of subjective words (SL)5 to classify eachWordNet sense with at least two subjective wordsin its gloss as subjective and all other senses as ob-jective.
We then count how often two senses re-lated via a given relation have the same or a dif-ferent subjectivity label.
The weight is computedby #same/(#same+#different).
Results are listed inTable 1.Table 1: Relation weights (Method 2)Method #Same #Different WeightAntonym 2,808 309 0.90Similar-to 6,887 1,614 0.81Derived-from 4,630 947 0.83Direct-Hypernym 71,915 8,600 0.89Direct-Hyponym 71,915 8,600 0.89Attribute 350 109 0.76Also-see 1,037 337 0.75Extended-Antonym 6,917 1,651 0.81Domain 4,387 892 0.83Domain-member 4,387 892 0.83Example graph: An example graph is shown inFigure 1.
The three example vertices correspondto the senses religious?extremely scrupulous andconscientious, scrupulous?having scruples; aris-ing from a sense of right and wrong; principled;and flicker, spark, glint?a momentary flash of lightrespectively.
The vertex ?scrupulous?
is unlabeleddata derived from the vertex ?religious?
(a test item)by the relation ?similar-to?.4 Experiments and Evaluation4.1 DatasetsWe conduct the experiments on two different goldstandard datasets.
One is the Micro-WNOp corpus,5Available at http://www.cs.pitt.edu/mpqa4scrupulousreligioussubjective objectiveflicker0.24 0.760.83 0.170.16 0.840.81similar-toFigure 1: Graph of Word Senseswhich is representative of the part-of-speech distri-bution in WordNet 6.
It includes 298 words with703 objective and 358 subjective WordNet senses.The second one is the dataset created by Wiebeand Mihalcea (2006).7 It only contains noun andverb senses, and includes 60 words with 236 ob-jective and 92 subjective WordNet senses.
As theMicro-WNOp set is larger and also contains adjec-tive and adverb senses, we describe our results inmore detail on that corpus in the Section 4.3 and4.4.
In Section 4.5, we shortly discuss results onWiebe&Mihalcea?s dataset.4.2 Baseline and EvaluationWe compare to a baseline that assigns the mostfrequent category objective to all senses, whichachieves an accuracy of 66.3% and 72.0% onMicro-WNOp and Wiebe&Mihalcea?s dataset respectively.We use the McNemar test at the significance level of5% for significance statements.
All evaluations arecarried out by 10-fold cross-validation.4.3 Standard Supervised LearningWe use an SVM classifier to compare our proposedsemi-supervised Mincut approach to a reasonable6Available at http://www.comp.leeds.ac.uk/markert/data.
This dataset was first used with a differentannotation scheme in Esuli and Sebastiani (2007) and we alsoused it in Su and Markert (2008).7Available at http://www.cs.pitt.edu/?wiebe/pubs/papers/goldstandard.total.acl06.baseline.8 Three different feature types are used.Lexical Features (L): a bag-of-words representa-tion of the sense glosses with stop word filtering.Relation Features (R): First, we use two featuresfor each of the ten WordNet relations in Table 1, de-scribing how many relations of that type the sensehas to senses in the subjective or objective part of thetraining set, respectively.
This provides a non-graphsummary of subjectivity-preserving links.
Second,we manually collected a small set (denoted bySubjSet) of seven subjective verb and noun senseswhich are close to the root in WordNet?s hypernymtree.
A typical example element of SubjSet is psy-chological feature ?a feature of the mental life of aliving organism, which indicates subjectivity for itshyponyms such as hope ?
the general feeling thatsome desire will be fulfilled.
A binary feature de-scribes whether a noun/verb sense is a hyponym ofan element of SubjSet.Monosemous Feature (M): for each sense, wescan if a monosemous word is part of its synset.
Ifso, we further check if the monosemous word is col-lected in the subjective word list (SL).
The intuitionis that if a monosemous word is subjective, obvi-ously its (single) sense is subjective.
For example,the sense uncompromising, inflexible?not makingconcessions is subjective, as ?uncompromising?
isa monosemous word and also in SL.We experiment with different combinations offeatures and the results are listed in Table 2, prefixedby ?SVM?.
All combinations perform significantlybetter than the more frequent category baseline andsimilarly to the supervised Naive Bayes classifier(see S&M in Table 2) we used in Su and Mark-ert (2008).
However, improvements by adding morefeatures remain small.In addition, we compare to a supervised classifier(see Lesk in Table 2) that just assigns each sensethe subjectivity label of its most similar sense inthe training data, using Lesk?s similarity measurefrom Pedersen?s WordNet similarity package9.
Weuse Lesk as it is one of the few measures applicableacross all parts-of-speech.8This SVM is also used to provide the edge weights to theclassification vertices in the Mincut approach.9Available at http://www.d.umn.edu/?tpederse/similarity.html.5Table 2: Results of SVM and Mincuts with different settings of featureMethod Subjective Objective AccuracyPrecision Recall F-score Precision Recall F-scoreBaseline N/A 0 N/A 66.3% 100% 79.7% 66.3%S&M 66.2% 64.5% 65.3% 82.2% 83.2% 82.7% 76.9%Lesk 65.6% 50.3% 56.9% 77.5% 86.6% 81.8% 74.4%SVM-L 69.6% 37.7% 48.9% 74.3% 91.6% 82.0% 73.4%L-SL 82.0% 43.3% 56.7% 76.7% 95.2% 85.0% 77.7%L-NoSL 80.8% 43.6% 56.6% 76.7% 94.7% 84.8% 77.5%SVM-LM 68.9% 42.2% 52.3% 75.4% 90.3% 82.2% 74.1%LM-SL 83.2% 44.4% 57.9% 77.1% 95.4% 85.3% 78.2%LM-NoSL 83.6% 44.1% 57.8% 77.1% 95.6% 85.3% 78.2%SVM-LR 68.4% 45.3% 54.5% 76.2% 89.3% 82.3% 74.5%LR-SL 82.7% 65.4% 73.0% 84.1% 93.0% 88.3% 83.7%LR-NoSL 82.4% 65.4% 72.9% 84.0% 92.9% 88.2% 83.6%SVM-LRM 69.8% 47.2% 56.3% 76.9% 89.6% 82.8% 75.3%LRM-SL 85.5% 65.6% 74.2% 84.4% 94.3% 89.1% 84.6%LRM-NoSL 84.6% 65.9% 74.1% 84.4% 93.9% 88.9% 84.4%1 L, R and M correspond to the lexical, relation and monosemous features respectively.2 SVM-L corresponds to using lexical features only for the SVM classifier.
Likewise, SVM-LRM corresponds to using a combination for lexical, relation, and monosemous featuresfor the SVM classifier.3 L-SL corresponds to the Mincut that uses only lexical features for the SVM classifier,and subjective list (SL) to infer the weight of WordNet relations.
Likewise, LM-NoSLcorresponds to the Mincut algorithm that uses lexical and monosemous features for theSVM, and predefined constants for WordNet relations (without subjective list).4.4 Semi-supervised Graph MincutsUsing our formulation in Section 3.3, we import3,220 senses linked by the ten WordNet relations toany senses in Micro-WNOp as unlabeled data.
Weconstruct edge weights to classification vertices us-ing the SVM discussed above and use WordNet re-lations for links between example vertices, weightedby either constants (NoSL) or via the method illus-trated in Table 1 (SL).
The results are also summa-rized in Table 2.
Semi-supervised Mincuts alwayssignificantly outperform the corresponding SVMclassifiers, regardless of whether the subjectivity listis used for setting edge weights.
We can also seethat we achieve good results without using any otherknowledge sources (setting LR-NoSL).The example in Figure 1 explains why semi-supervised Mincuts outperforms the supervised ap-proach.
The vertex ?religious?
is initially assignedthe subjective/objective probabilities 0.24/0.76 bythe SVM classifier, leading to a wrong classification.However, in our graph-based Mincut framework, thevertex ?religious?
might link to other vertices (forexample, it links to the vertex ?scrupulous?
in theunlabeled data by the relation ?similar-to?).
Themincut algorithm will put vertices ?religious?
and?scrupulous?
in the same cut (subjective category) asthis results in the least cost 0.93 (ignoring the cost ofassigning the unrelated sense of ?flicker?).
In otherwords, the edges between the vertices are likely tocorrect some initially wrong classification and pullthe vertices into the right cuts.In the following we will analyze the best mini-mum cut algorithm LRM-SL in more detail.
Wemeasure its accuracy for each part-of-speech in theMicro-WNOp dataset.
The number of noun, adjec-tive, adverb and verb senses in Micro-WNOp is 484,265, 31 and 281, respectively.
The result is listedin Table 3.
The significantly better performance ofsemi-supervised mincuts holds across all parts-of-speech but the small set of adverbs, where there isno significant difference between the baseline, SVMand the Mincut algorithm.6Table 3: Accuracy for Different Part-Of-SpeechMethod Noun Adjective Adverb VerbBaseline 76.9% 61.1% 77.4% 72.6%SVM 81.4% 63.4% 83.9% 75.1%Mincut 88.6% 78.9% 77.4% 84.0%We will now investigate how LRM-SL performswith different sizes of labeled and unlabeled data.All learning curves are generated via averaging 10learning curves from 10-fold cross-validation.Performance with different sizes of labeled data:we randomly generate subsets of labeled data A1,A2... An, and guarantee that A1 ?
A2... ?
An.Results for the best SVM (LRM) and the best min-imum cut (LRM-SL) are listed in Table 4, and thecorresponding learning curve is shown in Figure 2.As can be seen, the semi-supervised Mincuts isconsistently better than SVM.
Moreover, the semi-supervised Mincut with only 200 labeled data itemsperforms even better than SVM with 954 trainingitems (78.9% vs 75.3%), showing that our semi-supervised framework allows for a training data re-duction of more than 80%.Table 4: Accuracy with different sizes of labeled data# labeled data SVM Mincuts100 69.1% 72.2%200 72.6% 78.9%400 74.4% 82.7%600 75.5% 83.7%800 76.0% 84.1%900 75.6% 84.8%954 (all) 75.3% 84.6%Performance with different sizes of unlabeleddata: We propose two different settings.Option1: Use a subset of the ten relations togenerate the unlabeled data (and edges betweenexample vertices).
For example, we first use{antonym, similar-to} only to obtain a unlabeleddataset U1, then use a larger subset of the relationslike {antonym, similar-to, direct-hyponym, direct-hypernym} to generate another unlabeled datasetU2, and so forth.
Obviously, Ui is a subset of Ui+1.Option2: Use all the ten relations to generate theunlabeled data U .
We then randomly select subsetsof U , such as subset U1, U2 and U3, and guaranteethat U1 ?
U2 ?
U3 ?
.
.
.
U .6871747780838689100  200  300  400  500  600  700  800  900 1000Accuracy(%)Size of Labeled DataMincutsSVMFigure 2: Learning curve with different sizes of labeleddataThe results are listed in Table 5 and Table 6 re-spectively.
The corresponding learning curves areshown in Figure 3.
We see that performance im-proves with the increase of unlabeled data.
In addi-tion, the curves seem to converge when the size ofunlabeled data is larger than 3,000.
From the resultsin Tabel 5 one can also see that hyponymy is the re-lation accounting for the largest increase.Table 6: Accuracy with different sizes of unlabeled data(random selection)# unlabeled data Accuracy0 75.9%200 76.5%500 78.6%1000 80.2%2000 82.8%3000 84.0%3220 84.6%Furthermore, these results also show that a super-vised mincut without unlabeled data performs onlyon a par with other supervised classifiers (75.9%).The reason is that if we exclude the unlabeled data,there are only 67 WordNet relations/edges betweensenses in the small Micro-WNOp dataset.
In con-trast, the use of unlabeled data adds more edges(4,586) to the graph, which strongly affects thegraph cut partition (see also Figure 1).4.5 Comparison to Prior ApproachesIn our previous work (Su and Markert, 2008), we re-port 76.9% as the best accuracy on the same Micro-7Table 5: Accuracy with different sizes of unlabeled data from WordNet relationRelation # unlabeled data Accuracy{?}
0 75.3%{similar-to} 418 79.1%{similar-to, antonym} 514 79.5%{similar-to, antonym, direct-hypernym, direct-hyponym}2,721 84.4%{similar-to, antonym, direct-hypernym, direct-hyponym, also-see, extended-antonym}3,004 84.4%{similar-to, antonym, direct-hypernym, direct-hyponym, also-see, extended-antonym, derived-from,attribute, domain, domain-member}3,220 84.6%75777981838587890  500  1000  1500  2000  2500  3000  3500Accuracy(%)Size of Unlabeled DataOption1Option2Figure 3: Learning curve with different sizes of unlabeleddataWNOp dataset used in the previous sections, using asupervised Naive Bayes (S&M in Tabel 2).
Our bestresult from Mincuts is significantly better at 84.6%(see LRM-SL in Table 2).For comparison to Wiebe and Mihalcea (2006),we use their dataset for testing, henceforth calledWiebe (see Section 4.1 for a description).
Wiebeand Mihalcea (2006) report their results in precisionand recall curves for subjective senses, such as a pre-cision of about 55% at a recall of 50% for subjectivesenses.
Their F-score for subjective senses seems toremain relatively static at 0.52 throughout their pre-cision/recall curve.We run our best Mincut LRM-SL algorithm withtwo different settings on Wiebe.
Using Micro-WNOp as training set and Wiebe as test set, weachieve an accuracy of 83.2%, which is similar to theresults on the Micro-WNOp dataset.
At the recall of50% we achieve a precision of 83.6% (in compari-son to their precision of 55% at the same recall).
OurF-score is 0.63 (vs. 0.52).To check whether the high performance is just dueto our larger training set, we also conduct 10-foldcross-validation on Wiebe.
The accuracy achievedis 81.1% and the F-score 0.56 (vs. 0.52), suggestingthat our algorithm performs better.
Our algorithmcan be used on all WordNet senses whereas theirs isrestricted to senses that have distributionally similarwords in the MPQA corpus (see Section 2).
How-ever, they use an unsupervised algorithm i.e.
theydo not need labeled word senses, although they doneed a large, manually annotated opinion corpus.5 Conclusion and Future WorkWe propose a semi-supervised minimum cut algo-rithm for subjectivity recognition on word senses.The experimental results show that our proposed ap-proach is significantly better than a standard super-vised classification framework as well as a super-vised Mincut.
Overall, we achieve a 40% reductionin error rates (from an error rate of about 25% to anerror rate of 15%).
To achieve the results of standardsupervised approaches with our model, we need lessthan 20% of their training data.
In addition, we com-pare our algorithm to previous state-of-the-art ap-proaches, showing that our model performs betteron the same datasets.Future work will explore other graph construc-tion methods, such as the use of morphological re-lations as well as thesaurus and distributional sim-ilarity measures.
We will also explore other semi-supervised algorithms.8ReferencesAlekh Agarwal and Pushpak Bhattacharyya.
2005.
Sen-timent Analysis: A new Approach for Effective Useof Linguistic Knowledge and Exploiting Similaritiesin a Set of Documents to be Classified.
Proceedings ofICON?05.Alina Andreevskaia and Sabine Bergler.
2006.
MiningWordNet for Fuzzy Sentiment: Sentiment Tag Extrac-tion from WordNet Glosses.
Proceedings of EACL?06.Avrim Blum and Shuchi Chawla.
2001.
Learning fromLabeled and Unlabeled Data using Graph Mincuts.Proceedings of ICML?01.Thomas Cormen, Charles Leiserson, Ronald Rivest andClifford Stein.
2002.
Introduction to Algorithms.Second Edition, the MIT Press.Kushal Dave, Steve Lawrence, and David Pennock.2003.
Mining the Peanut Gallery: Opinion Extrac-tion and Semantic Classification of Product Reviews.Proceedings of WWW?03.Andrea Esuli and Fabrizio Sebastiani.
2006.
SentiWord-Net: A Publicly Available Lexical Resource for Opin-ion Mining.
Proceedings of LREC?06.Andrea Esuli and Fabrizio Sebastiani.
2007.
PageRank-ing WordNet Synsets: An application to Opinion Min-ing.
Proceedings of ACL?07.Vasileios Hatzivassiloglou and Kathleen McKeown.1997.
Predicting the Semantic Orientation of Adjec-tives.
Proceedings of ACL?97.Nobuhiro Kaji and Masaru Kitsuregawa.
2007.
Build-ing Lexicon for Sentiment Analysis from MassiveCollection of HTML Documents.
Proceedings ofEMNLP?07.Japp Kamps, Maarten Marx, Robert Mokken, andMaarten de Rijke.
2004.
Using WordNet to MeasureSemantic Orientation of Adjectives.
Proceedings ofLREC?04.Soo-Min Kim and Eduard Hovy.
2004.
Determining theSentiment of Opinions.
Proceedings of COLING?04.Soo-Min Kim and Eduard Hovy.
2005.
Automatic De-tection of Opinion BearingWords and Sentences.
Pro-ceedings of ICJNLP?05.Taku Kudo and Yuji Matsumoto.
2004.
A BoostingAlgorithm for Classification of Semi-structured Text.Proceedings of EMNLP?04.Iadh Ounis, Maarten de Rijke, Craig Macdonald, GiladMishne and Ian Soboroff.
2006.
Overview of theTREC-2006 Blog Track.
Proceedings of TREC?06.Bo Pang and Lillian Lee.
2004.
A Sentiment Education:Sentiment Analysis Using Subjectivity summarizationBased on Minimum Cuts.
Proceedings of ACL?04.Bo Pang and Lillian Lee.
2008.
Opinion Mining andSentiment Analysis.
Foundations and Trends in Infor-mation Retrieval 2(1-2).Bo Pang, Lillian Lee, and Shivakumar Vaithyanathan.2002.
Thumbs up?
Sentiment Classification us-ing Machine Learning Techniques.
Proceedings ofEMNLP?02.Livia Polanyi and Annie Zaenen.
2004.
Contextual Va-lence Shifters.
Proceedings of the AAAI Spring Sym-posium on Exploring Attitude and Affect in Text: The-ories and Applications.Ana-Maria Popescu and Oren Etzioni.
2005.
Extract-ing Product Features and Opinions from Reviews Pro-ceedings of EMNLP?05.Ellen Riloff, Janyce Wiebe, and Theresa Wilson.
2003.Learning Subjective Nouns using Extraction PatternBootstrapping.
Proceedings of CoNLL?03Fangzhong Su and Katja Markert.
2008.
From Wordsto Senses: A Case Study in Subjectivity Recognition.Proceedings of COLING?08.Fangzhong Su and Katja Markert.
2008a.
Elicit-ing Subjectivity and Polarity Judgements on WordSenses.
Proceedings of COLING?08 workshop of Hu-man Judgements in Computational Linguistics.Hiroya Takamura, Takashi Inui, and Manabu Okumura.2005.
Extracting Semantic Orientations of Words us-ing Spin Model.
Proceedings of ACL?05.Matt Thomas, Bo Pang and Lilian Lee.
2006.
Getout the vote: Determining support or opposition fromCongressional floor-debate transcripts.
Proceedings ofEMNLP?06.Peter Turney.
2002.
Thumbs up or Thumbs down?
Se-mantic orientation applied to unsupervised classifica-tion of reviews.
Proceedings of ACL?02.Peter Turney and Michael Littman.
2003.
MeasuringPraise and Criticism: Inference of Semantic Orienta-tion from Association.
ACM Transaction on Informa-tion Systems.Hong Yu and Vasileios Hatzivassiloglou.
2003.
TowardsAnswering Opinion Questions: Separating Facts fromOpinions and Identifying the Polarity of Opinion Sen-tences.
Proceedings of EMNLP?03.Janyce Wiebe and Rada Micalcea.
2006.
Word Senseand Subjectivity.
Proceedings of ACL?06.Janyce Wiebe, Theresa Wilson, and Claire Cardie.
2005.Annotating Expressions of Opinions and Emotions inLanguage.
Language Resources and Evaluation.Theresa Wilson, Janyce Wiebe, and Paul Hoff-mann.
2005.
Recognizing Contextual Polarity inPhrase-Level Sentiment Analysis.
Proceedings ofHLT/EMNLP?05.9
