Proceedings of the COLING/ACL 2006 Main Conference Poster Sessions, pages 176?182,Sydney, July 2006. c?2006 Association for Computational LinguisticsA Bio-inspired Approach for Multi-Word Expression ExtractionJianyong Duan, Ruzhan LuWeilin Wu, Yi HuDepartment of Computer ScienceShanghai Jiao Tong UniversityShanghai, 200240, P.R.
Chinaduanjy@hotmail.com{lu-rz,wl-wu,huyi}@cs.sjtu.edu.cnYan TianSchool of Foreign LanguagesDepartment of Computer ScienceShanghai Jiao Tong UniversityShanghai, 200240, P.R.
Chinatianyan@sjtu.edu.cnAbstractThis paper proposes a new approach forMulti-word Expression (MWE)extractionon the motivation of gene sequence align-ment because textual sequence is simi-lar to gene sequence in pattern analy-sis.
Theory of Longest Common Subse-quence (LCS) originates from computerscience and has been established as affinegap model in Bioinformatics.
We per-form this developed LCS technique com-bined with linguistic criteria in MWE ex-traction.
In comparison with traditionaln-gram method, which is the major tech-nique for MWE extraction, LCS approachis applied with great efficiency and per-formance guarantee.
Experimental resultsshow that LCS-based approach achievesbetter results than n-gram.1 IntroductionLanguage is under continuous development.
Peo-ple enlarge vocabulary and let words carry moremeanings.
Meanwhile the language also devel-ops larger lexical units to carry specific meanings;specifically MWE?s, which include compounds,phrases, technical terms, idioms and collocations,etc.
The MWE has relatively fixed pattern becauseevery MWE denotes a whole concept.
In compu-tational view, the MWE repeats itself constantly incorpus(Taneli,2003).The extraction of MWE plays an important rolein several areas, such as machine translation (Pas-cale,1997), information extraction (Kalliopi,2000)etc.
On the other hand, there is also a needfor MWE extraction in a much more widespreadscenario namely that of human translation andtechnical writing.
Many efforts have been de-voted to the study of MWE extraction (Beat-rice,2003; Ivan,2002; Jordi,2001).
These statis-tical methods detect MWE by frequency of can-didate patterns.
Linguistic information as a filter-ing strategy is also performed to improve precisionby ranking their candidates (Violeta,2003; Ste-fan,2004; Arantza,2002).
Some measures basedon advance statistical methods are also used,such as mutual expectation with single statis-tic model (Paul,2005),C-value/NC-value method(Katerina,2000),etc.Frequent information is the original data forfurther MWE extraction.
Most approaches adoptn-gram technique(Daniel,1977; Satanjeev,2003;Makoto,1994).
n-gram concerns about one se-quence for each time.
Every sequence can becut into some segments with varied lengths be-cause any length of segment has the possibility tobecome candidate MWE.
The larger the contextwindow is, the more difficulty its parameters ac-quire.
Thus data sparseness problem deteriorates.Another problem arises from the flexible MWEwhich can be separated by an arbitrary number ofblanks, for instance, ?make.
.
.
.
.
.
decision?.
Thesemodels cannot effectively distinguish all kinds ofvariations in flexible MWE.On the consideration of relations between tex-tual sequence and gene sequence, we propose anew bio-inspired approach for MWE identifica-tion.
Both statistical and linguistic information areincorporated into this model.2 Multi-word ExpressionMulti-word Expression( in general, term) as thelinguistic representation of concepts, also hassome special statistical features.
The componentwords of terms co-occur in the same context fre-176quently.
MWE extraction can be viewed as a prob-lem of pattern extraction.
It has two major phases.The first phase is to search the candidateMWEs bytheir frequent occurrence in the corpus.
The sec-ond phase is to filter true MWEs from noise candi-dates.
Filtering process involves linguistic knowl-edge and some intelligent observations.MWE can be classified into strict patterns andflexible patterns by structures of their componentwords(Joaquim,1999).
For example, a textual se-quence s = w1w2 ?
?
?wi ?
?
?wn, may contain twokinds of patterns:Strict pattern: pi = wiwi+1wi+2Flexible pattern: pj = wiunionsqwi+2unionsqwi+4, pk =wi unionsq unionsqwi+3wi+4where unionsq denotes the variational or active ele-ment in pattern.
The flexible pattern extraction isalways a bottleneck for MWE extraction for lackof good knowledge of global solution.3 Algorithms for MWE Extraction3.1 Pure Mathematical MethodAlthough sequence alignment algorithm has beenwell-developed in bioinformatics (Michael,2003),(Knut,2000), (Hans,1999), it was rarely reportedin MWE extraction.
In fact, it also applies toMWE extraction especially for complex struc-tures.Algorithm.1.1.
Input:tokenlized textual sequences Q ={s1, s2, ?
?
?
, sn}2.
Initionalization : pool, ?
= {?k},?3.
Computation:I. Pairwise sequence alignmentfor all si, sj ?
Q, si 6= sjSimilarity(si, sj)Align(si, sj)path(li,lj)??
{li, lj , ck}pool ?
pool ?
{(li, ck), (lj , ck)}?
?
?
?
ckII.
Creation of consistent setfor all ck ?
?, (li, ck) ?
pool?k ?
?k + {li}pool ?
pool ?
(li, ck)III.
Multiple sequence alignmentfor all ?kstar align(?k) ?
MWU ?
??
?MWU4.
Output: ?Our approach is directly inspired by gene se-quence alignment as algorithm.
1. showed.
Thetextual sequence should be preprocessed before in-put.
For example, plurals recognition is a rela-tively simple task for computers which just needto check if the word accord with the general ruleincluding rule (+s) and some alternative rules (-y +ies), etc.
A set of tense forms, such as past, presentand future forms, are also transformed into origi-nal forms.
These tokenlized sequences will im-prove extraction quality.Pairwise sequence alignment is a crucial step.Our algorithm uses local alignment for textual se-quences.
The similarity score between s[1 .
.
.
i]and t[1 .
.
.
i] can be computed by three arraysG[i, j], E[i, j] ,F[i, j] and zero, where entry ?
(x, y)means word x matches with word y; V[i, j] de-notes the best score of entry ?
(x, y); G[i, j] de-notes s[i] matched with t[j]:?
(s[i], t[j]); E[i, j]denotes a blank of string s matched with t[j] :?
(unionsq, t[j]); F [i, j] denotes s[i] matched with ablank of string t : ?
(s[i],unionsq).Initialization:V [0, 0] = 0; V [i, 0] = E[i, 0] = 0; 1 ?
i ?m.
V [0, j] = F [0, j] = 0; 1 ?
j ?
n.A dynamic programming solution:V [i, j] = max{G[i, j], E[i, j], G[i, j], 0};G[i, j] = ?
(i, j) + max?????????G[i?
1, j ?
1]E[i?
1, j ?
1]F [i?
1, j ?
1]0E[i, j] = max??????????
(h + g) + G[i, j ?
1]?
g + E[i, j ?
1]?
(h + g) + F [i, j ?
1]0F [i, j] = max??????????
(h + g) + G[i?
1, j]?
(h + g) + E[i?
1, j]?
g + F [i?
1, j]0Here we explain the meaning of these arrays:I. G[i, j] includes the entry ?
(i, j), it denotesthe sum score is the last row plus the max-imal score between prefix s[1 .
.
.
i ?
1] andt[1 .
.
.
j ?
1].177II.
Otherwise the related prefixes s[1 .
.
.
i] andt[1 .
.
.
j ?
1] are needed1.
They are used tocheck the first blank or additional blank in or-der to give appropriate penalty.a.
ForG[i, j?1] and F [i, j?1], they don?tend with a blank in string s. The blanks[i] is the first blank.
Its score isG[i, j?1] (or F [i, j ?
1]) minus (h + g).b.
For E[i, j ?
1],The blank is the addi-tional blank which should be only sub-tracted g.In the maximum entry, it records the best scoreof optimum local alignment.
This entry can beviewed as the started point of alignment.
Thenwe backtrack entries by checking arrays which aregenerated from dynamic programming algorithm.When the score decrease to zero, alignment exten-sion terminates.
Finally, the similarity and align-ment results are easily acquired.Lots of aligned segments are extracted frompairwise alignment.
Those segments with com-mon component words (ck) will be collected intothe same set.
It is called as consistent set forfurther multiple sequence alignment.
These con-sistent sets collect similar sequences with scoregreater than certain threshold.We perform star-alignment in multiple se-quence alignment.
The center sequence in the con-sistent set which has the highest score in com-parison with others, is picked out from this set.Then all the other sequences gather to the cen-ter sequence with the technique of ?once a blank,always a blank?.
These aligned sequences formcommon regions with n-column or a column.
Ev-ery column contains one or more words.
Calcula-tion of dot-matrices is a widespread tool for com-mon region analysis.
Dot-plot agreement is de-veloped to identify common patterns and reliablyaligned regions in a set of related sequences.
Ifseveral plots calculate consistently in a sequenceset, it displays the similarity among them.
It in-creases credibility of extracted pattern in this con-sistent set.
Finally MWE with detailed patternemerges from this aligned sequence set.1Analysis approaches for F [i, j] and E[i, j] are the same,here only E[i, j] is given its detailed explanation.3.2 Linguistic Knowledge Combination3.2.1 Heuristic KnowledgeOriginal candidate set is noise.
Many meaning-less patterns are extracted from corpus.
Some lin-guistic rules (Argamon,1999) are introduced intoour model.
It is observed that candidate patternshould contain content words.
Some patterns areonly organized by pure function words, such as themost frequent patterns ?the to?, ?of the?.
Thesepatterns should be moved out from the candidateset.
Filter table with certain words is also per-formed.
For example, some words, like ?then?,cannot occur in the beginning position of MWE.These filters will reduce the number of noise pat-terns in great extent.3.2.2 Embedded Base Phrase detectionShort textual sequence is apt to produce frag-ments of MWE because local alignment ends pat-tern extension when similarity score reduces tozero.
The matched component words increasesimilarity score while unmatched words decreaseit.
The similarity scores of candidates in textualsequences are lower for lack of matched compo-nent words.
Without accumulation of higher sim-ilarity score, pattern extension terminates quickly.Pattern extension becomes especially sensitive tounmatched words.
Some isolated fragments aregenerated in this circumstance.
One solution is togive higher scores for matched component words.It strengthens pattern extension ability at the ex-pense of introducing noise.We propose Embedded base phrase(EBP) de-tection as algorithm.2.
It improves pattern ex-traction by giving lower penalty for longer basephrase.
EBP is the base phrase in a gap (Changn-ing,2000).
It does not contain other phrase recur-sively.
Good quality of MWE should avoid irrela-tive unit in its gap.
The penalty function discernsthe true EBP and irrelative unit in a gap only bylength information.
Longer gap means more irrel-ative unit.
It builds a rough penalty model for lackof semantic information.
We improve this modelby POS information.
POS tagged textual sequenceis convenient to grammatical analysis.
True EBP2gives comparatively lower penalty.Algorithm.2.1.
Input: LCS of sl, sk2The performance of our EBP tagger is 95% accuracy forbase noun phrase and 90% accuracy for general use.1782.
Check breakpoint in LCSi.
Anchor neighbored common words anddenote gapsfor all ws = wp, wt = wqif ws ?
ls, wt ?
lt, ls 6= ltdenote gst, gpqii.
Detect EBP in gapsgstEBP??
g?st, gpqEBP??
g?pqiii.
Compute new similariy matrix in gapssimilarity(g?st, g?pq)3.
Link broken segmentif path(g?st, g?pq)lst = ls + lt, lpq = lp + lqFor textual sequence: w1w2 ?
?
?wn, and itscorresponding POS tagged sequence: t1t2 ?
?
?
tn,we suppose [wi ?
?
?wj ] is a gap from wi to wjin sequence ?
?
?
wi?1 [wi ?
?
?wj ]wj ?
?
?
.
Thecorresponding tag sequence is [ti ?
?
?
tj ] .
Weonly focus on EBP analysis in a gap instead ofglobal sequence.
Context Free Grammar (CFG)is employed in EBP detection.
CFG rules followthis form:(1)EBP ?
adj.
+ noun(2)EBP ?
noun + ?of?
+ noun(3)EBP ?
adv.
+ adj.
(4)EBP ?
art.
+ adj.
+ noun?
?
?The sequences inside breakpoint of LCS are an-alyzed by EBP detection.
True base phrase willbe given lower penalty.
When the gap penalty forbreakpoint is lower than threshold, the broken seg-ment reunites.
Based on experience knowledge,when the length of a gap is less than four words,EBP detection using CFG can gain good results.Lower penalty for true EBP will help MWE toemerge from noise pattern easily.4 Experiments4.1 ResourcesA large amount of free texts are collected in orderto meet the need of MWE extraction.
These textsare downloaded from internet with various aspectsincluding art, entertainment, military, business,etc.
Our corpus size is 200, 000 sentences.
Theaverage sentence length is 15 words in corpus.In addition, result evaluation is a hard job.
Itsdifficulty comes from two aspects.
Firstly, MWEidentification for test corpus is a kind of labor-intensive business.
The judgment of MWEs re-quires great efforts of domain expert.
It is hard andboring to make a standard test corpus for MWEidentification use.
It is a bottleneck for large scalesuse.
Secondly it relates to human cognition in psy-chological world.
It is proved by experience thatvarious opinions cannot simply be judged true orfalse.
As a compromise way, gold standard setcan be established by some accepted resources, forexample, WordNet, as an online lexical referencesystem, including many compounds and phrases.Some terms extracted from dictionaries are alsoemployed in our experiments.
There are nearly70,000 MWEs in our list.4.2 Results and Discussion4.2.1 Close TestWe created a closed test set of 8,000 sen-tences.
MWEs in corpus are extracted by man-ual work.
Every measure in both n-gram and LCSapproaches complies with the same threshold, forexample threshold for frequency is five times.Twoconclusions are drawn from Tab.1.Firstly, LCS has higher recall than n-gram butlower precision on the contrary.
In close test set,LCS recall is higher than n-gram.
LCS unifies allthe cases of flexible patterns by GAM.
Howevern-gram only considers limited flexible patterns be-cause of model limitation.
LCS nearly includesall the n-gram results.
Higher recall decreases itsprecision to a certain extent because some flexiblepatterns are noisier than strict patterns.
Flexiblepatterns tend to be more irrelevant than strict pat-terns.
The GAM just provides a wiser choice forall flexible patterns by its gap penalty function.
N-gram gives up analysis on many flexible patternswithout further ado.
N-gram ensures its precisionby taking risk of MWE loss .Secondly, advanced evaluation criterion canplace more MWEs in the front rank of candi-date list.
Evaluation metrics for extracted pat-terns play an important role in MWE extraction.Many criteria, which are reported with better per-formances, are tested.
MWE identification is sim-ilar to IR task.
These measures have their ownadvantages to move interested patterns forwardin the candidate list.
For example, Frequencydata contains much noise.
True mutual infor-179Table 1: Close Test for N-gram and LCS ApproachesMeasure N-Gram LCSPrecision Recall F-Measure Precision Recall F-Measure(%) (%) (%) (%) (%) (%)Frequency 35.2 38.0 36.0 32.1 48.2 38.4TMI 44.7 56.2 49.1 43.2 62.1 51.4ME 51.6 52.6 51.2 44.7 65.2 52.0Rankratio 62.1 61.5 61.1 57.0 83.1 68.5mation (TMI) concerns mutual information withlogarithm(Manning,1999).
Mutual expectation(ME) takes into account the relative probability ofeach word compared to the phrase(Joaquim,1999).Rankratio performs the best on both n-gram andLCS approaches because it provides all the con-texts which associated with each word in the cor-pus and ranks them(Paul,2005).
With the help ofadvanced statistic measures, the scores of MWEsare high enough to be detected from noisy pat-terns.4.2.2 Open TestIn open test, we just show the extracted MWEnumbers in different given corpus sizes.
Two phe-nomena are observed in Fig.1.FRUSXVVL]H0:8QXPEHU         1*UDP/&6Figure 1: Open Test for N-gram and LCS Ap-proachesFirstly, with the enlargement of corpussize(every step of corpus size is 10,000 sen-tences), the detected MWE numbers increase inboth approaches.
When the corpus size reachescertain values, their increment speeds turn slower.It is reasonable on condition that MWE follownormal distribution.
In the beginning, frequentMWEs are detected easily, and the numberincreases quickly.
At a later phase, the detectiongoes into comparatively infrequent area.
Miningthese MWEs always need more corpus support.Lower increment speed appears.Secondly, although LCS always keeps ahead indetecting MWE numbers, their gaps reduce withthe increment of corpus size.
LCS is sensitiveto the MWE detection because of its alignmentmechanism in which there is no difference be-tween flexible pattern and strict pattern.
In thebeginning phase, LCS can detect MWEs whichhave high frequencies with flexible patterns.
N-gram cannot effectively catch these flexible pat-terns.
LCS detects a larger number of MWE thann-gram does.
In the latter phase, many variablepatterns for flexible MWE can also be observed,among which relatively strict patterns may appearin the larger corpus.
They will be catched byn-gram.
On the surface of observation, the dis-crepancy of detected numbers is gradually closeto LCS.
In nature, n-gram just makes up its lim-itation at the expense of corpus size because itsdetection mechanism for flexible patterns has noradical change.5 ConclusionIn this article, our LCS-based approach is inspiredby gene sequence alignment.
In a new view, wereconsider MWE extraction task.
These two taskscoincide with each other in pattern recognition.Some new phenomena in natural language are alsoobserved.
For example, we improve MWE min-ing result by EBP detection.
Comparisons withvariant n-gram approaches, which are the leadingapproaches, are performed for verifying the effec-tiveness of our approach.
Although LCS approachresults in better extraction model, a lot of im-provements for more robust model are still needed.180Each innovation presented here only opens theway for more research.
Some established theoriesbetween Computational Linguistics and Bioinfor-matics can be shared in a broader way.6 AcknowledgementsThe authors would like to thank three anony-mous reviewers for their careful reading and help-ful suggestions.
This work is supported byNational Natural Science Foundation of China(NSFC) (No.60496326) and 863 project of China(No.2001AA114210-11).
Our thanks also go toYushi Xu and Hui Liu for their coding and techni-cal support.ReferencesArantza Casillas, Raquel Mart?nez , 2002.
AligningMultiword Terms Using a Hybrid Approach.
LectureNotes in Computer Science 2276: The 3rd Interna-tional Conference of Computational Linguistics andIntelligent Text Processing.Argamon, Shlomo, Ido Dagan and Yuval Kry-molowski, 1999.
A memory based approach tolearning shallow natural language patterns.
Journalof Experimental and Theoretical AI.
11, 369-390.Beatrice Daille, 2003.
Terminology Mining.
LectureNotes in Computer Science 2700: Extraction in theWeb Era.Changning Huang, Endong Xun, Zhou Ming, 2000.A Unified Statistical Model for the Identification ofEnglish BaseNP.
The 38th Annual Meeting of theAssociation for Computational Linguistics.Daniel S. Hirschberg, 1977.
Algorithms for the LongestCommon Subsequence Problem, Journal of theACM, 24(4), 664-675.Diana Binnenpoorte, Catia Cucchiarini, Lou Bovesand Helmer Strik,2005.
Multiword expressionsin spoken language: An exploratory study onpronunciation variation.
Computer Speech andLanguage,19(4):433-449Hans Peter Lenhof, Burkhard Morgenstern, Knut Rein-ert, 1999.
An exact solution for the segment-to-segment multiple sequence alignment problem.Bioinformatics.
15(3): 203-210.Ivan A.
Sag, Timothy Baldwin, Francis Bond, AnnA.
Copestake, Dan Flickinger, 2002.
Multiword Ex-pressions: A Pain in the Neck for NLP.
LectureNotes in Computer Science 2276: The 3rd Interna-tional Conference of Computational Linguistics andIntelligent Text Processing.Jakob.
H. Havgaard, R. Lyngs, G .D.
Stormo and J.Gorodkin, 2005.
Pairwise local structural alignmentof RNA sequences with sequence similarity less than40 percernt.
Bioinfomrmatics.
21(9), 1815-1824.Joaquim Ferreira da Silva, Gael Dias, Sylvie Guil-lore, Jose Gabriel Pereira Lopes, 1999.
Using Lo-calMaxs Algorithm for the Extraction of Contigu-ous and Non-contiguous Multiword Lexical Units.The 9th Portuguese Conference on Artificial Intelli-gence.Jordi Vivaldi, Llu?s Marquez, Horacio Rodr?guez,2001.
Improving Term Extraction by System Com-bination Using Boosting.
Lecture Notes in Com-puter Science 2167: The 12th European Conferenceon Machine Learning.Kalliopi Zervanou and John McNaught, 2000.
A Term-Based Methodology for Template Creation in Infor-mation Extraction.
Lecture Notes in Computer Sci-ence 1835: Natural Language Processing.Katerina Frantzi, Sophia Ananiadou, Hideki Mima,2000.
Automatic recognition of multi-word terms:the C-value/NC-value method.
Int J Digit Libr.
3(2),115C130.Knut Reinert, Jens Stoye, Torsten Will, 2000.
An it-erative method for faster sum-of-pairs multiple se-quence alignment.
Bioinformatics.
16(9): 808-814.Makoto Nagao, Shinsuke Mori, 1994.
A New Methodof N-gram Statistics for Large Number of n and Au-tomatic Extraction of Words and Phrases from LargeText Data of Japanese.
The 15th International Con-ference on Computational Linguistics.Manning,C.D.,H.,Schutze,1999.Foundations of statis-tical natural language processing.
MIT Press.Marcus A. Zachariah, Gavin E. Crooks, Stephen R.Holbrook, Steven E. Brenner, 2005.
A GeneralizedAffine Gap Model Significantly Improves ProteinSequence Alignment Accuracy.
PROTEINS: Struc-ture, Function, and Bioinformatics.
58(2), 329 - 338Michael.
Sammeth, B. Morgenstern, and J. Stoye,2003.
Divide-and-conquer multiple alignment withsegment-based constraints.
Bioinformatics.
19(2),189-195.Mike Paterson, Vlado Dancik ,1994.
Longest CommonSubsequences.
Mathematical Foundations of Com-puter Science.Pascale Fung, Kathleen Mckeown, 1997.
A Techni-cal Word and Term Translation Aid Using NoisyParallel Corpora across Language Groups.
MachineTranslation.
12, 53C87.Paul Deane, 2005.
A Nonparametric Method for Ex-traction of Candidate Phrasal Terms.
The 43rd An-nual Meeting of the Association for ComputationalLinguistics.181Robertson, A.M. and Willett, P., 1998.
Applications ofn-grams in textual information systems.
Journal ofDocumentation, 54(1), 48-69.Satanjeev Banerjee, Ted Pedersen, 2003.
The Design,Implementation, and Use of the Ngram StatisticsPackage.
Lecture Notes in Computer Science 2588:The 4th International Conference of ComputationalLinguistics and Intelligent Text Processing.Smith, T.F., Waterman, M.S., 1981.
Identification ofcommon molecular subsequences.
J. Molecular Bi-ology.
147(1), 195-197.Stefan Diaconescu, 2004.
Multiword ExpressionTranslation Using Generative Dependency Gram-mar.
Lecture Notes in Computer Science 3230: Ad-vances in Natural Language Processing.Suleiman H. Mustafa, 2004.
Character contiguity inN-gram-based word matching: the case for Arabictext searching.
Information Processing and Manage-ment.
41(4), 819-827.Taneli Mielikainen, 2003.
Frequency-Based Views toPattern Collections.
IFIP/SIAM Workshop on Dis-crete Mathematics and Data Mining.Violeta Seretan, Luka Nerima, Eric Wehrl, 2003.
Ex-traction of Multi-Word Collocations Using Syntac-tic Bigram Composition.
International Conferenceon Recent Advances in NLP.182
