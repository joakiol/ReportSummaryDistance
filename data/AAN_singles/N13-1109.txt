Proceedings of NAACL-HLT 2013, pages 888?896,Atlanta, Georgia, 9?14 June 2013. c?2013 Association for Computational LinguisticsA Just-In-Time Keyword Extraction from Meeting TranscriptsHyun-Je Song Junho Go Seong-Bae Park Se-Young ParkSchool of Computer Science and EngineeringKyungpook National UniversityDaegu, Korea{hjsong,jhgo,sbpark,sypark}@sejong.knu.ac.krAbstractIn a meeting, it is often desirable to extractkeywords from each utterance as soon as it isspoken.
Thus, this paper proposes a just-in-time keyword extraction from meeting tran-scripts.
The proposed method considers twomajor factors that make it different from key-word extraction from normal texts.
The firstfactor is the temporal history of preceding ut-terances that grants higher importance to re-cent utterances than old ones, and the sec-ond is topic relevance that forces only the pre-ceding utterances relevant to the current utter-ance to be considered in keyword extraction.Our experiments on two data sets in Englishand Korean show that the consideration of thefactors results in performance improvement inkeyword extraction from meeting transcripts.1 IntroductionA meeting is generally accomplished by a numberof participants and a wide range of subjects are dis-cussed.
Therefore, it would be helpful to meetingparticipants to provide them with some additionalinformation related to the current subject.
For in-stance, assume that a participant is discussing a spe-cific topic with other participants at a meeting.
Thesummary of previous meetings on the topic is thenone of the most important resources for her discus-sion.In order to provide information on a topic to par-ticipants, keywords should be first generated for thetopic since keywords are often representatives of atopic.
A number of techniques have been proposedfor automatic keyword extraction (Frank et al 1999;Turney, 2000; Mihalcea and Tarau, 2004; Wan et al2007), and they are designed to extract keywordsfrom a written document.
However, they are notsuitable for meeting transcripts.
In a meeting, it isoften desirable to extract keywords at the time atwhich a new utterance is made for just-in-time ser-vice of additional information.
Otherwise, the ex-tracted keywords become just the important wordsat the end of the meeting.Two key factors for just-in-time keyword extrac-tion from meeting transcripts are time of precedingutterances and topic of current utterance.
First, cur-rent utterance is affected by temporal history of pre-ceding utterances.
That is, when a new utteranceis made it is likely to be related more closely withlatest utterances than old ones.
Second, the preced-ing utterances which carry similar topics to currentutterance are more important than irrelevant utter-ances.
Since a meeting consists of several topics,the utterances that have nothing to do with currentutterance are inappropriate as a history of the cur-rent utterance.This paper proposes a graph-based keyword ex-traction to reflect these factors.
The proposedmethod represents an utterance as a graph of whichnodes are candidate keywords.
The preceding utter-ances are also expressed as a history graph in whichthe weight of an edge is the temporal importanceof the keywords connected by the edge.
To reflectthe temporal history of utterances, forgetting curve(Wozniak, 1999) is adopted in updating the weightsof edges in the history graph.
It expresses effectivelynot only the reciprocal relation between memory re-888tention and time, but also active recall that makesfrequent words more consequential in keyword ex-traction.
Then, a subgraph that is relevant to thecurrent utterance is derived from the history graph,and used as an actual history of the current utterance.The keywords of the current utterance are extractedby TextRank (Mihalcea and Tarau, 2004) from themerged graph of the current utterance and the his-tory graphs.The proposed method is evaluated with two kindsof data sets: the National Assembly transcriptsin Korean and the ICSI meeting corpus (Janin etal., 2003) in English.
The experimental resultsshow that it outperforms both the TFIDF frame-work (Frank et al 1999; Liu et al 2009) and thePageRank-based graph model (Wan et al 2007).One thing to note is that the proposed method im-proves even the supervised methods that do not re-flect utterance time and topic relevance for the ICSIcorpus.
This proves that it is critical to consider timeand content of utterances simultaneously in keywordextraction from meeting transcripts.The rest of the paper is organized as follows.
Sec-tion 2 reviews the related studies on keyword extrac-tion.
Section 3 explains the overall process of theproposed method, and Section 4 addresses its de-tailed description how to reflect meeting character-istics.
Experimental results are presented in Section5.
Finally, Section 6 draws some conclusions.2 Related WorkKeyword extraction has been of interest for a longtime in various fields such as information retrieval,document clustering, summarization, and so on.Thus, there have been many studies on automatickeyword extraction.
The frequency-based key-word extraction with TFIDF weighting (Frank et al1999) and the graph-based keyword extraction (Mi-halcea and Tarau, 2004) are two base models for thistask.
Many studies recently tried to extend them byincorporating specific information such as linguisticknowledge (Hulth, 2003), web-based resource (Tur-ney, 2003), and semantic knowledge (Chen et al2010).
As a result, they show good performance onwritten text.
However, it is difficult to use them di-rectly for spoken genres, since spoken genres havesignificantly different characteristics from writtentext.There have been a few studies focused on key-word extraction from spoken genres.
Among them,the extraction from meetings has attracted more con-cern, since the need for grasping important pointsof a meeting or an opinion of each participant hasincreased.
The studies on meetings focused onthe exterior features of meeting dialogues such asunstructured and ill-formed sentences.
Liu et al(2009) used some knowledge sources such as Part-of-Speech (POS) filtering, word clustering, and sen-tence salience to reflect dialogue features, and theyfound out that a simple TFIDF-based keyword ex-traction using these knowledge sources works rea-sonably well.
They also extended their work byadopting various features such as decision makingsentence features, speech-related features, and sum-mary features that reflect meeting transcripts better(Liu et al 2011).
Chen et al(2010) extracted key-words from spoken course lectures.
In this study,they considered prosodic information from HKTforced alignment and topics in a lecture generatedby Probabilistic Latent Semantic Analysis (pLSA).These studies focused on the exterior characteris-tics of spoken genres, since they assumed that entirescripts are given in advance and then they extractedkeywords that best describe the scripts.
However, tothe best of our knowledge, there is no previous studyconsidered time of utterances which is an intrinsicelement of spoken genres.The relevance between current utterance and pre-ceding utterances is also a critical feature in keywordextraction from meeting transcripts.
The study thatconsiders this relevance explicitly is CollabRankproposed by Wan and Xiao (2008).
This is collabo-rative approach to extract keywords in a document.In this study, it is assumed that a few neighbor doc-uments close to a current document can help extractkeywords.
Therefore, they applied a clustering al-gorithm to a document set and then extracted wordsthat are reinforced by the documents within a clus-ter.
However, this method also does not consider theutterance time, since it is designed to extract key-words from normal documents.
As a result, if it isapplied to meeting transcripts, all preceding utter-ances would affect the current utterance uniformly,which leads to a poor performance.889Currentutterancegraph (G1)Historygraph (G2)Subgraph (G3)Expanded graph (G4)Keywordgraph (G5)SubgraphextractionExpandKeywordextractionMergeKeywordsCurrentutteranceFilterFigure 1: The overall process of the just-in-time keyword extraction from meeting transcripts.3 Just-In-Time Keyword Extraction for aMeetingFigure 1 depicts the overall process of extractingkeywords from an utterance as soon as it is spo-ken.
We represent all the components in a meetingas graphs.
This is because graphs are effective to ex-press the relationship between words, and the graphoperations that are required for keyword extractionare also efficiently performed.
That is, whenever anutterance is spoken, it is represented as a graph (G1)of which nodes are the potential keywords in the ut-terance.
This graph is named as current utterancegraph.The summary of all preceding utterances is alsorepresented as a history graph (G2).
We assume thatonly the preceding utterances that are directly re-lated with the current utterance are important for ex-tracting keywords from the current utterance.
There-fore, a subgraph of G2 that maximally covers thecurrent utterance graph (G1) is extracted.
This sub-graph is labeled as G3 in Figure 1.
Then, the currentutterance graph G1 is expanded by merging it andG3.
This expanded graph (G4) is a combined rep-resentation of the current and preceding utterances,and then the keywords of the current utterance is ex-tracted from this graph.
The keywords are so-calledhub nodes of G4.After keywords are extracted from the current ut-terance, the current utterance becomes a part of thehistory graph for the next utterance.
For this, theextracted keywords are also represented as a graph(G5), and it is merged into the current history G2.This merged graph becomes a new history graphfor the next utterance.
In merging two graphs, theweight of each edge in G2 is updated to reflect thetemporal history.
If an edge is connecting two nounsfrom an old utterance, its weight becomes small.
Inthe same way, the weights for the edges from recentutterances get large.
The weights of the edges fromG5 are 1, the largest possible value.4 Graph Representation and WeightUpdate4.1 Current Utterance Graph and HistoryGraphCurrent utterance graph is a graph-representation ofthe current utterance.
When current utterance con-sists of m words, we first extract the potential key-890words from the current utterance.
Since all wordswithin the current utterance are not keywords, somewords are filtered out.
For this filtering out, we fol-low the POS filtering approach proposed by Liu etal.
(2009).
This approach filters out non-keywordsusing a stop-word list and POS tags of the words.Assume that n words remain after the filtering out,where n ?
m. These n words become the verticesof the current utterance graph.Formally, the current utterance graph G1 =(V1, E1) is an undirected graph, where |V1| = n.E1 is a set of edges and each edge implies that thenouns connected by the edge co-occur within a win-dow sizedW .
For each e1ij ?
E1 that connects nodesv1i and v1j , its weight is given byw1ij ={1 if v1i &v1j cooccur within the window,0 otherwise.
(1)In a meeting, preceding utterances affect the cur-rent utterance.
We assume that only the keywordsof preceding utterances are effective.
Therefore, thehistory graph G2 = (V2, E2) is an undirected graphof keywords in the preceding utterances.
That is,all vertices in V2 are keywords extracted from oneor more previous utterances, and the edge betweentwo keywords implies that they co-occurred at leastonce.
Every edge in E2 has a weight that representsits temporal importance.The history graph is updated whenever keywordsare extracted from a new utterance.
This is becausethe current utterance becomes a part of the historygraph for the next utterance.
As a history, old ut-terances are less important than recent ones.
Thus,the temporal importance should decrease graduallyaccording to the passage of time.
In addition, thekeywords which occur frequently at a meeting aremore important than those mentioned just once ortwice.
Since the frequently-mentioned keywords arenormally major topics of the meeting, their influenceshould last for a long time.To model these characteristics, the forgettingcurve (Wozniak, 1999) is adopted in updating thehistory graph.
It models the decline of memory re-tention in time.
Figure 2 shows a typical represen-tation of the forgetting curve.
The X-axis of thisfigure is time and the Y-axis is memory retention.As shown in this figure, memory retention of newTimeMemoryRetentionFigure 2: Memory retention according to time.information decreases gradually by the exponentialnature of forgetting.
However, whenever the infor-mation is repeated, it is recalled longer.
This is for-mulated asR = e?tS ,where R is memory retention, t is time, and S is therelative strength of memory.Based on the forgetting curve, the weight of eachedge e2ij ?
E2 between keywords v2i and v2j is set asw2ij = exp?
tf(vi,vj) , (2)where t is the elapse of utterance time and f(vi, vj)is the frequency that vi and vj co-occur from thebeginning of the meeting to now.
According tothis equation, the temporal importance between key-words decreases gradually as time passes by, but thekeyword relations repeated during the meeting areremembered for a long time in the history graph.4.2 Keyword Extraction by Merging CurrentUtterance and History GraphsAll words within the history graph are not equallyimportant in extracting keywords from the currentutterance.
In general, many participants discuss awide range of topics in a meeting.
Therefore, somepreceding utterances that shares topics with the cur-rent utterance are more significant.
We assume thatthe preceding utterances that contain the nouns inthe current utterance share topics with the currentutterance.
Thus, only a subgraph of G2 that containwords in G1 is relevant for keyword extraction fromG1.891Given the current utterance graph G1 = (V1, E1)and the history graph G2 = (V2, E2), the relevantgraph G3 = (V3, E3) is a subgraph of G2.
Here,V3 = (V1?V2)?adjacency(V1) and adjacency(V1)is a set of vertices from G2 which are directly con-nected to the words in V1.
That is, V3 containsthe words of G1 and their direct neighbor words inG2.
E3 is a subset of E2.
Only the edges that ap-pear in E2 are included in E3.
The weight w3ij ofeach e3ij ?
E3 is also borrowed from G2.
That is,w3ij = w2ij .
Therefore, G3 is a 1-walk subgraph1 ofG2 in which words in G1 and their neighbor wordsappear.The keywords of the current utterance should re-flect the relevant history as well as the current utter-ance itself.
For this purpose, G1 is expanded withrespect to G3.
The expanded graph G4 = (V4, E4)of G1 is defined asV4 = V1 ?
V3,E4 = E1 ?
E3.For each edge e4ij ?
E4, its weightw4ij is determinedto be the larger value between w1ij and w3ij if it ap-pears in both G1 and G3.
When it appears in onlyone of the graphs, w4ij is set to be the weight of itscorresponding graph.
That is,w4ij =??????
?max(w1ij , w3ij) if e4ij ?
E1 and e4ij ?
E3,w1ij if e4ij ?
E1 and e4ij /?
E3,w3ij otherwise.From this expanded graph G4, the keywords areextracted by TextRank (Mihalcea and Tarau, 2004).TextRank is an unsupervised graph-based methodfor keyword extraction.
It singles out the key ver-tices of a graph by providing a ranking mechanism.In order to rank the vertices, it computes the scoreof each vertex v4i ?
V4 byS(v4i ) = (1?
d)+ d ?
?v4j?adj(v4i )w4ji?v4k?adj(v4j )w4jkS(v4j ),(3)1If a m-walk subgraph (m > 1) is used, more affluent his-tory is used.
However, this graph contains some words irrel-evant to the current utterance.
According to our experiments,1-walk subgraph outperforms other m-walk subgraphs wherem > 1.
In addition, extracting G3 becomes expensive for largem.where 0 ?
d ?
1 is a damping factor and adj(vi)denotes vi?s neighbors.
Finally, the words whosescore is larger than a specific threshold ?
are cho-sen as keywords.
Especially when the current utter-ance is the first utterance of a meeting, the historygraph does not exist.
In this case, the current utter-ance graph becomes the expanded graph (G4 = G1),and keywords are extracted from the current utter-ance graph.The proposed method extracts keywords when-ever an utterance is spoken.
Thus, it tries to extractkeywords even if the current utterance is not relatedto the topics of a meeting or is too short.
However,if the current utterance is irrelevant to the meeting,it has just a few connections with other previous ut-terances, and thus the potential keywords in this ut-terance are apt to have a low score.
The proposedmethod, however, does not select the words whosescore is smaller than the threshold ?
as keywords.As a result, it extracts only the relevant keywordsduring the meeting.Since the keywords for the current utteranceshould be the history for the next utterance, theyhave to be reflected into the history graph.
There-fore, a keyword graph G5 = (V5, E5) is constructedfrom the keywords.
Here, V5 is a set of keywordsextracted from G4, and E5 is a subset of E4 thatcorresponds to V5.
The weights of edges in E5 aresame with those in E4.
That is, w5ij = w4ij .
The key-word graph G5 is then merged into the history graphG2 in the same way that G1 and G3 are merged.
Asstated above, the weights of the edges in the historygraph G2 are updated by Equation (2).
Therefore,before merging G5 and G2, all weights of G2 areupdated by increasing t as t + 1 to reflect temporalimportance of preceding utterances.5 ExperimentsThe proposed method is evaluated with two kinds ofdata sets: the National Assembly transcripts in Ko-rean and the ICSI meeting corpus in English.
Bothdata sets are the records of meetings that are manu-ally dictated by human transcribers.892Table 1: Simple statistics of the National Assembly transcriptsthe first meeting the second meetingNo.
of utterances 1,280 573Average No.
of words per utterance 7.22 10.175.1 National Assembly Transcripts in KoreanThe first corpus used to evaluate our method is theNational Assembly transcripts2.
This corpus is ob-tained from the Knowledge Management Systemof the National Assembly of KoreaIt is transcribedfrom the 305th assembly record of the KnowledgeEconomy Committee in 2012.
Table 1 summa-rizes simple statistics of the National Assembly tran-scripts.
The 305th assembly record actually consistsof two meetings.
The first meeting contains 1,280utterances and the second has 573 utterances.
Theaverage number of words per utterance in the firstmeeting is 7.22 while the second meeting contains10.17 words per utterance on average.
The secondmeeting transcript is used as a development data setto determine window size W of Equation (1), thedamping factor d of Equation (3), and the threshold?.
For all experiments below, d is set 0.85, W is 10,and ?
is 0.28.
The remaining first meeting transcriptis used as a data set to extract keywords since thistranscript contains more utterances.
Only nouns areconsidered as potential keywords.
That is, only thewords whose POS tag is NNG (common noun) orNNP (proper noun) can be a keyword.Three annotators are engaged to extract keywordsmanually for each utterance in the first meetingtranscript, since the Knowledge Management Sys-tem does not provide the keywords3.
The aver-age number of keywords per utterance is 2.58.
Tosee the inter-judge agreement among the annotators,the Kappa coefficient (Carletta, 1996) was investi-gated.
The kappa agreement of the National Assem-bly transcript is 0.31 that falls under the category of?Fair?.
Even though all congressmen in the transcriptbelong to the same committee, they discussed vari-ous topics at the meeting.
As a result, the keywordsare difficult to be agreed unanimously by all three2The data set is available: http://ml.knu.ac.kr/dataset/keywordextraction.html3A guideline was given to the annotators that keywords mustbe a single word and the maximum number of keywords perutterance is five.annotators.
Therefore, in this paper the words thatare recommended by more than two annotators arechosen as keywords.The evaluation is done with two metics: F-measure and the weighted relative score (WRS).Since the previous work by Liu et al(2009) re-ported only F-measure and WRS, F-measure insteadof precision/recall are used for the comparison withtheir method.
The weighted relative score is de-rived from Pyramid metric (Nenkova and Passon-neau, 2004).
When a keyword extraction systemgenerates keywords which many annotators agree,a higher score is given to it.
On the other hand, alower score is given if fewer annotators agree.The proposed method is compared with two base-line models to see its relative performance.
One isthe frequency-based keyword extraction with TFIDFweighting (Frank et al 1999) and the other is Tex-tRank in which the weight of edges is mutual in-formation between vertices (Wan et al 2007).
InTFIDF, each utterance is considered as a document,and thus all utterances including the current oneare regarded as whole documents.
The frequency-based TFIDF chooses top-K words according totheir TFIDF value from the set of words appearing inthe meeting transcript.
Since the human annotatorsare restricted to extract up to five keywords, the key-word extraction systems including our method arealso requested to select top-5 keywords when morethan five keywords are produced.In order to see the effect of preceding utterances inbaseline models, the performances are measured ac-cording to the number of preceding utterances used.Figure 3 shows the results.
The X-axis of this fig-ure is the number of preceding utterances and the Y-axis represents F-measures.
As shown in this figure,the performance of the baseline models improvesmonotonically at first as the number of precedingutterances increases.
However, the performance im-provement stops when many preceding utterancesare involved, and the performance begins to drop893Figure 3: The performance of baseline models accordingto the number of preceding utterancesTable 2: The experimental results on the National Assem-bly transcriptsMethods F-measure WRSTextRank 0.478 0.387TFIDF 0.481 0.394Proposed method 0.533 0.421when too many utterances are considered.
The per-formance of TextRank model drops from 20 preced-ing utterances, while that of TFIDF model begins todrops at 50 utterances.
When too many precedingutterances are taken into account, it is highly pos-sible that some of their topics are irrelevant to thecurrent utterance, which leads to performance drop.Table 2 compares our method with the baselinemodels on the National Assembly transcripts.
Theperformances of baseline models are obtained whenthey show the best performance for various numberof preceding utterances.
TextRank model achievesF-measure of 0.478 and weighted relative score of0.387, while TFIDF reports its best F-measure of0.481 and weighted relative score of 0.394.
Thus,the difference between TFIDF and TextRank is notsignificant.
However, F-measure and weighted rel-ative score of the proposed method are 0.533 and0.421 respectively, and they are much higher thanthose of baseline models.
In addition, our methodachieves precision of 0.543 and recall of 0.523 andTable 3: The importance of temporal historyF-measure WRSWith Temporal History 0.533 0.421Without Temporal History 0.518 0.413this is much higher performance than TextRankwhose precision is just 0.510.
Since the proposedmethod uses, as history, the preceding utterancesrelevant to the current utterance, its performance iskept high even if whole utterances are used.
There-fore, it could be inferred that it is important to adoptonly the relevant history in keyword extraction frommeeting transcripts.One of the key factors of our method is the tem-poral history.
Its importance is given in Table 3.
Asexplained above, the temporal history is achieved byEquation (2).
Thus, the proposed model does notreflect the temporal importance of preceding utter-ances if w2ij = 1 always.
That is, under w2ij = 1,old utterances are regarded as important as recent ut-terances.
Without temporal history, F-measure andweighted relative score are just 0.518 and 0.413 re-spectively.
These poor performances prove the im-portance of the temporal history in keyword extrac-tion from meeting transcripts.5.2 ICSI Meeting Corpus in EnglishThe proposed method is also evaluated on the ICSImeeting corpus (Janin et al 2003) which consists ofnaturally occurring meetings recordings.
This cor-pus is widely used for summarizing and extractingkeywords of meetings.
We followed all the exper-imental settings proposed by Liu et al(2009) forthis corpus.
Among 26 meeting transcripts chosenby Liu et alfrom 161 transcripts of the ICSI meet-ing corpus, 6 transcripts are used as developmentdata and the remaining transcripts are used as datato extract keywords.
The parameters for the ICSImeeting corpus are set to be d = 0.85,W = 10,and ?
= 0.20.
Each meeting of the corpus consistsof several topic segments, and every topic segmentcontains three sets of keywords that are annotated bythree annotators.
Up to five keywords are annotatedfor a topic segment.Table 4 shows simple statistics of the ICSI meet-ing data.
Total number of topic segments in the 26meetings is originally 201, but some of them do not894Table 4: Simple statistics of the ICSI meeting dataInformation Value# of meetings 26# of topic segments 201# of topic segments used actually 140Average # of utterances per topic segment 260Average # of words per utterance 7.22Table 5: The experimental results on the ICSI corpusMethods F-measure WRSTFIDF-Liu 0.290 0.404TextRank-Liu 0.277 0.380ME model 0.312 0.401Proposed method 0.334 0.533have keywords.
Such segments are discarded, andthe remaining 140 topic segments are actually used.The average number of utterances in a topic segmentis 260 and the average number of words per utter-ance is 7.22.Unlike the National Assembly transcripts, thekeywords of the ICSI meeting corpus are annotatedat the topic segment level, not the utterance level.Therefore, the proposed method which extracts key-words at the utterance level can not be applied di-rectly to this corpus.
In order to obtain keywordsfor a topic segment with the proposed method, thekeywords are first extracted from each utterance inthe segment by the proposed method and then theyare all accumulated.
The proposed method extractskeywords for a topic segment from these accumu-lated utterance-level keywords as follows.
Assumethat a topic segment consists of l utterances.
Sinceour method can extract up to 5 keywords for eachutterance, the number of keywords for the segmentcan reach to 5 ?
l. From these keywords, we selecttop-5 keywords ranked by Equation (3).The proposed method is compared with three pre-vious studies.
The first two are the methods pro-posed by Liu et al(2009) One is the frequency-based method of TFIDF weighting with the fea-tures such as POS filtering, word clustering, and sen-tence salience score, and the other is the graph-basedmethod with POS filtering.
The last method is amaximum entropy model applied to this task (Liuet al 2008).
Note that the maximum entropy is asupervised learning model.Table 6: The effect of considering topic relevanceMethods F-measure WRSWith topic relevance 0.334 0.533Without topic relevance 0.291 0.458Table 5 summarizes the comparison results.
Asshown in this table, the proposed method outper-forms all previous methods.
Our method achievesprecision of 0.311 and recall of 0.361, and thusthe F-score is 0.334.
The weight relative scoreof the proposed method is 0.533.
This is the im-provement of up to 0.044 in F-measure and 0.129in weighted relative score over other unsupervisedmethods (TFIDF-Liu and TextRank-Liu).
It shouldbe also noted that the proposed method outperformseven the supervised method (ME model).
The differ-ence between our method and the maximum entropymodel in weighted relative score is 0.132.One possible variant of the proposed method forthe ICSI corpus is to simply merge the current utter-ance graph (G1) with the history graph (G2) ratherthan to extract keywords from each utterance.
Af-ter the current utterance graph of the last utterancein a topic segment is merged into the history graph,the keywords for the segment are extracted from thehistory graph.
This variant and the proposed methodboth rely on the temporal history, but the differenceis that the history graph of the variant accumulatesall information within the topic segment.
Thus, thekeywords extracted from the history graph by thisvariant are those without consideration of topic rel-evance.Table 6 compares the proposed method with thevariant.
The performance of the variant is higherthan those of TFIDF-Liu and TextRank-Liu.
Thisproves the importance of the temporal history inkeyword extraction from meeting transcripts.
How-ever, the proposed method still outperforms the vari-ant, and it demonstrates the importance of topic rel-evance.
Therefore, it can be concluded that the con-sideration of temporal history and topic relevanceis critical in keyword extraction from meeting tran-scripts.8956 ConclusionIn this paper, we have proposed a just-in-time key-word extraction from meeting transcripts.
Wheneveran utterance is spoken, the proposed method extractskeywords from the utterance that best describe theutterance.
Based on the graph representation of allcomponents in a meeting, the proposed method ex-tracts keywords by TextRank with some graph oper-ations.Temporal history and topic of the current utter-ance are two major factors especially in keyword ex-traction from meeting transcripts.
This is because re-cent utterances are more important than old ones andonly the preceding utterances of which topic is rele-vant to the current utterance are important.
To modelthe temporal importance of the preceding utterances,the concept of forgetting curve is used in updatingthe history graph of preceding utterances.
In addi-tion, the subgraph of the history graph that shareswords appearing in the current utterance graph isused to extract keywords rather than whole historygraph.
The proposed method was evaluated with theNational Assembly transcripts and the ICSI meetingcorpus.
According to our experimental results onthese data sets, the performance of keyword extrac-tion is improved when we consider temporal historyand topic relevance.AcknowledgmentsThis research was supported by the Converging Re-search Center Program funded by the Ministry ofEducation, Science and Technology (2012K001342)ReferencesJean Carletta.
1996.
Assessing agreement on classifi-cation tasks: The kappa statistic.
Computational Lin-guistics, 22(2):249?254.Yun-Nung Chen, Yu Huang, Sheng-Yi Kong, , and Lin-Shan Lee.
2010.
Automatic key term extraction fromspoken course lectures using branching entropy andprosodic/semantic features.
In Proceedings of IEEEWorkshop on Spoken Language Technology, pages265?270.Eibe Frank, Gordon W. Paynter, Ian H. Witten, CarlGutwin, and Craig G. Nevill-Manning.
1999.Domain-specific keyphrase extraction.
In Proceedingsof the 18th International Joint Conference on Artificialintelligence, pages 668?671.Anette Hulth.
2003.
Improved automatic keyword ex-traction given more linguistic knowledge.
In Proceed-ings of International Conference on Empirical Meth-ods in Natural Language Processing, pages 216?223.Adam Janin, Don Baron, Jane Edwards, Dan Ellis,David Gelbart, Nelson Morgan, Barbara Peskin, ThiloPfau, Elizabeth Shriberg, Andreas Stolcke, and ChuckWooters.
2003.
The icsi meeting corpus.
In Proceed-ings of International Conference on Acoustics, Speech,and Signal Processing, pages 364?367.Fei Liu, Feifan Liu, and Yang Liu.
2008.
Automatic key-word extraction for the meeting corpus using super-vised approach and bigram expansion.
In Proceedingsof IEEE Spoken Language Technology, pages 181?184.Feifan Liu, Deana Pennell, Fei Liu, and Yang Liu.
2009.Unsupervised approaches for automatic keyword ex-traction using meeting transcripts.
In Proceedings ofAnnual Conference of the North American Chapter ofthe ACL, pages 620?628.Fei Liu, Feifan Liu, and Yang Liu.
2011.
A super-vised framework for keyword extraction from meetingtranscripts.
IEEE Transactions on Audio, Speech, andLanguage Processing, 19(3):538?548.Rada Mihalcea and Paul Tarau.
2004.
Textrank: Bring-ing order into texts.
In Proceedings of InternationalConference on Empirical Methods in Natural Lan-guage Processing, pages 404?411.Ani Nenkova and Rebecca Passonneau.
2004.
Evaluat-ing content selection in summarization: The pyramidmethod.
In Proceedings of Annual Conference of theNorth American Chapter of the ACL, pages 145?152.Peter D. Turney.
2000.
Learning algorithms forkeyphrase extraction.
Information Retrieval, 2:303?336.Peter D. Turney.
2003.
Coherent keyphrase extrac-tion via web mining.
In Proceedings of the 18th In-ternational Joint Conference on Artificial intelligence,pages 434?439.Xiaojun Wan and Jianguo Xiao.
2008.
Collabrank:Towards a collaborative approach to single-documentkeyphrase extraction.
In Proceedings of InternationalConference on Computational Linguistics, pages 969?976.Xiaojun Wan, Jianwu Yang, and Jianguo Xiao.
2007.
To-wards an iterative reinforcement approach for simulta-neous document summarization and keyword extrac-tion.
In Proceedings of the 45th Annual Meeting of theAssociation of Computational Linguistics, pages 552?559.Robert H. Wozniak.
1999.
Classics in Psychology,1855?1914: Historical Essays.
Thoemmes Press.896
