Proceedings of the Fifteenth Conference on Computational Natural Language Learning, pages 238?246,Portland, Oregon, USA, 23?24 June 2011. c?2011 Association for Computational LinguisticsLearning with Lookahead:Can History-Based Models Rival Globally Optimized Models?Yoshimasa Tsuruoka??
Yusuke Miyao??
Jun?ichi Kazama??
Japan Advanced Institute of Science and Technology (JAIST), Japan?
National Institute of Informatics (NII), Japan?
National Institute of Information and Communications Technology (NICT), Japantsuruoka@jaist.ac.jp yusuke@nii.ac.jp kazama@nict.go.jpAbstractThis paper shows that the performance ofhistory-based models can be significantly im-proved by performing lookahead in the statespace when making each classification deci-sion.
Instead of simply using the best ac-tion output by the classifier, we determinethe best action by looking into possible se-quences of future actions and evaluating thefinal states realized by those action sequences.We present a perceptron-based parameter op-timization method for this learning frame-work and show its convergence properties.The proposed framework is evaluated on part-of-speech tagging, chunking, named entityrecognition and dependency parsing, usingstandard data sets and features.
Experimentalresults demonstrate that history-based modelswith lookahead are as competitive as globallyoptimized models including conditional ran-dom fields (CRFs) and structured perceptrons.1 IntroductionHistory-based models have been a popular ap-proach in a variety of natural language process-ing (NLP) tasks including part-of-speech (POS) tag-ging, named entity recognition, and syntactic pars-ing (Ratnaparkhi, 1996; McCallum et al, 2000; Ya-mada and Matsumoto, 2003; Nivre et al, 2004).The idea is to decompose the complex structuredprediction problem into a series of simple classifi-cation problems and use a machine learning-basedclassifier to make each decision using the informa-tion about the past decisions and partially completedstructures as features.Although history-based models have many prac-tical merits, their accuracy is often surpassed byglobally optimized models such as CRFs (Laffertyet al, 2001) and structured perceptrons (Collins,2002), mainly due to the label bias problem.
To-day, vanilla history-based models such as maximumentropy Markov models (MEMMs) are probably notthe first choice for those who are looking for a ma-chine learning model that can deliver the state-of-the-art accuracy for their NLP task.
Globally opti-mized models, by contrast, are gaining popularity inthe community despite their relatively high compu-tational cost.In this paper, we argue that history-based mod-els are not something that should be left behindin research history, by demonstrating that their ac-curacy can be significantly improved by incorpo-rating a lookahead mechanism into their decision-making process.
It should be emphasized that weuse the word ?lookahead?
differently from some lit-erature on syntactic parsing in which lookahead sim-ply means looking at the succeeding words to choosethe right parsing actions.
In this paper, we use theword to refer to the process of choosing the best ac-tion by considering different sequences of future ac-tions and evaluating the structures realized by thosesequences.
In other words, we introduce a looka-head mechanism that performs a search in the spaceof future actions.We present a perceptron-based training algorithmthat can work with the lookahead process, togetherwith a proof of convergence.
The algorithm enablesus to tune the weight of the perceptron in such a waythat we can correctly choose the right action for the238State Operation Stack Queue0 I saw a dog with eyebrows1 shift I saw a dog with eyebrows2 shift I saw a dog with eyebrows3 reduceL saw(I) a dog with eyebrows.
.
.4 saw(I) dog(a) with eyebrows5 shift saw(I) dog(a) with eyebrows6 shift saw(I) dog(a) with eyebrows7 reduceR saw(I) dog(a) with(eyebrows)8 reduceR saw(I) dog(a, with(eyebrows))5?
reduceR saw(I, dog(a)) with eyebrows6?
shift saw(I, dog(a)) with eyebrows7?
shift saw(I, dog(a)) with eyebrows8?
reduce R saw(I, dog(a)) with(eyebrows)9?
reduce R saw(I, dog(a), with(eyebrows))Figure 1: Shift-reduce dependency parsingcurrent state at each decision point, given the infor-mation obtained from a search.To answer the question of whether the history-based models enhanced with lookahead can actuallycompete with globally optimized models, we eval-uate the proposed framework with a range of stan-dard NLP tasks, namely, POS tagging, text chunking(a.k.a.
shallow parsing), named entity recognition,and dependency parsing.This paper is organized as follows.
Section 2presents the idea of lookahead with a motivatingexample from dependency parsing.
Section 3 de-scribes our search algorithm for lookahead and aperceptron-based training algorithm.
Experimen-tal results on POS tagging, chunking, named entityrecognition, and dependency parsing are presentedin Section 4.
We discuss relationships between ourapproach and some related work in Section 5.
Sec-tion 6 offers concluding remarks with some potentialresearch directions.2 MotivationThis section describes an example of dependencyparsing that motivates the introduction of lookaheadin history-based models.A well-known history-based approach to depen-dency parsing is shift-reduce parsing.
This al-gorithm maintains two data structures, stack andqueue: A stack stores intermediate parsing results,and a queue stores words to read.
Two operations(actions), shift and reduce, on these data structuresconstruct dependency relations one by one.For example, assume that we are given the follow-ing sentence.I saw a dog with eyebrows.In the beginning, we have an empty stack, and aqueue filled with a list of input words (State 0 in Fig-ure 1).
The shift operation moves the left-most ele-ment of the queue to the stack.
In this example, State1 is obtained by applying shift to State 0.
After thetwo shift operations, we reach State 2, in which thestack has two elements.
When we have two or moreelements in the stack, we can apply the other opera-tion, reduce, which merges the two stack elementsby creating a dependency relation between them.When we apply reduceL, which means to have theleft element as a dependent of the right element, wereach State 3: The word ?I?
has disappeared fromthe stack and instead it is attached to its head word?saw?.1 In this way, the shift-reduce parsing con-structs a dependency tree by reading words from thequeue and constructing dependency relations on thestack.1In Figure 1, H(D1, D2, .
.
.)
indicates that D1, D2, .
.
.
arethe dependents of the head H .239Let?s say we have now arrived at State 4 after sev-eral operations.
At this state, we cannot simply de-termine whether we should shift or reduce.
In suchcases, conventional methods rely on a multi-classclassifier to determine the next operation.
That is,a classifier is used to select the most plausible oper-ation, by referring to the features about the currentstate, such as surface forms and POSs of words inthe stack and the queue.In the lookahead strategy, we make this decisionby referring to future states.
For example, if we ap-ply shift to State 4, we will reach State 8 in the end,which indicates that ?with?
attaches to ?dog?.
Theother way, i.e., applying reduceR to State 4, eventu-ally arrives at State 9?, indicating ?with?
attaches to?saw?.
These future states indicate that we were im-plicitly resolving PP-attachment ambiguity at State4.
While conventional methods attempt to resolvesuch ambiguity using surrounding features at State4, the lookahead approach resolves the same ambi-guity by referring to the future states, for example,State 8 and 9?.
Because future states can provide ad-ditional and valuable information for ambiguity res-olution, improved accuracy is expected.It should be noted that Figure 1 only shows onesequence of operations for each choice of operationat State 4.
In general, however, the number of poten-tial sequences grows exponentially with the looka-head depth, so the lookahead approach requires us topay the price as the increase of computational cost.The primary goal of this paper is to demonstrate thatthe cost is actually worth it.3 Learning with LookaheadThis section presents our framework for incorporat-ing lookahead in history-based models.
In this pa-per, we focus on deterministic history-based modelsalthough our method could be generalized to non-deterministic cases.We use the word ?state?
to refer to a partiallycompleted analysis as well as the collection of his-torical information available at each decision pointin deterministic history-based analysis.
State transi-tions are made by ?actions?
that are defined at eachstate.
In the example of dependency parsing pre-sented in Section 2, a state contains all the infor-mation about past operations, stacks, and queues as1: Input2: d: remaining depth of search3: S0: current state4: Output5: S: state of highest score6: v: highest score7:8: function SEARCH(d, S0)9: if d = 0 then10: return (S0,w ?
?
(S0))11: (S, v)?
(null,??
)12: for each a ?
POSSIBLEACTIONS(S0)13: S1 ?
UPDATESTATE(S0, a)14: (S?, v?)?
SEARCH(d?
1, S1)15: if v?
> v then16: (S, v)?
(S?, v?
)17: return (S, v)Figure 2: Search algorithm.well as the observation (i.e.
the words in the sen-tence).
The possible actions are shift, reduceR, andreduceL.
In the case of POS tagging, for example, astate is the words and the POS tags assigned to thewords on the left side of the current target word (ifthe tagging is conducted in the left-to-right manner),and the possible actions are simply defined by thePOS tags in the annotation tag set.3.1 SearchWith lookahead, we choose the best action at eachdecision point by considering possible sequences offuture actions and the states realized by those se-quences.
In other words, we need to perform asearch for each possible action.Figure 2 describes our search algorithm in pseudocode.
The algorithm performs a depth-first search tofind the state of the highest score among the states inits search space, which is determined by the searchdepth d. This search process is implemented witha recursive function, which receives the remainingsearch depth and the current state as its input andreturns the state of the highest score together withits score.We assume a linear scoring model, i.e., the scoreof each state S can be computed by taking the dotproduct of the current weight vector w and ?
(S),the feature vector representation of the state.
The2401: Input2: C: perceptron margin3: D: depth of lookahead search4: S0: current state5: ac: correct action6:7: procedure UPDATEWEIGHT(C,D, S0, ac)8: (a?, S?, v)?
(null, null,??
)9: for each a ?
POSSIBLEACTIONS(S0)10: S1 ?
UPDATESTATE(S0, a)11: (S?, v?
)?SEARCH(D,S1)12: if a = ac then13: v?
?
v?
?
C14: S?c ?
S?15: if v?
> v then16: (a?, S?, v)?
(a, S?, v?
)17: if a?
6= ac then18: w ?
w + ?
(S?c )?
?(S?
)Figure 3: Perceptron weight updatescores are computed at each leaf node of the searchtree and backed up to the root.2Clearly, the time complexity of determinis-tic tagging/parsing with this search algorithm isO(nmD+1), where n is the number of actionsneeded to process the sentence, m is the (average)number of possible actions at each state, and D isthe search depth.
It should be noted that the timecomplexity of k-th order CRFs is O(nmk+1), soa history-based model with k-depth lookahead iscomparable to k-th order CRFs in terms of train-ing/testing time.Unlike CRFs, our framework does not require thelocality of features since it is history-based, i.e., thedecisions can be conditioned on arbitrary features.One interpretation of our learning framework is thatit trades off the global optimality of the learned pa-rameters against the flexibility of features.3.2 Training a margin perceptronWe adapt a learning algorithm for margin percep-trons (Krauth and Mezard, 1987) to our purpose of2In actual implementation, it is not efficient to compute thescore of a state from scratch at each leaf node.
For most ofthe standard features used in tagging and parsing, it is usuallystraight-forward to compute the scores incrementally every timethe state is updated with an action.optimizing the weight parameters for the lookaheadsearch.
Like other large margin approaches suchas support vector machines, margin perceptrons areknown to produce accurate models compared to per-ceptrons without a margin (Li et al, 2002).Figure 3 shows our learning algorithm in pseudocode.
The algorithm is very similar to the standardtraining algorithm for margin perceptrons, i.e., weupdate the weight parameters with the difference oftwo feature vectors (one corresponding to the cor-rect action, and the other the action of the highestscore) when the perceptron makes a mistake.
Thefeature vector for the second best action is also usedwhen the margin is not large enough.
Notice that thefeature vector for the second best action is automat-ically selected by using a simple trick of subtractingthe margin parameter from the score for the correctaction (Line 13 in Figure 3).The only difference between our algorithm andthe standard algorithm for margin perceptrons is thatwe use the states and their scores obtained fromlookahead searches (Line 11 in Figure 3), which arebacked up from the leaves of the search trees.
In Ap-pendix A, we provide a proof of the convergence ofour training algorithm and show that the margin willapproach at least half the true margin (assuming thatthe training data are linearly separable).As in many studies using perceptrons, we averagethe weight vector over the whole training iterationsat the end of the training (Collins, 2002).4 ExperimentsThis section presents four sets of experimental re-sults to show how the lookahead process improvesthe accuracy of history-based models in commonNLP tasks.4.1 Sequence prediction tasksFirst, we evaluate our framework with three se-quence prediction tasks: POS tagging, chunking,and named entity recognition.
We compare ourmethod with the CRF model, which is one of the defacto standard machine learning models for such se-quence prediction tasks.
We trained L1-regularizedfirst-order CRF models using the efficient stochasticgradient descent (SGD)-based training method pre-sented in Tsuruoka et al (2009).
Since our main in-241terest is not in achieving the state-of-the-art resultsfor those tasks, we did not conduct feature engineer-ing to come up with elaborate features?we sim-ply adopted the feature sets described in their paper(with an exception being tag trigram features testedin the POS tagging experiments).
The experimentsfor these sequence prediction tasks were carried outusing one core of a 3.33GHz Intel Xeon W5590 pro-cessor.The first set of experiments is about POS tagging.The training and test data were created from theWallStreet Journal corpus of the Penn Treebank (Marcuset al, 1994).
Sections 0-18 were used as the trainingdata.
Sections 19-21 were used for tuning the metaparameters for learning (the number of iterations andthe margin C).
Sections 22-24 were used for thefinal accuracy reports.The experimental results are shown in Table 1.Note that the models in the top four rows use exactlythe same feature set.
It is clearly seen that the looka-head improves tagging accuracy, and our history-based models with lookahead is as accurate as theCRF model.
We also created another set of modelsby simply adding tag trigram features, which can-not be employed by first-order CRF models.
Thesefeatures have slightly improved the tagging accu-racy, and the final accuracy achieved by a searchdepth of 3 was comparable to some of the best re-sults achieved by pure supervised learning in thistask (Shen et al, 2007; Lavergne et al, 2010).The second set of experiments is about chunking.We used the data set for the CoNLL 2000 sharedtask, which contains 8,936 sentences where each to-ken is annotated with the ?IOB?
tags representingtext chunks.
The experimental results are shownin Table 2.
Again, our history-based models withlookahead were slightly more accurate than the CRFmodel using exactly the same set of features.
Theaccuracy achieved by the lookahead model with asearch depth of 2 was comparable to the accuracyachieved by a computationally heavy combinationof max-margin classifiers (Kudo and Matsumoto,2001).
We also tested the effectiveness of additionalfeatures of tag trigrams using the development data,but there was no improvement in the accuracy.The third set of experiments is about named en-tity recognition.
We used the data provided forthe BioNLP/NLPBA 2004 shared task (Kim et al,2004), which contains 18,546 sentences where eachtoken is annotated with the ?IOB?
tags representingbiomedical named entities.
We performed the tag-ging in the right-to-left fashion because it is knownthat backward tagging is more accurate than forwardtagging on this data set (Yoshida and Tsujii, 2007).Table 3 shows the experimental results, togetherwith some previous performance reports achievedby pure machine leaning methods (i.e.
without rule-based post processing or external resources such asgazetteers).
Our history-based model with no looka-head was considerably worse than the CRF modelusing the same set of features, but it was signifi-cantly improved by the introduction of lookaheadand resulted in accuracy figures better than that ofthe CRF model.4.2 Dependency parsingWe also evaluate our method in dependency parsing.We follow the most standard experimental settingfor English dependency parsing: The Wall StreetJournal portion of Penn Treebank is converted to de-pendency trees by using the head rules of Yamadaand Matsumoto (2003).3 The data is split into train-ing (section 02-21), development (section 22), andtest (section 23) sets.
The parsing accuracy was eval-uated with auto-POS data, i.e., we used our looka-head POS tagger (depth = 2) presented in the previ-ous subsection to assign the POS tags for the devel-opment and test data.
Unlabeled attachment scoresfor all words excluding punctuations are reported.The development set is used for tuning the meta pa-rameters, while the test set is used for evaluating thefinal accuracy.The parsing algorithm is the ?arc-standard?method (Nivre, 2004), which is briefly described inSection 2.
With this algorithm, state S correspondsto a parser configuration, i.e., the stack and thequeue, and action a corresponds to shift, reduceL,and reduceR.
In this experiment, we use the sameset of feature templates as Huang and Sagae (2010).Table 4 shows training time, test time, and parsingaccuracy.
In this table, ?No lookahead (depth = 0)?corresponds to a conventional shift-reduce parsingmethod without any lookahead search.
The results3Penn2Malt is applied for this conversion, while depen-dency labels are removed.242Training Time (sec) Test Time (sec) AccuracyCRF (L1 regularization & SGD training) 847 3 97.11 %No lookahead (depth = 0) 85 5 97.00 %Lookahead (depth = 1) 294 9 97.19 %Lookahead (depth = 2) 8,688 173 97.19 %No lookahead (depth = 0) + tag trigram features 88 5 97.11 %Lookahead (depth = 1) + tag trigram features 313 10 97.22 %Lookahead (depth = 2) + tag trigram features 10,034 209 97.28 %Structured perceptron (Collins, 2002) n/a n/a 97.11 %Guided learning (Shen et al, 2007) n/a n/a 97.33 %CRF with 4 billion features (Lavergne et al, 2010) n/a n/a 97.22 %Table 1: Performance of English POS tagging (training times and accuracy scores on test data)Training time (sec) Test time (sec) F-measureCRF (L1 regularization & SGD training) 74 1 93.66No lookahead (depth = 0) 22 1 93.53Lookahead (depth = 1) 73 1 93.77Lookahead (depth = 2) 1,113 9 93.81Voting of 8 SVMs (Kudo and Matsumoto, 2001) n/a n/a 93.91Table 2: Performance of text chunking (training times and accuracy scores on test data).clearly demonstrate that the lookahead search boostsparsing accuracy.
As expected, training and testspeed decreases, almost by a factor of three, whichis the branching factor of the dependency parser.The table also lists accuracy figures reported inthe literature on shift-reduce dependency parsing.Most of the latest studies on shift-reduce depen-dency parsing employ dynamic programing or beamsearch, which implies that deterministic methodswere not as competitive as those methods.
It shouldalso be noted that all of the listed studies learn struc-tured perceptrons (Collins and Roark, 2004), whileour parser learns locally optimized perceptrons.
Inthis table, our parser without lookahead search (i.e.depth = 0) resulted in significantly lower accuracythan the previous studies.
In fact, it is worse than thedeterministic parser of Huang et al (2009), whichuses (almost) the same set of features.
This is pre-sumably due to the difference between locally opti-mized perceptrons and globally optimized structuredperceptrons.
However, our parser with lookaheadsearch is significantly better than their determinis-tic parser, and its accuracy is close to the levels ofthe parsers with beam search.5 DiscussionThe reason why we introduced a lookahead mech-anism into history-based models is that we wantedthe model to be able to avoid making such mistakesthat can be detected only in later stages.
Probabilis-tic history-based models such as MEMMs should beable to avoid (at least some of) such mistakes by per-forming a Viterbi search to find the highest proba-bility path of the actions.
However, as pointed outby Lafferty et al (2001), the per-state normaliza-tion of probabilities makes it difficult to give enoughpenalty to such incorrect sequences of actions, andthat is primarily why MEMMs are outperformed byCRFs.Perhaps the most relevant to our work in termsof learning is the general framework for search andlearning problems in history-based models proposedby Daume?
III and Marcu (2005).
This framework,called LaSO (Learning as Search Optimization), caninclude many variations of search strategies such asbeam search and A* search as a special case.
In-deed, our lookahead framework could be regardedas a special case in which each search node con-243Training time (sec) Test time (sec) F-measureCRF (L1 regularization & SGD training) 235 4 71.63No lookahead (depth = 0) 66 4 70.17Lookahead (depth = 1) 91 4 72.28Lookahead (depth = 2) 302 7 72.00Lookahead (depth = 3) 2,419 33 72.21Semi-Markov CRF (Okanohara et al, 2006) n/a n/a 71.48Reranking (Yoshida and Tsujii, 2007) n/a n/a 72.65Table 3: Performance of biomedical named entity recognition (training times and accuracy scores on test data).Training time (sec) Test time (sec) AccuracyNo lookahead (depth = 0) 1,937 4 89.73Lookahead (depth = 1) 4,907 13 91.00Lookahead (depth = 2) 12,800 31 91.10Lookahead (depth = 3) 31,684 79 91.24Beam search (k = 64) (Zhang and Clark, 2008) n/a n/a 91.4Deterministic (Huang et al, 2009) n/a n/a 90.2Beam search (k = 16) (Huang et al, 2009) n/a n/a 91.3Dynamic programming (Huang and Sagae, 2010) n/a n/a 92.1Table 4: Performance of English dependency parsing (training times and accuracy scores on test data).sists of the next and lookahead actions4, althoughthe weight updating procedure differs in several mi-nor points.
Daume?
III and Marcu (2005) did not trya lookahead search strategy, and to the best of ourknowledge, this paper is the first that demonstratesthat lookahead actually works well for various NLPtasks.Performing lookahead is a very common tech-nique for a variety of decision-making problemsin the field of artificial intelligence.
In computerchess, for example, programs usually need to per-form a very deep search in the game tree to find agood move.
Our decision-making problem is sim-ilar to that of computer Chess in many ways, al-though chess programs perform min-max searchesrather than the ?max?
searches performed in our al-gorithm.
Automatic learning of evaluation functionsfor chess programs can be seen as the training ofa machine learning model.
In particular, our learn-ing algorithm is similar to the supervised approach4In addition, the size of the search queue is always truncatedto one for the deterministic decisions presented in this paper.Note, however, that our lookahead framework can also be com-bined with other search strategies such as beam search.
In thatcase, the search queue is not necessarily truncated.
(Tesauro, 2001; Hoki, 2006) in that the parametersare optimized based on the differences of the featurevectors realized by the correct and incorrect actions.In history-based models, the order of actions is of-ten very important.
For example, backward taggingis considerably more accurate than forward taggingin biomedical named entity recognition.
Our looka-head method is orthogonal to more elaborate tech-niques for determining the order of actions such aseasy-first tagging/parsing strategies (Tsuruoka andTsujii, 2005; Elhadad, 2010).
We expect that incor-porating such elaborate techniques in our frameworkwill lead to improved accuracy, but we leave it forfuture work.6 ConclusionWe have presented a simple and general frameworkfor incorporating a lookahead process in history-based models and a perceptron-based training algo-rithm for the framework.
We have conducted ex-periments using standard data sets for POS tagging,chunking, named entity recognition and dependencyparsing, and obtained very promising results?theaccuracy achieved by the history-based models en-244hanced with lookahead was as competitive as glob-ally optimized models including CRFs.In most of the experimental results, steady im-provement in accuracy has been observed as thedepth of the search is increased.
Although it isnot very practical to perform deeper searches withour current implementation?we naively exploredall possible sequences of actions, future work shouldencompass extending the depths of search spaceby introducing elaborate pruning/search extensiontechniques.In this work, we did not conduct extensive featureengineering for improving the accuracy of individ-ual tasks because our primary goal with this paper isto present the learning framework itself.
However,one of the major merits of using history-based mod-els is that we are allowed to define arbitrary featureson the partially completed structure.
Another inter-esting direction of future work is to see how muchwe could improve the accuracy by performing ex-tensive feature engineering in this particular learningframework.Appendix A: Convergence of the LearningProcedureLet {xi, aic}Ki=1 be the training examples where aicis the correct first action for decision point xi, andlet Si be the set of all the states at the leaves ofthe search trees for xi generated by the lookaheadsearches and Sic be the set of all the states at theleaves of the search tree for the correct action aic.We also define Si = Si \ Sic.
We write the weightvector before the k-th update as wk.
We defineS?c = argmaxS?Sicw??
(S) and S?
= argmaxS?Siw??
(S)5.Then the update rule can be interpreted as wk+1 =wk +(?
(S?c )??(S?)).
Note that this update is per-formed only when ?
(Sc) ?wk?C < ?(S?)
?wk forall Sc ?
Sc since otherwise S?
in the learning algo-rithm cannot be a state with an incorrect first action.In other words, ?
(Sc) ?w ?
?(S?)
?w ?
C for allSc ?
Sc after convergence.Given these definitions, we prove the convergencefor the separable case.
That is, we assume the exis-tence of a weight vector u (with ||u|| = 1), ?
(> 0),5S?c and S?
depend on the weight vector at each point, butwe omit it from the notation for brevity.and R (> 0) that satisfy:?i,?Sc ?
Sic,?S ?
Si ?
(Sc) ?
u?
?
(S) ?
u ?
?,?i,?Sc ?
Sic,?S ?
Si ||?(Sc)?
?
(S)|| ?
R.The proof is basically an adaptation of the proofsin Collins (2002) and Li et al (2002).
First, we ob-tain the following relation:wk+1 ?
u = wk ?
u+ (?
(S?c ) ?
u?
?(S?)
?
u)= wk ?
u+ ?
?
w1 ?
u+ k?
= k?.Therefore, ||wk+1 ?
u||2 = ||wk+1||2 ?
(k?
)2 ?(1).
We assumed w1 = 0 but this is not an essentialassumption.Next, we also obtain:||wk+1||2 ?
||wk||2 + 2(?
(S?c )?
?(S?))
?wk+||?
(S?c )?
?(S?)||2?
||wk||2 + 2C +R2?
||w1||2 + k(R2 + 2C) = k(R2 + 2C)?
(2)Combining (1) and (2), we obtain k ?
(R2 +2C)/?2.
That is, the number of updates is boundedfrom above, meaning that the learning procedureconverges after a finite number of updates.
Substi-tuting this into (2) gives ||wk+1|| ?
(R2 + 2C)/??
(3).Finally, we analyze the margin achieved by thelearning procedure after convergence.
The margin,?
(w), is defined as follows in this case.?
(w) = minximinSc?Sic,S?Si?
(Sc) ?w ?
?
(S) ?w||w||= minximinSc?Sic?
(Sc) ?w ?
?(S?)
?w||w||After convergence (i.e., w = wk+1), ?
(Sc) ?
w ??(S?
)?w ?
C for all Sc ?
Sc as we noted.
Togetherwith (3), we obtain the following bound:?
(w) ?
minxi?C2C +R2= ?C2C +R2=(?2)(1?
R22C +R2)As can be seen, the margin approaches at least halfthe true margin, ?/2 as C ?
?
(at the cost of infi-nite number of updates).245ReferencesMichael Collins and Brian Roark.
2004.
Incrementalparsing with the perceptron algorithm.
In Proceedingsof ACL, pages 111?118.Michael Collins.
2002.
Discriminative training meth-ods for hidden markov models: Theory and experi-ments with perceptron algorithms.
In Proceedings ofEMNLP, pages 1?8.Hal Daume?
III and Daniel Marcu.
2005.
Learning assearch optimization: Approximate large margin meth-ods for structured prediction.
In Proceedings of ICML,pages 169?176.Yoav Goldbergand Michael Elhadad.
2010.
An effi-cient algorithm for easy-first non-directional depen-dency parsing.
In Proceedings of NAACL-HLT, pages742?750.Kunihito Hoki.
2006.
Optimal control of minimaxsearch results to learn positional evaluation.
In Pro-ceedings of the 11th Game Programming Workshop(GPW), pages 78?83 (in Japanese).Liang Huang and Kenji Sagae.
2010.
Dynamic program-ming for linear-time incremental parsing.
In Proceed-ings of ACL, pages 1077?1086.Liang Huang, Wenbin Jiang, and Qun Liu.
2009.Bilingually-constrained (monolingual) shift-reduceparsing.
In Proceedings of EMNLP, pages 1222?1231.J.-D. Kim, T. Ohta, Y. Tsuruoka, Y. Tateisi, and N. Col-lier.
2004.
Introduction to the bio-entity recogni-tion task at JNLPBA.
In Proceedings of the Interna-tional Joint Workshop on Natural Language Process-ing in Biomedicine and its Applications (JNLPBA),pages 70?75.W Krauth and M Mezard.
1987.
Learning algorithmswith optimal stability in neural networks.
Journal ofPhisics A, 20(11):L745?L752.Taku Kudo and Yuji Matsumoto.
2001.
Chunking withsupport vector machines.
In Proceedings of NAACL.John Lafferty, Andrew McCallum, and Fernando Pereira.2001.
Conditional random fields: Probabilistic mod-els for segmenting and labeling sequence data.
In Pro-ceedings of ICML, pages 282?289.Thomas Lavergne, Olivier Cappe?, and Franc?ois Yvon.2010.
Practical very large scale CRFs.
In Proceed-ings of ACL, pages 504?513.Yaoyong Li, Hugo Zaragoza, Ralf Herbrich, John Shawe-Taylor, and Jaz S. Kandola.
2002.
The perceptronalgorithm with uneven margins.
In Proceedings ofICML, pages 379?386.Mitchell P. Marcus, Beatrice Santorini, and Mary AnnMarcinkiewicz.
1994.
Building a large annotated cor-pus of English: The Penn Treebank.
ComputationalLinguistics, 19(2):313?330.Andrew McCallum, Dayne Freitag, and FernandoPereira.
2000.
Maximum entropy markov models forinformation extraction and segmentation.
In Proceed-ings of ICML, pages 591?598.Joakim Nivre, Johan Hall, and Jens Nilsson.
2004.Memory-based dependency parsing.
In Proceedingsof CoNLL, pages 49?56.Joakim Nivre.
2004.
Incrementality in deterministic de-pendency parsing.
In ACL 2004 Workshop on Incre-mental Parsing: Bringing Engineering and CognitionTogether, pages 50?57.Daisuke Okanohara, Yusuke Miyao, Yoshimasa Tsu-ruoka, and Jun?ichi Tsujii.
2006.
Improving the scal-ability of semi-markov conditional random fields fornamed entity recognition.
In Proceedings of COL-ING/ACL, pages 465?472.Adwait Ratnaparkhi.
1996.
A maximum entropy modelfor part-of-speech tagging.
In Proceedings of EMNLP1996, pages 133?142.Libin Shen, Giorgio Satta, and Aravind Joshi.
2007.Guided learning for bidirectional sequence classifica-tion.
In Proceedings of ACL, pages 760?767.Gerald Tesauro, 2001.
Comparison training of chessevaluation functions, pages 117?130.
Nova SciencePublishers, Inc.Yoshimasa Tsuruoka and Jun?ichi Tsujii.
2005.
Bidirec-tional inference with the easiest-first strategy for tag-ging sequence data.
In Proceedings of HLT/EMNLP2005, pages 467?474.Yoshimasa Tsuruoka, Jun?ichi Tsujii, and Sophia Ana-niadou.
2009.
Stochastic gradient descent trainingfor l1-regularized log-linear models with cumulativepenalty.
In Proceedings of ACL-IJCNLP, pages 477?485.Hiroyasu Yamada and Yuji Matsumoto.
2003.
Statisticaldependency analysis with support vector machines.
InProceedings of IWPT, pages 195?206.Kazuhiro Yoshida and Jun?ichi Tsujii.
2007.
Rerankingfor biomedical named-entity recognition.
In Proceed-ings of ACL Workshop on BioNLP, pages 209?216.Yue Zhang and Stephen Clark.
2008.
A tale oftwo parsers: investigating and combining graphbasedand transition-based dependency parsing using beam-search.
In Proceedings of EMNLP, pages 562?571.246
