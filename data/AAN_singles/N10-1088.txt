Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 627?635,Los Angeles, California, June 2010. c?2010 Association for Computational LinguisticsExtracting Glosses to Disambiguate Word SensesWeisi DuanCarnegie Mellon UniversityLanguage Technologies Institute5000 Forbes Ave.Gates Hillman Complex 5407Pittsburgh, PA 15213wduan@cs.cmu.eduAlexander YatesTemple UniversityComputer and Information Sciences1805 N. Broad St.Wachman Hall 303APhiladelphia, PA 19122yates@temple.eduAbstractLike most natural language disambiguationtasks, word sense disambiguation (WSD) re-quires world knowledge for accurate predic-tions.
Several proxies for this knowledgehave been investigated, including labeled cor-pora, user-contributed knowledge, and ma-chine readable dictionaries, but each of theseproxies requires significant manual effort tocreate, and they do not cover all of the ambigu-ous terms in a language.
We investigate thetask of automatically extracting world knowl-edge, in the form of glosses, from an unlabeledcorpus.
We demonstrate how to use theseglosses to automatically label a training cor-pus to build a statistical WSD system that usesno manually-labeled data, with experimentalresults approaching that of a supervised SVM-based classifier.1 IntroductionFor many semantic natural language processingtasks, systems require world knowledge to disam-biguate language utterances.
Word sense disam-biguation (WSD) is no exception ?
systems forWSD require world knowledge to figure out whichaspects of a word?s context indicate one sense overanother.
A fundamental problem for WSD is that therequired knowledge is open-ended.
That is, for ev-ery ambiguous term, new kinds of information aboutthe world become important, and the knowledge thata system may have acquired for previously-studiedambiguous terms may have little or no impact on thenext ambiguous term.
Thus open-ended knowledgeacquisition is a fundamental obstacle to strong per-formance for this disambiguation task.Researchers have investigated a variety of tech-niques that address this knowledge acquisition bot-tleneck in different ways.
Supervised WSD tech-niques, for instance, can learn to associate featuresin the context of a word with a particular sense ofthat word.
Knowledge-based techniques rely onmachine-readable dictionaries or lexical resourceslike WordNet (Fellbaum, 1998) to provide the nec-essary knowledge.
And most recently, systemshave used resources like Wikipedia, which containuser-contributed knowledge in the form of sense-disambiguated links, to acquire world knowledge forWSD.
Yet each of these approaches is limited by theamount of manual effort that is needed to build thenecessary resources, and as a result the techniquesare limited to a subset of English words for whichthe manually-constructed resources are available.In this work we investigate an alternative ap-proach that attacks the problem of knowledge acqui-sition head-on.
We use information extraction (IE)techniques to extract glosses, or short textual char-acterizations of the meaning of one sense of a word.In the ideal case, we would extract full logical formsto define word senses, but here we instead focus ona more feasible, but still very useful, sub-task: for agiven word sense, extract a collection of terms thatare highly correlated with that sense and no othersense of the ambiguous word.
Our system requiresas input only an unlabeled corpus of documents thateach contain the ambiguous term of interest.In experiments, we demonstrate that our glossextraction system can often determine key aspectsof a word?s senses.
In one experiment our sys-tem was able to extract glosses with 60% precisionfor 20 ambiguous biomedical terms, while discov-ering 7 senses of those terms that never appearedin a widely-used dictionary of biomedical terminol-ogy.
In addition, we demonstrate that our extractedglosses are useful for real WSD problems: our sys-627tem outperforms a state-of-the-art unsupervised sys-tem, and it comes close to the performance of a su-pervised WSD system on a challenging dataset.In the next section, we describe previous work.
InSection 3, we formally define the gloss extractiontask and refine it into a sub-task that is feasible foran IE approach, and Section 5 presents our techniquefor using extracted glosses in a WSD task.
Section6 discusses our experiments and results.2 Previous WorkMany previous systems (Cui et al, 2007; Androut-sopoulos and Galanis, 2005) have studied the re-lated task of answering definitional questions on theWeb, such as ?What does cold mean??.
Such sys-tems are focused on information retrieval for humanconsumption, and especially on recall of definitionalinformation (Velardi et al, 2008).
They generallydo not consider the problem of how to merge thelarge number of similar extracted definitions into asingle item (Fujii and Ishikawa, 2000), so that theoverall result contains one definition per sense ofthe word.
A separate approach (Pasca, 2005) relieson the WordNet lexical database to supply the set ofsenses, and extracts alternate glosses for the sensesthat have already been defined.
When glosses are tobe used by computational methods, as in a WSD sys-tem in our case, it becomes critical that the systemextract one coherent gloss per sense.
As far as weare aware, no previous system has extracted glossesfor word sense disambiguation.Gloss extraction is related to the task of ontol-ogy extraction, in which systems extract hierarchiesof word classes (Snow et al, 2006; Popescu et al,2004).
Gloss extraction differs from ontology ex-traction in that it extracts definitional informationcharacterizing senses of a single word, rather thantrying to place a word in a hierarchy of other words.Most WSD systems have relied on hand-labeledtraining examples (Leroy and Rindflesch, 2004;Joshi et al, 2005; Mohammad and Pedersen, 2004)or on dictionary glosses (Lesk, 1986; Stevensonand Wilks, 2001) or the WordNet hierarchy (Boyd-Graber et al, 2007) to help make disambiguationchoices.
In recent coarse-grained evaluations, suchsystems have achieved accuracies of close to 90%(Pradhan et al, 2007; Agirre and Soroa, 2007; Schi-jvenaars et al, 2005).
However, by some estimates,English contains over a million word types, and newwords and new senses are added to the language ev-ery day.
It is unreasonable to expect that any systemwill have access to hand-labeled training examplesor useful dictionary glosses for each of them.More recent techniques based on user-contributedknowledge (Mihalcea, 2007; Chklovski and Mihal-cea, 2002; Milne and Witten, 2008), such as thatfound in Wikipedia, suffer from similar problems ?Wikipedia contains many articles on well known en-tities, categories, and events, but very few articlesthat disambiguate verbs, adjectives, adverbs, andcertain kinds of nouns which are poorly representedin an encyclopedia.On the other hand, word usages in large corporalike the Web reflect nearly all of the word sensesin use in English today, albeit without manually-supplied labels.
Unsupervised approaches to WSDuse clustering techniques to group instances ofwords into clusters that correspond to differentsenses (Pantel and Lin, 2002).
While such systemsare more general than supervised and dictionary-based approaches in that they can handle any wordtype and word sense, they have lagged behind otherapproaches in terms of accuracy thus far ?
for ex-ample, the best system in the recent word sense in-duction task of Semeval 2007 (Agirre and Soroa,2007) achieved an F1 score of 78.7, slightly belowthe baseline (78.9) in which all instances of a wordare part of a single cluster.
Part of the problemis that the clustering techniques operate in a bag-of-words-like representation.
This is an extremelyhigh-dimensional space, and it is difficult in sucha space to determine which dimensions are noiseand which ones correlate with different senses.
Ourgloss extraction technique helps to address this curseof dimensionality by reducing the large vocabularyof a corpus to a much smaller set of terms that arehighly relevant for WSD.
Others (Kulkarni and Ped-ersen, 2005) have used feature selection techniqueslike mutual information to reduce dimensionality,but so far these techniques have only been able tofind features that correlate with an ambiguous term.With gloss extraction, we are able to find featuresthat correlate with individual senses of a term.3 Overview: The Gloss Extraction TaskGiven an input corpus C of documents where eachdocument contains at least one instance of a keywordk, a Gloss Extraction system should produce a set ofglosses G = {gi}, where each gi is a logical expres-sion defining the meaning of a particular sense si of628Glosses:1. cold(a) ?
isA(a, b) ?
disease(b) ?
symptom(a, c) ?
possibly-includes(c, d) ?
fever(d)2. cold(a) ?
isA(a, b) ?
physical-entity(b) ?
temperature(a, c) ?
less-than(c, 25C)Sense Indicators:1. common cold, virus, symptom, fever2.
hot, ice cold, lukewarm, cold room, room temperatureFigure 1: Example glosses and sense indicators for two senses of the word cold.k, to the exclusion of all other senses of k. Note thatthe system must discover the appropriate number ofsenses in addition to the gloss for each sense.While extraction technology has made impressiveadvancements, it is not yet at a stage where it canproduce full logical forms for sense glosses.
As afirst step towards this goal, we introduce the task ofSense Indicator Extraction, in which each gloss giconsists of a set of features that, when present in thecontext of an instance of k, strongly indicate that theinstance has sense si, and no other sense.
Exam-ples of both tasks are given in Figure 1.
The SenseIndicator Extraction task represents a nontrivial ex-traction challenge, but it is much more feasible thanfull Gloss Extraction.
And the task preserves keyproperties of Gloss Extraction: the results are quiteuseful for word sense disambiguation.
The resultsare also readily interpreted upon inspection, makingit easy to monitor a system?s accuracy.4 Extracting Word Sense GlossesWe present the GLOSSY system, an unsupervised in-formation extraction system for Sense Indicator Ex-traction.
GLOSSY proceeds in two phases: a col-location detection phase, in which the system de-tects components of the glosses, and an arrangementphase, in which the system decides how many dis-tinct senses there are, and puts together the compo-nents of the glosses.4.1 Collocation DetectionThe first major challenge to a Gloss Extraction sys-tem is that the space of possible features is enor-mous, and almost all of them are irrelevant to thetask at hand.
Supervised techniques can use la-beled examples to provide clues, but in an unsu-pervised setting the curse of dimensionality can beoverwhelming.
Indeed, unsupervised WSD tech-niques suffer from exactly this problem.GLOSSY?s answer to this problem is based on thefollowing observation: pairs of potential featureswhich rarely or never co-occur in the same docu-ment in a large corpus are likely to represent fea-tures for two distinct senses.
The well-known obser-vation that words rarely exhibit more than one senseper discourse (Yarowsky, 1995) implies that featuresclosely associated with a particular sense have a lowprobability of appearing in the same document asfeatures associated with another sense.
Features thatare independent of any particular sense of the key-word, on the other hand, have no such restriction,and are just as likely to appear in the context of onesense as any other.
As a consequence, a low countfor the co-occurrence of two potential features overa large corpus of documents for keyword k is a re-liable indicator that the two features are part of theglosses of two distinct senses of k.GLOSSY?s collocation detector begins by index-ing the corpus and counting the frequency of eachvocabulary word.
Using the index, the collocationdetector determines all pairs of potential featuressuch that each feature appears at least T times, andthe pair of features never co-occurs in the same doc-ument.
We call the pairs that this step finds the ?non-overlapping?
features.
Finally, we rank the featurepairs according to the total number of documentsthey appear in, and choose the most frequent Npairs.
This excludes non-overlapping pairs that havenot been seen often enough to provide reliable evi-dence that they are features of different senses, andit cuts down on processing time for the next phase ofthe algorithm.
The collocation detector outputs theset of features F = {f |?f ?
(f, f ?)
or (f ?, f) is oneof the top N non-overlapping pairs}.
The GLOSSYsystem uses stems, words, and bigrams as potentialfeatures.
We use N = 100 and T = 50 in our ex-periments.
Figure 2 shows an example corpus andthe set of features that the collocation detector wouldoutput.629Corpus of documents for term cold:DOCUMENT 1: ?Symptoms of the common cold may include fever, headache, sore throat, and coughing.
?DOCUMENT 2: ?Hibernation is a common response to the cold winter weather of temperate climates.
?Non-overlapping feature pairs:(symptoms,temperate) (headache, climate) (cold winter, common cold) (response, headache)Detected collocations:symptoms, temperate, headache, climate, cold winter, common cold, responseArranged glosses:cold 1: symptoms, common cold, headachecold 2: temperate, climate, cold winterFigure 2: Example operation of the GLOSSY extraction system.
The collocation detector finds potential featuresusing its non-overlapping pair heuristic.
The arranger selects a subset of the potential features (in this example, itdrops the feature response) and clusters them to produces glosses containing sense indicators.4.2 Arranging GlossesGiven the corpus C for keyword k and the features Fthat GLOSSY?s collocation detector has discovered,the arrangement phase groups these features into co-herent sense glosses.
Figure 2 shows an example ofhow the features found during collocation detectionmay be arranged to form coherent glosses for twosenses of the word ?cold.
?GLOSSY?s Arranger component uses a combina-tion of a small set of statistics to determine whethera particular arrangement of the features into glossesis warranted, based on the given corpus.
Let A ?
2Fbe an arrangement of the features into clusters rep-resenting glosses.
We require that clusters in A bedisjoint, but we do not require every feature in F tobe included in a cluster in A ?
in other words, Ais a partition of a subset of F .
We define a scoringfunction S that is a linear interpolation of severalstatistics of the arrangement A and the corpus C:S(A|C,w) =?iwifi(A, C) (1)After experimenting with a number of options, wesettled on the following for our statistics fi:NUMCLUSTERS: the number of clusters in A. Weuse a negative weight for this statistic to favor fewersenses and encourage clustering.DOCSCOVERED: the total number of documents inC in which at least one feature from A appears.
Weuse this statistic to encourage the Arranger to find anarrangement that explains the sense of as many ex-amples of the keyword as possible.BADOVERLAPS: the number of pairs of featuresthat co-occur in at least one document in C, and thatbelong to different clusters of A.
A negative weightfor this statistic encourages overlapping feature pairsto be placed in the same cluster.BADNONOVERLAPS: the number of pairs of fea-tures that never co-occur in C, and that belong to thesame cluster in A.
A negative weight for this statis-tic encourages non-overlapping feature pairs to beplaced in different clusters.Given such an optimization function, the Ar-ranger attempts to maximize its value by search-ing for an optimal A.
Note that this is a struc-tured prediction task in which the choice for somesub-component of A can greatly affect the choice ofother clusters and features.
GLOSSY addresses thisoptimization problem with a greedy hill-climbingsearch with random restarts.
Each round of hill-climbing is initialized with a randomly chosen sub-set of features, which are then all assigned to a sin-gle cluster.
Using a randomly chosen search opera-tor from a pre-defined set, the search procedure at-tempts to move to a new arrangement A?.
It acceptsthe move to A?
if the optimization function gives ahigher value than at the previous state; otherwise, itcontinues from the previous state.
Our set of searchoperators include a move that splits a cluster; a movethat joins two clusters; a move that swaps a featurefrom one cluster to another; a move that removes afeature from the arrangement altogether; and a move630that adds a feature from the pool of unused features.We used 100 rounds of hill-climbing, and found thateach round converged in fewer than 1000 moves.To estimate the weights wi for each of the fourfeatures of the Arranger, we use a development cor-pus consisting of 185 documents each containing thesame ambiguous term, and each labeled with senseinformation.
Because of the small number of pa-rameters, we performed a grid search on the devel-opment data for the optimal values of the weights.5 A Bootstrapping WSD SystemYarowsky (1995) first recognized that it is possi-ble to use a small number of features for differentsenses to bootstrap an unsupervised word sense dis-ambiguation system.
In Yarowsky?s work, his sys-tem requires an initial, manually-supplied colloca-tion as a feature for each sense of a keyword.
In con-trast, we can use GLOSSY?s extracted glosses to sup-ply starter features fully automatically, using only anunlabeled corpus.
Thus GLOSSY complements theefforts of Yarowsky and other bootstrapping tech-niques for WSD (Diab, 2004; Mihalcea, 2002).Building on their efforts, we design a boot-strapping WSD system using GLOSSY?s extractedglosses as follows.
Let A be the arranged featuresrepresenting glosses for a keyword.
We first retrieveall the documents from our unlabeled corpus whichcontain features in A.
We then label appearancesof the target word according to the cluster of thefeatures that appear in that document.
If featuresfor more than one cluster appear in the same docu-ment, we discard it.
The result is an automaticallylabeled corpus containing examples of all the ex-tracted senses.We use this automatically labeled ?bootstrap cor-pus?
to perform supervised WSD.
This allows oursystem a great deal of flexibility once the bootstrapcorpus is created: we can use any features of thecorpus, plus the labels, in our classifier.
Importantly,this means we do not need to rely on just the featuresin the extracted glosses.
We use a multi-class SVMclassifier with a linear kernel and default parametersettings.
We use LibSVM (Chang and Lin, 2001) forall of our experiments.
We use standard features forsupervised WSD (Liu et al, 2004): all stems, words,bigrams, and trigrams within a context window of 20words surrounding the ambiguous term.6 ExperimentsWe ran two types of experiments, one to measurethe accuracy of our sense gloss extractor, and one tomeasure the usefulness of the extracted knowledgefor word sense disambiguation.6.1 DataWe use a dataset of biomedical literature abstractsfrom Duan et al(2009).
The data contains a set ofdocuments for 21 ambiguous terms.
We reservedone of these terms (?MCP?)
for setting parameters,and ran our algorithms on the remaining keywords.The ambiguous terms vary from acronyms (7 terms),which are common and important in biomedical lit-erature, to ambiguous biomedical terminology (3terms), to terms like ?culture?
and ?mole?
that havesome biomedical senses and some senses that arepart of the general lexicon (11 terms).
There were onaverage 271 labeled documents per term; the small-est number of documents for a term is 125, andthe largest is 503.
For every ambiguous term, weadded on average 9625 (minimum of 1544, maxi-mum of 15711) unlabeled documents to our collec-tion by searching for the term on PubMed Centraland downloading additional PubMed abstracts.6.2 Extracting GlossesWe measured the performance of GLOSSY?s glossextraction by comparing the extracted glosses withdefinitions contained in the Unified Medical Lan-guage System (UMLS) Metathesaurus.
First, foreach ambiguous term, we looked up the set of ex-act matches for that term in the Metathesaurus, anddownloaded definitions for all of the different senseslisted under that term.
Wherever possible, we usedthe MeSH definition of a sense; when that was un-available, we used the definition from the NCI The-saurus; and when both were unavailable, we used thedefinition from the resource listed first.
34 senses(40%) had no available definitions at all, but in allcases, the Metathesaurus lists a short (usually 1-3word) gloss of the sense, which we used instead.We manually aligned extracted glosses withUMLS senses in a way that maximizes the numberof matched senses for every ambiguous term.
Weconsider an extracted gloss to match a UMLS sensewhen the extracted gloss unambiguously refers toa single sense of the ambiguous term, and thatsense matches the definition in UMLS.
Typically,this means that the extracted features in the gloss631overlap content words in the UMLS definition (e.g.,the extracted feature ?symptoms?
for the ?commoncold?
sense of the term ?cold?).
In some cases, how-ever, there was no strict overlap in content wordsbetween the extracted gloss and the UMLS defini-tion, but the sense of the extracted gloss still unam-biguously matched a unique UMLS sense: e.g., forthe term ?transport,?
the extracted gloss ?intracel-lular transport?
was matched with the UMLS senseof ?Molecular Transport,?
which the NCI Thesaurusdefines as, ?Any subcellular or molecular process in-volved in translocation of a biological entity, suchas a macromolecule, from one site or compartmentto another.?
In the end, such matchings were deter-mined by hand.
Table 1 shows extracted glosses andUMLS definitions for the term ?mole.
?For each ambiguous term, we measure the num-ber of extracted glosses, the number of UMLSsenses, and the number of matches between thetwo.
We report on the precision (number of matches/ number of extracted glosses), recall (number ofmatches / number of UMLS senses), and F1 score(harmonic mean of precision and recall).
Table 2shows the average of the precision and recall num-bers over all terms.
Since these terms have differentnumbers of senses, we can compute this average intwo different ways: a Macro average, in which eachterm has equal weight in the average; and a Microaverage, in which each term?s weight in the averageis proportional to the number of senses (extractedsenses for the precision, and UMLS senses for therecall).
We report on both.A strict matching between GLOSSY?s glosses andUMLS senses is potentially unfair to GLOSSY inseveral ways: GLOSSY may discover valid sensesthat happen not to appear in UMLS; UMLS sensesmay overlap one another, and so multiple UMLSsenses may match a single GLOSSY gloss; and thetwo sets of senses may differ in granularity.
For thesake of repeatable experiments, in this evaluation wemake no attempt to change existing UMLS senses.However, to highlight one particular strength ofthe Gloss Extraction paradigm, we do consider aseparate evaluation that allows for new senses thatGLOSSY discovers, but do not appear in UMLS.For instance, ?biliopancreatic diversion?
and ?bipo-lar disorder?
are both valid senses for the acronym?BPD.?
GLOSSY discovers both, but UMLS doesnot contain entries for either, so in our original eval-uation both senses would count against GLOSSY?sprecision.
To correct for this, our second evalua-tion adds senses to the list of UMLS senses when-ever GLOSSY discovers valid entries missing fromthe Metathesaurus.
The last five columns of Table 2show our results under these conditions.Despite the difficulty of the task, GLOSSY is ableto find glosses with 53% precision and 47% re-call (Macro average, no discovered senses) usingonly unlabeled corpora as input, and it is extract-ing roughly the right number of senses for each am-biguous term.
In addition, GLOSSY is able to iden-tify 7 valid senses missing from UMLS for the 20terms in our evaluation.
Including these senses inthe evaluation increases GLOSSY?s F1 by 6.2 pointsMicro (4.7 Macro).
We are quite encouraged bythe results, especially because they hold promise forWSD.
Note that in order to improve upon a WSDbaseline which tags all instances of a word as thesame sense, GLOSSY only needs to be able to sep-arate one sense from the rest.
GLOSSY is findingbetween 1.85 and 2.2 correct glosses per term, morethan enough to help with WSD.6.3 WSD with Extracted GlossesWhile extracting glosses is an important applicationin its own right, we also aim to show that this ex-tracted knowledge is useful for an established ap-plication: namely, word sense disambiguation.
Ournext experiment compares the performance of ourWSD system with an established unsupervised al-gorithm, and with a supervised technique ?
supportvector machines (SVMs).Using the same dataset as above, we trainedGLOSSY on the ambiguous term ?MCP?, and testedit on the remaining ones.
For comparison, we alsoreport the state-of-the-art results of Duan et al?s(2009) SENSATIONAL system, and the results of aBASELINE system that lumps all documents intoa single cluster.
SENSATIONAL is a fast cluster-ing system based on minimum spanning trees anda pruning mechanism that eliminates noisy pointsfrom consideration during clustering.
Since SEN-SATIONAL uses both ?MCP?
and ?white?
to train asmall set of parameters, we leave ?white?
out of ourcomparison as well.
We measure accuracy by align-ing each system?s clusters with the gold standardclusters in such a way as to maximize the numberof elements that belong to aligned clusters.
We usean implementation of the MaxFlow algorithm to de-termine this alignment.
We then compute accuracy632GLOSSY UMLS1.
choriocarcinoma,invasive, complete,hydatidiform mole,hydatidiform1.
Hydatidiform Mole ?
Trophoblastic hyperplasia associated with normal gestation,or molar pregnancy.
.
.
.
Hydatidiform moles or molar pregnancy may be catego-rized as complete or partial based on their gross morphology, histopathology, andkaryotype.2.
grams per mole 2.
Mole, unit of measurement ?
A unit of amount of substance, one of the sevenbase units of the International System of Units.
It is the amount of substance thatcontains as many elementary units as there are atoms in 0.012 kg of carbon-12.3.
mole fractions -- 3.
Nevus ?
A circumscribed stable malformation of the skin .
.
.
.- 4.
Talpidae ?
Any of numerous burrowing mammals found in temperate regions .
.
.Table 1: GLOSSY?s extracted glosses and UMLS dictionary entries for the example term ?mole?.Without Discovered Senses With Discovered SensesGLOSSY UMLS UMLSSenses Senses Matches P R F1 Senses Matches P R F1Macro Avg 4.35 4.25 1.85 53.1 47.1 49.9 4.6 2.2 60.6 49.7 54.6Micro Avg N/A N/A N/A 42.5 43.5 43.0 N/A N/A 50.6 47.8 49.2Table 2: GLOSSY can automatically discover glosses that match definitions in an online dictionary.
?WithoutDiscovered Senses?
counts only the senses that are listed in the UMLS Metathesaurus; ?With Discovered Senses?enhances the Metathesaurus with 7 new senses that GLOSSY has automatically discovered.as the percentage of elements that belong to alignedclusters.
This metric is very similar to the so-called?supervised?
evaluation of Agirre et al (2006).The first four columns of Table 3 show our results.Clearly, both SENSATIONAL and GLOSSY outper-form the BASELINE significantly, and traditionallythis is a difficult baseline for unsupervised WSD sys-tems to beat.
SENSATIONAL outperforms GLOSSYby approximately 6%.
There appear to be two rea-sons for this.
In other experiments, SENSATIONALhas been shown to be competitive with supervisedsystems, but only when the corpus consists mostlyof two, fairly well-balanced senses, as is true forthis particular dataset, where the two most commonsenses always covered at least 70% of the examplesfor every ambiguous term.A more serious problem for GLOSSY is that theunlabeled corpus that it extracts glosses from maynot match well with the labeled test data.
If the rela-tive frequency of senses in the unlabeled documentsdoes not match the relative frequency of senses inthe labeled test set, GLOSSY may not extract theright set of glosses.
Manual inspection of the ex-tracted glosses shows that this is indeed a problem:for example, the labeled data contains two senses ofthe word ?mole?
: a discolored area of skin (78%),and a burrowing mammal (22%); our unlabeled datacontains both of these senses, but the additionalsense of ?mole?
as a unit of measurement is by farpredominant.
GLOSSY manages to extract glossesfor ?skin?
and ?unit of measurement,?
but misses outon ?mammal?
as a result of the skew in the data.Note that this problem, though serious for our ex-periments, is largely artificial from the point of viewof applications.
In a typical usage of a WSD system,there is a supply of data that the system needs to dis-ambiguate, and accuracy is measured on a labeledsample of this data.
Here, we started from a sampleof labeled data, constructed a larger corpus that doesnot necessarily match it, and then ran our algorithm.To correct for the artificial bias in our experiment,we ran a second test in which we manually labeled arandom sample of 100 documents for each ambigu-ous term from the larger unlabeled corpus.
We useda subset of 14 of the 21 keywords in the originaldataset.
As before, we compared our system againstSENSATIONAL and the most-frequent-sense BASE-LINE.
We also compare against an SVM system us-ing 3-fold cross-validation.
We use a linear kernelSVM, with the same set of features that are available633Duan et al(2009) Data Sampled DataNum.
BASE- SENSE- Num.
BASE- SENSE-Keyword senses LINE GLOSSY ATIONAL senses LINE ATIONAL GLOSSY SVMANA 2 63.1 87.9 100 13 75 79 74 75.8BPD 3 39.8 71.6 52.9 7 33 48 85 66.7BSA 2 50.1 77.9 94.7 5 97 53 89 87.9CML 2 55.0 99.2 89.5 4 81 75 84 75.8MAS 2 50.0 100 100 35 46 90 67 66.7VCR 2 79.2 79.2 64.0 8 72 32 72 75.8cold 3 37.1 73.3 66.8 3 87 81 44 90.9culture 2 52.0 67.1 81.7 3 74 39 62 66.7discharge 2 66.3 82.4 95.1 5 57 41 84 54.5fat 2 50.6 50.1 53.2 2 97 60 97 97.0mole 2 78.3 71.3 95.8 7 78 47 57 84.8pressure 2 52.1 69.8 86.4 5 47 60 65 75.8single 2 50.0 59.7 99.5 4 53 63 37 45.4white - - - - 7 32 33 58 51.5fluid 2 64.3 83.5 99.6 - - - - -glucose 2 50.5 64.5 50.5 - - - - -inflammation 3 35.5 52.8 50.4 - - - - -inhibition 2 50.4 50.4 54.2 - - - - -nutrition 3 38.8 53.8 54.9 - - - - -transport 2 50.6 41.1 56.8 - - - - -AVERAGE 2.16 53.4 70.3 76.1 7.71 66.3 57.2 69.6 72.5Diff from BL - 0.0 +16.9 +22.7 - 0.0 -9.1 +3.3 +6.2Table 3: GLOSSY?s extracted glosses can be used to create an unsupervised WSD system that achieves an accu-racy within 3% of a supervised system.
Our WSD system outperforms our BASELINE system, widely recognizedas a difficult baseline for unsupervised WSD, by 16.9% and 3.3% on two different datasets.to the SVM in the GLOSSY system.
We run our un-supervised systems on all of the unlabeled data, andthen intersect the resulting clusters with the docu-ment set that we randomly sampled.The last four columns of Table 3 show our results.The sampled data set appears to be a significantlyharder test, since even the supervised SVM achievesonly a 6% gain over the BASELINE.
The SEN-SATIONAL system does significantly worse on thisdata, where there is a wider variation in the distri-bution of senses.
The GLOSSY system outperformsboth the SENSATIONAL system and the BASELINE.7 Conclusion and Future WorkGloss Extraction is an important, and difficult task ofextracting definitions of words from unlabeled text.The GLOSSY system succeeds at a more feasible re-finement of this task, the Sense Indicator Extrac-tion task.
GLOSSY?s extractions have proven use-ful as seed definitions in an unsupervised WSD task.There is a great deal of room for future work in ex-panding the ability of Gloss Extraction systems toextract sense glosses that more closely match themeanings of a word.
An important first step in thisdirection is to extract relations, rather than ngrams,that make up the definition a word?s senses.AcknowledgmentsPresentation of this work was supported by the Insti-tute of Education Sciences, U.S. Department of Ed-ucation, through Grant R305A080157 to CarnegieMellon University.
The opinions expressed are thoseof the authors and do not necessarily represent theviews of the Institute or the U.S. Department of Edu-cation.
The authors thank the anonymous reviewersfor their helpful suggestions and comments.634ReferencesEneko Agirre and Aitor Soroa.
2007.
Semeval 2007 task02: Evaluating word sense induction and discrimina-tion systems.
In Proceedings of the Fourth Interna-tional Workshop on Semantic Evaluations (SemEval),pages 7?12.E.
Agirre, O. Lopez de Lacalle, D. Martinez, andA.
Soroa.
2006.
Evaluating and optimizing the param-eters of an unsupervised graph-based WSD algorithm.In Proceedings of the NAACL Textgraphs Workshop.I.
Androutsopoulos and D. Galanis.
2005.
A practi-cally unsupervised learning method to identify single-snippet answers to definition questions on the web.
InProceedings of HLT-EMNLP, pages 323?330.Jordan Boyd-Graber, David M. Blei, and Xiaojin Zhu.2007.
A topic model for word sense disambiguation.In Empirical Methods in Natural Language Process-ing.Chih-Chung Chang and Chih-Jen Lin, 2001.
LIBSVM: alibrary for support vector machines.T.
Chklovski and R. Mihalcea.
2002.
Building a sensetagged corpus with Open Mind Word Expert.
In Pro-ceedings of the Workshop on Word Sense Disambigua-tion: Recent Successes and Future Directions.H.
Cui, M.K.
Kan, and T.S.
Chua.
2007.
Soft patternmatching models for definitional question answering.ACM Trans.
Information Systems, 25(2):1?30.Mona Diab.
2004.
Relieving the data acquisition bottle-neck in word sense disambiguation.
In Proceedings ofthe ACL.Weisi Duan, Min Song, and Alexander Yates.
2009.
Fastmax-margin clustering for unsupervised word sensedisambiguation in biomedical texts.
BMC Bioinfor-matics, 10(S3)(S4).Christiane Fellbaum, editor.
1998.
WordNet: An Elec-tronic Lexical Database.
Bradford Books.A.
Fujii and T. Ishikawa.
2000.
Utilizing the world wideweb as an encyclopedia: Extracting term descriptionsfrom semi-structured texts.
In Proceedings of ACL,pages 488?495.M.
Joshi, T. Pedersen, and R. Maclin.
2005.
A compar-ative study of support vector machines applied to thesupervised word sense disambiguation problem in themedical domain.
In Proceedings of the Second IndianInternational Conference on Artificial Intelligence.Anagha Kulkarni and Ted Pedersen.
2005.
Name dis-crimination and email clustering using unsupervisedclustering and labeling of similar contexts.
In Pro-ceedings of the Second Indian International Confer-ence on Artificial Intelligence, pages 703?722.Gondy Leroy and Thomas C. Rindflesch.
2004.
Us-ing symbolic knowledge in the umls to disambiguatewords in small datasets with a naive bayes classifier.In MEDINFO.M.E.
Lesk.
1986.
Automatic sense disambiguation us-ing machine readable dictionaries: How to tell a pinecone from an ice cream cone.
In Proceedings of theSIGDOC Conference.Hongfang Liu, Virginia Teller, and Carol Friedman.2004.
A multi-aspect comparison study of supervisedword sense disambiguation.
Journal of the AmericanMedical Informatics Association, 11:320?331.Rada Mihalcea.
2002.
Bootstrapping large sense-taggedcorpora.
In International Conference on LanguagesResources and Evaluations (LREC).Rada Mihalcea.
2007.
Using wikipedia for automaticword sense disambiguation.
In Proceedings of theNAACL.David Milne and Ian H. Witten.
2008.
Learning to linkwith wikipedia.
In Proceedings of the 17th Conferenceon Information and Knowledge Management (CIKM).S.
Mohammad and Ted Pedersen.
2004.
Combining lex-ical and syntactic features for supervised word sensedisambiguation.
In Proceedings of CoNLL.P.
Pantel and D. Lin.
2002.
Discovering word sensesfrom text.
In Procs.
of ACM Conference on Knowl-edge Discovery and Data Mining (KDD-02).Marius Pasca.
2005.
Finding instance names and alterna-tive glosses on the web: WordNet reloaded.
In Com-putational Linguistics and Intelligent Text Processing,pages 280?292.
Springer Berlin / Heidelberg.Ana-Maria Popescu, Alexander Yates, and Oren Etzioni.2004.
Class extraction from the world wide web.
InAAAI-04 ATEM Workshop, pages 65?70.Sameer Pradhan, Edward Loper, Dmitriy Dligach, andMartha Palmer.
2007.
Semeval-2007 task-17: Englishlexical sample, srl and all words.
In Proceedings ofthe Fourth International Workshop on Semantic Eval-uations (SemEval).B.J.
Schijvenaars, B. Mons, M. Weeber, M.J. Schuemie,E.M.
van Mulligen, H.M. Wain, and J.A.
Kors.
2005.Thesaurus-based disambiguation of gene symbols.BMC Bioinformatics, 6.R.
Snow, D. Jurafsky, and A. Y. Ng.
2006.
Semantictaxonomy induction from heterogenous evidence.
InCOLING/ACL.M.
Stevenson and Yorick Wilks.
2001.
The interactionof knowledge sources in word sense disambiguation.Computational Linguistics, 27(3):321?349.Paola Velardi, Roberto Navigli, and Pierluigi D?Amadio.2008.
Mining the web to create specialized glossaries.IEEE Intelligent Systems, 23(5):18?25.David Yarowsky.
1995.
Unsupervised word sense dis-ambiguation rivaling supervised methods.
In Proceed-ings of the ACL.635
