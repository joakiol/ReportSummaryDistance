Proceedings of the Workshop on Language in Social Media (LSM 2011), pages 86?95,Portland, Oregon, 23 June 2011. c?2011 Association for Computational LinguisticsEmail Formality in the Workplace: A Case Study on the Enron CorpusKelly Peterson, Matt Hohensee, and Fei XiaLinguistics DepartmentUniversity of WashingtonSeattle, WA 98195{kellypet, hohensee, fxia}@uw.eduAbstractEmail is an important way of communicationin our daily life and it has become the subjectof various NLP and social studies.
In this pa-per, we focus on email formality and explorethe factors that could affect the sender?s choiceof formality.
As a case study, we use the En-ron email corpus to test how formality is af-fected by social distance, relative power, andthe weight of imposition, as defined in Brownand Levinson?s model of politeness (1987).Our experiments show that their model largelyholds in the Enron corpus.
We believe thatthe methodology proposed in the paper can beapplied to other social media domains and beused to test other linguistic or social theories.1 IntroductionEmail has become an important way of communica-tion in our daily life.
Because of its wide usage,it has been the subject of various studies such associal network analysis (e.g., (Leuski, 2004; Dies-ner et al, 2005; Carvalho et al, 2007)), deceptiondetection (e.g., (Zhou et al, 2004; Keila and Skill-corn, 2005)), information extraction (e.g., (Culottaet al, 2004; Minkov et al, 2005)), and topic discov-ery (e.g., (McCallum et al, 2007)).
In this study, wefocus on email formality in various social settings;that is, we want to determine whether the choice offormality in email communication is affected by fac-tors such as the social distance and relative powerbetween the senders and the recipients.While an early perspective of email communica-tion held that email is a lean medium which lacks vi-tal social cues (Daft and Lengel, 1986), other workhas shown that senders of email exhibit a wide rangeof language and form choices which vary in differ-ent social contexts (Orlikowski and Yates, 1994).Through various theories of sociolinguistics, it isproposed that these changes take place in a pre-dictable manner.Brown and Levinson (1987) have proposed amodel where in order to save the ?face?
or publicself image of the hearer of a message, a speaker canemploy a range of verbal strategies.
Their modelof politeness states that in social situations thereare three factors which are considered in a decisionwhether or when to use communication techniquessuch as formality:1.
The ?social distance?
between the participantsas a symmetric relation2.
The relative ?power?
between the participantsas an asymmetric relation3.
The weight of an imposition such as a requestAbdullah (2006) examines email interactionsfrom the perspective of Brown and Levinson?s po-liteness model in a Malaysian corporation from over180 participants and a corpus of 770 email mes-sages.
This work directly examines the factors men-tioned previously which influence email formality.Unfortunately, the methodology and data were notprovided for this study.The goal of our work is to test whether Brownand Levinson?s model holds in a real setting with amuch larger data set.
In this study, we chose the En-ron Email Corpus as our dataset.
We first built twoclassifiers: one labels an email as formal or informal86and the other determines whether an email containsa request.
Next, we used the classifiers to label ev-ery email in the Enron corpus.
Finally, we testedwhether the three factors in Brown and Levinson?stheory indeed affect formality in email communica-tion.
While we consider the work a case study, webelieve that the methodology proposed in the papercan be applied to other social media domains and beused to test other linguistic or social theories.12 Overview of the Enron email corpusThe Enron email corpus, which consists of hundredsof thousands of emails from over a hundred Enronemployees over a period of 3.5 years (1998 to 2002),was made public during the US government?s legalinvestigation of Enron.
The corpus was first pro-cessed and released by Klimt and Yang (2004) atCarnegie Mellon University (CMU), and this CMUdataset has later been re-processed by several otherresearch groups.
In this section, we briefly introducethe datasets that we used in our experiments.2.1 The ISI datasetThe CMU dataset contains many duplicates.
It waslater processed and cleaned by Shetty and Adibiat ISI and released as a relational database.
TheISI database comprises 252,759 messages from theemail folders of 150 employees (Shetty and Adibi,2004).2 We use the ISI dataset as the starting pointfor all of our experiments except for the one in Sec-tion 5.1.2.2 The Sheffield datasetThe Enron email corpus contains both personal andbusiness emails.
In 2006, Jabbari and his colleaguesat the University of Sheffield manually annotateda subset of the emails in the CMU dataset with?Business?
or ?Personal?
categories (Jabbari et al,2006).
The subset contains 14,818 emails and 3,598of them (24.2%) are labeled as ?personal?.3 We usethis dataset in the personal vs. business experiment1Our data including annotations and results can be found athttp://students.washington.edu/kellypet/enron-formality/2The dataset can be downloaded fromhttp://www.isi.edu/?adibi/Enron/Enron.htm3The dataset is available athttp://staffwww.dcs.shef.ac.uk/people/L.Guthrie/nlp/research.htm.as described in Section 5.1.42.3 The ISI Enron employee position tableIn addition to the ISI database, ISI also provided atable of 161 employees and their positions in thecompany.5 In Section 5.3, we study the effect ofseniority on the formality of a message, and we usethis table to determine the relative seniority betweensenders and recipients of a given email.3 Creating the gold standardIn this study, we build two classifiers: a formalityclassifier that determines whether an email is formal,and a request classifier that determines whether anemail contains a request.
In order to train and eval-uate the classifiers, 400 email messages were ran-domly chosen from the Enron corpus and manuallylabeled for formality and request.3.1 Formality annotationFormality is a concept which is difficult to defineprecisely and human judgment on whether an emailis formal can be subjective.
To determine how muchhuman annotators can agree on the concept, weasked three annotators to label 100 out of the 400emails with four labels: ?very formal?, ?somewhatformal?, ?somewhat informal?, and ?very informal?.Because formality is hard to define, we did notgive annotators a concrete definition.
Instead, weprovided a few guidelines and asked annotators tofollow the guidelines and their intuition.
One ofthese guidelines was that the formality of an emailshould not necessarily be dictated by the relationshipbetween the sender and the recipient if their rela-tionship can be inferred from the message.
Anotherguideline stressed that the nature of an email beingbusiness or personal should not necessarily dictateits formality.
Other than these guidelines, annota-tors were asked to come up with their own criteriafor formality while doing the annotation.Table 1 shows the agreement between each anno-tator pair and the average score of the three pairs.For agreement, we calculated the accuracy, which4The ISI dataset and the Sheffield dataset contain significantoverlap as both were derived from the CMU dataset, but theformer is not necessarily a superset of the latter.5We downloaded the table in January 2011 fromhttp://www.isi.edu/?adibi/Enron/Enron Employee Status.xls87Annotator 2-way 4-waypair Agreement Agreement(Acc/F1) (Acc)A vs. B 87.3/77.8 53.7A vs. C 85.4/77.2 40.6B vs. C 84.5/72.9 36.1Pairwise Ave 85.7/76.0 43.5Table 1: Inter-annotator agreement for formality annota-tionis the percentage of emails that receive the samelabel from the two annotators.
2-way agreementmeans the agreement is calculated after the labelvery formal has been merged with somewhat formal,and very informal with somewhat informal; 4-wayagreement means that the agreement is calculatedwith the four formality labels used by the annota-tors.
With the 2-way distinction (formal vs. infor-mal), we also calculate the f-score for identifyinginformal emails, treating one annotation as the goldstandard and the other as the system output.
Thistable shows that, although the concept of formalityis intuitive, the inter-annotator agreement on formal-ity is pretty low (especially when making the 4-waydistinction).Finally, Annotator A, who had the highest agree-ment with other annotators, annotated the remain-ing 300 emails, and his annotation was treated as thegold standard for our formality classifier.3.2 Request annotationIn order to train and evaluate our request classifier,we asked two annotators to go over the same 400emails and label each message as containing-requestor no-request.
A message is considered to contain arequest if it is clear that the sender of the messageexpects the recipient to take some action to respondto the message.
For instance, if a message includesa question such as what do you think?
or a requestsuch as please call me tomorrow, it should be la-beled as containing-request as the sender expects therecipient to call the sender or answer the question.Our definition is slightly different from the defini-tion of request used in speech acts, and it can beseen as a synonym of require-action.While some emails clearly contain requests andothers clearly do not, there is some gray area in be-tween, which results in the disagreement betweenthe annotators.
Many of the disagreed emails in-clude sentences such as Let me know if you haveany questions.
This very commonly used expressionis itself ambiguous between the meanings ?Let meknow whether you have any questions?
and ?If youhave any questions, please inform me of that fact?.Furthermore, this sentence often appears as a markerof politeness or an offer to clarify further, rather thana request for action.
So the correct label of an emailcontaining this expression depends on the context.For the 400 messages, the two annotators agreedon 361 messages, for an inter-annotator agreementof 90.3% and a F1-score of 87.9% for identifyingemails that contain requests.4 Building classifiersIn this section, we discuss the feature sets used forthe two classifiers and report their performances.4.1 Data pre-processingBefore forming the feature vectors for the classi-fiers, we preprocessed all the emails in the ISI andSheffield dataset in several steps.
First, we removedany replied or forwarded message from the emailbody as we want to use only the text written by thesender.
If the email body becomes empty after thisstep, the email is excluded from the analysis con-ducted in Section 5.
After this step, the size of theISI dataset reduces from 252,759 to 232,815 emails,and the size of the Sheffield dataset changes from14,818 to 13,882 emails.
Second, the email mes-sages were segmented into sentences and tokenizedwith tools in the NLTK package (Bird et al, 2009).4.2 Formality classifierFor the formality classifier, we use two labels: for-mal and informal.4.2.1 Features for formalityDuring formality annotation, after the 100 emailshad been annotated, the three annotators were askedto provide a few paragraphs describing their criteriafor formality.
In these criteria, more cues are indi-cators of informality (e.g., the use of vulgar words)than indicators of formality.
We use the follow-ing features to capture the informal ?style?
of the88emails:6F1: Informal Word Features, which check the oc-currences of informal words (see the next sec-tion for detail)F2: Punctuation Features:?
Exclamation Points (?!?)?
Absence of sentence final punctuation?
Frequency of ellipsis (?.
.
.
?
)F3: Case features:?
All lowercase Subject line?
Frequency of sentences which were en-tirely lower case?
Frequency of sentences whose first wordis lower case4.2.2 Informal wordsWe designed a simple heuristic method to ex-tract a list of informal words from the Enron cor-pus.
First, we collect all the unigrams in the Enroncorpus.
Second, we retrieve the information abouteach unigram from Wordnik,7 a website that pro-vides access to retrieve word definitions from mul-tiple source dictionaries.
Among the several dictio-naries crawled by Wordnik, we find Wiktionary to bethe best source for our task since its labels on worddefinitions such as ?informal?, ?offensive?, ?vulgar?,?colloquial?
and ?misspelling?
were the most con-sistent and relevant to our definition of ?formality?.In addition to these labels, the part of speech cate-gory for ?interjection?
was also used to determine ifa word might be considered informal in email com-munication.
Third, we use the gathered word defini-tions to determine whether a word is informal.One issue with the last step is that words oftenhave multiple meanings and some meanings are in-formal and others are not.
For instance, the wordbloody can be formal or informal depending onwhich meaning is used in an email.
As word sensedisambiguation is out of the scope of this work, weuse some simple heuristics to determine whethera word should be treated as informal or not.
Inessence, the process treats a word as informal if a6We did not use ngram features as they might be too specificto the small training data we have and might not work well whenapplied to other emails in the Enron corpus or emails in otherdomain.7http://www.wordnik.comlarge percentage of definitions for the word have cer-tain labels (e.g., vulgar, offensive, and misspelling)or certain part-of-speech tags (e.g., interjection).84.2.3 Performance of the formality classifierWe trained a Maximum Entropy (MaxEnt) classi-fier in the Mallet package (McCallum, 2002).
Table2 shows classification accuracy and precision, recall,and F1-score for identifying informal emails.
Thebaseline system labels every email as formal because62.7% of the emails in the dataset were annotatedas formal; its F1-score is zero as the recall is zero.The numbers for the inter-annotator agreement roware copied from the pairwise average of the 2-wayagreement in Table 1.
The table shows that, withvery few features, the performance of the formal-ity classifier is much better than the baseline and isclose to inter-annotator agreement.
All three typesof features beat the baselines and combining themprovides additional improvement.Acc Prec Rec F1Baseline 62.7 - - -Inter-annotatoragreement85.7 89.5 66.8 76.0F1: Informal words 69.2 75.0 26.7 39.3F2: Punctuation 74.4 82.5 45.8 58.9F3: Case features 69.7 80.0 26.5 39.8F1+F2 76.4 77.3 51.1 61.5F1+F3 72.8 74.3 39.4 51.5F2+F3 80.3 85.2 59.7 70.2F1+F2+F3 80.6 85.7 62.1 72.0Table 2: Performance of the formality classifier.
We use10-fold cross validation on the 400 emails.
Baseline: la-bel every email as formal.4.3 Request classifierThe request classifier uses two labels: containing-request and no-request.8We manually checked the list of informal words extractedand estimated that the number the false positives is less than 1%.However, the list is definitely not complete as many informalwords in the Enron corpus do not appear in the dictionaries usedby Wordnik.890.600.650.700.750.800.850.900.951.001-gram 2-gram 3-gram 4-gram 5-gramTop 5000Top 1000At least 5At least 10BaselineFigure 1: Accuracy of the request classifier with differentfeature sets4.3.1 Features for requestThere has been considerable research into cate-gorizing email messages by function.
Cohen, Car-valho, and Mitchell (2004) described the classifica-tion of email into ?email speech acts?, building onthe speech act theory of Searle (1975).
Carvalho andCohen (2006) achieved high-precision results cate-gorizing messages into categories such as ?request?and ?proposal?
when preprocessing the text in cer-tain ways and using unigram, bigram, and trigramfeatures only.Unlike formality, which is more about the style ofthe messages (e.g., whether the email is all in lower-case), the content words are more relevant for iden-tifying requests.
Following the work in (Carvalhoand Cohen, 2006), we used word ngrams as features.To prevent the features from being too specific tothe small training data, we experimented with twoways of feature selection: by feature counts and bychi-square scores.
N-grams were extracted from theemail body only.
For pre-processing, in addition tothe pre-processing step mentioned in 4.1, we also re-placed some name entities (e.g., numbers and dates)with special labels and lowercased the text.4.3.2 Performance of the request classifierWe trained a MaxEnt classifier and ran 10-foldcross validation on the 400-email dataset.
Figure1 shows the accuracy of the classifier with differ-ent feature sets.
The bottom dotted line is the base-line result.
In the 400 emails, 244 are labeled as no-request, so a baseline system that labels everythingas no-request has an accuracy of 61%.
The middletwo lines are the accuracy with features that occur nofewer than 5 or 10 times.
For the top two curves, fea-tures are sorted according to the chi-square scores,and the top one thousand or five thousand are kept.X-axis shows the value of n for word ngrams; e.g.,3-gram means features include word unigrams, bi-grams, and trigrams.
Figure 1 shows that chi-squarescores outperform feature counts for feature selec-tion, and varying the value of n does not affect theaccuracy very much.Table 3 shows classification accuracy and preci-sion, recall, and F1-score for identifying request-containing emails when n is set to 3.
The tableshows that our classifier, regardless of methods usedfor feature selection, greatly outperforms the base-line system, and there is a small gap between theperformance of our classifier and the inter-annotatoragreement.
For the rest of our experiment, we willuse 3-gram, Top5000 as the feature set for the re-quest classifier.Acc Prec Rec F1Baseline 61.0 - - -Inter-annotatoragreement90.3 90.4 85.5 87.9Using all features 79.5 76.8 68.0 72.1At least 5 79.0 75.7 68.0 71.6At least 10 79.3 75.9 68.6 72.1Top1000 85.5 88.3 72.4 79.6Top5000 85.5 88.3 72.4 79.6Table 3: Performance of the request classifier with 3-gram features: We use 10-fold cross validation on the 400emails.
Baseline: label every email as no-request.5 Factors influencing formalityAs mentioned in Section 1, Brown and Levinson(1987) proposed three factors that influence commu-nication choices such as formality: social distance,relative power, and the weight of an imposition.
Inthis section, we test whether these factors indeed af-fect formality in emails.We measure social distance in two ways: one isbased on the nature of emails (personal vs. busi-ness), and the other is based on the number of emailssent from the sender to the recipient.
While theseaspects do not directly define the social distance be-tween individuals, they are employed to illustrate90related social properties in absence of data whichoutlines the social distance of all Enron employees.For relative power, we use the rank difference of thepositions that the sender and the recipient held inEnron.
Since relative power is complex to definewithout more data, this definition of rank differenceserves as one dimension in which we can study rel-ative power.
For the weight of imposition, we com-pare emails that contain requests and the ones thatdo not.5.1 Social distance: Personal vs. BusinessIn general, friends, family and other such personalcontacts are presumably closer in social distancethan business colleagues.
Therefore, it is possiblethat email messages of a personal nature will bemore likely to be informal than those of a businessnature.
To test the hypothesis, we compare the de-gree of formality in business vs. personal emails.We use the Sheffield dataset, which contains 13,822non-empty emails that have been manually labeledas ?personal?
or ?business?.
We ran the formalityclassifier on the data, and the results are in Table 4.The first and second columns show the number ofemails that are labeled as formal or informal by ourformality classifier, and the last column shows thepercentage of emails in that row that are labeled in-formal (a.k.a.
the rate of informality).The table demonstrates that the rate of informal-ity in personal emails (56.0%) is indeed much higherthan that of business emails (21.3%).
We have runthe Chi-square test and G test with the counts in thetable, and both tests indicate that formality (formalvs.
informal) is not independent from the businessnature of an email message (business vs. personal) atp=0.0001.
The same is true for formality and othersocial factors that we tested in this section (see Ta-bles 5, 7, 8, and 9).99There are two caveats for using these statistical tests to de-termine whether two random variables (formality and a socialfactor) are independent.
First, the counts in the tables are basedon the output of the two classifiers, which could be differentfrom the real counts.
Second, the data points in some experi-ments were not chosen randomly from the whole email corpus;for instance, the emails in Table 7 were from a small set of peo-ple whose ranks at Enron were known.Formal Informal Inf %Personal 1410 1793 56.0%Business 8409 2270 21.3%Total 9819 4063 29.3%Table 4: Formality in personal vs business emails, p <0.00015.2 Social distance: Amount of contactBesides the difference in personal and business mat-ters, another way to measure social distance is theamount of contact that two individuals have witheach other.
People with more email exchange arelikely to be closer in social distance than those withless email exchange, and are therefore likely to havea higher rate of informality.
To test this hypothesis,we started with the ISI dataset and looked at the sub-set of emails where an email has exactly one recipi-ent, and both the sender and the recipient are in theenron.com domain.
The emails were then groupedinto several buckets based on the number of emailsfrom a sender to a recipient.The results are in Table 5.
The first column isthe range of the numbers of emails from a senderto a recipient, and the last column is the number of(sender, recipient) pairs where the number of emailsthat the sender sent to the recipient is in the rangespecified in the first column.
The second column isthe total number of formal emails from the sendersto the recipients in those pairs.
The third column isdefined similarly, and the 4th column is the rate ofinformality.
Note that the rates of informality in thefirst two rows are about the same; it might be due tothe fact that the Enron corpus contains emails onlyin a 3.5-year period.
The rate of formality does goup in the third and fourth rows.Emails sentfrom A to BFormal Inf Inf % # ofpairs1 to 10 23,423 7,566 24.4% 14,87711 to 50 11,484 3,558 23.7% 73751 to 100 3,236 1,363 29.6% 66101 or more 2,114 1,271 37.5% 21Total 40,257 13,758 25.5% 15,701Table 5: Formality and the number of emails from thesender to the recipient, p < 0.0001915.3 Relative power: Rank differenceAnother factor that could affect the sender?s choiceof formality is the relative difference in power orrank between sender and recipient.
For example, ifa manager sends an email to the CEO of an organi-zation, the email is more likely to be formal than ifthe recipient has a lower rank than the sender.To investigate this, we started with the emails inthe ISI dataset whose senders are employees appear-ing in the ISI Enron employee position table and re-cipients are in the enron.com domain.
We groupedthe emails by the sender?s position and calculatedthe rate of informality in each group.
The results arein Table 6: the first two columns are the title andthe rank of the positions in Enron; the third columnis the number of employees with that position; thefourth column is the total number of emails sent bythese employees; the fifth column is the rate of infor-mality; the last column is the percentage of emailsthat contain requests according to our request classi-fier.
It is interesting to see that the rates of informal-ity and request vary a lot for different positions; forinstance, lawyers are more formal and make morerequests than traders.Position Rank # ofempEmailssentInf%Req%CEO 6 4 836 19.4% 21.7%President 5 4 2,680 34.3% 19.3%VP 4 28 11,425 22.2% 18.1%ManagDir3 6 4,953 14.0% 14.7%Director 2 22 1,879 29.4% 15.2%Manager 1 13 6,563 12.4% 25.3%In-houselawyer0 3 1,548 7.0% 26.9%Trader 0 12 1,743 33.1% 13.4%Employee 0 38 11,770 19.1% 19.1%Total - 130 43,397 22.0% 19.2%Table 6: The set of Enron employees used in the formalityvs.
rank studyTo study the effect of rank difference on formal-ity, we used the first six rows in Table 6 as the rel-ative ranks of the next three rows are not so clearlydefined (Diesner et al, 2005).
In total, there are 77employees with rank 1-6, and we call this set of peo-ple RankSet.
We then extracted from the ISI datasetonly those emails that have exactly one recipient andboth sender and recipient are members of RankSet.We grouped this small set, 3999 emails in total, ac-cording to the rank difference (which is defined tobe the rank of the recipient minus the rank of thesender).
The results are in Table 7: the last column isthe number of (sender, recipient) pairs with that rankdifference.
For instance, the -2 row indicates that,among those messages addressed two ranks lower inthe organizational hierarchy, 24.7% are informal.In general, Table 7 shows a lower rate of informal-ity when an email is addressed to a recipient of su-perior rank.
For example, the informality rate of anemail addressed to someone 4 or more ranks higherthan the sender (15.6%) is less than half that of anemail addressed to someone 4 or more ranks lower(31.6%).
We do not know what causes the increaseof informality from +1 to +2; nevertheless, from+2 to +4 (in emails addressing someone 2-4 rankshigher), there is another decrease in informality rate.Rank diff Formal Inf Inf % # ofpairs-4 or less 39 18 31.6% 16-3 84 32 27.6% 32-2 226 74 24.7% 56-1 499 141 22.0% 820 989 275 21.8% 190+1 784 175 18.2% 95+2 270 121 30.9% 58+3 125 38 23.3% 46+4 or more 92 17 15.6% 29Total 3108 891 22.3% 604Table 7: Formality and rank difference, p < 0.0001.
Rankdiff is equal to recipient rank minus sender rank.5.4 Weight of imposition: RequestsAccording to Brown and Levinson?s model of polite-ness, the greater weight of an imposition, the greaterthe usage of polite speech acts including formality.In this model, a request is one of the most imposingspeech acts.
Therefore, when a request is made, wewould expect a lower rate of informality.To investigate this, we used the ISI dataset andthe results of our request classifier to determine the92rate of informality for request and no-request emails.Table 8 shows that there is indeed a lower rate ofinformality when a request is being made.Formal Informal Inf %Request 42,313 9,928 19.0%No-request 128,958 51,616 28.6%Total 171,271 61,544 26.4%Table 8: Formality and request, p < 0.00015.5 Number of recipientsAnother hypothesis we considered is the assumptionthat a sender is less likely to be informal when thereare more recipients on an email since he does notwant to broadcast a style which is more personaland could be perceived as unprofessional.
To testthis hypothesis, we started with the ISI dataset andlooked at the subset of emails where an email has atleast one recipient.10 The emails were then groupedbased on the number of recipients in the emails.Table 9 shows the rate of informality with differ-ent numbers of recipients.
For the most part in theseresults, a greater number of recipients results in alower rate of informality.
For instance, the rate ofinformality is nearly cut in half when there are 3 to 5recipients as opposed to a single recipient.
However,at the upper end of this scale, the rate of informalityrises again slightly.
One possible explanation is thatwhen an email is addressed to a very large number ofrecipients, the strategies employed (e.g., the modelof saving face) might differ from those employed inan email addressed to a small audience.6 DiscussionIn this study, we explored the relation between for-mality and five factors: (1) personal vs. business,(2) amount of contact, (3) rank difference, (4) re-quest, and (5) number of recipients.
The experi-ments show that the general patterns between therate of informality and the five factors are consistentwith Brown and Levinson?s model and our intuition;10Some emails in the ISI dataset do not contain any recipi-ent information.
We suspect that the recipient information hasbeen somehow removed before the data was released to the pub-lic.
With the at-least-one-recipient requirement, the number ofnon-empty emails in the ISI dataset is reduced from 232,815 to180,757.# of recipients Formal Inf Inf %1 70,361 33,115 32.0%2 5,807 1,914 24.8%3-5 22,139 4,383 16.5%6-10 12,903 2,626 16.9%11 or greater 22,080 5,429 19.7%Total 133,290 47,467 26.3%Table 9: Formality and the number of recipients, p <0.0001for instance, an email tends to be more formal if itis about business matter, it is sent to someone witha higher rank, or it contains a request.
But the ex-periments did produce some unexpected results; forinstance, the rate of informality increased slightlywhen the number of recipients is more than 10.There are several possible reasons for the unex-pected results.
One is due to the limitation of ourdataset.
For instance, the social interaction betweentwo people could easily go beyond the 3.5 years cov-ered by the Enron corpus, and people could chooseother ways of communication besides email.
There-fore, the Enron corpus alone may not be sufficientto capture the social distance between two people inthe corpus.
Another possible reason is that the errorsmade by our classifiers could contribute to some ofthe unexpected results.The third possible reason, the one that is most in-teresting to us, is that there are indeed some inter-esting phenomena which can explain away the un-expectedness of the results.
For instance, an emailsent to a large number of strangers (e.g., an adver-tisement sent to a large mailing list) may choose touse an informal and entertaining style in order tocatch the recipients?
attention.
Therefore, a theorythat intends to account for people?s email behaviormay need to distinguish emails sent to a large num-ber of strangers from those sent to a small group offriends.
The benefit of a study like ours is that itallows researchers to test a linguistic or social the-ory on a large data set in a real setting.
The studycan either provide supporting evidence for a theoryor reveal certain discrepancies between the predic-tion made by the theory and the statistics in the realdata, which could lead to revision or refinement ofthe theory.While this case study has concentrated on email93communication, it would be interesting to study for-mality behavior in other communication media suchas Facebook and Twitter.
By applying our method-ology to other media, it would be possible to deter-mine whether there are other social factors that in-fluence formality on these media.
For example, itwould be useful to determine whether there is a dif-ference in formality with respect to the number of?friends?
or ?followers?
that a person has.
Similarly,it would be interesting to examine correlations onthe basis of whether a Facebook profile is config-ured as ?public?
or ?private?
since the potential view-ing audience would be reduced in the case of ?pri-vate?
profiles.
Since Facebook also contains profileswhich are associated with both individuals and busi-nesses, it would be interesting to compare these aswe did with personal and business emails.
Finally, itremains to be seen whether requests could be exam-ined in these media but other social factors (includ-ing whether posts related to personal matters, socialcauses, or event promotion) could be explored to ex-amine formality behavior.7 Conclusions and Future WorkWe believe that NLP techniques can be used to testlinguistic or social theories.
As a case study, wechoose Brown and Levinson?s model of politeness(1987), which states that three factors are consideredin a decision whether or when to use communicationtechniques such as formality.
We test the theory onthe Enron email corpus, and our experimental resultsare largely consistent with the theory and human in-tuition.For future work, we plan to improve the perfor-mance of our formality and request classifier byadding additional features such as the ones that lookat the layout and zoning of an email (e.g., greetingsand signoffs).
We also plan to apply our methodol-ogy to other genres of data (e.g., blogs, Facebook,Twitter) or to test other theories.Another direction for future work is to explorewhat communication techniques such as formalitycan reveal about the culture of a particular social net-work.
For instance, among all the positions listed inthe ISI Enron employee position table, lawyers havethe lowest rate of informality (7.0%), compared toother positions (e.g., 33.1% for traders).
This im-plies that the workplace behavior of lawyers (at leastwith respect to emails) is very different from thatof traders.
It will be interesting to compare the be-haviors of people from different occupations or fromdifferent social networks.
Furthermore, if we coulddefine the norm of behavior within a social group,we could then identify the outliers who might de-serve special attention for various reasons.Acknowledgment We would like to thank Todd Lin-gren, Chris Rogers and three anonymous reviewersfor helpful comments and Katherine Coleman, Car-men Harris and David Horton for providing emailannotation.
Special thanks are extended to DrewMarre` for his insight in application of this data.ReferencesNor Azni Abdullah.
2006.
Constructing BusinessEmail Messages: A Model of Writer?s Choice.
ESPMalaysia, 12:53?63.Steven Bird, Edward Loper, and Ewan Klein.
2009.
Nat-ural Language Processing with Python.
O?Reilly Me-dia Inc.Penelope Brown and Stephen C. Levinson.
1987.
Po-liteness: Some Universals in Language Usage.
Cam-bridge: Cambridge University Press.Vitor R. Carvalho and William W. Cohen.
2006.
Improv-ing ?email speech acts?
analysis via n-gram selection.In Proceedings of the Analyzing Conversations in Textand Speech (ACTS) Workshop at HLT-NAACL 2006,pages 35?41, New York.Vitor R. Carvalho, Wen Wu, and William W. Cohen.2007.
Discovering leadership roles in email work-groups.
In Proc.
of the 4th Conference on Email andAnti-Spam (CEAS 2007).William W. Cohen, Vitor R. Carvalho, and Tom M.Mitchell.
2004.
Learning to classify email into?speech acts?.
In Proceedings of the EMNLP-2004,Barcelona, Spain.A.
Culotta, R. Bekkerman, and A. McCallum.
2004.
Ex-tracting social networks and contact information fromemail and the web.
In Proc.
of the Conference onEmail and Anti-Spam (CEAS 2004).Richard L. Daft and Robert H. Lengel.
1986.
Organi-sational Information Requirements, Media Richness,and Structural Determinants.
Management Science,32:554?571.Jana Diesner, Terill Frantz, and Kathleen Carley.
2005.Communication networks from the enron email corpus?it?s always about the people.
enron is no different?.94Computational & Mathematical Organization Theory,11(3):201?228.Sanaz Jabbari, Ben Allison, David Guthrie, and LouiseGuthrie.
2006.
Towards the Orwellian nightmare:separation of business and personal emails.
In Pro-ceedings of the COLING/ACL 2006 Main ConferencePoster Sessions, pages 407?411.Parambir S. Keila and David B. Skillcorn.
2005.
Detect-ing unusual and deceptive communication in email.Technical report, Queens University, Ontario, Canada.Brian Klimt and Yiming Yang.
2004.
Enron corpus: Anew data set for email classification research.
Techni-cal report, Carnegie Mellon University.Anton Leuski.
2004.
Email is a stage: Discovering peo-ple roles from email archives.
In Proc.
of the 27thAnnual International ACM SIGIR Conference on Re-search and Development in Information Retrieval (SI-GIR 2004), pages 502?503.Andrew McCallum, Xuerui Wang, and Andres Corrada-Emmanuel.
2007.
Topic and role discovery in so-cial networks with experiments on enron and academicemail.
Journal of Artificial Intelligence Research,30:249?272.Andrew McCallum.
2002.
Mallet: A machine learningfor language toolkit.
http://mallet.cs.umass.edu.Einat Minkov, Richard C. Wang, and WilliamW.
Cohen.2005.
Extracting personal names from email: Apply-ing named entity recognition to informal text.
In Proc.of EMNLP-2005.Wanda Orlikowski and JoAnne Yates.
1994.
Genrerepertoire: The structuring of communicative practicesin organizations.
Administrative Science Quarterly,39(4):541?574.John R. Searle.
1975.
A taxonomy of illocutionary acts.In K. Gunderson, editor, Language, Mind, and Knowl-edge, pages 344?369.
Minneapolis.Jitesh Shetty and Jafar Adibi.
2004.
The enron emaildataset database schema and brief statistical report.Technical report, Information Sciences Institute atUniversity of South California.L.
Zhou, J.K. Burgoon, J.F.
Nunamaker Jr, andD.
Twitchel.
2004.
Automating linguistics-based cuesfor detecting deception in text-based asynchronouscomputer-mediated communication.
Group Decisionand Negotiation, 13:81?106.95
