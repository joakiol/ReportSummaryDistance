Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 809?815,Baltimore, Maryland, USA, June 23-25 2014.c?2014 Association for Computational LinguisticsTailoring Continuous Word Representations for Dependency ParsingMohit Bansal Kevin Gimpel Karen LivescuToyota Technological Institute at Chicago, IL 60637, USA{mbansal,kgimpel,klivescu}@ttic.eduAbstractWord representations have proven usefulfor many NLP tasks, e.g., Brown clustersas features in dependency parsing (Koo etal., 2008).
In this paper, we investigate theuse of continuous word representations asfeatures for dependency parsing.
We com-pare several popular embeddings to Brownclusters, via multiple types of features, inboth news and web domains.
We find thatall embeddings yield significant parsinggains, including some recent ones that canbe trained in a fraction of the time of oth-ers.
Explicitly tailoring the representationsfor the task leads to further improvements.Moreover, an ensemble of all representa-tions achieves the best results, suggestingtheir complementarity.1 IntroductionWord representations derived from unlabeled texthave proven useful for many NLP tasks, e.g., part-of-speech (POS) tagging (Huang et al, 2014),named entity recognition (Miller et al, 2004),chunking (Turian et al, 2010), and syntacticparsing (Koo et al, 2008; Finkel et al, 2008;T?ackstr?om et al, 2012).
Most word representa-tions fall into one of two categories.
Discrete rep-resentations consist of memberships in a (possiblyhierarchical) hard clustering of words, e.g., via k-means or the Brown et al (1992) algorithm.
Con-tinuous representations (or distributed representa-tions or embeddings) consist of low-dimensional,real-valued vectors for each word, typically in-duced via neural language models (Bengio et al,2003; Mnih and Hinton, 2007) or spectral meth-ods (Deerwester et al, 1990; Dhillon et al, 2011).Koo et al (2008) found improvement on in-domain dependency parsing using features basedon discrete Brown clusters.
In this paper, we ex-periment with parsing features derived from con-tinuous representations.
We find that simple at-tempts based on discretization of individual wordvector dimensions do not improve parsing.
Wesee gains only after first performing a hierarchi-cal clustering of the continuous word vectors andthen using features based on the hierarchy.We compare several types of continuous rep-resentations, including those made available byother researchers (Turian et al, 2010; Collobert etal., 2011; Huang et al, 2012), and embeddings wehave trained using the approach of Mikolov et al(2013a), which is orders of magnitude faster thanthe others.
The representations exhibit differentcharacteristics, which we demonstrate using bothintrinsic metrics and extrinsic parsing evaluation.We report significant improvements over our base-line on both the Penn Treebank (PTB; Marcus etal., 1993) and the English Web treebank (Petrovand McDonald, 2012).While all embeddings yield some parsing im-provements, we find larger gains by tailoring themto capture similarity in terms of context withinsyntactic parses.
To this end, we use two sim-ple modifications to the models of Mikolov et al(2013a): a smaller context window, and condition-ing on syntactic context (dependency links and la-bels).
Interestingly, the Brown clusters of Koo etal.
(2008) prove to be difficult to beat, but we findthat our syntactic tailoring can lead to embeddingsthat match the parsing performance of Brown (onall test sets) in a fraction of the training time.
Fi-nally, a simple parser ensemble on all the represen-tations achieves the best results, suggesting theircomplementarity for dependency parsing.2 Continuous Word RepresentationsThere are many ways to train continuous represen-tations; in this paper, we are primarily interestedin neural language models (Bengio et al, 2003),which use neural networks and local context tolearn word vectors.
Several researchers havemade their trained representations publicly avail-809Representation Source Corpus Types, Tokens V D TimeBROWN Koo et al (2008) BLLIP 317K, 43M 316,710 ?
2.5 days?SENNA Collobert et al (2011) Wikipedia 8.3M, 1.8B 130,000 50 2 months?TURIAN Turian et al (2010) RCV1 269K, 37M 268,810 50 few weeks?HUANG Huang et al (2012) Wikipedia 8.3M, 1.8B 100,232 50 ?CBOW, SKIP, SKIPDEPMikolov et al (2013a) BLLIP 317K, 43M 316,697 100 2-4 mins.
?Table 1: Details of word representations used, including datasets, vocabulary size V , and dimensionality D. Continuousrepresentations require an additional 4 hours to run hierarchical clustering to generate features (?3.2).
RCV1 = Reuters Corpus,Volume 1. ?
= time reported by authors.
?
= run by us on a 3.50 GHz desktop, using a single thread.able, which we use directly in our experiments.In particular, we use the SENNA embeddings ofCollobert et al (2011); the scaled TURIAN em-beddings (C&W) of Turian et al (2010); and theHUANG global-context, single-prototype embed-dings of Huang et al (2012).
We also use theBROWN clusters trained by Koo et al (2008).
De-tails are given in Table 1.Below, we describe embeddings that we trainourselves (?2.1), aiming to make them more usefulfor parsing via smaller context windows (?2.1.1)and conditioning on syntactic context (?2.1.2).
Wethen compare the representations using two intrin-sic metrics (?2.2).2.1 Syntactically-tailored RepresentationsWe train word embeddings using the continu-ous bag-of-words (CBOW) and skip-gram (SKIP)models described in Mikolov et al (2013a;2013b) as implemented in the open-source toolkitword2vec.
These models avoid hidden layersin the neural network and hence can be trainedin only minutes, compared to days or even weeksfor the others, as shown in Table 1.1We adaptthese embeddings to be more useful for depen-dency parsing in two ways, described next.2.1.1 Smaller Context WindowsThe CBOW model learns vectors to predict aword given its set of surrounding context wordsin a window of size w. The SKIP model learnsembeddings to predict each individual surround-ing word given one particular word, using an anal-ogous window size w. We find that w affectsthe embeddings substantially: with large w, wordsgroup with others that are topically-related; withsmall w, grouped words tend to share the samePOS tag.
We discuss this further in the intrinsicevaluation presented in ?2.2.1We train both models on BLLIP (LDC2000T43) withPTB removed, the same corpus used by Koo et al (2008) totrain their BROWN clusters.
We created a special vector forunknown words by averaging the vectors for the 50K leastfrequent words; we did not use this vector for the SKIPDEP(?2.1.2) setting because it performs slightly better without it.2.1.2 Syntactic ContextWe expect embeddings to help dependency pars-ing the most when words that have similar parentsand children are close in the embedding space.
Totarget this type of similarity, we train the SKIPmodel on dependency context instead of the linearcontext in raw text.
When ordinarily training SKIPembeddings, words v?are drawn from the neigh-borhood of a target word v, and the sum of log-probabilities of each v?given v is maximized.
Wepropose to instead choose v?from the set contain-ing the grandparent, parent, and children words ofv in an automatic dependency parse.A simple way to implement this idea is to trainthe original SKIP model on a corpus of depen-dency links and labels.
For this, we parse theBLLIP corpus (minus PTB) using our baseline de-pendency parser, then build a corpus in which eachline contains a single child word c, its parent wordp, its grandparent g, and the dependency label ` ofthe ?c, p?
link:?`<L>g<G>p c `<L>?,that is, both the dependency label and grandparentword are subscripted with a special token to avoidcollision with words.2We train the SKIP model onthis corpus of tuples with window size w = 1, de-noting the result SKIPDEP.
Note that this approachneeds a parsed corpus, but there also already ex-ist such resources (Napoles et al, 2012; Goldbergand Orwant, 2013).2.2 Intrinsic Evaluation of RepresentationsShort of running end-to-end parsing experiments,how can we choose which representations to usefor parsing tasks?
Several methods have been pro-posed for intrinsic evaluation of word representa-2We use a subscript on g so that it will be treated dif-ferently from c when considering the context of p. We re-moved all g<G>from the vocabulary after training.
We alsotried adding information about POS tags.
This increases M-1(?2.2), but harms parsing performance, likely because the em-beddings become too tag-like.
Similar ideas have been usedfor clustering (Sagae and Gordon, 2009; Haffari et al, 2011;Grave et al, 2013), semantic space models (Pad?o and Lapata,2007), and topic modeling (Boyd-Graber and Blei, 2008).810Representation SIM M-1BROWN ?
89.3SENNA 49.8 85.2TURIAN 29.5 87.2HUANG 62.6 78.1CBOW, w = 2 34.7 84.8SKIP, w = 1 37.8 86.6SKIP, w = 2 43.1 85.8SKIP, w = 5 44.4 81.1SKIP, w = 10 44.6 71.5SKIPDEP34.6 88.3Table 2: Intrinsic evaluation of representations.
SIM columnhas Spearman?s ??
100 for 353-pair word similarity dataset.M-1 is our unsupervised POS tagging metric.
For BROWN,M-1 is simply many-to-one accuracy of the clusters.
Bestscore in each column is bold.tions; we discuss two here:Word similarity (SIM): One widely-used evalu-ation compares distances in the continuous spaceto human judgments of word similarity using the353-pair dataset of Finkelstein et al (2002).
Wecompute cosine similarity between the two vectorsin each word pair, then order the word pairs bysimilarity and compute Spearman?s rank correla-tion coefficient (?)
with the gold similarities.
Em-beddings with high ?
capture similarity in terms ofparaphrase and topical relationships.Clustering-based tagging accuracy (M-1): In-tuitively, we expect embeddings to help parsingthe most if they can tell us when two words aresimilar syntactically.
To this end, we use a met-ric based on unsupervised evaluation of POS tag-gers.
We perform clustering and map each clusterto one POS tag so as to maximize tagging accu-racy, where multiple clusters can map to the sametag.
We cluster vectors corresponding to the to-kens in PTB WSJ sections 00-21.3Table 2 shows these metrics for representationsused in this paper.
The BROWN clusters havethe highest M-1, indicating high cluster purity interms of POS tags.
The HUANG embeddings havethe highest SIM score but low M-1, presumablybecause they were trained with global context,making them more tuned to capture topical sim-ilarity.
We compare several values for the win-dow size (w) used when training the SKIP embed-dings, finding that smallw leads to higher M-1 andlower SIM.
Table 3 shows examples of clustersobtained by clustering SKIP embeddings of w = 1versus w = 10, and we see that the former cor-respond closely to POS tags, while the latter are3For clustering, we use k-means with k = 1000 and ini-tialize by placing centroids on the 1000 most-frequent words.w Example clusters1 [Mr., Mrs., Ms., Prof., ...], [Jeffrey, Dan, Robert,Peter, ...], [Johnson, Collins, Schmidt, Freedman,...], [Portugal, Iran, Cuba, Ecuador, ...], [CST, 4:30,9-10:30, CDT, ...], [his, your, her, its, ...], [truly,wildly, politically, financially, ...]10 [takeoff, altitude, airport, carry-on, airplane, flown,landings, ...], [health-insurance, clinic, physician,doctor, medical, health-care, ...], [financing, equity,investors, firms, stock, fund, market, ...]Table 3: Example clusters for SKIP embeddings with win-dow size w = 1 (syntactic) and w = 10 (topical).much more topically-coherent and contain mixedPOS tags.4For parsing experiments, we choosew = 2 for CBOW and w = 1 for SKIP.
Finally,our SKIPDEPembeddings, trained with syntacticcontext and w = 1 (?2.1.2), achieve the highestM-1 of all continuous representations.
In ?4, wewill relate these intrinsic metrics to extrinsic pars-ing performance.3 Dependency Parsing FeaturesWe now discuss the features that we add to ourbaseline dependency parser (second-order MST-Parser; McDonald and Pereira, 2006) based ondiscrete and continuous representations.3.1 Brown Cluster FeaturesWe start by replicating the features of Koo et al(2008) using their BROWN clusters; each word isrepresented by a 0-1 bit string indicating the pathfrom the root to the leaf in the binary merge tree.We follow Koo et al in adding cluster versions ofthe first- and second-order features in MSTParser,using bit string prefixes of the head, argument,sibling, intermediate words, etc., to augment orreplace the POS and lexical identity information.We tried various sets of prefix lengths on the devel-opment set and found the best setting to use pre-fixes of length 4, 6, 8, and 12.53.2 Continuous Representation FeaturesWe tried two kinds of indicator features:Bucket features: For both parent and child vec-tors in a potential dependency, we fire one indi-cator feature per dimension of each embedding4A similar effect, when changing distributional contextwindow sizes, was found by Lin and Wu (2009).5See Koo et al (2008) for the exact feature templates.They used the full string in place of the length-12 prefixes,but that setting worked slightly worse for us.
Note that thebaseline parser used by Koo et al (2008) is different from thesecond-order MSTParser that we use here; their parser allowsgrandparent interactions in addition to the sibling interactionsin ours.
We use their clusters, available at http://people.csail.mit.edu/maestro/papers/bllip-clusters.gz.811vector, where the feature consists of the dimen-sion index d and a bucketed version of the embed-ding value in that dimension, i.e., bucketk(Evd)for word index v and dimension d, where E is theV ?D embedding matrix.6We also tried standardconjunction variants of this feature consisting ofthe bucket values of both the head and argumentalong with their POS-tag or word information, andthe attachment distance and direction.7Cluster bit string features: To take into accountall dimensions simultaneously, we perform ag-glomerative hierarchical clustering of the embed-ding vectors.
We use Ward?s minimum variancealgorithm (Ward, 1963) for cluster distance andthe Euclidean metric for vector distance (via MAT-LAB?s linkage function with {method=ward,metric=euclidean}).
Next, we fire features on thehierarchical clustering bit strings using templatesidentical to those for BROWN, except that we uselonger prefixes as our clustering hierarchies tendto be deeper.84 Parsing ExperimentsSetup: We use the publicly-available MST-Parser for all experiments, specifically its second-order projective model.9We remove all fea-tures that occur only once in the training data.For WSJ parsing, we use the standard train(02-21)/dev(22)/test(23) split and apply the NP brack-eting patch by Vadas and Curran (2007).
ForWeb parsing, we still train on WSJ 02-21, buttest on the five Web domains (answers, email,newsgroup, reviews, and weblog) of the ?EnglishWeb Treebank?
(LDC2012T13), splitting each do-main in half (in original order) for the develop-ment and test sets.10For both treebanks, we con-vert from constituent to dependency format us-ing pennconverter (Johansson and Nugues,2007), and generate POS tags using the MXPOSTtagger (Ratnaparkhi, 1996).
To evaluate, we use6Our bucketing function bucketk(x) converts the realvalue x to its closest multiple of k. We choose a k valueof around 1/5th of the embedding?s absolute range.7We initially experimented directly with real-valued fea-tures (instead of bucketed indicator features) and similar con-junction variants, but these did not perform well.8We use prefixes of length 4, 6, 8, 12, 16, 20, and full-length, again tuned on the development set.9We use the recommended MSTParser settings: training-k:5 iters:10 loss-type:nopunc decode-type:proj10Our setup is different from SANCL 2012 (Petrov andMcDonald, 2012) because the exact splits and test data wereonly available to participants.System Dev TestBaseline 92.38 91.95BROWN 93.18 92.69SENNA (Buckets) 92.64 92.04SENNA (Bit strings) 92.88 92.30HUANG (Buckets) 92.44 91.86HUANG (Bit strings) 92.55 92.36CBOW (Buckets) 92.57 91.93CBOW (Bit strings) 93.06 92.53Table 4: Bucket vs. bit string features (UAS on WSJ).System Dev TestBaseline 92.38 91.95BROWN 93.18 92.69SENNA 92.88 92.30TURIAN 92.84 92.26HUANG 92.55 92.36CBOW 93.06 92.53SKIP 92.94 92.29SKIPDEP93.33 92.69Ensemble ResultsALL ?
BROWN 93.46 92.90ALL 93.54 92.98Table 5: Full results with bit string features (UAS on WSJ).unlabeled attachment score (UAS).11We reportstatistical significance (p < 0.01, 100K sam-ples) using the bootstrap test (Efron and Tibshi-rani, 1994).Comparing bucket and bit string features: InTable 4, we find that bucket features based on in-dividual embedding dimensions do not lead to im-provements in test accuracy, while bit string fea-tures generally do.
This is likely because indi-vidual embedding dimensions rarely correspond tointerpretable or useful distinctions among words,whereas the hierarchical bit strings take into ac-count all dimensions of the representations simul-taneously.
Their prefixes also naturally define fea-tures at multiple levels of granularity.WSJ results: Table 5 shows our main WSJresults.
Although BROWN yields one of thehighest individual gains, we also achieve statis-tically significant gains over the baseline fromall embeddings.
The CBOW embeddings per-form as well as BROWN (i.e., no statisticallysignificant difference) but are orders of magni-tude faster to train.
Finally, the syntactically-trained SKIPDEPembeddings are statistically indis-tinguishable from BROWN and CBOW, and sig-nificantly better than all other embeddings.
Thissuggests that targeting the similarity captured bysyntactic context is useful for dependency parsing.11We find similar improvements under labeled attachmentscore (LAS).
We ignore punctuation : , ?
?
.
in our evalua-tion (Yamada and Matsumoto, 2003; McDonald et al, 2005).812System ans eml nwg rev blog AvgBaseline 82.6 81.2 84.3 83.8 85.5 83.5BROWN 83.4 81.7 85.2 84.5 86.1 84.2SENNA 83.7 81.9 85.0 85.0 86.0 84.3TURIAN 83.0 81.5 85.0 84.1 85.7 83.9HUANG 83.1 81.8 85.1 84.7 85.9 84.1CBOW 82.9 81.3 85.2 83.9 85.8 83.8SKIP 83.1 81.1 84.7 84.1 85.4 83.7SKIPDEP83.3 81.5 85.2 84.3 86.0 84.1Ensemble ResultsALL?BR 83.9 82.2 85.9 85.0 86.6 84.7ALL 84.2 82.3 85.9 85.1 86.8 84.9Table 6: Main UAS test results on Web treebanks.
Here,ans=answers, eml=email, nwg=newsgroup, rev=reviews,blog=weblog, BR=BROWN, Avg=Macro-average.Web results: Table 6 shows our main Web re-sults.12Here, we see that the SENNA, BROWN,and SKIPDEPembeddings perform the best on av-erage (and are statistically indistinguishable, ex-cept SENNA vs. SKIPDEPon the reviews domain).They yield statistically significant UAS improve-ments over the baseline across all domains, exceptweblog for SENNA (narrowly misses significance,p=0.014) and email for SKIPDEP.13Ensemble results: When analyzing errors, wesee differences among the representations, e.g.,BROWN does better at attaching proper nouns,prepositions, and conjunctions, while CBOWdoes better on plural common nouns and adverbs.This suggests that the representations might becomplementary and could benefit from combina-tion.
To test this, we use a simple ensemble parserthat chooses the highest voted parent for each ar-gument.14As shown in the last two rows of Ta-bles 5 and 6, this leads to substantial gains.
The?ALL ?
BROWN?
ensemble combines votes fromall non-BROWN continuous representations, andthe ?ALL?
ensemble also includes BROWN.Characteristics of representations: We now re-late the intrinsic metrics from ?2.2 to parsingperformance.
The clearest correlation appearswhen comparing variations of a single model,e.g., for SKIP, the WSJ dev accuracies are 93.33(SKIPDEP), 92.94 (w = 1), 92.86 (w = 5), and92.70 (w = 10), which matches the M-1 score or-der and is the reverse of the SIM score order.12We report individual domain results and macro-averageover domains.
We do not tune any features/parameters onWeb dev sets; we only show the test results for brevity.13Note that SENNA and HUANG are trained on Wikipediawhich may explain why they work better on Web parsing ascompared to WSJ parsing.14This does not guarantee a valid tree.
Combining featuresfrom representations will allow training to weigh them appro-priately and also guarantee a tree.5 Related WorkIn addition to work mentioned above, relevantwork that uses discrete representations exists forPOS tagging (Ritter et al, 2011; Owoputi etal., 2013), named entity recognition (Ratinovand Roth, 2009), supersense tagging (Grave etal., 2013), grammar induction (Spitkovsky et al,2011), constituency parsing (Finkel et al, 2008),and dependency parsing (Tratz and Hovy, 2011).Continuous representations in NLP have beenevaluated for their ability to capture syntactic andsemantic word similarity (Huang et al, 2012;Mikolov et al, 2013a; Mikolov et al, 2013b) andused for tasks like semantic role labeling, part-of-speech tagging, NER, chunking, and sentimentclassification (Turian et al, 2010; Collobert et al,2011; Dhillon et al, 2012; Al-Rfou?
et al, 2013).For dependency parsing, Hisamoto et al (2013)also used embedding features, but there are severaldifferences between their work and ours.
First,they use only one set of pre-trained embeddings(TURIAN) while we compare several and also trainour own, tailored to the task.
Second, their em-bedding features are simpler than ours, only us-ing flat (non-hierarchical) cluster IDs and binarystrings obtained via sign quantization (1[x > 0])of the vectors.
They also compare to a first-orderbaseline and only evaluate on the Web treebanks.Concurrently, Andreas and Klein (2014) inves-tigate the use of embeddings in constituent pars-ing.
There are several differences: we work on de-pendency parsing, use clustering-based features,and tailor our embeddings to dependency-stylesyntax; their work additionally studies vocabularyexpansion and relating in-vocabulary words viaembeddings.6 ConclusionWe showed that parsing features based on hierar-chical bit strings work better than those based ondiscretized individual embedding values.
Whilethe Brown clusters prove to be well-suited to pars-ing, we are able to match their performance withour SKIPDEPembeddings that train much faster.Finally, we found the various representations tobe complementary, enabling a simple ensembleto perform best.
Our SKIPDEPembeddings andbit strings are available at ttic.edu/bansal/data/syntacticEmbeddings.zip.813ReferencesRami Al-Rfou?, Bryan Perozzi, and Steven Skiena.2013.
Polyglot: Distributed word representationsfor multilingual NLP.
In Proceedings of CoNLL.Jacob Andreas and Dan Klein.
2014.
How much doword embeddings encode about syntax?
In Pro-ceedings of ACL.Yoshua Bengio, R?ejean Ducharme, Pascal Vincent, andChristian Janvin.
2003.
A neural probabilistic lan-guage model.
Journal of Machine Learning Re-search, 3:1137?1155, March.Jordan L. Boyd-Graber and David M. Blei.
2008.
Syn-tactic topic models.
In Proceedings of NIPS.Peter F. Brown, Peter V. Desouza, Robert L. Mercer,Vincent J. Della Pietra, and Jenifer C. Lai.
1992.Class-based N-gram models of natural language.Computational Linguistics, 18(4):467?479.Ronan Collobert, Jason Weston, L?eon Bottou, MichaelKarlen, Koray Kavukcuoglu, and Pavel Kuksa.2011.
Natural language processing (almost) fromscratch.
Journal of Machine Learning Research,12:2493?2537.Scott C. Deerwester, Susan T. Dumais, Thomas K. Lan-dauer, George W. Furnas, and Richard A. Harshman.1990.
Indexing by latent semantic analysis.
JASIS,41(6):391?407.Paramveer Dhillon, Dean P. Foster, and Lyle H. Ungar.2011.
Multi-view learning of word embeddings viaCCA.
In Proceedings of NIPS.Paramveer Dhillon, Jordan Rodu, Dean P. Foster, andLyle H. Ungar.
2012.
Two Step CCA: A new spec-tral method for estimating vector models of words.In Proceedings of ICML.Bradley Efron and Robert J. Tibshirani.
1994.
An in-troduction to the bootstrap, volume 57.
CRC press.Jenny Rose Finkel, Alex Kleeman, and Christopher D.Manning.
2008.
Efficient, feature-based, condi-tional random field parsing.
In Proceedings of ACL.Lev Finkelstein, Evgeniy Gabrilovich, Yossi Matias,Ehud Rivlin, Zach Solan, Gadi Wolfman, and Ey-tan Ruppin.
2002.
Placing search in context: Theconcept revisited.
ACM Transactions on Informa-tion Systems, 20(1):116?131.Yoav Goldberg and Jon Orwant.
2013.
A dataset ofsyntactic-ngrams over time from a very large cor-pus of English books.
In Second Joint Conferenceon Lexical and Computational Semantics (* SEM),volume 1, pages 241?247.Edouard Grave, Guillaume Obozinski, and FrancisBach.
2013.
Hidden markov tree models for se-mantic class induction.
In Proceedings of CoNLL.Gholamreza Haffari, Marzieh Razavi, and AnoopSarkar.
2011.
An ensemble model that combinessyntactic and semantic clustering for discriminativedependency parsing.
In Proceedings of ACL.Sorami Hisamoto, Kevin Duh, and Yuji Matsumoto.2013.
An empirical investigation of word repre-sentations for parsing the web.
In Proceedings ofANLP.Eric H. Huang, Richard Socher, Christopher D. Man-ning, and Andrew Y. Ng.
2012.
Improving WordRepresentations via Global Context and MultipleWord Prototypes.
In Proceedings of ACL.Fei Huang, Arun Ahuja, Doug Downey, Yi Yang,Yuhong Guo, and Alexander Yates.
2014.
Learningrepresentations for weakly supervised natural lan-guage processing tasks.
Computational Linguistics,40(1).Richard Johansson and Pierre Nugues.
2007.
Ex-tended constituent-to-dependency conversion forEnglish.
In 16th Nordic Conference of Computa-tional Linguistics.Terry Koo, Xavier Carreras, and Michael Collins.2008.
Simple semi-supervised dependency parsing.In Proceedings of ACL.Dekang Lin and Xiaoyun Wu.
2009.
Phrase clusteringfor discriminative learning.
In Proceedings of ACL-IJCNLP.Mitchell P. Marcus, Mary Ann Marcinkiewicz, andBeatrice Santorini.
1993.
Building a large anno-tated corpus of English: The Penn Treebank.
Com-putational Linguistics, 19:313?330.Ryan T. McDonald and Fernando C. Pereira.
2006.Online learning of approximate dependency parsingalgorithms.
In Proceedings of EACL.Ryan T. McDonald, Koby Crammer, and Fernando C.Pereira.
2005.
Spanning tree methods for discrim-inative training of dependency parsers.
TechnicalReport MS-CIS-05-11, University of Pennsylvania.Tomas Mikolov, Kai Chen, Greg Corrado, and JeffreyDean.
2013a.
Efficient estimation of word represen-tations in vector space.
In Proceedings of ICLR.Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg Cor-rado, and Jeffrey Dean.
2013b.
Distributed repre-sentations of words and phrases and their composi-tionality.
In Proceedings of NIPS.Scott Miller, Jethran Guinness, and Alex Zamanian.2004.
Name tagging with word clusters and discrim-inative training.
In Proceedings of HLT-NAACL.Andriy Mnih and Geoffrey Hinton.
2007.
Three newgraphical models for statistical language modelling.In Proceedings of ICML.814Courtney Napoles, Matthew Gormley, and BenjaminVan Durme.
2012.
Annotated gigaword.
In Pro-ceedings of the Joint Workshop on Automatic Knowl-edge Base Construction and Web-scale KnowledgeExtraction, AKBC-WEKEX ?12, pages 95?100,Stroudsburg, PA, USA.
Association for Computa-tional Linguistics.Olutobi Owoputi, Brendan OConnor, Chris Dyer,Kevin Gimpel, Nathan Schneider, and Noah A.Smith.
2013.
Improved part-of-speech tagging foronline conversational text with word clusters.
InProceedings of NAACL.Sebastian Pad?o and Mirella Lapata.
2007.Dependency-based construction of semantic spacemodels.
Computational Linguistics, 33(2):161?199.Slav Petrov and Ryan McDonald.
2012.
Overview ofthe 2012 shared task on parsing the web.
In Notesof the First Workshop on Syntactic Analysis of Non-Canonical Language (SANCL).Lev Ratinov and Dan Roth.
2009.
Design challengesand misconceptions in named entity recognition.
InProceedings of CoNLL.Adwait Ratnaparkhi.
1996.
A maximum entropymodel for part-of-speech tagging.
In Proceedingsof EMNLP.Alan Ritter, Sam Clark, Mausam, and Oren Etzioni.2011.
Named entity recognition in tweets: An ex-perimental study.
In Proceedings of EMNLP.Kenji Sagae and Andrew S. Gordon.
2009.
Cluster-ing words by syntactic similarity improves depen-dency parsing of predicate-argument structures.
InProceedings of the 11th International Conference onParsing Technologies.Valentin I. Spitkovsky, Hiyan Alshawi, Angel X.Chang, and Daniel Jurafsky.
2011.
Unsuperviseddependency parsing without gold part-of-speechtags.
In Proceedings of EMNLP.Oscar T?ackstr?om, Ryan McDonald, and Jakob Uszko-reit.
2012.
Cross-lingual word clusters for directtransfer of linguistic structure.
In Proceedings ofNAACL.Stephen Tratz and Eduard Hovy.
2011.
A fast, ac-curate, non-projective, semantically-enriched parser.In Proceedings of EMNLP.Joseph Turian, Lev-Arie Ratinov, and Yoshua Bengio.2010.
Word representations: A simple and generalmethod for semi-supervised learning.
In Proceed-ings of ACL.David Vadas and James R. Curran.
2007.
Adding nounphrase structure to the Penn Treebank.
In Proceed-ings of ACL.Joe H. Ward.
1963.
Hierarchical grouping to optimizean objective function.
Journal of the American sta-tistical association, 58(301):236?244.Hiroyasu Yamada and Yuji Matsumoto.
2003.
Statis-tical dependency analysis with support vector ma-chines.
In Proceedings of International Conferenceon Parsing Technologies.815
