Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008), pages 993?1000Manchester, August 2008Domain Adaptation for Statistical Machine Translation with DomainDictionary and Monolingual CorporaHua Wu,  Haifeng WangToshiba (China) R&D CenterBeijing, 100738, Chinawuhua@rdc.toshiba.com.cnwanghaifeng@rdc.toshiba.com.cnChengqing ZongNLPR, Institute of AutomationChinese Academy of SciencesBeijing 100080, Chinacqzong@nlpr.ia.ac.cnAbstract traStatistical machine translation systemsare usually trained on large amounts ofbilingual text and monolingual text.
Inthis paper, we propose a method to per-form domain adaptation for statisticalmachine translation, where in-domain bi-lingual corpora do not exist.
This methodfirst uses out-of-domain corpora to train abaseline system and then uses in-domaintranslation dictionaries and in-domainmonolingual corpora to improve the in-domain performance.
We propose an al-gorithm to combine these different re-sources in a unified framework.
Experi-mental results indicate that our methodachieves absolute improvements of 8.16and 3.36 BLEU scores on Chinese toEnglish translation and English to Frenchtranslation respectively, as comparedwith the baselines using only out-of-domain corpora.1 IntroductionIn statistical machine translation (SMT), thetranslation process is modeled to obtain thetranslation  of the source sentence f  bymaximizing the following posterior probability(Brown et al, 1993).beste)()(maxarg)(maxarg||eefeefeeLMbestppp==(1)State-of-the-art SMT systems are trained onlarge collections of bilingual corpora for the?C 2008.
Licensed under the Creative Commons Attri-bution-Noncommercial-Share Alike 3.0 Unportedlicense (http://creativecommons.org/licenses/by-nc-sa/3.0/).
Some rights reserved.nslation model )( | efp  and monolingual tar-get language corpora for the language model(LM) )(eLMp .
The trained SMT systems aresuitable for translating texts in the same domainas the training corpus.
However, for some spe-cific domains, it is difficult to obtain a bilingualcorpus.
In this case, the performance of SMTsystems will be degraded.Generally, it is easier to obtain in-domainmonolingual corpora in either source or targetlanguage.
Moreover, in some specific domains,although in-domain bilingual corpora do not ex-ist, in-domain translation dictionaries, whichusually contain domain-specific terms and theirtranslations, are available.
And even if such dic-tionaries are not available, it is easier to compileone than to build a bilingual corpus.
Thus, in thispaper, we address the problem of domain-specific SMT, where only domain-specific dic-tionaries and/or monolingual corpora exist.
In aspecific domain, there are two kinds of words:common words, which also frequently occur inout-of-domain corpora, and domain-specificwords, which only occur in the specific domain.Thus, we can combine the out-of-domain bilin-gual corpus, the in-domain translation dictionary,and monolingual corpora for in-domain transla-tion.If an in-domain translation dictionary is avail-able, we combine it with the out-of-domaintranslation model to improve translation quality.If an in-domain target language corpus (TLC) isavailable, we use it to build an in-domain lan-guage model, which can be combined with theout-of-domain language model to further im-prove translation quality.
Moreover, if an in-domain source language corpus (SLC) is avail-able, we automatically translate it and obtain asynthetic in-domain bilingual corpus.
By addingthis synthetic bilingual corpus to the training data,we rebuild the translation model to improve993translation quality.
We can repeatedly translatethe in-domain source language corpus with theimproved model until no more improvement canbe made.
This is similar to transductive learningdescribed in (Ueffing et al, 2007).We perform domain adaptation experimentson two tasks: one is the Chinese to English trans-lation, using the test set released by the Interna-tioinese to English translationanMT sys-temlation model and language model adapta-in domain adaptation fordel adaptation has beenshared task focused on do-main adaptation for machine translation amongEurpora.
Adding the extracted bilin-guthe performance of a SMT systemtraMoses (Koehn et al, 2007).
Inbabilities, reorder-e model probabili-nal Workshop on Spoken Language Transla-tion 2006 (IWSLT 2006), and the other is theEnglish to French translation, using the data re-leased by the Second Workshop on StatisticalMachine Translation (WMT 2007) (Callison-Burch et al, 2007).Experimental results indicate that our methodachieves absolute improvements of 8.16 and 3.36BLEU scores on Chd English to French translation respectively, ascompared with the baselines only using the out-of-domain corpora.
The results on both transla-tion tasks also show that the translation qualityachieved by our methods is comparable to that ofthe method using both in-domain and out-of-domain bilingual corpora.
Moreover, even if in-domain and out-of-domain bilingual corpora areavailable, adding an in-domain dictionary alsohelps to improve the translation quality.The remainder of the paper is organized as fol-lows.
In section 2, we describe the related work.Section 3 briefly introduces the baselineused in our experiments.
Section 4 describesour domain adaptation method of using in-domain dictionary and monolingual corpora.
Andthen we present the experimental results in sec-tions 5.
In the last section, we conclude this pa-per.2 Related WorkTranstion are usually usedSMT.
Language mowidely used in speech recognition (Bacchianiand Roark, 2003).
In recent years, languagemodel adaptation has also been studied for SMT(Bulyko et al, 2007).
They explored discrimina-tive estimation of language model weights bydirectly optimizing machine translation perform-ances such as BLEU score (Papineni et al, 2002).Their experiments indicated about 0.4 BLEUscore improvement.A shared task is organized as part of the Sec-ond Workshop on Statistical Machine Transla-tion.
A part of thisropean languages.
Several studies investigatedmixture model adaptation for both translationmodel and language model in SMT (Civera andJuan, 2007; Foster and Kuhn, 2007).
Koehn andSchroeder (2007) investigated different adapta-tion methods for SMT.
Their experiments indi-cate an absolute improvement of more than 1BLEU score.To enlarge the in-domain bilingual corpus,Munteanu and Marcu (2005) automatically ex-tracted in-domain bilingual sentence pairs fromcomparable coal corpus to the training data improved the per-formance of the MT system.
In addition, Ueffinget al (2007) explored transductive learning forSMT, where source language corpora are used totrain the models.
They repeatedly translatedsource sentences from the development set andtest set.
Then the generated translations are usedto improve the performance of the SMT system.This kind of transductive learning can be seen asa means to adapt the SMT system to a new typeof texts.In this paper, we use an in-domain translationdictionary and/or in-domain monolingual corpora(in both source language and target language) toimproveined on the out-of-domain corpora.
Thus, ourmethod uses these resources, instead of an in-domain bilingual corpus, to adapt a baseline sys-tem trained on the out-of-domain corpora to in-domain texts.3 Baseline MT SystemThe phrase-based SMT system used in our ex-periments isMoses, phrase translation proing probabilities, and languagties are combined in the log-linear model to ob-tain the best translation beste  of the source sen-tence f :?=?=Mp | )(maxarg fee ebest(2)mmmh1,(maxarg f)ee ?The weights are set by a discriminative train-ing method using a held-out data set as describin (Och, 2003).
The models or features which areemployed by the decoder are (a) one or severalphedrases tables, (b) one or more language modelstrained with SRILM toolkit (Stolcke, 2002), (c)distance-based and lexicalized distortion models,(d) word penalty, (e) phrase penalty.994Input  Out-of-domain training data OLary D  In-domain translation diction  IIn-domain target language corpus IT  (optional)In-d, where  represents the general model.efIfeing step: Translate  with  to get a synthetic bilingual corpusUnEEndOutpu l  for in-domain translationomain source language corpus I  (optional) SBegin Assign translation probabilities to IDIf IT  is availableTraining step: (L Estimate= ),, IIO TD?Els?Training step: ),( IO DL Estimate=?End iS  is availableI?
=)0( ?,  0=iR peat1+= iiLabel IS)1( ?i?
ILTraining step: ),, IIOtil no more improvement can be achieved()(i LDL estimate-Re=?)(i?
?=nd ift  Mode  ?Figure 1.The domain adaptation algorithm4 The Frameworkn about the algorithm isour algorithm, a phraseavailable, we train an in-domain LM, which iscoilable, we use the built -linearm).
With theorder toto assign prob-nary.m4.1 The AlgorithmThe detailed informatioshown in Figure 1.
Intable and a language model are first constructedbased on the out-of-domain corpus OL .
Thenprobabilities are automatically assigned to theentries in the in-domain translation dictionaryID , from which another phrase table is con-structed.
At last, the two phrase tables are com-d.
This is the procedure of the training step),( IO DL Estimate=?
.If an in-domain target language corpus isbinembined with the out-of-domain LM.
The builtphrase tables and LMs are integrated in the log-linear model as described in section 3.
This is theprocedure of ),,( IIO TDL Estimate=?
.Moreover, if an in-domain source languagecorpus is ava logodel to translate the in-domain source texts andobtain a synthetic bilingual corpus.
And then weadd the synthetic bilingual corpus into the train-ing data to improve the current log-linear modelimproved model, we repeatedly translate the in-domain source texts until no more improvementon a development set can be achieved.4.2 Dictionary ProbabilitiesIn general, there is no translation probability in amanually-made translation dictionary.
In( estima-Re=?
),,()( IIOi LDL teconstruct a phrase table, we haveabilities to the entries in the dictioUniform Translation Probability: Since wehave no parallel corpus to estimate the translationprobabilities, we simply assign uniform prob-abilities to the entries in the dictionary.
With thisethod, if a source word has n  translations, thenwe assign n1  to each translation of this phrasefor all the four scores of the phrase pair.Constant Translation Probability: For eachentry in the dictionary, we as ign a fixed score.In this case e sum of the translation probabilityis not necessarily equal to 1.s, thanslation probabili-ties for the entries in the dictionary.
And for theCorpus Translation Probability: If an in-domain monolingual source corpus exists, wetranslate it with the method as described in Fig-ure 1, and then estimate the tr995ens.mmonly-usedtries whose translation probabilities are notestimated, we assign average probabilities thatare calculated from the entries that have obtainedprobabilities.4.3 Combining Phrase TablesIn the algorithm, there are two kinds of phrasetables.
We need to combine them to translate thein-domain textMixture Model: The most comethod is linear interpolation.
)()1()()( ||| fefefe oI ppp ??
?+=  (3)Wion probabili-ties.here )( | feIp  and )( | feop  are the in-domain and out-of-domain translat?
is the interpolation weight.Discriminative Model: An alternative is totwo tables in the log-linearanslation oses, foren uses themfohe out-of-domain lan-tigate twoation andtwo different tasks: oneis the Chinese to English translation in IWSLTthe other is the English ton adaptation translation in thered task.airs, with about 3 mil-liocombine the r model.During t with M each phrase inthe sentence, the decoder obtains all of its trans-lations in both phrase tables, and thr translation expansion.4.4 Combining Language ModelsIf an in-domain target language corpus exists, weuse it to construct an in-domain language model,which is combined with tguage model.
In this paper, we invescombination methods: linear interpollog-linear interpolation.5 Experiments5.1 SettingWe ran experiments on2006 evaluation, andFrench domaiWMT 2007 shaFor the Chinese to English translation task, weuse the Chinese-English bilingual corpus pro-vided by the Chinese Linguistic Data Consortium(CLDC)2 as the out-of-domain corpus.
It con-tains 156,840 sentence pn English words and about 5 million Chinesecharacters.
In addition, we use the Basic Travel-ing Expression Corpus (BTEC) released byIWSLT 2006 (Paul, 2006) to construct an in-domain phrase table, as a comparison with thatone constructed with the in-domain dictionary.2  http://www.chineseldc.org/EN/index.htm.
The catalognumber is CLDC-LAC-2003-004.
It is a balanced corpuscontaining sentence pairs in multiple domains.Corpora Sentences OOVCLDC 156,840 89 (6.31%)BTEC 39,953 179 (12.69%)IWSLT06-dev4 489 NAIWSLT06-test 500 NATable 1.
Ch sh coraries Entriesinese-Engli poraDiction OOVLDC 82,090 228 (16.16%)in-domain 32  .57%) ,821 121 (8T scesable 2.
Chinese-English dictionarieCorpora Senten OOVEuroparl 949,410 412 (5.90%)NC 43,060 599 (8.58%)WMT07 dev 1,057 NAWMT07 test 2,007 NATable 3.
E ch corI art and rt a seda in m ual  ex-p  ta rts of  CLDC andBTEC are used for language m  construction(see Tab uation,nglish-Fren porats source p  target pga re separately ura in ours the in-domaeonolin corpoeriments.
Th rget pa  bothodelle 1).
From the IWSLT 2006 evalwe choose the devset4 as our development data.Evaluation was performed on IWSLT 2006 testset.
The references for the test set contain lower-case words and punctuations.
The detailed in-formation is shown in Table 1.We use two kinds of manually-made diction-aries for comparison: one is the LDC Chinese-English Translation Lexicon Version 3.0(LDC2002L27), and the other is the in-domainspoken language dictionary made by ourselves,which contains in-domain Chinese words andtheir English translations.
The dictionary is ma-nually constructed.
Some entries of the diction-ary are collected from phrase books.
Some ofthem are collected from the general-domain dic-tionaries.
And then, the entries are filtered andmodified by a Chinese native speaker specializedin English.
The detailed information is shown inTable 2.
If a source word has two translations, itis counted as two entries.
The OOV rates of thetest set uncovered by the LDC dictionary and thein-domain dictionary are 16.16% and 8.57%,respectively.For the English to French translation task, theout-of-domain corpus is the Europarl corpus dis-tributed for the shared task of WMT 2007 (Calli-son-Burch et al, 2007)3.
We filter the sentencepairs whose lengths are above 40 words.
For the3 http://www.statmt.org/wmt07/shared-task.html996in-domain corpus, we use the News Commentary(NC) corpus distributed in WMT 2007.
We alsouse the same development set and test set in thedomain adaptation shared task (see Table 3).
Wemanually built an in-domain English-French dic-tionary according to the in-domain bilingual cor-pus, which includes 26,821 entries.
It containsin-domain English words and their French trans-lations.
The OOV rate of the test set uncoveredby this dictionary is 22.34%.5.2 Evaluation MeasuresTo perform phrase-based SMT, we use thert training scripts.efault settings andaryrase table.
With the in-domain translation dictionary, we construct in-dolts, log-lineartrationary into the training corpus.
Inthomaincorpus to train a phrase table.
Then we use bothMoses decoder and its suppoWe run the decoder with its dthen use Moses' implementation of minimumerror rate training (Och, 2003) to tune the featureweights on the development set.
Translationquality was evaluated using BLEU score (Pap-ineni et al, 2002).5.3 Results on Chinese-English TranslationTranslation DictionWith the out-of-domain bilingual corpus, wetrain an out-of-domain phmain phrase tables by assigning differenttranslation probabilities with two different meth-ods: uniform and constant.
For the constanttranslation probability, we set the score using thedevelopment set.
In our experiments, we set it to1.
We use the target part of the out-of-domaincorpus to train a language model4.With two phrase tables, we combine them in alinear or log-linear method as described in sec-tion 4.3.
In our experimental resunslation models outperform the linear models(16.38 vs. 15.12), where the entries of the dic-tionary are assigned with the constant translationprobabilities.
Thus, we will use log-linear modelsfor phrase table combination in the followingexperiments.Another method to combine the out-of-domaincorpus and the translation dictionary is to add thein-domain dicis case, only one phrase table is trained.Table 4 describes the results using the out-of-domain corpus and the in-domain dictionary.
Thebaseline method only uses the out-of-d4 We also used LDC English Gigaword to train a large lan-guage model.
However, this language model did not im-prove the translation quality.Methods Resources Used BLEU(%)baseline out-of-domain corpus 13.59+dictionary as corpus 15.52+uniform prob.
16.00+constant prob.
16.38baseline +dictionary+corpus prob.
16.72Table 4.
Translation results of using out-of-d ictionarythe out-of-dom  the in-dom c-5 .
Thealsoimbine it with thelinear inter-polation and log-linear interpolation.
The ex-peovesthifference be-twomain corpus and in-domain dain corpus and ain ditionary.
The results indicate that adding an in-domain dictionary significantly improves thetranslation quality by 2.79 BLEU scoremethods using the dictionary as a phrase tableoutperform the method adding it to the trainingcorpus.
And the method using constant transla-tion probabilities significantly outperforms thatusing the uniform translation probabilities.For comparison, we also assign corpus prob-abilities to the entries in the dictionary by trans-lating the source part of the BTEC corpus withthe method described in Section 4.2.
Thisproves the translation quality.In-Domain Monolingual CorporaWe use the target part of the BTEC corpus totrain an in-domain LM.
We comout-of-domain LM in two methods:rimental results indicate that linear interpola-tion outperforms log-linear interpolation (17.16vs.
16.20).
Thus, we will use linear interpolationfor LMs in all of the following experiments.Table 5 describes the results of using the in-terpolated language model.
As compared withthe results in Table 4, it can be seen that addingthe in-domain language model greatly impre translation quality.
It achieves an absoluteimprovement of 3.57 BLEU score as comparedwith the baseline model.
If the in-domain transla-tion dictionary is used, the translation quality isfurther improved by 4 BLEU score.If the in-domain source language data is avail-able, we translate it and obtain a synthetic bilin-gual corpus.
Then we perform transductive learn-ing as described in Figure 1.
The deen our method and that in (Ueffing et al,2007) is that we translate a larger in-domainsource corpus, and we use 1-best translation5 We use the method described in (Koehn and Monz, 2006)for significance test.
In this paper, significant improvementmeans method A outperforms method B on a significancelevel of no less than 95%.997Methods Models  Resources used BLEU(%)baseline Model 1 out-of-domain corpus 13.59baseline + TLC  Model 2 + in-domain TLC 17.16Model 3 + in-domain TLC + dictionary (uniform prob.)
20.83 baseline + TLCdictionary (constant prob.)
+ dictionary Model 4 + in-domain TLC + 21.16Model 5 + in-domain SLC 15.98Model 6 + in-domain SLC and TLC 18.19transductivelearningnd TLC + dictionary (corpus prob.)
Model 7 + in-domain SLC a 21.75Table 5.Diction  BLEU(%)Translation results of using in-domain resourcesary types Entries OOVgeneral domain LDC 228 (1 %) 82,090 6.16 19.11manual 121 ) 32,821 (8.57% 21.16in-domainextracted 11,765 330 (23.39%) 19.88LD al C + manu 106,572 45 (3.19%) 21.34 combinedLDC + extracted 95,660 202 (14.31%) 20.49Table 6.
Comparison of ictionarresult with full re-training.that transductive learninged, the transla-tiotionarieswith concern to the translation quality.
Besidesld as described in (Wu andWang, 2007).phrase table, extract thetheir translations.uswhen an in-domain bi-md  different iesThe results indicateimproves translation?
From the filteredChinese words andquality in all cases.
For example, Model 5achieves an absolute improvement of 2.39 BLEUscore over Model 1, and Model 6 achieves 1.03BLEU score improvement over Model 2.
Model7 uses the in-domain dictionary with corpustranslation probabilities, which are obtained fromthe phrase table trained with the synthetic bilin-gual corpus.
The results indicate that Model 7outperforms Model 4, with a significant im-provement of 0.59 BLEU score.The results also indicate that when only the in-domain monolingual corpus is usn quality is improved by 4.6 BLEU score(Model 6 vs. Model 1).
By adding the in-domaindictionary, the translation quality is further im-proved, achieving an absolute improvement of8.16 BLEU score (Model 7 vs. Model 1).Comparison of Different DictionariesWe compare the effects of different dicthe manually-made in-domain dictionary, we useother two dictionaries: the LDC dictionary andan automatically built dictionary, which is ex-tracted from the BTEC corpus.
This extracteddictionary only contains Chinese words and theirtranslations.
The extraction method is as follows:?
Build a phrase table with the in-domain bi-lingual corpus.?
Filter those phrase pairs whose values arebelow a thresho?
Assign constant translation probabilities tothe entries of the extracted dictionary.Table 6 shows the translation results.
All ofthe methods use the out-of-domain corpus, thein-domain target language corpus, and the corre-sponding translation dictionaries with constanttranslation probabilities.
The results indicate thating the general-domain dictionary also im-proves translation quality, achieving an im-provement of about 2 BLEU score as comparedwith Model 2 in Table 5.
It can also be seen thatthe in-domain dictionaries significantly outper-form the LDC dictionary although the extracteddictionary has a higher OOV rate than the LDCdictionary.
Further analysis shows that the LDCdictionary does not contain the in-domain trans-lations of some words.
Results also indicate thatcombining the two kinds of dictionaries helps toslightly improve translation quality since theOOV rates are reduced.Comparison with In-domain Bilingual CorpusThe aim of this section is to investigatewhether the in-domain dictionary helps to im-prove translation qualitylingual corpus is available.
And we will alsocompare the translation results with those of theethods only using in-domain dictionaries andmonolingual corpora.To train the in-domain translation model, weuse the BTEC corpus.
The translation results are9981314151617181920212223100k 200k 300k allIn-domain sentence pairsBLEU(%)CLDCBTECCLDC+BTECCLDC+BTEC+DicCLDC+Mono+DicFigure 2.
Comparison of different methods usingdifferent resources.shown in Figure 2.
CLDC and BTEC represents linear interpolation of theInterpolated LM" means thatthe methods that only use the out-of-domain andthe in-domain corpus, respectively.
The method"CLDC+BTEC" usephrase tables and LMs trained with CLDC andBTEC.
"Dic" means using the in-domain dic-tionary, and "Mono" means using in-domainsource and target language corpora.From the results, it can be seen that (a) even ifan in-domain bilingual corpus exists, the in-domain dictionary also helps to improve thetranslation quality, as "CLDC+BTEC+Dic"achieves an improvement of about 1 BLEU scorein comparison with "CLDC+BTEC"; (b) themethod "CLDC+Mono+Dic", which uses boththe in-domain monolingual corpora and the in-domain dictionary, achieves high translationquality.
It achieves slightly higher translationquality than "CLDC+BTEC" that uses the in-domain bilingual corpus (21.75 vs.  21.62)  andachieves slightly lower translation quality than"CLDC+BTEC+Dic" (21.75 vs. 22.05).
But thedifferences are not significant.
This indicates thatour method using an in-domain dictionary andin-domain monolingual corpora is effective fordomain adaptation.5.4 Results on English-French TranslationWe perform the same experiments for English toFrench translation.
Table 7 describes the domainadaptation results.
"we use the target part of the NC corpus to trainan in-domain LM, and then linearly interpolate itwith the out-of-domain LM trained with the Eu-roparl corpus.
The results indicate that using anin-domain target corpus significantly improvesthe translation quality, achieving an improve-ment of 2.19 BLEU score (from 25.44 to 27.63).Methods Out-of-domain LMInterpolatedLMEuroparl 25.44 27.63Europarl+Dic 26.24 28.22transductivelearning - 2  8.80Europarl+NC - 29.19Europarl+NC+Dic - 29.41T ation result f using i ind onolingual corporaUsing the in-domain translation dictionary im-used (from29.19 to 29.41).n Table 7.
The results indicateth28.80).
Although the translation qualityisThisan out-of-domain corpus toystem, and then used an in-ion dictionary, to improve the transla-able 7.
Translictionary and ms o n-domaproves translation quality in all cases, even whenthe in-domain bilingual corpus isWe also perform transductive learning withthe source part of the NC corpus.
The modelused to translate the corpus is that one created by"Europarl+Dic" iat transductive learning significantly improvestranslation quality, achieving an absolute im-provement of 0.58 BLEU score (from 28.22 to28.80).In summary, using an in-domain dictionaryand in-domain monolingual corpora improves thetranslation quality by 3.36 BLEU score (from25.44 toslightly lower than that method of using bothin-domain and out-of-domain bilingual corpora,the difference is not statistically significant.6 ConclusionThis paper proposed a domain adaptation ap-proach for statistical machine translation.approach first usedbuild a baseline sdomain translation dictionary and in-domainmonolingual corpora to adapt it to the in-domaintexts.
The contribution of this paper lies in thefollowing points:?
We proposed a method to integrate a do-main-specific translation dictionary into aphrase-based SMT system for domain adap-tation.?
We investigated the way of using in-domainmonolingual corpora in either source or tar-get language, together with the in-domaintranslattion quality of a baseline system.999We performed experiments on both Chinese toEnglish and English to French translation.
Ex-perimental results on Chinese to English transla-tio-vised Language Model Adaptation.
In Proc.
of theational Conference on Acoustics,Signal Processing (ICASSP-2003),Brn.
Computational Linguistics,Buf the 32nd International Confer-CaStatistical Ma-Ci, pages 177-180.FoKoation of Machine Translation be-Koerico, Nicola Bertoldi,Koistical Ma-MOcranslation.
In Proc.
ofPa2.
BLEU: a Method for Auto-Paation Campaign.
In Proc.
of the InternationalStProc.
of InternationalUerning for StatisticalWics and Phrase-n indicate that all of the in-domain resourcesare useful to improve in-domain translation qual-ity, with an overall improvement of 8.16 BLEUscore as compared with the baseline trained without-of-domain corpora.
Results on English toFrench translation also show that using in-domain translation dictionaries and in-domainmonolingual corpora is effective for domain ad-aptation, achieving an absolute improvement of3.36 BLEU score.
And the results on both trans-lation tasks indicate that the translation qualityachieved by our methods is comparable with thatof the method using both in-domain and out-of-domain bilingual corpora.
Moreover, even if in-domain and out-of-domain bilingual corpora areavailable, adding an in-domain dictionary alsohelps to improve the translation quality.In the future work, we will investigate to as-sign translation probabilities to the dictionariesusing comparable in-domain corpora and exam-ine its effect on the MT performance.
And wewill also examine the effect of an in-domain dic-tionary on transductive learning in more details.ReferencesBacchiani, Michiel and Brian Roark.
2003.
Unsuper28th InternSpeech, andpages 224-227.own, Peter F., Stephen A. Della Pietra, Vincent J.Della Pietra, and Robert L. Mercer.
1993.
TheMathematics of Statistical Machine Translation:Parameter Estimatio19(2): 263-311.lyko, Ivan, Spyros Matsoukas, Richard Schwartz,Long Nguyen, and John Makhoul.
2007.
LanguageModel Adaptation in Machine Translation fromSpeech.
In Proc.
oence on Acoustics, Speech, and Signal Processing(ICASSP-2007), pages 117-120.llison-Burch, Chris, Cameron Fordyce, PhilippKoehn, Christof Monz, and Josh Schroeder.
2007.
(Meta-) Evaluation of Machine Translation.
InProc.
of the Second Workshop onchine Translation, pages 136-158.vera, Jorge and Alfons Juan.
2007.
Domain Adapta-tion in Statistical Machine Translation with Mix-ture Modelling.
In Proc.
of the Second Workshopon Statistical Machine Translationster, George and Roland Kuhn.
2007.
Mixture-Model Adaptation for SMT.
In Proc.
of the SecondWorkshop on Statistical Machine Translation,pages 128-135.ehn, Philipp and Christof Monz.
2006.
Manual andAutomatic Evalutween European Languages.
In Proc.
of the HLT-NAACL 2006 Workshop on Statistical MachineTranslation, pages 102-121.ehn, Philipp, Hieu Hoang, Alexanda Birch, ChrisCallison-Burch, Marcello FedBrooke Cowan, Wade Shen, Christine Moran,Richard Zens, Chris Dyer, Ondrej Bojar, AlexandraConstantin, and Evan Herbst.
2007.
Moses: OpenSource Toolkit for Statistical Machine Translation.In Proc.
of the 45th Annual Meeting of the Associa-tion for Computational Linguistics (ACL-2007),demonstration session, pages 177-180.ehn, Philipp and Josh Schroeder.
2007.
Experi-ments in Domain Adaptation for Statchine Translation.
In Proc.
of the Second Workshopon Statistical Machine Translation, pages 224-227.unteanu, Dragos Stefan, and Daniel Marcu.
2005.Improving Machine Translation Performance byExploiting Non-Parallel Corpora.
ComputationalLinguistics, 31(4): 477-504.h, Franz Josef.
2003.
Minimum Error Rate Train-ing in Statistical Machine Tthe 41st Annual Meeting of the Association forComputational Linguistics (ACL-2003), pages 160-167.pineni, Kishore, Salim Roukos, Todd Ward, andWei-Jing Zhu.
200matic Evaluation of Machine Translation.
In Proc.of the 40th Annual Meeting of the Association ofComputational Linguistics (ACL-2002), pages 311-318.ul, Michael.
2006.
Overview of the IWSLT 2006EvaluWorkshop on Spoken Language Translation(IWSLT-2006), pages 1-15.olcke, Andrea.
2002.
SRILM -- an Extensible Lan-guage Modeling Toolkit.
InConference on Spoken Language Processing(ICSLP-2002), pages 901-904.ffing, Nicola, Gholamreza Haffari, and AnoopSarkar.
2007.
Transductive LeaMachine Translation.
In Proc.
of 45th AnnualMeeting of the Association of Computational Lin-guistics (ACL-2007), pages 25-32.u, Hua and Haifeng Wang.
2007.
ComparativeStudy of Word Alignment HeuristBased SMT.
In Proc.
of Machine TranslationSummit XI, pages 507-514.1000
