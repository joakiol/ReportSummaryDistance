Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, pages 467?473,Berlin, Germany, August 7-12, 2016.c?2016 Association for Computational LinguisticsScience Question Answering using Instructional MaterialsMrinmaya Sachan Avinava Dubey Eric P. XingSchool of Computer ScienceCarnegie Mellon University{mrinmays, akdubey, epxing}@cs.cmu.eduAbstractWe provide a solution for elementary sci-ence tests using instructional materials.We posit that there is a hidden structurethat explains the correctness of an answergiven the question and instructional ma-terials and present a unified max-marginframework that learns to find these hid-den structures (given a corpus of question-answer pairs and instructional materials),and uses what it learns to answer novelelementary science questions.
Our eval-uation shows that our framework outper-forms several strong baselines.1 IntroductionWe propose an approach for answering multiple-choice elementary science tests (Clark, 2015) us-ing the science curriculum of the student and otherdomain specific knowledge resources.
Our ap-proach learns latent answer-entailing structuresthat align question-answers with appropriate snip-pets in the curriculum.
The student curriculumusually comprises of a set of textbooks.
Each text-book, in-turn comprises of a set of chapters, eachchapter is further divided into sections ?
each dis-cussing a particular science concept.
Hence, theanswer-entailing structure consists of selecting aparticular textbook from the curriculum, pickinga chapter in the textbook, picking a section inthe chapter, picking a few sentences in the sec-tion and then aligning words/multi-word expres-sions (mwe?s) in the hypothesis (formed by com-bining the question and an answer candidate) towords/mwe?s in the picked sentences.
The answer-entailing structures are further refined using ex-ternal domain-specific knowledge resources suchas science dictionaries, study guides and semi-structured tables (see Figure 1).
These domain-specific knowledge resources can be very usefulforms of knowledge representation as shown inprevious works (Clark et al, 2016).Alignment is a common technique in many NLPapplications such as MT (Blunsom and Cohn,2006), RTE (Sammons et al, 2009; MacCartneyet al, 2008; Yao et al, 2013; Sultan et al, 2014),QA (Berant et al, 2013; Yih et al, 2013; Yaoand Van Durme, 2014; Sachan et al, 2015), etc.Yet, there are three key differences between ourapproach and alignment based approaches for QAin the literature: (i) We incorporate the curriculumhierarchy (i.e.
the book, chapter, section bifurca-tion) into the latent structure.
This helps us jointlylearn the retrieval and answer selection modules ofa QA system.
Retrieval and answer selection areusually designed as isolated or loosely connectedcomponents in QA systems (Ferrucci, 2012) lead-ing to loss in performance ?
our approach mit-igates this shortcoming.
(ii) Modern textbookstypically provide a set of review questions aftereach section to help students understand the ma-terial better.
We make use of these review prob-lems to further improve our model.
These re-view problems have additional value as part ofthe latent structure is known for these questions.
(ii) We utilize domain-specific knowledge sourcessuch as study guides, science dictionaries or semi-structured knowledge tables within our model.The joint model is trained in max-margin fash-ion using a latent structural SVM (LSSVM) wherethe answer-entailing structures are latent.
We trainand evaluate our models on a set of 8thgradescience problems, science textbooks and multipledomain-specific knowledge resources.
We achievesuperior performance vs. a number of baselines.2 MethodScience QA as Textual Entailment: First, we467!!!
!!!!!!!!!!!!!!!!!
greenhouse effect             CO2    CH4                     O3Text: ?
Natural greenhouse gases include carbon dioxide, methane, water vapor, and ozone ... CFCs and some other man-made compounds are also greenhouse gases ?Hypothesis: CO2, CH4, O3 and CFC gases cause the greenhouse effect Q: Which of the following gases cause the greenhouse effect?
!
A: CO2, CH4, O3 and CFCGrade&8&Science&Curriculum&Textbook:&Life&Science&Chapter:&Animals&Section:&Aligators&and&Crocodile& ...&...&Textbook:&Earth&Science&Chapter:&Atmosphere,&Weather,&and&Climate&Composition&of&the&atmosphere& ...&...&...&...&coreference& elaboration!!!!!!!!!!!!!!
!CurriculumHierarchyAlignmentcause!Figure 1: An example answer-entailing struc-ture.
The answer-entailing structure consists of se-lecting a particular textbook from the curriculum,picking a chapter in the textbook, picking a sec-tion in the chapter, picking sentences in the sectionand then aligning words/mwe?s in the hypothesis(formed by combining the question and an answercandidate) to words/mwe?s in the picked sentencesor some related ?knowledge?
appropriately cho-sen from additional knowledge stores.
In this case,the relation (greenhouse gases, cause, greenhouseeffect) and the equivalences (e.g.
carbon diox-ide = CO2) ?
shown in violet ?
are hypothesizedusing external knowledge resources.
The dashedred lines show the word/mwe alignments from thehypothesis to the sentences (some word/mwe arenot aligned, in which case the alignments are notshown), the solid black lines show coreferencelinks in the text and the RST relation (elaboration)between the two sentences.
The picked sentencesdo not have to be contiguous sentences in the text.All mwe?s are shown in green.consider the case when review questions are notused.
For each question qi?
Q, let Ai={ai1, .
.
.
, aim} be the set of candidate answers tothe question1.
We cast the science QA problem asa textual entailment problem by converting eachquestion-answer candidate pair (qi, ai,j) into a hy-pothesis statement hij(see Figure 1)2.
For eachquestion qi, the science QA task thereby reduces topicking the hypothesis?hithat has the highest like-lihood of being entailed by the curriculum amongthe set of hypotheses hi= {hi1, .
.
.
, him} gener-ated for that question.
Let h?i?
hibe the correcthypothesis corresponding to the correct answer.Latent Answer-Entailing Structures help themodel in providing evidence for the correct hy-pothesis.
As described before, the structure de-pends on: (a) snippet from the curriculum hierar-chy chosen to be aligned to the hypothesis, (b) ex-ternal knowledge relevant for this entailment, and(c) the word/mwe alignment.
The snippet fromthe curriculum to be aligned to the hypothesis isdetermined by walking down the curriculum hier-archy and then picking a set of sentences from thesection chosen.
Then, a subset of relevant exter-nal knowledge in the form of triples and equiva-lences (called knowledge bits) is selected from our1Candidate answers may be pre-defined, as in multiple-choice QA, or may be undefined but easy to extract with adegree of confidence (e.g., by using a pre-existing system)2We use a set of question matching/rewriting rules toachieve this transformation.
The rules match each questioninto one of a large set of pre-defined templates and applies aunique transformation to the question & answer candidate toachieve the hypothesis.
Code provided in the supplementary.reservoir of external knowledge (science dictio-naries, cheat sheets, semi-structured tables, etc).Finally, words/mwe?s in the hypothesis are alignedto words/mwe?s in the snippet or knowledge bits.Learning these alignment edges helps the modeldetermine which semantic constituents should becompared to each other.
These alignments arealso used to generate more effective features.
Thechoice of snippets, choice of the relevant externalknowledge and the alignments in conjunction formthe latent answer-entailing structure.
Let zijrep-resent the latent structure for the question-answercandidate pair (qi, ai,j).Max-Margin Approach: We treat science QA asa structured prediction problem of ranking the hy-pothesis set hisuch that the correct hypothesis isat the top of this ranking.
We learn a scoring func-tion Sw(h, z) with parameter w such that the scoreof the correct hypothesis h?iand the correspondingbest latent structure z?iis higher than the score ofthe other hypotheses and their corresponding bestlatent structures.
In fact, in a max-margin fashion,we want that Sw(h?i, z?i) > S(hij, zij) + 1 ?
?ifor all hj?
h \ h?for some slack ?i.
Writing therelaxed max margin formulation:min||w||12||w||22+ C?imaxzij,hij?hi\h?iSw(hij, zij) + ?
(h?i, hij)?C?iSw(h?i, z?i) (1)We use 0-1 cost, i.e.
?
(h?i, hij) = 1(h?i6= hij) Ifthe scoring function is convex then this objectiveis in concave-convex form and hence can be468solved by the concave-convex programmingprocedure (CCCP) (Yuille and Rangarajan,2003).
We assume the scoring function to belinear:Sw(h, z) = wT?
(h, z).
Here, ?
(h, z) is afeature map discussed later.
The CCCP algorithmessentially alternates between solving for z?i,zij?j s.t.
hij?
hi\ h?iand w to achieve a localminima.
In the absence of information regardingthe latent structure z we pick the structure thatgives the best score for a given hypothesis i.e.arg maxzSw(h, z).
The complete procedure isgiven in the supplementary.Inference and knowledge selection: We usebeam search with a fixed beam size (5) forinference.
We infer the textbook, chapter, section,snippet and alignments one by one in this order.
Ineach step, we only expand the five most promising(given by the current score) substructure candi-dates so far.
During inference, we select top 5knowledge bits (triples, equivalences, etc.)
fromthe knowledge resources that could be relevant forthis question-answer.
This is done heuristically bypicking knowledge bits that explain parts of thehypothesis not explained by the chosen snippets.Incorporating partially known structures:Now, we describe how review questions canbe incorporated.
As described earlier, moderntextbooks often provide review problems at theend of each section.
These review problems havevalue as part of the answer-entailing structure(textbook, chapter and section) is known for theseproblems.
In this case, we use the formulation(equation 1) except that the max over z for thereview questions is only taken over the unknownpart of the latent structure.Multi-task Learning: Question analysis is a keycomponent of QA systems.
Incoming questionsare often of different types (counting, negation,entity queries, descriptive questions, etc.).
Dif-ferent types of questions usually require differentprocessing strategies.
Hence, we also extend ofour LSSVM model to a multi-task setting whereeach question qinow also has a pre-defined as-sociated type tiand each question-type is treatedas a separate task.
Yet, parameters are sharedacross tasks,which allows the model to exploit thecommonality among tasks when required.
We usethe MTLSSVM formulation from Evgeniou andPontil (2004) which was also used in a readingcomprehension setting by Sachan et al (2015).In a nutshell, the approach redefines the LSSVMfeature map and shows that the MTLSSVMobjective takes the same form as equation 1 witha kernel corresponding to the feature map.
Hence,one can simply redefine the feature map and reuseLSSVM algorithm to solve the MTLSSVM.Features: Our feature vector ?
(h, z) decomposesinto five parts, where each part corresponds toa part of the answer-entailing structure.
For thefirst part, we index all the textbooks and score thetop retrieved textbook by querying the hypothesisstatement.
We use tf-idf and BM25 scorers re-sulting in two features.
Then, we find the jaccardsimilarity of bigrams and trigrams in the hypothe-sis and the textbook to get two more features forthe first part.
Similarly, for the second part weindex all the textbook chapters and compute thetf-idf, BM25 and bigram, trigram features.
For thethird part we index all the sections instead.
Thefourth part has features based on the text snippetpart of the answer-entailing structure.
Herewe do a deeper linguistic analysis and includefeatures for matching local neighborhoods in thesnippet and the hypothesis: features for matchingbigrams, trigrams, dependencies, semantic roles,predicate-argument structure as well as the globalsyntactic structure: a tree kernel for matchingdependency parse trees of entire sentences (Sri-vastava and Hovy, 2013).
If a text snippet containsthe answer to the question, it should intuitively besimilar to the question as well as to the answer.Hence, we add features that are the element-wiseproduct of features for the text-question matchand text-answer match.
Finally, we also havefeatures corresponding to the RST (Mann andThompson, 1988) and coreference links to enableinference across sentences.
RST tells us thatsentences with discourse relations are related toeach other and can help us answer certain kindsof questions (Jansen et al, 2014).
For example,the ?cause?
relation between sentences in the textcan often give cues that can help us answer ?why?or ?how?
questions.
Hence, we add additionalfeatures - conjunction of the rhetorical structurelabel from a RST parser and the question word- to our feature vector.
Similarly, the entity andevent co-reference relations allow us to reasonabout repeating entities or events.
Hence, wereplace an entity/event mention with their firstmentions if that results into a greater score.
Forthe alignment part, we induce features basedon word/mwe level similarity of aligned words:469(a) Surface-form match (Edit-distance), and (b)Semantic word match (cosine similarity usingSENNA word vectors (Collobert et al, 2011) and?Antonymy?
?Class-Inclusion?
or ?Is-A?
relationsusing Wordnet).
Distributional vectors for mwe?sare obtained by adding the vector representationsof comprising words (Mitchell and Lapata, 2008).To account for the hypothesized knowledge bits,whenever we have the case that a word/mwe inthe hypothesis can be aligned to a word/mwe in ahypothesized knowledge bit to produce a greaterscore, then we keep the features for the alignmentwith the knowledge bit instead.Negation Negation is a concern for our approachas facts usually align well with their negatedversions.
To overcome this, we use a simpleheuristic.
During training, if we detect negationusing a set of simple rules that test for the presenceof negation words (?not?, ?n?t?, etc.
), we flip thepartial order adding constraints that require thatthe correct hypothesis to be ranked below all theincorrect ones.
During test phase if we detectnegation, we predict the answer corresponding tothe hypothesis with the lowest score.3 ExperimentsDataset: We used a set of 8thgrade science ques-tions released as the training set in the Allen AIScience Challenge3for training and evaluatingour model.
The dataset comprises of 2500 ques-tions.
Each question has 4 answer candidates, ofwhich exactly one is correct.
We used questions 1-1500 for training, questions 1500-2000 for devel-opment and questions 2000-2500 for testing.
Wealso used publicly available 8thgrade science text-books available through ck12.org.
The sciencecurriculum consists of seven textbooks on Physics,Chemistry, Biology, Earth Science and Life Sci-ence.
Each textbook on an average has 18 chap-ters, and each chapter in turn is divided into 12sections on an average.
Also, as described be-fore, each section, on an average, is followed by3-4 multiple choice review questions (total 1369review questions).
We collected a number of do-main specific science dictionaries, study guides,flash cards and semi-structured tables (Simple En-glish Wiktionary and Aristo Tablestore) availableonline and create triples and equivalences used asexternal knowledge.3https://www.kaggle.com/c/the-allen-ai-science-challenge/Question Category ExampleQuestions withoutcontext:Which example describes a learned behavior in a dog?Questions withcontext:When athletes begin to exercise, their heart rates and res-piration rates increase.
At what level of organization doesthe human body coordinate these functions?Negation Ques-tions:A teacher builds a model of a hydrogen atom.
A red golfball is used for a proton, and a green golf ball is used foran electron.
Which is not accurate concerning the model?Table 1: Example questions for Qtype classificationBaselines: We compare our framework with tenbaselines.
The first two baselines (Lucene andPMI) are taken from Clark et al (2016).
TheLucene baseline scores each answer candidate aiby searching for the combination of the ques-tion q and answer candidate aiin a lucene-basedsearch engine and returns the highest scoring an-swer candidate.
The PMI baseline similarly scoreseach answer candidate aiby computing the point-wise mutual information to measure the strengthof the association between parts of the question-answer candidate combine and parts of the CK12curriculum.
The next three baselines, inspiredfrom Richardson et al (2013), retrieve the top twoCK12 sections querying q+aiin Lucene and scorethe answer candidates using these documents.
TheSW and SW+D baselines match bag of words con-structed from the question and the answer answercandidate to the retrieved document.
The RTEbaseline uses textual entailment (Stern and Dagan,2012) to score answer candidates as the likelihoodof being entailed by the retrieved document.
Thenwe also tried other approaches such as the RNNapproach described in Clark et al (2016), Jacanaaligner (Yao et al, 2013) and two neural networkapproaches, LSTM (Hochreiter and Schmidhuber,1997) and QANTA (Iyyer et al, 2014) They formour next four baselines.
To test if our approachindeed benefits from jointly learning the retrievaland the answer selection modules, our final base-line Lucene+LSSVM Alignment retrieves the topsection by querying q + aiin Lucene and thenlearns the remaining answer-entailment structure(alignment part of the answer-entailing structurein Figure 1) using a LSSVM.Task Classification for Multitask Learning:We explore two simple question classificationschemes.
The first classification scheme classi-fies questions based on the question word (what,why, etc.).
We call this Qword classification.The second scheme is based on the type of thequestion asked and classifies questions into threecoarser categories: (a) questions without context,470(b) questions with context and (c) negation ques-tions.
This classification is based on the observa-tion that many questions lay down some contextand then ask a science concept based on this con-text.
However, other questions are framed withoutany context and directly ask for the science con-cept itself.
Then there is a smaller, yet, importantsubset of questions that involve negation that alsoneeds to be handled separately.
Table 1 gives ex-amples of this classification.
We call this classifi-cation Qtype classification4.Results: We compare variants of our method5where we consider our modification for negationor not and multi-task LSSVMs.
We consider bothkinds of task classification strategies and jointtraining (JT).
Finally, we compare our methodsagainst the baselines described above.
We reportaccuracy (proportion of questions correctly an-swered) in our results.
Figure 2 shows the results.First, we can immediately observe that all theLSSVM models have a better performance thanall the baselines.
We also found an improvementwhen we handle negation using the heuristic de-scribed above6.
MTLSSVMs showed a boost oversingle task LSSVM.
Qtype classification schemewas found to work better than Qword classifica-tion which simply classifies questions based on thequestion word.
The multi-task learner could bene-fit even more if we can learn a better separation be-tween the various strategies needed to answer sci-ence questions.
We found that joint training withreview questions helped improve accuracy as well.Feature Ablation: As described before, our fea-ture set comprises of five parts, where each partcorresponds to a part of the answer-entailing struc-ture ?
textbook (z1), chapter (z2), section (z3),snippets (z4), and alignment (z5).
It is interestingto know the relative importance of these parts inour model.
Hence, we perform feature ablation onour best performing model - MTLSSVM(QWord,JT) where we remove the five feature parts oneby one and measure the loss in accuracy.
Figure4We wrote a set of question matching rules (similar to therules used to convert question answer pairs to hypotheses) toachieve this classification5We tune the SVM regularization parameter C on the de-velopment set.
We use Stanford CoreNLP, the HILDA parser(Feng and Hirst, 2014), and jMWE (Kulkarni and Finlayson,2011) for linguistic preprocessing6We found that the accuracy over test questions taggedby our heuristic as negation questions went up from 33.64percent to 42.52 percent and the accuracy over test questionsnot tagged as negation did not decrease significantly40.04?31.6?
32.56?
37.84?32.42?31.86?27.74?
29.32?
31.26?41.38?
43.18?
44.82?45.44?46.09?46.66?46.86?
47.68?47.84?25?30?35?40?45?50?Accuracy?Lucene?
PMI?
SW?SW+D?
RTE?
Jacana?RNNLM?
LSTM?
QANTA?Lucene+LSSVM?Alignment?
LSSVM?
LSSVM(Nega?on)?LSSVM(JT)?
LSSVM(JT,?Nega?on)?
MTLSSVM(Qword)?MTLSSVM(Qtype)?
MTLSSVM(Qword,?JT)?
MTLSSVM(Qtype,?JT)?Figure 2: Variations of our method vs several baselines onthe Science QA dataset.
Differences between the baselinesand LSSVMs, the improvement due to negation, the im-provements due to multi-task learning and joint-learning aresignificant (p < 0.05) using the two-tailed paired T-test.47.84?47.08?46.56?42.72?45.62?39.7?45.28?35?
40?
45?Unablated?Remove?z1?Remove?z2?Remove?z3?Remove?z4?Remove?z5?Remove?K?Figure 3: Ablation on MTLSSVM(Qword, JT) model3 shows that the choice of section and alignmentare important components of our model.
Yet, allcomponents are important and removing any ofthem will result in a loss of accuracy.
Finally, inorder to understand the value of external knowl-edge resources (K), we removed the componentthat induces and aligns the hypothesis with knowl-edge bits.
This results in significant loss in perfor-mance, estabishing the efficacy of adding in exter-nal knowledge via our approach.4 ConclusionWe addressed the problem of answering 8thgradescience questions using textbooks, domain spe-cific dictionaries and semi-structured tables.
Weposed the task as an extension to textual entail-ment and proposed a solution that learns latentstructures that align question answer pairs withappropriate snippets in the textbooks.
Using do-main specific dictionaries and semi-structured ta-bles, we further refined the structures.
The task re-quired handling a variety of question types so weextended our technique to multi-task setting.
Ourtechnique showed improvements over a number ofbaselines.
Finally, we also used a set of associatedreview questions, which were used to gain furtherimprovements.471References[Berant et al2013] Jonathan Berant, Andrew Chou,Roy Frostig, and Percy Liang.
2013.
Semantic pars-ing on freebase from question-answer pairs.
In Pro-ceedings of Empirical Methods in Natural LanguageProcessing, pages 1533?1544.
[Blunsom and Cohn2006] Phil Blunsom and TrevorCohn.
2006.
Discriminative word alignment withconditional random fields.
In Proceedings of the21st International Conference on ComputationalLinguistics and the 44th annual meeting of the Asso-ciation for Computational Linguistics, pages 65?72.Association for Computational Linguistics.
[Clark et al2016] Peter Clark, Oren Etzioni, DanielKhashabi, Tushar Khot, Ashish Sabharwal, OyvindTafjord, and Peter Turney.
2016.
Combining re-trieval, statistics, and inference to answer elemen-tary science questions.
In Proceedings of AAAI.
[Clark2015] Peter Clark.
2015.
Elementary SchoolScience and Math Tests as a Driver for AI:Take theAristo Challenge!
In Proceedings of IAAI.
[Collobert et al2011] Ronan Collobert, Jason Weston,L?eon Bottou, Michael Karlen, Koray Kavukcuoglu,and Pavel Kuksa.
2011.
Natural language process-ing (almost) from scratch.
The Journal of MachineLearning Research, 12:2493?2537.
[Evgeniou and Pontil2004] Theodoros Evgeniou andMassimiliano Pontil.
2004.
Regularized multi?tasklearning.
In Proceedings of the Tenth ACM SIGKDDInternational Conference on Knowledge Discoveryand Data Mining, pages 109?117.
[Feng and Hirst2014] Vanessa Wei Feng and GraemeHirst.
2014.
A linear-time bottom-up discourseparser with constraints and post-editing.
In Proceed-ings of the 52nd Annual Meeting of the Associationfor Computational Linguistics (Volume 1: Long Pa-pers), pages 511?521.
[Ferrucci2012] David A Ferrucci.
2012.
Introductionto ?this is watson?.
IBM Journal of Research andDevelopment, 56(3.4):1?1.
[Hochreiter and Schmidhuber1997] Sepp Hochreiterand J?urgen Schmidhuber.
1997.
Long short-termmemory.
Neural computation, 9(8):1735?1780.
[Iyyer et al2014] Mohit Iyyer, Jordan Boyd-Graber,Leonardo Claudino, Richard Socher, and HalDaum?e III.
2014.
A neural network for factoidquestion answering over paragraphs.
In Proceedingsof Empirical Methods in Natural Language Process-ing.
[Jansen et al2014] Peter Jansen, Mihai Surdeanu, andPeter Clark.
2014.
Discourse complements lexicalsemantics for non-factoid answer reranking.
In Pro-ceedings of the 52nd Annual Meeting of the Associa-tion for Computational Linguistics, ACL 2014, June22-27, 2014, Baltimore, MD, USA, Volume 1: LongPapers, pages 977?986.
[Kulkarni and Finlayson2011] Nidhi Kulkarni andMark Alan Finlayson.
2011. jmwe: A java toolkitfor detecting multi-word expressions.
In Proceed-ings of the Workshop on Multiword Expressions:from Parsing and Generation to the Real World,pages 122?124.
Association for ComputationalLinguistics.
[MacCartney et al2008] Bill MacCartney, Michel Gal-ley, and Christopher D Manning.
2008.
A phrase-based alignment model for natural language infer-ence.
In Proceedings of the conference on empiricalmethods in natural language processing, pages 802?811.
[Mann and Thompson1988] William C Mann and San-dra A Thompson.
1988.
{Rhetorical Struc-ture Theory: Toward a functional theory of textorganisation}.
Text, 3(8):234?281.
[Mitchell and Lapata2008] Jeff Mitchell and MirellaLapata.
2008.
Vector-based models of seman-tic composition.
In ACL 2008, Proceedings of the46th Annual Meeting of the Association for Compu-tational Linguistics, June 15-20, 2008, Columbus,Ohio, USA, pages 236?244.
[Richardson et al2013] Matthew Richardson, Christo-pher JC Burges, and Erin Renshaw.
2013.
Mctest:A challenge dataset for the open-domain machinecomprehension of text.
In Proceedings of Em-pirical Methods in Natural Language Processing(EMNLP).
[Sachan et al2015] Mrinmaya Sachan, Avinava Dubey,Eric P Xing, and Matthew Richardson.
2015.Learning answer-entailing structures for machinecomprehension.
In Proceedings of the Annual Meet-ing of the Association for Computational Linguis-tics.
[Sammons et al2009] M. Sammons, V. Vydiswaran,T.
Vieira, N. Johri, M. Chang, D. Goldwasser,V.
Srikumar, G. Kundu, Y. Tu, K. Small, J. Rule,Q.
Do, and D. Roth.
2009.
Relation alignment fortextual entailment recognition.
In TAC.
[Srivastava and Hovy2013] Shashank Srivastava andDirk Hovy.
2013.
A walk-based semantically en-riched tree kernel over distributed word representa-tions.
In Proceedings of Empirical Methods in Nat-ural Language Processing, pages 1411?1416.
[Stern and Dagan2012] Asher Stern and Ido Dagan.2012.
Biutee: A modular open-source system forrecognizing textual entailment.
In Proceedings ofthe ACL 2012 System Demonstrations, pages 73?78.
[Sultan et al2014] Arafat Md Sultan, Steven Bethard,and Tamara Sumner.
2014.
Back to basics formonolingual alignment: Exploiting word similarityand contextual evidence.
Transactions of the Asso-ciation of Computational Linguistics ?
Volume 2, Is-sue 1, pages 219?230.472[Yao and Van Durme2014] Xuchen Yao and BenjaminVan Durme.
2014.
Information extraction overstructured data: Question answering with freebase.In Proceedings of the 52nd Annual Meeting of theAssociation for Computational Linguistics (Volume1: Long Papers), pages 956?966.
Association forComputational Linguistics.
[Yao et al2013] Xuchen Yao, Benjamin Van Durme,Chris Callison-Burch, and Peter Clark.
2013.A lightweight and high performance monolingualword aligner.
In ACL (2), pages 702?707.
[Yih et al2013] Wentau Yih, Ming-Wei Chang,Christopher Meek, and Andrzej Pastusiak.
2013.Question answering using enhanced lexical se-mantic models.
In Proceedings of the 51st AnnualMeeting of the Association for ComputationalLinguistics.
[Yuille and Rangarajan2003] A. L. Yuille and AnandRangarajan.
2003.
The concave-convex procedure.Neural Comput.473
