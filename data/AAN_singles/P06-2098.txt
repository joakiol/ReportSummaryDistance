Proceedings of the COLING/ACL 2006 Main Conference Poster Sessions, pages 763?770,Sydney, July 2006. c?2006 Association for Computational LinguisticsExact Decoding for Jointly Labeling and Chunking SequencesNobuyuki ShimizuDepartment of Computer ScienceState University of New York at AlbanyAlbany, NY 12222, USAnobuyuki@shimizu.nameAndrew HaasDepartment of Computer ScienceState University of New York at AlbanyAlbany, NY 12222 USAhaas@cs.albany.eduAbstractThere are two decoding algorithms essen-tial to the area of natural language pro-cessing.
One is the Viterbi algorithmfor linear-chain models, such as HMMsor CRFs.
The other is the CKY algo-rithm for probabilistic context free gram-mars.
However, tasks such as noun phrasechunking and relation extraction seem tofall between the two, neither of them be-ing the best fit.
Ideally we would like tomodel entities and relations, with two lay-ers of labels.
We present a tractable algo-rithm for exact inference over two layersof labels and chunks with time complexityO(n2), and provide empirical results com-paring our model with linear-chain mod-els.1 IntroductionThe Viterbi algorithm and the CKY algorithms aretwo decoding algorithms essential to the area of nat-ural language processing.
The former models a lin-ear chain of labels such as part of speech tags, andthe latter models a parse tree.
Both are used to ex-tract the best prediction from the model (Manningand Schutze, 1999).However, some tasks seem to fall between thetwo, having more than one layer but flatter than thetrees created by parsers.
For example, in relationextraction, we have entities in one layer and rela-tions between entities as another layer.
Another taskis shallow parsing.
We may want to model part-of-speech tags and noun/verb chunks at the same time,since performing simultaneous labeling may resultin increased joint accuracy by sharing informationbetween the two layers of labels.To apply the Viterbi decoder to such tasks, weneed two models, one for each layer.
We must feedthe output of one layer to the next layer.
In such anapproach, errors in earlier processing nearly alwaysaccumulate and produce erroneous results at the end.If we use CKY, we usually end up flattening the out-put tree to obtain the desired output.
This seems likea round-about way of modeling two layers.There are previous attempts at modeling twolayer labeling.
Dynamic Conditional Random Fields(DCRFs) by (McCallum et al 2003; Sutton et al2004) is one such attempt, however, exact inferenceis in general intractable for these models and theauthors were forced to settle for approximate infer-ence.Our contribution is a novel model for two layerlabeling, for which exact decoding is tractable.
Ourexperiments show that our use of label-chunk struc-tures results in significantly better performance overcascaded CRFs, and that the model is a promisingalternative to DCRFs.The paper is organaized a follows: In Section 2and 3, we describe the model and present the de-coding algorithm.
Section 4 describes the learningmethods applicable to our model and the baselinemodels.
In Section 5 and 6, we describe the experi-ments and the results.763Token POS NPU.K.
JADJ Bbase NOUN Irates NOUN Iare VERB Oat OTHER Otheir OTHER Bhighest JADJ Ilevel NOUN Iin OTHER Oeight OTHER Byears NOUN I.
OTHER OTable 1: Example with POS and NP tags2 Model for Joint Labeling and ChunkingConsider the task of finding noun chunks.
The nounchunk extends from the beginning of a noun phraseto the head noun, excluding postmodifiers (whichare difficult to attach correctly).
Table 1 shows asentence labeled with POS tags and segmented intonoun chunks.
B marks the first word of a nounchunk, I the other words in a noun chunk, and Othe words that are not in a noun chunk.
Note thatwe collapsed the 45 different POS labels into 5 la-bels, following (McCallum et al 2003).
All differ-ent types of adjectives are labeled as JADJ.Each word carries two tags.
Given the first layer,our aim is to present a model that can predict thesecond and third layers of tags at the same time.Assume we have n training samples, {(xi, yi)}ni=1,where xi is a sequence of input tokens and yi is alabel-chunk structure for xi.
In this example, thefirst column contains the tokens xi and the secondand third columns together represent the label-chunkstructures yi.
We will present an efficient exact de-coding for this structure.The label-chunk structure, shown in Table 2, is arepresentation of the two layers of tags.
The tuplesin Table 2 are called parts.
If the token at index rcarries a POS tag P and a chunk tag C , the first layerincludes part ?C,P, r?.
This part is called a node.If the tokens at index r ?
1 and r are in the samechunk, and C is the label of that chunk, the first layeralso includes part ?C,P0, P, r?1, r?
(where P0 andP are the POS tags of the tokens at r ?
1 and rToken First Layer (POS) Second Layer (NP)U.K. ?I, JADJ, 0?
?I, JADJ, NOUN, 0, 1?base ?I, NOUN, 1?
?I, NOUN, NOUN, 1, 2?rates ?I, NOUN, 2?
?I, 0, 2?
?I,O, 2, 3?are ?O, VERB, 3?
?O, VERB, OTHER, 3, 4?at ?O, OTHER, 4?
?O, 3, 4?
?O, I, 4, 5?their ?I, OTHER, 5?
?I, OTHER, JADJ, 5, 6?highest ?I, JADJ, 6?
?I, JADJ, NOUN, 6, 7?level ?I, NOUN, 7?
?I, 5, 7?
?I,O, 7, 8?in ?O, OTHER, 8?
?O, 8, 8?
?O, I, 8, 9?eight ?I, OTHER, 9?
?I, OTHER, NOUN, 9, 10?years ?I, NOUN, 10?
?I, 9, 10?
?I,O, 10, 11?.
?O, OTHER, 11?
?O, 11, 11?Table 2: Example Partsrespectively).
This part is called a transition.
If achunk tagged C extends from the token at q to thetoken at r inclusive, the second layer includes part?C, q, r?.
This part is a chunk node.
And if the tokenat q?1 is the last token in a chunk tagged C0, whilethe token at q is the first token of a chunk tagged C ,the second layer includes part ?C0, C, q?1, q?.
Thispart is a chunk transition.In this paper we use the common method of fac-toring the score of the label-chunk structure as thesum of the scores of all the parts.
Each part in alabel-chunk structure can be lexicalized, and givesrise to several features.
For each feature, we have acorresponding weight.
If we sum up the weights forthese features, we have the score for the part, and ifwe sum up the scores of the parts, we have the scorefor the label-chunk structure.Suppose we would like to score a pair (xi, yi) inthe training set, and it happens to be the one shownin Table 2.
To begin, let?s say we would like to findthe features for the part ?I,NOUN, 7?
of POS nodetype (1st Layer).
This is the NOUN tag on the sev-enth token ?level?
in Table 2.
By default, the POSnode type generates the following binary feature.?
Is there a token labeled with ?NOUN?
in achunk labeled with ?I?
?764Now, to have more features, we can lexicalize POSnode type.
Suppose we use xr to lexicalize POSnode ?C,P, r?, then we have the following binaryfeature, as it is ?I,NOUN, 7?
and xi7 = ?level?.?
Is there a token ?level?
labeled with ?NOUN?in a chunk labeled with ?I?
?We can also use xr?1 and xr to lexicalize the partsof POS node type.?
Is there a token ?level?
labeled with ?NOUN?in a chunk labeled with ?I?
that?s preceded by?highest?
?This way, we have a complete specification of thefeature set given the part type, lexicalization for eachpart type and the training set.
Let us define f aboolean feature vector function such that each di-mension of f(xi, yi) contains 1 if the pair (xi, yi)has the feature, 0 otherwise.
Now define a real-valued weight vector w with the same dimensionas f .
To represent the score of the pair (xi, yi), wewrite s(xi, yi) = w?f(xi, yi) We could also havew?f(xi, {p}) where p just a single part, in whichcase we just write s(p).Assuming an appropriate feature representationas well as a weight vector w, we would like tofind the highest scoring label-chunk structure y =argmaxy?
(w?f(x, y?))
given an input sentence x.In the upcoming section, we present a decodingalgorithm for the label-chunk structures, and laterwe give a method for learning the weight vector usedin the decoding.3 DecodingThe decoding algorithm is shown in Figure 1.
Theidea is to use two tables for dynamic programming:label table and chunk table.Suppose we are examining the current positionr, and would like to consider extending the chunk[q, r?
1] to [q, r].
We need to know the chunk tag Cfor [q, r?
1] and the last POS tag P0 at index r?
1.The array entry label table[q][r ?
1] keeps track ofthis information.Then we examine how the current chunk is con-nected with the previous chunk.
The array entrychunk table[q][C0] keeps track of the score of thebest label-chunk structure from 0 up to the index qthat has the ending chunk tag C0.
Now checkingthe chunk transition from C0 to C at the index q issimple, and we can record the score of this chunk tochunk table[r][C], so that the next chunk starting atr can use this information.In short, we are executing two Viterbi algorithmson the first and second layer at the same time.
Oneextends [q, r ?
1] to [q, r], considering the node in-dexed by r (first layer).
The other extends [0, q] to[0, r], considering the node indexed by [q, r] (sec-ond layer).
The dynamic programming table for thefirst layer is kept in the label table (r ?
1 and P0are used in the Viterbi algorithm for this layer) andthat for the second layer in the chunk table (q andC0 used).
The algorithm returns the best score ofthe label-chunk structure.To recover the structure, we simply need to main-tain back pointers to the items that gave rise to theeach item in the dynamic programming table.
Thisis just like maintaining back pointers in the Viterbialgorithm for sequences, or the CKY algorithm forparsing.The pseudo-code shows that the run-time com-plexity of the decoding algorithm is O(n2) unlikethat of CFG parsing, O(n3).
Thus the algorithm per-forms better on long sentences.
On the other hand,the constant is c2p2 where c is the number of chunktags and p is the number of POS tags.4 Learning4.1 Voted PerceptronIn the CKY and Viterbi decoders, we use theforward-backward or inside-outside algorithm tofind the marginal probabilities.
Since we don?t yethave the inference algorithm to find the marginalprobabilities of the parts of a label-chunk structure,we use an online learning algorithm to train themodel.
Despite this restriction, the voted percep-tron is known for its performance (Sha and Pereira,2003).The voted perceptron we use is the adaptation of(Freund and Schapire, 1999) to the structured set-ting.
Algorithm 4.1 shows the pseudo code for thetraining, and the function update(wk, xi, yi, y?)
re-turns wk ?
f(xi, y?)
+ f(xi, yi) .Given a training set {(xiyi)}ni=1 and the epochnumber T, Algorithm 4.1 will return a list of765Algorithm 3.1: DECODE(the scoring function s(p))score := 0;for q := index start to index endfor length := 1 to index end ?
qr := q + length;for each Chunk Tag Cfor each Chunk Tag C0for each POS Tag Pfor each POS Tag P0score := 0;if (length > 1)#Add the score of the transition from r-2 to r-1.
(1st Layer, POS)score := score + s(?C,P0, P, r ?
2, r ?
1?)
+ label table[q][r ?
1][C][P0];#Add the score of the node at r-1.
(1st Layer, POS)score := score + s(?C,P, r ?
1?
);if (score >= label table[q][r][C][P ])label table[q][r][C][P ] := score;#Add the score of the chunk node at [q,r-1].
(2nd Layer, NP)score := score + s(?C, q, r ?
1?
);if (index start < q)#Add the score of the chunk transition from q-1 to q.
(2nd Layer, NP)score := score + s(?C0, C, q ?
1, q?)
+ chunk table[q][C0];if (score >= chunk table[r][C])chunk table[r][C] := score;end forend forend forend forend forend forscore := 0;for each C in chunk tagsif (chunk table[index end][C] >= score)score := chunk table[index end][C];last symbol := C;end forreturn (score)Note: Since the scoring function s(p) is defined as w?f(xi, {p}), the input sequence xi and the weightvector w are also the inputs to the algorithm.Figure 1: Decoding Algorithm766weighted perceptrons {(w1, c1), ..(wk, ck)}.
The fi-nal model V uses the weight vectorw =?kj=1(cjwj)Tn(Collins, 2002).Algorithm 4.1: TRAIN(T, {(xi, yi)}ni=1)k := 0;w1 := 0;c1 := 0;for t := 1 to Tfor i := 1 to ny?
:= argmaxy(w?k f(y, xi))if (y?
= yi)ck := ck + 1;elsewk+1 := update(wk, xi, yi, y?
);ck+1 := 1;k := k + 1;ck := ck + 1;end forend forreturn ({(w1, c1), ..(wk, ck)})Algorithm 4.2: UPDATE1(wk, xi, yi, y?
)return (wk ?
f(xi, y?)
+ f(xi, yi))Algorithm 4.3: UPDATE2(wk, xi, yi, y?)?
= max(0,min( li(y?)?s(xi,yi)+s(xi,y?)?fi(yi)?fi(y?
)?2, 1));return (wk ?
?f(xi, y?)
+ ?f(xi, yi))4.2 Max Margin4.2.1 Sequential Minimum OptimizationA max margin method minimizes the regularizedempirical risk function with the hard (penalized)marginminw12?w?2?
?i(s(xi, yi)?maxy(s(xi, y)?li(y)))li finds the loss for y with respect to yi, and it is as-sumed that the function is decomposable just as y isdecomposable to the parts.
This equation is equiva-lent tominw 12?w?2 + C?i ?i?i, y, s(xi, yi) + ?i ?
s(xi, y) ?
li(y)After taking the Lagrange dual formation, we havemax??0?12?
?i,y?i(y)(f(xi, yi)?
f(xi, y))?2 +?i,y?i(y)li(y)such that?y?i(y) = Candw =?i,y?i(y)(f(xi, yi) ?
f(xi, y)) (1)This quadratic program can be optimized by bi-coordinate descent, known as Sequential MinimumOptimization.
Given an example i and two label-chunk structures y?
and y?
?,d = li(y?)
?
li(y??)
?
(s(xi, y??)
?
s(xi, y?))?fi(y??)
?
fi(y?)?2(2)?
= max(??i(y?
),min(d, ?i(y??
))The updated values are : ?i(y?)
:= ?i(y?)
+ ?
and?i(y??)
:= ?i(y??)
?
?.Using the equation (1), any increase in ?
can betranslated to w. For a naive SMO, this update isexecuted for each training sample i, for all pairs ofpossible parses y?
and y??
for xi.
See (Taskar andKlein, 2005; Zhang, 2001; Jaakkola et al 2000).Here is where we differ from (Taskar et al 2004).We choose y??
to be the correct parse yi, and y?to be the best runner-up.
After setting the ini-tial weights using yi, we also set ?i(yi) = 1 and?i(y?)
= 0.
Although these alphas are not correct,as optimization nears the end, the margin is wider;?i(yi) and ?i(y?)
gets closer to 1 and 0 respec-tively.
Given this approximation, we can compute ?.Then, the function update(wk, xi, yi, y?)
will returnwk?
?f(xi, y?
)+?f(xi, yi) and we have reduced theSMO to the perceptron weight update.4.2.2 Margin Infused Relaxed AlgorithmWe can think of maximizing the margin in termsof extending the Margin Infused Relaxed Algorithm(MIRA) (Crammer and Singer, 2003; Crammer etal, 2003) to learning with structured outputs.
(Mc-Donald et al 2005) presents this approach for de-pendency parsing.In particuler, Single-best MIRA (McDonald etal, 2005) uses only the single margin constraint forthe runner up y?
with the highest score.
The result-ing online update would be wk+1 with the following767condition: min?wk+1 ?
wk?
such that s(xi, yi) ?s(xi, y?)
?
li(y?)
where y?
= argmaxys(xi, y).Incidentally, the equation (2) for d above when?i(yi) = 1 and ?i(y?)
= 0 solves this minimizationproblem as well, and the weight update is the sameas the SMO case.4.2.3 Conditional Random FieldsInstead of minimizing the regularized empiricalrisk function with the hard (penalized) margin, con-ditional random fields try to minimize the same withthe negative log loss:minw12?w?2 ?
?i(s(xi, yi) ?
log(?ys(xi, y)))Usually, CRFs use marginal probabilities of parts todo the optimization.
Since we have not yet comeup with the algorithm to compute marginals for alabel-chunk structure, the training methods for CRFsis not applicable to our purpose.
However, on se-quence labeling tasks CRFs have shown very goodperformance (Lafferty et al 2001; Sha and Pereira,2003), and we will use them for the baseline com-parison.5 Experiments5.1 Task: Base Noun Phrase ChunkingThe data for the training and evaluation comes fromthe CoNLL 2000 shared task (Tjong Kim Sang andBuchholz, 2000), which is a portion of the WallStreet Journal.We consider each sentence to be a training in-stance xi, with single words as tokens.The shared task data have a standard training setof 8936 sentences and a test set of 2012 sentences.For the training, we used the first 447 sentences fromthe standard training set, and our evaluation wasdone on the standard test set of the 2012 sentences.Let us define the set D to be the first 447 samplesfrom the standard training set .There are 45 different POS labels, and the threeNP labels: begin-phrase, inside-phrase, and other.
(Ramshaw and Marcus, 1995) To reduce the infer-ence time, following (McCallum et al 2003), wecollapsed the 45 different POS labels contained inthe original data.
The rules for collapsing the POSlabels are listed in the Table 3.Original Collapsedall different types of nouns NOUNall different types of verbs VERBall different types of adjectives JADJall different types of adverbs RBPthe remaining POS labels OTHERTable 3: Rules for collapsing POS tagsToken POS Collapsed Chunk NPU.K.
JJ JADJ B-NP Bbase NN NOUN I-NP Irates NNS NOUN I-NP Iare VBP VERB B-VP Oat IN OTHER B-PP Otheir PRP$ OTHER B-NP Bhighest JJS JADJ I-NP Ilevel NN NOUN I-NP Iin IN OTHER B-PP Oeight CD OTHER B-NP Byears NNS NOUN I-NP I. .
OTHER O OTable 4: Example with POS and NP labels, beforeand after collapsing the labels.We present two experiments: one comparingour label-chunk model with a cascaded linear-chainmodel and a simple linear-chain model, and onecomparing different learning algorithms.The cascaded linear-chain model uses one linear-chain model to predict POS tags, and another linear-chain model to predict NP labels, using the POS tagspredicted by the first model as a feature.More specifically, we trained a POS-tagger usingthe training set D. We then used the learned modeland replaced the POS labels of the test set with thelabels predicted by the learned model.
The linear-chain NP chunker was again trained on D and eval-uated on this new test set with POS supplied by theearlier processing.
Note that the new test set has ex-actly the same word tokens and noun chunks as theoriginal test set.5.2 Systems5.2.1 POS Tagger and NP ChunkerThere are three versions of POS taggers and NPchunkers: CRF, VP, MMVP.
For CRF, L-BFGS,a quasi-Newton optimization method was used forthe training, and the implementation we used isCRF++ (Kudo, 2005).
VP uses voted perceptron,and MMVP uses max margin update for the votedperceptron.
For the voted perceptron, we used aver-768if xq matches then tq is[A-Z][a-z]+ CAPITAL[A-Z] CAP ONE[A-Z]+ CAP ALL[A-Z]+[a-z]+[A-Z]+[a-z] CAP MIX.*[0-9].
* NUMBERTable 5: Rules to create tq for each token xqFirst Layer (POS)Node ?C,P, r?
Trans.
?C,P0, P, r ?
1, r?xr?1 xr?1xr xrxr+1trSecond Layer (NP)Node ?C, q, r?
Trans.
?C0, C, q ?
1, q?xq xq?1xq?1 xqxrxr+1Table 6: Lexicalized Features for Joint Modelsaging of the weights suggested by (Collins, 2002).The features are exactly the same for all three sys-tems.5.2.2 Cascaded ModelsFor each CRF, VP, MMVP, the output of a POStagger was used as a feature for the NP chunker.The feeds always consist of a POS tagger and NPchunker of the same kind, thus we have CRF+CRF,VP+VP, and MMVP+MMVP.5.2.3 Joint ModelsSince CRF requires the computation of marginalsfor each part, we were not able to use the learningmethod.
VP and MMVP were used to train the label-chunk structures with the features explained in thefollowing section.5.3 FeaturesFirst, as a preprocessing step, for each word tokenxq, feature tq was created with the rule in Table 5,and included in the input files.
This feature is in-cluded in x along with the word tokens.
The featuretells us whether the token is capitalized, and whetherdigits occur in the token.
No outside resources suchas a list of names or a gazetteer were used.Table 6 shows the lexicalized features for the jointlabeling and chunking.
For the first iteration of train-ing, the weights for the lexicalized features were notPOS tagging POS NP F1CRF 91.56% N/A N/AVP 90.55% N/A N/AMMVP 90.02% N/A N/ANP chunking POS NP F1CRF given 94.44% 87.52%VP given 94.28% 86.96%MMVP given 94.17% 86.79%Both POS & NP POS NP F1CRF + CRF above 90.16% 79.08%VP + VP above 89.21% 76.26%MMVP + MMVP above 88.95% 75.28%VP Joint 88.42% 90.60% 79.69%MMVP Joint 88.69% 90.84% 80.34%Table 7: Performanceupdated.
The intention is to have more weights onthe unlexicalized features, so that when lexical fea-ture is not found, unlexicalized features could pro-vide useful information and avoid overfitting, muchas back-off probabilities do.6 ResultWe evaluated the performance of the systems usingthree measures: POS accuracy, NP accuracy, and F1measure on NP.
These figures show how errors ac-cumulate as the systems are chained together.
Forthe statistical significance testing, we have used pair-samples t test, and for the joint labeling and chunk-ing task, everything was found to be statistically sig-nificant except for CRF + CRF vs VP Joint.One can see that the systems with joint label-ing and chunking models perform much better thanthe cascaded models.
Surprisingly, the perceptronupdate motivated by the max margin principle per-formed significantly worse than the simple percep-tron update for linear-chain models but performedbetter on joint labeling and chunking.Although joint labeling and chunking model takeslonger time per sample because of the time complex-ity of decoding, the number of iteration needed toachieve the best result is very low compared to othersystems.
The CPU time required to run 10 iterationsof MMVP is 112 minutes.7 ConclusionWe have presented the decoding algorithm for label-chunk structure and showed its effectiveness in find-ing two layers of information, POS tags and NPchunks.
This algorithm has a place between the769POS tagging IterationsVP 30MMVP 40CRF 126NP chunking IterationsVP 70MMVP 50CRF 101Both POS & NP IterationsVP 10MMVP 10Table 8: Iterations needed for the resultViterbi algorithm for linear-chain models and theCKY algorithm for parsing, and the time complex-ity is O(n2).
The use of our label-chunk structuresignificantly boosted the performance over cascadedCRFs despite the online learning algorithms used totrain the system, and shows itself as a promising al-ternative to cascaded models, and possibly dynamicconditional random fields for modeling two layers oftags.
Further work includes applying the algorithmto relation extraction, and devising an effective algo-rithm to find the marginal probabilities of parts.ReferencesM.
Collins.
2002.
Discriminative training methods forhidden Markov models: Theory and experiments withperceptron algorithms.
In Proc.
of Empirical Methodsin Natural Language Processing (EMNLP)K. Crammer and Y.
Singer.
2003.
Ultraconservative on-line algorithms for multiclass problems.
Journal ofMachine Learning ResearchK.
Crammer, O. Dekel, S. Shalev-Shwartz, and Y. Singer.2003.
Online passive aggressive algorithms.
In Ad-vances in Neural Information Processing Systems 15K.
Crammer, R. McDonald, and F. Pereira.
2004.
Newlarge margin algorithms for structured prediction.
InLearning with Structured Outputs Workshop (NIPS)Y. Freund and R. Schapire 1999.
Large Margin Classi-fication using the Perceptron Algorithm.
In MachineLearning, 37(3):277-296.T.S.
Jaakkola, M. Diekhans, and D. Haussler.
2000.
Adiscriminative framework for detecting remote proteinhomologies.
Journal of Computational BiologyT.
Kudo 2005.
CRF++: Yet Another CRF toolkit.
Avail-able at http://chasen.org/?taku/software/CRF++/J.
Lafferty, A. McCallum, and F. Pereira.
2001.
Condi-tional Random Fields: Probabilistic Models for Seg-menting and Labeling Sequence Data.
In Proc.
of the18th International Conference on Machine Learning(ICML)F. Peng and A. McCallum.
2004.
Accurate Informa-tion Extraction from Research Papers using Condi-tional Random Fields.
In Proc.
of the Human Lan-guage Technology Conf.
(HLT)F. Sha and F. Pereira.
2003.
Shallow parsing with condi-tional random fields.
In Proc.
of the Human LanguageTechnology Conf.
(HLT)C. Manning and H. Schutze.
1999.
Foundations of Sta-tistical Natural Language Processing MIT Press.A.
McCallum, K. Rohanimanesh and C. Sutton.
2003.Dynamic Conditional Random Fields for Jointly La-beling Multiple Sequences.
In Proc.
of Workshop onSyntax, Semantics, Statistics.
(NIPS)R. McDonald, K. Crammer, and F. Pereira.
2005.
Onlinelarge-margin training of dependency parsers.
In Proc.of the 43rd Annual Meeting of the ACLL.
Ramshaw and M. Marcus.
1995.
Text chunking us-ing transformation-based learning.
In Proc.
of ThirdWorkshop on Very Large Corpora.
ACLC.
Sutton, K. Rohanimanesh and A. McCallum.
2004.Dynamic Conditional Random Fields: FactorizedProbabilistic Models for Labeling and Segmenting Se-quence Data.
In Proc.
of the 21st International Con-ference on Machine Learning (ICML)B. Taskar, D. Klein, M. Collins, D. Koller, and C. Man-ning 2004.
Max Margin Parsing.
In Proc.
ofEmpirical Methods in Natural Language Processing(EMNLP)B. Taskar and D. Klein.
2005.
Max-Margin Methods forNLP: Estimation, Structure, and Applications Avail-able at http://www.cs.berkeley.edu/?taskar/pubs/max-margin-acl05-tutorial.pdfE.
F. Tjong Kim Sang and S. Buchholz.
2000.
Introduc-tion to the CoNLL-2000 shared task: Chunking.
InProc.
of the 4th Conf.
on Computational Natural Lan-guage Learning (CoNLL)T. Zhang.
2001.
Regularized winnow methods.
In Ad-vances in Neural Information Processing Systems 13770
