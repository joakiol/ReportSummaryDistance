Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 1169?1178,Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational LinguisticsA Pronoun Anaphora Resolution System based onFactorial Hidden Markov ModelsDingcheng LiUniversity of Minnesota,Twin Cities, Minnesostalixxx345@umn.eduTim MillerUniversity of WisconsinMilwaukee, Wisconsintmill@cs.umn.eduWilliam SchulerThe Ohio State UniversityColumbus, Ohioschuler@ling.osu.eduAbstractThis paper presents a supervised pronounanaphora resolution system based on factorialhidden Markov models (FHMMs).
The ba-sic idea is that the hidden states of FHMMsare an explicit short-term memory with an an-tecedent buffer containing recently describedreferents.
Thus an observed pronoun can findits antecedent from the hidden buffer, or interms of a generative model, the entries in thehidden buffer generate the corresponding pro-nouns.
A system implementing this model isevaluated on the ACE corpus with promisingperformance.1 IntroductionPronoun anaphora resolution is the task of find-ing the correct antecedent for a given pronominalanaphor in a document.
It is a subtask of corefer-ence resolution, which is the process of determin-ing whether two or more linguistic expressions ina document refer to the same entity.
Adopting ter-minology used in the Automatic Context Extraction(ACE) program (NIST, 2003), these expressionsare called mentions.
Each mention is a referenceto some entity in the domain of discourse.
Men-tions usually fall into three categories ?
proper men-tions (proper names), nominal mentions (descrip-tions), and pronominal mentions (pronouns).
Thereis a great deal of related work on this subject, sothe descriptions of other systems below are thosewhich are most related or which the current modelhas drawn insight from.Pairwise models (Yang et al, 2004; Qiu et al,2004) and graph-partitioning methods (McCallumand Wellner, 2003) decompose the task into a col-lection of pairwise or mention set coreference de-cisions.
Decisions for each pair or each groupof mentions are based on probabilities of featuresextracted by discriminative learning models.
Theaforementioned approaches have proven to be fruit-ful; however, there are some notable problems.
Pair-wise modeling may fail to produce coherent parti-tions.
That is, if we link results of pairwise deci-sions to each other, there may be conflicting corefer-ences.
Graph-partitioning methods attempt to recon-cile pairwise scores into a final coherent clustering,but they are combinatorially harder to work with indiscriminative approaches.One line of research aiming at overcoming thelimitation of pairwise models is to learn a mention-ranking model to rank preceding mentions for agiven anaphor (Denis and Baldridge, 2007) This ap-proach results in more coherent coreference chains.Recent years have also seen the revival of in-terest in generative models in both machine learn-ing and natural language processing.
Haghighiand Klein (2007), proposed an unsupervised non-parametric Bayesian model for coreference resolu-tion.
In contrast to pairwise models, this fully gener-ative model produces each mention from a combina-tion of global entity properties and local attentionalstate.
Ng (2008) did similar work using the same un-supervised generative model, but relaxed head gen-eration as head-index generation, enforced agree-ment constraints at the global level, and assignedsalience only to pronouns.Another unsupervised generative model was re-cently presented to tackle only pronoun anaphora1169resolution (Charniak and Elsner, 2009).
Theexpectation-maximization algorithm (EM) was ap-plied to learn parameters automatically from theparsed version of the North American News Cor-pus (McClosky et al, 2008).
This model generates apronoun?s person, number and gender features alongwith the governor of the pronoun and the syntacticrelation between the pronoun and the governor.
Thisinference process allows the system to keep track ofmultiple hypotheses through time, including multi-ple different possible histories of the discourse.Haghighi and Klein (2010) improved their non-parametric model by sharing lexical statistics at thelevel of abstract entity types.
Consequently, theirmodel substantially reduces semantic compatibilityerrors.
They report the best results to date on thecomplete end-to-end coreference task.
Further, thismodel functions in an online setting at mention level.Namely, the system identifies mentions from a parsetree and resolves resolution with a left-to-right se-quential beam search.
This is similar to Luo (2005)where a Bell tree is used to score and store thesearching path.In this paper, we present a supervised pro-noun resolution system based on Factorial HiddenMarkov Models (FHMMs).
This system is moti-vated by human processing concerns, by operatingincrementally and maintaining a limited short termmemory for holding recently mentioned referents.According to Clark and Sengul (1979), anaphoricdefinite NPs are much faster retrieved if the an-tecedent of a pronoun is in immediately previoussentence.
Therefore, a limited short term memoryshould be good enough for resolving the majority ofpronouns.
In order to construct an operable model,we also measured the average distance between pro-nouns and their antecedents as discussed in next sec-tions and used distances as important salience fea-tures in the model.Second, like Morton (2000), the current sys-tem essentially uses prior information as a dis-course model with a time-series manner, using adynamic programming inference algorithm.
Third,the FHMM described here is an integrated system,in contrast with (Haghighi and Klein, 2010).
Themodel generates part of speech tags as simple struc-tural information, as well as related semantic in-formation at each time step or word-by-word step.While the framework described here can be ex-tended to deeper structural information, POS tagsalone are valuable as they can be used to incorpo-rate the binding features (described below).Although the system described here is evaluatedfor pronoun resolution, the framework we describecan be extended to more general coreference resolu-tion in a fairly straightforward manner.
Further, asin other HMM-based systems, the system can be ei-ther supervised or unsupervised.
But extensions tounsupervised learning are left for future work.The final results are compared with a few super-vised systems as the mention-ranking model (De-nis and Baldridge, 2007) and systems compared intheir paper, and Charniak and Elsner?s (2009) unsu-pervised system, emPronouns.
The FHMM-basedpronoun resolution system does a better job than theglobal ranking technique and other approaches.
Thisis a promising start for this novel FHMM-based pro-noun resolution system.2 Model DescriptionThis work is based on a graphical model frameworkcalled Factorial Hidden Markov Models (FHMMs).Unlike the more commonly known Hidden MarkovModel (HMM), in an FHMM the hidden state ateach time step is expanded to contain more than onerandom variable (as shown in Figure 1).
This al-lows for the use of more complex hidden states bytaking advantage of conditional independence be-tween substates.
This conditional independence al-lows complex hidden states to be learned with lim-ited training data.2.1 Factorial Hidden Markov ModelFactorial Hidden Markov Models are an extensionof HMMs (Ghahramani and Jordan, 1997).
HMMsrepresent sequential data as a sequence of hiddenstates generating observation states (words in thiscase) at corresponding time steps t. A most likelysequence of hidden states can then be hypothesizedgiven any sequence of observed states, using BayesLaw (Equation 2) and Markov independence as-sumptions (Equation 3) to define a full probability asthe product of a Transition Model (?T ) prior prob-ability and an Observation Model (?O) likelihood1170probability.h?1..Tdef= argmaxh1..TP(h1..T | o1..T ) (1)def= argmaxh1..TP(h1..T ) ?
P(o1..T |h1..T ) (2)def= argmaxh1..TT?t=1P?T (ht |ht?1) ?
P?O(ot |ht)(3)For a simple HMM, the hidden state correspondingto each observation state only involves one variable.An FHMM contains more than one hidden variablein the hidden state.
These hidden substates are usu-ally layered processes that jointly generate the ev-idence.
In the model described here, the substatesare also coupled to allow interaction between theseparate processes.
As Figure 1 shows, the hiddenstates include three sub-states, op, cr and pos whichare short forms of operation, coreference feature andpart-of-speech.
Then, the transition model expandsthe left term in (3) to (4).P?T (ht |ht?1)def= P(opt | opt?1, post?1)?P(crt | crt?1, opt?1)?P(post | opt, post?1)(4)The observation model expands from the rightterm in (3) to (5).P?O(ot |ht)def= P(ot | post, crt) (5)The observation state depends on more than one hid-den state at each time step in an FHMM.
Each hid-den variable can be further split into smaller vari-ables.
What these terms stand for and the motiva-tions behind the above equations will be explainedin the next section.2.2 Modeling a Coreference Resolver withFHMMsFHMMs in our model, like standard HMMs, can-not represent the hierarchical structure of a syntac-tic phrase.
In order to partially represent this in-formation, the head word is used to represent thewhole noun phrase.
After coreference is resolved,the coreferring chain can then be expanded to thewhole phrase with NP chunker tools.In this system, hidden states are composed ofthree main variables: a referent operation (OP),coreference features (CR) and part of speech tags(POS) as displayed in Figure 1.
The transition modelis defined as Equation 4.opt-1=copypost-1=VBZot-1=loveset-1=per,orggt-1=neu,femcrt-1opt=oldpost=PRPot=themgt=fem,neucrtht-1 htet=org,pernt-1=plu,singnt=sing,pluit-1=-,2it=0,2Figure 1: Factorial HMM CR ModelThe starting point for the hidden state at each timestep is the OP variable, which determines whichkind of referent operations will occur at the currentword.
Its domain has three possible states: none,new and old.The none state indicates that the present state willnot generate a mention.
All previous hidden statevalues (the list of previous mentions) will be passeddeterministically (with probability 1) to the currenttime step without any changes.
The new state signi-fies that there is a new mention in the present timestep.
In this event, a new mention will be added tothe entity set, as represented by its set of feature val-ues and position in the coreference table.
The oldstate indicates that there is a mention in the presenttime state and that this mention refers back to someantecedent mention.
In such a case, the list of enti-ties in the buffer will be reordered deterministically,moving the currently mentioned entity to the top ofthe list.Notice that opt is defined to depend on opt?1and post?1.
This is sometimes called a switchingFHMM (Duh, 2005).
This dependency can be use-ful, for example, if opt?1 is new, in which case opthas a higher probability of being none or old.
If1171post?1 is a verb or preposition, opt has more proba-bility of being old or new.One may wonder why opt generates post, andnot the other way around.
This model only roughlymodels the process of (new and old) entity genera-tion, and either direction of causality might be con-sistent with a model of human entity generation,but this direction of causality is chosen to representthe effect of semantics (referents) generating syn-tax (POS tags).
In addition, this is a joint model inwhich POS tagging and coreference resolution areintegrated together, so the best combination of thosehidden states will be computed in either case.2.3 Coreference FeaturesCoreference features for this model refer to featuresthat may help to identify co-referring entities.In this paper, they mainly include index (I),named entity type (E), number (N) and gender (G).The index feature represents the order that a men-tion was encountered relative to the other mentionsin the buffer.
The latter three features are wellknown and described elsewhere, and are not them-selves intended as the contribution of this work.
Thenovel aspect of this part of the model is the fact thatthe features are carried forward, updated after ev-ery word, and essentially act as a discourse model.The features are just a shorthand way of represent-ing some well known essential aspects of a referent(as pertains to anaphora resolution) in a discoursemodel.Features ValuesI positive integers from 1. .
.nG male, female, neutral, unknownN singular, plural, unknownE person, location, organization,GPE, vehicle,company, facilityTable 1: Coreference features stored with each mention.Unlike discriminative approaches, generativemodels like the FHMM described here do not haveaccess to all observations at once.
This model mustthen have a mechanism for jointly considering pro-nouns in tandem with previous mentions, as well asthe features of those mentions that might be used tofind matches between pronouns and antecedents.Further, higher order HMMs may contain moreaccurate information about observation states.
Thisis especially true for coreference resolution becausepronouns often refer back to mentions that are faraway from the present state.
In this case, we wouldneed to know information about mentions which areat least two mentions before the present one.
Inthis sense, a higher order HMM may seem idealfor coreference resolution.
However, higher orderHMMs will quickly become intractable as the orderincreases.In order to overcome these limitations, two strate-gies which have been discussed in the last sectionare taken: First, a switching variable called OP isdesigned (as discussed in last section); second, amemory of recently mentioned entities is maintainedto store features of mentions and pass them forwardincrementally.OP is intended to model the decision to use thecurrent word to introduce a new referent (new), referto an antecedent (old), or neither (none).
The entitybuffer is intended to model the set of ?activated?
en-tities in the discourse ?
those which could plausiblybe referred to with a pronoun.
These designs allowsimilar benefits as longer dependencies of higher-order HMMs but avoid the problem of intractability.The number of mentions maintained must be limitedin order for the model to be tractable.
Fortunately,human short term memory faces effectively similarlimitations and thus pronouns usually refer back tomentions not very far away.Even so, the impact of the size of the buffer ondecoding time may be a concern.
Since the buffer ofour system will carry forward a few previous groupsof coreference features plus op and pos, the compu-tational complexity will be exorbitantly high if wekeep high beam size and meanwhile if each featureinteracts with others.
Luckily, we have successfullyreduced the intractability to a workable system inboth speed and space with following methods.
First,we estimate the size of buffer with a simple countof average distances between pronouns and their an-tecedents in the corpus.
It is found that about six isenough for covering 99.2% of all pronouns.Secondly, the coreference features we have usedhave the nice property of being independent fromone another.
One might expect English non-personentities to almost always have neutral gender, and1172thus be modeled as follows:P(et, gt | et?1, gt?1) = P(gt | gt?1, et) ?
P(et | et?1)(6)However, a few considerations made us reconsider.First, exceptions are found in the corpus.
Personalpronouns such as she or he are used to refer to coun-try, regions, states or organizations.
Second, existingmodel files made by Bergsma (2005) include a largenumber of non-neutral gender information for non-person words.
We employ these files for acquiringgender information of unknown words.
If we useEquation 6, sparsity and complexity will increase.Further, preliminary experiments have shown mod-els using an independence assumption between gen-der and personhood work better.
Thus, we treat eachcoreference feature as an independent event.
Hence,we can safely split coreference features into sepa-rate parts.
This way dramatically reduces the modelcomplexity.
Thirdly, our HMM decoding uses theViterbi algorithm with A-star beam search.The probability of the new state of the coreferencetable P(crt | crt?1, opt) is defined to be the productof probabilities of the individual feature transitions.P(crt | crt?1, opt) = P(it | it?1, opt)?P(et | et?1, it, opt)?P(gt | gt?1, it, opt)?P(nt |nt?1, it, opt)(7)This supposes that the features are conditionally in-dependent of each other given the index variable, theoperator and previous instance.
Each feature onlydepends on the operator and the corresponding fea-ture at the previous state, with that set of featuresre-ordered as specified by the index model.2.4 Feature PassingEquation 7 is correct and complete, but in fact theswitching variable for operation type results in threedifferent cases which simplifies the calculation ofthe transition probabilities for the coreference fea-ture table.Note the following observations about corefer-ence features: it only needs a probabilistic modelwhen opt is old ?
in other words, only when themodel must choose between several antecedents tore-refer to.
gt, et and nt are deterministic exceptwhen opt is new, when gender, entity type, and num-ber information must be generated for the new entitybeing introduced.When opt is none, all coreference variables (en-tity features) will be copied over from the previoustime step to the current time step, and the probabil-ity of this transition is 1.0.
When opt is new, it ischanged deterministically by adding the new entityto the first position in the list and moving every otherentity down one position.
If the list of entities isfull, the least recently mentioned entity will be dis-carded.
The values for the top of the feature listsgt, et, and nt will then be generated from feature-specific probability distributions estimated from thetraining data.
When opt is old, it will probabilisti-cally select a value 1 .
.
.
n, for an entity list contain-ing n items.
The selected value will deterministi-cally order the gt, nt and et lists.
This distributionis also estimated from training data, and takes intoaccount recency of mention.
The shape of this dis-tribution varies slightly depending on list size andnoise in the training data, but in general the probabil-ity of a mention being selected is directly correlatedto how recently it was mentioned.With this understanding, coreference table tran-sition probabilities can be written in terms of onlytheir non-deterministic substate distributions:P(crt | crt?1, old) = Pold(it | it?1)?Preorder(et | et?1, it)?Preorder(gt | gt?1, it)?Preorder(nt |nt?1, it)(8)where the old model probabilistically selects the an-tecedent and moves it to the top of the list as de-scribed above, thus deciding how the reordering willtake place.
The reorder model actually implementsthe list reordering for each independent feature bymoving the feature value corresponding to the se-lected entity in the index model to the top of thatfeature?s list.
The overall effect is simply the prob-abilistic reordering of entities in a list, where eachentity is defined as a label and a set of features.P(crt | crt?1, new) = Pnew(it | it?1)?Pnew(gt | gt?1)?Pnew(nt |nt?1)?Pnew(et | et?1)(9)where the new model probabilistically generates a1173feature value based on the training data and puts itat the top of the list, moves every other entity downone position in the list, and removes the final item ifthe list is already full.
Each entity in i takes a valuefrom 1 to n for a list of size n. Each g can be one offour values ?
male, female, neuter and unknown; none of three values ?
plural, singular and unknownand e around eight values.Note that post is used in both hidden states andobservation states.
While it is not considered acoreference feature as such, it can still play an im-portant role in the resolving process.
Basically, thesystem tags parts of speech incrementally while si-multaneously resolving pronoun anaphora.
Mean-while, post?1 and opt?1 will jointly generate opt.This point has been discussed in Section 2.2.Importantly, the pos model can help to imple-ment binding principles (Chomsky, 1981).
It isapplied when opt is old.
In training, pronounsare sub-categorised into personal pronouns, reflex-ive and other-pronoun.
We then define a vari-able loct whose value is how far back in the listof antecedents the current hypothesis must havegone to arrive at the current value of it.
If wehave the syntax annotations or parsed trees, then,the part of speech model can be defined whenopt is old as Pbinding(post | loct, sloct).
For ex-ample, if post ?
ref lexive, P(post | loct, sloct)where loct has smaller values (implying closer men-tions to post) and sloct = subject should havehigher values since reflexive pronouns always re-fer back to subjects within its governing domains.This was what (Haghighi and Klein, 2009) did andwe did this in training with the REUTERS cor-pus (Hasler et al, 2006) in which syntactic rolesare annotated.
We finally switched to the ACEcorpus for the purpose of comparison with otherwork.
In the ACE corpus, no syntactic roles areannotated.
We did use the Stanford parser to ex-tract syntactic roles from the ACE corpus.
Butthe result is largely affected by the parsing accu-racy.
Again, for a fair comparison, we extract simi-lar features to Denis and Baldridge (2007), which isthe model we mainly compare with.
They approx-imate syntactic contexts with POS tags surround-ing the pronoun.
Inspired by this idea, we success-fully represent binding features with POS tags be-fore anaphors.
Instead of using P(post | loct, sloct),we train P(post | loct, posloct) which can playthe role of binding.
For example, suppose thebuffer size is 6 and loct = 5, posloct = noun.Then, P(post = ref lexive | loct, posloct) is usu-ally higher than P(post = pronoun | loct, posloct),since the reflexive has a higher probability of refer-ring back to the noun located in position 5 than thepronoun.In future work expanding to coreference resolu-tion between any noun phrases we intend to inte-grate syntax into this framework as a joint model ofcoreference resolution and parsing.3 Observation ModelThe observation model that generates an observedstate is defined as Equation 5.
To expand that equa-tion in detail, the observation state, the word, de-pends on its part of speech and its coreference fea-tures as well.
Since FHMMs are generative, we cansay part of speech and coreference features generatethe word.In actual implementation, the observed model willbe very sparse, since crt will be split into more vari-ables according to how many coreference features itis composed of.
In order to avoid the sparsity, wetransform the equation with Bayes?
law as follows.P?O(ot |ht) =P (ot) ?
P(ht | ot)?o?
P (o?
)P(ht | o?
)(10)= P (ot) ?
P(post, crt | ot)?o?
P (o?
)P(post, crt | o?
)(11)We define pos and cr to be independent of eachother, so we can further split the above equation as:P?O(ot |ht)def= P (ot) ?
P(post | ot) ?
P(crt | ot)?o?
P (o?)
?
P(post | o?)
?
P(crt | o?
)(12)where P(crt | ot) = P(gt | ot)P(nt | ot)P(et | ot) andP(crt | o?)
= P(gt | o?
)P(nt | o?
)P(et | o?
).This change transforms the FHMM to a hybridFHMM since the observation model no longer gen-erates the data.
Instead, the observation model gen-erates hidden states, which is more a combinationof discriminative and generative approaches.
Thisway facilitates building likelihood model files of fea-tures for given mentions from the training data.
The1174hidden state transition model represents prior proba-bilities of coreference features associated with eachwhile this observation model factors in the probabil-ity given a pronoun.3.1 Unknown Words ProcessingIf an observed word was not seen in training, thedistribution of its part of speech, gender, number andentity type will be unknown.
In this case, a specialunknown words model is used.The part of speech of unknown wordsP(post |wt = unkword) is estimated using adecision tree model.
This decision tree is builtby splitting letters in words from the end of theword backward to its beginning.
A POS tag isassigned to the word after comparisons betweenthe morphological features of words trained fromthe corpus and the strings concatenated from thetree leaves are made.
This method is about asaccurate as the approach described by Klein andManning (2003).Next, a similar model is set up for estimatingP(nt |wt = unkword).
Most English words haveregular plural forms, and even irregular words havetheir patterns.
Therefore, the morphological featuresof English words can often be used to determinewhether a word is singular or plural.Gender is irregular in English, so model-basedpredictions are problematic.
Instead, we followBergsma and Lin (2005) to get the distribution ofgender from their gender/number data and then pre-dict the gender for unknown words.4 Evaluation and Discussion4.1 Experimental SetupIn this research, we used the ACE corpus (Phase 2) 1for evaluation.
The development of this corpus in-volved two stages.
The first stage is called EDT (en-tity detection and tracking) while the second stageis called RDC (relation detection and characteriza-tion).
All markables have named entity types suchas FACILITY, GPE (geopolitical entity), PERSON,LOCATION, ORGANIZATION, PERSON, VEHI-CLE and WEAPONS, which were annotated in thefirst stage.
In the second stage, relations between1See http://projects.ldc.upenn.edu/ace/annotation/previous/ for details on the corpus.named entities were annotated.
This corpus includethree parts, composed of different genres: newspa-per texts (NPAPER), newswire texts (NWIRE) andbroadcasted news (BNEWS).
Each of these is splitinto a train part and a devtest part.
For the trainpart, there are 76, 130 and 217 articles in NPA-PER, NWIRE and BNEWS respectively while forthe test part, there are 17, 29 and 51 articles respec-tively.
Though the number of articles are quite dif-ferent for three genres, the total number of words arealmost the same.
Namely, the length of NPAPERis much longer than BNEWS (about 1200 words,800 word and 500 words respectively for three gen-res).
The longer articles involve longer coreferencechains.
Following the common practice, we usedthe devtest material only for testing.
Progress duringthe development phase was estimated only by usingcross-validation on the training set for the BNEWSsection.
In order to make comparisons with publica-tions which used the same corpus, we make effortsto set up identical conditions for our experiments.The main point of comparison is Denis andBaldridge (2007), which was similar in that it de-scribed a new type of coreference resolver usingsimple features.Therefore, similar to their practice, we use allforms of personal and possessive pronouns that wereannotated as ACE ?markables?.
Namely, pronounsassociated with named entity types could be used inthis system.
In experiments, we also used true ACEmentions as they did.
This means that pleonasticsand references to eventualities or to non-ACE enti-ties are not included in our experiments either.
Inall, 7263 referential pronouns in training data setand 1866 in testing data set are found in all threegenres.
They have results of three different systems:SCC (single candidate classifier), TCC (twin candi-date classifier) and RK (ranking).
Besides the threeand our own system, we also report results of em-Pronouns, which is an unsupervised system basedon a recently published paper (Charniak and Elsner,2009).
We select this unsupervised system for tworeasons.
Firstly, emPronouns is a publicly availablesystem with high accuracy in pronoun resolution.Secondly, it is necessary for us to demonstrate oursystem has strong empirical superiority over unsu-pervised ones.
In testing, we also used the OPNLPNamed Entity Recognizer to tag the test corpus.1175During training, besides coreference annotationitself, the part of speech, dependencies betweenwords and named entities, gender, number and indexare extracted using relative frequency estimation totrain models for the coreference resolution system.Inputs for testing are the plain text and the trainedmodel files.
The entity buffer used in these exper-iments kept track of only the six most recent men-tions.
The result of this process is an annotationof the headword of every noun phrase denoting itas a mention.
In addition, this system does notdo anaphoricity detection, so the antecedent oper-ation for non-anaphora pronoun it is set to be none.Finally, the system does not yet model cataphora,about 10 cataphoric pronouns in the testing datawhich are all counted as wrong.4.2 ResultsThe performance was evaluated using the ratio ofthe number of correctly resolved anaphors over thenumber of all anaphors as a success metrics.
All thestandards are consistent with those defined in Char-niak and Elsner (2009).During development, several preliminary experi-ments explored the effects of starting from a simplebaseline and adding more features.
The BNEWScorpus was employed in these development exper-iments.
The baseline only includes part of speechtags, the index feature and and syntactic roles.
Syn-tactic roles are extracted from the parsing resultswith Stanford parser.
The success rate of this base-line configuration is 0.48.
This low accuracy is par-tially due to the errors of automatic parsing.
Withgender and number features added, the performancejumped to 0.65.
This shows that number and gen-der agreements play an important role in pronounanaphora resolution.
For a more standard compari-son to other work, subsequent tests were performedon the gold standard ACE corpus (using the modelas described with named entity features instead ofsyntactic role features).
As shown in Denis andBaldridge (2007), they employ all features we useexcept syntactic roles.
In these experiments, the sys-tem got better results as shown in Table 2.The result of the first one is obtained by runningthe publicly available system emPronouns2.
It is a2the available system in fact only includes the testing part.Thus, it may be unfair to compare emPronouns this way withSystem BNEWS NPAPER NWIREemPronouns 58.5 64.5 60.6SCC 62.2 70.7 68.3TCC 68.6 74.7 71.1RK 72.9 76.4 72.4FHMM 74.9 79.4 74.5Table 2: Accuracy scores for emPronouns, the single-candidate classifier (SCC), the twin-candidate classifier(TCC), the ranker and FHMMhigh-accuracy unsupervised system which reportedthe best result in Charniak and Elsner (2009).The results of the other three systems are thosereported by Denis and Baldridge (2007).
As Table 2shows, the FHMM system gets the highest averageresults.The emPronouns system got the lowest resultspartially due to the reason that we only directlyrun the existing system with its existing model fileswithout retraining.
But the gap between its resultsand results of our system is large.
Thus, we maystill say that our system probably can do a better jobeven if we train new models files for emPronounswith ACE corpus.With almost exactly identical settings, why doesour FHMM system get the highest average results?The convincing reason is that FHMM is strongly in-fluenced by the sequential dependencies.
The rank-ing approach ranks a set of mentions using a set offeatures, and it also maintains the discourse model,but it is not processing sequentially.
The FHMMsystem always maintain a set of mentions as wellas a first-order dependencies between part of speechand operator.
Therefore, context can be more fullytaken into consideration.
This is the main reason thatthe FHMM approach achieved better results than theranking approach.From the result, one point we may notice is thatNPAPER usually obtains higher results than bothBNEWS and NWIRE for all systems while BNEWSlower than other two genres.
In last section, wemention that articles in NPAPER are longer thanother genres and also have denser coreference chainswhile articles in BENEWS are shorter and havesparer chains.
Then, it is not hard to understandwhy results of NPAPER are better while those ofother systems.1176BNEWS are poorer.In Denis and Baldridge (2007), they also reportednew results with a window of 10 sentences for RKmodel.
All three genres obtained higher results thanthose when with shorter ones.
They are 73.0, 77.6and 75.0 for BNEWS,NPAPER and NWIRE respec-tively.
We can see that except the one for NWIRE,the results are still poorer than our system.
ForNWIRE, the RK model got 0.5 higher.
The averageof the RK is 75.2 while that of the FHMM system is76.3, which is still the best.Since the emPronoun system can output sample-level results, it is possible to do a paired Student?st-test.
That test shows that the improvement of oursystem on all three genres is statistically significant(p < 0.001).
Unfortunately, the other systems onlyreport overall results so the same comparison wasnot so straightforward.4.3 Error AnalysisAfter running the system on these documents, wechecked which pronouns fail to catch their an-tecedents.
There are a few general reasons for er-rors.First, pronouns which have antecedents very faraway cannot be caught.
Long-distance anaphora res-olution may pose a problem since the buffer sizecannot be too long considering the complexity oftracking a large number of mentions through time.During development, estimation of an acceptablesize was attempted using the training data.
It wasfound that a mention distance of fourteen would ac-count for every case found in this corpus, thoughmost cases fall well short of that distance.
Futurework will explore optimizations that will allow forlarger or variable buffer sizes so that longer distanceanaphora can be detected.A second source of error is simple misjudgmentswhen more than one candidate is waiting for selec-tion.
A simple case is that the system fails to distin-guish plural personal nouns and non-personal nounsif both candidates are plural.
This is not a problemfor singular pronouns since gender features can tellwhether pronouns are personal or not.
Plural nounsin English do not have such distinctions, however.Consequently, demands and Israelis have the sameprobability of being selected as the antecedents forthey, all else being equal.
If demands is closer tothey, demands will be selected as the antecedent.This may lead to the wrong choice if they in factrefers to Israelis.
This may require better measuresof referent salience than the ?least recently used?heuristic currently implemented.Third, these results also show difficulty resolv-ing coordinate noun phrases due to the simplisticrepresentation of noun phrases in the input.
Con-sider this sentence: President Barack Obama andhis wife Michelle Obama visited China last week.They had a meeting with President Hu in Beijing.In this example, the pronoun they corefers with thenoun phrase President Barack Obama and his wifeMichelle Obama.
The present model cannot repre-sent both the larger noun phrase and its containednoun phrases.
Since the noun phrase is a coordinateone that includes both noun phrases, the model can-not find a head word to represent it.Finally, while the coreference feature annotationsof the ACE are valuable for learning feature mod-els, the model training may still give some mislead-ing results.
This is brought about by missing fea-tures in the training corpus and by the data sparsity.We solved the problem with add-one smoothing anddeleted interpolation in training models besides thetransformation in the generation order of the obser-vation model.5 Conclusion and Future WorkThis paper has presented a pronoun anaphora resolu-tion system based on FHMMs.
This generative sys-tem incrementally resolves pronoun anaphora withan entity buffer carrying forward mention features.The system performs well and outperforms otheravailable models.
This shows that FHMMs andother time-series models may be a valuable modelto resolve anaphora.AcknowledgmentsWe would like to thank the authors and maintainersof ranker models and emPronouns.
We also wouldlike to thank the three anonymous reviewers.
Thefinal version is revised based on their valuable com-ments.
Thanks are extended to Shane Bergsma, whoprovided us the gender and number data distribution.In addition, Professor Jeanette Gundel and our lab-mate Stephen Wu also gave us support in paper edit-ing and in theoretical discussion.1177ReferencesS Bergsma.
2005.
Automatic acquisition of genderinformation for anaphora resolution.
page 342353.Springer.Eugene Charniak and Micha Elsner.
2009.
Em worksfor pronoun anaphora resolution.
In Proceedings ofthe Conference of the European Chapter of the As-sociation for Computational Linguistics (EACL-09),Athens, Greece.Noam Chomsky.
1981.
Lectures on government andbinding.
Foris, Dordercht.H.H.
Clark and CJ Sengul.
1979.
In search of refer-ents for nouns and pronouns.
Memory & Cognition,7(1):35?41.P.
Denis and J. Baldridge.
2007.
A ranking approach topronoun resolution.
In Proc.
IJCAI.Kevin Duh.
2005.
Jointly labeling multiple sequences:a factorial HMM approach.
In ACL ?05: Proceedingsof the ACL Student Research Workshop, pages 19?24,Ann Arbor, Michigan.Zoubin Ghahramani and Michael I. Jordan.
1997.
Facto-rial hidden markov models.
Machine Learning, 29:1?31.A.
Haghighi and D. Klein.
2007.
Unsupervised coref-erence resolution in a nonparametric bayesian model.In Proceedings of the 45th annual meeting on Associ-ation for Computational Linguistics, page 848.A.
Haghighi and D. Klein.
2009.
Simple coreferenceresolution with rich syntactic and semantic features.In Proceedings of the 2009 Conference on EmpiricalMethods in Natural Language Processing: Volume 3-Volume 3, pages 1152?1161.
Association for Compu-tational Linguistics.A.
Haghighi and D. Klein.
2010.
Coreference resolu-tion in a modular, entity-centered model.
In HumanLanguage Technologies: The 2010 Annual Conferenceof the North American Chapter of the Association forComputational Linguistics, pages 385?393.
Associa-tion for Computational Linguistics.L.
Hasler, C. Orasan, and K. Naumann.
2006.
NPsfor events: Experiments in coreference annotation.
InProceedings of the 5th edition of the InternationalConference on Language Resources and Evaluation(LREC2006), pages 1167?1172.
Citeseer.Dan Klein and Christopher D. Manning.
2003.
Accu-rate unlexicalized parsing.
In Proceedings of the 41stAnnual Meeting of the Association for ComputationalLinguistics, pages 423?430, Sapporo, Japan.X Luo.
2005.
On coreference resolution performancemetrics.
pages 25?32.
Association for ComputationalLinguistics Morristown, NJ, USA.A.
McCallum and B. Wellner.
2003.
Toward condi-tional models of identity uncertainty with applicationto proper noun coreference.
In IJCAI Workshop on In-formation Integration on the Web.
Citeseer.David McClosky, Eugene Charniak, and Mark Johnson.2008.
BLLIP North American News Text, Complete.Linguistic Data Consortium.
LDC2008T13.T.S.
Morton.
2000.
Coreference for NLP applications.In Proceedings of the 38th Annual Meeting on Associ-ation for Computational Linguistics, pages 173?180.Association for Computational Linguistics.V.
Ng.
2008.
Unsupervised models for coreference reso-lution.
In Proceedings of the Conference on EmpiricalMethods in Natural Language Processing, pages 640?649.
Association for Computational Linguistics.US NIST.
2003.
The ACE 2003 Evaluation Plan.
US Na-tional Institute for Standards and Technology (NIST),Gaithersburg, MD.
[online, pages 2003?08.L.
Qiu, M.Y.
Kan, and T.S.
Chua.
2004.
A public ref-erence implementation of the rap anaphora resolutionalgorithm.
Arxiv preprint cs/0406031.X.
Yang, J. Su, G. Zhou, and C.L.
Tan.
2004.
Im-proving pronoun resolution by incorporating corefer-ential information of candidates.
In Proceedings ofthe 42nd Annual Meeting on Association for Compu-tational Linguistics, page 127.
Association for Com-putational Linguistics.1178
