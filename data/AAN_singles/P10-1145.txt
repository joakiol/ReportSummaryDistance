Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 1433?1442,Uppsala, Sweden, 11-16 July 2010. c?2010 Association for Computational LinguisticsConstituency to Dependency Translation with ForestsHaitao Mi and Qun LiuKey Laboratory of Intelligent Information ProcessingInstitute of Computing TechnologyChinese Academy of SciencesP.O.
Box 2704, Beijing 100190, China{htmi,liuqun}@ict.ac.cnAbstractTree-to-string systems (and their forest-based extensions) have gained steady pop-ularity thanks to their simplicity and effi-ciency, but there is a major limitation: theyare unable to guarantee the grammatical-ity of the output, which is explicitly mod-eled in string-to-tree systems via target-side syntax.
We thus propose to com-bine the advantages of both, and presenta novel constituency-to-dependency trans-lation model, which uses constituencyforests on the source side to direct thetranslation, and dependency trees on thetarget side (as a language model) to en-sure grammaticality.
Medium-scale exper-iments show an absolute and statisticallysignificant improvement of +0.7 BLEUpoints over a state-of-the-art forest-basedtree-to-string system even with fewerrules.
This is also the first time that a tree-to-tree model can surpass tree-to-stringcounterparts.1 IntroductionLinguistically syntax-based statistical machinetranslation models have made promising progressin recent years.
By incorporating the syntactic an-notations of parse trees from both or either side(s)of the bitext, they are believed better than phrase-based counterparts in reorderings.
Depending onthe type of input, these models can be broadly di-vided into two categories (see Table 1): the string-based systems whose input is a string to be simul-taneously parsed and translated by a synchronousgrammar, and the tree-based systems whose inputis already a parse tree to be directly converted intoa target tree or string.
When we also take into ac-count the type of output (tree or string), the tree-based systems can be divided into tree-to-stringand tree-to-tree efforts.tree on examples (partial) fast gram.
BLEUsource Liu06, Huang06 + - +target Galley06, Shen08 - + +both Ding05, Liu09 + + -both our work + + +Table 1: A classification and comparison of lin-guistically syntax-based SMT systems, wheregram.
denotes grammaticality of the output.On one hand, tree-to-string systems (Liu et al,2006; Huang et al, 2006) have gained significantpopularity, especially after incorporating packedforests (Mi et al, 2008; Mi and Huang, 2008; Liuet al, 2009; Zhang et al, 2009).
Compared withtheir string-based counterparts, tree-based systemsare much faster in decoding (linear time vs. cu-bic time, see (Huang et al, 2006)), do not re-quire a binary-branching grammar as in string-based models (Zhang et al, 2006; Huang et al,2009), and can have separate grammars for pars-ing and translation (Huang et al, 2006).
However,they have a major limitation that they do not have aprincipled mechanism to guarantee grammatical-ity on the target side, since there is no linguistictree structure of the output.On the other hand, string-to-tree systems ex-plicitly model the grammaticality of the outputby using target syntactic trees.
Both string-to-constituency system (e.g., (Galley et al, 2006;Marcu et al, 2006)) and string-to-dependencymodel (Shen et al, 2008) have achieved signif-icant improvements over the state-of-the-art for-mally syntax-based system Hiero (Chiang, 2007).However, those systems also have some limita-tions that they run slowly (in cubic time) (Huanget al, 2006), and do not utilize the useful syntacticinformation on the source side.We thus combine the advantages of both tree-to-string and string-to-tree approaches, and propose1433a novel constituency-to-dependency model, whichuses constituency forests on the source side to di-rect translation, and dependency trees on the tar-get side to guarantee grammaticality of the out-put.
In contrast to conventional tree-to-tree ap-proaches (Ding and Palmer, 2005; Quirk et al,2005; Xiong et al, 2007; Zhang et al, 2007;Liu et al, 2009), which only make use of a sin-gle type of trees, our model is able to combinetwo types of trees, outperforming both phrase-based and tree-to-string systems.
Current tree-to-tree models (Xiong et al, 2007; Zhang et al, 2007;Liu et al, 2009) still have not outperformed thephrase-based system Moses (Koehn et al, 2007)significantly even with the help of forests.1Our new constituency-to-dependency model(Section 2) extracts rules from word-aligned pairsof source constituency forests and target depen-dency trees (Section 3), and translates source con-stituency forests into target dependency trees witha set of features (Section 4).
Medium data exper-iments (Section 5) show a statistically significantimprovement of +0.7 BLEU points over a state-of-the-art forest-based tree-to-string system evenwith less translation rules, this is also the first timethat a tree-to-tree model can surpass tree-to-stringcounterparts.2 ModelFigure 1 shows a word-aligned source con-stituency forest Fc and target dependency tree De,our constituency to dependency translation modelcan be formalized as:P(Fc, De) =?Cc?FcP(Cc, De)=?Cc?Fc?o?OP(O)=?Cc?Fc?o?O?r?oP(r),(1)where Cc is a constituency tree in Fc, o is a deriva-tion that translates Cc to De, O is the set of deriva-tion, r is a constituency to dependency translationrule.1According to the reports of Liu et al (2009), their forest-based constituency-to-constituency system achieves a com-parable performance against Moses (Koehn et al, 2007), buta significant improvement of +3.6 BLEU points over the 1-best tree-based constituency-to-constituency system.2.1 Constituency Forests on the Source SideA constituency forest (in Figure 1 left) is a com-pact representation of all the derivations (i.e.,parse trees) for a given sentence under a context-free grammar (Billot and Lang, 1989).More formally, following Huang (2008), sucha constituency forest is a pair Fc = Gf =?V f , Hf ?, where V f is the set of nodes, and Hfthe set of hyperedges.
For a given source sen-tence c1:m = c1 .
.
.
cm, each node vf ?
V f isin the form of X i,j , which denotes the recognitionof nonterminal X spanning the substring from po-sitions i through j (that is, ci+1 .
.
.
cj).
Each hy-peredge hf ?
Hf is a pair ?tails(hf ), head(hf )?,where head(hf ) ?
V f is the consequent node inthe deductive step, and tails(hf ) ?
(V f )?
is thelist of antecedent nodes.
For example, the hyper-edge hf0 in Figure 1 for deduction (*)NPB0,1 CC1,2 NPB2,3NP0,3 , (*)is notated:?
(NPB0,1, CC1,2, NPB2,3), NP0,3?.wherehead(hf0) = {NP0,3},andtails(hf0) = {NPB0,1,CC1,2,NPB2,3}.The solid line in Figure 1 shows the best parsetree, while the dashed one shows the second besttree.
Note that common sub-derivations like thosefor the verb VPB3,5 are shared, which allows theforest to represent exponentially many parses in acompact structure.We also denote IN (vf ) to be the set of in-coming hyperedges of node vf , which representsthe different ways of deriving vf .
Take node IP0,5in Figure 1 for example, IN (IP0,5) = {hf1 , hf2}.There is also a distinguished root node TOP ineach forest, denoting the goal item in parsing,which is simply S0,m where S is the start symboland m is the sentence length.2.2 Dependency Trees on the Target SideA dependency tree for a sentence represents eachword and its syntactic dependents through directedarcs, as shown in the following examples.
Themain advantage of a dependency tree is that it canexplore the long distance dependency.14341: talkblank a blan blan2: heldBush bla blk talka blwithb SharonWe use the lexicon dependency grammar (Hell-wig, 2006) to express a projective dependencytree.
Take the dependency trees above for exam-ple, they will be expressed:1: ( a ) talk2: ( Bush ) held ( ( a ) talk ) ( with ( Sharon ) )where the lexicons in brackets represent the de-pendencies, while the lexicon out the brackets isthe head.More formally, a dependency tree is also a pairDe = Gd = ?V d, Hd?.
For a given target sen-tence e1:n = e1 .
.
.
en, each node vd ?
V d isa word ei (1 6 i 6 n), each hyperedge hd ?Hd is a directed arc ?vdi , vdj ?
from node vdi toits head node vdj .
Following the formalization ofthe constituency forest scenario, we denote a pair?tails(hd), head(hd)?
to be a hyperedge hd, wherehead(hd) is the head node, tails(hd) is the nodewhere hd leaves from.We also denote Ll(vd) and Lr(vd) to be the leftand right children sequence of node vd from thenearest to the farthest respectively.
Take the nodevd2 = ?held?
for example:Ll(vd2) ={Bush},Lr(vd2) ={talk, with}.2.3 HypergraphActually, both the constituency forest and the de-pendency tree can be formalized as a hypergraphG, a pair ?V,H?.
We use Gf and Gd to distinguishthem.
For simplicity, we also use Fc and De to de-note a constituency forest and a dependency treerespectively.
Specifically, the size of tails(hd) ofa hyperedge hd in a dependency tree is a constantone.IPNPx1:NPB CCyu?x2:NPBx3:VPB?
(x1) x3 (with (x2))Figure 2: Example of the rule r1.
The Chinese con-junction yu?
?and?
is translated into English prepo-sition ?with?.3 Rule ExtractionWe extract constituency to dependency rules fromword-aligned source constituency forest and targetdependency tree pairs (Figure 1).
We mainly ex-tend the tree-to-string rule extraction algorithm ofMi and Huang (2008) to our scenario.
In this sec-tion, we first formalize the constituency to stringtranslation rule (Section 3.1).
Then we presentthe restrictions for dependency structures as wellformed fragments (Section 3.2).
Finally, we de-scribe our rule extraction algorithm (Section 3.3),fractional counts computation and probabilities es-timation (Section 3.4).3.1 Constituency to Dependency RuleMore formally, a constituency to de-pendency translation rule r is a tuple?lhs(r), rhs(r), ?
(r)?, where lhs(r) is thesource side tree fragment, whose internal nodesare labeled by nonterminal symbols (like NP andVP), and whose frontier nodes are labeled bysource language words ci (like ?yu??)
or variablesfrom a set X = {x1, x2, .
.
.
}; rhs(r) is expressedin the target language dependency structure withwords ej (like ?with?)
and variables from the setX ; and ?
(r) is a mapping from X to nontermi-nals.
Each variable xi ?
X occurs exactly once inlhs(r) and exactly once in rhs(r).
For example,the rule r1 in Figure 2,lhs(r1) = IP(NP(x1 CC(yu?)
x2) x3),rhs(r1) = (x1) x3 (with (x2)),?
(r1) = {x1 7?
NPB, x2 7?
NPB, x3 7?
VPB}.3.2 Well Formed Dependency FragmentFollowing Shen et al (2008), we also restrictrhs(r) to be well formed dependency fragment.The main difference between us is that we usemore flexible restrictions.
Given a dependency1435IP0,5?
(Bush) ..
Sharon))?hf1NP0,3?
(Bush) unionsq (with (Sharon))?NPB0,1?Bush?Bu`sh?
?hf0CC1,2?with?yu?VP1,5?held .. Sharon))?PP1,3?with (Sharon)?P1,2?with?NPB2,3?Sharon?Sha?lo?ngVPB3,5?held ((a) talk)?VV3,4?held ((a)*)?ju?x?
?ngleNPB4,5?talk?hu?`ta?nhf2Minimal rules extractedIP (NP(x1:NPB x2:CC x3:NPB) x4:VPB)?
(x1) x4 (x2 (x3) )IP (x1:NPB x2:VP)?
(x1) x2VP (x1:PP x2:VPB)?
x2 (x1)PP (x1:P x2:NPB)?
x1 (x2)VPB (VV(ju?x?
?ngle)) x1:NPB)?
held ((a) x1)NPB (Bu`sh??)?
BushNPB (hu?`ta?n)?
talkCC (yu?)?
withP (yu?)?
withNPB (Sha?lo?ng)?
Sharon( Bush ) held ( ( a ) talk ) ( with ( Sharon ) )Figure 1: Forest-based constituency to dependency rule extraction.fragment di:j composed by the words from i to j,two kinds of well formed structures are defined asfollows:Fixed on one node vdone, fixed for short, if itmeets the following conditions:?
the head of vdone is out of [i, j], i.e.
: ?hd, iftails(hd) = vdone ?
head(hd) /?
ei:j .?
the heads of other nodes except vdone are in[i, j], i.e.
: ?k ?
[i, j] and vdk 6= vdone,?hd iftails(hd) = vdk ?
head(hd) ?
ei:j .Floating with multi nodes M , floating forshort, if it meets the following conditions:?
all nodes in M have a same head node,i.e.
: ?x /?
[i, j],?hd if tails(hd) ?
M ?head(hd) = vhx .?
the heads of other nodes not in M are in[i, j], i.e.
: ?k ?
[i, j] and vdk /?
M, ?hd iftails(hd) = vdk ?
head(hd) ?
ei:j .Take the ?
(Bush) held ((a) talk))(with (Sharon))?
for example: partial fixed examples are ?
(Bush)held ?
and ?
held ((a) talk)?
; while the partial float-ing examples are ?
(talk) (with (Sharon)) ?
and ?
((a) talk) (with (Sharon)) ?.
Please note that thefloating structure ?
(talk) (with (Sharon)) ?
can notbe allowed in Shen et al (2008)?s model.The dependency structure ?
held ((a))?
is not awell formed structure, since the head of word ?a?is out of scope of this structure.3.3 Rule Extraction AlgorithmThe algorithm shown in this Section is mainly ex-tended from the forest-based tree-to-string extrac-tion algorithm (Mi and Huang, 2008).
We extractrules from word-aligned source constituency for-est and target dependency tree pairs (see Figure 1)in three steps:(1) frontier set computation,(2) fragmentation,(3) composition.The frontier set (Galley et al, 2004) is the po-tential points to ?cut?
the forest and dependencytree pair into fragments, each of which will form aminimal rule (Galley et al, 2006).However, not every fragment can be used forrule extraction, since it may or may not respectto the restrictions, such as word alignments andwell formed dependency structures.
So we say afragment is extractable if it respects to all re-strictions.
The root node of every extractable treefragment corresponds to a faithful structure onthe target side, in which case there is a ?transla-tional equivalence?
between the subtree rooted atthe node and the corresponding target structure.For example, in Figure 1, every node in the forestis annotated with its corresponding English struc-ture.
The NP0,3 node maps to a non-contiguousstructure ?
(Bush) unionsq (with (Sharon))?, the VV3,4node maps to a contiguous but non-faithful struc-ture ?held ((a) *)?.1436Algorithm 1 Forest-based constituency to dependency rule extraction.Input: Source constituency forest Fc, target dependency tree De, and alignment aOutput: Minimal rule setR1: fs ?
FRONTIER(Fc, De, a) .
compute frontier set2: for each vf ?
fs do3: open ?
{?
?, {vf}?}
.
initial queue of growing fragments4: while open 6= ?
do5: ?hs, exps?
?
open.pop() .
extract a fragment6: if exps = ?
then .
nothing to expand?7: generate a rule r using fragment hs .
generate a rule8: R.append(r)9: else .
incomplete: further expand10: v?
?
exps .pop() .
a non-frontier node11: for each hf ?
IN (v?)
do12: newexps ?
exps ?
(tails(hf ) \ fs) .
expand13: open .append(?hs ?
{hf},newexps?
)Following Mi and Huang (2008), given a sourcetarget sentence pair ?c1:m, e1:n?
with an alignmenta, the span of node vf on source forest is the setof target words aligned to leaf nodes under vf :span(vf ) , {ei ?
e1:n | ?cj ?
yield(vf ), (cj , ei) ?
a}.where the yield(vf ) is all the leaf nodes un-der vf .
For each span(vf ), we also denotedep(vf ) to be its corresponding dependency struc-ture, which represents the dependency struc-ture of all the words in span(vf ).
Take thespan(PP1,3) ={with, Sharon} for example, thecorresponding dep(PP1,3) is ?with (Sharon)?.
Adep(vf ) is faithful structure to node vf if it meetsthe following restrictions:?
all words in span(vf ) form a continuous sub-string ei:j ,?
every word in span(vf ) is only aligned to leafnodes of vf , i.e.
: ?ei ?
span(vf ), (cj , ei) ?a?
cj ?
yield(vf ),?
dep(vf ) is a well formed dependency struc-ture.For example, node VV3,4 has a non-faithfulstructure (crossed out in Figure 1), since itsdep(VV3,4 = ?
held ((a) *)?
is not a well formedstructure, where the head of word ?a?
lies in theoutside of its words covered.
Nodes with faithfulstructure form the frontier set (shaded nodes inFigure 1) which serve as potential cut points forrule extraction.Given the frontier set, fragmentation step is to?cut?
the forest at all frontier nodes and formtree fragments, each of which forms a rule withvariables matching the frontier descendant nodes.For example, the forest in Figure 1 is cut into 10pieces, each of which corresponds to a minimalrule listed on the right.Our rule extraction algorithm is formalized inAlgorithm 1.
After we compute the frontier setfs (line 1).
We visit each frontier node vf ?
fson the source constituency forest Fc, and keep aqueue open of growing fragments rooted at vf .
Wekeep expanding incomplete fragments from open ,and extract a rule if a complete fragment is found(line 7).
Each fragment hs in open is associatedwith a list of expansion sites (exps in line 5) beingthe subset of leaf nodes of the current fragmentthat are not in the frontier set.
So each fragmentalong hyperedge h is associated withexps = tails(hf ) \ fs.A fragment is complete if its expansion sites isempty (line 6), otherwise we pop one expansionnode v?
to grow and spin-off new fragments byfollowing hyperedges of v?, adding new expansionsites (lines 11-13), until all active fragments arecomplete and open queue is empty (line 4).After we get al the minimal rules, we glue themtogether to form composed rules following Galleyet al (2006).
For example, the composed rule r1in Figure 2 is glued by the following two minimalrules:1437IP (NP(x1:NPB x2:CC x3:NPB) x4:VPB) r2?
(x1) x4 (x2 (x3) )CC (yu?)?
with r3where x2:CC in r2 is replaced with r3 accordingly.3.4 Fractional Counts and Rule ProbabilitiesFollowing Mi and Huang (2008), we penalize arule r by the posterior probability of the corre-sponding constituent tree fragment lhs(r), whichcan be computed in an Inside-Outside fashion, be-ing the product of the outside probability of itsroot node, the inside probabilities of its leaf nodes,and the probabilities of hyperedges involved in thefragment.??
(lhs(r)) =?(root(r))?
?hf ?
lhs(r)P(hf )?
?vf ?
leaves(lhs(r))?
(vf )(2)where root(r) is the root of the rule r, ?
(v) and?
(v) are the outside and inside probabilities ofnode v, and leaves(lhs(r)) returns the leaf nodesof a tree fragment lhs(r).We use fractional counts to compute three con-ditional probabilities for each rule, which will beused in the next section:P(r | lhs(r)) = c(r)?r?:lhs(r?
)=lhs(r) c(r?
), (3)P(r | rhs(r)) = c(r)?r?:rhs(r?
)=rhs(r) c(r?
), (4)P(r | root(r)) = c(r)?r?:root(r?
)=root(r) c(r?).
(5)4 DecodingGiven a source forest Fc, the decoder searches forthe best derivation o?
among the set of all possiblederivations O, each of which forms a source sideconstituent tree Tc(o), a target side string e(o), anda target side dependency tree De(o):o?
= arg maxTc?Fc,o?O?1 log P(o | Tc)+?2 log Plm(e(o))+?3 log PDLMw(De(o))+?4 log PDLMp(De(o))+?5 log P(Tc(o))+?6ill(o) + ?7|o|+ ?8|e(o)|,(6)where the first two terms are translation and lan-guage model probabilities, e(o) is the target string(English sentence) for derivation o, the third andforth items are the dependency language modelprobabilities on the target side computed withwords and POS tags separately, De(o) is the targetdependency tree of o, the fifth one is the parsingprobability of the source side tree Tc(o) ?
Fc, theill(o) is the penalty for the number of ill-formeddependency structures in o, and the last two termsare derivation and translation length penalties, re-spectively.
The conditional probability P(o | Tc)is decomposes into the product of rule probabili-ties:P(o | Tc) =?r?oP(r), (7)where each P(r) is the product of five probabili-ties:P(r) =P(r | lhs(r))?9 ?
P(r | rhs(r))?10?
P(r | root(lhs(r)))?11?
Plex(lhs(r) | rhs(r))?12?
Plex(rhs(r) | lhs(r))?13 ,(8)where the first three are conditional probabilitiesbased on fractional counts of rules defined in Sec-tion 3.4, and the last two are lexical probabilities.When computing the lexical translation probabili-ties described in (Koehn et al, 2003), we only takeinto accout the terminals in a rule.
If there is noterminal, we set the lexical probability to 1.The decoding algorithm works in a bottom-upsearch fashion by traversing each node in forestFc.
We first use pattern-matching algorithm of Miet al (2008) to convert Fc into a translation for-est, each hyperedge of which is associated with aconstituency to dependency translation rule.
How-ever, pattern-matching failure2 at a node vf will2Pattern-matching failure at a node vf means there is notranslation rule can be matched at vf or no translation hyper-edge can be constructed at vf .1438cut the derivation path and lead to translation fail-ure.
To tackle this problem, we construct a pseudotranslation rule for each parse hyperedge hf ?IN (vf ) by mapping the CFG rule into a target de-pendency tree using the head rules of Magerman(1995).
Take the hyperedge hf0 in Figure1 for ex-ample, the corresponding pseudo translation ruleis:NP(x1:NPB x2:CC x3:NPB)?
(x1) (x2) x3,since the x3:NPB is the head word of the CFGrule: NP?
NPB CC NPB.After the translation forest is constructed, wetraverse each node in translation forest also inbottom-up fashion.
For each node, we use thecube pruning technique (Chiang, 2007; Huangand Chiang, 2007) to produce partial hypothesesand compute all the feature scores including thedependency language model score (Section 4.1).If all the nodes are visited, we trace back alongthe 1-best derivation at goal item S0,m and builda target side dependency tree.
For k-best searchafter getting 1-best derivation, we use the lazy Al-gorithm 3 of Huang and Chiang (2005) that worksbackwards from the root node, incrementally com-puting the second, third, through the kth best alter-natives.4.1 Dependency Language Model ComputingWe compute the score of a dependency languagemodel for a dependency tree De in the same wayproposed by Shen et al (2008).
For each nonter-minal node vdh = eh in De and its children se-quences Ll = el1 , el2 ...eli and Lr = er1 , er2 ...erj ,the probability of a trigram is computed as fol-lows:P(Ll, Lr | eh?)
= P(Ll | eh?)
?P(Lr | eh?
), (9)where the P(Ll | eh?)
is decomposed to be:P(Ll | eh?)
=P(ell | eh?)?
P(el2 | el1 , eh?)...?
P(eln | eln?1 , eln?2).
(10)We use the suffix ???
to distinguish the headword and child words in the dependency languagemodel.In order to alleviate the problem of data sparse,we also compute a dependency language modelfor POS tages over a dependency tree.
We storethe POS tag information on the target side for eachconstituency-to-dependency rule.
So we will alsogenerate a POS taged dependency tree simulta-neously at the decoding time.
We calculate thisdependency language model by simply replacingeach ei in equation 9 with its tag t(ei).5 Experiments5.1 Data PreparationOur training corpus consists of 239K sentencepairs with about 6.9M/8.9M words in Chi-nese/English, respectively.
We first word-alignthem by GIZA++ (Och and Ney, 2000) with re-finement option ?grow-diag-and?
(Koehn et al,2003), and then parse the Chinese sentences usingthe parser of Xiong et al (2005) into parse forests,which are pruned into relatively small forests witha pruning threshold 3.
We also parse the Englishsentences using the parser of Charniak (2000) into1-best constituency trees, which will be convertedinto dependency trees using Magerman (1995)?shead rules.
We also store the POS tag informa-tion for each word in dependency trees, and com-pute two different dependency language modelsfor words and POS tags in dependency tree sepa-rately.
Finally, we apply translation rule extractionalgorithm described in Section 3.
We use SRI Lan-guage Modeling Toolkit (Stolcke, 2002) to train a4-gram language model with Kneser-Ney smooth-ing on the first 1/3 of the Xinhua portion of Giga-word corpus.
At the decoding step, we again parsethe input sentences into forests and prune themwith a threshold 10, which will direct the trans-lation (Section 4).We use the 2002 NIST MT Evaluation test setas our development set and the 2005 NIST MTEvaluation test set as our test set.
We evaluate thetranslation quality using the BLEU-4 metric (Pap-ineni et al, 2002), which is calculated by the scriptmteval-v11b.pl with its default setting which iscase-insensitive matching of n-grams.
We use thestandard minimum error-rate training (Och, 2003)to tune the feature weights to maximize the sys-tem?s BLEU score on development set.5.2 ResultsTable 2 shows the results on the test set.
Ourbaseline system is a state-of-the-art forest-basedconstituency-to-string model (Mi et al, 2008), orforest c2s for short, which translates a source for-est into a target string by pattern-matching the1439constituency-to-string (c2s) rules and the bilin-gual phrases (s2s).
The baseline system extracts31.9M c2s rules, 77.9M s2s rules respectively andachieves a BLEU score of 34.17 on the test set3.At first, we investigate the influence of differ-ent rule sets on the performance of baseline sys-tem.
We first restrict the target side of transla-tion rules to be well-formed structures, and weextract 13.8M constituency-to-dependency (c2d)rules, which is 43% of c2s rules.
We also extract9.0M string-to-dependency (s2d) rules, which isonly 11.6% of s2s rules.
Then we convert c2d ands2d rules to c2s and s2s rules separately by re-moving the target-dependency structures and feedthem into the baseline system.
As shown in thethird line in the column of BLEU score, the per-formance drops 1.7 BLEU points over baselinesystem due to the poorer rule coverage.
However,when we further use all s2s rules instead of s2drules in our next experiment, it achieves a BLEUscore of 34.03, which is very similar to the base-line system.
Those results suggest that restrictionson c2s rules won?t hurt the performance, but re-strictions on s2s will hurt the translation qualitybadly.
So we should utilize all the s2s rules in or-der to preserve a good coverage of translation ruleset.The last two lines in Table 2 show the results ofour new forest-based constituency-to-dependencymodel (forest c2d for short).
When we only usec2d and s2d rules, our system achieves a BLEUscore of 33.25, which is lower than the baselinesystem in the first line.
But, with the same rule set,our model still outperform the result in the sec-ond line.
This suggests that using dependency lan-guage model really improves the translation qual-ity by less than 1 BLEU point.In order to utilize all the s2s rules and increasethe rule coverage, we parse the target strings ofthe s2s rules into dependency fragments, and con-struct the pseudo s2d rules (s2s-dep).
Then weuse c2d and s2s-dep rules to direct the translation.With the help of the dependency language model,our new model achieves a significant improvementof +0.7 BLEU points over the forest c2s baselinesystem (p < 0.05, using the sign-test suggested by3According to the reports of Liu et al (2009), with a morelarger training corpus (FBIS plus 30K) but no name entitytranslations (+1 BLEU points if it is used), their forest-basedconstituency-to-constituency model achieves a BLEU scoreof 30.6, which is similar to Moses (Koehn et al, 2007).
So ourbaseline system is much better than the BLEU score (30.6+1)of the constituency-to-constituency system and Moses.System Rule Set BLEUType #forest c2sc2s 31.9M 34.17s2s 77.9Mc2d 13.8M 32.48(?1.7)s2d 9.0Mc2d 13.8M 34.03(?0.1)s2s 77.9Mforest c2dc2d 13.8M 33.25(?0.9)s2d 9.0Mc2d 13.8M 34.88(?0.7)s2s-dep 77.9MTable 2: Statistics of different types of rules ex-tracted on training corpus and the BLEU scoreson the test set.Collins et al (2005)).
For the first time, a tree-to-tree model can surpass tree-to-string counterpartssignificantly even with fewer rules.6 Related WorkThe concept of packed forest has been used inmachine translation for several years.
For exam-ple, Huang and Chiang (2007) use forest to char-acterize the search space of decoding with in-tegrated language models.
Mi et al (2008) andMi and Huang (2008) use forest to direct trans-lation and extract rules rather than 1-best tree inorder to weaken the influence of parsing errors,this is also the first time to use forest directlyin machine translation.
Following this direction,Liu et al (2009) and Zhang et al (2009) applyforest into tree-to-tree (Zhang et al, 2007) andtree-sequence-to-string models(Liu et al, 2007)respectively.
Different from Liu et al (2009), weapply forest into a new constituency tree to de-pendency tree translation model rather than con-stituency tree-to-tree model.Shen et al (2008) present a string-to-dependency model.
They define the well-formeddependency structures to reduce the size oftranslation rule set, and integrate a dependencylanguage model in decoding step to exploit longdistance word relations.
This model shows asignificant improvement over the state-of-the-arthierarchical phrase-based system (Chiang, 2005).Compared with this work, we put fewer restric-tions on the definition of well-formed dependencystructures in order to extract more rules; the1440other difference is that we can also extract moreexpressive constituency to dependency rules,since the source side of our rule can encodemulti-level reordering and contain more variablesbeing larger than two; furthermore, our rules canbe pattern-matched at high level, which is morereasonable than using glue rules in Shen et al(2008)?s scenario; finally, the most important oneis that our model runs very faster.Liu et al (2009) propose a forest-basedconstituency-to-constituency model, they putmore emphasize on how to utilize parse forestto increase the tree-to-tree rule coverage.
Bycontrast, we only use 1-best dependency treeson the target side to explore long distance rela-tions and extract translation rules.
Theoretically,we can extract more rules since dependencytree has the best inter-lingual phrasal cohesionproperties (Fox, 2002).7 Conclusion and Future WorkIn this paper, we presented a novel forest-basedconstituency-to-dependency translation model,which combines the advantages of both tree-to-string and string-to-tree systems, runs fast andguarantees grammaticality of the output.
To learnthe constituency-to-dependency translation rules,we first identify the frontier set for all thenodes in the constituency forest on the sourceside.
Then we fragment them and extract mini-mal rules.
Finally, we glue them together to becomposed rules.
At the decoding step, we firstparse the input sentence into a constituency for-est.
Then we convert it into a translation for-est by patter-matching the constituency to stringrules.
Finally, we traverse the translation forestin a bottom-up fashion and translate it into a tar-get dependency tree by incorporating string-basedand dependency-based language models.
Using allconstituency-to-dependency translation rules andbilingual phrases, our model achieves +0.7 pointsimprovement in BLEU score significantly over astate-of-the-art forest-based tree-to-string system.This is also the first time that a tree-to-tree modelcan surpass tree-to-string counterparts.In the future, we will do more experimentson rule coverage to compare the constituency-to-constituency model with our model.
Furthermore,we will replace 1-best dependency trees on thetarget side with dependency forests to further in-crease the rule coverage.AcknowledgementThe authors were supported by National NaturalScience Foundation of China, Contracts 60736014and 90920004, and 863 State Key Project No.2006AA010108.
We thank the anonymous review-ers for their insightful comments.
We are alsograteful to Liang Huang for his valuable sugges-tions.ReferencesSylvie Billot and Bernard Lang.
1989.
The structure ofshared forests in ambiguous parsing.
In Proceedingsof ACL ?89, pages 143?151.Eugene Charniak.
2000.
A maximum-entropy inspiredparser.
In Proceedings of NAACL, pages 132?139.David Chiang.
2005.
A hierarchical phrase-basedmodel for statistical machine translation.
In Pro-ceedings of ACL, pages 263?270, Ann Arbor, Michi-gan, June.David Chiang.
2007.
Hierarchical phrase-based trans-lation.
Comput.
Linguist., 33(2):201?228.Michael Collins, Philipp Koehn, and Ivona Kucerova.2005.
Clause restructuring for statistical machinetranslation.
In Proceedings of ACL, pages 531?540.Yuan Ding and Martha Palmer.
2005.
Machine trans-lation using probabilistic synchronous dependencyinsertion grammars.
In Proceedings of ACL, pages541?548, June.Heidi J.
Fox.
2002.
Phrasal cohesion and statisticalmachine translation.
In In Proceedings of EMNLP-02.Michel Galley, Mark Hopkins, Kevin Knight, andDaniel Marcu.
2004.
What?s in a translation rule?In Proceedings of HLT/NAACL.Michel Galley, Jonathan Graehl, Kevin Knight, DanielMarcu, Steve DeNeefe, Wei Wang, and IgnacioThayer.
2006.
Scalable inference and training ofcontext-rich syntactic translation models.
In Pro-ceedings of COLING-ACL, pages 961?968, July.Peter Hellwig.
2006.
Parsing with Dependency Gram-mars, volume II.
An International Handbook ofContemporary Research.Liang Huang and David Chiang.
2005.
Better k-bestparsing.
In Proceedings of IWPT.Liang Huang and David Chiang.
2007.
Forest rescor-ing: Faster decoding with integrated language mod-els.
In Proceedings of ACL, pages 144?151, June.Liang Huang, Kevin Knight, and Aravind Joshi.
2006.Statistical syntax-directed translation with extendeddomain of locality.
In Proceedings of AMTA.1441Liang Huang, Hao Zhang, Daniel Gildea, , and KevinKnight.
2009.
Binarization of synchronous context-free grammars.
Comput.
Linguist.Liang Huang.
2008.
Forest reranking: Discriminativeparsing with non-local features.
In Proceedings ofACL.Philipp Koehn, Franz Joseph Och, and Daniel Marcu.2003.
Statistical phrase-based translation.
In Pro-ceedings of HLT-NAACL, pages 127?133, Edmon-ton, Canada, May.Philipp Koehn, Hieu Hoang, Alexandra Birch, ChrisCallison-Burch, Marcello Federico, Nicola Bertoldi,Brooke Cowan, Wade Shen, Christine Moran,Richard Zens, Chris Dyer, Ondrej Bojar, AlexandraConstantin, and Evan Herbst.
2007.
Moses: Opensource toolkit for statistical machine translation.
InProceedings of ACL, pages 177?180, June.Yang Liu, Qun Liu, and Shouxun Lin.
2006.
Tree-to-string alignment template for statistical machinetranslation.
In Proceedings of COLING-ACL, pages609?616, Sydney, Australia, July.Yang Liu, Yun Huang, Qun Liu, and Shouxun Lin.2007.
Forest-to-string statistical translation rules.
InProceedings of ACL, pages 704?711, June.Yang Liu, Yajuan Lu?, and Qun Liu.
2009.
Improvingtree-to-tree translation with packed forests.
In Pro-ceedings of ACL/IJCNLP, August.David M. Magerman.
1995.
Statistical decision-treemodels for parsing.
In Proceedings of ACL, pages276?283, June.Daniel Marcu, Wei Wang, Abdessamad Echihabi, andKevin Knight.
2006.
Spmt: Statistical machinetranslation with syntactified target language phrases.In Proceedings of EMNLP, pages 44?52, July.Haitao Mi and Liang Huang.
2008.
Forest-based trans-lation rule extraction.
In Proceedings of EMNLP2008, pages 206?214, Honolulu, Hawaii, October.Haitao Mi, Liang Huang, and Qun Liu.
2008.
Forest-based translation.
In Proceedings of ACL-08:HLT,pages 192?199, Columbus, Ohio, June.Franz J. Och and Hermann Ney.
2000.
Improved sta-tistical alignment models.
In Proceedings of ACL,pages 440?447.Franz J. Och.
2003.
Minimum error rate training instatistical machine translation.
In Proceedings ofACL, pages 160?167.Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu.
2002.
BLEU: a method for automaticevaluation of machine translation.
In Proceedingsof ACL, pages 311?318, Philadephia, USA, July.Chris Quirk, Arul Menezes, and Colin Cherry.
2005.Dependency treelet translation: Syntactically in-formed phrasal SMT.
In Proceedings of ACL, pages271?279, June.Libin Shen, Jinxi Xu, and Ralph Weischedel.
2008.
Anew string-to-dependency machine translation algo-rithm with a target dependency language model.
InProceedings of ACL-08: HLT, June.Andreas Stolcke.
2002.
SRILM - an extensible lan-guage modeling toolkit.
In Proceedings of ICSLP,volume 30, pages 901?904.Deyi Xiong, Shuanglong Li, Qun Liu, and ShouxunLin.
2005.
Parsing the Penn Chinese Treebank withSemantic Knowledge.
In Proceedings of IJCNLP2005, pages 70?81.Deyi Xiong, Qun Liu, and Shouxun Lin.
2007.
Adependency treelet string correspondence model forstatistical machine translation.
In Proceedings ofSMT, pages 40?47.Hao Zhang, Liang Huang, Daniel Gildea, and KevinKnight.
2006.
Synchronous binarization for ma-chine translation.
In Proc.
of HLT-NAACL.Min Zhang, Hongfei Jiang, Aiti Aw, Jun Sun, Sheng Li,and Chew Lim Tan.
2007.
A tree-to-tree alignment-based model for statistical machine translation.
InProceedings of MT-Summit.Hui Zhang, Min Zhang, Haizhou Li, Aiti Aw, andChew Lim Tan.
2009.
Forest-based tree sequenceto string translation model.
In Proceedings of theACL/IJCNLP 2009.1442
