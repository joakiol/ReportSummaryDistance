Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 311?316,Jeju, Republic of Korea, 8-14 July 2012. c?2012 Association for Computational LinguisticsPost-ordering by Parsing for Japanese-English StatisticalMachine TranslationIsao Goto Masao UtiyamaMultilingual Translation Laboratory, MASTAR ProjectNational Institute of Information and Communications Technology3-5 Hikaridai, Keihanna Science City, Kyoto, 619-0289, Japan{igoto, mutiyama, eiichiro.sumita}@nict.go.jpEiichiro SumitaAbstractReordering is a difficult task in translatingbetween widely different languages such asJapanese and English.
We employ the post-ordering framework proposed by (Sudoh etal., 2011b) for Japanese to English transla-tion and improve upon the reordering method.The existing post-ordering method reordersa sequence of target language words in asource language word order via SMT, whileour method reorders the sequence by: 1) pars-ing the sequence to obtain syntax structuressimilar to a source language structure, and 2)transferring the obtained syntax structures intothe syntax structures of the target language.1 IntroductionThe word reordering problem is a challenging onewhen translating between languages with widelydifferent word orders such as Japanese and En-glish.
Many reordering methods have been proposedin statistical machine translation (SMT) research.Those methods can be classified into the followingthree types:Type-1: Conducting the target word selection andreordering jointly.
These include phrase-based SMT(Koehn et al, 2003), hierarchical phrase-based SMT(Chiang, 2007), and syntax-based SMT (Galley etal., 2004; Ding and Palmer, 2005; Liu et al, 2006;Liu et al, 2009).Type-2: Pre-ordering (Xia and McCord, 2004;Collins et al, 2005; Tromble and Eisner, 2009; Ge,2010; Isozaki et al, 2010b; DeNero and Uszkoreit,2011; Wu et al, 2011).
First, these methods re-order the source language sentence into the targetlanguage word order.
Then, they translate the re-ordered source word sequence using SMT methods.Type-3: Post-ordering (Sudoh et al, 2011b; Ma-tusov et al, 2005).
First, these methods translatethe source sentence almost monotonously into a se-quence of the target language words.
Then, theyreorder the translated word sequence into the targetlanguage word order.This paper employs the post-ordering frameworkfor Japanese-English translation based on the dis-cussions given in Section 2, and improves upon thereordering method.
Our method uses syntactic struc-tures, which are essential for improving the targetword order in translating long sentences betweenJapanese (a Subject-Object-Verb (SOV) language)and English (an SVO language).Before explaining our method, we explain the pre-ordering method for English to Japanese used in thepost-ordering framework.In English-Japanese translation, Isozaki et al(2010b) proposed a simple pre-ordering method thatachieved the best quality in human evaluations,which were conducted for the NTCIR-9 patent ma-chine translation task (Sudoh et al, 2011a; Goto etal., 2011).
The method, which is called head final-ization, simply moves syntactic heads to the end ofcorresponding syntactic constituents (e.g., phrasesand clauses).
This method first changes the Englishword order into a word order similar to Japaneseword order using the head finalization rule.
Then,it translates (almost monotonously) the pre-ordered311JapaneseHFEmonotone translationEnglishpost-orderingFigure 1: Post-ordering framework.English words into Japanese.There are two key reasons why this pre-orderingmethod works for estimating Japanese word order.The first reason is that Japanese is a typical head-final language.
That is, a syntactic head word comesafter nonhead (dependent) words.
Second, input En-glish sentences are parsed by a high-quality parser,Enju (Miyao and Tsujii, 2008), which outputs syn-tactic heads.
Consequently, the parsed English in-put sentences can be pre-ordered into a Japanese-like word order using the head finalization rule.Pre-ordering using the head finalization rule nat-urally cannot be applied to Japanese-English trans-lation, because English is not a head-final language.If we want to pre-order Japanese sentences into anEnglish-like word order, we therefore have to buildcomplex rules (Sudoh et al, 2011b).2 Post-ordering for Japanese to EnglishSudoh et al (2011b) proposed a post-orderingmethod for Japanese-English translation.
The trans-lation flow for the post-ordering method is shown inFigure 1, where ?HFE?
is an abbreviation of ?HeadFinal English?.
An HFE sentence consists of En-glish words in a Japanese-like structure.
It can beconstructed by applying the head-finalization rule(Isozaki et al, 2010b) to an English sentence parsedby Enju.
Therefore, if good rules are applied to thisHFE sentence, the underlying English sentence canbe recovered.
This is the key observation of the post-ordering method.The process of post-ordering translation consistsof two steps.
First, the Japanese input sentence istranslated into HFE almost monotonously.
Then, theword order of HFE is changed into an English wordorder.Training for the post-ordering method is con-ducted by first converting the English sentences ina Japanese-English parallel corpus into HFE sen-tences using the head-finalization rule.
Next, amonotone phrase-based Japanese-HFE SMT modelis built using the Japanese-HFE parallel corpusJapanese: kare    wa        kinou        hon        wo       kattaHFE:he_va0yesterday    books_va2boughtHFE:he_va0yesterday    books_va2boughtNP_ST NP_STVP_SWVP_SWS_STEnglish:he   (_va0)   bought    books   (_va2)   yesterdayNP NPVPVPSParsingReorderingFigure 2: Example of post-ordering by parsing.whose HFE was converted from English.
Finally,an HFE-to-English word reordering model is builtusing the HFE-English parallel corpus.3 Post-ordering Models3.1 SMT ModelSudoh et al (2011b) have proposed using phrase-based SMT for converting HFE sentences into En-glish sentences.
The advantage of their method isthat they can use off-the-shelf SMT techniques forpost-ordering.3.2 Parsing ModelOur proposed model is called the parsing model.The translation process for the parsing model isshown in Figure 2.
In this method, we first parse theHFE sentence into a binary tree.
We then swap thenodes annotated with ?
SW?
suffixes in this binarytree in order to produce an English sentence.The structures of the HFE sentences, which areused for training our parsing model, can be obtainedfrom the corresponding English sentences as fol-lows.1 First, each English sentence in the trainingJapanese-English parallel corpus is parsed into a bi-nary tree by applying Enju.
Then, for each node inthis English binary tree, the two children of eachnode are swapped if its first child is the head node(See (Isozaki et al, 2010b) for details of the head1The explanations of pseudo-particles ( va0 and va2) andother details of the HFE is given in Section 4.2.312final rules).
At the same time, these swapped nodesare annotated with ?
SW?.
When the two nodes arenot swapped, they are annotated with ?
ST?
(indi-cating ?Straight?).
A node with only one child isnot annotated with either ?
ST?
or ?
SW?.
The re-sult is an HFE sentence in a binary tree annotatedwith ?
SW?
and ?
ST?
suffixes.Observe that the HFE sentences can be regardedas binary trees annotated with syntax tags aug-mented with swap/straight suffixes.
Therefore, thestructures of these binary trees can be learnable byusing an off-the-shelf grammar learning algorithm.The learned parsing model can be regarded as anITG model (Wu, 1997) between the HFE and En-glish sentences.
2In this paper, we used the Berkeley Parser (Petrovand Klein, 2007) for learning these structures.
TheHFE sentences can be parsed by using the learnedparsing model.
Then the parsed structures can beconverted into their corresponding English struc-tures by swapping the ?
SW?
nodes.
Note that thisparsing model jointly learns how to parse and swapthe HFE sentences.4 Detailed Explanation of Our MethodThis section explains the proposed method, whichis based on the post-ordering framework using theparsing model.4.1 Translation MethodFirst, we produce N-best HFE sentences us-ing Japanese-to-HFE monotone phrase-based SMT.Next, we produce K-best parse trees for each HFEsentence by parsing, and produce English sentencesby swapping any nodes annotated with ?
SW?.
Thenwe score the English sentences and select the En-glish sentence with the highest score.For the score of an English sentence, we usethe sum of the log-linear SMT model score forJapanese-to-HFE and the logarithm of the languagemodel probability of the English sentence.2There are works using the ITG model in SMT: ITG wasused for training pre-ordering models (DeNero and Uszkoreit,2011); hierarchical phrase-based SMT (Chiang, 2007), which isan extension of ITG; and reordering models using ITG (Chen etal., 2009; He et al, 2010).
These methods are not post-orderingmethods.4.2 HFE and ArticlesThis section describes the details of HFE sentences.In HFE sentences: 1) Heads are final except forcoordination.
2) Pseudo-particles are inserted afterverb arguments: va0 (subject of sentence head),va1 (subject of verb), and va2 (object of verb).3) Articles (a, an, the) are dropped.In our method of HFE construction, unlike thatused by (Sudoh et al, 2011b), plural nouns are leftas-is instead of converted to the singular.Applying our parsing model to an HFE sentenceproduces an English sentence that does not havearticles, but does have pseudo-particles.
We re-moved the pseudo-particles from the reordered sen-tences before calculating the probabilities used forthe scores of the reordered sentences.
A reorderedsentence without pseudo-particles is represented byE.
A language model P (E) was trained from En-glish sentences whose articles were dropped.In order to output a genuine English sentence E?fromE, articles must be inserted intoE.
A languagemodel trained using genuine English sentences isused for this purpose.
We try to insert one of thearticles {a, an, the} or no article for each word in E.Then we calculate the maximum probability wordsequence through dynamic programming for obtain-ing E?.5 Experiment5.1 SetupWe used patent sentence data for the Japanese toEnglish translation subtask from the NTCIR-9 and8 (Goto et al, 2011; Fujii et al, 2010).
Therewere 2,000 test sentences for NTCIR-9 and 1,251for NTCIR-8.
XML entities included in the datawere decoded to UTF-8 characters before use.We used Enju (Miyao and Tsujii, 2008) v2.4.2 forparsing the English side of the training data.
Mecab3 v0.98 was used for the Japanese morphologicalanalysis.
The translation model was trained usingsentences of 64 words or less from the training cor-pus as (Sudoh et al, 2011b).
We used 5-gram lan-guage models using SRILM (Stolcke et al, 2011).We used the Berkeley parser (Petrov and Klein,2007) to train the parsing model for HFE and to3http://mecab.sourceforge.net/313parse HFE.
The parsing model was trained using 0.5million sentences randomly selected from trainingsentences of 40 words or less.
We used the phrase-based SMT system Moses (Koehn et al, 2007) tocalculate the SMT score and to produce HFE sen-tences.
The distortion limit was set to 0.
We used10-best Moses outputs and 10-best parsing resultsof Berkeley parser.5.2 Compared MethodsWe used the following 5 comparison methods:Phrase-based SMT (PBMT), Hierarchical phrase-based SMT (HPBMT), String-to-tree syntax-basedSMT (SBMT), Post-ordering based on phrase-basedSMT (PO-PBMT) (Sudoh et al, 2011b), and Post-ordering based on hierarchical phrase-based SMT(PO-HPBMT).We used Moses for these 5 systems.
ForPO-PBMT, a distortion limit 0 was used for theJapanese-to-HFE translation and a distortion limit20 was used for the HFE-to-English translation.The PO-HPBMT method changes the post-orderingmethod of PO-PBMT from a phrase-based SMTto a hierarchical phrase-based SMT.
We used amax-chart-span 15 for the hierarchical phrase-basedSMT.
We used distortion limits of 12 or 20 forPBMT and a max-chart-span 15 for HPBMT.The parameters for SMT were tuned by MERTusing the first half of the development data with HFEconverted from English.5.3 Results and DiscussionWe evaluated translation quality based on the case-insensitive automatic evaluation scores of RIBESv1.1 (Isozaki et al, 2010a) and BLEU-4.
The resultsare shown in Table 1.Ja-to-En NTCIR-9 NTCIR-8RIBES BLEU RIBES BLEUProposed 72.57 31.75 73.48 32.80PBMT (limit 12) 68.44 29.64 69.18 30.72PBMT (limit 20) 68.86 30.13 69.63 31.22HPBMT 69.92 30.15 70.18 30.94SBMT 69.22 29.53 69.87 30.37PO-PBMT 68.81 30.39 69.80 31.71PO-HPBMT 70.47 27.49 71.34 28.78Table 1: Evaluation results (case insensitive).From the results, the proposed method achievedthe best scores for both RIBES and BLEU forNTCIR-9 and NTCIR-8 test data.
Since RIBES issensitive to global word order and BLEU is sensitiveto local word order, the effectiveness of the proposedmethod for both global and local reordering can bedemonstrated through these comparisons.In order to investigate the effects of our post-ordering method in detail, we conducted an ?HFE-to-English reordering?
experiment, which shows themain contribution of our post-ordering method inthe framework of post-ordering SMT as comparedwith (Sudoh et al, 2011b).
In this experiment, wechanged the word order of the oracle-HFE sentencesmade from reference sentences into English, this isthe same way as Table 4 in (Sudoh et al, 2011b).The results are shown in Table 2.This results show that our post-ordering methodis more effective than PO-PBMT and PO-HPBMT.Since RIBES is based on the rank order correla-tion coefficient, these results show that the proposedmethod correctly recovered the word order of theEnglish sentences.
These high scores also indicatethat the parsing results for high quality HFE arefairly trustworthy.oracle-HFE-to-En NTCIR-9 NTCIR-8RIBES BLEU RIBES BLEUProposed 94.66 80.02 94.93 79.99PO-PBMT 77.34 62.24 78.14 63.14PO-HPBMT 77.99 53.62 80.85 58.34Table 2: Evaluation resutls focusing on post-ordering.In these experiments, we did not compare ourmethod to pre-ordering methods.
However, somegroups used pre-ordering methods in the NTCIR-9Japanese to English translation subtask.
The NTT-UT (Sudoh et al, 2011a) and NAIST (Kondo et al,2011) groups used pre-ordering methods, but couldnot produce RIBES and BLEU scores that both werebetter than those of the baseline results.
In contrast,our method was able to do so.6 ConclusionThis paper has described a new post-orderingmethod.
The proposed method parses sentences thatconsist of target language words in a source lan-guage word order, and does reordering by transfer-ring the syntactic structures similar to the source lan-guage syntactic structures into the target languagesyntactic structures.314ReferencesHan-Bin Chen, Jian-Cheng Wu, and Jason S. Chang.2009.
Learning Bilingual Linguistic ReorderingModel for Statistical Machine Translation.
In Pro-ceedings of Human Language Technologies: The 2009NAACL, pages 254?262, Boulder, Colorado, June.
As-sociation for Computational Linguistics.David Chiang.
2007.
Hierarchical Phrase-Based Trans-lation.
Computational Linguistics, 33(2):201?228.Michael Collins, Philipp Koehn, and Ivona Kucerova.2005.
Clause Restructuring for Statistical MachineTranslation.
In Proceedings of the 43rd ACL, pages531?540, Ann Arbor, Michigan, June.
Association forComputational Linguistics.John DeNero and Jakob Uszkoreit.
2011.
Inducing Sen-tence Structure from Parallel Corpora for Reordering.In Proceedings of the 2011 Conference on EmpiricalMethods in Natural Language Processing, pages 193?203, Edinburgh, Scotland, UK., July.
Association forComputational Linguistics.Yuan Ding and Martha Palmer.
2005.
Machine Transla-tion Using Probabilistic Synchronous Dependency In-sertion Grammars.
In Proceedings of the 43rd ACL,pages 541?548, Ann Arbor, Michigan, June.
Associa-tion for Computational Linguistics.Atsushi Fujii, Masao Utiyama, Mikio Yamamoto, Take-hito Utsuro, Terumasa Ehara, Hiroshi Echizen-ya, andSayori Shimohata.
2010.
Overview of the PatentTranslation Task at the NTCIR-8 Workshop.
In Pro-ceedings of NTCIR-8, pages 371?376.Michel Galley, Mark Hopkins, Kevin Knight, and DanielMarcu.
2004.
What?s in a translation rule?
InDaniel Marcu Susan Dumais and Salim Roukos, ed-itors, HLT-NAACL 2004: Main Proceedings, pages273?280, Boston, Massachusetts, USA, May 2 - May7.
Association for Computational Linguistics.Niyu Ge.
2010.
A Direct Syntax-Driven ReorderingModel for Phrase-Based Machine Translation.
In Pro-ceedings of NAACL-HLT, pages 849?857, Los Ange-les, California, June.
Association for ComputationalLinguistics.Isao Goto, Bin Lu, Ka Po Chow, Eiichiro Sumita, andBenjamin K. Tsou.
2011.
Overview of the Patent Ma-chine Translation Task at the NTCIR-9 Workshop.
InProceedings of NTCIR-9, pages 559?578.Yanqing He, Yu Zhou, Chengqing Zong, and HuilinWang.
2010.
A Novel Reordering Model Based onMulti-layer Phrase for Statistical Machine Translation.In Proceedings of the 23rd Coling, pages 447?455,Beijing, China, August.
Coling 2010 Organizing Com-mittee.Hideki Isozaki, Tsutomu Hirao, Kevin Duh, KatsuhitoSudoh, and Hajime Tsukada.
2010a.
Automatic Eval-uation of Translation Quality for Distant LanguagePairs.
In Proceedings of the 2010 EMNLP, pages 944?952.Hideki Isozaki, Katsuhito Sudoh, Hajime Tsukada, andKevin Duh.
2010b.
Head Finalization: A Simple Re-ordering Rule for SOV Languages.
In Proceedings ofthe Joint Fifth Workshop on Statistical Machine Trans-lation and MetricsMATR, pages 244?251, Uppsala,Sweden, July.
Association for Computational Linguis-tics.Philipp Koehn, Franz J. Och, and Daniel Marcu.
2003.Statistical Phrase-Based Translation.
In Proceedingsof the 2003 HLT-NAACL, pages 48?54.Philipp Koehn, Hieu Hoang, Alexandra Birch, ChrisCallison-Burch, Marcello Federico, Nicola Bertoldi,Brooke Cowan, Wade Shen, Christine Moran, RichardZens, Chris Dyer, Ondrej Bojar, Alexandra Con-stantin, and Evan Herbst.
2007.
Moses: Open SourceToolkit for Statistical Machine Translation.
In Pro-ceedings of the 45th ACL, pages 177?180, Prague,Czech Republic, June.
Association for ComputationalLinguistics.Shuhei Kondo, Mamoru Komachi, Yuji Matsumoto, Kat-suhito Sudoh, Kevin Duh, and Hajime Tsukada.
2011.Learning of Linear Ordering Problems and its Applica-tion to J-E Patent Translation in NTCIR-9 PatentMT.In Proceedings of NTCIR-9, pages 641?645.Yang Liu, Qun Liu, and Shouxun Lin.
2006.
Tree-to-String Alignment Template for Statistical MachineTranslation.
In Proceedings of the 21st ACL, pages609?616, Sydney, Australia, July.
Association forComputational Linguistics.Yang Liu, Yajuan Lu?, and Qun Liu.
2009.
ImprovingTree-to-Tree Translation with Packed Forests.
In Pro-ceedings of the Joint Conference of the 47th AnnualMeeting of the ACL and the 4th International JointConference on Natural Language Processing of theAFNLP, pages 558?566, Suntec, Singapore, August.Association for Computational Linguistics.E.
Matusov, S. Kanthak, and Hermann Ney.
2005.
Onthe Integration of Speech Recognition and StatisticalMachine Translation.
In Proceedings of Interspeech,pages 3177?3180.Yusuke Miyao and Jun?ichi Tsujii.
2008.
Feature ForestModels for Probabilistic HPSG Parsing.
In Computa-tional Linguistics, Volume 34, Number 1, pages 81?88.Slav Petrov and Dan Klein.
2007.
Improved Infer-ence for Unlexicalized Parsing.
InNAACL-HLT, pages404?411, Rochester, New York, April.
Association forComputational Linguistics.Andreas Stolcke, Jing Zheng, Wen Wang, and VictorAbrash.
2011.
SRILM at Sixteen: Update andOutlook.
In Proceedings of IEEE Automatic SpeechRecognition and Understanding Workshop.315Katsuhito Sudoh, Kevin Duh, Hajime Tsukada, MasaakiNagata, Xianchao Wu, Takuya Matsuzaki, andJun?ichi Tsujii.
2011a.
NTT-UT Statistical MachineTranslation in NTCIR-9 PatentMT.
In Proceedings ofNTCIR-9, pages 585?592.Katsuhito Sudoh, Xianchao Wu, Kevin Duh, HajimeTsukada, and Masaaki Nagata.
2011b.
Post-orderingin Statistical Machine Translation.
In Proceedings ofthe 13th Machine Translation Summit, pages 316?323.Roy Tromble and Jason Eisner.
2009.
Learning LinearOrdering Problems for Better Translation.
In Proceed-ings of the 2009 EMNLP, pages 1007?1016, Singa-pore, August.
Association for Computational Linguis-tics.Xianchao Wu, Katsuhito Sudoh, Kevin Duh, HajimeTsukada, and Masaaki Nagata.
2011.
Extracting Pre-ordering Rules from Chunk-based Dependency Treesfor Japanese-to-English Translation.
In Proceedingsof the 13th Machine Translation Summit, pages 300?307.Dekai Wu.
1997.
Stochastic Inversion TransductionGrammars and Bilingual Parsing of Parallel Corpora.Computational Linguistics, 23(3):377?403.Fei Xia and Michael McCord.
2004.
Improving a Statis-tical MT System with Automatically Learned RewritePatterns.
In Proceedings of Coling, pages 508?514,Geneva, Switzerland, Aug 23?Aug 27.
COLING.316
