Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and ComputationalNatural Language Learning, pp.
967?975, Prague, June 2007. c?2007 Association for Computational LinguisticsImproving Translation Quality by Discarding Most of the PhrasetableJ Howard Johnson and Joel MartinInteractive Information GroupNational Research Council CanadaOttawa, Ontario, Canadafirstname.lastname@nrc.gc.caGeorge Foster and Roland KuhnInteractive Language Technologies GroupNational Research Council CanadaGatineau, Que?bec, Canadafirstname.lastname@nrc.gc.caAbstractIt is possible to reduce the bulk of phrase-tables for Statistical Machine Translation us-ing a technique based on the significancetesting of phrase pair co-occurrence in theparallel corpus.
The savings can be quitesubstantial (up to 90%) and cause no reduc-tion in BLEU score.
In some cases, an im-provement in BLEU is obtained at the sametime although the effect is less pronouncedif state-of-the-art phrasetable smoothing isemployed.1 IntroductionAn important part of the process of Statistical Ma-chine Translation (SMT) involves inferring a largetable of phrase pairs that are translations of eachother from a large corpus of aligned sentences.These phrase pairs together with estimates of con-ditional probabilities and useful feature weights,called collectively a phrasetable, are used to matcha source sentence to produce candidate translations.The choice of the best translation is made basedon the combination of the probabilities and featureweights, and much discussion has been made of howto make the estimates of probabilites, how to smooththese estimates, and what features are most usefulfor discriminating among the translations.However, a cursory glance at phrasetables pro-duced often suggests that many of the translationsare wrong or will never be used in any translation.On the other hand, most obvious ways of reducingthe bulk usually lead to a reduction in translationquality as measured by BLEU score.
This has led toan impression that these pairs must contribute some-thing in the grand scheme of things and, certainly,more data is better than less.Nonetheless, this bulk comes at a cost.
Large ta-bles lead to large data structures that require moreresources and more time to process and, more im-portantly, effort directed in handling large tablescould likely be more usefully employed in more fea-tures or more sophisticated search.In this paper, we show that it is possible to prunephrasetables using a straightforward approach basedon significance testing, that this approach does notadversely affect the quality of translation as mea-sured by BLEU score, and that savings in terms ofnumber of discarded phrase pairs can be quite sub-stantial.
Even more surprising, pruning can actu-ally raise the BLEU score although this phenomenonis less prominent if state of the art smoothing ofphrasetable probabilities is employed.Section 2 reviews the basic ideas of StatisticalMachine Translation as well as those of testing sig-nificance of associations in two by two contingencytables departing from independence.
From this, afiltering algorithm will be described that keeps onlyphrase pairs that pass a significance test.
Section 3outlines a number of experiments that demonstratethe phenomenon and measure its magnitude.
Sec-tion 4 presents the results of these experiments.
Thepaper concludes with a summary of what has beenlearned and a discussion of continuing work thatbuilds on these ideas.9672 Background Theory2.1 Our Approach to Statistical MachineTranslationWe define a phrasetable as a set of source phrases (n-grams) s?
and their translations (m-grams) t?, alongwith associated translation probabilities p(s?|t?)
andp(t?|s?).
These conditional distributions are derivedfrom the joint frequencies c(s?, t?)
of source / tar-get n,m-grams observed in a word-aligned parallelcorpus.
These joint counts are estimated using thephrase induction algorithm described in (Koehn etal., 2003), with symmetrized word alignments gen-erated using IBM model 2 (Brown et al, 1993).Phrases are limited to 8 tokens in length (n,m ?
8).Given a source sentence s, our phrase-based SMTsystem tries to find the target sentence t?
that is themost likely translation of s. To make search moreefficient, we use the Viterbi approximation and seekthe most likely combination of t and its alignment awith s, rather than just the most likely t:t?
= argmaxtp(t|s) ?
argmaxt,ap(t,a|s),where a = (s?1, t?1, j1), ..., (s?K , t?K , jK); t?k are tar-get phrases such that t = t?1...t?K ; s?k are sourcephrases such that s = s?j1 ...s?jK ; and s?k is the trans-lation of the kth target phrase t?k.To model p(t,a|s), we use a standard loglinearapproach:p(t,a|s) ?
exp[?i?ifi(s, t,a)]where each fi(s, t,a) is a feature function, andweights ?i are set using Och?s algorithm (Och,2003) to maximize the system?s BLEU score (Pa-pineni et al , 2001) on a development corpus.
Thefeatures used are: the length of t; a single-parameterdistortion penalty on phrase reordering in a, as de-scribed in (Koehn et al, 2003); phrase translationmodel probabilities; and 4-gram language modelprobabilities log p(t), using Kneser-Ney smooth-ing as implemented in the SRILM toolkit (Stolcke,2002).Phrase translation model probabilities are featuresof the form:log p(s|t,a) ?K?k=1log p(s?k|t?k)i.e., we assume that the phrases s?k specified by a areconditionally independent, and depend only on theiraligned phrases t?k.The ?forward?
phrase probabilities p(t?|s?)
are notused as features, but only as a filter on the set ofpossible translations: for each source phrase s?
thatmatches some ngram in s, only the 30 top-rankedtranslations t?
according to p(t?|s?)
are retained.
Oneof the reviewers has pointed out correctly that tak-ing only the top 30 translations will interact with thesubject under study; however, this pruning techniquehas been used as a way of controlling the width ofour beam search and rebalancing search parameterswould have complicated this study and taken it awayfrom our standard practice.The phrase translation model probabilities aresmoothed according to one of several techniques asdescribed in (Foster et al, 2006) and identified in thediscussion below.2.2 Significance testing using two by twocontingency tablesEach phrase pair can be thought of as am n,m-gram(s?, t?)
where s?
is an n-gram from the source side ofthe corpus and t?
is an m-gram from the target sideof the corpus.We then define: C(s?, t?)
as the number of parallelsentences that contain one or more occurrences ofs?
on the source side and t?
on the target side; C(s?
)the number of parallel sentences that contain one ormore occurrences of s?
on the source side; and C(t?
)the number of parallel sentences that contain one ormore occurrences of t?
on the target side.
Togetherwith N , the number of parallel sentences, we haveenough information to draw up a two by two contin-gency table representing the unconditional relation-ship between s?
and t?.
This table is shown in Table1.A standard statistical technique used to assess theimportance of an association represented by a con-tingency table involves calculating the probabilitythat the observed table or one that is more extremecould occur by chance assuming a model of inde-pendence.
This is called a significance test.
Intro-ductory statistics texts describe one such test calledthe Chi-squared test.There are other tests that more accurately applyto our small tables with only two rows and columns.968Table 1: Two by two contingency table for s?
and t?C(s?, t?)
C(s?)?
C(s?, t?)
C(s?)C(t?)?
C(s?, t?)
N ?
C(s?)?
C(t?)
+ C(s?, t?)
N ?
C(s?)C(t?)
N ?
C(t?)
NIn particular, Fisher?s exact test calculates probabil-ity of the observed table using the hypergeometricdistibution.ph(C(s?, t?))
=(C(s?
)C(s?, t?
))(N ?
C(s?)C(t?)?
C(s?, t?))(NC(t?
))The p-value associated with our observed table isthen calculated by summing probabilities for tablesthat have a larger C(s?, t?
)).p-value(C(s?, t?))
=??k=C(s?,t?
)ph(k)This probability is interpreted as the probabilityof observing by chance an association that is at leastas strong as the given one and hence its significance.Agresti (1996) provides an excellent introduction tothis topic and the general ideas of significance test-ing in contingency tables.Fisher?s exact test of significance is considered agold standard since it represents the precise proba-bilities under realistic assumptions.
Tests such as theChi-squared test or the log-likelihood-ratio test (yetanother approximate test of significance) depend onasymptotic assumptions that are often not valid forsmall counts.Note that the count C(s?, t?)
can be larger orsmaller than c(s?, t?)
discussed above.
In most cases,it will be larger, because it counts all co-occurrencesof s?
with t?
rather than just those that respect theword alignment.
It can be smaller though becausemultiple co-occurrences can occur within a singlealigned sentence pair and be counted multiple timesin c(s?, t?).
On the other hand, C(s?, t?)
will not countall of the possible ways that an n,m-grammatch canoccur within a single sentence pair; it will count thematch only once per sentence pair in which it occurs.Moore (2004) discusses the use of signifi-cance testing of word associations using the log-likelihood-ratio test and Fisher?s exact test.
Heshows that Fisher?s exact test is often a practicalmethod if a number of techniques are followed:1. approximating the logarithms of factorials us-ing commonly available numerical approxima-tions to the log gamma function,2.
using a well-known recurrence for the hyperge-ometic distribution,3.
noting that few terms usually need to besummed, and4.
observing that convergence is usually rapid.2.3 Significance pruningThe idea behind significance pruning of phrasetablesis that not all of the phrase pairs in a phrasetable areequally supported by the data and that many of theweakly supported pairs could be removed because:1. the chance of them occurring again might below, and2.
their occurrence in the given corpus may be theresult of an artifact (a combination of effectswhere several estimates artificially compensatefor one another).
This concept is usually re-ferred to as overfit since the model fits aspectsof the training data that do not lead to improvedprediction.Phrase pairs that cannot stand on their own bydemonstrating a certain level of significance are sus-pect and removing them from the phrasetable may969be beneficial in terms of reducing the size of datastructures.
This will be shown to be the case in rathergeneral terms.Note that this pruning may and quite often willremove all of the candidate translations for a sourcephrase.
This might seem to be a bad idea but it mustbe remembered that deleting longer phrases will al-low combinations of shorter phrases to be used andthese might have more and better translations fromthe corpus.
Here is part of the intuition about howphrasetable smoothing may interact with phrasetablepruning: both are discouraging longer but infrequentphrases from the corpus in favour of combinations ofmore frequent, shorter phrases.Because the probabilities involved below will beso incredibly tiny, we will work instead with the neg-ative of the natural logs of the probabilities.
Thusinstead of selecting phrase pairs with a p-value lessthan exp(?20), we will select phrase pairs with anegative-log-p-value greater than 20.
This has theadvantage of working with ordinary-sized numbersand the happy convention that bigger means morepruning.2.4 C(s?, t?)
= 1, 1-1-1 Tables and the ?ThresholdAn important special case of a table occurs when aphrase pair occurs exactly once in the corpus, andeach of the component phrases occurs exactly oncein its side of the parallel corpus.These phrase pairs will be referred to as 1-1-1phrase pairs and the corresponding tables will becalled 1-1-1 contingency tables because C(s?)
= 1,C(t?)
= 1, and C(s?, t?)
= 1.Moore (2004) comments that the p-value for thesetables under Fisher?s exact test is 1/N .
Since we areusing thresholds of the negative logarithm of the p-value, the value ?
= log(N) is a useful threshold toconsider.In particular, ?
+  (where  is an appropriatelysmall positive number) is the smallest threshold thatresults in none of the 1-1-1 phrase pairs being in-cluded.
Similarly, ?
?
 is the largest threshold thatresults in all of the 1-1-1 phrase pairs being included.Because 1-1-1 phrase pairs can make up a large partof the phrase table, this is important observation forits own sake.Since the contingency table with C(s?, t?)
= 1 hav-ing the greatest significance (lowest p-value) is the1-1-1 table, using the threshold of ?+  can be usedto exclude all of the phrase pairs occurring exactlyonce (C(s?, t?)
= 1).The common strategy of deleting all of the 1-count phrase pairs is very similar in effect to the useof the ?
+  threshold.3 Experiments3.1 WMT06The corpora used for most of these experiments arepublicly available and have been used for a num-ber of comparative studies (Workshop on Statisti-cal Machine Translation, 2006).
Provided as part ofthe materials for the shared task are parallel corporafor French?English, Spanish?English, and German?English as well as language models for English,French, Spanish, and German.
These are all basedon the Europarl resources (Europarl, 2003).The only change made to these corpora was toconvert them to lowercase and to Unicode UTF-8.Phrasetables were produced by symmetrizing IBM2conditional probabilities as described above.The phrasetables were then used as a list ofn,m-grams for which counts C(s?, t?
), C(s?
), andC(t?)
were obtained.
Negative-log-p-values underFisher?s exact test were computed for each of thephrase pairs in the phrasetable and the entry wascensored if the negative-log-p-value for the test wasbelow the pruning threshold.
The entries that arekept are ones that are highly significant.A number of combinations involving many differ-ent pruning thresholds were considered: no pruning,10, ?
?, ?+, 15, 20, 25, 50, 100, and 1000.
In ad-dition, a number of different phrasetable smoothingalgorithms were used: no smoothing, Good-Turingsmoothing, Kneser-Ney 3 parameter smoothing andthe loglinear mixture involving two features calledZens-Ney (Foster et al, 2006).3.2 ChineseTo test the effects of significance pruning on largercorpora, a series of experiments was run on a muchlarger corpus based on that distributed for MT06Chinese?English (NIST MT, 2006).
Since the ob-jective was to assess how the method scaled we usedour preferred phrasetable smoothing technique of9701000100101BLEU by Pruning Thresholdno smoothing33333 3333GT (+1)+ ++++ ++++KN3 (+2)2 2222 2222ZN (+3)?
????
???
?1071061051000100101Phrasetable Size by Pruning Thresholdsize3 33333333107106105BLEU by Phrasetable Sizeno smoothing333333333GT (+1)+++++++++KN3 (+2)222222222ZN (+3)????????
?Figure 1: WMT06: Results for French ??
English.
[to separate the curves, graphs for smoothed meth-ods are shifted by +1, +2, or +3 BLEU points]Table 2: Corpus Sizes and ?
Valuesnumber ofparallel sentences ?WMT06: fr??
en 688,031 13.4415892WMT06: es??
en 730,740 13.501813WMT06: de??
en 751,088 13.5292781Chinese?English: best 3,164,228 14.9674197Chinese?English: UN-v2 4,979,345 15.4208089Zens-Ney and separated our corpus into two phrase-tables, one based on the UN corpus and the otherbased on the best of the remaining parallel corporaavailable to us.Different pruning thresholds were considered: nopruning, 14, 16, 18, 20, and 25.
In addition, anothermore aggressive method of pruning was attempted.Moore points out, correctly, that phrase pairs that oc-cur in only one sentence pair, (C(s?, t?)
= 1 ), are lessreliable and might require more special treatment.These are all pruned automatically at thresholds of16 and above but not at threshold of 14.
A spe-cial series of runs was done for threshold 14 with allof these singletons removed to see whether at thesethresholds it was the significance level or the prun-ing of phrase pairs with (C(s?, t?)
= 1 ) that was moreimportant.
This is identified as 14?
in the results.4 ResultsThe results of the experiments are described in Ta-bles 2 through 6.Table 2 presents the sizes of the various parallelcorpora showing the number of parallel sentences,N , for each of the experiments, together with the ?thresholds (?
= log(N)).Table 3 shows the sizes of the phrasetables thatresult from the various pruning thresholds describedfor the WMT06 data.
It is clear that this is extremelyaggressive pruning at the given levels.Table 4 shows the corresponding phrasetable sizesfor the large corpus Chinese?English data.
Thepruning is not as aggressive as for the WMT06 databut still quite sizeable.Tables 5 and 6 show the main results for theWMT06 and the Chinese?English large corpus ex-periments.
To make these results more graphic, Fig-ure 1 shows the French ??
English data from theWMT06 results in the form of three graphs.
Note971Table 3: WMT06: Distinct phrase pairs by pruning thresholdthreshold fr??
en es??
en de??
ennone 9,314,165 100% 11,591,013 100% 6,954,243 100%10 7,999,081 85.9% 10,212,019 88.1% 5,849,593 84.1%??
 6,014,294 64.6% 7,865,072 67.9% 4,357,620 62.7%?
+  1,435,576 15.4% 1,592,655 13.7% 1,163,296 16.7%15 1,377,375 14.8% 1,533,610 13.2% 1,115,559 16.0%20 1,152,780 12.4% 1,291,113 11.1% 928,855 13.4%25 905,201 9.7% 1,000,264 8.6% 732,230 10.5%50 446,757 4.8% 481,737 4.2% 365,118 5.3%100 235,132 2.5% 251,999 2.2% 189,655 2.7%1000 22,873 0.2% 24,070 0.2% 16,467 0.2%Table 4: Chinese?English: Distinct phrase pairs by pruning thresholdthreshold best UN-v2none 18,858,589 100% 20,228,273 100%14 7,666,063 40.7% 13,276,885 65.6%16 4,280,845 22.7% 7,691,660 38.0%18 4,084,167 21.7% 7,434,939 36.8%20 3,887,397 20.6% 7,145,827 35.3%25 3,403,674 18.0% 6,316,795 31.2%also pruning C(s?, t?)
= 114?
4,477,920 23.7% 7,917,062 39.1%that an artificial separation of 1 BLEU point hasbeen introduced into these graphs to separate them.Without this, they lie on top of each other and hidethe essential point.
In compensation, the scale forthe BLEU co-ordinate has been removed.These results are summarized in the followingsubsections.4.1 BLEU as a function of thresholdIn tables 5 and 6, the largest BLEU score for eachset of runs has been marked in bold font.
In addition,to highlight that there are many near ties for largestBLEU, all BLEU scores that are within 0.1 of thebest are also marked in bold.When this is done it becomes clear that pruningat a level of 20 for the WMT06 runs would not re-duce BLEU in most cases and in many cases wouldactually increase it.
A pruning threshold of 20 cor-responds to discarding roughly 90% of the phrase-table.For the Chinese?English large corpus runs, a levelof 16 seems to be about the best with a small in-crease in BLEU and a 60% ?
70% reduction in thesize of the phrasetable.4.2 BLEU as a function of depth of pruningAnother view of this can be taken from Tables 5and 6.
The fraction of the phrasetable retained isa more or less simple function of pruning thresholdas shown in Tables 3 and 4.
By including the per-centages in Tables 5 and 6, we can see that BLEUgoes up as the fraction approaches between 20% and30%.This seems to be a relatively stable observationacross the experiments.
It is also easily explained byits strong relationship to pruning threshold.4.3 Large corporaTable 6 shows that this is not just a small corpus phe-nomenon.
There is a sizeable benefit both in phrase-table reduction and a modest improvement to BLEUeven in this case.4.4 Is this just the same as phrasetablesmoothing?One question that occurred early on was whether thisimprovement in BLEU is somehow related to theimprovement in BLEU that occurs with phrasetablesmoothing.972It appears that the answer is, in the main, yes, al-though there is definitely something else going on.It is true that the benefit in terms of BLEU is less-ened for better types of phrasetable smoothing butthe benefit in terms of the reduction in bulk holds.
Itis reassuring to see that no harm to BLEU is done byremoving even 80% of the phrasetable.4.5 Comment about C(s?, t?)
= 1Another question that came up is the role of phrasepairs that occur only once: C(s?, t?)
= 1.
In particu-lar as discussed above, the most significant of theseare the 1-1-1 phrase pairs whose components alsoonly occur once: C(s?)
= 1, and C(t?)
= 1.
Thesephrase pairs are amazingly frequent in the phrase-tables and are pruned in all of the experiments ex-cept when pruning threshold is equal to 14.The Chinese?English large corpus experimentsgive us a good opportunity to show that significancelevel seems to be more an issue than the case thatC(s?, t?)
= 1.Note that we could have kept the phrase pairswhose marginal counts were greater than one butmost of these are of lower significance and likelyare pruned already by the threshold.
The given con-figuration was considered the most likely to yield abenefit and its poor performance led to the wholeidea being put aside.5 Conclusions and Continuing WorkTo sum up, the main conclusions are five in number:1.
Phrasetables produced by the standard Diag-Andmethod (Koehn et al, 2003) can be aggres-sively pruned using significance pruning with-out worsening BLEU.2.
If phrasetable smoothing is not done, the BLEUscore will improve under aggressive signifi-cance pruning.3.
If phrasetable smoothing is done, the improve-ment is small or negligible but there is still noloss on aggressive pruning.4.
The preservation of BLEU score in the pres-ence of large-scale pruning is a strong effect insmall and moderate size phrasetables, but oc-curs also in much larger phrasetables.5.
In larger phrasetables based on larger corpora,the percentage of the table that can be dis-carded appears to decrease.
This is plausiblesince a similar effect (a decrease in the benefitof smoothing) has been noted with phrasetablesmoothing (Foster et al, 2006).
Together theseresults suggest that, for these corpus sizes, theincrease in the number of strongly supportedphrase pairs is greater than the increase in thenumber of poorly supported pairs, which agreeswith intuition.Although there may be other approaches to prun-ing that achieve a similar effect, the use of Fisher?sexact test is mathematically and conceptually one ofthe simplest since it asks a question separately foreach phrase pair: ?Considering this phase pair inisolation of any other analysis on the corpus, could ithave occurred plausibly by purely random processesinherent in the corpus construction??
If the answeris ?Yes?, then it is hard to argue that the phrase pairis an association of general applicability from theevidence in this corpus alone.Note that the removal of 1-count phrase pairs issubsumed by significance pruning with a thresholdgreater than ?
and many of the other simple ap-proaches (from an implementation point of view)are more difficult to justify as simply as the abovesignificance test.
Nonetheless, there remains workto do in determining if computationally simpler ap-proaches do as well.
Moore?s work suggests thatlog-likelihood-ratio would be a cheaper and accurateenough alternative, for example.We will now return to the interaction of the se-lection in our beam search of the top 30 candidatesbased on forward conditional probabilities.
This willaffect our results but most likely in the followingmanner:1.
For very small thresholds, the beam will be-come much wider and the search will takemuch longer.
In order to allow the experimentsto complete in a reasonable time, other meanswill need to be employed to reduce the choices.This reduction will also interact with the sig-nificance pruning but in a less understandablemanner.2.
For large thresholds, there will not be 30973choices and so there will be no effect.3.
For intermediate thresholds, the extra prun-ing might reduce BLEU score but by a smallamount because most of the best choices areincluded in the search.Using thresholds that remove most of the phrase-table would no doubt qualify as large thresholds sothe question is addressing the true shape of the curvefor smaller thresholds and not at the expected operat-ing levels.
Nonetheless, this is a subject for furtherstudy, especially as we consider alternatives to our?filter 30?
approach for managing beam width.There are a number of important ways that thiswork can and will be continued.
The code base fortaking a list of n,m-grams and computing the re-quired frequencies for signifance evaluation can beapplied to related problems.
For example, skip-n-grams (n-grams that allow for gaps of fixed or vari-able size) may be studied better using this approachleading to insight about methods that weakly ap-proximate patterns.The original goal of this work was to better un-derstand the character of phrasetables, and it re-mains a useful diagnostic technique.
It will hope-fully lead to more understanding of what it takesto make a good phrasetable especially for languagesthat require morphological analysis or segmentationto produce good tables using standard methods.The negative-log-p-value promises to be a usefulfeature and we are currently evaluating its merits.6 AcknowledgementThis material is based upon work supported bythe Defense Advanced Research Projects Agency(DARPA) under Contract No.
HR0011-06-C-0023.Any opinions, findings and conclusions or recom-mendations expressed in this material are those ofthe authors and do not necessarily reflect the viewsof the Defense Advanced Research Projects Agency(DARPA).
?ReferencesAlan Agresti.
1996.
An Introduction to Categorical DataAnalysis.
Wiley.Peter F. Brown, Stephen A. Della Pietra, Vincent J. DellaPietra and Robert L. Mercer.
1993.
The Mathemat-ics of Statistical Machine Translation: Parameter es-timation.
Computational Linguistics, 19(2):263?312,June.Philipp Koehn 2003.
Europarl: A Mul-tilingual Corpus for Evaluation of Ma-chine Translation.
Unpublished draft.
seehttp://www.iccs.inf.ed.ac.uk/?pkoehn/publications/europarl.pdfGeorge Foster, Roland Kuhn, and Howard Johnson.2006.
Phrasetable Smoothing for Statistical MachineTranslation.
In Proceedings of the 2006 Conferenceon Empirical Methods in Natural Language Process-ing, Sydney, Australia.Reinhard Kneser and Hermann Ney.
1995.
Improvedbacking-off for m-gram language modeling.
In Pro-ceedings of the International Conference on Acoustics,Speech, and Signal Processing (ICASSP) 1995, pages181?184, Detroit, Michigan.
IEEE.Philipp Koehn, Franz Josef Och, and Daniel Marcu.2003.
Statistical phrase-based translation.
In EduardHovy, editor, Proceedings of the Human LanguageTechnology Conference of the North American Chap-ter of the Association for Computational Linguistics,pages 127?133, Edmonton, Alberta, Canada, May.NAACL.Robert C. Moore.
2004.
On Log-Likelihood-Ratios andthe Significance of Rare Events.
In Proceedings ofthe 2004 Conference on Empirical Methods in Natu-ral Language Processing, Barcelona, Spain.NIST.
2006.
NIST MT Benchmark Test.
seehttp://www.nist.gov/speech/tests/mt/Franz Josef Och.
2003.
Minimum error rate training forstatistical machine translation.
In Proceedings of the41th Annual Meeting of the Association for Computa-tional Linguistics(ACL), Sapporo, July.Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu.
2001.
BLEU: A method for automaticevaluation of Machine Translation.
Technical ReportRC22176, IBM, September.NAACL Workshop on Statistical Machine Translation.2006.
see http://www.statmt.org/wmt06/Andreas Stolcke.
2002.
SRILM - an extensible languagemodeling toolkit.
In Proceedings of the 7th Inter-national Conference on Spoken Language Processing(ICSLP) 2002, Denver, Colorado, September.Richard Zens and Hermann Ney.
2004.
Improvements inphrase-based statistical machine translation.
In Pro-ceedings of Human Language Technology Conference/ North American Chapter of the ACL, Boston, May.974Table 5: WMT06 Results: BLEU by type of smoothing and pruning thresholdthreshold phrasetable % fr ??
en es ??
en de ??
en en ??
fr en ??
es en ??
derelative frequency: no smoothingnone 100% 25.39 27.26 20.74 27.29 27.17 14.7110 84?88% 25.97 27.81 21.08 27.82 27.71 15.09??
 63?68% 26.32 28.00 21.27 28.11 28.09 15.19?
+  14?17% 26.34 28.27 21.22 28.16 28.08 15.2415 13?15% 26.36 28.50 21.14 28.20 28.18 15.2920 11?13% 26.51 28.45 21.36 28.28 28.06 15.2825 8?10% 26.50 28.38 21.28 28.32 27.97 15.2550 4?5% 26.26 27.88 20.87 28.05 27.90 15.08100 2% 25.66 27.07 20.07 27.38 27.11 14.661000 0.2% 20.49 21.66 15.23 22.51 22.31 11.36Good-Turingnone 100% 25.96 28.14 21.17 27.84 27.95 15.1310 84?88% 26.33 28.33 21.38 28.18 28.27 15.22??
 63?68% 26.54 28.63 21.50 28.36 28.39 15.31?
+  14?17% 26.24 28.49 21.15 28.22 28.16 15.2815 13?15% 26.48 28.03 21.21 28.27 28.21 15.3120 11?13% 26.65 28.45 21.41 28.36 28.14 15.2525 8?10% 26.54 28.56 21.31 28.35 28.04 15.2850 4?5% 26.26 27.78 20.94 28.07 27.95 15.08100 2% 25.70 27.07 20.12 27.41 27.13 14.661000 0.2% 20.49 21.66 15.52 22.53 22.31 11.37Kneser-Ney (3 parameter)none 100% 26.89 28.70 21.78 28.64 28.71 15.5010 84?88% 26.79 28.78 21.71 28.63 28.41 15.3515 13?15% 26.49 28.69 21.34 28.60 28.57 15.5220 11?13% 26.73 28.67 21.54 28.56 28.44 15.4125 8?10% 26.84 28.70 21.29 28.54 28.21 15.4250 4?5% 26.44 28.16 20.93 28.17 28.05 15.17100 2% 25.72 27.27 20.11 27.50 27.26 14.581000 0.2% 20.48 21.70 15.28 22.58 22.36 11.33Zens-Neynone 100% 26.87 29.07 21.55 28.75 28.54 15.5010 84?88% 26.81 29.00 21.65 28.72 28.52 15.5415 13?15% 26.92 28.67 21.74 28.79 28.32 15.4420 11?13% 26.93 28.47 21.72 28.69 28.42 15.4525 8?10% 26.85 28.79 21.58 28.59 28.27 15.3750 4?5% 26.51 27.96 20.96 28.30 27.96 15.27100 2% 25.82 27.34 20.02 27.57 27.30 14.511000 0.2% 20.50 21.76 15.46 22.68 22.33 11.56Table 6: Chinese Results: BLEU by pruning thresholdthreshold phrasetable % nist04 nist05 nist06-GALE nist06-NISTZens-Ney Smoothing applied to all phrasetablesnone 100% 32.14 30.69 13.06 27.9714 40?65% 32.66 31.14 13.11 28.3516 22?38% 32.73 30.97 13.14 28.0018 21?36% 31.56 30.45 12.49 27.0320 20?35% 32.00 30.73 12.50 27.3325 18?31% 30.54 29.58 11.68 26.12also pruning C(s?, t?)
= 114?
23?39% 32.08 30.99 12.75 27.66975
