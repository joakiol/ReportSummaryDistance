Proceedings of NAACL-HLT 2013, pages 211?220,Atlanta, Georgia, 9?14 June 2013. c?2013 Association for Computational LinguisticsDiscriminative Joint Modeling of Lexical Variation and Acoustic Confusionfor Automated Narrative Retelling AssessmentMaider Lehr?, Izhak Shafran?, Emily Prud?hommeaux?
and Brian Roark?
?Center for Spoken Language Understanding, Oregon Health & Science University?Center for Language Sciences, University of Rochester{maiderlehr,zakshafran,emilpx,roarkbr}@gmail.comAbstractAutomatically assessing the fidelity of aretelling to the original narrative ?
a task ofgrowing clinical importance ?
is challenging,given extensive paraphrasing during retellingalong with cascading automatic speech recog-nition (ASR) errors.
We present a word tag-ging approach using conditional random fields(CRFs) that allows a diversity of featuresto be considered during inference, includingsome capturing acoustic confusions encodedin word confusion networks.
We evaluate theapproach under several scenarios, includingboth supervised and unsupervised training, thelatter achieved by training on the output ofa baseline automatic word-alignment model.We also adapt the ASR models to the domain,and evaluate the impact of error rate on per-formance.
We find strong robustness to ASRerrors, even using just the 1-best system out-put.
A hybrid approach making use of both au-tomatic alignment and CRFs trained taggingmodels achieves the best performance, yield-ing strong improvements over using either ap-proach alone.1 IntroductionNarrative production tasks are an essential compo-nent of many standard neuropsychological test bat-teries.
For example, narration of a wordless pic-ture book is part of the Autism Diagnostic Obser-vation Schedule (ADOS) (Lord et al 2002) andretelling of previously narrated stories is part of boththe Developmental Neuropsychological Assessment(NEPSY) (Korkman et al 1998) and the Wech-sler Logical Memory (WLM) test (Wechsler, 1997).Such tests also arise in reading comprehension, sec-ond language learning and other computer-based tu-toring systems (Xie et al 2012; Zhang et al 2008).The accuracy of automated scoring of a narrativeretelling depends on correctly identifying which ofthe source narrative?s propositions or events (whatwe will call ?story elements?)
have been includedin the retelling.
Speakers may choose to relatethese elements using diverse words or phrases, andan automated method of identifying these elementsneeds to model the permissible variants and para-phrasings.
In previous work (Lehr et al 2012;Prud?hommeaux and Roark, 2012; Prud?hommeauxand Roark, 2011), we developed models based onautomatic word-alignment methods, as describedbriefly in Section 3.
Such alignments are learnedin an unsupervised manner from a parallel corpus ofmanual or ASR transcripts of retellings and the orig-inal source narrative, much as in machine translationtraining.Relying on manual transcripts to train the align-ment models limits the ability of these methods tohandle ASR errors.
By instead training on ASRtranscripts, these methods can automatically capturesome regularities of lexical variants and their com-mon realizations by the recognizer.
Additionally, ev-idence of acoustic confusability is available in wordlattice output from the recognizer, which can be ex-ploited to yield more robust automatic scoring, par-ticularly in high error-rate scenarios.In this paper, we present and evaluate the use ofword tagging models for this task, in contrast tojust using automatic (unsupervised) word-alignmentmethods.
The approach is general enough to al-211low tagging of word confusion networks derivedfrom lattices, thus allowing us to explore the utilityof such representations to achieve robustness.
Wepresent results under a range of experimental condi-tions, including: variously adapting the ASR mod-els to the domain; using maximum entropy modelsrather than CRFs; differing tagsets (BIO versus IO);and with varying degrees of supervision.
Finally,we demonstrate improved utility in terms of usingthe automatic scores to classify elderly individualsas having Mild Cognitive Impairment.
Ultimatelywe find that hybrid approaches, making use of bothword-alignment and tagging models, yield strongimprovements over either used independently.2 Wechsler Logical Memory (WLM) taskThe Wechsler Logical Memory (WLM) task (Wech-sler, 1997), a widely used subtest of a battery of neu-ropsychological tests used to assess memory func-tion in adults, has been shown to be a good indicatorof Mild Cognitive Impairment (MCI) (Storandt andHill, 1989; Petersen et al 1999; Wang and Zhou,2002; Nordlund et al 2005), the stage of cogni-tive decline that is often a precursor to dementia ofthe Alzheimer?s type.
In the WLM, the subject lis-tens to the examiner read a brief narrative and thenretells the narrative twice: immediately upon hear-ing it and after about 20 minutes.
The examinergrades the subject?s response by counting how manyof the story elements the subject recalled.An excerpt of the text read by the clinician whileadministering the WLM task is shown in Figure 1.The story elements in the text are delineated usingslashes, 25 elements in all.
An example retellingis shown in Figure 2 to illustrate how the retellingsare scored.
The clinical evaluation guidelines spec-ify what lexical substitutions, if any, are allowedfor each element.
Some elements, such as cafeteriaand Thompson, must be recalled verbatim.
In othercases, subjects are given credit for variants, such asAnnie for Anna, or paraphrasing of concepts such assympathetic for touched by the woman?s story.
Theexample retelling received a score of 12, with onepoint for each of the recalled story elements: Anna,Boston, employed, as a cook, and robbed of, she hadfour, small children, reported, station, touched bythe woman?s story, took up a collection and for her.Anna / Thompson / of South / Boston / em-ployed / as a cook / in a school / cafeteria /reported / at the police / station / that she hadbeen held up / on State Street / the night be-fore / and robbed / .
.
.
/ police / touched by thewoman?s story / took up a collection / for her.Figure 1: Reference text and the set of story elements.Ann Taylor worked in Boston as a cook.
Andshe was robbed of sixty-seven dollars.
Isthat right?
And she had four children andreported at the some kind of station.
The fel-low sympathetic and made a collection for herso that she can feed the children.Figure 2: An example retelling with 12 recalled story elements.3 Unsupervised generative automatedscoring with word alignmentIn previous work (Lehr et al 2012; Prud?hommeauxand Roark, 2012; Prud?hommeaux and Roark,2011), we developed a pipeline for automaticallyscoring narrative retellings for the WLM task.
Theutterances corresponding to a retelling were rec-ognized using an ASR system.
The story ele-ments were identified from the 1-best ASR transcriptusing word alignments produced by the Berkeleyaligner (Liang et al 2006), an EM-based wordalignment package developed to align parallel textsfor machine translation.
The word alignment modelwas estimated in an unsupervised manner from aparallel corpus consisting of source narrative andmanual transcripts of retellings from a small set oftraining subjects, and from a pairwise parallel cor-pus of manual retelling transcripts.During inference or test, the ASR transcripts ofthe retellings were aligned using the estimated align-ment model to the source narrative text.
If a wordin the retelling was mapped by the alignment modelto a content word in the source narrative, the ele-ment associated with that content word was countedas correctly recalled in that retelling.
Recall thatthe models were trained on unsupervised data so thealigned words may not always be permissible vari-ants of the target elements.
To alleviate such extra-neous as well as unaligned words, the alignmentsbelow a threshold of posterior probability are dis-carded while decoding.2124 Supervised discriminative automatedscoring with log-linear modelsIn this work, we frame the task of detecting storyelements as a tagging task.
Thus, our problem re-duces to assigning a tag to each word position in theretelling, the tag indicating the story element that theword is associated with.
In its simplest form, wehave 26 tags: one for each of the 25 story elementsindicating the word is ?in?
that element (e.g., I15);and one for ?outside?
of any story element (?O?).
Bytagging word positions, we are framing the problemin a general enough way to allow tagging of wordconfusion networks (Mangu et al 2000), which en-code word confusions that may provide additionalrobustness, particularly in high word-error rate sce-narios.
We make use of log-linear models, whichhave been used for tagging confusion networks (Ku-rata et al 2012), and which allow very flexible fea-ture vector definition and discriminative optimiza-tion.The model allows us to experiment with threetypes of inputs as illustrated in the Figure 3 ?
themanual transcript, the 1-best ASR transcript, and theword confusion network.
To create supervised train-ing data, we force-align ASR transcripts to manualtranscripts and transfer manually annotated story el-ement tags from the reference transcripts to word po-sitions in the confusion network or 1-best ASR out-put using the word-level time marks.
Our unsuper-vised training scenario instead derives story elementtags from a baseline word-alignment based model.Figure 3: Feature vectors at each word position includes lexi-cal variants and acoustic confusions.Markov order 0 Markov order 1(MaxEnt) (CRF)Context yi yi?1yiindependent (CI) yixi yi?1yixiContext yixi?1 yi?1yixi?1dependent (CD) yixi+1 yi?1yixi+1Table 1: Feature templates either using or not using neighbor-ing tag yi?1 (MaxEnt vs. CRF); and for using or not usingneighboring words xi?1, xi+1 (CI vs. CD).4.1 FeaturesGiven a sequence of word positions x = x1 .
.
.
xn,the tagger assigns a sequence of labels y = y1 .
.
.
ynfrom a tag lexicon.
For each word xi in the se-quence, we can define features in the log-linearmodel based on word and tag identities.
Table 1presents several sets of features, defined over wordsand tags at various positions relative to the currentword xi and tag yi and compound features are de-noted as concatenated symbols.Features that rely only on the current tag yi areused in a Markov order 0 model, i.e., one for whicheach tag is labeled independently.
A maximum en-tropy classifier (see Section 4.2) is used with thesefeature sets.
Features that include prior tags en-code dependencies between adjacent tags, and areused within conditional random fields models (seeSection 4.3).
To examine the utility of surroundingwords xi?1 and xi+1, we distinguish between mod-els trained with context independent features (justxi) and context dependent features.
Note that mod-els including context dependent feature sets also in-clude the context independent features, and Markovorder 1 models also include Markov order 0 features.Two other details about our use of the feature tem-plates are worth noting.
First, when tagging confu-sion networks, each word in the network at positioni results in a feature instance.
Thus, if there are fiveconfusable words at position i, then there will befive different xi values being used to instantiate thefeatures in Table 1.
Second, following Kurata et al(2012), we multiply the feature counts for the con-text dependent features by a weight to control theirinfluence on the model.
In this paper, the scalingweight of the context-dependent features was 0.3.We investigate two different tagsets for this task,as presented in Table 2.
The simpler tagset (IO) sim-ply identifies words that are in a story element; the213Tagging anna rent was dueIO-tags I1 I19 I19 I19BIO-tags B1 B19 I19 I19Table 2: Two possible tagsets for labeling.larger tagset (BIO) differentiates among positions ina story element chunk.
The latter tagset is only ofutility for models with Markov order greater thanzero, and hence are only used with CRF models.4.2 MaxEnt-based multiclass classifierOur baseline model is a Maximum Entropy (Max-Ent) classifier where each position i from theretelling x gets assigned one of the IO output tagsyi corresponding to the set of 25 story elements anda null (?O?)
symbol.
The output tag is modeled asthe conditional probability p(yi | xi) given the wordxi at position i in the retelling.p(yi | xi) =exp(d?k=1?k?k(xi, yi))Z(xi)where Z(xi) is a normalization factor.
The featurefunctions ?
(xi, yi) are the Markov order 0 featuresas defined as in the previous section.
The parame-ters ?
?
<d are estimated by optimizing the aboveconditional probability, with L2 regularization.
Weuse the MALLET Toolkit (McCallum, 2002) withdefault regularization parameters.4.3 CRF-based sequence labeling modelThe MaxEnt models assign a tag to each positionfrom the input retelling independently.
However,there are a few reasons why reframing the task asa sequence modeling problem may improve taggingperformance.
First, some of the story elements aremultiword sequences, such as she had been held upor on State Street.
Second, even if a retelling ordersrecalled elements differently than the original narra-tive, there is a tendency for story elements to occurin certain orders.The parameters of the CRF model, ?
?
<d areestimated by optimizing the following conditionalprobability:P (y | x) =exp(d?k=1?k?k(x, y))Z(x)where ?
(x, y) aggregates features across the entiresequence, and Z(x) is a global normalization con-stant over the sequence, rather than local for a partic-ular position as with MaxEnt.
Features for the CRFmodel are Markov order 1 features, and as with theMaxEnt training, we use default (L2) regularizationparameters within the MALLET toolkit.5 Combining tagging and alignmentThis paper contrasts a discriminatively trained tag-ging approach with an unsupervised alignment-based approach, but there are several ways in whichthe two approaches can be combined.
First, thealignment model is unsupervised and can provideits output as training data to the tagging approach,resulting in an unsupervised discriminative model.Second, the alignment model can provide features tothe log-linear tagging model in the supervised condi-tion.
We explore both methods of combination here.5.1 Unsupervised discriminative taggerThe tagging task based on log-linear models pro-vides an appropriate framework to easily incorpo-rate diverse features and discriminatively estimatethe parameters of the model.
However, this ap-proach requires supervised tagged training data, inthis case manual labels indicating the correspon-dence of phrases in the retellings with story elementsin the original narrative.
These manual annotationsare used to derive sequences of story element tagslabeling the words of the retelling.
Manually la-beling the retellings is costly, and the scoring (thuslabeling) scheme is very specific to the test beinganalyzed.
To avoid manual labeling and provide ageneral framework that can easily be adopted in anyretelling based assessment task, we experiment herewith an unsupervised discriminative approach.In this unsupervised approach, the labeled train-ing data required by the log-linear model is providedby the automatic word alignments trained withoutsupervision.
The resulting tag sequences replace themanual tag sequences used in the standard super-vised approach.5.2 Word-alignment derived featuresWhen training discriminative models it is a commonpractice to incorporate into the feature space the out-put from a generative model, since it is a good esti-214mator.
Here we augment the feature space of thelog-linear models with the tags generated by the au-tomatic word alignments.
In addition to the featuresdefined in Section 4.1, we include new features thatmatch predicted labels zi from the word-alignmentmodel with possible labels in the tagger yi.
Our fea-tures include the current tagger label with (1) thecurrent predicted word-alignment label; (2) the pre-vious predicted label; and (3) the next predicted la-bel.
Thus, the new features were yizi, yizi?1 andyizi+1.6 Experimental evaluationsCorpus: Our models were trained on immediate anddelayed retellings from 144 subjects with a meanage of 85.4, of whom 36 were clinically diagnosedwith MCI (training set).
We evaluated our modelson a set of retellings from 70 non-overlapping sub-jects with a mean age of 88.5, half of whom hadreceived a diagnosis of MCI (test set).
In contrastto the unsupervised word-alignment based method,the method outlined here required manual story el-ement labels of the retellings.
The training andtest sets from this paper are therefore different fromthe sets used in previous work (Lehr et al 2012;Prud?hommeaux and Roark, 2012; Prud?hommeauxand Roark, 2011), and the results are not directlycomparable.The recordings were sometimes made in an infor-mal setting, such as the subject?s home or a seniorcenter.
For this reason, there are often extraneousnoises in the recordings such as music, footsteps,and clocks striking the hour.
Although this presentsa challenge for ASR, part of the goal of our workis to demonstrate the robustness of our methods tonoisy audio.6.1 Automatic transcriptionThe baseline ASR system used in the current workis a Broadcast News system which is modeled af-ter Kingsbury et al(2011).
Briefly, the acousticsof speech are modeled by 4000 clustered allophonestates defined over a pentaphone context, wherestates are represented by Gaussian mixture modelswith a total of 150K mixture components.
The ob-servation vectors consist of PLP features, stackedfrom 10 neighboring frames and projected to a 50-1-best oracle oracleSystem (%) WCN(%) lat(%)Baseline 47.2 39.7 27.7AM adaptation 38.2 35.5 21.2LM adaptation 28.3 30.7 19.9AM+LM adaptation 25.6 26.5 16.5Table 3: Improvement in ASR word error-rate by adapting theBroadcast News models to the domain of narrative retelling.dimension space using linear discriminant analysis(LDA).
The acoustic models were trained on 430hours of transcribed speech from Broadcast Newscorpus (LDC97S44, LDC98S71).
The languagemodel is defined over an 84K vocabulary and con-sists of about 1.8M, 1M and and 331K bigrams, tri-grams and 4-grams, estimated from standard Broad-cast news corpus.
The decoding is performed in sev-eral stages using successively refined acoustic mod-els ?
a context-dependent model, a vocal-tract nor-malized model, a speaker-adapted maximum likeli-hood linear regression (MLLR) model, and finallya discriminatively trained model with the boostedMMI criteria (Povey et al 2008).
The system givesa word error rate of 13.1% on the 2004 Rich Tran-scription benchmark by NIST (Fiscus et al 2007),which is comparable to state-of-the-art for equiva-lent amounts of acoustic training data.
On the WLMcorpus, the recognition word error rate was signifi-cantly higher at 47.2% due to a mismatch in domainand the skewed demographics (age) of the speakers.We improved the performance of the aboveBroadcast News models by adapting to the domainof the WLM retellings.
The acoustic models wereadapted using standard MLLR, where linear trans-forms were estimated in an unsupervised mannerto maximize the likelihood over the transcripts ofthe retellings.
The transcripts were generated fromthe baseline system after the final stage of decod-ing with the discriminative model.
The languagemodels were adapted by interpolating the in-domainmodel (weight=0.7) with the out-of-domain model.The gains from these adaptations are reported inthe Table 3.
As expected, we find substantial gainsfrom both acoustic model (AM) and language model(LM) adaptation.
Furthermore, we find benefit inemploying them simultaneously.
We also includethe oracle word error rate (WER) of the WCNs andlattices for each ASR configuration.215One thing to note is that the oracle WER of theWCNs is worse than the 1-best WER when adaptingthe language models.
We speculate that this is dueto bias introduced by the language model adaptedto the story retellings, resulting in word candidatesin the bins that are not truly acoustically confusablecandidates.
This is one potential reason for the lackof utility of WCNs in low WER conditions.6.2 Evaluating retelling scoringWe analyzed the performance of the retelling scor-ing methods under five different input conditions forproducing transcripts: (1) the out-of-domain Broad-cast News recognizer with no adaptation; (2) do-main adapted acoustic model; (3) domain adaptedlanguage model; (4) domain adapted acoustic andlanguage models; and (5) manual (reference) tran-scripts.
Each story element is automatically labeledby the systems as either having been recalled or not,and this is compared with manual scores to derive anF-score accuracy, by calculating precision and recallof recalled story elements.
Derived word alignmentsor tag sequences are converted to binary story ele-ment indicators by simply setting the element to 1if any open-class word is tagged for (or aligned to)that story element.6.2.1 Word alignment based scoringWe evaluate the word alignment approach only on1-best ASR transcripts and manual transcripts, notWCNs.
The first row of Table 4 reports the story ele-ment F-scores for a range of ASR adaptation scenar-ios.
The performance of the model improves signifi-cantly as the WER reduces with adaptation.
With thefully adapted ASR the F-score improves more than13%, and it is only 3.4% worse than with the man-ual transcripts.
The alignments produced in each ofthese scenarios are used as training data in the unsu-pervised condition evaluated below.6.2.2 Log-linear based automated scoringContext-independent features Table 4 summa-rizes the performance of the log-linear models us-ing context independent features (CI) in supervised(section 4), unsupervised (section 5.1) and hybrid(section 5.2) training scenarios for different inputs(reference transcript, ASR 1-best, and word confu-sion network ASR output) and four different ASRconfigurations.The results show a few clear trends.
Both inthe supervised and unsupervised training scenariosthe CRF model provides substantial improvementsover the MaxEnt classifier.
The F-scores obtainedin the unsupervised training scenario are slightlyworse than with supervision, though they are compa-rable to supervised results and an improvement overjust using the word alignment approach, particularlyin high WER scenarios.
The hybrid training sce-nario ?
supervised learning with word alignment de-rived features ?
leads to reduced differences betweenMaxEnt and CRF training compared to the other twotraining scenarios.
In fact, in high WER scenarios,the MaxEnt slightly outperforms the CRF.As expected the best performance is obtained withmanual transcripts and the worst with 1-best tran-scripts generated by the out-of-domain ASR withrelatively high word error rate.
For this ASR con-figuration, using WCNs provide some gain, thoughthe gain is insignificant for the hybrid approach.
Inthe hybrid approach, the output labels of the wordalignment are already good indicators of the outputtag and incorporating the confusable words from theTable 4: Story element F-score achieved by baseline word-alignment model and log-linear models (MaxEnt and CRF) usingcontext independent features (CI) under 3 different scenarios, with 3 different inputs (1-best ASR, word confusion network, andmanual transcripts) and different ASR models (baseline out-of-domain, AM adapted, LM adapted and AM+LM adapted).Training Transcripts: 1-best WCN manualScenario ASR: baseline AM LM AM+LM baseline AM LM AM+LM N/ABaseline word-alignment: 71.9 77.3 84.3 85.4 N/A 88.8Supervised MaxEnt-CI 76.0 81.7 84.6 85.6 78.9 83.4 84.0 84.7 86.4CRF-CI 80.3 87.3 89.7 91.4 83.7 88.8 88.2 90.8 94.4Unsupervised MaxEnt-CI 72.1 79.3 82.7 84.2 77.5 81.2 83.4 83.2 84.8CRF-CI 79.4 85.4 86.8 88.0 81.2 85.8 86.2 87.2 90.5Hybrid MaxEnt-CI 88.1 89.4 89.2 89.6 87.6 89.2 88.8 89.5 91.8CRF-CI 87.0 90.9 91.5 92.1 87.4 91.5 90.1 92.4 94.6216Training Transcripts: 1-best WCN manualScenario ASR: baseline AM LM AM+LM baseline AM LM AM+LM N/ASupervised MaxEnt-CD 80.1 87.3 90.0 91.1 83.5 88.6 88.2 90.3 93.3CRF-CD-IO 80.6 88.0 89.9 91.2 84.2 89.6 88.8 90.5 94.7CRF-CD-BIO 81.1 87.9 90.6 91.7 84.5 89.5 88.8 90.8 94.7Un- MaxEnt-CD 77.1 83.1 86.5 89.0 80.2 85.0 86.2 87.6 90.7supervised CRF-CD-IO 79.1 85.3 87.1 88.3 81.0 85.9 86.4 87.5 90.3CRF-CD-BIO 79.1 85.6 87.2 88.4 81.3 85.9 86.2 87.3 90.6Hybrid MaxEnt-CD 88.4 90.2 90.7 91.6 88.6 90.5 90.4 91.4 93.5CRF-CD-IO 87.9 91.3 91.6 92.5 88.3 91.7 90.7 92.1 94.8CRF-BIO 87.8 91.9 91.8 93.0 88.7 92.0 90.7 92.3 94.7Table 5: Story element F-score achieved by log-linear models (MaxEnt and CRF) when adding context dependent features (CD)and BIO tags for the CRF models, under 3 different scenarios, with 3 different inputs (1-best ASR, word confusion network, andmanual transcripts) and different ASR models (baseline out-of-domain, AM adapted, LM adapted and AM+LM adapted).WCN into the feature vector apparently mainly addsnoise.When the transcripts are generated with theadapted models, the word confidence score of the 1-best is higher and the WCN bins have fewer acous-tically confusable words.
Still, the WCN input ishelpful in the AM-adapted ASR system.
Whenthe transcripts are generated with LM adapted mod-els, the performance is better with 1-best than withWCNs.
As mentioned earlier, adapting the lan-guage models may introduce a bias due to the rel-atively low LM perplexity for this domain.
In thelowest WER scenarios, the best performing systemsachieve over 90% F-score, within two percent of theperformance achieved with manual transcripts.Context-dependent features Exercising the flex-ibility of log-linear models, we investigated the im-pact of using context-dependent (CD) features in-stead of the CI features used in the previous exper-iments.
Our CD features take into account the twoimmediately neighboring word positions.
As men-tioned earlier, following Kurata et al(2012), thecounts from the neighboring word positions wereweighted (?
= 0.3) to avoid data sparsity.
This re-duces the sensitivity of the model to time alignmenterrors between the tag and feature vector sequenceswithout increasing the dimensions.
In Table 5, wereport the F-scores for the different ASR configu-rations, inputs, and log-linear models with contextdependent features, using the standard IO tagset asin Table 4.Although there are some exceptions, adding con-text information from the input features improvesthe performance of the models.
In particular, theMaxEnt models benefit from incorporating this ex-tra information.
The MaxEnt models improve theirperformance substantially for all three training sce-narios, while the gains for the CRF models are moremodest, especially for the unsupervised approachwhere the performance degrades or does not changemuch, since some context information is alreadycaptured by the Markov order 1 features.BIO tagset As detailed in Section 4.1, story el-ements sometimes span multiple words, so for theCRF models we investigated two different schemesfor tagging, following typical practice in named en-tity extraction (Ratinov and Roth, 2009) and syn-tactic chunking (Sha and Pereira, 2003).
The BIOtagging scheme makes the distinction between thetokens from the story elements that are in the be-ginning from the ones that are not.
The O tag isassigned to the tokens that do not belong to any ofthe story elements.
The IO tagging uses a single tagfor the tokens that fall in the same story element,which is the approach we have followed so far.
Inaddition to presenting results using context depen-dent features, Table 5 presents results with the BIOtagset.For the supervised and hybrid approaches, theBIO tagging provides insignificant but consistentgains for most of the scenarios.
The unsupervisedapproach provides mixed results.
This may be due tothe way in which the word alignment model scoresthe retellings.
It tags only those words from theretelling that are aligned with a content word in thesource narrative, which may result in the loss of the217Training Transcripts: 1-best WCN manualScenario ASR: baseline AM LM AM+LM baseline AM LM AM+LM N/ABaseline word-alignment: 0.65 0.67 0.74 0.76 N/A 0.79Supervised MaxEnt-CD 0.65 0.73 0.76 0.77 0.70 0.73 0.77 0.77 0.81CRF-CD-BIO 0.69 0.76 0.77 0.76 0.73 0.76 0.77 0.78 0.82Un- MaxEnt-CD 0.65 0.72 0.75 0.76 0.70 0.75 0.75 0.76 0.80supervised CRF-CD-BIO 0.74 0.75 0.78 0.78 0.71 0.74 0.77 0.76 0.81Hybrid MaxEnt-CD 0.72 0.76 0.77 0.78 0.74 0.76 0.77 0.77 0.82CRF-CD-BIO 0.72 0.76 0.78 0.78 0.76 0.77 0.78 0.79 0.81Table 6: Classification performance (AUC) for the baseline word-alignment model and the best performing log-linear models ofboth types (MaxEnt and CRF) under 3 different scenarios with 3 types of input and 4 types of ASR models.structure of some multiwords story elements that weare trying to capture with the BIO scheme.6.3 Evaluating MCI classificationEach of the individuals producing retellings in ourcorpus underwent a battery of neuropsychologicaltests, and were assigned a Clinical Dementia Rating(CDR) (Morris, 1993), which is a composite scorederived from measures of cognitive function in sixdomains, including memory.
Importantly, it is as-signed independently of the Wechsler Logical Mem-ory test we are analyzing in this paper, which allowsus to evaluate the utility of our WLM analyses inan unbiased manner.
MCI is defined as a CDR of0.5 (Ritchie and Touchon, 2000), and subjects in thisstudy have either a CDR of 0 (no impairment) or 0.5(MCI).In previous work, we found that the featuresextracted from the retellings are useful in dis-tinguishing subjects with MCI from neurotyp-ical age-matched controls (Lehr et al 2012;Prud?hommeaux and Roark, 2012; Prud?hommeauxand Roark, 2011).
From each retellings, we extractBoolean features for each story element, for a totalof 50 features for classification.
Each feature indi-cates whether the retelling contained that story ele-ment.In this paper, we carry out similar classificationexperiments to investigate the impact of using log-linear models on the extraction of features for classi-fication.
We build a support vector machine (SVM)using the LibSVM (Chang and Lin, 2011) exten-sion to the WEKA data mining Java API (Hall et al2009).
This allows recollection of different elementsto be weighted differently.
This is unlike the manualscoring of WLM based on clinical guidelines whereall elements are weighted equally irrespective of thedifficulty.
The SVM was trained on manually ex-tracted story element feature vectors.
We comparedthe performance of the MCI classification for threetypes of input and four ASR configurations underthe supervised, unsupervised, and hybrid scenarios.For each scenario we chose the best scoring systemfrom among the automated systems reported in Ta-bles 4 and 5.
Classification results, evaluated as areaunder the curve (AUC), are reported in Table 6, bothfor the log-linear trained tagging models and for thebaseline word-alignment based method.
For refer-ence (not shown in the table), the SVM classifierperformed at 0.83 when features values are manu-ally populated.The results show that the AUC improves steadilyas the quality of the transcription is improved, go-ing from the baseline system to the adapted mod-els.
This is consistent with the improvements seen inthe F-score for detecting story elements.
The differ-ent approaches for detecting the story elements fromthe transcriptions did not ultimately show significantdifferences in MCI classification results.
Overall,the best classification values are given by the hy-brid approach, which performs slightly better thanthe other two approaches.
The best AUC in thehybrid scenario (0.79, very close to the AUC=0.81achieved with manual transcripts) is obtained witha CRF trained with WCNs from the fully adaptedASR model and with context dependent features andBIO tags.Comparing WCN versus 1-best as inputs, usingWCN as input improves classification performancewhen the 1-best transcripts are poor, as in the caseof out-of-domain ASR.
The adapted recognizer im-proves the performance of the 1-best significantlymaking it unnecessary to resort to WCN as inputs.Comparing the MaxEnt model with CRF model218for extracting story elements, we see that the averageF-scores for the MaxEnt models trained on CD fea-tures are nearly as good as and sometimes slightlybetter than those produced using the CRF models.The CRF extracted story elements, however, tend toyield classifiers that perform slightly better, espe-cially in the unsupervised approach with 1-best in-puts.7 Summary and discussionThis paper examines the task of automatically scor-ing narrative retellings in terms of their fidelity tothe original narrative content, using discriminativelytrained log-linear tagging models.
Fully automaticscoring must account for both lexical variation andacoustic confusion from ASR errors.
Lexical vari-ation ?
due to extensive paraphrasing on the partof the individuals retelling the narrative ?
can bemodeled effectively using word-alignment modelssuch as those employed in machine translation sys-tems (Lehr et al 2012; Prud?hommeaux and Roark,2011).
This paper focuses on an alternative ap-proach, where both lexical variation and ASR con-fusions are modeled using log-linear models.
In ad-dition to very flexible feature definitions, the log-linear models bring the advantage of a discrimina-tive model to the task.
We see improvements instory element F-score using these models over unsu-pervised word-alignment models.
Further, the fea-ture definition flexibility allows us to incorporate theunsupervised word-alignment labels into these mod-els, resulting either in fully unsupervised approachesthat perform competitively with the supervised mod-els or in hybrid (supervised) approaches that providethe best performing systems in this study.Our tagging models are able to process word con-fusion networks as inputs and thus improve perfor-mance over using 1-best ASR transcripts in scenar-ios where the speech recognition error rate is high.These improvements carry through to the MCI clas-sification task, making use of features computedfrom the automatic scoring of narrative retelling.One advantage of the word-alignment model isthat such approaches do not require manual anno-tation of the story elements, which is more labor in-tensive than typical manual transcription of speech.Thus, the word-alignment model can exploit largenumbers of retellings in an unsupervised mannerwhen trained on ASR transcripts of the retellings.Controlled experiments here with relatively limitedtraining sets demonstrate that semi-supervised ap-proaches on larger untranscribed sets are likely tobe successful.Finally, experiments with different amounts ofASR adaptation show that both acoustic and lan-guage model adaptations in this domain are effec-tive, yielding scenarios that are competitive withmanual transcription both for detecting story ele-ments as well as for subsequent classification.
Withfull model adaptation to the domain, the 1-besttranscripts improved significantly, and their perfor-mance was found to be at par with WCNs.In future work, we would like to investigate twoquestions left open by these results.
First, word-alignment models can be extended to process ASRlattices or word confusion networks as part of theunsupervised alignment learning algorithm, and in-corporated into our approach.
Second, the con-textual features can be refined (e.g., concatenatedfeatures instead of smoothed features) when largeamounts of training data is available.It is noteworthy to mention that the lexical vari-ants and paraphrasing learned from the data usingautomated method may be useful in refining the clin-ical guidelines for scoring (e.g., allowing additionallexical variants and paraphrasings, or assigning un-equal credits for different story elements to reflectthe difficulty of recollecting them) or to create theguidelines for new languages or stories.AcknowledgmentsThis research was supported in part by NIH awards5K25AG033723-02 and P30 AG024978-05 andNSF awards 1027834, 0958585, 0905095, 0964102and 0826654.
Any opinions, findings, conclusionsor recommendations expressed in this publicationare those of the authors and do not reflect the viewsof the NIH or NSF.
We thank Brian Kingsbury andIBM for the use of their ASR software tools; JeffreyKaye and Diane Howeison for their valuable input;and the clinicians at the Oregon Center for Agingand Technology for their care in collecting the data.219ReferencesChih-Chung Chang and Chih-Jen Lin.
2011.
LIBSVM:A library for support vector machines.
ACM Transac-tions on Intelligent Systems and Technology, 2(27):1?27.Jonathan Fiscus, John Garofolo, Audrey Le, Alvin Mar-tin, greg Sanders, Mark Przybocki, and David Pallett.2007.
2004 spring nist rich transcription (rt-04s)evaluation data.
http://www.ldc.upenn.edu/Catalog/catalogEntry.jsp?catalogId=LDC2007S12.Mark Hall, Eibe Frank, Geoffrey Holmes, BernhardPfahringer, Peter Reutemann, and Ian H. Witten.2009.
The WEKA data mining software: An update.SIGKDD Explorations, 11(1).Brian Kingsbury, Hagen Soltau, George Saon,Stephen M. Chu, Hong-Kwang Kuo, Lidia Mangu,Suman V. Ravuri, Nelson Morgan, and Adam Janin.2011.
The IBM 2009 GALE Arabic speech tran-scription system.
In Proceedings of ICASSP, pages4672?4675.Marit Korkman, Ursula Kirk, and Sally Kemp.
1998.NEPSY: A developmental neuropsychological assess-ment.
The Psychological Corporation, San Antonio.Gakuto Kurata, Nobuyasu Itoh, Masafumi Nishimura,Abhinav Sethy, and Bhuvana Ramabhadran.
2012.Leveraging word confusion networks for named entitymodeling and detection from conversational telephonespeech.
Speech Communication, 54(3):491?502.Maider Lehr, Emily Prud?hommeaux, Izhak Shafran, andBrian Roark.
2012.
Fully automated neuropsycho-logical assessment for detecting mild cognitive impair-ment.
In Interspeech.Percy Liang, Ben Taskar, and Dan Klein.
2006.
Align-ment by agreement.
In Proceedings of HLT-NAACL.Catherine Lord, Michael Rutter, Pamela DiLavore, andSusan Risi.
2002.
Autism Diagnostic ObservationSchedule (ADOS).
Western Psychological Services,Los Angeles.Lidia Mangu, Eric Brill, and Andreas Stolcke.
2000.Finding consensus in speech recognition: Word errorminimization and other applications of confusion net-works.
Computer Speech and Language, 14(4):373?400.Andrew Kachites McCallum.
2002.
Mallet: A machinelearning for language toolkit.
http://mallet.cs.umass.edu.John Morris.
1993.
The clinical dementia rating(CDR): Current version and scoring rules.
Neurology,43:2412?2414.A.
Nordlund, S. Rolstad, P. Hellstrom, M. Sjogren,S.
Hansen, and A. Wallin.
2005.
The Goteborg MCIstudy: Mild cognitive impairment is a heterogeneouscondition.
Journal of Neurology, Neurosurgery andPsychiatry, 76(11):1485?1490.Ronald Petersen, Glenn Smith, Stephen Waring, RobertIvnik, Eric Tangalos, and Emre Kokmen.
1999.
Mildcognitive impairment: Clinical characterizations andoutcomes.
Archives of Neurology, 56:303?308.Daniel Povey, Dimitri Kanevsky, Brian Kingsbury,Bhuvana Ramabhadran, George Saon, and KarthikVisweswariah.
2008.
Boosted mmi for model and fea-ture space discriminative training.
In Proceedings ofICASSP.Emily Prud?hommeaux and Brian Roark.
2011.
Align-ment of spoken narratives for automated neuropsycho-logical assessment.
In Proceedings of ASRU.Emily Prud?hommeaux and Brian Roark.
2012.
Graph-based alignment of narratives for automated neuropsy-chological assessment.
In Proceedings of the NAACL2012 Workshop on Biomedical Natural Language Pro-cessing (BioNLP).Lev Ratinov and Dan Roth.
2009.
Design challengesand misconceptions in named entity recognition.
InEMNLP.Karen Ritchie and Jacques Touchon.
2000.
Mild cogni-tive impairment: Conceptual basis and current noso-logical status.
Lancet, 355:225?228.Fei Sha and Fernando Pereira.
2003.
Shallow pars-ing with conditional random fields.
In Proceedings ofHLT-NAACL.Martha Storandt and Robert Hill.
1989.
Very mild seniledementia of the Alzheimer?s type: II Psychometric testperformance.
Archives of Neurology, 46:383?386.Qing-Song Wang and Jiang-Ning Zhou.
2002.
Retrievaland encoding of episodic memory in normal aging andpatients with mild cognitive impairment.
Brain Re-search, 924:113?115.David Wechsler.
1997.
Wechsler Memory Scale - ThirdEdition.
The Psychological Corporation, San Antonio.Shasha Xie, Keelan Evanini, and Klaus Zechner.
2012.Exploring content features for automated speech scor-ing.
In Proceedings of HLT-NAACL.Xiaonan Zhang, Xiaonan Zhang, Jack Mostow, JackMostow, Nell Duke, Christina Trotochaud, Joseph Va-leri, and Al Corbett.
2008.
Mining free-form spokenresponses to tutor prompts.
In Proceedings of the FirstInternational Conference on Educational Data Min-ing, pages 234?241.220
