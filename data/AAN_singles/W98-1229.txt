III!IIIIIITowards  Language Acqu is i t ion  by an At tent ion-Shar ing  RobotHideki Kozima Akira ItoCommunications Research Laboratory588-2, Iwaoka-cho, Iwaoka, Nishi-ku,Kobe 651-2401, Japan{xkozima, ai}@crl, go.
jpAbstractThis paper describes our preliminary researchon "attention-sharing" in infants' language ac-quisition.
Attention-sharing is the activity ofpaying one's attention to someone lse's atten-tional target.
This enables one to observe oth-ers' sensory-input (what they are perceivingfrom the target) and motor-output (what theyare doing in response to the target).
Beinginspired by lack of attention-sharing i  autis-tic children, we assumed that observation ofothers' behavior by attention-sharing plays anindispensable role in symbol acquisition.
Asa test-bed for attention-sharing, we are de-veloping a robot that can follow people's at-tentional targets by means of monitoring theirgaze-direction.1 IntroductionMachine acquisition of natural anguage is one of themost challenging targets of cognitive science.
As abasis for language acquisition, we deal with acqui-sition of a symbol system, which articulates thingsand events in the world into categories and givesphonological labels to the categories.
The relation-ships between the categories and labels are arbitraryconventions shared by people, so that infants haveto learn them through interaction with people.This paper describes the role of "attention-shar-ing" (Baron-Cohen, 1995), especially that based ongaze, in infants' symbol acquisition.
Figure 1 il-lustrates how attention-sharing is achieved: self (S)captures gaze-direction ofan agent (A), then the selfsearches in the direction and identifies the target(T).
Shared attention spotlights things and eventsbeing mentioned and makes the communication co-herent about the same target.2 Attention-Sharing andSymbol AcquisitionObservation of others' verbal behavior provides in-fants with learning data for symbol acquisition.
Letus consider that an agent, looking at a cat, says"cat", as illustrated in Figure 2.
In order to mimicthis verbal behavior, the self has to observe the(1) capture gazeFigure 1.
(2) identify the targetAttention-sharing based on gaze.0 = "cat"o'1/z' ?Figure 2.
Observing other's verbal behavior.I '  O' I '  O'F igure 3.
Introducing mediators between I/O.agent's ensory-input I (stimulus from the cat) andmotor-output O (verbal response) and to make theassociation between them.Attention-sharing enables us to observe someoneelse's input and output, as also shown in Figure 2.Attention-sharing guarantees that I '  (the self's in-put) resembles I, since both are paying attention tothe same target.
At the gaze-capturing stage (Fig-ure 1, left), the self can observe the agent's outputO and map it onto the self's motor image Oq (Weassume an innate mapping mechanism like imitationof facial gestures by neonates.
)Although thus observed relationships between in-put space and output space may vary in many ways(size, color, tone, volume, etc.
), one can constructan efficient mediator between these spaces.
As illus-trated in Figure 3, the complex relationships (left)Kozima and 1to 245 Language Acquisition by an Attention-Sharing RobotHideki Kozima and Akira Ito (1998) Towards Language Acquisition by an Attentlon-Sharing Robot.
In D.M.W.
Powers (ed.
)NeMLaP3/CoNLL 98: New Methods in Language Processing and Computational Natural Language Learning, ACL, pp 245-246.F igure  4.
The attention-sharing robot.can be decomposed into several (almost separated)components by introducing a hidden mediator spaceM (right), on which "symbols" can emerge.3 Imp l i ca t ions  o f  Aut i smAttention-sharing is commonly seen in infants at thepre-verbal stage.
Its development starts before 6months old, and is completed at around 18 monthsold (Butterworth, 1991).
Also in some non-humanprimates how attention-sharing (Itakura, 1996).Most of infants and children with autism do notshow attention-sharing; being instructed by an ex-perimenter, however, they can do it (Baron-Cohen,1995).
This means they axe unaware that one's gaze-direction implies his or her attentional target.Being unaware of others' attention, children withautism show typical disorders in verbal and non-verbal communication (Frith, 1989).
Most of chil-dren with autism can not acquire language or uselanguage properly.
This is because (1) they failedin observing verbal (and also pragmatic) behaviorof others, and (2) they failed in observing positive/negative feedback for elaborating their hypotheticlanguage models.4 The  At tent ion -Shar ing  RobotWe are developing a robot, Infanoid, as a test-bedfor our model of attention-sharing.
The robot is in-tended to create shared attention with humans interms of monitoring their gaze-direction.The robot has a head, as shown in Figure 4,with four CCD cameras (left/right x zoom/wide)and servo motors to drive the "eyes" at the speedof human saccade.
The images taken by the cam-eras are sent to a workstation for gaze-monitoring.The gaze-monitoring process consists of the fol-lowing tasks, as also shown in Figure 5: (1) detect aface in a scene, (2) saccade to the face and switch tothe zoom cameras, (3) detect eyes and determine thegaze-direction i terms of the position of the pupils,and (4) search for an object in the direction.
If some-thing relevant is found, the robot identifies it as thetarget.We have developed a prototype of the robot andthe real-time face/eye detectors.
W'e are now work-(1) detect a face(3) capture gazeFigure 5.
(2) saccade and zoomN(4) identify the targetGaze-monitoring processing on gaze capturing and target selection.
Our pre-liminary study found that these tasks require sometop-down information like the object's "relevance"(Sperber and Wilson, 1986) to the current context.5 Conc lus ion  and  Future  ResearchWe described our preliminary model of attention-sharing as a device for observing learning data (oth-ers' verbal behavior) for symbol acquisition.The model will work in the bootstrapping stage ofinfants' symbol acquisition; it only deals with refer-ring to physical objects.
Infants at this stage tendto take an unknown label as a category name of aphysical object, and then apply the label to otherobjects with similar shape (Imai, 1997).In future research, we have to fully implementthe gaze-monitoring process and to evaluate it inhuman-robot interaction.
Also we are planningan experiment on evaluating the accuracy of hu-man gaze-monitoring; this will reveal how humansrely on top-down semantic/pragmatic information inattention-sharing.Re ferencesSimon Baron-Cohen.
1995.
Mindblindness: An Es-say on Autism and Theory of Mind, MIT Press.George Butterworth and Nicholas Jarrett.
1991.What minds have in common is space.
BritishJournal of Developmental Psychology, 9:55-72.Uta Frith.
1989.
Autism: Explaining the Enigma,Blackwell.Mutsumi Imai.
1997.
Origins of word-learning prin-ciples.
Cognitive Studies, 4:75-98.
(in Japanese)Shoji Itakura.
1996.
An exploratory study of gaze-monitoring in nonhuman primates.
Japanese Psy-chological Research, 38:174-180.Dan Sperber and Deirdre Wilson.
1986.
Relevance:Communication and Cognition, Blackwell.Kozima and It(> 246 Language Acquisition by an Attention-Sharing RobotII!1IIIIIIII
