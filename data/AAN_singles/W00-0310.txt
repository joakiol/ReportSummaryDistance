Using Dialogue Representations for Concept-to-SpeechGenerationChrist ine H. NakataniJennifer Chu-CarrollAbstractWe present an implemented concept-to-speech(CTS) syst@n'~J tl~at offers original proposals forcertain couplings-oi r dialogue computation withprosodic computation.
Specifically, the semantic in-terpretation, task modeling and dialogue strategymodules in a working spoken dialogue system areused to generate prosodic features to better conveythe meaning of system replies.
The new CTS systemembodies and extends theoretical work on intona-tional meaning in a more general, robust and rigor-ous way than earlier approaches, by reflecting com-positional aspects of both dialogue and intonationinterepretation i  an original computational frame-work for prosodic generation.1 In t roduct ionConversational systems that use speech as the inputand output modality are often realized by architec-tures that decouple speech processing componentsfrom language processing components.
In this pa-per, we show how speech generation can be moreclosely coupled with the dialogue manager of a work-ing mixed-initiative spoken dialogue system.
In par-ticular, we use representations from the semantic in-terpretation, task model and dialogue strategy mod-ules to better communicate the meaning of systemreplies through prosodically appropriate syntheticspeech.While dialogue prosody has been a topic of muchstudy, our implemented concept-to-speech (CTS)system offers original proposals for specific couplingsof dialogue computation with prosodic omputation.Further, it embodies and extends theoretical workon intonational meaning in a more general, robustand rigorous way than earlier CTS systems, in anarchitecture that reflects compositional aspects ofdialogue and intonation interpretation.2 Theoretical FoundationsIn this work, we implement and extend the com-positional theory of intonational meaning proposedby Pierrehumbert and Hirschberg (1986; 1990),who sought o identify correspondences between theBell Laboratories, Lucent Technologies600 Mountain AvenueMurray Hill, NJ 07974 USA{chn I j encc}?research, bell-labs, comGrosz and Sidner (1986) computational model of dis-course interpretation and Pierrehumbert's prosodicgrammar for American English (1980).In the present work, certain aspects of the orig-inal theories are modified and adapted to the ar-chitecture of the dialogue system in which the CTScomponent is embedded.
Below, we present he im-portant fundamental definitions and principles of in-tonation underlying our CTS system.2.1 Intonat ional  SystemIn our CTS system, the prosodic elements that arecomputed are based on the intonational system ofPierrehumbert (1980), who defined a formal lan-guage for describing American English intonationusing the following regular grammar:Inton Phrase ---~ (Interm Phrase) + Bndry ToneInterm Phrase ~ (Pitch Acc)+ Phrase AceMajor phrases, or inlonational phrases, are madeup of one or more minor phrases, or inlermedialephrases.
Melodic movements in intermediate andintonational phrases are in turn expressed by threekinds of tonal elements.
These include six pilch ac-cents: a low pitch excursion (L*), a high pitch excur-sion (H*), or a combination of both low and high ex-cursions (L*+H, L+H*, H*+L, It+L*); two phraseaccents: a high (H-) or low (L-) tonal target thatguides the interpolation of the melodic ontour fromfinal pitch accent o intermediate phrase nding; andtwo boundary lones: a high (H%) or low (L%) tonaltarget that guides interpolation from phrase accentto intonational phrase ending.2.2 Intonat ional  MeaningTheoretical work on intonational meaning has at-tempted to relate the grammatical e ements of Pier-rehumbert's system - -  pitch accent, phrase accentand boundary tone, to interpretive processes at dif-ferent levels of discourse and dialogue structure.Hirschberg and Pierrehumbert (1986) conjecturedthat the absence or presence of accentuation conveysdiscourse focus status, while the tonal properties ofthe accent itself (i.e.
pitch accent ype) convey se-mantic focus information.48MIMIC:User:MIMIC:User:MIMIC:hello this is mimic the movie information systemhow can I help youwhere in hoboken is october sky playingoctober sky is playing at hoboken cinema in hobokencan I help you with anything elsewhen is it playing thereoctober sky is playing at hoboken cinema in hoboken at 3:45pm, 5:50pm, 7:lOpm, and lOpmcan i help you with anything elseFigure 1: A MIMIC dialogue.In later work, pitch accent type was said toexpress whether the accented information was in-tended by the speaker to be "predicated" or not bythe hearer (Pierrehumbert and Hirschberg, 1990).Nompredicated-~forernation was said to bear low-star accentuation (L*-, L*+H, H+L*), while predi-cated information would be marked by high-star ac-cents (H*, L+H*, H*+L).
The theory further statedthat L*+H conveys uncertainty or lack of speakercommitment to the expressed propositional content,while L+H* marks correction or contrast.
The com-plex accent, H*+L, was said to convey that an infer-ence path was required to support he predication;usage of H+L* similarly was said to imply an in-ference path, but did not suggest a predication of amutual belief.
Finally, phrase accents and bound-ary tones were said to reflect aspects of discoursestructure.3 Systems FoundationsOur task is to improve the communicative compe-tence of a spoken dialogue agent, by making re-course to our knowledge of intonational meaning, di-alogue processing and relations between the two.
Ofcourse, a worthwhile CTS system must also outper-form out-of-the-box text-to-speech (TTS) systemsthat may determine prosodic mark-up in linguisti-cally sophisticated ways.
As in (Nakatani, 1998), wetake the prosodic output of an advanced researchsystem that implements he Pierrehumbert theoryof intonation, namely the Bell Labs TTS system,as our baseline xperimental system to be enhancedby CTS algorithms.
We embed the CTS system inMIMIC, a working spoken dialogue system repre-senting state-of-the-art dialogue management prac-tices, to develop CTS algorithms that can be eventu-ally realistically evaluated using task-based perfor-mance metrics.3.1 Dialogue System: Mixed-Init iat iveMovie Information Consultant(MIMIC)The dialogue system whose baseline speech gen-eration capabilities we enhance is the Mixed-Initiative Movie Information Consultant (MIMIC)(Chu-Carroll, 2000).
MIMIC" provides movie list-ing information involving knowledge about towns,theaters, movies and showtimes, as demonstratedin Figure 1.
MIMIC currently utilizes template-driven text generation, and passes on text stringsto a stand-alone TTS system.
In the version ofMIMIC enhanced with concept-to-speech capabili-ties, MIMIC-CTS, contextual knowledge is used tomodify the prosodic features of the slot and fillermaterial in the templates; we are currently integrat-ing the algorithms in MIMIC-CTS with a grammar-driven generation system.
Further details of MIMICare presented in the relevant sections below, but see(Chu-Carroll, 2000) for a complete overview.3.2 TTS: The Bell Labs SystemFor default prosodic processing and speech synthe-sis realization, we use a research version of theBell Labs TTS System, circa 1992 (Sproat, 1997),that generates intonational contours based on Pier-rehumbert's intonation theory (1980), as describedin (Pierrehumbert, 1981).
Of relevance is the factthat various pitch accent ypes, phrase accent andboundary tones in Pierrehumbert's theory are di-rectly implemented in this system, so that by gener-ating a Pierrehumbert-style prosodic transcription,the work of the CTS system is done.
More pre-cisely, MIMIC-CTS computes prosodic annotationsthat override the default prosodic processing that isperformed by the Bell Labs TTS system.To our knowledge, the intonation component ofthe Bell Labs TTS system utilizes more linguisticknowledge to compute prosodic annotations thanany other unrestricted TTS system, so it is reason-able to assume that improvements upon it are mean-ingful in practice as well as in theory.4 MIMIC's Concept-to-SpeechComponent  (MIMIC-CTS)In MIMIC-CTS, the MIMIC dialogue system is en-hanced with a CTS component to better communi-cate the meaning of system replies through contex-tually conditioned prosodic features.
MIMIC-CTSmakes use of three distinct levels of dialogue rep-resentations to convey meaning through intonation.MIMIC's semantic representations allow MIMIC-CTS to decide which information to prosodically49highlight.
MIMIC's task model in turn determineshow to prosodically highlight selected information,based on the pragmatic properties of the systemreply.
MIMIC's dialogue strategy selection processinforms various choices in prosodic contour and ac-centing that convey logico-semantic aspects of mean-ing, such as contradiction.4.1 Highl ighting Informat ion usingSemantic Representat ionsMIMIC employs a statistically-driven semantic in-terpretation engine to "spot" values for key at-tributes that make up a valid MIMIC query in arobust fashion) To simplify matters, for each ut-terance, MIMIC computes an attribute-value ma-trix (AVM)-~epresentation, identifying importantpieces of information for accomplishing a given setof tasks.
The AVM created from the following ut-terance, "When is October Sky playing at HobokenCinema in Hoboken?
", for example, is given in Fig-ure 2.Attribute 11 ValueTaskMovieTheatreTownTimewhenOctober SkyHoboken CinemaHobokenFigure 2: Attribute Value Matrix (AVM), computedby MIMIC's semantic interpreter.Attribute names and attribute values are criticalto the task at hand.
In MIMIC-CTS, attributenames and values that occur in templates are typed,so that MIMIC-CTS can highlight hese items inthe following way:1.
All lexical items realizing attribute values areaccented.2.
Attribute values are synthesized at a slowerspeaking rate.3.
Attribute values are set off by phrase bound-aries.4.
Attribute names are always accented.These modifications are entirely rule-based, given alist of attribute names and typed attribute values.1 Specifically, MIMIC uses an n-dimensional call routerfront-end (Chu-Carroll, 2000), which is a generalization ofthe vector-based call-routing paradigm of semantic interpre-tation (Chu-CarroU and Carpenter, 1999); that is, instead ofdetecting one concept per utterance, MIMIC's semantic in-terpretation engine detects multiple (n) concepts or classesconveyed by a single utterance, by using n call touters inparallel.Even such minimal use of dialogue informationcan make a difference.
For example, changing thedefault accent for the following utterance highlightsthe kind of information that the system is seeking,instead of highlighting the semantically vacuousmain verb, like: 2Default TTS: what movie would you LIKEMIMIC-CTS: what MOVIE would you like4.2 Conveying In format ion Status usingthe Task ModelMIMIC performs a set of information-giving tasks,i.e.
what, where, when, location, that are conciselydefined by a task model.
MIMIC processes theAVM for each utterance and then evaluates whetherit should perform a database query based on thetask specifications given in Figure 3.
The taskmode\] defines which attribute values must be filledin (Y), must not 56 filled in (N), or may optionallybe filled in (-), to "license" a database query action.If no task is "specified" by the current AVM state,Task Movie Theater TownFigure 3: Task Specifications for MIMIC.MIMIC employs various strategies to progresstoward a complete and valid task specification.For example, in response to the follgwing userutterance, MIMIC initiates an information-seekingsubdialogue to instantiate the theater attributevalue to accomplish a when task:User: when is october sky playingin hobokenMIMIC-CTS: what THEATER would you likeTo better convey the structure of the task model,which is learned by the user through interactionwith the system, we define four information statusesbased on properties of the task model, which alignon a scale of given and new in the following order:OLD INFERRABLE KEY HEARER-NEW\[given\] \[new\]KEY information is that which is necessary toformulate a valid database query, and is exchangedand (implicitly or explicitly) confirmed betweenthe system and user.
INFERRABLE information isnot explicitly exchanged between the system and2In the examples, small capitalization denotes a word isaccented.50Task Specification StatusRequired (Y)Optional (-)Not allowed (N)Information Status Pitch AccentKEY L+H*INFERRABLE/OLD L*+H/L*HEARER-NEW H*Table 1: Highlighting relevance of information based on task model (and discourse history).User:MIMIC:where in montclair is analyze this playingana lyze  this  is p lay ing  at we lhnont  theat re  and c learv iews sc reen ing  zonein mont  clairANALYZE THIS is PLAYING at WELLIMONT THEATERLWH* LWH* L* - H* H* L-H%CLEARVIEWS SCREENING ZONE in MONTCLAIR~= - H* H* H* L-H% LWH* L-L%andFigure 4: Above, dialogue excerpt of MIMIC performing a where task.
Below, the modified version of thebold-faced reply string, generated by MIMIC-CTS.user, but is derived by MIMIC's limited inferenceengine that seeks to instantiate as many attributevalues as possible.
For instance, a theater namemay be inferred given a town name, if there is onlyone theater in the given town.
OLD informationis inherited from the discourse history, based onupdating rules relying on confidence scores forattribute values.
HEARER-NEW information (c.f.
(Prince, 1988)) is that which is requested by theuser, and constitutes the only new information onthe scale.
But note that KEY information, whilegiven, is still clearly in discourse focus, along withHEARER-NEW information.The next step is to map the information statuses,ordered from given to new, to a scale of pitchaccent, or accent melodies, ordered from given tonew as follows:L* L*?H L+H* H*&ivan\] \[new\]Table 1 summarizes this original mapping of infor-mation statuses to pitch accent melodies, and Fig-ure 4 illustrates the use of this mapping in an ex-ample.
It obeys the general principle of Pierrehum-bert and Hirschberg's work, that low tonality sig-nifies discourse givenness and high tonality signifiesdiscourse newness, but extends this principle beyondits vague definition in terms of predication of mutualbeliefs.
Instead, the principle is operationalized herein a practically motivated manner that is consistentwith and perhaps illuminating of the theory.4.3 Ass igning "D ia logue  Prosody"  us ingDia logue St rateg iesAs in earlier CTS systems, special logico-semanticrelations, such as contrast or correction, are effec-tively conveyed in MIMIC-CTS by prosodic ues.
InMIMIC-CTS, however, these situations are not stip-ulated in an ad hoc manner, but can be determinedto a large degree by MIMIC's dialogue strategy se-lection process that identifies appropriate dialogueacts to realize a dialogue goal.
aFor example, the dialogue act ANSWER may beselected to achieve the dialogue goal of providing ananswer to a successful user query, while the dialogueact NOTIFYFAILURE may be performed to achievethe dialogue goal of providing an answer in situationswhere no movie listing in the database matches theuser query.
The template associated with the di-alogue act, NOTIFYFAILURE, when compared withthat for ANSWER, contains an additional negativeauxiliary associated with the key attribute responsi-ble for the query failure, in an utterance conveying acontradiction i beliefs between the user and system(namely, the presupposition on the part of the userthat the query can be satisfied).Theoretical work on intonational interpretationleads us to prosodically mark the negative auxil-iary, as well as the associated focus position (Rooth,1985).
We choose to mark the negative auxiliary notwith the L+H* pitch accent to convey correction,while marking the material in the associated focusposition with the L*+H pitch accent to convey (the3Importantly, MIMIC's adaptive dialogue strategy selec-tion algorithm takes into account the outcome of an initia-tive tracking module that we do not discuss here (see (Chu-Carroll, 2000)).51User:MIMIC:where is the corruptor playing in cranfordthe corruptor  is not playing in cranfordthe corruptor is playing at lincoln cinemas in arlingtonTHE CORRUPTOR is NOT playing in CRANFORDL+H* L+H* LWH* !H* L*+H L-H%Figure 5: Above, dialogue xcerpt of MIMIC performing a NOTIFYFAILURE dialogue act.
Below, the modifiedversion of the bold-faced reply string, generated by MIMIC-CTS.
Note the diacritic "!"
denotes a downsteppedaccent (see (Pierrehumbert, 1980)).system's) lack of commitment to the (user's) pre-supposition at hand.
Finally, the NOTIFYFAILUREdialogue act is conveyed by assigning the so-calledrise-fall-risd-cbntfadiction c tour, L*+tt L-H%, tothe utterance-at large (c.f.
(Hirschberg and Ward,1991)).
An example generated by MIMIC-CTS ap-pears in Figure 5.
Note that pitch accent types forthe remaining attribute values are assigned using thetask model, as described in section 4.2.
Thus in Fig-ure 5, the movie title is treated as KEY information,marked by the L+H* pitch accent.MIMIC-CTS contains additional prosodic rulesfor logical connectives, and clarification and confir-mation suhdialogues.5 Related WorkAlthough a number of earlier CTS systems havecaptured linguistic phenomena that we address inour work, the computation ofprosody from dialoguerepresentations is often not as rigorous, detailed orcomplete as in MIMIC-CTS.
Further, while severalsystems use given/new information status to decidewhether to accent or deaccent a lexical item, no sys-tem has directly implemented general rules for pitchaccent type assignment.
Together, MIMIC-CTS'scomputation of accentuation, pitch accent ype anddialogue prosody constitutes the most general andcomplete implementation f a compositional theoryof intonational meaning in a CTS system to date.Nevertheless, elements of a handful of previ-ous CTS systems support the approaches takenin MIMIC-CTS toward conveying semantic, taskand dialogue level meaning.
For example, the Di-rection Assistant system (Davis and Hirschberg,1988) mapped a hand-crafted route grammar to adiscourse structure for generated irections.
Thediscourse structure determined accentuation, withdeaccenting ofdiscourse-old entities realized (by lex-ically identical morphs) in the current or previousdiscourse segment.
Other material was assigned ac-centuation based on lexical category information,with the exception that certain contrastive cases ofaccenting, such as left versus right, were stipulatedfor the domain.Accent assignment in the SUNDIAL travel infor-mation system (House and Yond, 1990) also reliedon discourse and task models.
Mutually known en-tities, said to be in negative focus, were deaccented;entities in the current task space, in referring focus,received (possibly contrastive) accenting; and enti-ties of the same type as a previously mentioned ob-ject, were classified-as in either referring or emphaticfocus, depending on the dialogue act~ in the casesof corrective situations or repeated system-intitiatedqueries, the contrasting or corrective items were em-phatically accented.The BRIDGE project on speech generation(Zacharski etal., 1992) identified four main factorsaffecting accentability: linear 0rder, lexical category,semantic weight and givenness.
In relatedwork(Monaghan, 1994), word accentability was quanti-tatively scored by hand-crafted rules based on infor-mation status, semantic focus and Word class.
Thegivenness hierarchy of Gundel and'colleagues (1989),which associates lexical forms of expression with in-formation statuses, was divided into four intervals,with scores assigned to each.
A binary semantic fo-cus score was based on whether the word occurredin the topic or comment of a sentence.
Finally, lex-ical categories determined word class scores.
Thesescores were combined, and metrical phonologicalrules then referred to final acce'ntability scores toassign a final accenting pattern.To summarize, all of the above CTS systems em-ploy either hand-crafted or heuristic techniques forrepresenting semantic and discourse focus informa-tion.
Further, only SUNDIAL makes use of dialogueacts.6 Conc lus ion  and Future  WorkWe are presently carrying out evaluations ofMIMIC-CTS.
An initial corpus-based analysis comparesthe prosodic annotations assigned to three ac-tual MIMIC dialogues, which were previously col-lected during an overall system evaluation (Chu-Carroll and Nickerson, 2000).
The corpus of di-alogues is made up of 37 system/user turns, in-cluding 40 system-generated sentences.
Three ver-sions of the MIMIC dialogues are being analysed,with prosodic features arising from three differ-52ent sources: MIMIC-CTS, MIMIC operating withdefault Bell Labs TTS, and a professional voicetalent who read the dialogue scripts in context.This corpus-based assessment - -  comparing theprosody of CTS-generated, TTS-generated, and hu-man speech, will enable more domain-dependenttuning of the MIMIC-CTS algorithms, as well as therefinement of general prosodic patterns for linguis-tic structures, uch as lists and conjunctive phrases.Ultimately; the value of MIMIC-CTS must be mea-sured based on its contribution to overall task pefor-mance by real MIMIC users.
Such a study is underdesign, following (Chu-Carroll and Nickerson, 2000).In conclusion, we have shown how prosodic com-putation can be conditioned on various dialoguerepresentations, for robust and domain-independentCTS synthesis.
-While some rules for prosody as-signment depend on the task model, others must betied closely to the particular choices of content inthe replies, at the level of dialogue goals and dia-logue acts.
At this level as well, however, linguis-tic principles of intonation interpretation can be ap-plied to determine the mappings.
In sum, the lessonlearned is that a unitary notion of "concept" fromwhich we generate a unitary prosodic structure, doesnot apply to state-of-the-art spoken dialogue gener-ation.
Instead, the representation f dialogue mean-ing in experimental rchitectures, such as MIMIC's,is compositional tosome degree, and we take advan-tage of this fact to implement a compositional theoryof intonational meaning in a new concept-to-speechsystem, MIMIC-CTS.Re ferencesJennifer Chu-Carroll and Bob Carpenter.
1999.Vector:based natural language call routing.
Com-putational Linguistics, 25(3):361-388.Jennifer Chu-Carroll and Jill S. Nickerson.
2000.Evaluating automatic dialogue strategy adapta-tion for a spoken dialogue system.
In Proceed-ings of the 1st Conference of the North Ameri-can Chapter of the Association for ComputationalLinguistics, Seattle.Jennifer Chu-Carroll.
2000.
Mimic: an adaptivemixed initiative spoken dialogue system for infor-mation queries.
In Proceedings of the 6th Con-ference on Applied Natural Language Processing,Seattle.J.
R. Davis and :l. Hirschberg.
1988.
Assigning into-national features in synthesized spoken directions.In Proceedings of the 26th Annual Meeting of theAssociation for Computational Linguistics, pages187-193, Buffalo.Barbara Grosz and Candace Sidner.
1986.
Atten-tion, intentions, and the structure of discourse.Computational Linguistics, 12(3):175-204.J.
Gundel, N. Hedberg, and R. Zacharski.
1989.Givenness, implicature and demonstrative expres-sions in English discourse.
In Proceedings ofCLS-25, Parasession on Language in Context, pages89-103.
Chicago Linguistics Society.Julia Hirschberg and Janet Pierrehumbert.
1986.The intonational structuring of discourse.
In Pro-ceedings of the 2~lh Annual Meeting of the Asso-ciation for Computational -Linguistics, New York.J.
Hirschberg and G. Ward.
1991.
The influence ofpitch range, duration, amplitude, and spectral fea-tures on the interpretation f l*+h I h%.
Journalof Phonetics.Jill House and Nick Youd.
1990.
Contextually ap-propriate intonation in speech synthesis.
In Pro-ceedings of the European Speech CommunicationAssociation Workshop on Speech Synthesis, pages185-188, Autrans.A.
I. C. Monaghan.
1,994.
Intonation accent place-ment in a concept-to-dialogue system.
In Proceed-ings of the ESCA/IEEE Workshop on Speech Syn-thesis, pages 171-174, New Paltz, NY.C.
H. Nakatani.
1998.
Constituent-based accentprediction.
In Proceedings of the 36th AnnualMeeting of the Association for ComputationalLinguistics, Montreal.J.
Pierrehumbert and J. Hirschberg.
1990.
Themeaning of intonational contours in the interpre-tation of discourse.
In Intentions in Communica-tion.
MIT Press, Cambridge, MA.Janet Pierrehumbert.
1980.
The Phonology andPhonetics of English Intonation.
Ph.D. thesis,Massachusetts Institute of Technology, Septem-ber.
Distributed by the Indiana University Lin-guistics Club.J.
Pierrehumbert.
1981.
Synthesising intonation.Journal of the Acoustical Society of America,70(4):985-995.Ellen Prince.
1988.
The ZPG letter: subjects, defi-niteness, and information status.
In S. Thompsonand W. Mann, editors, Discourse Description: Di-verse Analyses of a Fund Raising Text.
ElsevierScience Publishers, Amsterdam.Mats Rooth.
1985.
Association with Focus.
Ph.D.thesis, University of Massachusetts, Amherst MA.Richard Sproat, editor.
1997.
Multilingual Text-to-Speech Synthesis: The Bell Labs Approach.Kluwer Academic, Boston.Ron Zacharski, A. I. C. Monaghan, D. R. Ladd,and Judy Delin.
1992.
BaIDGE: Basic researchon intonation for dialogue generation.
Technicalreport, University of Edinburgh.53
