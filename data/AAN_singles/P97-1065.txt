Incorporating Context Information for the Extraction of TermsKater ina T. FrantziDept .
of  Comput ingManchester  Met ropo l i tan  Un ivers i tyManchester ,  M1 5GD,  U.K .K.
F rantz i@doc .
mmu.
ac.
ukAbst rac tThe information used for the extraction ofterms can be considered as rather 'inter-nal', i.e.
coming from the candidate stringitself.
This paper presents the incorpora-tion of 'external' information derived fromthe context of the candidate string.
Itis embedded to the C-value approach forautomatic term recognition (ATR), in theform of weights constructed from statisti-cal characteristics of the context words ofthe candidate string.1 In t roduct ion  &: Re la ted  WorkThe applications of term recognition (specialised ic-tionary construction and maintenance, human andmachine translation, text categorization, etc.
), andthe fact that new terms appear with high speed insome domains (e.g.
in computer science), enforce theneed for automating the extraction of terms.
ATRalso gives the potential to work with large amountsof real data, that it would not be able to handle man-ually.
We should note that by ATR we neither meandictionary string matching, nor term interpretation(which deals with the relations between terms andconcepts).Terms may consist of either one or more words.When the aim is the extraction of single-word terms,domain-dependent linguistic information (i.e.
mor-phology) is used (Ananiadou, 1994).
Multi-wordATR usually uses linguistic information in the formof a grammar that mainly allows noun phrases orcompounds to be extracted as candidate terms:(Bourigault, 1992) extracts maximal-length nounphrases and their subgroups (depending on theirgrammatical structure and position) as candidateterms.
(Dagan and Church, 1994), accept sequen-cies of nouns, which give them high precision, butnot such a good recall as that of (Justeson andKatz, 1995), which allow some prepositions (i.e.
oj~to be part of the extracted candidate terms.
(Frantziand Ananiadou, 1996), stand between these two ap-proaches, allowing the extracted compounds to con-tain adjectives but no prepositions.
(Daille et al,1994) also allow adjectives to be part of the two-word English terms they extract.From the above, only (Bourigault, 1992) does notuse any statistical information.
(Justeson and Katz,1995) and (Dagan and Church, 1994) use the fre-quency of occurrence of the candidate string as ameasure of its likelihood to be a term.
(Daille et al,1994) agree that frequency of occurrence "presentsthe best histogram", but also suggest the likeli-hood ratio for the extraction of two-word Englishterms.
(Frantzi and Ananiadou, 1996), besides thefrequency of occurrence, also consider the frequencyof the candidate string as a part of longer candidateterms, as well as the number of these longer candi-date terms it is found nested in.In this paper, we extend C-value, the statisti-cal measure proposed by (Frantzi and Ananiadou,1996), incorporating information gained from thetextual context of the candidate term.2 Context  informat ion fo r  te rmsThe idea of incorporating context information forterm extraction came from that "Extended termunits are different in type from extended word unitsin that they cannot be freely modified" (Sager,1978).
Therefore, information from the modifiersof the candidate strings could be used in the pro-cedure of their evaluation as candidate terms.
Thiscould be extended beyond adjective/noun modifica-tion, to verbs that belong to the candidate string'scontext.
For example, the form shows of the verb toshow in medical domains, is very often followed bya term, e.g.
shows a basal cell carcinoma.
There arecases where the verbs that appear with terms caneven be domain independent, like the form called of501the verb to call, or the form known of the verb toknow, which are often involved in definitions in var-ious areas, e.g.
is known as the singular existentialquantifier, is called the Cartesian product.Since context carries information about terms itshould be involved in the procedure for their ex-traction.
We incorporate context information in theform of weights constructed in a fully automatic way.2.1 The  L inguist ic  Par tThe corpus is tagged, and a linguistic filter will onlyaccept specific part-of-speech sequencies.
The choiceof the linguistic filter affects the precision and re-call of the results: having a 'closed' filter, that is,a strict one regarding the part-of-speech sequenciesit accepts, like the N + that (Dagan and Church,1994) use, wilt improve the precision but have badeffect on the recall.
On the other side, an 'open'filter, one that accepts more part-of-speech sequen-cies, like that of (Justeson and Katz, 1995) that ac-cepts prepositions as well as adjectives and nouns,will have the opposite result.In our choice of the linguistic filter, we lie some-where in the middle, accepting strings consisting ofadjectives and nouns:( N ounlAdjective) + Noun (1)However, we do not claim that this specific fil-ter should be used at all cases, but that its choicedepends on the application: the construction ofdomain-specific dictionaries requires high coverage,and would therefore allow low precision in order toachieve high recall, while when speed is required,high quality would be better appreciated, so thatthe manual filtering of the extracted list of candidateterms can be as fast as possible.
So, in the first casewe could choose an 'open' linguistic filter (e.g.
onethat accepts prepositions), while in the second, a'closed' one (e.g.
one that only accepts nouns).The type of context involved on the extractionof candidate terms is also an issue.
At this stageof this work, the adjectives, nouns and verbs areconsidered.
However, further investigation is neededover the context used (as it is discussed in the futurework).2.2 The  Stat is t ica l  Par tThe procedure involves the following steps:Step 1: The raw corpus is tagged and fromthe tagged corpus the strings that obey the(NounlAdjective)+Noun expression are extracted.Step 2: For these strings, C-value is calculatedresulting in a list of candidate terms (ranked by C-value as their likelihood of being terms).
The lengthof the string is incorporated in the C-value measureresulting to C-value'C-value' (a) -=- Iwherelog2 lalf(a) lal = max, ~,~, ~(b)log2 lal(f(a) - p(ro) )otherwise(2)a is the examined string,lal the length of a in terms of number of words,f(a) the frequency of a in the corpus,Ta the set of candidate terms that contain a,P(T~) the number of these candidate terms.At this point the incorporation of the context in-formation will take place.Step 3: Since C-value is a measure for extract-ing terms, the top of the previously constructed listpresents the higher density on terms among anyother part of the list.
This top of the list, or else,the 'first' of these ranked candidate terms will givethe weights to the context.
We take the top rankedcandidate strings, and from the initial corpus we ex-tract their context which currently are the adjec-tives, nouns and verbs that surround the candidateterm.
For each of these adjectives, nouns and verbs,we consider three parameters:1. its total frequency in the corpus,2.
its frequency as a context word (of the 'first'candidate terms),3. the number of these 'first' candidate terms itappears with.These characteristics are combined in the followingway to assign a weight to the context wordft(w) ) Weight(w) = 0.5(~ -~ + f(w) (3)wherew is the noun/verb/adjective to be assigned aweight,n the number of the 'first' candidate terms consid-ered,t(w) the number of candidate terms the word w ap-pears with,ft(w) w's total frequency appearing with candidateterms,f(w) w's total frequency in the corpus.A variation to improve the results, that involveshuman interaction, is the following: the candidateterms involved for the extraction of context arefirstly manually evaluated, and only the 'real terms'will proceed to the extraction of the context and as-signment of weights (as previously).502At this point a list of context words together withtheir weights has been created.Step 4: The previously created by C-value r list willnow be re-ordered considering the weights obtainedfrom step 3.
For each of the candidate strings of thelist.
its context (adjectives, nouns and verbs thatsurround it) are extracted from the corpus.
Thesecontext words have either been found at step 3 andtherefore assigned a weight, or not.
In the lattercase, they are now assigned weight equal to 0.Each of these candidate strings is now ready to beassigned a context weight which would be the sumof the weights of its context words:wei(a) = Weight(b) + 1 (4)b~C?wherea is the examined n-gram,Ca the context of a,Weight(b) the calculated (from step 3) weight forthe word b.The candidate terms will be now re-ranked accordingto:1 NC.value(a) = ~ C-value'(a) ?
wei(a) (5) tog(.
r)wherea is the examined n-gram,C-value'(a) calculated from step 2,wei(a), the calculated from step 4 sum of the contextweights for a,N the size of the corpus in terms of number of words.3 Future  workOur future work involves1.
The investigation of the context used for theevaluation of the candidate string, and the amountof information that various context carries.
We saidthat for this prototype we considered the adjectives,nouns and verbs that surround the candidate string.However, could ~something else' also carry useful in-formation?
Should adjectives, nouns and verbs allbe considered to carry the same amount of informa-tion, or should they be assigned ifferent weights?2.
The investigation of the assignment of weightson the parameters used for the measures.
Currently,the measures contain the parameters in a 'flat' way.That is, not really considering the 'weight' (the im-portance) of each of them.
So, the measures are atthis point a description of which parameters to beused, and not on the degree to which they should beused.3.
The comparison of this method with other ATRapproaches.
The experimentation real data willshow if this approach actually brings improvement tothe results in comparison with previous approaches.Moreover, the application on real data should covermore than one domains.4 AcknowledgementI thank my supervisors Dr. S. Ananiadou andProf.
J. Tsujii.
Also Dr. T. Sharpe from the Med-ical School of the University of Manchester for theeye-pathology corpus.Re ferencesSophia Ananiadou.
1988.
A Methodology for Auto-matic Term Recognition.
Ph.D Thesis, Universityof Manchester Institute of Science and Technol-ogy.Didier Bourigault.
1992.
Surface GrammaticalAnalysis for the Extraction of TerminologicalNoun Phrases.
In Proceedings of the Interna-tional Conference on Computational Linguistics,COLING-92, pages 977-981.Ido Dagan and Ken Church.
1994.
Termight: Iden-tifying and Translating Technical Terminology.
InProceedings of the European Chapter of the Asso-ciation for Computational Linguistics, EACL-94,pages 34-40.B~atrice Daille, I~ric Gaussier and Jean-Marc Lang,.1994.
Towards Automatic Extraction of Monolin-gual and Bilingual Terminology.
In Proceedingsof the International Conference on ComputationalLinguistics, COLING-94, pages 515-521.Katerina T. Frantzi and Sophia Ananiadou.
1996.A Hybrid Approach to Term Recognition.
In Pro-ceedings of the International Conference on Nat-ural Language Processing and Industrial Applica-tions, NLP+L4-96.
pages 93-98.John S. Justeson and Slava M. Katz.
1995.
Tech-nical terminology: some linguistic properties andan algorithm for identification in text.
In NaturalLanguage Engineering, 1:9-27.Juan C. Sager.
1978.
Commentary in Table Rondesur les Probldmes du Ddcourage du Terme.
Ser-vice des Publications, Direction des Francaise,Montreal, 1979, pages 39-52.503
