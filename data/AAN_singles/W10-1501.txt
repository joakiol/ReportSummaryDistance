Proceedings of the NAACL HLT 2010 Sixth Web as Corpus Workshop, pages 1?7,Los Angeles, California, June 2010. c?2010 Association for Computational LinguisticsNoWaC: a large web-based corpus for NorwegianEmiliano GuevaraTekstlab,Institute for Linguistics and Nordic Studies,University of Osloe.r.guevara@iln.uio.noAbstractIn this paper we introduce the first versionof noWaC, a large web-based corpus of Bok-m?l Norwegian currently containing about700 million tokens.
The corpus has beenbuilt by crawling, downloading and process-ing web documents in the .no top-level in-ternet domain.
The procedure used to col-lect the noWaC corpus is largely based onthe techniques described by Ferraresi et al(2008).
In brief, first a set of ?seed?
URLscontaining documents in the target languageis collected by sending queries to commer-cial search engines (Google and Yahoo).
Theobtained seeds (overall 6900 URLs) are thenused to start a crawling job using the Heritrixweb-crawler limited to the .no domain.
Thedownloaded documents are then processed invarious ways in order to build a linguistic cor-pus (e.g.
filtering by document size, languageidentification, duplicate and near duplicate de-tection, etc.
).1 Introduction and motivationsThe development, training and testing of NLP toolsrequires suitable electronic sources of linguistic data(corpora, lexica, treebanks, ontological databases,etc.
), which demand a great deal of work in or-der to be built and are, very often copyright pro-tected.
Furthermore, the ever growing importance ofheavily data-intensive NLP techniques for strategictasks such as machine translation and informationretrieval, has created the additional requirement thatthese electronic resources be very large and generalin scope.Since most of the current work in NLP is carriedout with data from the economically most impact-ing languages (and especially with English data),an amazing wealth of tools and resources is avail-able for them.
However, researchers interested in?smaller?
languages (whether by the number ofspeakers or by their market relevance in the NLPindustry) must struggle to transfer and adapt theavailable technologies because the suitable sourcesof data are lacking.
Using the web as corpus is apromising option for the latter case, since it can pro-vide with reasonably large and reliable amounts ofdata in a relatively short time and with a very lowproduction cost.In this paper we present the first version ofnoWaC, a large web-based corpus of Bokm?l Nor-wegian, a language with a limited web presence,built by crawling the .no internet top level domain.The computational procedure used to collect thenoWaC corpus is by and large based on the tech-niques described by Ferraresi et al (2008).
Ourinitiative was originally aimed at collecting a 1.5?2billion word general-purpose corpus comparable tothe corpora made available by the WaCky initiative(http://wacky.sslmit.unibo.it).
How-ever, carrying out this project on a language with arelatively small online presence such as Bokm?l haslead to results which differ from previously reportedsimilar projects.
In its current, first version, noWaCcontains about 700 million tokens.11.1 Norwegian: linguistic situation andavailable corporaNorway is a country with a population of ca.
4.8 mil-lion inhabitants that has two official national writtenstandards: Bokm?l and Nynorsk (respectively, ?booklanguage?
and ?new Norwegian?).
Of the two stan-dards, Bokm?l is the most widely used, being ac-tively written by about 85% of the country?s popu-lation (cf.
http://www.sprakrad.no/ for de-tailed up to date statistics).
The two written stan-dards are extremely similar, especially from thepoint of view of their orthography.
In addition, Nor-way recognizes a number of regional minority lan-guages (the largest of which, North Sami, has ca.15,000 speakers).While the written language is generally standard-ized, the spoken language in Norway is not, andusing one?s dialect in any occasion is tolerated andeven encouraged.
This tolerance is rapidly extend-ing to informal writing, especially in modern meansof communication and media such as internet fo-rums, social networks, etc.There is a fairly large number of corpora of theNorwegian language, both spoken and written (inboth standards).
However, most of them are of alimited size (under 50 million words, cf.
http://www.hf.uio.no/tekstlab/ for an overview).To our knowledge, the largest existing written cor-pus of Norwegian is the Norsk Aviskorpus (Hofland2000, cf.
http://avis.uib.no/), an expand-ing newspaper-based corpus currently containing700 million words.
However, the Norsk Aviskor-pus is only available though a dedicated web inter-face for non commercial use, and advanced researchtasks cannot be freely carried out on its contents.Even though we have only worked on building aweb corpus for Bokm?l Norwegian, we intend toapply the same procedures to create web-corporaalso for Nynorsk and North Sami, thus covering thewhole spectrum of written languages in Norway.1.2 Obtaining legal clearanceThe legal status of openly accessible web-documents is not clear.
In practice, when onevisits a web page with a browsing program, anelectronic exact copy of the remote document iscreated locally; this logically implies that any onlinedocument must be, at least to a certain extent,copyright-free if it is to be visited/viewed at all.This is a major difference with respect to other typesof documents (e.g.
printed materials, films, musicrecords) which cannot be copied at all.However, when building a web corpus, we do notonly wish to visit (i.e.
download) web documents,but we would like to process them in various ways,index them and, finally, make them available to otherresearchers and users in general.
All of this wouldideally require clearance from the copyright holdersof each single document in the corpus, somethingwhich is simply impossible to realize for corporathat contain millions of different documents.1In short, web corpora are, from the legal point ofview, still a very dark spot in the field of computa-tional linguistics.
In most countries, there is simplyno legal background to refer to, and the internet is asort of no-man?s land.Norway is a special case: while the law explicitlyprotects online content as intellectual property,there is rather new piece of legislation in Forskrifttil ?ndsverkloven av 21.12 2001 nr.
1563, ?
1-4that allows universities and other research insti-tutions to ask for permission from the Ministryof Culture and Church in order to use copyrightprotected documents for research purposes thatdo not cause conflict with the right holders?own use or their economic interests (cf.
http://www.lovdata.no/cgi-wift/ldles?ltdoc=/for/ff-20011221-1563.html).We have been officially granted this permission forthis project, and we can proudly say that noWaCis a totally legal and recognized initiative.
Theresults of this work will be legally made availablefree of charge for research (i.e.
non commercial)purposes.
NoWaC will be distributed in associationwith the WaCky initiative and also directly from theUniversity of Oslo.1Search engines are in a clear contradiction to the copyrightpolicies in most countries: they crawl, download and index bil-lions of documents with no clearance whatsoever, and also re-distribute whole copies of the cached documents.22 Building a corpus of Bokm?l byweb-crawling2.1 Methods and toolsIn this project we decided to follow the methodsused to build the WaCky corpora, and to use the re-lated tools as much as possible (e.g.
the BootCaTtools).
In particular, we tried to reproduce the pro-cedures described by Ferraresi et al (2008) and Ba-roni et al (2009).
The methodology has alreadyproduced web-corpora ranging from 1.7 to 2.6 bil-lion tokens (German, Italian, British English).
How-ever, most of the steps needed some adaptation, fine-tuning and some extra programming.
In particu-lar, given the relatively complex linguistic situationin Norway, a step dedicated to document languageidentification was added.In short, the building and processing chain usedfor noWaC comprises the following steps:1.
Extraction of list of mid-frequency Bokm?lwords from Wikipedia and building querystrings2.
Retrieval of seed URLs from search engines bysending automated queries, limited to the .notop-level domain3.
Crawling the web using the seed URLS, limitedto the .no top-level domain4.
Removing HTML boilerplate and filtering doc-uments by size5.
Removing duplicate and near-duplicate docu-ments6.
Language identification and filtering7.
Tokenisation8.
POS-taggingAt the time of writing, the first version of noWaC isbeing POS-tagged and will be made available in thecourse of the next weeks.2.2 Retrieving seed URLs from search enginesWe started by obtaining the Wikipedia text dumpsfor Bokm?l Norwegian and related languages(Nynorsk, Danish, Swedish and Icelandic) and se-lecting the 2000 most frequent words that are uniqueto Bokm?l.
We then sent queries of 2 randomlyselected Bokm?l words though search engine APIs(Google and Yahoo!).
A maximum of ten seedURLs were saved for each query, and the retrievedURLs were collapsed in a single list of root URLs,deduplicated and filtered, only keeping those in the.no top level domain.After one week of automated queries (limitedto 1000 queries per day per search engine by therespective APIs) we had about 6900 filtered seedURLs.2.3 CrawlingWe used the Heritrix open-source, web-scalecrawler (http://crawler.archive.org/)seeded with the 6900 URLs we obtained to tra-verse the internet .no domain and to downloadonly HTML documents (all other document typeswere discarded from the archive).
We instructedthe crawler to use a multi-threaded breadth-firststrategy, and to follow a very strict politeness policy,respecting all robots.txt exclusion directiveswhile downloading pages at a moderate rate (90second pause before retrying any URL) in order notto disrupt normal website activity.The final crawling job was stopped after 15 days.In this period of time, a total size of 1 terabytewas crawled, with approximately 90 million URLsbeing processed by the crawler.
Circa 17 millionHTML documents were downloaded, adding up toan overall archive size of 550 gigabytes.
Onlyabout 13.5 million documents were successfully re-trieved pages (the rest consisting of various ?pagenot found?
replies and other server-side error mes-sages).The documents in the archive were filtered bysize, keeping only those documents that were be-tween 5Kb and 200Kb in size (following Ferraresiet al 2008 and Baroni et al 2009).
This resultedin a reduced archive of 11.4 million documents forpost-processing.2.4 Post-processing: removing HTMLboilerplate and de-duplicationAt this point of the process, the archive containedraw HTML documents, still very far from being alinguistic corpus.
We used the BootCaT toolkit (Ba-roni and Bernardini 2004, cf.
http://sslmit.unibo.it/~baroni/bootcat.html) to per-form the major operations to clean our archive.First, every document was processed with theHTML boilerplate removal tool in order to select3only the linguistically interesting portions of textwhile removing all HTML, Javascript and CSS codeand non-linguistic material (made mainly of HTMLtags, visual formatting, tables, navigation links, etc.
)Then, the archive was processed with the dupli-cate and near-duplicate detecting script in the theBootCaT toolkit, based on a 5-gram model.
This isa very drastic strategy leading to a huge reduction inthe number of kept documents: any two documentssharing more than 1/25 5-grams were considered du-plicates, and both documents were discarded.
Theoverall number of documents in the archive wentdown from 11.40 to 1.17 million after duplicate re-moval.22.5 Language identification and filteringThe complex linguistic situation in Norway makesus expect that the Norwegian internet be at least abilingual domain (Bokm?l and Nynorsk).
In addi-tion, we also expect a number of other languages tobe present to a lesser degree.We used Damir Cavar?s tri-gram algorithmfor language identification (cf.
http://ling.unizd.hr/~dcavar/LID/), training 16 lan-guage models onWikipedia text from languages thatare closely related to, or that have contact with Bok-m?l (Bokm?l, Danish, Dutch, English, Faeroese,Finnish, French, German, Icelandic, Italian, North-ern Sami, Nynorsk, Polish, Russian, Spanish andSwedish).
The best models were trained on 1Mbof random Wikipedia lines and evaluated against adatabase of one hundred 5 Kb article excerpts foreach language.
The models performed very well,often approaching 100% accuracy; however, the ex-tremely similar orthography of Bokm?l and Nynorskmake them the most difficult pair of languages tospot for the system, one being often misclassified asthe other.
In any case, our results were relativelygood: Bokm?l Precision = 1.00, Recall = 0.89, F-measure = 0.94, Nynorsk Precision = 0.90, Recall =1.00, F-measure = 0.95.The language identifying filter was applied on adocument basis, recognizing about 3 out of 4 docu-2As pointed out by an anonymous reviewer, this drastic re-duction in number of documents may be due to faults in theboilerplate removal phase, leading to 5-grams of HTML or sim-ilar code counting as real text.
We are aware of this issue, andthe future versions of noWaC will be revised to this effect.ments as Bokm?l:?
72.25% Bokm?l?
16.00% Nynorsk?
05.80% English?
02.43% Danish?
01.95% SwedishThis filter produced another sensible drop in theoverall number of kept documents: from 1.17 to 0.85million.2.6 POS-tagging and lemmatizationAt the time of writing noWaC is in the process of be-ing POS-tagged.
This is not at all an easy task, sincethe best and most widely used tagger for Norwe-gian (the Oslo-Bergen tagger, cf.
Hagen et al 2000)is available as a binary distribution which, besidesnot being open to modifications, is fairly slow anddoes not handle large text files.
A number of statisti-cal taggers have been trained, but we are still unde-cided about which system to use because the avail-able training materials for Bokm?l are rather lim-ited (about 120,000 words).
The tagging accuracywe have obtained so far is still not comparable tothe state-of-the-art (94.32% with TnT, 94.40% withSVMT).
In addition, we are also working on creat-ing a large list of tagged lemmas to be used withnoWaC.
We estimate that a final POS-tagged andlemmatized version of the corpus will be availablein the next few weeks (in any case, before the WAC6workshop).3 Comparing resultsWhile it is still too early for us to carry out afully fledged qualitative evaluation of noWaC, weare able to compare our results with previous pub-lished work, especially with the WaCky corpora wetried to emulate.3.1 NoWaC and the WaCky corporaAs we stated above, we tried to follow the WaCkymethodology as closely as possible, in the hopes thatwe could obtain a very large corpus (we aimed atcollecting above 1 billion tokens).
However, eventhough our crawling job produced a much biggerinitial archive than those reported for German, Ital-ian and British English in Baroni et al (2009), and4even though after document size filtering was ap-plied our archive contained roughly twice as manydocuments, our final figures (number of tokens andnumber of documents) only amount to about half thesize reported for the WaCky corpora (cf.
table 1).In particular, we observe that the most significantdrop in size and in number of documents took placeduring the detection of duplicate and near-duplicatedocuments (drastically dropping from 11.4 milliondocuments to 1.17 million documents after duplicatefiltering).
This indicates that, even if a huge num-ber of documents in Bokm?l Norwegian are presentin the internet, a large portion of them must bemachine generated content containing repeated n-grams that the duplicate removal tool successfullyidentifies and discards.3These figures, although unexpected by us, mayactually have a reasonable explanation.
If we con-sider that Bokm?l Norwegian has about 4.8 millionpotential content authors (assuming that every Nor-wegian inhabitant is able to produce web documentsin Bokm?l), and given that our final corpus contains0.85 million documents, this means that we have sofar sampled roughly one document every five poten-tial writers: as good as it may sound, it is a highlyunrealistic projection, and a great deal of noise andpossibly also machine generated content must stillbe present in the corpus.
The duplicate removal toolsare only helping us understand that a speaker com-munity can only produce a limited amount of lin-guistically relevant online content.
We leave the in-teresting task of estimating the size of this contentand its growth rate for further research.
The Norwe-gian case, being a relatively small but highly devel-oped information society, might prove to be a goodstarting point.3.2 Scaling noWaC: how much Bokm?l isthere?
How much did we get?The question arises immediately.
We want to knowhow representative our corpus is, in spite of the factthat we now know that it must still contain a greatdeal of noise and that a great deal of documents wereplausibly not produced by human speakers.To this effect, we applied the scaling factors3Although we are aware that the process of duplicate re-moval in noWaC must be refined further, constituting in itselfan interesting research area.methodology used by Kilgarrif (2007) to estimatethe size of the Italian and German internet on thebasis of the WaCky corpora.
The method consistsin comparing document hits for a sample of mid-frequency words in Google and in our corpus beforeand after duplicate removal.
The method assumesthat Google does indeed apply duplicate removal tosome extent, though less drastically than we have.Cf.
table 2 for some example figures.From this document hit comparison, two scalingfactors are extracted.
The scaling ratio tells us howmuch smaller our corpus is compared to the Googleindex for Norwegian (including duplicates and non-running-text).
The duplicate ratio gives us an ideaof how much duplicated material was found in ourarchive.Since we do not know exactly how much dupli-cate detection Google performs, we will multiplythe duplicate ratio by a weight of 0.1, 0.25 and 0.5(these weights, in turn, assume that Google discards10 times less, 4 times less and half what our dupli-cate removal has done ?
the latter hypothesis is usedby Kilgarriff 2007).?
Scaling ratio (average):Google frq.
/ noWaC raw frq.
= 24.9?
Duplicate ratio (average):noWaC raw frq.
/ dedup.
frq.
= 7.8We can then multiply the number of tokens in ourfinal cleaned corpus by the scaling ratio and by theduplicate ratio (weighted) in order to obtain a roughestimate of how much Norwegian text is containedin the Google index.
We can also estimate howmuchof this amount is present in noWaC.
Cf.
table 3.Using exactly the same procedure as Kilgarrif(2007) leads us to conclude that noWaC shouldcontain over 15% of the Bokm?l text indexed byGoogle.
A much more restrictive estimate gives usabout 3%.
More precise estimates are extremelydifficult to make, and these results should be takenonly as rough approximations.
In any case, noWaCcertainly is a reasonably representative web-corpuscontaining between 3% and 15% of all the currentlyindexed online Bokm?l (Kilgarriff reports an esti-mate of 3% for German and 7% for Italian in theWaCky corpora).5deWaC itWaC ukWaC noWaCN.
of seed pairs 1,653 1,000 2,000 1,000N.
of seed URLs 8,626 5,231 6,528 6,891Raw crawl size 398GB 379GB 351GB 550GBSize after document size filter 20GB 19GB 19GB 22GBN.
of docs after document size filter 4.86M 4.43M 5.69M 11.4MSize after near-duplicate filter 13GB 10GB 12GB 5GBN.
of docs after near-duplicate filter 1.75M 1.87M 2.69M 1.17MN.
of docs after lang-ID ?
?
?
0.85MN.
of tokens 1.27 Bn 1.58 Bn 1.91 Bn 0.69 BnN.
of types 9.3M 3.6M 3.8M 6.0MTable 1: Figure comparison of noWaC and the published WaCky corpora (German, Italian and British English datafrom Baroni et al 2009)Word Google frq.
noWaC raw frq.
noWaC dedup.
frq.bilavgifter 33700 1637 314mekanikk 82900 3266 661musikkpris 16700 570 171Table 2: Sample of Google and noWaC document frequencies before and after duplicate removal.noWaC Scaling ratio Dup.
ratio (weight) Google estimate % in noWaC0.78 (0.10) 21.8 bn 3.15%0.69 bn 24.9 1.97 (0.25) 8.7 bn 7.89%3.94 (0.50) 4.3 bn 15.79%Table 3: Estimating the size of the Bokm?l Norwegian internet as indexed by Google in three different settings (methodfrom Kilgarriff 2007)4 Concluding remarksBuilding large web-corpora for languages with arelatively small internet presence and with a lim-ited speaker population presents problems and chal-lenges that have not been found in previous work.In particular, the amount of data that can be col-lected with similar efforts is considerably smaller.
Inour experience, following as closely as possible theWaCky corpora methodology yielded a corpus thatis roughly between one half and one third the size ofthe published comparable Italian, German and En-glish corpora.In any case, the experience has been very success-ful so far, and the first version of the noWaC cor-pus is about the same size than the largest currentlyavailable corpus of Norwegian (i.e.
Norske Avisko-rpus, 700 million tokens), and it has been created injust a minimal fraction of the time it took to build it.Furthermore, the scaling experiments showed thatnoWaC is a very representative web-corpus contain-ing a significant portion of all the online content inBokm?l Norwegian, in spite of our extremely drasticcleaning and filtering strategies.There is clearly a great margin for improvementin almost every processing step we applied in thiswork.
And there is clearly a lot to be done in or-der to qualitatively assess the created corpus.
In thefuture, we intend to pursue this activity by carryingout an even greater crawling job in order to obtain alarger corpus, possibly containing over 1 billion to-kens.
Moreover, we shall reproduce this corpus cre-ation process with the remaining two largest writtenlanguages of Norway, Nynorsk and North Sami.
Allof these resources will soon be publicly and freelyavailable both for the general public and for the re-search community.6AcknowledgementsBuilding noWaC has been possible thanks to NO-TUR advanced user support and assistance from theResearch Computing Services group (VitenskapeligDatabehandling) at USIT, University of Oslo.
Manythanks are due to Eros Zanchetta (U. of Bologna),Adriano Ferraresi (U. of Bologna) and Marco Ba-roni (U. of Trento) and two anonymous reviewersfor their helpful comments and help.ReferencesM.
Baroni and S. Bernardini.
2004.
Bootcat: Bootstrap-ping corpora and terms from the web.
In Proceedingsof LREC 2004, pages 1313?1316, Lisbon.
ELDA.Marco Baroni, Silvia Bernardini, Adriano Ferraresi, andEros Zanchetta.
2009.
The wacky wide web: acollection of very large linguistically processed web-crawled corpora.
Language Resources and Evalua-tion, 43(3):209?226, 09.David Crystal.
2001.
Language and the Internet.
Cam-bridge University Press, Cambridge.A.
Ferraresi, E. Zanchetta, M. Baroni, and S. Bernardini.2008.
Introducing and evaluating ukWaC, a very largeweb-derived corpus of English.
In Proceedings of theWAC4 Workshop at LREC 2008.R.
Ghani, R. Jones, and D. Mladenic.
2001.
Mining theweb to create minority language corpora.
In Proceed-ings of the tenth international conference on Informa-tion and knowledge management, pages 279?286.Johannessen J.B. N?klestad A. Hagen, K. 2000.
Aconstraint-based tagger for norwegian.
Odense Work-ing Papers in Language and Communication, 19(I).Knut Hofland.
2000.
A self-expanding corpus based onnewspapers on the web.
In Proceedings of the Sec-ond International Language Resources and EvaluationConference, Paris.
European Language Resources As-sociation.Adam Kilgarriff and Marco Baroni, editors.
2006.
Pro-ceedings of the 2nd International Workshop on theWebas Corpus (EACL 2006 SIGWACWorkshop).
Associa-tion for Computational Linguistics, East Stroudsburg,PA.Adam Kilgarriff and Gregory Grefenstette.
2003.
In-troduction to the special issue on the web as corpus.Computational Linguistics, 29(3):333?348.A.
Kilgarriff.
2007.
Googleology is bad science.
Com-putational Linguistics, 33(1):147?151.S.
Sharoff.
2005.
Open-source corpora: Using the net tofish for linguistic data.
International Journal of Cor-pus Linguistics, (11):435?462.7
