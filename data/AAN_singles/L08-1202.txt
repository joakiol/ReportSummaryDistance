Combining Multiple Models for Speech Information RetrievalMuath Alzghool and Diana InkpenSchool of Information Technology and EngineeringUniversity of Ottawa{alzghool,diana}@ site.uottawa.caAbstractIn this article we present a method for combining different information retrieval models in order to increase the retrieval performancein a Speech Information Retrieval task.
The formulas for combining the models are tuned on training data.
Then the system is evaluatedon test data.
The task is particularly difficult because the text collection is automatically transcribed spontaneous speech, with manyrecognition errors.
Also, the topics are real information needs, difficult to satisfy.
Information Retrieval systems are not able to obtaingood results on this data set, except for the case when manual summaries are included.1.
IntroductionConversational speech such as recordings of interviews orteleconferences is difficult to search through.
Thetranscripts produced with Automatic Speech Recognition(ASR) systems tend to contain many recognition errors,leading to low Information Retrieval (IR) performance(Oard et al, 2007).Previous research has explored the idea of combiningthe results of different retrieval strategies; the motivation isthat each technique will retrieve different sets of relevantdocuments; therefore combining the results could producea better result than any of the individual techniques.
Wepropose new data fusion techniques for combining theresults of different IR models.
We applied our data fusiontechniques to the Mallach collection (Oard et al, 2007)used in the Cross-Language Speech Retrieval (CLSR) taskat Cross-Language Evaluation Forum (CLEF) 2007.
TheMallach collection comprises 8104 ?documents?
which aremanually-determined topically-coherent segments takenfrom 272 interviews with Holocaust survivors, witnessesand rescuers, totalling 589 hours of speech.
Figure 1 showsthe document structure in CLSR test collection, two ASRtranscripts are available for this data, in this work we usethe ASRTEXT2004A field provided by IBM research witha word error rate of 38%.
Additionally, metadata fields foreach document include: two sets of 20 automaticallyassigned keywords determined using two different kNNclassifiers (AK1 and AK2), a set of a varying number ofmanually-assigned keywords (MK), and a manual3-sentence summary written by an expert in the field.
A setof 63 training topics and 33 test topics were generated forthis task.
The topics provided with the collection werecreated in English from actual user requests.
Topics werestructured using the standard TREC format of Title,Description and Narrative fields.
To enable CL-SRexperiments the topics were translated into Czech, German,French, and Spanish by native speakers; Figure 2 and 3show two examples for English and its translation inFrench respectively.
Relevance judgments were generatedusing a search-guided procedure and standard poolingmethods.
See (Oard et al, 2004) for full details of thecollection design.We present results on the automatic transcripts forEnglish queries and translated queries (cross-language)for two combination methods; we also present resultswhen manual summaries and manual keywords areindexed.<DOC><DOCNO>VHF[IntCode]-[SegId].
[SequenceNum]</DOCNO\><INTERVIEWDATA>Interviewee name(s) andbirthdate</INTERVIEWDATA><NAME>Full name of every person mentioned</NAME><MANUALKEYWORD>Thesaurus keywords assigned to thesegment</MANUALKEYWORD><SUMMARY>3-sentence segment summary</SUMMARY><ASRTEXT2004A>ASR transcript produced in2004</ASRTEXT2004A><ASRTEXT2006A>ASR transcript produced in2006</ASRTEXT2006A><AUTOKEYWORD2004A1>Thesaurus keywords from a kNNclassifier</AUTOKEYWORD2004A1><AUTOKEYWORD2004A2>Thesaurus keywords from a secondkNN classifier</AUTOKEYWORD2004A2></DOC>Figure 1.
Document structure in CL-SR test collection.<top><num>1159<title>Child survivors in Sweden<desc>Describe survival mechanisms of children bornin 1930-1933 who spend the war in concentrationcamps or in hiding and who presently live in Sweden.<narr>The relevant material should describe thecircumstances and inner resources of the survivingchildren.
The relevant material also describes howthe wartime experience affected their post-waradult life.
</top>Figure 2.
Example for English topic in CL-SR test collection.<top><num>1159<title>Les enfants survivants en Su?de<desc>Descriptions des m?canismes de survie desenfants n?s entre 1930 et 1933 qui ont pass?
laguerre en camps de concentration ou cach?s et quivivent actuellement en Su?de.<narr>?</top>Figure 3.
Example for French topic in CL-SR test collection.2.
System DescriptionOur Cross-Language Information Retrieval systemswere built with off-the-shelf components.
For the retrievalpart, the SMART (Buckley, Salton, &Allan, 1992; Salton&Buckley, 1988) IR system and the Terrier (Amati &VanRijsbergen, 2002; Ounis et al, 2005) IR system weretested with many different weighting schemes forindexing the collection and the queries.SMART was originally developed at CornellUniversity in the 1960s.
SMART is based on the vectorspace model of information retrieval.
We use the standardnotation: weighting scheme for the documents, followedby dot, followed by the weighting scheme for the queries,each term-weighting scheme is described as acombination of term frequency, collection frequency, andlength normalization components where the schemes areabbreviated according to its components variations (n nonormalization, c cosine, t idf, l log, etc.)
We used nnn.ntn,ntn.ntn, lnn.ntn, ann.ntn, ltn.ntn, atn.ntn, ntn.nnn ,nnc.ntc, ntc.ntc, ntc.nnc, lnc.ntc, anc.ntc, ltc.ntc, atc.ntcweighting schemes (Buckley, Salton, &Allan, 1992;Salton &Buckley, 1988);  lnn.ntn performs very well inCLEF-CLSR 2005 and 2006 (Alzghool &Inkpen, 2007;Inkpen, Alzghool, &Islam, 2006); lnn.ntn means that lnnwas used for documents and ntn for queries according tothe following formulas:0.1)ln(nln += tfweight        (1)tnNtfweight logntn ?=     (2)where tf denotes the term frequency of a term t in thedocument or query, N denotes the number of documentsin the collection, and nt denotes the number of documentsin which the term t occurs.Terrier was originally developed at the University ofGlasgow.
It is based on Divergence from Randomnessmodels (DFR) where IR is seen as a probabilistic process(Amati &Van Rijsbergen, 2002; Ounis et al, 2005).
Weexperimented with the In_expC2 (Inverse ExpectedDocument Frequency model with Bernoulli after-effectand normalization) weighting model, one of Terrier?sDFR-based document weighting models.Using the In_expC2 model, the relevance score of adocument d for a query q is given by the formula:(3) ?
?=qtdtwqtfqdsim ),(.
),(where qtf is the frequency of term t in the query q, and w(t,d)is the relevance score of a document d for the query term t,given by:)5.01log())1(1(),( 2 ++?
?+?+=eeet nNtfntfnnFdtw   (4)where-F is the term frequency of t in the whole collection.-N is the number of document in the whole collection.-nt is the document frequency of t.-ne is given by ))1(1( Fte NnNn??
?=  (5)- tfne is the normalized within-document frequency of theterm t in the document d. It is given by the normalization 2(Amati &Van Rijsbergen, 2002; Ounis et al, 2005):)_1(logllavgctftfn ee ?+?=     (6)where c is a parameter, tf is the within-documentfrequency of the term t in the document d, l is thedocument length, and avg_l is the average documentlength in the whole collection.We estimated the parameter c of the Terrier'snormalization 2 formula by running some experiments onthe training data, to get the best values for c depending onthe topic fields used.
We obtained the following values:c=0.75 for queries using the Title only, c=1 for queriesusing the Title and Description fields, and c=1 for queriesusing the Title, Description, and Narrative fields.
We selectthe c value that has a best MAP score according to thetraining data.For translating the queries from French and Spanishinto English, several free online machine translation toolswere used.
The idea behind using multiple translations isthat they might provide more variety of words andphrases, therefore improving the retrieval performance.Seven online MT systems (Inkpen, Alzghool, &Islam,2006) were used for translating from Spanish and fromFrench into English.
We combined the outputs of the MTsystems by simply concatenating all the translations.
Allseven translations of a title made the title of the translatedquery; the same was done for the description and narrativefields.We propose two methods for combining IR models.
Weuse the sum of normalized weighted similarity scores of 15different IR schemes as shown in the following formulas:??
?+=schemsIRiiMAPr NormSimiWiWFusion )]()([134      (7)??
?=schemsIRiiMAPr NormSimiWiWFusion )(*)(234      (8)where Wr(i) and WMAP(i) are experimentally determinedweights based on the recall (the number of relevantdocuments retrieved) and precision (MAP score) values foreach IR scheme computed on the training data.
Forexample, suppose that two retrieval runs r1 and r2 give 0.3and 0.2 (respectively) as  MAP scores on training data; wenormalize these scores by dividing them by the maximumMAP value: then WMAP(r1) is 1 and WMAP(r2) is 0.66 (thenwe compute the power 3 of these weights, so that oneweight stays 1 and the other one decreases; we chose power3 for MAP score and power 4 for recall, because the MAPis more important than the recall).
We hope that when wemultiply the similarity values with the weights and take thesummation over all the runs, the performance of thecombined run will improve.
NormSimi is the normalizedsimilarity for each IR scheme.
We did the normalization bydividing the similarity by the maximum similarity in therun.
The normalization is necessary because differentweighting schemes will generate different range ofsimilarity values, so a normalization method shouldapplied to each run.
Our method is differed than the workdone by Fox and Shaw in (1994), and Lee in ( 1995); theycombined the results by taking the summation of thesimilarity scores without giving any weight to each run.
Inour work we weight each run according to the precisionand recall on the training data.3.
Experimental ResultsWe applied the data fusion methods described in section 2to 14 runs produced by SMART and one run produced byTerrier.
Performance results for each single run and fusedruns are presented in Table 1, in which % change is givenwith respect to the run providing better effectiveness ineach combination on the training data.
The ManualEnglish column represents the results when only themanual keywords and the manual summaries were usedfor indexing the documents using English topics, theAuto-English column represents the results whenautomatic transcripts are indexed from the documents, forEnglish topics.
For cross-languages experiments theresults are represented in the columns Auto-French, andAuto-Spanish, when using the combined translationsproduced by the seven online MT tools, from French andSpanish into English.
Since the result of combinedtranslation for each language was better than when usingindividual translations from each MT tool on the trainingdata (Inkpen, Alzghool, &Islam, 2006), we used only thecombined translations in our experiments.Data fusion helps to improve the performance (MAPscore) on the test data.
The best improvement using datafusion (Fusion1) was on the French cross-languageexperiments with 21.7%, which is statistically significantwhile on monolingual the improvement was only 6.5%which is not significant.
We computed theseimprovements relative to the results of the bestsingle-model run, as measured on the training data.
Thissupports our claim that data fusion improves the recall bybringing some new documents that were not retrieved byall the runs.
On the training data, the Fusion2 methodgives better results than Fusion1 for all cases except onManual English, but on the test data Fusion1 is better thanFusion2.
In general, the data fusion seems to help,because the performance on the test data in not alwaysgood for weighting schemes that obtain good results onthe training data, but combining models allows thebest-performing weighting schemes to be taken intoconsideration.The retrieval results for the translations from Frenchwere very close to the monolingual English results,especially on the training data, but on the test data thedifference was significantly worse.
For Spanish, thedifference was significantly worse on the training data,but not on the test data.Experiments on manual keywords and manualsummaries available in the test collection showed highimprovements, the MAP score jumped from 0.0855 to0.2761 on the test data.4.
ConclusionWe experimented with two different systems: Terrierand SMART, with combining the various weightingschemes for indexing the document and query terms.
Weproposed two methods to combine different weightingscheme from different systems, based on weightedsummation of normalized similarity measures; the weightfor each scheme was based on the relative precision andrecall on the training data.
Data fusion helps to improvethe retrieval significantly for some experiments(Auto-French) and for other not significantly (ManualEnglish).
Our result on automatic transcripts for Englishqueries (the required run for the CLSR task at CLEF2007), obtained a MAP score of 0.0855.
This result wassignificantly better than the other 4 systems thatparticipated in the CLSR task at CLEF 2007(Pecina et al,2007).In future work we plan to investigate more methods ofdata fusion (to apply a normalization scheme scalable tounseen data), removing or correcting some of the speechrecognition errors in the ASR content words, and to usespeech lattices for indexing.5.
ReferencesAlzghool, M. & Inkpen, D. (2007).
Experiments for thecross language speech retrieval task at CLEF 2006.
InC. Peters, (Ed.
), Evaluation of multilingual andmulti-modal information retrieval (Vol.
4730/2007,pp.
778-785).
Springer.Amati, G. & Van Rijsbergen, C. J.
(2002).
Probabilisticmodels of information retrieval based on measuringthe divergence from randomness (Vol.
20).
ACM,New York.Buckley, C., Salton, G., & Allan, J.
(1992).
Automaticretrieval with locality information using smart.
InText retrieval conferenc (TREC-1) (pp.
59-72).Inkpen, D., Alzghool, M., & Islam, A.
(2006).
Usingvarious indexing schemes and multiple translations inthe CL-SR task at CLEF 2005.
In C. Peters, (Ed.
),Accessing multilingual information repositories(Vol.
4022/2006, pp.
760-768).
Springer,  London.Lee, J. H. (1995).
Combining multiple evidence fromdifferent properties of weighting schemes,Proceedings of the 18th annual international ACMSIGIR conference on Research and development ininformation retrieval.
ACM, Seattle, Washington,United States.Oard, D. W., Soergel, D., Doermann, D., Huang, X.,Murray, G. C., Wang, J., Ramabhadran, B., Franz,M., & Gustman, S. (2004).
Building an informationretrieval test collection for spontaneousconversational speech, Proceedings of the 27thannual international ACM SIGIR conference onResearch and development in information retrieval.ACM, Sheffield, United Kingdom.Oard, D. W., Wang, J., Jones, G. J. F., White, R. W.,Pecina, P., Soergel, D., Huang, X., & Shafran, I.(2007).
Overview of the CLEF-2006 cross-languagespeech retrieval track.
In C. Peters, (Ed.
), Evaluationof multilingual and multi-modal informationretrieval (Vol.
4730/2007, pp.
744-758).
Springer,Heidelberg.Ounis, I., Amati, G., Plachouras, V., He, B., Macdonald,C., & Johnson, D. (2005).
Terrier informationretrieval platform In Advances in informationretrieval (Vol.
3408/2005, pp.
517-519).
Springer,Heidelberg.Pecina, P., Hoffmannov?a, P., Jones, G. J. F., Zhang, Y.,& Oard, D. W. (2007).
Overview of the CLEF-2007cross language speech retrieval track, Working Notesof the CLEF- 2007 Evaluation, .
CLEF2007,Budapest-Hungary.Salton, G. & Buckley, C. (1988).
Term weightingapproaches in automatic text retrieval.
InformationProcessing and Management, 24(5): 513-523.Shaw, J.
A.
& Fox, E. A.
(1994).
Combination of multiplesearches.
In Third text retrieval conference (trec-3)(pp.
105-108).
National Institute of Standards andTechnology Special Publication.Manual English Auto-English Auto-French Auto-Spanish Weightingscheme Training Test Training Test Training Test Training Testnnc.ntc 0.2546 0.2293 0.0888 0.0819 0.0792 0.055 0.0593 0.0614ntc.ntc 0.2592 0.2332 0.0892 0.0794 0.0841 0.0519 0.0663 0.0545lnc.ntc 0.2710 0.2363 0.0898 0.0791 0.0858 0.0576 0.0652 0.0604ntc.nnc 0.2344 0.2172 0.0858 0.0769 0.0745 0.0466 0.0585 0.062anc.ntc 0.2759 0.2343 0.0723 0.0623 0.0664 0.0376 0.0518 0.0398ltc.ntc 0.2639 0.2273 0.0794 0.0623 0.0754 0.0449 0.0596 0.0428atc.ntc 0.2606 0.2184 0.0592 0.0477 0.0525 0.0287 0.0437 0.0304nnn.ntn 0.2476 0.2228 0.0900 0.0852 0.0799 0.0503 0.0599 0.061ntn.ntn 0.2738 0.2369 0.0933 0.0795 0.0843 0.0507 0.0691 0.0578lnn.ntn 0.2858 0.245 0.0969 0.0799 0.0905 0.0566 0.0701 0.0589ntn.nnn 0.2476 0.2228 0.0900 0.0852 0.0799 0.0503 0.0599 0.061ann.ntn 0.2903 0.2441 0.0750 0.0670 0.0743 0.038 0.057 0.0383ltn.ntn 0.2870 0.2435 0.0799 0.0655 0.0871 0.0522 0.0701 0.0501atn.ntn 0.2843 0.2364 0.0620 0.0546 0.0722 0.0347 0.0586 0.0355In_expC2 0.3177 0.2737 0.0885 0.0744 0.0908 0.0487 0.0747 0.0614Fusion 1 0.3208 0.2761 0.0969 0.0855 0.0912 0.0622 0.0731 0.0682% change 1.0% 0.9% 0.0% 6.5% 0.4% 21.7% -2.2% 10.0%Fusion 2 0.3182 0.2741 0.0975 0.0842 0.0942 0.0602 0.0752 0.0619% change 0.2% 0.1% 0.6% 5.1% 3.6% 19.1% 0.7% 0.8%Table 1.
Results (MAP scores) for 15 weighting schemes using Smart and Terrier (the In_expC2 model), and the resultsfor the two Fusions Methods.
In bold are the best scores for the 15 single runs on the training data and the correspondingresults on the test data.WeightingschemeManual English Auto-English Auto- French Auto- SpanishTrain.
Test Train.
Test Train.
Test Train.
Testnnc.
ntc 2371 1827 1726 1306 1687 1122 1562 1178ntc.ntc 2402 1857 1675 1278 1589 1074 1466 1155lnc.ntc 2402 1840 1649 1301 1628 1111 1532 1196ntc.nnc 2354 1810 1709 1287 1662 1121 1564 1182anc.ntc 2405 1858 1567 1192 1482 1036 1360 1074ltc.ntc 2401 1864 1571 1211 1455 1046 1384 1097atc.ntc 2387 1858 1435 1081 1361 945 1255 1011nnn.ntn 2370 1823 1740 1321 1748 1158 1643 1190ntn.ntn 2432 1863 1709 1314 1627 1093 1502 1174lnn.ntn 2414 1846 1681 1325 1652 1130 1546 1194ntn.nnn 2370 1823 1740 1321 1748 1158 1643 1190ann.ntn 2427 1859 1577 1198 1473 1027 1365 1060ltn.ntn 2433 1876 1582 1215 1478 1070 1408 1134atn.ntn 2442 1859 1455 1101 1390 975 1297 1037In_expC2 2638 1823 1624 1286 1676 1061 1631 1172Fusion 1 2645 1832 1745 1334 1759 1147 1645 1219% change 0.3% 0.5 % 0.3% 1.0% 0.6% -1.0% 0.1% 2.4%Fusion 2 2647 1823 1727 1337 1736 1098 1631 1172% change 0.3% 0.0% 0.8% 1.2% -0.7% -5.5% -0.7% -1.5%Table 2.
Results (number of relevant documents retrieved) for 15 weighting schemes using Terrier and SMART, and theresults for the Fusions Methods.
In bold are the best scores for the 15 single runs on training data and the correspondingtest data.
