Proceedings of the 2nd Workshop on ?Collaboratively Constructed Semantic Resources?, Coling 2010, pages 46?54,Beijing, August 2010Measuring Conceptual Similarity by Spreading Activation overWikipedia?s Hyperlink StructureStephan Gouws, G-J van Rooyen, and Herman A. EngelbrechtStellenbosch University{stephan,gvrooyen,hebrecht}@ml.sun.ac.zaAbstractKeyword-matching systems based onsimple models of semantic relatednessare inadequate at modelling the ambigu-ities in natural language text, and cannotreliably address the increasingly com-plex information needs of users.
Inthis paper we propose novel methodsfor computing semantic relatedness byspreading activation energy over the hy-perlink structure of Wikipedia.
Wedemonstrate that our techniques canapproach state-of-the-art performance,while requiring only a fraction of thebackground data.1 IntroductionThe volume of information available to userson the World Wide Web is growing at anexponential rate (Lyman and Varian, 2003).Current keyword-matching information retrieval(IR) systems suffer from several limitations,most notably an inability to accurately modelthe ambiguities in natural language, such as syn-onymy (different words having the same mean-ing) and polysemy (one word having multipledifferent meanings), which is largely governedby the context in which a word appears (Metzlerand Croft, 2006).In recent years, much research attention hastherefore been given to semantic techniques ofinformation retrieval.
Such systems allow forsophisticated semantic search, however, requirethe use of a more difficult-to-understand query-syntax (Tran et al, 2008).
Furthermore, thesemethods require specially encoded (and thuscostly) ontologies to describe the particular do-main knowledge in which the system operates,and the specific interrelations of concepts withinthat domain.In this paper, we focus on the problem ofcomputationally estimating similarity or related-ness between two natural-language documents.A novel technique is proposed for comput-ing semantic similarity by spreading activationover the hyperlink structure of Wikipedia, thelargest free online encyclopaedia.
New mea-sures for computing similarity between individ-ual concepts (inter-concept similarity, such as?France?
and ?Great Britain?
), as well as be-tween documents (inter-document similarity)are proposed and tested.
It will be demonstratedthat the proposed techniques can achieve compa-rable inter-concept and inter-document similar-ity accuracy on similar datasets as compared tothe current state of the art Wikipedia Link-basedMeasure (WLM) (Witten and Milne, 2008) andExplicit Semantic Analysis (ESA) (Gabrilovichand Markovitch, 2007) methods respectively.Our methods outperform WLM in computinginter-concept similarity, and match ESA forinter-document similarity.
Furthermore, we usethe same background data as for WLM, which isless than 10% of the data required for ESA.In the following sections we introduce workrelated to our work and an overview of ourapproach and the problems that have to besolved.
We then discuss our method in detail andpresent several experiments to test and compareit against other state-of-the-art methods.462 Related Work and OverviewAlthough Spreading Activation (SA) is foremosta cognitive theory modelling semantic mem-ory (Collins and Loftus, 1975), it has been ap-plied computationally to IR with various lev-els of success (Preece, 1982), with the biggesthurdle in this regard the cost of creating an as-sociative network or knowledge base with ad-equate conceptual coverage (Crestani, 1997).Recent knowledge-based methods for comput-ing semantic similarity between texts based onWikipedia, such as Wikipedia Link-based Mea-sure (WLM) (Witten and Milne, 2008) and Ex-plicit Semantic Analysis (ESA) (Gabrilovich andMarkovitch, 2007), have been found to out-perform earlier WordNet-based methods (Bu-danitsky and Hirst, 2001), arguably due toWikipedia?s larger conceptual coverage.WLM treats the anchor text in Wikipedia arti-cles as links to other articles (all links are treatedequally), and compare concepts based on howmuch overlap exists in the out-links of the arti-cles representing them.
ESA discards the linkstructure and uses only the text in articles to de-rive an explicit concept space in which each di-mension represents one article/concept.
Text iscategorised as vectors in this concept space andsimilarity is computed as the cosine similarity oftheir ESA vectors.
The most similar work to oursis Yeh (2009) in which the authors derive a graphstructure from the inter-article links in Wikipediapages, and then perform random walks over thegraph to compute relatedness.In Wikipedia, users create links between arti-cles which are seen to be related to some degree.Since links relate one article to its neighbours,and by extension to their neighbours, we ex-tract and process this hyperlink structure (usingSA) as an Associative Network (AN) (Bergeret al, 2004) of concepts and links relating themto one another.
The SA algorithm can brieflybe described as an iterative process of propagat-ing real-valued energy from one or more sourcenodes, via weighted links over an associative net-work (each such a propagation is called a pulse).The algorithm consists of two steps: First, oneor more pulses are triggered, and second, ter-mination checks determine whether the processshould continue or halt.
This process of acti-vating more and more nodes in the network andchecking for termination conditions are repeatedpulse after pulse, until all termination conditionsare met, which results in a final activation statefor the network.
These final node activationsare then translated into a score of relatedness be-tween the initial nodes.Our work presents a computational imple-mentation of SA over the Wikipedia graph.We therefore overcome the cost of produc-ing a knowledge base of adequate coverage byutilising the collaboratively-created knowledgesource Wikipedia.
However, additional strate-gies are required for translating the hyperlinkstructure of Wikipedia into a suitable associativenetwork format, and for this new techniques areproposed and tested.3 Extracting the Hyperlink GraphStructureOne article in Wikipedia covers one specifictopic (concept) in detail.
Hyperlinks link a pageA to a page B, and are thus directed.
Wecan model Wikipedia?s hyperlink structure us-ing standard graph theory as a directed graph G,consisting of a set of vertices V, and a set ofedges E. Each edge eij ?
E connects two ver-tices vi, vj ?
V. For consistency, we use theterm node to refer to a vertex (Wikipedia article)in the graph, and link to refer to an edge (hyper-link) between such nodes.In this model, each Wikipedia article is seento represent a single concept, and the hyperlinkstructure relates these concepts to one another.
Inorder to compute relatedness between two con-cepts vi and vj , we use spreading activation andrely on the fundamental principle of an associa-tive network, namely that it connects nodes thatare associated with one another via real-valuedlinks denoting how strongly the objects are re-lated.
Since Wikipedia was not created as an as-sociative network, but primarily as an online en-cyclopaedia, none of these weights exist, and wewill have to deduce these (see Fan-out constraintin Section 4).47Links into pages are used, since this leads tobetter results (Witten and Milne, 2008).
TheWikipedia graph structure is represented in anadjacency list structure, i.e.
for each node vi westore its list of neighbour nodes in a dictionaryusing vi?s id as key.
This approach is preferredover an adjacency matrix structure, since mostarticles are linked to by only 34 articles on aver-age, which would lead to a very sparse adjacencymatrix structure.4 Adapting Spreading Activation forWikipedia?s Hyperlink StructureEach pulse in the Spreading Activation (SA) pro-cess consists of three stages: 1) pre-adjustment,2) spreading, and 3) post-adjustment (Crestani,1997).
During pre- and post-adjustment, someform of activation decay is optionally applied tothe active nodes.
This serves both to avoid re-tention of activation from previous pulses, and,from a connectionist point of view, models ?lossof interest?
when nodes are not continually acti-vated.Let ai,in denote the total energy input (acti-vation) for node vi, and N(vi) the set of vi?sneighbour nodes with incoming links to vi.
Also,let aj,out denote the output activation of a nodevj connected to node vi, and let wij denote theweight of connection between node vi and vj .For a node vi, we can then describe the puremodel of spreading activation as follows:ai,in =?vj?N(vi)aj,outwij .
(1)This pure model of SA has several significantproblems, the most notable being that activationcan saturate the entire network unless certainconstraints are imposed, namely limiting howfar activation can spread from the initially acti-vated nodes (distance constraint), and limitingthe effect of very highly-connected nodes (fan-out constraint) (Crestani, 1997).
In the followingthree sections we discuss how these constraintswere implemented in our model for SA.Distance constraintFor every pulse in the spreading process, anode?s activation value is multiplied by a globalnetwork decay parameter 0 < d < 1.
Wetherefore substitute wij in Equation 1 for wijd.This decays activation exponentially in the pathlength.
For a path length of one, activation is de-cayed by d, for a path length of two, activationis decays by dd = d2, etc.
This penalises activa-tion transfer over longer paths.
We also includea maximum path length parameter Lp,max whichlimits how far activation can spread.Fan-out constraintAs noted above, in an associative network, linkshave associated real-valued weights to denote thestrength of association between the two nodesthey connect (i.e.
wij in Equation 1).
Theseweights have to be estimated for the Wikipediahyperlink graph, and for this purpose we proposethe use of three weighting schemes:In pure Energy Distribution (ED), a nodevi?s weight w is made inversely proportional toits in-degree (number of neighbours N(vi) ?
1with incoming links to vi1).
Thus ED(vi, vj) =wij = 1|N(vi)| .
This reduces the effect of veryconnected nodes on the spreading process (con-straint 2 above).For instance, we consider a path connectingtwo nodes via a general article such as USA (con-nected to 322,000 articles) not nearly as indica-tive of a semantic relationship, as a path con-necting them via a very specific concept, suchas Hair Pin (only connected to 20 articles).Inverse Link-Frequency (ILF) is inspired bythe term-frequency inverse document-frequency(tf-idf) heuristic (Salton and McGill, 1983) inwhich a term?s weight is reduced as it is con-tained in more documents in the corpus.
It isbased on the idea that the more a term appearsin documents across the corpus, the less it candiscriminate any one of those documents.We define a node vi?s link-frequency as thenumber of nodes that vi is connected to |N(vi)|divided by the number of possible nodes it couldbe connected to in the entire Wikipedia graph1All orphan nodes are removed from the AN.48|G|, and therefore give the log-smoothed inverselink-frequency of node vi as:ILF(vi) , log( |G||N(vi)|)?
0 (2)As noted above for pure energy distribution, weconsider less connected nodes as more specific.If one node connects to another via a very spe-cific node with a low in-degree, |G||N(vi)| is verylarge and ILF(vi) > 1, thus boosting that spe-cific link?s weight.
This has the effect of ?boost-ing?
paths (increasing their contribution) whichcontain nodes that are less connected, and there-fore more meaningful in our model.To evaluate the effect of this boosting ef-fect described above, we also define a thirdnormalised weighting scheme called the Nor-malised Inverse Link-Frequency (NILF), 0 ?NILF(vi) ?
1:NILF(vi) , ILF(vi)log |G| .
(3)ILF reaches a maximum of log |G| when|N(vi)| = 1 (see Equation 2).
We therefore di-vide by log |G| to normalise its range to [0,1].Threshold constraintFinally, the above-mentioned constraints are en-forced through the use of a threshold parameter0 < T < 1.
Activation transfer to a next nodeceases when a node?s activation value drops be-low a certain threshold T .5 Strategies for InterpretingActivationsAfter spreading has ceased, we are left with avector of nodes and their respective values ofactivation (an activation vector).
We wish totranslate this activation vector into a score re-sembling strength of association or relatednessbetween the two initial nodes.We approach this problem using two differ-ent approaches, the Target Activation Approach(TAA) and the Agglomerative Approach (AA).These approaches are based on two distinct hy-potheses, namely: Relatedness between twonodes can be measured as either 1) the ratio ofinitial energy that reaches the target node, or 2)the amount of overlap between their individualactivation vectors by spreading from both nodesindividually.Target Activation Approach (TAA)To measure the relatedness between vi and vj ,we set ai to some initial valueKinit (usually 1.0),and all node activations including aj = 0.
Af-ter the SA process has terminated, vj is activatedwith some aj,in.
Relatedness is computed as theratio simTAA(vi, vj) , aj,inKinit .Agglomerative Approach (AA)The second approach is called the Agglomera-tive Approach since we agglomerate all activa-tions into one score resembling relatedness.
Af-ter spreading has terminated, relatedness is com-puted as the amount of overlap between the indi-vidual nodes?
activation vectors, using either thecosine similarity (AA-cos), or an adapted ver-sion of the information theory based WLM (Wit-ten and Milne, 2008) measure.Assume the same set of initial nodes vi andvj .
Let Ak be the N -dimensional vector of real-valued activation values obtained by spreadingover the N nodes in the graph from node vk(called an activation vector).
We use akx to de-note the element at position x in Ak.
Further-more, let Vk = {vk1, ..., vkM} denote the set ofM nodes activated by spreading from vk, i.e.
theset of identifiers of nodes with non-zero activa-tions in Ak after spreading has terminated (andtherefore M ?
N ).We then define the cosine Agglomerative Ap-proach (henceforth called AA-cos) assimAA,cos(Ai,Aj), Ai ?Aj||Ai||||Aj || (4)For our adaptation of the Wikipedia Link-basedMeasure (WLM) approach to spreading activa-tion, we define the WLM Agglomerative Ap-proach (henceforth called AA-wlm2) as2AA-wlm is our adaptation of WLM (Witten and Milne,2008) for SA, not to be confused with their method, whichwe simply call WLM.49simAA,wlm(Vi,Vj), log(max(|Vi|,|Vj|))?log(|Vi?Vj|)log(|G|)?log(min(|Vi|,|Vj|)) (5)with |G| representing the number of nodes in theentire Wikipedia hyperlink graph.
Note that theAA-wlm method does not take activations intoaccount, while the AA-cos method does.6 Spreading Activation AlgorithmBoth the TAA and AA approaches describedabove rely on a function to spread activationfrom one node to all its neighbours, and itera-tively to all their neighbours, subject to the con-straints listed.
TAA stops at this point and com-putes relatedness as the ratio of energy receivedto energy sent between the target and sourcenode respectively.
However, AA repeats the pro-cess from the target node and computes related-ness as some function (cosine or information the-ory based) of the two activation vectors, as givenby Equation 4 and Equation 5.We therefore define SPREAD UNIDIR() asshown in Algorithm 1.
Prior to spreading fromsome node vi, its activation value ai is set tosome initial activation value Kinit (usually 1.0).The activation vector A is a dynamic node-value-pair list, updated in-place.
P is a dynamiclist of nodes in the path to vi to avoid cycles.7 Parameter Optimisation:Inter-concept SimilarityThe model for SA as introduced in this paper re-lies on several important parameters, namely thespreading strategy (TAA, AA-cos, or AA-wlm),weighting scheme (pure ED, ILF, and NILF),maximum path length Lp,max, network decay d,and threshold T .
These parameters have a largeinfluence on the accuracy of the proposed tech-nique, and therefore need to be optimised.Experimental MethodIn order to compare our method with results re-ported by Gabrilovich and Markovitch (2007)and Witten and Milne (2008), we followedthe same approach by randomly selectingAlgorithm 1 Pseudo code to spread activationdepth-first from node vi up to level Lp,max, us-ing global decay d, and threshold T , given anadjacency list graph structure G and a weightingscheme W such that 0 < wij ?W < 1.Require: G,Lp,max, d, Tfunction SPREAD UNIDIR(vi,A,P)if (vi, ai) /?
A or ai < T then .
Thresholdreturnend ifAdd vi to P .
To avoid cyclesfor vj ?
N(vi) do .
Process neighboursif (vj , aj) /?
A thenaj = 0end ifif vj /?
P and |P| ?
Lp,max thena?j = aj + ai ?
wij ?
dReplace (vj , aj) ?
A with (vj , a?j )SPREAD UNIDIR(vj ,A,P)end ifend forreturnend function50 word-pairs from the WordSimilarity-353dataset (Gabrilovich, 2002) and correlatingour method?s scores with the human-assignedscores.
To reduce the possibility of overestimat-ing the performance of our technique on a sam-ple set that happens to be favourable to our tech-nique, we furthermore implemented a techniqueof repeated holdout (Witten and Frank, 2005):Given a sample test set of N pairs of wordswith human-assigned ratings of relatedness, ran-domly divide this set into k parts of roughlyequal size3.
Hold out one part of the data anditeratively evaluate the performance of the algo-rithm on the remaining k?1 parts until all k partshave been held out once.
Finally, average the al-gorithm?s performance over all k runs into onescore resembling the performance for that set ofparameters.Since there are five parameters (spreadingstrategy, weighting scheme, path length, networkdecay, and threshold), a grid search was imple-mented by holding three of the five parametersconstant, and evaluating combinations of decayand threshold by stepping over the possible pa-rameter space using some step size.
A coarse-grained grid search was first conducted with step3k was chosen as 5.50Table 1: Spreading results by spreadingstrategy (TAA=Target Activation Approach,AA=Agglomerative Approach, Lp,max = max-imum path length used, ED=energy distri-bution only, ILF=Inverse Link Frequency,NILF=normalised ILF.)
Best results in bold.Strategy ?max ParametersTAA 0.56 ED, Lp,max=3, d=0.6, T=0.001AA-wlm 0.60 NILF, Lp,max=3, d=0.1, T=10?6AA-cos 0.70 ILF, Lp,max=3, d=0.5, T=0.1size of 0.1 over d and a logarithmic scale overT , thus T = {0, 0.1, 0.01, 0.001, ..., 10?9}.
Thebest values for d and T were then chosen to con-duct a finer-grained grid search.Influence of the different ParametersThe spreading strategy determines how activa-tions resulting from the spreading process areconverted into scores of relatedness or similar-ity between two nodes.
Table 1 summarises thebest results obtained for each of the three strate-gies, with the specific set of parameters that wereused in each run.Results are better using the AA (?max =0.70 for AA-cos) than using the TAA (?max =0.56).
Secondly, the AA-cos spreading strat-egy significantly outperforms the AA-wlm strat-egy over this sample set (?max,wlm = 0.60vs ?max,cos = 0.70).
These results comparefavourably to similar inter-concept results re-ported for WLM (Witten and Milne, 2008) (?
=0.69) and ESA (Gabrilovich and Markovitch,2007) (?
= 0.75).Maximum path length Lp,max is related tohow far one node can spread its activation in thenetwork.
We extend the first-order link modelused by WLM, by approaching the link structureas an associative network and by using spreadingactivation.To evaluate if this is a useful approach, testswere conducted by using maximum path lengthsof one, two, and three.
Table 2 summarisesthe results for this experiment.
Increasing pathlength from one to two hops increases per-formance from ?max = 0.47 to ?max =Table 2: Spreading results by maximum pathlength Lp,max.
Best results in bold.Lp,max ?max Parameters1 0.47 TAA, ED/ILF/NILF2 0.66 AA-cos, ILF, d=0.4, T=0.13 0.70 AA-cos, ILF, d=0.5, T=0.1Table 3: Spreading results by weighting schemew.
Best results in bold.w ?max ParametersNILF 0.63 AA-cos, Lp,max = 3, d=0.9, T=0.01ED 0.64 AA-cos, Lp,max = 3, d=0.9, T=0.01ILF 0.70 AA-cos, Lp,max = 3, d=0.5, T=0.10.66.
Moreover, increasing Lp,max from two tothree hops furthermore increases performance to?max = 0.70.In an associative network, each link has areal-valued weight denoting the strength of as-sociation between the two nodes it connects.The derived Wikipedia hyperlink graph lacksthese weights.
We therefore proposed three newweighting schemes (pure ED, ILF, and NILF) toestimate these weights.Table 3 summarises the best performances us-ing the different weighting schemes.
ILF outper-forms both ED and NILF.
Furthermore, both EDand NILF perform best using higher decay val-ues (both 0.9) and lower threshold values (both0.01), compared to ILF (0.5 and 0.1 respectivelyfor d and T ).
We attribute this observation tothe boosting effect of the ILF weighting schemefor less connected nodes, and offer the followingexplanation:Recall from the section on ILF that in ourmodel, strongly connected nodes are viewed asmore general, and nodes with low in-degreesare seen as very specific concepts.
We arguedthat a path connecting two concepts via thesemore specific concepts are more indicative ofa stronger semantic relationship than throughsome very general concept.
In the ILF weightingscheme, paths containing these less connectednodes are automatically boosted to be more im-51portant.
Therefore, by not boosting less mean-ingful paths, a lower decay and higher thresholdeffectively limits the amount of non-importantnodes that are activated, since their activationsare more quickly decayed, whilst at the sametime requiring a higher threshold to continuespreading.
Boosting more important nodes cantherefore lead to activation vectors which capturethe semantic context of the source nodes moreaccurately, leading to higher performance.8 Computing document similarityTo compute document similarity, we first extractkey representative Wikipedia concepts from adocument to produce document concept vec-tors4.
This process is known as wikifica-tion (Csomai and Mihalcea, 2008), and we usedan implementation of Milne and Witten (2008).This produces document concept vectors of theform Vi = {(id1, w1), (id2, w2), ...} with idisome Wikipedia article identifier andwi a weightdenoting how strongly the concept relates to thecurrent document.
We next present two algo-rithms, MAXSIM and WIKISPREAD, for com-puting document similarity, and test these overthe Lee (2005) document similarity dataset, aset of 50 documents between 51 and 126 wordseach, with the averaged gold standard similarityratings produced by 83 test subjects (see (Lee etal., 2005)).The first metric we propose is calledMAXSIM (see Algorithm 2) and is based onthe idea of measuring document similarity bypairing up each Wikipedia concept in one docu-ment?s concept vector with its most similar con-cept in the other document.
We average thosesimilarities to produce an inter-document simi-larity score, weighted by how strongly each con-cept is seen to represent a document (0 < pi <1).
The contribution of a concept is furtherweighted by its ILF score, so that more specificconcepts contribute more to final relatedness.The second document similarity metric wepropose is called the WIKISPREAD method andis a natural extension of the inter-concept spread-4Vectors of Wikipedia topics (concepts) and howstrongly they are seen to relate to the current document.Algorithm 2 Pseudo code for the MaxSim al-gorithm for computing inter-document similar-ity.
vi is a Wikipedia concept and 0 < pi < 1how strongly it relates to the current document.Require: ILF lookup functionfunction MAXSIM(V1,V2)num=0den=0for (vi, pi) ?
V1 dosk = 0 .
sk = maxj sim(vi, vj)for vj ?
V2 do .
Find most related topicsj = sim(vi, vj)if sj > sk thenvk = vj.
Topic in V2 most related to visk = sjend ifend fornum += skpiILF(vk)den += ILF(vk)end forreturn num / denend functionAlgorithm 3 Pseudo code for the WikiSpread al-gorithm for computing inter-document similar-ity.
Kinit = 1.0.function WIKISPREAD(V1,V2)A1 = ?
.
Dynamic activation vectors.A2 = ?for (vi, pi) ?
V1 do .
Document 1ai = Kinit ?
pi .
Update ai ?
piAdd (vi, ai) to A1SPREAD UNIDIR(vi,A1, ?
)end forfor (vj , pj) ?
V2 do .
Document 2aj = Kinit ?
pjAdd (vj , aj) to A2SPREAD UNIDIR(vj ,A2, ?
)end forCompute similarity using AA-cos or AA-wlmend functioning activation work introduced in the previoussection.
We view a document concept vector asa cluster of concepts, and build a single docu-ment activation vector (see Algorithm 3) ?
i.e.
avector of article ids and their respective activa-tions ?
for each document, by iteratively spread-ing from each concept in the document conceptvector.
Finally, similarity is computed using ei-ther the AA-cos or AA-wlm methods given byEquation 4 and Equation 5 respectively.Knowledge-based approaches such as theWikipedia-based methods can capture morecomplex lexical and semantic relationships than52Table 4: Summary of final document similaritycorrelations over the Lee & Pincombe documentsimilarity dataset.
ESA score from Gabrilovichand Markovitch (2007).Pearson ?Cosine VSM (with tf-idf) only 0.56MaxSim method 0.68WikiSpread method 0.62ESA 0.72Combined (Cosine + MaxSim) 0.72keyword-matching approaches, however, noth-ing can be said about concepts not adequatelyrepresented in the underlying knowledge base(Wikipedia).
We therefore hypothesise that com-bining the two approaches will lead to more ro-bust document similarity performance.
There-fore, the final document similarity metric weevaluate (COMBINED) is a linear combinationof the best-performing Wikipedia-based meth-ods described above, and the well-known VectorSpace Model (VSM) with cosine similarity andtf-idf (Salton and McGill, 1983).ResultsThe results obtained on the Lee (2005) documentsimilarity dataset using the three document sim-ilarity metrics (MAXSIM, WIKISPREAD, andCOMBINED) are summarised in Table 4.
Ofthe two Wikipedia-only methods, the MaxSimmethod achieves the best correlation score of?
= 0.68.
By combining the standard co-sine VSM with tf-idf with the MaxSim metricin the ratio ?
and (1 ?
?)
for 0 < ?
< 1,and performing a parameter sweep over ?, wecan weight the contributions made by the indi-vidual methods and observe the effect this hason final performance.
The results are shownin Fig 1.
Note that both methods contributeequally (?
= 0.5) to the final best correlationscore of ?
= 0.72.
This suggests that selectiveknowledge-based augmentation of simple VSMmethods can lead to more accurate documentsimilarity performance.Figure 1: Parameter sweep over ?
showing con-tributions from cosine (?)
and Wikipedia-basedMAXSIM method (1 ?
?)
to the final perfor-mance over the Lee (2005) dataset.9 ConclusionIn this paper, the problem of computing con-ceptual similarity between concepts and docu-ments are approached by spreading activationover Wikipedia?s hyperlink graph.
New strate-gies are required to infer weights of associa-tion between articles, and for this we introduceand test three new weighting schemes and findour Inverse Link-Frequency (ILF) to give bestresults.
Strategies are also required for trans-lating resulting activations into scores of relat-edness, and for this we propose and test threenew strategies, and find that our cosine Agglom-erative Approach gives best results.
For com-puting document similarity, we propose and testtwo new methods using only Wikipedia.
Finally,we show that using our best Wikipedia-basedmethod to augment the cosine VSM method us-ing tf-idf, leads to the best results.
The finalresult of ?
= 0.72 is equal to that reportedfor ESA (Gabrilovich and Markovitch, 2007),while requiring less than 10% of the Wikipediadatabase required for ESA.
Table 4 summarisesthe document-similarity results.AcknowledgementsWe thank Michael D. Lee for his document simi-larity data and MIH Holdings Ltd. for financiallysupporting this research.53ReferencesBerger, Helmut, Michael Dittenbach, and DieterMerkl.
2004.
An adaptive information retrievalsystem based on associative networks.
APCCM?04: Proceedings of the first Asian-Pacific confer-ence on Conceptual Modelling, pages 27?36.Budanitsky, A. and G. Hirst.
2001.
Semantic dis-tance in WordNet: An experimental, application-oriented evaluation of five measures.
In Work-shop on WordNet and Other Lexical Resources,volume 2.
Citeseer.Collins, A.M. and E.F. Loftus.
1975.
A spreading-activation theory of semantic processing.
Psycho-logical review, 82(6):407?428.Crestani, F. 1997.
Application of Spreading Activa-tion Techniques in Information Retrieval.
Artifi-cial Intelligence Review, 11(6):453?482.Csomai, A. and R. Mihalcea.
2008.
Linking docu-ments to encyclopedic knowledge.
IEEE Intelli-gent Systems, 23(5):34?41.Gabrilovich, E. and S. Markovitch.
2007.
Comput-ing Semantic Relatedness Using Wikipedia-basedExplicit Semantic Analysis.
Proceedings of the20th International Joint Conference on ArtificialIntelligence, pages 6?12.Gabrilovich, E. 2002.
The WordSimilarity-353 TestCollection.
Using Information Content to Evalu-ate Semantic Similarity in a Taxonomy.Lee, M.D., B. Pincombe, and M. Welsh.
2005.
AComparison of Machine Measures of Text Docu-ment Similarity with Human Judgments.
In 27thAnnual Meeting of the Cognitive Science Society(CogSci2005), pages 1254?1259.Lyman, P. and H.R.
Varian.
2003.
How muchinformation?
http://www2.sims.berkeley.edu/research/projects/how-much-info-2003/index.htm.
Ac-cessed: May, 2010.Metzler, Donald and W. Bruce Croft.
2006.
Beyondbags of words: Modeling implicit user preferencesin information retrieval.
AAAI?06: Proceedings ofthe 21st National Conference on Artificial Intelli-gence, pages 1646?1649.Milne, David and Ian H. Witten.
2008.
Learning tolink with wikipedia.
CIKM ?08: Proceeding of the17th ACM Conference on Information and Knowl-edge Management, pages 509?518.Preece, SE.
1982.
Spreading Activation NetworkModel for Information Retrieval.
Ph.D. thesis.Salton, G. and M.J. McGill.
1983.
Introduction toModern Information Retrieval.
McGraw-Hill NewYork.Tran, T., P. Cimiano, S. Rudolph, and R. Studer.2008.
Ontology-based Interpretation of Keywordsfor Semantic Search.
The Semantic Web, pages523?536.Witten, I.H.
and E. Frank.
2005.
Data Min-ing: Practical Machine Learning Tools and Tech-niques.
Morgan Kaufmann.Witten, I.H.
and D. Milne.
2008.
An Effective, Low-Cost Measure of Semantic Relatedness ObtainedFrom Wikipedia Links.
In Proceeding of AAAIWorkshop on Wikipedia and Artificial Intelligence:an Evolving Synergy, AAAI Press, Chicago, USA,pages 25?30.Yeh, E., D. Ramage, C.D.
Manning, E. Agirre, andA.
Soroa.
2009.
WikiWalk: Random walks onWikipedia for semantic relatedness.
In Proceed-ings of the 2009 Workshop on Graph-based Meth-ods for Natural Language Processing, pages 41?49.
Association for Computational Linguistics.54
