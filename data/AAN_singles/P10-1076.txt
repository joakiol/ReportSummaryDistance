Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 739?748,Uppsala, Sweden, 11-16 July 2010. c?2010 Association for Computational LinguisticsBoosting-based System Combination for Machine TranslationTong Xiao, Jingbo Zhu, Muhua Zhu, Huizhen WangNatural Language Processing Lab.Northeastern University, China{xiaotong,zhujingbo,wanghuizhen}@mail.neu.edu.cnzhumuhua@gmail.comAbstractIn this paper, we present a simple and effectivemethod to address the issue of how to generatediversified translation systems from a singleStatistical Machine Translation (SMT) enginefor system combination.
Our method is basedon the framework of boosting.
First, a se-quence of weak translation systems is gener-ated from a baseline system in an iterativemanner.
Then, a strong translation system isbuilt from the ensemble of these weak transla-tion systems.
To adapt boosting to SMT sys-tem combination, several key components ofthe original boosting algorithms are redes-igned in this work.
We evaluate our method onChinese-to-English Machine Translation (MT)tasks in three baseline systems, including aphrase-based system, a hierarchical phrase-based system and a syntax-based system.
Theexperimental results on three NIST evaluationtest sets show that our method leads to signifi-cant improvements in translation accuracyover the baseline systems.1 IntroductionRecent research on Statistical Machine Transla-tion (SMT) has achieved substantial progress.Many SMT frameworks have been developed,including phrase-based SMT (Koehn et al, 2003),hierarchical phrase-based SMT (Chiang, 2005),syntax-based SMT (Eisner, 2003; Ding andPalmer, 2005; Liu et al, 2006; Galley et al, 2006;Cowan et al, 2006), etc.
With the emergence ofvarious structurally different SMT systems, moreand more studies are focused on combining mul-tiple SMT systems for achieving higher transla-tion accuracy rather than using a single transla-tion system.The basic idea of system combination is to ex-tract or generate a translation by voting from anensemble of translation outputs.
Depending onhow the translation is combined and what votingstrategy is adopted, several methods can be usedfor system combination, e.g.
sentence-level com-bination (Hildebrand and Vogel, 2008) simplyselects one from original translations, whilesome more sophisticated methods, such as word-level and phrase-level combination (Matusov etal., 2006; Rosti et al, 2007), can generate newtranslations differing from any of the originaltranslations.One of the key factors in SMT system combi-nation is the diversity in the ensemble of transla-tion outputs (Macherey and Och, 2007).
To ob-tain diversified translation outputs, most of thecurrent system combination methods requiremultiple translation engines based on differentmodels.
However, this requirement cannot bemet in many cases, since we do not always havethe access to multiple SMT engines due to thehigh cost of developing and tuning SMT systems.To reduce the burden of system development, itmight be a nice way to combine a set of transla-tion systems built from a single translation en-gine.
A key issue here is how to generate an en-semble of diversified translation systems from asingle translation engine in a principled way.Addressing this issue, we propose a boosting-based system combination method to learn acombined translation system from a single SMTengine.
In this method, a sequence of weak trans-lation systems is generated from a baseline sys-tem in an iterative manner.
In each iteration, anew weak translation system is learned, focusingmore on the sentences that are relatively poorlytranslated by the previous weak translation sys-tem.
Finally, a strong translation system is builtfrom the ensemble of the weak translation sys-tems.Our experiments are conducted on Chinese-to-English translation in three state-of-the-art SMTsystems, including a phrase-based system, a hier-archical phrase-based system and a syntax-based739Input:   a model u, a sequence of (training) samples {(f1, r1), ..., (fm, rm)} where fi is thei-th source sentence, and ri is the set of reference translations for fi.Output: a new translation systemInitialize: D1(i) = 1 / m for all i = 1, ..., mFor t = 1, ..., T1.
Train a translation system u(?
*t) on {(fi, ri)} using distribution Dt2.
Calculate the error rate t?
of u(?
*t) on {(fi, ri)}3.
Set1 1ln( )2ttt??
?+=                                                         (3)4.
Update weights1( )( )t iltttD i eD iZ?
?+ =                                                    (4)where li is the loss on the i-th training sample, and Zt is the normalization factor.Output the final system:v(u(?
*1), ..., u (?
*T))Figure 1: Boosting-based System Combinationsystem.
All the systems are evaluated on threeNIST MT evaluation test sets.
Experimental re-sults show that our method leads to significantimprovements in translation accuracy over thebaseline systems.2 BackgroundGiven a source string f, the goal of SMT is tofind a target string e* by the following equation.
* arg max(Pr( | ))ee e f=                (1)where Pr( | )e f is the probability that e is thetranslation of the given source string f. To modelthe posterior probability Pr( | )e f , most of thestate-of-the-art SMT systems utilize the log-linear model proposed by Och and Ney (2002),as follows,1' 1exp( ( , ))Pr( | )exp( ( , '))Mm mmMm me mh f ee fh f e?
?==?= ???
?
(2)where {hm( f, e ) | m = 1, ..., M} is a set of fea-tures, and ?m is the feature weight correspondingto the m-th feature.
hm( f, e ) can be regarded as afunction that maps every pair of source string fand target string e into a non-negative value, and?m can be viewed as the contribution of hm( f, e )to the overall score Pr( | )e f .In this paper, u denotes a log-linear model thathas M fixed features {h1( f ,e ), ..., hM( f ,e )}, ?
={?1, ..., ?M} denotes the M parameters of u, andu(?)
denotes a SMT system based on u with pa-rameters ?.
Generally, ?
is trained on a trainingdata set1 to obtain an optimized weight vector ?
*and consequently an optimized system u(?
*).3 Boosting-based System Combinationfor Single Translation EngineSuppose that there are T available SMT systems{u1(?
*1), ..., uT(?
*T)}, the task of system combina-tion is to build a new translation systemv(u1(?
*1), ..., uT(?
*T)) from {u1(?
*1), ..., uT(?
*T)}.Here v(u1(?
*1), ..., uT(?
*T)) denotes the combina-tion system which combines translations from theensemble of the output of each ui(?*i).
We callui(?
*i) a member system of v(u1(?
*1), ..., uT(?
*T)).As discussed in Section 1, the diversity amongthe outputs of member systems is an importantfactor to the success of system combination.
Toobtain diversified member systems, traditionalmethods concentrate more on using structurallydifferent member systems, that is u1?
u2 ?...?uT.
However, this constraint condition cannot besatisfied when multiple translation engines arenot available.In this paper, we argue that the diversifiedmember systems can also be generated from asingle engine u(?
*) by adjusting the weight vector?
* in a principled way.
In this work, we assumethat u1 = u2 =...= uT  = u.
Our goal is to find a se-ries of ?
*i and build a combined system from{u(?*i)}.
To achieve this goal, we propose a1 The data set used for weight training is generally calleddevelopment set or tuning set in the SMT field.
In this paper,we use the term training set to emphasize the training oflog-linear model.740boosting-based system combination method (Fig-ure 1).Like other boosting algorithms, such asAdaBoost (Freund and Schapire, 1997; Schapire,2001), the basic idea of this method is to useweak systems (member systems) to form a strongsystem (combined system) by repeatedly callingweak system trainer on different distributionsover the training samples.
However, since mostof the boosting algorithms are designed for theclassification problem that is very different fromthe translation problem in natural language proc-essing, several key components have to be redes-igned when boosting is adapted to SMT systemcombination.3.1 TrainingIn this work, Minimum Error Rate Training(MERT) proposed by Och (2003) is used to es-timate feature weights ?
over a series of trainingsamples.
As in other state-of-the-art SMT sys-tems, BLEU is selected as the accuracy measureto define the error function used in MERT.
Sincethe weights of training samples are not taken intoaccount in BLEU2, we modify the original defi-nition of BLEU to make it sensitive to the distri-bution Dt(i) over the training samples.
The modi-fied version of BLEU is called weighted BLEU(WBLEU) in this paper.Let E = e1 ... em be the translations producedby the system, R = r1 ... rm be the reference trans-lations where ri = {ri1, ..., riN}, and Dt(i) be theweight of the i-th training sample (fi, ri).
Theweighted BLEU metric has the following form:{ }( )11 1111/ 4m4 1 1m11WBLEU( , )( ) min | ( ) |exp 1 max 1,( ) | ( ) |( ) ( ) ( )(5)( ) ( )mijti j NmitiNi ijt n ni jin t niE RD i g rD i g eD i g e g rD i g e= ?
?== == =?
??
??
??
?= ?
??
??
??
??
??
??
??
??
??
??
??
?????
?I Uwhere ( )ng s  is the multi-set of all n-grams in astring s. In this definition, n-grams in ei and {rij}are weighted by Dt(i).
If the i-th training samplehas a larger weight, the corresponding n-gramswill have more contributions to the overall scoreWBLEU( , )E R .
As a result, the i-th trainingsample gains more importance in MERT.
Obvi-2 In this paper, we use the NIST definition of BLEU wherethe effective reference length is the length of the shortestreference translation.ously the original BLEU is just a special case ofWBLEU when all the training samples areequally weighted.As the weighted BLEU is used to measure thetranslation accuracy on the training set, the errorrate is defined to be:1 WBLEU( , )t E R?
= ?
(6)3.2 Re-weightingAnother key point is the maintaining of the dis-tribution Dt(i) over the training set.
Initially allthe weights of training samples are set equally.On each round, we increase the weights of thesamples that are relatively poorly translated bythe current weak system so that the MERT-basedtrainer can focus on the hard samples in nextround.
The update rule is given in Equation 4with two parameters t?
and li in it.t?
can be regarded as a measure of the im-portance that the t-th weak system gains in boost-ing.
The definition of t?
guarantees that t?
al-ways has a positive value3.
A main effect of t?is to scale the weight updating (e.g.
a larger t?means a greater update).li is the loss on the i-th sample.
For each i, let{ei1, ..., ein} be the n-best translation candidatesproduced by the system.
The loss function is de-fined to be:*11BLEU( , ) BLEU( , )ki i i ij ijl e ek == ?
?r r  (7)where BLEU(eij, ri) is the smoothed sentence-levelBLEU score (Liang et al, 2006) of the transla-tion e with respect to the reference translations ri,and ei* is the oracle translation which is selectedfrom {ei1, ..., ein} in terms of BLEU(eij, ri).
li canbe viewed as a measure of the average cost thatwe guess the top-k translation candidates insteadof the oracle translation.
The value of li countsfor the magnitude of weight update, that is, a lar-ger li means a larger weight update on Dt(i).
Thedefinition of the loss function here is similar tothe one used in (Chiang et al, 2008) where onlythe top-1 translation candidate (i.e.
k = 1) istaken into account.3.3 System Combination SchemeIn the last step of our method, a strong transla-tion system v(u(?
*1), ..., u(?
*T)) is built from the3 Note that the definition of t?
here is different from that inthe original AdaBoost algorithm (Freund and Schapire,1997; Schapire, 2001) where t?
is a negative number when0.5t?
> .741ensemble of member systems {u(?
*1), ..., u(?
*T)}.In this work, a sentence-level combinationmethod is used to select the best translation fromthe pool of the n-best outputs of all the membersystems.Let H(u(?
*t)) (or Ht for short) be the set of then-best translation candidates produced by the t-thmember system u(?
*t), and H(v) be the union setof all Ht (i.e.
( ) tH v H=U ).
The final translationis generated from H(v) based on the followingscoring function:*1( )arg max ( ) ( , ( ))T t tte H ve e e H v?
?
?=?= ?
+?
(8)where ( )t e?
is the log-scaled model score of e inthe t-th member system, and t?
is the corre-sponding feature weight.
It should be noted thatie H?
may not exist in any 'i iH ?
.
In this case,we can still calculate the model score of e in anyother member systems, since all the member sys-tems are based on the same model and share thesame feature space.
( , ( ))e H v?
is a consensus-based scoring function which has been success-fully adopted in SMT system combination (Duanet al, 2009; Hildebrand and Vogel, 2008; Li etal., 2009).
The computation of ( , ( ))e H v?
isbased on a linear combination of a set of n-gramconsensuses-based features.
( , ( )) ( , ( ))n nne H v h e H v?
?
+ += ?
+?
( , ( ))n nnh e H v?
?
???
(9)For each order of n-gram, ( , ( ))nh e H v+ and( , ( ))nh e H v?
are defined to measure the n-gramagreement and disagreement between e and othertranslation candidates in H(v), respectively.
n?
+and n?
?
are the feature weights corresponding to( , ( ))nh e H v+ and ( , ( ))nh e H v?
.
As ( , ( ))nh e H v+ and( , ( ))nh e H v?
used in our work are exactly thesame as the features used in (Duan et al, 2009)and similar to the features used in (Hildebrandand Vogel, 2008; Li et al, 2009), we do not pre-sent the detailed description of them in this paper.If p orders of n-gram are used in computing( , ( ))e H v?
, the total number of features in thesystem combination will be 2T p+ ?
(T model-score-based features defined in Equation 8 and2 p?
consensus-based features defined in Equa-tion 9).
Since all these features are combinedlinearly, we use MERT to optimize them for thecombination model.4 OptimizationIf implemented naively, the translation speed ofthe final translation system will be very slow.For a given input sentence, each member systemhas to encode it individually, and the translationspeed is inversely proportional to the number ofmember systems generated by our method.
For-tunately, with the thought of computation, thereare a number of optimizations that can make thesystem much more efficient in practice.A simple solution is to run member systems inparallel when translating a new sentence.
Sinceall the member systems share the same data re-sources, such as language model and translationtable, we only need to keep one copy of the re-quired resources in memory.
The translationspeed just depends on the computing power ofparallel computation environment, such as thenumber of CPUs.Furthermore, we can use joint decoding tech-niques to save the computation of the equivalenttranslation hypotheses among member systems.In joint decoding of member systems, the searchspace is structured as a translation hypergraphwhere the member systems can share their trans-lation hypotheses.
If more than one member sys-tems share the same translation hypothesis, wejust need to compute the corresponding featurevalues only once, instead of repeating the com-putation in individual decoders.
In our experi-ments, we find that over 60% translation hy-potheses can be shared among member systemswhen the number of member systems is over 4.This result indicates that promising speed im-provement can be achieved by using the jointdecoding and hypothesis sharing techniques.Another method to speed up the system is toaccelerate n-gram language model with n-gramcaching techniques.
In this method, a n-gramcache is used to store the most frequently andrecently accessed n-grams.
When a new n-gramis accessed during decoding, the cache ischecked first.
If the required n-gram hits thecache, the corresponding n-gram probability isreturned by the cached copy rather than re-fetching the original data in language model.
Asthe translation speed of SMT system dependsheavily on the computation of n-gram languagemodel, the acceleration of n-gram languagemodel generally leads to substantial speed-up ofSMT system.
In our implementation, the n-gramcaching in general brings us over 30% speed im-provement of the system.7425 ExperimentsOur experiments are conducted on Chinese-to-English translation in three SMT systems.5.1 Baseline SystemsThe first SMT system is a phrase-based systemwith two reordering models including the maxi-mum entropy-based lexicalized reordering modelproposed by Xiong et al (2006) and the hierar-chical phrase reordering model proposed by Gal-ley and Manning (2008).
In this system allphrase pairs are limited to have source length ofat most 3, and the reordering limit is set to 8 bydefault4.The second SMT system is an in-house reim-plementation of the Hiero system which is basedon the hierarchical phrase-based model proposedby Chiang (2005).The third SMT system is a syntax-based sys-tem based on the string-to-tree model (Galley etal., 2006; Marcu et al, 2006), where both theminimal GHKM and SPMT rules are extractedfrom the bilingual text, and the composed rulesare generated by combining two or three minimalGHKM and SPMT rules.
Synchronous binariza-tion (Zhang et al, 2006; Xiao et al, 2009) is per-formed on each translation rule for the CKY-style decoding.In this work, baseline system refers to the sys-tem produced by the boosting-based systemcombination when the number of iterations (i.e.T ) is set to 1.
To obtain satisfactory baseline per-formance, we train each SMT system for 5 timesusing MERT with different initial values of fea-ture weights to generate a group of baseline can-didates, and then select the best-performing onefrom this group as the final baseline system (i.e.the starting point in the boosting process) for thefollowing experiments.5.2 Experimental SetupOur bilingual data consists of 140K sentencepairs in the FBIS data set5.
GIZA++ is employedto perform the bi-directional word alignment be-tween the source and target sentences, and thefinal word alignment is generated using the inter-sect-diag-grow method.
All the word-alignedbilingual sentence pairs are used to extractphrases and rules for the baseline systems.
A 5-gram language model is trained on the target-side4 Our in-house experimental results show that this systemperforms slightly better than Moses on Chinese-to-Englishtranslation tasks.5 LDC catalog number: LDC2003E14of the bilingual data and the Xinhua portion ofEnglish Gigaword corpus.
Berkeley Parser isused to generate the English parse trees for therule extraction of the syntax-based system.
Thedata set used for weight training in boosting-based system combination comes from NISTMT03 evaluation set.
To speed up MERT, all thesentences with more than 20 Chinese words areremoved.
The test sets are the NIST evaluationsets of MT04, MT05 and MT06.
The translationquality is evaluated in terms of case-insensitiveNIST version BLEU metric.
Statistical signifi-cant test is conducted using the bootstrap re-sampling method proposed by Koehn (2004).Beam search and cube pruning (Huang andChiang, 2007) are used to prune the search spacein all the three baseline systems.
By default, bothof the beam size and the size of n-best list are setto 20.In the settings of boosting-based system com-bination, the maximum number of iterations isset to 30, and k (in Equation 7) is set to 5.
The n-gram consensuses-based features (in Equation 9)used in system combination ranges from unigramto 4-gram.5.3 Evaluation of TranslationsFirst we investigate the effectiveness of theboosting-based system combination on the threesystems.Figures 2-5 show the BLEU curves on the de-velopment and test sets, where the X-axis is theiteration number, and the Y-axis is the BLEUscore of the system generated by the boosting-based system combination.
The points at itera-tion 1 stand for the performance of the baselinesystems.
We see, first of all, that all the threesystems are improved during iterations on thedevelopment set.
This trend also holds on the testsets.
After 5, 7 and 8 iterations, relatively stableimprovements are achieved by the phrase-basedsystem, the Hiero system and the syntax-basedsystem, respectively.
The BLEU scores tend toconverge to the stable values after 20 iterationsfor all the systems.
Figures 2-5 also show that theboosting-based system combination seems to bemore helpful to the phrase-based system than tothe Hiero system and the syntax-based system.For the phrase-based system, it yields over 0.6BLEU point gains just after the 3rd iteration onall the data sets.Table 1 summarizes the evaluation results,where the BLEU scores at iteration 5, 10, 15, 20and 30 are reported for the comparison.
We seethat the boosting-based system method stably ac-7433334353637380  5  10  15  20  25  30BLEU4[%]iteration numberBLEU on MT03 (dev.
)phrase-basedhierosyntax-basedFigure 2: BLEU scores on the development set3334353637380  5  10  15  20  25  30BLEU4[%]iteration numberBLEU on MT04 (test)phrase-basedhierosyntax-basedFigure 3: BLEU scores on the test  set of MT043233343536370  5  10  15  20  25  30BLEU4[%]iteration numberBLEU on MT05 (test)phrase-basedhierosyntax-basedFigure 4: BLEU scores on the test set of MT053031323334350  5  10  15  20  25  30BLEU4[%]iteration numberBLEU on MT06 (test)phrase-basedhierosyntax-basedFigure 5: BLEU scores on the test set of MT06Phrase-based Hiero Syntax-basedDev.
MT04 MT05 MT06 Dev.
MT04 MT05 MT06 Dev.
MT04 MT05 MT06Baseline 33.21 33.68 32.68 30.59 33.42 34.30 33.24 30.62 35.84 35.71 35.11 32.43Baseline+600best 33.32 33.93 32.84 30.76 33.48 34.46 33.39 30.75 35.95 35.88 35.23 32.58Boosting-5Iterations 33.95* 34.32* 33.33* 31.33* 33.73 34.48 33.44 30.83 36.03 35.92 35.27 33.09Boosting-10Iterations 34.14* 34.68* 33.42* 31.35* 33.75 34.65 33.75* 31.02 36.14 36.39* 35.47 33.15*Boosting-15Iterations 33.99* 34.78* 33.46* 31.45* 34.03* 34.88* 33.98* 31.20* 36.36* 36.46* 35.53* 33.43*Boosting-20Iterations 34.09* 35.11* 33.56* 31.45* 34.17* 35.00* 34.04* 31.29* 36.44* 36.79* 35.77* 33.36*Boosting-30Iterations 34.12* 35.16* 33.76* 31.59* 34.05* 34.99* 34.05* 31.30* 36.52* 36.81* 35.71* 33.46*Table 1: Summary of the results (BLEU4[%]) on the development and test sets.
* = significantly betterthan baseline (p < 0.05).hieves significant BLEU improvements after 15iterations, and the highest BLEU scores are gen-erally yielded after 20 iterations.Also as shown in Table 1, over 0.7 BLEUpoint gains are obtained on the phrase-based sys-tem after 10 iterations.
The largest BLEU im-provement on the phrase-based system is over 1BLEU point in most cases.
These results reflectthat our method is relatively more effective forthe phrase-based system than for the other twosystems, and thus confirms the fact we observedin Figures 2-5.We also investigate the impact of n-best listsize on the performance of baseline systems.
Forthe comparison, we show the performance of thebaseline systems with the n-best list size of 600(Baseline+600best in Table 1) which equals tothe maximum number of translation candidatesaccessed in the final combination system (combi-ne 30 member systems, i.e.
Boosing-30Iterations).7441520253035400  5  10  15  20  25  30Diversity(TER[%])iteration numberDiversity on MT03 (dev.
)phrase-basedhierosyntax-basedFigure 6: Diversity on the development set1015202530350  5  10  15  20  25  30Diversity(TER[%])iteration numberDiversity on MT04 (test)phrase-basedhierosyntax-basedFigure 7: Diversity on the test set of MT0415202530350  5  10  15  20  25  30Diversity(TER[%])iteration numberDiversity on MT05 (test)phrase-basedhierosyntax-basedFigure 8: Diversity on the test set of MT051520253035400  5  10  15  20  25  30Diversity(TER[%])iteration numberDiversity on MT06 (test)phrase-basedhierosyntax-basedFigure 9: Diversity on the test set of MT06As shown in Table 1, Baseline+600best obtainsstable improvements over Baseline.
It indicatesthat the access to larger n-best lists is helpful toimprove the performance of baseline systems.However, the improvements achieved by Base-line+600best are modest compared to the im-provements achieved by Boosting-30Iterations.These results indicate that the SMT systems canbenefit more from the diversified outputs ofmember systems rather than from larger n-bestlists produced by a single system.5.4 Diversity among Member SystemsWe also study the change of diversity among theoutputs of member systems during iterations.The diversity is measured in terms of the Trans-lation Error Rate (TER) metric proposed in(Snover et al, 2006).
A higher TER score meansthat more edit operations are performed if wetransform one translation output into anothertranslation output, and thus reflects a larger di-versity between the two outputs.
In this work, theTER score for a given group of member systemsis calculated by averaging the TER scores be-tween the outputs of each pair of member sys-tems in this group.Figures 6-9 show the curves of diversity onthe development and test sets, where the X-axisis the iteration number, and the Y-axis is the di-versity.
The points at iteration 1 stand for thediversities of baseline systems.
In this work, thebaseline?s diversity is the TER score of the groupof baseline candidates that are generated in ad-vance (Section 5.1).We see that the diversities of all the systemsincrease during iterations in most cases, though afew drops occur at a few points.
It indicates thatour method is very effective to generate diversi-fied member systems.
In addition, the diversitiesof baseline systems (iteration 1) are much lower745than those of the systems generated by boosting(iterations 2-30).
Together with the results shownin Figures 2-5, it confirms our motivation thatthe diversified translation outputs can lead toperformance improvements over the baselinesystems.Also as shown in Figures 6-9, the diversity ofthe Hiero system is much lower than that of thephrase-based and syntax-based systems at eachindividual setting of iteration number.
This inter-esting finding supports the observation that theperformance of the Hiero system is relativelymore stable than the other two systems as shownin Figures 2-5.
The relative lack of diversity inthe Hiero system might be due to the spuriousambiguity in Hiero derivations which generallyresults in very few different translations in trans-lation outputs (Chiang, 2007).5.5 Evaluation of Oracle TranslationsIn this set of experiments, we evaluate the oracleperformance on the n-best lists of the baselinesystems and the combined systems generated byboosting-based system combination.
Our primarygoal here is to study the impact of our method onthe upper-bound performance.Table 2 shows the results, where Base-line+600best stands for the top-600 translationcandidates generated by the baseline systems,and Boosting-30iterations stands for the ensem-ble of 30 member systems?
top-20 translationcandidates.
As expected, the oracle performanceof Boosting-30Iterations is significantly higherthan that of Baseline+600best.
This result indi-cates that our method can provide much ?better?translation candidates for system combinationthan enlarging the size of n-best list naively.
Italso gives us a rational explanation for the sig-nificant improvements achieved by our methodas shown in Section 5.3.DataSetMethod Phrase-basedHiero Syntax-basedBaseline+600best 46.36 46.51 46.92 Dev.Boosting-30Iterations 47.78* 47.44* 48.70*Baseline+600best 43.94 44.52 46.88 MT04Boosting-30Iterations 45.97* 45.47* 49.40*Baseline+600best 42.32 42.47 45.21 MT05Boosting-30Iterations 44.82* 43.44* 47.02*Baseline+600best 39.47 39.39 40.52 MT06Boosting-30Iterations 41.51* 40.10* 41.88*Table 2: Oracle performance of various systems.
* = significantly better than baseline (p < 0.05).6 Related WorkBoosting is a machine learning (ML) method thathas been well studied in the ML community(Freund, 1995; Freund and Schapire, 1997;Collins et al, 2002; Rudin et al, 2007), and hasbeen successfully adopted in natural languageprocessing (NLP) applications, such as documentclassification (Schapire and Singer, 2000) andnamed entity classification (Collins and Singer,1999).
However, most of the previous work didnot study the issue of how to improve a singleSMT engine using boosting algorithms.
To ourknowledge, the only work addressing this issue is(Lagarda and Casacuberta, 2008) in which theboosting algorithm was adopted in phrase-basedSMT.
However, Lagarda and Casacuberta(2008)?s method calculated errors over thephrases that were chosen by phrase-based sys-tems, and could not be applied to many otherSMT systems, such as hierarchical phrase-basedsystems and syntax-based systems.
Differingfrom Lagarda and Casacuberta?s work, we areconcerned more with proposing a generalframework which can work with most of the cur-rent SMT models and empirically demonstratingits effectiveness on various SMT systems.There are also some other studies on buildingdiverse translation systems from a single transla-tion engine for system combination.
The firstattempt is (Macherey and Och, 2007).
They em-pirically showed that diverse translation systemscould be generated by changing parameters atearly-stages of the training procedure.
FollowingMacherey and Och (2007)?s work, Duan et al(2009) proposed a feature subspace method tobuild a group of translation systems from variousdifferent sub-models of an existing SMT system.However, Duan et al (2009)?s method relied onthe heuristics used in feature sub-space selection.For example, they used the remove-one-featurestrategy and varied the order of n-gram languagemodel to obtain a satisfactory group of diversesystems.
Compared to Duan et al (2009)?smethod, a main advantage of our method is thatit can be applied to most of the SMT systemswithout designing any heuristics to adapt it to thespecified systems.7 Discussion and Future WorkActually the method presented in this paper isdoing something rather similar to MinimumBayes Risk (MBR) methods.
A main differencelies in that the consensus-based combinationmethod here does not model the posterior prob-ability of each hypothesis (i.e.
all the hypothesesare assigned an equal posterior probability whenwe calculate the consensus-based features).746Greater improvements are expected if MBRmethods are used and consensus-based combina-tion techniques smooth over noise in the MERTpipeline.In this work, we use a sentence-level systemcombination method to generate final transla-tions.
It is worth studying other more sophisti-cated alternatives, such as word-level andphrase-level system combination, to further im-prove the system performance.Another issue is how to determine an appro-priate number of iterations for boosting-basedsystem combination.
It is especially importantwhen our method is applied in the real-worldapplications.
Our empirical study shows that thestable and satisfactory improvements can beachieved after 6-8 iterations, while the largestimprovements can be achieved after 20 iterations.In our future work, we will study in-depth prin-cipled ways to determine the appropriate numberof iterations for boosting-based system combina-tion.8 ConclusionsWe have proposed a boosting-based system com-bination method to address the issue of buildinga strong translation system from a group of weaktranslation systems generated from a single SMTengine.
We apply our method to three state-of-the-art SMT systems, and conduct experimentson three NIST Chinese-to-English MT evalua-tions test sets.
The experimental results show thatour method is very effective to improve thetranslation accuracy of the SMT systems.AcknowledgementsThis work was supported in part by the NationalScience Foundation of China (60873091) and theFundamental Research Funds for the CentralUniversities (N090604008).
The authors wouldlike to thank the anonymous reviewers for theirpertinent comments, Tongran Liu, ChunliangZhang and Shujie Yao for their valuable sugges-tions for improving this paper, and Tianning Liand Rushan Chen for developing parts of thebaseline systems.ReferencesDavid Chiang.
2005.
A hierarchical phrase-basedmodel for statistical machine translation.
In Proc.of ACL 2005, Ann Arbor, Michigan, pages 263-270.David Chiang.
2007.
Hierarchical phrase-based trans-lation.
Computational Linguistics, 33(2):201-228.David Chiang, Yuval Marton and Philip Resnik.
2008.Online Large-Margin Training of Syntactic andStructural Translation Features.
In Proc.
ofEMNLP 2008, Honolulu, pages 224-233.Michael Collins and Yoram Singer.
1999.
Unsuper-vised Models for Named Entity Classification.
InProc.
of EMNLP/VLC 1999, pages 100-110.Michael Collins, Robert Schapire and Yoram Singer.2002.
Logistic Regression, AdaBoost and BregmanDistances.
Machine Learning, 48(3): 253-285.Brooke Cowan, Ivona Ku?erov?
and Michael Collins.2006.
A discriminative model for tree-to-tree trans-lation.
In Proc.
of EMNLP 2006, pages 232-241.Yuan Ding and Martha Palmer.
2005.
Machine trans-lation using probabilistic synchronous dependencyinsertion grammars.
In Proc.
of ACL 2005, AnnArbor, Michigan, pages 541-548.Nan Duan, Mu Li, Tong Xiao and Ming Zhou.
2009.The Feature Subspace Method for SMT SystemCombination.
In Proc.
of EMNLP 2009, pages1096-1104.Jason Eisner.
2003.
Learning non-isomorphic treemappings for machine translation.
In Proc.
of ACL2003, pages 205-208.Yoav Freund.
1995.
Boosting a weak learning algo-rithm by majority.
Information and Computation,121(2): 256-285.Yoav Freund and Robert Schapire.
1997.
A decision-theoretic generalization of on-line learning and anapplication to boosting.
Journal of Computer andSystem Sciences, 55(1):119-139.Michel Galley, Jonathan Graehl, Kevin Knight,Daniel Marcu, Steve DeNeefe, Wei Wang andIgnacio Thayer.
2006.
Scalable inferences andtraining of context-rich syntax translation models.In Proc.
of ACL 2006, Sydney, Australia, pages961-968.Michel Galley and Christopher D. Manning.
2008.
ASimple and Effective Hierarchical Phrase Reorder-ing Model.
In Proc.
of EMNLP 2008, Hawaii,pages 848-856.Almut Silja Hildebrand and Stephan Vogel.
2008.Combination of machine translation systems viahypothesis selection from combined n-best lists.
InProc.
of the 8th AMTA conference, pages 254-261.Liang Huang and David Chiang.
2007.
Forest rescor-ing: Faster decoding with integrated languagemodels.
In Proc.
of ACL 2007, Prague, Czech Re-public, pages 144-151.747Philipp Koehn, Franz Och and Daniel Marcu.
2003.Statistical Phrase-Based Translation.
In Proc.
ofHLT-NAACL 2003, Edmonton, USA, pages 48-54.Philipp Koehn.
2004.
Statistical Significance Tests forMachine Translation Evaluation.
In Proc.
ofEMNLP 2004, Barcelona, Spain, pages 388-395.Antonio Lagarda and Francisco Casacuberta.
2008.Applying Boosting to Statistical Machine Transla-tion.
In Proc.
of the 12th EAMT conference, pages88-96.Mu Li, Nan Duan, Dongdong Zhang, Chi-Ho Li andMing Zhou.
2009.
Collaborative Decoding: PartialHypothesis Re-Ranking Using Translation Consen-sus between Decoders.
In Proc.
of ACL-IJCNLP2009, Singapore, pages 585-592.Percy Liang, Alexandre Bouchard-C?t?, Dan Kleinand Ben Taskar.
2006.
An end-to-end discrimina-tive approach to machine translation.
In Proc.
ofCOLING/ACL 2006, pages 104-111.Yang Liu, Qun Liu and Shouxun Lin.
2006.
Tree-to-String Alignment Template for Statistical MachineTranslation.
In Proc.
of ACL 2006, pages 609-616.Wolfgang Macherey and Franz Och.
2007.
An Em-pirical Study on Computing Consensus Transla-tions from Multiple Machine Translation Systems.In Proc.
of EMNLP 2007, pages 986-995.Daniel Marcu, Wei Wang, Abdessamad Echihabi andKevin Knight.
2006.
SPMT: Statistical machinetranslation with syntactified target languagephrases.
In Proc.
of EMNLP 2006, Sydney, Aus-tralia, pages 44-52.Evgeny Matusov, Nicola Ueffing and Hermann Ney.2006.
Computing consensus translation from mul-tiple machine translation systems using enhancedhypotheses alignment.
In Proc.
of EACL 2006,pages 33-40.Franz Och and Hermann Ney.
2002.
DiscriminativeTraining and Maximum Entropy Models for Statis-tical Machine Translation.
In Proc.
of ACL 2002,Philadelphia, pages 295-302.Franz Och.
2003.
Minimum Error Rate Training inStatistical Machine Translation.
In Proc.
of ACL2003, Japan, pages 160-167.Antti-Veikko Rosti, Spyros Matsoukas and RichardSchwartz.
2007.
Improved Word-Level SystemCombination for Machine Translation.
In Proc.
ofACL 2007, pages 312-319.Cynthia Rudin, Robert Schapire and Ingrid Daube-chies.
2007.
Analysis of boosting algorithms usingthe smooth margin function.
The Annals of Statis-tics, 35(6): 2723-2768.Robert Schapire and Yoram Singer.
2000.
BoosTexter:A boosting-based system for text categorization.Machine Learning, 39(2/3):135-168.Robert Schapire.
The boosting approach to machinelearning: an overview.
2001.
In Proc.
of MSRIWorkshop on Nonlinear Estimation and Classifica-tion, Berkeley, CA, USA, pages 1-23.Matthew Snover, Bonnie Dorr, Richard Schwartz,Linnea Micciulla and John Makhoul.
2006.
AStudy of Translation Edit Rate with Targeted Hu-man Annotation.
In Proc.
of the 7th AMTA confer-ence, pages 223-231.Tong Xiao, Mu Li, Dongdong Zhang, Jingbo Zhu andMing Zhou.
2009.
Better Synchronous Binarizationfor Machine Translation.
In Proc.
of EMNLP 2009,Singapore, pages 362-370.Deyi Xiong, Qun Liu and Shouxun Lin.
2006.
Maxi-mum Entropy Based Phrase Reordering Model forStatistical Machine Translation.
In Proc.
of ACL2006, Sydney, pages 521-528.Hao Zhang, Liang Huang, Daniel Gildea and KevinKnight.
2006.
Synchronous Binarization for Ma-chine Translation.
In Proc.
of HLT-NAACL 2006,New York, USA, pages 256- 263.748
