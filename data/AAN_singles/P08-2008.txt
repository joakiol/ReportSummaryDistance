Proceedings of ACL-08: HLT, Short Papers (Companion Volume), pages 29?32,Columbus, Ohio, USA, June 2008. c?2008 Association for Computational LinguisticsNovel Semantic Features for Verb Sense DisambiguationDmitriy DligachThe Center for ComputationalLanguage and EducationResearch1777 Exposition DriveBoulder, Colorado 80301Dmitriy.Dligach@colorado.eduMartha PalmerDepartment of LinguisticsUniversity of Coloradoat Boulder295 UCBBoulder, Colorado 80309Martha.Palmer@colorado.eduAbstractWe propose a novel method for extractingsemantic information about a verb's argumentsand apply it to Verb Sense Disambiguation(VSD).
We contrast this method with twopopular approaches to retrieving this informa-tion and show that it improves the perform-ance of our VSD system and outperforms theother two approaches1 IntroductionThe task of Verb Sense Disambiguation (VSD)consists in automatically assigning a sense to averb (target verb) given its context.
In a supervisedsetting, a VSD system is usually trained on a set ofpre-labeled examples; the goal of this system is totag unseen examples with a sense from some senseinventory.An automatic VSD system usually has at itsdisposal a diverse set of features among which thesemantic features play an important role: verbsense distinctions often depend on the distinctionsin the semantics of the target verb's arguments(Hanks, 1996).
Therefore, some method of captur-ing the semantic knowledge about the verb's argu-ments is crucial to the success of a VSD system.The approaches to obtaining this kind ofknowledge can be based on extracting it from elec-tronic dictionaries such as WordNet (Fellbaum,1998), using Named Entity (NE) tags, or a combi-nation of both (Chen, 2005).
In this paper, we pro-pose a novel method for obtaining semanticknowledge about words and show how it can beapplied to VSD.
We contrast this method with theother two approaches and compare their perform-ances in a series of experiments.2 Lexical and Syntactic FeaturesWe view VSD as a supervised learning problem,solving which requires three groups of features:lexical, syntactic, and semantic.
Lexical featuresinclude all open class words; we extract them fromthe target sentence and the two surrounding sen-tences.
We also use as features two words on theright and on the left of the target verb as well astheir POS tags.
We extract syntactic features fromconstituency parses; they indicate whether the tar-get verb has a subject/object and what their headwords and POS tags are, whether the target verb isin a passive or active form, whether the target verbhas a subordinate clause, and whether the targetverb has a PP adjunct.
Additionally, we implementseveral new syntactic features, which have notbeen used in VSD before: the path through theparse tree from the target verb to the verb's argu-ments and the subcategorization frame, as used insemantic role labeling.3 Semantic FeaturesConsider the verb prepare for which our sense in-ventory defines two senses: (1) to put together,assemble (e.g.
He is going to prepare breakfast forthe whole crowd ; I haven't prepared my lecture29yet); (2) to make ready (e.g.
She prepared the chil-dren for school every morning).
Knowing the se-mantic class of the objects breakfast, lecture andchildren is the decisive factor in distinguishing thetwo senses and facilitates better generalizationfrom the training data.
One way to obtain thisknowledge is from WordNet (WN) or from theoutput of a NE-tagger.
However, both approachessuffer from the same limitation: they collapse mul-tiple semantic properties of nouns into a finitenumber of predefined static classes.
E.g., the mostimmediate hypernym of breakfast in WN is meal,while the most immediate hypernym of lecture isaddress, which makes these two nouns unrelated.Yet, breakfast and lecture are both social eventswhich share some semantic properties: they bothcan be attended, hosted, delivered, given, held,organized etc.
To discover these class-like descrip-tions of nouns, one can observe which verbs takethese nouns as objects.
E.g.
breakfast can serve asthe object of serve, host, attend, and cook  whichare all indicative of breakfast's semantic proper-ties.Given a noun, we can dynamically retrieveother verbs that take that noun as an object from adependency-parsed corpus; we call this kind ofdata Dynamic Dependency Neighbors  (DDNs)because it is obtained dynamically and based onthe dependency relations in the neighborhood ofthe noun of interest.
The top 501 DDNs can beviewed as a reliable inventory of semantic proper-ties of the noun.
To collect this data, we utilizedtwo resources: (1) MaltParser (Nivre, 2007) ?
ahigh-efficiency dependency parser; (2) EnglishGigaword ?
a large corpus of 5.7M news articles.We preprocessed Gigaword with MaltParser, ex-tracted all pairs of nouns and verbs that wereparsed as participants of the object-verb relation,and counted the frequency of occurrence of all theunique pa irs.
Finally, we indexed the resulting re-cords of the form <frequency, verb, object> usingthe Lucene2 indexing engine.As an example, consider four nouns: dinner,breakfast, lecture, child.
When used as the objectsof prepare, the first three of them correspond to theinstances of the sense 1 of prepare; the fourth one1 In future, we will try to optimize this parameter2 Available at http://lucene.apache.org/corresponds to an instance of the sense 2.
With thehelp of our index, we can retrieve their DDNs.There is a considerable overlap among the DDNsof the first three nouns and a much smaller overlapbetween child  and the first three nouns.
E.g., din-ner and breakfast have 34 DDNs in common,while dinner and child  only share 14.Once we have set up the framework for the ex-traction of DDNs, the algorithm for applying themto VSD is straightforward: (1) find the noun objectof the ambiguous verb (2) extract the DDNs forthat noun (3) sort the DDNs by frequency and keepthe top 50 (4) include these DDNs in the featurevector so that each of the extracted verbs becomesa separate feature.4 Relevant WorkAt the core of our work lies the notion of distrib u-tional similarity (Harris, 1968), which states thatsimilar words occur in similar contexts.
In varioussources, the notion of context ranges from bag-of-words-like approaches to more structured ones inwhich syntax plays a role.
Schutze (1998) usedbag-of-words contexts for sense discrimination.Hindle (1990) grouped nouns into thesaurus-likelists based on the similarity of their syntactic con-texts.
Our approach is similar with the differencethat we do not group noun arguments into finitecategories, but instead leave the category bounda-ries blurry and allow overlaps.The DDNs are essentially a form of worldknowledge which we extract automatically andapply to VSD.
Other researches attacked the prob-lem of unsupervised extraction of world knowl-edge: Schubert (2003) reports a method forextracting general facts about the world from tree-banked Brown corpus.
Lin and Pantel in (2001)describe their DIRT system for extraction of para-phrase-like inference rules.5 EvaluationWe selected a subset of the verbs annotated in theOntoNotes project (Chen, 2007) that had at least50 instances.
The resulting data set consisted of46,577 instances of 217 verbs.
The predominantsense baseline for this data is 68%.
We used30libsvm3 for classification.
We computed the accu-racy and error rate using 5-fold cross-validation.5.1 Experiments with a limited set of featuresThe main objective of this experiment was to iso-late the effect of the novel semantic features weproposed in this paper, i.e.
the DDN features.
To-ward that goal, we stripped our system of all thefeatures but the most essential ones to investigatewhether the DDN features would have a clearlypositive or negative impact on the system perform-ance.
Lexical features are the most essential to oursystem: a model that includes only the lexical fea-tures achieves an accuracy of 80.22, while the ac-curacy of our full-blown VSD system is 82.88%4.Since the DDN features have no effect when theobject is not present, we identified 18,930 in-stances where the target verb had an object (about41% of all instances) and used only them in theexperiment.We built three models that included (1) thelexical features only (2) the lexical and the DDNfeatures (3) the lexical and the object features.
Theobject features consist of the head word of the NPobject and the head word's POS tag.
The object isincluded since extracting the DDN features re-quires knowledge of the object; therefore the per-formance of a model that only includes lexicalfeatures cannot be considered a fair baseline forstudying the effect of the DDN features.
Resultsare in Table 4.Features Included inModelAccuracy, % Error Rate, %Lexical 78.95 21.05Lexical + Object  79.34 20.66Lexical + DDN 82.40 17.60Table 4.
Experiments with object instancesAs we see, the model that includes the DDNfeatures performs more than 3 percentage pointsbetter than the model that only includes the objectfeatures (approximately 15% reduction in errorrate).
Also, based on the comparison of the per-formance of the "lexical features only" and the"lexical + DDN" models, we can claim that the3 http://www.csie.ntu.edu.tw/~cjlin/libsvm/4 Given this high baseline, we include error rate when report-ing the results of the experiments as it is more informativeknowledge of the DDNs provides richer semanticknowledge than just the knowledge of the object'shead word.5.2 Integrating the DDN features into a full-fledged VSD systemThe objective of this experiment was to investigatewhether the DDN features improve the perform-ance of a full-fledged VSD system.
We built twomodels which consisted of (1) the entire set of fea-tures (2) all the features of the first model exclud-ing the DDN features.
The entire data set (46Kinstances) participated in the experiment.
Resultsare in Table 5.Features Included inModelAccuracy, % Error Rate, %All Features ?
DDN 82.38 17.62All Features 82.88 17.12Table 5.
Performance of the full-fledged VSD systemThe DDN features improved performance by0.5% (3% drop in error rate).
The difference be-tween the accuracies is statistically significant(p=0.05).5.3 Relative Contribution of Various Seman-tic Fe aturesThe goal of this experiment was to study the rela-tive contribution of various semantic features tothe performance of our VSD system.
We built fivemodels each of which, in addition to the lexicaland syntactic features, included only certaintype(s) of semantic feature: (1) WN (2) NE (3)WN and NE (4) DDN (5) no semantic features(baseline).
All 46K instances participated in theexperiment.
The results are shown in Table 6.Features Included in Model Accuracy,%Error Rate,%Lexical + Syntactic 81.82 18.18Lexical + Syntactic + WN 82.34 17.60Lexical + Syntactic + NE 82.01 17.99Lexical + Syntactic + WN + NE 82.38 17.62Lexical + Syntactic + DDN 82.97 17.03Table 6.
Relative Contribution of Semantic FeaturesThe DDN features outperform the other twotypes of semantic features used separately and inconjunction.
The difference in performance is sta-tistically significant (p=0.05).316 Discussion and ConclusionAs we saw, the novel semantic features we pro-posed are beneficial to the task of VSD: they re-sulted in a decrease in error rate from 3% to 15%,depending on the particular experiment.
We alsodiscovered that the DDN features contributed twiceas much as the other two types of semantic featurescombined: adding the WN and NE features to thebaseline resulted in about a 3% decrease in errorrate, while adding the DDN features caused a morethan 6% drop.Our results suggest that DDNs duplicate the ef-fect of WN and NE: our system achieved the sameperformance when all three types of semantic fea-tures were used and when we discarded WN andNE features and kept only the DDNs.
This findingis important because such resources as WN andNE-taggers are domain and language specificwhile the DDNs have the advantage of being ob-tainable from a large collection of texts in the do-main or language of interest.
Thus, the DDNs canbecome a crucial part of building a robust VSDsystem for a resource-poor domain or language,given a high-accuracy parser.7 Future WorkIn this paper we only experimented with verbs'objects, however the concept of DDNs can be eas-ily extended to other arguments of the target verb.Also, we only utilized the object-verb relation inthe dependency parses, but the range of potentiallyuseful relations does not have to be limited only toit.
Finally, we used as features the 50 most fre-quent verbs that took the noun argument as an ob-ject.
However, the raw frequency is certainly notthe only way to rank the verbs; we plan on explor-ing other metrics such as Mutual Information.AcknowledgementsWe gratefully acknowledge the support of the Na-tional Science Foundation Grant NSF-0715078,Consistent Criteria for Word Sense Disambigua-tion, and the GALE program of the Defense Ad-vanced Research Projects Agency, Contract No.HR0011-06-C-0022, a subcontract from the BBN-AGILE Team.
Any opinions, findings, and con-clusions or recommendations expressed in this ma-terial are those of the authors and do notnecessarily reflect the views of the National Sc i-ence Foundation.
We also thank our colleaguesRodney Nielsen and Philipp Wetzler for parsingEnglish Gigaword with MaltParser.ReferencesJinying Chen, Dmitriy Dligach and Martha Palmer.2007.
Towards Large-scale High-Performance Eng-lish Verb Sense Disambiguation by Using Linguisti-cally Motivated Features.
In InternationalConference on Semantic Computing.
Issue , 17-19.Jinying Chen and Martha Palmer.
2005.
Towards Ro-bust High Performance Word Sense Disambiguationof English Verbs Using Rich Linguistic Features.
InProceedings of the 2nd International Joint Confer-ence on Natural Language Processing, Korea.Christiane Fellbaum.
1998.
WordNet - an ElectronicLexical Database.
The MIT Press, Cambridge, Mas-sachusetts, London, UK.Patrick Hanks, 1996.
Contextual Dependencies andLexical Sets.
In The Int.
Journal of Corpus Linguis-tics, 1:1Zelig S. Harris.
1968.
Mathematical Structures of Lan-guage.
New York.
Wiley.Donald Hindle.
1990.
Noun Classification from Predi-cate-Argument Structures.
In Proceedings of the 28thAnnual Meeting of Association for ComputationalLinguistics.
Pages 268-275Dekang Lin and Patrick Pantel.
2001.
DIRT - Discoveryof Inference Rules from Text.
In Proceedings ofACM Conference on Knowledge Discovery and DataMining.
pp.
323-328.
San Francisco, CA.Joakim Nivre, Johan Hall, Jens Nilsson, et.
al.
Malt -Parser: A language-independent system for data-driven dependency parsing.
2007.
In Natural Lan-guage Engineering, 13(2), 95-135.Lenhart Schubert and Matthew Tong, Extracting andevaluating general world knowledge from the Browncorpus.
2003.
In Proc.
of the HLT/NAACL Workshopon Text Meaning, May 31, Edmonton, Alberta, Can-ada.Hinrich Schutze.
1998.
Automatic Word Sense Dis-crimination.
In Computational Linguistics, 24(1):97-12332
