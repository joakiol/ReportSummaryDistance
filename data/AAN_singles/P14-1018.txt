Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 186?196,Baltimore, Maryland, USA, June 23-25 2014.c?2014 Association for Computational LinguisticsInferring User Political Preferences from Streaming CommunicationsSvitlana Volkova,1Glen Coppersmith2and Benjamin Van Durme1,21Center for Language and Speech Processing,2Human Language Technology Center of Excellence,Johns Hopkins University, Baltimore, MD 21218svitlana@jhu.edu, coppersmith@jhu.edu, vandurme@cs.jhu.eduAbstractExisting models for social media per-sonal analytics assume access to thou-sands of messages per user, even thoughmost users author content only sporadi-cally over time.
Given this sparsity, we:(i) leverage content from the local neigh-borhood of a user; (ii) evaluate batch mod-els as a function of size and the amountof messages in various types of neighbor-hoods; and (iii) estimate the amount oftime and tweets required for a dynamicmodel to predict user preferences.
Weshow that even when limited or no self-authored data is available, language fromfriend, retweet and user mention commu-nications provide sufficient evidence forprediction.
When updating models overtime based on Twitter, we find that polit-ical preference can be often be predictedusing roughly 100 tweets, depending onthe context of user selection, where thiscould mean hours, or weeks, based on theauthor?s tweeting frequency.1 IntroductionInferring latent user attributes such as gender, age,and political preferences (Rao et al, 2011; Za-mal et al, 2012; Cohen and Ruths, 2013) auto-matically from personal communications and so-cial media including emails, blog posts or publicdiscussions has become increasingly popular withthe web getting more social and volume of dataavailable.
Resources like Twitter1or Facebook2become extremely valuable for studying the un-derlying properties of such informal communica-tions because of its volume, dynamic nature, anddiverse population (Lunden, 2012; Smith, 2013).1http://www.demographicspro.com/2http://www.wolframalpha.com/facebook/The existing batch models for predicting latentuser attributes rely on thousands of tweets perauthor (Rao et al, 2010; Conover et al, 2011;Pennacchiotti and Popescu, 2011a; Burger et al,2011; Zamal et al, 2012; Nguyen et al, 2013).However, most Twitter users are less prolific thanthose examined in these works, and thus do notproduce the thousands of tweets required to obtaintheir levels of accuracy e.g., the median number oftweets produced by a random Twitter user per dayis 10.
Moreover, recent changes to Twitter APIquerying rates further restrict the speed of accessto this resource, effectively reducing the amount ofdata that can be collected in a given time period.In this paper we analyze and go beyond staticmodels formulating personal analytics in socialmedia as a streaming task.
We first evaluate batchmodels that are cognizant of low-resource predic-tion setting described above, maximizing the effi-ciency of content in calculating personal analytics.To the best of our knowledge, this is the first workthat makes explicit the tradeoff between accuracyand cost (manifest as calls to the Twitter API),and optimizes to a different tradeoff than state-of-the-art approaches, seeking maximal performancewhen limited data is available.
In addition, wepropose streaming models for personal analyticsthat dynamically update user labels based on theirstream of communications which has been ad-dressed previously by Van Durme (2012b).
Suchmodels better capture the real-time nature of evi-dence being used in latent author attribute predic-tions tasks.
Our main contributions include:- develop low-resource and real-time dynamicapproaches for personal analytics using as anexample the prediction of political preferenceof Twitter users;- examine the relative utility of six differentnotions of ?similarity?
between users in animplicit Twitter social network for personalanalytics;186- experiments are performed across multipledatasets supporting the prediction of politi-cal preference in Twitter, to highlight the sig-nificant differences in performance that arisefrom the underlying collection and annota-tion strategies.2 Identifying Twitter Social GraphTwitter users interact with one another and en-gage in direct communication in different wayse.g., using retweets, user mentions e.g., @youtubeor hashtags e.g., #tcot, in addition to having ex-plicit connections among themselves such as fol-lowing, friending.
To investigate all types of socialrelationships between Twitter users and constructTwitter social graphs we collect lists of followersand friends, and extract user mentions, hashtags,replies and retweets from communications.32.1 Social Graph DefinitionLets define an attributed, undirected graph G =(V,E), where V is a set of vertices and E is a setof edges.
Each vertex virepresents someone ina communication graph i.e., communicant: herea Twitter user.
Each vertex is attributed with afeature vector~f(vi) which encodes communica-tions e.g., tweets available for a given user.
Eachvertex is associated with a latent attribute a(vi),in our case it is binary a(vi) ?
{D,R}, whereD stands for Democratic and R for Republicanusers.
Each edge eij?
E represents a connec-tion between viand vj, eij= (vi, vj) and definesdifferent social circles between Twitter users e.g.,follower (f), friend (b), user mention (m), hash-tag (h), reply (y) and retweet (w).
Thus, E ?V(2)?
{f, b, h,m,w, y}.
We denote a set of edgesof a given type as ?r(E) for r ?
{f, b, h,m,w, y}.We denote a set of vertices adjacent to viby so-cial circle type r as Nr(vi) which is equivalent to{vj| eij?
?r(E)}.
Following Filippova (2012)we refer to Nr(vi) as vi?s social circle, otherwiseknown as a neighborhood.
In most cases, we onlywork with a sample of a social circle, denoted byN?r(vi) where |N?r(vi)| = k is its size for vi.Figure 1 presents an example of a social graphderived from Twitter.
Notably, users from differ-ent social circles can be shared across the users ofthe same or different classes e.g., a user vjcan be3The code and detailed explanation on how we col-lected all six types of user neighbors and their com-munications using Twitter API can be found here:http://www.cs.jhu.edu/ svitlana/Figure 1: An example of a social graph with follower, friend,@mention, reply, retweet and hashtag social circles for eachuser of interest e.g., blue: Democratic, red: Republican.in both follower circle vj?
Nf(vi), vi?
D andretweet circle vj?
Nw(vk), vk?
R.2.2 Candidate-Centric GraphWe construct candidate-centric graph Gcandbylooking into following relationships between theusers and Democratic or Republican candidatesduring the 2012 US Presidential election.
In theFall of 2012, leading up to the elections, we ran-domly sampled n = 516 Democratic and m =515 Republican users.
We labeled users as Demo-cratic if they exclusively follow both Democraticcandidates4?
BarackObama and JoeBiden butdo not follow both Republican candidates ?
Mit-tRomney and RepPaulRyan and vice versa.
Wecollectively refer to D and R as our ?users of in-terest?
for which we aim to predict political prefer-ence.
For each such user we collect recent tweetsand randomly sample their immediate k = 10neighbors from follower, friend, user mention, re-ply, retweet and hashtag social circles.2.3 Geo-Centric GraphWe construct a geo-centric graph Ggeoby col-lecting n = 135 Democratic and m = 135 Re-publican users from the Maryland, Virginia andDelaware region of the US with self-reported po-litical preference in their biographies.
Similar tothe candidate-centric graph, for each user we col-lect recent tweets and randomly sample user socialcircles in the Fall of 2012.
We collect this data toget a sample of politically less active users com-pared to the users from candidate-centric graph.2.4 ZLR GraphWe also consider a GZLRgraph constructed froma dataset previously used for political affiliation4As of Oct 12, 2012, the number of followers for Obama,Biden, Romney and Ryan were 2m, 168k, 1.3m and 267k.187classification (Zamal et al, 2012).
This datasetconsists of 200 Republican and 200 Democraticusers associated with 925 tweets on average peruser.5Each user has on average 6155 friends with642 tweets per friend.
Sharing restrictions and ratelimits on Twitter data collection only allowed us torecreate a semblance of ZLR data6?
193 Demo-cratic and 178 Republican users with 1K tweetsper user, and 20 neighbors of four types includingfollower, friends, user mention and retweet with200 tweets per neighbor for each user of interest.3 Batch ModelsBaseline User Model As input we are given a setof vertices representing users of interest vi?
Valong with feature vectors~f(vi) derived from con-tent authored by the user of interest.
Each useris associated with a non-zero number of publiclyposted tweets.
Our goal is assign to a categoryeach user of interest vibased on~f(vi).
Here wefocus on a binary assignment into the categoriesDemocratic D or Republican R. The log-linearmodel7for such binary classification is:?vi={D (1 + exp[?~?
?~f(vi)])?1?
0.5,R otherwise.
(1)where features are normalized word ngram countsextracted from vi?s tweets~ft(vi) : D?t(vi)?
R.The proposed baseline model follows the sametrends as the existing state-of-the-art approachesfor user attribute classification in social media asdescribed in Section 8.
Next we propose to ex-tend the baseline model by taking advantage oflanguage in user social circles as describe below.Neighbor Model As input we are given user-localneighborhood Nr(vi), where r is a neighborhoodtype.
Besides the neighborhood?s type r, each ischaracterized by:?
the number of communications per neighbor~ft(Nr), t = {5, 10, 15, 25, 50, 100, 200};5The original dataset was collected in 2012 and hasbeen recently released at http://icwsm.cs.mcgill.ca/.
Politi-cal labels are extracted from http://www.wefollow.com as de-scribed by Pennacchiotti and Popescu (2011b).6This inability to perfectly replicate prior work based onTwitter is a recognized problem throughout the community ofcomputational social science, arising from the data policies ofTwitter itself, it is not specific to this work.7We use log-linear models over reasonable alternativessuch as perceptron or SVM, following the practice of a widerange of previous work in related areas (Smith, 2004; Liu etal., 2005; Poon et al, 2009) including text classification in so-cial media (Van Durme, 2012b; Yang and Eisenstein, 2013).?
the order of the social circle ?
the num-ber of neighbors per user of interest |Nr| =deg(vi), n = {1, 2, 5, 10}.Our goal is to classify users of interest usingevidence (e.g., communications) from their localneighborhood?n~ft[Nr(vi)] ?~f(Nr) as Demo-cratic or Republican.
The corresponding log-linear model is defined as:?Nr={D (1 + exp[?~?
?~f(Nr)])?1?
0.5,R otherwise.
(2)To check whether our static models are cog-nizant of low-resource prediction settings we com-pare the performance of the user model from Eq.1and the neighborhood model from Eq.2.
Follow-ing the streaming nature of social media, we seethe scarce available resource as the number of re-quests allowed per day to the Twitter API.
Herewe abstract this to a model assumption where wereceive one tweet tkat a time and aim to maximizeclassification performance with as few tweets peruser as possible:8?
for the baseline user model:minimizek?ktk(vi),(3)?
for the neighborhood model:minimizek?n?ktk[Nr(vi)].
(4)4 Streaming ModelsWe rely on straightforward Bayesian rule updateto our batch models in order to simulate a real-time streaming prediction scenario as a first stepbeyond the existing models as shown in Figure 2.The model makes predictions of a latent user at-tribute e.g., Republican under a model assumptionof sequentially arriving, independent and identi-cally distributed observations T = (t1, .
.
.
, tk)9.The model dynamically updates posterior proba-bility estimates p(a(vi) = R|tk) for a given user8The separate issue is that many authors simply don?ttweet very often.
For instance, 85.3% of all Twitterusers post less than one update per day as reported athttp://www.sysomos.com/insidetwitter/.
Thus, their commu-nications are scare even if we could get al of them withoutrate limiting from Twitter API.9Given the dynamic character of online discourse it willclearly be of interest in the future to consider models that gobeyond the iid assumption.188p(R|t1)0.6vivivip(R|t1, t2)0.7p(R|t1, .
.
.
tk)0.9NrNrNrTime, ?
?2?1 ?kFigure 2: Stream-based classification of an attribute a(vi) ?
{R,D} given a stream of communications t1, t2, .
.
.
, tkau-thored by a user vior user immediate neighbors from Nrsocial circles at time ?1, ?2, .
.
.
, ?k.vias an additional evidence tkis acquired, as de-fined in a general form below for any latent at-tribute a(vi) ?
A given the tweets T of user vi:p(a(vi) = x ?
A | T ) =p(T | a(vi) = x) ?
p(a(vi) = x)?y?Ap(T | a(vi) = y) ?
p(a(vi) = y)=?kp(tk| a(vi) = x) ?
p(a(vi) = x)?y?A?kp(tk| a(vi) = y) ?
p(a(vi) = y),(5)where y is the number of all possible attribute val-ues, and k is the number of tweets per user.For example, to predict user political prefer-ence, we start with a prior P (R) = 0.5, and se-quentially update the posterior p(R | T ) by accu-mulating evidence from the likelihood p(tk|R):p(R | T ) =?kp(tk|R) ?
p(R)?kP (tk|R) ?
p(R) +?kP (tk|D) ?
p(D).
(6)Our goal is to maximize posterior probabilityestimates given a stream of communications foreach user in the data over (a) time ?
and (b) thenumber of tweets T .
For that, for each user wetake tweets that arrive continuously over time andapply two different streaming models:?
User Model with Dynamic Updates: re-lies exclusively on user tweets t(vi)1, .
.
.
, t(vi)kfollowing the order they arrive over time ?
,where for each user viwe dynamically up-date the posterior p(R | t(vi)1, .
.
.
, t(vi)k).?
User-Neighbor Model with Dynamic Up-dates: relies on both neighbor Nrcommu-nications including friend, follower, retweet,user mention and user tweets t(vi)1, .
.
.
, t(Nr)kfollowing the order they arrive over time ?
;here we dynamically update the posteriorprobability p(R | t(vi)1, .
.
.
, t(Nr)k).5 Experimental SetupWe design a set of experiments to analyze staticand dynamic models for political affiliation classi-fication defined in Sections 3 and 4.5.1 Batch Classification ExperimentsWe first answer whether communications fromuser-local neighborhoods can help predict politi-cal preference for the user.
To explore the con-tribution of different neighborhood types we learnstatic user and neighbor models on Gcand, Ggeoand GZLRgraphs.
We also examine the ability ofour static models to predict user political prefer-ences in low-resource setting e.g., 5 tweets.The existing models follow a standard setupwhen either user or neighbor tweets are availableduring train and test.
For a static neighbor modelwe go beyond that, and train our the model on alldata available per user, but only apply part of thedata at the test time, pushing the boundaries ofhow little is truly required for classification.
Forexample, we only use follower tweets for Gtest,but we use tweets from all types of neighbors forGtrain.
Such setup will simulate different real-world prediction scenarios which have not beenpreviously explored, to our knowledge e.g., whena user has a private profile or has not tweeted yet,and only user neighbor tweets are available.We experiment with our static neighbor modeldefined in Eq.2 with the aim to:1. evaluate neighborhood size influence, wechange the number of neighbors and try n =[1, 2, 5, 10] neighbor(s) per user;2. estimate neighbor content influence, we alter-nate the amount of content per neighbor andtry t = [5, 10, 15, 25, 50, 100, 200] tweets.We perform 10-fold cross validation10and run100 random restarts for every n and t parame-ter combination.
We compare our static neigh-bor and user models using the cost functionsfrom Eq.3 and Eq.4.
For all experiments we useLibLinear (Fan et al, 2008), integrated in theJerboa toolkit (Van Durme, 2012a).
Both mod-els defined in Eq.1 and Eq.2 are learned usingnormalized count-based word ngram features ex-tracted from either user or neighbor tweets.1110For each fold we split the data into 3 parts: 70% train,10% development and 20% test.11For brevity we omit reporting results for bigram and tri-gram features, since unigrams showed superior performance.1895.2 Streaming Classification ExperimentsWe evaluate our models with dynamic Bayesianupdates on a continuous stream of communica-tions over time as shown in Figure 2.
Unlike staticmodel experiments, we are not modeling the in-fluence of the number of neighbors or the amountof content per neighbor.
Here, we order user andneighbor communication streams by real worldtime of posting and measure changes in posteriorprobabilities over time.
The main purpose of theseexperiments is to quantitatively evaluate (1) thenumber of tweets and (2) the amount of real worldtime it takes to observe enough evidence on Twit-ter to make reliable predictions.We experiment with log-linear models definedin Eq.
1 and 2 and continuously estimate the poste-rior probabilities P (R | T ) as defined in Eq.6.
Weaverage the posterior probability results over theusers in Gcand, Ggeoand GZLRgraphs.
We trainstreaming models on an attribute balanced subsetof tweets for each user viexcluding vi?s tweets (orvi?s neighbor tweets for a joint model).
This setupis similar to leave-one-out classification.
The clas-sifier is learned using binary word ngram featuresextracted from user or user-neighbor communi-cations.
We prefer binary to normalized count-based features to overcome sparsity issues causedby making predictions on each tweet individually.6 Static Classification Results6.1 Modeling User Content InfluenceWe investigate classification decision probabilitiesfor our static user model ?viby making predic-tions on a random set of 5 vs. 100 tweets per user.To our knowledge only limited work on personal0 20 40 60 80 1000.00.20.40.60.81.0UserClassificationdecision(probability)misclassifiedmisclassifiedcorrectcorrectFigure 3: Classification probabilities for ?viestimated over100 users in Gcandtested on 5 (blue) vs. 100 (green) tweetsper user where Republican = 1, Democratic = 0, filled mark-ers = correctly classified, not filled = misclassified users.l l l ll l l5 10 20 50 100 2000.500.550.600.650.70log(Tweets Per Neighbor)Accuracy10 50 100 200 400FriendFollowerHashtagUsermentionRetweetReplyUserl l l l ll ll l l ll l ll ll l l lll l l l ll ll l ll l(a) Ggeo: 2 neighborsl l l ll l l5 10 20 50 100 2000.500.550.600.650.70log(Tweets Per Neighbor)Accuracy50 100 250 500 1000 2000FriendFollowerHashtagUsermentionRetweetReplyUserl l l l l l ll l l ll ll ll l l lll l ll l l ll l l ll l l(b) Ggeo: 10 neighborsll ll ll l5 10 20 50 100 2000.500.550.600.650.700.75log(Tweets Per Neighbor)Accuracy10 50 100 200 400FriendFollowerHashtagUsermentionRetweetReplyUserlll ll l l l lll ll ll ll ll ll ll ll ll l l(c) Gcand: 2 neighborsll ll l ll5 10 20 50 100 2000.500.550.600.650.700.75log(Tweets Per Neighbor)Accuracy50 100 250 500 1000 2000FriendFollowerHashtagUsermentionRetweetReplyUserl ll l ll l ll l l lll l lll ll l ll ll ll l l(d) Gcand: 10 neighborsFigure 4: Modeling the influence of the number of tweets perneighbor t=[5, .., 200] for Gcandand Ggeographs.analytics (Burger et al, 2011; Van Durme, 2012b)have performed this straight-forward comparison.For that purpose, we take a random partition con-taining 100 users ofGcandgraph and perform fourindependent classification experiments ?
two runsusing 5 and two runs using 100 tweets per user.Figure 3 demonstrates that more tweets duringprediction time lead to higher accuracy by show-ing that more users with 100 tweets are correctlyclassified e.g., filled green markers in the right up-per quadrant are true Republicans and in the leftlower quadrant are true Democrats.
Moreover, alot of users with 100 tweets are close to 0.5 deci-sion probability which suggests that the classifieris just uncertain rather then being completely off,e.g., misclassified Republican users with 5 tweets(not filled blue markers in the right lower quad-rant) are close to 0.
These results follow natu-rally from the underlying feature representation:having more tweets per user leads to a lower vari-ance estimate of a target multinomial distribution.The more robustly this distribution is estimated(based on having more tweets) the more confidentwe should be in the classifier output.6.2 Modeling Neighbor Content InfluenceHere we discuss the results for our static neighbor-hood model.
We study the influence of the neigh-borhood type r and size in terms of the number ofneighbors n and tweets t per neighbor.190llll1 2 5 100.500.550.600.650.700.75log(Number of Neighbors)Accuracy5 10 25 50FriendFollowerHashtag UsermentionRetweetReplyllll llllllllll lll(a) Gcand: 5 tweetsllll1 2 5 100.500.550.600.650.700.75log(Number of Neighbors)Accuracy200 400 1000 2000FriendFollowerHashtag UsermentionRetweetReplyllllllllllllllllllll(b) Gcand: 200 tweetslll l1 2 5 100.500.550.600.650.70log(Number of Neighbors)Accuracy5 10 25 50FriendFollowerHashtag UsermentionRetweetReplyl ll ll ll lllllll llll(c) Ggeo: 5 tweetslll l1 2 5 100.500.550.600.650.70log(Number of Neighbors)Accuracy200 400 1000 2000FriendFollowerHashtag UsermentionRetweetReplyl ll ll llllllllllll ll l(d) Ggeo: 200 tweetsFigure 5: Modeling the influence of the number of neighborsper user n=[1, .., 10] for Gcandand Ggeographs.In Figure 4 we present accuracy results forGcandand Ggeographs.
Following Eq.3 and 4, wespent an equal amount of resources to obtain 100user tweets and 10 tweets from 10 neighbors.
Weannotate these ?points of equal number of commu-nications?
with a line on top marked with a corre-sponding number of user tweets.We show that three of six social circles ?
friend,retweet and user-mention yield better accuracycompared to the user model for all graphs whent ?
250.
Thus, for effectively classifying a givenuser viit is better to take 200 tweets each from 10neighbors rather than 2,000 tweets from the user.The best accuracy for Gcandis 0.75 for friend,follower, retweet and user-mention neighborhoodswhich is 0.03 higher than the user baseline; forGgeois 0.67 for user-mention and 0.64 for retweetcircles compared to 0.57 for the user model; forGZLRis 0.863 for retweet and 0.849 for friendcircles which is 0.11 higher that the user baseline.Finally, similarly to the results for the user modelgiven in Figure 3, increasing the number of tweetsper neighbor from 5 to 200 leads to a significantgain in performance for all neighborhood types.6.3 Modeling Neighborhood SizeIn Figure 5 we present accuracy results to showneighborhood size influence on classification per-formance for Ggeoand Gcandgraphs.
Our re-sults demonstrate that even small changes to theneighborhood size n lead to better performancewhich does not support the claims by Zamal et al(2012).
We demonstrate that increasing the sizeof the neighborhood leads to better performanceacross six neighborhood types.
Friend, user men-tion and retweet neighborhoods yield the highestaccuracy for all graphs.
We observe that when thenumber of neighbors is n = 1, the difference inaccuracy across all neighborhood types is less sig-nificant but for n ?
2 it becomes more significant.7 Streaming Classification Results7.1 Modeling Dynamic Posterior Updatesfrom a User StreamFigures 6a and 6b demonstrate dynamic usermodel prediction results averaged over users fromGcandand GZLRgraphs.
Each figure outlineschanges in sequential average probability esti-mates p?
(R | T ) for each individual self-authoredtweet tkas defined in Eq.
6.
The average proba-bility estimates p?
(R | T ) are reported for every 5tweets in a stream T = (t1, .
.
.
tk) as?nP (R|tk)n,where n is the total number of users with the sameattribute R or D. We represent p?
(R | T ) as abox and whisker plot with the median, lower andupper quantiles to show the variance; the length ofwhiskers indicate lower and upper extreme values.We find similar behavior across all three graphs.In particular, the posterior estimates convergefaster when predicting Democratic than Republi-can users but it has been trained on an equal num-ber of tweets per class.
We observe that averageposterior estimates P?
(R | T ) converge faster to 00.50.60.70.80.91.00 20 40 60p(Republican|T)0.00.10.20.30.40.50 20 40 60Tweet Stream (T)p(Republican|T)(a) User Gcand0.50.60.70.80.91.00 50 100 150p(Republican|T)0.00.10.20.30.40.50 50 100 150Tweet Stream (T)p(Republican|T)(b) User GZLRFigure 6: Streaming classification results from user commu-nications for Gcandand GZLRgraphs averaged over every 5tweets (red - Republican, blue - Democratic).1913004005000 5 10 15Time in WeeksUsers(a) User Gcand01002000 20 40 60 80Time in WeeksUsers(b) User GZLR3004005000 1 2 3 4 5Time in WeeksUsers(c) User-Neigh Gcand01002000 10 20 30 40Time in WeeksUsers(d) User-Neigh GZLRFigure 7: Time needed for (a) - (b) dynamic user model and(c) - (d) joint user-neighbor model to infer political prefer-ences of Democratic (blue) and Republican (red) users at75% (dotted line) and 95% (solid line) accuracy levels.
(Democratic) than to 1 (Republican) in Figures 6aand 6b.
It suggests that language of Democrats ismore expressive of their political preference thanlanguage of Republicans.
For example, frequentpolitically influenced terms used widely by Demo-cratic users include faith4liberty, constitutionally,pass, vote2012, terroristic.The variance for average posterior estimatesdecreases when the number of tweets increasesfor all three datasets.
Moreover, we detect thatP?
(R|T ) estimates for users in Gcandconverge 2-3 times faster in terms of number of tweets thanfor users in GZLR.
The lowest convergence is de-tected for Ggeowhere after tk= 250 tweets theaverage posterior estimate P?
(R | tk) = 0.904 ?0.044 and P?
(D | tk) = 0.861 ?
0.008.
It meansthat users inGcandare more politically vocal com-pared to users in GZLRand Ggeo.
As a result,less active users in Ggeojust need more than 250tweets to converge to a true 0 or 1 class.
These re-sults are coherent with the outcomes for our staticmodels shown in Figures 4 and 5.
These findingsfurther confirm that differences in performance arecaused by various biases present in the data due todistinct sampling and annotation approaches.Figure 7a and 7b illustrate the amount of timerequired for the user model to infer political pref-erences estimated for 1,031 users inGcandand 371users inGZLR.
The amount of time needed can beevaluated for different accuracy levels e.g., 0.75and 0.95.
Thus, with 75% accuracy we classify:?
100 (?20%) Republican users in 3.6 hoursand Democratic users in 2.2 hours for Gcand;?
100 (?56%) R users in 20 weeks and 100(?52%) D users in 8.9 weeks for GZLRwhich is 800 times longer that for Gcand;?
100 (?75%) R users in 12 weeks and 80(?60%) D users in 19 weeks for Ggeo.Such extreme divergences in the amount of timerequired for classification across all graphs shouldbe of strong interest to researchers concerned withlatent attribute prediction tasks because Twitterusers produce messages with extremely differentfrequencies.
In our case, users in GZLRtweet ap-proximately 800 times less frequently than usersin Gcand.7.2 Modeling Dynamic Posterior Updatesfrom a Joint User-Neighbor StreamWe estimate dynamic posterior updates from ajoint stream of user and neighbor communicationsin Ggeo, Gcandand GZLRgraphs.
To make a faircomparison with a streaming user model, we startwith the same user tweet t0(vi).
Then instead ofwaiting for the next user tweet we rely on anyneighbor tweets that appear until the user producesthe next tweet t1(vi).
We rely on communicationsfrom four types of neighbors such as friends, fol-lowers, retweets and user mentions.The convergence rate for the average posteriorprobability estimates P?
(R|T ) depending on thenumber of tweets is similar to the user model re-sults presented in Figure 6.
However, for Ggeothe variance for P?
(R|T ) is higher for Democraticusers; for GZLRP?
(R|T ) ?
1 for Republicansin less than 110 tweets which is ?t = 40 tweetsfaster than the user model; for Gcandthe conver-gence for both P?
(R|T ) ?
1 and P?
(D|T ) ?
0is not significantly different than the user model.Figures 7c and 7d show the amount of time re-quired for a joint user-neighbor model to infer po-litical preferences estimated for users inGcandandGZLR.
We find that with 75% accuracy we canclassify 100 users for:?
Gcand: Republican users in 23 minutes andDemocratic users in 10 minutes;?
GZLR: R users in 3.2 weeks and D users in1.1 weeks which is 7 times faster on averageacross attributes than for the user model;?
Ggeo: R users in 1.2 weeks and D users in3.5 weeks which is on average 6 times fasteracross attributes than for the user model.Similar or better P?
(R|T ) convergence in termsof the number of tweets and, especially, in theamount of time needed for user and user-neighbor192models further confirms that neighborhood con-tent is useful for political preference prediction.Moreover, communications from a joint stream al-low to make an inference up to 7 times faster.8 Related WorkSupervised Batch Approaches The vast major-ity of work on predicting latent user attributes insocial media apply supervised static SVM mod-els for discrete categorical e.g., gender and re-gression models for continuous attributes e.g., agewith lexical bag-of-word features for classifyinguser gender (Garera and Yarowsky, 2009; Rao etal., 2010; Burger et al, 2011; Van Durme, 2012b),age (Rao et al, 2010; Nguyen et al, 2011; Nguyenet al, 2013) or political orientation.
We present anoverview of the existing models for political pref-erence prediction in Table 1.Bergsma et al (2012) following up on Rao?swork (2010) on adding socio-linguistic featuresto improve gender, ethnicity and political prefer-ence prediction show that incorporating stylisticand syntactic information to the bag-of-word fea-tures improves gender classification.Other methods characterize Twitter users by ap-plying limited amounts of network structure in-formation in addition to lexical features.
Con-Approach Users Tweets Features Accur.Rao et al(2010)1K 2Mngramssocio-lingstacked0.8240.6340.809Pennacchiottiand Popescu(2011a)10.3K ?ling-allsoc-allfull0.7700.8630.889Conover etal.
(2011)1,000 1Mfull-texthashtagsclusters0.7920.9080.949Zamal et al(2012)400400K3.85M4.25MUserOnlyNbrUser-Nbr110.8900.9200.932Cohen andRuths(2013)3971.8K262196397K1.8M262K196Kfeaturesfrom (Za-mal et al,2012)0.9100.8400.6800.870This paper(batch clas-sification)Gcand1,031Ggeo270GZLR371206K2M54K540K371K1.5Muser ngramsneighboruser ngramsneighboruser ngramsneighbor0.7200.7500.5700.6700.8860.920This paper(dynamicBayesianupdate clas-sification)Gcand1,031Ggeo270GZLR371103K130K54K67K74K185Kuser streamuser-neigh.user streamuser-neigh.user streamuser-neigh.0.9950.9990.8430.8820.8920.999Table 1: Overview of the existing approaches for politicalpreference classification in Twitter.nover et al (2011) rely on identifying strong parti-san clusters of Democratic and Republican usersin a Twitter network based on retweet and usermention degree of connectivity, and then combinethis clustering information with the follower andfriend neighborhood size features.
Pennacchiottiet al (2011a; 2011b) focus on user behavior, net-work structure and linguistic features.
Similar toour work, they assume that users from a partic-ular class tend to reply and retweet messages ofthe users from the same class.
We extend this as-sumption and study other relationship types e.g.,friends, user mentions etc.
Recent work by Wonget al (2013) investigates tweeting and retweet-ing behavior for political learning during 2012 USPresidential election.
The most similar work toours is by Zamal et al (2012), where the authorsapply features from the tweets authored by a user?sfriend to infer attributes of that user.
In this paper,we study different types of user social circles inaddition to a friend network.Additionally, using social media for mining po-litical opinions (O?Connor et al, 2010a; May-nard and Funk, 2012) or understanding socio-political trends and voting outcomes (Tumasjanet al, 2010; Gayo-Avello, 2012; Lampos et al,2013) is becoming a common practice.
For in-stance, Lampos et al (2013) propose a bilinearuser-centric model for predicting voting intentionsin the UK and Australia from social media data.Other works explore political blogs to predict whatcontent will get the most comments (Yano et al,2013) or analyze communications from CapitolHill12to predict campaign contributors based onthis content (Yano and Smith, 2013).Unsupervised Batch Approaches Bergsma etal.
(2013) show that large-scale clustering of usernames improves gender, ethnicity and locationclassification on Twitter.
O?Connor et al (2010b)following the work by Eisenstein (2010) proposea Bayesian generative model to discover demo-graphic language variations in Twitter.
Rao etal.
(2011) suggest a hierarchical Bayesian modelwhich takes advantage of user name morphologyfor predicting user gender and ethnicity.
Golbecket al (2010) incorporate Twitter data in a spatialmodel of political ideology.Streaming Approaches Van Durme (2012b)proposed streaming models to predict user gen-der in Twitter.
Other works suggested to process12http://www.tweetcongress.org193text streams for a variety of NLP tasks e.g., real-time opinion mining and sentiment analysis in so-cial media (Pang and Lee, 2008), named entitydisambiguation (Sarmento et al, 2009), statisticalmachine translation (Levenberg et al, 2011), firststory detection (Petrovi?c et al, 2010), and unsu-pervised dependency parsing (Goyal and Daum?e,2011).
Massive Online Analysis (MOA) toolkitdeveloped by Bifet et al (2010) is an alternative tothe Jerboa package used in this work developedby Van Durme (2012a).
MOA has been effec-tively used to detect sentiment changes in Twitterstreams (Bifet et al, 2011).9 Conclusions and Future WorkIn this paper, we extensively examined state-of-the-art static approaches and proposed novel mod-els with dynamic Bayesian updates for streamingpersonal analytics on Twitter.
Because our stream-ing models rely on communications from Twitterusers and content from various notions of user-local neighborhood they can be effectively appliedto real-time dynamic data streams.
Our resultssupport several key findings listed below.Neighborhood content is useful for personalanalytics.
Content extracted from various notionsof a user-local neighborhood can be as effectiveor more effective for political preference classifi-cation than user self-authored content.
This maybe an effect of ?sparseness?
of relevant user data,in that users talk about politics very sporadicallycompared to a random sample of their neighbors.Substantial signal for political preferenceprediction is distributed in the neighborhood.Querying for more neighbors per user is more ben-eficial than querying for extra content from theexisting neighbors e.g., 5 tweets from 10 neigh-bors leads to higher accuracy than 25 tweets from2 neighbors or 50 tweets from 1 neighbor.
Thismay be also the effect of data heterogeneity insocial media compared to e.g., political debatetext (Thomas et al, 2006).
These findings demon-strate that a substantial signal is distributed overthe neighborhood content.Neighborhoods constructed from friend,user mention and retweet relationships aremost effective.
Friend, user mention and retweetneighborhoods show the best accuracy for predict-ing political preferences of Twitter users.
We thinkthat friend relationships are more effective thane.g., follower relationships because it is very likelythat users share common interests and preferenceswith their friends, e.g.
Facebook friends can evenbe used to predict a user?s credit score.13Usermentions and retweets are two primary ways of in-teraction on Twitter.
They both allow to share in-formation e.g., political news, events with othersand to be involved in direct communication e.g.,live political discussions, political groups.Streaming models are more effective thanbatch models for personal analytics.
The predic-tions made using dynamic models with Bayesianupdates over user and joint user-neighbor commu-nication streams demonstrate higher performancewith lower resources spent compared to the batchmodels.
Depending on user political involvement,expressiveness and activeness, the perfect predic-tion (approaching 100% accuracy) can be madeusing only 100 - 500 tweets per user.Generalization of the classifiers for politicalpreference prediction.
This work raises a veryimportant but under-explored problem of the gen-eralization of classifiers for personal analytics insocial media, also recently discussed by Cohenand Ruth (2013).
For instance, the existing modelsdeveloped for political preference prediction areall trained on Twitter data but report significantlydifferent results even for the same baseline mod-els trained using bag-of-word lexical features asshown in Table 1.
In this work we experiment withthree different datasets.
Our results for both staticand dynamic models show that the accuracy in-deed depends on the way the data was constructed.Therefore, publicly available datasets need to bereleased for a meaningful comparison of the ap-proaches for personal analytics in social media.In future work, we plan to incorporate itera-tive model updates from newly classified com-munications similar to online perceptron-style up-dates.
In addition, we aim to experiment withneighborhood-specific classifiers applied towardsthe tweets from neighborhood-specific streamse.g., friend classifier used for friend tweets,retweet classifier applied to retweet tweets etc.AcknowledgmentsThe authors would like to thank the anonymousreviewers for their helpful comments.13http://money.cnn.com/2013/08/26/technology/social/facebook-credit-score/194ReferencesShane Bergsma, Matt Post, and David Yarowsky.
2012.Stylometric analysis of scientific articles.
In Pro-ceedings of the Conference of the North AmericanChapter of the Association for Computational Lin-guistics: Human Language Technologies (NAACL-HLT), pages 327?337.Shane Bergsma, Mark Dredze, Benjamin Van Durme,Theresa Wilson, and David Yarowsky.
2013.Broadly improving user classification viacommunication-based name and location clus-tering on Twitter.
In Proceedings of the Conferenceof the North American Chapter of the Associationfor Computational Linguistics: Human LanguageTechnologies (NAACL-HLT), pages 1010?1019.Albert Bifet, Geoff Holmes, Bernhard Pfahringer,Philipp Kranen, Hardy Kremer, Timm Jansen, andThomas Seidl.
2010.
MOA: Massive online analy-sis, a framework for stream classification and clus-tering.
Journal of Machine Learning Research,11:44?50.Albert Bifet, Geoffrey Holmes, Bernhard Pfahringer,and Ricard Gavald`a.
2011.
Detecting sentimentchange in Twitter streaming data.
Journal of Ma-chine Learning Research, 17:5?11.John D. Burger, John Henderson, George Kim, andGuido Zarrella.
2011.
Discriminating gender onTwitter.
In Proceedings of the Conference on Em-pirical Methods in Natural Language Processing(EMNLP), pages 1301?1309.Raviv Cohen and Derek Ruths.
2013.
Classifying Po-litical Orientation on Twitter: It?s Not Easy!
In Pro-ceedings of the International AAAI Conference onWeblogs and Social Media (ICWSM), pages 91?99.Michael D. Conover, Bruno Gonc?alves, JacobRatkiewicz, Alessandro Flammini, and FilippoMenczer.
2011.
Predicting the political alignmentof Twitter users.
In Proceedings of Social Comput-ing, pages 192?199.Jacob Eisenstein, Brendan O?Connor, Noah A. Smith,and Eric P. Xing.
2010.
A latent variable model forgeographic lexical variation.
In Proceedings of theConference on Empirical Methods in Natural Lan-guage Processing (EMNLP), pages 1277?1287.Rong En Fan, Kai Wei Chang, Cho Jui Hsieh, Xi-ang Rui Wang, and Chih Jen Lin.
2008.
LIBLIN-EAR: A library for large linear classification.
Jour-nal of Machine Learning Research, 9:1871?1874.Katja Filippova.
2012.
User demographics and lan-guage in an implicit social network.
In Proceed-ings of the 2012 Joint Conference on EmpiricalMethods in Natural Language Processing and Com-putational Natural Language Learning (EMNLP-CoNLL), pages 1478?1488.Nikesh Garera and David Yarowsky.
2009.
Modelinglatent biographic attributes in conversational genres.In Proceedings of the Joint Conference of the 47thAnnual Meeting of the ACL and the 4th InternationalJoint Conference on Natural Language Processingof the AFNLP, pages 710?718.Daniel Gayo-Avello.
2012.
No, you cannot predictelections with Twitter.
Internet Computing, IEEE,16(6):91?94.Jennifer Golbeck, Justin M. Grimes, and AnthonyRogers.
2010.
Twitter use by the u.s. congress.Journal of the American Society for Information Sci-ence and Technology, 61(8):1612?1621.Amit Goyal and Hal Daum?e, III.
2011.
Approxi-mate scalable bounded space sketch for large dataNLP.
In Proceedings of the Conference on Em-pirical Methods in Natural Language Processing(EMNLP), pages 250?261.Vasileios Lampos, Daniel Preotiuc-Pietro, and TrevorCohn.
2013.
A user-centric model of voting inten-tion from social media.
In Proceedings of the Asso-ciation for Computational Linguistics (ACL), pages993?1003.Abby Levenberg, Miles Osborne, and David Matthews.2011.
Multiple-stream language models for statis-tical machine translation.
In Proceedings of theSixth Workshop on Statistical Machine Translation(WMT), pages 177?186.Yang Liu, Qun Liu, and Shouxun Lin.
2005.
Log-linear models for word alignment.
In Proceedingsof the Annual Meeting on Association for Computa-tional Linguistics (ACL), pages 459?466.Ingrid Lunden.
2012.
Analyst: Twitterpassed 500M users in june 2012, 140m ofthem in US; Jakarta ?biggest tweeting?
city.http://techcrunch.com/2012/07/30/analyst-twitter-passed-500m-users-in-june-2012-140m-of-them-in-us-jakarta-biggest-tweeting-city/.Diana Maynard and Adam Funk.
2012.
Automatic de-tection of political opinions in tweets.
In Proceed-ings of the 8th International Conference on The Se-mantic Web (ESWC), pages 88?99.Felix Ming Fai Wong, Chee Wei Tan, Soumya Sen, andMung Chiang.
2013.
Quantifying political leaningfrom tweets and retweets.
In Proceedings of the In-ternational AAAI Conference on Weblogs and SocialMedia (ICWSM).Dong Nguyen, Noah A. Smith, and Carolyn P. Ros?e.2011.
Author age prediction from text using lin-ear regression.
In Proceedings of the 5th ACL-HLT Workshop on Language Technology for Cul-tural Heritage, Social Sciences, and Humanities(LaTeCH), pages 115?123.195Dong Nguyen, Rilana Gravel, Dolf Trieschnigg, andTheo Meder.
2013.
?How old do you think I am?
?A study of language and age in Twitter.
In Proceed-ings of the AAAI Conference on Weblogs and SocialMedia (ICWSM), pages 439?448.Brendan O?Connor, Ramnath Balasubramanyan,Bryan R. Routledge, and Noah A. Smith.
2010a.From tweets to polls: Linking text sentiment topublic opinion time series.
In Proceedings of theInternational AAAI Conference on Weblogs andSocial Media (ICWSM), pages 122?129.Brendan O?Connor, Jacob Eisenstein, Eric P. Xing, andNoah A. Smith.
2010b.
A mixture model of de-mographic lexical variation.
In Proceedings of theNIPS Workshop on Machine Learning and SocialComputing.Bo Pang and Lillian Lee.
2008.
Opinion mining andsentiment analysis.
Foundations of Trends in Infor-mation Retrieval, 2(1-2):1?135, January.Marco Pennacchiotti and Ana-Maria Popescu.
2011a.Democrats, republicans and starbucks afficionados:user classification in twitter.
In Proceedings of the17th ACM SIGKDD Conference on Knowledge Dis-covery and Data Mining (KDD), pages 430?438.Marco Pennacchiotti and Ana Maria Popescu.
2011b.A machine learning approach to Twitter user clas-sification.
In Proceedings of the International AAAIConference on Weblogs and Social Media (ICWSM),pages 281?288.Sa?sa Petrovi?c, Miles Osborne, and Victor Lavrenko.2010.
Streaming first story detection with appli-cation to Twitter.
In Proceedings of Human Lan-guage Technologies: The 2010 Annual Conferenceof the North American Chapter of the Associationfor Computational Linguistics (NAACL).Hoifung Poon, Colin Cherry, and Kristina Toutanova.2009.
Unsupervised morphological segmentationwith log-linear models.
In Proceedings of HumanLanguage Technologies: The Annual Conference ofthe North American Chapter of the Association forComputational Linguistics, pages 209?217.Delip Rao, David Yarowsky, Abhishek Shreevats, andManaswi Gupta.
2010.
Classifying latent user at-tributes in Twitter.
In Proceedings of the 2nd In-ternational Workshop on Search and Mining User-generated Contents (SMUC), pages 37?44.Delip Rao, Michael Paul, Clay Fink, David Yarowsky,Timothy Oates, and Glen Coppersmith.
2011.
Hier-archical Bayesian models for latent attribute detec-tion in social media.
In Proceedings of the Inter-national AAAI Conference on Weblogs and SocialMedia (ICWSM).Lu?
?s Sarmento, Alexander Kehlenbeck, Eug?enioOliveira, and Lyle Ungar.
2009.
An approach toweb-scale named-entity disambiguation.
In Pro-ceedings of the 6th International Conference on Ma-chine Learning and Data Mining in Pattern Recog-nition (MLDM), pages 689?703.Noah A. Smith.
2004.
Log-linear models.Craig Smith.
2013.
May 2013 by thenumbers: 16 amazing Twitter stats.http://expandedramblings.com/index.php/march-2013-by-the-numbers-a-few-amazing-twitter-stats/.Matt Thomas, Bo Pang, and Lillian Lee.
2006.
Getout the vote: determining support or opposition fromcongressional floor-debate transcripts.
In Proceed-ings of the Conference on Empirical Methods in Nat-ural Language Processing, pages 327?335.A.
Tumasjan, T. O. Sprenger, P. G. Sandner, and I. M.Welpe.
2010.
Predicting elections with Twitter:What 140 characters reveal about political senti-ment.
In Proceedings of the International AAAIConference on Weblogs and Social Media, pages178?185.Benjamin Van Durme.
2012a.
Jerboa: A toolkit forrandomized and streaming algorithms.
Technical re-port, Human Language Technology Center of Excel-lence.Benjamin Van Durme.
2012b.
Streaming analysis ofdiscourse participants.
In Proceedings of the Con-ference on Empirical Methods in Natural LanguageProcessing (EMNLP), pages 48?58.Yi Yang and Jacob Eisenstein.
2013.
A log-linearmodel for unsupervised text normalization.
In Pro-ceedings of Conference on Empirical Methods inNatural Language Processing, pages 61?72.Tao Yano and Noah A. Smith.
2013.
What?s worthyof comment?
content and comment volume in po-litical blogs.
In International AAAI Conference onWeblogs and Social Media (ICWSM).Tao Yano, Dani Yogatama, and Noah A. Smith.
2013.A penny for your tweets: Campaign contributionsand capitol hill microblogs.
In Proceedings of theInternational AAAI Conference on Weblogs and So-cial Media (ICWSM).Faiyaz Al Zamal, Wendy Liu, and Derek Ruths.
2012.Homophily and latent attribute inference: Inferringlatent attributes of Twitter users from neighbors.
InProceedings of the International AAAI Conferenceon Weblogs and Social Media, pages 387?390.196
