Proceedings of the Human Language Technology Conference of the North American Chapter of the ACL, pages 97?100,New York, June 2006. c?2006 Association for Computational LinguisticsSyntactic Kernels for Natural Language Learning:the Semantic Role Labeling CaseAlessandro MoschittiDepartment of Computer ScienceUniversity of Rome ?Tor Vergata?Rome, Italymoschitti@info.uniroma2.itAbstractIn this paper, we use tree kernels to exploitdeep syntactic parsing information for nat-ural language applications.
We study theproperties of different kernels and we pro-vide algorithms for their computation inlinear average time.
The experiments withSVMs on the task of predicate argumentclassification provide empirical data thatvalidates our methods.1 IntroductionRecently, several tree kernels have been applied tonatural language learning, e.g.
(Collins and Duffy,2002; Zelenko et al, 2003; Cumby and Roth, 2003;Culotta and Sorensen, 2004; Moschitti, 2004).
De-spite their promising results, three general objec-tions against kernel methods are raised: (1) only asubset of the dual space features are relevant, thus,it may be possible to design features in the primalspace that produce the same accuracy with a fastercomputation time; (2) in some cases the high num-ber of features (substructures) of the dual space canproduce overfitting with a consequent accuracy de-crease (Cumby and Roth, 2003); and (3) the compu-tation time of kernel functions may be too high andprevent their application in real scenarios.In this paper, we study the impact of the sub-tree (ST) (Vishwanathan and Smola, 2002), subsettree (SST) (Collins and Duffy, 2002) and partial tree(PT) kernels on Semantic Role Labeling (SRL).
ThePT kernel is a new function that we have designedto generate larger substructure spaces.
Moreover,to solve the computation problems, we propose al-gorithms which evaluate the above kernels in linearaverage running time.We experimented such kernels with Support Vec-tor Machines (SVMs) on the classification of seman-tic roles of PropBank (Kingsbury and Palmer, 2002)and FrameNet (Fillmore, 1982) data sets.
The re-sults show that: (1) the kernel approach provides thesame accuracy of the manually designed features.
(2) The overfitting problem does not occur althoughthe richer space of PTs does not provide better ac-curacy than the one based on SST.
(3) The averagerunning time of our tree kernel computation is linear.In the remainder of this paper, Section 2 intro-duces the different tree kernel spaces.
Section 3 de-scribes the kernel functions and our fast algorithmsfor their evaluation.
Section 4 shows the compara-tive performance in terms of execution time and ac-curacy.2 Tree kernel SpacesWe consider three different tree kernel spaces: thesubtrees (STs), the subset trees (SSTs) and the novelpartial trees (PTs).An ST of a tree is rooted in any node and includesall its descendants.
For example, Figure 1 shows theparse tree of the sentence "Mary brought a cat"together with its 6 STs.
An SST is a more generalstructure since its leaves can be associated with non-terminal symbols.
The SSTs satisfy the constraintthat grammatical rules cannot be broken.
For exam-ple, Figure 2 shows 10 SSTs out of 17 of the sub-tree of Figure 1 rooted in VP.
If we relax the non-breaking rule constraint we obtain a more generalform of substructures, i.e.
the PTs.
For example,97Figure 3 shows 10 out of the total 30 PTs, derivedfrom the same tree as before.SNNPD NVPV MarybroughtacatNPD NacatNcatDaVbroughtNMaryNPD NVPVbroughtacatFigure 1: A syntactic parse tree with its subtrees (STs).NPD NacatNPD NNPD NaNPD NNPD NVPVbroughtacatcat NP D NVPVacatNPD NVPVNcatDaVbroughtNMary ?Figure 2: A tree with some of its subset trees (SSTs).NPD NVPVbroughtacatNPD NVPVacatNPD NVPacatNPD NVPaNPDVPaNPDVPNPNVPNPNNP NPD N DNP?VPFigure 3: A tree with some of its partial trees (PTs).3 Fast Tree Kernel FunctionsThe main idea of tree kernels is to compute thenumber of common substructures between two treesT1 and T2 without explicitly considering the wholefragment space.
We designed a general functionto compute the ST, SST and PT kernels.
Our fastalgorithm is inspired by the efficient evaluation ofnon-continuous subsequences (described in (Shawe-Taylor and Cristianini, 2004)).
To further increasethe computation speed, we also applied the pre-selection of node pairs which have non-null kernel.3.1 Generalized Tree Kernel functionGiven a tree fragment space F = {f1, f2, .., fF}, weuse the indicator function Ii(n) which is equal to 1 ifthe target fi is rooted at node n and 0 otherwise.
Wedefine the general kernel as:K(T1, T2) =?n1?NT1?n2?NT2?
(n1, n2), (1)where NT1 and NT2 are the sets of nodes in T1 andT2, respectively and ?
(n1, n2) =?|F|i=1 Ii(n1)Ii(n2),i.e.
the number of common fragments rooted at then1 and n2 nodes.
We can compute it as follows:- if the node labels of n1 and n2 are different then?
(n1, n2) = 0;- else:?
(n1, n2) = 1 +?~J1, ~J2,l(~J1)=l(~J2)l(~J1)?i=1?
(cn1 [ ~J1i], cn2 [ ~J2i])(2)where ~J1 = ?J11, J12, J13, ..?
and ~J2 = ?J21, J22, J23, ..?are index sequences associated with the orderedchild sequences cn1 of n1 and cn2 of n2, respectively,~J1i and ~J2i point to the i-th children in the two se-quences, and l(?)
returns the sequence length.
Wenote that (1) Eq.
2 is a convolution kernel accord-ing to the definition and the proof given in (Haus-sler, 1999).
(2) Such kernel generates a featurespace richer than those defined in (Vishwanathanand Smola, 2002; Collins and Duffy, 2002; Zelenkoet al, 2003; Culotta and Sorensen, 2004; Shawe-Taylor and Cristianini, 2004).
Additionally, we addthe decay factor as follows: ?
(n1, n2) =?
(?2+?~J1, ~J2,l(~J1)=l(~J2)?d(~J1)+d(~J2)l(~J1)?i=1?
(cn1 [ ~J1i], cn2 [ ~J2i]))(3)where d( ~J1) = ~J1l(~J1) ?
~J11 and d( ~J2) = ~J2l(~J2) ?
~J21.In this way, we penalize subtrees built on childsubsequences that contain gaps.
Moreover, tohave a similarity score between 0 and 1, we alsoapply the normalization in the kernel space, i.e.K?
(T1, T2) = K(T1,T2)?K(T1,T1)?K(T2,T2) .
As the summationin Eq.
3 can be distributed with respect to differenttypes of sequences, e.g.
those composed by pchildren, it follows that?
(n1, n2) = ?
(?2 +?lmp=1 ?p(n1, n2)), (4)where ?p evaluates the number of common subtreesrooted in subsequences of exactly p children (of n1and n2) and lm = min{l(cn1), l(cn2)}.
Note also that ifwe consider only the contribution of the longest se-quence of node pairs that have the same children, weimplement the SST kernel.
For the STs computationwe need also to remove the ?2 term from Eq.
4.Given the two child sequences c1a = cn1 andc2b = cn2 (a and b are the last children), ?p(c1a, c2b) =?
(a, b)?|c1|?i=1|c2|?r=1?|c1|?i+|c2|?r ?
?p?1(c1[1 : i], c2[1 : r]),where c1[1 : i] and c2[1 : r] are the child subse-quences from 1 to i and from 1 to r of c1 and c2.
Ifwe name the double summation term as Dp, we canrewrite the relation as:98?p(c1a, c2b) ={?
(a, b)Dp(|c1|, |c2|) if a = b;0 otherwise.Note that Dp satisfies the recursive relation:Dp(k, l) = ?p?1(s[1 : k], t[1 : l]) + ?Dp(k, l ?
1)+?Dp(k ?
1, l) + ?2Dp(k ?
1, l ?
1).By means of the above relation, we can computethe child subsequences of two sets c1 and c2 inO(p|c1||c2|).
This means that the worst case com-plexity of the PT kernel is O(p?2|NT1 ||NT2 |), where?
is the maximum branching factor of the two trees.Note that the average ?
in natural language parsetrees is very small and the overall complexity can bereduced by avoiding the computation of node pairswith different labels.
The next section shows our fastalgorithm to find non-null node pairs.3.2 Fast non-null node pair computationTo compute the kernels defined in the previous sec-tion, we sum the ?
function for each pair ?n1, n2?
?NT1 ?
NT2 (Eq.
1).
When the labels associatedwith n1 and n2 are different, we can avoid evaluating?
(n1, n2) since it is 0.
Thus, we look for a node pairset Np ={?n1, n2??
NT1 ?NT2 : label(n1) = label(n2)}.To efficiently build Np, we (i) extract the L1 andL2 lists of nodes from T1 and T2, (ii) sort them inalphanumeric order and (iii) scan them to find Np.Step (iii) may require only O(|NT1 |+ |NT2 |) time, but,if label(n1) appears r1 times in T1 and label(n2) is re-peated r2 times in T2, we need to consider r1 ?
r2pairs.
The formal can be found in (Moschitti, 2006).4 The ExperimentsIn these experiments, we study tree kernel perfor-mance in terms of average running time and accu-racy on the classification of predicate arguments.
Asshown in (Moschitti, 2004), we can label seman-tic roles by classifying the smallest subtree that in-cludes the predicate with one of its arguments, i.e.the so called PAF structure.The experiments were carried out withthe SVM-light-TK software available athttp://ai-nlp.info.uniroma2.it/moschitti/which encodes the fast tree kernels in the SVM-lightsoftware (Joachims, 1999).
The multiclassifierswere obtained by training an SVM for each classin the ONE-vs.-ALL fashion.
In the testing phase,we selected the class associated with the maximumSVM score.For the ST, SST and PT kernels, we found that thebest ?
values (see Section 3) on the development setwere 1, 0.4 and 0.8, respectively, whereas the best ?was 0.4.4.1 Kernel running time experimentsTo study the FTK running time, we extracted fromthe Penn Treebank several samples of 500 trees con-taining exactly n nodes.
Each point of Figure 4shows the average computation time1 of the kernelfunction applied to the 250,000 pairs of trees of sizen.
It clearly appears that the FTK-SST and FTK-PT(i.e.
FTK applied to the SST and PT kernels) av-erage running time has linear behavior whereas, asexpected, the na?
?ve SST algorithm shows a quadraticcurve.0204060801001205 10 15 20 25 30 35 40 45 50 55Number of Tree Nodes??
?
?secondsFTK-SSTnaive-SSTFTK-PTFigure 4: Average time in ?seconds for the na?
?ve SST kernel,FTK-SST and FTK-PT evaluations.4.2 Experiments on SRL datasetWe used two different corpora: PropBank(www.cis.upenn.edu/?ace) along with PennTreebank 2 (Marcus et al, 1993) and FrameNet.PropBank contains about 53,700 sentences anda fixed split between training and testing used inother researches.
In this split, sections from 02 to21 are used for training, section 23 for testing andsection 22 as development set.
We considered atotal of 122,774 and 7,359 arguments (from Arg0to Arg5, ArgA and ArgM) in training and testing,respectively.
The tree structures were extractedfrom the Penn Treebank.From the FrameNet corpus (www.icsi.berkeley.edu/?framenet) we extracted all1We run the experiments on a Pentium 4, 2GHz, with 1 Gbram.990.750.780.800.830.850.880 10 20 30 40 50 60 70 80 90 100% Training DataAccuracyST SSTLinear PTFigure 5: Multiclassifier accuracy according to different train-ing set percentage.24,558 sentences of the 40 Frames selected forthe Automatic Labeling of Semantic Roles task ofSenseval 3 (www.senseval.org).
We consideredthe 18 most frequent roles, for a total of 37,948examples (30% of the sentences for testing and70% for training/validation).
The sentences wereprocessed with the Collins?
parser (Collins, 1997)to generate automatic parse trees.We run ST, SST and PT kernels along withthe linear kernel of standard features (Carreras andMa`rquez, 2005) on PropBank training sets of dif-ferent size.
Figure 5 illustrates the learning curvesassociated with the above kernels for the SVM mul-ticlassifiers.The tables 1 and 2 report the results, using allavailable training data, on PropBank and FrameNettest sets, respectively.
We note that: (1) the accu-racy of PTs is almost equal to the one produced bySSTs as the PT space is a hyperset of SSTs.
Thesmall difference is due to the poor relevance of thesubstructures in the PT ?
SST set, which degradethe PT space.
(2) The high F1 measures of tree ker-nels on FrameNet suggest that they are robust withrespect to automatic parse trees.Moreover, the learning time of SVMs using FTKfor the classification of one large argument (Arg 0)is much lower than the one required by na?
?ve algo-rithm.
With all the training data FTK terminated in6 hours whereas the na?
?ve approach required morethan 1 week.
However, the complexity burden ofworking in the dual space can be alleviated with re-cent approaches proposed in (Kudo and Matsumoto,2003; Suzuki et al, 2004).Finally, we carried out some experiments with thecombination between linear and tree kernels and wefound that tree kernels improve the models based onmanually designed features by 2/3 percent points,thus they can be seen as a useful tactic to boost sys-tem accuracy.Args Linear ST SST PTAcc.
87.6 84.6 87.7 86.7Table 1: Evaluation of kernels on PropBank data and goldparse trees.Roles Linear ST SST PTAcc.
82.3 80.0 81.2 79.9Table 2: Evaluation of kernels on FrameNet data encoded inautomatic parse trees.ReferencesXavier Carreras and Llu?
?s Ma`rquez.
2005.
Introduction to theCoNLL-2005 shared task: Semantic role labeling.
In Pro-ceedings of CoNLL05.Michael Collins and Nigel Duffy.
2002.
New ranking algo-rithms for parsing and tagging: Kernels over discrete struc-tures, and the voted perceptron.
In ACL02.Michael Collins.
1997.
Three generative, lexicalized modelsfor statistical parsing.
In Proceedings of the ACL97.Aron Culotta and Jeffrey Sorensen.
2004.
Dependency treekernels for relation extraction.
In Proceedings of ACL04.Chad Cumby and Dan Roth.
2003.
Kernel methods for rela-tional learning.
In Proceedings of ICML03.Charles J. Fillmore.
1982.
Frame semantics.
In Linguistics inthe Morning Calm.D.
Haussler.
1999.
Convolution kernels on discrete struc-tures.
Technical report ucs-crl-99-10, University of Califor-nia Santa Cruz.T.
Joachims.
1999.
Making large-scale SVM learning practical.In B. Scho?lkopf, C. Burges, and A. Smola, editors, Advancesin Kernel Methods - Support Vector Learning.Paul Kingsbury and Martha Palmer.
2002.
From Treebank toPropBank.
In Proceedings of LREC02.Taku Kudo and Yuji Matsumoto.
2003.
Fast methods forkernel-based text analysis.
In Proceedings of ACL03.M.
P. Marcus, B. Santorini, and M. A. Marcinkiewicz.
1993.Building a large annotated corpus of english: The Penn Tree-bank.
Computational Linguistics.Alessandro Moschitti.
2004.
A study on convolution kernelsfor shallow semantic parsing.
In proceedings of ACL04.Alessandro Moschitti.
2006.
Making tree kernels practical fornatural language learning.
In Proceedings of EACL06.John Shawe-Taylor and Nello Cristianini.
2004.
Kernel Meth-ods for Pattern Analysis.
Cambridge University Press.Jun Suzuki, Hideki Isozaki, and Eisaku Maeda.
2004.
Con-volution kernels with feature selection for natural languageprocessing tasks.
In Proceedings of ACL04.S.V.N.
Vishwanathan and A.J.
Smola.
2002.
Fast kernels onstrings and trees.
In Proceedings of NIPS02.D.
Zelenko, C. Aone, and A. Richardella.
2003.
Kernel meth-ods for relation extraction.
JMLR.100
