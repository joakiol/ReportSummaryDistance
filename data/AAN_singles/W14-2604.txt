Proceedings of the 5th Workshop on Computational Approaches to Subjectivity, Sentiment and Social Media Analysis, pages 18?23,Baltimore, Maryland, USA.
June 27, 2014.c?2014 Association for Computational LinguisticsInducing Domain-specific Noun Polarity Guided by Domain-independentPolarity Preferences of AdjectivesManfred KlennerComputational LinguisticsUniversity of ZurichSwitzerlandklenner@cl.uzh.chMichael AmslerComputational LinguisticsUniversity of ZurichSwitzerlandmamsler@ifi.uzh.chNora HollensteinComputational LinguisticsUniversity of ZurichSwitzerlandhollenstein@ifi.uzh.chAbstractIn this paper, we discuss how domain-specific noun polarity lexicons can be in-duced.
We focus on the generation ofgood candidates and compare two ma-chine learning scenarios in order to estab-lish an approach that produces high pre-cision.
Candidates are generated on thebasis of polarity preferences of adjectivesderived from a large domain-independentcorpus.
The polarity preference of a word,here an adjective, reflects the distributionof positive, negative and neutral argumentsthe word takes (here: its nominal head).Given a noun modified by some adjectives,a vote among the polarity preferences ofthese adjectives establishes a good indica-tor of the polarity of the noun.
In our ex-periments with five domains, we achievedf-measure of 59% up to 88% on the basisof two machine learning approaches car-ried out on top of the preference votes.1 IntroductionPolarity lexicons are crucial for fine-grained sen-timent analysis.
For instance, in approachescarrying out sentiment composition (Moilanenand Pulman, 2007), where phrase-level polar-ity is composed out of word level polarity (e.g.disappointed?hope+?
NP?).
However, oftenfreely available lexicons are domain-independent,which is a problem with domain-specific texts,since lexical gaps reduce composition anchors.But how many domain-specific words do we haveto expect?
Is it a real or rather a marginal problem?In our experiments, we found that domain-specificnouns do occur quite often - so they do matter.
Inone of our domains, we identified about 1000 neg-ative nouns, 409 were domain-specific.
In that do-main, the finance sector, more than 13?000 nountypes exist that do not occur at all in the DeWaccorpus - a large Web corpus (in German) with over90 Million sentences.
Thus, most of them mustbe regarded as domain-specific.
It would be quitetime-consuming to go through all of them in orderto identify and annotate the polar ones.
Could we,rather, predict good candidates?
We would needpolarity predictors - words that take other, polarwords e.g.
as their heads.
If they, moreover, hada clear-cut preference, i.e.
they mostly took onekind of polar words, say negative, then they wereperfect predictors of the polarity of nouns.
Wefound that adjectives (e.g.
acute) can be used assuch polarity predictors (e.g.
acute mostly takesnegative nouns, denoted n?, e.g.
acute pain)).Our hypothesis is that the polarity prefer-ences of adjectives are (more or less) domain-independent.
We can learn the preferences fromdomain-independent texts and apply it to domain-specific texts and get good candidates of domain-specific polar nouns.
Clearly, if the polarity pref-erences of an adjective are balanced (0.33 for eachpolarity), than the predictions could not help at all.But if one polarity clearly prevails, we might evenget a good performance by just classifying the po-larity of unknown nouns in a domain according tothe dominant polarity preference of the adjectivesthey co-occur with.In this paper, we show how to generatesuch a preference model on the basis of alarge, domain-independent German corpus anda domain-independent German polarity lexicon.We use this model to generate candidate nounsfrom five domain-specific text collections - rang-ing from 3?200 up to 37?000 texts per domain.In order to see how far an automatic inductionof a domain-specific noun lexicon could go, wealso experimented with machine learning scenar-ios on the output of the baseline system.
We ex-perimented with a distributional feature setting onthe basis of unigrams and used the Maximum En-18tropy learner, Megam (Daum?e III, 2004), to learna classifier.
We also worked with Weka (Frank etal., 2010) and features derived from the Germanpolarity lexicon.
Both approaches yield significantgains in terms of precision - so they realize a high-precision scenario.2 Inducing the Preference ModelWe seek to identify adjectives which impose aclear-cut polar preference on their head nouns.The polarity preference of an adjective reflects thedistribution of positive, negative and neutral nounsthe adjective modifies given to some text corpus.We used the domain-independent DeWac corpus(Baroni M., 2009) comprising about 90 millionGerman sentences.
We selected those adjectivesthat frequently co-occurred with polar nouns fromPoLex, a freely available German polarity lexicon(Clematide and Klenner, 2010).
Since the originalpolarity lexicon contained no neutral nouns, wefirst identified 2100 neutral nouns and expandedthe lexicon1.
Altogether 5?500 nouns were avail-able, 2100 neutral, 2100 negative and 1250 pos-itive.
For each adjective, we counted how oftenit took (i.e.
modified) positive, negative or neu-tral nouns in the DeWac corpus and determinedtheir polarity preferences for each class (positive,negative and neutral).
This way, 28?500 adjec-tives got a probability distribution, most of them,however, with a dominating neutral polarity pref-erence.
Two lexicons were derived from it: a pos-itive and a negative polarity preference lexicon.An adjective obeys a polar polarity preference ifthe sum of its positive and negative polarity pref-erences is higher than its neutral preference.
If thepositive preference is higher than the negative, theadjective is a positive polarity predictor, otherwiseit is a negative polarity predictor.
This procedureleaves us with 506 adjectives, 401 negative polar-ity predictors and 105 positive polarity predictors.Figure 1 shows some examples of negative polar-ity predictors.
It reveals that, for instance, the ad-jective akut (acute) is mostly coupled with neg-ative nouns (61.50%).
Nouns not in PoLex thatco-occur with an adjective are not considered.
Weassume that these unknown nouns of an adjectivefollow the same distribution that we are samplingfrom the known co-occurring nouns.
Note that po-1We searched for nouns that frequently co-occurred withthe same adjectives the polar nouns from the polarity lexicondid and stopped annotating when we reached 2?100 neutralnouns.larity predictors not necessarily must have a priorpolarity itself.
Actually, only 3 of the 12 adjectivesfrom Figure 1 do have a prior polarity (indicated asn?).
For instance, the adjective pl?otzlich (immedi-ate) is not polar but has a negative polarity pref-erence.
The polarity preference of a word is notuseful in composition, it just reveals the empirical(polar) context of the word.
If, however, the polar-ity of the context word is unknown, the preferencemight license an informed polarity guess.adjective English POS NEG #n?arg?bad/very 02.65 55.14 301heftig intensive 07.73 48.77 814v?ollig total 25.79 42.43 787akut acute 06.27 61.50 478latent latent 07.96 47.76 402ziemlich rather 14.16 52.36 233drohend?threatening 35.1 52.54 824pl?otzlich immediate 17.78 41.82 703gravierend grave 04.5 48.5 400chronisch chronic 03.26 72.11 398schleichend subtle 03.76 52.97 319hemmunglos?unscrupulous 15.49 43.19 213Figure 1: Negative Polarity PredictorsHere is the formula for the estimation of thenegative polarity preference as given in Figure 1(n?denotes a negative noun from PoLex, ajanadjective modifying an instance of n?)2:prefn?
(aj) =#ajn?#a+,?,=jNote that we count the number of adj-nountypes (#ajn?
), not tokens.
#a+,?,=jis the num-ber of adj-noun types of the adjective ajfor allclasses: positive (+), negative(-) and neutral (=).Figure 2 gives examples of positive polarity pre-dictors with some of their nouns.German English POSungetr?ubt unclouded joyunbeirrbar unerring hope?uberstr?omend overwhelming happinessbewunderswert mirable competencefalschverstanden falsely-understood tolerancewiedergewonnen regained freedomFigure 2: Positive Polarity Predictors3 Applying the Preference ModelWe applied the preference model to texts from fivedomains: banks (37?346 texts), transport (3221),2This could be interpreted as the conditional probabilityof a negative noun given the adjective.19insurance (4768), politics (3208) and pharma(4790).
These texts have been manually classifiedover the last 15 years by an institute carrying outmedia monitoring3, not only wrt.
their domain,but also wrt.
target-specific polarity (we just usethe domain annotation, currently).The polarity of a noun is predicted by the voteof the adjectives it occurred with.
The followingformula shows the polarity prediction pol+,?,=forthe class negative (pol?):pol?
(ni) = Ai??aj?PM???(aj,ni)prefn?
(aj)Aiis the number of adjectives that modify thenoun niin the domain-specific texts.
PM?is theset of adjectives from the polarity model (PM )with a negative polarity preference and (aj, ni) istrue, if the adjective ajmodifies the noun niac-cording to the domain-specific documents.4 Improving the PredictionsThe preference model serves two purposes: it gen-erates a list of candidates for polar nouns and itestablishes a baseline.
We experimented with twofeature settings in order to find out whether wecould improve on these results.In the first setting, the WK setting, we wanted toexploit the fact that for some adjectives that mod-ify a noun, we know their prior polarity (from thepolarity lexicon).
These adjectives do not nec-essarily have a clear positive or negative polaritypreference.
If not, then they are not used in theprediction of the noun polarity.But could the co-occurrence of a noun with ad-jectives bearing a prior polarity also be indicativeof the noun polarity?
For instance, if a noun iscoupled frequently and exclusively with negativeadjectives.
Does this indicate something?
Onesintuition might mislead, but a machine learningapproach could reveal correlations.
We used Sim-ple Logistic Regression (SRL) from Weka and thefollowing features:1. the number of positive adjectives with a priorpolarity that modify the noun2.
the number of negative adjectives with a priorpolarity that modify the noun3We would like to thank the f?og institute (cf.www.foeg.uzh.ch/) for these data (mainly newspaper texts inGerman).3. the difference between 1) and 2): absoluteand ratio4.
the ratio of positive and negative adjectives5.
two binary features indicating the majorityclass6.
three features for the output of the prefer-ence model: the positive, negative and neu-tral scores: pol?, pol+, pol=, respectively.In the second setting, the MG setting, we trainedMegam, a Maximum Entropy learner, among thefollowing lines: we took all polar nouns fromPoLex and extracted from the DeWac corpus allsentences containing these nouns.
For each noun,all (context) words (nouns, adjectives, verbs) co-occurring with it in these sentences are used asbag of words training vectors.
In other words, welearned a tri-partite classifier to predict the polarityclass (positive, negative or neutral) given a targetnoun and its context, i.e.
those nouns co-occurringwith it in a text collection.5 ExperimentsThe goal of our experiments were the predictionof positive and negative domain-specific nouns infive domains.
We used our preference model togenerate candidates.
Then we manually annotatedthe results in order to obtain a domain-specificgold standard.
We evaluated the output of thepreference model relative to the new gold stan-dards and we run our experiments with Megamand Weka?s Simple Logistic Regression (SRL).Megam and Weka?s SLR were trained on the basisof the positive, negative and neutral nouns fromPoLex and the DeWac corpus.Figure 3 shows the results.
#PM gives the num-ber of nouns predicted by the preference modelto be negative (e.g.
220 in the politics domain).These are the nouns we annotated for polarityand that formed our gold standard afterwards (e.g.75.90 out of 110 predicted are true negative nounsand are kept as the gold standard).
Since the gen-eration of the gold standard is based on the prefer-ence model?s output, its recall is 1.
We cannot fixthe real recall since this would require to manu-ally classify all nouns occurring in those texts (e.g.13?000 in the banks domain).
However, since wewanted to compare the machine learning perfor-mance with the preference model, we had to mea-20ID domain texts #PM prec f #WK prec rec f #MG prec rec fD1 politics 3208 220 75.90 86.29 195 78.97 92.22 83.26 130 81.54 63.48 69.13D2 transport 3221 141 71.63 83.47 127 73.22 92.07 80.57 64 78.12 49.50 58.54D3 insurance 4768 255 76.86 86.91 238 78.57 95.40 85.13 155 79.35 62.75 69.09D4 pharma 4790 257 71.59 83.44 228 76.75 95.11 81.69 137 87.83 65.40 68.35D5 banks 37346 1013 70.38 88.02 825 77.84 90.07 79.02 437 81.23 49.78 58.32Figure 3: Prediction of Negative Nounssure recall, otherwise we could not determine theoverall performance.From Figure 3 we can see that the preferencemodel (PM) performs best in terms of f-measure(in bold).
Of course, recall (i.e.
1, not shown) isidealized, since we took the output of the prefer-ence model to generate the gold standard.
Notehowever that this was our premise, that we neededan approach that delivers good candidates, other-wise we were lost given the vast amount of can-didate nouns (e.g.
remember the 13?000 nouns inthe finance sector).German EnglishWertverminderung impairment of assetsStagflation stagflationGeldschwemme money glut?Uberhitzungssymptom overheating symptomHyperinflation hyperinflationEuroschw?ache weakness of the euroWerterosion erosion in valueNachfrage?uberhang surplus in demandMargendruck pressure on marginsKlumpenrisiko cluster riskVirus virusHandekzem hand eczemaSchweinegrippe swine fluGeb?armutterriss ruptured uterusAlzheimer AlzheimerSehst?orung defective eye sightTinnitus tinnitusFigure 4: Domain-specific Negative NounsFigure 4 shows examples of negative nounsfrom two domains: banks and pharma.
But: are allfound nouns domain-specific negative nouns?
Inthe bank domain, we have manually annotated fordomain specificity: out of 1013 nouns predictedto be negative by the model, 409 actually weredomain-specific (40.3 %)4.
The other nouns couldalso be in a domain-independent polarity lexicon.Now, we turn to the prediction of positivedomain-specific nouns.
It is not really surpris-ing that the preference model is unbalanced - thatthere are far more negative than positive polaritypredictors: 401 compared to 105.
PoLex, the pool451 of the 131 (38.93%) as positive classified nouns actu-ally were domain-specific.of nouns used for learning of the polarity prefer-ences already is unbalanced (2100 negative com-pared to 1250 positive nouns).
Also, the major-ity of the texts in our five domains are negative(all texts are annotated for document-level polar-ity).
It is obvious then that our model is betterin the prediction of negative than positive polarity.Actually, our base model comprising 105 positivepolarity predictors does not trigger often withinthe whole corpus.
For instance, only 10 predic-tions were made in the banks domain, despite the37?346 texts.
Clearly, newspaper texts often arecritical and thus more negative than positive vo-cabulary is used.
This explains the very low recall.However, what if we relaxed our model?
If we,for example, keep those adjectives in our modelthat have a positive polarity preference > 0.35, atleast 35 out of 100 nouns co-occurring with thoseadjectives should be positive.ID #1 prec #2 prec #3 prec #4 precD1 18 66.6 25 60.0 25 60.0 8 50D2 14 85.7 16 75.0 0 0 3 33.3D3 13 69.2 15 60.0 5 100 1 100D4 13 84.6 15 80.0 9 55.5 2 100D5 135 76.2 174 71.2 58 87.9 40 82.5Figure 5: Prediction of Positive NounsWe report the results of two runs.
The first one,labelled #1, where adjectives are used to predict apositive noun polarity if they have a positive po-larity preference > 0.35 and where the negativepolarity preference is < 0.1.
In the second run, la-belled #2, we only require the positive preferenceto be > 0.35.
Table 5 shows the results.
We alsoshow the results of Weka (label #3) and Megam(label #4) for the candidates generated by #2.Compared to the negative settings, the numberof found positive nouns is rather low.
For instance,in the banks domain, 174 nouns were suggestedcompared to 1013 negative ones.
However, pre-cision has not dropped and it is especially higherthan the threshold value of 0.35 seemed to indi-cate (as discussed previously).
Weka (#3) andMegam (#4) again show better precision, however21the number of found nouns is too low (in a settingthat suffers already from low numbers).
Figure 6shows a couple of found positive nouns.German EnglishVersammlungsfreiheit freedom of assemblyAusl?anderintegration integration of foreignersEinlagesicherung deposit protectionLohntransparenz wage transparencyHaushaltsdisziplin budgetary disciplineVertriebsst?arke marketing strengthAnlegervertrauen confidence of investorsKritikf?ahigkeit ability for criticismF?uhrungskompetenz leadership competenciesFigure 6: Predicted Positive NounsSo far, we have discussed a binary approachwhere each class (positive, negative) was predictedand classified independently and where especiallyno adjectives with a neutral preference where con-sidered.
What happens if we include these adjec-tives?
The results are given in Figure 7.domain #neg prec #pos precbanks 288 80.16 3 66.66pharma 141 70.92 32 68.75transport 78 67.94 0 0politics 115 76.52 0 0insurance 132 66.66 0 0Figure 7: Unrestricted Prediction of Noun PolarityAlthough precision is good, the results are veryconservative, e.g.
in the banks domain, only 288nouns were found compared to 1013 nouns giventhe binary mode.
Recall and f-measure are lowercompared to the binary setting.
The huge amountof neutral preference adjectives (about 28?000)seems to neutralize polar tendencies.
But eventhen, some predictions survive - so these contextsseem to be strong.6 Related WorkThe expansion or creation of sentiment lexiconshas been investigated in many variations from dif-ferent perspectives and for various goals.
Liuand Zhang (2012) subdivide the work in this fieldinto three groups: manual approaches, dictionary-based approaches and corpus-based approaches.While the manual approach is time-consuming, itis still often used to create core lexicons which arenot domain-specific, e.g.
(Taboada et al., 2011).The dictionary-based approaches which are alsocalled thesaurus-based approaches (Huang et al.,2014) try to make use of existing dictionaries orthesauri like WordNet (e.g.
(Esuli and Sebastiani,2006; Baccianella et al., 2010; Neviarouskaya etal., 2011)) while the corpus-based approaches relyon statistical measures based on different con-cepts, for example, sentiment consistency (Hatzi-vassiloglou and McKeown, 1997), pointwise mu-tual information (Turney, 2002), context co-herency (Kanayama and Nasukawa, 2006), doublepropagation (Qiu et al., 2011) or label propagation(Huang et al., 2014).
Our approach is based on theuse of an existing dictionary and of an domain-independent corpus.
But rather than using the cor-pus to directly detect new entries for the lexicon,we use it to derive the polarity preference of adjec-tives which in turn is used to generate candidatesfrom the domain-specific corpus.The model most similar to our approach is(Klenner and Petrakis, 2014), where the contex-tual and prior polarity of nouns is learned from thepolarity preference of verbs for the verb?s directobject.
However, no attempt is made to inducedomain-specific polarity as we do.
We also fo-cus on the polarity preference of adjectives and wealso try to improve precision by machine learning.7 ConclusionsWe have introduced a plain model for the in-duction of domain-specific noun lexicons.
First,the polarity preferences of adjectives are learnedfrom domain-independent text and from a gen-eral polarity lexicon.
A voting approach then pre-dicts noun polarity from adjective noun pairingssampled from domain-specific texts.
The predic-tions based only on adjectives acting as positiveor negative polarity predictors perform astonish-ingly well.
Machine Learning can be used to im-prove precision at the cost of recall.
Our approachthus even might be useful for fully automatic gen-eration of a high precision, domain-specific priornoun polarity lexicons.In future work, we will apply our approach toother languages than German.
We then will alsohave to cope with multiword expressions as well,since compounds not longer - as in German - comeas single words.
We also would like to carry outan extrinsic evaluation in order to see how big theimpact of an induced domain-specific lexicon onpolarity text classification actually is.22ReferencesStefano Baccianella, Andrea Esuli, and Fabrizio Sebas-tiani.
2010.
Sentiwordnet 3.0: An enhanced lexicalresource for sentiment analysis and opinion mining.In Proc.
of LREC 2010, volume 10, pages 2200?2204.Ferraresi A. Zanchetta E. Baroni M., Bernardini S.2009.
The WaCky Wide Web: A collection of verylarge linguistically processed Web-crawled corpora.Language Resources and Evaluation, 43(3):209?226.Simon Clematide and Manfred Klenner.
2010.
Eval-uation and extension of a polarity lexicon for Ger-man.
In Proceedings of the First Workshop on Com-putational Approaches to Subjectivity and SentimentAnalysis, pages 7?13.Hal Daum?e III.
2004.
Notes on CG and LM-BFGSoptimization of logistic regression.
Paper avail-able at http://pub.hal3.name#daume04cg-bfgs, im-plementation available at http://hal3.name/megam.Andrea Esuli and Fabrizio Sebastiani.
2006.
Senti-wordnet: A publicly available lexical resource foropinion mining.
In Proc.
of LREC 2006, volume 6,pages 417?422.Eibe Frank, Mark Hall, Geoffrey Holmes, RichardKirkby, Bernhard Pfahringer, Ian H. Witten, and LenTrigg.
2010.
Weka-A Machine Learning Work-bench for Data Mining.
In Oded Maimon and LiorRokach, editors, Data Mining and Knowledge Dis-covery Handbook, chapter 66, pages 1269?1277.Springer US, Boston, MA.Vasileios Hatzivassiloglou and Kathleen R McKeown.1997.
Predicting the semantic orientation of adjec-tives.
In Proc.
of the ACL 1997, pages 174?181.Association for Computational Linguistics.Sheng Huang, Zhendong Niu, and Chongyang Shi.2014.
Automatic construction of domain-specificsentiment lexicon based on constrained label prop-agation.
Knowledge-Based Systems, 56:191?200.Hiroshi Kanayama and Tetsuya Nasukawa.
2006.Fully automatic lexicon expansion for domain-oriented sentiment analysis.
In Proc.
of EMNLP2006, pages 355?363.
Association for Computa-tional Linguistics.Manfred Klenner and Stefanos Petrakis.
2014.
Induc-ing the contextual and prior polarity of nouns fromthe induced polarity preference of verbs.
Data &Knowledge Engineering, 90:13?21.Bing Liu and Lei Zhang.
2012.
A survey of opinionmining and sentiment analysis.
In Mining Text Data,pages 415?463.
Springer.Karo Moilanen and Stephen Pulman.
2007.
Sentimentcomposition.
In Proc.
of RANLP 2007, pages 378?382, Borovets, Bulgaria, September 27-29.Alena Neviarouskaya, Helmut Prendinger, and MitsuruIshizuka.
2011.
Sentiful: A lexicon for sentimentanalysis.
Affective Computing, IEEE Transactionson, 2(1):22?36.Guang Qiu, Bing Liu, Jiajun Bu, and Chun Chen.2011.
Opinion word expansion and target extractionthrough double propagation.
Computational Lin-guistics, 37(1):9?27.Maite Taboada, Julian Brooke, Milan Tofiloski, Kim-berly Voll, and Manfred Stede.
2011.
Lexicon-based methods for sentiment analysis.
Computa-tional Linguistics, 37(2):267?307.Peter D Turney.
2002.
Thumbs up or thumbs down?
:semantic orientation applied to unsupervised clas-sification of reviews.
In Proc.
of the ACL 2002,pages 417?424.
Association for Computational Lin-guistics.23
