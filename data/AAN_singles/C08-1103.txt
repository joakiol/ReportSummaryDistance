Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008), pages 817?824Manchester, August 2008Topic Identification for Fine-Grained Opinion AnalysisVeselin Stoyanov and Claire CardieDepartment of Computer ScienceCornell University{stoyanov,cardie}@cs.cornell.eduAbstractWithin the area of general-purpose fine-grained subjectivity analysis, opinion topicidentification has, to date, received littleattention due to both the difficulty of thetask and the lack of appropriately anno-tated resources.
In this paper, we pro-vide an operational definition of opiniontopic and present an algorithm for opiniontopic identification that, following our newdefinition, treats the task as a problem intopic coreference resolution.
We develop amethodology for the manual annotation ofopinion topics and use it to annotate topicinformation for a portion of an existinggeneral-purpose opinion corpus.
In exper-iments using the corpus, our topic identi-fication approach statistically significantlyoutperforms several non-trivial baselinesaccording to three evaluation measures.1 IntroductionSubjectivity analysis is concerned with extract-ing information about attitudes, beliefs, emotions,opinions, evaluations, sentiment and other privatestates expressed in texts.
In contrast to the prob-lem of identifying subjectivity or sentiment at thedocument level (e.g.
Pang et al (2002), Turney(2002)), we are interested in fine-grained subjec-tivity analysis, which is concerned with subjec-tivity at the phrase or clause level.
We expectfine-grained subjectivity analysis to be useful forquestion-answering, summarization, informationextraction and search engine support for queries ofc?
2008.
Licensed under the Creative CommonsAttribution-Noncommercial-Share Alike 3.0 Unported li-cense (http://creativecommons.org/licenses/by-nc-sa/3.0/).Some rights reserved.the form ?How/what does entity X feel/think abouttopic Y?
?, for which document-level opinion anal-ysis methods can be problematic.Fine-grained subjectivity analyses typicallyidentify SUBJECTIVE EXPRESSIONS in context, charac-terize their POLARITY (e.g.
positive, neutral or neg-ative) and INTENSITY (e.g.
weak, medium, strong,extreme), and identify the associated SOURCE, orOPINION HOLDER, as well as the TOPIC, or TARGET, ofthe opinion.
While substantial progress has beenmade in automating some of these tasks, opiniontopic identification has received by far the least at-tention due to both the difficulty of the task and thelack of appropriately annotated resources.1This paper addresses the problem of topic iden-tification for fine-grained opinion analysis of gen-eral text.2We begin by providing a new, opera-tional definition of opinion topic in which the topicof an opinion depends on the context in whichits associated opinion expression occurs.
We alsopresent a novel method for general-purpose opin-ion topic identification that, following our new def-inition, treats the problem as an exercise in topiccoreference resolution.
We evaluate the approachusing the existing MPQA corpus (Wiebe et al,2005), which we extend with manual annotationsthat encode topic information (and refer to here-after as the MPQATOPICcorpus).Inter-annotator agreement results for the manualannotations are reasonably strong across a num-ber of metrics and the results of experiments thatevaluate our topic identification method in the con-text of fine-grained opinion analysis are promising:1Section 3 on related work provides additional discussion.2The identification of products and their components andattributes from product reviews is a related, but quite differenttask from that addressed here.
Section 3 briefly discusses, andprovides references, to the most relevant research in that area.817using either automatically or manually identifiedtopic spans, we achieve topic coreference scoresthat statistically significantly outperform two topicsegmentation baselines across three coreferenceresolution evaluation measures (B3, ?
and CEAF).For the B3metric, for example, the best base-line achieves a topic coreference score on theMPQATOPICcorpus of 0.55 while our topic coref-erence algorithm scores 0.57 and 0.71 using au-tomatically, and manually, identified topic spans,respectively.In the remainder of the paper, we define opin-ion topics (Section 2), present related work (Sec-tion 3), and motivate and describe the key ideaof topic coreference that underlies our methodol-ogy for both the manual and automatic annota-tion of opinion topics (Section 4).
Creation ofthe MPQATOPICcorpus is described in Section 5and our topic identification algorithm, in Section 6.The evaluation methodology and results are pre-sented in Sections 7 and 8, respectively.2 Definitions and ExamplesConsider the following opinion sentences:(1)[OHJohn] adores [TARGET+TOPIC SPANMarseille] andvisits it often.
(2)[OHAl] thinks that [TARGET SPAN[TOPIC SPAN?thegovernment] should [TOPIC SPAN?tax gas] more in order to[TOPIC SPAN?curb [TOPIC SPAN?CO2emissions]]].A fine-grained subjectivity analysis should iden-tify: the OPINION EXPRESSION3as ?adores?
in Exam-ple 1 and ?thinks?
in Example 2; the POLARITY aspositive in Example 1 and neutral in Example 2;the INTENSITY as medium and low, respectively; andthe OPINION HOLDER (OH) as ?John?
and ?Al?, re-spectively.
To be able to discuss the opinion TOPICin each example, we begin with three definitions:?
Topic.
The TOPIC of a fine-grained opinion isthe real-world object, event or abstract entity that isthe subject of the opinion as intended by the opin-ion holder.?
Topic span.
The TOPIC SPAN associated with anOPINION EXPRESSION is the closest, minimal span oftext that mentions the topic.?
Target span.
In contrast, we use TARGET SPANto denote the span of text that covers the syntactic3For simplicity, we will use the term opinion throughoutthe paper to cover all types of private states expressed in sub-jective language.surface form comprising the contents of the opin-ion.In Example 1, for instance, ?Marseille?
is boththe TOPIC SPAN and the TARGET SPAN associated withthe city of Marseille, which is the TOPIC of the opin-ion.
In Example 2, the TARGET SPAN consists of thetext that comprises the complement of the subjec-tive verb ?thinks?.
Example 2 illustrates why opin-ion topic identification is difficult: within the sin-gle target span of the opinion, there are multiplepotential topics, each identified with its own topicspan.
Without more context, however, it is impos-sible to know which phrase indicates the intendedtopic.
If followed by sentence 3, however,(3)Although he doesn?t like government-imposed taxes, hethinks that a fuel tax is the only effective solution.the topic of Al?s opinion in 2 is much clearer ?
itis likely to be fuel tax, denoted via the TOPIC SPAN?tax gas?
or ?tax?.3 Related WorkAs previously mentioned, there has been much re-cent progress in extracting fine-grained subjectiv-ity information from general text.
Previous effortshave focused on the extraction of opinion expres-sions in context (e.g.
Bethard et al (2004), Brecket al (2007)), the assignment of polarity to theseexpressions (e.g.
Wilson et al (2005), Kim andHovy (2006)), source extraction (e.g.
Bethard etal.
(2004), Choi et al (2005)), and identification ofthe source-expresses-opinion relation (e.g.
Choi etal.
(2006)), i.e.
linking sources to the opinions thatthey express.Not surprisingly, progress has been driven bythe creation of language resources.
In this regard,Wiebe et al?s (2005) opinion annotation schemefor subjective expressions was used to create theMPQA corpus, which consists of 535 documentsmanually annotated for phrase-level expressions ofopinions, their sources, polarities, and intensities.Although other opinion corpora exist (e.g.
Bethardet al (2004), Voorhees and Buckland (2003), theproduct review corpora of Liu4), we are not awareof any corpus that rivals the scale and depth of theMPQA corpus.In the related area of opinion extraction fromproduct reviews, several research efforts have fo-cused on the extraction of the topic of the opin-ion (e.g.
Kobayashi et al (2004), Yi et al (2003),4http://www.cs.uic.edu/ liub/FBS/sentiment-analysis.html818Popescu and Etzioni (2005), Hu and Liu (2004)).For this specialized text genre, it has been suf-ficient to limit the notion of topic to mentionsof product names and components and their at-tributes.
Thus, topic extraction has been effec-tively substituted with a lexicon look-up and tech-niques have focused on how to learn or acquire anappropriate lexicon for the task.
While the tech-niques have been very successful for this genreof text, they have not been applied outside theproduct reviews domain.
Further, there are anal-yses (Wiebe et al, 2005) and experiments (Wilsonet al, 2005) that indicate that lexicon-lookup ap-proaches to subjectivity analysis will have limitedsuccess on general texts.Outside the product review domain, there hasbeen little effort devoted to opinion topic annota-tion.
The MPQA corpus, for example, was orig-inally intended to include topic annotations, butthe task was abandoned after confirming that itwas very difficult (Wiebe, 2005; Wilson, 2005),although target span annotation is currently under-way.
While useful, target spans alone will be insuf-ficient for many applications: they neither containinformation indicating which opinions are aboutthe same topic, nor provide a concise textual rep-resentation of the topics.Due to the lack of appropriately annotated cor-pora, the problem of opinion topic extraction hasbeen largely unexplored in NLP.
A notable excep-tion is the work of Kim and Hovy (2006).
Theypropose a model that extracts opinion topics forsubjective expressions signaled by verbs and ad-jectives.
Their model relies on semantic framesand extracts as the topic the syntactic constituentat a specific argument position for the given verbor adjective.
In other words, Kim and Hovy extractwhat we refer to as the target spans, and do so fora subset of the opinion-bearing words in the text.Although on many occasions target spans coincidewith opinion topics (as in Example 1), we have ob-served that on many other occasions this is not thecase (as in Example 2).
Furthermore, hampered bythe lack of resources with manually annotated tar-gets, Kim and Hovy could provide only a limitedevaluation.As we have defined it, opinion topic identifica-tion bears some resemblance to topic segmenta-tion, the goal of which is to partition a text intoa linear sequence of topically coherent segments.Existing methods for topic segmentation typicallyassume that fragments of text (e.g.
sentences orsequences of words of a fixed length) with sim-ilar lexical distribution are about the same topic;the goal of these methods is to find the boundarieswhere the lexical distribution changes (e.g.
Choi(2000), Malioutov and Barzilay (2006)).
Opin-ion topic identification differs from topic segmen-tation in that opinion topics are not necessarily spa-tially coherent ?
there may be two opinions inthe same sentence on different topics, as well asopinions that are on the same topic separated byopinions that do not share that topic.
Nevertheless,we will compare our topic identification approachto a state-of-the-art topic segmentation algorithm(Choi, 2000) in the evaluation.Other work has successfully adopted the use ofclustering to discover entity relations by identify-ing entities that appear in the same sentence andclustering the intervening context (e.g.
Hasegawaet al (2004), Rosenfeld and Feldman (2007)).
Thiswork, however, considers named entities and headsof proper noun phrases rather than topic spans,and the relations learned are those commonly heldbetween NPs (e.g.
senator-of-state, city-of-state,chairman-of-organization) rather than a more gen-eral coreference relation.4 A Coreference Approach to TopicIdentificationGiven our initial definition of opinion topics (Sec-tion 2), the next task is to determine which com-putational approaches might be employed for au-tomatic opinion topic identification.
We begin thisexercise by considering some of the problematiccharacteristics of opinion topics.Multiple potential topics.
As noted earlier viaExample 2, a serious problem in opinion topicidentification is the mention of multiple potentialtopics within the target span of the opinion.
Al-though an issue for all opinions, this problem istypically more pronounced in opinions that do notcarry sentiment (as in Example 2).
Our currentdefinition of opinion topic requires the NLP sys-tem (or a human annotator) to decide which of theentities described in the target span, if any, refersto the intended topic.
This decision can be aidedby the following change to our definition of opin-ion topic, which introduces the idea of a context-dependent information focus: the TOPIC of an opin-ion is the real-world entity that is the subject of theopinion as intended by the opinion holder based819on the discourse context.With this modified definition in hand, and givenExample 3 as the succeeding context for Example2, we argue that the intended subject, and hencethe TOPIC, of Al?s opinion in 2 can be quickly iden-tified as the FUEL TAX, which is denoted by the TOPICSPANS ?tax gas?
in 2 and ?fuel tax?
in 3.Opinion topics not always explicitly mentioned.In stark contrast to the above, on many occasionsthe topic is not mentioned explicitly at all withinthe target span, as in the following example:(5)[OHJohn] identified the violation of Palestinian humanrights as one of the main factors.
TOPIC: ISRAELI-PALESTINIAN CONFLICTWe have further observed that the opinion topicis often not mentioned within the same paragraphand, on a few occasions, not even within the samedocument as the opinion expression.4.1 Our Solution: Topic CoreferenceWith the above examples and problems in mind,we hypothesize that the notion of topic corefer-ence will facilitate both the manual and automaticidentification of opinion topics: We say that twoopinions are topic-coreferent if they share thesame opinion topic.
In particular, we conjec-ture that judging whether or not two opinions aretopic-coreferent is easier than specifying the topicof each opinion (due to the problems describedabove).5 Constructing the MPQATOPICCorpusRelying on the notion of topic coreference, we nextintroduce a newmethodology for the manual anno-tation of opinion topics in text:1.
The annotator begins with a corpus of documents thathas been annotated w.r.t.
OPINION EXPRESSIONS.
Witheach opinion expression, the corpus provides POLARITY andOPINION HOLDER information.
(We use the aforementionedMPQA corpus.)2.
The annotator maintains a list of the opinion expressionsthat remain to be annotated (initially, all opinion expressionsin the document) as well as a list of the current groupings (i.e.clusters) of opinion expressions that have been identified astopic-coreferent (initially this list is empty).3.
For each opinion expression, in turn, the annotator decideswhether the opinion is on the same topic as the opinions inone of the existing clusters or should start a new cluster, andinserts the opinion in the appropriate cluster.4.
The annotator labels each cluster with a string that de-scribes the opinion topic that covers all opinions in the cluster.5.
The annotator marks the TOPIC SPAN of each opinion.
(This can be done at any point in the process.
)The manual annotation procedure is de-scribed in a set of instructions available athttp://www.cs.cornell.edu/?ves.
In addition, wecreated a GUI that facilitates the annotation proce-dure.
With the help of these resources, one personannotated opinion topics for a randomly selectedset of 150 of the 535 documents in the MPQAcorpus.
In addition, 20 of the 150 documents wereselected at random and annotated by a secondannotator for the purposes of an inter-annotatoragreement study, the results of which are presentedin Section 8.1.
The MPQATOPICand the procedureby which it was created are described in moredetail in (Stoyanov and Cardie, 2008).6 The Topic Coreference AlgorithmAs mentioned in Section 4, our computational ap-proach to opinion topic identification is based ontopic coreference: For each document (1) find theclusters of coreferent opinions, and (2) label theclusters with the name of the topic.
In this paperwe focus only on the first task, topic coreferenceresolution ?
the most critical step for topic identi-fication.
We conjecture that the second step can beperformed through frequency analysis of the termsin each of the clusters and leave it for future work.Topic coreference resolution resembles anotherwell-known problem in NLP ?
noun phrase (NP)coreference resolution.
Therefore, we adapt astandard machine learning-based approach to NPcoreference resolution (Soon et al, 2001; Ng andCardie, 2002) for our purposes.
Our adaptation hasthree steps: (i) identify the topic spans; (ii) performpairwise classification of the associated opinionsas to whether or not they are topic-coreferent; and,(iii) cluster the opinions according to the results of(ii).
Each step is discussed in more detail below.6.1 Identifying Topic SpansDecisions about topic coreference should dependon the text spans that express the topic.
Ideally,we would be able to recover the topic span of eachopinion and use its content for the topic corefer-ence decision.
However, the topic span depends onthe topic itself, so it is unrealistic that topic spanscan be recovered with simple methods.
Neverthe-less, in this initial work, we investigate two sim-820ple methods for automatic topic span identificationand compare them to two manual approaches:?
Sentence.
Assume that the topic span is thewhole sentence containing the opinion.?
Automatic.
A rule-based method for identi-fying the topic span (developed using MPQAdocuments that are not part of MPQATOPIC).Rules depend on the syntactic constituenttype of the opinion expression and rely onsyntactic parsing and grammatical role label-ing.?
Manual.
Use the topic span marked by thehuman annotator.
We included this methodto provide an upper bound on performance ofthe topic span extractor.?
Modified Manual.
Meant to be a more real-istic use of the manual topic span annotations,this method returns the manually identifiedtopic span only when it is within the sentenceof the opinion expression.
When this spanis outside the sentence boundary, this methodreturns the opinion sentence.Of the 4976 opinions annotated across the 150documents of MPQATOPIC, the topic spans associ-ated with 4293 were within the same sentence asthe opinion; 3653 were within the span extractedby our topic span extractor.
Additionally, the topicspans of 173 opinions were outside of the para-graph containing the opinion.6.2 Pairwise Topic Coreference ClassificationThe heart of our method is a pairwise topic coref-erence classifier.
Given a pair of opinions (andtheir associated polarity and opinion holder infor-mation), the goal of the classifier is to determinewhether the opinions are topic-coreferent.
We usethe manually annotated data to automatically learnthe pairwise classifier.
Given a training document,we construct a training example for every pair ofopinions in the document (each pair is representedas a feature vector).
The pair is labeled as a posi-tive example if the two opinions belong to the sametopic cluster, and a negative example otherwise.Pairwise coreference classification relies criti-cally on the expressiveness of the features usedto describe the opinion pair.
We use three cate-gories of features: positional, lexico-semantic andopinion-based features.Positional features These features are intendedto exploit the fact that opinions that are close toeach other are more likely to be on the same topic.We use six positional features:?
Same Sentence/Paragraph5True if the twoopinions are in the same sentence/paragraph.?
Consecutive Sentences/Paragraphs True ifthe two opinions are in consecutive sen-tences/paragraphs.?
Number of Sentences/Paragraphs Thenumber of sentences/paragraphs that separatethe two opinions.TOPIC SPAN-based lexico-semantic features Thefeatures in this group rely on the topic spans andare recomputed w.r.t.
each of the four topic spanmethods.
The intuition behind this group of fea-tures is that topic-coreferent opinions are likely toexhibit lexical and semantic similarity within thetopic span.?
tf.idf The cosine similarity of the tf.idfweighted vectors of the terms contained in thetwo spans.?
Word overlap True if the two topic spanscontain any contain words in common.?
NP coref True if the two spans contain NPsthat are determined to be coreferent by a sim-ple rule-based coreference system.?
NE overlap True if the two topic spans con-tain named entities that can be consideredaliases of each other.Opinion features The features in this group de-pend on the attributes of the opinion.
In the cur-rent work, we obtain these features directly fromthe manual annotations of the MPQATOPICcorpus,but they might also be obtained from automaticallyidentified opinion information using the methodsreferenced in Section 3.?
Source Match True if the two opinions havethe same opinion holder.?
Polarity Match True if the two opinions havethe same polarity.5We use sentence/paragraph to describe two features ?
onebased on the sentence and one on the paragraph.821?
Source-PolarityMatch False if the two opin-ions have the same opinion holder but con-flicting polarities (since it is unlikely that asource will have two opinions with conflict-ing polarities on the same topic).We employ three classifiers for pairwise corefer-ence classification ?
an averaged perceptron (Fre-und and Schapire, 1998), SVMlight(Joachims,1998) and a rule-learner ?
RIPPER (Cohen, 1995).However, we report results only for the averagedperceptron, which exhibited the best performance.6.3 ClusteringPairwise classification provides an estimate of thelikelihood that two opinions are topic-coreferent.To form the topic clusters, we follow the pairwiseclassification with a clustering step.
We selecteda simple clustering algorithm ?
single-link cluster-ing, which has shown good performance for NPcoreference.
Given a threshold, single-link cluster-ing proceeds by assigning pairs of opinions with atopic-coreference score above the threshold to thesame topic cluster and then performs transitive clo-sure of the clusters.67 Evaluation MethodologyFor training and evaluation we use the 150-document MPQATOPICcorpus.
All machine learn-ing methods were tested via 10-fold cross valida-tion.
In each round of cross validation, we useeight of the data partitions for training and one forparameter estimation (we varied the threshold forthe clustering algorithm), and test on the remainingpartition.
We report results for the three evaluationmeasures of Section 7 using the four topic spanextraction methods introduced in Section 6.
Thethreshold is tuned separately for each evaluationmeasure.
As noted earlier, all runs obtain opinioninformation from the MPQATOPICcorpus (i.e.
thiswork does not incorporate automatic opinion ex-traction).7.1 Topic Coreference BaselinesWe compare our topic coreference system to fourbaselines.
The first two are the ?default?
baselines:?
one topic ?
assigns all opinions to the samecluster.6Experiments using best-first and last-first clustering ap-proaches provided similar or worse results.?
one opinion per cluster ?
assigns each opin-ion to its own cluster.The other two baselines attempt to perform topicsegmentation (discussed in Section 3) and assignall opinions within the same segment to the sameopinion topic:?
same paragraph ?
simple topic segmenta-tion by splitting documents into segments atparagraph boundaries.?
Choi 2000 ?
Choi?s (2000) state-of-the-artapproach to finding segment boundaries.
Weuse the freely available C99 software de-scribed in Choi (2000), varying a parameterthat allows us to control the average numberof sentences per segment and reporting thebest result on the test data.7.2 Evaluation MetricsBecause there is disagreement among researchersw.r.t.
the proper evaluation measure for NP coref-erence resolution, we use three generally acceptedmetrics7to evaluate our topic coreference system.B-CUBED.
B-CUBED (B3) is a commonlyused NP coreference metric (Bagga and Baldwin,1998).
It calculates precision and recall for eachitem (in our case, each opinion) based on the num-ber of correctly identified coreference links, andthen computes the average of the item scores ineach document.
Precision/recall for an item i iscomputed as the proportion of items in the inter-section of the response (system-generated) and key(gold standard) clusters containing i divided by thenumber of items in the response/key cluster.CEAF.
As a representative of another group ofcoreference measures that rely on mapping re-sponse clusters to key clusters, we selected Luo?s(2005) CEAF score (short for Constrained Entity-Alignment F-Measure).
Similar to the ACE (2005)score, CEAF operates by computing an optimalmapping of response clusters to key clusters andassessing the goodness of the match of each of themapped clusters.Krippendorff?s ?.
Finally, we use Passonneau?s(2004) generalization of Krippendorff?s (1980) ??
a standard metric employed for inter-annotator7The MUC scoring algorithm (Vilain et al, 1995) wasomitted because it led to an unjustifiably high MUC F-score(.920) for the ONE TOPIC baseline.822B3?
CEAFAll opinions .6424 .5476 .6904Sentiment opinions .7180 .7285 .7967Strong opinions .7374 .7669 .8217Table 1: Inter-annotator agreement results.reliability studies.
Krippendorff?s ?
is basedon a probabilistic interpretation of the agreementof coders as compared to agreement by chance.While Passonneau?s innovation makes it possibleto apply Krippendorff?s ?
to coreference clusters,the probabilistic interpretation of the statistic is un-fortunately lost.8 Results8.1 Inter-annotator AgreementAs mentioned previously, out of the 150 anno-tated documents, 20 were annotated by two anno-tators for the purpose of studying the agreementbetween coders.
Inter-annotator agreement resultsare shown in Table 1.
We compute agreement forthree subsets of opinions: all available opinions,only the sentiment-bearing opinions and the sub-set of sentiment-bearing opinions judged to havepolarity of medium or higher.The results support our conjecture that topicsof sentiment-bearing opinions are much easier toidentify: inter-annotator agreement for opinionswith non-neutral polarity (SENTIMENT OPINIONS) im-proves by a large margin for all measures.
As inother work in subjectivity annotation, we find thatstrong sentiment-bearing opinions are easier to an-notate than sentiment-bearing opinions in general.Generally, the ?
score aims to probabilisticallycapture the agreement of annotation data and sep-arate it from chance agreement.
It is generally ac-cepted that an ?
score of .667 indicates reliableagreement.
The score that we observed for theoverall agreement was an?
of .547, which is belowthe generally accepted level, while ?
for the twosubsets of sentiment-bearing opinions is above .72.However, as discussed above, due to the way thatit is adapted to the problem of coreference resolu-tion, the ?
score loses its probabilistic interpreta-tion.
For example, the ?
score requires that a pair-wise distance function between clusters is speci-fied.
We used one sensible choice for such a func-tion (we measured the distance between clusters Aand B as dist(A,B) = (2?
|A?B|)/(|A|+ |B|)),B3?
CEAFOne topic .3739 -.1017 .2976One opinion per cluster .2941 .2238 .2741Same paragraph .5542 .3123 .5090Choi .5399 .3734 .5370Sentence .5749 .4032 .5393Rule-based .5730 .4056 .5420Modified manual .6416 .5134 .6124Manual .7097 .6585 .6184Table 2: Results for the topic coreference algo-rithms.but other sensible choices for the distance lead tomuch higher scores.
Furthermore, we observedthat the behavior of the ?
score can be rather er-ratic ?
small changes in one of the clusterings canlead to big differences in the score.Perhaps a better indicator of the reliability ofthe coreference annotation is a comparison withthe baselines, shown in the top half of Table 2.All baselines score significantly lower than theinter-annotator agreement scores.
With one excep-tion, the inter-annotator agreement scores are alsohigher than those for the learning-based approach(results shown in the lower half of Table 2), aswould typically be expected.
The exception is theclassifier that uses the manual topic spans, but aswe argued earlier these spans carry significant in-formation about the decision of the annotator.8.2 BaselinesResults for the four baselines are shown in the firstfour rows of Table 2.
As expected, the two base-lines performing topic segmentation show substan-tially better scores than the two ?default?
base-lines.8.3 Learning methodsResults for the learning-based approaches areshown in the bottom half of Table 2.
First, wesee that each of the learning-based methods out-performs the baselines.
This is the case even whensentences are employed as a coarse substitute forthe true topic span.
A Wilcoxon Signed-Rank testshows that differences from the baselines for thelearning-based runs are statistically significant forthe B3and ?
measures (p < 0.01); for CEAF,using sentences as topic spans for the learning al-gorithm outperforms the SAME PARAGRAPH baseline(p < 0.05), but the results are inconclusive when823compared with the system of CHOI.In addition, relying on manual topic span infor-mation (MANUAL and MODIFIED MANUAL) allows thelearning-based approach to perform significantlybetter than the two runs that use automaticallyidentified spans (p < 0.01, for all three measures).The improvement in the scores hints at the impor-tance of improving automatic topic span extrac-tion, which will be a focus of our future work.9 ConclusionsWe presented a new, operational definition of opin-ion topics in the context of fine-grained subjec-tivity analysis.
Based on this definition, we in-troduced an approach to opinion topic identifi-cation that relies on the identification of topic-coreferent opinions.
We further employed theopinion topic definition for the manual annotationof opinion topics to create the MPQATOPICcorpus.Inter-annotator agreement results show that opin-ion topic annotation can be performed reliably.Finally, we proposed an automatic approach foridentifying topic-coreferent opinions, which sig-nificantly outperforms all baselines across threecoreference evaluation metrics.Acknowledgments The authors of this paperwould like to thank Janyce Wiebe and TheresaWilson for many insightful discussions.
This workwas supported in part by National Science Foun-dation Grants BCS- 0624277 and IIS-0535099 andby DHS Grant N0014-07-1-0152.ReferencesACE.
2005.
The NIST ACE evaluation website.http://www.nist.gov/speech/tests/ace/.Bagga, A. and B. Baldwin.
1998.
Algorithms for scoringcoreference chains.
In In Proceedings of MUC7.Bethard, S., H. Yu, A. Thornton, V. Hativassiloglou, andD.
Jurafsky.
2004.
Automatic extraction of opinion propo-sitions and their holders.
In 2004 AAAI Spring Symposiumon Exploring Attitude and Affect in Text.Breck, E., Y. Choi, and C. Cardie.
2007.
Identifying expres-sions of opinion in context.
In Proceedings of IJCAI.Choi, Y., C. Cardie, E. Riloff, and S. Patwardhan.
2005.
Iden-tifying sources of opinions with conditional random fieldsand extraction patterns.
In Proceedings of EMNLP.Choi, Y., E. Breck, and C. Cardie.
2006.
Joint extraction ofentities and relations for opinion recognition.
In Proceed-ings of EMNLP.Choi, F. 2000.
Advances in domain independent linear textsegmentation.
Proceedings of NAACL.Cohen, W. 1995.
Fast effective rule induction.
In Proceed-ings of ICML.Freund, Y. and R. Schapire.
1998.
Large margin classifi-cation using the perceptron algorithm.
In Proceedings ofComputational Learing Theory.Hasegawa, T., S. Sekine, and R. Grishman.
2004.
Discover-ing relations among named entities from large corpora.
InProceedings of ACL.Hu, M. and B. Liu.
2004.
Mining opinion features in cus-tomer reviews.
In AAAI.Joachims, T. 1998.
Making large-scale support vector ma-chine learning practical.
In B. Sch?olkopf, C. Burges,A.
Smola, editor, Advances in Kernel Methods: SupportVector Machines.
MIT Press, Cambridge, MA.Kim, S. and E. Hovy.
2006.
Extracting opinions, opinionholders, and topics expressed in online news media text.In Proceedings of ACL/COLING Workshop on Sentimentand Subjectivity in Text.Kobayashi, N., K. Inui, Y. Matsumoto, K. Tateishi, andT.
Fukushima.
2004.
Collecting evaluative expressionsfor opinion extraction.
In Proceedings of IJCNLP.Krippendorff, K. 1980.
Content Analysis: An Introduction toIts Methodology.
Sage Publications, Beverly Hills, CA.Luo, X.
2005.
On coreference resolution performance met-rics.
In Proceedings of EMNLP.Malioutov, I. and R. Barzilay.
2006.
Minimum cut modelfor spoken lecture segmentation.
In Proceedings ofACL/COLING.Ng, V. and C. Cardie.
2002.
Improving machine learningapproaches to coreference resolution.
In In Proceedings ofACL.Pang, B., L. Lee, and S. Vaithyanathan.
2002.
Thumbsup?
Sentiment classification using machine learning tech-niques.
In Proceedings of EMNLP.Passonneau, R. 2004.
Computing reliability for coreferenceannotation.
In Proceedings of LREC.Popescu, A. and O. Etzioni.
2005.
Extracting productfeatures and opinions from reviews.
In Proceedings ofHLT/EMNLP.Rosenfeld, B. and R. Feldman.
2007.
Clustering for unsuper-vised relation identification.
In Proceedings of CIKM.Soon, W., H. Ng, and D. Lim.
2001.
A machine learningapproach to coreference resolution of noun phrases.
Com-putational Linguistics, 27(4).Stoyanov, V. and C. Cardie.
2008.
Annotating topics of opin-ions.
In Proceedings of LREC.Turney, P. 2002.
Thumbs up or thumbs down?
Semantic ori-entation applied to unsupervised classification of reviews.In Proceedings of ACL.Vilain, M., J. Burger, J. Aberdeen, D. Connolly, andL.
Hirschman.
1995.
A model-theoretic coreference scor-ing scheme.
In Proceedings of the MUC6.Voorhees, E. and L. Buckland.
2003.
Overview of theTREC 2003 Question Answering Track.
In Proceedingsof TREC 12.Wiebe, J., T. Wilson, and C. Cardie.
2005.
Annotating ex-pressions of opinions and emotions in language.
LanguageResources and Evaluation, 1(2).Wiebe, J.
2005.
Personal communication.Wilson, T., J. Wiebe, and P. Hoffmann.
2005.
Recognizingcontextual polarity in phrase-level sentiment analysis.
InProceedings of HLT/EMNLP.Wilson, T. 2005.
Personal communication.Yi, J., T. Nasukawa, R. Bunescu, and W. Niblack.
2003.
Sen-timent analyzer: Extracting sentiments about a given topicusing natural language processing techniques.
In Proceed-ings of ICDM.824
