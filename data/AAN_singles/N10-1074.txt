Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 501?509,Los Angeles, California, June 2010. c?2010 Association for Computational LinguisticsLearning Words and Their Meanings from Unsegmented Child-directedSpeechBevan K. Jones & Mark JohnsonDept of Cognitive and Linguistic SciencesBrown UniversityProvidence, RI 02912, USA{Bevan Jones,Mark Johnson}@Brown.eduMichael C. FrankDept of Brain and Cognitive ScienceMassachusetts Institute of TechnologyCambridge, MA 02139, USAmcfrank@mit.eduAbstractMost work on language acquisition treatsword segmentation?the identification of lin-guistic segments from continuous speech?and word learning?the mapping of those seg-ments to meanings?as separate problems.These two abilities develop in parallel, how-ever, raising the question of whether theymight interact.
To explore the question, wepresent a new Bayesian segmentation modelthat incorporates aspects of word learning andcompare it to a model that ignores word mean-ings.
The model that learns word meaningsproposes more adult-like segmentations forthe meaning-bearing words.
This result sug-gests that the non-linguistic context may sup-ply important information for learning wordsegmentations as well as word meanings.1 IntroductionAcquiring a language entails mastering many learn-ing tasks simultaneously, including identifyingwhere words begin and end in continuous speechand learning meanings for those words.
It is com-mon to treat these tasks as separate, sequential pro-cesses, where segmentation is a prerequisite to wordlearning but otherwise there are few if any depen-dencies.
The earliest evidence of segmentation,however, is for words bordering a child?s own name(Bortfeld et al, 2005).
In addition, infants beginlearning their first words before they achieve adult-level competence in segmentation.
These two piecesof evidence raise the question of whether the tasks ofmeaning learning and segmentation might mutuallyinform one another.To explore this question we present a joint modelthat simultaneously identifies word boundaries andattempts to associate meanings with words.
In do-ing so we make two contributions.
First, by model-ing the two levels of structure in parallel we simu-late a more realistic situation.
Second, a joint modelallows us to explore possible synergies and interac-tions.
We find evidence that our joint model per-forms better on a segmentation task than an alterna-tive model that does not learn word meanings.The picture in Figure 1 depicts a language learn-ing situation from our corpus (originally from Fer-nald and Morikawa, 1993; recoded in Frank et al,2009) where a mother talks while playing with var-ious toys.
Setting down the dog and picking up thehand puppet of a pig, she asks, ?Is that the pig?
?Starting out, a young learner not only does not knowthat the word ?pig?
refers to the puppet but does noteven know that ?pig?
is a word at all.
Our modelsimulates the learning task, taking as input the un-segmented phonemic representation of the speechalong with the set of objects in the non-linguisticcontext as shown in Figure 1 (a), and infers both asegmentation and a word-object mapping as in Fig-ure 1 (b).One can formulate the word learning task asthat of finding a reasonably small set of reusableword-meaning pairs consistent with the underlyingcommunicative intent.
Infant directed speech oftenrefers to objects in the immediate environment, andearly word learning seems to involve associating fre-quently co-occurring word-object pairs (Akhtar andMontague, 1999; Markman, 1990).
Several compu-tational models are based on this idea that a word501Figure 1: (a) The input to our system for the utterance?Is that the pig??
consists of an unsegmented sequenceof phonemes and the set of objects representing the non-linguistic context.
These objects were manually iden-tified by inspecting the associated video, a frame fromwhich is shown above.
(b) The gold-standard segmenta-tion and word-object assignments of the same utterance,against which the output of our system is evaluated (allwords except ?pIg?
are mapped to a special ?null?
object,as explained in the text).that frequently occurs in the presence of an objectand not so frequently in its absence is likely to re-fer to that object (Frank et al, 2009a; Siskind, 1996;Yu and Ballard, 2007).
Importantly, all these modelsassume words are pre-segmented in the input.While the word segmentation task relates lessclearly to the communicative content, it can be for-mulated according to a similar objective, that of at-tempting to explain the sound sequences in the inputin terms of some reasonably small set of reusableunits, or words.
Computational models have suc-cessfully addressed the problem in much this way(Johnson and Goldwater, 2009; Goldwater et al,2009; Brent, 1999), and the general approach is con-sistent with experimental observations that humansare sensitive to statistics of sound sequences (Saffranet al, 1996; Frank et al, 2007).The two tasks can be integrated in a relativelyseamless way, since, as we have just formulatedthem, they have a common objective, that of findinga minimal, consistent set of reusable units.
However,the two deal with different types of information withdifferent dependencies.
The basic idea is that learn-ing a vocabulary that both meets the constraints ofthe word-learning task and is consistent with the ob-jective of the segmentation task can yield a bettersegmentation.
That is, we hope to find a synergy inthe joint inference of meaning and segmentation.Note that to the best of our knowledge there isvery little computational work that combines wordform and word meaning learning (Frank et al 2006takes a first step but their model is applicable onlyto small artificial languages).
Frank et al (2009a)and Regier (2003) review pure word learning mod-els and, in addition to the papers we have alreadycited, Brent (1999) presents a fairly comprehensivereview of previous pure segmentation models.
How-ever, none of the models reviewed make any attemptto jointly address the two problems.
Similarly, in thebehavioral literature on development, we are awareof only one segmentation study (Graf-Estes et al,2007) that involves non-linguistic context, thoughthis study treats the two tasks sequentially ratherthan jointly.We now describe our model and inference proce-dure and follow with evaluation and discussion.2 Model DefinitionCross-situational meaning learning in our joint wordlearning and segmenting model is inspired by themodel of Frank et al (2009a).
Our model canbe viewed as a variant of the Latent Dirichlet Al-location (LDA) topic model of Blei et al (2003),where topics are drawn from the objects in the non-linguistic context.
The model associates each utter-ance with a single referent object, the topic, and ev-ery word in the utterance is either generated from adistribution over words associated with that objector else from a distribution associated with a special?null?
object shared by all utterances.
Note that inthis paper we use ?topic?
to denote the referent ob-ject of an utterance, otherwise we depart from topicmodeling convention and use the term ?object?
in-stead.Segmentation is based on the unigram model pro-posed by Brent (1999) and reformulated by Goldwa-ter et al (2009) in terms of a Dirichlet process.
Sinceboth LDA and the unigram segmenter are based onunigram distributions it is relatively straightforward502Figure 2: Topical Unigram Model: Oj is the set of objectsin the non-linguistic context of the jth utterance, zj is theutterance topic, wji is the ith word of the utterance, xji isthe category of the word (referring or non-referring), andthe other variables are distribution parameters.to integrate the two to simultaneously infer wordboundaries and word-object associations.Figure 2 illustrates a slightly simplified form ofthe model, and the the relevant distributions are asfollows:z|O ?
Uniform(O)Gz|z, ?0, ?1, P0 ?
{DP(?1, P0) if z 6= 0DP(?0, P0) otherwisepi ?
Beta(1, 1)x|pi ?
Bernoulli(pi)w|G, z, x ?
{Gz if x = 1G0 if x = 0Note that Uniform(O) denotes a discrete uniformdistribution over the elements of the set O. P0 isdescribed later.Briefly, each utterance has a single topic zj , drawnfrom the objects in the non-linguistic context Oj ,and then for each word wji we first flip a coin xjito determine if it refers to the topic or not.
Then, de-pending on xji the word is either drawn from a dis-tribution specific to the topic (xji = 1) or from a dis-tribution associated with the ?null?
object (xji = 0).In slightly greater detail but still glossing over thedetails of how the multinomial parameters are gen-erated, the generative story proceeds as follows:1.
For each utterance, indexed by j2.
(a) Pick a single topic zj uniformly from the setof objects in the environment Oj(b) For each word wji of the utterance(c) i.
Determine if it refers to zj or not by set-ting xji to 1 (referring) with probability pi,and to 0 (non-referring) otherwise.ii.
if xji is 1, draw wji from the topic specificdistribution over words Gzj .iii.
otherwise, draw wji from G0, the distribu-tion over words associated with the ?null?object.This generative story is a simplification since itdoes not describe how we model utterance bound-aries.
It is important for segmentation purposesto explicitly model utterance boundaries since, un-like utterance-internal word boundaries, we as-sume utterance boundaries are observed.
Thus,the story is complicated by the fact that there isa chance each time we generate a word that wealso generate an utterance boundary.
The choice ofwhether to terminate the utterance or not is capturedby a Bernoulli(?)
random variable $ji indicatingwhether the ith word was the last word of the jthutterance.?
?
Beta(1, 1)$|?
?
Bernoulli(?
)The Gz multinomial parameters are generatedfrom a Dirichlet process with base distribution overwords, P0, which describes how new word typesare generated from their constituent phonemes.Phonemes are generated sequentially, i.i.d.
uni-formly from m phonemic types.
In addition, thereis a probability p# of generating a word boundary.P0(w) = (1?
p#)|w|?1p#1m|w|The concentration parameters ?0 and ?1 also playa critical role in the generation of words and wordtypes.
Any given word has a certain probabilityof either being produced from the set of previouslyseen word types, or from an entirely new one.
The503greater the concentration parameter, the more likelythe model is to appeal to the base distribution P0 tointroduce a new word type.Like Frank et al (2009a), we distinguish betweentwo coarse grammatical categories, referring andnon-referring.
Referring words are generated by thetopic, while non-referring words are drawn from G0,a distribution associated with the ?null?
object.
Thedistinction ensures sparse word-object maps thatobey the principle of mutual exclusion.
Otherwiseall words in the utterance would be associated withthe topic object, resulting in a very large set of wordsfor each object that is very likely to overlap with thewords for other objects.
As a further bias towarda small lexicon, we employ different concentrationparameters (?0 and ?1) for the non-referring and re-ferring words, using a much smaller value for thereferring words.
Intuitively, there should be a rela-tively small prior probability of introducing a newword-object pair, corresponding to a small ?1 value.On the other hand, most other words don?t refer tothe topic object (or any other object for that matter),corresponding to a much larger ?0 value.Note that this topical unigram model is a straight-forward generalization of the unigram segmentationmodel (Goldwater et al, 2009) to the case of multi-ple topics.
In fact, if all words were assumed to referto the same object (or to no object at all) the modelswould be identical.Unlike LDA, each ?document?
has only one topic,which is necessitated by the fact that in our modeldocuments correspond single utterances.
The ut-terances in our corpus of child directed speech areoften only four or five words long, whereas thegeneral LDA model assumes documents are muchlarger.
Thus, there may not be enough words to in-fer a useful utterance specific distribution over top-ics.
Consequently, rather than inferring a separatetopic distribution for each utterance, we simply as-sume a uniform distribution over objects in the non-linguistic context.
In effect, we rely entirely on thenon-linguistic context and word-object associationsto infer topics.
Though necessitated by data sparsityissues, we also note that it is very rare in our cor-pus for utterances to refer to more than one object inthe non-linguistic context, so the choice of a singletopic may also be a more accurate model.
In fact,even with multi-sentence documents, LDA may per-form better if only one topic is assumed per sentence(Gruber et al, 2007).3 InferenceWe use a collapsed Gibbs sampling procedure, in-tegrating over all possible Gz , pi, and ?
values andthen iteratively sample values for each variable con-ditioned on the current state of all other variables.We visit each utterance once per iteration, sample atopic, and then visit each possible word boundary lo-cation to sample the boundary and word categoriessimultaneously according to their joint probability.A single topic is sampled for each utterance, con-ditioned on the words and their current determina-tions as referring or non-referring.
Since zj is drawnfrom a uniform distribution, this probability is sim-ply proportionate to the conditional probability ofthe words given zj and the xji variables.P (zj |wj,xj,h?j) ??
(?Wjw n(h?
)w,zj + ?1P0(w))?
(?Wjw n(h)w,zj + ?1P0(w))?Wj?w?
(n(h)w,zj + ?1P0(w))?(n(h?
)w,zj + ?1P0(w))Here, P (zj |wj,xj,h?j) is the probability of topiczj given the current hypothesis h for all variables ex-cluding those for the current utterance.
Also, n(h?j)w,zjis the count of occurrences of word type w that referto topic zj among the current variable assignments,and Wj is the set of word types appearing in utter-ance j.
The vectors of word and category variablesin utterance j are represented as wj and xj, respec-tively.
Note that only referring words have any bear-ing on the appropriate selection of zj and so all fac-tors involving only non-referring words are absorbedby the constant of proportionality.The word categories can be sampled conditionedon the current word boundary states according to thefollowing conditional probability, where n(h?ji)xji isthe number of words categorized according to label504xji over the entire corpus excluding word wji.P (xji|wji, zj ,h?ji) ?
P (wji|zj , xji,h?ji)?P (xji|h?ji)=n(h?ji)wji,xjizj + ?xjiP0(wji)n(h?ji)?,xjizj + ?xji?
n(h?ji)xji + 1n(h?ji)?
+ 2(1)In practice, however, we actually sample the wordcategory variables jointly with the boundary states,using a scheme similar to that outlined in Gold-water et al (2009).
We visit each possible wordboundary location (any point between two consec-utive phonemes) and compute probabilities for thehypotheses for which the phonemic environmentmakes up either one word or two.
As illustrated be-low there are two sets of cases: those where we treatthe segment as a single word, and those where wetreat it as two words.x1 x2 x3.
.
.#w1#.
.
.
vs. .
.
.#w2#w3#.
.
.?
?The probabilities of the hypotheses can be derivedby application of equation 1.
Since the x variablescan each describe two possible events, there are a to-tal of six different cases to consider for each bound-ary assignment: two cases without and four with aword boundary.The probability of each of the two cases withouta word boundary can be computed as follows:P (w1, x1|z,h?)
=n(h?
)w1,x1z + ?x1P0(w1)n(h?
)?,x1z + ?x1?n(h?
)x1 + 1n(h?)?
+ 2?n(h?
)$1 + 1n(h?)?
+ 2Here h?
signifies the current hypothesis for allvariables excluding those for the current segmentand n(h?
)$1 is the count for h?
of either utterance fi-nal words if w1 is utterance final or non-utterancefinal words if w1 is also not utterance final.In the four cases with a word boundary, we havetwo words and two categories to sample.P (w2, x2, w3, x3|z,h?)
=n(h?
)w2,x2z + ?x2P0(w2)n(h?
)?,x2z + ?x2?n(h?
)x2 + 1n(h?)?
+ 2?n(h?
)$2=0 + 1n(h?)?
+ 2?n(h?
)w3,x3z + ?x2(x3)?w2(w3) + ?x3P0(w3)n(h?
)?,x3z + ?x2(x3) + ?x3?n(h?
)x3 + ?x2(x3) + 1n(h?)?
+ 3?n(h?
)$3 + ?$2($3) + 1n(h?)?
+ 3Here ?x(y) is 1 if x = y and 0 otherwise.4 Results & Model Comparisons4.1 CorpusOur training corpus (Fernald and Morikawa, 1993;Frank et al, 2009b) consists of about 22,000 wordsand 5,600 utterances.
Video recordings consistingof mother-child play over pairs of toys were ortho-graphically transcribed, and each utterance was an-notated with the set of objects present in the non-linguistic context.
The object referred to by the ut-terance, if any, was noted, as described in Frank et al(2009b).
We used the VoxForge dictionary to maporthographic words to phoneme sequences in a pro-cess similar to that described in Brent (1999).Figure 1 (a) presents an example of the codingof phonemic transcription and non-linguistic contextfor a single utterance.
The input to the system con-sists solely of the phonemic transcription and the ob-jects in the non-linguistic context.4.2 EvaluationWe ran the sampler ten times for 100,000 iterationswith parameter settings of ?1 = 0.01, ?0 = 20, andp# = 0.5, keeping only the final sample for evalu-ation.
We defined the word-object pairs for a sam-ple as the words in the referring category that werepaired at least once with a particular topic.
Thesepairs were then compared against a gold standardset of word-object pairs, while segmentation perfor-mance was evaluated by comparing the final bound-ary assignments against the gold standard segmenta-tion.5054.2.1 Word LearningTo explore the contribution of word boundariesto the joint word learning and segmenting task, wecompare our full joint model against a variant thatonly infers topics, using the gold standard segmen-tation as input.
In this way we also reproduce theusual assumption of a sequential relationship be-tween segmentation and word learning and test thenecessity of the simplifying assumption.
The re-sults are shown in Table 2.
We compare them withthree different metric types: topic accuracy; preci-sion, recall, and F-score of the word-object pairs;and Kullback-Liebler (KL) divergence.First, treating utterances with no referring wordsas though they have no topic, we compute the ac-curacy of the inferred topics.
Note that we don?treport accuracy for the the variant with no non-linguistic context, since in this case the objects areinterchangeable, and we have a problem identifyingwhich cluster corresponds to which topic.
Table 2shows that the joint segmentation and word learningmodel gets the topic right for 81% of the utterances.The variant that assumes pre-segmented input doescomparably well with an accuracy of 79%.
Surpris-ingly, it seems that knowing the gold segmentationdoesn?t add very much, at least for the topic infer-ence task.To evaluate how well we discovered the word-object map, we manually compiled a list of all thenouns in the corpus that named one of the 30 ob-jects.
We used this set of nouns, cross-referencedwith their topic objects, as a gold standard set ofword-object pairs.
By counting the co-occurrences,we also compute a gold standard probability distri-bution for the words given the topic, P (w|z, x = 1).Precision, recall and F-score are computed as perFrank et al (2009a).
In particular, precision is thefraction of gold pairs among the sampled set and re-call is the fraction of sampled pairs among the goldstandard pairs.p = |Sampled ?
Gold||Sampled| , r =|Sampled ?
Gold||Gold|KL divergence is a way of measuring the differ-ence between distributions.
Small numbers gener-ally indicate a close match and is zero only whenthe two are equal.
Using the empirical distributionObject WordsBOX thebox boxBRUSH brushBUNNY rabbit RosieBUS busCAR car thecarCHEESE cheeseDOG thedoggy doggyDOLL doll thedoll yeah beniceDOUGH doughERNIE ErnieTable 1: Subset of an inferred word-object mapping.
Forclarity, the proposed words have been converted to stan-dard English orthography.p r f KL accJoint 0.21 0.45 0.28 2.78 0.81Gold Seg 0.21 0.60 0.31 1.82 0.79Table 2: Word Learning Performance.
Comparingprecision, recall, and F-score of word-object pairs,DKL(P (w, z)||Q(w, z)), and accuracy of utterance top-ics for the full joint model and a variant that only infersmeanings given a gold standard segmentation.over gold topics P (z), we use the standard formulafor KL divergence to compare the gold standard dis-tribution P against the inferred distribution Q. I.e.,we compute DKL(P (w, z)||Q(w, z)).The model learns fairly meaningful word-objectassociations; results are shown in Table 2.
As in thecase of topic accuracy, the joint and word learningonly variants perform similarly, this time with some-what better performance for the easier task with anF-score and KL divergence of 0.31 and 1.82 vs. 0.28and 2.78 for the joint task.Table 1 illustrates the sort of word-object pairsthe model discovers.
As can be seen, many of theerrors are due to the segmentation, usually under-segmentation errors where it segments two words asone.
This is a general problem with the unigram seg-menter on which our model is based (Goldwater etal., 2009).
Yet, even though these segmentation er-rors are also counted as word learning errors, theyare often still meaningful in the sense that the truereferring word is a subsequence.So, word segmentation has an impact on wordlearning.
Yet, the joint model still tends to uncoverreasonable meanings.
The next question is whetherthese meanings have an impact on the segmentation.506NoCon Random JointReferring Nouns 0.36 0.35 0.50Neighbors 0.33 0.33 0.37Utt Final Nouns 0.36 0.36 0.52Entire Corpus 0.53 0.53 0.54Table 3: Segmentation performance.
F-score for threesubsets and the full corpus for three variants: the modelwithout non-linguistic context, the model with randomtopics, and the full joint model.4.2.2 Word SegmentationTo measure the impact of word learning on seg-mentation, we again compare the model on the fulljoint task against two other variants: one where top-ics are randomly selected, and one that ignores thenon-linguistic context.
For the random topics vari-ant, we choose each topic during initialization ac-cording to the empirical distribution over gold topicsand treat these topic assignments as observed vari-ables for subsequent iterations.
The variant that ig-nores non-linguistic context draws topics uniformlyfrom the entire set of objects ever discussed in thecorpus, another test of the contribution of the non-linguistic context to segmentation.
We report tokenF-score, computed as per Goldwater et al (2009),where any segment proposed by the model is a truepositive only if it matches the gold segmentation andis a false positive otherwise.
Any segment in thegold data not found by the model is a false negative.Table 3 shows the segmentation performance forvarious subsets as well as for the entire corpus.
Be-cause we are primarily interested in synergies be-tween word learning and segmentation, we focus onthe words most directly impacted by the meanings:gold standard referring nouns and their neighboringwords.The model behaves the same with randomizedtopics as without context; it learns none of the goldstandard pairs (no matter how we identify clusterswith topics for the contextless case).
On all subsets,the full joint model outperforms the other two vari-ants.
In particular, the greatest gain is for the refer-ring nouns, with a 21% reduction in error.
Also, sim-ilar to the findings of Bortfeld et al (2005) regarding6 month olds?
abilities to segment words adjoining afamiliar name, we also find that neighboring wordsbenefit from sharing a word boundary with a learnedword.The model performs exceptionally well on utter-ance final referring nouns, with a 24% reductionin error.
This may explain certain psycholinguisticobservations.
Frank et al (2006) performed an ar-tificial language experiment with humans subjectsdemonstrating that adults were able to learn wordsat the same time as they learned to segment the lan-guage.
However, subjects did much better on a wordlearning task when the meaning bearing words wereconsistently placed at the end of utterances.
Thereare several possible reasons why this might havebeen the case.
For instance, it is common in Englishfor the object noun to occur at the end of the sen-tence, and since the subjects were all English speak-ers, they may have found it easier to learn an artifi-cial language with a similar pattern.
However, ourmodel predicts another simple possibility: the seg-mentation task is easier at the end because one ofthe two word boundaries is already known (the ut-terance boundary itself).4.3 DiscussionThe word learning model generally prefers a verysparse word-to-object map.
This is enforced by us-ing a concentration parameter ?1 that is quite smallrelative to the ?0 parameter, and it biases the modelso that the distributions over referring words arevery different from that over non-referring words.
Asmall concentration parameter biases the estimatorto prefer a small set of word types.
In contrast, therelatively large concentration parameter for the non-referring words tends to result in most of the wordsreceiving highest probability as non-referring words.The model thus categorizes words accordingly.
It isin part due to this tendency towards sparse word-object maps that the model enforces mutual exclu-sivity, a phenomenon well documented among natu-ral word learners (Markman, 1990).Aside from contributing to mutual exclusivityand specialization among the topical word distri-butions, the small concentration parameter also hasimportant implications for the segmentation task.A very small value for ?1 discourages the learnerfrom acquiring more word types for each mean-ing than absolutely necessary, thereby forcing thesegmenter to use fewer types to explain the se-quence of phonemes.
A model without any notion507of meaning cannot maintain separate distributionsfor different topics, and must in some sense treat allwords as non-referring.
A segmenting model with-out meanings cannot share the word learner?s reluc-tance to propose new meaning-bearing word typesand might propose three separate types for ?yourbook?, ?a book?, and ?the book?.
However, witha small enough prior on new referring word types,the word learner that discovers a common refer-ent for all three sequences and, preferring fewer re-ferring word types, is more likely to discover thecommon subsequence ?book?.
With a single word-object pair (?book?, BOOK), the word learner couldexplain reference for all three sequences instead ofusing the three separate pairs (?yourbook?, BOOK),(?abook?, BOOK), and (?thebook?, BOOK).While relying on non-linguistic context helps seg-ment the meaning-bearing words, the overall im-provement is small in our current corpus.
One rea-son for this small improvement was that only 9%of the tokens in the corpus were referring words.In corpora containing a larger variety of objects ?and in cases where sub- and super-ordinate labelslike ?eyes?
and ?ears?
are coded ?
this percentage islikely to be much higher, leading to a greater boostin overall segmentation performance.We should acknowledge that the decisions en-tailed in enriching the annotations are neither triv-ial nor without theoretic implication, however.
It isnot immediately obvious how to represent the non-linguistic correlates of verbs, for instance.
Devel-opmentally, verbs are typically acquired much laterthan nouns, and it has been argued that this may bedue to the difficulty of producing a cognitive rep-resentation of the associated meaning (Gentner andBoroditsky, 2001).
Even among concrete nouns, notall are equal.
Children tend to have a bias towardwhole objects when mapping novel words to theirnon-linguistic counterparts (Markman, 1990).
De-cisions about more sophisticated encoding of non-linguistic information may thus require more knowl-edge about children?s representations of the worldaround them5 Conclusion and Future WorkWe find (1) that it is possible to jointly infer bothmeanings and a segmentation in a fully unsupervisedway and (2) that doing so improves the segmenta-tion performance of our model.
In particular, wefound that although the word learning side sufferedfrom segmentation errors, and performed worse thana model that learned from a gold standard segmen-tation, the loss was only slight.
On the other hand,segmentation performance for the meaning bearingwords improved a great deal.
The first result sug-gests that is not necessary to assume fully segmentedinput in order to learn word meanings, and that thesegmentation and word learning tasks can be effec-tively modeled in parallel, allowing us to explore po-tential developmental interactions.
The second re-sult suggests that synergies do actually exist and ar-gue not only that we can model the two as parallelprocesses, but that doing so could prove fruitful.Our model is relatively simple both in terms ofword learning and in terms of word segmentation.For instance, social cues and shared attention, or dis-course effects, might all play a role (Frank et al,2009b).
Shared features or other relationships canalso potentially impact how quickly one might gen-eralize a label to multiple instances (Tenenbaum andXu, 2000).
There are many ways to elaborate on theword learning task, with additional potential syner-gistic implications.We might also elaborate the linguistic structureswe incorporate into the word learning model.
Forinstance, Johnson (2008) explores synergies in syl-lable and morphological structures in word segmen-tation.
Aspects of linguistic structure, such as mor-phology, may contribute to word meaning learningbeyond its contribution to word segmentation per-formance.AcknowledgmentsThis research was funded by NSF awards 0544127and 0631667 to Mark Johnson and by NSF DDRIG0746251 to Michael C. Frank.
We would also liketo thank Anne Fernald for providing the corpus andMaeve Cullinane for help in coding it.ReferencesNameera Akhtar and Lisa Montague.
1999.
Early lexi-cal acquisition: The role of cross-situational learning.First Language, 19(57 Pt 3):347?358.508David Blei, Andrew Ng, and Michael Jordan.
2003.
La-tent dirichlet alocation.
Journal of Machine LearningResearch, 3:993?1022.Heather Bortfeld, James L. Morgan, Roberta MichnickGolinkoff, and Karen Rathbun.
2005.
Mommyand me: Familiar names help launch babies intospeechstream segmentation.
Psychological Science,16(4):298?304.Michael R. Brent.
1999.
An efficient, probabilisticallysound algorithm for segmentation and word discovery.Machine Learning, 34:71?105.Anne Fernald and Hiromi Morikawa.
1993.
Commonthemes and cultural variations in japanese and ameri-can mothers?
speech to infants.
In Child Development,number 3, pages 637?656, June.Michael C. Frank, Vikash Mansinghka, Edward Gibson,and Joshua B. Tenenbaum.
2006.
Word segmentationas word learning: Integrating stress and meaning withdistributional cues.
In Proceedings of the 31st AnnualBoston University Conference on Language Develop-ment.Michael C. Frank, Sharon Goldwater, Vikash Mans-inghka, Tom Griffiths, and Joshua Tenenbaum.
2007.Modeling human performance in statistical word seg-mentation.
Proceedings of the 29th Annual Meeting ofthe Cognitive Science Society, pages 281?286.Michael C. Frank, Noah D. Goodman, and Joshua B.Tenenbaum.
2009a.
Using speakers?
referential inten-tions to model early cross-situational word learning.Psychological Science, 5:578?585.Michael C. Frank, Noah D. Goodman, Joshua B. Tenen-baum, and Anne Fernald.
2009b.
Continuity of dis-course provides information for word learning.Dedre Gentner and Lera Boroditsky.
2001.
Individua-tion, relativity, and early word learning.
Language,culture, & cognition, 3:215?56.Sharon Goldwater, Thomas L. Griffiths, and Mark John-son.
2009.
A bayesian framework for word segmen-tation: Exploring the effects of context.
Cognition,112(1):21?54.Katharine Graf-Estes, Julia L. Evans, Martha W. Alibali,and Jenny R. Saffran.
2007.
Can infants map meaningto newly segmented words?
statistical segmentationand word learning.
Psychological Science, 18(3):254?260.Amit Gruber, Michal Rosen-Zvi, and Yair Weiss.
2007.Hidden topic markov models.
In Artificial Intelligenceand Statistics (AISTATS), March.Mark Johnson and Sharon Goldwater.
2009.
Improvingnonparameteric bayesian inference: experiments onunsupervised word segmentation with adaptor gram-mars.
In Proceedings of Human Language Technolo-gies: The 2009 Annual Conference of the North Ameri-can Chapter of the Association for Computational Lin-guistics, pages 317?325, Boulder, Colorado, June.
As-sociation for Computational Linguistics.Mark Johnson.
2008.
Using adaptor grammars to identi-fying synergies in the unsupervised acquisition of lin-guistic structure.
In Proceedings of the 46th AnnualMeeting of the Association of Computational Linguis-tics, Columbus, Ohio.
Association for ComputationalLinguistics.Ellen M. Markman.
1990.
Constraints children place onword learning.
Cognitive Science, 14:57?77.Terry Regier.
2003.
Emergent constraints on word-learning: A computational review.
Trends in CognitiveSciences, 7:263?268.Jenny R. Saffran, Elissa L. Newport, and Richard N.Aslin.
1996.
Word segmentation: The role of dis-tributional cues.
Journal of memory and Language,35:606?621.Jeffrey M. Siskind.
1996.
A computational studyof cross-situational techniques for learning word-to-meaning mappings.
Cognition, 61(1-2):39?91.Joshua B. Tenenbaum and Fei Xu.
2000.
Word learn-ing as bayesian inference.
In Proceedings of the 22ndAnnual Conference of the Cognitive Science Society,pages 517?522.Chen Yu and Dana H. Ballard.
2007.
A unified model ofearly word learning: Integrating statistical and socialcues.
Neurocomputing, 70(13-15):2149?2165.509
