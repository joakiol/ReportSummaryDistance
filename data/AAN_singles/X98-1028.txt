A Text-Extract ion Based SummarizerTomek Strzalkowski,  Gees C. Stein and G.GE Corporate Research & Development1 Research CircleNiskayuna, NY 12309, USABowden WiseAbstractWe present an automated method of generatinghuman-readable summaries from a variety of textdocuments including newspaper articles, businessreports, government documents, even broadcastnews transcripts.
Our approach exploits an em-pirical observation that much of the written textdisplay certain regularities of organization andstyle, which we call the Discourse Macro Structure(DMS).
A summary is therefore created to reflectthe components of a given DMS.
In order to pro-duce a coherent and readable summary we selectcontinuous, well-formed passages from the sourcedocument and assemble them into a mini-documentwithin a DMS template.
In this paper we describean automated summarizer that can generate bothshort indicative abstracts, useful for quick scanningof a list of documents, as well as longer informativedigests that can serve as surrogates for the full text.The summarizer can assist the users of an informa-tion retrieval system in assessing the quality of theresults returned from a search, preparing reportsand memos for their customers, and even buildingmore effective search queries.In t roduct ionA good summarization tool can be of enormous helpfor those who have to process large amounts of doc-uments.
In information retrieval one would bene-fit greatly from having content-indicative quick-readsummaries upplied along with the titles returnedfrom search.
Similarly, application areas like rout-ing, news on demand, market intelligence and topictracking would benefit from a good summarizationtool.Perhaps the most difficult problem in designingan automatic text summarization is to define whata summary is, and how to tell a summary from anon-summary, or a good summary from a bad one.The answer depends in part upon who the summaryis intended for, and in part upon what it is meant oachieve, which in large measure precludes any objec-tive evaluation.
A good summary should at least bea good reflection of the original document while be-ing considerably shorter than the original thus sav-ing the reader valuable reading time.In this paper we describe an automatic way togenerate summaries from text-only documents.
Thesummarizer we developed can create general andtopical indicative summaries, and also topical in-formative summaries.
Our approach is domain-independent and takes advantage of certain organi-zation regularities that were observed in news-typedocuments.
The system participated in a third-party evaluation program and turned out to be oneof the top-performing summarizers.
Especially thequality/length ratio was very good since our sum-maries tend to be very short (10% of the originallength).The summarizer is still undergoing improvementand expansion in order to be able to summarize awide variety of documents.
It is also used success-fully as a tool to solve different problems, like infor-mation retrieval and topic tracking.Task  Descr ip t ion  and  Re la ted  WorkFor most of us, a summary is a brief synopsis of thecontent of a larger document, an abstract recount-ing the main points while suppressing most details.One purpose of having a summary is to quickly learnsome facts, and decide what you want to do with theentire story.
Depending on how they are meant o beused one can distinguish between two kinds of sum-maries.
Indicative summaries are not a replacementfor the original text but are meant to be a good re-flection of the kind of information that can be foundin the original document.
Informative summariescan be used as a replacement of the original docu-ment and should contain the main facts of the docu-ment.
Independent of their usage summaries can beclassified as general summaries or topical summaries.A general summary addresses the main points of thedocument ignoring unrelated issues.
A topical sum-223mary will report he main issues relevant to a certaintopic, which might have little to do with the maintopic of the document.
Both summaries might givevery different impressions of the same document.
Inthis paper we describe a summarizer that summa-rizes one document, text only, at a time.
It is capa-ble of producing both topical and generic indicativesummaries, and topical informative summaries.Our early inspiration, and a benchmark, havebeen the Quick Read Summaries, posted daily offthe front page of New York Times on-line edition(http://www.nytimes.com).
These summaries, pro-duced manually by NYT staff, are assembled out ofpassages, sentences, and sometimes entence frag-ments taken from the main article with very few, ifany, editorial adjustments.
The effect is a collectionof perfectly coherent idbits of news: the who, thewhat, and when, but perhaps not why.
Indeed, thesesummaries leave out most of the details, and cannotserve as surrogates for the full article.
Yet, they M-low the reader to learn some basic facts, and then tochoose which stories to open.This kind of summarization, where appropriatepassages are extracted from the original text, is veryefficient, and arguably effective, because it doesn'trequire generation of any new text, and thus low-ers the risk of misinterpretation.
It is also relativelyeasier to automate, because we only need to identifythe suitable passages among the other text, a taskthat can be accomplished via shallow NLP and sta-tistical techniques.
Nonetheless, there are a num-ber of serious problems to overcome before an ac-ceptable quality summarizer can be built.
For one,quantitative methods alone are generally too weakto deal adequately with the complexities of naturallanguage text.
For example, one popular approachto automated abstract generation has been to selectkey sentences from the original text using statisti-cal and linguistic cues, perform some cosmetic ad-justments in order to restore cohesiveness, and thenoutput the result as a single passage, e.g., (Luhn1958) (Paice 1990) (Brandow, Mitze, & Rau 1995)(Kupiec, Pedersen, & Chen 1995).
The main advan-tage of this approach is that it can be applied toalmost any kind of text.
The main problem is thatit hardly ever produces an intelligible summary: theresulting passage often lacks coherence, is hard tounderstand, sometimes misleading, and may be justplain incomprehensible.
In fact, some studies show(cf.
(Brandow, Mitze, & Ran 1995)) that simply se-lecting the first paragraph from a document endsto produce better summaries than a sentence-basedalgorithm.A far more difficult, but arguably more "human-like" method to summarize text (with the possi-ble exception of editorial staff of some well-knowndailies) is to comprehend it in its entirety, and thenwrite a summary "in your own words."
What thisamounts to, computationally, is a full linguistic anal-ysis to extract key text components from which asummary could be built.
One previously exploredapproach, e.g., (Ono, Sumita, & Miike 1994) (McK-eown & Radev 1995), was to extract discourse struc-ture elements and then generate the summary withinthis structure.
In another approach, e.g., (DeJong1982) (Lehnert 1981) pre-defined summary tem-plates were filled with text elements obtained usinginformation extraction techniques.
Marcu (Marcu1997a) uses rhetorical structure analysis to guide theselection of text segments for the summary; simi-larly Teufel and Moens (Teufel & Moens 1997) ana-lyze argumentative structure of discourse to extractappropriate sentences.
While these approaches canproduce very good results, they are yet to be demon-strated in a practical system applied to a reasonablesize domain.
The main difficulty is the lack of an effi-cient and reliable method of computing the requireddiscourse structure.Our ApproachThe approach we adopted in our work falls some-where between simple sentence xtraction and text-understanding, although philosophically we arecloser to NYT cut-and-paste editors.
We overcomethe shortcomings of sentence-based summarizationby working on paragraph level instead.
Our sum-marizer is based on taking advantage of paragraphsegmentation and the underlying Discourse MacroStructure of News texts.
Both will be discussed be-low.ParagraphsParagraphs are generally self-contained units, moreso than single sentences, they usually address a sin-gle thought or issue, and their relationships withthe surrounding text are somewhat easier to trace.This notion has been explored by Cornell's group(Salton et al 1994) to design a summarizer thattraces inter-paragraph relationships and selects the"best connected" paragraphs for the summary.
Likein Cornell's system, our summaries are made up ofparagraphs taken out of the original text.
In addi-tion, in order to obtain more coherent summaries,we impose some fundamental discourse constraintson the generation process, but avoid a full discourseanalysis.We would like to note at this point that the sum-marization algorithm, as described in detail later,224does not explicitly depend on nor indeed require in-put text that is pre-segmented into paragraphs.
Ingeneral, any length passages can be used, althoughthis choice will impact the complexity of the solu-tion.
Lifting well-defined paragraphs from a docu-ment and then recombining them into a summaryis relatively more straightforward than recombiningother text units.
For texts where there is no struc-ture at all, as in a closed-captioned stream in broad-cast television, there are several ways to create arti-ficial segments.
The simplest would be to use fixedword-count passages.
Or, content-based segmen-tation techniques may be applicable, e.g., Hearst'sText-Tiling (Hearst 1997).On the other hand, we may argue that essentiallyany length segments of text can be used so longas one could figure out a way to reconnect heminto paragraph-like passages even if their boundarieswere somewhat off.
This is actually not unlike deal-ing with the texts with very fine grained paragraphs,as is often the case with news-wire articles.
Forsuch texts, in order to obtain an appropriate l vel ofchunking, some paragraphs need to be reconnectedinto longer passages.
This may be achieved by track-ing co-references and other text cohesiveness devices,and their choice will depend upon the initial segmen-tation we work up from.D iscourse  Macro  S t ructure  o f  a TextIt has been observed, eg., (Rino & Scott 1994),(Weissberg & Buker 1990), that certain types oftexts, such as news articles, technical reports, re-search papers, etc., conform to a set of styleand organization constraints, called the DiscourseMacro Structure (DMS) which help the author toachieve a desired communication effect.
For in-stance, both physics papers and abstracts alignclosely with the Introduction-Methodology-Results-Discussion-Conclusion macro structure.
It is likelythat other scientific and technical texts will also con-form to this or similar structure, since this is exactlythe structure suggested in technical writing guide-books, e.g.
(Weissberg & Buker 1990).
One obser-vation to make here is that perhaps a proper sum-mary or an abstract should reflect the DMS of theoriginal document.
On the other hand, we need tonote that a summary can be given a different DMS,and this choice would reflect our interpretation ofthe original text.
A scientific paper, for example,can be treated as a piece of news, and serve as abasis of an un-scientific summary.News reports tend to be built hierarchically outof components which fall roughly into one of thetwo categories: the What-Is- The-News category, andthe optional Background category.
The Background,if present, supplies the context necessary to under-stand the central story, or to make a follow-up storyself-contained.
The Background section is optional:when the background is common knowledge or is im-plied in the main news section, it can, and usuallyis omitted.
The What-Is-The-News section coversthe new developments and the new facts that makethe news.
This organization is often reflected in thesummary, as illustrated in the example below fromNYT 10/15/97, where the highlighted portion pro-vides the background for the main news:SPIES JUST WOULDN'T COME IN FROM COLDWAR, FILES SHOWTerry Squillacote was a Pentagon lawyerwho hated her job.
Kurt Stand was a unionleader with an aging beatnik's louch.
Jim Clarkwas a lonely private investigator.
\[A 200-pageaffidavit filed last week by\] the Federal Bureauof Investigation says the three were out-of-workspies for East Germany.
And after that statewithered away, it says, they desperately reachedout for anyone who might want them as secretagents.In this example, the two passages are non-consecutive paragraphs in the original text; thestring in the square brackets at the opening of thesecond passage has been omitted in the summary.Here the human summarizer's actions appear rela-tively straightforward, and it would not be difficultto propose an algorithmic method to do the same.This may go as follows:1.
Choose a DMS template for the summary; e.g.,Background+News.Select appropriate passages from the originM textand fill the DMS template.Assemble the summary in the desired order; deleteextraneous words.It is worth noting here that the background-context passage is critical for understanding of thissummary, but as such provides essentially no rele-vant information except for the names of the peopleinvolved.
Incidentally, this is precisely the informa-tion required to make the summary self-contained, iffor no other reason than to supply the antecedents othe anaphors in the main passage (the three, they).The A lgor i thmThe summarizer can work in two modes: genericand topical.
In the generic mode, it simply sum-marizes the main points of the original document...225In the topical mode, it takes a user supplied state-ment of interest, a topic, and derives a summaryrelated to this topic.
A topical summary is thususually different from the generic summary of thesame document.
The summarizer can produce bothindicative and informative summaries.
An indica-tive summary, typically 5-10% of the original text,is when there is just enough material retained fromthe original document o indicate its content.
Aninformative summary, on the other hand, typically20-30% of the text, retains all the relevant facts thata user may need from the original document, that is,it serves as a condensed surrogate, a digest.The process of assembling DMS components intoa summary depends upon the complexity of the dis-course structure itself.
For news or even for scientifictexts, it may be just a matter of concatenating com-ponents together with a little of "cohesiveness glue",which may include deleting some obstructing sen-tences, expanding acronyms, adjusting verb forms,etc.
In a highly specialized omain (e.g., court rul-ings) the final assembly may be guided by a verydetailed pattern or a script that conforms to specificstyle and content requirements.Below we present a 10-step algorithm for gener-ating summaries of news-like texts.
This is the al-gorithm underlying our current summarizer.
Thereader may notice that there is no explicit provi-sion for dealing with DMS structures here.
Indeed,the basic Background+News summary pattern hasbeen tightly integrated into the passage selectionand weighting process.
This obviously streamlinesthe summarization process, but it also reflects thenotion that news-style summarization is in manyways basic and subsumes other more complex sum-marization requirements.THE GENERALIZED SUMMARIZATION ALGORITHMsO: Segment ext into passages.
Use any availablehandles, including indentation, SGML, emptylines, sentence ends, etc.
If no paragraph orsentence structure is available, use approximatelyequal size chunks.sl :  Build a paragraph-search query out of the contentwords, phrases and other terms found in the title,a user-supplied topic description (if available), aswell as the terms occurring frequently in the text.s2: Reconnect adjacent passages that display strongcohesiveness by one-way background links, usinghandles such as outgoing anaphors and otherbackward references.
A background link from pas-sage N?I to passage Nmeans that if passage N+Is3:s4:s5:s6:is selected for a summary, passage N must also beselected.
Link consecutive passages until all refer-ences are covered.Score all passages, including the linked groupswith respect o the paragraph-search query.
As-sign a point for each co-occurring term.
The goalis to maximize the overlap, so multiple occur-rences of the same term do not increase the score.Normalize passage scores by their length, takinginto account he desired target length of the sum-mary.
The goal is to keep summary length as closeto the target length as possible.
The weightingformula is designed so that small deviations fromthe target length are acceptable, but large devia-tions will rapidly decrease the passage score.
Theexact formulation of this scheme depends uponthe desired tradeoff between summary length andcontent.
The following is the basic formula forscoring passage P of length I against he passage-search query Q and the target summary length oft, as used in current version of our summarizer:NormScore(P, Q) = RawScore(P, Q)+1where:RawSeore(P, Q) = Z weight(q, P) + prem(P)qEQwith sum over unique content erms q, and1 i fqEPweight(q,P) = 0 otherwisewith prem(P) as a cummulative non-contentbased score premium (cf s7).Discard all passages with length in excess of 1.5times the target length.
This reduces the num-ber of passage combinations the summarizer hasto consider, thus improving its efficiency.
The de-cision whether to use this condition depends uponour tolerance to length variability.
In extremecases, to prevent obtaining empty summaries, thesummarizer will default to the first paragraph ofthe original text.Combine passages into groups of 2 or more basedon their content, composition and length.
Thegoal is to maximize the score, while keeping thelength as close to the target length as possible.226Any combination of passages is allowed, includ- (5)ing non-consecutive passages, although the origi-nal ordering of passages i  retained.
If a passage (6)attached to another through a background link isincluded into a group, the other passage must alsobe included, and this rule is applied recursively.We need to note that the background links workonly one way: a passage which is a background for (7)another passage, may stand on its own if selectedinto a candidate summary.S7: Recalculate scores for all newly created groups.This is necessary, and cannot be obtained as asum of scores because of possible term repetitions.Again, discard any passage groups longer than 1.5times the target length.
Add premium scores togroups based on the inverse degree of text dis-continuity measured as a total amount of elidedtext material between the passages within a group.Add other premiums as applicable.s8: Rank passage groups by score.
All groups becomecandidate summaries.s9: Repeat steps s6 through s8 until there is nochange in top-scoring passage group through 2consecutive iterations.
Select the top scoring pas-sage or passage group as the final summary.Imp lementat ion  and  some ExamplesThe summarizer has been implemented in C++ witha Java interface as a demonstration system, primar-ily for news summarization.
At this time it can runin both batch and interactive modes under Solaris,and it can also be accessed via Web using a Javacompatible browser.
Below, we present a few exam-ple summaries.
For an easy orientation paragraphsare numbered in order they appear in the originaltext.TITLE: Mrs. Clinton Says U.S.
Needs 'Ways ThatValue Families'SUMMARY TYPE:  indicativeTARGET LENGTH:  5%TOPIC: none(6) The  Uni ted States,  Mrs. C l inton said, must  become "a nat ionthat  doesn ' t  jus t  ta lk  about  fami ly values but  acts in ways thatvalues famil ies.
"SUMMARY TYPE:  indicativeTARGET LENGTH:  15%TOPIC: Hidden cameras used in news reporting(4) Roone Ar ledge,  the pres ident  of ABC News, defended themethods  used to repor t  the segment  and said ABC would ap-peal  the verdict .
"They  could never contest  he t ru th"  of the broadcast ,  Ar ledgesaid.
"These people were doing awful th ings  in these stores.
"Wednesday 's  verdict  was only the second t ime puni t ive  dam-ages had been meted out by a ju ry  in a h idden-camera  case.
Itwas the first t ime puni t ive damages  had  been awarded aga instproducers  of such a segment ,  said Neville L. Johnson,  a lawyerin Los Angeles who has filed numerous  h idden-camera  casesagainst  he major  networks.Many journa l i s ts  argue that  h idden cameras  and  other  under-cover repor t ing  techniques have long been necessary tools forexposing vital  issues of publ ic  pol icy and  heal th.
But  manymedia  experts  say television producers  have overused them inrecent years in a push to create splashy shows and  bolster  rat-ings.
The jurors ,  those experts  added,  may have been lashingout at what  they  perceived as undisc ip l ined and overly aggres-sive news organizat ions.TITLE: U.S.
Buyer of Russian Uranium Said to PutProfits Before SecuritySUMMARY TYPE:  informativeTARGET LENGTH:  25~TOPIC: nuclear nonproliferation(:)(2)(7)(8)(19)(2o)In a postscr ip t  o the Cold War ,  the Amer ican  government-owned corporat ion  that  is charged with resell ing much of Rus-sia's mi l i tary  stockpi le of u ran ium as civi l ian nuc lear  reactorfuel tu rned down repeated  requests this year  to buy  mater ia lsufficient to bui ld 400 Hiroshima-s ize bombs.The incident raises the quest ion of whether  the corporat ion ,the U.S. Enr ichment  Corp. ,  put  its own f inancial  interest  aheadof the nat iona l -secur i ty  goal of prevent ing weapons-grade  ura-n ium from fal l ing into the hands  of terror ists  or rogue states.The  corporat ion  has thus far taken del ivery f rom Russ ia  ofreactor  fuel der ived f rom 13 tons of bomb-grade  uran ium.
"The nonpro l i ferat ion object ives of the agreement  are beingachieved," a spokesman for the Enr ichment  Corp.  said.But  since the beg inn ing  of the program,  skeptics have ques-t ioned the wisdom of des ignat ing  the Enr ichment  Corp.  asWash ington 's  "execut ive agent"  in manag ing  the deal withRuss ia 's  Min is t ry  of Atomic  Energy,  or M INATOM.Domenici ,  cha i rman of the energy subcommit tee  of the Sen-ate Appropr ia t ions  Commit tee ,  which is shepherd ing  the pri-vat izat ion p lan through Congress,  was never in formed of theoffer by the admin is t ra t ion .
After  learn ing of the rebuff  tothe Russians,  he wrote to Curt is  ask ing that  the Enr ichmentCorp.
"be immediate ly  replaced as execut ive agent"  and warn-ing that  "under  no c i rcumstances  should the sale of the USECproceed unti l  th is  mat ter  is resolved."
Once Domenic i  enteredthe fray, the admin is t ra t ion  changed its tune.Curt is  sent a letter  to Domenic i  s ta t ing  that  all the prob lemsblocking acceptance  of the ext ra  six tons had been solved.
Peo-ple close to the admin is t ra t ion  said that  the Enr ichment  Corp.has now been advised to buy  the full 18-ton sh ipment  in 1997.Moreover, Cur t i s  quickly convened a new commit tee  to moni-tor  the Enr ichment  Corp.  for signs of foot -dragg ing.Eva luat ionOur program has been tested on a variety of news-like documents, including Associated Press news-wire messages, articles from the New York Times,The Wall Street Journal, Financial Times, San JoseMercury, as well as documents from the Federal Reg-ister, and the Congressional Record.
The summa-rizer is domain independent, and it can be easilyadapted to most European languages.
It is also227very robust: we used it to derive summaries ofthousands of documents returned by an informationretrieval system.
Early results from these evalua-tions indicate that the summaries generated usingour DMS method offer an excellent radeoff betweentime/length and accuracy.
Our summaries tend tobe shorter and contain less extraneous material thanthose obtained using different methods.
This is fur-ther confirmed by the favorable responses we re-ceived from the users.Thus far there has been only one systematic multi-site evaluation of summarization approaches, con-ducted in early 1998, organized by U.S. DARPA 1 inthe tradition of Message Understanding Conferences(MUC) (DAR 1993) and Text Retrieval Conferences(TREC) (Harman 1997a), which have proven suc-cessful in stimulating research in their respectiveareas: information extraction and information re-trieval.
The summarization evaluation focused oncontent representativeness of indicative summariesand comprehensiveness of informative summaries.Other factors affecting the quality of summaries,such as brevity, readability, and usefulness were eval-uated indirectly, as parameters of the main scores.For more details see (Firmin & Sundheim 1998).The indicative summaries were scored for rele-vance to pre-selected topics and compared to theclassification of respective full documents.
In thisevaluation, a summary was considered successful if itpreserved the original document's relevance or non-relevance to a topic.
Moreover, the recall and preci-sion scores were normalized by the length of the sum-mary (in words) relative to the length of the originaldocument, as well as by the clock time taken by theevaluators to reach their topic relevance decisions.The first normalization measured the degree of con-tent compression provided by the summaries, whilethe second normalization was intended to gaugetheir readability.
The results showed a strong corre-lation between these two measures, which may indi-cate that readability was in fact equated with mean-ingfulness, that is, hard to read summaries werequickly judged non-relevant.For all the participants the best summaries scoredbetter than the fixed-length summaries.
When nor-malized for length our summarizer had the highestscore for best summaries and took the second placefor fixed-length summaries.
The F-scores for indica-tive topical summaries (best and fixed-length) werevery close for all participants.
Apparently it is easierto generate a topical summary then a general sum-mary.
Normalizing for length did move our score1(The U.S.) Defense Advanced Research ProjectsAgencyup, but again, there was no significant difference be-tween participants.The informative (topical) summaries were scoredfor their ability to provide answers to who, what,when, how, etc.
questions about the topics.
Thesequestions were unknown to the developers, so sys-tems could not directly extract facts to satisfy them.Again, scores were normalized for summary length,but no time normalization was used.
This evalua-tion was done on a significantly smaller scale thanfor the indicative summaries, simply because scor-ing for question answering was more time consumingfor the human judges than categorization decisions.This evaluation could probably be recast as catego-rization problem, if we only assumed that the ques-tions in the test were the topics, and that a summaryneeds to be relevant o multiple topics.Informative summaries were generated using thesame general algorithm with two modifications.First, the expected summary length was set at 30%of the original, following an observation by the con-ference organizers while evaluating human generatedsummaries.
Second, since the completeness of an in-formative summary was judged on the basis of itcontaining satisfactory answers to questions whichwere not part of the topic specification, we addedextra scores to passages containing possible answers:proper names (who, where) and numerics (when,how much).
Finally, we note that the test data usedfor evaluation, while generally of news-like genre,varied greatly in content, style and the subject mat-ter, therefore domain-independence was critical.Again our summarizer performed quite well, al-though the results are less significant since the ex-periment was carried out on such a small scale.
Theresults were separated out for three different queries.For two queries the system was very close to the topperforming system, and for the third query the sys-tem had an F-score of about 0.61 versus 0.77 for thebest system.In general we are quite pleased with the summa-rizer performance, especially since our system wasnot trained on the kind of texts that we had to sum-marize.Re la ted  Work  and  Future  WorkThe current summarizer is still undergoing improve-ment and adaptation in order to be able to sum-marize more than a single text news document ata time.
At the same time we are investigating howsummarization can be used in related but differentproblems.
Both will be described below.228A bet ter  and  more  f lex ib le  summarizerCurrently our summarizer is especially tuned for En-glish one-document text-only news summarization.While we are still working on improving this, we alsowant the system to be able to summarize a wider va-riety of documents.
Many challenges remain, includ-ing summarization of non-news documents, multi-modal documents (such as web pages), foreign lan-guage documents and (small or large) groups of doc-uments covering one or more topics.Typically, a user needs summarization the mostwhen dealing with a large number of documents.Therefore, the next logical step is to summarizemore than one documents at a time.
At the mo-ment we are focusing on multi-document (cross-document) summarization of English text-only newsdocuments.
Just as for single-document summariza-tion, multi-document summarization can be genericor topical and indicative or informative.
Other fac-tors that will influence the types of summary are thenumber of documents (a large versus a small set) andthe variety of topics discussed by the documents (arethe documents closely related or can they cover verydifferent opics).
Presentation of a multi-documentoffers a wide variety of choices.
One could createone large text summary that gives an overview of allthe main issues mentioned in all summaries.
Or per-haps give different short summaries for similar doc-uments.
If the number of documents is very largeit might be best to create nested summaries withhigh-level descriptions and the possibility to 'zoomin' on a subgroup with a more specific summary.
Auser will probably want to have the ability to traceinformation in a summary back to its original doc-ument; source information should be a part of thesummary.
If one views summarization i the con-text of tracking a topic, the main goal of the sum-mary might be to show the new information everynext document contains, while not repeating infor-mation already mentioned in previous documents.Another type of summary might highlight the sim-ilarities documents have (e.g., all these documentsare on protection of endangered species) and point-ing out the differences they have (e.g., one on baldeagles, some on bengal tigers,..).
As one can see,there are many questions to be answered and theanswers depend partially on the task environmentthe summarizer will be used in.Currently we are focussing on summarizing asmall set of text-only documents (around 20) all ona similar topic.
The summary will reflect the mainpoints/topics discussed by the documents.
Topicsdiscussed by more than one document should onlybe mentioned once in the summary together with itsdifferent sources.
When generating the summary wewant to ensure coherence by placing related topicclose to each other.
The main issues we are address-ing is the detection of similar information in order toavoid repetition in the summary and the detectionof related information in order to generated a coher-ent summary.
This work is right now in progress.Our next step will be summarizing large amounts ofsimilar information.Applying summarization to differentprob lemsInformation retrieval (IR) is a task of selecting docu-ments from a database in response to a user's query,and ranking these documents according to relevance.Currently we are investigating the usage of summa-rization in order to build (either automatically orwith the help of the user) more effective informationneed statements for an automated ocument searchsystem.
The premise is quite simple: use the ini-tial user's statement of information eed to samplethe database for documents, ummarize the returneddocuments topically, then add selected summaries tothe initial statement to make it richer and more spe-cific.
Adding appropriate summaries can be eitherdone by the user who reads the summaries or au-tomatically.
Both approaches are described in ourother paper appearing in this volume.The task of tracking a topic consists of identifyingthose information segments in a information streamthat are relevant o a certain topic.
Topic tracking isone of the three main tasks in the TDT (Topic De-tection and Tracking) tasks that we hope to use oursummarizer for.
The information stream consists ofnews, either from a tv broadcast or a radio broad-cast.
Speech from these programs has been recog-nized by a state-of-the-art automatic speech recog-nition system and also transcribed by human tran-scriptionists.
A topic is defined implicitly by a setof training stories that are given to be on this topic.The basic idea behind our approach is simple.
Weuse the training stories to create a set of keywords(the query).
Since we process continuous news theinput is not segmented into paragraphs or any othermeaningful text unit.
Before applying our summa-rizer each story is divided into equal word-size seg-ments.
We summarize very story using our query,and use similarity of the summary to the query todecide whether a story is on topic or not.
We arestill in the process of refining our system and hopeto have our first results soon.
Initial results sug-gest that this is a viable approach.
It is encouragingto notice that the absence of a paragraph structuredoes not prevent he system from generating useful229summaries.CONCLUSIONSWe have developed a method to derive quick-readsummaries from news-like texts using a number ofshallow NLP techniques and simple quantitativemethods.
In our approach, a summary is assem-bled out of passages extracted from the original text,based on a pre-determined Background-News dis-course template.
The result is a very efficient, ro-bust, and portable summarizer that can be appliedto a variety of tasks.
These include brief indica-tive summaries, both generic and topical, as well aslonger informative digests.
Our method has beenshown to produce summaries that offer an excellenttradeoff between text reduction and content preser-vation, as indicated by the results of the government-sponsored formal evaluation.The present version of the summarizer can han-dle most written texts with well-defined paragraphstructure.
While the algorithm is primarily tunedto newspaper-like articles, we believe it can producenews-style summaries for other factual texts, as longas their rhetorical structures are reasonably linear,and no prescribed stylistic organization is expected.For such cases a more advanced iscourse analysiswill be required along with more elaborate DMStemplates.We used the summarizer to build effective searchtopics for an information retrieval system.
Thishas been demonstrated to produce dramatic per-formance improvements in TREC evaluations.
Webelieve that this topic expansion approach will alsoprove useful in searching very large databases whereobtaining a full index may be impractical or impos-sible, and accurate sampling will become critical.Our future development plans will focus on im-proving the quality of the summaries by implement-ing additional passage scoring functions.
Furtherplans include handling more complex DMS's, andadaptation of the summarizer to texts other thannews, as well as to texts written in foreign languages.We plan further experiments with topic expansionwith the goal of achieving a full automation of theprocess while retaining the performance gains.AcknowledgementsWe would like to acknowledge the significant con-tributions to this project from two former membersof our group: Fang Lin and Wang Jin.
This workwas supported in part by the Defense Advanced Re-search Projects Agency under Tipster Phase-3 Con-tract 97-F157200-000 through the Office of Researchand Development.Re ferencesBrandow, R.; Mitze, K.; and Rau, L. 1995.
Automatic onden-sation of electronic publications by sentence selection.
Informa-tion Processing and Management 31(5):675-686.DARPA.
1993.
Proceedings of the 5th Message UnderstandingConference, San Francisco, CA: Morgan Kaufman Publishers.DARPA.
1996.
Tipster Text Phase 2:24 month Conference,Morgan-Kaufmann.DeJong, G. G. 1982.
An overview of the frump system.
In Lehn-ert, W., and Ringle, M., eds., Strategies for Natural LanguageProcessing.
Lawrence Erlbaum, Hillsdale, NJ.Firmin, T., and Sundheim, B.
1998.
Tipster/summac evaluationanalysis.
In Tipster Phase III 18-month Workshop.Harman, D., ed.
1997a.
The 5th Text Retrieval Conference(TREC.5), number 500-253.
National Institute of Standardsand Technology.Hearst, M. 1997.
Texttiling: Segmenting text into multi-paragraph subtopic passages.
Computational Linguistics23(1):33-64.Kupiec, J.; Pedersen, J.; and Chen, F. 1995.
A trainable doc-ument summarizer.
In Conference of the ACM Special InteresGroup on Information Retrieval (SIGIR), 68-73.Lehnert, W. 1981.
Plots units and narrative summarization.Cognitive Science 4:293-331.Luhn, H. 1958.
The automatic reation of l iterature abstracts.IBM Journal 159-165.Marcu, D. 1997a.
From discourse structures to text summaries.In Proceedings of the ACL Workshop on Intelligent, ScallableText Summarization, 82-88.Marcu, D. 1997b.
The rhetorical parsing of natural languagetexts.
In Proceedings of 35th Annual Meetings of the ACL,96-103.McKeown, K., and Radev, D. 1995.
Generating summaries ofmultiple news articles.
In Proceedings of the 8th Annual ACMSIGIR Conference on R~4D in IR.Ono, K.; Sumita, K.; and Miike, S. 1994.
Abstract gener-ation based on rhetorical structure xtraction.
In Proceedingsof the International Conference on Computational Linguisites(COLING-9~), 344-348.Paice, C. 1990.
Constructing l iterature abstracts by computer:techniques and prospects.
Information Processing and Man-agement 26(1):171-186.Rino, L., and Scott, D. 1994.
Content selection in summary gen-eration.
In Third International Conference on the CognitiveScience of Natural Language Processing.Salton, G.; Allan, J.; Buckley, C.; and Singhal, A.
1994.
Au-tomatic analysis, theme generation, and summarization of ma-chine readable texts.
Science 264:1412-1426.Strzalkowski, T., and Wang, J.
1996.
A self-learning universalconcept spotter.
In Proceedings of the 17th International Con-ference on Computational Linguistics (COLING?96), 931-936.Teufel, S., and Moens, M. 1997.
Sentence xtraction as classi-fication task.
In Proceedings of the ACL Workshop on Intelli-gent, Scallable Text Summarization.Weissberg, R., and Buker, S. 1990.
Writing up Research: Ex-perimental Research Report Writing for Student of English.Prentice Hall, Inc.230
