Proceedings of the 4th International Workshop on Semantic Evaluations (SemEval-2007), pages 438?441,Prague, June 2007. c?2007 Association for Computational LinguisticsUSFD: Preliminary Exploration of Featuresand Classifiers for the TempEval-2007 TasksMark HeppleDept of Computer ScienceUniversity of SheffieldRegent Court211 Portobello StreetSheffield S1 4DP, UKhepple@dcs.shef.ac.ukAndrea SetzerDept of Computer ScienceUniversity of SheffieldRegent Court211 Portobello StreetSheffield S1 4DP, UKandrea@dcs.shef.ac.ukRob GaizauskasDept of Computer ScienceUniversity of SheffieldRegent Court211 Portobello StreetSheffield S1 4DP, UKrobertg@dcs.shef.ac.ukAbstractWe describe the Sheffield system usedin TempEval-2007.
Our system takesa machine-learning (ML) based approach,treating temporal relation assignment as asimple classification task and using featureseasily derived from the TempEval data, i.e.which do not require ?deeper?
NLP analy-sis.
We aimed to explore three questions:(1) How well would a ?lite?
approach ofthis kind perform?
(2) Which features con-tribute positively to system performance?
(3) Which ML algorithm is better suited forthe TempEval tasks?
We used the WekaML workbench to facilitate experimentingwith different ML algorithms.
The paper de-scribes our system and supplies preliminaryanswers to the above questions.1 IntroductionThe Sheffield team were involved in TempEval asco-proposers/co-organisers of the task.1 For our par-ticipation in the task, we decided to pursue an ML-based approach, the benefits of which have been ex-plored elsewhere (Boguraev and Ando, 2005; Maniet al, 2006).
For the TempEval tasks, this is easilydone by treating the assignment of temporal relationtypes as a simple classification task, using readilyavailable information for the instance features.
Morespecifically, the features used were ones provided as1We maintained a strict separation between persons assistingin annotation of the test corpus and those involved in systemdevelopment.attributes in the TempEval data annotation for theevents/times being related, plus some additional fea-tures that could be straightforwardly computed fromdocuments, i.e.
without the use of more heavily ?en-gineered?
NLP components.
The aims of this workwere three-fold.
First, we wanted to see whether a?lite?
approach of this kind could yield reasonableperformance, before pursuing possibilities that re-lied on using ?deeper?
NLP analysis methods.
Sec-ondly, we were interested to see which of the fea-tures considered would contribute positively to sys-tem performance.
Thirdly, rather than selecting asingle ML approach (e.g.
one of those currently invogue within NLP), we wanted to look across MLalgorithms to see if any approach was better suitedto the TempEval tasks than any other, and conse-quently we used the Weka workbench (Witten andFrank, 2005) in our ML experiments.In what follows, we will first describe how oursystem was constructed, before going on to discussour main observations around the key aims men-tioned above.
For example, in regard to our ?lite?
ap-proach, we would observe (c.f.
the results reportedin the Task Description paper) that although someother systems scored more highly, the score differ-ences were relatively small.
Regarding features, wefound for example that the system performed betterfor Task A when, surprisingly, the tense attributeof EVENTs was excluded.
Regarding ML algo-rithms, we found not only that there was substantialvariation between the effectiveness of different algo-rithms for assigning relations (as one might expect),but also that there was considerable differences inthe relative effectiveness of algorithms across tasks,438i.e.
so that an algorithm performing well on one task(compared to the alternatives), might perform ratherpoorly on another task.
The paper closes with somecomments about future research directions.2 System DescriptionThe TempEval training and test data is marked upto identify all event and time expressions occurringwithin documents, and also to record the TLINK re-lations that are relevant for each task (except thatTLINK relation types are absent in the test data).These annotations provide additional informationabout these entities in the form of XML attributes,e.g.
for EVENT annotations we find attributes suchas tense, aspect, part-of-speech and so on.Our system consists of a suite of Perl scriptsthat create the input files required for Weka, andhandle its output.
These include firstly an ?ex-traction?
script, which extracts information aboutEVENT, TIMEXs and TLINKs from the data files, andsecondly a ?feature selection/reformatting?
script,which allows the information that is to be suppliedto Weka to be selected, and recasts it into the formatthat Weka requires for its training/test files.
A finalscript takes Weka?s output over the test files and con-nects it back to the original test documents to pro-duce the final output files required for scoring.The information that the first extraction script ex-tracts for each EVENT, TIMEX and TLINK largelycorresponds to attributes/values associated with theannotations of these items in the initial data files(although not all such attributes are of use for ma-chine learning purposes).
In addition, the script de-termines for each EVENT expression whether it isone deemed relevant by the Event Target List (ETL)for Tasks A and B.
This script also maps EVENTsand TIMEXs into sequential order ?
intra-sententialorder for task A and inter-sentential order for taskC.
This information can be used to compute various?order?
features, such as:event-first: do a related EVENT and TIMEX(for Task A) appear with the EVENT before or afterthe TIMEX?adjacent: do a related EVENT and TIMEX (againfor Task A) appear adjacently in the sequence oftemporal entities or not?
(Note that this allows anEVENT and TIMEX to be adjacent if there tokensTaskType Attribute A B CEVENT aspect X X XEVENT polarity X X ?EVENT POS X X XEVENT stem X ?
?EVENT string ?
?
?EVENT class ?
X XEVENT tense ?
X XORDER adjacent X N/A N/AORDER event-first X N/A N/AORDER event-between ?
N/A N/AORDER timex-between ?
N/A N/ATIMEX3 mod X ?
N/ATIMEX3 type X ?
N/ATLINK reltype X X XTable 1: Featuresthat intervene, but not any other temporal entities.
)event-between: for a related EVENT/TIMEXpair, do any other events appear between them?timex-between: for a related EVENT/TIMEXpair, do any other timexes appear between them?Table 1 lists all the features that we tried usingfor any of the three tasks.
Aside from the OR-DER features (as designated in the leftmost col-umn), which were computed as just described, andthe EVENT string feature (which is the literaltagged expression from the text), all other featurescorrespond to annotation attributes.
Note that theTLINKreltype is extracted from the training datato provide the target attribute for training (a dummyvalue is provided for this in test data).The output of the extraction script is converted toa format suitable for use by Weka by a second script.This script also allows a manual selection to be madeas to the features that are included.
For each of thethree tasks, a rough-and-ready process was followedto find a ?good?
set of features for use with thattask, which proceeded as follows.
Firstly, the maxi-mal set of features considered for the task was triedwith a few ML algorithms in Weka (using a 10-foldcross-validation over the training data) to find onethat seemed to work quite well for the task.
Thenusing only that algorithm, we checked whether thestring feature could be dropped (since this fea-439ture?s value set was always of quite high cardinality),i.e.
if its omission improved performance, which forall three tasks was the case.
Next, we tried droppingeach of the remaining features in turn, to identifythose whose exclusion improved performance, andthen for those features so identified, tried droppingthem in combination to arrive at a final ?optimal?
fea-ture set.
Table 1 shows for each of the tasks whichof the features were considered for inclusion (thosemarked N/A were not), and which of these remainedin the final optimal feature set (X).Having determined the set of features for use witheach task, we tried out a range of ML algorithms(again with a 10-fold cross-validation over the train-ing data), to arrive at the final feature-set/ML algo-rithm combination that was used for the task in thecompetitive evaluation.
This was trained over theentire training data and applied to the test data toproduce the final submitted results.3 DiscussionLooking to Table 1, and the features that were con-sidered for each task and then included in the finalset, various observations can be made.
First, notethat the string feature was omitted for all tasks,which is perhaps not surprising, since its values willbe sparsely distributed, so that there will be very fewtraining instances for most of its individual values.However, the stem feature was found to be use-ful for Task A, which can be interpreted as evidencefor a ?lexical effect?
on local event-timex relations,e.g.
perhaps with different verbs displaying differenttrends in how they relate to timexes.
No correspond-ing effects were observed for Tasks B and C.The use of ORDER features for Task A was foundto be useful ?
specifically the features indicatingwhether the event or timex appeared linearly first inthe sentence and whether the two were adjacent ornot.
The more elaborate ORDER features, address-ing more specific cases of what might intervene be-tween the related timex and event expression, werenot found to be helpful.Perhaps the most striking observation to be maderegarding the table is that it was found beneficial toexclude the feature tense for Task A, whilst thefeature aspect was retained.
We have no expla-nation to offer for this result.
Likewise, the eventTaskAlgorithm A B Cbaseline 49.8 62.1 42.0lazy.KStar 58.2 76.7 54.0rules.DecisionTable 53.3 79.0 52.9functions.SMO (svm) 55.1 78.1 55.5rules.JRip 50.7 78.6 53.4bayes.NaiveBayes 56.3 76.2 50.7Table 2: Comparing different algorithms (%-acc.scores, from cross-validation over training data)class feature, which distinguishes e.g.
perceptionvs.
reporting vs. aspectual etc verbs, was excludedfor Task A, although it was retained for Task B.In regard to the use of different ML algorithms forthe classification tasks addressed in TempEval, weobserved considerable variation between algorithmsas to their performance, and this was not unexpected.However, given the seemingly high similarity of thethree tasks, we were rather more surprised to see thatthere was considerable variation between the perfor-mance of algorithms across tasks, i.e.
so that an al-gorithm performing well on one task (compared tothe alternatives), might perform rather poorly on an-other task.
This is illustrated by the results in Table 2for a selected subset of the algorithms considered,which shows %-accuracy scores that were computedby cross-validation over the training data, using thefeature set chosen as ?optimal?
for each task.2 Thealgorithm names in the left-hand column are theones used in WEKA (of which functions.SMOis the WEKA implementation of support-vector ma-chines or SVM).
The first row of results give a ?base-line?
for performance, corresponding to the assign-ment of the most common label for the task.
(Thesewere produced using WEKA?s rules.ZeroR al-gorithm, which does exactly that.
)The best results observed for each task are shownin bold in the table.
These best performing al-gorithms were used for the corresponding tasks inthe competition.
Observe that the lazy.KStar2These scores are computed under the ?strict?
requirementthat key and response labels should be identical.
The TempE-val competition also uses a ?relaxed?
metric which gives par-tial credit when one (or both) label is disjunctive and there is apartial match, e.g.
between labels AFTER and OVERLAP-OR-AFTER.
See (Verhagen et al, 2007) for details.440Task A Task B Task CFS FR FS FR FS FRUSFD 0.59 0.60 0.73 0.74 0.54 0.59ave.
0.56 0.59 0.74 0.75 0.51 0.60max.
0.62 0.64 0.80 0.81 0.55 0.66Table 3: Competition task scores for Sheffield sys-tem (USFD), plus average/max scores across allcompeting systemsmethod, which gives the best performance for TaskA, gives a rather ?middling?
performance for TaskB.
Similarly, the SVM method that gives the bestresults for Task C falls quite a way below the per-formance of KStar on Task A.
A more extremecase is seen with the results for rules.JRip(Weka?s implementation of the RIPPER algorithm),whose score for Task B is close to that of the best-performing system, but which scores only slightlyabove baseline on Task A.The competition scores for our system are givenin Table 3, shown as (harmonic) F-measures underboth strict (FS) and relaxed (FR) metrics (see foot-note 2).
The table also shows the average score foreach task/metric across all systems taking part in thecompetition, as well as the maximum score returnedby any system.
See (Verhagen et al, 2007) for a fulltabulation of results for all systems.34 Future DirectionsSIGNALs and SLINKs are possible candidates asadditional features ?
signals obviously so, whereasthe benefits of exploiting subordination informationare less clear.
Our initial exploratory efforts inthis direction involved pulling information regard-ing SIGNALs and SLINKs across from TimeBank4(Pustejovsky et al, 2003) so as to make this avail-3The TempEval test data identifies precisely the temporalentity pairs to which a relation label must be assigned.
Whena fixed set of items is classified, the scores for precision, recalland F-measure will be identical, being the same as the score forsimple accuracy.
However, not all the participating systems fol-low this pattern of assigning labels to ?all and only?
the entitypairs identified in the test data, i.e.
some systems decide whichentity pairs to label, as well as which label to assign.
Accord-ingly, the performance results given in (Verhagen et al, 2007)are reported using metrics of precision, recall and F-measure.4This was possible because both the trial and training datawere derived from TimeBank.able for use with the TempEval tasks, in the hopethat this would allow us to determine if this informa-tion would be useful without first facing the cost ofdeveloping SIGNAL and SLINK recognisers.
Re-garding SIGNALs, however, we ran into the prob-lem that there are many TLINKs in the TempEvaldata for which no corresponding TLINK appearsin TimeBank, and hence for which SIGNAL infor-mation could not be imported.
We were unable toprogress this work sufficiently in the time availablefor there to be any useful results to report here.5 ConclusionWe have explored using a ML-based approach tothe TempEval tasks, which does not rely on the useof deeper NLP-analysis components.
We observethat although some other systems in the competi-tion have produced higher scores for the tasks, thescore differences are relatively small.
In the courseof this work, we have made some interesting ob-servations regarding the performance variability ofdifferent ML algorithms when applied to the diffentTempEval tasks, and regarding the features that con-tribute to the system?s performance.ReferencesB.
Boguraev and R. Kubota Ando.
2005.
TimeML-Compliant Text Analysis for Temporal Reasoning.
InProceedings of IJCAI-05, pages 997?1003.Inderjeet Mani, Marc Verhagen, Ben Wellner, Chong MinLee, and James Pustejovsky.
2006.
Machine Learningof Temporal Relations.
In ACL ?06: Proceedings ofthe 21st International Conference on ComputationalLinguistics and the 44th annual meeting of the ACL,pages 753?760, Morristown, NJ, USA.
Association forComputational Linguistics.J.
Pustejovsky, D. Day, L. Ferro, R. Gaizauskas, P. Hanks,M.
Lazo, D. Radev, R.
Saur?
?, A.
See, A. Setzer, andB.
Sundheim.
2003.
The TIMEBANK Corpus.
InProceedings of Corpus Linguistics 2003, pages 647?656, Lancaster, March.M.
Verhagen, R. Gaizauskas, F. Schilder, M. Hepple,G.
Katz, and J. Pustejovsky.
2007.
SemEval-2007Task 15: TempEval Temporal Relation Identification.In Proceedings of SemEval-2007: 4th InternationalWorkshop on Semantic Evaluations.I.H.
Witten and E. Frank, editors.
2005.
Data Mining:Practical Machine Learning Tools and Techniques.Morgan Kaufmann, San Francisco, second edition.441
