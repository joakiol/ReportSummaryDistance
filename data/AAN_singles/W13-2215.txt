Proceedings of the Eighth Workshop on Statistical Machine Translation, pages 134?140,Sofia, Bulgaria, August 8-9, 2013 c?2013 Association for Computational LinguisticsThe TALP-UPC Phrase-based Translation Systems for WMT13:System Combination with Morphology Generation,Domain Adaptation and Corpus FilteringLlu?
?s Formiga?, Marta R.
Costa-jussa`?, Jose?
B. Marin?o?Jose?
A. R.
Fonollosa?, Alberto Barro?n-Ceden?o?
?, Llu?
?s Ma`rquez?
?TALP Research Centre ?Facultad de Informa?ticaUniversitat Polite`cnica de Catalunya Universidad Polite?cnica de MadridBarcelona, Spain Madrid, Spain{lluis.formiga,marta.ruiz,jose.marino,jose.fonollosa}@upc.edu{albarron, lluism}@lsi.upc.eduAbstractThis paper describes the TALP participa-tion in the WMT13 evaluation campaign.Our participation is based on the combi-nation of several statistical machine trans-lation systems: based on standard phrase-based Moses systems.
Variations includetechniques such as morphology genera-tion, training sentence filtering, and do-main adaptation through unit derivation.The results show a coherent improvementon TER, METEOR, NIST, and BLEUscores when compared to our baseline sys-tem.1 IntroductionThe TALP-UPC center (Center for Language andSpeech Technologies and Applications at Univer-sitat Polite`cnica de Catalunya) focused on the En-glish to Spanish translation of the WMT13 sharedtask.Our primary (contrastive) run is an internalsystem selection comprised of different train-ing approaches (without CommonCrawl, unlessstated): (a) Moses Baseline (Koehn et al2007b), (b) Moses Baseline + Morphology Gener-ation (Formiga et al 2012b), (c) Moses Baseline+ News Adaptation (Henr?
?quez Q. et al 2011),(d) Moses Baseline + News Adaptation + Mor-phology Generation , and (e) Moses Baseline +News Adaptation + Filtered CommonCrawl Adap-tation (Barro?n-Ceden?o et al 2013).
Our sec-ondary run includes is the full training strategymarked as (e) in the previous description.The main differences with respect to our lastyear?s participation (Formiga et al 2012a) are: i)the inclusion of the CommonCrawl corpus, usinga sentence filtering technique and the system com-bination itself, and ii) a system selection schemeto select the best translation among the differentconfigurations.The paper is organized as follows.
Section 2presents the phrase-based system and the mainpipeline of our baseline system.
Section 3 de-scribes the our approaches to improve the baselinesystem on the English-to-Spanish task (special at-tention is given to the approaches that differ fromlast year).
Section 4 presents the system combi-nation approach once the best candidate phrase ofthe different subsystems are selected.
Section 5discusses the obtained results considering both in-ternal and official test sets.
Section 6 includes con-clusions and further work.2 Baseline system: Phrase-Based SMTOur contribution is a follow up of our last year par-ticipation (Formiga et al 2012a), based on a fac-tored Moses from English to Spanish words plustheir Part-of-Speech (POS).
Factored corpora aug-ments words with additional information, such asPOS tags or lemmas.
In that case, factors otherthan surface (e.g.
POS) are usually less sparse, al-lowing the construction of factor-specific languagemodels with higher-order n-grams.
Such languagemodels can help to obtain syntactically more cor-rect outputs.We used the standard models available in Mosesas feature functions: relative frequencies, lexi-cal weights, word and phrase penalties, wbe-msd-bidirectional-fe reordering models, and two lan-guage models (one for surface and one for POStags).
Phrase scoring was computed using Good-Turing discounting (Foster et al 2006).As aforementioned, we developed five factoredMoses-based independent systems with different134approaches.
We explain them in Section 3.
Asa final decision, we applied a system selectionscheme (Formiga et al 2013; Specia et al 2010)to consider the best candidate for each sentence,according to human trained quality estimation(QE) models.
We set monotone reordering ofthe punctuation signs for the decoding using theMoses wall feature.We tuned the systems using the MosesMERT (Och, 2003) implementation.
Our focuswas on minimizing the BLEU score (Papineni etal., 2002) of the development set.
Still, for ex-ploratory purposes, we tuned configuration (c) us-ing PRO (Hopkins and May, 2011) to set the ini-tial weights at every iteration of the MERT algo-rithm.
However, it showed no significant differ-ences compared to the original MERT implemen-tation.We trained the baseline system using allthe available parallel corpora, except forcommon-crawl.
That is, European Parlia-ment (EPPS) (Koehn, 2005), News Commentary,and United Nations.
Regarding the monolingualdata, there were more News corpora organizedby years for Spanish.
The data is available atthe Translation Task?s website1.
We used allthe News corpora to busld the language model(LM).
Firstly, a LM was built for every corpusindependently.
Afterwards, they were combinedto produce de final LM.For internal testing we used the News 2011 andNews 2012 data and concatenated the remainingthree years of News data as a single parallel corpusfor development.We processed the corpora as in our participa-tion to WMT12 (Formiga et al 2012a).
Tok-enization and POS-tagging in both Spanish andEnglish was obtained with FreeLing (Padro?
et al2010).
Stemming was carried out with Snow-ball (Porter, 2001).
Words were conditionally casefolded based on their POS: proper nouns and ad-jectives were separated from other categories todetermine whether a string should be fully folded(no special property), partially folded (noun or ad-jective) or not folded at all in (acronym).Bilingual corpora was filtered with the clean-corpus-n script of Moses (Koehn et al 2007a), re-moving those pairs in which a sentence was longerthan 70.
For the CommonCrawl corpus we used amore complex filtering step (cf.
Section 3.3).1http://www.statmt.org/wmt13/translation-task.htmlPostprocessing included two special scripts torecover contractions and clitics.
Detruecasing wasdone forcing the capitals after the punctuationsigns.
Furthermore we used an additional script inorder to check the casing of output names with re-spect to the source.
We reused our language mod-els and alignments (with stems) from WMT12.3 Improvement strategiesWe tried three different strategies to improve thebaseline system.
Section 3.1 shows a strategybased on morphology simplification plus genera-tion.
Its aim is dealing with the problems raisedby morphology-rich languages, such as Spanish.Section 3.2 presents a domain?adaptation strategythat consists of deriving new units.
Section 3.3presents an advanced strategy to filter the good bi-sentences from the CommonCrawl corpus, whichmight be useful to perform the domain adaptation.3.1 Morphology generationFollowing the success of our WMT12 participa-tion (Formiga et al 2012a), our first improve-ment is based on the morphology generalizationand generation approach (Formiga et al 2012b).We focus our strategy on simplifying verb formsonly.The approach first translates into Spanish sim-plified forms (de Gispert and Marin?o, 2008).
Thefinal inflected forms are predicted through a mor-phology generation step, based on the shallowand deep-projected linguistic information avail-able from both source and target language sen-tences.Lexical sparseness is a crucial aspect to dealwith for an open-domain robust SMT when trans-lating to morphology-rich languages (e.g.
Span-ish) .
We knew beforehand (Formiga et al 2012b)that morphology generalization is a good methodto deal with generic translations and it providesstability to translations of the training domain.Our morphology prediction (generation) sys-tems are trained with the WMT13 corpora (Eu-roparl, News, and UN) together with noisy data(OpenSubtitles).
This combination helps to obtainbetter translations without compromising the qual-ity of the translation models.
These kind of mor-phology generation systems are trained with a rel-atively short amount of parallel data compared tostandard SMT training corpora.Our main enhancement to this strategy is the135addition of source-projected deep features to thetarget sentence in order to perform the morphol-ogy prediction.
These features are DependencyFeatures and Semantic Role Labelling, obtainedfrom the source sentence through Lund Depen-dency Parser2.
These features are then projectedto the target sentence as explained in (Formiga etal., 2012b).Projected deep features are important to pre-dict the correct verb morphology from clean andfluent text.
However, the projection of deep fea-tures is sentence-fluency sensitive, making it un-reliable when the baseline MT output is poor.
Inother words, the morphology generation strategybecomes more relevant with high-quality MT de-coders, as their output is more fluent, making theshallow and deep features more reliable classifierguides.3.2 Domain Adaptation through pivotderived unitsUsually the WMT Translation Task focuses onadapting a system to a news domain, offering anin-domain parallel corpus to work with.
How-ever this corpus is relatively small compared tothe other corpora.
In our previous participationwe demonstrated the need of performing a moreaggressive domain adaptation strategy.
Our strat-egy was based on using in-domain parallel data toadapt the translation model, but focusing on thedecoding errors that the out-of-domain baselinesystem makes when translating the in-domain cor-pus.The idea is to identify the system mistakes anduse the in-domain data to learn how to correctthem.
To that effect, we interpolate the transla-tion models (phrase and lexical reordering tables)with a new adapted translation model with derivedunits.
We obtained the units identifying the mis-matching parts between the non-adapted transla-tion and the actual reference (Henr?
?quez Q. et al2011).
This derivation approach uses the origi-nal translation as a pivot to find a word-to-wordalignment between the source side and the targetcorrection (word-to-word alignment provided byMoses during decoding).The word-to-word monolingual alignment be-tween output translation target correction was ob-tained combining different probabilities such asi)lexical identity, ii) TER-based alignment links,2http://nlp.cs.lth.se/software/Corpus Sent.
Words Vocab.
avg.len.Original EN 1.48M 29.44M 465.1k 19.90ES 31.6M 459.9k 21.45Filtered EN 0.78M 15.3M 278.0k 19.72ES 16.6M 306.8k 21.37Table 1: Commoncrawl corpora statistics forWMT13 before and after filtering.iii) lexical model probabilities, iv) char-based Lev-enshtein distance between tokens and v) filteringout those alignments from NULL to a stop word(p = ??
).We empirically set the linear interpolationweight as w = 0.60 for the baseline translationmodels and w = 0.40 for the derived units trans-lations models.
We applied the pivot derived unitsstrategy to the News domain and to the filteredCommoncrawl corpus (cf.
Section 5).
The proce-dure to filter out the Commoncrawl corpus is ex-plained next.3.3 CommonCrawl FilteringWe used the CommonCrawl corpus, provided forthe first time by the organization, as an impor-tant source of information for performing aggres-sive domain adaptation.
To decrease the impactof the noise in the corpus, we performed an auto-matic pre-selection of the supposedly more correct(hence useful) sentence pairs: we applied the au-tomatic quality estimation filters developed in thecontext of the FAUST project3.
The filters?
pur-pose is to identify cases in which the post-editionsprovided by casual users really improve over auto-matic translations.The adaptation to the current framework is asfollows.
Example selection is modelled as a bi-nary classification problem.
We consider triples(src, ref , trans), where src and ref stand for thesource-reference sentences in the CommonCrawlcorpus and trans is an automatic translation of thesource, generated by our baseline SMT system.
Atriple is assigned a positive label iff ref is a bet-ter translation from src than trans.
That is, if thetranslation example provided by CommonCrawl isbetter than the output of our baseline SMT system.We used four feature sets to characterize thethree sentences and their relationships: sur-face, back-translation, noise-based and similarity-based.
These features try to capture (a) the simi-larity between the different texts on the basis of3http://www.faust-fp7.eu136diverse measures, (b) the length of the differentsentences (including ratios), and (c) the likelihoodof a source or target text to include noisy text.4Most of them are simple, fast-calculation andlanguage-independent features.
However, back-translation features require that trans and ref areback-translated into the source language.
We didit by using the TALP es-en system from WMT12.Considering these features, we trained lin-ear Support Vector Machines using SVMlight(Joachims, 1999).
Our training collection was theFFF+ corpus, with +500 hundred manually anno-tated instances (Barro?n-Ceden?o et al 2013).
Noadaptation to CommonCrawl was performed.
Togive an idea, classification accuracy over the testpartition of the FFF+ corpus was only moderatelygood (?70%).
However, ranking by classificationscore a fresh set of over 6,000 new examples, andselecting the top ranked 50% examples to enrich astate-of-the-art SMT system, allowed us to signifi-cantly improve translation quality (Barro?n-Ceden?oet al 2013).For WMT13, we applied these classifiers torank the CommonCrawl translation pairs and thenselected the top 53% instances to be processed bythe domain adaptation strategy.
Table 1 displaysthe corpus statistics before and after filtering.4 System CombinationWe approached system combination as a systemselection task.
More concretely, we applied Qual-ity Estimation (QE) models (Specia et al 2010;Formiga et al 2013) to select the highest qual-ity translation at sentence level among the trans-lation candidates obtained by our different strate-gies.
The QE models are trained with humansupervision, making use of no system-dependentfeatures.In a previous study (Formiga et al 2013),we showed the plausibility of building reliablesystem-independent QE models from human an-notations.
This type of task should be addressedwith a pairwise ranking strategy, as it yields bet-ter results than an absolute quality estimation ap-proach (i.e., regression) for system selection.
Wealso found that training the quality estimationmodels from human assessments, instead of au-tomatic reference scores, helped to obtain better4We refer the interested reader to (Barro?n-Ceden?o et al2013) for a detailed description of features, process, and eval-uation.models for system selection for both i) mimickingthe behavior of automatic metrics and ii) learningthe human behavior when ranking different trans-lation candidates.For training the QE models we used the datafrom the WMT13 shared task on quality estima-tion (System Selection Quality Estimation at Sen-tence Level task5), which contains the test setsfrom other WMT campaigns with human assess-ments.
We used five groups of features, namely:i) QuestQE: 17 QE features provided by the Questtoolkit6; ii) AsiyaQE: 26 QE features provided bythe Asiya toolkit for MT evaluation (Gime?nez andMa`rquez, 2010a); iii) LM (and LM-PoS) perplex-ities trained with monolingual data; iv) PR: Clas-sical lexical-based measures -BLEU (Papineni etal., 2002), NIST (Doddington, 2002), and ME-TEOR (Denkowski and Lavie, 2011)- computedwith a pseudo-reference approach, that is, usingthe other system candidates as references (Sori-cut and Echihabi, 2010); and v) PROTHER: Ref-erence based metrics provided by Asiya, includingGTM, ROUGE, PER, TER (Snover et al 2008),and syntax-based evaluation measures also with apseudo-reference approach.We trained a Support Vector Machine ranker bymeans of pairwise comparison using the SVMlighttoolkit (Joachims, 1999), but with the ?-z p?
pa-rameter, which can provide system rankings forall the members of different groups.
The learneralgorithm was run according to the following pa-rameters: linear kernel, expanding the working setby 9 variables at each iteration, for a maximum of50,000 iterations and with a cache size of 100 forkernel evaluations.
The trade-off parameter wasempirically set to 0.001.Table 2 shows the contribution of different fea-ture groups when training the QE models.
Forevaluating performance, we used the Asiya nor-malized linear combination metric ULC (Gime?nezand Ma`rquez, 2010b), which combines BLEU,NIST, and METEOR (with exact, paraphrases andsynonym variants).
Within this scenario, it canbe observed that the quality estimation features(QuestQE and AsiyaQE) did not obtain good re-sults, perhaps because of the high similarity be-tween the test candidates (Moses with differentconfigurations) in contrast to the strong differ-ence between the candidates in training (Moses,5http://www.quest.dcs.shef.ac.uk/wmt13 qe.html6http://www.quest.dcs.shef.ac.uk137Features Asiya ULCWMT?11 WMT?12 AVG WMT?13QuestQE 60.46 60.64 60.55 60.06AsiyaQE 61.04 60.89 60.97 60.29QuestQE+AsiyaQE 60.86 61.07 60.96 60.42LM 60.84 60.63 60.74 60.37QuestQE+AsiyaQE+LM 60.80 60.55 60.67 60.21QuestQE+AsiyaQE+PR 60.97 61.12 61.05 60.54QuestQE+AsiyaQE+PR+PROTHER 61.05 61.19 61.12 60.69PR 61.24 61.08 61.16 61.04PR+PROTHER 61.19 61.16 61.18 60.98PR+PROTHER+LM 61.11 61.29 61.20 61.03QuestQE+AsiyaQE+PR+PROTHER+LM 60.70 60.88 60.79 60.14Table 2: System selection scores (ULC) obtained using QE models trained with different groups offeatures.
Results displayed for WMT11, WMT12 internal tests, their average, and the WMT13 testEN?ES BLEU TERwmt13 Primary 29.5 0.586wmt13 Secondary 29.4 0.586Table 4: Official automatic scores for the WMT13English?Spanish translations.RBMT, Jane, etc.).
On the contrary, the pseudo-reference-based features play a crucial role in theproper performance of the QE model, confirmingthe hypothesis that PR features need a clear dom-inant system to be used as reference.
The PR-based configurations (with and without LM) hadno big differences between them.
We choose thebest AVG result for the final system combination:PR+PROTHER+LM, which it is consistent withthe actual WMT13 evaluated afterwards.5 ResultsEvaluations were performed considering differentquality measures: BLEU, NIST, TER, and ME-TEOR in addition to an informal manual analy-sis.
This manifold of metrics evaluates distinct as-pects of the translation.
We evaluated both overthe WMT11 and WMT12 test sets as internal in-dicators of our systems.
We also give our perfor-mance on the WMT13 test dataset.Table 3 presents the obtained results for thedifferent strategies: (a) Moses Baseline (w/ocommoncrawl) (b) Moses Baseline+MorphologyGeneration (w/o commoncrawl) (c) Moses Base-line+News Adaptation through pivot based align-ment (w/o commoncrawl) (d) Moses Baseline +News Adaptation (b) + Morphology Generation(c) (e) Moses Baseline + News Adaptation (b) +Filtered CommonCrawl Adaptation.The official results are in Table 4.
Our primary(contrastive) run is the system combination strat-egy whereas our secondary run is the full trainingstrategy marked as (e) on the system combination.Our primary system was ranked in the second clus-ter out of ten constrained systems in the officialmanual evaluation.Independent analyzes of the improvementstrategies show that the highest improvementcomes from the CommonCrawl Filtering + Adap-tation strategy (system e).
The second best strat-egy is the combination of the morphology pre-diction system plus the news adaptation system.However, for the WMT12 test the News Adap-tation strategy contributes to main improvementwhereas for the WMT13 this major improvementis achieved with the morphology strategy.
Analyz-ing the distance betweem each test set with respectto the News and CommonCrawl domain to furtherunderstand the behavior of each strategy seems aninteresting future work.
Specifically, for furthercontrasting the difference in the morphology ap-proach, it would be nice to analyze the variation inthe verb inflection forms.
Hypothetically, the per-son or the number of the verb forms used may havea higher tendency to be different in the WMT13test set, implying that our morphology approach isfurther exploited.Regarding the system selection step (internalWMT12 test), the only automatic metric that hasan improvement is TER.
However, TER is one of138EN?ES BLEU NIST TER METEORwmt12 Baseline 32.97 8.27 49.27 49.91wmt12 + Morphology Generation 33.03 8.29 49.02 50.01wmt12 + News Adaptation 33.22 8.31 49.00 50.16wmt12 + News Adaptation + Morphology Generation 33.29 8.32 48.83 50.29wmt12 + News Adaptation + Filtered CommonCrawl Adaptation 33.61 8.35 48.82 50.52wmt12 System Combination 33.43 8.34 48.78 50.44wmt13 Baseline 29.02 7.72 51.92 46.96wmt13 Morphology Generation 29.35 7.73 52.04 47.04wmt13 News Adaptation 29.19 7.74 51.91 47.07wmt13 News Adaptation + Morphology Generation 29.40 7.74 51.96 47.12wmt13 News Adaptation + Filtered CommonCrawl Adaptation 29.47 7.77 51.82 47.22wmt13 System Combination 29.54 7.77 51.76 47.34Table 3: Automatic scores for English?Spanish translations.the most reliable metrics according to human eval-uation.
Regarding the actual WMT13 test, the sys-tem selection step is able to overcome all the auto-matic metrics.6 Conclusions and further workThis paper described the TALP-UPC participa-tion for the English-to-Spanish WMT13 transla-tion task.
We applied the same systems as in lastyear, but enhanced with new techniques: sentencefiltering and system combination.Results showed that both approaches performedbetter than the baseline system, being the sentencefiltering technique the one that most improvementreached in terms of all the automatic quality indi-cators: BLEU, NIST, TER, and METEOR.
Thesystem combination was able to outperform theindependent systems which used morphologicalknowledge and/or domain adaptation techniques.As further work would like to focus on furtheradvancing on the morphology-based techniques.AcknowledgmentsThis work has been supported in part bySpanish Ministerio de Econom?
?a y Competitivi-dad, contract TEC2012-38939-C03-02 as wellas from the European Regional DevelopmentFund (ERDF/FEDER) and the European Commu-nity?s FP7 (2007-2013) program under the fol-lowing grants: 247762 (FAUST, FP7-ICT-2009-4-247762), 29951 (the International OutgoingFellowship Marie Curie Action ?
IMTraP-2011-29951) and 246016 (ERCIM ?Alain Bensoussan?Fellowship).ReferencesAlberto Barro?n-Ceden?o, Llu?
?s Ma`rquez, Carlos A.Henr?
?quez Q, Llu?
?s Formiga, Enrique Romero, andJonathan May.
2013.
Identifying Useful Hu-man Correction Feedback from an On-line MachineTranslation Service.
In Proceedings of the Twenty-Third International Joint Conference on ArtificialIntelligence.
AAAI Press.Adria` de de Gispert and Jose?
B. Marin?o.
2008.
On theimpact of morphology in English to Spanish statis-tical MT.
Speech Communication, 50(11-12):1034?1046.Michael Denkowski and Alon Lavie.
2011.
Meteor1.3: Automatic Metric for Reliable Optimizationand Evaluation of Machine Translation Systems.
InProceedings of the EMNLP 2011 Workshop on Sta-tistical Machine Translation.George Doddington.
2002.
Automatic evaluationof machine translation quality using n-gram co-occurrence statistics.
In Proceedings of the sec-ond international conference on Human LanguageTechnology Research, HLT ?02, pages 138?145, SanFrancisco, CA, USA.
Morgan Kaufmann PublishersInc.Lluis Formiga, Carlos A.
Henr?
?quez Q., AdolfoHerna?ndez, Jose?
B. Marin?o, Enric Monte, and Jose?A.
R. Fonollosa.
2012a.
The TALP-UPC phrase-based translation systems for WMT12: Morphol-ogy simplification and domain adaptation.
In Pro-ceedings of the Seventh Workshop on StatisticalMachine Translation, pages 275?282, Montre?al,Canada, June.
Association for Computational Lin-guistics.Llu?
?s Formiga, Adolfo Herna?ndez, Jose?
B.
Marin?, andEnrique Monte.
2012b.
Improving english tospanish out-of-domain translations by morphologygeneralization and generation.
In Proceedings of139the AMTA Monolingual Machine Translation-2012Workshop.Llu?
?s Formiga, Llu?
?s Ma`rquez, and Jaume Pujantell.2013.
Real-life translation quality estimation for mtsystem selection.
In Proceedings of 14th MachineTranslation Summit (MT Summit), Nice, France,September.
EAMT.George Foster, Roland Kuhn, and Howard Johnson.2006.
Phrasetable smoothing for statistical machinetranslation.
In Proceedings of the 2006 Conferenceon Empirical Methods in Natural Language Pro-cessing, EMNLP ?06, pages 53?61, Stroudsburg,PA, USA.
Association for Computational Linguis-tics.Jesu?s Gime?nez and Llu?
?s Ma`rquez.
2010a.
Asiya:An Open Toolkit for Automatic Machine Translation(Meta-)Evaluation.
The Prague Bulletin of Mathe-matical Linguistics, (94):77?86.Jesu?s Gime?nez and Llu?
?s Ma`rquez.
2010b.
Linguisticmeasures for automatic machine translation evalu-ation.
Machine Translation, 24(3-4):209?240, De-cember.Carlos A.
Henr?
?quez Q., Jose?
B. Marin?o, and Rafael E.Banchs.
2011.
Deriving translation units usingsmall additional corpora.
In Proceedings of the 15thConference of the European Association for Ma-chine Translation.Mark Hopkins and Jonathan May.
2011.
Tuning asranking.
In Proceedings of the 2011 Conference onEmpirical Methods in Natural Language Process-ing, pages 1352?1362, Edinburgh, Scotland, UK.,July.
Association for Computational Linguistics.Thorsten Joachims, 1999.
Advances in Kernel Methods?
Support Vector Learning, chapter Making large-Scale SVM Learning Practical.
MIT Press.P.
Koehn, H. Hoang, A. Birch, C. Callison-Burch,M.
Federico, N. Bertoldi, B. Cowan, W. Shen,C.
Moran, R. Zens, et al2007a.
Moses: Opensource toolkit for statistical machine translation.
InProceedings of the 45th Annual Meeting of the ACLon Interactive Poster and Demonstration Sessions,pages 177?180.
Association for Computational Lin-guistics.Philipp Koehn, Hieu Hoang, Alexandra Birch, ChrisCallison-Burch, Marcello Federico, Nicola Bertoldi,Brooke Cowan, Wade Shen, Christine Moran,Richard Zens, Chris Dyer, Ondrej Bojar, Alexan-dra Constantin, and Evan Herbst.
2007b.
Moses:Open source toolkit for statistical machine transla-tion.
In Proceedings of the 45th Annual Meeting ofthe Association for Computational Linguistics Com-panion Volume Proceedings of the Demo and PosterSessions, pages 177?180, Prague, Czech Republic,June.
Association for Computational Linguistics.Philipp Koehn.
2005.
Europarl: A Parallel Corpus forStatistical Machine Translation.
In Machine Trans-lation Summit.Franz J. Och.
2003.
Minimum error rate training instatistical machine translation.
In Proceedings ofthe Annual Meeting of the Association for Compu-tational Linguistics (ACL).Llu?
?s Padro?, Miquel Collado, Samuel Reese, MarinaLloberes, and Irene Castello?n.
2010.
Freeling2.1: Five years of open-source language processingtools.
In Proceedings of 7th Language Resourcesand Evaluation Conference (LREC 2010), La Val-letta, MALTA, May.
ELRA.Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu.
2002.
BLEU: a method for automaticevaluation of machine translation.
In Proceedingsof the Annual Meeting of the Association for Com-putational Linguistics (ACL).M.
Porter.
2001.
Snowball: A language for stemmingalgorithms.Matthew Snover, Bonnie Dorr, and Richard Schwartz.2008.
Language and Translation Model Adaptationusing Comparable Corpora.
In Proceedings of the2008 Conference on Empirical Methods in NaturalLanguage Processing.Radu Soricut and Abdessamad Echihabi.
2010.Trustrank: Inducing trust in automatic translationsvia ranking.
In Proceedings of the 48th AnnualMeeting of the Association for Computational Lin-guistics, pages 612?621, Uppsala, Sweden, July.
As-sociation for Computational Linguistics.Lucia Specia, Dhwaj Raj, and Marco Turchi.
2010.Machine Translation Evaluation Versus Quality Es-timation.
Machine Translation, 24:39?50, March.140
