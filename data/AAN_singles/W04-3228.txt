Dependencies vs.
Constituents for Tree-Based AlignmentDaniel GildeaComputer Science DepartmentUniversity of RochesterRochester, NY 14627AbstractGiven a parallel parsed corpus, statistical tree-to-tree alignment attempts to match nodes inthe syntactic trees for a given sentence intwo languages.
We train a probabilistic treetransduction model on a large automaticallyparsed Chinese-English corpus, and evaluate re-sults against human-annotated word level align-ments.
We find that a constituent-based modelperforms better than a similar probability modeltrained on the same trees converted to a depen-dency representation.1 IntroductionStatistical approaches to machine translation, pio-neered by Brown et al (1990), estimate parame-ters for a probabilistic model of word-to-word cor-respondences and word re-orderings directly fromlarge corpora of parallel bilingual text.
In re-cent years, a number of syntactically motivated ap-proaches to statistical machine translation have beenproposed.
These approaches assign a parallel treestructure to the two sides of each sentence pair, andmodel the translation process with reordering oper-ations defined on the tree structure.
The tree-basedapproach allows us to represent the fact that syn-tactic constituents tend to move as unit, as well assystematic differences in word order in the gram-mars of the two languages.
Furthermore, the treestructure allows us to make probabilistic indepen-dence assumptions that result in polynomial timealgorithms for estimating a translation model fromparallel training data, and for finding the highestprobability translation given a new sentence.Wu (1997) modeled the reordering process withbinary branching trees, where each productioncould be either in the same or in reverse order goingfrom source to target language.
The trees of Wu?sInversion Transduction Grammar were derived bysynchronously parsing a parallel corpus, using agrammar with lexical translation probabilities at theleaves and a simple grammar with a single nonter-minal providing the tree structure.
While this gram-mar did not represent traditional syntactic categoriessuch as verb phrases and noun phrases, it served torestrict the word-level alignments considered by thesystem to those allowable by reordering operationson binary trees.Yamada and Knight (2001) present an algorithmfor estimating probabilistic parameters for a simi-lar model which represents translation as a sequenceof re-ordering operations over children of nodes ina syntactic tree, using automatic parser output forthe initial tree structures.
This gives the translationmodel more information about the structure of thesource language, and further constrains the reorder-ings to match not just a possible bracketing as in Wu(1997), but the specific bracketing of the parse treeprovided.Recent models of alignment have attempted toexploit syntactic information from both languagesby aligning a pair of parse trees for the same sen-tence in either language node by node.
Eisner(2003) presented such a system for transformingsemantic-level dependecy trees into syntactic-leveldependency trees for text generation.
Gildea (2003)trained a system on parallel constituent trees fromthe Korean-English Treebank, evaluating agreementwith hand-annotated word alignments.
Ding andPalmer (2004) align parallel dependency trees witha divide and conquer strategy, choosing a highlylikely word-pair as a splitting point in each tree.
Inaddition to providing a deeper level of representa-tion for the transformations of the translation modelto work with, tree-to-tree models have the advan-tage that they are much less computationally costlyto train than models which must induce tree struc-ture on one or both sides of the translation pair.Because Expectation Maximization for tree-to-treemodels iterates over pairs of nodes in the two trees,it is O(n2) in the sentence length, rather than O(n6)for Wu?s Inversion Transduction Grammar or O(n4)for the Yamada and Knight tree-to-string model.In this paper, we make a comparison of two tree-to-tree models, one trained on the trees produced byautomatic parsers for both our English and Chinesecorpora, and one trained on the same parser outputconverted to a dependency representation.
The treesare converted using a set of deterministic head rulesfor each language.
The dependency representationequalizes some differences in the annotation stylebetween the English and Chinese treebanks.
How-ever, the dependency representation makes the as-sumption that not only the bracketing structure, butalso the head word choices, will correspond in thetwo trees.
Our evaluation is in terms of agreementwith word-level alignments created by bilingual hu-man annotators.
Our model of alignment is that ofGildea (2003), reviewed in Section 2 and extendedto dependency trees in Section 3.
We describe ourdata and experiments in Section 4, and discuss re-sults in Section 5.2 The Tree-to-Tree ModelA tree-to-tree alignment model has tree transforma-tion operations for reordering a node?s children, in-serting and deleting nodes, and translating individ-ual words at the leaves of the parse trees.
The trans-formed tree must not only match the surface stringof the target language, but also the tree structure as-signed to the string by the parser.
In order to pro-vide enough flexibility to make this possible, treetransformation operations allow a single node in thesource tree to produce two nodes in the target tree,or two nodes in the source tree to be grouped to-gether and produce a single node in the target tree.The model can be thought of as a synchronous treesubstitution grammar, with probabilities parameter-ized to generate the target tree conditioned on thestructure of the source tree.The probability P (Tb|Ta) of transforming thesource tree Ta into target tree Tb is modeled in asequence of steps proceeding from the root of thetarget tree down.
At each level of the tree:1.
At most one of the current node?s children isgrouped with the current node in a single ele-mentary tree, with probability Pelem(ta|?a ?children(?a)), conditioned on the currentnode ?a and its children (ie the CFG produc-tion expanding ?a).2.
An alignment of the children of the currentelementary tree is chosen, with probabilityPalign(?|?a ?
children(ta)).
This alignmentoperation is similar to the re-order operationin the tree-to-string model, with the extensionthat 1) the alignment ?
can include insertionsand deletions of individual children, as nodesin either the source or target may not corre-spond to anything on the other side, and 2) inthe case where two nodes have been groupedinto ta, their children are re-ordered togetherin one step.In the final step of the process, as in the tree-to-string model, lexical items at the leaves of the treeare translated into the target language according toa distribution Pt(f |e).Allowing non-1-to-1 correspondences betweennodes in the two trees is necessary to handle thefact that the depth of corresponding words in thetwo trees often differs.
A further consequence ofallowing elementary trees of size one or two is thatsome reorderings not allowed when reordering thechildren of each individual node separately are nowpossible.
For example, with our simple treeABX YZif nodes A and B are considered as one elementarytree, with probability Pelem(ta|A ?
BZ), their col-lective children will be reordered with probabilityPalign({(1, 1)(2, 3)(3, 2)}|A ?
XYZ)AX Z Ygiving the desired word ordering XZY.
However,computational complexity as well as data sparsityprevent us from considering arbitrarily large ele-mentary trees, and the number of nodes consideredat once still limits the possible alignments.
For ex-ample, with our maximum of two nodes, no trans-formation of the treeABW XCY Zis capable of generating the alignment WYXZ.In order to generate the complete target tree, onemore step is necessary to choose the structure on thetarget side, specifically whether the elementary treehas one or two nodes, what labels the nodes have,and, if there are two nodes, whether each child at-taches to the first or the second.
Because we areOperation Parameterizationelementary tree grouping Pelem(ta|?a ?
children(?a))re-order Palign(?|?a ?
children(ta))insertion ?
can include ?insertion?
symbollexical translation Pt(f |e)cloning Pmakeclone(?)?
can include ?clone?
symbolTable 1: The probabilistic tree-to-tree modelultimately interested in predicting the correct targetstring, regardless of its structure, we do not assignprobabilities to these steps.
The nonterminals on thetarget side are ignored entirely, and while the align-ment algorithm considers possible pairs of nodes aselementary trees on the target side during training,the generative probability model should be thoughtof as only generating single nodes on the target side.Thus, the alignment algorithm is constrained by thebracketing on the target side, but does not generatethe entire target tree structure.While the probability model for tree transforma-tion operates from the top of the tree down, proba-bility estimation for aligning two trees takes placeby iterating through pairs of nodes from each tree inbottom-up order, as sketched below:for all nodes ?a in source tree Ta in bottom-up orderdofor all elementary trees ta rooted in ?a dofor all nodes ?b in target tree Tb in bottom-up or-der dofor all elementary trees tb rooted in ?b dofor all alignments ?
of the children of ta andtb do?
(?a, ?b) +=Pelem(ta|?a)Palign(?|?i)?(i,j)??
?
(?i, ?j)end forend forend forend forend forThe outer two loops, iterating over nodes in eachtree, require O(|T |2).
Because we restrict our ele-mentary trees to include at most one child of the rootnode on either side, choosing elementary trees for anode pair is O(m2), where m refers to the maxi-mum number of children of a node.
Computing thealignment between the 2m children of the elemen-tary tree on either side requires choosing which sub-set of source nodes to delete, O(22m), which sub-set of target nodes to insert (or clone), O(22m), andhow to reorder the remaining nodes from source totarget tree, O((2m)!).
Thus overall complexity ofthe algorithm is O(|T |2m242m(2m)!
), quadratic inthe size of the input sentences, but exponential inthe fan-out of the grammar.2.1 Clone OperationBoth our constituent and dependency models makeuse of the ?clone?
operation introduced by Gildea(2003), which allows words to be aligned evenin cases of radically mismatched trees, at a costin the probability of the alignment.
Allowing m-to-n matching of up to two nodes on either sideof the parallel treebank allows for limited non-isomorphism between the trees.
However, evengiven this flexibility, requiring alignments to matchtwo input trees rather than one often makes tree-to-tree alignment more constrained than tree-to-stringalignment.
For example, even alignments with nochange in word order may not be possible if thestructures of the two trees are radically mismatched.Thus, it is helpful to allow departures from the con-straints of the parallel bracketing, if it can be donein without dramatically increasing computationalcomplexity.The clone operation allows a copy of a node fromthe source tree to be made anywhere in the targettree.
After the clone operation takes place, the trans-formation of source into target tree takes place usingthe tree decomposition and subtree alignment oper-ations as before.
The basic algorithm of the previ-ous section remains unchanged, with the exceptionthat the alignments ?
between children of two ele-mentary trees can now include cloned, as well as in-serted, nodes on the target side.
Given that ?
speci-fies a new cloned node as a child of ?j , the choice ofwhich node to clone is made as in the tree-to-stringmodel:Pclone(?i|clone ?
?)
=Pmakeclone(?i)?k Pmakeclone(?k)Because a node from the source tree is cloned withequal probability regardless of whether it has al-ready been ?used?
or not, the probability of a cloneoperation can be computed under the same dynamicprogramming assumptions as the basic tree-to-treemodel.
As with the tree-to-string cloning operation,this independence assumption is essential to keepthe complexity polynomial in the size of the inputsentences.3 Dependency Tree-to-Tree AlignmentsDependencies were found to be more consistentthan constituent structure between French and En-glish by Fox (2002), though this study used a treerepresentation on the English side only.
We wish toinvestigate whether dependency trees are also moresuited to tree-to-tree alignment.Figure 1 shows a typical Xinhua newswire sen-tence with the Chinese parser output, and the sen-tence?s English translation with its parse tree.
Theconversion to dependency representation is shownbelow the original parse trees.Examination of the trees shows both cases wherethe dependency representation is more similaracross the two languages, as well as its potentialpitfalls.
The initial noun phrase, ?14 Chinese openborder cities?
has two subphrases with a level ofconstituent structure (the QP and the lower NP)not found in the English parse.
In this case, thedifference in constituent structure derives primar-ily from differences in the annotation style betweenthe original English and Chinese treebanks (Marcuset al, 1993; Xue and Xia, 2000; Levy and Man-ning, 2003).
These differences disappear in the con-stituent representation.
In general, the number oflevels of constituent structure in a tree can be rela-tively arbitrary, while it is easier for people (whetherprofessional syntacticians or not) to agree on theword-to-word dependencies.In some cases, differences in the number of levelmay be handled by the tree-to-tree model, for ex-ample by grouping the subject NP and its base NPchild together as a single elementary tree.
How-ever, this introduces unnecessary variability into thealignment process.
In cases with large differencein the depths of the two trees, the aligner may notbe able to align the corresponding terminal nodesbecause only one merge is possible at each level.In this case the aligner will clone the subtree, at aneven greater cost in probability.The rest of our example sentence, however,shows cases where the conversion to dependencystructure can in some cases exacerbate differencesin constituent structure.
For example, jingji andjianshe are sisters in the original constituent struc-ture, as are their English translations, economic andconstruction.
In the conversion to Chinese depen-dency structure, they remain sisters both dependenton the noun chengjiu (achievements) while in En-glish, economic is a child of construction.
Thecorrespondence of a three-noun compound in Chi-nese to a noun modified by prepositional phraseand an adjective-noun relation in English means thatthe conversion rules select different heads even forpieces of tree that are locally similar.3.1 The Dependency Alignment ModelWhile the basic tree-to-tree alignment algorithm isthe same for dependency trees, a few modificationsto the probability model are necessary.First, the lexical translation operation takes placeat each node in the tree, rather than only at theleaves.
Lexical translation probabilities are main-tained for each word pair as before, and the lexicaltranslation probabilities are included in the align-ment cost for each elementary tree.
When both el-ementary trees contain two words, either alignmentis possible between the two.
The direct alignmentbetween nodes within the elementary tree has prob-ability 1?Pswap.
A new parameter Pswap gives theprobability of the upper node in the elementary treein English corresponding to the lower node in Chi-nese, and vice versa.
Thus, the probability for thefollowing transformation:ABX Y?
B?A?X Yis factored as Pelem(AB|A?B) Pswap Pt(A?|A)Pt(B?|B) Palign({(1, 1)(2, 2)}|A ?
XY ).Our model does not represent the position of thehead among its children.
While this choice wouldhave to be made in generating MT output, for thepurposes of alignment we simply score how manytree nodes are correctly aligned, without flatteningour trees into a string.We further extended the tree-to-tree alignment al-gorithm by conditioning the reordering of a node?schildren on the node?s lexical item as well as its syn-tactic category at the categories of its children.
Thelexicalized reordering probabilities were smoothedwith the nonlexicalized probabilities (which arethemselves smoothed with a uniform distribution).We smooth using a linear interpolation of lexical-ized and unlexicalized probabilities, with weightsproportional to the number of observations for eachtype of event.4 ExperimentsWe trained our translation models on a parallelcorpus of Chinese-English newswire text.
We re-IPNPNPNRZhongguoQPCDshisiCLPMgeNPNNbianjingNNkaifangNNchengshiNPNNjingjiNNjiansheNNchengjiuVPVVxianzhuSNPCD14NNPChineseJJopenNNborderNNScitiesVPVBPmakeNPNPJJsignificantNNSachievementsPPINinNPJJeconomicNNconstructionVV:xianzhuNN:chengshiNR:Zhongguo CD:shisiM:geNN:bianjing NN:kaifangNN:chengjiuNN:jingji NN:jiansheVV:makeNNS:citiesCD:14 NNP:Chinese JJ:open NN:borderNNS:achievementsJJ:significant IN:inNN:constructionJJ:economicFigure 1: Constituent and dependency trees for a sample sentenceAlignmentPrecision Recall Error RateIBM Model 1 .56 .42 .52IBM Model 4 .67 .43 .47Constituent Tree-to-Tree .51 .48 .50Dependency Tree-to-Tree .44 .38 .60Dependency, lexicalized reordering .41 .37 .61Table 2: Alignment results on Chinese-English corpus.
Higher precision and recall correspond to loweralignment error rate.stricted ourselves to sentences of no more than 25words in either language, resulting in a training cor-pus of 18,773 sentence pairs with a total of 276,113Chinese words and 315,415 English words.
TheChinese data were automatically segmented into to-kens, and English capitalization was retained.
Wereplace words occurring only once with an unknownword token, resulting in a Chinese vocabulary of23,783 words and an English vocabulary of 27,075words.
Chinese data was parsed using the parserof Bikel (2002), and English data was parsed us-ing Collins (1999).
Our hand-aligned test data werethose used in Hwa et al (2002), and consisted of 48sentence pairs also with less than 25 words in eitherlanguage, for a total of 788 English words and 580Chinese words.
The hand aligned data consisted of745 individual aligned word pairs.
Words could bealigned one-to-many in either direction.
This limitsthe performance achievable by our models; the IBMmodels allow one-to-many alignments in one direc-tion only, while the tree-based models allow onlyone-to-one alignment unless the cloning operationis used.
A separate set of 49 hand-aligned sentencepairs was used to control overfitting in training ourmodels.We evaluate our translation models in terms ofagreement with human-annotated word-level align-ments between the sentence pairs.
For scoringthe viterbi alignments of each system against gold-standard annotated alignments, we use the align-ment error rate (AER) of Och and Ney (2000),which measures agreement at the level of pairs ofwords:1AER = 1 ?
2|A ?
G||A| + |G|where A is the set of word pairs aligned by the auto-matic system, and G the set algned in the gold stan-dard.
For a better understanding of how the models1While Och and Ney (2000) differentiate between sure andpossible hand-annotated alignments, our gold standard align-ments come in only one variety.differ, we break this figure down into precision:P = |A ?
G||A|and recall:R = |A ?
G||G|Since none of the systems presented in this com-parison make use of hand-aligned data, they maydiffer in the overall proportion of words that arealigned, rather than inserted or deleted.
This affectsthe precision/recall tradeoff; better results with re-spect to human alignments may be possible by ad-justing an overall insertion probability in order tooptimize AER.Table 2 provides a comparison of results using thetree-based models with the word-level IBM models.IBM Models 1 and 4 refer to Brown et al (1993).We used the GIZA++ package, including the HMMmodel of Och and Ney (2000).
We trained eachmodel until AER began to increase on our held-outcross validation data, resulting in running Model 1for three iterations, then the HMM model for threeiterations, and finally Model 4 for two iterations(the optimal number of iterations for Models 2 and3 was zero).
?Constituent Tree-to-Tree?
indicatesthe model of Section 2 trained and tested directlyon the trees output by the parser, while ?Depen-dency Tree-to-Tree?
makes the modifications to themodel described in Section 3.
For reasons of com-putational efficiency, our constituent-based trainingprocedure skipped sentences for which either treehad a node with more than five children, and thedependency-based training skipped trees with morethan six children.
Thus, the tree-based models wereeffectively trained on less data than IBM Model 4:11,422 out of 18,773 sentence pairs for the con-stituent model and 10,662 sentence pairs for the de-pendency model.
Our tree-based models were ini-tialized with lexical translation probabilities trainedusing IBM Model 1, and uniform probabilities forthe tree reordering operations.
The models weretrained until AER began to rise on our held-outcross-validation data, though in practice AER wasnearly constant for both tree-based models after thefirst iteration.5 DiscussionThe constituent-based version of the alignmentmodel significantly outperforms the dependency-based model.
The IBM models outperform the con-stituent tree-to-tree model to a lesser degree, withtree-to-tree achieving higher recall, and IBM higherprecision.
It is particularly significant that the tree-based model gets higher recall than the other mod-els, since it is limited to one-to-one alignments un-less the clone operation is used, bounding the recallit can achieve.In order to better understand the differences be-tween the constituent and dependency representa-tions of our data, we analyzed how well the tworepresentations match our hand annotated alignmentdata.
We looked for consistently aligned pairs ofconstituents in the two parse trees.
By consistentlyaligned, we mean that all words within the Englishconstituent are aligned to words inside the Chineseconstituent (if they are aligned to anything), andvice versa.
In our example in Figure 1, the NP ?14Chinese border cities?
and the Chinese subject NP?Zhongguo shisi ge bianjing kaifang chengshi?
areconsistenly aligned, but the PP ?in economic con-struction?
has no consistently aligned constituent inthe Chinese sentence.
We found that of the 2623constituents in our English parse trees (not count-ing unary consituents, which have the same bound-aries as their children), for 1044, or 40%, there ex-ists some constituent in the Chinese parse tree thatis consistently aligned.
This confirms the results ofFox (2002) and Galley et al (2004) that many trans-lation operations must span more than one parse treenode.
For each of our consistently aligned pairs, wethen found the head word of both the Chinese andEnglish constituents according to our head rules.The two head words correspond in the annotatedalignments 67% of the time (700 out of 1044 con-sistently aligned constituent pairs).
While the head-swapping operation of our translation model will beable to handle some cases of differing heads, it canonly do so if the two heads are adjacent in both treestructures.Our system is trained and test on automaticallygenerated parse trees, which may contribute to themismatches in the tree structures.
As our testdata was taken from the Chinese Treebank, hand-annotated parse trees were available for the Chinese,but not the English, sentences.
Running the analy-sis on hand-annotated Chinese trees found slightlybetter English/Chinese agreement overall, but therewere still disagreements in the head words choicesfor a third of all consistently aligned constuent pairs.Running our alignment system on gold standardtrees did not improve results.
The comparison be-tween parser output and gold standard trees is sum-marized in Table 3.We used head rules developed for statisticalparsers in both languages, but other rules may bebetter suited to the alignment task.
For example,the tensed auxiliary verb is considered the head ofEnglish progressive and perfect verb phrases, ratherthan the present or past particple of the main verb.Such auxiliaries carry agreement information rele-vant to parsing, but generally have no counterpart inChinese.
A semantically oriented dependency struc-ture, such as Tree Adjoining Grammar derivationtrees, may be more appropriate for alignment.6 ConclusionWe present a comparison of constituent and de-pendency models for tree-to-tree alignment.
De-spite equalizing some mismatches in tree structure,the dependency representation does not perform aswell, likely because it is less robust to large differ-ences between the tree structures.Acknowledgments We are very grateful to Re-becca Hwa, Hao Zhang, everyone at the 2003 JohnHopkins speech and language summer researchworkshop, and EMNLP?s reviewers for their assis-tance, criticism, and data.
This work was partiallysupported by NSF ITR IIS-09325646, NSF researchinfrastructure grant EIA-0080124, and NSF grant0121285 to the summer workshop.ReferencesDaniel M. Bikel.
2002.
Design of a multi-lingual,parallel-processing statistical parsing engine.
InProceedings ARPA Workshop on Human Lan-guage Technology.Peter F. Brown, John Cocke, Stephen A. DellaPietra, Vincent J. Della Pietra, Frederick Je-linek, John D. Lafferty, Robert L. Mercer, andPaul S. Roossin.
1990.
A statistical approach tomachine translation.
Computational Linguistics,16(2):79?85, June.Peter F. Brown, Stephen A. Della Pietra, Vincent J.Della Pietra, and Robert L. Mercer.
1993.
Themathematics of statistical machine translation:Parameter estimation.
Computational Linguis-tics, 19(2):263?311.Michael John Collins.
1999.
Head-driven Statisti-Chinese Parse TreesAutomatic TreebankProportion of English constits w/ consistently aligned Chinese constit .40 .42Proportion of above with heads words aligned .67 .66Constituent-Based AER .50 .51Dependency-Based AER .60 .62Table 3: Comparison of automatically generated and hand-annotated Chinese parse trees.cal Models for Natural Language Parsing.
Ph.D.thesis, University of Pennsylvania, Philadelphia.Yuan Ding and Martha Palmer.
2004.
Automaticlearning of parallel dependency treelet pairs.
InThe First International Joint Conference on Nat-ural Language Processing (IJCNLP).Jason Eisner.
2003.
Learning non-isomorphic treemappings for machine translation.
In Proceed-ings of the 41st Meeting of the Association forComputational Linguistics, companion volume,Sapporo, Japan.Heidi J.
Fox.
2002.
Phrasal cohesion and statisti-cal machine translation.
In In Proceedings of the2002 Conference on Empirical Methods in Natu-ral Language Processing (EMNLP 2002), pages304?311.Michel Galley, Mark Hopkins, Kevin Knight, andDaniel Marcu.
2004.
What?s in a translationrule?
In Proceedings of the Human LanguageTechnology Conference/North American Chapterof the Association for Computational Linguistics(HLT/NAACL).Daniel Gildea.
2003.
Loosely tree-based alignmentfor machine translation.
In Proceedings of the41th Annual Conference of the Association forComputational Linguistics (ACL-03), pages 80?87, Sapporo, Japan.Rebecca Hwa, Philip Resnik, Amy Weinberg, andOkan Kolak.
2002.
Evaluating translational cor-respondence using annotation projection.
In Pro-ceedings of the 40th Annual Conference of theAssociation for Computational Linguistics (ACL-02).Roger Levy and Christopher Manning.
2003.
Isit harder to parse Chinese, or the Chinese Tree-bank?
In Proceedings of the 41th Annual Con-ference of the Association for Computational Lin-guistics (ACL-03), Sapporo, Japan.Mitchell P. Marcus, Beatrice Santorini, andMary Ann Marcinkiewicz.
1993.
Building alarge annotated corpus of English: The Penntreebank.
Computational Linguistics, 19(2):313?330, June.Franz Josef Och and Hermann Ney.
2000.
Im-proved statistical alignment models.
In Proceed-ings of the 38th Annual Conference of the Asso-ciation for Computational Linguistics (ACL-00),pages 440?447, Hong Kong, October.Dekai Wu.
1997.
Stochastic inversion transductiongrammars and bilingual parsing of parallel cor-pora.
Computational Linguistics, 23(3):377?403.Nianwen Xue and Fei Xia.
2000.
The bracketingguidelines for the penn chinese treebank.
Tech-nical Report IRCS-00-08, IRCS, University ofPennsylvania.Kenji Yamada and Kevin Knight.
2001.
A syntax-based statistical translation model.
In Proceed-ings of the 39th Annual Conference of the Asso-ciation for Computational Linguistics (ACL-01),Toulouse, France.
