Bootstrapping Statistical Processinginto a Rule-based Natural Language ParserStephen D. RichardsonMicrosoft ResearchOne Microsoft WayRedmond, WA 98052AbstractThis paper describes a "bootstrapping" methodwhich uses a broad-coverage, rule-based parser tocompute probabilities while parsing an untaggedcorpus of NL text, and which then incorporatesthose probabilities into the processing of the sameparser as it analyzes new text.
Results are reportedwhich show that this method can significantlyimprove the speed and accuracy of the parserwithout requiring the use of annotated corpora orhuman-supervised training during the computationof probabilities.1 IntroductionFor decades, the majority of NL parsers have been"rule-based."
In such parsers, knowledge about hesyntactic structure of a language is written in theform of linguistic rules, and these rules are appliedby the parser to input text segments in order toproduce the resulting parse trees.
Informationabout individual words, such as what parts-of-speech they may be, is usually stored in an onlinedictionary, or "lexicon," which is accessed by theparser for each word in the input text prior toapplying the linguistic rules.Although rule-based parsers are widely-used inreal, working NLP systems, they have thedisadvantage that extensive amounts of (dictionary)data and labor (to write the rules) by highly-skilledlinguists are required in order to create, enhance,and maintain them.
This is especially true if theparser is required to have "broad coverage", i.e., ifit is to be able to parse NL text from manydifferent domains (what one might call '!general"text).In the last few years, there has been increasingactivity in the computational linguistics communityfocused on making use of statistical methods toacquire information from large corpora of NL text,and on using that information in statistical NLparsers.
Instead of being stored in the traditionalform of dictionary data and grammatical rules,linguistic knowledge in these parsers is representedas statistical parameters, or probabilities.
Theseprobabilities are commonly used together withsimpler, less specified, dictionary data and/or ules,thereby taking the place of much of theinformation created by sldlled labor in rule-basedsystems.Advantages of the statistical approach that areclaimed by its proponents include a significantdecrease in the amount of rule coding required tocreate a parser that performs adequately, and theability to "tune" a parser to a particular type of textsimply by extracting statistical information fromthe same type of text.
Perhaps the most significantdisadvantage appears to be the requirement forlarge amounts of training data, often in the form oflarge NL text corpora that have been annotatedwith hand-coded tags specifying parts-of-speech,syntactic function, etc.
There have been a numberof efforts to extract information from corpora thatare not tagged (e.g., Kupiec and Maxwell 1992),but the depth of information thus obtained and itsutility in "automatically" creating a NL parser isusually limited.To overcome the need for augmenting corpora withtags in order to obtain more useful inforrnation,96researchers in statistical NLP have experimentedwith a variety of strategies, ome of which employvarying degrees of traditional linguistic abstraction.Su and Chang (1992) group words in untaggedcorpora into equivalence lasses, according to theirpossible parts-of-speech.
They then performstatistical analyses over these equivalence classes,rather than over the words themselves, in order toobtain higher-level trigram language models thatwill be used later by their statistics-based parser.Brown et al (1992) have similarly resorted toreducing inflected word forms to their underlyinglemmas before estimation of statistical parameters.Bnscoe and Carroll (1993) carry the use oftraditional rule-based linguistics a step further byusing a unification-based grammar as a startingpoint.
Through a process of human-supervisedtraining on a small corpus of text, a statisticalmodel is then developed which is used to rank theparses produced by the grammar for a given input.A similar method of interactive training has beenused by Simmons and Yu (1991) to producefavorable results.Beyond the realm of simply using traditionallinguistics to enhance the quality of data extractedfrom corpora by statistical methods, there havebeen attempts to create hybrid systems thatincorporate statistical information into alreadywell-developed rule-based frameworks.
Forexample, McKee and Maloney (1992) have usedcommon statistical methods to extract informationsuch as part-of-speech frequency, verb sub-categorization frames, and prepositional phraseattachment preferences from corpora nd have thenincorporated it into the processing in theirknowledge-based parser in order to quickly expandits coverage in new domains.In comparing rule-based approaches with thosewhich are more purely statistics-based, andincluding everything in between, one could claimthat there is some constant amount of linguisticknowledge that is required to create an NL parser,and one must either code it explicitly into the parser(using rules), or use statistical methods to extract itfrom sources uch as text corpora.
Furthermore, inthe latter case, the extraction of useful informationfrom the raw data in corpora is facilitated byadditional information provided through manualtagging, through "seeding" the process withlinguistic abstractions (e.g., parts-of-speech), orthrough the interaction of human supervisorsduring the extraction process.
In any case, itappears that in addition to information that may beobtained by statistical methods, generalizedlinguistic knowledge from a human source is alsoclearly desirable, if not required, in order to createtruly capable parsers.Proponents of statistical metheds usually point tothe data-driven aspect of their approach as enablingthem to create robust parsers that can handle "realtext."
Although many rule-based parsers have beenlimited in scope, we believe that it is indeedpossible to create and maintain broad-coverage,rule-based NL systems (e.g., Jensen 1993), bycarefully studying and using ample amounts of datato refme those systems.
It has been our experiencethat the complexity and difficulty of creating suchrule-based systems can be readily managed if onehas a powerful and comprehensive s t of tools.Nevertheless, it is also clearly desirable to be ableto use statistical methods to adapt (or tune) rule-based systems automatically for particular types oftext as well as to acquire additional inguisticinformation from corpora and to integrate it withinformation that has been developed by trainedlinguists.To the end of incorporating statistics-basedprocessing into a rule-based parser, we havedevised a "bootstrapping" method.
This methoduses a rule-based parser to compute part-of-speechand rule probabilities while processing a large, non-annotated corpus.
These probabilities are thenincorporated into the very same parser, therebyproviding uidance to the parser as it assigns partsof speech to words and applies rules during theprocessing of new text.Although our method relies on the existence of abroad-coverage, rule-based parser, which, asdiscussed at the beginning of this paper, is nottrivial to develop, the benefits of this approach arethat relevant statistical information can be obtainedautomatically from large untagged corpora, andthat this information can be used to improve97significantly the speed and accuracy of the parser.This method also obviates the need for any human-supervised training during the parsing process andallows for "tuning" the parser to particular types oftext.2 The Bootstrapping MethodWe use a broad-coverage, rule-based, bottom-up,chart parser as the basis for this work.
It utilizesthe Microsoft English Grammar (MEG), which is aset of augmented phrase structure grammar rulescontaining conditions designed to eliminate manypotential, but less-preferred, parses.
It seeks toproduce a single approximate syntactic parse foreach input, although it may also produce multipleparses or even a "fitted" parse in the event hat awell-formed parse is not obtained.
The"approximate" nature of a parse is exemplified bythe packing of many attachment ambiguities, wherephrases often default o simple fight attachment anda notation is made for further processing to resolvethe ambiguity at a later point in the NLP system.The bootstrapping method begins by using the rule-based parser to parse a large corpus of untaggedNL text.
During parsing, frequencies that will beused to compute rule and part-of-speechprobabilities are obtained.
For rule probabilities,these frequencies in their simplest form include thenumber of times that each rule r creates a node n,in a well-formed parse tree and the total number oftimes that r was attempted (i.e., the sequence ofconstituents c~ ..... c~, that trigger r occurred in thechart and r's conditions were evaluated relative tothose constituents).
At the end of parsing thecorpus, the former frequency isdivided by the latterfrequency to obtain the probability for each rule, asgiven in Figure !
below.
The reason for using thedenominator as given rather than the number oftimes ct ..... Cs occurs below n, in a parse tree is thatit adjusts for the conditions on rules contained inMEG, which may allow many such sequences ofconstituents tooccur in the chart, but only very fewof them to occur in the final parse tree.
In thiscase, the probability of a rule might be skewed infavor of trying it more often than it should be,unless the denominator were based on constituentsin the chart vs. in the parse tree.
(# times n r occurs in trees)P(r~Cl ... .
.
crn )= (# times cl ..... c m occur in chart)Figure 1.
Simple rule probabilityFor part-of-speech probabilities, the frequenciesobtained during parsing include the number oftimes a word w occurs having a particular part-of-speech p in a well-formed parse tree and the totalnumber of times that w occurs.
Once again, at theend of parsing, the former frequency is divided bythe latter to obtain the simple probability that aword will occur with a particular part of speech, asgiven in Figure 2.p(plw) = (# times w occurs having p in trees)(# times w occurs in trees)Figure 2.
Simple part-of.speech probabilitySince the choice was made to use the denominatorfor role probabilities given above, the part-of-speech probabilities must be normalized so that thetwo sets of probabilities are compatible and may beused together during the probabilistic algorithmdescribed below.
The normalization is achieved bymultiplying each part-of-speech probability by theratio of the average probability of all the rules overthe average probability of all the parts of speechfor all the words.
This effectively lowers the part-of-speech probabilities into the same range as therule probabilities, so that as the probabilisticalgorithm proceeds, it will try lower probabilityparts of speech for words at a consistent pointrelative to the application of lower probabilityrules.After computing and normalizing the probabilities,they are incorporated into the same rule-basedparser used to compute them.
The parser is guidedby these probabilities, while parsing any new input,to seek the most probable path through the parsesearch space, instead of taking the "all-paths"breadth-first approach it took when parsing without98the use of the probabilities.
A simplifieddescription of the chart parsing algorithm as guidedby probabilities is given in Figure 3 below.
Theterm record used in the algorithm ay be likened toan edge in traditional chart parsing terminology.
Apart-of-speech record refers to an edgerepresenting one (of possibly many) of the parts ofspeech for a given word.
A list (PLIST below) ofpotential rule applications and part-of-speechrecords, sorted by probability in descending order(i.e., highest probability first), is maintainedthroughout the execution of the algorithm.
The nextmost probable potential rule application or part-of-speech record is always located at the top ofPLIST.1.
Put all of the part-of-speech records for eachword in the input into PLIST, forcing theprobability of the highest probability part-of-speech record for each word to 1 (ensuring thatat least one part-of-speech record for eachword will be put into the chart immediately).2.
Process the next most probable item in PLIST:a.
If it is a potential rule application, remove itfrom PLIST and try the rule.
If the rolesucceeds, add a record representing a newsub-tree to the chart.b, Otherwise, if it is a part-of-speech record,remove it from PLIST and add it directlyto the chart.3.
If a record was added to the chart in step 2,identify all new potential rule applications (byexamining the constituent sequences in thechart), obtain their probabilities (from thosethat were computed and stored previously), andput them in their appropriate position inPLIST.4.
Stop if a record representing a parse tree forthe entire input string was generated or ifPLIST is empty, otherwise go to step 2.Figure 3.
Probability-directed chart parsingalgorithmThe PLIST in this algorithm is similar to theordered agenda used in the "best first" parserdescribed by Allen (1994).
However, in contrast toAllen's parser, the probabilities used by thisalgorithm do not take into account the probabilities99of the underlying nodes in each subtree, which inthe former case are multiplied together (on thebasis of a pragmatically motivated independenceassumption) to obtain a probability representativeof the entire subtree.
Therefore, this algorithm isnot guaranteed toproduce the most probable parsefirst.
In practice, though, the algorithm doesachieve good results and avoids having to deal withthe problems that Allen admits are encounteredwhen trying to apply a best-first strategy based onindependence assumptions to a large-scalegrammar.
These include a rapid drop-off of theprobabilities as subtrees grow deeper, causing aregression to nearly breadth-first searching.
Wedesire instead to maintain parsing efficiency at thecost of potentially not generating some number ofmost probable parses, while still generating a largenumber of those that are most probable.
Theresults reported below appear to bear this out.3 DiscussionOne potential disadvantage of the bootstrappingmethod is that the parser can reinforce its own badbehavior.
However, this may be controlled byparsing a large amount of data, ~ and then by usingonly the probabilities computed for "shorter"sentences (currently, those less than 35 words) forwhich a single, well-formed parse is obtained (incontrast o those for which multiple or "fitted"parses are obtained).
Our assessment thus far isthat our parser generates straightforward structuresfor the large majority of such sentences, resulting infairly accurate rule and part-of-speechprobabilities.
In many ways, this strategy issimilar to the strategies employed by Hindle andRooth (1993) and by Kinoshita et al (1993) in thatwe rely on processing of less ambiguous data toprovide information to assist the parser inprocessing the more difficult, ambiguous cases.Another factor in avoiding the reinforcement of badbehavior is our linguist's kill in making sure thatthe most common structures parse accurately.
Ast We have used the I million word Brown corpus tocompute our current set of statistics, but anticipateusing larger corpora.we evaluate the output of the probabilistic versionof our parser, our linguist continues, in a principledmanner, to add and change conditions on rules tocorrect problems with parse structures and parts-of-speech.
We have just made changes to theparser that enable it to use one set of probabilities(along with the changes our linguist made on thatbase) during parsing while computing another set.This will allow us to iterate during the developmentof the parser in a rule-based/statistics-based cycle,and to experiment with the effects of one set ofmethods on the other.Also, the simple probabilities described in theprevious ection are only a starting point.
Already,we have dependently conditioned the probabilitiesof rules on the following characteristics of theparse tree nodes generated by them:I. l, the length (in words) of the text covered bythe node, divided by 52. d, the distance (in words) that the text coveredby the node is from the end of the sentence,divided by 53. m, the minimal path length (in nodes) of thenodeThe division of the first two conditioning factors by5 serves to lump together the values obtainedduring probability computation, thereby decreasingthe potential for sparse data.
The third factor, theminimal path length of a node, is defined as thesmallest number of contiguous nodes that coversthe text between the node and the end of thesentence, where nodes are contiguous if the textstrings they represent are contiguous in thesentence.
The rule probability computation,including these three conditioning factors, is givenin Figure 4.
The term "composite" in thedenominator means that the specific li, di, and miare computed as if the constituents ct..... Cm wereone node.Although these conditioning factors are notlinguistically motivated in the theoretical sense,they have nevertheless contributed significantly tofurther improving the speed and accuracy of theparser.
Results based on their use are provided inthe next section.
They were identified based on aninspection of the conditions in the MEG rules andhow those rules go about building up well-formedparse tree structures (namely, right to left betweencertain clause and phrase boundaries).
Throughexperimentation, it was confirmed that these threefactors are all helpful in guiding the parser toexplore the most probable linguistic structures inthe search space in an order that is consistent withhow the MEG rules tend to build these structures.Specifically, MEG tends to extend structures fromright to left that are longer and span from any givenword to the end of a clause, especially to the end ofthe sentence.
The advantageous use of theseconditions points to the importance of carefullyconsidering various aspects of the existing rule setwhen integrating statistical processing within arule-based parser.P(~Cl .
.
.
.
.
cm, l i ,d i ,mi  ) =(# times n r with l i ,d  i, and m i occurs in trees) ,l # times c !
..... c m with composite l i ,d  i, and m i ) occur in chartFigure 4.
Conditioned rule probabilityIn the future, we anticipate conditioning theprobabilities further based on truly linguisticconsiderations, uch as the rule history or headword of a given structure.
This has been suggestedin works such as Black, et al (1993).
We alsoanticipate experimenting cautiously with variousindependence assumptions in order to decrease ourparameter space as we increase the number ofconditioning factors.
In all of these endeavors, wewill seek to determine the most beneficial interplaybetween the rule-based and statistics-based aspectsof our system.4 ResultsWe have used our parser to compute both simpleand conditioned probabilities, as described above,during the parsing of the !
million word Browncorpus.
In round numbers, this process took about34 hours on a 486/66 PC, for an average of 2.5seconds per sentence.
There are about 55,000sentences in the Brown corpus, averaging 18 words100in length, but those over 35 words in length (morethan 7,000) were not parsed, for the reasons givenearlier.The probabilities thus computed were incorporatedfor use by the probabilistic algorithm of the parser,and the parser was then applied to two sets ofselected sentences in order to evaluate theanticipated improvements in parsing speed andaccuracy.
The first set contained 500 sentences,averaging 17 words in length, and randomlyselected from different sources, including articlesfrom Time magazine and the Wall Street Journal,linguistic textbook examples, and correspondence.The efficiency of the parser in processing thesesentences, both with and without probabilities, isdocumented in Table 1.NoprobabilitiesSimpleprobabilitiesConditionedI probabilitiesAveragerecords inchart364Averagerulesattempted12836Averageparsingtime ~secs)2.416238 6633 1.8792367 181 1.231Table 1.
Comparison of parsing efficiency over500 sentencesUseful measures of parsing efficiency include thetotal number of records in the chart when a parse isobtained or the parser otherwise stops, the numberof rules aRempted for a given input string and, ofcourse, the time required to parse a string(assuming a dedicated, non-multi-tasking computersystem).
On average, using the conditionedprobabilities resulted in half as many records beingplaced in the chart during the processing of asentence and a corresponding speed-up by a factorof 2.
Rule attempts decreased by more than afactor of 5.
A large number of sentences parsedmany times faster than with the non-probabilisticalgorithm, but this was tempered in the averagingprocess by a number of long sentences that parsedin nearly the same time, and on very rare occasions,slightly slower.
2In the probabilistic algorithm used in thisevaluation, we also implemented a low-probabilitycutoff to stop the parser from continuing to applyrules after a certain number of rules (whoseprobability is less than the average probability ofall the rules) had been attempted.
This number ismultiplied by the number of words in a sentence (toadjust for the obvious fact that more ruleapplications are needed for longer sentences) andhas been determined experimentally b running theparser on sets of sentences and examining howoften a well-formed (in contrast to "fiRed") parse isactually obtained after a certain number of less-than-average rules have been attempted.
Theparser currently produces a fired parse for justover 20% of the sentences in the first set describedabove.
In practice, using this low-probability cutoffrarely increases the number of fitted parsesobtained, and then only slightly (perhaps apercentage point or so).
This is more than offset bythe use of the probabilities which, due tO theirpositive effect on parsing efficiency, allow for thesuccessful parsing of much longer and morecomplicated sentences without exhaustingcomputational resources such as availablecomputer memory.The second set of sentences on which the parserwas evaluated contained 100 sentences, roughlyhalf being randomly selected from a linguistictextbook and the other half from some Timemagazine articles.
Although the former half werefairly short (10 words/sentence), they exhibited avariety of linguistic structures, in contrast o thesomewhat more straightforward, but longer (17words/sentence), sentences from the latter half.All the sentences in this set shared thecharacteristic hat the parser produced two or moreparses for each of them.
The parse trees producedby the parser for these sentences were examinedand it was determined whether the correct parse2 Slower parsing is actually possible, when theprobabilities turn out to be useless for a given sentence,because of the overhead of maintaining and accessingthe PLIST described in Figure 3.101was produced first by the probabilistic algorithm,using both simple and conditioned probabilities.For the non-probabilistic algorithm, the parse treeswere ordered according to the degree of fightattachment they exhibited (i.e., deepest structuresfirst).
As shown in Table 2, the algorithm usingconditioned probabilities elected the correct parsemore than twice as often as simple rightattachment.
It is interesting to note that while theprobabilistic algorithm performed somewhat betteron the shorter textbook sentences than on the longermagazine sentences, fight attachment performedworse.
This is most likely due to the wide varietyof (not simple fight-branching) linguistic structuresin the textbook sentences.No probabilities(ordered by degreeof fight attachment)SimpleprobabilitiesConditionedprobabilitiesLinguistictextbooki sentences446)33%TirnemagazineI sentences Total!~54) (lO0)43% 38%74% 67%89% 76%70%82%Table 2.
Comparison of correct parse selectionover 100 sentences for which multiple parsesare produced5 ConclusionWe have described a "bootstrapping" method,which uses a broad-coverage, rule-based parser tocompute probabilities while parsing a non-annotated corpus of NL text, and whichincorporates those probabilities into the very sameparser for use in analyzing new text.
The resultsreported from an evaluation of this method showthat it can significantly improve the speed andaccuracy of the parser.
A salient feature of thismethod is that it does not require the use ofannotatod corpora or human-supervised trainingduring the computation of the probabilities.ReferencesAllen, J.
1994.
Natural Language Understanding,2nd Edition, ch.
7.
New York:Benjamin/Cummings.Black, E., F. Jelinek, J. Lafferty, D. Magerman, R.Mercer, and S. Roukos.
1993.
Towardshistory-based grammars: using richermodels for probabilistic parsing.
InProceedings ofthe 3 lst Annual Meeting ofthe Association for ComputationalLinguistics, 31-37.Briscoe, T., and J. Carroll.
1993.
Generalizedprobabilistic LR parsing of naturallanguage (corpora) with unification-basedgrammars.
Computational Linguistics, 19,no.
1:25-59.Brown, P., S. Della Pietra, V. Della Pietra, J.Lafferty, and R. Mercer.
1992.
Analysis,statistical transfer, and synthesis inmachine translation.
In Proceedings of theFourth International Conference onTheoretical nd Methodological Issues inMachine Translation (Montreal, Canada),83-98.Hindle, D., and M. Rooth.
1993.
Structuralambiguity and lexical relations.Computational Linguistics, 19, no.
1:103-120.Jensen, K. 1993.
PEG: the PLNLP Englishgrammar.
In Natural LanguageProcessing: the PLNLP Approach, ed.
K.Jensen, G. Heidom, and S. Richardson, 29-45.
Boston: Kluwer Academic Publishers.Kinoshita, S., M. Shimazu, and H. Hirakawa.1993.
Better translation with knowledgeextracted from the source text.
InProceedings of the Fifth InternationalConference on Theoretical andMethodological Issues in MachineTranslation (Kyoto, Japan), 240-251.Kupiec, J., and J. Maxwell.
1992.
Trainingstochastic grammars from unlabelled textcorpora.
In AAAI-92 Workshop Programon Statistically-Based NLP Techniques(San Jose, CA), 14-19.102McKee, D., and J. Maloney.
1992.
Using statisticsgained from corpora in a knowledge-basedNLP system.
In AAAI-92 WorkshopProgram on Statistically-Based NLPTechniques (San Jose, CA), 81-89.Simmons, R., and Y. Yu.
1991.
The acquisitionand application of context sensitivegrammar for English.
In Proceedings ofthe 29th Annual Meeting of theAssociation for ComputationalLinguistics, 122-129.Su, K., and J. Chang.
1992.
Why corpus-basedstatistics-oriented machine translation.
InProceedings of the Fourth InternationalConference on Theoretical andMethodological Issues in MachineTranslation (Montreal, Canada), 249-262.lO3
