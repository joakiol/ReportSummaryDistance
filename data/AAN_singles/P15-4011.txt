Proceedings of ACL-IJCNLP 2015 System Demonstrations, pages 61?66,Beijing, China, July 26-31, 2015.c?2015 ACL and AFNLPPlug Latent Structures and Play Coreference ResolutionSebastian Martschat, Patrick Claus and Michael StrubeHeidelberg Institute for Theoretical Studies gGmbHSchloss-Wolfsbrunnenweg 3569118 Heidelberg, Germany(sebastian.martschat|patrick.claus|michael.strube)@h-its.orgAbstractWe present cort, a modular toolkit for de-vising, implementing, comparing and an-alyzing approaches to coreference resolu-tion.
The toolkit allows for a unified rep-resentation of popular coreference reso-lution approaches by making explicit thestructures they operate on.
Several of theimplemented approaches achieve state-of-the-art performance.1 IntroductionCoreference resolution is the task of determiningwhich mentions in a text refer to the same en-tity.
Machine learning approaches to coreferenceresolution range from simple binary classificationmodels on mention pairs (Soon et al., 2001) tocomplex structured prediction approaches (Durrettand Klein, 2013; Fernandes et al., 2014).In this paper, we present a toolkit that imple-ments a framework that unifies these approaches:in the framework, we obtain a unified representa-tion of many coreference approaches by makingexplicit the latent structures they operate on.Our toolkit provides an interface for definingstructures for coreference resolution, which weuse to implement several popular approaches.
Anevaluation of the approaches on CoNLL sharedtask data (Pradhan et al., 2012) shows that theyobtain state-of-the-art results.
The toolkit also canperform end-to-end coreference resolution.We implemented this functionality on top of thecoreference resolution error analysis toolkit cort(Martschat et al., 2015).
Hence, this toolkit nowprovides functionality for devising, implementing,comparing and analyzing approaches to corefer-ence resolution.
cort is released as open source1and is available from the Python Package Index2.1http://smartschat.de/software2http://pypi.python.org/pypi.
Install it viapip install cort.2 A Framework for CoreferenceResolutionIn this section we briefly describe a structured pre-diction framework for coreference resolution.2.1 MotivationThe popular mention pair approach (Soon et al.,2001; Ng and Cardie, 2002) operates on a list ofmention pairs.
Each mention pair is considered in-dividually for learning and prediction.
In contrast,antecedent tree models (Yu and Joachims, 2009;Fernandes et al., 2014; Bj?orkelund and Kuhn,2014) operate on a tree which encodes all anaphor-antecedent decisions in a document.Conceptually, both approaches have in commonthat the structures they employ are not annotatedin the data (in coreference resolution, the annota-tion consists of a mapping of mentions to entityidentifiers).
Hence, we can view both approachesas instantiations of a generic structured predictionapproach with latent variables.2.2 SettingOur aim is to learn a prediction function f that,given an input document x ?
X , predicts a pair(h, z) ?
H?Z .
h is the (unobserved) latent struc-ture encoding the coreference relations betweenmentions in x. z is the mapping of mentions toentity identifiers (which is observed in the trainingdata).
Usually, z is obtained from h by taking thetransitive closure over coreference decisions en-coded in h. H and Z are the spaces containingall such structures and mappings.2.3 RepresentationFor a document x ?
X , we write Mx={m1, .
.
.
,mn} for the mentions in x. Follow-ing previous work (Chang et al., 2012; Fernandeset al., 2014), we make use of a dummy mentionwhich we denote as m0.
If m0is predicted as the61antecedent of a mention mi, we consider minon-anaphoric.
We define M0x= {m0} ?Mx.Inspired by previous work (Bengtson and Roth,2008; Fernandes et al., 2014; Martschat andStrube, 2014), we adopt a graph-based represen-tation of the latent structures h ?
H. In particular,we express structures by labeled directed graphswith vertex set M0x.m0m1m2m3m4m5Figure 1: Latent structure underlying the mentionranking and the antecedent tree approach.
Theblack nodes and arcs represent one substructurefor the mention ranking approach.Figure 1 shows a structure underlying the men-tion ranking and the antecedent tree approach.An arc between two mentions signals coreference.For antecedent trees (Fernandes et al., 2014), thewhole structure is considered, while for mentionranking (Denis and Baldridge, 2008; Chang etal., 2012) only the antecedent decision for oneanaphor is examined.
This can be expressed viaan appropriate segmentation into subgraphs whichwe refer to as substructures.
One such substruc-ture encoding the antecedent decision for m3iscolored black in the figure.Via arc labels we can express additional infor-mation.
For example, mention pair models (Soonet al., 2001) distinguish between positive and neg-ative instances.
This can be modeled by labelingarcs with appropriate labels, such as + and ?.2.4 Inference and LearningAs is common in natural language processing, wemodel the prediction of (h, z) via a linear model.That is,f(x) = f?
(x) = argmax(h,z)?H?Z?
?, ?
(x, h, z)?,where ?
?
Rdis a parameter vector and ?
: X ?H ?
Z ?
Rdis a joint feature representationfor inputs and outputs.
When employing substruc-tures, one maximization problem has to be solvedfor each substructure (instead of one maximizationproblem for the whole structure).To learn the parameter vector ?
?
Rdfromtraining data, we employ a latent structured per-ceptron (Sun et al., 2009) with cost-augmentedinference (Crammer et al., 2006) and averaging(Collins, 2002).3 ImplementationWe now describe our implementation of the frame-work presented in the previous section.3.1 AimsBy expressing approaches in the framework, re-searchers can quickly devise, implement, com-pare and analyze approaches for coreference res-olution.
To facilitate development, it should be aseasy as possible to define a coreference resolutionapproach.
We first describe the general architec-ture of our toolkit before giving a detailed descrip-tion of how to implement specific coreference res-olution approaches.3.2 ArchitectureThe toolkit is implemented in Python.
It can pro-cess raw text and data conforming to the format ofthe CoNLL-2012 shared task on coreference res-olution (Pradhan et al., 2012).
The toolkit is or-ganized in four modules: the preprocessingmodule contains functionality for processing rawtext, the core module provides mention extrac-tion and computation of mention properties, theanalysis module contains error analysis meth-ods, and the coreference module implementsthe framework described in the previous section.3.2.1 preprocessingBy making use of NLTK3, this module providesclasses and functions for performing the prepro-cessing tasks necessary for mention extractionand coreference resolution: tokenization, sentencesplitting, parsing and named entity recognition.3.2.2 coreWe employ a rule-based mention extractor, whichalso computes a rich set of mention attributes, in-cluding tokens, head, part-of-speech tags, namedentity tags, gender, number, semantic class, gram-matical function and mention type.
These at-tributes, from which features are computed, canbe extended easily.3http://www.nltk.org/62cort?visualization:?wsj_0174_part_0001.
ORTEGA ?ENDED?a?truce?with?
the?Contras ?and?said?
elections ?werethreatened?.2.
The?Nicaraguan?president ?,?citing?attacks?by?
the?U.S.??
?backedrebels ?,?
suspended a?19???month???old?cease??
?fire ?and?accused?
Bushof?``?promoting?death?.?''3.
While?
he ?reaffirmed?support?for?
the?country?
's ?Feb.
?25?elections ?,Ortega ?indicated?that?renewed?U.S.?military?aid?to?
the?Contras ?couldthwart?
the?balloting ?.4.
He ?said?U.S.?assistance?should?be?used?to?demobilize?
the?rebels ?.5.
A?
White?House ?spokesman?condemned?
the?truce?suspension ?as?``deplorable?''?but?brushed?off?talk?of?renewing?military?funding?for?
theinsurgents ?.6.
The?
Contra ?military?command?,?in?a?statement?from?Honduras?,?saidSandinista?troops?had?launched?a?major?offensive?against?
the?rebelforces ?.7.
East?German?leader?Krenz ?called?the?protests?in?
his ?country ?a?``good?sign?,?''?saying?that?many?of?those?marching?for?democraticfreedoms?were?showing?support?for?``?the?renovation?for?socialism?.?''8.
The?Communist?Party?chief ?,?in?Moscow?for?talks?with?Soviet ?officials?,also?said?East?Germany ?would?follow?
Gorbachev?
's ?restructuring?plans.9.
Thousands?of?East?Germans?fled?to?Czechoslovakia?after?the?East?Berlingovernment?lifted?
travel?restrictions ?.10.
The?ban?on?cross?border?movement ?was?imposed?
last?month ?after?amassive?exodus?of?emigres?to?
West?Germany ?.11.
Also?,?a?Communist?official?for?the?first?time?said?the?future?of?the?BerlinDocumentswsj_0174_part_000wsj_2278_part_000wsj_2400_part_000wsj_2401_part_000wsj_2402_part_000wsj_2403_part_000wsj_2404_part_000Errors?(45)Precision?(16)NAM:?5NOM:?10PRO:?1Recall?
(29)NAM:?6NOM:?21PRO:?2Reference?EntitiesORTEGAtruceContraselectionssuspendedBushWhite?Houseleader?KrenzSystem?EntitiesORTEGAContraselectionspresidentrebelsBushcountryballotingRecallRecallRecallRecallFigure 2: Visualization of errors.3.2.3 analysisTo support system development, this moduleimplements the error analysis framework ofMartschat and Strube (2014).
Users can extract,analyze and visualize recall and precision errorsof the systems they are working on.
Figure 2shows a screenshot of the visualization.
A moredetailed description can be found in Martschat etal.
(2015).3.2.4 coreferenceThis module provides features for coreferenceresolution and implements the machine learningframework described in the previous section.We implemented a rich set of features employedin previous work (Ng and Cardie, 2002; Bengtsonand Roth, 2008; Bj?orkelund and Kuhn, 2014), in-cluding lexical, rule-based and semantic features.The feature set can be extended by the user.The module provides a structured latent percep-tron implementation and contains classes that im-plement the workflows for training and prediction.As its main feature, it provides an interface fordefining coreference resolution approaches.
Wealready implemented various approaches (see Sec-tion 4).3.3 Defining ApproachesThe toolkit provides a simple interface for devis-ing coreference resolution approaches via struc-tures.
The user just needs to specify two func-tions: an instance extractor, which defines theListing 1 Instance extraction for the mention rank-ing model with latent antecedents.def extract_substructures(doc):substructures = []# iterate over mentionsfor i, ana in enumerate(doc.system_mentions):ana_arcs = []# iterate in reversed order over# candidate antecedentsfor ante in sorted(doc.system_mentions[:i],reverse=True):ana_arcs.append((ana, ante))substructures.append(ana_arcs)return substructuresListing 2 Decoder for the mention ranking modelwith latent antecedents.class RankingPerceptron(perceptrons.Perceptron):def argmax(self, substructure,arc_information):best_arc, best_arc_score, \best_cons_arc, best_cons_arc_score, \consistent = self.find_best_arcs(substructure, arc_information)return ([best_arc], [],[best_arc_score],[best_cons_arc], [],[best_cons_arc_score],consistent)search space for the optimal (sub)structures, anda decoder, which, given a parameter vector, findsoptimal (sub)structures.
The toolkit then performstraining and prediction using these user-specifiedfunctions.
The user can further customize the ap-proach by defining cost functions to be used dur-ing cost-augmented inference, and clustering al-gorithms to extract coreference chains from latentstructures, such as closest-first (Soon et al., 2001)or best-first (Ng and Cardie, 2002).In the remainder of this section, we present anexample implementation of the mention rankingmodel with latent antecedents (Chang et al., 2012)in our toolkit.3.3.1 Instance ExtractorsThe instance extractor receives a document as in-put and defines the search space for the maximiza-tion problem to be solved by the decoder.
To doso, it needs to output the segmentation of the la-63Listing 3 Cost function for the mention rankingmodel with latent antecedents.def cost_based_on_consistency(arc):ana, ante = arcconsistent = \ana.decision_is_consistent(ante)# false newif not consistent and \ante.is_dummy():return 2# wrong linkelif not consistent:return 1# correctelse:return 0tent structure for one document into substructures,and the candidate arcs for each substructure.Listing 1 shows source code of the instance ex-tractor for the mention ranking model with latentantecedents.
In this model, each antecedent de-cision for a mention corresponds to one substruc-ture.
Therefore, the extractor iterates over all men-tions.
For each mention, arcs to all preceding men-tions are extracted and stored as candidate arcs forone substructure.3.3.2 DecodersThe decoder solves the maximization problemsfor obtaining the highest-scoring latent substruc-tures consistent with the gold annotation, and thehighest-scoring cost-augmented latent substruc-tures.Listing 2 shows source code of a decoder for themention ranking model with latent antecedents.The input to the decoder is a substructure, whichis a set of arcs, and a mapping from arcs to infor-mation about arcs, such as features or costs.
Theoutput is a tuple containing?
a list of arcs that constitute the highest-scoring substructure, together with their la-bels (if any) and scores,?
the same for the highest-scoring substructureconsistent with the gold annotation,?
the information whether the highest-scoringsubstructure is consistent with the gold anno-tation.To obtain this prediction, we invoke the aux-iliary function self.find best arcs.
Thisfunction searches through a set of arcs to find theoverall highest-scoring arc and the overall highest-scoring arc consistent with the gold annotation.Furthermore, it also outputs the scores of thesearcs according to the model, and whether the pre-diction of the best arc is consistent with the goldannotation.For the mention ranking model, we let the func-tion search through all candidate arcs for a sub-structure, since these represent the antecedent de-cision for one anaphor.
Note that the mentionranking model does not use any labels.The update of the parameter vector is handledby our implementation of the structured percep-tron.3.3.3 Cost FunctionsCost functions allow to bias the learner towardsspecific substructures, which leads to a large mar-gin approach.
For the mention ranking model, weemploy a cost function that assigns a higher cost toerroneously determining anaphoricity than to se-lecting a wrong link, similar to the cost functionsemployed by Durrett and Klein (2013) and Fer-nandes et al.
(2014).
The source code is displayedin Listing 3.3.3.4 Clustering AlgorithmsThe mention ranking model selects one antecedentfor each anaphor, therefore there is no need tocluster antecedent decisions.
Our toolkit providesclustering algorithms commonly used for men-tion pair models, such as closest-first (Soon et al.,2001) or best-first (Ng and Cardie, 2002).3.4 Running cortcort can be used as a Python library, but also pro-vides two command line tools cort-train andcort-predict.4 EvaluationWe implemented a mention pair model with best-first clustering (Ng and Cardie, 2002), the mentionranking model with closest (Denis and Baldridge,2008) and latent (Chang et al., 2012) antecedents,and antecedent trees (Fernandes et al., 2014).Only slight modifications of the source code dis-played in Listings 1 and 2 were necessary to im-plement these approaches.
For the ranking modelsand antecedent trees we use the cost function de-scribed in Listing 3.We evaluate the models on the English test dataof the CoNLL-2012 shared task on multilingualcoreference resolution (Pradhan et al., 2012).
Weuse the reference implementation of the CoNLL64MUC B3CEAFeModel R P F1R P F1R P F1Average F1CoNLL-2012 English test dataFernandes et al.
(2014) 65.83 75.91 70.51 51.55 65.19 57.58 50.82 57.28 53.86 60.65Bj?orkelund and Kuhn (2014) 67.46 74.30 70.72 54.96 62.71 58.58 52.27 59.40 55.61 61.63Mention Pair 67.16 71.48 69.25 51.97 60.55 55.93 51.02 51.89 51.45 58.88Ranking: Closest 67.96 76.61 72.03 54.07 64.98 59.03 51.45 59.02 54.97 62.01Ranking: Latent 68.13 76.72 72.17 54.22 66.12 59.58 52.33 59.47 55.67 62.47Antecedent Trees 65.34 78.12 71.16 50.23 67.36 57.54 49.76 58.43 53.75 60.82Table 1: Results of different systems and models on CoNLL-2012 English test data.
Models below thedashed lines are implemented in our toolkit.scorer (Pradhan et al., 2014), which computesthe average of the evaluation metrics MUC (Vi-lain et al., 1995), B3, (Bagga and Baldwin, 1998)and CEAFe(Luo, 2005).
The models are trainedon the concatenation of training and developmentdata.The evaluation of the models is shown in Table1.
To put the numbers into context, we comparewith Fernandes et al.
(2014), the winning systemof the CoNLL-2012 shared task, and the state-of-the-art system of Bj?orkelund and Kuhn (2014).The mention pair model performs decently,while the antecedent tree model exhibits perfor-mance comparable to Fernandes et al.
(2014), whouse a very similar model.
The ranking models out-perform Bj?orkelund and Kuhn (2014), obtainingstate-of-the-art performance.5 Related WorkMany researchers on coreference resolution re-lease an implementation of the coreference modeldescribed in their paper (Lee et al., 2013; Durrettand Klein, 2013; Bj?orkelund and Kuhn, 2014, in-ter alia).
However, these implementations imple-ment only one approach following one paradigm(such as mention ranking or antecedent trees).Similarly to cort, research toolkits such asBART (Versley et al., 2008) or Reconcile (Stoy-anov et al., 2009) provide a framework to im-plement and compare coreference resolution ap-proaches.
In contrast to these toolkits, we makethe latent structure underlying coreference ap-proaches explicit, which facilitates developmentof new approaches and renders the developmentmore transparent.
Furthermore, we provide ageneric and customizable learning algorithm.6 ConclusionsWe presented an implementation of a frame-work for coreference resolution that represents ap-proaches to coreference resolution by the struc-tures they operate on.
In the implementation weplaced emphasis on facilitating the definition ofnew models in the framework.The presented toolkit cort can process raw textand CoNLL shared task data.
It achieves state-of-the-art performance on the shared task data.The framework and toolkit presented in this pa-per help researchers to devise, analyze and com-pare representations for coreference resolution.AcknowledgementsWe thank Benjamin Heinzerling for helpful com-ments on drafts of this paper.
This work has beenfunded by the Klaus Tschira Foundation, Ger-many.
The first author has been supported by aHITS Ph.D. scholarship.ReferencesAmit Bagga and Breck Baldwin.
1998.
Algorithmsfor scoring coreference chains.
In Proceedingsof the 1st International Conference on LanguageResources and Evaluation, Granada, Spain, 28?30May 1998, pages 563?566.Eric Bengtson and Dan Roth.
2008.
Understandingthe value of features for coreference resolution.
InProceedings of the 2008 Conference on EmpiricalMethods in Natural Language Processing, Waikiki,Honolulu, Hawaii, 25?27 October 2008, pages 294?303.Anders Bj?orkelund and Jonas Kuhn.
2014.
Learn-ing structured perceptrons for coreference resolu-tion with latent antecedents and non-local features.65In Proceedings of the 52nd Annual Meeting of theAssociation for Computational Linguistics (Volume1: Long Papers), Baltimore, Md., 22?27 June 2014,pages 47?57.Kai-Wei Chang, Rajhans Samdani, Alla Rozovskaya,Mark Sammons, and Dan Roth.
2012.
Illinois-Coref: The UI system in the CoNLL-2012 sharedtask.
In Proceedings of the Shared Task of the16th Conference on Computational Natural Lan-guage Learning, Jeju Island, Korea, 12?14 July2012, pages 113?117.Michael Collins.
2002.
Discriminative training meth-ods for Hidden Markov Models: Theory and experi-ments with perceptron algorithms.
In Proceedingsof the 2002 Conference on Empirical Methods inNatural Language Processing, Philadelphia, Penn.,6?7 July 2002, pages 1?8.Koby Crammer, Ofer Dekel, Joseph Keshet, ShaiShalev-Shwartz, and Yoram Singer.
2006.
Onlinepassive-aggressive algorithms.
Journal of MachineLearning Research, 7:551?585.Pascal Denis and Jason Baldridge.
2008.
Specializedmodels and ranking for coreference resolution.
InProceedings of the 2008 Conference on EmpiricalMethods in Natural Language Processing, Waikiki,Honolulu, Hawaii, 25?27 October 2008, pages 660?669.Greg Durrett and Dan Klein.
2013.
Easy victoriesand uphill battles in coreference resolution.
In Pro-ceedings of the 2013 Conference on Empirical Meth-ods in Natural Language Processing, Seattle, Wash.,18?21 October 2013, pages 1971?1982.Eraldo Fernandes, C?
?cero dos Santos, and Ruy Milidi?u.2014.
Latent trees for coreference resolution.
Com-putational Linguistics, 40(4):801?835.Heeyoung Lee, Angel Chang, Yves Peirsman,Nathanael Chambers, Mihai Surdeanu, and Dan Ju-rafsky.
2013.
Deterministic coreference resolu-tion based on entity-centric, precision-ranked rules.Computational Linguistics, 39(4):885?916.Xiaoqiang Luo.
2005.
On coreference resolutionperformance metrics.
In Proceedings of the Hu-man Language Technology Conference and the 2005Conference on Empirical Methods in Natural Lan-guage Processing, Vancouver, B.C., Canada, 6?8October 2005, pages 25?32.Sebastian Martschat and Michael Strube.
2014.
Recallerror analysis for coreference resolution.
In Pro-ceedings of the 2014 Conference on Empirical Meth-ods in Natural Language Processing, Doha, Qatar,25?29 October 2014, pages 2070?2081.Sebastian Martschat, Thierry G?ockel, and MichaelStrube.
2015.
Analyzing and visualizing corefer-ence resolution errors.
In Proceedings of the 2015Conference of the North American Chapter of theAssociation for Computational Linguistics: Demon-strations, Denver, Col., 31 May ?
5 June 2015, pages6?10.Vincent Ng and Claire Cardie.
2002.
Improving ma-chine learning approaches to coreference resolution.In Proceedings of the 40th Annual Meeting of the As-sociation for Computational Linguistics, Philadel-phia, Penn., 7?12 July 2002, pages 104?111.Sameer Pradhan, Alessandro Moschitti, Nianwen Xue,Olga Uryupina, and Yuchen Zhang.
2012.
CoNLL-2012 Shared Task: Modeling multilingual unre-stricted coreference in OntoNotes.
In Proceedingsof the Shared Task of the 16th Conference on Com-putational Natural Language Learning, Jeju Island,Korea, 12?14 July 2012, pages 1?40.Sameer Pradhan, Xiaoqiang Luo, Marta Recasens, Ed-uard Hovy, Vincent Ng, and Michael Strube.
2014.Scoring coreference partitions of predicted men-tions: A reference implementation.
In Proceed-ings of the 52nd Annual Meeting of the Associationfor Computational Linguistics (Volume 2: Short Pa-pers), Baltimore, Md., 22?27 June 2014, pages 30?35.Wee Meng Soon, Hwee Tou Ng, and DanielChung Yong Lim.
2001.
A machine learning ap-proach to coreference resolution of noun phrases.Computational Linguistics, 27(4):521?544.Veselin Stoyanov, Claire Cardie, Nathan Gilbert, EllenRiloff, David Buttler, and David Hysom.
2009.Reconcile: A coreference resolution research plat-form.
Technical report, Cornell University.Xu Sun, Takuya Matsuzaki, Daisuke Okanohara, andJun?ichi Tsujii.
2009.
Latent variable perceptron al-gorithm for structured classification.
In Proceedingsof the 21th International Joint Conference on Artifi-cial Intelligence, Pasadena, Cal., 14?17 July 2009,pages 1236?1242.Yannick Versley, Simone Paolo Ponzetto, MassimoPoesio, Vladimir Eidelman, Alan Jern, Jason Smith,Xiaofeng Yang, and Alessandro Moschitti.
2008.BART: A modular toolkit for coreference resolu-tion.
In Companion Volume to the Proceedings ofthe 46th Annual Meeting of the Association for Com-putational Linguistics, Columbus, Ohio, 15?20 June2008, pages 9?12.Marc Vilain, John Burger, John Aberdeen, Dennis Con-nolly, and Lynette Hirschman.
1995.
A model-theoretic coreference scoring scheme.
In Proceed-ings of the 6th Message Understanding Conference(MUC-6), pages 45?52, San Mateo, Cal.
MorganKaufmann.Chun-Nam John Yu and Thorsten Joachims.
2009.Learning structural SVMs with latent variables.
InProceedings of the 26th International Conference onMachine Learning, Montr?eal, Qu?ebec, Canada, 14?18 June 2009, pages 1169?1176.66
