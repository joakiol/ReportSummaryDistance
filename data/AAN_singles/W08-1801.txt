Coling 2008: Proceedings of the 2nd workshop on Information Retrieval for Question Answering (IR4QA), pages 1?8Manchester, UK.
August 2008Improving Text Retrieval Precision andAnswer Accuracy in Question Answering SystemsMatthew W. Bilotti and Eric NybergLanguage Technologies InstituteCarnegie Mellon University5000 Forbes AvenuePittsburgh, PA 15213 USA{ mbilotti, ehn }@cs.cmu.eduAbstractQuestion Answering (QA) systems are of-ten built modularly, with a text retrievalcomponent feeding forward into an answerextraction component.
Conventional wis-dom suggests that, the higher the quality ofthe retrieval results used as input to the an-swer extraction module, the better the ex-tracted answers, and hence system accu-racy, will be.
This turns out to be a poorassumption, because text retrieval and an-swer extraction are tightly coupled.
Im-provements in retrieval quality can be lostat the answer extraction module, which cannot necessarily recognize the additionalanswer candidates provided by improvedretrieval.
Going forward, to improve ac-curacy on the QA task, systems will needgreater coordination between text retrievaland answer extraction modules.1 IntroductionThe task of Question Answering (QA) involvestaking a question phrased in natural human lan-guage and locating specific answers to that ques-tion expressed within a text collection.
Regard-less of system architecture, or whether the sys-tem is operating over a closed text collection orthe web, most QA systems use text retrieval as afirst step to narrow the search space for the an-swer to the question to a subset of the text col-lection (Hirschman and Gaizauskas, 2001).
Theremainder of the QA process amounts to a gradualnarrowing of the search space, using successivelymore finely-grained filters to extract, validate andpresent one or more answers to the question.c?
2008.
Licensed under the Creative CommonsAttribution-Noncommercial-Share Alike 3.0 Unported li-cense (http://creativecommons.org/licenses/by-nc-sa/3.0/).Some rights reserved.Perhaps the most popular system architecture inthe QA research community is the modular archi-tecture, in most variations of which, text retrievalis represented as a separate component, isolatedby a software abstraction from question analysisand answer extraction mechanisms.
The widely-accepted pipelined modular architecture imposes astrict linear ordering on the system?s control flow,with the analysis of the input question used as in-put to the text retrieval module, and the retrievedresults feeding into the downstream answer extrac-tion components.Proponents of the modular architecture naturallyview the QA task as decomposable, and to a cer-tain extent, it is.
The modules, however, can neverbe fully decoupled, because question analysis andanswer extraction components, at least, depend ona common representation for answers and perhapsalso a common set of text processing tools.
Thisdependency is necessary to enable the answer ex-traction mechanism to determine whether answersexist in retrieved text, by analyzing it and compar-ing it against the question analysis module?s an-swer specification.
In practice, the text retrievalcomponent does not use the common representa-tion for scoring text; either the question analysismodule or an explicit query formulation compo-nent maps it into a representation queryable by thetext retrieval component.The pipelined modular QA system architecturealso carries with it an assumption about the com-positionality of the components.
It is easy to ob-serve that errors cascade as the QA process movesthrough downstream modules, and this leads to theintuition that maximizing performance of individ-ual modules minimizes the error at each stage ofthe pipeline, which, in turn, should maximize over-all end-to-end system accuracy.It is a good idea to pause to question what thisintuition is telling us.
Is end-to-end QA systemperformance really a linear function of individual1[ARG0 [PERSON John]] [TARGET loves] [ARG1 [PERSON Mary]]Figure 1: Example OpenEphyra semantic representation for the sentence, John loves Mary.
Note thatJohn is identified as the ARG0, the agent, or doer, of the love action.
Mary is identified as the ARG1, thepatient, or to whom the love action is being done.
Both John and Mary are also identified as PERSONnamed entity types.components?
Is component performance really ad-ditive?
This paper argues that the answer is no,not in general, and offers the counterexample of ahigh-precision text retrieval system that can checkconstraints against the common representation atretrieval time, which is integrated into a publicly-available pipelined modular QA system that is oth-erwise unchanged.Ignoring the dependency between the answerextraction mechanism and the text retrieval com-ponent creates a problem.
The answer extractionmodule is not able to handle the more sophisti-cated types of matches provided by the improvedtext retrieval module, and so it ignores them, leav-ing end-to-end system performance largely un-changed.
The lesson learned is that a module im-proved in isolation does not necessarily provide animprovement in end-to-end system accuracy, andthe paper concludes with recommendations for fur-ther research in bringing text retrieval and answerextraction closer together.2 Improving Text Retrieval in IsolationThis section documents an attempt to improve theperformance of a QA system by substituting itsexisting text retrieval component with for high-precision retrieval system capable of checking lin-guistic and semantic constraints at retrieval time.2.1 The OpenEphyra QA SystemOpenEphyra is the freely-available, open-sourceversion of the Ephyra1 QA system (Schlaefer etal., 2006; Schlaefer et al, 2007).
OpenEphyra is apipelined modular QA system having four stages:question analysis, query generation, search and an-swer extraction and selection.
OpenEphyra alsoincludes support for answer projection, or the useof the web to find answers to the question, whichare then used to find supporting text in the cor-pus.
Answer projection support was disabled forthe purposes of this paper.1See: http://www.ephyra.infoThe common representation in OpenEphyra isa verb predicate-argument structure, augmentedwith named entity types, in which verb argumentsare labeled with semantic roles in the style of Prop-Bank (Kingsbury et al, 2002).
This feature re-quires the separate download2 of a semantic parsercalled ASSERT (Pradhan et al, 2004), which wastrained on PropBank.
See Figure 1 for an examplerepresentation for the sentence, John loves Mary.OpenEphyra comes packaged with standardbaseline methods for answer extraction and se-lection.
For example, it extracts answers fromretrieved text based on named entity instancesmatching the expected answer type as determinedby the question analysis module.
It can also lookfor predicate-argument structures that match thequestion structure, and can extract the argumentcorresponding to the argument in the question rep-resenting the interrogative phrase.
OpenEphyra?sdefault answer selection algorithm filters out an-swers containing question keyterms, merges sub-sets, and combines scores of duplicate answers.2.2 Test CollectionThe corpus used in this experiment is theAQUAINT corpus (Graff, 2002), the standardcorpus for the TREC3 QA evaluations held in2002 through 2005.
The corpus was preparedusing MXTerminator (Reynar and Ratnaparkhi,1997) for sentence segmentation, BBN Identi-finder (Bikel et al, 1999) for named entity recog-nition, as well as the aforementioned ASSERTfor identification of verb predicate-argument struc-tures and PropBank-style semantic role labeling ofthe arguments.The test collection consists of 109 questionsfrom the QA track at TREC 2002 with extensivedocument-level relevance judgments (Bilotti et al,2004; Lin and Katz, 2006) over the AQUAINTcorpus.
A set of sentence-level judgments was pre-2See: http://www.cemantix.org3Text REtrieval Conferences organized by the U.S. Na-tional Institute of Standards and Technology2Existing query #combine[sentence]( #any:person first person reachsouth pole )Top-ranked result Dufek became the first person to land an airplane at the South Pole.Second-ranked result He reached the North Pole in 1991.High-precision query #combine[sentence]( #max( #combine[target]( scored#max( #combine[./arg1]( #any:person ))#max( #combine[./arg2](#max( #combine[target]( reach#max( #combine[./arg1]( south pole )))))))))Top-ranked result [ARG1 Norwegian explorer [PERSON Roald Admundsen]] [TARGET becomes](relevant) [ARG2 [ARG0 first man] to [TARGET reach] [ARG1 [LOCATION South Pole]]]Figure 2: Retrieval comparison between OpenEphrya?s existing text retrieval component, and the high-precision version it was a replaced with, for question 1475, Who was the first person to reach the SouthPole?
Note that the top two results retrieved by the existing text retrieval component are not relevant,and the top result from the high-precision component is relevant.
The existing component does retrievethis answer-bearing sentence, but ranks it third.pared by manually determining whether each sen-tence matching the TREC-provided answer patternfor a given question was answer-bearing accordingto the definition that an answer-bearing sentencecompletely contains and supports the answer to thequestion, without requiring inference or aggrega-tion outside of that sentence.
Questions withoutany answer-bearing sentences were removed fromthe test collection, leaving 91 questions.Questions were manually reformulated so thatthey contain predicates.
For example, question1432, Where is Devil?s Tower?
was changed toWhere is Devil?s Tower located?, because AS-SERT does not cover verbs, including be and have,that do not occur in its training data.
Hand-corrected ASSERT parses for each question werewere cached in the question analysis module.
Re-formulated questions are used as input to both theexisting and high-precision text retrieval modules,to avoid advantaging one system over the other.2.3 High-Precision Text RetrievalOpenEphyra?s existing text retrieval module wasreplaced with a high-precision text retrieval sys-tem based on a locally-modified version of the In-dri (Strohman et al, 2005) search engine, a part ofthe open-source Lemur toolkit4.
While the existingversion of the text retrieval component supportsquerying on keyterms, phrases and placeholders4See: http://www.lemurproject.orgfor named entity types, the high-precision versionalso supports retrieval-time constraint-checkingagainst the semantic representation based on verbpredicate-argument structures, PropBank-style se-mantic role labels, and named entity recognition.To make use of this expanded text retrieval ca-pability, OpenEphyra?s query formulation modulewas changed to source pre-prepared Indri queriesthat encode using structured query operators thepredicate-argument and named entity constraintsthat match the answer-bearing sentences for eachquestion.
If questions have multiple queries asso-ciated with them, each query is evaluated individu-ally, with the resulting ranked lists fused by RoundRobin (Voorhees et al, 1994).
Round Robin,which merges ranked lists by taking the top-rankedelement from each list in order followed by lower-ranking elements, was chosen because Indri, theunderlying retrieval engine, gives different queriesscores that are not comparable in general, makingit difficult to choose a fusion method that uses re-trieval engine score as a feature.Figure 2 shows a comparison of querying andretrieval behavior between OpenEphyra?s existingtext retrieval module and the high-precision ver-sion with which it is being replaced for question1475, Who was the first person to reach the SouthPole?
The bottom of the figure shows an answer-bearing sentence with the correct answer, RoaldAdmundsen.
The predicate-argument structure, se-3mantic role labels and named entities are shown.The high-precision text retrieval module sup-ports storing of extents representing sentences, tar-get verbs and arguments and named entity typesas fields in the index.
At query time, con-straints on these fields can be checked using struc-tured query operators.
The queries in Figure 2are shown in Indri syntax.
Both queries beginwith #combine[sentence], which instructsIndri to score and rank sentence extents, ratherthan entire documents.
The query for the ex-isting text retrieval component contains keytermsas well an #any:type operator that matches in-stances of the expected answer type, which in thiscase is person.
The high-precision query encodesa verb predicate-argument structure.
The nested#combine[target] operator scores a sentenceby the predicate-argument structures it contains.The #combine[./role] operators are used to in-dicate constraints on specific argument roles.
Thedot-slash syntax tells Indri that the argument ex-tents are related to but not enclosed by the targetextent.
Throughout, the #max operator is used toselect the best matching extent in the event thatmore than one satisfy the constraints.Figure 3 compares average precision at the toptwenty ranks over the entire question set betweenOpenEphyra?s existing text retrieval module andthe high-precision text retrieval module, showingthat the latter performs better.2.4 ResultsTo determine what effect improving text retrievalquality has on the end-to-end QA system, it suf-fices to run the system on the entire test collection,00.050.10.150.20.250.30.350.40.450.51 6 11 16High-precisionExistingRankAveragePrecisionat RankFigure 3: Comparison of average precision at toptwenty ranks between OpenEphyra?s existing textretrieval module, and the high-precision versionthat took its place.replace the text retrieval component with the high-precision version while holding the other modulesconstant, and repeat the test run.
Table 1 summa-rizes the MAP, average end-to-end system accu-racy (whether the top-ranked returned answer iscorrect), and the mean reciprocal rank (MRR) ofthe correct answer (one over the rank at which thecorrect answer is returned).
If the correct answerto a question is returned beyond rank twenty, thereciprocal rank for that question is considered tobe zero.Table 1: Summary of end-to-end QA system ac-curacy and MRR when the existing text retrievalmodule is replaced with a high-precision versionRetrieval MAP Accuracy MRRExisting 0.3234 0.1099 0.2080High-precision 0.5487 0.1319 0.2020Table 1 shows that, despite the improvement inaverage precision, the end-to-end system did notrealize a significant improvement in accuracy orMRR.
Viewed in the aggregate, the results are dis-couraging, because it seems that the performancegains realized after the text retrieval stage of thepipeline are lost in downstream answer extractioncomponents.Figure 4 compares OpenEphyra both before andafter the integration of the high-precision text re-trieval component on the basis of average precisionand answer MRR.
The horizontal axis plots the dif-ference in average precision; a value of positiveone indicates that the high-precision version of themodule was perfect, ranking all answer-bearingsentences at the top of the ranked list, and that theexisting version retrieved no relevant text at all.Negative one indicates the reverse.
The verticalaxis plots the difference in answer MRR.
As be-fore, positive one indicates that the high-precisioncomponent led the system to rank the correct an-swer first, and the existing component did not, andnegative one indicates the reverse.
The zero pointon each axis is where the high-precision and ex-isting text retrieval components performed equallywell.The expectation is that there will be a posi-tive correlation between average precision and an-swer MRR; when the retrieval component provideshigher quality results, the job of the answer extrac-tion module should be easier.
This is illustratedin the bottom portion of Figure 4, which was cre-4-1-0.500.51-1 -0.5 0 0.5 1Difference in Average PrecisionIdeal Answer ExtractionOpenEphyraDifferenceinAnswer MRR-1-0.500.51-1 -0.5 0 0.5 1Figure 4: Scatter plot comparing the difference inaverage precision between the high-precision re-trieval component and the existing retrieval com-ponent on the horizontal axis, to the difference inanswer MRR on the vertical axis.
Ideally, therewould be a high correlation between the two; as av-erage precision improves, so should answer MRR.ated by assuming that the answer extraction mod-ule could successfully extract answers without er-ror from all answer-bearing sentences returned bythe text retrieval component.Interestingly, actual extraction performance,shown in the top portion of Figure 4, bears lit-tle resemblance to the ideal.
Note the large con-centration of data points along the line represent-ing zero difference in answer MRR.
This indicatesthat, regardless of improvement in average pre-cision of the results coming out of the retrievalmodule, the downstream answer extraction perfor-mance remains the same as it was when the ex-isting text retrieval component was in use.
Thisoccurs because the answer extraction module doesnot know how to extract answers from some of thetypes of answer-bearing sentences retrieved by thehigh-precision version of the retrieval module andnot by the existing version.There are several data points in the top right-hand quadrant of the top half of Figure 4, indicat-ing that for some questions, answer extraction wasable to improve as average precision improved.This is likely due to better rankings for types ofanswer-bearing sentences that answer extractionalready knows how to handle.
Data points occur-ring in the lower right-hand portion of the graph in-dicate depressed answer extraction performance asaverage precision is increasing.
This phenomenoncan be explained by the higher-precision text re-trieval module ranking answer-bearing sentencesthat answer extraction can not handle ahead ofthose that it can handle.3 Failure AnalysisThe results presented in the previous section con-firm that an improvement made to the text retrievalcomponent, in isolation, without a correspondingimprovement to the downstream answer extractionmodules, can fail to translate into a correspondingimprovement in end-to-end QA system accuracy.The increased average precision in the retrieved re-sults is coming in the form of answer-bearing sen-tences of types that the answer extraction machin-ery does not know how to handle.
To address thisgap in answer extraction coverage, it is first nec-essary to examine examples of the types of errorsmade by the OpenEphyra answer extraction mod-ule, summarized in Table 2.Question 1497, What was the original name be-fore ?The Star Spangled Banner??
is an exam-ple of a question for which OpenEphyra?s answerextraction machinery failed outright.
An answer-bearing sentence was retrieved, however, contain-ing the answer inside a quoted phrase: His poemwas titled ?Defense of Fort M?Henry?
and byNovember 1814 had been published as ?The Star-Spangled Banner?.
The expected answer type ofthis question does not match a commonly-usednamed entity type, so OpenEphrya?s named entity-based answer extractor found no candidates in thissentence.
Predicate-argument structure-based an-swer extraction fails as well because the old andnew names do not appear within the same struc-ture.
Because OpenEphyra does not include sup-port for positing quoted phrases as answer candi-dates, no answer to this question can be found de-spite the fact that an answer-bearing sentence wasretrieved.Question 1417, Who was the first person to runthe mile in less than four minutes?
is an exam-ple of a question for which average precision im-proved greatly, by 0.7208, but for which extractionquality remained the same.
The existing text re-trieval module ranks 14 sentences ahead of the firstanswer-bearing sentence, but only one contains anamed entity of type person, so despite the im-provement in retrieval quality, the correct answer5Table 2: Summary of end-to-end QA system results on the question setResult Type CountExtraction failure 42Retrieval better, extraction same 20Retrieval better, extraction worse 13Retrieval better, extraction better 10Retrieval worse, extraction better 3Retrieval worse, extraction worse 3Total 91moves up only one rank in the system output.For ten questions, extraction performance doesimprove as average precision improves.
Ques-tion 1409, Which vintage rock and roll singer wasknown as ?The Killer??
For each of these ques-tions, OpenEphyra?s existing text retrieval modulecould not rank an answer-bearing sentence highlyor retrieve one at all.
Adding the high-precisionversion of the text retrieval component solved thisproblem.
In each case, named entity-based an-swer extraction was able extract the correct an-swer.
These eleven questions range over a varietyof answer types, and have little in common exceptfor the fact that there are relatively few answer-bearing sentences in the corpus, and large numbersof documents matched by a bag-of-words queryformulated using the keyterms from the question.There are three questions for which extractionperformance degrades as retrieval performance de-grades.
Question 1463, What is the North Koreannational anthem?
is an example.
In this case,there is only one relevant sentence, and, owingto an annotation error, it has a predicate-argumentstructure that is very generic, having North Koreaas the only argument: Some of the North Koreancoaches broke into tears as the North?s anthem,the Patriotic Song, played.
The high-precision re-trieval component retrieved a large number of sen-tences matching the that predicate-argument struc-ture, but ranked the one answer-bearing sentencevery low.Some questions actually worsened in terms ofthe reciprocal rank of the correct answer when av-erage precision improved.
An example is question1504, Where is the Salton Sea?
The high-precisiontext retrieval module ranked answer-bearing sen-tences such as The combination could go a longway to removing much of the pesticides, fertilizers,raw sewage carried by the river into the SaltonSea, the largest lake in California, but a failureof the named entity recognition tool did not iden-tify California as an instance of the expected an-swer type, and therefore it was ignored.
Sen-tences describing other seas near other locationsprovided answers such as Central Asia, Russia,Turkey and Ukraine that were ranked ahead of Cal-ifornia, which was eventually extracted from an-other answer-bearing sentence.And finally, for some questions, high-precisionretrieval was more of a hindrance than a help,retrieving more noise than answer-bearing sen-tences.
A question for which this is true is ques-tion 1470, When did president Herbert Hooverdie?
The high-precision text retrieval module usesa predicate-argument structure to match the targetverb die, theme Hoover and a date instance oc-curring in a temporal adjunct.
Interestingly, thetext collection contains a great deal of die struc-tures that match partially, including those referringto deaths of presidents of other nations, and thosereferring to the death of J. Edgar Hoover, who wasnot a U.S. president but the first director of the U.S.Federal Bureau of Investigation (FBI).
False posi-tives such as these serve to push the true answerdown on the ranked list of answers coming out ofthe QA system.4 Improving Answer ExtractionThe answer extraction and selection algorithmspackaged with OpenEphyra are widely-acceptedbaselines, but are not sophisticated enough toextract answer candidates from the additionalanswer-bearing text retrieved by the high-precisiontext retrieval module, which can check linguisticand semantic constraints at query time.The named-entity answer extraction method se-lects any candidate answer that is an instance ofthe expected answer type, so long as it co-occurwith query terms.
Consider question 1467, What6year did South Dakota become a state?
Giventhat the corpus consists of newswire text report-ing on current events, years that are contempo-rary to the corpus often co-occur with the ques-tion focus, as in the following sentence, Monaghanalso seized about $87,000 from a Santee accountin South Dakota in 1997.
Of the top twenty an-swers returned for this question, all but four arecontemporary to the corpus or in the future.
Min-imal sanity-checking on candidate answers couldsave the system the embarrassment of returning adate in the future as the answer.
Going one stepfurther would involve using external sources to de-termine that 1997 is too recent to be the year a statewas admitted to the union.OpenEphyra?s predicate-argument structure-based answer extraction algorithm can avoidsome of these noisy answers by comparing someconstraints from the question against the retrievedtext and only extracting answers if the constraintsare satisfied.
Consider question 1493, When wasDavy Crockett born?
One relevant sentence saysCrockett was born Aug. 17, 1786, in what is noweastern Tennessee, and moved to Lawrenceburgin 1817.
The SRL answer extraction algorithmextracts Aug. 17, 1786 because it is located in anargument labeled argm-tmp with respect to theverb, and ignores the other date in the sentence,1817.
The named entity-based answer extractionapproach proposes both dates as answer candi-dates, but the redundancy-based answer selectionprefers 1786.The predicate-argument structure-based answerextraction algorithm is limited because it only ex-tracts arguments from text that shares the structureas the question.
The high-precision text retrievalapproach is actually able to retrieve additionalanswer-bearing sentences with different predicate-argument structures from the question, but answerextraction is not able to make use of it.
Considerthe sentence, At the time of his 100 point game withthe Philadelphia Warriors in 1962, Chamberlainwas renting an apartment in New York.
Thoughthis sentence answers the question What year didWilt Chamberlain score 100 points?, its predicate-argument structure is different from that of thequestion, and predicate-argument structure-basedanswer extraction will ignore this result because itdoes not contain a score verb.In addition to answer extraction, end-to-end per-formance could be improved by focusing on an-swer selection.
OpenEphyra does not include sup-port for sanity-checking the answers it returns,and its default answer selection mechanism isredundancy-based.
As a result, nonsensical an-swers are occasionally retrieved, such as moonfor question 1474, What is the lowest point onEarth?
Sophisticated approaches, however, do ex-ist for answer validation and justification, includ-ing use of resources such as gazetteers and ontolo-gies (Buscaldi and Rosso, 2006), Wikipedia (Xuet al, 2002), the Web (Magnini et al, 2002), andcombinations of the above (Ko et al, 2007).5 ConclusionsThis paper set out to challenge the assumption ofcompositionality in pipelined modular QA systemsthat suggests that an improvement in an individualmodule should lead to an improvement in the over-all end-to-end system performance.
An attemptwas made to validate the assumption by showingan improvement in the end-to-end system accu-racy of an off-the-shelf QA system by substitut-ing its existing text retrieval component for a high-precision retrieval component capable of checkinglinguistic and semantic constraints at query time.End-to-end system accuracy remained roughly un-changed because the downstream answer extrac-tion components were not able to extract answersfrom the types of the answer-bearing sentences re-turned by the improved retrieval module.The reality of QA systems is that there is ahigh level of coupling between the different systemcomponents.
Ideally, text retrieval should have anunderstanding of the kinds of results that answerextraction is able to utilize to extract answers, andshould not offer text beyond the capabilities of thedownstream modules.
Similarly, question analy-sis and answer extraction should be agreeing ona common representation for what constitutes ananswer to the question so that answer extractioncan use that information to locate answers in re-trieved text.
When a retrieval module is availablethat is capable of making use of the semantic rep-resentation of the answer, it should do so, but an-swer extraction needs to know what it can assumeabout incoming results so that it does not have tore-check constraints already guaranteed to hold.The coupling between text retrieval and answerextraction is important for a QA system to per-form well.
Improving the quality of text retrievalis essential because once the likely location of7the answer is narrowed down to a subset of thetext collection, anything not retrieved text can notbe searched for answers in downstream modules.Equally important is the role of answer extraction.Even the most relevant retrieved text is useless toa QA system unless answers can be extracted fromit.
End-to-end QA system performance can notbe improved by improving text retrieval qualityin isolation.
Improvements in answer extractionmust keep pace with progress on text retrieval tech-niques to reduce errors resulting from a mismatchin capabilities.
Going forward, research on the lin-guistic and semantic constraint-checking capabili-ties of text retrieval systems to support the QA taskcan drive research in answer extraction techniques,and in QA systems in general.ReferencesBikel, D., R. Schwartz, and R. Weischedel.
1999.
Analgorithm that learns what?s in a name.
MachineLearning, 34(1?3):211?231.Bilotti, M., B. Katz, and J. Lin.
2004.
What works bet-ter for question answering: Stemming or morpholog-ical query expansion?
In Proceedings of the Infor-mation Retrieval for Question Answering (IR4QA)Workshop at SIGIR 2004.Bilotti, M., P. Ogilvie, J. Callan, and E. Nyberg.
2007.Structured retrieval for question answering.
In Pro-ceedings of the 30th Annual International ACM SI-GIR Conference on Research and Development in In-formation Retrieval.Buscaldi, D. and P. Rosso.
2006.
Mining knowledgefrom wikipedia for the question answering task.
InProceedings of the International Conference on Lan-guage Resources and Evaluation.Cui, H., R. Sun, K. Li, M. Kan, and T. Chua.
2005.Question answering passage retrieval using depen-dency relations.
In Proceedings of the 28th AnnualInternational ACM SIGIR Conference on Researchand Development in Information Retrieval.Graff, D. 2002.
The AQUAINT Corpus of EnglishNews Text.
Linguistic Data Consortium (LDC).
Cat.No.
LDC2002T31.Hirschman, L. and R. Gaizauskas.
2001.
Naturallanguage question answering: The view from here.Journal of Natural Language Engineering, SpecialIssue on Question Answering, Fall?Winter.Kingsbury, P., M. Palmer, and M. Marcus.
2002.Adding semantic annotation to the penn treebank.
InProceedings of the 2nd International Conference onHuman Language Technology Research (HLT 2002).Ko, J., L. Si, and E. Nyberg.
2007.
A probabilisticgraphical model for joint answer ranking in questionanswering.
In Proceedings of the 30th Annual Inter-national ACM SIGIR Conference on Research andDevelopment in Information Retrieval.Lin, J. and B. Katz.
2006.
Building a reusable test col-lection for question answering.
Journal of the Amer-ican Society for Information Science and Technol-ogy, 57(7):851?861.Magnini, B., M. Negri, R. Pervete, and H. Tanev.
2002.Comparing statistical and content-based techniquesfor answer validation on the web.
In Proceedings ofthe VIIIo Convegno AI*IA.Narayanan, S. and S. Harabagiu.
2004.
Question an-swering based on semantic structures.
In Proceed-ings of the 20th international conference on Compu-tational Linguistics.Pradhan, S., W. Ward, K. Hacioglu, J. Martin, andD.
Jurafsky.
2004.
Shallow semantic parsing usingsupport vector machines.
In Proceedings of the Hu-man Language Technology Conference of the NorthAmerican Chapter of the Association for Computa-tional Linguistics (HLT-NAACL 2004).Reynar, J. and A. Ratnaparkhi.
1997.
A maximum en-tropy approach to identifying sentence boundaries.In Proceedings of the Fifth Conference on AppliedNatural Language Processing.Schlaefer, N., P. Gieselmann, and G. Sautter.
2006.The ephyra qa system at trec 2006.
In Proceedingsof the Fifteenth Text REtrieval Conference (TREC).Schlaefer, N., J. Ko, J. Betteridge, G. Sautter,M.
Pathak, and E. Nyberg.
2007.
Semantic exten-sions of the ephyra qa system for trec 2007.
In Pro-ceedings of the Sixteenth Text REtrieval Conference(TREC).Strohman, T., D. Metzler, H. Turtle, and W. B. Croft.2005.
Indri: A language model-based search enginefor complex queries.
In Proceedings of the Interna-tional Conference on Intelligence Analysis.Sun, R., J. Jiang, Y. Tan, H. Cui, T. Chua, and M. Kan.2005.
Using syntactic and semantic relation analysisin question answering.
In Proceedings of the Four-teenth Text REtrieval Conference (TREC-14).Voorhees, E., N. Gupta, and B. Johnson-Laird.
1994.The collection fusion problem.
In Proc.
of TREC-3.Xu, J., A. Licuanan, J.
May, S. Miller, andR.
Weischedel.
2002.
Trec 2002 qa at bbn: Answerselection and confidence estimation.
In Proceedingsof the Text REtrieval Conference.8
