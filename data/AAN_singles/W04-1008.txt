Task-focused Summarization of EmailSimon Corston-Oliver, Eric Ringger, Michael Gamon and Richard CampbellMicrosoft ResearchOne Microsoft WayRedmond, WA 98052 USA{simonco, ringger, mgamon, richcamp}@microsoft.comAbstractWe describe SmartMail, a prototype system forautomatically identifying action items (tasks) inemail messages.
SmartMail presents the user witha task-focused summary of a message.
Thesummary consists of a list of action items extractedfrom the message.
The user can add these actionitems to their ?to do?
list.1 IntroductionEmail for many users has evolved from a merecommunication system to a means of organizingworkflow, storing information and tracking tasks(i.e.
?to do?
items) (Bellotti et al, 2003; Cadiz etal., 2001).
Tools available in email clients formanaging this information are often cumbersomeor even so difficult to discover that users are notaware that the functionality exists.
For example, inone email client, Microsoft Outlook, a user mustswitch views and fill in a form in order to create atask corresponding to the current email message.By automatically identifying tasks that occur in thebody of an email message, we hope to simplify theuse of email as a tool for task creation andmanagement.In this paper we describe SmartMail, a prototypesystem that automatically identifies tasks in email,reformulates them, and presents them to the user ina convenient interface to facilitate adding them to a?to do?
list.SmartMail performs a superficial analysis of anemail message to distinguish the header, messagebody (containing the new message content), andforwarded sections.
1  SmartMail breaks the1  This simple division into header, message body, andforwarded sections was sufficient for the corpus of emailmessages we considered.
Messages containing originalmessages interleaved with new content were extremelymessage body into sentences, then determinesthe speech act of each sentence in the messagebody by consulting a machine-learned classifier.If the sentence is classified as a task, SmartMailperforms additional linguistic processing toreformulate the sentence as a task description.This task description is then presented to theuser.2 DataWe collected a corpus of 15,741 emailmessages.
The messages were divided intotraining, development test and blind test.
Thetraining set contained 106,700 sentences inmessage bodies from 14,535 messages.
Toavoid overtraining to individual writing styles,we limited the number of messages from agiven sender to 50.
To ensure that ourevaluations are indicative of performance onmessages from previously unencounteredsenders, we selected messages from 3,098senders, assigning all messages from a givensender to either the training or the test sets.Three human annotators labeled the messagebody sentences, selecting one tag from thefollowing set: Salutation, Chit-chat (i.e., socialdiscussion unrelated to the main purpose of themessage), Task, Meeting (i.e., a proposal tomeet), Promise, Farewell, various componentsof an email signature (Sig_Name, Sig_Title,Sig_Affiliation, Sig_Location, Sig_Phone,Sig_Email, Sig_URL, Sig_Other), and thedefault category ?None of the above?.
The set oftags can be considered a set of application-specific speech acts analogous to the ratherparticular tags used in the Verbmobil project,such as ?Suggest_exclude_date?
anduncommon in our corpus.
Most senders were usingMicrosoft Outlook, which places the insertion point fornew content at the top of the message.?Motivate_appointment?
(Warnke et al, 1997;Mast et al, 1996) or the form-based tags of Stolckeet al (1998).All three annotators independently labeledsentences in a separate set of 146 messages notincluded in the training, development or blind testsets.
We measured inter-annotator agreement forthe assignment of tags to sentences in the messagebodies using Cohen?s Kappa.
Annotator 1 andannotator 2 measured 85.8%; annotator 1 andannotator 3 measured 82.6%; annotator 2 andannotator 3 measured 82.3%.
We consider thislevel of inter-annotator agreement good for a novelset of application-specific tags.The development test and blind test sets ofmessages were tagged by all three annotators, andthe majority tag for each sentence was taken.
If anysentence did not have a majority tag, the entiremessage was discarded, leaving a total of 507messages in the development test set and 699messages in the blind test set.The set of tags was intended for a series ofrelated experiments concerning linguisticprocessing of email.
For example, greetings andchit-chat could be omitted from messagesdisplayed on cell phones, or the components of anemail signature could be extracted and stored in acontact database.
In the current paper we focusexclusively on the identification of tasks.Annotators were instructed to mark a sentenceas containing a task if it looked like an appropriateitem to add to an on-going ?to do?
list.
By thiscriterion, simple factual questions would notusually be annotated as tasks; merely respondingwith an answer fulfills any obligation.
Annotatorswere instructed to consider the context of an entiremessage when deciding whether formulaic endingsto email such as Let me know if you have anyquestions were to be interpreted as mere socialconvention or as actual requests for review andcomment.
The following are examples of actualsentences annotated as tasks in our data:Since Max uses a pseudo-random number generator, youcould possibly generate thesame sequence of numbers toselect the same cases.Sorry, yes, you would have toretrain.An even fast [sic] thingwould be to assign your ownID as a categorical feature.Michael, it?d be great ifyou could add some stuff reMSRDPS.Could you please remotedesktop in and try runningit on my machine.If CDDG has its own notionof what makes for goodresponses, then we shoulduse that.3 FeaturesEach sentence in the message body is describedby a vector of approximately 53,000 features.The features are of three types: properties of themessage (such as the number of addressees, thetotal size of the message, and the number offorwarded sections in the email thread),superficial features and linguistic features.The superficial features include wordunigrams, bigrams and trigrams as well ascounts of special punctuation symbols (e.g.
@,/, #), whether the sentence contains words withso-called ?camel caps?
(e.g., SmartMail),whether the sentence appears to contain thesender?s name or initials, and whether thesentence contains one of the addressees?
names.The linguistic features were obtained byanalyzing the given sentence using the NLPWinsystem (Heidorn 2000).
The linguistic featuresinclude abstract lexical features, such as part-of-speech bigrams and trigrams, and structuralfeatures that characterize the constituentstructure in the form of context-free phrasestructure rewrites (e.g., DECL:NP-VERB-NP;i.e., a declarative sentence consisting of a nounphrase followed by a verb and another nounphrase).
Deeper linguistic analysis yieldedfeatures that describe part-of-speechinformation coupled with grammatical relations(e.g., Verb-Subject-Noun indicating a nominalsubject of a verb) and features of the logicalform analysis such as transitivity, tense andmood.4 ResultsWe trained support vector machines (SVMs)(Vapnik, 1995) using an implementation of thesequential minimal optimization algorithm(Platt, 1999).
We trained linear SVMs, whichhave proven effective in text categorization withlarge feature vectors (Joachims, 1998; Dumais etal., 1998).Figure 1 illustrates the precision-recall curve forthe SVM classifier trained to distinguish tasks vs.non-tasks measured on the blind test set.We conducted feature ablation experiments onthe development test set to assess the contributionof categories of features to overall classificationperformance.
In particular we were interested inthe role of linguistic analysis features compared tousing only surface features.
Within the linguisticfeatures, we distinguished deep linguistic features(phrase structure features and semantic features)from POS n-gram features.
We conductedexperiments with three feature sets:1. all features (message level features + wordunigram, bigram and trigram2.
features + POS bigram and trigramfeatures + linguistic analysis features)3. no deep linguistic features (no phrasestructure or semantic features)4. no linguistic features at all (no deeplinguistic features and no POS n-gramfeatures)Based on these experiments on the developmenttest set, we chose the feature set used for our run-time applications.Figure 1 shows final results for these featuresets on the blind test set: for recall betweenapproximately 0.2 and 0.4 and betweenapproximately 0.5 and 0.6 the use of all featuresproduces the best results.
The distinctionbetween the ?no linguistic features?
and ?nodeep linguistic features?
scenarios is negligible;word n-grams appear to be highly predictive.Based on these results, we expect that forlanguages where we do not have an NLPWinparser, we can safely exclude the deeperlinguistic features and still expect goodclassifier performance.Figure 2 illustrates the accuracy ofdistinguishing messages that contain tasks fromthose that do not, using all features.
A messagewas marked as containing a task if it containedat least one sentence classified as a task.
Sinceonly one task has to be found in order for theentire message to be classified as containing atask, accuracy is substantially higher than on aper-sentence basis.
In section 6, we discuss thescenarios motivating the distinction betweensentence classification and messageclassification.0.00.10.20.30.40.50.60.70.80.91.00.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0RecallPrecisionAll featuresNo deep linguistic featuresNo linguistic featuresFigure 1: Precision-Recall curves for ablation experiments0.00.10.20.30.40.50.60.70.80.91.00.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0RecallPrecisionPer sentencePer messageFigure 2: Precision-Recall curves comparing message classification and sentence classification5 Reformulation of TasksSmartMail performs post-processing of sentencesidentified as containing a task to reformulate themas task-like imperatives.
The process ofreformulation involves four distinct knowledge-engineered steps:1.
Produce a logical form (LF) for theextracted sentence (Campbell and Suzuki,2001).
The nodes of the LF correspond tosyntactic constituents.
Edges in the LFrepresent semantic and deep syntacticrelations among nodes.
Nodes bearsemantic features such as tense, numberand mood.2.
Identify the clause in the logical form thatcontains the task; this may be the entiresentence or a subpart.
We consider suchlinguistic properties as whether the clauseis imperative, whether its subject is secondperson, and whether modality words suchas please or a modal verb are used.
Allparts of the logical form not subsumed bythe task clause are pruned.3.
Transform the task portion of the LF toexclude extraneous words (e.g.
please,must, could), extraneous subordinateclauses, adverbial modifiers, and vocativephrases.
We replace certain deicticelements (i.e., words or phrases whosedenotation varies according to the writer orthe time and place of utterance) with non-deictic expressions.
For example, firstperson pronouns are replaced by either thename of the sender of the email or by athird person pronoun, if such a pronounwould unambiguously refer to the sender.Similarly, a temporal expression such asThursday, which may refer to a differentdate depending on the week in which it iswritten, is replaced by an absolute date(e.g., 4/1/2004).4.
Pass the transformed LF to a sentencerealization module to yield a string(Aikawa et al, 2001).Below we illustrate the reformulation of tasks withsome examples from our corpus.Example 1:On the H-1 visa issue, I ampositive that you need to goto the Embassy in London toget your visa stamped intoyour passport.Reformulation:Go to the Embassy in London toget your visa stamped intoyour passport.In this example, the embedded sententialcomplement, that is, the part of the sentencefollowing positive, is selected as the part of thesentence containing the task, because of the modalverb need and the second person subject; only thatpart of the sentence gets reformulated.
The modalverb and the second person subject are deleted toform an imperative sentence.Example 2:Can you please send me thefollow up information for thedemo(s) listed in this EmailASAP.Reformulation:Send Kendall the follow upinformation for the demolisted in this Email ASAP.In this example, the whole sentence is selectedas containing the task (modal verb, second personsubject); modal elements including please aredeleted along with the second person subject toform an imperative.
In addition, the first personpronoun me is replaced by a reference to thesender, Kendall in this instance.Example 3:I've been Wednesday at thelecture on Amalgam you gave inthe 113/1021 Room (which Ireally liked), and I've beenwondering how feasible wouldit be to use Amalgam forlearning requirements or codecorpus structures and rules(and eventually rephrase themin some way).Reformulation:On June 5, 2002 Pablo wrote:?I've been Wednesday at thelecture on Amalgam you gave inthe 113/1021 Room (which Ireally liked), and I've beenwondering how feasible wouldit be to use Amalgam forlearning requirements or codecorpus structures and rules(and eventually rephrase themin some way).
'This example illustrates what happens whenNLPWin is unable to produce a spanning parse andhence a coherent LF; in this case NLPWinmisanalyzed the clause following wondering as amain clause, instead of correctly analyzing it as acomplement clause.
SmartMail?s back-off strategyfor non-spanning parses is to enclose the entireoriginal sentence in quotes, prefixed with a matrixsentence indicating the date and the name of thesender.6 Task-Focused SummarizationWe have considered several scenarios forpresenting the tasks that SmartMail identifies.Under the most radical scenario, SmartMail wouldautomatically add extracted tasks to the user?s ?todo?
list.
This scenario has received a fairlynegative reception when we have suggested it topotential users of a prototype.
From an applicationperspective, this scenario is ?fail hard?
; i.e.,classification errors might result in garbage beingadded to the ?to do?
list, with the result that theuser would have to manually remove items.
Sinceour goal is to reduce the workload on the user, thisoutcome would seem to violate the maxim ?First,do no harm?.Figure 3 and Figure 4 illustrate several ideas forpresenting tasks to the user of Microsoft Outlook.Messages that contain tasks are flagged, using theexisting flag icons in Outlook for proof of concept.Users can sort mail to see all messages containingtasks.
This visualization amounts to summarizingthe message down to one bit, i.e., +/- Task, and isconceptually equivalent to performing documentclassification.The right-hand pane in Figure 3 is magnified asFigure 4 and shows two more visualizations.
At thetop of the pane, the tasks that have been identifiedare presented in one place, with a check box besidethem.
Checking the box adds the task to the Tasksor ?to do?
list, with a link back to the originalmessage.
This presentation is ?fail soft?
: the usercan ignore incorrectly classified tasks, or tasks thatwere correctly identified but which the user doesnot care to add to the ?to do?
list.
This list of tasksamounts to a task-focused summary of thedocument.
This summary is intended to be read asa series of disconnected sentences, thus side-stepping the issue of producing a coherent textfrom a series of extracted sentences.
In the eventthat users prefer to view these extracted sentencesas a coherent text, it may prove desirable toattempt to improve the textual cohesion by usinganaphoric links, cue phrases and so on.Finally, Figure 3 also shows tasks highlighted incontext in the message, allowing the user to skimthe document and read the surrounding text.In the prototype we allow the user to vary theprecision and recall of the classifier by adjusting aslider (not illustrated here) that sets the probabilitythreshold on the probability of Task.Figure 3 and Figure 4 illustrate a convention thatwe observed in a handful of emails: proper namesoccur as section headings.
These names have scopeover the tasks enumerated beneath them, i.e.
thereis a list of tasks assigned to Matt, a list assigned toEric or Mo, and a list assigned to Mo.
SmartMaildoes not currently detect this explicit assignmentof tasks to individuals.Important properties of tasks beyond the text ofthe message could also be automatically extracted.For example, the schema for tasks in Outlookincludes a field that specifies the due date of thetask.
This field could be filled with date and timeinformation extracted from the sentence containingthe task.
Similarly the content of the sentencecontaining the task or inferences about socialrelationships of the email interlocutors could beused to mark the priority of tasks as High, Low, orNormal in the existing schema.7 ConclusionIn this paper we have presented aspects ofSmartMail, which provides a task-orientedsummary of email messages.
This summary isproduced by identifying the task-related sentencesin the message and then reformulating each task-related sentence as a brief (usually imperative)summation of the task.
The set of tasks extractedand reformulated from a given email message isthus a task-focused summary of that message.We plan to conduct user studies by distributingthe prototype as an Outlook add-in to volunteerswho would use it to read and process their ownmail over a period of several weeks.
We intend tomeasure more than the precision and recall of ourclassifier by observing how many identified tasksusers actually add to their ?to do?
list and byadministering qualitative surveys of usersatisfaction.The ability to reformulate tasks is in principleseparate from the identification of tasks.
In ourplanned usability study we will distribute variantsof the prototype to determine the effect ofreformulation.
Do users prefer to be presented withthe extracted sentences with no additionalprocessing, the tasks reformulated as described inSection 5, or an even more radical reformulation toa telegraphic form consisting of a verb plus object,such as Send information or Schedule subjects?Figure 3: Prototype system showing ways of visualizing tasksFigure 4: Magnified view of prototype system showing message with enumerated tasks8  AcknowledgementsMany of the ideas presented here were formulatedin discussion with Bob Atkinson, Dave Reed andMalcolm Pearson.
Our thanks go to JeffStevenson, Margaret Salome and Kevin Gaughenfor annotating the data.ReferencesAikawa, Takako, Maite Melero, Lee Schwartz andAndi Wu.
2001.
Multilingual natural languagegeneration.
EAMT.Bellotti, Victoria, Nicolas Ducheneaut, MarkHoward , Ian Smith.
2003.
Taking email totask: the design and evaluation of a taskmanagement centered email tool.
Proceedingsof the conference on human factors incomputing systems, pages 345-352.Cadiz, J. J., Dabbish, L., Gupta, A., & Venolia, G.D. 2001.
Supporting email workflow.
MSR-TR-2001-88: Microsoft Research.Campbell, Richard and Hisami Suzuki.
2002.Language neutral representation of syntacticstructure.
Proceedings of SCANALU 2002.Dumais, Susan, John Platt, David Heckerman,Mehran Sahami 1998: Inductive learningalgorithms and representations for textcategorization.
Proceedings of CIKM-98, pages148-155.Heidorn, George.
2000.
Intelligent writingassistance.
In R. Dale, H. Moisl and H.
Somers,(eds.
), Handbook of Natural LanguageProcessing.
Marcel Dekker.Joachims, Thorsten.
1998.
Text categorizationwith support vector machines: Learning withmany relevant features.
Proceedings of ECML1998, pages 137-142.Mast, M., Kompe, R., Harbeck, S., Kiessling, A.,Niemann, H., N?th, E., Schukat-Talamazzini,E.
G. and Warnke., V. 1996.
Dialog actclassification with the help of prosody.
ICSLP96.Platt, John.
1999.
Fast training of SVMs usingsequential minimal optimization.
In B.Schoelkopf, C. Burges and A. Smola (eds.
)Advances in Kernel Methods: Support VectorLearning, pages 185-208, MIT Press,Cambridge, MA.Stolcke, A., E. Shriberg, R. Bates, N. Coccaro, D.Jurafsky, R. Martin, M. Meteer, K. Ries, P.Taylor and C. Van Ess-Dykema.
1998.
Dialogact modeling for conversational speech.Proceedings of the AAAI-98 Spring Symposiumon Applying Machine Learning to DiscourseProcessing.Vapnik, V. 1995.
The Nature of StatisticalLearning Theory.
Springer-Verlag, New York.Warnke, V., R. Kompe, H. Niemann and E. N?th.1997.
Integrated dialog act segmentation andclassification using prosodic features andlanguage models.
Proc.
European Conf.
onSpeech Communication and Technology, vol 1,pages 207?210.
