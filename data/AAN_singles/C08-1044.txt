Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008), pages 345?352Manchester, August 2008Modeling Chinese Documentswith Topical Word-Character ModelsWei Hu1           Nobuyuki Shimizu21Department of Computer ScienceShanghai Jiao Tong UniversityShanghai, China 200240{no_bit,hysheng}@sjtu.edu.cnHiroshi Nakagawa2           Huanye Sheng12Information Technology CenterThe University of TokyoTokyo, Japan 113-0033{shimizu, nakagawa}@r.dl.itc.u-tokyo.ac.jpAbstractAs Chinese text is written without wordboundaries, effectively recognizing Chi-nese words is like recognizing colloca-tions in English, substituting charactersfor words and words for collocations.However, existing topical models that in-volve collocations have a common limi-tation.
Instead of directly assigning a top-ic to a collocation, they take the topic ofa word within the collocation as the topicof the whole collocation.
This is unsatis-factory for topical modeling of Chinesedocuments.
Thus, we propose a topicalword-character model (TWC), which al-lows two distinct types of topics: wordtopic and character topic.
We evaluatedTWC both qualitatively andquantitatively to show that it is a power-ful and a promising topic model.1 IntroductionTopic models (Blei et al, 2003; Griffiths &Steyvers 2004, 2007) are a class of statisticalmodels in which documents are expressed asmixtures of topics, where a topic is a probabilitydistribution over words.
A topic model is a gen-erative model for documents: it specifies a prob-abilistic procedure for generating documents.
Tomake a new document, we choose a distributionover topics.
Then, for each word in this docu-ment, we randomly select a topic from the distri-bution, and draw a word from the topic.
Once wehave a topic model, we can invert the generatingprocess, inferring the set of topics that was re-sponsible  for  generating  a  collection  of  docu-?
2008.
Licensed under the Creative Commons Attri-bution-Noncommercial-Share Alike 3.0 Unportedlicense (http://creativecommons.org/licenses/by-nc-sa/3.0/).
Some rights reserved.ments.Although most topic models treat a documentas a bag-of-words, the assumption has obviousshortcomings.
Suppose there are many docu-ments about art and musicals in New York.
Then,we find a topic represented by words such as?art?, ?musical?, and ?New?, instead of getting?New York?.
The bag-of-words assumptionmakes the topic model split a collocation?aphrase with meaning beyond the individualwords?into individual words that have a differ-ent meaning.
One example of a collocation is thephrase ?white house?.
In politics, it carries a spe-cial meaning beyond a house that is white,whereas ?yellow house?
does not.While it is reasonable for English, the equiva-lent bag-of-characters assumption is especiallytroublesome for modeling Chinese documents,where almost all basic vocabularies are the equi-valents of English collocations.
In Chinese, someof the most commonly used couple-of-thousandcharacters are combined to make up a word, andno word boundary is given in the text.Effectively, Chinese words are like collocationsin English.
The difficulty is that there are over-whelmingly more of them, enough to render abag-of-character assumption unreasonable forChinese.
Therefore, a topical model for Chineseshould be capable of detecting the boundary be-tween two words, as well as assigning a topic toeach word.While topic models for Chinese documentsbear some similarity to collocation models inEnglish, existing topical collocation discoverymodels, such as the LDA (latent Dirichelt alloca-tion) Collocation model (LDACOL) (Griffiths etal., 2007) and the topical N-gram model (TNG)(Wang et al, 2007), do not directly assign a topicto a collocation.
These models find the bounda-ries of phrases and assign a topic to each word.The problem is in the next step?the topic of thecollocation is exactly the same as one of thewords.
This is like saying that the topic of ?white345house?
is the same as either that of ?white?
or?house?.
We propose a new topical model, thetopical word-character model (TWC), whichaims to overcome these limitations.We evaluated the model both quantitativelyand qualitatively.
For the quantitative analysis,we compared the performance of TWC and TNGusing a standard measure perplexity.
For thequalitative analysis, we evaluated TWC?s abilityto discover Chinese words and assign topics incomparison with TNG.The rest of the paper is organized as follows.Section 2 reviews topic models that aim to in-clude collocations explicitly in the model andanalyzes their limitations.
Section 3 presents ournew model TWC.
Section 4 gives details of ourconsideration on inference for TWC.
Section 5presents our qualitative and quantitative experi-ments.
Section 6 concludes with a summary andbriefly mentions future work.2 Topic Models for Collocation Discov-erySince Chinese word discovery is similar toEnglish collocation discovery, we first reviewsome related topic models for collocationdiscovery.Although collocation discovery has long beenstudied, most methods are based on frequency orvariance.
LDACOL is an attempt to model collo-cations in a topical scheme.
Starting from theLDA topic model, LDACOL introduces specialrandom variables xG .
Variable xi = 1 implies thatthe  corresponding  word  wi  and  previous  wordwi-1 belong to the same phrase, while xi = 0 im-plies otherwise.
Thus, LDACOL can decide thelength of a phrase dynamically.TNG is a powerful generalization of LDACOL.Its graphical model is shown in Figure 1.Figure 1: Topical n-gram model.The model is defined in terms of three sets ofvariables: a sequence of words wG, a sequence oftopics zG, and a sequence of indicators xG.
TNGassumes the following generative process fordocuments.1.
For each document d, draw ?d ~Dirichlet(?).2.
For each topic z, draw ?z ~ Dirichlet(?).3.
For each topic z and each word w, draw ?zw~ Dirichlet(?).4.
For each topic z and each word w, draw ?zw~ Beta(?).5.
For each word wd,,i in document d:(a) draw xd,,i ~ Bernoulli(d ,i 1 d ,i 1z ,w?
?
?
),(b) draw zd,,i ~ Discrete(?d),(c) draw wd,,i ~ Discrete(d ,iz? )
if xd,,i=0,draw wd,,i ~ Discrete(d ,i d ,i 1z ,w?
? )
if xd,,i=1,where ?, ?, ?
are Dirichlet priors and ?
is a Betaprior, zd,i denotes the ith topic assignment indocument d, wd,i denotes the ith word in documentd, and xd,i denotes the indicator between wd,i-1 andwd,i.
Note that the variable xd,i = 1 implies thatword wd,i-1 and its neighbor wd,i belong to thesame phrase, while xd,i = 0 implies otherwise.However, the topics assigned to them (zd,i-1 andzd,i) are not required to be identical to each other.To decide the topic of a phrase, we can simplytake the first (or last) word?s topic or the mostcommon topic in the phrase.
The authors of TNGprefer to choose the last word?s topic as thephrase topic because the last noun in a colloca-tion is usually the ?head noun?
in English.However, this simple strategy may be ineffec-tive when we apply TNG to Chinese documents.The topics of ????
(game) and ?????
(tournament) should be represented by their lastcharacters while those of ????
(farmer) and????
(agriculture) should be represented bytheir first characters.
And occasionally, the topicof a Chinese word is not identical to any topic ofits component characters.
For example ????
(Bluetooth) is neither a color nor a tooth.To overcome the limitation of TNG, we mustdiscard its underlying assumption: that the topicof a whole word is the same as the topic of atleast one of its components.3 Modeling Word Topic and CharacterTopicThis section describes our topical word-charactermodel (TWC), which models two distinct typesof topics: word topic and character topic.?
??
??
????????????????????
?zi+1 zi+2 zi zi-1xi+2 xi+3xi-1xi xi-1wi+1 wi+2 wi wi-13463.1 Word topic and character topicTo solve the problem associated with the ????
(Bluetooth) example, we need to distinguishbetween the topics of characters and words.Therefore, we introduce a new type of topic forwords in addition to the topics assigned to char-acters.
When generating a Chinese character, wefirst draw a word topic and then choose a charac-ter topic.
A schematic description of this modelis shown in Figure 2.Figure 2: Schematic description of modelingChinese documents with character and word top-ics.Here, we use random variables zGand tGtodenote word and character topics, respectively.Note that the word topic and character topicshave a hierarchical tree-like structure (upperlayer in Figure 2), whereas character topics andcharacters form a hidden Markov model (HMM)(lower layer in Figure 2).3.2 Topical word-character model (TWC)There are some indicators in the upper-rightcorner of each character topic in Figure 2.
Theyhelp us to tell whether the current character be-longs to the same word as the previous one.
Nowthe question left is how to probabilistically drawthese indicators, i.e., how to determine the lengthof the Markov chain.There are two ways to set the values of the in-dicators.
One is similar to that applied in the hid-den semi-Markov model (HSMM), which gener-ates the duration of a segment from the state.
Ac-cordingly, we could first choose the length of aword from the distribution associated with theword topic and then assign 0 or 1 to each indica-tor.
The other method is to directly draw indica-tors from the distribution associated with theprevious character and topic, just as LDACOLand TNG do.
The difference between these twomethods is that the former determines the lengthof a word in advance while the latter increasesthe length dynamically.We  prefer the  second choice  because it takesinto consideration a lot of context information.
Infact, our experimental results indicate that it hasbetter performance.The formal definition of our model with wordand character topics is as follows.Figure 3: Topical word-character model.TWC has four sets of variables: a sequence ofcharacters cG, a sequence of character topics tG, asequence of word topics zG, and a sequence ofindicators xG.
A document is generated via thefollowing procedure.1.
For each document d, draw ?d ~Dirichlet(?);2.
For each word topic z, draw ?z ~Dirichlet(?);3.
For each word topic z and each charactertopic t, draw ?zt ~ Dirichlet(?);4.
For each word topic z, each character topic tand each character c, draw ?ztc ~ Beta(?);5.
For each character topic t, draw ?t ~Dirichlet(?);6.
For each character cd,,i in document d:(a) draw xd,,i ~ Bernoulli( ?
?
?d ,i 1 d ,i 1 d ,i 1z ,t ,c?
);(b) draw zd,,i ~ Discrete (?d)  if xd,,i=0;zd,,i= zd,,i-1    if xd,,i=1;(c) draw td,,i ~ Discrete(d ,iz? )
if xd,,i=0;draw td,,i ~ Discrete( ?d ,i d ,i 1z ,t? )
if xd,,i=1;(d) draw cd,,i ~ Discrete(d ,it?
).Here, ?, ?, ?, ?
are Dirichlet priors and ?
is aBeta prior, zd,i denotes the ith word topic assignmentin document d, td,i denotes the ith character topic as-signment in document d, cd,i denotes the ith characterin document d, and xd,i denotes the indicator betweencd,i-1 and cd,i.Note that compared with the schematic modelin Figure 2, each character has its correspondingCharactertopicWordtopic?z2 z1t13 t21 t12 t11c13 c21 c12 c11t22c221 1 0 1 0?
??
?????
?
????????????????????????
?zi+1 zi+2 zizi-1ti+1 ti+2 titi-1ci+1 ci+2 cici-1xi+2 xi+3 xi-1xixi-1347word topic in the TWC model.
This is becausewe cannot decide how many words there will bein a document and how many characters therewill be in a certain word in advance.
In otherwords, the structure of the ideal model is notfixed.
Therefore, we duplicate word topic vari-ables for each character.4 Inference with TWCMany approximate inference techniques such asvariational methods, expectation propagation,and Gibbs sampling can be applied to graphicalmodels.
We use Gibbs sampling to perform ourBayesian inference in TWC.Gibbs sampling is a simple and widely appli-cable Markov chain Monte Carlo (MCMC) algo-rithm.
In a traditional procedure, variables aresequentially sampled from their distributionsconditioned on all other variables in the model.An extension of the basic approach is tochoose blocks of variables first and then samplejointly from the variables in each block in turn,conditioned on the remaining variables; this iscalled blocking Gibbs sampling.When sampling for TWC, we separate vari-ables into three types of blocks in the followingmanner (as shown in Figure 4).1. character variables ti2.
indicators xi, whose value is 1 after n itera-tions3.
word topics zi, zi+1, ?, zi+l-1 and indicator xi,satisfying xi=xi+l=1 and xj=0 (j from i to i+l-1)after n iterationsFigure 4: Illustration of partition in a certain it-eration.Note that variables , , ,?
?
?
?G G GGand ?Gare notsampled.
This is because we can integrate themout according to their conjugate priors.
We onlyneed to sample variables zG, xG, and tG.Before discussing the inference of conditionalprobabilities, let us analyze our partition strategyin detail.
We will explain the reasons for (1)sampling zi, zi+1, ?, zi+l-1 together and (2) sam-pling zGand xi together1.
Why do we sample zi, zi+1, ?, zi+l-1 together?Assume that we draw zi, zi+1, ?, zi+l-1 one byone, and it is now time to sample zi+1 accordingto the conditional probability( )( | , , , , , , , , )i 1 i 1P z j z x t c ?
?
?
?
?+ ?
+= GG GG ,where ( )i 1z?
+Gdenotes a word topic except i 1z + .Recall step 6-b in the generative TWC model: itsays ?If xd,,i=1, then zd,,i= zd,,i-1?, which implies( | , ) ( )i 1 i i 1 i 1 iP z z x 1 I z z+ + += = = .As this probability is a factorial of the targetprobability, it follows that( )( | , , , , , , , , )i 1 i 1P z j z x t c ?
?
?
?
?
0+ ?
+= =GG GGfor all ij z?
.
In other words, zi+1 should be equalto zi and not change during sampling.It seems that step 6-b in the generative modelcauses the problem.
But supposing that we do notset zi+1 to zi; it is still more reasonable to samplezGtogether.
According to our partition principle,xi, xi+1, ?, xi+l-1, xi+l is a continuous indicatorsequence whose head and tail are both 0 and therest are 1, which implies that character string ci,ci+1, ?, ci+l-1 forms a word and has the sameword topic.
Recall the schematic model in Figure2: the word topic and character topics have atree-like structure and each word has only oneword topic node.
We add some auxiliary dupli-cates just because the ideal model is not fixed.Therefore, it is natural to sample the word topictogether with its duplicates.2.
Why do we sample zGand xi together?Let us consider the probability of converting xifrom 0 to 1 in the current sampling iteration.
As-sume that the number of word topics is 3, zi-1=2,and( ... | ) / ( ) ,( | ... ) / ( ) ,i i l 1 ii i i l 1P z z j x 0 1 3 1 j 3P x k z z 2 1 2 0 k 1+ ?+ ?= = = = = ?
?= = = = = ?
?where other variables and priors are omitted.
Ifwe first sample zGand next sample xi, then theprobability of drawing 1 for xi is 1/6, accordingto the multiplication principle.
If we sample zGand xi together, the probability of drawing 1 for xiis ( ... , )i i l 1 iP z z 2 x 1+ ?= = = = .Since( ... , )( ... , )( ... , ) ( )1 3i i l 1 ik 0 j 13i i l 1 ij 1i i l 1 i i 11 P z z j x kP z z j x 0z z 2 x 1 z 2+ ?= =+ ?=+ ?
?==== = = = == = = = =+ = = = = =??
?and( ... , )i i l 1 iP z z 2 x 1+ ?= = = =1 0 1 1 0????????
?ti ti+2 ti-1 ti-2??
?zi zi+1 zi-1 zi-2 zi+2ti+2?????
?0348( ... , )( ... , ) ( )i i l 1 ii i l 1 iP z z 2 x 0P z z j x 0 1 j 3+ ?+ ?= = = = == = = = = ?
?
,we get( ... , ) / /i i l 1 iP z z 2 x 1 1 4 1 6+ ?= = = = = > .In conclusion, the model is more likely to formlong words, if we sample zGand xi together.
Thisis preferred because both TNG and TWC tend toproduce shorter words than we would like.For each type of block, we need to work outthe corresponding conditional probability., ,, ,, , , ,, ( : ) ,( | , , , , , , , , )( | , , , , , , , , )( ,| , , , , , , , , )d i d id i d id i d i 1 d i l 1 d id i i l 1 d iP t s z x t c ?
?
?
?
?P x k z x t c ?
?
?
?
?P z z z j x kz x t c ?
?
?
?
?P?
?+ + ??
+ ?
?=== = = = =GG GGGG GG"GG GGwhere ,d it ?Gdenotes the character topic assign-ments except td,i, ,d ix ?Gdenotes the indicators ex-cept xd,i, and , ( : )d i i l 1z ?
+ ?Gdenotes the word topicassignments except ,d jzG(j from i to i+l-1).
De-tails of the derivation of these conditional prob-abilities are provided in Appendix A.1.5 ExperimentsIn this section, we discuss our evaluation ofTWC in Chinese document modeling and Chi-nese word and topic discovery.5.1 Modeling documentsTo evaluate the generalization performance ofour model, we trained both TWC and TNG on aChinese corpus and computed the perplexity ofthe held-out test set.
Perplexity, which indicatesthe uncertainty in predicting a single character, isa standard measure of performance for statisticalmodels of natural language.
A lower perplexityscore indicates better generalization performance.Formally, the perplexities for TWC and TNGare defined as follows.
( )?
?
?
?
?log ( | , , , , )exp{ }TWC testDdd 1Ddd 1perplexity Dp c ?
?
?
?
?N=== ??
?GG G G GG,where Dtest is the testing data, D is the number ofdocuments in Dtest, Nd is the number of charactersin document d, ,d?
z?
is simply set to 1/Z (Z isnumber of word topics), and ?
?
?
?, , ,?
?
?
?G G GGare poste-rior estimates provided by applying TWC totraining data.
Details of the parameter estimationfor TWC are provided in Appendix A.2.
( )TNG testperplexity D?
?
?
?log ( | , , , )exp{ }Ddd 1Ddd 1p c ?
?
?
?N=== ??
?GG G GG,where Dtest, D, and Nd are the same as defined forthe TWC perplexity, ,d?
z?
is simply set to 1/T (T isnumber of topics), and ?
?
?, ,?
?
?G GGare posterior esti-mates provided by applying TNG to training data.Now, the remaining question is how to workout the likelihood function in the definition ofperplexity.
The likelihood function can be ob-tained by marginalizing latent variables, but thetime complexity is exponential.
Therefore, wepropose an efficient method of computing thelikelihood that is similar to the forward algo-rithm for an HMM.
Details of the forward ap-proach to computing likelihood for TWC andTNG are provided in Appendix B.In our experiments, we used a subset of Chi-nese corpus LDC2005T14.
The dataset contains6000 documents with 4476 unique characters and2,454,616 characters.
We evaluated both TWCand TNG using 10-fold cross validation.
In eachexperiment, both models ran for 500 iterations on90% of the data and computed the complexity forthe remaining 10% of the data.TWC used fixed Dirichlet (Beta) priors ?=1,?=1, ?=1, ?=0.1 and ?=0.01 while TNG used?=1, ?=0.01, ?=0.01, and ?=0.1.0501001502002500 20 40 60 80 100No.
of topics (TNG)No.
of charcter topics *  no.
of word topicsPerplexityTNG TWCFigure 5: Perplexity results with LDC2005T14corpora for TNG and TWC.The results of these computations are shown inFigure 5.
Note that the abscissa for TWC is thenumber of word topics (Z) multiplied by the(TWC)349number of character topics (T), while the ab-scissa for TNG is the number of topics (T).
Theyboth represent the number of partitions intowhich the model classifies characters.Chance performance results in a perplexityequal to the number of unique characters, whichwas 4476 in our experiments.
Therefore, bothTWC and TNG are competitive models.
And thelower curve shows that TWC is much better thanTNG.We also found that both perplexity curves in-creased with the number of partitions.
In otherwords, both models suffer from overfitting issues.This is because the documents in a test set arevery likely to contain words that do not appear inany of the documents in the training set.
Suchwords will have a very low probability, which isinversely proportional to the number of partitions.Therefore, the perplexity of TWC increased from7.3513 (Z*T=2*2) to 8.9953 (Z*T=10*10), whilethat of TNG increased from 20.3789 (T=5) to193.6065 (T=100).5.2 Chinese word and topic discoveryAs shown in the previous subsection, TWC isa competitive method for topically modelingChinese documents.
Next, we show its ability toextract Chinese words and topics in comparisonwith TNG.In our qualitative experiments, the task ofChinese word and topic discovery was addressedas a supervised learning problem, where a set ofwords with their topical assignments was givenas a seed set.
Each seed can be viewed as someconstraints imposed on the TWC and TNG mod-els.
For example, suppose that ????
(teacher)together with its assignment ?education?
is aseed.
This assumption implies that the indicatorbetween characters ???
and ???
is 1 and thatthe (word) topic for each character is ?education?.We make use of such constraints in a simplebut effective way.
In each sampling iteration, wefirst sample all variables as usual and then resetobserved variables according to the constraints.We used 8000 Chinese documents in the Chi-nese Gigaword corpus (LDC2005T14) providedby the Linguistic Data Consortium for our ex-periments.
The dataset contains 4651 uniquecharacters and 3,295,810 characters.The number of word topics in TWC, the num-ber of character topics in TWC, and the numberof topics in TNG were all set to 15.
Furthermore,16 seeds scattered in 4 distinct topics were given,as listed in Table 1 column ?seed?.
Dirichlet(Beta)   priors  were  set  to  the  same  values   asdescribed in the previous subsection.Word and topic assignments were extracted af-ter running 300 Gibbs sampling iterations on thetraining corpus together with the seed set.
For theTNG model, we took the first character?s topic asthe topic of the word.
We omitted one-characterwords and ranked the extracted words using thefollowing formula( )( )i15ii 1occ Wocc W=?,where occi(W) represents how many words wereassigned to (word) topic i.
The top-50 extractedwords are presented in Table 1.We find found that both TWC and TNG couldassemble many common words used in corre-sponding topics.
And the TWC model had ad-vantages over the TNG model in the followingthree respects.First, TNG drew more words related to theseeds.
In Table 1, highly related words aremarked in pink (underline) and partly relatedwords are marked in blue (italic).
It is clear thatthe TWC column is more colorful than the TNGcolumn.Secondly, we found that many words extractedby TNG had the same prefix.
For example, con-sider the topic ?agriculture?
: there are 14 wordsmarked with superscript 1 in Table 1.
They allhave the prefix ???.
This is because we took thefirst character?s topic as the topic of the word.Although this strategy is beneficial in some cases,such as for words with prefix ??
?, it is detri-mental in other cases.
For example, ???
?
(sugar cane) and ????
(Gansu) have the sameprefix and topic assignment, but the latter is aname of a province in China and is not related toagriculture.
Similarly, even though the characterstring ????
does not form a Chinese word, thisstring ????
and ????
(Iran) are classified inthe same cluster.
Compared with TNG, TWC canalso extract words whose topics are identical tothe topic of any character.
For example, the top-ics ?????
(freestyle swimming), ?????
(medley swimming), and ???
?
(butterflystroke) depend on their suffixes.Thirdly, although TNG stands for ?topical n-gram model?, it infrequently draws words con-taining more than two characters.
On the otherhand, the TWC model extracts many n-characterwords, such as ????????
(president ofUnited States, George Bush), ???????
(individual medley), and  ??????
(four  per-350Seeds TNG TWC??(football)??(player)??(match)??(championship)??,??,??,??,??,??,??,??,??,??,??,??,??,??,??,??,??,??,??,??,??,??,??,??,??,??,??,??,??,??,??,??,??,??,??,??,??,??,??,??,??,??,??,??,??,??,??,??,??,????,??
2,??,???,???,??,??,??,??,??,??,??,??,??,??,???,??,??,??,???,??,??,??,??
2,??,???,??,???,???
2,?????,??,??,??,??,??,??,??,??,??,???,???,??,??,??,??,??,??,??,???,????
(foodstuff)??
(country)??
(farmer)??
(paddies)??,??
1,??,??
1,??,??
1,??
1,??1,??,??
1,??
1,??,??
1,??
1,??,??,??
1,??,??
1,??,??
1,??
1,??,??
1,??,??,??,??,??,??,??,??,??,??,??,??,??,??,??,??,??,??,??,??,??,??,??,??,??,????,??,??,??,??,??,??,??,??,??,??,???,??,??,??,??,??,??,???,??,??
2,??,???
2,??,????,??
2,??,??,??,??,???
2,??,??,??,??,??,??,???
2,??,??,??,??
2,??
2,??,????,??,??,??,??,????
(school)??
(teacher)??
(student)??
(education)??,??,??,??,??,??,??,??,??,??,??,??,???,??,??,??,??,??,??,??,??,??,??,??,??,??,??,??,??,??,??,??,??,??,??,??,??,??,??,??,??,??,??,??,??,??,??,??,??,????,??,??,??,??,??,??,??,??,??,??,??,??,??,??,??,??,??,??,??,????,??,???,??,??,??,??,??,??,??,???,??,??,??,??,??,??,??,??,??,??,??,??,??,??,??,??,??,??,????
(war)??
(solider)??
(general)??
(weapon)??,??,??,??,??,??,??,??,??,??,??,??,??,??,??,??,??,??,??,??,??,??,??,??,??,??,??,??,??,??,??,??,??,??,??,??,??,??,??,??,??,??,??,??,??,???,??,??,??,?????,??,??,??,???,??,??,??,??,??????,??,????,?????
2,???
2,???,????,??,??,????,??,????,??,??,???,??,??,??,??,????,??,???,???
2,??,??,??,??,??,??,??,??,??,??,??,??,??,??,??,??,??,?
?Table 1: Top-50 words extracted by TWC and TNG.cent).
This is partly due to our sampling strategy,discussed in subsection 4.1, which increases theprobability of forming long words.We also found that some extracted characterstrings were very close to real Chinese words.For instance, ????
is a substring of ?????
(tournament); ?????
is a suffix of ??????
(Chinese player), ??????
(Americanplayer), and ??????
(French player); and?????
is a substring of  ??????
(100thousand kilograms) and ??????
(50 thou-sand kilograms).
(Such substrings are markedwith superscript 2 in Table 1.)
We believe thatthis result occurred because the training corpuswas not large enough and that TWC will achievebetter performance with a large dataset.6 Conclusion and Future WorkIn this paper, we presented a topical word-character (TWC) model, which models two dis-tinct types of topics: word topic and charactertopic.
The experimental results show that TWCis a powerful approach to modeling Chinesedocuments according to the standard evaluationmeasure of perplexity.
We also demonstratedTWC?s ability to detect words and assign topics.Since TWC is a straightforward improvementthat removes the limitations of existing topicalcollocation models, we expect that its applicationto English collocation will also result in higherperformance.Appendix A.1 Gibbs Sampling Derivationfor TWCSymbols used here are defined as follows.C is the number of unique characters, T is the num-ber of character topics, and Z is the number of wordtopics.Nd denotes the number of characters in a documentd.
( )I ?
is an indicator function, taking the value 1when its argument is true, and 0 otherwise.qd,z,0 represents how words are assigned to topic z indocument d; pz,t,c,k represents how many times an indi-cator is k given the previous character c, the previouscharacter topic t, and the previous word topic z; nz,t,0represents how many times a character topic is t givena word topic z and the corresponding indicator 0;mz,v,t,1 represents how many times a character topic is tgiven a word topic z, the previous character topic v,and the corresponding indicator 1; and rt,c representshow many times character c is assigned to charactertopic t.''', ' ', ' ', ', ,' ', ' ', ' ', ' '' ' '', ', ' ', ' , ,' ' ' ' ''( | , , , , , , , , )( | ) ( | , , )( | ) ( | )(ddd i 1 d i 1 d i 1d i d iND Dd d i d i d i 1 dd 1 d 1 i 1NZ T C Dz t c d i z t cz 1 t 1 c 1 d 1 i 1zP t s z x t c ?
?
?
?
?P ?
?
P z x z ?
d?P ?
?
P x ?
d?P ??
?
??
?= = == = = = ==?
?
?
??
???
??????
??
?GG GGG G GG G GG '', '', ' ', ' ', '' ' ' ' '| ) ( | ) ( | , ,dd iNZ Z T Dz t d i d i zz 1 z 1 t 1 d 1 i 1?
P ?
?
P t x ?= = = = =?
??
??
????
GG351'', ' ', ' ', '', ', ', '', ', ' ', ', ',, ,, ,, ' ', '' ' '' '' ' ' ' ') ( | ) ( | )?
( )( ) ( )?
( )?
( )?
( )dd i d i 1 d iz t c xz t c z t cd i 1z s cd i d iNT Dz t t d i tt 1 d 1 i 1Z T C 2 2px ?
1 x2z 1 t 1 c 1 x 1 x 1xCt?
d?
d?
P ?
?
P c ?
d?2??
??C??
d??
?+= = =?= = = = =?
?
?
?
??
?
?
??
??
??????
?
?
?G G G GG GG', ' ,' '', ',,', ', ',, ,' '' ' '' '' '' ' ' ' '' '', ' ', '' ' ,( ) ( )?
( ) ?
( )( ) ( )?
( ) ?
( )( ) ( )t c d it t sz t 0d iz t v 1d i d i 1T C Cr cc ?
1 c1 c 1 c 1Z T T Z Tnt ?
1 tz zT Tz 1 t 1 t 1 z 1 t 1sT Tzmv ?
1 vz t z tv 1 v 1 z t?
?
?
d?T?
T??
??
???
??
?
?= = =?= = = = =?= =?
?
??
?
?
??
??
?
???
?
?
?????
?G,,, , , ,, ,, ,, ,,,, ,,,*,, , , ,, , ,, , ,* ,*,, ,*,d id id i d i d i d id i d i 1d i d id i d i 1d isd iz s 0d iz 0z s c x s cz t s 1z s c sd iz t 1x 0d?
d?x 1n ?x 0n T?p ?
r ?m ?p 2?
r C?x 1m T????
=?
?
??
=??+?
=?
++ + ??
?
?
?
++ + ?
=?
+?G GSimilarly,,, , ,, , ,, ,,, , ,, ,, ,, ,, , ,,*,, , ,*, ,, ,,*,, , ,, ,*,( | , , , , , , , , )( )d id i 1 d i 1 d i 1d i 1 d i 1 d i 1d i d id id i d i 1 d id i d i 1d i d id z 0z t c kd 0z t cd i d i 1z t 0z 0z t t 1z tP x k z x t c ?
?
?
?
?q ?p ?k 0q Z?p 2?I z z k 1n ?k 0n T?m ?m?
?
??
?
?????=+?
+=?
+?
?
??
+?
= =?++?=+GG GG1k 1T??????
=?
+?, , ,, , ,, , ,, , , , , ( : ) ,, ,, , ,,*,, , ,*,, , ,( , | , , ,, , , , , )( )d i 1 d i 1 d i 1d i 1 d i 1 d i 1d i u 2 d i u 2 d i u 1d i d i 1 d i l 1 d i d i i l 1 d id j 0z t c kd 0z t cd i 1j t c xjP z z z j x k z x tc ?
?
?
?
?q ?k 0 p ?q Z?p 2?I z j kP1p ?p?
?
??
?
?+ ?
+ ?
+ ?+ + ?
?
+ ?
?
?= = = = =+?
= +?
+?
?
???+?
= =?+GGG"G, ,, , ,,, ,,, , ,, , ,* , ,*, ,,*,, , ,, ,*,( )d i u 2 d i u 1d i u 2 d i u 2 d i u 2d id i 1 d id i 1l 1 lj t t 1u 2 u 2t c j t 1j t 0j 0j t t 1j t 1m ?2?
m T?n ?k 0n T?m ?k 1m T?+ ?
+ ?+ ?
+ ?
+ ??
?+= =?+?
?+ ++?
=?
+??
+?
=?
+??
?Appendix A.2 Parameter estimation forTWCAfter each Gibbs sampling iteration, we obtain pos-terior estimates ?
?
?
?, , ,?
?
?
?G G GGand r by, , , , ,, , , ,,*, , , ,*, , , , ,, , ,, ,*, ,*,,,,*?
???
?d z 0 z t c kd z z t c kd 0 z t cz v t 1 z t 0z v t z tz v 1 z 0t ct ctq ?
p ??
?q Z?
p 2?m ?
n ??
?m T?
n T?r ?
?r C?+ += =+ ++ += =+ ++= +,where the symbols are the same as those defined inAppendix A.1.
These values correspond to the predic-tive distribution over new word topics, new indicators,new character topics, and new characters.Appendix B.
Likelihood Function Deriva-tion for TWC and TNGTo compute the likelihood function for TWC, a qua-ternion function gi is defined as follows: (formula hasa broken character), , , , ,, ,( , , , ) ( , ,..., , , ,?
?
?
?
?, | , , , , )i d 1 d 2 d i d i d id i 1 d ig r s u v P c c c z r x st u t v ?
?
?
?
?
?====== == ======G G G GG .Then, it is clear that?
?
?
??
( | , , , , ) ( , , , )dZ 1 T Td Nr 1 s 0 u 1 v 1P c ?
?
?
?
?
g r s u v= = = ==???
?GG G G GG ,where Z is the number of word topics and T is thenumber of character topics.
The function gi can berewritten in a recursive manner.,,,, ,,( , , , )?
( , , , )?
?
( , , , ) ( , , , )?
??
( )d 1d i 1d i1cr v1 d r vZ 1 Tcsi 1 i j u c vj 1 k 0 l 1vrrdvr ug r 1 u v 01g r 0 u v ?
?
?Tg r s u v g j k l u ?
?s 0?
?s 1?I r j++= = === ?
?
?= ?
??
=??
??
==??===========??
?Similarly we can define function hi to help computethe likelihood for TNG.
(formula has a broken charac-ter),,, ,,, ,,,?
?
??
( , ) ( ,..., , , | , , , )?
?
??
( | , , , ) ( , )( , )?
?
( , )???
( , ) ( , )?dd 1d i 1d i d i 1d ii d 1 d i i iZ 1d Nr 1 s 01cr1 d rcZ 1rs ri 1 i j c d cj 1 k 0 r ch r s P c c z r x s ?
?
?
?P c ?
?
?
?
h r sh r 1 0h r 0 ?
??
s 0h r s h j k ?
?s 1?++= =+= == ==== ??
=?= ?
?
?
?
=?????
?G G GGGG G GGReferencesBlei, D. M., Ng, A. Y., and Jordan, M. J.
2003.
LatentDirichlet alocation.
Journal of Machine LearningResearch, 3:993?1022.Griffiths, T. L. and Steyvers, M. 2004.
Finding Scien-tific Topics.
Proceedings of the National Academyof Sciences, 101 (suppl.
1), 5228-5235.Steyvers, M. and Griffiths, T. L. 2007.
Probabilistictopic models.
Latent Semantic Analysis: A Road toMeaning.
Laurence Erlbaum.Griffiths, T. L., Steyvers, M., and Tenenbaum, J.
B. T.2007.
Topics in Semantic Representation Psycho-logical Review, 114(2), 211-244.Wang, X., McCallum, A., and Wei, X.
2007.
TopicalN-grams: Phrase and Topic Discovery, with anApplication to Information Retrieval.
Proceedingsof the 7th IEEE International Conference on DataMining (ICDM 2007).352
