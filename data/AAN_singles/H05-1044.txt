Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural LanguageProcessing (HLT/EMNLP), pages 347?354, Vancouver, October 2005. c?2005 Association for Computational LinguisticsRecognizing Contextual Polarity in Phrase-Level Sentiment AnalysisTheresa WilsonIntelligent Systems ProgramUniversity of PittsburghPittsburgh, PA 15260twilson@cs.pitt.eduJanyce WiebeDepartment of Computer ScienceUniversity of PittsburghPittsburgh, PA 15260wiebe@cs.pitt.eduPaul HoffmannIntelligent Systems ProgramUniversity of PittsburghPittsburgh, PA 15260hoffmanp@cs.pitt.eduAbstractThis paper presents a new approach tophrase-level sentiment analysis that firstdetermines whether an expression is neu-tral or polar and then disambiguates thepolarity of the polar expressions.
With thisapproach, the system is able to automat-ically identify the contextual polarity fora large subset of sentiment expressions,achieving results that are significantly bet-ter than baseline.1 IntroductionSentiment analysis is the task of identifying positiveand negative opinions, emotions, and evaluations.Most work on sentiment analysis has been done atthe document level, for example distinguishing pos-itive from negative reviews.
However, tasks suchas multi-perspective question answering and sum-marization, opinion-oriented information extraction,and mining product reviews require sentence-levelor even phrase-level sentiment analysis.
For exam-ple, if a question answering system is to successfullyanswer questions about people?s opinions, it must beable to pinpoint expressions of positive and negativesentiments, such as we find in the sentences below:(1) African observers generally approved+ of hisvictory while Western governments denounced?it.
(2) A succession of officers filled the TVscreen to say they supported+ the people and thatthe killings were ?not tolerable?.?
(3) ?We don?t hate+ the sinner,?
he says,?but we hate?
the sin.
?A typical approach to sentiment analysis is to startwith a lexicon of positive and negative words andphrases.
In these lexicons, entries are tagged withtheir a priori prior polarity: out of context, doesthe word seem to evoke something positive or some-thing negative.
For example, beautiful has a positiveprior polarity, and horrid has a negative prior polar-ity.
However, the contextual polarity of the phrasein which a word appears may be different from theword?s prior polarity.
Consider the underlined polar-ity words in the sentence below:(4) Philip Clapp, president of the National Environ-ment Trust, sums up well the general thrust of thereaction of environmental movements: ?There is noreason at all to believe that the polluters are sud-denly going to become reasonable.
?Of these words, ?Trust,?
?well,?
?reason,?
and ?rea-sonable?
have positive prior polarity, but they arenot all being used to express positive sentiments.The word ?reason?
is negated, making the contex-tual polarity negative.
The phrase ?no reason at allto believe?
changes the polarity of the propositionthat follows; because ?reasonable?
falls within thisproposition, its contextual polarity becomes nega-tive.
The word ?Trust?
is simply part of a referringexpression and is not being used to express a senti-ment; thus, its contextual polarity is neutral.
Simi-larly for ?polluters?
: in the context of the article, itsimply refers to companies that pollute.
Only ?well?has the same prior and contextual polarity.Many things must be considered in phrase-levelsentiment analysis.
Negation may be local (e.g., notgood), or involve longer-distance dependencies suchas the negation of the proposition (e.g., does notlook very good) or the negation of the subject (e.g.,347no one thinks that it?s good).
In addition, certainphrases that contain negation words intensify ratherthan change polarity (e.g., not only good but amaz-ing).
Contextual polarity may also be influenced bymodality (e.g., whether the proposition is asserted tobe real (realis) or not real (irrealis) ?
no reason at allto believe is irrealis, for example); word sense (e.g.,Environmental Trust versus He has won the peo-ple?s trust); the syntactic role of a word in the sen-tence (e.g., polluters are versus they are polluters);and diminishers such as little (e.g., little truth, lit-tle threat).
(See (Polanya and Zaenen, 2004) for amore detailed discussion of contextual polarity in-fluencers.
)This paper presents new experiments in automat-ically distinguishing prior and contextual polarity.Beginning with a large stable of clues marked withprior polarity, we identify the contextual polarity ofthe phrases that contain instances of those clues inthe corpus.
We use a two-step process that employsmachine learning and a variety of features.
Thefirst step classifies each phrase containing a clue asneutral or polar.
The second step takes all phrasesmarked in step one as polar and disambiguates theircontextual polarity (positive, negative, both, or neu-tral).
With this approach, the system is able to auto-matically identify the contextual polarity for a largesubset of sentiment expressions, achieving resultsthat are significantly better than baseline.
In addi-tion, we describe new manual annotations of contex-tual polarity and a successful inter-annotator agree-ment study.2 Manual Annotation SchemeTo create a corpus for the experiments below, weadded contextual polarity judgments to existing an-notations in the Multi-perspective Question Answer-ing (MPQA) Opinion Corpus1, namely to the an-notations of subjective expressions2.
A subjectiveexpression is any word or phrase used to expressan opinion, emotion, evaluation, stance, speculation,1The MPQA Corpus is described in (Wiebe et al, 2005) andavailable at nrrc.mitre.org/NRRC/publications.htm.2In the MPQA Corpus, subjective expressions are directsubjective expressions with non-neutral expression intensity,plus all the expressive subjective elements.
Please see (Wiebeet al, 2005) for more details on the existing annotations in theMPQA Corpus.etc.
A general covering term for such states is pri-vate state (Quirk et al, 1985).
In the MPQA Cor-pus, subjective expressions of varying lengths aremarked, from single words to long phrases.For this work, our focus is on sentiment expres-sions ?
positive and negative expressions of emo-tions, evaluations, and stances.
As these are types ofsubjective expressions, to create the corpus, we justneeded to manually annotate the existing subjectiveexpressions with their contextual polarity.In particular, we developed an annotationscheme3 for marking the contextual polarity of sub-jective expressions.
Annotators were instructed totag the polarity of subjective expressions as positive,negative, both, or neutral.
The positive tag is forpositive emotions (I?m happy), evaluations (Greatidea!
), and stances (She supports the bill).
The neg-ative tag is for negative emotions (I?m sad), eval-uations (Bad idea!
), and stances (She?s against thebill).
The both tag is applied to sentiment expres-sions that have both positive and negative polarity.The neutral tag is used for all other subjective ex-pressions: those that express a different type of sub-jectivity such as speculation, and those that do nothave positive or negative polarity.Below are examples of contextual polarity anno-tations.
The tags are in boldface, and the subjectiveexpressions with the given tags are underlined.
(5) Thousands of coup supporters celebrated (posi-tive) overnight, waving flags, blowing whistles .
.
.
(6) The criteria set by Rice are the following: thethree countries in question are repressive (nega-tive) and grave human rights violators (negative).
.
.
(7) Besides, politicians refer to good and evil(both) only for purposes of intimidation andexaggeration.
(8) Jerome says the hospital feels (neutral) no dif-ferent than a hospital in the states.The annotators were asked to judge the contex-tual polarity of the sentiment that is ultimately be-ing conveyed by the subjective expression, i.e., oncethe sentence has been fully interpreted.
Thus, thesubjective expression, they have not succeeded, and3The annotation instructions are available athttp://www.cs.pitt.edu/?twilson.348will never succeed, was marked as positive in thesentence, They have not succeeded, and will neversucceed, in breaking the will of this valiant people.The reasoning is that breaking the will of a valiantpeople is negative; hence, not succeeding in break-ing their will is positive.3 Agreement StudyTo measure the reliability of the polarity annotationscheme, we conducted an agreement study with twoannotators, using 10 documents from the MPQACorpus.
The 10 documents contain 447 subjectiveexpressions.
Table 1 shows the contingency table forthe two annotators?
judgments.
Overall agreement is82%, with a Kappa (?)
value of 0.72.Neutral Positive Negative Both TotalNeutral 123 14 24 0 161Positive 16 73 5 2 96Negative 14 2 167 1 184Both 0 3 0 3 6Total 153 92 196 6 447Table 1: Agreement for Subjective Expressions(Agreement: 82%, ?
: 0.72)For 18% of the subjective expressions, at least oneannotator used an uncertain tag when marking po-larity.
If we consider these cases to be borderlineand exclude them from the study, percent agreementincreases to 90% and Kappa rises to 0.84.
Thus, theannotator agreement is especially high when bothare certain.
(Note that all annotations are includedin the experiments described below.
)4 CorpusIn total, 15,991 subjective expressions from 425documents (8,984 sentences) were annotated withcontextual polarity as described above.
Of these sen-tences, 28% contain no subjective expressions, 25%contain only one, and 47% contain two or more.
Ofthe 4,247 sentences containing two or more subjec-tive expressions, 17% contain mixtures of positiveand negative expressions, and 62% contain mixturesof polar (positive/negative/both) and neutral subjec-tive expressions.The annotated documents are divided into twosets.
The first (66 documents/1,373 sentences/2,808subjective expressions) is a development set, usedfor data exploration and feature development.
Weuse the second set (359 documents/7,611 sen-tences/13,183 subjective expressions) in 10-foldcross-validation experiments, described below.5 Prior-Polarity Subjectivity LexiconFor the experiments in this paper, we use a lexicon ofover 8,000 subjectivity clues.
Subjectivity clues arewords and phrases that may be used to express pri-vate states, i.e., they have subjective usages (thoughthey may have objective usages as well).
For thiswork, only single-word clues are used.To compile the lexicon, we began with a list ofsubjectivity clues from (Riloff and Wiebe, 2003).The words in this list were grouped in previous workaccording to their reliability as subjectivity clues.Words that are subjective in most contexts weremarked strongly subjective (strongsubj), and thosethat may only have certain subjective usages weremarked weakly subjective (weaksubj).We expanded the list using a dictionary and athesaurus, and also added words from the GeneralInquirer positive and negative word lists (General-Inquirer, 2000) which we judged to be potentiallysubjective.
We also gave the new words reliabilitytags, either strongsubj or weaksubj.The next step was to tag the clues in the lexiconwith their prior polarity.
For words that came frompositive and negative word lists (General-Inquirer,2000; Hatzivassiloglou and McKeown, 1997), welargely retained their original polarity, either posi-tive or negative.
We assigned the remaining wordsone of the tags positive, negative, both or neutral.By far, the majority of clues, 92.8%, aremarked as having either positive (33.1%) or nega-tive (59.7%) prior polarity.
Only a small number ofclues (0.3%) are marked as having both positive andnegative polarity.
6.9% of the clues in the lexiconare marked as neutral.
Examples of these are verbssuch as feel, look, and think, and intensifiers such asdeeply, entirely, and practically.
These words are in-cluded because, although their prior polarity is neu-tral, they are good clues that a sentiment is beingexpressed (e.g., feels slighted, look forward to).
In-cluding them increases the coverage of the system.3496 ExperimentsThe goal of the experiments described below is toclassify the contextual polarity of the expressionsthat contain instances of the subjectivity clues inour lexicon.
What the system specifically does isgive each clue instance its own label.
Note that thesystem does not try to identify expression bound-aries.
Doing so might improve performance and is apromising avenue for future research.6.1 Definition of the Gold StandardWe define the gold standard used to train and test thesystem in terms of the manual annotations describedin Section 2.The gold standard class of a clue instance that isnot in a subjective expression is neutral: since theclue is not even in a subjective expression, it is notcontained in a sentiment expression.Otherwise, if a clue instance appears in just onesubjective expression (or in multiple subjective ex-pressions with the same contextual polarity), thenthe class assigned to the clue instance is the classof the subjective expression(s).
If a clue appearsin at least one positive and one negative subjectiveexpression (or in a subjective expression marked asboth), then its class is both.
If it is in a mixture ofnegative and neutral subjective expressions, its classis negative; if it is in a mixture of positive and neu-tral subjective expressions, its class is positive.6.2 Performance of a Prior-Polarity ClassifierAn important question is how useful prior polarityalone is for identifying contextual polarity.
To an-swer this question, we create a classifier that sim-ply assumes that the contextual polarity of a clue in-stance is the same as the clue?s prior polarity, and weexplore the classifier?s performance on the develop-ment set.This simple classifier has an accuracy of 48%.From the confusion matrix given in Table 2, we seethat 76% of the errors result from words with non-neutral prior polarity appearing in phrases with neu-tral contextual polarity.6.3 Contextual Polarity DisambiguationThe fact that words with non-neutral prior polarityso frequently appear in neutral contexts led us toPrior-Polarity ClassifierNeut Pos Neg Both TotalNeut 798 784 698 4 2284Pos 81 371 40 0 492Gold Neg 149 181 622 0 952Both 4 11 13 5 33Total 1032 1347 1373 9 3761Table 2: Confusion matrix for the prior-polarityclassifier on the development set.adopt a two-step approach to contextual polarity dis-ambiguation.
For the first step, we concentrate onwhether clue instances are neutral or polar in context(where polar in context refers to having a contextualpolarity that is positive, negative or both).
For thesecond step, we take all clue instances marked aspolar in step one, and focus on identifying their con-textual polarity.
For both steps, we develop classi-fiers using the BoosTexter AdaBoost.HM (Schapireand Singer, 2000) machine learning algorithm with5000 rounds of boosting.
The classifiers are evalu-ated in 10-fold cross-validation experiments.6.3.1 Neutral-Polar ClassificationThe neutral-polar classifier uses 28 features, listedin Table 3.Word Features: Word context is a bag of threeword tokens: the previous word, the word itself, andthe next word.
The prior polarity and reliabilityclass are indicated in the lexicon.Modification Features: These are binary rela-tionship features.
The first four involve relationshipswith the word immediately before or after: if theword is a noun preceded by an adjective, if the pre-ceding word is an adverb other than not, if the pre-ceding word is an intensifier, and if the word itselfis an intensifier.
A word is considered an intensifierif it appears in a list of intensifiers and if it precedesa word of the appropriate part-of-speech (e.g., an in-tensifier adjective must come before a noun).The modify features involve the dependency parsetree for the sentence, obtained by first parsing thesentence (Collins, 1997) and then converting the treeinto its dependency representation (Xia and Palmer,2001).
In a dependency representation, every nodein the tree structure is a surface word (i.e., there areno abstract nodes such as NP or VP).
The edge be-tween a parent and a child specifies the grammaticalrelationship between the two words.
Figure 1 shows350Word Features Sentence Features Structure Featuresword token strongsubj clues in current sentence: count in subject: binaryword part-of-speech strongsubj clues in previous sentence: count in copular: binaryword context strongsubj clues in next sentence: count in passive: binaryprior polarity: positive, negative, both, neutral weaksubj clues in current sentence: countreliability class: strongsubj or weaksubj weaksubj clues in previous sentence: countModification Features weaksubj clues in next sentence: count Document Featurepreceeded by adjective: binary adjectives in sentence: count document topicpreceeded by adverb (other than not): binary adverbs in sentence (other than not): countpreceeded by intensifier: binary cardinal number in sentence: binaryis intensifier: binary pronoun in sentence: binarymodifies strongsubj: binary modal in sentence (other than will): binarymodifies weaksubj: binarymodified by strongsubj: binarymodified by weaksubj: binaryTable 3: Features for neutral-polar classificationThe human rightsreportaposessubstantialchallengetoUStheinterpretationofgood and evildet detdetadj adjobjsubjmodmodconj conjpobjpobjpp(pos) (neg)(pos)(neg)(pos)Figure 1: The dependency tree for the sentence The humanrights report poses a substantial challenge to the US interpre-tation of good and evil.
Prior polarity is marked in parenthesesfor words that match clues from the lexicon.an example.
The modifies strongsubj/weaksubj fea-tures are true if the word and its parent share anadj, mod or vmod relationship, and if its parent isan instance of a clue from the lexicon with strong-subj/weaksubj reliability.
The modified by strong-subj/weaksubj features are similar, but look for rela-tionships and clues in the word?s children.Structure Features: These are binary featuresthat are determined by starting with the word in-stance and climbing up the dependency parse treetoward the root, looking for particular relationships,words, or patterns.
The in subject feature is true ifwe find a subj relationship.
The in copular feature istrue if in subject is false and if a node along the pathis both a main verb and a copular verb.
The in pas-sive features is true if a passive verb pattern is foundon the climb.Sentence Features: These are features that werefound useful for sentence-level subjectivity classifi-cation by Wiebe and Riloff (2005).
They includecounts of strongsubj and weaksubj clues in the cur-rent, previous and next sentences, counts of adjec-tives and adverbs other than not in the current sen-tence, and binary features to indicate whether thesentence contains a pronoun, a cardinal number, anda modal other than will.Document Feature: There is one document fea-ture representing the topic of the document.
A doc-ument may belong to one of 15 topics ranging fromspecific (e.g., the 2002 presidential election in Zim-babwe) to more general (e.g., economics) topics.Table 4 gives neutral-polar classification resultsfor the 28-feature classifier and two simpler classi-fiers that provide our baselines.
The first row in thetable lists the results for a classifier that uses justone feature, the word token.
The second row showsthe results for a classifier that uses both the word to-ken and the word?s prior polarity as features.
Theresults for the 28-feature classifier are listed in thelast row.
The 28-feature classifier performs signifi-cantly better (1-tailed t-test, p ?
.05) than the twosimpler classifiers, as measured by accuracy, polarF-measure, and neutral F-measure (?
= 1).
It has anaccuracy of 75.9%, with a polar F-measure of 63.4and a neutral F-measure of 82.1.Focusing on the metrics for polar expressions, it?sinteresting to note that using just the word token as afeature produces a classifier with a precision slightlybetter than the 28-feature classifier, but with a recallthat is 20% lower.
Adding a feature for the prior351Word Featuresword tokenword prior polarity: positive, negative, both, neutralPolarity Featuresnegated: binarynegated subject: binarymodifies polarity: positive, negative, neutral, both, notmodmodified by polarity: positive, negative, neutral, both, notmodconj polarity: positive, negative, neutral, both, notmodgeneral polarity shifter: binarynegative polarity shifter: binarypositive polarity shifter: binaryTable 6: Features for polarity classificationpolarity improves recall so that it is only 4.4% lower,but this hurts precision, which drops to 4.2% lowerthan the 28-feature classifier?s precision.
It is onlywith all the features that we get the best result, goodprecision with the highest recall.The clues in the prior-polarity lexicon have19,506 instances in the test set.
According to the28-feature neutral-polar classifier, 5,671 of these in-stances are polar in context.
It is these clue instancesthat are passed on to the second step in the contex-tual disambiguation process, polarity classification.6.3.2 Polarity ClassificationIdeally, this second step in the disambiguationprocess would be a three-way classification task, de-termining whether the contextual polarity is posi-tive, negative or both.
However, although the major-ity of neutral expressions have been filtered out bythe neutral-polar classification in step one, a numberstill remain.
So, for this step, the polarity classifica-tion task remains four-way: positive, negative, both,and neutral.Table 6 lists the features used by the polarity clas-sifier.
Word token and word prior polarity are un-changed from the neutral-polar classifier.
Negatedis a binary feature that captures whether the word isbeing locally negated: its value is true if a negationword or phrase is found within the four preceedingwords or in any of the word?s children in the de-pendency tree, and if the negation word is not in aphrase that intensifies rather than negates (e.g., notonly).
The negated subject feature is true if the sub-ject of the clause containing the word is negated.The modifies polarity, modified by polarity, andconj polarity features capture specific relationshipsbetween the word instance and other polarity wordsit may be related to.
If the word and its parent inthe dependency tree share an obj, adj, mod, or vmodrelationship, the modifies polarity feature is set tothe prior polarity of the word?s parent (if the parentis not in our prior-polarity lexicon, its prior polarityis set to neutral).
The modified by polarity featureis similar, looking for adj, mod, and vmod relation-ships and polarity clues within the word?s children.The conj polarity feature determines if the word isin a conjunction.
If so, the value of this feature is itssibling?s prior polarity (as above, if the sibling is notin the lexicon, its prior polarity is neutral).
Figure 1helps to illustrate these features: modifies polarity isnegative for the word ?substantial,?
modified by po-larity is positive for the word ?challenge,?
and conjpolarity is negative for the word ?good.
?The last three polarity features look in a windowof four words before, searching for the presence ofparticular types of polarity influencers.
General po-larity shifters reverse polarity (e.g., little truth, lit-tle threat).
Negative polarity shifters typically makethe polarity of an expression negative (e.g., lack ofunderstanding).
Positive polarity shifters typicallymake the polarity of an expression positive (e.g.,abate the damage).The polarity classification results for this secondstep in the contextual disambiguation process aregiven in Table 5.
Also listed in the table are resultsfor the two simple classifiers that provide our base-lines.
The first line in Table 5 lists the results forthe classifier that uses just one feature, the word to-ken.
The second line shows the results for the clas-sifier that uses both the word token and the word?sprior polarity as features.
The last line shows theresults for the polarity classifier that uses all 10 fea-tures from Table 6.Mirroring the results from step one, the morecomplex classifier performs significantly better thanthe simpler classifiers, as measured by accuracyand all of the F-measures.
The 10-feature classi-fier achieves an accuracy of 65.7%, which is 4.3%higher than the more challenging baseline providedby the word + prior polarity classifier.
Positive F-measure is 65.1 (5.7% higher); negative F-measureis 77.2 (2.3% higher); and neutral F-measure is 46.2(13.5% higher).Focusing on the metrics for positive and negativeexpressions, we again see that the simpler classifiers352Acc Polar Rec Polar Prec Polar F Neut Rec Neut Prec Neut Fword token 73.6 45.3 72.2 55.7 89.9 74.0 81.2word+priorpol 74.2 54.3 68.6 60.6 85.7 76.4 80.728 features 75.9 56.8 71.6 63.4 87.0 77.7 82.1Table 4: Results for Step 1 Neutral-Polar ClassificationPositive Negative Both NeutralAcc Rec Prec F Rec Prec F Rec Prec F Rec Prec Fword token 61.7 59.3 63.4 61.2 83.9 64.7 73.1 9.2 35.2 14.6 30.2 50.1 37.7word+priorpol 63.0 69.4 55.3 61.6 80.4 71.2 75.5 9.2 35.2 14.6 33.5 51.8 40.710 features 65.7 67.1 63.3 65.1 82.1 72.9 77.2 11.2 28.4 16.1 41.4 52.4 46.2Table 5: Results for Step 2 Polarity Classification.Experiment Features RemovedAB1 negated, negated subjectAB2 modifies polarity, modified by polarityAB3 conj polarityAB4 general, negative, and positive polarity shiftersTable 7: Features for polarity classificationtake turns doing better or worse for precision andrecall.
Using just the word token, positive preci-sion is slightly higher than for the 10-feature clas-sifier, but positive recall is 11.6% lower.
Add theprior polarity, and positive recall improves, but atthe expense of precision, which is 12.6% lower thanfor the 10-feature classifier.
The results for negativeexpressions are similar.
The word-token classifierdoes well on negative recall but poorly on negativeprecision.
When prior polarity is added, negativerecall improves but negative precision drops.
It isonly with the addition of the polarity features that weachieve both higher precisions and higher recalls.To explore how much the various polarity featurescontribute to the performance of the polarity classi-fier, we perform four experiments.
In each experi-ment, a different set of polarity features is excluded,and the polarity classifier is retrained and evaluated.Table 7 lists the features that are removed for eachexperiment.The only significant difference in performance inthese experiments is neutral F-measure when themodification features (AB2) are removed.
Theseablation experiments show that the combination offeatures is needed to achieve significant results overbaseline for polarity classification.7 Related WorkMuch work on sentiment analysis classifies docu-ments by their overall sentiment, for example deter-mining whether a review is positive or negative (e.g.,(Turney, 2002; Dave et al, 2003; Pang and Lee,2004; Beineke et al, 2004)).
In contrast, our ex-periments classify individual words and phrases.
Anumber of researchers have explored learning wordsand phrases with prior positive or negative polarity(another term is semantic orientation) (e.g., (Hatzi-vassiloglou and McKeown, 1997; Kamps and Marx,2002; Turney, 2002)).
In contrast, we begin witha lexicon of words with established prior polarities,and identify the contextual polarity of phrases inwhich instances of those words appear in the cor-pus.
To make the relationship between that taskand ours clearer, note that some word lists used toevaluate methods for recognizing prior polarity areincluded in our prior-polarity lexicon (General In-quirer lists (General-Inquirer, 2000) used for evalu-ation by Turney, and lists of manually identified pos-itive and negative adjectives, used for evaluation byHatzivassiloglou and McKeown).Some research classifies the sentiments of sen-tences.
Yu and Hatzivassiloglou (2003), Kim andHovy (2004), Hu and Liu (2004), and Grefenstette etal.
(2001)4 all begin by first creating prior-polaritylexicons.
Yu and Hatzivassiloglou then assign a sen-timent to a sentence by averaging the prior semanticorientations of instances of lexicon words in the sen-tence.
Thus, they do not identify the contextual po-larity of individual phrases containing clues, as we4In (Grefenstette et al, 2001), the units that are classified arefixed windows around named entities rather than sentences.353do in this paper.
Kim and Hovy, Hu and Liu, andGrefenstette et al multiply or count the prior po-larities of clue instances in the sentence.
They alsoconsider local negation to reverse polarity.
However,they do not use the other types of features in ourexperiments, and they restrict their tags to positiveand negative (excluding our both and neutral cate-gories).
In addition, their systems assign one sen-timent per sentence; our system assigns contextualpolarity to individual expressions.
As seen above,sentences often contain more than one sentiment ex-pression.Nasukawa, Yi, and colleagues (Nasukawa and Yi,2003; Yi et al, 2003) classify the contextual polarityof sentiment expressions, as we do.
Thus, their workis probably most closely related to ours.
They clas-sify expressions that are about specific items, anduse manually developed patterns to classify polarity.These patterns are high-quality, yielding quite highprecision, but very low recall.
Their system classi-fies a much smaller proportion of the sentiment ex-pressions in a corpus than ours does.8 ConclusionsIn this paper, we present a new approach tophrase-level sentiment analysis that first determineswhether an expression is neutral or polar and thendisambiguates the polarity of the polar expressions.With this approach, we are able to automaticallyidentify the contextual polarity for a large subset ofsentiment expressions, achieving results that are sig-nificantly better than baseline.9 AcknowledgmentsThis work was supported in part by the NSF undergrant IIS-0208798 and by the Advanced Researchand Development Activity (ARDA).ReferencesP.
Beineke, T. Hastie, and S. Vaithyanathan.
2004.
The sen-timental factor: Improving review classification via human-provided information.
In ACL-2004.M.
Collins.
1997.
Three generative, lexicalised models for sta-tistical parsing.
In ACL-1997.K.
Dave, S. Lawrence, and D. M. Pennock.
2003.
Mining thepeanut gallery: Opinion extraction and semantic classifica-tion of product reviews.
In WWW-2003.The General-Inquirer.
2000.http://www.wjh.harvard.edu/?inquirer/spreadsheet guide.htm.G.
Grefenstette, Y. Qu, J.G.
Shanahan, and D.A.
Evans.
2001.Coupling niche browsers and affect analysis for an opinionmining application.
In RIAO-2004.V.
Hatzivassiloglou and K. McKeown.
1997.
Predicting thesemantic orientation of adjectives.
In ACL-1997.M.
Hu and B. Liu.
2004.
Mining and summarizing customerreviews.
In KDD-2004.J.
Kamps and M. Marx.
2002.
Words with attitude.
In 1stInternational WordNet Conference.S-M. Kim and E. Hovy.
2004.
Determining the sentiment ofopinions.
In Coling 2004.T.
Nasukawa and J. Yi.
2003.
Sentiment analysis: Capturingfavorability using natural language processing.
In K-CAP2003.B.
Pang and L. Lee.
2004.
A sentimental education: Sen-timent analysis using subjectivity summarization based onminimum cuts.
In ACL-2004.L.
Polanya and A. Zaenen.
2004.
Contextual valence shifters.In Working Notes ?
Exploring Attitude and Affect in Text(AAAI Spring Symposium Series).R.
Quirk, S. Greenbaum, G. Leech, and J. Svartvik.
1985.
AComprehensive Grammar of the English Language.
Long-man, New York.E.
Riloff and J. Wiebe.
2003.
Learning extraction patterns forsubjective expressions.
In EMNLP-2003.R.
E. Schapire and Y.
Singer.
2000.
BoosTexter: A boosting-based system for text categorization.
Machine Learning,39(2/3):135?168.P.
Turney.
2002.
Thumbs up or thumbs down?
Semantic orien-tation applied to unsupervised classification of reviews.
InACL-2002.J.
Wiebe and E. Riloff.
2005.
Creating subjective and objec-tive sentence classifiers from unannotated texts.
In CICLing-2005.J.
Wiebe, T. Wilson, and C. Cardie.
2005.
Annotating expres-sions of opinions and emotions in language.
Language Re-sources and Evalution (formerly Computers and the Human-ities), 1(2).F.
Xia and M. Palmer.
2001.
Converting dependency structuresto phrase structures.
In HLT-2001.J.
Yi, T. Nasukawa, R. Bunescu, and W. Niblack.
2003.
Senti-ment analyzer: Extracting sentiments about a given topic us-ing natural language processing techniques.
In IEEE ICDM-2003.H.
Yu and V. Hatzivassiloglou.
2003.
Towards answering opin-ion questions: Separating facts from opinions and identify-ing the polarity of opinion sentences.
In EMNLP-2003.354
