Sequenc ing  in a Connect ion is t  Mode l  o f  Language Process ing  IMichael GASSER 2Michael G. DYERAI LaboratoryComputer Science DepartmentUniversity of CaliforniaLos Angeles, California 90024, USAAbstractRet~ ~nt research suggests hat human language processing canbe profitably viewed in terms of the spread of activation through anetwork of simple processing units.
Decision making in connectionistmodels uch as these is distributed anti consists in selections madefrom sets of mutually inhibiting candidate items which are activated onthe basis of input features.
In these models, however, there is theproblem, espocially for generation, of obtaining sequential behaviorfrom an essentially parallel process.
The thrust of this paper is tlmtsequencing can also be modelled as a process of competition betweencandidates activated on the basis of input features.
In the case ofsequencing, the competition concerns which of a set of phraseconstituents will appear in a particular output position.
This accountallows outpu,: ordering to arise out of the interaction of syntactic withsemantic anti pragmatic factors, as seems to be the case for humanlanguage generation.
The paper describes a localized connectionistmodel of language generation, focusing on the representation a d useof sequencing information.
We also show how these samesequencing representations and mechanisms are usable in parsing aswell.1.
The Prnblem of Sequencing in GenerationThe order in which the constituents of an utterance appeardepends on two kinds of factors: language-specific conventions andmore or less universal tendencies.
Examples of conventions are theplacement of relative clauses after nouns in English and the reverseordering in Japanese.
Some of these conventions are absolute: relativeclauses always follow nouns in English.
Others are only tendenciesand can be overridden.
For example, in English direct objects usuallyfollow verbs, but they may also come at the beginnings of clauses.Universal tendencies include in particular the appearance r lativelyearly in a clause of material which is primed in some way (Bock1982).
Such psychological considerations may be the sole factordetermining an item's position, as often happens in languages withrelatively free word order such as Russian.
But they also come intoplay when there is a linguistic sequencing convention which is atendency rather than an absolute constraint.Consider the case of the position of the arguments inditransitive sentences in English.
These sentences generally refer to aninstance of some kind of transfer f om one person to another.
In suchsentences the argument referring to the semantic OBJECT may precedeor follow the argument referring to the RECIPIENT of the transfer.Other things b~ing equal, if one of these arguments refers to somethingwhich has been mentioned recently, it will tend to come first.
Thistendency explains the strangeness of sentences (lb) and (lb).
(la) l'ns~:ead of call ing John, Mary sent ~ A?
(ib) Instead of cal l ing John, Mary sentl~ l :~  to him.
(la) Instead of throwing away the letter, Marysent: it.
to xID\]~l.?
(lb) Instead of throwing away the letter, Marysent: ~ ~,.One way to view this variation is in terms of competitionbetween the two argmnents tofill the position following the verb.
Oneargument may have a head start if it has been primed in some way, inparticular if its referent has just been mentioned.
In the examplesentences, the givenness of the referent results both in the priming thatleads that NP to come first and in the realization of the NP as apronoun rather than a full noun phrase.This explanation, in terms of competition for output positions,can account for other types of constituent order variation as well.
Anexample is the alternative orders possible with transitive verb-plus-particle combinations in English: take out t ~ ,  take/t a//t.Thus sequencing is a phenomenon involving competition andquantitative tendencies rather titan absolute constraints.
Theseproperties make it reasonable to deal with sequencing within theframework of connectionist models, which we discuss in the nextsection.2.
Conneetionism and Language ProcessingIn recent years there has been increasing interest in cognitivemodels built on networks of simple processing units which ~espond tothe parallel spread of activation through the network (Feldman &Ballard 1982, McClelland, Rumelhart, & the PDP Research Group1986).
In the area of natural anguage processing, these models,generally referred to as eorurectlonist, have been shown to exhibitinteresting properties not shared by more conventional symbolicapproaches.
In particular, connectionist approaches to language,analysis (e.g., Cottrell & Small 1983, McClelland & Kawamoto 1986,Waltz & Pollack 1985) are able to model priming effects and theinteraction of different knowledge sources in lexical access.
Therehave been only limited attempts to apply connectionist models tolanguage generation (e.g., Dell 1986, Kukich 1986) bot the potentialthere is also clear.
While generation is usually conceived of as a top-down process involving sequential stages, it also involves bottom-upaspects, a good deal of parallelism, and "leaking" between tbe variousstages, in addition to the priming effects which are handled well byspreading activation approaches.Still, there are significant problems to be surmounted whentreating language processing in a connectionist framework.
Animportant one is the representation and utilization of i~ffmmation aboutthe sequencing ofconstituents.
While information about serial order iscertainly a key element in parsing, it has been possible in existingconnectionist parsing schemes to avoid dealing with this problembecause of the limited sets of examples that are treated.
Generation isanother matter: no sentence can be generated without attention to theordering of constituents.
If connectionism is to succeed as anapproach to human language processing, it must be possible to handlethis sort of information within the confines imposed by theframework.
This paper presents a localized connectionist model oflanguage generation i which sequencing is dealt with in terms of basicfeatures characteristic of these models: spreading activation, firingthresholds, and mutual inhibition.
The same sequencing informationis also usable during parsing.
Most importantly, the approach offers apsychologically plausible account of sequencing in which syntacticand semantic factors interact to yield a particular ordering.
The modelis implemented in a program called CHIE which has been used to testtbe model's adequacy for a limited set of English and Japanesestructures.3.
A Framework for Conneetionist Language ProcessingIn this section we give an overview of knowledgerepresentation a d processing in the model.
The main features of themodel are the following:1.
Memory consists of a network of nodes joined by weightedconnections.
The system's knowledge is embodied entirely ini~5these connections.2.
Conceptsare r presented asschemas consisting of subnetworksof the memory.3.
The basic units of linguistic knowledge are schematicsubnetworks a sociating form directly with function.
These form-function mappings comprise an inventory from which selectionsare made during generation and parsing.4.
Formally, the linguistic units are composed of surface-levelpatterns ranging from phrasal lexical pattems to purely syntacticpatterns.5.
Processing consists in the parallel spread of activation through thenetwork starting with nodes representing inputs.
The amount ofactivation spreading along a connection depends on theconnection's weight and may be either positive (excitatory) ornegative (inhibitory).
Activation on nodes decays over time.6.
Decision making in the model takes the form of competitionamong sets of mutually inhibiting nodes and the eventualdominance of one over the others.7.
Processing is more interactional than modular.
Pragmatic,semantic, and syntactic information may be involvedsimultaneously in the selection of units of linguistic knowledge.The model provides a better account of human languagegeneration than other computational models.
In particular, it offersthese advantages:1.
Parallelism and competition, which characterize human languagegeneration, are basic features of the model.2.
Priming effects are naturally accommodated.
Nodes are primedwhen there is activation remaining on them as a result of recentprocessing, and priming disappears as activation decays.2.
The system exhibits robusmess in that it can find patterns to matchconceptual input even when there are no perfect matches.3.
The approach allows for a combination oftop-down (goal-driven)and bottom-up (context-driven) processing.4.
Generation i the model is flexible because spreading activationautomatically finds alternate ways of conveying particularconcepts.5.
Linguistic and non-linguistic knowledge take the form oftendencies with degrees of associated strength rather than strictrides or constraints.The model is described in detail in Gasser (1988).3.1.
Linguistic MemoryMemory in the model is a localized connectionistimplementation f a semantic network similar to Fahlman's NETL(1979).
In NETL roles (slots), such as ACTOR, COLOR, andSUBJECT, take the form of nodes rather than links, and links areconfined to a small primitive set representing in particular the IS-A,HAS-A, and DISTINCTNESS relations.
In the present model, semanticnetwork links are replaced by pairs of weighted, directed connectionsof a single type, one connection for each direction.Linguistic knowledge is integrated into the rest of memory.
'Ille basic units of linguistic knowledge are generalizations of two typesof acts: illocntions and utterances.
In this paper we will be mainlyconcerned with the latter.
A generalized utterance (GU) is a schema(implemented asa network fragment) associating a morphosyntacticpattern with a semantic content and possibly contextual factors.
GUsinclude schemas for clauses, noun phrases, adjective phrases, andprepositional phrases.
They are arranged in a generalization hierarchywith syntactic structures at its more general end and phrasal lexicalentries at its more specific end.
Thus lexical entries in the model arejust a relatively specific type ofGU.
A GU normally has a noderepresenting the whole phrase, one or more nodes representingconstituents of the phrase, and one or more nodes representingsemantic or pragmatic aspects of the phrase.Figure 1 shows how a lexical ennui would be represented in asimplified version of the system which does not incorporateinformation about sequencing.
Nodes are denoted by rectangles andpairs of connections by lines.
For convenience schema boundaries areindicated by fuzzy rectangles with rounded comers, but theseboundaries have no significance in processing.
Node names likewise186are shown for convenience only; they are not accessible to the basicprocedures.
Names of lexical entries begin with an asterisk.
I.ower-case names indicate roles, and role names preceded by a colon areabbreviations of longer names.
In the figure, for example, ":content"represents the CONTENT of *SEND-MAIL.
The lexical entry shown inthe figure, *SEND-MAIL, represents clauses with a form of the wordsend as their main verb, the concept of ABSTRACT-TRANSFER astheir CONTENT, and MAIL as the MEANS of the transfer.
The schemais represented as a subtype of the general schema for clauses, fromwhich *SEND-MAIL implicitly inherits other iniormation (not shown inthe figure).Note that the *SEND-MAIL entry includes tile informationneeded to associate semantic and synaetic roles.
For example, there isa connection joining the CONTENT of the SUBJECT 3 constituent withthe ACTOR of the CONTENT of the whole clause, that is, the personperforming the instance of ABSTRACT-TRANSFER that is beingreferred to.
The other two constituents shown represent the nounphrases referring to the semantic OBJECT and the RECIPIENT of theABSTRACT-TRANSFER.
The former could also be referred to as the"direct object" of the clause.
The latter is realized either as an "indirectobject", as in Mary sent John the letter, or a prepositional phrase withto, as in Mary sent he letter to John.3.2.
Processing in GeneralEach node in the network has at any given time an activationlevel.
When the activation of a node reaches its filing threshold, thenode fires and sends activation along all of its output comaections.
Thefiring of a node represents a decision made by the system.
Forexample, the selection of a schema matching an input pattern isrepresented by the firing of the head node of the schema.
Followingfiring, a node is inhibited for an interval during which its state isunaffected by inputs from other nodes.
After this interval has passed,the node retains asmall amount of positive activation and can be furtheractivated from other nodes.Tim amount of activation spreading from one node to anotheris proportional to the weight on the connection from the source to thedestination ode.
The weight may be high enough to cause thedestination ode to fire on the basis of that activation alone.
Forexample, when activation spreads along a cmmection from an instanceto a type node, say, from JOHN to HUMAN, we generally want the typenode to fire immediately.
In most cases, however, activation frommore than one source is required for a node to fire.
Connectionweights may also be negative, in which case the relationship is aninhibitory one because the negative activation spread lessens thelikelihood of the destination node's firing.To simulate parallelism, the process is broken into time steps.During each time step, activation spreads from each firing node to theset of nodes directly connected to it.
(In some cases activation maycontinue to spread beyond this point.
)Sometimes we want only one node from a set to fire at a giventime.
For example, in the generation of a clause, the system shouldselect only one of the set of verb lexical entries.
In such cases themembers of the set form a network of mutually inhibiting nodes calleda winner-take-all (WTA) network (Feldman & Ballard 1982).The nodes art; activated through the firing of a source node which isconnected to all of the network members.
At this time one of thenetwork memher nodes may already have enough activathm tofire.
Ifnot, a specified interval is allowed to pass and if none of the membershas yet fired, they receive additional activation, which is usuallyenough to cause one of them to fire.
In any case, when one of thenodes fires, it immediately inhibits the others, effectively preventingthem from firing for the time being.3.3.
Language ProcessingLanguage processing can be viewed as a series of selections,eacll made or, the basis of a set of factors which make quantitativecontributions tothe decisions.
During sentence generation the itemsselected include general morphosyntaetic patterns for the sentence andits constituents (e.g., STATEMENT, COULD-YOU-QIJESTION,COUNTABLF.-NP, etc.)
and a set of lexical items to fill the slots in thesepatterns.
Dining sentence ~malysis the items selected include wordsenses, semantic roles to be assigned to referents, and intentions to beattributed to the speaker.In the present model the selection process is implemented interms of 1) the parallel convergence of activation on one or morecandidate nodes and 2) the eventual domin,'mce of one of these nodesover the others as a result of mutual inhibition through a WTAnetwork.
Consider the case of lexical selection in generation.
Alllcxical entries, such as *SEND-MAIL above, have a CONTENT role,and it is through this role that entries are selected uring generation.Activation converges on the CONTENT role of a lexical entry startingfrom nodes representing conceptual features of an input.
Any numberof lexical e\[mies may receive some activation for a given input, butN~.canse the CONTENT roles of entries inhibit each other through aWTA network, only one is selected.Input to generation consists of a set of firing nodesrepresenting a oal of the speaker.
As activation spreads from the inputnodes, it conw:rges on nodes representing a eneral pattern appropriatefor the goal type, for example, the STATEMENT pattern, and a set ofpatterns apprnpdate for the propositional content of the goal.
Theseinclude lexical patterns such as *SEND-MAIL and *LETTER as well asgl~unmatical patterns such as PAST-CLAUSE and INDEFINITE-NP.While some important aspects of parsing have not yet beenimplemented in CItlE, the basic mechanism works for parsing as wellas for generation.
Input consists of firing nodes representing words.These are given to the progran~ at intervals of four time steps.Activation from the word nodes converges on entries for lexical andsyntactic patterns.
For definite noun phrases, this leads to the firing ofnodes representing referents.
Verb entries specify the generalproposition types and also provide for temporary "role binding".
Rolebinding amomtts to the firing in close proximity of a node or set ofnodes representing a referent and a node representing itssemantic rolein the proposition.
However, the program, like most otherconnectionist models, currently has no way of storing these rolebindings in long-term emory.The model also has a decay mechanism reflecting theimportance of recency in processing.
The activation level of all nodesdecreases ata fixed rate.4.
SequencingIt is not a straightforward matter to implement sequentialbehavior within the confines of a system consisting of simpleprocessing units that are activated in parallel.
Alongside the basicproblem of ereating emergent sequential behavior from a parallelprocess, there is the need for sequencing information of two types to betransmitted.
When it is time for a constituent tobe produced, itneedsto signal its own daughter constituents to be produced in theappropriate sequence and, when these are completed, to signal sisterconstituents which follow it to be produced.The lhmst of this paper is that sequencing can be modelled likethe rest of language processing, that is, as a series of selections madeon the basis of interacting quantitative factors.
Consider first how theparallel activation spread is turned into a sequential process duringgeneration.
Activation spreads initially from nodes representing thesemantics and pragmatics of the utterance to nodes representing thelexical and grammatical patterns to be used, hut the thresholds of theconstituent odes of these patterns are such that the nodes cannot yetfire.
They fire ~nly when they have received addition,"d activation alongconnections specifying sequencing relations between constituents.When more than one constituent may follow a given constituent, therearc connections to all of the alternatives.
The weights on theseconnections represent degrees of syntactic expectation regarding whichconstituent will follow, and the constituent odes inhibit each othertlu~ugh a WFA network wlfich permits only one at a time to fire.
It isthe combination of the activation ret)resenting syntactic information midthat from other sources which determines which constituent wins outover the others and fires.
The firing of the winning constituentrepresents the selection of an item to fill the next output position.A second problem involves the two types of signals whichconstituent odes must send.
This problem is handled by having twonodes for each constituent orphrase, one representing the start and theother the end of the unit.
The start node signals daughter constituentsto be produced, and the end node signals following sister constituentsto be produced.Figure 2 illustrates some of the sequencing infonnation i  thc*SEND-MAIL entry.
Sta,t-end node pai1~ arc denoted by pairs of smallsquares urrounded by rectangles with rounded corners.
The uppersquare represents he start, the lower square the end of the word orphrase.
Single directional connections are indicated by arrow heads,and pairs of inhibitory coimections are denoted by fuzzy lines.
Tilefigalre includes ome sequence connections and the WTA networkwhich represents he competition between the OBJEC%REFERENCEand RECIPIENT-REFERENCE constituents for the position followingthe VERB.
Hem the WTA source is the VERB/end node, which sendsactivation to both the OBJECT-REFERENCE/start and RECIPIENT-REFERENCE\]start nodes.
These two nodes inhibit each other.
; .............................................................................................. " " "  7"~I MAiL I \[ I I rofo'renceii I NP IF!gore 2: Sequencin I~.~ormatiun in a Portion of *SEND-MAIL5.
An Example5.1.
GenerationConsider now the generation of sentence (la): Instead ofcalling John, Mary sent him a letter.
Generation begins with the firingof a set of network nodes representing a goal of the speaker.
In thisease the goal is that the hearer believe that a particular event (thesending of the letter) replaces one previously assumed to occur (themaking of a telephone call).
This type of goal leads the system togenerate a STATEMENT referring to the event preceded by a phrasewhich denies the assumption (i stead of calling John).
We concentratehere on the generation of the clause beginning with Mary and in18 "7particular on the Sequencing ofthe last two constituents.The event o be referred to is represented asan instance of thegeneral ABSTRACT-TRANSFER predicate (Schank & Abelson 1977)with MARY as the ACTOR, an instance of the concept LETI'ER as theOBJECT, JOHN as the RECIPIENT, and MAIL as the MEANS of thetransfer.
We ignore time and tense in order to simplify the discussion.The utterance of the initial instead of phrase results in processing of theconcepts of MARY and JOHN, so there is residual activation on thesenodes and the nodes immediately connected tothem.
A portion of thenetwork at this point is shown in Figure 3.
Nodes with hatchedpatterns are those with activation below the firing threshold level..~, '~J/////J/Z I/ \ v////~.-//tt/.-~ it - ' - 'g?SS"q.....
Figure 3: Portion of Input to Generation of (1 a) .
.
.
.
.
.
.
.
.
.
.Activation spreading from ABSTRACT-TRANSFER8 (i.e., thespecific transfer instance) converges on a set of verb lexical entries thatmay be used to describe the input notion.
Competition among theCONTENT roles of these ntries eventually forces one to win out.
Forthis example, we assume that the *SEND-MAIL entry wouldpredominate because of the fact that it matches the input MEANSfeature, though the entry for the verb mail would also be a strongcandidate.
A simplified view of this lexical entry selection process isshown in Figure 4.
The path of activation spread is indicated byarrows in the figure, blackened nodes are those that fire initially, andnodes with wide borders are those that fire in response to the spread ofactivation.
The fuzzy lines emanating from *SEND-MAIL:CONTENTare inhibitory connections toother verb CONTENT roles.Figure 4: Selection oftbe *SEND-MAIL Schema for (la)At the same time, activation spreading from ABSTRACT-TRANSFER8 causes the primed RECIPIENT node to fire, leading to aseries of firing nodes and eventually to the priming of the RECIPIENT-REFERENCE role in the *SEND-MAIL entry.
This process is shown inFigure 5.Once the *SEND-MAIL entry has been selected, activationspreads through it, resulting in the priming of the nodes representingthe constituents of the clause.
At the same time activation has alsospread to the constituent odes of the higher-level CLAUSE schema.The connections within this schema determine the order of theSUBJECT and VERB in the sentence.
The fact that he event referred to?
occurred before the time of speaking also leads to the selection of thePAST-CLAUSE schema, and this in combination with the *SEND-MAILschema results in the firing of the node representing the word sent.
Forthe purposes of this paper, we ignore the details o(these processes.When the verb has been produced, the VERB/end node in the*SEND-MAIL entry fires.
From here activation spreads to the nodesrepresenting the beginnings of the two possible following constituents:RECIPIENT-REFERENCE/start and OBJECT-REFERENCE/starL Thesenodes compete with one another via a WTA network.
In this case thepriming on the RECIPIENT-REFERENCE/start node leads thisconstituent towin out over OBJECT-REFERENCE/start.
The situationat this point is shown in Figure 6.Fi\[gure.5: Primin~ of RECIPIENT Constituent for (l a)Figure 6: Selection of Constituent toFollow Verb.!n.
(la)Next the NP schema takes over.
At this point there iscompetition between the schema for pronouns and that for full NPs.The pronoun schema wins out when there is evidence that he hearer iscurrently conscious of the referent.
In this case such evidence isavailable in the form of residual activation resulting from the referenceto John in the phrase instead of calling John.
For details on howspreading activation and competition implement the selection ofpronouns over full NPs, see Gasser (1988).When the NP is complete, activation is sent back to theRECIPIENT-REFERENCE/end node, which then activates the nodesrepresenting the two possibilities for what follows.
One is that theclause is complete.
This option would be the appropriate one if theRECIPIENT-REFERENCE had followed the OBJECT-REFERENCE (asin Mary sent a letter to John).
The other option, the one that isappropriate for tiffs example, is that he OBJECT-REFERENCE follows.The reason that both possibilities need to be represented is that thesystem has no explicit memory for what has or has not already beengenerated.
The weights on the two connections are such that thesecond alternative isthe default and will be preferred in this ease.
That188is, OBJECT-REFERENCE wins out, and the OBJECT-REFERENCE/start Imde fires.
As shown in Figure 7, tire selection offile OBJECT.REFERENCE role leads eventually to the firing of theOBJECT iole i l l  ATRANS8 and the LE'lqT.,R node,\ [~_%"  .
.
.
.
.
.
~ , .
.
~r / .
.
.
.
.
.\ ,tl j.Agaill contlol is passed to file NP schema, l/ere two lilrdlcrselections tat e place.
The fact that them is no evidcnce that the hearerknows the referent leads to selection of the INDEFINITE-NP schemaover the DEFINITE-NP schema by default.
INDENNITE-NP six:citiesthe indclinik~ mlicle a.
Finally, the lexical entry *\[,ETIEI( is selected asas a result of activation spreading from the I,ETTER i:lode.
Thisschema provides fl~e l~tun letter lor the OBJECf NP.Once the final constituent iscomplete, actiwttion is sent backto tile OBJECT-REFERF, NCE/end node.
Again there are twopossibilities lot what may ~ollow, the end of rite clau~z or file to casemarke~' and the following RECIPIENT-RFJ3ERENCE cottstitoent, Noun,however, rant there is an inhibitory connection from RECIPIENT-REFERENCE/end to RECIPIENT-MARKER/star.. That is, tl,,ecompletion of the RECIPIENT-tlEFERENCE effectively prevents thelater generali~m of the case marker, and as a consequence the repetitionof the RECIPIENT-REFFIIENCE itself.
Tile state of the network at thispoint is shown ill Figure 8.
The fuzzy filled pattern on RECIP1ENT-MARKFa//sla,:t indicates lhat tie node is inhibited.| Mi~IL/ F~\] - -TO'~.
.
.
.
.
.
.
.
.
l :~ ;  Co~let ion  of Genelation of (In)bl this example we have iw.-,~de use of sequencing intbrmationfound in the l~ieal entry *SEND-MA~L.
'Ills sort of irttb~anation alsoappear~ in irate general lexical entries such ~ *SEND and in non-lexieal GUs such ~ ATRANS-CLAUSE, file schema for elanses~eferfing to sa ABSTRAC~I'-TRANSFER.
If a specilic entry lacks therequired itffol mutton, a mote germral schema isused automatically.~.2.
ParshtgNow cow,sider how i/~z same information wotdd Ix; used in filepulsing of tl~, sentence Mary sent hbn a letter.
Recognition of theword Mary  leads to rite selection of the *MARY entry and theconsequent firing of the MARY node.
Recognition of the wold sentresults in the selection of the *SEND entry, which is similar to the morespecific *SEND-MAlt, entry shown in Figures 1, 2, 4, 5, 6, 7, ,and 8.Activation is scant immediately to the SUBJEC:r cons'tituent of the enlry,resulting eventually in file firing of tile ACTOR node.
li.
is file closepmximation of the firing of MARY and ACTOR which represents herole binding aspect of parsing.
Recall from 3.3 aixwe that thet~e iscurrently no way to record this binding pemlanently in file system'smemory.The firing of tile VERB\]end node in file *SEND schelna leads,as in the generation of the same sentence, to file activation of ntxlcs forboth of the constituents which may follow.
At this point neither ofthese constituents has enough activation to fire.
The activation that ispresent l~prescnts ile expectation that there will now be a reference ~oeither file RECIPIENT or the OP.JE(71'.Next the word him is recognized, leading to file activation ofall male humm~s lhat the system is cmrently "thinking abeut".
There isonly one such entity, John, and the JOHN node then fires.
Activationspreads to nodes for featnrcs of John including tile HUMAN node.Since humanness i  a default property of the RECIPIF, NT of anABS'IRACI'-TRANSFER, Ibis last node is connected to the RECIPIENTnode, which can now fin., sending activation ill turn eventually to tiltRECIPIENT-REFERENCE/start node in the *SEND schema.
't'hcadditional activalion now causes this node to lire, rcpresentir~g thesystem's recognition that tile current constituent refers Io tileRECIPIENT rather titan the OBJECF of the ABSTRA(YF-Ti~ANSI: ElL.Frorn this point on, the process, at least with respcc~ ,x)sequencing, is sinfilar to what goes on during generation.
After lh,?apt~aranee of the word him, activation spreads from the RECIPIENT.REFERENCE/end node to the nodes representing the two possiblealternatives, the end of the clause or the appearance of the OBJLCT-REFERENCE.
"File latter will predominate in this ez, a~nple once thebcgimfing of tile NP the letter is lecognizxxl.
POllowing the completionof this NP, there, will again be two alternatives, lu this case theCLAUSE/end option will win out, as in the generation case, because ofinlfibition on the node for the al|em~ttive.6.
implementation and CoverageTim, model described in this paper is implemented ill a plogrmncalled CIilE.
The program has two components, a hand-codedmemory network representing both world knowledge and linguisticknowledge and a set of procedures implementing spreading activationand inhibition through WTA networks.
CHIE generates sentences inEnglish and Japanese given input in the form of activated networknodes representing speaker gnats.
The model has been tested for asmall li'agment of the grammars of these languages: imple declarativeand interrogative clauses and imun phrases with adjective modifiers.In addition to obligatory constituents like those in the example above,tile program handles optional and optionally iterating constituents.
Theprogram also "parses" tilt structm'es that it ganerates using the samememory and the stone basic procedures, but, as noted in 3.3, it doesnot save a semantic interpretation; that is, it does not know how tocreate schema instantiations with role bindings.
(See Dolan & Dyer1987 for an approach to this problem within the connectionistfi'eanework.
)7o Related WorkWhile not adhering strictly to any familiar theoreticalframework, tbe present model has aspects in common with the PhlusalLexicon approach, (e.g., Jacobs 1985, Zemik & Dyer forthcoming),with plu'ase-oriented work in linguistics and psycholinguistics (e.g.,Filhnore, Kay, & O'Connor 1986, Pawley & Syder 1983, PeteJs1983), with other localized eonnectionist models (e.g., Cottrell &Small 1983, Waltz & Pollack 1985), ram with psychological modelsmaking use of spreading activation (e.g., Dell 1986, MacKay 1987).
'llae approach described in this paper is apparently the firsteftort o model anguage genelation lotally within the connectionistfi'~nncwork.
Ttve~ have been more limited effolls, however.
Kukich(1986) has lookexl at the distributed representation f ptrrases and howflte~ might be learned; however, she does not consider interactingI t9factors in sequencing.
Dell (1986) has developed a psycholinguisticmodel using spreading activation for selecting candidate items, but hismodel deals mainly with effects at the level of morphology andphonology.
Hasida, Ishizaki, and Isahara (1986) use a spreadingactivation mechanism toselect important information for generatingabstracts.
We view these three areas of research as complementing ourmodel.Unlike distributed eonnectionist models, e.g., those describedin McClelland, Rumelhart, and the PDP Research Group (1986),memory in the present model is localized; that is, each concept isrepresented by a single memory node.
This mode of representationbrings with it certain disadvantages, in particular, the property thatprocessing does not degrade gracefully when a portion of the memoryis destroyed.
On the other hand, the model maintains the constituencythat is basic to symbolic models and the need for which, as Fodor andPylyshyn (1988) argue, presents the most serious challenge todistributed models.
It should be clear from this paper that constituencyis fundamental to the way in which sequencing information isrepresented and used in the model.Within eonnectionist models the approach to sequencingadopted here is most similar to that suggested by Feldman and Ballard(1982) in that sequencing relations are represented xplicitly in the formof connections.
What we have done is elaborated on this approach todeal with the complications that arise in the generation oflanguage, inparticular, the interaction of semantic and syntactic effects insequencing.
In addition, our model appears to be the first connectionistmodel to make use of the same representation f sequencinginformation for generation and parsing (but see MacKay 1987 for apsychological theory with similar claims).Our work can also be constrasted with other approaches tosyntax in language analysis.
In some respects the flow of activationthrough entries uch as *SEND and *SEND-MAIL resembles what goeson in recursive transition etworks; however, there are three importantdifferences.
First, in this model syntax and semantics interact inprocessing, and the output of the system when used in parsingrepresents both a syntactic and a semantic analysis of the input.Second, the network can be used in both the generation and the parsingdirections.
Third, the formalism permits the representation and use oftendencies a weU as absolute constraints regarding sequencing.8.
Conclusions and Future WorkIn tiffs paper we have argued that since sequencing in languageinvolves competition among various quantitative factors, it can beprofitably modelled within a connectionist framework, and we havepresented a localized conneetionist scheme for representing and usingsequencing information i  language processing.
Key features of theapproach are the representation of phrasal units and their constituents aspairs of network nodes, one for the start and one for the end of thesequence; the representation of ordering constraints and tendencies aweighted connections; and the use of winner-take-all networks toimpose sequentiality on a parallel spreading activation mechanism.The model has been tested for a small set of simple clause andNP types.
It remains to be seen whether it can cover the range ofsequencing constraints and tendencies found in human languages, forexample, the requirement i  German and Dutch that he verb appear insecond position in clauses and the apparent total lack/of syntacticordering conventions in some Australian language& We are currentlyattempting toextend the model to handle such features.
We are alsoworking on a means of incorporating backtracking (in both generation;and parsing) into the model to simulate garden path effects.A further area of future research is the incorporation of alearning capability in the model, The major weakness of the modelthus far is the need to hand-wire the memory network, in particular toset the weights on the connections.
What we are wofldng toward is amodel that is able to adjust its own connection weights in response topresentations of input-output mappings, as is done in many distributedconneodonist approaches.NoteslThe research reported on here was supported in part by grants from190the ITA Foundation and the JTF program of be  U.S. Department ofDefense.2Address from August 15, 1988: Computer Science Department,Indiana University, Bloomington, Indiana 47405, USA3In its present form the entry applies to active clauses only.
Forsimplification we have ignored the possibility of passives.ReferencesBook, J. K. (1982) 'Toward a cognitive psychology of syntax:Information processing contributions to sentence formulation.
'Psychological Review 89, pp.
1-47.Cottrell, G. W. & S. L. Small (1983) 'A connectionist cheme formodelling word sense disambiguation.'
Cognition and BrainTheory 6, pp.
89-120.Dell, G. S. (1986) 'A spreading-activation theory of retrieval insentence production.'
Psychological Review 93, pp.
283-321.Dolan, C. P & M. G. Dyer (1987) 'Symbolic schemata, role binding,and the evoluation of structure in connectionist memories.Proceedings ofthe IEEE First Annual International Conference onNeural Networks.Fahlman, S. E. (1979) NETL: A System for Representing and UsingReal-World Knowledge.
Cambridge, MA: MIT Press.Feldman, J.
A.
& D. H. Ballard (1982) 'Connectionist models andtheir properties.'
Cognitive Science 6, pp.
205-254.Fillmore, C. G., P. Kay, & M. C. O'Connor (1986) 'Regularity andidiomaticity in grammatical constructions: The case of let alne.
'Unpublished manuscript.Fodor, J.
A.
& Z. W. Pylyshyn (1988) 'Connectionism and cognitivearchitecture: A critical analysis.-' Cognition 28, pp.
3-71.Gasser, M. (1988) A Connectionist Model of Sentence Generation i aFirst and Second Language.
Unpublished octoral dissertation,University of Califumia, Los Angeles.Hasida, K., S. Ishizaki, & H. Isahara (1986) 'A connectionistapproach to the generation of abstracts.'
In Kempen, G.
(ed.
),Natural Language Generation.
Dordrecht: Martinus Nijhoff, pp.149-156.Jacobs, P. S. (1985) 'PHRED: A generator for natural anguageinterfaces.'
Computational Linguistics 11, pp.
219-242.Kukich, K. (1986).
'Where do phrases come from: Some preliminaryexperiments in connectionist phrase generation.'
In Kempen, G.(ed.
), Natural Language Generation.
Dordrecht: Martinus Nijhoff,pp.
405-421.Langacker, R. W. (1987) Foundations of Cognitive Grammar (Vol.1).
Stanford, CA: Stanford University Press.MacKay, D. G. (1987).
The Organization ofPerception and Action: ATheory for Language and Other Skills.
New York: Springer-Verlag.McClelland, J. L. & A. H. Kawamoto (1986) 'Mechanisms ofsentence processing: Assigning roles to constituents of sentences.
'In McClelland, Rumelhart, & the PDP Research Group (eds.
), pp.272-325.McCleUand, J. L., D. E. Rumellaart, & the PDP Research Group (eds.
)(1986) Parallel Distributed Processing.
Explorations in theMiemstruetures of Cognition: Vol.
2: Psychological nd BiologicalModels.
Cambridge, MA: MIT Press.Pawley, A.
& F. H. Syder (1983) 'Two puzzles for linguistic theory:Nativelike selection and nativeUke fluency.'
In Rtehards, J. C. &R. W. Schmldt (eds.
), Language and Communication.
London:Longman.Peters, A. M. (1983) The Units of Language Acquisition.
Cambridge:Cambridge University Press.Schank & Abelson (1977) Scripts, Plans, Goals, and Understanding.Hillsdale, NJ: Lawrence Eflbanm.Waltz, D. L. & J.
B. Pollack (1985) 'Massively parallel parsing: Astrongly interactive model of natural anguage interpretation.
'Coguitve Science 9, pp.
51-74.Zemik, U.
& M. G. Dyer (forthcoming) 'The self-entending phrasallexicon.'
Computational Linguistics.
