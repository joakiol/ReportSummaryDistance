Proceedings of the 6th Workshop on Statistical Machine Translation, pages 227?236,Edinburgh, Scotland, UK, July 30?31, 2011. c?2011 Association for Computational LinguisticsFuzzy Syntactic Reorderingfor Phrase-based Statistical Machine TranslationJacob Andreas and Nizar Habash and Owen RambowCenter for Computational Learning SystemsColumbia Universityjda2129@columbia.edu{habash,rambow}@ccls.columbia.eduAbstractThe quality of Arabic-English statistical ma-chine translation often suffers as a result ofstandard phrase-based SMT systems?
inabil-ity to perform long-range re-orderings, specif-ically those needed to translate VSO-orderedArabic sentences.
This problem is further ex-acerbated by the low performance of Arabicparsers on subject and subject span detection.In this paper, we present two parse ?fuzzi-fication?
techniques which allow the transla-tion system to select among a range of pos-sible S?V re-orderings.
With this approach,we demonstrate a 0.3-point improvement inBLEU score (69% of the maximum possibleusing gold parses), and a corresponding im-provement in the percentage of syntacticallywell-formed subjects under a manual evalua-tion.1 IntroductionThe question of how to effectively use phrase-basedstatistical machine translation (PSMT) to translatebetween language pairs which require long-range re-ordering has attracted a great deal of interest in re-cent years.
The inability to capture long-range re-ordering behaviors is a weakness inherent in PSMTsystems, which typically have only two mechanismsto control the reordering between source and tar-get language: (1) distortion penalties, which penal-ize or forbid long-distance re-orderings in order toreduce the search space explored by the decoder,and (2) lexicalized reordering models, which cap-ture the preferences of individual phrases to orientthemselves monotonically, reversed with their pre-ceding phrases or discontinuously.
Because bothof these mechanisms work at the phrase level, theyhave proven very effective at capturing short-rangereordering behaviors, but unable to describe longrange movements; in fact, the distortion penalty ef-fectively causes the translation system to not pre-fer long-range re-orderings, even when they are as-signed significantly higher probability by the lan-guage model.The problem is particularly acute in translatingfrom Arabic to English: Arabic sentences frequentlyexhibit a VSO ordering (both VSO and SVO arepermitted in Arabic), while English permits onlyan SVO order.
Past research has shown that verbanticipation and subject-span detection is a ma-jor source of error when translating from Arabicto English (Green et al, 2009; Bisazza and Fed-erico, 2010).
Unable to perform long-range reorder-ing, PSMT frequently produces English sentences inwhich verbs precede their subjects (sometimes with?hallucinated?
pronouns in front of them) or do notappear at all.
Intuitively, better handling of these re-orderings has the potential to improve both accuracyand fluency of translation.In this paper, we present two parse fuzzificationtechniques which allow the translation system to se-lect among a range of possible S?V re-orderings.With this approach, we demonstrate a 0.3-point im-provement in BLEU score (69% of the maximumpossible using gold parses), and a corresponding im-provement in the percentage of syntactically well-formed subjects under a manual evaluation.The rest of the paper is structured as follows.
Sec-tion 2 gives a review of research on this topic.
Sec-tion 3 motivates the approach discussed in Section 4.227Section 5 presents the results of a set of machinetranslation experiments using the automatic metricsBLEU (Papineni et al, 2002) and METEOR (Baner-jee and Lavie, 2005), and a manual-evaluation ofsubject integrity.
Section 6 discusses our conclu-sions and future plans.2 Related WorkThe general approach pursued in this paper?thatof using pre-ordering to improve translation output?has been explored by many researchers.
Mostwork has focused on automatically learning reorder-ing rules (Xia and McCord, 2004; Habash, 2007b;Elming, 2008; Elming and Habash, 2009; Dyerand Resnik, 2010).
Xia and McCord (2004) de-scribe an approach for translation from French toEnglish, where context-free constituency reorderingrules are acquired automatically using source andtarget parses and word alignment.
Elming (2008)and Elming and Habash (2009) use a large set oflinguistic features to automatically learn reorderingrules for English-Danish and English-Arabic; therules are used to pre-order the input into a latticeof variant orders.
Habash (2007b) learns syntacticreordering rules targeting Arabic-English word or-der differences and integrated them as deterministicpreprocessing.
He reports improvements in BLEUcompared to phrase-based SMT limited to mono-tonic decoding, but these improvements do not holdwith distortion.
He hypothesizes that parse errorsare responsible for lack of improvement.
Dyer andResnik (2010) use an input forest structure to rep-resent word-order alternatives and learn models forlong-range source reordering that maximize trans-lation quality.
Their results for Arabic-English arenegative.In contrast to these approaches, Collins et al(2005) apply six manually defined transformationsto German parse trees which yield an improvementon a German-English translation task.
In this paper,we follow Collins et al (2005) and restrict ourselvesto handcrafted rules (in our case, actually a singleover-generating rule) motivated by linguistic under-standing.One major concern not addressed in any of theaforementioned research on syntax-based reorderingis the fact that the quality of parsers for many lan-guages is still quite poor.
Collins et al (2005), forexample, assume that the parse trees they use arecorrect.
While the state-of-the-art in English pars-ing is fairly good (though far from perfect), thisis not the case in other languages, where parsingshows substantial error rates.
Moreover, when at-tempting to reorder so as to bring the source textmore grammatically in line with the target language,a bad parse can be disastrous: moving parts of thesentence that shouldn?t be moved, and introducingmore distortion error than it is able to correct.
To ad-dress the problem of noisy parse data, Bisazza andFederico (2010) identify the subject using a chunker,then fuzzify it, creating a lattice in which the transla-tion system has a choice of several different paths,corresponding to re-orderings of different subjectspans.In investigating syntax-based reordering for Ara-bic specifically, Carpuat et al (2010) show that asyntax-driven reordering of the training data onlyfor the purpose of alignment improvement leads toa substantial improvement in translation quality, butdo not report a corresponding improvement when re-ordering test data in a similar fashion.
Interestingly,Bisazza and Federico (2010) report that fuzzy re-ordering the test data improves MT output, suggest-ing that fuzzification may be the mechanism neces-sary to render reordering on test data useful.
To thebest of our knowledge, nobody has yet used fuzzifi-cation to correct the identified subject span of com-plete Arabic dependency parses.
Green et al (2009)use a conditional random field sequence classifierto detect Arabic noun phrase subjects in verb-initialclauses achieving an F-score of 61.3%.
They in-tegrate their classifier?s decisions as additional fea-tures in the Moses decoder (Koehn et al, 2007), butdo not show any gains.The present work may be thought of as extendingthe fuzzification explored by Bisazza and Federico(2010) to the domain of full parsing?a combina-tion, in some sense, of their approach with the workof Carpuat et al (2010).
The approach examined inthis paper differs from Collins et al (2005) in its useof fuzzification, from Bisazza and Federico (2010)in its use of a complete dependency parse, and fromCarpuat et al (2010) in its use of a reordered test set.228Figure 1: An example of a dependency tree of a Verb-Object-Subject Arabic sentence: 	?A??j.
?
??J?
@ ZA??
?AKQ?
@ 	Q?
?J 	j 	j 	??
?KPAJ?
+H.
hz AlryAD msA?
Alywm hjwmAn b+ syArtyn mfxxtyn ?Two car bombs shook Riyadh thisevening?.
The predicted tree (on the left) shows an incorrect subject span (words 5-8).Figure 1: An example of a dependency tree of a Verb-Object-Subject Arabic sentence: ???????
?
?????
?
??
?
???????
?
?????????
??
??
???
???????????
+??
hz AlryAD msA?
Alywm hjwmAn b+ syArtyn mfxxtyn ?Two car bombs shook Riyadh thisevening?.
The predicted tree (on the left) shows an incorrect subject span (words 5-8).Gold TreeVRB???
hz1?shook?OBJPROP???????
?
AlryAD2?RiyAdh?MODNOM??
?
msA?3?evening?IDFNOM?????
?
Alywm4?today?SBJNOM???????
?
hjwmAn5?two attacks?MODPRT+??
b+6?with?OBJNOM???????????
syArtyn7?two cars?MODNOM??????
??
??
???
mfxxtyn8?booby-trapped?Predicted TreeVRB???
hz1?shook?MODPROP???????
?
AlryAD2?RiyAdh?MODNOM??
?
msA?3?evening?IDFNOM?????
?
Alywm4?today?SBJNOM???????
?
hjwmAn5?two attacks?MODPRT+??
b+6?with?OBJNOM???????????
syArtyn7?two cars?MODNOM??????
??
??
???
mfxxtyn8?booby-trapped?We focused on correcting the largest sources of er-ror: incorrect span and false-positive subjects.
Asfalse-positive subject corrections were already cap-tured by providing a no-reorder option in the lattice,only span errors needed additional correction.In principle, spans can be marked incorrectly bothon their front and back ends; however, because left-dependency is fairly uncommon in Arabic and hap-pens in a limited number of predictable cases, thesystem made so few errors in identifying the leftboundary of spans (1.8%) that it is not worth try-ing to correct them.
[A note on terminology: ?left?and ?right?
are used throughout this paper with ref-erence to English word order.
?Left?
should be un-derstood to mean ?towards the beginning of the sen-tence?, and ?right?
to mean ?towards the end of thesentence.?
]The question is thus how to correct the right edgeof spans assuming that label and attachment havebeen predicted correctly.
Span classifications can bebroken into three categories: those that are too long(i.e.
that have too many right descendants), too short(i.e.
that have too few right descendants), or correct(so that the predicted tree has all the same descen-dants as the gold tree).
A comparison of gold andpredicted trees for MT05 was conducted, revealingthe following breakdown:Type # %Long 260 12.4%Short 293 14.0%Correct 1538 73.6%Total 2091 100%Table 1: Distribution of span errorsThese numbers are quite low: roughly 3 out ofevery 10 subjects identified in the corpus have theirspans incorrectly marked.
This suggest that fuzzifi-cation will provide room for improvement.
But whattechnique should we use to fuzzify the subjects?To answer this question, we examined more3 MotivationWhile the VSO order is common at both the matrixand non-matrix level in Arabic newswire text, ma-trix VSO constructions are almost always reorderedin translation, while non-matrix VSO constructionsare frequently translated monotonically (they are in-stead passivized or otherwise transformed in a fash-ion that leaves them parallel to the source Arabictext) (Carpuat et al, 2010).
This reordering, asnoted in the introduction, is notoriously difficult forphrase-based statistical machine translation systemsto capture.
It is further exacerbated by the lowquality of Arabic parsing especially for subject spanidentification (Green et al, 2009).3.1 ReorderingWe began by performing a series of reordering ex-periments using gold-standard parses of the NISTMT05 data set:1 (a) a baseline experiment with noreordering, (b) an experiment which forced reorder-ing on all matrix subjects, and (c) an experiment inwhich the translation system was presented with alattice, in which one path contained the original sen-tence and the other path contained the sentence withthe matrix subject reordered.
The baseline systemproduced a BLEU score of 47.13, forced reorder-ing produced a BLEU score of 47.43, and optionalreordering produced a BLEU score of 47.55.
Theseresults indicate that, given correct reordering bound-aries, the translation quality can indeed be improvedwith reordered test data.
Furthermore, the improve-ment noted above between the orced eordering andoptional reordering experiments, while small, indi-cates that even with correct parses it is sometimespreferable to leave the input sentence un-reordered.This is consistent with Carpuat et al (2010)?s ob-1The gold parses for NIST MT05 are part of the ColumbiaArabic Treeebank (CATiB) (Habash and Roth, 2009).229servation that even VS-ordered matrix verbs in Ara-bic are sometimes translated monotonically into En-glish (as, for example, in passive constructions).
Analternative explanation may be that since the train-ing data itself is not re-ordered, it is plausible thatsome re-ordering may cause otherwise good possi-ble matches in the phrase table to not match anymore.3.2 Parser ErrorThe problem of finding correct subject span bound-aries for reordering, however, is a particularly dif-ficult one.
Both Habash (2007b) and Green et al(2009) have noted previously that even state-of-the-art Arabic dependency parsers tend to performpoorly, and we would expect that incorrect bound-aries would do more harm than good for translation.In order to determine how to ?fix?
these spans, it isfirst necessary to understand the kinds of errors thatthe parser makes.
A set of predicted parses of theNIST MT05 data was compared to the gold parsesof the same data set.There are three categories of error the parser canmake in identifying subjects: labeling errors, attach-ment errors and span errors.
In labeling errors, theparser either incorrectly marks a node SBJ when nosuch label appears in the gold tree, or fails to identifyone of the gold-labeled SBJs.
In attachment error,the identified subject is marked as depending on thewrong node.
Finally, in span error, the descendantsassigned to a labeled SBJ are wrong.
The distribu-tion of parser errors in the NIST MT05 data is asfollows:?
Label errors: 19.8% of predicted subjects arenot gold subjects, and 19.1% of gold subjectsare not identified as predicted subjects.?
Attachment errors: 16.92% of gold subjects areincorrectly attached in the predicted tree.?
Span errors: 26.4% of predicted subject spansare incorrect.In this paper, we focus on correcting the largestsources of error: incorrect span and false-positivesubjects.
We now provide further analysis of thespan errors.In principle, spans can be marked incorrectly bothon their front and back ends; however, because left-dependency is fairly uncommon in Arabic and hap-pens in a limited number of predictable cases, theparser made so few errors in identifying the leftboundary of spans (1.8%) that it is not worth tryingto correct them.2The question is thus how to correct the right edgeof spans assuming that label and attachment havebeen predicted correctly.
Span classifications canbe broken into three categories: those that are toolong (i.e.
that have too many right descendants), tooshort (i.e.
that have too few right descendants), orcorrect (so that the predicted tree has all the samedescendants as the gold tree, without regard to theirsyntactic structure).
A comparison of gold and pre-dicted trees for MT05 was conducted, revealing thedistribution shown in Table 1.
We see that the 26.4%of subjects with incorrect spans are roughly equallydivided between subjects that are too short and sub-jects that are too long.Type # %Long 260 12.4%Short 293 14.0%Correct 1538 73.6%Total 2091 100%Table 1: Distribution of span errors in NIST MT05To gain further insight into the nature of the sub-ject span errors, we examined more closely the26.4% of cases where the span is incorrectly labeled,looking specifically at the ?difference box?
: the setof contiguous nodes that must be added to or re-moved from the predicted span to bring it into agree-ment with the gold span (see Fig.
1).3 Specifically,we wished to know how many top-level constituentsrequired addition or removal to cover the entire dif-ference.
The smaller the number of top-level con-stituents that needs to be added, the fewer reorder-ing variations possible, and the better the expectedperformance of the system.Roughly 2% of these difference boxes are whatwe might call ?pathological?
cases: due to some se-2A note on terminology: ?left?
and ?right?
are used through-out this paper with reference to word order when using the Latinalphabet.
?Left?
should be understood to mean ?towards the be-ginning of the sentence?, and ?right?
to mean ?towards the endof the sentence.
?3Arabic transliteration is presented in the Habash-Soudi-Buckwalter scheme (Habash et al, 2007).230r2r1a1a2 sbjr2r1a1a2originalverb++__Figure 2: A schematic representation of the fuzzification algorithm.
The black node is the matrix subject, + indicatesthat a node (and its descendants) can be added, ?
indicates that a node (and its descendants) can be removed, and theblack brackets denote the boundaries of the candidate spans.rious error in parsing, there is a constituent insidethe difference box with descendants outside the box.These are algorithmically very difficult to correct asthey require us to either add a constituent and thenprune it, or remove a constituent and then reattachsome of its children; attempting to correct for thispossibility in all sentences will lead to a combinato-rial explosion of possible parses.
Fortunately, thesepathological cases make up a small enough portionof the data set that they can be safely disregarded.More promisingly, 66.5% of incorrect spans canbe corrected with the addition or removal of a singleconstituent; in other words, the recall of span iden-tification can be improved from 73.6% to 91.2% byadding or removing at most one constituent at theend of the parser?s identified span.4 ApproachTo improve translation of matrix subjects, we im-plement fuzzy reordering by using a lattice-basedapproach similar to Bisazza and Federico (2010) tocorrect the matrix subject spans identified by a state-of-the-art dependency parser (Marton et al, 2010).Specifically, we take a twofold approach to fuzzyreordering.
First, we present the translation systemwith both un-reordered and reordered options.
Thisis motivated by the observation that on gold parses,optional reordering outperformed forced reordering(Section 3.1).
Second, we apply a fuzzification algo-rithm to the reordered subject span, adding yet moreoptions to the lattice.
This is motivated by the ob-servation that the greatest source of parsing errorsin subjects is span errors (Section 3.2).
We discussthese two techniques in turn.4.1 Optional ReorderingIn keeping with results from the initial gold experi-ments, we decided to generate a lattice identical tothat used for the optional-reordering experiment, inwhich the translation system was presented with theinput sentence both un-reordered and reordered, us-ing a predicted parse to perform the reordering.4.2 Subject Span FuzzificationThe observation that 91.2% of spans can be recalledwith single-constituent modifications led very natu-rally to the following fuzzification algorithm, whichis illustrated in Fig.
2:1.
For each matrix subject in the parse tree4, cre-ate an empty list to hold fuzzified boundaries.2.
Original span: Add to the list the tuple (l, r, v),where l is the index of the predicted span?s left-most descendant, r is the index of the predictedspan?s rightmost descendant and v is the verb4Allowance must be made for parsers which incorrectlyidentify multiple subjects for the matrix verb.231that the predicted span attaches to.
(This stepproduces the span labeled ?original?
in Fig.
2.)3.
Expansion: Add to the list all tuples of the form(l, r+, v), where r+ is the index of the right-most descendant of a node whose leftmost de-scendant has index r + 1.
(This step producesthe spans labeled ?a1?
and ?a2?
in Fig.
2.)4.
Contraction: Add to the list all tuples of theform (l, r?
?1, v), where r?
is the index of theleftmost descendant of a node whose rightmostdescendant has index r. (This step produces thespans labeled ?r1?
and ?r2?
in Fig.
2.)5.
Create the list of all valid combinations ofspans by taking the Cartesian product of allthe per-subject span lists, and rejecting all en-tries in which two spans overlap.
(This step ac-counts for multiple subject cases.
)The result of this algorithm is a list of lists of tuples,where each tuple defines a single reordering, andeach list of tuples defines a set of spans that must bemoved to the left of the matrix verb for one reorder-ing.
These re-orderings are then joined together toform the final lattice.
If a single-constituent correc-tion to the span exists (except in the aforementionedpathological and left-attachment cases), it is guaran-teed to appear as one path through the lattice.5 Evaluation5.1 Experimental SetupWe used the open-source Moses PSMT toolkit(Koehn et al, 2007).
Training data was a newswire(MSA-English) parallel text with 12M words on theArabic side (LDC2007E103)5 Sentences were re-ordered only for alignment, following the approachof Carpuat et al (2010).
Parses were obtained usinga publicly available parser for Arabic (Marton et al,2010).
GIZA++ was used for word alignment (Ochand Ney, 2003) and phrase translations of up to 10words are extracted in the Moses phrase table.
Thesame baseline phrase table was used in all experi-ments.The system?s language model was trained both onthe English portion of the training corpus and En-glish Gigaword (Graff and Cieri, 2003).
We used a5All data is available from the Linguistic Data Consortium:http://www.ldc.upenn.edu.5-gram language model with modified Kneser-Neysmoothing implemented using the SRILM toolkit(Stolcke, 2002).
Feature weights were tuned withMERT (Och, 2003) to maximize BLEU on the NISTMT06 corpus.
MERT was done only for the baselinesystem; these same weights were used for all exper-iments to control for the effect of MERT instability.In the future, we plan to experiment with approach-specific optimization and to use recent publishedsuggestions on controlling for optimizer instability(Clark et al, 2011).English data was tokenized using simplepunctuation-based rules.
Arabic data was seg-mented with to the Arabic Treebank tokeniza-tion scheme (Maamouri et al, 2004) using theMADA+TOKAN morphological disambiguator andtokenizer (Habash and Rambow, 2005; Habash,2007a; Roth et al, 2008).
The Arabic text wasalso Alif/Ya normalized (Habash, 2010).
MADA-produced Arabic lemmas were used for wordalignment.We compare four settings with predicted parses(as opposed to the gold parse experiments discussedin Section 3):?
BASE An un-reordered test set;?
FORCE A test set which forced reordering onmatrix verbs;?
OPT A test set with fuzzification through op-tional reordering on matrix verbs; and?
SPAN A test set with fuzzification through op-tional reordering on matrix verbs and throughfuzzification of the subject span according tothe algorithm shown in Section 4.2.Each reordering corpus used Moses?
lattice inputformat (Dyer et al, 2008) (including the baselines,which had only one path).
Results are presented interms of the standard BLEU metric (Papineni et al,2002), METEOR metric (Banerjee and Lavie, 2005)and a manual evaluation targeting subject span trans-lation correctness.5.2 Automatic Evaluation ResultsTable 2 presents the results for the experiments dis-cussed above.
Columns three and Four (Prec-1gand Prec-4g) indicate the corresponding 1-gram and4-gram (sub-BLEU) precision scores, respectively.232System BLEU Prec-1g Prec-4g METEORBASE 47.13 81.91 29.52 53.09FORCE 47.03 81.78 29.52 53.11OPT 47.42 81.88 30.04 53.22SPAN 47.41 81.92 30.03 53.21Table 2: Automatic evaluation resultsBoth OPT and SPAN showed a statistically signif-icant improvement in BLEU score over BASE andFORCE above the 95% level.
Statistical signifi-cance is computed using paired bootstrap resam-pling (Koehn, 2004).
The difference between OPTand SPAN, however, was not statistically significant.The relatively small difference in BLEU score be-tween the baseline and gold reordering (Section 3:baseline 47.13 and optional reordering 47.55) sug-gests that we should expect at most a modest in-crease in BLEU from improving the predicted trees.The first key observation in these results is thatwith a noisy parser, translation quality actually goesdown with forced reordering?the opposite of whatwas observed in the gold experiment.
By introduc-ing either optional reordering or complete fuzzifi-cation, however, BLEU score increases .3 past thebaseline to achieve nearly three quarters of the gainobtained by optional reordering using the gold parse(Section 3: baseline 47.13 and optional reordering47.55).
In other words, it is possible to compensatefor the parser noisiness without actually attemptingto correct spans: simply allowing the translation sys-tem to fall back on an un-reordered input leads to asignificant gain in BLEU.One possible explanation for this fact is that weonly ever correct for parses on the right-hand side?the left sides are virtually always correct.
Thus,when we perform any reordering, even if the subjectspan is not entirely perfect, we guarantee that webring at least one word from the sentence (and usu-ally more) into alignment where it was out of align-ment before; this obviously leads to better BLEUn-gram scores along that boundary.The general trend in these results is confirmed bythe results of a METEOR analysis, also provided inTab.
2.
Again, both the OPT and SPAN systemsresult exhibit comparable performance, and demon-strate an improvement over the baseline.The second observation is that introducing spanfuzzification did not improve over simple optionalreordering.
There are a several reasons this could behappening:?
The increased fluency and introduction of un-seen phrases cancel each other out.?
All the gains that come from reordering occurat the left; the presence or absence of correctwords at the right end is less important.?
Better sentences are proposed during the trans-lation process, but they are not selected duringthe final filtering stage.?
The sentences being output are actually better,but the improvement is not captured by the au-tomatic evaluation.Further experiments will be necessary to determinewhether any of the first three possibilities is the case.We next consider the fourth possibility in more de-tail.5.3 Manual EvaluationWe additionally conducted a manual evaluation toexamine how subject quality differed in fuzzified vs.unfuzzified parses.
Each sentence examined was as-signed one of the six labels below.
Examples arewith respect to the reference sentence ?Recep TayyipErdogan announced that Turkey is strong.??
MM: both verb and subject missing.
?Turkeyis strong.??
MV: verb missing.
?Recep Tayyip ErdoganTurkey is strong.??
MS: subject missing.
?announced that Turkeyis strong.??
SO: subject overlaps with verb.
?Recep an-nounced Tayyip Erdogan Turkey is strong.??
SI: verb precedes subject (as in Arabic).
?an-nounced Recep Tayyip Erdogan that Turkey isstrong.??
C: verb follows subject (as in English), i.e.
thecorrect ordering.
?Recep Tayyip Erdogan an-nounced that Turkey is strong.?
We also includein this category sentences where the Englishreference contains no verb (e.g.
in newspaperheadlines).233System MM MS MV SI SO C M* S* CBASE 8 13 11 9 3 53 33 12 53OPT 7 11 10 5 5 61 28 10 61SPAN 8 10 09 5 2 64 27 7 64Table 3: Subject integrity analysis results.
All numbers are %s.By grouping some of these categories together, weobtained the following label scheme:?
M*: MM, MV or MS, i.e.
verb or subject ismissing.?
S*: SO or SI, i.e.
word order is incorrect.?
C: as above.280 sentences selected randomly from our test setwere evaluated, generating 461 unique output sen-tences.
Annotation was performed by two Englishspeakers, with 40 input sentences (68 unique out-puts) annotated by both authors to collect agreementstatistics.
For the complete label scheme, the an-notators agreed on 86.8% of labels, with Cohen?s?
= 0.811.
For the simple label scheme, the an-notators agreed on 92.6% of labels, with ?
= .883.Results for the BASE, OPT and SPAN systems areshown in Table 3.
Each annotator?s labels were as-signed a weight of .5 in the section that was jointlyannotated.Again, both the OPT and SPAN systems displaystatistically significant improvements over the base-line system (p < 0.001).
While the SPAN systemconsistently displays better results than the OPT sys-tem, the significance is low (p < .3).
Statistical sig-nificance was measured using the McNemar test ofstatistical significance (McNemar, 1947).These results thus agree with the BLEU score inindicating that the OPT and SPAN systems are sub-stantially better than the baseline, but statistically in-distinguishable from each other.
They further in-dicate that most of the improvements in the OPTsystem come from preventing dropped subjects orverbs, while the improvements in the SPAN systemresult in roughly equal proportion from preventingword-dropping and ensuring correct ordering.6 Conclusion & Future WorkWe presented an approach for improving Arabic-English PSMT using syntactic information from anoisy parser.
We demonstrated that translation qual-ity goes down with forced reordering, but improveswith the introduction of either optional reorderingand subject span fuzzification.
The BLEU score in-creases by 0.3% absolute past the baseline achievenearly three quarters of the maximum possible gainstarting with gold parses.
A detailed manual eval-uation produces results generally consistent withBLEU, but highlights the small improvements thatcan be gained by subject span fuzzification.In the future, we plan to explore a more sophis-ticated approach to the lattice of re-orderings pre-sented here.
We would take into account the fact thatit is possible to suggest to the system that certainre-orderings are less likely than others without re-moving them from the search space completely.
Thesame can be done for the fuzzification task: whilewe might wish to add additional fuzzification op-tions, we also don?t want the correct choice to becrowded out by too many alternatives.AcknowledgementsWe would like to thank Marine Carpuat, Yuval Mar-ton, Amit Abbi and Ahmed El Kholy for helpful dis-cussions and feedback.
The second and third authorswere supported by the Defense Advanced ResearchProjects Agency (DARPA) under GALE ContractNo HR0011-08-C-0110.
Any opinions, findings andconclusions or recommendations expressed in thismaterial are those of the authors and do not neces-sarily reflect the views of DARPA.ReferencesSatanjeev Banerjee and Alon Lavie.
2005.
METEOR:An Automatic Metric for MT Evaluation with Im-proved Correlation with Human Judgments.
In Pro-ceedings of the ACL Workshop on Intrinsic and Ex-trinsic Evaluation Measures for Machine Transla-tion and/or Summarization, pages 65?72, Ann Arbor,Michigan.234Arianna Bisazza and Marcello Federico.
2010.
Chunk-based verb reordering in VSO sentences for Arabic-English statistical machine translation.
In Proceedingsof ACL 2010: Joint Fifth Workshop on Statistical Ma-chine Translation and MetricsMATR, Uppsala, Swe-den.Marine Carpuat, Yuval Marton, and Nizar Habash.2010.
Improving Arabic-to-English Statistical Ma-chine Translation by Reordering Post-Verbal Subjectsfor Alignment.
In Proceedings of the ACL 2010 Con-ference Short Papers, pages 178?183, Uppsala, Swe-den, July.Jonathan H. Clark, Chris Dyer, Alon Lavie, and Noah A.Smith.
2011.
Better hypothesis testing for statisti-cal machine translation: Controlling for optimizer in-stability.
In Proceedings of the 49th Annual Meet-ing of the Association for Computational Linguistics:Human Language Technologies, pages 176?181, Port-land, Oregon, USA.Michael Collins, Philipp Koehn, and Ivona Kucerova.2005.
Clause Restructuring for Statistical MachineTranslation.
In Proceedings of the 43rd Annual Meet-ing of the Association for Computational Linguistics(ACL?05), pages 531?540, Ann Arbor, Michigan.Chris Dyer and Philip Resnik.
2010.
Context-free re-ordering, finite-state translation.
In Human LanguageTechnologies: The 2010 Annual Conference of theNorth American Chapter of the Association for Com-putational Linguistics, pages 858?866, Los Angeles,California.Christopher Dyer, Smaranda Muresan, and Philip Resnik.2008.
Generalizing word lattice translation.
In Pro-ceedings of ACL-08: HLT, Columbus, Ohio.Jakob Elming and Nizar Habash.
2009.
Syntactic Re-ordering for English-Arabic Phrase-Based MachineTranslation.
In Proceedings of the EACL 2009 Work-shop on Computational Approaches to Semitic Lan-guages, pages 69?77, Athens, Greece, March.J.
Elming.
2008.
Syntactic reordering integrated withphrase-based smt.
In Proceedings of the ACL Work-shop on Syntax and Structure in Statistical Translation(SSST-2).David Graff and Christopher Cieri.
2003.
English Gi-gaword, LDC Catalog No.
: LDC2003T05.
LinguisticData Consortium, University of Pennsylvania.Spence Green, Conal Sathi, and Christopher D. Man-ning.
2009.
NP Subject Detection in Verb-initial Ara-bic Clauses.
In Proceedings of the Third Workshopon Computational Approaches to Arabic Script-basedLanguages (CAASL3).Nizar Habash and Owen Rambow.
2005.
Arabic Tok-enization, Part-of-Speech Tagging and MorphologicalDisambiguation in One Fell Swoop.
In Proceedings ofthe 43rd Annual Meeting of the Association for Com-putational Linguistics (ACL?05), pages 573?580, AnnArbor, Michigan.Nizar Habash and Ryan Roth.
2009.
CATiB: TheColumbia Arabic Treebank.
In Proceedings of theACL-IJCNLP 2009 Conference Short Papers, pages221?224, Suntec, Singapore.Nizar Habash, Abdelhadi Soudi, and Tim Buckwalter.2007.
On Arabic Transliteration.
In A. van den Boschand A. Soudi, editors, Arabic Computational Mor-phology: Knowledge-based and Empirical Methods.Springer.Nizar Habash.
2007a.
Arabic Morphological Repre-sentations for Machine Translation.
In A. van denBosch and A. Soudi, editors, Arabic ComputationalMorphology: Knowledge-based and Empirical Meth-ods.
Springer.Nizar Habash.
2007b.
Syntactic preprocessing for statis-tical machine translation.
In Proceedings of the 11thMT Summit.Nizar Habash.
2010.
Introduction to Arabic NaturalLanguage Processing.
Morgan & Claypool Publish-ers.Philipp Koehn, Hieu Hoang, Alexandra Birch, Christo-pher Callison-Burch, Marcello Federico, NicolaBertoldi, Brooke Cowan, Wade Shen, ChristineMoran, Richard Zens, Christopher Dyer, Ondrej Bo-jar, Alexandra Constantin, and Evan Herbst.
2007.Moses: open source toolkit for statistical machinetranslation.
In Proceedings of the 45th Annual Meet-ing of the Association for Computational LinguisticsCompanion Volume Proceedings of the Demo andPoster Sessions, pages 177?180, Prague, Czech Re-public.Philipp Koehn.
2004.
Statistical significance tests for-machine translation evaluation.
In Proceedings of theEmpirical Methods in Natural Language ProcessingConference (EMNLP?04), Barcelona, Spain.Mohamed Maamouri, Ann Bies, Tim Buckwalter, andWigdan Mekki.
2004.
The Penn Arabic Treebank:Building a Large-Scale Annotated Arabic Corpus.
InNEMLAR Conference on Arabic Language Resourcesand Tools, pages 102?109, Cairo, Egypt.Yuval Marton, Nizar Habash, and Owen Rambow.
2010.Improving Arabic Dependency Parsing with Lexicaland Inflectional Morphological Features.
In Proceed-ings of the NAACL HLT 2010 First Workshop on Sta-tistical Parsing of Morphologically-Rich Languages,pages 13?21, Los Angeles, CA, USA, June.Quinn McNemar.
1947.
Note on the sampling error ofthe difference between correlated proportions or per-centages.
Psychometrika, 12(2):153?157.F.
J. Och and H. Ney.
2003.
A systematic comparison ofvarious statistical alignment models.
ComputationalLinguistics, 29(1):19?51.235Franz Josef Och.
2003.
Minimum Error Rate Trainingfor Statistical Machine Translation.
In Proceedingsof the 41st Annual Conference of the Association forComputational Linguistics, pages 160?167, Sapporo,Japan.Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu.
2002.
BLEU: a Method for Automatic Eval-uation of Machine Translation.
In Proceedings of the40th Annual Meeting of the Association for Computa-tional Linguistics, pages 311?318, Philadelphia, PA.Ryan Roth, Owen Rambow, Nizar Habash, Mona Diab,and Cynthia Rudin.
2008.
Arabic Morphological Tag-ging, Diacritization, and Lemmatization Using Lex-eme Models and Feature Ranking.
In Proceedings ofACL-08: HLT, Short Papers, pages 117?120, Colum-bus, Ohio.Andreas Stolcke.
2002.
SRILM - an Extensible Lan-guage Modeling Toolkit.
In Proceedings of the Inter-national Conference on Spoken Language Processing(ICSLP), volume 2, pages 901?904, Denver, CO.Fei Xia and Michael McCord.
2004.
Improving a statis-tical mt system with automatically learned rewrite pat-terns.
In Proceedings of the 20th International Confer-ence on Computational Linguistics (COLING 2004),pages 508?514, Geneva, Switzerland.236
