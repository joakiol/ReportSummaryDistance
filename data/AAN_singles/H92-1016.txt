T\]he MIT ATIS System: February 1992 Progress Report 1Victor Zue~ James Glass, David Goddeau, David Goodine, Lynette Hirschman,Michael Phillips, Joseph Polifroni, and Stephanie SeneffSpoken Language Systems GroupLaboratory for Computer ScienceMassachusetts In titute of TechnologyCambridge, Massachusetts 02139ABSTRACTThis paper describes the status of the MIT ATIS systemas of February 1992, focusing especially on the changes madeto the SUMMIT recognizer.
These include context-dependentphonetic modelling, the use of a bigram language model inconjunction with a probabilistic LR parser, and refinementsmade to the lexicon.
Together with the use of a larger trainingset, these modifications combined to reduce the speech recog-nition word and sentence rror rates by a factor of 2.5 and 1.6,respectively, on the October '91 test set.
The weighted errorfor the entire spoken language system on the same test set is49.3%.
Similar results were also obtained on the February '92benchmark evaluation.INTRODUCTIONThis paper presents an update on the MIT ATIS sys-tem, which has been under development since 1990.
Wewill describe several changes made to our system sincethe last official common evaluation in February, 1991 \[8\],with particular emphasis on the speech recognition com-ponent.. We will also present our evaluation results forthe October '91 "dry-run" test set and the February '92test set.
We have also modified our natural languagecomponent o include a robust parsing strategy.
Thischange is described in detail in a companion paper \[9\].SPEECH RECOGNIT IONIn this section we will describe the changes we havemade over the past year to the speech recognition com-ponent (SUMMIT) of our ATIS system.
These include im-provements to both the phonetic and language models,and refinements on the lexicon.
We have also imple-mented the acoustic models on a set of DSP boards toallow near real-time evaluation and demonstration.The baseline SUMMIT system uses a mixture of up to16 diagonal Gaussian models for each lexical unit.
Inrecent months, we have been able to simplify the inputrepresentation of the models significantly with no loss inperformance.
The current representation consists of 391This research was supported by i)ARPA under ContractN00014-89-J-1332, monitored through the Office of Naval Research.84segmental measurements for each hypothesized segment.This vector is rotated via principal component analysisprior to mixture Gaussian modelling.
Segment durationis modelled separately, in the log domain, using a mixtureof Gaussians.
At the moment, spontaneous disfluenciesare represented by one model, and are required to be onesegment long.Training and Testing CorporaThe multi-site ATIS data collection effort has resultedin a significant increase in the amount of speech dataavailable to the community \[6\].
For speech recognitionsystem development, we started with all the MADCOWdata released by NIST, and augmented them with ATISdata collected earlier at MIT.
Some 9,711 utterances inthis pool were designated as training material, and an ad-ditional 1,595 utterances were set aside as a developmentset for independent evaluation.To facilitate a meaningful comparison, all the exper-iments described in this section are performed on theOctober '91 "dry-run" test set, containing some 362 ut-terances collected at BBN, CMU, MIT, and SRI.
Theexperiments that we conducted are summarized in Ta-ble 1, and will be described in this section.In order to monitor progress internally, we also ranthe same test set through our system as reported a yearago \[8\].
Our February '91 system had a vocabulary of577 words.
That system constrained the N-best searchwith the use of a word-pair grammar with a perplexity of92.
The N-best outputs were subsequently resorted usingour natural language component TINA.
It was trainedon some 2400 utterances collected at T I  and MIT.
Therecognition performance of that system on the October'91 "dry-run" test set, with and without the word-pairlanguage model, is shown in the first two rows of Table 1(labelled as AW and WP, respectively).LexiconWith the availability of a larger amount of trainingdata we enlarged our vocabulary to contain 841 words.This was done by examining word frequency counts inthe training data and adding all reasonable words thatoccurred more than once.
Examples of words that werenot added included misspellings or people's names.Other improvements o the lexicon included refine-ment of the pronunciation baseforms and the phonologi-cal rules used to generate the pronunciation networks.
Inpart, this involved improving pre-existing rules such asthe flapping rule.
We also introduced a number of spe-cific allophones for certain phonemes in certain contexts,such as a retroflexed / f /  or a stop closure following africative, and a number of new diphone units, allowing asequence of two phonemes to be treated as a diphthong,such as /e l /o r /a t / .
The inventory of phonetic units inthe expanded lexicon contained 115 distinct labels.As shown in the third row of Table 1 (labelled as AW,Small Training), these changes combined to reduce theword error rate from 62.5% to 55.4% for the system ayear ago using an all-word language model.
The nextrow in the same figure (labelled as AW, Full Training)shows that the word error rate is further reduced to 51%by using the full training set described earlier 2.
Thisresult is identical to the results of the February '91 systemusing a word-pair language model, although the latterachieved better sentence recognition accuracy.
Unlessotherwise specified, the remaining experiments describedin this section all use the full training set.Bigram Language ModelThe current SUMMIT system uses significantly morelanguage constraints than were used by its predecessor\[8\].
With the help of the available large training set, weconstructed a smoothed bigram grammar.
As has beendone elsewhere, the bigram was smoothed by interpolat-ing the bigram estimates with the prior probabilities ofeach word \[2,4\]:whereN(wa,wb)~ N(wa)N(wb)IS(Wb) ~ N(all words)The interpolation weights were set to vary with the num-ber of times we had observed the conditioning context:g(wa)=- i( a) + Kwhere K is a single constant hat was opt!mized so as tominimize the measured perplexity on the developmentdata set.
For the ATIS training data, we found that the2Due to computational limitations, we did not use the entiredesignated training set for training.
Instead, a subset of about7,500 utterances were used.85perplexity had a broad minimum when K was around20.
On our development data set this smoothed bigramhad a perplexity of 20.1.
The perplexity measures didnot include out of vocabulary words since our recognitionsystem does not currently have the capability of detectingthese words.
Including out-of-vocabulary words in theperplexity measure increased the value slightly to 20.8.Recognition results using the bigram language modelare shown in row 5 of Table 1 (labelled as BG).
The bi-gram language model is the single most effective changewe made to our system, reducing the word-error ate bymore than twofold from the best results obtained previ-ously.P robab i l i s t i c  LR  ParserA probabilistic LR parser was used in addition to abigram model to provide language constraints.
The LRalgorithm is a deterministic, table-driven, !eft-to-rightparsing algorithm for a subset of context-free grammars\[1\].
The probabilistic LR (PLR) model extends this al-gorithm to assign a probabilityP(wo...w,) = I~ P(wilwo...wi-~)i=Oto each word string, (rather than a binary value).
Inthe PLR model the conditional word probabilities areapproximated using the parser state.If P(Qj\]wo...w~-l) is the probability that the parseris in state Qj having just parsed the substring Wo...wi-1(without making any moves based on the value of wi),then the conditional word probability can be re-writtenas:P(w, = P(w, Qj Iwo...wi-1 ).JMaking the assumption that the parser state capturesmuch of the information in the substring wo...wi-1 rele-vant to the conditional probabilities, this can be approx-imated by:P(wi Iwo...wi-1) ~, ~ P(wilQ~)P(Qjlwo...wi-1).JThe set of Qj for which P(Qj\]wo...wi-1) is non-zero isdetermined by the grammar.
In particular, if the gram-mar is deterministic, then P(Qj \[w0...wi-1) = 1, for somej = j~, andP(wo...w,~) = H P(wilw?
"'w~-~) ~ ~I P(w~IQ~,).i iThe probabilities P(wi\[Qj) can be estimated from acorpus of training utterances using the ratio of the num-ber of times wi is the next word when the parser is instate Qj to the number of times the parser is in state Qj.SystemFeb '91Feb '92Feb '92Characteristics Sub (%)AW 41.7WP 33.6AW, Small Training 39.7AW, Full Training 37.0BG 15.3BG?PLR 13.2BG?CD 12.4BG+CD?PLR 11.7BG+CD?PLR?NL 11.6Del (%) Ins (%)10.5 10.410.0 7.47.5 8.27.3 6.75.4 3.56.3 2.55.5 2.75.3 2.35.1 2.1Wd.
Error (%) Sent.
Error(%)62.5 98.351.0 93.955.4 97.851.0 98.124.1 72.722.0 66.920.6 67.719.3 61.618.8 58.6Table 1: Speech recognition results on the October '91 test set for the various experiments described in this paper.
Inaddition to the word and sentence rror rates, errors due to substitution, insertion and deletion are also provided.
Performanceof the systems from a year ago on the same data set is included for reference.
The symbols are: AW=all-word languagemodel, WP----word-palr language model, BG=bigram language model, CD=context-dependent modelling, PLR=probabilisticLR parser, NL=NL filtering using TINA.In previous work using the PLR model for the vOY-aGER task \[3\], the language model implemented was strict,that is, it assigned probability 0 to word strings not gen-erated by the input grammar.
In order to apply thismodel to speech recognition (i.e., optimizing word accu-racy), the parse table was extended to "accept" all wordstrings.
This was accomplished by adding explicit errorstates to the parse table, and computing recovery actionsto allow normal parsing to resume in an appropriate stateafter an error 3.
Other extensions to the model describedpreviously \[3\] include various mechanisms for smoothingthe probabilities by changing the conditioning state.The ATIS gramrnar contains 971 rules, the vast ma-jority of which introduce lexical items, and the resultingparse table contains about 1600 states.
The lexicon of theparser is the same as that used by the recognizer.
Theprobabilities were trained on all 9,711 utterances in thetraining set.
The perplexity measured on the October'91 test set was 17.6.Row 6 of Table 1 (labelled as BG+PLR) shows thatfurther reduction in error rate is possible by incorporat-ing the PLR.
PLR is incorporated by using the parsescore in place of the bigram score to reorder the 50 N-best outputs produced by the recognizer.
The sentenceerror rate is reduced more than the word error rate, pre-sumably due to the fact that PLR can deal with some ofthe long distance constraints better than the bigram.Context -Dependent  ModellingAt the last DARPA meeting we first described ourwork towards accounting for contextual effects on thephonetic modelling component of SUMMIT \[5\].
We pro-posed using regression tree analysis to find the contex-3This is roughly equivalent o pars ing the word str ing as a se-quence of f ragments  rather  than  as a complete sentence.86tual factors that provided the greatest reduction in thedistortion of our phonetic models.
In an initial exper-iment, regression tree analysis was used to form a setof context-specific models for each phonetic unit.
How-ever, we found that we were able to obtain the best per-formance by using the regression trees to independentlylearn a context-normalization factor for each of the inputdimensions of the model.
The model for each phoneticunit is then trained using these context-normalized in-puts for all of the training samples in that class.We have extended this work by considering more con-textual effects, including phonetic labels two phones awayand whether or not the current segment is in a syllablebefore a pause or at a sentence boundary.
The new ef-fects were simply added to the list of questions that couldbe asked at each node in the tree splitting algorithm.When we applied this context-normalization to theATIS domain, we found that the word error rate droppedfrom 24.1% to 20.6%, as shown in rows 5 and 7 in Ta-ble 1) (labelled as BG and BG+CD, respectively).
Thisrepresents a 15% reduction in error rate.
In the ResourceManagement domain, we found a decrease in word errorrate from 10.3% to 7%, or 32% \[5\].
We believe that weare achieving a smaller eduction in error rate in the ATISdomain because a greater number of errors can be at-tributed to problems other than phonetic modelling (e.g.,out-of-vocabulary words, mismatch of language model,spontaneous speech effects, etc.).
In fact, if we look atthe performance of the phonetic models in terms of theirability to match the "forced-recognition" phonetic string(the string obtained uring recognition allowing only thecorrect word string), we see a much larger reduction inerror rate in the ATIS domain (37.5%) than in the Re-source Management domain (18.8%).
This may not besurprising, since we are now considering more contextualeffects.
In addition, it is likely that there are strongerII Input I Correct \[ Incorrect No Answer I Error HText 187.7(%)1 8.5(%) 3.9(%) 120.9(%)Speech 64.8(%) 14.1(%) 21.1(%) 49.3(~)Table 2: Overall system performance, for both text andspeech input, on the October '91 test set.II Sub (%) I Del (%) I Ins (%) I Wd.
Error (%) \] Sent.
Error (%) IIII 11"5 I 4.4 I 2.3 I 16.1 I 59.6 IITable S: Speech recognition results for the February '92 testset.contextual effects in a spontaneous speech corpus suchas ATIS than in a more carefully spoken "read" corpussuch as Resource Management.The combined effect of our improved phonetic andlanguage modelling is shown in row 8 of Table 1 (labelledas BG+CD+PLR) .
In this case, the PLR score is used inconjunction with the acoustic score to resort the N-bestoutputs.
As expected, there is again a more significantimprovement on the sentence rror rate.Finally, we incorporated our natural anguage systemTINA as a filter on the N-best outputs produced by therecognizer (with N = 40), and the results are shown inthe last row of Table 1 (labelled as BG+CD+PLR+NL) .Not surprisingly, the natural anguage component is ableto reduced the sentence rror rate much more than theword error rate.OTHER IMPROVEMENTSThe most significant improvement in the back-end,the augmentation of the system with a robust parsing ca-pability is described separately.
However, in addition, wehave continued to expand the capabilities of the back-endat all levels (syntactic coverage, concepts understood,discourse modelling, dialogue aspects, etc.)
We continueto improve the level of sophistication of the booking dia-logue, towards the goal of a natural and effective mixed-initiative dialogue to achieve a successful booking.The performance of our current spoken language sys-tem on the October '91 test set is summarized in Ta-ble 2.
The significant improvement in our NL result canbe attributed to the robust parsing strategy that we have?
adopted.
Discussion of these results can be found in acompanion paper \[9\].FEBRUARY BENCHMARKThe February '92 benchmark results were obtainedby running the official test set released by NIST throughour system once.
This test set contains 971 utterancescollected AT&T, BBN, CMU, MIT, and SRI.
The speechrecognition results are shown in Table 3.
Comparing Ta-ble 3 with the last row of Table 1, we see that the perfor-mance of our system on the two test sets is quite similar.87H Input Correct Incorrect No Answer I Error 11Text 80(%) 13(%) 7(%) \]32.5(%)Speech 61(%) 14(%) 25(%) I 52.8(%)Table 4: Overall system performance, for both text andspeech input, on the February '92 test set.The performance of our current spoken language sys-tem on the February '92 test set is summarized in Ta-ble 4.
Although the system's performance for speech in-put is similar to that on the October '91 test set, theNL results are not as good.
This is a direct reflectionof our research priority since October 1991.
That is, wehave focused our group's attention almost entirely on im-proving the speech recognition component, o the neglectof expanding our NL system capabilities to adequatelyconform to the principles of interpretation.
Again, dis-cussion of these results can be found elsewhere in theseproceedings \[9\].SUMMARY AND FUTUREWORKThis paper describes the improvements hat we havemade to the recognition component of our ATIS system.By incorporating more language constraints (using a bi-gram and a probabilistic LR parser) and performing con-text dependent phonetic modelling, a significant reduc-tion in recognition error rates is realized.
This has led toa corresponding decrease in weighted error of the overallspoken language system.
Much of the phonetic recogni-tion parts of our system has been ported to a set of off-the-shelf DSP boards.
The complete system, using anIBM RS6000 for lexical access and a Sun SPARCstation-II for the rest of the processing, now runs in 2-3 timesreal-time.In the coming months, we plan to conduct research inseveral directions that will hopefully lead to further im-Provement in system performance.
These areas includethe introduction of gender-specific a oustic models, mod-elling out-of-vocabulary words, modelling, spontaneousspeech effects such as pauses, increasing the size of thelexicon and training set size, and better language models.Our results show that better language modelling iscrucial to improved performance.
Our future researchin this area falls in several categories.
In addition todeveloping a bigram grammar, we have begun to explorethe use of class bigram's as well as more general N-grams.The class bigrams we examined grouped similar wordstogether in order to reduce the number of unseen wordpairs.
We investigated both grouping the conditioningcontext into classes:p( b I o) IC@o))as well as the word itself:I o)where C(wb) is the general class of words that wb belongsto.
We explored a number of different classes and foundthat we could reduce the development set perplexity bya small amount, to 19.5.We have also begun to explore the use of more gen-eral N-grams and class N-grams.
The N-gram languagemodel store all word sequences observed in the trainingdata.
In order to represent these grammars efficiently westore them in the form of a hierarchical tree, where eachnode deeper in the tree represents one word farther backin the past.
Smoothing becomes extremely important forthe N-gram.
Thus far we have used the generalizationof the bigram interpolation procedure so that N-gramsmoothing is done recursively:= + (1  -po =so thatp@, l o =pnOur initial experiments suggest hat, by incorporat-ing a class 4-gram directly into the N-best search, wecan reduce our sentence rror rate from 62.5% to 56.3%(for N = 1) on the development set, although there is acorresponding increase in the amount of search required.We have also found that by simply adding the class 4-gram scores into our N-best resorting algorithm we canreduce our sentence rror rate from 59.6% to 56.0%onthe February '92 test set.Future work in language modelling will focus on appli-cation of this general model.
In particular, conditioningthe probabilities on the entire parse stack rather thanon the current state (essentially the top of stack) shouldfurther reduce perplexity and bring long-distance con-stralnts to bear.ACKNOWLEDGEMENTSWe would like to acknowledge the assistance of HongC.
Leung, a former member of our group, whose has con-tributed directly and indirectly to several aspects of ourspoken language system development effort.REFERENCES\[1\] Aho, A. and Ullman, J., The Theory of Parsing, Trans-lation, and Compiling, Prentice-Hall, Englewood Cliffs,N J, 1973.\[2\] S. Austin, P. Peterson, P. Placeway, R. Schwartz, J. Van-dergrift, "Toward a Real-Time Spoken Language SystemUsing Commercial Hardware," Proc.
DARPA Speech andNatural Language Workshop: 72-77 June 1990.\[3\] Goddeau, D. and Zue, V., "Integrating ProbabilisticLR Parsing into Speech Understanding Systems," Proc.ICASSP 92, March, 1992.\[4\] Katz, S., "Estimation of Probabilities from Sparse Datafor the Language Model Component of a Speech Recog-nizer," IEEE Trans.
ASSP, ASSP-35, 3,400-401, March,1987.\[5\] Phillips, M., Glass, J., and Zue, V. "Modelling ContextDependency in Acoustic-Phonetic and Lexical Represen-tations," Proc.
DARPA Speech and Natural LanguageWorkshop: 71-76, February 1991.\[6\] MADCOW, "Multi-Site Data Collection for a SpokenLanguage Corpus," These Proceedings.\[7\] Polifroni, J. Seneff, S., and Zue, V., "Collection of Spon-taneous Speech for the sc Atis Domain and Compara-tive Analyses of Data Collected at MIT and TI," Proc.DARPA Speech and Natural Language Workshop: 360-365, February 1991.\[8\] Seneff, S., Glass, J., Goddeau, D., Goodine, D.,Hirschman, L., Leung, H., Phillips, M., Polifroni, J. andZue, V., "Development and Preliminary Evaluation ofthe MIT ATIS System," Proc.
DARPA Speech and Nat-ural Language Workshop: 88-93, February 1991.\[9\] Seneff, S., "A Relaxation Method for UnderstandingSpontaneous Speech Utterances," These Proceedings.The PLR model implemented for the ATIS system andthe N-gram models described above are both instancesof a larger class of language models based on Stack Au-tomata.
A Probabilistic Stack Automata language modelapproximates conditional word probabilities as:P(wi\[wo...wi-1) .~ P(wi\[stack after parsing Wo...wi-1)This model can also be considered as an extension to aclass N-gram, in which the class members can be phrasesas well as words.88
