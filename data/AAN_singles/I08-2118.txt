Learning Decision Lists with Known Rules for Text MiningVenkatesan ChakravarthyIBM India Research Labvechakra@in.ibm.comSachindra JoshiIBM India Research Labjsachind@in.ibm.comGanesh RamakrishnanIBM India Research Labganramkr@in.ibm.comShantanu GodboleIBM India Research Labshgodbol@in.ibm.comSreeram BalakrishnanIBM Silicon Valley Labsreevb@us.ibm.comAbstractMany real-world systems for handling unstructuredtext data are rule-based.
Examples of such systemsare named entity annotators, information extractionsystems, and text classifiers.
In each of these appli-cations, ordering rules into a decision list is an im-portant issue.
In this paper, we assume that a set ofrules is given and study the problem (MaxDL) of or-dering them into an optimal decision list with respectto a given training set.
We formalize this problemand show that it is NP-Hard and cannot be approxi-mated within any reasonable factors.
We then proposesome heuristic algorithms and conduct exhaustive ex-periments to evaluate their performance.
In our ex-periments we also observe performance improvementover an existing decision list learning algorithm, bymerely re-ordering the rules output by it.1 IntroductionRule-based systems have been extensively used forseveral problems in text mining.
Some problemsin text mining where rule-based systems have beensuccessfully used are part of speech tagging (Brill,1992), named entity annotation (Grishman, 1997;Appelt et al, 1995), information extraction (May-nard et al, 2001), question answering (Riloff andThelen, 2000) and classification (Han et al, 2003; Liand Yamanishi, 1999; Sasaki and Kita, 1998).
Sev-eral studies have been conducted that compare theperformance of rule-based systems and other ma-chine learning techniques with mixed results.
Whilethere is no clear winner between the two approachesin terms of performance, the rule-based approachis clearly preferred in operational settings (Borth-wick, 1999; Varadarajan et al, 2002).
Rule-basedsystems are human comprehensible and can be im-proved over time.
Therefore, it is imperative to de-velop methods that assist in building rule-based sys-tems.A rule-based system consists of a set of rules.These rules can either be manually designed orcould be learnt from a training set using rule-induction techniques (J. and G, 1994; Cohen, 1995).Each rule consists of an antecedent or pattern anda consequent or predicted annotation.
In this paper,we will restrict our attention to a broad class of rulesin which the antecedent describes a series of condi-tions on the input item and the consequent specifiesthe label that applies to instances covered by the an-tecedent.
The conditions could also be expressed aspatterns in regular or more powerful grammars.In general, rules could be ambiguous, i.e., multi-ple rules could cover an instance.
A common ap-proach for resolving this ambiguity is to define anordering on the rules (Maynard et al, 2001; Borth-wick, 1999).
A decision list is one such mecha-nism (Rivest, 1987).
A set of rules that are intendedto be interpreted in a sequence is called a decisionlist.
In other words, a decision list is an ordering ofthe given set of rules.
Given an instance t, the rulesare applied in the specified order until a pattern of arule R covers t. The instance t is assigned the pre-dicted annotation associated with R.In this paper, we study the problem of arranging agiven set of rules into the ?best?
decision list.
Learn-ing decision lists using training data has been stud-ied in the past (Rivest, 1987; J. and G, 1994; Cohen,1995; Li and Yamanishi, 1999).
These methods at-tempt to simultaneously learn rules and their order-ing.
Typically they use separate and conquer (Wit-ten and Frank, 2005) strategy and order generatedrules as they are discovered.
The generation and or-dering of rules are not considered as two separate835tasks.
In contrast, we assume that the rules are givento us and study the problem of arranging them intoan optimal decision list, where optimality is deter-mined over a training data set.
Our approach is mo-tivated by the observation that in many operationalsettings, it is easier and preferred to get a set of rulesdesigned by domain experts (Lewis et al, 2003).
Al-ternatively, the set of rules can be determined usingexisting techniques for rule learning (J. and G, 1994;Cohen, 1995; Califf and Mooney, 1998).
The sepa-ration of rule ordering from rule generation allowsus to analyze the problem of ordering in detail andto develop effective methods for rule ordering.
Wedemonstrate the usefulness of the proposed methodsfor ordering manually designed rules in the task ofnamed entity annotation and machine learnt rules inthe task of classification.We determine the ordering of the given set of rulesbased on a training set.
A training set consists of aset of pairs (ti, ai) where ti is an instance and aiis its actual annotation.
Given a set of rules and atraining data set, we define the problem as follows:Arrange the rules into a decision list such that max-imum number of instances are assigned the correctannotation.
We refer to this problem as the MAXDLproblem.
We show that this problem is NP hardand cannot approximated within a factor of n1?,for any  > 0.
We then propose some heuristicsand present an experimental study of these heuris-tics.
Our experimental results show performance im-provement over an existing decision list learning al-gorithm, by merely reordering the rules output bythat algorithm.
We also illustrate the performanceimprovements obtained by applying our algorithmsfor ordering named entity annotation rules and clas-sification rules.In the rest of the paper we formalize the MAXDLproblem (?2), show it is NP-hard and can?t beapproximated within reasonable factors (?3), andpropose heuristics in a greedy framework (?4).We present experiments (?5) and conclude withSection?6.2 MAXDL Problem Definition andNotationsThe input consists of a set of instances T ={t1, t2, .
.
.
, tm}, a set of annotations A and a set ofrulesR = {R1, R2, .
.
.
, Rn}.
Each ruleRi = (p, a)is a pair, where p is called the pattern and a ?
A iscalled the predicted annotation.
The patten p will begiven as a set p ?
I; we say that the instances inp are covered by R. The input also includes a map-ping A : T 7?
A, that provides for each instance tan annotation A(t), called the actual annotation oft.
The pair (T , A) is the training data.Given the above input, a decision list L is an or-dering (i.e.
permutation) of the input rules.
The listL assigns an annotation to each instance t as definedbelow.
We consider each rule according to the order-ing given by L until we find a rule Ri = (p, a) thatcovers t and assign the annotation a to t. We denoteby L(t) the annotation assigned by L to t. Thus, Ldefines a function L : I 7?
A.
We say that the listL correctly annotates an instance t, if the annota-tion assigned by L matches the actual annotation oft, i.e., L(t) = A(t).Given the above input, the MAXDL problem is toto construct a decision list L such that the numberof instances correctly annotated by L, is maximizedi.e., we want to maximize |{t|A(t) = L(t)}| .Notations:LetR = (p, a) be a rule and t be an instance coveredby R. We say that a rule R correctly covers t, ifa = A(t).
Similarly, R said to incorrectly cover t, ifa 6= A(t).Let L be a decision list.
We say that an instancet is happy under L, if L correctly annotates t, i.e.,L(t) = A(t).
Let Happy(L) denote the set of in-stances that are happy under L. Notice that theMAXDL problem asks for a decision list L such that|Happy(L)| is maximized.3 NP-Hardness and InapproximabilityIn this section, we prove that the MAXDL problemis NP-Hard and also show that the problem cannoteven be approximated with any constant factor.Theorem 1 The MAXDL problem is NP-Hard.Proof: We give a reduction from the maximum inde-pendent set problem (MAXIS ), a well-known NP-Hard problem (Garey and Johnson, 1979).
Recallthat an independent set in a graph refers to any sub-set of vertices such that no two vertices from the setshare an edge.
The MAXIS problem is to find thelargest independent set in a given undirected graph.836Let G = (V,E) be the input graph having vertexset V = {v1, v2, .
.
.
, vn}.
We create an instance ofthe MAXDL problem as follows.
For each vertexvi, we add an annotation ai toA, an instance ti to Tand a rule Ri to R. We declare ai to be the actualannotation of ti.
The predicted annotation of Ri isset to ai.
We define Ri to cover only the instanceti and the instances corresponding to the neighborsof vi.
Meaning, Ri covers the instances in the set{ti} ?
{tj |(vi, vj) ?
E}.
This completes the reduc-tion.
We claim that given a decision list L havingk happy instances, we can construct an independentset of size k and vice versa.
The NP-Hardness ofMAXDL follows from the claim.
We now proceedto prove the claim.Consider a decision list L. Notice that for anyinstance ti, Ri is the only rule that correctly coversti.
Take any two different instances ti and tj that arehappy under L. Without loss of generality, assumethat Ri appears before Rj in L. Now, if Ri coverstj , tj would be unhappy under L. So, Ri does notcover tj , which implies that vj is not a neighbor ofvi (i.e., (vi, vj) 6?
E).
Hence, the set I = {vi|ti ?Happy(L)} is an independent set ofG.
We note that|I| = |Happy(L)|.Conversely, consider an independent set I of G.Let R(I) = {Ri|vi ?
I}.
Form a decision list L byfirst arranging the rules from R(I) in any arbitraryorder followed by arranging the rest of rules in anyarbitray order.
Notice that for any vertex vi ?
I ,Ri correctly covers ti and no other rule appearingbefore Ri covers ti.
Thus, ti is happy under L. Itfollows that |Happy(L)| ?
|I|.
We have provedthat the MAXDL problem is NP-Hard.
2In our NP-Hardness reduction, we had shown thatgiven a decision list L, we can construct an inde-pendent set I such that |Happy(L)| = |I|, andvice versa.
This means that any approximation algo-rithm for the MAXDL problem can be translated (bycombining it with our NP-Hardness reduction) intoan equally good approximation algorithm for theMAXIS problem.
Corollary 1 follows from (Zuck-erman, 2006).Corollary 1 If NP 6= P then for any  > 0,the MAXDL problem cannot approximated withina factor of n1?.
In particular, the problem is notapproximable within any constant factor.4 Heuristic Algorithms for the MAXDLProblemAs the MAXDL problem is hard to approximate, weturn to heuristic approaches.
All our heuristics fallinto a natural greedy framework, described below.4.1 A Greedy FrameworkOur greedy framework for finding a decision list isas follows.
In each iteration we greedily choose arule and output it.
For this purpose, we use somescoring function for assigning scores to the rules andchoose the rule having the maximum score.
Thenthe chosen rule is deleted.
The process is contin-ued until all the rules are output.
The above proce-dure gives us a decision list.
We present this generalframework in the Figure 1.
The only unspecified partin the above framework is the scoring function.
In-tuitively, the scoring function tries to measure thegoodness of a rule.Given rule set R = {R1, R2, .
.
.
, Rn}, instance set T and the actual annotationsA(?
)while R 6= null do(re)compute scores for each rule in R, based on the scoring functionselect the rule R that has the maximum scoreremove R from the set Rremove from T all the instances covered by Rend whileFigure 1: A Greedy Framework for MAXDL prob-lemFor a rule R and an instance t, we define follow-ing notations for further use:InstR = {t|R covers t}Inst+R = {t|R correctly covers t}Inst?R = {t|R incorrectly covers t}Rulest = {R|t is covered by R}Rules+t = {R|t is correctly covered by R}Rules?t = {R|t is incorrectly covered by R}4.2 Simple Precision ScoringWe now present our first candidate scoring function,which we call simple precision scoring.
A naturalscore for a rule R is its precision: the fraction of in-stances covered correctly by R among the instancescovered by it.ScoreSP(R) =|Inst+R||InstR|=|Inst+R||Inst+R| + |Inst?R |4.3 Weighted Precision ScoringUnder ScoreSP, the score of a rule R is determinedonly by the number of instances covered correctly(|Inst+R|) and incorrectly (|Inst?R|).
The nature ofinstances are not taken into account.
The variants ofScoreSP proposed here assigns weights to instances,based on which the scores are computed.
We assignweights to the instances based on how easy it is to837make them happy.
For an instance t, define the hap-piness quotient h(t) to be the fraction of rules thatcorrectly cover t among all the rules that cover t:h(t) =|Rules+t ||Rulest|.The value h(t) is a measure of how easy it is tomake t happy; the larger the value of h(t), it iseasier to make t happy.
For instance, if h(t) ?
1,then |Rules+t | ?
|Rulest|, meaning that almost anyrule that covers t will annotate it correctly.
Thus,it is easy to make t happy.
On the other extreme,if h(t) ?
0, then only a small fraction of the rulesthat cover t annotate it correctly.
Thus it is harder tomake t happy.When we schedule a rule R, the instances inInst+R become happy and those in Inst?R becomeunhappy.
Our new scoring functions give credit toR for each instance in Inst+R and award a penaltyR for each instance in Inst?R.
The credit and thepenalty depend on the happiness quotient of the in-stance.
Informally, we want to give more credit Rfor making hard instances happy; similarly, we wantto penalize R for making easy instances unhappy.
Anatural way of accomplishing the above is to awarda credit of (1 ?
h(t)) for each instance t ?
Inst+Rand a penalty of h(t) for each instance t ?
Inst?R.Below, we formally define the above quantities asgain and loss associated with R. For each rule R,defineGain(R) =Xt?Inst+R(1 ?
h(t))Loss(R) =Xt?Inst?Rh(t)Based on the above quantities, we define a naturalscoring function, called Weighted Precision:ScoreWP(R) =Gain(R)Gain(R) + Loss(R)4.4 Refined Weighted Precision ScoringOur third scoring function is a refinement of theweighted precision scoring.
In ScoreWP, we com-pute the happiness quotient of a token by taking inaccount the number of rules that cover the token andamong those the ones that cover it correctly.
The re-finement is obtained by also considering the natureof these rules.
We definehRP(t) =PR?Rules+tprecision(R)PR?Rulestprecision(R).Gain, loss and the scoring function are defined sim-ilar to that of ScoreWP:GainRP(R) =Xt?Inst+R(1 ?
hRP(t))LossRP(R) =Xt?Inst?RhRP(t)ScoreRP(R) =GainRP(R)GainRP(R) + LossRP(R)5 ExperimentsIn this section, we describe rule-ordering experi-ments on two real-world tasks.
1) named-entity(NE) annotation that relied on hand-crafted rules forMUC-7 dataset.
2) The second application we con-sider is rule-based multi-class text classification.
Weorder rules learnt on benchmark text classificationdatasets and observe consistent improvements bymerely re-ordering rules learnt by other rule learn-ers.5.1 Named Entity AnnotationRule-based named entity annotation is a natural in-stance of a decision list problem.
Typically, rule-based NE annotation systems (Cunningham et al,2002) require rules to be manually written as wellas ordered manually.
In this section, we show thatour proposed rule-ordering algorithms perform bet-ter than the natural heuristic.
Note that we do notintend to build a rule-based decision list which per-forms better than existing methods.Setup: In our problem formulation of MAXDL ,the set of instances T and mapping A from in-stances to actual annotations, together form a train-ing set.
We have access to a set of documentsD = {d1, d2, .
.
.
, dm}, that have all its named en-tities annotated.
To generate pairs (T , A) using theset of documentsD, let Tdi represent the set of tokensequences that are annotated in a document di ?
D.Let A(t) be the actual annotation for an instancet ?
Tdi .
Given a set of rules R and a documentcollection D, each rule R ?
R is applied to eachdocument di ?
D. The set of token sequences (in-stances here) which R covers (InstR), is includedin the set of instances T .
For all instances t ?
Tdi ,we add a mapping t ?
A(t) in A.
For all otherinstances t ?
{InstR ?
Tdi}, we have a mappingt ?
null included in A.
We perform these addi-tions for each document and rule pair.
Finally, weadd a rule R?
= (?, null) to the rule setR.
The pat-tern ?
matches every instance t ?
?R?R,R 6=R?InstR838and associates a null annotation with the instance.We only consider ?person name?, ?organization?and ?place name?
annotations.
We use two differentrule sets containing about 30 rules each.Table 1 presents accuracy achieved by our pro-posed algorithms for the two chosen rule sets.
In allthe cases our proposed methods perform better thanScoreSP.
The result also shows that our proposedmethods generalize better than simple ScoreSP.Rule-sets Accuracy ScoreSP ScoreWP ScorePRWPRule-set 1Trng 76.4 76.7 78.9Test 50.0 52.7 54.5Rule-set 2Training 70.1 71.6 73.3Test 49.1 51.4 52.0Table 1: Accuracies (in %) for different algorithmsDataset Acc-(avg.
# rules) -uracy JRip ScoreSP ScoreWP ScorePRWPla2s (37)Trng 86.16?0.39 86.02?0.16 86.68?0.16 87.04?0.17Test 76.93?0.43 77.88?0.16 78.05?0.17 78.1?0.15oh5 (28)Trng 86.95?0.41 88.26?0.21 88.8?0.16 89.06?0.17Test 76.43?0.58 79.08?0.37 79.37?0.38 79.24?0.35tr45 (17)Trng 91.88?0.38 92.61?0.18 92.84?0.23 93.3?0.21Test 78.9?0.47 80.99?0.29 81.19?0.28 81.3?0.3Table 2: Accuracies (in %) for RipRulesData set Accu- Multi-class-racy J48 NaiveBayes ScoreSP ScoreWP ScorePRWPla2s (18)Trng 94.75?0.39 85.78?0.29 94.64?0.14 95.9?0.03 95.99?0.01Test 73.43?0.64 73.68?0.37 78.0?0.21 78.46?0.23 78.64?0.29oh5 (30)Trng 95.08?0.21 99.56?0.09 96.27?0.14 98.43?0.09 98.45?0.09Test 78.08?0.76 74.16?0.77 82.72?0.25 83.16?0.24 83.98?0.26tr45 (30)Trng 97.91?0.11 87.16?1.18 97.71?0.14 98.93?0.06 98.98?0.05Test 85.25?1.02 69.91?1.33 84.06?0.44 86.1?0.39 86.42?0.41Table 3: Accuracies (in %) for BinRules5.2 Ordering classification rulesIn this section, we show another application of ouralgorithms in ordering classification rules.
Theantecedent of a classification rule is a series of testson the input and the consequent gives the class label.Since different rules can assign conflicting classes,rule-ordering becomes important in choosing acorrect class.
These rules come from a variety ofsources and could be hand-crafted or machine-learnt.
Machine learnt rules could be generatedusing association mining (Agrawal and Srikant,1994), inductive logic programming (Lavrac andDzeroski, 1994), or Ripper (Cohen, 1995).
Evenclassifiers can be seen as rules, e.g., linear discrim-inants are rules that assign one of two classes toexclusive partitions of input space.
Due to domainspecificity and unavailability of hand-tuned ruleswe illustrate rule-ordering on: (1) rules inducedby Ripper (Cohen, 1995) (RipRules), and (2) aheterogeneous set of rules obtained from naiveBayes and decision trees (BinRules).Setup: We used benchmark text classificationdatasets (Forman, 2003) available from the Wekasite1.
These multi-class datasets represent 229binary text classification problems, with positiveclass size avg.
149, and class skews avg.
1 : 31.These are subsets of various benchmark tasks likeReuters, TREC, and Ohsumed (oh).
We presentonly a subset of the results (with only ScoreWPand ScorePRWP) here for lack of space.
We reportexperiments over 10 random 50 : 50 train-test splits.The training split is used to learn rules and theirordering.
The orderings are evaluated on the testsplit and average train and test accuracies reported.Results:The RipRules setting: We induce rules (fromthe train split) using the JRip implementation inWeka2 (Witten and Frank, 2005).
We apply our vari-ous algorithms to merely re-order the rules output byJRip.
In Table 2 we present results comparing JRipoutput with their re-ordered versions obtained fromScoreSP, ScoreWP and ScorePRWP.
Along with thename of each data set, the average number of rulesinduced from the training splits are also mentionedin parentheses.
The best accuracies are marked inbold.
We observe that the re-ordered rule-sets us-ing ScoreWP and ScorePRWP perform better than bothbaselines ScoreSP and JRip with lower deviations.The BinRules setting: For an n-class problem weobtain classification rules by training a heteroge-neous collection of one-vs-rest binary classifiers.Each classifier is either a naive Bayes or a decisiontree classifier trained to discriminate one class fromthe rest (2n classifiers).
We treat each binary clas-sifier as a classification rule that covers an instanceif the binary classifier assigns its associated class tothat instance.
In addition, corresponding to everyclass, we introduce a default classification rule thatassigns the associated class to any instance it en-1http://www.cs.waikato.ac.nz/ml/weka/index_datasets.html2http://www.cs.waikato.ac.nz/ml/weka/839counters.
This gives us 3n rules.
We used the naiveBayes and J48 implementations in Weka to obtainbinary rules, ordered using ScoreWP and ScorePRWP,and compared with ScoreSP baseline in Table 3.We also show individual classifier accuracy, and thebest are marked bold.
It is encouraging to note thatall our rule-ordering techniques always outperformtheir multi-class counterparts on the test data set.
Weoutperform the baseline ScoreSP method on all datasets with lower deviations.6 ConclusionsIn this paper, we formulated and studied theMAXDL problem.
We proved the hardness of theproblem.
We then proposed some heuristic ap-proaches and established the usefulness of our meth-ods experimentally.
We observed improved perfor-mance in classification task by merely reordering therules obtained by an existing decision list learningalgorithm.
In future work, we would like to ex-plore how rule-ordering formulation can be appliedto ordering heterogeneous classifiers in the ensem-ble learning setting.ReferencesRakesh Agrawal and Ramakrishnan Srikant.
1994.
Fastalgorithms for mining association rules.
In VLDB,pages 487?499.D.
Appelt, J. Hobbs, J.
Bear, D. Israel, M. Kameyama,D.
Martin, K. Myers, and M. Tyson.
1995.
Sri inter-national fastus system: Muc-6 test results and analysis.In MUC6 ?95: Proc.
of the 6th conf.
on Message un-derstanding.A.
Borthwick.
1999.
A Maximum Entropy Approach toNamed Entity Recognition.
Ph.D. thesis, New YorkUniversity.Eric Brill.
1992.
A simple rule-based part-of-speech tag-ger.
In Proceedings of ANLP.M.
E. Califf and R. J. Mooney.
1998.
Relational learningof pattern-match rules for information extraction.
InWorking Notes of AAAI Spring Symposium on Apply-ing Machine Learning to Discourse Processing.William W. Cohen.
1995.
Fast effective rule induction.In ICML, pages 115?123.H.
Cunningham, D. Maynard, K. Bontcheva, andV.
Tablan.
2002.
GATE: A framework and graphi-cal development environment for robust NLP tools andapplications.
In Proceedings of ACL.George Forman.
2003.
An extensive empirical studyof feature selection metrics for text classification.JMLR Special Issue on Variable and Feature Selection,3:1289?1305.M.
R. Garey and D. S. Johnson.
1979.
Computers andIntractability.
Freeman.R.
Grishman.
1997.
Information extraction: Techniquesand challenges.
In SCIE ?97: Intnl.
summer School onInformation Extraction.Hui Han, Eren Manavoglu, C. Lee Giles, and HongyuanZha.
2003.
Rule-based word clustering for text classi-fication.
In SIGIR, pages 445?446.
ACM Press.Furnkranz J. and Widmer G. 1994.
Incremental re-duced error pruning.
In Machine Learning: Proc.
ofthe Eleventh International Conference.Nada Lavrac and Saso Dzeroski.
1994.
InductiveLogic Programming:Techniques and Applications.
El-lis Horwood, New York.David D. Lewis, Rayid Ghani, Dunja Mladenic, IsabelleMoulinier, and Mark Wasson.
2003.
Workshop onoperational text classification.
In conjunction withSIGKDD.Hang Li and Kenji Yamanishi.
1999.
Text classificationusing ESC-based stochastic decision lists.
In CIKM.D.
Maynard, V. Tablan, C. Ursu, H. Cunningham, andY.
Wilks.
2001.
Named entity recognition from di-verse text types.
In RANLP.Ellen Riloff and Michael Thelen.
2000.
A rule-basedquestion answering system for reading comprehensiontests.
In ANLP/NAACL 2000 Workshop on Readingcomprehension tests as evaluation for computer-basedlanguage understanding sytems.Ronald L. Rivest.
1987.
Learning decision lists.
Ma-chine Learning, 2(3):229?246.Minoru Sasaki and Kenji Kita.
1998.
Rule-based textcategorization using hierarchical categories.
In Pro-ceedings of SMC-98, IEEE International Conferenceon Systems, Man, and Cybernetics, pages 2827?2830.Sundar Varadarajan, Kas Kasravi, and Ronen Feldman.2002.
Text-mining: Application development chal-lenges.
In Proceedings of the Twenty-second SGAI In-ternational Conference on Knowledge Based Systemsand Applied Artificial Intelligence.Ian H.Witten and Eibe Frank.
2005.
Data Mining: Prac-tical Machine Learning Tools and Techniques.
Mor-gan Kaufmann.D.
Zuckerman.
2006.
Linear degree extractors and theinapproximability of max-clique and chromatic num-ber.
In STOC.840
