Determining the Specificity of Terms using Compositional and Con-textual InformationPum-Mo RyuDepartment of Electronic Engineering and Computer ScienceKAISTPum-Mo.Ryu@kaist.ac.krAbstractThis paper introduces new specificity de-termining methods for terms using com-positional and contextual information.Specificity of terms is the quantity ofdomain specific information that is con-tained in the terms.
The methods aremodeled as information theory like meas-ures.
As the methods don?t use domainspecific information, they can be appliedto other domains without extra processes.Experiments showed very promising re-sult with the precision of 82.0% when themethods were applied to the terms inMeSH thesaurus.1.
IntroductionTerminology management concerns primarilywith terms, i.e., the words that are assigned toconcepts used in domain-related texts.
A term isa meaningful unit that represents a specific con-cept within a domain (Wright, 1997).Specificity of a term represents the quantity ofdomain specific information contained in theterm.
If a term has large quantity of domain spe-cific information, specificity value of the term islarge; otherwise specificity value of the term issmall.
Specificity of term X is quantified to posi-tive real number as equation (1).
( )Spec X R+?
(1)Specificity of terms is an important necessarycondition in term hierarchy, i.e., if X1 is one ofancestors of X2, then Spec(X1) is less thanSpec(X2).
Specificity can be applied in automaticconstruction and evaluation of term hierarchy.When domain specific concepts are repre-sented as terms, the terms are classified into twocategories based on composition of unit words.
Inthe first category, new terms are created by add-ing modifiers to existing terms.
For example ?in-sulin-dependent diabetes mellitus?
was createdby adding modifier ?insulin-dependent?
to itshypernym ?diabetes mellitus?
as in Table 1.
InEnglish, the specific level terms are very com-monly compounds of the generic level term andsome modifier (Croft, 2004).
In this case, compo-sitional information is important to get theirmeaning.
In the second category, new terms arecreated independently to existing terms.
For ex-ample, ?wolfram syndrome?
is semantically re-lated to its ancestor terms as in Table 1.
But itshares no common words with its ancestor terms.In this case, contextual information is used todiscriminate the features of the terms.Node Number TermsC18.452.297 diabetes mellitusC18.452.297.267 insulin-dependent diabetes mellitusC18.452.297.267.960 wolfram syndromeTable 1.
Subtree of MeSH1 tree.
Node numbersrepresent hierarchical structure of termsContextual information has been mainly usedto represent the characteristics of terms.
(Cara-ballo, 1999A) (Grefenstette, 1994) (Hearst, 1992)(Pereira, 1993) and (Sanderson, 1999) used con-textual information to find hyponymy relationbetween terms.
(Caraballo, 1999B) also usedcontextual information to determine the specific-ity of nouns.
Contrary, compositional informa-tion of terms has not been commonly discussed.1 MeSH is available at  http://www.nlm.nih.gov/mesh.
MeSH 2003 was usedin this research.We propose new specificity measuring meth-ods based on both compositional and contextualinformation.
The methods are formulated as in-formation theory like measures.
Because themethods don't use domain specific information,they are easily adapted to terms of other domains.This paper consists as follow: compositionaland contextual information is discussed in section2, information theory like measures are describedin section 3, experiment and evaluation is dis-cussed in section 4, finally conclusions are drawnin section 5.2.
Information for Term SpecificityIn this section, we describe compositional infor-mation and contextual information.2.1.
Compositional InformationBy compositionality, the meaning of whole termcan be strictly predicted from the meaning of theindividual words (Manning, 1999).
Many termsare created by appending modifiers to existingterms.
In this mechanism, features of modifiersare added to features of existing terms to makenew concepts.
Word frequency and tf.idf valueare used to quantify features of unit words.
Inter-nal modifier-head structure of terms is used tomeasure specificity incrementally.We assume that terms composed of low fre-quency words have large quantity of domain in-formation.
Because low frequency words appearonly in limited number of terms, the words canclearly discriminate the terms to other terms.tf.idf, multiplied value of term frequency (tf)and inverse document frequency (idf), is widelyused term weighting scheme in information re-trieval (Manning, 1999).
Words with high termfrequency and low document frequency get largetf.idf value.
Because a document usually dis-cusses one topic, and words of large tf.idf valuesare good index terms for the document, the wordsare considered to have topic specific information.Therefore, if a term includes words of large tf.idfvalue, the term is assumed to have topic or do-main specific information.If the modifier-head structure of a term isknown, the specificity of the term is calculatedincrementally starting from head noun.
In thismanner, specificity value of a term is always lar-ger than that of the base (head) term.
This resultanswers to the assumption that more specificterm has larger specificity value.
However, it isvery difficult to analyze modifier-head structureof compound noun.
We use simple nesting rela-tions between terms to analyze structure of terms.A term X is nested to term Y, when X is substringof Y (Frantzi, 2000) as follows:Definition 1 If two terms X and Y are terms insame category and X is nested in Y as W1XW2,then X is base term, and W1 and W2 are modifiersof X.For example two terms, ?diabetes mellitus?and ?insulin dependent diabetes mellitus?, are alldisease names, and the former is nested in thelatter.
In this case, ?diabetes mellitus?
is baseterm and ?insulin dependent?
is modifier of ?in-sulin dependent diabetes mellitus?
by definition 1.If multiple terms are nested in a term, the longestterm is selected as head term.
Specificity of Y ismeasured as equation (2).1 2( ) ( ) ( ) ( )Spec Y Spec X Spec W Spec W?
?= + ?
+ ?
(2)where Spec(X), Spec(W1), and Spec(W2) arespecificity values of X, W1, W2 respectively.
?and ?
, real numbers between 0 and 1, areweighting schemes for specificity of modifiers.They are obtained experimentally.2.2.
Contextual InformationThere are some problems that are hard to addressusing compositional information alone.
Firstly,although features of ?wolfram syndrome?
sharemany common features with features of ?insulin-dependent diabetes mellitus?
in semantic level,they don?t share any common words in lexicallevel.
In this case, it is unreasonable to comparetwo specificity values measured based on compo-sitional information alone.
Secondly, when sev-eral words are combined to a term, there areadditional semantic components that are not pre-dicted by unit words.
For example, ?wolframsyndrome?
is a kind of ?diabetes mellitus?.
Wecan not predict ?diabetes mellitus?
from twoseparate words ?wolfram?
and ?syndrome?.
Fi-nally, modifier-head structure of some terms isambiguous.
For instance, ?vampire slayer?
mightbe a slayer who is vampire or a slayer of vam-pires.
Therefore contextual is used to comple-ment these problems.Contextual information is distribution of sur-rounding words of target terms.
For example, thedistribution of co-occurrence words of the terms,the distribution of predicates which have theterms as arguments, and the distribution of modi-fiers of the terms are contextual information.General terms usually tend to be modified byother words.
Contrary, domain specific termsdon?t tend to be modified by other words, be-cause they have sufficient information in them-selves (Caraballo, 1999B).
Under this assumption,we use probabilistic distribution of modifiers ascontextual information.
Because domain specificterms, unlike general words, are rarely modifiedin corpus, it is important to collect statisticallysufficient modifiers from given corpus.
Thereforeaccurate text processing, such as syntactic pars-ing, is needed to extract modifiers.
As Cara-ballo?s work was for general words, theyextracted only rightmost prenominals as contextinformation.
We use Conexor functional depend-ency parser (Conexor, 2004) to analyze the struc-ture of sentences.
Among many dependencyfunctions defined in Conexor parser, ?attr?
and?mod?
functions are used to extract modifiersfrom analyzed structures.
If a term or modifiersof the term do not occur in corpus, specificity ofthe term can not be measured using contextualinformation3.
Specificity Measuring MethodsIn this section, we describe information theorylike methods using compositional and contextualinformation.
Here, we call information theorylike methods, because some probability valuesused in these methods are not real probability,rather they are relative weight of terms or words.Because information theory is well known for-malism describing information, we adopt themechanism to measure information quantity ofterms.In information theory, when a message withlow probability occurs on channel output, theamount of surprise is large, and the length of bitsto represent this message becomes long.
There-fore the large quantity of information is gainedby this message (Haykin, 1994).
If we considerthe terms in a corpus as messages of a channeloutput, the information quantity of the terms canbe measured using various statistics acquiredfrom the corpus.
A set of terms is defined asequation (3) for further explanation.
{ |1 }kT t k n= ?
?
(3)where tk is a term and n  is total number of terms.In next step, a discrete random variable X is de-fined as equation (4).
{ |1 }( ) Prob( )kk kX x k np x X x= ?
?= =                 (4)where xk is an event of a term tk occurs in corpus,p(xk) is the probability of event xk.
The informa-tion quantity, I(xk), gained after observing theevent xk, is defined by the logarithmic function.Finally I(xk) is used as specificity value of tk asequation (5).
( ) ( ) log ( )k k kSpec t I x p x?
= ?
(5)In equation (5), we can measure specificity oftk, by estimating p(xk).
We describe three estimat-ing methods of p(xk) in following sections.3.1.
Compositional Information basedMethod (Method 1)In this section, we describe a method using com-positional information introduced in section 2.1.This method is divided into two steps: In the firststep, specificity values of all words are measuredindependently.
In the second step, the specificityvalues of words are summed up.
For detail de-scription, we assume that a term tk consists of oneor more words as equation (6).1 2...k mt w w w=                       (6)where wi is i-th word in tk.
In next step, a discreterandom variable Y is defined as equation (7).
{ |1 }( ) Prob( )ii iY y i mp y Y y= ?
?= =                (7)where yi is an event of a word wi occurs in term tk,p(yi) is the probability of event yi.
Informationquantity, I(xk), in equation (5) is redefined asequation (8) based on previous assumption.1( ) ( ) log ( )mk i iiI x p y p y== ??
(8)where I(xk) is average information quantity of allwords in tk.
Two information sources, word fre-quency, tf.idf are used to estimate p(yi).
In thismechanism, p(yi) for informative words shouldbe smaller than that of non informative words.When word frequency is used to quantify fea-tures of words, p(yi) in equation (8) is estimatedas equation (9).
( )( ) ( )( )ii MLE ijjfreq wp y p wfreq w?
= ?
(9)where freq(w) is frequency of word w in corpus,PMLE(wi) is maximum likelihood estimation ofP(wi), and j is index of all words in corpus.
Inthis equation, as low frequency words are infor-mative, P(yi) for the words becomes small.When tf.idf is used to quantify features ofwords, p(yi) in equation (8) is estimated as equa-tion (10).
( )( ) ( ) 1( )ii MLE ijjtf idf wp y p wtf idf w??
= ?
??
(10)where tf?idf(w) is tf.idf value of word w. In thisequation, as words of large tf.idf values are in-formative, p(yi) of the words becomes small.3.2.
Contextual Information based Method(Method 2)In this section, we describe a method using con-textual information introduced in section 2.2.Entropy of probabilistic distribution of modifiersfor a term is defined as equation (11).
( ) ( , ) log ( , )mod k i k i kiH t p mod t p mod t= ??
(11)where p(modi,tk) is the probability of modi modi-fies tk and is estimated as equation (12).
( , )( , )( , )i kMLE i kj kjfreq mod tp mod tfreq mod t= ?
(12)where freq(modi,tk) is number of frequencies thatmodi modifies tk in corpus, j is index of all modi-fiers of tk in corpus.
The entropy calculated byequation (11) is the average information quantityof all (modi,tk) pairs.
Specific terms have low en-tropy, because their modifier distributions aresimple.
Therefore inversed entropy is assigned toI(xk) in equation (5) to make specific terms getlarge quantity of information as equation (13).1( ) max( ( )) ( )k mod i mod ki nI x H t H t?
??
?
(13)where the first term of approximation is themaximum value among modifier entropies of allterms.3.3.
Hybrid Method (Method 3)In this section, we describe a hybrid method toovercome shortcomings of previous two methods.This method measures term specificity as equa-tion (14).1( ) 1 1( ) (1 )( )( ) ( )kCmp k Ctx kI xI x I x?
?
?+ ?
(14)where ICmp(xk) and ICtx(xk) are normalized I(xk)values between 0 and 1, which are measured bycompositional and contextual information basedmethods respectively.
(0 1)?
??
?
is weight of twovalues.
If 0.5?
= , the equation is harmonic meanof two values.
Therefore I(xk) becomes largewhen two values are equally large.4.
Experiment and EvaluationIn this section, we describe the experiments andevaluate proposed methods.
For convenience, wesimply call compositional information basedmethod, contextual information based method,hybrid method as method 1, method 2, method 3respectively.4.1.
EvaluationA sub-tree of MeSH thesaurus is selected for ex-periment.
?metabolic diseases(C18.452)?
node isroot of the subtree, and the subtree consists of436 disease names which are target terms ofspecificity measuring.
A set of journal abstractswas extracted from MEDLINE2 database usingthe disease names as quires.
Therefore, all theabstracts are related to some of the disease names.The set consists of about 170,000 abstracts(20,000,000 words).
The abstracts are analyzedusing Conexor parser, and various statistics areextracted: 1) frequency, tf.idf of the diseasenames, 2) distribution of modifiers of the diseasenames, 3) frequency, tf.idf of unit words of thedisease names.The system was evaluated by two criteria,coverage and precision.
Coverage is the fraction2 MEDLINE is a database of biomedical articles serviced by National Libraryof Medicine, USA.
(http://www.nlm.nih.gov)of the terms which have specificity values bygiven measuring method as equation (15).##of terms with specificitycof all terms=        (15)Method 2 gets relatively lower coverage thanmethod 1, because method 2 can measure speci-ficity when both the terms and their modifiersappear in corpus.
Contrary, method 1 can meas-ure specificity of the terms, when parts of unitwords appear in corpus.
Precision is the fractionof relations with correct specificity values asequation (16).#   ( , )#    ( , )of R p c with correct specificitypof all R p c=  (16)where R(p,c) is a parent-child relation in MeSHthesaurus, and this relation is valid only whenspecificity of two terms are measured by givenmethod.
If child term c has larger specificityvalue than that of parent term p, then the relationis said to have correct specificity values.
We di-vided parent-child relations into two types.
Rela-tions where parent term is nested in child termare categorized as type I.
Other relations arecategorized as type II.
There are 43 relations intype I and 393 relations in type II.
The relationsin type I always have correct specificity valuesprovided structural information method describedsection 2.1 is applied.We tested prior experiment for 10 human sub-jects to find out the upper bound of precision.The subjects are all medical doctors of internalmedicine, which is closely related division to?metabolic diseases?.
They were asked to iden-tify parent-child relation of given two terms.
Theaverage precisions of type I and type II were96.6% and 86.4% respectively.
We set these val-ues as upper bound of precision for suggestedmethods.Specificity values of terms were measuredwith method 1, method 2, and method 3 as Table2.
In method 1, word frequency based method,word tf.idf based method, and structure informa-tion added methods were separately experi-mented.
Two additional methods, based on termfrequency and term tf.idf, were experimented tocompare compositionality based method andwhole term based method.
Two methods whichshowed the best performance in method 1 andmethod 2 were combined into method 3.Word frequency and tf.idf based methodshowed better performance than term basedmethods.
This result indicates that the informa-tion of terms is divided into unit words ratherthan into whole terms.
This result also illustratebasic assumption of this paper that specific con-cepts are created by adding information to exist-ing concepts, and new concepts are expressed asnew terms by adding modifiers to existing terms.Word tf.idf based method showed better preci-sion than word frequency based method.
Thisresult illustrate that tf.idf of words is more infor-mative than frequency of words.Method 2 showed the best performance, preci-sion 70.0% and coverage 70.2%, when wecounted modifiers which modify the target termstwo or more times.
However, method 2 showedworse performance than word tf.idf and structurebased method.
It is assumed that sufficient con-textual information for terms was not collectedfrom corpus, because domain specific terms arerarely modified by other words.Method 3, hybrid method of method 1 (tf.idfof words, structure information) and method 2,showed the best precision of 82.0% of all, be-cause the two methods interacted complementary.PrecisionMethodsType I Type II TotalCoverageHuman subjects(Average) 96.6 86.4 87.4Term frequency 100.0 53.5 60.6 89.5Term tf?idf 52.6 59.2 58.2 89.5Word Freq.
0.37 72.5 69.0 100.0Word Freq.+Structure (?=?=0.2) 100.0 72.8 75.5 100.0Word tf?idf 44.2 75.3 72.2 100.0CompositionalInformationMethod(Method 1) Word tf?idf +Structure (?=?=0.2) 100.0 76.6 78.9 100.0Contextual Information Method (Method 2) (mod cnt>1) 90.0 66.4 70.0 70.2Hybrid Method (Method 3)  (tf?idf + Struct, ?=0.8) 95.0 79.6 82.0 70.2Table 2.
Experimental results (%)The coverage of this method was 70.2% whichequals to the coverage of method 2, because thespecificity value is measured only when thespecificity of method 2 is valid.
In hybrid method,the weight value 0.8?
=  indicates that composi-tional information is more informatives than con-textual information when measuring thespecificity of domain-specific terms.
The preci-sion of 82.0% is good performance compared toupper bound of 87.4%.4.2.
Error AnalysisOne reason of the errors is that the names ofsome internal nodes in MeSH thesaurus are cate-gory names rather disease names.
For example,as ?acid-base imbalance (C18.452.076)?
is nameof disease category, it doesn't occur as frequentlyas other real disease names.Other predictable reason is that we didn?t con-sider various surface forms of same term.
Forexample, although ?NIDDM?
is acronym of ?noninsulin dependent diabetes mellitus?, the systemcounted two terms independently.
Therefore theextracted statistics can?t properly reflect semanticlevel information.If we analyze morphological structure of terms,some errors can be reduced by internal structuremethod described in section 2.1.
For example,?nephrocalcinosis?
have modifier-head structurein morpheme level; ?nephro?
is modifier and?calcinosis?
is head.
Because word formationrules are heavily dependent on the domain spe-cific morphemes, additional information isneeded to apply this approach to other domains.5.
ConclusionsThis paper proposed specificity measuring meth-ods for terms based on information theory likemeasures using compositional and contextualinformation of terms.
The methods are experi-mented on the terms in MeSH thesaurus.
Hybridmethod showed the best precision of 82.0%, be-cause two methods complemented each other.
Asthe proposed methods don't use domain depend-ent information, the methods easily can beadapted to other domains.In the future, the system will be modified tohandle various term formations such as abbrevi-ated form.
Morphological structure analysis ofwords is also needed to use the morpheme levelinformation.
Finally we will apply the proposedmethods to terms of other domains and terms ingeneral domains such as WordNet.AcknowledgementsThis work was supported in part by Ministry ofScience & Technology of Korean governmentand Korea Science & Engineering Foundation.ReferencesCaraballo, S. A.
1999A.
Automatic construction of ahypernym-labeled noun hierarchy from text Cor-pora.
In the proceedings of ACLCaraballo, S. A.  and Charniak, E. 1999B.
Determin-ing the Specificity of Nouns from Text.
In the pro-ceedings of the Joint SIGDAT Conference onEmpirical Methods in Natural Language Processingand Very Large CorporaConexor.
2004.
Conexor Functional DependencyGrammar Parser.
http://www.conexor.comFrantzi, K., Anahiadou, S. and Mima, H. 2000.
Auto-matic recognition of multi-word terms: the C-value/NC-value method.
Journal of Digital Librar-ies, vol.
3, num.
2Grefenstette, G. 1994.
Explorations in Automatic The-saurus Discovery.
Kluwer Academic PublishersHaykin, S. 1994.
Neural Network.
IEEE Press, pp.
444Hearst, M. A.
1992.
Automatic Acquisition of Hypo-nyms from Large Text Corpora.
In proceedings ofACLManning, C. D. and Schutze, H. 1999.
Foundations ofStatistical Natural Language Processing.
The MITPresssPereira, F., Tishby, N., and Lee, L. 1993.
Distributa-tional clustering of English words.
In the proceed-ings of ACLSanderson, M. 1999.
Deriving concept hierarchiesfrom text.
In the Proceedings of the 22th AnnualACM S1GIR Conference on Research and Devel-opment in Information RetrievalWright, S. E., Budin, G.. 1997.
Handbook of TermManagement: vol.
1.
John Benjamins publishingcompanyWilliam Croft.
2004.
Typology and Universals.
2nd ed.Cambridge Textbooks in Linguistics, CambridgeUniv.
Press
