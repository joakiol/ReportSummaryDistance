Proceedings of the NAACL HLT Workshop on Semantic Evaluations: Recent Achievements and Future Directions, pages 58?63,Boulder, Colorado, June 2009. c?2009 Association for Computational LinguisticsUsing Lexical Patterns in the Google Web 1T Corpus to DeduceSemantic Relations Between NounsPaul Nulty Fintan CostelloSchool of Computer Science and Informatics School of Computer Science and InformaticsUniversity College Dublin, Belfield University College Dublin, BelfieldDublin 4, Ireland  Dublin 4, Irelandpaul.nulty@ucd.ie fintan.costello@ucd.ieAbstractThis paper investigates methods for using lexical pat-terns in a corpus to deduce the semantic relation thatholds between two nouns in a noun-noun compoundphrase such as ?flu virus?
or ?morning exercise?.
Muchof the previous work in this area has used automatedqueries to commercial web search engines.
In our exper-iments we use the Google Web 1T corpus.
This corpuscontains every 2,3, 4 and 5 gram occurring morethan 40 times in Google's index of the web, but has theadvantage of being available to researchers directlyrather than through a web interface.
This paper evalu-ates the performance of the Web 1T corpus on the taskcompared to similar systems in the literature, and alsoinvestigates what kind of lexical patterns are most in-formative when trying to identify a semantic relationbetween two nouns.1 IntroductionNoun-noun combinations occur frequently in manylanguages, and the problem of semantic disambig-uation of these phrases has many potential applica-tions  in  natural  language  processing  and  otherareas.
Search engines which can identify the rela-tions between nouns may be able to return moreaccurate  results.
Hand-built  ontologies  such  asWordNet at present only contain a few basic se-mantic  relations  between  nouns,  such  as  hyper-nymy and meronymy.If the process of discovering semantic relationsfrom  text  were  automated,  more  links  couldquickly be built up.
Machine translation and ques-tion-answering  are  other  potential  applications.Noun compounds are very common in English, es-pecially  in  technical  documentation  and  neolo-gisms.
Latin languages tend to favour prepositionalparaphrases instead of direct compound translation,and to select the correct preposition it is oftennecessary to know the semantic relation.
One verycommon approach to this problem is to define aset of semantic relations which capture the interac-tion between the modifier and the head noun, andthen attempt to assign one of these semantic rela-tions to each noun-modifier pair.
For example, thephrase flu virus could be assigned the semantic re-lation causal (the virus causes the flu); the relationfor desert wind could be location (the storm is loc-ated in the desert).There is no consensus as to which set of semanticrelations best captures the differences in meaningof various noun phrases.
Work in theoretical lin-guistics has suggested that noun-noun compoundsmay be formed by the deletion of a predicate verbor preposition (Levi 1978).
However, whether theset of possible predicates numbers 5 or 50, thereare  likely to  be  some  examples  of  noun phrasesthat fit into none of the categories and some that fitin multiple categories.2 Related WorkThe idea of searching a large corpus for specificlexicosyntactic phrases to indicate a semantic rela-tion  of  interest  was  first  described  by  Hearst(1992).
Lauer  (1995)  tackled the  problem of  se-mantically disambiguating noun phrases by tryingto find the preposition which best describes the re-lation  between  the  modifier  and  head  noun.
Hismethod  involves  searching  a  corpus  for  occur-rences paraphrases of the form ?noun prepositionmodifier?.
Whichever preposition is most frequentin this context is chosen to represent the predicate58of the nominal, which poses the same problem ofvagueness  as  Levi's  approach.
Lapata  and Keller(2005)  improved  on  Lauer's  results  on  the  sametask by using the web as a corpus.Turney  and  Littman  (2005)  used  queries  to  theAltaVista search engine as the basis for their learn-ing  algorithm.
Using  the  dataset  of  Nastase  andSzpakowicz (2003), they experimented with a setof 64 short prepositional and conjunctive phrasesthey call ?joining terms?
to generate exact queriesfor AltaVista of the form ?noun joining term mod-ifier?, and ?modifier joining term noun?.
These hitcounts  were  used  with  a  nearest  neighbour  al-gorithm to assign the noun phrases semantic rela-tions.Nakov and Hearst (2006) present a system that dis-covers verbs that characterize the relation betweentwo  nouns in a compound.
By writing structuredqueries  to a web search engine and syntacticallyparsing  the  returned  'snippet',  they  were  able  toidentify verbs that were suitable predicates.
For ex-ample, for the compound neck vein, they retrievedverbs  and  verb-preposition  such  as  predicatesemerge from, pass through, terminate in, and oth-ers.
However, their evaluation is qualitative; theydo not attempt to use the verbs directly to categor-ize a compound as a particular semantic relation.Turney  (2006)  examines  similarity  measures  forsemantic relations.
He notes that there are at leasttwo  kinds  of  similarity:  attributional  similarity,which applies between words, and relational simil-arity, which holds between pairs of words.Words that have a high attributional similarity areknown as synonyms; e.g.
chair and stool.
When therelations in each of two pairs of words are similar,it is said that there is an analogy between the twopairs of words, e.g.
stone:mason, carpenter:wood.Turney points out that word pairs with high rela-tional similarity do not necessarily contain wordswith high attributional similarity.
For example, al-though the relations are similar in traffic:street andwater:riverbed, water is not similar to traffic, norstreet similar to riverbed.Therefore, a measure of similarity of semantic rela-tions allows a more reliable judgment of analogythan the first-order similarity of the nouns3 MotivationWhen  looking  for  lexical  patterns  between  twonouns, as is required with vector-space approaches,data  sparseness  is  a  common  problem.
To  over-come this, many of the best-performing systems inthis area rely on automated queries to web search-engines  (Lapata  and  Keller  (2005),  Turney  andLittman  (2005),  Nakov  and  Hearst  (2006)).
Themost  apparent  advantage  of  using  search-enginequeries is simply the greater volume of data avail-able.Keller and Lapata (2003) demonstrated the useful-ness of this extra data on a type of word-sense dis-ambiguation test and also found that web frequen-cies of bigrams correlated well withfrequencies in a standard corpus.Kilgarriff  (2007)  argues  against  the  use  of  com-mercial  search engines  for  research,  and outlinessome  of  the  major  drawbacks.
Search  enginecrawlers  do  not  lemmatize  or  part-of-speech  tagtheir text.
This means that to obtain frequencies formay different inflectional forms, researchers mustperform a  separate  query for  each possible  formand sum the results.If part-of-speech tagging is required, the 'snippet'of  text  that  is  returned  with  each  result  may  betagged after the query has been executed, howeverthe APIs for the major search engines have limita-tions on how many snippets may be retrieved for agiven query (100 -1000).Another problem is that search engine query syn-tax is  limited,  and sometimes  mysterious.
In  thecase of Google, only basic boolean operators aresupported (AND, OR, NOT), and the function ofthe wildcard symbol (*) is limited, difficult to de-cipher and may have changed over time.Kilgarriff also points out that the search API ser-vices to the major search engines have constraintson the number of searches that are allowed per userper day.
Because of the multiple searches that areneeded to cover inflectional  variants and recoversnippets for  tagging,  a limit  of  1000 queries perday, as with the Google API, makes experimenta-tion slow.
This paper will describe the use of theWeb 1T corpus, made available by Google in 2006(Brants and Franz 2006).
This corpus consists of n-grams collected from web data, and is available toresearchers  in  its  entirety,  rather  than  through  aweb search interface.
This means that there is no59limit  to the amount of searches that may be per-formed, and an arbitrarily complex query syntax ispossible.Despite being available since 2006, few research-ers have made use of the Web 1T corpus.
Hawker(2006) provides an example of using the corpus forword sense documentation, and describes a methodfor  efficient  searching.
We  will  outline  the  per-formance of the corpus on the task of identifyingthe semantic relation between two nouns.
Anothermotivation behind this paper is to examine the use-fulness of different lexical patterns for the task ofdeducing semantic relations.In this paper, we are interested in whether the fre-quency with which a joining term occurs betweentwo nouns is related to how it indicates a semanticinteraction.
This is in part motivated by Zipf?s the-ory which states that the more frequently a wordoccurs in a corpus the more meanings or senses itis  likely to  have  (Zipf  1929).
If  this  is  true,  wewould expect that very frequent prepositions, suchas ?of?, would have many possible meanings andtherefore not reliably predict a semantic relation.However, less frequent prepositions, such as ?dur-ing?
would have a more limited set of senses andtherefore  accurately  predict  a  semantic  relation.Zipf also showed that the frequency of a term is re-lated  to  its  length.
We  will  investigate  whetherlonger lexical patterns are more useful at identify-ing semantic relations than shorter patterns, andwhether less frequent patterns perform better thanmore frequent ones.4 Web 1T CorpusThe Web1T corpus consists of n-grams taken fromapproximately  one  trillion  words  of  English  texttaken from web pages in Google's  index of webpages.
The data includes all 2,3,4 and 5-grams thatoccur more than 40 times in these pages.
The datacomes  in  the  form  of  approximately  110  com-pressed files for each of the window sizes.
Each ofthese files consists of exactly 10 million n-grams,with their frequency counts.
Below is an exampleof the 3-gram data:ceramics collection and 43ceramics collection at 52ceramics collection is 68ceramics collection | 59ceramics collections , 66ceramics collections .
60The uncompressed 3-grams,  4-grams 5-grams to-gether take up 80GB on disk.
In order to make itpossible to index and search this data, we excludedn-grams that contained any punctuation or non-al-phanumeric characters.
We also excluded n-gramsthat contained any uppercase letters,  although wedid allow for the first letter of the first word to beuppercase.We indexed the data using Ferret, a Ruby port ofthe Java search engine package Lucene.
We wereable to index all of the data in under 48 hours, us-ing 32GB of hard disk space.
The resulting indexwas searchable by first word, last word, and inter-vening pattern.
Only n-grams with a frequency of40 or higher are included in the dataset, which ob-viously means that an average query returns fewerresults than a web search.
However, with the dataavailable  on  local  disk  it  is  stable,  reliable,  andopen to any kind of query syntax or lemmatization.5 Lexical Patterns for DisambiguationModifier-noun phrases are often used interchange-ably with paraphrases which contain the modifierand  the  noun  joined  by  a  preposition  or  simpleverb.
For example, the noun-phrase ?morning exer-cise?
may be paraphrased as ?exercise in the morn-ing?
or ?exercise during the morning?.
In a verylarge corpus, it is possible to find many reasonableparaphrases  of  noun  phrases.
These  paraphrasescontain information about the relationshipbetween the modifier and the head noun that is notpresent in the bare modifier-noun phrase.
By ana-lyzing these paraphrases, we can deduce what se-mantic  relation  is  most  likely.
For  example,  theparaphrases  ?exercise  during  the  morning?
and?exercise in the morning?
are likely to occur morefrequently  than  ?exercise  about  the  morning?
or?exercise at the morning?.One  method  for  deducing  semantic  relationsbetween words  in  compounds  involves  gatheringn-gram frequencies of these paraphrases, contain-ing a noun,  a modifier  and a lexical  pattern thatlinks  them.
Some algorithm can then be used tomap from lexical patterns to frequencies to semant-60ic relations and so find the correct relation for thecompound in question.
This is the approach we usein our experiments.In order to describe the semantic relation betweentwo  nouns  in  a  compound  ?noun1  noun2?
wesearch for ngrams that begin with  noun2  and endwith noun1, since in English the head of the nouncompound is the second word.
For example, for thecompound 'flu virus', we look at n-grams that beginwith 'virus' and end with 'flu'.
We extract the wordsthat occur between the two nouns (a string of 1-3words) and use these lexical patterns as features forthe machine learning algorithm.For  each  compound  we  also  include  n-gramswhich have the plural form of noun1 or noun2.
Weassign a score to each of these lexical patterns, asthe log of the frequency of the n-gram.
We usedthe 400 most frequent lexical patterns extracted asthe features for the model.
Below are examples ofsome of the lexical patterns that were extracted:andof theofin theforand thefor theto thewithinoron thefrom thethetoof awith theonthat thefromFigure 1: The 20 most frequent patternsThe simplest way to use this vector space model toclassify noun-noun combinations is  to use a dis-tance metric to compare a novel pair of nouns toones previously annotated with semantic relations.Nulty  (2007)  compares  these  nearest  neighbormodels with other machine learning techniques andfinds that using a support vector machine leads toimproved classification.In our experiments we used the support vector ma-chine and k-nearest-neighbor algorithms from theWEKA machine learning toolkit.
All experimentswere conducted using leave-one-out cross valida-tion: each example in the dataset is in turn testedalone, with all the other examples used for training.The first dataset used in these experiments was cre-ated by Nastase and Szpackowicz (2003) and usedin experiments by Turney and Littmann (2005) andTurney  (2006).
The  data  consists  of  600  noun-modifier  compounds.
Of  the  600 examples,  fourcontained hyphenated modifiers, for example ?test-tube baby?.
These were excluded from our dataset,leaving 596 examples.
The data is labeled with twodifferent sets of semantic relations: one set of 30relations with fairly specific meanings and anotherset of 5 relations with more abstract relations.
Inthese experiments  we use only the set  of  5 rela-tions.
The reason for this is that splitting a set of600 examples into 30 classes results in few trainingexamples per class.
This problem is compoundedby the fact that the dataset is uneven, with far moreexamples in some classes than in others.
Below arethe five relations and some examples.Relation: Example:causal flu virus, onion teartemporal summer travel, night classspatial west coast, home remedyparticipant mail sorter, blood donorquality rice paper, picture bookFigure 2: Example phrases and their semantic relationsFor our research we are particularly interested innoun-noun combinations.
Of the 596 examples inthe dataset, we found that 325 were clearly noun-noun  combinations,  e.g.
?picture  book?,  ricepaper?, while in the remainder the modifier was anadjective, for example ?warm air?, ?heavy storm?.We used only the noun-noun combinations in ourexperiments,  as this  is  the focus of  our research.We experimented with both lemmatization of thedata and excluding semantically empty stop words(determiners  and  conjunctions)  from  the  lexicalpatterns,  however  neither  of  these  methods  im-proved  performance.
Below  are  the  results  ob-tained with the k-nearest neighbor algorithm.
Theoptimum value of k was 3.Precision Recall f-score class.442 .452 .447 Quality.75 .444 .558 Temporal.243 .167 .198 Causal.447 .611 .516 Participant.571 .138 .222 SpatialFigure 3: Results using the K-NN algorithmThe overall accuracy was 44% and the macro-aver-aged f-value was .39.61Below are the results obtained using the support-vector machine algorithm:Precision Recall f-score class.725 .345 .468 Quality.733 .407 .524 Temporal.545 .111 .185 Causal.472 .885 .615 Participant.462 .207 .268 SpatialFigure 4: Results using the Support Vector MachineThe  overall  accuracy  was  51.7% and  the  mac-roaveraged  f-value  was  .42.
A  majority  classbaseline(always predicting the largest class) would achievean accuracy of 43.7%.6 Which Lexical Patterns are Most Use-ful?In addition to evaluating the Google Web 1T cor-pus,  a  motivation for this  paper is  to investigatewhat  kind of lexical  patterns are most  useful  fordeducing semantic relations.
In order to investigatethis, we repeated the experiment one using the 3-grams,  4-grams  and  5-grams  separately,  whichgave lexical patterns of length 1, 2 and 3 respect-ively.
Accuracy obtained using the support vectormachine and k-nearest-neighbor algorithms are be-low:3-grams 4grams 5-grams AllKNN 36 42.5 42.4 44SVM 44.3 49.2 43.4 51.7Figure 5: Results for different sizes of lexical patternsAgain, in each case the support vector machineperforms  better  than  the  nearest  neighbor  al-gorithm.
The 4- grams (two-word lexical patterns)give the best performance.
One possible explana-tion for this is that the single word lexical patternsdon't convey a very specific relation, while the 3word  patterns  are  relatively  rare  in  the  corpus,leading  to  many  missing  values  in  the  trainingdata.We were also interested in how the frequency ofthe lexical patterns related to their ability to predictthe correct semantic relation.
To evaluate this, weordered the 400 lexical patterns retrieved by fre-quency and then split them into three groups.
Wetook  the  64  most  frequent  patterns,  the  patternsranked  100-164  in  frequency,  and  those  ranked300-364.
We chose to include 64 patterns in eachgroup to  allow for  comparison  with  Turney andLittman  (2001),  who use  64 hand-generated  pat-terns.
Examples of the most frequent patterns areshown in Fig 1.
Below are  examples  of  patternsfrom the other two groups.as well asout of theof oneof freshintofor allwaswith yourrelated to thein the earlymyon Fridaywithoutwhich thewith myand theiraround thewhenwhoseduringFigure 6: Frequency Ranks 100-120to producebutthat causeof socialwhile theor any othersuch as theare in theto provideif afrom oneoneprovidesfrom yourof ediblelevels andcomes fromchosen by theproducingdoes notthan thebelonging to theFigure 7: Frequency Ranks 300-320The accuracies obtained using patterns in the dif-ferent frequency groups are shown below.1-64 100-164 300-364KNN 40.9 43.5 41.9SVM 47.6 45.2 41.5Figure 8: Results for different frequency bands of pat-ternsAlthough there is no large effect to the accuracy ofthe KNN algorithm, the Support Vector Machineseems to perform better with the most frequent pat-terns.
One possible explanation for this is that al-though the  less  frequent  patterns  seem more  in-formative, they more often result in zero matchesin the corpus, which simply leaves a missing valuein the training data.627 ConclusionThis paper reports several experiments on the se-mantic disambiguation of noun-noun phrases usingthe Google Web 1T corpus, and shows that the res-ults are comparable to previous work which has re-lied on a web interface to search engines.
Having auseful corpus based on web data that can be storedand  searched  locally  means  that  results  will  bestable across time and can be subject to complexqueries.
Experiments designed to evaluate the use-fulness  of  different  lexical  patterns  did not  yieldstrong results and further work is required in thisarea.ReferencesThorsten Brants and Alex Franz.
2006.
Web 1T 5-gramCorpus  Version  1.1.
Technical  report,  Google  Re-searchTobias Hawker.
2006.
Using Contexts of One TrillionWords for WSD.
Proceedings of the 10th Conferenceof the Pacific Association for ComputationalLinguistics, pages 85?93.Marti A. Hearst: 1992.
Automatic Acquisition ofHyponyms  from  Large  Text  Corpora.
COLING:539-545Keller, Frank and Mirella Lapata.
2003.
Using theWeb to Obtain Frequencies for Unseen BigramsComputational Linguistics 29:3, 459-484.Adam Kilgarriff, 2007.
Googleology is Bad Science.Comput.
Linguist.
33, 1 147-151.Lapata, Mirella and Frank Keller.
2005.
Web BasedModels for Natural Language Processing.
ACMTransactions on Speech and LanguageProcessing 2:1, 1-31.Mark Lauer.
Designing Statistical Language Learners:Experiments on Noun Compounds.
PhD thesis,Macquarie University NSW 2109 Australia.Judith Levi.
(1978) The Syntax and Semantics ofComplex Nominals, Academic Press, New York, NY.Phil Maguire (2007) A cognitive model of conceptualcombination Unpublished PhD Thesis, UCD DublinPreslav Nakov and Marti Hearst.
2006.
Using Verbs toCharacterize Noun-Noun Relations, in theProceedings of AIMSA 2006,Preslav Nakov and Marti Hearst.
2005.
Using the Webas an Implicit Training Set: Application to StructuralAmbiguity Resolution, in HLT/EMNLP'0Vivi Nastase and Stan Szpakowicz.
2003.
ExploringNoun-Modifier Semantic Relations.
InternationalWorkshop on Computational Semantics, Tillburg,Netherlands, 2003Paul Nulty and Fintan Costello, 2007.
SemanticClassification of Noun Phrases Using Web Countsand Learning Algorithms.
Proceedings ofACL 2007 Student Reseach Workshop.Barbara Rosario and Marti A. Hearst.
2001.
Classifyingthe semantic relations in noun compounds via adomain-specific lexical hierarchy.
In Proceedings ofthe 2001Conference on Empirical Methods inNatural Language Processing.
ACLPeter D. Turney.
2005.
Measuring semantic similarityby latent relational analysis.
In Proceedings of theNineteenth International Joint Conference onArtificial Intelligence (IJCAI-05), pages 1136-1141.Peter D.
Turney., and Michael L. Littman,.
2006.Corpus based learning of analogies and semanticrelations.
Machine Learning, 60(1?3):251?278Ian H. Witten and Eibe Frank.
1999.
Data Mining:Practical Machine Learning Tools and Techniqueswith Java Implementations, Morgan Kaufman (1999)George K. Zipf.
1932.
Selected Studies of the Principleof Relative Frequency in Language.
Cambridge, MA63
