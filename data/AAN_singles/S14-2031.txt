Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 198?202,Dublin, Ireland, August 23-24, 2014.Columbia NLP: Sentiment Detection of Sentences and Subjective Phrasesin Social MediaSara RosenthalDept.
of Computer ScienceColumbia UniversityNew York, NY 10027, USAsara@cs.columbia.eduApoorv AgarwalDept.
of Computer ScienceColumbia UniversityNew York, NY 10027, USAapoorv@cs.columbia.eduKathleen McKeownDept.
of Computer ScienceColumbia UniversityNew York, NY 10027, USAkathy@cs.columbia.eduAbstractWe present two supervised sentiment de-tection systems which were used to com-pete in SemEval-2014 Task 9: Senti-ment Analysis in Twitter.
The first sys-tem (Rosenthal and McKeown, 2013) clas-sifies the polarity of subjective phrases aspositive, negative, or neutral.
It is tai-lored towards online genres, specificallyTwitter, through the inclusion of dictionar-ies developed to capture vocabulary usedin online conversations (e.g., slang andemoticons) as well as stylistic featurescommon to social media.
The second sys-tem (Agarwal et al., 2011) classifies entiretweets as positive, negative, or neutral.
Ittoo includes dictionaries and stylistic fea-tures developed for social media, severalof which are distinctive from those in thefirst system.
We use both systems to par-ticipate in Subtasks A and B of SemEval-2014 Task 9: Sentiment Analysis in Twit-ter.
We participated for the first time inSubtask B: Message-Level Sentiment De-tection by combining the two systems toachieve improved results compared to ei-ther system alone.1 IntroductionIn this paper we describe two prior sentiment de-tection algorithms for social media.
Both systems(Rosenthal and McKeown, 2013; Agarwal et al.,2011) classify the polarity of sentence phrases andThis work is licensed under a Creative Commons At-tribution 4.0 International License.
Page numbers and pro-ceedings footer are added by the organizers.
License details:http://creativecommons.org/licenses/by/4.0/tweets as positive, negative, or neutral.
These al-gorithms were used to participate in the the expres-sion level task (Subtask A) and message level task(Subtask B) of the SemEval-2014 Task 9: Senti-ment Analysis in Twitter (Rosenthal et al., 2014)which one of the authors helped organize.We first show improved results compared to ourparticipation in the prior year in the expression-level task (Subtask A) by incorporating a new dic-tionary and new features into the system.
Our fo-cus this year was on Subtask B which we partici-pated in for the first time.
We integrated two sys-tems to achieve improved results compared to ei-ther system alone.
Our analysis shows that the firstsystem performs better on recall while the secondsystem performs better on precision.
We used con-fidence metrics outputted by the systems to deter-mine which answer should be used.
This resultedin a slight improvement in the Twitter dataset com-pared to either system alone.
In this rest of thispaper, we discuss related work, the methods foreach system, and experiments and results for eachsubtask using the data provided by Semeval-2014Task 9: Sentiment Analysis in Twitter (Rosenthalet al., 2014).2 Related WorkSeveral recent papers have explored sentimentanalysis in Twitter.
Go et al (2009) and Pakand Paroubek (2010) classify the sentiment oftweets containing emoticons using n-grams andPOS.
Barbosa and Feng (2010) detect sentimentusing a polarity dictionary that includes web vo-cabulary and tweet-specific social media features.Bermingham and Smeaton (2010) compare polar-ity detection in twitter to blogs and movie reviewsusing lexical features.Finally, there is a large amount of related work198through the participants of Semeval 2013 Task2, and Semeval 2014 Task9: Sentiment Analysisin Twitter (Nakov et al., 2013; Rosenthal et al.,2014).
A full list of teams and results can be foundin the task description papers.3 Phrased-Based Sentiment DetectionWe developed a phrase based sentiment detectionsystem geared towards Social Media by augment-ing the state of the art system developed by Agar-wal et al.
(2009) to include additional dictionar-ies such as Wiktionary and new features such asword lengthening (e.g.
helllllloooo) and emoti-cons (e.g.
:)) (Rosenthal and McKeown, 2013).We initially evaluated our system through our par-ticipation in the first Sentiment Analysis in Twittertask (Nakov et al., 2013).
We have improved oursystem this year by adding a new dictionary andadditional features.3.1 LexiconsWe assign a prior polarity score to each word byusing the scores provided by the Dictionary ofAffect in Language (DAL) (Whissel, 1989) aug-mented with WordNet (Fellbaum, 1998) to im-prove coverage.
We additionally augment it withWiktionary, emoticon, and acronym dictionariesto improve coverage in social media (Rosenthaland McKeown, 2013).
The DAL covers 50.1% ofthe vocabulary, 16.5% are proper nouns which weexclude due to their lack of polarity.
WordNet cov-ers 8.7% of the vocabulary and Wiktionary covers12.5% of the vocabulary.
Finally, 3.6% of the vo-cabulary are emoticons, acronyms, word length-ening, and forms of punctuation.
8.6% of the vo-cabulary is not covered which means we find aprior polarity for 96.4% of the vocabulary.
In ad-dition to these dictionaries we also use SentiWord-Net (Baccianella et al., 2010) as a new distinct fea-ture that is used in addition to the prior polaritycomputed from the DAL scores.3.2 MethodWe include POS tags and the top n-gram fea-tures as described in prior work (Agarwal et al.,2009; Rosenthal and McKeown, 2013).
The DALand other dictionaries are used along with a nega-tion state machine (Agarwal et al., 2009) to deter-mine the polarity for each word in the sentence.We include all the features described in the orig-inal system (Agarwal et al., 2009) such as theData Set Majority 2013 2014Twitter Dev 38.14 77.6 81.5Twitter Test 42.22 N/A 76.54Twitter Sarcasm 39.81 N/A 61.76SMS 31.45 73.3 74.55LiveJournal 33.42 N/A 78.19Table 1: A comparison between the 2013 and 2014results for Subtask A using the SemEval Twittertraining corpus.
All results exceed the majoritybaseline of the positive class significantly.DAL scores, polar chunk n-grams, and count ofsyntactic chunks with their prior polarity basedon the chunks position.
Finally, we include sev-eral lexical-stylistic features that can occur in alldatasets.
We divide these features into two groups,general: ones that are common across online andtraditional genres (e.g.
exclamation points), andsocial media: one that are far more common inonline genres (e.g.
emoticons).
The features aredescribed in further detail in the precursor to thiswork (Rosenthal and McKeown, 2013).
Featureselection was performed using chi-square in Weka(Hall et al., 2009).In addition we introduce some new featuresthat were not used in the prior year.
SentiWord-Net (Baccianella et al., 2010) is a sentiment dic-tionary built upon WordNet that contains scoresfor each word where scores > 0 indicate the wordis positive and scores < 0 indicate the word is neg-ative.
We sum the scores for each word in thephrase and use this as a single polarity feature.We found that this feature alone gave us a 2% im-provement over our best results from last year.
Wealso include some other minor features: tweet andphrase length and the position of the phrase withinthe tweet.3.3 Experiments and ResultsWe ran all of our experiments in Weka (Hall et al.,2009) using Logistic Regression.
We also exper-imented with other learning methods (e.g.
SVMand Naive Bayes) but found that Logistic Regres-sion worked the same or better than other methods.All results are shown using the average F-measureof the positive and negative class.
The results arecompared against the majority baseline of the pos-itive class.
We do not use neutral/objective as themajority class because it is not included in the av-erage F-score in the Semeval task.The full results in the participation of SemEval2014: Sentiment Analysis in Twitter, Subtask A,199are shown in Table 1.
Our system outperforms themajority baseline significantly in all classes.
Oursubmitted system was trained using 3-way clas-sification (positive/negative/polarity).
It includedall the dictionaries from prior years and the top100 n-grams with feature selection.
In addition,it included SentiWordNet and the other new fea-tures added in 2014 which provided a 4% increasecompared to our best results during the prior year(77.6% to 81.5%) and a rank of 10/20 amongst theconstrained systems which used no external data.Our results on the new test set is 76.54% for a rankof 14/20.
We do not do well in detecting the po-larity of phrases in sarcastic tweets.
This is consis-tent with the other teams as sarcastic tweets tend tohave their polarity flipped.
The improvements toour system provided a 1% boost in the SMS datawith a rank of 15/20.
Finally, in the LiveJournaldataset we had an F-Score of 78.19% for a rank of12/20.4 Message-Level Sentiment DetectionOur message-level system combines two prior sys-tems to achieve improved results.
The first systeminputs an entire tweet as a ?phrase?
to the phrase-level sentiment detection system described in Sec-tion 3.
The second system is described below.4.1 LexiconsThe second system (Agarwal et al., 2011) makesuse of two dictionaries distinctive from the othersystem: 1) an emoticon dictionary and 2) anacronym dictionary.
The emoticon dictionary wasprepared by hand-labeling 170 emoticons listed onWikipedia.1For example, :) is labeled as positivewhereas :=( is labeled as negative.
Each emoticonis assigned a label from the following set of labels:Extremely-positive, Extremely-negative, Positive,Negative, and Neutral.
We compile an acronymdictionary from an on-line resource.2The dictio-nary has translations for 5,184 acronyms.
For ex-ample, lol is translated to laughing out loud.4.2 Prior Polarity ScoringA number of our features are based on prior po-larity of words.
As in the phrase-based system wetoo build off of prior work (Agarwal et al., 2009)by using the DAL and augmenting it with Word-net.
However, we do not follow the earlier method1http://en.wikipedia.org/wiki/List of emoticons2http://www.noslang.com/but use it as motivation.
We consider words withwith a polarity score (using the pleasantness met-ric from the DAL) of less than 0.5 as negative,higher than 0.8 as positive and the rest as neutral.If a word is not directly found in the dictionary, weretrieve all synonyms from Wordnet.
We then lookfor each of the synonyms in the DAL.
If any syn-onym is found in the DAL, we assign the originalword the same pleasantness score as its synonym.If none of the synonyms is present in the DAL, theword is not associated with any prior polarity.
Forthe given data we directly found the prior polar-ity of 50.1% of the words.
We find the polarity ofanother 8.7% of the words by using WordNet.
Sowe find prior polarity of about 58.7% of Englishlanguage words.4.3 FeaturesWe propose a set of 50 features.
We calculate thesefeatures for the whole tweet and for the last one-third of the tweet.
In total, we get 100 additionalfeatures.
Our features may be divided into threebroad categories: ones that are primarily countsof various features and therefore the value of thefeature is a natural number ?
N. Second, we in-clude features whose value is a real number ?
R.These are primarily features that capture the scoreretrieved from DAL.
The third category is featureswhose values are boolean ?
B.
These are bag ofwords, presence of exclamation marks and capital-ized text.
Each of these broad categories is dividedinto two subcategories: Polar features and Non-polar features.
We refer to a feature as polar if wecalculate its prior polarity either by looking it up inDAL (extended through WordNet) or in the emoti-con dictionary.
All other features which are notassociated with any prior polarity fall in the Non-polar category.
Each of the Polar and Non-polarfeatures is further subdivided into two categories:POS and Other.
POS refers to features that cap-ture statistics about parts-of-speech of words andOther refers to all other types of features.A more detailed explanation of the system canbe found in Agarwal et al (2011).4.4 Combined SystemOur analysis showed that the first system performsbetter on recall while the second system performsbetter on precision.
We also found that there were785 tweets in the development set where one sys-tem got it correct and the other one got it incorrect.This leaves room for a significant improvement200Experiment Twitter SMS LiveJournalDev Test SarcasmMajority 29.19 34.64 27.73 19.03 27.21Phrase-Based System 62.09 64.74 40.75 56.86 62.22Tweet-Level System 62.4 63.73 42.41 60.54 69.44Combined System 64.6 65.42 40.02 59.84 68.79Table 2: A comparison between the different systems using the Twitter training corpus provided by theSemEval task for Subtask B.
All results exceed the majority baseline of the positive class significantly.compared to using each system independently.
Wecombined the two systems for the evaluation byusing the confidence provided by the phrase-basedsystem.
If the phrase-based system was < 70%confident we use the message-level system.4.5 Experiments and ResultsThis task was evaluated on the Twitter dataset pro-vided by Semeval-2013 Task 2, Subtask B.
All re-sults are shown using the average F-measure of thepositive and negative class.
The full results in theparticipation of SemEval 2014: Sentiment Anal-ysis in Twitter, Subtask B, are shown in Table 2.All the results outperform the majority baseline ofthe more prominent positive polarity class signifi-cantly.
The combined system outperforms the in-dividual systems for the Twitter development andtest set.
It does not outperform the sarcasm test set,but this may be due to the small size; it containsonly 100 tweets.
The Tweet-Level system outper-forms the phrase-based and combined system forthe LiveJournal and SMS test sets.
A closer look atthe results indicated that the phrase-based systemhas particular difficulty with the short sentenceswhich are more common in SMS and LiveJour-nal.
For example, the average number of charac-ters in a tweet is 120 whereas it is 95.6 in SMSmessages (Nakov et al., 2013).
Short sentencesare harder because there are fewer polarity wordswhich causes the phrase-based system to incor-rectly pick neutral.
In addition, short sentences areharder because the BOW feature space, which ishuge and already sparse, becomes sparser and in-dividual features start to over-fit.
Part of this prob-lem is handled by using Senti-features so the spacewill be less sparse.Our ranking in the Twitter 2013 and SMS 2013development data is 18/50 and 20/50 respectively.Our rank in the Twitter 2014 test set is 15/50 andour rank in the LiveJournal test set is 19/50.
Basedon our rankings it is clear that our systems aregeared more towards Twitter than other social me-dia.
Finally our ranking in the Sarcasm test set is41/50.
Although this ranking is quite low, it is infact encouraging.
It indicates that the sarcasm hasswitched the polarity of the tweet.
In the future wewould like to include a system (e.g.
(Gonz?alez-Ib?a?nez et al., 2011)) that can detect whether thetweet is sarcastic.5 Discussion and Future WorkWe participated in Semeval-2014 Task 9: Senti-ment Analysis in Twitter Subtasks A and B. InSubtask A, we show that adding additional fea-tures related to location and using SentiWord-Net gives us improvement compared to our priorsystem.
In Subtask B, we show that combiningtwo systems achieves slight improvements overusing either system alone.
Combining the twosystem achieves greater coverage as the systemsuse different emoticon and acronym dictionar-ies and the phrase-based system uses Wiktionary.The message-level system is geared toward entiretweets whereas the phrase-based is geared towardphrases (even though, in this case we consider theentire tweet to be a ?phrase?).
This is reflective inseveral features, such as the position of the targetphrase and the syntactic chunk scores in the phrasebased system and the features related to the lastthird of the tweet in the message-level system.
Inthe future, we?d like to perform an error analysis todetermine the source of our errors and specific ex-amples of the kind of differences found in the twosystems.
Finally, we have found that at times thescores of the DAL do not line up with polarity insocial media.
Therefore, we would like to exploreincluding more sentiment dictionaries instead of,or in addition to, the DAL.6 AcknowledgementsThis research was funded by the DARPA DEFTProgram.
All statements of fact, opinion or con-clusions contained herein are those of the authorsand should not be construed as representing theofficial views, policies, or positions of the Depart-ment of Defense, or the U.S. Government.201ReferencesApoorv Agarwal, Fadi Biadsy, and Kathleen R. Mcke-own.
2009.
Contextual phrase-level polarity anal-ysis using lexical affect scoring and syntactic n-grams.
In Proceedings of the 12th Conference of theEuropean Chapter of the Association for Computa-tional Linguistics, EACL ?09, pages 24?32, Strouds-burg, PA, USA.
Association for Computational Lin-guistics.Apoorv Agarwal, Boyi Xie, Ilia Vovsha, Owen Ram-bow, and Rebecca Passonneau.
2011.
Sentimentanalysis of twitter data.
In Proceedings of the Work-shop on Language in Social Media (LSM 2011),pages 30?38, Portland, Oregon, June.
Associationfor Computational Linguistics.Stefano Baccianella, Andrea Esuli, and Fabrizio Sebas-tiani.
2010.
Sentiwordnet 3.0: An enhanced lexicalresource for sentiment analysis and opinion mining.In Nicoletta Calzolari (Conference Chair), KhalidChoukri, Bente Maegaard, Joseph Mariani, JanOdijk, Stelios Piperidis, Mike Rosner, and DanielTapias, editors, Proceedings of the Seventh Interna-tional Conference on Language Resources and Eval-uation (LREC?10), Valletta, Malta, may.
EuropeanLanguage Resources Association (ELRA).Luciano Barbosa and Junlan Feng.
2010.
Robust sen-timent detection on twitter from biased and noisydata.
In COLING (Posters), pages 36?44.Adam Bermingham and Alan F. Smeaton.
2010.
Clas-sifying sentiment in microblogs: is brevity an advan-tage?
In Jimmy Huang, Nick Koudas, Gareth J. F.Jones, Xindong Wu, Kevyn Collins-Thompson, andAijun An, editors, CIKM, pages 1833?1836.
ACM.Christiane Fellbaum, editor.
1998.
WordNet An Elec-tronic Lexical Database.
The MIT Press, Cam-bridge, MA ; London, May.Alec Go, Richa Bhayani, and Lei Huang.
2009.
Twit-ter sentiment classification using distant supervision.Processing, pages 1?6.Roberto Gonz?alez-Ib?a?nez, Smaranda Muresan, andNina Wacholder.
2011.
Identifying sarcasm intwitter: A closer look.
In Proceedings of the 49thAnnual Meeting of the Association for Computa-tional Linguistics: Human Language Technologies:Short Papers - Volume 2, HLT ?11, pages 581?586,Stroudsburg, PA, USA.
Association for Computa-tional Linguistics.Mark Hall, Eibe Frank, Geoffrey Holmes, BernhardPfahringer, Peter Reutemann, and Ian H. Witten.2009.
The weka data mining software: an update.SIGKDD Explor.
Newsl., 11(1):10?18, November.Preslav Nakov, Sara Rosenthal, Zornitsa Kozareva,Veselin Stoyanov, Alan Ritter, and Theresa Wilson.2013.
Semeval-2013 task 2: Sentiment analysis intwitter.
In Second Joint Conference on Lexical andComputational Semantics (*SEM), Volume 2: Pro-ceedings of the Seventh International Workshop onSemantic Evaluation (SemEval 2013), pages 312?320, Atlanta, Georgia, USA, June.
Association forComputational Linguistics.Alexander Pak and Patrick Paroubek.
2010.
Twitter asa corpus for sentiment analysis and opinion mining.In Nicoletta Calzolari (Conference Chair), KhalidChoukri, Bente Maegaard, Joseph Mariani, JanOdijk, Stelios Piperidis, Mike Rosner, and DanielTapias, editors, Proceedings of the Seventh Interna-tional Conference on Language Resources and Eval-uation (LREC?10), Valletta, Malta, may.
EuropeanLanguage Resources Association (ELRA).Sara Rosenthal and Kathleen McKeown.
2013.Columbia nlp: Sentiment detection of subjectivephrases in social media.
In Second Joint Conferenceon Lexical and Computational Semantics (*SEM),Volume 2: Proceedings of the Seventh InternationalWorkshop on Semantic Evaluation (SemEval 2013),pages 478?482, Atlanta, Georgia, USA, June.
Asso-ciation for Computational Linguistics.Sara Rosenthal, Preslav Nakov, Alan Ritter, andVeselin Stoyanov.
2014.
Semeval-2014 task 9: Sen-timent analysis in twitter.
In Proceedings of the8th International Workshop on Semantic Evaluation(SemEval 2014), Dublin, Ireland, August.
The COL-ING 2014 Organizing Committee.C.
M. Whissel.
1989.
The dictionary of affect in lan-guage.
In R. Plutchik and H. Kellerman, editors,Emotion: theory research and experience, volume 4,London.
Acad.
Press.202
