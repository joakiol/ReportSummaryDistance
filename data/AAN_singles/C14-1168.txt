Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers,pages 1783?1792, Dublin, Ireland, August 23-29 2014.Adapting taggers to Twitter with not-so-distant supervisionBarbara Plank1, Dirk Hovy1, Ryan McDonald2and Anders S?gaard1Center for Language Technology, University of Copenhagen1Google Inc.2{bplank,dirkh}@cst.dk,ryanmcd@google.com,soegaard@hum.ku.dkAbstractWe experiment with using different sources of distant supervision to guide unsupervised andsemi-supervised adaptation of part-of-speech (POS) and named entity taggers (NER) to Twitter.We show that a particularly good source of not-so-distant supervision is linked websites.
Specif-ically, with this source of supervision we are able to improve over the state-of-the-art for TwitterPOS tagging (89.76% accuracy, 8% error reduction) and NER (F1=79.4%, 10% error reduction).1 IntroductionTwitter contains a vast amount of information, including first stories and breaking news (Petrovic et al.,2010), fingerprints of public opinions (Jiang et al., 2011) and recommendations of relevance to poten-tially very small target groups (Benson et al., 2011).
In order to automatically extract this information,we need to be able to analyze tweets, e.g., determine the part-of-speech (POS) of words and recognizenamed entities.
Tweets, however, are notoriously hard to analyze (Foster et al., 2011; Eisenstein, 2013;Baldwin et al., 2013).
The challenges include dealing with variations in spelling, specific conventionsfor commenting and retweeting, frequent use of abbreviations and emoticons, non-standard syntax, frag-mented or mixed language, etc.Gimpel et al.
(2011) showed that we can induce POS tagging models with high accuracy on in-sampleTwitter data with relatively little annotation effort.
Learning taggers for Twitter data from small amountsof labeled data has also been explored by others (Ritter et al., 2011; Owoputi et al., 2013; Derczynskiet al., 2013).
Hovy et al.
(2014), on the other hand, showed that these models overfit their respectivesamples and suffer severe drops when evaluated on out-of-sample Twitter data, sometimes performingeven worse than newswire models.
This may be due to drift on Twitter (Eisenstein, 2013) or simply dueto the heterogeneous nature of Twitter, which makes small samples biased.
So while existing systemsperform well on their own (in-sample) data sets, they over-fit the samples they were induced from, andsuffer on other (out-of-sample) Twitter data sets.
This bias can, at least in theory, be corrected by learningfrom additional unlabeled tweets.
This is the hypothesis we explore in this paper.We present a semi-supervised learning method that does not require additional labeled in-domain datato correct sample bias, but rather leverages pools of unlabeled Twitter data.
However, since taggerstrained on newswire perform poorly on Twitter data, we need additional guidance when utilizing theunlabeled data.
This paper proposes distant supervision to help our models learn from unlabeled data.Distant supervision is a weakly supervised learning paradigm, where a knowledge resource is exploitedto gather (possible noisy) training instances (Mintz et al., 2009).
Our basic idea is to can use linguisticanalysis of linked websites as a novel kind of distant supervision for learning how to analyze tweets.
Weexplore standard sources of distant supervision, such as Wiktionary for POS tagging, but we also proposeto use the linked websites of tweets with URLs as supervision.
The intuition is that we can use websitesto provide a richer linguistic context for our tagging decisions.
We exploit the fact that tweets with URLsprovide a one-to-one map between an unlabeled instance and the source of supervision, making thisThis work is licensed under a Creative Commons Attribution 4.0 International Licence.
Page numbers and proceedings footerare added by the organisers.
Licence details: http://creativecommons.org/licenses/by/4.0/17831: X = {?xi, yi?
}Ni=1labeled tweets2: U = {?xi, wi?
}Mi=1unlabeled tweet-website pairs3: I iterations4: k = 1000 pool size5: v=train(X) base model6: for i ?
I do7: for ?x, w?
?
poolk(U) do8: y?=predict(?x, w?
;v)9: X ?
X ?
{?y?,x?
}10: end for11: v=train(X)12: end for13: return vFigure 1: Semi-supervised learning with not-so-distant supervision, i.e.
tweet-website pairs {?xi, wi?
}.SELF-TRAINING, WEB, DICT, DICT?WEB and WEB?DICT differ only in how predict() (line 8) isimplemented (cf.
Section 2).less distant supervision.
Note that we use linked websites only for semi-supervised learning, but do notrequire them at test time.Our semi-supervised learning method enables us to learn POS tagging and NER models that performmore robustly across different samples of tweets than existing approaches.
We consider both the scenariowhere a small sample of labeled Twitter data is available, and the scenario where only newswire data isavailable.
Training on a mixture of out-of-domain (WSJ) and in-domain (Twitter) data as well as unla-beled data, we get the best reported results in the literature for both POS tagging and NER on Twitter.
Ourtagging models are publicly available at https://bitbucket.org/lowlands/ttagger-nsd2 Tagging with not-so-distant supervisionWe assume that our labeled data is highly biased by domain differences (Jiang and Zhai, 2007), popula-tion drift (Hand, 2006), or by our sample size simply being too small.
To correct this bias, we want to useunlabeled Twitter data.
It is well-known that semi-supervised learning algorithms such as self-trainingsometimes effectively correct model biases (McClosky et al., 2006; Huang et al., 2009).
This paperpresents an augmented self-training algorithm that corrects model bias by exploiting unlabeled data andnot-so-distant supervision.
More specifically, the idea is to use hyperlinks to condition tagging deci-sions in tweets on a richer linguistic context than what is available in the tweets.
This semi-supervisedapproach gives state-of-the-art performance across available Twitter POS and NER data sets.The overall semi-supervised learning algorithm is presented in Figure 1.
The aim is to correct modelbias by predicting tag sequences on small pools of unlabeled tweets, and re-training the model acrossseveral iterations to gradually correct model bias.
Since information from hyperlinks will be important,the unlabeled data U is a corpus of tweets containing URLs.
We present a baseline and four systemproposals that only differ in their treatment of the predict() function.In the SELF-TRAINING baseline, predict() corresponds to standard Viterbi inference on the unlabeledTwitter data.
This means, the current model v is applied to the tweets by disregarding the websites inthe tweet-website pairs, i.e., tagging x without considering w. Then the automatically tagged tweets areadded to the current pool of labeled data and the procedure is iterated (line 7-11 in Figure 1).In the WEB method, we additionally use the information from the websites.
The current model vis used to predict tags for the pooled tweets and the website they linked to.
For all the words thatoccur both in a tweet and on the corresponding website, we then project the tag most frequentlyassigned to those words on the website to their occurrences in the tweet.
This enables us to basicallycondition the tag decision for each such word on its accumulated context on the website.
The assumptionof course being that the word in the tweet has the part-of-speech it most often has on the website linked to.1784Example Here is an example of a tweet that contains a URL:(1) #Localization #job: Supplier / Project Manager - Localisation Vendor - NY, NY, United Stateshttp://bit.ly/16KigBg #nlppeopleThe words in the tweet are all common words, but they occur without linguistic context that couldhelp a tagging model to infer whether these words are nouns, verbs, named entities, etc.
However, on thewebsite that the tweet refers to, all of these words occur in context:(2) The Supplier/Project Manager performs the selection and maintenance .
.
.For illustration, the Urbana-Champaign POS tagger1incorrectly tags Supplier in (1) as an adjective.In (2), however, it gets the same word right and tags it as a noun.
The tagging of (2) could potentiallyhelp us infer that Supplier is also a noun in (1).Obviously, the superimposition of tags in the WEB method may change the tag of a tweet word suchthat it results in an unlikely tag sequence, as we will discuss later.
Therefore we also implementedtype-constrained decoding (T?ackstr?om et al., 2013), i.e., prune the lattice such that the tweet words ob-served on the website have one of the tags they were labeled with on the website (soft constraints), or,alternatively, were forced during decoding to have the most frequent tags they were labeled with (hardconstraint decoding), thereby focusing on licensed sequences.
However, none of these approaches per-formed significantly better than the simple WEB approach on held-out data.
This suggests that sequentialdependencies are less important for tagging Twitter data, which is of rather fragmented nature.
Also, theWEB approach allows us to override transitional probabilities that are biased by the observations wemade about the distribution of tags in our out-of-domain data.Furthermore, we combine the not-so-distant supervision from linked websites (WEB) with supervisionfrom dictionaries (DICT).
The idea here is to exploit the fact that many word types in a dictionary areactually unambiguous, i.e., contain only a single tag.
In particular, 93% of the word types in Wiktionary2are unambiguous.
Wiktionary is a crowdsourced tag dictionary that has previously been used for mini-mally supervised POS tagging (Li et al., 2012; T?ackstr?om et al., 2013).
In the case of NER, we use agazetteer that combines information on PER, LOC and ORG from the KnownLists of the Illinois tagger.3For this gazetteer, 79% of the word types contained only a single named entity tag.We experiment with a model that uses the dictionary only (DICT) and two ways to combine the twosources.
In the former setup, the current model is first applied to tag the tweets, then any token thatappears in the dictionary and is unambiguous is projected back to the tweet.
The next two methods arecombinations of WEB and DICT: either first project the predicted tags from the website and then, in caseof conflicts, overrule predictions by the dictionary (WEB?DICT), or the other way around (DICT?WEB).The intuition behind the idea of using linked websites as not-so-distant supervision is that while tweetsare hard to analyze (even for humans) because of the limited context available in 140 character messages,tweets relate to real-world events, and Twitter users often use hyperlinks to websites to indicate whatreal-world events their comments address.
In fact, we observed that about 20% of tweets contain URLs.The websites they link to are often newswire sites that provide more context and are written in a morecanonical language, and are therefore easier to process.
Our analysis of the websites can then potentiallyinform our analysis of the tweets.
The tweets with the improved analyses can then be used to bootstrapour tagging models using a self-training mechanism.
Note that our method does not require tweets tocontain URLs at test time, but rather uses unlabeled tweets with URLs during training to build bettertagging models for tweets in general.
At test time, these models can be applied to any tweet.1http://cogcomp.cs.illinois.edu/demo/pos/2http://en.wiktionary.org/ - We used the Wiktionary version derived by Li et al.
(2012).3http://cogcomp.cs.illinois.edu/page/software_view/NETagger17853 Experiments3.1 ModelIn our experiments we use a publicly available implementation of conditional random fields (CRF) (Laf-ferty et al., 2001).4We use the features proposed by Gimpel et al.
(2011), in particular features for wordtokens, a set of features that check for the presence of hyphens, digits, single quotes, upper/lowercase,3 character prefix and suffix information.
Moreover, we add Brown word cluster features that use 2ifori ?
1, ..., 4 bitstring prefixes estimated from a large Twitter corpus (Owoputi et al., 2013), which is pub-licly available.5We use a pool size of 1000 tweets.
We experimented with other pool sizes {500,2000}showing similar performance.
The number of iterations i is set on the development data.For NER on websites, we use the Stanford NER system (Finkel et al., 2005)6with POS tags from theLAPOS tagger (Tsuruoka et al., 2011).7For POS we found it to be superior to use the current POS modelfor re-tagging websites; for NER it was slightly better to use the Stanford NER tagger and thus off-lineNER tagging rather than retagging the websites in every iteration.3.2 DataIn our experiments, we consider two scenarios, sometimes referred to as unsupervised and semi-supervised domain adaptation (DA), respectively (Daum?e et al., 2010; Plank, 2011).
In unsupervisedDA, we assume only (labeled) newswire data, in semi-supervised DA, we assume labeled data from bothdomains, besides unlabeled target data, but the amount of labeled target data is much smaller than thelabeled source data.
Most annotated corpora for English are newswire corpora.
Some annotated Twitterdata sets have been made available recently, described next.POS NERtrainWSJ (700k) REUTER-CONLL (Tjong Kim Sang and De Meulder, 2003) (200k)GIMPEL-TRAIN (Owoputi et al., 2013) (14k) FININ-TRAIN (Finin et al., 2010) (170k)devFOSTER-DEV (Foster et al., 2011) (3k) n/aRITTER-DEV (Ritter et al., 2011) (2k) n/atestFOSTER-TEST (Foster et al., 2011) (2.8k) RITTER-TEST (Ritter et al., 2011) (46k)GIMPEL-TEST (Gimpel et al., 2011) (7k) FININ-TEST (Finin et al., 2010) (51k)HOVY-TEST (Hovy et al., 2014) FROMREIDE-TEST (Fromreide et al., 2014) (20k)Table 1: Overview of data sets.
Number in parenthesis: size in number of tokens.Training data.
An overview of the different data sets is given in Table 3.2.
In our experiments, weuse the SANCL shared task8splits of the OntoNotes 4.0 distribution of the WSJ newswire annotationsas newswire training data for POS tagging.9For NER, we use the CoNLL 2003 data sets of annotatednewswire from the Reuters corpus.10The in-domain training POS data comes from Gimpel et al.
(2011),and the in-domain NER data comes from Finin et al.
(2010) (FININ-TRAIN).
These data sets are addedto the newswire sets when doing semi-supervised DA.
Note that for NER, we thus do not rely on expert-annotated Twitter data, but rely on crowdsourced annotations.
We use MACE11(Hovy et al., 2013) toresolve inter-annotator conflicts between turkers (50 iterations, 10 restarts, no confidence threshold).
Webelieve relying on crowdsourced annotations makes our set-up more robust across different samples ofTwitter data.Development and test data.
We use several evaluation sets for both tasks to prevent overfitting to aspecific sample.
We use the (out-of-sample) development data sets from Ritter et al.
(2011) and Foster4http://www.chokkan.org/software/crfsuite/5http://www.ark.cs.cmu.edu/TweetNLP/6http://http://nlp.stanford.edu/software/CRF-NER.shtml7http://www.logos.ic.i.u-tokyo.ac.jp/?tsuruoka/lapos/8https://sites.google.com/site/sancl2012/home/shared-task9LDC2011T03.10http://www.clips.ua.ac.be/conll2003/ner/11http://www.isi.edu/publications/licensed-sw/mace/1786et al.
(2011).
For NER, we simply use the parameters from our POS tagging experiments and thus donot assume to have access to further development data.
For both POS tagging and NER, we have threetest sets.
For POS tagging, the ones used in Foster et al.
(2011) (FOSTER-TEST) and Ritter et al.
(2011)(RITTER-TEST),12as well as the one presented in Hovy et al.
(2014) (HOVY-TEST).
For NER, we usethe data set from Ritter et al.
(2011) and the two data sets from Fromreide et al.
(2014) as test sets.One is a manual correction of a held-out portion of FININ-TRAIN, named FININ-TEST; the other oneis referred to as FROMREIDE-TEST.
Since the different POS corpora use different tag sets, we map allof them corpora onto the universal POS tag set by Petrov et al.
(2012).
The data sets also differ in afew annotation conventions, e.g., some annotate URLs as NOUN, some as X.
Moreover, our newswiretagger baselines tend to get Twitter-specific symbols such as URLs, hashtags and user accounts wrong.Instead of making annotations more consistent across data sets, we follow Ritter et al.
(2011) in using afew post-processing rules to deterministically assign Twitter-specific symbols to their correct tags.
Themajor difference between the NER data sets is whether Twitter user accounts are annotated as PER.
Wefollow Finin et al.
(2010) in doing so.Unlabeled data We downloaded 200k tweet-website pairs from the Twitter search API over a periodof one week in August 2013 by searching for tweets that contain the string http and downloading thecontent of the websites they linked to.
We filter out duplicate tweets and restrict ourselves to websitesthat contain more than one sentence (after removing boilerplate text, scripts, HTML, etc).13We alsorequire website and tweet to have at least one matching word that is not a stopword (as defined by theNLTK stopword list).14Finally we restrict ourselves to pairs where the website is a subsite, becausewebsite head pages tend to contain mixed content that is constantly updated.
The resulting files are alltokenized using the Twokenize tool.15Tweets were treated as one sentence, similar to the approaches inGimpel et al.
(2011) and Owoputi et al.
(2013); websites were processed by applying the Moses sentencesplitter.16The out-of-vocabulary (OOV) rates in Figure 2 show that in-domain training data reduces the numberof unseen words considerably, especially in the NER data sets.
They also suggest that some evaluationdata sets share more vocabulary with our training data than others.
In particular, we would expect betterperformance on FOSTER-TEST than on RITTER-TEST and HOVY-TEST in POS tagging, as well as betterperformance on FININ-TEST than on the other two NER test sets.
In POS tagging, we actually do seebetter results with FOSTER-TEST across the board, but in NER, FININ-TEST actually turns out to be thehardest data set.4 Results4.1 POS resultsBaselines We use three supervised CRF models as baselines (cf.
the first part of Table 2).
The firstsupervised model is trained only on WSJ.
This model does very well on FOSTER-DEV and FOSTER-TEST, presumably because of the low OOV rates (Figure 2).
The second supervised model is trainedonly on GIMPEL-TRAIN; the third on the concatenation of WSJ and GIMPEL-TRAIN.
While the secondbaseline performs well on held-out data from its own sample (90.3% on GIMPEL-DEV), it performspoorly across our out-of-sample test and development sets.
Thus, it seems to overfit the sample oftweets described in Gimpel et al.
(2011).
The third model trained on the concatenation of WSJ andGIMPEL-TRAIN achieves the overall best baseline performance (88.4% macro-average accuracy).
Wenote that this is around one percentage point better than the best available off-the-shelf system for Twitter(Owoputi et al., 2013) with an average accuracy of 87.5%.12Actually (Ritter et al., 2011) do cross-validation over this data, but we use the splits of Derczynski et al.
(2013) for POS.13Using https://github.com/miso-belica/jusText14ftp://ftp.cs.cornell.edu/pub/smart/english.stop15https://github.com/brendano/ark-tweet-nlp16https://github.com/moses-smt/mosesdecoder/blob/master/scripts/ems/support/split-sentences.perl1787Figure 2: Test set (type-level) OOV rates for POS (left) and NER (right).l lll l ll l l l l l ll l l l l l l l l l l l l l l l l l l l l l0 5 10 15 20 25 3088.589.089.590.0DEV?avg wsjiterationaccuracyl self?trainingWebDictWeb<DictDict<Webl ll l ll l l l l l ll l l l l l l l l l l ll ll l l l0 5 10 15 20 25 3088.889.089.289.489.689.890.090.2DEV?avg wsj+gimpeliterationaccuracyFigure 3: Learning curves on DEV-avg for systems trained on WSJ (left) and WSJ+GIMPEL (right) usedto set the hyperparameter i.Learning with URLs The results of our approaches are presented in Table 2.
The hyperparameter iwas set on the development data (cf.
Figure 3).
Note, again, that they do not require the test data tocontain URLs.
First of all, naive self-training does not work: accuracy declines or is just around baselineperformance (Table 2 and Figure 3).
In contrast, our augmented self-training methods with WEB orDICT reach large improvements.
In case we assume no target training data (train on WSJ only, i.e.unsupervised DA), we obtain improvements of up to 9.1% error reduction.
Overall the system improvesfrom 88.42% to 89.07%.
This also holds for the second scenario, i.e.
training on WSJ+GIMPEL-TRAIN(semi-supervised DA, i.e., the case where we have some labeled target data, besides the pool of unlabeledtweets) where we reach error reductions of up to 10%.
Our technique, in other words, improves therobustness of taggers, leading to much better performance on new samples of tweets.4.2 NER resultsFor our NER results, cf.
Table 3, we used the same feature models and parameter settings as those used forPOS tagging, except conditioning also on POS information.
It is conceivable that other parameter settingswould have led to better results, but we did not want to assume the existence of in-domain developmentdata for this task.
Our baselines are again supervised systems, as well as off-the-shelf systems.
Our in-1788DEV-avg TEST TEST-avgFOSTER HOVY RITTERBaselines trained onWSJ 88.82 91.87 87.01 86.38 88.42GIMPEL-TRAIN 83.32 84.86 86.03 81.67 84.19WSJ+GIMPEL-TRAIN 89.07 91.59 87.50 87.39 88.83Systems trained on WSJSELF-TRAINING i = 25 85.52 91.80 86.72 85.90 88.14DICT i = 25 85.61 92.08 87.63 85.68 88.46WEB i = 25 85.27 92.47 87.30 86.60 88.79DICT?WEB i = 25 86.11 92.61 87.70 86.69 89.00WEB?DICT i = 25 86.15 92.57 88.12 86.51 89.07max err.red 4.7% 9.1% 8.6% 2.3% 4.2%Systems trained on WSJ+GIMPEL-TRAINSELF-TRAINING i = 27 89.12 91.83 86.88 87.43 88.71DICT i = 27 89.43 92.22 88.38 87.69 89.43WEB i = 27 89.82 92.43 87.43 88.21 89.36DICT?WEB i = 27 90.04 92.43 88.38 88.48 89.76WEB?DICT i = 27 90.04 92.40 87.99 88.39 89.59max err.red 8.9% 10% 7.1% 8.6% 8.4%Table 2: POS results.house supervised baselines perform better than the available off-the-shelf systems, including the systemprovided by Ritter et al.
(2011) (TEST-avg of 54.2%).
We report micro-average F1-scores over entitytypes, computed using the publicly available evaluation script.17Our approaches again lead to substantialerror reductions of 8?13% across our NER evaluation data sets.TEST TEST-avgRITTER FROMREIDE FININBaseline trained onCONLL+FININ-TRAIN 77.44 82.13 74.02 77.86Systems trained on CONLL+FININ-TRAINSELF-TRAINING i = 27 78.63 82.88 74.89 78.80DICT i = 27 65.24 69.1 65.45 66.60WEB i = 27 78.29 83.82 74.99 79.03DICT?WEB i = 27 78.53 83.91 75.83 79.42WEB?DICT i = 27 65.97 69.92 65.86 67.25err.red 9.1% 13.3% 8.0% 9.8%Table 3: NER results.5 Error analysisThe majority of cases where our taggers improve on the ARK tagger (Owoputi et al., 2013) seem torelate to richer linguistic context.
The ARK tagger incorrectly tags the sequence Man Utd as PRT-NOUN, whereas our taggers correctly predict NOUN-NOUN.
In a similar vein, our taggers correctlypredict the tag sequence NOUN-NOUN for Radio Edit, while the ARK tagger predicts NOUN-VERB.However, some differences seem arbitrary.
For example, the ARK tagger tags the sequence Nokia17http://www.cnts.ua.ac.be/conll2000/chunking/1789D5000 in FOSTER-TEST as NOUN-NUM.
Our systems correctly predict NOUN-NOUN, but it is notclear which analysis is better in linguistic terms.
Our systems predict a sequence such as Love his versionto be VERB-PRON-NOUN, whereas the ARK tagger predicts VERB-DET-NOUN.
Both choices seemlinguistically motivated.Finally, some errors are made by all systems.
For example, the word please in please, do that, forexample, is tagged as VERB by all systems.
In FOSTER-TEST, this is annotated as X (which in the PTBstyle was tagged as interjection UH).
Obviously, please often acts as a verb, and while its part-of-speechin this case may be debatable, we see please annotated as a verb in similar contexts in the PTB, e.g.
:(3) Please/VERB make/VERB me/PRON .
.
.It is interesting to look at the tags that are projected from the websites to the tweets.
Several of theobserved projections support the intuition that coupling tweets and the websites they link to enables usto condition our tagging decisions on a richer linguistic context.
Consider, for example Salmon-Safe,initially predicted to be a NOUN, but after projection correctly analyzed as an ADJ:Word Context Initial tag Projected tagSalmon-Safe .
.
.
parks NOUN ADJSnohomish .
.
.
Bakery ADJ NOUNtoxic ppl r .
.
.
NOUN ADJOne of the most frequent projections is analyzing you?re, correctly, as a VERB rather than an ADV (ifthe string is not split by tokenization).One obvious limitation of the WEB-based models is that the projections apply to all occurrences of aword.
In rare cases, some words occur with different parts of speech in a single tweet, e.g., wish in:(4) If I gave you one wish that will become true .
What?s your wish ?...
?
i wish i?ll get <num> wishesfrom you :p <url>In this case, our models enforce all occurrences of wish to, incorrectly, be verbs.6 Related workPrevious work on tagging tweets has assumed labeled training data (Ritter et al., 2011; Gimpel et al.,2011; Owoputi et al., 2013; Derczynski et al., 2013).
Strictly supervised approaches to analyzing Twitterhas the weakness that labeled data quickly becomes unrepresentative of what people write on Twitter.This paper presents results using no in-domain labeled data that are significantly better than several off-the-shelf systems, as well as results leveraging a mixture of out-of-domain and in-domain labeled datato reach new highs across several data sets.Type-constrained POS tagging using tag dictionaries has been explored in weakly supervised settings(Li et al., 2012), as well as for cross-language learning (Das and Petrov, 2011; T?ackstr?om et al., 2013).Our type constraints in POS tagging come from tag dictionaries, but also from linked websites.
The ideaof using linked websites as distant supervision is similar in spirit to the idea presented in Ganchev etal.
(2012) for search query tagging.Ganchev et al.
(2012), considering the problem of POS tagging search queries, tag search queries andthe associated snippets provided by the search engine, projecting tags from the snippets to the queries,guided by click-through data.
They do not incorporate tag dictionaries, but consider a slightly moreadvanced matching of snippets and search queries, giving priority to n-gram matches with larger n.Search queries contain limited contexts, like tweets, but are generally much shorter and exhibit lessspelling variation than tweets.In NER, it is common to use gazetteers, but also dictionaries as distant supervision (Kazama andTorisawa, 2007; Cucerzan, 2007).
R?ud et al.
(2011) consider using search engines for distant supervisionof NER of search queries.
Their set-up is very similar to Ganchev et al.
(2012), except they do not useclick-through data.
They use the search engine snippets to generate feature representations rather thanprojections.
Want et al.
(2013) also use distant supervision for NER, i.e., Wikipedia page view counts,1790applying their model to Twitter data, but their results are considerably below the state of the art.
Also,their source of supervision is not linked to the individual tweets in the way mentioned websites are.In sum, our method is the first successful application of distant supervision to POS tagging and NERfor Twitter.
Moreover, it is, to the best of our knowledge, the first paper that addresses both problemsusing the same technique.
Finally, our results are significantly better than state-of-the-art results in bothPOS tagging and NER.7 ConclusionWe presented a semi-supervised approach to POS tagging and NER for Twitter data that uses dictionariesand linked websites as a source of not-so-distant (or linked) supervision to guide the bootstrapping.
Ourapproach outperforms off-the-shelf taggers when evaluated across various data sets, achieving averageerror reductions across data sets of 5% on POS tagging and 10% on NER over state-of-the-art baselines.ReferencesTimothy Baldwin, Paul Cook, Marco Lui, Andrew MacKinlay, and Li Wang.
2013.
How noisy social media text,how diffrnt social media sources?
In IJCNLP.Edward Benson, Aria Haghighi, and Regina Barzilay.
2011.
Event discovery in social media feeds.
In ACL.Silvia Cucerzan.
2007.
Large-scale named entity disambiguation based on wikipedia data.
In EMNLP-CoNLL.Dipanjan Das and Slav Petrov.
2011.
Unsupervised part-of-speech tagging with bilingual graph-based projections.In ACL.Hal Daum?e, Abhishek Kumar, and Avishek Saha.
2010.
Frustratingly easy semi-supervised domain adaptation.In ACL Workshop on Domain Adaptation for NLP.Leon Derczynski, Alan Ritter, Sam Clark, and Kalina Bontcheva.
2013.
Twitter part-of-speech tagging for all:overcoming sparse and noisy data.
In RANLP.Jacob Eisenstein.
2013.
What to do about bad language on the internet.
In NAACL.Tim Finin, Will Murnane, Anand Karandikar, Nicholas Keller, Justin Martineau, and Mark Dredze.
2010.
Anno-tating named entities in Twitter data with crowdsourcing.
In NAACL-HLT 2010 Workshop on Creating Speechand Language Data with Amazon?s Mechanical Turk.Jenny Finkel, Trond Grenager, and Christopher Manning.
2005.
Incorporating non-local information into infor-mation extraction systems by Gibbs sampling.
In ACL.Jennifer Foster, Ozlem Cetinoglu, Joachim Wagner, Josef Le Roux, Joakim Nivre, Deirde Hogan, and Josef vanGenabith.
2011.
From news to comments: Resources and benchmarks for parsing the language of Web 2.0.
InIJCNLP.Hege Fromreide, Dirk Hovy, and Anders S?gaard.
2014.
Crowdsourcing and annotating ner for twitter #drift.
InLREC.Kuzman Ganchev, Keith Hall, Ryan McDonald, and Slav Petrov.
2012.
Using search-logs to improve querytagging.
In ACL.Kevin Gimpel, Nathan Schneider, Brendan O?Connor, Dipanjan Das, Daniel Mills, Jacob Eisenstein, MichaelHeilman, Dani Yogatama, Jeffrey Flanigan, and Noah A. Smith.
2011.
Part-of-Speech Tagging for Twitter:Annotation, Features, and Experiments.
In ACL.David Hand.
2006.
Classifier technology and illusion of progress.
Statistical Science, 21(1):1?15.Dirk Hovy, Taylor Berg-Kirkpatrick, Ashish Vaswani, and Eduard Hovy.
2013.
Learning whom to trust withMACE.
In NAACL.Dirk Hovy, Barbara Plank, and Anders S?gaard.
2014.
When pos datasets don?t add up: Combatting sample bias.In LREC.1791Zhongqiang Huang, Mary Harper, and Slav Petrov.
2009.
Self-training with products of latent variable grammars.In EMNLP.Jing Jiang and ChengXiang Zhai.
2007.
Instance weighting for domain adaptation in NLP.
In ACL.Long Jiang, Mo Yo, Ming Zhou, Xiaohua Liu, and Tiejun Zhao.
2011.
Target-dependent Twitter sentimentclassification.
In ACL.Jun?ichi Kazama and Kentaro Torisawa.
2007.
Exploiting wikipedia as external knowledge for named entityrecognition.
In EMNLP-CoNLL.John Lafferty, Andrew McCallum, and Fernando Pereira.
2001.
Conditional random fields: probabilistic modelsfor segmenting and labeling sequence data.
In ICML.Shen Li, Jo?ao Grac?a, and Ben Taskar.
2012.
Wiki-ly supervised part-of-speech tagging.
In EMNLP.David McClosky, Eugene Charniak, and Mark Johnson.
2006.
Effective self-training for parsing.
In HLT-NAACL.Mike Mintz, Steven Bills, Rion Snow, and Dan Jurafsky.
2009.
Distant supervision for relation extraction withoutlabeled data.
In ACL.Olutobi Owoputi, Brendan O?Connor, Chris Dyer, Kevin Gimpel, Nathan Schneider, and Noah A Smith.
2013.Improved part-of-speech tagging for online conversational text with word clusters.
In NAACL.Slav Petrov, Dipanjan Das, and Ryan McDonald.
2012.
A universal part-of-speech tagset.
In LREC.Sasa Petrovic, Miles Osborne, and Victor Lavrenko.
2010.
Streaming first story detection with application toTwitter.
In NAACL.Barbara Plank.
2011.
Domain Adaptation for Parsing.
Ph.D. thesis, University of Groningen.Alan Ritter, Sam Clark, Oren Etzioni, et al.
2011.
Named entity recognition in tweets: an experimental study.
InEMNLP.Stefan R?ud, Massimiliano Ciaramita, Jens M?uller, and Hinrich Sch?utze.
2011.
Piggyback: Using search enginesfor robust cross-domain named entity recognition.
In ACL.Oscar T?ackstr?om, Dipanjan Das, Slav Petrov, Ryan McDonald, and Joakim Nivre.
2013.
Token and type con-straints for cross-lingual part-of-speech tagging.
TACL, 1:1?12.Erik F Tjong Kim Sang and Fien De Meulder.
2003.
Introduction to the conll-2003 shared task: Language-independent named entity recognition.
In In CoNLL.Yoshimasa Tsuruoka, Yusuke Miyao, and Jun?ichi Kazama.
2011.
Learning with lookahead: can history-basedmodels rival globally optimized models?
In CoNLL.Chun-Kai Wang, Bo-June Hsu, Ming-Wei Chang, and Emre Kiciman.
2013.
Simple and knowledge-intensivegenerative model for named entity recognition.
Technical report, Microsoft Research.1792
