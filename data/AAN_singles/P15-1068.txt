Proceedings of the 53rd Annual Meeting of the Association for Computational Linguisticsand the 7th International Joint Conference on Natural Language Processing, pages 697?707,Beijing, China, July 26-31, 2015.c?2015 Association for Computational LinguisticsHow Far are We from Fully Automatic High QualityGrammatical Error Correction?Christopher BryantDepartment of Computer ScienceNational University of Singapore13 Computing DriveSingapore 117417bryant@comp.nus.edu.sgHwee Tou NgDepartment of Computer ScienceNational University of Singapore13 Computing DriveSingapore 117417nght@comp.nus.edu.sgAbstractIn this paper, we first explore the roleof inter-annotator agreement statistics ingrammatical error correction and concludethat they are less informative in fieldswhere there may be more than one correctanswer.
We next created a dataset of 50student essays, each corrected by 10 dif-ferent annotators for all error types, and in-vestigated how both human and GEC sys-tem scores vary when different combina-tions of these annotations are used as thegold standard.
Upon learning that even hu-mans are unable to score higher than 75%F0.5, we propose a new metric based onthe ratio between human and system per-formance.
We also use this method to in-vestigate the extent to which annotatorsagree on certain error categories, and findthat similar results can be obtained from asmaller subset of just 10 essays.1 IntroductionInterest in grammatical error correction (GEC)systems has grown considerably in the past fewyears, thanks mainly to the success of the recentHelping Our Own (HOO) (Dale and Kilgarriff,2011; Dale et al, 2012) and Conference on Natu-ral Language Learning (CoNLL) (Ng et al, 2013;Ng et al, 2014) shared tasks.
Despite this increas-ing attention, however, one of the most significantchallenges facing GEC today is the lack of a robustevaluation practice.
In fact Chodorow et al (2012)even go as far to say that it is sometimes ?hardto draw meaningful comparisons between differ-ent approaches, even when they are evaluated onthe same corpus.
?One of the reasons for this is that, tradition-ally, system performance has only ever been eval-uated against the gold standard annotations of asingle native speaker (rarely, two native speakers).As such, system output is not actually scored onthe basis of grammatical acceptability alone, butrather is also constrained by the idiosyncrasies ofthe particular annotators.The obvious solution to this problem would beto compare systems against the gold standard an-notations of multiple annotators, in an effort to di-lute the effect of individual annotator bias, how-ever creating manual annotations is often consid-ered too time consuming and expensive.
In spite ofthis, while other studies have instead elected to usecrowdsourcing to produce multiply-corrected an-notations, often concerning only a limited numberof error types (Madnani et al, 2011; Pavlick et al,2014; Tetreault et al, 2014), one of the main con-tributions of this paper is the provision of a datasetof 10 human expert annotations, annotated in thetradition of CoNLL-2014, that is moreover anno-tated for all error types.1With this new dataset, we have, for the firsttime, been able to compare system output againstthe gold standard annotations of a larger group ofhuman annotators, in a realistic grammar check-ing scenario, and consequently been able to quan-tify the extent to which additional annotators af-fect system performance.
Additionally, we alsonoticed that some annotators tend to agree on cer-tain error categories more than others and so at-tempt to explain this.In light of the results, we also explore how hu-man annotators themselves compare against thecombined annotations of the remaining annotatorsand thus calculate an upper bound F0.5score forthe given dataset and number of annotators; e.g., ifone human versus nine other humans is only ableto score a maximum of 70% F0.5, then it is unrea-sonable to expect a machine to do better.
For thisreason, we propose a more informative method of1http://www.comp.nus.edu.sg/?nlp/sw/10gec_annotations.zip697evaluating a system based on the ratio of that sys-tem?s F0.5score against the equivalent human F0.5score.Section 2 contains an overview of some of thelatest research in both GEC and SMT that makesuse of IAA statistics.
Section 3 shows an examplesentence from our dataset and qualitatively anal-yses how individual annotator bias affects theirchoice of corrections.
Section 4 describes the datacollection process and presents some preliminaryresults.
Section 5 discusses the main quantitativeresults of the paper, formalizing the formulas usedand introducing the more informative method ofratio scoring for GEC, while Section 6 summa-rizes the results from our additional experimentson category agreement and essay subsets.
Section7 concludes the paper.2 Inter-Annotator Agreement (IAA)Whenever we discuss multiple annotators, re-searchers invariably raise the issue of inter-annotator agreement (IAA), or rather the extent towhich annotators agree with each other.
This isbecause data which shows a higher level of agree-ment is often believed to be in some way more reli-able than data which has a lower agreement score.Within GEC, agreement has often been reported interms of Cohen?s-?
(Cohen, 1960), although otheragreement statistics could also be used.2In the rest of this section, however, we wish tochallenge the use of IAA statistics in GEC andquestion their value in this field.
Specifically,while IAA statistics may be informative in areaswhere items can be classified into single, well-defined categories, such as in part-of-speech tag-ging, we argue that they are less well-suited toGEC and SMT, where there is often more than onecorrect answer.
For example, two annotators maycorrect or translate a given sentence in two com-pletely different yet valid ways, but IAA statisticsare only able to interpret the alternative answers asdisagreements.2.1 Inter-Annotator Agreement in GECOne important study that made use of ?
as a mea-sure of agreement between raters is by Tetrault andChodorow (2008) (also in Tetreault et al (2014)),who asked two native English speakers to inserta missing preposition into 200 randomly chosen,2See Hayes and Krippendorff (2007) or Artstein and Poe-sio (2008) for the pros and cons of different IAA metrics.well-formed sentences from which a single prepo-sition had been removed.Despite the simplicity of this correction task,the authors reported ?-agreement of just 0.7, not-ing that in cases where the raters disagreed, theirdisagreements were often ?licensed by context?and thus actually ?acceptable alternatives?.
Thisled them to conclude that they would ?expect evenmore disagreement when the task is preposition er-ror detection in ?noisy?
learner texts?
and, by ex-tension, imply that detection of all error types in?noisy?
texts would show more disagreement still.The most important question to ask then, as aresult of this study, is whether low ?-scores in?noisy?
texts are truly indicative of real disagree-ment, or whether, as in this preposition test, thedisagreement is actually the result of multiple cor-rect answers, and therefore not disagreement at all.In a related study, and aware of the fact thatthere are often multiple ways to correct individualwords in sentence, Rozovskaya and Roth (2010)instead chose to compute agreement at the sen-tence level.
Specifically, three raters were askedsimply to decide whether they thought 200 sen-tences were correct or not.This time, despite operating at the more gen-eral sentence level, the authors reported ?
scoresof just 0.16, 0.4 and 0.23, surmising that ?the lownumbers reflect the difficulty of the task and thevariability of the native speakers?
judgments aboutacceptable usage.?
If that is the case, then true dis-agreement may be indistinguishable from nativevariability, and we should be wary of using IAAstatistics as a measure of agreement or evaluationin GEC.2.2 Inter-Annotator Agreement in SMTIn fact, the issues regarding the reliability of IAAmetrics are not unique to GEC and we can alsodraw a parallel with the field of statistical machinetranslation (SMT).
In the same way that there isoften more than one way to correct a sentence inGEC, it is also well known that there is often morethan one way to translate a sentence in SMT.Nevertheless, while several papers have suc-cessfully discussed ways to minimize annotatorbias effects in SMT (Snover et al, 2006; Madnaniet al, 2008), IAA metrics such as ?
still unhelp-fully play a role in the field and have, for exam-ple, been reported almost every year in the Work-shop on Machine Translation (WMT) conference.698Source:To put it in the nutshell, I believe that people should have the obligation to tell their relatives about thegenetic testing result for the good of their health.A1To put it in a nutshell, I believe that people should be obliged to tell their relatives about their genetic testresults for the good of their health.A2In a nutshell, I believe that people should have an obligation to tell their relatives about the genetic testingresult for the good of their health.A3In summary, I believe that people should have the obligation to tell their relatives about the genetic testingresult for the good of their health.A4In a nutshell, I believe that people should be obligated to tell their relatives about the genetic testing result forthe good of their health.A5To put it in a nutshell, I believe that people should be obligated to tell their relatives about the genetic testingresults for the good of their health.A6To put it in the nutshell, I believe that people should have an obligation to tell their relatives about their genetictest results for the good of their health.A7To put it in a nutshell, I believe that people should have the obligation to tell their relatives about the genetictesting result for the good of their health.A8To put it in a nutshell, I believe that people should be obligated to tell their relatives about the genetic testingresult for the good of their health.A9To put it in a nutshell, I believe that people should have the obligation to tell their relatives about the genetictest result for the good of their health.A10To put it in a nutshell, I believe that people should have the obligation to tell their relatives about the genetictest results for the good of their health.Table 1: Table showing how each of the 10 annotators edited the same source sentence in Essay 25.
Thewords in the source sentence that were changed are highlighted in bold.This is in spite of the fact that the average inter-annotator ?
score across all language pairs overthe past five years has never been higher than 0.4(Bojar et al, 2014).One important paper that attempts to explainwhy IAA metrics score so poorly in SMT is byLommel et al (2014), who asked annotators tohighlight and categorize sections of automaticallytranslated text they believed to be erroneous.
Theirresults showed that while annotators were oftenable to agree on the rough locations of errors, theyoften disagreed as to the specific boundaries ofthose errors: for instance, given the phrase ?hadgo?, some annotators considered just the partici-ple ?go??
?gone?
to be the minimal error, whileothers considered the whole verbal unit, ?had go??
?had gone?, to be the minimal error.
Simi-larly, the authors also noted that annotators some-times had problems categorizing ambiguous errorswhich could be classified into more than one errorcategory.In short, while annotators already vary as towhat they consider an error, these observationsshow that even when they do apparently agree,there is no guarantee that every annotator will de-fine the error in exactly the same terms.
This posesa problem for IAA statistics, which rely on an ex-act match to measure agreement.Finally, it is also worth mentioning that a relatedstudy, by Denkowski and Lavie (2010), suggestedthat ?annotators also have difficulty agreeing withthemselves?
(shown from intra-annotator agree-ment ?
scores of about 0.6), and so we should beespecially wary of using IAA metrics to validatedatasets that may even be unreliable for a singleannotator.3 Annotator BiasIn an effort to better understand how annotators?judgments might differ, we first carried out asmall-scale qualitative analysis on a handful ofrandom sentences corrected by the 10 human an-notators in our dataset.
One such sentence, and allits various corrections, is shown in Table 1.It is interesting to note that, for even as short anidiom as ?To put it in the nutshell?, there are stillmultiple alternative edits.
Although 8 out of the 10annotators elected to replace the article ?the?
with?a?, among them, A2 and A4 also deleted ?To putit?
from the expression.
Of the remaining 2 an-notators, A3 chose to replace the idiom entirelywith ?In summary?, while A6 made no correctionat all.
Although no correction appears to be un-acceptable to the majority of annotators, it is alsonot completely ungrammatical (just idiomaticallyawkward) so it may be that A6 has a higher tol-erance for this kind of error than the other anno-tators.
Alternatively, there is also always the pos-sibility that, given such a large amount of text tocorrect, this error was simply overlooked.Another noteworthy difference is that annota-tors A1, A4, A5, and A8 all elected to change the699verb ?have the obligation?
from active to passive,although A1 still disagreed with the others on theform of the participle.
Similarly, there is also agreat difference of opinion on whether ?testing re-sult?
should be corrected or not, and if so, how.While half of the annotators left the phrase un-changed, A1, A6, and A10 all changed both wordsto ?test results?.
Meanwhile, somewhere in be-tween, A5 decided to change ?result?
to ?results?,but not ?testing?
to ?test?, while, conversely, A9decided to do the opposite.
This would suggestthat error correction of even minor phrases fallsalong a continuum governed by each annotator?snatural bias.Finally, one of the most important results ofthis qualitative evaluation is that even though all10 annotators edited the same sentence to a levelthey deemed grammatical, not one single annota-tor agreed with another exactly.
This fact alonesuggests IAA statistics are not a good way to eval-uate GEC data and that a more robust agreementmetric must take into account the possibility of al-ternative correct answers.4 Data CollectionThe raw text data in our dataset was originally pro-duced by 25 students at the National University ofSingapore (NUS) who were non-native speakersof English.
They were asked to write two essayson the topics of genetic testing and social mediarespectively.
All essays were of similar length andquality.
This was important because varying theskill level of the essays is likely to further affectthe natural bias of the annotators, who may thenconsistently over- or under-correct essays.
Theseraw essays also formed the basis of the CoNLL-2014 test data (Ng et al, 2014).
See Table 2 forsome basic statistics on the resulting 50 essays.The 10 annotators who annotated all 50 essaysinclude: the 2 official annotators of CoNLL-2014,the first author of this paper, and 7 freelancerswho were recruited via online recruitment website,Elance.3All annotators are native British Englishspeakers, many of whom also have backgrounds inEnglish language teaching, proofreading, and/orLinguistics.All annotations were made using an online an-notation platform, WAMP, especially designed forannotating ESL errors (Dahlmeier et al, 2013).Using this platform, annotators were asked to3http://www.elance.comTotal Average per essay# Paragraphs 252 5.0# Sentences 1312 26.2# Tokens 30144 602.9Table 2: Statistics for the 50 unannotated essays.highlight a minimal error string in the source text,provide an appropriate correction, and then cate-gorize their selection according to the same 28-category error framework used by CoNLL-2014.Before commencing annotation, however, eachannotator was given detailed instructions on howto use the tool, along with an explanation of eachof the error categories.
In cases of uncertainty, an-notators were also encouraged to ask questions.As it was slightly harder to control the qual-ity of the 7 independently recruited annotators viaElance, they were each preliminarily asked to an-notate only the first two essays before being givendetailed feedback on their work.
The main pur-pose of this feedback was to make sure that theya) understood the error category framework, andb) knew how to deal with more complicated casessuch as word insertions, punctuation, etc.
Unlessit was felt that they had overlooked an obvious er-ror in these first two essays, the feedback did notgo so far as to tell annotators what they should andshould not highlight in an effort to preserve indi-vidual annotator bias.In all, while the specific time taken to completeannotation of all 50 essays was not calculated, allannotators completed the task over a period ofabout 3 weeks, at a rate of about 45 minutes peressay.4.1 Early ObservationsTo investigate the extent to which different anno-tators have different biases, we first counted thetotal number of edits made by each annotator andsorted them by error category (Table 3).As can be seen, there is quite a difference be-tween the annotator who made the most edits (A1)and the annotator who made the fewest edits (A7),with A1 making more than twice the number ofedits as A7.
This just goes to show how variedjudgments on grammaticality can be.
Incidentally,annotators A3 and A7, who are among those whomade the fewest edits, were also the two officialgold standard annotators in CoNLL-2014.There is also a large difference between edits in700Category A1 A2 A3 A4 A5 A6 A7 A8 A9 A10 TotalArtOrDet 879 639 443 503 665 620 331 358 390 624 5452Cit 0 0 0 0 0 1 0 2 0 0 3Mec 227 376 493 325 411 336 228 733 598 780 4507Nn 404 290 228 264 360 300 215 254 277 365 2957Npos 21 21 15 21 31 28 19 25 29 23 233Others 42 186 49 116 95 43 44 34 125 105 839Pform 431 52 18 57 30 83 47 53 19 18 808Pref 4 79 153 18 223 53 96 92 250 180 1148Prep 755 488 390 421 502 556 211 276 362 459 4420Rloc?
488 308 199 331 187 244 94 174 296 240 2561Sfrag 1 5 1 3 1 5 13 2 12 2 45Smod 1 4 5 0 1 0 0 3 1 1 16Spar 0 18 24 0 2 11 3 2 8 0 68Srun 157 38 21 16 17 18 7 15 17 37 343Ssub 74 54 10 4 25 81 68 21 18 82 437SVA 162 123 154 95 140 114 105 132 144 144 1313Trans 248 100 78 147 118 81 93 199 87 95 1246Um 5 12 42 25 25 12 12 19 7 8 167V0 137 35 37 50 81 69 31 58 51 85 634Vform 388 168 91 100 156 125 132 78 122 124 1484Vm 71 48 37 67 119 24 49 39 4 62 520Vt 100 209 150 200 82 237 133 234 117 188 1650Wa 0 1 1 3 1 1 0 2 4 2 15Wci 623 476 479 446 456 595 340 250 212 346 4223Wform 126 107 103 150 136 145 77 103 107 81 1135WOadv 23 48 27 23 61 76 12 94 41 62 467WOinc 187 67 54 78 53 74 22 24 87 103 749Wtone 6 30 15 65 38 27 9 10 12 15 227Total 5560 3982 3317 3528 4016 3959 2391 3286 3397 4231 37667Table 3: Table showing how many annotations each annotator made in terms of error category.
See Nget al (2014) Table 1 for a more detailed description of error categories.terms of category use, with almost half of all editsfalling into the categories for article or determiner(ArtOrDet), spelling or punctuation (Mec), prepo-sition (Prep), or word choice (Wci) errors.5 Quantitative AnalysisIn the main phase of experimentation, we first in-vestigated how different numbers of annotators af-fected the performance of various systems in thecontext of the CoNLL-2014 shared task.
To dothis, we downloaded the official system outputof all the participating teams4and then the Max-Match (M2) Scorer5(Dahlmeier and Ng, 2012),which was the official scorer of the previousCoNLL-2013 and CoNLL-2014 shared tasks.This scorer evaluates a system at the sentencelevel in terms of correct edits, proposed edits,and gold edits, and uses these to calculate anF-score for each team.
When more than oneset of gold standard annotations is available, thescorer will calculate F-scores for each alternative4http://www.comp.nus.edu.sg/?nlp/conll14st/official_submissions.tar.gz5http://www.comp.nus.edu.sg/?nlp/sw/m2scorer.tar.gzgold-standard sentence and choose the one fromwhichever annotator scored the highest.
As inCoNLL-2014, we calculate F0.5, which weightsprecision twice as much as recall, because it ismore important for a system to be accurate than tocorrect every possible error.
See (Ng et al, 2014)for more details on how F0.5is calculated.5.1 Pairwise EvaluationIn order to quantify how much the F-score canvary in a realistic grammar checking scenariowhen there is only one gold standard annotator, wefirst computed the scores for a participating sys-tem vs each annotator in a pairwise fashion.
Table4 hence shows how the top team in CoNLL-2014,CAMB (Felice et al, 2014), performed againsteach of the 10 human annotators individually.While Tetrault and Chodorow (2008) andTetreault et al (2014) reported a difference of 10%precision and 5% recall between their two individ-ual annotators in their simplified preposition cor-rection task, Table 4 shows this difference can ac-tually be as much as almost 15% precision (A1 vsA7) and 6% recall (A1 vs A3) in a more realisticfull scale correction task.
This equates to a differ-701CAMB P R F0.5A1 39.64 14.06 29.06A2 35.73 17.35 29.48A3 35.22 20.29 30.70A4 32.69 17.88 28.04A5 35.74 17.26 29.43A6 35.76 17.73 29.72A7 24.96 19.62 23.67A8 29.17 16.92 25.48A9 32.03 18.28 27.84A10 35.52 16.26 28.72Table 4: Table showing the F0.5scores for the topteam in CoNLL-2014, CAMB, against each of the10 annotators individually.ence of over 7% F0.5(A3 vs A7) and once againshows how varied annotator?s judgments can be.5.2 All Combinations5.2.1 Human vs HumanWhereas previously we could only calculate F0.5scores on a system vs human basis, when thereare two or more annotators, we can also calculatescores on a human vs human basis.
In fact, as thenumber of annotators increases, we can also startto calculate scores against different combinationsof gold standard annotations.6To give an example, since we have 10 annota-tors, a subset of these annotators, say annotatorsa2?a8, could be chosen as the gold standard anno-tations.
We could then evaluate how each of the re-maining annotators (i.e., annotator a1, a9, and a10)performs against this gold standard, by comput-ing the M2 score for annotator a1 against annota-tors a2?a8, annotator a9 against annotators a2?a8,and annotator a10 against annotators a2?a8.
Wethen average these 3 M2 scores, to determine how,on average, an annotator performs when measuredagainst gold standard annotators a2?a8.It is worth reiterating, however, that when morethan one annotator is used as the gold standard,the M2 scorer will choose whichever annotator forthe given sentence produces the highest F-score;i.e., if a2?a8 are the gold standard and we wantto compute the F-score for a9, the M2 scorer willcompute a9 vs a2, a9 vs a3, .
.
.
, a9 vs a8 separatelyfor each sentence, and choose the highest.6Note that by combinations of annotators, we mean sim-ply that the M2 scorer has access to a larger number of alter-native gold standard corrections; we do not attempt to mergeannotations in any way.The above calculations can be formalized asEquation 1:g(X) =1|A| ?
|X|?a?A\Xf(a,X) (1)where A is the set of all annotators (|A| = 10 inour case) and X is a non-empty and proper subsetof A, denoting the set of annotators chosen to bein the gold standard.
The function f(a,X) is thescore computed by the M2 scorer to evaluate anno-tator a against each set of gold standard annotatorsX .
g(X) is thus the average M2 scores for the re-maining annotators against the input gold standardcombination X .So far, in our example, we have chosen anno-tators a2?a8 to be the gold standard.
There are,however, many other different ways of choosing 7annotators to serve as the gold standard.
For exam-ple, we could have chosen { a1, a2, .
.
.
, a7 }, { a1,a3, a4, .
.
.
, a8 }, etc.
In fact, there are(107)= 120different combinations of 7 annotators.
As such,we can also compute how an individual human an-notator performs when measured against any com-bination of 7 gold standard annotators, by averag-ing these 120 M2 scores.
The above calculation isformalized in the general case in Equation 2:hi=1(|A||X|)?X:|X|=ig(X) (2)where(|A||X|)is the binomial coefficient for |A|choose |X| and 1 ?
i < |A|.
The function g(X)is defined in Equation 1.The resulting hivalues are hence the averageF0.5scores achieved by any human against anycombination of i other humans, and so, in someways, also represent the upper bound of humanperformance on the current dataset.
The specificvalues for hiare shown in the second column ofTable 5.5.2.2 CaveatOne caveat regarding this method is that the num-ber of all possible combinations of annotators isof the order 2|A|, which quickly becomes compu-tationally expensive for large values of |A|.
Fortu-nately however, in a realistic GEC evaluation sce-nario, it is only the last row of Table 5 that we aremost interested in, and so it is actually only neces-sary to calculate a much more manageable(|A||A|?1)gold standard combinations, which is conveniently702Gold Human (hi) AMU CAMB CUUIAnnotators (i) Avg F0.5 Avg F0.5 Ratio Avg F0.5 Ratio Avg F0.5 Ratio1 45.91 24.20 52.71% 28.22 61.46% 26.76 58.29%2 56.68 33.47 59.05% 37.77 66.64% 36.04 63.59%3 61.83 38.35 62.03% 42.68 69.03% 40.76 65.92%4 65.05 41.53 63.85% 45.87 70.51% 43.77 67.29%5 67.33 43.84 65.11% 48.17 71.54% 45.94 68.23%6 69.07 45.62 66.06% 49.93 72.29% 47.60 68.92%7 70.45 47.06 66.80% 51.34 72.87% 48.94 69.46%8 71.60 48.26 67.40% 52.50 73.32% 50.05 69.89%9 72.58 49.28 67.90% 53.47 73.67% 50.99 70.25%Table 5: Table showing average human F0.5scores over all combinations of 1 ?
i < 10 gold annotatorscompared to the same averages for the top 3 systems in CoNLL-2014, and the ratio percentage of eachteam?s average score versus the human average score.equal to the total number of annotators.
We onlycompute all combinations here in order to quan-tify, for the first time, how much each additionalannotator affects performance.5.2.3 System vs HumanIn addition to calculating scores on a human vshuman basis, we also calculated the F-scores forthe top three CoNLL-2014 teams, AMU (Junczys-Dowmunt and Grundkiewicz, 2014), CAMB (Fe-lice et al, 2014), and CUUI (Rozovskaya et al,2014), versus all the combinations of humans(Equation 3).si=1(|A||X|)?X:|X|=if(s,X) (3)Specifically, s ?
S, where S is the set of allthree shared task systems, i.e., {AMU, CAMB,CUUI}, and f(s,X) is the same function in Equa-tion 1 which is the score computed by the M2scorer to evaluate system s against the set of an-notators X chosen to be in the gold standard.
Theaverage F0.5scores for each of the team?s systemsversus increasing numbers of i annotators are alsoshown in Table 5.We notice from these scores that, as expected,both system and human performance increases asmore annotators are used in a gold standard.
Wedo now, however, have data that quantifies exactlyhow much each additional annotator affects thescore.
This effect can be more clearly seen in Fig-ure 1.It is important to note, however, that even with9 annotators, human output itself does not reachclose to 100% F0.5and instead, the difference be-tween the systems and the humans is about 20%F0.5.
Furthermore, the curves for humans and sys-tems also remain roughly parallel, suggesting hu-man corrections gain as much benefit as systemcorrections from larger sets of gold standard an-notations.5.3 Ratio ScoringIn light of the above observation that even humansvs humans are unable to score 100% F0.5, it thusseems unreasonable to expect machines to do thesame.
As such, we propose that it is much moreinformative to score system output against the av-erage performance of humans instead of againstthe theoretical maximum score.
The ratio valuesfor the three CoNLL-2014 teams against the hu-man gold standards of various sizes are hence alsoreported in Table 5.
The most important thing tonote is that these figures are not only much higherthan the low F0.5values currently reported in theliterature, they are also more representative of thestate of the art.
For instance, it is highly significantthat we can report that the top system in CoNLL-2014, CAMB, is actually able to perform 73% asreliably as a human, which suggests GEC may ac-tually be a more viable technology than was pre-viously thought.6 Additional Experiments6.1 Error CategoriesAs well as carrying out experiments at the systemlevel, we also carried out similar experiments atthe error category level.
More specifically, we re-calculated the values of Equation 1 and 2 for caseswhere the set of annotations consisted of only a7031 2 3 4 5 678 9020406080100Number of Gold Standard AnnotatorsF0.5HumanAMUCAMBCUUIFigure 1: Graph showing how average F0.5scoresfor humans and systems increase as the number ofgold standard annotators also increases (all errortypes, 50 Essays).single specific error type.
Since the participatingteams in CoNLL-2014 were not asked to classifythe type of errors their systems corrected, we wereonly able to calculate these new values using the10 sets of human annotations.Like Figure 1, we can see from Figure 2 thatthe F0.5performance of individual error types in-creases diminishingly as the number of annotatorsin the gold standard also increases.
More impor-tantly, however, we notice that some error typesachieve much higher scores than others, whichsuggests some annotators agree on certain cate-gories more than others.In particular, noun number (Nn) and subject-verb agreement (SVA) errors achieve the highestscores, at just under 90% F0.5, which is also notfar from the 100% F0.5that would be achieved ifwe had gold standard answers for all possible al-ternative corrections of this type.
The most likelyreason for this is that, as the correction of theseerror types typically only involves the addition orremoval of an -s suffix, i.e., a minor change innumber morphology, there is very little room forannotators to disagree.In contrast, the next highest category, article anddeterminer errors (ArtOrDet), has a slightly largerconfusion set, {the, a/an, }, which may accountfor the slightly lower score.
Similarly, the nextgroup of error categories, spelling and punctuation1 2 3 4 5 678 9020406080100Number of Gold Standard AnnotatorsF0.5Nn V t WOincSV A Wform WciArtOrDet PrepMec TransFigure 2: Graph showing how average F0.5scoresfor various error categories increase as the num-ber of gold standard annotators also increases (50essays).
Calculations based on human annotationsonly.
(Mec), verb tense (Vt), and word form (Wform),which all often involve a similar type of edit op-eration to a word lemma, likewise have slightlylarger confusion sets that include a larger varietyof possible morphological inflections.
It is likelythat the next category, prepositions (Prep), also hasa confusion set of a similar size.The last three categories, conjunctions (all-types) (Trans), word order (WOinc) and wordchoice (Wci), are all notable because they per-form significantly worse than the hitherto men-tioned categories.
The main reason for this is thatthese error types all typically have a scope muchlarger than most other categories in that they ofteninvolve changes at the structural or semantic level;e.g., changing an active to a passive or choosing asynonym.
For this reason, there are often manymore alternative ways to correct them, meaningthey are also much more likely to be affected byannotator bias.7041 2 3 4 5 678 9020406080100Number of Gold Standard AnnotatorsF0.5HumanAMUCAMBCUUIFigure 3: Graph showing how average F0.5scoresfor humans and systems increase as the number ofgold standard annotators also increases (all errortypes, 10 Essays).6.2 Essay SubsetsNow that we had empirical evidence showing howF0.5scores varied with the number of annotators,an additional question to ask was whether the sametrends for 50 essays were also present in a smallersubset of essays.
We therefore repeated the mainexperiment with all error types, but this time usedjust 10 essays (specifically, essays 1?10) in boththe hypothesis and gold standard.
The results areshown in Figure 3.Compared to Figure 1, the most significant dif-ference between these two graphs is that the rank-ing for AMU and CUUI has changed, although notby much in terms of F0.5.
The most likely reasonfor this is that the distribution of error types in thesmaller subset of essays is better suited to AMU?smore general SMT approach than to CUUI?s moretargeted classifier based approach.
For instance,see Table 9 in Ng et al (2014) to compare eachteam?s performance on different error types in theCoNLL-2014 shared task.In other words, while the overall relationshipbetween the system and human scores on 10 and50 essays remains more or less the same, re-searchers must be aware that smaller datasets mayhave more skewed error distributions, which inturn may affect system performance, dependentupon correction strategy.
With a balanced test setthough, it would seem feasible to carry out futureevaluation research on as few as 10 essays (about6000 words).7 ConclusionTo summarize, we first showed that 10 individualannotators can all correct the same sentence in 10different ways, yet alo all produce valid alterna-tives.
This implies that inter-annotator agreementstatistics, which rely on exact matching, are notwell-suited to grammatical error correction, be-cause it may not be the case that annotators trulydisagree, but rather that they have a bias towards aparticular type of alternative answer.We next showed that, as has long been sus-pected, increasing the number of annotators in thegold standard also leads to an increase in F0.5, al-though at a diminishing rate.
This data can be usedto help researchers decide how many gold standardannotations should be used in GEC evaluation.The main result of this paper however, is that bycomputing scores for human against human, wedetermined that it is not true that any human cor-rection is able to score 100% F0.5.
Instead, wefound that the human upper bound is roughly 73%F0.5and that the top 3 teams from CoNLL-2014actually perform, on average, between 67-73% asreliably as this human upper bound.
This resultis highly significant, because it suggests GEC sys-tems may actually be more viable than their previ-ously low F0.5scores would suggest.In addition to the above, we also found that hu-mans tend to agree on some error categories morethan others, and suggest that one of the main rea-sons for this concerns the size of the confusion setof the particular error type.Finally, not only are we making the correctionsby 10 annotators of all 50 essays available withthis paper, we also showed that the trends foundin the data are also consistent with the annotationsof just 10 essays, allowing future research to beconducted on much less text.AcknowledgmentsThis research is supported by Singapore Ministryof Education Academic Research Fund Tier 2grant MOE2013-T2-1-150.
We would also liketo thank the three anonymous reviewers for theircomments.705ReferencesRon Artstein and Massimo Poesio.
2008.
Inter-coderagreement for computational linguistics.
Computa-tional Linguistics, 34(4):555?596.Ondrej Bojar, Christian Buck, Christian Federmann,Barry Haddow, Philipp Koehn, Johannes Leveling,Christof Monz, Pavel Pecina, Matt Post, HerveSaint-Amand, Radu Soricut, Lucia Specia, and Ale?sTamchyna.
2014.
Findings of the 2014 Workshopon Statistical Machine Translation.
In Proceedingsof the Ninth Workshop on Statistical Machine Trans-lation, pages 12?58, Baltimore, Maryland, USA,June.
Association for Computational Linguistics.Martin Chodorow, Markus Dickinson, Ross Israel, andJoel R. Tetreault.
2012.
Problems in evaluatinggrammatical error detection systems.
In COLING,pages 611?628.Jacob Cohen.
1960.
A coefficient of agreementfor nominal scales.
Educational and PsychologicalMeasurement, 20(1):37?46.Daniel Dahlmeier and Hwee Tou Ng.
2012.
Betterevaluation for grammatical error correction.
In HLT-NAACL, pages 568?572.Daniel Dahlmeier, Hwee Tou Ng, and Siew Mei Wu.2013.
Building a large annotated corpus of learnerEnglish: The NUS Corpus of Learner English.
InProceedings of the Eighth Workshop on InnovativeUse of NLP for Building Educational Applications,pages 22?31, Atlanta, Georgia, USA.Robert Dale and Adam Kilgarriff.
2011.
Helping OurOwn: The HOO 2011 pilot shared task.
In Pro-ceedings of the Generation Challenges Session atthe 13th European Workshop on Natural LanguageGeneration, pages 242?249.Robert Dale, Ilya Anisimoff, and George Narroway.2012.
Helping Our Own: HOO 2012: A reporton the preposition and determiner error correctionshared task.
In Proceedings of the Seventh Work-shop on Innovative Use of NLP for Building Educa-tional Applications, pages 54?62.Michael Denkowski and Alon Lavie.
2010.
Choos-ing the right evaluation for machine translation: anexamination of annotator and automatic metric per-formance on human judgment tasks.
Proceedings ofAMTA.Mariano Felice, Zheng Yuan, ?istein E Andersen, He-len Yannakoudakis, and Ekaterina Kochmar.
2014.Grammatical error correction using hybrid systemsand type filtering.
In Proceedings of the Eigh-teenth Conference on Computational Natural Lan-guage Learning: Shared Task, pages 15?24.Andrew F Hayes and Klaus Krippendorff.
2007.
An-swering the call for a standard reliability measurefor coding data.
Communication Methods and Mea-sures, 1(1):77?89.Marcin Junczys-Dowmunt and Roman Grundkiewicz.2014.
The AMU system in the CoNLL-2014shared task: Grammatical error correction by data-intensive and feature-rich statistical machine trans-lation.
In Proceedings of the Eighteenth Confer-ence on Computational Natural Language Learn-ing: Shared Task, pages 25?33.Arle Richard Lommel, Maja Popovic, and AljoschaBurchardt.
2014.
Assessing inter-annotator agree-ment for translation error annotation.
In MTE:Workshop on Automatic andManual Metrics for Op-erational Translation Evaluation.Nitin Madnani, Philip Resnik, Bonnie J. Dorr, andRichard Schwartz.
2008.
Are multiple referencetranslations necessary?
Investigating the value ofparaphrased reference translations in parameter op-timization.
Proceedings of the Eighth Conferenceof the Association for Machine Translation in theAmericas, October.Nitin Madnani, Martin Chodorow, Joel R. Tetreault,and Alla Rozovskaya.
2011.
They can help: Usingcrowdsourcing to improve the evaluation of gram-matical error detection systems.
In Proceedings ofthe 49th Annual Meeting of the Association for Com-putational Linguistics, pages 508?513.Hwee Tou Ng, Siew Mei Wu, Yuanbin Wu, Chris-tian Hadiwinoto, and Joel R. Tetreault.
2013.
TheCoNLL-2013 shared task on grammatical error cor-rection.
In Proceedings of the Seventeenth Confer-ence on Computational Natural Language Learn-ing: Shared Task, pages 1?12, Sofia, Bulgaria.
ACL.Hwee Tou Ng, Siew Mei Wu, Ted Briscoe, ChristianHadiwinoto, Raymond Hendy Susanto, and Christo-pher Bryant.
2014.
The CoNLL-2014 shared taskon grammatical error correction.
In Proceedings ofthe Eighteenth Conference on Computational Natu-ral Language Learning: Shared Task, pages 1?14,Baltimore, Maryland, USA.
ACL.Ellie Pavlick, Rui Yan, and Chris Callison-Burch.2014.
Crowdsourcing for grammatical error cor-rection.
In Proceedings of the Companion Publi-cation of the 17th ACM Conference on ComputerSupported Cooperative Work and Social Computing,CSCW Companion ?14, pages 209?212, New York,NY, USA.
ACM.Alla Rozovskaya and Dan Roth.
2010.
AnnotatingESL errors: Challenges and rewards.
In NAACLWorkshop on Innovative Use of NLP for BuildingEducational Applications, pages 28?36.Alla Rozovskaya, Kai-Wei Chang, Mark Sammons,Dan Roth, and Nizar Habash.
2014.
The Illinois-Columbia system in the CoNLL-2014 shared task.In Proceedings of the Eighteenth Conference onComputational Natural Language Learning: SharedTask, pages 34?42.706Matthew Snover, Bonnie Dorr, Richard Schwartz, Lin-nea Micciulla, and Ralph Weischedel.
2006.
Astudy of translation error rate with targeted humanannotation.
In Proceedings of the Association forMachine Transaltion in the Americas.Joel R. Tetrault and Martin Chodorow.
2008.
Na-tive judgments of non-native usage: Experiments inpreposition error detection.
In COLING Workshopon Human Judgments in Computational Linguistics,pages 24?32, Manchester, UK.Joel R. Tetreault, Martin Chodorow, and Nitin Mad-nani.
2014.
Bucking the trend: improved evalua-tion and annotation practices for ESL error detec-tion systems.
Language Resources and Evaluation,48(1):5?31.707
