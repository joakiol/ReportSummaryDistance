Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 1546?1556,Edinburgh, Scotland, UK, July 27?31, 2011. c?2011 Association for Computational LinguisticsActive Learning with Amazon Mechanical TurkFlorian Laws Christian Scheible Hinrich Schu?tzeInstitute for Natural Language ProcessingUniversita?t Stuttgart{lawsfn, scheibcn}@ims.uni-stuttgart.deAbstractSupervised classification needs large amountsof annotated training data that is expensive tocreate.
Two approaches that reduce the costof annotation are active learning and crowd-sourcing.
However, these two approacheshave not been combined successfully to date.We evaluate the utility of active learning incrowdsourcing on two tasks, named entityrecognition and sentiment detection, and showthat active learning outperforms random selec-tion of annotation examples in a noisy crowd-sourcing scenario.1 IntroductionSupervised classification is the predominant tech-nique for a large number of natural language pro-cessing (NLP) tasks.
The large amount of labeledtraining data that supervised classification relies onis time-consuming and expensive to create, espe-cially when experts perform the data annotation.Recently, crowdsourcing services like Amazon Me-chanical Turk (MTurk) have become available as analternative that offers acquisition of non-expert an-notations at low cost.
MTurk is a software servicethat outsources small annotation tasks ?
called HITs?
to a large group of freelance workers.
The cost ofMTurk annotation is low, but a consequence of us-ing non-expert annotators is much lower annotationquality.
This requires strategies for quality controlof the annotations.Another promising approach to the data acqui-sition bottleneck for supervised learning is activelearning (AL).
AL reduces annotation effort by set-ting up an annotation loop where, starting from asmall seed set, only the maximally informative ex-amples are chosen for annotation.
With these an-notated examples, the classifier is then retrained toagain select more informative examples for furtherannotation.
In general, AL needs a lot fewer anno-tations to achieve a desired performance level thanrandom sampling.AL has been successfully applied to a number ofNLP tasks such as part-of-speech tagging (Ringgeret al, 2007), parsing (Osborne and Baldridge, 2004),text classification (Tong and Koller, 2002), senti-ment detection (Brew et al, 2010), and named entityrecognition (NER) (Tomanek et al, 2007).
Untilrecently, most AL studies focused on simulating theannotation process by using already available goldstandard data.
In reality, however, human annota-tors make mistakes, leading to noise in the annota-tions.
For this reason, some authors have questionedthe applicability of AL to noisy annotation scenariossuch as MTurk (Baldridge and Palmer, 2009; Re-hbein et al, 2010).AL and crowdsourcing are complementary ap-proaches: AL reduces the number of annotationsused while crowdsourcing reduces the cost per an-notation.
Combined, the two approaches could sub-stantially lower the cost of creating training sets.Our main contribution in this paper is that weshow for the first time that AL is significantly bet-ter than randomly selected annotation examples ina real crowdsourcing annotation scenario.
Ourexperiments directly address two tasks, named en-tity recognition and sentiment detection, but our1546evidence suggests that AL is of general benefit incrowdsourcing.
We also show that the effectivenessof MTurk annotation with AL can be further en-hanced by using two techniques that increase labelquality: adaptive voting and fragment recovery.2 Related Work2.1 CrowdsourcingPioneered by Snow et al (2008), Crowdsourcing,especially using MTurk, has become a widely usedservice in the NLP community.
A number of stud-ies have looked at crowdsourcing for NER.
Voyer etal.
(2010) use a combination of expert and crowd-sourced annotations.
Finin et al (2010) annotateTwitter messages ?
short sequences of words ?
andthis is reflected in their vertically oriented user in-terface.
Lawson et al (2010) choose an annotationinterface where annotators have to drag the mouseto select entities.
Carpenter and Poesio (2010) ar-gue that dragging is less convenient for workers thanmarking tokens.These papers do not address AL in crowdsourc-ing.
Another important difference is that previousstudies on NER have used data sets for which no?linguistic?
gold annotation is available.
In con-trast, we reannotate the CoNLL-2003 English NERdataset.
This allows us to conduct a detailed com-parison of MTurk AL to conventional expert anno-tation.2.2 Active Learning with Noisy LabelsHachey et al (2005) were among the first to in-vestigate the effect of actively sampled instanceson agreement of labels and annotation time.
Theydemonstrate applicability of AL when annotators aretrained experts.
This is an important result.
How-ever, AL depends on accurate assessments of uncer-tainty and informativeness and such an accurate as-sessment is made more difficult if labels are noisyas is the case in crowdsourcing.
For this reason, theproblem of AL performance with noisy labels hasbecome a topic of interest in the AL community.
Re-hbein et al (2010) investigate AL with human expertannotators for word sense disambiguation, but donot find convincing evidence that AL reduces anno-tation cost in a realistic (non-simulated) annotationscenario.
Brew et al (2010) carried out experimentson sentiment active learning through crowdsourcing.However, they use a small set of volunteer labelersinstead of anonymous paid workers.Donmez and Carbonell (2008) propose a methodto choose annotators from a set of noisy annotators.However, in a crowdsourcing scenario, it is not pos-sible to ask specific annotators for a label, as crowd-sourcing workers join and leave the site.
Further-more, they only evaluate their approach in simula-tions.
We use the actual labels of human annotatorsto avoid the risk of unrealistic assumptions whenmodeling annotators.We are not aware of any study that shows that ALis significantly better than a simple baseline of hav-ing annotators annotate randomly selected examplesin a highly noisy annotation setting like crowdsourc-ing.
While AL generally is superior to this base-line in simulated experiments, it is not clear thatthis result carries over to crowdsourcing annotation.Crowdsourcing differs in a number of ways fromsimulated experiments: the difficulty and annotationconsistency of examples drawn by AL differs fromthat drawn by random sampling; crowdsourcing la-bels are noisy; and because of the noisiness of labelsstatistical classifiers behave differently in simulatedand real annotation experiments.3 Annotation SystemOne fundamental design criterion for our annotationsystem was the ability to select examples in real timeto support, e.g., the interactive annotation experi-ments presented in this paper.
Thus, we could notuse the standard MTurk workflow or services likeCrowdFlower.1We therefore designed our own system for anno-tation experiments.
It consists of a two-tiered ap-plication architecture.
The frontend tier is a webapplication that serves two purposes.
First, the ad-ministrator can manage annotation experiments us-ing a web interface and publish annotation tasks as-sociated with an experiment on MTurk.
The front-end also provides tools for efficient review of thereceived answers.
Second, the frontend web appli-cation presents annotation tasks to MTurk workers.Because we wanted to implement interactive anno-tation experiments, we used the ?external question?1http://crowdflower.com/1547feature of MTurk.
An external question containsan URL to our frontend web application, which isqueried when a worker views an annotation task.Our frontend then in turn queries our backend com-ponent for an example to be annotated and renders itin HTML.The backend component is responsible for selec-tion of an example to be annotated in response to aworker?s request for an annotation task.
The back-end implements a diverse choice of random and ac-tive selection strategies as well as the multilabel-ing strategies described in section 3.2.
The backendcomponent runs as a standalone server and is queriedby the frontend via REST-like HTTP calls.For the NER task, we present one sentence perHIT, segmented into tokens, with a select box under-neath each token containing the classes.
The defini-tion of the classes is based on the CoNLL-2003 an-notation guidelines (Tjong Kim Sang and De Meul-der, 2003).
Examples were given for every class.Annotators are forced to make a selection for upper-case tokens.
Lowercase tokens are prelabeled with?O?
(no named entity), but annotators are encour-aged to change this label if the token is in fact partof an entity phrase.For sentiment annotation, we found in prelim-inary experiments that using simple radio buttonselection for the choice of the document label(positive or negative) leads to a very highamount of spam submissions, taking the overall clas-sification accuracy down to around 55%.
We thendesigned a template that forced annotators to typethe label as well as a randomly chosen word fromthe text.
Individual label accuracy was around 75%in this scheme.3.1 Concurrent example selectionAL works by setting up an interactive annotationloop where at each iteration, the most informativeexample is selected for annotation.
We use a pool-based AL setup where the most informative exam-ple is selected from a pool of unlabeled examples.Informativeness is calculated as uncertainty (Lewisand Gale, 1994) using the margin metric (Scheinand Ungar, 2007).
This metric chooses examples forwhich the margin of probabilities from the classifierbetween the two most probable classes is the small-est:Mn = |P?
(c1|xn) ?
P?
(c2|xn)|Here, xn is the instance to be classified, c1 and c2are the two most likely classes, and P?
the classifier?sestimate of probability.For NER, the margins of the tokens are averagedto get an uncertainty assessment of the sentence.
Forsentiment, whole documents are classified, thus un-certainties can be used directly.After annotation, the selected example is removedfrom the unlabeled pool and, together with its la-bel(s), added to the set of labeled examples.
Theclassifier is then retrained on the labeled examplesand the informativeness of the remaining examplesin the pool is re-evaluated.Depending on the classifier and the sizes of pooland labeled set, retraining and reevaluation can takesome time.
To minimize wait times, traditional ALimplementations select examples in batches of then most informative examples.
However, batch se-lection might not give the optimum selection (exam-ples in a batch are likely to be redundant, see Brinker(2003)) and wait times can still occur between onebatch and the next.When performing annotation with MTurk, waittimes are unacceptable.
Thus, we perform the re-training and uncertainty rescoring concurrently withthe annotation user interface.
The unlabeled pool isstored in a priority queue that is ordered according tothe examples?
informativeness.
The annotation userinterface takes the most informative example fromthe pool and presents it to the annotator.
The la-beled example is then inserted into a second queuethat feeds and updates retraining and rescoring pro-cesses.
The pool queue then is resorted according tothe new informativeness.
In this way, annotation andexample selection can run in parallel.
This is similarto Haertel et al (2010).3.2 Adaptive voting and fragment recoveryMTurk labels often have a high error rate.
A com-mon strategy for improving label quality is to ac-quire multiple labels by different workers for eachexample and then consolidate the annotations intoa single label of higher quality.
To trade off num-ber of annotated examples against quality of anno-tations, we adopt adaptive voting.
It uses majority1548NER SentimentBudget 5820 6931 1130 1756#train F1 cost/sent w.-accuracy #train F1 #train Acc cost/doc w.-accuracy #train AccRS 1 S 5820 59.6 1.00 51.6 ?
?
1130 70.4 1 74.8 ?
?2 3-v 1624 61.4?
3.58 70.1 ?
?
?
?
?
?
?
?3 5/4-v 1488 63.0?
3.91 71.6 1774 63.5 450 71.2 2.51 89.6 735 79.24 5-v+f 1996 63.6?
2.91 71.8 2385 64.9?
?
?
?
?
?
?AL 5 S 5820 67.0 1.00 66.5 ?
?
1130 74.8 1 76.0 ?
?6 3-v 1808 70.0?
3.21 78.8 ?
?
?
?
?
?
?
?7 5/4-v 1679 70.4?
3.46 79.6 1966 70.6 455 77.4 2.48 89.0 715 81.88 5-v+f 2165 70.5 2.68 79.3 2691 71.2 ?
?
?
?
?
?Table 1: For NER, active learning consistently beats random sampling on MTurk.
NER F1 evaluated onCoNLL test set A.
#train = number of sentences in training set, S = single, 3-v = 3-voting, 5/4-voting = 5-and 4-voting for NER and sentiment resp., +f = using fragments; sentiment budget 1130 for run 1, sentimentbudget 1756 averaged over 2 runs.voting and is adaptive in the number of repeated an-notations.
For NER, a sentence is first annotated bytwo workers.
Then majority voting is performed foreach token individually.
If there is a majority for ev-ery token that is greater than an agreement threshold?, the sentence is accepted with each token labeledwith the majority label.
Otherwise additional anno-tations are requested.
A sentence is discarded if thenumber of repeated annotations exceeds a discardthreshold d (d-voting).2 We use the same schemefor sentiment; note that there is just one decision perHIT in this case, not several as in NER.For NER, we also use fragment recovery: we sal-vage tokens with agreeing labels from discarded sen-tences.
We cut the token sequence of a discardedsentence into several fragments that have agreeingtokens and discard only those parts that disagree.
Wethen include these recovered fragments in the train-ing data just like complete sentences.Software release.
Our active learning frameworkused can be downloaded at http://www.ims.uni-stuttgart.de/?lawsfn/active/.4 Experiments, Results and Analysis4.1 ExperimentsIn our NER experiments, we have workers reanno-tate the English corpus of the CoNLL-2003 NERshared task.
We chose this corpus to be able to com-pare crowdsourced annotations with gold standard2It can take a while in this scheme for annotators to agreeon a final annotation for a sentence.
We make tentative labelsof a sentence available to the classifier immediately and replacethem with the final labels once voting is completed.annotations.
A HIT is one sentence and is offeredfor a base payment of $0.01.
We filtered out answersthat contained unannotated tokens or were obviousspam (e.g., all tokens labeled as MISC).
For test-ing NER performance, we used a system based onconditional random fields with standard named en-tity features including the token itself, orthographicfeatures like the occurrence of capitalization or spe-cial characters and context information about the to-kens to the left/right of the current token.The sentiment detection task was modeled after awell-known document analysis setup for sentimentclassification, introduced by Pang et al (2002).
Weuse their corpus of 1000 positive and 1000 negativemovie reviews and the Stanford maximum entropyclassifier (Manning and Klein, 2003) to predict thesentiment label of each document d from a unigramrepresentation of d. We randomly split this corpusinto a test set of 500 reviews and an active learn-ing pool of 1500 reviews.
Each HIT consists of onedocument, valued at $0.01.We compare random sampling (RS) and AL incombination with the proposed voting and fragmentstrategies with different parameters.
We want toavoid rerunning experiments on MTurk over andover again, but on the other hand, we believe that us-ing synthetic data for simulations is problematic be-cause it is difficult to generate synthetic data with arealistic model of annotator errors.
Thus, we loggeda play-by-play record of the annotator interactionsand labels.
With this recording, we can then rerunstrategies with different parameters.We chose voting with at most d = 5 repetitions as1549our main reannotation strategy for both random andactive sampling for NER annotation.
We use simplemajority voting (?
= .5) for NER.For sentiment, we set d = 4 and minimum agree-ment ?
= .75 because the number of labels issmaller (2 vs. 5) and so random agreement is morelikely for sentiment.To get results for 3-voting NER, we take therecording and discard 5-voting votes not needed in3-voting.
This will result in roughly the same num-ber of annotated sentences, but at a lower cost.
Thissimulation of 3-voting is not exactly what wouldhave happened on MTurk (e.g., the final vote on asentence might be different, which then influencesAL example selection), but we will assume that dif-ferences are rare and simulated and actual resultsare similar.
The same considerations apply to sin-gle votes and to the sentiment experiments.We always compare two strategies for the sameannotation budget.
For example, the number oftraining sentences in Table 1 differ in the two rel-evant columns, but all strategies compared use ex-actly the same annotation budget (5820, 6931, 1130,and 1756, respectively).For the single annotation strategy, each interac-tion record contained only about 40% usable anno-tations, the rest were repeats.
A comparison withthe single annotation strategy over approx.
2000 sen-tences or 450 documents would not have been mean-ingful; therefore we chose to run an extra experimentwith the single annotation strategy to match this upwith the budgets of the voting strategies.
The re-sults are presented in two separate columns of Ta-ble 1 (budgets 6931 and 1756).4.2 ResultsFor sentiment detection, worker accuracy or labelquality ?
the percentage of correctly annotated doc-uments ?
is 74.8.
In contrast, for NER, worker accu-racy ?
the percentage of non-O tokens annotated cor-rectly ?
is only 51.6 (Table 1, line 1).
This demon-strates the challenge of using MTurk for NLP an-notation tasks.
When we use single annotations ofeach sentence, NER performance is 59.6 F1 for ran-dom sampling (line 1).
When training with gold la-bels on the same sentences, the performance is 80.0(not shown).
This means we lose more than 20%due to poor worker accuracy.
Adaptive voting andfragment recovery manage to recover a small part ofthe lost performance (lines 2?4); each of the threeF1 scores is significantly better than the one aboveit as indicated by ?
(Approximate RandomizationTest (Noreen, 1989; Chinchor et al, 1993) as im-plemented by Pado?
(2006)).Using AL turns out to be quite successful for NERperformance.
For single annotations, NER perfor-mance is 67.0 (line 5), an improvement of 7.4%compared to random sampling.
Adaptive votingand fragment recovery again increase worker accu-racy (lines 6?8) although total improvement of 3.5%(lines 8 vs. 5) is smaller than 4% for random (lines4 vs. 1).
The learning curves of AL vs. random inFigure 1 (top left) confirm this good result for AL.These learning curves are for tokens ?
not for sen-tences ?
to show that the reason for AL?s better per-formance is not that it selects slightly longer sen-tences than random.
In addition, the relative advan-tage of AL vs random decreases over time, which istypical of pool-based AL experiments.We carried out two runs of the same experimentfor sentiment to validate our first positive result sincethe difference between the two conditions is not aslarge as in NER (Figure 1, top right).
After about300 documents, active learning consistently outper-forms random sampling.
The first AL run performsbetter because of higher label quality in the begin-ning.
The overall advantage of AL over randomis lower than for NER because the set of labels issmaller in sentiment, making the classification taskeasier.
Second, there is a large amount of simple lex-ical clues for detecting sentiment (cf.
Wilson et al(2005)).
It is likely that some of them can be learnedwell through random sampling at first; however, ac-tive learning can gain accuracy over time because itselects examples with more difficult clues.In Figure 1 (bottom), we compare single annota-tion with adaptive voting.
The graphs show F1 asa function of cost.
Adaptive voting trades quantityof sampled sentences for quality of labels and thusincurs higher net costs per sentence.
This results ina smaller dataset for a given budget, but this datasetis still more useful for classifier training.
For NER(Figure 1, bottom left), the single annotation strat-egy has a faster start; so for small budgets, cover-ing a somewhat larger portion of the sample spaceis beneficial.
For larger budgets, however, quality of15500 5000 10000 15000 200000.40.50.60.70.8TokensF?Scoreactive, 5?votingrandom, 5?voting0 200 400 6000.40.50.60.70.80.9DocumentsAccuracyactive 1active 2random 1random 20 1000 2000 3000 4000 5000 60000.40.50.60.70.8CostF?Scoresingle ann.3?voting5?voting5?voting +frags0 500 1000 1500 20000.40.50.60.70.80.9CostAccuracysingle ann.4?votingFigure 1: Top: Active learning vs. Random sampling for NER (left) and sentiment (right).
Bottom: Activelearning: adaptive voting vs. single annotation for NER (left) and sentiment (right).the voted labels trumps quantity.For sentiment (Figure 1, bottom right), results aresimilar: voting has no benefit initially, but as find-ing maximally informative examples to annotate be-comes harder in later stages of learning, adaptivevoting gains an advantage over single annotations.The main result of the experiment is that activelearning is better by about 7% F1 than random sam-pling for NER and by 2.6% accuracy for sentiment(averaged over two runs at budget 1756).
Adaptivevoting further improves AL performance for bothNER and sentiment.4.3 Annotation time per tokenMost AL work assumes constant cost per annotationunit.
This assumption has been questioned becauseAL often selects hard examples that take longer toannotate (Hachey et al, 2005; Settles et al, 2008).In annotation with MTurk, cost is not a functionof annotation time because workers are paid a fixedamount per HIT.
Nevertheless, annotation time playsa part in whether workers are willing to work on agiven task for the offered reward.
This is particularlyproblematic for NER since workers have to examineeach token individually.
We therefore investigatefor NER whether the time MTurk workers spend onannotating sentences differs for random vs. AL.We first compute median and mean annotationtimes and number of tokens per sentence:sec/sentence tokens/sentencestrategy median mean all requiredrandom 17.2 33.1 15.0 3.4AL 17.8 33.0 17.7 4.0We see that most sentences are annotated in a veryshort time; but the mean is much larger than the me-dian because there are outliers of up to eight min-utes.
AL tends to select slightly longer sentences as15510 500 1000 1500 20000.40.50.60.70.80.9SentencesF?Scoregold selection, gold labelsMTurk selection, gold labelsMTurk selection, MTurk labels0 200 400 6000.500.600.700.80DocumentsAccuracygold selection, gold labelsMTurk selection, gold labelsMTurk selection, MTurk labelsFigure 3: Performance on gold labels.
Left: NER.
Right: sentiment (run 1).0 2 4 6 8 10 13 16 19 23 290100200300400Number of uppercase tokensAnnotationtime(seconds)randomactiveFigure 2: Annotation time vs. # uppercase tokenswell as sentences with slightly more uppercase to-kens that require annotation.In a more detailed analysis, we attempt to distin-guish between (i) the effect of more uppercase (?an-notation required?)
tokens vs. (ii) the effect of ex-ample difficulty.
We fit a linear regression modelto annotation time vs. the number of uppercase to-kens.
For the regression fit, we removed all annota-tion times > 60 seconds.
Such long times indicatedistraction of the worker and are not a reliable mea-sure of difficulty.Figure 2 shows the distribution of annotationtimes for both cases combined and the fitted modelsfor each.
The model estimated an annotation time of2.3 secs for each required token for random vs. 2.7secs for AL.
We conclude that the difference in dif-ficulty between sentences selected by random sam-pling vs. AL is small, but noticeable.4.4 Influence of noise on the selection processWhile NER performance for AL is much higher thanfor random sampling, it is still quite a bit lower thanwhat is possible on gold labels.
In the case of AL,there are two reasons why this happens: (i) Thenoisy labels negatively affect the classifier?s abilityto learn a good model that is used for classifying thetest set.
(ii) The noisy labels result in bad interme-diate models that then select suboptimal examplesto be annotated next.
The AL selection process is?misled?
by the noisy examples.We conduct an experiment to determine the con-tribution of factors (i) and (ii) to the performanceloss.
First, we preserve the sequence of sentenceschosen by our AL experiments on MTurk, with 5-voting for NER and 4-voting for sentiment but re-place the noisy worker-provided labels by gold la-bels.
The performance of classifiers trained on thissequence is the dashed line ?MTurk selection, goldlabels?
in Figure 3 for NER (left) and sentiment(right).Second, we compare with a traditional simulatedAL experiment with gold labels.
Here, the selectiontoo is controlled by gold labels, so the selection hasa noiseless classifier available for scoring and canperform optimal uncertainty selection.
These are the15521 5 10 50 100 5000.00.20.40.60.81.0Number of sentencesQuality(%correctentitytokens)1 2 5 10 20 50 100 2000.00.20.40.60.81.0Number of documentsQuality(%correctdocumentlabels)Figure 4: Worker accuracy vs. number of HITs.
Each point corresponds to one worker (?
= active, +=random sampling; black and grey for different runs).
Left: NER.
Right: Sentiment.dotted lines ?gold selection, gold labels?
in Figure 3.We used a batch-mode AL setup for this compari-son experiment.
For a fair comparison, we adjust thebatchsize to be equal to the average staleness of a se-lected example in concurrent MTurk active learning.The staleness of an example is defined as the num-ber of annotations the system has received, but notyet incorporated in the computation of an example?suncertainty score (Haertel et al, 2010).For our concurrent NER system, the average stal-eness of an example was about 12 (min: 1, max: 40),for sentiment it was about 2.
The figure for NER ishigher than the number cited by Haertel et al (2010)because there are more annotators accessing our sys-tem at the same time via MTurk but not as high forsentiment since documents are longer and retrainingthe sentiment classifier is faster.
The average stale-ness of an example in a batch-mode system is halfthe batch size.
Thus, we set the batch size of ourcomparison system to 25 for NER and to 4 for sen-timent.Returning to the two factors introduced above ?
(i) final effect of noise on test set performance vs.(ii) intermediate effect of noise on example selec-tion ?
we see in Figure 3 that (i) has a large effecton NER whereas (ii) has a noticeable, but small ef-fect.3 For example, at 1966 sentences, F1 scores are3Our comparison unit for NER is the sentence.
We can-not compare on cost here since we do not know what the per-sentence cost of a ?gold?
expert annotation is.70.6 (MTurk-MTurk), 81.4 (MTurk-gold) and 84.9(gold-gold).
This means that a performance differ-ence of 10 points F1 has to be attributed to noisylabels resulting in a worse final classifier (effect i),and another 3.5 points are lost due to sub-optimalexample selection (effect ii).For sentiment, the results are different.
There isno clear difference between the three runs.
We at-tribute this to the fact that the quality of the labelsis higher in sentiment than in NER.
Our initial ex-periments on sentiment were all negative (showingno improvement of AL compared to random) be-cause label quality was too low.
Only after we intro-duced the template described in Section 3 and used4-voting with ?
= .75 did we get positive results forAL.
This leads to an overall label quality of about90% (over all runs) which is so high that the differ-ence to using gold labels is small if present at all.5 Worker QualitySo far we have assumed that all workers provideannotations of the same quality.
However, this isnot the case.
Figure 4 shows plots of worker accu-racy as a function of worker productivity (numberof annotated examples).
Some workers submit onlyone or two HITs just to try out the task.
For NER,the majority of workers submit between 5 and 10sentences, with label qualities between 0.5 and 0.8.The chance level for correctness is around 0.25 (four1553different named entity categories for uppercase to-kens).
For sentiment, most workers submit 1 to 5documents, with label qualities between 0.5 and 1.Chance level lies at around 0.5 (for two equally dis-tributed labels).While quality for highly productive workers ismediocre in our experiments, other researchers havefound extremely bad quality for their most prolificworkers (Callison-Burch, 2009).
Some of theseworkers might be spammers who try to submit an-swers with automatic scripts.
We encountered somespammers that our heuristics did not detect (shownin the bottom-right areas of Figure 4, left), but thevoting mechanism was able to mitigate their nega-tive influence.Given the large variation in Figure 4, using workerquality in crowdsourcing for improved training setcreation seems promising.
We now test two suchstrategies for NER in an oracle setup.5.1 Blocking low-quality workersA simple approach is to refuse annotations fromworkers that have been determined to provide lowquality answers.
We simulated this strategy on NERdata using oracle quality ratings.
We chose NER be-cause of its lower overall label quality.
The re-sults are presented in Figure 5 for random (a) andAL (b).
For random, quality filtering with low cut-offs helps by removing bad annotations that likelycome from spammers.
While the voting strategyprevented a performance decrease with bad anno-tations, it needed to expend many extra annotationsfor correction.
With filtering, these extra annotationsbecome unnecessary and the system can learn faster.When low-quality workers are less active, as in theAL dataset, we find no meaningful performance in-crease for low cutoffs up to 0.4.
For very high cut-offs (0.7), the beginning of the performance curveshows that further cost reductions can be achieved.However, we did not have enough recorded humanannotations available to perform a simulation for thefull budget.5.2 Trusting high-quality workersThe complementary approach is to take annotationsfrom highly rated workers at face value and imme-diately accept them as the correct label, bypassingthe voting procedure.
Bypassing saves the cost ofrepeated annotation of the same sentence.
Figure 5shows learning curves for two bypass thresholds onworker quality (measured as proportion of correctnon-O tokens) for random (c) and AL (d).
Bypass-ing performs surprisingly well.
We find a steeperrise of the learning curve, meaning less cost for thesame performance.
Not only do we find substantialcost reductions, but also higher overall performance.We believe this is because high-quality annotationscan sometimes be voted down by other annotations.If we can identify high-quality workers and directlyuse their annotations, this can be avoided.These experiments are oracle experiments usinggold data that is normally not available.
In futurework, we would like to repeat the experiments usingmethods for worker quality estimation (Ipeirotis etal., 2010; Donmez et al, 2009).
For AL, the choiceas to which labels are used (as a result of voting, by-passing or other) also has an influence on the selec-tion.
However, we had to keep the sequence of theselected sentences fixed in the simulations reportedabove.
While our method of sample selection forAL proved to be quite robust even in the presenceof noise, higher quality labels do have an influenceon the sample selection (see section 4.4), so the im-provement could be even better than indicated here.5.3 Differences in quality between AL andrandomThe essence of AL is to select examples that are dif-ficult to classify.
As observed in our experimentson annotation time, this difficulty is reflected in theamount of time a human needs to work on examplesselected through AL.
Another effect to expect fromdifficulty could be lower annotation accuracy.
Wetherefore examined the accuracies for each workerwho contributed to both the AL and the random ex-periment.
We found that in the NER task, the 20workers in this group had a slightly higher (0.07) av-erage quality for randomly selected examples.
Thisdifference is low and does not suggest a significantdrop in accuracy for examples selected in AL.6 ConclusionWe have investigated the use of AL in a real-lifeannotation experiment with human annotators in-stead of traditional simulations with gold labels for1554(a) (b) (c) (d)0 1000 2000 3000 40000.500.550.600.650.700.75CostF?Scorebaseline 5?votingmin.
quality 0.1min.
quality 0.4min.
quality 0.70 1000 2000 3000 40000.500.550.600.650.700.75CostF?Scorebaseline 5?votingmin.
quality 0.1min.
quality 0.4min.
quality 0.70 1000 2000 3000 40000.500.550.600.650.700.75CostF?Scorebaseline 5?votingbypass 0.9bypass 0.70 1000 2000 3000 40000.500.550.600.650.700.75CostF?Scorebaseline 5?votingbypass 0.9bypass 0.7Figure 5: Blocking low-quality workers: (a) random, (b) AL.
Bypass voting: (c) random, (d) AL.named entity recognition and sentiment classifica-tion.
The annotation was performed using MTurk inan AL framework that features concurrent exampleselection without wait times.
We also evaluated twostrategies, adaptive voting and fragment recovery, toimprove label quality at low additional cost.
We findthat even for the relatively high noise levels of anno-tations gathered with MTurk, AL is successful, im-proving performance by +6.9 points F1 compared torandom sampling for NER and by +2.6% accuracyfor sentiment.
Furthermore, this performance levelis reached at a smaller MTurk cost compared to ran-dom sampling.
Thus AL not only reduces annotationcosts, but also offers an improvement in absoluteperformance for these tasks.
This is clear evidencethat active learning and crowdsourcing are comple-mentary methods for lowering annotation cost andshould be used together in training set creation fornatural language processing tasks.We have also conducted oracle experiments thatshow that further performance gains and cost sav-ings can be achieved by using information aboutworker quality.
We plan to confirm these results byusing estimates of quality in the future.7 AcknowledgmentsFlorian Laws is a recipient of the Google EuropeFellowship in Natural Language Processing, andthis research is supported in part by his fellowship.Christian Scheible is supported by the DeutscheForschungsgemeinschaft project Sonderforschungs-bereich 732.ReferencesJason Baldridge and Alexis Palmer.
2009.
How welldoes active learning actually work?
Time-based eval-uation of cost-reduction strategies for language docu-mentation.
In Proceedings of the 2009 Conference onEmpirical Methods in Natural Language Processing,pages 296?305.Anthony Brew, Derek Greene, and Pa?draig Cunningham.2010.
Using crowdsourcing and active learning totrack sentiment in online media.
In Proceeding of the2010 conference on ECAI 2010: 19th European Con-ference on Artificial Intelligence, pages 145?150.Klaus Brinker.
2003.
Incorporating diversity in activelearning with support vector machines.
In Proceed-ings of the Twentieth International Conference on Ma-chine Learning (ICML 2003), pages 59?66.Chris Callison-Burch.
2009.
Fast, cheap, and cre-ative: evaluating translation quality using Amazon?sMechanical Turk.
In Proceedings of the 2009 Confer-ence on Empirical Methods in Natural Language Pro-cessing, pages 286?295.Bob Carpenter and Massimo Poesio.
2010.
Modelsof data annotation.
Tutorial at the seventh interna-tional conference on Language Resources and Eval-uation (LREC 2010).Nancy Chinchor, David D. Lewis, and LynetteHirschman.
1993.
Evaluating message understandingsystems: an analysis of the third message understand-ing conference (muc-3).
Computational Linguistics,19(3):409?449.Pinar Donmez and Jaime G. Carbonell.
2008.
Proactivelearning: cost-sensitive active learning with multipleimperfect oracles.
In Proceeding of the 17th ACM con-ference on Information and knowledge management,pages 619?628.Pinar Donmez, Jaime G. Carbonell, and Jeff Schnei-der.
2009.
Efficiently learning the accuracy of la-1555beling sources for selective sampling.
In Proceedingsof the 15th ACM SIGKDD international conferenceon Knowledge discovery and data mining, pages 259?268.Tim Finin, William Murnane, Anand Karandikar,Nicholas Keller, Justin Martineau, and Mark Dredze.2010.
Annotating named entities in twitter data withcrowdsourcing.
In Proceedings of the NAACL HLT2010 Workshop on Creating Speech and LanguageData with Amazon?s Mechanical Turk, pages 80?88.Ben Hachey, Beatrice Alex, and Markus Becker.
2005.Investigating the effects of selective sampling on theannotation task.
In CoNLL ?05: Proceedings of the9th Conference on Computational Natural LanguageLearning, pages 144?151.Robbie Haertel, Paul Felt, Eric K. Ringger, and KevinSeppi.
2010.
Parallel active learning: Eliminatingwait time with minimal staleness.
In Proceedings ofthe NAACL HLT 2010 Workshop on Active Learningfor Natural Language Processing, pages 33?41.Panagiotis G. Ipeirotis, Foster Provost, and Jing Wang.2010.
Quality management on amazon mechanicalturk.
In Proceedings of the ACM SIGKDD Workshopon Human Computation (HCOMP ?10).Nolan Lawson, Kevin Eustice, Mike Perkowitz, andMeliha Yetisgen-Yildiz.
2010.
Annotating large emaildatasets for named entity recognition with mechanicalturk.
In Proceedings of the NAACL HLT 2010 Work-shop on Creating Speech and Language Data withAmazon?s Mechanical Turk, pages 71?79.David D. Lewis and William A. Gale.
1994.
A sequentialalgorithm for training text classifiers.
In Proceedingsof the 17th Annual International ACM SIGIR Confer-ence on Research and Development in Information Re-trieval, pages 3?12.Christopher Manning and Dan Klein.
2003.
Optimiza-tion, maxent models, and conditional estimation with-out magic.
In Proceedings of the 2003 Conferenceof the North American Chapter of the Association forComputational Linguistics on Human Language Tech-nology: Tutorials - Volume 5, pages 8?8.Eric W. Noreen.
1989.
Computer-intensive methods fortesting hypotheses: an introduction.
Wiley.Miles Osborne and Jason Baldridge.
2004.
Ensemble-based active learning for parse selection.
InDaniel Marcu Susan Dumais and Salim Roukos, edi-tors, HLT-NAACL 2004: Main Proceedings, pages 89?96.Sebastian Pado?, 2006.
User?s guide to sigf: Signifi-cance testing by approximate randomisation.Bo Pang, Lillian Lee, and Shivakumar Vaithyanathan.2002.
Thumbs up?
Sentiment classification usingmachine learning techniques.
In Proceedings of the2002 Conference on Empirical Methods in NaturalLanguage Processing (EMNLP), pages 79?86.Ines Rehbein, Josef Ruppenhofer, and Alexis Palmer.2010.
Bringing active learning to life.
In Proceed-ings of the 23rd International Conference on Compu-tational Linguistics (Coling 2010), pages 949?957.Eric Ringger, Peter McClanahan, Robbie Haertel, GeorgeBusby, Marc Carmen, James Carroll, Kevin Seppi, andDeryle Lonsdale.
2007.
Active learning for part-of-speech tagging: Accelerating corpus annotation.
InProceedings of the Linguistic Annotation Workshop atACL-2007, pages 101?108.Andrew Schein and Lyle Ungar.
2007.
Active learn-ing for logistic regression: An evaluation.
MachineLearning, 68(3):235?265.Burr Settles, Mark Craven, and Lewis Friedland.
2008.Active learning with real annotation costs.
In Proceed-ings of the NIPS Workshop on Cost-Sensitive Learn-ing, pages 1069?1078.Rion Snow, Brendan O?Connor, Daniel Jurafsky, and An-drew Ng.
2008.
Cheap and fast ?
but is it good?evaluating non-expert annotations for natural languagetasks.
In Proceedings of the 2008 Conference onEmpirical Methods in Natural Language Processing,pages 254?263.Erik F. Tjong Kim Sang and Fien De Meulder.
2003.Introduction to the conll-2003 shared task: language-independent named entity recognition.
In Proceedingsof the seventh conference on Natural language learn-ing at HLT-NAACL (CoNLL 2003), pages 142?147.Katrin Tomanek, Joachim Wermter, and Udo Hahn.2007.
An approach to text corpus construction whichcuts annotation costs and maintains reusability of an-notated data.
In Proceedings of the 2007 Joint Confer-ence on Empirical Methods in Natural Language Pro-cessing and Computational Natural Language Learn-ing, pages 486?495.Simon Tong and Daphne Koller.
2002.
Support vec-tor machine active learning with applications to textclassification.
The Journal of Machine Learning Re-search, 2:45?66.Robert Voyer, Valerie Nygaard, Will Fitzgerald, and Han-nah Copperman.
2010.
A hybrid model for anno-tating named entity training corpora.
In Proceedingsof the Fourth Linguistic Annotation Workshop, pages243?246.Theresa Wilson, Janyce Wiebe, and Paul Hoffmann.2005.
Recognizing contextual polarity in phrase-levelsentiment analysis.
In Proceedings of the conferenceon Human Language Technology and Empirical Meth-ods in Natural Language Processing, pages 347?354.1556
