2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 49?59,Montre?al, Canada, June 3-8, 2012. c?2012 Association for Computational LinguisticsMachine Translation of Arabic DialectsRabih Zbib, Erika Malchiodi, Jacob Devlin, David Stallard, Spyros Matsoukas,Richard Schwartz, John Makhoul, Omar F.
Zaidan?, Chris Callison-Burch?Raytheon BBN Technologies, Cambridge MA?Microsoft Research, Redmond WA?Johns Hopkins University, Baltimore MDAbstractArabic Dialects present many challenges formachine translation, not least of which is thelack of data resources.
We use crowdsourc-ing to cheaply and quickly build Levantine-English and Egyptian-English parallel cor-pora, consisting of 1.1M words and 380kwords, respectively.
The dialectal sentencesare selected from a large corpus of Arabic webtext, and translated using Amazon?s Mechan-ical Turk.
We use this data to build Dialec-tal Arabic MT systems, and find that smallamounts of dialectal data have a dramatic im-pact on translation quality.
When translatingEgyptian and Levantine test sets, our Dialec-tal Arabic MT system performs 6.3 and 7.0BLEU points higher than a Modern StandardArabic MT system trained on a 150M-wordArabic-English parallel corpus.1 IntroductionThe Arabic language is a well-known example ofdiglossia (Ferguson, 1959), where the formal vari-ety of the language, which is taught in schools andused in written communication and formal speech(religion, politics, etc.)
differs significantly in itsgrammatical properties from the informal varietiesthat are acquired natively, which are used mostly forverbal communication.
The spoken varieties of theArabic language (which we refer to collectively asDialectal Arabic) differ widely among themselves,depending on the geographic distribution and thesocio-economic conditions of the speakers, and theydiverge from the formal variety known as Mod-ern Standard Arabic (MSA) (Embarki and Ennaji,2011).
Significant differences in the phonology,morphology, lexicon and even syntax render someof these varieties mutually incomprehensible.The use of Dialectal Arabic has traditionally beenconfined to informal personal speech, while writ-ing has been done almost exclusively using MSA(or its ancestor Classical Arabic).
This situation isquickly changing, however, with the rapid prolifer-ation of social media in the Arabic-speaking partof the world, where much of the communicationis composed in dialect.
The focus of the ArabicNLP research community, which has been mostly onMSA, is turning towards dealing with informal com-munication, with the introduction of the DARPABOLT program.
This new focus presents new chal-lenges, the most obvious of which is the lack of di-alectal linguistic resources.
Dialectal text, which isusually user-generated, is also noisy, and the lackof standardized orthography means that users oftenimprovise spelling.
Dialectal data also includes awider range of topics than formal data genres, suchas newswire, due to its informal nature.
These chal-lenges require innovative solutions if NLP applica-tions are to deal with Dialectal Arabic effectively.In this paper:?
We describe a process for cheaply and quicklydeveloping parallel corpora for Levantine-English and Egyptian-English using Amazon?sMechanical Turk crowdsourcing service (?3).?
We use the data to perform a variety of machinetranslation experiments showing the impact ofmorphological analysis, the limited value ofadding MSA parallel data, the usefulness ofcross-dialect training, and the effects of trans-lating from dialect to MSA to English (?4).We find that collecting dialect translations has a lowcost ($0.03/word) and that relatively small amountsof data has a dramatic impact on translation quality.When trained on 1.5M words of dialectal data, oursystem performs 6.3 to 7.0 BLEU points higher thanwhen it is trained on 100 times more MSA data froma mismatching domain.492 Previous WorkExisting work on natural language processing of Di-alectal Arabic text, including machine translation, issomewhat limited.
Previous research on DialectalArabic MT has focused on normalizing dialectal in-put words into MSA equivalents before translatingto English, and they deal with inputs that containa limited fraction of dialectal words.
Sawaf (2010)normalized the dialectal words in a hybrid (rule-based and statistical) MT system, by performing acombination of character- and morpheme-level map-pings.
They then translated the normalized sourceto English using a hybrid MT or alternatively aStatistical MT system.
They tested their methodon proprietary test sets, observing about 1 BLEUpoint (Papineni et al, 2002) increase on broadcastnews/conversation and about 2 points on web text.Salloum and Habash (2011) reduced the proportionof dialectal out-of-vocabulary (OOV) words also bymapping their affixed morphemes to MSA equiva-lents (but did not perform lexical mapping on theword stems).
They allowed for multiple morpho-logical analyses, passing them on to the MT systemin the form of a lattice.
They tested on a subset ofbroadcast news and broadcast conversation data setsconsisting of sentences that contain at least one re-gion marked as non-MSA, with an initial OOV rateagainst an MSA training corpus of 1.51%.
Theyobtained a 0.62 BLEU point gain.
Abo Bakr etal.
(2008) suggested another hybrid system to mapEgyptian Arabic to MSA, using morphological anal-ysis on the input and an Egyptian-MSA lexicon.Other work that has focused on tasks besides MTincludes that of Chiang et al (2006), who built aparser for spoken Levantine Arabic (LA) transcriptsusing an MSA treebank.
They used an LA-MSAlexicon in addition to morphological and syntac-tic rules to map the LA sentences to MSA.
Riesaand Yarowsky (2006) built a statistical morphologi-cal segmenter for Iraqi and Levantine speech tran-scripts, and showed that they outperformed rule-based segmentation with small amounts of training.Some tools exist for preprocessing and tokenizingArabic text with a focus on Dialectal Arabic.
For ex-ample, MAGEAD (Habash and Rambow, 2006) is amorphological analyzer and generator that can ana-lyze the surface form of MSA and dialect words intotheir root/pattern and affixed morphemes, or gener-ate the surface form in the opposite direction.Amazon?s Mechanical Turk (MTurk) is becom-ing an essential tool for creating annotated resourcesfor computational linguistics.
Callison-Burch andDredze (2010) provide an overview of various tasksfor which MTurk has been used, and offer a set ofbest practices for ensuring high-quality data.Zaidan and Callison-Burch (2011a) studied thequality of crowdsourced translations, by quantifyingthe quality of non-professional English translationsof 2,000 Urdu sentences that were originally trans-lated by the LDC.
They demonstrated a variety ofmechanisms that increase the translation quality ofcrowdsourced translations to near professional lev-els, with a total cost that is less than one tenth thecost of professional translation.Zaidan and Callison-Burch (2011b) created theArabic Online Commentary (AOC) dataset, a 52M-word monolingual dataset rich in dialectal content.Over 100k sentences from the AOC were annotatedby native Arabic speakers on MTurk to identify thedialect level (and dialect itself) in each, and the col-lected labels were used to train automatic dialectidentification systems.
Although a large numberof dialectal sentences were identified (41% of sen-tences), none were passed on to a translation phase.3 Data Collection and AnnotationFollowing Zaidan and Callison-Burch (2011a,b), weuse MTurk to identify Dialectal Arabic data and tocreate a parallel corpus by hiring non-professionaltranslators to translate the sentences that were la-beled as being dialectal.
We had Turkers performthree steps for us: dialect classification, sentencesegmentation, and translation.Since Dialectal Arabic is much less common inwritten form than in spoken form, the first challengeis to simply find instances of written Dialectal Ara-bic.
We draw from a large corpus of monolingualArabic text (approximately 350M words) that washarvested from the web by the LDC, largely fromweblog and online user groups.1 Before present-ing our data to annotators, we filter it to identify1Corpora: LDC2006E32, LDC2006E77, LDC2006E90,LDC2007E04, LDC2007E44, LDC2007E102, LDC2008E41,LDC2008E54, LDC2009E14, LDC2009E93.50MaghrebiEgyIraqiGulfOtherLevFigure 1: One possible breakdown of spoken Arabic intodialect groups: Maghrebi, Egyptian, Levantine, Gulf andIraqi.
Habash (2010) gives a breakdown along mostlythe same lines.
We used this map as an illustration forannotators in our dialect classification task (Section 3.1),with Arabic names for the dialects instead of English.segments most likely to be dialectal (unlike Zaidanand Callison-Burch (2011b), who did no such pre-filtering).
We eliminate documents with a large per-centage of non-Arabic or MSA words.
We thenretain documents that contain some number of di-alectal words, using a set of manually selected di-alectal words that was assembled by culling throughthe transcripts of the Levantine Fisher and Egyp-tian CallHome speech corpora.
After filtering, thedataset contained around 4M words, which we usedas a starting point for creating our Dialectal Arabic-English parallel corpus.3.1 Dialect ClassificationTo refine the document set beyond our keyword fil-tering heuristic and to label which dialect each doc-ument is written in, we hire Arabic annotators onMTurk to perform classification similar to Zaidanand Callison-Burch (2011b).
Annotators were askedto classify the filtered documents for being in MSAor in one of four regional dialects: Egyptian, Lev-antine, Gulf/Iraqi or Maghrebi, and were shown themap in Figure 1 to explain what regions each of thedialect labels corresponded to.
We allowed an addi-tional ?General?
dialect option for ambiguous docu-ments.
Unlike Zaidan and Callison-Burch, our clas-sification was applied to whole documents (corre-sponding to a user online posting) instead of individ-ual sentences.
To perform quality control, we useda set of documents for which correct labels wereknown.
We presented these 20% of the time, andDialect Classification HIT $10,064Sentence Segmentation HIT $1,940Translation HIT $32,061Total cost $44,065Num words translated 1,516,856Cost per word 2.9 cents/wordTable 1: The total costs for the three MTurk subtasks in-volved with the creation of our Dialectal Arabic-Englishparallel corpus.eliminated workers who did not correctly classifythem (2% of labels).Identifying the dialect of a text snippet can bechallenging in the absence of phonetic cues.
Wetherefore required 3 classifications from differentworkers for every document, and accepted a dialectlabel if at least two of them agreed.
The dialect dis-tribution of the final output was: 43% Gulf/Iraqi,28% Levantine, 11% Egyptian, and 16% could notbe classified.
MSA and the other labels accountedfor 2%.
We decided to translate only the Levantineand Egyptian documents, since the pool of MTurkworkers contained virtually no workers from Iraq orthe Gulf region.3.2 Sentence SegmentationSince the data we annotated was mostly user-generated informal web content, the existing punc-tuation was often insufficient to determine sentenceboundaries.
Since sentence boundaries are impor-tant for correct translation, we segmented passagesinto individual sentences using MTurk.
We only re-quired sentences longer than 15 words to be seg-mented, and allowed Turkers to split and rejoin atany point between the tokens.
The instructions weresimply to ?divide the Arabic text into individual sen-tences, where you believe it would be appropriateto insert a period.?
We also used a set of correctlysegmented passages for quality control, and scoredTurkers using a metric based on the precision andrecall of correct segmentation points.
The rejectionrate was 1.2%.3.3 Translation to EnglishFollowing Zaidan and Callison-Burch (2011a), wehired non-professional translators on MTurk totranslate the Levantine and Egyptian sentences into51Sentence Arabic EnglishData Set Pairs Tokens TokensMSA-150MW 8.0M 151.4M 204.4MDialect-1500KW 180k 1,545,053 2,257,041MSA-1300KW 71k 1,292,384 1,752,724MSA-Web-Tune 6,163 145,260 184,185MSA-Web-Test 5,454 136,396 172,357Lev-Web-Tune 2,600 20,940 27,399Lev-Web-Test 2,600 21,092 27,793Egy-Web-Test 2,600 23,671 33,565E-Facebook-Tune 3,351 25,130 34,753E-Facebook-Test 3,188 25,011 34,244Table 2: Statistics about the training/tuning/test datasetsused in our experiments.
The token counts are calculatedbefore MADA segmentation.English.
Among several quality control measures,we rendered the Arabic sentences as images to pre-vent Turkers from simply copying the Arabic textinto translation software.
We still spot checked thetranslations against the output of Google Translateand Bing Translator.
We also rejected gobbledygookgarbage translations that have a high percentage ofwords not found in an English lexicon.We quantified the quality of an individual Turker?stranslations in two ways: first by asking native Ara-bic speaker judges to score a sample of the Turker?stranslations, and second by inserting control sen-tences for which we have good reference translationsand measuring the Turker?s METEOR (Banerjee andLavie, 2005) and BLEU-1 scores (Papineni et al,2002).2 The rejection rate of translation assignmentswas 5%.
We promoted good translators to a re-stricted access ?preferred worker queue?.
They werepaid at a higher rate, and were required to translatecontrol passages only 10% of the time as opposedto 20% for general Turkers, thus providing us with ahigher translation yield for unseen data.Worker turnout was initially slow, but increasedquickly as our reputation for being reliable payerswas established; workers started translating largervolumes and referring their acquaintances.
We had121 workers who each completed 20 or more trans-lation assignments.
We eventually reached and sus-tained a rate of 200k words of acceptable quality2BLEU-1 provided a more reliable correlation with humanjudgment in this case that the regular BLEU score (which usesn-gram orders 1, .
.
.
, 4), given the limited size of the samplemeasured.translated per week.
Unlike Zaidan and Callison-Burch (2011a), who only translated 2,000 Urdu sen-tences, we translated sufficient volumes of DialectalArabic to train machine translation systems.
In total,we had 1.1M words of Levantine and 380k words ofEgyptian translated into English, corresponding toabout 2.3M words on the English side.Table 1 outlines the costs involved with creatingour parallel corpus.
The total cost was $44k, or$0.03/word ?
an order of magnitude cheaper thanprofessional translation.4 Experiments in Dialectal Arabic-EnglishMachine TranslationWe performed a set of experiments to contrast sys-tems trained using our dialectal parallel corpus withsystems trained on a (much larger) MSA-Englishparallel corpus.
All experiments use the same meth-ods for training, decoding and parameter tuning, andwe only varied the corpora used for training, tun-ing and testing.
The MT system we used is basedon a phrase-based hierarchical model similar to thatof Shen et al (2008).
We used GIZA++ (Och andNey, 2003) to align sentences and extract hierar-chical rules.
The decoder used a log-linear modelthat combines the scores of multiple feature scores,including translation probabilities, smoothed lexi-cal probabilities, a dependency tree language model,in addition to a trigram English language model.Additionally, we used 50,000 sparse, binary-valuedsource and target features based on Chiang et al(2009).
The English language model was trained on7 billion words from the Gigaword and from a webcrawl.
The feature weights were tuned to maximizethe BLEU score on a tuning set using the Expected-BLEU optimization procedure (Devlin, 2009).The Dialectal Arabic side of our corpus consistedof 1.5M words (1.1M Levantine and 380k Egyp-tian).
Table 2 gives statistics about the varioustrain/tune/test splits we used in our experiments.Since the Egyptian set was so small, we split it onlyto training/test sets, opting not to have a tuning set.The MSA training data we used consisted of Arabic-English corpora totaling 150M tokens (Arabic side).The MSA train/tune/test sets were constructed forthe DARPA GALE program.We report translation quality in terms of BLEU52Simple Segment MADA SegmentTraining Tuning BLEU OOV BLEU OOV ?BLEU ?OOVMSA-Web-TestMSA-150MW MSA-Web 26.21 1.69% 27.85 0.48% +1.64 -1.21%MSA-1300KW 21.24 7.20% 25.23 1.95% +3.99 -5.25%Egyptian-Web-TestDialect-1500KW Levantine-Web 18.55 6.31% 20.66 2.85% +2.11 -3.46%Levantine-Web-TestDialect-1500KW Levantine-Web 17.00 6.22% 19.29 2.96% +2.29 -3.26%Table 3: Comparison of the effect of morphological segmentation when translating MSA web text and DialectalArabic web text.
The morphological segmentation uniformly improves translation quality, but the improvements aremore dramatic for MSA than for Dialectal Arabic when comparing similarly-sized training corpora.Training Tuning BLEU OOV BLEU OOV BLEU OOVEgyptian-Web-Test Levantine-Web-Test MSA-Web-TestMSA-150MW MSA-Web 14.76 4.42% 11.83 5.53% 27.85 0.48%MSA-150MW Lev-Web 14.34 4.42% 12.29 5.53% 24.63 0.48%MSA-150MW+Dial-1500KW 20.09 2.04% 19.11 2.27% 24.30 0.45%Dialect-1500KW 20.66 2.85% 19.29 2.96% 15.53 3.70%Egyptian-360KW 19.04 4.62% 11.21 9.00% - -Levantine-360KW 14.05 7.11% 16.36 5.24% - -Levantine-1100KW 17.79 4.83% 19.29 3.31% - -Table 4: A comparison of translation quality of Egyptian, Levantine, andMSAweb text, using various training corpora.The highest BLEU scores are achieved using the full set of dialectal data (which combines Levantine and Egyptian),since the Egyptian alone is sparse.
For Levantine, adding Egyptian has no effect.
In both cases, adding MSA to thedialectal data results in marginally worse translations.score.3 In addition, we also report the OOV rate ofthe test set relative to the training corpus in each ex-perimental setups.4.1 Morphological DecompositionArabic has a complex morphology compared to En-glish.
Preprocessing the Arabic source by morpho-logical segmentation has been shown to improve theperformance of Arabic MT (Lee, 2004; Habash andSadat, 2006) by decreasing the size of the source vo-cabulary, and improving the quality of word align-ments.
The morphological analyzers that underliemost segmenters were developed for MSA, but thedifferent dialects of Arabic share many of the mor-phological affixes of MSA, and it is therefore notunreasonable to expect MSA segmentation to alsoimprove Dialect Arabic to English MT.
To test this,3We also computed TER (Snover et al, 2006) andMETEORscores, but omit them because they demonstrated similar trends.we ran experiments using the MADA morpholog-ical analyzer (Habash and Rambow, 2005).
Table3 shows the effect of applying segmentation to thetext, for both MSA and Dialectal Arabic.
The BLEUscore improves uniformly, although the improve-ments are most dramatic for smaller datasets, whichis consistent with previous work (Habash and Sadat,2006).
Morphological segmentation gives a smallergain on dialectal input, which could be due to twofactors: the segmentation accuracy likely decreasessince we are using an unmodified MSA segmenter,and there is higher variability in the written form ofdialect compared to MSA.
Given the significant, al-beit smaller gain on dialectal input, we use MADAsegmentation in all our experiments.4.2 Effect of Dialectal Training Data SizeWe next examine how the size of the dialectal train-ing data affects MT performance, and whether it isuseful to combine it with MSA training data.
We53ohtime(spaceomitted).Appeared withinapoem.11yAzmn?like you(corruptionofMSAmvlk).10mtlk"#$bymuch (corruptionofMSAbkvyr).11bktyr&'$()I missyou (spokentoafemale)?Egyptian.14wH$tyny/0'$12?The last name (Al-Na'oom) of aforumadmin.16AlnEwm?
:;0<?a looot(corruptionofMSAkvyrA).17ktyyyr&'''$?really/for real ?Levantine.31EnjdDE0FEnglishEquivalentCountTLArabicTable 5: The most frequent OOV?s (with counts ?
10) of the dialectal test sets against the MSA training data.Source (EGY):  ?
? ???
 ? !
!Transliteration: Ant btEml lh AElAn wlA Ayh?!!MSA-Sys.
Output: You are working for a declarationand not?Dial-Sys.
Output: You are making the advertisementfor him or what?Reference: Are you promoting it or what?!
!Source (EGY):  01?.
??
78 6 35 34?
?9:;?
<=>Transliteration: nfsY Atm}n Elyh bEd mA $AfAlSwrh dyMSA-Sys.
Output: Myself feel to see this image.Dial-Sys.
Output: I wish to check on him afterhe saw this picture.Reference: I wish to be sure that he is fineafter he saw this imagesSource (LEV):  ?0???
E7770 ?F?
G7HTransliteration: lhyk Aljw ktyyyr kwwwlMSA-Sys.
Output: God you the atmosphere.Dial-Sys.
Output: this is why the weather is so coolReference: This is why the weather is so coolSource (LEV):  ?L MG3 0?
;Transliteration: Twl bAlk Em nmzHMSA-Sys.
Output: Do you think about a joke long.Dial-Sys.
Output: Calm down we are kiddingReference: calm down, we are kiddingFigure 2: Examples of improvement in MT output whentraining on our Dialectal Arabic-English parallel corpusinstead of an MSA-English parallel corpus.Source (EGY):   	  ?Transliteration: qAltlp Tb tEAlY nEd ,MSA-Sys.
Output: Medicine almighty promise.Dial-Sys.
Output: She said, OK, come and thenReference: She told him, OK, lets count them ,Source (LEV):  "#$%& #'01 ?-%.
!
-,%+?
??
?
2Transliteration: fbqrA w>HyAnA bqDyhA Em>tslY mE rfqAtyMSA-Sys.
Output: I read and sometimes with gowith my uncle.Dial-Sys.
Output: So I read, and sometimes I spendtrying to make my self comfortwith my friendsReference: So i study and sometimes I spendthe time having fun with my friendsSource (LEV):  ?@ ?< ??
' => +?
&#:9?
B:C12D E???
%$?+GTransliteration: Allh ysAmHkn hlq kl wAHd TAlbqrb bykwn bdw ErwsMSA-Sys.
Output: God now each student near theBedouin bride.Dial-Sys.
Output: God forgive you, each one is aclose student would want the brideReference: God forgive you.
Is every oneasking to be close, want a bride!Figure 3: Examples of ambiguous words that are trans-lated incorrectly by the MSA-English system, but cor-rectly by the Dialectal Arabic-English system.54!
"!#!$!%"&""&' "&&' #&&' %&&' !(&&'!"#$%!
"#$%&'()*#"+"+,(-./0(/1(2/*345)*+,-./0123-./0123Egyptian web test!
"!#!$!%"&""&' "&&' #&&' %&&' !(&&'!"#$!
"#$%&'()*#"+"+,(-./0(/1(2/*345)*+,-./0123-./0123Levantine web testFigure 4: Learning curves showing the effects of increas-ing the size of dialectal training data, when combinedwith the 150M-word MSA parallel corpus, and whenused alone.
Adding the MSA training data is only use-ful when the dialectal data is scarce (200k words).started with a baseline system trained on the 150M-word MSA parallel corpus, and added various sizedportions of the dialect parallel corpus to it.
Figure 4shows the resulting learning curve, and compares itto the learning curve for a system trained solely onthe dialectal parallel corpus.
When only 200k wordsof dialectal data are available, combining it with the150M-word MSA corpus results in improved BLEUscores, adding 0.8?1.5 BLEU points.
When 400kwords or more of dialectal data are available, theMSA training data ceases to provide any gain, andin fact starts to hurt the performance.The performance of a system trained on the 1.5M-word dialectal data is dramatically superior to a sys-tem that uses only the 150M-word MSA data: +6.32BLEU points on the Egyptian test set, or 44% rela-tive gain, and +7.00 BLEU points on the Levantinetest set, or 57% relative gain (fourth line vs. secondline of Table 4).
In Section 4.4, we show that thosegains are not an artifact of the similarity between testand training datasets, or of using the same translatorpool to translate both sets.Inspecting the difference in the outputs of the Di-alectal vs. MSA systems, we see that the improve-ment in score is a reflection of a significant improve-ment in the quality of translations.
Figure 2 showsa few examples of sentences whose translations im-prove significantly using the Dialectal system.
Fig-ure 3 shows a particularly interesting category of ex-amples.
Many words are homographs, with differentmeanings (and usually different pronunciations) inMSA vs. one or more dialects.
The bolded tokensin the sentences in Figure 3 are examples of suchwords.
They are translated incorrectly by the MSAsystem, while the dialect system translates them cor-rectly.4 If we examine the most frequent OOVwordsagainst the MSA training data (Table 5), we find anumber of corrupted MSA words and names, butthat a majority of OOVs are dialect words.4.3 Cross-Dialect TrainingSince MSA training data appeared to have little ef-fect when translating dialectal input, we next inves-tigated the effect of training data from one dialect ontranslating the input of another dialect.
We trained asystem with the 360k-word Egyptian training subsetof our dialectal parallel corpus, and another systemwith a similar amount of Levantine training data.
Weused each system to translate the test set of the otherdialect.
As expected, a system performs better whenit translates a test set in the same dialect that it wastrained on (Table 4).That said, since the Egyptian training set is sosmall, adding the (full) Levantine training data im-proves performance (on the Egyptian test set) by1.62 BLEU points, compared to using only Egyp-tian training data.
In fact, using the Levantinetraining data by itself outperforms the MSA-trainedsystem on the Egyptian test set by more than 3BLEU points.
(For the Levantine test set, addingthe Egyptian training data has no affect, possiblydue to the small amount of Egyptian data.)
Thismay suggest that the mismatch between dialects isless severe than the mismatch between MSA anddialects.
Alternatively, the differences may be dueto the changes in genre from the MSA parallel cor-pus (which is mainly formal newswire) to the news-groups and weblogs that mainly comprise the dialec-tal corpus.4The word nfsY of Figure 2 (first word of second example)is also a homograph, as it means myself in MSA and I wish inDialectal Arabic.55Training Tuning BLEU OOVMSA-150MW Levantine-Web 13.80 4.16%MSA-150MW+Dialect-1500KW 16.71 2.43%Dialect-1500KW 15.75 3.79%MSA-150MW Egyptian-Facebook 15.80 4.16%MSA-150MW+Dialect-1500KW 18.50 2.43%Dialect-1500KW 17.90 3.79%Dialect-1000KW (random selection) Egyptian-Facebook 17.09 4.64%Dialect-1000KW (no Turker overlap) 17.10 4.60%Table 6: Results on a truly independent test set, consisting of data harvested from Egyptian Facebook pages that areentirely distinct from the our dialectal training set.
The improvements over the MSA baseline are still considerable:+2.9 BLEU points when no Facebook data is available for tuning and +2.7 with a Facebook tuning set.4.4 Validation on Independent Test DataTo eliminate the possibility that the gains are solelydue to similarity between the test/training sets in thedialectal data, we ran experiments using the samedialectal training data, but using truly independenttest/tuning data sets selected at random from a largerset of monolingual data that we collected from pub-lic Egyptian Facebook pages.
This data consists ofa set of original user postings and the subsequentcomments on each, giving the data a more conversa-tional style than our other test sets.
The postingsdeal with current Egyptian political affairs, sportsand other topics.
The test set we selected consistedof 25,011 words (3,188 comments and 427 postingsfrom 86 pages), and the tuning set contained 25,130words (3,351 comments and 415 conversations from58 pages).
We obtained reference translations forthose using MTurk as well.Table 6 shows that using the 1.5M-word dialectparallel corpus for training yields a 2 point BLEUimprovement over using the 150M-word MSA cor-pus.
Adding the MSA training data does yield animprovement, though of less than a single BLEUpoint.
It remains true that training on 1.5M wordsof dialectal data is better than training on 100 timesmore MSA parallel data.
The system performanceis sensitive to the tuning set choice, and improveswhen it matches the test set in genre and origin.To eliminate another potential source of artificialbias, we also performed an experiment where weremoved any training translation contributed by aTurker who translated any sentence in the EgyptianFacebook set, to eliminate translator bias.
For this,we were left with 1M words of dialect training data.This gave the same BLEU score as when trainingwith a randomly selected subset of the same size(bottom part of Table 6).4.5 Mapping from Dialectal Arabic to MSABefore Translating to EnglishGiven the large amount of linguistic resources thathave been developed for MSA over the past years,and the extensive research that was conducted onmachine translation from MSA to English and otherlanguages, an obvious research question is whetherDialectal Arabic is best translated to English by firstpivoting through MSA, rather than directly.
Theproximity of Dialectal Arabic to MSA makes themapping in principle easier than general machinetranslation, and a number of researchers have ex-plored this direction (Salloum and Habash, 2011).In this scenario, the dialectal source would first beautomatically transformed to MSA, using either arule-based or statistical mapping module.The Dialectal Arabic-English parallel corpus wecreated presents a unique opportunity to comparethe MSA-pivoting approach against direct transla-tion.
First, we collected equivalent MSA data forthe Levantine Web test and tuning sets, by askingTurkers to transform dialectal passages to valid andfluent MSA.
Turkers were shown example transfor-mations, and we encouraged fewer changes whereapplicable (e.g.
morphological rather than lexicalmapping), but allowed any editing operation in gen-eral (deletion, substitution, reordering).
Sample sub-missions were independently shown to native Ara-bic speaking judges, who confirmed they were validMSA.
A lowOOV rate also indicated the correctnessof the mappings.
By manually transforming the test56Training BLEU OOV BLEU OOV ?BLEU ?OOVDirect dialect trans Map to MSA then transMSA-150MW 12.29 5.53% 14.59 1.53% +2.30 -4.00%MSA-150MW+Dialect-200KW 15.37 3.59% 15.53 1.22% +0.16 -2.37%MSA-150MW+Dialect-400KW 16.62 3.06% 16.25 1.13% -0.37 -1.93%MSA-150MW+Dialect-800KW 17.83 2.63% 16.69 1.04% -1.14 -1.59%MSA-150MW+Dialect-1500KW 19.11 2.27% 17.20 0.98% -1.91 -1.29%Table 7: A comparison of the effectiveness of performing Levantine-to-MSA mapping before translating into English,versus translating directly from Levantine into English.
The mapping from Levantine to MSA was done manually, so itis an optimistic estimate of what might be done automatically.
Although initially helpful to the MSA baseline system,the usefulness of pivoting through MSA drops as more dialectal data is added, eventually hurting performance.dialectal sentence into MSA, we establish an opti-mistic estimate of what could be done automatically.Table 7 compares direct translation versus piv-oting to MSA before translating, using the base-line MSA-English MT system.5 The performanceof the system improves by 2.3 BLEU points withdialect-to-MSA pivoting, compared to attempting totranslate the untransformed dialectal input directly.As we add more dialectal training data, the BLEUscore when translating the untransformed dialecttest set improves rapidly (as seen previously in theMSA+Dialect learning curve in Figure 4), while theimprovement is less rapid when the text is first trans-formed to MSA.
Direct translation becomes a betteroption than mapping to MSA once 400k words of di-alectal data are added, despite the significantly lowerOOV rate with MSA-mapping.
This indicates thatsimple vocabulary coverage is not sufficient, anddata domain mismatch, quantified by more complexmatching patterns, is more important.5 ConclusionWe have described a process for building a Dialec-tal Arabic-English parallel corpus, by selecting pas-sages with a relatively high percentage of non-MSAwords from a monolingual Arabic web text corpus,then using crowdsourcing to classify them by di-alect, segment them into individual sentences andtranslate them to English.
The process was success-fully scaled to the point of reaching and sustaining arate of 200k translated words per week, at 1/10 thecost of professional translation.
Our parallel corpus,consisting of 1.5M words, was produced at a total5The systems in each column of the table are tuned consis-tently, using their corresponding tuning sets.cost of $40k, or roughly $0.03/word.We used the parallel corpus we constructed toanalyze the behavior of a Dialectal Arabic-EnglishMT system as a function of the size of the dialec-tal training corpus.
We showed that relatively smallamounts of training data render larger MSA corporafrom different data genres largely ineffective for thistest data.
In practice, a system trained on the com-bined Dialectal-MSA data is likely to give the bestperformance, since informal Arabic data is usuallya mixture of Dialectal Arabic and MSA.
An area offuture research is using the output of a dialect clas-sifier, or other features to bias the translation modeltowards the Dialectal or the MSA parts of the data.We also validated the models built from the di-alectal corpus by using them to translate an inde-pendent data set collected from Egyptian Facebookpublic pages.
We finally investigated using MSAas a ?pivot language?
for Dialectal Arabic-Englishtranslation, by simulating automatic dialect-to-MSAmapping using MTurk.
We obtained limited gainsfrom mapping the input to MSA, even when themapping is of good quality, and only at lower train-ing set sizes.
This suggests that the mismatch be-tween training and test data is an important aspect ofthe problem, beyond simple vocabulary coverage.The aim of this paper is to contribute to settingthe direction of future research on Dialectal ArabicMT.
The gains we observed from using MSA mor-phological segmentation can be further increasedwith dialect-specific segmenters.
Input preprocess-ing can also be used to decrease the noise of theuser-generated data.
Topic adaptation is another im-portant problem to tackle if the large MSA linguisticresources already developed are to be leveraged forDialectal Arabic-English MT.57AcknowledgmentsThis work was supported in part by DARPA/IPTOContract No.
HR0011-12-C-0014 under the BOLTProgram, and in part by the EuroMatrixPlus projectfunded by the European Commission (7th Frame-work Programme).
The views expressed are thoseof the authors and do not reflect the official policyor position of the Department of Defense or the U.S.Government.
Distribution Statement A (Approvedfor Public Release, Distribution Unlimited).ReferencesHitham M. Abo Bakr, Khaled Shaalan, and IbrahimZiedan.
2008.
A hybrid approach for converting writ-ten Egyptian colloquial dialect into diacritized Arabic.In The 6th International Conference on Informaticsand Systems, INFOS2008, Cairo, Egypt.Satanjeev Banerjee and Alon Lavie.
2005.
Meteor:An automatic metric for MT evaluation with improvedcorrelation with human judgments.
In In Proc.
of ACL2005 Workshop on Intrinsic and Extrinsic EvaluationMeasures for MT and/or Summarization, Ann Arbor,Michigan.Chris Callison-Burch and Mark Dredze.
2010.
Creatingspeech and language data with Amazon?s MechanicalTurk.
In Proceedings of the NAACL HLT 2010 Work-shop on Creating Speech and Language Data withAmazon?s Mechanical Turk, pages 1?12, Los Angeles,June.David Chiang, Mona Diab, Nizar Habash, Owen Ram-bow, and Safiullah Shareef.
2006.
Parsing Arabic di-alects.
In Proceedings of the Conference of the Eu-ropean Chapter of the Association for ComputationalLinguistics, Trento, Italy.David Chiang, Kevin Knight, and Wei Wang.
2009.11,001 new features for statistical machine translation.In NAACL ?09: Proceedings of the 2009 Human Lan-guage Technology Conference of the North AmericanChapter of the Association for Computational Linguis-tics, Boulder, Colorado.Jacob Devlin.
2009.
Lexical features for statistical ma-chine translation.
Master?s thesis, University of Mary-land, December.Mohamed Embarki and Moha Ennaji, editors.
2011.Modern Trends in Arabic Dialectology.
The Red SeaPress.Charles A. Ferguson.
1959.
Diglossia.
Word, 15:325?340.Nizar Habash and Owen Rambow.
2005.
Arabic tok-enization, part-of-speech tagging and morphologicaldisambiguation in one fell swoop.
In Proceedings ofthe 43th Annual Meeting of the Association for Com-putational Linguistics (ACL), Ann Arbor, Michigan.Nizar Habash and Owen Rambow.
2006.
MAGEAD: Amorphological analyzer and generator for the Arabicdialects.
In Proceedings of the 44th Annual Meeting ofthe Association for Computational Linguistics (ACL),Sydney, Australia.Nizar Habash and Fatiha Sadat.
2006.
Arabic prepro-cessing schemes for statistical machine translation.
InProceedings of the 2006 Human Language Technol-ogy Conference of the North American Chapter of theAssociation for Computational Linguistics, New York,New York.Nizar Y. Habash.
2010.
Introduction to Arabic NaturalLanguage Processing.
Morgan & Claypool.Young-Suk Lee.
2004.
Morphological analysis forstatistical machine translation.
In HLT-NAACL ?04:Proceedings of HLT-NAACL 2004, Boston, Mas-sachusetts.Franz Josef Och and Hermann Ney.
2003.
A system-atic comparison of various statistical alignment mod-els.
Computational Linguistics, 29(1):19?51.Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu.
2002.
Bleu: a method for automatic evalua-tion of machine translation.
In Proceedings of the 40thAnnual Meeting of the Association for ComputationalLinguistics (ACL), Philadelphia, PA.Jason Riesa and David Yarowsky.
2006.
Minimallysupervised morphological segmentation with applica-tions to machine translation.
In Proceedings of the 7thConf.
of the Association for Machine Translation in theAmericas (AMTA 2006), Cambridge, MA.Wael Salloum and Nizar Habash.
2011.
Dialectal to stan-dard Arabic paraphrasing to improve Arabic-Englishstatistical machine translation.
In Proceedings of the2011 Conference of Empirical Methods in NaturalLanguage Processing, Edinburgh, Scotland, UK.Hassan Sawaf.
2010.
Arabic dialect handling in hybridmachine translation.
In Proceedings of the 9th Conf.
ofthe Association for Machine Translation in the Ameri-cas (AMTA 2010), Denver, Colorado.Libin Shen, Jinxi Xu, and Ralph Weischedel.
2008.
Anew string-to-dependency machine translation algo-rithm with a target dependency language model.
InProceedings of the 46th Annual Meeting of the As-sociation for Computational Linguistics (ACL), pages577?585, Columbus, Ohio.Matthew Snover, Bonnie Dorr, Richard Schwartz, LinneaMicciulla, and Ralph Weischedel.
2006.
A study oftranslation error rate with targeted human annotation.In Proceedings of the 7th Conf.
of the Association forMachine Translation in the Americas (AMTA 2006),pages 223?231, Cambridge, MA.58Omar F. Zaidan and Chris Callison-Burch.
2011a.The Arabic online commentary dataset: an annotateddataset of informal Arabic with high dialectal content.In Proceedings of the 49th Annual Meeting of the As-sociation for Computational Linguistics: Human Lan-guage Technologies, pages 37?41, Portland, Oregon,June.Omar F. Zaidan and Chris Callison-Burch.
2011b.Crowdsourcing translation: Professional quality fromnon-professionals.
In Proceedings of the 49th AnnualMeeting of the Association for Computational Lin-guistics: Human Language Technologies, pages 1220?1229, Portland, Oregon, June.59
