Proceedings of the 12th Conference of the European Chapter of the ACL, pages 843?851,Athens, Greece, 30 March ?
3 April 2009. c?2009 Association for Computational LinguisticsFeature-based Method for Document Alignment inComparable News CorporaThuy Vu, Ai Ti Aw, Min ZhangDepartment of Human Language Technology, Institute for Infocomm Research1 Fusionopolis Way, #21-01 Connexis, South Tower, Singapore 138632{tvu, aaiti, mzhang}@i2r.a-star.edu.sgAbstractIn this paper, we present a feature-based me-thod to align documents with similar contentacross two sets of bilingual comparable cor-pora from daily news texts.
We evaluate thecontribution of each individual feature andinvestigate the incorporation of these diversestatistical and heuristic features for the task ofbilingual document alignment.
Experimentalresults on the English-Chinese and English-Malay comparable news corpora show thatour proposed Discrete Fourier Transform-based term frequency distribution feature isvery effective.
It contributes 4.1% and 8% toperformance improvement over Pearson?scorrelation method on the two comparablecorpora.
In addition, when more heuristic andstatistical features as well as a bilingual dic-tionary are utilized, our method shows an ab-solute performance improvement of 23.2%and 15.3% on the two sets of bilingual corpo-ra when comparing with a prior informationretrieval-based method.1 IntroductionThe problem of document alignment is describedas the task of aligning documents, news articlesfor instance, across two corpora based on contentsimilarity.
The groups of corpora can be in thesame or in different languages, depending on thepurpose of one?s task.
In our study, we attempt toalign similar documents across comparable cor-pora which are bilingual, each set written in adifferent language but having similar content anddomain coverage for different communicationneeds.Previous works on monolingual documentalignment focus on automatic alignment betweendocuments and their presentation slides or be-tween documents and their abstracts.
Kan (2007)uses two similarity measures, Cosine and Jac-card, to calculate the candidate alignment scorein his SlideSeer system, a digital library softwarethat retrieves documents and their narrated slidepresentations.
Daum?
and Marcu (2004) use aphrase-based HMM model to mine the alignmentbetween documents and their human-written ab-stracts.
The main purpose of this work is to in-crease the size of the training corpus for astatistical-based summarization system.The research on similarity calculation for mul-tilingual comparable corpora has attracted moreattention than monolingual comparable corpora.However, the purpose and scenario of theseworks are rather varied.
Steinberger et al (2002)represent document contents using descriptorterms of a multilingual thesaurus EUROVOC1,and calculate the semantic similarity based on thedistance between the two documents?
representa-tions.
The assignment of descriptors is trained bylog-likelihood test and computed by ????
?, Co-sine, and Okapi.
Similarly, Pouliquen et al(2004) use a linear combination of three types ofknowledge: cognates, geographical place namesreference, and map documents based on theEUROVOC.
The major limitation of these worksis the use of EUROVOC, which is a specific re-source workable only for European languages.Aligning documents across parallel corpora isanother area of interest.
Patry and Langlais (2005)use three similarity scores, Cosine, NormalizedEdit Distance, and Sentence Alignment Score, tocompute the similarity between two parallel doc-uments.
An Adaboost classifier is trained on a listof scored text pairs labeled as parallel or non-parallel.
Then, the learned classifier is used tocheck the correctness of each alignment candidate.Their method is simple but effective.
However,the features used in this method are only suitablefor parallel corpora as the measurement is mainlybased on structural similarity.
One goal of docu-ment alignment is for parallel sentence extractionfor applications like statistical machine transla-tion.
Cheung and Fung (2004) highlight that most1 EUROVOC is a multilingual thesaurus covering the fieldsin which the European Communities are active.843of the current sentence alignment models are ap-plicable for parallel documents, rather than com-parable documents.
In addition, they argue thatdocument alignment should be done before paral-lel sentence extraction.Tao and Zhai (2005) propose a general methodto extract comparable bilingual text without us-ing any linguistic resources.
The main feature ofthis method is the frequency correlation of wordsin different languages.
They assume that thosewords in different languages should have similarfrequency correlation if they are actually transla-tions of each other.
The association between twodocuments is then calculated based on this in-formation using Pearson?s correlation togetherwith two monolingual features 25??
, a termfrequency normalization (Stephan et al, 1994),and ???.
The main advantages of this approachare that it is purely statistical-based and it is lan-guage-independent.
However, its performancemay be compromised due to the lack of linguisticknowledge, particularly across corpora which arelinguistically very different.
Recently, Munteanu(2006) introduces a rather simple way to get thegroup of similar content document in multilin-gual comparable corpus by using the Lemur IRToolkit (Ogilvie and Callan, 2001).
This methodfirst pushes all the target documents into the da-tabase of the Lemur, and then uses a word-by-word translation of each source document as aquery to retrieve similar content target docu-ments.This paper will leverage on previous work,and propose and explore diverse range of fea-tures in our system.
Our document alignmentsystem consists of three stages: candidate genera-tion, feature extraction and feature combination.We verify our method on two set of bilingualnews comparable corpora English-Chinese andEnglish-Malay.
Experimental results show that1) when only using Fourier Transform-basedterm frequency, our method outperforms our re-implementation of Tao (2005)?s method by 4.1%and 8% for the top 100 alignment candidates and,2) when using all features, our method signifi-cantly outperforms our implementation of Mun-teanu?s (2006) method by 23.2% and 15.3%.The paper is organized as follows.
In section2, we describe the overall architecture of our sys-tem.
Section 3 discusses our improved frequencycorrelation-based feature, while Section 4 de-scribes in detail the document relationship heu-ristics used in our model.
Section 5 reports theexperimental results.
Finally, we conclude ourwork in section 6.2 System ArchitectureFig 1 shows the general architecture of our doc-ument alignment system.
It consists of threecomponents: candidate generation, feature ex-traction, and feature combination.
Our systemworks on two sets of monolingual corpora to de-rive a set of document alignments that are com-parable in their content.Fig 1.
Architecture for Document Alignment Model.2.1 Candidate GenerationLike many other text processing systems, thesystem first defines two filtering criteria to pruneout ?clearly bad?
candidates.
This will dramati-cally reduce the search space.
We implement thefollowing filers for this purpose:Date-Window Filter: As mentioned earlier,the data used for the present work are news cor-pora?a text genre that has very strong links withthe time element.
The published date of docu-ment is available in data, and can easily be usedas an indicator to evaluate the relation betweentwo articles in terms of time.
Similar to Muntea-nu?s (2006), we aim to constrain the number ofcandidates by assuming that documents withsimilar content should have publication dateswhich are fairly close to each other, even thoughthey reside in two different sets of corpora.
Byimposing this constraint, both the complexity andthe cost in computation can be reduced tremend-ously as the number of candidates would be sig-nificantly reduced.
For example, when a 1-daywindow size is set, this means that for a givensource document, the search for its target candi-dates is set within 3 days of the source document:the same day of publication, the day after, andthe day before.
With this filter, using the data ofone-month in our experiment, a reduction of 90%of all possible alignments can be achieved (sec-tion 5.1).
Moreover, with our evaluation data,844after filtering out document pairs using a 1-daywindow size, up to 81.6% for English-Chineseand 80.3% for English-Malay of the goldenalignments are covered.
If the window size isincreased to 5, the coverage is 96.6% and 95.6%for two language pairs respectively.Title-n-Content Filter: previous date windowfilter constrains the number of candidates basedpurely on temporal information without exploit-ing any knowledge of the documents?
contents.The number of candidates to be generated is thusdependent on the number of published articlesper day, instead of the candidates?
potential con-tent similarity.
For this reason, we introduceanother filter which makes use of document titlesto gauge content-wise cross document similarity.As document titles are available in news data, wecapitalize on words found in these documenttitles, favoring alignment candidates where atleast one of the title-words in the source docu-ment has its translation found in the content ofthe other target document.
This filter can reducea further 47.9% (English-Chinese) and 26.3%(English-Malay) of the remaining alignment can-didates after applying the date-window filter.2.2 Feature ExtractionThe second step extracts all the features for eachcandidate and computes the score for each indi-vidual feature function.
In our model, the featureset is composed of the Title-n-Content score(???
), Linguistic-Independent-Unit score (???
),and Monolingual Term Distribution similarity(???).
We will discuss all three features in sec-tions 3 and 4.2.3 Feature CombinationThe final score for each alignment candidate iscomputed by combining all the feature functionscores into a unique score.
In literature, there aremany methods concerning the estimation of theoverall score for a given feature set, which varyfrom supervised to unsupervised method.
Super-vised methods such as Support Vector Machine(SVM) and Maximum Entropy (ME) estimatethe weight of each feature based on training datawhich are then used to calculate the final score.However, these supervised learning-based me-thods may not be applicable to our proposed is-sue as we are motivated to build a languageindependent unsupervised system.
We simplytake a product of all normalized features to ob-tain one unique score.
This is because our fea-tures are probabilistically independent.
In ourimplementation, we normalize the scores to makethem less sensitive to the absolute value by tak-ing the logarithm ???.
?
as follows:???????
?
?????
?
?
?, ?
?
??
?
?
?1, ????(1)??
?
??
is a threshold for ?
to contribute posi-tively to the unique score.
In our experiment, weempirically choose ?
be 2.2 , and the thresholdfor ?
is 0.51828 (as ?
?
2.71828).3 Monolingual Term Distribution3.1 Baseline ModelThe main feature used in Tao and Zhai (2005) isthe frequency distribution similarity or frequencycorrelation of words in two given corpora.
It isassumed that frequency distributions of topically-related words in multilingual comparable corporaare often correlated due to the correlated cover-age of the same events.Let ?
?
??
?, ?
?, ?
, ???
and ?
?
??
?, ?
?, ?
, ??
?be the frequency distribution vectors of twowords ?
and ?
in two documents respectively.The frequency correlation of the two words iscomputed by Pearson?s Correlation Coefficientin (2).??
?, ??
??
????????
??
??????
?
?????????
???????
?????
??????
?????
???????
?????
??????
???
(2)The similarity of two documents is calculatedwith the addition of two features namely InverseDocument Frequency (???)
and 25??
term fre-quency normalization shown in the equation (3).???
?, ???
?
?
??????
?
??????
?
??
?, ??
?????,????25???
?, ???
?
25???
?, ???
(3)Where 25???
?, ??
is the word frequencynormalization for word ?
in document?
, and?????????
is the average length of a document.25???
?, ??
?
?????,?????,???????????|?|??????????
(4)It is noted that the key feature used by Tao andZhai (2005) is the ??
?, ??
score which dependspurely on statistical information.
Therefore, ourmotivation is to propose more features to link thesource and target documents more effectively fora better performance.3.2 Study on Frequency CorrelationWe further investigate the frequency correlation ofwords from comparable sets of corpora compris-ing three different languages using the above-defined model.845Fig 2.
Sample of frequency correlation for ?Bank Dunia?, ?World Bank?, and ?????
?.Fig 3.
Sample of frequency correlation for ?Dunia?, ?World?, and ???
?.Fig 4.
Sample of frequency correlation for ?Filipina?, ?Information Technology?, and ????
?.Using three months - May to July, 2006 ?
of dailynewspaper in Strait Times2 (in English), Zao Bao3(in Chinese), and Berita Harian4 (in Malay), weconduct the experiments described in the follow-ing Fig 2, Fig 3, and Fig 4 showing three differentcases of term or word correlation.
In these figures,the ?-axis denotes time and the ?-axis shows thefrequency distribution of the term or word.Multi-word versus Single-word: Fig 2illustrates that the distributions for multi-wordterm such as ?World Bank?, ?????
(WorldBank in Chinese)?, and ?Bank Dunia (WorldBank in Malay)?
in the three language corporaare almost similar because of the discriminativepower of that phrase.
The phrase has no varianceand contains no ambiguity.
On the other hand,the distributions for single words may have muchless similarity.2 http://www.straitstimes.com/ an English news agency inSingapore.
Source ?
Singapore Press Holdings Ltd.3 http://www.zaobao.com/ a Chinese news agency in Singa-pore.
Source ?
Singapore Press Holdings Ltd.4 http://cyberita.asia1.com.sg/ a Malay news agency in Sin-gapore.
Source ?
Singapore Press Holdings Ltd.Related Common Word: we also investigatethe similarity in frequency distribution for relatedcommon single words in the case of ?World?,???
(world in Chinese)?, and ?Dunia (world inMalay)?
as shown in Fig 3.
It can be observedthat the correlation of these common words is notas strong as that in the multi-word sample illu-strated in Fig 2.
The reason is that there are manyvariances of these common words, which usuallydo not have high discriminative power due to theambiguities presented within them.
Nonetheless,among these variances, there is still a small simi-lar distribution trends that can be detected, whichmay enable us to discover the associations be-tween them.Unrelated Common Word: Fig 4 shows thefrequency distribution of three unrelated com-mon words over the same three-month period.No correlation in distribution is found amongthem.00.050.10.150.21 11 21 31 41 51 61 71 81 91Bank?Dunia World?Bank ???
?00.010.020.031 11 21 31 41 51 61 71 81 91Dunia World ?
?00.050.10.151 11 21 31 41 51 61 71 81 91Filipina Information?Technology ??
?8463.3 Enhancement from Baseline Model3.3.1 Monolingual Term CorrelationDue to the inadequacy of the baseline?s purelystatistical approach, and our studies on the corre-lations of single, multiple and commonly appear-ing words, we propose using ?term?
or ?multi-word?
instead of ?single-word?
or ?word?
to cal-culate the similarity of term frequencydistribution between two documents.
Thispresents us with two main advantages.
Firstly,the smaller number of terms compared to thenumber of words present in any document wouldimply fewer possible document alignment pairsfor the system.
This increases the computationspeed remarkably.
To extract automatically thelist of terms in each document, we use the termextraction model from Vu et al (2008).
In corpo-ra used in our experiments, the average ratios ofword/term per document are 556/37, 410/28 and384/28 for English, Chinese, and Malay respec-tively.
The other advantage of using terms is thatterms are more distinctive than words as theycontain less ambiguity, thus enabling high corre-lation to be observed when compared with singlewords.3.3.2 Bilingual Dictionary IncorporationIn addition to using terms for the computation,we observed from equation (3) that the only mu-tual feature relating the two documents is thefrequency distribution coefficient ??
?, ??
.
It islikely that the alignment performance could beenhanced if more features relating the two doc-uments are incorporated.Following that, we introduce a linguistic fea-ture, ?????????
?, ??
, to the baseline model toenhance the association between two documents.This feature involves the comparison of thetranslations of words within a particular term inone language, and the presence of these transla-tions in the corresponding target language term.If more translations obtained from a bilingualdictionary of words within a term are found inthe term extracted from the other language?sdocument, it is more likely that the 2 bilingualterms are translations of each other.
This featurecounts the number of word translation found be-tween the two terms, as described in the follow-ing.
Let ??
and ??
be the term list of ??
and ?
?respectively, the similarity score in our model is:??????
?, ???
?
?
??????
?
??????
?
??
?, ??
?????,?????????????
?, ??
?
25???
?, ???
?
25???
?, ???
(5)3.3.3 Distribution Similarity Measurementusing Monolingual TermFinally, we apply the results of time-series re-search to replace Pearson?s correlation which isused in the baseline model, in our calculation ofthe similarity score of two frequency distribu-tions.
A popular technique for time sequencematching is to use Discrete Fourier Transform(??? )
(Agrawal et al 1993).
More recently,Klementiev and Roth (2006) also use F-index(Hetland, 2004), a score using ??
?, to calculatethe time distribution similarity.
In our model, weassume that the frequency chain of a word is asequence, and calculate ???
score for eachchain by the following formula:??
?
???.
???????????
(6)In time series research, it is proven that onlythe first few ?
coefficients of a ???
chain arestrong and important for comparison (Agrawal etal, 1993).
Our experiments in section 5 show thatthe best value for ?
is 7 for both language pairs.??
?, ??
?????????
?
?????????????
(7)The ??
?, ??
in equation (5) is replaced by??
?, ??
in equation (8) to calculate the Monolin-gual Term Distribution (???)
score.4 Document Relationship HeuristicsBesides the ??
?, we also propose two heuristic-based features that focus directly on therelationship between two multilingual documents,namely the Title-n-Content score?
???
, whichmeasures the relationship between the title andcontent of a document pair, and Linguistic Inde-pendent Unit score ?
???
, which make use oforthographic similarity between unit of words forthe different languages.4.1 Title-n-Content Score (???
)Besides being a filter for removing bad align-ment candidates, ???
is also incorporated as afeature in the computation of document align-ment score.
In the corpora used, in most docu-ments, ?title?
does reveal the main topic of adocument.
The use of words in a news title is??????
?, ???
?
?
??????
?
??????????,?????
??
?, ??
?
?????????
?, ???
25???
?, ???
?
25???
?, ???
(8)847typically concise and conveys the essence of theinformation in the document.
Thus, a high ??
?score would indicate a high likelihood of similar-ity between two bilingual documents.
Therefore,we use ???
as a quantitative feature in our fea-ture set.
Function ???
?, ??
checks whether thetranslation of a word in a document?s title isfound in the content of its aligned document:???
?, ??
?
?1,     translation of ?
is in ?0,     else                                (9)The ???
score of document ??
and ??
is cal-culated by the following formula:?????
?, ???
??
????
?, ??????T??
?
????
?, ??????T?
(10)Where ??
and ??
are the content of document??
and ??
; and ??
and ??
are the set of title wordsof two documents.In addition, this method speeds up the align-ment process without compromising perfor-mance when compared with the calculationbased only on contents on both sides.4.2 Linguistic Independent Unit (???
)Linguistic Independent Unit score (LIU) is de-fined as the piece of information, which is writ-ten in the same way for different languages.
Thefollowing highlight the number 25, 11, and 50 aslinguistic-independent-units for the two sen-tences.English: Between Feb 25 and March 11 thisyear, she used counterfeit $50 notes 10 times topay taxi fares ranging from $2.50 to $4.20.Chinese:?????????????
2 ?25 ??
3?
11 ???
50 ???????????
2?
5??
4?
2?????
?5 Experiment and Evaluation5.1 Experimental SetupThe experiments were conducted on two sets ofcomparable corpora namely English-Chinese andEnglish-Malay.
The data are from three newspublications in Singapore: the Strait Times (ST,English), Lian He Zao Bao (ZB, Chinese), andBerita Harian (BH, Malay).
Since these languag-es are from different language families 5 , ourmodel can be considered as language indepen-dent.5 English is in Indo-European; Chinese is in Sino-Tibetan;Malay is in Austronesian family [Wikipedia].The evaluation is conducted based on a set ofmanually aligned documents prepared by a groupof bilingual students.
It is done by carefully read-ing through each article in the month of June(2006) for both sets of corpora and trying to findarticles of similar content in the other languagewithin the given time window.
Alignment isbased on similarity of content where the samestory or event is mentioned.
Any two bilingualarticles with at least 50% content overlapping areconsidered as comparable.
This set of referencedata is cross-validated between annotators.
Table1 shows the statistics of our reference data fordocument alignment.Language pair ST ?
ZB ST ?
BHDistinct source 396 176Distinct target 437 175Total alignments 438 183Table 1.
Statistics on evaluation data.Note that although there are 438 alignmentsfor ST-ZB, the number of unique ST articles are396, implying that the mapping is not one-to-one.5.2 Evaluation MetricsEvaluation is performed on two levels to reflectperformance from two different perspectives.
?Macro evaluation?
is conducted to assess thecorrectness of the alignment candidates giventheir rank among all the alignment candidates.
?Micro evaluation?
concerns about the correctnessof the aligned documents returned for a givensource document.Macro evaluation: we present the perfor-mance for macro evaluation using average preci-sion.
It is used to evaluate the performance of aranked list and gives higher score for the list thatreturns more correct alignment in the top.Micro evaluation: for micro evaluation, weevaluate the F-Score, calculated from recall andprecision, based on the number of correct align-ments for the top of alignment candidates foreach source document.5.3 Experiment and ResultFirst we implement the method of Tao and Zhai(2005) as the baseline.
Basically, this methoddoes not depend on any linguistic resources andcalculates the similarity between two documentspurely by comparing all possible pairs of words.In addition to this, we also implement Muntea-nu?s (2006) method which uses Okapi scoringfunction from the Lemur Toolkit (Ogilvie and848Callan, 2001) to obtain the similarity score.
Thisapproach relies heavily on bilingual dictionaries.To assess performances more fairly, the resultfrom baseline method of Tao and Zhai are com-pared against the results of the following list ofincremental approaches: the baseline (A); thebaseline using term instead of word (B); replac-ing ??
?, ??
by ??
?, ??
for ???
feature, with andwithout bilingual dictionaries in (C) and (D) re-spectively; and including ???
and ???
for ourfinal model in (E).
Our model is also comparedour model with results from the implementationof Munteanu (2006) using Okapi (F), and theresults from a combination of our model withOkapi (G).
Table 2 and Table 3 show the expe-rimental results for two language pairs English ?Chinese (ST-ZB) and English ?
Malay (ST-BH),respectively.
Each row displays the result of eachexperiment at a certain cut-off among the topreturned alignments.
The ?Top?
columns reflectthe cut-off threshold.The first three cases (A), (B) and (C), whichdo not rely on linguistic resources, suggest thatour new features lead to better performance im-provement over the baseline.
It can be seen thatthe use of term and ???
significantly improvesthe performance.
The improvement indicated bya sharp increase in all cases from (C) to (D)shows that dictionaries can indeed help ???
fea-tures.Based on the result of (E), our final modelsignificantly outperforms the model of Munteanu(F) in both macro and micro evaluation.
It isnoted that our features rely less heavily on dic-tionaries as it only makes use of this resource totranslate term words and title words of a docu-ment while Munteanu (2006) needs to translateentire documents, exclude stopword, and relyingon an IR system.
It is also observed that the per-formance of (G) shows that although the incor-poration of Okapi score in our final model (E)improves the average precision performance ofST-ZB slightly, it does not appear to be helpfulfor our ST-BH data.
However, Okapi does helpin the F-Measure on both corpora.Pair?
Strait?Times???Zao?Bao?Level?
Top?
A?
B?
C?
D?
E?
F?
G?Ave/Precision?Macro?
50?
0.042?
0.083?
0.08?
0.559?
0.430?
0.209?
0.508?100?
0.042?
0.069?
0.083?
0.438?
0.426?
0.194?
0.479?200?
0.025?
0.069?
0.110?
0.342?
0.396?
0.153?
0.439?500?
0.025?
0.054?
0.110?
0.270?
0.351?
0.111?
0.376?F?Measure?Micro?1?
0.005?
0.007?
0.009?
0.297?
0.315?
0.157?
0.333?2?
0.006?
0.005?
0.013?
0.277?
0.286?
0.133?
0.308?5?
0.005?
0.006?
0.009?
0.200?
0.190?
0.096?
0.206?10?
0.005?
0.005?
0.007?
0.123?
0.119?
0.063?
0.126?20?
0.006?
0.008?
0.007?
0.073?
0.074?
0.038?
0.076?Table 2.
Performance of Strait Times ?
Zao Bao.Pair?
Strait?Times???Berita?Harian?Level?
Top?
A?
B?
C?
D?
E?
F?
G?Ave/Precision?Macro?
50?
0.000?
0.000?
0.000?
0.514?
0.818?
0.000?
0.782?100?
0.000?
0.000?
0.080?
0.484?
0.759?
0.052?
0.729?200?
0.000?
0.008?
0.090?
0.443?
0.687?
0.073?
0.673?500?
0.005?
0.008?
0.010?
0.383?
0.604?
0.078?
0.591?F?Measure?Micro?1?
0.000?
0.000?
0.005?
0.399?
0.634?
0.119?
0.650?2?
0.000?
0.004?
0.010?
0.340?
0.515?
0.128?
0.515?5?
0.002?
0.005?
0.010?
0.205?
0.270?
0.105?
0.273?10?
0.004?
0.014?
0.013?
0.130?
0.150?
0.076?
0.150?20?
0.006?
0.017?
0.017?
0.074?
0.078?
0.043?
0.078?Table 3.
Performance of Strait Times ?
Berita Harian.8495.4 DiscussionIt can be seen from Table 2 and Table 3 that byexploiting the frequency distribution of termsusing Discrete Fourier Transform instead ofwords on Pearson?s Correlation, performance isnoticeably improved.
Fig 5 shows the incremen-tal improvement of our model for top-200 andtop-2 alignments using macro and micro evalua-tion respectively.
The sharp increase can be seenin Fig 5 from point (C) onwards.Fig 5.
Step-wise improvement at top-200 for macroand top-2 for micro evaluation.Fig 6 compares the performance of our systemwith Tao and Zhai (2005) and Munteanu (2006).It is shown that our systems outperform thesetwo systems under the same experimentalparameters.
Moreover, even without the use ofdictionaries, our system?s performance on ST-BH data is much better than Munteanu?s (2006)on the same data.Fig 6.
System comparison for ST-ZB and ST-BH attop-500 for macro and top-5 for micro evaluation.We find that dictionary usage contributesmuch more to performance improvement in ST-BH compared to that in ST-ZB.
We attribute thisto the fact that the feature LIU already contri-butes markedly to the increase in the perfor-mance of ST-BH.
As a result, it is harder to makefurther improvements even with the applicationof bilingual dictionaries.6 Conclusion and Future WorkIn this paper, we propose a feature based modelfor aligning documents from multilingual com-parable corpora.
Our feature set is selected basedon the need for a method to be adaptable to newlanguage-pairs without relying heavily on lin-guistic resources, unsupervised learning strategy.Thus, in the proposed method we make use ofsimple bilingual dictionaries, which are ratherinexpensive and easily obtained nowadays.
Wealso explore diverse features, including Mono-lingual Term Distribution (???
), Title-and-Content (???
), and Linguistic Independent Unit(???)
and measure their contributions in an in-cremental way.
The experiment results show thatour system can retrieve similar documents fromtwo comparable corpora much better than usingan information retrieval, such as that used byMunteanu (2006).
It also performs better than aword correlation-based method such as Tao?s(2005).Besides document alignment as an end, thereare many tasks that can directly benefit fromcomparable corpora with documents that arewell-aligned.
These include sentence alignment,term alignment, and machine translation, espe-cially statistical machine translation.
In the future,we aim to extract other valuable informationfrom comparable corpora which benefits fromcomparable documents.AcknowledgementsWe would like to thank the anonymous review-ers for their many constructive suggestions forimproving this paper.
Our thanks also go to Ma-hani Aljunied for her contributions to the linguis-tic assessment in our work.ReferencesPercy Cheung and Pascale Fung.
2004.
SentenceAlignment in Parallel, Comparable, and Quasi-comparable Corpora.
In Proceedings of 4th Inter-national Conference on Language Resources andEvaluation (LREC).
Lisbon, Portugal.Hal Daume III and Daniel Marcu.
2004.
A Phrase-Based HMM Approach to Document/AbstractAlignment.
In Proceedings of Empirical Methodsin Natural Language Processing (EMNLP).
Spain.00.10.20.30.40.50.60.70.8A B C D EST??
ZB?A/Prec ST??
ZB?F?ScoreST??
BH?A/Prec ST??
BH?F?Score00.10.20.30.40.50.60.7A/Prec F?Score A/Prec F?ScoreST??
ZB ST??
BHTao?and?Zhai?
(2005) Our?System?w/o?DictOur?System?w?Dict Munteanu?
(2006)850Min-Yen Kan. 2007.
SlideSeer: A Digital Library ofAligned Document and Presentation Pairs.
In Pro-ceedings of the Joint Conference on Digital Libra-ries (JCDL).
Vancouver, Canada.Soto Montalvo, Raquel Martinez, Arantza Casillas,and Victor Fresno.
2006.
Multilingual DocumentClustering: a Heuristic Approach Based on Cog-nate Named Entities.
In Proceedings of the 21st In-ternational Conference on ComputationalLinguistics and the 44th Annual Meeting of theACL.Stephen E. Robertson, Steve Walker, Susan Jones,Micheline Hancock-Beaulieu, and Mike Gatford.1994.
Okapi at TREC-3.
In Proceedings of theThird Text REtrieval Conference (TREC 1994).Gaithersburg, USA.Dragos Stefan Munteanu.
2006.
Exploiting Compara-ble Corpora.
PhD Thesis.
Information Sciences In-stitute, University of Southern California.
USA.Ogilvie, P., and Callan, J.
2001.
Experiments usingthe Lemur toolkit.
In Proceedings of the 10th TextREtrieval Conference (TREC).Alexandre Patry and Philippe Langlais.
2005.
Auto-matic Identification of Parallel Documents withlight or without Linguistics Resources.
In Proceed-ings of 18th Annual Conference on Artificial Intel-ligent.Bruno Pouliquen, Ralf Steinberger, Camelia Ignat,Emilia Kasper, and Irina Temnikova.
2004.
Multi-lingual and Cross-lingual news topic tracking.
InProceedings of the 20th International Conferenceon Computational Linguistics (COLING).Ralf Steinberger, Bruno Pouliquen, and Johan Hag-man.
2002.
Cross-lingual Document SimilarityCalculation Using the Multilingual ThesaurusEUROVOC.
Computational Linguistics and Intel-ligent Text Processing.Tao Tao and ChengXiang Zhai.
2005.
Mining Com-parable Bilingual Text Corpora for Cross-Language Information Integration.
In Proceedingsof the 2005 ACM SIGKDD International Confe-rence on Knowledge Discovery and Data Mining.Thuy Vu, Ai Ti Aw and Min Zhang.
2008.
Term ex-traction through unithood and termhood unification.In Proceedings of the 3rd International Joint Con-ference on Natural Language Processing(IJCNLP-08).
Hyderabad, India.ChengXiang Zhai and John Lafferty.
2001.
A study ofsmoothing methods for language models applied toAd Hoc information retrieval.
In Proceedings ofthe 24th annual international ACM SIGIR confe-rence on Research and development in informationretrieval.
Louisiana, United States.R.
Agrawal, C. Faloutsos, and A. Swami.
1993.
Effi-cient similarity search in sequence databases.
InProceedings of the 4th International Conference onFoundations of Data Organization and Algorithms.Chicago, United States.Magnus Lie Hetland.
2004.
A survey of recent me-thods for efficient retrieval of similar time se-quences.
In Data Mining in Time Series Databases.World Scientific.Alexandre Klementiev and Dan Roth.
2006.
WeaklySupervised Named Entity Transliteration and Dis-covery from Multilingual Comparable Corpora.
InProceedings of the 21st International Conferenceon Computational Linguistics and the 44th AnnualMeeting of the ACL.851
