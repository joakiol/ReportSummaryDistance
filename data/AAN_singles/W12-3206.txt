Proceedings of the ACL-2012 Special Workshop on Rediscovering 50 Years of Discoveries, pages 55?65,Jeju, Republic of Korea, 10 July 2012. c?2012 Association for Computational LinguisticsExtracting glossary sentences from scholarly articles:A comparative evaluation of pattern bootstrapping and deep analysisMelanie Reiplinger1 Ulrich Scha?fer2 Magdalena Wolska1?1Computational Linguistics, Saarland University, D-66041 Saarbru?cken, Germany2DFKI Language Technology Lab, Campus D 3 1, D-66123 Saarbru?cken, Germany{mreiplin,magda}@coli.uni-saarland.de, ulrich.schaefer@dfki.deAbstractThe paper reports on a comparative study oftwo approaches to extracting definitional sen-tences from a corpus of scholarly discourse:one based on bootstrapping lexico-syntacticpatterns and another based on deep analysis.Computational Linguistics was used as the tar-get domain and the ACL Anthology as thecorpus.
Definitional sentences extracted for aset of well-defined concepts were rated by do-main experts.
Results show that both meth-ods extract high-quality definition sentencesintended for automated glossary construction.1 IntroductionSpecialized glossaries serve two functions: Firstly,they are linguistic resources summarizing the ter-minological basis of a specialized domain.
Sec-ondly, they are knowledge resources, in that theyprovide definitions of concepts which the terms de-note.
Glossaries find obvious use as sources of ref-erence.
A survey on the use of lexicographical aidsin specialized translation showed that glossaries areamong the top five resources used (Dura?n-Mun?oz,2010).
Glossaries have also been shown to facil-itate reception of texts and acquisition of knowl-edge during study (Weiten et al, 1999), while self-explanation of reasoning by referring to definitionshas been shown to promote understanding (Alevenet al, 1999).
From a machine-processing point ofview, glossaries may be used as input for domainontology induction; see, e.g.
(Bozzato et al, 2008).
?Corresponding authorThe process of glossary creation is inherently de-pendent on expert knowledge of the given domain,its concepts and language.
In case of scientific do-mains, which constantly evolve, glossaries need tobe regularly maintained: updated and continuallyextended.
Manual creation of specialized glossariesis therefore costly.
As an alternative, fully- andsemi-automatic methods of glossary creation havebeen proposed (see Section 2).This paper compares two approaches to corpus-based extraction of definitional sentences intendedto serve as input for a specialized glossary of a scien-tific domain.
The bootstrapping approach acquireslexico-syntactic patterns characteristic of definitionsfrom a corpus of scholarly discourse.
The deep ap-proach uses syntactic and semantic processing tobuild structured representations of sentences basedon which ?is-a?-type definitions are extracted.
Inthe present study we used Computational Linguis-tics (CL) as the target domain of interest and theACL Anthology as the corpus.Computational Linguistics, as a specialized do-main, is rich in technical terminology.
As a cross-disciplinary domain at the intersection of linguistics,computer science, artificial intelligence, and mathe-matics, it is interesting as far as glossary creationis concerned in that its scholarly discourse rangesfrom descriptive informal to formal, including math-ematical notation.
Consider the following two de-scriptions of Probabilistic Context-Free Grammar(PCFG):(1) A PCFG is a CFG in which each productionA ?
?
in the grammar?s set of productionsR is associated with an emission probabil-55ity P (A ?
?)
that satisfies a normalizationconstraint??:A??
?RP (A?
?)
= 1and a consistency or tightness constraint [...](2) A PCFG defines the probability of a stringof words as the sum of the probabilities ofall admissible phrase structure parses (trees)for that string.While (1) is an example of a genus-differentiadefinition, (2) is a valid description of PCFG whichneither has the typical copula structure of an ?is-a?-type definition, nor does it contain the level of de-tail of the former.
(2) is, however, well-usable for aglossary.
The bootstrapping approach extracts defi-nitions of both types.
Thus, at the subsequent glos-sary creation stage, alternative entries can be used togenerate glossaries of different granularity and for-mal detail; e.g., targeting different user groups.Outline.
Section 2 gives an overview of relatedwork.
Section 3 presents the corpora and the prepro-cessing steps.
The bootstrapping procedure is sum-marized in Section 4 and deep analysis in Section 5.Section 6 presents the evaluation methodology andthe results.
Section 7 presents an outlook.2 Related WorkMost of the existing definition extraction methods?
be it targeting definitional question answering orglossary creation ?
are based on mining part-of-speech (POS) and/or lexical patterns typical of def-initional contexts.
Patterns are then filtered heuris-tically or using machine learning based on featureswhich refer to the contexts?
syntax, lexical content,punctuation, layout, position in discourse, etc.DEFINDER (Muresan and Klavans, 2002), a rule-based system, mines definitions from online medicalarticles in lay language by extracting sentences us-ing cue-phrases, such as ?x is the term for y?, ?xis defined as y?, and punctuation, e.g., hyphens andbrackets.
The results are analyzed with a statisticalparser.
Fahmi and Bouma (2006) train supervisedlearners to classify concept definitions from medi-cal pages of the Dutch Wikipedia using the ?is a?pattern and apply a lexical filter (stopwords) to theclassifier?s output.
Besides other features, the posi-tion of a sentence within a document is used, which,due to the encyclopaedic text character of the cor-pus, allows to set the baseline precision at above75% by classifying the first sentences as definitions.Westerhout and Monachesi (2008) use a complex setof grammar rules over POS, syntactic chunks, andentire definitory contexts to extract definition sen-tences from an eLearning corpus.
Machine learn-ing is used to filter out incorrect candidates.
Gaudioand Branco (2009) use only POS information in asupervised-learning approach, pointing out that lex-ical and syntactic features are domain and languagedependent.
Borg et al (2009) use genetic program-ming to learn rules for typical linguistic forms ofdefinition sentences in an eLearning corpus and ge-netic algorithms to assign weights to the rules.
Ve-lardi et al (2008) present a fully-automatic end-to-end methodology of glossary creation.
First, Term-Extractor acquires domain terminology and Gloss-Extractor searches for definitions on the web usinggoogle queries constructed from a set of manuallylexical definitional patterns.
Then, the search resultsare filtered using POS and chunk information as wellas term distribution properties of the domain of in-terest.
Filtered results are presented to humans formanual validation upon which the system updatesthe glossary.
The entire process is automated.Bootstrapping as a method of linguistic patterninduction was used for learning hyponymy/is-a re-lations already in the early 90s by Hearst (1992).Various variants of the procedure ?
for instance, ex-ploiting POS information, double pattern-anchors,semantic information ?
have been recently pro-posed (Etzioni et al, 2005; Pantel and Pennacchiotti,2006; Girju et al, 2006; Walter, 2008; Kozareva etal., 2008; Wolska et al, 2011).
The method pre-sented here is most similar to Hearst?s, however, weacquire a large set of general patterns over POS tagsalone which we subsequently optimize on a smallmanually annotated corpus subset by lexicalizing theverb classes.3 The Corpora and PreprocesssingThe corpora.
Three corpora were used in thisstudy.
At the initial stage two development corporawere used: a digitalized early draft of the Jurafsky-56Martin textbook (Jurafsky and Martin, 2000) and theWeScience Corpus, a set of Wikipedia articles in thedomain of Natural Language Processing (Ytrest?l etal., 2009).1 The former served as a source of seeddomain terms with definitions, while the latter wasused for seed pattern creation.For acquisition of definitional patterns and pat-tern refinement we used the ACL Anthology, a dig-ital archive of scientific papers from conferences,workshops, and journals on Computational Linguis-tics and Language Technology (Bird et al, 2008).2The corpus consisted of 18,653 papers published be-tween 1965 and 2011, with a total of 66,789,624tokens and 3,288,073 sentences.
This corpus wasalso used to extract sentences for the evaluation us-ing both extraction methods.Preprocessing.
The corpora have been sentenceand word-tokenized using regular expression-basedsentence boundary detection and tokenization tools.Sentences have been part-of-speech tagged using theTnT tagger (Brants, 2000) trained on the Penn Tree-bank (Marcus et al, 1993).3Next, domain terms were identified using the C-Value approach (Frantzi et al, 1998).
C-Value isa domain-independent method of automatic multi-word term recognition that rewards high frequencyand high-order n-gram candidates, but penalizesthose which frequently occur as sub-strings of an-other candidate.
10,000 top-ranking multi-word to-ken sequences, according to C-Value, were used.Domain terms.
The set of domain terms was com-piled from the following sub-sets: 1) the 10,000 au-tomatically identified multi-word terms, 2) the setof terms appearing on the margins of the Jurafsky-Martin textbook; the intuition being that these aredomain-specific terms which are likely to be definedor explained in the text along which they appear,3) a set of 5,000 terms obtained by expanding fre-quent abbreviations and acronyms retrieved from theACL Anthology corpus using simple pattern match-ing.
The token spans of domain terms have beenmarked in the corpora as these are used in the courseof definition pattern acquisition (Section 4.2).1http://moin.delph-in.net/WeScience2http://aclweb.org/anthology/3The accuracy of tokenization and tagging was not verified.Seedtermsmachine translation language modelneural network reference resolutionfinite(-| )state automaton hidden markov modelspeech synthesis semantic role label(l)?ingcontext(-| )free grammar ontologygenerative grammar dynamic programmingmutual informationSeedpatternsT .
* (is|are|can be) usedT .
* calledT .
* (is|are) composedT .
* involv(es|ed|e|ing)T .
* perform(s|ed|ing)?T \( or .*?
\)task of .
* T .*?
isterm T .*?
refer(s|red|ring)?Table 1: Seed domain terms (top) and seed patterns (bot-tom) used for bootstrapping; T stands for a domain term.4 Bootstrapping Definition PatternsBootstrapping-based extraction of definitional sen-tences proceeds in two stages: First, aiming at recall,a large set of lexico-syntactic patterns is acquired:Starting with a small set of seed terms and patterns,term and pattern ?pools?
are iteratively augmentedby searching for matching sentences from the ACLAnthology while acquiring candidates for definitionterms and patterns.
Second, aiming at precision,general patterns acquired at the first stage are sys-tematically optimized on set of annotated extracteddefinitions.4.1 Seed Terms and Seed PatternsAs seed terms to initialize pattern acquisition, wechose terms which are likely to have definitions.Specifically, from the top-ranked multi-word terms,ordered by C-value, we selected those which werealso in either the Jurafsky-Martin term list or the listof expanded frequent abbreviations.
The resulting13 seed terms are shown in the top part of Table 1.Seed definition patterns were created by inspect-ing definitional contexts in the Jurafsky-Martin andWeScience corpora.
First, 12 terms from Jurafsky-Martin and WeScience were selected to find domainterms with which they co-occurred in simple ?is-a?patterns.
Next, the corpora were searched again tofind sentences in which the term pairs in ?is-a?
rela-tion occur.
Non-definition sentences were discarded.57Finally, based on the resulting definition sentences,22 seed patterns were constructed by transformingthe definition phrasings into regular expressions.
Asubset of the seed phrases extracted in this way isshown in the bottom part of Table 1.44.2 Acquiring PatternsPattern acquisition proceeds in two stages: First,based on seed sets, candidate defining terms arefound and ranked.
Then, new patterns are acquiredby instantiating existing patterns with pairs of likelyco-occurring domain terms, searching for sentencesin which the term pairs co-occur, and creating POS-based patterns.
These steps are summarized below.Finding definiens candidates.
Starting with a setof seed terms and a set of definition phrases, the firststage finds sentences with the seed terms in the T-placeholder position of the seed phrases.
For eachterm, the set of extracted sentences is searched forcandidate defining terms (other domain terms in thesentence) to form term-term pairs which, at the sec-ond stage, will be used to acquire new patterns.Two situations can occur: a sentence may con-tain more than one domain term (other than one ofthe seed terms) or the same domain terms may befound to co-occur with multiple seed terms.
There-fore, term-term pairs are ranked.Ranking.
Term-term pairs are ranked using fourstandard measures of association strength: 1) co-occurrence count, 2) pointwise mutual information(PMI), 3) refined PMI; compensates bias towardlow-frequency events by multiplying PMI with co-occurrence count (Manning and Schu?tze, 1999), and4) mutual dependency (MD); compensates bias to-ward rare events by subtracting co-occurrence?s self-information (entropy) from its PMI (Thanopoulos etal., 2002).
The measures are calculated based on thecorpus for co-occurrences within a 15-word window.Based on experimentation, mutual dependencywas found to produce the best results and therefore itwas used in ranking definiens candidates in the finalevaluation of patterns.
The top-k candidates makeup the set of defining terms to be used in the patternacquisition stage.
Table 2 shows the top-20 candi-4Here and further in the paper, regular expressions are pre-sented in Perl notation.Domain term Candidate defining termslexical functional phrase structure grammargrammar (LFG) formal systemfunctional unification grammargrammatical representationphrase structuregeneralized phrasefunctional unificationbinding theorysyntactic theoriesfunctional structuregrammar formalism(s)grammarslinguistic theor(y|ies)Table 2: Candidate defining phrases of the term ?LexicalFunctional Grammar (LFG)?.date defining terms for the term ?Lexical FunctionalGrammar?, according to mutual dependency.Pattern and domain term acquisition.
At thepattern acquisition stage, definition patterns are re-trieved by 1) coupling terms with their definiens can-didates, 2) extracting sentences that contain the pairwithin the specified window of words, and finally3) creating POS-based patterns corresponding to theextracted sentences.
All co-occurrences of eachterm together with each of its defining terms withinthe fixed window size are extracted from the POS-tagged corpus.
At each iteration also new definien-dum and definiens terms are found by applying thenew abstracted patterns to the corpus and retrievingthe matching domain terms.The newly extracted sentences and terms are fil-tered (see ?Filtering?
below).
The remaining dataconstitute new instances for further iterations.
Thelinguistic material between the two terms in the ex-tracted sentences is taken to be an instantiation of apotential definition pattern for which its POS patternis created as follows:?
The defined and defining terms are replaced byplaceholders, T and DEF,?
All the material outside the T and DEF anchorsis removed; i.e.
the resulting patterns have theform ?T ... DEF?
or ?DEF ...
T??
Assuming that the fundamental elements of adefinition pattern, are verbs and noun phrases,58all tags except verb, noun, modal and the in-finitive marker ?to?
are replaced with by place-holders denoting any string; punctuation is pre-served, as it has been observed to be infor-mative in detecting definitions (Westerhout andMonachesi, 2008; Fahmi and Bouma, 2006),?
Sequences of singular and plural nouns andproper nouns are replaced with noun phraseplaceholder, NP; it is expanded to match com-plex noun phrases when applying the patternsto extract definition sentences.The new patterns and terms are then fed as inputto the acquisition process to extract more sentencesand again abstract new patterns.Filtering.
In the course of pattern acquisition in-formation on term-pattern co-occurrence frequen-cies is stored and relative frequencies are calculated:1) for each term, the percentage of seed patterns itoccurs with, and 2) for each pattern, the percentageof seed terms it occurs with.
These are used in thebootstrapping cycles to filter out terms which do notoccur as part of a sufficient number of patterns (pos-sibly false positive definiendum candidates) and pat-terns which do not occur with sufficient number ofterms (insufficient generalizing behavior).Moreover, the following filtering rules are ap-plied: Abstracted POS-pattern sequences of theform ?T .+ DEF?5 and ?DEF T?
are discarded;the former because it is not informative, the latterbecause it is rather an indicator of compound nounsthan of definitions.
From the extracted sentences,those containing negation are filtered out; negationis contra-indicative of definition (Pearson, 1996).For the same reason, auxiliary constructions with?do?
and ?have?
are excluded unless, in case of thelatter, ?have?
is followed by a two past participletags as in, e.g., ?has been/VBN defined/VBN (as).
?4.3 Manual RefinementWhile the goal of the bootstrapping stage was to findas many candidate patterns for good definition termsas possible, the purpose of the refinement stage is toaim at precision.
Since the automatically extractedpatterns consist only of verb and noun phrase tags5?.+?
stands for at least one arbitrary character.# Definitions # Non-definitions25 is/VBZ 24 is/VBZ8 represents/VBZ 14 contains/VBZ6 provides/VBZ 9 employed/VBD6 contains/VBZ 6 includes/VBZ6 consists/VBZ 4 reflects/VBZ3 serves/VBZ 3 uses/VBZ3 describes/VBZ 3 typed/VBN3 constitutes/VBZ 3 provides/VBZ3 are/VBP 3 learning/VBGTable 3: Subset of verbs occurring in sentences matchedby the most frequently extracted patterns.between the definiendum and its defining term an-chors, they are too general.In order to create more precise patterns, we tunedthe pattern sequences based on a small developmentsub-corpus of the extracted sentences which we an-notated.
The development corpus was created by ex-tracting sentences using the most frequent patternsinstantiated with the term which occurred with thehighest percentage of seed patterns.
The term ?on-tology?
appeared with more than 80% of the patternsand was used for this purpose.
The sentences werethen manually annotated as to whether they are true-positive or false examples of definitions (101 and163 sentences, respectively).Pattern tuning was done by investigating whichverbs are and which are not indicative of defini-tions based on the positive and negative examplesentences.
Table 3 shows the frequency distribu-tion of verbs (or verb sequences) in the annotatedcorpus which occurred more than twice.
Abstract-ing over POS sequences of the sentences contain-ing definition-indicative verbs, we created 13 pat-terns, extending the automatically found patterns,that yielded 65% precision on the development set,matching 51% of the definition sentences, and re-ducing noise to 17% non-definitions.
Patterns re-sulting from verb tuning were used in the evaluation.Examples of the tuned patterns are shown below:T VBZ DT JJ?
NP .
* DEFT , NP VBZ IN NP .
* DEFT , .+ VBZ DT .+ NP .
* DEFT VBZ DT JJ?
NP .
* DEFThe first pattern matches our both introductory59example definitions of the term ?PCFG?
(cf.
Sec-tion 1) with ?T?
as a placeholder for the term it-self, ?NP?
denoting a noun phrase, and ?DEF?
oneof the term?s defining phrases, in the first case, (1),?grammar?, in the second case, (2), ?probabilities?.The examples annotated with matched pattern ele-ments are shown below:6[PCFG]T [is]VBZ [a]DT [CFG]NP [in which eachproduction A ?
?
in the].?
[grammar]DEF ?sset of productionsR is associated with an emis-sion probability .
.
.A [PCFG]T [defines]VBZ [the]DT[probability]DEF of a string of words asthe sum of the probabilities of all admissiblephrase structure parses (trees) for that string.5 Deep Analysis for Definition ExtractionAn alternative, largely domain-independent ap-proach to the extraction of definition sentences isbased on the sentence-semantic index generation ofthe ACL Anthology Searchbench (Scha?fer et al,2011).Deep syntactic parsing with semantic predicate-argument structure extraction of each of the approx.3.3 million sentences in the 18,653 papers ACL An-thology corpus is used for our experiments.
Webriefly describe how in this approach we get fromthe sentence text to the semantic representation.The preprocessing is shared with thebootstrapping-based approach for definitionsentence extraction, namely PDF-to-text extraction,sentence boundary detection (SBR), and trigram-based POS tagging with TnT (Brants, 2000).
Thetagger output is combined with information froma named entity recognizer that in addition delivershypothetical information on citation expressions.The combined result is delivered as input to thedeep parser PET (Callmeier, 2000) running the opensource HPSG grammar (Pollard and Sag, 1994)grammar for English (ERG; Flickinger (2002)).The deep parser is made robust and fast througha careful combination of several techniques; e.g.
:(1) chart pruning: directed search during parsing to6Matching pattern elements in square brackets; tags fromthe pattern subscripted.increase performance and coverage for longer sen-tences (Cramer and Zhang, 2010); (2) chart map-ping: a framework for integrating preprocessing in-formation from PoS tagger and named entity recog-nizer in exactly the way the deep grammar expects it(Adolphs et al, 2008)7; (3) a statistical parse rank-ing model (WeScience; (Flickinger et al, 2010)).The parser outputs sentence-semantic represen-tation in the MRS format (Copestake et al, 2005)that is transformed into a dependency-like vari-ant (Copestake, 2009).
From these DMRS represen-tations, predicate-argument structures are derived.These are indexed with structure (semantic subject,predicate, direct object, indirect object, adjuncts) us-ing a customized Apache Solr8 server.
Matchingof arguments is left to Solr?s standard analyzer forEnglish with stemming; exact matches are rankedhigher than partial matches.The basic semantics extraction algorithm consistsof the following steps: 1) calculate the closure foreach (D)MRS elementary predication based on theEQ (variable equivalence) relation and group thepredicates and entities in each closure respectively;2) extract the relations of the groups, which results ina graph as a whole; 3) recursively traverse the graph,form one semantic tuple for each predicate, and fillinformation under its scope, i.e.
subject, object, etc.The semantic structure extraction algorithm gen-erates multiple predicate-argument structures forcoordinated sentence (sub-)structures in the in-dex.
Moreover, explicit negation is recognized andnegated sentences are excluded for the task for thesame reasons as in the bootstrapping approach above(see Section 4.2, ?Filtering?
).Further details of the deep parsing approach aredescribed in (Scha?fer and Kiefer, 2011).
In theSearchbench online system9, the definition extrac-tion can by tested with any domain term T by usingstatement queries of the form ?s:T p:is?.6 EvaluationFor evaluation, we selected 20 terms, shown in Ta-ble 4, which can be considered domain terms in the7PoS tagging, e.g., helps the deep parser to cope with wordsunknown to the deep lexicon, for which default entries based onthe PoS information are generated on the fly.8http://lucene.apache.org/solr9http://aclasb.dfki.de60integer linear programming (ILP)conditional random field (CRF)support vector machine (SVM)latent semantic analysis (LSA)combinatory categorial grammar (CCG)lexical-functional grammar (LFG)probabilistic context-free grammar (PCFG)discourse representation theory (DRT)discourse representation structure (DRS)phrase-based machine translation (PSMT;PBSMT)statistical machine translation (SMT)multi-document summarization (MDS)word sense disambiguation (WSD)semantic role labeling (SRL)coreference resolutionconditional entropycosine similaritymutual information (MI)default unification (DU)computational linguistics (CL)Table 4: Domain-terms used in the rating experimentdomain of computational linguistics.
Five generalterms, such as ?English text?
or ?web page?, werealso included in the evaluation as a control sample;since general terms of this kind are not likely to bedefined in scientific papers in CL, their definitionsentences were of low quality (false positives).
Wedo not include them in the summary of the evalua-tion results for space reasons.
?Computational lin-guistics?, while certainly a domain term in the do-main, is not likely to be defined in the articles in theACL Anthology, however, the term as such shouldrather be included in a glossary of computational lin-guistics, therefore, we included it in the evaluation.Due to the lack of a gold-standard glossary defi-nitions in the domain, we performed a rating exper-iment in which we asked domain experts to judgetop-ranked definitional sentences extracted using thetwo approaches.
Below we briefly outline the evalu-ation setup and the procedure.6.1 Evaluation DataA set of definitional sentences for the 20 domainterms was extracted as follows:Lexico-syntactic patterns (LSP).
For the lexico-syntactic patterns approach, sentences extracted bythe set of refined patterns (see Section 4.3) wereconsidered for evaluation only if they contained atleast one of the term?s potential defining phrases asidentified by the first stage of the glossary extraction(Section 4.2).
Acronyms were allowed as fillers ofthe domain term placeholders.The candidate evaluation sentences were rankedusing single linkage clustering in order to find sub-sets of similar sentences.
tf.idf-based cosine be-tween vectors of lemmatized words was used as asimilarity function.
As in (Shen et al, 2006), thelongest sentence was chosen from each of the clus-ters.
Results were ranked by considering the size ofthe clusters as a measure of how likely it representsa definition.
The larger the cluster, the higher it wasranked.
Five top-ranked sentences for each of the 20terms were used for the evaluation.Deep analysis (DA).
The only pattern used fordeep analysis extraction was ?subject:T predi-cate:is?, with ?is?
restricted by the HPSG grammarto be the copula relation and not an auxiliary such asin passive constructions, etc.
Five top-ranked sen-tences ?
as per the Solr?s matching algorithm ?
ex-tracted with this pattern were used for the evaluation.In total, 200 candidate definition sentences for20 domain terms were evaluated, 100 per extractionmethods.
Examples of candidate glossary sentencesextracted using both methods, along with their rat-ings, are shown in the appendix.6.2 Evaluation MethodCandidate definition sentences were presented to 6human domain experts by a web interface display-ing one sentence at a time in random order.
Judgeswere asked to rate sentences on a 5-point ordinalscale with the following descriptors:105: The passage provides a precise and concise de-scription of the concept4: The passage provides a good description of theconcept3: The passage provides useful information aboutthe concept, which could enhance a definition10Example definitions at each scale point selected by the au-thors were shown for the concept ?hidden markov model?.61DALSP100,0%80,0%60,0%40,0%20,0%0,0%21,6727,3318,1715,3332,5026,1716,3316,0011,3315,1712345RatingFigure 1: Distribution of ratings across the 5 scale points;LSP: lexico-syntactic patterns, DA: deep analysis2: The passage is not a good enough descriptionof the concept to serve as a definition; for in-stance, it?s too general, unfocused, or a subcon-cept/superconcept of the target concept is de-fined instead1: The passage does not describe the concept at allThe judges participating in the rating experimentwere PhD students, postdoctoral researchers, or re-searchers of comparable expertise, active in the ar-eas of computational linguistics/natural languageprocessing/language technology.
One of the raterswas one of the authors of this paper.
The raters wereexplicitly instructed to think along the lines of ?whatthey would like to see in a glossary of computationallinguistics terms?.6.3 ResultsFigure 1 shows the distribution of ratings acrossthe five scale points for the two systems.
Around57% of the LSP ratings and 60% of DA ratings fallwithin the top three scale-points (positive ratings)and 43% and 40%, respectively, within the bottomtwo scale-points (low ratings).
Krippendorff?s or-dinal ?
(Hayes and Krippendorff, 2007) was 0.66(1,000 bootstrapped samples) indicating a modestdegree of agreement, at which, however, tentativeconclusions can be drawn.ILPCRFSVMLSACCGLFGPCFGDRTDRSPSMT;PBSMTSMTMDSWSDSRLcoref.
resolutioncond.
entropycos similarityMIDUCLMode ratings54321DALSPMethodFigure 2: Mode values of ratings per method for the indi-vidual domain terms; see Table 4Figure 2 shows the distribution of mode ratingsof the individual domain terms used in the evalua-tion.
Definitions of 6 terms extracted using the LSPmethod were rated most frequently at 4 or 5 as op-posed to the majority of ratings at 3 for most termsin case of the DA method.A Wilcoxon signed-rank test was conducted toevaluate whether domain experts favored defini-tional sentences extracted by one the two methods.11The results indicated no significant difference be-tween ratings of definitions extracted using LSP andDA (Z = 0.43, p = 0.68).Now, considering that the ultimate purpose of thesentence extraction is glossary creation, we werealso interested in how the top-ranked sentences wererated; that is, assuming we were to create a glossaryusing only the highest ranked sentences (accordingto the methods?
ranking schemes; see Section 6.1)we wanted to know whether one of the methods pro-poses rank-1 candidates with higher ratings, inde-pendently of the magnitude of the difference.
A signtest indicated no statistical difference in ratings ofthe rank-1 candidates between the two methods.11Definition sentences for each domain term were paired bytheir rank assigned by the extraction methods: rank-1 DA sen-tence with rank-1 LSP, etc.
; see Section 6.1.627 Conclusions and Future WorkThe results show that both methods have the poten-tial of extracting good quality glossary sentences:the majority of the extracted sentences provide atleast useful information about the domain concepts.However, both methods need improvement.The rating experiment suggests that the concept ofdefinition quality in a specialized domain is largelysubjective (borderline acceptable agreement overalland ?
= 0.65 for rank-1 sentences).
This calls fora modification of the evaluation methodology andfor additional tests of consistency of ratings.
Thelow agreement might be remedied by introducinga blocked design in which groups of judges wouldevaluate definitions of a small set of concepts withwhich they are most familiar, rather than a large setof concepts from various CL sub-areas.An analysis of the extracted sentences and theirratings12 revealed that deep analysis reduces noise insentence extraction.
Bootstrapping, however, yieldsmore candidate sentences with good or very goodratings.
While in the present work pattern refine-ment was based only on verbs, we observed that alsothe presence and position of (wh-)determiners andprepositions might be informative.
Further exper-iments are needed 1) to find out how much speci-ficity can be allowed without blocking the patterns?productivity and 2) to exploit the complementarystrengths of the methods by combining them.Since both approaches use generic linguistic re-sources and preprocessing (POS-tagging, named-entity extraction, etc.)
they can be considereddomain-independent.
To our knowledge, this is,however, the first work that attempts to identifydefinitions of Computational Linguistics concepts.Thus, it contributes to evaluating pattern bootstrap-ping and deep analysis in the context of the defini-tion extraction task in our own domain.AcknowledgmentsThe C-Value algorithm was implemented by Mi-hai Grigore.
We are indebted to our colleaguesfrom the Computational Linguistics department andDFKI in Saarbru?cken who kindly agreed to partic-ipate in the rating experiment as domain experts.12Not included in this paper for space reasonsWe are also grateful to the reviewers for their feed-back.
The work described in this paper has beenpartially funded by the German Federal Ministryof Education and Research, projects TAKE (FKZ01IW08003) and Deependance (FKZ 01IW11003).ReferencesP.
Adolphs, S. Oepen, U. Callmeier, B. Crysmann,D.
Flickinger, and B. Kiefer.
2008.
Some Fine Pointsof Hybrid Natural Language Parsing.
In Proceedingsof the 6th LREC, pages 1380?1387.V.
Aleven, K. R. Koedinger, and K. Cross.
1999.
Tutor-ing Answer Explanation Fosters Learning with Under-standing.
In Artificial Intelligence in Education, pages199?206.
IOS Press.S.
Bird, R. Dale, B. Dorr, B. Gibson, M. Joseph, M.-Y.
Kan, D. Lee, B. Powley, D. Radev, and Y. F. Tan.2008.
The ACL Anthology Reference Corpus: A Ref-erence Dataset for Bibliographic Research in Compu-tational Linguistics.
In Proceedings of the 6th LREC,pages 1755?1759.C.
Borg, M. Rosner, and G. Pace.
2009.
EvolutionaryAlgorithms for Definition Extraction.
In Proceedingsof the 1st Workshop on Definition Extraction, pages26?32.L.
Bozzato, M. Ferrari, and A. Trombetta.
2008.
Build-ing a Domain Ontology from Glossaries: A GeneralMethodology.
In Proceedings of the 5th Workshop onSemantic Web Applications and Perspectives, pages 1?10.T.
Brants.
2000.
TnT ?
a statistical part-of-speech tagger.In Proceedings of ANLP, pages 224?231.U.
Callmeier.
2000.
PET ?
A Platform for Experimenta-tion with Efficient HPSG Processing Techniques.
Nat-ural Language Engineering, 6(1):99?108.A.
Copestake, D. Flickinger, I.
A.
Sag, and C. Pollard.2005.
Minimal Recursion Semantics: an Introduction.Research on Language and Computation, 3(2?3):281?332.A.
Copestake.
2009.
Slacker semantics: why superficial-ity, dependency and avoidance of commitment can bethe right way to go.
In Proceedings of the 12th EACLConference, pages 1?9.B.
Cramer and Y. Zhang.
2010.
Constraining robustconstructions for broad-coverage parsing with preci-sion grammars.
In Proceedings of the 23rd COLINGConference, pages 223?231.I.
Dura?n-Mun?oz, 2010. eLexicography in the 21st cen-tury: New challenges, new applications, volume 7,chapter Specialised lexicographical resources: a sur-vey of translators?
needs, pages 55?66.
Presses Uni-versitaires de Louvain.63O.
Etzioni, M. Cafarella, D. Downey, A-M. Popescu,T.
Shaked, S. Soderland, D.S.
Weld, and A. Yates.2005.
Unsupervised named-entity extraction from theWeb: an experimental study.
Artificial Intelligence,165:91?134.I.
Fahmi and G. Bouma.
2006.
Learning to identify defi-nitions using syntactic features.
In Proceedings of theEACL Workshop on Learning Structured Informationin Natural Language Applications, pages 64?71.D.
Flickinger, S. Oepen, and G. Ytrest?l.
2010.
Wiki-Woods: Syntacto-semantic annotation for EnglishWikipedia.
In Proceedings of the 7th LREC, pages1665?1671.D.
Flickinger.
2002.
On building a more efficient gram-mar by exploiting types.
In Collaborative LanguageEngineering.
A Case Study in Efficient Grammar-based Processing, pages 1?17.
CSLI Publications,Stanford, CA.K.
Frantzi, S. Ananiadou, and H. Mima.
1998.
Au-tomatic recognition of multi-word terms: the C-value/NC-value method.
In Proceedings of the 2ndEuropean Conference on Research and AdvancedTechnology for Digital Libraries, pages 585?604.R.
Del Gaudio and A. Branco.
2009.
Language inde-pendent system for definition extraction: First resultsusing learning algorithms.
In Proceedings of the 1stWorkshop on Definition Extraction, pages 33?39.R.
Girju, A. Badulescu, and D. Moldovan.
2006.
Au-tomatic discovery of part-whole relations.
Computa-tional Linguistics, 32(1):83?135.A.
F. Hayes and K. Krippendorff.
2007.
Answering thecall for a standard reliability measure for coding data.Communication Methods and Measures, 1(1):77?89.M.
A. Hearst.
1992.
Automatic acquisition of hyponymsfrom large text corpora.
In Proceedings of the 14thCOLING Conference, pages 539?545.D.
Jurafsky and J. H. Martin.
2000.
Speech andLanguage Processing: An Introduction to NaturalLanguage Processing, Computational Linguistics andSpeech Recognition.
Prentice Hall Series in ArtificialIntelligence.
2nd Ed.
Online draft (June 25, 2007).Z.
Kozareva, E. Riloff, and E. Hovy.
2008.
Seman-tic Class Learning from the Web with Hyponym Pat-tern Linkage Graphs.
In Proceedings of the 46th ACLMeeting, pages 1048?1056.C.
D. Manning and H. Schu?tze.
1999.
Foundations ofstatistical natural language processing.
MIT Press,Cambridge, MA, USA.M.
P. Marcus, B. Santorini, and M. A. Marcinkiewicz.1993.
Building a large annotated corpus of En-glish.
The Penn Treebank.
Computational Linguistics,19:313?330.S.
Muresan and J. Klavans.
2002.
A method for automat-ically building and evaluating dictionary resources.
InProceedings of the 3rd LREC, pages 231?234.P.
Pantel and M. Pennacchiotti.
2006.
Espresso: Lever-aging Generic Patterns for Automatically HarvestingSemantic Relations.
In Proceedings of the 21st COL-ING and the 44th ACL Meeting, pages 113?120.J.
Pearson.
1996.
The expression of definitions in spe-cialised texts: A corpus-based analysis.
In Proceed-ings of Euralex-96, pages 817?824.C.
Pollard and I.
A.
Sag.
1994.
Head-Driven PhraseStructure Grammar.
Studies in Contemporary Lin-guistics.
University of Chicago Press, Chicago.U.
Scha?fer and B. Kiefer.
2011.
Advances in deepparsing of scholarly paper content.
In R. Bernardi,S.
Chambers, B. Gottfried, F. Segond, and I. Za-ihrayeu, editors, Advanced Language Technologies forDigital Libraries, number 6699 in LNCS, pages 135?153.
Springer.U.
Scha?fer, B. Kiefer, C. Spurk, J. Steffen, and R. Wang.2011.
The ACL Anthology Searchbench.
In Proceed-ings of ACL-HLT 2011, System Demonstrations, pages7?13, Portland, Oregon, June.Y.
Shen, G. Zaccak, B. Katz, Y. Luo, and O. Uzuner.2006.
Duplicate Removal for Candidate Answer Sen-tences.
In Proceedings of the 1st CSAIL Student Work-shop.A.
Thanopoulos, N. Fakotakis, and G. Kokkinakis.
2002.Comparative evaluation of collocation extraction met-rics.
In Proceedings of the 3rd Language ResourcesEvaluation Conference, pages 620?625.P.
Velardi, R. Navigli, and P. D?Amadio.
2008.
Miningthe Web to Create Specialized Glossaries.
IEEE Intel-ligent Systems, pages 18?25.S.
Walter.
2008.
Linguistic description and automaticextraction of definitions from german court decisions.In Proceedings of the 6th LREC, pages 2926?2932.W.
Weiten, D. Deguara, E. Rehmke, and L. Sewell.1999.
University, Community College, and HighSchool Students?
Evaluations of Textbook Pedagogi-cal Aids.
Teaching of Psychology, 26(1):19?21.E.
Westerhout and P. Monachesi.
2008.
Creating glos-saries using pattern-based and machine learning tech-niques.
In Proceedings of the 6th LREC, pages 3074?3081.M.
Wolska, U. Scha?fer, and The Nghia Pham.
2011.Bootstrapping a domain-specific terminological taxon-omy from scientific text.
In Proceedings of the 9th In-ternational Conference on Terminology and ArtificialIntelligence (TIA-11), pages 17?23.
INALCO, Paris.G.
Ytrest?l, D. Flickinger, and S. Oepen.
2009.
Extract-ing and annotating wikipedia sub-domains.
In Pro-ceedings of the 7th Workshop on Treebanks and Lin-guistic Theories, pages 185?197.64AppendixRated glossary sentences for ?word sense disambiguation (WSD)?
and ?mutual information (MI)?.
As shownin Figure 2, for WSD, mode ratings of LSP sentences were higher, while for MI it was the other way round.word sense disambiguation (WSD)mode ratings of LSP sentences:WSD is the task of determining the sense of a polysemous word within a specific context (Wang et al, 2006).
5Word sense disambiguation or WSD, the task of identifying the correct sense of a word in context, is a central problemfor all natural language processing applications, and in particular machine translation: different senses of a word translatedifferently in other languages, and resolving sense ambiguity is needed to identify the right translation of a word.4Unlike previous applications of co-training and self-training to natural languagelearning, where one general classifier isbuild to cover the entire problem space, supervised word sense disambiguation implies a different classifier for each in-dividual word, resulting eventually in thousands of different classifiers, each with its own characteristics (learning rate,sensitivity to new examples, etc.
).3NER identifies different kinds of names such as ?person?, ?location?
or ?date?, while WSD distinguishes the senses ofambiguous words.3This paper presents a corpus-based approach to word sense disambiguation that builds an ensemble of Naive Bayesianclassifiers, each of which is based on lexical features that represent co-occurring words in varying sized windows of context.1DA sentences:Word Sense Disambiguation (WSD) is the task of formalizing the intended meaning of a word in context by selecting anappropriate sense from a computational lexicon in an automatic manner.5Word Sense Disambiguation(WSD) is the process of assigning a meaning to a word based on the context in which it occurs.
{4,5}Word sense disambiguation (WSD) is a difficult problem in natural language processing.
2word sense disambiguation, Hownet, sememe, co-occurrence Word sense disambiguation (WSD) is one of the most difficultproblems in NLP.
{1,2}There is a general concern within the field of word sense disambiguation about the inter-annotator agreement betweenhuman annotators.1mutual information (MI)mode ratings of LSP sentences:According to Fano (1961), if two points (words), x and y, have probabilities P (x) and P (y), then their mutual information,I(x, y), is defined to be I(x, y) = log2P (x,y)P (x)P (y) ); informally, mutual information compares the probability of observing xand y together (the joint probability) with the probabilities of observing x and y independently (chance).5Mutual information, I(v; c/s), measures the strength of the statistical association between the given verb v and the candi-date class c in the given syntactic position s.3In this equation, pmi(i, p) is the pointwise mutual information score (Church and Hanks, 1990) between a pattern, p (e.g.consist-of), and a tuple, i (e.g.
engine-car), and maxpmi is the maximum PMI score between all patterns and tuples.
{1,3}Note that while differential entropies can be negative and not invariant under change of variables, other properties of entropyare retained (Huber et al, 2008), such as the chain rule for conditional entropy which describes the uncertainty in Y givenknowledge of X , and the chain rule for mutual information which describes the mutual dependence between X and Y .2The first term of the conditional probability measures the generality of the association, while the second term of the mutualinformation measures the co-occurrence of the association.2DA sentences:Mutual information (Shannon and Weaver, 1949) is a measure of mutual dependence between two random variables.
43 Theory Mutual information is a measure of the amount of information that one random variable contains about anotherrandom variable.4Conditional mutual information is the mutual information of two random variables conditioned on a third one.
{1,3}Thus, the mutual information is log25 or 2.32 bits, meaning that the joint probability is 5 times more likely than chance.
1Thus, the mutual information is log20, meaning that the joint is infinitely less likely than chance.
165
