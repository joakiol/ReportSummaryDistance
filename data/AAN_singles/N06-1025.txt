Proceedings of the Human Language Technology Conference of the North American Chapter of the ACL, pages 192?199,New York, June 2006. c?2006 Association for Computational LinguisticsExploiting Semantic Role Labeling, WordNet and Wikipediafor Coreference ResolutionSimone Paolo Ponzetto and Michael StrubeEML Research gGmbHSchloss-Wolfsbrunnenweg 3369118 Heidelberg, Germanyhttp://www.eml-research.de/nlpAbstractIn this paper we present an extension ofa machine learning based coreference res-olution system which uses features in-duced from different semantic knowledgesources.
These features represent knowl-edge mined from WordNet and Wikipedia,as well as information about semantic rolelabels.
We show that semantic features in-deed improve the performance on differ-ent referring expression types such as pro-nouns and common nouns.1 IntroductionThe last years have seen a boost of work devoted tothe development of machine learning based coref-erence resolution systems (Soon et al, 2001; Ng &Cardie, 2002; Yang et al, 2003; Luo et al, 2004,inter alia).
While machine learning has proved toyield performance rates fully competitive with rulebased systems, current coreference resolution sys-tems are mostly relying on rather shallow features,such as the distance between the coreferent expres-sions, string matching, and linguistic form.
How-ever, the literature emphasizes since the very begin-ning the relevance of world knowledge and infer-ence for coreference resolution (Charniak, 1973).This paper explores whether coreference resolu-tion can benefit from semantic knowledge sources.More specifically, whether a machine learning basedapproach to coreference resolution can be improvedand which phenomena are affected by such infor-mation.
We investigate the use of the WordNet andWikipedia taxonomies for extracting semantic simi-larity and relatedness measures, as well as semanticparsing information in terms of semantic role label-ing (Gildea & Jurafsky, 2002, SRL henceforth).We believe that the lack of semantics in the cur-rent systems leads to a performance bottleneck.In order to correctly identify the discourse entitieswhich are referred to in a text, it seems essential toreason over the lexical semantic relations, as well asthe event representations embedded in the text.
Asan example, consider a fragment from the AutomaticContent Extraction (ACE) 2003 data.
(1) But frequent visitors say that given the sheer weight ofthe country?s totalitarian ideology and generations ofmass indoctrination, changing this country?s course willbe something akin to turning a huge ship at sea.
OpeningNorth Korea up, even modestly, and exposing people tothe idea that Westerners ?
and South Koreans ?
are notdevils, alone represents an extraordinary change.
[...] ashis people begin to get a clearer idea of the deprivationthey have suffered, especially relative to their neighbors.
?This is a society that has been focused most of all onstability, [...]?.In order to correctly resolve the anaphoric expres-sions highlighted in bold, it seems that some kindof lexical semantic and encyclopedic knowledge isrequired.
This includes that North Korea is a coun-try, that countries consist of people and are soci-eties.
The resolution requires an encyclopedia (i.e.Wikipedia) look-up and reasoning on the content re-latedness holding between the different expressions(i.e.
as a path measure along the links of the Word-Net and Wikipedia taxonomies).
Event representa-tions seem also to be important for coreference res-olution, as shown below:(2) A state commission of inquiry into the sinking of theKursk will convene in Moscow on Wednesday, theInterfax news agency reported.
It said that the divingoperation will be completed by the end of next week.192In this example, knowing that the Interfax newsagency is the AGENT of the report predicate and Itbeing the AGENT of say could trigger the (seman-tic parallelism based) inference required to correctlylink the two expressions, in contrast to anchoringthe pronoun to Moscow.
SRL provides the seman-tic relationships that constituents have with predi-cates, thus allowing us to include such document-level event descriptive information into the relationsholding between referring expressions (REs).Instead of exploring different kinds of data rep-resentations, task definitions or machine learningtechniques (Ng & Cardie, 2002; Yang et al, 2003;Luo et al, 2004) we focus on a few promising se-mantic features which we evaluate in a controlledenvironment.
That way we try to overcome theplateauing in performance in coreference resolutionobserved by Kehler et al (2004).2 Related WorkVieira & Poesio (2000), Harabagiu et al (2001),and Markert & Nissim (2005) explore the use ofWordNet for different coreference resolution sub-tasks, such as resolving bridging reference, other-and definite NP anaphora, and MUC-style corefer-ence resolution.
All of them present systems whichinfer coreference relations from a set of potential an-tecedents by means of a WordNet search.
Our ap-proach to WordNet here is to cast the search resultsin terms of semantic similarity measures.
Their out-put can be used as features for a learner.
These mea-sures are not specifically developed for coreferenceresolution but simply taken ?off-the-shelf?
and ap-plied to our task without any specific tuning ?
i.e.in contrast to Harabagiu et al (2001), who weightWordNet relations differently in order to computethe confidence measure of the path.To the best of our knowledge, we do not knowof any previous work using Wikipedia or SRL forcoreference resolution.
In the case of SRL, thislayer of semantic context abstracts from the specificlexical expressions used, and therefore represents ahigher level of abstraction than (still related) workinvolving predicate argument statistics.
Kehler et al(2004) observe no significant improvement due topredicate argument statistics.
The improvement re-ported by Yang et al (2005) is rather caused by theirtwin-candidate model than by the semantic knowl-edge.
Employing SRL is closer in spirit to Ji et al(2005), who explore the employment of the ACE2004 relation ontology as a semantic filter.3 Coreference Resolution Using SemanticKnowledge Sources3.1 Corpora UsedTo establish a competitive coreference resolver, thesystem was initially prototyped using the MUC-6and MUC-7 data sets (Chinchor & Sundheim, 2003;Chinchor, 2001), using the standard partitioningof 30 texts for training and 20-30 texts for test-ing.
Then, we moved on and developed and testedthe system with the ACE 2003 Training Data cor-pus (Mitchell et al, 2003)1.
Both the Newswire(NWIRE) and Broadcast News (BNEWS) sectionswhere split into 60-20-20% document-based par-titions for training, development, and testing, andlater per-partition merged (MERGED) for systemevaluation.
The distribution of coreference chainsand referring expressions is given in Table 1.3.2 Learning AlgorithmFor learning coreference decisions, we used a Maxi-mum Entropy (Berger et al, 1996) model.
This wasimplemented using the MALLET library (McCal-lum, 2002).
To prevent the model from overfitting,we employed a tunable Gaussian prior as a smooth-ing method.
The best parameter value is found bysearching in the [0,10] interval with step value of0.5 for the variance parameter yielding the highestMUC score F-measure on the development data.Coreference resolution is viewed as a binary clas-sification task: given a pair of REs, the classifier hasto decide whether they are coreferent or not.
TheMaxEnt model produces a probability for each cat-egory y (coreferent or not) of a candidate pair, con-ditioned on the context x in which the candidate oc-curs.
The conditional probability is calculated by:p(y|x) = 1Zx[?i?ifi(x, y)]1We used the training data corpus only, as the availabilityof the test data is restricted to ACE participants.
Therefore, theresults we report cannot be compared directly with those usingthe official test data.193BNEWS (147 docs ?
33,479 tokens) NWIRE (105 docs ?
57,205 tokens)#coref ch.
#pron.
#comm.
nouns #prop.
names #coref ch.
#pron.
#comm.
nouns #prop.
namesTRAIN.
587 876 572 980 904 1,037 1,210 2,023DEVEL 201 315 163 465 399 358 485 923TEST 228 291 238 420 354 329 484 712TOTAL 1,016 1,482 973 1,865 1,657 1,724 2,179 3,658TOTAL (%) 34.3% 22.5% 43.2% 22.8% 28.8% 48.4%Table 1: Partitions of the ACE 2003 training data corpuswhere fi(x, y) is the value of feature i on outcome yin context x, and ?i is the weight associated with i inthe model.
Zx is a normalization constant.
The fea-tures used in our model are all binary-valued featurefunctions (or indicator functions), e.g.fI SEMROLE(ARG0/RUN, COREF) =??????????
?1 if candidate pair iscoreferent and antecedentis the semantic argumentARG0 of predicate run0 elseIn our system, a set of pre-processing compo-nents including a POS tagger (Gime?nez & Ma`rquez,2004), NP chunker (Kudoh & Matsumoto, 2000)and the Alias-I LingPipe Named Entity Recognizer2is applied to the text in order to identify the nounphrases, which are further taken as referring ex-pressions (REs) to be used for instance generation.Therefore, we use automatically extracted nounphrases, rather than assuming perfect NP chunk-ing.
This is in contrast to other related worksin coreference resolution (e.g.
Luo et al (2004),Kehler et al (2004)).Instances are created following Soon et al (2001).We create a positive training instance from each pairof adjacent coreferent REs.
Negative instances areobtained by pairing the anaphoric REs with any REoccurring between the anaphor and the antecedent.During testing each text is processed from left toright: each RE is paired with any preceding RE fromright to left, until a pair labeled as coreferent is out-put, or the beginning of the document is reached.The classifier imposes a partitioning on the availableREs by clustering each set of expressions labeled ascoreferent into the same coreference chain.2http://alias-i.com/lingpipe3.3 Baseline System FeaturesFollowing Ng & Cardie (2002), our baseline sys-tem reimplements the Soon et al (2001) system.The system uses 12 features.
Given a potential an-tecedent REi and a potential anaphor REj the fea-tures are computed as follows3.
(a) Lexical featuresSTRING MATCH T if REi and REj have thesame spelling, else F.ALIAS T if one RE is an alias of the other; else F.(b) Grammatical featuresI PRONOUN T if REi is a pronoun; else F.J PRONOUN T if REj is a pronoun; else F.J DEF T if REj starts with the; else F.J DEM T if REj starts with this, that, these, orthose; else F.NUMBER T if both REi and REj agree in number;else F.GENDER U if either REi or REj have an undefinedgender.
Else if they are both defined and agreeT; else F.PROPER NAME T if both REi and REj areproper names; else F.APPOSITIVE T if REj is in apposition with REi;else F.(c) Semantic featuresWN CLASS U if either REi or REj have an unde-fined WordNet semantic class.
Else if they bothhave a defined one and it is the same T; else F.(d) Distance featuresDISTANCE how many sentences REi and REj areapart.3Possible values are U(nknown), T(rue) and F(alse).
Notethat in contrast to Ng & Cardie (2002) we interpret ALIAS asa lexical feature, as it solely relies on string comparison andacronym string matching.1943.4 WordNet FeaturesIn the baseline system semantic information is lim-ited to WordNet semantic class matching.
Unfor-tunately, a WordNet semantic class lookup exhibitsproblems such as coverage, sense proliferation andambiguity4, which make the WN CLASS featurevery noisy.
We enrich the semantic informationavailable to the classifier by using semantic similar-ity measures based on the WordNet taxonomy (Ped-ersen et al, 2004).
The measures we use includepath length based measures (Rada et al, 1989; Wu &Palmer, 1994; Leacock & Chodorow, 1998), as wellas ones based on information content (Resnik, 1995;Jiang & Conrath, 1997; Lin, 1998).In our case, the measures are obtained by comput-ing the similarity scores between the head lemmataof each potential antecedent-anaphor pair.
In orderto overcome the sense disambiguation problem, wefactorise over all possible sense pairs: given a can-didate pair, we take the cross product of each an-tecedent and anaphor sense to form pairs of synsets.For each measure WN SIMILARITY, we computethe similarity score for all synset pairs, and createthe following features.WN SIMILARITY BEST the highest similarityscore from all ?SENSEREi,n, SENSEREj ,m?
synsetpairs.WN SIMILARITY AVG the average similarityscore from all ?SENSEREi,n, SENSEREj ,m?
synsetpairs.Pairs containing REs which cannot be mapped toWordNet synsets are assumed to have a null simi-larity measure.3.5 Wikipedia FeaturesWikipedia is a multilingual Web-based free-contentencyclopedia5 .
The English version, as of 14 Febru-ary 2006, contains 971,518 articles with 16.8 mil-lion internal hyperlinks thus providing a large cover-age available knowledge resource.
In addition, sinceMay 2004 it provides also a taxonomy by means ofthe category feature: articles can be placed in one4Following the system to be replicated, we simply mappedeach RE to the first WordNet sense of the head noun.5Wikipedia can be downloaded at http://download.wikimedia.org/.
In our experiments we use the EnglishWikipedia database dump from 19 February 2006.or more categories, which are further categorized toprovide a category tree.
In practice, the taxonomyis not designed as a strict hierarchy or tree of cat-egories, but allows multiple categorisation schemesto co-exist simultaneously.
Because each article canappear in more than one category, and each categorycan appear in more than one parent category, the cat-egories do not form a tree structure, but a more gen-eral directed graph.
As of December 2005, 78% ofthe articles have been categorized into 87,000 differ-ent categories.Wikipedia mining works as follows (for an in-depth description of the methods for computingsemantic relatedness in Wikipedia see Strube &Ponzetto (2006)): given the candidate referring ex-pressions REi and REj we first pull the pages theyrefer to.
This is accomplished by querying the pagetitled as the head lemma or, in the case of NEs, thefull NP.
We follow all redirects and check for dis-ambiguation pages, i.e.
pages for ambiguous entrieswhich contain links only (e.g.
Lincoln).
If a disam-biguation page is hit, we first get al the hyperlinksin the page.
If a link containing the other queried REis found (i.e.
a link containing president in the Lin-coln page), the linked page (President of the UnitedStates) is returned, else we return the first articlelinked in the disambiguation page.
Given a candi-date coreference pair REi/j and the Wikipedia pagesPREi/j they point to, obtained by querying pages ti-tled as TREi/j , we extract the following features:I/J GLOSS CONTAINS U if no Wikipedia pagetitled TREi/j is available.
Else T if the first para-graph of text of PREi/j contains TREj/i ; else F.I/J RELATED CONTAINS U if no Wikipediapage titled as TREi/j is available.
Else T if atleast one Wikipedia hyperlink of PREi/j con-tains TREj/i ; else F.I/J CATEGORIES CONTAINS U if no Wiki-pedia page titled as TREi/j is available.
Else T ifthe list of categories PREi/j belongs to containsTREj/i ; else F.GLOSS OVERLAP the overlap score between thefirst paragraph of text of PREi and PREj .
Fol-lowing Banerjee & Pedersen (2003) we computethe score as?n m2 for n phrasal m-word over-laps.195Additionally, we use the Wikipedia category graph.We ported the WordNet similarity path length basedmeasures to the Wikipedia category graph.
How-ever, the category relations in Wikipedia cannot onlybe interpreted as corresponding to is-a links in ataxonomy since they denote meronymic relationsas well.
Therefore, the Wikipedia-based measuresare to be taken as semantic relatedness measures.The measures from Rada et al (1989), Leacock &Chodorow (1998) and Wu & Palmer (1994) are com-puted in the same way as for WordNet.
Path searchtakes place as a depth-limited search of maximumdepth of 4 for a least common subsumer.
We no-ticed that limiting the search improves the results asit yields a better correlation of the relatedness scoreswith human judgements (Strube & Ponzetto, 2006).This is due to the high regions of the Wikipedia cat-egory tree being too strongly connected.In addition, we use the measure from Resnik(1995), which is computed using an intrinsic in-formation content measure relying on the hierar-chical structure of the category tree (Seco et al,2004).
Given PREi/j and the lists of categoriesCREi/j they belong to, we factorise over all pos-sible category pairs.
That is, we take the crossproduct of each antecedent and anaphor category toform pairs of ?Wikipedia synsets?.
For each mea-sure WIKI RELATEDNESS, we compute the relat-edness score for all category pairs, and create thefollowing features.WIKI RELATEDNESS BEST the highest relat-edness score from all ?CREi,n, CREj ,m?
cate-gory pairs.WIKI RELATEDNESS AVG the average relat-edness score from all ?CREi,n, CREj ,m?
cate-gory pairs.3.6 Semantic Role FeaturesThe last semantic knowledge enhancement for thebaseline system uses SRL information.
In our exper-iments we use the ASSERT parser (Pradhan et al,2004), an SVM based semantic role tagger whichuses a full syntactic analysis to automatically iden-tify all verb predicates in a sentence together withtheir semantic arguments, which are output as Prop-Bank arguments (Palmer et al, 2005).
It is of-ten the case that the semantic arguments output bythe parser do not align with any of the previouslyidentified noun phrases.
In this case, we pass asemantic role label to a RE only when the twophrases share the same head.
Labels have the form?ARG1 pred1 .
.
.
ARGn predn?
for n semantic rolesfilled by a constituent, where each semantic argu-ment label is always defined with respect to a predi-cate.
Given such level of semantic information avail-able at the RE level, we introduce two new features6.I SEMROLE the semantic role argument-predicate pairs of REi.J SEMROLE the semantic role argument-predicate pairs of REj .For the ACE 2003 data, 11,406 of 32,502 automati-cally extracted noun phrases were tagged with 2,801different argument-predicate pairs.4 Experiments4.1 Performance MetricsWe report in the following tables the MUCscore (Vilain et al, 1995).
Scores in Table 2 arecomputed for all noun phrases appearing in eitherthe key or the system response, whereas Tables 3and 4 refer to scoring only those phrases which ap-pear in both the key and the response.
We thereforediscard those responses not present in the key, as weare interested in establishing the upper limit of theimprovements given by our semantic features.
Thatis, we want to define a baseline against which to es-tablish the contribution of the semantic informationsources explored here for coreference resolution.In addition, we report the accuracy score for allthree types of ACE mentions, namely pronouns,common nouns and proper names.
Accuracy is thepercentage of REs of a given mention type correctlyresolved divided by the total number of REs of thesame type given in the key.
A RE is said to be cor-rectly resolved when both it and its direct antecedentare placed by the key in the same coreference class.6During prototyping we experimented unpairing the argu-ments from the predicates, which yielded worse results.
Thisis supported by the PropBank arguments always being definedwith respect to a target predicate.
Binarizing the features ?
i.e.do REi and REj have the same argument or predicate label withrespect to their closest predicate?
?
also gave worse results.196MUC-6 MUC-7original R P F1 R P F1Soon et al 58.6 67.3 62.3 56.1 65.5 60.4duplicatedbaseline 64.9 65.6 65.3 55.1 68.5 61.1Table 2: Results on MUC4.2 Feature SelectionFor determining the relevant feature sets we followan iterative procedure similar to the wrapper ap-proach for feature selection (Kohavi & John, 1997)using the development data.
The feature subset se-lection algorithm performs a hill-climbing searchalong the feature space.
We start with a modelbased on all available features.
Then we train mod-els obtained by removing one feature at a time.
Wechoose the worst performing feature, namely the onewhose removal gives the largest improvement basedon the MUC score F-measure, and remove it fromthe model.
We then train classifiers removing eachof the remaining features separately from the en-hanced model.
The process is iteratively run as longas significant improvement is observed.4.3 ResultsTable 2 compares the results between our duplicatedSoon baseline and the original system.
We assumethat the slight improvements of our system are dueto the use of current pre-processing components andanother classifier.
Tables 3 and 4 show a comparisonof the performance between our baseline system andthe ones incremented with semantic features.
Per-formance improvements are highlighted in bold7.4.4 DiscussionThe tables show that semantic features improve sys-tem recall, rather than acting as a ?semantic filter?improving precision.
Semantics therefore seems totrigger a response in cases where more shallow fea-tures do not seem to suffice (see examples (1-2)).Different feature sources account for differentRE type improvements.
WordNet and Wikipediafeatures tend to increase performance on common7All changes in F-measure are statistically significant at the0.05 level or higher.
We follow Soon et al (2001) in performinga simple one-tailed, paired sample t-test between the baselinesystem?s MUC score F-measure and each of the other systems?F-measure scores on the test documents.nouns, whereas SRL improves pronouns.
Word-Net features are able to improve by 14.3% and7.7% the accuracy rate for common nouns on theBNEWS and NWIRE datasets (+34 and +37 cor-rectly resolved common nouns out of 238 and 484respectively), whereas employing Wikipedia yieldsslightly smaller improvements (+13.0% and +6.6%accuracy increase on the same datasets).
Similarly,when SRL features are added to the baseline system,we register an increase in the accuracy rate for pro-nouns, ranging from 0.7% in BNEWS and NWIREup to 4.2% in the MERGED dataset (+26 correctlyresolved pronouns out of 620).If semantics helps for pronouns and commonnouns, it does not affect performance on propernames, where features such as string matching andalias suffice.
This suggests that semantics plays arole in pronoun and common noun resolution, wheresurface features cannot account for complex prefer-ences and semantic knowledge is required.The best accuracy improvement on pronoun res-olution is obtained on the MERGED dataset.
Thisis due to making more data available to the classi-fier, as the SRL features are very sparse and inher-ently suffer from data fragmentation.
Using a largerdataset highlights the importance of SRL, whosefeatures are never removed in any feature selectionprocess8.
The accuracy on common nouns showsthat features induced from Wikipedia are competi-tive with the ones from WordNet.
The performancegap on all three datasets is quite small, which indi-cates the usefulness of using an encyclopedic knowl-edge base as a replacement for a lexical taxonomy.As a consequence of having different knowledgesources accounting for the resolution of different REtypes, the best results are obtained by (1) combin-ing features generated from different sources; (2)performing feature selection.
When combining dif-ferent feature sources, we register an accuracy im-provement on pronouns and common nouns, as wellas an increase in F-measure due to a higher recall.Feature selection always improves results.
Thisis due to the fact that our full feature set is ex-8To our knowledge, most of the recent work in coreferenceresolution on the ACE data keeps the document source sepa-rated for evaluation.
However, we believe that document sourceindependent evaluation provides useful insights on the robust-ness of the system (cf.
the CoNLL 2005 shared task cross-corpora evaluation).197BNEWS NWIRER P F1 Ap Acn Apn R P F1 Ap Acn Apnbaseline 46.7 86.2 60.6 36.4 10.5 44.0 56.7 88.2 69.0 37.6 23.1 55.6+WordNet 54.8 86.1 66.9 36.8 24.8 47.6 61.3 84.9 71.2 38.9 30.8 55.5+Wiki 52.7 86.8 65.6 36.1 23.5 46.2 60.6 83.6 70.3 38.0 29.7 55.2+SRL 53.3 85.1 65.5 37.1 13.9 46.2 58.0 89.0 70.2 38.3 25.0 56.0all features 59.1 84.4 69.5 37.5 27.3 48.1 63.1 83.0 71.7 39.8 31.8 52.8Table 3: Results on the ACE 2003 data (BNEWS and NWIRE sections)R P F1 Ap Acn Apnbaseline 54.5 88.0 67.3 34.7 20.4 53.1+WordNet 56.7 87.1 68.6 35.6 28.5 49.6+Wikipedia 55.8 87.5 68.1 34.8 26.0 50.5+SRL 56.3 88.4 68.8 38.9 21.6 51.7all features 61.0 84.2 70.7 38.9 29.9 51.2Table 4: Results ACE (merged BNEWS/NWIRE)tremely redundant: in order to explore the useful-ness of the knowledge sources we included overlap-ping features (i.e.
using best and average similar-ity/relatedness measures at the same time), as well asfeatures capturing the same phenomenon from dif-ferent point of views (i.e.
using multiple measuresat the same time).
In order to yield the desired per-formance improvements, it turns out to be essentialto filter out irrelevant features.Table 5 shows the relevance of the best perform-ing features on the BNEWS section.
As our fea-ture selection mechanism chooses the best set of fea-tures by removing them (see Section 4.2), we eval-uate the contributions of the remaining features asfollows.
We start with a baseline system using allthe features from Soon et al (2001) that were notremoved in the feature selection process (i.e.
DIS-TANCE).
We then train classifiers combining thecurrent feature set with each feature in turn.
Wethen choose the best performing feature based on theMUC score F-measure and add it to the model.
Weiterate the process until all features are added to thebaseline system.
The table indicates that all knowl-edge sources are relevant for coreference resolution,as it includes SRL, WordNet and Wikipedia features.The Wikipedia features rank high, indicating againthat it provides a valid knowledge base.5 Conclusions and Future WorkThe results are somehow surprising, as one wouldnot expect a community-generated categorizationto be almost as informative as a well structuredFeature set F1baseline (Soon w/o DISTANCE) 58.4%+WIKI WU PALMER BEST +4.3%+J SEMROLE +1.8%+WIKI PATH AVG +1.2%+I SEMROLE +0.8%+WN WU PALMER BEST +0.7%Table 5: Feature selection (BNEWS section)lexical taxonomy such as WordNet.
NeverthelessWikipedia offers promising results, which we expectto improve as well as the encyclopedia goes underfurther development.In this paper we investigated the effects of usingdifferent semantic knowledge sources within a ma-chine learning based coreference resolution system.This involved mining the WordNet taxonomy andthe Wikipedia encyclopedic knowledge base, as wellas including semantic parsing information, in orderto induce semantic features for coreference learning.Empirical results show that coreference resolutionbenefits from semantics.
The generated model isable to learn selectional preferences in cases wheresurface morpho-syntactic features do not suffice, i.e.pronoun and common name resolution.
While theresults given by using ?the free encyclopedia thatanyone can edit?
are satisfactory, major improve-ments can come from developing efficient querystrategies ?
i.e.
a more refined disambiguation tech-nique taking advantage of the context in which thequeries (e.g.
referring expressions) occur.Future work will include turning Wikipedia intoan ontology with well defined taxonomic relations,as well as exploring its usefulness of for other NLPapplications.
We believe that an interesting aspect ofWikipedia is that it offers large coverage resourcesfor many languages, thus making it a natural choicefor multilingual NLP systems.Semantics plays indeed a role in coreferenceresolution.
But semantic features are expensive to198compute and the development of efficient methodsis required to embed them into large scale systems.Nevertheless, we believe that exploiting semanticknowledge in the manner we described will assistthe research on coreference resolution to overcomethe plateauing in performance observed by Kehleret al (2004).Acknowledgements: This work has been fundedby the Klaus Tschira Foundation, Heidelberg, Ger-many.
The first author has been supported by aKTF grant (09.003.2004).
We thank Katja Filip-pova, Margot Mieskes and the three anonymous re-viewers for their useful comments.ReferencesBanerjee, S. & T. Pedersen (2003).
Extended gloss overlap asa measure of semantic relatedness.
In Proc.
of IJCAI-03, pp.805?810.Berger, A., S. A. Della Pietra & V. J. Della Pietra (1996).
Amaximum entropy approach to natural language processing.Computational Linguistics, 22(1):39?71.Charniak, E. (1973).
Jack and Janet in search of a theory ofknowledge.
In Advance Papers from the Third InternationalJoint Conference on Artificial Intelligence, Stanford, Cal.,pp.
337?343.Chinchor, N. (2001).
Message Understanding Conference(MUC) 7.
LDC2001T02, Philadelphia, Penn: LinguisticData Consortium.Chinchor, N. & B. Sundheim (2003).
Message UnderstandingConference (MUC) 6.
LDC2003T13, Philadelphia, Penn:Linguistic Data Consortium.Gildea, D. & D. Jurafsky (2002).
Automatic labeling of seman-tic roles.
Computational Linguistics, 28(3):245?288.Gime?nez, J.
& L. Ma`rquez (2004).
SVMTool: A general POStagger generator based on support vector machines.
In Proc.of LREC ?04, pp.
43?46.Harabagiu, S. M., R. C. Bunescu & S. J. Maiorano (2001).
Textand knowledge mining for coreference resolution.
In Proc.of NAACL-01, pp.
55?62.Ji, H., D. Westbrook & R. Grishman (2005).
Using semantic re-lations to refine coreference decisions.
In Proc.
HLT-EMNLP?05, pp.
17?24.Jiang, J. J.
& D. W. Conrath (1997).
Semantic similarity basedon corpus statistics and lexical taxonomy.
In Proceedings ofthe 10th International Conference on Research in Computa-tional Linguistics (ROCLING).Kehler, A., D. Appelt, L. Taylor & A. Simma (2004).
The(non)utility of predicate-argument frequencies for pronouninterpretation.
In Proc.
of HLT-NAACL-04, pp.
289?296.Kohavi, R. & G. H. John (1997).
Wrappers for feature subsetselection.
Artificial Intelligence Journal, 97(1-2):273?324.Kudoh, T. & Y. Matsumoto (2000).
Use of Support Vector Ma-chines for chunk identification.
In Proc.
of CoNLL-00, pp.142?144.Leacock, C. & M. Chodorow (1998).
Combining local con-text and WordNet similarity for word sense identifica-tion.
In C. Fellbaum (Ed.
), WordNet.
An Electronic LexicalDatabase, Chp.
11, pp.
265?283.
Cambridge, Mass.
: MITPress.Lin, D. (1998).
An information-theoretic definition of similar-ity.
In Proceedings of the 15th International Conference onMachine Learning, pp.
296?304.Luo, X., A. Ittycheriah, H. Jing, N. Kambhatla & S. Roukos(2004).
A mention-synchronous coreference resolution al-gorithm based on the Bell Tree.
In Proc.
of ACL-04, pp.136?143.Markert, K. & M. Nissim (2005).
Comparing knowledgesources for nominal anaphora resolution.
ComputationalLinguistics, 31(3):367?401.McCallum, A. K. (2002).
MALLET: A Machine Learning forLanguage Toolkit.Mitchell, A., S. Strassel, M. Przybocki, J. Davis, G. Dodding-ton, R. Grishman, A. Meyers, A. Brunstain, L. Ferro &B. Sundheim (2003).
TIDES Extraction (ACE) 2003 Mul-tilingual Training Data.
LDC2004T09, Philadelphia, Penn.
:Linguistic Data Consortium.Ng, V. & C. Cardie (2002).
Improving machine learning ap-proaches to coreference resolution.
In Proc.
of ACL-02, pp.104?111.Palmer, M., D. Gildea & P. Kingsbury (2005).
The propositionbank: An annotated corpus of semantic roles.
ComputationalLinguistics, 31(1):71?105.Pedersen, T., S. Patwardhan & J. Michelizzi (2004).
Word-Net::Similarity ?
Measuring the relatedness of concepts.
InCompanion Volume of the Proceedings of the Human Tech-nology Conference of the North American Chapter of the As-sociation for Computational Linguistics, pp.
267?270.Pradhan, S., W. Ward, K. Hacioglu, J. H. Martin & D. Juraf-sky (2004).
Shallow semantic parsing using Support VectorMachines.
In Proc.
of HLT-NAACL-04, pp.
233?240.Rada, R., H. Mili, E. Bicknell & M. Blettner (1989).
Devel-opment and application of a metric to semantic nets.
IEEETransactions on Systems, Man and Cybernetics, 19(1):17?30.Resnik, P. (1995).
Using information content to evaluate seman-tic similarity in a taxonomy.
In Proc.
of IJCAI-95, Vol.
1, pp.448?453.Seco, N., T. Veale & J. Hayes (2004).
An intrinsic informationcontent metric for semantic similarity in WordNet.
In Proc.of ECAI-04, pp.
1089?1090.Soon, W. M., H. T. Ng & D. C. Y. Lim (2001).
A machinelearning approach to coreference resolution of noun phrases.Computational Linguistics, 27(4):521?544.Strube, M. & S. P. Ponzetto (2006).
WikiRelate!
Computingsemantic relatedness using Wikipedia.
In Proc.
of AAAI-06.Vieira, R. & M. Poesio (2000).
An empirically-based system forprocessing definite descriptions.
Computational Linguistics,26(4):539?593.Vilain, M., J. Burger, J. Aberdeen, D. Connolly & L. Hirschman(1995).
A model-theoretic coreference scoring scheme.
InProceedings of the 6th Message Understanding Conference(MUC-6), pp.
45?52.Wu, Z.
& M. Palmer (1994).
Verb semantics and lexical selec-tion.
In Proc.
of ACL-94, pp.
133?138.Yang, X., J. Su & C. L. Tan (2005).
Improving pronoun reso-lution using statistics-based semantic compatibility informa-tion.
In Proc.
of ACL-05, pp.
165?172.Yang, X., G. Zhou, J. Su & C. L. Tan (2003).
Coreferenceresolution using competition learning approach.
In Proc.
ofACL-03, pp.
176?183.199
