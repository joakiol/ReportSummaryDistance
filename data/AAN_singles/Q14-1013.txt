Senti-LSSVM: Sentiment-Oriented Multi-Relation Extractionwith Latent Structural SVMLizhen QuMax Planck Institutefor Informaticslqu@mpi-inf.mpg.deYi ZhangNuance Communicationsyi.zhang@nuance.comRui WangDFKI GmbHmars198356@hotmail.comLili JiangMax Planck Institutefor Informaticsljiang@mpi-inf.mpg.deRainer GemullaMax Planck Institutefor Informaticsrgemulla@mpi-inf.mpg.deGerhard WeikumMax Planck Institutefor Informaticsweikum@mpi-inf.mpg.deAbstractExtracting instances of sentiment-oriented re-lations from user-generated web documents isimportant for online marketing analysis.
Un-like previous work, we formulate this extrac-tion task as a structured prediction problemand design the corresponding inference as aninteger linear program.
Our latent structuralSVM based model can learn from training cor-pora that do not contain explicit annotations ofsentiment-bearing expressions, and it can si-multaneously recognize instances of both bi-nary (polarity) and ternary (comparative) re-lations with regard to entity mentions of in-terest.
The empirical evaluation shows thatour approach significantly outperforms state-of-the-art systems across domains (camerasand movies) and across genres (reviews andforum posts).
The gold standard corpus thatwe built will also be a valuable resource forthe community.1 IntroductionSentiment-oriented relation extraction (Choi et al.,2006) is concerned with recognizing sentiment po-larities and comparative relations between entitiesfrom natural language text.
Identifying such rela-tions often requires syntactic and semantic analysisat both sentence and phrase level.
Most prior workon sentiment analysis consider either i) subjectivesentence detection (Yu and K?bler, 2011), ii) po-larity classification (Johansson and Moschitti, 2011;Wilson et al., 2005), or iii) comparative relationidentification (Jindal and Liu, 2006; Ganapathib-hotla and Liu, 2008).
In practice, however, differ-ent types of sentiment-oriented relations frequentlycoexist in documents.
In particular, we found thatmore than 38% of the sentences in our test corpuscontain more than one type of relations.
The iso-lated analysis approach is inappropriate because i) itsacrifices acuracy by ignoring the intricate interplayamong different types of relations; ii) it could lead toconflicting predictions such as estimating a relationcandidate as both negative and comparative.
There-fore, in this paper, we identify instances of both sen-timent polarities and comparative relations for enti-ties of interest simultaneously.
We assume that allthe mentions of entities and attributes are given, andentities are disambiguated.
It is a widely used as-sumption when evaluating a module in a pipelinesystem that the outputs of preceding modules areerror-free.To the best of our knowledge, the only exist-ing system capable of extracting both comparisonsand sentiment polarities is a rule-based system pro-posed by Ding et al.
(2009).
We argue that it isbetter to tackle the task by using a unified modelwith structured outputs.
It allows us to consider aset of correlated relation instances jointly and char-acterize their interaction through a set of soft andhard constraints.
For example, we can encode con-straints to discourage an attribute to participate ina polarity relation and a comparative relation at thesame time.
As a result, the system extracts a set ofcorrelated instances of sentiment-oriented relationsfrom a given sentence.
For example, with the sen-tence about the camera Canon 7D, ?The sensor isgreat, but the price is higher than Nikon D7000.
?the expected output is positive(Canon 7D, sensor)155Transactions of the Association for Computational Linguistics, 2 (2014) 155?168.
Action Editor: Janyce Wiebe.Submitted 6/2013; Revised 11/2013; Published 4/2014.
c?2014 Association for Computational Linguistics.and preferred(Nikon D7000, Canon 7D, textit-price).However, constructing a fully annotated train-ing corpus for this task is labor-intensive and re-quires strong linguistic background.
We minimizethis overhead by applying a simplified annotationscheme, in which annotators mark mentions of en-tities and attributes, disambiguate the entities, andlabel instances of relations for each sentence.
Basedon the new scheme, we have created a small Senti-ment Relation Graph (SRG) corpus for the domainsof cameras and movies, which significantly differsfrom the corpora used in prior work (Wei and Gulla,2010; Kessler et al., 2010; Toprak et al., 2010;Wiebe et al., 2005; Hu and Liu, 2004) in the follow-ing ways: i) both sentiment polarities and compar-ative relations are annotated; ii) all mentioned en-tities are disambiguated; and iii) no subjective ex-pressions are annotated, unless they are part of entitymentions.The new annotation scheme raises a new chal-lenge for learning algorithms in that they need toautomatically find textual evidences for each anno-tated relation during training.
For example, with thesentence ?I like the Rebel a little better, but that isanother price jump?, simply assigning a sentiment-bearing expression to the nearest relation candidateis insufficient, especially when the sentiment is notexplicitly expressed.In this paper, we propose SENTI-LSSVM, a latentstructural SVM based model for sentiment-orientedrelation extraction.
SENTI-LSSVM is applied to findthe most likely set of the relation instances expressedin a given sentence, where the latent variables areused to assign the most appropriate textual evidencesto the respective instances.In summary, the contributions of this paper are thefollowing:?
We propose SENTI-LSSVM: the first unified sta-tistical model with the capability of extractinginstances of both binary and ternary sentiment-oriented relations.?
We design a task-specific integer linear pro-gramming (ILP) formulation for inference.?
We construct a new SRG corpus as a valuableasset for the evaluation of sentiment relationextraction.?
We conduct extensive experiments with on-line reviews and forum posts, showing thatSENTI-LSSVM model can effectively learn froma training corpus without explicitly annotatedsubjective expressions and that its performancesignificantly outperforms state-of-the-art sys-tems.2 Related WorkThere are ample works on analyzing sentiment po-larities and entity comparisons, but the majority ofthem studied the two tasks in isolation.Most prior approaches for fine-grained sentimentanalysis focus on polarity classification.
Super-vised approaches on expression-level analysis re-quire the annotation of sentiment-bearing expres-sions as training data (Jin et al., 2009; Choiand Cardie, 2010; Johansson and Moschitti, 2011;Yessenalina and Cardie, 2011; Wei and Gulla,2010).
However, the corresponding annotation pro-cess is time-consuming.
Although sentence-levelannotations are easier to obtain, the analysis at thislevel cannot cope with sentences conveying relationsof multiple types (McDonald et al., 2007; T?ckstr?mand McDonald, 2011; Socher et al., 2012).
Lexicon-based approaches require no training data (Ku et al.,2006; Kim and Hovy, 2006; Godbole et al., 2007;Ding et al., 2008; Popescu and Etzioni, 2005; Liu etal., 2005) but suffer from inferior performance (Wil-son et al., 2005; Qu et al., 2012).
In contrast, ourmethod requires no annotation of sentiment-bearingexpressions for training and can predict both senti-ment polarities and comparative relations.Sentiment-oriented comparative relations havebeen studied in the context of user-generated dis-course (Jindal and Liu, 2006; Ganapathibhotla andLiu, 2008).
Approaches rely on linguistically moti-vated rules and assume the existence of independentkeywords in sentences which indicate comparativerelations.
Therefore, these methods fall short of ex-tracting comparative relations based on domain de-pendent information.Both Johansson and Moschitti (2011) and Wu etal.
(2011) formulate fine-grained sentiment analy-sis as a learning problem with structured outputs.However, they focus only on polarity classification156of expressions and require annotation of sentiment-bearing expressions for training as well.While ILP has been previously applied for infer-ence in sentiment analysis (Choi and Cardie, 2009;Somasundaran and Wiebe, 2009; Wu et al., 2011),our task requires a complete ILP reformulation dueto 1) the absence of annotated sentiment expressionsand 2) the constraints imposed by the joint extrac-tion of both sentiment polarity and comparative re-lations.3 System OverviewThis section gives an overview of the whole systemfor extracting sentiment-oriented relation instances.Prior to presenting the system architecture, we in-troduce the essential concepts and the definitions oftwo kinds of directed hypergraphs as the represen-tation of correlated relation instances extracted fromsentences.3.1 Concepts and DefinitionsEntity.
An entity is an abstract or concrete thing,which needs not be of material existence.
An entityin this paper refers to either a product or a brand.Attribute.
An attribute is an object closely associ-ated with or belonging to an entity, such as the lensof digital camera.Sentiment-Oriented Relation.
A sentiment-oriented relation is either a sentiment polarity or acomparative relation, defined on tuples of entitiesand attributes.
A sentiment polarity relation conveyseither a positive or a negative attitude towards enti-ties or their attributes, whereas a comparative rela-tion indicates the preference of one entity over theother entity w.r.t.
an attribute.Relation Instance.
An instance of sentiment polar-ity takes the form r(entity, attribute) with r ?
{pos-itive, negative}, such as positive(Canon 7D, sen-sor).
The polarity instances expressed in the formof unary relations, such as ?Nikon D7000 is ex-cellent.
?, are denoted as binary relations r(entity,whole), where the attribute whole indicates the en-tity as a whole.
In contrast, an instance of compar-ative relation is in the form of preferred{entity, en-tity, attribute}, e.g.
preferred(Canon 7D, NikonD7000, price).
For brevity, we refer to an instanceset of sentiment-oriented relations extracted from asentence as an sSoR.
To represent the instancesof the remaining relations, we represent them asother{entity, attribute}, such as textitpartOf{wheel,car}.
These relations include objective relationsand the subjective relations other than sentiment-oriented relations.Mention-Based Relation Instances.
A mention-based relation instance refers to a tuple of entitymentions with a certain relation.
This concept is in-troduced as the representation of instances in a sen-tence by replacing entities with the correspondingentity mentions, such as positive(?Canon SD880i?,?wide angle view?
).Figure 1: An example of MRG.Mention-Based Relation Graph.
A mention-basedrelation graph (or MRG ) represents a collection ofmention-based relation instances expressed in a sen-tence.
As illustrated in Figure 1, an MRG is a di-rected hypergraph G = ?M,E?
with a vertex setM and an edge set E. A vertex mi ?
M denotesa mention of an entity or an attribute occurring ei-ther within the sentence or in its context.
We saythat a mention is from the context if it is mentionedin the previous sentence or is an attribute impliedin the current sentence.
An instance of a binary re-lation in an MRG takes the form of a binary edgeel = (mi,ma), where mi and ma denote an en-tity mention and an attribute mention respectively,and the type l ?
{positive, negative, other}.
Aternary edge el indicating comparative relation isrepresented as el = (mi,mj ,ma), where two en-tity mentions mi and mj are compared with respectto the attribute mention ma.
We define the typel ?
{better,worse} to indicate two possible direc-tions of the relation and assume mi occurs beforemj .
As a result, we have a set L of five relationtypes: positive, negative, better, worse or other.
Ac-cording to these definitions, the annotations in theSRG corpus are actually MRGs and disambiguatedentities.
If there are multiple mentions referring tothe same entity, annotators are asked to choose the157most obvious one because it saves annotation timeand is less demanding for the entity recognition anddiambiguation modules.Figure 2: An example of eMRG.
The textual evi-dences are wrapped by green dashed boxes.Evidentiary Mention-Based Relation Graph.
Anevidentiary mention-based relation graph, coinedeMRG , extends an MRG by associating each edgewith a textual evidence to support the correspondingrelation assertions (see Figure 2).
Consequently, anedge in an eMRG is denoted by a pair (a, c), wherea represents a mention-based relation instance andc is the associated textual evidence.
It is also re-ferred to as an evidentiary edge.
represented asel = (mi,mj ,ma), an MRG as an evidentiary MRG(eMRG) and the edges of eMRGs as evidentiaryedges, as shown in Figure 2.3.2 System ArchitectureFigure 3: System architecture.As illustrated by Figure 3, at the core of our sys-tem is the SENTI-LSSVM model, which extracts setsof mention-based relationships in the form of eMRGsfrom sentences.
For a given sentence with knownentity mentions, we select all possible mention setsas relation candidates, where each set includes atleast one entity mention.
Then we associate eachrelation candidate with a set of constituents or thewhole sentence as the textual evidence candidates(cf.
Section 6.1).
Subsequently, the inference com-ponent aims to find the most likely eMRG from allpossible combinations of mention-based relation in-stances and their textual evidences (cf.
Section 6.2).The representation eMRG is chosen because it char-acterizes exactly the model outputs by letting eachedge correspond to an instance of mention-based re-lation and the associated textual evidence.
Finally,the model parameters of this model are learned byan online algorithm (cf.
Section 7).Since instance sets of sentiment-oriented relations(sSoRs) are the expected outputs, we can obtainsSoRs from MRGs by using a simple rule-based al-gorithm.
The algorithm essentially maps the men-tions from an MRG into entities and attributes in ansSoR and label the corresponding tuples with the re-lation types of the edges from an MRG.
For instancesof comparative relation, the label better or worse ismapped to the relation type preferred.4 SENTI-LSSVM ModelThe task of sentiment-oriented relation extractionis to determine the most likely sSoR in a sentence.Since sSoRs are derived from the correspondingMRGs as described in Section 3, the task is reducedto find the most likely MRG for each sentence.
Sincean MRG is created by assigning relation types to asubset of all relation candidates, which are possibletuples of mentions with unknown relation types, thenumber of MRGs can be extremely high.To tackle the task, one solution is to employan edge-factored linear model in the framework ofstructural SVM (Martins et al., 2009; Tsochantaridiset al., 2004).
The model suggests that a bag of fea-tures should be specified for each relation candidate,and then the model predicts the most likely candi-date sets along with their relation types to form theoptimal MRGs.
As we observed, for a relation can-didate, the most informative features are the wordsnear its entity mentions in the original text.
How-158ever, if we represent a candidate by all these words,it is very likely that the instances of different relationtypes share overly similar features, because a men-tion is often involved in more than one relation can-didate, as shown in Figure 2.
As a consequence, theinstances of different relations represented by overlysimilar features can easily confuse the learning algo-rithm.
Thus, it is critical to select proper constituentsor sentences as textual evidences for each relationcandidate in both training and testing.Consequently, we divide the task of sentiment-oriented relation extraction into two subtasks : i)identifying the most likely MRGs; ii) assigningproper textual evidences to each edge of MRGs tosupport their relation assertions.
It is desirable tocarry out the two subtasks jointly as these two sub-tasks could enhance each other.
First, the identifi-cation of relation types requires proper textual ev-idences; second, the soft and hard constraints im-posed by the correlated relation instances facilitatethe recognition of the corresponding textual evi-dences.
Since the eMRGs are created by attachingevery MRG with a set of textual evidences, tacklingthe two subtasks simultaneously is equivalent to se-lecting the most likely eMRG from a set of eMRGcandidates.
It is challenging because our SRG corpusdoes not contain any annotation of textual evidences.Formally, let X denote the set of all available sen-tences, and we define y ?
Y(x)(x ?
X ) as the setof labeled edges of an MRG and Y = ?x?XY(x).Since the assignments of textual evidences are notobserved, an assignment of evidences to y is de-noted by a latent variable h ?
H(x) and H =?x?XH(x).
Then (y, h) corresponds to an eMRG,and (a, c) ?
(y, h) is a labeled edge a attachedwith a textual evidence c. Given a labeled datasetD = {(x1, y1), ..., (xn, yn)} ?
(X ?
Y)n, we aimto learn a discriminant function f : X ?
Y?H thatoutputs the optimal eMRG (y, h) ?
Y(x)?H(x) fora given sentence x.Due to the introduction of latent variables, weadopt the latent structural SVM (Yu and Joachims,2009) for structural classification.
Our discriminantfunction is defined asf(x) = argmax(y,h)?Y(x)?H(x)?>?
(x, y, h) (1)where ?
(x, y, h) is the feature function of an eMRG(y, h) and ?
is the corresponding weight vector.To ensure tractability, we also employ edge-basedfactorization for our model.
Let Mp denote a set ofentity mentions and yr(mi) be a set of edges labeledwith sentiment-oriented relations incident to mi, thefactorization of ?
(x, y, h) is given as?
(x, y, h) =?(a,c)?
(y,h)?e(x, a, c) + (2)?mi?Mp?a,a?
?yr(mi),a 6=a?
?c(a, a?
)where ?e(x, a, c) is a local edge feature functionfor a labeled edge a attached with a textual evidencec and ?c(a, a?)
is a feature function capturing co-occurrence of two labeled edges ami and a?mi inci-dent to an entity mention mi.5 Feature SpaceThe following features are used in the feature func-tions (Equation 2):Unigrams: As mentioned before, a textual evi-dence attached to an edge in MRG is either a word,phrase or sentence.
We consider all lemmatized un-igrams in the textual evidence as unigram features.Context: Since web users usually express relatedsentiments about the same entity across sentenceboundaries, we describe the sentiment flow using aset of contextual binary features.
For example, if en-tity A is mentioned in both the previous sentence andthe current sentence, a set of contextual binary fea-tures are used to indicate all possible combinationsof the current and the previous mentioned sentiment-oriented relations regarding to entity A.Co-occurrence: We have mentioned the co-occurrence feature in Equation 2, indicated by?c(a, a?).
It captures the co-occurrence of two la-beled edges incident to the same entity mention.Note that the co-occurrence feature function is con-sidered only if there is a contrast conjunction such as?but?
between the non-shared entity mentions inci-dent to the two labeled edges.Senti-predictors: Following the idea of (Qu etal., 2012), we encode the prediction results fromthe rule-based phrase-level multi-relation predic-tor (Ding et al., 2009) and from the bag-of-opinionspredictor (Qu et al., 2010) as features based on thetextual evidence.
The output of the first predictoris an integer value, while the output of the secondpredictor is a sentiment relation, such as ?positive?,159?negative?, ?better?
or ?worse?.
We map the rela-tional outputs into integer values and then encodethe outputs from both predictors as senti-predictorfeatures.Others: The commonly used part-of-speech tagsare also included as features.
Moreover, for an edgecandidate, a set of binary features are used to denotethe types of the edge and its entity mentions.
For in-stance, a binary feature indicates whether an edge isa binary edge related to an entity mentioned in con-text.
To characterize the syntactic dependencies be-tween two adjacent entity mentions, we use the pathin the dependency tree between the heads of the cor-responding constituents, the number of words andother mentions in-between as features.
Additionally,if the textual evidence is a constituent, its featurew.r.t.
an edge is the dependency path to the clos-est mention of the edge that does not overlap withthis constituent.6 Structural InferenceIn order to find the best eMRG for a given sentencewith a well trained model, we need to determinethe most likely relation type for each relation candi-date and support the corresponding assertions withproper textual evidences.
We formulate this taskas an Integer Linear Programming (ILP).
Instead ofconsidering all constituents of a sentence, we empir-ically select a subset as textual evidences for eachrelation candidate.6.1 Textual Evidence Candidates SelectionTextual evidences are selected based on the con-stituent trees of sentences parsed by the Stanfordparser (Klein and Manning, 2003).
For each men-tion in a sentence, we first locate a constituent inthe tree with the maximal overlap by Jaccard sim-ilarity.
Starting from this constituent, we considertwo types of candidates: type I candidates are con-stituents at the highest level which contain neitherany word of another mention nor any contrast con-junctions such as ?but?
; type II candidates are con-stituents at the highest level which cover exactly twomentions of an edge and do not overlap with anyother mentions.
For a binary edge connecting an en-tity mention and an attribute mention, we considera type I candidate starting from the attribute men-tion.
For a binary edge connecting two entity men-tions, we consider type I candidates starting fromboth mentions.
Moreover, for a comparative ternaryedge, we consider both type I and type II candidatesstarting from the attribute mention.
This strategy isbased on our observation that these candidates of-ten cover the most important information w.r.t.
thecovered entity mentions.6.2 ILP FormulationWe formulate the inference problem of finding thebest eMRG as an ILP problem due to its convenientintegration of both soft and hard constraints.Given the model parameters ?, we reformulatethe score of an eMRG in the discriminant function(1) as follows,?>?
(x, y, h) =?(a,c)?
(y,h)saczac +?mi?Mp?a,a?
?yr(mi),a 6=a?saa?zaa?where sac = ?>?e(x, a, c) denotes the score of alabeled edge a attached with a textual evidence c,saa?
= ?>?c(a, a?)
is the edge co-occurrence score,the binary variable zac indicates the presence or ab-sence of the corresponding edge, and zaa?
indicatesif two edges co-occurr.
As not every edge set canform an eMRG, we require that a valid eMRG shouldsatisfy a set of linear constraints, which form ourconstraint space.
Then function (1) is equivalent tomaxz?B s>z + ?zds.t.
A??z????
?
dz,?, ?
?
Bwhere B = 2S with S = {0, 1}, and ?
and ?
areauxiliary binary variables that help define the con-straint space.
The above optimization problem takesexactly the form of an ILP because both the con-straints and the objective function are linear, and allvariables take only integer values.In the following, we consider two types of con-straint space, 1) an eMRG with only binary edges and2) an eMRG with both binary and ternary edges.160eMRG with only Binary Edges: An eMRG hasonly binary edges if a sentence contains no attributemention or at most one entity mention.
We expectthat each edge has only one relation type and is sup-ported by a single textual evidence.
To facilitate theformulation of constraints, we introduce ?el to de-note the presence or absence of a labeled edge el,and ?ec to indicate if a textual evidence c is assignedto an unlabeled edge e. Then the binary variable forthe corresponding evidentiary edge zelc = ?ec ?
?el ,where the ILP formulation of conjunction can befound in (Martins et al., 2009).Let Ce denote the set of textual evidence candi-dates of an unlabeled edge e. The constraint of atmost one textual evidence per edge is formulated as:?c?Ce?ec ?
1 (3)Once a textual evidence is assigned to an edge,their relation labels should match and the numberof labeled edges must agree with the number of at-tached textual evidences.
Further, we assume that atextual evidence c conveys at most one relation sothat an evidence will not be assigned to the relationsof different types, which is the main problem for thestructural SVM based model.
Let ?cl indicate thatthe textual evidence c is labeled by the relation typel.
The corresponding constraints are expressed as,?l?Le?el =?c?Ce?ec; zelc ?
?cl;?l?L?cl ?
1where Le denotes the set of all possible labels foran unlabeled edge e, and L is the set of all relationtypes of MRGs (cf.
Section 3).In order to avoid a textual evidence being overlyreused by multiple relation candidates, we first pe-nalize the assignment of a textual evidence c to alabeled edge a by associating the corresponding zacwith a fixed negative cost ??
in the objective func-tion.
Then the selection of one textual evidence peredge a is encouraged by associating ?
to zdc in theobjective function, where zdc =?e?Sc ?ec and Sc isthe set of edges that the textual evidence c serves asa candidate.
The disjunction zdc is expressed as:zdc ?
?e, e ?
Sczdc ?
?e?Sc?e(a) Binary edge structure(b) Ternary edge structureFigure 4: Alternative structures associated with anattribute mention.This soft constraint not only encourages one textualevidence per edge, but also keeps it eligible for mul-tiple assignments.For any two labeled edge a and a?
incidentto the same entity mention, the edge-to-edge co-occurrence is described by zca,a?
= za ?
za?
.eMRG with both Binary and Ternary Edges: Ifthere are more than one entity mentions and at leastone attribute mention in a sentence, an eMRG canpotentially have both binary and ternary edges.
Inthis case, we assume that each mention of attributescan participate either in binary relations or in ternaryrelations.
The assumption holds in more than 99.9%of the sentences in our SRG corpus, thus we describeit as a set of hard constraints.
Geometrically, the as-sumption can be visualized as the selection betweentwo alternative structures incident to the same at-tribute mention, as shown in Figure 4.
Note that,in the binary edge structure, we include not only theedges incident to the attribute mention but also theedge between the two entity mentions.Let Sbmi be the set of all possible labeled edgesin a binary edge structure of an attribute mentionmi.
Variable ?
bmi =?el?Sbmi?el indicates whetherthe attribute mention is associated with a binaryedge structure or not.
In the same manner, we use?
tmi =?el?Stmi?el to indicate the association of thean attribute mention mi with an ternary edge struc-ture from the set of all incident ternary edges Stmi .The selection between two alternative structures is161formulated as ?
bmi + ?
tmi = 1.
As this influencesonly the edges incident to an attribute mention, wekeep all the constraints introduced in the previoussection unchanged except for constraint (3), whichis modified as?c?Ce?ec ?
?
bmi ;?c?Ce?ec ?
?
tmiTherefore, we can have either binary edges orternary edges for an attribute mention.7 Learning Model ParametersGiven a set of training sentences D ={(x1, y1), .
.
.
, (xn, yn)}, the best weight vec-tor ?
of the discriminant function (1) is found bysolving the following optimization problem:min?1nn?i=1[ max(y?,h?)?Y(x)?H(x)(?>?
(x, y?, h?)+?
(h?, y?, y))?
maxh??H(x)?>?
(x, y, h?)]
+ ?|?|] (4)where ?
(h?, y?, y) is a loss function measuring the dis-crepancies between an eMRG (y, h?)
with gold stan-dard edge labels y and an eMRG (y?, h?)
with inferredlabeled edges y?
and textual evidences h?.
Due to thesparse nature of the lexical features, we apply L1regularizer to the weight vector ?, and the degree ofsparsity is controlled by the hyperparameter ?.Since the L1 norm in the above optimizationproblem is not differentiable at zero, we apply theonline forward-backward splitting (FOBOS) algo-rithm (Duchi and Singer, 2009).
It requires two stepsfor updating the weight vector ?
by using a singletraining sentence x on each iteration t.?t+ 12 = ?t ?
?t?t?t+1 = arg min?12??
?
?t?2 + ?t?|?|where ?t is the subgradient computed without con-sidering the L1 norm and ?t is the learning rate.For a labeled sentence x, ?t = ?
(x, y?
?, h??)
??
(x, y, h??
), where the feature functions of the corre-sponding eMRGs are inferred by solving (y?
?, h??)
=arg max(h?,y?)?H(x)?Y(x)[?>?
(x, y?, h?)
+ ?
(h?, y?, y)]and (y, h??)
= arg maxh?
?H(x) ?>?
(x, y, h?
), as in-dicated in the optimization problem (4).The former inference problem is similar to theone we considered in the previous section exceptthe inclusion of the loss function.
We incorporatethe loss function into the ILP formulation by defin-ing the loss between an MRG (y, h) and a gold stan-dard MRG as the sum of per-edge costs.
In our ex-periments, we consider a positive cost ?
for eachwrongly labeled edge a, so that if an edge a has adifferent label from the gold standard, we add ?
tothe coefficient sac of the corresponding variable zacin the objective function of the ILP formulation.In addition, since the non-positive weights of edgelabels in the initial learning phrase often lead toeMRGs with many unlabeled edges, which harms thelearning performance, we fix it by adding a con-straint for the minimal number of labeled edges inan eMRG, ?a?A?c?Ca?ac ?
?
(5)where A is the set of all labeled edge candidates and?
denotes the minimal number of labeled edges.Empirically, the best way to determine ?
is tomake it equal to the maximal number of labelededges in an eMRG with the restriction that a tex-tual evidence can be assigned to at most one edge.By considering all the edge candidates A and all thetextual evidence candidates C as two vertex sets in abipartite graph G?
= ?V = (A,C), E?
(with edges inE indicating which textual evidence can be assignedto which edge), ?
corresponds to exactly the size ofa maximum matching of the bipartite graph1.To find the optimal eMRG (y, h??
), for the gold la-bel k of each edge, we consider the following set ofconstraints for inference since the labels of the edgesare known for the training data,?c?Ce?ec ?
1; ?ec ?
lck?k??Llck?
?
1;?e?Sc?ec ?
1We include also the soft constraints, which avoida textual evidence being overly reused by multiplerelations, and the constraints similar to (5) to ensurea minimal number of labeled edges and a minimalnumber of sentiment-oriented relations.1It is computed by the Hopcroft-Karp algorithm (Hopcroftand Karp, 1973) in our implementation.1628 SRG CorpusFor evaluation we constructed the SRG corpus,which in total consists of 1686 manually annotatedonline reviews and forum posts in the digital cameraand movie domains2.
For each domain, we maintaina set of attributes and a list of entity names.The annotation scheme for the sentiment repre-sentation asserts minimal linguistic knowledge fromour annotators.
By focusing on the meanings of thesentences, the annotators make decisions based ontheir language intuition, not restricted by specificsyntactic structures.
Taking the example in Figure2, the annotators only need to mark the mentions ofentities and attributes from both the sentences andthe context, disambiguate them, and label (?Canon7D?, ?Nikon D7000?, price) as worse and (?Canon7D?, ?sensor?)
as positive, whereas in prior work,people have annotated the sentiment-bearing expres-sions such as ?great?
and link them to the respectiverelation instances as well.
This also enables themto annotate instances of both sentiment polarity andcomparative relaton, which are conveyed by not onlyexplicit sentiment-bearing expressions like ?excel-lent performance?, but also factual expressions im-plying evaluations such as ?The 7V has 10x opticalzoom and the 9V has 16x.
?.Camera MovieReviews Forums Reviews Forumspositive 386 1539 879 905negative 165 363 529 331comparison 30 480 39 35Table 1: Distribution of relation instances in SRG corpus.14 annotators participated in the annotationproject.
After a short training period, annotatorsworked on randomly assigned documents one at atime.
For product reviews, the system lists all rel-evant information about the entity and the prede-fined attributes.
For forum posts, the system showsonly the attribute list.
For each sentence in a doc-ument, the annotator first determines if it refers toan entity of interest.
If not, the sentence is marked2The 107 camera reviews are from bestbuy.com and Ama-zon.com; the 667 camera forum posts are downloaded from fo-rum.digitalcamerareview.com; the 138 movie reviews and 774forum posts are from imdb.com and boards.ie respectivelyas off-topic.
Otherwise, the annotator will identifythe most obvious mentions, disambiguate them, andmark the MRGs.
We evaluate the inter-annotatoragreement on sSoRs in terms of Cohen?s Kappa(?)
(Cohen, 1968).
An average Kappa value of 0.698was achieved on a randomly selected set consistingof 412 sentences.Table 1 shows the corpus distribution after nor-malizing them into sSoRs.
Camera forum posts con-tain the largest proportion of comparisons becausethey are mainly about the recommendation of dig-ital cameras.
In contrast, web users are much lessinterested in comparing movies, in both reviews andforums.
In all subsets, positive relations play a dom-inant role since web users intend to express morepositive attitudes online than negative ones (Pangand Lee, 2007).9 ExperimentsThis section describes the empirical evaluation ofSENTI-LSSVM together with two competitive base-lines on the SRG corpus.9.1 Experimental SetupWe implemented a rule-based baseline (DING-RULE) and a structural SVM (Tsochantaridis etal., 2004) baseline (SENTI-SSVM) for comparison.The former system extends the work of Ding etal.
(2009), which designed several linguistically-motivated rules based on a sentiment polarity lexi-con for relation identification and assumes there isonly one type of sentiment relation in a sentence.
Inour implementation, we keep all the rules of (Ding etal., 2009) and add one phrase-level rule when thereare more than one mention in a sentence.
The ad-ditional rule assigns sentiment-bearing words andnegators to its nearest relation candidates based onthe absolute surface distance between the words andthe corresponding mentions.
In this case, the phrase-level sentiment-oriented relations depend only onthe assigned sentiment words and negators.
The lat-ter system is based on a structural SVM and doesnot consider the assignment of textual evidences torelation instances during inference.
The textual fea-tures of a relation candidate are all lexical and sen-timent predictor features within a surface distanceof four words from the mentions of the candidate.163Thus, this baseline does not need the inference con-straints of SENTI-LSSVM for the selection of textualevidences.
To gain more insights into the model,we also evaluate the contribution of individual fea-tures of SENTI-LSSVM.
In addition, to show if identi-fying sentiment polarities and comparative relationsjointly works better than tackling each task on itsown, we train SENTI-LSSVM for each task separatelyand combine their predictions according to compat-ibility rules and the corresponding graph scores.For each domain and text genre, we withheld 15%documents for development and use the remainingfor cross validation.
The hyperparameters of all sys-tems are tuned on the development datasets.
For allexperiments of SENTI-LSSVM, we use ?
= 0.0001for the L1 regularizer in Eq.
(4) and ?
= 0.05 forthe loss function; and for SENTI-SSVM, ?
= 0.0001and ?
= 0.01.
Since the relation type of off-topicsentences is certainly other, we evaluate all systemswith 5-fold cross-validation only on the on-topicsentences in the evaluation dataset.
Since the samesSoR can have several equivalent MRGs and the rela-tion type other is not of our interest, we evaluate thesSoRs in terms of precision, recall and F-measure.All reported numbers are averages over the 5 folds.9.2 ResultsTable 2 shows the complete results of all sys-tems.
Here our model SENTI-LSSVM outperformedall baselines in terms of the average F-measurescores and recalls by a large margin.
The F-measureon movie reviews is about 14% over the best base-line.
The rule-based system has higher precisionthan recall in most cases.
However, simply increas-ing the coverage of the domain independent senti-ment polarity lexicon might lead to worse perfor-mance (Taboada et al., 2011) because many sen-timent oriented relations are conveyed by domaindependent expressions and factual expressions im-plying evaluations, such as ?This camera does nothave manual control.?
Compared to DING-RULE,SENTI-SSVM performs better in the camera domainbut worse for the movies due to many misclassi-fication of negative relation instances as other.
Italso wrongly predicted more positive instances asother than SENTI-LSSVM.
We found that the recallsof these instances are low because they often haveoverly similar features with the instances of the typeother linking to the same mentions.
The problemgets worse in the movie domain since i) many sen-tences contain no explicit sentiment-bearing words;ii) the prior polarity of the sentiment-bearing wordsdo not agree with their contextual polarity in thesentences.
Consider the following example from aforum post about the movie ?Superman Returns?
:?Have a look at Superman: the Animated Series orJustice League Unlimited .
.
.
that is how the char-acters of Superman and Lex Luthor should be.?.
Incontrast, our model minimizes the overlapping fea-tures by assigning them to the most likely relationcandidates.
This leads to significantly better per-formance.
Although SENTI-SSVM has low recall forboth positive and negative relations, it achieves thehighest recall for the comparative relation among allsystems in the movie domain and camera reviews.Since less than 1% of all instances are for compara-tive relations in these document sets and all modelsare trained to optimize the overall accuracy, SENTI-LSSVM intends to trade off the minority class for theoverall better performance.
This advantage disap-pears on the camera forum posts, where the numberof instances of comparative relation is 12 times morethan that in the other data sets.All systems perform better in predicting positiverelations than the negative ones.
This correspondswell to the empirical findings in (Wilson, 2008) thatpeople intend to use more complex expressions fornegative sentiments than their affirmative counter-parts.
It is also in accordance with the distribution ofthese relations in our SRG corpus which is randomlysampled from the online documents.
For learningsystems, it can also be explained by the fact that thetraining data for positive relations are considerablymore than those for negative ones.
The comparativerelation is the hardest one to process since we foundthat many corresponding expressions do not containexplicit keywords for comparison.To understand the performance of the key fea-ture groups in our model better, we remove eachgroup from the full SENTI-LSSVM system and eval-uate the variations with movie reviews and cameraforum posts, which have relatively balanced distri-bution of relation types.
As shown in Table 3, thefeatures from the sentiment predictors make signif-icant contributions for both datasets.
The differ-ent drops of the performance indicate that the po-164Positive Negative Comparison Micro-averageP R F P R F P R F P R FCameraForumDING-RULE 56.4 39.0 46.1 46.2 24.0 31.6 42.6 14.0 21.0 53.4 30.8 39.0SENTI-SSVM 60.2 35.6 44.8 44.2 38.5 41.2 28.0 40.1 32.9 43.7 36.7 39.9SENTI-LSSVM 69.2 38.9 49.8 50.8 39.3 44.3 42.6 35.1 38.5 56.5 38.0 45.4CameraRe-view DING-RULE 83.6 69.0 75.6 68.6 38.8 49.6 30.0 16.9 21.6 81.1 58.6 68.1SENTI-SSVM 72.6 75.4 74.0 63.9 62.5 63.2 28.0 38.9 32.5 68.1 70.4 69.3SENTI-LSSVM 77.3 85.4 81.2 68.9 61.3 64.9 22.3 20.7 21.6 73.1 73.4 73.7MovieForumDING-RULE 63.7 37.4 47.1 27.6 34.3 30.6 8.9 5.6 6.8 48.2 35.9 41.2SENTI-SSVM 66.2 30.1 41.3 25.6 17.3 20.7 44.2 56.7 49.7 53.3 27.9 36.6SENTI-LSSVM 63.3 44.2 52.1 29.7 45.6 36.0 40.1 45.0 42.4 49.7 44.6 47.0Movie Re-view DING-RULE 66.5 47.2 55.2 42.0 39.1 40.5 31.4 12.0 17.4 56.2 44.0 49.4SENTI-SSVM 61.3 54.0 57.4 45.2 13.7 21.1 24.5 63.3 35.3 54.6 39.2 45.7SENTI-LSSVM 59.0 79.1 67.6 53.3 51.4 52.3 28.3 34.0 30.9 57.9 68.8 62.9Table 2: Evaluation results for DING-RULE, SENTI-SSVM and SENTI-LSSVM.
Boldface figures are statisticallysignificantly better than all others in the same comparison group under t-test with p = 0.05.Feature Models Movie Reviews Camera Forumsfull system 62.9 45.4?unigram 63.2 (+0.3) 41.2 (-4.2)?context 54.5 (-8.4) 46.0 (+0.6)?co-occurrence 62.6 (-0.3) 44.9 (-0.5)?senti-predictors 61.3 (-1.6) 34.3 (-11.1)Table 3: Micro-average F-measure of SENTI-LSSVMwith different feature modelslarities predicted by rules are more consistent incamera forum posts than in movie reviews.
Dueto the complexity of expressions in the movie re-views our model cannot benefit from the unigramfeatures but these features are a good compensationfor the sentiment predictor features in camera fo-rum posts.
The sharp drop by removing the contextfeatures from our model on movie reviews indicatesthat the sentiments in movie reviews depend highlyon the relations of the previous sentences.
In con-trast, the sentiment-oriented relations of the previ-ous sentences could be a reason of overfitting forcamera forum data.
The edge co-occurrence fea-tures do not play an important role in our modelsince the number of co-occurred sentiment-orientedrelations in the sentences with contrast conjunctionslike ?but?
is small.
However, we found that allow-ing the co-occurrence of any sentiment-oriented re-lations would harm the performance of the model.In addition, our experiments showed that the sep-arated approach, which trains a model for senti-ment polarities and comparative relations respec-tively, leads to a decrease by almost 1% in terms ofthe F-measure averaged over all four datasets.
Thelargest drop of F-measure is 3% on camera forumposts, since this dataset contains the largest propor-tion of comparative relations.
We found that the er-rors are increased when the trained models makeconflicting predictions.
In this case, the joint ap-proach can take all factors into account and makemore consistent decisions than the separated ap-proaches.10 ConclusionWe proposed SENTI-LSSVM model for extracting in-stances of both sentiment polarities and comparativerelations.
For evaluating and training the model, wecreated an SRG corpus by using a lightweight an-notation scheme.
We showed that our model canautomatically find textual evidences to support itsrelation predictions and achieves significantly bet-ter F-measure scores than alternative state-of-the-artmethods.ReferencesYejin Choi and Claire Cardie.
2009.
Adapting a polaritylexicon using integer linear programming for domain-specific sentiment classification.
In Proceedings ofthe 2009 Conference on Empirical Methods in Natural165Language Processing: Volume 2 - Volume 2, EMNLP?09, pages 590?598, Stroudsburg, PA, USA.
Associa-tion for Computational Linguistics.Yejin Choi and Claire Cardie.
2010.
Hierarchical se-quential learning for extracting opinions and their at-tributes.
In Proceedings of the Annual meeting ofthe Association for Computational Linguistics, pages269?274.
Association for Computational Linguistics.Yejin Choi, Eric Breck, and Claire Cardie.
2006.
Jointextraction of entities and relations for opinion recog-nition.
In Proceedings of the Conference on EmpiricalMethods in Natural Language Processing, pages 431?439, Stroudsburg, PA, USA.
Association for Compu-tational Linguistics.Jacob Cohen.
1968.
Weighted Kappa: Nominal ScaleAgreement Provision for Scaled Disagreement or Par-tial Credit.
Psychological bulletin, 70(4):213.Xiaowen Ding, Bing Liu, and Philip S. Yu.
2008.
Aholistic lexicon-based approach to opinion mining.
InProceedings of the 2008 International Conference onWeb Search and Data Mining, pages 231?240, NewYork, NY, USA.
ACM.Xiaowen Ding, Bing Liu, and Lei Zhang.
2009.
Entitydiscovery and assignment for opinion mining applica-tions.
In Proceedings of the ACM SIGKDD Confer-ence on Knowledge Discovery and Data Mining, pages1125?1134.John Duchi and Yoram Singer.
2009.
Efficient onlineand batch learning using forward backward splitting.The Journal of Machine Learning Research, 10:2899?2934.Murthy Ganapathibhotla and Bing Liu.
2008.
Miningopinions in comparative sentences.
In Proceedings ofthe 22nd International Conference on ComputationalLinguistics - Volume 1, pages 241?248, Stroudsburg,PA, USA.
Association for Computational Linguistics.Namrata Godbole, Manjunath Srinivasaiah, and StevenSkiena.
2007.
Large-scale sentiment analysis fornews and blogs (system demonstration).
In Proceed-ings of the International AAAI Conference on Weblogsand Social Media.John E Hopcroft and Richard M Karp.
1973.
An n?5/2algorithm for maximum matchings in bipartite graphs.SIAM Journal on computing, 2(4):225?231.Minqing Hu and Bing Liu.
2004.
Mining and summa-rizing customer reviews.
In Proceedings of the tenthACM SIGKDD international conference on Knowl-edge discovery and data mining, Proceedings of theACM SIGKDD Conference on Knowledge Discov-ery and Data Mining, pages 168?177, New York, NY,USA.
ACM.Wei Jin, Hung Hay Ho, and Rohini K. Srihari.
2009.Opinionminer: a novel machine learning system forweb opinion mining and extraction.
In Proceedingsof the 15th ACM SIGKDD international conference onKnowledge discovery and data mining, pages 1195?1204, New York, NY, USA.
ACM.Nitin Jindal and Bing Liu.
2006.
Mining comparativesentences and relations.
In Proceedings of the 21st In-ternational Conference on Artificial Intelligence - Vol-ume 2, AAAI?06, pages 1331?1336.
AAAI Press.Richard Johansson and Alessandro Moschitti.
2011.Extracting opinion expressions and their polarities?exploration of pipelines and joint models.
In Proceed-ings of the Annual meeting of the Association for Com-putational Linguistics, volume 11, pages 101?106.Jason S. Kessler, Miriam Eckert, Lyndsie Clark, andNicolas Nicolov.
2010.
The 2010 icwsm jdpa sent-ment corpus for the automotive domain.
In 4th Inter-national AAAI Conference on Weblogs and Social Me-dia Data Workshop Challenge (ICWSM-DWC 2010).Soo-Min Kim and Eduard Hovy.
2006.
Extracting opin-ions, opinion holders, and topics expressed in onlinenews media text.
In Proceedings of the Workshop onSentiment and Subjectivity in Text, SST ?06, pages 1?8,Stroudsburg, PA, USA.
Association for ComputationalLinguistics.Dan Klein and Christopher D. Manning.
2003.
Accurateunlexicalized parsing.
In Proceedings of the 41st An-nual Meeting on Association for Computational Lin-guistics - Volume 1, ACL ?03, pages 423?430, Strouds-burg, PA, USA.
Association for Computational Lin-guistics.Lun-Wei Ku, Yu-Ting Liang, and Hsin-Hsi Chen.
2006.Opinion extraction, summarization and tracking innews and blog corpora.
In AAAI Spring Sympo-sium: Computational Approaches to Analyzing We-blogs, pages 100?107.Bing Liu, Minqing Hu, and Junsheng Cheng.
2005.Opinion observer: analyzing and comparing opinionson the web.
In Proceedings of the 14th internationalconference on World Wide Web, pages 342?351, NewYork, NY, USA.
ACM.Andr?
L. Martins, Noah A. Smith, and Eric P. Xing.2009.
Concise integer linear programming formula-tions for dependency parsing.
In Proceedings of theAnnual meeting of the Association for ComputationalLinguistics, pages 342?350.Ryan T. McDonald, Kerry Hannan, Tyler Neylon, MikeWells, and Jeffrey C. Reynar.
2007.
Structured mod-els for fine-to-coarse sentiment analysis.
In Proceed-ings of the Annual meeting of the Association for Com-putational Linguistics.Bo Pang and Lillian Lee.
2007.
Opinion mining andsentiment analysis.
Foundations and Trends in Infor-mation Retrieval, 2(1-2):1?135.166Ana-Maria Popescu and Oren Etzioni.
2005.
Extract-ing product features and opinions from reviews.
InProceedings of the conference on Human LanguageTechnology and Empirical Methods in Natural Lan-guage Processing, HLT ?05, pages 339?346, Strouds-burg, PA, USA.
Association for Computational Lin-guistics.Lizhen Qu, Georgiana Ifrim, and Gerhard Weikum.2010.
The bag-of-opinions method for review rat-ing prediction from sparse text patterns.
In Chu-RenHuang and Dan Jurafsky, editors, Proceedings of the23rd International Conference on Computational Lin-guistics (Coling 2010), ACL Anthology, pages 913?921, Beijing, China.
Tsinghua University Press.Lizhen Qu, Rainer Gemulla, and Gerhard Weikum.
2012.A weakly supervised model for sentence-level seman-tic orientation analysis with multiple experts.
In JointConference on Empirical Methods in Natural Lan-guage Processing and Computational Natural Lan-guage Learning (EMNLP-CoNLL), pages 149?159,Jeju Island, Korea, July.
Proceedings of the Annualmeeting of the Association for Computational Linguis-tics.Richard Socher, Brody Huval, Christopher D. Manning,and Andrew Y. Ng.
2012.
Semantic compositionalitythrough recursive matrix-vector spaces.
In Proceed-ings of the Conference on Empirical Methods in Natu-ral Language Processing, pages 1201?1211.Swapna Somasundaran and Janyce Wiebe.
2009.
Rec-ognizing stances in online debates.
In Proceedings ofthe Joint conference of the 47th Annual Meeting of theAssociation for Computational Linguistics and the 4thInternational Joint Conference on Natural LanguageProcessing of the Asian Federation of Natural Lan-guage Processing, pages 226?234.Maite Taboada, Julian Brooke, Milan Tofiloski, Kim-berly D. Voll, and Manfred Stede.
2011.
Lexicon-based methods for sentiment analysis.
ComputationalLinguistics, 37(2):267?307.Oscar T?ckstr?m and Ryan McDonald.
2011.
Discov-ering fine-grained sentiment with latent variable struc-tured prediction models.
In Proceedings of the 33rdEuropean conference on Advances in information re-trieval, ECIR?11, pages 368?374, Berlin, Heidelberg.Springer-Verlag.Cigdem Toprak, Niklas Jakob, and Iryna Gurevych.2010.
Sentence and expression level annotation ofopinions in user-generated discourse.
In Proceedingsof the 48th Annual Meeting of the Association forComputational Linguistics, ACL ?10, pages 575?584,Stroudsburg, PA, USA.
Association for ComputationalLinguistics.Ioannis Tsochantaridis, Thomas Hofmann, ThorstenJoachims, and Yasemin Altun.
2004.
Support vec-tor machine learning for interdependent and structuredoutput spaces.
In Proceedings of the InternationalConference on Machine Learning, pages 104?112.Wei Wei and Jon Atle Gulla.
2010.
Sentiment learn-ing on product reviews via sentiment ontology tree.
InProceedings of the Annual meeting of the Associationfor Computational Linguistics, pages 404?413.Janyce Wiebe, Theresa Wilson, and Claire Cardie.
2005.Annotating expressions of opinions and emotions inlanguage.
Language Resources and Evaluation, 39(2-3):165?210.Theresa Wilson, Janyce Wiebe, and Paul Hoffmann.2005.
Recognizing contextual polarity in phrase-levelsentiment analysis.
In Proceedings of the confer-ence on Human Language Technology and EmpiricalMethods in Natural Language Processing, HLT ?05,pages 347?354, Stroudsburg, PA, USA.
Associationfor Computational Linguistics.Theresa Ann Wilson.
2008.
Fine-grained subjectivityand sentiment analysis: recognizing the intensity, po-larity, and attitudes of private states.
Ph.D. thesis,UNIVERSITY OF PITTSBURGH.Yuanbin Wu, Qi Zhang, Xuanjing Huang, and Lide Wu.2011.
Structural opinion mining for graph-based sen-timent representation.
In Proceedings of the Confer-ence on Empirical Methods in Natural Language Pro-cessing, pages 1332?1341.Ainur Yessenalina and Claire Cardie.
2011.
Composi-tional matrix-space models for sentiment analysis.
InProceedings of the Conference on Empirical Methodsin Natural Language Processing, pages 172?182.Chun-Nam John Yu and Thorsten Joachims.
2009.Learning structural svms with latent variables.
In Pro-ceedings of the International Conference on MachineLearning, page 147.Ning Yu and Sandra K?bler.
2011.
Filling the gap:Semi-supervised learning for opinion detection acrossdomains.
In Proceedings of the Fifteenth Conferenceon Computational Natural Language Learning, pages200?209.
Association for Computational Linguistics.167168
