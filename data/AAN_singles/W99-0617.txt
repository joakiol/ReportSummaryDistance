IPOS Tags and Decision Trees for Language ModelingPeter A. HeemanDepartment of Computer Science and EngineeringOregon Graduate InstitutePO Box 91000, Portland OR 97291heeman @ cse.ogi.eduAbstractLanguage models for speech recognition con-centrate solely on recognizing the words thatwere spoken.
In this paper, we advocate re-defining the speech recognition problem so thatits goal is to find both the best sequence ofwords and their POS tags, and thus incorpo-rate POS tagging.
To use POS tags effectively,we use clustering and decision tree algorithms,which allow generalizations between POS tagsand words to be effectively used in estimatingthe probability distributions.
We show that ourPOS model gives, a reduction in word error rateand perplexity for the Trains corpus in compar-ison to word and class-based approaches.
Byusing the Wall Street Journal corpus, we showthat this approach scales up when more trainingdata is available.1 IntroductionFor recognizing spontaneous speech, the acous-tic signal is to weak to narrow down the numberof word candidates.
Hence, recognizers employa language model to take into account the likeli-hood of word seqiaences.
To do this, the recog-nition problem is Cast as finding the most likelyword sequence l?g given the acoustic signal A(Jelinek, 1985).~v = argmax Pr(WIA )WPr(AIW) Pr(W)= arg  maxw Pr(A)-- argmaxPr(AIW) Pr(W)W(1)The last line involves two probabilities thatneed to be estimated--the first due to the acous-tic model Pr(AIW ) and the second due to thelanguage model Pr(W).
The language modelprobability can be expressed as follows, wherewe rewrite the sequence W explicitly as the se-quence of N words Wi,N.NPr(W1,N) = IX Pr(W, IW~-~) (2)i=1To estimate the probability distributionPr(WilWl, i-a), a training corpus is used todetermine the relative frequencies.
Due tosparseness ofdata, one must define equivalenceclasses amongst the contexts W~,i-1, which canbe done by limiting the context o an n-gramlanguage model (Jelinek, 1985).
One can alsomix in smaller size language models whenthere is not enough data to support he largercontext by using either interpolated estimation(Jelinek and Mercer, 1980) or a backoff ap-proach (Katz, 1987).
A way of measuring theeffectiveness of the estimated probability dis-tribution is to measure the perplexity that it as-signs to a test corpus (Bahl et al, 1977).
Per-plexity is an estimate of how well the languagemodel is able to predict he next word of a testcorpus in terms of the number of alternativesthat need to be considered at each point.
Theperplexity of a test set Wi,N is calculated as 2 H,where H is the entropy, defined as follows.1 Nn - N Y~l?g2tSr(wilw~i-1) (3)i=11.1 Class-based Language ModelsThe choice of equivalence classes for a lan-guage model need not be the previous words.Words can be grouped into classes, and theseclasses can be used as the basis of the equiva-lence classes of the context rather than the word129identities (Jelinek, 1985).
Below we give theequation usually used for a class-based trigrammodel, where the function 9 maps each word toits unambiguous class.Pr(Wilg(Wd ) Pr(g(Wdlg(W~-~ )g(W~-2) )Using classes has the potential of reducing theproblem of sparseness ofdata by allowing en-eralizations over similar words, as well as re-ducing the size of the language model.To determine the word classes, one can usethe algorithm of Brown et al (1992), whichfinds the classes that give high mutual informa-tion between the classes of adjacent words.
Inother words, for each bigram wi-lwi in a train-ing corpus, choose the classes such that theclasses for adjacent words 9(wi-1) and 9(wi)lose as little information about each other aspossible.
Brown et al give a greedy algorithmfor finding the classes.
They start with eachword in a separate class and iteratively com-bine classes that lead to the smallest decreasein mutual information between adjacent words.Kneser and Ney (1993) found that a class-basedlanguage model results in a perplexity improve-ment for the LOB corpus from 541 for a word-based bigram model to 478 for a class-based bi-gram model.
Interpolating the word-based andclass-based models resulted in an improvementto 439.1.2 Previous POS-Based ModelsOne can also use POS tags, which capture thesyntactic role of each word, as the basis of theequivalence lasses (Jelinek, 1985).
Considerthe utterances "load the oranges" and "the loadof bananas".
The word "load" is being usedas an untensed verb in the first example, andas a noun in the second; and "oranges" and"bananas" are both being used as plural nouns.The POS tag of a word is influenced by, and in-fluences the neighboring words and their POStags.
To use POS tags in language modeling,the typical approach is to sum over all of thePOS possibilities.
Below, we give the deriva-tion based on using trigrams.Pr(W1,N)= Z Pr(W1,NP1,N)/~,NN= ~ H Pr(WilW~-~&*) Pr(PilW~i-lPti-i)P1,N i=1N'~ ~ IX Pr(W~tPd Pr(P~IP~-I) (4)P1,N i=1N~ H Pr(WilPd Pr(P~IP~-2,i-~)P1,N i= l(5)Note that line 4 involves some simplifyingassumptions; namely, that Pr(WilW~i-lP~i)can be approximated by Pr(WiIP~) and thatPr(PilWti-lP~i-1 ) can be approximated byPr(P/IPti_i).
These assumptions simplify thetask of estimating the probability distributions.Relative frequency can be used directly for es-timating the word probabilities, and trigrambackoff and linear interpolation can be used forestimating the POS probabilities.The above approach for incorporating POSinformation i to a language model has not beenof much success in improving speech recogni-tion performance.
Srinivas (1996) reported a24.5% increase in perplexity over a word-basedmodel on the Wall Street Journal; Niesler andWoodland (1996) reported an 11.3% increase(but a 22-fold decrease inthe number of param-eters of such a model) for the LOB corpus; andKneser and Ney (1993) report a 3% increaseon the LOB corpus.
The POS tags remove toomuch of the lexical information that is neces-sary for predicting the next word.
Only by in-terpolating it with a word-based model is anim-provement seen (Jelinek, 1985).1.3 Our ApproachIn past work (Heeman and Allen, 1997; Hee-man, 1998), we introduced an alternative for-mulation for using POS tags in a languagemodel.
Here, POS tags are elevated from inter-mediate objects to be part of the output of thespeech recognizer.
Furthermore, we do not usethe simplifying assumptions of the previous ap-proach.
Rather, we use a clustering algorithmto find words and POS tags that behave sim-ilarly.
The output of the clustering algorithmis used by a decision tree algorithm to build a130set of equivalenc e classes of the contexts fromwhich the word and POS probabilities are esti-mated.In this paper, we show that the perplexityreduction that we previous reported using ourPOS-based model on the Trains corpus doestranslate into a word error rate reduction.
TheTrains corpus is very smal| with only 58,000words of data.
Hence, we also report on per-plexity results using much larger amounts oftraining data, as afforded by using the WallStreet Journal corpus.
We discuss how we takeadvantage of the POS tags to both improve andexpedite the clustering and decision tree algo-rithms.2 Redefining the ProblemTo add POS tags rinto the language model, werefrain from simply summing over all POS se-quences as prior approaches have done.
In-stead, we redefine the speech recognition prob-lem so that it finds the best word and POS se-quence.
Let P be a POS sequence for the wordsequence W. The goal of the speech recognizeris to now solve the following.~V 15 = arg ~apX Pr(W PIA)Pr(AIWP ) Pr(WP)= arg maxwP Pr(A)=argmaxPr(AIWP ) Pr(WP) (6) wpThe first term Pr(AIWP ) is the acousticmodel, which traditionally excludes the cate-gory assignment.
In fact, the acoustic modelcan probably be reasonably approximated byPr(AIW ).
The second term Pr (WP)  is thePOS-based language model and accounts forboth the sequence of words and their POS as-signment.
We rewrite the sequence WP ex-plicitly in terms of the N words and their cor-responding POS tags, thus giving the sequenceW1,NP1,N.
The probability Pr(Wi,NP1,N)forms the basis for POS taggers, with the ex-ception that POS taggers work from a sequenceof given words.As in Equation 2, we rewrite Pr(W1,NP1,N)using the definition of conditional probability.Pr(W1,N P1,N)N= I I  Pr(WiP~IW1,HS,H)i=1N= I I  Pr(WilW~HP~,d Pr(P~IW~,HP~,H) (7)i=1Equation 7 involves two probability distribu-tions that need to be estimated.
Previous at-tempts at using POS tags in a language modelas well as POS taggers (i.e.
(Charniak et al,1993)) simplify these probability distributions,as given in Equations 8 and 9.Pr(W, lW~i-~Ptd ,~ Pr(W~\[/}) (8)Pr(PilWl, i-lPa, i-1) ~ Pr(PilP~i-1) (9)However, to successfully incorporate POS in-formation, we need to account for the full rich-ness of the probability distributions.
Hence, aswe will show in Table 1, we cannot use thesetwo assumptions when learning the probabilitydistributions.3 Estimating the ProbabilitiesTo estimate the probability distributions, wefollow the approach of Bahl et al (1989) anduse a decision tree learning algorithm (Breimanet al, 1984) to partition the context into equiv-alence classes.3.1 POS ProbabilitiesFor estimating the POS probability distribution,the algorithm starts with a single node with allof the training data.
It then finds a questionto ask about the POS tags and word identitiesof the preceding words (Pl, i-lWl, i-1) in orderto partition the node into two leaves, each be-ing more informative as to which POS tag oc-curred than the parent node.
Information the-oretic metrics, such as minimizing entropy, areused to decide which question to propose.
Theproposed question is then verified using heldoutdata: if the split does not lead to a decrease inentropy according to the heldout data, the splitis rejected and the node is not further explored(Bahl et al, 1989).
This process continues withthe new leaves and results in a hierarchical par-titioning of the context.After growing a tree, the next step is to usethe partitioning of the context induced by the131decision tree to determine the probability esti-mates.
Using the relative frequencies in eachnode will be biased towards the training datathat was used in choosing the questions.
Hence,Bahl et al smooth these probabilities with theprobabilities of the parent node using interpo-lated estimation with a second heldout dataset.Using the decision tree algorithm to estimateprobabilities is attractive since the algorithmcan choose which parts of the context are rel-evant, and in what order.
Hence, this approachlends itself more readily to allowing extra con-textual information to be included, such as boththe word identifies and POS tags, and even hi-erarchical clusterings of them.
If the extra in-formation is not relevant, it will not be used.3.2 Word Probabilities?
The procedure for estimating the word proba-bility is almost identical to the above.
However,rather than start with all of the training data ina single node, we first partition the data by thePOS tag of the word being estimated.
Hence,we start with the probability Pr(Wi \[Pi) as esti-mated by relative frequency.
This is the samevalue with which non-decision tree approachesstart (and end).
We then use the decision treealgorithm to further efine the equivalence con-texts by allowing it to ask questions about thepreceding words and POS tags.Starting the decision tree algorithm with aseparate root node for each POS tag has the fol-lowing advantages.
Words only take on a smallset of POS tags.
For instance, a word that isa superlative adjective cannot be a relative ad-jective.
For the Wall Street Journal, each tokenon average takes on 1.22 of the 46 POS tags.If we start with all training data in a single rootnode, the smoothing (no matter how small) willend up putting some probability for each wordoccurring as every POS tag, leading to less ex-act probability estimates.
Second, if we sta_twith a root node for each POS tag, the numberof words that need to be distinguished at eachnode in the tree is much less than the full vo-cabulary size.
For the Wall Street Journal cor-pus, there are approximately 42,700 differentwords in the training data, but the most com-mon POS tag, proper nouns (NNP), only has12,000 different words.
Other POS tags havemuch fewer, such as the personal pronouns withonly 36 words.
Making use of this smaller vo-cabulary size results in a faster algorithm andless memory space.A significant number of words in the train-ing corpus have a small number of occurrences.Such words will prove problematic for the de-cision tree algorithm to predict.
For each POStag, we group the low occurring words into asingle token for the decision tree to predict.This not only leads to better probability es-timates, but also reduces the number of pa-rameters in the decision tree.
For the WallStreet Joumal corpus, excluding words that oc-cur less than five times reduces the vocabularysize to 14,000 and the number of proper nounsto 3126.3.3 Questions about POS TagsThe context hat we use for estimating the prob-abilities includes both word identities and POStags.
To make effective use of this information,we need to allow the decision tree algorithm togeneralize between words and POS tags thatbehave similarly.
To learn which words be-have similarly, Black et al(1989) and Mager-man (1994) used the clustering algorithm ofBrown et al (1992) to build a hierarchical c as-sification tree.
Figure 1 gives the classifica-tion tree that we built for the POS tags fromthe Trains corpus.
The algorithm starts witheach token in a separate class and iterativelyfinds two classes to merge that results in thesmallest lost of information about POS adja-cency.
Rather than stopping at a certain num-ber of classes, one continues until only a sin-gle class remains.
However, the order in whichclasses were merged gives a hierarchical binarytree with the root corresponding to the entiretagset, each leaf to a single POS tag, and in-termediate nodes to groupings of tags that oc-cur in statistically similar contexts.
The pathfrom the root to a tag gives the binary encod-ing for the tag.
The decision tree algorithm canask which partition aword belongs to by askingquestions about the binary encoding.
Of courseit doesn't make sense to ask questions about hebits before the higher level bits are asked about.But we do allow it to ask complex bit encodingquestions o that it can find more optimal ques-132Figure 1: Classification Tree for POS Tagstions (Heeman, 1997).3.4 Questions ~ibout Word IdentitiesFor handling word identities, one could followthe approach used for handling the POS tags(e.g.
(Black et al, 1992; Magerman, 1994))and view the POS tags and word identities astwo separate sources of information.
Instead,we view the word identities as a further efine-ment of the POS tags.
We start the clusteringalgorithm with a separate class for each wordand each POS tag that it takes on and only al-low it to merge c!asses if the POS tags are thesame.
This results in a word classification treefor each POS tag.
Using POS tags in wordclustering means that words that take on differ-ent POS tags can ibe better modeled (Heeman,1997).
For instance, the word "load" can beused as a verb (V B) or as a noun (NN), andthis usage affects with which words it is simi-lar.
Furthermore, restricting merges to those of~you < low> 2 them 157me 85us 176they 89we 7668~ i  1123Figure 2: Classification Tree for Personal Pro-nounsthe same POS tag allows us to make use of thehand-annotated linguistic knowledge for clus-tering words, which allows more effective treesto be built.
It also significantly speeds up theclustering algorithm.
For the Wall Street Jour-nal, only 13% of all merges are between wordsof the same POS tag, and hence do not need tobe considered.To deal with low occurring words in thetraining data, we follow the same approach aswe do in in building the classification tree.
Wegroup all words that occur less than some fresh-hold into a single token for each POS tag beforeclustering.
This not only significantly reducesthe input size to the clustering algorithm, butalso relieves the clustering algorithm from try-ing to statistically cluster words for which thereis not enough training data.
Since low occur-ring words are grouped by POS tag, we havebetter handling of this data than if all low oc-curing words were grouped into a single token.Figure 2 shows the classification tree for thepersonal pronouns (PRP) from the Trains cor-pus.
For reference, we list the number of occur-rences of each word.
Notice that the algorithmdistinguished between the subjective pronouns"I", "we", and "they", and the objective pro-nouns "me", "us" and "them".
The pronouns"you" and "it" take both cases and were prob-ably clustered according to their most commonusage in the corpus.
Although we could haveadded extra POS tags to distinguish betweenthese two types of pronouns, it seems that theclustering algorithm can make up for some ofthe shortcomings of the POS tagset.Since words are viewed as a further refine-ment of POS information, we restrict the de-cision tree algorithm from asking about theword identity until the POS tag of the word isuniquely identified.
We also restrict he deci-133sion tree from asking more specific bit ques-tions until the less specific bits are unquely de-termined.4 Results on Trains CorpusWe ran our first set of experiments on the Trainscorpus, a corpus of human-human task orienteddialogs (Heeman and Allen, 1995).4.1 Experimental SetupTo make the best use of the limited size ofthe Trains corpus, we used a six-fold cross-validation procedure: each sixth of the data wastested using the rest of the data for training.This was done for both acoustic and languagemodels.
Dialogs for each pair of speakers weredistributed as evenly between the six partitionsin order to minimize the new speaker problem.For our perplexity results, we ran the ex-periments on the hand-collected transcripts.Changes in speaker are marked in the wordtranscription with the token <turn>.
Contrac-tions, such as "that'll" and "gonna", are treatedas separate words: "that" and '"11" for the firstexample, and "going" and "ta" for the second.All word fragments were changed to the to-ken <fragment>.
In searching for the best se-quence of POS tags for the transcribed words,we follow the technique proposed by Chow andSchwartz (1989) and only keep a small numberof alternative paths by pruning the low proba-bility paths after processing each word.For our speech recognition results, we usedOGI's large vocabulary speech recognizer (Yanet al, 1998; Wu et al, 1999), using acousticmodels trained from the Trains corpus.
Weran the decoder in a single pass using cross-word acoustic modeling and a trigram word-based backoff model (Katz, 1987) built with theCMU toolkit (Rosenfeld, 1995).
For the firstpass, contracted words were treated as singletokens in order to improve acoustic recognitionof them.
The result of the first pass was a wordgraph, which we rescored in a second pass us-ing our other trigram language models.4.2 Comparison with Word-Based ModelColumn two of Table 1 gives the results of theword-based backoff model and column threegives the results of our POS-based model.
BothWord Full SimpleBackoff Context ContentPOS Errors - 1573 1718POS Error Rate - 2.69 2.94Word Perplexity 24.8 22.6 42.4Word Error Rate 26.0 24.9 28.9Sentence Error Rate \] 56.6 55.2 58.1Table 1: Comparison with Word-Based Modelmodels were restricted to only looking at theprevious two words (and POS tags) in the con-text, and hence are trigram models.
Our POS-based model gives a perplexity reduction of8.9% and an absolute word error rate reductionof 1.1%, which was found significant by theWilcoxon test on the 34 different speakers inthe Trains corpus (Z-score of -4.64).
The POS-based model also achieves an absolute sentenceerror rate reduction of 1.3%, which was foundsignificant by the McNemar test.One reason for the good performance of ourPOS-based model is that we use all of the in-formation in the context in estimating the wordand POS probabilities.
To show this effect,we contrast he results of our model, whichuses the full context, with the results given incolumn four of a model that uses the simplercontext afforded by the approximations givenin Equation 8 and 9, which ignore word co-occurence information.
This simpler modeluses the same decision tree techniques to esti-mate the probability distributions, but the deci-sion tree can only ask questions of the simplercontext, rather than the full context.
In termsof POS tagging results, we see that using thefull context leads to a POS error rate reductionof 8.4%.
1 But more importantly, using the fullcontext gives a 46.7% reduction in perplexity,and a 4.0% absolute reduction in the word er-ror rate.
In fact, the simpler model does noteven perform as well as the word-based model.Hence, to use POS tags in speech recognition,one must use a richer context for estimatingthe probabilities than what has been tradition-ally used, and must properly account for co-occurence information.1pos errors were calculated by running both modelsagainst the actual transcripts, in the same way that per-plexity is calculated.134J4.3 Other Decision Tree ModelsThe differences between our POS-based modeland the backoff word-based model are partiallydue to the extra power of the decision tree ap-proach in estimating the probabilities.
To factorout this difference, we compare our POS-basedmodel to word and class-based models built us-ing our decision~ tree approach for estimatingthe probabilities: For the word-based model,we treated all words as having the same POStag and hence built a trivial POS classificationtree and a single word hierarchical classifica-tion tree, and then estimated the probabilitiesusing our decision tree algorithm.We also built a class-based model to test outif a model with automatically learned unam-biguous classes could perform as well as ourPOS-based model.
The classes were obtainedfrom our word clustering algorithm, but stop-ping once a certain number of classes has beenreached.
Unfortunately, the clustering algo-rithm of Brown et al does not have a mech-anism to decide an optimal number of wordclasses (cf.
(Kne:ser and Ney, 1993)).
Hence,to give an optimal evaluation of the class-basedapproach, we chose the number of classes thatgave the best word error rate, which was 30classes.
We then ,used this class-assignment i -stead of the POS tags, and used our existing al-gorithms to build our decision tree models.The results of: the three decision tree mod-els are given in Table 2, along with the resultsfrom the backoff'word-based model.
First, ourWord PerplexityWord Error RateSentence Error RateI Back?ffl Decision Tree IWord Word Class POSTable 2: POS, Class and Word-Based Modelsword-based ecision tree model outperformsthe word backoff model, giving an absoluteword-error ate reduction of 0.5%, which wasfound significant by the Wilcoxon test (Z-score-3.26).
Hence, some of the improvement of ourPOS-based model is because we use decisiontrees with word c!ustering to estimate the prob-abilities.
Second, there is little improvementfrom using unambiguous word classes.
This isbecause we are already using a word hierarchi-cal classification tree, which allows the deci-sion tree algorithm to make generalizations be-tween words, in the same way that classes do(which explains for why so few classes givesthe optimal word error rate).
Third, usingPOS tags does lead to an improvement overthe class-based model, with an absolute reduc-tion in word error rate of 0.5%, an improve-ment found significant by the Wilcoxon test (Z-score -2.73).
Hence, using shallow syntacticinformation, in the form of POS tags, does im-prove speech recognition since it allows syn-tactic knowledge to be used in predicting thesubsequent words.
This syntactic knowledge isalso used to advantage in building the classifi-cation trees, since we can use the hand-codedknowledge present in the POS tags in our clas-sification and we can better classify words thatcan be used in different ways.5 Results on Wall Street JournalIn order to show that our model scales up tolarger training data sizes and larger vocabu-lary sizes, we ran perplexity experiments on theWall Street Journal corpus in the Penn Tree-bank, which is annotated with POS tags.
Weused one-eighth of the corpus as our test set,and the rest for training.Figure 3 gives the results of varying theamount of training data from approximately45,000 words up to 1.1 million words.
Weshow both the perplexity of the POS-basedmodel and the word-based backoff model.
2 We2The perplexity measure only includes words knownin the training data.
As the training data size increases,300 , .~ Word Model280 .
~ POS-based Model ~o26O240220" ~'~'- "e+' "e" - - "~- - - ' ? '
' '~ -_200 "~'--.~...~.--'-------......__..__._Amount of Training DataFigure 3: Wall Street Journal Results135see that the POS-based model shows a consis-tent perplexity reduction over the word-basedmodel.
When using all of the available trainingdata, the POS-based model achieves a perplex-ity rate of 165.9, in comparison to 216.6 for theword-based backoff model, an improvement of23.4%.For the POS-based model, all word-POScombinations that occurred less than five timesin the training data were grouped together forclustering the words and for building the deci-sion tree.
Thus, we built the word classificationtree using 14,000 word/POS tokens, rather thanthe full set of 52,100 that occurred in the train-ing data.
Furthermore, the decision tree algo-rithm was not allowed to split a leaf with lessthan 6 datapoints.
This gave us 103,000 leafnodes (contexts) for the word tree, each withan average of 1277 probabilities, and I 11,000leaf nodes for the POS tree, each with 47 prob-abilities, for a total of 136 million parameters.In contrast, the word-based model was com-posed of 795K trigrams, 376K bigrams, and43K unigrams and used a total of 2.8 millionparameters)In the above, we compared our decision-treebased approach against he backoff approach.Although our approach gives a 23.4% reductionin perplexity, it also gives a 49-fold increasein the size of the language model.
We havedone some preliminary experiments in reduc-ing the model size.
The word and POS treescan be reduced by decreasing the number ofleaf nodes.
The word decision tree can alsobe reduced by decreasing the number of prob-ablities in each leaf, which can be done by in-creasing the number of words put into the low-occurring group.
We built a language modelusing our decision tree approach that uses only2.8 million parameters by grouping all wordsthe vocabulary increases from approximately 7500 to42,700.
Hence, fewer words of the test data are beingexcluded from the perplexity measure.3The count of 2.8 million parameters includes 795Ktrigram probabilities, 376K bigram probabilities, 376Kbigram backoff weights, 43K unigram probabilities andd3K unigram backoffweights.
Since the trigrams and bi-grams are sparse, we include 795K to indicate which tri-grams are included, and 376K to indicate which bigramsare included.that occur 40 times or fewer into the low oc-curring class, disallowing nodes to be split ifthey have 50 or fewer datapoints, and pruningback nodes that give the smallest improvementin node impurity.
The resulting word tree has13,700 leaf nodes, each with an average of 80probabilities, and the POS tree has 12,800 leafnodes, each with 47 probabilities.
This modelachieves a perplexity of 191.7, which is still a11.5% improvement over the word backoff ap-proach.
Hence, even for the same model size,the decision tree approach gives a perplexity re-duction over the word backoff approach.
46 ConclusionUnlike previous approaches that use POS tags,we redefined the speech recognition problemso that it includes finding the best word se-quence and best POS tag interpretation forthose words.
Thus this work can be seen as afirst-step towards tightening the integration be-tween speech recognition and natural languageprocessing.In order to estimate the probabilities of ourPOS-based model, we use standard algorithmsfor clustering and growing decision trees; how-ever, we have modified these algorithms to bet-ter use the POS information.
The POS-basedmodel results in a reduction in perplexity and inword error rate in comparison to a word-basedbackoff approach.
Part of this improvement isdue to the decision tree approach for estimatingthe probabilities.7 AcknowledgmentsWe wish to thank James Allen, GeraldineDamnati, Chaojun Liu, Xintian Wu, andYonghong Yan.
This research work waspartially supported by NSF under grant IRI-9623665, by the Intel Research Council, and byCNET France T616com, while the author wasvisiting there.aWe compared a word-backoff model that does notexclude any trigrams or bigrams based on thresholds.Hence, the word-backoff approach can produce muchsmaller models.
We still need to contrast our decisiontree approach with smaller backoff models.136ReferencesL.
Bahl, J. Baker, E Jelinek, and R. Mercer.1977.
Perplexity--a measure of the diffi-culty of speech: recognition tasks.
In Pro-ceedings of the '94th Meeting of the Acous-tical Society of America.L.
Bahl, P. Brown, P. deSouza, and R. Mer-cer.
1989.
A tree-based statistical languagemodel for natural anguage speech recog-nition.
IEEE Transactions on Acoustics,Speech, and Signal Processing, 36(7): 1001-1008.E.
Black, E Jelinek, J. Lafferty, D. Magerman,R.
Mercer, and S. Roukos.
1992.
Towardshistory-based grammars: Using richer mod-els for probabilistic parsing.
In Proceedingsof the DARPA Speech and Natural LanguageWorkshop, pg.
134-139.L.
Breiman, J. Friedman, R. Olshen, andC.
Stone.
1984.
Classification and Regres-sion Trees.
Wadsworth & Brooks.E Brown, V. Delia Pietra, E deSouza, J. Lai,and R. Mercer.
1992.
Class-based n-grammodels of natural language.
ComputationalLinguistics, 18(4):467-479.E.
Charniak, C. Hendrickson, N. Jacobson, andM.
Perkowitz.
11993.
Equations for part-of-speech tagging.
In Proceedings of the Na-tional Conference on Artificial Intelligence.Y.
Chow and R. Schwartz.
1989.
The n-bestalgorithm: An efficient procedure for findingtop n sentence hypotheses.
In Proceedingsof the DARPA Speech and Natural LanguageWorkshop, pg.
199-202.E Heeman and J. Allen.
1995.
The Trainsspoken dialog corpus.
CD-ROM, LinguisticsData Consortium.E Heeman and J. Allen.
1997.
Incorporat-ing POS tagging into language modeling.
InProceedings of the 5th European Conferenceon Speech Communication and Technology,pg.
2767-2770, Rhodes, Greece.E Heeman.
1997.
Speech repairs, intonationalboundaries and discourse markers: Modelingspeakers' utterances in spoken dialog.
Tech-nical Report 673, Department of ComputerScience, University of Rochester.
Doctoraldissertation.P.
Heeman.
1998.
POS tagging versus classesin language modeling.
In Sixth Workshop onVery Large Corpora, pg.
179-187, Montreal.E Jelinek and R. Mercer.
1980.
Interpo-lated estimation of markov source parame-ters from sparse data.
In Proceedings, Work-shop on Pattern Recognition in Practice,pg.
381-397, Amsterdam.E Jelinek.
1985.
Self-organized languagemodeling for speech recognition.
Technicalreport, IBM T.J. Watson Research Center,Continuous Speech Recognition Group.S.
Katz.
1987.
Estimation of probabilities fromsparse data for the language model compo-nent of a speech recognizer.
IEEE Transac-tions on Acoustics, Speech, and Signal Pro-cessing, 35(3):400--401.R.
Kneser and H. Ney.
1993.
Improved cluster-ing techniques for class-based statistical lan-guage modelling.
In Proceedings of the 3rdEuropean Conference on Speech Communi-cation and Technology, pg.
973-976.D.
Magerman.
1994.
Natural anguage pars-ing as statistical pattern recognition.
Doc-toral dissertation, Stanford University.T.
Niesler and P. Woodland.
1996.
Avariable-length category-based n-gram lan-guage model.
In Proceedings of the Inter-national Conference on Audio, Speech andSignal Processing (ICASSP), pg.
164-167.R.
Rosenfeld.
1995.
The CMU statistical lan-guage modeling toolkit and its use in the1994 ARPA CSR evaluation.
In Proceedingsof the ARPA Spoken Language Systems Tech-nology Workshop.B.
Srinivas.
1996.
"Almost parsing" tech-niques for language modeling.
In Proceed-ings of the 4th International Conferenceon Spoken Language Processing, pg.
1169-1172.X.
Wu, C. Liu, Y. Yan, D. Kim, S. Cameron,and R. Parr.
1999.
The 1998 ogi-fonixbroadcast news transcription system.
InDARPA Broadcast News Workshop.Y.
Yan, X. Wu, J. Shalkwyk, and R. Cole.1998.
Development of CSLU LVCSR: The1997 DARPA HUB4 evaluation system.
InDARPA Broadcast News Transcription andUnderstanding Workshop.137
