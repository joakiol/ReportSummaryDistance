Proceedings of the 9th Conference on Computational Natural Language Learning (CoNLL),pages 209?212, Ann Arbor, June 2005. c?2005 Association for Computational LinguisticsMaximum Entropy based Semantic Role LabelingKyung-Mi Park and Hae-Chang RimDepartment of Computer Science & Engineering, Korea University5-ka, Anam-dong, SeongBuk-gu, SEOUL, 136-701, KOREA{kmpark, rim}@nlp.korea.ac.kr1 IntroductionThe semantic role labeling (SRL) refers to findingthe semantic relation (e.g.
Agent, Patient, etc.)
be-tween a predicate and syntactic constituents in thesentences.
Especially, with the argument informa-tion of the predicate, we can derive the predicate-argument structures, which are useful for the appli-cations such as automatic information extraction.
Asprevious work on the SRL, there have been manymachine learning approaches.
(Gildea and Jurafsky,2002; Pradhan et al, 2003; Lim et al, 2004).In this paper, we present a two-phase SRL methodbased on a maximum entropy (ME) model.
We firstidentify parse constituents that represent valid se-mantic arguments of a given predicate, and then as-sign appropriate semantic roles to the the identifiedparse constituents.
In the two-phase SRL method,the performance of the argument identification phaseis very important, because the argument classifica-tion is performed on the region identified at the iden-tification phase.
In this study, in order to improve theperformance of identification, we try to incorporateclause boundary restriction and tree distance restric-tion into pre-processing of the identification phase.Since features for identifying arguments are dif-ferent from features for classifying a role, we needto determine different feature sets appropriate for thetasks.
We determine final feature sets for each phasewith experiments.
We participate in the closed chal-lenge of the CoNLL-2005 shared task and report re-sults on both development and test sets.
A detaileddescription of the task, data and related work can befound in Carreras and Ma`rquez (2005).2 System DescriptionIn this section, we describe our system that iden-tifies and classifies semantic arguments.
First, weexplain pre-processing of the identification phase.Next, we describe features employed.
Finally, weexplain classifiers used in each phase.2.1 Pre-processingWe thought that the occurrence of most semanticarguments are sensitive to the boundary of the im-mediate clause or the upper clauses of a predicate.Also, we assumed that they exist in the uniform dis-tance on the parse tree from the predicate?s parentnode (called Pp) to the parse constituent?s parentnode (called Pc).
Therefore, for identifying seman-tic arguments, we do not need to examine all parseconstituents in a parse tree.
In this study, we usethe clause boundary restriction and the tree distancerestriction, and they can provide useful informationfor spotting the probable search space which includesemantic arguments.In Figure 1 and Table 1, we show an example ofapplying the tree distance restriction.
We show thedistance between Pp=VP and the nonterminals of aparse tree in Figure 1.
For example, NP2:d=3 means3 times downward movement through the parse treefrom Pp=VP to Pc=NP2.
NP4 does not have the dis-tance from Pp because we allow to move only up-ward or only downward through the tree from Pp toPc.
In Table 1, we indicate all 14 argument can-didates that correspond to tree distance restriction(d?3).
Only 2 of the 14 argument candidates areactually served to semantic arguments (NP4, PP).209Figure 1: Distance between Pp=VP and Pc.distance direction Pc argument candidatesd=1 UP S NP4d=0 - VP PPd=1 DOWN PP IN, NP3d=2 DOWN NP3 NP1, CONJP, NP2d=3 DOWN NP1 JJ, NNS, NNd=3 DOWN CONJP RB, INd=3 DOWN NP2 NNS, NNTable 1: Probable argument candidates (d?3).2.2 FeaturesThe following features describe properties of theverb predicate.
These featues are shared by all theparse constituents in the tree.?
pred lex: this is the predicate itself.?
pred POS: this is POS of the predicate.?
pred phr: this is the syntactic category of Pp.?
pred type: this represents the predicate usagesuch as to-infinitive form, the verb predicate ofa main clause, and otherwise.?
voice: this is a binary feature identifyingwhether the predicate is active or passive.?
sub cat: this is the phrase structure rule ex-panding the predicate?s parent node in the tree.?
pt+pl: this is a conjoined feature of pred typeand pred lex.
Because the maximum entropymodel assumes the independence of features,we need to conjoin the coherent features.The following features characterize the internalstructure of a argument candidate.
These featureschange with the constituent under consideration.?
head lex: this is the headword of the argumentcandidate.
We extracts the headword by usingthe Collins?s headword rules.?
head POS: this is POS of the headword.?
head phr: this is the syntactic category of Pc.?
cont lex: this is the content word of the argu-ment candidate.
We extracts the content wordby using the head table of the chunklink.pl 1.?
cont POS: this is POS of the content word.?
gov: this is the governing category introducedby Gildea and Jurafsky (2002).The following features capture the relations be-tween the verb predicate and the constituent.?
path: this is the syntactic path through the parsetree from the parse constituent to the predicate.?
pos: this is a binary feature identifying whetherthe constituent is before or after the predicate.?
pos+clau: this, conjoined with pos, indicateswhether the constituent is located in the imme-diate clause, in the first upper clause, in the sec-ond upper clause, or in the third upper clause.?
pos+VP, pos+NP, pos+SBAR: these are nu-meric features representing the number of thespecific chunk types between the constituentand the predicate.?
pos+CC, pos+comma, pos+colon, pos+quote:these are numeric features representing thenumber of the specific POS types between theconstituent and the predicate .?
pl+hl (pred lex + head lex), pl+cl (pred lex +cont lex), v+gov (voice + gov).2.3 ClassifierThe ME classifier for the identification phase clas-sifies each parse constituent into one of the follow-ing classes: ARG class or NON-ARG class.
The MEclassifier for the classification phase classifies theidentified argument into one of the pre-defined se-mantic roles (e.g.
A0, A1, AM-ADV, AM-CAU, etc.
).1http://pi0657.kub.nl/s?abine/chunklink/chunklink 2-2-2000 for conll.pl210#exa.
%can.
#can.
%arg.
F?=1no restrictionAll1 3,709,080 - 233,394 96.06 79.37All2 2,579,278 - 233,004 95.90 79.52All3 1,598,726 100.00 231,120 95.13 79.92restriction on clause boundary1/0 1,303,596 81.54 222,238 91.47 78.971/1 1,370,760 85.74 223,571 92.02 79.142/0 1,403,630 87.80 228,891 94.21 79.662/1 1,470,794 92.00 230,224 94.76 79.893/0 1,439,755 90.06 229,548 94.48 79.633/1 1,506,919 94.26 230,881 95.03 79.79restriction on tree distance6/1 804,413 50.32 226,875 93.38 80.176/2 936,021 58.55 227,637 93.69 79.947/1 842,453 52.70 228,129 93.90 80.447/2 974,061 60.93 228,891 94.21 80.038/1 871,541 54.51 228,795 94.17 80.248/2 1,003,149 62.75 229,557 94.48 80.04restriction on clause boundary & tree distance2/1,7/1 786,951 49.22 227,523 93.65 80.122/1,8/1 803,040 50.23 228,081 93.88 80.113/1,7/1 800,740 50.09 227,947 93.82 80.283/1,8/1 822,225 51.43 228,599 94.09 80.06Table 2: Different ways of reducing candidates.3 ExperimentsTo test the proposed method, we have experimentedwith CoNLL-2005 datasets (Wall Street sections 02-21 as training set, Charniak?
trees).
The results havebeen evaluated by using the srl-eval.pl script pro-vided by the shared task organizers.
For buildingclassifiers, we utilized the Zhang le?s MaxEnt toolkit2, and the L-BFGS parameter estimation algorithmwith Gaussian Prior smoothing.Table 2 shows the different ways of reducing thenumber of argument candidates.
The 2nd and 3rdcolumns (#can., %can.)
indicate the number of ar-gument candidates and the percentage of argumentcandidates that satisfy each restriction on the train-ing set.
The 4th and 5th columns (#arg., %arg.
)indicate the number of correct arguments and thepercentage of correct arguments that satisfy each re-striction on the training set.
The last column (F?=1)indicates the performance of the identification taskon the development set by applying each restriction.In no restriction, All1 extracts candidates from allthe nonterminals?s child nodes of a tree.
All2 fil-ter the nonterminals which include at least one non-2http://www.nlplab.cn/zhangle/maxent toolkit.htmlPrec.
Recall F?=1 Accu.All 82.57 78.41 80.44 86.00All-(pred lex) 82.80 77.78 80.21 84.93All-(pred POS) 83.40 76.72 79.92 85.95All-(pred phr) 83.11 77.57 80.24 85.87All-(pred type) 82.76 77.91 80.26 85.99All-(voice) 82.87 77.88 80.30 85.88All-(sub cat) 82.48 77.68 80.00 84.88All-(pt+pl) 83.20 77.40 80.20 85.62All-(head lex) 82.58 77.87 80.16 85.61All-(head POS) 82.66 77.88 80.20 85.89All-(head phr) 83.52 76.82 80.03 85.81All-(cont lex) 82.57 77.87 80.15 85.64All-(cont POS) 82.65 77.92 80.22 86.09All-(gov) 82.69 78.34 80.46 85.91All-(path) 78.39 67.96 72.80 85.69All-(pos) 82.70 77.74 80.14 85.85All-(pos+clau) 82.94 78.34 80.57 86.19All-(pos+VP) 82.69 77.87 80.20 85.87All-(pos+NP) 82.78 77.69 80.15 85.77All-(pos+SBAR) 82.51 78.00 80.19 85.83All-(pos+CC) 82.84 78.10 80.40 85.70All-(pos+comma) 82.78 77.69 80.15 85.70All-(pos+colon) 82.67 77.96 80.25 85.72All-(pos+quote) 82.63 77.98 80.24 85.66All-(pl+hl) 82.62 77.71 80.09 84.98All-(pl+cl) 82.72 77.79 80.18 85.24All-(v+gov) 82.93 77.81 80.29 85.85Prec.
Recall F?=1 Accu.Iden.
82.56 78.72 80.59 -clas.
- - - 87.16Iden.+Clas.
72.68 69.16 70.87 -Table 3: Performance of various feature combina-tions (top) and performance of each phase (bottom).terminal child 3.
All3 filter the nonterminals whichinclude at least one nonterminal child and have dis-tance from Pp.
We use All3 as a baseline.In restriction on clause boundary, for example,2/0 means that the left search boundary for identi-fying the argument is set to the left boundary of thesecond upper clause, and the right search boundaryis set to the right boundary of the immediate clause.In restriction on tree distance, for example, 7/1means that it is possible to move up to 7 times up-ward (d?7) through the parse tree from Pp to Pc, andit is possible to move up to once downward (d?1)through the parse tree from Pp to Pc.In clause boundary & tree distance, for example,3/1,7/1 means the case when we use both the clauseboundary (3/1) and the tree distance (7/1).3We ignore the nonterminals that have only pre-terminalchildren (e.g.
in Figure 1, NP1, CONJP, NP2).211Precision Recall F?=1Development 72.68% 69.16% 70.87Test WSJ 74.69% 70.78% 72.68Test Brown 64.58% 60.31% 62.38Test WSJ+Brown 73.35% 69.37% 71.31Test WSJ Precision Recall F?=1Overall 74.69% 70.78% 72.68A0 85.02% 81.53% 83.24A1 73.98% 72.25% 73.11A2 63.20% 57.57% 60.25A3 62.96% 49.13% 55.19A4 73.40% 67.65% 70.41A5 100.00% 40.00% 57.14AM-ADV 56.73% 50.00% 53.15AM-CAU 70.21% 45.21% 55.00AM-DIR 46.48% 38.82% 42.31AM-DIS 70.95% 65.62% 68.18AM-EXT 87.50% 43.75% 58.33AM-LOC 44.09% 46.28% 45.16AM-MNR 55.56% 52.33% 53.89AM-MOD 97.59% 95.64% 96.61AM-NEG 96.05% 95.22% 95.63AM-PNC 40.68% 41.74% 41.20AM-PRD 50.00% 20.00% 28.57AM-REC 0.00% 0.00% 0.00AM-TMP 70.11% 61.73% 65.66R-A0 84.68% 83.93% 84.30R-A1 73.33% 70.51% 71.90R-A2 50.00% 31.25% 38.46R-A3 0.00% 0.00% 0.00R-A4 0.00% 0.00% 0.00R-AM-ADV 0.00% 0.00% 0.00R-AM-CAU 100.00% 25.00% 40.00R-AM-EXT 0.00% 0.00% 0.00R-AM-LOC 85.71% 57.14% 68.57R-AM-MNR 16.67% 16.67% 16.67R-AM-TMP 72.50% 55.77% 63.04V 97.32% 97.32% 97.32Table 4: Overall results (top) and detailed results onthe WSJ test (bottom).Precision Recall F?=1one-phase 71.94 68.70 70.29two-phase 72.68 69.16 70.87Table 5: Performance of one-phase vs. two-phase.According to the experimental results, we use7/1 tree distance restriction for all following ex-periments.
By applying the restriction, we can re-move about 47.3% (%can.=52.70%) of total argu-ment candidates as compared with All3.
93.90%(%arg.)
corresponds to the upper bound on recall.In order to estimate the relative contribution ofeach feature, we measure performance of each phaseon the development set by leaving out one feature ata time, as shown in the top of Table 3.
Precision,Recall, and F?=1 represent the performance of theidentification task, and Accuracy represent the per-formance of the classification task only with 100%correct argument identification respectively.
All rep-resents the performance of the experiment when all26 features introduced by section 2.2 are considered.Finally, for identification, we use 24 features exceptgov and pos+clau, and obtain an F?=1 of 80.59%, asshown in the bottom of Table 3.
Also, for classifica-tion, we use 23 features except pred type, cont POS,and pos+clau, and obtain an Accuracy of 87.16%.Table 4 presents our best system performance onthe development set, and the performance of thesame system on the test set.
Table 5 shows theperformance on the development set using the one-phase method and the two-phase method respec-tively.
The one-phase method is implemented by in-corporating the identification into the classification.one-phase shows the performance of the experimentwhen 25 features except pos+clau are used.
Exper-imental results show that the two-phase method isbetter than the one-phase method in our evaluation.4 ConclusionWe have presented a two-phase SRL method basedon a ME model.
In the two-phase method, in order toimprove the performance of identification that dom-inate the overall performance, we have performedpre-processing.
Experimental results show that oursystem obtains an F?=1 of 72.68% on the WSJ testand that the introduction of pre-processing improvesthe performance, as compared with the case whenall parse constituents are considered.ReferencesXavier Carreras and Llu?
?s Ma`rquez.
2005.
Introduction to theCoNLL-2005 Shared Task: Semantic Role Labeling.
Pro-ceedings of CoNLL-2005.Daniel Gildea and Daniel Jurafsky.
2002.
Automatic Labelingof Semantic Roles.
Computational Linguistics.Joon-Ho Lim, Young-Sook Hwang, So-Young Park and Hae-Chang Rim.
2004.
Semantic Role Labeling using MaximumEntropy Model.
Proceedings of CoNLL.Sameer Pradhan, Kadri Hacioglu, Valerie Krugler, WayneWard, James H. Martin and Daniel Jurafsky.
2003.
ShallowSemantic Parsing Using Support Vector Machines.
Techni-cal Report, TR-CSLR-2003-03.212
