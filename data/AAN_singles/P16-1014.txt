Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, pages 140?149,Berlin, Germany, August 7-12, 2016.c?2016 Association for Computational LinguisticsPointing the Unknown WordsCaglar GulcehreUniversit?e de Montr?ealSungjin AhnUniversit?e de Montr?ealRamesh NallapatiIBM T.J. Watson ResearchBowen ZhouIBM T.J. Watson ResearchYoshua BengioUniversit?e de Montr?ealCIFAR Senior FellowAbstractThe problem of rare and unknown wordsis an important issue that can potentiallyeffect the performance of many NLP sys-tems, including traditional count-basedand deep learning models.
We propose anovel way to deal with the rare and unseenwords for the neural network models us-ing attention.
Our model uses two softmaxlayers in order to predict the next word inconditional language models: one predictsthe location of a word in the source sen-tence, and the other predicts a word in theshortlist vocabulary.
At each timestep, thedecision of which softmax layer to use isadaptively made by an MLP which is con-ditioned on the context.
We motivate thiswork from a psychological evidence thathumans naturally have a tendency to pointtowards objects in the context or the envi-ronment when the name of an object is notknown.
Using our proposed model, we ob-serve improvements on two tasks, neuralmachine translation on the Europarl En-glish to French parallel corpora and textsummarization on the Gigaword dataset.1 IntroductionWords are the basic input/output units in most ofthe NLP systems, and thus the ability to cover alarge number of words is a key to building a ro-bust NLP system.
However, considering that (i)the number of all words in a language includingnamed entities is very large and that (ii) languageitself is an evolving system (people create newwords), this can be a challenging problem.A common approach followed by the recentneural network based NLP systems is to use asoftmax output layer where each of the output di-mension corresponds to a word in a predefinedword-shortlist.
Because computing high dimen-sional softmax is computationally expensive, inpractice the shortlist is limited to have only top-K most frequent words in the training corpus.
Allother words are then replaced by a special word,called the unknown word (UNK).The shortlist approach has two fundamentalproblems.
The first problem, which is known asthe rare word problem, is that some of the wordsin the shortlist occur less frequently in the train-ing set and thus are difficult to learn a good repre-sentation, resulting in poor performance.
Second,it is obvious that we can lose some important in-formation by mapping different words to a singledummy token UNK.
Even if we have a very largeshortlist including all unique words in the trainingset, it does not necessarily improve the test perfor-mance, because there still exists a chance to see anunknown word at test time.
This is known as theunknown word problem.
In addition, increasingthe shortlist size mostly leads to increasing rarewords due to Zipf?s Law.These two problems are particularly criticalin language understanding tasks such as factoidquestion answering (Bordes et al, 2015) where thewords that we are interested in are often named en-tities which are usually unknown or rare words.In a similar situation, where we have a limitedinformation on how to call an object of interest, itseems that humans (and also some primates) havean efficient behavioral mechanism of drawing at-tention to the object: pointing (Matthews et al,2012).
Pointing makes it possible to deliver in-formation and to associate context to a particularobject without knowing how to call it.
In partic-ular, human infants use pointing as a fundamentalcommunication tool (Tomasello et al, 2007).In this paper, inspired by the pointing behav-ior of humans and recent advances in the atten-140tion mechanism (Bahdanau et al, 2014) and thepointer networks (Vinyals et al, 2015), we pro-pose a novel method to deal with the rare or un-known word problem.
The basic idea is that wecan see many NLP problems as a task of predict-ing target text given context text, where some ofthe target words appear in the context as well.
Weobserve that in this case we can make the modellearn to point a word in the context and copy it tothe target text, as well as when to point.
For exam-ple, in machine translation, we can see the sourcesentence as the context, and the target sentence aswhat we need to predict.
In Figure 1, we showan example depiction of how words can be copiedfrom source to target in machine translation.
Al-though the source and target languages are differ-ent, many of the words such as named entities areusually represented by the same characters in bothlanguages, making it possible to copy.
Similarly,in text summarization, it is natural to use somewords in the original text in the summarized textas well.Specifically, to predict a target word at eachtimestep, our model first determines the source ofthe word generation, that is, whether to take onefrom a predefined shortlist or to copy one fromthe context.
For the former, we apply the typicalsoftmax operation, and for the latter, we use theattention mechanism to obtain the pointing soft-max probability over the context words and pickthe one of high probability.
The model learns thisdecision so as to use the pointing only when thecontext includes a word that can be copied to thetarget.
This way, our model can predict even thewords which are not in the shortlist, as long asit appears in the context.
Although some of thewords still need to be labeled as UNK, i.e., if it isneither in the shortlist nor in the context, in ex-periments we show that this learning when andwhere to point improves the performance in ma-chine translation and text summarization.Guillaume et Cesar ont une voiture bleue a Lausanne.Guillaume and Cesar have a blue car in Lausanne.Copy Copy CopyFrench:English:Figure 1: An example of how copying can happenfor machine translation.
Common words that ap-pear both in source and the target can directly becopied from input to source.
The rest of the un-known in the target can be copied from the inputafter being translated with a dictionary.The rest of the paper is organized as follows.
Inthe next section, we review the related works in-cluding pointer networks and previous approachesto the rare/unknown problem.
In Section 3, wereview the neural machine translation with atten-tion mechanism which is the baseline in our ex-periments.
Then, in Section 4, we propose ourmethod dealing with the rare/unknown word prob-lem, called the Pointer Softmax (PS).
The exper-imental results are provided in the Section 5 andwe conclude our work in Section 6.2 Related WorkThe attention-based pointing mechanism is intro-duced first in the pointer networks (Vinyals et al,2015).
In the pointer networks, the output space ofthe target sequence is constrained to be the obser-vations in the input sequence (not the input space).Instead of having a fixed dimension softmax out-put layer, softmax outputs of varying dimension isdynamically computed for each input sequence insuch a way to maximize the attention probabilityof the target input.
However, its applicability israther limited because, unlike our model, there isno option to choose whether to point or not; it al-ways points.
In this sense, we can see the pointernetworks as a special case of our model where wealways choose to point a context word.Several approaches have been proposed towardssolving the rare words/unknown words problem,which can be broadly divided into three categories.The first category of the approaches focuses onimproving the computation speed of the softmaxoutput so that it can maintain a very large vocabu-lary.
Because this only increases the shortlist size,it helps to mitigate the unknown word problem,but still suffers from the rare word problem.
Thehierarchical softmax (Morin and Bengio, 2005),importance sampling (Bengio and Sen?ecal, 2008;Jean et al, 2014), and the noise contrastive esti-mation (Gutmann and Hyv?arinen, 2012; Mnih andKavukcuoglu, 2013) methods are in the class.The second category, where our proposedmethod also belongs to, uses information from thecontext.
Notable works are (Luong et al, 2015)and (Hermann et al, 2015).
In particular, ap-plying to machine translation task, (Luong et al,2015) learns to point some words in source sen-tence and copy it to the target sentence, similarlyto our method.
However, it does not use atten-tion mechanism, and by having fixed sized soft-141max output over the relative pointing range (e.g.,-7, .
.
.
, -1, 0, 1, .
.
.
, 7), their model (the Posi-tional All model) has a limitation in applying tomore general problems such as summarization andquestion answering, where, unlike machine trans-lation, the length of the context and the pointinglocations in the context can vary dramatically.
Inquestion answering setting, (Hermann et al, 2015)have used placeholders on named entities in thecontext.
However, the placeholder id is directlypredicted in the softmax output rather than predict-ing its location in the context.The third category of the approaches changesthe unit of input/output itself from words to asmaller resolution such as characters (Graves,2013) or bytecodes (Sennrich et al, 2015; Gillicket al, 2015).
Although this approach has themain advantage that it could suffer less from therare/unknown word problem, the training usuallybecomes much harder because the length of se-quences significantly increases.Simultaneously to our work, (Gu et al, 2016)and (Cheng and Lapata, 2016) proposed modelsthat learn to copy from source to target and bothpapers analyzed their models on summarizationtasks.3 Neural Machine Translation Modelwith AttentionAs the baseline neural machine translation sys-tem, we use the model proposed by (Bahdanau etal., 2014) that learns to (soft-)align and translatejointly.
We refer this model as NMT.The encoder of the NMT is a bidirectionalRNN (Schuster and Paliwal, 1997).
The forwardRNN reads input sequence x = (x1, .
.
.
, xT)in left-to-right direction, resulting in a sequenceof hidden states (?
?h1, .
.
.
,??hT).
The backwardRNN reads x in the reversed direction and outputs(?
?h1, .
.
.
,??hT).
We then concatenate the hiddenstates of forward and backward RNNs at each timestep and obtain a sequence of annotation vectors(h1, .
.
.
,hT) where hj=[??hj||??hj].
Here, ||denotes the concatenation operator.
Thus, each an-notation vector hjencodes information about thej-th word with respect to all the other surroundingwords in both directions.In the decoder, we usually use gated recur-rent unit (GRU) (Cho et al, 2014; Chung et al,2014).
Specifically, at each time-step t, the soft-alignment mechanism first computes the relevanceweight etjwhich determines the contribution ofannotation vector hjto the t-th target word.
Weuse a non-linear mapping f (e.g., MLP) whichtakes hj, the previous decoder?s hidden state st?1and the previous output yt?1as input:etj= f(st?1,hj, yt?1).The outputs etjare then normalized as follows:ltj=exp(etj)?Tk=1exp(etk).
(1)We call ltjas the relevance score, or the align-ment weight, of the j-th annotation vector.The relevance scores are used to get the contextvector ctof the t-th target word in the translation:ct=T?j=1ltjhj,The hidden state of the decoder stis computedbased on the previous hidden state st?1, the con-text vector ctand the output word of the previoustime-step yt?1:st= fr(st?1, yt?1, ct), (2)where fris GRU.We use a deep output layer (Pascanu et al,2013) to compute the conditional distribution overwords:p(yt= a|y<t,x) ?exp(?a(Wo,bo)fo(st, yt?1, ct)),(3)where W is a learned weight matrix and b is abias of the output layer.
fois a single-layer feed-forward neural network.
?(Wo,bo)(?)
is a functionthat performs an affine transformation on its input.And the superscript a in ?aindicates the a-th col-umn vector of ?.The whole model, including both the encoderand the decoder, is jointly trained to maximize the(conditional) log-likelihood of target sequencesgiven input sequences, where the training corpusis a set of (xn,yn)?s.
Figure 2 illustrates the ar-chitecture of the NMT.4 The Pointer SoftmaxIn this section, we introduce our method, calledas the pointer softmax (PS), to deal with the rareand unknown words.
The pointer softmax can be142ltwtst-1ct... sth0 hk......st+1EncoderFigure 2: A depiction of neural machine transla-tion architecture with attention.
At each timestep,the model generates the attention weights lt. Weuse ltthe encoder?s hidden state to obtain the con-text ct.
The decoder uses ctto predict a vector ofprobabilities for the words wtby using softmax.applicable approach to many NLP tasks, becauseit resolves the limitations about unknown wordsfor neural networks.
It can be used in parallel withother existing techniques such as the large vocabu-lary trick (Jean et al, 2014).
Our model learns twokey abilities jointly to make the pointing mech-anism applicable in more general settings: (i) topredict whether it is required to use the pointingor not at each time step and (ii) to point any lo-cation of the context sequence whose length canvary widely over examples.
Note that the pointernetworks (Vinyals et al, 2015) are in lack of theability (i), and the ability (ii) is not achieved in themodels by (Luong et al, 2015).To achieve this, our model uses two softmaxoutput layers, the shortlist softmax and the loca-tion softmax.
The shortlist softmax is the sameas the typical softmax output layer where eachdimension corresponds a word in the predefinedword shortlist.
The location softmax is a pointernetwork where each of the output dimension cor-responds to the location of a word in the contextsequence.
Thus, the output dimension of the loca-tion softmax varies according to the length of thegiven context sequence.At each time-step, if the model decides to usethe shortlist softmax, we generate a word wtfromthe shortlist.
Otherwise, if it is expected that thecontext sequence contains a word which needs tobe generated at the time step, we obtain the loca-tion of the context word ltfrom the location soft-max.
The key to making this possible is decid-ing when to use the shortlist softmax or the lo-cation softmax at each time step.
In order to ac-complish this, we introduce a switching networkto the model.
The switching network, which isa multilayer perceptron in our experiments, takesthe representation of the context sequence (similarto the input annotation in NMT) and the previoushidden state of the output RNN as its input.
It out-puts a binary variable ztwhich indicates whetherto use the shortlist softmax (when zt= 1) or thelocation softmax (when zt= 0).
Note that if theword that is expected to be generated at each time-step is neither in the shortlist nor in the context se-quence, the switching network selects the shortlistsoftmax, and then the shortlist softmax predictsUNK.
The details of the pointer softmax modelcan be seen in Figure 3 as well.ltwtst-1ctp 1 - pft... sth0 hk...EncoderFigure 3: A simple depiction of the Pointer Soft-max(PS) architecture.
At each timestep as usuallt,ctand the wtfor the words over the limited vocab-ulary(shortlist) is being generated.
We have an ad-ditional switching variable ztthat decides whetherto use wtor copy the word from the input via lt.The final word prediction will be performed viapointer softmax ftwhich can either copy the wordfrom the source or predict the word from the short-list vocabulary.More specifically, our goal is to maximize theprobability of observing the target word sequencey = (y1, y2, .
.
.
, yTy) and the word generationsource z = (z1, z2, .
.
.
, zTy), given the context se-quence x = (x1, x2, .
.
.
, xTx):p?
(y, z|x) =Ty?t=1p?
(yt, zt|y<t, z<t,x).
(4)143Note that the word observation ytcan be eithera word wtfrom the shortlist softmax or a loca-tion ltfrom the location softmax, depending onthe switching variable zt.Considering this, we can factorize the aboveequation furtherp(y, z|x) =?t?Twp(wt, zt|(y, z)<t,x)??t?
?Tlp(lt?, zt?|(y, z)<t?,x).
(5)Here, Twis a set of time steps where zt= 1, and Tlis a set of time-steps where zt= 0.
And, Tw?Tl={1, 2, .
.
.
, Ty} and Tw?
Tl= ?.
We denote allprevious observations at step t by (y, z)<t.
Notealso that ht= f((y, z)<t).Then, the joint probabilities inside each productcan be further factorized as follows:p(wt, zt|(y, z)<t) = p(wt|zt= 1, (y, z)<t)?p(zt= 1|(y, z)<t) (6)p(lt, zt|(y, z)<t) = p(lt|zt= 0, (y, z)<t)?p(zt= 0|(y, z)<t) (7)here, we omitted x which is conditioned on allprobabilities in the above.The switch probability is modeled as a multi-layer perceptron with binary output:p(zt= 1|(y, z)<t,x) = ?
(f(x,ht?1; ?))
(8)p(zt= 0|(y, z)<t,x) = 1?
?
(f(x,ht?1; ?)).
(9)And p(wt|zt= 1, (y, z)<t,x) is the shortlist soft-max and p(lt|zt= 0, (y, z)<t,x) is the locationsoftmax which can be a pointer network.
?(?
)stands for the sigmoid function, ?
(x) =1exp(-x)+1.GivenN such context and target sequence pairs,our training objective is to maximize the followinglog likelihood w.r.t.
the model parameter ?argmax?1NN?n=1log p?
(yn, zn|xn).
(10)4.1 Basic Components of the Pointer SoftmaxIn this section, we discuss practical details of thethree fundamental components of the pointer soft-max.
The interactions between these componentsand the model is depicted in Figure 3.Location Softmax lt: The location of the wordto copy from source text to the target is predictedby the location softmax lt.
The location soft-max outputs the conditional probability distribu-tion p(lt|zt= 0, (y, z)<t,x).
For models using theattention mechanism such as NMT, we can reusethe probability distributions over the source wordsin order to predict the location of the word to point.Otherwise we can simply use a pointer network ofthe model to predict the location.Shortlist Softmax wt: The subset of the wordsin the vocabulary V is being predicted by theshortlist softmax wt.Switching network dt: The switching networkdtis an MLP with sigmoid output function thatoutputs a scalar probability of switching betweenltand wt, and represents the conditional prob-ability distribution p(zt|(y, z)<t,x).
For NMTmodel, we condition the MLP that outputs theswitching probability on the representation of thecontext of the source text ctand the hidden stateof the decoder ht.
Note that, during the training,dtis observed, and thus we do not have to sample.The output of the pointer softmax, ftwill be theconcatenation of the the two vectors, dt?wtand(1?
dt)?
lt.At test time, we compute Eqn.
(6) and (7) forall shortlist word wtand all location lt, and pickthe word or location of the highest probability.5 ExperimentsIn this section, we provide our main experimen-tal results with the pointer softmax on machinetranslation and summarization tasks.
In our ex-periments, we have used the same baseline modeland just replaced the softmax layer with pointersoftmax layer at the language model.
We use theAdadelta (Zeiler, 2012) learning rule for the train-ing of NMT models.
The code for pointer softmaxmodel is available at https://github.com/caglar/pointer_softmax.5.1 The Rarest Word DetectionWe construct a synthetic task and run some prelim-inary experiments in order to compare the resultswith the pointer softmax and the regular softmax?sperformance for the rare-words.
The vocabularysize of our synthetic task is |V |= 600 using se-quences of length 7.
The words in the sequencesare sampled according to their unigram distribu-144tion which has the form of a geometric distribu-tion.
The task is to predict the least frequent wordin the sequence according to unigram distributionof the words.
During the training, the sequencesare generated randomly.
Before the training, val-idation and test sets are constructed with a fixedseed.We use a GRU layer over the input sequenceand take the last-hidden state, in order to get thesummary ctof the input sequence.
The wt, ltare only conditioned on ct, and the MLP pre-dicting the dtis conditioned on the latent repre-sentations of wtand lt. We use minibatches ofsize 250 using adam adaptive learning rate algo-rithm (Kingma and Adam, 2015) using the learn-ing rate of 8 ?
10?4and hidden layers with 1000units.We train a model with pointer softmax wherewe assign pointers for the rarest 60 words and therest of the words are predicted from the shortlistsoftmax of size 540.
We observe that increasingthe inverse temperature of the sigmoid output ofdtto 2, in other words making the decisions of dtto become sharper, works better, i.e.
dt= ?
(2x).At the end of training with pointer softmax weobtain the error rate of 17.4% and by using soft-max over all 600 tokens, we obtain the error-rateof 48.2%.5.2 SummarizationIn these series of experiments, we use the anno-tated Gigaword corpus as described in (Rush et al,2015).
Moreover, we use the scripts that are madeavailable by the authors of (Rush et al, 2015)1to preprocess the data, which results to approxi-mately 3.8M training examples.
This script gen-erates about 400K validation and an equal numberof test examples, but we use a randomly sampledsubset of 2000 examples each for validation andtesting.
We also have made small modifications tothe script to extract not only the tokenized words,but also system-generated named-entity tags.
Wehave created two different versions of training datafor pointers, which we call UNK-pointers data andentity-pointers data respectively.For the UNK-pointers data, we trim the vocabu-lary of the source and target data in the training setand replace a word by the UNK token whenevera word occurs less than 5 times in either sourceor target data separately.
Then, we create pointers1https://github.com/facebook/NAMASfrom each UNK token in the target data to the posi-tion in the corresponding source document wherethe same word occurs in the source, as seen in thedata before UNKs were created.
It is possible thatthe source can have an UNK in the matching posi-tion, but we still created a pointer in this scenarioas well.
The resulting data has 2.7 pointers per100 examples in the training set and 9.1 pointersrate in the validation set.In the entity-pointers data, we exploit thenamed-entity tags in the annotated corpus and firstanonymize the entities by replacing them with aninteger-id that always starts from 1 for each doc-ument and increments from left to right.
Entitiesthat occur more than once in a single documentshare the same id.
We create the anonymization attoken-level, so as to allow partial entity matchesbetween the source and target for multi-token en-tities.
Next, we create a pointer from the targetto source on similar lines as before, but only forexact matches of the anonymized entities.
The re-sulting data has 161 pointers per 100 examples inthe training set and 139 pointers per 100 examplesin the validation set.If there are multiple matches in the source,either in the UNK-pointers data or the entity-pointers data, we resolve the conflict in favor ofthe first occurrence of the matching word in thesource document.
In the UNK data, we modelthe UNK tokens on the source side using a sin-gle placeholder embedding that is shared acrossall documents, and in the entity-pointers data, wemodel each entity-id in the source by a distinctplaceholder, each of which is shared across alldocuments.In all our experiments, we use a bidirectionalGRU-RNN (Chung et al, 2014) for the encoderand a uni-directional RNN for the decoder.
Tospeed-up training, we use the large-vocabularytrick (Jean et al, 2014) where we limit the vocab-ulary of the softmax layer of the decoder to 2000words dynamically chosen from the words in thesource documents of each batch and the most com-mon words in the target vocabulary.
In both ex-periments, we fix the embedding size to 100 andthe hidden state dimension to 200.
We use pre-trained word2vec vectors trained on the same cor-pus to initialize the embeddings, but we finetunedthem by backpropagating through the embeddingsduring training.
Our vocabulary sizes are fixed to125K for source and 75K for target for both exper-145iments.We use the reference data for pointers for themodel only at the training time.
During the testtime, the switch makes a decision at every timestepon which softmax layer to use.For evaluation, we use full-length Rouge F1 us-ing the official evaluation tool2.
In their work, theauthors of (Bahdanau et al, 2014) use full-lengthRouge Recall on this corpus, since the maximumlength of limited-length version of Rouge recallof 75 bytes (intended for DUC data) is alreadylong for Gigaword summaries.
However, sincefull-length Recall can unfairly reward longer sum-maries, we also use full-length F1 in our experi-ments for a fair comparison between our models,independent of the summary length.The experimental results comparing the PointerSoftmax with NMT model are displayed in Ta-ble 1 for the UNK pointers data and in Table 2for the entity pointers data.
As the experimentsshow, pointer softmax improves over the baselineNMT on both UNK data and entities data.
Ourhope was that the improvement would be largerfor the entities data since the incidence of point-ers was much greater.
However, it turns out thisis not the case, and we suspect the main reasonis anonymization of entities which removed data-sparsity by converting all entities to integer-idsthat are shared across all documents.
We believethat on de-anonymized data, our model could helpmore, since the issue of data-sparsity is more acutein this case.Table 1: Results on Gigaword Corpus when point-ers are used for UNKs in the training data, usingRouge-F1 as the evaluation metric.Rouge-1 Rouge-2 Rouge-LNMT + lvt 34.87 16.54 32.27NMT + lvt + PS 35.19 16.66 32.51Table 2: Results on anonymized Gigaword Corpuswhen pointers are used for entities, using Rouge-F1 as the evaluation metric.Rouge-1 Rouge-2 Rouge-LNMT + lvt 34.89 16.78 32.37NMT + lvt + PS 35.11 16.76 32.552http://www.berouge.com/Pages/default.aspxTable 3: Results on Gigaword Corpus for model-ing UNK?s with pointers in terms of recall.Rouge-1 Rouge-2 Rouge-LNMT + lvt 36.45 17.41 33.90NMT + lvt + PS 37.29 17.75 34.70In Table 3, we provide the results for summa-rization on Gigaword corpus in terms of recallas also similar comparison done by (Rush et al,2015).
We observe improvements on all the scoreswith the addition of pointer softmax.
Let us notethat, since the test set of (Rush et al, 2015) is notpublicly available, we sample 2000 texts with theirsummaries without replacement from the valida-tion set and used those examples as our test set.In Table 4 we present a few system gener-ated summaries from the Pointer Softmax modeltrained on the UNK pointers data.
From those ex-amples, it is apparent that the model has learned toaccurately point to the source positions wheneverit needs to generate rare words in the summary.5.3 Neural Machine TranslationIn our neural machine translation (NMT) experi-ments, we train NMT models with attention overthe Europarl corpus (Bahdanau et al, 2014) overthe sequences of length up to 50 for English toFrench translation.3.
All models are trained withearly-stopping which is done based on the negativelog-likelihood (NLL) on the development set.
Ourevaluations to report the performance of our mod-els are done on newstest2011 by using BLUEscore.4We use 30, 000 tokens for both the source andthe target language shortlist vocabularies (1 of thetoken is still reserved for the unknown words).The whole corpus contains 134, 831 unique En-glish words and 153, 083 unique French words.We have created a word-level dictionary fromFrench to English which contains translation of15,953 words that are neither in shortlist vocab-ulary nor dictionary of common words for boththe source and the target.
There are about 49, 490words shared between English and French parallelcorpora of Europarl.3In our experiments, we use an existing code, pro-vided in https://github.com/kyunghyuncho/dl4mt-material, and on the original model we onlychanged the last softmax layer for our experiments4We compute the BLEU score using the multi-blue.perlscript from Moses on tokenized sentence pairs.146Table 4: Generated summaries from NMT with PS.
Boldface words are the words copied from the source.Source #1 china ?s tang gonghong set a world record with a clean andjerk lift of ### kilograms to win the women ?s over-## kilogramweightlifting title at the asian games on tuesday .Target #1 china ?s tang <unk>,sets world weightlifting recordNMT+PS #1 china ?s tang gonghong wins women ?s weightlifting weightlift-ing title at asian gamesSource #2 owing to criticism , nbc said on wednesday that it was endinga three-month-old experiment that would have brought the firstliquor advertisements onto national broadcast network television.Target #2 advertising : nbc retreats from liquor commercialsNMT+PS #2 nbc says it is ending a three-month-old experimentSource #3 a senior trade union official here wednesday called on ghana ?sgovernment to be ?
mindful of the plight ?
of the ordinary peoplein the country in its decisions on tax increases .Target #3 tuc official,on behalf of ordinary ghanaiansNMT+PS #3 ghana ?s government urged to be mindful of the plightDuring the training, in order to decide whetherto pick a word from the source sentence using at-tention/pointers or to predict the word from theshort-list vocabulary, we use a simple heuristic.
Ifthe word is not in the short-list vocabulary, we firstcheck if the word ytitself appears in the sourcesentence.
If it is not, we check if the word it-self is in the source sentence by using the sharedwords lookup table for the source and the targetlanguage.
If the word is in the source sentence,we then use the location of the word in the sourceas the target.
Otherwise we check if one of theEnglish senses from the cross-language dictionaryof the French word is in the source.
If it is in thesource sentence, then we use the location of thatword as our translation.
Otherwise we just use theargmax of ltas the target.For switching network dt, we observed that us-ing a two-layered MLP with noisy-tanh activation(Gulcehre et al, 2016) function with residual con-nection from the lower layer (He et al, 2015) ac-tivation function to the upper hidden layers im-proves the BLEU score about 1 points over thedtusing ReLU activation function.
We initializedthe biases of the last sigmoid layer of dtto ?1such that if dtbecomes more biased toward choos-ing the shortlist vocabulary at the beginning of thetraining.
We renormalize the gradients if the normof the gradients exceed 1 (Pascanu et al, 2012).In Table 5, we provided the result of NMT withpointer softmax and we observe about 3.6 BLEUTable 5: Europarl Dataset (EN-FR)BLEU-4NMT 20.19NMT + PS 23.76score improvement over our baseline.In Figure 4, we show the validation curvesof the NMT model with attention and the NMTmodel with shortlist-softmax layer.
Pointer soft-max converges faster in terms of number of mini-batch updates and achieves a lower validationnegative-log-likelihood (NLL) (63.91) after 200kupdates over the Europarl dataset than the NMTmodel with shortlist softmax trained for 400kminibatch updates (65.26).
Pointer softmax con-verges faster than the model using the short-list softmax, because the targets provided to thepointer softmax also acts like guiding hints to theattention.6 ConclusionIn this paper, we propose a simple extension tothe traditional soft attention-based shortlist soft-max by using pointers over the input sequence.
Weshow that the whole model can be trained jointlywith single objective function.
We observe no-ticeable improvements over the baselines on ma-chine translation and summarization tasks by us-ing pointer softmax.
By doing a very simple mod-1470 5 10 15 20 25 30 35 40# Iterations (x5000 minibatches)60708090100110120130140150ValidNLLOriginal modelPointer SoftmaxFigure 4: A comparison of the validation learning-curves of the same NMT model trained withpointer softmax and the regular softmax layer.
Ascan be seen from the figures, the model trainedwith pointer softmax converges faster than the reg-ular softmax layer.
Switching network for pointersoftmax in this Figure uses ReLU activation func-tion.ification over the NMT, our model is able to gen-eralize to the unseen words and can deal with rare-words more efficiently.
For the summarizationtask on Gigaword dataset, the pointer softmax wasable to improve the results even when it is usedtogether with the large-vocabulary trick.
In thecase of neural machine translation, we observedthat the training with the pointer softmax is alsoimproved the convergence speed of the model aswell.
For French to English machine translationon Europarl corpora, we observe that using thepointer softmax can also improve the training con-vergence of the model.References[Bahdanau et al2014] Dzmitry Bahdanau, KyunghyunCho, and Yoshua Bengio.
2014.
Neural machinetranslation by jointly learning to align and translate.CoRR, abs/1409.0473.
[Bengio and Sen?ecal2008] Yoshua Bengio and Jean-S?ebastien Sen?ecal.
2008.
Adaptive importancesampling to accelerate training of a neural proba-bilistic language model.
Neural Networks, IEEETransactions on, 19(4):713?722.
[Bordes et al2015] Antoine Bordes, Nicolas Usunier,Sumit Chopra, and Jason Weston.
2015.
Large-scale simple question answering with memory net-works.
arXiv preprint arXiv:1506.02075.
[Cheng and Lapata2016] Jianpeng Cheng and MirellaLapata.
2016.
Neural summarization by ex-tracting sentences and words.
arXiv preprintarXiv:1603.07252.
[Cho et al2014] Kyunghyun Cho, BartVan Merri?enboer, Caglar Gulcehre, DzmitryBahdanau, Fethi Bougares, Holger Schwenk,and Yoshua Bengio.
2014.
Learning phraserepresentations using rnn encoder-decoder forstatistical machine translation.
arXiv preprintarXiv:1406.1078.
[Chung et al2014] Junyoung Chung, C?aglar G?ulc?ehre,KyungHyun Cho, and Yoshua Bengio.
2014.
Em-pirical evaluation of gated recurrent neural networkson sequence modeling.
CoRR, abs/1412.3555.
[Gillick et al2015] Dan Gillick, Cliff Brunk, OriolVinyals, and Amarnag Subramanya.
2015.
Mul-tilingual language processing from bytes.
arXivpreprint arXiv:1512.00103.
[Graves2013] Alex Graves.
2013.
Generating se-quences with recurrent neural networks.
arXivpreprint arXiv:1308.0850.
[Gu et al2016] Jiatao Gu, Zhengdong Lu, Hang Li,and Victor OK Li.
2016.
Incorporating copyingmechanism in sequence-to-sequence learning.
arXivpreprint arXiv:1603.06393.
[Gulcehre et al2016] Caglar Gulcehre, MarcinMoczulski, Misha Denil, and Yoshua Bengio.2016.
Noisy activation functions.
arXiv preprintarXiv:1603.00391.
[Gutmann and Hyv?arinen2012] Michael U Gutmannand Aapo Hyv?arinen.
2012.
Noise-contrastive esti-mation of unnormalized statistical models, with ap-plications to natural image statistics.
The Journal ofMachine Learning Research, 13(1):307?361.
[He et al2015] Kaiming He, Xiangyu Zhang, Shao-qing Ren, and Jian Sun.
2015.
Deep resid-ual learning for mage recognition.
arXiv preprintarXiv:1512.03385.
[Hermann et al2015] Karl Moritz Hermann, TomasKocisky, Edward Grefenstette, Lasse Espeholt, WillKay, Mustafa Suleyman, and Phil Blunsom.
2015.Teaching machines to read and comprehend.
In Ad-vances in Neural Information Processing Systems,pages 1684?1692.
[Jean et al2014] S?ebastien Jean, Kyunghyun Cho,Roland Memisevic, and Yoshua Bengio.
2014.
Onusing very large target vocabulary for neural ma-chine translation.
arXiv preprint arXiv:1412.2007.
[Kingma and Adam2015] Diederik P Kingma andJimmy Ba Adam.
2015.
A method for stochasticoptimization.
In International Conference onLearning Representation.
[Luong et al2015] Minh-Thang Luong, Ilya Sutskever,Quoc V Le, Oriol Vinyals, and Wojciech Zaremba.2015.
Addressing the rare word problem in neuralmachine translation.
In Proceedings of ACL.148[Matthews et al2012] Danielle Matthews, TanyaBehne, Elena Lieven, and Michael Tomasello.2012.
Origins of the human pointing gesture: atraining study.
Developmental science, 15(6):817?829.
[Mnih and Kavukcuoglu2013] Andriy Mnih and KorayKavukcuoglu.
2013.
Learning word embeddingsefficiently with noise-contrastive estimation.
In Ad-vances in Neural Information Processing Systems,pages 2265?2273.
[Morin and Bengio2005] Frederic Morin and YoshuaBengio.
2005.
Hierarchical probabilistic neural net-work language model.
In Aistats, volume 5, pages246?252.
Citeseer.
[Pascanu et al2012] Razvan Pascanu, Tomas Mikolov,and Yoshua Bengio.
2012.
On the difficulty oftraining recurrent neural networks.
arXiv preprintarXiv:1211.5063.
[Pascanu et al2013] Razvan Pascanu, Caglar Gulcehre,Kyunghyun Cho, and Yoshua Bengio.
2013.
Howto construct deep recurrent neural networks.
arXivpreprint arXiv:1312.6026.
[Rush et al2015] Alexander M. Rush, Sumit Chopra,and Jason Weston.
2015.
A neural attention modelfor abstractive sentence summarization.
CoRR,abs/1509.00685.
[Schuster and Paliwal1997] Mike Schuster andKuldip K Paliwal.
1997.
Bidirectional recur-rent neural networks.
Signal Processing, IEEETransactions on, 45(11):2673?2681.
[Sennrich et al2015] Rico Sennrich, Barry Haddow,and Alexandra Birch.
2015.
Neural machine trans-lation of rare words with subword units.
arXivpreprint arXiv:1508.07909.
[Theano Development Team2016] Theano Develop-ment Team.
2016.
Theano: A Python frameworkfor fast computation of mathematical expressions.arXiv e-prints, abs/1605.02688, May.
[Tomasello et al2007] Michael Tomasello, MalindaCarpenter, and Ulf Liszkowski.
2007.
A new look atinfant pointing.
Child development, 78(3):705?722.
[Vinyals et al2015] Oriol Vinyals, Meire Fortunato,and Navdeep Jaitly.
2015.
Pointer networks.
In Ad-vances in Neural Information Processing Systems,pages 2674?2682.
[Zeiler2012] Matthew D Zeiler.
2012.
Adadelta:an adaptive learning rate method.
arXiv preprintarXiv:1212.5701.7 AcknowledgmentsWe would also like to thank the developers ofTheano5, for developing such a powerful tool5http://deeplearning.net/software/theano/for scientific computing (Theano DevelopmentTeam, 2016).
We acknowledge the support ofthe following organizations for research fundingand computing support: NSERC, Samsung, Cal-cul Qu?ebec, Compute Canada, the Canada Re-search Chairs and CIFAR.
C. G. thanks for IBMT.J.
Watson Research for funding this researchduring his internship between October 2015 andJanuary 2016.149
