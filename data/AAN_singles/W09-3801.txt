Proceedings of the 11th International Conference on Parsing Technologies (IWPT), pages 1?12,Paris, October 2009. c?2009 Association for Computational LinguisticsParsing Algorithms based on Tree AutomataAndreas MalettiDepartament de Filologies Roma`niquesUniversitat Rovira i Virgili, Tarragona, Spainandreas.maletti@urv.catGiorgio SattaDepartment of Information EngineeringUniversity of Padua, Italysatta@dei.unipd.itAbstractWe investigate several algorithms relatedto the parsing problem for weighted au-tomata, under the assumption that the in-put is a string rather than a tree.
Thisassumption is motivated by several natu-ral language processing applications.
Weprovide algorithms for the computation ofparse-forests, best tree probability, insideprobability (called partition function), andprefix probability.
Our algorithms are ob-tained by extending to weighted tree au-tomata the Bar-Hillel technique, as definedfor context-free grammars.1 IntroductionTree automata are finite-state devices that recog-nize tree languages, that is, sets of trees.
Thereis a growing interest nowadays in the naturallanguage parsing community, and especially inthe area of syntax-based machine translation, forprobabilistic tree automata (PTA) viewed as suit-able representations of grammar models.
In fact,probabilistic tree automata are generatively morepowerful than probabilistic context-free gram-mars (PCFGs), when we consider the latter as de-vices that generate tree languages.
This differencecan be intuitively understood if we consider that acomputation by a PTA uses hidden states, drawnfrom a finite set, that can be used to transfer infor-mation within the tree structure being recognized.As an example, in written English we can em-pirically observe different distributions in the ex-pansion of so-called noun phrase (NP) nodes, inthe contexts of subject and direct-object positions,respectively.
This can be easily captured usingsome states of a PTA that keep a record of the dif-ferent contexts.
In contrast, PCFGs are unable tomodel these effects, because NP node expansionshould be independent of the context in the deriva-tion.
This problem for PCFGs is usually solved byresorting to so-called parental annotations (John-son, 1998), but this, of course, results in a differenttree language, since these annotations will appearin the derived tree.Most of the theoretical work on parsing and es-timation based on PTA has assumed that the in-put is a tree (Graehl et al, 2008), in accordancewith the very definition of these devices.
How-ever, both in parsing as well as in machine transla-tion, the input is most often represented as a stringrather than a tree.
When the input is a string, sometrick is applied to map the problem back to thecase of an input tree.
As an example in the con-text of machine translation, assume a probabilistictree transducer T as a translation model, and aninput string w to be translated.
One can then inter-mediately construct a tree automaton Mw that rec-ognizes the set of all possible trees that have w asyield, with internal nodes from the input alphabetof T .
This automaton Mw is further transformedinto a tree transducer implementing a partial iden-tity translation, and such a transducer is composedwith T (relation composition).
This is usuallycalled the ?cascaded?
approach.
Such an approachcan be easily applied also to parsing problems.In contrast with the cascaded approach above,which may be rather inefficient, in this paper weinvestigate a more direct technique for parsingstrings based on weighted and probabilistic treeautomata.
We do this by extending to weightedtree automata the well-known Bar-Hillel construc-tion defined for context-free grammars (Bar-Hillelet al, 1964) and for weighted context-free gram-mars (Nederhof and Satta, 2003).
This providesan abstract framework under which several pars-ing algorithms can be directly derived, based onweighted tree automata.
We discuss several appli-cations of our results, including algorithms for thecomputation of parse-forests, best tree probability,inside probability (called partition function), andprefix probability.12 Preliminary definitionsLet S be a nonempty set and ?
be an associativebinary operation on S. If S contains an element 1such that 1 ?
s = s = s ?
1 for every s ?
S, then(S, ?, 1) is a monoid.
A monoid (S, ?, 1) is com-mutative if the equation s1 ?
s2 = s2 ?
s1 holdsfor every s1, s2 ?
S. A commutative semiring(S,+, ?, 0, 1) is a nonempty set S on which a bi-nary addition + and a binary multiplication ?
havebeen defined such that the following conditions aresatisfied:?
(S,+, 0) and (S, ?, 1) are commutativemonoids,?
?
distributes over + from both sides, and?
s ?
0 = 0 = 0 ?
s for every s ?
S.A weighted string automaton, abbreviated WSA,(Schu?tzenberger, 1961; Eilenberg, 1974) is a sys-tem M = (Q,?,S, I, ?, F ) where?
Q is a finite alphabet of states,?
?
is a finite alphabet of input symbols,?
S = (S,+, ?, 0, 1) is a semiring,?
I : Q?
S assigns initial weights,?
?
: Q???Q?
S assigns a weight to eachtransition, and?
F : Q?
S assigns final weights.We now proceed with the semantics of M .
Letw ?
??
be an input string of length n. For eachinteger i with 1 ?
i ?
n, we write w(i) to denotethe i-th character of w. The set Pos(w) of posi-tions of w is {i | 0 ?
i ?
n}.
A run of M on wis a mapping r : Pos(w) ?
Q.
We denote the setof all such runs by RunM (w).
The weight of arun r ?
RunM (w) iswtM (r) =n?i=1?(r(i?
1), w(i), r(i)) .We assume the right-hand side of the above equa-tion evaluates to 1 in case n = 0.
The WSA Mrecognizes the mapping M : ??
?
S, which isdefined for every w ?
??
of length n by1M(w) = ?r?RunM (w)I(r(0)) ?wtM (r) ?F (r(n)) .In order to define weighted tree automata (Bers-tel and Reutenauer, 1982; E?sik and Kuich, 2003;Borchardt, 2005), we need to introduce some addi-tional notation.
Let ?
be a ranked alphabet, that1We overload the symbolM to denote both an automatonand its recognized mapping.
However, the intended meaningwill always be clear from the context.is, an alphabet whose symbols have an associatedarity.
We write ?k to denote the set of all k-arysymbols in ?.
We use a special symbol e ?
?0to syntactically represent the empty string ?.
Theset of ?-trees, denoted by T?, is the smallest setsatisfying both of the following conditions?
for every ?
?
?0, the single node labeled ?,written ?
(), is a tree of T?,?
for every ?
?
?k with k ?
1 and for everyt1, .
.
.
, tk ?
T?, the tree with a root node la-beled ?
and trees t1, .
.
.
, tk as its k children,written ?
(t1, .
.
.
, tk), belongs to T?.As a convention, throughout this paper we assumethat ?
(t1, .
.
.
, tk) denotes ?
() if k = 0.
The sizeof the tree t ?
T?, written |t|, is defined as thenumber of occurrences of symbols from ?
in t.Let t = ?
(t1, .
.
.
, tk).
The yield of t is recur-sively defined byyd(t) =??????
if ?
?
?0 \ {e}?
if ?
= eyd(t1) ?
?
?
yd(tk) otherwise.The set of positions of t, denoted by Pos(t), isrecursively defined byPos(?
(t1, .
.
.
, tk)) ={?}
?
{iw | 1 ?
i ?
k,w ?
Pos(ti)} .Note that |t| = |Pos(t)| and, according to our con-vention, when k = 0 the above definition providesPos(?
()) = {?}.
We denote the symbol of t atposition w by t(w) and its rank by rkt(w).A weighted tree automaton (WTA) is a systemM = (Q,?,S, ?, F ) where?
Q is a finite alphabet of states,?
?
is a finite ranked alphabet of input symbols,?
S = (S,+, ?, 0, 1) is a semiring,?
?
is an indexed family (?k)k?N of mappings?k : ?k ?
SQ?Qk , and?
F : Q?
S assigns final weights.In the above definition, Qk is the set of all stringsover Q having length k, with Q0 = {?}.
Fur-ther note that SQ?Qk is the set of all matriceswith elements in S, row index set Q, and columnindex set Qk.
Correspondingly, we will use thecommon matrix notation and write instances of ?in the form ?k(?)q0,q1??
?qk .
Finally, we assumeq1 ?
?
?
qk = ?
if k = 0.We define the semantics also in terms of runs.Let t ?
T?.
A run of M on t is a mappingr : Pos(t)?
Q.
We denote the set of all such runs2by RunM (t).
The weight of a run r ?
RunM (t)iswtM (r) =?w?Pos(t)rkt(w)=k?k(t(w))r(w),r(w1)??
?r(wk) .Note that, according to our convention, the stringr(w1) ?
?
?
r(wk) denotes ?
when k = 0.
TheWTA M recognizes the mapping M : T?
?
S,which is defined byM(t) = ?r?RunM (t)wtM (r) ?
F (r(?
))for every t ?
T?.
We say that t is recognizedby M if M(t) 6= 0.In our complexity analyses, we use the follow-ing measures.
The size of a transition (p, ?, q) in(the domain of ?
in) a WSA is |p?q| = 3.
The sizeof a transition in a WTA, viewed as an instance(?, q0, q1 ?
?
?
qk) of some mapping ?k, is definedas |?q0 ?
?
?
qk|, that is, the rank of the input symboloccurring in the transition plus two.
Finally, thesize |M | of an automaton M (WSA or WTA) isdefined as the sum of the sizes of its nonzero tran-sitions.
Note that this does not take into accountthe size of the representation of the weights.3 BinarizationWe introduce in this section a specific transfor-mation of WTA, called binarization, that reducesthe transitions of the automaton to some normalform in which no more than three states are in-volved.
This transformation maps the set of rec-ognized trees into a special binary form, in such away that the yields of corresponding trees and theirweights are both preserved.
We use this transfor-mation in the next section in order to guaranteethe computational efficiency of the parsing algo-rithm we develop.
The standard ?first-child, next-sibling?
binary encoding for trees (Knuth, 1997)would eventually result in a transformed WTA ofquadratic size.
To obtain instead a linear sizetransformation, we introduce a slightly modifiedencoding (Ho?gberg et al, 2009, Section 4), whichis inspired by (Carme et al, 2004) and the classicalcurrying operation.Let ?
be a ranked alphabet and assume afresh symbol @ /?
?
(corresponding to the ba-sic list concatenation operator).
Moreover, let?
= ?2 ?
?1 ?
?0 be the ranked alphabet suchthat ?2 = {@}, ?1 = ?k?1 ?k, and ?0 = ?0.
In?????
??
???@???@?
@?@?
?
?Figure 1: Input tree t and encoded tree enc(t).words, all the original non-nullary symbols from?
are now unary, @ is binary, and the originalnullary symbols from ?
have their rank preserved.We encode each tree of T?
as a tree of T?
as fol-lows:?
enc(?)
= ?
() for every ?
?
?0,?
enc(?
(t)) = ?
(enc(t)) for every ?
?
?1 andt ?
T?, and?
for k ?
2, ?
?
?k, and t1, .
.
.
, tk ?
T?enc(?
(t1, .
.
.
, tk)) =?
(@(enc(t1), .
.
.
@(enc(tk?1), enc(tk)) ?
?
?
)).An example of the above encoding is illustratedin Figure 1.
Note that |enc(t)| ?
O(|t|) for everyt ?
T?.
Furthermore, t can be easily reconstructedfrom enc(t) in linear time.Definition 1 LetM = (Q,?,S, ?, F ) be a WTA.The encoded WTA enc(M) is (P,?,S, ?
?, F ?
)whereP = {[q] | q ?
Q} ??
{[w] |?k(?
)q,uw 6= 0, u ?
Q?, w ?
Q+},F ?
([q]) = F (q) for every q ?
Q, and the transi-tions are constructed as follows:(i) ??0(?)[q],?
= ?0(?)q,?
for every ?
?
?0,(ii) ??1(?
)[q],[w] = ?k(?
)q,w for every ?
?
?k,k ?
1, q ?
Q, and w ?
Qk, and(iii) ?
?2(@)[qw],[q][w] = 1 for every [qw] ?
P with|w| ?
1 and q ?
Q.All remaining entries in F ?
and ??
are 0.
2Notice that each transition of enc(M) involves nomore than three states from P .
Furthermore, wehave |enc(M)| ?
O(|M |).
The following result israther intuitive (Ho?gberg et al, 2009, Lemma 4.2);its proof is therefore omitted.3Theorem 1 Let M = (Q,?,S, ?, F ) be a WTA,and let M ?
= enc(M).
Then M(t) = M ?
(enc(t))for every t ?
T?.
24 Bar-Hillel constructionThe so-called Bar-Hillel construction was pro-posed in (Bar-Hillel et al, 1964) to show thatthe intersection of a context-free language anda regular language is still a context-free lan-guage.
The proof of the result consisted in aneffective construction of a context-free grammarProd(G,N) from a context-free grammar G anda finite automaton N , such that Prod(G,N) gen-erates the intersection of the languages generatedby G and N .It was later recognized that the Bar-Hillel con-struction constitutes one of the foundations of thetheory of tabular parsing based on context-freegrammars.
More precisely, by taking the finiteautomaton N to be of some special kind, accept-ing only a single string, the Bar-Hillel constructionprovides a framework under which several well-known tabular parsing algorithms can easily be de-rived, that were proposed much later in the litera-ture.In this section we extend the Bar-Hillel con-struction to WTA, with a similar purpose of es-tablishing an abstract framework under which onecould easily derive parsing algorithms based onthese devices.
In order to guarantee computationalefficiency, we avoid here stating the Bar-Hillelconstruction for WTA with alphabets of arbitraryrank.
The next result therefore refers to WTA withalphabet symbols of rank at most 2.
These may,but need not, be automata obtained through the bi-nary encoding discussed in Section 3.Definition 2 Let M = (Q,?,S, ?, F ) be a WTAsuch that the maximum rank of a symbol in ?
is 2,and let N = (P,?0 \ {e},S, I, ?,G) be a WSAover the same semiring.
We construct the WTAProd(M,N) = (P ?Q?
P,?,S, ?
?, F ?
)as follows:(i) For every ?
?
?2, states p0, p1, p2 ?
P , andstates q0, q1, q2 ?
Q let??2(?
)(p0,q0,p2),(p0,q1,p1)(p1,q2,p2) = ?2(?
)q0,q1q2 .
(ii) For every symbol ?
?
?1, states p0, p1 ?
P ,and states q0, q1 ?
Q let??1(?
)(p0,q0,p1),(p0,q1,p1) = ?1(?
)q0,q1 .p0 p2p0 p1 p1 p2?= = =p0 p1p0 p1?= =p0 ?
p1?
(p0, ?, p1)p0 e p0=Figure 2: Information transport in the first andthird components of the states in our Bar-Hillelconstruction.
(iii) For every symbol ?
?
?0, states p0, p1 ?
P ,and q ?
Q let??0(?)(p0,q,p1),?
= ?0(?)q,?
?
swheres ={?
(p0, ?, p1) if ?
6= e1 if ?
= e and p0 = p1 .
(iv) F ?
(p0, q, p1) = I(p0) ?F (q) ?G(p1) for everyp0, p1 ?
P and q ?
Q.All remaining entries in ??
are 0.
2Theorem 2 Let M and N be as in Definition 2,and let M ?
= Prod(M,N).
If S is commutative,thenM ?
(t) = M(t) ?N(yd(t)) for every t ?
T?.2PROOF For a state q ?
P ?
Q ?
P , we write qito denote its i-th component with i ?
{1, 2, 3}.Let t ?
T?
and r ?
RunM ?
(t) be a run of M ?on t. We call the run r well-formed if for everyw ?
Pos(t):(i) if t(w) = e, then r(w)1 = r(w)3,(ii) if t(w) /?
?0, then:(a) r(w)1 = r(w1)1,(b) r(w rkt(w))3 = r(w)3, and(c) if rkt(w) = 2, then r(w1)3 = r(w2)1.Note that no conditions are placed on the secondcomponents of the states in r. We try to illustratethe conditions in Figure 2.A standard proof shows that wtM ?
(r) = 0 forall runs r ?
RunM ?
(t) that are not well-formed.We now need to map runs of M ?
back into ?cor-responding?
runs for M and N .
Let us fix somet ?
T?
and some well-formed run r ?
RunM ?
(t).4We define the run piM (r) ?
RunM (t) by lettingpiM (r)(w) = r(w)2,for every w ?
Pos(t).
Let {w1, .
.
.
, wn} ={w?
| w?
?
Pos(t), t(w?)
?
?0 \ {e}}, withw1 < ?
?
?
< wn according to the lexico-graphic order on Pos(t).
We also define the runpiN (r) ?
RunN (yd(t)) by lettingpiN (r)(i?
1) = r(wi)1,for every 1 ?
i < n, andpiN (r)(n) = r(wn)3 .Note that conversely every run of M on t and ev-ery run of N on yd(t) yield a unique run of M ?on t.Now, we claim thatwtM ?
(r) = wtM (piM (r)) ?
wtN (piN (r))for every well-formed run r ?
RunM ?(t).
Toprove the claim, let t = ?
(t1, .
.
.
, tk) for some?
?
?k, k ?
2, and t1, .
.
.
, tk ?
T?.
Moreover,for every 1 ?
i ?
k let ri(w) = r(iw) for everyw ?
Pos(ti).
Note that ri ?
RunM ?
(ti) and thatri is well-formed for every 1 ?
i ?
k.For the induction base, let ?
?
?0; we can writewtM ?
(r)= ??0(?)r(?),?={?0(?)r(?)2,?
?
?(r(?
)1, ?, r(?
)3) if ?
6= e?0(?)r(?)2,?
if ?
= e= wtM (piM (r)) ?
wtN (piN (r)) .In the induction step (i.e., k > 0) we havewtM ?
(r)= ?w?Pos(t)rkt(w)=n??n(t(w))r(w),r(w1)??
?r(wn)= ??k(?)r(?),r(1)??
?r(k) ?k?i=1wtM ?
(ri) .Using the fact that r is well-formed, commutativ-ity, and the induction hypothesis, we obtain= ?k(?)r(?)2,r(1)2??
?r(k)2 ?
?k?i=1(wtM (piM (ri)) ?
wtN (piN (ri)))= wtM (pi2(r)) ?
wtN (piN (r)) ,where in the last step we have again used the factthat r is well-formed.
Using the auxiliary state-ment wtM ?
(r) = wtM (piM (r)) ?wtN (piN (r)), themain proof now is easy.M ?
(t)= ?r?RunM?
(t)wtM ?
(r) ?
F ?(r(?
))= ?r?RunM?
(t)r well-formedwtM (piM (r)) ?
wtN (piN (r)) ??
I(r(?
)1) ?
F (r(?
)2) ?G(r(?
)3)=( ?r?RunM (t)wtM (r) ?
F (r(?)))??
( ?w=yd(t)r?RunN (w)I(r(0)) ?
wtN (r) ?G(r(|w|)))= M(t) ?N(yd(t)) Let us analyze now the computational complex-ity of a possible implementation of the construc-tion in Definition 2.
In step (i), we could restrictthe computation by considering only those transi-tions inM satisfying ?2(?
)q0,q1q2 6= 0, which pro-vides a number of choices in O(|M |).
Combinedwith the choices for the states p0, p1, p2 of N ,this provides O(|M | ?
|P |3) non-zero transitionsin Prod(M,N).
This is also a bound on the over-all running time of step (i).
Since we additionallyassume that weights can be multiplied in constanttime, it is not difficult to see that all of the remain-ing steps can be accommodated within such a timebound.
We thus conclude that the construction inDefinition 2 can be implemented to run in time andspace O(|M | ?
|P |3).5 Parsing applicationsIn this section we discuss several applications ofthe construction presented in Definition 2 that arerelevant for parsing based on WTA models.5.1 Parse forestParsing is usually defined as the problem of con-structing a suitable representation for the set of allpossible parse trees that are assigned to a given in-put string w by some grammar model.
The set ofall such parse trees is called parse forest.
The ex-tension of the Bar-Hillel construction that we have5presented in Section 4 can be easily adapted to ob-tain a parsing algorithm for WTA models.
This isdescribed in what follows.First, we should represent the input string w ina WSA that recognizes the language {w}.
Suchan automaton has a state set P = {p0, .
.
.
, p|w|}and transition weights ?
(pi?1, w(i), pi) = 1 foreach i with 1 ?
i ?
|w|.
We also set I(p0) = 1and F (p|w|) = 1.
Setting all the weights to 1 fora WSA N amounts to ignoring the weights, i.e.,those weights will not contribute in any way whenapplying the Bar-Hillel construction.Assume now that M is our grammar model,represented as a WTA.
The WTA Prod(M,N)constructed as in Definition 2 is not necessarilytrim, meaning that it might contain transitionswith non-zero weight that are never used in therecognition.
Techniques for eliminating such use-less transitions are well-known, see for instance(Ge?cseg and Steinby, 1984, Section II.6), and canbe easily implemented to run in linear time.
OnceProd(M,N) is trim, we have a device that rec-ognizes all and only those trees that are assignedby M to the input string w, and the weights ofthose trees are preserved, as seen in Theorem 2.The WTA Prod(M,N) can then be seen as a rep-resentation of a parse forest for the input string w,and we conclude that the construction in Defini-tion 2, combined with some WTA reduction al-gorithm, represents a parsing algorithm for WTAmodels working in cubic time on the length of theinput string and in linear time on the size of thegrammar model.More interestingly, from the framework devel-oped in Section 4, one can also design more effi-cient parsing algorithms based on WTA.
Borrow-ing from standard ideas developed in the litera-ture for parsing based on context-free grammars,one can specialize the construction in Definition 2in such a way that the number of useless transi-tions generated for Prod(M,N) is considerablyreduced, resulting in a more efficient construction.This can be done by adopting some search strat-egy that guides the construction of Prod(M,N)using knowledge of the input string w as well asknowledge about the source model M .As an example, we can apply step (i) only on de-mand, that is, we process a transition ??2(?
)q0,q1q2in Prod(M,N) only if we have already computednon-zero transitions of the form ?
?k1(?1)q1,w1 and?
?k2(?2)q2,w2 , for some ?1 ?
?k1 , w1 ?
Qk1 and?2 ?
?k2 , w2 ?
Qk2 where Q is the state setof Prod(M,N).
The above amounts to a bottom-up strategy that is also used in the Cocke-Kasami-Younger recognition algorithm for context-freegrammars (Younger, 1967).More sophisticated strategies are also possible.For instance, one could adopt the Earley strategydeveloped for context-free grammar parsing (Ear-ley, 1970).
In this case, parsing is carried out ina top-down left-to-right fashion, and the binariza-tion construction of Section 3 is carried out on theflight.
This has the additional advantage that itwould be possible to use WTA models that are notrestricted to the special normal form of Section 3,still maintaining the cubic time complexity in thelength of the input string.
We do not pursue thisidea any further in this paper, since our main goalhere is to outline an abstract framework for pars-ing based on WTA models.5.2 Probabilistic tree automataLet us now look into specific semirings that arerelevant for statistical natural language process-ing.
The semiring of non-negative real numbersis R?0 = (R?0,+, ?, 0, 1).
For the remainder ofthe section, let M = (Q,?,R?0, ?, F ) be a WTAover R?0.
M is convergent if?t?T?M(t) < ?.We say that M is a probabilistic tree automa-ton (Ellis, 1971; Magidor and Moran, 1970),or PTA for short, if ?k(?)q,q1??
?qk ?
[0, 1]and F (q) ?
[0, 1], for every ?
?
?k andq, q1, .
.
.
, qk ?
Q.
In other words, in a PTA allweights are in the range [0, 1] and can be inter-preted as probabilities.
For a PTA M we thereforewrite pM (r) = wt(r) and pM (t) = M(t), foreach t ?
T?
and r ?
RunM (t).A PTA is proper if?q?Q F (q) = 1 and????k,k?0,w?Qk?k(?
)q,w = 1for every q ?
Q.
Since the set of symbols is finite,we could have only required that the sum over allweights as shown with w ?
Qk equals 1 for everyq ?
Q and ?
?
?k.
A simple rescaling would thenbe sufficient to arrive at our notion.
Furthermore, aPTA is consistent if ?t?T?
pM (t) = 1.
If a PTAis consistent, then pM is a probability distributionover the set T?.6The WTAM is unambiguous if for every inputtree t ?
T?, there exists at most one r ?
RunM (t)such that r(?)
?
F and wtM (r) 6= 0.
In otherwords, in an unambiguous WTA, there exists atmost one successful run for each input tree.
Fi-nally, M is in final-state normal form if there ex-ists a state qS ?
Q such that?
F (qS) = 1,?
F (q) = 0 for every q ?
Q \ {qS}, and?
?k(?
)q,w = 0 if w(i) = qS for some1 ?
i ?
k.We commonly denote the unique final state by qS .For the following result we refer the readerto (Droste et al, 2005, Lemma 4.8) and (Bozapa-lidis, 1999, Lemma 22).
The additional propertiesmentioned in the items of it are easily seen.Theorem 3 For every WTA M there exists anequivalent WTA M ?
in final-state normal form.?
If M is convergent (respectively, proper, con-sistent), then M ?
is such, too.?
If M is unambiguous, then M ?
isalso unambiguous and for everyt ?
T?
and r ?
RunM (t) we havewtM ?(r?)
= wtM (r) ?
F (r(?))
wherer?(?)
= qS and r?
(w) = r(w) for everyw ?
Pos(t) \ {?}.
2It is not difficult to see that a proper PTA infinal-state normal form is always convergent.In statistical parsing applications we use gram-mar models that induce a probability distributionon the set of parse trees.
In these applications,there is often the need to visit a parse tree withhighest probability, among those in the parse for-est obtained from the input sentence.
This imple-ments a form of disambiguation, where the mostlikely tree under the given model is selected, pre-tending that it provides the most likely syntacticanalysis of the input string.
In our setting, theabove approach reduces to the problem of ?unfold-ing?
a tree from a PTA Prod(M,N), that is as-signed the highest probability.In order to find efficient solutions for the aboveproblem, we make the following two assumptions.?
M is in final-state normal form.
By Theo-rem 3 this can be achieved without loss ofgenerality.?
M is unambiguous.
This restrictive assump-tion avoids the so-called ?spurious?
ambigu-ity, that would result in several computationsin the model for an individual parse tree.It is not difficult to see that PTA satisfying these1: Function BESTPARSE(M)2: E ?
?3: repeat4: A ?
{q |?k(?)q,q1??
?qk > 0, q /?
E ,q1, .
.
.
, qk ?
E}5: for all q ?
A do6: ?(q)?
max???k,k?0q1,...,qk?E?k(?)q,q1??
?qk ?k?i=1?
(qi)7: E ?
E ?
{argmaxq?A?
(q)}8: until qS ?
E9: return ?
(qS)Figure 3: Search algorithm for the most probableparse in an unambiguous PTAM in final-state nor-mal form.two properties are still more powerful than theprobabilistic context-free grammar models that arecommonly used in statistical natural language pro-cessing.Once more, we borrow from the literature onparsing for context-free grammars, and adapt asearch algorithm developed by Knuth (1977); seealso (Nederhof, 2003).
The basic idea here isto generalize Dijkstra?s algorithm to compute theshortest path in a weighted graph.
The search al-gorithm is presented in Figure 3.The algorithm takes as input a trim PTA M thatrecognizes at least one parse tree.
We do not im-pose any bound on the rank of the alphabet sym-bols forM .
Furthermore,M needs not be a properPTA.
In order to simplify the presentation, we pro-vide the algorithm in a form that returns the largestprobability assigned to some tree by M .The algorithm records into the ?
(q) variablesthe largest probability found so far for a run thatbrings M into state q, and stores these states intoan agenda A.
States for which ?
(q) becomes opti-mal are popped from A and stored into a set E .Choices are made on a greedy base.
Note thatwhen a run has been found leading to an optimalprobability ?
(q), from our assumption we knowthat the associated tree has only one run that endsup in state q.Since E is initially empty (line 2), only weightssatisfying ?0(?)q,?
> 0 are considered when line 4is executed for the first time.
Later on (line 7)the largest probability is selected among all thosethat can be computed at this time, and the set E ispopulated.
As a consequence, more states become7available in the agenda in the next iteration, andnew transitions can now be considered.
The algo-rithm ends when the largest probability has beencalculated for the unique final state qS .We now analyze the computational complexityof the algorithm in Figure 3.
The ?repeat-until?loop runs at most |Q| times.
Entirely reprocess-ing setA at each iteration would be too expensive.We instead implement A as a priority heap andmaintain a clock for each weight ?k(?)q,q1??
?qk ,initially set to k. Whenever a new optimal proba-bility ?
(q) becomes available through E , we decre-ment the clock associated with each ?k(?)q,q1??
?qkby d, in case d > 0 occurrences of q are foundin the string q1 ?
?
?
qk.
In this way, at each it-eration of the ?repeat-until?
loop, we can con-sider only those weights ?k(?)q,q1??
?qk with asso-ciated clock of zero, compute new values ?
(q),and update the heap.
For each ?k(?)q,q1??
?qk > 0,all clock updates and the computation of quan-tity ?k(?)q,q1??
?qk ?
?ki=1 ?
(qi) (when the associ-ated clock becomes zero) both take an amount oftime proportional to the length of the transitionitself.
The overall time to execute these opera-tions is therefore linear in |M |.
Accounting forthe heap, the algorithm has overall running timein O(|M |+ |Q| log|Q|).The algorithm can be easily adapted to return atree having probability ?
(qS), if we keep a recordof all transitions selected in the computation alongwith links from a selected transition and all of thepreviously selected transitions that have caused itsselection.
If we drop the unambiguity assump-tion for the PTA, then the problem of comput-ing the best parse tree becomes NP-hard, througha reduction from similar problems for finite au-tomata (Casacuberta and de la Higuera, 2000).
Incontrast, the problem of computing the probabilityof all parse trees of a string, also called the insideprobability, can be solved in polynomial time inmost practical cases and will be addressed in Sub-section 5.4.5.3 NormalizationConsider the WTA Prod(M,N) obtained as inDefinition 2.
If N is a WSA encoding an in-put string w as in Subsection 5.1 and if M is aproper and consistent PTA, then Prod(M,N) isa PTA as well.
However, in general Prod(M,N)will not be proper, nor consistent.
Properness andconsistency of Prod(M,N) are convenient in allthose applications where a statistical parsing mod-ule needs to be coupled with other statistical mod-ules, in such a way that the composition of theprobability spaces still induces a probability dis-tribution.
In this subsection we deal with the moregeneral problem of how to transform a WTA thatis convergent into a PTA that is proper and con-sistent.
This process is called normalization.
Thenormalization technique we propose here has beenpreviously explored, in the context of probabilis-tic context-free grammars, in (Abney et al, 1999;Chi, 1999; Nederhof and Satta, 2003).We start by introducing some new notions.
Letus assume that M is a convergent WTA.
For everyq ?
Q, we definewtM (q) =?t?T?,r?RunM (t)r(?
)=qwtM (r) .Note that quantity wtM (q) equals the sum of theweights of all trees in T?
that would be recognizedby M if we set F (q) = 1 and F (p) = 0 for eachp ?
Q \ {q}, that is, if q is the unique final stateof M .
It is not difficult to show that, since M isconvergent, the sum in the definition of wtM (q)converges for each q ?
Q.
We will show in Sub-section 5.4 that the quantities wtM (q) can be ap-proximated to any desired precision.To simplify the presentation, and without anyloss of generality, throughout this subsection weassume that our WTA are in final-state normalform.
We can now introduce the normalizationtechnique.Definition 3 Let M = (Q,?,R?0, ?, F ) be aconvergent WTA in final-state normal form.
Weconstruct the WTANorm(M) =(Q,?,R?0, ?
?, F ) ,where for every ?
?
?k, k ?
0, andq, q1, .
.
.
, qk ?
Q??k(?)q,q1??
?qk = ?k(?)q,q1??
?qk ??
wtM (q1) ?
.
.
.
?
wtM (qk)wtM (q) .
2We now show the claimed property for ourtransformation.Theorem 4 Let M be as in Definition 3, and letM ?
= Norm(M).
Then M ?
is a proper andconsistent PTA, and for every t ?
T?
we haveM ?
(t) = M(t)wtM (qS) .
28PROOF Clearly, M ?
is again in final-state normalform.
An easy derivation shows thatwtM (q) =????kq1,...,qk?Q?k(?)q,q1??
?qk ?k?i=1wtM (qi)for every q ?
Q.
Using the previous remark, weobtain????k,q1,...,qk?Q??k(?)q,q1??
?qk= ????k,q1,...,qk?Q?k(?)q,q1??
?qk ??
wtM (q1) ?
.
.
.
?
wtM (qk?
)wtM (q)=????k,q1,...,qk?Q?k(?)q,q1??
?qk ?k?i=1wtM (qi)????k,p1,...,pk?Q?k(?)q,p1??
?pk ?k?i=1wtM (pi)= 1 ,which proves that M ?
is a proper PTA.Next, we prove an auxiliary statement.
Lett = ?
(t1, .
.
.
, tk) for some ?
?
?k, k ?
0, andt1, .
.
.
, tk ?
T?.
We claim thatwtM ?
(r) = wtM (r)wtM (r(?
))for every r ?
RunM (t) = RunM ?(t).
For ev-ery 1 ?
i ?
k, let ri ?
RunM (ti) be such thatri(w) = r(iw) for every w ?
Pos(ti).
ThenwtM ?
(r) =?w?Pos(t)rkt(w)=n??n(t(w))r(w),r(w1)??
?r(wn)= ??k(?)r(?),r(1)??
?r(k) ?k?i=1wtM ?
(ri)= ??k(?)r(?),r1(?)???rk(?)
?k?i=1wtM (ri)wtM (ri(?
))= ?k(?)r(?),r(1)??
?r(k) ?
wtM (r1) ?
?
?
?
?
wtM (rk)wtM (r(?
))= wtM (r)wtM (r(?))
.Consequently,M ?
(t) = ?r?RunM?
(t)r(?
)=qSwtM ?
(r)= ?r?RunM (t)r(?
)=qSwtM (r)wtM (qS) =M(t)wtM (qS)and?t?T?M ?
(t) = ?t?T?,r?RunM?
(t)r(?
)=qSwtM ?
(r)= ?t?T?,r?RunM (t)r(?
)=qSwtM (r)wtM (qS)= wtM (qS)wtM (qS) = 1 ,which prove the main statement and the consis-tency of M ?, respectively.
5.4 Probability mass of a stateAssumeM is a convergent WTA.
We have definedquantities wtM (q) for each q ?
Q.
Note that whenM is a proper PTA in final-state normal form, thenwtM (q) can be seen as the probability mass that?rests?
on state q.
When dealing with such PTA,we use the notation ZM (q) in place of wtM (q),and call ZM the partition function of M .
Thisterminology is borrowed from the literature on ex-ponential or Gibbs probabilistic models.In the context of probabilistic context-freegrammars, the computation of the partition func-tion has several applications, including the elim-ination of epsilon rules (Abney et al, 1999) andthe computation of probabilistic distances betweenprobability distributions realized by these for-malisms (Nederhof and Satta, 2008).
Besideswhat we have seen in Subsection 5.3, we will pro-vide one more application of partition functionsfor the computations of so-called prefix probabil-ities in Subsection 5.5 We also add that, whencomputed on the Bar-Hillel automata of Section 4,the partition function provides the so-called insideprobabilities of (Graehl et al, 2008) for the givenstates and substrings.Let |Q| = n and let us assume an arbitrary or-dering q1, .
.
.
, qn for the states in Q.
We can thenrewrite the definition of wtM (q) aswtM (q) =???
?k,k?0qi1 ,...,qik?Q?k(?
)q,qi1 ??
?qik ?k?j=1wtM (qij )(see proof of Theorem 4).
We rename wtM (qi)with the unknown Xqi , 1 ?
i ?
n, and derive a9system of n nonlinear polynomial equations of theformXqi =???
?k,k?0qi1 ,...,qik?Q?k(?
)q,qi1 ??
?qik ?Xqi1 ?
.
.
.
?Xqik= fqi(Xq1 , .
.
.
, Xqn) , (1)for each i with 1 ?
i ?
n.Throughout this subsection, we will considersolutions of the above system in the extended non-negative real number semiringR?
?0 = (R?0 ?
{?
},+, ?, 0, 1)with the usual operations extended to ?.
Wecan write the system in (1) in the compact formX = F (X), where we represent the unknownsas a vector X = (Xq1 , .
.
.
, Xqn) and F is a map-ping of type (R?
?0)n ?
(R?
?0)n consisting of thepolynomials fqi(X).We denote the vector (0, .
.
.
, 0) ?
(R?
?0)n asX0.
Let X,X ?
?
(R??0)n.
We write X ?
X ?if Xqi ?
X ?qi for every 1 ?
i ?
n. Sinceeach polynomial fqi(X) has coefficients repre-sented by positive real numbers, it is not difficultto see that, for each X,X ?
?
(R?
?0)n, we haveF (X) ?
F (X ?)
whenever X0 ?
X ?
X ?.
Thismeans that F is an order preserving, or monotone,mapping.We observe that ((R??0)n,?)
is a completelattice with least element X0 and greatest el-ement (?, .
.
.
,?).
Since F is monotone ona complete lattice, by the Knaster-Tarski theo-rem (Knaster, 1928; Tarski, 1955) there exists aleast and a greatest fixed-point of F that are solu-tions ofX = F (X).The Kleene theorem states that the least fixed-point solution of X = F (X) can be obtainedby iterating F starting with the least element X0.In other words, the sequence Xk = F (Xk?1),k = 1, 2, .
.
.
converges to the least fixed-point so-lution.
Notice that each Xk provides an approxi-mation for the partition function of M where onlytrees of depth not larger than k are considered.This means that limk?
?Xk converges to the par-tition function of M , and the least fixed-point so-lution is also the sought solution.
Thus, we canapproximate wtM (q) with q ?
Q to any degree byiterating F a sufficiently large number of times.The fixed-point iteration method discussedabove is also well-known in the numerical calcu-lus literature, and is frequently applied to systemsof nonlinear equations in general, because it canbe easily implemented.
When a number of stan-dard conditions are met, each iteration of the algo-rithm (corresponding to the value of k above) addsa fixed number of bits to the precision of the ap-proximated solution; see (Kelley, 1995) for furtherdiscussion.Systems of the form X = F (X) where allfqi(X) are polynomials with nonnegative real co-efficients are called monotone system of poly-nomials.
Monotone systems of polynomials as-sociated with proper PTA have been specificallyinvestigated in (Etessami and Yannakakis, 2005)and (Kiefer et al, 2007), where worst case resultson exponential rate of convergence are reportedfor the fixed-point method.5.5 Prefix probabilityIn this subsection we deal with one more applica-tion of the Bar-Hillel technique presented in Sec-tion 4.
We show how to compute the so-calledprefix probabilities, that is, the probability that atree recognized by a PTA generates a string start-ing with a given prefix.
Such probabilities haveseveral applications in language modeling.
As anexample, prefix probabilities can be used to com-pute the probability distribution on the terminalsymbol that follows a given prefix (under the givenmodel).For probabilistic context-free grammars, theproblem of the computation of prefix probabili-ties has been solved in (Jelinek et al, 1992); seealso (Persoon and Fu, 1975).
The approach wepropose here, originally formulated for probabilis-tic context-free grammars in (Nederhof and Satta,2003; Nederhof and Satta, 2009), is more abstractthan the previous ones, since it entirely rests onproperties of the Bar-Hillel construction that wehave already proved in Section 4.Let M = (Q,?,R?0, ?, F ) be a properand consistent PTA in final-state normal form,?
= ?0 \ {e}, and let u ?
?+ be some string.We assume here that M is in the binary formdiscussed in Section 3.
In addition, we assumethat M has been preprocessed in order to removefrom its recognized trees all of the unary branchesas well as those branches that generate the nullstring ?.
Although we do not discuss this con-struction at length in this paper, the result followsfrom a transformation casting weighted context-free grammars into Chomsky Normal Form (Fu10and Huang, 1972; Abney et al, 1999).We definePref(M,u) = {t | t ?
T?, M(t) > 0,yd(t) = uv, v ?
??}
.The prefix probability of u underM is defined as?t?Pref(M,u)pM (t) .Let |u| = n. We define a WSA Nu with stateset P = {p0, .
.
.
, pn} and transition weights?
(pi?1, u(i), pi) = 1 for each i with 1 ?
i ?
n,and ?
(pn, ?, pn) = 1 for each ?
?
?.
We alsoset I(p0) = 1 and F (pn) = 1.
It is easy to seethat Nu recognizes the language {uv | v ?
??
}.Furthermore, the PTA Mp = Prod(M,Nu) spec-ified as in Definition 2 recognizes the desired treeset Pref(M,u), and it preserves the weights ofthose trees with respect to M .
We therefore con-clude that ZMp(qS) is the prefix probability of uunder M .
Prefix probabilities can then be approx-imated using the fixed-point iteration method ofSubsection 5.4.
Rather than using an approxima-tion method, we discuss in what follows how theprefix probabilities can be exactly computed.Let us consider more closely the product au-tomaton Mp, assuming that it is trim.
Each stateof Mp has the form pi = (pi, q, pj), pi, pj ?
P andq ?
Q, with i ?
j.
We distinguish three, mutuallyexclusive cases.
(i) j < n: From our assumption that M (andthus Mp) does not have unary or ?
branches,it is not difficult to see that all ZMp(pi) can beexactly computed in time O((j ?
i)3).
(ii) i = j = n: We have pi = (pn, q, pn).Then the equations for ZMp(pi) exactlymirror the equations for ZM (q), andZMp(pi) = ZMp(q).
Because M is properand consistent, this means that ZMp(pi) = 1.
(iii) i < j = n: A close inspection of Definition 2reveals that in this case the equations (1) areall linear, assuming that we have already re-placed the solutions from (i) and (ii) aboveinto the system.
This is because any weight?2(?
)pi0,pi1pi > 0 in Mp with pi = (pi, q, pn)and i < n must have (pi1)3 < n. Quanti-ties ZMp(pi) can then be exactly computed asthe solution of a linear system of equations intime O(n3).Putting together all of the observations above,we obtain that for a proper and consistent PTA thathas been preprocessed, the prefix probability of ucan be computed in cubic time in the length of theprefix itself.6 Concluding remarksIn this paper we have extended the Bar-Hillel con-struction to WTA, closely following the method-ology proposed in (Nederhof and Satta, 2003) forweighted context-free grammars.
Based on the ob-tained framework, we have derived several parsingalgorithms for WTA, under the assumption that theinput is a string rather than a tree.As already remarked in the introduction, WTAare richer models than weighted context-freegrammar, since the formers use hidden states inthe recognition of trees.
This feature makes itpossible to define a product automaton in Defini-tion 2 that generates exactly those trees of interestfor the input string.
In contrast, in the context-free grammar case the Bar-Hillel technique pro-vides trees that must be mapped to the tree of in-terest using some homomorphism.
For the samereason, one cannot directly convert WTA intoweighted context-free grammars and then applyexisting parsing algorithms for the latter formal-ism, unless the alphabet of nonterminal symbolsis changed.
Finally, our main motivation in de-veloping a framework specifically based on WTAis that this can be extended to classes of weightedtree transducers, in order to deal with computa-tional problems that arise in machine translationapplications.
We leave this for future work.AcknowledgmentsThe first author has been supported by the Minis-terio de Educacio?n y Ciencia (MEC) under grantJDCI-2007-760.
The second author has been par-tially supported by MIUR under project PRIN No.2007TJNZRE 002.ReferencesS.
Abney, D. McAllester, and F. Pereira.
1999.
Relat-ing probabilistic grammars and automata.
In 37thAnnual Meeting of the Association for Computa-tional Linguistics, Proceedings of the Conference,pages 542?549, Maryland, USA, June.Y.
Bar-Hillel, M. Perles, and E. Shamir.
1964.
Onformal properties of simple phrase structure gram-mars.
In Y. Bar-Hillel, editor, Language and Infor-mation: Selected Essays on their Theory and Appli-cation, chapter 9, pages 116?150.
Addison-Wesley,Reading, Massachusetts.11J.
Berstel and C. Reutenauer.
1982.
Recognizable for-mal power series on trees.
Theoret.
Comput.
Sci.,18(2):115?148.B.
Borchardt.
2005.
The Theory of Recognizable TreeSeries.
Ph.D. thesis, Technische Universita?t Dres-den.S.
Bozapalidis.
1999.
Equational elements in additivealgebras.
Theory Comput.
Systems, 32(1):1?33.J.
Carme, J. Niehren, and M. Tommasi.
2004.
Query-ing unranked trees with stepwise tree automata.
InProc.
RTA, volume 3091 of LNCS, pages 105?118.Springer.F.
Casacuberta and C. de la Higuera.
2000.
Com-putational complexity of problems on probabilis-tic grammars and transducers.
In L. Oliveira, edi-tor, Grammatical Inference: Algorithms and Appli-cations; 5th International Colloquium, ICGI 2000,pages 15?24.
Springer.Z.
Chi.
1999.
Statistical properties of probabilisticcontext-free grammars.
Computational Linguistics,25(1):131?160.M.
Droste, C. Pech, and H. Vogler.
2005.
A Kleenetheorem for weighted tree automata.
Theory Com-put.
Systems, 38(1):1?38.J.
Earley.
1970.
An efficient context-free parsing algo-rithm.
Communications of the ACM, 13(2):94?102,February.S.
Eilenberg.
1974.
Automata, Languages, and Ma-chines, volume 59 of Pure and Applied Math.
Aca-demic Press.C.
A. Ellis.
1971.
Probabilistic tree automata.
Infor-mation and Control, 19(5):401?416.Z.
E?sik and W. Kuich.
2003.
Formal tree series.
J.Autom.
Lang.
Combin., 8(2):219?285.K.
Etessami and M. Yannakakis.
2005.
RecursiveMarkov chains, stochastic grammars, and monotonesystems of nonlinear equations.
In 22nd Interna-tional Symposium on Theoretical Aspects of Com-puter Science, volume 3404 of Lecture Notes inComputer Science, pages 340?352, Stuttgart, Ger-many.
Springer-Verlag.K.S.
Fu and T. Huang.
1972.
Stochastic grammars andlanguages.
International Journal of Computer andInformation Sciences, 1(2):135?170.F.
Ge?cseg and M. Steinby.
1984.
Tree Automata.Akade?miai Kiado?, Budapest.J.
Graehl, K. Knight, and J.
May.
2008.
Training treetransducers.
Comput.
Linguist., 34(3):391?427.J.
Ho?gberg, A. Maletti, and H. Vogler.
2009.
Bisim-ulation minimisation of weighted automata on un-ranked trees.
Fundam.
Inform.
to appear.F.
Jelinek, J.D.
Lafferty, and R.L.
Mercer.
1992.
Basicmethods of probabilistic context free grammars.
InP.
Laface and R. De Mori, editors, Speech Recogni-tion and Understanding ?
Recent Advances, Trendsand Applications, pages 345?360.
Springer-Verlag.M.
Johnson.
1998.
PCFG models of linguistictree representations.
Computational Linguistics,24(4):613?632.C.
T. Kelley.
1995.
Iterative Methods for Linear andNonlinear Equations.
Society for Industrial and Ap-plied Mathematics, Philadelphia, PA.S.
Kiefer, M. Luttenberger, and J. Esparza.
2007.
Onthe convergence of Newton?s method for monotonesystems of polynomial equations.
In Proceedings ofthe 39th ACM Symposium on Theory of Computing,pages 217?266.B.
Knaster.
1928.
Un the?ore`me sur les fonctionsd?ensembles.
Ann.
Soc.
Polon.
Math., 6:133?134.D.
E. Knuth.
1977.
A generalization of Dijkstra?s al-gorithm.
Information Processing Letters, 6(1):1?5,February.D.
E. Knuth.
1997.
Fundamental Algorithms.
The Artof Computer Programming.
Addison Wesley, 3rdedition.M.
Magidor and G. Moran.
1970.
Probabilistic treeautomata and context free languages.
Israel Journalof Mathematics, 8(4):340?348.M.-J.
Nederhof and G. Satta.
2003.
Probabilistic pars-ing as intersection.
In 8th International Workshopon Parsing Technologies, pages 137?148, LORIA,Nancy, France, April.M.-J.
Nederhof and G. Satta.
2008.
Computation ofdistances for regular and context-free probabilisticlanguages.
Theoretical Computer Science, 395(2-3):235?254.M.-J.
Nederhof and G. Satta.
2009.
Computing parti-tion functions of PCFGs.
Research on Language &Computation, 6(2):139?162.M.-J.
Nederhof.
2003.
Weighted deductive parsingand Knuth?s algorithm.
Computational Linguistics,29(1):135?143.E.
Persoon and K.S.
Fu.
1975.
Sequential classi-fication of strings generated by SCFG?s.
Interna-tional Journal of Computer and Information Sci-ences, 4(3):205?217.M.
P. Schu?tzenberger.
1961.
On the definition of afamily of automata.
Information and Control, 4(2?3):245?270.A.
Tarski.
1955.
A lattice-theoretical fixpoint theoremand its applications.
Pacific J.
Math., 5(2):285?309.D.
H. Younger.
1967.
Recognition and parsing ofcontext-free languages in time n3.
Information andControl, 10:189?208.12
