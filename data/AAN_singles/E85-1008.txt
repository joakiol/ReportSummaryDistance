Effective Parsing With Generalised Phrase Structure GrammarAllan RamsayCognitive Studies Program, University of SussexBrighton, BN1 9QN, EnglandAbstractGeneralised phrase structure grammars (GPSG's)appear to offer a means by which the syntacticproperties of natural languages may be very con-cisely described.
The main reason for this is thatthe GPSG framework allows you to state a variety ofmeta-grammatical rules which generate new rulesfrom old ones, so that you can specify rules witha wide variety of realisations via a very smallnumber of explicit statements.
Unfortunately,trying to analyse a piece of text in terms of suchrules is a very awkward task, as even a small setof GPSG statements will generate a large number ofunderlying rules.This paper discusses some of the difficulties ofparsing with GPSG's, and presents a fairlystraightforward bottom-up parser for them.
Thisparser is, in itself, no more than adequate - allits components are implemented quite efficiently,but there is nothing tremendously clever abouthow it searches the space of possible rules tofind an analysis of the text it is working on.Its power comes from the fact that it learns fromexperience: not new rules, but how to recogniserealisations of complex combinations of itsexisting rules.
The improvement in the system'sperformance after even a few trials is dramatic.This is brought about by a mechanism for recordingthe analysis of text fragments.
Such recordingsmay be used very effectively to guide the sub-sequent analysis of similar pieces of text.
Givensuch guidance it becomes possible to deal even withtext containing unknown or ambiguous words withvery little search.I.
Generalised Phrase Structure GrammarThere has been considerable interest recently ina grammatical framework known as "generalisedphrase structure grammar" (GPSG).
This frameworkextends the expressive power of simple contextfree grammars (CFG's} in a number of ways whichenable complex systems of regularities andrestrictions to be stated very easily.
Advocatesof GPSG claim that it enables concise statementsof general rules; and that it provides precisedescriptions of the syntactic properties of stringsof lexical items.
For the purpose of this paperI shall assume without further discussion thatthese claims are true enough for GPSG's to beconsidered interesting and potentially useful.The problem is that straightforward parsingalgorithms for GPSG's can take a long time to run- the CFG which you get by expanding out all therules of a moderately complex GPSG is so enormousthat finding a set of rules which fits a giveninput string is a very time-consuming task.
Theaim of this paper is to show how some of that timemay be saved.The GPSG framework has been described indetai l  ina number of other places.
The discussion in thispaper follows Gazdar and Pullum \[Gazdar & Pullum\],\[Gazdar et al\], though as these authors point outa number of the ideas they present have beendiscussed by other people as well.
For readerswho are entirely unfamiliar with GPSG I shallbriefly outline enough of its most salient featuresto make the remainder of the paper comprehensible- other readers should skip to the next section.GPSG starts by taking simple CF rules and notingthat they carry two sorts of information.
TheCF rule(I) s --) NP vPsays that whenever you have the symbol S you mayrewrite it as NP VP, i.e.
as the set NP, VP withNP written before the VP.
GPSG separates outthese facets of the rule, so that a grammar con-sisting of the single CF rule given above wouldbe written as(2a) S -~ NP, VP(2b) NP << VPi.e.
as an "~mmediate dominance" (ID) rule, sayingthat the set of symbols ~S~ may be replaced by theset of symbols NP, VP and a "linear precedence"(LP) rule which says that in any application ofany ID rule involving a NP and a VP, the NP mustprecede the VP.
There is some doubt as to whetherthey should be tied to specific groups of ID rules.It makes little difference to the algorithmsoutlined here one way or the other - for simplicityof exposition it will be assumed that LP rules areuniversal.In the trivial case cited here, the switch from aCFG to ID/LP format has increased the number ofrules required, but in more complicated cases itgenerally decreases the number of statementsneeded in order to specify the grammar.57ID/LP format allows you to specify large sets ofCF rules in a few statements.
GPSG provides twofurther ways of extending the sets of CF rules inyour grammar.
The first is to allow the elementsof a rule to be complex sets of feature/value pairs,rather than Just allowing atomic symbols.
The rhsof rule 2a, for instance, refers to items whichcontain the feature/value pairs \[category NP:\] and\[category VP\] respectively, with no explicitreference to other features or their expectedvalues (though there will generally be a numberof implicit restrictions on these, derived fromthe specification of the features in the grammar andtheir interactions).
Thus 2a in fact specifies awhole family of CF ID rules, namely the set \[allpossible combinations of feature/value pairs whichinclude \[category NP\]) X \[all possible combinationsof feature/value pairs which include \[category VP\]}.In theory tbls set could be expanded out, but itis not a tempting prospect - it would simply takea lot of effort, waste a lot of space, and lose thegeneralisation captured by 2a.The other way of extending the grammar is to includemetarules, i.e.
rules which say that if you have arule that matches a given pattern, you should alsohave another, derived, rule.
For instance, themetarule(3) VP -9 .. .
.
NP ==>VP \[passive\] -9 ..., PP\[by\]says that for any rule stating that a VP may bemade up of some set of items including a NP (the... means any, possible empty, set of items), youshould have a rule which states that a passive VPmay be made up of the same set of items but withthe NP replaced by a PP of type "by".
Metarulesare applied until they close, i.e.
whenever ametarule is applied and produces a new rule, theentire set of metarules is scanned to see if anyof them can be applied to this new rule.There are two further points about GPSG which areworth noting before we move on to see how to parseusing the vast set of rules induced by a set ofID, LP and meta rules.
Firstly, it is customaryto include in the feature set of each lexlcal itema list containing the names of all the ID rules inwhich that item may take part.
This induces afiner classification of lexical items than the oneimplied by the simple division into categories suchas verb, noun, preposition, ... (this classificationis often referred to as "lexical subcategorisation",i.e.
splitting lexical items into subsets of theusual categories).
Secondly, the inheritance offeatures when several items are combined to makea single more complex structure is governed by tworules, the "head feature convention" (HFC) and the"foot feature principle" (FFP).
Very briefly:features are divided into "head features" and"foot features".
The HFC says that head featuresare inherited from the "head", i.e.
that sub-structure which has the same basic category (verb,noun, ...) as the complex structure and which isof lowest degree out of all the substructures ofthis type.
The FFP says that foot features areinherited by studying all the other, non-bead,substructures and copying those foot features onwhich they do not disagree (i.e.
they need not allinclude a value for each foot feature, but a footfeature will not be copied if there are items whichinclude different values for it).The foregoing is very far from being a completedescription of the GPSG framework.
It should bedetailed enough to give an idea of how rules arestated within the framework; and it should bedetailed enough to make the rest of the papercomprehensible.2o ParsinB Witb GPSO'sParsing with a GPSG is essentially the same asparsing with any of the other common grammaticalsystems.
Given a string of lexical items, find somesequence of rules from the grammar which will combineitems from the string together so that all thatremains is a single structure, labelled with thestart symbol of the grammar and covering the wholeof the original text.
The same decisions have tobe made when designing a parser for GPSG as forthe design of any parser for a grammar specifiedas a set of rewrite rules (this includes ATN's)- top down : bottom up, left - right : islandbuilding, depth first : breadth first : pseudoparallel.
With GPSG there is yet another questionto be answered before you can start to put yourparser together: how far should the rule set beexpanded when the rules are read in?There are two extreme positions on this.
(i) Youcould leave the rules in the form in which theywere stated, i.e.
as a collection of ID rules, plusa set of metarules which will generate new rulesfrom the base set, plus a set of LP rules whichrestrict the order in which constituents of therhs of a rule may appear.
( i i )  You could expandout the entire set of CF rules, first comparingthe ID rules with the metarules and constructingnew ID rules as appropriate until no new rules weregenerated; then generating all the ordered per-mutations of rhs's allowed by the LP rules; andfinally expanding the specified feature sets whichmake up each constituent of a rule in all possibleways.Neither of these options is attractive.
As Thompsonpointed out, (i) is untenable, since metarules canalter rules by adding or deleting arbitrary elements\[Thompson 82\].
This means that if you were workingtop down, you would not even know how the startsymbol might be rewritten without considering allthe metarules that might expand the basic ID ruleswhich rewrite it; working bottom up would be nobetter, since you would always have to worry aboutbasic ID rules which might be altered so theycovered the case you were looking at.
At everystage, whether you are working down from the topor up from the bottom, the rule you want may beone that is introduced by a metaruie; you haveno way of knowing, and no easy way of selectingpotentially relevant basic rules and metarules.On the other hand, expanding the grammar right outto the underlying CF rules, as in (li), looks asthough it will introduce very large numbers ofrules which are only trivially distinct.
It mayS8conceivably be easier to parse with families offully instantiated rules than with rule schemaswith underdetermined feature sets, e.g.
with(4a) S -9 NP \ [num:  sing\], VP \ [num = sing\](4b) S -~ NP \ [num= plural\], VP \ [num:  plural\]rather than(4c) S -9 NP \[num = NUM\], VP \ [num= NUM\]However, complete expansion of this sort willdefinitely require orders of magnitude more space- one simple item such as NP could easily require10 - 15 other features to be specified before it Wasfully instantiated.
The combinatorial potential oftrying to find all compatible sets of values forthese features for each item in a rule, and then allcompatible combinations of these sets, is conside-rable.
It is unlikel Z that the possible gains inspeed of parsing will be worth the cost of con-structing all these combinations a priori.To a large extent, then, the choice of how far toexpand the grammar when the rules are first read isforced.
We must expand the metarules as far as wecan; we would rather not expand underdeterminedfeature sets into collections of fully determinedones.
The remaining question is, should we leavethe rules which result from metarule applicationin ID/LP format, or should we expand them into setsof CF rules where the order in which items occur onthe rhs of the rule specifies the order they are toappear in the parse?
For top down analysis, it islikely that CF rules should be generated immediatelyfrom the ID/LP basis, since otherwise they willinevitably be generated every time the potentialexpansions of anode are required.
For bottom upanalysis the question is rather more open.
Itis, at the very least, worth keeping an index whichlinks item descriptions to rules for which the itemsare potential initial constituents; this indexshould clearly be pruned to ensure that nothing isentered as a potential initial constituent if theLP rules say that it cannot be.We can summarise our discussion of how to parseusing GPSG's as follows.
(i) Metarules should beexpanded out into sets of ID rules as soon as thegrammar is read in.
( i l )  It may also be worthexpanding ID rules into sets of rules where theorder of the rhs is significant.
(iii) It is nota good idea to expand ID rules into families ofCF rules with all legal combinations of feature:value pairs made explicit.
We also note that ifwe are simply going to treat the rules as ways ofdescribing constituent structure then some sort ofchart parser is likely to be the most appropriatemechanism for finding out how these rules describethe input text \[Shieber 84\].These are all reasonable decisions.
However, oncewe come to work with non-trlvial GPSG grammars, itappears that general purpose parsing algorithms,even efficient ones, do rather a lot of work.
Weneed some way of converting the declarativeknowledge embodied in the rules of the grammarinto procedural knowledge about how to analysetext.
The approach described in this paper involvesusing two parsing algorithm together.
We havea standard bottom-up chart parser, which simplytries out grammatical rules as best it can untilit arrives at some combination which fits the textit is working on; and a "direct recogniser", whichuses patterns of words which have previously beenanalysed by the chart parser to suggest analysesdirectly.There is not much to say about the chart parser.It uses the rules of the grammar in a form wherethe metarules have been applied, but the permu-tations implied by the LP rules have not beenexplicitly expanded.
This means that we havefewer rules to worry about, but silghtly morework to do each time we apply one (since we haveto check that we are applying it in a way allowedby the LP rules).
The extra work is minimised byusing the LP rules, at the time when the grammaris first read in, to index ID rules by theirpossible legal initial substructures.
Thisprevents the parser trying out completely point-less rules.It is hard to see many ways in which this parser,considered as a general purpose grammar applyingalgorithm, could be improved.
And yet it isnowhere near good enough.
With a grammar consistingof about 120 rule schemas (which expands to about300 schemas by the time the metarules have beenapplied), it takes several thousand rule appli-cations to analyse a sentence like "I want to seeyou doing it".
This is clearly unsatisfactory.To deal with this, we keep a record of text fragment~that we have previously managed to analyse.
Whenwe make an entry in this record, we abstract awayfrom the text the detai lsof  exactly which wordswere present.
What we want is a general descrip-tion of them in terms of their lexical categories,features such as transitivity, and endings (e.g.
"-ing" or "-ed").
These abstracted word stringsare akin to entries in Becker's "phrasal lexicon"\[Becker 75\].
Alongside each of them we keep anabstracted version of the structure that wasfound, i.e.
of the parse tree that was constructedto represent the way we did the analysis.
Againthe abstraction is produced by throwing away thedetails of the actual words that were present,replacing them this time by indicators sayingwhere in the original text they appeared.It is clearly very easy to compare such anabstracted text string with a piece of text, andto instantiate the associated structure if theyare found to match.
However, even if we throwaway the details of the particular words thatwere present in the original text, we are likelyto find that we have so many of these string:structure pairs that it will take us just as longto do all the required comparisons as it wouldhave done to use the basic chart parser with theoriginal set of rules.To prevent this happening, we condense our setof recognised strings by merging strings withcommon initial sequence, e.g.
if we have tworecognised fragments llke59(3) det, adj, adJ, noun ---3adJlist = \[2 3\], n = \[4\])(4) det, adJ, noun ........adJ l l s t  = \ [2\ ] ,  n = \[3\])NP(det = \[I\],NP(det = \[I\],we take advantage of their shared structure to storethem away like(5) det, adJ,adj, noun ---3 NP(det = \[I\],adJlist = \[2 3\], n = \[4\])noun .
.
.
.
.
.
.
9 NP(det = \[I\],adJlist = \[2\], n = \[3\])Merging our recognised fragments into a network llkethis means that if we have lexically unambiguoustext we can find the longest known fragment startingat any point in the text with very little effortindeed - we simply follow the path through thenetwork dlhtated by the categories (and otherfeatures, which have been left out of (3), (4) and(5) for simplicity) of the successive words in thetext.This "direct recognition" algorithm providesextremely rapid analyses of text which matchespreviously analysed input.
It is not, however,"complete" - it is a mechanism for rapid recognitionof previously encountered expansions of rules fromthe gr~m, ar, and it will not work if what we haveis something which is legal according to thegrammar but which the system has not previouslyencountered.
The chart parser Is complete in thissense.
If the input string has a legal analysisthen the chart parser will - eventually - produceit.For this reason we need to integrate the twomechanisms.
This is a surprisingly intricatetask, largely because the chart parser assumesthat all rules which include completed substructuresare initiated together, even if some of them arenot followed up immediately.
This assumptionbreaks down if we use our direct recogniser, sincecomplete structures will be entered into the chartwithout their components ever being explicitlyadded.
It is essential to be very careful inte-grating the two systems if we want to benefitfrom the speed of the direct recogniser withoutlosing the completeness of the chart parser.
Ourcurrent solution is to start by running the directrecognition algorithm across the text, repeatedlytaking the longest recognised substring, addingall its known analyses to the chart, and thencontinuing from the position immediately followingthis string.
If we do not recognise anything ata particular point, we simply make an entry in thechart for the current word and move on.
When wehave done this there will be a number of completeedges in the chart, put there by the directrecogniser, and a number of potential combinationsto follow up.
At this point we allow normal chartparsing to take place, hoping that the recognisedstructures will turn out to be constituents ofthe final analysis.
If they are not, we have togo back and successively add single word edgeswherever we jumped in with a guess about whatwas there.3.
Ambiguous And Unknown WordsThe combination of chart parser and directrecogniser is sufficiently effective that we canafford to use it on text that contains ambiguouswords without worrying about the extra work thesewill entail.
This is fortunate, given the numberof words in English which are ambiguous as tolexical category - "chart", "direct", "can", "use","work" and "entail" from the first sentence ofthis paragraph alone!Lexical ambiguity generally causes problems forbottom-up parsers because each interpretation ofa given word will tend to indicate the presence ofa different type of structure.
It will often turnout that when all the possibilities have beenexplored only one of the interpretations actuallycontributed to a complete, consistent parse, butit may take some time to check them all.
By lookingfor structures cued by strings of words we get astrong indication of which is the most promisinginterpretation - interpretations which are notgoing to be part of the final analysis are notlikely to appear inside substantial recognisedstrings.
To take a simple example, consider thetwo sentences "I don't see the use" and "I will useit".
In the first the interpretation of "use"as a noun fits easily into wider patterns of thesort we will have stored away, such as \[det, noun 39 NP or \[verb, det, noun\] @ VP, whereas itsinterpretation as a verb does not.
In the secondthe interpretation as a verb fits into plausiblepatterns like aux, verb 9 VSEQ or \[aux, verb,pronoun\] ~ VP, while the interpretatlon as asingular noun does not seem to fit well into anysurrounding patterns.These cues are ef fect ive enough for  us to be ableto fo l low \[Thorne et a l .
68\] in merging the "open"lex ica l  categories, i .e .
noun, verb, adj and adv.In the vast majority of cases, the final analysisof the text will tell us which of the various sub-classes of the category "open" a particularinstance of a given word must have belonged to.We do, of course, make heavy use of the connectionsbetween these categories and the suffix system- if a word has had "-ing" added to it, forinstance, then it must be functioning as a verbalform.
Not only does the final analysis usuallydetermine uniquely the interpretation for eachopen category word in the input, the combinedrecogniser and parser produce this final analysiswith comparatively little search.
We are thusable to deal with input that contains ambiguouswords just about as effectively as with inputthat doesn't.
The disambiguation is performedlargely by having the system recognise that it hasnever seen, say, an open category word functioningas a verb surrounded by the current local con-figuration of words, whereas it has seen somethingin this context which was eventually interpretedas a noun.
This has the added advantage ofenabling us to produce a syntactic analysis oftext containing previously unknown words - theyare immediately assigned to the open category,and their particular function in the currentcontext is discovered at the end of the analysis.How you construct a meaning representation from60such an analysis is another matter.5.
ConclusionsThe parser and rule learner described above performfar far better than the parser by itself - on complexcases, the parser may find the correct analysisseveral hundred times as quickly using learnt rulesas it would have done with Just the basic set.Experience with the system to date indicates thatthe introduction of new rules does not slow downthe process of selecting relevant rules all thatmuch, partly because the indexing of patternsagainst initial elements cuts out quite a lot ofpotentially pointless searching.
It is conceivablethat when the system has been run on large numbersof examples, the gains introduced by abstractingover long, unusual strings will be outweighed bythe extra effort involved in testing for them whenthey are not relevant.
If so, it may be a goodidea to put a limit on the length of string forwhich compound rules should be recorded.
Thereis no indication as yet that this will be necessary.It is of interest that the compound rules thesystem creates are akin to the productions used inMarcus' deterministic parser \[Marcus\] - patternsof descriptions of items which the parser isprepared to react to, combined with packets ofsimple actions to be taken when a pattern isrecognised.
There is no suggestion here that thesystem described above could ever be fullydeterministic - there are Just too many possi-bilities to be explored for this to be likely -but it certainly explores fewer dead ends withlearnt compound rules than with the initial basicones.AcknowledgmentsMy understanding of GPSG owes a great deal todiscussions with Roger Evans and Gerald Gazdar.The idea of using recognisable sequences ofcategories to find shortcuts in the analysis arosepartly out of conversations some time ago withAaron Sloman.
Gerald Gazdar and Steve Isardread and commented on this paper and an earlier,even more misguided one.
Steve Isard implementedthe basic chart parser which was adapted for thework reported here.
Any remaining errors, etc.are as usual the author's responsibility.ReferencesBecket, The Phrasal Lexicon.
TINLAP, 1975.Gazdar, G. Klein, E., Pullum, G.K., Sag, I.A.,Generalised Phrase Structure Grammar.Blackwell, Oxford (in press - 1985).Marcus, M., A Theory of Natural Language ProcessingPhD thesis, MIT, 1980.Shieber, S.M., Direct Parsing of ID/LP GrammarsLinguistics & Philosophy 7/2, 1984.Thorne, J.P., Bratley, P. & Dewar, H., TheSyntactic Analysis of English By Machine inMachine Intelligence 3, ed.
Michie, EdinburghUP, 1968.Thomson, H. Handling Metarules In A Parser ForGPSG DAIRP 175, University of Edinburgh, 1982.61
