Named Entity Recognition using an HMM-based Chunk TaggerGuoDong Zhou               Jian SuLaboratories for Information Technology21 Heng Mui Keng TerraceSingapore 119613zhougd@lit.org.sg          sujian@lit.org.sgAbstractThis paper proposes a Hidden MarkovModel (HMM) and an HMM-based chunktagger, from which a named entity (NE)recognition (NER) system is built torecognize and classify names, times andnumerical quantities.
Through the HMM,our system is able to apply and integratefour types of internal and externalevidences: 1) simple deterministic internalfeature of the words, such as capitalizationand digitalization; 2) internal semanticfeature of important triggers; 3) internalgazetteer feature; 4) external macro contextfeature.
In this way, the NER problem canbe resolved effectively.
Evaluation of oursystem on MUC-6 and MUC-7 English NEtasks achieves F-measures of 96.6% and94.1% respectively.
It shows that theperformance is significantly better thanreported by any other machine-learningsystem.
Moreover, the performance is evenconsistently better than those based onhandcrafted rules.1 IntroductionNamed Entity (NE) Recognition (NER) is toclassify every word in a document into somepredefined categories and "none-of-the-above".
Inthe taxonomy of computational linguistics tasks, itfalls under the domain of "information extraction",which extracts specific kinds of information fromdocuments as opposed to the more general task of"document management" which seeks to extract allof the information found in a document.Since entity names form the main content of adocument, NER is a very important step towardmore intelligent information extraction andmanagement.
The atomic elements of informationextraction -- indeed, of language as a whole -- couldbe considered as the "who", "where" and "howmuch" in a sentence.
NER performs what is knownas surface parsing, delimiting sequences of tokensthat answer these important questions.
NER canalso be used as the first step in a chain of processors:a next level of processing could relate two or moreNEs, or perhaps even give semantics to thatrelationship using a verb.
In this way, furtherprocessing could discover the "what" and "how" ofa sentence or body of text.While NER is relatively simple and it is fairlyeasy to build a system with reasonable performance,there are still a large number of ambiguous casesthat make it difficult to attain human performance.There has been a considerable amount of work onNER problem, which aims to address many of theseambiguity, robustness and portability issues.
Duringlast decade, NER has drawn more and moreattention from the NE tasks [Chinchor95a][Chinchor98a] in MUCs [MUC6] [MUC7], whereperson names, location names, organization names,dates, times, percentages and money amounts are tobe delimited in text using SGML mark-ups.Previous approaches have typically usedmanually constructed finite state patterns, whichattempt to match against a sequence of words inmuch the same way as a general regular expressionmatcher.
Typical systems are Univ.
of Sheffield'sLaSIE-II [Humphreys+98], ISOQuest's NetOwl[Aone+98] [Krupha+98] and Univ.
of Edinburgh'sLTG [Mikheev+98] [Mikheev+99] for EnglishNER.
These systems are mainly rule-based.However, rule-based approaches lack the ability ofcoping with the problems of robustness andportability.
Each new source of text requiressignificant tweaking of rules to maintain optimalperformance and the maintenance costs could bequite steep.The current trend in NER is to use themachine-learning approach, which is moreComputational Linguistics (ACL), Philadelphia, July 2002, pp.
473-480.Proceedings of the 40th Annual Meeting of the Association forattractive in that it is trainable and adaptable and themaintenance of a machine-learning system is muchcheaper than that of a rule-based one.
Therepresentative machine-learning approaches used inNER are HMM (BBN's IdentiFinder in [Miller+98][Bikel+99] and KRDL's system [Yu+98] forChinese NER.
), Maximum Entropy (New YorkUniv.
's MEME in [Borthwick+98] [Borthwich99])and Decision Tree (New York Univ.
's system in[Sekine98] and SRA's system in [Bennett+97]).Besides, a variant of Eric Brill'stransformation-based rules [Brill95] has beenapplied to the problem [Aberdeen+95].
Amongthese approaches, the evaluation performance ofHMM is higher than those of others.
The mainreason may be due to its better ability of capturingthe locality of phenomena, which indicates namesin text.
Moreover, HMM seems more and moreused in NE recognition because of the efficiency ofthe Viterbi algorithm [Viterbi67] used in decodingthe NE-class state sequence.
However, theperformance of a machine-learning system isalways poorer than that of a rule-based one by about2% [Chinchor95b] [Chinchor98b].
This may bebecause current machine-learning approachescapture important evidence behind NER problemmuch less effectively than human experts whohandcraft the rules, although machine-learningapproaches always provide important statisticalinformation that is not available to human experts.As defined in [McDonald96], there are two kindsof evidences that can be used in NER to solve theambiguity, robustness and portability problemsdescribed above.
The first is the internal evidencefound within the word and/or word string itselfwhile the second is the external evidence gatheredfrom its context.
In order to effectively apply andintegrate internal and external evidences, wepresent a NER system using a HMM.
The approachbehind our NER system is based on theHMM-based chunk tagger in text chunking, whichwas ranked the best individual system [Zhou+00a][Zhou+00b] in CoNLL'2000 [Tjong+00].
Here, aNE is regarded as a chunk, named "NE-Chunk".
Todate, our system has been successfully trained andapplied in English NER.
To our knowledge, oursystem outperforms any publishedmachine-learning systems.
Moreover, our systemeven outperforms any published rule-basedsystems.The layout of this paper is as follows.
Section 2gives a description of the HMM and its applicationin NER: HMM-based chunk tagger.
Section 3explains the word feature used to capture both theinternal and external evidences.
Section 4 describesthe back-off schemes used to tackle the sparsenessproblem.
Section 5 gives the experimental results ofour system.
Section 6 contains our remarks andpossible extensions of the proposed work.2 HMM-based Chunk Tagger2.1  HMM ModelingGiven a token sequence nn gggG L211 = , the goalof NER is to find a stochastic optimal tag sequencenn tttT L211 =  that maximizes                     (2-1))()(),(log)(log)|(log1111111 nnnnnnnGPTPGTPTPGTP?+=The second item in (2-1) is the mutualinformation between nT1  andnG1 .
In order tosimplify the computation of this item, we assumemutual information independence:?==nininn GtMIGTMI1111 ),(),(   or              (2-2)?= ?=?ninininnnnGPtPGtPGPTPGTP1 111111)()(),(log)()(),(log    (2-3)Applying it to equation (2.1), we have:?
?==+?=nininiinnnGtPtPTPGTP111111)|(log)(log)(log)|(log(2-4)The basic premise of this model is to considerthe raw text, encountered when decoding, as thoughit had passed through a noisy channel, where it hadbeen originally marked with NE tags.
The job of ourgenerative model is to directly generate the originalNE tags from the output words of the noisy channel.It is obvious that our generative model is reverse tothe generative model of traditional HMM1, as used1 In traditional HMM to maximise )|(log 11 nn GTP , first weapply Bayes' rule:)(),()|(11111 nnnnnGPGTPGTP =and have:in BBN's IdentiFinder, which models the originalprocess that generates the NE-class annotatedwords from the original NE tags.
Anotherdifference is that our model assumes mutualinformation independence (2-2) while traditionalHMM assumes conditional probabilityindependence (I-1).
Assumption (2-2) is muchlooser than assumption (I-1) because assumption(I-1) has the same effect with the sum ofassumptions (2-2) and (I-3)2.
In this way, our modelcan apply more context information to determinethe tag of current token.From equation (2-4), we can see that:1) The first item can be computed by applyingchain rules.
In ngram modeling, each tag isassumed to be probabilistically dependent on theN-1 previous tags.2) The second item is the summation of logprobabilities of all the individual tags.3) The third item corresponds to the "lexical"component of the tagger.We will not discuss both the first and seconditems further in this paper.
This paper will focus onthe third item?=nini GtP11 )|(log , which is the maindifference between our tagger and other traditionalHMM-based taggers, as used in BBN's IdentiFinder.Ideally, it can be estimated by using theforward-backward algorithm [Rabiner89]recursively for the 1st-order [Rabiner89] or 2nd-order HMMs [Watson+92].
However, analternative back-off modeling approach is appliedinstead in this paper (more details in section 4).2.2 HMM-based Chunk Tagger))(log)|((logmaxarg)|(logmaxarg11111nnnTnnTTPTGPGTP+=Then we assume conditional probabilityindependence: ?==niiinn tgPTGP111 )|()|(                 (I-1)and have:))(log)|(log(maxarg)|(logmaxarg1111nniiiTnnTTPtgPGTP+= ?=(I-2)2 We can obtain equation (I-2) from (2.4) by assuming)|(log)|(log1 iini tgPGtP =                                    (I-3)For NE-chunk tagging, we havetoken >=< iii wfg , , where nn wwwW L211 =  is theword sequence and nn fffF L211 =  is theword-feature sequence.
In the meantime, NE-chunktag it  is structural and consists of three parts:1) Boundary Category: BC = {0, 1, 2, 3}.
Here 0means that current word is a whole entity and1/2/3 means that current word is at thebeginning/in the middle/at the end of an entity.2) Entity Category: EC.
This is used to denote theclass of the entity name.3) Word Feature: WF.
Because of the limitednumber of boundary and entity categories, theword feature is added into the structural tag torepresent more accurate models.Obviously, there exist some constraints between1?it  and it  on the boundary and entity categories, asshown in Table 1, where "valid" / "invalid" meansthe tag sequence ii tt 1?
is valid / invalid while "validon" means ii tt 1?
is valid with an additionalcondition ii ECEC =?1 .
Such constraints have beenused in Viterbi decoding algorithm to ensure validNE chunking.0 1 2 30 Valid Valid Invalid Invalid1 Invalid Invalid Valid on Valid on2 Invalid Invalid Valid Valid3 Valid Valid Invalid InvalidTable 1: Constraints between 1?it  and it  (Column:1?iBC  in 1?it ; Row: iBC  in it )3 Determining Word FeatureAs stated above, token is denoted as ordered pairs ofword-feature and word itself: >=< iii wfg , .Here, the word-feature is a simple deterministiccomputation performed on the word and/or wordstring with appropriate consideration of context aslooked up in the lexicon or added to the context.In our model, each word-feature consists ofseveral sub-features, which can be classified intointernal sub-features and external sub-features.
Theinternal sub-features are found within the wordand/or word string itself to capture internalevidence while external sub-features are derivedwithin the context to capture external evidence.3.1 Internal Sub-FeaturesOur model captures three types of internalsub-features: 1) 1f : simple deterministic internalfeature of the words, such as capitalization anddigitalization; 2) 2f : internal semantic feature ofimportant triggers; 3) 3f : internal gazetteer feature.1) 1f  is the basic sub-feature exploited in thismodel, as shown in Table 2 with the descendingorder of priority.
For example, in the case ofnon-disjoint feature classes such asContainsDigitAndAlpha andContainsDigitAndDash, the former will takeprecedence.
The first eleven features arise fromthe need to distinguish and annotate monetaryamounts, percentages, times and dates.
The restof the features distinguish types of capitalizationand all other words such as punctuation marks.In particular, the FirstWord feature arises fromthe fact that if a word is capitalized and is thefirst word of the sentence, we have no goodinformation as to why it is capitalized (but notethat AllCaps and CapPeriod are computed beforeFirstWord, and take precedence.)
Thissub-feature is language dependent.
Fortunately,the feature computation is an extremely smallpart of the implementation.
This kind of internalsub-feature has been widely used inmachine-learning systems, such as BBN'sIdendiFinder and New York Univ.
's MENE.
Therationale behind this sub-feature is clear: a)capitalization gives good evidence of NEs inRoman languages; b) Numeric symbols canautomatically be grouped into categories.2) 2f  is the semantic classification of importanttriggers, as seen in Table 3, and is unique to oursystem.
It is based on the intuitions thatimportant triggers are useful for NER and can beclassified according to their semantics.
Thissub-feature applies to both single word andmultiple words.
This set of triggers is collectedsemi-automatically from the NEs and their localcontext of the training data.3) Sub-feature 3f , as shown in Table 4, is theinternal gazetteer feature, gathered from thelook-up gazetteers: lists of names of persons,organizations, locations and other kinds ofnamed entities.
This sub-feature can bedetermined by finding a match in thegazetteer of the corresponding NE typewhere n (in Table 4) represents the wordnumber in the matched word string.
In steadof collecting gazetteer lists from trainingdata, we collect a list of 20 public holidays inseveral countries, a list of 5,000 locationsfrom websites such as GeoHive3, a list of10,000 organization names from websitessuch as Yahoo4 and a list of 10,000 famouspeople from websites such as ScopeSystems5.
Gazetters have been widely usedin NER systems to improve performance.3.2 External Sub-FeaturesFor external evidence, only one external macrocontext feature 4f , as shown in Table 5, is capturedin our model.
4f  is about whether and how theencountered NE candidate is occurred in the list ofNEs already recognized from the document, asshown in Table 5 (n is the word number in thematched NE from the recognized NE list and m isthe matched word number between the word stringand the matched NE with the corresponding NEtype.).
This sub-feature is unique to our system.
Theintuition behind this is the phenomena of namealias.During decoding, the NEs already recognizedfrom the document are stored in a list.
When thesystem encounters a NE candidate, a name aliasalgorithm is invoked to dynamically determine itsrelationship with the NEs in the recognized list.Initially, we also consider part-of-speech (POS)sub-feature.
However, the experimental result isdisappointing that incorporation of POS evendecreases the performance by 2%.
This may bebecause capitalization information of a word issubmerged in the muddy of several POS tags andthe performance of POS tagging is not satisfactory,especially for unknown capitalized words (sincemany of NEs include unknown capitalized words.
).Therefore, POS is discarded.3 http://www.geohive.com/4 http://www.yahoo.com/5 http://www.scopesys.com/Sub-Feature 1f  Example  Explanation/IntuitionOneDigitNum 9 Digital NumberTwoDigitNum 90 Two-Digit yearFourDigitNum 1990 Four-Digit yearYearDecade 1990s Year DecadeContainsDigitAndAlpha A8956-67 Product CodeContainsDigitAndDash 09-99 DateContainsDigitAndOneSlash 3/4 Fraction or DateContainsDigitAndTwoSlashs 19/9/1999 DATEContainsDigitAndComma 19,000 MoneyContainsDigitAndPeriod 1.00 Money, PercentageOtherContainsDigit 123124 Other NumberAllCaps IBM OrganizationCapPeriod M. Person Name InitialCapOtherPeriod St. AbbreviationCapPeriods N.Y. AbbreviationFirstWord First word of sentence No useful capitalization informationInitialCap Microsoft Capitalized WordLowerCase Will Un-capitalized WordOther $ All other wordsTable 2: Sub-Feature 1f : the Simple Deterministic Internal Feature of the WordsNE Type (No of Triggers) Sub-Feature 2f  Example Explanation/IntuitionPERCENT (5) SuffixPERCENT % Percentage SuffixPrefixMONEY $ Money Prefix MONEY (298)SuffixMONEY Dollars Money SuffixSuffixDATE Day Date SuffixWeekDATE Monday Week DateMonthDATE July Month DateSeasonDATE Summer Season DatePeriodDATE1 Month Period DatePeriodDATE2 Quarter Quarter/Half of YearEndDATE Weekend Date EndDATE (52)ModifierDATE Fiscal Modifier of DateSuffixTIME a.m. Time Suffix TIME (15)PeriodTime Morning Time PeriodPrefixPERSON1 Mr.
Person TitlePrefixPERSON2 President Person DesignationPERSON (179)FirstNamePERSON Micheal Person First NameLOC (36) SuffixLOC River Location SuffixORG (177) SuffixORG Ltd Organization SuffixOthers (148) Cardinal, Ordinal, etc.
Six,, Sixth Cardinal and Ordinal NumbersTable 3: Sub-Feature 2f : the Semantic Classification of Important TriggersNE Type (Size of Gazetteer) Sub-Feature 3f  ExampleDATE (20) DATEnGn Christmas Day: DATE2G2PERSON (10,000) PERSONnGn Bill Gates: PERSON2G2LOC (5,000) LOCnGn Beijing: LOC1G1ORG (10,000) ORGnGn United Nation: ORG2G2Table 4: Sub-Feature 3f : the Internal Gazetteer Feature (G means Global gazetteer)NE Type Sub-Feature ExamplePERSON PERSONnLm Gates: PERSON2L1 ("Bill Gates" already recognized as a person name)LOC LOCnLm N.J.: LOC2L2 ("New Jersey" already recognized as a location name)ORG ORGnLm UN: ORG2L2 ("United Nation" already recognized as a org name)Table 5: Sub-feature 4f : the External Macro Context Feature (L means Local document)4  Back-off ModelingGiven the model in section 2 and word feature insection 3, the main problem is how tocompute ?=nini GtP11 )/( .
Ideally, we would havesufficient training data for every event whoseconditional probability we wish to calculate.Unfortunately, there is rarely enough training datato compute accurate probabilities when decoding onnew data, especially considering the complex wordfeature described above.
In order to resolve thesparseness problem, two levels of back-offmodeling are applied to approximate )/( 1ni GtP :1) First level back-off scheme is based on differentcontexts of word features and words themselves,and nG1  in )/( 1ni GtP  is approximated in thedescending order of iiii wfff 12 ??
, 21 ++ iiii ffwf ,iii wff 1?
, 1+iii fwf , iii fwf 11 ??
, 11 ++ iii wff ,iii fff 12 ??
, 21 ++ iii fff , ii wf , iii fff 12 ??
, 1+ii ffand if .2) The second level back-off scheme is based ondifferent combinations of the four sub-featuresdescribed in section 3, and kf  is approximatedin the descending order of 4321 kkkk ffff ,31kk ff ,41kk ff ,21kk ff  and1kf .5 Experimental ResultsIn this section, we will report the experimentalresults of our system for English NER on MUC-6and MUC-7 NE shared tasks, as shown in Table 6,and then for the impact of training data size onperformance using MUC-7 training data.
For eachexperiment, we have the MUC dry-run data as theheld-out development data and the MUC formal testdata as the held-out test data.For both MUC-6 and MUC-7 NE tasks, Table 7shows the performance of our system using MUCevaluation while Figure 1 gives the comparisons ofour system with others.
Here, the precision (P)measures the number of correct NEs in the answerfile over the total number of NEs in the answer fileand the recall (R) measures the number of correctNEs in the answer file over the total number of NEsin the key file while F-measure is the weightedharmonic mean of precision and recall:PRRPF++= 22 )1(?
?with 2?
=1.
It shows that theperformance is significantly better than reported byany other machine-learning system.
Moreover, theperformance is consistently better than those basedon handcrafted rules.Statistics(KB)TrainingDataDry RunDataFormal TestDataMUC-6 1330 121 124MUC-7 708 156 561Table 6: Statistics of Data from MUC-6and MUC-7 NE TasksF P RMUC-6 96.6 96.3 96.9MUC-7 94.1 93.7 94.5Table 7: Performance of our System on MUC-6and MUC-7 NE TasksComposition F P R1ff =  77.6 81.0 74.121 fff =  87.4 88.6 86.1321 ffff =  89.3 90.5 88.2421 ffff =  92.9 92.6 93.14321 fffff =  94.1 93.7 94.5Table 8: Impact of Different Sub-FeaturesWith any learning technique, one importantquestion is how much training data is required toachieve acceptable performance.
More generallyhow does the performance vary as the training datasize changes?
The result is shown in Figure 2 forMUC-7 NE task.
It shows that 200KB of trainingdata would have given the performance of 90%while reducing to 100KB would have had asignificant decrease in the performance.
It alsoshows that our system still has some room forperformance improvement.
This may be because ofthe complex word feature and the corresponding sparseness problem existing in our system.Figure 1: Comparison of our system with otherson MUC-6 and MUC-7 NE tasks8085909510080 85 90 95 100RecallPrecision Our MUC-6 SystemOur MUC-7 SystemOther MUC-6 SystemsOther MUC-7 SyetemsFigure 2: Impact of Various Training Data on Performance80859095100100 200 300 400 500 600 700 800Training Data Size(KB)F-measureMUC-7Another important question is about the effect ofdifferent sub-features.
Table 8 answers the questionon MUC-7 NE task:1) Applying only 1f  gives our system theperformance of 77.6%.2) 2f  is very useful for NER and increases theperformance further by 10% to 87.4%.3) 4f  is impressive too with another 5.5%performance improvement.4)  However, 3f  contributes only further 1.2% tothe performance.
This may be becauseinformation included in 3f  has already beencaptured by 2f  and 4f .
Actually, theexperiments show that the contribution of 3fcomes from where there is no explicit indicatorinformation in/around the NE and there is noreference to other NEs in the macro context ofthe document.
The NEs contributed by 3f  arealways well-known ones, e.g.
Microsoft, IBMand Bach (a composer), which are introduced intexts without much helpful context.6  ConclusionThis paper proposes a HMM in that a newgenerative model, based on the mutual informationindependence assumption (2-3) instead of theconditional probability independence assumption(I-1) after Bayes' rule, is applied.
Moreover, itshows that the HMM-based chunk tagger caneffectively apply and integrate four different kindsof sub-features, ranging from internal wordinformation to semantic information to NEgazetteers to macro context of the document, tocapture internal and external evidences for NERproblem.
It also shows that our NER system canreach "near human performance".
To ourknowledge, our NER system outperforms anypublished machine-learning system and anypublished rule-based system.While the experimental results have beenimpressive, there is still much that can be donepotentially to improve the performance.
In the nearfeature, we would like to incorporate the followinginto our system:?
List of domain and application dependent person,organization and location names.?
More effective name alias algorithm.?
More effective strategy to the back-off modelingand smoothing.References[Aberdeen+95] J. Aberdeen, D. Day, L.Hirschman, P. Robinson and M. Vilain.
MITRE:Description of the Alembic System Used forMUC-6.
MUC-6.
Pages141-155.
Columbia,Maryland.
1995.
[Aone+98] C. Aone, L. Halverson, T. Hampton,M.
Ramos-Santacruz.
SRA: Description of the IE2System Used for MUC-7.
MUC-7.
Fairfax, Virginia.1998.
[Bennett+96] S.W.
Bennett, C. Aone and C.Lovell.
Learning to Tag Multilingual TextsThrough Observation.
EMNLP'1996.
Pages109-116.Providence, Rhode Island.
1996.
[Bikel+99] Daniel M. Bikel, Richard Schwartzand Ralph M. Weischedel.
An Algorithm thatLearns What's in a Name.
Machine Learning(Special Issue on NLP).
1999.
[Borthwick+98] A. Borthwick, J.
Sterling, E.Agichtein, R. Grishman.
NYU: Description of theMENE Named Entity System as Used in MUC-7.MUC-7.
Fairfax, Virginia.
1998.
[Borthwick99] Andrew Borthwick.
A MaximumEntropy Approach to Named Entity Recognition.Ph.D.
Thesis.
New York University.
September,1999.
[Brill95] Eric Brill.
Transform-basedError-Driven Learning and Natural LanguageProcessing: A Case Study in Part-of-speechTagging.
Computational Linguistics 21(4).Pages543-565.
1995.
[Chinchor95a] Nancy Chinchor.
MUC-6 NamedEntity Task Definition (Version 2.1).
MUC-6.Columbia, Maryland.
1995.
[Chinchor95b] Nancy Chinchor.
StatisticalSignificance of MUC-6 Results.
MUC-6.
Columbia,Maryland.
1995.
[Chinchor98a] Nancy Chinchor.
MUC-7 NamedEntity Task Definition (Version 3.5).
MUC-7.Fairfax, Virginia.
1998.
[Chinchor98b] Nancy Chinchor.
StatisticalSignificance of MUC-7 Results.
MUC-7.
Fairfax,Virginia.
1998.
[Humphreys+98] K. Humphreys, R. Gaizauskas,S.
Azzam, C. Huyck, B. Mitchell, H. Cunningham,Y.
Wilks.
Univ.
of Sheffield: Description of theLaSIE-II System as Used for MUC-7.
MUC-7.Fairfax, Virginia.
1998.
[Krupka+98]  G. R. Krupka, K. Hausman.IsoQuest Inc.: Description of the NetOwlTMExtractor System as Used for MUC-7.
MUC-7.Fairfax, Virginia.
1998.
[McDonald96] D. McDonald.
Internal andExternal Evidence in the Identification andSemantic Categorization of Proper Names.
In B.Boguraev and J. Pustejovsky editors: CorpusProcessing for Lexical Acquisition.
Pages21-39.MIT Press.
Cambridge, MA.
1996.
[Miller+98] S. Miller, M. Crystal, H. Fox, L.Ramshaw, R. Schwartz, R. Stone, R. Weischedel,and the Annotation Group.
BBN: Description of theSIFT System as Used for MUC-7.
MUC-7.
Fairfax,Virginia.
1998.
[Mikheev+98] A. Mikheev, C. Grover, M.Moens.
Description of the LTG System Used forMUC-7.
MUC-7.
Fairfax, Virginia.
1998.
[Mikheev+99] A. Mikheev, M. Moens, and C.Grover.
Named entity recognition without gazeteers.EACL'1999.
Pages1-8.
Bergen, Norway.
1999.
[MUC6] Morgan Kaufmann Publishers, Inc.Proceedings of the Sixth Message UnderstandingConference (MUC-6).
Columbia, Maryland.
1995.
[MUC7] Morgan Kaufmann Publishers, Inc.Proceedings of the Seventh Message UnderstandingConference (MUC-7).
Fairfax, Virginia.
1998.
[Rabiner89] L. Rabiner.
A Tutorial on HiddenMarkov Models and Selected Applications inSpeech Recognition?.
IEEE 77(2).
Pages257-285.1989.
[Sekine98] Satoshi Sekine.
Description of theJapanese NE System Used for MET-2.
MUC-7.Fairfax, Virginia.
1998.
[Tjong+00] Erik F. Tjong Kim Sang and SabineBuchholz.
Introduction to the CoNLL-2000 SharedTask: Chunking.
CoNLL'2000.
Pages127-132.Lisbon, Portugal.
11-14 Sept 2000.
[Viterbi67] A. J. Viterbi.
Error Bounds forConvolutional Codes and an AsymptoticallyOptimum Decoding Algorithm.
IEEE Transactionson Information Theory.
IT(13).
Pages260-269,April 1967.
[Watson+92] B. Watson and Tsoi A Chunk.Second Order Hidden Markov Models for SpeechRecognition?.
Proceeding of 4th AustralianInternational Conference on Speech Science andTechnology.
Pages146-151.
1992.
[Yu+98] Yu Shihong, Bai Shuanhu and WuPaul.
Description of the Kent Ridge Digital LabsSystem Used for MUC-7.
MUC-7.
Fairfax, Virginia.1998.
[Zhou+00] Zhou GuoDong, Su Jian and TeyTongGuan.
Hybrid Text Chunking.
CoNLL'2000.Pages163-166.
Lisbon, Portugal, 11-14 Sept 2000.
[Zhou+00b] Zhou GuoDong and Su Jian,Error-driven HMM-based Chunk Tagger withContext-dependent Lexicon.
EMNLP/ VLC'2000.Hong Kong, 7-8 Oct 2000.
