Proceedings of the ACL Workshop on Computational Approaches to Semitic Languages, pages 47?54,Ann Arbor, June 2005. c?2005 Association for Computational LinguisticsPart of Speech tagging for Amharic using Conditional Random FieldsSisay Fissaha AdafreInformatics Institute, University of AmsterdamKruislaan 403, 1098 SJ Amsterdam, The Netherlandssfissaha@science.uva.nlAbstractWe applied Conditional Random Fields(CRFs) to the tasks of Amharic word seg-mentation and POS tagging using a smallannotated corpus of 1000 words.
Giventhe size of the data and the large number ofunknown words in the test corpus (80%),an accuracy of 84% for Amharic wordsegmentation and 74% for POS taggingis encouraging, indicating the applicabil-ity of CRFs for a morphologically com-plex language like Amharic.1 IntroductionPart-of-speech (POS) tagging is often consideredas the first phase of a more complex natural lan-guage processing application.
The task is partic-ularly amenable to automatic processing.
Specifi-cally, POS taggers that are trained on pre-annotatedcorpora achieve human-like performance, which isadequate for most applications.
The road to suchhigh performance levels is, however, filled with ahierarchy of sub-problems.
Most techniques gener-ally assume the availability of large POS annotatedcorpora.
The development of annotated corpora inturn requires a standard POS tagset.
None of theseresources are available for Amharic.
This is duemainly to the fact that data preparation, i.e., devel-oping a comprehensive POS tagset and annotating areasonably sized text, is an arduous task.
Althoughthe POS tagging task, taken as a whole, seems chal-lenging, a lot can be gained by analyzing it into sub-problems and dealing with each one step-by-step,and also bringing in the experience from other lan-guages in solving these problems, since POS taggershave been developed for several languages resultingin a rich body of knowledge.Several attempts have been made in the pastto develop algorithms for analyzing Amharicwords.
Among these is the stemming algorithmof Nega (1999), which reduces Amharic wordsinto their common stem forms by removing affixes.Nega?s work focuses on investigating the effective-ness of the stemming algorithm in information re-trieval for Amharic.
Abyot (2000) developed a wordparser for Amharic verbs that analyses verbs intotheir constituting morphemes and determines theirmorphosyntactic categories.
Abyot?s work only cov-ers verbs and their derivations.
Mesfin (2001) devel-oped a Hidden Markov Model (HMM) based part ofspeech tagger for Amharic.
Building on the work ofMesfin, Atelach (2002) developed a stochastic syn-tactic parser for Amharic.
Sisay and Haller (2003a;2003b) applied finite-state tools, and corpus-basedmethods for the Amharic morphological analysis.This work provided important insights into the is-sues surrounding the development of Amharic nat-ural language processing applications, especially, incompiling a preliminary POS tagset for Amharic.In this paper, our aim is to explore recent develop-ments in the morphological analysis of related lan-guages, such as Arabic and Hebrew, and machinelearning approaches, and apply them to the Amhariclanguage.
Amharic belongs to the Semitic family oflanguages, and hence shares a number of commonmorphological properties with Arabic and Hebrewfor which active research is being carried out.
Stud-47ies on these languages propose two alternative POStagging approaches which differ on the unit of anal-ysis chosen; morpheme-based and word-based (Bar-Haim et al, 2004).
The former presupposes a seg-mentation phase in which words are analysed intoconstituting morphemes which are then passed tothe POS tagging step, whereas the latter applies POStagging directly on fully-inflected word forms.
Dueto scarce resources, it is impossible for us to fullycarry out these tasks for Amharic.
Therefore, thesegmentation and POS tagging tasks are carried outindependently.
Furthermore, POS tagging is appliedonly on fully-inflected word forms.
The motivationfor doing the segmentation task comes from the needto provide some measure of the complexity of thetask in the context of the Amharic language.
Asregards implementation, new models have been in-troduced recently for segmentation and sequence-labeling tasks.
One such model is Conditional Ran-dom Fields (CRFs) (Lafferty et al, 2001).
In thispaper, we describe important morphosyntactic char-acteristics of Amharic, and apply CRFs to Amharicword segmentation and POS tagging.The paper is organized as follows.
Section 2 pro-vides a brief description of Amharic morphology.Section 3 presents some of the work done in thearea of Amharic morphological analysis, and exam-ines one POS tagset proposed by previous studies.This tagset has been revised and applied on a sampleAmharic newspaper text, which is discussed in Sec-tion 4.
Section 5 describes the tasks in greater de-tail.
Section 6 provides a brief description of CRFs,the machine learning algorithm that will be appliedin this paper.
Section 7 describes the experimentalsetup and Section 8 presents the result of the exper-iment.
Finally, Section 9 makes some concludingremarks.2 Amharic MorphologyAmharic is one of the most widely spoken lan-guages in Ethiopia.
It has its own script that is bor-rowed from Ge?ez, another Ethiopian Semitic lan-guage (Leslau, 1995).
The script is believed to haveoriginated from the South Sabean script.
It is a syl-labary writing system where each character repre-sents an open CV syllable, i.e., a combination of aconsonant followed by a vowel (Daniels, 1997).Amharic has a complex morphology.
Wordformation involves prefixation, suffixation, infixa-tion, reduplication, and Semitic stem interdigitation,among others.
Like other Semitic languages, e.g.,Arabic, Amharic verbs and their derivations con-stitute a significant part of the lexicon.
In Semiticlanguages, words, especially verbs, are best viewedas consisting of discontinuous morphemes that arecombined in a non-concatenative manner.
Put dif-ferently, verbs are commonly analyzed as consist-ing of root consonants, template patterns, and vowelpatterns.
With the exception of very few verb forms(such as the imperative), all derived verb forms takeaffixes in order to appear as independent words.Most function words in Amharic, such as Con-junction, Preposition, Article, Relative marker,Pronominal affixes, Negation markers, are boundmorphemes, which are attached to content words,resulting in complex Amharic words composed ofseveral morphemes.
Nouns inflect for the mor-phosyntactic features number, gender, definiteness,and case.
Amharic adjectives share some morpho-logical properties with nouns, such as definiteness,case, and number.
As compared to nouns and verbs,there are fewer primary adjectives.
Most adjec-tives are derived from nouns or verbs.
Amharichas very few lexical adverbs.
Adverbial meaningis usually expressed morphologically on the verb orthrough prepositional phrases.
While prepositionsare mostly bound morphemes, postpositions are typ-ically independent words.The segmentation task (cf.
Section 7.1) consid-ers the following bound morphemes as segments:Prepositions, Conjunctions, Relative Makers, Aux-iliary verbs, Negation Marker and Coordinate Con-junction.
Other bound morphemes such as definitearticle, agreement features (i.e., number, gender),case markers, etc are not considered as segments andwill be treated as part of the word.
These are chosensince they are commonly treated as separate units inmost syntactic descriptions.Although the above description of Amharic is farfrom complete, it highlights some of the major char-acteristics of Amharic, which it shares with otherSemitic languages such as Arabic.
It is, therefore,worthwhile to take into consideration the work donefor other Semitic languages in proposing a methodfor Amharic natural language processing.483 Amharic POS TagsetMesfin (2001) compiled a total of 25 POS tags: N,NV, NB, NP, NC, V, AUX, VCO, VP, VC, J, JC,JNU, JPN, JP, PREP, ADV, ADVC, C, REL, ITJ,ORD, CRD, PUNC, and UNC.
These tags captureimportant properties of the language at a higher levelof description.
For example, the fact that thereis no category for Articles indicates that Amharicdoes not have independent lexical forms for arti-cles.
However, a close examination of the de-scription of some of the tags reveals some miss-classification that we think will lead to tagging in-consistency.
For example, the tag JPN is assignedto nouns with the ?ye?
prefix morpheme that func-tion as an adjective, e.g.
yetaywan sahn - ATaiwan made plate (Mesfin, 2001).
This ex-ample shows that grammatical function takes prece-dence over morphological form in deciding the POScategory of a word.
In Amharic, the ye+NOUN con-struction can also be used to represent other kindsof relation such as Possession relation.
On theother hand, the ye+NOUN construction is a simplemorphological variant of the NOUN that can easilybe recognized.
Therefore, treating ye+NOUN con-struction as a subclass of a major noun class will re-sult in a better tagging consistency than treating it asan adjective.
Furthermore, a hierarchical tagset, or-ganized into major classes and subclasses, seems tobe a preferred design strategy (Wilson, 1996; Khojaet al, 2001).
Although it is possible to guess (fromthe tagset description) some abstract classes such as,N* (nouns), V* (verbs), J* (adjectives), etc., such ahierarchical relation is not clearly indicated.
One ad-vantage of such a hierarchical organization is that itallows one to work at different levels of abstraction.The POS tags that are used in this paper are ob-tained by collapsing some of the categories proposedby Mesfin (2001).
The POS tags are Noun (N), Verb(V), Auxiliary verbs (AUX), Numerals (NU), Ad-jective (AJ), Adverb (AV), Adposition (AP), Inter-jection (I), Residual (R), and Punctuation (PU).
Themain reason for working with a set of abstract POStags is resource limitation, i.e., the absence of a largeannotated corpus.
Since we are working on a smallannotated corpus, 25 POS tags make the data sparseand the results unreliable.
Therefore, we have foundit necessary to revise the tagset.4 Application of the Revised TagsetThe above abstract POS tags are chosen by tak-ing into account the proposals made in Amharicgrammar literature and the guidelines of other lan-guages (Baye, 1986; Wilson, 1996; Khoja et al,2001).
It is, however, necessary to apply the revisedtagset to a real Amharic text and see if it leads to anyunforeseeable problems.
It is also useful to see thedistribution of POS tags in a typical Amahric news-paper text.
Therefore, we selected 5 Amharic newsarticles and applied the above tagset.All the tokens in the corpus are assigned oneof the tags in the proposed tagset relatively easily.There do not seem to be any gaps in the tagset.Unlike Mesfin (2001), who assigns collocations asingle POS tag, we have assumed that each tokenshould be treated separately.
This means that wordsthat are part of a collocation are assigned tags indi-vidually.
This in turn contributes towards a bettertagging consistency by minimizing context depen-dent decision-making steps.Table 1 shows the distribution of POS tags in thecorpus.
Nouns constitute the largest POS categoryin the corpus based on the above tagging scheme.This seems to be characteristic of other languagestoo.
However, Amharic makes extensive use of nounclauses for representing different kinds of subordi-nate clauses.
Noun clauses are headed by a verbalnoun, which is assigned a noun POS tag.
This addsto the skewedness of POS tag distributions whichin turn biases the POS tagger that relies heavily onmorphological features as we will show in Section 7.Interjections, on the other hand, do not occur in thesample corpus, as these words usually do not appearoften in newspaper text.Once the POS tagset has been compiled andtested, the next logical step is to explore automaticmethods of analyzing Amharic words, which we ex-plore in the next section.5 POS Tagging of AmharicSemitic languages like Arabic, Hebrew and Amharichave a much more complex morphology than En-glish.
In these languages, words usually consistof several bound morphemes that would normallyhave independent lexical entries in languages likeEnglish.
Furthermore, in Arabic and Hebrew, the49Description POS tag FrequencyNoun N 586Verb V 203Auxiliary AUX 20Numeral NU 65Adjective AJ 31Adverb AV 8Adposition AP 30Interjection I 0Punctuation PU 36Residual R 15Table 1: Distribution of POS tagsdiacritics that represent most vowels and gemina-tion patterns are missing in written texts.
AlthoughAmharic does not have a special marker for gem-ination, the Amharic script fully encodes both thevowels and the consonants, hence it does not sufferfrom the ambiguity problem that may arise due tothe missing vowels.As mentioned briefly in Section 1, the morpho-logical complexity of these languages opens up dif-ferent alternative approaches in developing POStaggers for them (Bar-Haim et al, 2004; Diabet al, 2004).
Bar-Haim et al (2004) showedthat morpheme-based tagging performs better thanword-based tagging; they used Hidden MarkovModels (HMMs) for developing the tagger.On the basis of the idea introduced by Bar-Haimet al (2004), we formulate the following two relatedtasks for the analysis of Amharic words: segmen-tation and POS tagging (sequence labeling).
Seg-mentation refers to the analysis of a word into con-stituting morphemes.
The POS tagging task, on theother hand, deals with the assignment of POS tagsto words.
The revised POS tags that are introducedin Section 3 will be used for this task.
The mainreason for choosing words as a unit of analysis andadopting the abstract POS tags is that the limited re-source that we have prohibits us from carrying outfine-grained classification experiments.
As a resultof this, we choose to aim at a less ambitious goal ofinvestigating to what extent the strategies used forunknown word recognitions can help fill the gap leftby scarce resources.
Therefore, we mainly focus onword-based tagging and explore different kinds offeatures that contribute to tagging accuracy.Although the segmentation and POS tagging taskslook different, both can be reduced to sequence la-beling tasks.
Since the size of the annotated cor-pora is very small, a method needs to be chosenthat allows an optimal utilization of the limited re-sources that are available for Amharic.
In this re-spect, CRFs are more appropriate than HMMs sincethey allow us to integrate information from differentsources (Lafferty et al, 2001).
In the next section,we provide a brief description of CRFs.6 Conditional Random FieldsConditional Random Fields are conditional proba-bility distributions that take the form of exponentialmodels.
A special case of CRFs, linear chain CRF,which takes the following form, has been widelyused for sequence labeling tasks.P (y | x) =1Z (x)exp(?t=1?k?kfk (t, yt?1, yt, x)),where Z (x) is the normalization factor, X ={x1, .
.
.
, xn} is the observation sequence, Y ={y1, .
.
.
, yT } is the label sequences, fk and ?kare the feature functions and their correspondingweights respectively (Lafferty et al, 2001).An important property of these models is thatprobabilities are computed based on a set of featurefunctions, i.e.
fk, (usually binary valued), whichare defined on both the observation X and label se-quences Y .
These feature functions describe differ-ent aspect of the data and may overlap, providinga flexible way of describing the task.
CRFs havebeen shown to perform well in a number of naturallanguage processing applications, such as POS tag-ging (Lafferty et al, 2001), shallow parsing or NPchunking (Sha and Pereira, 2003), and named entityrecognition (McCallum and Li, 2003).In POS tagging, context information such as sur-rounding words and their morphological features,i.e., suffixes and prefixes, significantly improves per-formance.
CRFs allow us to integrate large set ofsuch features easily.
Therefore, it would be interest-ing to see to what extent the morphological featureshelp in predicting Amharic POS tags.
We used theminorThird implementation of CRF (Cohen, 2004).507 ExperimentsThere are limited resources for the Amharic lan-guage, which can be used for developing POS tag-ger.
One resource that may be relevant for the cur-rent task is a dictionary consisting of some 15,000entries (Amsalu, 1987).
Each entry is assigned oneof the five POS tags; Noun, Verb, Adjectives, Ad-verb, and Adposition.
Due to the morphologicalcomplexity of the language, a fully inflected dic-tionary consisting only of 15,000 entries is boundto have limited coverage.
Furthermore, the dictio-nary contains entries for phrases, which do not fallinto any of the POS categories.
Therefore the actualnumber of useful entries is a lot less than 15,000.The data for the experiment that will be describedbelow consists of 5 annotated news articles (1000words).
The Amharic text has been transliterated us-ing the SERA transliteration scheme, which encodesAmharic scripts using Latin alphabets (Daniel,1996).
This data is very small compared to the dataused in other segmentation and POS tagging experi-ments.
However, it is worthwhile to investigate howsuch a limited resource can meaningfully be used fortackling the aforementioned tasks.7.1 SegmentationThe training data for segmentation task consists of 5news articles in which the words are annotated withsegment boundaries as shown in the following ex-ample.. .
.<seg>Ind</seg><seg>astawequt</seg>#<seg>le</seg><seg>arso</seg>#<seg> aderu</seg># <seg>be</seg><seg>temeTaTaN</seg> .
.
.In this example, the morphemes are enclosed in<seg> and </seg> XML tags.
Word-boundariesare indicated using the special symbol #.
The reduc-tion of the segmentation task to a sequence labelingtask is achieved by converting the XML-annotatedtext into a sequence of character-tag pairs.
Eachcharacter constitutes a training (test) instance.
Thefollowing five tags are used for tagging the char-acters; B(egin), C(ontinue), E(nd), U(nique) andN(egative).
Each character in the segment is as-signed one of these tags depending on where it ap-pears in the segment; at the beginning (B), at the end(E), inside (C), or alone (U).
While the tags BCE areused to capture multi-character morphemes, the Utag is used to represent single-character morphemes.The negative tag (N) is assigned to the special sym-bol # used to indicate the word boundary.
Thoughexperiments have been carried out with less elab-orate tagging schemes such as BIO (Begin-Inside-Outside), no significant performance improvementhas been observed.
Therefore, results are reportedonly for the BCEUN tagging scheme.The set of features that are used for training arecomposed of character features, morphological fea-tures, dictionary features, the previous tag, and char-acter bi-grams.
We used a window of eleven charac-ters centered at the current character.
The charac-ter features consist of the current character, the fivecharacters to the left and to the right of the currentcharacters.
Morphological features are generated byfirst merging the set of characters that appear be-tween the word boundaries (both left and right) andthe current character.
Then a binary feature will begenerated in which its value depends on whether theresulting segment appears in a precompiled list ofvalid prefix and suffix morphemes or not.
The samesegment is also used to generate another dictionary-based feature, i.e., it is checked whether it exists inthe dictionary.
Character bi-grams that appear to theleft and the right of the current character are alsoused as features.
Finally, the previous tag is alsoused as a feature.7.2 POS TaggingThe experimental setup for POS tagging is similar tothat of the segmentation task.
However, in our cur-rent experiments, words, instead of characters, areannotated with their POS tags and hence we havemore labels now.
The following example shows theannotation used in the training data.. .
.<V>yemikahEdut</V><N>yemrmr</N><N>tegbarat</N><V>yatekorut</V><N>bemgb</N> <N>sebl</N>.
.
.51Each word is enclosed in an XML tag that denotes itsPOS tag.
These tags are directly used for the trainingof the sequence-labeling task.
No additional reduc-tion process is carried out.The set of features that are used for training arecomposed of lexical features, morphological fea-tures, dictionary features, the previous two POStags, and character bi-grams.
We used a window offive words centered at the current word.
The lex-ical features consist of the current word, the twowords to the left and to the right of the current word.Morphological features are generated by extractinga segment of length one to four characters long fromthe beginning and end of the word.
These segmentsare first checked against a precompiled list of validprefix and suffix morphemes of the language.
If thesegment is a valid morpheme then an appropriatefeature will be generated.
Otherwise the null pre-fix or suffix feature will be generated to indicate theabsence of an affix.
The dictionary is used to gen-erate a binary feature for a word based on the POStag found in the dictionary.
In other words, if theword is found in the dictionary, its POS tag will beused as a feature.
For each word, a set of characterbi-grams has been generated and each character bi-gram is used as a feature.
Finally, the last two POStags are also used as a feature.8 ResultsWe conducted a 5-fold cross-validation experiment.In each run, one article is used as a test dataset andthe remaining four articles are used for training.
Theresults reported in the sections below are the averageof these five runs.
On average 80% of the words inthe test files are unknown words.
Most of the un-known words (on average 60%) are nouns.8.1 Segmentation ResultAs mentioned in Section 7.1, four sets of features,i.e., character features, morphological features, dic-tionary features, and previous label, are used for thesegmentation task.
Table 2 shows results for somecombinations of these features.
The results withoutthe previous label feature are also shown (WithoutPrev.
Label).The simple character features are highly informa-tive features, as can be seen in Table 2 (Row 1).Using only these features, the system with previouslabel feature already achieved an accuracy of 0.819.The dictionary feature improved the result by 2%whereas the morphological features brought minorimprovements.
As more features are added the vari-ation between the different runs increases slightly.Performace significantly decreases when we omitthe previous label feature as it is shown in WithoutPrev.
Label column.8.2 POS Tagging ResultsTable 3 shows the word-based evaluation results ofthe POS tagging experiment.
The baseline (Row 1)means assigning all the words the most frequentlyoccurring POS tag, i.e., N (noun).
The result ob-tained using only lexical features (Row 2) is bet-ter than the baseline.
Adding morphological fea-tures improves the result almost by the same amount(Row 3).
Incorporation of the dictionary feature,however, has brought only slight improvement.
Theaddition of bi-gram features improved the result by3%.As mentioned before, it is not possible to com-pare the results, i.e.
74% accuracy (With Prev.
La-bel), with other state of the art POS taggers since ourdata is very small compared to the data used by otherPOS taggers.
It is also difficult to claim with abso-lute certainty as to the applicability of the techniquewe have applied.
However, given the fact that 80%of the test instances are unseen instances, an accu-racy of 74% is an acceptable result.
This claim re-ceives further support when we look at the results re-ported for unknown word guessing methods in otherPOS tagging experiments (Nakagawa et al, 2001).As we add more features, the system shows less vari-ation among the different folds.
As with segmenta-tion task, the omission of the previous label featuredecreases performace.
The system with only lexicalfeatures and without previous label feature has thesame performace as the baseline system.8.3 Error AnalysisThe results of both the segmentation and POS tag-ging tasks show that they are not perfect.
An ex-amination of the output of these systems shows cer-tain patterns of errors.
In case of the segmenta-tion task, most of the words that are incorrectly seg-mented have the same beginning or ending charac-52With Prev.
Label Without Prev.
LabelFeatures accuracy stddev accuracy stddevChar.
0.819 0.7 0.661 4.7Char.+Dict.
0.837 1.6 0.671 4.1Char.+Dict.+Morph.
0.841 1.7 0.701 3.9Table 2: Segmentation ResultsWith Prev.
Label Without Prev.
LabelFeatures accuracy stddev accuracy stddevBaseline 0.513 6.4 ?
?Lex.
0.613 5.3 0.513 6.4Lex.+Morph.
0.700 5.0 0.688 5.2Lex.+Morph.+Dict.
0.713 4.3 0.674 5.6Lex.+Morph.+Dict.+Bigram 0.748 4.3 0.720 2.9Table 3: Word-based evaluation results of POS taggingters as words with affix morphemes.
Increasing thesize of the lexical resources, such as the dictionary,can help the system in distinguishing between wordsthat have affixes from those that do not.The POS tagging system, on the other hand,has difficulties in distinguishing between nouns andother POS tags.
This in turn shows how similarnouns are to words in other POS tags morpholog-ically, since our experiment relies heavily on mor-phological features.
This is not particularly sur-prising given that most Amharic affixes are sharedamong nouns and words in other POS tags.
InAmharic, if a noun phrase contains only the headnoun, most noun affixes, such as prepositions, def-inite article, and case marker appear on the headnoun.
If, on the other hand, a noun phrase containsprenominal constituents such as adjectives, numer-als, and other nouns, then the above noun affixesappear on prenominal constituents, thereby blurringthe morphological distinction between the nounsand other constituents.
Furthermore, similar setsof morphemes are used for prepositions and subor-dinate conjunctions, which again obscures the dis-tinction among the nouns and verbs.
This, togetherwith the fact that nouns are the dominant POS cate-gory in the data, resulted in most words being miss-classified as nouns.In general, we believe that the above problems canbe alleviated by making more training data availableto the system, which will enable us to determine im-proved parameters for both segmentation and POStagging models.9 Concluding RemarksIn this paper, we provided preliminary results of theapplication of CRFs for Amharic word segmentationand POS tagging tasks.
Several features were exam-ined for these tasks.
Character features were foundto be useful for the segmentation task whereas mor-phological and lexical features significantly improvethe results of the POS tagging task.
Dictionary-based features contribute more to the segmentationtask than to the POS tagging task.
In both experi-ments, omition of previous label feature hurts per-formance.Although the size of the data limits the scope ofthe claims that can be made on the basis of the re-sults, the results are good especially when we lookat them from the perspective of the results achievedin unknown word recognition methods of POS tag-ging experiments.
These results could be achievedsince CRFs allow us to integrate several overlappingfeatures thereby enabling optimum utilization of theavailable information.In general, the paper dealt with a restricted as-pect of the morphological analysis of Amharic, i.e.,Amharic word segmentation and POS tagging.
Fur-thermore, these tasks were carried out relatively in-dependently.
Future work should explore how thesetasks could be integrated into a single system that53allows for fine-grained POS tagging of Amharicwords.
Parallel to this, resource development needsto be given due attention.
As mentioned, the lackof adequate resources such as a large POS annotatedcorpus imposes restrictions on the kind of methodsthat can be applied.
Therefore, the development ofa standard Amharic POS tagset and annotation of areasonably sized corpus should be given priority.AcknowledgementsThis research was supported by the NetherlandsOrganization for Scientific Research (NWO) underproject number 220-80-001.ReferencesNega Alemayehu.
1999.
Development of stemming al-gorithm for Amharic text retrieval.
PhD Thesis, Uni-versity of Sheffield.Atelach Alemu.
2002.
Automatic Sentence Parsingfor Amharic Text: An Experiment using ProbabilisticContext Free Grammars.
Master Thesis, Addis AbabaUniversity.Amsalu Aklilu.
1987.
Amharic-English Dictionary.
Ku-raz Publishing Agency.Roy Bar-Haim, Khalil Simaan and Yoad Winter.
2004.Part-of-Speech Tagging for Hebrew and Other SemiticLanguages.
Technical Report.Abiyot Bayou.
2000.
Developing automatic word parserfor Amharic verbs and their derivation.
Master The-sis, Addis Ababa University.W.
Cohen.
2004.
Methods for Identifying Namesand Ontological Relations in Text using Heuristicsfor Inducing Regularities from Data.
http://minorthird.sourceforge.netPeter T. Daniels, 1997.
Script of Semitic Languages in:Robert Hetzron, editor,Proceedings of the Corpus Lin-guistics.
16?45.Mona Diab, Kadri Hacioglu and Daniel Jurafsky.
2004.Automatic tagging of Arabic text: From row text tobase phrase chunks.
In Daniel Marku, Susan Dumaisand Salim Roukos, editors, HLT-NAACL 2004: Shortpapers, pages 149?152, Boston, Massachusetts, USA,May 2?May 7.
Association for Computational Lin-guisticsSisay Fissaha Adafre and Johann Haller.
2003a.Amharic verb lexicon in the context of machine trans-lation.
Traitement Automatique des Langues Na-turelles 2:183?192Sisay Fissaha Adafre and Johann Haller.
2003b.
Ap-plication of corpus-based techniques to Amharic texts.Machine Translation for Semitic languages MT Sum-mit IX Workshop, New OrleansMesfin Getachew.
2001.
Automatic part of speechtagging for Amharic language: An experiment usingstochastic HMM.
Master Thesis, Addis Ababa Uni-versity.Wolf Leslau.
1995.
Reference Grammar of Amharic.Otto Harrassowitz, Wiesbaden.A.
McCallum and W. Li.
2003.
Early results for NamedEntity Recognition with conditional random fields,feature induction and web-enhanced lexicons.
Pro-ceedings of the 7th CoNLL.Tetsuji Nakagawa, Taku Kudo and Yuji Matsumoto.2001.
Unknown Word Guessing and Part-of-Speech Tagging Using Support Vector Machines.NLPRS pages 325-331, Boston, Massachusetts,USA, May 2?May 7. http://www.afnlp.org/nlprs2001/pdf/0053-01.pdfJ.
Lafferty, F. Pereira and A. McCallum.
2001.
Con-ditional random fields: Probabilistic models for seg-menting and labeling sequence data.
Proceedings ofthe International Conference on Machine Learning.F.
Sha and F. Pereira.
2004.
Shallow parsing with con-ditional random fields.
Proceedings of Human Lan-guage Technology-NAACL.Khoja S., Garside R., and Knowles G. 2001.
A Tagset forthe Morphosyntactic Tagging of Arabic Proceedingsof the Corpus Linguistics.
Lancaster University (UK),Volume 13 - Special issue, 341.Leech G. Wilson.
1996.
Recommendationsfor the Morphosyntactic Annotation of Cor-pora.
EAGLES Report EAG-TCWG-MAC/R,http://www.ilc.pi.cnr.it/EAGLES96/annotate/annotate.htmlDaniel Yacob.
1996.
System for Ethiopic Representationin ASCII.
http://www.abyssiniagateway.net/fidelBaye Yimam.
1999.
Root.
Ethiopian Journal of Lan-guage Studies, 9:56?88.Baye Yimam.
1986.
Yamara Swasw E.M.P.D.A, AddisAbaba.54
