SPEECH DIALOGUE WITH FACIAL  D ISPLAYS:MULT IMODAL HUMAN-COMPUTER CONVERSATIONKatash i  Nagao  and Ak ikazu  Takeuch iSony Computer  Science Laboratory  Inc.3 -14-13 Higashi -gotanda,  Sh inagawa-ku,  Tokyo 141, JapanE-mail:  { nagao,t  akeuchi} @csl.sony.co.j pAbst ractHuman face-to-face onversation is an ideal modelfor human-computer dialogue.
One of the majorfeatures of face-to-face ommunication is its multi-plicity of communication channels that act on mul-tiple modalities.
To realize a natural multimodaldialogue, it is necessary to study how humans per-ceive information and determine the informationto which humans are sensitive.
A face is an in-dependent communication channel that conveysemotional and conversational signals, encoded asfacial expressions.
We have developed an experi-mental system that integrates speech dialogue andfacial animation, to investigate the effect of intro-ducing communicative facial expressions as a newmodality in human-computer conversation.
Ourexperiments have showen that facial expressionsare helpful, especially upon first contact with thesystem.
We have also discovered that featuringfacial expressions at an early stage improves ub-sequent interaction.In t roduct ionHuman face-to-face onversation is an ideal nmdelfor human-computer dialogue.
One of the majorfeatures of face-to-face communication is its mul-tiplicity of communication channels that act onmultiple modalities.
A channel is a communica-tion medium associated with a particular encod-ing method.
Examples are the auditory channel(carrying speech) and the visual channel (carry-ing facial expressions).
A modality is the senseused to perceive signals from the outside world.Many researchers have been developing mul-timodal dialogue systems.
In some cases, re-searchers have shown that information in onechannel complements or modifies information inanother.
As a simple example, the phrase "deleteit" involves the coordination of voice with ges-ture.
Neither makes ense without the other.
Re-searchers have also noticed that nonverbal (ges-ture or gaze) information plays a role in set-ting the situational context which is useful in re-stricting the hypothesis pace constructed ur-ing language processing.
Anthropomorphic inter-faces present another approach to nmltimodal di-alogues.
An anthropomorphic interface, such asGuides \[Don et al, 1991\], provides a means torealize a new style of interaction.
Such researchattempts to computationally capture the commu-nicative power of the human face and apply it tohuman-computer dialogue.Our research is closely related to the last ap-proach.
The aim of this research is to improvehuman-computer dialogue by introducing human-like behavior into a speech dialogue system.
Suchbehavior will include factors uch as facial expres-sions and head and eye movement.
It will help toreduce any stress experienced by users of comput-ing systems, lowering the complexity associatedwith understanding system status.Like most dialogue systems developed by nat-ural language researchers, our current system canhandle domain-dependent, i formation-seeking di-alogues.
Of course, the system encounters prob-lems with ambiguity and missing intbrmation (i.e.,anaphora nd ellipsis).
The system tries to re-solve them using techniques from natural languageunderstanding (e.g., constraint-based, case-based.and plan-based methods).
We are also studyingthe use of synergic multimodality to resolve lin-guistic problems, as in conventional multimodalsystems.
This work will bc reported in a separatepublication.In this paper, we concentrate on the roleof nonverbal nlodality for increasing flexibility ofhuman-computer dialogue and reducing the men-tal barriers that many users associate with com-puter systems.Research  Overv iew o f  Mu l t imoda lD ia loguesMultimodal dialogues that combine verbal andnonverbal communication have been pursued102mainly from the following three viewpoints.1.
Combining direct manipulation with natural an-guage (deictic) expressions"Direct manipulation (DM)" was suggested byShneiderinan \[1983\].
The user can interact di-rectly with graphical objects displayed on thecomputer screen with rapid, iNcremeNtal, re-versible operations whose effects on the objectsof interest are immediately visible.The semantics of natural language (NL) ex-pressions is anchored to real-world objects andevents by means of pointing and demoNstratiNgactions and deictic expressions such as "this,""that," "here," "there," "theN," and "now.
"Some research on dialogue systems has coin-bined deictic gestures aNd natural anguage suchas Put-That-There \[Bolt, 1980\], CUBRICON\[Neal et al, 1988\], and ALFREsco \[Stock, 1991\].One of the advantages of combined NL/DM in-teraction is that it can easily resolve the miss-ing information in NL expressions.
For exam-ple, wheN the system receives a user request inspeech like "delete that object," it can fill in themissing information by looking for a pointinggesture from the user or objects on the screenat the time the request is made.2.
Using nonverbal inputs to specify the ;~ontextand filter out unrelated informationThe focus of attention or the focal point playsa very important role in processing applicationswith a broad hypothesis pace such as speechrecognition.
One example of focusing modalityis following the user's looking behavior.
Fixa-tion or gaze is useful for the dialogue systemto determine the context of the user's inter-est.
For example, when a user is looking ata car, that the user says at that time may berelated to the car.
Prosodic information (e.g.,voice tones) in the user's utterance also helpsto determine focus.
In this case, the systemuses prosodic information to infer the user's be-liefs Or intentions.
Combining estural informa-tion with spoken language comprehension showsanother example of how context may be deter-mined by the user's nonverbal behavior \[Ovi-att et al, 1993\].
This research uses multimodalforms that prompt a user to speak or write intolabeled fields.
The forms are capable of guidingand segmenting inputs, of conveying the kind ofinformation the system is expecting, and of re-ducing ambiguities in utterances by restrictingsyntactic and semantic omplexities.3.
Incorporating human-like behavior into dialoguesystems to reduce operation complexity andstress often associated with computer systemsDesigning human-computer dialogue requiresthat the computer makes appropriate backchan-nel feedbacks like NoddiNg or expressions suchas "aha" and "I see."
One of the major ad-vantages of using such nonverbal behavior inhuman-computer conversation is that reactionsare quicker than those fl'om voice-based re-spouses.
For example, the facial backchannelplays an important role in hulnan face-to-faceconversation.
We consider such quick reac-tions as being situated actions \[Suchman, 1987\]which are necessary for resource-bounded dia-logue participants.
Timely responses are crucialto successfid conversation, since some delay inreactions can imply specific meanings or makemessages unnecessarily ambiguous.Generally, visual channels contribute to quickuser recognition of system status.
For example,the system's gaze behavior (head and eye move-meat) gives a strong impression of whether itis paying attention or not.
If the system's eyeswander around aimlessly, the user easily recog-nizes the system's attention elsewhere, perhapseven unaware that he or she is speaking to it.Thus, gaze is an important indicator of system(in this case, speech recognition) status.By using human-like nonverbal behavior, thesystem can more flexibly respond to the userthan is possible by using verbal modality alone.We focused on the third viewpoint and devel-oped a system that acts like a human.
We em-ployed communicative facial expressions as a newmodality in human-computer conversation.
Wehave already discussed this, however, in anotherpaper \[Takeuchi and Nagao, 1993\].
Here, we con-sider our implemented system as a testbed for in-corporating human-like (nonverbal) behavior intodialogue systems.The following sections give a system overview,an example dialogue along with a brief explanationof the process, and our experimental results.I ncorporat ing  Facial  D isp lays  in to  aSpeech  D ia logue  SystemFacial Displays as a New Modal i tyThe study of facial expressions has attracted theinterest of a number of different disciplines, in-cluding psychology, ethology, and interpersonalcommunications.
Currently, there are two basicschools of thought.
One regards facial expres-sions as beiu~ expressioNs of emotion \[Ekman andFriesen, 1984\].
The other views facial expressionsin a social context, regarding them as being com-municative signals \[Chovil, 1991\].
The term "fa-cial displays" is essentially the same as "facial ex-pressions," but is less reminiscent of emotion.
Inthis paper, therefore, we use "facial displays.
"103A face is an independent communication chan-nel that conveys emotional and conversational sig-nals, encoded as facial displays.
Facial displayscan be also regarded as being a modality becausethe human brain has a special circuit dedicated totheir processing.Table 1 lists all the communicative facial dis-plays used in the experiments described in a latersection.
The categorization framework, terminol-ogy, and individual displays are based on the workof Chovil \[1991\], with the exception of the em-phasizer, underliner, and facial shrug.
These werecoined by Ekman \[1969\].Table 1: Communicative Facial Displays Used inthe Experiments.
(Categorization based mostlyon Chovil \[1991\])Syntactic Display~ a t i o n2.
Question mark3.
Emphasizer4.
Underliner5.
Punctuation6.
End of an utterance7.
Beginning of a story8.
Story continuation9.
End of a story10.
Think'rag Remembering11.
Facial shrug:"I don't know"12.
Interactive: "You know?"13.
Metacommunicative:Indication of sarcasm or joke14.
"Yes"15, "No"15, "Not"17.
*'But"Listener Comment Disp--~ay18.
Backchannel:Indication of attendance19.
Indication of loudnessUnderstanding levels20.
Confident21.
Moderately confident22, Not confident23.
"Yes"~ gEyebrow raising or loweringEyebrow raising or loweringLonger eyebrow raisingEyebrow movementEyebrow raisingEyebrow raisingAvoid eye contactEye contactEyebrow raising or lowering-T-closing the eyes,pulling back one mouth sideEyebrow flashes,mouth corners pulled down,mouth corners pulled backEyebrow raisingEyebrow raising andlooking up and offEyebrow actionsEyebrow act ionsEyebrow actionsEyebrow actionsEyebrow raising,mouth corners turned downEyebrows drawn to centerEyebrow raising, head nodEyebrow raisingEyebrow loweringEyebrow raisingEvaluation of utterances24.
Agreement Eyebrow raising25.
Request for more information Eyebrow raising26.
Incredulity Longer eyebrow raisingThree major categories are defined as follows.Syntact i c  displays.
These are facial displaysthat (1) place stress on particular words or clauses,(2) are connected with the syntactic aspects of anutterance, or (3) are connected with the organiza-tion of the talk.Speaker  displays.
Speaker displays are facialdisplays that (1) illustrate the idea being verballyconveyed, or (2) add additional information to theongoing verbal content.L is tener  comment  displays.
These are facialdisplays made by the person who is not speaking,in response to the utterances of the speaker.An  In tegrated  System o f  SpeechD ia logue  and  Fac ia l  An imat ionWe have developed an experimental system thatintegrates speech dialogue and facial animation toinvestigate the effects of human-like behavior inhuman-computer dialogue.The system consists of two subsystems, a fa-cial animation subsystem that generates a three-dimensional face capable of a range of facial dis-plays, and a speech dialogue subsystem that rec-ognizes and interprets peech, and generates voiceoutputs.
Currently, the animation subsystem runson an SGI 320VGX and the speech dialogue sub-system on a Sony NEWS workstation.
These twosubsystems communicate with each other via anEthernet network.Figure 1 shows the configuration of tlle inte-grated system.
Figure 2 illustrates the interactionof a user with the system.i .................. t .
~-~T~ --6---~.~ -., .Speech recognition \~ 11 ,~.
~ Word sequence ~\~ ~ Symactic & semantic analysis ~ ~',.
?
...................................... \-,I i .
.
.
.
.
,o?
.. .
.
.
.
.
.
.......... ~  sr,~E's in=ntion "\1"~--~'.'.
...... L:...il ~ ........ : ,i .
.
.
.
.  "
' "~  .
_"~'~------~i'y m of fa~ ~'1 d i~C"~"~-- -  __ } ~ Muscle paramemrs i !
~ System's responsei \] Facial animation ~ i !
I Voice synthesis.:.
............ ~-_ .
.
-= .
: : .
.
- :E .
.
.~to_ :o .
, .
!
!~ ,  ~_ .~-~.
.
.
:=~.
.
.~  .......... ~ .......Facial display ~ VoiceFacial animation subsystem Speech dialogue subsystcmFigure 1: System ConfigurationFac ia l  An imat ion  SubsystemThe face is modeled three-dimensionally.
Our cur-rent version is composed of approximately 500polygons.
The face can be rendered with a skin-like surface material, by applying a texture maptaken from a photograph or a video frame.In 3D computer graphics, a facial display isrealized by local deformation of the polygons rep-resenting the face.
Waters showed that deforma-tion that simulates the action of muscles under-lying the face looks more natural \[Waters, 1987\].We therefore use munerical equations to simulatemuscle actions, as defined by Waters.
Currently,104oii iiiiiiiiiiiiiiiiiiiiiiiiiiiiii!iiiii!iii!iiiii~iiii!iiiiiii)iiiii !
!
iii:jiiii+iiiiiiiiiiiiiii+ili iiiiii i+ i i '.......... ; i l lFigure 2: Dialogue Snapshotthe system incorporates 16 muscles and 10 pa-rameters, controlling mouth opening, jaw rotation,eye movement, eyelid oI)ening, and head orienta-tion.
These 16 nmscles were deternfined by Wa-ters, considering the correspondence with actionunits in the Facial Action Coding System (FACS)\[Ekman and Friesen.
1978\].
For details of the fa-cial modeling and animation system, see \[Takeuchiand Franks, 1992\].We use 26 synthesized facial displays, corre-sponding to those listed in Table 1, and two ad-ditional displays.
All facial displays are generatedby the above method, and rendered with a texturemap of a young boy's face.
The added displaysare "Smile" and "Neutral."
The "Neutral" displayfeatures no muscle contraction whatsoever, and isused when no conversational signal is needed.At run-time, the animation subsystem awaitsa request fi'om the speech subsystem.
When theanimation subsystem receives a request hat spec-ifies values for the 26 parameters, it starts to de-form the face, on the basis of the received values.The deformation process is controlled by the dif-ferential equation ff = a - f ,  where f is a param-eter value at time t and f '  is its time derivativeat time t. a is the target value specified in therequest,.
A feature of this equation is that defor-mation is fast in the early phase but soon slows,corresponding closely to the real dynamics of fa-cial displays.
Currently, the base performance ofthe animation subsystem is around 20-25 framesper second when running on an SGI Power Series.This is sufficient o enable real-time animation.Speech Dialogue SubsystemOur speech dialogue subsystem works as follows.First, a voice input is acoustically analyzed by abuilt-in sound processing board.
Then, a speechrecognition module is invoked to output word se-quences that have been assigned higher scores bya probabilistic phoneme model.
These word se-quen(:es are syntactically and semantically ana-lyzed and disambiguated by applying a relativelyloose grammar and a restricted omain knowledge.Using a semantic representation of the input ut-terance, a I)lan recognition module extracts thespeaker's intention.
For example, ti'om the ut-terance "I am interested in Sony's workstation.
"the module interprets the speaker's intention as"he wants to get precise information about Sony'sworkstation."
Once the system deternfines thespeaker's intention, a response generation moduleis invoked.
This generates a response to satisfy thespeaker's request.
Finally, the system's response isoutput as voice by a voice synthesis module.
Thismodule also sends the information about lip syn-chronization that describes phonemes (includingsilence) in the response and their time durationsto the facial animation subsystem.With the exception of the voice synthesis nmd-ule, each nmdule can send messages to the facialanimation subsystem to request he generation ofa facial display.
The relation between the speechdialogues and facial displays is discussed later.In this case, the specific task of the systemis to provide information about Sony's computer-related products.
For example, the system can an-swer questions about price, size, weight, and spec-ifications of Sony's workstations and PCs.Below, we describe the modules of the speechdiMogue subsystem.Speech recogni t ion .
This module was jointlydeveloped with the ElectrotechnicM Laboratoryand Tokyo Institute of Technology.
Speaker-independent continuous speech inputs are ac-cepted without special hardware.
To obtain ahigh level of accuracy, context-dependent pho-netic hidden Marker models are used to constructphoneme-level hypotheses \[Itou et al.
1992\].
Thisnmdule can generate N-best word-level hypothe-ses.Syntact i c  and semant ic  analysis.
This mod-ule consists of a parsing n~echanism, a semanticanalyzer, a relatively loose grammar consisting of24 rules, a lexicon that includes 34 nouns.
8 verbs.4 adjectives and 22 particles, and a fl'ame-basedknowledge base consisting of 61 conceptual frames.Our semantic analyzer can handle ambiguities insyntactic structures and generates a semantic rep-resentation of the speaker's utterance.
We ap-plied a preferential constraint satisfaction tech-nique \[Nagao, 1992\] for perfornfing disambigua-tion and semantic analysis.
By allowing the prefer-ences to control the application of the constraints.105ambiguities can be efficiently resolved, thus avoid-ing combinatorial explosions.P lan  recogni t ion .
This module determines thespeaker's intention by constructing a model ofhis/her beliefs, dynamically adjusting and expand-ing the model as the dialogue progresses \[Nagao,1993\].
The model deals with the dynamic natureof dialogues by applying the following two mech-anisms.
First, preferences among the contexts aredynamically computed based on the facts and as-sumptions within each context.
The preferenceprovides a measure of the plausibility of a context.The currently most preferable context contains acurrently recognized plan.
Secondly, changing themost plausible context among mutually exclusivecontexts within a dialogue is formally treated asbelief revision of a plan-recognizing agent.
How-ever, in some dialogues, many alternatives mayhave very similar preference values.
In this situ-ation, one may wish to obtain additional infor-mation, allowing one to be more certain aboutcommitting to the preferable context.
A crite-rion for detecting such a critical situation basedon the preference measures for mutually exclusivecontexts is being explored.
The module also main-tains the topic of the current dialogue and can han-dle anaphora (reference of pronouns) and ellipsis(omission of subjects).Response  generat ion .
This module generates aresponse by using domain knowledge (database)and text templates (typical patterns of utter-ances).
It selects appropriate templates and com-bines them to construct a response that satisfiesthe speaker's request.In our prototype system, the method used tocomprehend speech is a specific combination ofspecific types of knowledge sources with a ratherfixed information flow, preventing flexible inter-action between them.
A new method that en-ables flexible control of omni-directional informa-tion flow in a very context-sensitive fashion hasbeen announced \[Nagao et al, 19931.
Its archi-tecture is based on dynamical constraint \[Hasidaet al, 19931 which defines a fine classificationbased on the dimensions of satisfaction and the vi-olation of constraints.
A constraint is representedin terms of a clausal ogic program.
A fine-graineddeclarative semantics i defined for this constraintby measuring the degree of violation in terms ofreal-valued potential energy.
A field of force arisesalong the gradient of this energy, inferences be-ing controlled on the basis of the dynamics.
Thisallows us to design combinatorial behaviors un-der declarative semantics within tractable com-putational complexity.
Our forthcoming systemcan, therefore, concentrate on its computationalresources according to a dynamic focal point thatis important o speech processing with broad by-pothesis pace, and apply every kind of constraint,from phonetic to pragmatic, at the same time.Cor respondence  betweenConversat iona l  S i tuat ions  and  FacialDisplaysThe speech dialogue subsystem recognizes a num-ber of typical conversational situations that areimportant o dialogues.
We associate these situ-ations with an appropriate facial display(s).
Forexample, in situations where speech input is notrecognized or where it is syntactically invalid, thelistener comment display "Not confident" is dis-played.
If the speaker's request exceeds the rangeof the system's domain knowledge, then the sys-tem displays a facial shrug and replies "I cannotanswer such a question."
The relationships be-tween conversational situations and facial displaysare listed in Table 2.Example DialogueHere is an example of a possible dialogue with thesystem 1.
Un: indicates user speech inputs, Sn:indicates voice responses delivered by the systemto the user.
Terms enclosed within square bracketsillustrate the types of facial displays introducedin previous ection.
"Continuing" means that thesystem facial expression remains the same.U I :  Hello.SI:  \]Attend and BOStory\] Hi.
This is Sony Com-puter Science Laboratory.S2: \[Continuing\] I can answer ally question aboutcomputer-related products.U2: uh ...$3: \[NotConfident \] I beg your pardon.U3: I want to know about a personal (:(unlmter.$4: \[Question\] Do you want to know about a Sonypersonal computer?U4: No, I don't.$5: \[Shrug\] I cannot answer such a question.US: Please tell me about a Sony personal com-puter.$6: \[BOStory\] Sony personal computer "Quar-terL" is a standard IBM comI)atible notebook-style personal computer.U6: What can I do with it?$7: \[BOStory\] You can use all IBM PC software.$8: \[Continuing\] For example, you can use a wordprocessor, and a spreadsheet.UT: Can I use UNIX with it?$9: \[BOStory\] If you want to use UNIX,1Actually, the system accepts and speaks onlyJapanese.
The example is a translation.106Table 2: Relation between Conversational Situations and Facial DisplaysCONVERSATIONAL SITUATIONRecognition failureSyntactically invalid utteranceMany recognition cmldidateswith close scoresBeginning of a dialogueIntroduction to a topicShift "to 'another topicClarification dialogue"Underline a remarkAnswer "Yes"Answer "No"Out of the domainAnswer "Yes" With emphasisViolation of pragmaticconstraintsReply to "Thmlks"FACIAL DISPLAY(S)NotConfident (Listener comment display "Not confident")NotConfidentModConfident (Listener comment display "Moderately confident")Attend (Listener comment display "Indication of attendance")BOStory (Syntactic display "Beginning of a story")EOStory (Syntactic display "End of a story") and BOStoryQuestion (Syntactic display "Question mark")Underliner (Syntactic display "Underliner")SpeakerNo (Speaker display "No")Shrug (Speaker display "Facial shrug")SpeakerYes and Enlphasizer (Syntactic display "Emphasizer")Incredulity (Listener comment display "Incredulity")ListenerYes (Listener comment display "Yes") andSmile (Complementary display "Smile" )$10: \[Underliner\] I recommend you get a work-station.U8: Tell me about a workstation.S l1:  \[EOStory and BOStory\] Sony workstation"NEWS" is a high-performance laptop worksta-tion.S12: \[Continuing\] Its CPU is an R3081 RISC pro-cessor, and its processing speed of 37 MIPS isthe fastest in this class.U9: Is it large?SlO: \[SpeakerNo and Emphasizer\] No, it isn't.S l1:  \[BOStory\] "NEWS" is 32.4 cm in width,36.4 cm in depth, and 6.9 cm in height.U10: Is it light?812: \[SpeakerYes and Emphasizer\] Yes, it is.S13: \[BOStory\] The weight of "NEWS" is 4.5 kg.U l l :  How much?S14: \[BOStory\] "NEWS" costs 700,000 yen.U12: What does the personal computer cost?S15: \[EOStory and BOStory\] "QuarterL" costs398,000 yen.U13: Thank you.S16: \[ListenerYes and Smile\] You are welcome.S17: \[Continuing\] It's my pleasure.U2 is an example of noisy inputs.
The systemcould not recognize the expression and displayedthe facial backchannel NotConfident and replied "Ibeg your pardon."
In U3, there is missing infor-mation about personal computer maker.
So, thesystem enters a clarification dialogue $4, showingthe Question display.
In this case, the system triedto drive the user into the domMn with which thesystem is familiar.
However, the user refused toagree with the system by utterance U4, then thesystem revealed its discouragement by showing afacial shrug.
In US, the user changes the topic, byasking for workstation information.
The systemrecognizes this by comparison with the prior topic(i.e., personal computers).
Therefore, in responseto question S l l ,  the system displays EOStory andsubsequently BOStory to indicate the shift to adifferent opic.
The system also manages the topicstructure so that it can handle anaphora nd el-lipsis in utterances such as ug ,  UIO, and U l l .Exper imenta l  Resu l tsTo examine the effect of facial displays on the in-teraction between humans and computers, exper-iments were performed using the prototype sys-tem.
The system was tested on 32 volunteer sub-jects.
Two experiments were prepared.
In oneexperiment, called F, the subjects held a conver-sation with the system, which used facial displaysto reinforce its response.
In the other experiment,called N, the subjects held a conversation withthe system, which answered using short phrasesinstead of facial displays.
The short phrases weretwo- or three-word sentences that described thecorresponding facial displays.
For example, in-stead of the "Not confident" display, it simplydisplayed the words "I am not confident."
Thesubjects were divided into two groups, FN andNF.
As the names indicate, the subjects in theFN group were first subjected to experiment Fand then N. The subjects in the NF  group werefirst subjected to N and then F. In both experi-ments, the subjects were assigned the goal of en-107quiring about the functions and prices of Sony'scomputer products.
In each experiment, he sub-jects were requested to complete the conversationwithin 10 minutes.
During the experiments, thenumber of occurrences of each facial display wascounted.
The conversation content was also evalu-ated based on how many topics a subject coveredintentionally.
The degree of task achievement re-flects how it is preferable to obtain a greater num-ber of visit more topics, and take the least amountof time possible.
According to the frequenciesof appeared facial displays and the conversationalscores, the conversations that occurred during theexperiments can be classified into two types.
Thefirst is "smooth conversation" in which the score isrelatively high and the displays "Moderately con-fident," "Beginning of a story," and "Indicationof attendance" appear most often.
The second is"dull conversation," characterized by a lower scoreand in which the displays "Neutral" and "Not con-fident" appear more frequently.The results are summarized as follows.
Thedetails of the experiments were presented in an-other paper \[Takeuchi and Nagao, 1993\].1.
The first experiments of the two groups arecompared.
Conversation using facial displaysis clearly more successful (classified as smoothconversation) than that using short phrases.
Wecan therefore conclude that facial displays helpconversation i  the case of initial contact.2.
The overall results for both groups are com-pared.
Considering that the only difference be-tween the two groups is the order in which theexperiments were conducted, we can concludethat early interaction with facial displays con-tributes to success in the later interaction.3.
The experiments using facial displays 1 e andthose using short phrases N are compared.
Con-trary to our expectations, the result indicatesthat facial displays have little influence on suc-cessful conversation.
This means that the learn-ing effect, occurring over the duration of the ex-periments, is equal in effect to the facial dis-plays.
However, we believe that the effect ofthe facial displays will overtake the learning ef-fect once the qualities of speech recognition andfacial animation have been improved.The premature settings of the prototype sys-tem, and the strict restrictions imposed on theconversation inevitably detract from the poten-tial advantages available from systems using com-municative facial displays.
We believe that fur-ther elaboration of the system will greatly im-prove the results.
The subjects were relativelywell-experienced in using computers.
Experimentswith computer novices should also be done.Concluding Remarks and FurtherWorkOur experiments showed that facial displays arehelpful, especially upon first contact with the sys-tem.
It was also shown that early interactionwith facial displays improves ubsequent interac-tion, even though the subsequent interaction doesnot use facial displays.
These results prove quan-titatively that interfaces with facial displays helpto break down the mental barrier that many usershave toward computing systems.As a future research direction, we plan to in-tegrate more communication channels and modal-ities.
Among these, the prosodic information pro-cessing in speech recognition and speech synthe-sis are of special interest, as well as the recogni-tion of users' gestures and facial displays.
Also,further work needs to be done on the designand implementation of the coordination of mul-tiple communication modalities.
We believe thatsuch coordination is an emergent phenomenonfrom the tight interaction between the system andits ever-changing environments (including humansand other interactive systems) by means of situ-ated actions and (more deliberate) cooperative ac-tions.
Precise control of multiple coordinated ac-tivities is not, therefore, directly implementable.Only constraints or relationships among percep-tion, conversational situations, and action will beimplementable.To date, conversation with computing sys-tems has been over-regulated conversation.
Thishas been made necessary by communication be-ing done through limited channels, making it nec-essary to avoid information collision in the nar-row channels.
Multiple chamlels reduce the ne-cessity for conversational regulation, allowing newstyles of conversation to appear.
A new style ofconversation has smaller granularity, is highly in-terruptible, and invokes more spontaneous utter-ances.
Such conversation is (:loser to our daily con-versation with families and friends, and this willfurther increase familiarity with computers.Co-constructive conversation, that is less con-strained by domMns or tasks, is one of our fu-ture goals.
We are extending our conversationalmodel to deal with a new style of human-computerinteraction called social interaction \[Nagao andTakeuchi, 1994\] which includes co-constructiveconversation.
This style of conversation featuresa group of individuMs where, say, those individ-uals talk about the food they ate together in arestraurant a month ago.
There are no specialroles (like the chairperson) for the participants toplay.
They all have the same role.
The conversa-tion terminates only once all the participants aresatisfied with the conclusion.108We are also interested in developing interac-tive characters and stories as an application forinteractive ntertainment.
We are now building aconversational, anthropomorphic computer char-acter that we hope will entertain us with somepleasant stories.ACKNOWLEDGMENTSThe authors would like to thank Mario Tokoro andcolleagues at Sony CSL for their encouragementand helpful advice.
We also extend our thanks toNicole Chovil for her useful comments on a draftof this paper, and Sat0ru Hayamizu, KatunobuItou, and Steve Franks for their contributions tothe implementation of the prototype system.
Spe-ciM thanks go to Keith Waters for granting per-mission to access his original animation system.REFERENCES\[Bolt, 1980\] Richard A. Bolt.
1980.
Put-That-There:Voice and gesture at the graphics interface.
Com-puter Graphics, 14(3):262-270.\[Chovil, 1991\] Nicole Chovil.
1991.
Discourse-orientedfacial displays in conversation.
Research on Lan.guage and Social Interaction, 25:163-194.\[Don et aL, 1991\] Abbe Don, Tim Oren, and BrendaLaurel.
1991.
Guides 3.0.
In Proceedings of ACMCHI'91: Conference on Human Factors in Comput-ing Systems, pages 447-448.
ACM Press.\[Ekmaal and Friesen, 1969\] Paul Ekman and Wal-lace V. Friesen.
1969.
The repertoire of nonverbalbehavior: Categories, origins, usages, and coding.Semiotics, 1:49-98.\[Ekman and Friesen, 1978\] Paul Ekman and Wal-lace V. Friesen.
1978.
Facial Action Coding System..Consulting Psychologists Press, Palo Alto, Califor-nia.\[Ekman and Friesen, 1984\] Paul Ekman and Wal-lace V. Friesen.
1984.
Unmasking the Face.
Con-sulting Psychologists Press, Palo Alto, California.\[Hasida et al, 1993\] K(3iti Hasida, Katashi Nagao,and Takashi Miyata.
1993.
Joint utterance: In-trasentential speaker/hearer switch as an emergentphenomenon.
In Proceedings of the Thirteenth In-ternational Joint Conference on Artificial Intelli-gence (IJCAI-93), pages 1193-1199.
Morgan Kauf-mann Publishers, Inc.\[Itouet al, 1992\] Katunobu Itou, Satoru ttayamizu,and Hozumi Tanaka.
1992.
Continuous speechrecognition by context-dependent phonetic HMMand an efficient algorithm for finding N-best sen-tence hypotheses.
In Proceedings of the Interna-tional Conference on Acoustics, Speech, and SignalProcessing (ICASSP-92), pages 1.21-I.24.
IEEE.\[Nagao and Takeuchi, 1994\] Katashi Nagaoand Akikazu Takeuchi.
1994.
Social interaction:Multimodal conversation with social agents.
In Pro-ceedings of the Twelfth National Conference on Ar-tificial Intelligence (AAAI-9~).
The MIT Press.\[Nagao et al, 1993\] Katashi Nagao, KSiti Hasida,and Takashi Miyata.
1993.
Understanding spokennatural aalguage with omni-directional informationflow.
In Proceedings of the Thirteenth InternationalJoint Conference on Artificial Intelligence (IJCAI-93), pages 1268-1274.
Morgan Kaufmann Publish-ers, Inc.\[Nagao, 1992\] Katashi Nagao.
1992.
A preferentialconstraint satisfaction technique for natural lan-guage analysis.
In Proceedings of the Tenth Euro-pean Conference on Artificial Intelligence (ECAI-92), pages 523-527.
John Wiley & Sons.\[Nagao, 1993\] Katashi Nagao.
1993.
Abduction anddynamic preference in plan-based ialogue under-standing.
In Proceedings of the Thirteenth Inter-national Joint Conference on Artificial Intelligence(IJCAI-93), pages 1186-1192.
Morgan KaufmannPublishers, Inc.\[Neal et al, 1988l Jeannette G. Neal, Zuzana Dobes,Keith E. Bettinger, and Jong S. Byoun.
1988.
Multi-modal references in human-computer dialogue.
InProceedings of the Seventh National Conference onArtificial Intelligence (AAAI-88)~ pages 819-823.Morgan Kaufmann Publishers, Inc.\[Oviatt et al, 1993\] Sharon L. Oviatt, Philip R. Co-hen, and Michelle Wang.
1993.
Reducing linguis-tic variability in speech and handwriting throughselection of presentation format.
In Proceedingsof the International Symposium on Spoken Dia-logue (ISSD- 93), pages 227-230.
Waseda University,Tokyo, Japan.\[Shneiderman, 1983\] Ben Shneiderman.
1983.
Directmanipulation: A step beyond programming lan-guages.
IEEE Computer, 16:57-69.\[Stock, 1991\] Oliviero Stock.
1991.
Natural languageand exploration of an information space: the AL-FRESCO interactive system.
In Proceedings of theTwelfth International Joint Conference on Artifi-cial Intelligence (IJCAI-91), pages 972-978.
Mor-gan Kaufmann Publishers, Inc.\[Suchman, 1987\] Lucy Suchman.
1987.
Plans and Sit-uated Actions.
Cambridge University Press.\[Takeuchi and Franks, 1992\] Akikazu Takeuchi andSteve Franks.
1992.
A rapid face construction lab.Technical Report SCSL-TR-92-010, Sony ComputerScience Laboratory Inc., Tokyo, Japan.\[Takeuchi and Nagao, 1993\] Akikazu Takeuchi andKatashi Nagao.
1993.
Communicative facial dis-plays as a new conversational modality.
In Proceed-ings of ACM/IFIP INTERCHI'93: Conference onHuman Factors in Computing Systems, pages 187-193.
ACM Press.\[Waters, 1987\] Keith Waters.
1987.
A muscle modelfor animating three-dimensional f cial expression.Computer Graphics, 21(4):17-24.109
