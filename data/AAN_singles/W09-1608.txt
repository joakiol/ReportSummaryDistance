Proceedings of CLIAWS3, Third International Cross Lingual Information Access Workshop, pages 53?60,Boulder, Colorado, June 2009. c?2009 Association for Computational LinguisticsAn Approach to Text SummarizationSankar K Sobha LAU-KBC Research Centre AU-KBC Research CentreMIT Campus, Anna University MIT Campus, Anna UniversityChennai- 44.
Chennai- 44.sankar@au-kbc.org sobha@au-kbc.orgAbstractWe propose an efficient text summarizationtechnique that involves two basic opera-tions.
The first operation involves findingcoherent chunks in the document and thesecond operation involves ranking the textin the individual coherent chunks and pick-ing the sentences that rank above a giventhreshold.
The coherent chunks are formedby exploiting the lexical relationship be-tween adjacent sentences in the document.Occurrence of words through repetition orrelatedness by sense relation plays a majorrole in forming a cohesive tie.
The pro-posed text ranking approach is based on agraph theoretic ranking model applied totext summarization task.1 IntroductionAutomated summarization is an important area inNLP research.
A variety of automated summariza-tion schemes have been proposed recently.
NeATS(Lin and Hovy, 2002) is a sentence position, termfrequency, topic signature and term clusteringbased approach and MEAD (Radev et al, 2004) isa centroid based approach.
Iterative graph basedRanking algorithms, such as Kleinberg?s HITSalgorithm (Kleinberg, 1999) and Google?s Page-Rank (Brin and Page, 1998) have been traditionallyand successfully used in web-link analysis, socialnetworks and more recently in text processing ap-plications (Mihalcea and Tarau, 2004), (Mihalceaet al, 2004), (Erkan and Radev, 2004) and (Mihal-cea, 2004).
These iterative approaches have a hightime complexity and are practically slow in dy-namic summarization.
Proposals are also made forcoherence based automated summarization system(Silber and McCoy, 2000).We propose a novel text summarization tech-nique that involves two basic operations, namelyfinding coherent chunks in the document and rank-ing the text in the individual coherent chunksformed.For finding coherent chunks in the document, wepropose a set of rules that identifies the connectionbetween adjacent sentences in the document.
Theconnected sentences that are picked based on therules form coherent chunks in the document.
Fortext ranking, we propose an automatic and unsu-pervised graph based ranking algorithm that givesimproved results when compared to other rankingalgorithms.
The formation of coherent chunksgreatly improves the amount of information of thetext picked for subsequent ranking and hence thequality of text summarization.The proposed text ranking technique employs ahybrid approach involving two phases; the firstphase employs word frequency statistics and thesecond phase involves a word position and stringpattern based weighing algorithm to find theweight of the sentence.
A fast running time isachieved by using a compression hash on each sen-tence.53This paper is organized as follows: section 2discusses lexical cohesion, section 3 discusses thetext ranking algorithm and section 4 describes thesummarization by combining lexical cohesion andsummarization.2 Lexical CohesionCoherence in linguistics makes the text semantical-ly meaningful.
It is achieved through semantic fea-tures such as the use of deictic (a deictic is anexpression which shows the direction.
ex: that,this.
), anaphoric (a referent which requires an ante-cedent in front.
ex: he, she, it.
), cataphoric (a refe-rent which requires an antecedent at the back.
),lexical relation and proper noun repeating elements(Morris and Hirst, 1991).
Robert De Beaugrandeand Wolfgang U. Dressler define coherence as a?continuity of senses?
and ?the mutual access andrelevance within a configuration of concepts andrelations?
(Beaugrande and Dressler, 1981).
Thus atext gives meaning as a result of union of meaningor senses in the text.The coherence cues present in a sentence are di-rectly visible when we go through the flow of thedocument.
Our approach aims to achieve this ob-jective with linguistic and heuristic information.The identification of semantic neighborhood, oc-currence of words through repetition or relatednessby sense relation namely synonyms, hyponyms andhypernym, plays a major role in forming a cohesivetie (Miller et al, 1990).2.1 Rules for finding Coherent chunksWhen parsing through a document, the relationshipamong adjacent sentences is determined by thecontinuity that exists between them.We define the following set of rules to find co-herent chunks in the document.Rule 1The presence of connectives (such as accordingly,again, also, besides) in present sentence indicatesthe connectedness of the present sentence with theprevious sentence.
When such connectives arefound, the adjacent sentences form coherentchunks.Rule 2A 3rd person pronominal in a given sentence refersto the antecedent in the previous sentence, in sucha way that the given sentence gives the completemeaning with respect to the previous sentence.When such adjacent sentences are found, they formcoherent chunks.Rule 3The reappearance of NERs in adjacent sentencesis an indication of connectedness.
When such adja-cent sentences are found, they form coherentchunks.Rule 4An ontology relationship between words acrosssentences can be used to find semantically relatedwords across adjacent sentences that appear in thedocument.
The appearance of related words is anindication of its coherence and hence forms cohe-rent chunks.All the above rules are applied incrementally toachieve the complete set of coherent chunks.2.1.1 Connecting WordThe ACE Corpus was used for studying the cohe-rence patterns between adjacent sentences of thedocument.
From our analysis, we picked out a setof keywords such that, the appearance of thesekeywords at the beginning of the sentence providea strong lexical tie with the previous sentence.The appearance of the keywords ?accordingly,again, also, besides, hence, henceforth, however,incidentally, meanwhile, moreover, namely, never-theless, otherwise, that is, then, therefore, thus,and, but, or, yet, so, once, so that, than, that, till,whenever, whereas and wherever?, at the begin-ning of the present sentence was found to be highlycoherent with the previous sentence.Linguistically a sentence cannot start with theabove words without any related introduction inthe previous sentence.Furthermore, the appearance of the keywords?consequently, finally, furthermore?, at the begin-ning or middle of the present sentence was foundto be highly cohesive with the previous sentence.Example 1541. a The train was late.1.
b However I managed to reach the weddingon time.In Example 1, the connecting word however bindswith the situation of the train being late.Example 21. a The cab driver was late.1.
b The bike tyre was punctured.1.
c The train was late.1 .d Finally, I managed to arrive at the wed-ding on time by calling a cab.Example 31. a The cab driver was late.1.
b The bike tyre was punctured.1.
c The train was late.1.
d I could not wait any more; I finally ma-naged to reach the wedding on time by calling acab.In Example 2, the connecting word finally bindswith the situation of him being delayed.
Similarly,in Example 3, the connecting word finally, thoughit comes in the middle of the sentence, it still bindswith the situation of him being delayed.2.1.2 PronominalsIn this approach we have a set of pronominalswhich establishes coherence in the text.
From ouranalysis, it was observed that if the pronominals?he, she, it, they, her, his, hers, its, their, theirs?,appear in the present sentence; its antecedent maybe in the same or previous sentence.It is also found that if the pronominal is not pos-sessive (i.e.
the antecedent appears in the previoussentence or previous clause), then the present sen-tence and the previous sentences are connected.However, if the pronominal is possessive then itbehaves like reflexives such as ?himself?, ?herself?which has subject as its antecedent.
Hence the pos-sibility of connecting it with the previous sentenceis very unlikely.
Though pronominal resolutioncannot be done at a window size of 2 alone, stillwe are looking at window size 2 alone to pickguaranteed connected sentences.Example 41. a Ravi is a good boy.1.
b He always speaks the truth.In Example 4, the pronominal he in the second sen-tence refers to the antecedent Ravi in the first sen-tence.Example 51. a He is the one who got the first prize.In example 5 the pronominal he is possessive andit doesn?t need an antecedent to convey the mean-ing.2.1.3 NERs ReappearanceTwo adjacent sentences are said to be coherentwhen both the sentences contain one or more reap-pearing nouns.Example 61. a Ravi is a good boy.1.
b Ravi scored good marks in exams.Example 71. a The car race starts at noon.1.
b Any car is allowed to participate.Example 6 and Example 7 demonstrates the cohe-rence between the two sentences through reappear-ing nouns.2.1.4 Thesaurus RelationshipsWordNet covers most of the sense relationships.To find the semantic neighborhood between adja-cent sentences, most of the lexical relationshipssuch as synonyms, hyponyms, hypernyms, mero-nyms, holonyms and gradation can be used (Fell-baum 1998).
Hence, semantically related terms arecaptured through this process.Example 81. a The bicycle has two wheels.1.
b The wheels provide speed and stability.In Example 8, bicycle and wheels are relatedthrough bicycle is the holonym of wheels.2.2 Coherence Finding AlgorithmThe algorithm is carried out in four phases.
Initial-ly, each of the 4 cohesion rules is individually ap-plied over the given document to give coherentchunks.
Next, the coherent chunks obtained in each55phases are merged together to give the global cohe-rent chunks in the document..Figure 1: Flow of Coherence chunkerFigure 1, shows the flow and rule positions in thecoherence chunk identification module.2.3 EvaluationOne way to evaluate the coherence finding algo-rithm is to compare against human judgmentsmade by readers, evaluating against text premarked by authors and to see the improved resultin the computational task.
In this paper we will seethe computational method to see the improved re-sult.3 Text RankingThe proposed graph based text ranking algorithmconsists of three steps: (1) Word Frequency Analy-sis; (2) A word positional and string pattern basedweight calculation algorithm; (3) Ranking the sen-tences by normalizing the results of step (1) and(2).The algorithm is carried out in two phases.
Theweight metric obtained at the end of each phase isaveraged to obtain the final weight metric.
Sen-tences are sorted in non ascending order of weight.3.1 GraphLet G (V, E) be a weighted undirected completegraph, where V is set of vertices and E is set ofweighted edges.S1S2S3S6S5S4Figure 2: A complete undirected graphIn figure 2, the vertices in graph G represent the setof all sentences in the given document.
Each sen-tence in G is related to every other sentencethrough the set of weighted edges in the completegraph.3.2 Phase 1Let the set of all sentences in document S= {si | 1 ?i ?
n}, where n is the number of sentences in S.The sentence weight (SW) for each sentence is cal-culated by average affinity weight of words in it.For a sentence si= {wj | 1 ?
j ?
mi} where mi is thenumber of words in sentence si, (1 ?
i ?
n) the af-finity weight AW of a word wj is calculated as fol-lows:( , )( )( )j kkjIsEqual w ww SAW wWC S?
?=?
(1)where S is the set of all sentences in the givendocument, wk is a word in S, WC (S) is the totalnumber of words in S and function IsEqual(x, y)returns an integer count 1 if x and y are equal elseinteger count 0 is returned by the function.Input TextConnecting WordPossessive PronounNoun ReappearanceCoherent ChunksThesaurus Relationships56Next, we find the sentence weight SW (si) ofeach sentence si (1 ?
i ?
n) as follows:1( ) ( )i ji j iSW s AW wm w s=?
??
(2)At the end of phase 1, the graph vertices holdthe sentence weight as illustrated in figure 4.Figure 2: Sample text taken for the rankingprocess.Figure 4: Sample graph of Sentence weight calcu-lation in phase 13.3 Compression hashA fast compression hash function over word w isgiven as follows:H (w) = (c1ak-1+c2ak-2 +c3ak-3+...+cka0) mod p    (3)where w= {c1, c2, c3 ... ck} is the ordered set ofASCII equivalents of alphabets in w and k the totalnumber of alphabets in w. The choice of a=2 per-mits the exponentiations and term wise multiplica-tions in equation 3 to be binary shift operations ona micro processor, thereby speeding up the hashcomputation over the text.
Any lexicographicallyordered bijective map from character to integermay be used to generate set w. The recommenda-tion to use ASCII equivalents is solely for imple-mentation convenience.
Set p = 26 (for English), tocover the sample space of the set of alphabets un-der consideration.Compute H (w) for each word in sentence si toobtain the hashed set1 2( ) { ( ), ( )... ( )}ii mH s H w H w H w=             (4)Next, invert each element in the set H (si) backto its ASCII equivalent to obtain the set1 2?
?
?
?
?
( ) { ( ), ( )... ( )}ii mH s H c H c H c=                (5)Then, concatenate the elements in set ?
iH(s )  toobtain the string ?is ; where ?is  is the compressedrepresentation of sentence si.
The hash operationsare carried out to reduce the computational com-plexity in phase 2, by compressing the sentencesand at the same time retaining their structuralproperties, specifically word frequency, word posi-tion and sentence patterns.3.4 Levenshtein DistanceLevenshtein distance (LD) between two stringsstring1 and string2 is a metric that is used to findthe number of operations required to convertstring1 to string2 or vice versa; where the set ofpossible operations on the character is insertion,deletion, or substitution.The LD algorithm is illustrated by the followingexampleLD (ROLL, ROLE) is 1LD (SATURDAY, SUNDAY) is 3[1]"The whole show is dreadful," she cried, com-ing out of the menagerie of M.
Martin.
[2]She had just been looking at that daring specu-lator "working with his hyena" to speak in thestyle of the program.
[3]"By what means," she continued, "can he havetamed these animals to such a point as to be cer-tain of their affection for.
"[4]"What seems to you a problem," said I, inter-rupting, "is really quite natural."[5]"Oh!"
she cried, letting an incredulous smilewander over her lips.
[6]"You think that beasts are wholly without pas-sions?"
Quite the reverse; we can communicate tothem all the vices arising in our own state of civi-lization.573.5 Levenshtein Similarity WeightConsider two strings, string1 and string2 where ls1is the length of string1 and ls2 be the length ofstring2.
Compute MaxLen=maximum (ls1, ls2).Then LSW between string1 and string2 is the dif-ference between MaxLen and LD, divided by Max-Len.
Clearly, LSW lies in the interval 0 to 1.
In caseof a perfect match between two words, its LSW is 1and in case of a total mismatch, its LSW is 0.
In allother cases, 0 < LSW <1.
The LSW metric is illu-strated by the following example.LSW (ABC, ABC) =1LSW (ABC, XYZ) =0LSW (ABCD, EFD) =0.25Hence, to find the Levenshtein similarityweight, first find the Levenshtein distance LD us-ing which LSW is calculated by the equation?
?
?
?
( , ) ( , )?
?
( , )?
?
( , )i j i ji ji jMaxLen s s LD s sLSW s sMaxLen s s?=       (6)where, ?is and j?s are the concatenated string out-puts of equation 5.3.6 Phase 2Let S = {si | 1 ?
i ?
n} be the set of all sentences inthe given document; where n is the number of sen-tences in S. Further, si = {wj | 1 ?
j ?
m}, where mis the number of words in sentence si.Figure 5: Sample graph for Sentence weight calcu-lation in phase 2is S ?
?
,find 1 2?
?
?
?
?
( ) { ( ), ( )... ( )}ii mH s H c H c H c=using equation 3 and 4.
Then, concatenate the ele-ments in set ?
iH(s )  to obtain the string ?is ; where ?isis the compressed representation of sentence si.Each string ?is ; 1 ?
i ?
n is represented as thevertex of the complete graph as in figure 5and ?
i?S={s |1 i n}?
?
.
For the graph in figure 5,find the Levenshtein similarity weight LSW be-tween every vertex using equation 6.
Find vertexweight (VW) for each string i?s ; 1 ?
l ?
n by1?
?
?
( ) ( , )??
l?l l iiVW s LSW s sns s S=?
?
??
(7)4 Text RankingThe rank of sentence si; 1 ?
i ?
n is computed as?
( ) ( )( ) ;12i iiSW s VW sRank s i n+= ?
?
(8)where, ( )iSW s  is calculated by equation 2 ofphase 1 and ?
( )iVW s  is found using equation 7 ofphase 2.
Arrange the sentences si; 1 ?
i ?
n, in nonincreasing order of their ranks.
( )iSW s  in phase 1 holds the sentence affinity interms of word frequency and is used to determinethe significance of the sentence in the overall rak-ing scheme.
?
( )iVW s  in phase 2 helps in the overallranking by determining largest common subse-quences and other smaller subsequences then as-signing weights to it using LSW.
Further, sincenamed entities are represented as strings, repeatedoccurrences are weighed efficiently by LSW, the-reby giving it a relevant ranking position.5 SummarizationSummarization is done by applying text rankingover the global coherent chunks in the document.The sentences whose weight is above the thresholdis picked and rearranged in the order in which thesentences appeared in the original document.586 EvaluationThe ROUGE evaluation toolkit is employed toevaluate the proposed algorithm.
ROUGE, an au-tomated summarization evaluation package basedon Ngram statistics, is found to be highly corre-lated with human evaluations (Lin and Hovy,2003a).The evaluations are reported in ROUGE-1 me-trics, which seeks unigram matches between thegenerated and the reference summaries.
TheROUGE-1 metric is found to have high correlationwith human judgments at a 95% confidence leveland hence used for evaluation.
(Mihalcea and Ta-rau, 2004) a graph based ranking model withRouge score 0.4904, (Mihalcea, 2004) Graph-based Ranking Algorithms for Sentence Extrac-tion, Applied to Text Summarization with Rougescore 0.5023.Table 1 shows the ROUGE Score of 567 newsarticles provided during the Document Under-standing Evaluations 2002(DUC, 2002) using theproposed algorithm without the inclusion of cohe-rence chunker module.Table 2 shows the ROUGE Score of 567 newsarticles provided during the Document Under-standing Evaluations 2002(DUC, 2002) using theproposed algorithm after the inclusion of cohe-rence chunker module.Comparatively Table 2, which is the theROUGE score for summary including the cohe-rence chunker module gives better result.7 Related WorkText extraction is considered to be the importantand foremost process in summarization.
Intuitive-ly, a hash based approach to graph based rankingalgorithm for text ranking works well on the taskof extractive summarization.
A notable study re-port on usefulness and limitations of automaticsentence extraction is reported in (Lin and Hovy,2003b), which emphasizes the need for efficientalgorithms for sentence ranking and summariza-tion.8 ConclusionsIn this paper, we propose a coherence chunkermodule and a hash based approach to graph basedranking algorithm for text ranking.
In specific, wepropose a novel approach for graph based textranking, with improved results comparative to ex-isting ranking algorithms.
The architecture of thealgorithm helps the ranking process to be done in atime efficient way.
This approach succeeds ingrabbing the coherent sentences based on the lin-guistic and heuristic rules; whereas other super-vised ranking systems do this process by trainingthe summary collection.
This makes the proposedalgorithm highly portable to other domains andlanguages.ReferencesACE Corpus.
NIST 2008 Automatic Content ExtractionEvaluation(ACE08).http://www.itl.nist.gov/iad/mig/tests/ace/2008/Brin and L. Page.
1998.
The anatomy of a large scalehypertextualWeb search engine.
Computer Networksand ISDN Systems, 30 (1 ?
7).Erkan and D. Radev.
2004.
Lexpagerank: Prestige inmulti document text summarization.
In Proceedingsof the Conference on Empirical Methods in NaturalLanguage Processing, Barcelona, Spain, July.Fellbaum, C., ed.
WordNet: An electronic lexical data-base.
MIT Press, Cambridge (1998).Kleinberg.
1999.
Authoritative sources in a hyper-linked environment.
Journal of the ACM, 46(5):604-632.ROUGE-1ROUGE-L0.53120.4978ScoreROUGE-1ROUGE-L0.51030.4863ScoreTable 1: ROUGE Score for the news articlesummarization task without coherencechunker, calculated across 567 articles.Table 2: ROUGE Score for the news articlesummarization task with coherence chunker,calculated across 567 articles.59Lin and E.H. Hovy.
From Single to Multi documentSummarization: A Prototype System and its Evalua-tion.
In Proceedings of ACL-2002.Lin and E.H. Hovy.
2003a.
Automatic evaluation ofsummaries using n-gram co-occurrence statistics.
InProceedings of Human Language Technology Confe-rence (HLT-NAACL 2003), Edmonton, Canada, May.Lin and E.H. Hovy.
2003b.
The potential and limitationsof sentence extraction for summarization.
In Pro-ceedings of the HLT/NAACL Workshop on AutomaticSummarization, Edmonton, Canada, May.Mihalcea.
2004.
Graph-based ranking algorithms forsentence extraction, applied to text summarization.
InProceedings of the 42nd Annual Meeting of the Asso-ciation for Computational Linguistics (ACL 2004)(companion volume), Barcelona, Spain.Mihalcea and P. Tarau.
2004.
TextRank - bringing orderinto texts.
In Proceedings of the Conference on Em-pirical Methods in Natural Language Processing(EMNLP 2004), Barcelona, Spain.Mihalcea, P. Tarau, and E. Figa.
2004.
PageRank onsemantic networks, with application to word sensedisambiguation.
In Proceedings of the 20th Interna-tional Conference on Computational Linguistics(COLING 2004), Geneva, Switzerland.Miller, G. A., Beckwith, R., Fellbaum, C., Gross, D.,Miller, K. J.
Introduction to WordNet: An on-linelexical database.
Journal of Lexicography (1990).Morris, J., Hirst, G. Lexical cohesion computed by the-saural relations as an indicator of the structure oftext.
Computational Linguistics (1991).Radev, H. Y. Jing, M. Stys and D. Tam.
Centroid-basedsummarization of multiple documents.
InformationProcessing and Management, 40: 919-938, 2004.Robert de Beaugrande and Wolfgang Dressler.
Intro-duction to Text Linguistics.
Longman, 1981.Silber, H. G., McCoy, K. F. Efficient text summariza-tion using lexical chains.
In Proceedings of Intelli-gent User Interfaces.
(2000).60
