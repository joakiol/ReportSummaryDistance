Jumping Distance based Chinese Person Name Disambiguation1Yu Hong  Fei Pei  Yue-hui Yang  Jian-min Yao  Qiao-ming ZhuSchool of Computer Science and Technology, Soochow UniversityNo.1 Shizi street, Suzhou City, Jiansu Province, China{hongy, 20094527004, 0727401137, jyao, qmzhu}@suda.edu.cnAbstractIn this paper, we describe a Chinese personname disambiguation system for news articlesand report the results obtained on the data set ofthe CLP 2010 Bakeoff-31.
The main task of theBakeoff is to identify different persons from thenews stories that contain the same person-namestring.
Compared to the traditional methods,two additional features are used in our system:1) n-grams co-occurred with target name string;2) Jumping distance among the n-grams.
On thebasis, we propose a two-stage clustering algo-rithm to improve the low recall.1   Our Novel TryFor this task, we propose a Jumping-Distancebased n-gram model (abbr.
DJ n-gram) to de-scribe the semantics of the closest contexts ofthe target person-name strings.The generation of the DJ n-gram modelmainly involves two steps.
First, we mine theJumping tree for the target string; second, wegive the statistical description of the tree.z Jumping TreeGiven a target string, we firstly extract thesentence where it locates as its closest context.Then we segment the sentence into n-grams(Chen et al ,2009) (only Bi-gram and Tri-gram are used in this paper).
For each n-gram,we regard it as the beginning of a jumping jour-ney.
And the places where we jump are the sen-tences which involve the n-gram.
By the sameway, we segment the sentences into n-gramswhich will be regarded as the new beginnings toopen further jumping.
The procedure will runiteratively until there are no sentences in thedocument (viz.
the document which involvesthe target string) can be used to jump.
Actually,we find there are only 3 jumps in average in ourprevious test and simultaneously 11 sentencesin a document can be involved into the jumpingjourney.
Thus, we can obtain a Jumping Treewhere each jumping route from the initially n-gram (viz.
the gram in the closes context) referto a branch.
And for each intermediate node, itschild-nodes are the n-grams co-occurred with itin the same sentences.The motivation to generate the Jumping Treeis to imitate the thinking model of human rec-ognizing the word senses and semantics.
In de-tail, for each intermediate node of the tree, itschild-nodes all come from its closest contexts,especially the nodes co-occur with it in thesame sentences which involve the real grammarand semantic relations.
Thus the child-nodesnormally provide the natural inference for itsword sense.
For example, given the string?SARS?, we can deduce its sense from its childnodes ?Severe?, ?Acute?, ?Respiratory?
and?Syndromes?
even if we see the string for thefirst time.
On the basis, the procedure of infer-ence run iteratively, that is, the tree always usethe child nodes deduce the meaning of their fa-ther nodes then further ancestor nodes until theroot.
Thus the tree acts as a hierarchical under-standing procedure.
Additionally, the distancesamong nodes in the tree give the degree of se-mantic relation.In the task of person-name disambiguation,we use the Jumping Tree to deduce the identi-ties and backgrounds of a person.
Each branchof the tree refers to a property of the person.z Jumping-Distance based n-gram modelIn this paper, we give a simple statisticalmodel to describe the Jumping Tree.
Given anode in the tree (viz.
an n-gram), we record the Supported by the National Natural Science Foundationof China under Grant No.
60970057, No.60873105.steps jumping from the root to it, viz.
the depthof the node in the tree.
Then based on the priori-trained TFIDF value, we calculate the genera-tion probability of the node as follows:depthTFP D?where the D  denotes the smoothing factor.In fact, we create more comprehensive mod-els to describe the semantic correlations amongthe nodes in the Jumping Tree.
The models welluse the distances among the nodes in localJumping Tree (viz.
the tree generated based onthe test document) and that normalized on thelarge-scale training data to calculate the prob-ability of n-grams correboratively generate asemantics.
They try to imitate the thinkingmodel of human combine differents features tounderstand panoramic knowledge.
In the task ofname disambiguation, we can use the models toimprove the distinguishment of different per-sons who have the same name.
And we haveillustrate the well effectiveness on the topic de-scription and relevance measurement in othertasks, such as Link Detection.
But we actuallydidn?t use the models to perform the task ofname diaambiguation this time with the aim topurely evaluate the usefulness of the JumpingTree.2    SystemsFor the task of Chinese person name disam-biguation, we submitted two systems as follows:z System1The system involves two main components:DJ-based name Identification error detectionand DJ-based person name disambiguation.The first component, viz.
DJ-based namesegmentation error detection, aims to distin-guish the target string referring to person namefrom that referring to something else.
Such as,the string ????
can be a person name ?HaiHuang?
but also a name of sea ?the YellowSea?.
And the detection component focuses onobtaining the pure person name ?Hai Huang?.The detection component firstly establish twoclasses of features which respectively describethe nature of human and that of things.
Such as,the features ?professor?, ?research?, ?honest?
etal., can roughly be determined as the nature ofhuman, and conversely the features ?solid?,?collapse?, ?deep?
et al, can be that of things.For obtaining the features, we extract 10,000documents that discuss person, eg.
?Albert Ein-stein?
and 6000 documents that discuss tech-nology, science, geography, et.al., fromWikipedia2.
For each document, we generate itsJumping Tree, and regard the nodes in the treeas the features.
After that, we combine theweights of the same features and normalized thevalue by dividing that by the average weight inthe specific class of features.Based on the two classes of features, given atarget string and the document where it occurs,the detection component firstly generate theJumping Tree of the document, and then deter-mines whether the string is person name orthings by measuring the similarity of  the tree tothe classes of features.
Here, we simply use theVSM and Cosine metric ?Bagga and Baldwin,1998?
to obtain the similarity.The second component, viz.
DJ-based personname disambiguation, firstly generates theJumping trees for all documents that involvespecific person name.
And a two-stage cluster-ing algorithm is adopted to divide the docu-ments and refer each cluster to a person.
Thefirst stage of the algorithm runs a strict divisionwhich focuses on obtaining high precision.
Thesecond stage performs a soft division which isused to improve recall.
The two-stage clusteringalgorithm(Ikeda et al,2009) initially obtains theoptimal parameters that respectively refer to themaximum precision and recall based on trainingdata, and then regards a statistical tradeoff asthe final value of the parameters.
Here, the Af-finity Propagation clustering tools (Frey BJ andDueck D, 2007) is in use.z System2The system is similar to the system1 exceptthat it additionally involve Named Entity Identi-fication (Artiles et.al,2009B; Popescu,O.
andMagnini, B.,2007)before the two-stage cluster-ing in the component of person name disam-biguation.
In detail, given a person name andthe documents that it occurs in, the disambigua-tion component of System2 firstly adopt NERCRF++ toolkit3  provided by MSRA to identifyNamed Entities(Chen et al, 2006) that involvethe given name string, such as the entity ?????
(viz.
Gao-ming Li in English) when giventhe target name string ????(viz.
Ming Gao inEnglish).
Thus the documents can be roughlydivided into different clusters of Named Entitieswithout name segmentation errors.
After that,we additionally adopt the two-stage clusteringalgorithm to further divide each cluster.
Thuswe can deal with the issue of disambiguationwithout the interruption of name segmentationerrors.3   Data setsz Training dataset: They contain about 30Chinese personal names, and a document set ofabout 100-300 news articles from collection ofXinhua news documents in a time span of four-teen years are provided for each personal name.z External dataset: Chinese Wikipedia2 per-sonal attribution (Cucerzan, 2007; Nguyen andCao,2008).z Test dataset: There are about 26 Chinesepersonal names, which are similar to train datasets.4     ExperimentsThe systems that run on test dataset are evalu-ated by both B-Cubed (Bagga and  Baldwin,1998; Artiles et al,2009A) and P-IP (Artiles  etal., 2007 ;Artiles et al,2009A).
And the systemsthat run on training dataset were only evaluatedby B-Cubed.In experiments, we firstly evaluate the per-formance of name segmentation error detectionon the training dataset.
For comparison, we ad-ditionally perform another detection methodwhich only using Name Entity Identifcation(NER CRF++ tools) to distinguish name-stringsfrom the discarded ones.
The results are shownin table 1.
We can find that our error detectionmethod can achieve more recall than NER, butlower precision.Besides, we evaluate the performance of thetwo-stage clustering in the component of namedisambiguation step by step.
Four steps are inuse to evaluate the first-stage clustering methodas follows:z DJ2This step look like to run the system1 men-tionedin in section 3 which don?t involve theprior-division of documents by using NER be-fore the first-stage clustering in the componentof name disambiguation.
Especially it don?tperform the second-stage clustering to improvethe recall probability.z DJ2+NERThis step is similar to the step of DJ2 men-tioned above except that it perform the prior-divison of documents by using NER.z NER+DJThis step is also similar to the step of DJ2 ex-cept that its name segmentation error detectionperforms by using the NER.z NER2+DJThis step is similar to the step of NER+DJexcept that it involve the treatment of prior-divison as that in DJ2+NER.The performances of the four steps are shownin table 2.
We can find that all steps achievepoor recall.
And the step of DJ2 achieve the bestF-score although it don?t involve the prior-division.
That is because NER is helpful to im-prove precision but not recall, as shown in table1.
Conversely, DJ2 can avoid the bias caused bythe procedure of greatly maximizing the preci-sion.P recall F-scoreDJ-based 0.62 0.81 0.70NER-based 0.91 0.77 0.71Table 1: Performance of name segmentationerror detectionP IP F-scoreDJ2 80.49 53.85 60.12DJ2+NER 88.56 51.30 59.02NER+DJ 93.27 46.78 57.44NER2+DJ 97.79 42.13 55.47Table 2: Performances of the-stage clusteringAdditionally, another two steps are used toevluate the both two stages of clustering inname disambiguation.
The steps are as follows:z DJ2+NER_2This step is similar to the step of DJ2+NERexcept that it additionally run the second-stageclustering to improve recall.z NER2+DJ_2This step also run the second-stage clusteringon the basis of NER2+DJ.The performances of the two step are shownin table 3.
We can find that the F-scores bothhave been improved substantially.
And the twosteps still maintain the original distribution be-tween precision and recall.
That is, theDJ2+NER_2, which has outperformance on re-call in the name segmentation error detection,still maintain the higher recall at the second-stage clustering.
And NER2+DJ_2 also main-tains higher precision.
This illustrates that theclustering has no ability to remedy the short-comings of NER in the prior-division.P IP F-scoreDJ2+NER_2 82.65 63.40    66.59NER2+DJ_2 87.71 60.45 66.23Table 3: Performances of two-stage clusteringThe test results of the two systems mentioned insection 3 are shown in the table 4.
We alsoshow the performances of each stage clusteringas that on training dataset.
We can find that thepoor performance mainly come from the lowrecall, which illustrates that the DJ-based n-gram disambiguation is not robust.B-Cubedprecision recall F-ScoreSystem1(onet )85.26 28.43 37.74System1(botht )84.51 44.17 51.42P-IPP IP F-ScoreSystem2(onet )88.4 39.47 50.52System2(botht )88.36 55.23 63.89Table 4 :Test results5.ConclusionsIn this paper, we report a hybrid Chinese per-sonal disambiguation system and a novel algo-rithm for extract useful global n-gram featuresfrom the context .Experiment showed that ouralgorithm performed high precision and poorrecall.
Furthermore, two-stage clustering canhandl a change in the one-stage clustering algo-rithm, especially for recall score.
In the future,we will investigate global new types of featuresto improve the recall score and local new typesof features to improve the precision score.
Forinstance, the location and organization besidesthe person in the named-entities.
And we try touse Hierarchical Agglomerative Clustering al-gorithm to help raise the recall score.ReferencesArtiles J, J Gonzalo and S Sekine.
2007.
TheSemEval-2007 WePS Evaluation: ?Establish-ing a benchmark for the Web People SearchTask.
?, The SemEval-2007, 64-69, Associa-tion for Computational Linguistics.Artiles Javier, Julio Gonzalo and Satoshi Se-kine.2009A.
?WePS 2 Evaluation Campaign:overview of the Web People Search Cluster-ing Task,?
In 2nd Web People SearchEvaluation Workshop (WePS 2009), 18thWWW Conference.Artiles J, E Amig?o and J Gonzalo.
2009B.TheRole of Named Entities in Web PeopleSearch.
Proceedings of the 2009 Conferenceon Empirical Methods Natural LanguageProcessing, 534?542,Singapore, August 2009.Bagga A and Baldwin B.
1998.
Entity-basedcross-document coreferenceing using theVector Space Model.Proceedings of the 17thinternational conference on computationallinguistics.
Volume 1, 79-85.Chen,Ying., Sophia Yat., Mei Lee and Chu-RenHuang.
2009.
PolyUHK:A Roubust Informa-tion Extraction System for Web PersonalNames In 2nd Web People Search EvaluationWorkshop (WePS 2009), 18th WWW Con-ference.Chen Wen-liang, Zhang Yu-jie.
2006.
ChineseNamed Entity Recognition with ConditionalRandom Fields.
Proceedings of the FifthSIGHAN Workshop on Chinese LanguageProcessing.Cucerzan, Silviu.
2007.
Large scale named en-tity Disambiguation based on Wikipedia data.In The EMNLP-CoNLL-2007.Frey BJ and Dueck D. 2007.
Clustering byPassing Messages Between DataPoints .science, 2007 - sciencemag.org.Ikeda MS, Ono I, Sato MY and Nakagawa H.2009.
Person Name disambiguation on theWeb by Two-Stage Clustering.
In 2nd WebPeople Search Evaluation Workshop(WePS2009),18th WWW Conference.Popescu,O and Magnini, B.
2007.
IRST-BP:Web People Search Using Name Enti-ties.Proceeding s of the 4th InternationalWorkshop on Semantic Evaluations (SemE-val-2007), 195-198, Prague June 2007.
Asso-ciation for Computational Linguistics.
