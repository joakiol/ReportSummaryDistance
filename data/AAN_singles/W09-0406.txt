Proceedings of the Fourth Workshop on Statistical Machine Translation , pages 47?50,Athens, Greece, 30 March ?
31 March 2009. c?2009 Association for Computational LinguisticsCMU System Combination for WMT?09Almut Silja HildebrandCarnegie Mellon UniversityPittsburgh, USAsilja@cs.cmu.eduStephan VogelCarnegie Mellon UniversityPittsburgh, USAvogel@cs.cmu.eduAbstractThis paper describes the CMU entry forthe system combination shared task atWMT?09.
Our combination method is hy-pothesis selection, which uses informationfrom n-best lists from several MT systems.The sentence level features are indepen-dent from the MT systems involved.
Tocompensate for various n-best list sizes inthe workshop shared task including first-best-only entries, we normalize one of ourhigh-impact features for varying sub-listsize.
We combined restricted data trackentries in French - English, German - En-glish and Hungarian - English using pro-vided data only.1 IntroductionFor the combination of machine translation sys-tems there have been two main approaches de-scribed in recent publications.
One uses confusionnetwork decoding to combine translation systemsas described in (Rosti et al, 2008) and (Karakos etal., 2008).
The other approach selects whole hy-potheses from a combined n-best list (Hildebrandand Vogel, 2008).Our setup follows the approach described in(Hildebrand and Vogel, 2008).
We combine theoutput from the available translation systems intoone joint n-best list, then calculate a set of fea-tures consistently for all hypotheses.
We use MERtraining on a development set to determine featureweights and re-rank the joint n-best list.2 FeaturesFor our entries to the WMT?09 we used the fol-lowing feature groups:?
Language model score?
Word lexicon scores?
Sentence length features?
Rank feature?
Normalized n-gram agreementThe details on language model and word lexi-con scores can be found in (Hildebrand and Vogel,2008).
We use two sentence length features, whichare the ratio of the hypothesis length to the lengthof the source sentence and the difference betweenthe hypothesis length and the average length ofthe hypotheses in the n-best list for the respec-tive source sentence.
We also use the rank of thehypothesis in the original system?s n-best list as afeature.2.1 Normalized N-gram AgreementThe participants of the WMT?09 shared transla-tion task provided output from their translationsystems in various sizes.
Most submission were1st-best translation only, some submitted 10-bestup to 300-best lists.In preliminary experiments we saw that addinga high scoring 1st-best translation to a joint n-bestlist composed of several larger n-best lists does notyield the desired improvement.
This might be dueto the fact, that hypotheses within an n-best listoriginating from one single system (sub-list) tendto be much more similar to each other than to hy-potheses from another system.
This leads to hy-potheses from larger sub-lists scoring higher in then-best list based features, e.g.
because they collectmore n-gram matches within their sub-list, which?supports?
them the more the larger it is.Previous experiments on Chinese-Englishshowed, that the two feature groups with thehighest impact on the combination result are thelanguage model and the n-best list based n-gramagreement.
Therefore we decided to focus on then-best list n-gram agreement for exploring sub-list47size normalization to adapt to the data situationwith various n-best list sizes.The n-gram agreement score of each n-gram inthe target sentence is the relative frequency of tar-get sentences in the n-best list for one source sen-tence that contain the n-gram e, independent ofthe position of the n-gram in the sentence.
Thisfeature represents the percentage of the transla-tion hypotheses, which contain the respective n-gram.
If a hypothesis contains an n-gram morethan once, it is only counted once, hence the max-imum for the agreement score a(e) is 1.0 (100%).The agreement score a(e) for each n-gram e is:a(e) =CL(1)where C is the count of the hypotheses containingthe n-gram and L is the size of the n-best list forthis source sentence.To compensate for the various n-best list sizesprovided to us we modified the n-best list n-gramagreement by normalizing the count of hypothesesthat contain the n-gram by the size of the sub-listit came from.
It can be viewed as either collectingfractional counts for each n-gram match, or as cal-culating the n-gram agreement percentage for eachsub-list and then interpolating them.
The normal-ized n-gram agreement score anorm(e) for each n-gram e is:anorm(e) =1PP?j=1CjLj(2)where P is the number of systems, Cj is the countof the hypotheses containing the n-gram e in thesublist pj and Lj is the size of the sublist pj .For the extreme case of a sub-list size of onethe fact of finding an n-gram in that hypothesisor not has a rather strong impact on the normal-ized agreement score.
Therefore we introduce asmoothing factor ?
in a way that it has an increas-ing influence the smaller the sub-list is:asmooth(e) =1PP?j=1[CjLj(1?
?Lj)]+[Lj ?
CjLj?Lj] (3)where P is the number of systems, Cj is the countof the hypotheses containing the n-gram in thesublist pj and Lj is the size of the sublist pj .
Weused an initial value of ?
= 0.1 for our experi-ments.In all three cases the score for the whole hypoth-esis is the sum over the word scores normalizedby the sentence length.
We use n-gram lengthsn = 1..6 as six separate features.3 Preliminary ExperimentsArabic-EnglishFor the development of the modification on the n-best list n-gram agreement feature we used n-bestlists from three large scale Arabic to English trans-lation systems.
We evaluate using the case insen-sitive BLEU score for the MT08 test set with fourreferences, which was unseen data for the individ-ual systems as well as the system combination.
Ta-ble 1 shows the initial scores of the three input sys-tems.system MT08A 47.47B 46.33C 44.42Table 1: Arabic-English Baselines: BLEUTo compare the behavior of the combinationresult for different n-best list sizes we combinedthe 100-best lists from systems A and C and thenadded three n-best list sizes from the middle sys-tem B into the combination: 1-best, 10-best andfull 100-best.
For each of these four combinationoptions we ran the hypothesis selection using theplain version of the n-gram agreement feature a aswell as the normalized version without anorm andwith smoothing asmooth .combination a anorm asmoothA & C 48.04 48.09 48.13A & C & B1 47.84 48.34 48.21A & C & B10 48.29 48.33 48.47A & C & B100 48.91 48.95 49.02Table 2: Combination results: BLEU on MT08The modified feature has as expected no impacton the combination of n-best lists of the same size(see Table 2), however it shows an improvementof BLEU +0.5 for the combination with the 1st-best from system B.
The smoothing seems to haveno significant impact for this dataset, but differ-ent smoothing factors will be investigated in thefuture.484 Workshop ResultsTo train our language models and word lexicawe only used provided data.
Therefore we ex-cluded systems from the combination, which wereto our knowledge using unrestricted training data(google).
We did not include any contrastive sys-tems.We trained the statistical word lexica on the par-allel data provided for each language pair1.
Foreach combination we used two language models,a 1.2 giga-word 3-gram language model, trainedon the provided monolingual English data and a 4-gram language model trained on the English partof the parallel training data of the respective lan-guages.
We used the SRILM toolkit (Stolcke,2002) for training.For each of the three language pairs we submit-ted a combination that used the plain version of then-gram agreement feature as well as one using thenormalized smoothed version.The provided system combination developmentset, which we used for tuning our feature weights,was the same for all language pairs, 502 sentenceswith only one reference.For combination we tokenized and lowercasedall data, because the n-best lists were submittedin various formats.
Therefore we report the caseinsensitive scores here.
The combination was op-timized toward the BLEU metric, therefore resultsfor TER and METEOR are not very meaningfulhere and only reported for completeness.4.1 French-English14 systems were submitted to the restricted datatrack for the French-English translation task.
Thescores on the combination development set rangefrom BLEU 27.56 to 15.09 (case insensitive eval-uation).We received n-best lists from five systems, a300-best, a 200-best two 100-best and one 10-bestlist.
We included up to 100 hypotheses per systemin our joint n-best list.For our workshop submission we combined thetop nine systems with the last system scoring24.23 as well as all 14 systems.
Comparing theresults for the two combinations of all 14 systems(see Table 3), the one with the sub-list normaliza-tion for the n-gram agreement feature gains +0.81http://www.statmt.org/wmt09/translation-task.html#trainingBLEU on unseen data compared to the one with-out normalization.system dev test TER Meteorbest single 27.56 26.88 56.32 52.68top 9 asmooth 29.85 28.07 55.23 53.90all 14 asmooth 30.39 28.46 55.12 54.35all 14 29.49 27.65 55.41 53.74Table 3: French-English Results: BLEUOur system combination via hypothesis selec-tion could improve the translation quality by +1.6BLEU on the unseen test set compared to the bestsingle system.A  177 B* 434 C  104177 434 1040%10%20%30%40%50%60%70%80%90%100% N     7M   18L    16K    12J    10I*  264H    41G  110F* 423E* 584D* 562C  104B* 434A  177*****Figure 1: Contributions of the individual systemsto the final translation.Figure 1 shows, how many hypotheses werecontributed by the individual systems to the fi-nal translation (unseen data).
The systems A toN are ordered by their BLEU score on the devel-opment set.
The systems which provided n-bestlists, marked with a star in the diagram, clearlydominate the selection.
The low scoring systemscontribute very little as expected.4.2 German-English14 systems were submitted to the restricted datatrack for the German-English translation task.
Thescores on the combination development set range49from BLEU 27.56 to 7 (case insensitive evalua-tion).
The two lowest scoring systems at BLEU11 and 7 were so far from the rest of the systemsthat we decided to exclude them, assuming an er-ror had occurred.Within the remaining 12 submissions were fourn-best lists, three 100-best and one 10-best.For our submissions we combined the top sevensystems between BLEU 22.91 and 20.24 as well asthe top 12 systems where the last one of those wasscoring BLEU 16.00 on the development set.
Forthis language pair the combination with the nor-malized n-gram agreement also outperforms theone without by +0.8 BLEU (see Table 4).system dev test TER Meteorbest single 22.91 21.03 61.87 47.96top 7 asmooth 25.13 22.86 60.73 49.71top 12 asmooth 25.32 22.98 60.72 50.01top 12 25.12 22.20 60.95 49.33Table 4: German-English Results: BLEUOur system combination via hypothesis selec-tion could improve translation quality by +1.95BLEU on the unseen test set over the best singlesystem.4.3 Hungarian-EnglishOnly three systems were submitted for theHungarian-English translation task.
Scores on thecombination development set ranged from BLEU13.63 to 10.04 (case insensitive evaluation).
Onlythe top system provided an n-best list.
We used100-best hypotheses.system dev test TER Meteorbest single 13.63 12.73 68.75 36.763 sys asmooth 14.98 13.74 72.34 38.203 sys 14.14 13.18 74.29 37.52Table 5: Hungarian-English Results: BLEUWe submitted combinations of the three systemsby using the modified smoothed n-gram agree-ment feature and the plain version of the n-gramagreement feature.
Here also the normalized ver-sion of the feature gives an improvement of +0.56BLEU with an overall improvement of +1.0 BLEUover the best single system (see Table 5).5 SummaryIt is beneficial to include more systems, even ifthey are more than 7 points BLEU behind the bestsystem, as the comparison to the combinationswith fewer systems shows.In the mixed size data situation of the workshopthe modified feature shows a clear improvementfor all three language pairs.
Different smoothingfactors should be investigated for these data setsin the future.AcknowledgmentsWe would like to thank the participants in theWMT?09 workshop shared translation task forproviding their data, especially n-best lists.ReferencesAlmut Silja Hildebrand and Stephan Vogel.
2008.Combination of machine translation systems via hy-pothesis selection from combined n-best lists.
InMT at work: Proceedings of the Eighth Confer-ence of the Association for Machine Translation inthe Americas, pages 254?261, Waikiki, Hawaii, Oc-tober.
Association for Machine Translation in theAmericas.Damianos Karakos, Jason Eisner, Sanjeev Khudanpur,and Markus Dreyer.
2008.
Machine translationsystem combination using itg-based alignments.
InProceedings of ACL-08: HLT, Short Papers, pages81?84, Columbus, Ohio, June.
Association for Com-putational Linguistics.Antti-Veikko Rosti, Bing Zhang, Spyros Matsoukas,and Richard Schwartz.
2008.
Incremental hy-pothesis alignment for building confusion networkswith application to machine translation system com-bination.
In Proceedings of the Third Workshopon Statistical Machine Translation, pages 183?186,Columbus, Ohio, June.
Association for Computa-tional Linguistics.Andreas Stolcke.
2002.
Srilm - an extensible lan-guage modeling toolkit.
In Proceedings Interna-tional Conference for Spoken Language Processing,Denver, Colorado, September.50
