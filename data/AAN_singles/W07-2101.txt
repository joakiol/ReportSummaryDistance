Proceedings of the 4th International Workshop on Semantic Evaluations (SemEval-2007), pages 454?459,Prague, June 2007. c?2007 Association for Computational LinguisticsUTD-HLT-CG: Semantic Architecture for Metonymy Resolution andClassification of Nominal RelationsCristina Nicolae, Gabriel Nicolae and Sanda HarabagiuHuman Language Technology Research InstituteThe University of Texas at DallasRichardson, Texas{cristina, gabriel, sanda}@hlt.utdallas.eduAbstractIn this paper we present a semantic archi-tecture that was employed for processingtwo different SemEval 2007 tasks: Task4 (Classification of Semantic Relations be-tween Nominals) and Task 8 (MetonymyResolution).
The architecture uses multi-ple forms of syntactic, lexical, and semanticinformation to inform a classification-basedapproach that generates a different model foreach machine learning algorithm that imple-ments the classification.
We used decisiontrees, decision rules, logistic regression andlazy classifiers.
A voting module selects thebest performing module for each task evalu-ated in SemEval 2007.
The paper details theresults obtained when using the semantic ar-chitecture.1 IntroductionAutomatic semantic interpretations of natural lan-guage text rely on (1) semantic theories that cap-ture the subtleties employed by human communi-cations; (2) lexico-semantic resources that encodevarious forms of semantic knowledge; and (3) com-putational methods that model the selection of theoptimal interpretation derived from the textual data.Two of the SemEval 2007 tasks, namely Task 4(Classification of Semantic Relations between Nom-inals)and Task 8 (Metonymy Resolution) employeddistinct theories for the interpretation of their cor-responding semantic phenomena, but, nevertheless,they also shared several lexico-semantic resources,and, furthermore, both these tasks could have beencast as classification problems, in vein with most ofthe recent work in computational semantic process-ing.
Based on this observation, we have designedand implemented a semantic architecture that wasused in both tasks.
In Section 2 of this paper wegive a brief description of the semantic theories cor-responding to each of the two tasks, while in Section3 we detail the semantic architecture.
Section 4 de-scribes the experimental results and evaluation.We have used three lexico-semantic resources: (i)the WordNet lexico-semantic database; (ii) VerbNet;and (iii) the Lexical Conceptual Structure (LCS)database.
Used only by Task 4, WordNet is a lexico-semantic database created at Princeton University1(Fellbaum, 1998), which encodes a vast majorityof the English nouns, verbs, adjectives and adverbs,and groups synonym words into synsets.
VerbNet2is a broad-coverage, comprehensive verb lexiconcreated at University of Pennsylvania, compatiblewith WordNet, but with explicitly stated syntacticand semantic information, using Levin verb classes(Levin, 1993) to systematically construct lexical en-tities.
Classes are hierarchically organized and eachclass in the hierarchy has its corresponding syntac-tic frames, semantic predicates and a list of typicalverb arguments.
The Lexical Conceptual Structure(Traum and Habash, 2000) is a compositional ab-straction with language-independent properties.
AnLCS is a directed graph with a root.
Each node is as-sociated with certain information, including a type, aprimitive and a field.
An LCS captures the semantics1http://wordnet.princeton.edu2http://verbs.colorado.edu/verb-index/verbnet-2.1.tar.gz454Relation Positive example1.
CAUSE-EFFECT Earplugs relieve the discomfort from traveling with a cold allergy or sinus condition.2.
INSTRUMENT-AGENCY The judge hesitates, gavel poised, shooting them a warning look.3.
PRODUCT-PRODUCER The boy who made the threat was arrested, charged, and had items confiscated from his home.4.
ORIGIN-ENTITY Cinnamon oil is distilled from bark chips and used to alleviate stomach upsets.5.
THEME-TOOL The port scanner is a utility to scan a system to get the status of the TCP.6.
PART-WHOLE The granite benches are former windowsills from the Hearst Memorial Mining Building.7.
CONTENT-CONTAINER The kitchen holds patient drinks and snacks.Table 1: Examples of semantic relations.of a lexical item through a combination of semanticstructure and semantic content.2 Semantic TasksThe two semantic tasks addressed in this paperare: Classification of Semantic Relations betweenNominals (Task 4), defined in (Girju et al, 2007)and Metonymy Resolution (Task 8), defined in(Markert and Nissim, 2007).
Please refer to thesetask description papers for more details.
Both arecast as classification tasks: given an unlabeled in-stance, a system must label it according to one classof a set specific to each task.The training and testing datasets for themetonymy resolution task are annotated in anXML format.
There are 1090 training and 842testing instances for companies, and 941 trainingand 908 testing instances for locations.
Eachtraining instance corresponds to a context inwhich a single name is annotated with its read-ing (metonymic/literal/mixed) and, in case ofmetonymy, its type (metotype).
The testing datasetfor this task is annotated in a similar manner, onlythe reading of the name is left unknown and mustbe decided by the system.For the classification of semantic relations be-tween nominals, there exist seven training sets of140 instances each for the seven semantic relations,and seven corresponding testing sets of around 70instances each.
A training instance is annotated withinformation about the boundaries of the two nom-inals whose relation must be determined, the truthvalue of their relation, the WordNet sense of eachnominal, and the query that was employed by the an-notators to retrieve this example from the Web.
Thetesting instances are similar, with the only differencebeing that the truth value of the relations is unknownand must be determined.3 Semantic ArchitectureThe semantic architecture that we have designedis illustrated in Figure 1, which contains the basicmodules and resources used in the various phases ofprocessing the input data towards the final submis-sion format.
The grayed-out modules are all usedonly for the semantic relations classification task,while the part of the figure represented by dottedlines appears only in the metonymy resolution al-gorithm.
The input to the system, for both tasks,comprises the annotated instances, either from thetraining or the testing dataset.
Before any feature isextracted, the data passes through a pipeline of pre-processing modules.
The text is first split into tokensin a heuristic manner.
The resulting tokenized text isgiven as input to Brill?s part of speech tagger3 , whichassociates each word with its part of speech (e.g.,NN, PRP).
The data further goes through Collins?syntactic parser4, which builds the syntactic trees forall the sentences in the text.Additionally, for semantic relations classification,the system creates the dependency structures forall the sentences, using the dependency parser builtat Stanford5 and described in (de Marneffe et al,2006).
The dependency parser extracts some of 48grammatical relations for each pair of words in asentence.
A second module that is specific only tothis task is (Surdeanu and Turmo, 2005)?s seman-tic role labeler, which extracts the shallow seman-tic structure for each sentence, that is, the predicatesand their arguments.In order to extract the features for the machinelearning algorithm, the modules described aboveare used, and, in addition, information from Word-Net, VerbNet and the LCS Database is incorporated,3http://www.cs.jhu.edu/?brill/4http://people.csail.mit.edu/mcollins/code.html5http://nlp.stanford.edu/downloads/lex-parser.shtml455ParserPropBankWordNetGenerationModelsExtractionFeatureSelectionFeatureVotingSubmissionGenerationmodel 2model 1model kVerbNet LCS DatabaseParserDependencyPOS TaggerTokenizerParserSyntacticHandcrafted ...INSTANCESINSTANCESANNOTATEDFigure 1: Semantic architecture.Category Feature name Feature descriptionsyntactic prevpos part of speech of previous word in the sentencenextpos part of speech of next word in the sentencedeterminer if the word has a determinerprepgoverning if the word is governed by a prepositional phrase (PP), we extract the prepositioninsidequotes if the word is inside quoteslemmapost if the word is postmodifier for a noun, take the lemma of the nounlemmapre if the word is premodifier for a noun, take the lemma of the nounpossession if the word is a possessor, and what it possessessemantic role the role(s) of the name in the sentence: subject, object, under PProlelemma the combination between the role and the lemma of the verb whose argument the word isrolevn same as above, but using the VerbNet class instead of the verb?s lemmarolelevin same as above, but using the Levin class instead of the verb?s lemmarolelcs same as above, but using LCS primitives from the LCS database instead of the verb?s lemmaTable 2: Features for metonymy resolution.along with other features, based on the manual an-notations for both the training and testing datasetsby the task organizers.
These other features use thegrammatical annotations for the possibly metonymicname, in the case of metonymy resolution, and thequery that was used to retrieve that particular in-stance and the disambiguated WordNet sense for thetwo nominals, in the case of semantic relations clas-sification.The features implemented for the two tasks aredescribed in Tables 2 and 3.
Their types are: syn-tactic, semantic, lexical and other.
The syntacticfeatures express the relationships between the tar-get words and words from the rest of the sentence(e.g., the part of speech of the previous word in thesentence, or the dependency relations between twowords).
The semantic features make use of the in-formation given by the resources used by the system(e.g., the VerbNet class of the verb whose argumentthe word is, or the lexicographic category of a wordin WordNet).
The lexical feature is the lemma of theword.
The other feature is the query provided byTask 4.Using these sets of features, a number of modelswere generated by different machine learning tech-niques included with the Weka data mining software(Witten and Frank, 2005).
The machine learningclassifiers comprise decision trees, decision rules,logistic regression, and ?lazy?
classifiers like k-nearest-neighbor.
Because of too many features gen-erated for a relatively small training dataset, featureselection is performed by Weka before creating themodels.
Metonymy resolution uses in addition theentire set of features, since the dataset has seventimes more instances than the other task.
For theclassification of semantic relations, the initial totaland the number of features that remain after the se-lection are printed in Table 4.For metonymy resolution, there are six sub-tasks to be resolved, which result from allcombinations between organization/location andcoarse/medium/fine granularity of the label.
For theclassification of nominal relations, there are 28 sub-tasks, resulting from the processing of the seven se-456Category Feature name Feature descriptionsyntactic dependency the dependency relations between the two wordsmodifier if one word is a modifier of the otherprepositions the prepositions immediately before and after both wordsdeterminers the determiners of the two wordspattern the simplified pattern that exists in the sentence between the two wordslexical lemmas the lemmas of the wordssemantic predicates the predicates whose arguments the two words arepredtypes the predicate types of the predicates abovesamepred if the two words are arguments of the same predicate, which one that islexname the lexicographic category of each word in WordNethyponym if one word is a hyponym of the other in WordNetpartof if one word is a part of the other in WordNetshareholonym if the two words share a holonym in WordNetshareparent if the two words share a parent in WordNetother query the query that was used by the annotators to retrieve the training example from the WebTable 3: Features for classification of semantic relations between nominals.R1 R2 R3 R4 R5 R6 R7before 682 1200 913 898 861 849 677after 13 19 10 15 15 8 16Table 4: The number of features before and af-ter Weka selection, for each semantic relationdataset: R1 CAUSE-EFFECT, R2 INSTRUMENT-AGENCY, R3 PRODUCT-PRODUCER, R4 ORIGIN-ENTITY, R5 THEME-TOOL, R6 PART-WHOLE, andR7 CONTENT-CONTAINER.mantic relations, in which four experiments are con-ducted, each with an increasing number of train-ing instances.
We treated each subtask as a sepa-rate classification problem.
Its training set and fea-tures are fed into Weka to create several models.Each classification algorithm mentioned before isemployed to obtain one model.
For each subtask,the voting module selects the best performing modelon 10-fold crossvalidation, which is used to classifythe test instances.
These annotated instances makeup the submission dataset for that particular subtask.To note is that the coarse metonymic level and thesemantic relations classification are binary classifi-cations, while the rest of the metonymic subtasksare multi-class classifications, performed in a singlestage.4 Experimental Results and EvaluationBoth the metonymy resolution system and the sys-tem for classification of semantic relations per-formed well in the SemEval 2007 competition.
TheBase type Coarse Medium Fine BALocations 84.1 84.0 82.2 79.4Organizations 73.9 71.1 71.1 61.8Table 5: Accuracy for the metonymy resolution sys-tem at three granularity levels.Base type Reading P R F BALocations literal 88.2 92.4 90.2 79.4non-literal 64.1 52.4 57.6 20.6Organizations literal 75.8 84.8 80.0 61.8non-literal 69.6 56.2 62.2 38.2Table 6: Performance for the metonymy resolutionsystem for the coarse level.experiments presented in this paper were done onthe training and testing datasets for each subtask.
Tonote is that no other training data was collected orused than the one provided by the organizers.4.1 Results for Metonymy ResolutionThis system was scored by measuring its accuracy atthree granularity levels (coarse, medium, and fine)and the precision, recall and F score for all com-binations of locations/organizations and literal/non-literal.
These results are tabulated in Tables 5, 6, 7and 8.All results are compared with the baseline accu-racy values (BA).
In Table 5, the baselines are com-puted by taking all readings to be literal; for the rest,the baseline is the percentage in the gold test dataof each reading.
As can be observed, the readings457Base type Reading P R F BALocations literal 87.8 93.5 90.5 79.4mixed 0.0 0.0 0.0 2.2metonymic 63.6 52.3 58.0 18.4Organizations literal 74.3 90.0 81.4 61.8mixed 28.6 13.1 18.0 7.2metonymic 66.8 47.1 55.3 31.0Table 7: Performance for the metonymy resolutionsystem for the medium level.Base type Reading P R F BALoc literal 85.7 94.6 89.9 79.4mixed 0.0 0.0 0.0 2.2othermet 0.0 0.0 0.0 1.2obj-for-name 0.0 0.0 0.0 0.0obj-for-repr 0.0 0.0 0.0 0.0place-for-people 57.1 45.4 50.6 15.5place-for-event 0.0 0.0 0.0 1.1place-for-prod 0.0 0.0 0.0 0.1Org literal 74.4 90.4 81.6 61.8mixed 50.0 3.33 6.25 7.1othermet 0.0 0.0 0.0 1.0obj-for-name 80.0 66.7 72.7 0.7obj-for-repr 0.0 0.0 0.0 0.0org-for-members 61.3 64.0 62.6 19.1org-for-event 0.0 0.0 0.0 0.1org-for-prod 60.6 29.9 40.0 8.0org-for-fac 0.0 0.0 0.0 1.9org-for-index 0.0 0.0 0.0 0.4Table 8: Performance for the metonymy resolutionsystem for the fine level.for locations were more reliably identified than theones for companies.
An explanation for this differ-ence in performance lies in the fact that locations, intheir literal readings, are inactive entities, whereas intheir non-literal readings they are very often active,especially in the annotated instances of the trainingdataset.
This cannot be said for organizations?
theycan be active in their literal readings.
The active vs.inactive criterion, therefore, functions better for lo-cations.
Furthermore, since the training set containsa ratio literals/non-literals of 1.7 for organizationsand 3.9 for locations, the models were skewed, iden-tifying literal readings more easily than non-literalones, as shown in Table 6.4.2 Results for Classification of SemanticRelations between NominalsThis task?s performance was measured by accuracy,precision, recall and F-measure, the latter constitut-Semantic relation P R F Acc InstCause-Effect 65.5 87.8 75.0 70.0 80Instrument-Agency 68.3 73.7 70.9 70.5 78Product-Producer 66.7 96.8 78.9 65.6 93Origin-Entity 62.9 61.1 62.0 66.7 81Theme-Tool 70.0 24.1 35.9 64.8 71Part-Whole 55.6 76.9 64.5 69.4 72Content-Container 82.4 36.8 50.9 63.5 74Average 67.3 65.3 62.6 67.2 78.4Avg baseline 81.3 42.9 56.2 57.0 78.4Table 9: Performance of the semantic relations clas-sification system for each semantic relation.ing the score for ranking the systems in the com-petition.
Table 9 presents these scores by seman-tic relation.
The column entitled ?Inst?
contains thenumber of instances in the testing sets correspond-ing to each relation.
The average baseline valueswere computed by guessing the label to be the ma-jority in the dataset for each relation.
From this tableit can be observed that the PRODUCT-PRODUCER,INSTRUMENT-AGENCY, and CAUSE-EFFECT rela-tions were detected with a relatively very high per-formance score, whereas the THEME-TOOL relationclassification yielded a relatively small score.
Thiscan be explained as the effect of their specifications;the three best-ranked relations are well-defined byhuman standards, while the THEME-TOOL relationis more ambiguous.Table 10 contains the scores of the 10-fold cross-validation experiments that were performed on thetraining dataset in order to select the best classifi-cation algorithm.
The classifiers used in these ex-periments were, in the order of appearance in thetable: JRip, Random Forest, ADTree, Logistic Re-gression, IBk, and Random Tree.
The Logistic Re-gression classifier was chosen in the vast majority ofcases, because it achieved the highest score for sixout of the seven relations.
For R6, PART-WHOLE,Random Forest was preferred.
This ranking betweenthe scores of classifying relations, done consider-ing training accuracy only, does not however antic-ipate the final F score ranking in Table 9.
In par-ticular, the crossvalidation accuracy of R5, THEME-TOOL, is better than the accuracy for R3, PRODUCT-PRODUCER, which came first in the final results,whereas R5 came last and at a large distance fromthe others.
These lower-than-expected results in the458Alg R1 R2 R3 R4 R5 R6 R7JRip 72.1 76.4 68.6 66.4 68.6 66.4 73.6RandF 78.6 85.0 72.1 77.1 74.3 70.7 73.6ADTree 72.9 79.3 70.0 70.7 70.7 68.6 69.3LogReg 79.3 85.7 72.1 80.0 76.4 70.0 75.7IBk 78.6 83.6 70.7 75.7 74.3 70.0 72.1RandT 79.3 85.7 71.4 77.1 75.0 70.0 72.1Table 10: Results on 10-fold crossvalidation foreach relation and each classifier.evaluation were caused in part by the drastic featureselection module that was applied before generatingthe models.
In experiments performed on the de-velopment data, the accuracy on 10-fold crossvali-dation was increased with an average of 7% by fea-ture selection, but the same feature set on the test-ing data obtained a final score 4.7% less than theone obtained by using all the features (F=67.3%).The results submitted in the evaluation were basedon feature selection because of this misleading per-formance shift observed on the development set.The task of classification of semantic relations be-tween nominals required data to be separated intofour training sets: the first 35 instances (D1), thefirst 70 instances (D2), the first 105 instances (D3),and the entire set, 140 instances (D4).
The letter ?D?stands for systems that use both the WordNet and thequery information provided by the organizers.
Theresults on the four sets are illustrated in Figure 2.The results generally increase with the size of train-ing data, and tend to be the same on D3 and D4,which means that the D4 set does not bring signifi-cant new information compared to D3.0102030405060708090D1 D2 D3 D4R1R2R3R4R5R6R7Figure 2: Results of training on different portions ofthe training dataset.5 ConclusionsThis paper has presented a semantic architecturethat participated in the SemEval 2007 competitionto evaluate two tasks, one for metonymy resolution,and the other for the classification of semantic re-lations between nominals.
Although the tasks werevery different, the architecture produced competitiveresults.
The experimental results are reported in thispaper in a detailed manner, and some interesting ob-servations can be drawn from them.ReferencesMarie-Catherine de Marneffe, Bill MacCartney, andChristopher D. Manning.
2006.
Generating typed de-pendency parses from phrase structure parses.
In 5thInternational Conference on Language Resources andEvaluation (LREC 2006).C.
Fellbaum, editor.
1998.
WordNet: An Electronic Lexi-cal Database and Some of its Applications.
MIT Press.Roxana Girju, Dan Moldovan, Marta Tatu, and DanielAntohe.
2005.
On the semantics of noun compounds.In Computer Speech and Language, volume 19, pages479?496.Roxana Girju, Marti Hearst, Preslav Nakov, Vivi Nas-tase, Stan Szpakowicz, Peter Turney, and Deniz Yuret.2007.
Task 04: Classification of semantic relations be-tween nominal at semeval 2007.
In SemEval 2007.Beth Levin.
1993.
English Verb Classes and Alterna-tions.
The University of Chicago Press, Chicago andLondon.Katja Markert and Malvina Nissim.
2002.
Metonymyresolution as a classification task.
In the 2002 Con-ference on Empirical Methods in Natural LAnguageProcessing (EMNLP2002).Katja Markert and Malvina Nissim.
2007.
Task 08:Metonymy resolution at semeval 2007.
In SemEval2007.Mihai Surdeanu and Jordi Turmo.
2005.
Semantic rolelabeling using complete syntactic analysis.
In CoNLL2005, Shared Task.David Traum and Nizar Habash.
2000.
Generation fromlexical conceptual structure.
In Workshop on AppliedInterlinguas, ANLP-2000.Ian H. Witten and Eibe Frank.
2005.
Data Mining: Prac-tical machine learning tools and techniques.
MorganKaufmann, 2nd edition.459
