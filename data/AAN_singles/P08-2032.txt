Proceedings of ACL-08: HLT, Short Papers (Companion Volume), pages 125?128,Columbus, Ohio, USA, June 2008. c?2008 Association for Computational LinguisticsRobust Extraction of Named Entity Including Unfamiliar WordMasatoshi Tsuchiya?
Shinya Hida?
Seiichi Nakagawa?
?Information and Media Center / ?Department of Information and Computer Sciences,Toyohashi University of Technologytsuchiya@imc.tut.ac.jp, {hida,nakagawa}@slp.ics.tut.ac.jpAbstractThis paper proposes a novel method to extractnamed entities including unfamiliar wordswhich do not occur or occur few times in atraining corpus using a large unannotated cor-pus.
The proposed method consists of twosteps.
The first step is to assign the most simi-lar and familiar word to each unfamiliar wordbased on their context vectors calculated froma large unannotated corpus.
After that, tra-ditional machine learning approaches are em-ployed as the second step.
The experiments ofextracting Japanese named entities from IREXcorpus and NHK corpus show the effective-ness of the proposed method.1 IntroductionIt is widely agreed that extraction of named entity(henceforth, denoted as NE) is an important sub-task for various NLP applications.
Various ma-chine learning approaches such as maximum en-tropy(Uchimoto et al, 2000), decision list(Sassanoand Utsuro, 2000; Isozaki, 2001), and SupportVector Machine(Yamada et al, 2002; Isozaki andKazawa, 2002) were investigated for extracting NEs.All of them require a corpus whose NEs are an-notated properly as training data.
However, it is dif-ficult to obtain an enough corpus in the real world,because there are increasing the number of NEs likepersonal names and company names.
For example,a large database of organization names(Nichigai As-sociates, 2007) already contains 171,708 entries andis still increasing.
Therefore, a robust method to ex-tract NEs including unfamiliar words which do notoccur or occur few times in a training corpus is nec-essary.This paper proposes a novel method of extract-ing NEs which contain unfamiliar morphemes us-ing a large unannotated corpus, in order to resolvethe above problem.
The proposed method consistsTable 1: Statistics of NE Types of IREX CorpusNE Type Frequency (%)ARTIFACT 747 (4.0)DATE 3567 (19.1)LOCATION 5463 (29.2)MONEY 390 (2.1)ORGANIZATION 3676 (19.7)PERCENT 492 (2.6)PERSON 3840 (20.6)TIME 502 (2.7)Total 18677of two steps.
The first step is to assign the mostsimilar and familiar morpheme to each unfamiliarmorpheme based on their context vectors calculatedfrom a large unannotated corpus.
The second step isto employ traditional machine learning approachesusing both features of original morphemes and fea-tures of similar morphemes.
The experiments ofextracting Japanese NEs from IREX corpus andNHK corpus show the effectiveness of the proposedmethod.2 Extraction of Japanese Named Entity2.1 Task of the IREX WorkshopThe task of NE extraction of the IREX workshop(Sekine and Eriguchi, 2000) is to recognize eightNE types in Table 1.
The organizer of the IREXworkshop provided a training corpus, which consistsof 1,174 newspaper articles published from January1st 1995 to 10th which include 18,677 NEs.
In theJapanese language, no other corpus whose NEs areannotated is publicly available as far as we know.12.2 Chunking of Named EntitiesIt is quite common that the task of extractingJapanese NEs from a sentence is formalized asa chunking problem against a sequence of mor-1The organizer of the IREX workshop also provides the test-ing data to its participants, however, we cannot see it becausewe did not join it.125phemes.
For representing proper chunks, we em-ploy IOB2 representation, one of those which havebeen studied well in various chunking tasks ofNLP (Tjong Kim Sang, 1999).
This representationuses the following three labels.B Current token is the beginning of a chunk.I Current token is a middle or the end of achunk consisting of more than one token.O Current token is outside of any chunk.Actually, we prepare the 16 derived labels from thelabel B and the label I for eight NE types, in orderto distinguish them.When the task of extracting Japanese NEs froma sentence is formalized as a chunking problem of asequence of morphemes, the segmentation boundaryproblem arises as widely known.
For example, theNE definition of IREX tells that a Chinese character?
(bei)?
must be extracted as an NE means Amer-ica from a morpheme ?
(hou-bei)?
which meansvisiting America.
A naive chunker using a mor-pheme as a chunking unit cannot extract such kind ofNEs.
In order to cope this problem, (Uchimoto et al,2000) proposed employing translation rules to mod-ify problematic morphemes, and (Asahara and Mat-sumoto, 2003; Nakano and Hirai, 2004) formalizedthe task of extracting NEs as a chunking problemof a sequence of characters instead of a sequence ofmorphemes.
In this paper, we keep the naive formal-ization, because it is still enough to compare perfor-mances of proposed methods and baseline methods.3 Robust Extraction of Named EntitiesIncluding Unfamiliar WordsThe proposed method of extracting NEs consistsof two steps.
Its first step is to assign the mostsimilar and familiar morpheme to each unfamiliarmorpheme based on their context vectors calculatedfrom a large unannotated corpus.
The second step isto employ traditional machine learning approachesusing both features of original morphemes and fea-tures of similar morphemes.
The following sub-sections describe these steps respectively.3.1 Assignment of Similar MorphemeA context vector Vm of a morpheme m is a vectorconsisting of frequencies of all possible unigramsand bigrams,Vm =????
?f(m,m0), ?
?
?
f(m,mN ),f(m,m0,m0), ?
?
?
f(m,mN ,mN ),f(m0,m), ?
?
?
f(mN ,m),f(m0,m0,m), ?
?
?
f(mN ,mN ,m)????
?,where M ?
{m0,m1, .
.
.
,mN} is a set of all mor-phemes of the unannotated corpus, f(mi,mj) is afrequency that a sequence of a morpheme mi anda morpheme mj occurs in the unannotated corpus,and f(mi,mj ,mk) is a frequency that a sequenceof morphemes mi,mj and mk occurs in the unan-notated corpus.Suppose an unfamiliar morpheme mu ?
M?MF ,where MF is a set of familiar morphemes that occurfrequently in the annotated corpus.
The most sim-ilar morpheme m?u to the morpheme mu measuredwith their context vectors is given by the followingequation,m?u = argmaxm?MFsim(Vmu , Vm), (1)where sim(Vi, Vj) is a similarity function betweencontext vectors.
In this paper, the cosine function isemployed as it.3.2 FeaturesThe feature set Fi at i-th position is defined as a tupleof the morpheme feature MF (mi) of the i-th mor-pheme mi, the similar morpheme feature SF (mi),and the character type feature CF (mi).Fi = ?
MF (mi), SF (mi), CF (mi) ?The morpheme feature MF (mi) is a pair of the sur-face string and the part-of-speech of mi.
The similarmorpheme feature SF (mi) is defined asSF (mi) ={MF (m?i) if mi ?
M ?
MFMF (mi) otherwise,where m?i is the most similar and familiar morphemeto mi given by Equation (1).
The character type fea-ture CF (mi) is a set of four binary flags to indi-cate that the surface string of mi contains a Chinesecharacter, a hiragana character, a katakana charac-ter, and an English alphabet respectively.When we identify the chunk label ci for the i-th morpheme mi, the surrounding five feature setsFi?2, Fi?1, Fi, Fi+1, Fi+2 and the preceding twochunk labels ci?2, ci?1 are refered.126Morpheme Feature Similar Morpheme Feature Character(English POS (English POS Type Chunk Labeltranslation) translation) Feature(kyou) (today) Noun?Adverbial (kyou) (today) Noun?Adverbial ?1, 0, 0, 0?
O(no) gen Particle (no) gen Particle ?0, 1, 0, 0?
O(Ishikari) (Ishikari) Noun?Proper (Kantou) (Kantou) Noun?Proper ?1, 0, 0, 0?
B-LOCATION(heiya) (plain) Noun?Generic (heiya) (plain) Noun?Generic ?1, 0, 0, 0?
I-LOCATION(no) gen Particle (no) gen Particle ?0, 1, 0, 0?
O(tenki) (weather) Noun?Generic (tenki) (weather) Noun?Generic ?1, 0, 0, 0?
O(ha) top Particle (ha) top Particle ?0, 1, 0, 0?
O(hare) (fine) Noun?Generic (hare) (fine) Noun?Generic ?1, 1, 0, 0?
OFigure 1: Example of Training Instance for Proposed Method??
Parsing Direction ?
?Feature set Fi?2 Fi?1 Fi Fi+1 Fi+2Chunk label ci?2 ci?1 ciFigure 1 shows an example of training instance ofthe proposed method for the sentence ?
(kyou)(no) (Ishikari) (heiya) (no)(tenki) (ha) (hare)?
which means ?It is fine atIshikari-plain, today?.
?
(Kantou)?
is assignedas the most similar and familiar morpheme to ?(Ishikari)?
which is unfamiliar in the training corpus.4 Experimental Evaluation4.1 Experimental SetupIREX Corpus is used as the annotated corpus to trainstatistical NE chunkers, and MF is defined experi-mentally as a set of all morphemes which occur fiveor more times in IREX corpus.
Mainichi News-paper Corpus (1993?1995), which contains 3.5Msentences consisting of 140M words, is used asthe unannotated corpus to calculate context vectors.MeCab2(Kudo et al, 2004) is used as a preprocess-ing morphological analyzer through experiments.In this paper, either Conditional RandomFields(CRF)3(Lafferty et al, 2001) or Support Vec-tor Machine(SVM)4(Cristianini and Shawe-Taylor,2000) is employed to train a statistical NE chunker.4.2 Experiment of IREX CorpusTable 2 shows the results of extracting NEs of IREXcorpus, which are measured with F-measure through5-fold cross validation.
The columns of ?Proposed?show the results with SF , and the ones of ?Base-line?
show the results without SF .
The column of?NExT?
shows the result of using NExT(Masui et2http://mecab.sourceforge.net/3http://chasen.org/?taku/software/CRF++/4http://chasen.org/?taku/software/yamcha/Table 2: NE Extraction Performance of IREX CorpusProposed Baseline NExTCRF SVM CRF SVMARTIFACT 0.487 0.518 0.458 0.457 -DATE 0.921 0.909 0.916 0.916 0.682LOCATION 0.866 0.863 0.847 0.846 0.696MONEY 0.951 0.610 0.937 0.937 0.895ORGANIZATION 0.774 0.766 0.744 0.742 0.506PERCENT 0.936 0.863 0.928 0.928 0.821PERSON 0.825 0.842 0.788 0.787 0.672TIME 0.901 0.903 0.902 0.901 0.800Total 0.842 0.834 0.821 0.820 0.732Table 3: Statistics of NE Types of NHK CorpusNE Type Frequency (%)DATE 755 (19%)LOCATION 1465 (36%)MONEY 124 (3%)ORGANIZATION 1056 (26%)PERCENT 55 (1%)PERSON 516 (13%)TIME 101 (2%)Total 4072al., 2002), an NE chunker based on hand-craftedrules, without 5-fold cross validation.As shown in Table 2, machine learning ap-proaches with SF outperform ones without SF .Please note that the result of SVM without SF andthe result of (Yamada et al, 2002) are comparable,because our using feature set without SF is quitesimilar to their feature set.
This fact suggests thatSF is effective to achieve better performances thanthe previous research.
CRF with SF achieves betterperformance than SVM with SF , although CRF andSVM are comparable in the case without SF .
NExTachieves poorer performance than CRF and SVM.4.3 Experiment of NHK CorpusNippon Housou Kyoukai (NHK) corpus is a set oftranscriptions of 30 broadcast news programs whichwere broadcasted from June 1st 1996 to 12th.
Ta-ble 3 shows the statistics of NEs of NHK corpuswhich were annotated by a graduate student except127Table 4: NE Extraction Performance of NHK CorpusProposed Baseline NExTCRF SVM CRF SVMDATE 0.630 0.595 0.571 0.569 0.523LOCATION 0.837 0.825 0.797 0.811 0.741MONEY 0.988 0.660 0.971 0.623 0.996ORGANIZATION 0.662 0.636 0.601 0.598 0.612PERCENT 0.538 0.430 0.539 0.435 0.254PERSON 0.794 0.813 0.752 0.787 0.622TIME 0.250 0.224 0.200 0.247 0.260Total 0.746 0.719 0.702 0.697 0.615Table 5: Extraction of Familiar/Unfamiliar NEsFamiliar Unfamiliar OtherCRF (Proposed) 0.789 0.654 0.621CRF (Baseline) 0.757 0.556 0.614for ARTIFACT in accordance with the NE definitionof IREX.
Because all articles of IREX corpus hadbeen published earlier than broadcasting programsof NHK corpus, we can suppose that NHK corpuscontains unfamiliar NEs like real input texts.Table 4 shows the results of chunkers trained fromwhole IREX corpus against NHK corpus.
The meth-ods with SF outperform the ones without SF .
Fur-thermore, performance improvements between theones with SF and the ones without SF are greaterthan Table 2.The performance of CRF with SF and one ofCRF without SF are compared in Table 5.
The col-umn ?Familiar?
shows the results of extracting NEswhich consist of familiar morphemes, as well as thecolumn ?Unfamiliar?
shows the results of extractingNEs which consist of unfamiliar morphemes.
Thecolumn ?Other?
shows the results of extracting NEswhich contain both familiar morpheme and unfa-miliar one.
These results indicate that SF is espe-cially effective to extract NEs consisting of unfamil-iar morphemes.5 Concluding RemarksThis paper proposes a novel method to extract NEsincluding unfamiliar morphemes which do not occuror occur few times in a training corpus using a largeunannotated corpus.
The experimental results showthat SF is effective for robust extracting NEs whichconsist of unfamiliar morphemes.
There are othereffective features of extracting NEs like N -best mor-pheme sequences described in (Asahara and Mat-sumoto, 2003) and features of surrounding phrasesdescribed in (Nakano and Hirai, 2004).
We will in-vestigate incorporating SF and these features in thenear future.ReferencesMasayuki Asahara and Yuji Matsumoto.
2003.
Japanesenamed entity extraction with redundant morphologicalanalysis.
In Proc.
of HLT?NAACL ?03, pages 8?15.Nello Cristianini and John Shawe-Taylor.
2000.
AnIntroduction to Support Vector Machines and OtherKernel-based Learning Methods.
Cambridge Univer-sity Press.Hideki Isozaki and Hideto Kazawa.
2002.
Efficient sup-port vector classifiers for named entity recognition.
InProc.
of the 19th COLING, pages 1?7.Hideki Isozaki.
2001.
Japanese named entity recogni-tion based on a simple rule generator and decision treelearning.
In Proc.
of ACL ?01, pages 314?321.Taku Kudo, Kaoru Yamamoto, and Yuji Matsumoto.2004.
Appliying conditional random fields to japanesemorphological analysis.
In Proc.
of EMNLP2004,pages 230?237.John Lafferty, Andrew McCallum, and Fernando Pereira.2001.
Conditional Random Fields: Probabilistic Mod-els for Segmenting and Labeling Sequence Data.
InProceedings of ICML, pages 282?289.Fumito Masui, Shinya Suzuki, and Junichi Fukumoto.2002.
Development of named entity extraction toolNExT for text processing.
In Proceedings of the 8thAnnual Meeting of the Association for Natural Lan-guage Processing, pages 176?179.
(in Japanese).Keigo Nakano and Yuzo Hirai.
2004.
Japanese namedentity extraction with bunsetsu features.
Transac-tions of Information Processing Society of Japan,45(3):934?941, Mar.
(in Japanese).Nichigai Associates, editor.
2007.
DCS Kikan-mei Jisho.Nichigai Associates.
(in Japanese).Manabu Sassano and Takehito Utsuro.
2000.
Namedentity chunking techniques in supervised learning forjapanese named entity recognition.
In Proc.
of the 18thCOLING, pages 705?711.Satoshi Sekine and Yoshio Eriguchi.
2000.
Japanesenamed entity extraction evaluation: analysis of results.In Proc.
of the 18th COLING, pages 1106?1110.E.
Tjong Kim Sang.
1999.
Representing text chunks.
InProc.
of the 9th EACL, pages 173?179.Kiyotaka Uchimoto, Ma Qing, Masaki Murata, HiromiOzaku, Masao Utiyama, and Hitoshi Isahara.
2000.Named entity extraction based on a maximum entropymodel and transformation rules.
Journal of NaturalLanguage Processing, 7(2):63?90, Apr.
(in Japanese).Hiroyasu Yamada, Taku Kudo, and Yuji Matsumoto.2002.
Japanese named entity extraction using supportvector machine.
Transactions of Information Process-ing Society of Japan, 43(1):44?53, Jan. (in Japanese).128
