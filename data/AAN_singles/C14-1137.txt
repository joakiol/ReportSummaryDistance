Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers,pages 1447?1456, Dublin, Ireland, August 23-29 2014.A Markovian approach to distributional semanticswith application to semantic compositionality?Edouard GraveEECS DepartmentUC Berkeleygrave@berkeley.eduGuillaume ObozinskiLIGM ?
Universit?e Paris-Est?Ecole des Ponts ?
ParisTechguillaume.obozinski@imagine.enpc.frFrancis BachInria ?
Sierra project-team?Ecole Normale Sup?erieurefrancis.bach@ens.frAbstractIn this article, we describe a new approach to distributional semantics.
This approach relieson a generative model of sentences with latent variables, which takes the syntax into accountby using syntactic dependency trees.
Words are then represented as posterior distributions overthose latent classes, and the model allows to naturally obtain in-context and out-of-context wordrepresentations, which are comparable.
We train our model on a large corpus and demonstratethe compositionality capabilities of our approach on different datasets.1 IntroductionIt is often considered that words appearing in similar contexts tend to have similar meaning (Harris,1954).
This idea, known as the distributional hypothesis was famously summarized by Firth (1957)as follow: ?you shall know a word by the company it keeps.?
The distributional hypothesis has beenapplied in computational linguistics in order to automatically build word representations that capturetheir meaning.
For example, simple distributional information about words, such as co-occurence counts,can be extracted from a large text corpus, and used to build a vectorial representation of words (Lundand Burgess, 1996; Landauer and Dumais, 1997).
According to the distributional hypothesis, two wordshaving similar vectorial representations must have similar meanings.
It is thus possible and easy tocompare words using their vectorial representations.In natural languages, sentences are formed by the composition of simpler elements: words.
It isthus reasonable to assume that the meaning of a sentence is determined by combining the meaningsof its parts and the syntactic relations between them.
This principle, often attributed to the Germanlogician Frege, is known as semantic compositionality.
Recently, researchers in computational linguisticsstarted to investigate how the principle of compositionality could be applied to distributional models ofsemantics (Clark and Pulman, 2007; Mitchell and Lapata, 2008).
Given the representations of individualwords, such as federal and agency, is it possible to combine them in order to obtain a representationcapturing the meaning of the noun phrase federal agency?Most approaches to distributional semantics represent words as vectors in a high-dimensional spaceand use linear algebra operations to combine individual word representations in order to obtain represen-tations for complex units.
In this article, we propose a probabilistic approach to distributional semantics.This approach is based on the generative model of sentences with latent variables, which was introducedby Grave et al.
(2013).
We make the following contributions:?
Given the model introduced by Grave et al.
(2013), we describe how in-context and ouf-of-contextwords can be represented by posterior distributions over latent variables (section 4).?
We evaluate out-of-context representations on human similarity judgements prediction tasks anddetermine what kind of semantic relations are favored by our approach (section 5).?
Finally, we evaluate in-context representations on two similarity tasks for short phrases (section 6).This work is licenced under a Creative Commons Attribution 4.0 International License.
Page numbers and proceedings footerare added by the organizers.
License details: http://creativecommons.org/licenses/by/4.0/14472 Related workMost approaches to distributional semantics are based on vector space models (VSM), in which wordsare represented as vectors in a high-dimensional space.
These vectors are obtained from a large textcorpus, by extracting distributional information about words such as the contexts in which they appear.A corpus is then represented as a word-by-context co-occurence matrix.
Contexts can be defined asdocuments in which the target word appear (Deerwester et al., 1990; Landauer and Dumais, 1997) oras words that appear in the neighbourhood of the target word, for example in the same sentence or in afixed-size window around the target word (Schutze, 1992; Lund and Burgess, 1996).Next to vector space models, other approaches to distributional semantics are based on probabilisticmodels of documents, such as probabilistic latent semantic analysis (pLSA) introduced by Hofmann(1999) and which is inspired by latent semantic analysis, or latent Dirichlet allocation (LDA), introducedby Blei et al.
(2003).
In those models, each document is viewed as a mixture of k topics, where eachtopic is a distribution over the words of the vocabulary.The previous models do not take into account the linguistic structure of the sentences used to buildword representations.
Several models have been proposed to address this limitation.
In those models, thecontexts are defined by using the syntactic relations between words (Lin, 1998; Curran and Moens, 2002;Turney, 2006; Pad?o and Lapata, 2007; Baroni and Lenci, 2010).
For example, two words are consideredin the same context if there exists a syntactic relation between them, or if there is a path between them inthe dependency graph.One of the first approaches to semantic compositionality using vector space models was proposedby Mitchell and Lapata (2008).
In this study, individual word representations are combined using linearalgebra operations such as addition, componentwise multiplication, tensor product or dilation.
Those dif-ferent composition operations are then used to disambiguate intransitive verbs given a subject (Mitchelland Lapata, 2008) or to compute similarity scores between pairs of small phrases (Mitchell and Lapata,2010).Another approach to semantic compositionality is to learn the function used to compose individualword representations.
First, a semantic space containing representations for both individual words andphrases is built.
For example, the words federal, agency and the phrase federal agency all have a vectorialrepresentation.
Then, a function mapping individual word representations to phrase representations canbe learnt in a supervised way.
Guevara (2010) proposed to use partial least square regression to learnthis function.
Similarly, Baroni and Zamparelli (2010) proposed to learn a matrix A for each adjective,such that the vectorial representation p of the adjective-noun phrase can be obtained from the vectorialrepresentation b of the noun by the matrix-vector multiplication:p = Ab.Socher et al.
(2012) later generalized this model by proposing to represent each node in a parse tree by avector capturing the meaning and a matrix capturing the compositional effects.
A composition function,inspired by artificial neural networks, is recursively applied in the tree to compute those representations.Following the theoretical framework introduced by Coecke et al.
(2010), Grefenstette and Sadrzadeh(2011) proposed to represent relational words (such as verbs) by tensors and theirs arguments (suchas nouns) by vectors.
Composing a relational word with its arguments is then performed by takingthe pointwise product between the tensor and the Kronecker product of the vectors representing thearguments.
Jenatton et al.
(2012) and Van de Cruys et al.
(2013) proposed two approaches to modelsubject-verb-object triples based on tensor factorization.Finally, research in computation of word meaning in context is closely related to distributional seman-tic compositionality.
Erk and Pad?o (2008) proposed a structured vector space model in which a wordis represented by multiple vectors, capturing its meaning but also the selectional restrictions it has forthe different arguments.
Those different vectors can then be combined to compute a word representationin context.
This model was later generalized by Thater et al.
(2010).
Dinu and Lapata (2010) intro-duced a probabilistic model for computing word representations in context.
In their approach, words arerepresented as probability distributions over latent senses.1448Computers can be designed to do anything with informationc0c1c2c3c4c5c6c7c8c9w1w2w3w4w5w6w7w8w9Figure 1: Example of a dependency tree and its corresponding graphical model.3 Model of semanticsIn this section we briefly review the generative model of sentences introduced by Grave et al.
(2013), andwhich serves as the basis of our approach to distributional semantics.3.1 Generative model of sentencesWe denote the tokens of a sentence of length K by the K-uple w = (w1, ..., wK) ?
{1, ..., V }K, whereV is the size of the vocabulary and each integer represents a word.
We suppose that each token wkisassociated to a corresponding semantic class ck?
{1, ..., C}, where C is the number of semantic classes.Finally, the syntactic dependency tree corresponding to the sentence is represented by the function pi :{1, ...,K} 7?
{0, ...,K}, where pi(k) represents the parent of word k and 0 is the root of the tree (whichis not associated to a word).Given a tree pi, the semantic classes and the words of a sentence are generated as follows.
The semanticclass of the root of the tree is set to a special start symbol, represented by the integer 0.1Then, thesemantic classes corresponding to words are recursively generated down the tree: each semantic classckis drawn from a multinomial distribution pT(ck| cpi(k)), conditioned on the semantic class cpi(k)ofits parent in the tree.
Finally, each word wkis also drawn from a multinomial distribution pO(wk| ck),conditioned on its corresponding semantic class ck.
Thus, the joint probability distribution on words andsemantic classes can be factorized asp(w, c) =K?k=1pT(ck| cpi(k))pO(wk| ck),where the variable c0= 0 represents the root of the tree.
The initial class probability distributionpT(ck| c0= 0) is parameterized by the probability vector q, while the transition probability distributionbetween classes pT(ck| cpi(k)) and the emission probability distribution pO(wk| ck) are parameterizedby the stochastic matrices T and O (i.e., matrices with non-negative elements and unit-sum columns).This model is a hidden Markov model on a tree (instead of a chain).
See Fig.
1 for an example of asentence and its corresponding graphical model.3.2 Corpus and learningWe train the generative model of sentences on the ukWac corpus (Baroni et al., 2009).
This corpus, whichcontains approximately 1.9 billions tokens, was POS-tagged and lemmatized using TreeTagger (Schmid,1994) and parsed using MaltParser (Nivre et al., 2007).
Each word of our vocabulary is a pair of lemmaand its part-of-speech.
We perform smoothing by only keeping the V most frequent pairs, the infrequentones being replaced by a common token.
The parameters ?
= (q,T,O) of the model are learnedusing the algorithm described by Grave et al.
(2013).
The number of latent states C and the number oflemma/POS pairs V were set using the development set of Bruni et al.
(2012).1We recall that the semantic classes corresponding to words are represented by integers between 1 and C.1449presidentchiefchairmandirectorexecutivemanagereyefaceshoulderhandleg foothead head-2head-1Figure 2: Comparison of out-of-context (black) and in-context (red) word representations.
The two-dimensional visualization is obtained by using multidimensional scaling (Borg, 2005).
See text for de-tails.4 Word representationsGiven a trained hidden Markov model, we now describe how to obtain word representations, for both in-context and out-of-context words.
In both cases, words will be represented as a probability distributionover the latent semantic classes.In-context word representation.
Obtaining a representation of a word in the context of a sentence isvery natural using the model introduced in the previous section: we start by parsing the sentence in orderto obtain the syntactic dependency tree.
We then compute the posterior distribution of semantic classes cfor that word, and use this probability distribution to represent the word.
More formally, given a sentencew = (w1, ..., wK), the kth word of the sentence is represented by the vector uk?
RCdefined byuki= P(Ck= i |W = w).The vector ukis the posterior distribution of latent classes corresponding to the kth word of the sentence,and thus, sums to one.
It is efficiently computed using the message passing algorithm (a.k.a.
forward-backward algorithm for HMM).Out-of-context representation.
In order to obtain word representations that are independent of thecontext, we compute the previously introduced in-context representations on a very large corpus, and foreach word type, we average all the in-context representations for all the occurrences of that word typein the corpus.
More formally, given a large set of pairs of tokens and their in-context representations(wk,uk) ?
N?
RC, the representation of the word type a is the vector va?
RC, defined byva=1Za?k : wk=auk,where Zais the number of occurrences of the word type a.
The vector vais thus the posterior distributionof semantic classes averaged over all the occurrences of word type a.Comparing in-context and out-of-context representations.
Since in-context and out-of-contextword representations are defined on the same space (the simplex of dimension C) it is possible to com-pare in-context and out-of-context representations easily.
As an example, we have plotted in Figure 2the out-of-context representation for the words head, president, chief, chairman, director, executive, eye,face, shoulder, hand, leg, etc.
and the in-context representations for the word head in the context of thetwo following sentences:1.
The nurse stuck her head in the room to announce that Dr. Reitz was on the phone.2.
A well-known Wall Street figure may join the Cabinet as head of the Treasury Department.1450Distance RG65 WS353Cosine 0.68 0.50Kullback-Leibler 0.69 0.47Jensen-Shannon 0.72 0.50Hellinger 0.73 0.51Agirre et al.
(BoW) 0.81 0.65Distance SIM.
REL.Cosine 0.68 0.34Kullback-Leibler 0.64 0.31Jensen-Shannon 0.69 0.33Hellinger 0.70 0.34Agirre et al.
(BoW) 0.70 0.62Table 1: Left: Spearman?s rank correlation coefficient ?
between human and distributional similarity, onthe RG65 and WORDSIM353 datasets.
Right: Spearman?s rank correlation coefficient ?
between humanand distributional similarity on two subsets (similarity v.s.
relatedness) of the WORDSIM353 dataset.The two-dimensional visualization is obtained by using multidimensional scaling (Borg, 2005).
First ofall, we observe that the words are clustered in two groups, one containing words belonging to the bodypart class, the other containing words belonging to the leader class, and the word head, appears betweenthose two groups.
Second, we observe that the in-context representations are shifted toward the clustercorresponding to the disambiguated sense of the ambiguous word head.5 Out-of-context evaluationIn this section, we evaluate out-of-context word representations on a similarity prediction task and deter-mine what kind of semantic relations are favored by our approach.5.1 Similarity judgements predictionIn word similarity prediction tasks, pairs of words are presented to human subjects who are asked torate the relatedness between those two words.
These human similarity scores are then compared todistributional similarity scores induced by our models, by computing the correlation between them.Methodology.
We use the RG65 dataset, introduced by Rubenstein and Goodenough (1965) and theWORDSIM353 dataset, collected by Finkelstein et al.
(2001).
These datasets comprise 65 and 353 wordpairs respectively.
Human subjects rated the relatedness of those word pairs.
We use the Spearman?srank correlation coefficient ?
to compare human and distributional score distributions.Comparison of similarity measures.
Since words are represented by posterior distributions over la-tent semantic classes, we have considered distances (or divergences) that are adapted to probability dis-tributions to compute the similarity between word representations: the symmetrised Kullback-Leiblerdivergence, the Jensen-Shannon divergence, and the Hellinger distance.
We use the opposite of thesedissimilarity measures in order to obtain similarity scores.
We also included the cosine similarity mea-sure as a baseline, as it is widely used in the field of distributional semantics.We report results on both datasets in Table 1.
Unsurprisingly, we observe that the dissimilarity mea-sures giving the best results are the one tailored for probability distribution, namely the Jensen-Shannondivergence and the Hellinger distance.
The Kullback-Leibler divergence is too sensitive to fluctuationsof small probabilities and thus does not perform as well as other similarity measures between probabilitydistributions.
In the following, we will use the Hellinger distance.
It should be noted that the resultsreported by Agirre et al.
(2009) were obtained using a corpus containing 1.6 terawords, making it 1,000times larger than ours.
They also report results for various corpus sizes, and when using a corpus whosesize is comparable to ours, their result on WORDSIM353 drops to 0.55.Relatedness v.s.
similarity.
As noted by Agirre et al.
(2009), words might be rated as related fordifferent reasons since different kinds of semantic relations exist between word senses.
Some words,such as telephone and communication might even be rated as related because they belong to the samesemantic field.
Thus, they proposed to split the WORDSIM353 dataset into two subsets: the first onecomprising words that are similar, i.e., synonyms, antonyms and hyperonym-hyponym and the second1451cohyp hyper mero attri event randn randj randv210123cohyp hyper mero attri event randn randj randvFigure 3: Similarity score distributions for various semantic relations on the BLESS dataset, withoutusing the transition matrix (left) and with using the transition matrix (right) for comparing adjectives andverbs with nouns.one comprising words that are related, i.e., meronym-holonym and topically related words.
We reportresults on these two subsets in Table 1.
We observe that our model capture similarity (?
= 0.70) muchbetter than relatedness (?
= 0.34).
This is not very surprising since our model takes the syntax intoaccount.5.2 Semantic relations captured by our word representationsAs we saw in the previous section, different semantic relations between words are not equally capturedby our word representations.
In this section, we thus investigate which kind of semantic relations arefavored by our approach.The BLESS dataset.
The BLESS dataset (Baroni and Lenci, 2011) comprises 200 concrete conceptsand eight relations.
For each pair of concept-relation, a list of related words, referred to as relatum, isgiven.
Five semantic relations are considered: co-hyponymy, hypernymy, meronymy, attribute and event.The attribute relation means that the relatum is an adjective expressing an attribute of the concept, whilethe event relation means that the relatum is a verb designing an activity or an event in which the conceptis involved.
The dataset also contains three random relations (randn, randj ans randv), obtained by theassociation of a random relatum, for different POS: noun, adjective and verb.Methodology.
We follow the evaluation proposed by the authors: for each pair of concept-relation, wekeep the score of the most similar relatum associated to that pair of concept-relation.
Thus, for eachconcept, we have eight scores, one for each relation.
We normalize these eight scores (mean: 0, std: 1),in order to reduce concept-specific effects.
We then report the score distributions for each relation as boxplots in Figure 3 (left).Results.
We observe that the co-hyponymy relation is the best captured relation by a large margin.It is followed by the hypernymy and meronymy relations.
The random noun relation is prefered overthe attribute and the event relations.
This happens because words with different part-of-speeches tendto appear in different semantic classes.
It is thus impossible to compare words with different parts-of-speeches and thus to capture relation such as the event or the attribute relation as defined in the BLESSdataset.
It is however possible to make a more principled use of the model to overcome this issue.Comparing adjectives with nouns and nouns with verbs.
In syntactic relations between nouns andadjectives, the noun is the head word and the adjective is the dependent.
Similarly, in syntactic relationsbetween nouns and verbs, most often the verb is the head and the noun is the dependent.
Given a vectorvarepresenting an adjective and a vector vnrepresenting a noun, it is thus natural to left multiply them bythe transition matrix of the model to obtain a vector uacomparable to nouns and a vector uncomparableto verbs:ua= T>vaand un= T>vn.1452small houseemphasise needscholar write bookc2c1w2w1c1c2w1w2c2c1c3w2w1w3Figure 4: Graphical models used to compute in-context word representations for the compositional tasks.We report in Figure 3 (right) the new score distributions obtained when adjective and noun representa-tions are transformed before being compared to nouns and verbs.
We observe that, when using thesetransformations, the attribute and event relations are better captured than the random relations.
Thisdemonstrates that the transition matrix T captures selectional preferences.6 Compositional semanticsSo far, we have only evaluated how well our representations are able to capture the meaning of wordstaken as individual and independent units.
However, natural languages are highly compositional, and itis reasonable to assume that the meaning of a sentence or a phrase can be deduced from the meanings ofits parts and the syntactic relations between them.
This assumption is known as the principle of semanticcompositionality.In this section, we thus evaluate our representations on semantic composition tasks.
More precisely, wedetermine if using in-context word representations helps to compute the similarity between short phrasessuch as adjective-noun, verb-object, compound-noun or subject-verb-object phrases.
We use two datasetsof human similarity scores, introduced respectively by Mitchell and Lapata (2010) and Grefenstette andSadrzadeh (2011).6.1 MethodologyWe compare different ways to obtain a representation of a short phrase given our model.
First, as abaseline, we represent a phrase by the out-of-context representation of its head word.
In that case, thereis no composition at all.
Second, following Mitchell and Lapata (2008), we represent a phrase by thesum of the out-of-context representations of the words forming that phrase.
Third, we represent a phraseby the in-context representation of its head word.
Finally, we represent a phrase by the sum of the twoin-context representations of the words forming that phrase.
The graphical models used to compute in-context word representations are represented in Fig 4.
The probability distribution p(c1) of the head?ssemantic class is set to the uniform distribution (and not to the initial class distribution pT(ck| c0= 0)).6.2 DatasetsThe first dataset we consider was introduced by Mitchell and Lapata (2010), and is composed of pairs ofadjective-noun, compound-noun and verb-object phrases, whose similarities were evaluated by humansubjects on a 1?
7 scale.
We compare our results with the one reported by (Mitchell and Lapata, 2010).The second dataset we consider was introduced by Grefenstette and Sadrzadeh (2011).
Each example ofthis dataset consists in a triple of subject-verb-object, forming a small transitive sentence, and a landmarkverb.
Human subjects were asked to evaluate the similarity between the verb and its landmark in thecontext of the small sentence.
Following Van de Cruys et al.
(2013), we compare the contextualized verbwith the non-contextualized landmark, meaning that the landmark is always represented by its out-of-context representation.
We do so because it is believed to better capture the compositional ability of ourmodel and it works better in practice.
We compare our results with the one reported by Van de Cruys etal.
(2013).1453AN NN VNhead (out-of-context) 0.44 0.26 0.41add (out-of-context) 0.50 0.45 0.42head (in-context) 0.49 0.42 0.43add (in-context) 0.51 0.46 0.41M&L (vector space model) 0.46 0.49 0.38Humans 0.52 0.49 0.55SVOhead (out-of-context) 0.25add (out-of-context) 0.25head (in-context) 0.41add (in-context) 0.40Van de Cruys et al.
0.37Humans 0.62Table 2: Spearman?s rank correlation coefficients between human similarity judgements and similaritycomputed by our models on the Mitchell and Lapata (2010) dataset (left) and on the Grefenstette andSadrzadeh (2011) dataset (right).
AN stands for adjective-noun, NN stands for compoundnoun and VNstands for verb-object.6.3 DiscussionBefore discussing the results, it is interesting to note that our approach provides a way to evaluate theimportance of disambiguation for compositional semantics.
Indeed, the in-context representations pro-posed in this paper are a way to disambiguate their out-of-context equivalents.
It was previously noted byReddy et al.
(2011) that disambiguating the vectorial representations of words improve the performanceon compositional tasks.Mitchell and Lapata (2010) dataset.
We report results on the Mitchell and Lapata (2010) dataset inTable 2 (left).
Overall, in-context representations achieves better performance than out-of-context ones.For the adjective-noun pairs and the verb-noun pairs, using only the in-context representation of the headword works almost as well (AN) or even better (VN) than adding the representations of the two wordsforming a pair.
This means that for those particular tasks, disambiguation plays an important role.
Onthe other hand, this is not the case for the noun-noun pairs.
On that task, most improvement over thebaseline comes from the add operation.Grefenstette and Sadrzadeh (2011) dataset.
We report results in Table 2 (right).
First, we observethat in-context representations clearly outperform out-of-context ones.
Second, we note that adding thesubject, object and verb representations does not improve the result over only using the representation ofthe verb.
These two conclusions are not really surprising since this task is mainly a disambiguation task,and disambiguation is achieved by using the in-context representations.
We also note that our approachyields better results than those obtained by Van de Cruys et al.
(2013), while their method was specificallydesigned to model subject-verb-object triples.7 Conclusion and future workIn this article, we introduced a new approach to distributional semantics, based on a generative modelof sentences.
This model is somehow to latent Dirichlet allocation as structured vector space models areto latent semantic analysis.
Indeed, our approach is based on a probabilistic model of sentences, whichtakes the syntax into account by using dependency trees.
Similarly to LDA, our model can be viewedas a topic model, the main difference being that the topics are generated using a Markov process on asyntactic dependency tree instead of using a Dirichlet process.The approach we propose seems quite competitive with other distributional models of semantics.
Inparticular, we match or outperform state-of-the-art methods on semantic compositionality tasks.
Thanksto its probabilistic nature, it is very easy to derive word representations for various tasks: the same modelcan be used to compute in-context word representations for adjective-noun phrases, subject-verb-objecttriples or even full sentences, which is not the case of the tensor based approach proposed by Van deCruys et al.
(2013).1454Currently, the model of sentences does not use the dependency labels, which is the most significantlimitation that we would like to address in future work.
We also plan to explore spectral methods (Anand-kumar et al., 2012) to provide better initialization for learning the parameters of the model.
Indeed, webelieve this could speed up learning and yields better results, since the expectation-maximization al-gorithm is quite sensitive to bad initialization.
Finally, the code corresponding to this article will beavailable on the first author webpage.AcknowledgmentsEdouard Grave is supported by a grant from INRIA (Associated-team STATWEB).
Francis Bach ispartially supported by the European Research Council (SIERRA Project)ReferencesE.
Agirre, E. Alfonseca, K. Hall, J. Kravalova, M. Pas?ca, and A. Soroa.
2009.
A study on similarity and relatednessusing distributional and wordnet-based approaches.
In Proceedings of Human Language Technologies: The2009 Annual Conference of the North American Chapter of the Association for Computational Linguistics.A.
Anandkumar, R. Ge, D. Hsu, S. M. Kakade, and M. Telgarsky.
2012.
Tensor decompositions for learning latentvariable models.
arXiv preprint arXiv:1210.7559.M.
Baroni and A. Lenci.
2010.
Distributional memory: A general framework for corpus-based semantics.
Com-putational Linguistics, 36(4):673?721.M.
Baroni and A. Lenci.
2011.
How we blessed distributional semantic evaluation.
In Proceedings of the GEMS2011 Workshop on GEometrical Models of Natural Language Semantics, pages 1?10.
Association for Compu-tational Linguistics.M.
Baroni and R. Zamparelli.
2010.
Nouns are vectors, adjectives are matrices: Representing adjective-nounconstructions in semantic space.
In Proceedings of the 2010 Conference on Empirical Methods in NaturalLanguage Processing.M.
Baroni, S. Bernardini, A. Ferraresi, and E. Zanchetta.
2009.
The WaCky wide web: a collection of very largelinguistically processed web-crawled corpora.
Language resources and evaluation, 43(3):209?226.D.
M. Blei, A. Y. Ng, and M. I. Jordan.
2003.
Latent dirichlet allocation.
The Journal of Machine LearningResearch.I.
Borg.
2005.
Modern multidimensional scaling: Theory and applications.
Springer.E.
Bruni, G. Boleda, M. Baroni, and N. K. Tran.
2012.
Distributional semantics in technicolor.
In Proceedingsof the 50th Annual Meeting of the Association for Computational Linguistics: Long Papers-Volume 1, pages136?145.
Association for Computational Linguistics.S.
Clark and S. Pulman.
2007.
Combining symbolic and distributional models of meaning.
In AAAI SpringSymposium: Quantum Interaction, pages 52?55.B.
Coecke, M. Sadrzadeh, and S. Clark.
2010.
Mathematical foundations for a compositional distributional modelof meaning.
arXiv preprint arXiv:1003.4394.J.
R. Curran and M. Moens.
2002.
Scaling context space.
In Proceedings of the 40th Annual Meeting on Associa-tion for Computational Linguistics.S.
Deerwester, S. T. Dumais, G. W. Furnas, T. K. Landauer, and R. Harshman.
1990.
Indexing by latent semanticanalysis.
Journal of the American society for information science.G.
Dinu and M. Lapata.
2010.
Measuring distributional similarity in context.
In Proceedings of the 2010 Confer-ence on Empirical Methods in Natural Language Processing.K.
Erk and S. Pad?o.
2008.
A structured vector space model for word meaning in context.
In Proceedings of the2008 Conference on Empirical Methods in Natural Language Processing.L.
Finkelstein, E. Gabrilovich, Y. Matias, E. Rivlin, Z. Solan, G. Wolfman, and E. Ruppin.
2001.
Placing searchin context: The concept revisited.
In Proceedings of the 10th international conference on World Wide Web.1455J.
R. Firth.
1957.
A synopsis of linguistic theory, 1930-1955.E.
Grave, G. Obozinski, and F. Bach.
2013.
Hidden Markov tree models for semantic class induction.
In Proceed-ings of the Seventeenth Conference on Computational Natural Language Learning.E.
Grefenstette and M. Sadrzadeh.
2011.
Experimental support for a categorical compositional distributionalmodel of meaning.
In Proceedings of the 2011 Conference on Empirical Methods in Natural Language Pro-cessing.E.
Guevara.
2010.
A regression model of adjective-noun compositionality in distributional semantics.
In Proceed-ings of the 2010 Workshop on GEometrical Models of Natural Language Semantics.Z.
S. Harris.
1954.
Distributional structure.
Springer.T.
Hofmann.
1999.
Probabilistic latent semantic analysis.
In Proceedings of the Fifteenth conference on Uncer-tainty in artificial intelligence.R.
Jenatton, N. Le Roux, A. Bordes, and G. Obozinski.
2012.
A latent factor model for highly multi-relationaldata.
In Advances in Neural Information Processing Systems 25.T.
K Landauer and S. T. Dumais.
1997.
A solution to Plato?s problem: The latent semantic analysis theory ofacquisition, induction, and representation of knowledge.
Psychological review.D.
Lin.
1998.
Automatic retrieval and clustering of similar words.
In Proceedings of the 17th internationalconference on Computational linguistics-volume 2.K.
Lund and C. Burgess.
1996.
Producing high-dimensional semantic spaces from lexical co-occurrence.
Behav-ior Research Methods, Instruments, & Computers.J.
Mitchell and M. Lapata.
2008.
Vector-based models of semantic composition.
In Proceedings of the 46thAnnual Meeting of the Association of Computational Linguistics.J.
Mitchell and M. Lapata.
2010.
Composition in distributional models of semantics.
Cognitive Science.J.
Nivre, J.
Hall, J. Nilsson, A. Chanev, G. Eryigit, S. K?ubler, S. Marinov, and E. Marsi.
2007.
Maltparser: Alanguage-independent system for data-driven dependency parsing.
Natural Language Engineering.S.
Pad?o and M. Lapata.
2007.
Dependency-based construction of semantic space models.
Computational Linguis-tics.S.
Reddy, I. P. Klapaftis, D. McCarthy, and S. Manandhar.
2011.
Dynamic and static prototype vectors forsemantic composition.
In IJCNLP, pages 705?713.H.
Rubenstein and J.
B. Goodenough.
1965.
Contextual correlates of synonymy.
Communications of the ACM,8(10):627?633.H.
Schmid.
1994.
Probabilistic part-of-speech tagging using decision trees.
In Proceedings of internationalconference on new methods in language processing.H.
Schutze.
1992.
Dimensions of meaning.
In Supercomputing?92.
Proceedings.
IEEE.R.
Socher, B. Huval, C. D. Manning, and A. Y. Ng.
2012.
Semantic compositionality through recursive matrix-vector spaces.
In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Pro-cessing and Computational Natural Language Learning.S.
Thater, H. F?urstenau, and M. Pinkal.
2010.
Contextualizing semantic representations using syntactically en-riched vector models.
In Proceedings of the 48th Annual Meeting of the Association for Computational Lin-guistics.P.
D. Turney.
2006.
Similarity of semantic relations.
Computational Linguistics.T.
Van de Cruys, T. Poibeau, and A. Korhonen.
2013.
A tensor-based factorization model of semantic composi-tionality.
In Proceedings of NAACL-HLT.1456
