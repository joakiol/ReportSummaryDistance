The 7th Workshop on the Innovative Use of NLP for Building Educational Applications, pages 105?115,Montre?al, Canada, June 3-8, 2012. c?2012 Association for Computational LinguisticsOn using context for automatic correctionof non-word misspellings in student essaysMichael Flor Yoko FutagiEducational Testing Service Educational Testing ServiceRosedale Road Rosedale RoadPrinceton, NJ, 08541, USA  Princeton, NJ, 08541, USAmflor@ets.org yfutagi@ets.orgAbstractIn this paper we present a new spell-checkingsystem that utilizes contextual information forautomatic correction of non-word misspel-lings.
The system is evaluated with a largecorpus of essays written by native and non-native speakers of English to the writingprompts of high-stakes standardized tests(TOEFL?
and GRE?).
We also present com-parative evaluations with Aspell and the spel-ler from Microsoft Office 2007.
Usingcontext-informed re-ranking of candidate sug-gestions, our system exhibits superior error-correction results overall and also corrects er-rors generated by non-native English writerswith almost same rate of success as it does forwriters who are native English speakers.1 IntroductionMisspellings are ubiquitous in student writing.Connors and Lunsford (1988) have found that spel-ling errors accounted for about one quarter of allerrors found in a random sample of 300 studentessays.
Desmet and Balthazor (2006) found thatspelling errors are among the five most frequenterrors in first-year college composition of US stu-dents.
Lunsford and Lunsford (2008) found thatspelling errors constituted about 6.5% of all errorsfound in a US national sample of 3000 collegecomposition essays, despite the fact that writershad access to spellcheckers.Misspellings are even more ubiquitous in textswritten by non-native speakers of English, espe-cially English Language Learners (ELL).
Thetypes of misspellings produced by L2 writers aretypically different from errors produced by nativespeakers (Hovermale, 2010; Al-Jarf, 2010; Okada,2005).In the area of automatic assessment of writing,detection of misspellings is utilized in computer-aided language learning applications and in someautomatic scoring systems, especially when feed-back to users is involved (Dikli, 2006; Warschauerand Ware, 2006).
Yet spelling errors may have adeeper influence on automated text assessment.
Asnoted by Nagata, et al (2011), sub-optimal auto-matic detection of grammar and mechanics errorsmay be attributed to poor performance of NLPtools over noisy text.Presence of spelling errors also hinders systemsthat require only lexical analysis of text (Landauer,et al , 2003; P?rez, et al, 2004).
Granger andWynne (1999) have shown that spelling errors canaffect automated estimates of lexical variation,which in turn are used as predictors of text quality(Crossley, et al, 2008; Yu, 2010).
In the context ofautomated preposition and determiner error correc-tion in L2 English, De Felice and Pulman (2008)noted that the process is often disrupted by miss-pellings.
Futagi (2010) described how misspellingspose problems in development of a tool for detec-tion of phraseological collocation errors.Given this state of affairs, it is only natural forautomatic text assessment systems to utilize auto-matic spellchecking components.
However, gener-ic spellcheckers are typically oriented for errorsproduced by writers who are native speakers of alanguage.
Rimrott and Heift (2008, 2005) havedemonstrated that a generic speller has poor per-formance on data from German language learners.105Bestgen and Granger (2011) and Hovermale(2010) have demonstrated similar results on datafrom ELL.Many researchers have suggested that spell-checkers for L2 users need to be adapted for theparticular patterns of errors that characterize eachnative language (L1), by studying patterns of inter-ference and influence from L1 to L2 (Mitton andOkada, 2007; Mitton, 1996; Rimrott and Heift,2008, 2005; Bestgen and Granger, 2011; Hover-male, 2010).
We have set up to explore a differentpath, in the context of automated text assessment.Our goal in the present study is to examine to whatextent detection and automatic correction of non-word misspellings can be improved by utilizingessay context, for data from both native and non-native English speakers.The rest of this paper is organized as follows.Section 2 provides a description of the corpus oftexts and misspellings that was used in this study.Section 3 describes the ConSpel automatic spell-checking system.
Section 4 presents results from acomparative evaluation of our system, ConSpel,the popular Aspell speller and the Microsoft Office2007 speller.
Section 5 compares our findings withsome recent studies and discusses implications forfurther development of automatic spell-checkingsystems.2 CorpusThe corpus used in this study is a collection of es-says, annotated for misspellings by trained annota-tors.
It is developed for evaluation of automaticspellcheckers, and for research on patterns ofmisspellings produced by both native Englishspeakers and ELL.2.1 TextsThe corpus comprises essays written by exami-nees on the writing sections of GRE?
(GraduateRecord Examinations) and TOEFL?
(Test of Eng-lish as a Foreign Language) (ETS, 2011a,b).
TheTOEFL test includes two different writing tasks: ashort opinion essay, on a pre-assigned topic, and asummary essay that compares arguments from twodifferent sources (both supplied during the test).GRE also includes two different writing tasks: oneis a short argumentative essay taking a position onan assigned topic, the other is an essay evaluatingthe soundness of arguments presented in prompt.Both tests are delivered on computer (at test cen-ters around the world and via Internet), always us-ing the standard English language computerkeyboard (QWERTY).
Editing tools such as aspellchecker are not provided in the test-deliverysoftware (ETS, 2011a).
All writing tasks have timeconstraints.In the current phase of the project, the corpusincludes 3000 essays, for a total of 963,428 words.The essays were selected equally from the twotests (4 tasks, 10 prompts per task, 75 essays perprompt), also covering full range of scores (as aproxy for English proficiency levels) for each task.The majority of essays in this collection were writ-ten by examinees for whom English is not the firstlanguage (98.73% of TOEFL essays, 57.86% ofGRE essays).2.2 AnnotationEach text was independently reviewed by twoannotators, who are native English speakers expe-rienced in linguistic annotation.
Annotators wereasked to identify all non-word misspellings andprovide the adequate correction for each one.
Inter-annotator agreement was quite high - annotatorsagreed in 82.6% of the cases (Cohen?s Kappa=0.8,p<.001).
All disagreements were resolved by athird annotator (adjudicator).
For details of the an-notation procedure, see Flor and Futagi (2011).The Annotation Scheme for this project providesthree classes of misspellings, as summarized inTable 1.
Classification of annotated misspellingswas automatic.Type Description Count in corpus1 single token non-word(e.g.
?businees?, ?inthe?
)21,1602 single token non-word for which noplausible correction was found523 multi-token non-word misspelling(e.g.
?mor efun?
for ?more fun?
)383Total 21,595Table 1.
Classification of misspellingsannotated in the study corpus.106The annotation effort focused specifically onmisspellings, rather than on a wider category oforthographic errors in general.
The annotation ig-nored repeated words, missing spaces1 and impro-per capitalization.
Many of the essays haveinconsistent capitalization and essays written fullyin capital letters are not uncommon (not only inour corpus).
In addition, different spelling variantswere acceptable.
This consideration stems from theinternational nature of the two tests ?
the exami-nees come from all around the world, being accus-tomed to either British, American, or some otherEnglish spelling standard; so, it is only fair to ac-cept all of them.Overall, the annotated corpus of 3,000 essayshas the following statistics.
Average essay length is321 words (the range is 28-798 words).
148 essaysturned out to have no misspellings at all.
Totalspelling error counts are given in Table 1; 2.24%of the words in the corpus are non-word misspel-lings.3 Spelling correction systems3.1 BackgroundClassic approaches to the problem of spelling cor-rection of non-word errors were reviewed by Ku-kich (1992).
The typical approach for errordetection is using good spelling dictionaries.
Thetypical approach for correction of non-word errorsis to include modules for computing edit distance(Damerau, 1964; Levenshtein, 1966) and phoneticsimilarity.
These are used for ranking suggestionsby their orthographic and phonetic similarity to themisspelled word.
A more recent feature utilizesword frequency data for candidate ranking.
Mitton(2009) and Deorowicz and Ciura (2005) describestate of the art approaches to non-word correctionwithout contextual information.The use of context for spelling correction wasinitially proposed by Mayes, et al (1991) only for?contextual spelling?
?
correcting real-word errors(e.g.
writing ?fig?
instead of ?fog?).
A commonstrategy for this task is using pre-defined confusionsets, which makes it more amenable to classifier-based approaches (Golding and Roth, 1999).
Sev-1 Annotation ignored missing spaces around punctuation (e.g.
?chairs,tables?, but all cases where missing spaces result infused words were marked in annotation (e.g.
?inthe?
).eral recent studies used a web-scale language mod-el (Google Web1T n-gram corpus ?
Brants andFranz, 2006) for ?context-sensitive?
(i.e.
real-words) spelling correction (Bergsma, et al, 2009;Islam and Inkpen, 2009; Carlson and Fette, 2007).Chen, et al (2007) used a LM for pruning candi-date corrections for non-words in web queries.Whitelaw, et al (2009) used a LM for correctingnon-word and real-word errors without a dictionaryand using a statistically trained error model.
Ourstudy extends the use of language models to auto-matic correction of non-word errors, with a dictio-nary, but without any explicit error model.3.2 ConSpel systemThe ConSpel system was designed and imple-mented as a fully automatic system for detectionand correction of spelling errors.
The current ver-sion is focused on non-word misspellings.
The sys-tem has two intended uses.
One is to serve as acomponent in NLP systems for automatic evalua-tion of student essays.
The other use is to facilitateautomation for research on patterns of misspellingsin ELL essays.In ConSpel, detection policy is quite simple.
Atoken in a text is potentially a misspelling if thestring is not in the system dictionaries.
A text mayinclude some non-dictionary tokens that systemati-cally are not misspellings.
ConSpel has severalparameterized options to handle such cases.
Bydefault, the system will ignore numbers, dates, weband email addresses, and mixed alpha-numericstrings (e.g.
?RV400?).
The system can be in-structed to ignore capitalized words (e.g.
?Lon-don?)
and/or words in all uppercase (e.g.
?ROME?
).ConSpel spelling dictionaries include about360,000 entries.
The core set includes 245,000 en-tries, providing a comprehensive coverage of mod-ern English vocabulary.
This lexicon includes allinflectional variants for a given word (e.g.
?love?,?loved?, ?loves?, ?loving?
), and international spel-ling variants (e.g.
American and British English).Additional dictionaries include about 120,000 en-tries for international surnames and first names,and names for geographical places.Dictionaries are also the source of suggestedcorrections.
Candidate suggestions for each de-tected misspelling are generated by returning alldictionary words that have an edit distance up to agiven threshold.
With the default threshold of 5, a107misspelling can easily get hundreds of correctioncandidates.
Since ConSpel is intended to work onELL data, and ELL misspellings can be quite dis-similar from the intended words, starting with alarge number of candidates is a deliberate strategyto ensure that the adequate correction will be in-cluded in the candidate set.
Candidates are prunedduring the re-ranking process, so that only a fewcandidates from the initial set survive to the finaldecision making stage.Candidate suggestions for each detected miss-pelling are ranked using a set of algorithms.
Anedit distance module is used to compute ortho-graphic similarity between each candidate and theoriginal misspelling.
Phonetic similarity is com-puted using the Double Metaphone algorithm (Phi-lips, 2000).
Word frequency is computed for eachcandidate using a very large word-frequency datasource.The main thrust of our new spelling correctionsystem is the conjecture that non-word misspel-lings can be corrected better when their context istaken into account.Local context (several words around the miss-pelled word in the text) provides lots of informa-tion for choosing the adequate correction.
For eachcandidate, we check the frequency of its co-occurrence (in a language model) with the adjacentwords in the text.
This approach borrows from thefamily of noisy-channel error-correction models(Zhang, et al, 2006; Cucerzan and Brill, 2004;Kernigham, et al, 1990).
With the advent of verylarge word n-gram language models, we can utilizelarge contexts (about 4 words on each side of amisspelling).
Our current language model uses afiltered version of the Google Web1T collection,containing 1,881,244,352 n-gram types of size 1-5,with punctuation included.2 Notably, ConSpel doesnot use any statistical error model.A second context-sensitive algorithm utilizesnon-local context in the essay.
The idea is quitesimple ?
given a misspelled token in a text and aset of correction-candidates for that word, for eachcandidate we check whether that candidate stringoccurs elsewhere in the text.
Since content wordshave some tendency of recurrence in same text, the2 ConSpel system uses the TrendStream n-gram compressionsoftware library (Flor, 2012) for fast and memory efficientretrieval of n-gram data.
As a result, the ConSpel system runseven on modest hardware (e.g.
a 4GB RAM laptop), concur-rently with other applications.misspelled token might be such a case, and thecandidate should be strengthened.
The idea issomewhat similar to cache-based language modeladaptation (Kuhn and De Mori, 1990), thoughthere are considerable differences.
First, our sys-tem looks not only in preceding context, but overthe whole essay text.
Second, and unique to oursystem, ConSpel looks not only in the text, but alsointo the k-best candidate correction lists of the oth-er misspelled words.
Thus, if a word is systemati-cally misspelled in a document, ConSpel willstrengthen a candidate correction that appears as acandidate for multiple misspelled instances.3For each misspelling found in a text, each algo-rithm produces ranking scores for each candidate.We use a linear-weighted ensemble method tocombine scores from different algorithms.
First,scores for all candidates of a given misspelling arenormalized into a 0-1 range, separately for eachranker.
Normalized scores are then summed usinga set of constant weights.4The ConSpel system is implemented as a flexi-ble configurable system.
Configuration settingsinclude choice of dictionaries, choice of algorithmsand weights for computing the final ranking, andchoice of the output formats.4 Comparative evaluationIn this section we report the results of evaluationon data from our gold-standard corpus of 3,000essays described in section 2.
This evaluation fo-cuses on detection and correction of the 21,212single-token non-word misspellings (types 1 and 2in Table 1) as well as false alarms raised by spell-checkers.Evaluation included three systems.
In addition toConSpel, we tested Aspell (version 0.60.6), a pop-ular open-source spell checking library (Atkinson,2011).
The third system is spellchecker included inMicrosoft Office 2007 (hereafter ?MS Word?
).All evaluations were performed ?in full context?
(rather than word-by-word) ?
each essay in thecorpus was submitted to each system separately, asa simple text file.
All evaluations used standard3 A detailed comparative study of different context utilizationmethods is under way.4 The current weights were found experimentally, prior to theannotation effort described in this article.
We intend to usemachine learning methods in future research, using the anno-tated corpus for this purpose.108measures of recall, precision and F-score (Leacock,et al, 2010).Evaluations for Aspell and MS Word were con-ducted twice ?
once with their original dictiona-ries5 and once with the ConSpel spelling dictionaryof about 360,000 word forms.
Evaluations whereAspell and MS Word were bundled with ConSpeldictionary are marked below as Aspell+ and MSWord+.4.1 Error DetectionDetection results for non-word misspellings arepresented in Table 2.
All systems show very strongrecall rates, above 99%.
There is more variabilitywhen precision of error detection is concerned.Both MS Word and Aspell benefit from using thelarger dictionary ?
they raise much less falsealarms than with original dictionaries (Aspell im-proves precision by about 4% and MS Word byabout 6%).
ConSpel shows best precision, the dif-ference with second-best (MS Word+) is statisti-cally significant at p<.01.System Recall Precision F-scoreAspell 99.45 86.66 92.62Aspell+ 99.14 90.92 94.85ConSpel 99.40 98.43 98.91MS Word 99.55 90.26 94.68MS Word+ 99.32 96.16 97.71Table 2.
Evaluation results: non-word error detection4.2 Error CorrectionFor evaluating spelling correction, we again usethe measures of recall, precision and F-score.
Notethat precision of error correction is defined as pro-portion of adequately corrected misspellings out oftotal number of misspellings that a system tried tocorrect (this excludes cases missed in detection).We conducted error-correction evaluations withConSpel in two variants.
The baseline variant,ConSpel-A, ranks candidate suggestions using editdistance, phonetic similarity and word-frequency.5 Notably, both Aspell and MS Word in this evaluation camewith respective default dictionaries for US spelling, and gen-erated many false alarms when flagging words that are Britishand other international spelling variants.
Such false alarmcases were discounted from the evaluation statistics.The contextual variant, ConSpel-B, adds contex-tual information in the ranking process.Results of error-correction evaluations areshown in Table 3.
While MS Word speller pro-vided the adequate correction (top ranked sugges-tion) in about 73% of annotated cases, its precisionis only about 67-69%, due to large number of falsealarms.
Aspell has markedly lower accuracy ?
bothin recall and precision.
ConSpel-A has approx-imately same recall as MS-Word, but better preci-sion (due to low rate of false alarms).
ConSpel-B,which uses contextual information in ranking can-didate suggestions, shows markedly better recalland precision than either ConSpel-A or MS Word(statistically significant at p<.01).For Aspell, use of the larger spelling dictionaryimproved detection precision (fewer false alarms ?see Table 2), but it has led to degradation in errorcorrection ?
as shown in Table 3 (possibly rankingof candidates is affected by larger dictionaries).System Recall Precision F-scoreAspell 61.53 53.62 57.30Aspell+ 54.17 49.68 51.83ConSpel-A 72.65 71.94 72.29ConSpel-B 78.32 77.55 77.93MS Word 73.34 66.49 69.74MS Word+ 71.71 69.44 70.56Table 3.
Evaluation results: non-word error correction(top ranked candidates only)An additional way to evaluate automatic spel-ling correction is to consider how often the ade-quate target correction is found among the k-bestof the candidate suggestions (Mitton, 2009; Brilland Moore, 2000).
Figure 1 shows error-correctionrecall and precision results for four systems6 usingk-best values 1-5 and 10.When two-or-more best-ranked candidates areconsidered for each misspelling, the baseline Con-Spel-A system shows better performance than MSWord.
Aspell results lag significantly below theother systems, although it catches up with MS-Word beyond k=5.
ConSpel-B system outperformsall other systems, in both recall and precision.
It6 ?MS Word?
and ?MS Word+?
overlap for all values of k,except for k=1, thus only ?MS Word?
is shown in Figure 1.109places the target correction among the top twocandidates in 88% of cases, and among top three ormore candidates in beyond 90% of cases.Figure 1.
Error correction recall and precisionfor four systems, with different k-best cutoffs.4.3 Evaluation with data from native andnon-native English speakersIn this section we report the results of spell-checkevaluation with data breakdown by native and non-native English speakers.
Out of 21,212 single-token non-word misspellings in our corpus, 2,859came from 570 essays written by native Englishspeakers (NS) and 18,353 misspellings came from2,282 essays written by test-takers who are not na-tive speakers of English (NNS).Comparison of error-detection for five systemsis presented in Table 4.
All systems show verystrong recall results for both types of populations(all values are above 99%).
The results are a bitdifferent for error-detection precision.
ConSpelachieves best results in both populations (the dif-ferences with second-best, MS Word+, are statisti-cally significant at p<.01).
MS Word has precisionaround 91%, approximately same in both popula-tions.
Compared to MS Word, MS Word+ has bet-ter recall rates, in both populations ?
due to alarger dictionary, it raises much less false alarms.Aspell lags behind in this comparison.
Using alarger dictionary helps, as Aspell+ precision is bet-ter than that of Aspell in both populations; im-provement is manifest for NNS data and only 2%for NS data.
Aspell detection precision on NS data(77%) is lower than its precision on NNS data(88%).
This may be due to Aspell having a prob-lem with possessive forms (80% of the false alarmson NS data are possessives, but only 70% for NNSdata).7System  Recall Precision F-scoreAspell ns:nns:99.799.476.788.586.793.6Aspell+ ns:nns:99.699.378.793.387.996.3ConSpel ns:nns:99.599.496.298.897.999.1MS Word ns:nns:99.699.691.190.195.194.6MS Word+ ns:nns:99.299.394.496.596.797.9Table 4.
Evaluation results: percent correct fornon-word error detection, with breakdown for data fromnative (ns) and non-native (nns) English speakersResults of error-correction recall, with k-best le-vels 1-5 and 10, are presented in Figure 2.
In com-parisons of recall, with k=1, on NS data (rightpanel), MS Word (81.3%) and ConSpel-B (80.7%)show best results (the difference is not significant).For larger k-values, MS Word correction rate8 im-proves to a ceiling of about 88.5% and both Con-Spel variants have better improvement than MSWord.
The context-informed ConSpel-B systemhas error-correction recall above 90% for k?2 andreaches 94.2% at k=5.On NNS data, ConSpel-B has a clear advantageover all other systems.
At k=1, ConSpel-A and MSWord show equal correction performance (72%).For k ?2, ConSpel-A shows constant improvement,while MS Word improves to a ceiling of about85%.
For both NS and NNS populations, Aspellerror-correction performance lags considerablybehind the other systems, although it catches upwith and even outperforms MS Word for k?3.
Inte-restingly, Aspell+ performs consistently worsethan Aspell; the larger dictionary has detrimentaleffect on error-correction for Aspell, but not forMS Word.7 ConSpel dictionary does not contain possessive forms.8 Results for ?MS Word?
and ?MS Word+?
on this data overlapfor all values of k, in both populations.110Figure 2.
Error-correction recall for five systems, datafrom native (ns) and non-native (nns) English speakers.Error-correction precision results are shown inFigure 3.
Overall, ConSpel-B outperforms all othersystems, for both NS and NNS populations.
On NSdata, for k=1, MS Word+ (77%), ConSpel-A(76%) and MS Word (75%) are very close.
Fork?2, ConSpel-A shows better improvement, reach-ing 89.4% at k=5, while MS Word+ reaches a ceil-ing of about 85% (81% for MS Word).
Aspellperformance lags clearly behind the other systems,although it also improves considerably with largerk-values.
For NNS data, the separation betweensystems is even clearer.
Aspell lags behind, al-though it catches up to MS Word at k?5.Except for ConSpel-B, all systems have mani-festly better error-correction precision on NS datathan on NNS data ?
misspellings made by non-native English speakers are harder to correct.
Con-Spel-B, with context-informed ranking of spellingsuggestions, performs almost equally well for bothpopulations.
For k=1, its error-correction precisionis 77.5% for NNS data and 78% for NS data.
Fork=2, precision is 87.9% for NNS and 88.2% for NSdata.
These differences are not statistically signifi-cant.
For both populations, precision rises beyond90% for k?3.
ConSpel-B also shows remarkablyclose error-correction recall in both populations: atk=1, recall is 77.9% for NNS and 80.7% for NS; atk=2, recall is 88.4% for NNS, 91.4% for NS (thedifferences are statistically significant).
For k?3,recall is beyond 90% for both populations, withabout 2% advantage for NS population.Figure 3.
Error-correction precision for six systems, fornative (ns) and non-native (nns) English speakers.Table 5 presents F-scores for error-correctionevaluation results, for six systems, for k-best val-ues 1-5 and 10, for NS and NNS data.
For eachvalue of k, the ConSpel-B system has best valuesfor both NS and NNS data.
For each cell in Table5, we calculated the absolute difference betweenthe NS and NNS F-scores.
The results are shown inFigure 4.
Except for ConSpel-B, all systems havemarked differences in performance on NS andNNS data.
The differences tend to diminish forlarger k-values.
ConSpel-B is the only system forwhich the differences in error-correction betweenNS and NNS data are consistently below 2%, evenfor k=1.K-best: 1 2 3 4 5 10Aspell 60.654.965.962.375.672.877.076.578.978.980.381.8Aspell+ 56.949.661.855.674.069.876.474.078.577.382.482.9MS Word 78.268.483.276.084.478.584.979.585.180.185.280.6MS Word+ 78.769.384.878.786.981.487.382.487.583.087.983.9ConSpel-A 77.271.585.181.287.785.089.787.090.988.792.491.6ConSpel-B 79.377.789.888.191.690.592.191.392.691.793.292.5Table 5.
Error-correction evaluation results:F-scores for six systems, data from native (upper valuein each cell) and non-native English speakers111Figure 4.
Error-correction F-scores absolute differencesA question we need to address is whether thereare any real differences in misspellings producedby NS and NNS writers in our corpus.
Our initialanalyses show that there are some distinguishingcharacteristics.One characterization is obtained when we lookat the ?complexity?
of the error, defining it as theedit distance between misspelling and correctword.
The data is presented in Table 6.
NativeEnglish speakers make significantly more simpleerrors (edit distance 1) than non-native speakers,while the latter make more complex errors (editdistance 4+).Edit distance betweenmisspelling and correct form NS NNS1 83.3% *79.9%2 13.0% 14.0%3 3.1% 3.9%4+ 0.6% *2.1%Table 6.
Percent of non-word misspellings (tokens)by edit distance to correct word,for native and non-native populations.
* difference significant at p<.01Another difference we found in our data is thelength (number of characters) of the correct wordthat was misspelled, for each population (Figure5).
For words of length 2 to 7, non-native speakersproduce relatively more misspellings than nativespeakers.
For words of length 8 and longer, nativespeakers produce relatively more misspellings thannon-native speakers.ConSpel-B performs about the same on NS andNNS data, and better than the other systems.
Giventhe above differences of NS and NNS misspellingsin our corpus, and given that all evaluated systems,except ConSpel-B, show better correction on NSdata, we conclude that ConSpel-B shows this realadvantage due to utilization of contextual data.Figure 5.
Percent of non-word misspellings (tokens)by length of the ?intended?
correct word,for native and non-native populations.5 DiscussionLarge scale comparative studies of spellcheckerperformance on data from non-native languagespeakers are scarce, possibly due to large amountof effort required for expert annotation of data.Hovermale (2010) analyzed 500 spelling errorsfrom a corpus of essays produced by ELL in Japan.In that study, MS Word 2007 successfully cor-rected 72% of non-word errors, while Aspell had asuccess rate of 81% (presumably at k=1).
In ourstudy, with data from an international sample ofnon-native English speakers, Aspell error-correction precision rate is only 52% at k=1, andrises to 78% for k=5.
MS Word and ConSpel-A(no-context) begin with precision of about 75-77%at k=1.
At k=5, MS Word improves to about 85%,and ConSpel-A to above 89%.Bestgen and Granger (2011) analyzed 222 ar-gumentative essays from the ICLE corpus (Gran-ger et al, 2009), written by European EFL studentsacross different levels of English proficiency.Their sample included about 150,000 words andhad 1,549 spelling errors.
This amounts to spel-112ling-error rate of about 1%, compared to 2.2% inour data.
In that study, MS Word 2007 had detec-tion recall of 80.43%, and detection precision of82.35%.
In our study, MS Word had 99.6% recalland 90.1% precision in error detection.
The differ-ence may be attributed to the fact that we focus onsingle-token non-word misspellings, while Bestgenand Granger included other categories, specificallymulti-token errors.
Error-correction recall was 71%and precision 59% (at k=1).
In our study, at k=1,MS Word achieved 72% recall and 65% precision,which is quite close to the above figures.Given that our context-informed system has er-ror-correction F-score of 77.9% at k=1, and 91.8%at k=5, it is obvious that the system picks up theright corrections.
There is a potential for improve-ment, possibly by better ranking.
Why doesn?t thecontext help even more?
Could the system performwith 90% at k=1?
We have tentatively identifiedthree major types of influences that detract the sys-tem from better performance.
Those are a) localerror density; b) poor grammar; and c) competitionamong inflectional variants.
Local error densitymeans simply that adjacent words are misspelledso there is not enough reliable context to use n-grams in such cases.Poor grammar is also problematic for n-gram-based approach.
In a fragment ?If docter want tooperate, he...?, the intended word was ?doctor?, but?doctor want?
is a subject-verb agreement error,which is not frequent in the normative n-gram data.Thus, under n-gram frequency influence, the sys-tem prefers ?doctors?
as top ranked candidate.There is competition of inflectional variants inpresence of grammatical errors.We have observed that even in absence ofgrammatical errors, sometimes an inadequate topranked candidate is an inflectional variant of theadequate correction.
For example: ?They receivedfresh air, interacte with other youth their age, solvedproblems...?.
The adequate correction is 'interacted',but ConSpel ranks it third, while 'interacts' comessecond and 'interact' is ranked first.
Notably, non-local context is not always beneficial ?
for a exam-ple, the presence of word 'interact' elsewhere in theessay will strengthen the wrong candidate.
Possi-bly, additional linguistic information could helpimprove ranking in such cases, e.g.
by observingthat all verbs in this sentence come in past tense.Mitton (1996) suggested that it should be possi-ble to adapt a spellchecker to cope specificallywith L1-characteristic errors of English learners.Granger and Wynne (1999) analyzed misspellingsproduced by students with several different L1backgrounds and have also suggested that it mightbe ?useful to adapt tools such as spellcheckers tothe needs of non-native users.?
Mitton and Okada(2007) have demonstrated a successful adaptationof a spellchecker (oriented for native Englishspeakers) to Japanese learners of English.9 Howev-er, adaptation to each specific L1 would requireconsiderable resources.
As noted by Hovermale(2010), it is not clear whether it is worthwhile tocustomize spellchecker heuristics for each learnerpopulation or better to just have one ELL spell-checker.
Results from our study indicate that it is atleast feasible to produce a general-purpose spell-checker that can successfully correct misspellingsproduced by non-native English speakers, almostas well as it does for native English speakers.
Akey for such development is utilization of essaycontext for re-ranking of spelling suggestions.6 ConclusionsIn this paper we presented a method for context-informed correction of single-token non-word spel-ling errors.
Our results with ConSpel system dem-onstrate that utilizing contextual information helpsimprove automatic correction of non-word miss-pellings, for both native and non-native speakers ofEnglish, at least for essays written by test takers onstandardized English proficiency tests.
In futurework we intend to produce a detailed study of thedifferent ways of context utilization.
We also in-tend to expand the system to handle multi-wordspelling errors.AcknowledgmentsMany thanks to Chong Min Lee and DanielBlanchard for assisting in evaluation with Aspelland Microsoft Office 2007; to our annotators, Ni-cole DiCrecchio, Julia Farnum, Melissa Lopez,Susanne Miller, Matthew Mulholland, Sarah Ohls,and Waverely VanWinkle.
The manuscript hasalso benefited from the comments of three ano-nymous reviewers.9 Boyd (2009) used non-native (Japanese ELL) pronunciationmodeling to improve a speller that uses just an orthographicerror-model.
Her combined system achieved 65% correctionprecision at k=1, and 82.6% at k=5.
Our context-informedsystem achieves 77.5% and 91.4% respectively.113ReferencesReima Al-Jarf.
2010.
Spelling error corpora in EFL.Sino-US English Teaching, 7(1):6-15.Kevin Atkinson.
2011.
GNU Aspell.
Software availableat http://aspell.net.Shane Bergsma, Dekang Lin and Randy Goebel.
2009.Web-Scale N-gram Models for Lexical Disambigua-tion.
In Proceedings of International Joint Confe-rence on Artificial Intelligence (IJCAI-2009), pages1507-1512.Yves Bestgen and Sylviane Granger.
2011.
Categorisingspelling errors to assess L2 writing.
InternationalJournal of Continuing Engineering Education andLife-Long Learning, 21(2/3):235-252.Adriane Boyd.
2009.
Pronunciation Modeling in Spel-ling Correction for Writers of English as a ForeignLanguage.
In Proceedings of the NAACL HLT 2009Student Research Workshop and Doctoral Consor-tium, pages 31?36.Torsten Brants and Alex Franz.
2006.
Web 1T 5-gramVersion 1.
LDC2006T13.
Philadelphia, PA, USA:Linguistic Data Consortium.Eric Brill and Robert C. Moore.
2000.
An improvederror model for noisy channel spelling correction.
InProceedings of the 38th Annual Meeting of ACL,pages 286-293Andrew Carlson and Ian Fette.
2007.
Memory-BasedContext-Sensitive Spelling Correction at Web Scale.In Proceedings of the Sixth International Conferenceon Machine Learning and Applications, pages 166-171.Qing Chen, Mu Li and Ming Zhou.
2007.
Improvingquery spelling correction using web search results.
InProceedings of the 2007 Conference on EmpiricalMethods in Natural Language (EMNLP 2007), pages181-189.Robert Connors and Andrea A. Lunsford.
1988.
Fre-quency of Formal Error in Current College Writing,or Ma and Pa Kettle Do Research.
College Composi-tion and Communication, 39(4):395?409.Scott A. Crossley, Tom Salsbury, Philip McCarthy andDanielle S. McNamara.
2008.
Using latent semanticanalysis to explore second language lexical develop-ment.
In Wilson, D. and Chad Lane, H.
(Eds.
): Pro-ceedings of the 21st International Florida ArtificialIntelligence Research Society Conference, pages136?141.Silviu Cucerzan and Eric Brill.
2004.
Spelling correc-tion as an iterative process that exploits the collectiveknowledge of web users.
In Proceedings of the Con-ference on Empirical Methods in Natural LanguageProcessing (EMNLP-2004), pages 293?300.Frederick Damerau.
1964.
A technique for computerdetection and correction of spelling errors.
Commu-nications of the ACM, 7(3): 659-664.Rachele De Felice and Stephen G. Pulman.
2008.
AClassifier-Based Approach to Preposition and De-terminer Error Correction in L2 English.
In Proceed-ings of the 22nd International Conference onComputational Linguistics (COLING 2008), pages69-176.Sebastian Deorowicz and Marcin G. Ciura.
2005.
Cor-recting spelling errors by modelling their causes.
In-ternational Journal of Applied Mathematics andComputer Science, 15(2), pages 275?285.Christy Desmet and Ron Balthazor.
2006.
Finding Pat-terns in Textual Corpora: Data Mining, Research, andAssessment in First-year Composition.
Paper pre-sented at Computers and Writing 2006, Lubbock,Texas, May 25?29, 2006.Semire Dikli.
2006.
An overview of automated scoringof essays.
Journal of Technology, Learning, and As-sessment, 5(1):4-35. ejournals.bc.edu/ojs/index.php/jtla(last accessed on February 22, 2012).ETS.
2011a.
GRE?
: Introduction to the AnalyticalWriting Measure.
Educational Testing Service.www.ets.org/gre/revised_general/prepare/analytical_writing  (last accessed on March 9, 2012).ETS.
2011b.
TOEFL?
iBT?
Test Content.
EducationalTesting Service.
www.ets.org/toefl/ibt/about/content(last accessed on March 9, 2012).Michael Flor.
2012.
A fast and flexible architecture forvery large word n-gram datasets.
Natural LanguageEngineering.
Available on CJO 2012doi:10.1017/S1351324911000349journals.cambridge.org/action/displayJournal?jid=NLEMichael Flor and Yoko Futagi.
2011.
Producing an an-notated corpus with automatic spelling correction.Presented at the Learner Corpus Research 2011 Con-ference, 15-17 September 2011, Louvain-la-Neuve,Belgium.
Submitted for publication.Yoko Futagi.
2010.
The effects of learner errors on thedevelopment of a collocation detection tool.
In Pro-ceedings of the Fourth Workshop on Analytics forNoisy Unstructured Text Data (AND '10), pages 27-34.Sylviane Granger, Estelle Dagneaux, Fanny Meunierand Magali Paquot.
2009.
The International Corpusof Learner English.
Handbook and CD-ROM (Ver-sion 2), Presses Universitaires de Louvain, Louvain-la-Neuve.Sylviane Granger and Martin Wynne.
1999.
Optimisingmeasures of lexical variation in EFL learner corpora.in Kirk, J.
(Ed.
): Corpora Galore, pages 249?257,Rodopi, Amsterdam.Andrew Golding and Dan Roth.
1999.
A Winnow basedapproach to Context-Sensitive Spelling Correction.Machine Learning, 34(1-3):107-130.114DJ Hovermale.
2010.
An analysis of the spelling errorsof L2 English learners.
Presented at CALICO 2010Conference, Amherst, MA, USA, June 10-12, 2010.Available electronically from http://www.ling.ohio-state.edu/~djh/presentations/djh_CALICO2010.pptxAminul Islam and Diana Inkpen.
2009.
Real-word spel-ling correction using Google Web 1T n-gram withbackoff.
In Proceedings of the IEEE InternationalConference on Natural Language Processing andKnowledge Engineering (IEEE NLP-KE?09), pages1-8.Mark Kernighan, Kenneth Church and William Gale.1990.
A spelling correction program based on a noisychannel model.
In Proceedings of the 13th Confe-rence on Computational Linguistics (COLING ?90),pages 205-210.Roland Kuhn and Renato De Mori.
1990.
A cache-basednatural language model for speech recognition.
IEEETransactions on Pattern Analysis and Machine Intel-ligence, 12(6):570?583.Karen Kukich, 1992.
Techniques for automatically cor-recting words in text.
ACM Computing Surveys,24:377-439.Thomas K. Landauer, Darrell Laham and Peter Foltz.2003.
Automatic essay assessment.
Assessment inEducation, 10(3):295?308.Claudia Leacock, Martin Chodorow, Michael Gamon,and Joel Tetreault, 2010.
Automated grammatical er-ror detection for language learners.
Synthesis Lec-tures on Human Language Technologies, No.
9,Morgan & Claypool, Princeton, USA.Vladimir Levenshtein.
1966.
Binary codes capable ofcorrecting deletions, insertions and reversals.
SovietPhysics Doklady, 10:707-710.Andrea A. Lunsford and Karen J. Lunsford.
2008.
Mis-takes Are a Fact of Life: A National ComparativeStudy.
College Composition and Communication,59(4):781-806.Eric Mays, Fred J. Damerau and Robert L. Mercer.1991.
Context based spelling correction.
InformationProcessing & Management, 27(5):517?522.Roger Mitton.
1996.
English spelling and the computer.Harlow, Essex: Longman Group.
Available electron-ically from http://eprints.bbk.ac.uk/469Roger Mitton.
2009.
Ordering the suggestions of aspellchecker without using context.
Natural Lan-guage Engineering, 15(2):173?192.Roger Mitton and Takeshi Okada.
2007.
The adaptationof an English spellchecker for Japanese writers.
Pre-sented at: Symposium on Second Language Writing,15-17 Sept 2007, Nagoya, Japan.
Available electron-ically from http://eprints.bbk.ac.uk/592Ryo Nagata, Edward Whittaker and Vera Sheinman.2011.
Creating a manually error-tagged and shallow-parsed learner corpus.
In Proceedings of the 49th An-nual Meeting of the Association for ComputationalLinguistics, pages 1210?1219, Portland, Oregon: As-sociation for Computational Linguistics.Takeshi Okada.
2005.
Spelling errors made by JapaneseEFL writers: with reference to errors occurring at theword-initial and the word-final position.
In V. Cookand B. Bassetti (Ed.
), Second language writing sys-tems, pages 164-183.
Clevedon: Multilingual Mat-ters.Diana P?rez, Enrique Alfonseca and Pilar Rodr?guez.2004.
Application of the Bleu method for evaluatingfree-text answers in an e-learning environment.
Pro-ceedings of the Language Resources and EvaluationConference (LREC-2004), pages 1351-1354.Lawrence Philips.
2000.
The Double-metaphone SearchAlgorithm.
C/C++ User's Journal, June, 2000.Anne Rimrott and Trude Heift.
2005.
Language learnersand generic spell checkers in CALL.
CALICO Jour-nal, 23(1):17-48.Anne Rimrott and Trude Heift.
2008.
Evaluating auto-matic detection of misspellings in German.
LanguageLearning & Technology, 12(3):73-92.Casey Whitelaw, Ben Hutchinson, Grace Y Chung andGerard Ellis.
2009.
Using the Web for language in-dependent spellchecking and autocorrection.
In Pro-ceedings of the Conference on Empirical Methods inNatural Language Processing (EMNLP 2009), pages890-899.Mark Warschauer and Paige Ware.
2006.
Automatedwriting evaluation: defining the classroom researchagenda.
Language Teaching Research, 10(2):157?180.Guoxing Yu.
2010.
Lexical diversity in writing andspeaking task performances.
Applied Linguistics,31(2):236?259.Yang Zhang, Pilian He, Wei Xiang and Mu Li.
2006.Discriminative reranking for spelling correction.
InProceedings of the 20th Pacific Asia Conference onLanguage, Information and Computation, pages 64-71.115
