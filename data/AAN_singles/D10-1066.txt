Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 674?683,MIT, Massachusetts, USA, 9-11 October 2010. c?2010 Association for Computational LinguisticsTop-Down Nearly-Context-Sensitive ParsingEugene CharniakBrown Laboratory for Linguistic Information Processing (BLLIP)Brown University, Providence, RI 02912ec@cs.brown.eduAbstractWe present a new syntactic parser thatworks left-to-right and top down, thusmaintaining a fully-connected parse treefor a few alternative parse hypotheses.
Allof the commonly used statistical parsersuse context-free dynamic programming al-gorithms and as such work bottom up onthe entire sentence.
Thus they only finda complete fully connected parse at thevery end.
In contrast, both subjectiveand experimental evidence show that peo-ple understand a sentence word-to-word asthey go along, or close to it.
The con-straint that the parser keeps one or morefully connected syntactic trees is intendedto operationalize this cognitive fact.
Ourparser achieves a new best result for top-down parsers of 89.4%,a 20% error reduc-tion over the previous single-parser bestresult for parsers of this type of 86.8%(Roark, 2001).
The improved performanceis due to embracing the very large featureset available in exchange for giving up dy-namic programming.1 IntroductionWe present a new syntactic parser that workstop-down and left-to-right, maintaining a fully-connected parse tree for a few alternative parsehypotheses.
It is a Penn treebank (Marcus et al,1993) parser in that it is capable of parsing thePenn treebank test sets, and is trained on thenow standard training set.
It achieves a newbest result for this parser type.All of the commonly used statistical parsersavailable on the web such as the Collins(/Bikel)(Collins, 2003) Charniak-Johnson(Charniak andJohnson, 2005), and Petrov-Klein (Petrov etal., 2006), parsers use context-free dynamic pro-gramming algorithms so they work bottom upon the entire sentence.
Thus they only find acomplete fully-connected parse at the very end.In contrast human syntactic parsing must befully connected (or close to it) as people areable to apply vast amounts of real-world knowl-edge to the process as it proceeds from word-to-word(van Gompel and Pickering, 2007).
Thusany parser claiming cognitive plausibility must,to a first approximation, work in this left-to-right top-down fashion.Our parser obtains a new best result for top-down parsers of 89.4% (on section 23 of the PennTreebank).
This is a 20% error reduction overthe previous best single-parser result of 86.8%,achieved by Rork(Roark, 2001).Our model is in the tradition of this lat-ter parser.
The current work?s superior per-formance is not due to any innovation in ar-chitecture but in how probability distributionsare computed.
It differs from Roark in its ex-plicit recognition that by giving up context-freedynamic programming we may embrace nearcontext sensitivity and condition on many di-verse pieces of information.
(It is only ?near?because we still only condition on a finiteamount of information.)
This is made possi-ble by use of random-forests (Amit and Geman,1997; Breiman, 2004; Xu and Jelinek, 2004) tochoose features, provide smoothing, and finallydo the probability computation.
To the bestof our knowledge ours is the first application ofrandom-forests to parsing.674Section two describes previous work on thistype of parser, and in particular gives details onthe Roark architecture we use.
Section three de-scribes how random forests allow us to integratethe diverse information sources that context-sensitive parsing allows.
Section four gives im-plementation details.
Section five is devoted tothe main experimental finding of the paper alongwith subsidiary results showing the effects of thelarge feature set we now may use.
Finally, sec-tion six suggests that because this parser type iscomparatively little explored one may hope forfurther substantial improvements, and proposesavenues to be explored.2 Previous Work on Top-DownParsing and the Roark ModelWe care about top-down incremental parsing be-cause it automatically satisfies the criteria wehave established for cognitive plausibility.
Be-fore looking at previous work on this type modelwe briefly discuss work that does not meet thecriteria we have set out, but which people oftenassume does so.We are using the terms ?top-down?
and ?left-to-right?
following e.g., (Abney and Johnson,1991; Roark, 2001).
In particularIn top-down strategies a node is enu-merated before any of its descen-dents.
(Abney and Johnson, 1991)In this era of statistical parsers it is useful tothink in terms of possible conditioning informa-tion.
In typical bottom up CKY parsing whencreating, say, a constituent X from positions ito j we may not condition on its parent.
Thatthe grammar is ?context-free?
means that thisconstituent may be used anywhere.Using our definition, the Earley parsing al-gorithm(Earley, 1970), which is often cited as?top-down,?
is no such thing.
In fact, it is longbeen noted that the Earley algorithm is ?almostidentical?
(Graham et al, 1980) to CKY.
Again,when Earley posits an X it may not conditionon the parent.Similarly, consider the more recent work ofNivre(Nivre, 2003) and Henderson(Henderson,parse (w0,n?1)1 C[0](= h =< q, r, t >)?< 1, 1, ROOT >2 for i = 0, n3 do while ABOVE-THRESHOLD (h,C,N)4 remove h from C5 for all x such that p(x | t) > 06 let h?
=< q?, r?, t?
>7 where q?
= q ?
p(x | t),r?
= LAP(t?, w?
),and t?
= t ?
x8 if(x = w) then w?
= wi+1insert h?
in N9 else w?
= w10 insert h?
in C11 empty C12 exchange C and N13 output t(C[0]).Figure 1: Roark?s Fully-Connected Parsing Algo-rithm2003).
The reason these are not fully-connectedis the same.
While they are incremental parsers,they are not top down ?
both are shift-reduceparsers.
Consider a constituency shift-reducemechanism.
Suppose we have a context-free ruleS?
NP VP.
As we go left-to-right various termi-nal and non-terminals are added to and removedfrom the stack until at some point the top twoitems are NP and VP.
Then a reduce operationreplaces them with S. Note that this means thatnone of the words or sub-constituents of eitherthe NP or VP are integrated into a single overalltree until the very end.
This is clearly not fullyconnected.
Since Nivre?s parser is a dependencyparser this exact case does not apply (as it doesnot use CFG rules), but similar situations arise.In particular, whenever a word is dependent ona word that appears later in the string, it re-mains unconnected on the stack until the sec-ond word appears.
Naturally this is transitiveso that the parser can, and presumably does,process an unbounded number of words beforeconnecting them all together.Here we follow the work of Roark (Roark,2001) which is fully-connected.
The basic al-675Current hypotheses Prob of next tree element1 < 1.4 ?
10?3, 5 ?
10?2,(S (NP (NNS Terms)> p(x=?)?
| t)=.642 < 7 ?
10?5, 5 ?
10?2,(S (S (NP (NNS Terms)>3 < 9 ?
10?4, 8 ?
10?2,(S (NP (NNS Terms))> p(x=VP | t) =.884 p(x=S | t)= 2 ?
10?45 < 7 ?
10?5, 5 ?
10?2,(S (S (NP (NNS Terms)>6 < 9 ?
10?4, 9 ?
10?2,(S (NP (NNS Terms)) (VP> p(x=AUX | t)= .387 < 7 ?
10?5, 5 ?
10?2,(S (S (NP (NNS Terms)>8 < 2 ?
10?8, 9 ?
10?2,(S (NP (NNS Terms)) (S>9 < 3 ?
10?4, 2 ?
10?1,(S (NP (NNS Terms)) (VP (AUX > p(x=?were?
| t)= .2110 < 7 ?
10?5, 5 ?
10?2,(S (S (NP (NNS Terms)>11 < 2 ?
10?8, 9 ?
10?2,(S (NP (NNS Terms)) (S>12 < 7 ?
10?5, 3 ?
10?1,(S (NP (NNS Terms)) (VP (AUX were)>Figure 2: Parsing the second word of ?Terms were not disclosed.
?gorithm is given in Figure 1.
(Note we havesimplified the algorithm in several ways.)
Theinput to the parser is a string of n words w0,n?1.We pad the end of the string with an end-of-sentence marker wn = ?.
This has the specialproperty that p(?
| t) = 1 for a complete tree tof w0,n?1, zero otherwise.There are two priority queues of hypotheses,C (current), and N (next).
A hypothesis h is athree-tuple < q, r, t > where q is the probabil-ity assigned to the current tree t. In Figure 1 halways denotes C[0] the top-most element of C.While we call t the ?tree?, it is a vector represen-tation of a tree.
For example, the tree (ROOT)would be a vector of two elements, ROOT and?
)?, the latter indicating that the constituent la-beled root is completed.
Thus elements of thevector are the terminals, non-terminals, and ?)??
the close parenthesis element.
Lastly r is the?look-ahead probability?
or LAP.
LAP(w,h) is(a crude estimate of) the probability that thenext word is w given h. We explain its purposebelow.We go through the words one at a time.
Atthe start of our processing of wi we have hy-potheses on C ordered by p ?q ?
the probabilityof the hypothesis so far times an estimate q ofthe probability cost we encounter when tryingto now integrate wi.
We remove each h from Cand integrate a new tree symbol x.
If x = wiit means that we have successfully added thenew word to the tree and this hypothesis goesinto the queue for the next word N .
Other-wise h does not yet represent an extension towi and we put it back on C to compete with theother hypotheses waiting to be extended.
Thelook-ahead probability LAP(h) = q is intendedto keep a level playing field.
If we put h backonto C it?s probability p is lowered by the factorp(x | h).
On the other hand, if x is the correctsymbol, q should go up, so the two should offsetand h is still competitive.We stop processing a word and move ontothe next when ABOVE-THRESHOLD returnsfalse.
Without going into details, we haveadopted exactly the decision criteria and asso-ciated parameters used by Roark so that theaccuracy numbers presumably reflect the sameamount of search.
(The more liberal ABOVE-THRESHOLD, the more search, and presum-ably the more accurate results, everything elsebeing equal.
)Figure 2 shows a few points in the process-ing of the second word of ?Terms were not dis-closed.?
Lines one and two show the currentqueue at the start of processing.
Line one has676the ultimately correct partial tree (S (NP (NNSTerms).
Note that the NP is not closed off asthe parser defers closing constituents until nec-essary.
On the right of line 1 we show the pos-sible next tree pieces that could be added.
Herewe simply have one, namely a right parenthesisto close off the NP.
(In reality there would bemany such x?s.)
The result is that the hypoth-esis of line 1 is removed from the queue, anda new hypothesis is added back on C as thisnew hypothesis does not incorporate the secondword.Lines 3 and 5 now show the new state of C.Again we remove the top candidate from C. Theright-hand side of lines 3 and 4 show two pos-sible continuations for the h of line 3, start anew VP or a new S. With line 3 removed fromthe queue, and its two extensions added, we getthe new queue state shown in lines 6,7 and 8.Line 6 shows the top-most hypothesis extendedby an AUX.
This still has not yet incorporatedthe next word into the parse, so this extensionis inserted in the current queue giving us thequeue state shown in 9,10,11.
Finally line 9 isextended with the word ?were.?
This additionincorporates the current word, and the resultingextension, shown in line 12 is inserted in N , notC, ending this example.3 Random ForestsThe Roark model we emulate requires the esti-mation of two probability distributions: one forthe next tree element (non-terminals,terminals,and ?)?)
in the grammar, and one for the look-ahead probability of the yet-to-be-incorporatednext word.
In this section we use the first ofthese for illustration.We first consider how to construct a single(non-random) decision tree for estimating thisdistribution.
A tree is a fully-connected directedacyclic graph where each node has one input arc(except for the root) and, for reasons we go intolater, either zero or two output arcs ?
the treeis binary.
A node is a four-tuple < d, s, p, q >,where d is a set of training instances, p, a prob-ability distribution of the correct decisions forall of the examples in d, and q a binary ques-tion about the conditioning information for theexamples in d. The 0/1 answer to this ques-tion causes the decision-tree program to followthe left/right arc out of the node to the childrennodes.
If q is null, the node is a leaf node.
sis a strict subset of the domain of the q for theparent of h.Decision tree construction starts with the rootnode n where d consists of the several millionsituations in the training data where the nexttree element needs to be guessed (according toour probability distribution) based upon previ-ous words and the analysis so far.
At each itera-tion one node is selected from a queue of unpro-cessed nodes.
A question q is selected, and basedupon its answers two descendents n1 and n2 arecreated with d1 and d2 respectively, d1 ?
d2 = d.These are inserted in the queue of unprocessednodes and the process repeats.
Termination canbe handled in multiple ways.
We have chosento simply pick the number of nodes we create.Nodes left on the queue are the leaf nodes ofthe decision tree.
We pick nodes from the heapbased upon how much they increased the prob-ability of the data.Still open is the selection of q at each iter-ation.
First pick a query type qt from a usersupplied set.
In our case there are 27 types.
Ex-amples include the parent of the non-terminalto be created, its predecessor, 2 previous, etc.A complete list is given in Figure 4.
Note thatthe answers to these queries are not binary.Secondly we turn our qt into a binary questionby creating two disjoint sets s1 and s2 s1?s2 = swhere s is the domain of qt.
If a particular his-tory h ?
d is such that qt(h) = x and x ?
s1 thenh is put in d1.
Similarly for s2.
For example, ifqt is the parent relation, and the parent in h isNP, then h goes in d1 iff NP ?
s1.
We create thesets si by initializing them randomly, and thenfor each x ?
s try moving x to the opposite setsi.
If this results in a higher data probability wekeep it in its new si, otherwise it reverts to it?soriginal si.
This is repeated until no switch low-ers probability.
(Or were the a?s are individualwords, until no more than two words switch.
)We illustrate with a concrete example.
Oneimportant fact quickly impressed on the cre-677No.
Q S p(?of?)
p(?in?
)0 11 1 NN,IN 0.05 0.034 1 NNS,IN 0.09 0.0612 2 RB,IN 0.17 0.1116 3 PP,WHPP 0.27 0.1839 20 NP,NX 0.51 0.1640 20 S,VP 0.0004 0.19Figure 3: Some nodes in a decision tree for p(wi | t)ator of parsers is the propensity of preposi-tional phrases (PP) headed by ?of?
to attachto noun phrases (NP) rather than verb phrases(VP).
Here we illustrate how a decision tree forp(wi | t) captures this.
Some of the top nodes inthis decision tree are shown in Figure 3.
Eachline gives a node number, Q ?
the questionasked at that node, examples of answers, andprobabilities for ?of?
and ?in?.
Questions arespecified by the question type numbers given inFigure 4 in the next section.
Looking at node 0we see that the first question type is 1 ?
par-ent of the proposed word.
The children treesare 1 and 2.
We see that prepositions (IN) havebeen put in node 1.
Since this is a binary choice,about half the preterminals are covered in thisnode.
To get a feel for who is sharing this nodewith prepositions each line gives two examples.For node 1 this includes a lot of very differenttypes, including NN (common singular noun).Node 1 again asks about the preterminal,leading to node 4.
At this point NN has splitoff, but NNS (common plural noun) is still there.Node 4 again asks about the preterminal, lead-ing to node 12.
By this point IN is only groupedwith things that are much closer, e.g.
RB (ad-verb).Also note that at each node we give the prob-ability of both ?of?
and ?to?
given the questionsand answers leading to that node.
We can seethat the probability of ?of?
goes up from 0.05at node 1 to 0.27 at node 16.
The probabili-ties for ?to?
go in lockstep.
By node 16 we areconcentrating on prepositions heading preposi-tional phrases, but nothing has been asked thatwould distinguish between these two preposi-tions.
However, at node 16 we ask the ques-tion ?who is the grandparent?
leading to nodes39 and 40.
Node 39 is restricted to the answer?noun phrase?
and things that look very muchlike noun phrases ?
e.g., NX, a catchall for ab-normal noun phrases, while 40 is restricted toPP?s attaching to VP?s and S?s.
At this pointnote how the probability of ?of?
dramaticallyincreases for node 39, and decreases for 40.That the tree is binary forces the decisiontree to use information about words and non-terminals one bit at a time.
In particular, wecan now ask for a little information about manydifferent previous words in the sentence.We go from a single decision tree to a randomforest by creating many trees, randomly chang-ing the questions used at every node.
First notethat in our greedy selection of si?s the outcomedepends on the initial random assignment of a?s.Secondly, each qt produces its own binary ver-sion q.
Rather than picking the one that raisesthe data probability the most, we choose it withprobability m. With probability 1 ?
m we re-peat this procedure on the list of q?s minus thebest.
Given a forest of f trees we compute thefinal probability by taking the average over allthe trees:p(x | t) = 1f?j=1,fpj(x | t)where pj denotes the probability computed bytree j.4 Implementation DetailsWe have twenty seven basic query types asshown in Figure 4.
Each entry first gives identi-fication numbers for the query types followed bya description of types.
The description is fromthe point of view of the tree entry x to be addedto the tree.
So the first line of Figure 4 speci-fies six query types, the most local of which isthe label of the parent of x.
For example, if wehave the local context ?
(S (NP (DT the)?
andwe are assigning a probability to the pretermi-nal after DT, (e.g., NN) then the parent of x isNP.
Similarly one of the query types from linetwo is one-previous, which is DT.
Two previousis ?, signifying nothing in this position.6781-6 The non-terminal of the parent, grandpar-ent, parent3, up to parent67-10 The previous non-terminal, 2-previous, upto 4-previous11-14 The non-terminal just prior to the par-ent, 2-prior, up to 4 prior15-16 The non-terminal and terminals of thehead of the previous constituent17-18 Same, but 2 previous19-20 Same but previous to the parent21-22 Same but 2 previous to the parent23-24 The non-terminal and terminal symbolsjust prior to the start of the current parentconstituent25 The non-terminal prior to the grandparent26 Depth in tree, binned logarithmically27 Is a conjoined phrase prior to parent.Figure 4: Question typesRandom forests, at least in obvious imple-mentations, are somewhat time intensive.
Thuswe have restricted their use to the distributionp(x | t).
The forest size we picked is 4400 nodes.For the look-ahead probability, LAP, we use asingle decision tree with greedy optimal ques-tions and 1600 nodes.We smooth our random forest probabilitiesby successively backing off to distributions threeearlier in the decision tree.
We use linear inter-polation sopl(x | t) = ?
(cl)?p?l(x | t)+(1??
(cl))?pl?3(x | t)Here pl is the smoothed distribution for level lof the tree and p?l is the maximum likelihood (un-smoothed) distribution.
We use Chen smooth-ing so the linear interpolation parameters ?
arefunctions of the Chen number of the level l node.See (Chen and Goodman, 1996).
We could backoff to l ?
1, but this would slow the algorithm,and seemed unnecessary.Following (Klein and Manning, 2003) we han-dle unknown and rare words by replacing themwith one of about twenty unknown word types.For example, ?barricading?
would be replacedby UNK-ING, denoting an unknown word end-ing in ?ing.?
Any word that occurs less thantwenty times in the training corpus is consid-ered rare.
The only information that is retainedabout it is the parts of speech with which it hasappeared.
Future uses are restricted to thesepre-terminals.Because random forests have so much latitudein picking combinations of words for specific sit-uations we have the impression that it can over-fit the training data, although we have not donean explicit study to confirm this.
As a mild cor-rective we only allow verbs appearing 75 times ormore, and all other words appearing 250 times ormore, to be conditioned upon in question types16, 18, 20, 22, and 27.
Because the inner loop ofrandom-forest training involves moving a condi-tioning event to the other decedent node to seeif this raises training data probability, this alsosubstantially speeds up training time.Lastly Roark obtained the results we quotehere with selective use of left-corner transforms(Demers, 1977; Johnson and Roark, 2000).
Wealso use this technique but the details differ.Roark uses left-corner transforms only for im-mediately recursive NP?s, the most common sit-uation by far.
As it was less trouble to doso, we use them for any immediately recursiveconstituent.
However, we are also aware thatin some respects left-corner transforms workagainst the fully-connected tree rule as opera-tionalizing the ?understand as you go along?cognitive constraint.
For example, the normalsentence initial NP serves as the subject of thesentence.
However in Penn-treebank grammarstyle an initial NP could also be a possessiveNP as in (S (NP (NP (DT The) (NN dog)(POS ?s)))) Clearly this NP is not the subject.Thus using left corner transforms on all NP?sallows the parser to conflate differing semanticsituations into a single tree.
To avoid this wehave added the additional restriction that weonly allow left-corner treatment when the headwords (and thus presumably the meaning) are679Precision Recall FCollins 2003 88.3 88.1 88.2Charniak 2000 89.6 89.5 89.6C/J 2005 91.2 90.9 91.1Petrov et.al.
2006 90.3 90.0 90.2Roark 2001 87.1 86.6 86.8C/R Perceptron 87.0 86.3 86.6C/R Combined 89.1 88.4 88.8This paper 89.8 89.0 89.4Figure 5: Precision/Recall measurements, PennTreebank Section 23, Sentence length ?
100the same.
(Generally head-word rules dictatethat the POS is the head of the possessive NP.
)5 Results and AnalysisWe trained the parser on the standard sections2-21 of the Penn Tree-bank, and tested on allsentences of length ?
100 of section 23.
Weused section 24 for development.Figure 5 shows the performance of our model(last line, in bold) along with the performanceof other parsers.
The first group of results showthe performance of standard parsers now in use.While our performance of 89.4% f-measure needsimprovement before it would be worth-while us-ing this parser for routine work, it has movedpast the accuracy of the Collins-Bikel (Collins,2003; Bikel, 2004) parser and is not statisticallydistinguishable from (Charniak, 2000).The middle group of results in Figure 5 showa very significant improvement over the originalRoark parser, (89.4% vs.86.8%).
Although wehave not discussed it to this point, (Collins andRoark, 2004) present a perceptron algorithm foruse with the Roark architecture.
As seen above(C/R Perceptron), this does not give any im-provement over the original Roark model.
Asis invariably the case, when combined the twomodels perform much better than either by it-self (C/R Combined ?
88.8%).
However we stillachieve a 0.6% improvement over that result.Naturally, a new combination using our parserwould almost surely register another significantgain.Conditioning Conditioning F-measureNon-terminals Terminals8 1 86.610 2 88.013 3 88.317 4 88.821 6 89.0Figure 6: Labeled precision-recall results on section24 of the Penn Tree-bank.
All but one sentence oflength ?
100.
(Last one not parsed).In Figure 6 we show results illustrating howparser performance improves as the probabilitydistributions are conditioned on more diverse in-formation from the partial trees.
The first linehas results when we condition on only the ?clos-est?
eight non-terminal and the previous word.We successively add more distant conditioningevents.
The last line (89.0% F-measure) corre-sponds to our complete model but since we areexperimenting here on the development set theresult is not the same as in Figure 5.
(The resultis consistent with the parsing community?s ob-servation that the test set is slightly easier thanthe development set ?
e.g., average sentencelength is less.
)One other illustrative result: if we keep allsystem settings constant and replace the randomforest mechanism by a single greedy optimal de-cision tree for probability computation, perfor-mance is reduced to 86.3% f-measure.
While thisalmost certainly overstates the performance im-provement due to many random trees (the sys-tem parameters could be better adjusted to theone-tree case), it strongly suggests that nothinglike our performance could have been obtainedwithout the forests in random forests.6 Conclusions and Future WorkWe have presented a new top-down left-to-rightparsing model.
Its performance of 89.4% is a20% error reduction over the previous single-parser performance, and indeed is a small im-provement (0.6%) over the best combination-parser result.
The code is publically available.11http://bllip.cs.brown.edu/resources.shtml#software680PPINthanNPNNP NNPGeneral MotorsFigure 7: The start of an incorrect analysis for ?thanGeneral Motors is worth?INthan NP VPSGeneral Motors is NPworthSBARFigure 8: The correct analysis for ?than General Mo-tors is worth?Furthermore, as models of this sort have re-ceived comparatively little attention, it seemsreasonable to think that significant further im-provements may be found.One particular topic in need of more study issearch errors.
Consider the following example:The government had to spend morethan General Motors is worth.which is difficult for our parser.
The problem isintegrating the words starting with ?than Gen-eral Motors.?
Initially the parser believes thatthis is a prepositional phrase as shown in Fig-ure 7.
However, the correct tree-bank parse in-corporates a subordinate sentential clause?thanGeneral Motors is worth?, as in Figure 8.
Un-fortunately, before it gets to ?is?
which disam-biguates the two alternatives, the subordinateclause version has fallen out of the parser?s beam(unless, of course, one sets the beam-width to anunacceptably high level).
Furthermore, it doesnot seem that there is any information availableINthan NP VPSGeneral MotorsSBAR-NONE-Figure 9: Alternative analysis for ?than General Mo-tors?when one starts working on ?than?
to allow aperson to immediately pick the correct continu-ation.
It is also the case that the parsing modelgives the correct parse a higher probability if itis available, showing that this is a search error,not a model error.If there is no information that would allowa person to make the correct decision in time,perhaps people do not need to make this deci-sion.
Rather the problem could be in the tree-bank representation itself.
Suppose we reana-lyzed ?than General Motors?
in this context asin Figure 9.
Here we would not need to guessanything in advance of the (missing) VP.
Fur-thermore, we can make this change without loos-ing the great benefit of the treebank for trainingand testing.
The change is local and determin-istic.
We can tree-transform the training dataand then untransform before scoring.
It is ourimpression that a few examples like this wouldremove a large set of current search errors.Three other kinds of information are oftenadded as additional annotations to syntactictrees: Penn-Treebank form-function tags, traceelements, and semantic roles.
Most researchon such annotation takes the parsing process asfixed and is solely concerned with improving theretrieval of the annotation in question.
Whenthey have been integrated with parsing, findingthe parse and the further annotation jointly hasnot improved the parse.
While it is certainlypossible that this would prove to be the same forthis new model, the use of random forests to in-tegrate more diverse information sources mighthelp us to reverse this state of affairs.Finally there is no reason why we even need681to stop our collection of features at sentenceboundaries ?
information from previous sen-tences is there for our perusal.
There aremany known intra-sentence correlations, for ex-ample ?sentences?
that are actually fragmentsare much more common if the previous sentenceis a question.
The tense of sequential sentencesmain verbs are correlated.
Main clause sub-jects are more likely to be co-referent.
Certainlythe ?understanding?
humans pick up helps themassign structure to subsequent phrases.
Howmuch, if any, of this meaning we can glean givenour current (lack-of) understanding of semanticsand pragmatics is an interesting question.7 AcknowledgementsThanks to members of BLLIP and Mark John-son who read earlier drafts of this paper.
Alsothanks to Brian Roark and Mark Johnson forpointers to previous work.ReferencesStephen Abney and Mark Johnson.
1991.
Mem-ory requirements and local ambiguities of parsingstrategies.
Journal of Psycholinguistic Research,20(3):233?250.Y.
Amit and D. Geman.
1997.
Shape quantizationand recognition with randomized trees.
NeuralComputation, 9:1545?1588.Dan Bikel.
2004.
On the Parameter Space of Lex-icalized Statistical Parsing Models.
Ph.D. thesis,University of Pennsylvania.Leo Breiman.
2004.
Random forests.
MachineLearning, 45(1):5?32.Eugene Charniak and Mark Johnson.
2005.
Coarse-to-fine n-best parsing and MaxEnt discrimina-tive reranking.
In Proceedings of the 43rd An-nual Meeting of the Association for ComputationalLinguistics, pages 173?180, Ann Arbor, Michigan,June.
Association for Computational Linguistics.Eugene Charniak.
2000.
A maximum-entropy-inspired parser.
In The Proceedings of the NorthAmerican Chapter of the Association for Compu-tational Linguistics, pages 132?139.Stanley F. Chen and Joshua Goodman.
1996.
Anempirical study of smoothing techniques for lan-guage modeling.
In Arivind Joshi and MarthaPalmer, editors, Proceedings of the Thirty-FourthAnnual Meeting of the Association for Computa-tional Linguistics, pages 310?318, San Francisco.Morgan Kaufmann Publishers.Michael Collins and Brian Roark.
2004.
Incremen-tal parsing with the perceptron algorithm.
In Pro-ceedings of the 42nd Meeting of the Association forComputational Linguistics (ACL?04), Main Vol-ume, pages 111?118, Barcelona, Spain, July.Michael Collins.
2003.
Head-driven statistical mod-els for natural language parsing.
ComputationalLinguistics, 29(4):589?638.A.
Demers.
1977.
Generalized left-corner parsing.In Conference Record of the Fourth ACM Sym-posium on Principles of Programming Languages,1977 ACM SIGACT/SIGPLAN, pages 170?182.Jay Earley.
1970.
An efficient contex-free parsing al-gorithm.
Communications of the ACM, 6(8):451?445.Suzan L. Graham, Michael Harrison, and Walter L.Ruzzo.
1980.
An improved context-free recog-nizer.
ACM Transations on Programming Lan-guages and Systems, 2(3):415?462.James Henderson.
2003.
Inducing history represen-tations for broad coverage statistical parsing.
InProceedings of HLT-NAACL 2003.Mark Johnson and Brian Roark.
2000.
Compactnon-left-recursive grammars using the selectiveleft-corner transform and factoring.
In Proceed-ings of COLING-2000.Dan Klein and Christopher Manning.
2003.
Accu-rate unlexicalized parsing.
In Proceedings of the41st Annual Meeting of the Association for Com-putational Linguistics.Michell P. Marcus, Beatrice Santorini, andMary Ann Marcinkiewicz.
1993.
Building alarge annotated corpus of English: The PennTreebank.
Computational Linguistics, 19(2):313?330.Joakim Nivre.
2003.
An efficient algorithm for pro-jective dependency parsing.
In Proceedings of the8th International Workshop on Parsing.Slav Petrov, Leon Barrett, Romain Thibaux, andDan Klein.
2006.
Learning accurate, compact,and interpretable tree annotation.
In Proceedingsof the 21st International Conference on Computa-tional Linguistics and 44th Annual Meeting of theAssociation for Computational Linguistics, pages433?440, Sydney, Australia, July.
Association forComputational Linguistics.Brian Roark.
2001.
Probabilistic top-down parsingand language modeling.
Computational Linguis-tics, 27(2):249?276.682Roger van Gompel and Martin J. Pickering.
2007.Syntactic parsing.
In G. Gatskil, editor, The Ox-ford handbook of psycholinguistics.
Oxford Univer-sity Press.Peng Xu and Frederick Jelinek.
2004.
Randomforests in language modeling.
In Proceedings ofthe 2004 Empirical Methods in Natural LanguageProcessing Conference.
The Association for Com-putational Linguistics.683
