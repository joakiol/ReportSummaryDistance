Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 1423?1432,Uppsala, Sweden, 11-16 July 2010. c?2010 Association for Computational LinguisticsCoreference Resolution across Corpora:Languages, Coding Schemes, and Preprocessing InformationMarta RecasensCLiC - University of BarcelonaGran Via 585Barcelona, Spainmrecasens@ub.eduEduard HovyUSC Information Sciences Institute4676 Admiralty WayMarina del Rey CA, USAhovy@isi.eduAbstractThis paper explores the effect that dif-ferent corpus configurations have on theperformance of a coreference resolutionsystem, as measured by MUC, B3, andCEAF.
By varying separately three param-eters (language, annotation scheme, andpreprocessing information) and applyingthe same coreference resolution system,the strong bonds between system and cor-pus are demonstrated.
The experimentsreveal problems in coreference resolutionevaluation relating to task definition, cod-ing schemes, and features.
They also ex-pose systematic biases in the coreferenceevaluation metrics.
We show that systemcomparison is only possible when corpusparameters are in exact agreement.1 IntroductionThe task of coreference resolution, which aims toautomatically identify the expressions in a text thatrefer to the same discourse entity, has been an in-creasing research topic in NLP ever since MUC-6made available the first coreferentially annotatedcorpus in 1995.
Most research has centered aroundthe rules by which mentions are allowed to corefer,the features characterizing mention pairs, the algo-rithms for building coreference chains, and coref-erence evaluation methods.
The surprisingly im-portant role played by different aspects of the cor-pus, however, is an issue to which little attentionhas been paid.
We demonstrate the extent to whicha system will be evaluated as performing differ-ently depending on parameters such as the corpuslanguage, the way coreference relations are de-fined in the corresponding coding scheme, and thenature and source of preprocessing information.This paper unpacks these issues by running thesame system?a prototype entity-based architec-ture called CISTELL?on different corpus config-urations, varying three parameters.
First, we showhow much language-specific issues affect perfor-mance when trained and tested on English andSpanish.
Second, we demonstrate the extent towhich the specific annotation scheme (used on thesame corpus) makes evaluated performance vary.Third, we compare the performance using gold-standard preprocessing information with that us-ing automatic preprocessing tools.Throughout, we apply the three principal coref-erence evaluation measures in use today: MUC,B3, and CEAF.
We highlight the systematic prefer-ences of each measure to reward different config-urations.
This raises the difficult question of whyone should use one or another evaluation mea-sure, and how one should interpret their differ-ences in reporting changes of performance scoredue to ?secondary?
factors like preprocessing in-formation.To this end, we employ three corpora: ACE(Doddington et al, 2004), OntoNotes (Pradhanet al, 2007), and AnCora (Recasens and Mart??,2009).
In order to isolate the three parametersas far as possible, we benefit from a 100k-wordportion (from the TDT collection) that is commonto both ACE and OntoNotes.
We apply the samecoreference resolution system in all cases.
The re-sults show that a system?s score is not informativeby itself, as different corpora or corpus parameterslead to different scores.
Our goal is not to achievethe best performance to date, but rather to ex-pose various issues raised by the choices of corpuspreparation and evaluation measure and to shedlight on the definition, methods, evaluation, andcomplexities of the coreference resolution task.The paper is organized as follows.
Section 2sets our work in context and provides the motiva-tions for undertaking this study.
Section 3 presentsthe architecture of CISTELL, the system used inthe experimental evaluation.
In Sections 4, 5,1423and 6, we describe the experiments on three differ-ent datasets and discuss the results.
We concludein Section 7.2 BackgroundThe bulk of research on automatic coreference res-olution to date has been done for English and usedtwo different types of corpus: MUC (Hirschmanand Chinchor, 1997) and ACE (Doddington et al,2004).
A variety of learning-based systems havebeen trained and tested on the former (Soon et al,2001; Uryupina, 2006), on the latter (Culotta etal., 2007; Bengtson and Roth, 2008; Denis andBaldridge, 2009), or on both (Finkel and Manning,2008; Haghighi and Klein, 2009).
Testing on bothis needed given that the two annotation schemesdiffer in some aspects.
For example, only ACEincludes singletons (mentions that do not corefer)and ACE is restricted to seven semantic types.1Also, despite a critical discussion in the MUC taskdefinition (van Deemter and Kibble, 2000), theACE scheme continues to treat nominal predicatesand appositive phrases as coreferential.A third coreferentially annotated corpus?thelargest for English?is OntoNotes (Pradhan et al,2007; Hovy et al, 2006).
Unlike ACE, it is notapplication-oriented, so coreference relations be-tween all types of NPs are annotated.
The identityrelation is kept apart from the attributive relation,and it also contains gold-standard morphological,syntactic and semantic information.Since the MUC and ACE corpora are annotatedwith only coreference information,2 existing sys-tems first preprocess the data using automatic tools(POS taggers, parsers, etc.)
to obtain the infor-mation needed for coreference resolution.
How-ever, given that the output from automatic toolsis far from perfect, it is hard to determine thelevel of performance of a coreference module act-ing on gold-standard preprocessing information.OntoNotes makes it possible to separate the coref-erence resolution problem from other tasks.Our study adds to the previously reported evi-dence by Stoyanov et al (2009) that differences incorpora and in the task definitions need to be takeninto account when comparing coreference resolu-tion systems.
We provide new insights as the cur-rent analysis differs in four ways.
First, Stoyanov1The ACE-2004/05 semantic types are person, organiza-tion, geo-political entity, location, facility, vehicle, weapon.2ACE also specifies entity types and relations.et al (2009) report on differences between MUCand ACE, while we contrast ACE and OntoNotes.Given that ACE and OntoNotes include some ofthe same texts but annotated according to their re-spective guidelines, we can better isolate the effectof differences as well as add the additional dimen-sion of gold preprocessing.
Second, we evaluatenot only with the MUC and B3 scoring metrics,but also with CEAF.
Third, all our experimentsuse true mentions3 to avoid effects due to spuri-ous system mentions.
Finally, including differentbaselines and variations of the resolution model al-lows us to reveal biases of the metrics.Coreference resolution systems have beentested on languages other than English only withinthe ACE program (Luo and Zitouni, 2005), prob-ably due to the fact that coreferentially annotatedcorpora for other languages are scarce.
Thus therehas been no discussion of the extent to which sys-tems are portable across languages.
This paperstudies the case of English and Spanish.4Several coreference systems have been devel-oped in the past (Culotta et al, 2007; Finkeland Manning, 2008; Poon and Domingos, 2008;Haghighi and Klein, 2009; Ng, 2009).
It is not ouraim to compete with them.
Rather, we conductthree experiments under a specific setup for com-parison purposes.
To this end, we use a different,neutral, system, and a dataset that is small and dif-ferent from official ACE test sets despite the factthat it prevents our results from being compareddirectly with other systems.3 Experimental Setup3.1 System DescriptionThe system architecture used in our experiments,CISTELL, is based on the incrementality of dis-course.
As a discourse evolves, it constructs amodel that is updated with the new informationgradually provided.
A key element in this modelare the entities the discourse is about, as they formthe discourse backbone, especially those that arementioned multiple times.
Most entities, however,are only mentioned once.
Consider the growth ofthe entity Mount Popocate?petl in (1).53The adjective true contrasts with system and refers to thegold standard.4Multilinguality is one of the focuses of SemEval-2010Task 1 (Recasens et al, 2010).5Following the ACE terminology, we use the term men-tion for an instance of reference to an object, and entity for acollection of mentions referring to the same object.
Entities1424(1) We have an update tonight on [this, the volcano inMexico, they call El Popo]m3 .
.
.
As the sun risesover [Mt.
Popo]m7 tonight, the only hint of the firestorm inside, whiffs of smoke, but just a few hoursearlier, [the volcano]m11 exploding spewing rockand red-hot lava.
[The fourth largest mountain inNorth America, nearly 18,000 feet high]m15, erupt-ing this week with [its]m20 most violent outburst in1,200 years.Mentions can be pronouns (m20), they can be a(shortened) string repetition using either the name(m7) or the type (m11), or they can add new infor-mation about the entity: m15 provides the super-type and informs the reader about the height of thevolcano and its ranking position.In CISTELL,6 discourse entities are conceivedas ?baskets?
: they are empty at the beginning ofthe discourse, but keep growing as new attributes(e.g., name, type, location) are predicated aboutthem.
Baskets are filled with this information,which can appear within a mention or elsewherein the sentence.
The ever-growing amount of in-formation in a basket alows richer comparisons tonew mentions encountered in the text.CISTELL follows the learning-based corefer-ence architecture in which the task is split intoclassification and clustering (Soon et al, 2001;Bengtson and Roth, 2008) but combines them si-multaneously.
Clustering is identified with basket-growing, the core process, and a pairwise clas-sifier is called every time CISTELL considerswhether a basket must be clustered into a (grow-ing) basket, which might contain one or morementions.
We use a memory-based learning clas-sifier trained with TiMBL (Daelemans and Bosch,2005).
Basket-growing is done in four differentways, explained next.3.2 Baselines and ModelsIn each experiment, we compute three baselines(1, 2, 3), and run CISTELL under four differentmodels (4, 5, 6, 7).1.
ALL SINGLETONS.
No coreference link isever created.
We include this baseline giventhe high number of singletons in the datasets,since some evaluation measures are affectedby large numbers of singletons.2.
HEAD MATCH.
All non-pronominal NPs thathave the same head are clustered into thesame entity.containing one single mention are referred to as singletons.6?Cistell?
is the Catalan word for ?basket.?3.
HEAD MATCH + PRON.
Like HEAD MATCH,plus allowing personal and possessive pro-nouns to link to the closest noun with whichthey agree in gender and number.4.
STRONG MATCH.
Each mention (e.g., m11) ispaired with previous mentions starting fromthe beginning of the document (m1?m11, m2?m11, etc.
).7 When a pair (e.g., m3?m11) isclassified as coreferent, additional pairwisechecks are performed with all the mentionscontained in the (growing) entity basket (e.g.,m7?m11).
Only if all the pairs are classifiedas coreferent is the mention under consider-ation attached to the existing growing entity.Otherwise, the search continues.85.
SUPER STRONG MATCH.
Similar to STRONGMATCH but with a threshold.
Coreferencepairwise classifications are only acceptedwhen TiMBL distance is smaller than 0.09.96.
BEST MATCH.
Similar to STRONG MATCHbut following Ng and Cardie (2002)?s bestlink approach.
Thus, the mention under anal-ysis is linked to the most confident men-tion among the previous ones, using TiMBL?sconfidence score.7.
WEAK MATCH.
A simplified version ofSTRONG MATCH: not all mentions in thegrowing entity need to be classified as coref-erent with the mention under analysis.
A sin-gle positive pairwise decision suffices for themention to be clustered into that entity.103.3 FeaturesWe follow Soon et al (2001), Ng and Cardie(2002) and Luo et al (2004) to generate mostof the 29 features we use for the pairwisemodel.
These include features that capture in-formation from different linguistic levels: textualstrings (head match, substring match, distance,frequency), morphology (mention type, coordi-nation, possessive phrase, gender match, numbermatch), syntax (nominal predicate, apposition, rel-ative clause, grammatical function), and semanticmatch (named-entity type, is-a type, supertype).7The opposite search direction was also tried but gaveworse results.8Taking the first mention classified as coreferent followsSoon et al (2001)?s first-link approach.9In TiMBL, being a memory-based learner, the closer thedistance to an instance, the more confident the decision.
Wechose 0.09 because it appeared to offer the best results.10STRONG and WEAK MATCH are similar to Luo et al(2004)?s entity-mention and mention-pair models.1425For Spanish, we use 34 features as a few varia-tions are needed for language-specific issues suchas zero subjects (Recasens and Hovy, 2009).3.4 EvaluationSince they sometimes provide quite different re-sults, we evaluate using three coreference mea-sures, as there is no agreement on a standard.?
MUC (Vilain et al, 1995).
It computes thenumber of links common between the trueand system partitions.
Recall (R) and preci-sion (P) result from dividing it by the mini-mum number of links required to specify thetrue and the system partitions, respectively.?
B3 (Bagga and Baldwin, 1998).
R and P arecomputed for each mention and averaged atthe end.
For each mention, the number ofcommon mentions between the true and thesystem entity is divided by the number ofmentions in the true entity or in the systementity to obtain R and P, respectively.?
CEAF (Luo, 2005).
It finds the best one-to-one alignment between true and system en-tities.
Using true mentions and the ?3 sim-ilarity function, R and P are the same andcorrespond to the number of common men-tions between the aligned entities divided bythe total number of mentions.4 Parameter 1: LanguageThe first experiment compared the performanceof a coreference resolution system on a Germanicand a Romance language?English and Spanish?to explore to what extent language-specific issuessuch as zero subjects11 or grammatical gendermight influence a system.Although OntoNotes and AnCora are two dif-ferent corpora, they are very similar in those as-pects that matter most for the study?s purpose:they both include a substantial amount of textsbelonging to the same genre (news) and manu-ally annotated from the morphological to the se-mantic levels (POS tags, syntactic constituents,NEs, WordNet synsets, and coreference relations).More importantly, very similar coreference anno-tation guidelines make AnCora the ideal Spanishcounterpart to OntoNotes.11Most Romance languages are pro-drop allowing zerosubject pronouns, which can be inferred from the verb.Datasets Two datasets of similar size were se-lected from AnCora and OntoNotes in order torule out corpus size as an explanation of any differ-ence in performance.
Corpus statistics about thedistribution of mentions and entities are shown inTables 1 and 2.
Given that this paper is focused oncoreference between NPs, the number of mentionsonly includes NPs.
Both AnCora and OntoNotesannotate only multi-mention entities (i.e., thosecontaining two or more coreferent mentions), sosingleton entities are assumed to correspond toNPs with no coreference annotation.Apart from a larger number of mentions inSpanish (Table 1), the two datasets look very sim-ilar in the distribution of singletons and multi-mention entities: about 85% and 15%, respec-tively.
Multi-mention entities have an averageof 3.9 mentions per entity in AnCora and 3.5 inOntoNotes.
The distribution of mention types (Ta-ble 2), however, differs in two important respects:AnCora has a smaller number of personal pro-nouns as Spanish typically uses zero subjects, andit has a smaller number of bare NPs as the definitearticle accompanies more NPs than in English.Results and Discussion Table 3 presents CIS-TELL?s results for each dataset.
They make evi-dent problems with the evaluation metrics, namelythe fact that the generated rankings are contradic-tory (Denis and Baldridge, 2009).
They are con-sistent across the two corpora though: MUC re-wards WEAK MATCH the most, B3 rewards HEADMATCH the most, and CEAF is divided betweenSUPER STRONG MATCH and BEST MATCH.These preferences seem to reveal weaknessesof the scoring methods that make them biased to-wards a type of output.
The model preferred byMUC is one that clusters many mentions together,thus getting a large number of correct coreferencelinks (notice the high R for WEAK MATCH), butAnCora OntoNotesPronouns 14.09 17.62Personal pronouns 2.00 12.10Zero subject pronouns 6.51 ?Possessive pronouns 3.57 2.96Demonstrative pronouns 0.39 1.83Definite NPs 37.69 20.67Indefinite NPs 7.17 8.44Demonstrative NPs 1.98 3.41Bare NPs 33.02 42.92Misc.
6.05 6.94Table 2: Mention types (%) in Table 1 datasets.1426#docs #words #mentions #entities (e) #singleton e #multi-mention eAnCoraTraining 955 299,014 91,904 64,535 54,991 9,544Test 30 9,851 2,991 2,189 1,877 312OntoNotesTraining 850 301,311 74,692 55,819 48,199 7,620Test 33 9,763 2,463 1,790 1,476 314Table 1: Corpus statistics for the large portion of OntoNotes and AnCora.MUC B3 CEAFP R F P R F P / R / FAnCora - Spanish1.
ALL SINGLETONS ?
?
?
100 73.32 84.61 73.322.
HEAD MATCH 55.03 37.72 44.76 91.12 79.88 85.13 75.963.
HEAD MATCH + PRON 48.22 44.24 46.14 86.21 80.66 83.34 76.304.
STRONG MATCH 45.64 51.88 48.56 80.13 82.28 81.19 75.795.
SUPER STRONG MATCH 45.68 36.47 40.56 86.10 79.09 82.45 77.206.
BEST MATCH 43.10 35.59 38.98 85.24 79.67 82.36 75.237.
WEAK MATCH 45.73 65.16 53.75 68.50 87.71 76.93 69.21OntoNotes - English1.
ALL SINGLETONS ?
?
?
100 72.68 84.18 72.682.
HEAD MATCH 55.14 39.08 45.74 90.65 80.87 85.48 76.053.
HEAD MATCH + PRON 47.10 53.05 49.90 82.28 83.13 82.70 75.154.
STRONG MATCH 47.94 55.42 51.41 81.13 84.30 82.68 78.035.
SUPER STRONG MATCH 48.27 47.55 47.90 84.00 82.27 83.13 78.246.
BEST MATCH 50.97 46.66 48.72 86.19 82.70 84.41 78.447.
WEAK MATCH 47.46 66.72 55.47 70.36 88.05 78.22 71.21Table 3: CISTELL results varying the corpus language.also many spurious links that are not duly penal-ized.
The resulting output is not very desirable.12In contrast, B3 is more P-oriented and scores con-servative outputs like HEAD MATCH and BESTMATCH first, even if R is low.
CEAF achieves abetter compromise between P and R, as corrobo-rated by the quality of the output.The baselines and the system runs perform verysimilarly in the two corpora, but slightly betterfor English.
It seems that language-specific issuesdo not result in significant differences?at leastfor English and Spanish?once the feature set hasbeen appropriately adapted, e.g., including fea-tures about zero subjects or removing those aboutpossessive phrases.
Comparing the feature ranks,we find that the features that work best for eachlanguage largely overlap and are language inde-pendent, like head match, is-a match, and whetherthe mentions are pronominal.5 Parameter 2: Annotation SchemeIn the second experiment, we used the 100k-wordportion (from the TDT collection) shared by theOntoNotes and ACE corpora (330 OntoNotes doc-12Due to space constraints, the actual output cannot beshown here.
We are happy to send it to interested requesters.uments occurred as 22 ACE-2003 documents, 185ACE-2004 documents, and 123 ACE-2005 docu-ments).
CISTELL was trained on the same textsin both corpora and applied to the remainder.
Thethree measures were then applied to each result.Datasets Since the two annotation schemes dif-fer significantly, we made the results comparableby mapping the ACE entities (the simpler scheme)onto the information contained in OntoNotes.13The mapping allowed us to focus exclusively onthe differences expressed on both corpora: thetypes of mentions that were annotated, the defi-nition of identity of reference, etc.Table 4 presents the statistics for the OntoNotesdataset merged with the ACE entities.
The map-ping was not straightforward due to several prob-lems: there was no match for some mentionsdue to syntactic or spelling reasons (e.g., El Popoin OntoNotes vs. Ell Popo in ACE).
ACE men-tions for which there was no parse tree node inthe OntoNotes gold-standard tree were omitted, ascreating a new node could have damaged the tree.Given that only seven entity types are annotatedin ACE, the number of OntoNotes mentions is al-13Both ACE entities and types were mapped onto theOntoNotes dataset.1427#docs #words #mentions #entities (e) #singleton e #multi-mention eOntoNotesTraining 297 87,068 22,127 15,983 13,587 2,396Test 33 9,763 2,463 1,790 1,476 314ACETraining 297 87,068 12,951 5,873 3,599 2,274Test 33 9,763 1,464 746 459 287Table 4: Corpus statistics for the aligned portion of ACE and OntoNotes on gold-standard data.MUC B3 CEAFP R F P R F P / R / FOntoNotes scheme1.
ALL SINGLETONS ?
?
?
100 72.68 84.18 72.682.
HEAD MATCH 55.14 39.08 45.74 90.65 80.87 85.48 76.053.
HEAD MATCH + PRON 47.10 53.05 49.90 82.28 83.13 82.70 75.154.
STRONG MATCH 46.81 53.34 49.86 80.47 83.54 81.97 76.785.
SUPER STRONG MATCH 46.51 40.56 43.33 84.95 80.16 82.48 76.706.
BEST MATCH 52.47 47.40 49.80 86.10 82.80 84.42 77.877.
WEAK MATCH 47.91 64.64 55.03 71.73 87.46 78.82 71.74ACE scheme1.
ALL SINGLETONS ?
?
?
100 50.96 67.51 50.962.
HEAD MATCH 82.35 39.00 52.93 95.27 64.05 76.60 66.463.
HEAD MATCH + PRON 70.11 53.90 60.94 86.49 68.20 76.27 68.444.
STRONG MATCH 64.21 64.21 64.21 76.92 73.54 75.19 70.015.
SUPER STRONG MATCH 60.51 56.55 58.46 76.71 69.19 72.76 66.876.
BEST MATCH 67.50 56.69 61.62 82.18 71.67 76.57 69.887.
WEAK MATCH 63.52 80.50 71.01 59.76 86.36 70.64 64.21Table 5: CISTELL results varying the annotation scheme on gold-standard data.most twice as large as the number of ACE men-tions.
Unlike OntoNotes, ACE mentions includepremodifiers (e.g., state in state lines), nationaladjectives (e.g., Iraqi) and relative pronouns (e.g.,who, that).
Also, given that ACE entities corre-spond to types that are usually coreferred (e.g.,people, organizations, etc.
), singletons only rep-resent 61% of all entities, while they are 85% inOntoNotes.
The average entity size is 4 in ACEand 3.5 in OntoNotes.A second major difference is the definition ofcoreference relations, illustrated here:(2) [This] was [an all-white, all-Christian communitythat all the sudden was taken over ... by differentgroups].
(3) [ [Mayor] John Hyman] has a simple answer.
(4) [Postville] now has 22 different nationalities ... Forthose who prefer [the old Postville], Mayor JohnHyman has a simple answer.In ACE, nominal predicates corefer with theirsubject (2), and appositive phrases corefer withthe noun they are modifying (3).
In contrast,they do not fall under the identity relation inOntoNotes, which follows the linguistic under-standing of coreference according to which nom-inal predicates and appositives express propertiesof an entity rather than refer to a second (corefer-ent) entity (van Deemter and Kibble, 2000).
Fi-nally, the two schemes frequently disagree on bor-derline cases in which coreference turns out to beespecially complex (4).
As a result, some featureswill behave differently, e.g., the appositive featurehas the opposite effect in the two datasets.Results and Discussion From the differencespointed out above, the results shown in Table 5might be surprising at first.
Given that OntoNotesis not restricted to any semantic type and is basedon a more sophisticated definition of coreference,one would not expect a system to perform betteron it than on ACE.
The explanation is given by theALL SINGLETONS baseline, which is 73?84% forOntoNotes and only 51?68% for ACE.
The factthat OntoNotes contains a much larger number ofsingletons?as Table 4 shows?results in an ini-tial boost of performance (except with the MUCscore, which ignores singletons).
In contrast, thescore improvement achieved by HEAD MATCH ismuch more noticeable on ACE than on OntoNotes,which indicates that many of its coreferent men-tions share the same head.The systematic biases of the measures that wereobserved in Table 3 appear again in the case of1428MUC and B3.
CEAF is divided between BESTMATCH and STRONG MATCH.
The higher valueof the MUC score for ACE is another indicationof its tendency to reward correct links much morethan to penalize spurious ones (ACE has a largerproportion of multi-mention entities).The feature rankings obtained for each datasetgenerally coincide as to which features are rankedbest (namely NE match, is-a match, and headmatch), but differ in their particular ordering.It is also possible to compare the OntoNotes re-sults in Tables 3 and 5, the only difference beingthat the first training set was three times larger.Contrary to expectation, the model trained on alarger dataset performs just slightly better.
Thefact that more training data does not necessarilylead to an increase in performance conforms tothe observation that there appear to be few generalrules (e.g., head match) that systematically gov-ern coreference relationships; rather, coreferenceappeals to individual unique phenomena appear-ing in each context, and thus after a point addingmore training data does not add much new gener-alizable information.
Pragmatic information (dis-course structure, world knowledge, etc.)
is proba-bly the key, if ever there is a way to encode it.6 Parameter 3: PreprocessingThe goal of the third experiment was to determinehow much the source and nature of preprocess-ing information matters.
Since it is often statedthat coreference resolution depends on many lev-els of analysis, we again compared the two cor-pora, which differ in the amount and correctnessof such information.
However, in this experiment,entity mapping was applied in the opposite direc-tion: the OntoNotes entities were mapped onto theautomatically preprocessed ACE dataset.
This ex-poses the shortcomings of automated preprocess-ing in ACE for identifying all the mentions identi-fied and linked in OntoNotes.Datasets The ACE data was morphologicallyannotated with a tokenizer based on manual rulesadapted from the one used in CoNLL (TjongKim Sang and De Meulder, 2003), with TnT 2.2,a trigram POS tagger based on Markov models(Brants, 2000), and with the built-in WordNet lem-matizer (Fellbaum, 1998).
Syntactic chunks wereobtained from YamCha 1.33, an SVM-based NP-chunker (Kudoh and Matsumoto, 2000), and parsetrees from Malt Parser 0.4, an SVM-based parser(Hall et al, 2007).Although the number of words in Tables 4 and 6should in principle be the same, the latter con-tains fewer words as it lacks the null elements(traces, ellipsed material, etc.)
manually anno-tated in OntoNotes.
Missing parse tree nodes inthe automatically parsed data account for the con-siderably lower number of OntoNotes mentions(approx.
5,700 fewer mentions).14 However, theproportions of singleton:multi-mention entities aswell as the average entity size do not vary.Results and Discussion The ACE scores for theautomatically preprocessed models in Table 7 areabout 3% lower than those based on OntoNotesgold-standard data in Table 5, providing evidencefor the advantage offered by gold-standard prepro-cessing information.
In contrast, the similar?ifnot higher?scores of OntoNotes can be attributedto the use of the annotated ACE entity types.
Thefact that these are annotated not only for propernouns (as predicted by an automatic NER) but alsofor pronouns and full NPs is a very helpful featurefor a coreference resolution system.Again, the scoring metrics exhibit similar bi-ases, but note that CEAF prefers HEAD MATCH+ PRON in the case of ACE, which is indicative ofthe noise brought by automatic preprocessing.A further insight is offered from comparing thefeature rankings with gold-standard syntax to thatwith automatic preprocessing.
Since we are evalu-ating now on the ACE data, the NE match featureis also ranked first for OntoNotes.
Head and is-amatch are still ranked among the best, yet syntac-tic features are not.
Instead, features like NP typehave moved further up.
This reranking probablyindicates that if there is noise in the syntactic infor-mation due to automatic tools, then morphologicaland syntactic features switch their positions.Given that the noise brought by automatic pre-processing can be harmful, we tried leaving out thegrammatical function feature.
Indeed, the resultsincreased about 2?3%, STRONG MATCH scoringthe highest.
This points out that conclusions drawnfrom automatically preprocessed data about thekind of knowledge relevant for coreference reso-lution might be mistaken.
Using the most success-ful basic features can lead to the best results whenonly automatic preprocessing is available.14In order to make the set of mentions as similar as possibleto the set in Section 5, OntoNotes singletons were mappedfrom the ones detected in the gold-standard treebank.1429#docs #words #mentions #entities (e) #singleton e #multi-mention eOntoNotesTraining 297 80,843 16,945 12,127 10,253 1,874Test 33 9,073 1,931 1,403 1,156 247ACETraining 297 80,843 13,648 6,041 3,652 2,389Test 33 9,073 1,537 775 475 300Table 6: Corpus statistics for the aligned portion of ACE and OntoNotes on automatically parsed data.MUC B3 CEAFP R F P R F P / R / FOntoNotes scheme1.
ALL SINGLETONS ?
?
?
100 72.66 84.16 72.662.
HEAD MATCH 56.76 35.80 43.90 92.18 80.52 85.95 76.333.
HEAD MATCH + PRON 47.44 54.36 50.66 82.08 83.61 82.84 74.834.
STRONG MATCH 52.66 58.14 55.27 83.11 85.05 84.07 78.305.
SUPER STRONG MATCH 51.67 46.78 49.11 85.74 82.07 83.86 77.676.
BEST MATCH 54.38 51.70 53.01 86.00 83.60 84.78 78.157.
WEAK MATCH 49.78 64.58 56.22 75.63 87.79 81.26 74.62ACE scheme1.
ALL SINGLETONS ?
?
?
100 50.42 67.04 50.422.
HEAD MATCH 81.25 39.24 52.92 94.73 63.82 76.26 65.973.
HEAD MATCH + PRON 69.76 53.28 60.42 86.39 67.73 75.93 68.054.
STRONG MATCH 58.85 58.92 58.89 73.36 70.35 71.82 66.305.
SUPER STRONG MATCH 56.19 50.66 53.28 75.54 66.47 70.72 63.966.
BEST MATCH 63.38 49.74 55.74 80.97 68.11 73.99 65.977.
WEAK MATCH 60.22 78.48 68.15 55.17 84.86 66.87 59.08Table 7: CISTELL results varying the annotation scheme on automatically preprocessed data.7 ConclusionRegarding evaluation, the results clearly exposethe systematic tendencies of the evaluation mea-sures.
The way each measure is computed makesit biased towards a specific model: MUC is gen-erally too lenient with spurious links, B3 scorestoo high in the presence of a large number of sin-gletons, and CEAF does not agree with either ofthem.
It is a cause for concern that they providecontradictory indications about the core of coref-erence, namely the resolution models?for exam-ple, the model ranked highest by B3 in Table 7 isranked lowest by MUC.
We always assume eval-uation measures provide a ?true?
reflection of ourapproximation to a gold standard in order to guideresearch in system development and tuning.Further support to our claims comes from theresults of SemEval-2010 Task 1 (Recasens et al,2010).
The performance of the six participatingsystems shows similar problems with the evalua-tion metrics, and the singleton baseline was hardto beat even by the highest-performing systems.Since the measures imply different conclusionsabout the nature of the corpora and the preprocess-ing information applied, should we use them nowto constrain the ways our corpora are created inthe first place, and what preprocessing we includeor omit?
Doing so would seem like circular rea-soning: it invalidates the notion of the existence ofa true and independent gold standard.
But if ap-parently incidental aspects of the corpora can havesuch effects?effects rated quite differently by thevarious measures?then we have no fixed groundto stand on.The worrisome fact that there is currently noclearly preferred and ?correct?
evaluation measurefor coreference resolution means that we cannotdraw definite conclusions about coreference reso-lution systems at this time, unless they are com-pared on exactly the same corpus, preprocessedunder the same conditions, and all three measuresagree in their rankings.AcknowledgmentsWe thank Dr. M. Anto`nia Mart??
for her generosityin allowing the first author to visit ISI to work withthe second.
Special thanks to Edgar Gonza`lez forhis kind help with conversion issues.This work was partially supported by the Span-ish Ministry of Education through an FPU schol-arship (AP2006-00994) and the TEXT-MESS 2.0Project (TIN2009-13391-C04-04).1430ReferencesAmit Bagga and Breck Baldwin.
1998.
Algorithms forscoring coreference chains.
In Proceedings of theLREC 1998 Workshop on Linguistic Coreference,pages 563?566, Granada, Spain.Eric Bengtson and Dan Roth.
2008.
Understandingthe value of features for coreference resolution.
InProceedings of EMNLP 2008, pages 294?303, Hon-olulu, Hawaii.Thorsten Brants.
2000.
TnT ?
A statistical part-of-speech tagger.
In Proceedings of ANLP 2000, Seat-tle, WA.Aron Culotta, Michael Wick, Robert Hall, and AndrewMcCallum.
2007.
First-order probabilistic modelsfor coreference resolution.
In Proceedings of HLT-NAACL 2007, pages 81?88, Rochester, New York.Walter Daelemans and Antal Van den Bosch.
2005.Memory-Based Language Processing.
CambridgeUniversity Press.Pascal Denis and Jason Baldridge.
2009.
Global jointmodels for coreference resolution and named entityclassification.
Procesamiento del Lenguaje Natural,42:87?96.George Doddington, Alexis Mitchell, Mark Przybocki,Lance Ramshaw, Stephanie Strassel, and RalphWeischedel.
2004.
The Automatic Content Extrac-tion (ACE) Program - Tasks, Data, and Evaluation.In Proceedings of LREC 2004, pages 837?840.Christiane Fellbaum.
1998.
WordNet: An ElectronicLexical Database.
The MIT Press.Jenny Rose Finkel and Christopher D. Manning.2008.
Enforcing transitivity in coreference resolu-tion.
In Proceedings of ACL-HLT 2008, pages 45?48, Columbus, Ohio.Aria Haghighi and Dan Klein.
2009.
Simple coref-erence resolution with rich syntactic and semanticfeatures.
In Proceedings of EMNLP 2009, pages1152?1161, Singapore.
Association for Computa-tional Linguistics.Johan Hall, Jens Nilsson, Joakim Nivre, Gu?lsenEryigit, Bea?ta Megyesi, Mattias Nilsson, andMarkus Saers.
2007.
Single malt or blended?A study in multilingual parser optimization.
InProceedings of the CoNLL shared task session ofEMNLP-CoNLL 2007, pages 933?939.Lynette Hirschman and Nancy Chinchor.
1997.
MUC-7 Coreference Task Definition ?
Version 3.0.
In Pro-ceedings of MUC-7.Eduard Hovy, Mitchell Marcus, Martha Palmer,Lance Ramshaw, and Ralph Weischedel.
2006.OntoNotes: the 90% solution.
In Proceedings ofHLT-NAACL 2006, pages 57?60.Taku Kudoh and Yuji Matsumoto.
2000.
Use of sup-port vector learning for chunk identification.
In Pro-ceedings of CoNLL 2000 and LLL 2000, pages 142?144, Lisbon, Portugal.Xiaoqiang Luo and Imed Zitouni.
2005.
Multi-lingualcoreference resolution with syntactic features.
InProceedings of HLT-EMNLP 2005, pages 660?667,Vancouver.Xiaoqiang Luo, Abe Ittycheriah, Hongyan Jing, NandaKambhatla, and Salim Roukos.
2004.
A mention-synchronous coreference resolution algorithm basedon the Bell tree.
In Proceedings of ACL 2004, pages21?26, Barcelona.Xiaoqiang Luo.
2005.
On coreference resolutionperformance metrics.
In Proceedings of HLT-EMNLP 2005, pages 25?32, Vancouver.Vincent Ng and Claire Cardie.
2002.
Improvingmachine learning approaches to coreference resolu-tion.
In Proceedings of ACL 2002, pages 104?111,Philadelphia.Vincent Ng.
2009.
Graph-cut-based anaphoricity de-termination for coreference resolution.
In Proceed-ings of NAACL-HLT 2009, pages 575?583, Boulder,Colorado.Hoifung Poon and Pedro Domingos.
2008.
Joint unsu-pervised coreference resolution with Markov logic.In Proceedings of EMNLP 2008, pages 650?659,Honolulu, Hawaii.Sameer S. Pradhan, Eduard Hovy, Mitch Mar-cus, Martha Palmer, Lance Ramshaw, and RalphWeischedel.
2007.
Ontonotes: A unified rela-tional semantic representation.
In Proceedings ofICSC 2007, pages 517?526, Washington, DC.Marta Recasens and Eduard Hovy.
2009.
ADeeper Look into Features for Coreference Res-olution.
In S. Lalitha Devi, A. Branco, andR.
Mitkov, editors, Anaphora Processing and Ap-plications (DAARC 2009), volume 5847 of LNAI,pages 29?42.
Springer-Verlag.Marta Recasens and M. Anto`nia Mart??.
2009.
AnCora-CO: Coreferentially annotated corpora for Spanishand Catalan.
Language Resources and Evaluation,DOI 10.1007/s10579-009-9108-x.Marta Recasens, Llu?
?s Ma`rquez, Emili Sapena,M.
Anto`nia Mart?
?, Mariona Taule?, Ve?ronique Hoste,Massimo Poesio, and Yannick Versley.
2010.SemEval-2010 Task 1: Coreference resolution inmultiple languages.
In Proceedings of the Fifth In-ternational Workshop on Semantic Evaluations (Se-mEval 2010), Uppsala, Sweden.Wee M. Soon, Hwee T. Ng, and Daniel C. Y. Lim.2001.
A machine learning approach to coreferenceresolution of noun phrases.
Computational Linguis-tics, 27(4):521?544.1431Veselin Stoyanov, Nathan Gilbert, Claire Cardie, andEllen Riloff.
2009.
Conundrums in noun phrasecoreference resolution: Making sense of the state-of-the-art.
In Proceedings of ACL-IJCNLP 2009,pages 656?664, Singapore.Erik F. Tjong Kim Sang and Fien De Meulder.2003.
Introduction to the CoNLL-2003 SharedTask: Language-independent Named Entity Recog-nition.
In Walter Daelemans and Miles Osborne, ed-itors, Proceedings of CoNLL 2003, pages 142?147.Edmonton, Canada.Olga Uryupina.
2006.
Coreference resolution withand without linguistic knowledge.
In Proceedingsof LREC 2006.Kees van Deemter and Rodger Kibble.
2000.
On core-ferring: Coreference in MUC and related annotationschemes.
Computational Linguistics, 26(4):629?637.Marc Vilain, John Burger, John Aberdeen, Dennis Con-nolly, and Lynette Hirschman.
1995.
A model-theoretic coreference scoring scheme.
In Proceed-ings of MUC-6, pages 45?52, San Francisco.1432
