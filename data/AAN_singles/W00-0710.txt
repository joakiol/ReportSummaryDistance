In: Proceedings of CoNLL-2000 and LLL-2000, pages 55-60, Lisbon, Portugal, 2000.Learning Distributed Linguistic ClassesStephan Raa i jmakersNetherlands Organisation for Applied Scientific Research (TNO)Inst i tute for Applied PhysicsDelftThe Netherlandsraaijmakers@tpd, tno.
nlAbst rac tError-correcting output codes (ECOC) haveemerged in machine learning as a success-ful implementation of the idea of distributedclasses.
Monadic class symbols are replacedby bit strings, which are learned by an ensem-ble of binary-valued classifiers (dichotomizers).In this study, the idea of ECOC is applied tomemory-based language learning with local (k-nearest neighbor) classifiers.
Regression analy-sis of the experimental results reveals that, inorder for ECOC to be successful for languagelearning, the use of the Modified Value Differ-ence Metric (MVDM) is an important factor,which is explained in terms of population den-sity of the class hyperspace.1 In t roduct ionSupervised learning methods applied to natu-ral language classification tasks commonly op-erate on high-level symbolic representations,with linguistic lasses that are usually monadic,without internal structure (Daelemans et al,1996; Cardie et al, 1999; Roth, 1998).
Thiscontrasts with the distributed class encodingcommonly found in neural networks (Schmid,1994).
Error-correcting output codes (ECOC)have been introduced to machine learning asa principled and successful approach to dis-tributed class encoding (Dietterich and Bakiri,1995; Ricci and Aha, 1997; Berger, 1999).
WithECOC, monadic classes are replaced by code-words, i.e.
binary-valued vectors.
An ensem-ble of separate classifiers (dichotomizers) mustbe trained to learn the binary subclassificationsfor every instance in the training set.
Duringclassification, the bit predictions of the vari-ous dichotomizers are combined to produce acodeword prediction.
The class codeword whichhas minimal Hamming distance to the predictedcodeword etermines the classification of the in-stance.
Codewords are constructed such thattheir Hamming distance is maximal.
Extra bitsare added to allow for error recovery, allowingthe correct class to be determinable even if somebits are wrong.
An error-correcting output codefor a k-class problem constitutes a matrix withk rows and 2 k-1-1 columns.
Rows are the code-words corresponding to classes, and columns arebinary subclassifications or bit functions fi suchthat, for an instance , and its codeword vectorCfi(e) = ~-i(c) (1)(~-i(v) the i-th coordinate of vector v).
Ifthe minimum Hamming distance between ev-ery codeword is d, then the code has an error-correcting capability of \ [ -~ J .
Figure 1 showsthe 5 x 15 ECOC matrix, for a 5-class problem.In this code, every codeword has a Hammingdistance of at least 8 to the other codewords,so this code has an error-correcting capabilityof 3 bits.
ECOC have two natural interpreta-011000100000001\ ]  01101001011101 ~101000111010111100111011000111110100011011Figure h ECOC for a five-class problem.tions.
From an information-theoretic perspec-tive, classification with ECOC is like channelcoding (Shannon, 1948): the class of a patternto be classified is a datum sent over a noisy com-munication channel.
The communication chan-nel consists of the trained classifier.
The noiseconsists of the bias (systematic error) and vari-ance (training set-dependent error) of the classi-fier, which together make up for the overall error55of the classifier.
The received message must bedecoded before it can be interpreted as a classi-fication.
Adding redundancy to a signal beforetransmission is a well-known technique in digi-tal communication to allow for the recovery oferrors due to noise in the channel, and this isthe key to the success of ECOC.
From a ma-chine learning perspective, an error-correctingoutput code uniquely partitions the instancesin the training set into two disjoint subclasses,0 or 1.
This can be interpreted as learning a setof class boundaries.
To illustrate this, considerthe following binary code for a three-class prob-lem.
(This actually is a one-of-c code with noerror-correcting capability (the minimal Ham-ming distance between the codewords i 1).
Assuch it is an error-correcting code with lowesterror correction, but it serves to illustrate thepoint.
)fl f2 f3C1 0 0 1C2 01  0 (2)C3 1 0 0For every combination of classes (C1-C2, C1-C3, C2-C3), the Hamming distance between thecodewords i 2.
These horizontal relations havevertical repercussions as well: for every suchpair, two bit functions disagree in the classesthey select.
For C1-C2, f2 selects C2 and f3 se-lects C1.
For C1-C3, f l  selects C3 and f3 selectsC1.
Finally, for C2-C3, f l  selects C3 and f2 se-lects C2.
So, every class is selected two times,and this implies that every class boundary asso-ciated with that class in the feature hyperspaceis learned twice.
In general (Kong and Diet-terich, 1995), if the minimal Hamming distancebetween the codewords of an (error-correcting)code is d, then every class boundary is learnedtimes.
For the error-correcting code from abovethis implies an error correction of zero: only twovotes support a class boundary, and no vote canbe favored in case of a conflict.
The decodingof the predicted bit string to a class symbol ap-pears to be a form of voting over class bound-aries (Kong and Dietterich, 1995), and is able toreduce both bias and variance of the classifier.2 D ichotomizer  EnsemblesDichotomizer ensembles must be diverse apartfrom accurate.
Diversity is necessary in orderto decorrelate he predictions of the various di-chotomizers.
This is a consequence of the votingmechanism underlying ECOC, where bit func-tions can only outvote other bit functions if theydo not make similar predictions.
Selecting dif-ferent features per dichotomizer was proposedfor this purpose (Ricci and Aha, 1997).
An-other possibility is to add limited non-locality toa local classifier, since classifiers that use globalinformation such as class probabilities duringclassification, are much less vulnerable to cor-related predictions.
The following ideas weretested empirically on a suite of natural languagelearning tasks.?
A careful feature selection approach, whereevery dichotomizer is trained to select (pos-sibly) different features.?
A careless feature selection approach,where every bit is predicted by a votingcommittee of dichotomizers, each of whichrandomly selects features (akin in spirit tothe Multiple Feature Subsets approach fornon-distributed classifiers (Bay, 1999).?
A careless feature selection approach,where blocks of two adjacent bits are pre-dicted by a voting committee of quadro-tomizers, each of which randomly selectsfeatures.
Learning blocks of two bits al-lows for bit codes that are twice as long(larger error-correction), but with half asmany classifiers.
Assuming a normal dis-tribution of errors and bit values in every 2bits-block, there is a 25% chance that bothbits in a 2-bit block are wrong.
The other75% chance of one bit wrong would pro-duce performance equal to voting per bit.Formally, this implies a switch from N two-class problems to N/2 four-class problems,where separate regions of the class land-scape are learned jointly.?
Adding non-locality to 1-3 in the form oflarger values for k.?
The use of the Modified Value DifferenceMetric, which alters the distribution of in-stances over the hyperspace of features,yielding different class boundaries.3 Memory-based  learn ingThe memory-based learning paradigm viewscognitive processing as reasoning by analogy.Cognitive classification tasks are carried out by56matching data to be classified with classifieddata stored in a knowledge base.
This latterdata set is called the training data, and its ele-ments are called instances.
Every instance con-sists of a feature-value vector and a class label.Learning under the memory-based paradigm islazy, and consists only of storing the traininginstances in a suitable data structure.
The in-stance from the training set which resemblesthe most the item to be classified determinesthe classification of the latter.
This instance iscalled the nearest neighbor, and models basedon this approach to analogy are called nearestneighbor models (Duda and Hart, 1973).
So-called k-nearest neighbor models elect a winnerfrom the k nearest neighbors, where k is a pa-rameter and winner selection is usually based onclass frequency.
Resemblance between instancesis measured using distance metrics, which comein many sorts.
The simplest distance metric isthe overlap metric:k (3) 5(vi, vj) = 0 if vi = vj5(vi, vj) = 1 if vi ?
vj(~ri(I) is the i-th projection of the feature vec-tor I.)
Another distance metric is the Mod-ified Value Difference Metric (MVDM) (Costand Salzberg, 1993).
The MVDM defines sim-ilarity between two feature values in terms ofposterior probabilities:5(vi, vj) = ~ I P(c I vi) - P(c Ivj) l (4)cEClassesWhen two values share more classes, they aremore similar, as 5 decreases.
Memory-basedlearning has fruitfully been applied to natu-ral language processing, yielding state-of-the-art performance on all levels of linguistic analy-sis, including grapheme-to-phoneme conversion(van den Bosch and Daelemans, 1993), PoS-tagging (Daelemans et al, 1996), and shallowparsing (Cardie et al, 1999).
In this study,the following memory-based models are used,all available from the TIMBL package (Daele-mans et al, 1999).
IB i - IG is a k-nearest dis-tance classifier which employs a weighted over-lap metric:~(I~, b )  = ~ wkS(~k(/~), ~( I j ) )  (5)kIn stead of drawing winners from the k-nearestneighbors pool, IBi-IG selects from a pool ofinstances for k nearest distances.
Features areseparately weighted based on Quinlan's infor-mation gain ratio (Quinlan, 1993), which mea-sures the informativity of features for predictingclass labels.
This can be computed by subtract-ing the entropy of the knowledge of the featurevalues from the general entropy of the class la-bels.
The first quantity is normalized with the apriori probabilities of the various feature valuesof feature F:H(C)  - Eveva  es(F) P(v) ?
H(QF=v\]) (6)Here, H(C) is the class entropy, defined asH(C) =-  ~ P(c) log 2P(c).
(7)cEClassH(C\[F=v\] ) is the class entropy computed overthe subset of instances that have v as value forFi.
Normalization for features with many valuesis obtained by dividing the information gain fora feature by the entropy of its value set (calledthe split info of feature Fi.H(C)--~veValues(Fi) P(v)xH(C\[F=v\])Wi ---- split_in f o( Fi )split - info(Fi) = - ~ P(v) log 2 P(v)vE Values( Fi )(s)IGTREE is a heuristic approximation of IB1-IG which has comparable accuracy, but is op-timized for speed.
It is insensitive to k-valueslarger than 1, and uses value-class cooccurrenceinformation when exact matches fail.4 ExperimentsThe effects of a distributed class representa-tion on generalization accuracy were measuredusing an experimental matrix based on 5 lin-guistic datasets, and 8 experimental condi-tions, addressing feature selection-based ECOCvs.
voting-based ECOC, MVDM, values ofk larger than 1, and dichotomizer weight-ing.
The following linguistic tasks were used.DIMIN is a Dutch diminutive formation task de-rived from the Celex lexical database for Dutch(Baayen et al, 1993).
It predicts Dutch nomi-nal diminutive suffixes from phonetic properties(phonemes and stress markers) of maximally the57last three syllables of the noun.
The STRESStask, also derived from the Dutch Celex lexPcal database, assigns primary stress on the ba-sis of phonemic values.
MORPH assigns mor-phological boundaries (a.o.
root morpheme,stress-changing affix, inflectional morpheme),based on English CELEX data.
The WSJ-NPVP task deals with NP-VP chunking of PoS-tagged Wall Street Journal material.
GRAPHON,finally, is a grapheme-to-phoneme conversiontask for English based on the English Celex lex-ical database.
Numeric characteristics of thedifferent asks are listed in table 1.
All taskswith the exception of GRAPHON happened tobe five-class problems; for GRAPHON, a five-class subset was taken from the original trainingset, in order to keep computational demandsmanageable.
The tasks were subjected to theData set Features Classes InstancesDIMIN 12 5 3,000STRESS 12 5 3,000MORPH 9 5 300,000NPVP 8 5 200,000GRAPHON 7 5 73,525Table 1: Data sets.8 different experimental situations of table 2.For feature selection-based ECOC, backward se-quential feature elimination was used (Raaij-makers, 1999), repeatedly eliminating featuresin turn and evaluating each elimination stepwith 10-fold cross-validation.
For dichotomizerweighting, error information of the dichotomiz-ers, determined from separate unweighted 10-fold cross-validation experiments on a separatetraining set, produced a weighted Hamming dis-tance metric.
Error-based weights were basedon raising a small constant ~ in the interval\[0, 1) to the power of the number of errors madeby the dichotomizer (Cesa-Bianchi et al, 1996).Random feature selection drawing features withreplacement created feature sets of both differ-ent size and composition for every dichotomizer.5 Resu l tsTable 3 lists the generalization accuracies forthe control groups, and table 4 for the ECOCalgorithms.
All accuracy results are based on10-fold cross-validation, with p < 0.05 usingpaired t-tests.
The results show that dis-ALGORITHM DESCRIPTIONE1?2E3E4?5?6?7$8ECOC, feature selection per bit (15),k----l, unweightedECOC, feature selection per bit (15),k----l, weightedECOC, feature selection per bit (15),MVDM, k=l, unweightedECOC, feature selection per bit (15),MVDM, k=l, weightedECOC, feature selection per bit (15),MVDM, k----3, unweightedECOC, feature selection per bit (15),MVDM, k=3, weightedECOC, voting (100) per bit (30),MVDM, k=3ECOC, voting (100) per bit block(15), MVDM, k=3Table 2: AlgorithmsGRouP I II III IVIBi-IG IBi-IG IBi-IG IBi-IG'k=l k=3 k=l k=3MVDM MVDM98.1?0.5 DIMINSTRESSMORPHNPVPGRAPHON98.1?0.583.5?2.692.5?1.496.4?0.297.1?2.495.8?0.581.3?2.992.0?1.497.1?0.297.2?2.397.7?0.786.2?2.092.5?1.497.0?0.197.7?0.786.7?1.892.5?1.497.0?0.197.7?0.8Table 3: Generalization accuracies control groups.tributed class representations can lead to sta-tistically significant accuracy gains for a varietyof linguistic tasks.
The ECOC algorithm basedon feature selection and weighted Hamming dis-tance performs best.
Voting-based ECOC per-forms poorly on DIMIN and STRESS with vot-ing per bit, but significant accuracy gains areachieved by voting per block, putting it on a parwith the best performing algorithm.
Regressionanalysis was applied to investigate the effect ofthe Modified Value Difference Metric on ECOCaccuracy.
First, the accuracy gain of MVDMas a function of the information gain ratio ofthe features was computed.
The results show ahigh correlation (0.82, significant at p < 0.05)between these variables, indicating a linear re-lation.
This is in line with the idea underlyingMVDM: whenever two feature values are verypredictive of a shared class, they contribute tothe similarity between the instances they belongto, which will lead to more accurate classifiers.Next, regression analysis was applied to deter-mine the effect of MVDM on ECOC, by relatingthe accuracy gain of MVDM (k=3) compared to58TASK ?1 (I) g2(I) $3 (III) $4 (III) $5 (IV) $6 (IV) $7 (IV) g8 ($6)DIMIN 98.6=k0.4x/ 98.5=k0.4x/ 98.6:k0.6~/ 98.7::k0.6x/ 98.8::k0.5x/ 98.9::k0.4~/ 96.6:k0.9x 98.4=E0.4STRESS 85.3::kl.Sx/ 86.3::k2.0X/ 88.2=hl.7x/ 88.8=t:1.7X/ 88.2:kl.7x/ 89.3::kl.9~/ 86.5=k2.3x 88.8::kl.7MORPH 93.2:kl.6x/ 93.2=kl.5x/ 93.2=kl.3x/ 93.2=kl.3~/ 93.2=1=1.6~/ 93.2=kl.5~/ 93.0::kl.6x/ 93.4:t=1.5x/NPVP t 96.8::k0.1~/ 96.9::k0.2x/ 96.8=E0.1 96.9:k0.1 96.8::k0.1 96.9=k0.1 96.8=h0.2x 96.8=t:0.2GRAPHON 98.2=t=0.7 98.3=t=0.7 98.4:k0.6X/ 98.3=E0.5X/ 98.3::h0.6X/ 98.5::k0-5X/ 97.6=k0.7x 97.6:h0.8xTable 4: Generalization accuracies for feature selection-based ECOC (x/ indicates significant improvement overcontrol group (in round brackets) , and x deterioration at p < 0.05 using paired t-tests).
A 1" indicates 25 voters forperformance reasons.control group II to the accuracy gain of ECOC(algorithm $6, compared to control group IV).The correlation between these two variables isvery high (0.93, significant at p < 0.05), againindicative of a linear relation.
From the per-spective of learning class boundaries, the strongeffect of MVDM on ECOC accuracy can be un-derstood as follows.
When the overlap metric isused, members of a training set belonging to thesame class may be situated arbitrarily remotefrom each other in the feature hyperspace.
Forinstance, consider the following two instancestaken from DIMIN:.
.
.
.
.
.
.
.
.
d ,A ,k , je.
.
.
.
.
.
.
.
.
d,A,x,je(Hyphens indicate absence of feature values.
)These two instances encode the diminutive for-mation of Dutch dakje (little roo\]~ from dak(roo\]~, and dagje (lit.
little day, proverbiallyused) from dag (day).
Here, the values k and x,corresponding to the velar stop 'k' and the ve-lar fricative 'g', are minimally different from aphonetic perspective.
Yet, these two instanceshave coordinates on the twelfth dimension ofthe feature hyperspace that have nothing to dowith each other.
The overlap treats the k-xvalue clash just like any other value clash.
Thisphenomenon may lead to a situation where in-habitants of the same class are scattered overthe feature hyperspace.
In contrast, a value dif-ference metric like MVDM which attempts togroup feature values on the basis of class cooc-currence information, might group k and x to-gether if they share enough classes.
The effectof MVDM on the density of the feature hyper-space can be compared with the density ob-tained with the overlap metric as follows.
First,plot a random numerical transform of a featurespace.
For expository reasons, it is adequateto restrict attention to a low-dimensional (e.g.two-dimensional) subset of the feature space, fora specific class C. Then, plot an MVDM trans-form of this feature space, where every coordi-nate (a, b) is transformed into (P(Cla) ,  P(C Ib)).
This idea is applied to a subset of DIMIN,consisting of all instances classified as j e (oneof the five diminutive suffixes for Dutch).
Thefeatures for this subset were limited to the lasttwo, consisting of the rhyme and coda of thelast syllable of the word, clearly the most infor-mative features for this task.
Figure 2 displaysthe two scatter plots.
As can be seen, instancesare widely scattered over the feature space forthe numerical transform, whereas the MVDM-based transform forms many clusters and pro-duces much higher density.
In a condensed fea-70o60~30?D20++ ?
+% +++t4  t ++t +++ +% +#$+ + + ++ + ++ ?
+$?
St  ~$~ , ?t  ++ + + ~ ?$:$ +t+ - t t - i - t~ + +?
?i i | I I I I I5 10 '15 20 25 30 35 40 45Feature 11 DIMIN (random)'i0.90.80.7_z 0.6~.
0.40.3.
0.20.10-~.
@ + ?
4, ?41.0,1 0.2 0.3 0.4 0.5 0.6 0.7Feature I | DIMIN (MVDM)Figure 2: Random numerical transform of feature val-ues based on the overlap metric (left) vs. numericaltransform of feature values based on MVDM (right), fora two-features-one-class subset of DIMIN.ture hyperspace the number of class boundariesto be learned per bit function reduces.
For in-stance, figures 3 displays the class boundariesfor a relatively condensed feature hyperspace,where classes form localized populations, and ascattered feature hyperspace, with classes dis-tributed over non-adjacent regions.
The num-ber of class boundaries in the scattered featurespace is much higher, and this will put an addi-59tional burden on the learning problems consti-tuted by the various bit functions.C1 b13~ 3 b35hi2 i C5b15C2b24 bl2ii IC4 b14 CIFlF2 bl2i I C2 b24i I C4b35i C3 ~ _ ~b3 ii C4 b24ii C2FIFigure 3: Condensed feature space (left) vs. scatteredfeature space (right).6 Conc lus ionsThe use of error-correcting output codes(ECOC) for representing natural languageclasses has been empirically validated for a suiteof linguistic tasks.
Results indicate that ECOCcan be useful for datasets with features withhigh class predictivity.
These sets typically tendto benefit from the Modified Value DifferenceMetric, which creates a condensed hyperspaceof features.
This in turn leads to a lower num-ber of class boundaries to be learned per bitfunction, which simplifies the binary subclas-sification tasks.
A voting algorithm for learn-ing blocks of bits proves as accurate as an ex-pensive feature-selecting algorithm.
Future re-search will address further mechanisms of learn-ing complex regions of the class boundary land-scape, as well as alternative rror-correcting ap-proaches to classification.AcknowledgementsThanks go to Francesco Ricci for assistance ingenerating the error-correcting codes used inthis paper.
David Aha and the members of theInduction of Linguistic Knowledge (ILK) Groupof Tilburg University and Antwerp Universityare thanked for helpful comments and criticism.ReferencesH.
Baayen, R. Piepenbrock, and H. van Rijn.
1993.The CELEX database on CD-ROM.
LinguisticData Consortium.
Philadelpha, PA.S.
Bay.
1999.
Nearest neighbor classification frommultiple feature subsets.
Intelligent Data Analy-sis, 3(3):191-209.A.
Berger.
1999.
Error-correcting output codingfor text classification.
Proceedings of IJCAI'99:Workshop on machine learning for informationfiltering.C.
Cardie, S. Mardis, and D. Pierce.
1999.
Com-bining error-driven pruning and classification forpartial parsing.
Proceedings of the Sixteenth In-ternational Conference on Machine Learning, pp.87-96.N.
Cesa-Bianchi, Y. Freund, D. Helmbold, andM.
Warmuth.
1996.
On-line prediction and con-version strategies.
Machine Learning 27:71-110.S.
Cost and S. Salzberg.
1993.
A weighted near-est neighbor algorithm for learning with symbolicfeatures.
Machine Learning,10:57-78.W.
Daelemans, J. Zavrel, P. Berck, and S. Gillis.1996.
Mbt: A memory-based part of speech tag-ger generator.
Proceedings of the Fourth Work-shop on Very Large Corpora, ACL SIGDAT.W.
Daelemans, J. Zavrel, K. Van der Sloot, andA.
Van den Bosch.
1999.
Timbh Tilburg memorybased learner, version 2.0, reference guide.
ILKTechnical Report - ILK 99-01.
Tilburg.T.
Dietterich and G. Bakiri.
1995.
Solving multi-class learning problems via error-correcting out-put codes.
Journal of Artificial Intelligence Re-search, 2:263-286.R.
Duda and P. Hart.
1973.
Pattern classificationand scene analysis.
Wiley Press.E.
Kong and T. Dietterich.
1995.
Error-correctingoutput coding corrects bias and variance.
Pro-ceedings of the 12th International Conference onMachine Learning.J.R.
Quinlan.
1993.
C4.5: Programs for MachineLearning.
Morgan Kaufmann, San Mateo, Ca.S.
Raaijmakers.
1999.
Finding representations formemory-based language learning.
Proceedings ofCoNLL-1999.F.
Ricci and D. Aha.
1997.
Extending local learnerswith error-correcting output codes.
Proceedings ofthe l~th Conference on Machine Learning.D.
Roth.
1998.
A learning approach to shallow pars-ing.
Proceedings EMNLP- WVLC'99.H.
Schmid.
1994.
Part-of-speech tagging with neu-ral networks.
Proceedings COLING-9~.C.
Shannon.
1948.
A mathematical theory of com-munication.
Bell System Technical Journal,27:7,pp.
379-423, 27:10, pp.
623-656.A.
van den Bosch and W. Daelemans.
1993.
Data-oriented methods for grapheme-to-phoneme con-version.
Proceedings of the 6th Conference of theEACL.60
