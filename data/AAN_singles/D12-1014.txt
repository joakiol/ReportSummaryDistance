Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational NaturalLanguage Learning, pages 149?159, Jeju Island, Korea, 12?14 July 2012. c?2012 Association for Computational LinguisticsA Weakly Supervised Model for Sentence-Level Semantic OrientationAnalysis with Multiple ExpertsLizhen Qu and Rainer Gemulla and Gerhard WeikumMax Planck Institute for InformaticsSaarbru?cken, Germany{lqu,rgemulla,weikum}@mpi-inf.mpg.deAbstractWe propose the weakly supervised Multi-Experts Model (MEM) for analyzing the se-mantic orientation of opinions expressed innatural language reviews.
In contrast to mostprior work, MEM predicts both opinion po-larity and opinion strength at the level of in-dividual sentences; such fine-grained analysishelps to understand better why users like ordislike the entity under review.
A key chal-lenge in this setting is that it is hard to ob-tain sentence-level training data for both po-larity and strength.
For this reason, MEM isweakly supervised: It starts with potentiallynoisy indicators obtained from coarse-grainedtraining data (i.e., document-level ratings), asmall set of diverse base predictors, and, ifavailable, small amounts of fine-grained train-ing data.
We integrate these noisy indicatorsinto a unified probabilistic framework usingideas from ensemble learning and graph-basedsemi-supervised learning.
Our experiments in-dicate that MEM outperforms state-of-the-artmethods by a significant margin.1 IntroductionOpinion mining is concerned with analyzing opin-ions expressed in natural language text.
For example,many internet websites allow their users to provideboth natural language reviews and numerical ratingsto items of interest (such as products or movies).In this context, opinion mining aims to uncover therelationship between users and (features of) items.Preferences of users to items can be well understoodby coarse-grained methods of opinion mining, whichfocus on analyzing the semantic orientation of doc-uments as a whole.
To understand why users like ordislike certain items, however, we need to performmore fine-grained analysis of the review text itself.In this paper, we focus on sentence-level analy-sis of semantic orientation (SO) in online reviews.The SO consists of polarity (positive, negative, orother1) and strength (degree to which a sentence ispositive or negative).
Both quantities can be ana-lyzed jointly by mapping them to numerical ratings:Large negative/positive ratings indicate a strong neg-ative/positive orientation.
A key challenge in fine-grained rating prediction is that fine-grained train-ing data for both polarity and strength is hard toobtain.
We thus focus on a weakly supervised set-ting in which only coarse-level training data (suchas document ratings and subjectivity lexicons) and,optionally, a small amount of fine-grained trainingdata (such as sentence polarities) is available.A number of lexicon-based approaches for phrase-level rating prediction has been proposed in the liter-ature (Taboada et al2011; Qu et al2010).
Thesemethods utilize a subjectivity lexicon of words alongwith information about their semantic orientation;they focus on phrases that contain words from thelexicon.
A key advantage of sentence-level methodsis that they are able to cover all sentences in a reviewand that phrase identification is avoided.
To the bestof our knowledge, the problem of rating predictionat the sentence level has not been addressed in theliterature.
A naive approach would be to simply aver-age phrase-level ratings.
Such an approach performs1We assign polarity other to text fragments that are off-topicor not directly related to the entity under review.149poorly, however, since (1) phrases are analyzed outof context (e.g., modal verbs or conditional clauses),(2) domain-dependent information about semanticorientation is not captured in the lexicons, (3) onlyphrases that contain lexicon words are covered.
Here(1) and (2) lead to low precision, (3) to low recall.To address the challenges outlined above, we pro-pose the weakly supervised Multi-Experts Model(MEM) for sentence-level rating prediction.
MEMstarts with a set of potentially noisy indicators of SOincluding phrase-level predictions, language heuris-tics, and co-occurrence counts.
We refer to theseindicators as base predictors; they constitute the setof experts used in our model.
MEM is designedsuch that new base predictors can be easily integrated.Since the information provided by the base predictorscan be contradicting, we use ideas from ensemblelearning (Dietterichl, 2002) to learn the most con-fident indicators and to exploit domain-dependentinformation revealed by document ratings.
Thus, in-stead of averaging base predictors, MEM integratestheir features along with the available coarse-grainedtraining data into a unified probabilistic model.The integrated model can be regarded as a Gaus-sian process (GP) model (Rasmussen, 2004) witha novel multi-expert prior.
The multi-expert priordecomposes into two component distributions.
Thefirst component distribution integrates sentence-localinformation obtained from the base predictors.
Itforms a special realization of stacking (Dzeroski andZenko, 2004) but uses the features from the base pre-dictors instead of the actual predictions.
The secondcomponent distribution propagates SO informationacross similar sentences using techniques from graph-based semi-supervised learning (GSSL) (Zhu et al2003; Belkin et al2006).
It aims to improve thepredictions on sentences that are not covered wellenough by our base predictors.
Traditional GSSL al-gorithms support either discrete labels (classification)or numerical labels (regression); we extend thesetechniques to support both types of labels simulta-neously.
We use a novel variant of word sequencekernels (Cancedda et al2003) to measure sentencesimilarity.
Our kernel takes the relative positions ofwords but also their SO and synonymity into account.Our experiments indicate that MEM significantlyoutperforms prior work in both sentence-level ratingprediction and sentence-level polarity classification.2 Related WorkThere exists a large body of work on analyzing thesemantic orientation of natural language text.
Ourapproach is unique in that it is weakly supervised,predicts both polarity and strength, and operates onthe sentence level.Supervised approaches for sentiment analysis fo-cus mainly on opinion mining at the documentlevel (Pang and Lee, 2004; Pang et al2002; Pangand Lee, 2005; Goldberg and Zhu, 2006), but havealso been applied to sentence-level polarity classifi-cation in specific domains (Mao and Lebanon, 2006;Pang and Lee, 2004; McDonald et al2007).
Inthese settings, a sufficient amount of training data isavailable.
In contrast, we focus on opinion miningtasks with little or no fine-grained training data.The weakly supervised HCRF model (Ta?ckstro?mand McDonald, 2011b; Ta?ckstro?m and McDonald,2011a) for sentence-level polarity classification is per-haps closest to our work in spirit.
Similar to MEM,HCRF uses coarse-grained training data and, whenavailable, a small amount of fine-grained sentencepolarities.
In contrast to MEM, HCRF does not pre-dict the strength of semantic orientation and ignoresthe order of words within sentences.There exists a large number of lexicon-based meth-ods for polarity classification (Ding et al2008; Choiand Cardie, 2009; Hu and Liu, 2004; Zhuang et al2006; Fu and Wang, 2010; Ku et al2008).
Thelexicon-based methods of (Taboada et al2011; Quet al2010) also predict ratings at the phrase level;these methods are used as experts in our model.MEM leverages ideas from ensemble learning (Di-etterichl, 2002; Bishop, 2006) and GSSL meth-ods (Zhu et al2003; Zhu and Ghahramani, 2002;Chapelle et al2006; Belkin et al2006).
We extendGSSL with support for multiple, heterogenous labels.This allows us to integrate our base predictors as wellas the available training data into a unified modelthat exploits that strengths of algorithms from bothfamilies.3 Base PredictorsEach of our base predictors predicts the polarity orthe rating of a single phrase.
As indicated above,we do not use these predictions directly in MEM butinstead integrate the features of the base predictors150(see Sec.
4.4).
MEM is designed such that new basepredictors can be integrated easily.Our base predictors use a diverse set of availableweb and linguistic resources.
The hope is that this di-versity increases overall prediction performance (Di-etterichl, 2002): The statistical polarity predictor fo-cuses on local syntactic patterns; it is based on corpusstatistics for SO-carrying words and opinion topicwords.
The heuristic polarity predictor uses manu-ally constructed rules to achieve high precision butlow recall.
Both the bag-of-opinions rating predictorand the SO-CAL rating predictor are based on lexi-cons.
The BoO predictor uses a lexicon trained froma large generic-domain corpus and is recall-oriented;the SO-CAL predictor uses a different lexicon withmanually assigned weights and is precision-oriented.3.1 Statistical Polarity PredictorThe polarity of an SO-carrying word strongly de-pends on its target word.
For example, consider thephrase ?I began this novel with the greatest of hopes[...]?.
Here, ?greatest?
has a positive semantic orien-tation in all subjectivity lexicons, but the combination?greatest of hopes?
often indicates a negative senti-ment.
We refer to a pair of SO-carrying word (?great-est?)
and a target word (?hopes?)
as an opinion-targetpair.
Our statistical polarity predictor learns the po-larity of opinions and targets jointly, which increasesthe robustness of its predictions.Syntactic dependency relations of the formAR??
B are a strong indicator for opinion-targetpairs (Qiu et al2009; Zhuang et al2006); e.g.,?great?nmod?????product?.
To achieve high precision,we only consider pairs connected by the follow-ing predefined set of shortest dependency paths:verbsubj???
noun, verbobj??
noun, adjnmod????
noun,adjprd???
verbsubj???
noun.
We only retain opinion-target pairs that are sufficiently frequent.For each extracted pair z, we count how often itco-occurs with each document polarity y ?
Y , whereY = {positive, negative, other} denotes the set of po-larities.
If z occurs in a document but is preceded bya negator, we treat it as a co-occurrence of oppositedocument polarity.
If z occurs in a document with po-larity other, we count the occurrence with only halfweight, i.e., we increase both #z and #(other, z)by 0.5.
These documents are typically a mixture ofpositive and negative opinions so that we want toreduce their impact.
The marginal distribution ofpolarity label y given that z occurs in a sentence isestimated as P (y | z) = #(y, z)/#z.
The predictoris trained using the text and ratings of the reviews inthe training data, i.e., without relying on fine-grainedannotations.The statistical polarity predictor can be used to pre-dict sentence-level polarities by averaging the phrase-level predictions.
As discussed previously, such anapproach is problematic; we use it as a baseline ap-proach in our experimental study.
We also employphrase-level averaging to estimate the variance ofbase predictors; see Sec.
4.3.
Denote by Z(x) the setof opinion-target pairs in sentence x.
To predict thesentence polarity y ?
Y , we take the Bayesian aver-age of the phrase-level predictors: P (y | Z(x)) =?z?Z(x) P (y | z)P (z) =?z?Z(x) P (y, z).
Thusthe most likely polarity is the one with the highestco-occurrence count.3.2 Heuristic Polarity PredictorHeuristic patterns can also serve as base predictors.In particular, we found that some authors list positiveand negative aspects separately after keywords suchas ?pros?
and ?cons?.
A heuristic that exploits suchpatterns achieved a high precision (> 90%) but lowrecall (< 5%) in our experiments.3.3 Bag-of-Opinions Rating PredictorWe leverage the bag-of-opinion (BoO) model of Qu etal.
(2010) as a base predictor for phrase-level ratings.The BoO model was trained from a large genericcorpus without fine-grained annotations.In BoO, an opinion consists of three components:an SO-carrying word (e.g., ?good?
), a set of intensi-fiers (e.g., ?very?)
and a set of negators (e.g., ?not?
).Each opinion is scored based on these words (repre-sented as a boolean vector b) and the polarity of theSO-carrying word (represented as sgn(r) ?
{?1, 1})as indicated by the MPQA lexicon of Wilson etal.
(2005).
In particular, the score is computed assgn(r)?Tb, where ?
is the learned weight vector.The sign function sgn(r) ensures consistent weightassignment for intensifiers and negators.
For exam-ple, an intensifier like ?very?
can obtain a large posi-tive or a large negative weight depending on whetherit is used with a positive or negative SO-carrying151word, respectively.3.4 SO-CAL Rating PredictorThe Semantic Orientation Calculator (SO-CAL) ofTaboada et al2011) also predicts phrase-level rat-ings via a scoring function similar to the one of BoO.The SO-CAL predictor uses a manually created lexi-con, in which each word is classified as either an SO-carrying word (associated with a numerical score), anintensifier (associated with a modifier on the numer-ical score), or a negator.
SO-CAL employs variousheuristics to detect irrealis and to correct for the pos-itive bias inherent in most lexicon-based classifiers.Compared to BoO, SO-CAL has lower recall buthigher precision.4 Multi-Experts ModelOur multi-experts model incorporates features fromthe individual base predictors, coarse-grained labels(i.e., document ratings or polarities), similarities be-tween sentences, and optionally a small amount ofsentence polarity labels into an unified probabilisticmodel.
We first give an overview of MEM, and thendescribe its components in detail.4.1 Model OverviewDenote by X = {x1, .
.
.
,xN} a set of sentences.We associate each sentence xi with a set of initiallabels y?i, which are strong indicators of semanticorientation: the coarse-grained rating of the corre-sponding document, the polarity label of our heuristicpolarity predictor, the phrase-level ratings from theSO-CAL predictor, and optionally a manual polaritylabel.
Note that the number of initial labels may varyfrom sentence to sentence and that initial labels areheterogeneous in that they refer to either polaritiesor ratings.
Let Y?
= {y?1, .
.
.
, y?N}.
Our goal is topredict the unobserved ratings r = {r1, .
.
.
, rN} ofeach sentence.Our multi-expert model is a probabilistic modelfor X, Y?, and r. In particular, we model the ratingvector r via a multi-expert prior PE(r | X,?)
withparameter ?
(Sec.
4.2).
PE integrates both featuresfrom the base predictors and sentence similarities.We correlate ratings to initial labels via a set of con-ditional distributions Pb(y?b | r), where b denotes thetype of initial label (Sec.
4.3).
The posterior of r isthen given byP (r | X, Y?,?)
?
?bPb(y?b | r)PE(r | X,?
).Note that the posterior is influenced by both the multi-expert prior and the set of initial labels.We use MAP inference to obtain the most likelyrating of each sentence, i.e., we solveargminr,??
?blog(Pb(y?b | r))?
log(PE(r | X,?
)),where as before ?
denotes the model parameters.
Wesolve the above optimization problem using cycliccoordinate descent (Friedman et al2008).4.2 Multi-Expert PriorThe multi-expert prior PE(r | X,?)
consists of twocomponent distributions N1 and N2.
DistributionN1 integrates features from the base predictors, N2incorporates sentence similarities to propagate infor-mation across sentences.In a slight abuse of notation, denote by xi the set offeatures for the i-th sentence.
Vector xi contains thefeatures of all the base predictors but also includes bi-gram features for increased coverage of syntactic pat-terns; see Sec.
4.4 for details about the feature design.Letm(xi) = ?Txi be a linear predictor for ri, where?
is a real weight vector.
Assuming Gaussian noise,ri follows a Gaussian distribution N1(ri | mi, ?2)with mean mi = m(xi) and variance ?2.
Note thatpredictor m can be regarded as a linear combinationof base predictors because both m and each of thebase predictors are linear functions.
By integratingall features into a single function, the base predictorsare trained jointly so that weight vector ?
automati-cally adapts to domain-dependent properties of thedata.
This integrated approach significantly outper-formed the alternative approach of using a weightedvote of the individual predictions made by the basepredictors.
We regularize the weight vector ?
us-ing a Laplace prior P (?
| ?)
with parameter ?
toencourage sparsity.Note that the bigram features in xi partially cap-ture sentence similarity.
However, such features can-not be extended to longer subsequences such as tri-grams due to data sparsity: useful features becomeas infrequent as noisy terms.
Moreover, we would152like to capture sentence similarity using gapped (i.e.,non-consecutive) subsequences.
For example, thesentences ?The book is an easy read.?
and ?It is easyto read.?
are similar but do not share any consecutivebigrams.
They do share the subsequence ?easy read?,however.
To capture this similarity, we make use of anovel sentiment-augmented variant of word sequencekernels (Cancedda et al2003).
Our kernel is usedto construct a similarity matrix W among sentencesand the corresponding regularized Laplacian L?.
Tocapture the intuition that similar sentences shouldhave similar ratings, we introduce a Gaussian priorN2(r | 0, L?
?1) as a component into our multi-expertprior; see Sec.
4.5 for details and a discussion ofwhy this prior encourages similar ratings for similarsentences.Since the two component distributions feature dif-ferent expertise, we take their product and obtain themulti-expert priorPE(r | X,?)
?
N1(r |m, I?2)N2(r | 0, L?
?1)P (?
| ?
),where m = (m1, .
.
.
,mN ).
Note that the normal-izing constant of PE can be ignored during MAPinference since it does not depend on ?.4.3 Incorporating Initial LabelsRecall that the initial labels Y?
are strong indica-tors of semantic orientation associated with eachsentence; they correspond to either discrete polaritylabels or to continuous rating labels.
This hetero-geneity constitutes the main difficulty for incorporat-ing the initial labels via the conditional distributionsPb(y?b | r).
We assume independence throughout sothat Pb(y?b | r) =?i Pb(y?bi | ri).Rating Labels For continuous labels, we assumeGaussian noise and set Pb(y?bi | ri) = N (y?bi | ri, ?bi ),where variance ?bi is a type- and sentence-dependent.For SO-CAL labels, we simply set ?SO-CALi =?SO-CAL, where ?SO-CAL is a hyperparameter.
TheSO-CAL scores have limited influence in our overallmodel; we found that more complex designs lead tolittle improvement.
We proceed differently for docu-ment ratings.
Our experiment suggests that documentratings constitute the most important indicator of theSO of a sentence.
Thus sentence ratings should beclose to document ratings unless strong evidence tothe contrary exists.
In other words, we want variance?Doci to be small.When no manually created sentence-level polar-ity labels are available, we set the value of ?Doci de-pending on the polarity class.
In particular, we set?Doci = 1 for both positive and negative documents,and ?Doci = 2 for neutral documents.
The reasoningbehind this choice is that sentence ratings in neu-tral documents express higher variance because thesedocuments often contain a mixture of positive andnegative sentences.When a small set of manually created sentencepolarity labels is available, we train a classifier thatpredicts whether the sentence polarity coincides withthe document polarity.
If so, we set the correspondingvariance ?Doci to a small value; otherwise, we choosea larger value.
In particular, we train a logistic regres-sion classifier (Bishop, 2006) using the followingbinary features: (1) an indicator variable for eachdocument polarity, and (2) an indicator variable foreach triple of base predictor, predicted polarity, anddocument polarity (set to 1 if the polarities match).We then set ?Doci = (?pi)?1, where pi is the probabil-ity of matching polarities obtained from the classifierand ?
is a hyperparameter that ensures correct scal-ing.Polarity Labels We now describe how to modelthe correlation between the polarity of a sentence andits rating.
An simple and effective approach is topartition the range of ratings into three consecutivepartitions, one for each polarity class.
We thus consid-ering the polarity classes {positive, other, negative}as ordered and formulate polarity classification as anordinal regression problem (Chu and Ghahramani,2006).
We immediately obtain the distributionPb(y?bi = pos | ri) = ?
(ri ?
b+?
?b)Pb(y?bi = oth | ri) = ?
(b+ ?
ri??b)?
?(b?
?
ri?
?b)Pb(y?bi = neg | ri) = ?(b?
?
ri?
?b),where b+ and b?
are the partition boundaries betweenpositive/other and other/negative, respectively,2 ?
(x)denotes the cumulative distribution function of the2We set b+ = 0.3 and b?
= ?0.3 to calibrate to SO-CAL,which treats ratings in [?0.3, 0, 3] as polarity other.153Figure 1: Distribution of polarity given rating.Gaussian distribution, and variance ?b is a hyper-parameter.
It is easy to verify that?y?bi?Yp(y?bi |ri) = 1.
The resulting distribution is shown in Fig.
1.We can use the same distribution to use MEM forsentence-level polarity classification; in this case, wepick the polarity with the highest probability.4.4 Incorporating Base PredictorsBase predictors are integrated into MEM via compo-nent N1(ri | mi, ?2) of the multi-expert prior (seeSec.
4.2).
Recall that mi is a linear function of thefeatures xi of each sentence.
In this section, we dis-cuss how xi is constructed from the features of thebase predictors.
New base predictors can be inte-grated easily by exposing their features to MEM.Most base predictors operate on the phrase level;our goal is to construct features for the entire sen-tence.
Denote by nbi the number of phrases in thei-th sentence covered by base predictor b, and letobij denote a set of associated features.
Features obijmay or may not correspond directly to the featuresof base predictor b; see the discussion below.
Astraightforward strategy is to set xbi = (nbi)?1?j obij .We proceed slightly differently and average the fea-tures associated with phrases of positive prior polar-ity separately from those of phrases with negativeprior polarity (Taboada et al2011).
We then con-catenate the averaged feature vectors, i.e., we setxbi = (o?b,posij o?b,negij ), where o?b,pij denotes the averageof the feature vectors obij associated with phrases ofprior polarity p. This procedure allows us to learna different weight for each feature depending on itscontext (e.g., the weight of intensifier ?very?
may dif-fer for positive and negative phrases).
We constructxi by concatenating the sentence-level features xbi ofeach base predictor and a feature vector of bigrams.To integrate a base predictor, we only need tospecify the relevant features and, if applicable, priorphrase polarities.
For our choice of base predictors,we use the following features:SO-CAL predictor.
The prior polarity of a SO-CAL phrase is given by the polarity of its SO-carrying word in the SO-CAL lexicon.
The featurevector oSO-CALij consists of the weight of the SO-carrying word from the lexicon as well the set ofnegator words, irrealis marker words, and intensifierwords in the phrase.
Moreover, we add the first twowords preceding the SO-carrying word as contextfeatures (skipping nouns, negators, irrealis markers,and intensifiers, and stopping at clause boundaries).All words are encoded as binary indicator features.BoO predictor.
Similar to SO-CAL, we deter-mine the prior polarity of a phrase based on the BoOdictionary.
In contrast to SO-CAL, we directly usethe BoO score as a feature because the BoO predictorweights have been trained on a very large corpus andare thus reliable.
We also add irrealis marker wordsin the form of indicator features.Statistical polarity predictor.
Recall that the sta-tistical polarity predictor is based on co-occurrencecounts of opinion-topic pairs and document polar-ities.
We treat each opinion-topic pair as a phraseand use the most frequently co-occurring polarityas the phrase?s prior polarity.
We use the logarithmof the co-occurrence counts with positive, negative,and other polarity as features; this set of features per-formed better than using the co-occurrence counts orestimated class probabilities directly.
We also addthe same type of context features as for SO-CAL, butrescale each binary feature by the logarithm of theoccurrence count #z of the opinion-topic pair (i.e.,the features take values in {0, log #z}).4.5 Incorporating Sentence SimilaritiesThe component distribution N2(r | 0, L?
?1) in themulti-expert prior encourages similar sentences tohave similar ratings.
The main purpose of N2 is topropagate information from sentences on which thebase predictors perform well to sentences for whichbase prediction is unreliable or unavailable (e.g., be-154cause they do not contain SO-carrying words).
Toobtain this distribution, we first construct an N ?Nsentence similarity matrix W using a sentiment-augmented word sequence kernel (see below).
Wethen compute the regularized graph Laplacian L?
=L+I/?2 based on the unnormalized graph LaplacianL = D?W (Chapelle et al2006), where D be adiagonal matrix with dii =?j wij and hyperparam-eter ?2 controls the scale of sentence ratings.To gain insight into distribution N2, observe thatN2(r | 0, L??1)?
exp(?12?i,jwij(ri ?
rj)2 ?
?r?22/?2).The left term in the exponent forces the ratings ofsimilar sentences to be similar: the larger the sen-tence similarity wij , the more penalty is paid for dis-similar ratings.
For this reason, N2 has a smoothingeffect.
The right term is an L2 regularizer and encour-ages small ratings; it is controlled by hyperparameter?2.The entries wij in the sentence similarity matrixdetermine the degree of smoothing for each pair ofsentence ratings.
We compute these values by a novelsentiment-augmented word sequence kernel, whichextends the well-known word sequence kernel of Can-cedda et al2003) by (1) BoO weights to strengthenthe correlation of sentence similarity and rating sim-ilarity and (2) synonym resolution based on Word-Net (Miller, 1995).In general, a word sequence kernel computes asimilarity score of two sequences based on theirshared subsequences.
In more detail, we first de-fine a score function for a pair of shared subse-quences, and then sum up these scores to obtainthe overall similarity score.
Consider for examplethe two sentences ?The book is an easy read.?
(s1)and ?It is easy to read.?
(s2) along with the sharedsubsequence ?is easy read?
(u).
Observe that thewords ?an?
and ?to?
serve as gaps as they are notpart of the subsequence.
We represent subsequenceu in sentence s via a real-valued projection function?u(s).
In our example, ?u(s1) = ?is?gan?easy?readand ?u(s2) = ?is?easy?gto?read.
The decay factors?w ?
(0, 1] for matching words characterize theimportance of a word (large values for significantwords).
On the contrary, decay factors ?gw ?
(0, 1]for gap words are penalty terms for mismatches(small values for significant words).
The score ofsubsequence u is defined as ?u(s1)?u(s2).
Thustwo shared subsequences have high similarity if theyshare significant words and few gaps.
Following Can-cedda et al2003), we define the similarity betweentwo sequences askn(si, sj) =?u?
?n?u(si)?u(sj),where ?
is a finite set of words and n denotes thelength of the considered subsequences.
This sim-ilarity function can be computed efficiently usingdynamic programming.To apply the word sequence kernel, we need tospecify the decay factors.
A traditional choice is?w = log( NNw )/ log(N), where Nw is the documentfrequency of the word w and N is the total numberof documents.
This IDF decay factor is not well-suited to our setting: Important opinion words suchas ?great?
have a low IDF value due to their highdocument frequency.
To overcome this problem,we incorporate additional weights for SO-carryingwords using the BoO lexicon.
To do so, we firstrescale the BoO weights into [0, 1] using the sig-moid g(w) = (1 + exp(?a?w + b))?1, where ?wdenotes the BoO weight of word w.3 We then set?w = min(log( NNw )/ log(N) + g(w), 0.9).
The de-cay factor for gaps is given by ?gw = 1 ?
?w.
Thuswe strongly penalize gaps that consist of infrequentwords or opinion words.To address data sparsity, we incorporate synonymsand hypernyms from WordNet into our kernel.
Inparticular, we represent words found in WordNet bytheir first two synset names (for verbs, adjectives,nouns) and their direct hypernym (nouns only).
Twowords are considered the same when their synsetsoverlap.
Thus, for example, ?writer?
has the samerepresentation as ?author?.To build the similarity matrix W, we constructa k-nearest-neighbor graph for all sentences.4 Weconsider subsequences consisting of three words (i.e.,wij = k3(si, sj)); longer subsequences are overlysparse, shorter subsequences are covered by the bi-grams features in N1.3We set a = 2 and b = 1 in our experiments.4We use k = 15 and only consider neighbors with a similar-ity above 0.001.1555 ExperimentsWe evaluated both MEM and a number of alternativeapproaches for both sentence-level polarity classifi-cation and sentence-level strength prediction acrossa number of domains.
We found that MEM out-performs state-of-the-art approaches by a significantmargin.5.1 Experimental SetupWe implemented MEM as well as the HCRF classi-fier of (Ta?ckstro?m and McDonald, 2011a; Ta?ckstro?mand McDonald, 2011b), which is the best-performingestimator of sentence-level polarity in the weakly-supervised setting reported in the literature.
We trainboth methods using (1) only coarse labels (MEM-Coarse, HCRF-Coarse) and (2) additionally a smallnumber of sentence polarities (MEM-Fine, HCRF-Fine5).
We also implemented a number of baselinesfor both polarity classification and strength predic-tion: a document oracle (DocOracle) that simply usesthe document label for each sentence, the BoO rat-ing predictor (BaseBoO), and the SO-CAL rating pre-dictor (BaseSO-CAL).
For polarity classification, wecompare our methods also to the statistical polaritypredictor (Basepolarity).
To judge on the effectivenessof our multi-export prior for combining base predic-tors, we take the majority vote of all base predic-tors and document polarity as an additional baseline(Majority-Vote).
Similarly, for strength prediction,we take the arithmetic mean of the document rat-ing and the phrase-level predictions of BaseBoO andBaseSO-CAL as a baseline (Mean-Rating).
We use thesame hyperparameter setting for MEM across all ourexperiments.We evaluated all methods on Amazon reviewsfrom different domains using the corpus of Ding et al(2008) and the test set of Ta?ckstro?m and McDonald(2011a).
For each domain, we constructed a large bal-anced dataset by randomly sampling 33,000 reviewsfrom the corpus of Ding et al2008).
We chosethe books, electronics, and music domains for ourexperiments; the dvd domain was used for develop-ment.
For sentence polarity classification, we use thetest set of Ta?ckstro?m and McDonald (2011a), which5We used the best-performing model that fuses HCRF-Coarseand the supervised model (McDonald et al2007) by interpola-tion.contains roughly 60 reviews per domain (20 for eachpolarity).
For strength evaluation, we created a testset of 300 pairs of sentences per domain from thepolarity test set.
Each pair consisted of two sentencesof the same polarity; we manually determined whichof the sentences is more positive.
We chose this pair-wise approach because (1) we wanted the evaluationto be invariant to the scale of the predicted ratings,and (2) it much easier for human annotators to ranka pair of sentences than to rank a large collection ofsentences.We followed Ta?ckstro?m and McDonald (2011b)and used 3-fold cross-validation, where each foldconsisted of a set of roughly 20 documents from thetest set.
In each fold, we merged the test set with thereviews from the corresponding domain.
For MEM-Fine and HCRF-Fine, we use the data from the othertwo folds as fine-grained polarity annotations.
Forour experiments on polarity classification, we con-verted the predicted ratings of MEM, BaseBoO, andBaseSO-CAL into polarities by the method describedin Sec.
4.3.
We compare the performance of eachmethod in terms of accuracy, which is defined as thefraction of correct predictions on the test set (correctlabel for polarity / correct ranking for strength).
Allreported numbers are averages over the three folds.
Inour tables, boldface numbers are statistically signifi-cant against all other methods (t-test, p-value 0.05).5.2 Results for Polarity ClassificationTable 1 summarizes the results of our experiments forsentence polarity classification.
The base predictorsperform poorly across all domains, mainly due tothe aforementioned problems associated with aver-aging phrase-level predictions.
In fact, DocOracleperforms almost always better than any of the basepredictors.
However, accurracy increases when wecombine base predictors and DocOracle using ma-jority voting, which indicates that ensemble methodswork well.When no fine-grained annotations are available(HCRF-Coarse, MEM-Coarse), both MEM-Coarseand Majority-Vote outperformed HCRF-Coarse,which in turn has been shown to outperform a num-ber of lexicon-based methods as well as classifierstrained on document labels (Ta?ckstro?m and McDon-ald, 2011a).
MEM-Coarse also performs better thanMajority-Vote.
This is because MEM propagates156Book Electronics Music AvgBasepolarity 43.7 40.3 43.8 42.6BaseBoO 50.9 48.9 52.6 50.8BaseSO-CAL 44.6 50.2 45.0 46.6DocOracle 51.9 49.6 59.3 53.6Majority-Vote 53.7 53.4 58.7 55.2HCRF-Coarse 52.2 53.4 57.2 54.3MEM-Coarse 54.4 54.9 64.5 57.9HCRF-Fine 55.9 61.0 58.7 58.5MEM-Fine 59.7 59.6 63.8 61.0Table 1: Accuracy of polarity classification per do-main and averaged across domains.evidence across similar sentences, which is espe-cially useful when no explicit SO-carrying wordsexist.
Also, MEM learns weights of features of basepredictors, which leads to a more adaptive integration,and our ordinal regression formulation for polarityprediction allows direct competition among positiveand negative evidence for improved accuracy.When we incorporate a small amount of sentencepolarity labels (HCRF-Fine, MEM-Fine), the accu-racy of all models greatly improves.
HCRF-Fine hasbeen shown to outperform the strongest supervisedmethod on the same dataset (McDonald et al2007;Ta?ckstro?m and McDonald, 2011b).
MEM-Fine fallsshort of HCRF-Fine only in the electronics domainbut performs better on all other domains.
In the bookand music domains, where MEM-Fine is particularlyeffective, many sentences feature complex syntac-tic structure and SO-carrying words are often usedwithout reference to the quality of the product (but todescribe contents, e.g., ?a love story?
or ?a horribleaccident?
).Our models perform especially well when they areapplied to sentences containing no or few opinionwords from lexicons.
Table 2 reports the evaluationresults for both sentences containing SO-carryingwords from either MPQA or SO-CAL lexicons andfor sentences containing no such words.
The re-sults explain why our model falls short of HCRF-Fine in the electronics domain: reviews of electronicproducts contain many SO-carrying words, whichalmost always express opinions.
Nevertheless, MEM-Fine handles sentences without explicit SO-carryingwords well across all domains; here the propagationof information across sentences helps to learn the SOBook Electronics Musicop fact op fact op factHCRF-Fine 55.7 55.9 63.3 54.6 59.0 57.4MEM-Fine 58.9 62.4 60.7 56.7 64.5 60.8Table 2: Accuracy of polarity classification for sen-tences with opinion words (op) and without opinionwords (fact).of facts (such as ?short battery life?
).We found that for all methods, most of the errorsare caused by misclassifying positive/negative sen-tences as other and vice versa.
Moreover, sentenceswith polarity opposite to the document polarity arehard cases if they do not feature frequent strong pat-terns.
Another difficulty lies in off-topic sentences,which may contain explicit SO-carrying words butare not related to the item under review.
This is oneof the main reasons for the poor performance of thelexicon-based methods.Overall, we found that MEM-Fine is the method ofchoice.
Thus our multi-expert model can indeed bal-ance the strength of the individual experts to obtainbetter estimation accuracy.5.3 Results for Strength PredictionTable 3 shows the accuracy results for strength pre-diction.
Here our models outperformed all baselinesby a large margin.
Although document ratings arestrong indicators in the polarity classification task,they lead to worse performance than lexicon-basedmethods.
The main reason for this drop in accuracyis that the document oracle assigns the same ratingto all sentences within a review.
Thus DocOraclecannot rank sentences from the same review, whichis a severe limitation.
This shortage can be partlycompensated by averaging the base predictions anddocument rating (Mean-Rating).
Note that it is non-trivial to apply existing ensemble methods for theweights of individual base predictors because of theabsence of the sentence ratings as training labels.
Incontrast, our MEM models use indirect supervisionto adaptively assign weights to the features from basepredictors.
Similar to polarity classification, a smallamount of sentence polarity labels often improvedthe performance of MEM.157Book Electronics Music AvgBaseBoO 58.3 51.6 53.5 54.5BaseSO-CAL 60.6 57.1 47.6 55.1DocOracle 45.1 36.2 41.4 40.9Mean-Rating 70.3 57.0 60.8 62.7MEM-Coarse 68.7 60.5 69.5 66.2MEM-Fine 72.4 63.3 67.2 67.6Table 3: Accuracy of strength prediction.6 ConclusionWe proposed the Multi-Experts Model for analyz-ing both opinion polarity and opinion strength atthe sentence level.
MEM is weakly supervised; itcan run without any fine-grained annotations but isalso able to leverage such annotations when avail-able.
MEM is driven by a novel multi-expert prior,which integrates a number of diverse base predictorsand propagates information across sentences using asentiment-augmented word sequence kernel.
Our ex-periments indicate that MEM achieves better overallaccuracy than alternative methods.ReferencesMikhail Belkin, Partha Niyogi, and Vikas Sindhwani.2006.
Manifold regularization: A geometric frame-work for learning from labeled and unlabeled examples.The Journal of Machine Learning Research, 7:2399?2434.Christopher M. Bishop.
2006.
Pattern recognition andmachine learning, volume 4.
Springer New York.Nicola Cancedda, E?ric Gaussier, Cyril Goutte, and Jean-Michel Renders.
2003.
Word-sequence kernels.
Jour-nal of Machine Learning Research, 3:1059?1082.Oliver Chapelle, Bernhard Scho?lkopf, and Alexander Zien.2006.
Semi-Supervised Learning.
MIT Press.Yejin Choi and Claire Cardie.
2009.
Adapting a polaritylexicon using integer linear programming for domain-specific sentiment classification.
In Proceedings of theConference on Empirical Methods in Natural LanguageProcessing, volume 2, pages 590?598.Wei Chu and Zoubin Ghahramani.
2006.
Gaussian pro-cesses for ordinal regression.
Journal of MachineLearning Research, 6(1):1019.Thomas G. Dietterichl.
2002.
Ensemble learning.
TheHandbook of Brain Theory and Neural Networks, pages405?408.Xiaowen Ding, Bing Liu, and Philip S. Yu.
2008.
Aholistic lexicon-based approach to opinion mining.
InProceedings of the International Conference on WebSearch and Data Mining, pages 231?240.Saso Dzeroski and Bernard Zenko.
2004.
Is combiningclassifiers with stacking better than selecting the bestone?
Machine Learning, 54(3):255?273.Jerome H. Friedman, Trevor Hastie, and Rob Tibshirani.2008.
Regularization paths for generalized linear mod-els via coordinate descent.
Technical report.Guohong Fu and Xin Wang.
2010.
Chinese sentence-level sentiment classification based on fuzzy sets.
InProceedings of the International Conference on Com-putational Linguistics, pages 312?319.
Association forComputational Linguistics.Andrew B. Goldberg and Xiaojun Zhu.
2006.
Seeingstars when there aren?t many stars: Graph-based semi-supervised learning for sentiment categorization.
InHLT-NAACL 2006 Workshop on Textgraphs: Graph-based Algorithms for Natural Language Processing.Minqing Hu and Bing Liu.
2004.
Mining and summariz-ing customer reviews.
In Proceedings of the ACMSIGKDD Conference on Knowledge Discovery andData Mining, pages 168?177.Lun-Wei Ku, I-Chien Liu, Chia-Ying Lee, Kuan hua Chen,and Hsin-Hsi Chen.
2008.
Sentence-level opinion anal-ysis by copeopi in ntcir-7.
In Proceedings of NTCIR-7Workshop Meeting.Yi Mao and Guy Lebanon.
2006.
Isotonic ConditionalRandom Fields and Local Sentiment Flow.
Advancesin Neural Information Processing Systems, pages 961?968.Ryan T. McDonald, Kerry Hannan, Tyler Neylon, MikeWells, and Jeffrey C. Reynar.
2007.
Structured modelsfor fine-to-coarse sentiment analysis.
In Proceedings ofthe Annual Meeting on Association for ComputationalLinguistics, volume 45, page 432.George A. Miller.
1995.
WordNet: a lexical database forEnglish.
Communications of the ACM, 38(11):39?41.Bo Pang and Lillian Lee.
2004.
A sentimental education:Sentiment analysis using subjectivity summarizationbased on minimum cuts.
In Proceedings of the AnnualMeeting on Association for Computational Linguistics,pages 271?278.Bo Pang and Lillian Lee.
2005.
Seeing stars: Exploitingclass relationships for sentiment categorization withrespect to rating scales.
In Proceedings of the AnnualMeeting of the Association for Computational Linguis-tics, pages 124?131.Bo Pang, Lillian Lee, and Shivakumar Vaithyanathan.2002.
Thumbs up?
: sentiment classification using ma-chine learning techniques.
In Proceedings of the Con-ference on Empirical Methods in Natural LanguageProcessing, pages 79?86.158Guang Qiu, Bing Liu, Jiajun Bu, and Chun Chen.
2009.Expanding Domain Sentiment Lexicon through Dou-ble Propagation.
In International Joint Conference onArtificial Intelligence, pages 1199?1204.Lizhen Qu, Georgiana Ifrim, and Gerhard Weikum.
2010.The bag-of-opinions method for review rating predic-tion from sparse text patterns.
In Proceedings of theInternational Conference on Computational Linguis-tics, pages 913?921.Carl Edward Rasmussen.
2004.
Gaussian processes inmachine learning.
Springer.Maite Taboada, Julian Brooke, Milan Tofiloski, Kim-berly D. Voll, and Manfred Stede.
2011.
Lexicon-based methods for sentiment analysis.
ComputationalLinguistics, 37(2):267?307.Oscar Ta?ckstro?m and Ryan T. McDonald.
2011a.
Dis-covering Fine-Grained Sentiment with Latent VariableStructured Prediction Models.
In Proceedings of theEuropean Conference on Information Retrieval, pages368?374.Oscar Ta?ckstro?m and Ryan T. McDonald.
2011b.
Semi-supervised latent variable models for sentence-levelsentiment analysis.
In Proceedings of the Annual Meet-ing of the Association for Computational Linguistics,pages 569?574.Theresa Wilson, Janyce Wiebe, and Paul Hoffmann.
2005.Recognizing contextual polarity in phrase-level senti-ment analysis.
In Proceedings of the Human LanguageTechnology Conference and the Conference on Empir-ical Methods in Natural Language Processing, pages347?354.Xiaojin Zhu and Zoubin Ghahramani.
2002.
Learningfrom labeled and unlabeled data with label propagation.Technical report.Xiaojin Zhu, Zoubin Ghahramani, and John Lafferty.2003.
Semi-supervised learning using Gaussian fieldsand harmonic functions.
In Proceedings of the Inter-national Conference on Machine Learning, pages 912?919.Li Zhuang, Feng Jing, and Xiaoyan Zhu.
2006.
Moviereview mining and summarization.
In Proceedings ofthe ACM international conference on Information andknowledge management, pages 43?50.159
