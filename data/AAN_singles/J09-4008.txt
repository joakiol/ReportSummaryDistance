An Investigation into the Validity of SomeMetrics for Automatically Evaluating NaturalLanguage Generation SystemsEhud Reiter?University of AberdeenAnja Belz?
?University of BrightonThere is growing interest in using automatically computed corpus-based evaluation metrics toevaluate Natural Language Generation (NLG) systems, because these are often considerablycheaper than the human-based evaluations which have traditionally been used in NLG.
Wereview previous work on NLG evaluation and on validation of automatic metrics in NLP, andthen present the results of two studies of how well some metrics which are popular in otherareas of NLP (notably BLEU and ROUGE) correlate with human judgments in the domain ofcomputer-generated weather forecasts.
Our results suggest that, at least in this domain, metricsmay provide a useful measure of language quality, although the evidence for this is not as strongas we would ideally like to see; however, they do not provide a useful measure of content quality.We also discuss a number of caveats which must be kept in mind when interpreting this andother validation studies.1.
IntroductionEvaluation is becoming an increasingly important topic in Natural Language Genera-tion (NLG), as in other fields of computational linguistics.
Many NLG researchers areimpressed by the BLEU evaluation metric (Papineni et al 2002) in Machine Translation(MT), which has allowed MT researchers to quickly and cheaply evaluate the impactof new ideas, algorithms, and data sets.
BLEU and related metrics work by comparingthe output of an MT system to a set of reference translations (human translations of thesource text), and in principle this kind of evaluation could be done with NLG systems aswell.
As in other areas of NLP, the advantages of automatic corpus-based evaluation arethat it is potentially much cheaper and quicker than human-based evaluation, and thatit is repeatable.
Indeed, NLG researchers have used BLEU in their evaluations for sometime (Langkilde 2002; Habash 2004).The use of such automatic evaluation metrics is, however, only sensible if they areknown to be correlated with the results of reliable human-based evaluations.
Althougha number of previous studies have analyzed correlations between human judgments?
Department of Computing Science, University of Aberdeen, UK.
E-mail: e.reiter@abdn.ac.uk.??
Natural Language Technology Group, University of Brighton, UK.
E-mail: A.S.Belz@brighton.ac.uk.Submission received: 23 March 2007; revised submission received: 6 October 2008; accepted for publication:29 December 2008.?
2009 Association for Computational LinguisticsComputational Linguistics Volume 35, Number 4and automatic evaluation metrics in machine translation and document summarization(Doddington 2002; Papineni et al 2002; Lin and Hovy 2003), much less is known abouthow well automatic metrics correlate with human judgments in NLG.
In this article wepresent two empirical studies of how well BLEU and various other corpus-based metricsagree with human judgments, when evaluating the outputs of several NLG systemsthat generate texts which describe changes in the wind (for weather forecasts).
We alsodiscuss several caveats that need to be kept in mind when interpreting our study andperhaps other validation studies of automatic metrics as well.2.
Background: Evaluation in NLG and Related FieldsAs Hirschman (1998), Mellish and Dale (1998), and others have pointed out, evaluationscan be used for many purposes, and different evaluations are often needed for differentstakeholders.
For example, the BabyTalk project at Aberdeen (Portet et al 2009), whichis attempting to create a set of NLG systems which can generate textual summaries ofclinical data about babies in a neonatal intensive care unit (NICU), is a collaborationbetween medical researchers, psychologists, computer scientists, and a commercialsoftware house.
Each of these groups has its own evaluation agenda: The medical researchers want to know if BabyTalk is medically effective.
Toevaluate this, they ideally would like to do a study similar to Cunninghamet al?s (1998) evaluation of the effectiveness of a visualization system in anintensive care unit; that is, deploy BabyTalk in a hospital, use it for half ofthe children in a ward, and determine if there is any difference in outcome(e.g., mortality) between the children in the BabyTalk group and thecontrol group. The psychologists want to understand the effectiveness of textualpresentation of information for decision support.
To evaluate this, theywould like to do a study similar to Law et al (2005); that is, show medicalsubjects textual summaries (as well as standard graphical visualizations asa control) in a controlled ?off-ward?
context, ask them to make a treatmentdecision, and compare this decision against a gold standard. The computer scientists want to know if BabyTalk is effective (under eitherof these measures).
They also would like to conduct evaluationsthroughout the project, so as to assess whether their development effortsare making the system better or worse; the stakeholders, in contrast,would be satisfied with a single evaluation at the end of the project. The software house would like to know if BabyTalk would be commerciallyprofitable.
This partially depends on medical effectiveness (see previouspoint), which determines the demand for the system.
But it also dependson how expensive it is to develop and support BabyTalk; from thisperspective the company is especially interested in evaluations of the costof adapting/porting BabyTalk to different hospitals in the NICU domainin the short term, and to different medical domains in the longer term.
SeeHarris (2008) for a commercial perspective on medical NLG systems.All of these stakeholders are interested in evaluations which assess the quality and effec-tiveness of generated texts; such evaluations are the focus of our article.
The software530Reiter and Belz Validity of Some Metrics for NLG Evaluationhouse and the computer scientists are also interested in engineering-cost evaluations;although this is a very important topic, we will not discuss it here: a separate articlewould be needed to do justice to this topic.2.1 Evaluation in NLGThe quality of texts generated by NLG systems has been evaluated in many differentways in the past, most of which can be classified as evaluations based on task perfor-mance, human judgments and ratings, or comparison to corpus texts using automaticmetrics.2.1.1 Task-Based Evaluation.
Task-based evaluations involve directly measuring the im-pact of generated texts on end users; these are extrinsic evaluations (Spa?rck Jones andGalliers 1995), and typically involve techniques from psychology or from an applicationdomain such as medicine.
One of the first task-based evaluations of an NLG system wasdone by Young (1999), who generated instructional texts using four different algorithms,asked subjects to carry out the instructions, and then measured how many mistakesthey made.
Although task performance is the most common measure used in task-basedevaluations in NLG, other measures can also be used.
For example, Carenini and Moore(2006) evaluated the impact of persuasive texts (in a house-selling context) by seeinghow users ranked houses in a hot list; and Di Eugenio, Glass, and Trolio (2002) evaluatedthe impact of adding an NLG component to an intelligent tutoring system by measuringlearning gain.We have been involved in a number of task-based evaluations of NLG systems andcomponents.
STOP (Reiter, Robertson, and Osman 2003), which generates personalizedsmoking-cessation letters, was evaluated on the basis of medical effectiveness; we senta group of 2,000 smokers either STOP-generated letters or one of two kinds of controlletters, and measured how many smokers in each group managed to quit smoking.BT45 (Portet et al 2009) (which is one of the BabyTalk systems) was evaluated for itsdecision-support effectiveness, using the ?psychologist?
methodology described earlier(van der Meulen et al 2009).
SKILLSUM (Williams and Reiter 2008), which generatesfeedback reports from literacy assessments, was evaluated on the basis of educationaleffectiveness; we gave 200 assessment takers either SKILLSUM texts or control texts, andmeasured whether they increased the accuracy of self-assessments of their literacy skills.We also evaluated several referring-expression generation algorithms by conductingexperiments in which participants were presented with generated referring expressionsand asked to identify the target referent (Belz and Gatt 2007; Gatt, Belz, and Kow2008, 2009); these were carried out in conjunction with shared-task events organizedunder the Generation Challenges initiative (Generation Challenges is further discussedin Section 2.1.4).Task-based evaluations have traditionally been regarded as the most meaningfulkind of evaluation in NLG, especially in contexts where the evaluation needs to convincepeople in other communities (such as psychologists and doctors).
However, they canbe expensive and time-consuming.
The STOP evaluation cost UK?75,000, and required20 months to design, carry out, and analyze; the SKILLSUM and BT45 evaluations(which are perhaps more typical) cost about UK?20,000 over six months.
The referring-expression identification experiments were cheaper (less than UK?1,000 each, not count-ing data and system creation), because they involved smaller numbers of subjects, and531Computational Linguistics Volume 35, Number 4evaluated system components in laboratory-based settings, rather than by means ofsystems deployed in the real world.In addition to monetary and time costs, all of these evaluations also depended ongoodwill from participants, in most cases busy domain experts who used their ownstanding in their community to arrange access to subjects and otherwise facilitate theevaluation.
Such goodwill in itself is a scarce resource which must be used with care.2.1.2 Evaluations Based on Human Ratings and Judgments.
Another way of evaluating anNLG system is to ask human subjects to rate generated texts on an n-point rating scale;this is an intrinsic form of evaluation (Spa?rck Jones and Galliers 1995).
This methodol-ogy was first used in NLG by Lester and Porter (1997), who asked eight domain expertsto each rate 15 texts on a number of different dimensions: overall quality and coherence,content, organization, writing style, and correctness.
Some of the texts were human-written and some were computer-generated, but the judges did not know the origin ofspecific texts they read.
Many more such evaluations have been performed since, oftenwith fewer dimensions.
For example, Binsted, Pain, and Ritchie (1997) evaluated a joke-generation system by asking children to rate the funniness of texts on a 5-point scale;and Walker, Rambow, and Rogati (2002) evaluated the SPOT sentence-planning systemby asking human subjects to rate the overall quality of generated texts on a 5-pointscale.
A variation of this technique is to show subjects different versions of a text, andask them which one they prefer.
For example, the SUMTIME weather-forecast generatorwas evaluated by showing subjects both human corpus texts and computer-generatedtexts, and asking which they preferred (Reiter et al 2005).Evaluations based on human ratings and judgments are currently probably the mostpopular way of evaluating NLG systems, perhaps in part because such evaluations tendto be significantly quicker and cheaper to carry out than task-based evaluations, anddo not require as much support from domain experts.
For example, the previouslymentioned evaluation of SUMTIME was carried out in two months without any externalresearch grant funding.In addition to resource issues, another reason why some researchers prefer evalua-tions based on human ratings over task-based evaluations is that task-based evaluationsneed to focus on a very specific task, and performance on this task may not correlatewith performance on other tasks.
For example, as mentioned previously, the medicalresearchers in BabyTalk would like to conduct a medical effectiveness evaluation, whichinvolves operationally deploying systems in a real ward and measuring impact onpatient outcome.
However, for ethical reasons such an experiment cannot be carried outuntil we have good evidence that the BabyTalk systems are effective, which is not yet thecase.
We carried out an off-ward task-based evaluation of BT45 using the ?psychologist?methodology (van der Meulen et al 2009), and we would like to think that the resultsof this evaluation would correlate with the results of a medical effectiveness evaluation.However, we do not have any empirical evidence that this is the case, and certainlythere are major differences between the off-ward and on-ward contexts (for example,doctors in the off-ward experiment could not visually observe the babies, which is avery important information source when doctors are actually caring for a baby in ahospital ward).
From this perspective, an argument can be made that asking doctorsto explicitly rate the medical usefulness of the texts might tell us as much about theirgenuine medical effectiveness as our off-ward task-based evaluation.Last but not least, it is not always possible to conduct meaningful task-based evalua-tions of some NLG systems.
For example, it is unclear how to evaluate the overall quality532Reiter and Belz Validity of Some Metrics for NLG Evaluationof jokes produced by a humor generation system other than by asking for human ratings(Binsted, Pain, and Ritchie 1997), although one can perform a task-based evaluationof the educational impact of humor generation software (Black et al 2007), or (morespeculatively) perhaps evaluate the psychological impact of a joke by monitoring facialexpressions and laughter (which is a non-task-based extrinsic evaluation).Little is known about how well human ratings of texts produced by NLG systemscorrelate with task-effectiveness measures.
Law et al (2005), who worked in the samedomain (NICU) as BabyTalk, conducted an off-ward decision-support evaluation whichcompared human-written text summaries and graphical visualizations of clinical data.They found that subjects preferred the visualizations, but were more likely to makecorrect decisions from the text summaries.
It is unclear whether this is because subjectshad inappropriate preferences, or because there was a big difference between genuinemedical effectiveness and off-ward decision-support effectiveness (as mentioned ear-lier).
The only studies we are aware of which examined how well human judgmentspredict task-effectiveness of computer-generated texts occurred in the recent GenerationChallenges evaluations of referring expression generation, which measured the corre-lations between human assessments of language quality and adequacy of content withtask-performance measures (referent identification time and accuracy) (Gatt, Belz, andKow 2009).
The results revealed a strong and highly significant correlation betweenhuman judgments of content adequacy and identification accuracy; there was alsoa significant inverse correlation between human judgments of language quality andidentification speed (i.e., those systems that tended to be judged more fluent by thehuman assessors also tended to have shorter identification times).2.1.3 Evaluations Based on Automatic Metrics which Compare Computer-Generated Textsto Human-Authored Corpus Texts.
In recent years there has been growing interest inevaluating NLG texts by comparing them to a corpus of human-written reference texts,using automatic metrics such as string-edit distance, tree similarity, or BLEU (Papineniet al 2002); this is another type of intrinsic evaluation.
Such evaluations have beenused by Bangalore, Rambow, and Whittaker (2000) and Marciniak and Strube (2004),for example.
Langkilde (2002) evaluated an NLG system by parsing texts from a corpus,feeding the parser output to her NLG system, and then comparing the generatedtexts to the original corpus texts.
Similar ?corpus regeneration?
evaluations havesince been used by a number of other researchers (Callaway 2003; Zhong and Stent2005; Cahill and van Genabith 2006).
Corpus-based evaluation has been especiallypopular in the evaluation of surface realizers.
This may be because the most importantattribute of many realizers is grammatical coverage and robust handling of special andunusual cases, and corpus-based techniques are well suited to evaluating this.
Also,the range of acceptable outputs can be smaller in realizer evaluations because content,microplanning, and (in some cases) lexical choices do not vary; this means there is lessconcern about reference texts not adequately covering the solution space.Automatic corpus-based evaluations are appealing in NLG, as in other areas of NLP,because they are relatively cheap and quick to do if a corpus is available, do not requiresupport from domain experts, and are repeatable.
However, their use in NLG is contro-versial, at least when evaluating systems as a whole instead of just surface realizers,because many people are concerned that the results of such evaluations may not bemeaningful.
For example Reiter and Sripada (2002) point out that corpus texts are oftennot of high enough quality to form good reference texts; and Scott and Moore (2007)express concern that metrics will not be able to evaluate many important linguisticproperties such as information structure.533Computational Linguistics Volume 35, Number 4A more general concern is that automatic metrics based on comparison to referencetexts measure how well a text matches what writers do, whereas most human evalua-tions (task or judgment-based) measure the impact of a text on readers.
Because writersdo not always produce optimal texts from a reader?s perspective (Oberlander 1998;Reiter et al 2005), a metric which is a good evaluator of how likely it is that a text hasbeen written by a human writer is not necessarily a good predictor of how effective anduseful the text is from the perspective of a human reader.
Of course automatic metricsdo not need to be writer-based.
Indeed, some reader-based automatic metrics, such asthe Flesch score (Flesch 1949) (based on average sentence and word length), are widelyused as practical tools to help writers, but such metrics have not been widely used toevaluate NLP systems.An important practical consideration is that corpus-based evaluations require acorpus of human-written reference texts; BLEU-like metrics in fact work best when thereference text corpus contains several reference texts for the same input, written bydifferent authors.
If reference texts have to be created specifically for an evaluation,this can be an expensive endeavor.
In the BabyTalk domain, for example, it can takean experienced clinician several hours to write a corpus text from the raw data; hencecreating a corpus of 100 reference texts in this domain could require 2?3 months effortby a clinician (as they do not create such reports in the course of their normal work).Getting this much time from an expert doctor or nurse would be difficult unless a verystrong case could be made for the utility of the evaluation.2.1.4 Other Validation Studies.
In recent years some validation studies which examine cor-relations between automatic metrics and human evaluations in NLG have been carriedout.
The first such study we are aware of is Bangalore, Rambow, and Whittaker (2000),who looked at string-edit and tree-edit metrics (this work predates BLEU and ROUGE)using a small number of manually simulated system ?outputs.?
Probably the mostsimilar study to our work is that by Stent, Marge, and Singhai (2005), who examined thecorrelation between human judgments and several automatic metrics when evaluatingcomputer-generated paraphrases; this is further discussed in Section 3.3.3.Very recently some validation studies have been done in the context of the Gen-eration Challenges initiative for shared tasks in NLG, by evaluating systems enteredin the shared task using automatic metrics, human ratings, and task-based evaluation,and analyzing correlations between these.
For example Belz and Gatt (2008) analyzedcorrelations between several automatic evaluation metrics and task performance in areferring-expression generation task; they found that there was no significant correla-tion between any of the automatic metrics they looked at (which included specializedmetrics for the reference task as well as BLEU and ROUGE) and their task-based measuresof effectiveness, such as how long it took human subjects to identify objects from areferring expression, and how many mistakes the subjects made.
However the differentautomatic metrics they examined did tend to correlate with each other, as did the differ-ent measures of task performance.
In general shared tasks offer a promising context forvalidation studies, and we hope that future Generation Challenges events will continueto provide data on how well automatic metrics correlate with human-based evaluations.2.2 Insights from Evaluations in Other Areas of NLPOf course, evaluation and experimentation are crucial to all fields of NLP; here we lookat insights from two other NLP subfields which need to evaluate the quality of texts:machine translation and document summarization.534Reiter and Belz Validity of Some Metrics for NLG Evaluation2.2.1 Evaluation in Machine Translation.
There is a rich literature in MT evaluation, includ-ing a number of specialist workshops on this topic; as in NLG, there is also considerableinterest in using shared-task events to provide data about how well different evaluationtechniques correlate with each other (Callison-Burch et al 2008).
From an NLG perspec-tive, the most surprising aspect of current MT evaluation is the dominance of BLEU andother automatic corpus-based metrics (Callison-Burch, Osborne, and Koehn 2006).
BLEUwas first proposed as a supplement (the U in BLEU stands for ?understudy?)
for humanevaluation (Papineni et al 2002), but it is now routinely used as the main technique forevaluating research contributions.
It is accepted and indeed the norm for an article onMT in Computational Linguistics to report evaluations that are solely based on automaticcorpus-based metrics; this is not the case in NLG, where human evaluations are expectedat least in high-prestige venues.We are not aware of any studies in MT that have tried to correlate BLEU-like met-rics with the results of task-effectiveness studies.
Although a number of studies haveanalyzed the correlation between BLEU-type metrics and human judgments, most ofthese have used human judgments from NIST MT evaluations.
Human judgments inmost of these evaluations were solicited from monolingual subjects who were asked tocompare the output of MT systems to a single reference translation, without any context;also in many of these studies the subjects were asked to assess individual sentences oreven phrases, not complete texts (Doddington 2002).
As Coughlin (2003) and othershave pointed out, it is not clear that human judgments solicited in this way wouldmatch the judgments of bilingual subjects who were shown complete source and MTtexts, and asked to evaluate the quality of the translation in a specific real-world context.Papineni et al (2002) in fact found that BLEU scores were more highly correlated withhuman judgments from monolingual subjects than human judgments from bilingualsubjects.In any case, regardless of the effectiveness of BLEU as an MT evaluation metric,another issue is whether an MT evaluation technique can in general be expected towork as an NLG evaluation technique.
There are some obvious differences between MTsystems and NLG systems; for example: Content determination: NLG systems need to decide on what informationshould be communicated in a text, as well as how this information islinguistically expressed; MT systems generally do not have to performcontent determination. Linguistic variety: Many NLG systems produce text that is fairly simplefrom a linguistic perspective (partially because many NLG users prefersuch texts); MT systems, in contrast, usually need to produce linguisticallycomplex texts. Genre/domain: Most applied NLG systems (with some exceptions) tryto generate high-quality texts in a limited domain and genre such asmarine weather forecasts; MT systems, in contrast, typically generatelower-quality texts in a broad text category such as newspaper articles.These differences presumably need to be considered when deciding whether it makessense to use an MT evaluation technique in NLG.
For example, there is no reason toexpect MT evaluation techniques to be useful for evaluating NLG content determination,since MT systems do not perform this task.
Also, MT evaluation techniques which535Computational Linguistics Volume 35, Number 4work well when evaluating less-than-human-quality texts from an MT system maynot necessarily work well when evaluating human-quality texts produced by an NLGsystem.2.2.2 Evaluation in Document Summarization.
Another branch of NLP which requires theevaluation of textual documents is document summarization.
From an evaluation per-spective, an important difference between MT and summarization is that summarizationevaluations have placed much more emphasis on content determination.
Perhaps inpart because of this, the summarization community places more emphasis on humanevaluations.
Although there are automatic corpus-based metrics for summarizationsuch as ROUGE (Lin and Hovy 2003), they do not seem to dominate summarizationevaluation in the same way that BLEU-type metrics dominate MT evaluation.The main summarization evaluation technique in the NIST TAC 2008 summa-rization track is the pyramid technique (Nenkova and Passonneau 2004), which is astructured human-based evaluation, based on asking human judges to identify ?sum-marization content units?
(SCU) in model and system-generated summaries, and mea-suring how many SCUs from the model summaries occur in the system summary.
Thisis an interesting technique for evaluating content, and might be worth investigating forevaluating content determination in NLG systems.In terms of validation, a number of studies have claimed that ROUGE correlates withhuman ratings, for example Lin and Hovy (2003) and Dang (2006).
Dorr et al (2005)checked if ROUGE scores correlated with task effectiveness; they did not find a strongcorrelation.2.3 SummaryIn summary, evaluation of NLG texts in the past has primarily been done using humansubjects, either by measuring the impact of texts on task performance, or by askingsubjects to rate texts.
However, a growing number of NLG researchers are using auto-matic metrics to evaluate their systems, perhaps inspired by the popularity of automaticmetrics in other areas of NLP which involve evaluating output texts, most notably ma-chine translation and document summarization.
This use of metrics assumes that theycorrelate with human-based evaluations.
A number of studies in machine translationand document summarization have shown that some automatic metrics correlate withhuman ratings; however we are not aware of any studies in these areas which haveshown any metric to strongly correlate with task performance.
Fewer validation studieshave been carried out in NLG, although this is beginning to change as researchers placemore importance on such studies.3.
Our ExperimentsGiven the growing interest in using automatic evaluation metrics such as BLEU inNLG, we decided to carry out some experiments to determine how well such metricspredicted the results of human judgments.
As in other such studies, we did this by eval-uating a number of systems with the same input/output functionality, using differentevaluation techniques, and then analyzing the correlation between the techniques.One potential weakness of our experiments was that we did not look at correlationswith task-effectiveness evaluations.
This was because we did not have the resources(money and domain-expert goodwill) to conduct a task-based evaluation.
This issue isfurther discussed in Section 4.2.536Reiter and Belz Validity of Some Metrics for NLG Evaluation3.1 Domain and SystemsOur work was done in the domain of computer-generated weather forecasts.
This isone of the most popular applications of NLG (Goldberg, Driedger, and Kittredge 1994;Coch 1998; Reiter et al 2005), and several NLG weather-forecast systems have beenfielded and used.
Weather forecast generation is probably the closest that NLG comes toa ?standard?
application domain, and hence seems a good choice for validation studiesfrom this perspective.On the other hand, though, one could also argue that weather-forecast generatorsare atypical in that the language they generate tends to be very simple, even by thestandards of NLG systems: very limited syntax (which differs from conventional Eng-lish), very small vocabulary, no real text structure above the sentence level, and so on.Hence it is not clear to what degree results obtained in this domain will generalize toother domains with less simple language and content (such as BabyTalk); this is furtherdiscussed in Section 4.1.From a practical perspective, the great advantage of the weather forecast domainwas that we had access to a number of systems, built using different NLG technologies,with the same input/output functionality; at the time (2006) there was no other domainwhere this was the case.
This situation is changing, because of the emergence of shared-task events in NLG such as Generation Challenges (see Section 2.1.4).3.1.1 SUMTIME Systems and SUMTIME-METEO Corpus.
In particular, we based our experi-ments on the SUMTIME system (Sripada et al 2004; Reiter et al 2005) and its associatedSUMTIME-METEO corpus (Sripada et al 2003), which were developed at Aberdeen.SUMTIME generates textual weather forecasts from numerical forecast data for off-shore oil rigs.
It has two modules: a content-determination module that determinesthe content of the weather forecast by analyzing the numerical data using linear seg-mentation and other data analysis techniques; and a microplanning and realizationmodule which generates texts based on this content by choosing appropriate words,deciding on aggregation, enforcing the sublanguage grammar, and so forth.
SUMTIMEgenerates very high-quality texts; in some cases forecast users believe SUMTIME textsare better than human-written texts (Reiter et al 2005; see also Table 4 of this paper).The SUMTIME system has been used operationally to produce draft weather forecasts;these are post-edited by meteorologists before they are released to end users (Sripadaet al 2004).SUMTIME is a knowledge-based NLG system.
Although its design was informedby corpus analysis (Reiter, Sripada, and Robertson 2003), the system is composed ofmanually authored rules and code.The SUMTIME project also created a corpus and data set, called SUMTIME-METEO(Sripada et al 2003).
This consists of a corpus of 1,045 weather forecasts written byprofessional forecasters, and the numerical predictions of wind, temperature, and soforth, that forecasters examined when they wrote the forecasts.
For wind descriptionsonly, the corpus also contains simple content representations containing informationabout wind speed and direction, time of day, and position in forecast (we call these?content tuples?).
The content tuples were created by parsing the corpus texts andextracting the relevant information (Reiter and Sripada 2003), and are similar to therepresentations produced by the SUMTIME content-determination system.
Figures 1, 2,and 3 show an extract from a numerical data file, an extract from the correspondinghuman-written forecast, and the content tuples derived from the human text.537Computational Linguistics Volume 35, Number 4wind avg wind max (gust)day/hour direction speed wind speed05/06 SSW 18 2205/09 S 16 2005/12 S 14 1705/15 S 14 1705/18 SSE 12 1505/21 SSE 10 1206/00 VAR 6 7Figure 1Extract from meteorological data file for 05-10-2000 (morning forecast).FORECAST 06-24 GMT, THURSDAY, 05-Oct 2000WIND(KTS) CONFIDENCE: HIGH10M: SSW 16-20 GRADUALLY BACKING SSE THEN FALLINGVARIABLE 04-08 BY LATE EVENING...Figure 2Extract from corpus (human) forecast for 05-10-2000 (morning forecast).index wind min wind max wind timedirection speed speed1 SSW 16 20 06002 SSE - - -3 VAR 04 08 2400Figure 3Content tuples extracted from the forecast in Figure 2.In addition to the main SUMTIME system, two other generation methods weredeveloped in the SUMTIME project and used in the experiments described here.SUMTIME-Hybrid uses the SUMTIME microplanner/realizer to generate text from thecorpus-derived content tuples (Figure 3).
In other words, it combines human content-determination with SUMTIME microplanning and realization.
The other method is analgorithm which is based on a spreadsheet and flowchart which one of the forecast-ers gave to the SUMTIME team at the beginning of the project (Reiter, Sripada, andRobertson 2003, page 499); a simplified version of this algorithm is presented in Figure 4.We did not implement this algorithm as software, but we manually executed it for anumber of forecasts.
We refer to it below as the Template system since the linguisticpart of the flowchart was based on template-filling.3.1.2 pCRU Generators for the SUMTIME-METEO Domain.
Independently of the SUMTIMEProject, we created a range of statistical generators for the SUMTIME-METEO domainusing pCRU generation (probabilistic context-free representational underspecification)(Belz 2008).
These took content tuples as input (as in Figure 3), not meteorological datafiles (as in Figure 1); in other words, they did not perform content determination.pCRU is a probabilistic language generation framework that was developed with theaim of providing the formal underpinnings for creating narrow-domain, applied NLGsystems that are driven by comprehensive probabilistic models of the entire generation538Reiter and Belz Validity of Some Metrics for NLG EvaluationStart text with direction, speed (5 kt range around actual value) from the first entryin the data fileFor each subsequent data file entryIf direction has changed 45 degrees or more since last mentioned direction, ordirection has changed at all, and speed is greater than 15 kts, thenadd the following phrase to the text?
veering if direction change is clockwise, backing otherwise?
new direction?
new speed (5 kt range around value) if speed has changed by 5 kts or more?
time phrase (from fixed table which maps numeric time to a phrase)Else if speed has changed by 5 kts or more since last mentioned speed, thenadd the following phrase to the text?
becoming?
new speed (5 kt range around actual value)?
time phrase (from fixed table which maps numeric time to a phrase)end ifend forFigure 4Template algorithm (simplified by removing special cases).space.
NLG systems are modeled as sets of generation rules that apply transformationsto representations.
The basic idea in pCRU is that as long as the generation rules are all ofthe form relation(arg1, ...argn) ?
relation1(arg1, ...argp) ... relationm(arg1, ...argq), m ?
1,n, p, q ?
0,then the set of all generation rules can be seen as defining a context-free language anda single probabilistic model can be estimated from raw or annotated text to guide thegeneration processes.pCRU uses straightforward context-free technology in combination with underspec-ification techniques, to encode a base generator as a set of expansion rules G. ThepCRU decision-maker is created by estimating a probability distribution over the basegenerator, as follows:1.
Convert corpus into multi-treebank: Determine for each sentence all(left-most) derivation trees licensed by the base generator?s rules, usingmaximal partial derivations if there is no complete derivation tree;annotate the (sub)strings in the sentence with the derivation trees,resulting in a set of generation trees for the sentence.2.
Train base generator: Obtain frequency counts for each individualgeneration rule from the multi-treebank, adding 1/n to the count for everyrule, where n is the number of alternative derivation trees; convert countsinto probability distributions over alternative rules, using add-1smoothing and standard maximum likelihood estimation.The resulting probability distribution is used in one of the following three ways tocontrol generation.1.
Viterbi generation: Do a Viterbi search of the generation forest for a giveninput, which maximizes the joint likelihood of all decisions taken in thegeneration process.539Computational Linguistics Volume 35, Number 42.
Greedy generation: Make the single most likely decision at each choice point(rule expansion) in a generation process.3.
Greedy roulette-wheel generation: Base decisions on a non-uniform randomdistribution proportional to the likelihoods of alternatives.We also implemented two baseline pCRU systems, both of which ignore pCRU probabili-ties: the randommode, which randomly selects generation rules; and the n-grammode,which generates the set of alternatives and selects the most likely one according to ann-gram language model (Langkilde and Knight 1998).Combining the pCRU, SUMTIME, and Template systems gave us a set of systemswhich had the same target functionality, but attempted to achieve it using quite differentNLG techniques and technologies.
Tables 1 and 2 show examples of texts produced byhumans and our systems.
The human texts include reference texts for automatic metrics(see Section 3.3) as well as the corpus texts.Note that we could not use other marine weather-forecast generators in our exper-iments, such as FOG (Goldberg, Driedger, and Kittredge 1994), because they use differ-ent inputs (that is, different numerical weather prediction models), and they produceoutputs targeted at different audiences (e.g., FOG forecasts are intended for mariners ingeneral, whereas SUMTIME forecasts are targeted at the offshore oil industry).3.2 Human EvaluationsWe conducted two experiments where we asked human subjects to rate texts producedby our different marine weather-forecast generators.
The main difference was that thefirst experiment focused on evaluating linguistic quality, and only looked at texts withthe same information content.
The second experiment also evaluated content quality,and used texts that varied in content as well as in linguistic expression.
We also changedthe experimental design in the second experiment, based on our experiences in the firstexperiment.3.2.1 First Human Evaluation.
In Experiment 1 (the main results of which we reportedpreviously in Belz and Reiter [2006]), we focused on the content-to-realization mapping,so we restricted ourselves to systems which generated texts from content tuples (Fig-ure 3) (SUMTIME-Hybrid and the pCRU systems).
We also included the correspondingtexts from the SUMTIME-METEO corpus.We used a randomly selected subset of 21 forecast dates from the SUMTIME-METEOcorpus.
We restricted ourselves to morning forecasts (half the corpus), as these arebased on a single data file (evening forecasts are based on two data files), and to thefirst wind description in a forecast, as subsequent wind descriptions have the addedconstraint of being consistent in form and content with earlier wind descriptions.
Foreach of these dates, we obtained seven texts: the corpus text, the texts produced by thepreviously mentioned systems, and one of the reference texts used by the automaticmetrics (Section 3.3.1).1 This gave us a total of 147 texts.1 We wanted to obtain ratings for reference texts to check that these were regarded as reasonable by thehuman subjects.
This was indeed the case, but we do not report the ratings of the reference texts here,because we do not have permission to do so.
Also we cannot use human ratings of reference texts in thecorrelation studies reported in Section 3.3, because automatic metrics cannot be used to evaluate theirown reference texts.540Reiter and Belz Validity of Some Metrics for NLG EvaluationTable 1Texts produced for 5 Oct 2000, from content tuples in Figure 3.Human texts:Corpus SSW 16-20 GRADUALLY BACKING SSE THEN FALLING VARIABLE 4-8 BYLATE EVENINGReference 1 SSW?LY 16-20 GRADUALLY BACKING SSE?LY THEN DECREASING VARIABLE4-8 BY LATE EVENINGReference 2 SSW 16-20 GRADUALLY BACKING SSE BY 1800 THEN FALLING VARIABLE4-8 BY LATE EVENINGReference 3 SSW 16-20 GRADUALLY BACKING SSE THEN FALLING VARIABLE 04-08BY LATE EVENINGSystem-generated texts:ST-Hybrid SSW 16-20 GRADUALLY BACKING SSE THEN BECOMING VARIABLE 10 ORLESS BY MIDNIGHTpCRU-greedy SSW 16-20 BACKING SSE FOR A TIME THEN FALLING VARIABLE 4-8 BYLATE EVENINGpCRU-roulette SSW 16-20 GRADUALLY BACKING SSE AND VARIABLE 4-8pCRU-2gram SSW 16-20 BACKING SSE VARIABLE 4-8 LATERpCRU-random SSW 16-20 AT FIRST FROM MIDDAY BECOMING SSE DURING THE AFTERNOONTHEN VARIABLE 4-8Table 2Texts produced for 6 Oct 2000, directly from numerical wind data.Human texts:Corpus N-NW 12-18 BACKING / EASING SSW LESS THAN 08 BY END OFPERIODReference 1 N-NW 12-17 DECREASING 10 OR LESS BY LATE AFTERNOON AND BACKING SWLATERReference 2 NW-NNW 12-15 KNOTS DECREASING AND BACKING TO BE WSW 05-08 BYEVENING AND VARIABLE LESS THAN 05 KNOTS BY MIDNIGHTReference 3 W-NNW 12-16 BACKING AND EASING THROUGH LATE AFTERNOON / EARLYEVENING SW-SSW LESS THAN 10System-generated texts:SUMTIME NNW 12-17 EASING 10 OR LESS BY MID AFTERNOON THEN BACKING WSW BYLATE AFTERNOON AND SSW BY MIDNIGHTTemplate NNW 13-18 BECOMING 08-13 IN THE MID-AFTERNOON BACKING WSW IN THEEARLY EVENING BECOMING 03-08 DURING THE NIGHT BACKING SSW 00-05AROUND MIDNIGHTFor our human evaluators, we recruited nine people with experience reading fore-casts for offshore oil rigs (?experts?).
Note that these were experienced forecast readers,not forecast writers.
We also recruited 21 people with no experience in reading forecastsfor offshore oil rigs (?non-experts?).
The reason for including non-experts was that wewanted to see if ratings by non-experts were similar to ratings by experts (as non-expertsare often much easier to recruit for experiments).
None of the subjects had a backgroundin NLP, and all were native speakers of English.Subjects were shown forecast texts from all the content-to-text generators, and fromthe corpus, and asked to give each text a single score on a scale of 0 to 5, which wasexplained as reflecting readability, clarity, and general appropriateness.
Experts (only) werealso shown the numerical weather data that the forecast text was based on.
Subjectswere not shown reference texts, as is often done in MT evaluations (Section 2.2.1).
The541Computational Linguistics Volume 35, Number 4experiment was done over the World Wide Web, at a time and place convenient to thesubjects.All subjects were shown two practice examples at the beginning of the test whichwere not included in the analysis.
Expert subjects were then shown one randomlyselected text for 18 of the dates.
The non-experts were shown 21 forecast texts, in aRepeated Latin Squares experimental design, where each of the 147 texts was rated bythree subjects.The average scores assigned by experts and non-experts are shown in Table 3.
Therewas good correlation between experts and non-experts: Pearson?s r = 0.874 (p = 0.011,one-tailed).
Experts and non-experts also agreed about relative rankings, except thatexperts rank pCRU-greedy second and the corpus texts third, whereas the non-expertshave these the other way around.To determine if any of the differences were statistically significant, we used SPSS?sGeneral Linear Model (GLM), with rating as the dependent variable, and generator,subject, and forecast date as independent variables; we used a post hoc Tukey HSD test toidentify significant differences between individual systems.
This analysis is essentiallyequivalent to normalizing (adjusting) the ratings to remove differences due to subjects(some people give lower ratings than others) and forecast dates (some meteorologicaldata sets are harder to describe than others), and then performing a one-way ANOVA onthe normalized scores using generator as the independent variable.The GLM analysis showed a very significant effect of generator on ratings, p <0.001 (two-tailed).
Table 3 shows the homogeneous subsets identified by the TukeyHSD post-hoc test.
These correspond to the following pairwise results.
For the experts,SUMTIME-Hybrid was significantly better than pCRU-random and pCRU-2gram, andpCRU-greedy was better than pCRU-random.
For the non-experts, all systems werebetter than pCRU-random, and SUMTIME-Hybrid was also better than pCRU-2gram.The SPSS GLM analysis also showed that scores were significantly affected by sub-ject, for both experts and non-experts (p < 0.001); in other words, different individualsrated texts differently.
Non-expert scores were also significantly influenced by forecastTable 3Experiment 1: Mean human ratings (single criterion of output quality), and homogeneoussubsets from Tukey HSD analysis.Experts Non-ExpertsMean Subsets Mean SubsetsSUMTIME-Hybrid 3.82 A SUMTIME-Hybrid 3.90 ApCRU-greedy 3.59 A B SUMTIME-Corpus 3.62 A BSUMTIME-Corpus 3.22 A B C pCRU-greedy 3.51 A BpCRU-roulette 3.11 A B C pCRU-roulette 3.49 A BpCRU-2gram 2.68 B C pCRU-2gram 3.29 BpCRU-random 2.43 C pCRU-random 2.51 CThe subsets show which differences are statistically significant.
More specifically, systems whichare in the same subset do not have statistically significant differences in their mean ratings; systemswhich are not in the same subset do have statistically significant differences in their mean ratings.For example, for both experts and non-experts, pCRU-greedy is not significantly different fromSUMTIME-Hybrid (since both are in subset A), and pCRU-greedy is not significantly different frompCRU-2gram (since both are in subset B).
However, SUMTIME-Hybrid is significantly differentfrom pCRU-2gram, since no subset contains both of these.542Reiter and Belz Validity of Some Metrics for NLG Evaluationdate (p < 0.001); in other words, some forecast data sets were harder to describe thanothers.The fact that subject and forecast date influence human ratings suggests that a LatinSquare experimental design should be used.
In a Latin Square design, every subjectrates the same number of texts from each generator, and every generator is rated anequal number of times on each forecast date; this reduces the impact of idiosyncraticdifferences between individual subjects (and forecast dates).
For example, in Exper-iment 1, expert subject TR gave higher ratings than average (mean of 4.17, againstan overall mean for expert subjects of 3.06), as did non-expert subject MP (mean of4.57, again an overall mean for non-experts of 3.34).
In the non-Latin-Square designused for expert subjects, TR rated 4 texts from pCRU-greedy, but only one text fromSUMTIME-Hybrid; this may have inflated pCRU-greedy?s mean score (Table 3) relativeto SUMTIME-Hybrid?s.
In the Latin Square design used for non-experts, in contrast, allthe subjects, including MP, rated the same number of texts (three) from each generator;hence MP?s generosity presumably benefited all generators equally, and did not inflatethe score of any one generator compared to the other generators.3.2.2 Second Human Evaluation.
Our first experiment focused on linguistic expression,but of course content determination is very important in NLG, so we decided to runanother experiment which also included texts generated from meteorological data, bysystems which performed content determination (that is, SUMTIME and Template).
Inthis experiment we showed subjects the raw forecast data and asked them for separateratings on ?clarity and readability?
(which was intended to elicit an assessment oflinguistic quality) and ?accuracy and appropriateness?
(which was intended to elicitan assessment of content quality).
For brevity, we refer to these scores as Clarity andAccuracy, respectively, herein.We used 14 new randomly selected forecast dates, and 14 new expert subjects (wedid not ask non-experts to rate these texts, because we were not confident that theycould assess the accuracy and appropriateness of texts).
The subjects were asked to rateseven types of texts: corpus texts, SUMTIME texts, Template texts, and texts producedby the Experiment 1 systems (except that we dropped pCRU-2gram); we did not in thisexperiment ask subjects to rate reference texts.
We would have liked to recruit morethan 14 subjects, but this proved difficult (see also Section 4.2); however, 14 subjects isan improvement over the 9 expert subjects used in Experiment 1 from the perspectiveof limiting the impact of individual differences between subjects.We also made a number of changes to our experimental design, based on issuesidentified in the first experiment with expert subjects.
The most important ones werethat we used a Latin Square design (with two subjects rating each system/date combi-nation), we asked for ratings on a seven-point scale instead of a six-point one (so thescale had a middle position which subjects could select), we explicitly gave instructionsas to what the ratings meant (to reduce variation due to differing interpretations of thescale), and we carried out a non-parametric as well as parametric statistical analysis.
Ascreenshot from the experiment is shown in Figure 5.There was a significant (p < 0.001) correlation between the accuracy and clarityscores that subjects gave to texts (Pearson r = 0.58), when computed on the 196 indi-vidual ratings made by subjects (when correlation is computed on the mean values, sig-nificance cannot be shown, because there are far fewer data points: Pearson?s r = 0.572,p = 0.09).
It is not clear whether this is because subjects did not properly distinguishaccuracy from clarity, or because generators that generated high-accuracy texts (such asSUMTIME) also generated high-clarity texts.543Computational Linguistics Volume 35, Number 4Figure 5Screenshot from Experiment 2.The averaged results of the human evaluations in Experiment 2 are shown in Table 4.Because the Experiment 1 texts were communicating the same content, and only dif-fered in linguistic expression, it seems likely that Experiment 2?s clarity scores shouldcorrelate with Experiment 1?s scores.
This is indeed the case: The correlation betweenaverage scores for the five systems that were included in both experiments is high withPearson?s r = 0.9 (p < 0.05).Table 4Experiment 2: Clarity and Accuracy average scores (expert subjects); homogeneous subsets fromTukey HSD analysis.Clarity AccuracyMean Subsets Mean SubsetsSUMTIME 5.04 A SUMTIME 5.07 ApCRU-greedy 4.79 A B Template 4.46 A BSUMTIME-Hybrid 4.61 A B pCRU-greedy 4.32 A BpCRU-roulette 4.54 A B SUMTIME-Corpus 4.11 BSUMTIME-Corpus 4.50 A B SUMTIME-Hybrid 3.96 BTemplate 4.04 B C pCRU-roulette 3.89 BpCRU-random 3.64 C pCRU-random 3.79 B544Reiter and Belz Validity of Some Metrics for NLG EvaluationThe SPSS GLM found a very significant effect of generator on both accuracy andclarity scores (p < 0.001).
Table 4 shows the homogeneous subsets identified by theTukey HSD post hoc test.
The corresponding pairwise results are as follows.
For theclarity scores, SUMTIME is significantly better than Template and pCRU-random; andall systems except Template are better than pCRU-random.
For the accuracy scores,SUMTIME is significantly better than all systems except pCRU-greedy and Template.
TheGLM analysis also showed that subject and forecast date (as well as generator) had asignificant impact on accuracy and clarity ratings (p < 0.002).This statistical analysis assumes that it is appropriate to use ANOVA-like tests toanalyze quality ratings.
Although this is common practice in many NLP papers, in-cluding most previous validation studies of automatic metrics which we are awareof, a good argument can be made that quality ratings should be analyzed using non-parametric tests.
This is because ratings are ordinal, so it is not clear that it makes senseto compute their mean, which is what the ANOVA and GLM tests do.
Hence we alsocarried out a non-parametric analysis to identify significant differences in the humanratings.
More specifically, we used the Wilcoxon Signed-Rank test, with a Bonferronimultiple-hypothesis correction, to identify pairs of systems that had significantly differ-ent ratings.
When comparing two systems, the Wilcoxon test requires each rating of thefirst system to be paired with a related rating of the second system.
Because we had tworatings from every subject for each system, we paired the lowest rating that a subjectgave to a text produced by the first system with the lowest rating that that subject gaveto a text produced by the second system; we similarly paired the highest ratings givenby each subject to the two systems.For example, subject AG evaluated two SUMTIME texts and gave them clarityratings of 5 and 6; he also evaluated two Template texts, and gave them clarity ratingsof 4 and 6.
In our non-parametric analysis, we paired the lowest rating given by AGto a SUMTIME text (that is, 5) with the lowest rating given by AG to a Template text(that is, 4); we also paired the highest rating given by AG to a SUMTIME text (that is,6) with the highest rating given by AG to a Template text (that is, 6).
This pairing waspossible because we used a Latin Square design in this experiment, which meant thatevery subject rated the same number of texts (two) from each generator.This procedure identified the same four significant differences in Accuracy as inTable 4?namely, SUMTIME is significantly better than all systems except pCRU-greedyand Template.
However, it identified only three significant differences in Clarity?namely, SUMTIME is significantly better than Template and pCRU-random, and pCRU-greedy is better than pCRU-random (this is a subset of the significant differences inClarity shown in Table 4).3.3 Correlation between Automatic Metrics and Human JudgmentsIn line with standard practice in validating metrics in MT, our main tool in analyzing theability of automatically calculated metrics to predict human judgments is calculatingPearson correlation coefficients between sets of scores produced by metrics and thehuman ratings from Section 3.2.
We computed correlations between metric scores forthe different systems and the mean human ratings for these systems; for example, wecomputed the correlation between the BLEU score for SUMTIME and the average ratinggiven by the human subjects to SUMTIME texts.
We did not compute correlations onindividual texts; for example, we did not try to correlate the BLEU score for the specificSUMTIME text shown in Table 2 against the human ratings of this specific text.
This545Computational Linguistics Volume 35, Number 4is because metrics such as BLEU and ROUGE are not intended to be meaningful forindividual sentences.Note that because correlations are being computed on a small set of numbers (sevenat most), fairly high correlation coefficients are needed to achieve significance.
In partbecause of this, we were less conservative in our statistical significance calculationsthan in the experiments reported in Section 3.2.
In particular, we computed statisticalsignificance of correlations using one-tailed (instead of two-tailed) tests, and we did notapply a Bonferroni multiple-hypothesis correction.
None of the correlations presentedsubsequently would be significant if Bonferroni-adjusted two-tailed p-values wereused; indeed we could only realistically expect to get significant correlations under thismeasure if we looked at more systems and/or fewer metrics (this is further discussedin Section 4.3).3.3.1 Metrics and Reference Texts Used.
We tested five automatic corpus-based metrics:two variants of the BLEU metric used in machine translation (Papineni et al 2002); twovariants of the ROUGE metric used in document summarization (Lin and Hovy 2003);and a simple sting-edit distance metric (as a baseline).BLEU is a precision metric that assesses the quality of a generated text in terms ofthe proportion of its word n-grams that it shares with reference texts.
BLEU scores rangefrom 0 to 1, where 1 is the highest which can only be achieved by a generated text if all itssubstrings can be found in one of the reference texts (hence a reference text will alwaysscore 1).
BLEU should be calculated on a large test set with multiple reference texts.
Weused BLEU-42 (that is, BLEU calculated using n-grams of size up to n = 4) because thisversion of BLEU is the main metric used in recent NIST Machine Translation evaluations(and indeed seems to have become a standard in the MT community).
We also used theNIST3MT evaluation score (Doddington 2002); this is an adaptation of BLEU which givesmore weight to less frequent n-grams which are assumed to be more informative.There are several different ROUGE metrics.
The simplest is ROUGE-N, which com-putes the highest proportion in any reference text of n-grams of length N that arematched by the generated text.
A procedure is applied that averages the score acrossleave-one-out subsets of the set of reference texts.
ROUGE-N is an almost straightforwardn-gram recall metric between two texts, and has several counter-intuitive properties,including that even a text composed entirely of sentences from reference texts cannotscore 1 (unless there is only one reference text).
ROUGE-SUN looks at ?skip bigrams?that occur in the generated text and reference texts; a skip bigram is two words whichare not necessarily adjacent, but may be separated by up to N intermediate words.
Weused ROUGE-2 and ROUGE-SU44 because these are the main automatic metrics used inrecent NIST Document Understanding Conferences (DUC).We also included string-edit distance as a very simple automatic metric, which canbe considered a sort of baseline.
String-edit distance (SE) was computed with substitu-tion at cost 2, and deletion and insertion at cost 1, and normalized to range 0 to 1 (perfectmatch).
When multiple reference texts are used, the SE score for a generated text is theaverage of its scores against the reference texts; the SE score for a set of generated textsis the average of scores for the individual texts.2 Calculated by ftp://jaguar.ncsl.nist.gov/mt/resources/mteval-v11b.pl.3 Calculated by http://cio.nist.gov/esd/emaildir/lists/mt list/bin00000.bin.4 ROUGE code obtained via http://www.isi.edu/~cyl/rouge/latest.html.546Reiter and Belz Validity of Some Metrics for NLG EvaluationTable 5Experiment 1: Metric scores against three reference texts (produced by rewriting corpus texts),for the set of 18 forecasts used in expert evaluation.System Exp Non NIST-5 BLEU-4 ROUGE-SU4 ROUGE-2 SEST-Hybrid 3.82 3.90 (1) 6.382 (3) 0.584 (4) 0.558 (4) 0.528 (4) 0.705 (5)pCRU-greedy 3.59 3.51 (3) 6.871 (2) 0.694 (2) 0.656 (2) 0.634 (2) 0.800 (2)ST-Corpus 3.22 3.62 (2) 8.705 (1) 0.951 (1) 0.839 (1) 0.815 (1) 0.917 (1)pCRU-roulette 3.11 3.49 (4) 6.206 (4) 0.563 (5) 0.554 (5) 0.51 (5) 0.735 (4)pCRU-2gram 2.68 3.29 (5) 5.925 (5) 0.598 (3) 0.586 (3) 0.556 (3) 0.783 (3)pCRU-random 2.43 2.51 (6) 4.608 (6) 0.355 (6) 0.462 (6) 0.419 (6) 0.649 (6)For convenience, expert (Exp) and non-expert (Non) scores are also shown.All of these automatic metrics require reference texts.
For Experiment 1, whichfocused on content-to-text, we asked three meteorologists5 (who had not contributedto the original SUMTIME-METEO corpus) to rewrite the corpus texts for the 21 datesused in Experiment 1 (each meteorologist rewrote all 21 corpus texts), correcting andimproving them as they saw fit; examples are shown in Table 1.
In principle, it wouldhave been preferable to ask the forecasters to write texts based on content tuples (theactual input to the systems), but this is not a natural task for forecasters (they write textsfrom data, not from content tuples).
However, asking them to rewrite corpus texts meantthey were unable to change the content, and focused on lexical choice and syntacticstructure, as intended.For Experiment 2, which also looked at systems which performed content determi-nation, we asked the same three meteorologists to write reference texts based on the rawnumerical input data for the 14 dates used in Experiment 2 (each meteorologist wrote atext for all 14 dates); examples are shown in Table 2.
They were not shown the corpusforecasts corresponding to the data.3.3.2 Correlation with Results from Experiment 1.
Table 5 shows the average scores eachmetric assigned to each system when calculated on the texts used in the Experiment 1expert evaluations.6 The metrics all rank the corpus texts highest, the pCRU-greedy textssecond, and the pCRU-random texts lowest.
Their strong preference for the corpus textsis probably an artefact of the way the reference texts were produced.
The forecasterswere asked to rewrite the corpus texts, which resulted in considerable similarity be-tween the reference texts and the corpus texts.
In calculating correlation figures (shownin Table 6), we therefore produced two sets of figures, one for the NLG systems andthe corpus texts (I in the table) and one for just the NLG systems (II); set II should beregarded as a post hoc analysis.
For set I, none of the metrics significantly correlate5 When we first reported results for Experiment 1 in Belz and Reiter (2006), we only had reference textsfrom two meteorologists, but we have since obtained reference texts from a third meteorologist.
This iswhy the numbers in Tables 5 and 6 differ from the numbers given in Belz and Reiter (2006).6 The important information in Table 5 is the differences in the scores assigned by the same metric todifferent systems.
Differences in the scores assigned by different metrics to the same system are notmeaningful; they are just mathematical artefacts of the formulas used to calculate the metrics.
Forexample, the fact that BLEU-4 gives SUMTIME-Hybrid a higher score than pCRU-random is important: thisshows that BLEU-4 (correctly) predicts that human subjects prefer SUMTIME-Hybrid texts topCRU-random texts.
The fact that String-Edit (SE) gives a higher rating than BLEU-4 to SUMTIME-Hybridis not important, as it does not tell us anything about how well SE or BLEU-4 can predict differences inhuman ratings of texts.547Computational Linguistics Volume 35, Number 4Table 6Experiment 1: Correlation (Pearson?s r) between human scores and automatic metrics.Experts Non-exp.
NIST-5 BLEU-4 ROUGE-SU4 ROUGE-2 SEI.
Pearson?s r, all NLG systems and corpus textsExperts 1 0.874 0.534 0.461 0.362 0.379 0.260Non-exp.
0.874 1 0.685 0.639 0.518 0.527 0.483II.
Pearson?s r, just NLG systems (not corpus texts) (post hoc)Experts 1 0.884 0.836 0.700 0.611 0.616 0.339Non-exp.
0.884 1 0.886 0.797 0.654 0.647 0.505Significant correlations (1-tailed, no Bonferroni correction) are shown in bold.with human ratings.
For set II, NIST-5 has a significant correlation with both expert andnon-expert scores.3.3.3 Correlation with Results from Experiment 2.
Table 7 shows the average scores eachmetric assigned to each system for the texts used in Experiment 2: The rankings assignedby NIST-5 and BLEU-4 are identical, and SE largely agrees, with small differences in thetop rankings (where differences between scores are very small), whereas the ROUGEmetrics disagree with the other metrics to some degree.
Table 8 shows the correspondingcorrelation figures for all NLG systems and the corpus texts (I), all NLG systems (II),and texts that communicate the same content (III).
Set III consists of corpus texts andtexts produced by systems whose input is corpus-derived content tuples (pCRU andSUMTIME-Hybrid).
Note that the figures in sets I and II are similar; because referencetexts for Experiment 2 were produced from raw data, the automatic metrics are notbiased towards the corpus texts as they were in Experiment 1.The most striking result from the correlation figures is that not a single metric corre-lates significantly with human judgments of accuracy.
Clarity is significantly correlatedwith NIST-5 in sets I and III, and BLEU-4 and SE in set III.This analysis assumes that it is sensible to use mean values of the human ratings;however, as mentioned at the end of Section 3.2.2, a good argument can be made thatordinal ratings should not be averaged.
An alternative way of assessing the predictiveaccuracy of the metrics is to count how many of the significant differences identifiedby the non-parametric analysis in Section 3.2.2 were predicted by differences inmetric scores.
For example, the non-parametric analysis showed that human subjectsTable 7Experiment 2: Metric scores against three reference texts (written from raw data), for the set of14 forecasts.System Cla.
Acc.
NIST-5 BLEU-4 ROUGE-SU4 ROUGE-2 SESUMTIME 5.04 5.07 (1) 4.668 (5) 0.187 (5) 0.241 (5) 0.155 (6) 0.392 (5)pCRU-greedy 4.79 4.32 (3) 5.118 (2) 0.244 (2) 0.258 (3) 0.180 (3) 0.42 (2)ST-Hybrid 4.61 3.96 (5) 5.223 (1) 0.281 (1) 0.281 (1) 0.227 (1) 0.415 (3)pCRU-roulette 4.54 3.89 (6) 4.798 (4) 0.221 (4) 0.244 (4) 0.169 (4) 0.421 (1)ST-Corpus 4.50 4.11 (4) 4.969 (3) 0.227 (3) 0.281 (1) 0.21 (2) 0.415 (3)Template 4.04 4.46 (2) 3.299 (7) 0.106 (7) 0.168 (7) 0.089 (7) 0.333 (7)pCRU-random 3.64 3.79 (7) 4.011 (6) 0.162 (6) 0.238 (6) 0.156 (5) 0.379 (6)For convenience, human ratings also shown.548Reiter and Belz Validity of Some Metrics for NLG EvaluationTable 8Experiment 2: Correlation (Pearson?s r) between human score and metrics.Cla.
Acc.
NIST-5 BLEU-4 ROUGE-SU4 ROUGE-2 SEI.
Pearson?s r, all NLG systems and corpus textsClarity 1 0.572 0.701 0.577 0.431 0.401 0.571Accuracy 0.572 1 ?0.118 ?0.281 ?0.308 ?0.375 ?0.286II.
Pearson?s r, just NLG systems (not corpus texts)Clarity 1 0.583 0.711 0.578 0.455 0.417 0.578Accuracy 0.583 1 ?0.092 ?0.265 ?0.285 ?0.358 ?0.266III.
Pearson?s r, content-to-text NLG systems and corpus texts (same content)Clarity 1 0.741 0.959 0.858 0.550 0.549 0.969Accuracy 0.741 1 0.682 0.502 0.432 0.294 0.607Significant (1-tailed, no Bonferroni correction) correlations are shown in bold.considered SUMTIME to be significantly more accurate than pCRU-random.
If we lookat the metric scores shown in Table 7, we see that ROUGE-2 rated pCRU-random higherthan SUMTIME, and all other metrics rated SUMTIME higher than pCRU-random.Hence ROUGE-2 did not predict this significant difference in human Accuracy ratings(SUMTIME better than pCRU-random), but the other metrics did.Table 9 shows the predictive accuracy of the metrics (in this sense).
Overall thisagrees with the main finding of the parametric analysis (see Table 3), namely thatexisting metrics are better at predicting Clarity than Accuracy.It is interesting to compare our findings with Stent, Marge, and Singhai (2005), whoexamined the correlation between human judgments and several automatic metrics(including BLEU, NIST, and ROUGE) when evaluating computer-generated paraphrases.They in fact found more or less the opposite result, namely, that the metrics correlatedwith adequacy (similar to our Accuracy) but not fluency (similar to our Clarity).
Thismay partially be due to the fact that Stent, Marge, and Singhai used a single referencetext, which was the original input text to the paraphraser.
With such a reference text, wewonder if the metrics largely measured the amount of paraphrasing done; that is, textswith less paraphrasing (whose surface form was thus closer to the original texts) wererated higher by the metrics.
If so, it would not be surprising if the metrics correlated withaccuracy but not with fluency, because it is possible that subjects regarded sentenceswhose surface form was close to the original text as more accurate but not necessarilymore fluent.
In their discussion section, Stent, Marge, and Singhai acknowledged thatusing a single reference text is problematic, and recommended that multiple referenceTable 9Number of significant non-parametric differences predicted by each metric.Metric Clarity AccuracyNIST-5 3 1BLEU-4 3 1ROUGE-SU4 3 1ROUGE-2 2 0SE 3 1actual number ofsignificant diff 3 4549Computational Linguistics Volume 35, Number 4texts should be used if possible; however they also pointed out that this is not a panacea,since even 3?4 reference texts are unlikely to capture all of the acceptable variations ina text.3.3.4 Summary of Results.
In Experiment 1 all systems communicated the same content(they take the same content tuples as inputs), subjects were asked to give a single overallrating for texts, and reference texts were created by rewriting corpus texts.
Under theseconditions, NIST-5 scores are significantly correlated with expert scores (Table 6).In Experiment 2, systems communicated different contents, subjects were askedto give separate clarity and accuracy ratings for a system, and reference texts werecreated by writing new texts from numerical data.
Under these conditions, NIST-5 issignificantly correlated with human clarity judgments.
If only texts which communicatethe same content at the content tuple level are included in the analysis, then NIST-5 andSE are strongly correlated with clarity judgments (r > 0.95), and BLEU-4 also correlatessignificantly (Table 8).
However, no metric correlates significantly with human accuracyjudgments under any analysis.4.
Discussion: What Can We Conclude from Our ResultsThe most obvious interpretation of our results is that it is acceptable to use BLEU-likemetrics (with caution) to estimate the linguistic quality of generated texts, especiallywhen comparing texts which are communicating the same content; but current auto-matic metrics should not be used to evaluate the quality of the content of generatedtexts.
However, there are a number of caveats which must be considered which wediscuss subsequently.
These caveats may have relevance to other validation studies ofautomatic metrics in NLP.Determining the validity of ?cheap?
evaluation techniques which are intended toapproximate the genuine outcome measure is of course a problem that occurs in manyareas of science, and we believe it is useful to look at what other fields do in this regard.Hence we relate our discussion to validation requirements in clinical medicine for?surrogate measures?
(Greenhalgh 2006) (for example, using blood tests that measureHIV viral load to evaluate the effectiveness of AIDS treatments, instead of measuringactual mortality); and criterion validation requirements in psychology for psychometricand other tests (Kaplan and Saccuzzo 2001).One general lesson from psychology is that there can be a strong temptation touse evaluation techniques which are quick, cheap, and appear to be impartial, even ifthey are known to have very limited validity.
For example, psychometric tests whichclaim to predict academic success, such as the American SAT test, are very heavily usedby American universities when they make admissions decisions, despite the fact thatnumerous validation studies have shown that these tests are poor predictors of howwell a student does at university over the four-year span of a typical degree (althoughthey do have a limited correlation with academic performance in a student?s first yearat university).4.1 Generality across Domains, Genres, and SystemsOur experiments have been carried out in the specific domain and genre of marineweather forecasts for offshore oil rigs, and are based on a set of seven specific NLGsystems.
Will they apply to other domains, and indeed even to marine-weather-forecastgenerators built with different NLG technologies?
Of course similar concerns have been550Reiter and Belz Validity of Some Metrics for NLG Evaluationraised about automatic metrics in other areas of NLP.
For example, most validationstudies of automatic metrics in machine translation and document summarization havebeen done with newswire texts; it is not clear, however, that results obtained fromtranslated newswire texts also apply to translated scientific papers, for example.A similar point is made strongly in the medical and psychological literature: Vali-dation studies are performed in a particular context, and it is very risky to generalizethem to other contexts, without additional evidence that they are effective in these newcontexts.
For example, Kaplan and Saccuzzo (2001, chapters 11 and 19) discuss theWAIS intelligence test, the original version of which was developed solely using datafrom subjects of European descent.
Later research suggested it was not valid for othersubjects; for example a variant called WISC, which was used in some school systems todecide which children should go to special education classes, was shown in the 1970s tocorrelate with teacher assessments of children of European descent, but not with teacherassessments of children from other ethnic groups.
The test was subsequently revised toenhance its validity for children from diverse backgrounds.We do not know how generalizable our findings are to other NLG contexts.
Wewould have a better idea of generalizability if we performed similar experiments inother domains and genres, using systems built with a wide range of NLG technologies;but of course we cannot realistically expect to conduct enough experiments to examineall domains, genres, and technologies of interest.Ultimately, the key to generalizing experimental results is a good theoretical modelwhich is scientifically plausible and fits the experimental data.
Perhaps for this reason,surrogate measures used in medicine are expected to be biologically plausible predictorsof the actual outcome measures as well as empirically correlated with them (Greenhalgh2006, page 95).
The theoretical basis behind most current metrics used in NLG seems tobe an intuition that similarity in surface forms should correlate with similarity in taskeffectiveness.
It may be worth investigating whether psycholinguistic models of lan-guage comprehension (for example, Kintsch 1998) could provide a stronger theoreticalbasis for metric plausibility.For what it is worth, our intuition is that our findings will apply to other applicationdomains which involve generating texts which are short, linguistically simple, and notvery varied.
We would be extremely cautious about attempting to generalize our resultsto application domains which require texts which are longer, more complex, and morevaried, such as BabyTalk.4.2 Does Correlation with Human Judgments Mean Correlationwith Task-Effectiveness?As mentioned in Section 2.1.1, task-effectiveness evaluations are the most highly re-garded evaluations in NLG; ultimately what we usually want to know is how effectiveNLG texts are in achieving their communicative goal, not whether readers like themor not.
From this perspective, a major weakness in our study is that we correlatedautomatic ratings with human ratings, not task-effectiveness evaluations.Attempts to correlate automatic metrics with task-based evaluations have beenquite rare.
The only ones we are aware of in NLG took place in the Generation Chal-lenges events mentioned earlier; none of the automatic metrics used in these events hada significant correlation with task performance.
In the summarization community, Dorret al (2005) found very weak correlation between an automatic metric (ROUGE) andtask performance.
We are not aware of any studies in machine translation which haveanalyzed correlation between automatic metrics and task-performance.551Computational Linguistics Volume 35, Number 4This is a major concern (as noted by Belz 2009) because we also do not know howwell human ratings predict task-effectiveness; in other words, the fact that NIST scorespredict human clarity ratings of NLG texts does not guarantee that NIST scores willpredict task effectiveness, because we do not know that human clarity ratings correlatewith task effectiveness.
Looking again at medicine and psychology, validation studiesin these fields need to show correlation with the actual outcome variable or at least apreviously validated measure; in the words of Kaplan and Saccuzzo (2001, page 141),?a meaningless [test] which is well correlated with another meaningless [test] remainsmeaningless.
?A major reason why so few correlation studies have been done between automaticmetrics and task effectiveness is the significant amount of resources needed for suchstudies (and this is why we did not look at task-effectiveness in this study).
The problemis not just time and money, it is also that task-based evaluations require support fromdomain experts (as mentioned in Section 2.1.1), and such support can be difficult toget for validation studies.
To take a concrete example, a senior consultant at a hospitalmight be willing to encourage his medical colleagues to participate in the evaluationof a high-quality NLG system, for the purpose of determining whether this system wasa useful medical decision-support aid; but such a consultant might be less willing toencourage his colleagues to participate in the evaluation of several NLG systems ofmixed quality, for the purpose of determining whether human ratings correlated withautomatic metrics.Even obtaining subjects for ratings-based correlation studies can be difficult.
Forexample, when we ran a human judgment-based study to test the effectiveness ofSUMTIME texts (Reiter et al 2005), we managed to recruit 72 subjects in a few weeks; incontrast it took us several months to recruit the 23 expert subjects who participated inthe studies reported in this article.
Both experiments required similar time commitmentsfrom similar subjects.
However, subjects (and the domain experts who facilitated subjectrecruitment) were much more enthusiastic about testing the effectiveness of a systemwhich they might themselves use; they were less enthused about testing hypothesesabout correlations between NLG evaluation metrics.A related issue is how well human judgments of the clarity of texts correlate withhuman judgments of the overall quality of texts; this is important because our resultssuggest that current metrics are much better at predicting human clarity judgmentsthan human accuracy judgments.
Intuitively, it seems likely that readers place moreimportance on content than on linguistic expression.
In the SUMTIME domain, thisintuition is supported by the SUMTIME evaluation (Reiter et al 2005), in which forecastreaders were asked to compare two forecasts, and say which was easier to read, whichwas more accurate, and which was overall more appropriate.
In cases where subjectsrated one forecast as easier to read and another as more accurate, they said the ?moreaccurate?
forecast was overall more appropriate in 55% of cases, and the ?easier toread?
forecast was overall more appropriate in only 18% of cases (in 27% of the casesthey said neither of the forecasts was overall more appropriate than the other); this issignificant at p < 0.001.
Given this, it is a pity that the metrics we examined were somuch better at predicting clarity than they were at predicting accuracy.4.3 Statistical IssuesLike other scientific experiments, NLP evaluations are regarded as producing a signifi-cant result if they have a p-value (likelihood of incorrectly rejecting the null hypothesis)of 0.05 or less.
Of course, statistical significance can be calculated in many ways; for552Reiter and Belz Validity of Some Metrics for NLG Evaluationexample parametric or non-parametric tests can be applied, multiple-hypothesis (e.g.,Bonferroni) corrections may or may not be applied, one or two-tailed p-values can beused, post hoc findings may or not be presented, and so on.In medicine, recent work by Ioannidis (2005a, 2005b) and others (partially based onanalyses of whether experimental results are replicated in follow-up studies) has sug-gested that a very conservative statistical analysis should be used in medical research.Ionnadis concludes that a very high quality medical experiment with very conservativestatistical analysis has about an 85% chance of being replicated successfully; and thatthis chance quickly declines to noise levels once the design, execution, and/or statisticalanalysis of the experiment becomes less than ideal.One could argue that computational linguistics should insist on similarly strictstatistical analyses; in particular always use two-tailed p-values, always apply multiplehypothesis corrections, always discard post hoc findings (unless they are from testsspecifically designed for post hoc analysis, such as Tukey HSD), and always use non-parametric tests if there is any doubt about the appropriateness of parametric tests.As we mentioned in Section 3.3, none of the correlations we observed between auto-matic metrics and human judgments would be considered significant under such aconservative statistical analysis.
Indeed, in order to have a reasonable chance of seeinga statistically significant correlation under a conservative statistical analysis (to havesufficient power in a statistical sense), we would need to either look at more systems(since the value of Pearson?s r needed to achieve statistically significant correlationsdecreases as the number of points in the correlation increase), and/or look at fewermetrics (since the impact of multiple hypothesis corrections decreases when fewerhypotheses are being tested).The last point is particularly worth bearing in mind, because there is a strongtemptation in validation studies to include as many metrics as possible.
After all, oncethe human evaluations have been collected and the reference corpora have been created,we can compute correlations with other metrics (additional metrics such as METEOR(Banerjee and Lavie 2005), and variations of metrics we are already examining, such asBLEU-2 and BLEU-3 as well as BLEU-4) at the touch of a button.
But if we are applyingmultiple hypothesis corrections, then there is a major drawback to including a largenumber of metrics in the study, which is that this will make it more difficult to findstatistically significant correlations.However, perhaps it is wrong to use such strict statistics in computational lin-guistics, and indeed we are aware of many reports in computational linguistics whichpresent one-tailed p-values, do not apply multiple hypothesis corrections, presentpost hoc analyses as significant, and/or use parametric tests to analyse data which doesnot have the characteristics assumed by the parametric test.
In this respect the practice incomputational linguistics is perhaps closer to psychology, where (for example) multiplehypothesis corrections are less common than they are in medicine; indeed a textbook onstatistics for psychologists used at the University of Aberdeen does not even mentionthe topic.So should our results be considered statistically insignificant (using a very strictstatistical analysis) or statistically significant (using a less strict statistical analysis)?
Ourpersonal opinion is that the correlations we have observed are real, but it would beextremely useful to verify this by running larger experiments which showed significantresults under a stricter statistical analysis.
However, other readers may have differentopinions, and in this article we have tried to follow the advice of Greenhalgh (2006) bygiving enough information about our statistical analyses to enable readers to make theirown informed judgments as to how to interpret them.553Computational Linguistics Volume 35, Number 45.
Discussion: When Should Automatic Metrics be Used in Evaluating NLG?Our goal in this experiment was to shed light on when automatic metrics should beused in NLG.
Given all the previously mentioned caveats, we cannot of course drawfirm conclusions about this topic.
But we can make some suggestions.First of all, the automatic metrics we examined should not be used to predicthuman judgments of content quality; none of them had a significant correlation withhuman accuracy judgments, even when statistical significance is calculated in a less-than-conservative fashion.Second of all, even when evaluating linguistic quality, current automatic metricsshould be used with caution, as a supplement rather than a replacement for humanevaluation; similar comments have been made about the use of automatic metrics inMT (Papineni et al 2002; Callison-Burch, Osborne, and Koehn 2006).
Particular cautionshould be used when evaluating NLG systems which generate significantly longer andmore complex texts than the marine weather forecasts we examined here.
We are notaware of any validation studies on such texts, and there are important aspects of thelinguistic quality of longer and more complex texts (such as discourse coherence) whichare not measured by current metrics.Thirdly, automatic metrics are most appealing in contexts where a suitable corpusof reference texts already exists.
Creating good-quality reference texts is an expensiveendeavor, especially in domains (such as summaries of clinical data) where texts mustbe written by skilled domain experts.
Therefore we suspect that it may be difficult inmany cases to justify creating large corpora of reference texts solely for the purposes ofautomatic evaluation of NLG systems.5.1 Should Automatic Metrics be Used in Shared-Task Evaluations?In the wider NLP community, automatic metrics are especially popular in shared-taskevaluations.
This is partially because such metrics have a very low marginal cost com-pared to human evaluations.
Automatic metrics need reference texts, and obtaininggood reference texts can be costly; but once a collection of reference texts has beencreated, it can be used to evaluate any number of systems.
Also automatic metrics arevery easy to use once the software and reference corpus has been created; developers donot need to be trained in using BLEU and ROUGE.
In contrast human-based evaluationsgenerally need a certain number of subjects per system, so their cost goes up with thenumber of systems evaluated.
Also, expertise and/or training is needed to conductexperiments with people, which not all NLP researchers possess.
Finally, evaluation withmetrics is entirely reproducible.However, despite the cost-effectiveness and other appealing aspects of automaticmetrics in shared tasks, we do not believe that shared tasks in NLG should use auto-matic metrics as the sole evaluation criterion.
Until there is better evidence that au-tomatic metrics correlate with human evaluations, shared tasks in NLG should alsoinclude human evaluations, preferably task-effectiveness ones.
This strategy is beingfollowed in the Generation Challenges shared-task NLG events (Section 2.1.4).5.2 Should Automatic Metrics be Used in Diagnostic Evaluation?We have focused in this article on evaluations that measure the quality of generatedtexts, but many NLG developers are also interested in diagnostic evaluations whosepurpose is to identify problems in a system and suggest improvements.
From this554Reiter and Belz Validity of Some Metrics for NLG Evaluationperspective, an advantage of human evaluations is that human subjects can be askedto make free-text comments on the texts that they see, and these comments are oftenextremely useful from a diagnostic perspective.
On the other hand, an advantage ofautomatic metrics is that they allow developers to rapidly evaluate changes to systemsand algorithms; indeed, some machine translation researchers use automatic metrics toautomatically tune parameters without human intervention (Och 2003).
However, asOch points out, this is only sensible if automatic metrics are known to be very accuratepredictors of text quality.
Because our results suggest that current automatic metricsare not highly accurate predictors of the quality of texts produced by NLG systems, werecommend developers be cautious in using metrics for diagnostic evaluation, and donot use metrics for automatic parameter tuning.On the other hand, automatic metrics do have a potential advantage in small diag-nostic evaluations, which is that they are not influenced by the individual preferences ofa small number of human subjects.
There are large differences in how different humansubjects rate texts, as we pointed out at the end of Section 3.2.1.
Such differences arenot unusual: we have seen them in most human evaluations of NLG systems which wehave carried out.
These differences can be controlled for in a large experiment whichuses many subjects.
But if a diagnostic evaluation is conducted with a small number ofsubjects, who are chosen partially on the basis of being easy to recruit, there is a risk thatthe preferences expressed by these subjects will not be representative of users in general,and hence may mislead the developer as to how the system should be changed.6.
ConclusionsAutomatic evaluation metrics have many desirable properties, such as being fast,cheap, and repeatable, and they have had a significant impact in many areas ofNLP.
We have compared the scores produced by several popular metrics, includingBLEU and ROUGE, to human evaluations of NLG systems, in the domain of weatherforecasts.
Our results suggest that it may be appropriate to use existing automaticmetrics (with caution) to evaluate the linguistic quality of generated texts, especiallyif metric evaluations supplement (rather than replace) human evaluations; for examplemetric-based evaluations could be used to provide diagnostic feedback to developersin the period before a large human evaluation.
NIST-5 is perhaps the best metric to usefor this purpose (out of the ones we investigated).
However, existing metrics shouldnot be used to evaluate the content of texts.
Also it would be premature to use metricsto test hypotheses about the effectiveness of NLG systems; we need more experimentalvalidation data (including validation against task effectiveness measures) and ideally agood theoretical model as well.AcknowledgmentsAnja Belz?s work was in part supportedunder UK EPSRC grant GR/S24480/01.Many thanks to Yaji Sripada for his helpobtaining SUMTIME texts, and to RogerEvans, Chris Mellish, Kees van Deemter,and the anonymous reviewers for theirvery helpful comments.
Last but notleast, we would like to thank the3 meteorologists, 23 expert forecast readers(master mariners for the most part), and 21non-experts (family, friends, and colleagues)for their generous help in the evaluationexperiments.ReferencesBanerjee, Satanjeev and Alon Lavie.
2005.Meteor: An automatic metric for MTevaluation with improved correlation withhuman judgments.
In Proceedings of the ACLWorkshop on Intrinsic and Extrinsic EvaluationMeasures for MT and/or Summarization,pages 65?72, Ann Arbor, MI.555Computational Linguistics Volume 35, Number 4Bangalore, Srinivas, Owen Rambow, andSteve Whittaker.
2000.
Evaluation metricsfor generation.
In Proceedings of the 1stInternational Conference on Natural LanguageGeneration, pages 1?8, Mitzpe Ramon.Belz, Anja.
2008.
Automatic generationof weather forecast texts usingcomprehensive probabilisticgeneration-space models.
NaturalLanguage Engineering, 14:431?455.Belz, Anja.
2009.
That?s nice ... what do youdo with it?
Computational Linguistics,35:111?118.Belz, Anja and Albert Gatt.
2007.
Theattribute selection for GRE challenge:Overview and evaluation results.
InProceedings of the 2nd UCNLG Workshop:Language Generation and MachineTranslation (UCNLG+MT), pages 75?83,Copenhagen.Belz, Anja and Albert Gatt.
2008.
Intrinsic vs.extrinsic evaluation measures for referringexpression generation.
In Proceedings of the46th Annual Meeting of the Association forComputational Linguistics (ACL?08),pages 197?200, Columbus, OH.Belz, Anja and Ehud Reiter.
2006.
Comparingautomatic and human evaluation of NLGsystems.
In Proceedings of the EACL?06,pages 313?320, Trento.Binsted, Kim, Helen Pain, and GraemeRitchie.
1997.
Children?s evaluation ofcomputer-generated punning riddles.Pragmatics and Cognition, 5:309?358.Black, Rolf, Annalu Waller, Graeme Ritchie,Helen Pain, and Ruli Manurung.
2007.Evaluation of joke-creation software withchildren with complex communicationneeds.
Communication Matters, 21:23?28.Cahill, Aoife and Josef van Genabith.2006.
Robust PCFG-based generationusing automatically acquired LFGapproximations.
In Proceedings of ACL?06,pages 1033?1044, Sydney.Callaway, Charles.
2003.
Evaluating coveragefor large symbolic NLG grammars.
InProceedings of the 18th International JointConference on Artificial Intelligence (IJCAI2003), pages 811?817, Acapulco.Callison-Burch, Chris, Cameron Fordyce,Philipp Koehn, Christof Monz, andJosh Schroeder.
2008.
Furthermeta-evaluation of machine translation.In Proceedings of the Third Workshop onStatistical Machine Translation,pages 70?106, Columbus, OH.Callison-Burch, Chris, Miles Osborne, andPhilipp Koehn.
2006.
Re-evaluating therole of BLEU in machine translationresearch.
In Proceedings of EACL-2006,pages 249?256, Trento.Carenini, Giuseppe and Johanna D. Moore.2006.
Generating and evaluatingevaluative arguments.
ArtificialIntelligence, 170:925?952.Coch, Jose?.
1998.
Interactive generation andknowledge administration in MultiMeteo.In Proceedings of the Ninth InternationalWorkshop on Natural-Language Generation(INLG-1996), pages 300?303, Sussex.Coughlin, Deborah.
2003.
Correlatingautomated and human assessments ofmachine translation quality.
In Proceedingsof MT Summit IX, pages 63?70, NewOrleans, LA.Cunningham, Steven, Sarah Deere, AndrewSymon, Robert Elton, and Neil McIntosh.1998.
A randomized, controlled trial ofcomputerized physiologic trendmonitoring in an intensive care unit.Critical Care Medicine, 26:2053?2060.Dang, Hoa.
2006.
Duc 2005: Evaluation ofquestion-focused summarization systems.In Proceedings of the ACL-COLING 2006Workshop on Task-Focused Summarizationand Question Answering, pages 48?55,Sydney.Di Eugenio, Barbara, Michael Glass,and Michael Trolio.
2002.
The DIAGexperiments: Natural language generationfor tutoring systems.
In Proceedings of theSecond International Conference on NaturalLanguage Generation (INLG-2002),pages 120?127, Harriman, NY.Doddington, George.
2002.
Automaticevaluation of machine translation qualityusing n-gram co-occurrence statistics.In Proceedings of the Second InternationalConference on Human Language TechnologyResearch, pages 138?145, San Diego, CA.Dorr, Bonnie, Christof Monz, Stacy President,Richard Schwartz, and David Zajic.
2005.Methodology for extrinsic evaluation oftext summarization: Does ROUGEcorrelate?
Proceedings of the ACL Workshopon Intrinsic and Extrinsic EvaluationMeasures for Machine Translation and/orSummarization, pages 1?8, Ann Arbor, MI.Flesch, Rudolf.
1949.
The Art of ReadableWriting.
Harper Brothers, New York.Gatt, Albert, Anja Belz, and Eric Kow.
2008.The TUNA Challenge 2008: Overview andevaluation results.
In Proceedings of the 5thInternational Natural Language GenerationConference (INLG?08), pages 198?206, SaltFork, OH.Gatt, Albert, Anja Belz, and Eric Kow.2009.
The TUNA-REG Challenge556Reiter and Belz Validity of Some Metrics for NLG Evaluation2009: Overview and evaluation results.
InProceedings of the 12th European Workshop onNatural Language Generation (ENLG?09),pages 174?182, Athens.Goldberg, Eli, Norbert Driedger, and RichardKittredge.
1994.
Using natural-languageprocessing to produce weather forecasts.IEEE Expert, 9(2):45?53.Greenhalgh, Trisha.
2006.
How to Read aPaper: The Basics of Evidence Based Medicine.BMJ Books, Oxford, third edition.Habash, Nizar.
2004.
The use of astructural n-gram language model ingeneration-heavy hybrid machinetranslation.
In Proceedings of the 3rdInternational Conference on Natural LanguageGeneration (INLG ?04), volume 3123 ofLNAI, pages 61?69, Brockenhurst.Harris, Mary Dee.
2008.
Building alarge-scale commercial NLG systemfor am EMR.
In Proceedings of INLG-2008,pages 157?160, Salt Fork, OH.Hirschman, Lynette.
1998.
The evolution ofevaluation: Lessons from the messageunderstanding conferences.
ComputerSpeech and Language, 12:283?285.Ioannidis, John.
2005a.
Contradicted andinitially stronger effects in highly citedclinical research.
Journal of AmericanMedical Association, 294:218?228.Ioannidis, John.
2005b.
Why most publishedresearch findings are false.
PLoS Medicine,2, doi:10.1371/journal.pmed.0020124.Kaplan, Robert and Dennis Saccuzzo.2001.
Psychological Testing: Principles,Applications, and Issues (Fifth Edition).Wadsworth, London.Kintsch, Walter.
1998.
Comprehension.Cambridge University Press.Langkilde, Irene.
2002.
An empiricalverification of coverage and correctness fora general-purpose sentence generator.
InProceedings of the 2nd International NaturalLanguage Generation Conference (INLG ?02),pages 17?24, Harriman, NY.Langkilde, Irene and Kevin Knight.
1998.Generation that exploits corpus-basedstatistical knowledge.
In Proceedings ofCOLING-ACL 1998, pages 704?710,Montreal.Law, Anna, Yvonne Freer, Jim Hunter, RobertLogie, Neil McIntosh, and John Quinn.2005.
Generating textual summaries ofgraphical time series data to supportmedical decision making in the neonatalintensive care unit.
Journal of ClinicalMonitoring and Computing, 19:183?194.Lester, James and Bruce Porter.
1997.Developing and empirically evaluatingrobust explanation generators: TheKNIGHT experiments.
ComputationalLinguistics, 23(1):65?101.Lin, Chin-Ye and Eduard Hovy.
2003.Automatic evaluation of summariesusing n-gram co-occurrence statistics.In Proceedings of HLT-NAACL 2003,pages 71?78, Edmonton.Marciniak, Tomasz and Michael Strube.2004.
Classification-based generationusing TAG.
In Natural LanguageGeneration: Proceedings of INLG-2994.Springer, pages 100?109, Brockenhurst.Mellish, Chris and Robert Dale.
1998.Evaluation in the context of naturallanguage generation.
Computer Speechand Language, 12:349?373.Nenkova, Ani and Rebecca Passonneau.2004.
Evaluating content selection insummarization: The pyramid method.In Proceedings of NAACL-2004,pages 145?152, Boston, MA.Oberlander, Jon.
1998.
Do the right thing .
.
.but expect the unexpected.
ComputationalLinguistics, 24:501?507.Och, Franz Josef.
2003.
Minimum error ratetraining in statistical machine translation.In Proceedings of ACL-2003, pages 160?167,Sapporo.Papineni, Kishore, Salim Roukos, ToddWard, and Wei-Jing Zhu.
2002.
BLEU: Amethod for automatic evaluation ofmachine translation.
In Proceedings ofACL-2002, pages 311?318, Philadelphia, PA.Portet, Franc?ois, Ehud Reiter, Albert Gatt,Jim Hunter, Somayajulu Sripada, YvonneFreer, and Cindy Sykes.
2009.
Automaticgeneration of textual summaries fromneonatal intensive care data.
ArtificialIntelligence, 173:789?816.Reiter, Ehud, Roma Robertson, and LieslOsman.
2003.
Lessons from a failure:Generating tailored smoking cessationletters.
Artificial Intelligence, 144:41?58.Reiter, Ehud and Somayajulu Sripada.2002.
Should corpora texts be goldstandards for NLG?
In Proceedings of the2nd International Conference on NaturalLanguage Generation, pages 97?104,Harriman, NY.Reiter, Ehud and Somayajulu Sripada.
2003.Learning the meaning and usage of timephrases from a parallel text-data corpus.In Proceedings of the HLT-NAACL 2003Workshop on Learning Word Meaning fromNon-Linguistic Data, pages 78?85,Edmonton.Reiter, Ehud, Somayajulu Sripada, JimHunter, and Jin Yu.
2005.
Choosing words557Computational Linguistics Volume 35, Number 4in computer-generated weather forecasts.Artificial Intelligence, 167:137?169.Reiter, Ehud, Somayajulu Sripada, and RomaRobertson.
2003.
Acquiring correct knowledgefor natural language generation.
Journal ofArtificial Intelligence Research, 18:491?516.Scott, Donia and Johanna Moore.
2007.
AnNLG evaluation competition?
Eightreasons to be cautious.
In Proceedings of theWorkshop on Shared Tasks and ComparativeEvaluation in Natural Language Generation,pages 22?23, Arlington, VA.Spa?rck Jones, K. and J. R. Galliers.
1995.Evaluating Natural Language ProcessingSystems: An Analysis and Review.
SpringerVerlag, Berlin.Sripada, Somayajulu, Ehud Reiter, Ian Davy,and Kristian Nilssen.
2004.
Lessons fromdeploying NLG technology for marineweather forecast text generation.
InProceedings of PAIS-2004, pages 760?764,Valencia.Sripada, Somayajulu, Ehud Reiter, JimHunter, and Jin Yu.
2003.
Exploiting aparallel TEXT-DATA corpus.
In Proceedingsof Corpus Linguistics 2003, pages 734?743,Lancaster.Stent, Amanda, Matthew Marge, and MohitSinghai.
2005.
Evaluating evaluationmethods for generation in the presence ofvariation.
In Proceedings of CICLing 2005,pages 341?351, Mexico City.van der Meulen, Marian, Robert Logie,Yvonne Freer, Cindy Sykes, Neil McIntosh,and Jim Hunter.
2009.
When a graph ispoorer than 100 words: A comparisonof computerised natural languagegeneration, human generated descriptionsand graphical displays in neonatalintensive care.
Applied CognitivePsychology, doi:10.1002/acp.1545.Walker, Marilyn, Owen Rambow, andMonica Rogati.
2002.
Training a sentenceplanner for spoken dialogue usingboosting.
Computer Speech and Language,16:409?433.Williams, Sandra and Ehud Reiter.
2008.Generating basic skills reports forlow-skilled readers.
Natural LanguageEngineering, 14:495?535.Young, Michael.
1999.
Using Grice?s maximof quantity to select the content of plandescriptions.
Artificial Intelligence,115:215?256.Zhong, Huayan and Amanda Stent.
2005.Building surface realizers automaticallyfrom corpora.
In Proceedings of UCNLG?05,pages 49?54, Birmingham.558
