Proceedings of the ACL-HLT 2011 Student Session, pages 88?93,Portland, OR, USA 19-24 June 2011. c?2011 Association for Computational LinguisticsCombining Indicators of AllophonyLuc BorutaUniv.
Paris Diderot, Sorbonne Paris Cite?, ALPAGE, UMR-I 001 INRIA, F-75205, Paris, FranceLSCP, De?partement d?E?tudes Cognitives, E?cole Normale Supe?rieure, F-75005, Paris, Franceluc.boruta@inria.frAbstractAllophonic rules are responsible for the greatvariety in phoneme realizations.
Infants cannot reliably infer abstract word representa-tions without knowledge of their native allo-phonic grammar.
We explore the hypothe-sis that some properties of infants?
input, re-ferred to as indicators, are correlated with al-lophony.
First, we provide an extensive evalu-ation of individual indicators that rely on dis-tributional or lexical information.
Then, wepresent a first evaluation of the combination ofindicators of different types, considering bothlogical and numerical combinations schemes.Though distributional and lexical indicatorsare not redundant, straightforward combina-tions do not outperform individual indicators.1 IntroductionThough the phonemic inventory of a language is typ-ically small, phonetic and phonological processesyield manifold variants1 for each phoneme.
Wordstoo are affected by this variability, yielding differentrealizations for a given underlying form.
Allophonicrules relate phonemes to their variants, expressingthe contexts in which the latter occur.
We are in-terested in describing procedures by which infants,learning their native allophonic grammar, could re-duce the variation and recover words.
Combining in-sights from both computational and behavioral stud-ies, we endorse the hypothesis that infants are gooddistributional learners (Maye et al, 2002; Saffranet al, 1996) and that they may ?bootstrap?
into lan-guage tracking statistical regularities in the signal.1We use allophony as an umbrella term for the continuumranging from typical allophones to mere coarticulatory variants.We seek to identify which features of infants?
in-put are most reliable for learning allophonic rules.
Afew indicators, based on distributional (Peperkampet al, 2006) and lexical (Martin et al, submitted) in-formation, have been described and validated in sil-ico.2 Yet, other aspects have barely been addressed,e.g.
the question of whether or not these indicatorscapture different aspects of allophony and, if so,which combination scheme yields better results.We present an extensive evaluation of individualindicators and, based on theoretical and empiricaldesiderata, we outline a more comprehensive frame-work to model the acquisition of allophonic rules.2 Indicators of allophonyWe build upon Peperkamp et al?s framework: thetask is to induce a two-class classifier deciding, forevery possible pair of segments, whether or not theyrealize the same phoneme.
Discrimination relies onindicators, i.e.
linguistic properties which are corre-lated with allophony.
As a model of language acqui-sition, this classifier is induced without supervision.In line with previous studies, we assume that in-fants are able to segment the continuous stream ofacoustic input into a sequence of discrete segments,and that they quantize each of these segments intoone of a finite number of phonetic categories.
Quan-tization is a necessary assumption for the frameworkto apply.
However, the larger the set of phonetic cat-egories, the closer we get to recent ?single-stage?
ap-proaches (e.g.
work by Dillon et al, in preparation)where phonological categories are acquired directlyfrom raw infant-directed speech.2See also the work of Dautriche (2009) on acoustic indica-tors of allophony, albeit using adult-directed speech.882.1 Distributional indicatorsComplementary distribution is a ubiquitous criterionfor the discovery of phonemes.
If two segments oc-cur in mutually exclusive contexts, the two may berealizations of the same phoneme.Bearing in mind that the signal may be noisy,Peperkamp et al (2006) looked for segments innear-complementary distributions.
Using the sym-metrised Kullback?Leibler divergence (henceforthKL), they compared the probability distributions ofhow often the contexts of each segment occur.
In afollow-up study, Le Calvez (2007) compared KL toother indicators, namely the Jensen?Shannon diver-gence (JS) and the Bhattacharyya coefficient (BC).32.2 Lexical indicatorsAdjacent segments can condition the realization ofa word?s initial and final phonemes.
If two wordsonly differ by their initial or final segments, thesesegments may be realizations of the same phoneme.Instantiating the general concept of functional load(Hockett, 1955), lexical indicators gauge the degreeof contrast in the lexicon between two segments.Using the simplest expression of functional load,Martin et al (submitted) defined a Boolean-valuedindicator, FL, satisfied by a single pair of minimallydifferent words.
As a result, FL is sensitive to noise.We define a finer-grained variant, FL*, which talliesthe number of such pairs.
Moreover, as words getlonger, it becomes increasingly unlikely that suchword pairs occur by chance.
Thus, for any such pair,FL* is incremented by the length of those words.We also propose an information-theoretic lexi-cal indicator, HFL, based on Hockett?s definition offunctional load.
HFL accounts for the fraction ofinformation content, represented by the language?sword entropy, that is lost when the opposition be-tween two segments is neutralized.
The ?brokentypewriter?
function used for neutralization guaran-tees that values lie in [0, 1] (Coolen et al, 2005).3 Corpora and experimental setupIn the absence of phonetic transcriptions of infant-directed speech, and as the number of allophones in-3As for the actual computations, we use the same definitionsas Le Calvez (2007) except that, as BC increases when distribu-tions overlap and 0 ?
BC ?
1, we actually use 1?BC.fants must learn is unknown (if assessable at all), weuse Boruta et al?s (submitted) corpora.
They createda range of possible inputs, applying artificial allo-phonic grammars4 of different sizes (Boruta, 2011)to the now-standard CHILDES ?Brent/Ratner?
cor-pus of English (Brent and Cartwright, 1996).
Wequantify the amount of variation in a corpus by itsallophonic complexity, i.e.
the ratio of the number ofphones to the number of phonemes in the language.Lexical indicators require an ancillary procedureyielding a lexicon.
Martin et al approximated a lex-icon by a list of frequent n-grams.
Here, the lexiconis induced from the output of an explicit word seg-mentation model, viz.
Venkataraman?s incremental(2001) model, using the unsegmented phonetic cor-pora as the input.
Though, obviously, infants cannot access it, we use the lexicon derived from theCHILDES orthographic transcripts for reference.4 Indicators?
discriminant powerAs the aforementioned indicators have been evalu-ated using various languages, allophonic grammarsand measures, we present a unified evaluation, con-ducted using Sing et al?s (2005) ROCR package.4.1 EvaluationNon-Boolean indicators require a threshold at andabove which pairs are classified as allophonic.
Weevaluate indicators across all possible discriminationthresholds, reporting the area under the ROC curve(henceforth AUC).
Equivalent to Martin et al?s ?,values lie in [0, 1] and are equal to the probabilitythat a randomly drawn allophonic pair will scorehigher than a randomly drawn non-allophonic pair;.5 thus indicates random prediction.Moreover, we evaluate indicators?
misclassifica-tions at the discrimination threshold maximizingMatthews?
(1975) correlation coefficient: let ?, ?, ?and ?
be, respectively, the number of false positives,false negatives, true positives and true negatives,MCC = (?????)/?(?+?)(?+?)(?+?)(?+?
).Values of 1, 0 and ?1 indicate perfect, random andinverse prediction, respectively.
This coefficient ismore appropriate than the accuracy or the F-measure4Because all allophonic rules implemented in the corporaare of the type p ?
a / c, FL and FL* only look for wordsminimally differing by their last segments.89when, as here, the true classes have very differ-ent sizes.5 Using this optimal, MCC-maximizingthreshold, we report the maximal MCC and, as per-centages, the accuracy (Acc), the false positive rate(FPR) and the false negative rate (FNR).4.2 Results and discussionIndicators?
AUC corroborate previous results fordistributional indicators: they perform almost iden-tically and do not accommodate high allophoniccomplexities at which they perform below chance(Figure 1.a) because, as suggested by Martin etal., every segment has an extremely narrow distri-bution and complementary distribution is the rulerather than the exception.
By contrast, all threelexical indicators are much more robust even if, aspredicted, FL?s coarseness impedes its discriminantpower (Figure 1.b).6 The reason why FL* outper-forms HFL may be due to the very definition ofHFL?s broken typewriter function: as the segments,e.g.
{x, y}, are collapsed into a single symbol, theindicator captures not only minimal alternations likewx ?
wy, but also word pairs such as xy ?
yx.AUC curves suggest that, for each type, indi-cators converge at medium allophonic complexity.Thus, misclassification scores are reported in Table 1only at low (2 allophones/phoneme) and medium (9)complexities.
Previous observations are confirmedby MCC and accuracy values: though all indicatorsare positively correlated with the underlying allo-phonic relation, correlation is stronger for lexical in-dicators.
Surprisingly, zero FPR values are observedfor some lexical indicators, meaning that they makeno false alarms and, as a consequence, that all errorsare caused by missed allophonic pairs.5 Indicators?
redundancyNone of the indicators we benchmarked in the previ-ous section makes a perfect discrimination betweenallophonic and non-allophonic pairs of segments.5If p phonemes have on average a allophones, out of thepa(pa?1)/2 possible pairs, only pa(a?1)/2 are allophonic,and a dummy indicator that rejects all pairs achieves a constantaccuracy of 1?
(a?1)/(pa?1), which is greater than 98%for any of our corpora.
Besides, the computation of precision,recall and the F-measure do not take true negatives into account.6These indicators perform similarly using the orthographiclexicon: we only report AUC for FL* (referred to as oFL*), asit gives the upper bound on lexical indicators?
performance.0 5 10 15 20 250.40.50.60.70.80.9 a. Distributionalllll l l ll KLJSBC0 5 10 15 20 250.40.50.60.70.80.9 b. Lexicalllll llll FLFL*HFLoFL*Figure 1: Indicators?
AUC as a function of allophoniccomplexity.
The dashed line indicates random prediction.2 allophones/phoneme 9 allophones/phonemeMCC Acc FPR FNR MCC Acc FPR FNRKL .095 88.2 11.3 58.5 .017 90.7 07.8 88.8JS .097 86.4 13.1 53.7 .014 93.3 05.1 93.0BC .097 86.8 12.8 54.4 .016 89.9 08.6 88.1FL .048 37.3 63.2 13.6 .116 73.1 26.8 35.2FL* .564 99.3 00.0 67.3 .563 98.6 00.4 53.0HFL .301 99.1 00.0 87.8 .125 94.1 04.5 78.7Table 1: Indicators?
performance at low and mediumcomplexities, using the MCC-maximizing thresholds.Boldface indicates the best value.
Italics indicate accura-cies below that of a dummy indicator rejecting all pairs.Yet, if some segment pairs are misclassified by onebut not all (types of) indicators, a suitable combi-nation should outperform individual indicators.
Inother words, combining indicators may yield betterresults only if, individually, indicators capture dif-ferent subsets of the underlying allophonic relation.5.1 EvaluationTo get a straightforward estimation of redundancy,we compute the Jaccard index between each indica-tor?s set of misclassified pairs: let D and L be setscontaining, respectively, a distributional and a lexi-cal indicator?s errors, J(D,L) = |D ?
L|/|D ?
L|.Values lie in [0, 1] and the lower the index, the morepromising the combination.
To distinguish false pos-itives from false negatives, we compute two Jaccardindices for each possible combination.5.2 Results and discussionJaccard indices, reported in Table 2, emphasize thedistinction between false positives and false nega-tives.
False negatives have rather high indices: most90allophonic pairs that are not captured by distribu-tional indicators are not captured either by lexicalindicators, and vice versa.
By contrast, there is littleor no redundancy in false positives, even at mediumallophonic complexity: though random pairs can beincorrectly classified as allophonic, the error is un-likely to recur across all types of indicators.It is also worth noting that though JS performsslightly better than KL and BC, the exact nature ofthe distributional indicator seems to have little influ-ence on the performance of the combination.6 Combining indicatorsAs distributional and lexical indicators are not com-pletely redundant, combining them is a natural ex-tension.
However, not all conceivable combinationschemes are appropriate for our task.
We present ourchoices in terms of Marr?s (1982) levels of analysis.At the computational level, a combination schemecan be either disjunctive or conjunctive, i.e.
each in-dicator can be either sufficient or (only) necessary.Aforementioned indicators were designed as neces-sary but not sufficient correlates of phonemehood.For instance, while a phoneme?s allophones havecomplementary distributions, not all segments thathave complementary distributions are allophones ofa single phoneme.
Therefore, we favor a conjunctivescheme,7 even if this conflicts with abovementionedresults: most errors are due to missed allophonicpairs but a conjunctive scheme, where every indi-cator must be satisfied, is likely to increase misses.At the algorithmic level, a combination schemecan be either logical or numerical.
A logical schemeuses a logical connective to join indicators?
Booleandecisions, typically by conjunction according to ourprevious decision.
By contrast, a numerical schemetries to approximate interactions between indicators?values, merging them using any monotone increas-ing function; discrimination then relies on a singlethreshold.
In practical terms, we use multiplicationas a numerical counterpart of conjunction.6.1 EvaluationSetting aside the following minor adjustments, weuse the same protocol as for individual indicators.7This generalizes Martin et al?s attempt at combination:they used FL as a high-pass lexical filter prior to the use of KL.2 allo./phon.
9 allo./phon.FP FN FP FNKL FL .096 .071 .113 .359JS FL .113 .076 .071 .355BC FL .110 .075 .118 .358KL FL* .000 .595 .008 .520JS FL* .000 .548 .005 .525BC FL* .000 .556 .007 .517KL HFL .000 .667 .087 .788JS HFL .000 .612 .033 .781BC HFL .000 .620 .089 .787Table 2: Indicators?
redundancy at low and medium allo-phonic complexities, estimated by the Jaccard indices be-tween their false positives (FP) and false negatives (FN).Boldface indicates the best value.0 5 10 15 20 250.40.50.60.70.80.9llll l lll JS .
FLJS .
FL*JS .
HFLJS .
oFL*Figure 2: Indicators?
AUC as a function of allophoniccomplexity, for the multiplicative combination scheme.The dashed line indicates random prediction.Logical combinations require one discriminationthreshold per combined indicator.
As it facilitatescomparison with previous results, we report perfor-mance at the thresholds maximizing the MCC ofindividual indicators (rather than at the thresholdsmaximizing the combined MCC) .Numerical combinations are sensitive to differ-ences in indicators?
magnitudes.
Equal contributionof all indicators may or may not be a desirable prop-erty, but in the absence of a priori knowledge ofindicators?
relative weights, each indicator?s valueswere standardized so that they lie in [0, 1], shiftingthe minimum to zero and rescaling by the range.6.2 Results and discussionIt is worth noting that, while the performance ofcombined indicators is still good (Table 3), it isless satisfactory than that of the best individual in-dicators.
Moreover, even if misclassification scores91Logical combination: conjunction Numerical combination: multiplication2 allophones/phoneme 9 allophones/phoneme 2 allophones/phoneme 9 allophones/phonemeMCC Acc FPR FNR MCC Acc FPR FNR MCC Acc FPR FNR MCC Acc FPR FNRKL FL .104 92.9 06.5 67.3 .037 94.7 03.6 91.3 .104 92.9 06.5 67.3 .116 73.1 26.7 35.2JS FL .109 91.7 07.8 62.6 .032 96.2 02.1 94.6 .110 91.5 07.9 61.9 .116 73.1 26.7 35.2BC FL .109 91.9 07.5 63.3 .038 94.5 03.9 90.8 .109 92.8 06.6 66.0 .116 73.1 26.7 35.2KL FL* .457 99.2 00.0 78.9 .207 98.2 00.1 93.3 .526 99.3 00.0 71.4 .371 98.4 00.1 81.6JS FL* .465 99.2 00.0 78.2 .153 98.2 00.0 95.7 .548 99.3 00.0 66.0 .393 98.4 00.2 78.3BC FL* .465 99.2 00.0 78.2 .211 98.2 00.1 93.0 .535 99.3 00.0 68.7 .388 98.4 00.1 79.0KL HFL .348 99.1 00.0 87.8 .078 97.0 01.3 93.5 .363 99.1 00.0 84.4 .117 90.3 08.4 73.7JS HFL .348 99.1 00.0 87.8 .068 97.9 00.3 96.5 .359 99.1 00.1 83.7 .119 90.4 08.4 73.9BC HFL .348 99.1 00.0 87.8 .077 96.9 01.4 93.2 .361 99.1 00.0 85.7 .119 90.3 08.4 73.5Table 3: Performance of combined distributional and lexical indicators, at low and medium allophonic complexity.Boldface indicates the best value.
Italics indicate accuracies below that of a dummy indicator rejecting all pairs.show that conjoined and multiplied indicators per-form similarly, disparities emerge at medium allo-phonic complexity: while multiplication yields bet-ter MCC and FNR, conjunction yields better accu-racy and FPR.
In that regard, observing FPR valuesof zero is quite satisfactory from the point of viewof language acquisition, as processing two segmentsas realizations of a single phoneme (while they arenot) may lead to the confusion of true minimal pairsof words.
Indeed, at a higher level, learning allo-phonic rules allows the infant to reduce the size ofits emerging lexicon, factoring out allophonic real-izations for each underlying word form.Furthermore, AUC curves for the multiplicativescheme (Figure 2),8 most notably FL?s, suggest thatdistributional indicators?
contribution to the combi-nations appears to be rather negative, except at verylow allophonic complexities.
One explanation (yetto be tested experimentally) would be that they comeinto play later in the learning process, once part ofallophony has been reduced using other indicators.7 ConclusionWe presented an evaluation of distributional and lex-ical indicators of allophony.
Although they all per-form well at low allophonic complexities, misclas-sifications increase, more or less seriously, when8We do not report a threshold-free evaluation for the logi-cal scheme.
As it requires the estimation of the volume under asurface, comparison between schemes becomes difficult.
More-over, as the exact definition of the distributional indicator doesnot affect the results, we only plot combinations with JS.the average number of allophones per phoneme in-creases.
We also presented a first evaluation of thecombination of indicators, and found no significantdifference between the two combination schemes wedefined.
Unfortunately, none of the combinationswe tested outperforms individual indicators.For comparability with previous studies, we onlyconsidered combination schemes requiring no mod-ification in the definition of the task; however,learning allophonic pairs becomes unnatural whenphonemes can have more than two realizations.Embedding each indicator?s segment-to-segment(dis)similarities in a multidimensional space, for ex-ample, would enable the use of clustering techniqueswhere minimally distant points would be analyzedas allophones of a single phoneme.Thus far, segments have been nothing but abstractsymbols and, for example, the task at hand is ashard for [a] ?
[a?]
as it is for [4] ?
[k].
However,not only do allophones of a given phoneme tend tobe acoustically similar, but acoustic differences maybe more salient and/or available earlier to the infantthan complementary distributions or minimally dif-fering words.
Therefore, the main extension towardsa comprehensive model of the acquisition of allo-phonic rules would be to include acoustic indicators.AcknowledgmentsThis work was supported by a graduate fellowshipfrom the French Ministery of Research.
We thankBeno?
?t Crabbe?, Emmanuel Dupoux and SharonPeperkamp for helpful comments and discussion.92ReferencesLuc Boruta, Sharon Peperkamp, Beno?
?t Crabbe?, and Em-manuel Dupoux.
Submitted.
Testing the robustness ofonline word segmentation: effects of linguistic diver-sity and phonetic variation.Luc Boruta.
2011.
A note on the generation of allo-phonic rules.
Technical Report 0401, INRIA.Michael R. Brent and Timothy A. Cartwright.
1996.
Dis-tributional regularity and phonotactic constraints areuseful for segmentation.
Cognition, 61:93?125.Anthony C. C. Coolen, Reimer Ku?hn, and Peter Sollich.2005.
Theory of Neural Information Processing Sys-tems.
Oxford University Press.Isabelle Dautriche.
2009.
Mode?lisation des processusd?acquisition du langage par des me?thodes statistiques.Master?s thesis, INSA, Toulouse.Brian Dillon, Ewan Dunbar, and William Idsardi.
Inpreparation.
A single stage approach to learningphonological categories: insights from inuktitut.Charles Hockett.
1955.
A manual of phonology.
Inter-national Journal of American Linguistics, 21(4).Rozenn Le Calvez.
2007.
Approche computationnellede l?acquisition pre?coce des phone`mes.
Ph.D. thesis,UPMC, Paris.David Marr.
1982.
Vision: a Computational Investiga-tion into the Human Representation and Processing ofVisual Information.
W. H. Freeman.Andrew T. Martin, Sharon Peperkamp, and EmmanuelDupoux.
Submitted.
Learning phonemes with apseudo-lexicon.Brian W. Matthews.
1975.
Comparison of the pre-dicted and observed secondary structure of T4 phagelysozyme.
Biochimica et Biophysica Acta, ProteinStructure, 405(2):442?451.Jessica Maye, Janet F. Werker, and LouAnn Gerken.2002.
Infant sensitivity to distributional informa-tion can affect phonetic discrimination.
Cognition,82(3):B101?B111.Sharon Peperkamp, Rozenn Le Calvez, Jean-PierreNadal, and Emmanuel Dupoux.
2006.
The acquisitionof allophonic rules: statistical learning with linguisticconstraints.
Cognition, 101(3):B31?B41.Jenny R. Saffran, Richard N. Aslin, and Elissa L. New-port.
1996.
Statistical learning by 8-month-old in-fants.
Science, 274(5294):1926?1928.Tobias Sing, Oliver Sander, Niko Beerenwinkel, andThomas Lengauer.
2005.
ROCR: visualizing classifierperformance in R. Bioinformatics, 21(20):3940?3941.Anand Venkataraman.
2001.
A statistical model forword discovery in transcribed speech.
ComputationalLinguistics, 27(3):351?372.93
