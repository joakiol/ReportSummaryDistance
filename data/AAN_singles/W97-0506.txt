Augmented and alternative NLP techniques for augmentative andalternative communicationAnn CopestakeCenter for the Study of Language and Information,Stanford University,Ventura Hall,Stanford, CA 94305,USAaac@csl i ,  stanford,  eduAbst rac tCurrent communication devices designedfor non-speaking users are inadequate tosupport conversation because the speedwith which a user can input information istypically very limited.
We describe somepractical work on word prediction, anddiscuss its limitations as a technique forspeeding up free text entry.
We then out-line an alternative approach, currently un-der development, which combines predic-tion with a constrained technique for nat-ural language generation.1 In t roduct ionThe work described in this paper concerns the com-munication eeds of people who cannot speak be-cause of motor disabilities.
It is possible to buildprosthetic devices for such users by linking a suit-able physical interface with a speech synthesizer, sothat text or other symbolic input can be convertedto speech.
However, while speech rates in normalconversation are around 150-200 words per minute(wpm), and skilled typists can achieve rates of 30-40wpm 1, conditions which impair physical ability tospeak usually cause more general loss of motor func-tion and typically speech prosthesis users can onlyoutput at best 10-15 wpm using a keyboard, withmuch lower rates if direct letter selection is not pos-sible.
This prevents natural conversation, not sim-ply because of the time which is taken but becausethe delays completely disrupt the usual processes ofISome typists can copy text much faster than this,but constructing text takes more time, even with in-formal text such as email.
Some people who spend alot of time contributing to online forums have reportedtyping speeds which are considerably higher than thisrange, but they still cannot approach normal conversa-tion speeds.turn-taking.
Thus the other speaker finds it hard toavoid interrupting the prosthesis user.This problem can potentially be alleviated in twoways: by improving the design of the physical inter-face (keyboard, head-stick, head-pointer, eye-trackeretc) or by minimizing the input that is required fora given output.
The research described here con-centrates on the latter aspect, although the utilityof various techniques is partially dependent on theinterface.Techniques which have been used in augmentativeand alternative communication (AAC) often involvethe use of alternative symbol systems for input, inparticular Minspeak (Baker, 1982).
However, thework reported here was prompted by the needs of anindividual who has lost the ability to speak due toamyotrophic lateral sclerosis (ALS or Lou Gehrig'sdisease).
Such users would prefer to continue usingtheir original language, rather than to learn an al-ternative symbol system.
Several commercial AACsystems which take text input exist, but we foundthat these had a variety of drawbacks for our user.
Inparticular, most are dedicated to speech output andcannot be used to aid writing text or email.
Thereare also limitations in compatibility with particularsoftware or hardware, and restrictions in the physicalinterfaces.
A system was developed at CSLI whichwould run on a standard laptop while still allowingthe use of other software (email, Web browser etc).The initial version of this system incorporates wordprediction, as describec~ in the next section, and alsoa small number of fixed text utterances, accessiblevia dedicated keys or menus.
Experience with thissuggested that an approach which allowed for moreflexible combination of fixed and free text might haveadvantages.
This is outlined in ?3.2 Exper iments  w i th  word  pred ic t ionPrediction techniques have been extensively used inAAC, see, for example, the review in Darragh and37Witten (1992).
The basic technique behind wordprediction is to give the user a choice of the words(or words and phrases) which are calculated to bethe most likely, based on the previous input.
Thechoices are usually displayed as some sort of menu:if the user selects one of the items, that word isoutput, but if no appropriate choice is present, theuser continues entering letters.
Prediction orderingis based on the user's previous input, and so the sys-tem can automatically adapt to the individual user.Unknown strings are added to the database, thus al-lowing them to be predicted subsequently.
For textinput, the simplest echnique is to use the initial let-ters of a word as context, and to predict words onthe basis of their frequencies.
This basic approachwas implemented in the first version of our system.In order to experiment with different algorithms,we ran simulations using 26,000 words of data loggedfrom the daily speech of one user.
The collecteddata was split into training and test sets (10% oftotal data).
We used the following scoring methodto measure performance:(1 - (keystrokes +_ menu selections) ) ?
100keystrokes needed without predictionFor example, choosing table after inputting 't' 'a'would give a score of 50%, since a space is automat-ically output after the word.
This scoring method isan idealization but is a reasonably accurate predic-tor of keystroke savings for our user.We should emphasize that a saving in keystrokeswill not correspond to an equivalent reduction intime taken to construct a message, since there isa cognitive cost in searching a menu for the desiredword.
Prediction is only useful when text input isvery slow and difficult - -  even someone using a head-stick may not find any advantage in it.
For our user,however, each movement is not only slow, but tir-ing and somewhat painful, so keystroke saving is auseful measurement of performance.
Prediction alsohas the side-effect of reducing misspellings and ty-pographic errors, which is useful because these oftenmake the synthesized speech incomprehensible.The graph in Figure 1 shows the keystroke sav-ings achieved with the basic method compared withan enhanced method which takes some account ofcontext.
Results are shown with varying menu sizes.Our user actually works with a menu size of 10, butclearly much larger menu sizes than this are imprac-ticah the graph shows sizes up to 20 because thiscrudely approximates results that might be achiev-able with a better predictor with smaller menus.The simplest way to add contextual information is totemporarily increase weights of recently seen words(recency).
We found this improved prediction ratesby an additional 0.9% at a menu size of 8.
The restof the improvement for the enhanced method is dueto the use of part of speech bigrams.
22.1 Part of speech bigramsGiven the great improvements that have been madein speech recognition by using Hidden Markov Mod-els (HMMs), it is natural to expect hat these tech-niques would be beneficial for word prediction.
How-ever existing text corpora do not make good mod-els for the speech of our user, and the amount oftraining data which we can collect is insufficient toextract reliable word-based trigrams.
26,000 wordsof data represents around three months of input,and much more data would be necessary to collectuseful trigrams (vocabulary size in the 26,000 wordsample is over 3,000).
One possible solution is toback off to broader categories, o we investigated theuse of part of speech (POS) transition probabilitiesextracted from existing tagged corpora (Penn Tree-bank).
However, this actually led to a degradationin performance with the corpora we tried, becausethey were a poor model for our data: we suspectthis is because our user makes frequent use of ques-tions, imperatives and interjections.
We thereforedecided to derive transition probabilities by taggingour collected ata.Somewhat surprisingly the Treebank corporaturned out to be a good model for our data with re-spect o the most likely POS associated with a wordand we obtained about 92% tagging accuracy sim-ply by choosing the most frequent tag on this basis.We could not improve on this with the taggers wetried, possibly because of the small size of our train-ing sample, and the very short length of most of theutterances.
However, we have not investigated thisin detail because we would expect improved taggingaccuracy to have only a small effect on transitionprobabilities and hence on prediction performance.Furthermore, not having to use a tagger makes itmuch simpler to implement a practical system whichadapts to the user and to the type of text.The data shown in Figure 1 are based on a set of 80tags, since we expanded the initial tagset by addingindividual tags for some frequent words.
Thus weare effectively using a combination of POS and wordtransition probabilities.
Expanding the tagset inthis way improves prediction performance: for in-stance distinguishing between personal pronouns al-lows greatly improved prediction of verb agreement.The only hand-coding involved was in reviewing and2A more detailed escription of some of the work de-scribed in this section is in Copestake (1996).38%of totalkeystrokessaved55k5045403530?
00?
0000o basic technique?
syntax and recency2 4 6 8 10 12 14 16 18 20Menu sizeFigure 1: Prediction results for various menu sizesrevising the tags for the most frequent 50 wordsin the collected ata.
Unknown words are treatedas proper names.
Misspellings occur at a rate ofroughly 1% in our collected data, usually involv-ing words which were not in the lexicon at thetime the corpus was collected and were therefore notpredicted.
3 We expect o be able to achieve slightlybetter results by further expanding the tagset byusing semantic tags of the sort discussed in ?3 toimprove the prediction rates for nouns (and perhapsverbs).
We could perhaps also improve results byusing trigrams rather than bigrams.2.2 The l imitations of word predict ionIt is difficult to compare our results with those inthe word prediction literature, because of the lackof common corpora and of an agreed standard ofmeasurement.
However our results appear at leastcomparable with those previously reported for anadult English vocabulary.
It seems to be very diffi-cult to achieve keystrokes savings much above 50%.It is sometimes assumed that estimates of entropy(e.g., Shannon's estimate that English is 75% re-dundant, Brown et als (1992) upper bound of 1.75bits per character for printed English) are directly3There are some cases where words are deliberatelymisspelled in order to get better output from the syn-thesizer, such as coyote spelled kiote.applicable to word prediction and imply that muchbetter keystroke savings could be achieved, in prin-ciple.
However, the relationship between entropyand word prediction is somewhat complex.
Firstly,if a standard keyboard or equivalent device is usedto enter letters, then no advantage is being takenof techniques such as Huffman encoding which re-duce the number of bits required per letter (althoughsuch encodings are relevant o the use of binaryswitches).
Secondly, consider the trivial exampleof a "language" which consists of 25 equiprobable"words" each of 4 letters, written without spaces, allof which have the common prefix ZZZ (i.e., ZZZA,ZZZB ... ZZZY).
This language is 75% redundantbut standard left-to-right prediction will not workwell (20% keystroke saving with a menu size of 8with the algorithm used here).
A language withwords AZZZ, BZZZ ... ZZZY has the same entropy,but allows much better prediction performance (56%saving).
4 To make the comparison between entropyand prediction, we have to consider the informationcontributed by each input.
Of course, in this artifi-cial language, the Z's actually carry no information.As a rough comparison of word prediction andentropy figures for English, note that in order to4The relevance of this to natural languages i thatsimple left-to-right prediction does not work well if thereare a large number of possible suffixes.39achieve 50% keystroke savings with input which, likethat of our user, averages 4 letters per word, it isnecessary to predict the word on average after 1.5letters have been input (assuming that the space ispredicted).
Assuming a letter can be Huffman en-coded in 4 bits and choosing from a menu of 8 itemsis 3 bits, we have an entropy figure of 1.8 bits percharacter, close to Brown et als result.
For 60%savings, the figure is 1.4 bits.
This admittedly crudecalculation suggests that it may be unrealistic to ex-pect much better than 50-60% keystroke savings onfree text with a usable menu size.We believe that although prediction techniquesare robust and flexible, by themselves they cannotoffer the improvement in text input speed neces-sary to allow natural conversation using a text-to-speech system.
Work at the University of Dundee(e.g., Aim et al 1992; Todman and Alm, this vol-ume) has shown that the extensive use of fixed textfor sequences such as greetings and prestored narra-tives is beneficial in AAC.
We would like to extendthis to the potentially much wider range of situa-tions where partially predefined strings are appro-priate.
We are therefore investigating an alternativeapproach, making use of cogeneration, a novel natu-ral language generation technique, which allows theflexible combination of free and fixed text.
This isdiscussed in the next section.3 The  cogenerat ion  approachTraditionally, natural language generation has beenseen as the inverse of parsing, where the input issome sort of meaning representation, such as pred-icate calculus expressions.
This is inappropriatefor AAC, since formal meaning representation lan-guages are hard to learn and anyway tend to be moreverbose than their natural anguage counterparts.Instead, in cogeneration, i put is partially specifiedas a series of text units by the user, and the job ofthe generator is to combine these units into gram-matical, coherent sentences which are idiomatic, ap--propriately polite and so on.
To accomplish this,the generator has to be able to order the text units,add inflections and insert extra words (both func-tion and content words).
Cogeneration thus buildson work on compansion (e.g., Demasco and McCoy,1992; McCoy, this volume).Cogeneration i volves a combination ofgrammat-ical, statistical, and template-based constraints.
ForAAC devices the templates are designed for partic-ular dialogue situations.
The choice of template ismade by the user, and the interface provides lotswhich the user instantiates with text.
In many cases,slots will be optional or have default fillers, con-structed according to context and previous inputs.We assume that instantiation of the slots can beaided by word and phrase prediction, conditioned onslot choice.
Prediction should be much more effec-tive than with free text, since the slots will providefine-grained syntactic and semantic constraints.
Thecogenerator perates by combining the constraintsspecified by the template(s) with the general con-straints of the grammar'to produce an output sen-tence, guided by statistical information.For example, a request template might have slotsfor requested action and for the requestee.
Sup-pose the user input open kitchen window into therequested action slot and that the requestee slot de-faulted to you.
The system might plausibly generatePlease could you open the kitchen window using fixedtext associated with the request template.
The userwould be given the option of making the output moreurgent and less polite, which might result in Openthe kitchen window.
( One significant advantage ofthe cogeneration technique is that extra informationis available to guide the speech synthesizer, allow-ing more appropriate intonation, prosody and evenvolume.Cogeneration involves three different types ofknowledge source: a grammar and lexicon, sta-tistical information about collocations and pre-ferred syntactic structures, application- and context-dependent templates.
We do not have space here togive details of all these aspects of the cogenerationsystem, and it would be inappropriate given that de-velopment is in its early stages.
However, one areawhich we will examine in slightly more detail, be-cause of its relevance to the work on word predic-tion discussed above, is the proposed use of seman-tic categories in the grammar and in the statisticalcomponent.Consider the example given above, where the userinputs open kitchen window as the requested actionin a request template.
Some of the actions requiredin the course of accepting and processing this inputare:1.
Predicting words (e.g.
window is more likely inthis context han work).. Recognizing kitchen window as a unit for gener-ation , so it does not get split up, and precedingit with a plausible determiner..
Giving the utterance the correct stress (com-pounds vary in stress in a way that partly de-pends on meaning: contrast cotton bag meaningbag made out of cotton and bag for cotton).40JObviously the requirements may differ to someextent for other grammatical constructions: forinstance, we have to recognize not just noun-noun compounds but conventionalized adjective-noun combinations (e.g., social security).
In general,achieving this sort of analysis requires ome statis-tical information about the words in the input andtheir collocations.
One possibility would be to usen-grams based on words (or word stems), but evenassuming that we could extract useful informationfrom one of the existing large text corpora ratherthan an AAC-specific one, the data is still likely tobe too sparse for all but the most frequent words.For example, kitchen window occurs only once in theapproximately one million word LOB corpus, anddoes not occur at all in the similarly sized portionof the Wall Street' Journal distributed as part of thePenn Treebank.The alternative strategy which we have adoptedis to back off to semantic lasses for infrequent orunseen collocations.
For example, kitchen might beclassified as space- loc,  whichis intended to encom-pass locations which have significant spatial extent,and window as f igure -ground (following Puste-jovsky (1995) who uses this nomenclature becausewindow belongs to a class of words which can refereither to an opening or its filler).
The semantic at-egories are arranged in a hierarchy.
The intention isthat these categories would improve prediction andsupport a form of partial parsing: e.g., we could de-tect that kitchen window was a plausible constituentbelonging to a particular class of noun-noun com-pounds, even if our corpus contained no instancesof kitchen window itself, because the class of space-loc f igure -ground compounds would be recorded,based on combining data on the frequencies of allcompounds which fit this schema (bedroom window,bathroom door, et'c).
Similarly, determiner choicewill depend to some extent on the verb governingthe noun phrase.
For instance, although the is muchmore frequent han a/an in most corpora, followingbuy the reverse is usually true.
But again, availablecorpora are too small or too unbalanced to deter-mine this information for less frequent verbs, and itis necessary to consider verb classes instead (Levin's(1993) verbs  o f  obta in ing  would be a relevant classfor this example).It may sometimes be necessary to do lexical dis-ambiguation to support generation, but often this isnot required.
For instance, it would not be necessaryto distinguish between house windows and windowson a computer screen if open window were inputto the request emplate considered above, becausethe generated utterance would be the same in eithercase.
Classes uch as f igure -ground are intended tocapture some types of systematic polysemy.
The useof rather broad categories also reduces the extent owhich words are ambiguous with respect o semanticcategory.
We think this is an advantage comparedto the use of classes which are more tightly linked toreal world knowledge.
For example, window is givenfour senses in WordNet (Miller, 1990), correspondingto the usage for buildings, vehicles, envelopes andcomputers (the metaphorical window o/opportunityuse is omitted).
However, for our purposes we sus-pect that it is unnecessary to distinguish these uses.In general, the granularity of the lexical semanticclasses has to be sufficiently coarse to enable reliablestatistics to be obtained, but also should not intro-duce unnecessary ambiguity.
For example, havinga class veh ic le -par t  might be counter-productive,because it would lead to words such as engine be-ing ambiguous between their use in vehicles and instationary objects, which is unwarranted linguisti-cally.
For our current purposes, even distinguishingbetween less related usages uch as the luggage useof trunk and the (American English) part-of-car useis probably unnecessary, since a more general class,such as conta iner ,  would capture most of the rele-vant behaviour of both.We have found that it is possible to use Word-Net as a knowledge source to semi-automatically de-rive semantic lasses of the appropriate granularity,even though our semantic hierarchy does not cor-respond to the WordNet taxonomy.
Effectively weregard WordNet as a source of information whichwe can exploit about word groups.
For example,we can identify nodes which are superordinate o agroup of senses which should be given the same code,such as room (sense 1) for the category space- loc.By associating the category with a relatively generalnode, we can automatically classify a large numberof words with a fair degree of reliability.
Because thesemantic hierarchy does not correspond in a simpleway to WordNet, a particular category may have tobe associated with several disjoint WordNet subhier-archies, and it is necessary to allow for exceptions.We are in the process of developing and refiningthe semantic lassification for nouns and verbs.
As afirst step, we intend to see whether these categoriescan be used to improve the prediction rate for freetext entry, by using the semantic lasses to expandthe tagset described in ?2.1.
We expect o use thesecategories directly in the symbolic grammar, to con-trol preposition selection and to provide schematafor compound nouns, for instance.414 Conclusions and future workWe have described a prediction system which canadapt to a user's vocabulary and syntax with fairlysmall amounts of data.
The techniques described areall ones which can be used on the fly, although forefficiency it might be desirable todo  morphologicalanalysis and n-gram frequency at times when thesystem is not being actively used (e.g.
overnight).We have discussed the limitations of the e~ciencyof prediction, and introduced the idea of cogenera-tion which combines free text entry with fixed textassociated with templates.Our prediction work has proven practical utilitysince our user selects predictions at close to the max-imal level (i.e., it is rare for a predicted possibilityto be missed).
It is difficult to determine bow muchsaving in utterance generation time results from pre-diction, but it is clear that it considerably reducesphysical effort.
Our user will not accept any sys-tem which does not incorporate prediction.
In con-trast, our work on cogeneration is at a very earlystage.
We aim to build the cogeneration system in amodular fashion that allows the reuse of knowledgesources.
For example, we expect he semantic ate-gories described in the previous ection to be usefulin prediction outside the context of a full cogener-ation system and also to allow better output fromthe speech synthesizer, e.g., in the pronunciation ofhomographs, such as bow, in stress placement forcompounds etc.
We believe that such flexibility isnecessary to maximize the chances of NLP researchhaving practical utility for AAC systems.ReferencesAim, Norman, John L. Arnott and Alan F. Newell(1992) 'Prediction and conversational momen-tum in an augmentative communication system',Communications of the ACM, 35(5), 47-57.Baker, B. R. (1982) 'Minspeak', Byte, 7(9), 186-202.?
Brown, P. F., S.A. DellaPietra, V.J.
DellaPietra,J.C.
Lai and R.L.
Mercer (1992) 'An estimateof an upper bound for the entropy of English',Computational Linguistics, 18(1), 31-40.Copestake, A.
(1996) 'Applying natural languageprocessing techniques to speech prostheses', Pro-ceedings of the AAAI Fall Symposium on devel-oping assistive technology for people with disabil-ities, MIT, Cambridge, MA.Darragh, J. J. and I. H. Witten (1992) The reactivekeyboard, Cambridge University Press.Demasco, Patrick W. and Kathleen F. McCoy (1992)'Generating text from compressed input:an in-telligent interface for people with severe mo-tor impediments', Communications ofthe A UM,35(5), 68-78.Levin, B.
(1993) English verb classes and alterna-tions, University of Chicago Press, Chicago.Miller, George (1990) 'WordNet: An on-line lexi-cal database', International Journal o\] Lexicog-raphy, 3, 235-312.Newell, Alan F., John L. Arnott, Alistair Y. Cairns,Ian W. Ricketts and Peter Gregor (1995) 'In-telligent systems for speech and language im-paired people: a portfolio of research' in Ed-wards, Alistair D. N.
(ed.
), Extra-OrdinaryHuman-Computer Interaction, Cambridge Uni-versity Press, pp.
83-101.Pustejovsky, J.
(1995) The Generative Lexicon, MITPress, Cambridge, MA.42
