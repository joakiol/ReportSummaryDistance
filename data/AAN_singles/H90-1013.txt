Session 3: Natural Language EvaluationLynette Hirschman, ChairUnisys Defense SystemsCenter for Advanced Information TechnologyPaoli, PA 19301The session on Natural Language Evaluation focused onmethods for evaluating text understanding systems.
Begin-ning with the first Message Understanding Conference(MUCK-l) in 1987, there has been increasing focus onhow to measure and evaluate text understanding systems.The MUCK-1 conference r quired evelopers to port theirsystem to a common domain of Navy intelligence mes-sages; MUCK-2 (May 1989) developed the first sconngsystem for evaluation based on template fill (also on adomain of Navy messages).
MUCK-3 (to take place in late1990 and early 1991) will refine that process and includean automated sconng procedure to grade quality oftemplate fill.
Meanwhile, a second evaluation effort hasstarted, related to the MURASAKI multi-lingual text un-derstanding project.
It also uses the notion of evaluatingsystems doing template fill.Both Murasaki and MUCK-3 were discussed uring theNatural Language Evaluation session.
The first (and only)paper of the session, - Evaluating Natural LanguageGenerated Database Records, given by Rita McCardell(DoD), described a detailed sconng algorithm proposed forthe Muraski project.
The evaluation proposes to scoretemplate fills based on correctness, completeness andsemantic proximity.
This paper was followed by an infor-mal presentation of plans for the third Message Under-standing Conference, MUCK-3, by Beth Sundheim(NOSC).During the discussion, several important points cameup.
Scoring algorithms for both Murasaki and MUCK-3 arecomplex and involve difficult issues, such as how to scoretemplate slots requiring multiple fills, whether and howmuch to penalize for incorrect answers, whether all slotscount the same, etc.
One area of concern was "how toscore the scoring algorithm" -- that is, how to decide if thealgorithm emphasizes the right things with respect o anintended application and how to calibrate the variousweighting factors in light of that intended application.
Todate, "applications" have seemed fairly artificial, so thatdevelopers had little guidance as to the relative importanceof precision (roughly how many mistakes the systemmakes) vs. recall (how many slots the system can fill in).In general, there was consensus that simpler scoringmethods were preferable.
Other suggestions included as-king system developers to provide a range of results how-ing how precision varied with recall, and evaluating sys-tems by giving them a multiple choice "reading com-prehension" exam.63
