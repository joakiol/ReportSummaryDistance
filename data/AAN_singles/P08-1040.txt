Proceedings of ACL-08: HLT, pages 344?352,Columbus, Ohio, USA, June 2008. c?2008 Association for Computational LinguisticsSentence Simplification for Semantic Role LabelingDavid Vickrey and Daphne KollerStanford UniversityStanford, CA 94305-9010{dvickrey,koller}@cs.stanford.eduAbstractParse-tree paths are commonly used to incor-porate information from syntactic parses intoNLP systems.
These systems typically treatthe paths as atomic (or nearly atomic) features;these features are quite sparse due to the im-mense variety of syntactic expression.
In thispaper, we propose a general method for learn-ing how to iteratively simplify a sentence, thusdecomposing complicated syntax into small,easy-to-process pieces.
Our method appliesa series of hand-written transformation rulescorresponding to basic syntactic patterns ?for example, one rule ?depassivizes?
a sen-tence.
The model is parameterized by learnedweights specifying preferences for some rulesover others.
After applying all possible trans-formations to a sentence, we are left with aset of candidate simplified sentences.
We ap-ply our simplification system to semantic rolelabeling (SRL).
As we do not have labeled ex-amples of correct simplifications, we use la-beled training data for the SRL task to jointlylearn both the weights of the simplificationmodel and of an SRL model, treating the sim-plification as a hidden variable.
By extractingand labeling simplified sentences, this com-bined simplification/SRL system better gener-alizes across syntactic variation.
It achievesa statistically significant 1.2% F1 measure in-crease over a strong baseline on the Conll-2005 SRL task, attaining near-state-of-the-artperformance.1 IntroductionIn semantic role labeling (SRL), given a sentencecontaining a target verb, we want to label the se-mantic arguments, or roles, of that verb.
For theverb ?eat?, a correct labeling of ?Tom ate a salad?is {ARG0(Eater)=?Tom?, ARG1(Food)=?salad?
}.Current semantic role labeling systems rely pri-marily on syntactic features in order to identify andSNP VPVPNP PPTom wants SatoeatVPNP NPsaladcroutonswithTom: NP S(NP) VPVP VPS TNP1croutons: VPPP(with)Tsalad: NP1 VP TFigure 1: Parse with path features for verb ?eat?.classify roles.
Features derived from a syntacticparse of the sentence have proven particularly useful(Gildea & Jurafsky, 2002).
For example, the syntac-tic subject of ?give?
is nearly always the Giver.
Pathfeatures allow systems to capture both general pat-terns, e.g., that the ARG0 of a sentence tends to bethe subject of the sentence, and specific usage, e.g.,that the ARG2 of ?give?
is often a post-verbal prepo-sitional phrase headed by ?to?.
An example sentencewith extracted path features is shown in Figure 1.A major problem with this approach is that thepath from an argument to the verb can be quitecomplicated.
In the sentence ?He expected to re-ceive a prize for winning,?
the path from ?win?
to itsARG0, ?he?, involves the verbs ?expect?
and ?re-ceive?
and the preposition ?for.?
The correspondingpath through the parse tree likely occurs a relativelysmall number of times (or not at all) in the trainingcorpus.
If the test set contained exactly the samesentence but with ?expected?
replaced by ?did notexpect?
we would extract a different parse path fea-ture; therefore, as far as the classifier is concerned,the syntax of the two sentences is totally unrelated.In this paper we learn a mapping from full, com-plicated sentences to simplified sentences.
For ex-ample, given a correct parse, our system simplifiesthe above sentence with target verb ?win?
to ?Hewon.?
Our method combines hand-written syntac-tic simplification rules with machine learning, which344determines which rules to prefer.
We then use theoutput of the simplification system as input to a SRLsystem that is trained to label simplified sentences.Compared to previous SRL models, our modelhas several qualitative advantages.
First, we be-lieve that the simplification process, which repre-sents the syntax as a set of local syntactic transfor-mations, is more linguistically satisfying than usingthe entire parse path as an atomic feature.
Improvingthe simplification process mainly involves addingmore linguistic knowledge in the form of simplifi-cation rules.
Second, labeling simple sentences ismuch easier than labeling raw sentences and allowsus to generalize more effectively across sentenceswith differing syntax.
This is particularly importantfor verbs with few labeled training instances; usingtraining examples as efficiently as possible can leadto considerable gains in performance.
Third, ourmodel is very effective at sharing information acrossverbs, since most of our simplification rules applyequally well regardless of the target verb.A major difficulty in learning to simplify sen-tences is that we do not have labeled data for thistask.
To address this problem, we simultaneouslytrain our simplification system and the SRL system.We treat the correct simplification as a hidden vari-able, using labeled SRL data to guide us towards?more useful?
simplifications.
Specifically, we trainour model discriminatively to predict the correct rolelabeling assignment given an input sentence, treat-ing the simplification as a hidden variable.Applying our combined simplification/SRLmodel to the Conll 2005 task, we show a significantimprovement over a strong baseline model.
Ourmodel does best on verbs with little training data andon instances with paths that are rare or have neverbeen seen before, matching our intuitions about thestrengths of the model.
Our model outperforms allbut the best few Conll 2005 systems, each of whichuses multiple different automatically-generatedparses (which would likely improve our model).2 Sentence SimplificationWewill begin with an example before describing ourmodel in detail.
Figure 2 shows a series of transfor-mations applied to the sentence ?I was not given achance to eat,?
along with the interpretation of eachtransformation.
Here, the target verb is ?eat.
?I was not given a chance to eat.Someone gave me a chance to eat.I had a chance to eat.I ate.depassivizegive -> havechance to XI was given a chance to eat.remove notFigure 2: ExamplesimplificationSam?s chance to eat has passed.Sam has a chance to eat.Sam ate.chance to XpossessiveFigure 3: Shared simplifica-tion structureThere are several important things to note.
First,many of the steps do lose some semantic informa-tion; clearly, having a chance to eat is not the sameas eating.
However, since we are interested only inlabeling the core arguments of the verb (which inthis case is simply the Eater, ?I?
), it is not importantto maintain this information.
Second, there is morethan one way to choose a set of rules which leadto the desired final sentence ?I ate.?
For example,we could have chosen to include a rule which wentdirectly from the second step to the fourth.
In gen-eral, the rules were designed to allow as much reuseof rules as possible.
Figure 3 shows the simplifica-tion of ?Sam?s chance to eat has passed?
(again withtarget verb ?eat?
); by simplifying both of these sen-tences as ?X had a chance to Y?, we are able to usethe same final rule in both cases.Of course, there may be more than one way tosimplify a sentence for a given rule set; this ambigu-ity is handled by learning which rules to prefer.In this paper, we use simplification to mean some-thing which is closer to canonicalization that sum-marization.
Thus, given an input sentence, our goalis not to produce a single shortened sentence whichcontains as much information from the original sen-tence as possible.
Rather, the goal is, for eachverb in the sentence, to produce a ?simple?
sentencewhich is in a particular canonical form (describedbelow) relative to that verb.3 Transformation RulesA transformation rule takes as input a parse tree andproduces as output a different, changed parse tree.Since our goal is to produce a simplified version ofthe sentence, the rules are designed to bring all sen-tences toward the same common format.A rule (see left side of Figure 4) consists of two345NP-7[Someone] VB-5 NPVP-4give chanceNP-2IS-1S-1NP-2 VP-3VB*-6VBN-5be VP-4TransformedRuleReplace 3 with 4Create new node 7 ?
[Someone]Substitute 7 for 2Add 2 after 5Set category of 5 to VBSNP VPVBDVBN NPwasVPgiven chanceIOriginalFigure 4: Rule for depassivizing a sentenceparts.
The first is a ?tree regular expression?
whichis most simply viewed as a tree fragment with op-tional constraints at each node.
The rule assignsnumbers to each node which are referred to in thesecond part of the rule.
Formally, a rule node Xmatches a parse-tree node A if: (1) All constraints ofnode X (e.g., constituent category, head word, etc.
)are satisfied by node A.
(2) For each child node Yof X, there is a child B of A that matches Y; twochildren of X cannot be matched to the same childB.
There are no other requirements.
A can haveother children besides those matched, and leaves ofthe rule pattern can match to internal nodes of theparse (corresponding to entire phrases in the origi-nal sentence).
For example, the same rule is used tosimplify both ?I had a chance to eat,?
and ?I had achance to eat a sandwich,?
(into ?I ate,?
and ?I atea sandwich,?).
The insertion of the phrase ?a sand-wich?
does not prevent the rule from matching.The second part of the rule is a series of simplesteps that are applied to the matched nodes.
For ex-ample, one type of simple step applied to the pair ofnodes (X,Y) removes X from its current parent andadds it as the final child of Y.
Figure 4 shows thedepassivizing rule and the result of applying it to thesentence ?I was given a chance.?
The transformationsteps are applied sequentially from top to bottom.Note that any nodes not matched are unaffected bythe transformation; they remain where they are rel-ative to their parents.
For example, ?chance?
is notmatched by the rule, and thus remains as a child ofthe VP headed by ?give.
?There are two significant pieces of ?machinery?
inour current rule set.
The first is the idea of a floatingnode, used for locating an argument within a subor-dinate clause.
For example, in the phrases ?The catthat ate the mouse?, ?The seed that the mouse ate?,and ?The person we gave the gift to?, the modifiednouns (?cat?, ?seed?, and ?person?, respectively) allSimplifiedOriginal#Rule CategoryI ate the food.Float(The food) Iate.5Floating nodesHe slept.I said he slept.4Sentence extractionFood is tasty.Salt makes food tasty.8?Make?
rewritesThe total includes tax.Including tax, the total?7Verb acting as PP/NPJohn has achance to eat.John?s chance to eat?7PossessiveI will eat.Will I eat?7Questions I will eat.Nor will I eat.7Inverted sentencesFloat(The food) Iate.The food I ate ?8Modified nounsI eat.I have a chance toeat.7Verb RC (Noun)I eat.I am likely to eat.6Verb RC (ADJP/ADVP) I eat.I want to eat.17Verb Raising/Control (basic)I eat.I must eat.14Verb Collapsing/RewritingI ate.I ate and slept.8Conjunctions John is a lawyer.John, a lawyer, ?20Misc Collapsing/RewritingA car hit me.I was hit by a car.5PassiveI slept Thursday.Thursday, I slept.24Sentence normalizationTable 1: Rule categories with sample simplifications.Target verbs are underlined.should be placed in different positions in the subor-dinate clauses (subject, direct object, and object of?to?)
to produce the phrases ?The cat ate the mouse,?
?The mouse ate the seed?, and ?We gave the gift tothe person.?
We handle these phrases by placing afloating node in the subordinate clause which pointsto the argument; other rules try to place the floatingnode into each possible position in the sentence.The second construct is a system for keeping trackof whether a sentence has a subject, and if so, whatit is.
A subset of our rule set normalizes the inputsentence by moving modifiers after the verb, leavingeither a single phrase (the subject) or nothing beforethe verb.
For example, the sentence ?Before leaving,I ate a sandwich,?
is rewritten as ?I ate a sandwichbefore leaving.?
In many cases, keeping track of thepresence or absence of a subject greatly reduces theset of possible simplifications.Altogether, we currently have 154 (mostly unlex-icalized) rules.
Our general approach was to writevery conservative rules, i.e., avoid making ruleswith low precision, as these can quickly lead to alarge blowup in the number of generated simple sen-tences.
Table 1 shows a summary of our rule-set,grouped by type.
Note that each row lists only onepossible sentence and simplification rule from that346S-1NP or S VPVB*eat#children(S-1) = 2S-1VPVB*eat#children(S-1) = 1Figure 5: Simple sentence constraints for ?eat?category; many of the categories handle a variety ofsyntax patterns.
The two examples without targetverbs are helper transformations; in more complexsentences, they can enable further simplifications.Another thing to note is that we use the terms Rais-ing/Control (RC) very loosely to mean situationswhere the subject of the target verb is displaced, ap-pearing as the subject of another verb (see table).Our rule set was developed by analyzing perfor-mance and coverage on the PropBank WSJ trainingset; neither the development set nor (of course) thetest set were used during rule creation.4 Simple Sentence ProductionWe now describe how to take a set of rules and pro-duce a set of candidate simple sentences.
At a highlevel, the algorithm is very simple.
We maintain aset of derived parses S which is initialized to con-tain only the original, untransformed parse.
One it-eration of the algorithm consists of applying everypossible matching transformation rule to every parsein S, and adding all resulting parses to S. With care-fully designed rules, repeated iterations are guaran-teed to converge; that is, we eventually arrive at a setS?
such that if we apply an iteration of rule applica-tion to S?, no new parses will be added.
Note thatwe simplify the whole sentence without respect to aparticular verb.
Thus, this process only needs to bedone once per sentence (not once per verb).To label arguments of a particular target verb, weremove any parse from our set which does not matchone of the two templates in Figure 5 (for verb ?eat?
).These select simple sentences that have all non-subject modifiers moved to the predicate and ?eat?as the main verb.
Note that the constraint VB* indi-cates any terminal verb category (e.g., VBN, VBD,etc.)
A parse that matches one of these templatesis called a valid simple sentence; this is exactlythe canonicalized version of the sentence which oursimplification rules are designed to produce.This procedure is quite expensive; we have tocopy the entire parse tree at each step, and in gen-eral, this procedure could generate an exponentialnumber of transformed parses.
The first issue can besolved, and the second alleviated, using a dynamic-programming data structure similar to the one usedto store parse forests (as in a chart parser).
This datastructure is not essential for exposition; we delaydiscussion until Section 7.5 Labeling Simple SentencesFor a particular sentence/target verb pair s, v, theoutput from the previous section is a set Ssv ={tsvi }i of valid simple sentences.
Although labelinga simple sentence is easier than labeling the originalsentence, there are still many choices to be made.There is one key assumption that greatly reduces thesearch space: in a simple sentence, only the subject(if present) and direct modifiers of the target verbcan be arguments of that verb.On the training set, we now extract a set of rolepatterns Gv = {gvj }j for each verb v. For exam-ple, a common role pattern for ?give?
is that of ?Igave him a sandwich?.
We represent this patternas ggive1 = {ARG0 = Subject NP, ARG1 =Postverb NP2, ARG2 = Postverb NP1}.
Notethat this is one atomic pattern; thus, we are keep-ing track not just of occurrences of particular rolesin particular places in the simple sentence, but alsohow those roles co-occur with other roles.For a particular simple sentence tsvi , we applyall extracted role patterns gvj to tsvi , obtaining a setof possible role labelings.
We call a simple sen-tence/role labeling pair a simple labeling and denotethe set of candidate simple labelings Csv = {csvk }k.Note that a given pair tsvi , gvj may generate morethan one simple labeling, if there is more than oneway to assign the elements of gvj to constituents intsvi .
Also, for a sentence s there may be severalsimple labelings that lead to the same role labeling.In particular, there may be several simple labelingswhich assign the correct labels to all constituents;we denote this set Ksv ?
Csv.6 Probabilistic ModelWe now define our probabilistic model.
Given a(possibly large) set of candidate simple labelingsCsv, we need to select a correct one.
We assigna score to each candidate based on its features:347Rule = DepassivizePattern = {ARG0 = Subj NP, ARG1 = PV NP2, ARG2 = PV NP1}Role = ARG0, Head Word = JohnRole = ARG1, Head Word = sandwichRole = ARG2, Head Word = IRole = ARG0, Category = NPRole = ARG1, Category = NPRole = ARG2, Category = NPRole = ARG0, Position = Subject NPRole = ARG1, Position = Postverb NP2Role = ARG2, Position = Postverb NP1Figure 6: Features for ?John gave me a sandwich.
?which rules were used to obtain the simple sentence,which role pattern was used, and features about theassignment of constituents to roles.
A log-linearmodel then assigns probability to each simple label-ing equal to the normalized exponential of the score.The first type of feature is which rules were usedto obtain the simple sentence.
These features are in-dicator functions for each possible rule.
Thus, we donot currently learn anything about interactions be-tween different rules.
The second type of feature isan indicator function of the role pattern used to gen-erate the labeling.
This allows us to learn that ?give?has a preference for the labeling {ARG0 = SubjectNP, ARG1 = Postverb NP2, ARG2 = Postverb NP1}.Our final features are analogous to those used in se-mantic role labeling, but greatly simplified due toour use of simple sentences: head word of the con-stituent; category (i.e., constituent label); and posi-tion in the simple sentence.
Each of these featuresis combined with the role assignment, so that eachfeature indicates a preference for a particular roleassignment (i.e., for ?give?, head word ?sandwich?tends to be ARG1).
For each feature, we have averb-specific and a verb-independent version, allow-ing sharing across verbs while still permitting dif-ferent verbs to learn different preferences.
The setof extracted features for the sentence ?I was givena sandwich by John?
with simplification ?John gaveme a sandwich?
is shown in Figure 6.
We omit verb-specific features to save space .
Note that we ?stem?all pronouns (including possessive pronouns).For each candidate simple labeling csvk we extracta vector of features f svk as described above.
We nowdefine the probability of a simple labeling csvk withrespect to a weight vector w P (csvk ) =ewT fsvkPk?
ewT fsvk?.Our goal is to maximize the total probability as-signed to any correct simple labeling; therefore, foreach sentence/verb pair (s, v), we want to increase?csvk ?Ksv P (csvk ).
This expression treats the simplelabeling (consisting of a simple sentence and a roleassignment) as a hidden variable that is summed out.Taking the log, summing across all sentence/verbpairs, and adding L2 regularization on the weights,we have our final objective F (w):?s,v?
?log?csvk ?Ksv ewT fsvk?csvk?
?Csv ewT fsvk???
?wTw2?2We train our model by optimizing the objectiveusing standard methods, specifically BFGS.
Due tothe summation over the hidden variable representingthe choice of simple sentence (not observed in thetraining data), our objective is not convex.
Thus,we are not guaranteed to find a global optimum; inpractice we have gotten good results using the de-fault initialization of setting all weights to 0.Consider the derivative of the likelihood compo-nent with respect to a single weight wl:?csvk ?Ksvf svk (l)P (csvk )?csvk?
?KsvP (csvk?
)?
?csvk ?Csvf svk (l)P (csvk )where f svk (l) denotes the lth component of f svk .This formula is positive when the expected value ofthe lth feature is higher on the set of correct simplelabelings Ksv than on the set of all simple labelingsCsv.
Thus, the optimization procedure will tend tobe self-reinforcing, increasing the score of correctsimple labelings which already have a high score.7 Simplification Data StructureOur representation of the set of possible simplifi-cations of a sentence addresses two computationalbottlenecks.
The first is the need to repeatedly copylarge chunks of the sentence.
For example, if we aredepassivizing a sentence, we can avoid copying thesubject and object of the original sentence by simplyreferring back to them in the depassivized version.At worst, we only need to add one node for eachnumbered node in the transformation rule.
The sec-ond issue is the possible exponential blowup of thenumber of generated sentences.
Consider ?I wantto eat and I want to drink and I want to play and.
.
.
?
Each subsentence can be simplified, yieldingtwo possibilities for each subsentence.
The numberof simplifications of the entire sentence is then ex-ponential in the length of the sentence.
However,348ROOTSNP([Someone])VP VBD(gave)SNP(chance)VPVBD(was)NP(I)VBN(given)VPFigure 7: Data structure after applying the depassivizerule to ?I was given (a) chance.?
Circular nodes are OR-nodes, rectangular nodes are AND-nodes.we can store these simplifications compactly as a setof independent decisions, ?I {want to eat OR eat}and I {want to drink OR drink} and .
.
.
?Both issues can be addressed by representing theset of simplifications using an AND-OR tree, a gen-eral data structure also used to store parse forestssuch as those produced by a chart parser.
In our case,the AND nodes are similar to constituent nodes in aparse tree ?
each has a category (e.g.
NP) and (if itis a leaf) a word (e.g.
?chance?
), but instead of hav-ing a list of child constituents, it instead has a list ofchild OR nodes.
Each OR node has one or more con-stituent children that correspond to the different op-tions at this point in the tree.
Figure 7 shows the re-sulting AND-OR tree after applying the depassivizerule to the original parse of ?I was given a chance.
?Because this AND-OR tree represents only two dif-ferent parses, the original parse and the depassivizedversion, only one OR node in the tree has more thanone child ?
the root node, which has two choices,one for each parse.
However, the AND nodes imme-diately above ?I?
and ?chance?
each have more thanone OR-node parent, since they are shared by theoriginal and depassivized parses1.
To extract a parsefrom this data structure, we apply the following re-cursive algorithm: starting at the root OR node, eachtime we reach an OR node, we choose and recurseon exactly one of its children; each time we reachan AND node, we recurse on all of its children.
InFigure 7, we have only one choice: if we go left atthe root, we generate the original parse; otherwise,we generate the depassivized version.Unfortunately, it is difficult to find the optimalAND-OR tree.
We use a greedy but smart proce-1In this particular example, both of these nodes are leaves,but in general shared nodes can be entire tree fragmentsdure to try to produce a small tree.
We omit detailsfor lack of space.
Using our rule set, the compactrepresentation is usually (but not always) small.For our compact representation to be useful, weneed to be able to optimize our objective without ex-panding all possible simple sentences.
A relativelystraight-forward extension of the inside-outside al-gorithm for chart-parses allows us to learn and per-form inference in our compact representation (a sim-ilar algorithm is presented in (Geman & Johnson,2002)).
We omit details for lack of space.8 ExperimentsWe evaluated our system using the setup of the Conll2005 semantic role labeling task.2 Thus, we trainedon Sections 2-21 of PropBank and used Section 24as development data.
Our test data includes both theselected portion of Section 23 of PropBank, plus theextra data on the Brown corpus.
We used the Char-niak parses provided by the Conll distribution.We compared to a strong Baseline SRL systemthat learns a logistic regression model using the fea-tures of Pradhan et al (2005).
It has two stages.The first filters out nodes that are unlikely to be ar-guments.
The second stage labels each remainingnode either as a particular role (e.g.
?ARGO?)
or as anon-argument.
Note that the baseline feature set in-cludes a feature corresponding to the subcategoriza-tion of the verb (specifically, the sequence of nonter-minals which are children of the predicate?s parentnode).
Thus, Baseline does have access to some-thing similar to our model?s role pattern feature, al-though the Baseline subcategorization feature onlyincludes post-verbal modifiers and is generally muchnoisier because it operates on the original sentence.Our Transforms model takes as input the Char-niak parses supplied by the Conll release, and labelsevery node with Core arguments (ARG0-ARG5).Our rule set does not currently handle either ref-erent arguments (such as ?who?
in ?The man whoate .
.
.
?)
or non-core arguments (such as ARGM-TMP).
For these arguments, we simply filled in us-ing our baseline system (specifically, any non-coreargument which did not overlap an argument pre-dicted by our model was added to the labeling).Also, on some sentences, our system did not gen-erate any predictions because no valid simple sen-2http://www.lsi.upc.es/ srlconll/home.html349Model Dev Test Test TestWSJ Brown WSJ+BrBaseline 74.7 76.9 64.7 75.3Transforms 75.6 77.4 66.8 76.0Combined 76.0 78.0 66.4 76.5Punyakanok 77.35 79.44 67.75 77.92Table 2: F1 Measure using Charniak parsestences were produced by the simplification system .Again, we used the baseline to fill in predictions (forall arguments) for these sentences.Baseline and Transforms were regularized usinga Gaussian prior; for both models, ?2 = 1.0 gavethe best results on the development set.For generating role predictions from our model,we have two reasonable options: use the labelinggiven by the single highest scoring simple labeling;or compute the distribution over predictions for eachnode by summing over all simple labelings.
The lat-ter method worked slightly better, particularly whencombined with the baseline model as described be-low, so all reported results use this method.We also evaluated a hybrid model that combinesthe Baseline with our simplification model.
For agiven sentence/verb pair (s, v), we find the set ofconstituents N sv that made it past the first (filtering)stage of Baseline.
For each candidate simple sen-tence/labeling pair csvk = (tsvi , gvj ) proposed by ourmodel, we check to see which of the constituentsin N sv are already present in our simple sentencetsvi .
Any constituents that are not present are then as-signed a probability distribution over possible rolesaccording to Baseline.
Thus, we fall back Base-line whenever the current simple sentence does nothave an ?opinion?
about the role of a particular con-stituent.
The Combined model is thus able to cor-rectly label sentences when the simplification pro-cess drops some of the arguments (generally due tounusual syntax).
Each of the two components wastrained separately and combined only at testing time.Table 2 shows results of these three systems onthe Conll-2005 task, plus the top-performing system(Punyakanok et al, 2005) for reference.
Baseline al-ready achieves good performance on this task, plac-ing at about 75th percentile among evaluated sys-tems.
Our Transforms model outperforms Baselineon all sets.
Finally, our Combined model improvesover Transforms on all but the test Brown corpus,Model Test WSJBaseline 87.6Transforms 88.2Combined 88.5Table 3: F1 Measure using gold parsesachieving a statistically significant increase over theBaseline system (according to confidence intervalscalculated for the Conll-2005 results).The Combined model still does not achieve theperformance levels of the top several systems.
How-ever, these systems all use information from multi-ple parses, allowing them to fix many errors causedby incorrect parses.
We return to this issue in Sec-tion 10.
Indeed, as shown in Table 3, performanceon gold standard parses is (as expected) much bet-ter than on automatically generated parses, for allsystems.
Importantly, the Combined model againachieves a significant improvement over Baseline.We expect that by labeling simple sentences, ourmodel will generalize well even on verbs with asmall number of training examples.
Figure 8 showsF1 measure on theWSJ test set as a function of train-ing set size.
Indeed, both the Transformsmodel andthe Combined model significantly outperform theBaseline model when there are fewer than 20 train-ing examples for the verb.
While theBaselinemodelhas higher accuracy than the Transforms model forverbs with a very large number of training examples,theCombinedmodel is at or above both of the othermodels in all but the rightmost bucket, suggestingthat it gets the best of both worlds.We also found, as expected, that our model im-proved on sentences with very long parse paths.
Forexample, in the sentence ?Big investment banks re-fused to step up to the plate to support the beleaguredfloor traders by buying blocks of stock, traders say,?
theparse path from ?buy?
to its ARG0, ?Big investmentbanks,?
is quite long.
The Transforms model cor-rectly labels the arguments of ?buy?, while theBase-line system misses the ARG0.To understand the importance of different types ofrules, we performed an ablation analysis.
For eachmajor rule category in Figure 1, we deleted thoserules from the rule set, retrained, and evaluated us-ing the Combined model.
To avoid parse-relatedissues, we trained and evaluated on gold-standardparses.
Most important were rules relating to (ba-350F1 vs. Verb Training Examples0.60.650.70.750.80.850.90-4 5-9 10-1920-4950-99100-199200-499500-9991000-19992000-4999 5000+Training ExamplesF1MeasureCombinedTransformsBaselineFigure 8: F1 Measure on the WSJ test set as a function oftraining set size.
Each bucket on the X-axis correspondsto a group of verbs for which the number of training ex-amples fell into the appropriate range; the value is theaverage performance for verbs in that bucket.sic) verb raising/control, ?make?
rewrites, modifiednouns, and passive constructions.
Each of these rulecategories when removed lowered the F1 score byapproximately .4%.
In constrast, removing rulesfor non-basic control, possessives, and inverted sen-tences caused a negligible reduction in performance.This may be because the relevant syntactic structuresoccur rarely; because Baseline does well on thoseconstructs; or because the simplification model hastrouble learning when to apply these rules.9 Related WorkOne area of current research which has similaritieswith this work is on Lexical Functional Grammars(LFGs).
Both approaches attempt to abstract awayfrom the surface level syntax of the sentence (e.g.,the XLE system3).
The most obvious difference be-tween the approaches is that we use SRL data to trainour system, avoiding the need to have labeled dataspecific to our simplification scheme.There have been a number of works which modelverb subcategorization.
Approaches include incor-porating a subcategorization feature (Gildea & Ju-rafsky, 2002; Xue & Palmer, 2004), such as the oneused in our baseline; and building a model whichjointly classifies all arguments of a verb (Toutanovaet al, 2005).
Our method differs from past work inthat it extracts its role pattern feature from the sim-plified sentence.
As a result, the feature is less noisy3http://www2.parc.com/isl/groups/nltt/xle/and generalizes better across syntactic variation thana feature extracted from the original sentence.Another group of related work focuses on summa-rizing sentences through a series of deletions (Jing,2000; Dorr et al, 2003; Galley & McKeown, 2007).In particular, the latter two works iteratively simplifythe sentence by deleting a phrase at a time.
We differfrom these works in several important ways.
First,our transformation language is not context-free; itcan reorder constituents and then apply transforma-tion rules to the reordered sentence.
Second, we arefocusing on a somewhat different task; these worksare interested in obtaining a single summary of eachsentence which maintains all ?essential?
informa-tion, while in our work we produce a simplificationthat may lose semantic content, but aims to containall arguments of a verb.
Finally, training our modelon SRL data allows us to avoid the relative scarcityof parallel simplification corpora and the issue of de-termining what is ?essential?
in a sentence.Another area of related work in the semantic rolelabeling literature is that on tree kernels (Moschitti,2004; Zhang et al, 2007).
Like our method, tree ker-nels decompose the parse path into smaller piecesfor classification.
Our model can generalize betteracross verbs because it first simplifies, then classifiesbased on the simplified sentence.
Also, through it-erative simplifications we can discover structure thatis not immediately apparent in the original parse.10 Future WorkThere are a number of improvements that could bemade to the current simplification system, includ-ing augmenting the rule set to handle more con-structions and doing further sentence normaliza-tions, e.g., identifying whether a direct object exists.Another interesting extension involves incorporatingparser uncertainty into the model; in particular, oursimplification system is capable of seamlessly ac-cepting a parse forest as input.There are a variety of other tasks for which sen-tence simplification might be useful, including sum-marization, information retrieval, information ex-traction, machine translation and semantic entail-ment.
In each area, we could either use the sim-plification system as learned on SRL data, or retrainthe simplification model to maximize performanceon the particular task.351ReferencesDorr, B., Zajic, D., & Schwartz, R. (2003).
Hedge:A parse-and-trim approach to headline genera-tion.
Proceedings of the HLT-NAACL Text Sum-marization Workshop and Document Understand-ing Conference.Galley, M., & McKeown, K. (2007).
Lexicalizedmarkov grammars for sentence compression.
Pro-ceedings of NAACL-HLT.Geman, S., & Johnson, M. (2002).
Dynamic pro-gramming for parsing and estimation of stochasticunification-based grammars.
Proceedings of ACL.Gildea, D., & Jurafsky, D. (2002).
Automatic label-ing of semantic roles.
Computational Linguistics.Jing, H. (2000).
Sentence reduction for automatictext summarization.
Proceedings of Applied NLP.Moschitti, A.
(2004).
A study on convolution ker-nels for shallow semantic parsing.
Proceedings ofACL.Pradhan, S., Hacioglu, K., Krugler, V., Ward, W.,Martin, J. H., & Jurafsky, D. (2005).
Support vec-tor learning for semantic argument classification.Machine Learning, 60, 11?39.Punyakanok, V., Koomen, P., Roth, D., & Yih, W.(2005).
Generalized inference with multiple se-mantic role labeling systems.
Proceedings ofCoNLL.Toutanova, K., Haghighi, A., &Manning, C. (2005).Joint learning improves semantic role labeling.Proceedings of ACL, 589?596.Xue, N., & Palmer, M. (2004).
Calibrating fea-tures for semantic role labeling.
Proceedings ofEMNLP.Zhang, M., Che, W., Aw, A., Tan, C. L., Zhou, G.,Liu, T., & Li, S. (2007).
A grammar-driven convo-lution tree kernel for semantic role classification.Proceedings of ACL.352
