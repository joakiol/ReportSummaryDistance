Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 615?620,October 25-29, 2014, Doha, Qatar.c?2014 Association for Computational LinguisticsQuestion Answering with Subgraph EmbeddingsAntoine BordesFacebook AI Research112 avenue de Wagram,Paris, Franceabordes@fb.comSumit ChopraFacebook AI Research770 Broadway,New York, USAspchopra@fb.comJason WestonFacebook AI Research770 Broadway,New York, USAjase@fb.comAbstractThis paper presents a system which learnsto answer questions on a broad range oftopics from a knowledge base using fewhand-crafted features.
Our model learnslow-dimensional embeddings of wordsand knowledge base constituents; theserepresentations are used to score naturallanguage questions against candidate an-swers.
Training our system using pairs ofquestions and structured representations oftheir answers, and pairs of question para-phrases, yields competitive results on a re-cent benchmark of the literature.1 IntroductionTeaching machines how to automatically answerquestions asked in natural language on any topicor in any domain has always been a long stand-ing goal in Artificial Intelligence.
With the riseof large scale structured knowledge bases (KBs),this problem, known as open-domain question an-swering (or open QA), boils down to being ableto query efficiently such databases with naturallanguage.
These KBs, such as FREEBASE (Bol-lacker et al., 2008) encompass huge ever growingamounts of information and ease open QA by or-ganizing a great variety of answers in a structuredformat.
However, the scale and the difficulty formachines to interpret natural language still makesthis task a challenging problem.The state-of-the-art techniques in open QA canbe classified into two main classes, namely, infor-mation retrieval based and semantic parsing based.Information retrieval systems first retrieve a broadset of candidate answers by querying the searchAPI of KBs with a transformation of the ques-tion into a valid query and then use fine-graineddetection heuristics to identify the exact answer(Kolomiyets and Moens, 2011; Unger et al., 2012;Yao and Van Durme, 2014).
On the other hand,semantic parsing methods focus on the correct in-terpretation of the meaning of a question by a se-mantic parsing system.
A correct interpretationconverts a question into the exact database querythat returns the correct answer.
Interestingly, re-cent works (Berant et al., 2013; Kwiatkowski etal., 2013; Berant and Liang, 2014; Fader et al.,2014) have shown that such systems can be ef-ficiently trained under indirect and imperfect su-pervision and hence scale to large-scale regimes,while bypassing most of the annotation costs.Yet, even if both kinds of system have shown theability to handle large-scale KBs, they still requireexperts to hand-craft lexicons, grammars, and KBschema to be effective.
This non-negligible hu-man intervention might not be generic enough toconveniently scale up to new databases with otherschema, broader vocabularies or languages otherthan English.
In contrast, (Fader et al., 2013) pro-posed a framework for open QA requiring almostno human annotation.
Despite being an interestingapproach, this method is outperformed by othercompeting methods.
(Bordes et al., 2014b) in-troduced an embedding model, which learns low-dimensional vector representations of words andsymbols (such as KBs constituents) and can betrained with even less supervision than the systemof (Fader et al., 2013) while being able to achievebetter prediction performance.
However, this ap-proach is only compared with (Fader et al., 2013)which operates in a simplified setting and has notbeen applied in more realistic conditions nor eval-uated against the best performing methods.In this paper, we improve the model of (Bor-des et al., 2014b) by providing the ability to an-swer more complicated questions.
The main con-tributions of the paper are: (1) a more sophisti-cated inference procedure that is both efficient andcan consider longer paths ((Bordes et al., 2014b)considered only answers directly connected to the615question in the graph); and (2) a richer represen-tation of the answers which encodes the question-answer path and surrounding subgraph of the KB.Our approach is competitive with the current state-of-the-art on the recent benchmark WEBQUES-TIONS (Berant et al., 2013) without using any lex-icon, rules or additional system for part-of-speechtagging, syntactic or dependency parsing duringtraining as most other systems do.2 Task DefinitionOur main motivation is to provide a system foropen QA able to be trained as long as it has ac-cess to: (1) a training set of questions paired withanswers and (2) a KB providing a structure amonganswers.
We suppose that all potential answers areentities in the KB and that questions are sequencesof words that include one identified KB entity.When this entity is not given, plain string match-ing is used to perform entity resolution.
Smartermethods could be used but this is not our focus.We use WEBQUESTIONS (Berant et al., 2013)as our evaluation bemchmark.
Since it containsfew training samples, it is impossible to learn onit alone, and this section describes the various datasources that were used for training.
These are sim-ilar to those used in (Berant and Liang, 2014).WebQuestions This dataset is built using FREE-BASE as the KB and contains 5,810 question-answer pairs.
It was created by crawling questionsthrough the Google Suggest API, and then obtain-ing answers using Amazon Mechanical Turk.
Weused the original split (3,778 examples for train-ing and 2,032 for testing), and isolated 1k ques-tions from the training set for validation.
WE-BQUESTIONS is built on FREEBASE since all an-swers are defined as FREEBASE entities.
In eachquestion, we identified one FREEBASE entity us-ing string matching between words of the ques-tion and entity names in FREEBASE.
When thesame string matches multiple entities, only the en-tity appearing in most triples, i.e.
the most popularin FREEBASE, was kept.
Example questions (an-swers) in the dataset include ?Where did EdgarAllan Poe died??
(baltimore) or ?What degreesdid Barack Obama get??
(bachelor of arts,juris doctor).Freebase FREEBASE (Bollacker et al., 2008)is a huge and freely available database ofgeneral facts; data is organized as triplets(subject, type1.type2.predicate, object),where two entities subject and object (identi-fied by mids) are connected by the relation typetype1.type2.predicate.
We used a subset, cre-ated by only keeping triples where one of theentities was appearing in either the WEBQUES-TIONS training/validation set or in CLUEWEB ex-tractions.
We also removed all entities appearingless than 5 times and finally obtained a FREEBASEset containing 14M triples made of 2.2M entitiesand 7k relation types.1Since the format of triplesdoes not correspond to any structure one couldfind in language, we decided to transform theminto automatically generated questions.
Hence, alltriples were converted into questions ?What is thepredicate of the type2 subject??
(using themid of the subject) with the answer being object.An example is ?What is the nationality of theperson barack obama??
(united states).
Moreexamples and details are given in a longer versionof this paper (Bordes et al., 2014a).ClueWeb Extractions FREEBASE data allowsto train our model on 14M questions but these havea fixed lexicon and vocabulary, which is not real-istic.
Following (Berant et al., 2013), we also cre-ated questions using CLUEWEB extractions pro-vided by (Lin et al., 2012).
Using string match-ing, we ended up with 2M extractions structuredas (subject, ?text string?, object) with bothsubject and object linked to FREEBASE.
Wealso converted these triples into questions by usingsimple patterns and FREEBASE types.
An exam-ple of generated question is ?Where barack obamawas allegedly bear in??
(hawaii).Paraphrases The automatically generated ques-tions that are useful to connect FREEBASE triplesand natural language, do not provide a satisfac-tory modeling of natural language because of theirsemi-automatic wording and rigid syntax.
Toovercome this issue, we follow (Fader et al., 2013)and supplement our training data with an indirectsupervision signal made of pairs of question para-phrases collected from the WIKIANSWERS web-site.
On WIKIANSWERS, users can tag pairs ofquestions as rephrasings of each other: (Fader etal., 2013) harvested a set of 2M distinct questionsfrom WIKIANSWERS, which were grouped into350k paraphrase clusters.1WEBQUESTIONS contains ?2k entities, hence restrict-ing FREEBASE to 2.2M entities does not ease the task for us.6163 Embedding Questions and AnswersInspired by (Bordes et al., 2014b), our modelworks by learning low-dimensional vector embed-dings of words appearing in questions and of enti-ties and relation types of FREEBASE, so that repre-sentations of questions and of their correspondinganswers are close to each other in the joint embed-ding space.
Let q denote a question and a a can-didate answer.
Learning embeddings is achievedby learning a scoring function S(q, a), so that Sgenerates a high score if a is the correct answer tothe question q, and a low score otherwise.
Notethat both q and a are represented as a combina-tion of the embeddings of their individual wordsand/or symbols; hence, learning S essentially in-volves learning these embeddings.
In our model,the form of the scoring function is:S(q, a) = f(q)>g(a).
(1)Let W be a matrix of Rk?N, where k is the di-mension of the embedding space which is fixed a-priori, andN is the dictionary of embeddings to belearned.
LetNWdenote the total number of wordsand NSthe total number of entities and relationtypes.
WithN = NW+NS, the i-th column ofWis the embedding of the i-th element (word, entityor relation type) in the dictionary.
The functionf(.
), which maps the questions into the embed-ding spaceRkis defined as f(q) =W?
(q), where?
(q) ?
NN, is a sparse vector indicating the num-ber of times each word appears in the question q(usually 0 or 1).
Likewise the function g(.)
whichmaps the answer into the same embedding spaceRkas the questions, is given by g(a) = W?
(a).Here ?
(a) ?
NNis a sparse vector representationof the answer a, which we now detail.3.1 Representing Candidate AnswersWe now describe possible feature representationsfor a single candidate answer.
(When there aremultiple correct answers, we average these rep-resentations, see Section 3.4.)
We consider threedifferent types of representation, corresponding todifferent subgraphs of FREEBASE around it.
(i) Single Entity.
The answer is represented asa single entity from FREEBASE: ?
(a) is a 1-of-NScoded vector with 1 corresponding tothe entity of the answer, and 0 elsewhere.
(ii) Path Representation.
The answer isrepresented as a path from the entitymentioned in the question to the answerentity.
In our experiments, we consid-ered 1- or 2-hops paths (i.e.
with either1 or 2 edges to traverse): (barack obama,people.person.place of birth, honolulu)is a 1-hop path and (barack obama,people.person.place of birth, location.location.containedby, hawaii) a 2-hopspath.
This results in a ?
(a) which is a3-of-NSor 4-of-NScoded vector, expressingthe start and end entities of the path and therelation types (but not entities) in-between.
(iii) Subgraph Representation.
We encode boththe path representation from (ii), and the en-tire subgraph of entities connected to the can-didate answer entity.
That is, for each entityconnected to the answer we include both therelation type and the entity itself in the repre-sentation ?(a).
In order to represent the an-swer path differently to the surrounding sub-graph (so the model can differentiate them),we double the dictionary size for entities, anduse one embedding representation if they arein the path and another if they are in the sub-graph.
Thus we now learn a parameter matrixRk?Nwhere N = NW+ 2NS(NSis the to-tal number of entities and relation types).
Ifthere areC connected entities withD relationtypes to the candidate answer, its representa-tion is a 3+C+D or 4+C+D-of-NScodedvector, depending on the path length.Our hypothesis is that including more informa-tion about the answer in its representation will leadto improved results.
While it is possible that allrequired information could be encoded in the k di-mensional embedding of the single entity (i), it isunclear what dimension k should be to make thispossible.
For example the embedding of a countryentity encoding all of its citizens seems unrealis-tic.
Similarly, only having access to the path ig-nores all the other information we have about theanswer entity, unless it is encoded in the embed-dings of either the entity of the question, the an-swer or the relations linking them, which might bequite complicated as well.
We thus adopt the sub-graph approach.
Figure 1 illustrates our model.3.2 Training and Loss FunctionAs in (Weston et al., 2010), we train our modelusing a margin-based ranking loss function.
LetD = {(qi, ai) : i = 1, .
.
.
, |D|} be the training set617?Who did Clooney marry in 1987??Embedding?matrix?W?G.
Clooney K. Preston1987J.
TravoltaModelHonoluluDetec?on?of?Freebase?en?ty?in?the?ques?on?Embedding modelFreebase subgraphBinary?encoding?of?the?subgraph??(a)?Embedding?of?the?subgraph?g(a)?Binary?encoding?of?the?ques?on??(q)?Embedding?of?the?ques?n?
f(q)?Ques?n?q?Subgraph?of?a?candidate?answer?a?(here?K.
?Preston)?Score S(q,a) How?the?candidate?answer?fits?the?ques?on?Dot?product?
Embedding?matrix?W?Figure 1: Illustration of the subgraph embedding model scoring a candidate answer: (i) locate entity inthe question; (ii) compute path from entity to answer; (iii) represent answer as path plus all connectedentities to the answer (the subgraph); (iv) embed both the question and the answer subgraph separatelyusing the learnt embedding vectors, and score the match via their dot product.of questions qipaired with their correct answer ai.The loss function we minimize is|D|?i=1?a??
?A(ai)max{0,m?S(qi, ai)+S(qi, a?
)}, (2)where m is the margin (fixed to 0.1).
MinimizingEq.
(2) learns the embedding matrix W so thatthe score of a question paired with a correct an-swer is greater than with any incorrect answer a?by at least m. a?
is sampled from a set of incor-rect candidates?A.
This is achieved by sampling50% of the time from the set of entities connectedto the entity of the question (i.e.
other candidatepaths), and by replacing the answer entity by a ran-dom one otherwise.
Optimization is accomplishedusing stochastic gradient descent, multi-threadedwith Hogwild!
(Recht et al., 2011), with the con-straint that the columns wiof W remain withinthe unit-ball, i.e., ?i, ||wi||2?
1.3.3 Multitask Training of EmbeddingsSince a large number of questions in our trainingdatasets are synthetically generated, they do notadequately cover the range of syntax used in natu-ral language.
Hence, we also multi-task the train-ing of our model with the task of paraphrase pre-diction.
We do so by alternating the training ofS with that of a scoring function Sprp(q1, q2) =f(q1)>f(q2), which uses the same embedding ma-trix W and makes the embeddings of a pair ofquestions (q1, q2) similar to each other if they areparaphrases (i.e.
if they belong to the same para-phrase cluster), and make them different other-wise.
Training Sprpis similar to that of S exceptthat negative samples are obtained by sampling aquestion from another paraphrase cluster.We also multitask the training of the embed-dings with the mapping of the mids of FREEBASEentities to the actual words of their names, so thatthe model learns that the embedding of the mid ofan entity should be similar to the embedding of theword(s) that compose its name(s).3.4 InferenceOnceW is trained, at test time, for a given ques-tion q the model predicts the answer with:a?
= argmaxa?
?A(q)S(q, a?)
(3)where A(q) is the candidate answer set.
This can-didate set could be the whole KB but this has bothspeed and potentially precision issues.
Instead, wecreate a candidate set A(q) for each question.We recall that each question contains one identi-fied FREEBASE entity.
A(q) is first populated withall triples from FREEBASE involving this entity.This allows to answer simple factual questionswhose answers are directly connected to them (i.e.1-hop paths).
This strategy is denoted C1.Since a system able to answer only such ques-tions would be limited, we supplement A(q) withexamples situated in the KB graph at 2-hops fromthe entity of the question.
We do not add all suchquadruplets since this would lead to very largecandidate sets.
Instead, we consider the follow-ing general approach: given that we are predictinga path, we can predict its elements in turn using618Method P@1 F1 F1(%) (Berant) (Yao)Baselines(Berant et al., 2013) ?
31.4 ?
(Bordes et al., 2014b) 31.3 29.7 31.8(Yao and Van Durme, 2014) ?
33.0 42.0(Berant and Liang, 2014) ?
39.9 43.0Our approachSubgraph & A(q) = C240.4 39.2 43.2Ensemble with (Berant & Liang, 14) ?
41.8 45.7VariantsWithout multiple predictions 40.4 31.3 34.2Subgraph & A(q) = All 2-hops 38.0 37.1 41.4Subgraph & A(q) = C134.0 32.6 35.1Path & A(q) = C236.2 35.3 38.5Single Entity & A(q) = C125.8 16.0 17.8Table 1: Results on the WEBQUESTIONS test set.a beam search, and hence avoid scoring all can-didates.
Specifically, our model first ranks rela-tion types using Eq.
(1), i.e.
selects which rela-tion types are the most likely to be expressed inq.
We keep the top 10 types (10 was selected onthe validation set) and only add 2-hops candidatesto A(q) when these relations appear in their path.Scores of 1-hop triples are weighted by 1.5 sincethey have one less element than 2-hops quadru-plets.
This strategy, denotedC2, is used by default.A prediction a?can commonly actually bea set of candidate answers, not just one an-swer, for example for questions like ?Who areDavid Beckham?s children??.
This is achievedby considering a prediction to be all the en-tities that lie on the same 1-hop or 2-hopspath from the entity found in the question.Hence, all answers to the above question areconnected to david beckham via the same path(david beckham, people.person.children,*).The feature representation of the prediction is thenthe average over each candidate entity?s features(see Section 3.1), i.e.
?all(a?)
=1|a?|?a?j:a??
(a?j)where a?jare the individual entities in the over-all prediction a?.
In the results, we compare to abaseline method that can only predict single can-didates, which understandly performs poorly.4 ExperimentsWe compare our system in terms of F1 score ascomputed by the official evaluation script2(F1(Berant)) but also with a slightly different F1 def-inition, termed F1 (Yao) which was used in (Yaoand Van Durme, 2014) (the difference being theway that questions with no answers are dealt with),2Available from www-nlp.stanford.edu/software/sempre/and precision @ 1 (p@1) of the first candidate en-tity (even when there are a set of correct answers),comparing to recently published systems.3Theupper part of Table 1 indicates that our approachoutperforms (Yao and Van Durme, 2014), (Berantet al., 2013) and (Bordes et al., 2014b), and per-forms similarly as (Berant and Liang, 2014).The lower part of Table 1 compares various ver-sions of our model.
Our default approach usesthe Subgraph representation for answers and C2as the candidate answers set.
Replacing C2byC1induces a large drop in performance becausemany questions do not have answers thatare di-rectly connected to their inluded entity (not inC1).
However, using all 2-hops connections asa candidate set is also detrimental, because thelarger number of candidates confuses (and slowsa lot) our ranking based inference.
Our resultsalso verify our hypothesis of Section 3.1, that aricher representation for answers (using the localsubgraph) can store more pertinent information.Finally, we demonstrate that we greatly improveupon the model of (Bordes et al., 2014b), whichactually corresponds to a setting with the Path rep-resentation and C1as candidate set.We also considered an ensemble of our ap-proach and that of (Berant and Liang, 2014).
Aswe only had access to their test predictions weused the following combination method.
Our ap-proach gives a score S(q, a) for the answer it pre-dicts.
We chose a threshold such that our approachpredicts 50% of the time (when S(q, a) is aboveits value), and the other 50% of the time we usethe prediction of (Berant and Liang, 2014) instead.We aimed for a 50/50 ratio because both meth-ods perform similarly.
The ensemble improves thestate-of-the-art, and indicates that our models aresignificantly different in their design.5 ConclusionThis paper presented an embedding model thatlearns to perform open QA using training datamade of questions paired with their answers andof a KB to provide a structure among answers, andcan achieve promising performance on the com-petitive benchmark WEBQUESTIONS.3Results of baselines except (Bordes et al., 2014b) havebeen extracted from the original papers.
For our experiments,all hyperparameters have been selected on the WEBQUES-TIONS validation set: k was chosen among {64, 128, 256},the learning rate on a log.
scale between 10?4and 10?1andwe used at most 100 paths in the subgraph representation.619ReferencesJonathan Berant and Percy Liang.
2014.
Semanticparsing via paraphrasing.
In Proceedings of the52nd Annual Meeting of the Association for Com-putational Linguistics (ACL?14), Baltimore, USA.Jonathan Berant, Andrew Chou, Roy Frostig, and PercyLiang.
2013.
Semantic parsing on Freebase fromquestion-answer pairs.
In Proceedings of the 2013Conference on Empirical Methods in Natural Lan-guage Processing (EMNLP?13), Seattle, USA.Kurt Bollacker, Colin Evans, Praveen Paritosh, TimSturge, and Jamie Taylor.
2008.
Freebase: a col-laboratively created graph database for structuringhuman knowledge.
In Proceedings of the 2008 ACMSIGMOD international conference on Managementof data, Vancouver, Canada.
ACM.Antoine Bordes, Sumit Chopra, and Jason Weston.2014a.
Question answering with subgraph embed-dings.
CoRR, abs/1406.3676.Antoine Bordes, Jason Weston, and Nicolas Usunier.2014b.
Open question answering with weakly su-pervised embedding models.
In Proceedings of the7th European Conference on Machine Learning andPrinciples and Practice of Knowledge Discoveryin Databases (ECML-PKDD?14), Nancy, France.Springer.Anthony Fader, Luke Zettlemoyer, and Oren Etzioni.2013.
Paraphrase-driven learning for open questionanswering.
In Proceedings of the 51st Annual Meet-ing of the Association for Computational Linguistics(ACL?13), Sofia, Bulgaria.Anthony Fader, Luke Zettlemoyer, and Oren Etzioni.2014.
Open question answering over curated andextracted knowledge bases.
In Proceedings of 20thSIGKDD Conference on Knowledge Discovery andData Mining (KDD?14), New York City, USA.ACM.Oleksandr Kolomiyets and Marie-Francine Moens.2011.
A survey on question answering technologyfrom an information retrieval perspective.
Informa-tion Sciences, 181(24):5412?5434.Tom Kwiatkowski, Eunsol Choi, Yoav Artzi, and LukeZettlemoyer.
2013.
Scaling semantic parsers withon-the-fly ontology matching.
In Proceedings of the2013 Conference on Empirical Methods in NaturalLanguage Processing (EMNLP?13), Seattle, USA,October.Thomas Lin, Mausam, and Oren Etzioni.
2012.
En-tity linking at web scale.
In Proceedings of the JointWorkshop on Automatic Knowledge Base Construc-tion and Web-scale Knowledge Extraction (AKBC-WEKEX?12), Montreal, Canada.Benjamin Recht, Christopher R?e, Stephen J Wright,and Feng Niu.
2011.
Hogwild!
: A lock-free ap-proach to parallelizing stochastic gradient descent.In Advances in Neural Information Processing Sys-tems (NIPS 24)., Vancouver, Canada.Christina Unger, Lorenz B?uhmann, Jens Lehmann,Axel-Cyrille Ngonga Ngomo, Daniel Gerber, andPhilipp Cimiano.
2012.
Template-based questionanswering over RDF data.
In Proceedings of the21st international conference on World Wide Web(WWW?12), Lyon, France.
ACM.Jason Weston, Samy Bengio, and Nicolas Usunier.2010.
Large scale image annotation: learning torank with joint word-image embeddings.
Machinelearning, 81(1).Xuchen Yao and Benjamin Van Durme.
2014.
Infor-mation extraction over structured data: Question an-swering with freebase.
In Proceedings of the 52ndAnnual Meeting of the Association for Computa-tional Linguistics (ACL?14), Baltimore, USA.620
