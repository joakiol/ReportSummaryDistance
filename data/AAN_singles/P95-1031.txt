Bayesian Grammar Induction for Language ModelingStan ley  F .
ChenAiken Computat ion  LaboratoryDivision of Appl ied SciencesHarvard Univers i tyCambridge,  MA 02138sfc@das, harvard, eduAbst rac tWe describe a corpus-based induction algo-rithm for probabilistic ontext-free gram-mars.
The algorithm employs a greedyheuristic search within a Bayesian frame-work, and a post-pass using the Inside-Outside algorithm.
We compare the per-formance of our algorithm to n-gram mo-dels and the Inside-Outside algorithm inthree language modeling tasks.
In two ofthe tasks, the training data is generated bya probabilistic context-free grammar and inboth tasks our algorithm outperforms theother techniques.
The third task involvesnaturally-occurring data, and in this taskour algorithm does not perform as well asn-gram models but vastly outperforms theInside-Outside algorithm.1 In t roduct ionIn applications uch as speech recognition, hand-writing recognition, and spelling correction, perfor-mance is limited by the quality of the language mo-del utilized (7; 7; 7; 7).
However, static languagemodeling performance has remained basically un-changed since the advent of n-gram language mo-dels forty years ago (7).
Yet, n-gram language mo-dels can only capture dependencies within an n-word window, where currently the largest practicaln for natural language is three, and many dependen-cies in natural anguage occur beyond a three-wordwindow.
In addition, n-gram models are extremelylarge, thus making them difficult to implement effi-ciently in memory-constrained applications.An appealing alternative is grammar-based lan-guage models.
Language models expressed as a pro-babilistic grammar tend to be more compact hann-gram language models, and have the ability to mo-del long-distance dependencies (7; 7; 7).
However,to date there has been little success in constructinggrammar-based language models competitive withn-gram models in problems of any magnitude.In this paper, we describe a corpus-based indue-tion algorithm for probabilistic ontext-free gram-mars that outperforms n-gram models and theInside-Outside algorithm (7) in medium-sized do-mains.
This result marks the first time a grammar-based language model has surpassed n-gram mode-ling in a task of at least moderate size.
The al-gorithm employs a greedy heuristic search withina Bayesian framework, and a post-pass using theInside-Outside algorithm.2 Grammar  Induct ion  as SearchGrammar induction can be framed as a search pro-blem, and has been framed as such almost withoutexception in past research (7).
The search space istaken to be some class of grammars; for example, inour work we search within the space of probabilisticcontext-free grammars.
The objective function is ta-ken to be some measure dependent on the trainingdata; one generally wants to find a grammar that insome sense accurately models the training data.Most work in language modeling, including n-gram models and the Inside-Outside algorithm, fallsunder the maximum-likelihood paradigm, where onetakes the objective function to be the likelihood ofthe training data given the grammar.
However, theoptimal grammar under this objective function isone which generates only strings in the training dataand no other strings.
Such grammars are poor lan-guage models, as they overfit the training data anddo not model the language at large.
In n-gram mo-dels and the Inside-Outside algorithm, this issue isevaded by bounding the size and form of the gram-mars considered, so that the "optimal" grammarcannot be expressed.
However, in our work we donot wish to limit the size of the grammars conside-red.The basic shortcoming ofthe maximum-likelihoodobjective function is that it does not encompass thecompelling intuition behind Occam's Razor, thatsimpler (or smaller) grammars are preferable overcomplex (or larger) grammars.
A factor in the ob-jective function that favors smaller grammars over228s --, sx  ( l -e )s x (,)X ~ A (p(A))Aa ---* a (1)VAeN-{S ,X}VaETN = the set of all nonterminal symbolsT = the set of all terminal symbolsProbabilities for each rule are in parentheses.Table 1: Initial hypothesis grammarlarge can prevent the objective function from pre-ferring grammars that overfit the training data.
?
)presents a Bayesian grammar induction frameworkthat includes such a factor in a motivated manner.The goM of grammar induction is taken to be fin-ding the grammar with the largest a posteriori pro-bability given the training data, that is, finding thegrammar G ~ wherec '  = arg m xp(GIo)and where we denote the training data as O, for ob-servations.
As it is unclear how to estimate p(GIO)directly, we apply Bayes' Rule and geta I = arg p(O la)p(a)  p(o)  = arg% xp(O\[a)p(a)Hence, we can frame the search for G ~ as a searchwith the objective function p(OIG)p(G), the likeli-hood of the training data multiplied by the priorprobabil ity of the grammar.We satisfy the goal of favoring smaller grammarsby choosing a prior that assigns higher probabilitiesto such grammars.
In particular, Solomonoff propo-ses the use of the universal a priori probability (?
),which is closely related to the minimum descriptionlength principle later proposed by (?).
In the caseof grammatical  language modeling, this correspondsto takingp(G) = 2 -t(a)where l(G) is the length of the description of thegrammar in bits.
The universal a priori probabi-lity has many elegant properties, the most salientof which is that it dominates all other enumerableprobability distributions multiplicativelyJ3 Search  A lgor i thmAs described above, we take grammar induction tobe the search for the grammar G ~ that optimizes theobjective function p(OlG)p(G ).
While this frame-work does not restrict us to a particular grammarformalism, in our work we consider only probabili-stic context-free grammars.1A very thorough discussion of the universal a prioriprobability is given by 7).We assume a simple greedy search strategy.
Wemaintain a single hypothesis grammar which is in-itialized to a small, trivial grammar.
We then try tofind a modification to the hypothesis grammar, suchas the addition of a grammar ule, that results in agrammar with a higher score on the objective func-tion.
When we find a superior grammar,  we makethis the new hypothesis grammar.
We repeat thisprocess until we can no longer find a modificationthat improves the current hypothesis grammar.For our initial grammar, we choose a grammarthat can generate any string, to assure that thegrammar can cover the training data.
The initialgrammar is listed in Table ??.
The sentential symbolS expands to a sequence of X's ,  where X expandsto every other nonterminal symbol in the grammar.Initially, the set of nonterminal symbols consists ofa different nonterminal symbol expanding to eachterminal symbol.Notice that this grammar models a sentence asa sequence of independently generated nonterminalsymbols.
We maintain this property throughout thesearch process, that is, for every symbol A ~ that weadd to the grammar,  we also add a rule X ---+ A I.This assures that the sentential symbol can expandto every symbol; otherwise, adding a symbol will notaffect the probabilities that the grammar assigns tostrings.We use the term move set to describe the set ofmodifications we consider to the current hypothesisgrammar to hopefully produce a superior grammar.Our move set includes the following moves:Move  1: Create a rule of the form A ---* BCMove 2: Create a rule of the form A --+ BICFor any context-free grammar,  it is possible to ex-press a weakly equivalent grammar using only rulesof these forms.
As mentioned before, with each newsymbol A we also create a rule X ---* A.3.1 Evaluating the Objective FunctionConsider the task of calculating the objective func-tion p(OIG)p(G ) for some grammar G. Calculating229SS XAslowlyi i iX A=az~s slowlyI \]A Ma,-y talksiMarySS X A,towtv\[ I $X Ataak, slowlyI iABo, talksiBobFigure 1: Initial Viterbi ParseSs / / " 'x  si i IX B XAMary Atatks Astowty ABobI i t iMary talks slowly BobSXiBAtatk, AjtowtyI Italks slowlyFigure 2: Predicted Viterbi Parsep(G) = 2 -/(G) is inexpensive2; however, calculatingp(OIG) requires a parsing of the entire training data.We cannot afford to parse the training data for eachgrammar considered; indeed, to ever be practical fordata sets of millions of words, it seems likely that wecan only afford to parse the data once.To achieve this goal, we employ several approxi-mations.
First, notice that we do not ever need tocalculate the actual value of the objective function;we need only to be able to distinguish when a moveapplied to the current hypothesis grammar producesa grammar  that has a higher score on the objectivefunction, that is, we need only to be able to calcu-late the difference in the objective function resultingfrom a move.
This can be done efficiently if we canquickly approximate how the probabil ity of the trai-ning data changes when a move is applied.To make this possible, we approximate the proba-bility of the training data p(OIG ) by the probabil ityof the single most probable parse, or Viterbi parse,of the training data.
Furthermore, instead of recal-culating the Viterbi parse of the training data fromscratch when a move is applied, we use heuristics topredict how a move will change the Viterbi parse.For example, consider the case where the trainingdata consists of the two sentencesO = {Bob talks slowly, Mary talks slowly}~Due to space limitations, we do not specify our me-thod for encoding rammars, i.e., how we calculate l(G)for a given G. However, this will be described in theauthor's forthcoming Ph.D. dissertation.In Figure ?
?, we display the Viterbi parse of thisdata under the initial hypothesis grammar used inour algorithm.Now, let us consider the move of adding the ruleB ---* Atalks Aslo~tyto the initial grammar (as well as the concomitantrule X ---* B).
A reasonable heuristic for predic-ting how the Viterbi parse will change is to replaceadjacent X 's  that expand to Atazk, and A~zo~,ty re-spectively with a single X that expands to B, asdisplayed in Figure ??.
This is the actual heuristicwe use for moves of the form A ---* BC, and we haveanalogous heuristics for each move in our move set.By predicting the differences in the Viterbi parse re-sulting from a move, we can quickly estimate thechange in the probabil ity of the training data.Notice that our predicted Viterbi parse can straya great deal from the actual Viterbi parse, as errorscan accumulate as move after move is applied.
Tominimize these effects, we process the training dataincrementally.
Using our initial hypothesis gram-mar, we parse the first sentence of the training dataand search for the optimal grammar  over just thatone sentence using the described search framework.We use the resulting grammar to parse the secondsentence, and then search for the optimal grammarover the first two sentences using the last grammaras the starting point.
We repeat this process, par-sing the next sentence using the best grammar  foundon the previous entences and then searching for the230best grammar taking into account his new sentence,until the entire training corpus is covered.Delaying the parsing of a sentence until all of theprevious sentences are processed should yield moreaccurate Viterbi parses during the search processthan if we simply parse the whole corpus with theinitial hypothesis grammar.
In addition, we stillachieve the goal of parsing each sentence but once.3.2 Parameter  T ra in ingIn this section, we describe how the parameters ofour grammar, the probabilities associated with eachgrammar ule, are set.
Ideally, in evaluating the ob-jective function for a particular grammar we shoulduse its optimal parameter settings given the trainingdata, as this is the full score that the given gram-mar can achieve.
However, searching for optimalparameter values is extremely expensive computa-tionally.
Instead, we grossly approximate the opti-mal values by deterministically setting parametersbased on the Viterbi parse of the training data par-sed so far.
We rely on the post-pass, described later,to refine parameter values.Referring to the rules in Table ?
?, the parametere is set to an arbitrary small constant.
The valuesof the parameters p(A) are set to the (smoothed)frequency of the X ~ A reduction in the Viterbiparse of the data seen so far.
The remaining symbolsare set to expand uniformly among their possibleexpansions.3.3 Const ra in ing  MovesConsider the move of creating a rule of the formA --* BC.
This corresponds to k 3 different specificrules that might be created, where k is the currentnumber of symbols in the grammar.
As it is toocomputationally expensive to consider each of theserules at every point in the search, we use heuristicsto constrain which moves are appraised.For the left-hand side of a rule, we always createa new symbol.
This heuristic selects the optimalchoice the vast majority of the time; however, underthis constraint he moves described earlier in thissection cannot yield arbitrary context-free langua-ges.
To partially address this, we add the moveMove 3: Create a rule of the form A ---* AB\[BWith this iteration move, we can construct gram-mars that generate arbitrary regular languages.
Asyet, we have not implemented moves that enablethe construction of arbitrary context-free grammars;this belongs to future work.To constrain the symbols we consider on theright-hand side of a new rule, we use what we call~riggcrs.
3 A ~rigger is a phenomenon i the Viterbiparse of a sentence that is indicative that a particularmove might lead to a better grammar.
For example,3This is not to be confused with the use of the termtriggers in dynamic language modeling.in Figure .9.9 the fact that the symbols Atalks andAszo,ozv occur adjacently is indicative that it couldbe profitable to create a rule B ---* At~t~sAsto,olv.
Wehave developed a set of triggers for each move in ourmove set, and only consider a specific move if it istriggered in the sentence currently being parsed inthe incremental processing.3.4 Post -PassA conspicuous shortcoming in our search frameworkis that the grammars in our search space are fairlyunexpressive.
Firstly, recall that our grammars mo-del a sentence as a sequence of independently gene-rated symbols; however, in language there is a largedependence between adjacent constituents.
Further-more, the only free parameters in our search are theparameters p(A); all other symbols (except S) arefixed to expand uniformly.
These choices were ne-cessary to make the search tractable.To address this issue, we use an Inside-Outside al-gorithm post-pass.
Our methodology is derived fromthat described by .9).
We create n new nonterminalsymbols {X1, .
.
.
,X ,} ,  and create all rules of theform:X~ ~ Xj Xk i,j, k e {1, .
.
.
,n}Xi - - *  A iE  {1 , .
.
.
,n} ,A E No~d- {S, X}Nold denotes the set of nonterminal symbols acqui-red in the initial grammar induction phase, and X1is taken to be the new sentential symbol.
Thesenew rules replace the first three rules listed in Ta-ble .9.9.
The parameters of these rules are initiMizedrandomly.
Using this grammar as the starting point,we run the Inside-Outside algorithm on the trainingdata until convergence.In other words, instead of using the naive SSXIX  rule to attach symbols together in parsingdata, we now use the Xi rules and depend on theInside-Outside algorithm to train these randomlyinitialized rules intelligently.
This post-pass allowsus to express dependencies between adjacent sym-bols.
In addition, it allows us to train parametersthat were fixed during the initial grammar induc-tion phase.4 P rev ious  WorkAs mentioned, this work employs the Bayesian gram-mar induction framework described by Solomonoff(.9; ?).
However, Solomonoff does not specify a con-crete search algorithm and only makes suggestionsas to its nature.Similar research includes work by Cook et al(1976) and Stolcke and Omohundro (1994).
Thiswork also employs a heuristic search within a Baye-sian framework.
However, a different prior proba-bility on grammars is used, and the algorithms areonly efficient enough to be applied to small data sets.231The grammar induction algorithms most suc-cessful in language modeling include the Inside-Outside algorithm (.7; ?
; ?
), a special case of theExpectation-Maximization algorithm (?
), and workby ?).
In the latter work, McCandless uses a heu-ristic search procedure similar to ours, but a verydifferent search criteria.
To our knowledge, neitheralgorithm has surpassed the performance of n-grammodels in a language modeling task of substantialscale.5 Resu l tsTo evaluate our algorithm, we compare the perfor-mance of our algorithm to that of n-gram modelsand the Inside-Outside algorithm.For n-gram models, we tried n - 1 , .
.
.
,10  foreach domain.
For smoothing a particular n-grammodel, we took a linear combination of all lower or-der n-gram models.
In particular, we follow stan-dard practice (?
; ?
; ?)
and take the smoothed i-gram probability to be a linear combination of the/-gram frequency in the training data and the smoo-thed (i - 1)-gram probability, that is,p(w01W = wi_ l .
.
.w- i )  =c(W~o0) +Ai,o(w) c(W)(1 - Ai ,c (w) )p (wo lw i_2  .
.
.
w -z )where c(W) denotes the count of the word sequenceW in the training data.
The smoothing parameters,~i,c are trained through the Forward-Backward al-gorithm (?)
on held-out data.
Parameters Ai.e aretied together for similar c to prevent data sparsity.For the Inside-Outside algorithm, we follow themethodology described by Lari and Young.
For agiven n, we create a probabilistic ontext-free gram-mar consisting of all Chomsky normal form rulesover the n nonterminal symbols {X1, ?.
?
Xn } and thegiven terminal symbols, that is, all rulesXi ---* Xj  Xk i, j ,  k E {1 , .
.
.
,n}Xi ---* a i E {1,. .
.
,n} ,a  E Twhere T denotes the set of terminal symbols in thedomain.
All parameters are initialized randomly.From this starting point, the Inside-Outside algo-rithm is run until convergence.For smoothing, we combine the expansion distri-bution of each symbol with a uniform distribution,that is, we take the smoothed parameter ps(A ---* a)to be1p,(A ~ a) = (1 - A)p,,(A ---* a) + An3 -F n\[T\[where p~ (A --~ a) denotes the unsmoothed parame-ter.
The value n 3 + n\[TI is the number of differentways a symbol expands under the Lari and Youngmethodology.
The parameter A is trained throughthe Inside-Outside algorithm on held-out data.
Thissmoothing is also performed on the Inside-Outsidepost-pass of our algorithm.
For each domain, wetried n -- 3 , .
.
.
,  10.Because of the computational demands of our al-gorithm, it is currently impractical to apply it tolarge vocabulary or large training set problems.
Ho-wever, we present the results of our algorithm inthree medium-sized omains.
In each case, we use4500 sentences for training, with 500 of these sent-ences held out for smoothing.
We test on 500 sent-ences, and measure performance by the entropy ofthe test data.In the first two domains, we created the trainingand test data artificially so as to have an ideal gram-mar in hand to benchmark results.
In particular, weused a probabilistic grammar to generate the data.In the first domain, we created this grammar byhand; the grammar was a small English-like probabi-listic context-free grammar consisting of roughly 10nonterminal symbols, 20 terminal symbols, and 30rules.
In the second domain, we derived the gram-mar from manually parsed text.
From a millionwords of parsed Wall Street Journal data from thePenn treebank, we extracted the 20 most frequentlyoccurring symbols, and the 10 most frequently oc-curring rules expanding each of these symbols.
Foreach symbol that occurs on the right-hand side of arule but which was not one of the most frequent 20symbols, we create a rule that expands that symbolto a unique terminal symbol.
After removing unre-achable rules, this yields a grammar of roughly 30nonterminals, 120 terminals, and 160 rules.
Para-meters are set to reflect the frequency of the corre-sponding rule in the parsed corpus.For the third domain, we took English text andreduced the size of the vocabulary by mapping eachword to its part-of-speech tag.
We used tagged WallStreet Journal text from the Penn treebank, whichhas a tag set size of about fifty.In Tables ?
?_?.7, we summarize our results.
Theideal grammar denotes the grammar used to gene-rate the training and test data.
For each algorithm,we list the best performance achieved over all n tried,and the best n column states which value realizedthis performance.We achieve a moderate but significant improve-ment in performance over n-gram models and theInside-Outside algorithm in the first two domains,while in the part-of-speech domain we are outper-formed by n-gram models but we vastly outperformthe Inside-Outside algorithm.In Table ?
?, we display a sample of the numberof parameters and execution time (on a Decstation5000/33) associated with each algorithm.
We choosen to yield approximately equivalent performance foreach algorithm.
The first pass row refers to the maingrammar induction phase of our algorithm, and thepost-pass row refers to the Inside-Outside post-pass.232best entropyn (bits/word)ideal grammar 2.30our algorithm 7 2.37n-gram model 4 2.46Inside-Outside 9 2.60entr.
relativeto n-gram-6.5%-3.7%+5.7%Table 2: English-like artificial grammarbest entropyn (bits/word)ideal grammarour algorithm 9n-gram model 4Inside-Outside 94.134.44entr.
relativeto n-gram--10.4%-3.7%4.614.64 +0.7%Table 3: Wall Street Journal-like artificial grammarNotice that our algorithm produces a significantlymore compact model than the n-gram model, whilerunning significantly faster than the Inside-Outsidealgorithm even though we use an Inside-Outsidepost-pass.
Part of this discrepancy is due to the factthat we require a smaller number of new nonterminalsymbols to achieve equivalent performance, but wehave also found that our post-pass converges morequickly even given the same number of nonterminalsymbols.6 D iscuss ionOur algorithm consistently outperformed the Inside-Outside algorithm in these experiments.
While wepartially attribute this difference to using a Bayesianinstead of maximum-likelihood bjective function,we believe that part of this difference results from amore effective search strategy.
In particular, thoughboth algorithms employ a greedy hill-climbing strat-egy, our algorithm gains an advantage by being ableto add new rules to the grammar.In the Inside-Outside algorithm, the gradient des-cent search discovers the "nearest" local minimum inthe search landscape to the initial grammar.
If thereare k rules in the grammar and thus k parameters,then the search takes place in a fixed k-dimensionalspace IR ~.
In our algorithm, it is possible to ex-pand the hypothesis grammar, thus increasing thedimensionality of the parameter space that is beingsearched.
An apparent local minimum in the space\]Rk may no longer be a local minimum in the space\]~k+l; the extra dimension may provide a pathwayfor further improvement of the hypothesis grammar.Hence, our algorithm should be less prone to sub-optimal ocal minima than the Inside-Outside algo-rithm.Outperforming n-gram models in the first two do-mains demonstrates that our algorithm is able totake advantage of the grammatical structure presentin data.
However, the superiority of n-gram modelsin the part-of-speech domain indicates that to becompetitive in modeling naturally-occurring data, itis necessary to model collocational information ac-curately.
We need to modify our algorithm to moreaggressively model n-gram information.7 Conc lus ionThis research represents a step forward in the questfor developing rammar-based language models fornatural language.
We induce models that, whilebeing substantially more compact, outperform n-gram language models in medium-sized omains.The algorithm runs essentially in time and space li-near in the size of the training data, so larger do-mains are within our reach.However, we feel the largest contribution of thiswork does not lie in the actual algorithm specified,but rather in its indication of the potential of the in-duction framework described by Solomonoffin 1964.We have implemented only a subset of the movesthat we have developed, and inspection of our re-sults gives reason to believe that these additionalmoves may significantly improve the performance ofour algorithm.Solomonoff's induction framework is not restric-ted to probabilistic ontext-free grammars.
Aftercompleting the implementation f our move set, weplan to explore the modeling of context-sensitivephenomena.
This work demonstrates that Solomo-noff's elegant framework deserves much further con-sideration.AcknowledgementsWe are indebted to Stuart Shieber for his suggestionsand guidance, as well as his invaluable comments onearlier drafts of this paper.
This material is based233best entropyn (bits/word)n-gram model 6our algorithm 7Inside-Outside 7entr.
relativeto n-gram3.013.15 +4.7%3.93 +30.6%Table 4: English sentence part-of-speech sequencesWSJ nartif.n-gram 3IO 9first passpost-pass 5entropy no.
(bits/word) params4.61 150004.64 20008004.60 4000time(sec)503000010005000Table 5: Parameters and Training Timeon work supported by the National Science Founda-tion under Grant Number IRI-9350192 to Stuart M.Shieber.Re ferencesD.
Angluin and C.H.
Smith.
1983.
Inductive in-ference: theory and methods.
ACM ComputingSurveys, 15:237-269.L.R.
Bahl, J.K. Baker, P.S.
Cohen, F. Jelinek, B.L.Lewis, and R.L.
Mercer.
1978.
Recognition of acontinuously read natural corpus.
In Proceedingsof the IEEE International Conference on Acou-stics, Speech and Signal Processing, pages 422-424, Tulsa, Oklahoma, April.Lalit R. Bahl, Frederick Jelinek, and Robert L. Mer-cer.
1983.
A maximum likelihood approach tocontinuous peech recognition.
IEEE Transac-tions on Pattern Analysis and Machine Intelli-gence, PAMI-5(2):179-190, March.J.K.
Baker.
1975.
The DRAGON system - an over-view.
IEEE Transactions on Acoustics, Speechand Signal Processing, 23:24-29, February.J.K.
Baker.
1979.
Trainable grammars for speechrecognition.
In Proceedings of the Spring Confe-rence of the Acoustical Society of America, pages547-550, Boston, MA, June.L.E.
Baum and J.A.
Eagon.
1967.
An inequalitywith application to statistical estimation for pro-babilistic functions of Markov processes and to amodel for ecology.
Bulletin of the American Ma-thematicians Society, 73:360-363.Peter F. Brown, Vincent J. DellaPietra, Peter V.deSouza, Jennifer C. Lai, and Robert L. Mercer.1992.
Class-based n-gram models of natural an-guage.
Computational Linguistics, 18(4):467-479,December.A.P.
Dempster, N.M. Laird, and D.B.
Rubin.
1977.Maximum likelihood from incomplete data via theEM algorithm.
Journal of the Royal StatisticalSociety, 39(B):1-38.Frederick Jelinek and Robert L. Mercer.
1980.
Inter-polated estimation of Markov source parametersfrom sparse data.
In Proceedings of the Workshopon Pattern Recognition in Practice, Amsterdam,The Netherlands: North-Holland, May.M.D.
Kernighan, K.W.
Church, and W.A.
Gale.1990.
A spelling correction program based on anoisy channel model.
In Proceedings of the Thir-teenth International Conference on Computatio-nal Linguistics, pages 205-210.K.
Lari and S.J.
Young.
1990.
The estimation ofstochastic context-free grammars using the inside-outside algorithm.
Computer Speech and Lan-guage, 4:35-56.K.
Lari and S.J.
Young.
1991.
Applications of sto-chastic context-free grammars using the inside-outside algorithm.
Computer Speech and Lan-guage, 5:237-257.Ming Li and Paul VitAnyi.
1993.
An Introductionto Kolmogorov Complexity and its Applications.Springer-Verlag.Michael K. McCandless and James R. Glass.
1993.Empirical acquisition of word and phrase classesin the ATIS domain.
In Third European Confe-rence on Speech Communication and Technology,Berlin, Germany, September.Fernando Pereira and Yves Schabes.
1992.
Inside-outside reestimation from partially bracket cor-pora.
In Proceedings of the 30th Annual Meetingof the ACL, pages 128-135, Newark, Delaware.P.
Resnik.
1992.
Probabilistic tree-adjoining gram-mar as a framework for statistical natural an-guage processing.
In Proceedings of the 14th In-234ternational Conference on Computational ?ingui-stics.J.
Rissanen.
1978.
Modeling by the shortest datadescription.
Automatica, 14:465-471.Y.
Schabes.
1992.
Stochastic lexicalized tree-adjoining grammars.
In Proceedings of the l~thInternational Conference on Computational Lin-guistics.C.E.
Shannon.
1951.
Prediction and entropy ofprinted English.
Bell Systems Technical Journal,30:50-64, January.R,.J.
Solomonoff.
1960.
A preliminary report ona general theory of inductive inference.
Techni-cal Report ZTB-138, Zator Company, Cambridge,MA, November.R.J.
Solomonoff.
1964.
A formal theory of inductiveinference.
Information and Control, 7:1-22,224-254, March, June.Rohini Srihari and Charlotte BMtus.
1992.
Combi-ning statisticM and syntactic methods in recogni-zing handwritten sentences.
In AAAI Symposium:Probabilistie Approaches to Natural Language, pa-ges 121-127.235
