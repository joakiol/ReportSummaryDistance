Proceedings of the SIGDIAL 2013 Conference, pages 31?40,Metz, France, 22-24 August 2013. c?2013 Association for Computational LinguisticsTraining an Integrated Sentence Planner on User DialogueBrian McMahanComputer ScienceRutgers Universitybrian.mcmahan@rutgers.eduMatthew StoneComputer ScienceRutgers Universitymatthew.stone@rutgers.eduAbstractAn appealing methodology for natural lan-guage generation in dialogue systems is totrain the system to match a target corpus.We show how users can provide such acorpus as a natural side effect of interact-ing with a prototype system, when the sys-tem uses mixed-initiative interaction and areversible architecture to cover a domainfamiliar to users.
We experiment withintegrated problems of sentence planningand realization in a referential communi-cation task.
Our model learns general andcontext-sensitive patterns to choose de-scriptive content, vocabulary, syntax andfunction words, and improves string matchwith user utterances to 85.8% from a hand-crafted baseline of 54.4%.1 IntroductionNatural language generation (NLG) in dialogueinvolves a complex array of choices.
It?s appeal-ing to scale up NLG by training systems to makethese choices with models derived from empiricaldata.
Sometimes, these choices have a measurableeffect on the flow of the interaction.
Systems canplan such choices with a model of dialogue dy-namics that predicts which utterances will fulfillcommunicative goals successfully and efficiently(Lemon, 2011; Janarthanam et al 2011; Garoufiand Koller, 2011).Other times, a wide variety of utterances workwell (Belz and Gatt, 2008).
In these cases, systemscan instead be designed simply to choose those ut-terances that most closely resemble specified tar-get behavior.
This paper describes and evaluatesa new data-driven methodology for training sen-tence planning and realization in interactive dia-logue systems this way.
Our work is particularlyinspired by Walker et al(2002), who train a di-alogue sentence planner by annotating its possi-ble outputs for quality; and Jordan and Walker(2005), who train a referring expression generatorto match annotated human?human dialogue.In text generation, researchers have been ableto exploit automatic analysis of existing resourceson such tasks as ordering words more naturally(Langkilde and Knight, 1998) and identifyingnamed entities in line with attested mentions (Sid-dharthan and Copestake, 2004).
However, previ-ous work on training dialogue generation has in-volved the acquisition or annotation of relevantdata ad hoc, for example by collecting human?human dialogue, running Wizard of Oz experi-ments, or rating system outputs.
Our work is dif-ferent: we use a bootstrapping approach that auto-matically mines interactions with a running proto-type to adapt NLG to match users.As described in Section 2, our work builds onthe COREF system of DeVault and Stone (2009).COREF and its users chat together to identifysimple objects in a visual scene.
COREF is de-signed with reversible models of language anddialogue?it tracks users?
utterances and its ownutterances with the same data structures and rep-resents them as updating the conversational statein parallel ways.
Because of this symmetry,COREF?s understanding of each user utterancedetermines an input?output pair that the systemcould take as a target for NLG.
We explain the sig-nificance of learning from such data in Section 3.However, we argue in Sections 4 and 5 that thislearning will yield significant results only if sys-tem and user do in fact turn out to make similarcontributions to dialogue.Our main experiment therefore uses data col-lected with a new version of COREF with moreflexible strategies for taking initiative, as describedin Section 6.
We use the system?s understand-ing of user utterances in the experiment, alongwith its productive capacity to generate alterna-31tive paraphrases of those utterances, to build anautomatically labeled training set of good and badNLG examples.
We learn a model of the differ-ence and evaluate its use in choosing novel utter-ances.
As documented in Section 7, the learnedmodel leads to improvements in naturalness overCOREF?s handcrafted baseline generator; our ex-periments document these improvements qualita-tively and quantitatively.Our work suggests new ways to design dialoguesystems to adhere to formal models with guaran-teed behavior (Paek and Pieraccini, 2008) whilereaping the benefits of data-driven approaches(Rieser and Lemon, 2011) by improving them-selves through ongoing interactions with users.Our experiments suggest that engaging with userexpertise is a key factor in enabling such new de-sign strategies.
Our technique crucially exploitssynergies in our domain between the architectureof the dialogue system, the specific dialogue pol-icy that the system implements, and users?
abilitiesto contribute to domain problem solving.2 BackgroundCOREF, short for ?collaborative reference?, com-municates with users through a text-chat windowfor human?computer dialogue.
A graphical inter-face provides task context and realizes domain ac-tions; it orchestrates a basic referential communi-cation task like those studied by Clark and Wilkes-Gibbs (1986) or Brennan and Clark (1996).
Ineach round of interaction, the participants in theconversation are presented with a set of simplegeometric shapes that they must talk about; theshapes are displayed on screen to human usersand described as a knowledge base to the COREFagent.
As the dialogue proceeds, one participant,assigned to work as the director, gets an indicationof which object to describe next.
The other partic-ipant, assigned to work as the matcher, must movethis target object to its final disposition.
Figure 1is a snapshot of the interface in a session where theuser works as matcher.
Experimental sessions nor-mally involve multiple rounds where participantsalternate serving as director and as matcher.COREF?s architecture factors its reasoning intothree integrative problem-solving modules, asshown in Figure 2.
The modules use differentalgorithms and control flow, but are linked to-gether by common representations and knowledgebases.
One shared resource is COREF?s prob-Figure 1: User?s view of the chat interface in aninteraction with COREF acting as director.abilistic context model, which tracks the likelystate of ongoing activity, maintains a linguisticcontext describing what has probably been saidand what should be salient as a result, and repre-sents the information available through the inter-face as grounded in interlocutors?
perception.
An-other shared resource is COREF?s tree-adjoininggrammar (TAG; Joshi and Schabes (1997)), whichassigns syntactic structures and semantic repre-sentations to utterances, and predicts what utter-ances will refer to in context and what dialoguemoves they will contribute.
Finally, both under-standing and generation use a common represen-tation of the interpretation of utterances, utteranceplans, which associate specific strings of wordswith the updates that they are predicted to achievevia grammar and context.The dialogue manager handles interaction withthe user, coordinates understanding and genera-tion, tracks updates to the context, and selects up-dates that COREF should contribute to the conver-sation.
In case of ambiguity, the dialogue man-ager propagates uncertainty forward in time andworks to resolve it through interaction.
(COREFhas general mechanisms for engaging in clarifica-tion subdialogues.)
In fact, by the time each ob-ject has been identified, COREF has committedretrospectively, in light of what has happened, toa single most likely interpretation for everythingthe user has said about it.
COREF has evidencethat other interpretations it originally entertainedwere not what the user intended.
This links eachuser utterance with a corresponding utterance planthat can be used for subsequent learning (DeVaultand Stone, 2009).The understanding module parses utterances us-ing the grammar and resolves them using the con-32Figure 2: COREF system architecture, showing representations and knowledge shared across modules:utterance plans show how each agent?s contributions follow from the system?s representations of gram-mar and context; update rules map out consistent contextual effects for each agent?s contributions.text model to recognize the possible utteranceplans behind them.
The generator, meanwhile,uses the grammar and the context model to syn-thesize an utterance plan for a grammatical expres-sion that is predicted to achieve some desired up-dates unambiguously, as in SPUD (Stone et al2003).
A range of choices are folded togetherby this integrated problem-solving process.
Forexample, the grammar specifies alternative real-izations involving different syntactic frames andfunctional items, as in the paraphrases ?the targetis a square?, ?a square?
and ?square?.
The gram-mar also specifies lexical paraphrases, as in theequivalents ?dark blue?
and ?navy blue?
or ?beige?and ?tan?.
SPUD?s problem solving also createschoices about how much descriptive content to in-clude in a reference, as ?the square?
versus ?theblue square?, and what kind of descriptive contentto include, as in ?the blue square?
versus ?the solidsquare?.
Full utterances involve all these choices,potentially in overlapping combinations, as in ?thetarget is the light brown object?
versus ?the solidsquare?.
See the Appendix for examples of NLGsearch, and DeVault (2008) for full details aboutCOREF?s design and implementation.COREF?s handcrafted NLG search heuristicsdraw on ideas from Stone et al(2003) and Daleand Reiter (1995) to prioritize efficient, specific ut-terances which use preferred descriptive attributesand respect built-in preferences for certain wordsand constructions.
When we implemented theseheuristics, we had no intention of revising themodel using learning.
However, COREF?s strat-egy never generates human-like overspecification,its lexical and syntactic choices are determinedby hand-coded logical constraints, and it offersfew tools to discriminate among comparable para-phrases.
In principle, a system like COREF oughtto be able to find out how people tend to make suchchoices in interacting with it, and learn to speakthe same way.
This is the central problem we ad-dress in this paper.3 Related WorkOur key contribution is demonstrating that a di-alogue system can bootstrap an integrated NLGstrategy from interactions with a prototype systemby training a model to imitate user utterances.
Thiscomplements DeVault and Stone (2009), who trainan interpretation model in a similar way.
Boot-strapping NLG for dialogue requires new insights,and require us to synthesize of a number of trendsin dialogue, in NLG and in social learning.A number of researchers have trained genera-tors for dialogue based on human specifications ofdesired output.
For example, Walker et al(2002)and Stent et al(2004) optimize sentence plansbased on expert ratings of candidate output utter-ances.
Jordan and Walker (2005) learn rules forpredicting the content of referring expressions tomatch patterns found in corpora of human descrip-tions in context.
Garoufi and Koller (2011) tunethe referential strategies of a general-purpose sen-tence planner based on metrics of utterance effec-tiveness mined from human?human interactions.Our work involves a new domain and for the firsttime involves integrated training of all these di-mensions of NLG, but we draw closely on the ar-chitectures, features and learning techniques de-veloped by these researchers.
The key differencethat they use data collected, and to some degreehand-annotated, specifically to train NLG.At the same time, a range of research hasexplored the way existing data sets can im-33prove NLG results.
For example, Langkilde andKnight (1998) n-gram statistics to bias a non-deterministic realization system towards frequentutterances.
Siddharthan and Copestake (2004) usereferences in corpora to bootstrap a generator fornamed entities in text.
Such methods, however,have generally focused on offline text generationapplications.
Our research shows that specific in-frastructure must be in place to tune NLG to a di-alogue system?s own experience.In addition, our work finds echoes in workacross AI on learning by imitation.
Interactiverobots can learn in new ways by modeling theirbehavior on competent humans (Breazeal et al2005).
Other domains require agents to developcooperative relationships and elicit meaningful be-havior from one another before they can learn toact effectively together (Zinkevich et al 2011).Our work helps to establish the connections ofthese ideas to dialogue.Finally, we note that our work is orthogonal toa range of other research that aims to extend andimprove NLG in dialogue through learning.
Givenspecified target utterances, knowledge acquisitiontechniques can be used to induce new resourcesthat describe those utterances for NLG as well asto optimize the use of those resources to match thecorpus (Higashinaka et al 2006; DeVault et al2008).
Moreover, given a model of the differen-tial effects of utterances on the conversation, rein-forcement learning can be used to identify utter-ances with the best outcomes (Lemon, 2011; Ja-narthanam et al 2011).
We see no reason not tocombine these techniques with imitation learningin the development of future systems.4 Training COREFOur method for mining COREF?s dialogue experi-ence involves three steps.
First, we compile train-ing data: positive instances are derived from userutterances and negative instances are derived fromthe generator?s alternative realizations of commu-nicative goals inferred from user utterances.
Next,we build a machine learning model to distinguishpositive from negative instances, using featuresdescribing the utterance itself, the current state ofthe conversation and relevant facts from the dia-logue history.
Finally, we apply the learned modelon new NLG problems by collecting candidateparaphrases and finding the one rated most likelyto be natural by the learned model.4.1 Data AnalysisEach user utterance in COREF?s interaction logsis associated with a particular state of the dialogueand with the utterance plan ultimately identifiedas its best interpretation.
Our method extracts thetask moves in the utterance plan as candidate com-municative goals for the utterance.
It swaps therole of the user and the system, so as to realizean NLG problem instance to plan a contributionwith the utterance?s inferred communicative goals,given the user?s role in the dialogue and their re-constructed dialogue state.
It then calls a revisedversion of the generator that?s non-deterministicand accumulates a range of plausible solutions.1This process automatically creates a representa-tion of the NLG problem faced by the user and theset of possible solutions to that problem implic-itly determined by COREF?s models of languagein context.
Our method partitions the training in-stances based on how the user chose to solve theNLG problem.
If the NLG output string matcheswhat the user actually said here, it becomes a pos-itive training example.
If it differs from what theuser actually said, it becomes a negative one.4.2 Machine LearningWe can now build a machine learning model ofthis data set.
Given an unlabeled candidate solu-tion to an NLG problem, we want to build a modelof the probability that the solution is representa-tive of human behavior in our transcripts.
We traina maximum entropy model (Berger et al 1996) tomake the prediction, using the MALLET softwarepackage (McCallum, 2002).
Given that the gener-ator ultimately wants to choose the best utterance,we could explore approaches to learn rankings di-rectly, such as RankBoost (Freund et al 2003).Formally, the machine learning model charac-terizes an input?output pair for NLG with a set offeatures that would be available to a generator inassessing a candidate output.
Each training exam-ple pairs an inventory of features with an observedvalue indicating whether the instance does or doesnot match the utterance produced by the humanuser.
Given a training set, MALLET selects a set1Our specific approach was to capture all the successfulutterances that differ from the preferred NLG path by anythree derivation steps of the lexicalized generation grammar.This heuristic was easy to implement with COREF?s existinginfrastructure for look-ahead search, and we found empiri-cally that more comprehensive search was expensive to carryout and tended primarily to add unnaturally verbose and re-dundant utterances.
See the Appendix for examples.34of features to use and fits numerical weights forthe features for logistic regression by maximumentropy.
That is, the features determine the pre-dicted probability that candidate output j for prob-lem t (utterance ut, j) is good (a match with a hy-pothetical user utterance), as a logistic functionof the sum of the feature weights describing theinstance?formally,P(ut, j = Good | features(ut, j)) =1/(1+ exp(?w0?
?i features(ut, j)i ?wi))This model can then be applied to unlabeled in-stances with features derived from novel NLGproblem instances and candidate outputs.The features we use in our experiments are de-scribed in full in Tables 4 and 5 in the Appendix.Most are from DeVault and Stone (2009).
We havefeatures describing the form of the output utter-ance: what phrase structure it has and what lexicalitems are used.
We have features describing whattask moves are achieved by the utterance and whatlinks the utterance has to context.
For complete-ness, we also add DeVault and Stone?s featuresdescribing the context itself, including the conver-sational tasks underway, the facts on the conversa-tional record, and the properties relevant to ongo-ing problem solving.2In designing features for learning, we also drawon the experience of Jordan and Walker (2005)in predicting the form of referring expressions.Many of their features closely align with thosewe inherit from DeVault and Stone (2009).
Onekind that doesn?t is Jordan and Walker?s concep-tual pacts feature set.
These features are de-signed to capture utterance choices that are con-tingent on other participants?
previous choicesin interaction?entrainment (Brennan and Clark,1996).
We make it possible for the learner to de-tect entrainment by introducing a new set of his-tory features, which list the presuppositions of re-cent utterances.We do not need Jordan and Walker?s distrac-tor features, however.
Unlike them, we do not tryto learn the difference between distinguishing de-scriptions and ambiguous ones.
Our architecture,2If these context features were shared across all outputsfor a given input, they would not affect what option for NLGwas best.
But this is not always the case in COREF, becausecontexts can be uncertain and because COREF can trigger ac-commodation that changes the context as part of NLG.
More-over, including these features might allow us to capture pos-sible variability in NLG, since the model can then predict thatotherwise marked utterances work naturally in some contexts.like that of Garoufi and Koller (2011), doesn?teven consider a candidate utterance unless it?s un-ambiguous on a standard reference model (Daleand Reiter, 1995).
Garoufi and Koller (2011) pro-vide evidence for the effectiveness of this kind offactorization of modeling and learning.4.3 Assessing the ModelTo use the trained model, we start from the NLGproblem of generating an utterance to achievespecified communicative goals in context.
OurNLG model constructs its space of candidate ut-terances.
Each candidate input?output pair is ana-lyzed in terms of its features, and then the learnedmodel assigns it a probability score.
We pick ouroutput via the candidate with the highest score.In evaluating how well this works, we are in-terested in how well the learned model predictsthe utterances of new subjects given data fromother subjects.
We assess this by reporting cross-validation results, predicting the choices of one,held-out subject given a model trained on the datafrom all other users in an experiment.
We reportan exact match error measure.
In a more complexgeneration task, we could measure error based onedit distance to give partial credit to NLG resultsthat are closer to user utterances.
As a baseline, wereport comparable measures for COREF?s originalNLG implementation.5 Pilot: The Need for ReciprocityWe applied our NLG training methodology to thedata set reported by DeVault and Stone (2009)with 20 subjects interacting with COREF.
The re-sults were not compelling.Analysis of this data set transforms human sub-jects?
utterances into 889 problem instances forNLG.
In 247 of these instances, the user?s utter-ance is not in the NLG search space, usually be-cause it is interpreted by robust methods ratherthan COREF?s grammar.
Of the remaining 642 ut-terances, our baseline generator already matchesthe user utterance 308 times (48%); it differs onthe other 334 instances (52%).
After learning,a model-based generator trained on the other 19users?
data now matches the utterance of a held-out user on 546 instances (85%) across cross-validation runs.
This sounds promising, but in factalmost all of the model successes (534 instances)are due to just five utterance types that fulfill sim-ple dialogue-management functions: ?yes?, ?no?,35?click continue?, ?done?
and ?ok?.There is in fact quite little evidence in this dataabout how COREF should make its typical genera-tion decisions.
Looking under the hood, the prob-lem is that COREF?s dialogue management pol-icy did not exploit the symmetry and reciprocityof its dialogue models and NL representations.COREF took the initiative in object-identificationdialogues when it was the director, offering de-scriptions of the target object, but it also took theinitiative when it was the matcher, asking the userto confirm or reject its suggestions about the iden-tity and properties of the target objects.System builders often make such design choicesto foster task success.
Giving the system the ini-tiative generally means that user utterances are un-derstood more reliably, which helps keep the di-alogue on track.
However, in settings where thesystem can potentially improve its behavior, wemay have to design the system to take more risksso it can acquire the data it needs; we may evenwant to sacrifice short-term task success to enablelong-term improvement.
Such trade-offs of explo-ration and exploitation are endemic in reinforce-ment learning, but learning by imitation gives theproblem a distinctively social dimension: gettingthe right data may mean not only trying new ac-tions in new situations, but actively creating theright relationship with the user.6 Collecting Mixed-initiative DataWe revised COREF?s dialogue strategy to betterreflect users?
interactive competence using sim-ple statistics about dialogue outcomes.
For eachclass of dialogue move by the agent in DeVaultand Stone?s evaluation data, we tabulated the num-ber of subsequent utterances required to identifythe object.
These measures give COREF?s plannedutterance an empirical score quantifying its antic-ipated effect in dialogue.
For example, after ask-ing if a particular object was the target, the sub-dialogue finished in 6.0 more turns on average.Analogous measures give a comparable score tothe most effective kind of contribution that?s po-tentially available to the user at each point in thedialogue.
For example, after saying that a particu-lar object was the target, the subdialogue finishedin 3.2 more turns on average.
Our new dialoguepolicy compares COREF?s planned move with theuser?s best option.
COREF proceeds with its ut-terance if its score is better but waits for the userif its score is worse.
This analysis gives our re-vised version of COREF an empirical thresholdfor taking initiative in the dialogue based on thestrengths of the contributions COREF and the usercould make next in context.
In practice, the re-vised strategy lets user directors drive the dialoguemuch more often than DeVault and Stone?s origi-nal handcrafted policy.
For example, COREF nowwaits for the user to propose a description ratherthan asking about a candidate object.We had 42 subjects interact with the revisedCOREF in a protocol of 29 object identificationtasks, grouped in blocks of 4, 9 and 16 as in De-Vault and Stone (2009).
Subjects were recruitedby advertisement and word of mouth from our in-stitution and were paid for their participation.
Thedata was collected as part of an independently-motivated assessment of COREF?s trade-offs be-tween asking for clarification and proceeding un-der uncertainty with its best interpretation, soCOREF varied these choices across the dialogues.Analysis of our new data set induces 2006 NLGproblem instances corresponding to human utter-ances, including 1382 cases where the user?s ut-terance is (1) completely described by COREF?sgrammar, (2) found in the NLG search space, and(3) represented as unambiguous by the underly-ing NLG model.
To confirm the diversity of utter-ances in this set, we automatically partitioned theutterances into four classes based on surface formand communicative goals achieved: acknowledg-ments that coordinate on the current state of thedialogue (569 instances), task instructions (23 in-stances), yes/no answers (434 instances) and otherdialogue contributions with explicit descriptivecontent (356 instances).
Thus, this data set con-tains substantial evidence about human strategiesin COREF?s domain.
We continue to performanalyses of utterances by category to document theresults of our learning experiment.7 ResultsTable 1 compares the aggregate performanceof the learned NLG module in comparison toCOREF?s baseline generator across all cross-validation runs (training on 41 users and testing ondata from one held-out user).
Except in the smallcategory of task instructions, where the baseline isalready good, the learned model offers a substan-tial improvement in rate of exact match to user ut-terance across all categories.
These differences in36Table 1: Comparison of learned model and baseline generator.System Descriptive Acknowledgments Yes/No Instructions TotalBaseline 170356 = 47.8%349569 = 61.3%210434 = 48.4%2323 = 100%7521382 = 54.4%Model 259356 = 72.8%477569 = 83.8%427434 = 98.4%2323 = 100%11861382 = 85.8%Evaluation of exact match to user utterances across hold-one-user-out cross-validation runs.
We reportnumber of matching instances out of number of instances with the user utterance in the NLG searchspace, along with percentage match, broken down by form and communicative goal of the utterance.Table 2: Comparison of accuracy by item.BaselineModelMatch MismatchMatch 720 466Mismatch 32 164(a) Counts of NLG problem instances of all types,comparing matches in the baseline generatoragainst matches in the learned model.BaselineModelMatch MismatchMatch 152 107Mismatch 18 79(b) Counts of NLG problem instances with sub-stantive contributions and explicit descriptive ma-terial, comparing matches in the baseline genera-tor against matches in the learned model.rates are all statistically significant (p < .005 byFisher?s exact test).Table 2 breaks down overall results (Table 2a)and results on descriptive utterances (Table 2b), toexplore associations between the performance ofthe baseline generator and the performance of thelearned model on individual items.
We find a clearlink between the two methods: when the modelgets an utterance wrong, the baseline method ismuch more likely to have gotten the utterancewrong as well (p < .001 by Fisher?s exact test).We conclude that the model is not just improv-ing on the baseline generator in aggregate, but haslearned to correct specific choices in the baselinesystem that are not representative of user behavior.The breakdown in Table 1 gives a sense of therange of cases covered by the learned model.
The?yes/no?
cases mostly involve training COREF tosay ?yes?
rather than ?yeah?.
The acknowledg-ment cases involve understanding the subtle waysthat people trade off alternatives such as ?ok?,?done?
and ?I added it?
?a difficult problem butone where we have little choice but to trust ma-chine learning results.Descriptive utterances are more substantial.
Tounderstand these cases better, we built an overallmodel with data from all 42 users and looked at thefeatures selected by MALLET and the weights fitfor them in the maximum entropy model.
Table 3shows a sample of the MALLET output.
We thinkof these features as establishing a network of prior-itized defaults; lower-weighted features must con-spire together to override higher-weighted ones.Syntax is the strongest effect; for example, thecontrast between [S DET N] and [S NP IS DET N]gives a preference of 1.27 to the simpler struc-ture.
Lexical features encode more natural items(?brown?
versus ?beige?)
but also implicitly en-code natural descriptive patterns (as with the colormodifier ?light?).
Presupposition features, mean-while, help ensure that words have their most natu-ral meanings.
On this analysis, the model contentscorroborate our hypothesis that user data gives ev-idence to refine a wide variety of NLG choices.8 DiscussionIn this paper, we show how users?
utterances cangive a dialogue system consistent and reliable in-dicators not only of how to solve its NLU prob-lems, as in DeVault and Stone (2009), but alsohow to solve its NLG problems.
Thus, we cannow design dialogue systems to learn to imitatetheir human users in certain cases.
To do so, thesystem needs to work in a domain where users areprepared to offer the same kind of contributions37Table 3: Sample features used to identify user tu-ples and their weights in an overall model.Syntax Features:Fits [S DET N] 2.29Fits [S COLOR N] 2.09Fits [S DET COLOR N] 1.86Fits [S NP IS DET N] 1.12Lexical Features:Includes word light 0.87Includes word dark 0.60Includes word brown 0.22Includes word beige 0.005Presupposition Features:Uses square for square object 2.05Uses diamond for rhombus 2.09Uses pink for pale red-purple 1.70Describes light blue as light 0.92as the system, the system needs to represent thosecontributions symmetrically, and the system needsto be able to actually elicit, analyze and learn fromrelevant user utterances.Our approach, like that of Garoufi and Koller(2011), is to combine a symbolic account of ut-terance interpretation with a learned model of ut-terance quality.
Thus, on our approach, systemutterances always come with formal guaranteesthat they fulfill specified communicative goals andhave a unique interpretation in context.
That mayhelp underwrite the guarantees that Paek and Pier-accini (2008) emphasize, that data-driven systemsmust respect the coherence of dialogue and mustcontinue to do so even as they learn to improvedialogue efficiency and naturalness.Our work suggests some natural followups.
Itwould be interesting to refine the NLG modelbased on the disambiguation strategy learned inDeVault and Stone (2009).
If the system discov-ers that utterances are not as ambiguous as the ini-tial model suggests, it opens up new possibilitiesfor tuning NLG to match what users say.
Scal-ing up the ideas, meanwhile, invites us to buildfactored models that describe NLG decisions in amore compositional way, as well as finding morepowerful and generalizable features.Further work is also required to use these tech-niques in a broader range of settings.
Our tech-nique requires the system to give users the op-portunity to say the same kinds of things it says,so it is most appropriate for collaborative prob-lem solving.
Further research is required to usethe methodology for asymmetric situations suchas information seeking.
Use in spoken dialoguesystems, meanwhile, would challenge the limitsof mixed-initiative interaction and would requiretechniques to discount users?
errors and disfluen-cies.
Although these limitations make our tech-niques difficult to use in many current applica-tions, we are optimistic that our methods willapply quite naturally to emerging open-domainsettings such as human?robot interaction, whereusers and systems meet on a more equal footing.AcknowledgmentsThis authors were supported by NSF DGE0549115 (IGERT) and IIS 1017811.
Thanks to thereviewers, and to David DeVault and Lara Martinfor discussion and assistance.ReferencesAnja Belz and Albert Gatt.
2008.
Intrinsic vs. extrinsicevaluation measures for referring expression gener-ation.
In Proceedings of ACL, pages 197?200.Adam L. Berger, Stephen Della Pietra, and VincentJ.
Della Pietra.
1996.
A maximum entropy ap-proach to natural language processing.
Computa-tional Linguistics, 22(1):39?71.Cynthia Breazeal, Daphna Buchsbaum, Jesse Gray, andBruce Blumberg.
2005.
Learning from and aboutothers: Towards using imitation to bootstrap the so-cial understanding of others by robots.
ArtificialLife, 11(1?2):1?32.Susan E. Brennan and Herbert H. Clark.
1996.
Con-ceptual pacts and lexical choice in conversation.
J.Experimental Psychology, 22(6):1482?1493.Herbert H. Clark and Deanna Wilkes-Gibbs.
1986.Referring as a collaborative process.
Cognition,22(1):1?39.Robert Dale and Ehud Reiter.
1995.
Computationalinterpretations of the Gricean maxims in the gener-ation of referring expressions.
Cognitive Science,18:233?263.David DeVault and Matthew Stone.
2009.
Learning tointerpret utterances using dialogue history.
In Pro-ceedings of EACL, pages 184?192.David DeVault, David Traum, and Ron Artstein.
2008.Practical grammar-based NLG from examples.
InProceedings of INLG, pages 78?85.David DeVault.
2008.
Contribution Tracking: Par-ticipating in Task-Oriented Dialogue under Uncer-tainty.
Ph.D. thesis, Rutgers.38Yoav Freund, Raj Iyer, Robert E. Schapire, and YoramSinger.
2003.
An efficient boosting algorithm forcombining preferences.
J.
Machine Learning Re-search, 4:933?969.Konstantina Garoufi and Alexander Koller.
2011.Combining symbolic and corpus-based approachesfor the generation of successful referring expres-sions.
In Proceedings of EWNLG, pages 121?131.Ryuichiro Higashinaka, Rashmi Prasad, and Mari-lyn A. Walker.
2006.
Learning to generate natu-ralistic utterances using reviews in spoken dialoguesystems.
In Proceedings of ICCL?ACL, pages 265?272.Srinivasan Janarthanam, Helen Hastie, Oliver Lemon,and Xingkun Liu.
2011.
?The day after the day aftertomorrow??
a machine learning approach to adap-tive temporal expression generation: training andevaluation with real users.
In Proceedings of SIG-DIAL, pages 142?151.Pamela W. Jordan and Marilyn A. Walker.
2005.Learning content selection rules for generating ob-ject descriptions in dialogue.
J. Artif.
Intell.
Res.
(JAIR), 24:157?194.Aravind K. Joshi and Yves Schabes.
1997.
Tree-adjoining grammars.
In Grzegorz Rozenberg andArto Salomaa, editors, Handbook of Formal Lan-guages, pages 69?123.
Springer.Irene Langkilde and Kevin Knight.
1998.
Generationthat exploits corpus-based statistical knowledge.
InProceedings of COLING?ACL, pages 704?710.Oliver Lemon.
2011.
Learning what to say and how tosay it: Joint optimisation of spoken dialogue man-agement and natural language generation.
Com-puter Speech & Language, 25(2):210?221.Andrew McCallum.
2002.
MALLET: AMAchine learning for LanguagE toolkit.http://mallet.cs.umass.edu.Tim Paek and Roberto Pieraccini.
2008.
Automatingspoken dialogue management design using machinelearning: An industry perspective.
Speech Commu-nication, 50(8?9):716?729.Verena Rieser and Oliver Lemon.
2011.
Reinforce-ment Learning for Adaptive Dialogue Systems: AData-driven Methodology for Dialogue Manage-ment and Natural Language Generation.
Springer.Advaith Siddharthan and Ann A. Copestake.
2004.Generating referring expressions in open domains.In Proceedings of ACL, pages 407?414.Amanda Stent, Rashmi Prasad, and Marilyn A. Walker.2004.
Trainable sentence planning for complex in-formation presentations in spoken dialog systems.In Proceedings of ACL, pages 79?86.Matthew Stone, Christine Doran, Bonnie Webber, To-nia Bleam, and Martha Palmer.
2003.
Microplan-ning with communicative intentions: the SPUD sys-tem.
Computational Intelligence, 19(4):314?381.Marilyn A. Walker, Owen Rambow, and Monica Ro-gati.
2002.
Training a sentence planner for spokendialogue using boosting.
Computer Speech & Lan-guage, 16(3?4):409?433.Martin Zinkevich, Michael H. Bowling, and MichaelWunder.
2011.
The lemonade stand game com-petition: solving unsolvable games.
SIGecom Ex-changes, 10(1):35?38.Appendix: NLG Search and FeaturesUser utterance pink squareGoal(s) found 1.
Target is pink2.
Target is square, or3.
Target is both pink and squareBaseline 1. the target is pink2.
the target is square3.
pink squareModel 1. pink square2.
square3.
pink squareCandidates a box, a fuschia box, a fuschiafuschia box, a fuschia fuschiasquare, a fuschia pink box,a fuschia pink square, a fuschiapurple box, a fuschia purplesquare, a fuschia square, a likefuschia box, a like fuschia square,a like pink box, a like pinksquare, a like purple box, a likepurple square, a pink box, a pinkfuschia box, a pink fuschiasquare, a pink pink box, a pinkpink square, a pink purple box,a pink purple square, a pinksquare, a purple box, a purplefuschia box, a purple fuschiasquare, a purple pink box,a purple pink square, a purplepurple box, a purple purplesquare, a purple square, a square,box, fuschia box, fuschia square,pink box, pink square, purplebox, purple square, square, thetarget is fuschia, the target ispink, the target is purple, thetarget is squareModel confirms baseline vocabulary, learns tooverspecify color goal (1) for more natural syn-tax.
COREF can?t spell ?fuchsia?.39Table 4: Features derived from the current state of the dialogue (st).feature set descriptionNumTasksUnderway The number of tasks underway in the state st .TasksUnderwayFor any task that is underway in state st , a feature includes itsname, its depth on the task stack, and its current status in itsformal task network.NumRemainingReferents The number of targets that remain to be identified in state st .TabulatedFactsFor any fact on the conversational record at state st there is acorresponding string feature?a formula with any unique ref-erence symbols anonymized (e.g.
X34 becomes some-object).CurrentTargetConstraints For any positive or negative constraint on the current target instate st , there is a corresponding string feature.UsefulProperties For any property instantiated in the display in state st there is acorresponding feature.History Each assertion and presupposition on the conversational recordin state st is represented as a string feature.Table 5: Features derived from the proposed utterance (ut, j).feature set descriptionPresuppositionsEach of the atomic presuppositions of the utterance ut, j is rep-resented as a string feature.
The string captures predicate?argument structure but anonymizes references to individuals(e.g.
target12 becomes sometarget).AssertionsEach of the dialogue moves that the utterance contributes cor-responds to a feature.
This string also captures predicate?argument structure but anonymizes references to individuals.Syntax A string representation of the bracketed phrase structure, in-cluding non-terminal categories, of the utterance.Words We represent each word that occurs in the utterance as a fea-ture.User utterance the light blue diamondGoal(s) found Target is specified objectBaseline the blue objectModel the light blue diamondCandidates the blue blue diamond,the blue blue object, the blueblue rhombus, the bluediamond, the blue diamondoutline, the blue object,the blue object outline,the blue rhombus, the bluerhombus outline, the emptyblue diamond, the empty blueobject, the empty bluerhombus, the hollow bluediamond, the hollow blueobject, the hollow bluerhombus, (continued)Candidates the light blue diamond,the light blue object, the lightblue rhombus, the lighter bluediamond, the lighter blueobject, the lighter bluerhombus, the like bluediamond, the like blue object,the like blue rhombus,the outline blue diamond,the outline blue object,the outline blue rhombus,the sky blue diamond, the skyblue object, the sky bluerhombusModel confirms baseline pattern of color and typereference but learns to overspecify color as lightblue and to use basic type diamond.40
