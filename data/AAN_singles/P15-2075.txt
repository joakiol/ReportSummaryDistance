Proceedings of the 53rd Annual Meeting of the Association for Computational Linguisticsand the 7th International Joint Conference on Natural Language Processing (Short Papers), pages 458?463,Beijing, China, July 26-31, 2015.c?2015 Association for Computational LinguisticsLearning Lexical Embeddings with Syntacticand Lexicographic KnowledgeTong WangUniversity of Torontotong@cs.toronto.eduAbdel-rahman MohamedMicrosoft Researchasamir@microsoft.comGraeme HirstUniversity of Torontogh@cs.toronto.eduAbstractWe propose two improvements on lexi-cal association used in embedding learn-ing: factorizing individual dependency re-lations and using lexicographic knowl-edge from monolingual dictionaries.
Bothproposals provide low-entropy lexical co-occurrence information, and are empiri-cally shown to improve embedding learn-ing by performing notably better than sev-eral popular embedding models in similar-ity tasks.1 Lexical Embeddings and RelatednessLexical embeddings are essentially real-valueddistributed representations of words.
As a vector-space model, an embedding model approximatessemantic relatedness with the Euclidean distancebetween embeddings, the result of which helpsbetter estimate the real lexical distribution in var-ious NLP tasks.
In recent years, researchers havedeveloped efficient and effective algorithms forlearning embeddings (Mikolov et al., 2013a; Pen-nington et al., 2014) and extended model applica-tions from language modelling to various areas inNLP including lexical semantics (Mikolov et al.,2013b) and parsing (Bansal et al., 2014).To approximate semantic relatedness with ge-ometric distance, objective functions are usu-ally chosen to correlate positively with the Eu-clidean similarity between the embeddings of re-lated words.
Maximizing such an objective func-tion is then equivalent to adjusting the embeddingsso that those of the related words will be geomet-rically closer.The definition of relatedness among words canhave a profound influence on the quality of theresulting embedding models.
In most existingstudies, relatedness is defined by co-occurrencewithin a window frame sliding over texts.
Al-though supported by the distributional hypothe-sis (Harris, 1954), this definition suffers from twomajor limitations.
Firstly, the window frame sizeis usually rather small (for efficiency and sparsityconsiderations), which increases the false negativerate by missing long-distance dependencies.
Sec-ondly, a window frame can (and often does) spanacross different constituents in a sentence, result-ing in an increased false positive rate by associ-ating unrelated words.
The problem is worsenedas the size of the window increases since eachfalse-positive n-gram will appear in two subsum-ing false-positive (n+1)-grams.Several existing studies have addressed theselimitations of window-based contexts.
Nonethe-less, we hypothesize that lexical embedding learn-ing can further benefit from (1) factorizing syntac-tic relations into individual relations for structuredsyntactic information and (2) defining relatednessusing lexicographic knowledge.
We will show thatimplementation of these ideas brings notable im-provement in lexical similarity tasks.2 Related WorkLexical embeddings have traditionally been usedin language modelling as distributed representa-tions of words (Bengio et al., 2003; Mnih and Hin-ton, 2009) and have only recently been used inother NLP tasks.
Turian et al.
(2010), for example,used embeddings from existing language models(Collobert and Weston, 2008; Mnih and Hinton,2007) as unsupervised lexical features to improvenamed entity recognition and chunking.
Embed-ding models gained further popularity thanks tothe simplicity and effectiveness of the word2vecmodel (Mikolov et al., 2013a), which implicitlyfactorizes the point-wise mutual information ma-trix shifted by biases consisting of marginal countsof individual words (Levy and Goldberg, 2014b).Efficiency is greatly improved by approximatingthe computationally costly softmax function with458negative sampling (similar to that of Collobert andWeston 2008) or hierarchical softmax (similar tothat of Mnih and Hinton 2007).To address the limitation of contextual localityin many language models (including word2vec),Huang et al.
(2012) added a ?global context score?to the local n-gram score (Collobert and Weston,2008).
The concatenation of word vectors anda ?document vector?
(centroid of the composingword vectors weighted by idf ) was used as modelinput.
Pennington et al.
(2014) proposed to explic-itly factorize the global co-occurrence matrix be-tween words, and the resulting log bilinear modelachieved state-of-the-art performance in lexicalsimilarity, analogy, and named entity recognition.Several later studies addressed the limitationsof window-based co-occurrence by extending theword2vec model to predict words that are syn-tactically related to target words.
Levy and Gold-berg (2014a) used syntactically related words non-discriminatively as syntactic context.
Bansal et al.
(2014) used a training corpus consisting of se-quences of labels following certain manually com-piled patterns.
Zhao et al.
(2014) employedcoarse-grained classifications of contexts accord-ing to the hierarchical structures in a parse tree.Semantic relations have also been explored as aform of lexical association.
Faruqui et al.
(2015)proposed to retrofit pre-trained embeddings (de-rived using window-based contexts) to semanticlexicons.
The goal is to derive a set of embeddingsto capture relatedness suggested by semantic lex-icons while maintaining their resemblance to thecorresponding window-based embeddings.
Bolle-gala et al.
(2014) trained an embedding model withlexical, part-of-speech, and dependency patternsextracted from sentences containing frequently co-occurring word pairs.
Each relation was repre-sented by a pattern representation matrix, whichwas combined and updated together with the wordrepresentation matrix (i.e., lexical embeddings) ina bilinear objective function.3 The Proposed Models3.1 Factorizing Dependency RelationsOne strong limitation of the existing dependency-based models is that no distinctions are madeamong the many different types of dependency re-lations.
This is essentially a compromise to avoidissues in model complexity and data sparsity, andit precludes the possibility of studying individualor interactive effects of individual dependency re-lations on embedding learning.Consequently, we propose a relation-dependentmodel to predict dependents given a governor un-der individual dependency relations.
For example,given a nominal governor apple of the adjectivemodifier relation (amod), an embedding modelwill be trained to assign higher probability to ob-served adjectival dependents (e.g., red, sweet, etc.
)than to rarely or never observed ones (e.g., pur-ple, savoury, etc.).
If a model is able to accuratelymake such predictions, it can then be said to ?un-derstand?
the meaning of apple by possessing se-mantic knowledge about its certain attributes.
Byextension, similar models can be trained to learnthe meaning of the governors in other dependencyrelations (e.g., adjectival governors in the inverserelation amod?1, etc.
).The basic model uses an objective function sim-ilar to that of Mikolov et al.
(2013a):log?(eTge?d)+k?i=1E?di[log?(?eTge?
?di)],where e?and e?
?are the target and the outputembeddings for the corresponding words, respec-tively, and ?
is the sigmoid function.
The sub-scripts g and d indicate whether an embedding cor-respond to the governor or the dependent of a de-pendency pair, and?d?correspond to random sam-ples from the dependent vocabulary (drawn by un-igram frequency).3.2 Incorporating Lexicographic KnowledgeSemantic information used in existing studies(Section 2) either relies on specialized lexical re-sources with limited availability or is obtainedfrom complex procedures that are difficult to repli-cate.
To address these issues, we propose to usemonolingual dictionaries as a simple yet effectivesource of semantic knowledge.
The defining rela-tion has been demonstrated to have good perfor-mance in various semantic tasks (Chodorow et al.,1985; Alshawi, 1987).
The inverse of the definingrelation (also known as the Olney Concordance In-dex, Reichert et al.
1969) has also been proven use-ful in building lexicographic taxonomies (Amsler,1980) and identifying synonyms (Wang and Hirst,2011).
Therefore, we use both the defining rela-tion and its inverse as sources of semantic associ-ation in the proposed embedding models.Lexicographic knowledge is represented byadopting the same terminology used in syntactic459dependencies: definienda as governors and defini-entia as dependents.
For example, apple is relatedto fruit and rosaceous as a governor under def, orto cider and pippin as a dependent under def?1.3.3 Combining Individual KnowledgeSourcesSparsity is a prominent issue in the relation-dependent models since each individual relationonly receives a limited share of the overall co-occurrence information.
We also propose a post-hoc, relation-independent model that combinesthe individual knowledge sources.
The input of themodel is the structured knowledge from relation-dependent models, for example, that somethingcan be red or sweet, or it can ripen or fall, etc.The training objective is to predict the originalword given the relation-dependent embeddings,with the intuition that if a model is trained to beable to ?solve the riddle?
and predict that thissomething is an apple, then the model is saidto possess generic, relation-independent knowl-edge about the target word by learning from therelation-dependent knowledge sources.Given input word wI, its relation-independentembedding is derived by applying a linear modelM on the concatenation of its relation-dependentembeddings (?ewI).
The objective function of arelation-independent model is then defined aslog?(e?TwIM?ewI)+k?i=1Ew?i[log?
(?e?Tw?iM?ewI)],where e?
?are the context embeddings for the corre-sponding words.
Since?ewIis a real-valued vector(instead of a 1-hot vector as in relation-dependentmodels), M can no longer be updated one columnat a time.
Instead, updates are defined as:?
?M= [1??(e?TwOM?ewI)]e?wO?eTwI?k?i=1[1??
(?e?TwiM?ewI)]e?wi?eTwI.Training is quite efficient in practice due to the lowdimensionality of M; convergence is achieved af-ter very few epochs.1Note that this model is different from the non-factorized models that conflate multiple depen-dency relations because the proposed model is a1We also experimented with updating the relation-dependent embeddings together with M, but this worsenedevaluation performance.deeper structure with pre-training on the factor-ized results (via the relation-dependent models) inthe first layer.4 Evaluations4.1 Training Data and BaselinesThe Annotated English Gigaword (Napoles et al.,2012) is used as the main training corpus.
It con-tains 4 billion words from news articles, parsed bythe Stanford Parser.
A random subset with 17 mil-lion words is also used to study the effect of train-ing data size (dubbed 17M).Semantic relations are derived from the defini-tion text in the Online Plain Text English Dictio-nary2.
There are approximately 806,000 definitionpairs, 33,000 distinct definienda and 24,000 dis-tinct defining words.
The entire corpus has 1.25million words in a 7.1MB file.Three baseline systems are used for compar-ison, including one non-factorized dependency-based model DEP (Levy and Goldberg, 2014a)and two window-based embedding models w2v(or word2vec, Mikolov et al.
2013a) and GloVe(Pennington et al., 2014).
Embedding dimensionis 50 for all models (baselines as well as the pro-posed).
Embeddings in the window-based mod-els are obtained by running the published softwarefor each of these systems on the Gigaword corpuswith default values for all hyper-parameters exceptfor vector size (50) and minimum word frequency(100 for the entire Gigaword corpus; 5 for the 17Msubset).
For the w2v model, for example, we usedthe skip-gram model with the default value 5 aswindow size, negative sample size, and epoch size,and 0.025 as initial learning rate.4.2 Lexical SimilarityRelation-Dependent ModelsTable 1 shows the results on four similaritydatasets: MC (Miller and Charles, 1991), RG(Rubenstein and Goodenough, 1965), FG (orwordsim353, Finkelstein et al.
2001), and SL (orSimLex, Hill et al.
2014b).
The first three datasetsconsist of nouns, while the last one also includesverbs (SLv) and adjectives (SLa) in addition tonouns (SLn).
Semantically, FG contains manyrelated pairs (e.g., movie?popcorn), whereas theother three datasets are purely similarity oriented.2http://www.mso.anu.edu.au/?ralph/OPTED/460Model MC RG FG SLnSLvSLaamod .766 .798 .572 .566 .154 .466amod?1.272 .296 .220 .218 .248 .602nsubj .442 .350 .376 .388 .392 .464nn .596 .620 .514 .486 .130 .068BaselinesDEP .640 .670 .510 .400 .240 .350w2v .656 .618 .600 .382 .237 .560GloVe .609 .629 .546 .346 .142 .517Table 1: Correlation between human judgementand cosine similarity of embeddings (trained onthe Gigaword corpus) on six similarity datasets.Performance is measured by Spearman?s ?
be-tween system scores and human judgements ofsimilarity between the pairs that accompany eachdataset.When dependency information is factorizedinto individual relations, models using the best-performing relation for each dataset3out-performthe baselines by large margins on 5 out of the 6datasets.
In comparison, the advantage of the syn-tactic information is not at all obvious when theyare used in a non-factorized fashion in the DEPmodel; it out-performs the window-based meth-ods (below the dashed line) on only 3 datasetswith limited margins.
However, the window-basedmethods consistently outperform the dependency-based methods on the FG dataset, confirming ourintuition that window-based methods are better atcapturing relatedness than similarity.When dependency relations are factorized intoindividual types, sparsity is a rather prominent is-sue especially when the training corpus is small.With sufficient training data, however, factorizedmodels consistently outperform all baselines byvery large margins on all but the FG dataset.
Av-erage correlation (weighted by the size of eachsub-dataset corresponding to the three POS?s) onthe SL dataset is 0.531, outperforming the best re-ported result on the dataset (Hill et al., 2014a).3We did not hold out validation data to choose the best-performing relations for each dataset.
Our assumption is thatthe dominant part-of-speech of the words in each dataset isthe determining factor of the top-performing syntactic rela-tion for that dataset.
Consequently, the choice of this re-lation should be relatively constant without having to relyon traditional parameter tuning.
For the four noun datasets,for example, we observed that amod is consistently the top-performing relation, and we subsequently assumed similarconsistency on the verb and the adjective datasets.
Thesame observations and rationales apply for the relation-independent experiments.Model MC RG FG SLnSLvSLaRel.
Dep.
#1 .512 .486 .380 .354 .222 .394Rel.
Dep.
#2 .390 .380 .360 .304 .206 .236Rel.
Indep.
.570 .550 .392 .360 .238 .338BaselinesDEP .530 .558 .506 .346 .138 .412w2v .563 .491 .562 .287 .065 .379GloVe .306 .368 .308 .132 ?.007 .254Table 2: Lexical similarity performance ofrelation-independent models (trained on the 17Mcorpus) combining top two best-performing rela-tions for each POS.Although the co-occurrence data is sparse, itis nonetheless highly ?focused?
(Levy and Gold-berg, 2014a) with much lower entropy.
As a result,convergence is much faster when compared to thenon-factorized models such as DEP, which takesup to 10 times more iterations to converge.Among the individual dependency relations, themost effective relations for nouns, adjectives, andverbs are amod, amod?1, and nsubj, respec-tively.
For nouns, we observed a notable gap inperformance between amod and nn.
Data inspec-tion reveals that a much higher proportion of nnmodifiers are proper nouns (64.0% compared toabout 0.01% in amod).
The comparison suggeststhat, as noun modifiers, amod describes the at-tributes of nominal concepts while nn are moreoften instantiations, which apparently is semanti-cally less informative.
On the other hand, nn isthe better choice if the goal is to train embeddingsfor proper nouns.Relation-Independent ModelThe relation-independent model (Section 3.3) isimplemented by combining the top two best-performing relations for each POS: amod anddobj?1for noun pairs, nsubj and dobj forverb pairs, and amod?1and dobj?1for adjectivepairs.Lexical similarity results on the 17M corpusare listed in Table 2.
The combined resultsimprove over the best relation-dependent mod-els for all categories except for SLa(adjectives),where only the top-performing relation-dependentmodel (amod?1) yielded statistically significantresults and thus, results are worsened by com-bining the second-best relation-dependent sourcedobj?1(which is essentially noise).
Compar-ing to baselines, the relation-independent modelachieves better results in four out of the six cat-461Model MC RG FG SLnSLvSLadef .640 .626 .378 .332 .320 .306def?1.740 .626 .436 .366 .332 .376Combined .754 .722 .530 .410 .356 .412w2v .656 .618 .600 .382 .237 .560Table 3: Lexical similarity performance of mod-els using dictionary definitions and compared toword2vec trained on the Gigaword corpus.egories.Using Dictionary DefinitionsEmbeddings trained on dictionary definitions arealso evaluated on the similarity datasets, andthe results are shown in Table 3.
The individ-ual relations (defining and inverse) perform sur-prisingly well on the datasets when comparedto word2vec.
The relation-independent modelbrings consistent improvement by combining therelations, and the results compare favourably toword2vec trained on the entire Gigaword cor-pus.
Similar to dependency relations, lexico-graphic information is also better at capturing sim-ilarity than relatedness, as suggested by the results.5 ConclusionsThis study explored the notion of relatedness inembedding models by incorporating syntactic andlexicographic knowledge.
Compared to exist-ing syntax-based embedding models, the proposedembedding models benefits from factorizing syn-tactic information by individual dependency rela-tions.
Empirically, syntactic information from in-dividual dependency types brings about notableimprovement in model performance at a muchhigher rate of convergence.
Lexicographic knowl-edge from monolingual dictionaries also helps im-prove lexical embedding learning.
Embeddingstrained on a compact, knowledge-intensive re-source rival state-of-the-art models trained on freetexts thousands of times larger in size.AcknowledgmentsWe thank Gerald Penn, Ruslan Salakhutdinov,Suzanne Stevenson, and Xiaodan Zhu for their in-sightful comments, as well as the anonymous re-viewers for their valuable feedback.
This study isfinancially supported by the Natural Sciences andEngineering Research Council of Canada.ReferencesHiyan Alshawi.
Processing dictionary definitionswith phrasal pattern hierarchies.
ComputationalLinguistics, 13(3-4):195?202, 1987.Robert Amsler.
The structure of the Merriam-Webster Pocket Dictionary.
PhD thesis, TheUniversity of Texas at Austin, 1980.Mohit Bansal, Kevin Gimpel, and Karen Livescu.Tailoring continuous word representations fordependency parsing.
In Proceedings of the An-nual Meeting of the Association for Computa-tional Linguistics, 2014.Yoshua Bengio, Holger Schwenk, Jean-S?ebastienSen?ecal, Fr?ederic Morin, and Jean-Luc Gau-vain.
Neural probabilistic language models.
InInnovations in Machine Learning, pages 137?186.
Springer, 2003.Danushka Bollegala, Takanori Maehara, YuichiYoshida, and Ken-ichi Kawarabayashi.
Learn-ing word representations from relational graphs.arXiv preprint arXiv:1412.2378, 2014.Martin Chodorow, Roy Byrd, and George Hei-dorn.
Extracting semantic hierarchies from alarge on-line dictionary.
In Proceedings ofthe 23rd Annual Meeting of the Associationfor Computational Linguistics, pages 299?304,Chicago, Illinois, USA, 1985.Ronan Collobert and Jason Weston.
A unifiedarchitecture for natural language processing:Deep neural networks with multitask learning.In Proceedings of the 25th International Con-ference on Machine Learning, pages 160?167.ACM, 2008.Manaal Faruqui, Jesse Dodge, Sujay KumarJauhar, Chris Dyer, Eduard Hovy, and Noah A.Smith.
Retrofitting word vectors to semanticlexicons.
In Proceedings of the 2015 Confer-ence of the North American Chapter of the As-sociation for Computational Linguistics: Hu-man Language Technologies, pages 1606?1615,Denver, Colorado, 2015.
Association for Com-putational Linguistics.Lev Finkelstein, Evgeniy Gabrilovich, Yossi Ma-tias, Ehud Rivlin, Zach Solan, Gadi Wolfman,and Eytan Ruppin.
Placing search in context:The concept revisited.
In Proceedings of the10th International Conference on World WideWeb, pages 406?414.
ACM, 2001.462Zellig Harris.
Distributional structure.
Word, 10(23):146?162, 1954.Felix Hill, Kyunghyun Cho, Sebastien Jean, Co-line Devin, and Yoshua Bengio.
Embeddingword similarity with neural machine translation.arXiv preprint arXiv:1412.6448, 2014a.Felix Hill, Roi Reichart, and Anna Korhonen.Simlex-999: Evaluating semantic models with(genuine) similarity estimation.
arXiv preprintarXiv:1408.3456, 2014b.Eric Huang, Richard Socher, Christopher D Man-ning, and Andrew Ng.
Improving word repre-sentations via global context and multiple wordprototypes.
In Proceedings of the 50th AnnualMeeting of the Association for ComputationalLinguistics: Long Papers-Volume 1, pages 873?882.
Association for Computational Linguis-tics, 2012.Omer Levy and Yoav Goldberg.
Dependency-based word embeddings.
In Proceedings ofthe 52nd Annual Meeting of the Association forComputational Linguistics, volume 2, 2014a.Omer Levy and Yoav Goldberg.
Neural word em-bedding as implicit matrix factorization.
In Ad-vances in Neural Information Processing Sys-tems, pages 2177?2185, 2014b.Tomas Mikolov, Kai Chen, Greg Corrado, and Jef-frey Dean.
Efficient estimation of word repre-sentations in vector space.
In Proceedings ofthe International Conference on Learning Rep-resentations, 2013a.Tomas Mikolov, Wen-tau Yih, and GeoffreyZweig.
Linguistic regularities in continuousspace word representations.
In Proceedings ofHuman Language Technologies: The 2013 An-nual Conference of the North American Chapterof the Association for Computational Linguis-tics, pages 746?751, 2013b.George Miller and Walter Charles.
Contextual cor-relates of semantic similarity.
Language andCognitive Processes, 6(1):1?28, 1991.Andriy Mnih and Geoffrey Hinton.
Three newgraphical models for statistical language mod-elling.
In Proceedings of the 24th InternationalConference on Machine Learning, pages 641?648.
ACM, 2007.Andriy Mnih and Geoffrey E Hinton.
A scalablehierarchical distributed language model.
In Ad-vances in Neural Information Processing Sys-tems, pages 1081?1088, 2009.Courtney Napoles, Matthew Gormley, and Ben-jamin Van Durme.
Annotated Gigaword.
InProceedings of the Joint Workshop on Auto-matic Knowledge Base Construction and Web-scale Knowledge Extraction, pages 95?100.
As-sociation for Computational Linguistics, 2012.Jeffrey Pennington, Richard Socher, and Christo-pher D Manning.
GloVe: Global vectors forword representation.
In Proceedings of the Con-ference on Empirical Methods in Natural Lan-guage Processing, 2014.Richard Reichert, John Olney, and James Paris.Two Dictionary Transcripts and Programs forProcessing Them ?
The Encoding Scheme,Parsent and Conix., volume 1.
DTIC ResearchReport AD0691098, 1969.Herbert Rubenstein and John Goodenough.
Con-textual correlates of synonymy.
Communica-tions of the ACM, 8(10):627?633, 1965.Joseph Turian, Lev Ratinov, and Yoshua Ben-gio.
Word representations: a simple and generalmethod for semi-supervised learning.
In Pro-ceedings of the 48th Annual Meeting of the As-sociation for Computational Linguistics, pages384?394, Uppsala, Sweden, July 2010.Tong Wang and Graeme Hirst.
Exploring pat-terns in dictionary definitions for synonym ex-traction.
Natural Language Engineering, 17,2011.Yinggong Zhao, Shujian Huang, Xinyu Dai, Jian-bing Zhang, and Jiajun Chen.
Learning wordembeddings from dependency relations.
In Pro-ceedings of 2014 International Conference onAsian Language Processing (IALP), pages 123?127.
IEEE, 2014.463
