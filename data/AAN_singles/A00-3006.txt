The use of  error tags in ARTFL 's  Encyclopgdie:Does  good error ident i f i cat ion  lead to good  error cor rect ion?Derr ick  H igg insDepartment ofLinguisticsUniversity of ChicagoAbst ractMany corpora which are prime candidatesfor automatic error correction, such as theoutput of OCR software, and electronic textsincorporating markup tags, include informa-tion on which portions of the text are mostlikely to contain errors.This paper describes how the errormarkup tag <?> is being incorporated inthe spell-checking of an electronic versionof Diderot's Encyclopddie, and evaluateswhether the presence of this tag has signif-icantly aided in correcting the errors whichit marks.
Although the usefulness of errortagging may vary from project to project,even as the precise way in which the taggingis done varies, error tagging does not nec-essarily confer any benefit in attempting tocorrect a given word.
It may, of course, nev-ertheless be useful in marking errors to befixed manually at a later stage of processingthe text.1 The Encyclopddie1.1 P ro jec t  Overv iewThe goal of this project is ultimately todetect and correct all errors in the elec-tronic version of the 18th century Frenchencyclopedia of Diderot and d'Alembert, acorpus of ca.
18 million words.
This text iscurrently under development by the Projectfor American and French Research on theTreasury of the French Language (ARTFL);a project overview and limited sample ofsearchable text from the Encyclopddie areavailable at:http ://humanities.
uchicago, edu/ARTFL/pro j ect s/encyc/.Andreev et al (1999) also provides athorough summary of the goals and statusof the project.The electronic text was largely transcribedfrom the original, although parts of it wereproduced by optical character recognition onscanned images.
Unfortunately, whether asection of text was transcribed or producedby OCR was not recorded at the time of datacapture, so that the error correction strategycannot be made sensitive to this parameter.Judging from a small hand-checked sectionof the text, the error rate is fairly low; aboutone word in 40 contains an error.
It shouldalso be added that the version of the textwith which I am working has already beensubjected to some corrective measures afterthe initial data capture stage.
For exam-ple, common and easily identifiable mistakessuch as the word enfant showing up as en-sant were simply globally repaired through-out the text.
(The original edition of the En-cyclop~die made use of the sharp 's', whichwas often confused with an 'f' during dataentry--cf.
Figure 1.
)At present, my focus is on non-word er-ror detection and correction, although use ofword n-grams eems to be a fairly straight-forward extension to allow for the kind ofcontext-sensitivity in error correction whichhas been the focus of much recent work(cf.
Golding and Roth (1999), Mays et al(1991), Yarowsky (1995)).The approach I am pursuing is an appli-cation of Bayesian statistics.
We treat theprocess by which the electronic text was pro-duced as a noisy channel, and take as ourgoal the maximization of the probability ofeach input word, given a string which is the30Figure 1: Example text from the Encyclopddie.
Note the similarity between the 'f' and theproblematic sharp 's' in signifie;' gn e /ABSI~N'f ,adj .
cn Droit,  nifie n-g I qui-conq,== eft:.61oign~ de fon domicile,~B~r.t~r, Cn mat2er?
de.p.ref'cri#tiO?l, fe  dit de cehdqttl eft: ?ta,~ une autre t,roi, it~ee qile cclle off eft: lept,ff~fl'e.ur d= ftJn, lJt:rlrage.
F .
P/~,r.scrt,t, rlo/,/o' Pru~-sr/?r.
Les al,feat qu i  le font pour l'mt6r~t de l'?
:a~ ?fon~ rt~ptl ~s pr~fe ns, quot ies de commodis ?orum ag~ tur.output of the noisy channel.
If we repre-sent the correct form by we, and the ob-served form by wo, our goal can be describedas finding the wc which maximizes p(wclwo),the probability that wc is the intended form,given that wo is the observed form.By Bayes' rule, this can be reduced tothe problem of maximizing p(wolwc)p(wc) Of pCwo)course, the probability of the observed stringwill be constant across all candidate correc-tions, so the same w~ will also maximizep(wolwc)p(w~).The term p(w~) (the prior probability) canbe estimated by doing frequency counts on acorpus.
In this case, I am using an interpo-lated model of Good-Turing smoothed wordand letter probabilities as the prior.The term p(WolW~) is called the errormodel.
Intuitively, it quantifies the prob-ability of certain kinds of errors resultingfrom the noisy channel.
It is implementedas a confusion matrix, which associates aprobability with each input/output charac-ter pair, representing the probability of theinput character being replaced by the out-put character.
These probabilities can beestimated from a large corpus tagged for er-rors, but since I do not have access to sucha source for this project, I needed to trainthe matrix as described in Kernighan et al(1990).Cf.
Jurafsky and Martin (1999) for an in-troduction to spelling correction using con-fusion matrices, and Kukich (1992) for a sur-vey of different strategies in spelling correc-tion.1.2 T reatment  of <?>A number of different SGML-style tags arecurrently used in the encoding of the En-cyclopddie, ranging from form-related tagssuch as <i> (for italic text), to semanticmarkup tags such as <art ic le>,  to the errortag <?>, the treatment of which is the focusof this article.
The data entry specificationfor the project prescribes the use of <?> inall cases in which the keyboard operator hasany doubt as to the identity of a printedcharacter, and also when symbols appearin the text which cannot be represented inthe Latin-1 codepage (except for Greek text,which is handled by other means).
Otherinstances of the <?> tag were produced asindications of mistakes in OCR output.Some examples of the use of the error tagfrom the actual corpus include the following:<?><?>dartsJ '<?>iab<?>ci<?><?>esd 'aut re<?>alad iesfor a Hebrew Rfor dansfor J'aifor abscissesfor d'autres maladiesThe first is a case where <?> was used tomark an untypeable character.
In the sec-31ond case, it was somehow inserted superflu-ously (most likely by OCR).
In the third row,<?> stands in for a single missing character,and in the fourth it does the same, but threetimes in a single word.
Finally, in the lastrow the error tag indicates the omission ofmultiple characters, and even a word bound-ary.In fact, as Table 1 shows, words which in-clude the error tag generally have error typeswhich are more difficult to correct than av-erage.
Our confusion matrix-based approachis best at handling substitutions (e.g., onfinenfin), deletions (apeUent --~ appellent),and insertions (asselain ~ asselin), and can-not correct words with multiple errors at all.
1"Unrecoverable" rrors are those in which no"correction" is possible, for example, whennon-ASCII symbols occur in the original.The fact that the error tag is used to codesuch a wide variety of irregularities in thecorpus makes it difficult to incorporate intoour general error correction strategy.
Since<?> so often occurred as a replacement for asingle, missing character, however, I treatedit as a character in the language model, butone with an extremely low probability, sothat any suggested correction would have toget rid of it in order to appreciably increasethe probability of the word.In short, <?> is included in the confusionmatrix as a character which may occur as theresult of interference from the noisy chan-nel, but is highly unlikely to appear inde-pendently in the language.
This approachignores the many cases of multiple errors in-dicated by the error tag, but these probablypose too difficult a problem for this stage ofthe project anyhow.
The funding availablefor the project does not currently allow us topursue the possibility of computer-aided er-ror correction; rather, the program must cor-rect as many errors as it can without humanintervention.
Toward this end, we are will-ing to sacrifice the ability to cope with more1 Actually, it does have a mechanism for dealingwith cases such as ab<?>ci<?><?>es, in which theerror tag occurs multiple times, but stands for a sin-gle letter in each case.esoteric error types in order to improve thereliability of the system on other error types.The actual performance of the spellingcorrection algorithm on words which includethe error tag, while comparable to the per-formance on other words, is perhaps not ashigh as we might initially have hoped, giventhat they were already tagged as errors.
Ofthe corrections uggested for words without<?>, 47% were accurate, while of the cor-rections suggested for words with <?>, 29%were accurate.
2 Actually, if we include casesin which the program correctly identified anerror as "unrecoverable", and opted to makeno change, the percentage for <?> sugges-tions rises to 71%.It may seem that these numbers in factundermine the thesis that  the error taggingin the Encyclopddie was not useful in errorcorrection.
I.e., if the correction algorithmexhibits the correct behavior on 47% of un-tagged errors, and on 71% of tagged errors,it seems that  the tags are helping out some-what.
Actually, this is not the case.
First,we should not give the same weight to cor-rect behavior on unrecoverable errors (whichmeans giving up on correction) and correctbehavior on other errors (which means actu-ally finding the correct form).
Second, thetagged errors are often simply 'worse' thanuntagged errors, so that  even if the OCR orkeyboard operator had made a guess at thecorrect form, they would have easily beenidentifiable as errors, and even errors of acertain type.
For example, I maintain thatthe form ab<?>ci<?><?>es would have beenno more difficult to correct had it occurredi ns tead  as ab fc i f fes .2 Conc lus ionIn sum, the errors which are marked withthe <?> tag in the electronic version of the2I admit that these numbers may seem low, butbear in mind that the percentage r flects the accu-racy of the first guess made by the system, since itsoperation is required to be entirely automatic.
Fur-thermore, the correction task is made more difficultby the fact that the corpus is an encyclopedia, whichcontains more infrequent words and proper namesthan most corpora.32Substitution Deletion37.4% 0%Insertion Word-breaking2.2% 0%Multiple16.5% Contains <?>Does notcontain <?> 58.5% 11.6% 6.8% 12.9% 10.2% 0%Unrecoverable44%Table 1: Breakdown of error types, according to whether the word contains <?>Encyclopddie ncompass so many distinct er-ror types, and errors of such difficulty, thatit is hard to come up with corrections formany of them without human intervention.For this reason, experience with the Ency-clopddie project suggests that error taggingis not necessarily a great aid in performingautomatic error correction.There is certainly a great deal of room forfurther investigation into the use of meta-data in spelling correction in general, how-ever.
While the error tag is a somewhatunique member of the tagset, in that it typ-ically flags a subpart of a word, rather thana string of words, this should not be takento mean that it is the only tag which couldbe employed in spelling correction.
If noth-ing else, "wider-scope" markup tags can behelpful in determining when certain parts ofthe corpus should not be seen as represen-tative of the language model, or should beseen as representative of a distinct languagemodel.
(For example, the italic tag <?>.
of-ten marks Latin text in the Encyclopddie.
)Ultimately, I believe that what is neededin order for text tagging to be useful in er-ror correction is a recognition that the tagsetwill influence the correction process.
Tagswhich are applied in such a way as to de-limit sections of text which are relevant ocorrection (such as names, equations, andforeign language text), will be of greater usethan tags which represent a mixture of suchclasses.
Error tagging in particular shouldbe most useful if it does not conflate quitedistinct things that may be "wrong" witha text, such as illegibility of the original,unrenderable symbols, and OCR inaccura-cies.
Such considerations are certainly rele-vant in the evaluation of emerging text en-coding standards, such as the specificationof the Text Encoding Initiative.Re ferencesLeonid Andreev, Jack Iverson, and MarkOlsen.
1999.
Re-engineering a war-machine: ARTFL 's Encyclopddie.
Liter-ary and Linguistic Computing, 14(1):11-28.Denis Diderot and Jean Le Rondd'Alembert, editors.
1976 \[1751-1765\].Encyclopddie, ou Dictionnaire raisonnddes sciences, des arts et des mdtiers.
Re-search Publications, New Haven, Conn.Microfilm.Andrew R. Golding and Dan Roth.
1999.A winnow-based approach to context-sensitive spelling correction.
MachineLearning, 34(1):107-130.Daniel Jurafsky and James Martin.
1999.Speech and Language Processing: An In-troduction to Speech Recognition, NaturalLanguage Processing and ComputationalLinguistics.
Prentice Hall.M.
D. Kernighan, K. W. Church, and W. A.Gale.
1990.
A spelling correction programbased on a noisy channel model.
In Pro-ceedings of the 13th International Confer-ence on Computational Linguistics (COL-ING '90), volume 2, pages 205-211.Karen Kukich.
1992.
Techniques for auto-matically correcting words in text.
A CMComputing Surveys, 24(4):377-439.Eric Mays, Fred J. Damerau, and Robert L.Mercer.
1991.
Context based spelling cor-rection.
Information Processing ~ Man-agement, 27(5):517-522.David Yarowsky.
1995.
Unsupervised wordsense disambiguation rivaling supervisedmethods.
In Proceedings of the 33rd An-33nual Meeting of the Association for Com-putational Linguistics, volume 33, pages189-196.34
