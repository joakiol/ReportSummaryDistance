Proceedings of the 10th Workshop on Multiword Expressions (MWE 2014), pages 17?25,Gothenburg, Sweden, 26-27 April 2014.c?2014 Association for Computational LinguisticsVPCTagger: Detecting Verb-Particle ConstructionsWith Syntax-Based MethodsIstv?an Nagy T.1and Veronika Vincze1,21Department of Informatics, University of Szeged?Arp?ad t?er 2., 6720 Szeged, Hungarynistvan@inf.u-szeged.hu2Hungarian Academy of Sciences, Research Group on Artificial IntelligenceTisza Lajos krt.
103., 6720 Szeged, Hungaryvinczev@inf.u-szeged.huAbstractVerb-particle combinations (VPCs) con-sist of a verbal and a preposition/particlecomponent, which often have some addi-tional meaning compared to the meaningof their parts.
If a data-driven morpholog-ical parser or a syntactic parser is trainedon a dataset annotated with extra informa-tion for VPCs, they will be able to iden-tify VPCs in raw texts.
In this paper,we examine how syntactic parsers performon this task and we introduce VPCTag-ger, a machine learning-based tool that isable to identify English VPCs in context.Our method consists of two steps: it firstselects VPC candidates on the basis ofsyntactic information and then selects gen-uine VPCs among them by exploiting newfeatures like semantic and contextual ones.Based on our results, we see that VPC-Tagger outperforms state-of-the-art meth-ods in the VPC detection task.1 IntroductionVerb-particle constructions (VPCs) are a subclassof multiword expressions (MWEs) that containmore than one meaningful tokens but the wholeunit exhibits syntactic, semantic or pragmaticidiosyncracies (Sag et al., 2002).
VPCs consistof a verb and a preposition/particle (like hand inor go out) and they are very characteristic of theEnglish language.
The particle modifies the mean-ing of the verb: it may add aspectual informa-tion, may refer to motion or location or may totallychange the meaning of the expression.
Thus, themeaning of VPCs can be compositional, i.e.
itcan be computed on the basis of the meaning ofthe verb and the particle (go out) or it can beidiomatic; i.e.
a combination of the given verb andparticle results in a(n unexpected) new meaning(do in ?kill?).
Moreover, as their syntactic sur-face structure is very similar to verb ?
preposi-tional phrase combinations, it is not straightfor-ward to determine whether a given verb + prepo-sition/particle combination functions as a VPC ornot and contextual information plays a very impor-tant role here.
For instance, compare the follow-ing examples: The hitman did in the president andWhat he did in the garden was unbelievable.
Bothsentences contain the sequence did in, but it isonly in the first sentence where it functions as aVPC and in the second case, it is a simple verb-prepositional phrase combination.
For these rea-sons, VPCs are of great interest for natural lan-guage processing applications like machine trans-lation or information extraction, where it is neces-sary to grab the meaning of the text.The special relation of the verb and particlewithin a VPC is often distinctively marked at sev-eral annotation layers in treebanks.
For instance,in the Penn Treebank, the particle is assigned aspecific part of speech tag (RP) and it also hasa specific syntactic label (PRT) (Marcus et al.,1993), see also Figure 1.
This entails that if a data-driven morphological parser or a syntactic parseris trained on a dataset annotated with extra infor-mation for VPCs, it will be able to assign thesekind of tags as well.
In other words, the morpho-logical/syntactic parser itself will be able to iden-tify VPCs in texts.In this paper, we seek to identify VPCs on thebasis of syntactic information.
We first examinehow syntactic parsers perform on Wiki50 (Vinczeet al., 2011), a dataset manually annotated fordifferent types of MWEs, including VPCs.
Wethen present our syntax-based tool called VPC-Tagger to identify VPCs, which consists of twosteps: first, we select VPC candidates (i.e.
verb-preposition/particle pairs) from the text and thenwe apply a machine learning-based technique toclassify them as genuine VPCs or not.
This17The hitman did in the president .rootdet nsubj prtdobjdetpunctFigure 1: A dependency parse of the sentence?The hitman did in the president?.method is based on a rich feature set with newfeatures like semantic or contextual features.
Wecompare the performance of the parsers with thatof our approach and we discuss the reasons for anypossible differences.2 Related WorkRecently, some studies have attempted to iden-tify VPCs.
For instance, Baldwin and Villavicen-cio (2002) detected verb-particle constructions inraw texts with the help of information based onPOS-tagging and chunking, and they also madeuse of frequency and lexical information in theirclassifier.
Kim and Baldwin (2006) built theirsystem on semantic information when decidingwhether verb-preposition pairs were verb-particleconstructions or not.
Nagy T. and Vincze (2011)implemented a rule-based system based on mor-phological features to detect VPCs in raw texts.The (non-)compositionality of verb-particlecombinations has also raised interest amongresearchers.
McCarthy et al.
(2003) implementeda method to determine the compositionality ofVPCs and Baldwin (2005) presented a dataset inwhich non-compositional VPCs could be found.Villavicencio (2003) proposed some methods toextend the coverage of available VPC resources.Tu and Roth (2012) distinguished genuineVPCs and verb-preposition combinations in con-text.
They built a crowdsourced corpus of VPCcandidates in context, where each candidate wasmanually classified as a VPC or not.
How-ever, during corpus building, they applied lexi-cal restrictions and concentrated only on VPCsformed with six verbs.
Their SVM-based algo-rithm used syntactic and lexical features to clas-sify VPCs candidates and they concluded that theirsystem achieved good results on idiomatic VPCs,but the classification of more compositional VPCsis more challenging.Since in this paper we focus on syntax-basedVPC identification more precisely, we also iden-tify VPCs with syntactic parsers, it seems nec-essary to mention studies that experimented withparsers for identifying different types of MWEs.For instance, constituency parsing models wereemployed in identifying contiguous MWEs inFrench and Arabic (Green et al., 2013).
Theirmethod relied on a syntactic treebank, an MWElist and a morphological analyzer.
Vincze et al.
(2013) employed a dependency parser for identi-fying light verb constructions in Hungarian textsas a ?side effect?
of parsing sentences and reportstate-of-the-art results for this task.Here, we make use of parsers trained on thePenn Treebank (which contains annotation forVPCs) and we evaluate their performance on theWiki50 corpus, which was manually annotated forVPCs.
Thus, we first examine how well theseparsers identify VPCs (i.e.
assigning VPC-specificsyntactic labels) and then we present how VPC-Tagger can carry out this task.
First, we selectVPC candidates from raw text and then, we clas-sify them as genuine VPCs or not.3 Verb-particle Constructions in EnglishAs mentioned earlier, verb-particle constructionsconsist of a verb and a particle.
Similar construc-tions are present in several languages, althoughthere might be different grammatical or ortho-graphic norms for such verbs in those languages.For instance, in German and in Hungarian, the par-ticle usually precedes the verb and they are spelt asone word, e.g.
aufmachen (up.make) ?to open?
inGerman or kinyitni (out.open) ?to open?
in Hun-garian.
On the other hand, languages like Swedish,Norwegian, Icelandic and Italian follow the samepattern as English; namely, the verb precedes theparticle and they are spelt as two words (Masini,2005).
These two typological classes require dif-ferent approaches if we would like identify VPCs.For the first group, morphology-based solutionscan be implemented that can identify the inter-nal structure of compound words.
For the secondgroup, syntax-based methods can also be success-ful, which take into account the syntactic relationbetween the verb and the particle.Many of the VPCs are formed with a motionverb and a particle denoting directions (like goout, come in etc.)
and their meaning reflects this:they denote a motion or location.
The meaningof VPCs belonging to this group is usually trans-18parent and thus they can be easily learnt by sec-ond language learners.
In other cases, the particleadds some aspectual information to the meaningof the verb: eat up means ?to consume totally?or burn out means ?to reach a state where some-one becomes exhausted?.
These VPCs still have acompositional meaning, but the particle has a non-directional function here, but rather an aspectualone (cf.
Jackendoff (2002)).
Yet other VPCs havecompletely idiomatic meanings like do up ?repair?or do in ?kill?.
In the latter cases, the meaningof the construction cannot be computed from themeaning of the parts, hence they are problematicfor both language learners and NLP applications.Tu and Roth (2012) distinguish between twosets of VPCs in their database: the more com-positional and the more idiomatic ones.
Dif-ferentiating between compositional and idiomaticVPCs has an apt linguistic background as well (seeabove) and it may be exploited in some NLP appli-cations like machine translation (parts of compo-sitional VPCs may be directly translated whileidiomatic VPCs should be treated as one unit).However, when grouping their data, Tu and Rothjust consider frequency data and treat one VPCas one lexical entry.
This approach is some-what problematic as many VPCs in their datasetare highly ambiguous and thus may have moremeanings (like get at, which can mean ?criticise?,?mean?, ?get access?, ?threaten?)
and some ofthem may be compositional, while others are not.Hence, clustering all these meanings and classify-ing them as either compositional or idiomatic maybe misleading.
Instead, VPC and non-VPC usesof one specific verb-particle combination could betruly distinguished on the basis of frequency data,or, on the other hand, a word sense disambigua-tion approach may give an account of the compo-sitional or idiomatic uses of the specific unit.In our experiments, we use the Wiki50 corpus,in which VPCs are annotated in raw text, but nosemantic classes are further distinguished.
Hence,our goal here is not the automatic semantic classi-fication of VPCs because we believe that first theidentification of VPCs in context should be solvedand then in a further step, genuine VPCs might beclassified as compositional or idiomatic, given amanually annotated dataset from which this kindof information may be learnt.
This issue will beaddressed in a future study.Figure 2: System Architecture4 VPC DetectionOur goal is to identify each individual VPC in run-ning texts; i.e.
to take individual inputs like Howdid they get on yesterday?
and mark each VPC inthe sentence.
Our tool called VPCTagger is basedon a two-step approach.
First, we syntacticallyparse each sentence, and extract potential VPCswith a syntax-based candidate extraction method.Afterwards, a binary classification can be usedto automatically classify potential VPCs as VPCsor not.
For the automatic classification of candi-date VPCs, we implemented a machine learningapproach, which is based on a rich feature set withnew features like semantic and contextual features.Figure 2 outlines the process used to identify eachindividual VPC in a running text.4.1 CorporaTo evaluate of our methods, we made use of twocorpora.
Statistical data on the corpora can be seenin Table 1.
First, we used Wiki50 (Vincze et al.,2011), in which several types of multiword expres-sions (including VPCs) and Named Entities weremarked.
This corpus consists of 50 Wikipediapages, and contains 466 occurrences of VPCs.Corpus Sentences Tokens VPCs #Wiki50 4,350 114,570 466 342Tu&Roth 1,348 38,132 878 23Table 1: Statistical data on the corpora.In order to compare the performance of our sys-tem with others, we also used the dataset of Tuand Roth (2012), which contains 1,348 sentencestaken from different parts of the British NationalCorpus.
However, they only focused on VPCs inthis dataset, where 65% of the sentences contain19a phrasal verb and 35% contain a simplex verb-preposition combination.
As Table 1 indicates,the Tu&Roth dataset only focused on 23 differentVPCs, but 342 unique VPCs were annotated in theWiki50 corpus.4.2 Candidate ExtractionIn this section, we concentrate on the first step ofour approach, namely how VPC candidates can beselected from texts.
As we mentioned in Section1, our hypothesis is that the automatic detection ofVPCs can be basically carried out by dependencyparsers.
Thus, we examined the performance oftwo parsers on VPC-specific syntactic labels.As we had a full-coverage VPC annotated cor-pus where each individual occurrence of a VPCwas manually marked, we were able to exam-ine the characteristics of VPCs in a running textand evaluate the effectiveness of the parsers onthis task.
Therefore, here we examine depen-dency relations among the manually annotatedgold standard VPCs, provided by the Stanfordparser (Klein and Manning, 2003) and the Bohnetparser (Bohnet, 2010) for the Wiki50 corpus.
Inorder to compare the efficiency of the parsers, bothwere applied using the same dependency represen-tation.
We found that only 52.57% and 58.16% ofthe annotated VPCs in Wiki50 had a verb-particlesyntactic relation when we used the Stanford andBohnet parsers, respectively.
As Table 2 shows,there are several other syntactic constructions inwhich VPCs may occur.Edge type Stanford Bohnet# % # %prt 235 52.57 260 58.16prep 23 5.15 107 23.94advmod 56 12.52 64 14.32sum 314 70.24 431 96.42other 8 1.79 1 0.22none 125 27.97 15 3.36sum 447 100.00 447 100.00Table 2: Edge types in the Wiki50 corpus.
prt: par-ticle.
prep: preposition.
advmod: adverbial mod-ifier.
other: other dependency labels.
none: nodirect syntactic connection between the verb andparticle.Therefore, we extended our candidate extrac-tion method, where besides the verb-particledependency relation, the preposition and adver-bial modifier syntactic relations were also investi-gated among verbs and particles.
With this modifi-cation, 70.24% and 96.42% of VPCs in the Wiki50corpus could be identified.
In this phase, we foundthat the Bohnet parser was more successful onthe Wiki50 corpus, i.e.
it could cover more VPCs,hence we applied the Bohnet parser in our furtherexperiments.Some researchers filtered LVC candidates byselecting only certain verbs that may be partof the construction.
One example is Tu andRoth (2012), where the authors examined a verb-particle combination only if the verbal compo-nents were formed with one of the previouslygiven six verbs (i.e.
make, take, have, give, do,get).Since Wiki50 was annotated for all VPC occur-rences, we were able to check what percentage ofVPCs could be covered if we applied this selec-tion.
As Table 3 shows, the six verbs used by Tuand Roth (2012) are responsible for only 50 VPCson the Wiki50 corpus, so it covers only 11.16% ofall gold standard VPCs.Table 4 lists the most frequent VPCs and theverbal components on the Wiki50 corpus.
Ascan be seen, the top 10 VPCs are responsiblefor only 17.41% of the VPC occurrences, whilethe top 10 verbal components are responsible for41.07% of the VPC occurrences in the Wiki50 cor-pus.
Furthermore, 127 different verbal compo-nent occurred in Wiki50, but the verbs have anddo ?
which are used by Tu and Roth (2012) ?do not appear in the corpus as verbal componentof VPCs.
All this indicates that applying lexicalrestrictions and focusing on a reduced set of verbswill lead to the exclusion of a considerable numberof VPCs occurring in free texts and so, real-worldtasks would hardly profit from them.verb #take 27get 10give 5make 3have 0do 0sum 50Table 3: The frequency of verbs on the Wiki50corpus used by Tu and Roth (2012).20VPC # verb #call for 11 set 28point out 9 take 27carry out 9 turn 26set out 8 go 21grow up 8 call 21set up 7 come 15catch up 7 carry 13turn on 7 look 13take up 6 break 10pass on 6 move 10sum 78 sum 184Table 4: The most frequent VPCs and verbal com-ponents on the Wiki50 corpus.4.3 Machine Learning Based CandidateClassicationIn order to perform an automatic classificationof the candidate VPCs, a machine learning-basedapproach was implemented, which will be elabo-rated upon below.
This method is based on a richfeature set with the following categories: ortho-graphic, lexical, syntactic, and semantic.
More-over, as VPCs are highly ambiguous in raw texts,contextual features are also required.?
Orthographic features: Here, we examinedwhether the candidate consists of two ormore tokens.
Moreover, if the particle com-ponent started with ?a?, which prefix, inmany cases, etymologically denotes a move-ment (like across and away), it was also notedand applied as a feature.?
Lexical features: We exploited the fact thatthe most common verbs occur most fre-quently in VPCs, so we selected fifteen verbsfrom the most frequent English verbs1.
Here,we examined whether the lemmatised verbalcomponent of the candidate was one of thesefifteen verbs.
We also examined whetherthe particle component of the potential VPCoccurred among the common English parti-cles.
Here, we apply a manually built par-ticle list based on linguistic considerations.Moreover, we also checked whether a poten-tial VPC is contained in the list of typicalEnglish VPCs collected by Baldwin (2008).1http://en.wikipedia.org/wiki/Most common words in English?
Syntactic features: the dependency labelbetween the verb and the particle can also beexploited in identifying LVCs.
As we typ-ically found when dependency parsing thecorpus, the syntactic relation between theverb and the particle in a VPC is prt, prepor advmod ?
applying the Stanford parserdependency representation, hence these syn-tactic relations were defined as features.
Ifthe candidate?s object was a personal pro-noun, it was also encoded as another syntac-tic feature.?
Semantic features: These features were basedon the fact that the meaning of VPCs maytypically reflect a motion or location like goon or take away.
First, we examine that theverbal component is a motion verb like goor turn, or the particle indicates a directionlike out or away.Moreover, the semantic type of the prepo-sitional object, object and subject in thesentence can also help to decide whetherthe candidate is a VPC or not.
Conse-quently, the person, activity, animal,artifact and concept semantic senseswere looked for among the upper level hyper-onyms of the nominal head of the preposi-tional object, object and subject in PrincetonWordNet 3.12.When several different machine learning algo-rithms were experimented on this feature set, thepreliminary results showed that decision trees per-formed the best on this task.
This is probably dueto the fact that our feature set consists of a fewcompact (i.e.
high-level) features.
The J48 clas-sifier of the WEKA package (Hall et al., 2009)was trained with its default settings on the above-mentioned feature set, which implements the C4.5(Quinlan, 1993) decision tree algorithm.
More-over, Support Vector Machines (SVM) (Cortes andVapnik, 1995) results are also reported to comparethe performance of our methods with that of Tuand Roth (2012).As the investigated corpora were not sufficientlylarge for splitting them into training and test setsof appropriate size, we evaluated our models in across validation manner on the Wiki50 corpus andthe Tu&Roth dataset.2http://wordnetweb.princeton.edu/perl/webwn21As Tu and Roth (2012) presented only the accu-racy scores on the Tu & Roth dataset, we alsoemployed an accuracy score as an evaluation met-ric on this dataset, where positive and negativeexamples were also marked.
But, in the caseof Wiki50 corpus, where only the positive VPCswere manually annotated, the F?=1score wasemployed and interpreted on the positive classas an evaluation metric.
Moreover, all potentialVPCs were treated as negative that were extractedby the candidate extraction method but were notmarked as positive in the gold standard.
Thus, inthe resulting dataset negative examples are over-represented.As Table 2 shows, the candidate extractionmethod did not cover all manually annotatedVPCs in the Wiki50 corpus.
Hence, we treated theomitted LVCs as false negatives in our evaluation.As a baseline, we applied a context-free dictio-nary lookup method.
In this case, we applied thesame VPC list that was described among the lex-ical features.
Then we marked candidates of thesyntax-based method as VPC if the candidate VPCwas found in the list.
We also compared our resultswith the rule-based results available for Wiki50(Nagy T. and Vincze, 2011) and also with the 5-fold cross validation results of Tu and Roth (2012).5 ResultsTable 5 lists the results obtained using the base-line dictionary lookup, rule-based method, depen-dency parsers and machine learning approacheson the Wiki50 corpus.
It is revealed that thedictionary lookup method performed worst andachieved an F-score of 35.43.
Moreover, thismethod only achieved a precision score of 49.77%.However, the rule-based method achieved thehighest precision score with 91.26%, but thedependency parsers also got high precision scoresof about 90% on Wiki50.
It is also clear that themachine learning-based approach, the VPCTag-ger, is the most successful method on Wiki50: itachieved an F-score 10 points higher than thosefor the rule-based method and dependency parsersand more than 45 points higher than that for thedictionary lookup.In order to compare the performance of our sys-tem with others, we evaluated it on the Tu&Rothdataset (Tu and Roth, 2012).
Table 6 compares theresults achieved by the dictionary lookup and therule-based method on the Tu&Roth dataset.
More-Method Prec.
Rec.
F-scoreDictionary Lookup 49.77 27.5 35.43Rule-based 91.26 58.52 71.31Stanford Parser 91.09 52.57 66.67Bohnet Parser 89.04 58.16 70.36ML J48 85.7 76.79 81.0ML SVM 89.07 65.62 75.57Table 5: Results obtained in terms of precision,recall and F-score.over, it also lists the results of Tu and Roth (2012)and the VPCTagger evaluated in the 5-fold crossvalidation manner, as Tu and Roth (2012) appliedthis evaluation schema.
As in the Tu&Roth datasetpositive and negative examples were also marked,we were able to use accuracy as evaluation met-ric besides the F?=1scores.
It is revealed thatthe dictionary lookup and the rule-based methodachieved an F-score of about 50, but our methodseems the most successful on this dataset, as it canyield an accuracy 3.32% higher than that for theTu&Roth system.Method Accuracy F-scoreDictionary Lookup 51.13 52.24Rule Based 56.92 43.84VPCTagger 81.92 85.69Tu&Roth 78.6% ?Table 6: 5-fold cross validation results on theTu&Roth dataset in terms of accuracy and F-score.6 DiscussionThe applied machine learning-based methodextensively outperformed our dictionary lookupand rule-based baseline methods, which under-lines the fact that our approach can be suitablyapplied to VPC detection in raw texts.
It iswell demonstrated that VPCs are very ambigu-ous in raw text, as the dictionary lookup methodonly achieved a precision score of 49.77% on theWiki50 corpus.
This demonstrates that the auto-matic detection of VPCs is a challenging task andcontextual features are essential.
In the case of thedictionary lookup, to achieve a higher recall scorewas mainly limited by the size of the dictionaryused.As Table 5 shows, VPCTagger achieved an F-score 10% higher than those for the dependency22parsers, which may refer to the fact that ourmachine learning-based approach performed wellon this task.
This method proved to be the mostbalanced as it got roughly the same recall, preci-sion and F-score results on the Wiki50 corpus.
Inaddition, the dependency parsers achieve high pre-cision with lower recall scores.Moreover, the results obtained with ourmachine learning approach on the Tu&Rothdataset outperformed those reported in Tu andRoth (2012).
This may be attributed to the inclu-sion of a rich feature set with new features likesemantic and contextual features that were used inour system.As Table 6 indicates, the dictionary lookupand rule-based methods were less effective whenapplied on the Tu&Roth dataset.
Since the corpuswas created by collecting sentences that containedphrasal verbs with specific verbs, this dataset con-tains a lot of negative and ambiguous examplesbesides annotated VPCs, hence the distribution ofVPCs in the Tu&Roth dataset is not comparableto those in Wiki50, where each occurrence of aVPCs were manually annotated in a running text.Moreover, in this dataset, only one positive or neg-ative example was annotated in each sentence, andthey examined just the verb-particle pairs formedwith the six verbs as a potential VPC.
However,the corpus probably contains other VPCs whichwere not annotated.
For example, in the sentenceThe agency takes on any kind of job ?
you justname the subject and give us some indication ofthe kind of thing you want to know, and then wego out and get it for you., the only phrase takes onwas listed as a positive example in the Tu&Rothdataset.
But two examples, (go out ?
positive andget it for ?
negative) were not marked.
This isproblematic if we would like to evaluate our can-didate extractor on this dataset as it would identifyall these phrases, even if it is restricted to verb-particle pairs containing one of the six verbs men-tioned above, thus yielding false positives alreadyin the candidate extraction phase.In addition, this dataset contains 878 positiveVPC occurrences, but only 23 different VPCs.Consequently, some positive examples were over-represented.
But the Wiki50 corpus may con-tain some rare examples and it probably reflectsa more realistic distribution as it contains 342unique VPCs.A striking difference between the Tu & Rothdatabase and Wiki50 is that while Tu and Roth(2012) included the verbs do and have in theirdata, they do not occur at all among the VPCscollected from Wiki50.
Moreover, these verbs arejust responsible for 25 positive VPCs examples inthe Tu & Roth dataset.
Although these verbs arevery frequent in language use, they do not seemto occur among the most frequent verbal compo-nents concerning VPCs.
A possible reason for thismight be that VPCs usually contain a verb refer-ring to movement in its original sense and neitherhave nor do belong to motion verbs.An ablation analysis was carried out to examinethe effectiveness of each individual feature typesof the machine learning based candidate classifi-cation.
Besides the feature classification describedin Section 4.3, we also examined the effectivenessof the contextual features.
In this case, the featurewhich examined whether the candidates objectwas a personal pronoun or not and the semantictype of the prepositional object, object and subjectwere treated as contextual features.
Table 7 showsthe usefulness of each individual feature type onthe Wiki50 corpus.
For each feature type, a J48classifier was trained with all of the features exceptthat one.
Then we compared the performance tothat got with all the features.
As the ablation anal-ysis shows, each type of feature contributed to theoverall performance.
We found that the lexicaland orthographic features were the most powerful,the semantic, syntactic features were also useful;while contextual features were less effective, butwere still exploited by the model.Features Prec.
Rec.
F-score Diff.All 85.7 76.79 81.0 ?Semantic 86.55 66.52 75.22 -5.78Orthographic 83.26 65.85 73.54 -7.46Syntax 84.31 71.88 77.6 -3.4Lexical 89.68 60.71 72.41 -8.59Contextual 86.68 74.55 80.16 -0.84Table 7: The usefulness of individual features interms of precision, recall and F-score using theWiki50 corpus.The most important features in our system arelexical ones, namely, the lists of the most frequentEnglish verbs and particles.
It is probably due tothe fact that the set of verbs used in VPCs is ratherlimited, furthermore, particles form a closed wordclass that is, they can be fully listed, hence the par-23ticle component of a VPC will necessarily comefrom a well-defined set of words.Besides the ablation analysis, we also investi-gated the decision tree model produced by ourexperiments.
The model profited most from thesyntactic and lexical features, i.e.
the dependencylabel provided by the parsers between the verb andthe particle also played an important role in theclassification process.We carried out a manual error analysis in orderto find the most typical errors our system made.Most errors could be traced back to POS-taggingor parsing errors, where the particle was classi-fied as a preposition.
VPCs that include an adverb(as labeled by the POS tagger and the parser)were also somewhat more difficult to identify, likecome across or go back.
Preposition stranding (ine.g.
relative clauses) also resulted in false positiveslike in planets he had an adventure on.Other types of multiword expressions were alsoresponsible for errors.
For instance, the systemclassified come out as a VPC within the idiomcome out of the closet but the gold standard anno-tation in Wiki50 just labeled the phrase as an idiomand no internal structure for it was marked.
A sim-ilar error could be found for light verb construc-tions, for example, run for office was marked asa VPC in the data, but run for was classified asa VPC, yielding a false positive case.
Multiwordprepositions like up to also led to problems: inhe taught up to 1986, taught up was erroneouslylabeled as VPC.
Finally, in some cases, annotationerrors in the gold standard data were the source ofmislabeled candidates.7 ConclusionsIn this paper, we focused on the automatic detec-tion of verb-particle combinations in raw texts.Our hypothesis was that parsers trained on textsannotated with extra information for VPCs canidentify VPCs in texts.
We introduced ourmachine learning-based tool called VPCTagger,which allowed us to automatically detect VPCsin context.
We solved the problem in a two-stepapproach.
In the first step, we extracted poten-tial VPCs from a running text with a syntax-based candidate extraction method and we applieda machine learning-based approach that made useof a rich feature set to classify extracted syntacticphrases in the second step.
In order to achieve agreater efficiency, we defined several new featureslike semantic and contextual, but according to ourablation analysis we found that each type of fea-tures contributed to the overall performance.Moreover, we also examined how syntacticparsers performed in the VPC detection task onthe Wiki50 corpus.
Furthermore, we comparedour methods with others when we evaluated ourapproach on the Tu&Roth dataset.
Our methodyielded better results than those got using thedependency parsers on the Wiki50 corpus and themethod reported in (Tu and Roth, 2012) on theTu&Roth dataset.Here, we also showed how dependency parsersperformed on identifying VPCs, and our resultsindicate that although the dependency label pro-vided by the parsers is an essential feature indetermining whether a specific VPC candidate isa genuine VPC or not, the results can be furtherimproved by extending the system with additionalfeatures like lexical and semantic features.
Thus,one possible application of the VPCTagger may beto help dependency parsers: based on the outputof VPCTagger, syntactic labels provided by theparsers can be overwritten.
With backtracking, theaccuracy of syntactic parsers may increase, whichcan be useful for a number of higher-level NLPapplications that exploit syntactic information.In the future, we would like to improve oursystem by defining more complex contextual fea-tures.
We also plan to examine how the VPCTag-ger improve the performance of higher level NLPapplications like machine translation systems, andwe would also like to investigate the systematicdifferences among the performances of the parsersand VPCTagger, in order to improve the accuracyof parsing.
In addition, we would like to com-pare different automatic detection methods of mul-tiword expressions, as different types of MWEsare manually annotated in the Wiki50 corpus.AcknowledgmentsIstv?an Nagy T. was partially funded by theNational Excellence Program T?AMOP-4.2.4.A/2-11/1-2012-0001 of the State of Hungary, co-financed by the European Social Fund.
VeronikaVincze was funded in part by the EuropeanUnion and the European Social Fund through theproject FuturICT.hu (grant no.
: T?AMOP-4.2.2.C-11/1/KONV-2012-0013).24ReferencesTimothy Baldwin and Aline Villavicencio.
2002.Extracting the unextractable: A case study on verb-particles.
In Proceedings of the 6th Conference onNatural Language Learning - Volume 20, COLING-02, pages 1?7, Stroudsburg, PA, USA.
Associationfor Computational Linguistics.Timothy Baldwin.
2005.
Deep lexical acquisition ofverb-particle constructions.
Computer Speech andLanguage, 19(4):398?414, October.Timothy Baldwin.
2008.
A resource for evaluatingthe deep lexical acquisition of English verb-particleconstructions.
In Proceedings of the LREC Work-shop Towards a Shared Task for Multiword Expres-sions (MWE 2008), pages 1?2.Bernd Bohnet.
2010.
Top accuracy and fast depen-dency parsing is not a contradiction.
In Proceedingsof Coling 2010, pages 89?97.Corinna Cortes and Vladimir Vapnik.
1995.
Support-vector networks, volume 20.
Kluwer AcademicPublishers.Spence Green, Marie-Catherine de Marneffe, andChristopher D. Manning.
2013.
Parsing models foridentifying multiword expressions.
ComputationalLinguistics, 39(1):195?227.Mark Hall, Eibe Frank, Geoffrey Holmes, BernhardPfahringer, Peter Reutemann, and Ian H. Witten.2009.
The WEKA data mining software: an update.SIGKDD Explorations, 11(1):10?18.Ray Jackendoff.
2002.
English particle constructions,the lexicon, and the autonomy of syntax.
In NicoleDeh, Ray Jackendoff, Andrew McIntyre, and SilkeUrban, editors, Verb-Particle Explorations, pages67?94, Berlin / New York.
Mouton de Gruyter.Su Nam Kim and Timothy Baldwin.
2006.
Automaticidentification of English verb particle constructionsusing linguistic features.
In Proceedings of the ThirdACL-SIGSEM Workshop on Prepositions, pages 65?72.Dan Klein and Christopher D. Manning.
2003.
Accu-rate unlexicalized parsing.
In Annual Meeting of theACL, volume 41, pages 423?430.Mitchell P. Marcus, Beatrice Santorini, and Mary AnnMarcinkiewicz.
1993.
Building a Large AnnotatedCorpus of English: The Penn Treebank.
Computa-tional Linguistics, 19(2):313?331.Francesca Masini.
2005.
Multi-word expressionsbetween syntax and the lexicon: The case of Italianverb-particle constructions.
SKY Journal of Linguis-tics, 18:145?173.Diana McCarthy, Bill Keller, and John Carroll.2003.
Detecting a continuum of compositional-ity in phrasal verbs.
In Proceedings of the ACL2003 Workshop on Multiword Expressions: Analy-sis, Acquisition and Treatment - Volume 18, MWE?03, pages 73?80, Stroudsburg, PA, USA.
Associa-tion for Computational Linguistics.Istv?an Nagy T. and Veronika Vincze.
2011.
Identify-ing Verbal Collocations in Wikipedia Articles.
InProceedings of the 14th International Conferenceon Text, Speech and Dialogue, TSD?11, pages 179?186, Berlin, Heidelberg.
Springer-Verlag.Ross Quinlan.
1993.
C4.5: Programs for MachineLearning.
Morgan Kaufmann Publishers, SanMateo, CA.Ivan A.
Sag, Timothy Baldwin, Francis Bond, AnnCopestake, and Dan Flickinger.
2002.
MultiwordExpressions: A Pain in the Neck for NLP.
InProceedings of CICLing 2002, pages 1?15, MexicoCity, Mexico.Yuancheng Tu and Dan Roth.
2012.
Sorting out theMost Confusing English Phrasal Verbs.
In Proceed-ings of the First Joint Conference on Lexical andComputational Semantics - Volume 1: Proceedingsof the Main Conference and the Shared Task, andVolume 2: Proceedings of the Sixth InternationalWorkshop on Semantic Evaluation, SemEval ?12,pages 65?69, Stroudsburg, PA, USA.
Associationfor Computational Linguistics.Aline Villavicencio.
2003.
Verb-particle constructionsand lexical resources.
In Proceedings of the ACL2003 Workshop on Multiword Expressions: Analy-sis, Acquisition and Treatment - Volume 18, MWE?03, pages 57?64, Stroudsburg, PA, USA.
Associa-tion for Computational Linguistics.Veronika Vincze, Istv?an Nagy T., and G?abor Berend.2011.
Multiword Expressions and Named Entitiesin the Wiki50 Corpus.
In Proceedings of RANLP2011, pages 289?295, Hissar, Bulgaria, September.RANLP 2011 Organising Committee.Veronika Vincze, J?anos Zsibrita, and Istv?an Nagy T.2013.
Dependency Parsing for Identifying Hungar-ian Light Verb Constructions.
In Proceedings ofthe Sixth International Joint Conference on Natu-ral Language Processing, pages 207?215, Nagoya,Japan, October.
Asian Federation of Natural Lan-guage Processing.25
