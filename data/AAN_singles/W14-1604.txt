Proceedings of the Eighteenth Conference on Computational Language Learning, pages 30?38,Baltimore, Maryland USA, June 26-27 2014.c?2014 Association for Computational LinguisticsAutomatic Transliteration of Romanized Dialectal ArabicMohamed Al-Badrashiny?, Ramy Eskander, Nizar Habash and Owen Rambow?Department of Computer Science, The George Washington University, Washington, DC?badrashiny@gwu.eduCenter for Computational Learning Systems, Columbia University, NYC, NY{reskander,habash,rambow}@ccls.columbia.eduAbstractIn this paper, we address the problemof converting Dialectal Arabic (DA) textthat is written in the Latin script (calledArabizi) into Arabic script following theCODA convention for DA orthography.The presented system uses a finite statetransducer trained at the character levelto generate all possible transliterations forthe input Arabizi words.
We then filterthe generated list using a DA morpholog-ical analyzer.
After that we pick the bestchoice for each input word using a lan-guage model.
We achieve an accuracy of69.4% on an unseen test set compared to63.1% using a system which represents apreviously proposed approach.1 IntroductionThe Arabic language is a collection of varieties:Modern Standard Arabic (MSA), which is usedin formal settings and has a standard orthogra-phy, and different forms of Dialectal Arabic (DA),which are commonly used informally and with in-creasing presence on the web, but which do nothave standard orthographies.
While both MSAand DA are commonly written in the Arabic script,DA (and less so MSA) is sometimes written inthe Latin script.
This happens when using an Ara-bic keyboard is dispreferred or impossible, for ex-ample when communicating from a mobile phonethat has no Arabic script support.
Arabic writtenin the Latin script is often referred to as ?Arabizi?.Arabizi is not a letter-based transliteration fromthe Arabic script as is, for example, the Buck-walter transliteration (Buckwalter, 2004).
Instead,roughly speaking, writers use sound-to-letter rulesinspired by those of English1as well as informally1In different parts of the Arab World, the basis for theLatin script rendering of DA may come from different lan-established conventions to render the sounds of theDA sentence.
Because the sound-to-letter rulesof English are very different from those of Ara-bic, we obtain complex mappings between the twowriting systems.
This issue is compounded by theunderlying problem that DA itself does not haveany standard orthography in the Arabic script.
Ta-ble 1 shows different plausible ways of writing anEgyptian Arabic (EGY) sentence in Arabizi andin Arabic script.Arabizi poses a problem for natural languageprocessing (NLP).
While some tools have recentlybecome available for processing EGY input, e.g.,(Habash et al., 2012b; Habash et al., 2013; Pashaet al., 2014), they expect Arabic script input (or aBuckwalter transliteration).
They cannot processArabizi.
We therefore need a tool that convertsfrom Arabizi to Arabic script.
However, the lackof standard orthography in EGY compounds theproblem: what should we convert Arabizi into?Our answer to this question is to use CODA, aconventional orthography created for the purposeof supporting NLP tools (Habash et al., 2012a).The goal of CODA is to reduce the data sparsenessthat comes from the same word form appearing inmany spontaneous orthographies in data (be it an-notated or unannotated).
CODA has been definedfor EGY as well as Tunisian Arabic (Zribi et al.,2014), and it has been used as part of different ap-proaches for modeling DA morphology (Habashet al., 2012b), tagging (Habash et al., 2013; Pashaet al., 2014) and spelling correction (Eskander etal., 2013; Farra et al., 2014).This paper makes two main contributions.
First,we clearly define the computational problem oftransforming Arabizi to CODA.
This improvesover previous work by unambiguously fixing theguages that natively uses the Latin script, such as Englishor French.
In this paper, we concentrate on Egyptian Arabic,which uses English as its main source of sound-to-letter rules.30target representation for the transformation.
Sec-ond, we perform experiments using different com-ponents in a transformation pipeline, and showthat a combination of character-based transduc-tion, filtering using a morphological analyzer, andusing a language model outperforms other archi-tectures, including the state-of-the-art system de-scribed in Darwish (2013).
Darwish (2013) pre-sented a conversion tool, but did not discuss con-version into a conventionalized orthography, anddid not investigate different architectures.
Weshow in this paper that our proposed architecture,which includes an EGY morphological analyzer,improves over Darwish?s architecture.This paper is structured as follows.
We start outby presenting relevant linguistic facts (Section 2)and then we discuss related work.
We present ourapproach in Section 4 and our experiments and re-sults in Section 5.2 Linguistic Facts2.1 EGY Spontaneous OrthographyAn orthography is a specification of how to usea particular writing system (script) to write thewords of a particular language.
In cases wherethere is no standard orthography, people use aspontaneous orthography that is based on dif-ferent criteria.
The main criterion is phonol-ogy: how to render a word pronunciation inthe given writing system.
This mainly de-pends on language-specific assumptions about thegrapheme-to-phoneme mapping.
Another crite-rion is to use cognates in a related language (sim-ilar language or a language variant), where twowords represent a cognate if they are related et-ymologically and have the same meaning.
Ad-ditionally, a spontaneous orthography may be af-fected by speech effects, which are the lengthen-ing of specific syllables to show emphasis or othereffects (such asQJJJ?
ktyyyyr2?veeeery?
).EGY has no standard orthography.
Instead,it has a spontaneous orthography that is relatedto the standard orthography of Modern StandardArabic.
Table 1 shows an example of writing asentence in EGY spontaneous orthography in dif-ferent variants.2Arabic transliteration is presented in the Habash-Soudi-Buckwalter scheme (Habash et al., 2007): (in alphabeticalorder) Abt?jHxd?rzs?SDT?D?
?fqklmnhwy and the additionalsymbols: ?
Z, ?
@,?A @,?A@, ?w?
', ?y Z?
', h??, ?
?.2.2 ArabiziArabizi is a spontaneous orthography used to writeDA using the Latin script, the so-called Arabicnumerals, and other symbols commonly found onvarious input devices such as punctuation.
Arabiziis commonly used by Arabic speakers to write insocial media and SMS and chat applications.The orthography decisions made for writingin Arabizi mainly depend on a phoneme-to-grapheme mapping between the Arabic pronunci-ation and the Latin script.
This is largely basedon the phoneme-to-grapheme mapping used in En-glish.
Crucially, Arabizi is not a simple transliter-ation of Arabic, under which each Arabic letter insome orthography is replaced by a Latin letter (asis the case in the Buckwalter transliteration usedwidely in natural language processing but nowhereelse).
As a result, it is not straightforward to con-vert Arabizi to Arabic.
We discuss some specificaspects of Arabizi.Vowels While EGY orthography omits vocalicdiacritics representing short vowels, Arabizi usesthe Latin script symbols for vowels (a, e, i, o, u, y)to represent EGY?s short and long vowels, makingthem ambiguous.
In some cases, Arabizi wordsomit short vowels altogether as is done in Arabicorthography.Consonants Another source of ambiguity is theuse of a single Latin letter to refer to multiple Ara-bic phonemes.
For example, the Latin letter "d" isused to represent the sounds of the Arabic lettersX d and?
D. Additionally, some pairs of Arabiziletters can ambiguously map to a single Arabic let-ter or pairs of letters: "sh" can be use to represent?
?
or ??
sh.
Arabizi also uses digits to repre-sent some Arabic letters.
For example, the dig-its 2, 3, 5, 6, 7 and 9 are used to represent theHamza (glottal stop), and the sounds of the letters?
?
, p x, ?
T, h H and ?
S, respectively.
How-ever, when followed by "?
", the digits 3, 6, 7 and9 change their interpretations to the dotted versionof the Arabic letter:?
?,?
?D, p x and?
D, re-spectively.
Moreover, "?"
(as well as "q") may alsorefer to the glottal stop.Foreign Words Arabizi contains a large num-ber of foreign words, that are either borrowingssuch as mobile or instances of code switching suchas I love you.Abbreviations Arabizi may also include someabbreviations such as isa which means ?<?
@ Z A??
@?An ?A?
Allh ?God willing?.31Orthography ExampleCODAhPAJ.?@??
?G.Am????
A?mA ?ft?
SHAby mn AmbArHNon-CODAhPAJ.?@??
?G.Ag????
??A?Arabic Script mA?wft?
SwHAb?
mn AmbArHhPAJ.K @??
?G.Am?????m?ft?
SHAb?
mn?AnbArHhPAJ.?@??
?G.Am??J??
A?mA ?fty?
SHAby mn?AmbArHArabizimashoftesh sohaby men embare7ma shftesh swhabi mn imbarehmshwftish swhaby min ambare7Table 1: The different spelling variants in EGY and Arabizi for writing the sentence "I have not seen myfriends since yesterday" versus its corresponding CODA form.2.3 CODACODA is a conventionalized orthography for Di-alectal Arabic (Habash et al., 2012a).
In CODA,every word has a single orthographic representa-tion.
CODA has five key properties (Eskanderet al., 2013).
First, CODA is an internally con-sistent and coherent convention for writing DA.Second, CODA is primarily created for computa-tional purposes, but is easy to learn and recognizeby educated Arabic speakers.
Third, CODA usesthe Arabic script as used for MSA, with no ex-tra symbols from, for example, Persian or Urdu.Fourth, CODA is intended as a unified frameworkfor writing all dialects.
CODA has been definedfor EGY (Habash et al., 2012a) as well as TunisianArabic (Zribi et al., 2014).
Finally, CODA aimsto maintain a level of dialectal uniqueness whileusing conventions based on similarities betweenMSA and the dialects.
For a full presentation ofCODA and a justification and explanation of itschoices, see (Habash et al., 2012a).CODA has been used as part of different ap-proaches for modeling DA morphology (Habashet al., 2012b), tagging (Habash et al., 2013; Pashaet al., 2014) and spelling correction (Eskander etal., 2013; Farra et al., 2014).
Converting Dialec-tal Arabic (written using a spontaneous Arabic or-thography or Arabizi) to CODA is beneficial toNLP applications that better perform on standard-ized data with less sparsity (Eskander et al., 2013).Table 1 shows the CODA form correspondingto spontaneously written Arabic.3 Related WorkOur proposed work has some similarities to Dar-wish (2013).
His work is divided into two sec-tions: language identification and transliteration.He used word and sequence-level features to iden-tify Arabizi that is mixed with English.
For Arabicwords, he modeled transliteration from Arabizi toArabic script, and then applied language model-ing on the transliterated text.
This is similar to ourproposed work in terms of transliteration and lan-guage modeling.
However, Darwish (2013) doesnot target a conventionalized orthography, whileour system targets CODA.
Additionally, Darwish(2013) transliterates Arabic words only after filter-ing out non-Arabic words, while we transliteratethe whole input Arabizi.
Finally, he does not useany morphological information, while we intro-duce the use of a morphological analyzer to sup-port the transliteration pipeline.Chalabi and Gerges (2012) presented a hybridapproach for Arabizi transliteration.
Their workrelies on the use of character transformation rulesthat are either handcrafted by a linguist or au-tomatically generated from training data.
Theyalso employ word-based and character-based lan-guage models for the final transliteration choice.Like Darwish (2013), the work done by Chalabiand Gerges (2012) is similar to ours except thatit does not target a conventionalized orthography,and does not use deep morphological information,while our system does.There are three commercial products that con-32vert Arabizi to Arabic, namely: Microsoft Maren,3Google Ta3reeb4and Yamli.5However, sincethese products are for commercial purposes, thereis not enough information about their approaches.But given their output, it is clear that they donot follow a well-defined standardized orthogra-phy like we do.
Furthermore, these tools are pri-marily intended as input method support, not fulltext transliteration.
As a result, their users?
goalis to produce Arabic script text not Arabizi text.We expect, for instance, that users of these inputmethod support systems will use less or no codeswitching to English, and they may employ char-acter sequences that help them arrive at the targetArabic script form, which otherwise they wouldnot write if they are targeting Arabizi.Eskander et al.
(2013) introduced a systemto convert spontaneous EGY to CODA, calledCODAFY.
The difference between CODAFY andour proposed system is that CODAFY works onspontaneous text written in Arabic script, whileour system works on Arabizi, which involves ahigher degree of ambiguity.
However, we useCODAFY as a black-box module in our prepro-cessing.Additionally, there is some work on convert-ing from dialectal Arabic to MSA, which is sim-ilar to our work in terms of processing a dialec-tal input.
However, our final output is in EGYand not MSA.
Shaalan et al.
(2007) introduced arule-based approach to convert EGY to MSA.
Al-Gaphari and Al-Yadoumi (2010) also used a rule-based method to transform from Sanaani dialect toMSA.
Sawaf (2010), Salloum and Habash (2011)and Salloum and Habash (2013) used morpholog-ical analysis and morphosyntactic transformationrules for processing EGY and Levantine Arabic.There has been some work on machine translit-eration by Knight and Graehl (1997).
Al-Onaizanand Knight (2002) introduced an approach for ma-chine transliteration of Arabic names.
Freemanet al.
(2006) also introduced a system for namematching between English and Arabic, whichHabash (2008) employed as part of generatingEnglish transliterations from Arabic words in thecontext of machine translation.
This work is sim-ilar to ours in terms of text transliteration.
How-ever, our work is not restricted to names.3http://www.getmaren.com4http://www.google.com/ta3reeb5http://www.yamli.com/4 Approach4.1 Defining the TaskOur task is as follows: for each Arabizi word inthe input, we choose the Arabic script word whichis the correct CODA spelling of the input wordand which carries the intended meaning (as deter-mined in the context of the entire available text).We do not merge two or more input words intoa single Arabic script word.
If CODA requirestwo consecutive input Arabizi words to be merged,we indicate this by attaching a plus to the end ofthe first word.
On the other hand, if CODA re-quires an input Arabizi word to be broken into twoor more Arabic script words, we indicate this byinserting a dash between the words.
We do thisto maintain the bijection between input and out-put words, i.e., to allow easy tracing of the Arabicscript back to the Arabizi input.4.2 Transliteration PipelineThe proposed system in this paper is called 3AR-RIB.6Using the context of an input Arabizi word,3ARRIB produces the word?s best Arabic scriptCODA transliteration.
Figure 1 illustrates the dif-ferent components of 3ARRIB in both the train-ing and processing phases.
We summarize the fulltransliteration process as follows.
Each Arabizisentence input to 3ARRIB goes through a pre-processing step of lowercasing (de-capitalization),speech effects handling, and punctuation split-ting.
3ARRIB then generates a list of all possi-ble transliterations for each word in the input sen-tence using a finite-state transducer that is trainedon character-level alignment from Arabizi to Ara-bic script.
We then experiment with different com-binations of the following two components:Morphological Analyzer We use CALIMA(Habash et al., 2012b), a morphological analyzerfor EGY.
For each input word, CALIMA providesall possible morphological analyses, including theCODA spelling for each analysis.
All generatedcandidates are passed through CALIMA.
If CAL-IMA has no analysis for a candidate, then thatcandidate gets filtered out; otherwise, the CODAspellings of the analyses from CALIMA becomethe new candidates in the rest of the transliterationpipeline.
For some words, CALIMA may sug-gest multiple CODA spellings that reflect differentanalyses of the word.63ARRIB (pronounced /ar-rib/) means ?Arabize!
?.33FSMCandidatesFSMCALIMA(+tokenization)BestSelectionsLMFST modelSRILMArabizi ?
ArabicParallel DataGiza++Training phaseInput ArabiziScriptFSTEgyptian CorpusCALIMAOutputA* SearchPreprocessingMADAMIRAOutput ArabicScriptFigure 1: An illustration of the different components of the 3ARRIB system in both the training andprocessing phases.
FST: finite-state Transducer; LM: Language Model; CALIMA: Morphological Ana-lyzer for Dialectal Arabic; MADAMIRA: Morphological Tagger for Arabic.Language Model We disambiguate among thepossibilities for all input words (which consti-tute a ?sausage?
lattice) using an n-gram languagemodel.4.3 PreprocessingWe apply the following preprocessing steps to theinput Arabizi text:?
We separate all attached emoticons such as(:D, :p, etc.)
and punctuation from the words.We only keep the apostrophe because it isused in Arabizi to distinguish between dif-ferent sounds.
3ARRIB keeps track of anyword offset change, so that it can reconstructthe same number of tokens at the end of thepipeline.?
We tag emoticons and punctuation to protectthem from any change through the pipeline.?
We lowercase all letters.?
We handle speech effects by replacing anysequence of the same letter whose length isgreater than two by a sequence of exactlylength two; for example, iiiii becomes ii.4.4 Character-Based TransductionWe use a parallel corpus of Arabizi-Arabic wordsto learn a character-based transduction model.The parallel data consists of two sources.
First,we use 2,200 Arabizi-to-Arabic script pairs fromthe training data used by (Darwish, 2013).
Wemanually revised the Arabic side to be CODA-compliant.
Second, we use about 6,300 pairsof proper names in Arabic and English fromthe Buckwalter Arabic Morphological Analyzer(Buckwalter, 2004).
Since proper names are typ-ically transliterated, we expect them to be a richsource for learning transliteration mappings.The words in the parallel data are turned intospace-separated character tokens, which we alignusing Giza++ (Och and Ney, 2003).
We then usethe phrase extraction utility in the Moses statisticalmachine translation system (Koehn et al., 2007) toextract a phrase table which operates over char-acters.
The phrase table is then used to build afinite-state transducer (FST) that maps sequencesof Arabizi characters into sequences of Arabicscript characters.
We use the negative logarithmicconditional probabilities of the Arabizi-to-Arabicpairs in the phrase tables as costs inside the FST.We use the FST to transduce an input Arabizi wordto one or more words in Arabic script, where ev-ery resulting word in Arabic script is given a prob-abilistic score.As part of the preprocessing of the parallel data,we associate all Arabizi letters with their wordlocation information (beginning, middle and end-ing letters).
This is necessary since some Arabizi34mapping phenomena happen only at specific loca-tions.
For example, the Arabizi letter "o" is likelyto be transliterated into@ ?
in Arabic if it appearsat the beginning of the word, but almost never soif it appears in the middle of the word.For some special Arabizi cases, we directlytransliterate input words to their correct Arabicform using a table, without going through the FST.For example, isa is mapped to ?<?
@ Z A??
@?An ?A?Allh ?God willing?.
There are currently 32 entriesin this table.4.5 Morphological AnalyzerFor every word in the Arabizi input, all the candi-dates generated by the character-based transduc-tion are passed through the CALIMA morpholog-ical analyzer.
For every candidate, CALIMA pro-duces a list of all the possible morphological anal-yses.
The CODA for these analyses need not bethe same.
For example, if the output from the char-acter based transducer is Aly, then CALIMA pro-duces the following CODA-compliant spellings:??@?Al?
?to?, ??@?Al?
?to me?
and ?
?@?Aly ?automatic?or ?my family?.
All of these CODA spellings arethe output of CALIMA for that particular inputword.
The output from CALIMA then becomesthe set of final candidates of the input Arabizi inthe rest of the transliteration pipeline.
If a wordis not recognized by CALIMA, it gets filtered outfrom the transliteration pipeline.
However, if allthe candidates of some word are not recognizedby CALIMA, then we retain them all since thereshould be an output for every input word.We additionally run a tokenization step thatmakes use of the generated CALIMA morphologi-cal analysis.
The tokenization scheme we target isD3, which separates all clitics associated with theword (Habash, 2010).
For every word, we keepa list of the possible tokenized and untokenizedCODA-compliant pairs.
We use the tokenized oruntokenized forms as inputs to either a tokenizedor untokenized language model, respectively, asdescribed in the next subsection.
The untokenizedform is necessary to retain the surface form at theend of the transliteration process.Standalone clitics are sometimes found in Ara-bizi such as lel ragel (which corresponds to?g.
@P +??
ll+ rAjl ?for the man?).
Since CALIMAdoes not handle most standalone clitics, we keepa lookup table that associates them with their tok-enization information.4.6 Language ModelWe then use an EGY language model that istrained on CODA-compliant text.
We investi-gate two options: a language model that has stan-dard CODA white-space word tokenization con-ventions (?untokenized?
), and a language modelthat has a D3 tokenized form of CODA in whichall clitics are separated (?tokenized?).
The outputof the morphological analyzer (which is the inputto the LM component) is processed to match thetokenization used in the LM.The language models are built from a largecorpus of 392M EGY words.7The corpus isfirst processed using CODAFY (Eskander et al.,2013), a system for spontaneous text convention-alization into CODA.
This is necessary so thatour system remains CODA-compliant across thewhole transliteration pipeline.
Eskander et al.
(2013) states that the best conventionalization re-sults are obtained by running the MLE componentof CODAFY followed by an EGY morphologicaltagger, MADA-ARZ (Habash et al., 2013).
In thework reported here, we use the newer version ofMADA-ARZ, named MADAMIRA (Pasha et al.,2014).
For the tokenized language model, we runa D3 tokenization step on top of the processed textby MADAMIRA.
The processed data is used tobuild a language model with Kneser-Ney smooth-ing using the SRILM toolkit (Stolcke, 2002).We use A* search to pick the best transliterationfor each word given its context.
The probability ofany path in the A* search space combines the FSTprobability of the words with the probability fromthe language model.
Thus, for any certain path ofselected Arabic possibilities A0,i= {a0, a1, ...ai}given the corresponding input Arabizi sequenceW0,i= {w0, w1, ...wi}, the transliteration prob-ability can be defined by equation (1).P (A0,i|W0,i) =i?j=0(P (aj|wj) ?
P (aj|aj?N+1,j?1)) (1)Where, N is the maximum affordable n-gram length in the LM, P (aj|wj) is theFST probability of transliterating the Ara-bizi word wjinto the Arabic word aj, andP (aj|aj?N+1,j?1) is the LM probability of the se-quence {aj?N+1, aj?N+2, ...aj}.7All of the resources we use are available from the Lin-guistic Data Consortium: www.ldc.upenn.edu.355 Experiments and Results5.1 DataWe use two in-house data sets for development(Dev; 502 words) and blind testing (Test; 1004words).
The data contains EGY Arabizi SMSconversations that are mapped to Arabic script inCODA by a CODA-trained EGY native speaker.5.2 ExperimentsWe conducted a suite of experiments to evaluatethe performance of our approach and identify op-timal settings on the Dev set.
The optimal resultand the baseline are then applied to the blind Testset.
During development, the following settingswere explored:?
INV-Selection: The training data of the finitestate transducer is used to generate the list ofpossibilities for each input Arabizi word.
Ifthe input word cannot be found in the FSTtraining data, the word is kept in Arabizi.?
FST-ONLY: Pick the top choice from the listgenerated by the finite state transducer.?
FST-CALIMA: Pick the top choice from thelist after the CALIMA filtering.?
FST-CALIMA-Tokenized-LM-5: Run thefull pipeline of 3ARRIB with a 5-gram to-kenized LM.8?
FST-CALIMA-Tokenized-LM-5-MLE:The same as FST-CALIMA-Tokenized-LM-5, but for an Arabizi word that appearsin training, force its most frequently seenmapping directly instead of running thetransliteration pipeline for that word.?
FST-CALIMA-Untokenized-LM-5: Runthe full pipeline of 3ARRIB with a 5-gramuntokenized LM.?
FST-Untokenized-LM-5: Run the fullpipeline of 3ARRIB minus the CALIMA fil-tering with a 5-gram untokenized LM.
Thissetup is analogous to the transliteration ap-proach proposed by (Darwish, 2013).
Thuswe use it as our baseline.Each of the above experiments is evaluatedwith exact match, and with Alif/Ya normalization(El Kholy and Habash, 2010; Habash, 2010).83, 5, and 7-gram LMs have been tested.
The 3 and 5-gram LMs give the same performance while the 7-gram LMis the worst.5.3 ResultsTable 2 summarizes the results on the Dev set.Our best performing setup is FST-CALIMA-Tokenized-LM-5 which has 77.5% accuracy and79.1% accuracy with normalization.
The baselinesystem, FST-Untokenized-LM-5, gives 74.1% ac-curacy and 74.9 % accuracy with normalization.This highlights the value of morphological filter-ing as well as sparsity-reducing tokenization.Table 3 shows how we do (best system and bestbaseline) on a blind Test set.
Although the accu-racy drops overall, the gap between the best sys-tem and the baseline increases.5.4 Error AnalysisWe conducted two error analyses for the best per-forming transliteration setting on the Dev set.
Wefirst analyze in which component the Dev set er-rors occur.
About 29% of the errors are caseswhere the FST does not generate the correct an-swer.
An additional 15% of the errors happen be-cause the correct answer is not covered by CAL-IMA.
The language model does not include thecorrect answer in an additional 8% of the errors.The rest of the errors (48%) are cases where thecorrect answer is available in all components butdoes not get selected.Motivated by the value of Arabizi transliterationfor machine translation into English, we distin-guish between two types of words: words that re-main the same when translated into English, suchas English words, proper nouns, laughs, emoti-cons, punctuations and digits (EN-SET) versusEGY-only words (EGY-SET).
Examples of wordsin EN-SET are: love you very much (code switch-ing), Peter (proper noun), haha (laugh), :D (emoti-con), !
(punctuation) and 123 (digits).While the overall performance of our best set-tings is 77.5%, the accuracy of the EGY-SET byitself is 84.6% as opposed to 46.2% for EN-SET.This large difference reflects the fact that we donot target English word transliteration into Arabicscript explicitly.We now perform a second error analysis only onthe errors in the EGY-SET, in which we categorizethe errors by their linguistic type.
About 25% ofthe errors are non-CODA-compliant system out-put, where the answer is a plausible non-CODAform, i.e., a form that may be written or read eas-ily by a native speaker who is not aware of CODA.For example, the system generates the non-CODA36System Exact-Matching A/Y-normalizationINV-Selection 37.1 40.6FST-ONLY (pick top choice) 63.1 65.1FST-CALIMA (pick top choice) 66.1 68.9FST-CALIMA-Tokenized-LM-5 77.5 79.1FST-CALIMA-Tokenized-LM-5-MLE 68.7 73.5FST-CALIMA-Untokenized-LM-5 77.3 78.9FST-Untokenized-LM-5 74.1 74.9Table 2: Results on the Dev set in terms of accuracy (%).System Exact-Matching A/Y-normalizationFST-CALIMA-Tokenized-LM-5 69.4 73.9FST-Untokenized-LM-5 63.1 65.4Table 3: Results on the blind Test set in terms of accuracy (%).form???JJ?
mynf??
instead of the correct CODAform???JKA?
mA ynf??
?it doesn?t work?.
Ignor-ing the CODA-related errors increases the overallaccuracy by about 3.0% to become 80.5%.
The ac-curacy of the EGY-SET rises to 88.3% as opposedto 84.6% when considering CODA compliance.Ambiguous Arabizi input contributes to an ad-ditional 27% of the errors, where the system as-signs a plausible answer that is incorrect in con-text.
For example, the word matar in the inputArabizi fel matar ?at the airport?
has two plausi-ble out-of-context solutions: PA??
mTAr ?airport?
(contextually correct) and Q??
mTr ?rain?
(contex-tually incorrect).In about 2% of the errors, the Arabizi input con-tains a typo making it impossible to produce thegold reference.
For example, the input Arabiziba7bet contains a typo where the final t should turninto k, so that it means ?J.kAK.bAHbk ?I love you[2fs]?.In the rest of the errors (about 46%), the sys-tem fails to come up with the correct answer.
In-stead, it assigns a completely different word oreven an impossible word.
For example, the cor-rect answer for the input Arabizi sora ?picture?
is?P??
Swrh?, while the system produces the wordP??
swr ?wall?.
Another example is the input Ara-bizi talabt ?I asked for?, where the output from thesystem is?J.?
A?
TAlbh?
?student?, while the correctanswer isIJ.??
tlbt ?I asked for, ordered?
instead.6 Conclusion and Future WorkWe presented a method for converting dialectalArabic (specifically, EGY) written in Arabizi toArabic script following the CODA convention forDA orthography.
We achieve a 17% error reduc-tion over our implementation of a previously pub-lished work (Darwish, 2013) on a blind test set.In the future, we plan to improve several aspectsof our models, particularly FST character map-ping, the morphological analyzer coverage, andlanguage models.
We also plan to work on theproblem of automatic identification of non-Arabicwords.
We will extend the system to work on otherArabic dialects.
We also plan to make the 3AR-RIB system publicly available.AcknowledgementThis paper is based upon work supported bythe Defense Advanced Research Projects Agency(DARPA) under Contract No.
HR0011-12-C-0014.
Any opinions, findings and conclusions orrecommendations expressed in this paper are thoseof the authors and do not necessarily reflect theviews of DARPA.ReferencesG.
Al-Gaphari and M. Al-Yadoumi.
2010.
A methodto convert Sana?ani accent to Modern Standard Ara-bic.
International Journal of Information Scienceand Management, pages 39?49.Yaser Al-Onaizan and Kevin Knight.
2002.
Machinetransliteration of names in arabic text.
In Proceed-ings of the ACL-02 Workshop on Computational Ap-proaches to Semitic Languages.37Tim Buckwalter.
2004.
Buckwalter Arabic Morpho-logical Analyzer Version 2.0.
LDC catalog numberLDC2004L02, ISBN 1-58563-324-0.Achraf Chalabi and Hany Gerges.
2012.
RomanizedArabic Transliteration.
In Proceedings of the Sec-ond Workshop on Advances in Text Input Methods(WTIM 2012).Kareem Darwish.
2013.
Arabizi Detection and Con-version to Arabic.
CoRR.Ahmed El Kholy and Nizar Habash.
2010.
Techniquesfor Arabic Morphological Detokenization and Or-thographic Denormalization.
In Proceedings of theseventh International Conference on Language Re-sources and Evaluation (LREC), Valletta, Malta.Ramy Eskander, Nizar Habash, Owen Rambow, andNadi Tomeh.
2013.
Processing Spontaneous Or-thography.
In Proceedings of the 2013 Conferenceof the North American Chapter of the Associationfor Computational Linguistics: Human LanguageTechnologies (NAACL-HLT), Atlanta, GA.Noura Farra, Nadi Tomeh, Alla Rozovskaya, andNizar Habash.
2014.
Generalized Character-LevelSpelling Error Correction.
In Proceedings of theConference of the Association for ComputationalLinguistics (ACL), Baltimore, Maryland, USA.Andrew Freeman, Sherri Condon, and ChristopherAckerman.
2006.
Cross linguistic name matchingin English and Arabic.
In Proceedings of the HumanLanguage Technology Conference of the NAACL,Main Conference, pages 471?478, New York City,USA.Nizar Habash, Abdelhadi Soudi, and Tim Buckwalter.2007.
On Arabic Transliteration.
In A. van denBosch and A. Soudi, editors, Arabic Computa-tional Morphology: Knowledge-based and Empiri-cal Methods.
Springer.Nizar Habash, Mona Diab, and Owen Rabmow.
2012a.Conventional Orthography for Dialectal Arabic.
InProceedings of the Language Resources and Evalu-ation Conference (LREC), Istanbul.Nizar Habash, Ramy Eskander, and Abdelati Hawwari.2012b.
A Morphological Analyzer for EgyptianArabic.
In Proceedings of the Twelfth Meeting of theSpecial Interest Group on Computational Morphol-ogy and Phonology, pages 1?9, Montr?al, Canada.Nizar Habash, Ryan Roth, Owen Rambow, Ramy Es-kander, and Nadi Tomeh.
2013.
MorphologicalAnalysis and Disambiguation for Dialectal Arabic.In Proceedings of the 2013 Conference of the NorthAmerican Chapter of the Association for Computa-tional Linguistics: Human Language Technologies(NAACL-HLT), Atlanta, GA.Nizar Habash.
2008.
Four Techniques for OnlineHandling of Out-of-Vocabulary Words in Arabic-English Statistical Machine Translation.
In Pro-ceedings of ACL-08: HLT, Short Papers, pages 57?60, Columbus, Ohio.Nizar Habash.
2010.
Introduction to Arabic NaturalLanguage Processing.
Morgan & Claypool Publish-ers.Kevin Knight and Jonathan Graehl.
1997.
Machinetransliteration.
In Proceedings of the Europeanchapter of the Association for Computational Lin-guistics.Philipp Koehn, Hieu Hoang, Alexandra Birch, ChrisCallison-Burch, Marcello Federico, Nicola Bertoldi,Brooke Cowan, Wade Shen, Christine Moran,Richard Zens, Chris Dyer, Ondrej Bojar, AlexandraConstantin, and Evan Herbst.
2007.
Moses: Opensource toolkit for statistical machine translation.
InProceedings of the Association for ComputationalLinguistics, Prague, Czech Republic.Franz Joseph Och and Hermann Ney.
2003.
A sys-tematic comparison of various statistical alignmentmodels.
Computational Linguistics, 29(1):19?51.Arfath Pasha, Mohamed Al-Badrashiny, Mona Diab,Ahmed El Kholy, Ramy Eskander, Nizar Habash,Manoj Pooleery, Owen Rambow, and Ryan M. Roth.2014.
MADAMIRA: A Fast, Comprehensive Toolfor Morphological Analysis and Disambiguation ofArabic.
In Proceedings of the Language Resourcesand Evaluation Conference (LREC), Reykjavik, Ice-land.Wael Salloum and Nizar Habash.
2011.
Dialectal toStandard Arabic Paraphrasing to Improve Arabic-English Statistical Machine Translation.
In Pro-ceedings of the First Workshop on Algorithms andResources for Modelling of Dialects and LanguageVarieties, pages 10?21, Edinburgh, Scotland.Wael Salloum and Nizar Habash.
2013.
DialectalArabic to English Machine Translation: Pivotingthrough Modern Standard Arabic.
In Proceedings ofthe 2013 Conference of the North American Chap-ter of the Association for Computational Linguistics:Human Language Technologies (NAACL-HLT), At-lanta, GA.Hassan Sawaf.
2010.
Arabic dialect handling in hybridmachine translation.
In Proceedings of the Confer-ence of the Association for Machine Translation inthe Americas (AMTA), Denver, Colorado.Khaled Shaalan, Hitham Abo Bakr, and IbrahimZiedan.
2007.
Transferring Egyptian Colloquialinto Modern Standard Arabic.
In International Con-ference on Recent Advances in Natural LanguageProcessing (RANLP), Borovets, Bulgaria.Andreas Stolcke.
2002.
SRILM - an Extensible Lan-guage Modeling Toolkit.
In Proceedings of the In-ternational Conference on Spoken Language Pro-cessing (ICSLP), volume 2, pages 901?904, Denver,CO.Ines Zribi, Rahma Boujelbane, Abir Masmoudi,Mariem Ellouze, Lamia Belguith, and Nizar Habash.2014.
A Conventional Orthography for TunisianArabic.
In Proceedings of the Language Resourcesand Evaluation Conference (LREC), Reykjavik, Ice-land.38
