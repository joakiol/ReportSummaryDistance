Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 450?458,Suntec, Singapore, 2-7 August 2009. c?2009 ACL and AFNLPExtracting Lexical Reference Rules from WikipediaEyal ShnarchComputer Science DepartmentBar-Ilan UniversityRamat-Gan 52900, Israelshey@cs.biu.ac.ilLibby BarakDept.
of Computer ScienceUniversity of TorontoToronto, Canada M5S 1A4libbyb@cs.toronto.eduIdo DaganComputer Science DepartmentBar-Ilan UniversityRamat-Gan 52900, Israeldagan@cs.biu.ac.ilAbstractThis paper describes the extraction fromWikipedia of lexical reference rules, iden-tifying references to term meanings trig-gered by other terms.
We present extrac-tion methods geared to cover the broadrange of the lexical reference relation andanalyze them extensively.
Most extrac-tion methods yield high precision levels,and our rule-base is shown to perform bet-ter than other automatically constructedbaselines in a couple of lexical expan-sion and matching tasks.
Our rule-baseyields comparable performance to Word-Net while providing largely complemen-tary information.1 IntroductionA most common need in applied semantic infer-ence is to infer the meaning of a target term fromother terms in a text.
For example, a Question An-swering system may infer the answer to a ques-tion regarding luxury cars from a text mentioningBentley, which provides a concrete reference to thesought meaning.Aiming to capture such lexical inferences wefollowed (Glickman et al, 2006), which coinedthe term lexical reference (LR) to denote refer-ences in text to the specific meaning of a targetterm.
They further analyzed the dataset of the FirstRecognizing Textual Entailment Challenge (Da-gan et al, 2006), which includes examples drawnfrom seven different application scenarios.
It wasfound that an entailing text indeed includes a con-crete reference to practically every term in the en-tailed (inferred) sentence.The lexical reference relation between twoterms may be viewed as a lexical inference rule,denoted LHS?
RHS.
Such rule indicates that theleft-hand-side term would generate a reference, insome texts, to a possible meaning of the right handside term, as the Bentley?
luxury car example.In the above example the LHS is a hyponym ofthe RHS.
Indeed, the commonly used hyponymy,synonymy and some cases of the meronymy rela-tions are special cases of lexical reference.
How-ever, lexical reference is a broader relation.
Forinstance, the LR rule physician ?
medicine maybe useful to infer the topic medicine in a text cate-gorization setting, while an information extractionsystem may utilize the rule Margaret Thatcher?
United Kingdom to infer a UK announcementfrom the text ?Margaret Thatcher announced?.To perform such inferences, systems need largescale knowledge bases of LR rules.
A prominentavailable resource is WordNet (Fellbaum, 1998),from which classical relations such as synonyms,hyponyms and some cases of meronyms may beused as LR rules.
An extension to WordNet waspresented by (Snow et al, 2006).
Yet, availableresources do not cover the full scope of lexical ref-erence.This paper presents the extraction of a large-scale rule base from Wikipedia designed to covera wide scope of the lexical reference relation.
Asa starting point we examine the potential of defi-nition sentences as a source for LR rules (Ide andJean, 1993; Chodorow et al, 1985; Moldovan andRus, 2001).
When writing a concept definition,one aims to formulate a concise text that includesthe most characteristic aspects of the defined con-cept.
Therefore, a definition is a promising sourcefor LR relations between the defined concept andthe definition terms.In addition, we extract LR rules from Wikipediaredirect and hyperlink relations.
As a guide-line, we focused on developing simple extrac-tion methods that may be applicable for otherWeb knowledge resources, rather than focusingon Wikipedia-specific attributes.
Overall, our rulebase contains about 8 million candidate lexical ref-450erence rules.
1Extensive analysis estimated that 66% of ourrules are correct, while different portions of therule base provide varying recall-precision trade-offs.
Following further error analysis we intro-duce rule filtering which improves inference per-formance.
The rule base utility was evaluatedwithin two lexical expansion applications, yield-ing better results than other automatically con-structed baselines and comparable results to Word-Net.
A combination with WordNet achieved thebest performance, indicating the significant mar-ginal contribution of our rule base.2 BackgroundMany works on machine readable dictionaries uti-lized definitions to identify semantic relations be-tween words (Ide and Jean, 1993).
Chodorow etal.
(1985) observed that the head of the definingphrase is a genus term that describes the definedconcept and suggested simple heuristics to find it.Other methods use a specialized parser or a set ofregular expressions tuned to a particular dictionary(Wilks et al, 1996).Some works utilized Wikipedia to build an on-tology.
Ponzetto and Strube (2007) identifiedthe subsumption (IS-A) relation from Wikipedia?scategory tags, while in Yago (Suchanek et al,2007) these tags, redirect links and WordNet wereused to identify instances of 14 predefined spe-cific semantic relations.
These methods dependon Wikipedia?s category system.
The lexical refer-ence relation we address subsumes most relationsfound in these works, while our extractions are notlimited to a fixed set of predefined relations.Several works examined Wikipedia texts, ratherthan just its structured features.
Kazama and Tori-sawa (2007) explores the first sentence of an ar-ticle and identifies the first noun phrase followingthe verb be as a label for the article title.
We repro-duce this part of their work as one of our baselines.Toral and Mun?oz (2007) uses all nouns in the firstsentence.
Gabrilovich and Markovitch (2007) uti-lized Wikipedia-based concepts as the basis for ahigh-dimensional meaning representation space.Hearst (1992) utilized a list of patterns indica-tive for the hyponym relation in general texts.Snow et al (2006) use syntactic path patterns asfeatures for supervised hyponymy and synonymy1For download see Textual Entailment Resource Pool atthe ACL-wiki (http://aclweb.org/aclwiki)classifiers, whose training examples are derivedautomatically from WordNet.
They use these clas-sifiers to suggest extensions to the WordNet hierar-chy, the largest one consisting of 400K new links.Their automatically created resource is regarded inour paper as a primary baseline for comparison.Many works addressed the more general notionof lexical associations, or association rules (e.g.
(Ruge, 1992; Rapp, 2002)).
For example, TheBeatles, Abbey Road and Sgt.
Pepper would allbe considered lexically associated.
However thisis a rather loose notion, which only indicates thatterms are semantically ?related?
and are likely toco-occur with each other.
On the other hand, lex-ical reference is a special case of lexical associa-tion, which specifies concretely that a reference tothe meaning of one term may be inferred from theother.
For example, Abbey Road provides a con-crete reference to The Beatles, enabling to infer asentence like ?I listened to The Beatles?
from ?Ilistened to Abbey Road?, while it does not referspecifically to Sgt.
Pepper.3 Extracting Rules from WikipediaOur goal is to utilize the broad knowledge ofWikipedia to extract a knowledge base of lexicalreference rules.
Each Wikipedia article providesa definition for the concept denoted by the titleof the article.
As the most concise definition wetake the first sentence of each article, following(Kazama and Torisawa, 2007).
Our preliminaryevaluations showed that taking the entire first para-graph as the definition rarely introduces new validrules while harming extraction precision signifi-cantly.Since a concept definition usually employsmore general terms than the defined concept (Ideand Jean, 1993), the concept title is more likelyto refer to terms in its definition rather than viceversa.
Therefore the title is taken as the LHS ofthe constructed rule while the extracted definitionterm is taken as its RHS.
As Wikipedia?s titles aremostly noun phrases, the terms we extract as RHSsare the nouns and noun phrases in the definition.The remainder of this section describes our meth-ods for extracting rules from the definition sen-tence and from additional Wikipedia information.Be-Comp Following the general idea in(Kazama and Torisawa, 2007), we identify the IS-A pattern in the definition sentence by extract-ing nominal complements of the verb ?be?, taking451No.
Extraction RuleJames Eugene ?Jim?
Carrey is a Canadian-American actorand comedian1 Be-Comp Jim Carrey?
Canadian-American actor2 Be-Comp Jim Carrey?
actor3 Be-Comp Jim Carrey?
comedianAbbey Road is an album released by The Beatles4 All-N Abbey Road?
The Beatles5 Parenthesis Graph?
mathematics6 Parenthesis Graph?
data structure7 Redirect CPU?
Central processing unit8 Redirect Receptors IgG?
Antibody9 Redirect Hypertension?
Elevated blood-pressure10 Link pet?
Domesticated Animal11 Link Gestaltist?
Gestalt psychologyTable 1: Examples of rule extraction methodsthem as the RHS of a rule whose LHS is the articletitle.
While Kazama and Torisawa used a chun-ker, we parsed the definition sentence using Mini-par (Lin, 1998b).
Our initial experiments showedthat parse-based extraction is more accurate thanchunk-based extraction.
It also enables us extract-ing additional rules by splitting conjoined nounphrases and by taking both the head noun and thecomplete base noun phrase as the RHS for sepa-rate rules (examples 1?3 in Table 1).All-N The Be-Comp extraction method yieldsmostly hypernym relations, which do not exploitthe full range of lexical references within the con-cept definition.
Therefore, we further create rulesfor all head nouns and base noun phrases withinthe definition (example 4).
An unsupervised reli-ability score for rules extracted by this method isinvestigated in Section 4.3.Title Parenthesis A common convention inWikipedia to disambiguate ambiguous titles isadding a descriptive term in parenthesis at the endof the title, as in The Siren (Musical), The Siren(sculpture) and Siren (amphibian).
From such ti-tles we extract rules in which the descriptive terminside the parenthesis is the RHS and the rest ofthe title is the LHS (examples 5?6).Redirect As any dictionary and encyclopedia,Wikipedia contains Redirect links that direct dif-ferent search queries to the same article, which hasa canonical title.
For instance, there are 86 differ-ent queries that redirect the user to United States(e.g.
U.S.A., America, Yankee land).
Redirectlinks are hand coded, specifying that both termsrefer to the same concept.
We therefore generate abidirectional entailment rule for each redirect link(examples 7?9).Link Wikipedia texts contain hyper links to ar-ticles.
For each link we generate a rule whose LHSis the linking text and RHS is the title of the linkedarticle (examples 10?11).
In this case we gener-ate a directional rule since links do not necessarilyconnect semantically equivalent entities.We note that the last three extraction methodsshould not be considered as Wikipedia specific,since many Web-like knowledge bases containredirects, hyper-links and disambiguation means.Wikipedia has additional structural features suchas category tags, structured summary tablets forspecific semantic classes, and articles containinglists which were exploited in prior work as re-viewed in Section 2.As shown next, the different extraction meth-ods yield different precision levels.
This may al-low an application to utilize only a portion of therule base whose precision is above a desired level,and thus choose between several possible recall-precision tradeoffs.4 Extraction Methods AnalysisWe applied our rule extraction methods over aversion of Wikipedia available in a database con-structed by (Zesch et al, 2007)2.
The extractionyielded about 8 million rules altogether, with over2.4 million distinct RHSs and 2.8 million distinctLHSs.
As expected, the extracted rules involvemostly named entities and specific concepts, typi-cally covered in encyclopedias.4.1 Judging Rule CorrectnessFollowing the spirit of the fine-grained humanevaluation in (Snow et al, 2006), we randomlysampled 800 rules from our rule-base and pre-sented them to an annotator who judged them forcorrectness, according to the lexical reference no-tion specified above.
In cases which were too dif-ficult to judge the annotator was allowed to ab-stain, which happened for 20 rules.
66% of the re-maining rules were annotated as correct.
200 rulesfrom the sample were judged by another annotatorfor agreement measurement.
The resulting Kappascore was 0.7 (substantial agreement (Landis and2English version from February 2007, containing 1.6 mil-lion articles.
www.ukp.tu-darmstadt.de/software/JWPL452Extraction Per Method AccumulatedMethod P Est.
#Rules P %obtainedRedirect 0.87 1,851,384 0.87 31Be-Comp 0.78 1,618,913 0.82 60Parenthesis 0.71 94,155 0.82 60Link 0.7 485,528 0.80 68All-N 0.49 1,580,574 0.66 100Table 2: Manual analysis: precision and estimated numberof correct rules per extraction method, and precision and %of correct rules obtained of rule-sets accumulated by method.Koch, 1997)), either when considering all the ab-stained rules as correct or as incorrect.The middle columns of Table 2 present, for eachextraction method, the obtained percentage of cor-rect rules (precision) and their estimated absolutenumber.
This number is estimated by multiplyingthe number of annotated correct rules for the ex-traction method by the sampling proportion.
In to-tal, we estimate that our resource contains 5.6 mil-lion correct rules.
For comparison, Snow?s pub-lished extension to WordNet3, which covers simi-lar types of terms but is restricted to synonyms andhyponyms, includes 400,000 relations.The right part of Table 2 shows the perfor-mance figures for accumulated rule bases, createdby adding the extraction methods one at a time inorder of their precision.
% obtained is the per-centage of correct rules in each rule base out ofthe total number of correct rules extracted jointlyby all methods (the union set).We can see that excluding the All-N methodall extraction methods reach quite high precisionlevels of 0.7-0.87, with accumulated precision of0.84.
By selecting only a subset of the extrac-tion methods, according to their precision, one canchoose different recall-precision tradeoff pointsthat suit application preferences.The less accurate All-N method may be usedwhen high recall is important, accounting for 32%of the correct rules.
An examination of the pathsin All-N reveals, beyond standard hyponymy andsynonymy, various semantic relations that satisfylexical reference, such as Location, Occupationand Creation, as illustrated in Table 3.
Typical re-lations covered by Redirect and Link rules include3http://ai.stanford.edu/?rion/swn/4As a non-comparable reference, Snow?s fine-grainedevaluation showed a precision of 0.84 on 10K rules and 0.68on 20K rules; however, they were interested only in the hy-ponym relation while we evaluate our rules according to thebroader LR relation.synonyms (NY State Trooper ?
New York StatePolice), morphological derivations (irritate ?
ir-ritation), different spellings or naming (Pytagoras?
Pythagoras) and acronyms (AIS?
Alarm Indi-cation Signal).4.2 Error AnalysisWe sampled 100 rules which were annotated as in-correct and examined the causes of errors.
Figure1 shows the distribution of error types.Wrong NP part - The most common error(35% of the errors) is taking an inappropriate partof a noun phrase (NP) as the rule right hand side(RHS).
As described in Section 3, we create tworules from each extracted NP, by taking both thehead noun and the complete base NP as RHSs.While both rules are usually correct, there arecases in which the left hand side (LHS) refers tothe NP as a whole but not to part of it.
For ex-ample, Margaret Thatcher refers to United King-dom but not to Kingdom.
In Section 5 we suggesta filtering method which addresses some of theseerrors.
Future research may exploit methods fordetecting multi-words expressions.All-N pattern errors13%Transparent head 11%Wrong NP part 35%Technical errors10%Dates and Places5% Link errors 5% Redirect errors 5%Relatedbut not Referring 16%Figure 1: Error analysis: type of incorrect rulesRelated but not Referring - Although all termsin a definition are highly related to the defined con-cept, not all are referred by it.
For example theorigin of a person (*The Beatles?
Liverpool5) orfamily ties such as ?daughter of?
or ?sire of?.All-N errors - Some of the articles start with along sentence which may include information thatis not directly referred by the title of the article.For instance, consider *Interstate 80 ?
Califor-nia from ?Interstate 80 runs from California toNew Jersey?.
In Section 4.3 we further analyzethis type of error and point at a possible directionfor addressing it.Transparent head - This is the phenomenon inwhich the syntactic head of a noun phrase does5The asterisk denotes an incorrect rule453Relation Rule Path PatternLocation Lovek?
Cambodia Lovek city in CambodiaOccupation Thomas H. Cormen?
computer science Thomas H. Cormen professor of computer scienceCreation Genocidal Healer?
James White Genocidal Healer novel by James WhiteOrigin Willem van Aelst?
Dutch Willem van Aelst Dutch artistAlias Dean Moriarty?
Benjamin Linus Dean Moriarty is an alias of Benjamin Linus on Lost.Spelling Egushawa?
Agushaway Egushawa, also spelled Agushaway...Table 3: All-N rules exemplifying various types of LR relationsnot bear its primary meaning, while it has a mod-ifier which serves as the semantic head (Fillmoreet al, 2002; Grishman et al, 1986).
Since parsersidentify the syntactic head, we extract an incorrectrule in such cases.
For instance, deriving *PrinceWilliam ?
member instead of Prince William ?British Royal Family from ?Prince William is amember of the British Royal Family?.
Even thoughwe implemented the common solution of using alist of typical transparent heads, this solution ispartial since there is no closed set of such phrases.Technical errors - Technical extraction errorswere mainly due to erroneous identification of thetitle in the definition sentence or mishandling non-English texts.Dates and Places - Dates and places where acertain person was born at, lived in or worked atoften appear in definitions but do not comply tothe lexical reference notion (*Galileo Galilei ?15 February 1564).Link errors - These are usually the result ofwrong assignment of the reference direction.
Sucherrors mostly occur when a general term, e.g.
rev-olution, links to a more specific albeit typical con-cept, e.g.
French Revolution.Redirect errors - These may occur in somecases in which the extracted rule is not bidirec-tional.
E.g.
*Anti-globalization ?
Movement ofMovements is wrong but the opposite entailmentdirection is correct, as Movement of Movements isa popular term in Italy for Anti-globalization.4.3 Scoring All-N RulesWe observed that the likelihood of nouns men-tioned in a definition to be referred by the con-cept title depends greatly on the syntactic pathconnecting them (which was exploited also in(Snow et al, 2006)).
For instance, the path pro-duced by Minipar for example 4 in Table 1 is titlesubj?
?album vrel?
?released by?subj??
bypcomp?n??
noun.In order to estimate the likelihood that a syn-tactic path indicates lexical reference we collectedfrom Wikipedia all paths connecting a title to anoun phrase in the definition sentence.
We notethat since there is no available resource which cov-ers the full breadth of lexical reference we couldnot obtain sufficiently broad supervised trainingdata for learning which paths correspond to cor-rect references.
This is in contrast to (Snow et al,2005) which focused only on hyponymy and syn-onymy relations and could therefore extract posi-tive and negative examples from WordNet.We therefore propose the following unsuper-vised reference likelihood score for a syntacticpath p within a definition, based on two counts:the number of times p connects an article title witha noun in its definition, denoted by Ct(p), and thetotal number of p?s occurrences in Wikipedia de-finitions, C(p).
The score of a path is then de-fined as Ct(p)C(p) .
The rational for this score is thatC(p)?
Ct(p) corresponds to the number of timesin which the path connects two nouns within thedefinition, none of which is the title.
These in-stances are likely to be non-referring, since a con-cise definition typically does not contain terms thatcan be inferred from each other.
Thus our scoremay be seen as an approximation for the probabil-ity that the two nouns connected by an arbitraryoccurrence of the path would satisfy the referencerelation.
For instance, the path of example 4 ob-tained a score of 0.98.We used this score to sort the set of rules ex-tracted by the All-N method and split the sorted listinto 3 thirds: top, middle and bottom.
As shown inTable 4, this obtained reasonably high precisionfor the top third of these rules, relative to the othertwo thirds.
This precision difference indicates thatour unsupervised path score provides useful infor-mation about rule reliability.It is worth noting that in our sample 57% of All-N errors, 62% of Related but not Referring incor-rect rules and all incorrect rules of type Dates and454Extraction Per Method AccumulatedMethod P Est.
#Rules P %obtainedAll-Ntop 0.60 684,238 0.76 83All-Nmiddle 0.46 380,572 0.72 90All-Nbottom 0.41 515,764 0.66 100Table 4: Splitting All-N extraction method into 3 sub-types.These three rows replace the last row of Table 2Places were extracted by the All-Nbottom methodand thus may be identified as less reliable.
How-ever, this split was not observed to improve per-formance in the application oriented evaluationsof Section 6.
Further research is thus needed tofully exploit the potential of the syntactic path asan indicator for rule correctness.5 Filtering RulesFollowing our error analysis, future research isneeded for addressing each specific type of error.However, during the analysis we observed that alltypes of erroneous rules tend to relate terms thatare rather unlikely to co-occur together.
We there-fore suggest, as an optional filter, to recognizesuch rules by their co-occurrence statistics usingthe common Dice coefficient:2 ?
C(LHS,RHS)C(LHS) + C(RHS)where C(x) is the number of articles in Wikipediain which all words of x appear.In order to partially overcome the Wrong NPpart error, identified in Section 4.2 to be the mostcommon error, we adjust the Dice equation forrules whose RHS is also part of a larger nounphrase (NP):2 ?
(C(LHS,RHS)?
C(LHS,NPRHS))C(LHS) + C(RHS)where NPRHS is the complete NP whose partis the RHS.
This adjustment counts only co-occurrences in which the LHS appears with theRHS alone and not with the larger NP.
This sub-stantially reduces the Dice score for those cases inwhich the LHS co-occurs mainly with the full NP.Given the Dice score rules whose score does notexceed a threshold may be filtered.
For example,the incorrect rule *aerial tramway?
car was fil-tered, where the correct RHS for this LHS is thecomplete NP cable car.
Another filtered rule ismagic?
cryptography which is correct only for avery idiosyncratic meaning.6We also examined another filtering score, thecosine similarity between the vectors representingthe two rule sides in LSA (Latent Semantic Analy-sis) space (Deerwester et al, 1990).
However, asthe results with this filter resemble those for Dicewe present results only for the simpler Dice filter.6 Application Oriented EvaluationsOur primary application oriented evaluation iswithin an unsupervised lexical expansion scenarioapplied to a text categorization data set (Section6.1).
Additionally, we evaluate the utility of ourrule base as a lexical resource for recognizing tex-tual entailment (Section 6.2).6.1 Unsupervised Text CategorizationOur categorization setting resembles typical queryexpansion in information retrieval (IR), where thecategory name is considered as the query.
The ad-vantage of using a text categorization test set isthat it includes exhaustive annotation for all doc-uments.
Typical IR datasets, on the other hand,are partially annotated through a pooling proce-dure.
Thus, some of our valid lexical expansionsmight retrieve non-annotated documents that weremissed by the previously pooled systems.6.1.1 Experimental SettingOur categorization experiment follows a typicalkeywords-based text categorization scheme (Mc-Callum and Nigam, 1999; Liu et al, 2004).
Tak-ing a lexical reference perspective, we assume thatthe characteristic expansion terms for a categoryshould refer to the term (or terms) denoting thecategory name.
Accordingly, we construct the cat-egory?s feature vector by taking first the categoryname itself, and then expanding it with all left-hand sides of lexical reference rules whose right-hand side is the category name.
For example, thecategory ?Cars?
is expanded by rules such as Fer-rari F50?
car.
During classification cosine sim-ilarity is measured between the feature vector ofthe classified document and the expanded vectorsof all categories.
The document is assigned tothe category which yields the highest similarityscore, following a single-class classification ap-proach (Liu et al, 2004).6Magic was the United States codename for intelligencederived from cryptanalysis during World War II.455Rule Base R P F1Baselines:No Expansion 0.19 0.54 0.28WikiBL 0.19 0.53 0.28Snow400K 0.19 0.54 0.28Lin 0.25 0.39 0.30WordNet 0.30 0.47 0.37Extraction Methods from Wikipedia:Redirect + Be-Comp 0.22 0.55 0.31All rules 0.31 0.38 0.34All rules + Dice filter 0.31 0.49 0.38Union:WordNet + WikiAll rules+Dice 0.35 0.47 0.40Table 5: Results of different rule bases for 20 newsgroupscategory name expansionIt should be noted that keyword-based textcategorization systems employ various additionalsteps, such as bootstrapping, which generalize tomulti-class settings and further improve perfor-mance.
Our basic implementation suffices to eval-uate comparatively the direct impact of differentexpansion resources on the initial classification.For evaluation we used the test set of the?bydate?
version of the 20-News Groups collec-tion,7 which contains 18,846 documents parti-tioned (nearly) evenly over the 20 categories8.6.1.2 Baselines ResultsWe compare the quality of our rule base expan-sions to 5 baselines (Table 5).
The first avoids anyexpansion, classifying documents based on cosinesimilarity with category names only.
As expected,it yields relatively high precision but low recall,indicating the need for lexical expansion.The second baseline is our implementation ofthe relevant part of the Wikipedia extraction in(Kazama and Torisawa, 2007), taking the firstnoun after a be verb in the definition sentence, de-noted as WikiBL.
This baseline does not improveperformance at all over no expansion.The next two baselines employ state-of-the-artlexical resources.
One uses Snow?s extension toWordNet which was mentioned earlier.
This re-source did not yield a noticeable improvement, ei-7www.ai.mit.edu/people/jrennie/20Newsgroups.8The keywords used as category names are: athe-ism; graphic; microsoft windows; ibm,pc,hardware;mac,hardware; x11,x-windows; sale; car; motorcycle;baseball; hockey; cryptography; electronics; medicine; outerspace; christian(noun & adj); gun; mideast,middle east;politics; religionther over the No Expansion baseline or over Word-Net when joined with its expansions.
The sec-ond uses Lin dependency similarity, a syntactic-dependency based distributional word similarityresource described in (Lin, 1998a)9.
We used var-ious thresholds on the length of the expansion listderived from this resource.
The best result, re-ported here, provides only a minor F1 improve-ment over No Expansion, with modest recall in-crease and significant precision drop, as can be ex-pected from such distributional method.The last baseline uses WordNet for expansion.First we expand all the senses of each categoryname by their derivations and synonyms.
Each ob-tained term is then expanded by its hyponyms, orby its meronyms if it has no hyponyms.
Finally,the results are further expanded by their deriva-tions and synonyms.10 WordNet expansions im-prove substantially both Recall and F1 relative toNo Expansion, while decreasing precision.6.1.3 Wikipedia ResultsWe then used for expansion different subsetsof our rule base, producing alternative recall-precision tradeoffs.
Table 5 presents the most in-teresting results.
Using any subset of the rulesyields better performance than any of the otherautomatically constructed baselines (Lin, Snowand WikiBL).
Utilizing the most precise extrac-tion methods of Redirect and Be-Comp yields thehighest precision, comparable to No Expansion,but just a small recall increase.
Using the entirerule base yields the highest recall, while filteringrules by the Dice coefficient (with 0.1 threshold)substantially increases precision without harmingrecall.
With this configuration our automatically-constructed resource achieves comparable perfor-mance to the manually built WordNet.Finally, since a dictionary and an encyclopediaare complementary in nature, we applied the unionof WordNet and the filtered Wikipedia expansions.This configuration yields the best results: it main-tains WordNet?s precision and adds nearly 50% tothe recall increase of WordNet over No Expansion,indicating the substantial marginal contribution ofWikipedia.
Furthermore, with the fast growth ofWikipedia the recall of our resource is expected toincrease while maintaining its precision.9Downloaded from www.cs.ualberta.ca/lindek/demos.htm10We also tried expanding by the entire hyponym hierarchyand considering only the first sense of each synset, but themethod described above achieved the best performance.456Category Name Expanding TermsPolitics opposition, coalition, whip(a)Cryptography adversary, cryptosystem, keyMac PowerBook, Radius(b), Grab(c)Religion heaven, creation, belief, missionaryMedicine doctor, physician, treatment, clinicalComputer Graphics radiosity(d), rendering, siggraph(e)Table 6: Some Wikipedia rules not in WordNet, which con-tributed to text categorization.
(a) a legislator who enforceleadership desire (b) a hardware firm specializing in Macin-tosh equipment (c) a Macintosh screen capture software (d)an illumination algorithm (e) a computer graphics conferenceConfiguration Accuracy Accuracy DropWordNet + Wikipedia 60.0 % -Without WordNet 57.7 % 2.3 %Without Wikipedia 58.9 % 1.1 %Table 7: RTE accuracy results for ablation tests.Table 6 illustrates few examples of useful rulesthat were found in Wikipedia but not in WordNet.We conjecture that in other application settingsthe rules extracted from Wikipedia might showeven greater marginal contribution, particularly inspecialized domains not covered well by Word-Net.
Another advantage of a resource based onWikipedia is that it is available in many more lan-guages than WordNet.6.2 Recognizing Textual Entailment (RTE)As a second application-oriented evaluation wemeasured the contributions of our (filtered)Wikipedia resource and WordNet to RTE infer-ence (Giampiccolo et al, 2007).
To that end, weincorporated both resources within a typical basicRTE system architecture (Bar-Haim et al, 2008).This system determines whether a text entails an-other sentence based on various matching crite-ria that detect syntactic, logical and lexical cor-respondences (or mismatches).
Most relevant forour evaluation, lexical matches are detected whena Wikipedia rule?s LHS appears in the text andits RHS in the hypothesis, or similarly when pairsof WordNet synonyms, hyponyms-hypernyms andderivations appear across the text and hypothesis.The system?s weights were trained on the devel-opment set of RTE-3 and tested on RTE-4 (whichincluded this year only a test set).To measure the marginal contribution of the tworesources we performed ablation tests, comparingthe accuracy of the full system to that achievedwhen removing either resource.
Table 7 presentsthe results, which are similar in nature to those ob-tained for text categorization.
Wikipedia obtaineda marginal contribution of 1.1%, about half of theanalogous contribution of WordNet?s manually-constructed information.
We note that for currentRTE technology it is very typical to gain just afew percents in accuracy thanks to external knowl-edge resources, while individual resources usuallycontribute around 0.5?2% (Iftene and Balahur-Dobrescu, 2007; Dinu and Wang, 2009).
SomeWikipedia rules not in WordNet which contributedto RTE inference are Jurassic Park ?
MichaelCrichton, GCC?
Gulf Cooperation Council.7 Conclusions and Future WorkWe presented construction of a large-scale re-source of lexical reference rules, as useful in ap-plied lexical inference.
Extensive rule-level analy-sis showed that different recall-precision tradeoffscan be obtained by utilizing different extractionmethods.
It also identified major reasons for er-rors, pointing at potential future improvements.We further suggested a filtering method which sig-nificantly improved performance.Even though the resource was constructed byquite simple extraction methods, it was proven tobe beneficial within two different application set-ting.
While being an automatically built resource,extracted from a knowledge-base created for hu-man consumption, it showed comparable perfor-mance to WordNet, which was manually createdfor computational purposes.
Most importantly, italso provides complementary knowledge to Word-Net, with unique lexical reference rules.Future research is needed to improve resource?sprecision, especially for the All-N method.
Asa first step, we investigated a novel unsupervisedscore for rules extracted from definition sentences.We also intend to consider the rule base as a di-rected graph and exploit the graph structure forfurther rule extraction and validation.AcknowledgmentsThe authors would like to thank Idan Szpektorfor valuable advices.
This work was partiallysupported by the NEGEV project (www.negev-initiative.org), the PASCAL-2 Network of Excel-lence of the European Community FP7-ICT-2007-1-216886 and by the Israel Science Foundationgrant 1112/08.457ReferencesRoy Bar-Haim, Jonathan Berant, Ido Dagan, IddoGreental, Shachar Mirkin, Eyal Shnarch, and IdanSzpektor.
2008.
Efficient semantic deduction andapproximate matching over compact parse forests.In Proceedings of TAC.Martin S. Chodorow, Roy J. Byrd, and George E. Hei-dorn.
1985.
Extracting semantic hierarchies from alarge on-line dictionary.
In Proceedings of ACL.Ido Dagan, Oren Glickman, and Bernardo Magnini.2006.
The pascal recognising textual entailmentchallenge.
In Lecture Notes in Computer Science,volume 3944, pages 177?190.Scott Deerwester, Susan T. Dumais, George W. Furnas,Thomas K. Landauer, and Richard Harshman.
1990.Indexing by latent semantic analysis.
Journal of theAmerican Society for Information Science, 41:391?407.Georgiana Dinu and Rui Wang.
2009.
Inference rulesfor recognizing textual entailment.
In Proceedingsof the IWCS.Christiane Fellbaum, editor.
1998.
WordNet: An Elec-tronic Lexical Database (Language, Speech, andCommunication).
The MIT Press.Charles J. Fillmore, Collin F. Baker, and Hiroaki Sato.2002.
Seeing arguments through transparent struc-tures.
In Proceedings of LREC.Evgeniy Gabrilovich and Shaul Markovitch.
2007.Computing semantic relatedness using wikipedia-based explicit semantic analysis.
In Proceedings ofIJCAI.Danilo Giampiccolo, Bernardo Magnini, Ido Dagan,and Bill Dolan.
2007.
The third pascal recogniz-ing textual entailment challenge.
In Proceedings ofACL-WTEP Workshop.Oren Glickman, Eyal Shnarch, and Ido Dagan.
2006.Lexical reference: a semantic matching subtask.
InProceedings of EMNLP.Ralph Grishman, Lynette Hirschman, and Ngo ThanhNhan.
1986.
Discovery procedures for sublanguageselectional patterns: Initial experiments.
Computa-tional Linguistics, 12(3):205?215.Marti Hearst.
1992.
Automatic acquisition of hy-ponyms from large text corpora.
In Proceedings ofCOLING.Nancy Ide and Ve?ronis Jean.
1993.
Extractingknowledge bases from machine-readable dictionar-ies: Have we wasted our time?
In Proceedings ofKB & KS Workshop.Adrian Iftene and Alexandra Balahur-Dobrescu.
2007.Hypothesis transformation and semantic variabilityrules used in recognizing textual entailment.
In Pro-ceedings of the ACL-PASCAL Workshop on TextualEntailment and Paraphrasing.Jun?ichi Kazama and Kentaro Torisawa.
2007.
Ex-ploiting Wikipedia as external knowledge for namedentity recognition.
In Proceedings of EMNLP-CoNLL.J.
Richard Landis and Gary G. Koch.
1997.
Themeasurements of observer agreement for categoricaldata.
In Biometrics, pages 33:159?174.Dekang Lin.
1998a.
Automatic retrieval and clusteringof similar words.
In Proceedings of COLING-ACL.Dekang Lin.
1998b.
Dependency-based evaluation ofMINIPAR.
In Proceedings of the Workshop on Eval-uation of Parsing Systems at LREC.Bing Liu, Xiaoli Li, Wee Sun Lee, and Philip S. Yu.2004.
Text classification by labeling words.
In Pro-ceedings of AAAI.Andrew McCallum and Kamal Nigam.
1999.
Textclassification by bootstrapping with keywords, EMand shrinkage.
In Proceedings of ACL Workshop forunsupervised Learning in NLP.Dan Moldovan and Vasile Rus.
2001.
Logic formtransformation of wordnet and its applicability toquestion answering.
In Proceedings of ACL.Simone P. Ponzetto and Michael Strube.
2007.
De-riving a large scale taxonomy from wikipedia.
InProceedings of AAAI.Reinhard Rapp.
2002.
The computation of word asso-ciations: comparing syntagmatic and paradigmaticapproaches.
In Proceedings of COLING.Gerda Ruge.
1992.
Experiment on linguistically-basedterm associations.
Information Processing & Man-agement, 28(3):317?332.Rion Snow, Daniel Jurafsky, and Andrew Y. Ng.
2005.Learning syntactic patterns for automatic hypernymdiscovery.
In NIPS.Rion Snow, Daniel Jurafsky, and Andrew Y. Ng.
2006.Semantic taxonomy induction from heterogenousevidence.
In Proceedings of COLING-ACL.Fabian M. Suchanek, Gjergji Kasneci, and GerhardWeikum.
2007.
Yago: A core of semantic knowl-edge - unifying wordnet and wikipedia.
In Proceed-ings of WWW.Antonio Toral and Rafael Mun?oz.
2007.
A proposalto automatically build and maintain gazetteers fornamed entity recognition by using wikipedia.
InProceedings of NAACL/HLT.Yorick A. Wilks, Brian M. Slator, and Louise M.Guthrie.
1996.
Electric words: dictionaries, com-puters, and meanings.
MIT Press, Cambridge, MA,USA.Torsten Zesch, Iryna Gurevych, and Max Mu?hlha?user.2007.
Analyzing and accessing wikipedia as a lex-ical semantic resource.
In Data Structures for Lin-guistic Resources and Applications, pages 197?205.458
