Inferring knowledge from a large semantic networkDominique Dutoit Thierry PoibeauMemodata et CRISCO17, rue Dumont d?UrvilleF-14000 CaenThales and LIPNDomaine de CorbevilleF-91404 Orsaymemodata@wanadoo.fr thierry.poibeau@thalesgroup.comAbstractIn this paper, we present a rich semanticnetwork based on a differential analysis.We then detail implemented measuresthat take into account common anddifferential features between words.
In alast section, we describe some industrialapplications.1 Introduction: textual anddifferential semanticsIn textual analysis, each lexical item from atext is broken down in a list of semanticfeatures.
Features are intended to differentiateone word from another: a naive example wouldbe a feature back that could express thedifference between a chair and a stool.
Ofcourse, most of the time, features are not soeasy to define.
Some feature typologies havebeen provided, but there are still muchdiscussions about the nature of a feature in atext.
Most of the studies concerning differentialsemantics are based on a human approach totexts (this can lead to different problems, seebelow).
Textual Semantics, also calleddifferential semantics, is revisiting theconcepts of continental structuralism likedecomponential semantics (Cavazza, 1998).The problem is then to have a lexicalformalism that allows, for a lexical item, asimple description and some other featureswhich could be dynamically inferred from thetext.
For example, the dictionary shouldmention that a ?door?
is an aperture, but it ismore questionable to mention in the dictionarythat ?one can walk through a door?.
However,it can be an important point for the interpretationof a sentence in context.That is the reason why Pustejovskyintroduced in the nineties the notion of?generative lexicon?
(Pustejovsky, 1991)(Pustejovsky, 1995).
His analysis has to dealwith the notion of context: he proposes toassociate to a word a core semantic description(the fact that a ?door?
is an ?aperture?)
and toadd some additional features, which can beactivated in context (?walk-through?
is the telicrole of a ?door?).
However, Pustejovsky doesnot take into account important notions such aslexical chains and text coherence.
He proposesan abstract model distant from real texts.Semantic features can be used to check outtext coherence through the notion of ?isotopy?.This notion is ?the recurrence within a given textsection (regardless of sentence boundaries) ofthe same semantic feature through differentwords?
(Cavazza, 1998).
The recurrences ofthese features throughout a text allows to extractthe topic of interest and some other points whichare marginally tackled in the text.
It providesinteresting ways to glance at the text without afull reading of it; it also helps the interpretation.In this paper, we present a rich semanticnetwork based on a differential analysis.
Wethen detail implemented measures that take intoaccount common and differential featuresbetween words.
In a last section, we describesome industrial applications.2 The semantic networkThe semantic network used in this experiment isa multilingual network providing information for5 European languages.
We quickly describe thenetwork and then give some detail about itsoverall structure.2.1 Overall organisationThe semantic network we use is called TheIntegral Dictionary.
This database is basicallystructured as a merging of three semanticmodels available for five languages.
Themaximal coverage is given for the Frenchlanguage, with 185.000 word-meaningsencoded in the database.
English Languageappears like the second language in term ofcoverage with 79.000 word-meanings.
Threeadditional languages (Spanish, Italian andGerman) are present for about 39.500 senses.These smallest dictionaries, with universalidentifiers to ensure the translation, define theBasic Multilingual Dictionary available fromthe ELRA.
Grefenstette (1998) has done acorpus coverage evaluation for the BasicMultilingual Dictionary.
The newspaperscorpora defined by the US-government-sponsored Text Retrieval Conference (TREC,2000) has been used as a test corpus.
The resultwas that the chance of pulling a random nounout of the different corpus was on average92%.
This statistic is given for the BasicMultilingual Dictionary and, of course, theFrench Integral Dictionary reaches the highestcoverage.This semantic network is richer thanWordnet (Bagga et al, 1997) (Fellbaum,1998): it has got a larger number of links and isbased on a componential lexical analysis.Because words are highly interconnected, thesemantic network is easily tunable for a newcorpus (see section 2.3).2.2 Measure of distance between wordsWe propose an original way to measure thesemantic proximity between two words.
Thismeasure takes into account the similaritybetween words (their common features) butalso their differences.Let?s take the following example:\Universe\Person    \Sell    \Flowersell  seller florist    flowerFigure 1: An example of semantic graphThe comparison between two words is based onthe structure of the graph: the algorithmcalculates a score taking into account thecommon ancestors but also the different ones.Let?s take the example of seller and florist.
Theyhave two common ancestors: \Person and\Sell, but also one differential element: theconcept \Flower that dominates florist butnot seller.The notion of ?nearest common ancestor?
isclassical in graph theory.
We extend this notionto distinguish between ?symmetric nearestcommon ancestor?
(direct common ancestor forboth nodes) and ?asymmetric nearest commonancestor?
(common ancestor, indirect at least forone node).Definition: Distance between two nodes in agraphWe note d the distance between two nodes Aand B in a graph.
This distance is equivalent tothe number of intervals between two nodes Aand B.
We have d(A, B) = d(B,A).Example: We have d(sell, \Sell) = 1 andd(sell, \Universe) = 2, from Figure 1.
Notethat d(sell, \Sell) = d(\Sell, sell) = 1.Given:h(f) = the set of ancestors of f .c(f) = the set of arcs between a daughter f andthe graph?s root.We have:h(seller) = {\Sell, \Person,\Universe}c(seller) = { (seller, \Sell),(seller, \Person), (\Sell,\Universe), (\Person, \Universe)}etc.Definition: Nearest common ancestors(NCA)The nearest common ancestors between twowords A and B are the set of nodes that aredaughters of c(A) ?
c(B) and that are notancestors in c(A) ?
c(B).Example: From Figure 1, we have:c(seller) ?
c(florist) = { (\Sell,\Universe), (\Person, \Universe) }DaughterNodes(c(seller) ?c(florist)) = { \Sell, \Person }AncestorNodes (c(seller) ?c(florist)) = { \Universe }The NCA is equal to the set of nodes in the setDaughterNodes (c(seller) ?
c(florist))but not in AncestorNodes (c(seller) ?c(florist)).
Given that no element fromAncestorNodes (c(seller) ?
c(florist)) appearsin DaughterNodes(c(seller) ?
c(florist)), wehave:NCA(seller, florist) = { \Sell,\Person }We then propose a measure to calculate thesimilarity between two words.
The measure iscalled activation and only takes into accountthe common features between two nodes in thegraph.
An equal weight is attributed to eachNCA.
This weight corresponds to the minimaldistance between the NCA and each of the twoconcerned nodes.Definition: activation (d?
)The activation measure d  is equal to the meanof the weight of each NCA calculated from Aand  B :d?
(A, B) = ?=+n1iii ))NCA,B(d)NCA,A(d(n1The activation measure has the followingproperties:?
d?
(A, A) = 0, because A is the uniqueNCA of A?A.?
d?
(A, B) = d?
(B, A)(symmetry)?
d?
(A, B) + d?
(B, C)>= d?
(A, C)(euclidianity)Example : According to Figure 1, we haveNCA(seller, florist) = { \Sell,\Person}.
Consequently, if we assign a weightequal to 1 to each link, we have:d?
(seller, florist) = (d(seller,\Sell)+d(\Sell, florist) +d(seller, \Person)+ d(\Person,florist)) / 2d?
(seller, florist)= (1 + 1 + 1 +1) / 2d?
(seller, florist)= 2We can verify that:d?
(florist, seller)= d?
(seller,florist)= 2The set of NCA takes into account the commonfeatures between two nodes A et B.
We thenneed another measure to take into account theirdifferences.
To be able to do that, we mustdefine the notion of asymmetric nearest commonancestor.Definition: Asymmetric nearest commonancestor (ANCA)The asymmetric nearest common ancestors froma node A to a node B is contained into the set ofancestors of c(B) ?
c(A) which have a directnode belonging to h(A) but not to h(B).Example: According to Figure 1, we have:AncestorNodesNotNCA (c (seller) ?c(florist)) = { \Universe }The concept \Universe does not have anydaughter that is a member of h(seller) but notof h(florist).
As a consequence, we have:ANCA(seller, florist) = ?On the other hand, the concept \Universe has adaughter \Flower that belongs to h(florist)but not to h(seller).
As a consequence, wehave:ANCA(florist, seller) = {\Universe}It is now possible to measure the distancebetween two words from their differences.
Aweight is allocated to each link going from nodeNi, asymmetric nearest common ancestor, to Aand B.
The weight is equal to the length of theminimal length of the path going from A to Niand from B to Ni.Definition: proximity (d?
)The proximity measure takes into account thecommon features but also the differencesbetween two elements A and B and is definedby the following function:d?
(A,B)= d?
(A,B)+?=+n1iii ))ANCA,B(d)ANCA,A(d(n1Because the set of ANCA from a node A to anode B is not the same as the one from a node Bto a node A, the proximity measure has thefollowing properties:?
d?
(A, A) = 0, because ANCA(A, A) =?.?
d?
(A, B) ?
d?
(B, A)if the set ofANCA is not empty (antisymmetry)?
d?
(A, B) + d?
(B, C) >= d?
(A, C)(euclidianity)The proximity measure is dependent from thestructure of the network.
However, one mustnotice that this measure is a relative one: if thesemantic network evolves, all the proximitymeasures between nodes are changed but therelations between nodes can stay relativelystable (note that the graph presented on Figure1 is extremely simplified: the real network islargely more connected).Example: Let?s calculate the semanticproximity between seller and florist: d?
(seller, florist).
We will then be able tosee that the proximity between florist andseller does not produce the same result(antisymmetry).Given that ANCA(seller, florist) = ?, thesecond element of the formula based on the setof ANCA is equal to 0.
We then have:d?
(seller, florist)  =d?
(seller, florist) + 0d?
(seller, florist)  = 2 + 0d?
(seller, florist)  = 2ANCA(seller, florist) is the setcontaining the concept \Universe,because the concept \Flower is an ancestor offloristbut not of seller.
We then have:d?
(florist, seller) = d?
(florist,seller) + (d(seller, \Universe) +d(\Universe, florist)) / 1d?
(florist, seller) = 2 + ( 2 + 2 ) /1d?
(florist, seller) = 6To sum up, we have:d?
(florist, seller)= 2d?
(seller, florist)= 2d?
(seller, florist)= 2d?
(florist, seller) = 6The proximity measure discriminates floristfrom seller, whereas the activation measure issymmetric.
The componential analysis of thesemantic network reflects some weak semanticdifferences between words.2.3 Link weightingAll the links in the semantic network are typedso that a weight can be allocated to each link,given its type.
This mechanism allows to veryprecisely adapt the network to the task: one doesnot use the same weighting to perform lexicalacquisition as to perform word-sensedisambiguation.
This characteristic makes thenetwork highly adaptive and appropriate toexplore some kind of lexical tuning.3 Experiment and evaluationthrough an information filteringtaskIn this section we propose to evaluate thesemantic network and the measures that havebeen implemented through a set of NLPapplications related to information filtering.
Tohelp the end-user focus on relevant informationin texts, it is necessary to provide filtering tools.The idea is that the end-user defines a ?profile?describing his research interests (van Rijsbergen,1979) (Voorhees, 1999).A profile is a set of words, describing theuser?s domain of interest.
Unfortunately themeasures we have described are only concernedwith simple words, not with set of words.We first need to slightly modify theactivation measure, so that it accepts to comparetwo sets of words, and not only two simplewords1.
We propose to aggregate the set ofnodes in the graphs corresponding to the set ofwords in the profile.
This node has thefollowing properties:n1i)m(h)M(h i==n1i)m(c)M(c i==where h(M) is the set of ancestors of M andc(M)the set of links between M and the root ofthe graph.
It is then possible to compare twoset of words, and not only two simple words.In the framework of an InformationExtraction task, we want to filter texts to focuson sentences that are of possible interest for theextraction process (sentences that could allowto fill a given slot).
We then need a veryprecise filtering process performing at thesentence level2.
We used the activationmeasure for the filtering task.
A sentence iskept if the activation score between thefiltering profile and the sentence is above acertain threshold (empirically defined by theend-user).
A filtering profile is a set of wordsin relation with the domain or the slot to be fill,defined by the end-user.We made a set of experiments on a Frenchfinancial newswire corpus.
The topic was thesame as in the MUC-6 conference (1995):companies purchasing other companies.
Wemade the experiment on a set of 100 newsstories (no training phase).The filtering profile was composed of thefollowing words: rachat, cession,enterprise (buy, purchase, company).The corpus has been manually processed toidentify relevant sentences (the referencecorpus).
We then compare the result of thefiltering task with the reference corpus.1This measure allows to compare two set of words,or two sentences.
For a sentence, it is first necessaryto delete empty words, to obtain a set of full words2This is original since most of the systems so farare concern with texts filtering, not sentencefiltering.In the different experiments we made, wemodified different parameters such as thefiltering threshold (the percentage of sentencesto be kept).
We obtained the following results:10% 20% 30% 40% 50%Precision .72 .54 .41 .33 .28Recall .43 .64 .75 .81 .85We also tried to normalize the corpus, that is tosay to replace entities by their type, to improvethe filtering process.
We used a state-of-the-artnamed entity recogniser that was part of a largertoolbox for named entity recognition.10% 20% 30% 40% 50%Precision .75 .56 .43 .34 .29Recall .49 .71 .82 .89 .94We notice that we obtain, from 10% of thecorpus, a 75% precision ratio (3 sentences out of4 are relevant) and nearly a 50% recall ratio.
Themain interest of this process is to help the end-user directly focus on relevant pieces of text.This strategy is very close from the EXDISCOsystem developed by R. Yangarber at NYU(2000), even if the algorithms we use aredifferent.4 Application services overviewIn this section, we detail some of theapplications developed from the semanticnetwork described above.
All of theseapplications are available through java API.They are part of the applicative part of thenetwork called the Semiograph3.
Most of theexamples will be given in French.4.1 Query expansionThis application gives a help to the users whoquery the web through a search engine.
In thisframework, the Semiograph has to determinate3Part of Speech tagging, syntactic analysis forFrench and Word Sense Disambiguation are alsoAPIs of the Semiograph.the sense of the query and generate (orsuggest) an expansion of the query inaccordance to the semantic and syntacticproperties of the source.The Semiograph links independentmechanisms of expansion defined by the user.Eight mechanisms are available :?
Alias: to get the graphics variant?
Synonyms: to get synonyms for ameaning?
Hypernyms: to get hypernyms for ameaning?
Hyponyms: to get hyponyms for ameaning?
Inflected forms : to get the inflected for ameaning?
Derived forms: to get correct lexicalfunctions in accordance or not with thesyntactical proposition?
Geographical belonging: to get toponyms?
Translation (language parameter) : to get atranslation of the query.Figure 2: Query expansion4.2 Word sense disambiguation andTerm spottingLexical semantics provides an originalapproach for the term spotting task.
Generallyspeaking, the main topics addressed by adocument are expressed by ambiguous words.Most of the time, these words can bedisambiguated from the context.
If a documenttreats of billiards, the context of billiards isnecessarily saturated by terms of larger topicslike games, competition, dexterity... and terms independence with billiard like ball, cue, cannon...Using this property, lexical topics are foundby measuring the semantic proximity of eachplain word of a text with the text itself.
Termsthat have the minimal semantic proximity are thebest descriptors.Note that this property may be used to verifythe relevance of keywords manually given by awriter.
An application may be the  struggle tothe spamming of search engine.
To give anexample of result of lexical summary, thealgorithm applied to this paper provides in the20 best words the terms : lexicon, dictionary,semantic network, semantics, measures anddisambiguation.
All these terms are highlyrelevant.4.3 Emails sorting and answeringIn this application, we have to classify a flow ofdocuments according to a set of existingprofiles.
Most systems execute this task after alearning phase.
A learning phase causes aproblem because it needs a costly preliminarymanual tagging of documents.
It is thenattractive to see if a complex lexicon couldperform an accurate classification without anylearning phase.In our experiments the end-user must have todefine profiles that correspond to his domains ofinterest.
The formalism is very light: firstly, wedefine an identifier for each profile; secondly wedefine a definition of this profile (a set ofrelevant terms according to the domain).
On thefollowing examples, identifiers are givenbetween parentheses and definitions are givenafter.
[guerre du Kosovo] guerre du Kosovo[tabac et jeunesse] tabac et jeunesse[alcoolisme et Bretagne] alcoolisme etBretagne[investissement immobilier en Ile-de-France] achat, vente et march?immobilier en ?le-de-FranceThe definitions may be given in English with theexactly same result.
The following text :Les loyers stagnent ?
Paris mais la baisse de laTVA sur les d?penses de r?paration de l?habitatdevrait soutenir le march?
de l?anciengives in term of semantic proximity:[guerre du Kosovo]  135[tabac et jeunesse]  140[alcoolisme et Bretagne]  129[investissement immobilier enIle-de-France]  9We observe that differences between themailboxes are very marked (the best score isthe lowest one).
Note that this approach maybe used to help the classifying of web sites thatis today entirely manually carry out.5 ConclusionIn this paper, we have shown an efficientalgorithm to semi-automatically acquireknowledge from a semantic network and acorpus.
A set of basic services are alsoavailable through java APIs developed abovethe semantic network.
We have shown that thisset of elements offers a versatile toolbox for alarge variety of NLP applications.6 ReferencesBagga A., Chai J.Y.
et Biermann A.
The Role ofWORDNET in the Creation of a Trainable MessageUnderstanding System.
In Proceedings of the 14thNational Conference on Artificial Intelligenceand the Ninth Conference on the InnovativeApplications of Artificial Intelligence(AAAI/IAAI?97), Rhode Island, 1997, pp.
941?948.Basili R., Catizone R., Pazienza M.T.,Stevenson M., Velardi P., Vindigni M. andWilks Y.
(1998) An empirical approach toLexical Tuning.
Workshop on Adapting lexicaland corpus resources to sublanguages andapplications, LREC (Grenada).Cavazza M. (1998) Textual semantics and corpus-specific lexicons.
Workshop on Adapting lexicaland corpus resources to sublanguages andapplications, LREC (Grenada).Fellbaum C. (1998) WordNet : An ElectronicLexical Database, edited by Fellbaum, M.I.T.press.Grefenstette G. (1998) Evaluating the adequancy of amultilingual transfer dictionary for the CrossLanguage Information Retrieval, LREC 1998.MUC-6 (1995) Proceedings Sixth MessageUnderstanding Conference (DARPA), MorganKaufmann Publishers, San Francisco.Pustejovsky J.
(1991) The generative lexicon.Computational Linguistics, 17(4).Pustejovsky J.
(1995) The generative lexicon, MITPress, Cambridge.TREC (2000)  The Ninth Text REtrieval Conference(TREC 9).
Gaithersburg, 2000.http://trec.nist.gov/pubs/trec9/t9_proceedings.html.van Rijsbergen C.J.
(1979) Information Retrieval.Butterworths, Londres.Voorhees, E.M. (1999) Natural language processingand information retrieval.
In M.T.
PAZIENZA (?d.
),Information extraction, toward scalable, adaptablesystems, Springer Verlag (Lecture Notes incomputer Science), Heidelberg, pp.
32?48.Yangarber R. (2000) Scenario Customization forInformation Extraction.
PhD Thesis, New YorkUniversity.
