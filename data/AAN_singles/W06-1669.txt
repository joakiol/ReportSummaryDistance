Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing (EMNLP 2006), pages 585?593,Sydney, July 2006. c?2006 Association for Computational LinguisticsTwo graph-based algorithms for state-of-the-art WSDEneko Agirre, David Mart?
?nez, Oier Lo?pez de Lacalle and Aitor SoroaIXA NLP GroupUniversity of the Basque CountryDonostia, Basque Contrya.soroa@si.ehu.esAbstractThis paper explores the use of two graphalgorithms for unsupervised induction andtagging of nominal word senses based oncorpora.
Our main contribution is the op-timization of the free parameters of thosealgorithms and its evaluation against pub-licly available gold standards.
We presenta thorough evaluation comprising super-vised and unsupervised modes, and bothlexical-sample and all-words tasks.
Theresults show that, in spite of the infor-mation loss inherent to mapping the in-duced senses to the gold-standard, theoptimization of parameters based on asmall sample of nouns carries over to allnouns, performing close to supervised sys-tems in the lexical sample task and yield-ing the second-best WSD systems for theSenseval-3 all-words task.1 IntroductionWord sense disambiguation (WSD) is a keyenabling-technology.
Supervised WSD tech-niques are the best performing in public evalu-ations, but need large amounts of hand-taggeddata.
Existing hand-annotated corpora like Sem-Cor (Miller et al, 1993), which is annotated withWordNet senses (Fellbaum, 1998) allow for asmall improvement over the simple most frequentsense heuristic, as attested in the all-words track ofthe last Senseval competition (Snyder and Palmer,2004).
In theory, larger amounts of training data(SemCor has approx.
700K words) would improvethe performance of supervised WSD, but no cur-rent project exists to provide such an expensive re-source.Supervised WSD is based on the ?fixed-list ofsenses?
paradigm, where the senses for a tar-get word are a closed list coming from a dic-tionary or lexicon.
Lexicographers and seman-ticists have long warned about the problems ofsuch an approach, where senses are listed sepa-rately as discrete entities, and have argued in fa-vor of more complex representations, where, forinstance, senses are dense regions in a contin-uum (Cruse, 2000).Unsupervised WSD has followed this line ofthinking, and tries to induce word senses directlyfrom the corpus.
Typical unsupervised WSD sys-tems involve clustering techniques, which grouptogether similar examples.
Given a set of inducedclusters (which represent word uses or senses1),each new occurrence of the target word will becompared to the clusters and the most similar clus-ter will be selected as its sense.Most of the unsupervised WSD work has beenbased on the vector space model, where eachexample is represented by a vector of features(e.g.
the words occurring in the context), andthe induced senses are either clusters of ex-amples (Schu?tze, 1998; Purandare and Peder-sen, 2004) or clusters of words (Pantel and Lin,2002).
Recently, Ve?ronis (Ve?ronis, 2004) has pro-posed HyperLex, an application of graph modelsto WSD based on the small-world properties ofcooccurrence graphs.
Graph-based methods havegained attention in several areas of NLP, includingknowledge-based WSD (Mihalcea, 2005; Navigliand Velardi, 2005) and summarization (Erkan andRadev, 2004; Mihalcea and Tarau, 2004).The HyperLex algorithm presented in (Ve?ronis,2004) is entirely corpus-based.
It builds a cooccur-rence graph for all pairs of words cooccurring inthe context of the target word.
Ve?ronis shows thatthis kind of graph fulfills the properties of smallworld graphs, and thus possesses highly connected1Unsupervised WSD approaches prefer the term ?worduses?
to ?word senses?.
In this paper we use them inter-changeably to refer to both the induced clusters, and to theword senses from some reference lexicon.585components (hubs) in the graph.
These hubs even-tually identify the main word uses (senses) of thetarget word, and can be used to perform wordsense disambiguation.
These hubs are used as arepresentation of the senses induced by the sys-tem, the same way that clusters of examples areused to represent senses in clustering approachesto WSD (Purandare and Pedersen, 2004).One of the problems of unsupervised systemsis that of managing to do a fair evaluation.Most of current unsupervised systems are evalu-ated in-house, with a brief comparison to a re-implementation of a former system, leading to aproliferation of unsupervised systems with littleground to compare among them.In preliminary work (Agirre et al, 2006), wehave shown that HyperLex compares favorablyto other unsupervised systems.
We defined asemi-supervised setting for optimizing the free-parameters of HyperLex on the Senseval-2 En-glish Lexical Sample task (S2LS), which con-sisted on mapping the induced senses onto theofficial sense inventory using the training part ofS2LS.
The best parameters were then used on theSenseval-3 English Lexical Sample task (S3LS),where a similar semi-supervised method was usedto output the official sense inventory.This paper extends the previous work in sev-eral aspects.
First of all, we adapted the PageR-ank graph-based method (Brin and Page, 1998)for WSD and compared it with HyperLex.We also extend the previous evaluation scheme,using measures in the clustering community whichonly require a gold standard clustering and nomapping step.
This allows for having a purelyunsupervised WSD system, and at the same timecomparing supervised and unsupervised systemsaccording to clustering criteria.We also include the Senseval-3 English All-words testbed (S3AW), where, in principle, unsu-pervised and semi-supervised systems have an ad-vantage over purely supervised systems due to thescarcity of training data.
We show that our sys-tem is competitive with supervised systems, rank-ing second.This paper is structured as follows.
We firstpresent two graph-based algorithms, HyperLexand PageRank.
Section 3 presents the two evalu-ation frameworks.
Section 4 introduces parameteroptimization.
Section 5 shows the experimentalsetting and results.
Section 6 analyzes the resultsand presents related work.
Finally, we draw theconclusions and advance future work.2 A graph algorithm for corpus-basedWSDThe basic steps for our implementation of Hyper-Lex and its variant using PageRank are common.We first build the cooccurrence graph, then we se-lect the hubs that are going to represent the sensesusing two different strategies inspired by Hyper-Lex and PageRank.
We are then ready to use theinduced senses to do word sense disambiguation.2.1 Building cooccurrence graphsFor each word to be disambiguated, a text corpusis collected, consisting of the paragraphs wherethe word occurs.
From this corpus, a cooccur-rence graph for the target word is built.
Verticesin the graph correspond to words2 in the text (ex-cept the target word itself).
Two words appear-ing in the same paragraph are said to cooccur, andare connected with edges.
Each edge is assigneda weight which measures the relative frequency ofthe two words cooccurring.
Specifically, let wij bethe weight of the edge3 connecting nodes i and j,then wij = 1 ?
max[P (i | j), P (j | i)], whereP (i | j) = freqijfreqj and P (j | i) =freqijfreqi .The weight of an edge measures how tightlyconnected the two words are.
Words which alwaysoccur together receive a weight of 0.
Words rarelycooccurring receive weights close to 1.2.2 Selecting hubs: HyperLex vs. PageRankOnce the cooccurrence graph is built, Ve?ronis pro-poses a simple iterative algorithm to obtain itshubs.
At each step, the algorithm finds the ver-tex with highest relative frequency4 in the graph,and, if it meets some criteria, it is selected as a hub.These criteria are determined by a set of heuristicparameters, that will be explained later in Section4.
After a vertex is selected to be a hub, its neigh-bors are no longer eligible as hub candidates.
Atany time, if the next vertex candidate has a relativefrequency below a certain threshold, the algorithmstops.Another alternative is to use the PageRank algo-rithm (Brin and Page, 1998) for finding hubs in the2Following Ve?ronis, we only work on nouns.3The cooccurrence graph is undirected, i.e.
wij = wji4In cooccurrence graphs, the relative frequency of a vertexand its degree are linearly related, and it is therefore possibleto avoid the costly computation of the degree.586coocurrence graph.
PageRank is an iterative algo-rithm that ranks all the vertices according to theirrelative importance within the graph following arandom-walk model.
In this model, a link betweenvertices v1 and v2 means that v1 recommends v2.The more vertices recommend v2, the higher therank of v2 will be.
Furthermore, the rank of a ver-tex depends not only on how many vertices pointto it, but on the rank of these vertices as well.Although PageRank was initially designed towork with directed graphs, and with no weightsin links, the algorithm can be easily extendedto model undirected graphs whose edges areweighted.
Specifically, let G = (V, E) be an undi-rected graph with the set of vertices V and set ofedges E. For a given vertex vi, let In(vi) be the setof vertices pointing to it5.
The rank of vi is definedas:P (vi) = (1?
d) + d?j?In(vi)wji?k?In(vj) wjkP (vj)where wij is the weight of the link between ver-tices vi and vj , and 0 ?
d ?
1. d is called thedamping factor and models the probability of aweb surfer standing at a vertex to follow a linkfrom this vertex (probability d) or to jump to a ran-dom vertex in the graph (probability 1 ?
d).
Thefactor is usually set at 0.85.The algorithm initializes the ranks of the ver-tices with a fixed value (usually 1N for a graph withN vertices) and iterates until convergence below agiven threshold is achieved, or, more typically, un-til a fixed number of iterations are executed.
Notethat the convergence of the algorithms doesn?t de-pend on the initial value of the ranks.After running the algorithm, the vertices of thegraph are ordered in decreasing order according toits rank, and a number of them are chosen as themain hubs of the word.
The hubs finally selecteddepend again of some heuristics and will be de-scribed in section 4.2.3 Using hubs for WSDOnce the hubs that represent the senses of the wordare selected (following any of the methods pre-sented in the last section), each of them is linkedto the target word with edges weighting 0, andthe Minimum Spanning Tree (MST) of the wholegraph is calculated and stored.5As G is undirected, the in-degree of a vertex v is equalto its out-degree.The MST is then used to perform word sensedisambiguation, in the following way.
For everyinstance of the target word, the words surroundingit are examined and looked up in the MST.
By con-struction of the MST, words in it are placed underexactly one hub.
Each word in the context receivesa set of scores s, with one score per hub, where allscores are 0 except the one corresponding to thehub where it is placed.
If the scores are organizedin a score vector, all values are 0, except, say, thei-th component, which receives a score d(hi, v),which is the distance between the hub hi and thenode representing the word v. Thus, d(hi, v) as-signs a score of 1 to hubs and the score decreasesas the nodes move away from the hub in the tree.For a given occurrence of the target word, thescore vectors of all the words in the context areadded, and the hub that receives the maximumscore is chosen.3 Evaluating unsupervised WSD systemsAll unsupervised WSD algorithms need some ad-dition in order to be evaluated.
One alternative, asin (Ve?ronis, 2004), is to manually decide the cor-rectness of the hubs assigned to each occurrenceof the words.
This approach has two main disad-vantages.
First, it is expensive to manually verifyeach occurrence of the word, and different runs ofthe algorithm need to be evaluated in turn.
Sec-ond, it is not an easy task to manually decide ifan occurrence of a word effectively correspondswith the use of the word the assigned hub refersto, specially considering that the person is given ashort list of words linked to the hub.
Besides, it iswidely acknowledged that people are leaned not tocontradict the proposed answer.A second alternative is to evaluate the systemaccording to some performance in an application,e.g.
information retrieval (Schu?tze, 1998).
This isa very attractive idea, but requires expensive sys-tem development and it is sometimes difficult toseparate the reasons for the good (or bad) perfor-mance.A third alternative would be to devise a methodto map the hubs (clusters) returned by the systemto the senses in a lexicon.
Pantel and Lin (2002)automatically mapped the senses to WordNet, andthen measured the quality of the mapping.
Morerecently, tagged corpora have been used to mapthe induced senses, and then compare the sys-tems over publicly available benchmarks (Puran-587dare and Pedersen, 2004; Niu et al, 2005; Agirreet al, 2006), which offers the advantage of com-paring to other systems, but converts the wholesystem into semi-supervised.
See Section 5 formore details on these systems.
Note that the map-ping introduces noise and information loss, whichis a disadvantage when comparing to other sys-tems that rely on the gold-standard senses.Yet another possibility is to evaluate the inducedsenses against a gold standard as a clustering task.Induced senses are clusters, gold standard sensesare classes, and measures from the clustering lit-erature like entropy or purity can be used.
In thiscase the manually tagged corpus is taken to be thegold standard, where a class is the set of examplestagged with a sense.We decided to adopt the last two alternatives,since they allow for comparison over publiclyavailable systems of any kind.3.1 Evaluation of clustering: hubs as clustersIn this setting the selected hubs are treated asclusters of examples and gold standard senses areclasses.
In order to compare the clusters with theclasses, hand annotated corpora are needed (for in-stance Senseval).
The test set is first tagged withthe induced senses.
A perfect clustering solutionwill be the one where each cluster has exactly thesame examples as one of the classes, and viceversa.
The evaluation is completely unsupervised.Following standard cluster evaluation prac-tice (Zhao and Karypis, 2005), we consider threemeasures: entropy, purity and Fscore.
The entropymeasure considers how the various classes of ob-jects are distributed within each cluster.
In gen-eral, the smaller the entropy value, the better theclustering algorithm performs.
The purity mea-sure considers the extent to which each clustercontained objects from primarily one class.
Thelarger the values of purity, the better the cluster-ing algorithm performs.
The Fscore is used in asimilar fashion to Information Retrieval exercises,with precision and recall defined as the percent-age of correctly ?retrieved?
examples for a clus-ter (divided by total cluster size), and recall as thepercentage of correctly ?retrieved?
examples for acluster (divided by total class size).
For a formaldefinition refer to (Zhao and Karypis, 2005).
If theclustering is identical to the original classes in thedatasets, FScore will be equal to one which meansthat the higher the FScore, the better the clusteringis.3.2 Evaluation as supervised WSD: mappinghubs to senses(Agirre et al, 2006) presents a straightforwardframework that uses hand-tagged material in or-der to map the induced senses into the senses usedin a gold standard .
The WSD system first tags thetraining part of some hand-annotated corpus withthe induced hubs.
The hand labels are then usedto construct a matrix relating assigned hubs to ex-isting senses, simply counting the times an occur-rence with sense sj has been assigned hub hi.
Inthe testing step we apply the WSD algorithm overthe test corpus, using the hubs-to-senses matrix toselect the sense with highest weights.
See (Agirreet al, 2006) for further details.4 Tuning the parametersThe behavior of the original HyperLex algorithmwas influenced by a set of heuristic parameters,which were set by Ve?ronis following his intuition.In (Agirre et al, 2006) we tuned the parameters us-ing the mapping strategy for evaluation.
We set arange for each of the parameters, and evaluated thealgorithm for each combination of the parameterson a fixed set of words (S2LS), which was differ-ent from the final test sets (S3LS and S3AW).
Thisensures that the chosen parameter set can be usedfor any noun, and is not overfitted to a small set ofnouns.In this paper, we perform the parameter tuningaccording to four different criteria, i.e., best su-pervised performance and best unsupervised en-tropy/purity/FScore performance.
At the end, wehave four sets of parameters (those that obtainedthe best results in S2LS for each criterion), andeach set is then selected to be run against the S3LSand S3AW datasets.The parameters of the graph-based algorithmcan be divided in two sets: those that affect howthe cooccurrence graph is built (p1?p4 below), andthose that control the way the hubs are extractedfrom it (p5?p8 below).p1 Minimum frequency of edges (occurrences)p2 Minimum frequency of vertices (words)p3 Edges with weights above this value are removedp4 Context containing fewer words are not processedp5 Minimum number of adjacent vertices in a hubp6 Max.
mean weight of the adjacent vertices of a hubp7 Minimum frequency of hubsp8 Number of selected hubs588Vr opt Pr fr (p7) and Pr fx (p8)Vr Range Best Range Bestp1 5 1-3 1 1-3 2p2 10 2-4 3 2-4 3p3 .9 .3-.7 .4 .4-.5 .5p4 4 4 4 4 4p5 6 1-7 1 ?
?p6 .8 .6-.95 .95 ?
?p7 .001 .0009-.003 .001 .0015-.0025 .0016p8 ?
?
?
50-65 55Table 1: Parameters of the HyperLex algorithmBoth strategies to select hubs from the coocur-rence graph (cf.
Sect.
2.2) share parameters p1?p4.
The algorithm proposed by Ve?ronis uses p5?p6 as requirements for hubs, and p7 as the thresh-old to stop looking for more hubs: candidates withfrequency below p7 are not eligible to be hubs.Regarding PageRank the original formulationdoes not have any provision for determining whichare hubs and which not, it just returns a weightedlist of vertices.
We have experimented with twomethods: a threshold for the frequency of the hubs(as before, p7), and a fixed number of hubs for ev-ery target word (p8).
For a shorthand we use Vr forVeronis?
original formulation with default param-eters, Vr opt for optimized parameters, and Pr frand Pr fx respectively for the two ways of usingPageRank.Table 1 lists the parameters of the HyperLex al-gorithm, with the default values proposed for themin the original work (second column), the rangesthat we explored, and the optimal values accordingto the supervised recall evaluation (cf.
Sect.
3.1).For Vr opt we tried 6700 combinations.
PageRankhas less parameters, and we also used the previousoptimization of Vr opt to limit the range of p4, soPr fr and Pr fx get respectively 180 and 288 com-binations.5 Experiment setting and resultsTo evaluate the HyperLex algorithm in a standardbenchmark, we will first focus on a more exten-sive evaluation of S3LS and then see the resultsin S3AW (cf.
Sec.
5.4).
Following the designfor evaluation explained in Section 3, we use thestandard train-test split for the supervised evalua-tion, while the unsupervised evaluation only usesthe test part.Table 2 shows the results of the 4 variants ofour algorithm.
Vr stands for the original Vero-nis algorithm with default parameters, Vr opt toour optimized version, and Pr fr and Pr fx to theSup.
UnsupervisedRec.
Entr.
Pur.
FSVr 59.9 50.3 58.2 44.1Vr opt 64.6 18.3 78.5 35.0Pr fr 64.5 18.7 77.2 34.3Pr fx 62.2 25.4 72.2 33.31ex-1hub 40.1 0.0 100.0 14.5MFS 54.5 53.2 52.8 28.3S3LS-best 72.9 19.9 67.3 63.8kNN-all 70.6 21.2 64.0 60.6kNN-BoW 63.5 22.6 61.1 57.1Cymfony (10%-S3LS) 57.9 25.0 55.7 52.0Prob0 (MFS-S3) 54.2 28.8 49.3 46.0clr04 (MFS-Sc) 48.8 25.8 52.5 46.2Ciaosenso (MFS-Sc) 48.7 28.0 50.3 48.8duluth-senserelate 47.5 27.2 51.1 44.9Table 2: Results for the nouns in S3LS using the 4 meth-ods (Vr, Vr opt, Pr fr and Pr fx).
Each of the methods wasoptimized in S2LS using the 4 evaluation criteria (Supervisedrecall, Entropy, Purity and Fscore) and evaluated on S3LS ac-cording to the respective evaluation criteria (in the columns).Two baselines, plus 3 supervised and 5 unsupervised systemsare also shown.
Bold is used for best results in each category.two variants of PageRank.
In the columns we findthe evaluation results according to our 4 criteria.For supervised evaluation we indicate only recall,which in our case equals precision, as the cover-age is 100% in all cases (values returned by theofficial Senseval scorer).
We also include 2 base-lines, a system returning a single cluster (that ofthe most frequent sense, MFS), and another re-turning one cluster for each example (1ex-1hub).The last rows list the results for 3 supervised and5 unsupervised systems (see Sect.
5.1).
We willcomment on the result of this table from differentperspectives.5.1 Supervised evaluationIn this subsection we will focus in the first fourevaluation rows in Table 2.
All variants of the al-gorithm outperform by an ample margin the MFSand the 1ex-1hub baselines when evaluated onS3LS recall.
This means that the method is ableto learn useful hubs.
Note that we perform this su-pervised evaluation just for comparison with othersystems, and to prove that we are able to providehigh performance WSD.The default parameter setting (Vr) gets theworst results, followed by the fixed-hub imple-mentation of PageRank (Pr fx).
Pagerank withfrequency threshold (Pr fr) and the optimizedVeronis (Vr opt) obtain a 10 point improvementover the MFS baseline with very similar results(the difference is not statistically significant ac-cording to McNemar?s test at 95% confidence589level).Table 2 also shows the results of three super-vised systems.
These results (and those of theother unsupervised systems in the table) where ob-tained from the Senseval website, and the onlyprocessing we did was to filter nouns.
S3LS-beststands for the the winner of S3LS (Mihalcea et al,2004), which is 8.3 points over our method.
Wealso include the results of two of our in-house sys-tems.
kNN-all is a state-of-the-art system (Agirreet al, 2005) using wide range of local and top-ical features, and only 2.3 points below the bestS3LS system.
kNN-BoW which is the same super-vised system, but restricted to bag-of-words fea-tures only, which are the ones used by our graph-based systems.
The table shows that Vr opt andPr fr are one single point from kNN-BoW, whichis an impressive result if we take into account theinformation loss of the mapping step and that wetuned our parameters on a different set of words.The last 5 rows of Table 2 show several un-supervised systems, all of which except Cym-fony (Niu et al, 2005) and (Purandare and Ped-ersen, 2004) participated in S3LS (check (Mihal-cea et al, 2004) for further details on the systems).We classify them according to the amount of ?su-pervision?
they have: some have access to most-frequent information (MFS-S3 if counted overS3LS, MFS-Sc if counted over SemCor), some use10% of the S3LS training part for mapping (10%-S3LS).
Only one system (Duluth) did not use inany way hand-tagged corpora.The table shows that Vr opt and Pr fr are morethan 6 points above the other unsupervised sys-tems, but given the different typology of unsuper-vised systems, it?s unfair to draw definitive con-clusions from a raw comparison of results.
Thesystem coming closer to ours is that described in(Niu et al, 2005).
They use hand tagged corporawhich does not need to include the target word totune the parameters of a rather complex clusteringmethod which does use local features.
They do usethe S3LS training corpus for mapping.
For everysense of the target word, three of its contexts inthe train corpus are gathered (around 10% of thetraining data) and tagged.
Each cluster is then re-lated with its most frequent sense.
The mappingmethod is similar to ours, but we use all the avail-able training data and allow for different hubs tobe assigned to the same sense.Another system similar to ours is (Purandareand Pedersen, 2004), which unfortunately wasevaluated on Senseval 2 data and is not includedin the table.
The authors use first and second or-der bag-of-word context features to represent eachinstance of the corpus.
They apply several cluster-ing algorithms based on the vector space model,limiting the number of clusters to 7.
They alsouse all available training data for mapping, butgiven their small number of clusters they opt for aone-to-one mapping which maximizes the assign-ment and discards the less frequent clusters.
Theyalso discard some difficult cases, like senses andwords with low frequencies (10% of total occur-rences and 90, respectively).
The different test setand mapping system make the comparison diffi-cult, but the fact that the best of their combina-tions beats MFS by 1 point on average (47.6% vs.46.4%) for the selected nouns and senses make usthink that our results are more robust (nearly 10%over MFS).5.2 Clustering evaluationThe three columns corresponding to fully unsu-pervised evaluation in Table 2 show that all our3 optimized variants easily outperform the MFSbaseline.
The best results are in this case for theoptimized Veronis, followed closely by Pagerankwith frequency threshold.The comparison with the supervised and unsu-pervised systems shows that our system gets betterentropy and purity values, but worse FScore.
Thiscan be explained by the bias of entropy and puritytowards smaller and more numerous clusters.
Infact the 1ex-1hub baseline obtains the best entropyand purity scores.
Our graph-based system tendsto induce a large number of senses (with averagesof 60 to 70 senses).
On the other hand FScore pe-nalizes the systems inducing a different number ofclusters.
As the supervised and unsupervised sys-tems were designed to return the same (or similar)number of senses as in the gold standard, they at-tain higher FScores.
This motivated us to comparethe results of the best parameters across evaluationmethods.5.3 Comparison across evaluation methodsTable 3 shows all 16 evaluation possibilities foreach variant of the algorithm, depending of theevaluation criteria used in S2LS (in the rows)and the evaluation criteria used in S3LS (in thecolumns).
This table shows that the best results (inbold for each variant) tend to be in the diagonal,590that is, when the same evaluation criterion is usedfor optimization and test, but it is not decisive.
Ifwe take the first row (supervised evaluation) as themost credible criterion, we can see that optimiz-ing according to entropy and purity get similar andsometimes better result (Pr fr and Pr fx).
On thecontrary the Fscore yields worse results by far.This indicates that a purely unsupervised sys-tem evaluated according to the gold standard(based on entropy or purity) yields optimal param-eters similar to the supervised (mapped) version.This is an important result, as it shows that thequality in performance does not come from themapping step, but from the algorithm and opti-mal parameter setting.
The table shows that op-timization on purity and entropy criteria do corre-late with good performance in the supervised eval-uation.The failure of FScore based optimization, in ouropinion, indicates that our clustering algorithmprefers smaller and more numerous clusters, com-pared to the gold standard.
FScore prefers cluster-ing solutions that have a similar number of clustersto that of the gold standard, but it is unable to drivethe optimization or our algorithm towards good re-sults in the supervised evaluation.All in all, the best results are attained withsmaller and more numerous hubs, a kind of micro-senses.
This effect is the same for all three vari-ants tried and all evaluation criteria, with Fscoreyielding less clusters.
At first we were uncom-fortable with this behavior, so we checked whetherHyperLex was degenerating into a trivial solution.This was the main reason to include the 1ex-1hubbaseline, which simulates a clustering algorithmreturning one hub per example, and its precisionwas 40.1, well below the MFS baseline.
We alsorealized that our results are in accordance withsome theories of word meaning, e.g.
the ?indef-initely large set of prototypes-within-prototypes?envisioned in (Cruse, 2000).
Ted Pedersen hasalso observed a similar behaviour in his vector-space model clustering experiments (PC).
We nowthink that the idea of having many micro-sensesis attractive for further exploration, specially if weare able to organize them into coarser hubs in fu-ture work.5.4 S3AW taskIn the Senseval-3 all-words task (Snyder andPalmer, 2004) all words in three document ex-Sup.
UnsupervisedAlg.
Opt.
Rec.
Entr.
Pur.
FSVr Sup 64.6 18.4 77.9 30.0Ent 64.6 18.3 78.3 29.1Pur 63.7 19.0 78.5 30.8Fsc 60.4 38.2 63.5 35.0Pr fr Sup 64.5 20.8 76.1 28.6Ent 64.6 18.7 77.7 27.2Pur 64.7 19.3 77.2 27.6Fsc 61.2 36.0 65.2 34.3Pr fx Sup 62.2 28.2 69.3 29.5Ent 63.1 25.4 72.2 28.4Pur 63.1 25.4 72.2 28.4Fsc 54.5 32.9 66.5 33.3Table 3: Cross-evaluation comparison.
In the rows the eval-uation method for optmizing over S2LS is shown, and in thecolumns the result over S3LS according to the different eval-uation methods.recallkuaw 70.9Pr fr 70.7Vr opt 70.1GAMBL 70.1MFS 69.9LCCaw 68.6Table 4: Results for the nouns in S3AW, compared to themost frequent baseline and the top three supervised systemscerpts need to be disambiguated.
Given thescarce amount of training data available in Sem-cor (Miller et al, 1993), supervised systems barelyimprove upon the simple most frequent heuris-tic.
In this setting the unsupervised evaluationschemes are not feasible, as many of the targetwords occur only once, so we used the map-ping strategy with Semcor to produce the requiredWordNet senses in the output.Table 4 shows the results for our systems withthe best parameters according to the supervisedcriterion on S2LS, plus the top three S3AW super-vised systems and the most frequent sense heuris-tic.
In order to focus the comparison, we only keptnoun occurrences of all systems and filtered outmultiwords, target words with two different lem-mas and unknown tags, leaving a total of 857 oc-currences of nouns.
We can see that Pr fr is only0.2 from the S3AW winning system, demonstrat-ing that our unsupervised graph-based systemsthat use Semcor for mapping are nearly equivalentto the most powerful supervised systems to date.In fact, the differences in performance for the sys-tems are not statistically significant (McNemar?stest at 95% significance level).5916 Conclusions and further workThis paper has explored the use of two graph algo-rithms for corpus-based disambiguation of nomi-nal senses.
We have shown that the parameter op-timization learnt over a small set of nouns signifi-cantly improves the performance for all nouns, andproduces a system which (1) in a lexical-samplesetting (Senseval 3 dataset) is 10 points over theMost-Frequent-Sense baseline, 1 point over a su-pervised system using the same kind of informa-tion (i.e.
bag-of-words features), and 8 points be-low the best supervised system, and (2) in the all-words setting is a` la par the best supervised sys-tem.
The performance of PageRank is statisticallythe same as that of HyperLex, with the advantageof PageRank of using less parameters.In order to compete on the same test set as su-pervised systems, we do use hand-tagged data, butonly to do the mapping from the induced sensesinto the gold standard senses.
In fact, we believethat using our WSD system as a purely unsuper-vised system (i.e.
returning just hubs), the per-fomance would be higher, as we would avoid theinformation loss in the mapping.
We would liketo test this on Information Retrieval, perhaps on asetting similar to that of (Schu?tze, 1998), whichwould allow for an indirect evaluation of the qual-ity and a comparison with supervised WSD systemon the same grounds.We have also shown that the optimization ac-cording to purity and entropy values (which doesnot need the supervised mapping step) yields verygood parameters, comparable to those obtained inthe supervised optimization strategy.
This indi-cates that we are able to optimize the algorithmin a completely unsupervised fashion for a smallnumber of words, and then carry over to tag newtext with the induced senses.Regarding efficiency, our implementation ofHyperLex is extremely fast.
Trying the 6700 com-binations of parameters takes 5 hours in a 2 AMDOpteron processors at 2GHz and 3Gb RAM.
Asingle run (building the MST, mapping and tag-ging the test sentences) takes only 16 sec.
For thisreason, even if an on-line version would be in prin-ciple desirable, we think that this batch version isreadily usable as a standalone word sense disam-biguation system.Both graph-based methods and vector-basedclustering methods rely on local information, typ-ically obtained by the occurrences of neighborwords in context.
The advantage of graph-based techniques over over vector-based cluster-ing might come from the fact that the former areable to measure the relative importance of a vertexin the whole graph, and thus combine both localand global cooccurrence information.For the future, we would like to look moreclosely the micro-senses induced by HyperLex,and see if we can group them into coarser clus-ters.
We would also like to integrate differentkinds of information, specially the local or syn-tactic features so successfully used by supervisedsystems, but also more heterogeneous informationfrom knowledge bases.Graph models have been very successful insome settings (e.g.
the PageRank algorithm ofGoogle), and have been rediscovered recentlyfor natural language tasks like knowledge-basedWSD, textual entailment, summarization and de-pendency parsing.
Now that we have set a ro-bust optimization and evaluation framework wewould like to test other such algorithms (e.g.HITS (Kleinberg, 1999)) in the same conditions.AcknowledgementsOier Lopez de Lacalle enjoys a PhD grant from theBasque Government.
We thank the comments ofthe three anonymous reviewers.ReferencesE.
Agirre, O. Lopez de Lacalle, and D. Martinez.
2005.Exploring feature spaces with svd and unlabeleddata for word sense disambiguation.
In Proc.
ofRANLP.E.
Agirre, O. Lopez de Lacalle, D. Martinez, andA.
Soroa.
2006.
Evaluating and optimizing the pa-rameters of an unsupervised graph-based wsd algo-rithm.
In Proc.
of the NAACL Texgraphs workshop.S.
Brin and L. Page.
1998.
The anatomy of a large-scale hypertextual web search engine.
ComputerNetworks and ISDN Systems, 30(1-7).D.
A. Cruse, 2000.
Polysemy: Theoretical and Compu-tational Approaches, chapter Aspects of the Micro-structure of Word Meanings, pages 31?51.
OUP.G Erkan and D. R. Radev.
2004.
Lexrank: Graph-based centrality as salience in text summarization.Journal of Artificial Intelligence Research (JAIR).C.
Fellbaum.
1998.
WordNet: An Electronic LexicalDatabase.
MIT Press.592Jon M. Kleinberg.
1999.
Authoritative sources ina hyperlinked environment.
Journal of the ACM,46(5):604?632.R.
Mihalcea and P Tarau.
2004.
Textrank: Bringingorder into texts.
In Proc.
of EMNLP2004.R.
Mihalcea, T. Chklovski, and A. Kilgarriff.
2004.The senseval-3 english lexical sample task.
InR.
Mihalcea and P. Edmonds, editors, Senseval-3proccedings, pages 25?28.
ACL, July.R.
Mihalcea.
2005.
Unsupervised large-vocabularyword sense disambiguation with graph-based algo-rithms for sequence data labeling.
In Proc.
ofEMNLP2005.G.A.
Miller, C. Leacock, R. Tengi, and R.Bunker.1993.
A semantic concordance.
In Proc.
of theARPA HLT workshop.R.
Navigli and P. Velardi.
2005.
Structural seman-tic interconnections: a knowledge-based approachto word sense disambiguation.
IEEE Transac-tions on Pattern Analysis and Machine Intelligence,7(27):1063?1074, June.C.
Niu, W. Li, R. K. Srihari, and H. Li.
2005.
Word in-dependent context pair classification model for wordsense disambiguation.
In Proc.
of CoNLL-2005.P.
Pantel and D. Lin.
2002.
Discovering word sensesfrom text.
In Proc.
of KDD02.A.
Purandare and T. Pedersen.
2004.
Word sense dis-crimination by clustering contexts in vector and sim-ilarity spaces.
In Proc.
of CoNLL-2004, pages 41?48.H.
Schu?tze.
1998.
Automatic word sense discrimina-tion.
Computational Linguistics, 24(1):97?123.B.
Snyder and M. Palmer.
2004.
The english all-wordstask.
In Proc.
of SENSEVAL.J.
Ve?ronis.
2004.
Hyperlex: lexical cartography for in-formation retrieval.
Computer Speech & Language,18(3):223?252.Y Zhao and G Karypis.
2005.
Hierarchical clusteringalgorithms for document datasets.
Data Mining andKnowledge Discovery, 10(2):141?168.593
