Proceedings of the COLING/ACL 2006 Main Conference Poster Sessions, pages 969?976,Sydney, July 2006. c?2006 Association for Computational LinguisticsBiTAM: Bilingual Topic AdMixture Models for Word AlignmentBing Zhao?
and Eric P.
Xing??
{bzhao,epxing}@cs.cmu.eduLanguage Technologies Institute?
and Machine Learning Department?School of Computer Science, Carnegie Mellon UniversityAbstractWe propose a novel bilingual topical ad-mixture (BiTAM) formalism for wordalignment in statistical machine transla-tion.
Under this formalism, the paral-lel sentence-pairs within a document-pairare assumed to constitute a mixture ofhidden topics; each word-pair follows atopic-specific bilingual translation model.Three BiTAM models are proposed to cap-ture topic sharing at different levels of lin-guistic granularity (i.e., at the sentence orword levels).
These models enable word-alignment process to leverage topical con-tents of document-pairs.
Efficient vari-ational approximation algorithms are de-signed for inference and parameter esti-mation.
With the inferred latent topics,BiTAM models facilitate coherent pairingof bilingual linguistic entities that sharecommon topical aspects.
Our preliminaryexperiments show that the proposed mod-els improve word alignment accuracy, andlead to better translation quality.1 IntroductionParallel data has been treated as sets of unre-lated sentence-pairs in state-of-the-art statisticalmachine translation (SMT) models.
Most currentapproaches emphasize within-sentence dependen-cies such as the distortion in (Brown et al, 1993),the dependency of alignment in HMM (Vogel etal., 1996), and syntax mappings in (Yamada andKnight, 2001).
Beyond the sentence-level, corpus-level word-correlation and contextual-level topicalinformation may help to disambiguate translationcandidates and word-alignment choices.
For ex-ample, the most frequent source words (e.g., func-tional words) are likely to be translated into wordswhich are also frequent on the target side; words ofthe same topic generally bear correlations and sim-ilar translations.
Extended contextual informationis especially useful when translation models arevague due to their reliance solely on word-pair co-occurrence statistics.
For example, the word shotin ?It was a nice shot.?
should be translated dif-ferently depending on the context of the sentence:a goal in the context of sports, or a photo withinthe context of sightseeing.
Nida (1964) statedthat sentence-pairs are tied by the logic-flow in adocument-pair; in other words, the document-pairshould be word-aligned as one entity instead of be-ing uncorrelated instances.
In this paper, we pro-pose a probabilistic admixture model to capturelatent topics underlying the context of document-pairs.
With such topical information, the trans-lation models are expected to be sharper and theword-alignment process less ambiguous.Previous works on topical translation modelsconcern mainly explicit logical representations ofsemantics for machine translation.
This includeknowledge-based (Nyberg and Mitamura, 1992)and interlingua-based (Dorr and Habash, 2002)approaches.
These approaches can be expen-sive, and they do not emphasize stochastic trans-lation aspects.
Recent investigations along thisline includes using word-disambiguation schemes(Carpua and Wu, 2005) and non-overlapping bilin-gual word-clusters (Wang et al, 1996; Och, 1999;Zhao et al, 2005) with particular translation mod-els, which showed various degrees of success.
Wepropose a new statistical formalism: BilingualTopic AdMixture model, or BiTAM, to facilitatetopic-based word alignment in SMT.Variants of admixture models have appeared inpopulation genetics (Pritchard et al, 2000) andtext modeling (Blei et al, 2003).
Statistically, anobject is said to be derived from an admixture if itconsists of a bag of elements, each sampled inde-pendently or coupled in some way, from a mixturemodel.
In a typical SMT setting, each document-pair corresponds to an object; depending on achosen modeling granularity, all sentence-pairs orword-pairs in the document-pair correspond to theelements constituting the object.
Correspondingly,a latent topic is sampled for each pair from a priortopic distribution to induce topic-specific transla-tions; and the resulting sentence-pairs and word-pairs are marginally dependent.
Generatively, thisadmixture formalism enables word translations tobe instantiated by topic-specific bilingual models969and/or monolingual models, depending on theircontexts.
In this paper we investigate three in-stances of the BiTAM model, They are data-drivenand do not need hand-crafted knowledge engineer-ing.The remainder of the paper is as follows: in sec-tion 2, we introduce notations and baselines; insection 3, we propose the topic admixture models;in section 4, we present the learning and inferencealgorithms; and in section 5 we show experimentsof our models.
We conclude with a brief discus-sion in section 6.2 Notations and BaselineIn statistical machine translation, one typicallyuses parallel data to identify entities such as?word-pair?, ?sentence-pair?, and ?document-pair?.
Formally, we define the following terms1:?
A word-pair (fj , ei) is the basic unit for wordalignment, where fj is a French word and eiis an English word; j and i are the positionindices in the corresponding French sentencef and English sentence e.?
A sentence-pair (f , e) contains the sourcesentence f of a sentence length of J ; a targetsentence e of length I .
The two sentences fand e are translations of each other.?
A document-pair (F,E) refers to two doc-uments which are translations of each other.Assuming sentences are one-to-one corre-spondent, a document-pair has a sequence ofN parallel sentence-pairs {(fn, en)}, where(fn, en) is the n?th parallel sentence-pair.?
A parallel corpus C is a collection of M par-allel document-pairs: {(Fd,Ed)}.2.1 Baseline: IBM Model-1The translation process can be viewed as opera-tions of word substitutions, permutations, and in-sertions/deletions (Brown et al, 1993) in noisy-channel modeling scheme at parallel sentence-pairlevel.
The translation lexicon p(f |e) is the keycomponent in this generative process.
An efficientway to learn p(f |e) is IBM-1:p(f |e) =J?j=1I?i=1p(fj |ei) ?
p(ei|e).
(1)1We follow the notations in (Brown et al, 1993) forEnglish-French, i.e., e ?
f , although our models are tested,in this paper, for English-Chinese.
We use the end-user ter-minology for source and target languages.IBM-1 has global optimum; it is efficient and eas-ily scalable to large training data; it is one of themost informative components for re-ranking trans-lations (Och et al, 2004).
We start from IBM-1 asour baseline model, while higher-order alignmentmodels can be embedded similarly within the pro-posed framework.3 Bilingual Topic AdMixture ModelNow we describe the BiTAM formalism thatcaptures the latent topical structure and gener-alizes word alignments and translations beyondsentence-level via topic sharing across sentence-pairs:E?=argmax{E}p(F|E)p(E), (2)where p(F|E) is a document-level translationmodel, generating the document F as one entity.In a BiTAM model, a document-pair (F,E) istreated as an admixture of topics, which is inducedby random draws of a topic, from a pool of topics,for each sentence-pair.
A unique normalized andreal-valued vector ?, referred to as a topic-weightvector, which captures contributions of differenttopics, are instantiated for each document-pair, sothat the sentence-pairs with their alignments aregenerated from topics mixed according to thesecommon proportions.
Marginally, a sentence-pair is word-aligned according to a unique bilin-gual model governed by the hidden topical assign-ments.
Therefore, the sentence-level translationsare coupled, rather than being independent as as-sumed in the IBM models and their extensions.Because of this coupling of sentence-pairs (viatopic sharing across sentence-pairs according toa common topic-weight vector), BiTAM is likelyto improve the coherency of translations by treat-ing the document as a whole entity, instead of un-correlated segments that have to be independentlyaligned and then assembled.
There are at leasttwo levels at which the hidden topics can be sam-pled for a document-pair, namely: the sentence-pair and the word-pair levels.
We propose threevariants of the BiTAM model to capture the latenttopics of bilingual documents at different levels.3.1 BiTAM-1: The FrameworksIn the first BiTAM model, we assume that topicsare sampled at the sentence-level.
Each document-pair is represented as a random mixture of la-tent topics.
Each topic, topic-k, is presented by atopic-specific word-translation table: Bk, which is970faJINMeB?
z?
?J BIfea?
?
zMNaJINMeB?
z?
f(a) (b) (c)Figure 1: BiTAM models for Bilingual document- and sentence-pairs.
A node in the graph represents a random variable, anda hexagon denotes a parameter.
Un-shaded nodes are hidden variables.
All the plates represent replicates.
The outmost plate(M -plate) represents M bilingual document-pairs, while the inner N -plate represents the N repeated choice of topics for eachsentence-pairs in the document; the inner J-plate represents J word-pairs within each sentence-pair.
(a) BiTAM-1 samplesone topic (denoted by z) per sentence-pair; (b) BiTAM-2 utilizes the sentence-level topics for both the translation model (i.e.,p(f |e, z)) and the monolingual word distribution (i.e., p(e|z)); (c) BiTAM-3 samples one topic per word-pair.a translation lexicon: Bi,j,k=p(f=fj |e=ei, z=k),where z is an indicator variable to denote thechoice of a topic.
Given a specific topic-weightvector ?d for a document-pair, each sentence-pairdraws its conditionally independent topics from amixture of topics.
This generative process, for adocument-pair (Fd,Ed), is summarized as below:1.
Sample sentence-number N from a Poisson(?).2.
Sample topic-weight vector ?d from a Dirichlet(?).3.
For each sentence-pair (fn, en) in the d?th doc-pair ,(a) Sample sentence-length Jn from Poisson(?
);(b) Sample a topic zdn from a Multinomial(?d);(c) Sample ej from a monolingual model p(ej);(d) Sample each word alignment link aj from a uni-form model p(aj) (or an HMM);(e) Sample each fj according to a topic-specifictranslation lexicon p(fj |e, aj , zn,B).We assume that, in our model, there are K pos-sible topics that a document-pair can bear.
Foreach document-pair, a K-dimensional Dirichletrandom variable ?d, referred to as the topic-weightvector of the document, can take values in the(K?1)-simplex following a probability density:p(?|?)
= ?
(?Kk=1 ?k)?Kk=1 ?(?k)?
?1?11 ?
?
?
?
?K?1K , (3)where the hyperparameter ?
is a K-dimensionvector with each component ?k>0, and ?
(x)is the Gamma function.
The alignment isrepresented by a J-dimension vector a ={a1, a2, ?
?
?
, aJ}; for each French word fj at theposition j, an position variable aj maps it to anEnglish word eaj at the position aj in English sen-tence.
The word level translation lexicon probabil-ities are topic-specific, and they are parameterizedby the matrix B = {Bk}.For simplicity, in our current models we omitthe modelings of the sentence-number N and thesentence-length Jn, and focus only on the bilin-gual translation model.
Figure 1 (a) shows thegraphical model representation for the BiTAMgenerative scheme discussed so far.
Note that, thesentence-pairs are now connected by the node ?d.Therefore, marginally, the sentence-pairs are notindependent of each other as in traditional SMTmodels, instead they are conditionally indepen-dent given the topic-weight vector ?d.
Specifi-cally, BiTAM-1 assumes that each sentence-pairhas one single topic.
Thus, the word-pairs withinthis sentence-pair are conditionally independent ofeach other given the hidden topic index z of thesentence-pair.The last two sub-steps (3.d and 3.e) in theBiTam sampling scheme define a translationmodel, in which an alignment link aj is proposedand an observation of fj is generated accordingto the proposed distributions.
We simplify align-ment model of a, as in IBM-1, by assuming thataj is sampled uniformly at random.
Given the pa-rameters ?, B, and the English part E, the jointconditional distribution of the topic-weight vector?, the topic indicators z, the alignment vectors A,and the document F can be written as:p(F,A, ?, z|E, ?,B)=p(?
|?)N?n=1p(zn|?
)p(fn,an|en, ?, Bzn),(4)where N is the number of the sentence-pair.Marginalizing out ?
and z, we can obtain themarginal conditional probability of generating Ffrom E for each document-pair:p(F,A|E, ?,Bzn) =?p(?|?
)( N?n=1?znp(zn|?
)p(fn,an|en, Bzn))d?, (5)where p(fn,an|en, Bzn) is a topic-specificsentence-level translation model.
For simplicity,we assume that the French words fj?s are condi-tionally independent of each other; the alignment971variables aj?s are independent of other variablesand are uniformly distributed a priori.
Therefore,the distribution for each sentence-pair is:p(fn,an|en, Bzn) = p(fn|en,an, Bzn)p(an|en, Bzn)= 1IJnnJn?j=1p(fnj |eanj , Bzn).
(6)Thus, the conditional likelihood for the entireparallel corpus is given by taking the productof the marginal probabilities of each individualdocument-pair in Eqn.
5.3.2 BiTAM-2: Monolingual AdmixtureIn general, the monolingual model for Englishcan also be a rich topic-mixture.
This is real-ized by using the same topic-weight vector ?d andthe same topic indicator zdn sampled accordingto ?d, as described in ?3.1, to introduce not onlytopic-dependent translation lexicon, but also topic-dependent monolingual model of the source lan-guage, English in this case, for generating eachsentence-pair (Figure 1 (b)).
Now e is generatedfrom a topic-based language model ?, instead of auniform distribution in BiTAM-1.
We refer to thismodel as BiTAM-2.Unlike BiTAM-1, where the information ob-served in ei is indirectly passed to z via the nodeof fj and the hidden variable aj , in BiTAM-2, thetopics of corresponding English and French sen-tences are also strictly aligned so that the informa-tion observed in ei can be directly passed to z, inthe hope of finding more accurate topics.
The top-ics are inferred more directly from the observedbilingual data, and as a result, improve alignment.3.3 BiTAM-3: Word-level AdmixtureIt is straightforward to extend the sentence-levelBiTAM-1 to a word-level admixture model, bysampling topic indicator zn,j for each word-pair(fj , eaj ) in the n?th sentence-pair, rather thanonce for all (words) in the sentence (Figure 1 (c)).This gives rise to our BiTAM-3.
The conditionallikelihood functions can be obtained by extendingthe formulas in ?3.1 to move the variable zn,j in-side the same loop over each of the fn,j .3.4 Incorporation of Word ?Null?Similar to IBM models, ?Null?
word is used forthe source words which have no translation coun-terparts in the target language.
For example, Chi-nese words ?de?
() , ?ba?
(r) and ?bei?
() generally do not have translations in English.?Null?
is attached to every target sentence to alignthe source words which miss their translations.Specifically, the latent Dirichlet alocation (LDA)in (Blei et al, 2003) can be viewed as a specialcase of the BiTAM-3, in which the target sentencecontains only one word: ?Null?, and the alignmentlink a is no longer a hidden variable.4 Learning and InferenceDue to the hybrid nature of the BiTAM models,exact posterior inference of the hidden variablesA, z and ?
is intractable.
A variational inferenceis used to approximate the true posteriors of thesehidden variables.
The inference scheme is pre-sented for BiTAM-1; the algorithms for BiTAM-2and BiTAM-3 are straight forward extensions andare omitted.4.1 Variational ApproximationTo approximate: p(?, z,A|E,F, ?,B), the jointposterior, we use the fully factorized distributionover the same set of hidden variables:q(?,z,A) ?
q(?|?, ?
)?N?n=1q(zn|?n)Jn?j=1q(anj , fnj |?nj , en,B),(7)where the Dirichlet parameter ?, the multino-mial parameters (?1, ?
?
?
, ?n), and the parameters(?n1, ?
?
?
, ?nJn) are known as variational param-eters, and can be optimized with respect to theKullback-Leibler divergence from q(?)
to the orig-inal p(?)
via an iterative fixed-point algorithm.
Itcan be shown that the fixed-point equations for thevariational parameters in BiTAM-1 are as follows:?k = ?k +Nd?n=1?dnk (8)?dnk ?
exp(?(?k)??(K?k?=1?k?
))?exp( Jdn?j=1Idn?i=1?dnji logBfj ,ei,k)(9)?dnji ?
exp( K?k=1?dnk logBfj ,ei,k), (10)where ?(?)
is a digamma function.
Note that inthe above formulas ?dnk is the variational param-eter underlying the topic indicator zdn of the n-thsentence-pair in document d, and it can be used topredict the topic distribution of that sentence-pair.Following a variational EM scheme (Beal andGhahramani, 2002), we estimate the model pa-rameters ?
and B in an unsupervised fashion.
Es-sentially, Eqs.
(8-10) above constitute the E-step,972where the posterior estimations of the latent vari-ables are obtained.
In the M-step, we update ?and B so that they improve a lower bound of thelog-likelihood defined bellow:L(?, ?, ?
;?,B)=Eq[log p(?|?
)]+Eq[log p(z|?
)]+Eq[log p(a)]+Eq[log p(f |z,a,B)]?Eq[log q(?
)]?Eq[log q(z)]?Eq[log q(a)].
(11)The close-form iterative updating formula B is:Bf,e,k ?M?dNd?n=1Jdn?j=1Idn?i=1?
(f, fj)?
(e, ei)?dnk?dnji (12)For ?, close-form update is not available, and weresort to gradient accent as in (Sjo?lander et al,1996) with re-starts to ensure each updated ?k>0.4.2 Data Sparseness and SmoothingThe translation lexicons Bf,e,k have a potentialsize of V 2K, assuming the vocabulary sizes forboth languages are V .
The data sparsity (i.e.,lack of large volume of document-pairs) poses amore serious problem in estimating Bf,e,k thanthe monolingual case, for instance, in (Blei etal., 2003).
To reduce the data sparsity problem,we introduce two remedies in our models.
First:Laplace smoothing.
In this approach, the matrixset B, whose columns correspond to parametersof conditional multinomial distributions, is treatedas a collection of random vectors all under a sym-metric Dirichlet prior; the posterior expectation ofthese multinomial parameter vectors can be esti-mated using Bayesian theory.
Second: interpola-tion smoothing.
Empirically, we can employ a lin-ear interpolation with IBM-1 to avoid overfitting:B?f,e,k = ?Bf,e,k+(1??
)p(f |e).
(13)As in Eqn.
1, p(f |e) is learned via IBM-1; ?
isestimated via EM on held out data.4.3 Retrieving Word AlignmentsTwo word-alignment retrieval schemes are de-signed for BiTAMs: the uni-direction alignment(UDA) and the bi-direction alignment (BDA).
Bothuse the posterior mean of the alignment indica-tors adnji, captured by what we call the poste-rior alignment matrix ?
?
{?dnji}.
UDA usesa French word fdnj (at the j?th position of n?thsentence in the d?th document) to query ?
to getthe best aligned English word (by taking the max-imum point in a row of ?
):adnj = argmaxi?[1,Idn]?dnji.
(14)BDA selects iteratively, for each f , the bestaligned e, such that the word-pair (f, e) is themaximum of both row and column, or its neigh-bors have more aligned pairs than the othercombpeting candidates.A close check of {?dnji} in Eqn.
10 re-veals that it is essentially an exponential model:weighted log probabilities from individual topic-specific translation lexicons; or it can be viewedas weighted geometric mean of the individual lex-icon?s strength.5 ExperimentsWe evaluate BiTAM models on the word align-ment accuracy and the translation quality.
Forword alignment accuracy, F-measure is reported,i.e., the harmonic mean of precision and recallagainst a gold-standard reference set; for transla-tion quality, Bleu (Papineni et al, 2002) and itsvariation of NIST scores are reported.Table 1: Training and Test Data StatisticsTrain #Doc.
#Sent.
#TokensEnglish ChineseTreebank 316 4172 133K 105KFBIS.BJ 6,111 105K 4.18M 3.54MSinorama 2,373 103K 3.81M 3.60MXinHua 19,140 115K 3.85M 3.93MTest 95 627 25,500 19,726We have two training data settings with dif-ferent sizes (see Table 1).
The small oneconsists of 316 document-pairs from Tree-bank (LDC2002E17).
For the large trainingdata setting, we collected additional document-pairs from FBIS (LDC2003E14, Beijing part),Sinorama (LDC2002E58), and Xinhua News(LDC2002E18, document boundaries are kept inour sentence-aligner (Zhao and Vogel, 2002)).There are 27,940 document-pairs, containing327K sentence-pairs or 12 million (12M) Englishtokens and 11M Chinese tokens.
To evaluate wordalignment, we hand-labeled 627 sentence-pairsfrom 95 document-pairs sampled from TIDES?01dryrun data.
It contains 14,769 alignment-links.To evaluate translation quality, TIDES?02 Eval.test is used as development set, and TIDES?03Eval.
test is used as the unseen test data.5.1 Model SettingsFirst, we explore the effects of Null word andsmoothing strategies.
Empirically, we find thatadding ?Null?
word is always beneficial to allmodels regardless of number of topics selected.973Topics-Lexicons Topic-1 Topic-2 Topic-3 Cooc.
IBM-1 HMM IBM-4p(ChaoXian (?m)|Korean) 0.0612 0.2138 0.2254 38 0.2198 0.2157 0.2104p(HanGuo (?I)|Korean) 0.8379 0.6116 0.0243 46 0.5619 0.4723 0.4993Table 2: Topic-specific translation lexicons are learned by a 3-topic BiTAM-1.
The third lexicon (Topic-3) prefers to translatethe word Korean into ChaoXian (?m:North Korean).
The co-occurrence (Cooc), IBM-1&4 and HMM only prefer to translateinto HanGuo (?I:South Korean).
The two candidate translations may both fade out in the learned translation lexicons.Unigram-rank 1 2 3 4 5 6 7 8 9 10Topic A. foreign china u.s. development trade enterprises technology countries year economicTopic B. chongqing companies takeovers company city billion more economic reached yuanTopic C. sports disabled team people cause water national games handicapped membersTable 3: Three most distinctive topics are displayed.
The English words for each topic are ranked according to p(e|z)estimated from the topic-specific English sentences weighted by {?dnk}.
33 functional words were removed to highlight themain content of each topic.
Topic A is about Us-China economic relationships; Topic B relates to Chinese companies?
merging;Topic C shows the sports of handicapped people.The interpolation smoothing in ?4.2 is effec-tive, and it gives slightly better performance thanLaplace smoothing over different number of topicsfor BiTAM-1.
However, the interpolation lever-ages the competing baseline lexicon, and this canblur the evaluations of BiTAM?s contributions.Laplace smoothing is chosen to emphasize moreon BiTAM?s strength.
Without any smoothing, F-measure drops very quickly over two topics.
In allour following experiments, we use both Null wordand Laplace smoothing for the BiTAM models.We train, for comparison, IBM-1&4 and HMMmodels with 8 iterations of IBM-1, 7 for HMMand 3 for IBM-4 (18h743) with Null word and amaximum fertility of 3 for Chinese-English.Choosing the number of topics is a model se-lection problem.
We performed a ten-fold cross-validation, and a setting of three-topic is cho-sen for both the small and the large training datasets.
The overall computation complexity of theBiTAM is linear to the number of hidden topics.5.2 Variational InferenceUnder a non-symmetric Dirichlet prior, hyperpa-rameter ?
is initialized randomly; B (K transla-tion lexicons) are initialized uniformly as did inIBM-1.
Better initialization of B can help to avoidlocal optimal as shown in ?
5.5.With the learned B and ?
fixed, the variationalparameters to be computed in Eqn.
(8-10) are ini-tialized randomly; the fixed-point iterative updatesstop when the change of the likelihood is smallerthan 10?5.
The convergent variational parameters,corresponding to the highest likelihood from 20random restarts, are used for retrieving the wordalignment for unseen document-pairs.
To estimateB, ?
(for BiTAM-2) and ?, at most eight varia-tional EM iterations are run on the training data.Figure 2 shows absolute 2?3% better F-measureover iterations of variational EM using two andthree topics of BiTAM-1 comparing with IBM-1.3 3.5 4 4.5 5 5.5 6 6.5 7 7.5 832333435363738394041Number of EM/Variational EM Iterations for IBM?1 and BiTam?1F?measure(%)BiTam with Null and Laplace Smoothing Over Var.
EM IterationsBiTam?1, Topic #=3BiTam?1, Topic #=2IBM?1Figure 2: performances over eight Variational EM itera-tions of BiTAM-1 using both the ?Null?
word and the laplacesmoothing; IBM-1 is shown over eight EM iterations forcomparison.5.3 Topic-Specific Translation LexiconsThe topic-specific lexicons Bk are smaller in sizethan IBM-1, and, typically, they contain topictrends.
For example, in our training data, NorthKorean is usually related to politics and translatedinto ?ChaoXian?
(?m); South Korean occursmore often with economics and is translated as?HanGuo?(?I).
BiTAMs discriminate the twoby considering the topics of the context.
Table 2shows the lexicon entries for ?Korean?
learnedby a 3-topic BiTAM-1.
The values are relativelysharper, and each clearly favors one of the candi-dates.
The co-occurrence count, however, only fa-vors ?HanGuo?, and this can easily dominate thedecisions of IBM and HMM models due to theirignorance of the topical context.
Monolingualtopics learned by BiTAMs are, roughly speak-ing, fuzzy especially when the number of topics issmall.
With proper filtering, we find that BiTAMsdo capture some topics as illustrated in Table 3.5.4 Evaluating Word AlignmentsWe evaluate word alignment accuracies in vari-ous settings.
Notably, BiTAM allows to test align-ments in two directions: English-to-Chinese (EC)and Chinese-to-English (CE).
Additional heuris-tics are applied to further improve the accura-cies.
Inter takes the intersection of the two direc-tions and generates high-precision alignments; the974SETTING IBM-1 HMM IBM-4 BITAM-1 BITAM-2 BITAM-3UDA BDA UDA BDA UDA BDACE (%) 36.27 43.00 45.00 40.13 48.26 40.26 48.63 40.47 49.02EC (%) 32.94 44.26 45.96 36.52 46.61 37.35 46.30 37.54 46.62REFINED (%) 41.71 44.40 48.42 45.06 49.02 47.20 47.61 47.46 48.18UNION (%) 32.18 42.94 43.75 35.87 48.66 36.07 48.99 36.26 49.35INTER (%) 39.86 44.87 48.65 43.65 43.85 44.91 45.18 45.13 45.48NIST 6.458 6.822 6.926 6.937 6.954 6.904 6.976 6.967 6.962BLEU 15.70 17.70 18.25 17.93 18.14 18.13 18.05 18.11 18.25Table 4: Word Alignment Accuracy (F-measure) and Machine Translation Quality for BiTAM Models, comparing with IBMModels, and HMMs with a training scheme of 18h743 on the Treebank data listed in Table 1.
For each column, the highlightedalignment (the best one under that model setting) is picked up to further evaluate the translation quality.Union of two directions gives high-recall; Refinedgrows the intersection with the neighboring word-pairs seen in the union, and yields high-precisionand high-recall alignments.As shown in Table 4, the baseline IBM-1 givesits best performance of 36.27% in the CE direc-tion; the UDA alignments from BiTAM-1?3 give40.13%, 40.26%, and 40.47%, respectively, whichare significantly better than IBM-1.
A close lookat the three BiTAMs does not yield significant dif-ference.
BiTAM-3 is slightly better in most set-tings; BiTAM-1 is slightly worse than the othertwo, because the topics sampled at the sentencelevel are not very concentrated.
The BDA align-ments of BiTAM-1?3 yield 48.26%, 48.63% and49.02%, which are even better than HMM andIBM-4 ?
their best performances are at 44.26%and 45.96%, respectively.
This is because BDApartially utilizes similar heuristics on the approx-imated posterior matrix {?dnji} instead of di-rect operations on alignments of two directionsin the heuristics of Refined.
Practically, we alsoapply BDA together with heuristics for IBM-1,HMM and IBM-4, and the best achieved perfor-mances are at 40.56%, 46.52% and 49.18%, re-spectively.
Overall, BiTAM models achieve per-formances close to or higher than HMM, usingonly a very simple IBM-1 style alignment model.Similar improvements over IBM models andHMM are preserved after applying the three kindsof heuristics in the above.
As expected, since BDAalready encodes some heuristics, it is only slightlyimproved with the Union heuristic; UDA, similarto the viterbi style alignment in IBM and HMM, isimproved better by the Refined heuristic.We also test BiTAM-3 on large training data,and similar improvements are observed over thoseof the baseline models (see Table.
5).5.5 Boosting BiTAM ModelsThe translation lexicons of Bf,e,k are initializeduniformly in our previous experiments.
Better ini-tializations can potentially lead to better perfor-mances because it can help to avoid the unde-sirable local optima in variational EM iterations.We use the lexicons from IBM Model-4 to initial-ize Bf,e,k to boost the BiTAM models.
This isone way of applying the proposed BiTAM mod-els into current state-of-the-art SMT systems forfurther improvement.
The boosted alignments aredenoted as BUDA and BBDA in Table.
5, cor-responding to the uni-direction and bi-directionalignments, respectively.
We see an improvementin alignment quality.5.6 Evaluating TranslationsTo further evaluate our BiTAM models, wordalignments are used in a phrase-based decoderfor evaluating translation qualities.
Similar tothe Pharoah package (Koehn, 2004), we extractphrase-pairs directly from word alignment to-gether with coherence constraints (Fox, 2002) toremove noisy ones.
We use TIDES Eval?02 CEtest set as development data to tune the decoderparameters; the Eval?03 data (919 sentences) is theunseen data.
A trigram language model is builtusing 180 million English words.
Across all thereported comparative settings, the key differenceis the bilingual ngram-identity of the phrase-pair,which is collected directly from the underlyingword alignment.Shown in Table 4 are results for the small-data track; the large-data track results are in Ta-ble 5.
For the small-data track, the baseline Bleuscores for IBM-1, HMM and IBM-4 are 15.70,17.70 and 18.25, respectively.
The UDA align-ment of BiTAM-1 gives an improvement overthe baseline IBM-1 from 15.70 to 17.93, andit is close to HMM?s performance, even thoughBiTAM doesn?t exploit any sequential structuresof words.
The proposed BiTAM-2 and BiTAM-3 are slightly better than BiTAM-1.
Similar im-provements are observed for the large-data track(see Table 5).
Note that, the boosted BiTAM-3 us-975SETTING IBM-1 HMM IBM-4 BITAM-3UDA BDA BUDA BBDACE (%) 46.73 49.12 54.17 50.55 56.27 55.80 57.02EC (%) 44.33 54.56 55.08 51.59 55.18 54.76 58.76REFINED (%) 54.64 56.39 58.47 56.45 54.57 58.26 56.23UNION (%) 42.47 51.59 52.67 50.23 57.81 56.19 58.66INTER (%) 52.24 54.69 57.74 52.44 52.71 54.70 55.35NIST 7.59 7.77 7.83 7.64 7.68 8.10 8.23BLEU 19.19 21.99 23.18 21.20 21.43 22.97 24.07Table 5: Evaluating Word Alignment Accuracies and Machine Translation Qualities for BiTAM Models, IBM Models,HMMs, and boosted BiTAMs using all the training data listed in Table.
1.
Other experimental conditions are similar to Table.
4.ing IBM-4 as the seed lexicon, outperform the Re-fined IBM-4: from 23.18 to 24.07 on Bleu score,and from 7.83 to 8.23 on NIST.
This result sug-gests a straightforward way to leverage BiTAMsto improve statistical machine translations.6 ConclusionIn this paper, we proposed novel formalism forstatistical word alignment based on bilingual ad-mixture (BiTAM) models.
Three BiTAM mod-els were proposed and evaluated on word align-ment and translation qualities against state-of-the-art translation models.
The proposed mod-els significantly improve the alignment accuracyand lead to better translation qualities.
Incorpo-ration of within-sentence dependencies such asthe alignment-jumps and distortions, and a bettertreatment of the source monolingual model worthfurther investigations.ReferencesM.
J. Beal and Zoubin Ghahramani.
2002.
The variationalbayesian em algorithm for incomplete data: with appli-cation to scoring graphical model structures.
In BayesianStatistics 7.David Blei, Andrew NG, and M.I.
Jordan.
2003.
Latentdirichlet alocation.
In Journal of Machine LearningResearch, volume 3, pages 1107?1135.P.F.
Brown, Stephen A. Della Pietra, Vincent.
J. Della Pietra,and Robert L. Mercer.
1993.
The mathematics ofstatistical machine translation: Parameter estimation.
InComputational Linguistics, volume 19(2), pages 263?331.Marine Carpua and Dekai Wu.
2005.
Evaluating the wordsense disambiguation performance of statistical machinetranslation.
In Second International Joint Conference onNatural Language Processing (IJCNLP-2005).Bonnie Dorr and Nizar Habash.
2002.
Interlingua approxi-mation: A generation-heavy approach.
In In Proceedingsof Workshop on Interlingua Reliability, Fifth Conferenceof the Association for Machine Translation in the Ameri-cas, AMTA-2002, Tiburon, CA.Heidi J.
Fox.
2002.
Phrasal cohesion and statistical machinetranslation.
In Proc.
of the Conference on EmpiricalMethods in Natural Language Processing, pages 304?311, Philadelphia, PA, July 6-7.Philipp Koehn.
2004.
Pharaoh: a beam search decoder forphrase-based smt.
In Proceedings of the Conference ofthe Association for Machine Translation in the Americans(AMTA).Eugene A. Nida.
1964.
Toward a Science of Translating:With Special Reference to Principles Involved in BibleTranslating.
Leiden, Netherlands: E.J.
Brill.Eric Nyberg and Truko Mitamura.
1992.
The kant system:Fast, accurate, high-quality translation in practical do-mains.
In Proceedings of COLING-92.Franz J. Och, Daniel Gildea, Sanjeev Khudanpur, AnoopSarkar, Kenji Yamada, Alex Fraser, Shankar Kumar, LibinShen, David Smith, Katherine Eng, Viren Jain, Zhen Jin,and Dragomir Radev.
2004.
A smorgasbord of featuresfor statistical machine translation.
In HLT/NAACL:Human Language Technology Conference, volume 1:29,pages 161?168.Franz J. Och.
1999.
An efficient method for determiningbilingal word classes.
In Ninth Conf.
of the Europ.Chapter of the Association for Computational Linguistics(EACL?99), pages 71?76.Kishore Papineni, Salim Roukos, Todd Ward, and Wei-JingZhu.
2002.
Bleu: a method for automatic evaluation ofmachine translation.
In Proc.
of the 40th Annual Conf.
ofthe Association for Computational Linguistics (ACL 02),pages 311?318, Philadelphia, PA, July.J.
Pritchard, M. Stephens, and P. Donnell.
2000.
Inferenceof population structure using multilocus genotype data.In Genetics, volume 155, pages 945?959.K.
Sjo?lander, K. Karplus, M. Brown, R. Hughey, A. Krogh,I.S.
Mian, and D. Haussler.
1996.
Dirichlet mixtures: Amethod for improving detection of weak but significantprotein sequence homology.
Computer Applications inthe Biosciences, 12.S.
Vogel, Hermann Ney, and C. Tillmann.
1996.
Hmmbased word alignment in statistical machine translation.In Proc.
The 16th Int.
Conf.
on Computational Lingustics,(Coling?96), pages 836?841, Copenhagen, Denmark.Yeyi Wang, John Lafferty, and Alex Waibel.
1996.
Wordclustering with parallel spoken language corpora.
In pro-ceedings of the 4th International Conference on SpokenLanguage Processing (ICSLP?96), pages 2364?2367.K.
Yamada and Kevin.
Knight.
2001.
Syntax-based statisti-cal translation model.
In Proceedings of the Conferenceof the Association for Computational Linguistics (ACL-2001).Bing Zhao and Stephan Vogel.
2002.
Adaptive parallelsentences mining from web bilingual news collection.
InThe 2002 IEEE International Conference on Data Mining.Bing Zhao, Eric P. Xing, and Alex Waibel.
2005.
Bilingualword spectral clustering for statistical machine translation.In Proceedings of the ACL Workshop on Building andUsing Parallel Texts, pages 25?32, Ann Arbor, Michigan,June.
Association for Computational Linguistics.976
