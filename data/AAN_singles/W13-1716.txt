Proceedings of the Eighth Workshop on Innovative Use of NLP for Building Educational Applications, pages 124?133,Atlanta, Georgia, June 13 2013. c?2013 Association for Computational LinguisticsNLI Shared Task 2013: MQ SubmissionShervin Malmasi Sze-Meng Jojo Wong Mark DrasCentre for Language TechnologyMacquarie UniversitySydney, Australia{shervin.malmasi,sze.wong,mark.dras}@mq.edu.auAbstractOur submission for this NLI shared task usedfor the most part standard features found in re-cent work.
Our focus was instead on two otheraspects of our system: at a high level, on pos-sible ways of constructing ensembles of multi-ple classifiers; and at a low level, on the gran-ularity of part-of-speech tags used as features.We found that the choice of ensemble com-bination method did not lead to much differ-ence in results, although exploiting the vary-ing behaviours of linear versus logistic regres-sion SVM classifiers could be promising in fu-ture work; but part-of-speech tagsets showednoticeable differences.We also note that the overall architecture, withits feature set and ensemble approach, had anaccuracy of 83.1% on the test set when trainedon both the training data and development datasupplied, close to the best result of the task.This suggests that basically throwing togetherall the features of previous work will achieveroughly the state of the art.1 IntroductionAmong the efflorescence of work on Native Lan-guage Identification (NLI) noted by the shared taskorganisers, there are two trends in recent work inparticular that we considered in building our sub-mission.
The first is the proposal and use of newfeatures that might have relevance to NLI: for exam-ple, Wong and Dras (2011), motivated by the Con-trastive Analysis Hypothesis (Lado, 1957) from thefield of Second Language Acquisition, introducedsyntactic structure as a feature; Swanson and Char-niak (2012) introduced more complex Tree Substi-tution (TSG) structures, learned by Bayesian infer-ence; and Bykh and Meurers (2012) used recurringn-grams, inspired by the variation n-gram approachto corpus error annotation detection (Dickinson andMeurers, 2003).
Starting from the features intro-duced in these papers and others, then, other recentpapers have compiled a comprehensive collection offeatures based on the earlier work ?
Tetreault etal.
(2012) is an example, combining and analysingmost of the features used in previous work.
Giventhe timeframe of the shared task, there seemed to benot much mileage in trying new features that werelikely to be more peripheral to the task.A second trend, most apparent in 2012, was theexamination of other corpora besides the Interna-tional Corpus of Learner English used in earlierwork, and in particular the use of cross-corpus evalu-ation (Brooke and Hirst, 2012; Tetreault et al 2012)to avoid topic bias in determining native language.Possible topic bias had been a reason for avoidinga full range of n-grams, in particular those contain-ing content words (Koppel et al 2009); the devel-opment of new corpora and the analysis of the effectof topic bias mitigated this.
The consequent use of afull range of n-grams further reinforced the view thatnovel features were unlikely to be a major source ofinteresting results.We therefore concentrated on two areas: the useof classifier ensembles, and the choice of part-of-speech tags.
With classifier ensembles, Tetreaultet al(2012) noted that these were highly useful intheir system; but while that paper had extensive fea-124ture descriptions, it did not discuss in detail the ap-proach to its ensembles.
We therefore decided toexamine a range of possible ensemble architectures.With part-of-speech tags, most work has used thePenn Treebank tagset, including those based on syn-tactic structure.
Kochmar (2011) on the other handused the CLAWS tagset,1 which is much richer andmore oriented to linguistic analysis than the PennTreebank one.
Given the much larger size of theTOEFL11 corpus used for this shared task than thecorpora used for much earlier work, data sparsitycould be less of an issue, and the tagset a viable onefor future work.The description of our submission is therefore inthree parts.
In ?2 we present the system description,with a focus on the ensemble architectures we inves-tigated; in ?3 we list the features we used, which arebasically those of much of the previous work; in ?4we present results of some of the variants we tried,particularly with respect to ensembles and tagsets;and in ?5 we discuss some of the interesting charac-teristics of the data we noted during the shared task.2 System DesignOur overall approach in terms of features and clas-sifiers used is a fairly standard one.
One differencefrom most approaches, but inspired by Tetreault etal.
(2012), is that we train multiple classifiers oversubsets of the features, over different feature rep-resentations, and over different regularisation ap-proaches; we then combine them in ensembles (Di-etterich, 2000).2.1 SVM Ensemble ConstructionTo construct our ensemble, we train individual clas-sifiers on a single feature type (e.g.
PoS n-grams),using a specific feature value representation andclassifier.
We utilise a parallel ensemble structurewhere the classifiers are run on the input texts in-dependently and their results are then fused into thefinal output using a combiner.Additionally, we also experiment with bagging(bootstrap aggregating), a commonly used methodfor ensemble generation (Breiman, 1996) to gener-ate multiple ensembles per feature type.1http://ucrel.lancs.ac.uk/claws/For our classifier, we use SVMs, specifically theLIBLINEAR SVM software package (Fan et al2008),2 which is well-suited to text classificationtasks with large numbers of features and large num-bers of documents.
LIBLINEAR provides both lo-gistic regression and linear SVMs; we experimentwith both.
In general, the linear classifier performsbetter, but it only provides the decision output.
Thelogistic regression classifier on the other hand givesprobability estimates, which are required by mostof our combination methods (?2.3).
We thereforemostly use the logistic regression classifiers.2.2 L1- and L2-regularized SVM ClassifiersIn our preliminary experiments we noted thatsome feature types performed better with L1-regularization and others with L2.
In this work wegenerate classifiers using both methods and evaluatetheir individual and combined performance.2.3 Classifier Combination MethodsWe experiment with the following decision combi-nation methods, which have been discussed in themachine learning literature.
Polikar (2006) providesan exposition of these rules and methods.Plurality vote: Each classifier votes for a singleclass label, the label with the highest number ofvotes wins.
Ties are broken arbitrarily.Sum: All probability estimates are added togetherand the label with the highest sum is picked.Average: The mean of all scores for each classis calculated and the label with the highest averageprobability is chosen.Median: Each label?s estimates are sorted and themedian value is selected as the final score for thatlabel.
The label with the highest value is picked.Product: For each class label, all of the probabil-ity estimates are multiplied together to create the la-bel?s final estimate.
The label with the highest esti-mate is selected.
A single low score can have a bigeffect on the outcome.Highest Confidence: In this simple method, theclass label that receives the vote with the largest de-gree of confidence is selected as the final output.2Available at http://www.csie.ntu.edu.tw/?cjlin/liblinear/125Borda Count: The confidence estimates are con-verted to ranks and the final label selected using theBorda count algorithm (Ho et al 1994).
In thiscombination approach, broadly speaking points areassigned to ranks, and these tallied for the overallweight.With the exception of the plurality vote, all ofthese can be weighted.
In our ensembles we also ex-periment with weighting the output of each classifierusing its individual accuracy on the training data asan indication of our degree of confidence in it.2.4 Feature RepresentationMost NLI studies have used two types of feature rep-resentations: binary (presence or absence of a fea-ture in a text) and normalized frequencies.
Althoughbinary feature values have been used in some stud-ies (e.g.
Wong and Dras (2011)), most have usedfrequency-based values.In the course of our experiments we have ob-served that the effect of the feature representationvaries with the feature type, size of the feature spaceand the learning algorithm itself.
In our current sys-tem, then, we generate two classifiers for each fea-ture type, one trained with frequency-based values(raw counts scaled using the L2-norm) and the otherwith binary.
Our experiments assess both their indi-vidual and joint performance.2.5 Proficiency-level Based ClassificationTo utilise the proficiency level information providedin the TOEFL11 corpus (texts are marked as eitherlow, medium or high proficiency), we also investi-gate classifiers that are trained using only texts fromspecific proficiencies.Tetreault et al(2012) established that the classi-fication accuracy of their system varied across pro-ficiency levels, with high proficiency texts being thehardest to classify.
This is most likely due to the factthat writers at differing skill levels commit distincttypes of errors at different rates (Ortega, 2009, forexample).
If learners of different backgrounds com-mit these errors with different distributions, thesepatterns could be used by a learner to further im-prove classification accuracy.
We will use these fea-tures in one of our experiments to investigate theeffectiveness of such proficiency-level based classi-fiers for NLI.3 FeaturesWe roughly divide out feature types into lexical,part-of-speech and syntactic.
In all of the featuretypes below, we perform no feature selection.3.1 Lexical FeaturesAs all previous work, we use function words as fea-tures.
In addition, given the attempts to control fortopic bias in the TOEFL11 corpus, we also makeuse of various lexical features which have been pre-viously avoided by researchers due to the reportedtopic bias (Brooke and Hirst, 2011) in other NLI cor-pora such as the ICLE corpus.Function Words In contrast to content words,function words do not have any meaning themselves,but rather can be seen as indicating the grammat-ical relations between other words.
Examples in-clude articles, determiners, conjunctions and auxil-iary verbs.
They have been widely used in studies ofauthorship attribution as well as NLI and establishedto be informative for these tasks.
We use the listof 398 common English function words from Wongand Dras (2011).
We also tested smaller sets, but ob-served that the larger sets achieve higher accuracy.Function Word n-grams We devised and tested anew feature that attempts to capture patterns of func-tion word use at the sentence level.
We define func-tion word n-grams as a type of word n-gram wherecontent words are skipped: they are thus a specificsubtype of skip-gram discussed by Guthrie et al(2006).
For example, the sentence We should allstart taking the bus would be reduced to we shouldall the, from which we would extract the n-grams.Character n-grams Tsur and Rappoport (2007)demonstrated that character n-grams are a usefulfeature for NLI.
These n-grams can be consideredas a sub-word feature and their effectiveness is hy-pothesized to be a result of phoneme transfer fromthe writer?s L1.
They can also capture orthographicconventions of a language.
Accordingly, we limitour n-grams to a maximum size of 3 as longer se-quences would correspond to short words and notphonemes or syllables.Word n-grams There has been a shift towards theuse of word-based features in several recent studies(Brooke and Hirst, 2012; Bykh and Meurers, 2012;126Tetreault et al 2012), with new corpora come intouse for NLI and researchers exploring and address-ing the issues relating to topic bias that previouslyprevented their use.
Lexical choice is considered tobe a prime feature for studying language transfer ef-fects, and researchers have found word n-grams tobe one of the strongest features for NLI.
Tetreaultet al(2012) expanded on this by integrating 5-gramlanguage models into their system.
While we did notreplicate this, we made use of word trigrams.3.2 POS n-gramsMost studies have found that POS tag n-grams area very useful feature for NLI (Koppel et al 2005;Bykh and Meurers, 2012, for example).
The tagsetprovided by the Penn TreeBank is the most widelyused in these experiments, with tagging performedby the Stanford Tagger (Toutanova et al 2003).We investigate the effect of tagset granularityon classification accuracy by comparing the clas-sification accuracy of texts tagged with the PTBtagset against those annotated by the RASP Tagger(Briscoe et al 2006).
The PTB POS tagset contains36 unique tags, while the RASP system uses a subsetof the CLAWS2 tagset, consisting of 150 tags.This is a significant size difference and we hy-pothesize that a larger tagset could provide richerlevels of syntactically meaningful info which ismore fine-grained in distinction between syntacticcategories and contains more morpho-syntactic in-formation such as gender, number, person, caseand tense.
For example, while the PTB tagsethas four tags for pronouns (PRP, PRP$, WP,WP$), the CLAWS tagset provides over 20 pronountags (PPHO1, PPIS1, PPX2, PPY, etc.)
dis-tinguishing between person, number and grammati-cal role.
Consequently, these tags could help bettercapture error patterns to be used for classification.3.3 Syntactic FeaturesAdaptor grammar collocations Drawing onWong et al(2012), we also utilise an adaptor gram-mar to discover arbitrary lengths of n-gram collo-cations for the TOEFL11 corpus.
We explore boththe pure part-of-speech (POS) n-grams as well asthe more promising mixtures of POS and functionwords.
Following a similar experimental setup asper Wong et al(2012), we derive two adaptor gram-mars where each is associated with a different set ofvocabulary: either pure POS or the mixture of POSand function words.
We use the grammar proposedby Johnson (2010) for capturing topical collocationsas presented below:Sentence ?
Docj j ?
1, .
.
.
,mDocj ?
j j ?
1, .
.
.
,mDocj ?
Docj Topici i ?
1, .
.
.
, t;j ?
1, .
.
.
,mTopici ?
Words i ?
1, .
.
.
, tWords ?
WordWords ?
Words WordWord ?
w w ?
Vpos;w ?
Vpos+fwAs per Wong et al(2012), Vpos contains 119distinct POS tags based on the Brown tagset andVpos+fw is extended with 398 function words usedin Wong and Dras (2011).
The number of topics tis set to 50 (instead of 25 as per Wong et al(2012))given that the TOEFL corpus is larger than the ICLEcorpus.
The inference algorithm for the adaptorgrammars are based on the Markov Chain MonteCarlo technique made available by Johnson (2010).3Tree Subtitution Grammar fragments In rela-tion to the context-free grammar (CFG) rules ex-plored in the previous NLI work of Wong and Dras(2011), Tree Substitution Grammar (TSG) frag-ments have been proposed by Swanson and Char-niak (2012) as another form of syntactic featuresfor NLI classification tasks.
Here, as an approxi-mation to deploying the Bayesian approach to in-duce a TSG (Post and Gildea, 2009; Swanson andCharniak, 2012), we first parse each of the essays inthe TOEFL training corpus with the Stanford Parser(version 2.0.4) (Klein and Manning, 2003) to obtainthe parse trees.
We then extract the TSG fragmentsfrom the parse trees using the TSG system madeavailable by Post and Gildea (2009).4Stanford dependencies In Tetreault et al(2012),Stanford dependencies were investigated as yet an-other form of syntactic features.
We follow asimilar approach: for each essay in the train-ing corpus, we extract all the basic (rather than3http://web.science.mq.edu.au/?mjohnson/Software.htm4https://github.com/mjpost/dptsg127the collapsed) dependencies returned by the Stan-ford Parser (de Marneffe et al 2006).
Simi-larly, we generate all the variations for each ofthe dependencies (grammatical relations) by sub-stituting each lemma with its corresponding PoStag.
For instance, a grammatical relation ofdet(knowledge, the) yields the followingvariations: det(NN, the), det(knowledge,DT), and det(NN, DT).4 Experiments and ResultsWe report our results using 10-fold cross-validationon the combined training and development sets, aswell as by training a model using the training anddevelopment data and running it on the test set.We note that for our submission, we trained onlyon the training data; the results here thus differ fromthe official ones.4.1 Individual Feature Results and AnalysisWe ran the classifiers generated for each feature typeto assess their performance.
The results are summa-rized in Table 1: the Train + Dev Set results were forthe system when trained on the training and develop-ment data with 10 fold cross-validation, and the TestSet results for the system trained on the training anddevelopment data combined.Character n-grams are an informative feature andour results are very similar to those reported by pre-vious researchers (Tsur and Rappoport, 2007).
Inparticular, it should be noted that the use of punc-tuation is a very powerful feature for distinguishinglanguages.
Romance language speakers were mostlikely to use more punctuation symbols (colons,semicolons, ellipsis, parenthesis, etc.)
and at higherrates.
Chinese, Japanese and Korean speakers werefar less likely to use punctuation.The performance for word n-grams, TSG frag-ments and Stanford Dependencies is very strong andcomparable to previously reported research.
For theadaptor grammar n-grams, the mixed POS/functionword version yielded best results and was includedin the ensemble.4.2 POS-based Classification and Tagset SizeTo compare the tagsets we trained individual classi-fiers for n-grams of size 1?4 using both tagsets andtested them.
The results are shown in Table 2 andFeature Train +Dev SetTest SetChance Baseline 9.1 9.1Character unigram 33.99 34.70Character bigram 51.64 49.80Character trigram 66.43 66.70RASP POS unigram 43.76 45.10RASP POS bigram 58.93 61.60RASP POS trigram 59.39 62.70Function word unigram 51.38 54.00Function word bigram 59.73 63.00Word unigram 74.61 75.50Word bigram 74.46 76.00Word trigram 63.60 65.00TSG Fragments 72.16 72.70Stanford Dependencies 73.78 75.90Adaptor GrammarPOS/FW n-grams69.76 70.00Table 1: Classification results for our individual features.N PTB RASP1 34.03 43.762 48.85 58.933 51.06 59.394 49.85 52.81Table 2: Classification accuracy results for POS n-gramsof size N using both the PTB and RASP tagset.
The largerRASP tagset performed significantly better for all N.N Accuracy1 51.382 59.733 52.14Table 3: Classification results for Function Word n-gramsof size N. Our proposed Function Word bigram and tri-gram features outperform the commonly used unigrams.128Ensemble Train +Dev SetTest SetComplete Ensemble 81.50 81.60Only binary values 82.46 83.10Only freq values 65.28 67.20L1-regularized solver only 80.33 81.10L2-regularized solver only 81.42 81.10Bin, L1-regularized only 81.57 82.00Bin, L2-regularized only 82.00 82.50Table 4: Classification results for our ensembles, best re-sult in column in bold (binary values with L1- and L2-regularized solvers).show that the RASP tagged data provided better per-formance in all cases.
While it is possible that thesedifferences could be attributed to other factors suchas tagging accuracy, we do not believe this to be thecase as the Stanford Tagger is known for its high ac-curacy (97%).
These differences are quite clear; thisfinding also has implications for other syntactic fea-tures that make use of POS tags, such as AdaptorGrammars, Stanford Dependencies and Tree Substi-tution Grammars.4.3 Function Word n-gramsThe classification results using our proposed Func-tion Word n-gram feature are shown in Table 3.They show that function word skip-grams are moreinformative than the simple function word countsthat have been previously used.4.4 Ensemble ResultsTable 4 shows the results from our ensembles.
Thefeature types included in the ensemble are thosewhose results are listed individually in Table 1.
(So,for example, we only use the RASP-tagged PoS n-grams, not the Penn Treebank ones.)
The completeensemble consists of four classifiers per feature type:L1-/L2-regularized versions with both binary andfreq.
values.Bagging Our experiments with bagging did notfind any improvements in accuracy, even with largernumbers of bootstrap samples (50 or more).
Bag-ging is said to be more suitable for unstable clas-sifiers which have greater variability in their perfor-mance and are more susceptible to noise in the train-ing data (Breiman, 1996).
In our experiments withindividual feature types we have found the classi-fiers to be quite stable in their performance, acrossdifferent folds and training set sizes.
This is one po-tential reason why bagging did not yield significantimprovements.Combiner Methods Of the methods outlined in?2.3 we found the sum and weighted sum combinersto be the best performing, but the weighted resultsdid not improve accuracy in general over their un-weighted counterparts.
Our results are reported us-ing the unweighted sum combiner.
A detailed com-parison of the results for the combiners has beenomitted here due to time constraints; the differencesacross all combination methods was roughly 1?2%.Any new approach to ensemble combination meth-ods would consequently want to be radically differ-ent to expect a notable improvement in performance.As noted at the start of this section, results hereare for the system trained on training and develop-ment data.
The best result on the test set (83.1%)is almost 4% higher than our submission result, andclose to the highest result achieved (83.6%).Binary & Frequency-Based Feature Values Ourresults are consistent with those of Brooke and Hirst(2012), who conclude that there is a preferencefor binary feature values instead of frequency-basedones.
Including both types in the ensemble did notimprove results.However, in other experiments on the TOEFL11corpus we have also observed that use of frequencyinformation often leads to significantly better resultswhen using a linear SVM classifier: in fact, the lin-ear classifier is better on all frequency feature types,and also on some of the binary feature types.
Wepresent results in Table 5 comparing the two.
An ap-proach using the linear SVM that provides an asso-ciated probability score ?
perhaps through bagging?
allowing it to be combined with the methods de-scribed in ?2.3 could then perhaps boost results.
Allthese results were from a system using the trainingdata with 10 fold cross-validation.Combining Regularisation Approaches Resultsshow that combining the L1- and L2-regularizedclassifiers in the ensemble provided a small in-129Feature L2-norm scaled counts Binarylinear log.
regr.
linear log.
regr.Char unigram 31.60 26.23 25.68 26.36Char bigram 51.59 41.81 41.20 45.11Char trigram 65.78 54.97 58.30 61.76RASP POS bigram 60.38 54.00 50.31 54.56RASP POS trigram 58.75 53.92 55.93 58.58Function word unigram 51.38 45.09 46.67 47.13Function word bigram 58.95 53.22 54.97 58.53Word unigram 70.33 55.60 69.40 72.00Word bigram 73.90 54.25 73.65 74.93Word trigram 63.78 52.46 64.78 64.94Table 5: Classification results for our individual features.crease in accuracy.
Ensembles with either the L1 orL2-regularized solver have lower accuracy than thecombined methods (row 2).4.5 Proficiency-level Based ClassificationTable 6 shows our results for training models withtexts of a given proficiency level and the accuracy onthe test set.
The numbers show that in general textsshould be classified with a learner trained with textsof a similar proficiency.
They also show that not alltexts in a proficiency level are of uniform quality assome levels perform better with data from the clos-est neighbouring levels (e.g.
Medium texts performbest with data from all proficiencies), suggestingthat the three levels form a larger proficiency con-tinuum where users may fall in the higher or lowerends of a level.
A larger scale with more than threelevels could help address this.5 Discussion5.1 Unused Experimental FeaturesWe also experimented with some other feature typesthat were not included in the final system.CCG SuperTag n-grams In order to introduceadditional rich syntactic information into our sys-tem, we investigated the use CCG SuperTags as fea-ture for NLI classification.
We used the C&C CCGTrain Test Acc.
Train Test Acc.Low Low 52.2 All Med 86.8Med Low 72.1 M + H Med 85.3High Low 40.3 L + M Med 83.8All Low 75.2 Low High 16.1L + M Low 76.0 Med High 68.1Low Med 40.7 High High 65.7Med Med 83.6 M + H High 74.7High Med 62.1 All High 75.2Table 6: Results for classifying the test set documentsusing classifiers trained with a specific proficiency level.Each level?s best result in bold.Parser and SuperTagger (Curran et al 2007) to ex-tract SuperTag n-grams from the corpus, which werethen used as features to construct classifiers.
Thebest results were achieved by using n-grams of size2?4, which achieved classification rates of around44%.
However, adding these features to our ensem-ble did not improve the overall system accuracy.
Webelieve that this is because when coupled with theother syntactic features in the system, the informa-tion provided by the SuperTags is redundant, andthus they were excluded from our final ensemble.Hapax Legomena and Dis Legomena The spe-cial word categories Hapax Legomena and Dislegomena refer to words that appear only once and130twice, respectively, in a complete text.
In practice,these features are a subset of our Word Unigramfeature, where Hapax Legomena correspond to un-igrams with an occurrence count of 1 and Hapax dislegomena are unigrams with a count of 2.In our experimental results we found that Ha-pax Legomena alone provides an accuracy of 61%.Combining the two features together yields an accu-racy of 67%.
This is an interesting finding as bothof these features alone provide an accuracy close tothe whole set of word unigrams.5.2 Corpus RepresentativenessWe conducted a brief analysis of our extracted fea-tures, looking at the most predictive ones accordingto their Information Gain.
Although we did not findany obvious indicators of topic bias, we noted someother issues of potential concern.Chinese, Japanese and Korean speakers make ex-cessive use of phrases such as However, First of alland Secondly.
At first glance, the usage rate of thesephrases seems unnaturally high (more than 50% ofKorean texts had a sentence beginning with How-ever).
This could perhaps be a cohort effect relat-ing to those individually attempting this particularTOEFL exam, rather than an L1 effect: it wouldbe useful to know how much variability there is interms of where candidates come from.It was also noticed that many writers mention thename of their country in their texts, and this couldpotentially create a high correlation between thosewords and the language class label, leading perhapsto an artificial boosting of results.
For example, thewords India, Turkey, Japan, Korea and Germany ap-pear with high frequency in the texts of their corre-sponding L1 speakers ?
hundreds of times, in fact,in contrast to frequencies in the single figures forspeakers of other L1s.
These might also be an arte-fact of the type of text, rather than related to the L1as such.5.3 Hindi vs. TeluguWe single out here this language pair because ofthe high level of confusion between the two classes.Looking at the results obtained by other teams, weobserve that this language pair provided the worstclassification accuracy for almost all teams.
Nosystem was able to achieve an accuracy of 80%for Hindi (something many achieved for other lan-guages).
In analysing the actual and predictedclasses for all documents classified as Hindi andTelugu by our system, we find that generally allof the actual Hindi and Telugu texts (96% and99%, respectively) are within the set.
Our classifieris clearly having difficulty discriminating betweenthese two specific classes.Given this, we posit that the confounding influ-ence may have more to do with the particular styleof English that is spoken and taught within thecountry, rather than the specific L1 itself.
Consult-ing other research about SLA differences in multi-lingual countries could shed further light on this.Analysing highly informative features providessome clues about the influence of a common cul-ture or national identity: in our classifier, the wordsIndia, Indian and Hindu were highly predictive ofboth Hindi and Telugu texts, but no other lan-guages.
In addition, there were terms that werenot geographically- or culturally-specific that werestrongly associated with both Hindi and Telugu:these included hence, thus, and etc, and a muchhigher rate of use of male pronouns.
It has beenobserved in a number of places (Sanyal, 2007, forexample) that the English spoken across India stillretains characteristics of the English that was spo-ken during the time of the Raj and the East IndiaCompany that have disappeared from other varitiesof English, so that it can sound more formal to otherspeakers, or retain traces of an archaic business cor-respondence style; the features just noted would fitthat pattern.
The effect is likely to occur regardlessof the L1.Looking at individual language pairs in this waycould lead to incremental improvement in the overallclassification accuracy of NLI systems.ReferencesLeo Breiman.
1996.
Bagging predictors.
In MachineLearning, pages 123?140.Ted Briscoe, John Carroll, and Rebecca Watson.
2006.The second release of the rasp system.
In Proceedingsof the COLING/ACL on Interactive presentation ses-sions, COLING-ACL ?06, pages 77?80, Stroudsburg,PA, USA.
Association for Computational Linguistics.Julian Brooke and Graeme Hirst.
2011.
Native languagedetection with ?cheap?
learner corpora.
In Conference131of Learner Corpus Research (LCR2011), Louvain-la-Neuve, Belgium.
Presses universitaires de Louvain.Julian Brooke and Graeme Hirst.
2012.
Robust, Lexical-ized Native Language Identification.
In Proceedingsof COLING 2012, pages 391?408, Mumbai, India, De-cember.
The COLING 2012 Organizing Committee.Serhiy Bykh and Detmar Meurers.
2012.
Native Lan-guage Identification using Recurring n-grams ?
In-vestigating Abstraction and Domain Dependence.
InProceedings of COLING 2012, pages 425?440, Mum-bai, India, December.
The COLING 2012 OrganizingCommittee.James Curran, Stephen Clark, and Johan Bos.
2007.Linguistically motivated large-scale nlp with c&c andboxer.
In Proceedings of the 45th Annual Meeting ofthe Association for Computational Linguistics Com-panion Volume Proceedings of the Demo and PosterSessions, pages 33?36, Prague, Czech Republic, June.Association for Computational Linguistics.Marie-Catherine de Marneffe, Bill Maccartney, andChristopher D. Manning.
2006.
Generating typed de-pendency parses from phrase structure parses.
In Pro-ceedings of the Fifth International Conference on Lan-guage Resources and Evaluation (LREC?06), pages449?454, Genoa, Italy.Markus Dickinson and W. Detmar Meurers.
2003.
De-tecting errors in part-of-speech annotation.
In Pro-ceedings of the 10th Conference of the EuropeanChapter of the Association for Computational Linguis-tics (EACL-03), pages 107?114, Budapest, Hungary.Thomas G Dietterich.
2000.
Ensemble methods in ma-chine learning.
In Multiple classifier systems, pages1?15.
Springer.Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, Xiang-RuiWang, and Chih-Jen Lin.
2008.
LIBLINEAR: A li-brary for large linear classification.
Journal of Ma-chine Learning Research, 9:1871?1874.David Guthrie, Ben Allison, Wei Liu, Louise Guthrie,and Yorick Wilks.
2006.
A Close Look at Skip-gramModelling.
In Proceedings of the Fifth InternationalConference on Language Resources and Evaluation(LREC 2006), pages 1222?1225, Genoa, Italy.Tin Kam Ho, Jonathan J.
Hull, and Sargur N. Srihari.1994.
Decision combination in multiple classifiersystems.
Pattern Analysis and Machine Intelligence,IEEE Transactions on, 16(1):66?75.Mark Johnson.
2010.
Pcfgs, topic models, adaptor gram-mars and learning topical collocations and the struc-ture of proper names.
In Proceedings of the 48thAnnual Meeting of the Association for ComputationalLinguistics, pages 1148?1157, Uppsala, Sweden, July.Association for Computational Linguistics.Dan Klein and Christopher D. Manning.
2003.
Accurateunlexicalized parsing.
In Proceedings of the 41st An-nual Meeting on Association for Computational Lin-guistics - Volume 1, ACL ?03, pages 423?430, Sap-poro, Japan.
Association for Computational Linguis-tics.Ekaterina Kochmar.
2011.
Identification of a writer?s na-tive language by error analysis.
Master?s thesis, Uni-versity of Cambridge.Moshe Koppel, Jonathan Schler, and Kfir Zigdon.
2005.Automatically determining an anonymous author?s na-tive language.
Intelligence and Security Informatics,pages 41?76.Moshe Koppel, Jonathan Schler, and Shlomo Argamon.2009.
Computational Methods in Authorship Attribu-tion.
Journal of the American Society for InformationScience and Technology, 60(1):9?26.Robert Lado.
1957.
Linguistics Across Cultures: Ap-plied Linguistics for Language Teachers.
Universityof Michigan Press, Ann Arbor, MI, US.Lourdes Ortega.
2009.
Understanding Second LanguageAcquisition.
Hodder Education, Oxford, UK.Robi Polikar.
2006.
Ensemble based systems in deci-sion making.
Circuits and Systems Magazine, IEEE,6(3):21?45.Matt Post and Daniel Gildea.
2009.
Bayesian learn-ing of a tree substitution grammar.
In Proceedingsof the ACL-IJCNLP 2009 Conference Short Papers,ACLShort ?09, pages 45?48, Suntec, Singapore.
As-sociation for Computational Linguistics.Jyoti Sanyal.
2007.
Indlish: The Book for Every English-Speaking Indian.
Viva Books Private Limited.Benjamin Swanson and Eugene Charniak.
2012.
Na-tive Language Detection with Tree Substitution Gram-mars.
In Proceedings of the 50th Annual Meeting ofthe Association for Computational Linguistics (Vol-ume 2: Short Papers), pages 193?197, Jeju Island, Ko-rea, July.
Association for Computational Linguistics.Joel Tetreault, Daniel Blanchard, Aoife Cahill, and Mar-tin Chodorow.
2012.
Native tongues, lost andfound: Resources and empirical evaluations in nativelanguage identification.
In Proceedings of COLING2012, pages 2585?2602, Mumbai, India, December.The COLING 2012 Organizing Committee.Kristina Toutanova, Dan Klein, Christopher D. Manning,and Yoram Singer.
2003.
Feature-rich part-of-speechtagging with a cyclic dependency network.
In IN PRO-CEEDINGS OF HLT-NAACL, pages 252?259.Oren Tsur and Ari Rappoport.
2007.
Using ClassifierFeatures for Studying the Effect of Native Languageon the Choice of Written Second Language Words.In Proceedings of the Workshop on Cognitive Aspectsof Computational Language Acquisition, pages 9?16,132Prague, Czech Republic, June.
Association for Com-putational Linguistics.Sze-Meng Jojo Wong and Mark Dras.
2011.
ExploitingParse Structures for Native Language Identification.In Proceedings of the 2011 Conference on Empiri-cal Methods in Natural Language Processing, pages1600?1610, Edinburgh, Scotland, UK., July.
Associa-tion for Computational Linguistics.Sze-Meng Jojo Wong, Mark Dras, and Mark Johnson.2012.
Exploring Adaptor Grammars for Native Lan-guage Identification.
In Proceedings of the 2012 JointConference on Empirical Methods in Natural Lan-guage Processing and Computational Natural Lan-guage Learning, pages 699?709, Jeju Island, Korea,July.
Association for Computational Linguistics.133
