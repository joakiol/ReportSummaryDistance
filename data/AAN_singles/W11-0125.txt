Incremental dialogue act understandingVolha PetukhovaTilburg Center for Creative ComputingTilburg University, The Netherlands,v.petukhova@uvt.nlHarry BuntTilburg Center for Creative ComputingTilburg University, The Netherlands,harry.bunt@uvt.nlAbstractThis paper presents a machine learning-based approach to the incremental understanding of dia-logue utterances, with a focus on the recognition of their communicative functions.
A token-basedapproach combining the use of local classifiers, which exploit local utterance features, and globalclassifiers which use the outputs of local classifiers applied to previous and subsequent tokens, isshown to result in excellent dialogue act recognition scores for unsegmented spoken dialogue.
Thiscan be seen as a significant step forward towards the development of fully incremental, on-line meth-ods for computing the meaning of utterances in spoken dialogue.1 IntroductionWhen reading a sentence in a text, a human language understander obviously does not wait trying tounderstand what he is reading until he has come to the end of the sentence.
Similarly for participantsin a spoken conversation.
There is overwhelming psycholinguistic evidence that human understandersconstruct syntactic, semantic, and pragmatic hypotheses on the fly, while receiving the written or spokeninput.
Dialogue phenomena such as backchannelling (providing feedback while someone else is speak-ing), the completion of a partner utterance, and requests for clarification that overlap the utterance of themain speaker, illustrate this.
Evidence from the analysis of nonverbal behaviour in multimodal dialoguelends further support to the claim that human understanding works incrementally, as input is being re-ceived.
Dialogue participants start to perform certain body movements and facial expressions that areperceived and interpreted by others as dialogue acts (such as head nods, smiles, frowns) while anotherparticipant is speaking, see e.g.
Petukhova and Bunt (2009).
As another kind of evidence, eye-trackingexperiments by Tanenhaus et al (1995), Sedivy et al (1999) and Sedivy (2003) showed that definitedescriptions are resolved incrementally when the referent is visually accessible.Traditional models of language understanding for dialogue systems, by contrast, are pipelined, mod-ular, and operate on complete utterances.
Typically, such a system has an automatic speech recognitionmodule, a language understanding module responsible for syntactic and semantic analysis, an interpre-tation manager, a dialogue manager, a natural language generation module, and a module for speechsynthesis.
The output of each module is the input for another.
The language understanding module typ-ically performs the following tasks: (1) segmentation: identification of relevant segments in the input,such as sentences;(2) lexical analysis: lexical lookup, possibly supported by morphological processing,and by additional resources such as WordNet, VerbNet, or lexical ontologies; (3) parsing: constructionof syntactic interpretations; (4) semantic analysis: computation of propositional, referential, or action-related content; and (5) pragmatic analysis: determination of speaker intentions.Of these tasks, lexical analysis, being concerned with local information at word level, can be donefor each word as soon as it has been recognized, and is naturally performed as an incremental partof utterance processing, but syntactic, semantic and pragmatic analysis are traditionally performed oncomplete utterances.
Tomita?s pioneering work in left-to-right syntactic parsing has shown that incre-mental parsing can be much more efficient and of equal quality as the parsing of complete utterances(Tomita (1986)).
Computational approaches to incremental semantic and pragmatic interpretation have235been less successful (see e.g.
Haddock (1989); Milward and Cooper (2009)), but work in computationalsemantics on the design of underspecified representation formalisms has shown that such formalisms,developed originally for the underspecified representation of quantifier scopes, can also be applied insituations where incomplete input information is available (see e.g.
Bos (2002); Bunt (2007), Hobbs(1985), Pinkal (1999)) and as such hold a promise for incremental semantic interpretation.Pragmatic interpretation, in particular the recognition of a speaker?s intentions in incoming dialogueutterances, is another major aspect of language understanding for dialogue systems.
Computationalmodelling of dialogue behaviour in terms of dialogue acts aims to capture speaker intentions in the com-municative functions of dialogue acts, and offers an effective integration with semantic content analysisthrough the information state update approach (Poesio and Traum (1998)).
In this approach, a dialogueact is viewed as having as its main components a communicative function and a semantic content, wherethe semantic content is the referential, propositional, or action-related information that the dialogue actaddresses, and the communicative function defines how an understander?s information state is to be up-dated with that information.Evaluation of a non-incremental dialogue system and its incremental counterpart reported in Aistet al (2007) showed that the latter is faster overall than the former due to the incorporation of pragmaticinformation in early stages of the understanding process.
Since users formulate utterances incrementally,partial utterances may be available for a substantial amount of time and may be interpreted by the system.An incremental interpretation strategy may allow the system to respond more quickly, by minimizing thedelay between the time the user finishes and the time the utterance is interpreted DeVault and Stone(2003).This suggests that a dialogue system performance may benefit from reliable partial processing ofinput.
This paper is concerned with the automatic recognition of dialogue acts based on partially availableinput and shows that in order to arrive at the best output prediction two different classification strategiesare needed: (1) local classification that is based on features observed in dialogue behaviour and that canbe extracted from the annotated data; and (2) global classification that takes the locally predicted contextinto account.This paper is structured as follows.
In Section 2 we will outline performed experiments describingthe data, tagset, features, algorithms and evaluation metrics that have been used.
Section 3 reports on theexperimental results, applying a variety of machine learning techniques and feature selection algorithms,to assess the automatic recognition and classification of dialogue acts using simultaneous incrementalsegmentation and dialogue act classification.
In Section 4 we discuss strategies in management andcorrection of the output of local classifies.
Section 5 concludes.2 Incremental understanding experiments2.1 Related workNakano et al (Nakano et al (1999)) proposed a method for the incremental understanding of utteranceswhose boundaries are not known.
The Incremental Sentence Sequence Search (ISSS) algorithm findsplausible boundaries of utterances, called significant utterances (SUs), which can be a full sentence or asubsentential phrase, such as a noun phrase or a verb phrase.
Any phrase that can change the belief stateis defined as a SU.
In this sense an SU corresponds more or less with what we call a ?functional segment?,which is defined as a minimal stretch of behaviour that has a communicative function (see Bunt et al(2010)).
ISSS maintains multiple possible belief states, and updates these each time a word hypothesisis input.
The ISSS approach does not deal with the multifunctionality of segments, however, and doesnot allow segments to overlap.Lendvai and Geertzen (Lendvai and Geertzen (2007)) proposed token-based dialogue act segmenta-tion and classification, which was worked out in more detail in Geertzen (2009).
This approach takesdialogue data that is not segmented into syntactic or semantic units, but operates on the transcribed speechas a stream of words and other vocal signs (e.g.
laughs), including disfluent elements (e.g.
abandoned236Dimension Frequency General-purpose function FrequencyTask 31.8 PropositionalQuestion 5.8Auto-Feedback 20.5 Set Question 2.3Allo-Feedback 0.7 Check Question 3.3Turn Management 50.2 Propositional Answer 9.8Social Obligation Management 0.5 Set Answer 3.9Discourse Structuring 2.8 Inform 11.7Own Communication Management 10.3 InformRhetorical 21.9Time Management 26.7 Instruct 0.3Partner Communication Management 0.3 Suggest 10.1Contact Management 0.1 Request 5.6Table 1: Distribution of functional tags across dimensions and general-purpose functions for the AMI corpus (in%).or interrupted words).
Segmentation and classification of dialogue acts are performed simultaneously inone step.
Geertzen (2009) reports on classifier performance on this task for the DIAMOND data1 usingDIT++ labels.
The success scores in terms of F-scores range from 47.7 to 81.7.
It was shown that per-forming segmentation and classification together results in better segmentation, but affects the dialogueact classification negatively.The incremental dialogue act recognition system proposed here takes the token-based approach forbuilding classifiers for the recognition (segmentation and classification) of multiple dialogue acts for eachinput token, and adopts the ISSS idea for information-state updates based on partial input interpretation.2.2 TagsetThe data selected for the experiments was annotated with the DIT++ tagset Release 42.
The DIT tax-onomy distinguishes 10 dimensions, addressing information about: the domain or task (Task), feedbackon communicative behaviour of the speaker (Auto-feedback) or other interlocutors (Allo-feedback), man-aging difficulties in the speaker?s contributions (Own-Communication Management) or those of otherinterlocutors (Partner Communication Management), the speaker?s need for time to continue the di-alogue (Time Management), establishing and maintaining contact (Contact Management), about whoshould have the next turn (Turn Management), the way the speaker is planning to structure the dialogue,introducing, changing or closing a topic (Dialogue Structuring), and conditions that trigger dialogue actsby social convention (Social Obligations Management), see Table 1.For each dimension, at most one communicative function can be assigned, which is either a functionthat can occur in this dimension alone (a dimension-specific (DS) function) or a function that can occur inany dimension (a general-purpose (GP) function).
Dialogue acts with a DS communicative function arealways concerned with a particular type of information, such as a Turn Grabbing act, which is concernedwith the allocation of the speaker role, or a Stalling act, which is concerned with the timing of utteranceproduction.
GP functions, by contrast, are not specifically related to any dimension in particular, e.g.one can ask a question about any type of semantic content, provide an answer about any type of content,or request the performance of any type of action (such as Could you please close the door or Could youplease repeat that).
These communicative functions include Question, Answer, Request, Offer, Inform,and many other familiar core speech acts.The tagset used in these studies contains 38 dimension-specific functions and 44 general-purposefunctions.
A tag consists either of a pair consisting of a communicative function (CF ) and the addresseddimension (D).1For more information see Geertzen,J., Girard,Y., and Morante,R.
2004.
The DIAMOND project.
Poster at the 8th Work-shop on the Semantics and Pragmatics of Dialogue (CATALOG 2004).2For more information about the tagset and the dimensions that are identified, please visit:http://dit.uvt.nl/ or seeBunt (2009).237Speaker Token Task Auto-F. Allo-F. TurnM.
TimeM.
ContactM.
DS OCM PCM SOMB it B;inf O O O O O O O O OB has I:inf O O O O O O O O OB to I:inf O O O O O O O O OB look I:inf O O O O O O O O OB you O O B:check O O O O O O OB know O O E:check O O O O O O OB cool I:inf O O O O O O O O OD mmhmm O BE:positive O O O O O O O OB and I:inf O O BE:t keep O O O O O OB gimmicky E:inf O O O O O O O O OFigure 1: Segment boundaries and dialogue act label encoding in different dimensions.2.3 Features and data encodingIn the recognition experiments we used data from the AMI meeting corpus3.
For training we used threeannotated AMI meetings that contain 17,335 tokens forming 3,897 functional segments.
The distributionof functional tags across dimensions is given in Table 1.Features extracted from the data considered here relate to dialogue history: functional tags of the10 previous turns; timing: token duration and floor-transfer offset4 computed in milliseconds; prosody:minimum, maximum, mean, and standard deviation for pitch (F0 in Hz), energy (RMS), voicing (fractionof locally unvoiced frames and number of voice breaks) and speaking rate (number of syllables persecond)5; and lexical information: token occurrence, bi- and trigram of those tokens.
In total, 1,668features are used for the AMI data.To be able to identify segment boundaries, we assign to each token its communicative function labeland indicate whether a token starts a segment (B), is inside a segment (I), ends a segment (E), is out-side a segment (O), or forms a functional segment on its own (BE).
Thus, the class labels consist of asegmentation prefix (IBOE) and a communicative function label, see example in Figure 1.2.4 Classifiers and evaluation metricsAwide variety of machine-learning techniques has been used for NLP tasks with various instantiations offeature sets and target class encodings.
For dialogue processing, it is still an open issue which techniquesare the most suitable for which task.
We used two different types of classifiers to test their performanceon our dialogue data: a probabilistic one and a rule inducer.As a probabilistic classifier we used Bayes Nets.
This classifier estimates probabilities rather thanproduce predictions, which is often more useful because this allows us to rank predictions.
Bayes Netsestimate the conditional probability distribution on the values of the class attributes given the values ofthe other attributes.As a rule induction algorithm we chose Ripper (Cohen (1995)).
The advantage of a rule inducer isthat the regularities discovered in the data are represented as human-readable rules.The results of all experiments were obtained using 10-fold cross-validation.7 As a baseline it iscommon practice to use the majority class tag, but for our data sets such a baseline is not very usefulbecause of the relatively low frequencies of the tags in some dimensions.
Instead, we use a baseline3The A?ugmented M?ulti-party I?nteraction meeting corpus consists of multimodal task-oriented human-human multi-partydialogues in English, for more information visit (http://www.amiproject.org/4Difference between the time that a turn starts and the moment the previous turn ends.5These features were computed using the PRAAT tool6.
We examined both raw and normalized versions of these features.Speaker-normalized features were obtained by computing z-scores (z = (X-mean)/standard deviation) for the feature, wheremean and standard deviation were calculated from all functional segments produced by the same speaker in the dialogues.
Wealso used normalizations by first speaker turn and by previous speaker turn.7In order to reduce the effect of imbalances in the data, it is partitioned ten times.
Each time a different 10% of the data isused as test set and the remaining 90% as training set.
The procedure is repeated ten times so that in the end, every instance hasbeen used exactly once for testing and the scores are averaged.
The cross-validation was stratified, i.e.
the 10 folds containedapproximately the same proportions of instances with relevant tags as in the entire dataset.238that is based on a single feature, namely, the tag of the previous dialogue utterance (see Lendvai et al(2003))).Several metrics have been proposed for the evaluation of a classifier?s performance: error metricsand performance metrics.
The word-based error rate metric, introduced in Ang et al (2005), measuresthe percentage of words that were placed in a segment perfectly identical to that in the reference.
Thedialogue act based metric (DER) was proposed in Zimmermann et al (2005).
In this metric a word isconsidered to be correctly classified if and only if it has been assigned the correct dialogue act type andit lies in exactly the same segment as the corresponding word of the reference.
We will use the combinedDERsc error metric to evaluate joint segmentation (s) and classification (c):DERsc =Tokens with wrong boundaries and/or function classtotal number of tokens?
100To assess the quality of classification results, the standard F-score metric is used, which representsthe balance between precision and recall.3 Classification resultsDialogue utterances are often multifunctional, having a function in more than one dimension (see e.g.Bunt (2010)).
This makes dialogue act recognition a complex task.
Splitting up the output structure maymake the task more manageable; for instance, a popular strategy is to split a multi-class learning taskinto several binary learning tasks.
Sometimes, however, learning of multiple classes allows a learningalgorithm to exploit the interactions among classes.
We will combine these two strategies.
We have builtin total 64 classifiers for dialogue act recognition for the AMI data.
Some of the tasks were defined asbinary ones, e.g.
the dimension recognition task, others are multi-class learning tasks.We first trained classifiers to recognize the boundaries of a segment and its communicative functions(joint multi-class learning task) per dimension, see Table 2.BL BayesNet RipperDimensions F1 DERsc F1 DERsc F1 DERscTask 32.7 51.2 52.1 48.7 66.7 42.6Auto-Feedback 43.2 84.4 62.7 33.9 60.1 45.6Allo-Feedback 70.2 59.5 73.7 35.1 71.3 49.1Turn Management:initial 34.2 95.2 57.0 58.4 54.3 81.3Turn Management:close 33.3 92.7 54.2 46.9 49.3 87.3Time Management 43.7 96.5 64.5 46.1 61.4 53.1Discourse Structuring 41.2 35.1 72.7 19.9 50.2 30.9Contact Management 59.9 53.2 71.4 49.9 83.3 37.2Own Communication Management 36.5 87.9 68.3 51.3 58.3 76.8Partner Communication Management 49.5 59.0 58.5 45.5 51.4 58.7Social Obligation Management 34.5 47.5 86.5 35.9 83.3 44.3Table 2: Overview of F-scores and DERsc for the baseline (BL) and the classifiers for joint segmentation andclassification for each DIT++ dimension, for the data of the AMI corpus.The results show that both classifiers outperform the baseline by a broad margin.
The Bayes Netsclassifier marginally outperforms the Ripper rule inducer, but shows no significant differences in overallperformance.
Though the results obtained are quite encouraging, the performance on the joint segmen-tation and classification task does not outperforms the two-step segmentation and classification task re-ported in Geertzen et al (2007).
There is a drop in F-scores compared to the results reported by Geertzenet al (2007), which is explained by the fact that recall was quite low.
This means that the classifiersmissed a lot of relevant cases.
Looking more closely at the predictions made by the classifiers, we no-ticed that beginnings and endings of many segments were not found.
For example, the beginnings of SetQuestions are identified with perfect precision (100%), but about 60% of the segment beginnings werenot found.
The reason that the classifiers still show a reasonable performance is that most tokens occur239inside segments and are better classified, e.g.
the inside-tokens of Set Questions are classified with highprecision (83%) and reasonably high recall scores (76%).
Still, this is rather worrying, since the correctidentification of, in particular, the start of a relevant segment is crucial for future decisions.
These obser-vations led us to the conclusion that the search space and the number of initially generated hypothesesfor classifiers should be reduced, and we split the classification task in such a way that a classifier needsto learn one particular type of communicative function.We trained a classifier for each general-purpose and dimension-specific function defined in theDIT++ taxonomy, and observed that this has the effect that the various classifiers perform significantlybetter.
These functions were learned (1) in isolation; (2) as semantically related functions together, e.g.all information-seeking functions (all types of questions) or all information-providing functions (all an-swers and all informs).
Both the recognition of communicative functions and that of segment boundariesimproves significantly.
Table 3 gives an overview of the overall performance (best obtained scores) ofthe trained classifiers after splitting the learning task.BL BayesNet RipperClassification task F1 DERsc F1 DERsc F1 DERscGeneral-purpose functionsPropositional Questions 47.0 39.1 94.9 3.9 75.8 23.5Check Questions 43.8 56.4 68.5 19.6 61.3 33.1Set Questions 44.8 52.1 74.1 18.6 76.3 17.7Inform 45.8 39.9 79.8 18.7 66.5 30.5Inform Rhetorical 37.2 38.9 69.1 13.4 68.7 23.9Agreement 41.3 79.1 72.1 12.6 71.6 60.2Propositional Answer 32.0 77.8 66.8 26.1 52.2 53.8Set Answer 44.3 54.2 77.5 13.2 57.3 44.1Suggest 45.8 38.4 65.6 17.3 48.8 35.6Request 45.8 49.3 75.8 14.5 50.3 36.9Instruct 46.3 49.3 60.5 14.5 46.3 36.9Dimension-specific functionsAuto-Feedback 57.1 23.5 78.8 13.2 66.7 15.5Allo-Feedback 89.3 4.4 95.1 2.9 94.3 3.9Turn Management:initial 24.8 21.9 72.8 7.4 46.3 10.7Turn Management:close 30.7 64.9 62.0 22.5 54.7 39.6Time management 68.3 32.3 82.4 13.7 92.8 11.4Discourse Structuring 40.7 13.6 72.6 2.5 74.5 1.7Contact Management 21.4 48.6 89.2 5.7 92.3 3.6Own Communication Management 26.7 48.6 78.0 11.6 68.1 20.0Partner Communication Management 33.4 18.2 77.8 8.5 88.9 6.5Social Obligation Management 60.0 18.7 88.9 8.3 90.1 5.5Table 3: Overview of F-scores and DERsc for the baseline (BL) and the classifiers upon joint segmentationand classification task for each DIT++ communicative function or cluster of functions.
(Best scores indicated bynumbers in bold face.
)Segments having a general-purpose functions may address any of the ten DIT dimensions.
The taskof dimension recognition can be approached in two ways.
One approach is to learn segment boundaries,communicative function label and dimension in one step (e.g.
the class label B:task;inform).
This task isvery complicated, however.
First, it leads to data which are high dimensional and sparse, which will havea negative influence on the performance of the trained classifiers.
Second, in many cases the dimensioncan be recognized reliably only with some delay; for the first few segment tokens it is often impossibleto say what the segment is about.
For example:(1) 1.
What do you think who we?re aiming this at?2.
What do you think we are doing next?3.
What do you think Craig?The three Set Questions in (1) start with exactly the same words, but they address different dimensions:Question 1 is about the Task (in AMI - the design the television remote control); Question 2 serves the240purpose of Discourse Structuring; and Question 3 elicits feedback.Another approach is to first recognize segment boundaries and communicative function, and definedimension recognition as a separate classification task.Tokens SetQuestion Task Auto-F. TurnM.
Complex label (BIOE:D;CF)label p label p label p label p label pwhat B:setQ 0.85 O 0.71 O 1 O 0.68 O 0.933you I:setQ 1 task 0.985 O 1 B:give 0.64 O 0.869guys I:setQ 1 task 0.998 O 1 E:give 0.66 O 0.937have I:setQ 1 task 0.997 O 1 O 1 I:task;setQ 0.989already I:setQ 1 task 0.996 O 1 O 0.99 I:task;setQ 0.903received I:setQ 1 task 0.987 O 1 O 1 I:task;setQ 0.813um O 0.93 O 0.89 O 1 BE:keep 0.99 O 0.982in I:setQ 1 task 0.826 O 1 O 0.89 I:task;setQ 0.875your I:setQ 1 task 0.996 O 1 O 0.99 I:task;setQ 0.948mails E:setQ 0.99 task 0.987 O 1 O 1 E:task;setQ 0.948Figure 2: Predictions with indication of confidence scores (highest p class probability selected) for each tokenassigned by five trained classifiers simultaneously.We tested both strategies.
The F-scores for the joint learning of complex class labels range from23.0 (DERsc = 68.3) to 45.3 (DERsc = 63.8).
For dimension recognition as a separate learning taskthe F-scores are significantly higher, ranging from 70.6 to 97.7.
The scores for joint segmentation andfunction recognition in the latter case are those listed in Table 3.
Figure 2 gives an example of predictionsmade by five classifiers for the input what you guys have already received um in your mails.4 Managing local classifiers4.1 Global classification and global searchAs shown in the previous section, given a certain input we obtain all possible output predictions (hypothe-ses) from local classifiers.
Some predictions are false, but once a local classifier has made a decision itis never revisited.
It is therefore important to base the decision on dialogue act labels not only on localfeatures of the input, but to take other parts of the output into account as well.
For example, the partialoutput predicted so far, i.e.
the history of previous predictions, may be taken as features for the nextclassification step, and helps to discover and correct errors.
This is known as ?recurrent sliding windowstrategy?
(see Dietterich (2002)) when the true values of previous predictions are used as features.
Thisapproach suffers from the label bias problem, however, when a classifier overestimates the importanceof certain features, and moreover does not apply in a realistic situation, since the true values of previouspredictions are not available to a classifier in real time.
A solution proposed by Van den Bosch (1997) isto apply adaptive training using the predicted output of previous steps as features.We trained higher-level classifiers (often referred to as ?global?)
that have, along with features ex-tracted locally from the input data as described above, the partial output predicted so far from all localclassifiers.
We used five previously predicted class labels, assuming that long distance dependenciesmay be important, and taking into account that the average length of a functional segment in our datais 4.4 tokens.
Table 4 gives an overview of the results of applying these global classifiers.
We see thatthe global classifiers make more accurate predictions than the local classifiers, showing an improvementof about 10% on average.
The classifiers still make some incorrect predictions, because the decisionis sometimes based on incorrect previous predictions.
An optimized global search strategy may lead tofurther improvements of these results.A strategy to optimize the use of output hypotheses, is to perform a global search in the output spacelooking for best predictions.
Our classifiers do not just predict the most likely class for an instance,but also generate a distribution of output classes.
Class distributions can be seen as confidence scoresof all predictions that led to a certain state.
Our confidence models are constructed based on tokenlevel information given the dialogue left-context (i.e.
dialogue history, wording of the previous and241Classification task BayesNet RipperF1 DERsc F1 DERscTask 65.3 14.9 79.1 21.8Auto-Feedback 72.9 8.1 77.8 7.2Allo-Feedback 67.7 10.9 74.2 9.5Turn Management:initial 72.2 11.5 69.5 11.4Turn Management:close 82.7 5.0 83.0 4.9Time Management 70.0 3.0 73.5 2.1Discourse Structuring 72.3 4.9 63.7 3.6Contact Management 79.1 4.5 84.3 4.6Own Communication Management 66.0 2.4 68.3 2.3Partner Communication Management 63.2 7.8 59.5 11.4Social Obligation Management 88.4 0.9 81.6 1.7Table 4: Overview of F-scores and DERsc of the global classifiers for the AMI data based on added previouspredictions of local classifiers.currently produced functional segment).
This is particular useful for dialogue act recognition becausethe recognition of intentions should be based on the system?s understanding of discourse and not just onthe interpretation of an isolated utterance.
Searching the (partial) output space for the best predictionsis not always the best strategy, however, since the highest-ranking predictions are not always correctin a given context.
A possible solution to this is to postpone the prediction until some (or all) futurepredictions have been made for the rest of the segment.
For training, the classifier then uses not onlyprevious predictions as additional features, but also some or all future predictions of local classifiers (tillthe end of the current segment or to the beginning of the next segment, depending on what is recognized).This forces the classifier to not immediately select the highest-ranking predictions, but to also considerlower-ranking predictions that could be better in the context of the rest of the sequence.Classification task BayesNet RipperF1 DERsc F1 DERscTask 82.6 9.5 86.1 8.3Auto-Feedback 81.9 1.9 95.1 0.6Allo-Feedback 96.3 0.6 95.7 0.5Turn Management:initial 85.7 1.5 81.5 1.6Turn Management:close 90.9 3.8 91.2 3.6Time management 90.4 2.4 93.4 1.7Discourse Structuring 82.1 1.7 78.3 1.8Contact Management 87.9 1.2 94.3 0.6Own Communication Management 78.4 2.2 81.6 2.0Partner Communication Management 71.8 2.4 70.0 4.6Social Obligation Management 98.6 0.4 98.6 0.5Table 5: Overview of F-scores and DERsc of global classifiers for the AMI data per DIT++ dimension.The results show the importance of optimal global classification for finding the best output prediction.We performed similar experiments on the English MapTask data8 and obtained comparable results,where F-scores on the global classification task range from 66.7 for Partner CommunicationManagementand Discourse Structuring to 79.7 for Task and 91.2 for Allo-Feedback.
For the MapTask corpus theperformance of human annotators on segmentation and classification has been assessed; standard kappascores reported in Bunt et al (2007) range between 0.92 and 1.00, indicating near perfect agreementbetween two expert annotators9.8For more information about the MapTask corpus see http://www.hcrc.ed.ac.uk/maptask/9Note, however, that a slightly simplified version of the DIT++ tagset has been used here, called the LIRICS tagset, inwhich the five DIT levels of processing in the Auto- and Allo-Feedback dimensions were collapsed into one.2425 Conclusions and future researchThe incremental construction of input interpretation hypotheses is useful in a language understandingsystem, since it has the effect that the understanding of a relevant input segment is already nearly readywhen the last token of the segment is received; when a dialogue act is viewed semantically as a recipe forupdating an information state, this means that the specification of the update operation is almost ready atthat moment, thus allowing an instantaneous response from the system.
It may even happen that the con-fidence score of a partially processed input segment is that high, that the systemmay decide to go forwardand update its information state without waiting until the end of the segment, and prepare or produce aresponse based on that update.
Of course, full incremental understanding of dialogue utterances includesnot only the recognition of communicative functions, but also that of semantic content.
However, manydialogue acts have no or only marginal semantic content, such as turn-taking acts, backchannels (m-hm)and other feedback acts (okay), time management acts (Just a moment), apologies and thankings andother social obligation management acts, and in general dialogue acts with a dimension-specific func-tion; for these acts the proposed strategy can work well without semantic content analysis, and willincrease the system?s interactivity significantly.
Moreover, given that the average length of a functionalsegment in our data is no more than 4.4 tokens, the semantic content of such a segment tends not to bevery complex, and its construction therefore does not seem to require very sophisticated computationalsemantic methods, applied either in an incremental fashion (see e.g.
Aist et al (2007) and DeVault andStone (2003)) or to a complete segment.Interactivity is however not the sole motivation for incremental interpretation.
The integration ofpragmatic information obtained from the dialogue act recognition module, as proposed here, at earlyprocessing stage can be beneficially used by the incremental semantic parser (but also syntactic parsermodule).
For instance, information about the communicative function of the incoming segment at earlyprocessing stage can defuse a number of ambiguous interpretations, e.g.
used for the resolution ofmany anaphoric expressions.
A challenge for future work is to integrate the incremental recognition ofcommunicative functions with incremental syntactic and semantic parsing, and to exploit the interactionof syntactic, semantic and pragmatic hypotheses in order to understand incoming dialogue segmentsincrementally in an optimally efficient manner.AcknowledgmentsThis research was conducted within the project ?Multidimensional Dialogue Modelling?, sponsored by the Netherlands Organ-isation for Scientific Research (NWO), under grant reference 017.003.090.
We are also very thankful to anonymous reviewersfor their valuable comments.ReferencesAist, G., J. Allen, E. Campana, C. Gomez Gallo, S. Stoness, M. Swift, and M. K. Tanenhaus (2007).
Incrementalunderstanding in human-computer dialogue and experimental evidence for advantages over nonincrementalmethods.
In Proceedings of the 11th Workshop on the Semantics and Pragmatics of Dialogue, Trento, Italy, pp.149?154.Ang, J., Y. Liu, and E. Shriberg (2005).
Automatic dialog act segmentation and classification in multiparty meet-ings.
In Proceedings of the ICASSP, Volume vol.
1, Philadelphia, USA, pp.
10611064.Bos, J.
(2002).
Underspecification and resolution in discourse semantics.
PhD Thesis.
Saarbru?cken: SaarlandUniversity.Bunt, H. (2007).
Semantic underspecification: which techniques for what purpose?
In Computing Meaning, Vol.3, pp.
55?85.
Dordrecht: Springer.Bunt, H. (2009).
The DIT++ taxonomy for functional dialogue markup.
In Proceedings of the AAMAS 2009Workshop ?Towards a Standard Markup Language for Embodied Dialogue Acts?
(EDAML 2009), Budapest.Bunt, H. (2010).
Multifunctionality in dialogue and its interpretation.
Computer, Speech and Language, Specialissue on dialogue modeling.243Bunt, H., J. Alexandersson, J. Carletta, J.-W. Choe, A. Fang, K. Hasida, K. Lee, V. Petukhova, A. Popescu-Belis,L.
Romary, C. Soria, and D. Traum (2010).
Language resource management ?
Semantic annotation framework?
Part 2: Dialogue acts.
ISO DIS 24617-2.
Geneva: ISO Central Secretariat.Bunt, H., V. Petukhova, and A. Schiffrin (2007).
Lirics deliverable d4.4.
multilingual test suites for semanticallyannotated data.
Available at http://lirics.loria.fr.Cohen, W. (1995).
Fast effective rule induction.
In Proceedings of the 12th International Conference on MachineLearning (ICML?95), pp.
115?123.DeVault, D. and M. Stone (2003).
Domain inference in incremental interpretation.
In Proceedings of the Workshopon Inference in Computational Semantics, INRIA Lorraine, Nancy, France.Dietterich, T. (2002).
Machine learning for sequential data: a review.
In Proceedings of the Joint IAPR Interna-tional Workshop on Structural, Syntactic, and Statistical Pattern Recognition, pp.
15?30.Geertzen, J.
(2009).
Dialogue act recognition and prediction: exploration in computational dialogue modelling.The Netherlands: Tilburg University.Geertzen, J., V. Petukhova, and H. Bunt (2007, September).
A multidimensional approach to utterance segmenta-tion and dialogue act classification.
In Proceedings of the 8th SIGdial Workshop on Discourse and Dialogue,Antwerp, Belgium, pp.
140?149.
Association for Computational Linguistics.Haddock, N. (1989).
Computational models of incremental semantic interpretation.
Language and CognitiveProcesses Vol.
14 (3), SI337?SI380.Hobbs, J.
(1985).
Ontological promiscuity.
In Proceedings 23rd Annual Meeting of the ACL, Chicago, pp.
61?69.Lendvai, P., v. d. A. Bosch, and E. Krahmer (2003).
Machine learning for shallow interpretation of user utter-ances in spoken dialogue systems.
In Proceedings of EACL-03 Workshop on Dialogue Systems: interaction,adaptation and styles of management, Budapest.Lendvai, P. and J. Geertzen (2007).
Token-based chunking of turn-internal dialogue act sequences.
In Proceedingsof the 8th SIGdial Workshop on Discourse and Dialogue, Antwerp, Belgium, pp.
174?181.Milward, D. and R. Cooper (2009).
Incremental interpretation: applications, theory, and relationship to dynamicsemantics.
In Proceedings COLING 2009, Kyoto, Japan, pp.
748?754.Nakano, M., N. Miyazaki, J. Hirasawa, K. Dohsaka, and T. Kawabata (1999).
Understanding unsegmented user ut-terances in real-time spoken dialogue systems.
In Proceedings of the 37th Annual Conference of the Associationof Computational Linguistics, ACL, pp.
200?207.Petukhova, V. and H. Bunt (2009).
Who?s next?
speaker-selection mechanisms in multiparty dialogue.
In Pro-ceedings of the Workshop on the Semantics and Pragmatics of Dialogue, Stockholm,, pp.
19?26.Pinkal, M. (1999).
On semantic underspecification.
In Computing Meaning, Vol.
1, pp.
33?56.
Dordrecht: Kluwer.Poesio, M. and D. Traum (1998).
Towards an Axiomatization of Dialogue Acts.
In Proceedings of the TwenteWorkshop on the Formal Semantics and Pragmatics of Dialogue, Twente, pp.
309?347.Sedivy, J.
(2003).
Pragmatic versus form-based accounts of referential contrast: Evidence for effects of informa-tivity expectations.
Journal of Psycolinguistic Research 32(1), 3?23.Sedivy, J., M. Tanenhaus, C. Chambers, and G. Carlson (1999).
Achieving incremental semantic interpretationthrough contextual representation.
Cognition 71, 109?147.Tanenhaus, M., M. Spivey-Knowlton, K. Eberhard, and J. Sedivy (1995).
Intergration of visual and linguisticinformation in spoken language comprehension.
Science 268, 1632?1634.Tomita, M. (1986).
Efficient parsing for natural language.
Dordrecht: Kluwer.Van den Bosch, A.
(1997).
Learning to pronounce written words: A study in inductive language learning.
PhDthesis.
The Netherlands: Maastricht University.Zimmermann, M., Y. Lui, E. Shriberg, and A. Stolcke (2005).
Toward joint segmentation and classification ofdialog acts in multiparty meetings.
In Proceedings of the Multimodal Interaction and Related Machine LearningAlgorithms Workshop (MLMI05), pp.
187?193.
Springer.244
