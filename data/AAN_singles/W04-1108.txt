Combining Neural Networks and Statistics forChinese Word sense disambiguationZhimao Lu  Ting Liu  Sheng LiInformation Retrieval Laboratory of Computer Science & Technology School,Harbin Institute of TechnologyHarbin, China, 150001{lzm, tliu}@ir.hit.edu.cnAbstractThe input of network is the key problem forChinese Word sense disambiguation utilizingthe Neural Network.
This paper presents aninput model of Neural Network that calculatesthe Mutual Information between contextualwords and ambiguous word by using statisticalmethod and taking the contextual words tocertain number beside the ambiguous wordaccording to (-M, +N).
The experiment adoptstriple-layer BP Neural Network model andproves how the size of training set and thevalue of M and N affect the performance ofNeural Network model.
The experimentalobjects are six pseudowords owning threeword-senses constructed according to certainprinciples.
Tested accuracy of our approach ona close-corpus reaches 90.31%,, and 89.62% ona open-corpus.
The experiment proves that theNeural Network model has good performanceon Word sense disambiguation.1 IntroductionIt is general that one word with many senses innatural language.
According statistics, there areabout 42% ambiguous words in Chinese corpus (Lu,2001).
Word sense disambiguation (WSD) is amethod to determine the sense of ambiguous wordgiven the context circumstance.WSD, a long-standing problem in NLP, has beena very active research topic,, which can be wellapplied in many NLP systems, such as InformationRetrieval, Text Mining, Machine Translation, TextCategorization, Text Summarization, SpeechRecognition, Text to Speech, and so on.With rising of Corpus linguistics, the machinelearning methods based on statistics are booming(Yarowsky, 1992).
These methods draw the supportfrom the high-powered computers, get the statistics oflarge real-world corpus, find and acquire knowledgeof linguistics automatically.
They deal with all changeby invariability, thus it is easy to trace the evaluationand development of natural language.
So the statisticmethods of NLP has attracted the attention ofprofessional researchers and become the mainstreambit by bit.
Corpus-based Statistical approaches areDecision Tree (Pedersen, 2001), Decision List,Genetic Algorithm, Naive-Bayesian Classifier(Escudero, 2000)?Maximum Entropy Model (Adam,1996; Li, 1999), and so on.Corpus-based statistical approaches can be dividedinto supervised and unsupervised according to whethertraining corpus is sense-labeled text.
Supervisedlearning methods have the good learning ability andcan get better accuracy in WSD experiments (Sch?tze,1998).
Obviously the data sparseness problem is abottleneck for supervised learning algorithm.
If youwant to get better learning and disambiguating effect,you can enlarge the size and smooth the data oftraining corpus.
According to practical demand, itwould spend much more time and manpower toenlarge the size of training corpus.
Smoothing data ismerely a subsidiary measure.
The sufficient large sizeof training corpus is still the foundation to get asatisfied effect in WSD experiment.Unsupervised WSD never depend on taggedcorpus and could realize the training of large realcorpus coming from all kinds of applying field.
Soresearchers begin to pay attention to this kind ofmethods (Lu, 2002).
The kind of methods canovercome the sparseness problem in a degree.It is obvious that the two kinds of methods basedon statistic have their own advantages anddisadvantages, and cannot supersede each other.This paper researches the Chinese WSD using themodel of artificial neural network and investigatesthe effect on WSD from input model of neuralnetwork constructed by the context words and thesize of training corpus.2 BP Neural NetworkAt the moment, there are about more than 30 kinds ofartificial neural network (ANN) in the domain ofresearch and application.
Especially, BP neuralnetwork is a most popular model of ANN nowadays.2.1 The structure of BP Neural NetworkThe BP model provides a simple method tocalculate the variation of network performancecased by variation of single weight.
This modelcontains not only input nodes and output nodes, butalso multi-layer or mono-layer hidden nodes.
Fig1.1is a construction chart of triple-layer BP neuralnetwork.
As it is including the weights modifyingprocess from the output layer to the input layerresulting from the total errors, the BP neuralnetwork is called Error Back Propagation network.Fig.
1.1 BP NetworkFig.1.1 The structure of BP neural networkExcept for the nodes of input layer, all nodes ofother layers are non-linear input and output.
So thefeature function should be differential on every partof function.
General speaking, we can choose thesigmoid, tangent inspired, or linear function as thefeature function because they are convenient forsearching and solving by gradient technique.Formula (1) is a sigmoid function.
?1?The output of sigmoid function ranges between 0and 1, increasing monotonically with its input.Because it maps a very large input domain to asmall range of outputs, it is often referred to as thesquashing function of the unit.
The output layer andhidden layer should adopt the sigmoid inspiredfunction under the condition of intervention on theoutput, such as confining the output between 0 and1.2.2 Back Propagation function of BPneural networkThe joint weights should be revised many timesduring the progress of the error propagating back inBP networks.
The variation of joint weights everytime is solved by the method of gradient descent.Because there is no objective output in hidden layer,the variation of joint weight in hidden layer issolved under the help of error back propagation inoutput layer.
If there are many hidden layers, thismethod can reason out the rest to the first layer byanalogy.1) the variation of joint weights in output layerTo calculate the variation of joint weights frominput i?th to output k?th is as following:?wik = ??
= ??
(2)= ?
(tk - Ok )f2?
O?i = ?
?ik O?i?ik = (tk ?
Ok )f2?
(3)?bki = ??
= ??
(4)= ?
(tk - Ok )f2?
= ?
?ik2) the variation of joint weights in hidden layerTo calculate the variation of joint weights frominput j?th to output i?th is as following:?w?ij = -?
= -?
(5)11 + e-xf (x)  = ???
?E ???
?wik?E ???
?Ok?Ok ???
?
wik?E ????
w?ijn?k=1?E ???
?bki?E ???
?Ok?Ok ???
?bki??
?????
?x1  x3 xn x2y1  y3 ym y2OutputsHiddenInputs?E ???
?Ok?Ok ????O?i?O?i????
w?ijn?k=1= ?
(tk - Ok )f2?
wik f1?pj= ?
?ij pjwhere:  ?ij = ei f1?
?ei =    ?ik wik          (6)?b?ki = ?
?ij                          (7)3.
The construction of WSD modelUnder the consideration of fact that onlynumerical data can be accepted by the input andoutput of neural network, if BP neural network isused on WSD, the prerequisite is to vector the part ofsemantic meaning (words or phrases) and sense.In the event of training BP model, the input vectorP and objective vector O of WSD should bedetermined firstly.
And then we should choose theconstruction of neural network that needs to bedesigned, say, how many layers is network, howmany neural nodes are in every layer, and theinspired function of hidden layer and output layer.The training of model still needs the vector addedweight, output, and error vector.
The training is overwhen the sum of square errors is less than theobjection of error.
Or the errors of output very toadjust the joint weight back and repeat the training.3.1 To vector the vocabularyWSD depends on the context to judge the meaningof ambiguous words.
So the input of model shouldbe the ambiguous words and the contextual wordsround them.
In order to vector the words in thecontext, the Mutual Information (MI) of ambiguouswords and context should be calculated.
So MI canshow the opposite distance of ambiguous words andcontextual words.
MI can replace every contextualword.
That is suitable to as the input model.
Thefunction of MI is as follow:(8)P(w1) and P(w2) are the probability of word w1and w2 to appear in the corpus separately.
WhileP(w1, w2) is the probability of word w1 and w2 toappear together.The experimental corpus in this article stemsfrom the People Daily of 1998.
The extent is123,882 lines (10,000,000 words), including121,400 words and phrases.3.2 The pretreatment of BP network modelThe supervised WSD need artificial mark ofmeaning.
But it is time consuming to mark artificially.So it is difficult to get the large scope and high qualitytraining linguistic corpus.
In order to overcome thisdifficulty and get large enough experimental linguisticcorpuses, we should turn to seek the new way.We use pseudoword in place of the real word.
Thatcan get the arbitrary large experimental corpusaccording to the real demand.3.2.1 The construction of PseudowordPseudoword is the artificial combination ofseveral real words on the basis of experimentaldemand to form an unreal word that possessesmany features of real words and instead of realword as the experimental object in natural languageresearch.In the real world, one word has many meaningsderives from the variation and flexible applicationof words.
That needs a long-term natural evolution.Frankly speaking, that evolution never ceases at alltimes.
For example, the word ??
?
(da3) extendssome new uses in recent years.
Actually, in theendless history river of human beings, thedevelopment and variation of words meaning arerapid so far as to be more rapid than thereplacement of dictionaries sometimes.
Usually thatmakes an awkward position when you usedictionary to define the words meanings.
Definitely,it is inconvenient for the research of naturallinguistics based on dictionary.But the meaning of pseudoword (Sch?tze, 1992)need not defined with the aid of dictionary andsimulates the real ambiguous word to survey theeffect of various algorithms of classified meanings.To form a pseudoword need the single meaningword as a morpheme.Set:   Wp =  w1 / w2 / ?
/ wiWp is a pseudoword formed with wi whichcontains i algorithms and meanings for everyn ?k=1MI(w1, w2) =P(w1, w2) ????
?P(w1)P(w2) logn ?k=1algorithm of pseudoword is single meaning andevery living example is about equal to apseudoword marked meaning in corpus.
That issimilar to the effect of artificial marked meaning.But the effect is more stable and reliable thanartificial marked meaning.
What?s more, the scopeof corpus can enlarge endless according to thedemand to avoid the phenomenon of sparse data.To define the number of algorithm, we count theaverage number of meanings according to thelarge-sized Chinese dictionaries (Table 3.1).
Table3.2 show the overall number of ambiguous wordand percentage of ambiguous word having 2~4meanings in all ambiguous word.
These two chartsindicate that verb is most active in Chinese and itsaverage number of meanings is most, about 2.56.The percentage of ambiguous word having 2~4meanings is most in all ambiguous word.part ofspeechAveragesense?includingsingle-senseword?Average sense?onlyambiguousword?noun 1.136452 2.361200verb 1.220816 2.558158adjective 1.144717 2.300774adverb 1.059524 2.078431Table 3.1 the average number of a Chineseword?s sense3.2.2 Define the input vectorIt should be based on context to determine thesense of ambiguous word.
The model?s input shouldbe the vector of the ambiguous word and contextwords.
It is well-known that the number of contextambiguousword 7955 /Bi-sensesword 5799 72.80%Tri-sensesword 1154 14.51%Four-sensesword 450 5.66%Table 3.2 the distributing of ambiguous wordwords showing on the both sides of ambiguousword is not fixed in different sentences.
But thenumber of vectors needed by BP network is fixed.In other words, the number of neural nodes of inputmodel is fixed in the training.
If the extractingmethod of feature vector is (-M, +N) in context, inother words there are M vectors on the left ofambiguous word and N vectors on the right, theextraction of feature vectors must span the limit ofsentences.
If the number of feature vectors is notenough, the ambiguous words on the left and rightboundaries of whole corpus do not participate in thetraining.According to the extracting method of featurevector (-M, +N), the vector of model input is asfollowing:V ??
= {MI11?MI 12??
?MI1i?MI 11?
?MI 12???
?MI 1j?
?MI21?MI 22??
?MI2i?MI 21?
?MI 22????MI2j?
?MI31?MI 32??
?MI3i?MI 31?
?MI 32???
?MI 3j?
}?1?i?M?1?j?N.Where, MI1i , MI1j?
are the MI of context and thefirst meaning of ambiguous word?MI2i , MI2j?
arethe MI of context and the second meaning ofambiguous word?MI3i ?MI3j?
are the MI of contextand the third meaning of ambiguous word.
MI1i,MI2i and MI3i are the feature words of ambiguousword on the left and MI of ambiguous word.
MI1j?,MI2j?
and MI2j?
are the feature words of ambiguousword on the right and MI of ambiguous word.pseudo-wordswordIDsamplenumberpseudo-wordswordIDsamplenumber34466 5550 84323 377371345 3715 12751 228431796 12098 52915 3900W1total 21363W4total 995771072 9296 53333 136278031 6024 29053 613548469 1509 75941 1205W2total 16829W5total 87027464 25925 39945 234677375 2478 71335 164023077 4704 51491 1012W3total 33107W6total 4998Table 3.3 the total number of the feature -vectorsample of ambiguous wordTraining corpus are 105,000 lines, and each lineis a paragraph, totally about 10,000,000 words.Table 3.3 shows the number of collected featurevector samples (the frequency of ambiguous word).3.3 The definition of output modelEvery ambiguous word has three meanings,totally eighteen meanings for six ambiguous words.Every ambiguous word trains a model and everymodel has three outputs showed by three-bit integerof binary system, such as the three meanings ofambiguous word W are showed as followed:si1 = 100    si2 = 010   si3 = 0013.4 The definition of network structureAccording to statistics, when (-M, +N) are (-8,+9) using the method of feature extraction, thecover percentage of effective information is morethan 87% (Lu, 2001).
However, if the sentence isvery short, collecting the contextual feature wordson the basis of (-8, +9) can include much uselessinformation to the input model.
Undoubtedly, thatwill increase more noisy effect and deduce themeaning-distinguish ability of verve network.This article makes an on-the-spot investigation ofexperimental corpus, a fairly integrated meaningunit (the marks of border including comma,semicolon, ellipsis, period, question mark,exclamation mark, and the like), which averagelength is between 9~10 words.
So this articlecollects the contextual feature words on the basis of(-5, +5) in the experiments, 10 feature wordsavailable that calculate MI with each meaning ofambiguous word separately to get 30 vectors.
Allpunctuation marks should be filtered while thefeature words are collected.
The input layer ofneural network model is regarded as 30 neuralnodes.
The triple-layer neural network adopts theinspired S function.
From that, the number of neuralnodes in hidden layer is defined as 12 on the basisof experimental contrast, and 3 neural nodes inoutput layer.
Hence, the structure of model is 30 ?12 ?
3, and the precision of differential training isdefined as 0.3 based on the experimental contrast.3.5 The test and training of modelThe experimental corpus appeared in front are123,882 lines.
It is divided to three parts accordingto the demand of experiment, C1 (15,000 lines), C2(60,000 lines), and C3 (105,000 lines).
The opentest corpus is 18,882 lines.Table 3.3 tells us that there is a great disparitybetween the sample numbers of differentambiguous words in the experimental corpus of thesame class.
And the distribution of differentmeanings is not even for same ambiguous word.For the trained neural network has the good abilityof differentiation for each word, the number oftraining sample should be about equal to each otherfor each meaning.
So this experiment selects theleast training samples.
For example, there are 200samples of the first meaning in training corpus, thesecond 400, and the third 500.
To balance the input,each meaning merely has 200 samples to be electedfor training.Three groups of training corpus can train 3 neuralnetworks possessing different vectors for everyambiguous word and make the unclose and opentest for these networks separately.4 The result of experimentIn order to analyze the effect that the extent oftraining corpus influences the meaning distinguishability of neural network, this article trains themodel of neural network using the experimentalcorpus individually, C1, C2 and C3, and makes theclose and open test for 6 ambiguities separately.The close test means the corpus are same in testand training.The experiment is divided into two groupsaccording to the extracting method of contextualfeature words.4.1 The first experiment oneTable 4.1 shows the result of the first experimentwhich extracts the contextual feature words usingthe method of ?
?5?+ 5?.In addition, the first experiment investigates thatthe extent of training corpus (the number of trainingsamples big or small) influences the ability todistinguish the models.
The result of test for 6close-test open-test pseudo-words accuracy Training set accuracyTrainingsetW1 0.8800 C2 0.8951 C3W2 0.8867 C2 0.8775 C2W3 0.8652 C3 0.8574 C3W4 0.8532 C3 0.8687 C3W5 0.8769 C3 0.8745 C3W6 0.8868 C2 0.8951 C3Table 4.1 The contrast chat of experimental resultfor six ambiguitiesambiguities is showed in table 4.2 (close test), table4.3 (open test), and table 4.4.
Considering thelength of this article, table 4.2 and table 4.3 showsthe detailed data, and table 4.4 is brief.Training set pseudo-words C1 C2 C3sense 1 0.9226 0.8169 0.8991sense 2 0.5513 0.8017 0.6872sense 3 0.8027 0.9564 0.9510W1average 0.7589 0.8800 0.8720sense 1 0.8121 0.8780 0.9377sense 2 0.8389 0.8968 0.8804sense 3 0.7248 0.8856 0.8370W6average 0.7919 0.8868 0.8850Table 4.2 The result of W1 and W6 in close testunder the different training corpus4.2 The second experimentThe second experiment investigates emphaticallythe effect that the method to collect the featurewords influences the ability to distinguish BPmodel.Training set pseudo-words C1 C2 C3sense 1 0.9019 0.7827 0.8942sense 2 0.4607 0.8097 0.7175sense 3 0.7792 0.9500 0.9515W1average 0.7573 0.8798 0.8951sense 1 0.8233 0.9093 0.9535sense 2 0.8799 0.8182 0.8604sense 3 0.7278 0.8544 0.8038W6average 0.8259 0.8683 0.8951Table 4.3 The result of W1 and W6 in open testunder the different training corpusThere are many methods adopted in thisexperiment, including (?10?+ 10),?
?3?+ 3?,?
?3?+ 7?,?
?7?+ 3?,?
?4?+ 6?and?
?6?+ 4?.
Merelythe ambiguous words W1 and W6 are regarded as theTraining set pseudo-words C1 C2 C3W2 0.6628 0.8867 0.8772W3 0.6695 0.8453 0.8652W4 0.7414 0.8452 0.8532W5close0.8283 0.8537 0.8769W2 0.7287 0.8613 0.8700W3 0.8085 0.8384 0.8574W4 0.7920 0.8655 0.8687W5open0.8288 0.8775 0.8745Table 4.4 The contrast chart of experimentalresult for four ambiguitiesTable 4.5 the experimental result under differentfeature collecting methodexperimental objects in this group experiment.
Seetable 4.5 for the correct percentage of WSD.5.
Analysis and discussionSee table 5.1 for the number of experimentalcorpus samples in experiment.According to the table 3.3 and 5.1, the frequency ofthe each meaning (morpheme) of ambiguous wordshowing in corpus is quite different.
That accordswith the distribution of the every meanings ofambiguous word.
However, there is one differentpoint that the frequency of the each meaning ofambiguous word is rather high (that is the outcomeselected by morpheme.).
In other words, there aremany examples showing for the each meaning ofambiguous word in training and test corpus.
On thecontrast, the difference of frequency is quiteaccuracy pseudo-wordsfeaturecollectingmethod close-test open-testTrainingset??10?+10?
0.8897 0.8685 C1??3?+3?
0.7917 0.7176 C2??4?+6?
0.8600 0.8888 C3??6?+4?
0.8797 0.8938 C2??3?+7?
0.8514 0.8827 C3W1??7?+3?
0.8431 0.8825 C3??10?+10?
0.9031 0.8962 C2??3?+3?
0.8487 0.8460 C2??4?+6?
0.8982 0.8873 C3??6?+4?
0.8480 0.8772 C2??3?+7?
0.8669 0.8359 C3W6??7?+3?
0.8982 0.8895 C3pseudo-wordsMorphemeIDsamplenumberpseudo-wordsMorphemeIDsamplenumber34466 1040 84323 59171345 662 12751 484W131796 2101W452915 82971072 1296 53333 27478031 1043 29053 1153W248469 315W575941 2387464 4389 39945 43077375 469 71335 308W323077 865W651491 158Table 5.1 The number of experimental samplesobvious for the each meaning of real ambiguousword, because some meanings are used in orallanguage.
But that never or seldom appears inexperimental corpus.The statistics can uncover this linguisticphenomenon.
We find that the meaning of the mostpercentage of ambiguous word showing in thecorpus is 83.54% on the whole percentage of eachmeaning.
That illustrates the distribution of eachmeaning has a great disparity in real ambiguousword.
Seeing that condition, to differentiate themeaning of ambiguous word is harder than that ofreal ambiguous word absolutely.5.1 The analysis and discussion of the firstexperimentTable 4.1 records the results of close and opentests in detail and the training materials to get theseresults.Seeing from the experimental results, the correctpercentage reaches 89.51% most (ambiguous wordW1 and W6) in open test of WSD, and 85.74% theleast (ambiguous word W3).The relationship of correct percentage and theextent of training corpus can be deduced from theexperimental results of table 4.2, 4.3 and 4.4.The larger the extent of training corpus (thenumber of training sample?, the larger the result ofclose test.
It is obvious to see that from C1 to C2.From C2 to C3 one or two experimental resultsfluctuate more or less.With the growing of training sample, theexperimental results of open test increase steadily,except ambiguous word W2 (a little bit difference).The experimental data prove the growing oftraining samples rise the correct percentage.However, when the rising reaches to a certaindegree, more rising is not good for the improvementof model.
What?s more, the effect of noise is moreand more remarkable.
That decreases the model?sability of differentiation in a certain degree.
On theother hand, after the growing of training corpus, thelinguistic phenomenon around ambiguities is richerand richer, more and more complex.
That makes itharder to determine the meaning.5.2 The analysis and discussion of the secondexperimentThis article emphasizes on the collecting methodof contextual feature words in experiment two, inother words, the effect that the different values of Mand N influence the model of BP network.
Theexperimental results (table 4.1 and 4.5) tell us thatthe context windows influence the correctpercentage heavily.
The correct percentageincreases almost by leaps and bounds from (?3?+ 3)to??5?+5?.
The discrepancy is obvious despiteclose test or open test.
The correct percentageincrease again to?
?10?+ 10?, in which the close testof ambiguous word W6 is more than 90% and89.62% the close test, with the exception of W1which open test is slightly special.
That illustratesthe more widely the context windows open, themore the effective information is caught to benefitthe WSD more.Comparing the four feature methods of collection,including ?
?3?+ 7?,?
?7?+ 3?,?
?4?+ 6?and?
?6?+ 4?
with?
?5?+ 5?, the number of featurewords besides the ambiguous word is various andthe experimental results (table 4.1 and 4.5) are notsame, although the windows are same.
Among them,the correct percentage of ?
?5?+ 5?is the highest.And that of?
?4?+ 6?and?
?6?+ 4?is better thanthat of?
?3?+ 7?and?
?7?+ 3?a bit.
That showsthe more balanceable the feature words besidesambiguous word, the more advantageous to judgemeaning, and the better the experimental results.In addition, some experimental results of opentest are better than that of close test.
The mainreason is the experimental corpus of open test issmaller than training corpus.
So the contextualmeanings of ambiguous word in experimentalcorpus are rather explicit.
Thereby, that explainswhy should be this kind of experimental result.5.3 ConclusionsConsidering the analysis of experimental data,the conclusions are as following:First, the artificial model of neural networkestablished in this article has good ability ofdifferentiation for Chinese meaning.Next, higher correct percentage of WSD stemsfrom the large enough corpus.At last, the larger the windows of contextualfeature words, the more the effective information.At the same time, the more balanceable the numberof feature words beside the ambiguous word, themore beneficial that for WSD.6 Concluding remarksAlthough the BP network is a classified modelapplied extensively, the report of research on WSDabout it is seldom.
Especially the report about theChinese WSD is less, and only one report (Zhang,2001) is available in internal reports.Zhang (2001) uses 96 semantic classes to insteadthe all words in training corpus according to theTongyiciCilin.
The input model is the codes ofsemantic class of contextual words and ambiguities.The experiment of WSD merely makes for onephrase ?
?
(cai2liao4) in this document and thecorrect percentage of open test is 80.4%.
?
?has 3 meanings and that is similar to theambiguities structured in my article.Using BP for Chinese WSD, the key point anddifficulty are on the determination of input model.The performance of input model may influence theconstruction of BP network and the output resultdirectly.We make the experiment on the input of BPnetwork many times and finally find the inputmodel introduced as above (table 3.1) which testresult is satisfied.Acknowledgements This work was supported bythe National Natural Science Foundation of China(Grant No.
60203020).ReferenceLu Song, Bai Shuo, et al 2001.
Supervised wordsense disambiguation bassed on Vector SpaceModel, Journal of Comouter Research &Development, 38(6): 662-667.Pedersen.
2001.
Lexical semantic ambiguous wordresolution with bigram-based decision trees, InProceedings of the Second International Conferenceon Intelligent Text Processing and ComputationalLinguistics, pages 157-168, Mexico City, February.Escudero, G., Marquez,L., et al 2000.
Naive Bayesand examplar based approaches to word sensedisambiguation revisited.
In Proceedings of the 14thEuropear Conference on Artificial Intelligence,ECAI.Adam,L.B.
1996.
A maximum entropy approach tonatural language proceeding.
ComputationalLinguistics, 22(1):39-71.Li, J.
Z.
1999.
An improved maximum languageand its application.
Journal of software, 3:257-263.Yarowsky, D. Word sense disambiguation usingstatistical models of Roget?s categories trained onlarge corpora.
In: Zampolli, A., ed.
ComputationLinguistic?92.
Nantas: Association forComputational Linguistics, 1992.
454~460.Hinrich Sch?tze, 1998.
Automatic word sensediscrimination.
Computational Linguistics, 24(1):97-124.Lu Song., Bai Shuo.
2002.
An unsupervisedapproach to word sense disambiguation based onsense-word in vector space model.
Journal ofSoftware.
13(06):1082-08Hinrich Sch?tze.
1992.
Context space.
In AAAIFall Symposium on Probabilistic Approaches toNatural Language, pages 113?120, Cambridge,MA.Lu Song Bai Shuo.
2001.
Quantitative Analysis ofContext Field.
In Natural Language Processing,CHINESEJ.COMPUTERS, 24(7) , 742-747Zhang Guoqing, Zhang Yongkui.
2001.
ANeural-network Based Word Sense DisambiguationMethod.
Computer Engineering, 27(12).
