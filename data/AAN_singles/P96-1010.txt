Combining Trigram-based and Feature-based Methods forContext-Sensitive Spelling CorrectionAndrew R .
Go ld ing  and  Yves  SchabesMi tsub ish i  E lec t r i c  Research  Laborator ies201 BroadwayCambr idge ,  MA 02139{golding, schabes}@merl, comAbst ractThis paper addresses the problem of cor-recting spelling errors that result in valid,though unintended words (such as peaceand piece, or quiet and quite) and alsothe problem of correcting particular wordusage errors (such as amount and num-ber, or among and between).
Such cor-rections require contextual information andare not handled by conventional spellingprograms such as Unix spell.
First, weintroduce a method called Trigrams thatuses part-of-speech trigrams to encode thecontext.
This method uses a small num-ber of parameters compared to previousmethods based on word trigrams.
How-ever, it is effectively unable to distinguishamong words that have the same partof speech.
For this case, an alternativefeature-based method called Bayes per-forms better; but Bayes is less effectivethan Trigrams when the distinction amongwords depends on syntactic onstraints.
Ahybrid method called Tribayes is then in-troduced that combines the best of the pre-vious two methods.
The improvement inperformance of Tribayes over its compo-nents is verified experimentally.
Tribayes isalso compared with the grammar checker inMicrosoft Word, and is found to have sub-stantially higher performance.1 In t roduct ionSpelling correction has become a very common tech-nology and is often not perceived as a problemwhere progress can be made.
However, conventionalspelling checkers, such as Unix spell, are concernedonly with spelling errors that result in words thatcannot be found in a word list of a given language.One analysis has shown that up to 15% of spellingerrors that result from elementary t pographical er-rors (character insertion, deletion, or transposition)yield another valid word in the language (Peterson,1986).
These errors remain undetected by tradi-tional spelling checkers.
In addition to typographicalerrors, words that can be easily confused with eachother (for instance, the homophones peace and piece)also remain undetected.
Recent studies of actual ob-served spelling errors have estimated that overall,errors resulting in valid words account for anywherefrom 25% to over 50% of the errors, depending onthe application (Kukich, 1992).We will use the term context-sensitive spelling cor-rection to refer to the task of fixing spelling errorsthat result in valid words, such as:(1) * Can I have a peace of cake?where peace was typed when piece was intended.The task will be cast as one of lexical disambigua-tion: we are given a predefined collection of confu-sion sets, such as {peace,piece}, {than, then}, etc.,which circumscribe the space of spelling errors tolook for.
A confusion set means that each wordin the set could mistakenly be typed when anotherword in the set was intended.
The task is to predict,given an occurrence of a word in one of the confusionsets, which word in the set was actually intended.Previous work on context-sensitive spelling cor-rection and related lexical disambiguation tasks hasits limitations.
Word-trigram ethods (Mays, Dam-erau, and Mercer, 1991) require an extremely largebody of text to train the word-trigram odel; evenwith extensive training sets, the problem of sparsedata is often acute.
In addition, huge word-trigramtables need to be available at run time.
More-over, word trigrams are ineffective at capturing long-distance properties such as discourse topic and tense.Feature-based approaches, uch as Bayesian clas-sifters (Gale, Church, and Yarowsky, 1993), deci-sion lists (Yarowsky, 1994), and Bayesian hybrids(Golding, 1995), have had varying degrees of suc-cess for the problem of context-sensitive spellingcorrection.
However, we report experiments thatshow that these methods are of limited effective-ness for cases such as {their, there, they're} and{than, then}, where the predominant distinction tobe made among the words is syntactic.71Confusion set Train Test Most freq.
Basetheir, there, they're 3265 850than, then 2096 514its, it's 1364 366your, you're 750 187begin, being 559 146passed, past 307 74quiet, quite 264 66weather, whether 239 61accept, except 173 50lead, led 173 49cite, sight, site 115 34principal, principle 147 34raise, rise 98 39affect, effect 178 49peace, piece 203 50country, county 268 62amount, number 460 123among, between 764 186their 56.8than 63.4its 91.3your 89.3being 93.2past 68.9quite 83.3whether 86.9except 70.0led 46.9sight 64.7principle 58.8rise 64.1effect 91.8peace 44.0country 91.9number 71.5between 71.5Table 1: Performance of the baseline method for 18 confusion sets.
"Train" and "Test" give the numberof occurrences of any word in the confusion set in the training and test corpora.
"Most freq."
is the wordin the confusion set that occurred most often in the training corpus.
"Base" is the percentage of correctpredictions of the baseline system on the test corpus.In this paper, we first introduce a method calledTrigrams that uses part-of-speech trigrams to en-code the context.
This method greatly reduces thenumber of parameters compared to known methods,which are based on word trigrams.
This methodalso has the advantage that training can be doneonce and for all, and quite manageably, for all con-fusion sets; new confusion sets can be added laterwithout any additional training.
This feature makesTrigrams a very easily expandable system.Empirical evaluation of the trigram methoddemonstrates that it performs well when the wordsto be discriminated have different parts of speech,but poorly when they have the same part of speech.In the latter case, it is reduced to simply guessingwhichever word in the confusion set is the most com-mon representative of its part-of-speech class.We consider an alternative method, Bayes, aBayesian hybrid method (Golding, 1995), for thecase where the words have the same part of speech.We confirm experimentally that Bayes and Trigramshave complementary performance, Trigrams beingbetter when the words in the confusion set have dif-ferent parts of speech, and Bayes being better whenthey have the same part of speech.
We introducea hybrid method, Tribayes, that exploits this com-plementarity by invoking each method when it isstrongest.
Tribayes achieves the best accuracy ofthe methods under consideration i all situations.To evaluate the performance of Tribayes with re-spect to an external standard, we compare it to thegrammar checker in Microsoft Word.
Tribayes isfound to have substantially higher performance.This paper is organized as follows: first we presentthe methodology used in the experiments.
We thendiscuss the methods mentioned above, interleavedwith experimental results.
The comparison with Mi-crosoft Word is then presented.
The final sectionconcludes.2 Methodo logyEach method will be described in terms of its op-eration on a single confusion set C = {Wl, .
.
.
,  w,};that is, we will say how the method disambiguatesoccurrences of words wl through wn.
The methodshandle multiple confusion sets by applying the sametechnique to each confusion set independently.Each method involves a training phase and atest phase.
We trained each method on 80%(randomly selected) of the Brown corpus (Ku6eraand Francis, 1967) and tested it on the remain-ing 20%.
All methods were run on a collection of18 confusion sets, which were largely taken fromthe list of "Words Commonly Confused" in theback of Random House (Flexner, 1983).
The con-fusion sets were selected on the basis of beingfrequently-occurring in Brown, and representing avariety of types of errors, including homophone con-fusions (e.g., {peace, piece}) and grammatical mis-takes (e.g., {among, between}).
A few confusion setsnot in Random House were added, representing ty-pographical errors (e.g., {begin, being}).
The confu-sion sets appear in Table 1.723 Base l ineAs an indicator of the difficulty of the task, we com-pared each of the methods to the method which ig-nores the context in which the word occurred, andjust guesses based on the priors.Table 1 shows the performance of the baselinemethod for the 18 confusion sets.4 T r ig ramsMays, Damerau, and Mercer (1991) proposed aword-trigram method for context-sensitive spellingcorrection based on the noisy channel model.
Sincethis method is based on word trigrams, it requires anenormous training corpus to fit all of these parame-ters accurately; in addition, at run time it requiresextensive system resources to store and manipulatethe resulting huge word-trigram table.In contrast, the method proposed here uses part-of-speech trigrams.
Given a target occurrence of aword to correct, it substitutes in turn each word inthe confusion set into the sentence.
Por each substi-tution, it calculates the probability of the resultingsentence.
It selects as its answer the word that givesthe highest probability.More precisely, assume that the word wh occursin a sentence W = wl .
.
.Wk.
.
.wn,  and that w~ is aword we are considering substituting for it, yieldingsentence W I.
Word w~ is then preferred over wk iffP(W') > P(W), where P(W) and P(W') are theprobabilities of sentences W and W f respectively.
1We calculate P(W) using the tag sequence of W asan intermediate quantity, and summing, over all pos-sible tag sequences, the probability of the sentencewith that tagging; that is:P(W) = ~ P(W, T)Twhere T is a tag sequence for sentence W.The above probabilities are estimated as is tra-ditionally done in trigram-based part-of-speech tag-ging (Church, 1988; DeRose, 1988):P(W,T) = P(WIT)P(T ) (1)= HP(wi \ [ t i )  HP(t, lt,_2t,_l)(2)i iwhere T = tl ...tn, and P(ti\]tl-2ti-1) is the prob a-bility of seeing a part-of-speech tag tl given the twopreceding part-of-speech tags ti-2 and ti-1.
Equa-tions 1 and 2 will also be used to tag sentencesW and W ~ with their most likely part-of-speech se-quences.
This will allow us to determine the tag that1To enable fair comparisons between sequences ofdif-ferent length (as when considering maybe and may be),we actually compare the per-word geometric mean of thesentence probabilities.
Otherwise, the shorter sequencewill usually be preferred, as shorter sequences tend tohave higher probabilities than longer ones.would be assigned to each word in the confusion setwhen substituted into the target sentence.Table 2 gives the results of the trigram method(as well as the Bayesian method of the next section)for the 18 confusion sets.
2 The results are brokendown into two cases: "Different tags" and "Sametags".
A target occurrence is put in the latter iff allwords in the confusion set would have the same tagwhen substituted into the target sentence.
In the"Different ags" condition, Trigrams generally doeswell, outscoring Bayes for all but 3 confusion sets - -and in each of these cases, making no more than 3errors more than Bayes.In the "Same tags" condition, however, Trigramsperforms only as well as Baseline.
This follows fromEquations 1 and 2: when comparing P(W) andP(WI), the dominant erm corresponds to the mostlikely tagging; and in this term, if the target wordwk and its substitute w~ have the same tag t, thenthe comparison amounts to comparing P(wk \[/) andP(w~lt ).
In other words, the decision reduces towhich of the two words, Wk and w~, is the morecommon representative of part-of-speech class t. 35 BayesThe previous ection showed that the part-of-speechtrigram method works well when the words in theconfusion set have different parts of speech, but es-sentially cannot distinguish among the words if theyhave the same part of speech.
In this case, a moreeffective approach is to learn features that char-acterize the different contexts in which each wordtends to occur.
A number of feature-based methodshave been proposed, including Bayesian classifiers(Gale, Church, and Yarowsky, 1993), decision lists(Yarowsky, 1994), Bayesian hybrids (Golding, 1995),and, more recently, a method based on the Winnowmultiplicative weight-updating algorithm (Goldingand Roth, 1996).
We adopt the Bayesian hybridmethod, which we will call Bayes, having experi-mented with each of the methods and found Bayes tobe among the best-performing for the task at hand.This method has been described elsewhere (Golding,1995) and so will only be briefly reviewed here; how-ever, the version used here uses an improved smooth-ing technique, which is mentioned briefly below.~In the experiments reported here, the trigrammethod was run using the tag inventory derived from theBrown corpus, except hat a handful of common func-tion words were tagged as themselves, namely: except,than, then, to, too, and whether.3 In a few cases, however, Trig'rams does not get ex-actly the same score as Baseline.
This can happen whenthe words in the confusion set have more than one tagin common; e.g., for (affect, effect}, the words can bothbe norms or verbs.
Trigrams may then choose differ-ently when the words are tagged as nouns versus verbs,whereas Baseline makes the same choice in all cases.73Confusion settheir, there, they'rethan, thenits, it'syour, you'rebegin, beingpassed, pastquiet, quiteweather, whetheraccept, exceptlead, ledcite, sight, siteprincipal, principleraise, riseaffect, effectpeace, piececountry, countyamount, numberamong, betweenBreak-downI00100100100100I00100100100I00I0029862000Different agsSystem scoresBase T B56.8 97.6 94.463.4 94.9 93.291.3 98.1 95.989.3 98.9 89.893.2 97.3 91.868.9 95.9 89.283.3 95.5 89.486.9 93.4 96.770.0 82.0 88.046.9 83.7 79.664.7 70.6 73.50.0 100.0 70.0100.0 100.0 100.0100.0 100.0 66.70.0 100.0 100.0Break-down000.0000000071929498100100100Same tagsSystem scoresBase T B83.3 83.3 91.761.1 61.1 72.291.3 93.5 97.844.9 42.9 89.891.9 91.9 85.571.5 73.2 82.971.5 71.5 75.3Table 2: Performance of the component methods, Baseline (Base), Trigrams (T), and Bayes (B).
Systemscores are given as percentages of correct predictions.
The results are broken down by whether or not allwords in the confusion set would have the same tagging when substituted into the target sentence.
The"Breakdown" columns how the percentage of examples that fall under each condition.Bayes uses two types of features: context wordsand collocations.
Context-word features test for thepresence of a particular word within +k words ofthe target word; collocations test for a pattern ofup to ~ contiguous words and/or part-of-speech tagsaround the target word.
Examples for the confusionset {dairy, diary} include:(2) milk within +10 words(3) in POSS-DETwhere (2) is a context-word feature that tends to im-ply dairy, while (3) is a collocation implying diary.Feature (3) includes the tag POSS-I)ET for possessivedeterminers (his, her, etc.
), and matches, for exam-ple, the sequence in his 4 in:(4) He made an entry in his diary.Bayes learns these features from a training corpusof correct text.
Each time a word in the confusionset occurs in the corpus, Bayes proposes every fea-ture that matches the context - -  one context-wordfeature for every distinct word within +k words ofthe target word, and one collocation for every way of4A tag is taken to match a word in the sentence iffthe tag is a member of the word's set of possible part-of-speech tags.
Tag sets are used, rather than actual tags,because it is in general impossible to tag the sentenceuniquely at spelling-correction time, as the identity ofthe target word has not yet been established.expressing a pattern of up to ~ contiguous elements.After working through the whole training corpus,Bayes collects and returns the set of features pro-posed.
Pruning criteria may be applied at this pointto eliminate features that are based on insufficientdata, or that are ineffective at discriminating amongthe words in the confusion set.At run time, Bayes uses the features learned dur-ing training to correct the spelling of target words.Let jr  be the set of features that match a particu-lar target occurrence.
Suppose for a moment hat wewere applying a naive Bayesian approach.
We wouldthen calculate the probability that each word wi inthe confusion set is the correct identity of the targetword, given that we have observed features 9r, usingBayes' rule with the independence assumption:P(w,l~') = P(flw,) P(5)where each probability on the right-hand side is cal-culated by a maximum-likelihood estimate (MLE)over the training set.
We would then pick as our an-swer the wi with the highest P(wiI.T" ).
The methodpresented here differs from the naive approach intwo respects: first, it does not assume independenceamong features, but rather has heuristics for de-tecting strong dependencies, and resolving them bydeleting features until it is left with a reduced set .T "~74of (relatively) independent features, which are thenused in place of ~" in the formula above.
Second,to estimate the P(flwi) terms, rather than using asimple MLE, it performs smoothing by interpolat-ing between the MLE of P(flwi) and the MLE ofthe unigram probability, P(f).
These enhancementsgreatly improve the performance of Bayes over thenaive Bayesian approach.The results of Bayes are shown in Table 2.
5 Gener-ally speaking, Bayes does worse than Trigrams whenthe words in the confusion set have different partsof speech.
The reason is that, in such cases, the pre-dominant distinction to be made among the wordsis syntactic; and the trigram method, which bringsto bear part-of-speech knowledge for the whole sen-tence, is better equipped to make this distinctionthan Bayes, which only tests up to two syntactic el-ements in its collocations.
Moreover, Bayes' use ofcontext-word features is arguably misguided here, ascontext words pick up differences in topic and tense,which are irrelevant here, and in fact tend to degradeperformance by detecting spurious differences.
In afew cases, such as {begin, being}, this effect is enoughto drive Bayes slightly below Baseline.
6For the condition where the words have the samepart of speech, Table 2 shows that Bayes almost al-ways does better than Trigrams.
This is because, asdiscussed above, Trigrams is essentially acting likeBaseline in this condition.
Bayes, on the other hand,learns features that allow it to discriminate amongthe particular words at issue, regardless of their partof speech.
The one exception is {country, county},for which Bayes scores somewhat below Baseline.This is another case in which context words actu-ally hurt Bayes, as running it without context wordsagain improved its performance to the Baseline level.6 T r ibayesThe previous ections demonstrated the complemen-tarity between Trigrams and Bayes: Trigrams worksbest when the words in the confusion set do not allhave the same part of speech, while Bayes works bestwhen they do.
This complementarity leads directlyto a hybrid method, Tribayes, that gets the best ofeach.
It applies Trigrams first; in the process, it as-certains whether all the words in the confusion setwould have the same tag when substituted into the5For the experiments reported here, Bayes was con-figured as follows: k (the half-width of the window ofcontext words) was set to 10; ?
(the maximum length of acollocation) was set to 2; feature strength was measuredusing the reliability metric; pruning of collocations attraining time was enabled; and pruning of context wordswas minimal - -  context words were pruned only if theyhad fewer than 2 occurrences ornon-occurrences.eWe confirmed this by running Bayes without contextwords (i.e., with collocations only).
Its performance wasthen always at or above Baseline.75target sentence.
If they do not, it accepts the answerprovided by Trigrams; if they do, it applies Bayes.Two points about the application of Bayes in thehybrid method: first, Bayes is now being asked todistinguish among words only when they have thesame part of speech.
It should be trained accord-ingly - -  that is, only on examples where the wordshave the same part of speech.
The Bayes componentof the hybrid will therefore be trained on a subsetof the examples that would be used for training thestand-alone version of Bayes.The second point about Bayes is that, like Tri-grams, it sometimes makes uninformed ecisions - -decisions based only on the priors.
For Bayes, thishappens when none of its features matches the targetoccurrence.
Since, for now, we do not have a good"third-string" algorithm to call when both Trigramsand Bayes fall by the wayside, we content ourselveswith the guess made by Bayes in such situations.Table 3 shows the performance of Tribayes com-pared to its components.
In the "Different ags" con-dition, Tribayes invokes Trigrams, and thus scoresidentically.
In the "Same tags" condition, Tribayesinvokes Bayes.
It does not necessarily score thesame, however, because, as mentioned above, it istrained on a subset of the examples that stand-aloneBayes is trained on.
This can lead to higher or lowerperformance - - higher because the training exam-ples are more homogeneous (representing only caseswhere the words have the same part of speech); lowerbecause there may not be enough training examplesto learn from.
Both effects show up in Table 3.Table 4 summarizes the overall performance of allmethods discussed.
It can be seen that Trigramsand Bayes each have their strong points.
Tribayes,however, achieves the maximum of their scores, byand large, the exceptions being due to cases whereone method or the other had an unexpectedly owscore (discussed in Sections 4 and 5).
The confusionset {raise, rise} demonstrates (albeit modestly) theability of the hybrid to outscore both of its compo-nents, by putting together the performance of thebetter component for both conditions.7 Compar i son  w i th  M ic roso f t  WordThe previous section evaluated the performance ofTribayes with respect o its components, and showedthat it got the best of both.
In this section,we calibrate this overall performance by compar-ing Tribayes with Microsoft Word (version 7.0), awidely used word-processing system whose grammarchecker epresents the state of the art in commercialcontext-sensitive spelling correction.Unfortunately we cannot evaluate Word using"prediction accuracy" (as we did above), as we donot always have access to the system's predictions - -sometimes it suppresses its predictions in an effortto filter out the bad ones.
Instead, in this sectionConfusion set Different ags Same tagsBreak- System scores Break- System scoresdown T TB down B TBtheir, there, they're 100 97.6 97.6 0than, then 100 94.9 94.9 0its, it's 100 98.1 98.1 0your, you're 100 98.9 98.9 0begin, being 100 97.3 97.3 0passed, past 100 95.9 95.9 0quiet, quite 100 95.5 95.5 0weather, whether 100 93.4 93.4 0accept, except 100 82.0 82.0 0lead, led 100 83.7 83.7 0cite, sight, site 100 70.6 70.6 0principal, principle 29 100.0  100.0 71 91.7 83.3raise, rise 8 100.0  100.0 92 72.2 75.0affect, effect 6 100.0 100.0 94 97.8 95.7peace, piece 2 100.0 100.0 98 89.8 89.8country, county 0 100 85.5 85.5amount, number 0 100 82.9 82.9among, between 0 100 75.3 75.3Table 3: Performance ofthe hybrid method, Tribayes (TB), as compared with Trigrams (T) and Bayes (B).System scores are given as percentages of correct predictions.
The results are broken down by whether ornot all words in the confusion set would have the same tagging when substituted into the target sentence.The "Breakdown" columns give the percentage ofexamples under each condition.Confusion set System scoresBase T B TBtheir, there, they'rethan, thenits, it'syour, you'rebegin, beingpassed, pastquiet, quiteweather, whetheraccept, exceptlead, ledcite, sight, siteprincipal, principleraise, riseaffect, effectpeace, piececountry, countyamount, numberamong, between56.8 97.6 94.4 97.663.4 94.9 93.2 94.991.3 98 .1  95.9 98.189.3 98.9 89.8 98.993.2 97.3 91.8 97.368.9 95.9 89.2 95.983.3 95.5 89.4 95.586.9 93.4 96.7 93.470.0 82.0 88.0 82.046.9 83.7 79.6 83.764.7 70.6 73.5 70.658.8 88.2 85.3 88.264.1 64 .1  74.4 76.991.8 93.9 95.9 95.944.0 44.0 90.0 90.091.9 91.9 85.5 85.571.5 73.2 82.9 82.971.5 71.5 75.3 75.3Table 4: Overall performance ofall methods: Baseline (Base), TrigramsSystem scores are given as percentages ofcorrect predictions.
(T), Bayes (B), and Tribayes (TB).76we will use two parameters to evaluate system per-formance: system accuracy when tested on correctusages of words, and system accuracy on incorrectusages.
Together, these two parameters give a com-plete picture of system performance: the score oncorrect usages measures the system's rate of falsenegative rrors (changing a right word to a wrongone), while the score on incorrect usages measuresfalse positives (failing to change a wrong word to aright one).
We will not attempt to combine these twoparameters into a single measure of system "good-ness", as the appropriate combination varies for dif-ferent users, depending on the user's typing accuracyand tolerance of false negatives and positives.The test sets for the correct condition are the sameones used earlier, based on 20% of the Brown corpus.The test sets for the incorrect condition were gener-ated by corrupting the correct test sets; in particu-lar, each correct occurrence of a word in the confu-sion set was replaced, in turn, with each other wordin the confusion set, yielding n - 1 incorrect occur-rences for each correct occurrence (where n is thesize of the confusion set).
We will also refer to theincorrect condition as the corrupted condition.To run Microsoft Word on a particular test set,we started by disabling error checking for all errortypes except those needed for the confusion set atissue.
This was done to avoid confounding effects.For {their, there, they're}, for instance, we enabled"word usage" errors (which include substitutions oftheir for there, etc.
), but we disabled "contractions"(which include replacing they're with they are).
Wethen invoked the grammar checker, accepting everysuggestion offered.
Sometimes errors were pointedout but no correction given; in such cases, weskipped over the error.
Sometimes the suggestionsled to an infinite loop, as with the sentence:(5) Be sure it's out when you leave.where the system alternately suggested replacing it'swith its and vice versa.
In such cases, we acceptedthe first suggestion, and then moved on.Unlike Word, Tribayes, as presented above, ispurely a predictive system, and never suppresses itssuggestions.
This is somewhat of a handicap in thecomparison, as Word can achieve higher scores in thecorrect condition by suppressing its weaker sugges-tions (albeit at the cost of lowering its scores in thecorrupted condition).
To put Tribayes on an equalfooting, we added a postprocessing step in which ituses thresholds to decide whether to suppress its sug-gestions.
A suggestion is allowed to go through iffthe ratio of the probability of the word being sug-gested to the probability of the word that appearedoriginally in the sentence is above a threshold.
Theprobability associated with each word is the per-word sentence probability in the case of Trigrams,or the conditional probability P(wi\[~) in the caseof Bayes.
The thresholds are set in a preprocessing77phase based on the training set (80% of Brown, inour case).
A single tunable parameter controls howsteeply the thresholds are set; for the study here, thisparameter was set to the middle of its useful range,providing a fairly neutral balance between reducingfalse negatives and increasing false positives.The results of Word and Tribayes for the 18 confu-sion sets appear in Table 5.
Six of the confusion sets(marked with asterisks in the table) are not handledby Word; Word's scores in these cases are 100% forthe correct condition and 0% for the corrupted con-dition, which are the scores one gets by never mak-ing a suggestion.
The opposite behavior - -  alwayssuggesting a different word - -  would result in scoresof 0% and 100% (for a confusion set of size 2).
Al-though this behavior is never observed in its extremeform, it is a good approximation of Word's behaviorin a few cases, such as {principal, principle}, whereit scores 12% and 94%.
In general, Word achievesa high score in either the correct or the corruptedcondition, but not both at once.Tribayes compares quite favorably with Word inthis experiment.
In both the correct and corruptedconditions, Tribayes' scores are mostly higher (oftenby a wide margin) or the same as Word's; in thecases where they are lower in one condition, theyare almost always considerably higher in the other.The one exception is {raise, rise}, where Tribayesand Word score about the same in both conditions.8 Conclus ionSpelling errors that result in valid, though unin-tended words, have been found to be very commonin the production of text.
Such errors were thoughtto be too difficult to handle and remain undetectedin conventional spelling checkers.
This paper in-troduced Trigrams, a part-of-speech trigram-basedmethod, that improved on previous trigram meth-ods, which were word-based, by greatly reducingthe number of parameters.
The method was sup-plemented by Bayes, a method that uses contextfeatures to discriminate among the words in theconfusion set.
Trigrams and Bayes were shown tohave complementary strengths.
A hybrid method,Tribayes, was then introduced to exploit this com-plementarity by applying Trigrams when the wordsin the confusion set do not have the same part ofspeech, and Bayes when they do.
Tribayes therebygets the best of both methods, as was confirmed ex-perimentally.
Tribayes was also compared with thegrammar checker in Microsoft Word, and was foundto have substantially higher performance.Tribayes is being used as part of a grammar-checking system we are currently developing.
Weare presently working on elaborating the system'sthreshold model; scaling up the number of confusionsets that can be handled efficiently; and acquiringconfusion sets (or confusion matrices) automatically.Confusion set Tribayes Microsoft WordCorrect Corrupted Correct Corruptedtheir, there, they'rethan, thenits, it'syour, you'rebegin, beingpassed, pastquiet, quiteweather, whetheraccept, exceptlead, ledcite, sight, siteprincipal, principlerMse, riseaffect, effectpeace, piececountry, countyamount, numberamong, between99.4 87.697.9 85.899.5 92.198.9 98.4100.0 84.2100.0 92.4100.0 72.7100.0 65.690.0 70.087.8 81.6100.0 35.394.1 73.592.3 48.798.0 93.996.0 74.090.3 80.691.9 68.388.7 54.898.8 59.8100.0 22.296.2 73.098.9 79.1100.0 * 0.0 *37.8 86.5100.0 * 0.0 *100.0 * 0.0 *74.0 36.0100.0 * 0.0 *17.6 66.211.8 94.192.3 51.3100.0 77.636.0 88.0100.0 * 0.0 *100.0 * 0.0 *97.8 0.0Table 5: Comparison of Tribayes with Microsoft Word.
System scores are given for two test sets, one con-taining correct usages, and the other containing incorrect (corrupted) usages.
Scores are given as percentagesof correct answers.
Asterisks mark confusion sets that are not handled by Microsoft Word.Re ferencesChurch, Kenneth Ward.
1988.
A stochastic partsprogram and noun phrase parser for unrestrictedtext.
In Second Conference on Applied NaturalLanguage Processing, pages 136-143, Austin, TX.DeRose, S.J.
1988.
Grammatical category disam-biguation by statistical optimization.
Computa-tional Linguistics, 14:31-39.Flexner, S. B., editor.
1983.
Random HouseUnabridged Dictionary.
Random House, NewYork.
Second edition.Gale, William A., Kenneth W. Church, and DavidYarowsky.
1993.
A method for disambiguatingword senses in a large corpus.
Computers and theHumanities, 26:415-439.Golding, Andrew P~.
and Dan Roth.
1996.
Apply-ing Winnow to context-sensitive spelling correc-tion.
In Lorenza Saitta, editor, Machine Learning:Proceedings of the 13th International Conference,Bari, Italy.
To appear.Golding, Andrew R. 1995.
A Bayesian hybridmethod for context-sensitive spelling correction.In Proceedings of the Third Workshop on VeryLarge Corpora, pages 39-53, Boston, MA.Kukich, Karen.
1992.
Techniques for automaticMlycorrecting words in text.
ACM Computing Sur-veys, 24(4):377-439, December.78Kuaera, H. and W. N. Francis.
1967.
Computa-tional Analysis of Present-Day American English.Brown University Press, Providence, RI.Mays, Eric, bred J. Damerau, and Robert L. Mercer.1991.
Context based spelling correction.
Informa-tion Processing and Management, 27(5):517-522.Peterson, James L. 1986.
A note on undetectedtyping errors.
Communications of the ACM,29(7):633-637, July.Yarowsky, David.
1994.
Decision lists for lexi-cal ambiguity resolution: Application to accentrestoration in Spanish and French.
In Proceedingsof the 32nd Annual Meeting of the Associa-tion for Computational Linguistics, pages 88-95,Las Cruces, NM.
