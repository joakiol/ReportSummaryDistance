Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational NaturalLanguage Learning, pages 699?709, Jeju Island, Korea, 12?14 July 2012. c?2012 Association for Computational LinguisticsExploring Adaptor Grammars for Native Language IdentificationSze-Meng Jojo Wong Mark Dras Mark JohnsonCentre for Language TechnologyMacquarie UniversitySydney, NSW, Australia{sze.wong,mark.dras,mark.johnson}@mq.edu.auAbstractThe task of inferring the native language ofan author based on texts written in a secondlanguage has generally been tackled as a clas-sification problem, typically using as featuresa mix of n-grams over characters and part ofspeech tags (for small and fixed n) and un-igram function words.
To capture arbitrar-ily long n-grams that syntax-based approacheshave suggested are useful, adaptor grammarshave some promise.
In this work we investi-gate their extension to identifying n-gram col-locations of arbitrary length over a mix of PoStags and words, using both maxent and in-duced syntactic language model approaches toclassification.
After presenting a new, simplebaseline, we show that learned collocationsused as features in a maxent model performbetter still, but that the story is more mixed forthe syntactic language model.1 IntroductionThe task of inferring the native language of an authorbased on texts written in a second language ?
na-tive language identification (NLI) ?
has, since theseminal work of Koppel et al2005), been primarilytackled as a text classification task using supervisedmachine learning techniques.
Lexical features, suchas function words, character n-grams, and part-of-speech (PoS) n-grams, have been proven to be use-ful in NLI (Koppel et al2005; Tsur and Rappoport,2007; Estival et al2007).
The recent work of Wongand Dras (2011), motivated by ideas from SecondLanguage Acquisition (SLA), has shown that syn-tactic features ?
potentially capturing syntactic er-rors characteristic of a particular native language ?improve performance over purely lexical ones.PoS n-grams can be leveraged to characterise sur-face syntactic structures: in Koppel et al2005),for example, ungrammatical structures were approx-imated by rare PoS bigrams.
For the purpose of NLI,small n-gram sizes like bigram or trigram might notsuffice to capture sequences that are characteristic ofa particular native language.
On the other hand, anattempt to represent these with larger n-grams wouldnot just lead to feature sparsity problems, but alsocomputational efficiency issues.
Some form of fea-ture selection should then come into play.Adaptor grammars (Johnson, 2010), a hierarchi-cal non-parametric extension of PCFGs (and also in-terpretable as an extension of LDA-based topic mod-els), hold out some promise here.
In that initialwork, Johnson?s model learnt collocations of arbi-trary length such as gradient descent and cost func-tion, under a topic associated with machine learning.Hardisty et al2010) applied this idea to perspectiveclassification, learning collocations such as pales-tinian violence and palestinian freedom, the use ofwhich as features was demonstrated to help the clas-sification of texts from the Bitter Lemons corpus aseither Palestinian or Israeli perspective.Typically in NLI and other authorship attribu-tion tasks, the feature sets exclude content words,to avoid unfair cues due to potentially different do-mains of discourse.
In our context, then, what we areinterested in are ?quasi-syntactic collocations?
of ei-ther pure PoS (e.g.
NN IN NN) or a mixture of PoSwith function words (e.g.
NN of NN).
The partic-ular question of interest for this paper, then, is to699investigate whether the power of adaptor grammarsto discover collocations ?
specifically, ones of ar-bitrary length that are useful for classification ?
ex-tends to features beyond the purely lexical.We examine two different approaches in this pa-per.
We first utilise adaptor grammars for discoveryof high performing ?quasi-syntactic collocations?
ofarbitrary length as mentioned above and use themas classification features in a conventional maximumentropy (maxent) model for identifying the author?snative language.
In the second approach, we adopta grammar induction technique to learn a grammar-based language model in a Bayesian setting.
Thegrammar learned can then be used to infer the mostprobable native language that a given text writtenin a second language is associated with.
The latterapproach is actually closer to the work of Hardistyet al2010) using adaptor grammars for perspec-tive modeling, which inspired our general approach.This alternative approach is also similar in natureto the work of Bo?rschinger et al2011) in whichgrounded learning of semantic parsers was reducedto a grammatical inference task.The structure of the paper is as follows.
In Sec-tion 2, we review the existing work of NLI as wellas the mechanics of adaptor grammars along withtheir applications to classification.
Section 3 detailsthe supervised maxent classification of NLI withcollocation (n-gram) features discovered by adaptorgrammars.
The language model-based classifier isdescribed in Section 4.
Finally, we present a dis-cussion in Section 5 and follow with concluding re-marks.2 Related Work2.1 Native Language IdentificationMost of the existing research treats the task of na-tive language identification as a form of text classi-fication deploying supervised machine learning ap-proaches.The earliest notable work in this classificationparadigm is that of Koppel et al2005) using asfeatures function words, character n-grams, and PoSbigrams, together with some spelling errors.
Theirexperiments were conducted on English essays writ-ten by authors whose native language one of Bulgar-ian, Czech, French, Russian, or Spanish.
The cor-pus used is the first version of International Corpusof Learner English (ICLE).
Apart from investigatinglexical features, syntactic features (errors in particu-lar) were highlighted by Koppel et al2005) as po-tentially useful features, but they only explored thisby characterising ungrammatical structures with rarePoS bigrams: they chose 250 rare bigrams from theBrown corpus.Features for this task can include content wordsor not: Koppel et al2009), in reviewing work inthe general area of authorship attribution (includingNLI), discuss the (perhaps unreasonable) advantagethat content word features can provide, and com-ment that consequently they ?are careful .
.
.
to dis-tinguish results that exploit content-based featuresfrom those that do not?.
We will not be using con-tent words as features; we therefore note only ap-proaches to NLI that similarly do not use them.Following Koppel et al2005), Tsur and Rap-poport (2007) replicated their work and hypothe-sised that word choices in second language writingis highly influenced by the frequency of native lan-guage syllables.
They investigated this through mea-suring classification performance with only charac-ter bigrams as features.Estival et al2007) tackled the broader task ofdeveloping profiles of authors, including native lan-guage and various other demographic and psycho-metric author traits, across a smaller set of languages(English, Spanish and Arabic).
To this end, they de-ployed various lexical and document structure fea-tures.Wong and Dras (2011), starting from the Kop-pel et al2005) approach, explored the usefulnessof syntactic features in a broader sense in whichthey characterised syntactic errors with cross sec-tions of parse trees obtained from statistical parsers,both horizontal slices of the parse trees in the formof CFG production rules, and the feature schemataused in discriminative parse reranking (Charniakand Johnson, 2005); they also found that using thetop 200 PoS bigrams helped.
Their results on thesecond version of the ICLE corpus, across sevenlanguages (those of Koppel et alplus two Orien-tal languages, Chinese and Japanese) demonstratedthat syntactic features of these kinds lead to signifi-cantly better performance than the Koppel et alea-tures alone, with a top accuracy (on 5-fold cross-validation) of 77.75%.700Subsequently, Wong et al2011) exploredBayesian topic modeling (Blei et al2003; Griffithsand Steyvers, 2004) as a form of feature dimension-ality reduction technique to discover coherent latentfactors (?topics?)
that might capture predictive fea-tures for individual native languages.
Their topics,rather than the typical word n-grams, consisted ofbigrams over (only) PoS.
However, while there wassome evidence of topic cluster coherence, this didnot improve classification performance.The work of the present paper differs in that ituses Bayesian techniques to discover collocations ofarbitrary length for use in classification, over a mixof both PoS and function words, rather than for useas feature dimensionality reduction.2.2 Adaptor GrammarsAdaptor Grammars are a non-parametric extensionto PCFGs that are associated with a Bayesian in-ference procedure.
Here we provide an informalintroduction to Adaptor Grammars; Johnson et al(2007) provide a definition of Adaptor Grammars asa hierarchy of mixtures of Dirichlet (or 2-parameterPoisson-Dirichlet) Processes to which the readershould turn for further details.Adaptor Grammars can be viewed as extendingPCFGs by permitting the grammar to contain anunbounded number of productions; they are non-parametric in the sense that the particular produc-tions used to analyse a corpus depends on the cor-pus itself.
Because the set of possible productionsis unbounded, they cannot be specified by simplyenumerating them, as is standard with PCFGs.
In-stead, the productions used in an adaptor gram-mar are specified indirectly using a base grammar:the subtrees of the base grammar?s ?adapted non-terminals?
serve as the possible productions of theadaptor grammar (Johnson et al2007), much inthe way that subtrees function as productions in TreeSubstitution Grammars .1Another way to view Adaptor Grammars is thatthey relax the independence assumptions associatedwith PCFGs.
In a PCFG productions are gener-ated independently conditioned on the parent non-terminal, while in an Adaptor Grammar the proba-bility of generating a subtree rooted in an adapted1For computational efficiency reasons Adaptor Grammarsrequire the subtrees to completely expand to terminals.
TheFragment Grammars of O?Donnell (2011) lift this restriction.non-terminal is roughly proportional to the numberof times it has been previously generated (a certainamount of mass is reserved to generate ?new?
sub-trees).
This means that the distribution generated byan Adaptor Grammar ?adapts?
based on the corpusbeing generated.2.2.1 Mechanics of adaptor grammarsAdaptor Grammars are specified by a PCFG G,plus a subset of G?s non-terminals that are calledthe adapted non-terminals, as well as a discountparameter aA, where 0 ?
aA < 1 and a con-centration parameter bA, where b > ?a, for eachadapted non-terminal A.
An adaptor grammar de-fines a two-parameter Poisson-Dirichlet Process foreach adapted non-terminal A governed by the pa-rameters aA and bA.
For computational purposes itis convenient to integrate out the Poisson-DirichletProcess, resulting in a predictive distribution spec-ified by a Pitman-Yor Process (PYP).
A PYP canbe understood in terms of a ?Chinese Restaurant?metaphor in which ?customers?
(observations) areseated at ?tables?, each of which is labelled with asample from a ?base distribution?
(Pitman and Yor,1997).In an Adaptor Grammar, unadapted non-terminalsexpand just as they do in a PCFG; a production r ex-panding the non-terminal is selected according to themultinomial distribution ?r over productions speci-fied in the grammar.
Each adapted non-terminalA isassociated with its own Chinese Restaurant, wherethe tables are labelled with subtrees generated bythe grammar rooted in A.
In the Chinese Restau-rant metaphor, the customers are expansions of A,each table corresponds to a particular subtree ex-panding A, and the PCFG specifies the base distri-bution for each of the adapted non-terminals.
Anadapted non-terminal A expands as follows.
A ex-pands to a subtree t with probability proportional tont, where nt is the number of times t has been pre-viously generated.
In addition, A expands using aPCFG rule r expanding A with probability propor-tional to (mA aA + bA) ?r, where mA is the numberof subtrees expanding A (i.e., the number of tablesin A?s restaurant).
Because the underlying Pitman-Yor Processes have a ?rich get richer?
property, theygenerate power-law distributions over the subtreesfor adapted non-terminals.7012.2.2 Adaptor grammars as LDA extensionWith the ability to rewrite non-terminals to en-tire subtrees, adaptor grammars have been used toextend unigram-based LDA topic models (Johnson,2010).
This allows topic models to capture se-quences of words with abitrary length rather thanjust unigrams of word.
It has also been shown that itis crucial to go beyond the bag-of-words assump-tion as topical collocations capture more meaninginformation and represent more interpretable topics(Wang et al2007).Taking the PCFG formulation for the LDA topicmodels, it can be modified such that each topicTopici generates sequences of words by adaptingeach of the Topici non-terminals (usually indicatedwith an underline in an adaptor grammar).
The over-all schema for capturing topical collocations with anadaptor grammar is as follows:Sentence?
Docj j ?
1, .
.
.
,mDocj ?
j j ?
1, .
.
.
,mDocj ?
Docj Topici i ?
1, .
.
.
, t;j ?
1, .
.
.
,mTopici ?Words i ?
1, .
.
.
, tWords?WordWords?Words WordWord?
w w ?
VThere is a non-grammar-based approach to find-ing topical collocations as demonstrated by Wang etal.
(2007).
Both of these approaches learned use-ful collocations: for instance, as mentioned in Sec-tion 1, Johnson (2010) found collocations such gra-dient descent and cost function associated with thetopic of machine learning; Wang et al2007) foundthe topic of human receptive system comprises ofcollocations such as visual cortext and motion de-tector.Adaptor grammars have also been deployed as aform of feature selection in discovering useful collo-cations for perspective classification.
Hardisty et al(2010) argued that indicators of perspectives are of-ten beyond the length of bigrams and demonstratedthat the use of the adaptor grammar inferred n-gramsof arbitrary length as features establishes the start-of-the-art performance for perspective classificationon the Bitter Lemons corpus, depicting two differ-ent perspectives of Israeli and Pelestinian.
We areadopting a similar approach in this paper for classi-fying texts with respect to the author?s native lan-guage; but the key difference with Hardisty et al(2010)?s approach is that our focus is on collocationsthat mix PoS and lexical elements, rather than beingpurely lexical.3 Maxent ClassificationIn this section, we first explain the procedures takento set up the conventional supervised classificationtask for NLI through the deployment of adaptorgrammars for discovery of ?quasi-syntactic colloca-tions?
of arbitrary length.
We then present the classi-fication results attained based on these selected setsof n-gram features.
In all of our experiments, weinvestigate two sets of collocations: pure PoS anda mixture of PoS and function words.
The idea ofexamining the latter set is motivated by the resultsof Wong and Dras (2011) where inclusion of parseproduction rules lexicalised with function words asfeatures had shown to improve the classification per-formance relative to unlexicalised ones.3.1 Experimental Setup3.1.1 Data and evaluationThe classification experiments are conducted onthe second version of ICLE (Granger et al2009).2Following our earlier NLI work in Wong and Dras(2011), our data set consists of 490 texts writtenin English by authors of seven different native lan-guage groups: Bulgarian, Czech, French, Russian,Spanish, Chinese, and Japanese.
Each native lan-guage contributes 70 out of the 490 texts.
As we areusing a relative small data set, we perform k-foldcross-validation, choosing k = 5.3.1.2 Adaptor grammars for supervisedclassificationWe derive two adaptor grammars for the maxentclassification setting, where each is associated witha different set of vocabulary (i.e.
either pure PoSor the mixture of PoS and function words).
We use2Joel Tetreault and Daniel Blanchard from ETS have pointedout (personal communication) that there is a subtle issue withICLE that could have an impact on the classification perfor-mance of NLI tasks; in particular, when character n-grams areused as features, some special characters used in some ICLEtexts might affect performance.
For our case, this should not beof much issue since they will not appear in our collocations.702the grammar of Johnson (2010) as presented in Sec-tion 2.2.2, except that the vocabulary differs: eitherw ?
Vpos or w ?
Vpos+fw.
For Vpos, there are119 distinct PoS tags based on the Brown tagset.Vpos+fw is extended with 398 function words as perWong and Dras (2011).
m = 490 is the number ofdocuments, and t = 25 the number of topics (chosenas the best performing one from Wong et al2011)).Rules of the form Docj ?
Docj Topici thatencode the possible topics that are associated witha document j are given similar ?
priors as usedin LDA (?
= 5/t where t = 25 in our experi-ments).
Likewise, similar ?
priors from LDA areplaced on the adapted rules expanding from Topici?
Words, representing the possible sequences ofwords that each topic comprises (?
= 0.01).3 Theinference algorithm for the adaptor grammars arebased on the Markov Chain Monte Carlo techniquemade available online by Johnson (2010).43.1.3 Classification models with n-gramfeaturesBased on the two adaptor grammars inferred, theresulting collocations (n-grams) are extracted as fea-tures for the classification task of identifying au-thors?
native language.
These n-grams found by theadaptor grammars are only a (not necessarily proper)subset of those n-grams that are strongly characteris-tic of a particular native language.
In principle, onecould find all strongly characteristic n-grams by enu-merating all the possible instances of n-grams up toa given length if the vocabulary is of a small enoughclosed set, such as for PoS tags, but this is infeasi-ble when the set is extended to PoS plus functionwords.
The use of adaptor grammars here can beviewed as a form of feature selection, as in Hardistyet al2010).Baseline models To serve as a baseline, we takethe commonly used PoS bigrams as per the previ-ous work of NLI (Koppel et al2005).
A set of200 PoS bigrams is selected in two ways: the 200most frequent in the training data (as in Wong andDras (2011)) and the 200 with the highest informa-tion gain (IG) values in the training data (not evalu-3The values of ?
and ?
are also based on the establishedvalues presented in Wong et al2011).4Adaptor grammar software is available on http://web.science.mq.edu.au/?mjohnson/Software.htm.ated in other work).Enumerated n-gram models Here, we enumer-ate all the possible n-grams up to a fixed length andselect the best of these according to IG, as a general-isation of the baseline.
The first motivation for thisfeature set is that, in a sense, this should give a roughupper bound for the adaptor grammar?s PoS-alone n-grams, as these latter should most often be a subsetof the former.
The second motivation is that it givesa robust comparison for the mixed PoS and functionword n-grams, where it is infeasible to enumerate allof them.ENUM-POS We enumerate all possible n-grams upto the length of 5, and select those that actuallyoccur (i.e.
of the?5i=1 119i possible n-grams,this is 218,042 based on the average of 5 folds).We look at the top n-grams up to length 5 selectedby IG: the top 2,800 and the top 6,500 (for com-parability with adaptor grammar feature sets, be-low), as well as the top 10,000 and the top 20,000(to study the effect of larger feature space).Adaptor grammar n-gram models The classifi-cation features are the two sets of selected colloca-tions inferred by the adaptor grammars which are themain interest of this paper.AG-POS This first set of the adaptor grammar-inferred features comprise of pure PoS n-grams(i.e.
Vpos).
The largest length of n-gram foundis 17, but about 97% of the collocations are oflength between 2 to 5.
We investigate three vari-ants of this feature set: top 200 n-grams of alllengths (based on IG), all n-grams of all lengths(n = 2, 795 on average), and all n-grams up tothe length of 5 (n = 2, 710 on average).AG-POS+FW This second set of the adaptorgrammar-inferred features are mixtures of PoSand function words (i.e.
Vpos+fw).
The largestlength of n-gram found for this set is 19 andthe total number of different collocations foundis much higher.
For the purpose of comparabil-ity with the first set of adaptor grammar features,we investigate the following five variants for thisfeature set: top 200 n-grams of all lengths, all n-grams of all lengths (n = 6, 490 on average), alln-grams up to the length of 5 (n = 6, 417 on av-erage), top 2,800 n-grams of all different lengths,703Features (n-grams) AccuracyBASELINE-POS [top200 MOST-FREQ] 53.87BASELINE-POS [top200 IG] 56.12AG-POS [top200 IG] 61.02AG-POS [all ?17-gram] (n ?
2800) 68.37AG-POS [all ?
5-gram] (n ?
2700) 68.57AG-POS+FW [top200 IG] 58.16AG-POS+FW [all ?19-gram] (n ?
6500) 74.49AG-POS+FW [all ?5-gram] (n ?
6400) 74.49AG-POS+FW [top2800 IG ?
19-gram] 71.84AG-POS+FW [top2800 IG ?
5-gram] 71.84ENUM-POS [top2800 IG ?
5-gram] 69.79ENUM-POS [top6500 IG ?
5-gram] 72.44ENUM-POS [top10K IG ?
5-gram] 71.02ENUM-POS [top20K IG ?
5-gram] 71.43Table 1: Maxent classification results for individual fea-ture sets (with 5-fold cross validation).and top 2,800 n-grams up to the length of 5.
(Allthe selections are based on IG).In our models, all feature values are of binarytype.
For the classifier, we employ a maximum en-tropy (MaxEnt) machine learner ?
MegaM (fifth re-lease) by Hal Daume?
III.53.2 Classification resultsTable 1 presents all the classification results for theindividual feature sets, along with the baselines.
Onthe whole, both sets of the collocations inferred bythe adaptor grammars perform better than the twobaselines.
We make the following observations:?
Regarding ENUM-POS as a (rough) upperbound, the adaptor grammar AG-POS with acomparable number of features performs al-most as well.
However, because it is possible toenumerate many more n-grams than are foundduring the sampling process, ENUM-POS opensup a gap over AG-POS of around 4%.?
Collocations with a mix of PoS and functionwords do in fact lead to higher accuracy ascompared to those of pure PoS (except for thetop 200 n-grams); for instance, compare the2,800 n-grams up to length 5 from the two cor-responding sets (71.84 vs.
68.57).?
Furthermore, the adaptor grammar-inferredcollocations with mixtures of PoS and function5MegaM software is available on http://www.cs.utah.edu/?hal/megam/.Features (n-grams) AccuracyAG-POS [all ?
5-gram] & FW 72.04ENUM-POS [top2800 ?
5-gram] & FW 73.67AG-POS+FW & AG-POS a 75.71AG-POS+FW & AG-POS b 74.90AG-POS+FW & ENUM-POS [top2800] a 73.88AG-POS+FW & ENUM-POS [top2800] b 74.69AG-POS+FW & ENUM-POS [top10K] b 74.90AG-POS+FW & ENUM-POS [top20K] b 75.10Table 2: Maxent classification results for combined fea-ture sets (with 5-fold cross validation).
aFeatures fromthe two sets are selected based on the overall top 3700with highest IG; bfeatures from the two sets are just lin-early concatenated.words (AG-POS+FW) in general perform betterthan our rough upper bound of PoS colloca-tions, i.e.
the enumerated PoS n-grams (ENUM-POS): the overall best results of the two featuresets are 74.49 and 72.44 respectively.Given that the AG-POS+FW n-grams are captur-ing different sorts of document characteristics, theycould potentially usefully be combined with thePoS-alone features.
We thus combined them withboth AG-POS and ENUM-POS feature sets, and theclassification results are presented in Table 2.
Wetried two ways of integrating the feature sets: oneway is to take the overall top 2,800 of the two setsbased on IG; the other way is to just combine the twosets of features by concatenation of feature vectors(as indicated by a and b respectively in the resulttable).
For comparability purposes, we consideredonly n-grams up to the length of 5.
A baseline ap-proach to this is just to add in function words as un-igram features by feature vector concatenation, giv-ing two further models, AG-POS [all ?
5-gram] &FW and ENUM-POS [top2800 ?
5-gram] & FW.Overall, the classification accuracies attained bythe combined feature sets are higher than the in-dividual feature sets.
The best performing of allthe models is achieved by combining the mixedPoS and function word collocations with the adap-tor grammar-inferred PoS, producing the best accu-racy thus far of 75.71.
This demonstrates that fea-tures inferred by adaptor grammars do capture someuseful information and function words are playinga role.
The way of integrating the two feature setshas different effects on the types of combination.
Asseen in Table 2, method a works better for the com-704bination of the two adaptor grammar feature sets;whereas method b works better for combining adap-tor grammar features with enumerated n-gram fea-tures.Using adaptor grammar collocations also outper-forms the alternative baseline of adding in functionwords as unigrams.
For instance, the best perform-ing combined feature set of both AG-POS and AG-POS+FW does result in higher accuracy as comparedto the two alternative baseline models, comparing75.71 with 72.04 (and 75.71 with 73.67).
Thisdemonstrates that our more general PoS plus func-tion word collocations derived from adaptor gram-mars are indeed useful, and supports the argumentof Wang et al2007) that they are a useful tech-nique for looking into features beyond just the bagof words.4 Language Model-based ClassificationIn this section, we take a language modeling ap-proach to native language identification; the ideahere is to adopt grammatical inference to learna grammar-based language model to represent thetexts written by non-English native users.
The gram-mar learned is then used to predict the most probablenative language that a document (a sentence) is as-sociated with.In a sense, we are using a parser-based languagemodel to rank the documents with respect to nativelanguage.
We draw on the work of Bo?rschinger etal.
(2011) for this section.
In that work, the taskwas grounded learning of a semantic parser.
Train-ing examples there consisted of natural languagestrings (descriptions of a robot soccer game) anda set of candidate meanings (actions in the robotsoccer game world) for the string; each was taggedwith a context identifier reflecting the actual actionof the game.
A grammar was then induced thatwould parse the examples, and was used on test data(where the context identifier was absent) to predictthe context.
We take a similar approach to devel-oping an grammatical induction technique, althoughwhere they used a standard LDA topic model-basedPCFG, we use an adaptor grammar.
We expect thatthe results will likely to be lower than for the dis-criminative approach of Section 3.
However, theapproach is of interest for a few reasons: because,whereas the adaptor grammar plays an ancillary, fea-ture selection role in Section 3, here the feature se-lection is an organic part of the approach as per theactual implementation of Hardisty et al2010); be-cause adaptor grammars can potentially be extendedin a natural way with unlabelled data; and because,for the purposes of this paper, it constitutes a second,quite different way to evaluate the use of n-gram col-locations.4.1 Language ModelsWe derive two adaptor grammar-based languagemodels.
One consists of only unigrams and bi-grams, and the other finds n-gram collocations, inboth cases over either PoS or the mix of PoS andfunction words.
The assumption that we make is thateach document (each sentence) is a mixture of twosets of topics: one is the native language-specifictopic (i.e.
characteristic of the native language) andthe other is the generic topic (i.e.
characteristic ofthe second language ?
English in our case).
Thegeneric topic is thus shared across all languages,and will behave quite differently from a language-specific topic, which is not shared.
In other words,there are eight topics, representing seven native lan-guage groups that are of interest (Bulgarian, Czech,French, Russian, Spanish, Chinese, and Japanese)and the second language English itself.6Bigram models The following rule schema isapplicable to both vocabulary types of PoS and themixture of PoS and function words.Root?
lang langTopicslangTopics?
langTopics langTopiclangTopics?
langTopics nullTopiclangTopics?
langTopiclangTopics?
nullTopiclangTopic?WordsnullTopic?WordsWords?Word WordWords?WordWord?
w w ?
Vpos; w ?
Vpos+fwN-gram models The grammar is the same asthe above with the exception that the non-terminalWords is now rewritten as follows in order to6We could just induce a regular PCFG here, rather than anadaptor grammar, by taking as terminals all pairs of PoS tags.We use the adaptor grammar formulation for comparability.705capture n-gram collocations of arbitrary length.Words?Words WordWords?WordIt should be noted that the two grammars abovecan in theory be applied to an entire document or onindividual sentences.
For this present work, we workon the sentence level as the run-time of the currentimplementation of the adaptor grammars grows pro-portional to the cube of the sentence length.
For eachgrammar we try both sparse and uniform Dirichletpriors (?
= {0.01, 0.1, 1.0}).
The sparse priors en-courage only a minority of the rules to be associatedwith high probabilities.4.2 Training and EvaluationAs we are using the same data set as per the pre-vious approach, we perform 5-fold cross validationas well.
However, the training for each fold is con-ducted with a different grammar consisting of onlythe vocabulary that occur in each training fold.
Thereason is that we are now having a form of super-vised topic models where the learning process isguided by the native languages.
Hence, each of thetraining sentences are prefixed with the (native) lan-guage identifiers lang, as seen in the Root rules ofthe grammar presented above.To evaluate the grammars learned, as inBo?rschinger et al2011) we need to slightly modifythe grammars above by removing the language iden-tifiers ( lang) from theRoot rules and then parse theunlabeled sentences using a publicly available CKYparser.7 The predicted native language is inferredfrom the parse output by reading off the langTopicsthat the Root is rewritten to.
We take that as themost probable native language for a particular testsentence.
At the document level, we select as theclass the language predicted for the largest numberof sentences in that document.4.3 Parsing ResultsTables 3 and 4 present the parsing results at the sen-tence level and the document level, respectively.
Onthe whole, the results at the sentence level are muchpoorer as compared to those at the document level.In light of the results of Section 3.2, it is surprising7CKY parser by Mark Johnson is available onhttp://web.science.mq.edu.au/?mjohnson/Software.htm.Features Accuracy(n-grams) (?
= 0.01) (?
= 0.1) (?
= 1.0)AG-POS [bigrams] 26.84 27.03 26.77AG-POS [n-grams] 25.85 25.78 25.62AG-POS+FW [bigrams] 28.58 28.40 27.43AG-POS+FW [n-grams] 26.64 27.64 28.75Table 3: Language modeling-based classification resultsbased on parsing (at the sentence level).Features Accuracy(n-grams) (?
= 0.01) (?
= 0.1) (?
= 1.0)AG-POS [bigrams] 41.22 38.88 39.69AG-POS [n-grams] 36.12 34.90 35.20AG-POS+FW [bigrams] 47.45 46.94 44.64AG-POS+FW [n-grams] 43.97 49.39 50.15Table 4: Language modeling-based classification resultsbased on parsing (at the document level).that bigram models appear to perform better than n-gram models for both types of vocabulary, with theexception of AG-POS+FW at the document level.
Infact, one would expect n-gram models to performbetter in general as it is a generalisation that wouldcontain all the potential bigrams.
Nonetheless, thelanguage models over the mixture of PoS and func-tion words appear to be a more suitable representa-tive of our learner corpus as compared to those overpurely PoS, confirming the usefulness of integratedfunction words for the NLI classification task.It should also be noted that sparse priors gen-erally appear to be more appriopriate; except thatfor AG-POS+FW n-grams, uniform priors are indeedbetter and resulted in the highest parsing result of50.15.
(Although all the parsing results are muchweaker as compared to the results presented in Sec-tion 3.2, they are all higher than the majority base-line of 14.29% i.e.
70/490).5 DiscussionHere we take a closer look at how well each ap-proach does in identifying the individual native lan-guages.
The confusion matrix for the best modelof two approaches are presented in Table 5 and Ta-ble 6.
Both approaches perform reasonably well forthe two Oriental languages (Chinese in particular);this is not a major surprise, as the two languagesare not part of the language family that the rest ofthe languages come from (i.e.
Indo-European).
Un-der the supervised maxent classification, misclassi-fications largely are observed in the Romance ones(French and Spanish) as well as Russian; for the lan-guage model-based approach, Bulgarian is identi-706BL CZ RU FR SP CN JPBL [52] 5 7 4 2 - -CZ 5 [50] 5 3 4 - 3RU 6 8 [46] 5 1 - 4FR 7 3 5 [43] 8 - 4SP 7 2 4 9 [47] - 1CN - - - - - [70] -JP - - 2 2 1 2 [63]Table 5: Confusion matrix based on the best performingmodel under maxent setting (BL:Bulgarian, CZ:Czech,RU:Russian, FR:French, SP:Spanish, CN:Chinese,JP:Japanese).BL CZ RU FR SP CN JPBL [20] 32 9 6 - 1 2CZ 2 [59] 3 1 - - 5RU 3 41 [19] 2 1 - 4FR 8 20 4 [31] 4 - 3SP 7 27 11 12 [9] - 4CN - 2 - 2 - [62] 4JP - 19 1 2 - 1 [47]Table 6: Confusion matrix based on the bestperforming model under language modeling setting(BL:Bulgarian, CZ:Czech, RU:Russian, FR:French,SP:Spanish, CN:Chinese, JP:Japanese).fied poorly, and Spanish moreso.
However, the latterapproach appears to be better in identifying Czech.On the whole, the maxent approach results in muchfewer misclassifications compared to its counterpart.In fact, there is a subtle difference in the exper-imental setting of the models derived from the twoapproaches with respect to the adaptor grammar: thenumber of topics.
Under the maxent setting, thenumber of topics t was set to 25, while we restrictedthe models with the language modeling approach toonly eight topics (seven for the individual native lan-guages and one for the common second language,English).
Looking more deeply into the topics them-selves reveals that there appears to be at least two outof the 25 topics (from the supervised models) asso-ciated with n-grams that are indicative of the nativelanguages, taking Chinese and Japanese as examples(see the associated topics in Table 7).8 Perhaps as-sociating each native language with only one gener-alised topic is not sufficient.Furthermore, the distribution of n-grams amongthe topics (i.e.
subtrees of collocations derivedfrom the adaptor grammars) are quite different be-tween the two approaches although the total num-8Taking the examples from Wong et al2011) as reference,we found similar n-grams that are indicative of Japanese andChinese.Top 10 Mixture N-gramsJapanese Chinesetopic2 topic23 topic9 topic17.
.
NN .we VB PPSS VB a NN NN NNour NNS my NN NN NN NNSour NN CC VBN by NNNN VBG NP .
RB ,PPSS VB PPSS think NP of NNabout NN : JJ NNbecause PPSS VBD ( NN .it .
RB as VBG NNwe are PPSS ?
NN NN NN NN NN NN NNTable 7: Top mixture n-grams (collocations) for 4 out ofthe 25 topics representative of Japanese and Chinese (un-der maxent setting).
N-grams of pronoun with verb arefound at the upper end of Topic2 and Topic23 reflectingthe frequent usage of Japanese; n-grams of noun are topn-grams under Topic9 and Topic17 indicating Chinese?scommon error of determiner-noun disagreement.ber of n-grams inferred by each approach is aboutthe same.
For the language modeling ones, a highnumber of n-grams were associated with the generictopic nullTopic9 and each language-specific topiclangTopic has a lower number of n-grams relativeto bi-grams (Table 8) associated with it.
For themaxent models, in contrast, the majority of the top-ics were associated with a higher number of n-grams(Table 9).
The smaller number of n-grams to be usedas features ?
and the fact that their extra lengthmeans that they will occur more sparsely in the doc-uments ?
seems to be the core of the problem.Nonetheless, the language models inferred dis-cover relevant n-grams that are representative ofindividual native languages.
For instance, the bi-gram NN NN, which Wong and Dras (2011) claimmay reflect the error of determiner-noun disagree-ment commonly found amongst Chinese learners,was found under the Chinese topic at the top-2 posi-tion with a probability of 0.052 as compared to theother languages at the probability range of 0.0005-0.003.
Similarly, one example for Japanese, the mix-ture bigram PPSS think, indicating frequent us-age of pronouns within Japanese was seen under theJapanese topic at the top-9 position with a probabil-ity of 0.025 in relation to other languages within therange of 0.0002-0.006: this phenomenon as char-9This is quite plausible as there should be quite a number ofstructures that are representative of native English speakers thatare shared by non-native speakers.707Model N-gram FrequencyTypes BGTopic CZTopic FRTopic RUTopic SPTopic CNTopic JPTopic NullTopic(a) (b) (a) (b) (a) (b) (a) (b) (a) (b) (a) (b) (a) (b) (a) (b)Bigrams 374 187 352 219 426 165 350 211 351 156 397 351 394 194 867 6169N-grams 177 159 226 217 151 152 148 202 128 147 357 255 209 226 3089 7794Table 8: Distribution of n-grams (collocations) for each topic under language modeling setting.
(a) subcolumns arefor n-grams of pure PoS and (b) subcolumns are for n-grams of mixtures of PoS and function words.N-gram FrequencyTopic1 Topic2 Topic3 Topic4 Topic5 Topic6 Topic7 Topic8 Topic9 Topic10(a) (b) (a) (b) (a) (b) (a) (b) (a) (b) (a) (b) (a) (b) (a) (b) (a) (b) (a) (b)174 443 145 441 136 245 141 341 236 519 169 748 127 340 182 473 109 339 190 236Topic11 Topic12 Topic13 Topic14 Topic15 Topic16 Topic17 Topic18 Topic19 Topic20(a) (b) (a) (b) (a) (b) (a) (b) (a) (b) (a) (b) (a) (b) (a) (b) (a) (b) (a) (b)57 259 126 455 103 543 211 225 170 459 81 309 238 207 152 475 119 452 333 423Topic21 Topic22 Topic23 Topic24 Topic25(a) (b) (a) (b) (a) (b) (a) (b) (a) (b)245 341 168 492 194 472 201 366 195 190Table 9: Distribution of n-grams (collocations) for each topic under maxent setting.
(a) subcolumns are for n-gramsof pure PoS and (b) subcolumns are for n-grams of mixtures of PoS and function words.Languages Excerpts from ICLEChinese ... the overpopulation problem in urban area ......
The development of country park can directly ...... when it comes to urban renewal project ...... As developing new town in ...... and reserve some country park as ...Japanese ...
I think many people will ......
I think governments should not ......
I think culture is the most significant ......
I think the state should not ......
I really think we must live ...Table 10: Excerpts from ICLE illustrating the commonphenomena observed amongst Chinese and Japanese.acteristic of Japanese speakers has also been notedfor different corpora by Ishikawa (2011).
(Note thatthis collocation as well as its pure PoS counterpartPPSS VB are amongst the top n-grams discoveredunder the maxent setting as seen in Table 7.)
Table10 presents some excerpts extracted from the corpusthat illustrate these two common phenomena.To investigate further the issue associated with thenumber of topics under the language modeling set-ting, we attempted to extend the adaptor grammarwith three additional topics that represent the lan-guage family of the seven native languages of inter-est: Slavic, Romance, and Oriental.
(The resultinggrammar is presented as below.)
However, the pars-ing result does not improve over the initial settingwith eight topics in total.Root?
lang langTopicslangTopics?
langTopics langTopiclangTopics?
langTopics familyTopiclangTopics?
langTopics nullTopiclangTopics?
langTopiclangTopics?
familyTopiclangTopics?
nullTopiclangTopic?WordsfamilyTopic?WordsnullTopic?WordsWords?Words WordWords?WordWord?
w w ?
Vpos; w ?
Vpos+fw6 Conclusion and Future WorkThis paper has shown that the extension of adap-tor grammars to discovering collocations beyond thelexical, in particular a mix of PoS tags and functionwords, can produce features useful in the NLI clas-sification problem.
More specifically, when addedto a new baseline presented in this paper, the com-bined feature set of both types of adaptor grammarinferred collocations produces the best result in thecontext of using n-grams for NLI.
The usefulness ofthe collocations does vary, however, with the tech-nique used for classification.Future work will involve a broader explorationof the parameter space of the adaptor grammars,in particular the number of topics and the valueof ?
; a look at other non-parametric extensions ofPCFGs, such as infinite PCFGs (Liang et al2007)for finding a set of non-terminals permitting morefine-grained topics; and an investigation of how theapproach can be extended to semi-supervised learn-ing to take advantage of the vast quantity of textswith errors available on the Web.708AcknowledgmentsWe would like to acknowledge the support of ARCLinkage Grant LP0776267.
We also thank theanonymous reviewers for useful feedback.
Muchgratitude is due to Benjamin Bo?rschinger for hishelp with the language modeling implementation.ReferencesDavid M. Blei, Andrew Ng, and Michael Jordan.
2003.Latent Dirichlet alation.
Journal of MachineLearning Research, 3:993?1022.Benjamin Bo?rschinger, Bevan K. Jones, and Mark John-son.
2011.
Reducing grounded learning tasks to gram-matical inference.
In Proceedings of the 2011 Confer-ence on Empirical Methods in Natural Language Pro-cessing, pages 1416?1425, Edinburgh, Scotland, July.Eugene Charniak and Mark Johnson.
2005.
Coarse-to-fine n-best parsing and MaxEnt discriminative rerank-ing.
In Proceedings of the 43rd Annual Meeting on As-sociation for Computational Linguistics, pages 173?180, Ann Arbor, Michigan, June.Dominique Estival, Tanja Gaustad, Son-Bao Pham, WillRadford, and Ben Hutchinson.
2007.
Author profilingfor English emails.
In Proceedings of the 10th Con-ference of the Pacific Association for ComputationalLinguistics (PACLING), pages 263?272.Sylviane Granger, Estelle Dagneaux, Fanny Meunier,and Magali Paquot.
2009. International Corpus ofLearner English (Version 2).
Presses Universitaires deLouvain, Louvian-la-Neuve.Thomas L. Griffiths and Mark Steyvers.
2004.
Find-ing scientific topics.
Proceedings of the NationalAcademy of Sciences, 101(suppl.
1):5228?5235.Eric A. Hardisty, Jordan Boyd-Graber, and Philip Resnik.2010.
Modeling perspective using adaptor grammars.In Proceedings of the 2010 Conference on EmpiricalMethods in Natural Language Processing, EMNLP?10, pages 284?292.Shun?ichiro Ishikawa.
2011.
A New Horizon in LearnerCorpus Studies: The Aim of the ICNALE Project.
InG.
Weir, S. Ishikawa, and K. Poonpon, editors, Cor-pora and Language Technologies in Teaching, Learn-ing and Research, pages 3?11.
University of Strath-clyde Press, Glasgow, UK.Mark Johnson, Thomas L. Griffiths, and Sharon Goldwa-ter.
2007.
Adaptor grammars: A framework for speci-fying compositional nonparametric bayesian models.In Advances in Neural Information Processing Sys-tems 19: Proceedings of the Twentieth Annual Confer-ence on Neural Information Processing Systems, pages641?648.Mark Johnson.
2010.
PCFGs, Topic Models, AdaptorGrammars and Learning Topical Collocations and theStructure of Proper Names.
In Proceedings of the 48thAnnual Meeting of the Association for ComputationalLinguistics, pages 1148?1157, Uppsala, Sweden, July.Moshe Koppel, Jonathan Schler, and Kfir Zigdon.
2005.Automatically determining an anonymous author?s na-tive language.
In Intelligence and Security Informat-ics, volume 3495 of Lecture Notes in Computer Sci-ence, pages 209?217.
Springer-Verlag.Moshe Koppel, Jonathan Schler, and Shlomo Argamon.2009.
Computational Methods in Authorship Attribu-tion.
Journal of the American Society for InformationScience and Technology, 60(1):9?26.Percy Liang, Slav Petrov, Michael I. Jordan, and DanKlein.
2007.
The infinite pcfg using hierarchicaldirichlet processes.
In Proceedings of the 2007 JointConference on Empirical Methods in Natural Lan-guage Processing and Computational Natural Lan-guage Learning, pages 688?697, Prague, Czech Re-public, June.Timothy O?Donnell.
2011.
Productivity and reuse inlanguage.
Ph.D. thesis, Harvard University.Jim Pitman and Marc Yor.
1997.
The two-parameterPoisson-Dirichlet distribution derived from a stablesubordinator.
Annals of Probability, 25(2):855?900.Oren Tsur and Ari Rappoport.
2007.
Using classifier fea-tures for studying the effect of native language on thechoice of written second language words.
In Proceed-ings of the Workshop on Cognitive Aspects of Compu-tational Language Acquisition, pages 9?16.Hans van Halteren.
2008.
Source language markers inEUROPARL translations.
In Proceedings of the 22ndInternational Conference on Computational Linguis-tics (COLING), pages 937?944.Xuerui Wang, Andrew McCallum, and Xing Wei.
2007.Topical n-grams: Phrase and topic discovery, with anapplication to information retrieval.
In Proceedings ofthe 2007 Seventh IEEE International Conference onData Mining, ICDM ?07, pages 697?702.Sze-Meng Jojo Wong and Mark Dras.
2011.
Exploit-ing parse structures for native language identification.In Proceedings of the 2011 Conference on Empiri-cal Methods in Natural Language Processing, pages1600?1610, Edinburgh, Scotland, July.Sze-Meng Jojo Wong, Mark Dras, and Mark Johnson.2011.
Topic modeling for native language identifi-cation.
In Proceedings of the Australasian LanguageTechnology Association Workshop 2011, pages 115?124, Canberra, Australia, December.709
