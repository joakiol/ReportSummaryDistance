Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1259?1269,Seattle, Washington, USA, 18-21 October 2013. c?2013 Association for Computational LinguisticsSentiment Analysis: How to Derive Prior Polarities from SentiWordNetMarco GueriniTrento RISEVia Sommarive 1838123 Povo in Trento, Italym.guerini@trentorise.euLorenzo GattiTrento RISEVia Sommarive 1838123 Povo in Trento, Italyl.gatti@trentorise.euMarco TurchiFondazione Bruno KesslerVia Sommarive 1838123 Povo in Trento, Italyturchi@fbk.euAbstractAssigning a positive or negative score to aword out of context (i.e.
a word?s prior polar-ity) is a challenging task for sentiment analy-sis.
In the literature, various approaches basedon SentiWordNet have been proposed.
In thispaper, we compare the most often used tech-niques together with newly proposed ones andincorporate all of them in a learning frame-work to see whether blending them can fur-ther improve the estimation of prior polarityscores.
Using two different versions of Sen-tiWordNet and testing regression and classifi-cation models across tasks and datasets, ourlearning approach consistently outperformsthe single metrics, providing a new state-of-the-art approach in computing words?
priorpolarity for sentiment analysis.
We concludeour investigation showing interesting biasesin calculated prior polarity scores when wordPart of Speech and annotator gender are con-sidered.1 IntroductionMany approaches to sentiment analysis make use oflexical resources ?
i.e.
lists of positive and neg-ative words ?
often deployed as baselines or asfeatures for other methods (usually machine learn-ing based) for sentiment analysis research (Liu andZhang, 2012).
In these lexica, words are associatedwith their prior polarity, i.e.
if that word out of con-text evokes something positive or something nega-tive.
For example, wonderful has a positive connota-tion ?
prior polarity ?
while horrible has a negativeone.
These approaches have the advantage of notneeding deep semantic analysis or word sense dis-ambiguation to assign an affective score to a wordand are domain independent (they are thus less pre-cise but more portable).SentiWordNet (henceforth SWN) is one of theseresources and has been widely adopted since it pro-vides a broad-coverage lexicon ?
built in a semi-automatic manner ?
for English (Esuli and Sebas-tiani, 2006).
Given that SWN provides polaritiesscores for each word sense (also called ?posteriorpolarities?
), it is necessary to derive prior polaritiesfrom the posteriors.
For example, the word cold hasa posterior polarity for the meaning ?having a lowtemperature?
?
like in ?cold beer?
?
that is differentfrom the one in ?cold person?
which refers to ?beingemotionless?.
This information must be consideredwhen reconstructing the prior polarity of cold.Several formulae to compute prior polarities start-ing from posterior polarities scores have been usedin the literature.
However, their performance variessignificantly depending on the adopted variant.
Weshow that researchers have not paid sufficient atten-tion to this posterior-to-prior polarity issue.
Indeed,we show that some variants outperform others ondifferent datasets and can represent a fairer state-of-the-art approach using SWN.
On top of this, we at-tempt to outperform the state-of-the-art formula us-ing a learning framework that combines the variousformulae together.In detail, we will address five main researchquestions: (i) is there any relevant difference inthe posterior-to-prior polarity formulae performance(both in regression and classification tasks), (ii) isthere any relevant variation in prior polarity values1259if we use different releases of SWN (i.e.
SWN1 orSWN3), (iii) can a learning framework boost per-formance of such formulae, (iv) considering wordPart of Speech (PoS), is there any relevant differencein formulae performance, (v) considering the genderdimension of the annotators (male/female) and thesentiment dimension (positive/negative), is there anyrelevant difference in SWN performance.In Section 2 we briefly describe our approach andhow it differentiates from similar sentiment analysistasks.
Then, in Sections 3 and 4, we present Sen-tiWordNet and overview various posterior-to-priorpolarity formulae based on this resource that ap-peared in the literature (included some new oneswe identified as potentially relevant).
In Section 5we describe the learning approach adopted on prior-polarity formulae.
In Section 6 we introduce theANEW and General Inquirer resources that will beused as gold standards.
Finally, in the two last sec-tions, we present a series of experiments, both inregression and classification tasks, that give an an-swer to the aforementioned research questions.
Theresults support the hypothesis that using a learningframework we can improve on state-of-the-art per-formance and that there are some interesting phe-nomena connected to PoS and annotator gender.2 Proposed ApproachIn the broad field of Sentiment Analysis we will fo-cus on the specific problem of posterior-to-prior po-larity assessment, using both regression and classifi-cation experiments.
A general overview on the fieldand possible approaches can be found in (Pang andLee, 2008) or (Liu and Zhang, 2012).For the regression task, we tackled the problemof assigning affective scores (along a continuum be-tween -1 and 1) to words using the posterior-to-priorpolarity formulae.
For the classification task (assess-ing whether a word is either positive or negative) weused the same formulae, but considering just the signof the result.
In these experiments we will also use alearning framework which combines the various for-mulae together.
The underlying hypothesis is that byblending these formulae, and looking at the same in-formation from different perspectives (i.e.
the pos-terior polarities provided by SWN combined in var-ious ways), we can give a better prediction.The regression task is harder than binary classifi-cation, since we want to assess not only that pretty,beautiful and gorgeous are positive words, but alsoto define a partial or total order so that gorgeousis more positive than beautiful which, in turn, ismore positive than pretty.
This is fundamental fortasks such as affective modification of existing texts,where words?
polarity together with their score arenecessary for creating multiple graded variations ofthe original text (Guerini et al 2008).
Some ofthe work that addresses the problem of sentimentstrength are presented in (Wilson et al 2004; Pal-toglou et al 2010), however, their approach is mod-eled as a multi-class classification problem (neutral,low, medium or high sentiment) at the sentence level,rather than a regression problem at the word level.Other works such as (Neviarouskaya et al 2011)use a fine grained classification approach too, butthey consider emotion categories (anger, joy, fear,etc.
), rather than sentiment strength categories.
Onthe other hand, even if approaches that go beyondpure prior polarities ?
e.g.
using word bigram fea-tures (Wang and Manning, 2012) ?
are better forsentiment analysis tasks, there are tasks that are in-trinsically based on the notion of words?
prior polar-ity.
Consider copywriting, where evocative namesare a key element to a successful product (O?zbal andStrapparava, 2012; O?zbal et al 2012).
In such casesno context is given and the brand name alone, withits perceived prior polarity, is responsible for statingthe area of competition and evoking semantic asso-ciations.
For example Mitsubishi changed the nameof one of its SUV for the Spanish market, since theoriginal name Pajero had a very negative prior po-larity, as it meant ?wanker?
in Spanish (Piller, 2003).To our knowledge, the only work trying to addressthe SWN posterior-to-prior polarity issue, compar-ing some of the approaches appeared in the literatureis (Gatti and Guerini, 2012).
However, in our previ-ous study we only considered a regression frame-work, we did not use machine learning and we onlytested SWN1.
So, we took this work as a startingpoint for our analysis and expanded on it.3 SentiWordNetSentiWordNet (Esuli and Sebastiani, 2006) is alexical resource in which each entry is a set of1260lemma-PoS pairs sharing the same meaning, called?synset?.
Each synset s is associated with the nu-merical scores Pos(s) and Neg(s), which rangefrom 0 to 1.
These scores ?
automatically as-signed starting from a bunch of seed terms ?
rep-resent the positive and negative valence (or pos-terior polarity) of the synset and are inherited byeach lemma-PoS in the synset.
According to thestructure of SentiWordNet, each pair can have morethan one sense and each of them takes the form oflemma#PoS#sense-number, where the small-est sense-number corresponds to the most frequentsense.Obviously, different senses can have different po-larities.
In Table 1, the first 5 senses of cold#apresent all possible combinations, included mixedscores (cold#a#4), where positive and negativevalences are assigned to the same sense.
Intuitively,mixed scores for the same sense are acceptable, asin ?cold beer?
(positive) vs. ?cold pizza?
(negative).PoS Offset Pos(s) Neg(s) SynsetTermsa 1207406 0.0 0.75 cold#a#1a 1212558 0.0 0.75 cold#a#2a 1024433 0.0 0.0 cold#a#3a 2443231 0.125 0.375 cold#a#4a 1695706 0.625 0.0 cold#a#5Table 1: First five SentiWordNet entries for cold#aIn our experiments we use two different versionsof SWN: SentiWordNet 1.0 (SWN1), the first re-lease of SWN, and its updated version SentiWord-Net 3.0 (Baccianella et al 2010) ?
SWN3.
InSWN3 the annotation algorithm used in SWN1was revised, leading to an increase in the accuracyof posterior polarities over the previous version.4 Prior Polarities FormulaeIn this section we review the main strategies forcomputing prior polarities used in previous stud-ies.
All the proposed approaches try to estimatethe prior polarity score from the posterior polari-ties of all the senses for a single lemma-PoS.
Givena lemma-PoS with n senses (lemma#PoS#n), ev-ery formula f is independently applied to all thePos(s) and Neg(s) .
This produces two scores,f(posScore) and f(negScore), for each lemma-PoS.
To obtain a unique prior polarity for eachlemma-PoS, f(posScore) and f(negScore) can bemapped according to different strategies:fm =????
?f(posScore) if f(posScore) ?f(negScore)?f(negScore) otherwisefd = f(posScore)?
f(negScore)where fm computes the absolute maximum ofthe two scores, while fd computes the differencebetween them.
It is worth noting that f(negScore)is always positive by construction.
To obtaina final prior polarity that ranges from -1 to 1,the negative sign is imposed.
So, consider-ing the first 5 senses of cold#a in Table 1,f(posScore) will be derived from the Pos(s) val-ues <0.0, 0.0, 0.0, 0.125, 0.625>, while f(negScore)from <0.750, 0.750, 0.0, 0.375, 0.0>.
Then, the fi-nal polarity strength returned will be either fm or fd.The formulae (f ) we tested are the following:fs.
In this formula only the first (and thusmost frequent) sense is considered for the givenlemma#PoS.
This is equivalent to considering onlythe SWN score for lemma#PoS#1.
Based on(Neviarouskaya et al 2009; Agrawal and Siddiqui,2009; Guerini et al 2008; Chowdhury et al 2013),this is the most basic form of prior polarities.mean.
It calculates the mean of the positiveand negative scores for all the senses of the givenlemma#PoS.
This formula has been used in (Thetet al 2009; Denecke, 2009; Devitt and Ahmad,2007; Sing et al 2012).uni.
Based on (Neviarouskaya et al 2009), itconsiders only those senses that have a Pos(s)greater than or equal to the corresponding Neg(s),and greater than 0 (the stronglyPos set).
In caseposScore is equal to negScore, the one with thehighest weight is returned, where weights are de-fined as the cardinality of stronglyPos divided bythe total number of senses.
The same applies for thenegative senses.
This is the only method, togetherwith rnd, for which we cannot apply fd, as it returnsa positive or negative score according to the weight.uniw.
Like uni but without the weighting system.w1.
This formula weighs each sense with a geo-metric series of ratio 1/2.
The rationale behind thischoice is based on the assumption that more frequent1261senses should bear more ?affective weight?
than raresenses when computing the prior polarity of a word.The system presented in (Chaumartin, 2007) uses asimilar approach of weighted mean.w2.
Similar to the previous one, this formulaweighs each lemma with a harmonic series, see forexample (Denecke, 2008).On top of these formulae, we implemented somenew formulae that were relevant to our task andhave not been implemented before.
These for-mulae mimic the ones discussed previously, butthey are built under a different assumption: thatthe saliency (Giora, 1997) of a word?s prior polar-ity might be more related to its posterior polari-ties score, rather than to sense frequencies.
Thuswe ordered posScore and negScore by strength,giving more relevance to ?valenced?
senses.
Forinstance, in Table 1, posScore and negScorefor cold#a become<0.625, 0.125, 0.0, 0.0, 0.0> and<0.750, 0.750, 0.375, 0.0, 0.0> respectively.w1s and w1n.
Like w1 and w2, but senses areordered by strength (sorting Pos(s) and Neg(s) in-dependently).w1n and w2n.
Like w1 and w2 respectively, butwithout considering senses that have a 0 score forboth Pos(s) and Neg(s).
Our motivation is that?empty?
senses are mostly noise.w1sn and w2sn.
Like w1s and w2s, but with-out considering senses that have a 0 score for bothPos(s) and Neg(s).median: return the median of the senses orderedby polarity score.All these prior polarities formulae are comparedagainst two gold standards (one for regression, onefor classification) both one by one, as in the worksmentioned above, and combined together in a learn-ing framework (to see whether combining these fea-tures ?
that capture different aspect of prior polari-ties ?
can further improve the results).Finally, we implemented two variants of a priorpolarity random baseline to asses possible advan-tages of approaches using SWN:rnd.
This formula represents the basic baselinerandom approach.
It simply returns a random num-ber between -1 and 1 for any given lemma#PoS.swnrnd.
This formula represents an advancedrandom approach that incorporates some ?knowl-edge?
from SWN.
It takes the scores of a randomsense for the given lemma#PoS.
We believe thisis a fairer baseline than rnd since SWN informa-tion can possibly constrain the values.
A similar ap-proach has been used in (Qu et al 2008).5 Learning AlgorithmsWe used two non-parametric learning approaches,Support Vector Machines (SVMs) (Shawe-Taylorand Cristianini, 2004) and Gaussian Processes (GPs)(Rasmussen and Williams, 2006), to test the perfor-mance of all the metrics in conjunction.
SVMs arenon-parametric deterministic algorithms that havebeen widely used in several fields, in particular inNLP where they are the state-of-the-art for varioustasks.
GPs, on the other hand, are an extremely flex-ible non-parametric probabilistic framework able toexplicitly model uncertainty, that, despite being con-sidered state-of-the-art in regression, have rarelybeen used in NLP.
To our knowledge only two pre-vious works did so (Polajnar et al 2011; Cohn andSpecia, 2013).Both methods take advantage of the kernel trick,a technique used to embed the original feature spaceinto an alternative space where data may be linearlyseparable.
This is performed by the kernel func-tion that transforms the input data in a new structure,called kernel.
How it is used to produce the predic-tion is one of the main differences between SVMsand GPs.
In classification SVMs use the geomet-ric mean to discriminate between the positive andnegative classes, while the GP model uses the pos-terior probability distribution over each class.
Bothframeworks support learning algorithms for regres-sion and classification.
An exhaustive explanationof the two methodologies can be found in (Shawe-Taylor and Cristianini, 2004) and (Rasmussen andWilliams, 2006).In the SVM experiments, we use C-SVM and -SVM implemented in the LIBSVM toolbox (Changand Lin, 2011).
The selection of the kernel (linear,polynomial, radial basis function and sigmoid) andthe optimization of the parameters are carried outthrough grid search in 10-fold cross-validation.GP regression models with Gaussian noise are arare exception where the exact inference with like-lihood functions is tractable, see ?2 in (Rasmussen1262and Williams, 2006).
Unfortunately, this is not validfor the classification task ?
see ?3 in (Rasmussen andWilliams, 2006) ?
where an approximation methodis required.
In this work, we use the Laplace ap-proximation method proposed in (Williams and Bar-ber, 1998).
Different kernels are tested (covariancefor constant functions, linear with and without au-tomatic relevance determination (ARD)1, Matern,neural network, etc.2) and the linear logistic (lll)and probit regression (prl) likelihood functions areevaluated in classification.
In our classification ex-periments we tried all possible combinations of ker-nels and likelihood functions, while in the regressiontests we ranged only on different kernels.
All the GPmodels were implemented using the GPML Matlabtoolbox3.
Unlike SVMs, the optimization of the ker-nel parameters can be performed without using gridsearch, but the optimal parameters can be obtainediteratively, by maximizing the marginal likelihood(or in classification, the Laplace approximation ofthe marginal likelihood).
We fix at 100 the maxi-mum number of iterations.An interesting property of the GPs is their capa-bility of weighting the features differently accord-ing to their importance in the data.
This is referredto as the automatic variance determination kernel.As demonstrated in (Weston et al 2000), SVMscan benefit from the application of feature selec-tion techniques especially when there are highly re-dundant features.
Since the prior polarities formu-lae tend to cluster in groups that provide similar re-sults (Gatti and Guerini, 2012) ?
creating noise forthe learner ?
we want to understand whether featureselection approaches can boost the performance ofSVMs.
For this reason, we also test feature selectionprior to the SVM training.
For that we used Ran-domized Lasso, or stability selection (Meinshausenand Bu?hlmann, 2010).
Re-sampling of the trainingdata is performed several times and a Lasso regres-sion model is fit on each sample.
Features that ap-pear in a given number of samples are retained.
Boththe fraction of the data to be sampled and the thresh-old to select the features can be configured.
In our1linone and linard in the result tables, respectively.2More detailed information on the available kernels are in?4 (Rasmussen and Williams, 2006)3http://www.gaussianprocess.org/gpml/code/matlab/doc/experiments we set the sampling fraction to 75%,the selection threshold to 25% and the number of re-samples to 1,000.
We refer to these as SVMfs.6 Gold StandardsTo assess how well prior polarity formulae perform,a gold standard with word polarities provided by hu-man annotators is needed.
There are many such re-sources in the literature, each with different cover-age and annotation characteristics.
ANEW (Bradleyand Lang, 1999) rates the valence score of 1,034words, which were presented in isolation to anno-tators.
The SO-CAL entries (Taboada et al 2011)were collected from corpus data and then manu-ally tagged by a small number of annotators witha multi-class label.
These ratings were further vali-dated through crowdsourcing.
Other resources, suchas the General Inquirer lexicon (Stone et al 1966),provide a binomial classification (either positive ornegative) of sentiment-bearing words.
The resourcepresented in (Wilson et al 2005) uses a similar bi-nomial annotation for single words; another inter-esting resource is WordNetAffect (Strapparava andValitutti, 2004) but it labels words senses and it can-not be used for the prior polarity validation task.In the following we describe in detail the tworesources we used for our experiments, namelyANEW for the regression experiments and the Gen-eral Inquirer (GI) for the classification ones.6.1 ANEWANEW (Bradley and Lang, 1999) is a resource de-veloped to provide a set of normative emotional rat-ings for a large number of words (roughly 1 thou-sand) in the English language.
It contains a set ofwords that have been rated in terms of pleasure (af-fective valence), arousal, and dominance.
In par-ticular for our task we considered the valence di-mension.
Since words were presented to subjectsin isolation (i.e.
no context was provided) this re-source represents a human validation of prior polar-ities scores for the given words, and can be used as agold standard.
For each word ANEW provides twomain metrics: anew?, which correspond to the av-erage of annotators votes, and anew?, which givesthe variance in annotators scores for the given word.In the same way these metrics are also provided for1263the male/female annotator groups.6.2 General InquirerThe Harvard General Inquirer dictionary is a widelyused resource, built for automatic text analysis(Stone et al 1966).
Its latest revision4 contains11789 words, tagged with 182 semantic and prag-matic labels, as well as with their part of speech.Words and their categories were initially takenfrom the Harvard IV-4 Psychosociological Dictio-nary (Dunphy et al 1974) and the Lasswell ValueDictionary (Lasswell and Namenwirth, 1969).
Forthis paper we consider the Positiv and Negativcategories (1,915 words the former, 2,291 words thelatter, for a total of 4,206 affective words).7 ExperimentsIn order to use the ANEW dataset to measureprior polarities formulae performance, we had toassign a PoS to all the words to obtain the SWNlemma#PoS format.
To do so, we proceeded asfollows: for each word, check if it is present amongboth SWN1 and SWN3 lemmas; if not, lemmatizethe word with the TextPro tool suite (Pianta et al2008) and check if the lemma is present instead5.If it is not found (i.e., the word cannot be alignedautomatically), remove the word from the list (thiswas the case for 30 words of the 1,034 present inANEW).
The remaining 1,004 lemmas were thenassociated with all the PoS present in SWN to getthe final lemma#PoS.
Note that a lemma can havemore than one PoS, for example, writer is presentonly as a noun (writer#n), while yellow is presentas a verb, a noun and an adjective (yellow#v,yellow#n, yellow#a).
This gave us a list of1,484 words in the lemma#PoS format.In a similar way we pre-processed the GI wordsthat uses the generic modif label to indicate ei-ther adjective or adverb (noun and verb PoS wereinstead consistently used).
Finally, all the sense-disambiguated words in the lemma#PoS#n formatwere discarded (1,114 words out of the 4,206 wordswith positive or negative valence).4http://www.wjh.harvard.edu/?inquirer/5We did not lemmatize everything to avoid duplications (forexample, if we lemmatize the ANEW entry addicted, we obtainaddict, which is already present in ANEW).After the two datasets were built this way, weremoved the words for which the posScore andnegScore contained all 0 in both SWN1 andSWN3 (523 lemma#PoS for ANEW and 484 forthe GI dataset), since these words are not informa-tive for our experiments.
The final dataset included961 entries for ANEW and 2,557 for GI.
For eachlemma#PoS in GI and ANEW, we then applied theprior polarity formulae described in Section 4, usingboth SWN1 and SWN3 and annotated the results.According to the nature of the human labels (realnumbers or -1/1), we ran several regression and clas-sification experiments.
In both cases, each datasetwas randomly split into 70% for training and the re-maining for test.
This process was repeated 5 timesto generate different splits.
For each partition, opti-mization of the learning algorithm parameters wasperformed on the training data (in 10-fold cross-validation for SVMs).
Training and test sets werenormalized using the z-score.To evaluate the performance of our regression ex-periments on ANEW we used the Mean AbsoluteError (MAE), that averages the error over a giventest set.
Accuracy was used for the classification ex-periments on GI instead.
We opted for accuracy ?rather than F1 ?
since for us True Negatives havesame importance as True Positives.
For each experi-ments we reported the average performance and thestandard deviation over the 5 random splits.
In thefollowing sections, to check if there was a statisti-cally significant difference in the results, we usedStudent?s t-test for regression experiments, whilean approximate randomization test (Yeh, 2000) wasused for the classification experiments.In Tables 2 and 3, the results of regression exper-iments over the ANEW dataset, using SWN1 andSWN3, are presented.
The results of the classifica-tion experiments over the GI dataset, using SWN1and SWN3 are shown in Tables 4 and 5.
For thesake of interpretability, results are divided accord-ing to the main approaches: randoms, posterior-to-prior formulae, learning algorithms.
Note that forclassification we report the generics f and not thefm and fd variants.
In fact, both versions alwaysreturn the same classification answer (we are clas-sifying according to the sign of f result and not itsstrength).
For the GPs, we report the two best con-figurations only.1264MAE ?
MAE ?rnd 0.652 0.026swnrndm 0.427 0.011swnrndd 0.426 0.009uniwm 0.420 0.009maxm 0.419 0.009fsd 0.413 0.011fsm 0.412 0.009uni 0.410 0.010uniwd 0.406 0.007w1snm 0.405 0.011maxd 0.404 0.005w2snm 0.402 0.011mediand 0.401 0.014w1d 0.401 0.010w1nd 0.399 0.008meand 0.398 0.010w2d 0.398 0.010medianm 0.397 0.015w1snd 0.397 0.008w2snd 0.397 0.008w2nd 0.397 0.008w1sm 0.396 0.010w1m 0.396 0.010w1nm 0.394 0.009meanm 0.393 0.011w2sd 0.393 0.008w1sd 0.393 0.009w2sm 0.392 0.010w2m 0.391 0.011w2nm 0.391 0.012GPlinard 0.398 0.014GPlinone 0.398 0.014SVM 0.367 0.010SVMfs 0.366 0.011AVERAGE 0.398 0.010Table 2: MAE results for metrics using SWN18 General DiscussionIn this section we sum up the main results of ouranalysis, providing an answer to the various ques-tions we introduced at the beginning of the paper:SentiWordNet improves over random.
One ofthe first things worth noting ?
in Tables 2, 3, 4 and5 ?
is that the random approach (rnd), as expected,is the worst performing metric, while all other ap-proaches, based on SWN, have statistically signif-icant improvements both for MAE and for Accu-racy (p < 0.001).
So, using SWN for posterior-to-prior polarity computation brings benefits, sinceit increases the performance above the baseline inwords?
prior polarity assessment.SWN3 is better than SWN1.
With respect toMAE ?
MAE ?rnd 0.652 0.026swnrndd 0.404 0.013swnrndm 0.402 0.010maxm 0.393 0.009fsd 0.382 0.008uniwm 0.382 0.015fsm 0.381 0.010medianm 0.377 0.008uniwd 0.377 0.012mediand 0.377 0.011uni 0.376 0.010maxd 0.372 0.011meand 0.371 0.010w1snm 0.371 0.011w2snm 0.369 0.010w1d 0.368 0.010w2d 0.367 0.010meanm 0.367 0.010w1m 0.365 0.010w2snd 0.364 0.011w1snd 0.364 0.010w1sm 0.363 0.009w1nd 0.362 0.009w2sd 0.362 0.010w2m 0.362 0.010w1sd 0.362 0.009w1nm 0.362 0.007w2nd 0.361 0.010w2sm 0.360 0.009w2nm 0.359 0.009GPlinone 0.356 0.008GPlinard 0.355 0.008SVM 0.333 0.004SVMfs 0.333 0.003AVERAGE 0.366 0.009Table 3: MAE results for regression using SWN3SWN1, using SWN3 enhances performance, bothin regression (MAE ?
0.398 vs. 0.366, p < 0.001)and classification (Accuracy ?
0.710 vs. 0.771,p < 0.001) tasks.
Since many of the approachesdescribed in the literature use SWN1 their resultsshould be revised and SWN3 should be used asstandard.
This difference in performance can bepartially explained by the fact that, even after pre-processing, for the ANEW dataset 137 lemma#PoShave all senses equal to 0 in SWN1, while in SWN3they are just 48.
In the GI lexicon the numbers are233 for SWN1 and 69 for SWN3.Not all formulae are created equal.
The formu-lae described in Section 4 have very different results,along a continuum.
While inspecting every differ-1265Acc.
?
Acc.
?rnd 0.447 0.019swn rndm 0.639 0.026swn rndd 0.646 0.021fs m 0.659 0.020uni 0.684 0.017median 0.686 0.022uniw 0.702 0.019max 0.710 0.022w1 0.712 0.021w1n 0.713 0.022w2n 0.714 0.023w2 0.715 0.021mean 0.718 0.023w2s 0.719 0.023w2sn 0.719 0.023w1s 0.719 0.023w1sn 0.719 0.023GP llllinard 0.721 0.026GP prllinard 0.722 0.025SVM 0.733 0.021SVMfs 0.743 0.021Average 0.710 0.022Table 4: Accuracy results for classification using SWN1ence in performance is out of the scope of the presentpaper, we can see that there is a strong difference be-tween best and worst performing formulae both inregression (in Table 2 w2nm is better than uniwm,in Table 3 w2nm is better than maxm) and classifi-cation (in Table 4 w1snm is better than fsm,in Ta-ble 5 w2m is better than fsm) and these differencesare all statistically significant (p < 0.001).
Again,these results indicate that the previous experimentsin the literature that use SWN as a baseline shouldbe revised to take these results into account.
Further-more, the new formulae we introduced, based on the?posterior polarities saliency?
hypothesis, proved tobe among the best performing in all experiments.This entails that there is room for inspecting newformulae variants other than those already proposedin the literature.Selecting just one sense is not a good choice.On a side note, the approaches that rely on only onesense polarity (namely fs, median and max) havesimilar results which do not differ significantly fromswnrnd (for maxm, fsd and fsm in Table 2, andfor maxm in Table 3).
These same approaches arealso far from the best performing formulae: in Ta-ble 3, mediand differs from w2nm (p < 0.05), asdo maxm, maxd, fsm and fsd (p < 0.001); in Ta-Acc.
?
Acc.
?rnd 0.447 0.019swn rndd 0.700 0.030swn rndm 0.706 0.034fs 0.723 0.014medianm 0.742 0.016uni 0.750 0.015uniw 0.762 0.023max 0.769 0.019w2s 0.777 0.017w2sn 0.777 0.017w1s 0.777 0.017w1sn 0.777 0.017w1n 0.780 0.021w2n 0.780 0.022mean 0.781 0.018w1 0.781 0.021w2 0.781 0.021SVM 0.779 0.016GPl 0.779 0.018GPg 0.781 0.018SVMfs 0.792 0.014Average 0.771 0.018Table 5: Accuracy results for classification using SWN3ble 3, fs, max and median in both their fm and fdvariants are significantly different from the best per-forming w2nm (p < 0.001).
For classification, inTable 4 and 5 the difference between the correspond-ing best performing formula and the single sensesformulae is always significant (at least p < 0.01).Among other things, this finding entails, surpris-ingly, that taking the first sense of a lemma#PoS insome cases has no improvement over taking a ran-dom sense, and that in all cases it is one of the worstapproaches with SWN .
This is surprising since inmany NLP tasks, such as word sense disambigua-tion, algorithms based on most frequent sense repre-sent a very strong baseline6.Learning improvements.
Combining the formu-lae in a learning framework further improves theresults over the best performing formulae, both inregression (MAE?
with SWN1 0.366 vs. 0.391,p < 0.001; MAE?
with SWN3 0.333 vs. 0.359,p < 0.001) and in classification (Accuracy?
forSWN1 is 0.743 vs. 0.719, p < 0.001; Accuracy?for SWN3 is 0.792 vs. 0.781, not significant p =0.07).
Another thing worth noting is that, in re-gression, GPs are outperformed by both versions of6In SemEval 2010, only 5 participants out of 29 performedbetter than the most frequent threshold (Agirre et al 2010).1266SVM (p < 0.001), see Tables 2 and 3.
This is incontrast with the results presented in (Cohn and Spe-cia, 2013), where GPs on the single task are on av-erage better than SVMs.
In classification, GPs havesimilar performance to SVM without feature selec-tion, and in some cases (see Table 5) even slightlybetter.
Analyzing the selected kernels for GPs andSVMs, we notice that in most of the splits SVMsprefer the radial based function, while the best per-formance with the GPs are obtained with linear ker-nels with and without ARD.
There is no significantdifference in using linear logistic and probit regres-sion likelihoods.
In all our experiments, SVM withfeature selection leads to the best performance.
Thisis not surprising due the high level of redundancyin the formulae scores.
Interestingly, inspecting themost frequent selected features by SVMfs, we seethat features from different groups are selected, andeven the worst performing formulae can add infor-mation, confirming the idea that viewing the sameinformation from different perspectives (i.e.
the pos-terior polarities provided by SWN combined in var-ious ways) can give better predictions.To sum up: the new state-of-the-art performancelevel in prior-polarity computation is representedby the SVMfs approach using SWN3, and thisshould be used as the reference from now on.9 PoS and Gender ExperimentsNext, we wanted to understand if the performance ofour approach, using SWN3, was consistent acrossword PoS.
In Table 6 we report the results for thebest performing formulae and learning algorithm onthe GI PoS classes.
In particular for ADJ there are1,073 words, 922 for NOUN and 508 for VERB.
Wediscarded adverbs since the class was too small toallow reliable evaluation and efficient learning (only54 instances).
The results show a greater accuracyfor adjectives (p < 0.01), while performance fornouns and verbs are similar.SVMfs best fAcc.
?
Acc.
?
Acc.
?
Acc.
?ADJ 0.829 0.019 0.821 0.016NOUN 0.784 0.021 0.765 0.023VERBS 0.782 0.052 0.744 0.046Table 6: Accuracy results for PoS using SWN3Finally we test against the male and female ratingsprovided by ANEW.
As can be seen from Table 7,SWN approaches are far more precise in predictingMale judgments rather than Female ones (MAE?goes from 0.392 to 0.323 with the best formula andfrom 0.369 to 0.292 with SVMfs, both differencesare significant p < 0.001).
Instead, in Table 8 ?which displays the results along gender and polaritydimensions ?
there is no statistically significant dif-ference in MAE on positive words between maleand female, while there is a strong statistical signifi-cance for negative words (p < 0.001).Interestingly, there is also a large difference be-tween positive and negative affective words (bothfor male and female dimensions).
This differenceis maximum for male scores on positive words com-pared to female scores on negative words (0.283 vs.0.399, p < 0.001).
Recent work by Warriner et al(2013) inspected the differences in prior polarity as-sessment due to gender.At this stage we can only note that prior polari-ties calculated with SWN are closer to ANEW maleannotations than female ones.
Understanding whythis happens would require an accurate examinationof the methods used to create WordNet and SWN(which will be the focus of our future work).Male femaleMAE ?
MAE ?
MAE ?
MAE ?SVMfs 0.292 0.020 0.369 0.008best f 0.323 0.022 0.392 0.010Table 7: MAE results for Male vs Female using SWN3Male femaleMAE ?
MAE ?
MAE ?
MAE ?Pos 0.283 0.022 0.340 0.009Neg 0.301 0.029 0.399 0.013Table 8: MAE for Male/Female - Pos/Neg using SWN310 ConclusionsWe have presented a study on the posterior-to-priorpolarity issue, i.e.
the problem of computing words?prior polarity starting from their posterior polarities.Using two different versions of SentiWordNet and30 different approaches that have been proposed inthe literature, we have shown that researchers havenot paid sufficient attention to this issue.
Indeed, we1267showed that the better variants outperform the oth-ers on different datasets both in regression and clas-sification tasks, and that they can represent a fairerstate-of-art baseline approach using SentiWordNet.On top of this, we also showed that these state-of-the-art formulae can be further outperformed usinga learning framework that combines the various for-mulae together.
We conclude our analysis with someexperiments investigating the impact of word PoSand annotator gender in gold standards, showing in-teresting phenomena that requires further investiga-tion.AcknowledgmentsThe authors thanks Jose?
Camargo De Souza for hishelp with feature selection.
This work has been par-tially supported by the Trento RISE PerTe project.ReferencesE.
Agirre, O.L.
De Lacalle, C. Fellbaum, S.K.
Hsieh,M.
Tesconi, M. Monachini, P. Vossen, and R. Segers.2010.
Semeval-2010 task 17: All-words word sensedisambiguation on a specific domain.
In Proceedingsof the 5th International Workshop on Semantic Evalu-ation (IWSE ?10), pages 75?80, Uppsala, Sweden.S.
Agrawal and T.J. Siddiqui.
2009.
Using syntactic andcontextual information for sentiment polarity analysis.In Proceedings of the 2nd International Conference onInteraction Sciences: Information Technology, Cultureand Human (ICIS ?09), pages 620?623, Seoul, Repub-lic of Korea.S.
Baccianella, A. Esuli, and F. Sebastiani.
2010.
Senti-WordNet 3.0: An enhanced lexical resource for sen-timent analysis and opinion mining.
In Proceed-ings of the 7th Conference on International LanguageResources and Evaluation (LREC ?10), pages 2200?2204, Valletta, Malta.M.M.
Bradley and P.J.
Lang.
1999.
Affective norms forEnglish words (ANEW): Instruction manual and af-fective ratings.
Technical Report C-1, University ofFlorida.C.C.
Chang and C.J.
Lin.
2011.
LIBSVM: A libraryfor support vector machines.
ACM Transactions onIntelligent Systems and Technology, 2:27:1?27:27.F.R.
Chaumartin.
2007.
UPAR7: A knowledge-basedsystem for headline sentiment tagging.
In Proceedingsof the 4th International Workshop on Semantic Evalu-ations (IWSE ?07), pages 422?425, Prague, Czech Re-public.F.M.
Chowdhury, M. Guerini, S. Tonelli, and A. Lavelli.2013.
Fbk: Sentiment analysis in twitter with tweet-sted.
In Second Joint Conference on Lexical and Com-putational Semantics (*SEM): Proceedings of the Sev-enth International Workshop on Semantic Evaluation(SemEval ?13), volume 2, pages 466?470, Atlanta,Georgia, USA, June.T.
Cohn and L. Specia.
2013.
Modelling annotator biaswith multi-task gaussian processes: An application tomachine translation quality estimation.
In Proceed-ings of the 51th Annual Meeting of the Associationfor Computational Linguistics (ACL ?13), pages 32?42, Sofia, Bulgaria.K.
Denecke.
2008.
Accessing medical experiencesand information.
In Proceedings of the 18th Euro-pean Conference on Artificial Intelligence, Workshopon Mining Social Data (MSoDa ?08), Patras, Greece.K.
Denecke.
2009.
Are SentiWordNet scores suited formulti-domain sentiment classification?
In Proceed-ings of the 4th International Conference on DigitalInformation Management (ICDIM ?09), pages 32?37,Ann Arbor, MI, USA.A.
Devitt and K. Ahmad.
2007.
Sentiment polarityidentification in financial news: A cohesion-based ap-proach.
In Proceedings of the 45th Annual Meetingof the Association for Computational Linguistics (ACL?07), pages 984?991, Prague, Czech Republic.D.C.
Dunphy, C.G.
Bullard, and E.E.M.
Crossing.
1974.Validation of the General Inquirer Harvard IV Dictio-nary.
Paper presented at the Pisa Conference on Con-tent Analysis.A.
Esuli and F. Sebastiani.
2006.
SentiWordNet: Apublicly available lexical resource for opinion min-ing.
In Proceedings of the 5th Conference on Inter-national Language Resources and Evaluation (LREC?06), pages 417?422, Genova, Italy.L.
Gatti and M. Guerini.
2012.
Assessing sentimentstrength in words prior polarities.
In Proceedings ofthe 24th International Conference on ComputationalLinguistics (COLING ?12), pages 361?370, Mumbai,India.R.
Giora.
1997.
Understanding figurative and literal lan-guage: The graded salience hypothesis.
Cognitive Lin-guistics, 8:183?206.M.
Guerini, O.
Stock, and C. Strapparava.
2008.Valentino: A tool for valence shifting of natural lan-guage texts.
In Proceedings of the 6th InternationalConference on Language Resources and Evaluation(LREC ?08), pages 243?246, Marrakech, Morocco.H.D.
Lasswell and J.Z.
Namenwirth.
1969.
The Lasswellvalue dictionary.
New Haven.B.
Liu and L. Zhang.
2012.
A survey of opinion miningand sentiment analysis.
Mining Text Data, pages 415?463.1268N.
Meinshausen and P. Bu?hlmann.
2010.
Stability selec-tion.
Journal of the Royal Statistical Society: Series B(Statistical Methodology), 72(4):417?473.A.
Neviarouskaya, H. Prendinger, and M. Ishizuka.2009.
Sentiful: Generating a reliable lexicon forsentiment analysis.
In Proceedings of the 3rd Affec-tive Computing and Intelligent Interaction (ACII ?09),pages 363?368, Amsterdam, Netherlands.A.
Neviarouskaya, H. Prendinger, and M. Ishizuka.2011.
Affect analysis model: novel rule-based ap-proach to affect sensing from text.
Natural LanguageEngineering, 17(1):95.G.
O?zbal and C. Strapparava.
2012.
A computational ap-proach to the automation of creative naming.
In Pro-ceedings of the 50th Annual Meeting of the Associationfor Computational Linguistics (ACL ?12), pages 703?711, Jeju Island, Korea.G.
O?zbal, C. Strapparava, and M. Guerini.
2012.
BrandPitt: A corpus to explore the art of naming.
In Pro-ceedings of the 8th International Conference on Lan-guage Resources and Evaluation (LREC ?12), pages1822?1828, Istanbul, Turkey.G.
Paltoglou, M. Thelwall, and K. Buckley.
2010.
On-line textual communications annotated with grades ofemotion strength.
In Proceedings of the 3rd Interna-tional Workshop of Emotion: Corpora for research onEmotion and Affect (satellite of LREC ?10), pages 25?31, Valletta, Malta.B.
Pang and L. Lee.
2008.
Opinion mining and senti-ment analysis.
Foundations and Trends in InformationRetrieval, 2(1-2):1?135.E.
Pianta, C. Girardi, and R. Zanoli.
2008.
The TextProtool suite.
In Proceedings of the 6th InternationalConference on Language Resources and Evaluation(LREC ?08), pages 2603?2607, Marrakech, Morocco.I.
Piller.
2003.
Advertising as a site of language contact.Annual Review of Applied Linguistics, 23:170?183.T.
Polajnar, S. Rogers, and M. Girolami.
2011.
Proteininteraction detection in sentences via gaussian pro-cesses: a preliminary evaluation.
International jour-nal of data mining and bioinformatics, 5(1):52?72.L.
Qu, C. Toprak, N. Jakob, and I. Gurevych.
2008.Sentence level subjectivity and sentiment analysis ex-periments in NTCIR-7 MOAT challenge.
In Proceed-ings of the 7th NTCIR Workshop Meeting (NTCIR ?08),pages 210?217, Tokyo, Japan.C.E.
Rasmussen and C.K.I.
Williams.
2006.
Gaussianprocesses for machine learning.
MIT Press.J.
Shawe-Taylor and N. Cristianini.
2004.
Kernel meth-ods for pattern analysis.
Cambridge university press.J.K.
Sing, S. Sarkar, and T.K.
Mitra.
2012.
Devel-opment of a novel algorithm for sentiment analysisbased on adverb-adjective-noun combinations.
In Pro-ceedings of the 3rd National Conference on EmergingTrends and Applications in Computer Science (NC-ETACS ?12), pages 38?40, Shillong, India.P.J.
Stone, D.C. Dunphy, and M.S.
Smith.
1966.
TheGeneral Inquirer: A Computer Approach to ContentAnalysis.
MIT press.C.
Strapparava and A. Valitutti.
2004.
WordNet-Affect:an affective extension of WordNet.
In Proceedingsof the 4th International Conference on Language Re-sources and Evaluation (LREC ?04), pages 1083 ?1086, Lisbon, Portugal.M.
Taboada, J. Brooke, M. Tofiloski, K. Voll, andM.
Stede.
2011.
Lexicon-based methods for senti-ment analysis.
Computational linguistics, 37(2):267?307.T.T.
Thet, J.C. Na, C.S.G.
Khoo, and S. Shakthikumar.2009.
Sentiment analysis of movie reviews on dis-cussion boards using a linguistic approach.
In Pro-ceedings of the 1st international CIKM workshop onTopic-sentiment analysis for mass opinion (TSA ?09),pages 81?84, Hong Kong.S.
Wang and C.D.
Manning.
2012.
Baselines and bi-grams: Simple, good sentiment and topic classifica-tion.
In Proceedings of the 50th Annual Meeting of theAssociation for Computational Linguistics (ACL ?12),pages 90?94, Jeju Island, Korea.A.B.
Warriner, V. Kuperman, and M. Brysbaert.
2013.Norms of valence, arousal, and dominance for 13,915english lemmas.
Behavior research methods, pages 1?17.J.
Weston, S. Mukherjee, O. Chapelle, M. Pontil, T. Pog-gio, and V. Vapnik.
2000.
Feature selection for SVMs.In Proceedings of the 14th Conference on Neural In-formation Processing Systems (NIPS ?00), pages 668?674, Denver, CO, USA.C.K.I.
Williams and D. Barber.
1998.
Bayesian clas-sification with gaussian processes.
Pattern Analy-sis and Machine Intelligence, IEEE Transactions on,20(12):1342?1351.T.
Wilson, J. Wiebe, and R. Hwa.
2004.
Just how madare you?
Finding strong and weak opinion clauses.
InProceedings of the 19th National Conference on Artifi-cial Intelligence (AAAI ?04), pages 761?769, San Jose,CA, USA.T.
Wilson, J. Wiebe, and P. Hoffmann.
2005.
Recogniz-ing contextual polarity in phrase-level sentiment anal-ysis.
In Proceedings of the Conference on HumanLanguage Technology and Empirical Methods in Nat-ural Language Processing (HLT/EMNLP ?05), pages347?354, Vancouver, Canada.A.
Yeh.
2000.
More accurate tests for the statistical sig-nificance of result differences.
In Proceedings of the18th International Conference on Computational Lin-guistics (COLING ?00), pages 947?953, Saarbru?cken,Germany.1269
