Proceedings of the 7th Workshop on Statistical Machine Translation, pages 71?75,Montre?al, Canada, June 7-8, 2012. c?2012 Association for Computational LinguisticsClass error rates for evaluation of machine translation outputMaja Popovic?German Research Center for Artificial Intelligence (DFKI)Language Technology (LT), Berlin, Germanymaja.popovic@dfki.deAbstractWe investigate the use of error classificationresults for automatic evaluation of machinetranslation output.
Five basic error classes aretaken into account: morphological errors, syn-tactic (reordering) errors, missing words, ex-tra words and lexical errors.
In addition, lin-ear combinations of these categories are in-vestigated.
Correlations between the class er-ror rates and human judgments are calculatedon the data of the third, fourth, fifth and sixthshared tasks of the Statistical Machine Trans-lation Workshop.
Machine translation outputsin five different European languages are used:English, Spanish, French, German and Czech.The results show that the following combina-tions are the most promising: the sum of allclass error rates, the weighted sum optimisedfor translation into English and the weightedsum optimised for translation from English.1 IntroductionRecent investigations have shown that it is possi-ble to carry out a reliable automatic error analysisof a given translation output in order to get moreinformation about actual errors and details aboutparticular strengthnesses and weaknesses of a sys-teml (Popovic?
and Ney, 2011).
The obtained resultscorrelate very well with the human error classifica-tion results.
The question we try to answer is: howthe class error rates correlate with the human eval-uation (ranking) results?
As a first step, we inves-tigate the correlations of five basic class error rateswith human rankings.
In the next step, linear com-binations (sums) of basic class error rates are inves-tigated.Spearman?s rank correlation coefficients on thedocument (system) level between all the metrics andthe human ranking are computed on the English,French, Spanish, German and Czech texts gener-ated by various translation systems in the frame-work of the third (Callison-Burch et al, 2008),fourth (Callison-Burch et al, 2009), fifth (Callison-Burch et al, 2010) and sixth (Callison-Burch et al,2011) shared translation tasks.2 Class error ratesIn this work, the method proposed in (Popovic?and Ney, 2011) is used, i.e.
classification ofthe translation errors into five basic categoriesbased on the Word Error Rate (WER) (Levenshtein,1966) together with the recall- and precision-basedPosition-independent Error Rates called ReferencePER (RPER) and Hypothesis PER (HPER).As a result of an error classification, two valuesare usually of interest: raw error counts for each er-ror class, and error rates for each class, i.e.
raw errorcounts normalised over the total number of runningwords.
Which of the values is preferred depends ofthe exact task.
For example, if only a distributionof error types within a translation output is of in-terest, the raw error counts are sufficient.
On theother hand, if we want to compare different transla-tion outputs, normalised values i.e.
error rates aremore suitable.
Therefore they are appropriate candi-dates to be used for the evaluation task.In this work, we explore the error rates calculatedon the word level as well as on the block level, where71a group of consecutive words labelled with the sameerror category is called a block.
The normalisationin both cases is carried out over the total number ofrunning words.
Therefore the block level error ratefor a particular error class is always less or equalthan the corresponding word level error rate.2.1 Basic class error ratesThe following five basic class error rates are ex-plored:INFER (inflectional error rate):Number of words translated into correct baseform but into incorrect full form, normalisedover the hypothesis length.RER (reordering error rate):Number of incorrectly positioned words nor-malised over the hypothesis length.MISER (missing word error rate):Number of words which should appear in thetranslation hypothesis but do not, normalisedover the reference length.EXTER (extra word error rate):Number of words which appear in the transla-tion hypothesis but should not, normalised overthe hypothesis length.LEXER (lexical error rate):Number of words translated into an incorrectlexical choice in the target language (false dis-ambiguation, unknown/untranslated word, in-correct terminology, etc.)
normalised over thehypothesis length.Table 1 presents an example of word and blocklevel class error rates.
Each erroneous word is la-belled with the corresponding error category, and theblocks are marked within the parentheses { and }.The error rates on the block level are marked with aletter ?b?
at the beginning.
It should be noted thatthe used method at its current stage does not enableassigning multiple error tags to one word.2.2 Combined error rates (sums)The following linear combinations (sums) of the ba-sic class error rates are investigated:reference:The famous journalist Gustav Chalupa ,born in C?eske?
Bude?jovice ,also confirms this .hypothesis containing 14 running words:The also confirms the famousAustrian journalist Gustav Chalupa ,from Budweis Lamborghini .hypothesis labelled with error classes:The {alsoorder confirmsorder}{theextra} {famousorder} {Austrianextra}{journalistorder Gustavorder Chalupaorder} ,{fromlex Budweislex Lamborghinilex} .class error rates:word order:RER = 6/14 = 42.8%bRER = 3/14 = 21.4%extra words:EXTER = 2/14 = 14.3%bEXTER = 2/14 = 14.3%lexical errors:LEXER = 3/14 = 21.4%bLEXER = 1/14 = 7.1%Table 1: Example of word and block level class errorrates: the word groups within the parentheses { and } areconsidered as blocks; all error rates are normalised overthe hypothesis length, i.e.
14 running words.W?ER (sum of word level error rates)1 :Sum of all basic class error rates on the wordlevel;B?ER (sum of block level error rates):Sum of all basic class error rates on the blocklevel;WB?ER (sum of word and block level error rates):Arithmetic mean of W?ER and B?ER.1This error rate has already been introduced in (Popovic?
andNey, 2011) and called ?ER; however, for the sake of clarity, inthis work we will call it W?ER, i.e.
word level ?ER.72XEN?ER (X?English sum of error rates):Linear interpolation of word level and blocklevel class error rates optimised for translationinto English;ENX?ER (English?X sum of error rates):Linear interpolation of word level and blocklevel class error rates optimised for translationfrom English.For the example sentence shown in Table 1,W?ER = 84.7%, B?ER = 46.2% and WB?ER =65.4%.
XEN?ER and ENX?ER are weighted sumswhich will be explained in the next section.The prerequisite for the use of the described met-rics is availability of an appropriate morphologicalanalyser for the target language which provides baseforms of the words.3 Experiments on WMT 2008, 2009, 2010and 2011 test data3.1 Experimental set-upThe class error rates described in Section 2 wereproduced for outputs of translations from Spanish,French, German and Czech into English and viceversa using Hjerson (Popovic?, 2011), an open-source tool for automatic error classification.
Span-ish, French, German and English base forms wereproduced using the TreeTagger2, and the Czech baseforms using Morc?e (Spoustova?
et al, 2007).
In thisway, all references and hypotheses were providedwith the base forms of the words.For each error rate, the system level Spearmancorrelation coefficients ?
with human ranking werecalculated for each document.
In total, 40 correla-tion coefficients were obtained for each error rate ?twelve English outputs from the WMT 2011, 2010and 2009 task and eight from the WMT 2008 task,together with twenty outputs in other four target lan-guages.
For further analysis, the obtained corre-lation results were summarised into the followingthree values:?
meanaverage correlation coefficient;2http://www.ims.uni-stuttgart.de/projekte/corplex/TreeTagger/?
rank>percentage of documents where the particularerror rate has better correlation than the othererror rates;?
rank?percentage of documents where the particularerror rate has better or equal correlation thanthe other error rates.3.2 Comparison of basic class error ratesOur first experiment was to compare correlations forthe basic set of class error rates in order to investi-gate a general behaviour of each class error rate andto see if some of the error categories are particularly(in)convenient for the evaluation task.
Since certaindifferences between English and non-English trans-lation outputs are observed for some error classes,the values described in the previous section werealso calculated separately.Table 2 presents the results of this experiment.The mean values over all documents, over the En-glish documents and over the non-English docu-ments are shown.According to the overall mean values, the mostpromising error categories are lexical and reorder-ing errors.
However, the mean values for Englishoutputs are significantly different than those for non-English outputs: the best error classes for Englishare in deed lexical and reordering errors, howeverfor the non-English outputs the inflectional errorsand missing words have higher correlations.
On theother hand, for the English outputs missing wordshave even negative correlations, whereas correla-tions for inflectional errors are relatively low.
Theextra word class seems to be the least convenient ingeneral, especially for non-English outputs.Therefore, the rank?
values were calculated onlyseparately for English and non-English outputs, andthe previous observations were confirmed: for theEnglish outputs lexical and reordering errors are themost relevant, whereas for the non-English outputsall classes except extra words are almost equally im-portant.Apart from this, it can be noticed that the group-ing of words into blocks significantly improves cor-relation for reordering errors.
The reason for thisis ambiguity of tagging words as reordering errors.73error mean rank?rate overall x?en en?x x?en en?xINFER 0.398 0.190 0.595 46.2 71.7RER 0.360 0.344 0.373 53.8 51.1MISER 0.173 -0.101 0.434 26.3 54.4EXTER 0.032 0.212 -0.195 42.7 12.2LEXER 0.508 0.669 0.355 86.0 58.3bINFER 0.423 0.211 0.624 47.9 75.6bRER 0.508 0.594 0.426 78.3 60.0bMISER 0.169 -0.121 0.446 21.1 53.9bEXTER -0.031 0.186 -0.238 36.8 10.0bLEXER 0.515 0.634 0.402 79.5 62.8Table 2: mean and rank?
values for each basic word leveland block level error rate over all documents, over En-glish documents and over non-English documents.For example, if the translation reference is ?a verygood translation?, and the obtained hypothesis is ?atranslation very good?
, one possibility is to markthe word ?translation?
as reordering error, anotherpossibility is to mark the words ?very good?
as re-ordering errors, and it is also possible to mark all thewords as reordering errors.
In such cases, the group-ing of consecutive word level errors into blocks isbeneficial.3.3 Comparison of error rate sumsA first step towards combining the basic class errorrates was investigation of simple sums, i.e.
W?ER,B?ER as well as WB?ER as arithmetic mean of pre-vious two.
The overall average correlation coeffi-cients of the sums were shown to be higher thanthose of the basic class error rates.
Further exper-iments have been carried out taking into account theresults described in the previous section.
Firstly, ex-tra word class was removed from all sums, howeverno improvement of correlation coefficients was ob-served.
Then the sums containing only the mostpromising error categories separately for Englishand non-English output were investigated, but thisalso resulted in no improvements.
Finally, we in-troduced weights for each translation direction ac-cording to the rank?
value for each of the basicclass error rates (see Table 2), and this approachwas promising.
In this way, the specialised sumsXEN?ER and ENX?ER were introduced.In Table 3 the results for all five error rate sumsare presented.
mean, rank> and rank?
values arepresented over all translation outputs, over Englishoutputs and over non-English outputs.
As alreadymentioned, the overall correlation coefficients of thesums are higher than those of the basic class errorrates.
This could be expected, since summing classerror rates is oriented towards the overall quality ofthe translation output whereas the class error ratesare giving more information about details.According to the overall values, the best error rateis combination of all word and block level class er-ror rates, i.e.
WB?ER followed by the block sumB?ER, whereas the W?ER and the specialised sumsXEN?ER and ENX?ER have lower correlations.For the translation into English, this error rate is alsovery promising, followed by the specialised sumXEN?ER.
On the other hand, for the translationfrom English, the most promising error rates are theblock sum B?ER and the corresponding specialisedsum ENX?ER.
Following these observations, wedecided to submit WB?ER scores for all transla-tion outputs together with XEN?ER and ENX?ERscores, each one for the corresponding translationdirection.
In addition, we submitted B?ER scoressince this error rate also showed rather good results,especially for the translation out of English.4 ConclusionsThe presented results show that the error classifica-tion results can be used for evaluation and rankingof machine translation outputs.
The most promis-ing way to do it is to sum all word level and blocklevel error rates, i.e.
to produce the WB?ER errorrate.
This error rate has eventually been submittedto the WMT 2012 evaluation task.
In addition, thenext best metrics have been submitted, i.e.
the blocklevel sum B?ER for all translation directions, andthe specialised sums XEN?ER and ENX?ER eachfor the corresponding translation outputs.The experiments described in this work are still atearly stage: promising directions for future work arebetter optimisation of weights3, further investigationof each language pair and also of each non-English3First steps have already been made in this direction usingan SVM classifier, and the resulting evaluation metric has alsobeen submitted to the WMT 2012.74error rate mean rank?
rank>overall x?en en?x overall x?en en?x overall x?en en?xW?ER 0.616 0.694 0.541 55.1 50.0 61.2 39.1 48.6 36.2B?ER 0.629 0.666 0.594 60.3 55.2 68.8 46.1 39.5 52.5WB?ER 0.639 0.696 0.585 68.0 67.1 63.7 48.7 52.6 45.0XEN?ER 0.587 0.692 0.487 51.9 63.2 41.2 37.8 52.6 23.7ENX?ER 0.599 0.595 0.602 50.6 38.1 62.5 39.1 32.9 45.0Table 3: mean, rank?
and rank> values for error rate sums compared over all documents, over English documentsand over non-English documents.target language separately, filtering error categoriesby POS classes, etc.AcknowledgmentsThis work has partly been developed within theTARAXU?
project4 financed by TSB Technologies-tiftung Berlin ?
Zukunftsfonds Berlin, co-financedby the European Union ?
European fund for regionaldevelopment.
Special thanks to Mark Fishel andOndr?ej Bojar.ReferencesChris Callison-Burch, Cameron Fordyce, Philipp Koehn,Christof Monz, and Josh Schroeder.
2008.
FurtherMeta-Evaluation of Machine Translation.
In Proceed-ings of the 3rd ACL 08 Workshop on Statistical Ma-chine Translation (WMT 2008), pages 70?106, Colum-bus, Ohio, June.Chris Callison-Burch, Philipp Koehn, Christof Monz,and Josh Schroeder.
2009.
Findings of the 2009Workshop on Statistical Machine Translation.
In Pro-ceedings of the Fourth Workshop on Statistical Ma-chine Translation (WMT 2009), pages 1?28, Athens,Greece, March.Chris Callison-Burch, Philipp Koehn, Christof Monz,Kay Peterson, Mark Przybocki, and Omar Zaidan.2010.
Findings of the 2010 Joint Workshop on Sta-tistical Machine Translation and Metrics for MachineTranslation.
In Proceedings of the Joint Fifth Work-shop on Statistical Machine Translation and Metrics-MATR (WMT 2010), pages 17?53, Uppsala, Sweden,July.Chris Callison-Burch, Philipp Koehn, Christof Monz,and Omar Zaidan.
2011.
Findings of the 2011 Work-shop on Statistical Machine Translation.
In Proceed-ings of the Sixth Workshop on Statistical Machine4http://taraxu.dfki.de/Translation (WMT 2011), pages 22?64, Edinburgh,Scotland, July.Vladimir Iosifovich Levenshtein.
1966.
Binary CodesCapable of Correcting Deletions, Insertions and Re-versals.
Soviet Physics Doklady, 10(8):707?710,February.Maja Popovic?
and Hermann Ney.
2011.
Towards Au-tomatic Error Analysis of Machine Translation Out-put.
Computational Linguistics, 37(4):657?688, De-cember.Maja Popovic?.
2011.
Hjerson: An Open Source Toolfor Automatic Error Classification of Machine Trans-lation Output.
The Prague Bulletin of MathematicalLinguistics, (96):59?68, October.Drahom?
?ra Spoustova?, Jan Hajic?, Jan Votrubec, Pavel Kr-bec, and Pavel Kve?ton?.
2007.
The best of two worlds:Cooperation of statistical and rule-based taggers forczech.
In Proceedings of the Workshop on Balto-Slavonic Natural Language Processing, ACL 2007,pages 67?74, Praha.75
