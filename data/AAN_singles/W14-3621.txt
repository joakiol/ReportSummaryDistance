Proceedings of the EMNLP 2014 Workshop on Arabic Natural Langauge Processing (ANLP), pages 155?159,October 25, 2014, Doha, Qatar.c?2014 Association for Computational LinguisticsTECHLIMED system description for the Shared Task on AutomaticArabic Error CorrectionDjamel MOSTEFATechlimed42 rue de l?Universit?eLyon, FranceOmar ASBAYOUTechlimed42 rue de l?Universit?eLyon, France{firstname.lastname}@techlimed.comRamzi ABBESTechlimed42 rue de l?Universit?eLyon, FranceAbstractThis article is a system description paperand reports on the participation of Tech-limed in the ?QALB-2014 shared task?
onevaluation of automatic arabic error cor-rection systems organized in conjunctionwith the EMNLP 2014 Workshop on Ara-bic Natural Language Processing.
Cor-recting automatically texts in Arabic is achallenging task due to the complexity andrich morphology of the Arabic languageand the lack of appropriate resources, (e.g.publicly available corpora and tools).
Todevelop our systems, we considered sev-eral approaches from rule based systemsto statistical methods.
Our results on thedevelopment set show that the statisticalsystem outperforms the lexicon driven ap-proach with a precision of 71%, a recall of50% and a F-measure of 59%.1 IntroductionAutomatic error correction is an important taskin Natural Language Processing (NLP).
It can beused in a wide range of applications such as wordprocessing tools (e.g.
Microsoft Office, Openof-fice, .
.
.
), machine translation, information re-trieval, optical character recognition .
.
.
Automaticerror correction tools on Arabic are underperform-ing in comparison with other languages like En-glish or French.
This can be explained by the lackof appropriate resources (e.g.
publicly availablecorpora and tools) and the complexity of the Ara-bic language.
Arabic is a challenging language forany NLP tool for many reasons.
Arabic has a richand complex morphology compared to other latinlanguages.
Short vowels are missing in the textsbut are mandatory from a grammatical point ofview.
Moreover they are needed to disambiguatebetween several possibilities of words.
Arabicis a rich language.There are many synonyms andArabic is a highly agglutinative, inflectional andderivational language and uses clitics (procliticsand enclitics).
Arabic has many varieties.
Mod-ern Standard Arabic includes the way Arabic iswritten in the news or in formal speech.
Classi-cal Arabic refers to religious and classical texts.Dialectal Arabic has no standard rules for orthog-raphy and is based on the pronunciation.
There-fore a same word can be written using many differ-ent surface forms depending on the dialectal ori-gin of the writer.
Another very popular way ofwriting Arabic on the Internet and the social me-dia like Facebook or Tweeter is to use ?Arabizi?, alatinized form of writing Arabic using latin lettersand digits (Aboelezz, 2009).For our participation in this evaluation task, wetried to implement two different approaches.
Thefirst approach is a lexicon driven spell checker.
Forthis, we have plan to adapt and test state-of-the-art spell checkers.
The second approach is a purestatistical approach by considering the correctionproblem as a statical machine translation task.The paper is organized as follows: section 2gives an overview of the automatic error correctionevaluation task and resources provided by the or-ganizers; section 3 describes the systems we havedeveloped for the evaluations; and finally in sec-tion 4 we discuss the results and draw some con-clusion.2 Task description and languageresourcesThe aim of the QALB Shared Task on AutomaticArabic Error Correction (Mohit, 2014) is to evalu-ate automatic text correction systems for the Ara-bic language.
The objective of the task is to cor-rect automatically texts in Arabic provided by theorganizers.
The QALB corpus is used for the eval-uation task.
A training set and a development setwith gold standard is provided for system train-155ing and development.
The training and develop-ment sets are made of sentences with errors com-ing from newspapers articles and the gold stan-dard is made of manual annotations of the sen-tences.
The annotations were made by humanannotators who used a correction guidelines de-scribed in (Zaghouani, 2014).
The corrections aremade of substitutions, insertions, deletions, splits,merges, moves of words and punctuation marks.The training set is made of 19,411 sentences and1M tokens.
The development set includes 1,017sentences for around 53k tokens.The evaluation is performed by comparing thegold standard with the hypothesis using the Lev-enshtein edit distance (Levenshtein, 1966) andthe implementation of the M2 scorer (Dahlmeier,2012).
Then for each sentence the Precision, Re-call and F-measure are calculated.Finally a test set of 968 sentences for 52k tokenswith no gold standard has to be corrected automat-ically for the evaluation.3 System descriptionFor our participation in this evaluation campaign,we studied two main approaches.
The first oneis a lexical driven approach using dictionaries tocorrect the errors.
Different lexicons were evalu-ated using Hunspell as spellchecking and correc-tion tool.The second approach is a statistical machine trans-lation point of view by considering the automaticerror correction problem as a translation task.
Forthis we used the statistical machine translation sys-tem Moses (Koehn, 2007), to train a model on thetraining data provided by the organizers.3.1 Baseline systemSince this the time first we are trying to developa spellchecker and correction tool for Arabic, wewanted to have some figures about the perfor-mance of spellcheckers on Arabic.We used the development set to test the per-formance of various spellchecker and correctiontools.
We corrected the development set automati-cally using the spellchecker module of the follow-ing softwares:?
Microsoft Word 2013?
OpenOffice 2014?
HunspellFor Microsoft Word and OpenOffice we usedthe default configuration for correcting Arabic textand disabled the grammar correction.Hunspell is an open source spellchecker widelyused in the open source community.
It is thespellchecker of many well-known applicationssuch as OpenOffice, LibreOffice, Firefox, Thun-derbird, Chrome, etc.
It is the next generation oflexical based spellcheckers in line with Myspell,Ispell and Aspell.
It is highly configurable, sup-ports Unicode and rich morphology languages likeArabic or Hungarian.
Hunspell uses mainly twofiles for spellchecking and correction.
The firstone is a dictionary file *.dic which contains ba-sically a wordlist and for each word, a list of ap-plicable rules that can be applied to the word.
Thesecond one is an affix file *.aff which contains alist of possible affixes and the rules of application.More information on these files can be found inthe Hunspell manual1.Hunspell is an interactive spellchecker.
It takesas an input a text to be corrected and for each wordthat is not found using the loaded dictionary andaffix files, it gives a list of suggestions to correctthe word.
For the correction which must be fullyautomatic, we forced Hunspell to always correctthe word with the first suggestion without any hu-man intervention.The dictionaries/affixes used for the evalua-tion is coming from the Ayaspell project(Ayaspell,2008).
The dictionary contains 52 725 entries andthe affix file contains 11859 rules.The results are given in Table 1Dictionary Precision Recall F-measureWord 45.7 16.6 24.3Hunspell 51.8 18.8 27.6OpenOffice 56.1 20.7 30.2Table 1: Results on the development set for Word,Hunspell/Ayaspell and OpenOffice(in percentage)The best results are the ones obtained byOpenOffice with a precision of 56.1%, a recall of20.7% and a F-measure of 30.2%.We would like to mention that these spellcheck-ers do not correct the punctuations which may ex-plain the relative low recall scores.1http://sourceforge.net/projects/hunspell/files/Hunspell/Documentation/1563.2 Statistical machine translation systemOur second approach is to consider the automaticcorrection problem as a translation problem byconsidering the sentences to be corrected as asource language and the correct sentences as a tar-get language.
Since the organizers provided uswith a 1 million tokens corpora with and with-out spelling errors, we tried to build a statisti-cal machine translation system using the paralleldata.
We used the Moses (Koehn, 2007), a Statis-tical Machine Translation (SMT) system to traina phrase based translation model with the train-ing data.
The training data provided is made oferroneous sentences and for each sentence a listof corrections to be applied.
To build the paral-lel error/correct text corpus we applied the correc-tions to the sentences.
We came up with a par-allel corpus of 19421 sentences and 102k tokensfor the error version and 112k tokens for the cor-rected version.
Moses requires a parallel corpusto train a translation model, a development set totune the translation model and also a monolinguallanguage model in the target language.
Since wehad to evaluate the performance on the develop-ment data provided by the organizers, we had touse part of the training data as a development datafor Moses.
So we split the 20k sentences includedin the training data in a new training set of 18kand a new development data of 2k sentences.
Wetrained standard phrase based models using thesurface word form with no morphological analy-sis or segmentation.
For the word alignment in thetraining process, we used GIZA++ (Och, 2003).The 2k sentences were used to tune the SMT mod-els.Corpus # Sentences Usagetrain18k 18000 traindev-train2k 1411 devdev 1017 testTable 2: Bitexts used for the SMT systemFor the language models we used corpora ofnewspapers publicly available or collected byTechlimed.
The sources are coming from theOpen Source Arabic Corpora (Saad, 2010) (20Mwords), the Adjir corpus (Adjir, 2005) (147Mwords) and other corpora we collected from var-ious online newspapers for a total of 300Mwords.
The language model was created with theIRSTLM toolkit (Federico, 2008).We evaluated the translation models on the de-velopment set using different sizes of monolin-gual corpus.
The 3 systems were trained on thesame parallel corpus but with different size for firmonolingual data for System100, System200 andSystem300 with respectively 100M words, 200Mwords and 300M words.
The results are given intable 3.System Precision Recall F-measureSystem100 70.7 48.8 57.8System200 70.7 49.6 58.3System300 70.8 50.1 58.7Table 3: Results on the development set (in per-centage) for the 3 SMT systemsWe can see from table 3 that the size of the lan-guage model has no impact on the precision butincreases slightly the recall of 1.3% in absolute(2.6% in relative).The BLEU scores (Papineni, 2002) measuredon Sytem100, System200, System300 are respec-tively 65.45, 65.82 and 65.98.We also tried to combine Hunspell/Ayaspellwith the SMT system by correcting the output ofthe SMT system with Hunspell/Ayaspell but didn?tget any improvement.4 DiscussionThe results obtained by the SMT system is muchmore better than the ones obtained with Hun-spell/Ayaspell with a F-measure of 58.7% for thebest SMT system and 27,6 for Hunspell/Ayaspell.We have to mention that the training corpus pro-vided by the organizers of 1 million words withthe manual annotations enabled us to train a statis-tical system that learn automatically the correctionmade by the annotators while Hunspell/Ayaspellwas not adapted to the correction guidelines.
Inparticular the punctuations are not corrected byHunspell/Ayaspell and this explains the differenceof recall between the SMT system (50.1%) andHunspell/Ayaspell (20.7%).
If we have a look atthe gold standard of the development set, 38.6%of the manual annotations concern punctuationmarks with 6266 punctuation marks annotationsfor an overall total of 16,231 annotations.
Whilethere are clear rules for strong punctuation markslike period, question or exclamation marks, thereare no clear grammatical rules for the weak punc-tuation marks, especially for commas which con-157cern 4,117 annotations of the gold standard ofthe development set (25.4%).
Another point thatwe would like to mention is that a spell checkerand correction tool is usually used in an inter-active mode by proposing n-best candidates forthe correction of a word.
When looking at Hun-spell/Ayspell correction candidates for an error,we saw the correction was not in position 1 butin the list of candidates.
So it would be interestingto compare the correction on the n-best candidatesand not only on the first candidate for Hunspelland the SMT system.5 ConclusionThis paper has reported on the participation ofTechlimed in the QALB Shared Task on Auto-matic Arabic Error Correction.
This is the firsttime we tried to develop a spellchecker for Arabicand have investigated two approaches.
The firstone is a lexicon driven approach using Hunspell asa spellchecker and correction tool and the secondone is a SMT systems using Moses for training astatistical machine translation model on the 1 mil-lion tokens corpus provided by the organizers.
Thebest results were obtained with the SMT systemwhich, especially, was able to deal with the punc-tuation marks corrections.
We also tested an hy-brid system by combining Hunspell and the SMTsystem but didn?t get better results than the SMTsystem alone.
Our perspective is to improve theresults by using hybrid systems based on the Di-iNAR lexical database (Abbes, 2004) and also alarge arabic named entity dictionary, both ownedand developped by Techlimed We will also try toused factored translation models with the Tech-limed Part-Of-Speech taggers.
And more trainingdata will also improve the quality of the correc-tions.AcknowledgmentsWe would like to thank the QALB Shared Task or-ganizers for setting up this evaluation campaign onautomatic error correction tool for Arabic and forproviding us with the language resources and toolsthat we used for the development of our systems.ReferencesRamzi Abb`es, Joseph Dichy, and Mohamed Hassoun.2004.
The architecture of a standard arabic lexicaldatabase: some figures, ratios and categories fromthe Diinar.
1 source program.
In Proceedings of theWorkshop on Computational Approaches to ArabicScript-based Languages, pages 15?22.
Associationfor Computational Linguistics, 2004.Mariam Aboelezz.
2009.
Latinised arabic and connec-tions to bilingual ability.
In Papers from the Lan-caster University Postgraduate Conference in Lin-guistics and Language Teaching, 2009.Ahmed Abdelali.
2005. http://aracorpus.e3rab.com/Ayaspell Arabic dictionary project, 2008.http://ayaspell.sourceforge.netDaniel Dahlmeier and Hwee Tou Ng.
2012.
Bet-ter evaluation for grammatical error correction.
InProceedings of the 2012 Conference of the NorthAmerican Chapter of the Association for Computa-tional Linguistics: Human Language Technologies,pages 568?572.
Association for Computational Lin-guistics, 2012.Marcello Federico, Nicola Bertoldi, and Mauro Cet-tolo.
2008.
Irstlm: an open source toolkit for han-dling large scale language models.
In Interspeech,pages 1618?1621, 2008.Hunspell, 2007. http://hunspell.sourceforge.net/Philipp Koehn, Hieu Hoang, Alexandra Birch, ChrisCallison-Burch, Marcello Federico, Nicola Bertoldi,Brooke Cowan, Wade Shen, Christine Moran,Richard Zens, et al.
2007.
Moses: Open sourcetoolkit for statistical machine translation.
In Pro-ceedings of the 45th Annual Meeting of the ACLon Interactive Poster and Demonstration Sessions,pages 177?180.
Association for Computational Lin-guistics, 2007.Vladimir Levenshtein.
1966.
Binary codes capable ofcorrecting deletions, insertions and reversals.
In So-viet physics doklady, volume 10, page 707, 1966.Behrang Mohit, Alla Rozovskaya, Nizar Habash, Wa-jdi Zaghouani, and Ossama Obeid.
2014.
The FirstQALB Shared Task on Automatic Text Correctionfor Arabic.
In Proceedings of EMNLP Workshop onArabic Natural Language Processing, Doha, Qatar,October 2014.Franz Josef Och and Hermann Ney.
2003.
A sys-tematic comparison of various statistical alignmentmodels.
Computational Linguistics, 29(1):19?51,2003.Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu.
2002.
Bleu: a method for automaticevaluation of machine translation.
In Proceedings ofthe 40th annual meeting on association for compu-tational linguistics, pages 311?318.
Association forComputational Linguistics, 2002.Motaz K Saad and Wesam Ashour.
2010 Osac: Opensource arabic corpora.
In 6th ArchEng Int.
Sympo-siums, EEECS, volume 10, 2010.158Wajdi Zaghouani, Behrang Mohit, Nizar Habash, Os-sama Obeid, Nadi Tomeh, Alla Rozovskaya, NouraFarra, Sarah Alkuhlani, and Kemal Oflazer.
2014.Large scale arabic error annotation: Guidelines andframework.
In Proceedings of the Ninth Inter-national Conference on Language Resources andEvaluation (LREC?14), Reykjavik, Iceland, May2014.
European Language Resources Association(ELRA).159
