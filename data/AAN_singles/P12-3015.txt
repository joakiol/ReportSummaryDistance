Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 85?90,Jeju, Republic of Korea, 8-14 July 2012. c?2012 Association for Computational LinguisticsCSNIPERAnnotation-by-query for non-canonical constructions in large corporaRichard Eckart de Castilho, Iryna GurevychUbiquitous Knowledge Processing Lab (UKP-TUDA)Department of Computer ScienceTechnische Universita?t Darmstadthttp://www.ukp.tu-darmstadt.deSabine BartschEnglish linguisticsDepartment of Linguistics and Literary StudiesTechnische Universita?t Darmstadthttp://www.linglit.tu-darmstadt.deAbstractWe present CSNIPER (Corpus Sniper), atool that implements (i) a web-based multi-user scenario for identifying and annotatingnon-canonical grammatical constructions inlarge corpora based on linguistic queries and(ii) evaluation of annotation quality by mea-suring inter-rater agreement.
This annotation-by-query approach efficiently harnesses expertknowledge to identify instances of linguisticphenomena that are hard to identify by meansof existing automatic annotation tools.1 IntroductionLinguistic annotation by means of automatic pro-cedures, such as part-of-speech (POS) tagging, isa backbone of modern corpus linguistics; POStagged corpora enhance the possibilities of corpusquery.
However, many linguistic phenomena arenot amenable to automatic annotation and are notreadily identifiable on the basis of surface features.Non-canonical constructions (NCCs), which are theuse-case of the tool presented in this paper, are acase in point.
NCCs, of which cleft-sentences area well-known example, raise a number of issues thatprevent their reliable automatic identification in cor-pora.
Yet, they warrant corpus study due to the rel-atively low frequency of individual instances, theirdeviation from canonical construction patterns andfrequent ambiguity.
This makes them hard to distin-guish from other, seemingly similar constructions.Expert knowledge is thus required to reliably iden-tify and annotate such phenomena in sufficientlylarge corpora like the 100 mil.
word British NationalCorpus (BNC Consortium, 2007).
This necessitatesmanual annotation which is time-consuming anderror-prone when carried out by individual linguists.To overcome these issues, CSNIPER implementsa web-based multi-user annotation scenario in whichlinguists formulate and refine queries that identifya given linguistic construction in a corpus and as-sess the query results to distinguish instances of thephenomenon under study (true positives) from suchexamples that are wrongly identified by the query(false positives).
Each expert linguist thus acts as arater rather than an annotator.
The tool records as-sessments made by each rater.
A subsequent evalua-tion step measures the inter-rater agreement.
The ac-tual annotation step is deferred until after this evalu-ation in order to achieve high annotation confidence.QueryAssessEvaluateAnnotatereviewassessmentsrefinequeryFigure 1: Annotation-by-query workflowCSNIPER implements an annotation-by-query ap-proach which entails the following interlinking func-tionalities (see fig.
1):Query development: Corpus queries can be de-veloped and refined within the tool.
Based on queryresults which are assessed and labeled by the user,queries can be systematically evaluated and refinedfor precision.
This transfers some of the ideas of85relevance feedback, which is a common method ofimproving search results in information retrieval, toa linguistic corpus query system.Assessment: Query results are presented to theuser as a list of sentences with optional additionalcontext; the user assesses and labels each sentenceas representing or not representing an instance of thelinguistic phenomenon under study.
The tool imple-ments a function that allows the user to commenton decisions and to temporarily mark sentences withuncertain assessments for later review.Evaluation: Evaluation is a central functional-ity of CSNIPER serving three purposes.
1) It in-tegrates with the query development by providingfeedback to refine queries and improve query pre-cision.
2) It provides information on sentences notlabeled consistently by all users, which can be usedto review the assessments.
3) It calculates the inter-rater agreement which is used in the corpus annota-tion step to ensure high annotation confidence.Corpus annotation: By assessing and labelingquery results as correct or wrong, raters provide thetool with their annotation decisions.
CSNIPER anno-tates the corpus with those annotation decisions thatexceed a certain inter-rater agreement threshold.This annotation-by-query approach of querying,assessing, evaluating and annotating allows multipledistributed raters to incrementally improve query re-sults and achieve high quality annotations.
In thispaper, we show how such an approach is well-suitedfor annotation tasks that require manual analysisover large corpora.
The approach is generalizableto any kind of linguistic phenomena that can be lo-cated in corpora on the basis of queries and requiremanual assessment by multiple expert raters.In the next two sections, we are providing a moredetailed description of the use-case driving the de-velopment of CSNIPER (sect.
2) and discuss why ex-isting tools do not provide viable solutions (sect.
3).Sect.
4 discusses CSNIPER and sect.
5 draws someconclusions and offers an outlook on the next steps.2 Non-canonical grammaticalconstructionsThe initial purpose of CSNIPER is the corpus-basedstudy of so-called non-canonical grammatical con-structions (NCC) (examples (2) - (5) below):1.
The media was now calling Reagan the front-runner.
(canonical)2.
It was Reagan whom the media was now callingthe frontrunner.
(it-cleft)3.
It was the media who was now calling Reaganthe frontrunner.
(it-cleft)4.
It was now that the media were calling Reaganthe frontrunner.
(it-cleft)5.
Reagan the media was not calling the front-runner.
(inversion)NCCs are linguistic constructions that deviatein characteristic ways from the unmarked lexico-grammatical patterning and informational orderingin the sentence.
This is exemplified by the con-structions of sentences (2) - (5) above.
While ex-pressing the same propositional content, the orderof information units available through the permissi-ble grammatical constructions offers interesting in-sights into the constructional inventory of a lan-guage.
It also opens up the possibility of comparingseemingly closely related languages in terms of thesets of available related constructions as well as therelations between instances of canonical and non-canonical constructions.In linguistics, a cleft sentence is defined as a com-plex sentence that expresses a single propositionwhere the clefted element is co-referential with thefollowing clause.
E.g., it-clefts are comprised of thefollowing constituents:dummysubject itmain verbto becleftedelementclauseThe NCCs under study pose interesting chal-lenges both from a linguistic and a natural languageprocessing perspective.
Due to their deviation fromthe canonical constructions, they come in a vari-ety of potential construction patterns as exemplifiedabove.
Non-canonical constructions can be expectedto be individually rarer in any given corpus than theircanonical counterparts.
Their patterns of usage andtheir discourse functions have not yet been describedexhaustively, especially not in representative corpusstudies because they are notoriously hard to identifywithout suitable software.
Their empirical distribu-tion in corpora is thus largely unknown.A major task in recognizing NCCs is distin-guishing them from structurally similar construc-86tions with default logical and propositional content.An example of a particular difficulty from the do-main of it-clefts are anaphoric uses of it as in (6) be-low that do not refer forward to the following clause,but are the antecedents of entities previously intro-duced in the context of preceding sentences.
Otherissues arise in cases of true relative clauses as exem-plified in (7) below:6.
London will be the only capital city in Eu-rope where rail services are expected to makea profit,?
he added.
It is a policy that could leadto economic and environmental chaos.
[BNC:A9N-s400]7.
It is a legal manoeuvre that declined in cur-rency in the ?80s.
[BNC: B1L-s576]Further examples of NCCs apart from the it-cleftsaddressed in this paper are wh-clefts and their sub-types, all-clefts, there-clefts, if-because-clefts anddemonstrative clefts as well as inversions.
All ofthese are as hard to identify in a corpus as it-clefts.The linguistic aim of our research is a comparisonof non-canonical constructions in English and Ger-man.
Research on these requires very large corporadue to the relatively low frequency of the individ-ual instances.
Due to the ambiguous nature of manyNCC candidates, automatically finding them in cor-pora is difficult.
Therefore, multiple experts have tomanually assess candidates in corpora.Our approach does not aim at the exhaustive an-notation of all NCCs.
The major goal is to improvethe understanding of the linguistic properties and us-age of NCCs.
Furthermore, we define a gold stan-dard to evaluate algorithms for automatic NCC iden-tification.
In our task, the total number of NCCs inany given corpus is unknown.
Thus, while we canmeasure the precision of queries, we cannot mea-sure their recall.
To address this, we exhaustivelyannotate a small part of the corpus and extrapolatethe estimated number of total NCC candidates.In summary, the requirements for a tool to supportmulti-user annotation of NCCs are as follows:1. querying large linguistically pre-processedcorpora and query refinement2.
assessment of sentences that are true instancesof NCCs in a multi-user setting3.
evaluation of inter-rater agreement and queryprecisionIn the following section, we review previous workto support linguistic annotation tasks.3 Related workWe differentiate three categories of linguistic toolswhich all partially fulfill our requirements: queryingtools, annotation tools, and transformation tools.Linguistic query tools: Such tools allow to querya corpus using linguistic features, e.g.
part-of-speech tags.
Examples are ANNIS2 (Zeldes et al,2009) and the IMS Open Corpus Workbench (CWB)(Christ, 1994).
Both tools provide powerful queryengines designed for large linguistically annotatedcorpora.
Both are server-based tools that can be usedconcurrently by multiple users.
However, they donot allow to assess the query results.Linguistic annotation tools: Such tools allowthe user to add linguistic annotations to a corpus.Examples are MMAX2 (Mu?ller and Strube, 2006)and the UIMA CAS Editor1.
These tools typicallydisplay a full document for the user to annotate.
AsNCCs appear only occasionally in a text, such toolscannot be effectively applied to our task, as they of-fer no linguistic query capabilities to quickly locatepotential NCCs in a large corpus.Linguistic transformation tools: Such tools al-low the creation of annotations using transforma-tion rules.
Examples are TextMarker (Kluegl et al,2009) and the UAM CorpusTool (O?Donnell, 2008).A rule has the form category := pattern and createsnew annotation of the type category on any part ofa text matching pattern.
A rule for the annotationof passive clauses in the UAM CorpusTool could bepassive-clause := clause + containing be% partici-ple.
These tools do not support the assessment ofthe results, though.
In contrast to the querying tools,transformation tools are not specifically designed tooperate efficiently on large corpora.
Thus, they arehardly productive for our task, which requires theanalysis of large corpora.4 CSNIPERWe present CSNIPER, an annotation tool for non-canonical constructions.
Its main features are:1http://uima.apache.org/87Figure 2: Search formAnnotation-by-query ?
Sentences potentiallycontaining a particular type of NCC are retrieved us-ing a query.
If the sentence contains the NCC ofinterest, the user manually labels it as correct andotherwise wrong.
Annotations are generated basedon the users?
assessments.Distributed multi-user setting ?
Our web-basedtool supports multiple users concurrently assessingquery results.
Each user can only see and edit theirown assessments and has a personal query history.Evaluation ?
The evaluation module provides in-formation on assessments, number of annotated in-stances, query precision and inter-rater agreement.4.1 Implementation and dataCSNIPER is implemented in Java and uses the CWBas its linguistic search engine (cf.
sect.
3).
Assess-ments are stored in a MySQL database.
Currently,the British National Corpus (BNC) is used in ourstudy.
Apache UIMA and DKPro Core2 are usedfor linguistic pre-processing, format conversion, andto drive the indexing of the corpora.
In particular,DKPro Core includes a reader for the BNC and awriter for the CWB.
As the BNC does not carrylemma annotations, we add them using the DKProTreeTagger (Schmid, 1994) module.4.2 Query (Figure 2)The user begins by selecting a 1?
corpus and a2?
construction type (e.g.
It-Cleft).
A query can bechosen from a 3?
list of examples, from the 4?
per-sonal query history, or a new 5?
query can be en-tered.
The query is applied to find instances of thatconstruction (e.g.
?It?
/VCC[] /PP[] /RC[]).
Af-ter pressing the 6?
Submit query button, the toolpresents the user with a KWIC view of the queryresults (fig.
3).
At this point, the user may choose to2http://www.ukp.tu-darmstadt.de/research/current-projects/dkpro/refine and re-run the query.As each user may use different queries, they willtypically assess different sets of query results.
Thiscan yield a set of sentences labeled by a single useronly.
Therefore, the tool can display those sentencesfor assessment that other users have assessed, but thecurrent user has not.
This allows getting labels fromall users for every NCC candidate.4.3 Assessment (Figure 3)If the query results match the expectation, the usercan switch to the assessment mode by clicking the7?
Begin assessment button.
At this point, an An-notationCandidate record is created in the databasefor each sentence unless a record is already present.These records contain the offsets of the sentence inthe original text, the sentence text and the construc-tion type.
In addition, an AnnotationCandidateLabelrecord is created for each sentence to hold the as-sessment to be provided by the user.In the assessment mode, an additional 8?
Labelcolumn appears in the KWIC view.
Clicking in thiscolumn cycles through the labels correct, wrong,check and nothing.
When the user is uncertain, thelabel check can be used to mark candidates for laterreview.
The view can be 9?
filtered for those sen-tences that need to be assessed, those that have beenassessed, or those that have been labeled with check.A 10?
comment can be left to further describe difficultcases or to justify decisions.
All changes are imme-diately saved to the database, so the user can stopassessing at any time and resume the process later.The proper assessment of a sentence as an in-stance of a particular construction type sometimesdepends on the context found in the preceding andfollowing sentences.
For this purpose, clicking onthe 11?
book icon in the KWIC view displays thesentence in its larger context (fig.
4).
POS tags areshown in the sentence to facilitate query refinement.4.4 Evaluation (Figure 5)The evaluation function provides an overview of thecurrent assessment state (fig.
5).
We support twoevaluation views: by construction type and by query.By construction type: In this view, one or more12?
corpora, 13?
types, and 14?
users can be selectedfor evaluation.
For these, all annotation candidatesand the respective statistics are displayed.
It is pos-88Figure 3: KWIC view of query results and assessmentssible to 15?
filter for correct, wrong, disputed, incom-pletely assessed, and unassessed candidates.
A can-didate is disputed if it is not labeled consistently byall selected users.
A candidate is incompletely as-sessed if at least one of the selected users labeledit and at least one other did not.
Investigating dis-puted cases and 16?
inter-rater agreement per typeusing Fleiss?
Kappa (Fleiss, 1971) are the main usesof this view.
The inter-rater agreement is calculatedusing only candidates labeled by all selected users.By query: In this view, query precision and as-sessment completeness are calculated for a set of17?
queries and 18?
users.
The query precision is cal-culated from the labeled candidates as:precision =|TP ||TP |+ |FP |We treat a candidate as a true positive (TP) if:1) the number of correct labels is larger than thenumber of wrong labels; 2) the ratio of correct labelscompared to the number of raters exceeds a given19?
threshold.
Candidates are conversely treated asfalse positives (FPs) if the number of wrong labelsis larger and the threshold is exceeded.
The thresh-old controls the confidence of the TP and, thus, ofthe annotations generated from them (cf.
sect.
4.5).Figure 4: Sentence context view with POS tagsIf a candidate is neither TP nor FP, it is unknown(UNK).
When calculating precision, UNK candi-dates are counted as FP.
The estimated precision isthe precision to be expected if TP and FP are equallydistributed over the set of candidates.
It takes intoaccount only the currently known TP and FP and ig-nores the UNK candidates.
Both values are the sameonce all candidates have been labeled by all users.4.5 AnnotationWhen the assessment process is complete, corpusannotations can be generated from the assessed can-didates.
Here, we employ the thresholded major-ity vote approach that we also use to determine theTP/FP in sect.
4.4.
Annotations for the respectiveNCC type are added directly to the corpus.
The aug-mented corpus can be used in further exploratorywork.
Alternatively, a file with all assessed candi-dates can be generated to serve as training data foridentification methods based on machine learning.5 ConclusionsWe have presented CSNIPER, a tool for the an-notation of linguistic phenomena whose investiga-tion requires the analysis of large corpora due toa relatively low frequency of instances and whoseidentification requires expert knowledge to distin-guish them from other similar constructions.
Ourtool integrates the complete functionality needed forthe annotation-by-query workflow.
It provides dis-tributed multi-user annotation and evaluation.
Thefeedback provided by the integrated evaluation mod-ule can be used to systematically refine queries andimprove assessments.
Finally, high-confidence an-notations can be generated from the assessments.89Figure 5: Evaluation by query and by NCC typeThe annotation-by-query approach can be gener-alized beyond non-canonical constructions to otherlinguistic phenomena with similar properties.
Anexample could be metaphors, which typically alsoappear with comparatively low frequency and re-quire expert knowledge to be annotated.
We planto integrate further automatic annotations and querypossibilities to support such further use-cases.AcknowledgmentsWe would like to thank Erik-La?n Do Dinh, who assistedin implementing CSNIPER as well as Gert Webelhuth andJanina Rado for testing and providing valuable feedback.This work has been supported by the Hessian researchexcellence program ?Landes-Offensive zur EntwicklungWissenschaftlich-o?konomischer Exzellenz?
(LOEWE) aspart of the research center ?Digital Humanities?
and bythe Volkswagen Foundation as part of the Lichtenberg-Professorship Program under grant No.
I/82806.Data cited herein have been extracted from the BritishNational Corpus, distributed by Oxford University Com-puting Services on behalf of the BNC Consortium.
Allrights in the texts cited are reserved.ReferencesBNC Consortium.
2007.
The British National Corpus,version 3 (BNC XML Edition).
Distributed by OxfordUniversity Computing Services p.p.
the BNC Consor-tium, http://www.natcorp.ox.ac.uk/.Oliver Christ.
1994.
A modular and flexible architec-ture for an integrated corpus query system.
In Proc.of the 3rd Conference on Computational Lexicographyand Text Research (COMPLEX?94), pages 23?32, Bu-dapest, Hungary, Jul.Joseph L. Fleiss.
1971.
Measuring nominal scale agree-ment among many raters.
In Psychological Bulletin,volume 76 (5), pages 378?381.
American Psychologi-cal Association, Washington, DC.Peter Kluegl, Martin Atzmueller, and Frank Puppe.2009.
TextMarker: A tool for rule-based informa-tion extraction.
In Christian Chiarcos, Richard Eckartde Castilho, and Manfred Stede, editors, Proc.
of theBiennial GSCL Conference 2009, 2nd UIMA@GSCLWorkshop, pages 233?240.
Gunter Narr Verlag, Sep.Christoph Mu?ller and Michael Strube.
2006.
Multi-levelannotation of linguistic data with MMAX2.
In SabineBraun, Kurt Kohn, and Joybrato Mukherjee, editors,Corpus Technology and Language Pedagogy: New Re-sources, New Tools, New Methods, pages 197?214.
Pe-ter Lang, Frankfurt am Main, Germany, Aug.Mick O?Donnell.
2008.
The UAM CorpusTool: Soft-ware for corpus annotation and exploration.
In Car-men M. et al Bretones Callejas, editor, Applied Lin-guistics Now: Understanding Language and Mind/ La Lingu??
?stica Aplicada Hoy: Comprendiendo elLenguaje y la Mente, pages 1433?1447.
Almer?
?a: Uni-versidad de Almer?
?a.Helmut Schmid.
1994.
Improvements in part-of-speechtagging with an application to German.
In Proc.
of Int.Conference on New Methods in Language Processing,pages 44?49, Manchester, UK, Sep.Amir Zeldes, Julia Ritz, Anke Lu?deling, and ChristianChiarcos.
2009.
ANNIS: A search tool for multi-layer annotated corpora.
In Proc.
of Corpus Linguis-tics 2009, Liverpool, UK, Jul.90
