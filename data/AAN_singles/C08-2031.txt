Coling 2008: Companion volume ?
Posters and Demonstrations, pages 123?126Manchester, August 2008Experiments in Base-NP Chunking and Its Role inDependency Parsing for ThaiShisanu Tongchim, Virach SornlertlamvanichThai Computational Linguistics LaboratoryNICT Asia Research Center112 Paholyothin RoadKlong 1, Klong LuangPathumthani 12120, Thailand{shisanu,virach}@tcllab.orgHitoshi IsaharaNICT3-5, Hikari-dai, Seika-choSoraku-gun, Kyoto, 619-0289, Japanisahara@nict.go.jpAbstractThis paper studies the role of base-NP in-formation in dependency parsing for Thai.The baseline performance reveals that thebase-NP chunking task for Thai is muchmore difficult than those of some lan-guages (like English).
The results showthat the parsing performance can be im-proved (from 60.30% to 63.74%) with theuse of base-NP chunk information, al-though the best chunker is still far fromperfect (F?=1 = 83.06%).1 IntroductionMany NLP applications require syntactic informa-tion and tools for syntactic analysis.
However,these linguistic resources are only available forsome languages.
In case of Thai, the research indeveloping tools for syntactic analysis and syntac-tically annotated corpora is still limited.
Most re-search in the past has focused on morphologicalanalysis (i.e.
word segmentation, part-of-speech(POS) tagging).
This can be viewed as a bottle-neck for developing NLP applications that requirea deeper understanding of the language.We have an ongoing project in developing a syn-tactically annotated corpus.
To accelerate the cor-pus annotation, some syntactic analysis tools canbe applied in a preprocessing step before correct-ing the results by human annotators.
In this pa-per, we use the first portion of completely anno-tated corpus to examine the dependency parsingand base-NP chunking.
The findings will providec?
2008.
Licensed under the Creative CommonsAttribution-Noncommercial-Share Alike 3.0 Unported li-cense (http://creativecommons.org/licenses/by-nc-sa/3.0/).Some rights reserved.some guidelines in selecting a parser and a base-NP chunker for our corpus annotation workflow.2 Dependency Parsing for ThaiThe dependency structure for Thai is more flexiblethan some languages like Japanese (Sekine et al,2000), Turkish (Eryigit and Oflazer, 2006), whileit is close to Chinese (Cheng et al, 2005) and En-glish (Nivre and Scholz, 2004).
An example ofa Thai sentence with dependency relations is out-lined in Fig.
1.
Note that the dependency links aredrawn from the dependents to their heads.
The de-pendency relations of Thai are bidirectional in na-ture and the root node can be found in arbitrarypositions.
Some languages (e.g.
Japanese) havemore constrained dependency structures, for ex-ample, the dependency relations are only from leftto right and the root node is at the rightmost.
Dueto the lack of structural constraints and larger num-ber of possible candidates, finding the correct de-pendency structure for Thai is more difficult.???
???????
???
?????
??
????
??????
?Teacher assign for(to) each person read book?The teacher assigns each person to read a book?Figure 1: An example of a Thai sentence with de-pendency relations.There are only few studies investigating the de-pendency parsing for Thai.
To our knowledge, thefirst research regarding dependency analysis wasdone in (Aroonmanakun, 1989).
However, this re-search is based on a very small corpus (50 sen-tences).
The lack of syntactically annotated cor-123pora may be a possible explanation why not muchresearch has been done in this area.
Some havebeen developed, but they are relatively small or notpublic, for example, a treebank of 400 sentencesused in (Satayamas et al, 2005).To overcome the shortage of corpora, we initiatethe development of a syntactically annotated cor-pus.
This corpus will be used as a fundamental lin-guistic resource for various projects.
To improvethe annotation workflow, we use the first portionof completely annotated corpus in experimentingwith dependency parsing and base-NP chunking.The results will be used to improve the preprocess-ing step of annotation.Two dependency parsers are included in our ex-periments.
Both are data-driven.?
Model 1 : The first model has been widelystudied in parsing Japanese text.
Some ma-chine learning techniques are used to estimatethe probability that word wi modifies wordwj .
Thus, the probability matrix of binary de-pendency relations can be derived from thisestimation.
Some search algorithms are thenused to find the most probable dependencystructure.
In this study, we use support vec-tor machines (SVMs) to estimate the proba-bility values and use a beam search algorithmto find the most likely dependency structure.In parsing Japanese text, the root position isnot an issue.
For Thai, however, we have toidentify the root position before finding thecomplete dependency relations.
Thus, we in-corporate an additional module to identify theroot node of the sentence.
This root findingmodule is also based on an SVM.The root finding module selects the word withhighest probability of being the root node.The following features are used in the rootmodel: 1.
POS, 2. position, 3. number ofverbs, 4. number of equivalent POS in frontof this word, 5. number of equivalent POS af-ter this word, 6. number of equivalent majorPOS in front of this word and 7. number ofequivalent major POS after this word.For building the dependency model (e.g.
re-lation between wi and wj), the following fea-tures are used: 1.
POS of wi and wj , 2. de-pendency direction, 3. distance, 4. major cat-egory of wi and wj , 5. major POS of wi andwj and 6. positions of wi and wj .Table 1: Performance of dependency parsingRA DA CSAModel 1?
85.4% 76.0% 44.8%Model 1?
86.2% 77.5% 47.9%Model 2?
89.31% 83.53% 60.30%Model 2?
91.22% 86.03% 65.27%Note: ?
(without chunk), ?
(with chunk)After identifying the root node and creatingthe probability matrix, the beam search (beamwidth=3) is performed.?
Model 2 : For the second model, we adoptMaltParser 1.0.4 (Nivre et al, 2007) which isa shift-reduce parser.
Machine learning algo-rithms are used to predict the sequence of ac-tions for parsing.
In this study, we use thedefault setting that utilizes an SVM for pre-dicting parsing actions.Assuming that {i0, i1, i2, i4} are the first fourtokens in the remaining input and {s0, s1} arethe two topmost tokens on the stack, we usethe default features including: 1.
POS of {i0,i1, i2, i3, s0, s1}, 2. word form of {s0, i0,i1, head(s0)}, 3. dependency type of s0 andits leftmost and rightmost dependent and theleftmost dependent of i0.To examine the role of base-NP chunk infor-mation in dependency parsing, we include chunklabels in the feature sets of both parsers.
Base-NP chunks are represented by using the IOB2 for-mat (Sang and Veenstra, 1999).
In the first parsingmodel, the chunk label of the current word is addedas a feature of the root model, while the chunk la-bels of both considered words are added in the de-pendency model.
We also add a feature showingthat both words reside in the same chunk or not tothe dependency model.
In the second model, weinclude chunk labels of {s0, s1, i0, i1, i2, i3} as itsfeature set.We use a section of completely annotated corpusconsisting of 2616 sentences to experiment withdependency parsing.
The sentence length rangesbetween 2 words to 20 words with an average of5.68.
These Thai sentences are part of our Thai-Japanese parallel corpus developed for the MTproject.
Since our MT project aims for the con-versation domain, the source sentences are adoptedmainly from dialogues and conversation books.
Amorphological analyzer is applied to these Thai124sentences for word segmentation and POS tagging,and the results are revised manually by our annota-tors.
The sentences are then assigned chunk labelswith IOB2 representation and syntactic structurerespectively.The corpus is divided into 2355 sentences asthe training set and 261 sentences as the test set.The experiment is done with gold-standard POStags and chunk labels.
Three performance met-rics are used: 1.
Root accuracy (RA): a portionof sentences with correctly identified roots, 2.
De-pendency accuracy (DA): a ratio of correct de-pendency relations to all dependency links and 3.Complete sentence accuracy (CSA): a portion ofsentences with correct roots and dependency pat-terns.Table 1 shows the accuracy of two parsers withand without using chunk information.
The resultsshow that chunk information helps in improvingthe performance of both parsers, especially in thenumber of completely correct sentences.
Malt-Parser (Model 2) which is a shift-reduce parserperforms better in parsing Thai sentences.
Thisconforms with previously published literature thatshift-reduce parsers have been widely applied tolanguages with dependency structure close to Thai(e.g.
English and Chinese), while variants ofModel 1 are applied to languages with more con-straints in dependency structure (like Japanese).Although the parsing accuracy can be improvedby chunk information, the results are based ongold-standard chunk labels.
To examine the possi-bility for deriving chunk labels automatically, weimplement and evaluate base-NP chunkers in thenext section.3 Base-NP ChunkingWe implement a simplified version of Kudo?schunker (Kudo and Matsumoto, 2001).
Kudo?schunker obtained very promising results onstandard English chunking tasks (e.g.
preci-sion=94.2%, recall=94.3%, F?=1 = 94.2%).
Weuse forward parsing method and employ an SVMfor identifying chunk labels.
The original featureset of Kudo?s chunker consists of: word form, POSand previous chunk labels.
Specifically, the fol-lowing features are used in identifying the chunklabel of the word wi: word form and POS of{wi?2, wi?1, wi, wi+1, wi+2}, chunk labels of{wi?2, wi?1}.
However, some preliminary resultsshow that the original feature set does not workwell with our problem.
The obtained model suffersfrom overfitting and lack of generalization.
Thus,we modify the feature set as: POS of {wi?2, wi?1,wi, wi+1}, chunk labels of {wi?2, wi?1} and thecurrent size of NP chunk in front of wi.
An SVMis trained to estimate the probability of the currentword being each of three chunk labels (B, I, O).
Abeam search strategy is used to find the most prob-able chunk sequence.In additional to the SVM-based chunkers, wealso examine chunkers based on conditional ran-dom fields (CRFs).
We use the implementationof CRF++ (Kudo, 2008).
CRFs outperform sev-eral methods on this task (Sha and Pereira, 2003).Three CRF-based chunkers are included in the ex-periment: the first one uses word form and POS asits feature set, the second one includes word class(function word, content word) as an additional fea-ture, the third one uses the previous three featuresand the major POS category.We use the training set and test set from pre-vious section to experiment with chunking.
Ta-ble 2 shows the performance of all chunkers.
Abaseline algorithm selects the chunk label which ismost frequently associated with POS of the cur-rent word.
From the results, all chunkers out-perform the baseline algorithm.
The best per-formance can be obtained by one of CRF-basedchunkers (F?=1 = 83.06%).
The inclusion ofmore features for CRF-based chunkers helps inimproving the performance.
In contrast, SVM-based chunkers tend to suffer from overfittingwhen adding more features.
The results also con-firm the findings of (Sha and Pereira, 2003) thatCRF-based chunkers can beat any single model.However, the results are still lower than the re-sults found in English experiments.
A reasonmay be that Thai NPs are more ambiguous thanEnglish NPs.
This is confirmed by a compari-son between our baseline result (F?=1=55.4%) andsome baseline results of English base-NP chunk-ing task (e.g.
precision=81.9%, recall=78.2%,F?=1=80.0% (Ramshaw and Marcus, 1995)).Since the baseline algorithms work exactly in thesame way, the results imply that the Thai chunkingtask is more difficult.We also examine the use of the best chunker asa preprocessing step of dependency parsing.
Us-ing the parser Model 2, the results are as follows:RA=90.84%, DA=84.99%, CSA=63.74%.
Over-all, the accuracy of using predicted chunk labels is125Table 2: Performance of base-NP chunkingPr.
R. F?=1Baseline 48.5% 64.5% 55.4%SVM+beam searchbeam width=1 70.1% 65.5% 67.7%beam width=3 70.6% 66.6% 68.5%beam width=5 69.6% 65.5% 67.5%beam width=10 71.0% 66.9% 68.9%beam width=20 71.0% 66.9% 68.9%CRFword+POS 84.79% 78.52% 81.54%word+POS+class 85.34% 79.93% 82.54%word+POS+class+main POS 86.04% 80.28% 83.06%lower than the use of gold-standard chunk labels,but still better than without any chunk information.Although the chunking accuracy is not high as inthe reported results of English chunking tasks, theresults show that the dependency parsing still ben-efits from the predicted chunk information.4 ConclusionsThe results from the chunking task show that thechunk identification for Thai is not trivial due toambiguities in Thai NPs.
The CRF-based chunkers(best:F?=1 = 83.06%) are found to be more effec-tive than the SVM-based chunkers (best:F?=1 =68.9%).Using the predicted chunk labels from the bestchunker in dependency parsing, the performanceof the best dependency parser can be improvedfrom CSA:60.30% to CSA:63.74%.
This accu-racy may further be improved if the performanceof chunker can be increased (as is shown in parsingaccuracy when using gold-standard chunk labels).ReferencesAroonmanakun, Wirote.
1989.
A dependency analy-sis of thai sentences for a computerized parsing sys-tem.
Master thesis, Department of Linguistics, Chu-lalongkorn University.Cheng, Yuchang, Masayuki Asahara, and Yuji Mat-sumoto.
2005.
Chinese deterministic dependencyanalyzer: Examining effects of global features androot node finder.
In Proceedings of the FourthSIGHAN Workshop on Chinese Language Process-ing, pages 17?24.Eryigit, Gu?lsen and Kemal Oflazer.
2006.
Statisticaldependency parsing for turkish.
In EACL.
The As-sociation for Computer Linguistics.Kudo, Taku and Yuji Matsumoto.
2001.
Chunking withsupport vector machines.
In NAACL.Kudo, Taku.
2008.
CRF++: Yet another CRF toolkit.http://crfpp.sourceforge.net/.Nivre, Joakim and Mario Scholz.
2004.
Deterministicdependency parsing of english text.
In Proceedingsof Coling 2004, pages 64?70, Geneva, Switzerland,Aug 23?Aug 27.
COLING.Nivre, Joakim, Johan Hall, Jens Nilsson, AtanasChanev, Gu?ls?en Eryig?it, Sandra Ku?bler, StetoslavMarinov, and Erwin Marsi.
2007.
Maltparser: Alanguage-independent system for data-driven depen-dency parsing.
Natural Language Engineering Jour-nal, 13(2):99?135.Ramshaw, Lance A. and Mitchell P. Marcus.
1995.Text chunking using transformation-based learning.In Proceedings of the Third Workshop on Very LargeCorpora, pages 82?94.Sang, Erik F. Tjong Kim and Jorn Veenstra.
1999.
Rep-resenting text chunks.
In Proceedings of the ninthconference on European chapter of the Associationfor Computational Linguistics, pages 173?179, Mor-ristown, NJ, USA.
Association for ComputationalLinguistics.Satayamas, Vee, Chalatip Thumkanon, and AsaneeKawtrakul.
2005.
Bootstrap cleaning and qualitycontrol for Thai tree bank construction.
In The 9thNational Computer Science and Engineering Con-ference, Bangkok, Thailand, Oct 27?Oct 28.
(InThai).Sekine, Satoshi, Kiyotaka Uchimoto, and Hitoshi Isa-hara.
2000.
Backward beam search algorithm fordependency analysis of japanese.
In COLING, pages754?760.
Morgan Kaufmann.Sha, Fei and Fernando C. N. Pereira.
2003.
Shal-low parsing with conditional random fields.
In HLT-NAACL.126
