Proceedings of the Eighth Workshop on Innovative Use of NLP for Building Educational Applications, pages 1?10,Atlanta, Georgia, June 13 2013. c?2013 Association for Computational LinguisticsThe Utility of Manual and Automatic Linguistic Error Codesfor Identifying Neurodevelopmental Disorders?Eric Morley, Brian Roark and Jan van SantenCenter for Spoken Language Understanding, Oregon Health & Science Universitymorleye@gmail.com, roarkbr@gmail.com, vansantj@ohsu.eduAbstractWe investigate the utility of linguistic featuresfor automatically differentiating between chil-dren with varying combinations of two po-tentially comorbid neurodevelopmental disor-ders: autism spectrum disorder and specificlanguage impairment.
We find that certainmanual codes for linguistic errors are usefulfor distinguishing between diagnostic groups.We investigate the relationship between cod-ing detail and diagnostic classification perfor-mance, and find that a simple coding schemeis of high diagnostic utility.
We propose a sim-ple method to automate the pared down codingscheme, and find that these automatic codesare of diagnostic utility.1 IntroductionIn Autism Spectrum Disorders (ASD), language im-pairments are common, but not universal (AmericanPsychiatric Association, 2000).
Whether these lan-guage impairments are distinct from those in Spe-cific Language Impairment (SLI) is an unresolvedissue (Williams et al 2008; Kjelgaard and Tager-Flusberg, 2001).
Accurate and detailed characteri-zation of these impairments is important not only forresolving this issue, but also for diagnostic practiceand remediation.Language ability is typically assessed with struc-tured instruments (?tests?)
that elicit brief, easy to?This research was supported in part by NIH NIDCD awardR01DC012033 and NSF award #0826654.
Any opinions, find-ings, conclusions or recommendations expressed in this publi-cation are those of the authors and do not reflect the views ofthe NIH or NSF.
Thanks to Emily Prud?hommeaux for usefuldiscussion on this topic and help with the data.score, responses to a sequence of items.
For exam-ple, the CELF-4 includes nineteen multi-item sub-tests with tasks such as object naming, word defini-tion, reciting the days of the week, or repeating sen-tences (Semel et al 2003).
Researchers are begin-ning to discuss the limits of structured instruments interms of which language impairments they tap intoand how well they do so, and are advocating the po-tential benefits of language sample analysis ?
an-alyzing natural language samples ?
to complementstructured assessment, specifically for language as-sessment in ASD where pragmatic and social com-munication issues are paramount yet are hard toassess in a conventional test format (e.g.
Tager-Flusberg et al2009).
However, language sampleanalysis faces two labor-intensive steps: transcrip-tion and detailed coding of the transcripts.To illustrate the latter, consider the SystematicAnalysis of Language Transcripts (SALT) (Millerand Chapman, 1985; Miller et al 2011), which isthe de-facto standard choice by clinicians lookingto code elicited language samples.
SALT comprisesa scheme for coding transcripts of recorded speech,together with software that tallies these codes, com-putes scores describing utterance length and errorcounts, and compares these scores with normativesamples.
SALT codes indicate bound morphemes,edits (which are referred to in the clinical literatureas ?mazes?
), and several types of errors in transcriptsof natural language, e.g., omitted or inappropriatewords.Although this has not been formally documented,our experience with SALT coding has shown that thecodes vary in terms of: 1) difficulty of manual cod-ing ?
e.g., relatively subtle pragmatic errors versusovergeneralization or marking bound morphemes;12) utility for identifying particular disorders; and 3)difficulty of automating the code.
This raises an im-portant question: Is there a combination of codesthat jointly discriminate well between relevant diag-nostic groups, and at the same time are either easyto code manually or can in principle be automated?This paper explores, first, how well the various man-ual SALT codes classify certain diagnostic groups;and, second, whether we can automate manual codesthat are of diagnostic utility.
Our goal is limited: itis not the automation of all SALT codes, but the au-tomation of those that in combination are of high di-agnostic utility.
Automating all SALT codes is sub-stantially more challenging; yet, we note that evenwhen some of these codes do not aid in classify-ing groups, they nevertheless may be of importancefor developing remediation strategies for individualchildren.
We are particularly interested in the im-pact of Autism in addition to language impairmentsfor the utility of particular SALT codes.The diagnostic groups are carefully chosen tobe pairwise matched either on language abilities oron autism symptomatology, thus enabling a pre-cise, ?surgical?
determination of the degrees towhich SALT codes reflect language-specific vs.autism-specific factors.
Specifically, the groups in-clude children with ASD with language impairment(ALI); ASD with no language impairment (ALN);SLI alone; and typically developing (TD), which isstrictly defined to exclude any neurodevelopmentaldisorder.
The TD and ALN groups, as well as theALI and SLI groups, are matched on language andoverall cognitive abilities, while the ALN and ALIgroups are matched on autism symptomatology butnot on language and overall cognitive abilities; allgroups are matched on chronological age.Regarding our algorithmic approach, we note thatautomatic detection of relatively subtle errors maybe exceedingly difficult, but perhaps such subtle er-rors are less critical for diagnosis than more obvi-ous ones.
Most prior work in grammaticality de-tection in spoken language has focused on special-ized detectors (e.g., Caines and Buttery 2010; Has-sanali and Liu 2011), such as mis-use of particularverb constructions rather than coarser detectors forthe presence of diverse classes of errors.
We demon-strate that these specialized error detectors can breakdown when confronted with real world dialogue, andthat in general, the features in these detectors re-stricts their utility in detecting other sorts of errors.We implement a detector to automatically extractcoarse SALT codes from an uncoded transcript.
Thisdetector only depends upon part of speech tags, asopposed to the parse features that are often used ingrammaticality detectors.
In most cases, these au-tomatically extracted codes enable us to distinguishbetween diagnostic groups more effectively than dofeatures that can be extracted trivially from an un-coded transcript.As far as we know, researchers have not pre-viously considered the utility of grammatical er-ror codes to identify ASD or SLI.
Prudhommeauxand Rouhizadeh (2012), however, found that au-tomatically extracted pragmatic features are usefulfor identifying children with ASD, among childrenboth with and without SLI.
Gabani et al(2009)found that features derived from language modelsare useful for distinguishing between children withand without a language impairment, both in mono-lingual English speakers, and in children who arebilingual in English and Spanish.Improving the characterization of a child?s lan-guage impairments is a prerequisite to developing asound plan for language training and education forthat child.
This paper presents a step in the directionof effective automated analysis of linguistic samplesthat can provide useful information even in the faceof comorbid disorders such as ASD and SLI.2 Systematic Analysis of LanguageTranscriptsHere we give an overview of what SALT requires oftranscriptions, and of SALT coding.
The approachhas been in wide use for nearly 30 years (Miller andChapman, 1985), and now also exists as a softwarepackage1 providing transcription and coding supportalong with tools for aggregating statistics for man-ual codes over the annotated corpora and comparingwith age norms.
The SALT software is not the focusof this investigation, so we do not discuss it further.2.1 Basic TranscriptionWe apply the automated methods to what will becalled basic transcripts.
Key for this concept is that,first, these transcripts do not require linguistic ex-pertise and thus can be performed by standard tran-scription services; and, second, that ?
as we shall1http://www.saltsoftware.com/2see ?
useful features can be automatically computedfrom them.Following the SALT guidelines, a basic transcriptshould indicate: the speaker of each utterance, par-tial words (or stuttering), overlapping speech, unin-telligible words, and non-speech sounds.
It shouldbe verbatim, regardless of whether a child?s utter-ance contains neologisms (novel words) or gram-matical errors (for example ?I goed?
should be writ-ten as such).A somewhat subtle issue is that SALT prescribesthat the basic transcript be broken into communi-cation units (which in this paper will be synony-mous with utterance).
Communication units aredefined as ?a main clause with all its dependentclauses?
(Miller et al 2011).
One reason for defin-ing utterance boundaries with communication units,rather than turns or sentences, is that in addition tothis being standard practice in language sample anal-ysis, doing so does not reward children for makinglong, but rather simple statements, nor does it penal-ize children for being interrupted.
To illustrate thefirst point, the utterance ?I like apples, and bananas,and pears, and oranges, and grapes.?
is one sen-tence long, but has five communication units (one ateach comma).
If the sentence were used as the ba-sic unit, the utterance would indicate the same levelcomplexity as the obviously more intricate ?for thepast three years we have lived in an apartment?.
Inthe basic transcript, each communication unit shouldbe terminated by one of the following punctuationmarks: ???
if it is a question, ???
if the speaker wasinterrupted, ?>?
if the speaker abandoned the utter-ance, and ?.?
in all other cases.
Thus, the aboveexample would be transcribed as ?C: I like apples.. .
.
C: and grapes.
?2.2 MarkupThere are three broad categories of SALT codes: in-dicators of 1) certain bound morphemes, 2) edits(discussed below), and 3) errors.Morphology The following inflectional suffixesmust be coded according to the SALT guidelines:plural -s (/S), possessive -?s (/Z), possessive plural-s?
(/S/Z), past tense -ed (/ED), 3rd person singular-s (/3S), progressive -ing (/ING).
The following cl-itics must also be delimited with a ?/?, provided theresulting root is unmodified in the surface form: n?t,?t, ?d, ?re, ?s, ?ve.
Since these morphemes are only in-dicated if the root is unmodified in the surface form,?won?t?
will remain unsegmented because ?wo?
isnot the root; ?can?t?
will be segmented ?can/?T?
and?don?t?
will be segmented ?do/N?T?, so as to pre-serve their respective roots.
Nominal or verbal formswith any of the preceding suffixes or clitics are writ-ten as the base form with the code appended, for ex-ample hitting?
hit/ING, bases?
base/S.Edits Edits consist of filler words such as ?like?,?um?
and ?uh?, false starts, and revisions.
There maybe multiple edits in a single utterance, as well asmultiple adjacent edits.
Edits are indicated by paren-theses, for example: ?
(And they like) and she (like)faint/3S.?
Note that in the SALT manual, and the lan-guage sample analysis literature, edits are referred toas mazes.
We use the term edit here because this isthe more widely used term for this phenomenon innatural language processing.Error codes The exact set of error codes used de-pends upon the clinician?s needs and the errors ofinterest.
Here we consider several key errors out-lined in the SALT manual.
These error codes andexamples are shown in Table 1.
Some of these codesdescribe precise classes of errors, for example [EO]or [OW], but others do not.
For example, [EW]can describe using the wrong verb, tense, preposi-tion or pronoun (in terms of case, person or gender),as well as other errors.
Note that [EU] (and [EC]) er-ror codes can occur in grammatical utterances.
The[EU] code marks utterances that are ungrammaticalfor reasons not captured by the other error codes, forexample severe problems with word order, or utter-Table 1: SALT error codes and examplesCode Meaning Example Count in Corpus[EC] Inappropriate response Did you help yourself stop?
Mom[EC].
9[EO] Overgeneralization Yeah, cuz I almost saw/ED[EO] one.
229[EW] Error word I play/ED of[EW] the cat.
1,456[EU] Utterance-level error You can see it very hard because it/?S under my hair[EU].
532[EX] Extraneous word Would you like to be[EX] fall down?
322[OM] Omitted morpheme The cat eat[OM] fish.
881[OW] Omitted word He [OW] going now.
7703ances which are simply nonsensical, as in Table 1.3 Evaluation of Manual CodesIn this section we use features extracted from SALT-coded transcripts for classification.
We consider twodifferent types of features: baseline features, whichare easily derived from a basic transcript; and fea-tures derived from SALT codes.
We investigatethese features to determine which SALT codes aremost worth automating for classification.3.1 DataOur data is a collection of 144 transcripts of theAutism Diagnostic Observation Schedule (ADOS),which is a semi-structured task that includes anexaminer and a child (Lord et al 2002).
Semi-structured means that the examiner carries out asequence of rigorously specified activities, but herprompts and questions are not scripted verbatim forall of them.
Detailed guidelines exist for scoringthe ADOS, but these are not considered in the cur-rent paper.
All transcripts have been manually codedwith SALT codes, described in Table 1.Subjects ranged in age between 4 and 8 years andwere required to be intelligible, to have a full-scaleIQ of greater than 70, and to have a mean length ofutterance (MLU) of at least 3.
Diagnoses of ASDand of SLI followed standard procedures, and werebased on clinical consensus in accordance to diag-nostic criteria outlined in the DSM-IV (AmericanPsychiatric Association, 2000).
Furthermore, ASDdiagnosis required ADOS and Social Communica-tion Questionnaire scores (SCQ) (Berument et al1999) to meet conventional thresholds.
Diagnosisof SLI required a CELF Core Language Score of atleast 1 standard deviation below the mean, in addi-tion to exclusion of ASD.Children were partitioned into pairs of groupsmatched on certain key measures.
Table 2 showsthese pairs and what they were matched on.
Theindividuals were selected from the initial pool ofall participants using the algorithm proposed by vanSanten et al(2010), in which, for a given pair ofgroups, children are iteratively removed from eachgroup until there is no significant difference (at p <0.02) on any measure on which we want the pair tobe matched.
We combined some groups into com-posite groups: ASD (ALI and ALN), nASD (SLIand TD), LN (?language normal?
: ALN and TD),and LI (?language impaired?
: ALI and SLI).Group 1 Group 2Group N Group N Matched onALI 25 ALN 21 Age, ADOS, SCQALI 24 SLI 19 Age, NVIQ, VIQALN 25 TD 27 Age, NVIQ, VIQASD 48 nASD 61 AgeLN 61 LI 39 AgeSLI 15 TD 38 AgeTable 2: Matched measures for paired groups (ADOS =ADOS score, NVIQ = non-verbal IQ, VIQ = verbal IQ)3.2 FeaturesThe term ?feature?
will be used to refer to instancesof various classes of SALT codes as well as to in-stances of other events that can be trivially extractedfrom the basic transcripts but do not involve SALTcodes (e.g, the ratio of ?uh?
to ?um?).
We distinguishbetween five levels of features, enumerated in Table3, that vary in the number and complexity of codesrequired.
This ranges from the baseline features thatrequire no manual codes to SALT-5 features that re-quire full SALT coding.
We consider two normal-ized variants of each feature: one normalized by thenumber of utterances spoken by the child, and theother normalized by the number of words spokenby the child (except for TKCT).
The ratios OCRATand UMUHRAT are never normalized.
Each featurelevel includes all features on lower levels.
Finally,to make our investigation into feature combinationsmore tractable, we do not consider combining twodifferent normalizations of the same feature.3.3 ClassificationWe perform six classification tasks in our investi-gation, according to the paired groups in Table 2:ALI/ALN; ALI/SLI; ALN/TD; ASD/nASD; LN/LI;and SLI/TD.
We extract various features from theADOS transcripts, and then classify the children ina leave-pair-out (LPO) schema (Cortes et al 2007)using the scikit logistic regression classifier with de-fault parameters (Pedregosa et al 2011).
For LPOanalysis, we iterate over all possible pairs that con-tain one positive and one negative instance (i.e.
chil-dren with different diagnoses), training on all otherinstances, and testing on that pair.
We count a trialas a success if the classifier assigns a higher proba-bility of being positive to the positive instance thanto the negative instance.
We then divide the num-ber of successes by the number of pairs to get anunbiased estimate of the area under the receiver op-erating curve (AUC) (Airola et al 2011).
AUC is4Group Feature DescriptionBaseline CEOLP # of times examiner speaks while child is talkingECOLP # of times child speaks while examiner is talkingINCCT Incomplete word countOCRAT Ratio of open- to closed-class wordsTKCT Token countTPCT Type countUMUHRAT Ratio of ?uh?
to ?um?UINTCT Unintelligible word countSALT-1 All baseline features +MPCT Morpheme countEDITCT Edit countSALT-2 All SALT-1 features +NERRUTT Number of utterances with any SALT error codesSALT-3 All SALT-2 features +ERRCT Count of SALT error codesSALT-4 All SALT-3 features +UTLERRCT Count of utterance level errors (EC / EU)WDLERRCT Count of word level errors (all other error codes)SALT-5 All SALT-4 features +XCT Count of individual error codes (X=EC, EO, .
.
.
; see Table 1)Table 3: Features by Levelthe probability that the classifier will assign a higherscore to a randomly chosen positive example than toa randomly chosen negative example.3.4 Determining Relevant FeaturesWe use a t-test based criterion as a simple way to de-termine which features to investigate for each clas-sification task.
For a given classification task, weperform a t-test for independent samples on eachfeature under both normalization schemes (if ap-propriate).
We retain a feature for investigation ifthat feature is significantly different between the twogroups at the ?
= 0.10 level.
If a particular featurevaries significantly between groups under both nor-malization schemes, we retain the version that hasthe larger T-statistic.
For the sake of brevity, wedo not report all of the features that varied betweengroups here, but this data is available upon requestfrom the authors.3.5 Initial Feature AblationWe perform feature ablation to see which featuresare most useful for performing each classificationtask.
Figure 1 shows the maximum performance (interms of AUC) over all subsets of features at eachfeature level (on the x-axis) on each of the six di-agnostic classification tasks.
Missing values for aparticular level of features for any comparison indi-cate that no features in that level that passed the t-testbased criterion for the two groups being compared.Figure 1 illustrates two important points.
First,classification difficulty depends heavily on the pairthat is being compared.
For example, the AUCfor ALI/SLI is at most 0.723 (SALT-5), while theAUC for SLI/TD reaches 0.982 (SALT-5).
This isnot surprising, as some pairs, most notably SLI/TD,differ widely in coarse measures of language abil-ity (such as non-verbal IQ), while other pairs, in-cluding ALI/SLI, do not.
Second, in many of thetasks, SALT-derived features are of high utility, butthe biggest gain in classification performance comeswith SALT-2, which is a count of the number ofsentences containing any SALT error code.
In fact,for all but one classification task (ASD/nASD), theAUC achieved with SALT-2 is at least 96% of themaximum AUC.
Furthermore, the best feature setusing SALT-2 features for most of these tasks is ei-ther the NERRUTT feature alone, or in the case ofALI/SLI, NERRUTT and TPCT.
These results leadus to conclude that the most important SALT-derivedfeature to code is NERRUTT.Perhaps surprisingly, Figure 1 also shows that forALN/TD and SLI/TD, performance at SALT-1 islower than the baseline.
There are two reasons forthis, which we explain in turn: 1) the SALT-1 fea-ture set must include a feature that is less useful thanthose in the optimal baseline feature set, and 2) theclassifier will not ignore this feature.
MPCT must beincluded in SALT-1 for both pairs, because the only5Figure 1: Maximum classification performance (AUC) at different feature levels (Bln=Baseline, S-N=SALT-N)other SALT-1 feature, EDITCT, does not vary signif-icantly between either ALN/TD or SLI/TD.
Further-more, MPCT is highly correlated with TKCT, yetTKCT is not in the best baseline feature set for ei-ther of these pairs.
Therefore, the SALT-1 featureset is required to include a feature that is less usefulthan the most useful ones in the baseline set, whichresults in lower performance.
Once MPCT is in-cluded in the SALT-1 feature set, the logistic regres-sion classifier will not ignore it by assigning it a zerocoefficient.
This is because MPCT distinguishes be-tween groups, and because the classifier is trainedat each round of LPO classification to maximize thelikelihood of the training data, rather than the AUCestimate provided by LPO classification.3.6 Counting Specific Error CodesThe single feature in SALT-2, NERRUTT, countshow many utterances spoken by the child contain atleast one SALT error code.
Some of these heteroge-nous errors, for example overgeneralization errors([EO]), should be straightforward to identify auto-matically.
Automatically identifying others, for ex-ample utterances that are inappropriate in context([EC]), would be more difficult.
Therefore, beforeautomating the extraction of NERRUTT, we shouldsee which errors most need to be identified, andwhich can safely be ignored.
To do this, we repeatour LPO classification procedure on various tasksusing SALT-2 features.We perform the following procedure to identifythe most diagnostically informative errors: for eachsubset s of SALT error codes, 1) compute the fea-ture NERRUTTSUBSET by counting the number ofutterances that contain any of the errors in s; then 2)perform the LPO diagnostic classification task usingNERRUTTSUBSET as the only feature.
The resultsof this experiment are in Table 4.
The ?% Max?
col-umn shows classification performance when a par-ticular subset of error codes were counted, relativeto the maximum performance yielded by any subsetof error codes for that particular task.
We excludethe ALN/TD and ASD/nASD tasks from this exper-iment because NERRUTT does not improve perfor-mance on these tasks.
This is perhaps unsurprising,because SALT codes were designed to be diagnosticof SLI, not ASD.We find that in all tasks, ignoring certain errorcodes raises performance.
These results also showthat it is not necessary, and indeed not ideal, to iden-tify utterances containing any SALT code.
Identi-fying utterances that contain any of the followingthree codes is sufficient to achieve at least 97% ofthe maximum AUC enabled by counting any sub-set of SALT codes: [EW], [OM], [OW].
For clarity,NERRUTTMOD is the count of utterances that con-tain any of those three SALT codes.Table 4: AUC from Counting Subsets of ErrorsClassification Errors Counted AUC % MaxALI/ALN EW, OM 0.762 100EW, OM, OW 0.739 97all 0.724 93ALI/SLI EW, OM 0.715 100EW, OM, OW 0.704 98all 0.676 95LN/LI EW, OM, OW 0.901 100all 0.881 98SLI/TD OM, OW 0.984 100EW, OM, OW 0.970 99all 0.951 9763.7 Robustness of NERRUTTMOD feature tonoise: a simulation experimentWe will consider two general ways of automaticallyextractingNERRUTTMOD.
The first way is to builda detector to identify utterances that contain at leastone relevant error.
The second way is to make de-tectors for the each relevant error, then combine theoutput of these detectors.
It is unlikely that any errordetector will perform perfectly.
Prior to investiga-tion of automation strategies, we would like to get anidea of how much such errors will affect diagnosticclassification performance.
To this end, we investi-gate how well we can perform the diagnostic classi-fication tasks when noise is deliberately introducedinto the NERRUTTMOD values via simulation.We consider two scenarios.
In the first, we as-sume a single error detector will be used to extractNERRUTTMOD.
We take each manually coded ut-terance, then randomly change whether or not thatsentence is counted as having an error to simulatedifferent precision and recall levels of the automatedNERRUTTMOD extractor.
We repeat this procedure100 times for each classification task, and then ex-amine the mean AUC over all trials.
In the sec-ond scenario, we assume a detector for each errorcode that counts a sentence as having an error anytime one of the detectors fires.
We randomly cor-rupt the detection of each error code considered inNERRUTTMOD in turn to simulate different preci-sion and recall levels of each individual error detec-tor.
We assume perfect detection of all errors notbeing randomly corrupted.
Again, we repeat thisprocedure 100 times for each classification task, andconsider the mean AUC over all trials.In both experiments, and in all classification tasks,we find that the NERRUTTMOD feature is ex-tremely robust to noise.
For example, finding theNERRUTTMOD feature with a single detector witha precision/recall of 0.1/0.3 enables SLI/TD clas-sification with an average AUC of 0.975, as com-pared to the maximum AUC of 0.984, enabled bya perfect detector.
When we use a cascaded de-tector to corrupt each of the two errors counted inNERRUTTMOD for classifying SLI/TD, so long asone error is detected perfectly, the other error onlyneeds to be detected with precision and recall of 0.1to enable a classification AUC within 0.02 of themaximum.The extreme robustness of this feature may appearsurprising, but it is easily explained by the data.
Themean value of NERRUTTMOD for the SLI groupis 7.8 times the mean value of this feature for theTD group.
So long as there is a correlation betweenthe true value of NERRUTTMOD and the estimatedvalue, as we have assumed in this experiment, thenthe estimated value is bound to be of utility in clas-sification.
This bodes well for the utility of automa-tion, even for a difficult task of discovering some ofthe relatively subtle errors coded in SALT.4 Automatic Feature Extraction4.1 Evaluating Hassanali and Liu?s SystemHassanali and Liu developed two grammaticality de-tectors that they used to identify ungrammatical ut-terances in transcriptions of speech from childrenboth with and without language impairments (Has-sanali and Liu, 2011).
They tested their grammati-cality detectors on the Paradise corpus, which con-sists of conversations with children elicited duringan investigation of otitis media, a hearing disor-der.
They present both a rule-based and a statis-tical grammaticality detector.
Both detectors con-sist of sub-detectors for the errors shown in Table5.
The rule-based and statistical detectors performwell, with the statistical detector outperforming therule-based one (F1=0.967 vs. 0.929).
The statisticaldetector, however, requires each error identified byany of the sub-detectors to be manually identified inthe training data.We reimplement both the rule based and statis-tical detectors proposed by Hassanali and Liu, andapply it to our data, with three modifications.
Thefirst two are minor: 1) we substitute the Charniak-Johnson reranking parser (2005) for Charniak?soriginal parser (Charniak, 2000), and 2) we use thescikit multinomial naive bayes classifier (Pedregosaet al 2011) instead of the one in WEKA (Hall et al2009).
The third difference is that we use these de-tectors to identify SALT error codes rather than theerrors these classifiers were originally built to detect.The mapping of the original errors to SALT errorcodes is given in Table 5.
To clarify, if we are train-ing the ?Missing Verb?
detector, then any utterancewith an [OW] code is taken to be a positive exam-ple.
This issue does not present itself with the rule-based detector because it is not trained.
Note that thetwo verb agreement features may correspond to ei-ther [EW] or [OM] SALT codes.
For example, ?youdoes?
would be [EW] because of the otiose 3rd per-7Error SALT codeMisuse of -ing participle [EW]Missing copulae [OW]Missing verb [OW]Subject-auxilliary agreement [EW]Subject-verb agreement [EW]/[OM]Missing infinitive ?to?
[OW]Table 5: Error detectors proposed by Hassanali and Liuson singular suffix, while ?he do?
would be an [OM]because it is missing that same suffix.Hassanali and Liu?s error detectors performpoorly on our data.
Table 6 reports the performanceof their detectors detecting utterances with variouserror codes.
Five of the six statistical error detec-tors that Hassanali and Liu proposed are unable toidentify any of the errors in our data.
The?misuseof -ing participle?
detector, however, is an excep-tion, and its performance detecting the analogouserror code [EW], using 10-fold cross validation is,shown in Table 6.
To detect the two pairs of er-ror codes, [EW][OM] and [OM][OW], and all threerelevant error codes ([EW][OM][OW]), we use theappropriate rule based detectors.
For example, todetect utterances with either [EW] or [OM] errors,we pool the detectors for the analogous error codes:?misuse of -ing participle?, ?subject-auxilliary agree-ment?, and ?subject-verb agreement?.There are three factors that may explain the poorperformance observed with most of Hassanali andLiu?s error detectors when used with our data.
Thefirst is that the three SALT codes we try to detect([EW], [OM], and [OW]) capture a wider variety oferrors than the six in Hassanali and Liu?s system.This could account for the low recall.
Second, thereare many utterances in our data that Hassanali andLiu?s system would label an error, but which are notmarked with any SALT error codes.
For example, ifthe examiner asks the child what she is doing, ?eat-ing spaghetti?
is a faultless response, even though itis missing both the subject and auxiliary verb.
Suchutterances may account for the low precision.
Fi-nally, most of Hassanali and Liu?s sub-detectors de-pend upon features describing the presence or ab-sence of specific structures in the parses of the input.The exception to this is the statistical ?misuse of -ingparticiple?
detector, which uses part of speech (POS)tag bigrams and skip bigrams as features.
It shouldcome as no surprise then that the ?misuse of -ing par-ticiple?
is the most robust of these detectors.
Indeed,CodesSystem Detected P R F1Hassanali [EW]?
0.074 0.218 0.110& Liu [EW][OM]* 0.049 0.277 0.083[OM][OW]* 0.028 0.191 0.049All three* 0.066 0.354 0.111POS-tag [EW] 0.074 0.218 0.110feature- [OM] 0.070 0.191 0.103based [OW] 0.064 0.210 0.099classifier [EW][OM] 0.102 0.269 0.148[OM][OW] 0.102 0.269 0.148All three 0.127 0.308 0.180Table 6: Performance on automatic detection of utter-ances with certain error codes using Hassanali and Liu?sdetectors, and general POS-tag-feature-based classifier.?
= ?misuse of -ing participle?, statistical; * = rule-basedin what follows, we make use of general POS-tagfeatures (tag n-gram and skip n-grams) as they do inthis detector, for a general purpose detector not tar-geted specifically at this particular construction, butrather to detect the presence of arbitrary given setsof error tags.4.2 Automatic SALT error code detectionWe compare three types of automatic error code de-tectors: 1) individual error code detectors; 2) pairdetectors, each of which detects a pair of error codesincluded in NERRUTTMOD, following Table 4; and3) a generic detector that identifies any utterancecontaining any of the following SALT codes: [EW],[OM], or [OW].
We investigate four different fea-tures, all of which are easily derived from the basictranscript: bigrams and skip bigrams of words, andPOS tags.
We use POS tags extracted from the out-put of the Charniak-Johnson reranking parser (2005)(also used in our reimplementation of Hassanali andLiu?s detectors) for simplicity.
We use the BernoulliNaive Bayes classifier in scikit with the default set-tings (Pedregosa et al 2011).We find that the word features do not aid clas-sification in any condition, and that using both bi-grams and skip bigrams of POS tags improves onusing either alone.
We report the performance ofthe three types of error detectors in Table 6.
Theseresults are from 10-fold cross-validation using POStag bigrams and skip bigrams as features.
Note thatthe general POS-tag-feature-based classifier uses thesame features as Hassanali and Liu?s statistical ?mis-use of -ing participle?
detector, which is why theperformance for detecting [EW] error codes alone8Manual features Automatic extractionBaseline SALT-2 SALT-2 featuresBaseline ?
Optimized ?Diagnoses AUC AUC ?
AUC ?
AUCALI/ALN 0.619?
0.723 0.5 0.611 0.94 0.676ALI/SLI 0.562 0.686 0.5 0.632 0.99 0.671LN/LI 0.755 0.881 0.5 0.801 0.50 0.801SLI/TD 0.840 0.951 0.5 0.805 0.99 0.840?
SALT-1; no significantly different baseline featuresTable 7: Diagnostic classification AUC using automatically extracted NERRUTTMODis identical between the two systems.The generic error detector yields higher perfor-mance than either the individual or pair error detec-tors.
Coding training data for the generic detector issimpler than doing so for the others because it onlyinvolves a single round of binary coding.4.3 Diagnostic ClassificationWe repeat the LPO diagnostic classification tasksusing the automatically extracted NERRUTTMODfeature.
We recompute NERRUTTMOD for eachspeaker at each iteration, training on all data exceptfor the two speakers in the test pair, and the speakerwhose NERRUTTMOD feature we are predicting.The results from this task are shown in Table 7.As can be seen in Table 7, diagnostic classifica-tion performance using the automatically extractedthe NERRUTTMOD feature is markedly lower thanwhen we extracted this feature from manual codes.However, raising the probability threshold ?
atwhich utterances are counted as containing an er-ror from its default value of 0.5, improves diagnos-tic classification performance for all but one pair(LN/LI).
This is because increasing the probabilitythreshold at which we count an utterance as hav-ing an error improves in NERRUTTMOD detection.For example, in the ALI/SLI group, using the de-fault ?
= 0.5, and a leave-one-out scenario, we canautomatically extract NERRUTTMOD with a preci-sion/recall score of 0.19/0.47.
When we increase ?to 0.99, the precision and recall become 0.23/0.24.Even though there is a massive drop in recall, theimprovement in precision is able to boost diagnosticclassification performance.In all but one pair (SLI/TD), the automati-cally extracted NERRUTTMOD feature improvesclassification over the baseline, even though theNERRUTTMOD extractor performs poorly in termsof intrinsic evaluation, with an F1 score of 0.180.These results are in line with the experiments per-forming diagnostic classification with an artificiallynoisy NERRUTTMOD feature (see Section 3.7).These results also demonstrate that the automati-cally extracted values of NERRUTTMOD are suffi-ciently correlated with the true values of this featureto be of some diagnostic utility.5 ConclusionsWe have found that the SALT codes provide use-ful information for distinguishing between certaindiagnostic groups, but not all of them.
Specifi-cally, and not surprisingly given SALT?s focus onlanguage disorders and not generally on atypicallanguage use characteristic of ASD, adding SALT-derived features to baseline features added littleto ASD/nASD, ALI/SLI, or ALN/TD classifica-tion accuracy, but added substantially to SLI/TD,ALI/ALN, and LN/LI classification accuracy.
Fur-thermore, we found that a simplified coding schemais almost as useful as the complete one for differ-entiating between these groups.
Finally, we haveproposed a simple method to automatically extracta variant of the most useful SALT-derived feature,NERRUTTMOD, which is a count of sentences thatcontain any of three types of errors (omitted mor-phemes or words, and generic word-level errors).Although this feature?s utility degrades when ex-tracted automatically, it still has considerable dis-criminative value.In future work, we will investigate the util-ity of more sophisticated features for extractingNERRUTTMOD and other SALT-derived features.We will also investigate the utility of other linguisticfeatures, for example parse structure, for the diag-nostic classification task.
Finally, we will also con-sider whether we can perform the diagnostic classi-fication task more effectively using cascaded binaryclassifiers (for example language impaired vs. lan-guage normal), as opposed to having a classifier forevery diagnostic pair.9ReferencesAntti Airola, Tapio Pahikkala, Willem Waegeman,Bernard De Baets, and Tapio Salakoski.
2011.
An ex-perimental comparison of cross-validation techniquesfor estimating the area under the roc curve.
Computa-tional Statistics & Data Analysis, 55(4):1828?1844.American Psychiatric Association.
2000.
DSM-IV-TR:Diagnostic and Statistical Manual of Mental Disor-ders.
American Psychiatric Publishing, Washington,DC, 4th edition.Sibel Kazak Berument, Michael Rutter, Catherine Lord,Andrew Pickles, and Anthony Bailey.
1999.
Autismscreening questionnaire: diagnostic validity.
TheBritish Journal of Psychiatry, 175(5):444?451.Andrew Caines and Paula Buttery.
2010.
You talking tome?
: A predictive model for zero auxiliary construc-tions.
In Proceedings of the 2010 Workshop on NLPand Linguistics: Finding the Common Ground, pages43?51.
Association for Computational Linguistics.Eugene Charniak and Mark Johnson.
2005.
Coarse-to-fine n-best parsing and maxent discriminative rerank-ing.
In Proceedings of the 43rd Annual Meeting on As-sociation for Computational Linguistics, pages 173?180.
Association for Computational Linguistics.Eugene Charniak.
2000.
A maximum-entropy-inspiredparser.
In Proceedings of the 1st North Americanchapter of the Association for Computational Linguis-tics conference, pages 132?139.
Morgan KaufmannPublishers Inc.Corinna Cortes, Mehryar Mohri, and Ashish Rastogi.2007.
An alternative ranking problem for search en-gines.
In Proceedings of WEA-2007, LNCS 4525,pages 1?21.
Springer-Verlag.Keyur Gabani, Melissa Sherman, Thamar Solorio, YangLiu, Lisa M Bedore, and Elizabeth D Pena.
2009.A corpus-based approach for the prediction of lan-guage impairment in monolingual english and spanish-english bilingual children.
In Proceedings of HumanLanguage Technologies: The 2009 Annual Conferenceof the North American Chapter of the Association forComputational Linguistics, pages 46?55.
Associationfor Computational Linguistics.M.
Hall, E. Frank, G. Holmes, B. Pfahringer, P. Reute-mann, and I.H.
Witten.
2009.
The weka data min-ing software: an update.
ACM SIGKDD ExplorationsNewsletter, 11(1):10?18.K.
Hassanali and Y. Liu.
2011.
Measuring language de-velopment in early childhood education: a case studyof grammar checking in child language transcripts.
InProceedings of the 6th Workshop on Innovative Useof NLP for Building Educational Applications, pages87?95.
Association for Computational Linguistics.Margaret M Kjelgaard and Helen Tager-Flusberg.
2001.An investigation of language impairment in autism:Implications for genetic subgroups.
Language andcognitive processes, 16(2-3):287?308.Catherine Lord, Michael Rutter, PC DiLavore, and SusanRisi.
2002.
Autism diagnostic observation schedule:ADOS.
Western Psychological Services.J.
Miller and R. Chapman.
1985.
Systematic analysis oflanguage transcripts.
Madison, WI: Language Analy-sis Laboratory.Jon F. Miller, Karen Andriacchi, and Ann Nockerts.2011.
Assessing language production using SALT soft-ware: A Clinician?s Guide to Language Sample Anal-ysis.
SALT Software, LLC.F.
Pedregosa, G. Varoquaux, A. Gramfort, V. Michel,B.
Thirion, O. Grisel, M. Blondel, P. Prettenhofer,R.
Weiss, V. Dubourg, J. Vanderplas, A. Passos,D.
Cournapeau, M. Brucher, M. Perrot, and E. Duches-nay.
2011.
Scikit-learn: Machine learning in Python.Journal of Machine Learning Research, 12:2825?2830.Emily Prudhommeaux and Masoud Rouhizadeh.
2012.Automatic detection of pragmatic deficits in childrenwith autism.
In Proceedings of the 3rd Workshop onChild, Computer and Interaction (WOCCI 2012).Eleanor Messing Semel, Elisabeth Hemmersam Wiig,and Wayne Secord.
2003.
Clinical evaluation of lan-guage fundamentals.
The Psychological Corporation,A Harcourt Assessment Company, Toronto, Canada,fourth edition.Helen Tager-Flusberg, Sally Rogers, Judith Cooper, Re-becca Landa, Catherine Lord, Rhea Paul, Mabel Rice,Carol Stoel-Gammon, Amy Wetherby, and Paul Yoder.2009.
Defining spoken language benchmarks and se-lecting measures of expressive language developmentfor young children with autism spectrum disorders.Journal of Speech, Language and Hearing Research,52(3):643.Jan PH van Santen, Emily T Prud?hommeaux, Lois MBlack, and Margaret Mitchell.
2010.
Computationalprosodic markers for autism.
Autism, 14(3):215?236.David Williams, Nicola Botting, and Jill Boucher.
2008.Language in autism and specific language impair-ment: Where are the links?
Psychological Bulletin,134(6):944.10
