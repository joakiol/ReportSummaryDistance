Proceedings of the ACL-IJCNLP 2009 Conference Short Papers, pages 17?20,Suntec, Singapore, 4 August 2009. c?2009 ACL and AFNLPHybrid Approach to User Intention Modeling for Dialog SimulationSangkeun Jung, Cheongjae Lee, Kyungduk Kim, Gary Geunbae LeeDepartment of Computer Science and EngineeringPohang University of Science and Technology(POSTECH){hugman, lcj80, getta, gblee}@postech.ac.krAbstractThis paper proposes a novel user intention si-mulation method which is a data-driven ap-proach but able to integrate diverse user dis-course knowledge together to simulate varioustype of users.
In Markov logic framework, lo-gistic regression based data-driven user inten-tion modeling is introduced, and human dialogknowledge are designed into two layers suchas domain and discourse knowledge, then it isintegrated with the data-driven model in gen-eration time.
Cooperative, corrective and self-directing discourse knowledge are designedand integrated to mimic such type of users.Experiments were carried out to investigatethe patterns of simulated users, and it turnedout that our approach was successful to gener-ate user intention patterns which are not onlyunseen in the training corpus and but also per-sonalized in the designed direction.1 IntroductionUser simulation techniques are widely used for learn-ing optimal dialog strategies in a statistical dialogmanagement framework and for automated evaluationof spoken dialog systems.
User simulation can belayered into the user intention level and user surface(utterance) level.
This paper proposes a novel inten-tion level user simulation technique.In recent years, a data-driven user intention model-ing is widely used since it is domain- and languageindependent.
However, the problem of data-drivenuser intention simulation is the limitation of user pat-terns.
Usually, the response patterns from data-drivensimulated user tend to be limited to the training data.Therefore, it is not easy to simulate unseen user inten-tion patterns, which is quite important to evaluate orlearn optimal dialog policies.
Another problem is pooruser type controllability in a data-driven method.Sometimes, developers need to switch testers betweenvarious type of users such as cooperative, uncoopera-tive or novice user and so on to expose their dialogsystem to various users.For this, we introduce a novel data-driven user in-tention simulation method which is powered by hu-man dialog knowledge in Markov logic formulation(Richardson and Domingos, 2006) to add diversityand controllability to data-driven intention simulation.2 Related workData-driven intention modeling approach uses statis-tical methods to generate the user intention given dis-course information (history).
The advantage of thisapproach lies in its simplicity and in that it is domain-and language independency.
N-gram based approach-es (Eckert et al, 1997, Levin et al, 2000) and otherapproaches (Scheffler and Young, 2001, Pietquin andDutoit, 2006, Schatzmann et al, 2007) are  introduced.There has been some work on combining rules withstatistical models especially for system side dialogmanagement (Heeman, 2007, Henderson et al, 2008).However, little prior research has tried to use bothknowledge and data-driven methods together in a sin-gle framework especially for user intention simulation.In this research, we introduce a novel data-drivenuser intention modeling technique which can be di-versified or personalized by integrating human dis-course knowledge which is represented in first-orderlogic in a single framework.
In the framework, di-verse type of user knowledge can be easily designedand selectively integrated into data-driven user inten-tion simulation.3 Overall architectureThe overall architecture of our user simulator isshown in Fig.
1.
The user intention simulator acceptsthe discourse circumstances with system intention asinput and generates the next user intention.
The userutterance simulator constructs a corresponding usersentence to express the given user intention.
The si-mulated user sentence is fed to the automatic speechrecognition (ASR) channel simulator, which then addsnoises to the utterance.
The noisy utterance is passedto a dialog system which consists of spoken languageunderstanding (SLU) and dialog management (DM)modules.
In this research, the user utterance simulatorand ASR channel simulator are developed using themethod of  (Jung et al, 2009).174 Markov logicMarkov logic is a probabilistic extension of finitefirst-order logic (Richardson and Domingos, 2006).
AMarkov Logic Network (MLN) combines first-orderlogic and probabilistic graphical models in a singlerepresentation.An MLN can be viewed as a template for construct-ing Markov networks.
From the above definition, theprobability distribution over possible worlds x speci-fied by the ground Markov network is given bywhere F is the number  of formulas in the MLN andni(x) is the number of true groundings of Fi in x. Asformula weights increase, an MLN increasingly re-sembles a purely logical KB, becoming equivalent toone in the limit of all infinite weights.
General algo-rithms for inference and learning in Markov logic arediscussed in (Richardson and Domingos, 2006).Since Markov logic is a first-order knowledge basewith a weight attached to each formula, it provides atheoretically fine framework integrating a statisticallylearned model with logically designed and inductedhuman knowledge.
So the framework can be used forbuilding up a hybrid user modeling with the advan-tages of knowledge-based and data-driven models.5 User intention modeling in MarkovlogicThe task of user intention simulation is to generatesubsequent user intentions given current discoursecircumstances.
Therefore, user intention simulationcan be formulated in the probabilistic formP(userIntention | context).In this research, we define the user intention stateuserIntention = [dialog_act, main_goal, compo-nent_slot], where dialog_act is a domain-independentlabel of an utterance at the level of illocutionary force(e.g.
statement, request, wh_question) and main_goalis the domain-specific user goal of an utterance (e.g.give_something, tell_purpose).
Component slotsrepresent domain-specific named-entities in the utter-ance.
For example, in the user intention state for theutterance ?I want to go to city hall?
(Fig.
2), the com-bination of each slot of semantic frame represents theuser intention symbol.
In this example, the state sym-bol is ?request+search_loc+[loc_name]?.
Dialogs oncar navigation deal with support for the informationand selection of the desired destination.The first-order language-based predicates whichare related with discourse context information andwith generating the next user intention are as follows:For example, after the following fragment of dialogfor the car navigation domain,the discourse context which is passed to the user si-mulator is illustrated in Fig.
3.Notice that the context information is composed ofsemantic frame (SF), discourse history (DH) and pre-vious system intention (SI).
?isFilledComponent?predicate indicates which component slots are filledduring the discourse.
?updatedEntity?
predicate istrue if the corresponding named entity is newly up-dated.
?hasSystemAct?
and ?hasSystemActAttr?predicate represent previous system intention andmentioned attributes.SFhasIntention(?ct_01?, ?request+search_loc+loc_name?)hasDialogAct(?ct_01?,?wh_question?
)hasMainGoal(?ct_01?, ?search_loc?
)hasEntity(?ct_01?, ?loc_keyword?
)DHisFilledComponent(?ct_01?, ?loc_keyword)!isFilledComponent(?ct_01?, ?loc_address)!isFilledComponent(?ct_01?, ?loc_name?
)!isFilledComponent(?ct_01?, ?route_type?
)updatedEntity(?ct_01?, ?loc_keyword?
)SIhasNumDBResult(?ct_01?, ?many?
)hasSystemAct(?ct_01?, ?inform?
)hasSystemActAttr(?ct_01?, ?address,name?)Fig.
3 Example of discourse context in car navigation domain.SF=Semantic Frame, DH=Discourse History, SI=System Inten-tion.raw user utterance I want to go to city hall.dialog_act requestmain_goal search_loccomponent.
[loc_name] cityhallFig.
2 Semantic frame for user intention simulation oncar navigation domain.Fig.
1 Overall architecture of dialog simulationUser(01) : Where are Chinese restaurants?// dialog_act=wh_question// main_goal=search_loc// named_entity[loc_keyword]=Chinese_restaurantSys(01) : There are Buchunsung and Idongbanjum inDaeidong.// system_act=inform// target_action_attribute=name,address?
User intention simulation related  predicatesGenerateUserIntention(context,userIntention)?
Discourse context related predicateshasIntention(context, userIntention)hasDialogAct(context, dialogAct)hasMainGoal(context, mainGoal)hasEntity(context, entity)isFilledComponent(context,entity)updatedEntity(contetx, entity)hasNumDBResult(context, numDBResult)hasSystemAct(context, systemAct)hasSystemActAttr(context, sytemActAttr)isSubTask(context, subTask)11( ) exp( ( ))F i iiP X x w n xZ ??
?
?185.1 Data-driven user intention modeling inMarkov logicThe formulas are defined between the predicateswhich are related with discourse context informationand corresponding user intention.
The formulas foruser intention modeling based on logistic regressionare as follows:?ct, pui, ui hasIntention(ct, pui)1=>  GenerateUserIntention(ct, ui)?ct, da, ui hasDialogAct(ct, da) => GenerateUserIntention(ct,ui)?ct, mg, ui hasMainGoal(ct, mg) => GenerateUserIntention(ct,ui)?ct, en, ui hasEntity(ct, en) =>GenerateUserIntention(ct,ui)?ct, en, ui isFilledComponent(ct,en)=> GenerateUserIntention(ct,ui)?ct, en, ui updatedEntity(ct, en) => GenerateUserIntention(ct,ui)?ct, dbr, ui hasNumDBResult(ct, dbr)=> GenerateUserIntention(ct, ui)?ct, sa, ui hasSystemAct(ct, sa) =>GenerateUserIntention(ct, ui)?ct, attr, ui hasSystemActAttr(ct, attr)=>  GenerateUserIntention(ct, ui)The weights of each formula are estimated fromthe data which contains the evidence (context) andcorresponding user intention of next turn (userInten-tion).5.2 User knowledgeIn this research, the user knowledge, which is used fordeciding user intention given discourse context, islayered into two levels: domain knowledge and dis-course knowledge.
Domain- specific and ?dependentknowledge is described in domain knowledge.
Dis-course knowledge is more general and abstractedknowledge.
It uses the domain knowledge as baseknowledge.
The subtask which is one of domainknowledge are defined as follows?isSubTask?
implies which subtask correspondsto the current context.
?subTaskHasIntention?describes which subtask has which user intention.?moveTo?
predicate implies the connection from sub-task to subtask node.Cooperative, corrective and self-directing discourseknowledge is represented in Markov logic to mimicfollowing users.?
Cooperative User: A user who is cooperative with asystem by answering what the system asked.?
Corrective User: A user who try to correct the mis-behavior of system by jumping to or repeating spe-cific subtask.?
Self-directing User: A user who tries to say whathe/she want to without considering system?s sugges-tion.Examples of discourse knowledge description forthree types of user are shown in Fig.
4.1 ct: context, ui: user intention, pui: previous user intention, da:dialog act, mg: main goal, en: entity, dbr:DB result, sa: systemaction, attr: target attribute of system actionBoth the formulas from data-driven model andformulas from discourse knowledge are used for con-structing MLN in generation time.In inference, the discourse context related predi-cates are given to MLN as true, then probabilities ofpredicate ?GenerateUserIntention?
over candi-date user intention are calculated.
One of exampleevidence predicates was shown in Fig.
3.
All of thepredicates of Fig.
3 are given to MLN as true.
Fromthe network, the probability of P(userIntention | con-text) is calculated.6 Experiments137 dialog examples from a real user and a dialogsystem in the car navigation domain were used totrain the data-driven user intention simulator.
TheSLU and DM are built in the same way of (Jung et al,2009).
After the training, simulations collected 1000dialog samples at each word error rate (WER) setting(WER=0 to 40%).
The simulator model can be variedaccording to the combination of knowledge.
We cangenerate eight different simulated users from A to Has Fig.
5.The overall trend of simulated dialogs are ex-amined by defining an average score function similarto the reward score commonly used in reinforcementlearning-based dialog systems for measuring both acost and task success.
We give 20 points for the suc-cessful dialog state and penalize 1 point for each ac-tion performed by the user to penalize longer dialogs.A B C D E F G HStatistical model (S) O O O O O O O OCooperative(CPR)  O   O O  OCorrective(COR)   O  O  O OSelf-directing(SFD)    O  O O OFig.
5 Eight different users (A to H) according to thecombination of knowledge.?
Subtask related predicatessubTaskHasIntention(subTask,userIntetion)moveTo(subtask, subTask)isCompletedSubTask (context, subTask)isSubtask(context,subTask)Cooperative Knoweldge// If system asks to specify an address explicitly, coop-erative users would specify the address by jumping tothe address setting subtask.?
ct, st  isSubTask(ct, st) ^hasSytemAct(ct, ?specify?)
^hasSystemActAttr(ct, ?address?
)=> moveTo(st, ?AddressSetting?
)Corrective Knowledge// If the current subtask fails, corrective users wouldrepeat current subtask.?
ct, st isSubTask(ct, st)^?
isCompletedSubTask(ct, st) ^subTaskHasIntention(st, ui)=> GenerateUserIntention(ct,ui)Self-directing Knowledge// Self-directing users do not make an utterance whichis not relevant with the next subtask in their knowledge.?
ct, st  isSubTask(ct, st) ^?
moveTo(st, nt) ^subTaskHasIntention(nt, ui)=> ?
GenerateUserIntention(ct, ui)Fig.
4 Example of cooperative, corrective and self-directing discourse knowledge.19Fig.
6 shows that simulated user C which has cor-rective knowledge with statistical model show signifi-cantly different trend over the most of word error ratesettings.
For the cooperative user (B), the difference isnot as large and not statistically significant.
It can beanalyzed that the cooperative user behaviors are rela-tively common patterns in human-machine dialogcorpus.
So, these behaviors can be already learned instatistical model (A).Using more than two type of knowledge togethershows interesting result.
Using cooperative know-ledge with corrective knowledge together (E) showsmuch different result than using each knowledgealone (B and C).
In the case of using self-directingknowledge with cooperative knowledge (F), the aver-age scores are partially increased against base linescores.
However, using corrective knowledge withself-directing knowledge does not show different re-sult.
It can be thought that the corrective knowledgeand self-directing knowledge are working as contra-dictory policy in deciding user intention.
Three dis-course knowledge combined user shows very interest-ing result.
H shows much higher improvement overall simulated users, and the differences are significantresults at p ?
0.001.To verify the proposed user simulation method cansimulate the unseen events, the unseen rates of unitswere calculated.
Fig.
7 shows the unseen unit rates ofintention sequence.
The unseen rate of n-gram variesaccording to the simulated user.
Notice that simulateduser C, E and H generates higher unseen n-gram pat-terns over all word error settings.
These users com-monly have corrective knowledge, and the patternsseem to not be present in the corpus.
But the unseenpatterns do not mean poor intention simulation.
High-er task completion rate of C, E and H imply that theseusers actually generate corrective user response tomake a successful conversation.7 ConclusionThis paper presented a novel user intention simulationmethod which is a data-driven approach but able tointegrate diverse user discourse knowledge together tosimulate various type of user.
A logistic regressionmodel is used for the statistical user intention modelin Markov logic.
Human dialog knowledge is sepa-rated into domain and discourse knowledge, and co-operative, corrective and self-directing discourseknowledge are designed to mimic such type user.
Theexperiment results show that the proposed user inten-tion simulation framework actually generates naturaland diverse user intention patterns what the developerintended.AcknowledgmentsThis research was supported by the MKE (Ministry ofKnowledge Economy), Korea, under theITRC(Information Technology Research Center) sup-port program supervised by the IITA(Institute for In-formation Technology Advancement) (IITA-2009-C1090-0902-0045).ReferencesEckert, W., Levin, E. and Pieraccini, R. 1997.
User model-ing for spoken dialogue system evaluation.
AutomaticSpeech Recognition and Understanding:80-87.Heeman, P. 2007.
Combining reinforcement learning withinformation-state update rules.
NAACL.Henderson, J., Lemon, O. and Georgila, K. 2008.
Hybridreinforcement/supervised learning of dialogue policiesfrom fixed data sets.
Comput.
Linguist., 34(4):487-511.Jung, S., Lee, C., Kim, K. and Lee, G.G.
2009.
Data-drivenuser simulation for automated evaluation of spoken dialogsystems.
Computer Speech & Lan-guage.doi:10.1016/j.csl.2009.03.002.Levin, E., Pieraccini, R. and Eckert, W. 2000.
A stochasticmodel of human-machine interaction for learning dialog-strategies.
IEEE Transactions on Speech and AudioProcessing, 8(1):11-23.Pietquin, O. and Dutoit, T. 2006.
A Probabilistic Frame-work for Dialog Simulation and Optimal Strategy Learn-ing.
IEEE Transactions on Audio, Speech and LanguageProcessing, 14(2):589-599.Richardson, M. and Domingos, P. 2006.
Markov logic net-works.
Machine Learning, 62(1):107-136.Schatzmann, J., Thomson, B. and Young, S. 2007.
Statistic-al User Simulation with a Hidden Agenda.
SIGDial.Scheffler, K. and Young, S. 2001.
Corpus-based dialoguesimulation for automatic strategy learning and evaluation.NAACL Workshop on Adaptation in Dialogue Sys-tems:64-70.Fig.
7 Unseen user intention sequence rate and task com-pletion rate over simulated users at word error rate of 10.WER(%)model0 10 20 30 40A:S (base line)14.22(0.00)9.13(0.00)5.55(0.00)1.33(0.00)-1.16(0.00)B:S+CPR14.39(0.17)9.78(0.65)5.38(-0.17)2.32?(0.99)-1.00(0.16)C:S+COR14.61?(0.40)10.91?(1.78)7.28?(1.74)2.62?(1.30)-0.81(0.35)D:S+SFD15.70?(1.48)10.10?(0.97)5.51(-0.04)1.89(0.56)-0.96?(0.20)E:S+CPR+COR14.75?(0.53)10.93?(1.79)6.88?(1.33)2.94?(1.61)-1.06?(0.11)F:S+CPR+SFD15.75?(1.54)10.16?(1.02)5.80(0.26)1.88(0.56)-0.03?
(1.13)G:S+COR+SFD14.39(0.17)9.18(0.05)5.04(-0.50)1.63(0.31)-1.52(-0.36)H:S+CPR+COR+SFD 15.70?(1.48)12.19?(3.05)9.20?(3.65)5.12?(3.80)1.32?(2.48)Fig.
6 Average scores of user intention models over used discourseknowledge.
The relative improvements against statistical modelsare described between parentheses.
Bold cells indicate the im-provements are higher than 1.0.?
: significantly different from the base line, p = 0.05,?
: significantly different from the base line, p = 0.01,?
: significantly different from the base line, p ?
0.00120
