Proceedings of the 5th International Workshop on Semantic Evaluation, ACL 2010, pages 308?312,Uppsala, Sweden, 15-16 July 2010. c?2010 Association for Computational Linguistics372:Comparing the Benefit of Different Dependency Parsers for Textu-al Entailment Using Syntactic Constraints OnlyAlexander Volokh G?nter Neumannalexander.volokh@dfki.de neumann@dfki.deDFKI DFKIStuhlsatzenhausweg 3 Stuhlsatzenhausweg 366123 Saarbr?cken, Germany 66123 Saarbr?cken, GermanyAbstractWe compare several  state of the art dependencyparsers  with  our  own  parser  based  on  a  linearclassification  technique.
Our  primary  goal  istherefore to use syntactic information only, in or-der to keep the comparison of the parsers as fairas possible.
We demonstrate, that despite the in-ferior result using the standard evaluation metricsfor  parsers  like  UAS  or  LAS  on  standard  testdata,  our  system  achieves  comparable  resultswhen used in an application, such as the SemEv-al-2 #12 evaluation exercise PETE.
Our submis-sion achieved the 4th position out of 19 participat-ing systems.
However, since it only uses a linearclassifier  it  works 17-20 times faster  than otherstate of the parsers, as for instance MaltParser orStanford Parser.1 IntroductionParsing is the process of mapping sentences totheir syntactic representations.
These representa-tions  can be used by computers  for  performingmany  interesting  natural  language  processingtasks, such as question answering or informationextraction.
In recent years  a lot of  parsers havebeen developed for this purpose.A very interesting and important  issue is  thecomparison between a large number of such pars-ing systems.
The most  widespread method is toevaluate the number of correctly recognized unitsaccording to a certain gold standard.
For depend-ency-based units unlabeled or labeled attachmentscores (percentage of correctly classified depend-ency relations, either with or without the depend-ency relation type) are usually used (cf.
Buchholzand Marsi, 2006).However, parsing is very rarely a goal in itself.In most cases it is a necessary preprocessing stepfor  a certain application.
Therefore it  is  usuallynot the best option to decide which parser suitsone's goals best by purely looking on its perform-ance on some standard test data set.
It is rathermore  sensible  to  analyse  whether  the  parser  isable  to  recognise  those  syntactic  units  or  rela-tions, which are most relevant for one's applica-tion.The  shared  task  #12  PETE in  the  SemEval-2010  Evaluation  Exercises  on  Semantic  Evalu-ation (Yuret, Han and Turgut, 2010) involved re-cognizing  textual  entailments  (RTE).
RTE  is  abinary  classification  task,  whose  goal  is  to  de-termine, whether for a pair of texts T and H themeaning  of  H is  contained  in  T  (Dagan et  al.,2006).
This task can be very complex dependingon the properties of these texts.
However, for thedata, released by the organisers of PETE, only thesyntactic information should be sufficient to reli-ably perform this task.
Thus it offers an ideal set-ting for  evaluating the performance of differentparsers.To our mind evaluation of parsers via RTE is avery good additional possibility, besides the usualevaluation metrics, since in most cases the mainthing in real-word applications is to recognize theprimary units, such as the subject, the predicate,308the objects, as well as their modifiers, rather thanthe other subordinate relations.We have been developing our own a multilin-gual dependency parser (called MDParser), whichis  based  on  linear  classification1.
Whereas  thesystem is quite fast because the classification islinear,  it  usually achieves inferior  results  (usingUAS/LAS evaluation metrics)  in  comparison  toother parsers, which for example use kernel-basedclassification  or  other  more  sophisticated  meth-ods.Therefore the PETE shared task was a perfectopportunity for us to investigate whether the in-ferior result of our parser is also relevant for itsapplicability  in  a  concrete  task.
We have  com-pared our system with three state of the art pars-ers made available on the PETE web page: Malt-Parser,  MiniPar  and  StandfordParser.
We  haveachieved the total score of 0.6545 (200/301 cor-rect  answers  on  the  test  data),  which  is  the  4thrank out of 19 submissions.2 MDParserMDParser stands for multilingual dependencyparser and is a data-driven system, which can beused  to  parse  text  of  an  arbitrary  language  forwhich training data is available.
It is a transition-based parser and uses a deterministic version ofthe Covington's algorithm (Covington, 2000).The models of the system are based on variousfeatures, which are extracted from the words ofthe  sentence,  including word forms  and part  ofspeech tags.
No additional morphological featuresor lemmas are currently used in our models, evenif they are available in the training data, since thesystem is especially designed for processing plaintext in different languages, and such componentsare not available for every language.The  preprocessing  components  of  MDParserinclude a.)
a  sentence splitter2,  since  the  parserconstructs a dependency structure for  individualsentences,  b.)
a  tokenizer,  in order to recognisethe elements between which the dependency rela-tions will be built3, and c.) a part of speech tagger,1http://www.dfki.de/~avolokh/mdparser.pdf2http://morphadorner.northwestern.edu/morphadorner/sen-tencesplitter/3http://morphadorner.northwestern.edu/morphadorner/word-tokenizer/in  order  to  determine  the  part  of  speech  tags,which are intensively used in the feature models4.MDParser is an especially fast system becauseit  uses  a  linear  classification  algorithm  L1R-LR(L1  regularised  logistic  regression)  from themachine learning package LibLinear (Lin et al,2008) for constructing its dependency structuresand  therefore  it  is  particularly  suitable  for  pro-cessing very large amounts of data.
Thus it can beused as a part of larger applications in which de-pendency structures are desired.Additionally,  significant efforts were made inorder to make the gap between our linear classi-fication and more advanced methods as small aspossible,  e.g.
by  introducing  features  conjunc-tions, which are complex features built out of or-dinary features, as well as methods for automatic-ally measuring feature usefulness in order to auto-mate and optimise feature engineering.3 Triple RepresentationEvery parser  usually produces  its  own some-how special  representation  of  the  sentence.
Wehave created such a representation, which we willcall  triple representation and have implementedan  automatic  transformation  of  the  results  ofMinipar,  MaltParser,  Stanford  Parser  and  ofcourse MDParser into it (cf.
Wang and Neumann,2007).The triple representation of a sentence is a setof  triple  elements  of  the  form  <parent,  label,child>, where child and parent elements stand forthe head and the modifier words and their parts ofspeech, and label stands for the relation betweenthem.
E.g.
<have:VBZ,  SBJ,  Somebody:NN>.This information is extractable from the results ofany dependency parser.4 Predicting EntailmentWhereas the first part of the PETE shared taskwas to construct syntactic representations for allT-H-pairs,  the  second important  subtask was  todetermine whether the structure of H is entailedby the structure of T. The PETE guide5 states thatthe following three phenomena were particularlyimportant to recognise the entailment relation:4The part of speech tagger was trained with the SVMToolhttp://www.lsi.upc.edu/~nlp/SVMTool/5http://pete.yuret.com/guide3091.
subject-verb  dependency  (John  kissedMary.
?
John kissed somebody.)2.
verb-object  dependency  (John  kissedMary ?
Mary was kissed.)3.
noun-modifier  dependency (The big redboat sank.
?
The boat was big.
)Thus we have manually formulated the follow-ing generic decision rule for determining the en-tailment relation between T and H:1. identify  the  root  triple  of  H  <null:null,ROOT, x>2.
check whether the subject and the com-plements(objects, verb complements) of the rootword in H are present in T. Formally: all triples ofH of the form <x, z, y>  should be contained inT(x in 1 and 2 is thus the same word).3. if 2 returns false we have to check wheth-er H is a structure in passive and T contains thesame content in active voice(a) or the other wayaround(b).
Formally:3a.
For triples of the form <be:VBZ, SBJ, s>and <be:VBZ, VC, t> in H check whether there isa  triple of the form <s, NMOD, t> in T.3b.
For triples of the form <u, OBJ,v> in Hcheck whether there is  a triple  of  the  form <v,NMOD, u> in T.It turned out that few additional modificationsto  the  base  rule  were  necessary  for  some  sen-tences: 1.)
For sentences containing conjunctions:If we were looking for a subject of a certain verband could not find it, we investigated whether thisverb is connected via a conjunction with anotherone.
If true, we compared the subject in H withthe subject of the conjunct verb.
2.)
For sentencescontaining special verbs, e.g.
modal verbs may orcan or auxiliary verbs like to have it turned out tobe important to go one level deeper into the de-pendency structure  and to  check whether  all  oftheir  arguments  in  H are  also present  in T,  thesame way as in 3.A triple <x,z,y> is contained in a set of triplesS, when there exists at least one of the triples in S<u,w,v>, such that x=u, w=z and y=v.
This is alsotrue  if  the  words  somebody,  someone or  some-thing are  used  on  one  of  the  equation  sides.Moreover, we use an English lemmatizer for allword forms, so when checking the equality of twowords we actually check their lemmas, e.g., is andare are also treated equally.5 ResultsWe have parsed the 66 pairs  of  the develop-ment  data  with  4  parsers:6 MiniPar,  StanfordParser, MaltParser and MDParser.
After applyingour rule we have achieved the following result:Accuracy Parsing SpeedMiniPar 45/66 1233 msStanford Parser 50/66 32889 msMaltParser 51/66 37149 msMDParser 50/66 1785 msWe used  the  latest  versions  of  MiniPar7 andStanford Parser8.
We did not re-test the perform-ance of these parsers on standard data, since wewere sure that these versions provide the best pos-sible results of these systems.As far as the MaltParser is concerned we had totrain our own model.
We have trained the modelwith the following LibSVM options: ?-s_0_-t_1_-d_2_-g_0.18_-c_0.4_-r_0.4_-e_1.0?.
We  wereable  to  achieve  a  result  of  83.86%  LAS  and87.25% UAS on the standard CoNLL English testdata,  a  result  which is  only slightly worse  thanthose reported in the literature, where the optionsare probably better tuned for the data.
The train-ing  data  used  for  training  was  the  same  as  forMDParser.The application of our rule for MDParser andMaltParser  was fully automated,  since both usethe  same  training  data  and  thus  work  over  thesame tag sets.
For MiniPar and Stanford Parser,which  construct  different  dependency structureswith  different  relation  types,  we  had  to  gothrough all pairs manually in order to investigatehow the rule should be adopted to their tag setsand structures.
However, since we have alreadycounted the  number  of  structures,  for  which anadoptation of the rule would work during this in-vestigation, we did not implement it in the end.Therefore  these  results  might  be  taken  with  apinch of salt, despite the fact that we have tried tostay as fair as possible and treated some pairs ascorrect, even if a quite large modification of the6For all results reported in this section a desktop PC withan Intel Core 2 Duo E8400 3.00 GHz processor and 4.00 GBRAM was used.7http://webdocs.cs.ualberta.ca/~lindek/minipar8http://nlp.stanford.edu/downloads/lex-parser.shtml310rule was necessary in order to adopt it to the dif-ferent tag set and/or dependency structure.For test  data we were only able to apply ourrule for the results of MDParser and MaltParser,since for such a large number of pairs (301) onlythe fully automated version of our mechanism forpredicting entailment could be applied.
For Mini-Par and Stanford Parser it was too tedious to ap-ply it to them manually or to develop a mappingbetween  their  dependency  annotations  and  theones used in MDParser or MaltParser.
Here arethe official results of our submissions for Malt-Parser and MDParser:Accuracy Parsing SpeedMDParser 197/301 8704 msMaltParser 196/301 147938 ms6 DiscussionWe were able to show that our parser based ona linear classification technique is especially fastcompared to other state of the art parsers.
Further-more, despite the fact, that it achieves an inferiorresult,  when using usual  evaluation metrics likeUAS or LAS, it is absolutely suitable for beingused in applications, since the most important de-pendency relations are recognized correctly evenwith  a  less  sophisticated  linear  classifier  as  theone being used in MDParser.As  far  as  the  overall  score  is  concerned  wethink a much better result  could be achieved, ifwe would put more effort into our mechanism forrecognizing  entailment  using  triple  representa-tions.
However, many of the pairs required morethan only syntactical information.
In many casesone would need to extend one's mechanism withlogic,  semantics  and  the  possibility  to  resolveanaphoric  expressions,  which  to  our  mind  goesbeyond the idea behind the PETE task.
Since wewere  primarly  interested  in  the  comparisonbetween MaltParser and MDParser, we have nottried to include solutions for such cases.
Here aresome of the pairs we think require more than onlysyntax:(4069  entailment="YES")  <t>Mr.
Sherwoodspeculated  that  the  leeway  that  Sea  Containershas means that Temple would have to "substan-tially  increase  their  bid  if  they're  going  to  topus.
"</t><h>Someone  would  have  to  increase  thebid.</h>(7003 entailment="YES") <t>After all, if  youwere going to set up a workshop you had to havethe proper equipment and that was that.</t><h>Somebody  had  to  have  the  equip-ment.</h>(3132.N entailment="YES")  <t>The first  wasthat America had become -- or was in danger ofbecoming  --  a  second-rate  military  power.</t><h>America was in danger.</h>?
4069,  7003 and 3132.N are  examples  forsentences were beyond syntactical information lo-gic  is  required.
Moreover  we are  surprised thatsentences of the form ?if A, then B?
entail B anda sentence of the form ?A or  B?
entails  B, since?or?
in this case means uncertainty.
(4071.N  entailment="NO")  <t>InterpublicGroup said its television programming operations-- which it expanded earlier this year -- agreed tosupply  more  than  4,000  hours  of  original  pro-gramming across Europe in 1990.</t><h>Interpublic Group expanded.</h>(6034  entailment="YES")  <t>"Oh,"  said  thewoman, "I've seen that picture already.
"</t><h>The woman has seen something.</h>?
In 4071.N one has to resolve ?it?
in ?it ex-panded?
to Interpublic Group.
In 6034 one has toresolve ?I?
in ?I've seen?
to ?the woman?.
Bothcases are examples for the necessity of anaphoraresolution, which goes beyond syntax as well.
(2055) <t>The Big Board also added computercapacity  to  handle  huge  surges  in  tradingvolume.</t><h>Surges were handled.</h>?
If something is added in order to do some-thing it does not entail that this something is thusautomatically done.
Anyways pure syntax is notsufficient,  since  the  entailment  depends  on  theverb used in such a construction.
(3151.N) <t>Most of them are Democrats andnearly all consider themselves, and are viewed as,liberals.</t><h>Some consider themselves liberal.</h>?
One has to know that the semantics of ?con-sider themselves as liberals?
and ?consider them-selves liberal?
is the same.Acknowledgements311The work presented here was partially suppor-ted by a research grant from the German FederalMinistry of Economics and Technology (BMWi)to  the  DFKI  project  Theseus  Ordo  TechWatch(FKZ: 01MQ07016).
We thank Joakim Nivre andJohan Hall for their support and tips when train-ing models with MaltParser.
Additionally, we arevery grateful to Sven Schmeier for providing uswith a trained part of speech tagger for Englishand for his support when using this tool.ReferencesMichael  A.  Covington,  2000.
A  Fundamental  Al-gorithm for  Dependency  Parsing.
In  Proceedings  ofthe 39th Annual ACM Southeast Conference.Dan Klein and Christopher D. Manning, 2003.
Accur-ate  Unlexicalized  Parsing.
Proceedings  of  the  41stMeeting  of  the  Association  for  Computational  Lin-guistics, pp.
423-430.Lin D, 2003.
Dependency-Based Evaluation Of Mini-par.
In  Building and using Parsed Corpora Edited by:Abeill?
A. Dordrecht: Kluwer; 2003.Sabine  Buchholz  and  Erwin  Marsi.
2006.
CoNLL-Xshared  task  on  multilingual  dependency  parsing.
InProceedings of CONLL-X, pages 149?164, New York.Ido  Dagan,  Oren  Glickman  and  Bernardo  Magnini.The  PASCAL Recognising  Textual  Entailment  Chal-lenge.
In  Quinonero-Candela, J.;  Dagan, I.;  Magnini,B.
;  d'Alche-Buc,  F.
(Eds.
),  Machine  Learning  Chal-lenges.
Lecture Notes in Computer Science, Vol.
3944,pp.
177-190, Springer, 2006.Nivre, J., J.
Hall and J. Nilsson, 2006.
MaltParser: AData-Driven Parser-Generator for Dependency Pars-ing.
In  Proceedings  of  the  fifth  international  confer-ence  on  Language  Resources  and  Evaluation(LREC2006),  May  24-26,  2006,  Genoa,  Italy,  pp.2216-2219.Rui  Wang and G?nter  Neumann,  2007.
RecognizingTextual  Entailment  Using  a  Subsequence  KernelMethod.
In Proceedings of AAAI 2007.R.
Fan,  K.  Chang,  C.  Hsieh,  X.  Wang,  and C.  Lin,2008.
LIBLINEAR: A Library for Large Linear Classi-fication.
Journal of Machine Learning Research, 9(4):1871?1874.Deniz Yuret,  Ayd?n Han and Zehra Turgut, 2010.
Se-mEval-2010 Task 12: Parser Evaluation using TextualEntailments.
In  Proceedings  of  the  SemEval-2010Evaluation Exercises on Semantic Evaluation.The  Stanford  Parser:  A  Statistical  Parser.http://nlp.stanford.edu/downloads/lex-parser.shtmlMaltparser.
http://maltparser.org/Minipar.
http://webdocs.cs.ualberta.ca/~lindek/mini-par.htmMDParser:  Multilingual  Dependency  Parser.http://mdparser.sb.dfki.de/312
