Proceedings of the ACL 2010 Conference Short Papers, pages 275?280,Uppsala, Sweden, 11-16 July 2010. c?2010 Association for Computational LinguisticsJointly optimizing a two-step conditional random field model for machinetransliteration and its fast decoding algorithmDong Yang, Paul Dixon and Sadaoki FuruiDepartment of Computer ScienceTokyo Institute of TechnologyTokyo 152-8552 Japan{raymond,dixonp,furui}@furui.cs.titech.ac.jpAbstractThis paper presents a joint optimizationmethod of a two-step conditional randomfield (CRF) model for machine transliter-ation and a fast decoding algorithm forthe proposed method.
Our method lies inthe category of direct orthographical map-ping (DOM) between two languages with-out using any intermediate phonemic map-ping.
In the two-step CRF model, the firstCRF segments an input word into chunksand the second one converts each chunkinto one unit in the target language.
In thispaper, we propose a method to jointly op-timize the two-step CRFs and also a fastalgorithm to realize it.
Our experimentsshow that the proposed method outper-forms the well-known joint source channelmodel (JSCM) and our proposed fast al-gorithm decreases the decoding time sig-nificantly.
Furthermore, combination ofthe proposed method and the JSCM givesfurther improvement, which outperformsstate-of-the-art results in terms of top-1 ac-curacy.1 IntroductionThere are more than 6000 languages in the worldand 10 languages of them have more than 100 mil-lion native speakers.
With the information revolu-tion and globalization, systems that support mul-tiple language processing and spoken languagetranslation become urgent demands.
The transla-tion of named entities from alphabetic to syllabarylanguage is usually performed through translitera-tion, which tries to preserve the pronunciation inthe original language.For example, in Chinese, foreign words arewritten with Chinese characters; in Japanese, for-eign words are usually written with special char-G o o g l e ??
?
?
English-to-JapaneseG o o g l e ??
English-to-ChineseSource Name       Target Name          Notegu ge Chinese Romanized writingguu gu ru Japanese Romanized writingFigure 1: Transliteration examplesacters called Katakana; examples are given in Fig-ure 1.An intuitive transliteration method (Knight andGraehl, 1998; Oh et al, 2006) is to firstly converta source word into phonemes, then find the corre-sponding phonemes in the target language, and fi-nally convert them to the target language?s writtensystem.
There are two reasons why this methoddoes not work well: first, the named entities havediverse origins and this makes the grapheme-to-phoneme conversion very difficult; second, thetransliteration is usually not only determined bythe pronunciation, but also affected by how theyare written in the original language.Direct orthographical mapping (DOM), whichperforms the transliteration between two lan-guages directly without using any intermediatephonemic mapping, is recently gaining more at-tention in the transliteration research community,and it is also the ?Standard Run?
of the ?NEWS2009 Machine Transliteration Shared Task?
(Li etal., 2009).
In this paper, we try to make our systemsatisfy the standard evaluation condition, whichrequires that the system uses the provided parallelcorpus (without pronunciation) only, and cannotuse any other bilingual or monolingual resources.The source channel and joint source channelmodels (JSCMs) (Li et al, 2004) have been pro-posed for DOM, which try to model P (T |S) andP (T, S) respectively, where T and S denote thewords in the target and source languages.
Ekbalet al (2006) modified the JSCM to incorporatedifferent context information into the model for275Indian languages.
In the ?NEWS 2009 MachineTransliteration Shared Task?, a new two-step CRFmodel for transliteration task has been proposed(Yang et al, 2009), in which the first step is tosegment a word in the source language into char-acter chunks and the second step is to perform acontext-dependent mapping from each chunk intoone written unit in the target language.In this paper, we propose to jointly optimize atwo-step CRF model.
We also propose a fast de-coding algorithm to speed up the joint search.
Therest of this paper is organized as follows: Sec-tion 2 explains the two-step CRF method, fol-lowed by Section 3 which describes our joint opti-mization method and its fast decoding algorithm;Section 4 introduces a rapid implementation of aJSCM system in the weighted finite state trans-ducer (WFST) framework; and the last sectionreports the experimental results and conclusions.Although our method is language independent, weuse an English-to-Chinese transliteration task inall the explanations and experiments.2 Two-step CRF method2.1 CRF introductionA chain-CRF (Lafferty et al, 2001) is an undi-rected graphical model which assigns a probabilityto a label sequence L = l1l2 .
.
.
lT , given an inputsequence C = c1c2 .
.
.
cT .
CRF training is usuallyperformed through the L-BFGS algorithm (Wal-lach, 2002) and decoding is performed by theViterbi algorithm.
We formalize machine translit-eration as a CRF tagging problem, as shown inFigure 2.T i m o t h y ??
?T/B i/N m/B o/N t/B h/N y/NTi/?
mo/?
thy/?Figure 2: An pictorial description of a CRF seg-menter and a CRF converter2.2 CRF segmenterIn the CRF, a feature function describes a co-occurrence relation, and it is usually a binary func-tion, taking the value 1 when both an observa-tion and a label transition are observed.
Yang etal.
(2009) used the following features in the seg-mentation tool:?
Single unit features: C?2, C?1, C0, C1, C2?
Combination features: C?1C0, C0C1Here, C0 is the current character, C?1 and C1 de-note the previous and next characters, and C?2 andC2 are the characters located two positions to theleft and right of C0.One limitation of their work is that only top-1segmentation is output to the following CRF con-verter.2.3 CRF converterSimilar to the CRF segmenter, the CRF converterhas the format shown in Figure 2.For this CRF, Yang et al (2009) used the fol-lowing features:?
Single unit features: CK?1, CK0, CK1?
Combination features: CK?1CK0,CK0CK1where CK represents the source language chunk,and the subscript notation is the same as the CRFsegmenter.3 Joint optimization and its fast decodingalgorithm3.1 Joint optimizationWe denote a word in the source language by S, asegmentation of S by A, and a word in the targetlangauge by T .
Our goal is to find the best word T?in the target language which maximizes the prob-ability P (T |S).Yang et al (2009) used only the best segmen-tation in the first CRF and the best output in thesecond CRF, which is equivalent toA?
= arg maxAP (A|S)T?
= arg maxTP (T |S, A?
), (1)where P (A|S) and P (T |S,A) represent twoCRFs respectively.
This method considers the seg-mentation and the conversion as two independentsteps.
A major limitation is that, if the segmenta-tion from the first step is wrong, the error propa-gates to the second step, and the error is very dif-ficult to recover.In this paper, we propose a new method tojointly optimize the two-step CRF, which can be276written as:T?
= arg maxTP (T |S)= arg maxT?AP (T,A|S)= arg maxT?AP (A|S)P (T |S,A)(2)The joint optimization considers all the segmen-tation possibilities and sums the probability overall the alternative segmentations which generatethe same output.
It considers the segmentation andconversion in a unified framework and is robust tosegmentation errors.3.2 N-best approximationIn the process of finding the best output usingEquation 2, a dynamic programming algorithm forjoint decoding of the segmentation and conversionis possible, but the implementation becomes verycomplicated.
Another direction is to divide the de-coding into two steps of segmentation and conver-sion, which is this paper?s method.
However, exactinference by listing all possible candidates explic-itly and summing over all possible segmentationsis intractable, because of the exponential computa-tion complexity with the source word?s increasinglength.In the segmentation step, the number of possiblesegmentations is 2N , where N is the length of thesource word and 2 is the size of the tagging set.
Inthe conversion step, the number of possible candi-dates is MN ?
, where N ?
is the number of chunksfrom the 1st step and M is the size of the taggingset.
M is usually large, e.g., about 400 in Chineseand 50 in Japanese, and it is impossible to list allthe candidates.Our analysis shows that beyond the 10th candi-date, almost all the probabilities of the candidatesin both steps drop below 0.01.
Therefore we de-cided to generate top-10 results for both steps toapproximate the Equation 2.3.3 Fast decoding algorithmAs introduced in the previous subsection, in thewhole decoding process we have to perform n-bestCRF decoding in the segmentation step and 10 n-best CRF decoding in the second CRF.
Is it reallynecessary to perform the second CRF for all thesegmentations?
The answer is ?No?
for candidateswith low probabilities.
Here we propose a no-lossfast decoding algorithm for deciding when to stopperforming the second CRF decoding.Suppose we have a list of segmentation candi-dates which are generated by the 1st CRF, rankedby probabilities P (A|S) in descending order A :A1, A2, ..., AN and we are performing the 2ndCRF decoding starting from A1.
Up to Ak,we get a list of candidates T : T1, T2, ..., TL,ranked by probabilities in descending order.
Ifwe can guarantee that, even performing the 2ndCRF decoding for all the remaining segmentationsAk+1, Ak+2, ..., AN , the top 1 candidate does notchange, then we can stop decoding.We can show that the following formula is thestop condition:Pk(T1|S) ?
Pk(T2|S) > 1 ?k?j=1P (Aj |S).
(3)The meaning of this formula is that the prob-ability of all the remaining candidates is smallerthan the probability difference between the bestand the second best candidates; on the other hand,even if all the remaining probabilities are added tothe second best candidate, it still cannot overturnthe top candidate.
The mathematical proof is pro-vided in Appendix A.The stop condition here has no approximationnor pre-defined assumption, and it is a no-loss fastdecoding algorithm.4 Rapid development of a JSCM systemThe JSCM represents how the source words andtarget names are generated simultaneously (Li etal., 2004):P (S, T ) = P (s1, s2, ..., sk, t1, t2, ..., tk)= P (< s, t >1, < s, t >2, ..., < s, t >k)=K?k=1P (< s, t >k | < s, t >k?11 ) (4)where S = (s1, s2, ..., sk) is a word in the sourcelangauge and T = (t1, t2, ..., tk) is a word in thetarget language.The training parallel data without alignment isfirst aligned by a Viterbi version EM algorithm (Liet al, 2004).The decoding problem in JSCM can be writtenas:T?
= arg maxTP (S, T ).
(5)277After the alignments are generated, we use theMITLM toolkit (Hsu and Glass, 2008) to build atrigram model with modified Kneser-Ney smooth-ing.
We then convert the n-gram to a WFSTM (Sproat et al, 2000; Caseiro et al, 2002).
To al-low transliteration from a sequence of characters,a second WFST T is constructed.
The input wordis converted to an acceptor I , and it is then com-bined with T and M according to O = I ?
T ?Mwhere ?
denotes the composition operator.
Then?best paths are extracted by projecting the out-put, removing the epsilon labels and applying then-shortest paths algorithm with determinization inthe OpenFst Toolkit (Allauzen et al, 2007).5 ExperimentsWe use several metrics from (Li et al, 2009) tomeasure the performance of our system.1.
Top-1 ACC: word accuracy of the top-1 can-didate2.
Mean F-score: fuzziness in the top-1 candi-date, how close the top-1 candidate is to the refer-ence3.
MRR: mean reciprocal rank, 1/MRR tells ap-proximately the average rank of the correct result5.1 Comparison with the baseline and JSCMWe use the training, development and test sets ofNEWS 2009 data for English-to-Chinese in ourexperiments as detailed in Table 1.
This is a paral-lel corpus without alignment.Training data Development data Test data31961 2896 2896Table 1: Corpus size (number of word pairs)We compare the proposed decoding methodwith the baseline which uses only the best candi-dates in both CRF steps, and also with the wellknown JSCM.
As we can see in Table 2, the pro-posed method improves the baseline top-1 ACCfrom 0.670 to 0.708, and it works as well as, oreven better than the well known JSCM in all thethree measurements.Our experiments also show that the decodingtime can be reduced significantly via using our fastdecoding algorithm.
As we have explained, with-out fast decoding, we need 11 CRF n-best decod-ing for each word; the number can be reduced to3.53 (1 ?the first CRF?+2.53 ?the second CRF?
)via the fast decoding algorithm.We should notice that the decoding time is sig-nificantly shorter than the training time.
Whiletesting takes minutes on a normal PC, the train-ing of the CRF converter takes up to 13 hours onan 8-core (8*3G Hz) server.Measure Top-1 Mean MRRACC F-scoreBaseline 0.670 0.869 0.750Joint optimization 0.708 0.885 0.789JSCM 0.706 0.882 0.789Table 2: Comparison of the proposed decodingmethod with the previous method and the JSCM5.2 Further improvementWe tried to combine the two-step CRF model andthe JSCM.
From the two-step CRF model we getthe conditional probability PCRF (T |S) and fromthe JSCM we get the joint probability P (S, T ).The conditional probability of PJSCM(T |S) canbe calculuated as follows:PJSCM (T |S) =P (T, S)P (S) =P (T, S)?T P (T, S).
(6)They are used in our combination method as:P (T |S) = ?PCRF (T |S) + (1 ?
?
)PJSCM (T |S)(7)where ?
denotes the interpolation weight (?
is setby development data in this paper).As we can see in Table 3, the linear combinationof two sytems further improves the top-1 ACC to0.720, and it has outperformed the best reported?Standard Run?
(Li et al, 2009) result 0.717.
(Thereported best ?Standard Run?
result 0.731 usedtarget language phoneme information, which re-quires a monolingual dictionary; as a result it isnot a standard run.
)Measure Top-1 Mean MRRACC F-scoreBaseline+JSCM 0.713 0.883 0.794Joint optimization+ JSCM 0.720 0.888 0.797state-of-the-art 0.717 0.890 0.785(Li et al, 2009)Table 3: Model combination results6 Conclusions and future workIn this paper we have presented our new jointoptimization method for a two-step CRF modeland its fast decoding algorithm.
The proposed278method improved the system significantly and out-performed the JSCM.
Combining the proposedmethod with JSCM, the performance was furtherimproved.In future work we are planning to combine oursystem with multilingual systems.
Also we wantto make use of acoustic information in machinetransliteration.
We are currently investigating dis-criminative training as a method to further im-prove the JSCM.
Another issue of our two-stepCRF method is that the training complexity in-creases quadratically according to the size of thelabel set, and how to reduce the training time needsmore research.Appendix A.
Proof of Equation 3The CRF segmentation provides a list of segmen-tations: A : A1, A2, ..., AN , with conditionalprobabilities P (A1|S), P (A2|S), ..., P (AN |S).N?j=1P (Aj |S) = 1.The CRF conversion, given a segmenta-tion Ai, provides a list of transliteration out-put T1, T2, ..., TM , with conditional probabilitiesP (T1|S,Ai), P (T2|S,Ai), ..., P (TM |S,Ai).In our fast decoding algorithm, we start per-forming the CRF conversion from A1, then A2,and then A3, etc.
Up to Ak, we get a list of can-didates T : T1, T2, ..., TL, ranked by probabili-ties Pk(T |S) in descending order.
The probabilityPk(Tl|S)(l = 1, 2, ..., L) is accumulated probabil-ity of P (Tl|S) over A1, A2, ..., Ak , calculated by:Pk(Tl|S) =k?j=1P (Aj |S)P (Tl|S,Aj)If we continue performing the CRF conversionto cover all N (N ?
k) segmentations, eventuallywe will get:P (Tl|S) =N?j=1P (Aj |S)P (Tl|S,Aj)?k?j=1P (Aj |S)P (Tl|S,Aj)= Pk(Tl|S) (8)If Equation 3 holds, then for ?i 6= 1,Pk(T1|S) > Pk(T2|S) + (1 ?k?j=1P (Aj |S))?
Pk(Ti|S) + (1 ?k?j=1P (Aj |S))= Pk(Ti|S) +N?j=k+1P (Aj |S)?
Pk(Ti|S)+N?j=k+1P (Aj |S)P (Ti|S,Aj)= P (Ti|S) (9)Therefore, P (T1|S) > P (Ti|S)(i 6= 1), and T1maximizes the probability P (T |S).279ReferencesCyril Allauzen, Michael Riley, Johan Schalkwyk, Wo-jciech Skut and Mehryar Mohri 2007.
OpenFst: AGeneral and Efficient Weighted Finite-State Trans-ducer Library.
Proceedings of the Ninth Interna-tional Conference on Implementation and Applica-tion of Automata, (CIAA), pages 11-23.Diamantino Caseiro, Isabel Trancosoo, Luis Oliveiraand Ceu Viana 2002.
Grapheme-to-phone using fi-nite state transducers.
Proceedings IEEE Workshopon Speech Synthesis.Asif Ekbal, Sudip Kumar Naskar and Sivaji Bandy-opadhyay.
2006.
A modified joint source-channelmodel for transliteration, Proceedings of the COL-ING/ACL, pages 191-198.Bo-June Hsu and James Glass 2008.
Iterative Lan-guage Model Estimation: Efficient Data Structure& Algorithms.
Proceedings Interspeech, pages 841-844.Kevin Knight and Jonathan Graehl.
1998.
MachineTransliteration, Association for Computational Lin-guistics.John Lafferty, Andrew McCallum, and FernandoPereira 2001.
Conditional Random Fields: Prob-abilistic Models for Segmenting and Labeling Se-quence Data., Proceedings of International Confer-ence on Machine Learning, pages 282-289.Haizhou Li, Min Zhang and Jian Su.
2004.
A jointsource-channel model for machine transliteration,Proceedings of the 42nd Annual Meeting on Asso-ciation for Computational Linguistics.Haizhou Li, A. Kumaran, Vladimir Pervouchine andMin Zhang 2009.
Report of NEWS 2009 Ma-chine Transliteration Shared Task, Proceedings ofthe 2009 Named Entities Workshop: Shared Task onTransliteration (NEWS 2009), pages 1-18Jong-Hoon Oh, Key-Sun Choi and Hitoshi Isahara.2006.
A comparison of different machine transliter-ation models , Journal of Artificial Intelligence Re-search, 27, pages 119-151.Richard Sproat 2000.
Corpus-Based Methods andHand-Built Methods.
Proceedings of InternationalConference on Spoken Language Processing, pages426-428.Andrew J. Viterbi 1967.
Error Bounds for Convolu-tional Codes and an Asymptotically Optimum De-coding Algorithm.
IEEE Transactions on Informa-tion Theory, Volume IT-13, pages 260-269.Hanna Wallach 2002.
Efficient Training of Condi-tional Random Fields.
M. Thesis, University of Ed-inburgh.Dong Yang, Paul Dixon, Yi-Cheng Pan, Tasuku Oon-ishi, Masanobu Nakamura and Sadaoki Furui 2009.Combining a Two-step Conditional Random FieldModel and a Joint Source Channel Model for Ma-chine Transliteration, Proceedings of the 2009Named Entities Workshop: Shared Task on Translit-eration (NEWS 2009), pages 72-75280
