Fast Methods for Kernel-based Text AnalysisTaku Kudo and Yuji MatsumotoGraduate School of Information Science,Nara Institute of Science and Technology{taku-ku,matsu}@is.aist-nara.ac.jpAbstractKernel-based learning (e.g., Support Vec-tor Machines) has been successfully ap-plied to many hard problems in NaturalLanguage Processing (NLP).
In NLP, al-though feature combinations are crucial toimproving performance, they are heuris-tically selected.
Kernel methods changethis situation.
The merit of the kernelmethods is that effective feature combina-tion is implicitly expanded without lossof generality and increasing the compu-tational costs.
Kernel-based text analysisshows an excellent performance in termsin accuracy; however, these methods areusually too slow to apply to large-scaletext analysis.
In this paper, we extenda Basket Mining algorithm to convert akernel-based classifier into a simple andfast linear classifier.
Experimental resultson English BaseNP Chunking, JapaneseWord Segmentation and Japanese Depen-dency Parsing show that our new classi-fiers are about 30 to 300 times faster thanthe standard kernel-based classifiers.1 IntroductionKernel methods (e.g., Support Vector Machines(Vapnik, 1995)) attract a great deal of attention re-cently.
In the field of Natural Language Process-ing, many successes have been reported.
Examplesinclude Part-of-Speech tagging (Nakagawa et al,2002) Text Chunking (Kudo and Matsumoto, 2001),Named Entity Recognition (Isozaki and Kazawa,2002), and Japanese Dependency Parsing (Kudo andMatsumoto, 2000; Kudo and Matsumoto, 2002).It is known in NLP that combination of featurescontributes to a significant improvement in accuracy.For instance, in the task of dependency parsing, itwould be hard to confirm a correct dependency re-lation with only a single set of features from eithera head or its modifier.
Rather, dependency relationsshould be determined by at least information fromboth of two phrases.
In previous research, featurecombination has been selected manually, and theperformance significantly depended on these selec-tions.
This is not the case with kernel-based method-ology.
For instance, if we use a polynomial ker-nel, all feature combinations are implicitly expandedwithout loss of generality and increasing the compu-tational costs.
Although the mapped feature spaceis quite large, the maximal margin strategy (Vapnik,1995) of SVMs gives us a good generalization per-formance compared to the previous manual featureselection.
This is the main reason why kernel-basedlearning has delivered great results to the field ofNLP.Kernel-based text analysis shows an excellent per-formance in terms in accuracy; however, its inef-ficiency in actual analysis limits practical applica-tion.
For example, an SVM-based NE-chunker runsat a rate of only 85 byte/sec, while previous rule-based system can process several kilobytes per sec-ond (Isozaki and Kazawa, 2002).
Such slow exe-cution time is inadequate for Information Retrieval,Question Answering, or Text Mining, where fastanalysis of large quantities of text is indispensable.This paper presents two novel methods that makethe kernel-based text analyzers substantially faster.These methods are applicable not only to the NLPtasks but also to general machine learning taskswhere training and test examples are represented ina binary vector.More specifically, we focus on a Polynomial Ker-nel of degree d, which can attain feature combina-tions that are crucial to improving the performanceof tasks in NLP.
Second, we introduce two fast clas-sification algorithms for this kernel.
One is PKI(Polynomial Kernel Inverted), which is an exten-sion of Inverted Index in Information Retrieval.
Theother is PKE (Polynomial Kernel Expanded), whereall feature combinations are explicitly expanded.
Byapplying PKE, we can convert a kernel-based clas-sifier into a simple and fast liner classifier.
In orderto build PKE, we extend the PrefixSpan (Pei et al,2001), an efficient Basket Mining algorithm, to enu-merate effective feature combinations from a set ofsupport examples.Experiments on English BaseNP Chunking,Japanese Word Segmentation and Japanese Depen-dency Parsing show that PKI and PKE perform re-spectively 2 to 13 times and 30 to 300 times fasterthan standard kernel-based systems, without a dis-cernible change in accuracy.2 Kernel Method and Support VectorMachinesSuppose we have a set of training data for a binaryclassification problem:(x1, y1), .
.
.
, (xL, yL) xj ?
<N , yj ?
{+1,?1},where xj is a feature vector of the j-th training sam-ple, and yj is the class label associated with thistraining sample.
The decision function of SVMs isdefined byy(x) = sgn( ?j?SVyj?j?
(xj) ?
?
(x) + b), (1)where: (A) ?
is a non-liner mapping function from<N to <H (N ?
H).
(B) ?j , b ?
<, ?j ?
0.The mapping function ?
should be designed suchthat all training examples are linearly separable in<H space.
Since H is much larger than N , it re-quires heavy computation to evaluate the dot prod-ucts ?
(xi) ?
?
(x) in an explicit form.
This problemcan be overcome by noticing that both constructionof optimal parameter ?i (we will omit the detailsof this construction here) and the calculation of thedecision function only require the evaluation of dotproducts ?
(xi) ??(x).
This is critical, since, in somecases, the dot products can be evaluated by a simpleKernel Function: K(x1,x2) = ?
(x1) ?
?(x2).
Sub-stituting kernel function into (1), we have the fol-lowing decision function.y(x) = sgn( ?j?SVyj?jK(xj ,x) + b)(2)One of the advantages of kernels is that they are notlimited to vectorial object x, but that they are appli-cable to any kind of object representation, just giventhe dot products.3 Polynomial Kernel of degree dFor many tasks in NLP, the training and test ex-amples are represented in binary vectors; or sets,since examples in NLP are usually represented in so-called Feature Structures.
Here, we focus on suchcases 1.Suppose a feature set F = {1, 2, .
.
.
, N} andtraining examples Xj(j = 1, 2, .
.
.
, L), all ofwhich are subsets of F (i.e., Xj ?
F ).
In thiscase, Xj can be regarded as a binary vector xj =(xj1, xj2, .
.
.
, xjN ) where xji = 1 if i ?
Xj ,xji = 0 otherwise.
The dot product of x1 and x2is given by x1 ?
x2 = |X1 ?X2|.Definition 1 Polynomial Kernel of degree dGiven sets X and Y , corresponding to binary fea-ture vectors x and y, Polynomial Kernel of degree dKd(X,Y ) is given byKd(x,y) = Kd(X,Y ) = (1 + |X ?
Y |)d, (3)where d = 1, 2, 3, .
.
..In this paper, (3) will be referred to as an implicitform of the Polynomial Kernel.1In the Maximum Entropy model widely applied in NLP, weusually suppose binary feature functions fi(Xj) ?
{0, 1}.
Thisformalization is exactly same as representing an example Xj ina set {k|fk(Xj) = 1}.It is known in NLP that a combination of features,a subset of feature set F in general, contributes tooverall accuracy.
In previous research, feature com-bination has been selected manually.
The use ofa polynomial kernel allows such feature expansionwithout loss of generality or an increase in compu-tational costs, since the Polynomial Kernel of degreed implicitly maps the original feature space F intoF d space.
(i.e., ?
: F ?
F d).
This property iscritical and some reports say that, in NLP, the poly-nomial kernel outperforms the simple linear kernel(Kudo and Matsumoto, 2000; Isozaki and Kazawa,2002).Here, we will give an explicit form of the Polyno-mial Kernel to show the mapping function ?(?
).Lemma 1 Explicit form of Polynomial Kernel.The Polynomial Kernel of degree d can be rewrittenasKd(X,Y ) =d?r=0cd(r) ?
|Pr(X ?
Y )|, (4)where?
Pr(X) is a set of all subsets of X with exactlyr elements in it,?
cd(r) =?dl=r(dl)(?rm=0(?1)r?m ?ml( rm)).Proof See Appendix A.cd(r) will be referred as a subset weight of the Poly-nomial Kernel of degree d. This function gives aprior weight to the subset s, where |s| = r.Example 1 Quadratic and Cubic KernelGiven sets X = {a, b, c, d} and Y = {a, b, d, e},the Quadratic Kernel K2(X,Y ) and the Cubic Ker-nel K3(X,Y ) can be calculated in an implicit formas:K2(X,Y ) = (1 + |X ?
Y |)2 = (1 + 3)2 = 16,K3(X,Y ) = (1 + |X ?
Y |)3 = (1 + 3)3 = 64.Using Lemma 1, the subset weights of theQuadratic Kernel and the Cubic Kernel can be cal-culated as c2(0) = 1, c2(1) = 3, c2(2) = 2 andc3(0)=1, c3(1)=7, c3(2)=12, c3(3)=6.In addition, subsets Pr(X ?Y ) (r = 0, 1, 2, 3)are given as follows: P0(X ?
Y ) ={?
}, P1(X?Y ) = {{a}, {b}, {d}}, P2(X?Y ) ={{a, b}, {a, d}, {b, d}}, P3(X ?Y ) = {{a, b, d}}.K2(X,Y ) and K3(X,Y ) can similarly be calcu-lated in an explicit form as:function PKI classify (X)r = 0 # an array, initialized as 0foreach i ?Xforeach j ?
h(i)rj = rj + 1endendresult = 0foreach j ?
SVresult = result+ yj?j ?
(1 + rj)dendreturn sgn(result+ b)endFigure 1: Pseudo code for PKIK2(X,Y ) = 1 ?
1 + 3 ?
3 + 2 ?
3 = 16,K3(X,Y ) = 1 ?
1 + 7 ?
3 + 12 ?
3 + 6 ?
1 = 64.4 Fast Classifiers for Polynomial KernelIn this section, we introduce two fast classificationalgorithms for the Polynomial Kernel of degree d.Before describing them, we give the baseline clas-sifier (PKB):y(X) = sgn( ?j?SVyj?j ?
(1 + |Xj ?X|)d + b).
(5)The complexity of PKB is O(|X| ?
|SV |), since ittakes O(|X|) to calculate (1+ |Xj ?X|)d and thereare a total of |SV | support examples.4.1 PKI (Inverted Representation)Given an item i ?
F , if we know in advance theset of support examples which contain item i ?
F ,we do not need to calculate |Xj ?X| for all supportexamples.
This is a naive extension of Inverted In-dexing in Information Retrieval.
Figure 1 shows thepseudo code of the algorithm PKI.
The function h(i)is a pre-compiled table and returns a set of supportexamples which contain item i.The complexity of the PKI is O(|X| ?B + |SV |),where B is an average of |h(i)| over all item i ?
F .The PKI can make the classification speed drasti-cally faster when B is small, in other words, whenfeature space is relatively sparse (i.e., B ?
|SV |).The feature space is often sparse in many tasks inNLP, since lexical entries are used as features.The algorithm PKI does not change the final ac-curacy of the classification.4.2 PKE (Expanded Representation)4.2.1 Basic Idea of PKEUsing Lemma 1, we can represent the decisionfunction (5) in an explicit form:y(X) = sgn(?j?SVyj?j( d?r=0cd(r) ?
|Pr(Xj ?X)|)+ b).
(6)If we, in advance, calculatew(s) =?j?SVyj?jcd(|s|)I(s ?
P|s|(Xj))(where I(t) is an indicator function 2) for all subsetss ?
?dr=0 Pr(F ), (6) can be written as the followingsimple linear form:y(X) = sgn( ?s?
?d(X)w(s) + b).
(7)where ?d(X) =?dr=0 Pr(X).The classification algorithm given by (7) will bereferred to as PKE.
The complexity of PKE isO(|?d(X)|) = O(|X|d), independent on the num-ber of support examples |SV |.4.2.2 Mining Approach to PKETo apply the PKE, we first calculate |?d(F )| de-gree of vectorsw = (w(s1), w(s2), .
.
.
, w(s|?d(F )|)).This calculation is trivial only when we use aQuadratic Kernel, since we just project the origi-nal feature space F into F ?
F space, which issmall enough to be calculated by a naive exhaustivemethod.
However, if we, for instance, use a poly-nomial kernel of degree 3 or higher, this calculationbecomes not trivial, since the size of feature spaceexponentially increases.
Here we take the followingstrategy:1.
Instead of using the original vector w, we usew?, an approximation of w.2.
We apply the Subset Mining algorithm to cal-culate w?
efficiently.2I(t) returns 1 if t is true,returns 0 otherwise.Definition 2 w?
: An approximation of wAn approximation of w is given by w?
=(w?
(s1), w?
(s2), .
.
.
, w?
(s|?d(F )|)), where w?
(s) isset to 0 if w(s) is trivially close to 0.
(i.e., ?neg <w(s) < ?pos (?neg < 0, ?pos > 0), where ?pos and?neg are predefined thresholds).The algorithm PKE is an approximation of thePKB, and changes the final accuracy according tothe selection of thresholds ?pos and ?neg.
The cal-culation of w?
is formulated as the following miningproblem:Definition 3 Feature Combination MiningGiven a set of support examples and subset weightcd(r), extract all subsets s and their weights w(s) ifw(s) holds w(s) ?
?pos or w(s) ?
?neg .In this paper, we apply a Sub-Structure Miningalgorithm to the feature combination mining prob-lem.
Generally speaking, sub-structures mining al-gorithms efficiently extract frequent sub-structures(e.g., subsets, sub-sequences, sub-trees, or sub-graphs) from a large database (set of transactions).In this context, frequent means that there are no lessthan ?
transactions which contain a sub-structure.The parameter ?
is usually referred to as the Mini-mum Support.
Since we must enumerate all subsetsof F , we can apply subset mining algorithm, in sometimes called as Basket Mining algorithm, to our task.There are many subset mining algorithms pro-posed, however, we will focus on the PrefixSpan al-gorithm, which is an efficient algorithm for sequen-tial pattern mining, originally proposed by (Pei etal., 2001).
The PrefixSpan was originally designedto extract frequent sub-sequence (not subset) pat-terns, however, it is a trivial difference since a setcan be seen as a special case of sequences (i.e., bysorting items in a set by lexicographic order, the setbecomes a sequence).
The basic idea of the PrefixS-pan is to divide the database by frequent sub-patterns(prefix) and to grow the prefix-spanning pattern in adepth-first search fashion.We now modify the PrefixSpan to suit to our fea-ture combination mining.?
size constraintWe only enumerate up to subsets of size d.when we plan to apply the Polynomial Kernelof degree d.?
Subset weight cd(r)In the original PrefixSpan, the frequency ofeach subset does not change by its size.
How-ever, in our mining task, it changes (i.e., thefrequency of subset s is weighted by cd(|s|)).Here, we process the mining algorithm byassuming that each transaction (support ex-ample Xj) has its frequency Cdyj?j , whereCd = max(cd(1), cd(2), .
.
.
, cd(d)).
Theweight w(s) is calculated by w(s) = ?
(s) ?cd(|s|)/Cd, where ?
(s) is a frequency of s,given by the original PrefixSpan.?
Positive/Negative support examplesWe first divide the support examples into posi-tive (yi > 0) and negative (yi < 0) examples,and process mining independently.
The resultcan be obtained by merging these two results.?
Minimum Supports ?pos, ?negIn the original PrefixSpan, minimum support isan integer.
In our mining task, we can give areal number to minimum support, since eachtransaction (support example Xj) has possiblynon-integer frequency Cdyj?j .
Minimum sup-ports ?pos and ?neg control the rate of approx-imation.
For the sake of convenience, we justgive one parameter ?, and calculate ?pos and?neg as follows?pos = ?
?
(#of positive examples#of support examples),?neg = ??
?
(#of negative examples#of support examples).After the process of mining, a set of tuples ?
={?s, w(s)?}
is obtained, where s is a frequent subsetand w(s) is its weight.
We use a TRIE to efficientlystore the set ?.
The example of such TRIE compres-sion is shown in Figure 2.
Although there are manyimplementations for TRIE, we use a Double-Array(Aoe, 1989) in our task.
The actual classification ofPKE can be examined by traversing the TRIE for allsubsets s ?
?d(X).5 ExperimentsTo demonstrate performances of PKI and PKE, weexamined three NLP tasks: English BaseNP Chunk-ing (EBC), Japanese Word Segmentation (JWS) and    	  	 	    	   fffi flffiffiflflfl!#"$"&%'(#) *'+,'-+.
'-(#) *.+-/.'0.'-+.
'-+s1Figure 2: ?
in TRIE representationJapanese Dependency Parsing (JDP).
A more de-tailed description of each task, training and test data,the system parameters, and feature sets are presentedin the following subsections.
Table 1 summarizesthe detail information of support examples (e.g., sizeof SVs, size of feature set etc.
).Our preliminary experiments show that aQuadratic Kernel performs the best in EBC, and aCubic Kernel performs the best in JWS and JDP.The experiments using a Cubic Kernel are suitableto evaluate the effectiveness of the basket miningapproach applied in the PKE, since a Cubic Kernelprojects the original feature space F into F 3 space,which is too large to be handled only using a naiveexhaustive method.All experiments were conducted under Linux us-ing XEON 2.4 Ghz dual processors and 3.5 Gbyte ofmain memory.
All systems are implemented in C++.5.1 English BaseNP Chunking (EBC)Text Chunking is a fundamental task in NLP ?
divid-ing sentences into non-overlapping phrases.
BaseNPchunking deals with a part of this task and recog-nizes the chunks that form noun phrases.
Here is anexample sentence:[He] reckons [the current account deficit]will narrow to [only $ 1.8 billion] .A BaseNP chunk is represented as sequence ofwords between square brackets.
BaseNP chunkingtask is usually formulated as a simple tagging task,where we represent chunks with three types of tags:B: beginning of a chunk.
I: non-initial word.
O:outside of the chunk.
In our experiments, we usedthe same settings as (Kudo and Matsumoto, 2002).We use a standard data set (Ramshaw and Marcus,1995) consisting of sections 15-19 of the WSJ cor-pus as training and section 20 as testing.5.2 Japanese Word Segmentation (JWS)Since there are no explicit spaces between words inJapanese sentences, we must first identify the wordboundaries before analyzing deep structure of a sen-tence.
Japanese word segmentation is formalized asa simple classification task.Let s = c1c2 ?
?
?
cm be a sequence of Japanesecharacters, t = t1t2 ?
?
?
tm be a sequence of Japanesecharacter types 3 associated with each character,and yi ?
{+1,?1}, (i = (1, 2, .
.
.
,m?1)) be aboundary marker.
If there is a boundary between ciand ci+1, yi = 1, otherwise yi = ?1.
The featureset of example xi is given by all characters as wellas character types in some constant window (e.g., 5):{ci?2, ci?1, ?
?
?
, ci+2, ci+3, ti?2, ti?1, ?
?
?
, ti+2, ti+3}.Note that we distinguish the relative position ofeach character and character type.
We use the KyotoUniversity Corpus (Kurohashi and Nagao, 1997),7,958 sentences in the articles on January 1st toJanuary 7th are used as training data, and 1,246sentences in the articles on January 9th are used asthe test data.5.3 Japanese Dependency Parsing (JDP)The task of Japanese dependency parsing is to iden-tify a correct dependency of each Bunsetsu (basephrase in Japanese).
In previous research, we pre-sented a state-of-the-art SVMs-based Japanese de-pendency parser (Kudo and Matsumoto, 2002).
Wecombined SVMs into an efficient parsing algorithm,Cascaded Chunking Model, which parses a sentencedeterministically only by deciding whether the cur-rent chunk modifies the chunk on its immediate righthand side.
The input for this algorithm consists ofa set of the linguistic features related to the headand modifier (e.g., word, part-of-speech, and inflec-tions), and the output from the algorithm is either ofthe value +1 (dependent) or -1 (independent).
Weuse a standard data set, which is the same corpus de-scribed in the Japanese Word Segmentation.3Usually, in Japanese, word boundaries are highly con-strained by character types, such as hiragana and katakana(both are phonetic characters in Japanese), Chinese characters,English alphabets and numbers.5.4 ResultsTables 2, 3 and 4 show the execution time, accu-racy4, and |?| (size of extracted subsets), by chang-ing ?
from 0.01 to 0.0005.The PKI leads to about 2 to 12 times improve-ments over the PKB.
In JDP, the improvement is sig-nificant.
This is because B, the average of h(i) overall items i ?
F , is relatively small in JDP.
The im-provement significantly depends on the sparsity ofthe given support examples.The improvements of the PKE are more signifi-cant than the PKI.
The running time of the PKE is30 to 300 times faster than the PKB, when we set anappropriate ?, (e.g., ?
= 0.005 for EBC and JWS,?
= 0.0005 for JDP).
In these settings, we couldpreserve the final accuracies for test data.5.5 Frequency-based PruningThe PKE with a Cubic Kernel tends to make ?
large(e.g., |?| = 2.32 million for JWS, |?| = 8.26 mil-lion for JDP).To reduce the size of ?, we examined sim-ple frequency-based pruning experiments.
Our ex-tension is to simply give a prior threshold ?
(=1, 2, 3, 4 .
.
.
), and erase all subsets which occur inless than ?
support examples.
The calculation of fre-quency can be similarly conducted by the PrefixS-pan algorithm.
Tables 5 and 6 show the results offrequency-based pruning, when we fix ?=0.005 forJWS, and ?=0.0005 for JDP.In JDP, we can make the size of set ?
about onethird of the original size.
This reduction gives usnot only a slight speed increase but an improvementof accuracy (89.29%?89.34%).
Frequency-basedpruning allows us to remove subsets that have largeweight and small frequency.
Such subsets may begenerated from errors or special outliers in the train-ing examples, which sometimes cause an overfittingin training.In JWS, the frequency-based pruning does notwork well.
Although we can reduce the sizeof ?
by half, the accuracy is also reduced(97.94%?97.83%).
It implies that, in JWS, featureseven with frequency of one contribute to the final de-cision hyperplane.4In EBC, accuracy is evaluated using F measure, harmonicmean between precision and recall.Table 1: Details of Data SetData Set EBC JWS JDP# of examples 135,692 265,413 110,355|SV| # of SVs 11,690 57,672 34,996# of positive SVs 5,637 28,440 17,528# of negative SVs 6,053 29,232 17,468|F | (size of feature) 17,470 11,643 28,157Avg.
of |Xj | 11.90 11.73 17.63B (Avg.
of |h(i)|)) 7.74 58.13 21.92(Note: In EBC, to handle K-class problems, we use a pairwiseclassification; building K?
(K?1)/2 classifiers considering allpairs of classes, and final class decision was given by majorityvoting.
The values in this column are averages over all pairwiseclassifiers.
)6 DiscussionThere have been several studies for efficient classi-fication of SVMs.
Isozaki et al propose an XQK(eXpand the Quadratic Kernel) which can make theirNamed-Entity recognizer drastically fast (Isozakiand Kazawa, 2002).
XQK can be subsumed intoPKE.
Both XQK and PKE share the basic idea; allfeature combinations are explicitly expanded and weconvert the kernel-based classifier into a simple lin-ear classifier.The explicit difference between XQK and PKE isthat XQK is designed only for Quadratic Kernel.
Itimplies that XQK can only deal with feature com-bination of size up to two.
On the other hand, PKEis more general and can also be applied not only tothe Quadratic Kernel but also to the general-style ofpolynomial kernels (1 + |X ?
Y |)d. In PKE, thereare no theoretical constrains to limit the size of com-binations.In addition, Isozaki et al did not mention how toexpand the feature combinations.
They seem to usea naive exhaustive method to expand them, which isnot always scalable and efficient for extracting threeor more feature combinations.
PKE takes a basketmining approach to enumerating effective featurecombinations more efficiently than their exhaustivemethod.7 Conclusion and Future WorksWe focused on a Polynomial Kernel of degree d,which has been widely applied in many tasks in NLPTable 2: Results of EBCPKE Time Speedup F1 |?|?
(sec./sent.)
Ratio (?
1000)0.01 0.0016 105.2 93.79 5180.005 0.0016 101.3 93.85 6680.001 0.0017 97.7 93.84 8580.0005 0.0017 96.8 93.84 889PKI 0.020 8.3 93.84PKB 0.164 1.0 93.84Table 3: Results of JWSPKE Time Speedup Acc.
(%) |?|?
(sec./sent.)
Ratio (?
1000)0.01 0.0024 358.2 97.93 1,2280.005 0.0028 300.1 97.95 2,3270.001 0.0034 242.6 97.94 4,3920.0005 0.0035 238.8 97.94 4,820PKI 0.4989 1.7 97.94PKB 0.8535 1.0 97.94Table 4: Results of JDPPKE Time Speedup Acc.
(%) |?|?
(sec./sent.)
Ratio (?
1000)0.01 0.0042 66.8 88.91 730.005 0.0060 47.8 89.05 1,9240.001 0.0086 33.3 89.26 6,6860.0005 0.0090 31.8 89.29 8,262PKI 0.0226 12.6 89.29PKB 0.2848 1.0 89.29Table 5: Frequency-based pruning (JWS)PKE time Speedup Acc.
(%) |?|?
(sec./sent.)
Ratio (?
1000)1 0.0028 300.1 97.95 2,3272 0.0025 337.3 97.83 9543 0.0023 367.0 97.83 591PKB 0.8535 1.0 97.94Table 6: Frequency-based pruning (JDP)PKE time Speedup Acc.
(%) |?|?
(sec./sent.)
Ratio (?
1000)1 0.0090 31.8 89.29 8,2622 0.0072 39.3 89.34 2,4503 0.0068 41.8 89.31 1,360PKB 0.2848 1.0 89.29and can attain feature combination that is crucial toimproving the performance of tasks in NLP.
Then,we introduced two fast classification algorithms forthis kernel.
One is PKI (Polynomial Kernel In-verted), which is an extension of Inverted Index.
Theother is PKE (Polynomial Kernel Expanded), whereall feature combinations are explicitly expanded.The concept in PKE can also be applicable to ker-nels for discrete data structures, such as String Ker-nel (Lodhi et al, 2002) and Tree Kernel (Kashimaand Koyanagi, 2002; Collins and Duffy, 2001).For instance, Tree Kernel gives a dot product ofan ordered-tree, and maps the original ordered-treeonto its all sub-tree space.
To apply the PKE, wemust efficiently enumerate the effective sub-treesfrom a set of support examples.
We can similarlyapply a sub-tree mining algorithm (Zaki, 2002) tothis problem.Appendix A.: Lemma 1 and its proofcd(r) =d?l=r(dl)( r?m=0(?1)r?m ?ml(rm)).Proof.Let X,Y be subsets of F = {1, 2, .
.
.
, N}.
In this case, |X ?Y | is same as the dot product of vector x,y, wherex = {x1, x2, .
.
.
, xN}, y = {y1, y2, .
.
.
, yN}(xj , yj ?
{0, 1})xj = 1 if j ?
X , xj = 0 otherwise.
(1 + |X ?
Y |)d = (1 + x ?
y)d can be expanded as follows(1 + x ?
y)d =d?l=0(dl)( N?j=1xjyj)l=d?l=0(dl)?
?(l)where?
(l) =k1+...+kN=l?kn?0l!k1!
.
.
.
kN !
(x1y1)k1 .
.
.
(xNyN )kN .Note that xkjj is binary (i.e., xkjj ?
{0, 1}), the num-ber of r-size subsets can be given by a coefficient of(x1y1x2y2 .
.
.
xryr).
Thus,cd(r) =d?l=r(dl)( k1+...+kr=l?kn?1,n=1,2,...,rl!k1!
.
.
.
kr!)=d?l=r(dl)(rl?
(r1)(r?1)l+(r2)(r?2)l ?
.
.
.
)=d?l=r(dl)( r?m=0(?1)r?m ?ml(rm)).
2ReferencesJunichi Aoe.
1989.
An efficient digital search algorithm by us-ing a double-array structure.
IEEE Transactions on SoftwareEngineering, 15(9).Michael Collins and Nigel Duffy.
2001.
Convolution kernelsfor natural language.
In Advances in Neural InformationProcessing Systems 14, Vol.1 (NIPS 2001), pages 625?632.Hideki Isozaki and Hideto Kazawa.
2002.
Efficient supportvector classifiers for named entity recognition.
In Proceed-ings of the COLING-2002, pages 390?396.Hisashi Kashima and Teruo Koyanagi.
2002.
Svm kernelsfor semi-structured data.
In Proceedings of the ICML-2002,pages 291?298.Taku Kudo and Yuji Matsumoto.
2000.
Japanese DependencyStructure Analysis based on Support Vector Machines.
InProceedings of the EMNLP/VLC-2000, pages 18?25.Taku Kudo and Yuji Matsumoto.
2001.
Chunking with supportvector machines.
In Proceedings of the the NAACL, pages192?199.Taku Kudo and Yuji Matsumoto.
2002.
Japanese dependencyanalyisis using cascaded chunking.
In Proceedings of theCoNLL-2002, pages 63?69.Sadao Kurohashi and Makoto Nagao.
1997.
Kyoto Universitytext corpus project.
In Proceedings of the ANLP-1997, pages115?118.Huma Lodhi, Craig Saunders, John Shawe-Taylor, Nello Cris-tianini, and Chris Watkins.
2002.
Text classification usingstring kernels.
Journal of Machine Learning Research, 2.Tetsuji Nakagawa, Taku Kudo, and Yuji Matsumoto.
2002.
Re-vision learning and its application to part-of-speech tagging.In Proceedings of the ACL 2002, pages 497?504.Jian Pei, Jiawei Han, and et al 2001.
Prefixspan: Miningsequential patterns by prefix-projected growth.
In Proc.
ofInternational Conference of Data Engineering, pages 215?224.Lance A. Ramshaw and Mitchell P. Marcus.
1995.
Text chunk-ing using transformation-based learning.
In Proceedings ofthe VLC, pages 88?94.Vladimir N. Vapnik.
1995.
The Nature of Statistical LearningTheory.
Springer.Mohammed Zaki.
2002.
Efficiently mining frequent trees in aforest.
In Proceedings of the 8th International Conferenceon Knowledge Discovery and Data Mining KDD, pages 71?80.
