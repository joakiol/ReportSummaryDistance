Proceedings of the Interactive Question Answering Workshop at HLT-NAACL 2006, pages 9?16,New York City, NY, USA.
June 2006. c?2006 Association for Computational LinguisticsWoZ Simulation of Interactive Question AnsweringTsuneaki KatoThe University of Tokyokato@boz.c.u-tokyo.ac.jpFumito MasuiMie Universitymasui@ai.info.mie-u.ac.jpJun?ichi FukumotoRitsumeikan Universityfukumoto@media.ritsumei.ac.jpNoriko KandoNational Institute of Informaticskando@nii.ac.jpAbstractQACIAD (Question Answering Chal-lenge for Information Access Dialogue)is an evaluation framework for measur-ing interactive question answering (QA)technologies.
It assumes that users inter-actively collect information using a QAsystem for writing a report on a giventopic and evaluates, among other things,the capabilities needed under such cir-cumstances.
This paper reports an ex-periment for examining the assumptionsmade by QACIAD.
In this experiment, di-alogues under the situation that QACIADassumes are collected using WoZ (Wiz-ard of Oz) simulating, which is frequentlyused for collecting dialogue data for de-signing speech dialogue systems, and thenanalyzed.
The results indicate that the set-ting of QACIAD is real and appropriateand that one of the important capabilitiesfor future interactive QA systems is pro-viding cooperative and helpful responses.1 IntroductionOpen-domain question answering (QA) technolo-gies allow users to ask a question using natural lan-guage and obtain the answer itself rather than a listof documents that contain the answer (Voorhees etal.2000).
While early research in this field concen-trated on answering factoid questions one by one inan isolated manner, recent research appears to bemoving in several new directions.
Using QA sys-tems in an interactive environment is one of thosedirections.
A context task was attempted in orderto evaluate the systems?
ability to track context forsupporting interactive user sessions at TREC 2001(Voorhees 2001).
Since TREC 2004, questions inthe task have been given as collections of questionsrelated to common topics, rather than ones that areisolated and independent of each other (Voorhees2004).
It is important for researchers to recognizethat such a cohesive manner is natural in QA, al-though the task itself is not intended for evaluatingcontext processing abilities since, as it is given thecommon topic, sophisticated context processing isnot needed.Such a direction has also been envisaged as a re-search roadmap, in which QA systems become moresophisticated and can be used by professional re-porters and information analysts (Burger et al2001).At some stage of that sophistication, a young re-porter writing an article on a specific topic will beable to translate the main issue into a set of simplerquestions and pose those questions to the QA sys-tem.Another research trend in interactive QA has beenobserved in several projects that are part of theARDA AQUAINT program.
These studies concernscenario-based QA, the aim of which is to handlenon-factoid, explanatory, analytical questions posedby users with extensive background knowledge.
Is-sues include managing clarification dialogues in or-der to disambiguate users?
intentions and interests;and question decomposition to obtain simpler andmore tractable questions (Small et al2003)(Hickl et9al.2004).The nature of questions posed by users and pat-terns of interaction vary depending on the users whouse a QA system and on the environments in whichit is used (Liddy 2002).
The user may be a young re-porter, a trained analyst, or a common man withoutspecial training.
Questions can be answered by sim-ple names and facts, such as those handled in earlyTREC conferences (Chai et al2004), or by shortpassages retrieved like some systems developed inthe AQUAINT program do (Small et al2003).
Thesituation in which QA systems are supposed to beused is an important factor of the system design andthe evaluation must take such a factor into account.QACIAD (Question Answering Challenge for Infor-mation Access Dialogue) is an objective and quan-titative evaluation framework to measure the abil-ities of QA systems used interactively to partici-pate in dialogues for accessing information (Kato etal.2004a)(Kato et al2006).
It assumes the situationin which users interactively collect information us-ing a QA system for writing a report on a given topicand evaluates, among other things, the capabilitiesneeded under such circumstances, i.e.
proper inter-pretation of questions under a given dialogue con-text; in other words, context processing capabilitiessuch as anaphora resolution and ellipses handling.We are interested in examining the assumptionsmade by QACIAD, and conducted an experiment,in which the dialogues under the situation QACIADassumes were simulated using the WoZ (Wizard ofOz) technique (Fraser et al1991) and analyzed.
InWoZ simulation, which is frequently used for col-lecting dialogue data for designing speech dialoguesystems, dialogues that become possible when a sys-tem has been developed are simulated by a human, aWoZ, who plays the role of the system, as well as asubject who is not informed that a human is behav-ing as the system and plays the role of its user.
An-alyzing the characteristics of language expressionsand pragmatic devices used by users, we confirmwhether QACIAD is a proper framework for eval-uating QA systems used in the situation it assumes.We also examine what functions will be needed forsuch QA systems by analyzing intelligent behaviorof the WoZs.2 QACIAD and the previous studyQACIAD was proposed by Kato et al as a task ofQAC, which is a series of challenges for evaluat-ing QA technologies in Japanese (Kato et al2004b).QAC covers factoid questions in the form of com-plete sentences with interrogative pronouns.
Anyanswers to those questions should be names.
Here,?names?
means not only names of proper itemsincluding date expressions and monetary values(called ?named entities?
), but also common namessuch as those of species and body parts.
Althoughthe syntactical range of the names approximatelycorresponds to compound nouns, some of them,such as the titles of novels and movies, deviate fromthat range.
The underlying document set consistsof newspaper articles.
Being given various open-domain questions, systems are requested to extractexact answers rather than text snippets that containthe answers, and to return the answer along with thenewspaper article from which it was extracted.
Thearticle should guarantee the legitimacy of the answerto a given question.In QACIAD, which assumes interactive use ofQA systems, systems are requested to answer seriesof related questions.
The series of questions and theanswers to those questions comprise an informationaccess dialogue.
All questions except the first one ofeach series have some anaphoric expressions, whichmay be zero pronouns, while each question is in therange of those handled in QAC.
Although the sys-tems are supposed to participate in dialogue inter-actively, the interaction is only simulated; systemsanswer a series of questions in batch mode.
Sucha simulation may neglect the inherent dynamics ofdialogue, as the dialogue evolution is fixed before-hand and therefore not something that the systemscan control.
It is, however, a practical compromisefor an objective evaluation.
Since all participantsmust answer the same set of questions in the samecontext, the results for the same test set are compa-rable with each other, and the test sets of the task arereusable by pooling the correct answers.Systems are requested to return one list consistingof all and only correct answers.
Since the number ofcorrect answers differs for each question and is notgiven, a modified F measure is used for the evalu-ation, which takes into account both precision and10recall.Two types of series were included in the QA-CIAD, which correspond to two extremes of infor-mation access dialogue: a gathering type in whichthe user has a concrete objective such as writing areport and summary on a specific topic, and asksa system a series of questions related to that topic;and a browsing type in which the user does nothave any fixed topic of interest.
Although the QA-CIAD assumes that users are interactively collect-ing information on a given topic and the gathering-type dialogue mainly occurs under such circum-stances, browsing-type series are included in the taskbased on the observation that even when focusingon information access dialogue for writing reports,the systems must handle focus shifts appearing inbrowsing-type series.
The systems must identify thetype of series, as it is not given, although they neednot identify changes of series, as the boundary isgiven.
The systems must not look ahead to questionsfollowing the one currently being handled.
This re-striction reflects the fact that the QACIAD is a simu-lation of interactive use of QA systems in dialogues.Examples of series of QACIAD are shown in Fig-ure 1.
The original questions are in Japanese and thefigure shows their direct translations.The evaluation of QA technologies based on QA-CIAD were conducted twice in QAC2 and QAC3,which are a part of the NTCIR-4 and NTCIR-5workshops1, respectively (Kato et al2004b)(Kato etal.2005).
It was one of the three tasks of QAC2 andthe only task of QAC3.
On each occasion, severalnovel techniques were proposed for interactive QA.Kato et al conducted an experiment for confirm-ing the reality and appropriateness of QACIAD, inwhich subjects were presented various topics andwere requested to write down series of questionsin Japanese to elicit information for a report onthat topic (Kato et al2004a)(Kato et al2006).
Thereport was supposed to describe facts on a giventopic, rather than state opinions or prospects on thetopic.
The questions were restricted to wh-typequestions, and a natural series of questions that maycontain anaphoric expressions and ellipses was con-1The NTCIR Workshop is a series of evaluation workshopsdesigned to enhance research in information access technolo-gies including information retrieval, QA, text summarization,extraction, and so on (NTCIR 2006).Series 30002What genre does the ?Harry Potter?
series belong to?Who is the author?Who are the main characters in the series?When was the first book published?What was its title?How many books had been published by 2001?How many languages has it been translated into?How many copies have been sold in Japan?Series 30004When did Asahi breweries Ltd. start selling their low-maltbeer?What is the brand name?How much did it cost?What brands of low-malt beer were already on themarket at that time?Which company had the largest share?How much low-malt beer was sold compared to regularbeer?Which company made it originally?Series 30024Where was Universal Studio Japan constructed?What is the nearest train station?Which actor attended the ribbon-cutting ceremony on theopening day?Which movie that he featured in was released in the NewYear season of 2001?What movie starring Kevin Costner was released in thesame season?What was the subject matter of that movie?What role did Costner play in that movie?Figure 1: Examples of Series in QACIADstructed.
Analysis of the question series collectedin such a manner showed that 58% to 75% of ques-tions for writing reports could be answered by val-ues or names; a wide range of reference expres-sions is observed in questions in such a situation;and sequences of questions are sometimes very com-plicated and include subdialogues and focus shifts.From these observations they concluded the realityand appropriateness of the QACIAD, and validatedthe needs of browsing-type series in the task.One of the objectives of our experiment is to con-firm these results in a more realistic situation.
Theprevious experiment setting is far from the actualsituations in which QA systems are used, in whichsubjects have to write down their questions withoutgetting the answers.
Using WoZ simulation, it isconfirmed whether or not this difference affected theresult.
Moreover, observing the behavior of WoZs,the capabilities and functions needed for QA sys-11tems used in such a situation are investigated.3 SettingReferring to the headlines in Mainichi and Yomi-uri newspapers from 2000 and 2001, we selected101 topics, which included events, persons, and or-ganizations.
On each of those topics, a summaryof between 800 and 1600 characters long and anabstract of around 100 characters long were con-structed using a full text search system on the news-paper articles.2 Four experts shared this prepara-tion work.
Twenty topics were selected from amongthe original 101 on the basis that enough informa-tion was gathered and compiled into the summary.3The topics consisted of 5 persons, 2 organizations,7 events, 5 artifacts, and 1 syndrome, which includeMr.
Colin Powell, Charles, Prince of Wales, An ac-cident of a Russian nuclear-powerd submarine, Con-corde, ?Harry Potter?, and Windows XP.The four experts who shared the preparation workacted as a WoZ in the simulated QA systems, andeach expert participated in dialogues on the topic forwhich she/he wrote the summary and abstract, andtried to answer questions from users using the sum-mary, a full text search system, and her/his memory.Six subjects participated in the experiment and 10topics were assigned to each subject.
That is, eachtopic was handled by three subjects.
The subjectsbehaved as users of simulated QA systems.
Pre-sented with the topic and abstract, the subjects weredirected to think about questions beforehand assum-ing that they had to write a report on that topic, andthen they participated in an information access dia-logue with the ?QA system.?
The suggested numberof questions for each topic was around 10.The dialogues were conducted through keyboardsand character displays.
The subjects were told thatthe system could answer only simple factoid ques-tions, and the WoZs were instructed not to answercomplicated questions such as asking for reasonsand opinions.
No restrictions on the usage of lan-guage expressions were imposed but were left tothe discretion of the WoZs, though canned phrases2Those lengths of Japanese texts roughly correspond to 300to 600 words and 40 words of English text, respectively.3We prepared 101 topics to meet the additional needs ofother experiments.
It is absolutely not the case that only 20out of 101 were acceptable for our experiments.USR: What is the title of the first book of the ?HarryPotter?
series?SYS: ?Harry Potter and the Sorcerer?s Stone.
?USR: How many books were sold?SYS: The Japanese translation sold 600 thousand copiesin the first half year after its publication.USR: Who is the author?SYS: J. K. Rowling.USR: Who are the main characters (in that series)?SYS: Harry Potter, Ron Weasley, Hagrid and so on.USR: What kind of person is Harry Potter?SYS: A child with a wizard and a witch for parents.USR: What is the plot?SYS: It is an adventure story involving Harry Potter and hisfriends who enter a school for witchcraft and wizardry.USR: How many people went to the movie?SYS: In the US, box-office sales of the movie ?Harry Potterand the Sorcerer?s Stone?
reached 188 milliondollars in the first 10 days after release.Figure 2: Example of dialogues collectedsuch as ?Please wait a moment?
and ?Sorry, the an-swer could not be found?
were prepared in advance.The WoZs were also instructed that they could clar-ify users?
questions when they were ambiguous orvague, and that their answers should be simple butcooperative and helpful responses were not forbid-den.An example of the dialogues collected is shown inFigure 2.
In the figure, SYS stands for utterances ofthe QA system simulated by a WoZ and USR repre-sents that of the user, namely a subject.
In the rest ofthe paper, these are referred to as system?s utterancesand user?s utterances, respectively.4 Coding and ResultsExcluding meta-utterances for dialogue control suchas ?Please wait a moment?
and ?That?s all,?
620pairs of utterances were collected, of which 22 sys-tem utterances were for clarification.
Among the re-maining 598 cases, the system gave some answers in502 cases, and the other 94 utterances were negativeresponses: 86 utterances said that the answer couldnot found; 10 utterances said that the question wastoo complicated or that they could not answer suchtype of question.4.1 Characteristics of questions and answersThe syntactic classification of user utterances and itsdistribution is shown in Table 1.
The numbers in12Table 1: Syntactic classification of user utterancesSyntactic formWh-type Question 87.7% (544)Yes-no Question 9.5% (59)Imperative (Information request) 2.6% (16)Declarative (Answer to clarification) 0.2% (1)Table 2: Categorization of user utterances by subjectAsking aboutWho, Where, What 32.5% (201)When 16.3% (101)How much/many 16.8% (104)(for several types of numerical values)Why 6.5% (40)How (for procedures or situations) 17.0% (105)Definitions, Descriptions, Explanations 10.8% (67)Other (Multiple Whs) 0.2% (1)parentheses are numbers of occurrences.
In spite ofthe direction of using wh-type questions, more than10% of utterances are yes-no questions and impera-tives for requesting information.
Most of the userresponses to clarification questions from the sys-tem are rephrasing of the question concerned; onlyone response has a declarative form.
Examples ofrephrasing will be shown in section 4.3.The classification of user questions and requestsaccording to the subject asked or requested is shownin Table 2; the classification of system answers ac-cording to their syntactic and semantic categoriza-tion is shown in Table 3.
In Table 2, the classificationof yes-no questions was estimated based on the in-formation provided in the helpful responses to those.The classification in Table 3 was conducted based onthe syntactic and semantic form of the exact part ofthe answer itself rather than on whole utterances ofthe system.
For example, the categorization of thesystem utterance ?He was born on April 5, 1935,?which is the answer to ?When was Mr. Colin Powellborn??
is not a sentence but a date expression.4.2 Pragmatic phenomenaJapanese has four major types of anaphoric devices:pronouns, zero pronouns, definite noun phrases,Table 3: Categorization of user utterances by answertypeAnswered inNumerical values 14.3% (72)Date expressions 16.7% (84)Proper names 22.1% (111)Common names 8.8% (44)Compound nouns except names 4.2% (21)Noun phrases 6.2% (31)Clauses, sentences, or texts 27.7% (139)Table 4: Pragmatic phenomena observedTypeNo reference expression 203Pronouns 14Zero pronouns 317Definite noun phrases 104Ellipses 1and ellipses.
Zero pronouns are very common inJapanese, in which pronouns are not apparent on thesurface.
As Japanese also has a completely differentdeterminer system from English, the difference be-tween definite and indefinite is not apparent on thesurface, and definite noun phrases usually have thesame form as generic noun phrases.
Table 4 showsa summary of such pragmatic phenomena observed.The total number is more than 620 as some utter-ances contain more than one anaphoric expression.
?How many crew members were in the submarinewhen the accident happened??
is an example of sucha question with multiple anaphoric expressions.Among 203 questions with no reference expres-sion, 60 questions (30%) are the first one of a seriesof utterances on the same topic.
The others are di-vided into two cases.
In the first and major case, thecurrent foci appear literally rather than in pronounsor definite noun phrases.
The second case, which isrelatively minor, is the beginning of and the resumefrom a subdialogue.
For example, when a user him-self resumed talking about Charles, Prince of Walesafter a few questions about his children, the userused a literal expression like ?Who is the long-termpartner of Charles, Prince of Wales??
; after talking13about the details of the accident, a user asked ?Whatis the major characteristic of Concorde?
?Out of 437 reference expressions excluding el-lipses in Table 4, 94 expressions (22%) refer to itemsother than the global topic.
Moreover, 76 expres-sions of those (81%) are a sole reference expressionin the utterance.
The existence of questions with-out referring to the global topic, accompanied withthe existence of some questions with no referenceexpression, means that the focus shifts in those dia-logues, and then the local focus can differ from theglobal topic.4.3 ClarificationsAs mentioned above, only 22 clarification utteranceswere observed in the experiment.
They are catego-rized into those asking about an ambiguous referent(13 utterances) and those asking about a vague ques-tion subject (9 utterances).
Examples of each caseare shown below.Clarification asking about an ambiguous referentUSR: When was he [Mr. Colin Powell] installed as head ofsecurity of the United States?SYS: December 6, 2000.USR: Who was the president at that time?SYS: Mr. George W. Bush.USR: Which political party did he belong to?SYS: Whose political party are you asking about?USR: Which political party did Mr. Colin Powell belong to?Clarification asking about a vague question subjectUSR: Who developed Windows XP?SYS: Are you asking for a company name?USR: What company developed Windows XP?4.4 Cooperative and helpful responsesRather than simple and literal answers to the users?questions and requests, cooperative and helpful re-sponses, in which, for example, information not re-quested directly by a user was additionally provided(Kaplan 1983), were observed frequently.
The re-sults of analyzing such cooperative and helpful be-havior in the system responses are shown in Table5.
Questions asking reasons, procedures, situations,definitions, descriptions or explanations, which arecategorized into the last three categories except oth-ers in Table 2, were excluded from this analysis,since for those questions it is difficult to judge objec-Table 5: Cooperative and helpful behavior observedType of behaviorProviding additional information 52 (15.6%)Providing detailed information 38 (11.4%)Elaboration of subject 38 (11.4%)Providing alternative information 47 (14.1%)Exemplification 28 (8.4%)Providing speculations 3 (0.9%)tively whether some information is additional or notand, in addition, most of those are out of the scopeof QACIAD, which is our main interest.
Table 5shows the number of occurrences and its percentageamong positive responses.
Sometimes, more thanone behavior was observed in one utterance.
148utterances out of 334 positive responses (44%) con-tained some helpful behavior.
The definitions andmanually constructed examples of cooperative andhelpful behavior listed in Table 5 are shown below.Providing additional information Informationnot requested directly from a user is addition-ally provided.USR: What year was Mr. Colin Powell born?SYS: He was born in 1935 in New York.Providing detailed information More detailed in-formation than that requested directly by a useris provided.USR: What year was Mr. Colin Powell born?SYS: He was born on April 5, 1935.Elaboration of subject A relatively vague subjectof a question is elaborated and made concrete.In the following example, a question with avague expression, ?how well?
is answered bya value as a concrete statistic.USR: How well did the movie do?SYS: In the US, box-office sales reached 188 milliondollars in the first 10 days after release.Providing alternative information Rather thanexact information a user requested, alternativerelated information is provided.
An alternativemay be proposed by relaxing the conditionimposed by the user.
In the following example,attendance figures were altered to box-officesales.USR: How many people went to the movie?14SYS: In the US, box-office sales reached 188 milliondollars in the first 10 days after release.Exemplification Instead of a comprehensive list orabstracted general description, some major in-stances are listed.USR: What kinds of character goods are on sale?SYS: Stuffed owls, uniforms for schools of witchcraft andwizardry, and so on.Providing speculation Rather than fact, specula-tions and opinions are provided, which can beconsidered a special case of providing alterna-tive information.
Although there are few casesof such behavior in Table 5, there were manysuch answers in response to questions askingfor reasons and situations.USR: What exploded?SYS: It is surmised that torpedoes in the shed at the foreexploded.Detecting and correcting erroneous user assump-tions is a famous helpful behavior discussed in theliterature (Pollack 1989) (Kaplan 1983).
Only twooccurrences of such behavior were observed, one ofwhich is shown below.USR: How long did it take from the explosion until theshipwreck?SYS: The explosion happened while the submarine was goingdown, and ...5 DiscussionThe analysis of dialogue collected by the WoZ sim-ulation showed very similar results to those obtainedin a previous study, in which sequences of questionswere written down by subjects without knowing theanswers to questions.
That is, as shown in Table 2,when users asked questions to get information for areport, the number of why-questions was relativelysmall.
Moreover, there were fewer questions re-questing an explanation or definition than expected,probably because definition questions such as ?Whois Mr. Colin Powell??
were decomposed into rela-tively concrete questions such as those asking for hisbirthday and birthplace.
The remainder (65%) couldbe answered in values and names.
Table 3 indicatesthat 62% of the questions in our experiments wereanswered by values or names.
If compound nounsdescribing events or situations, which are usuallydistinguished from names, are considered to be inthe range of answers, the percentage of answerablequestions reaches 68%.
From these results, the set-ting of QACIAD looks realistic where users write re-ports interacting with a QA system handling factoidquestions that have values and names as answers.A wide range of reference expressions is observedin information access dialogues for writing reports.Moreover, our study confirmed that those sequencesof questions were sometimes very complicated andincluded subdialogues and focus shifts.
It is ex-pected that using an interactive QA system that canmanage those pragmatic phenomena will enable flu-ent information access dialogue for writing reports.In this sense, the objective of QACIAD is appropri-ate.It could be concluded from these results that thereality and appropriateness of QACIAD was recon-firmed in a more realistic situation.
And yet suspi-cion remains that even in our WoZ simulation, thesubjects were not motivated appropriately, as sug-gested by the lack of dynamic dialogue developmentin the example shown in Figure 2.
Especially, theusers often gave up too easily when they did notobtain answers to prepared questions.4 The truth,however, may be that in the environment of gath-ering information for writing reports, dynamic dia-logue development is limited compared to the casewhen trained analysts use QA systems for problemsolving.
If so, research on this type of QA systemsrepresents a proper milestone toward interactive QAsystems in a broad sense.Another finding of our experiment is the impor-tance of cooperative and helpful responses.
Nearlyhalf of WoZ utterances were not simple literal re-sponses but included some cooperative and helpfulbehavior.
This situation contrasts with a relativelysmall number of clarification dialogues.
The im-portance of this behavior, which was emphasizedin research on dialogues systems in the 80s and90s, was reconfirmed in the latest research, althoughquestion-answering technologies were redefined inthe late 90s.
Some behavior such as providing alter-native information could be viewed as a second-best4It is understandable, however, that there were few rephras-ing attempts since users were informed that paraphrasing suchas ?What is the population of the US??
to ?How many peopleare living in the US??
are usually in vain.15strategy of resource-bounded human WoZs.
Evenso, it is impossible to eliminate completely the needfor such a strategy by improving core QA technolo-gies.
In addition, intrinsic cooperative and helpfulbehavior such as providing additional informationwas also often observed.
These facts, accompaniedby the fact that such dialogues are perceived as fluentand felicitous, suggest that the capability to behavecooperatively and helpfully is essential for interac-tive QA technologies.6 ConclusionThrough WoZ simulation, the capabilities and func-tions needed for interactive QA systems used as aparticipant in information access dialogues for writ-ing reports were examined.
The results are compati-ble with those of previous research, and reconfirmedthe reality and appropriateness of QACIAD.
A newfinding of our experiment is the importance of coop-erative and helpful behavior of QA systems, whichwas frequently observed in utterances of the WoZswho simulated interactive QA systems.
Designingsuch cooperative functions is indispensable.
Whilethis fact is well known in the context of past researchon dialogue systems, it has been reconfirmed in thecontext of the latest interactive QA technologies.ReferencesJoyce Y. Chai and Rong Jin.
2004.
Discource Struc-ture for Context Question Answering.
Proceedings ofHLT-NAACL2004 Workshop on Pragmatics of Ques-tion Answering, pp.
23-30.John Burger, Claire Cardie, Vinay Chaudhri, et al 2001.Issues, Tasks and Program Structures to Roadmap Re-search in Question & Answering (Q&A)http://www-nlpir.nist.gov/projrcts/duc/roadmpping.html.Norma M. Fraser and G. Nigel Gilbert.
1991.
Simulatingspeech systems.
Computer Speech and Language, Vol5, No.1, pp.
81-99.Andrew Hickl, Johm Lehmann, John Williams, andSanda Harabagiu.
2004.
Experiments with InteractiveQuestion Answering in Complex Scenarios.
Proceed-ings of HLT-NAACL2004 Workshop on Pragmatics ofQuestion Answering, pp.
60-69.Joerrold Kaplan.
1983.
Cooperative Responses from aPortable Natural Language Database Query System.Michael Brady and Robert C. Berwick eds.
Compu-tational Models of Discourse, pp.
167?208, The MITPress.Tsuneaki Kato, Jun?ichi Fukumoto, Fumito Masui andNoriko Kando.
2004a.
Handling Information AccessDialogue through QA Technologies ?
A novel chal-lenge for open-domain question answering ?.
Pro-ceedings of HLT-NAACL2004 Workshop on Pragmat-ics of Question Answering, pp.
70-77.Tsuneaki Kato, Jun?ici Fukumoto and Fumito Masui.2004b.
Question Answering Challenge for Informa-tion Access Dialogue ?
Overview of NTCIR4 QAC2Subtask 3 ?.
Proceedings of NTCIR-4 Workshop Meet-ing.Tsuneaki Kato, Jun?ici Fukumoto and Fumito Masui.2005.
An Overview of NTCIR-5 QAC3.
Proceedingsof Fifth NTCIR Workshop Meeting, pp.
361?372.Tsuneaki Kato, Jun?ici Fukumoto, Fumito Masui andNoriko Kando.
2006.
Are Open-domain QuestionAnswering Technologies Useful for Information Ac-cess Dialogues?
?
An empirical study and a proposalof a novel challenge ?
ACL Trans.
of Asian LanguageInformation Processing, In Printing.Elizabeth D. Liddy.
2002.
Why are People Askingthese Questions?
: A Call for Bringing Situation intoQuestion-Answering System Evaluation.
LREC Work-shop Proceedings on Question Answering ?
Strategyand Resources, pp.
5-8.NTCIR Project Home Page.
2006.http://research.nii.ac.jp/?ntcadm/index-en.htmlMartha E. Pollack.
1989.
Plans as Complex Mental At-titudes.
Philip R. Cohen, Jerry Morgan and Martha E.Pollack eds.
Intentions in Communication, pp.
77?103,The MIT Press.Sharon Small, Nobuyuki Shimizu, Tomek Strzalkowski,and Liu Ting 2003.
HITIQA: A Data Driven Ap-proach to Interactive Question Answering: A Prelim-inary Report AAAI 2003 Spring Symposium New Di-rections in Question Answering, pp.
94-104.Ellen M. Voorhees and Dawn M. Tice.
2000.
Building aQuestion Answering Test Collection the Proceedingsof the 23rd Annual International ACM SIGIR Confer-ence on Research and Development in Information Re-trieval, pp.
200 - 207.Ellen M. Voorhees.
2001.
Overview of the TREC 2001Question Answering Track.
Proceedings of TREC2001.Ellen M. Voorhees.
2004.
Overview of the TREC 2004Question Answering Track.
Proceedings of TREC2004.16
