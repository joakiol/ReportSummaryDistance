Proceedings of the Workshop on Language Technology for Cultural Heritage Data (LaTeCH 2007), pages 41?48,Prague, 28 June 2007. c?2007 Association for Computational LinguisticsCultural Heritage Digital Resources: from Extraction to QueryingMichel Ge?ne?reuxNatural Language Technology GroupUniversity of BrightonUmited KingdomM.Genereux@brighton.ac.ukAbstractThis article presents a method to extract andquery Cultural Heritage (CH) textual dig-ital resources.
The extraction and query-ing phases are linked by a common on-tological representation (CIDOC-CRM).
Atransport format (RDF) allows the ontol-ogy to be queried in a suitable query lan-guage (SPARQL), on top of which an inter-face makes it possible to formulate queriesin Natural Language (NL).
The extractionphase exploits the propositional nature ofthe ontology.
The query interface is basedon the Generate and Select principle, wherepotentially suitable queries are generated tomatch the user input, only for the most se-mantically similar candidate to be selected.In the process we evaluate data extractedfrom the description of a medieval city(Wolfenbu?ttel), transform and develop twomethods of computing similarity betweensentences based on WordNet.
Experimentsare described that compare the pros andcons of the similarity measures and evaluatethem.1 IntroductionThe CIDOC-CRM (DOERR, 2005) ontology is anISO standard created to describe in a formal lan-guage the explicit and implicit concepts and rela-tions underlying the documentation produced in CH.The ontology aims at accommodating a wide varietyof data from the CH domain, but its sheer complex-ity may make it difficult for non-experts to learn itquickly, let alne use it efficiently.
For others, it mayeven be simpler to find a way to translate automati-cally their data from the storage mechanism alreadyin place into CIDOC-CRM.
For practitioners unfa-miliar with strict formalisms, it may be more naturalto describe collections in natural language (e.g.
En-glish), and there is already an unprecedented wealthof information available on-line in natural languagefor almost anything, including CH.
Wouldn?t it bepractical to be able to describe a collection of arti-facts in plain English, with little or no knowledgeof the CIDOC-CRM formalism, and let languagetechnology take over and produce a CIDOC-CRMdatabase?
The principle behind that idea is basedon the observation that the building blocks of theCIDOC-CRM ontology, the triples, have a pred-icative nature, which is structurally consistent withthe way many natural languages are built (DOERR,2005):The domain class is analogous to thegrammatical subject of the phrase forwhich the property is analogous to theverb.
Property names in the CRM are de-signed to be semantically meaningful andgrammatically correct when read from do-main to range.
In addition, the inverseproperty name, normally given in paren-theses, is also designed to be semanti-cally meaningful and grammatically cor-rect when read from range to domain.A triple is defined as:DOMAIN PROPERTY RANGE41The domain is the class (or entity) for which a prop-erty is formally defined.
Subclasses of the domainclass inherit that property.
The range is the classthat comprises all potential values of a property.Through inheritance, subclasses of the range classcan also be values for that property.
Example 1 issomewhat artificial, but it illustrates how triples canbe extracted from natural language, where entitiesE48 and E53 are Place Name and Place respectively,while P1 is the property identify.
(1) RomeDOM E41E48identifiesPROP P1P1the capital of Italy.RANGE E1E53?Rome identifies the capital of Italy.
?The task of the natural language processing tool is tomap relevant parts of texts to entities and propertiesin such a way that triples can be constructed (see also(SHETH, 2003; SCHUTZ, 2005; DAGAN, 2006)).In a nutshell, the Noun Clauses (NC) Rome and thecapital of Italy are mapped to Entity 48 and Entity53, themselves subclasses of the domain E41 andrange E1 respectively, while the Verb Clause (VC)identifies is mapped to Property P1.On the other hand, a natural language interface(ANDROUTSOPOULOS, 1995) to query struc-turally complex and semantically intertwined datasuch as those that can be found in the archaeologicaldomain can lighten a great deal the tasks of browsingand searching.
This state of affairs is even more truefor people not familiar with formal languages, as isoften the case in archaeology in particular and cul-tural heritage in general.
With the Semantic Web1in full development and ontologies such as CIDOC-CRM teaming together to render semantic naviga-tion a realistic prospect, natural language interfacescan offer a welcomed simplified view of the under-lying data.One of the most important and Semantic Weboriented conceptual model available today is theCIDOC-CRM, which is becoming the new standardmodel to document CH data: the creation of toolsready to manage CIDOC-CRM compliant archiveswill be one of the most important goals of the com-ing years (HERMON, 2000).
The full implemen-tation of the CIDOC-CRM model is simplified to-1http://www.w3.org/2001/sw/day by a family of languages developed by theWorld Wide Web Consortium2 and XML-based (LI,2006).
One of its most important representative isRDF3, on top of which sits a query language such asSPARQL4.2 Extraction2.1 MethodologyTRIPLEhhhhhhh??(((((((NCNPRomeVCVVZidentifiesNCaaa!!!NCQQ?
?DTtheNNcapitalPCll,,INofNCNPItalyFigure 1: Linguistic parse tree for example 1.Figure 1 suggests that all pairs of NC separated bya VC (and possibly other elements) are potentiallyvalid CIDOC-CRM triples.
Part-of-speeches (POS)and phrasal clauses can be obtained with a POS tag-ger and chunker5.
To validate the triples, we mustfirst make sure that the predicate is relevant by ex-tracting the main verb of the verbal clause (VC) andsee if its meaning is similar (synonym) to at leastone of the CIDOC-CRM properties.
For example, itis possible to use the verb describe instead of iden-tify.
Once a set of possible properties is identified,we must verify if the noun clauses (NC) surround-ing the property are related to the DOMAIN and theRANGE of that property.
To establish the relation,the first step is to identify the semantics of each NCclause.
For English, a good indicator of the NC se-mantics is the rightmost NN in the clause, excludingany attached PC.
The rightmost NN is usually themost significant: for example, in the NC the mu-seum artifact, the main focus point is artifact, notmuseum.
In figure 1 the rightmost NN of the capital2W3C: http://www.w3.org/3http://www.w3.org/RDF/4http://www.w3.org/TR/2006/CR-rdf-sparql-query-20060406/5http://www.ims.uni-stuttgart.de/projekte/corplex/TreeTagger/42of Italy is capital (excluding the attached PC); thistells us that we are dealing with an object of typecapital.
The second step is to see if the type is asubclass of the DOMAIN or RANGE.
Because en-tity (E1) is a hypernym of capital, then we concludethat the clause the capital of Italy is a subclass ofE1:CRM Entity.
What if the NC has no NN?
Onepossibility6 is that the clause is made up of at leastone proper noun (Rome).
To establish the type ofa proper noun, we use the Web as corpus and com-pute a measure of semantic association (CHURCH,1989) with all CIDOC-CRM classes and choose themost similar as being the type of the NC clause.
Thiswould yield the following triple:E41Rome P1E1the capital of Italywhere E1 and E41 are the entities Appellation andCRM Entity respectively.2.2 Extracting a triple from free textThe following experiment shows the result of ex-tracting a triple from a textual description of the me-dieval city of Wolfenbu?ttel based on the method de-scribed previously.
The document was 3922 wordslong with 173 sentences.
The system extracted 197intermediate triples and 79 final triples.
Table 1shows a few processing steps for the following frag-ment of text:The street?s particular charm lies in itsbroad-faced half-timbered buildings.In step ?, an intermediate triple is extracted fromtexts, then we use synonyms and hypernyms in step?
to find mappings with domains (D), properties (P)and ranges (R) of the ontology.
The final triplesappears in step ?.
For example, consist is a syn-onym for lie, and object is an hypernym of building.In each case, we extracted from WordNet7 (PED-ERSEN, 2004) the synonyms and hypernyms of thethree most common uses for each word (verb, noun).6The other possibility, pronouns, is omitted for simplicity.7http://wordnet.princeton.edu/D [The street?s particular charm]?
P lies inR [its broad-facedhalf-timbered buildings]D [attribute, charm, entity,language, object]?
P [consist]R [activity, building, creation,entity, event, object]D [e13:Attribute Assignment]?
P p9:consists ofR [e7:Activity]Table 1: A triple extracted from free text.3 Querying3.1 NL Interface to SPARQL QueryingOur approach to the problem of mapping a query innatural language to a query expressed in a partic-ular query language (here SPARQL) is to generate(BURKE, 1997) the most likely candidates and se-lect the item which shows maximum semantic sim-ilarity with the input string.
We now explain bothsteps in turn.3.1.1 GenerationWe start from two parallel grammars describingboth the target query language and one or more nat-ural languages.
Here is an excerpt from one querylanguage (SPARQL),SelectQuery ?
Select{Var+Star}DC?
WC SM?DC ?
From TableWC ?
Where?
{ Filter }SM ?
OrderBy ModifierStar ?
?
*?Select ?
?select?From ?
?from?Table ?
?< OneTable >?and part of its equivalent in natural language (hereEnglish):Select ?{?select??show?}?the?
?From ?
?from?43Star ?
{?all records??everything?
}OneTable ?
?clients?Therefore, for a SPARQL query such as select *from <clients> {}, we are able to generate theequivalent in natural language: select all recordsfrom clients.
The generation space of SPARQL andnatural languages can be very large (in fact it can beinfinitely large in both cases), so generation must beconstrained in some way (it is in fact constrained bythe size of the input string).
More specifically, thegrammar generates candidate strings of length to becontained between a fraction f1 shorter and a frac-tion f2 longer than the size (in meaningful words) ofthe input strings.
Meaningful words are limited to beadjectives (tag J), nouns (tag N), verbs (tag V) andadverbs (tag RB), partly because they can be com-pared against each other using WordNet.
The val-ues of f2 is usually less than the value of f1, but theexact values are to be determined empirically.
Theidea behind this is based on the general observationthat queries expressed in natural languages are morelikely to be redundant than underspecified.
Let?slook at example 2, a particular example of a userquery.
(2) Could/MD you/PP show/VVP me/PP all/PDTthe/DT records/NNS you/PP have/VHPplease/VV ./SENTThere are three (show, records and have) meaning-ful words in 2.
Assuming that we have 0.4 and 0.1for the values of f1 and f2 respectively, the gener-ator would then be constrained to produce candi-date strings having a length in the range [3-0.4*3,3+0.1*3] or [1.8, 3.3], i.e.
between two and threewords after rounding.
The generative process mustalso be informed on possible values employed bythe user for the sake of filtering.
For example, inqueries such as Show me everything that has a salaryabove 500 and Select people named Smith, the valueof the fields salary and name are respectively speci-fied as being above 500 and Smith.
These values areused by the generator.
They are assume to be foundas symbols (SYM), foreign words (FW), nouns (N),cardinal numbers (CD) or adjectives (J) in the inputstring.
The whole generative process can be sum-marised as follows:1.
Compute the input query strings length I inmeaningful word tokens and detect potentialfield values (SYM, FW, N, CD or J)2.
Generates candidate strings of a given languageL with length in the range [I-f1*I, I+f2*I]3.
For each candidate string, generate the equiva-lent SPARQL queryThe candidate strings in language L from step 2 arepassed on to the selection process.3.1.2 SelectionThe selection process is based on a measure ofsimilarity between the input string and candidatesissued from generation.
The two similarity mea-sures we are presenting are based on an availablesemantic resource for English, Wordnet.
Both mea-sures assume that two sentences are semanticallysimilar if they share words with similar meanings.This assumption is certainly not true in general but,in the case of database querying, we can assumethat the use of metaphors, irony or contextualisedexpressions is relatively rare.
There are differentapproaches to compute similarity, but we are con-strained by the fact that the system must potentiallyanalyse and compare a large number of sentenceswith varying lengths.
The so-called Levenshtein dis-tance or edit distance is a simple option based ondynamic programming (RISTAD, 1998).
It can betransformed to become a semantic distance, that is,the semantic distance between two strings is givenby the minimum number of operations needed totransform one string into the other, where an oper-ation is an insertion (cost 1), deletion (cost 1), orsubstitution of a single word (as opposed to lettersin the original edit-distance).
The exact cost of sub-stitution is given by how dissimilar a pair of wordsis according to WordNet (from 0 to 2).
Two stringsare therefore similar if they have words semanticallyrelated, with a preference for the same word order.This last requirement is not always acceptable fornatural language, as can be illustrated by examples 3and 4, which are clear semantic equivalent, althougha measure based on the Levenhstein distance wouldbe unduly penalising because of a different word or-der.
(3) Show me the name and salary of all clients.44(4) Look into clients and show me their name andsalary.However, the edit distance is computationally attrac-tive and it is not clear whether word-order is such animportant factor when querying database in naturallanguage.One way to have more control over word-orderis to built a similarity matrix.
A similarity matrixprovides a pairwise measure of similarity betweeneach word of two sentences.
Let?s say we want tocompare the similarity of a user?s sentence 5 with acandidate query 6 generated by system.
(5) Show me salaries for names Smith.
(6) Select the salary where name is Smith.The corresponding similarity matrix is shown as ta-ble 2.
Each word is assigned a part-of-speech andtransformed to its base-form to simplify comparisonusing WordNet.
The similarity values in the table areSimilarity show salary name Smithselect 25 0 0 0salary 0 100 8 6name 0 8 100 17be 33 0 0 0Smith 0 6 17 100Table 2: Similarity matrix between two sentencesin the [0,100] range.
They are computed using sim-ple edge counting in WordNet, a technique similarto computing how two people are genetically relatedthrough their common ancestors (BUDANITSKY,2001).
Only nouns, verbs, adjectives and adverbscan be semantically related by WordNet, thereforestrings are initially stripped of all other grammaticalcategories.
For example, table 2 shows that the wordselect has a degree of similarity of 25 with show.This approach does not take on board word-order atall, and we introduce a slight correction for the valueof each entry in the table: similarity is decreasedwhen words appear in different positions in a string.This is a sensible compromise to consider word-order without undue penalties.
This approach can beexpressed as follows: similarity values are decreasedby a maximum of MaxDecrease only when a pair ofwords are significantly distant (by factor SigDistant)in their respective position within each string.
Thisis expressed by the following formula:IF | l ?
c |L > SigDistant THENSim ?
Sim ?(1?
| l ?
c |L ?MaxDecrease)where l and c are the line and column numbers re-spectively and L is the size of the longest string.
Ifwe set the values of SigDistant and MaxDecreaseto 0.2, then table 2 is transformed to 3.
In table 3,Similarity show salary name Smithselect 25 0 0 0salary 0 100 7 5name 0 7 100 17be 28 0 0 0Smith 0 5 16 100Table 3: Transf.
sim.
matrix between two sentenceswe can see that the similarity between show and behas been reduced from 33 to 28.
Once we have thetransformed similarity matrix, we can compute thesimilarity between the two sentences as such.
Thisis achieved by the following four steps:1.
Generate all possible squared (k*k) sub-matrices from the transformed similarity ma-trix.
There are Ckn = n!k!(n?k)!
such matriceswhere k is the size of the shortest sentence andn the longest2.
Generate all possible word-pairings for eachsub-matrices.
This amounts to selecting ele-ments being on a different row and column.There are k!
such pairings for each Ckn =n!k!(n?k)!
squared sub-matrices3.
Compute the similarity of each k!
word-pairsfor all Ckn sub-matrices by adding their similar-ity value4.
The similarity of the transformed matrix istaken to be the same as the highest among thek!
word-pairs * Ckn sub-matrices, divided (nor-malised) by the size of the longest string nFor our running example in table 3, step 1 yields five4*4 sub-matrices.
For each sub-matrix, there are 2445word-pairings (step 2).
It is easy to see which wordpairing from table 2 gives the highest similarity: (be-show,28), (salary-salary,100), (name-name,100) and(Smith-Smith,100), for a total of 328, normalisedto the length of the longest string (5): 328/5 =66.
For comparison, the semantic similarity dis-tance between the same two sentences using the edit-distance is 250, and this must be normalised to theadded length of the shortest and the longest sen-tence, 250/(5+4) = 28.
Since Levenhstein gives usa distance, we have 1-distance for similarity.
Thenormalising factor is (longest+shortest = 5+4), sincetwo strings completely different would necessitate kreplacements and n-k insertions.
The maximum costis therefore k*2 + (n-k) = k+n.We can get a flavour of the computational com-plexities involved in both measures in terms ofthe number of semantic similarity computations be-tween two words (the most costly computation).
Theration between these numbers for Matrix (n!/(n-k)!
)and Edit (k*n) is (n-1)!/k(n-k)!.
This ratio is equalor greater than 1 in all cases except when n=k=2 andn=k=3, which confirms the expected greater com-plexity of the Matrix method.
For example, whentwo strings of 8 words (n=k=8) are compared, com-plexity is 64 for Edit and 40320 for Matrix.3.2 Comparative EvaluationIn this experiment8 we aim at evaluating and com-paring the two (word-based) measures of semanticsimilarity between sentences previously describedand based on WordNet.
We will refer to these mea-sures as Edit and Matrix.
We need a reference cor-pus where phrases are paired as paraphrases, sowe used the Microsoft Research Paraphrase Corpus(QUIRK, 2004), which is described by the authorsas:.
.
.
a text file containing 5800 pairs of sen-tences which have been extracted fromnews sources on the web, along with hu-man annotations indicating whether eachpair captures a paraphrase/semantic equiv-alence relationship.8Values of parameters for the methods: cost of substitution= 2, word-pairings are centred, contiguous and do not exceed7, MaxDecrease=0.2, SigDistant=0.2, method for similarity =count of edgesOne of two levels of quality is assigned to each para-phrase (0 or 1).
For example, phrases 7 are betterparaphrases (annotated ?1?)
than 8 (annotated ?0?).
(7) Amrozi accused his brother, whom he called ?thewitness?, of deliberately distorting his evidence./Referring to him as only ?the witness?, Amrozi accusedhis brother of deliberately distorting his evidence.
(8) Yucaipa owned Dominick?s before selling the chain toSafeway in 1998 for $2.5 billion./ Yucaipa boughtDominick?s in 1995 for $693 million and sold it toSafeway for $1.8 billion in 1998.We selected random subsets of 100 pairs of goodparaphrases (i.e.
annotated with ?1?
), 100 pairs ofless good paraphrases (annotated with ?0?)
and 100pairs of phrases not paraphrases of each other.
Wecomputed semantic similarity for each subset usingboth methods.
Results are presented in table 4.
Foreach method the minimum and maximum values ofsimilarity are reported.
Variance is relatively lowand both methods appear to correlate.
As expected,paraphrases have higher similarity values, with type?1?
values slightly ahead.
Moreover, average val-ues for paraphrases are significantly higher than fornon-paraphrases, which is a sign that both methodscan discriminate between semantically related sen-tences.
When querying databases, we cannot alwaysCompar.
Min Avg Max Var CorNo(E) 5 12 24 0.2 0.7No(M) 3 14 30 0.4 0.7?0?
(E) 21 57 86 1.2 0.8?0?
(M) 20 54 84 3.3 0.8?1?
(E) 35 69 94 1.9 0.6?1?
(M) 34 61 84 2.4 0.6Table 4: Compar.
eval.
of the (E)dit and (M)atrixmethods for types ?0?, ?1?
and (No) paraphrases.expect a clear front runner, but a continuum of moreor less likely valuable candidates, more in line withthe case of paraphrases ?0?.2-best pairs In this last experiment, 40 sets of 9phrases are submitted to each method for evalua-tion.
Each set includes only one pair of paraphrases:sets 1 to 20 include type ?0?
paraphrases, while sets21 to 40 include type ?1?
paraphrases.
There wasno indication in the corpus that two phrases werenot paraphrase of each other, so we assumed that46phrases not paired as being paraphrases were not.Therefore, our random selection of non-paraphrasescan be more or less dissimilar.
Table 5 show the re-sults, where underlined similarity scores are thoseof the actual paraphrases, and columns BEST andSECOND give the actual measures of similarity forthe best match (the pair the system thinks are para-phrases) and its closest follower respectively.
Wecan see that all 40 paraphrases were selected as thebest by both methods (M and E).
Numbers in boldindicate cases where methods have selected differ-ent second best.
The differences between type ?0?and ?1?
are consistent with those observed in table4.
These are very encouraging results that suggestboth methods could be used in a real system.S Type 0 Type 1 SE Best Second Best Second ET M E M E M E M E T1 43 54 19 20 59 48 26 17 212 39 38 19 14 62 94 24 17 223 40 59 32 21 74 90 16 18 234 46 65 24 20 57 86 39 21 245 51 57 33 31 47 47 25 19 256 39 43 19 15 53 54 15 11 267 54 70 41 39 46 60 16 15 278 50 59 13 9 51 79 12 10 289 72 78 17 20 52 62 21 14 2910 60 67 33 23 56 60 42 29 3011 56 78 17 15 56 52 27 26 3112 36 50 15 14 84 79 21 17 3213 72 80 18 16 48 60 29 27 3314 66 68 29 25 80 79 16 13 3415 39 65 15 12 84 87 34 29 3516 52 58 10 9 52 77 22 14 3617 75 71 23 21 84 82 21 18 3718 48 53 22 19 84 87 21 17 3819 60 60 27 19 69 71 15 13 3920 84 63 18 14 55 80 18 18 40Table 5: Similarity scores for each of the 2 most sim-ilar pairs of phrases as computed by M and E3.3 Conclusions and Future WorkIt is difficult to have a comprehensive evaluation ofthe extraction phase through standard metrics (pre-cision, recall), since there is no benchmark for thistype of analysis.
A good benchmark would be aCIDOC-CRM human-annotated text.
Yet we cangive some evidence of the performance of the sys-tem.
In our experiment, we have collected 79 finaltriples from a 173 sentences long document describ-ing buildings and places of interest in a medievalcity.
The data was relatively clean, although punc-tuation was heavily used throughout the document,confusing the chunker.
Despite modest results, thereis no doubt that a system like this gives a head startto anyone wishing to build a collection using theCIDOC-CRM ontology.
A first pass in the docu-mentation gives a good idea of what the textual doc-umentation is about.
However, a fuller interpretationwill often involve combining many triples togetherto form paths.
Because of time restriction, we havedecided to process the three most common meaningsof each word that we looked up in WordNet (avoid-ing the need to select the correct meaning amongmany); this may have the side effect of lowering ac-curacy.
Speed was not an issue without access to theWeb, not an absolute necessity if we have a goodthesaurus for proper nouns.
Finally, we have tunedthe CRM to analyse impressions of a city, which isnot a domain for which the CRM is optimally in-tended.
We conjecture that texts about museum cat-alogues would have yielded better results.The approach to database querying presented inthis paper demonstrates that more and more seman-tic resources can be used to render natural languageinterfaces more efficient.
The semantic web pro-vides the backbone and the technology to supportcomplex querying of naturally complex data.
Lexi-cal resources such as WordNet makes it possible tocompute semantic similarity between sentences, al-lowing researchers to develop original ways to se-mantic parsing of natural languages.
Our experi-ments show that it is possible to map English queriesto a subset of SPARQL with high level of precisionand recall.
The main drawback of the Edit method isits overemphasis on word-order, making it less suit-able for some languages (e.g.
German).
The Ma-trix method is computationally greedy, and futureresearch must investigate efficient ways of cuttingdown the large search space.
Perhaps step 2 shouldlimit the number of word-pairings by taking only ad-jacent combinations.Another improvement might include less uncon-47ventional methods for generating the sentences suchas FUF/Surge or the realiser of the LKB system, aswell as the use of a corpus more relevant to CH.At this point we concede that the generation spacemay be problematic as input gets longer, but we con-jecture that user?s input should in most cases be ofmanageable length.
Finally, more standards evalu-ation metrics could serve to situate the two similar-ity measures that are being presented with regards tomore standard approaches used for the same purpose(KAUCHAK, 2006).Finally, we have avoided the issue raised by poly-semic words by considering only the most commonsenses found in WordNet, so the approach would bewell complemented by contribution from the field ofWord-Sense Disambiguation (WSD).AcknowledgementThis work has been conducted as part of the EPOCHnetwork of excellence (IST-2002-507382) within theIST (Information Society Technologies) section ofthe Sixth Framework Programme of the EuropeanCommission.
Thank you to the reviewers for usefulcomments.ReferencesANDROUTSOPOULOS I., RITCHIE G., THANISCH P.(1995).
Natural language interfaces to databases - anintroduction.
Journal of Language Engineering, 1(1),29.BUDANITSKY A., HIRST G. (2001).
Semantic dis-tance in wordnet : an experimental, application-oriented evaluation of five measures.
In NAACL 2001Workshop on WordNet and Other Lexical Resources,Pittsburgh.BURKE R.D., HAMMOND K.J., KULYUKIN V., LYTI-NEN S.L., TOMURO N., SCHOENBERG.
S. Ques-tion answering from frequently asked question files -experiences with the faq finder system.
AI Magazine,18(2), 57.CHURCH K.W., HANKS P. (1989) Word associationnorms, mutual information, and lexicography.
InProc.
of the 27th.
Annual Meeting of the ACL Van-vouver, B.V., 1989), pp.
76-83.CRESCIOLI M., D?ANDREA A., NICCOLUCCI F.(2002).
XML Encoding of Archaeological Unstruc-tured Data.
In G. Burenhault (ed.
), ArchaeologicalInformatics : Pushing the envelope.
In Proc.
of the29th CAA Conference, Gotland April 2001, BAR In-ternational Series 1016, Oxford 2002, 267-275.DAGAN I., GLICKMAN O., MAGNINI B.
(2006).
ThePASCAL Recognising Textual Entailment Challenge.Lecture Notes in Computer Science, Volume 3944, Jan2006, Pages 177 - 190.DOERR M. (2005) The CIDOC CRM, an Ontolog-ical Approach to Schema Heterogeneity.
Seman-tic Interoperability and Integration.
Dagstuhl Sem-inar Proceedings, pp.
1862-4405.
InternationalesBegegnungs- und Forschungszentrum fuer Informatik(IBFI), Schloss Dagstuhl, Germany.HERMON S., NICCOLUCCI F. (2000).
The Impact ofWeb-shared Knowledge on Archaeological ScientificResearch.
Proc.
of Intl CRIS 2000 Conf., Helsinki,Finland, 2000.KAUCHAK D., BARZILAY R. (2006).
Paraphrasingfor Automatic Evaluation.
In Proc.
of NAACL/HLT,2006.LI Y., YANG H., JAGADISH H. (2006).
Construct-ing a generic natural language interface for an xmldatabase.
International Conference on ExtendingDatabase TechnologyPEDERSEN T., PATWARDHAN S.,MICHELIZZIJ.(2004).
Wordnet::Similarity - Measuring theRelatedness of Concepts.
In Nineteenth NationalConference on Artificial Intelligence (AAAI-04), SanJose, CA.
(Intelligent Systems Demonstration).QUIRK C., BROCKETT C., DOLAN W.B.
(2004).Monolingual Machine Translation for ParaphraseGeneration.
In Proceedings of the 2004 Conferenceon Empirical Methods in Natural Language Process-ing, Barcelona Spain.RISTAD E.S., YIANILOS P. N. (1998).
Learning string-edit distance.
IEEE Transactions on Pattern Analysisand Machine Intelligence, 20(5), 522.SCHUTZ A., BUITELAR P. (2005).
RelExt: A Toolfor Relation Extraction in Ontology Extension.
In:Proc.
of the 4th International Semantic Web Confer-ence, Galway, Ireland, Nov. 2005.SHETH A.
(2003) Capturing and applying exist-ing knowledge to semantic applications.
InvitedTalk ?Sharing the Knowledge?
- International CIDOCCRM Symposium.
March 2003.
Washington DC.All web references visited on 02-05-2007.48
