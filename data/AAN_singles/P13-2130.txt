Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 742?747,Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational LinguisticsOutsourcing FrameNet to the CrowdMarco Fossati, Claudio Giuliano, and Sara TonelliFondazione Bruno KesslerTrento, Italy{fossati,giuliano,satonelli}@fbk.euAbstractWe present the first attempt to perform fullFrameNet annotation with crowdsourcingtechniques.
We compare two approaches:the first one is the standard annotationmethodology of lexical units and frameelements in two steps, while the secondis a novel approach aimed at acquiringframes in a bottom-up fashion, startingfrom frame element annotation.
We showthat our methodology, relying on a singleannotation step and on simplified role defi-nitions, outperforms the standard one bothin terms of accuracy and time.1 IntroductionAnnotating frame information is a complex task,usually modeled in two steps: first annotators areasked to choose the situation (or frame) evoked bya given predicate (the lexical unit, LU) in a sen-tence, and then they assign the semantic roles (orframe elements, FEs) that describe the participantstypically involved in the chosen frame.
Existingframe annotation tools, such as Salto (Burchardtet al 2006) and the Berkeley system (Fillmore etal., 2002) foresee this two-step approach, in whichannotators first select a frame from a large reposi-tory of possible frames (1,162 frames are currentlylisted in the online version of the resource), andthen assign the FE labels constrained by the cho-sen frame to LU dependents.In this paper, we argue that such workflowshows some redundancy which can be addressedby radically changing the annotation methodologyand performing it in one single step.
Our novel an-notation approach is also more compliant with thedefinition of frames proposed in Fillmore (1976):in his seminal work, Fillmore postulated that themeanings of words can be understood on the basisof a semantic frame, i.e.
a description of a typeof event or entity and the participants in it.
Thisimplies that frames can be distinguished one fromanother on the basis of the participants involved,thus it seems more cognitively plausible to startfrom the FE annotation to identify the frame ex-pressed in a sentence, and not the contrary.The goal of our methodology is to provide fullframe annotation in a single step and in a bottom-up fashion.
Instead of choosing the frame first, wefocus on FEs and let the frame emerge based onthe chosen FEs.
We believe this approach com-plies better with the cognitive activity performedby annotators, while the 2-step methodology ismore artificial and introduces some redundancybecause part of the annotators?
choices are repli-cated in the two steps (i.e.
in order to assign aframe, annotators implicitly identify the partici-pants also in the first step, even if they are anno-tated later).Another issue we investigate in this work is howsemantic roles should be annotated in a crowd-sourcing framework.
This task is particularlycomplex, therefore it is usually performed by ex-pert annotators under the supervision of linguis-tic experts and lexicographers, as in the case ofFrameNet.
In NLP, different annotation effortsfor encoding semantic roles have been carried out,each applying its own methodology and annota-tion guidelines (see for instance Ruppenhofer etal.
(2006) for FrameNet and Palmer et al(2005)for PropBank).
In this work, we present a pilotstudy in which we assess to what extent role de-scriptions meant for ?linguistics experts?
are alsosuitable for annotators from the crowd.
Moreover,we show how a simplified version of these descrip-tions, less bounded to a specific linguistic theory,improve the annotation quality.2 Related workThe construction of annotation datasets for NLPtasks via non-expert contributors has been ap-742proached in different ways, the most prominentbeing games with a purpose (GWAP) and micro-tasks.
Verbosity (Von Ahn et al 2006) was oneof the first attempts in gathering annotations witha GWAP.
Phrase Detectives (Chamberlain et al2008; Chamberlain et al 2009) was meant togather a corpus with coreference resolution an-notations.
Snow et al(2008) described designand evaluation guidelines for five natural languagemicro-tasks.
However, they explicitly chose a setof tasks that could be easily understood by non-expert contributors, thus leaving the recruitmentand training issues open.
Negri et al(2011) builta multilingual textual entailment dataset for statis-tical machine translation systems.The semantic role labeling problem has been re-cently addressed via crowdsourcing by Hong andBaker (2011).
Furthermore, Baker (2012) high-lighted the crucial role of recruiting people fromthe crowd in order to bypass the need for linguis-tics expert annotations.
Nevertheless, Hong andBaker (2011) focused on the frame discriminationtask, namely selecting the correct frame evoked bya given lemma.
Such task is comparable to theword sense disambiguation one as per (Snow etal., 2008), although the complexity increased, dueto lower inter-annotator agreement values.3 ExperimentsIn this section, we describe the anatomy and dis-cuss the results of the tasks we outsourced to thecrowd via the CrowdFlower1 platform.Golden data Quality control of the collectedjudgements is a key factor for the success ofthe experiments.
Cheating risk is minimized byadding gold units, namely data for which the re-quester already knows the answer.
If a workermisses too many gold answers within a giventhreshold, he or she will be flagged as untrustedand his or her judgments will be automatically dis-carded.Worker switching effect Depending on theiraccuracy in providing answers to gold units, work-ers may switch from a trusted to an untrusted sta-tus and vice versa.
In practice, a worker submitshis or her responses via a web page.
Each pagecontains one gold unit and a variable number ofregular units that can be set by the requester dur-ing the calibration phase.
If a worker becomes un-1https://crowdflower.comtrusted, the platform collects another judgment tofill the gap.
If a worker moves back to the trustedstatus, his or her previous contribution is addedto the results as free extra judgments.
Such phe-nomenon typically occurs when the complexity ofgold units is high enough to induce low agree-ment in workers?
answers.
Thus, the requester isconstrained to review gold units and to eventuallyforgive workers who missed them.
This has mas-sively happened in our experiments and is one ofthe main causes of the overall cost decrease andtime increase.Cost calibration The total cost of a genericcrowdsourcing task is naturally bound to a dataunit.
This represents an issue in most of our ex-periments, as the number of questions per unit(i.e.
a sentence) varies according to the numberof frames and FEs evoked by the LU contained ina sentence.
In order to enable cost comparison, foreach experiment we need to use the average num-ber of questions per sentence as a multiplier to aconstant cost per sentence.
We set the paymentper working page to 5 $ cents and the number ofsentences per page to 3, resulting in 1.83 $ centper sentence.3.1 Assessing task reproducibility andworker behavior changeSince our overall goal is to compare the perfor-mance of FrameNet annotation using our novelworkflow to the performance of the standard, 2-step approach, we first take into account past re-lated works and try to reproduce them.To our knowledge, the only attempt to annotateframe information through crowdsourcing is theone presented in Hong and Baker (2011), whichhowever did not include FE annotation.Modeling The task is designed as follows.
(a)Workers are invited to read a sentence where aLU is bolded.
(b) The question Which is thecorrect sense?
is combined with the set offrames evoked by the given LU, as well as theNone choice.
Finally, (c) workers must select thecorrect frame.
A set of example sentences corre-sponding to each possible frame is provided in theinstructions to facilitate workers.As a preliminary study, we wanted to assessto what extent the proposed task could be repro-duced and if workers reacted in a comparable wayover time.
Hong and Baker (2011) did not pub-lish the input datasets, thus we ignore which sen-743LU2013 2011Sentences Accuracy Accuracy(Gold)high.a 68 (9) 91.8 92history.n 72 (9) 84.6 86range.n 65 (8) 95 93rip.v 88 (12) 81.9 92thirst.n 29 (4) 90.4 95top.a 36 (5) 98.7 96Table 1: Comparison of the reproduced frame dis-crimination task as per (Hong and Baker, 2011)tences were used.
Besides, the authors computedaccuracy values directly from the results upon amajority vote ground truth.
Therefore, we de-cided to consider the same LUs used in Hongand Baker?s experiments, i.e.
high.a, history.n,range.n, rip.v, thirst.n and top.a, but we lever-aged the complete sets of FrameNet 1.5 expert-annotated sentences as gold-standard data for im-mediate accuracy computation.Discussion Table 1 displays the results weachieved, jointly with the experiments by Hongand Baker (2011).
For the latter, we only show ac-curacy values, as the number of sentences was setto a constant value of 18, 2 of which were gold.If we assume that the crowd-based ground truth in2011 experiments is approximately equivalent tothe expert one, workers seem to have reacted ina similar manner compared to Hong and Baker?svalues, except for rip.v.3.2 General task settingWe randomly chose the following LUs amongthe set of all verbal LUs in FrameNet evoking 2frames each: disappear.v [CEASING TO BE, DE-PARTING], guide.v [COTHEME, INFLUENCE OF -EVENT ON COGNIZER], heap.v [FILLING, PLAC-ING], throw.v [BODY MOVEMENT, CAUSE MO-TION].
We considered verbal LUs as they usuallyhave more overt arguments in a sentence, so thatwe were sure to provide workers with enough can-didate FEs to annotate.
Linguistic tasks in crowd-sourcing frameworks are usually decomposed tomake them accessible to the crowd.
Hence, weset the polysemy of LUs to 2 to ensure that allexperiments are executed using the smallest-scalesubtask.
More frames can then be handled by justreplicating the experiments.3.3 2-step approachAfter observing that we were able to achieve sim-ilar results on the frame discrimination task as inprevious work, we focused on the comparison be-tween the 2-step and the 1-step frame annotationapproaches.We first set up experiments that emulate the for-mer approach both in frame discrimination andFEs annotation.
This will serve as the baselineagainst our methodology.
Given the pipeline na-ture of the approach, errors in the frame discrim-ination step will affect FE recognition, thus im-pacting on the final accuracy.
The magnitude ofsuch effect strictly depends on the number of FEsassociated with the wrongly detected frame.3.3.1 Frame discriminationFrame discrimination is the first phase of the 2-step annotation procedure.
Hence, we need toleverage its output as the input for the next step.Modeling The task is modeled as per Sec-tion 3.1.Discussion Table 2 gives an insight into the re-sults, which confirm the overall good accuracy asper the experiments discussed in Section 3.1.3.3.2 Frame elements recognitionWe consider all sentences annotated in the previ-ous subtask with the frame assigned by the work-ers, even if it is not correct.Modeling The task is presented as follows.
(a)Workers are invited to read a sentence where a LUis bolded and the frame that was identified in thefirst step is provided as a title.
(b) A list of FE def-initions is then shown together with the FEs textchunks.
Finally, (c) workers must match each def-inition with the proper FE.Simplification Since FEs annotation is a verychallenging task, and FE definitions are usuallymeant for experts in linguistics, we experimentedwith three different types of FE definitions: theoriginal ones from FrameNet, a manually simpli-fied version, and an automatically simplified one,using the tool by Heilman and Smith (2010).
Thelatter simplifies complex sentences at the syntacticlevel and generates a question for each of the ex-tracted clauses.
As an example, we report belowthree versions obtained for the Agent definition inthe DAMAGING frame:744Approach 2-STEP 1-STEPTask FD FERAccuracy .900 .687 .792Answers 100 160 416Trusted 100 100 84Untrusted 21 36 217Time (h) 102 69 130Cost/question 1.83 2.74 8.41($ cents)Table 2: Overview of the experimental results.FD stands for Frame Discrimination, FER for FEsRecognitionOriginal: The conscious entity, generally a per-son, that performs the intentional action that re-sults in the damage to the Patient.Manually simplified: This element describes theperson that performs the intentional action result-ing in the damage to another person or object.Automatic system: What that performs the in-tentional action that results in the damage to thePatient?Simplification was performed by a linguistic ex-pert, and followed a set of straightforward guide-lines, which can be summarized as follows:?
When the semantic type associated with theFE is a common concept (e.g.
Location),replace the FE name with the semantic type.?
Make syntactically complex definitions assimple as possible.?
Avoid variability in FE definitions, try tomake them homogeneous (e.g.
they shouldall start with ?This element describes...?
orsimilar).?
Replace technical concepts such asArtifact or Sentient with com-mon words such as Object and Personrespectively.Although these changes (especially the lastitem) may make FE definitions less precise froma lexicographic point of view (for instance, sen-tient entities are not necessarily persons), annota-tion became more intuitive and had a positive im-pact on the overall quality.After few pilot annotations with the three typesof FE definitions, we noticed that the simplifiedone achieved a better accuracy and a lower num-ber of untrusted annotators compared to the oth-ers.
Therefore, we use the simplified definitionsin both the 2-step and the 1-step approach (Sec-tion 3.4).Discussion Table 2 provides an overview of theresults we gathered.
The total number of answersdiffers from the total number of trusted judgments,since the average value of questions per sentenceamounts to 1.5.2 First of all, we notice an increasein the number of untrusted judgments.
This iscaused by a generally low inter-worker agreementon gold sentences due to FE definitions, which stillpresent a certain degree of complexity, even af-ter simplification.
We inspected the full reportssentence by sentence and observed a propagationof incorrect judgments when a sentence involvesan unclear FE definition.
As FE definitions maymutually include mentions of other FEs from thesame frame, we believe this circularity generatedconfusion.3.4 1-step approachHaving set the LU polysemy to 2, in our case asentence S always contains a LU with 2 possibleframes (f1, f2), but only conveys one, e.g.
f1.
Weformulate the approach as follows.
S is replicatedin 2 data units (Sa, Sb).
Then, Sa is associated tothe set E1 of f1 FE definitions, namely the correctones for that sentence.
Instead, Sb is associated tothe set E2 of f2 FE definitions.
We call Sb a cross-frame unit.
Furthermore, we allow workers to se-lect the None answer.
In practice, we ask a totalamount of |E1 ?
E2| + 2 questions per sentenceS.
In this way, we let the frame directly emergefrom the FEs.
If workers correctly answer Noneto a FE definition d ?
E2, the probability that Sevokes f1 increases.Modeling Figure 1 displays a screenshot ofthe worker interface.
The task is designed as perSection 3.3.2, but with major differences withrespect to its content.
This is better describedby an example.
The sentence Karen threwher arms round my neck, spillingchampagne everywhere contains the LUthrow.v evoking the frame BODY MOVEMENT.However, throw.v is ambiguous and may alsoevoke CAUSE MOTION.
We ask to annotate boththe BODY MOVEMENT and the CAUSE MOTION2Cf.
Section 3 for more details745Figure 1: 1-step approach worker interfacecore FEs, respectively as regular and cross-frameunits.Discussion We do not interpret the None choiceas an abstention from judgment, since it is a cor-rect answer for cross-frame units.
Instead of pre-cision and recall, we are thus able to directly com-pute workers?
accuracy upon a majority vote.
Weenvision an improvement with respect to the 2-step methodology, as we avoid the proven risk oferror propagation originating from wrongly anno-tated frames in the first step.
Table 2 illustratesthe results we collected.
As expected, accuracyreached a consistent enhancement.
This demon-strates the hypothesis we stated in Section 1 onthe cognitive plausibility of a bottom-up approachfor frame annotation.
Furthermore, the execu-tion time decreases compared to the sum of the2 steps, namely 130 hours against 171.
Neverthe-less, the cost is sensibly higher due to the highernumber of questions that need to be addressed, inaverage 4.6 against 1.5.
Untrusted judgments se-riously grow, mainly because of the cross-framegold complexity.
Workers seem puzzled by thepresence of None, which is a required answer forsuch units.
If we consider the English FrameNetannotation agreement values between experts re-ported by Pado?
and Lapata (2009) as the upperbound (i.e., .897 for frame discrimination and .949for FEs recognition), we believe our experimentalsetting can be reused as a valid alternative.4 ConclusionIn this work, we presented an approach to performframe annotation with crowdsourcing techniques,based on a single annotation step and on manu-ally simplified FE definitions.
Since the resultsseem promising, we are currently running largerscale experiments with the full set of FrameNet 1.5annotated sentences.
Input data, interface screen-shots and full results are available and regularlyupdated at http://db.tt/gu2Mj98i.Future work will include the investigation of aframe assignment strategy.
In fact, we do not takeinto account the case of conflicting FE annotationsin cross-frame units.
Hence, we need a confidencescore to determine which frame emerges if work-ers selected contradictory answers in a subset ofcross-frame FE definitions.AcknowledgementsThe research leading to this paper was partiallysupported by the European Union?s 7th Frame-work Programme via the NewsReader Project(ICT-316404).746References[Baker2012] Collin F Baker.
2012.
Framenet, cur-rent collaborations and future goals.
Language Re-sources and Evaluation, pages 1?18.
[Burchardt et al006] Aljoscha Burchardt, Katrin Erk,Anette Frank, Andrea Kowalski, Sebastian Pado,and Manfred Pinkal.
2006.
Salto?a versatile multi-level annotation tool.
In Proceedings of LREC 2006,pages 517?520.
Citeseer.
[Chamberlain et al008] Jon Chamberlain, MassimoPoesio, and Udo Kruschwitz.
2008.
Phrase detec-tives: A web-based collaborative annotation game.Proceedings of I-Semantics, Graz.
[Chamberlain et al009] Jon Chamberlain, Udo Kr-uschwitz, and Massimo Poesio.
2009.
Constructingan anaphorically annotated corpus with non-experts:Assessing the quality of collaborative annotations.In Proceedings of the 2009 Workshop on The Peo-ple?s Web Meets NLP: Collaboratively ConstructedSemantic Resources, pages 57?62.
Association forComputational Linguistics.
[Fillmore et al002] Charles J. Fillmore, Collin F.Baker, and Hiroaki Sato.
2002.
The FrameNetDatabase and Software Tools.
In Proceedings ofthe Third International Conference on Language Re-sources and Evaluation (LREC 2002), pages 1157?1160, Las Palmas, Spain.
[Fillmore1976] Charles J. Fillmore.
1976.
Frame Se-mantics and the nature of language.
In Annals of theNew York Academy of Sciences: Conference on theOrigin and Development of Language, pages 20?32.Blackwell Publishing.
[Heilman and Smith2010] Michael Heilman andNoah A. Smith.
2010.
Extracting SimplifiedStatements for Factual Question Generation.
InProceedings of QG2010: The Third Workshop onQuestion Generation, Pittsburgh, PA, USA.
[Hong and Baker2011] Jisup Hong and Collin F Baker.2011.
How good is the crowd at ?real?
wsd?
ACLHLT 2011, page 30.
[Negri et al011] Matteo Negri, Luisa Bentivogli,Yashar Mehdad, Danilo Giampiccolo, and Alessan-dro Marchetti.
2011.
Divide and conquer: crowd-sourcing the creation of cross-lingual textual entail-ment corpora.
In Proceedings of the Conference onEmpirical Methods in Natural Language Process-ing, EMNLP ?11, pages 670?679, Stroudsburg, PA,USA.
Association for Computational Linguistics.[Pado?
and Lapata2009] Sebastian Pado?
and Mirella La-pata.
2009.
Cross-lingual annotation projection forsemantic roles.
Journal of Artificial Intelligence Re-search, 36(1):307?340.
[Palmer et al005] Martha Palmer, Dan Gildea, andPaul Kingsbury.
2005.
The Proposition Bank: ACorpus Annotated with Semantic Roles.
Computa-tional Linguistics, 31(1).
[Ruppenhofer et al006] Josef Ruppenhofer, MichaelEllsworth, Miriam R.L.
Petruck, Christopher R.Johnson, and Jan Scheffczyk.
2006.
FrameNetII: Extended Theory and Practice.
Available athttp://framenet.icsi.berkeley.edu/book/book.html.
[Snow et al008] Rion Snow, Brendan O?Connor,Daniel Jurafsky, and Andrew Y Ng.
2008.
Cheapand fast?but is it good?
: evaluating non-expert an-notations for natural language tasks.
In Proceedingsof the Conference on Empirical Methods in NaturalLanguage Processing, pages 254?263.
Associationfor Computational Linguistics.
[Von Ahn et al006] Luis Von Ahn, Mihir Kedia, andManuel Blum.
2006.
Verbosity: a game for col-lecting common-sense facts.
In Proceedings of theSIGCHI conference on Human Factors in computingsystems, pages 75?78.
ACM.747
