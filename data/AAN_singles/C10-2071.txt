Coling 2010: Poster Volume, pages 623?629,Beijing, August 2010A Post-processing Approach to Statistical Word AlignmentReflecting Alignment Tendency between Part-of-speechesJae-Hee Lee1, Seung-Wook Lee1, Gumwon Hong1,Young-Sook Hwang2, Sang-Bum Kim2,  Hae-Chang Rim11Dept.
of Computer and Radio Communications Engineering, Korea University2Institute of Future Technology, SK Telecom1{jlee,swlee,gwhong,rim}@nlp.korea.ac.kr,2{yshwang,sangbum.kim}@sktelecom.comAbstractStatistical word alignment often suffersfrom data sparseness.
Part-of-speechesare often incorporated in NLP tasks toreduce data sparseness.
In this paper,we attempt to mitigate such problem byreflecting alignment tendency betweenpart-of-speeches to statistical wordalignment.
Because our approach doesnot rely on any language-dependentknowledge, it is very simple and purelystatistic to be applied to any languagepairs.
End-to-end evaluation shows thatthe proposed method can improve notonly the quality of statistical wordalignment but the performance of sta-tistical machine translation.1 IntroductionWord alignment is defined as mapping corre-sponding words in parallel text.
A wordaligned parallel corpora are very valuable re-sources in NLP.
They can be used in variousapplications such as word sense disambigua-tion, automatic construction of bilingual lexi-con, and statistical machine translation (SMT).In particular, the initial quality of statisticalword alignment dominates the quality of SMT(Och and Ney 2000; Ganchev et al, 2008);almost all current SMT systems basically referto the information inferred from word align-ment result.One of the widely used approaches to statis-tical word alignment is based on the IBMmodels (Brown et al, 1993).
IBM models areconstructed based on words?
co-occurrenceand positional information.
If sufficient train-ing data are given, IBM models can be suc-cessfully applied to any language pairs.
How-ever, for minority language pairs such as Eng-lish-Korean and Swedish-Japanese, it is verydifficult to obtain large amounts of parallelcorpora.
Without sufficient amount of parallelcorpus, it is very difficult to learn the correctcorrespondences between words that infre-quently occur in the training data.Part-of-speeches (POS), which representmorphological classes of words, can give valu-able information about individual words andtheir neighbors.
Identifying whether a word isa noun or a verb can let us predict which wordsare likely to be mapped in word alignment andwhich words are likely to occur in its vicinityin target sentence generation.Many studies incorporate POS informationin SMT.
Some researchers perform POS tag-ging on their bilingual training data (Lee et al,2006; Sanchis and S?nchez, 2008).
Some ofthem replace individual words as new words,such as in ?word/POS?
form, producing new,extended vocabulary.
The advantage of thisapproach is that POS information can help toresolve lexical ambiguity and thus improvetranslation quality.On the other hand, Koehn et al (2007) pro-pose a factored translation model that can in-corporate any linguistic factors including POSinformation in phrase-based SMT.
The modelprovides a generalized representation of atranslation model, because it can map multiplesource and target factors.Although all of these approaches are shownto improve SMT performance by utilizing POSinformation, we observe that the influence isvirtually marginal in two ways:6231) The POS information tagged to each wordmay help to disambiguate in selectingword correspondences, but the increasedvocabulary can also make the training datamore sparse.2) The factored translation model may help toeffectively handle out-of-vocabulary(OOV) by incorporating many linguisticfactors, but it still crucially relies on the in-itial quality of word alignment that willdominate the translation probabilities.This paper focuses on devising a bettermethod for incorporating POS information inword alignment.
It attempts to answer the fol-lowing questions:1) Can the information regarding POS align-ment tendency affect the post-processingof word alignment?2) Can the result of word alignment affectedby such information help improving thequality of SMT?2 POS Alignment TendencyDespite the language pairs, words with similarPOSs often correspond to each other in statisti-cal word alignment.
Similarly, words with dif-ferent POSs are seldom aligned.
For example,Korean proper nouns very often align withEnglish proper nouns very often but seldomalign with English adverbs.
We believe thatthis phenomenon occurs not only on English-Korean pairs but also on most of other lan-guage pairs.Thus, in this study we hypothesize that allsource language (SL) POSs have some rela-tionship with target language (TL) POSs.
Fig-ure 1 exemplifies some results of using theIBM Models in English-Korean word align-ment.
As can be seen in the figure, the Englishword ?visiting?
is incorrectly and excessivelyaligned to four Korean morphemes ?maejang?,?chat?, ?yeoseong?, and ?gogaek?.
One reasonfor this is the sparseness of the training data;the only correct Korean morpheme ?chat?
doesnot sufficiently co-occur with ?visiting?
in thetraining data.
However, it is generally believedthat an English verb is more likely aligned to aKorean verb rather than a Korean noun.
Like-wise, we suppose that among many POSs,there are strong relationships between similarPOSs and relatively weak relationships be-tween different POSs.
We hypothesize that thediscovery of such relationships in advance canlead to better word alignment results.In this paper, we propose a new method toobtain the relationship from word alignmentresults.
The relationships among POSs, hence-forth the POS alignment tendency, can beidentified by the probability of the given POSpairs?
alignment result where the source lan-guage POS and the target language POS co-occur in bilingual sentences.
We formulate thisidea using the maximum likelihood estimationas follows:(          |   ( )    ( ))(              ( )    ( ))?
(           ( )    ( ))  *          +where f and e denote source word and targetword respectively.
count() is a function thatreturns the number of co-occurrence of f and ewhen they are aligned (or not aligned).
Then,we adjust the formula with the existing align-ment score between f and e.(   )        (   )(   ) (          |   ( )    ( ))where )|( efPIBM  indicates the alignment prob-ability estimated by the IBM models.
is aweighting parameter to interpolate the reliabili-ties of both alignment factors.
In the expe-Figure 1.
An example of inaccurate word alignment624riment,   is empirically set to improve theword alignment performance ( =0.5).3 Modifying AlignmentBased on the new scoring scheme as intro-duced in the previous section, we modify theresult of the initial word alignment.
The modi-fication is performed in the following proce-dure:1.
For each source word f that has out-boundalignment link other than null,2.
Find the target word e that has the maxi-mum alignment score according to theproposed alignment adjustment measure,and change the alignment result by map-ping f to e.This modification guarantees that the numberof alignment does not change; the algorithm isdesigned to minimize the risk by maintainingthe fertility of a word estimated by the IBMModel.
Figure 2 illustrates the result beforeand after the alignment modification.
Incor-rectly links from e1 and e3 are deleted andmissing links from e2 and e4 are generated dur-ing this alignment modification.The alignment modification through the re-flection of POS alignment tendency is per-formed on both e-to-f and f-to-e bidirectionalword alignments.
The bidirectional wordalignment results are then symmetrized.4 ExperimentsIn this paper, we attempt to reflect the POSalignment tendency in improving the wordalignment performance.
This section providesthe experimental setup and the results thatdemonstrate whether the proposed approachcan improve the statistical word alignment per-formance.We collected bilingual texts from major bi-lingual news broadcasting sites.
500K sentencepairs are collected and refined manually toconstruct correct parallel sentences pairs.
Thesame number of monolingual sentences is alsoused from the same sites to train Korean lan-guage.
We also prepared a subset of the bilin-gual text with the size of 50K to show that theproposed model is very effective when thetraining set is small.In order to evaluate the performance ofword alignment, we additionally constructed areference set with 400 sentence pairs.
Theevaluation is performed using precision, recall,and F-score.
We use the GIZA++ toolkit forword alignment as well as four heuristic sym-metrizations: intersection, union, grow-diag-final, and grow-diag (Och, 2000).4.1 Word AlignmentWe now evaluate the effectiveness of the pro-posed word alignment method.
Table 1 and 2report the experimental results by adding POSinformation to the parallel corpus.
?Lexical?denotes the result of conventional word align-ment produced by GIZA++.
No pre-processingor post-processing is applied in this result.?Lemma/POS?
is the result of word alignmentwith the pre-processing introduced Lee et al(2006).
Compared to the result, lemmatizedlexical and POS tags are proven to be usefulinformation for word alignment.
?Lemma/POS?consistently outperforms ?Lexical?
despite thesymmetrization heuristics in terms of precision,recall and F-score.
We expect this improve-ment is benefited from the alleviated datasparseness by using lemmatized lexical andPOS tags rather than using the lexical itself.Figure 2.
An example of word alignment modification625Alignment heuristic Precision Recall F-scoreLexicalIntersection 94.0% 50.8% 66.0%Union 53.2% 81.2% 64.3%Grow-diag-final 54.6% 80.9% 65.2%Grow-diag 60.9% 67.2% 63.9%Lemma/POSIntersection 95.8% 55.3% 70.1%Union 58.1% 83.3% 68.4%Grow-diag-final 59.7% 83.0% 69.5%Grow-diag 67.0% 71.6% 69.2%Lemma/POS+ POS alignmenttendencyIntersection 96.1% 63.5% 76.5%Union 67.4% 85.1% 75.2%Grow-diag-final 69.8% 84.9% 76.6%Grow-diag 80.0% 77.0% 78.5%Table 1.
The performance of word alignment using small training set (50k pairs)Experimental Setup Alignment heuristic Precision Recall F-scoreLexicalIntersection 96.8% 64.9% 77.7%Union 66.6% 87.4% 75.6%Grow-diag-final 67.8% 87.1% 76.2%Grow-diag 74.4% 79.2% 76.7%Lemma/POSIntersection 97.3% 66.2% 78.8%Union 70.7% 89.0% 78.8%Grow-diag-final 72.1% 88.8% 79.6%Grow-diag 78.8% 80.5% 79.7%Lemma/POS+ POS alignmenttendencyIntersection 97.2% 69.3% 80.9%Union 73.9% 86.7% 79.8%Grow-diag-final 75.6% 86.4% 80.7%Grow-diag 85.2% 81.5% 83.4%Table 2.
The performance of word alignment using a large training set (500k pairs)Experimental Setup Symmetrization Heuristic BLEU(50k) BLEU (500k)LexicalIntersection 20.1% 29.2%Union 18.6% 27.2%Grow-diag-final 19.9% 27.7%Grow-diag 20.2% 29.4%Lemma/POSIntersection 20.3% 26.4%Union 18.5% 27.8%Grow-diag-final 20.1% 29.2%Grow-diag 20.4% 30.8%Factored Model(Lemma, POS)Intersection 20.5% 30.0%Union 18.1% 27.5%Grow-diag-final 20.3% 28.2%Grow-diag 20.9% 31.1%Lemma/POS+ POS alignmenttendencyIntersection 21.8% 29.3%Union 19.5% 27.2%Grow-diag-final 21.3% 28.4%Grow-diag 20.8% 29.1%Table  3.
The performance of translation626Since lemmatized lexical and POS tags areshown to be useful, our post-processing meth-od is applied to ?Lemma/POS?.The experimental results show that the pro-posed method consistently improves wordalignment in terms of F-score.
It is interestingthat the proposed method improves the recallof the intersection result and the precision ofthe union result.
Thus, the proposed methodachieves the best alignment performance.As can be seen in Table 1 and 2, our methodconsistently improves the performance of wordalignment despite the size of training data.
In asmall data set, the improvement of our methodis much higher than that in a large set.
Thisimplies that our method is more helpful whenthe training data set is insufficient.We investigate whether the proposed meth-od actually alleviates the data sparseness prob-lem by analyzing the aligned word pairs of lowco-occurrence frequency.
There are multipleword pairs that share the same number of co-occurrence in the corpus.
For example, let usassume that ?report-bogoha?, ?newspaper-sinmun?
and ?China-jungguk?
pairs are co-occurred 1,000 times.
We can calculate themean of their individual recalls.
We refer tothis new measurement as average recall.
Theaverage recalls of these pairs are relativelyhigher than those of pairs with low co-occurrence frequency such as ?food-jinji?
and?center-chojeom?
pairs.
These pairs are diffi-cult to be linked, because the word alignmentmodel suffers from data sparseness when esti-mating their translation probability.Figure 3 shows the average recall accordingto the number of co-occurrence.
We can ob-serve that the word alignment model tends tolink word pairs more correctly if they are morefrequently co-occurred.
Both ?Lemma/POS?and our method consistently show higher aver-age recall throughout all frequencies, and theproposed method shows the best performance.It is also notable that the both ?Lemma/POS?and our method achieve much more improve-ment for low co-occurrence frequencies (e.g.,11~40).
This implies that the proposed methodincorporates POS information more effectivelythan the previous method, since the proposedmethod achieves much higher average recall.4.2 Statistical Machine TranslationNext, we examine the effect of the improve-ment of the word alignment on the translationquality.
For this, we built some SMT systemswith the word alignment results.
We use theMoses toolkit for translation (Koehn et al,2007).
Moses is an implementation of phrase-based statistical machine translation model thathas shown a state-of-the-art performance invarious evaluation sets.
We also perform theevaluation of the Factored model (Koehn et al,2007) using Moses.To investigate how the improved wordalignment affect the quality of machine trans-lation, we calculate the BLEU score for trans-lation results with different word alignmentsettings as shown in Table 3.
First of all, wecan easily conclude that the quality of thetranslation is strongly dominated by the size ofthe training data.
We can also find that thequality of the translation is correlated to theperformance of the word alignment.For a small test set, the proposed methodachieved the best performance in terms ofBLEU (21.8%).
For a larger test set, however,the proposed method could not improve theperformance of the translation with better wordalignment.
It is not feasible to investigate thefactors that affect this deterioration, since Mo-ses is a black box module to our system.
Thetraining of the phrase-based SMT model in-volves the extraction of phrases, and the resultof word alignment is reflected within this pro-cess.
When the training data is small, the num-ber of extracted phrases is also apparentlysmall.
However, abundant phrases are extract-ed from a large amount of training data.
In thiscase, we hypothesize that the most plausibleFigure 3.
Average recall of word alignment pairsaccording to the number of their co-occurrence627phrases are already obtained, and the effect ofmore accurate word alignment seems insignifi-cant.
More thorough analysis of this is re-mained as future work.4.3 Acquisition of Bilingual DictionaryOne of the most applications of word align-ment is the construction of bilingual dictionar-ies.
By using word alignment, we can collect a(ranked) list of bilingual word pairs.
Table 4reports the top 10 translations (the most ac-ceptable target words to align) for Koreanword ?bap?
(food).
The table contains theprobabilities estimated by the IBM Models, theadjusted scores, and the number of co-occurrence, respectively.
Italicized translationsare in fact incorrect translations.
Highlightedones are new translation candidates that arecorrect.
As can be seen in the table, the pro-posed approach shows a positive effect of rais-ing new and better candidates for translation.For example, ?bread?
and ?breakfast?
havecome up to the top 10 translations.
Thisdemonstrates that the low co-occurrences of?bap?
with ?bread?
and ?breakfast?
are notsuitably handled by alignments solely based onlexicals.
However, the proposed approachranks them at higher positions by reflecting thealignment tendency of POSs.5 ConclusionIn this paper, we propose a new method forincorporating the POS alignment tendency toimprove traditional word alignment model inpost processing step.
Experimental resultsshow that the proposed method helps to allevi-ate the data sparseness problem especiallywhen the training data is insufficient.It is still difficult to conclude that betterword alignment always leads to better transla-tion.
We plan on investigating the effective-ness of the proposed method using other trans-lation system, such as Hiero (Chiang et al,2005).
We also plan to incorporate our methodinto other effective models, such as Factoredtranslation model.ReferencesDavid Chiang et al, 2005.
The Hiero machinetranslation system: Extensions, evaluation, andanalysis.
In Proc.
of HLT-EMLP:779?786, Oct.Franz Josef Och.
2000.
Giza++: Training of statis-tical translation models.
Available at http://www-i6.informatik.rwthaachen.de/?och/software/GIZA++.html.Franz Josef Och & Hermann Ney.
2003.
A System-atic Comparison of Various Statistical AlignmentModels.
Computational Linguistics 29 (1):19-51.G.
Sanchis and J.A.
S?nchez.
Vocabulary exten-sion via POS information for SMT.
In MixingApproaches to Machine Translation, 2008.Jonghoon Lee, Donghyeon Lee and Gary GeunbaeLee.
Improving Phrase-based Korean-English Sta-tistical Machine Translation.
INTERSPEECH 2006.Kuzman Ganchev, Joao V. Graca and Ben Taskar.2008.
Better Alignments = Better Translations?Proceedings of ACL-08: HLT: 986?993.Peter F. Brown et al,1993.
The Mathematics ofStatistical Machine Translation: Parameter Esti-mation.
Computational Linguistics 9(2): 263-311RankIBM Model POS Alignment Tendencytranslation     (   ) #co-occur translation      (   ) #co-occur12345678910bob/NNPrice/NNeat/VBmeal/NNfood/NNbob/NNfeed/VBcook/VBliving/NNdinner/NN0.3480.1920.1070.0750.0430.0380.0100.0100.0080.00883735743291079410bob/NNPrice/NNmeal/NNfood/NNeat/VBbob/NNliving/NNdinner/NNbread/NNbreakfast/NN0.2140.1360.0780.0620.0610.0590.0450.0440.0440.04383734329571041096Table 4.
Top 10 translations for Korean word ?bap?
(food).628Philipp Koehn and Hieu Hoang.
Factored Transla-tion Models.
EMNLP 2007.Phillipp Koehn et al, 2007.
Moses: Open sourcetoolkit for statistical machine translation.In Pro-ceedings of the Annual Meeting of the Associationfor Computational Linguistics, demonstation ses-sion.629
