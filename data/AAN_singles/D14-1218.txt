Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 2039?2048,October 25-29, 2014, Doha, Qatar.c?2014 Association for Computational LinguisticsA Model of Coherence Based on Distributed Sentence RepresentationJiwei Li1and Eduard Hovy31Computer Science Department, Stanford University, Stanford, CA 94305, USA3Language Technology Institute, Carnegie Mellon University, Pittsburgh, PA 15213, USAjiweil@stanford.edu ehovy@andrew.cmu.eduAbstractCoherence is what makes a multi-sentencetext meaningful, both logically and syn-tactically.
To solve the challenge of or-dering a set of sentences into coherent or-der, existing approaches focus mostly ondefining and using sophisticated featuresto capture the cross-sentence argumenta-tion logic and syntactic relationships.
Butboth argumentation semantics and cross-sentence syntax (such as coreference andtense rules) are very hard to formalize.
Inthis paper, we introduce a neural networkmodel for the coherence task based ondistributed sentence representation.
Theproposed approach learns a syntactico-semantic representation for sentences au-tomatically, using either recurrent or re-cursive neural networks.
The architectureobviated the need for feature engineering,and learns sentence representations, whichare to some extent able to capture the?rules?
governing coherent sentence struc-ture.
The proposed approach outperformsexisting baselines and generates the state-of-art performance in standard coherenceevaluation tasks1.1 IntroductionCoherence is a central aspect in natural languageprocessing of multi-sentence texts.
It is essen-tial in generating readable text that the text plan-ner compute which ordering of clauses (or sen-tences; we use them interchangeably in this paper)is likely to support understanding and avoid con-fusion.
As Mann and Thompson (1988) define it,A text is coherent when it can be ex-plained what role each clause plays withregard to the whole.1Code available at stanford.edu/?jiweil/ or byrequest from the first author.Several researchers in the 1980s and 1990s ad-dressed the problem, the most influential ofwhich include: Rhetorical Structure Theory (RST;(Mann and Thompson, 1988)), which definedabout 25 relations that govern clause interde-pendencies and ordering and give rise to texttree structures; the stepwise assembly of seman-tic graphs to support adductive inference towardthe best explanation (Hobbs et al., 1988); Dis-course Representation Theory (DRT; (Lascaridesand Asher, 1991)), a formal semantic model ofdiscourse contexts that constrain coreference andquantification scoping; the model of intention-oriented conversation blocks and their stack-basedqueueing to model attention flow (Grosz and Sid-ner, 1986), and more recently an inventory of ahundred or so binary inter-clause relations and as-sociated annotated corpus (Penn Discourse Tree-bank.
Work in text planning implemented someof these models, especially operationalized RST(Hovy, 1988) and explanation relations (Mooreand Paris, 1989) to govern the planning of coher-ent paragraphs.
Other computational work definedso called schemas (McKeown, 1985), frames withfixed sequences of clause types to achieve stereo-typical communicative intentions.Little of this work survives.
Modern researchtries simply to order a collection of clauses or sen-tences without giving an account of which order(s)is/are coherent or what the overall text structureis.
The research focuses on identifying and defin-ing a set of increasingly sophisticated features bywhich algorithms can be trained to propose order-ings.
Features being explored include the clauseentities, organized into a grid (Lapata and Barzi-lay, 2005; Barzilay and Lapata, 2008), coreferenceclues to ordering (Elsner and Charniak, 2008),named-entity categories (Eisner and Charniak,2011), syntactic features (Louis and Nenkova,2012), and others.
Besides being time-intensive(feature engineering usually requites considerable2039Figure 1: Illustrations of coherent (positive) vs not-coherent (negative) training examples.effort and can depend greatly on upstream featureextraction algorithms), it is not immediately ap-parent which aspects of a clause or a coherent textto consider when deciding on ordering.
More im-portantly, the features developed to date are stillincapable of fully specifying the acceptable order-ing(s) within a context, let alone describe why theyare coherent.Recently, deep architectures, have been appliedto various natural language processing tasks (seeSection 2).
Such deep connectionist architectureslearn a dense, low-dimensional representation oftheir problem in a hierarchical way that is capa-ble of capturing both semantic and syntactic as-pects of tokens (e.g., (Bengio et al., 2006)), en-tities, N-grams (Wang and Manning, 2012), orphrases (Socher et al., 2013).
More recent re-searches have begun looking at higher level dis-tributed representations that transcend the tokenlevel, such as sentence-level (Le and Mikolov,2014) or even discourse-level (Kalchbrenner andBlunsom, 2013) aspects.
Just as words combineto form meaningful sentences, can we take advan-tage of distributional semantic representations toexplore the composition of sentences to form co-herent meanings in paragraphs?In this paper, we demonstrate that it is feasi-ble to discover the coherent structure of a textusing distributed sentence representations learnedin a deep learning framework.
Specifically, weconsider a WINDOW approach for sentences, asshown in Figure 1, where positive examples arewindows of sentences selected from original arti-cles generated by humans, and negatives examplesare generated by random replacements2.
The se-mantic representations for terms and sentences areobtained through optimizing the neural networkframework based on these positive vs negative ex-2Our approach is inspired by Collobert et al.
?s idea (2011)that a word and its context form a positive training samplewhile a random word in that same context gives a negativetraining sample, when training word embeddings in the deeplearning framework.amples and the proposed model produces state-of-art performance in multiple standard evaluationsfor coherence models (Barzilay and Lee, 2004).The rest of this paper is organized as follows:We describe related work in Section 2, then de-scribe how to obtain a distributed representationfor sentences in Section 3, and the window compo-sition in Section 4.
Experimental results are shownin Section 5, followed by a conclusion.2 Related WorkCoherence In addition to the early computa-tional work discussed above, local coherence wasextensively studied within the modeling frame-work of Centering Theory (Grosz et al., 1995;Walker et al., 1998; Strube and Hahn, 1999; Poe-sio et al., 2004), which provides principles to forma coherence metric (Miltsakaki and Kukich, 2000;Hasler, 2004).
Centering approaches suffer from asevere dependence on manually annotated input.A recent popular approach is the entity gridmodel introduced by Barzilay and Lapata (2008), in which sentences are represented by a vec-tor of discourse entities along with their gram-matical roles (e.g., subject or object).
Proba-bilities of transitions between adjacent sentencesare derived from entity features and then concate-nated to a document vector representation, whichis used as input to machine learning classifierssuch as SVM.
Many frameworks have extendedthe entity approach, for example, by pre-groupingentities based on semantic relatedness (Filippovaand Strube, 2007) or adding more useful typesof features such as coreference (Elsner and Char-niak, 2008), named entities (Eisner and Charniak,2011), and discourse relations (Lin et al., 2011).Other systems include the global graph model(Guinaudeau and Strube, 2013) which projects en-tities into a global graph.
Louis and Nenkova(2012) introduced an HMM system in which thecoherence between adjacent sentences is modeledby a hidden Markov framework captured by the2040Figure 2: Sentential compositionality obtained from (a) recurrent / (b) recursive neural network.
Thebottom layer represents word vectors in the sentence.
The top layer hsdenotes the resulting sentencevector.transition rules of different topics.Recurrent and Recursive Neural Networks Inthe context of NLP, recurrent neural networksview a sentence as a sequence of tokens and in-corporate information from the past (i.e., preced-ing tokens) (Schuster and Paliwal, 1997; Sutskeveret al., 2011) for acquisition of the current output.At each step, the recurrent network takes as inputboth the output of previous steps and the currenttoken, convolutes the inputs, and forwards the re-sult to the next step.
It has been successfully ap-plied to tasks such as language modeling (Mikolovet al., 2010) and spoken language understanding(Mesnil et al., 2013).
The advantage of recur-rent network is that it does not depend on exter-nal deeper structure (e.g., parse tree) and is easy toimplement.
However, in the recurrent framework,long-distance dependencies are difficult to capturedue to the vanishing gradient problem (Bengio etal., 1994); two tokens may be structurally close toeach other, even though they are far away in wordsequence3.Recursive neural networks comprise anotherclass of architecture, one that relies and operateson structured inputs (e.g., parse trees).
It com-putes the representation for each parent based onits children iteratively in a bottom-up fashion.
Aseries of variations have been proposed, each tai-lored to different task-specific requirements, suchas Matrix-Vector RNN (Socher et al., 2012) thatrepresents every word as both a vector and a ma-trix, or Recursive Neural Tensor Networks (Socheret al., 2013) that allow the model to have greater3For example, a verb and its corresponding direct objectcan be far away in terms of tokens if many adjectives lies inbetween, but they are adjacent in the parse tree (Irsoy andCardie, 2013).interactions between the input vectors.
Many taskshave benefited from this recursive framework, in-cluding parsing (Socher et al., 2011b), sentimentanalysis (Socher et al., 2013), and paraphrase de-tection (Socher et al., 2011a).2.1 Distributed RepresentationsBoth recurrent and recursive networks require avector representation of each input token.
Dis-tributed representations for words were first pro-posed in (Rumelhart et al., 1988) and have beensuccessful for statistical language modeling (El-man, 1990).
Various deep learning architectureshave been explored to learn these embeddings inan unsupervised manner from a large corpus (Ben-gio et al., 2006; Collobert and Weston, 2008;Mnih and Hinton, 2007; Mikolov et al., 2013),which might have different generalization capabil-ities and are able to capture the semantic mean-ings depending on the specific task at hand.
Thesevector representations can to some extent cap-ture interesting semantic relationships, such asKing?man ?
Queue?woman (Mikolov et al.,2010), and recently have been successfully usedin various NLP applications, including named en-tity recognition, tagging, segmentation (Wang etal., 2013), and machine translation (e.g.,(Collobertand Weston, 2008; Zou et al., 2013)).3 Sentence ModelIn this section, we demonstrate the strategyadopted to compute a vector for a sentence giventhe sequence of its words and their embeddings.We implemented two approaches, Recurrent andRecursive neural networks, following the de-scriptions in for example (Mikolov et al., 2010;Sutskever et al., 2011; Socher et al., 2013).
As2041the details of both approaches can be readily foundthere, we make this section brief and omit the de-tails for brevity.Let s denote a sentence, comprised of a se-quence of words s = {w1, w2, ..., wns}, where nsdenotes the number of words within sentence s.Each word w is associated with a specific vectorembedding ew= {e1w, e2w, ..., eKw}, where K de-notes the dimension of the word embedding.
Wewish to compute the vector representation for cur-rent sentence hs= {h1s, h2s, ..., hKs}.Recurrent Sentence Representation (Recur-rent) The recurrent network captures certaingeneral considerations regarding sentential com-positionality.
As shown in Figure 2 (a), for sen-tence s, recurrent network successively takes wordwiat step i, combines its vector representation etwwith former input hi?1from step i?
1, calculatesthe resulting current embedding ht, and passes itto the next step.
The standard recurrent networkcalculates htas follows:ht= f(VRecurrent?ht?1+WRecurrent?etw+bRecurrent)(1)where WRecurrentand VRecurrentare K ?K ma-trixes.
bRecurrentdenotes K ?
1 bias vector andf = tanh is a standard element-wise nonlinearity.Note that calculation for representation at timet = 1 is given by:h1= f(VRecurrent?h0+WRecurrent?e1w+bRecurrent)(2)where h0denotes the global sentence starting vec-tor.Recursive Sentence Representation (Recursive)Recursive sentence representation relies on thestructure of parse trees, where each leaf node ofthe tree corresponds to a word from the originalsentence.
It computes a representation for eachparent node based on its immediate children re-cursively in a bottom-up fashion until reaching theroot of the tree.
Concretely, for a given parent pin the tree and its two children c1(associated withvector representation hc1) and c2(associated withvector representation hc2), standard recursive net-works calculates hpfor p as follows:hp= f(WRecursive?
[hc1, hc2] + bRecursive) (3)where [hc1, hc2] denotes the concatenating vec-tor for children vector representation hc1and hc2.WRecursiveis a K ?
2K matrix and bRecursiveisthe 1?K bias vector.
f(?)
is tanh function.Recursive neural models compute parent vec-tors iteratively until the root node?s representationis obtained, and use the root embedding to repre-sent the whole sentence, as shown in Figure 2 (b).4 Coherence ModelThe proposed coherence model adopts a windowapproach (Collobert et al., 2011), in which wetrain a three-layer neural network based on a slid-ing windows of L sentences.4.1 Sentence ConvolutionWe treat a window of sentences as a clique C andassociate each clique with a tag yCthat takes thevalue 1 if coherent, and 0 otherwise4.
As shown inFigure 1, cliques taken from original articles aretreated as coherent and those with sentences ran-domly replaced are used as negative examples.
.The sentence convolution algorithm adopted inthis paper is defined by a three-layer neural net-work, i.e., sentence-level input layer, hidden layer,and overall output layer as shown in Figure 3.
For-mally, each clique C takes as input a (L?K)?
1vector hCby concatenating the embeddings ofall its contained sentences, denoted as hC=[hs1, hs2, ..., hsL].
(Note that if we wish to clas-sify the first and last sentences and include theircontext, we require special beginning and endingsentence vectors, which are defined as h<S>forsstartand h</S>for sendrespectively.
)Let H denote the number of neurons in the hid-den (second) layer.
Then each of the hidden lay-ers takes as input hCand performs the convolutionusing a non-linear tanh function, parametrized byWsenand bsen.
The concatenating output vectorfor hidden layers, defined as qC, can therefore berewritten as:qC= f(Wsen?
hC+ bsen) (4)where Wsenis a H?
(L?K) dimensional matrixand bsenis a H ?
1 dimensional bias vector.4instead of a binary classification (correct/incorrect), an-other commonly used approach is the contrastive approachthat minimizes the score function max(0, 1 ?
s + sc) (Col-lobert et al., 2011; Smith and Eisner, 2005).
s denotes thescore of a true (coherent) window and scthe score of a cor-rupt (containing incoherence) one) in an attempt to make thescore of true windows larger and corrupt windows smaller.We tried the contrastive one for both recurrent and recursivenetworks but the binary approach constantly outperformedthe contrastive one in this task.2042Figure 3: An example of coherence model based on a window of sentences (clique).The output layer takes as input qCand generatesa scalar using linear function UTqC+b.
A sigmodfunction is then adopted to project the value to a[0,1] probability space, which can be interpretedas the probability of whether one clique is coher-ent or not.
The execution at the output layer canbe summarized as:p(yC= 1) = sigmod(UTqC+ b) (5)where U is anH?1 vector and b denotes the bias.4.2 TrainingIn the proposed framework, suppose we have Mtraining samples, the cost function for recurrentneural network with regularization on the trainingset is given by:J(?)
=1M?C?trainset{?yClog[p(yC= 1)]?
(1?
yC) log[1?
p(yC= 1)]}+Q2M?????2(6)where?
= [WRecurrent,Wsen, Usen]The regularization part is paralyzed by Q to avoidoverfitting.
A similar loss function is applied tothe recursive network with only minor parameteraltering that is excluded for brevity.To minimize the objective J(?
), we use the di-agonal variant of AdaGrad (Duchi et al., 2011)with minibatches, which is widely applied in deeplearning literature (e.g.,(Socher et al., 2011a; Peiet al., 2014)).
The learning rate in AdaGrad isadapting differently for different parameters at dif-ferent steps.
Concretely, for parameter updates, letgi?denote the subgradient at time step for param-eter ?i, which is obtained from backpropagation5,the parameter update at time step t is given by:?
?= ???1????t=0?gi2?gi?
(7)where ?
denotes the learning rate and is set to 0.01in our approach.
Optimal performance is achievedwhen batch size is set between 20 and 30.4.3 InitializationElements in Wsenare initialized by randomlydrawing from the uniform distribution [?, ],where  =?6?H+K?Las suggested in (Collobertet al., 2011).
Wrecurrent, Vrecurrent, Wrecursiveand h0are initialized by randomly sampling froma uniform distribution U(?0.2, 0.2).
All bias vec-tors are initialized with 0.
Hidden layer numberHis set to 100.Word embeddings {e} are borrowed fromSenna (Collobert et al., 2011; Collobert, 2011).The dimension for these embeddings is 50.5 ExperimentsWe evaluate the proposed coherence model on twocommon evaluation approaches adopted in exist-ing work (Barzilay and Lapata, 2008; Louis andNenkova, 2012; Elsner et al., 2007; Lin et al.,2011): Sentence Ordering and Readability Assess-ment.5.1 Sentence OrderingWe follow (Barzilay and Lapata, 2008; Louis andNenkova, 2012; Elsner et al., 2007; Lin et al.,5For more details on backpropagation through RNNs, seeSocher et al.
(2010).20432011) that all use pairs of articles, one containingthe original document order and the other a ran-dom permutation of the sentences from the samedocument.
The pairwise approach is predicatedon the assumption that the original article is al-ways more coherent than a random permutation;this assumption has been verified in Lin et al.
?swork (2011).We need to define the coherence score Sdfora given document d, where d is comprised of aseries of sentences, d = {s1, s2, .., sNd}, and Nddenotes the number of sentences within d. Basedon our clique definition, document d is comprisedof Ndcliques.
Taking window size L = 3 as ex-ample, cliques generated from document d appearas follows:< sstart, s1, s2>,< s1, s2, s3>, ...,< sNd?2, sNd?1, sNd>,< sNd?1, sNd, send>The coherence score for a given document Sdisthe probability that all cliques within d are coher-ent, which is given by:Sd=?C?dp(yC= 1) (8)For document pair < d1, d2> in our task, wewould say document d1is more coherent than d2ifSd1> Sd2(9)5.1.1 DatasetWe use two corpora that are widely employedfor coherence prediction (Barzilay and Lee, 2004;Barzilay and Lapata, 2008; Elsner et al., 2007).One contains reports on airplane accidents fromthe National Transportation Safety Board and theother contains reports about earthquakes from theAssociated Press.
These articles are about 10sentences long and usually exhibit clear sentencestructure.
For preprocessing, we only lowercasethe capital letters to match with tokens in Sennaword embeddings.
In the recursive network, sen-tences are parsed using the Stanford Parser6andthen transformed into binary trees.
The accidentcorpus ends up with a vocabulary size of 4758 andan average of 10.6 sentences per document.
Theearthquake corpus contains 3287 distinct termsand an average of 11.5 sentences per document.6http://nlp.stanford.edu/software/lex-parser.shtmlFor each of the two corpora, we have 100 arti-cles for training and 100 (accidents) and 99 (earth-quakes) for testing.
A maximum of 20 randompermutations were generated for each test arti-cle to create the pairwise data (total of 1986 testpairs for the accident corpus and 1956 for earth-quakes)7.Positive cliques are taken from original trainingdocuments.
For easy training, rather than creatingnegative examples by replacing centered sentencesrandomly, the negative dataset contains cliqueswhere centered sentences are replaced only byother sentences within the same document.5.1.2 Training and TestingDespite the numerous parameters in the deeplearning framework, we tune only two principalones for each setting: window size L (tried on{3, 5, 7}) and regularization parameterQ (tried on{0.01, 0.1, 0.25, 0.5, 1.0, 1.25, 2.0, 2.5, 5.0}).
Wetrained parameters using 10-fold cross-validationon the training data.
Concretely, in each setting,90 documents were used for training and evalua-tion was done on the remaining articles, following(Louis and Nenkova, 2012).
After tuning, the finalmodel was tested on the testing set.5.1.3 Model ComparisonWe report performance of recursive and recurrentnetworks.
We also report results from some popu-lar approaches in the literature, including:Entity Grid Model : Grid model (Barzilay andLapata, 2008) obtains the best performance whencoreference resolution, expressive syntactic infor-mation, and salience-based features are incorpo-rated.
Entity grid models represent each sentenceas a column of a grid of features and apply ma-chine learning methods (e.g., SVM) to identify thecoherent transitions based on entity features (fordetails of entity models see (Barzilay and Lapata,2008)).
Results are directly taken from Barzilayand Lapata?s paper (2008).HMM : Hidden-Markov approach proposed byLouis and Nenkova (2012) to model the state(cluster) transition probability in the coherent con-text using syntactic features.
Sentences need to beclustered in advance where the number of clus-ters is tuned as a parameter.
We directly take7Permutations are downloaded from http://people.csail.mit.edu/regina/coherence/CLsubmission/.2044Acci Earthquake AverageRecursive 0.864 0.976 0.920Recurrent 0.840 0.951 0.895Entity Grid 0.904 0.872 0.888HMM 0.822 0.938 0.880HMM+Entity 0.842 0.911 0.877HMM+Content 0.742 0.953 0.848Graph 0.846 0.635 0.740Table 1: Comparison of Different CoherenceFrameworks.
Reported baseline results are amongthe best performance regarding each approach isreprinted from prior work from (Barzilay and Lap-ata, 2008; Louis and Nenkova, 2012; Guinaudeauand Strube, 2013).the results from Louis and Nenkova?s paper andreport the best results among different combi-nations of parameter and feature settings8.
Wealso report performances of models from Louisand Nenkova?s work that combine HMM and en-tity/content models in a unified framework.Graph Based Approach : Guinaudeau andStrube (2013) extended the entity grid model toa bipartite graph representing the text, where theentity transition information needed for local co-herence computation is embedded in the bipartitegraph.
The Graph Based Approach outperformsthe original entity approach in some of feature set-tings (Guinaudeau and Strube, 2013).As can be seen in Table 1, the proposed frame-works (both recurrent and recursive) obtain state-of-art performance and outperform all existingbaselines by a large margin.
One interpretationis that the abstract sentence vector representationscomputed by the deep learning framework is morepowerful in capturing exactly the relevant the se-mantic/logical/syntactic features in coherent con-texts than features or other representations devel-oped by human feature engineering are.Another good quality of the deep learningframework is that it can be trained easily andmakes unnecessary the effort required of featureengineering.
In contrast, almost all existing base-lines and other coherence methods require sophis-ticated feature selection processes and greatly relyon external feature extraction algorithm.The recurrent network is easier to implementthan the recursive network and does not rely onexternal resources (i.e., parse trees), but the recur-sive network obtains better performance by build-8The details for information about parameter and featureof best setting can be found in (Louis and Nenkova, 2012).ing the convolution on parse trees rather than sim-ply piling up terms within the sentence, which isin line with common expectation.Both recurrent and recursive models obtain bet-ter performance on the Earthquake than the Acci-dent dataset.
Scrutiny of the corpus reveals thatarticles reporting earthquakes exhibit a more con-sistent structure: earthquake outbreak, describingthe center and intensity of the earthquake, injuriesand rescue operations, etc., while accident articlesusually exhibit more diverse scenarios.5.2 Readability AssessmentBarzilay and Lapata (2008) proposed a readabilityassessment task for stylistic judgments about thedifficulty of reading a document.
Their approachcombines a coherence system with Schwarm andOstendorf?s (2005) readability features to clas-sify documents into two categories, more read-able (coherent) documents and less readable ones.The evaluation accesses the ability to differentiate?easy to read?
documents from difficult ones ofeach model.5.2.1 DatasetBarzilay and Lapata?s (2008) data corpus isfrom the Encyclopedia Britannica and theBritannica Elementary, the latter being a newversion targeted at children.
Both versions con-tain 107 articles.
The Encyclopedia Britannicacorpus contains an average of 83.1 sentencesper document and the Britannica Elementarycontains 36.6.
The encyclopedia lemmas arewritten by different authors and consequentlyvary considerably in structure and vocabularychoice.
Early researchers assumed that the chil-dren version (Britannica Elementary) is easierto read, hence more coherent than documents inEncyclopedia Britannica.
This is a somewhatquestionable assumption that needs further inves-tigation.5.2.2 Training and TestingExisting coherence approaches again apply a pair-wise ranking strategy and the article associatedwith the higher score is considered to be the morereadable.
As the replacement strategy for gener-ating negative example is apparently not well fit-ted to this task, we adopted the following trainingframework: we use all sliding windows of sen-tences from coherent documents (documents fromBritannica Elementary) as positive examples,2045Approach AccuracyRecurrent 0.803Recursive 0.828Graph Approach 0.786Entity 0.509S&O 0.786Entity+S&O 0.888Table 2: Comparison of Different CoherenceFrameworks on Readability Assessment.
Re-ported baselines results are are taken from (Barzi-lay and Lapata, 2008; Guinaudeau and Strube,2013).
S&O: Schwarm and Ostendorf (2005).and cliques from Encyclopedia Britannica asnegative examples, and again apply Eq.
6 for train-ing and optimization.
During testing, we turn toEquations 8 and 9 for pairwise comparison.
Weadopted five-fold cross-validation in the same wayas in (Barzilay and Lapata, 2008; Guinaudeau andStrube, 2013) for fair comparison.
Parameterswere tuned within each training set also using five-fold cross-validation.
Parameters to tune includedwindow size L and regularization parameter Q.5.3 ResultsWe report results of the proposed approaches inthe work along with entity model (Barzilay andLapata, 2008) and graph based approach (Elsnerand Charniak, 2008) in Table 2.
The tabs showsthat deep learning approaches again significantlyoutperform Entry and Global Approach baselinesand are nearly comparable to the combination ofentity and S&O features.
Again, the recursivenetwork outperforms the recurrent network in thistask.6 ConclusionIn this paper, we apply two neural networkapproaches to the sentence-ordering (coherence)task, using compositional sentence representationslearned by recurrent and recursive composition.The proposed approach obtains state-of-art per-formance on the standard coherence evaluationtasks.AcknowledgementsThe authors want to thank Richard Socher andPradeep Dasigi for the clarification of deep learn-ing techniques.
We also thank the three anony-mous EMNLP reviewers for helpful comments.ReferencesRegina Barzilay and Mirella Lapata.
2008.
Modelinglocal coherence: An entity-based approach.
Compu-tational Linguistics, 34(1):1?34.Regina Barzilay and Lillian Lee.
2004.
Catching thedrift: Probabilistic content models, with applicationsto generation and summarization.
In HLT-NAACL,pages 113?120.Yoshua Bengio, Patrice Simard, and Paolo Frasconi.1994.
Learning long-term dependencies with gra-dient descent is difficult.
Neural Networks, IEEETransactions on, 5(2):157?166.Yoshua Bengio, Holger Schwenk, Jean-S?ebastienSen?ecal, Fr?ederic Morin, and Jean-Luc Gauvain.2006.
Neural probabilistic language models.
InInnovations in Machine Learning, pages 137?186.Springer.Ronan Collobert and Jason Weston.
2008.
A unifiedarchitecture for natural language processing: Deepneural networks with multitask learning.
In Pro-ceedings of the 25th international conference onMachine learning, pages 160?167.
ACM.Ronan Collobert, Jason Weston, L?eon Bottou, MichaelKarlen, Koray Kavukcuoglu, and Pavel Kuksa.2011.
Natural language processing (almost) fromscratch.
The Journal of Machine Learning Re-search, 12:2493?2537.Ronan Collobert.
2011.
Deep learning for efficient dis-criminative parsing.
In International Conference onArtificial Intelligence and Statistics, number EPFL-CONF-192374.John Duchi, Elad Hazan, and Yoram Singer.
2011.Adaptive subgradient methods for online learningand stochastic optimization.
The Journal of Ma-chine Learning Research, 12:2121?2159.Micha Eisner and Eugene Charniak.
2011.
Extendingthe entity grid with entity-specific features.
In Pro-ceedings of the 49th Annual Meeting of the Associ-ation for Computational Linguistics: Human Lan-guage Technologies: short papers-Volume 2, pages125?129.
Association for Computational Linguis-tics.Jeffrey L Elman.
1990.
Finding structure in time.Cognitive science, 14(2):179?211.Micha Elsner and Eugene Charniak.
2008.Coreference-inspired coherence modeling.
InProceedings of the 46th Annual Meeting of theAssociation for Computational Linguistics on Hu-man Language Technologies: Short Papers, pages41?44.
Association for Computational Linguistics.Micha Elsner, Joseph L Austerweil, and Eugene Char-niak.
2007.
A unified local and global model fordiscourse coherence.
In HLT-NAACL, pages 436?443.2046Katja Filippova and Michael Strube.
2007.
Extend-ing the entity-grid coherence model to semanticallyrelated entities.
In Proceedings of the Eleventh Eu-ropean Workshop on Natural Language Generation,pages 139?142.
Association for Computational Lin-guistics.Barbara J Grosz and Candace L Sidner.
1986.
Atten-tion, intentions, and the structure of discourse.
Com-putational linguistics, 12(3):175?204.Barbara J Grosz, Scott Weinstein, and Aravind K Joshi.1995.
Centering: A framework for modeling the lo-cal coherence of discourse.
Computational linguis-tics, 21(2):203?225.Camille Guinaudeau and Michael Strube.
2013.Graph-based local coherence modeling.
In Proceed-ings of the 51st Annual Meeting of the Associationfor Computational Linguistics, pages 93?103.Laura Hasler.
2004.
An investigation into the use ofcentering transitions for summarisation.
In Proceed-ings of the 7th Annual CLUK Research Colloquium,pages 100?107.Jerry R Hobbs, Mark Stickel, Paul Martin, and Dou-glas Edwards.
1988.
Interpretation as abduction.
InProceedings of the 26th annual meeting on Associ-ation for Computational Linguistics, pages 95?103.Association for Computational Linguistics.Eduard H Hovy.
1988.
Planning coherent multisenten-tial text.
In Proceedings of the 26th annual meet-ing on Association for Computational Linguistics,pages 163?169.
Association for Computational Lin-guistics.Ozan Irsoy and Claire Cardie.
2013.
Bidirectional re-cursive neural networks for token-level labeling withstructure.
arXiv preprint arXiv:1312.0493.Nal Kalchbrenner and Phil Blunsom.
2013.
Recurrentconvolutional neural networks for discourse compo-sitionality.
arXiv preprint arXiv:1306.3584.Mirella Lapata and Regina Barzilay.
2005.
Automaticevaluation of text coherence: Models and represen-tations.
In IJCAI, volume 5, pages 1085?1090.Alex Lascarides and Nicholas Asher.
1991.
Discourserelations and defeasible knowledge.
In Proceedingsof the 29th annual meeting on Association for Com-putational Linguistics, pages 55?62.
Association forComputational Linguistics.Quoc Le and Tomas Mikolov.
2014.
Distributed repre-sentations of sentences and documents.Ziheng Lin, Hwee Tou Ng, and Min-Yen Kan. 2011.Automatically evaluating text coherence using dis-course relations.
In Proceedings of the 49th An-nual Meeting of the Association for ComputationalLinguistics: Human Language Technologies-Volume1, pages 997?1006.
Association for ComputationalLinguistics.Annie Louis and Ani Nenkova.
2012.
A coherencemodel based on syntactic patterns.
In Proceedingsof the 2012 Joint Conference on Empirical Methodsin Natural Language Processing and ComputationalNatural Language Learning, pages 1157?1168.
As-sociation for Computational Linguistics.William C Mann and Sandra A Thompson.
1988.Rhetorical structure theory: Toward a functional the-ory of text organization.
Text, 8(3):243?281.Kathleen R McKeown.
1985.
Discourse strategies forgenerating natural-language text.
Artificial Intelli-gence, 27(1):1?41.Gr?egoire Mesnil, Xiaodong He, Li Deng, and YoshuaBengio.
2013.
Investigation of recurrent-neural-network architectures and learning methods for spo-ken language understanding.
Interspeech.Tomas Mikolov, Martin Karafi?at, Lukas Burget, JanCernock`y, and Sanjeev Khudanpur.
2010.
Recur-rent neural network based language model.
In IN-TERSPEECH, pages 1045?1048.Tomas Mikolov, Kai Chen, Greg Corrado, and Jef-frey Dean.
2013.
Efficient estimation of wordrepresentations in vector space.
arXiv preprintarXiv:1301.3781.Eleni Miltsakaki and Karen Kukich.
2000.
The roleof centering theory?s rough-shift in the teaching andevaluation of writing skills.
In Proceedings of the38th Annual Meeting on Association for Computa-tional Linguistics, pages 408?415.
Association forComputational Linguistics.Andriy Mnih and Geoffrey Hinton.
2007.
Three newgraphical models for statistical language modelling.In Proceedings of the 24th international conferenceon Machine learning, pages 641?648.
ACM.Johanna D Moore and Cecile L Paris.
1989.
Planningtext for advisory dialogues.
In Proceedings of the27th annual meeting on Association for Computa-tional Linguistics, pages 203?211.
Association forComputational Linguistics.Wenzhe Pei, Tao Ge, and Chang Baobao.
2014.
Max-margin tensor neural network for chinese word seg-mentation.
In Proceedings of ACL.Massimo Poesio, Rosemary Stevenson, Barbara Di Eu-genio, and Janet Hitzeman.
2004.
Centering: Aparametric theory and its instantiations.
Computa-tional linguistics, 30(3):309?363.David E Rumelhart, Geoffrey E Hinton, and Ronald JWilliams.
1988.
Learning representations by back-propagating errors.
MIT Press, Cambridge, MA,USA.Mike Schuster and Kuldip K Paliwal.
1997.
Bidirec-tional recurrent neural networks.
Signal Processing,IEEE Transactions on, 45(11):2673?2681.2047Sarah E Schwarm and Mari Ostendorf.
2005.
Read-ing level assessment using support vector machinesand statistical language models.
In Proceedings ofthe 43rd Annual Meeting on Association for Com-putational Linguistics, pages 523?530.
Associationfor Computational Linguistics.Noah A Smith and Jason Eisner.
2005.
Contrastiveestimation: Training log-linear models on unlabeleddata.
In Proceedings of the 43rd Annual Meetingon Association for Computational Linguistics, pages354?362.
Association for Computational Linguis-tics.Richard Socher, Christopher D Manning, and An-drew Y Ng.
2010.
Learning continuous phraserepresentations and syntactic parsing with recursiveneural networks.
In Proceedings of the NIPS-2010Deep Learning and Unsupervised Feature LearningWorkshop, pages 1?9.Richard Socher, Eric H Huang, Jeffrey Pennington,Andrew Y Ng, and Christopher D Manning.
2011a.Dynamic pooling and unfolding recursive autoen-coders for paraphrase detection.
In NIPS, vol-ume 24, pages 801?809.Richard Socher, Cliff C Lin, Chris Manning, and An-drew Y Ng.
2011b.
Parsing natural scenes and nat-ural language with recursive neural networks.
InProceedings of the 28th International Conference onMachine Learning (ICML-11), pages 129?136.Richard Socher, Brody Huval, Christopher D Manning,and Andrew Y Ng.
2012.
Semantic compositional-ity through recursive matrix-vector spaces.
In Pro-ceedings of the 2012 Joint Conference on Empiri-cal Methods in Natural Language Processing andComputational Natural Language Learning, pages1201?1211.
Association for Computational Linguis-tics.Richard Socher, Alex Perelygin, Jean Y Wu, JasonChuang, Christopher D Manning, Andrew Y Ng,and Christopher Potts.
2013.
Recursive deep mod-els for semantic compositionality over a sentimenttreebank.
In Proceedings of the Conference on Em-pirical Methods in Natural Language Processing(EMNLP), pages 1631?1642.Michael Strube and Udo Hahn.
1999.
Functionalcentering: Grounding referential coherence in in-formation structure.
Computational linguistics,25(3):309?344.Ilya Sutskever, James Martens, and Geoffrey E Hin-ton.
2011.
Generating text with recurrent neuralnetworks.
In Proceedings of the 28th InternationalConference on Machine Learning (ICML-11), pages1017?1024.Marilyn A Walker, Aravind Krishna Joshi, andEllen Friedman Prince.
1998.
Centering theory indiscourse.
Oxford University Press.Sida Wang and Christopher D Manning.
2012.
Base-lines and bigrams: Simple, good sentiment and topicclassification.
In Proceedings of the 50th AnnualMeeting of the Association for Computational Lin-guistics: Short Papers-Volume 2, pages 90?94.
As-sociation for Computational Linguistics.Houfeng Wang, Longkai Zhang, Li Li, He Zhengyan,and Ni Sun.
2013.
Improving chinese word seg-mentation on micro-blog using rich punctuations.Will Y Zou, Richard Socher, Daniel Cer, and Christo-pher D Manning.
2013.
Bilingual word embed-dings for phrase-based machine translation.
In Con-ference on Empirical Methods in Natural LanguageProcessing (EMNLP 2013).2048
