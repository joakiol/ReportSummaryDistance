Exploiting Syntactic Structure for Language ModelingCiprian Che lba  and F reder ick  Je l inekCenter  for Language and Speech Process ingThe  Johns  Hopk ins  Univers i ty,  Bar ton  Hal l  3203400 N. Char les  St., Ba l t imore ,  MD-21218,  USA{chelba, je l inek} @jhu.eduAbst ractThe paper presents a language model that devel-ops syntactic structure and uses it to extract mean-ingful information from the word history, thus en-abling the use of long distance dependencies.
Themodel assigns probability to every joint sequenceof words-binary-parse-structure with headword an-notation and operates in a left-to-right manner - -therefore usable for automatic speech recognition.The model, its probabilistic parameterization, a d aset of experiments meant to evaluate its predictivepower are presented; an improvement over standardtrigram modeling is achieved.1 IntroductionThe main goal of the present work is to develop a lan-guage model that uses syntactic structure to modellong-distance dependencies.
During the summer96DoD Workshop a similar attempt was made by thedependency modeling roup.
The model we presentis closely related to the one investigated in (Chelbaet al, 1997), however different in a few importantaspects:?
our model operates in a left-to-right manner, al-lowing the decoding of word lattices, as opposed tothe one referred to previously, where only whole sen-tences could be processed, thus reducing its applica-bility to n-best list re-scoring; the syntactic structureis developed as a model component;?
our model is a factored version of the onein (Chelba et al, 1997), thus enabling the calculationof the joint probability of words and parse structure;this was not possible in the previous case due to thehuge computational complexity of the model.Our model develops yntactic structure incremen-tally while traversing the sentence from left to right.This is the main difference between our approachand other approaches to statistical natural anguageparsing.
Our parsing strategy is similar to the in-cremental syntax ones proposed relatively recentlyin the linguistic community (Philips, 1996).
Theprobabilistic model, its parameterization a d a fewexperiments hat are meant o evaluate its potentialfor speech recognition are presented./ / //~t ract  NPthe_DT contract NN ~,dc,l VBI)with INa DT Ioss_NN of_IN 7_CD ~: ~::, X~,'~ afterFigure 1: Partial parse2 The  Bas ic  Idea  and TerminologyConsider predicting the word a f te r  in the sentence:the contract  ended wi th  a loss of 7 centsafter trading as low as 89 cents.A 3-gram approach would predict after from(7, cents)  whereas it is intuitively clear that thestrongest predictor would be ended which is outsidethe reach of even 7-grams.
Our assumption is thatwhat enables humans to make a good prediction ofa f te r  is the syntactic structure in the past.
Thelinguistically correct partial parse of the word his-tory when predicting a f te r  is shown in Figure 1.The word ended is called the headword of the con-stituent (ended (with ( .
.
. )
)) and ended is an ex-posed headword when predicting a f te r  - -  topmostheadword in the largest constituent that contains it.The syntactic structure in the past filters out irrel-evant words and points to the important ones, thusenabling the use of long distance information whenpredicting the next word.Our model will attempt to build the syntacticstructure incrementally while traversing the sen-tence left-to-right.
The model will assign a probabil-ity P(W, T) to every sentence W with every possiblePOStag assignment, binary branching parse, non-terminal abel and headword annotation for everyconstituent of T.Let W be a sentence of length n words to whichwe have prepended <s> and appended </s> sothat Wo =<s> and w,+l =</s>.
Let Wk be theword k-prefix Wo...wk of the sentence and WkTk225" i  .
.
.
.  "
?''(?:s>.
SB) ....... (wp .
t p) (w {p?l }.
L_( I~-I }) ........ (wk .
t_k) w_(  k*l }.... </s.~T \[-ra}<s>h_ ( -2  } h (-!  }
h_O.
.
.
.
.
.
.
.
.Figure 2: A word-parse k-prefix Figure 4: Before an adjoin operation_ (<,'s>, TOP)(<s>, SB) (w_l, ~_1) ..................... ('*'_n, t_n) (</~, SE)Figure 3: Complete parsethe word-parse k-prefix.
To stress this point, aword-parse k-prefix contains - -  for a given parse- -  only those binary subtrees whose span is com-pletely included in the word k-prefix, excludingw0 =<s>.
Single words along with their POStagcan be regarded as root-only trees.
Figure 2 showsa word-parse k-prefix; h_0 ..  h_{-m} are the ex-posed heads, each head being a pair(headword, non-terminal abel), or (word, POStag) in the case of aroot-only tree.
A complete parse - -  Figure 3 - -  isany binary parse of the(w l , t l ) .
.
.
(wn , t , )  (</s>, SE) sequence with therestriction that (</s>, TOP') is the only allowedhead.
Note that ( (w l , t l ) .
.
.
(w , , t , ) )  needn't be aconstituent, but for the parses where it is, there isno restriction on which of its words is the headwordor what is the non-terminal label that accompaniesthe headword.The model will operate by means of three mod-ules:?
WORD-PREDICTOR predicts the next wordwk+l given the word-parse k-prefix and then passescontrol to the TAGGER;?
TAGGER predicts the POStag of the next wordtk+l given the word-parse k-prefix and the newlypredicted word and then passes control to thePARSER;?
PARSER grows the already existing binarybranching structure by repeatedly generating thetransitions:(unary, NTlabel), (adjoin-left, NTlabel) or(adjoin-right, NTlabel) until it passes controlto the PREDICTOR by taking a null transition.NTlabel is the non-terminal label assigned to thenewly built constituent and {left ,right} specifieswhere the new headword is inherited from.The operations performed by the PARSER areillustrated in Figures 4-6 and they ensure that allpossible binary branching parses with all possibleT'_{.m?l  <-<s.~.<s>h '{ - I  } = h_(-2 } h'_0= (h_{-I }.word, NTlabel)Figure 5: Result of adjoin-left under NTlabelheadword and non-terminal label assignments forthe wl .
.
.
wk word sequence can be generated.
Thefollowing algorithm formalizes the above descriptionof the sequential generation of a sentence with acomplete parse.Transition t; // a PARSER transitionpredict (<s>, SB);do{/ /WORD-PREDICTORand TAGGERpredict (next_word, POStag);//PARSERdo{if(h_{-l}.word != <s>){if(h_O.word == </s>)t = (adjoin-right, TOP');else{i f (h_O.tag== NTlabel)t = \[(adjoin-{left,right}, NTlabel),null\];elset = \[(unary, NTlabel),(adjoin-{left,right}, NTlabel),null\];}}else{if (h_O.tag == NTlabel)t = null;elset = \[(unary, NTlabel), null\] ;}}while(t != null) //done PARSER}while ( !
(h_0.
word==</s> && h_{- 1 }.
word==<s>) )t = (adjoin-right, TOP); //adjoin <s>_SB; DONE;The unary transition is allowed only when themost recent exposed head is a leaf of the tree --a regular word along with its POStag -- hence itcan be taken at most once at a given position in the226T'_l.m+l } <-<s>h' {- I }=h {-2} h'_0 = (h_0.word, NTlab?l)Figure 6: Result of adjoin-right under NTlabelinput word string.
The second subtree in Figure 2provides an example of a unary transition followedby a null transition.It is easy to see that any given word sequencewith a possible parse and headword annotation isgenerated by a unique sequence of model actions.This will prove very useful in initializing our modelparameters from a treebank - -  see section 3.5.3 P robab i l i s t i c  Mode lThe probability P(W, T) of a word sequence W anda complete parse T can be broken into:P(W, T) =1-I "+xr P(wk/Wk-aTk-x) " P(tk/Wk-lTk-x,wk)" k=X I.N~~I P(Pki /Wk-xTk-a' Wk, tk,pkx ... pLX)\](1)i=Xwhere:?
Wk-lTk-x is the word-parse (k - 1)-prefix?
wk is the word predicted by WORD-PREDICTOR* tk is the tag assigned to wk by the TAGGER?
Nk -- 1 is the number of operations the PARSERexecutes before passing control to the WORD-PREDICTOR (the Nk-th operation at position k isthe nu l l  transition); Nk is a function of T?
pi k denotes the i-th PARSER operation carried outat position k in the word string;p~ 6 {(unary, NTlabel),(adjoin-left, NTlabel),(adjoin-right, NTlabel), null},pk 6 { (adjoin-left, NTlabel),(adjoin-right, NTlabel)}, 1 < i < Nk ,p~ =null, i = NkOur model is based on three probabilities:P(wk/Wk-lTk-1) (2)P(tk/wk, Wk-lTk-x) (3)P(p~/wk,tk,Wk--xTk--l,p~.. k "Pi--X) C a)As can be seen, (wk, tk, Wk-xTk-x,p~...pki_x) is oneof the Nk word-parse k-prefixes WkTk at position kin the sentence, i = 1, Nk.To ensure a proper probabilistic model (1) wehave to make sure that (2), (3) and (4) are well de-fined conditional probabilities and that the modelhalts with probability one.
Consequently, certainPARSER and WORD-PREDICTOR probabilitiesmust be given specific values:?
P(null /WkTk) = 1, if h_{-1}.word = <s> andh_{0} ~ (</s>, TOP') - -  that is, before predicting</s> - -  ensures that (<s>, SB) is adjoined in thelast step of the parsing process;?
P ( (ad jo in - r ight ,  TOP)/WkTk) = 1, ifh_O = (</s>, TOP') and h_{-l}.word = <s>andP( (ad jo in - r ight ,  TOP')/WkTk) = 1, ifh_0 = (</s>, TOP') and h_{-1}.word ~ <s>ensure that the parse generated by our model is con-sistent with the definition of a complete parse;?
P ( (unary ,  NWlabel)/WkTk) = 0, if h_0.
tagPOStag ensures correct reatment of unary produc-tions;?
3e > O, VWk-lTk-l,P(wk=</s>/Wk-xTk-1) >_ eensures that the model halts with probability one.The word-predictor model (2) predicts the nextword based on the preceding 2 exposed heads, thusmaking the following equivalence classification:P(wk/Wk-lTk-1) = P(wk/ho, h- l )After experimenting with several equivalence clas-sifications of the word-parse prefix for the taggermodel, the conditioning part of model (3) was re-duced to using the word to be tagged and the tagsof the two most recent exposed heads:P(tk/Wk, Wk-lTk-1) = P(tk/wk, ho.tag, h-l.tag)Model (4) assigns probability to different parses ofthe word k-prefix by chaining the elementary oper-ations described above.
The workings of the parsermodule are similar to those of Spatter (Jelinek et al,1994).
The equivalence classification of the WkTkword-parse we used for the parser model (4) was thesame as the one used in (Collins, 1996):p (pk / Wk Tk ) = p (pk / ho , h-x)It is worth noting that if the binary branchingstructure developed by the parser were always right-branching and we mapped the POStag and non-terminal label vocabularies to a single type then ourmodel would be equivalent o a trigram languagemodel.3.1 Mode l ing  Too lsAll model components - -  WORD-PREDICTOR,TAGGER, PARSER - -  are conditional probabilis-tic models of the type P(y/x l ,x2, .
.
.
,xn)  wherey, Xx,X2,...,Xn belong to a mixed bag of words,POStags, non-terminal labels and parser operations(y only).
For simplicity, the modeling method wechose was deleted interpolation among relative fre-quency estimates of different orders fn(') using a227recursive mixing scheme:P(y /x l ,  .
.
.
,xn) =A(x l , .
.
.
, x , ) -P (y /x l , .
.
.
, x , _x )  +(1  - -  ~(X l , .
.
.
,Xn) ) "  fn (y /X l , .
.
.
,Xn) ,  (5 )f - l  (Y) = uniform(vocabulary(y)) (6)As can be seen, the context mixing scheme dis-cards items in the context in right-to-left order.
TheA coefficients are tied based on the range of thecount C(xx, .
.
.
,Xn) .
The approach is a standardone which doesn't require an extensive descriptiongiven the literature available on it (Jelinek and Mer-cer, 1980).3.2  Search  St ra tegySince the number of parses for a given word prefixWt grows exponentially with k, I{Tk}l ,,.
O(2k), thestate space of our model is huge even for relativelyshort sentences so we had to use a search strategythat prunes it.
Our choice was a synchronous multi-stack search algorithm which is very similar to abeam search.Each stack contains hypotheses - - partial parses- -  that have been constructed by the same number ofpredictor and the same number of parser operations.The hypotheses in each stack are ranked accordingto the ln(P(W, T)) score, highest on top.
The widthof the search is controlled by two parameters:?
the maximum stack depth - -  the maximum num-ber of hypotheses the stack can contain at any givenstate;?
log-probability threshold - -  the difference betweenthe log-probability score of the top-most hypothesisand the bottom-most hypothesis at any given stateof the stack cannot be larger than a given threshold.Figure 7 shows schematically the operations asso-ciated with the scanning of a new word Wk+l.
Theabove pruning strategy proved to be insufficient sowe chose to also discard all hypotheses whose scoreis more than the log-probability threshold below thescore of the topmost hypothesis.
This additionalpruning step is performed after all hypotheses instage k' have been extended with the null parsertransition and thus prepared for scanning a newword.3.3 Word  Level  Perp lex i tyThe conditional perplexity calculated by assigningto a whole sentence the probability:P(W/T*)  = f i  P(wk+l/WkT~), (7)k=Owhere T* = argrnaxTP(W, T), is not valid becauseit is not causal: when predicting wk+l we use T*which was determined by looking at the entire sen-tence.
To be able to compare the perplexity of our(k) \0 parser ot~k predict.
\[p parser opk predict.p+l parserk predict.P_k parserk predict.
(k') \~ 0 parser opt_"~+1 predict.
\[" -1 k+l predict.p+ 1 parser ~+1 predict\]z/~ P_k parser ~_predict V -_k+ 1 parse~e~1 predict.~"word predictorand tagger- (k+l)oqIII)~_-~-- ) \ ]pparser  opI- - =-~+1 predict.
I!
- -~+1 parser\]- -  7 - - >~+..} pred ic \ [ .
Iiii- - - !
- -~ lP  kparser!---:, - :-~+ 1 predict.Inullparser t ansitionsparser adjoin/unary transitionsFigure 7: One search extension cyclemodel with that resulting from the standard tri-gram approach, we need to factor in the entropy ofguessing the correct parse T~ before predicting wk+l,based solely on the word prefix Wk.The probability assignment for the word at posi-tion k + 1 in the input sentence is made using:P(Wk+l/Wk) =~TheS~ P(Wk+x/WkTk) " p(Wk,Tk), (8)p(Wk,Tk) = P(W Tk)/ P(WkTk) (9)TkESkwhich ensures a proper probability over strings W*,where Sk is the set of all parses present in our stacksat the current stage k.Another possibility for evaluating the word levelperplexity of our model is to approximate the prob-ability of a whole sentence:NP(W) = Z P(W, T (k)) (10)k=lwhere T (k) is one of the "N-best" - -  in the sensedefined by our search - -  parses for W. This is adeficient probability assignment, however useful forjustifying the model parameter re-estimation.The two estimates (8) and (10) are both consistentin the sense that if the sums are carried over all228possible parses we get the correct value for the wordlevel perplexity of our model.3.4 Parameter  Re-es t imat ionThe major problem we face when trying to reesti-mate the model parameters is the huge state space ofthe model and the fact that dynamic programmingtechniques imilar to those used in HMM parame-ter re-estimation cannot be used with our model.Our solution is inspired by an HMM re-estimationtechnique that works on pruned - -  N-best - -  trel-lises(Byrne t al., 1998).Let (W, T(k)), k = 1. .
.
N be the set of hypothe-ses that survived our pruning strategy until the endof the parsing process for sentence W. Each ofthem was produced by a sequence of model actions,chained together as described in section 2; let us callthe sequence of model actions that produced a given(W, T) the derivation(W, T).Let an elementary event in the derivation(W, T)be :, (m,) .~(m,)~ where:* l is the index of the current model action;* ml is the model component - -  WORD-PREDICTOR, TAGGER, PARSER - -  that takesaction number l in the derivation(W, T);, y~mt) is the action taken at position I in the deriva-tion:if mt = WORD-PREDICTOR,  then y~m,) is a word;if mt -- TAGGER, then y~m~) is a POStag;if mt = PARSER, then y~m~) is a parser-action;?
~m~) is the context in which the above action wastaken:if rat = WORD-PREDICTOR or PARSER, then_~,na) = (ho.tag, ho.word, h-1 .tag, h-l.word);if rat = TAGGER, then~mt) = (word-to-tag, ho.tag, h-l.tag).The probability associated with each model ac-tion is determined as described in section 3.1, basedon counts C (m) (y(m), x_("0), one set for each modelcomponent.Assuming that the deleted interpolation coeffi-cients and the count ranges used for tying them stayfixed, these counts are the only parameters to bere-estimated in an eventual re-estimation procedure;indeed, once a set of counts C (m) (y(m), x_(m)) is spec-ified for a given model ra, we can easily calculate:?
the relative frequency estimatesfn(m)/,,(m) Ix(m) ~ for all context orders kY I _n  /n = 0...maximum-order(model(m));?
the count c(m)(x_ (m)) used for determining theA(x_ (m)) value to be used with the order-n contextx(m)"This is all we need for calculating the probability ofan elementary event and then the probability of anentire derivation.One training iteration of the re-estimation proce-dure we propose is described by the following algo-rithm:N-best parse development data; // counts.El// prepare counts.E(i+l)for each model component c{gather_counts development model_c;}In the parsing stage we retain for each "N-best" hy-pothesis (W, T(k)), k = 1. .
.
N, only the quantity?
(W, T(k)) p(W,T(k))/ N = ~-~k=l P(W, T(k))and its derivation(W,T(k)).
We then scan allthe derivations in the "development set" and, foreach occurrence of the elementary event (y(m), x_(m))in derivation(W,T(k)) we accumulate the value?
(W,T (k)) in the C(m)(y(m),x__ (m)) counter to beused in the next iteration.The intuition behind this procedure is that?
(W,T (k)) is an approximation to the P(T(k)/w)probability which places all its mass on the parsesthat survived the parsing process; the above proce-dure simply accumulates the expected values of thecounts c(m)(y(m),x (m)) under the ?
(W,T (k)) con-ditional distribution.
As explained previously, theC(m) (y(m), X_(m)) counts are the parameters definingour model, making our procedure similar to a rigor-ous EM approach (Dempster et al, 1977).A particular - -  and very interesting - -  case is thatof events which had count zero but get a non-zerocount in the next iteration, caused by the "N-best"nature of the re-estimation process.
Consider a givensentence in our "development" set.
The "N-best"derivations for this sentence are trajectories throughthe state space of our model.
They will changefrom one iteration to the other due to the smooth-ing involved in the probability estimation and thechange of the parameters - - event counts - -  defin-ing our model, thus allowing new events to appearand discarding others through purging low probabil-ity events from the stacks.
The higher the numberof trajectories per sentence, the more dynamic thischange is expected to be.The results we obtained are presented in the ex-periments ection.
All the perplexity evaluationswere done using the left-to-right formula (8) (L2R-PPL) for which the perplexity on the "developmentset" is not guaranteed to decrease from one itera-tion to another.
However, we believe that our re-estimation method should not increase the approxi-mation to perplexity based on (10) (SUM-PPL) - -again, on the "development set"; we rely on the con-sistency property outlined at the end of section 3.3to correlate the desired decrease in L2R-PPL withthat in SUM-PPL.
No claim can be made aboutthe change in either L2R-PPL or SUM-PPL on testdata.229Y_!
Y_k Y_n Y l Y_k Y_nFigure 8: Binarization schemes3.5  In i t ia l  ParametersEach model component - -  WORD-PREDICTOR,TAGGER, PARSER - -  is trained initially from aset of parsed sentences, after each parse tree (W, T)undergoes:?
headword percolation and binarization - -  see sec-tion 4;?
decomposition i to its derivation(W, T).Then, separately for each m model component, we:?
gather joint counts cCm)(y(m),x (m)) from thederivations that make up the "development data"using ?
(W,T) = 1;?
estimate the deleted interpolation coefficients onjoint counts gathered from "check data" using theEM algorithm.These are the initial parameters used with the re-estimation procedure described in the previous sec-tion.4 Headword Percolat ion andBinar izat ionIn order to get initial statistics for our model com-ponents we needed to binarize the UPenn Tree-bank (Marcus et al, 1995) parse trees and perco-late headwords.
The procedure we used was to firstpercolate headwords using a context-free (CF) rule-based approach and then binarize the parses by us-ing a rule-based approach again.The headword of a phrase is the word that bestrepresents the phrase, all the other words in thephrase being modifiers of the headword.
Statisti-cally speaking, we were satisfied with the outputof an enhanced version of the procedure describedin (Collins, 1996) - -  also known under the name"Magerman & Black Headword Percolation Rules".Once the position of the headword within a con-stituent - -  equivalent with a CF production of thetype Z --~ Y1.--Yn , where Z, Y1,...Yn are non-terminal abels or POStags (only for Y/) - -  is iden-tified to be k, we binarize the constituent as follows:depending on the Z identity, a fixed rule is usedto decide which of the two binarization schemes inFigure 8 to apply.
The intermediate nodes createdby the above binarization schemes receive the non-terminal abel Z ~.5 ExperimentsDue to the low speed of the parser - -  200 wds/minfor stack depth 10 and log-probability threshold6.91 nats (1/1000) - -  we could carry out the re-estimation technique described in section 3.4 on only1 Mwds of training data.
For convenience we choseto work on the UPenn Treebank corpus.
The vocab-ulary sizes were:* word vocabulary: 10k, open - -  all words outsidethe vocabulary are mapped to the <unk> token;?
POS tag vocabulary: 40, closed;?
non-terminal tag vocabulary: 52, closed;?
parser operation vocabulary: 107, closed;The training data was split into "development" set- -  929,564wds (sections 00-20) - -  and "check set"- -  73,760wds (sections 21-22); the test set size was82,430wds (sections 23-24).
The "check" set hasbeen used for estimating the interpolation weightsand tuning the search parameters; the "develop-ment" set has been used for gathering/estimatingcounts; the test set has been used strictly for evalu-ating model performance.Table 1 shows the results of the re-estimation tech-nique presented in section 3.4.
We achieved a reduc-tion in test-data perplexity bringing an improvementover a deleted interpolation trigram model whoseperplexity was 167.14 on the same training-test data;the reduction is statistically significant according toa sign test.iteration DEV set TEST setnumber L2R-PPL L2R-PPLE0 24.70 167.47E1 22.34 160.76E2 21.69 158.97E3 21.26 158.283-gram 21.20 167.14Table 1: Parameter re-estimation resultsSimple linear interpolation between our model andthe trigram model:Q(wk+l /Wk)  =)~" P(Wk+I/Wk-I,Wk) + (1 -- A)" P(wk+l/Wk)yielded a further improvement in PPL, as shown inTable 2.
The interpolation weight was estimated oncheck data to be )~ = 0.36.An overall relative reduction of 11% over the trigrammodel has been achieved.6 Conclus ions and Future Direct ionsThe large difference between the perplexity of ourmodel calculated on the "development" set - -  used230II iterationnumberII EoE3\[l 3-gramTEST setL2R-PPL167.47158.28167.14TEST set3-gram interpolated PPL152.25 II148.90167.14 IITable 2: Interpolation with trigram resultsfor model parameter stimation -- and "test" set - -unseen data - -  shows that the initial point we choosefor the parameter values has already captured a lotof information from the training data.
The sameproblem is encountered in standard n-gram languagemodeling; however, our approach as more flexibilityin dealing with it due to the possibility of reestimat-ing the model parameters.We believe that the above experiments show thepotential of our approach for improved languagemodels.
Our future plans include:?
experiment with other parameterizations than thetwo most recent exposed heads in the word predictormodel and parser;?
estimate a separate word predictor for left-to-right language modeling.
Note that the correspond-ing model predictor was obtained via re-estimationaimed at increasing the probability of the "N-best"parses of the entire sentence;?
reduce vocabulary of parser operations; extremecase: no non-terminal labels/POS tags, word onlymodel; this will increase the speed of the parserthus rendering it usable on larger amounts of train-ing data and allowing the use of deeper stacks - -resulting in more "N-best" derivations per sentenceduring re-estimation;?
relax - -  flatten - -  the initial statistics in the re-estimation ofmodel parameters; this would allow themodel parameters to converge to a different pointthat might yield a lower word-level perplexity;?
evaluate model performance on n-best sentencesoutput by an automatic speech recognizer.7 AcknowledgmentsThis research as been funded by the NSFIRI-19618874 grant (STIMULATE).The authors would like to thank Sanjeev Khu-danpur for his insightful suggestions.
Also to HarryPrintz, Eric Ristad, Andreas Stolcke, Dekai Wu andall the other members of the dependency model-ing group at the summer96 DoD Workshop for use-ful comments on the model, programming supportand an extremely creative nvironment.
Also thanksto Eric Brill, Sanjeev Khudanpur, David Yarowsky,Radu Florian, Lidia Mangu and Jun Wu for usefulinput during the meetings of the people working onour STIMULATE grant.Referencesw.
Byrne, A. Gunawardana, nd S. Khudanpur.1998.
Information geometry and EM variants.Technical Report CLSP Research Note 17, De-partment of Electical and Computer Engineering,The Johns Hopkins University, Baltimore, MD.C.
Chelba, D. Engle, F. Jelinek, V. Jimenez, S. Khu-danpur, L. Mangu, H. Printz, E. S. Ristad,R.
Rosenfeld, A. Stolcke, and D. Wu.
1997.
Struc-ture and performance of a dependency languagemodel.
In Proceedings of Eurospeech, volume 5,pages 2775-2778.
Rhodes, Greece.Michael John Collins.
1996.
A new statistical parserbased on bigram lexical dependencies.
In Proceed-ings of the 34th Annual Meeting of the Associ-ation for Computational Linguistics, pages 184-191.
Santa Cruz, CA.A.
P. Dempster, N. M. Laird, and D. B. Rubin.
1977.Maximum likelihood from incomplete data via theEM algorithm.
In Journal of the Royal StatisticalSociety, volume 39 of B, pages 1-38.Frederick Jelinek and Robert Mercer.
1980.
Inter-polated estimation of markov source parametersfrom sparse data.
In E. Gelsema nd L. Kanal, ed-itors, Pattern Recognition in Practice, pages 381-397.F.
Jelinek, J. Lafferty, D. M. Magerman, R. Mercer,A.
Ratnaparkhi, and S. Roukos.
1994.
Decisiontree parsing using a hidden derivational model.In ARPA, editor, Proceedings of the Human Lan-guage Technology Workshop, pages 272-277.M.
Marcus, B. Santorini, and M. Marcinkiewicz.1995.
Building a large annotated corpus of En-glish: the Penn Treebank.
Computational Lin-guistics, 19(2):313-330.Colin Philips.
1996.
Order and Structure.
Ph.D.thesis, MIT.
Distributed by MITWPL.231
