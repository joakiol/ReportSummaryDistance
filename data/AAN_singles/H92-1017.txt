Recent Improvements  and Benchmark Results  forParamax ATIS SystemLewis M. Norton, Deborah A. Dahl, and Marcia C. Linebarger *Paramax Systems Corporat ion ,  a Un isys  CompanyP .O.
Box  517Paol i ,  PA ,  19301theABSTRACTThis paper describes three relatively domain- independent ca-pabi l i t ies recently added to the Paramax spoken language un-derstanding system: non-monotonic reasoning, implicit refer-ence resolution, and database query paraphrase.
In addition,we discuss the results of the February 1992 ATIS benchmarktests.
We describe a variation on the standard evaluationmetric which provides a more tightly controlled measure ofprogress.
Finally, we briefly describe an experiment whichwe have done in extending the n-best speech/language inte-gration architecture to improving OCR accuracy.1.
INTRODUCTIONIn recent work on the Paramax spoken language under-standing system we have focused on domain-independentcapabilities whose relevance xtends beyond the ATIS ap-plication.
These include non-monotonic reasoning, im-plicit reference resolution, and the ability to create asimple paraphrase of a query for feedback to the user.Although these capabilities were motivated by ATIS data,they are likely to be required for processing data fromother domains.
We describe these improvements in thefollowing section.
In addition, we discuss our results onthe ATIS benchmark tests.
Because test data vary fromtest to test, it can be difficult to know whether fluctua-tions in performance are real, or are due to idiosyncraciesin the particular test data selected.
We have experi-mented with an evaluation paradigm in which systemsused in earlier tests run the test data from the currenttest, thus controlling for the effect of variations in thetest data.
We suggest hat this paradigm could providea valuable supplement to the official tests.2.
SYSTEM IMPROVEMENTS2 .1 .
Non-Monoton ic  Reason ingWe previously described \[1\] a feature of the PUN-DIT natural language processing system whereby the*This paper was supported by DARPA contract N000014-89-C0171, administered by the Office of Naval Research, and by inter-nal funding from Paramax Systems Corporation (formerly UnisysDefense Systems).
We wish to thank Suzanne Taylor for help-ful discussions on applying natural anguage constraints to OCRenhancement.system makes inferences involving more than onedecomposition.
1 For example, the instantiated ecom-positions produced for "flights leaving Boston" are:f l i ght_C( f l ight l ,  source(_ )  .
.
.
.
)l eaveP( leave l ,  f l i ght ( f l ight l ) ,source(boston)  .
.
.
.
)Application of a rule relating the leaveP and the flight_Cdecompositions results in the source slot of the flight_Cdecomposition being instantiated to "boston".We have extended this feature to make it possible to re-tract such inferences.
This extension allows PUNDIT todo non-monotonic reasoning (\[2\]).
That is, the systemcan make and reason from tentative inferences, whichcan be removed from the context if they are not sup-ported by the developing dialog.
The facility has beenimplemented in a fully general way, so that any test thatcan be coded in PROLOG can be the trigger for retraction.Currently we use this capability to retract certain infer-ences which result in a database call with no answers.This facilitates better dialog processing.
If the query Doany flights from Boston to Denver serve dinner?
is an-swered by a list of one or more such dinner flights, thepreferred antecedent of a subsequent reference to "thoseflights" is the set of those dinner flights.
In contrast,if the answer to the query is "no", a subsequent refer-ence to "those flights" refers to all flights from Bostonto Denver.
As explained in detail below, the ability toconditionally retract the inference nables our system tocorrectly identify the preferred antecedent in both cases.In addition, this capability simplifies our system's pro-cessing of yes/no questions.
The same inference ruleapplies to both flights serving dinner and Do any flightsserve dinner; i.e., the rule makes no provision for dis-tinguishing between these two contexts.
Yet when theyare embedded in a dialog, there are some differences.
Ifa query such as Show me flights from Boston to Denverserving dinner revealed that there were no such flights,1A decomposition i  PUNDIT is a frame-fike structure createdfor most nominal concepts and for events~ processes and states.89a subsequent query about "those flights" would seemrather odd.
In contrast, as shown in the preceding para-graph, such a subsequent query can follow the yes/noquestion quite naturally.The detailed processing of the example queryDo any flights from Boston to Denver serve dinner?proceeds as follows.
First a rule relating the decompo-sitions for "flights" and "serve" causes the meals slot ofthe flight_C decomposition to be instantiated:flight_C(flight2, source(boston),goal(denver),meals (d inner l )  .
.
.
.
)Then a database request is made for all dinner flightsfrom Boston to Denver.
If there are any, the flight_Cdecomposition as modified by the inference is retained;i.e., the context is left with a concept of the dinner flightsfrom Boston to Denver.
If there are no such flights, theinference is retracted, leaving in the context a concept of(all) th.e flights from Boston to Denver.
In both cases,the correct concept is made available for subsequent ref-erence resolution.2.2.
Implicit Reference ResolutionThe pragmatics component in PUNDIT takes care of ex-plicit reference resolution (\[3\]), as in What is the cost ofthose flights?
But there are many cases where the ref-erence to be resolved is implicit.
A second extension toour system handles implicit reference resolution as in thefollowing pair of queries:Show me morning flights from Boston to Washington.Show me afternoon flights.We have implemented an AWIS-specific heuristic whichaddresses this need.
It is invoked when our system isattempting to produce a database request for flights orfares but cannot find either the origin or destination (orboth) in the current utterance.
The heuristic allows thesystem to broaden its search for this information to ear-lier inputs.
We have circumscribed this search in order tolimit incorrect inferences; currently the heuristic worksonly in the following restricted manner:The system finds the most recent flight entity in the dis-course context other than the one explicitly involved inthe current request, and checks that this entity satisfiestwo conditions:a.
If any origin or destination information is knownabout the current flights, the candidate ntity must haveno conflicting origin or destination information.
So, forexample, if a dialog proceedsShow me flights from Boston to Philadelphia.Show me the earliest flight from Boston.the condition will be satisfied and the heuristic will ap-ply, whereas forShow me flights from Boston to Philadelphia.Show me the earliest flight leaving Philadelphia.it will not apply.
Unfortunately, the heuristic currentlywill not apply in the following case, either:Show me flights from Boston to Philadelphia.Show me flights to Pittsburgh.It would not be hard to refine the heuristic to apply tothe above sequence, but we have not done so.b.
The query giving rise to the candidate entity musthave been successfully processed, and must have receiveda non-null response.
By successfully processed, we meanthat a database request was made for the query.
The sys-tem cannot tell, of course, if it was the correct request.But if no request was made, that is evidence that theearlier query either was not properly understood or thatit was flawed in some way, and that it would be danger-ous to use the candidate ntity as a referent.
Given thefact that our system currently fails to create databaserequests for over one-third of its inputs, taking this con-servative approach turns out to be well-justified.The requirement that the database request produce anon-null response is needed for cases such as:Show me afternoon flights from Boston to San Francisco.\[there aren't any\]Show me flights on wide-body aircraft.If the heuristic applied, it would create a request forafternoon flights from Boston to San Francisco on wide-body aircraft, and obviously none would be found.If the candidate ntity satisfies both of the above condi-tions, the non-conflicting properties of the current (ori-gin or destination-deficient) entity and the candidate n-tity are merged.
Thus for the pair of queriesShow me morning flights from Boston to San Francisco.Show me flights on wide-body aircraft.90our system generates a request for morning flights fromBoston to San Francisco on wide-body aircraft.
How-ever, forShow me flights from Boston to San Francisco that servelunch.Show me dinner flights.it asks for flights from Boston to San Francisco that servedinner, not flights that serve both lunch and dinner.If the candidate ntity fails to satisfy both conditions,the heuristic simply fails; no other candidate ntities areconsidered.On the basis of training data, we predicted that the im-plicit reference resolution heuristic would apply to about15 percent of discourse-dependent ut erances.
The re-cent benchmark test showed that our heuristic was morerelevant han we expected, although it also turned outto be somewhat more error-prone.
On the February1992 natural language test, it resulted in the produc-tion of a database request for 48 class D utterances thatotherwise would have been unanswered.
38 of these re-quests obtained the correct answer, 10 did not, so thatthe heuristic produced a net improvement in our overallscore in spite of its unfortunately high level of errors.
(Italso was invoked on 4 class A queries, presumably inap-propriately, although one of the four ended up with thecorrect answer!)
There were 285 class D utterances inthe test, so the heuristic was invoked for 16.8 percent ofthem.
In addition, there was an undetermined numberof other utterances for which it could have been invokedif our system had successfully processed the appropriateantecedent utterances.2 .3 .
Database  Query  ParaphrasesWe have added a database query paraphrase capabilityto our ATIS system, which is used as follows.When a database query is created by the system and theresponse received from the database, the query as wellas the response is passed to an output formatting rou-tine.
At first this routine merely formatted the tabularresponse, but it turned out to be difficult for users to no-tice if the table displayed to them contained the desiredinformation or not.
Some of the time the system wouldmis-interpret the user's query, but the misinterpretationwould go undetected.
For example, a request for flightsfrom Boston to Pittsburgh on Monday may have resultedin a table of all flights from Boston to Pittsburgh, and ofa large number of flights, perhaps only one did not oper-ate on Mondays.
To address this shortcoming, we imple-mented a query paraphraser.
The database query, afterall, encodes what is actually retrieved from the database,so if we label the output table with a description of whatit contains, any discrepancy between what the user re-quested and what the system provided can be spottedmore easily.
And in the majority of cases, when the sys-tem provided the desired output, the paraphrase servedas a useful header to the table.
For example, for theinput sentenceShow me round-trip fares for flights from Boston to Den-ver leaving on Sunday.the following paraphrase is produced.Displaying:Fare(s ) :- round- t r ip- on  Sunday- fo r  F l ights :- f rom Boston- to  DenverAs can be seen from the above example, the paraphraseis not in sentence form, but in a stylized form that is easyto read and understand.
Sometimes it gives useful feed-back concerning the system's interpretation of imprecisequeries by the user, as in:I need a flight from Boston to Pittsburgh that leaves earlyin the morning.Disp lay ing  :F l ight (s )  :- depar t ing  be fore  8 am- f rom Boston- to  P i t t sburghAnd when an error is made, the user can notice it easily,particularly if told to check the easy-to-read paraphrasesfor missing conditions, as in this example from the Oc-tober 1991 dry run test:I want to travel from Atlanta to Baltimore early in themorning first flight.Disp lay ing :F l ight (s ) :- f rom At lanta- to  Ba l t imoreAs in the above example, even though the system misin-terprets the user's query, sometimes the desired answercan be gotten from the response produced, particularlywith the guidance provided by the paraphrase of the re-sponse.
In this way the system as a whole becomes more91capable of assisting the user in reaching a successful con-clusion to the travel planning task.3.
BENCHMARK EVALUATION" Paramax undertook the February 1992 ATIS benchmarkevaluation tests, with the cooperation of BBN, who pro-vided us with speech recognizer output as described be-low.
Our results were, in a word, disappointing.
Onecomponent of the set of factors leading to this level ofperformance was our concentration on improvements oour system which were of applicability to spoken lan-guage understanding systems in general, as opposed tospecific features that would only be applicable to theATIS domain.
But it is clear from the experience ofthis test that domain-specific features will have to be in-cluded if we are to perform in the ATIS domain.
While weknew we were underemphasizing such features, we weresomewhat aken by surprise since as recently as threemonths ago our level of performance had been compara-ble with other sites.This latest test has not only gotten us thinking about theperformance l vel of our system, but also about the sub-ject of evaluation in general.
The DARPA Spoken Lan-guage Understanding program, along with the MUC ef-fort, has made significant advances in the state of the artof evaluation of language understanding systems (\[4\]).Particularly for the natural language community, theseadvances have given a new objectivity to the assessmentof the level of achievement of their systems.
Tests suchas the ATIS benchmarks allow participants to find outhow well their systems perform on suites of hundreds ofutterances, not only in the simple absolute sense, but inrelation to other similar systems.The benchmark tests as administered, however, fall shortof quantifying progress in any satisfactory sense.
Onemight be tempted to claim that a history of scores on aseries of similar tests gives an indication of progress orthe lack thereof.
It appears, however, that the only sensein which this might be meaningful is if one comparesrelative performance of different systems from one testto another.
For example, our system, as remarked above,performed comparably to a number of other systems inpast tests, yet did poorly in relation to those systems onthis test.
Yet that tells us nothing about whether oursystem improved, stagnated, or degraded.The reason that the current common evaluationparadigm fails to quantify progress over time is that thetest data varies from test to test.
This variability tendsto lessen the reliability of comparisons between systemperformance on different evaluations.
That is, it is dif-ficult to interpret variations in a single system's perfor-mance over time because we cannot quantify the effectof accidental differences in the particular test data thathappens to be selected for a particular evaluation.
Fur-thermore, the test paradigm has undergone a number ofchanges which make it difficult to compare results fromevaluation to evaluation.
For example, in June 1990, thetest data included only Class A utterances.
In February1991, Class D1 utterances were included, but in a sep-arate test.
In the February 1992 test, Class A, D, andX utterarices were all included together.
In addition, inthe current test scoring is being conducted under themin /max rules (\[5\]).
All of these differences contributeto lessening the reliability of comparisons.We have experimented with a more tightly controlledvariation of the common evaluation metric in which thesame test data is processed by several different versionsof our system - the current version, and two older ver-sions.
These older versions correspond generally to thesystem which was reported on in February 1991 (\[1\]) andthe system which we used to participate in an informalevaluation in October 1991.
2 By holding the data con-stant and varying the system, we eliminate the effect ofdata variations on scores.
Furthermore, by comparingscores produced this way with the scores our system hasreceived on the standard evaluations, we demonstratethat variations in the test data are a real concern, be-cause we see a much less consistent pattern of develop-ment over time with the standard evaluation.
Figure 1shows how the scores on the February 1992 natural lan-guage test for the three different versions of the Paramaxsystem varied.Perfonnance of 3 Pararnax Systems, 2i9210090-80-70-6C ~50-4@30-20-10"NA T F 'Weighted Error[ ~ Feb 91 ~\ ]  Oct 91 IBM Feb 92 \]Figure 1: Comparative performance of systems.From Figure 1 we can see that our system has made mod-2The database  revision released in May 1991 compl icated mat -ters somewhat .
As we have reported elsewhere, our sys tem hasseparate modules  for natura l  language processing (PUNDIT) anddatabase  query generat ion (QTIP).
We were forced to use the Oc-tober  1991 QTIP with the February  1991 PUNDIT.
Thus  the per-formance labelled "February 1991" is really an overest imate,  towhatever  degree QTIP improved in that  t ime.92est improvements over time, as shown by the decrease inweighted error rate.
Additionally, the percentage of cor-rectly answered (T) inputs has increased, while the per-centage of unanswered (NA) inputs has decreased andthe percentage of incorrectly answered (F) inputs hasremained nearly constant.
In contrast, if we compareour scores on the October dry run with the February1992 benchmark test, we find that our system obtaineda weighted error score of 64.1% on the October dry runwhen all utterances in classes A, D1, and D were consid-ered.
For the February benchmark, the correspondingfigure was 66.7%.
Breaking this down more finely, ourclass D error decreased from 97.9% to 83.9%, while ourclass A error increased from 47.4% to 54.5%.
From thiswe might have concluded that our class D performanceimproved at the expense of our class A performance andoverall system performance.80757065~ 60~ so454O3OFigure 2:Pararnax Class A PerformanceJun 90 Feb 91 Oct 91 Feb 92EvaluationsComparative performance on class A tests.Focusing only on class A utterances, we can go back far-ther in time, finding (Figure 2) that our system's errorconsistently decreased until the most recent test, whichwould reinforce the hypothesis that our class A perfor-mance degraded recently.
Yet we see from Figure 3,which elaborates upon the information in Figure 1, thatsuch a conclusion is unwarranted.By running the same test with different versions of theunderstanding system, as described above, we obtain im-portant information on the changes over time in systemperformance.
This simple extension of the evaluationmethodology already in place supplements comparisonsbetween systems from different sites and comparisons be-tween different ests with clear-cut documentation f theprogress made by an individual site on its system.
Assuch, it is a valuable tool to add to the ever-increasingarsenal of objective system evaluation techniques.Table 1 summarizes our scores on the February 1992natural anguage and spoken language benchmark tests.The SLS results were obtained by filtering nbest out-95"90"85"~ sem 75"6O-5GClass A and D Performance of 3 SystemsFeb 91 Oct 91 Feb 92Systemsl -~- Class A quod0s ~ Class D quedes IFigure 3: System performance on classes A and D.put from BBN's speech recognition system through theParamax natural language processing system, using anN of 6.
This SLS score is only five percentage pointsinferior to our NL score.
The corresponding differencefor other sites ranged from ten to thirty-six points, withno correlation between actual scores and this difference.At this time we have no explanation for this intriguingphenomenon.
3WeightedTest T F NA ErrorNL 331 102 254 66.7%SLS 322 128 237 71.8%Table 1: Paramax 1992 ATIS benchmark results.3.1.
Speech  Recogn i t ionThe speech recognition scores which Paramax submittedfor this evaluation were produced by using the Paramaxnatural language processing system to filter the n-bestoutput from BBN's speech recognition system.
The n-best output used in this evaluation had a word error3The Paramax scores reported in this paper for the February1992 benchmark tests are not the same as those disseminated byNIST.
The NIST comparator  is not guaranteed to score all utter-ances correctly.
It first computes an estimate of the difficulty itwill have in scoring an utterance, and if this estimate is too high, ascore of F is automatical ly assigned, even though the answer maybe correct according to the rules of ra in /max scoring.
The diffi-culty parameter  is R!/(R-H)!,  where R is the number  of columnsin the maximal answer, and H is the number  of columns in thesystem's answer.
This figure must  be less than 3 * 105.
Thus, forexample, if the maximal  answer has 15 columns, no more than 5of them can appear in the system's answer.
20 of our answers onboth the natural  anguage test and the spoken language test weresubject o this phenomenon.
It is NIST's belief that no other sitewas similarly affected on more than 2 utterances, and they havegiven us permission to present scores which have been adjusted toaccount for comparator  errors.93rate of 10.7%.
N was set to 6 for this test.
The nat-ural language system selected the first candidate in then-best which passed its syntactic, semantic, and appli-cation constraints, and output this candidate as the rec-ognized utterance.
If no candidate passed the naturallanguage constraints, the first candidate of the n-bestwas output as the recognized utterance.
After naturallanguage filtering, the official speech recognition scorewas 10.6%.
Although intuitively, natural anguage con-straints should be able to reduce speech recognition er-ror, they do not appear to do so in this case.
There are atleast three possible reasons for this outcome.
One, thespeech recognition is already quite good, consequentlythere is less room for improvement.
Two, the naturallanguage processing is not very good.
Three, there isalways going to be some residue of speech recognizer er-rors which result in perfectly reasonable recognized ut-terances, and no amount of natural anguage knowledgewill be able to correct hese.
We have not explored thesehypotheses in detail; however, we have done a relatedexperiment with n-best data of a totally different kind,which shows a remarkably similar pattern.
We brieflydescribe this experiment in the following section.3 .2 .
Opt ica l  Character  Recogn i t ionAlthough the nbest architecture was developed in thecontext, of spoken language understanding, it is in factapplicable to any kind of input where indeterminaciesin the input result in misrecognitions.
In addition tospeech recognizers, optical character recognition systems(OCR's) also have this property.
Although current OCRtechnology is quite accurate for relatively clean data, ac-curacy is greatly reduced as the data becomes less clean.For example, faxed documents tend to have a high OCRerror rate.
Many OCR errors result in output which ismeaningless, either because the output words are not le-gitimate words of the language, or because the outputsentences do not make sense.
We have applied linguis-tic constraints to the OCR problem using a variationof the N-best interface with a natural anguage process-ing system.
Because the OCR produces only one alter-native, an "alternative generator" was developed whichuses spelling correction, a lexicon, and various scoringmetrics to generate a list of alternatives from raw OCRoutput.
Just as in the case of speech recognizer output,alternatives are sent to the natural language processingsystem, which selects the first candidate which passes itsconstraints.The system was tested with 120 sentences of ATIS dataon which the natural language system had previouslybeen trained.
The text data was faxed and then scanned.NIST speech recognition scoring software was used tocompute word error rates.
The word error rate for out-put directly from the OCR was 13.1%.
After the outputwas sent through the spelling corrector, it was scored onthe basis of the first candidate of the N-best set of al-ternatives.
The error rate was reduced to 4.6%.
Finally,the output from the natural anguage system was scored,resulting in a final average rror rate of 4.2%.Sending the uncorrected OCR output directly into thenatural language system for processing without correc-tion led to a 73% average weighted error rate.
Spellingcorrection improved the error rates to 33%.
Finally, withnatural anguage correction, the weighted error rate im-proved to 28%.
Thus, although improvements in wordaccuracy were minimal, application accuracy was greatlyimproved.
This is consistent with previous experimentswe have done on speech recognizer output \[6\].
Additionaldetail on this experiment can be found in \[7\].Interestingly, training on the OCR data led to a process-ing time improvement.
Comparing the performance ofthe October system and the February system, we foundthat total cpu time for processing the February test datawas reduced by one-third.
This improvement was dueto improving processing inefficiencies which were notedduring analysis of the processing of the OCR data.
It isencouraging that the system is sufficiently general thattraining on OCR data can improve the processing of nat-ural language and spoken language data.Re ferences1.
L. M. Norton, M. C. Linebarger, D. A. DaM, andN.
Nguyen, "Augmented role filling capabilities for se-mantic interpretation f natural language," in Proceedingsof the DARPA Speech and Language Workshop, (PacificGrove, CA), February 1991.2.
M. L. Ginsberg, ed., Readings in Nonmonotonie Reason-ing.
Los Altos, California: Morgan Kaufmann, 1987.3.
D. A. DaM and C. N. Ball, "Reference resolution in PUN-DIT," in P. Saint-Dizier and S. Szpakowicz, eds., (Logicand logic grammars for language processing).
Ellis Hor-wood Limited, 1990.4.
D. A. DaM, D. Appelt, and C. Weir, "Multi-site naturallanguage processing evaluation: MUC and ATIS," in Pro-ceedings o\] the Workshop on Natural Language ProcessingEvaluation, (Berkeley, CA), June 1991.5.
MADCOW, "Multi-site data collection for a spoken lan-guage corpus," in Proceedings of the DARPA Speech andNatural Language Workshop, Morgan Kaufmann, Feb.1992.6.
D. A. Dahl, L. Hirschman, L. M. Norton, M. C.Linebarger, D. Magerman, and C. N. Ball, "Training andevaluation of a spoken language understanding system,"in Proceedings of the DARPA Speech and Language Work-shop, (Hidden Valley, PA), June 1990.7.
D. A. Dahl, S. L. Taylor, L. M. Norton, and M. C.Linebarger, "Using natural language processing to im-prove OCR output," Tech.
report, Paramax Systems Cor-poration.
In preparation.94
