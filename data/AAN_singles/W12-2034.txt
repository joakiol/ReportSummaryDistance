The 7th Workshop on the Innovative Use of NLP for Building Educational Applications, pages 289?294,Montre?al, Canada, June 3-8, 2012. c?2012 Association for Computational LinguisticsMemory-based text correction for preposition and determiner errorsAntal van den BoschRadboud University NijmegenP.O.
Box 9103NL-6500 HD Nijmegen, The Netherlandsa.vandenbosch@let.ru.nlPeter BerckTilburg UniversityP.O.
Box 90153NL-5000 LE Tilburg, The Netherlandsp.j.berck@tilburguniversity.eduAbstractWe describe the Valkuil.net team entry for theHOO 2012 Shared Task.
Our systems consistsof four memory-based classifiers that generatecorrection suggestions for middle positions insmall text windows of two words to the leftand to the right.
Trained on the Google 1TB 5-gram corpus, the first two classifiers determinethe presence of a determiner or a prepositionbetween all words in a text in which the actualdeterminers and prepositions are masked.
Thesecond pair of classifiers determines which isthe most likely correction given a masked de-terminer or preposition.
The hyperparametersthat govern the classifiers are optimized onthe shared task training data.
We point out anumber of obvious improvements to boost themedium-level scores attained by the system.1 IntroductionOur Valkuil.net team entry, known under the abbre-viation ?VA?
in the HOO 2012 Shared Task (Daleet al, 2012), is a simplistic text correction systembased on four memory-based classifiers.
The goal ofthe system is to be lightweight: simple to set up andtrain, fast in execution.
It requires a (preferably verylarge) corpus to train on, and a closed list of wordswhich together form the category of interest?in theHOO 2012 Shared Task context, the two categoriesof interest are prepositions and determiners.As a corpus we used the Google 1TB 5-gram cor-pus (Brants and Franz, 2006), and we used two lists,one consisting of 47 prepositions and one consist-ing of 24 determiners, both extracted from the HOO2012 Shared Task training data.
Using the Googlecorpus means that we restricted ourselves to a sim-ple 5-gram context, which obviously places a limiton the context sensitivity of our system; yet, we wereable to make use of the entire Google corpus.Memory-based classifiers have been used for con-fusible disambiguation (Van den Bosch, 2006) andagreement error detection (Stehouwer and Van denBosch, 2009).1 In both studies it is argued thatfast approximations of memory-based discrimina-tive classifiers are effective and efficient modules forspelling correction, particularly because of their in-sensitivity to the number of classes to be predicted.They can act as simple binary decision makers (e.g.for confusible pairs: given this context, is then orthan more likely?
), and at the same time they canhandle missing word prediction with up to millionsof possible outcomes, all in the same model.
Vanden Bosch (2006) also showed consistent log-linearperformance gains in learning curve experiments,indicating that more training data continues to bebetter for these models even at very large amountsof training data.
The interested reader is referred tothe two studies for more details.2 SystemOur system centers around four classifiers that alltake a windowed input of two words to the left ofthe focus, and two words to the right.
The focusmay either be a position between two words, or adeterminer or a preposition.
In case of a position1A working context-sensitive spelling checker for Dutchbased on these studies is released under the name Valkuil.net;see http://valkuil.net ?
hence the team name.289preposition?
determiner?which preposition?
which determiner?no preposition no determinerno noyesyespreposition determinerFigure 1: System architecture.
Shaded rectangles are the four classifiers.between two words, the task is to predict whetherthe position should actually be filled by a prepositionor a determiner.
When the focus is on a determineror preposition, the task may be to decide whether itshould actually be deleted, or whether it should bereplaced.The main system architecture is displayed in Fig-ure 1.
The classifiers are the shaded rectangularboxes.
They are all based on IGTree, an efficientdecision tree learner (Daelemans et al, 1997), a fastapproximation of memory-based or k-nearest neigh-bor classification, implemented within the TiMBL2software package (Daelemans et al, 2010).The first two classifiers, preposition?
and de-terminer?, are binary classifiers that determinewhether or not there should be a preposition or a de-terminer, respectively, between two words to the leftand two words to the right:?
The preposition?
classifier is trained on all118,105,582 positive cases of contexts in theGoogle 1 TB 5-gram corpus in which one of the47 known prepositions are found to occur in themiddle position of a 5-gram.
To enable the clas-sifier to answer negatively to other contexts,roughly the same amount of negative cases ofrandomly selected contexts with no prepositionin the middle are added to form a training setof 235,730,253 cases.
In the participating sys-2http://ilk.uvt.nl/timbltem we take each n-gram as a single token, andignore the Google corpus token counts.
Weperformed a validation experiment on a single90%-10% split of the training data; the classi-fier is able to make a correct decision on 89.1%of the 10% heldout cases.?
Analogously, the determiner?
classifier takesall 132,483,802 positive cases of 5-grams witha determiner in the middle position, and addsrandomly selected negative cases to arrive at atraining set of 252,634,322 cases.
On a 90%?10% split, the classifier makes the correct deci-sion in 88.4% of the 10% heldout cases.The second pair of classifiers perform the multi-label classification task of predicting which preposi-tion or determiner is most likely given a context oftwo words to the left and to the right.
Again, theseclassifiers are trained on the entire Google 1TB 5-gram corpus:?
The which preposition?
classifier is trained onthe aforementioned 118,105,582 cases of anyof the 47 prepositions occurring in the middleof 5-grams.
The task of the classifier is to gen-erate a class distribution of likely prepositionsgiven an input of the four words surroundingthe preposition, with 47 possible outcomes.
Ina 90%-10% split experiment on the completetraining set, this classifier labels 59.6% of the10% heldout cases correctly.290?
The which determiner?
classifier, by analogy,is trained on the 132,483,802 positive cases of5-grams with a determiner in the middle po-sition, and generates class distributions com-posed of the 24 possible class labels (the pos-sible determiners).
On a 90%-10% split of thetraining set, the classifier predicts 63.1% of allheldout cases correctly.Using the four classifiers and the system architec-ture depicted in Figure 1, the system is capable ofdetecting missing and unnecessary cases of preposi-tions and determiners, and of replacing prepositionsand determiners by other more likely alternatives.Focusing on the preposition half of the system, weillustrate how these three types of error detection andcorrection are carried out.First, Figure 2 illustrates how a missing preposi-tion is detected.
Given an input text, a four-wordwindow of two words to the left and two words to theright is shifted over all words.
At any word which isnot in the list of prepositions, the binary preposi-tion?
classifier is asked to determine whether thereshould be a preposition in the middle.
If the classi-fier says no, the window is shifted to the next posi-tion and nothing happens.
If the classifier says yesbeyond a certainty threshold (more on this in Sec-tion 3), the which preposition?
classifier is invokedto make a best guess on which preposition should beinserted.preposition?which preposition?no prepositionnoyesprepositionmissingprepositionsuggestionFigure 2: Workflow for detecting a missing preposition.Second, Figure 3 depicts the workflow of how apreposition deletion is suggested.
Given an inputtext, all cases of prepositions are sought.
Instancesof two words to the left and right of each preposi-tion are created, and these context windows are pre-sented to the preposition?
classifier.
If this classi-fier says no beyond a certainty threshold, the systemsignals that the preposition currently in focus shouldbe deleted.preposition?which preposition?no prepositionnoyesprepositionsuggesteddeletion ofprepositionFigure 3: Workflow for suggesting a preposition deletion.Third, Figure 4 illustrates how a replacement sug-gestion is generated.
Just as with the detection ofdeletions, an input text is scanned for all occurrencesof prepositions.
Again, contextual windows of twowords to the left and right of each found preposi-tion are created.
These contexts are presented to thewhich preposition?
classifier, which may produce adifferent most likely preposition (beyond a certaintythreshold) than the preposition in the text.
If so, thesystem signals that the original preposition shouldbe replaced by the new best guess.Practically, the system is set up as a master pro-cess (implemented in Python) that communicateswith the four classifiers over socket connections.The master process performs all necessary data con-version and writes its edits to the designated XMLformat.
First, missing prepositions and determin-ers are traced according to the procedure sketchedabove; second, the classifiers are employed to findreplacement errors; third, unnecessary determinersand prepositions are sought.
The system does notiterate over its own output.291preposition?which preposition?no prepositionnoyesprepositionsuggestedreplacementof prepositiondifferent?Figure 4: Workflow for suggesting a preposition replace-ment.3 Optimizing the systemWhen run unfiltered, the four classifiers tend to over-predict errors massively.
They are not very accurate(the binary classifiers operate at a classification ac-curacy of 88?89%; the multi-valued classifiers per-form at 60?63%).
On the other hand, they produceclass distributions that have properties that could beexploited to filter the classifications down to caseswhere the system is more certain.
This enables usto tune the precision and recall behavior of the clas-sifiers, and, for instance, optimize on F-Score.
Weintroduce five hyperparameter thresholds by whichwe can tune our four classifiers.First we introduce two thresholds for the two bi-nary classifiers preposition?
and determiner?
:M ?
When the two binary preposition?
and de-terminer?
classifiers are used for detectingmissing prepositions or determiners, the posi-tive class must be M times more likely than thenegative class.U ?
In the opposite case, when the two binary clas-sifiers are used for signalling the deletion of anunnecessary preposition or determiner, the neg-ative class must be U times more likely than thepositive class.For the two multi-label classifiers which prepo-sition?
and which determiner?
we introduce threeOptimizing onTask Thresh.
Precision Recall F-ScorePrep.
M 30 10 20U 30 4 4DS 5 50 50F 50 5 5R 10 20 20Det.
M 30 10 20U 30 2 2DS 5 50 20F 50 5 20R 10 20 20Table 1: Semi-automatically established thresholds thatoptimize precision, recall, and F-Score.
Optimizationwas performed on the HOO 2012 Shared Task trainingdata.thresholds (which again can be set separately for de-terminers and prepositions):DS ?
the distribution size (i.e.
the number of la-bels that have a non-zero likelihood accordingto the classifier) must be smaller than DS.
Alarge DS signals a relatively large uncertainty.F ?
the frequency of occurrence of the most likelyoutcome in the training set must be larger thanF .
Outcomes with a smaller number of occur-rences should be distrusted more.R ?
if the most likely outcome is different from thepreposition or determiner currently in the text,the most likely outcome should be at least Rtimes more likely than the current prepositionor determiner.
Preferably the likelihood of thelatter should be zero.On the gold training data provided during thetraining phase of the HOO 2012 Shared Task wefound, through a semi-automatic optimization pro-cedure, three settings that optimized precision, re-call, and F-Score, respectively.
Table 3 displays theoptimal settings found.
The results given in Sec-tion 4 always refer to the system optimized on F-Score, listed in the rightmost column of Table 3.The table shows that most of the ratio thresholdsfound to optimize F-Score are quite high; for ex-ample, the preposition?
classifier needs to assign292a likelihood to a positive classification that is at least20 times more likely than the negative classificationin order to trigger a missing preposition error.
Thethreshold for marking unnecessary prepositions isconsiderably lower at 4, and even at 2 for determin-ers.4 ResultsThe output of our system on the data provided dur-ing the test phase of the HOO 2012 Shared Task wasprocessed through the shared task evaluation soft-ware.
The original test data was revised in a correc-tion round in which a subset of the participants couldsuggest corrections to the gold standard.
We did notcontribute suggestions for revisions, but our scoresslightly improved after revisions.
Table 4 summa-rizes the best scores of our system optimized on F-Score, before and after revisions.
Our best score isan overall F-Score of 14.24 on error detection, af-ter revisions.
Our system performs slightly better onprepositions than on determiners, although the dif-ferences are small.
Optimizing on F-Score impliesthat a reasonable balance is found between recalland precision, but overall our results are not impres-sive, especially not in terms of correction.5 DiscussionWe presented a preposition and determiner error de-tection and correction system, the focus task of theHOO 2012 Shared Task.
Our system consists offour memory-based classifiers and a master processthat communicates with these classifiers in a simpleworkflow.
It takes several hours to train our systemon the Google 1TB 5-gram corpus, and it takes in theorder of minutes to process the 1,000 training doc-uments.
The system can be trained without need-ing linguistic knowledge or the explicit computationof linguistic analysis levels such as POS-tagging orsyntactic analyses, and is to a large extent language-independent (it does rely on tokenization).This simple generic approach leads to mediocreresults, however.
There is room for improvement.We have experimented with incorporating the n-gram counts in the Google corpus in our classi-fiers, leading to improved recall (post-competition).It still remains to be seen if the Google corpus isthe best corpus for this task, or for the particu-lar English-as-a-second-language writer data usedin the HOO 2012 Shared Task.
Another likely im-provement would be to limit which words get cor-rected by which other words based on confusionstatistics in the training data: for instance, the train-ing data may tell that ?my?
should rarely, if ever, becorrected into ?your?, but our system is blind to suchlikelihoods.AcknowledgementsThe authors thank Ko van der Sloot for his continuedimprovements of the TiMBL software.
This work isrooted in earlier joint work funded through a grantfrom the Netherlands Organization for Scientific Re-search (NWO) for the Vici project Implicit Linguis-tics.ReferencesT.
Brants and A. Franz.
2006.
LDC2006T13: Web 1T5-gram Version 1.W.
Daelemans, A.
Van den Bosch, and A. Weijters.
1997.IGTree: using trees for compression and classificationin lazy learning algorithms.
Artificial Intelligence Re-view, 11:407?423.W.
Daelemans, J. Zavrel, K. Van der Sloot, and A. Vanden Bosch.
2010.
TiMBL: Tilburg memory basedlearner, version 6.3, reference guide.
Technical ReportILK 10-01, ILK Research Group, Tilburg University.R.
Dale, I. Anisimoff, and G. Narroway.
2012.
HOO2012: A report on the preposition and determiner errorcorrection shared task.
In Proceedings of the SeventhWorkshop on Innovative Use of NLP for Building Ed-ucational Applications, Montreal, Canada.H.
Stehouwer and A.
Van den Bosch.
2009.
Putting thet where it belongs: Solving a confusion problem inDutch.
In S. Verberne, H. van Halteren, and P.-A.
Cop-pen, editors, Computational Linguistics in the Nether-lands 2007: Selected Papers from the 18th CLIN Meet-ing, pages 21?36, Nijmegen, The Netherlands.A.
Van den Bosch.
2006.
All-word prediction as theultimate confusible disambiguation.
In Proceedings ofthe HLT-NAACL Workshop on Computationally hardproblems and joint inference in speech and languageprocessing, New York, NY.293Before revisions After revisionsTask Evaluation Precision Recall F-Score Precision Recall F-ScoreOverall Detection 12.5 15.23 13.73 13.22 15.43 14.24Recognition 10.87 13.25 11.94 11.59 13.53 12.49Correction 6.16 7.51 6.77 7.25 8.46 7.8Prepositions Detection 13.44 14.41 13.91 14.23 14.75 14.49Recognition 11.46 12.29 11.86 12.65 13.11 12.88Correction 7.51 8.05 7.77 8.7 9.02 8.85Determiners Detection 11.04 15.21 12.79 11.71 15.28 13.26Recognition 10.37 14.29 12.02 10.7 13.97 12.12Correction 5.02 6.91 5.81 6.02 7.86 6.82Table 2: Best scores of our system before (left) and after (right) revisions.
Scores are reported at the overall level (top),on prepositions (middle), and determiners (bottom).294
