Proceedings of the 2011 Workshop on Biomedical Natural Language Processing, ACL-HLT 2011, pages 19?27,Portland, Oregon, USA, June 23-24, 2011. c?2011 Association for Computational LinguisticsLearning Phenotype Mapping for Integrating Large Genetic DataChun-Nan Hsu1,2,?, Cheng-Ju Kuo2, Congxing Cai1Sarah A. Pendergrass3, Marylyn D. Ritchie3,4 and Jose Luis Ambite11USC Information Sciences Institute, Marina del Rey, CA, USA2Institute of Information Sciences, Academia Sinica, Taipei, Taiwan3Center for Human Genetics Research, 4Dept.
of Molecular Physiology andBiophysics, Vanderbilt University, Nashville, TN, USA?chunnan@isi.eduAbstractAccurate phenotype mapping will play an im-portant role in facilitating Phenome-Wide As-sociation Studies (PheWAS), and potentiallyin other phenomics based studies.
The Phe-WAS approach investigates the association be-tween genetic variation and an extensive rangeof phenotypes in a high-throughput manner tobetter understand the impact of genetic varia-tions on multiple phenotypes.
Herein we de-fine the phenotype mapping problem posedby PheWAS analyses, discuss the challenges,and present a machine-learning solution.
Ourkey ideas include the use of weighted Jaccardfeatures and term augmentation by dictionarylookup.
When compared to string similaritymetric-based features, our approach improvesthe F-score from 0.59 to 0.73.
With augmenta-tion we show further improvement in F-scoreto 0.89.
For terms not covered by the dictio-nary, we use transitive closure inference andreach an F-score of 0.91, close to a level suffi-cient for practical use.
We also show that ourmodel generalizes well to phenotypes not usedin our training dataset.1 IntroductionThere is a wealth of biomedical data available inpublic and private repositories (e.g.
the databaseissue of Nucleic Acids Research (?).)
Along withthis explosion of information comes the need to inte-grate data from multiple sources to achieve sufficientstatistical power for analyses and/or to characterizephenomena more precisely.
This trend manifests it-self in two primary ways: the formation of largemulti-institution multi-study consortia and publicrepositories.
Although this situation occurs acrossmany areas of biomedicine and our techniques aregeneral, in this paper we will illustrate the ideas withexamples from genetic studies in which we are par-ticipating.Consider the National Center for Biotechnol-ogy Information (NCBI) database of Genotypesand Phenotypes (dbGaP) (www.ncbi.nlm.nih.gov/gap), that was developed to archive and dis-tribute the results of studies that have investigatedthe interaction of genotype and phenotype.
This is alarge repository that includes genome-wide associa-tion studies (GWAS), medical sequencing, molecu-lar diagnostic assays, as well as association betweengenotype and non-clinical traits.
Genetic studiesfunded by the National Institutes of Health (NIH)over a certain size are required to submit the ge-netic and phenotypic data to dbGaP.
There are over130 top-level studies, 1900 datasets, 5600 analyses,comprising about 125000 phenotypic variables.
Un-fortunately, each study uses its own set of variables,thus far dbGaP does not attempt to reconcile, matchor harmonize any of these variables.
For example,a variable called ?BMI?
in one study and ?BodyMass Index?
in another study are recorded asdifferent variables.
The task of matching or harmo-nizing these variables falls on each researcher thatobtains dbGaP data from multiple studies.Similarly, consider a large consortium, suchas the Population Architecture Using Genomicsand Epidemiology (PAGE) network.
PAGE(www.pagestudy.org) is a consortium of fourmajor studies with the goal of understanding the19association of genetic variants with complex dis-eases and traits across a variety of populations.
Thestudies that comprise PAGE include: the Women?sHealth Initiative (WHI, www.whiscience.org/); the Multiethnic Cohort (MEC,www.crch.org/multiethniccohort/,www.uscnorris.com/mecgenetics/); theCALiCo Consortium, comprised in turn of theAtherosclerosis Risk In Communities (ARIC) study(www.cscc.unc.edu/aric/), the CoronaryArtery Risk In Young Adults (CARDIA) study(www.cardia.dopm.uab.edu), the Cardio-vascular Heart Study (www.chs-nhlbi.org/),the Hispanic Community Health Study(www.cscc.unc.edu/hchs/), the StrongHeart Cohort Study, and the Strong Heart FamilyStudy (strongheart.ouhsc.edu/); and theEpidemiologic Architecture of Genes Linked toEnvironment (chgr.mc.vanderbilt.edu/eagle/) study, which utilizes genotypic and phe-notypic data from the National Health and NutritionExamination Surveys (NHANES) from the Centersfor Disease Control and Prevention (CDC).
Thestudies of PAGE represent a pool of over 200,000individuals with genotypic data collected acrossmultiple race/ethnicities, and an extremely diversecollection of phenotypic data.
Within PAGE thereare numerous analyses and writing groups thatfocus on specific diseases.
Each group selectsvariables relevant to their disease and harmonizesthe variables across studies.A group within PAGE is investigating a novelapproach to genetic association analysis called aPhenome Wide Association Studies (PheWAS) (?
).This is a different approach compared to the cur-rent paradigm of Genome Wide Association Stud-ies (GWAS) (?
; ?).
GWAS focus on calculatingthe association between the variation of hundredsof thousands of genotyped single nucleotide poly-morphisms (SNPs) and a single or small numberof phenotypes.
This approach has provided valu-able information about the contribution of geneticvariation to a wide range of diseases and pheno-types.
A common limitation of GWAS is the in-vestigation of a limited phenotypic domain.
In con-trast, PheWAS utilizes an extensive range of de-tailed phenotypic measurements including interme-diary biomarkers, in addition to prevalent and in-cident status for multiple common clinical condi-tions, risk factors, and quantitative traits for compre-hensively exploring the association between geneticvariations and all PheWAS phenotypes.
The inves-tigation of a broad range of phenotypes has the po-tential to identify pleiotropy, novel mechanistic in-sights fostering hypothesis generation, and to definea more complete picture of genetic variations andtheir impact on human diseases.In order to compare PheWAS results across stud-ies within PAGE to seek replication for significantgenotype/phenotype associations, an important stepis matching and mapping phenotypes across stud-ies.
As the number and range of phenotypes islarge across studies, manually matching phenotypesis less than ideal.
Therefore, an important step in im-proving the feasibility of PheWAS studies is to usecomputational approaches to map phenotypes acrossstudies, effectively matching related phenotypes.Definition Phenotype Mapping is the task of assign-ing every variable from each participating study toone out of a set of categories.
The categories can bedefined for a given integrated study or consortium,or can be taken from pre-existing ontologies, suchas PhenX (www.phenx.org).For one example, consider the variable hyptfrom WHI which is described by the text?Hypertension ever?
and the variableHAE5A from the EAGLE study described by thetext ?Now taking prescribed medicinefor HBP?.
To manually match these phenotypes,a human expert declares these two variables tobe relevant to class ?hypertension?.
Table 1shows additional examples.The phenotype mapping problem is quite chal-lenging.
First, the variable descriptions are quiteshort (around 10 words, often less).
Second, map-ping the variables to a category, such as hyperten-sion, may require significant background knowledge(HBP stands for High Blood Pressure, also knownas hypertension).
Third, there are large numbers ofvariables, so the solution needs to scale gracefully.In summary, in order to integrate data from publicrepositories, such as dbGaP, or from large consortia,such as the PAGE network, a critical task is to un-derstand how the available phenotypes relate to eachother.
In this paper, we present machine-learningtechniques for phenotype mapping that significantly20reduce the burden on researchers when integratingdata from multiple studies.2 Related WorkFrom the perspective of biomedical sciences, phe-notype mapping is a pre-requisite and a generaliza-tion for the task of phenotype harmonization (?).
Inharmonization, a single variable is identified or cal-culated for each phenotype within each study.
Thiscan only be accomplished for a very limited set ofvariables.
There is a need, however, to provideenough information on a much larger set of pheno-type variables so that researchers can determine thecommon denominator version of a measure acrossstudies.
For example, if a researcher is interestedin hypertension status as an outcome, there needsto be an assessment of how hypertension status wasascertained in each study.
Different approaches in-clude self-report, clinic-based blood pressure mea-surement and/or anti-hypertensive medication use.Only after this information is obtained, along withother information, such as at what visit was statusassessed and whether the variable is available forthe entire cohort or only a portion of it will the re-searcher be able to determine what to use in analysisand how to interpret the findings.
The phenotypemapping task that we address in this paper enablesa researcher to rapidly find all the phenotype vari-ables that are related to a given category, which thenconstitutes the input to the harmonization process.From the computer science perspective, the taskof phenotype mapping can be seen as an instance ofthe problem of entity linkage, which appears in a va-riety of forms across many contexts, namely recordlinkage (?
), object identification (?
), duplicate de-tection (?
), and coreference (?
; ?).
That is, the prob-lem of recognizing when multiple objects (in multi-ple sources) actually correspond to the same entity.Record linkage generally consists of three phases:(1) blocking, where the number of pairs of objectsis reduced, which is critical for large datasets (e.g.,(?
; ?
; ?
)), (2) field similarity, where the attributesof an object are compared (e.g., (?
; ?
; ?
; ?
; ?
), and(3) record similarity, which weights how differentattributes contribute to the similarity of records as awhole (e.g., (?
; ?)).
Machine learning techniques areused for many of these tasks.The task of phenotype mapping is related, but dif-fers from previous incarnations of record linkage.
Inour case, the variables are the objects to be mapped.However, the only attribute of an object is a tersetextual description (cf.
Table 1).
This makes theproblem harder since, as we will see, string simi-larity measures are not enough, and term expansionwith additional background knowledge is necessary.We do not consider blocking techniques in this pa-per, since the number of phenotypes is in the thou-sands and an exhaustive O(n2) comparison is stillfeasible.In this paper, we define and present an approach tophenotype mapping with good experimental perfor-mance, but there are many opportunities for refine-ment by incorporating additional techniques fromthe record linkage literature.3 Phenotype MappingFor the PAGE PheWAS study, phenotypes were firstmanually matched, through the creation of 106 phe-notype classes, in order to bring together relatedphenotypes across studies.
The following steps werethen used: First, the data from different studies werefiltered independently for any significant associa-tion results with p < 0.01.
Closely related phe-notypes were then matched up between studies andassigned to phenotype classes.
Finally, phenotypesfrom all studies, regardless of association results,were matched up to the already defined phenotypeclasses.
In this way, a phenotype that might nothave shown a significant association result for a sin-gle study, but that matched a phenotype class, wouldstill be added to the phenotype-class list.
To scale upthe process it is important to develop a semi or fullyautomatic approach for the task.Table 1 shows some example phenotypes andtheir classification.
Class labels were assigned whenwe manually matched the phenotypes.
The real IDof a phenotype in a study is given in column ID.Description will be the main clue for automaticmatching.
These examples were chosen to illustrateunique characteristics that we observed in the manu-ally matched data set and the challenges of the task.?
The descriptions are in a wide variety of forms.They may be a compound term, a phrase, a sen-tence, or even a question, and usually contain21Class Study ID DescriptionAllergy ARIC MHQA2A EVER TOLD HAD HAY FEVERAllergy ARIC MHQA2B STILL HAVE HAY FEVERAllergy EAGLEIII ALPBERFL Cat - flare length (mm)Allergy EAGLEIII ALPCATWL Cat - wheal length (mm)Allergy EAGLEIII ALPBERFL Cat - flare width (mm)Allergy EAGLEIII ALPCATWL Cat - wheal width (mm)Allergy MEC asthma History of Asthma, Hayfever, Skin Allergy,Food Allergy or Any Other Allergy fromBaseline QuestionnaireCigaretteSmokedPerDay ARIC HOM32 NUMBER OF CIGARETTES PER DAYCigaretteSmokedPerDay ARIC HOM35 OVERALL NUM OF CIGARETTES PER DAYCigaretteSmokedPerDay CHS AMOUNT CIGS SMOKED/DAYCigaretteSmokedPerDay WHI cigsday Smoke or smoked, cigarettes/dayHematocrit ARIC HMTA01 HEMATOCRITHematocrit EAGLEIII HTP Hematocrit (%)Hematocrit WHI hematocr Hematocrit (%)Hypertension ARIC HYPERT04 HYPERTENTION, DEFINITION 4Hypertension ARIC HOM10A HIGH BP EVER DIAGNOSEDHypertension CHS HYPER 1 CALCULATED HTN STATUSHypertension CHS HYPER 2 CALCULATED HTN STATUSHypertension CHS HYPER 3 CALCULATED HTN STATUSHypertension CHS HTNMED06 ANY HYPERTENTION MEDICATIONHypertension EAGLEIII HAE2 Doctor ever told had hypertension/HBPHypertension EAGLEIII HAE5A Now taking prescribed medicine for HBPHypertension MEC q2hibp History of High Blood Pressure from QX2Hypertension MEC hibp History of High Blood Pressure fromBaseline QuestionnaireHypertension WHI hypt f30 Hypertension everHypertension WHI htntrt f30 HypertensionSmoker ARIC CURSMK01 CURRENT CIGARETTE SMOKERSmoker CHS PRESSM PRESENT SMOKERSmoker WHI smoknow Smoke cigarettes nowTable 1: Example phenotypes and their classificationless than 10 words, so it is difficult to apply so-phisticated Natural Language Processing tech-niques.?
Phenotypes may be related in different ways:subsumption, overlapping, at the same layer ofsemantic hierarchy, etc.?
The granularity of the classes varies.
For exam-ple, we have classes as specifically defined asHematocrit, the ratio of the volume of redblood cells to the total volume of blood.
But theclass Allergy covers a wide range of allergysources and symptoms.
In Table 1, we showfour phenotype variables for allergies againstcats with flare and wheal sizes measured.
Sim-ilar variables include those for allergies of awide range of sources: alternaria, bermudagrass, german cockroach, mite, peanut, rag-weed, rye grass, Russian thistle, and white oak.While in the same class, MEC uses a single phe-notype asthma to cover just about all types ofallergies.
On the other hand, phenotypes aboutcigarette smoking are distinctively divided intotwo categories: cigarettes smoked per day andcurrently smoking.
As we explained earlier, themain criterion here is to maximize the chanceto detect unexpected associations, not necessar-ily to match the most semantically similar phe-notypes.
As a result, directly applying conven-tional clustering or topic modeling techniquesin Information Retrieval may not be appropri-ate here.?
Some phenotypes in the same class appearnearly identical.
For example, the three hemat-22ocrit phenotypes have almost identical descrip-tions.
HYPER 1, 2 and 3 of the study CHSin the class Hypertension have exactly thesame descriptions.
For those cases, apply-ing string similarity metrics can easily matchthem together.
However, some phenotypesin the same class appear completely differentdue to the use of synonyms and abbreviations.Again in class Hypertension, ?hyperten-sion,?
?HTN,?
?high blood pressure,?
?HBP,?
and?high BP?
are keywords appearing in the de-scriptions of phenotypes.
It is possible foran effective string similarity metric to recog-nize abbreviations like ?HTN?
for ?hyperten-sion,?
but without additional information thereis no way for a string similarity metric to match?hypertension?
and ?high blood pressure.
?4 MethodsWe formulate the task as a problem of learning toscore the degree of match of a pair of phenotypesbased on their descriptions.
By setting a thresholdof the score for match or not, the problem reduces toa standard binary classification problem in MachineLearning.We started by performing a pre-processing step ofdata cleaning to remove redundant phenotypes withno description, then pairing the resulting pheno-types for training and testing in a supervised learn-ing framework.
The data is skewed as most pairs arenegative.Studies 5 Phenotypes 733Classes 106 Total pairs 298378Positives 10906 Negatives 287472Table 2: Statistics of DataAnother pre-processing step is tokenization,which was applied to the description of each phe-notype before we extracted a set of features fromeach pairs.
The tokenization step includes convert-ing all uppercase letters to lowercase letters, re-moving punctuations, segmenting the text into to-kens, and using Porter?s stemmer (?)
to stem to-kens, removing stop words and digits.
For exam-ple, ?TRANSIENT ISCHEMIC ATTACK?
willbecome (transient, ischem, attack).
Notethat ?ic?
was removed from ?ischemic?
by thestemming process.The next step is feature extraction.
The goal hereis to represent each pair of phenotype variables bya set of feature values as the input to a machine-learning model.
We considered two types of fea-tures.
The first type is based on string similaritymetrics.
The idea is to combine the strength of a va-riety of string similarity metrics to measure the editdistance between the descriptions of a pair of pheno-types and use the result to determine if they matcheach other.
We chose 16 metrics as shown in Ta-ble 3.
Some of them are sophisticated and designedfor challenging record linkage tasks, such as match-ing personal records in census data.Levenshtein DistanceNeedleman-Wunch DistanceSmith-Waterman DistanceSmith-Waterman-Gotoh DistanceMonge Elkan Distance Q-grams DistanceJaro Distance Jaro WinklerBlock Distance Soundex DistanceMatching Coefficient Dice?s CoefficientJaccard Similarity Overlap CoefficientEuclidean Distance Cosine SimilarityTable 3: String similarity metricsWe used the Java implementation provided bySimMetrics1 to obtain the values of these metricsgiven a pair of phenotype descriptions.
SimMetricsalso provides descriptions and references of thesestring similarity metrics.
Each metric is treated asone feature and normalized into a real value between0 and 1, where 1 indicates that the two strings areidentical.These string similarity metrics, however, treat allwords equally but apparently some words are moreimportant than others when we match phenotypes.To assign different weights to different words, wedesigned a feature set that can be considered asweighted Jaccard as follows.
Let t be a token ora bi-gram (i.e., pair of consecutive tokens).
For eacht there are two features in the feature set of the fol-lowing forms:?
share-t: if t appears in the pre-processed de-scriptions of both variables, then its value is 11staffwww.dcs.shef.ac.uk/people/S.Chapman/simmetrics.html23and 0 otherwise;?
miss-t: if t appears in the pre-processed de-scription of one variable only, then its value is1 and 0 otherwise;For example, suppose we have tokenized variablesV1 = (age, menopause, start), and V2 =(menopause, start, when), then the features forthis pair will be(miss-?age?
: 1,share-?menopause?
: 1,share-?start?
: 1,miss-?when?
: 1,miss-?age menopause?
: 1,share-?menopause start?
: 1,miss-?start when?
: 1).All other features will have value 0.
In this way,each example pair of variables will be represented asa very high-dimensional feature vector of binary val-ues.
The dimensionality is proportional to the squareof the number of all distinct tokens appearing in thetraining set.Now we are ready to train a model by a machine-learning algorithm using the examples representedas feature vectors.
The model of our choice is themaximum entropy model (MaxEnt), also known aslogistic regression (?).
An advantage of this modelis that efficient learning algorithms are available fortraining this model with high-dimensional data andthe model not only classifies an example into posi-tive or negative but also gives an estimated probabil-ity as its confidence.
The basic idea of logistic re-gression is to search for a weight vector of the samedimension as the feature vector such that this weightvector when applied in the logit function of the prob-ability estimation of the training examples will max-imize the likelihood of the positive-negative assign-ment of the training examples (?).
The same modelcan also be derived from the principle of maximumentropy.
We randomly selected half of the pairs asthe training examples and the rest as the holdout setfor evaluation.We used the Merriam-Webster Medical Dictio-nary (?
)2 to augment the descriptions of phenotypes.If there is an entry for a token in the dictionary,2www.m-w.com/browse/medical/a.htmthen its definition will be included in the descriptionand then the same pre-processing and feature extrac-tion steps will be applied.
Pre-processing is also re-quired to remove useless words from the definitionsin the dictionary.
We chose this dictionary insteadof some ontology or phenotype knowledge base forits quality of contents and comprehensive coverageof biomedical terms.
The Merriam-Webster Med-ical Dictionary is also chosen as the only medicaldictionary included in the MedlinePlus3, a Web ser-vice produced by the National Library of Medicinefor the National Institute of Health to provide reli-able and up-to-date information about diseases, con-ditions and wellness issues to the patients and theirfamilies and friends.5 ResultsTable 4 shows the results in terms of precision, re-call, and F-score.
The first two rows show the use ofstring similarity metrics as features to train a NaiveBayes model and a MaxEnt model.
The F-scores ofboth models are similar, but Naive Bayes has higherfalse positives while MaxEnt made more false neg-ative errors.
MaxEnt with weighted Jaccard out-performs one with string-similarity features.
Aug-mentation by dictionary lookup (?w/ dictionary?)
isproved effective by improving recall from 0.59 to0.82, as more positive mappings were identified forthose phenotype pairs described in different terms.One may suspect that the augmentation may in-crease false positives due to incorrectly associatingcommon words in the descriptions.
But remarkably,the false positives also decreased, resulting in theimprovement in precision as well.Table 5 shows a set of selected examples to il-lustrate the effectiveness of augmentation by dictio-nary lookup.
The first column shows the original de-scriptions of the phenotype variable pairs.
The sec-ond and third columns show the classification results(0 for negative, 1 for positive) and the confidencescores by the MaxEnt model without augmentation.The next two columns are their counterparts for themodel with augmentation.For example, the definition of ?Goiter?
is?an enlargement of the thyroid gland.?
There-fore, after augmented by dictionary lookup, goi-3www.nlm.nih.gov/medlineplus24Method / Model Precision Recall F-scoreString similarity metrics featureNaiveBayes 0.5236 0.6492 0.5797MaxEnt 0.8092 0.4760 0.5994Weighted JaccardMaxEnt 0.9655 0.5931 0.7348w/ dictionary 0.9776 0.8208 0.8924w/ transitive closure (depth= 1) 0.9138 0.8064 0.8568w/ both 0.8961 0.9177 0.9068Table 4: Performance resultsPhenotypes w/o dic Score w/ dic ScoreGoiter everOveractive thyroid ever 0 0.014562 1 0.996656History of High Blood Pressure fromBaseline QuestionnaireHypertension ever 0 0.014562 1 0.641408DIABETES W/ FASTING GLUCOSE CUTPT.<126Insulin shots now 0 0.014562 1 0.523262TIA STATUS AT BASELINEStroke 0 0.014562 1 0.517444NUMBER OF CIGARETTES PER DAYCIGS SMOKED/DAY 0 0.014562 0 0.002509Table 5: Examples of Mapping Resultster can be matched with overactive thyroid.
Sim-ilarly, it is now possible to match ?High BloodPressure?
with ?hypertension?
and ?TIA?with ?stroke.?
?DIABETES?, ?GLUCOSE?and ?Insulin?
can also be associated together.However, terms must be covered in the medicaldictionary for this method to work.
For example,since ?CIGARETTES?
is not a medical term andeven the most sophisticated string similarity met-rics cannot match the local abbreviation ?CIGS?to ?CIGARETTES?, both models failed to match?SMOKE?
and ?CIGARETTES?
together.A solution to this issue is to compute transitiveclosure of the mapping.
For example, ifV1 = (SMOKE) andV2 = (SMOKE CIGARETTES)are matched together by the model because of ashared term ?smoke?
and so are V2 andV3 = (cigarettes),but not V1 and V3, then transitive closure will infera match of V1 and V3.
That will improve recall andF-score further.Figure 1 shows the performance of applying in-creasing depths of transitive closure to the results(a) without and (b) with augmentation by dictio-nary lookup.
Transitive closure improves the per-formance for both models in the beginning but de-grades quickly afterward because a phenotype maybe assigned to multiple classes.
As false positives in-crease, they will ripple when we infer new positivesfrom false positives.
Improvement for the model (a)is more obvious and degradation is not as grave.
Ap-plying transitive closure with depth = 1 yields thebest performance.
The exact scores are shown inTable 4 (See ?w/ transitive closure?
and ?w/ both?
).The results above were obtained by splitting theset of all pairs by half into training and test sets.It is possible that the model remembers phenotypedescriptions because they distribute evenly in bothtraining and test sets.
To apply the system in prac-tice, the model must generalize to unseen pheno-types.
To evaluate the generalization power, insteadof splitting the set of pairs, we split the set of vari-250 1 2 3 4 5 6 7 80.20.30.40.50.60.70.80.91Depth of transitive closurePerformance(a) MaxEntF?scorePrecisionRecall(a) MaxEnt model0 1 2 3 4 5 6 7 80.20.30.40.50.60.70.80.91Depth of transitive closurePerformance(b) With DictionaryF?scorePrecisionRecall(b) MaxEnt model with augmentation by dictionary lookupFigure 1: Performance with increasing depths of transitive closureables by 2 to 1, and used 2/3 of phenotype variablesto generate pairs as the training set and 1/3 to pairwith those in the 2/3 set as well as with each otherfor testing.
That resulted in 129286 pairs for trainingand 169092 pairs for testing.
In this test set, 6356pairs are positive.We used this training set to train MaxEnt mod-els using the weighted Jaccard feature set with andwithout dictionary augmentation.
Table 6 showsthe results.
Again, dictionary augmentation signif-icantly improves the performance in this case, too,with the F-score reaching 0.81.
Though the resultsdegrade slightly from the ones obtained by splittingby pairs, this is expected as the training set is smaller(129286 pairs vs. 149189 = 298378/2, see Ta-ble 2).
Consequently, the proposed models can gen-eralize well to unseen phenotypes to some extent.Method/Model Precision Recall F-scorew/o dictionary 0.9398 0.5817 0.7186w/ dictionary 0.8213 0.7977 0.8093Table 6: Performance results of splitting by variables6 Conclusions and Future WorkIn this paper, we define the problem of phenotypemapping and present a solution by learning to scoreand classify pairs of phenotypes.
We evaluate oursolution using a data set of manually matched phe-notypes from the PAGE PheWAS study.
We showthat weighted Jaccard features are more effective forthis problem than combining string similarity met-rics for a MaxEnt model and that dictionary aug-mentation improves the performance by allowingmatching of phenotypes with semantically relatedbut syntactically different descriptions.
We showthat inferring more positives by depth-one transitiveclosure fixes those false negatives due to the lack ofdictionary definitions.
Finally, the evaluation resultsof splitting-by-variables show that the models gen-eralize well to unseen variables, which is importantfor the solution to be practical.Our future work includes to apply blocking as apre-processing step to keep the number of pairs man-ageable and to apply active or unsupervised learningto alleviate the burden of generating training corporaby manual matching.AcknowledgmentsThis work was supported by NHGRI grantHG004801 to C.-N.H. and J.L.A.
and HG004798 toS.A.P.
and M.D.R.
C.-J.K. was supported by NSC99-3112-B-001-028, Taiwan.
The data were madeavailable by the participating components of theNHGRI PAGE program.
The complete list of PAGEmembers can be found at www.pagestudy.org.The contents of this paper are solely the responsi-bility of the authors and do not necessarily representthe official views of the NIH.26ReferencesSiiri N. Bennett, Neil Caporaso, Annette L. Fitzpatrick,Arpana Agrawal, Kathleen Barnes, Heather A. Boyd,Marilyn C. Cornelis, Nadia N. Hansel, Gerardo Heiss,John A. Heit, Jae Hee Kang, Steven J. Kittner, Pe-ter Kraft, William Lowe, Mary L. Marazita, Kris-tine R. Monroe, Louis R. Pasquale, Erin M. Ramos,Rob M. van Dam, Jenna Udren, Kayleen Williams, andfor the GENEVA Consortium.
2011.
Phenotype har-monization and cross-study collaboration in gwas con-sortia: the GENEVA experience.
Genetic Epidemiol-ogy, 35(3):159?173.Mikhail Bilenko and Raymond J. Mooney.
2003.Adaptive duplicate detection using learnable stringsimilarity measures.
In Proceedings of the NinthACM SIGKDD International Conference on Knowl-edge Discovery and Data Mining, pages 39?48, Wash-ington, DC, USA.William W. Cohen, Pradeep Ravikumar, and StephenFienberg.
2003.
A comparison of string distancemetrics for name-matching tasks.
In Proceedings ofIJCAI-03 Workshop on Information Integration on theWeb (IIWeb-03).The Wellcome Trust Case Control Consortium.
2007.Genome-wide association study of 14,000 cases ofseven common diseases and 3,000 shared controls.Nature, 447(7145):661?678, June.Joshua C. Denny, Marylyn D. Ritchie, Melissa A. Bas-ford, Jill M. Pulley, Lisa Bastarache, Kristin Brown-Gentry, Deede Wang, Dan R. Masys, Dan M. Ro-den, and Dana C. Crawford.
2010.
Phewas: demon-strating the feasibility of a phenome-wide scan todiscover gene-disease associations.
Bioinformatics,26(9):1205?1210.Ivan P. Felligi and Alan B. Sunter.
1969.
A theory forrecord linkage.
Journal of the American Statistical As-sociation, 64(328):1183?1210.Michael Y. Galperin and Guy R. Cochrane.
2011.
The2011 nucleic acids research database issue and the on-line molecular biology database collection.
NucleicAcids Research, 39(suppl 1):D1?D6.John Hardy and Andrew Singleton.
2009.
Genomewideassociation studies and human disease.
New EnglandJournal of Medicine, 360(17):1759?1768.T.
Hastie, R. Tibshirani, and J. Friedmann.
2009.
The El-ements of Statistical Learning (2nd Edition).
Springer-Verlag, New York, NY, USA.Mauricio A. Herna?ndez and Salvatore J. Stolfo.
1998.Real-world data is dirty: Data cleansing and themerge/purge problem.
Data Mining and KnowledgeDiscovery, 2:9?37.Jerry R. Hobbs.
1979.
Coherence and coreference.
Cog-nitive Science, 3(1):67?90.Andrew McCallum, Kamal Nigam, and Lyle Ungar.2000.
Efficient clustering of high-dimensional datasets with application to reference matching.
In Pro-ceedings of the Sixth ACM SIGKDD InternationalConference on Knowledge Discovery and Data Min-ing, pages 169?178.Merriam-Webster.
2006.
Medical Dictionary.
Merriam-Webster, Springfield, MA, USA.Matthew Michelson and Craig A. Knoblock.
2006.Learning blocking schemes for record linkage.
In Pro-ceedings of the 21st National Conference on ArtificialIntelligence (AAAI-06), Boston, MA.Steven Minton, Claude Nanjo, Craig A. Knoblock, mar-tin Michalowski, and Matthew Michelson.
2005.
Aheterogeneous field matching method for record link-age.
In Proceedings of the Fifth IEEE InternationalConference on Data Mining, Novemeber.Alvaro Monge and Charles Elkan.
1996.
The fieldmatching problem: Algorithms and applications.
InIn Proceedings of the Second International Confer-ence on Knowledge Discovery and Data Mining, pages267?270.Felix Naumann and Melanie Herschel.
2010.
An Intro-duction to Duplicate Detection.
Synthesis Lectures onData Management.
Morgan & Claypool Publishers.Vincent Ng and Claire Cardie.
2002.
Improving ma-chine learning approaches to coreference resolution.In Proceedings of the 40th Annual Meeting on Asso-ciation for Computational Linguistics, pages 104?111,Stroudsburg, PA, USA.
Association for ComputationalLinguistics.Martin F. Porter.
1980.
An algorithm for suffix stripping.Program, 14(3):130?137.Sheila Tejada, Craig A. Knoblock, and Steven Minton.2001.
Learning object identification rules for informa-tion integration.
Information Systems, 26(8).27
