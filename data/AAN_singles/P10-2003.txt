Proceedings of the ACL 2010 Conference Short Papers, pages 12?16,Uppsala, Sweden, 11-16 July 2010. c?2010 Association for Computational LinguisticsLearning Lexicalized Reordering Models from Reordering GraphsJinsong Su, Yang Liu, Yajuan Lu?, Haitao Mi, Qun LiuKey Laboratory of Intelligent Information ProcessingInstitute of Computing TechnologyChinese Academy of SciencesP.O.
Box 2704, Beijing 100190, China{sujinsong,yliu,lvyajuan,htmi,liuqun}@ict.ac.cnAbstractLexicalized reordering models play a crucialrole in phrase-based translation systems.
Theyare usually learned from the word-alignedbilingual corpus by examining the reorderingrelations of adjacent phrases.
Instead of justchecking whether there is one phrase adjacentto a given phrase, we argue that it is importantto take the number of adjacent phrases intoaccount for better estimations of reorderingmodels.
We propose to use a structure namedreordering graph, which represents all phrasesegmentations of a sentence pair, to learn lex-icalized reordering models efficiently.
Exper-imental results on the NIST Chinese-Englishtest sets show that our approach significantlyoutperforms the baseline method.1 IntroductionPhrase-based translation systems (Koehn et al,2003; Och and Ney, 2004) prove to be the state-of-the-art as they have delivered translation perfor-mance in recent machine translation evaluations.While excelling at memorizing local translation andreordering, phrase-based systems have difficulties inmodeling permutations among phrases.
As a result,it is important to develop effective reordering mod-els to capture such non-local reordering.The early phrase-based paradigm (Koehn et al,2003) applies a simple distance-based distortionpenalty to model the phrase movements.
More re-cently, many researchers have presented lexicalizedreordering models that take advantage of lexicalinformation to predict reordering (Tillmann, 2004;Xiong et al, 2006; Zens and Ney, 2006; Koehn etFigure 1: Occurrence of a swap with different numbersof adjacent bilingual phrases: only one phrase in (a) andthree phrases in (b).
Black squares denote word align-ments and gray rectangles denote bilingual phrases.
[s,t]indicates the target-side span of bilingual phrase bp and[u,v] represents the source-side span of bilingual phrasebp.al., 2007; Galley and Manning, 2008).
These mod-els are learned from a word-aligned corpus to pre-dict three orientations of a phrase pair with respectto the previous bilingual phrase: monotone (M ),swap (S), and discontinuous (D).
Take the bilingualphrase bp in Figure 1(a) for example.
The word-based reordering model (Koehn et al, 2007) ana-lyzes the word alignments at positions (s?1, u?1)and (s ?
1, v + 1).
The orientation of bp is setto D because the position (s ?
1, v + 1) containsno word alignment.
The phrase-based reorderingmodel (Tillmann, 2004) determines the presenceof the adjacent bilingual phrase located in position(s?
1, v+1) and then treats the orientation of bp asS.
Given no constraint on maximum phrase length,the hierarchical phrase reordering model (Galley andManning, 2008) also analyzes the adjacent bilingualphrases for bp and identifies its orientation as S.However, given a bilingual phrase, the above-mentioned models just consider the presence of anadjacent bilingual phrase rather than the number ofadjacent bilingual phrases.
See the examples in Fig-12Figure 2: (a) A parallel Chinese-English sentence pair and (b) its corresponding reordering graph.
In (b), we denoteeach bilingual phrase with a rectangle, where the upper and bottom numbers in the brackets represent the sourceand target spans of this bilingual phrase respectively.
M = monotone (solid lines), S = swap (dotted line), and D =discontinuous (segmented lines).
The bilingual phrases marked in the gray constitute a reordering example.ure 1 for illustration.
In Figure 1(a), bp is in a swaporder with only one bilingual phrase.
In Figure 1(b),bp swaps with three bilingual phrases.
Lexicalizedreordering models do not distinguish different num-bers of adjacent phrase pairs, and just give bp thesame count in the swap orientation.In this paper, we propose a novel method to betterestimate the reordering probabilities with the con-sideration of varying numbers of adjacent bilingualphrases.
Our method uses reordering graphs to rep-resent all phrase segmentations of parallel sentencepairs, and then gets the fractional counts of bilin-gual phrases for orientations from reordering graphsin an inside-outside fashion.
Experimental resultsindicate that our method achieves significant im-provements over the traditional lexicalized reorder-ing model (Koehn et al, 2007).This paper is organized as follows: in Section 2,we first give a brief introduction to the traditionallexicalized reordering model.
Then we introduceour method to estimate the reordering probabilitiesfrom reordering graphs.
The experimental resultsare reported in Section 3.
Finally, we end with aconclusion and future work in Section 4.2 Estimation of Reordering ProbabilitiesBased on Reordering GraphIn this section, we first describe the traditional lexi-calized reordering model, and then illustrate how toconstruct reordering graphs to estimate the reorder-ing probabilities.2.1 Lexicalized Reordering ModelGiven a phrase pair bp = (ei, fai), where ai de-fines that the source phrase fai is aligned to thetarget phrase ei, the traditional lexicalized reorder-ing model computes the reordering count of bp inthe orientation o based on the word alignments ofboundary words.
Specifically, the model collectsbilingual phrases and distinguishes their orientationswith respect to the previous bilingual phrase intothree categories:o =????
?M ai ?
ai?1 = 1S ai ?
ai?1 = ?1D |ai ?
ai?1| 6= 1(1)Using the relative-frequency approach, the re-ordering probability regarding bp isp(o|bp) = Count(o, bp)?o?
Count(o?, bp)(2)2.2 Reordering GraphFor a parallel sentence pair, its reordering graph in-dicates all possible translation derivations consistingof the extracted bilingual phrases.
To construct areordering graph, we first extract bilingual phrasesusing the way of (Och, 2003).
Then, the adjacent13bilingual phrases are linked according to the target-side order.
Some bilingual phrases, which haveno adjacent bilingual phrases because of maximumlength limitation, are linked to the nearest bilingualphrases in the target-side order.Shown in Figure 2(b), the reordering graph forthe parallel sentence pair (Figure 2(a)) can be rep-resented as an undirected graph, where each rect-angle corresponds to a phrase pair, each link is theorientation relationship between adjacent bilingualphrases, and two distinguished rectangles bs and beindicate the beginning and ending of the parallel sen-tence pair, respectively.
With the reordering graph,we can obtain all reordering examples containingthe given bilingual phrase.
For example, the bilin-gual phrase ?zhengshi huitan, formal meetings?
(seeFigure 2(a)), corresponding to the rectangle labeledwith the source span [6,7] and the target span [4,5],is in a monotone order with one previous phraseand in a discontinuous order with two subsequentphrases (see Figure 2(b)).2.3 Estimation of Reordering ProbabilitiesWe estimate the reordering probabilities from re-ordering graphs.
Given a parallel sentence pair,there are many translation derivations correspond-ing to different paths in its reordering graph.
As-suming all derivations have a uniform probability,the fractional counts of bilingual phrases for orien-tations can be calculated by utilizing an algorithm inthe inside-outside fashion.Given a phrase pair bp in the reordering graph,we denote the number of paths from bs to bp with?(bp).
It can be computed in an iterative way?
(bp) = ?bp?
?(bp?
), where bp?
is one of the pre-vious bilingual phrases of bp and ?(bs)=1.
In a sim-ilar way, the number of paths from be to bp, notatedas ?
(bp), is simply ?
(bp) = ?bp??
?(bp??
), wherebp??
is one of the subsequent bilingual phrases of bpand ?(be)=1.
Here, we show the ?
and ?
values ofall bilingual phrases of Figure 2 in Table 1.
Espe-cially, for the reordering example consisting of thebilingual phrases bp1=?jiang juxing, will hold?
andbp2=?zhengshi huitan, formal meetings?, marked inthe gray color in Figure 2, the ?
and ?
values can becalculated: ?
(bp1) = 1, ?
(bp2) = 1+1 = 2, ?
(bs) =8+1 = 9.Inspired by the parsing literature on pruningsrc span trg span ?
?
[0, 0] [0, 0] 1 9[1, 1] [1, 1] 1 8[1, 7] [1, 7] 1 1[4, 4] [2, 2] 1 1[4, 5] [2, 3] 1 3[4, 6] [2, 4] 1 1[4, 7] [2, 5] 1 2[2, 7] [2, 7] 1 1[5, 5] [3, 3] 1 1[6, 6] [4, 4] 2 1[6, 7] [4, 5] 1 2[7, 7] [5, 5] 3 1[2, 2] [6, 6] 5 1[2, 3] [6, 7] 2 1[3, 3] [7, 7] 5 1[8, 8] [8, 8] 9 1Table 1: The ?
and ?
values of the bilingual phrasesshown in Figure 2.
(Charniak and Johnson, 2005; Huang, 2008), thefractional count of (o, bp?, bp) isCount(o, bp?, bp) = ?(bp?)
?
?(bp)?
(bs) (3)where the numerator indicates the number of pathscontaining the reordering example (o, bp?, bp) andthe denominator is the total number of paths in thereordering graph.
Continuing with the reorderingexample described above, we obtain its fractionalcount using the formula (3): Count(M, bp1, bp2) =(1?
2)/9 = 2/9.Then, the fractional count of bp in the orientationo is calculated as described below:Count(o, bp) =?bp?Count(o, bp?, bp) (4)For example, we compute the fractional count ofbp2 in the monotone orientation by the formula (4):Count(M, bp2) = 2/9.As described in the lexicalized reordering model(Section 2.1), we apply the formula (2) to calculatethe final reordering probabilities.3 ExperimentsWe conduct experiments to investigate the effec-tiveness of our method on the msd-fe reorder-ing model and the msd-bidirectional-fe reorderingmodel.
These two models are widely applied in14phrase-based system (Koehn et al, 2007).
The msd-fe reordering model has three features, which rep-resent the probabilities of bilingual phrases in threeorientations: monotone, swap, or discontinuous.
If amsd-bidirectional-fe model is used, then the numberof features doubles: one for each direction.3.1 Experiment SetupTwo different sizes of training corpora are used inour experiments: one is a small-scale corpus thatcomes from FBIS corpus consisting of 239K bilin-gual sentence pairs, the other is a large-scale corpusthat includes 1.55M bilingual sentence pairs fromLDC.
The 2002 NIST MT evaluation test data isused as the development set and the 2003, 2004,2005 NIST MT test data are the test sets.
Wechoose the MOSES1 (Koehn et al, 2007) as the ex-perimental decoder.
GIZA++ (Och and Ney, 2003)and the heuristics ?grow-diag-final-and?
are used togenerate a word-aligned corpus, where we extractbilingual phrases with maximum length 7.
We useSRILM Toolkits (Stolcke, 2002) to train a 4-gramlanguage model on the Xinhua portion of Gigawordcorpus.In exception to the reordering probabilities, weuse the same features in the comparative experi-ments.
During decoding, we set ttable-limit = 20,stack = 100, and perform minimum-error-rate train-ing (Och, 2003) to tune various feature weights.
Thetranslation quality is evaluated by case-insensitiveBLEU-4 metric (Papineni et al, 2002).
Finally, weconduct paired bootstrap sampling (Koehn, 2004) totest the significance in BLEU scores differences.3.2 Experimental ResultsTable 2 shows the results of experiments with thesmall training corpus.
For the msd-fe model, theBLEU scores by our method are 30.51 32.78 and29.50, achieving absolute improvements of 0.89,0.66 and 0.62 on the three test sets, respectively.
Forthe msd-bidirectional-fe model, our method obtainsBLEU scores of 30.49 32.73 and 29.24, with abso-lute improvements of 1.11, 0.73 and 0.60 over thebaseline method.1The phrase-based lexical reordering model (Tillmann,2004) is also closely related to our model.
However, due tothe limit of time and space, we only use Moses-style reorderingmodel (Koehn et al, 2007) as our baseline.model method MT-03 MT-04 MT-05baseline 29.62 32.12 28.88m-f RG 30.51??
32.78??
29.50?baseline 29.38 32.00 28.64m-b-f RG 30.49??
32.73??
29.24?Table 2: Experimental results with the small-scale cor-pus.
m-f: msd-fe reordering model.
m-b-f: msd-bidirectional-fe reordering model.
RG: probabilities esti-mation based on Reordering Graph.
* or **: significantlybetter than baseline (p < 0 .05 or p < 0 .01 ).model method MT-03 MT-04 MT-05baseline 31.58 32.39 31.49m-f RG 32.44??
33.24??
31.64baseline 32.43 33.07 31.69m-b-f RG 33.29??
34.49??
32.79?
?Table 3: Experimental results with the large-scale cor-pus.Table 3 shows the results of experiments withthe large training corpus.
In the experiments ofthe msd-fe model, in exception to the MT-05 testset, our method is superior to the baseline method.The BLEU scores by our method are 32.44, 33.24and 31.64, which obtain 0.86, 0.85 and 0.15 gainson three test set, respectively.
For the msd-bidirectional-fe model, the BLEU scores producedby our approach are 33.29, 34.49 and 32.79 on thethree test sets, with 0.86, 1.42 and 1.1 points higherthan the baseline method, respectively.4 Conclusion and Future WorkIn this paper, we propose a method to improve thereordering model by considering the effect of thenumber of adjacent bilingual phrases on the reorder-ing probabilities estimation.
Experimental results onNIST Chinese-to-English tasks demonstrate the ef-fectiveness of our method.Our method is also general to other lexicalizedreordering models.
We plan to apply our methodto the complex lexicalized reordering models, forexample, the hierarchical reordering model (Galleyand Manning, 2008) and the MEBTG reorderingmodel (Xiong et al, 2006).
In addition, how to fur-ther improve the reordering model by distinguishingthe derivations with different probabilities will be-come another study emphasis in further research.15AcknowledgementThe authors were supported by National Natural Sci-ence Foundation of China, Contracts 60873167 and60903138.
We thank the anonymous reviewers fortheir insightful comments.
We are also grateful toHongmei Zhao and Shu Cai for their helpful feed-back.ReferencesEugene Charniak and Mark Johnson.
2005.
Coarse-to-fine n-best parsing and maxent discriminative rerank-ing.
In Proc.
of ACL 2005, pages 173?180.Michel Galley and Christopher D. Manning.
2008.
Asimple and effective hierarchical phrase reorderingmodel.
In Proc.
of EMNLP 2008, pages 848?856.Liang Huang.
2008.
Forest reranking: Discriminativeparsing with non-local features.
In Proc.
of ACL 2008,pages 586?594.Philipp Koehn, Franz Josef Och, and Daniel Marcu.2003.
Statistical phrase-based translation.
In Proc.of HLT-NAACL 2003, pages 127?133.Philipp Koehn, Hieu Hoang, Alexandra Birch, ChrisCallison-Burch, Marcello Federico, Nicola Bertoldi,Brooke Cowan, Wade Shen, Christine Moran, RichardZens, Chris Dyer, Ondrej Bojar, Alexandra Con-stantin, and Evan Herbst.
2007.
Moses: Open sourcetoolkit for statistical machine translation.
In Proc.
ofACL 2007, Demonstration Session, pages 177?180.Philipp Koehn.
2004.
Statistical significance tests formachine translation evaluation.
In Proc.
of EMNLP2004, pages 388?395.Franz Josef Och and Hermann Ney.
2003.
A system-atic comparison of various statistical alignment mod-els.
Computational Linguistics, 29(1):19?51.Franz Joseph Och and Hermann Ney.
2004.
The align-ment template approach to statistical machine transla-tion.
Computational Linguistics, pages 417?449.Franz Josef Och.
2003.
Minimum error rate training instatistical machine translation.
In Proc.
of ACL 2003,pages 160?167.Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu.
2002.
Bleu: a method for automatic eval-uation of machine translation.
In Proc.
of ACL 2002,pages 311?318.Andreas Stolcke.
2002.
Srilm - an extensible languagemodeling toolkit.
In Proc.
of ICSLP 2002, pages 901?904.Christoph Tillmann.
2004.
A unigram orientation modelfor statistical machine translation.
In Proc.
of HLT-ACL 2004, Short Papers, pages 101?104.Deyi Xiong, Qun Liu, and Shouxun Lin.
2006.
Maxi-mum entropy based phrase reordering model for statis-tical machine translation.
In Proc.
of ACL 2006, pages521?528.Richard Zens and Hermann Ney.
2006.
Discriminvativereordering models for statistical machine translation.In Proc.
of Workshop on Statistical Machine Transla-tion 2006, pages 521?528.16
