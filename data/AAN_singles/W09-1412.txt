Proceedings of the Workshop on BioNLP: Shared Task, pages 95?98,Boulder, Colorado, June 2009. c?2009 Association for Computational LinguisticsTunable Domain-Independent Event Extraction in the MIRA FrameworkGeorgi Georgiev1 Kuzman Ganchev1 Vassil Momtchev1georgi.georgiev@ontotext.com kuzman.ganchev@ontotext.com vassil.momtchev@ontotext.comDeyan Peychev1 Preslav Nakov1 Angus Roberts2deyan.peychev@ontotext.com preslav.nakov@ontotext.com a.roberts@dcs.shef.ac.uk1 Ontotext AD, 135 Tsarigradsko Chaussee, Sofia 1784, Bulgaria2 The Department of Computer Science, Regent Court 211 Portobello, Sheffield, S1 4DP.
UK.AbstractWe describe the system of the PIKB teamfor BioNLP?09 Shared Task 1, which targetstunable domain-independent event extraction.Our approach is based on a three-stage clas-sification: (1) trigger word tagging, (2) sim-ple event extraction, and (3) complex eventextraction.
We use the MIRA framework forall three stages, which allows us to trade pre-cision for increased recall by appropriatelychanging the loss function during training.
Wereport results for three systems focusing on re-call (R = 28.88%), precision (P = 65.58%),and F1-measure (F1 = 33.57%), respectively.1 IntroductionMolecular interactions have been the focus of inten-sive research in the development of in-silico biology.Recent developments like the Pathway and Interac-tion Knowledge Base (PIKB) aim to make availableto the user the large semantics of the existing molec-ular interactions data using massive knowledge syn-dication.
PIKB is part of LinkedLifeData1, a plat-form for semantic data integration based on RDF2syndication and lightweight reasoning.Our system is based on the MIRA frameworkwhere, by appropriately changing the loss functionon training, we can achieve any desirable balancebetween precision and recall.
For example, low pre-cision with high recall would be appropriate in asearch that aims to identify as many potential candi-dates as possible to be further examined by the user,1http://www.linkedlifedata.com2http://www.w3.org/RDF/while high precision might be essential when addingrelations to a knowledge base.
Such a tunable sys-tem is practical for a variety of important tasks, in-cluding but not limited to, populating extracted factsin PIKB and reasoning on top of new and old data.Our system is based on a three-stage classificationprocess: (1) trigger word tagging using a linear se-quence model, (2) simple event extraction, and (3)complex event extraction.
In stage (2), we generaterelations between a trigger word and one or moreproteins, while in stage (3), we look for complex in-teractions between simple events, trigger words andproteins.
We use MIRA for all three stages with aloss function tuned for high recall.2 One-best MIRA and Loss FunctionsIn what follows, xi will denote a generic input sen-tence, and yi will be the ?gold?
labeling of xi.
Foreach pair of a sentence xi and a labeling y, we com-pute a vector-valued feature representation f(xi, y).Given a weight vector w, the dot-product w ?
f(x, y)ranks the possible labelings y of x; we will denotethe top scoring labeling as yw(x).
As with hiddenMarkov models (Rabiner, 1989), yw(x) can be com-puted efficiently for suitable feature functions usingdynamic programming.The learning portion of our method requires find-ing a weight vector w that scores the correct labelingof the training data higher than any incorrect label-ing.
We used a one-best version of MIRA (Cram-mer, 2004; McDonald et al, 2005) to choose w.MIRA is an online learning algorithm that updatesthe weight vector w for each training sentence xiaccording to the following rule:95wnew = argminw?w ?
wold?s.t.
w ?
f(xi, yi) ?
w ?
f(x, y?)
?
L(yi, y?
)where L(yi, y) is a measure of the loss of using y in-stead of the correct labeling yi, and y?
is a shorthandfor ywold(xi).
In case of a single constraint, this pro-gram has a closed-form solution.
The most straight-forward and the most commonly used loss functionis the Hamming loss, which sets the loss of labelingy with respect to the gold labeling yi as the numberof training examples where the two labelings dis-agree.
Since Hamming loss is not flexible enoughfor targeted training towards recall or precision, weuse a number of task-specific loss functions (seeSections 3 and 5 for details).
We implemented one-best MIRA and the corresponding loss functions inan in-house toolkit called Edlin.
Edlin provides gen-eral machine learning architecture for linear modelsand a framework with implementations of popularlearning algorithms including Naive Bayes, percep-tron, maximum entropy, one-best MIRA, and condi-tional random fields (CRF) among others.3 Trigger Word TaggingThe training and the development abstracts werefirst tokenized and split into sentences using maxi-mum entropy models trained on the Genia3 corpora.Subsequently, we trained several sequence taggersin order to identify the trigger words in text.
Allour experiments used the standard BIO encoding(Ramshaw and Marcus, 1995) with different featuresets and learning procedures.
We focused on recallsince it determines the upper bound on the perfor-mance of our final system.
In our experiments, wefound that simultaneously identifying trigger wordsand the event types they trigger yielded low recall;thus, we settled on identifying trigger words in textas one kind of entity, regardless of event types.In our initial experiments, we used a CRF-based sequence tagger (Lafferty et al, 2001), whichyielded R=43.51%.
We further tried feature induc-tion (McCallum, 2003) and second-order Markovassumptions for the CRF, achieving 44.72% and49.64% recall, respectively.3http://www-tsujii.is.s.u-tokyo.ac.jp/GENIA/home/wiki.cgiFeature Set R P F1Baseline (current word) 44.82 2.86 05.38+ POS & char 3-gram 77.41 27.96 41.09+ previous POS tag 79.77 29.32 42.88+ lexicon (final tagger) 80.44 29.65 43.33Table 1: Recall (R), precision (P), and F1-measure for thetrigger words tagger (in %s) on the development datasetfor different feature sets using MIRA training with falsenegatives as a loss function.Feature Setsentity type of e1 and e2words in e1 and e2word bigrams in e1 and e2POS of e1 and e2words between e1 and e2word bigrams between e1 and e2POS between e1 and e2distance between e1 and e2distance between e1 and e2 in the dependency graphsteps in parse tree to get e1 and e2 in the same phrasevarious combinations of the above featuresTable 2: Our feature set for the MIRA classifier that pre-dicts binary relations.
Here e1 and e2 can be proteinsand/or trigger words.Subsequently, we settled on using MIRA so thatwe can trade-off precision for recall.
In order toboost recall, we defined the loss function as the num-ber of false negative trigger chunks.
Thus, a largerloss update was made whenever the model failed todiscover a trigger word, while discovering spurioustrigger words was penalized less severely.
We ex-perimented with popular feature sets previously usedfor named entity (McCallum and Li, 2003) and gene(McDonald and Pereira, 2005) recognition includingorthographic, part-of-speech (POS), shallow parsingand gazetteers.
However, we found that only a smallnumber of them was really helpful; a summary ispresented in Table 1.
In order to boost recall evenfurther, we prepared a gazetteer of trigger chunksderived from the training data, and we extended itwith the corresponding WordNet synsets; we thusachieved 80.44% recall for our final tagger.4 Event ExtractionThe input to our event extraction algorithm is a listof trigger words and a list of genes or gene prod-96ucts (e.g., proteins); the output is a set of relationsas defined for Task 1.
Our algorithm works in twostages.
First, we generate events corresponding torelations between a trigger word and one or moreproteins (simple events); then we generate events forrelations between trigger words, proteins and simpleevents (complex events).
The two stages differ onlyin the input data; thus, below we will describe oursystem for the first stage only.For each sentence, we considered all pairs of en-tities (trigger words and proteins), and we used anunstructured classifier to determine the relationshipfor a given pair.
These relationships encoded boththe type of event (e.g., binding, regulation) and enti-ties?
roles in that event (e.g., theme, cause); therewas also a special relationship for unrelated enti-ties.
We constructed labeled examples to train aMIRA classifier using the training data provided bythe task organizers; n-ary relations were then recon-structed from classifier?s predictions.
The featureswe used are summarized in Table 2: they are overthe words separating the two entities and their part-of-speech tags.
We further used some simple fea-tures from syntactic phrases (OpenNLP4 parser) anddependency parse trees (McDonald et al, 2005), ex-tracted using parsers trained on Genia corpora.After some initial experiments, we found that ourfeatures were not sufficiently rich to allow us to learnthe relationships between proteins that are part of thesame event: we achieved a very low recall of about20%.
Consequently, we focused on the relationshipsbetween a trigger word and a protein.
Since the com-petition stipulated that each trigger could be associ-ated with only one type of event, we first chose theevent type for each trigger by selecting the protein-label pair with the highest score.
We then fixed theevent type for this trigger word, and we discarded allproteins for which our classifier assigned a differentevent type to the target trigger-protein pair.
Finally,we added to our output list all binary relations wherethe role of the protein was theme.For some event classes ?
binding, regulation, pos-itive regulation and negative regulation ?
the outputof the binary classifier was further transformed sothat n-ary relations can be formed.
However, theway we did this was somewhat ad-hoc.
For bind-4http://opennlp.sourceforge.netEvent Class R P F1Localization 10.92 82.61 19.29Binding 7.20 39.68 12.20Gene expression 30.47 74.58 43.26Transcription 10.95 39.47 17.14Protein catabolism 28.57 57.14 38.10Phosphorylation 34.07 86.79 48.94Event Total 21.52 68.68 32.77Regulation 1.37 26.67 2.61Positive regulation 1.12 25.58 2.14Negative regulation 0.26 100.00 0.53Regulation Total 0.97 27.12 1.87Overall 10.84 64.13 18.55Table 3: Our official results: for an erroneous submission.ing events, we added a 3-ary relation between thetrigger, the highest scoring protein, and the secondhighest scoring protein.
For regulation events, weadded a 3-ary relation between the trigger and everypair of proteins where one was a theme and the otherone was a cause.
This aggressive addition of poten-tial matches slightly reduced the overall precision,but helped improve the recall for the final system.5 Results and DiscussionUnfortunately, we made an error when making ourofficial submission, which resulted in low scores;Table 3 shows the results for that submission.The rest of this section describes the results andthe implementation for the system we intended tosubmit.
All reported results are for exact spanmatches and were obtained using the online tool pro-vided by the task organizers.As stated in Section 4, we used a linear modeltrained using one-best MIRA with ten runs overthe data for the event extraction system.
We over-sampled the unstructured training instances that cor-responded to a relation so that they become roughlyequal in number to those that do not correspond to arelation.
Finally, we performed parameter averagingas described in (Freund and Schapire, 1999).
Thesedetails turned out to be very important for the systemperformance.Table 4 shows the results for three different lossfunctions that gave the best results in our experi-ments.
In describing the loss functions, we definethree different types of errors: (1) if the system cor-rectly predicted that a relation should be present,970-1 Loss High Recall High PrecisionEvent Class R P F1 R P F1 R P F1Localization 33.33 69.05 44.96 39.08 48.23 43.17 25.86 86.54 39.82Binding 38.33 32.60 35.23 46.97 24.51 32.21 24.50 37.95 29.77Gene expression 57.89 65.72 61.56 64.82 53.49 58.61 47.65 76.27 58.65Transcription 30.66 33.87 32.18 33.58 22.12 26.67 21.17 47.54 29.29Protein catabolism 42.86 85.71 57.14 42.86 60.00 50.00 42.86 85.71 57.14Phosphorylation 75.56 77.86 76.69 77.78 65.22 70.95 52.59 82.56 64.25Event total 49.64 54.60 52.00 55.98 41.55 47.70 37.93 65.83 48.13Regulation 0.00 0.00 0.00 2.41 22.58 4.35 0.00 0.00 0.00Positive regulation 1.73 30.91 3.28 5.29 25.24 8.75 0.20 28.57 0.40Negative regulation 0.53 40.00 1.04 1.06 23.53 2.02 0.26 100.00 0.53Regulation Total 1.15 30.16 2.21 3.81 24.80 6.61 0.18 37.50 0.36Overall 24.45 53.54 33.57 28.88 39.71 33.44 18.32 65.58 28.64Table 4: Results (in %s) for one-best MIRA with different loss functions.but guessed the wrong type, we call this a cross-labeling; (2) a false positive occurs when the learnerguessed some relation while there should have beennone; (3) the reverse is a false negative.
All lossfunctions we considered had a cross-labeling loss of1.
The 0-1 loss also has a loss of 1 for false positivesand false negatives.
The high-recall loss functionpenalizes false positives with 0.1 and false negativeswith 5.
The high-precision loss function penalizesfalse negatives with 0.1 and false positives with 5.The values 0.1 and 5 were chosen on the develop-ment data, but were not optimized aggressively.In conclusion, we have built three domain-independent event extraction systems based on theMIRA framework, each using a different loss func-tion.
Overall, they perform quite well and wouldhave been ranked second on precision5, and 6th onrecall, and 7th on F1-measure.6 Future WorkAfter integrating domain knowledge, which shouldimprove the recall for complex events and shouldboost the overall precision, we intend to transformthe system output into RDF and add it to the PIKBrepository.
The required efforts discouraged us frombuilding a middle ontology between the BioNLP andthe PIKB data models, especially given the time lim-itations for the present task competition.
However,we believe this is a promising direction, which weplan to pursue in future work.5Our official submission is second on precision as well.AcknowledgmentsThe work reported in this paper was partially sup-ported by the EU FP7 - 215535 LarKC.ReferencesKoby Crammer.
2004.
Online Learning of Complex Cat-egorial Problems.
Ph.D. thesis, Hebrew University ofJerusalem.Yoav Freund and Robert E. Schapire.
1999.
Large mar-gin classification using the perceptron algorithm.
InMachine Learning, pages 277?296.John Lafferty, Andrew McCallum, and Fernando Pereira.2001.
Conditional random fields: Probabilistic mod-els for segmenting and labeling sequence data.
In Pro-ceedings of ICML.
Morgan Kaufmann.Andrew McCallum and Wei Li.
2003.
Early resultsfor named entity recognition with conditional randomfields, feature induction and web-enhanced lexicons.In Proceedings of CoNLL.Andrew McCallum.
2003.
Efficiently inducing featuresof conditional random fields.
In Proceedings of UAI.Ryan McDonald and Fernando Pereira.
2005.
Identify-ing gene and protein mentions in text using conditionalrandom fields.
BMC Bioinformatics, (Suppl 1):S6(6).Ryan McDonald, Koby Crammer, and Fernando Pereira.2005.
Online large-margin training of dependencyparsers.
In Proceedings of ACL.
ACL.Lawrence Rabiner.
1989.
A tutorial on hidden Markovmodels and selected applications in speech recogni-tion.
Proceedings of the IEEE, 77(2).Lance Ramshaw and Mitch Marcus.
1995.
Text chunk-ing using transformation-based learning.
In DavidYarovsky and Kenneth Church, editors, Proceedingsof the Third Workshop on Very Large Corpora.
ACL.98
