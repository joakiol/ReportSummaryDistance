Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 145?153,Los Angeles, California, June 2010. c?2010 Association for Computational LinguisticsHitting the Right Paraphrases in Good TimeStanley KokDepartment of Computer Science & EngineeringUniversity of WashingtonSeattle, WA 98195, USAkoks@cs.washington.eduChris BrockettMicrosoft ResearchOne Microsoft WayRedmond, WA 98052, USAchrisbkt@microsoft.comAbstractWe present a random-walk-based approach tolearning paraphrases from bilingual parallelcorpora.
The corpora are represented as agraph in which a node corresponds to a phrase,and an edge exists between two nodes if theircorresponding phrases are aligned in a phrasetable.
We sample random walks to computethe average number of steps it takes to reacha ranking of paraphrases with better ones be-ing ?closer?
to a phrase of interest.
This ap-proach allows ?feature?
nodes that representdomain knowledge to be built into the graph,and incorporates truncation techniques to pre-vent the graph from growing too large for ef-ficiency.
Current approaches, by contrast, im-plicitly presuppose the graph to be bipartite,are limited to finding paraphrases that are oflength two away from a phrase, and do notgenerally permit easy incorporation of domainknowledge.
Manual evaluation of generatedoutput shows that our approach outperformsthe state-of-the-art system of Callison-Burch(2008).1 IntroductionAutomatically learning paraphrases, or alternativeways of expressing the same meaning, is an ac-tive area of NLP research because of its useful-ness in a variety of applications, e.g., question an-swering (Lin and Pantel, 2001; Ravichandran andHovy, 2002; Reizler et al, 2007), document sum-marization (Barzilay et al, 1999; McKeown et al,2002), natural language generation (Iordanskaja etal., 1991; Lenke, 1994; Stede, 1999), machine trans-lation (Kauchak and Barzilay, 2006; Callison-Burchet al, 2006; Madnani et al, 2007).Early work on paraphrase acquisition has focusedon using monolingual parallel corpora (Barzilay andMcKeown, 2001; Barzilay and Lee, 2003; Pang etal., 2003; Quirk et al, 2004).
While effective, suchmethods are hampered by the scarcity of monolin-gual parallel corpora, an obstacle that limits boththe quantity and quality of the paraphrases learned.To address this limitation, Bannard and Callison-Burch (2005) focused their attention on the abun-dance of bilingual parallel corpora.
The crux ofthis system (referred to below as ?BCB?)
is to alignphrases in a bilingual parallel corpus and hypothe-size English phrases as potential paraphrases if theyare aligned to the same phrase in another language(the ?pivot?).
Callison-Burch (2008) further refinesBCB with a system that constrains paraphrases tohave the same syntactic structure (Syntactic Bilin-gual Phrases: SBP).We take a graphical view of the state-of-the-artBCB and SBP approaches by representing the bilin-gual parallel corpora as a graph.
A node correspondsto a phrase, and an edge exists between two nodes iftheir corresponding phrases are aligned.
This graph-ical form makes the limitations of the BCB/SBP ap-proaches more evident.
The BCB/SBP graph is lim-ited to be bipartite with English nodes on one sideand foreign language nodes on the other, and anedge can only exist between nodes on different sides.This neglects information between foreign languagenodes that may aid in learning paraphrases.
Further,by only considering English nodes that are linkedvia a foreign language node as potential paraphrases,145these approaches will fail to find paraphrases sepa-rated by distances greater than length two.In this paper, we present HTP (Hitting TimeParaphraser), a paraphrase learning approach that isbased on random walks (Lova?sz, 1996) and hittingtimes (Aldous and Fill, 2001).
Hitting time mea-sures the average number of steps one needs to takein a random traversal of a graph before reaching adestination node from a source node.
Intuitively, thesmaller the hitting time from a phrase E to E?
(i.e.,the closer E?
is to E), the more likely it is that E?
isa good paraphrase of E. The advantages of HTP areas follows:?
By traversing paths of lengths greater than two,our approach is able to find more paraphrasesof a given phrase.?
We do not require the graph to be bipartite.Edges can exist between nodes of different for-eign languages if their corresponding phrasesare aligned.
This allows information from for-eign phrase alignments to be used in findingEnglish paraphrases.?
We permit domain knowledge to be easily in-corporated as nodes in the graph.
This allowsdomain knowledge to favor good paraphrasesover bad ones, thereby improving performance.In this paper, we focus on learning English para-phrases.
However, our system can be applied tolearning paraphrases in any language.We begin by reviewing random walks and hittingtimes in the next section.
Then we describe our para-phrase learning algorithm (Section 3), and report ourexperiments (Section 4).
We discuss related work inSection 5.
Finally, we conclude with future work(Section 6).2 BackgroundA directed graph consists of a set of nodes V , and aset of edges E. A directed edge is a pair (i, j) wherei, j ?
V .
Associated with the graph is a |V | ?
|V |adjacency matrix W .
Each entry Wij in the matrixis the weight of edge (i, j), or zero if the edge doesnot exist.In a random walk (Lova?sz, 1996), we traversefrom node to node via the edges.
Suppose at timestep t, we are at node i.
In the next step, we moveto its neighbor j with probability proportional tothe weight of the edge (i, j), i.e., with probabilityWij/?jWij .
This probability is known as the tran-sition probability from i to j.
Note that the transitionprobabilities from a node to its neighbors sum to 1.The hitting time hij (Aldous and Fill, 2001) fromnode i to j is defined as the average number of stepsone takes in a random walk starting from i to visit jfor the first time.
Hitting time has the property of be-ing robust to noise.
This is a desirable property forour system which works on bilingual parallel cor-pora containing numerous spurious alignments be-tween phrases (i.e., edges between nodes).
However,as observed by Liben-Nowell and Kleinberg (2003),hitting time has the drawback of being sensitive toportions of the graph that are far from the start nodebecause it considers paths of length up to?.To circumvent this problem, Sarkar and Moore(2007) introduced the notion of truncated hittingtime where random walks are limited to have at mostT steps.
The truncated hitting time hTij from node ito j is defined as the average number of steps onetakes to reach j for the first time starting from i in arandom walk that is limited to at most T steps.
hTijis defined to be 0 if i = j or T = 0, and to be T if jis not reach in T steps.
As T ?
?, hTij ?
hij .In a recent work, Sarkar et al (2008) showed thattruncated hitting time can be approximated accu-rately with high probability by sampling.
They runM independent length-T random walks from nodei.
In m of these runs, node j is visited for the firsttime at time steps t1j , .
.
.
, tmj .
The estimated trun-cated hitting time is given byh?Tij =?mk=1 tkjM+ (1?mM)T (1)They also showed that the number of samples of ran-dom walks M has to be at least 122 log2nd in orderfor the estimated truncated hitting time to be a goodestimate of the actual truncated hitting time withhigh probability, i.e., for P (|h?Tij?hTij |?T )?1?
?,where n is the number of nodes in the graph,  and ?are user-specified parameters, and 0 ?
, ?
?
1.3 Hitting Time Paraphraser (HTP)HTP takes a query phrase as input, and outputs a listof paraphrases, with better paraphrases at the top of146Figure 1: Graph created from English-French (E-F),English-German (E-G), and French-German (F-G) bilin-gual parallel corpora.
Bold edges have large positiveweights (high transition probabilities).the list.
HTP also requires as input a set of bilin-gual parallel corpora that have been processed intophrase tables of the kind used in statistical machinetranslation.A bilingual parallel corpus is made up of sen-tences in two languages.
Two sentences that aretranslations of one another are paired together, anda phrase in one sentence is aligned with a phrase inthe other with the same meaning.
From such align-ments, we can count for a phrase E both the num-ber of times it occurs (CountE), and the number oftimes it is aligned with a phrase F in the other lan-guage (CountE,F ).
With these counts we can es-timate the probability of F given E as P (F |E) =CountE,FCountE.HTP represents the aligned phrases as a graph.
Anode corresponds to a phrase, and a directed edgeexists from node i to j if their corresponding phrasesare aligned.
The weight of edge (i, j) is given byP (j|i) which is computed as described in the previ-ous paragraph.Figure 1 gives an example of a graph createdfrom English-French, English-German, and French-German parallel corpora.
We use this figure to il-lustrate the strengths of HTP.
First, by using moder-ately long random walks, HTP is able to find para-phrases that are separated by long paths.
For ex-ample, there is a high probability path of length 4(E1, F1, E2, F2, E3) from E1 to E3.
Because of thepath?s high probability, it will appear in many of therandom walks starting from E1 that are sampled onthe graph, and thus E3 will be visited in many ofthe samples.
This causes the truncated hitting timehTE1E3 to be small, allowing HTP to find E3 as aplausible paraphrase of E1.
Second, by allowingedges between nodes of different foreign languagesTable 1: The HTP algorithm.function HTP (E,C, d, n,m, T, ?, l)input: E, query phraseC, tables of aligned phrasesd, maximum distance of nodes from En, maximum number of nodes in graphm, number of samples of random walksT , maximum number of steps taken by arandom walk?, probability that estimated truncated hittingtime deviates from actual value by a largemargin (see Equation 1)l, number of top outgoing edges to select ateach node in a random walkoutput:(E?1, .
.
.
, E?k), paraphrases of E ranked inorder of increasing hitting timescalls: CreateGraph(E,C, d, n) creates graph Gfrom C containing at most n nodes that areat most d steps from EEstimateHitT imes(E,G,m, T, ?
), estimatesthe truncated hitting times of each node in Gby running m random walksPruneNodes((E1, .
.
.
, Ek), G), removes nodesfrom G if their hitting times is equal to T .AddFeatureNodes(G), adds nodesrepresenting domain knowledge to GG?
CreateGraph(E,C, d, n)(E1, .
.
.
, Ek)?
EstimateHitT imes(E,G,m, T, ?)G?
?PruneNodes((E1, .
.
.
, Ek), G)G???AddFeatureNodes(G?
)(E?1, .
.
.
, E?k)?
EstimateHitT imes(E,G?
?,m, T, ?
)return (E?1, .
.
.
, E?k)(i.e., by not requiring the graph to be bipartite), HTPallows strong correlation between foreign languagenodes to aid in finding paraphrases.
In the figure,even though E4 and E5 are not linked via a com-mon foreign language node, there is a high proba-bility path linking them (E4, F3, G1, E5).
This al-lows HTP to find E5 as a reasonable paraphrase ofE4.
Third, HTP enables domain knowledge to beincorporated as nodes in the graph.
For example,we could incorporate the domain knowledge thatphrases with lots of unigrams in common are likelyto be paraphrases.
In Figure 1, the ?feature?
noderepresents such knowledge, linking E4 and E1 aspossible paraphrases even though they have no for-eign language nodes in common.
Note that such147domain knowledge nodes can be linked to arbitrarynodes, not just English ones.The HTP algorithm is shown in Table 1.
It takesas input a query phrase and a set of bilingual phrasetables.
The algorithm begins by creating a graphfrom the phrase tables.
Then it estimates the trun-cated hitting times of each node from the query nodeby sampling random walks of length T .
Next itprunes nodes (and their associated edges) if theirtruncated hitting times are equal to T .
To the result-ing graph, it then adds nodes representing domainknowledge and estimates the truncated hitting timesof the nodes by sampling random walks as before.Finally, it returns the nodes in the same language asthe query phrase in order of increasing hitting times.3.1 Graph CreationAn obvious approach to creating a graph from bilin-gual parallel corpora is to create a node for everyphrase in the corpora, and two directed edges (i, j)and (j, i) for every aligned phrase pair i and j. LetH refer to the graph that is created in this manner.Such an approach is only tractable for small bilin-gual parallel corpora that would result in a smallH , but not for large corpora containing millions ofsentences, such as those described in Section 4.1.Therefore we approximate H with a graph H ?
thatonly contains nodes ?near?
to the node representingthe query phrase.
Specifically, we perform breadth-first search starting from the query node up to adepth d, or until the number of nodes visited in thesearch has reached a maximum of n nodes.
Somenodes at the periphery of H ?
have edges to nodesthat are not in H ?
but are in H .
For a periph-ery node j that has edges to nodes j1, .
.
.
, jk out-side H ?, we create a ?dummy?
node a, and replaceedges (j, j1), .
.
.
, (j, jk) with a single edge (j, a)with weight?kx=1Wj,jx .
We also add edges (a, j)and (a, a) (each with a heuristic weight of 0.5).
Thedummy nodes and their edges approximate the tran-sition probabilities at H ?
?s periphery.
Our empiricalresults show that this approximation works well inpractice.3.2 Graph PruningAfter H ?
is created, we run M independent length-T random walks on it starting from the query nodeto estimate the truncated hitting times of all nodes.Figure 2: Feature nodes representing domain knowledge.Feature nodes are shaded.
The bold node represents aquery phrase.
(a) n-gram nodes (b) ?syntax?
nodes (c)?not-substring/superstring-of?
nodes.A node in H ?
may have many outgoing edges, mostof which may be due to spurious phrase alignments.For efficiency, and to reduce the noise due to spuri-ous edges, we select among a node?s top l outgoingedges with the highest transition probabilities, whendeciding which node to visit next at each step of arandom walkFor each random walk k, we record the first timethat a node j is visited tkj .
Using Equation 1, we es-timate the truncated hitting time of each node.
Thenwe remove nodes (and their associated edges) thatare far from the query node, i.e., with times equalto T .
Such nodes either are not visited in any of therandom walks, or are always visited for the first timeat step T .3.3 Adding Domain KnowledgeNext we add nodes representing domain knowledgeto the pruned graph.
In this version of HTP, we im-plemented three types of feature nodes.First, we have n-gram nodes.
These nodes cap-ture the domain knowledge that phrases containingmany words in common are likely to be paraphrases.For each 1 to 4-gram that appears in English phrases,we create an n-gram node a.
We add directed edges(a, j) and (j, a) if node j represents an Englishphrase containing n-gram a.
For example, in Fig-ure 2(a), ?reach the objective?
is connected to ?ob-148jective?
because it contains that unigram.
Note thatsuch nodes create short paths between nodes withmany n-grams in common, thereby reducing the hit-ting times between them.Second, we have ?syntax?
nodes, which repre-sent syntactic classes of the start and end words ofEnglish phrases.
We created classes such as inter-rogatives (?whose?, ?what?, ?where?, etc.
), articles(?the?, ?a?, ?an?
), etc.
For each class c, we cre-ate syntax nodes ac and a?c to respectively representthe conditions that a phrase begins and ends with aword in class c. Directed edges (ac, j) and (j, ac)are added if node j starts with a word in class c (sim-ilarly we add (a?c, j) and (j, a?c) if it ends with a wordin class c).
For example, in Figure 2(b), ?the objec-tive is?
is linked to ?starts with article?
because itbegins with ?the?.
These syntax nodes allow HTP tocapture broad commonalities about structural distri-bution, without requiring syntactic equivalence as inCallison-Burch 2008 (or the use of a parser).Third, we have ?not-substring/superstring-of?nodes.
We observed that many English phrases (e.g.,?reach the objective?
and ?reach the?)
that are super-strings or substrings of each other tend to be alignedto several shared non-English phrases in the bilin-gual parallel corpora used in our experiments.
Mostsuch English phrase pairs are not paraphrases, butthey are linked by many short paths via their com-mon aligned foreign phrase, and thus have smallhitting times.
To counteract this, we create a ?not-substring/superstring-of?
node a.
The query node iis always connected to a via edges (i, a) and (a, i).We add edges (a, j) and (j, a) if English phrase jis not a substring or superstring of the query phrase(see Figure 2(c)).With the addition of the above, each node rep-resenting an English phrase can have four kindsof outgoing edges: edges to foreign phrase nodes,and edges to the three kinds of feature nodes.
Letfphrase, fngram, fsyntax, fsubstring denote the distri-bution of transition probabilities among the fourkinds of outgoing edges.
Note that fphrase +fngram + fsyntax + fsubstring = 1.0.
These valuesare user-specified or can be set with tuning data.
Anoutgoing edge from English phrase node i that orig-inally had weight (transition probability) Wij willnow have weight Wij ?
fphrase.
All k edges from ito n-gram nodes will have weight fngramk .
Likewisefor edges to the other two kinds of feature nodes.Each of the k outgoing edges from a feature node issimply set to have a weight of 1k .After adding the feature nodes, we again run Mindependent length-T random walks to estimate thetruncated hitting times of the nodes, and return theEnglish phrase nodes in order of increasing hittingtimes.4 ExperimentsWe conducted experiments to investigate how HTPcompares with the state of the art, and to evaluatethe contributions of its components.4.1 DatasetWe used the Europarl dataset (Koehn, 2005) forour experiments.
This dataset contains Englishtranscripts of the proceedings of the EuropeanParliament, and their translations into 10 otherEuropean languages.
In the dataset, there areabout a million sentences per language, and En-glish sentences are aligned with sentences in theother languages.
Callison-Burch (2008) alignedEnglish phrases with phrases in each of theother languages using Giza++ (Och and Ney,2004).
We used his English-foreign phrasal align-ments which are publicly available on the web athttp://ironman.jhu.edu/emnlp08.tar.
In addition, wepaired sentences of different non-English languagesthat correspond to the same English sentence, andaligned the phrases using 5 iterations of IBM model1 in each direction, followed by 5 iterations of HMMalignment with paired training using the algorithmdescribed in Liang et al (2006).
We further used thetechnique of Chen et al (2009) to remove a phrasealignment F -G (where F and G are phrases in dif-ferent foreign languages) if it was always alignedto different phrases in a third ?bridge?
foreign lan-guage.
As observed by Chen et al, this helped toremove spurious alignments.
We used Finnish as thebridge language; when either F or G is Finnish, weused Spanish as the bridge language; when F andG were Finnish and Spanish, we used English asthe bridge language.
In our experiments, we usedphrases of length 1 to 4 of the following six lan-guages: English, Danish, German, Spanish, Finnish,149and Dutch.
All the phrasal alignments between eachpair of languages (15 in total) were used as input toHTP and its comparison systems.
A small subset ofthe remaining phrase alignments were used for tun-ing parameters.4.2 SystemsWe compared HTP to the state-of-the-art SBP sys-tem (Callison-Burch, 2008).
We also investigatedthe contribution of the feature nodes by running HTPwithout them.
In addition, we ran HTP on a bipartitegraph, i.e., one created from English-foreign phrasealignments only without any phrase alignments be-tween foreign languages.We used Callison-Burch (2008)?s implemen-tation of SBP that is publicly available athttp://ironman.jhu.edu/emnlp08.tar.
SBP is basedon BCB (Bannard and Callison- Burch, 2005) whichcomputes the probability that English phrase E?
is aparaphrase of E using the following formula:P (E?|E) ?
?C?C?F?CP (E?|F )P (F |E) (2)where C is set of bilingual parallel corpora, and F isa foreign language phrase.
Representing phrases asnodes, and viewing P (E?|F ) and P (F |E) as tran-sition probabilities of edges (F,E?)
and (E,F ), wesee that BCB is summing over the transition prob-abilities of all length-two paths between E and E?.All E?
paraphrases of E can then be ranked in or-der of decreasing probability as given by Equation 2.The SBP system modifies Equation 2 to incorporatesyntactic information, thus:P (E?|E) ?1|C|?C?C?F?CP (E?|F, synE))P (F |E, synE) (3)where synE is the syntax of phrase E, andP (E?|F, synE)) = 0 ifE?
is not of the same syntac-tic category.
From Equation 3, we can see that SBPconstrains E?
to have the same syntactic structureas E. To obtain the syntactic structure of each En-glish phrase, each English sentence in every parallelcorpus has to be parsed to obtain its parse tree.
AnEnglish phrase can have several syntactic structuresbecause different parse trees can have the phrase astheir leaves, and in each of these, SBP associates theTable 2: Scoring Standards.0 Clearly wrong; grammatically incorrect, ordoes not preserve meaning1 Minor grammatical errors (e.g., subject-verbdisagreement or wrong tense), or meaning islargely preserved but not completely2 Totally correct; grammatically correct andmeaning is preservedphrase with all subtrees that have the phrase as theirleaves.
SBP thus offers several ways of choosingwhich syntactic structure a phrase should be asso-ciated with.
In our experiments, we used the bestperforming method of averaging Equation 3 over allsyntactic structures that E is associated with.4.3 MethodologyTo evaluate performance, we used 33,216 En-glish translations from the Linguistic Data Con-sortium?s Multiple Translation Chinese (MTC) cor-pora (Huang et al, 2002).
We randomly selected100 1- to 4-grams that appeared in both Europarland MTC sentences (excluding stop words, num-bers, and phrases containing periods and commas).For each of those 100 phrases, we randomly se-lected a MTC sentence containing that phrase.
Wethen replaced the phrase in the sentence with eachparaphrase output by the systems, and evaluated thecorrectness of the paraphrase in the context of thesentence.
We had two volunteers manually scorethe paraphrases on a 3-point scale (Table 2), usinga simplified version of the scoring system used byCallison-Burch (2008).
We deemed a paraphraseto be correct if it was scored 1 or 2, and wrongif it was scored 0.
Evaluation was blind, and theparaphrases were presented randomly to the volun-teers.
The Kappa measure of inter-annotator agree-ment was 0.62, which indicates substantial agree-ment between the evaluators.
We took the averagescore for each paraphrase.The parameters used for HTP were as follows(see Table 1 for parameter descriptions): d =6, n = 50, 000,m = 1, 000, 000, T = 10, ?
=0.05, l= 20, fphrase = 0.1, fngram = 0.1, fsyntax =0.4, fsubstring = 0.4.
(?
0.03 with these values ofn,m, T, and ?.
)150Table 3: HTP vs. SBP.HTP SBPCorrect top-1 paraphrases 71% 53%Correct top-k paraphrases 54% 39%Count of correct paraphrases 420 145Correct paraphrases 43% 39%Table 4: HTP vs. HTP without feature nodes.HTP HTP-NoFeatNodesCorrect top-1 paraphrases 61% 41%Correct top-k paraphrases 43% 29%Count of correct paraphrases 420 283Correct paraphrases 43% 29%4.4 ResultsHTP versus SBP.
Comparison between HTP andSBP is complicated by the fact that the two systemsdid not output the same number of paraphrases forthe 100 query phrases.
HTP output paraphrases forall the query phrases, but SBP only did so for 49query phrases.
Of those 49 query phrases, HTP re-turned at least as many paraphrases as SBP, and formany it returned more.To provide a fair comparison, we present resultsboth for these 49 query phrases, and for all para-phrases returned by each of the systems.
The up-per half of Table 3 shows results for the 49 queryphrases.
The first row of Table 3 reports the per-centage of top-1 paraphrases from this set that arecorrect, while the second row reports the percentageof correct top-k paraphrases from this set, where k isthe number of queries returned by SBP, and is lim-ited to at most 10.
The value of k may differ foreach query: if SBP and HTP return 3 and 20 para-phrases respectively, we only consider the top 3.
Onthe third and fourth rows, we present the numberof correct paraphrases and the percentage of correctparaphrases among the top 10 paraphrases returnedby HTP for all 100 queries and the correspondingfigures for the 49 queries for SBP.
(When a sys-tem returned fewer than 10 paraphrases for a query,we consider all the paraphrases for that query.)
Itis evident from Table 3 that HTP consistently out-performs SBP: not only does it return more cor-rect paraphrases overall (420 versus 145), it also hasTable 5: HTP vs. HTP with bipartite graph.HTP HTP-BipartiteCorrect top-1 paraphrases 62% 58%Correct top-k paraphrases 46% 41%Count of correct paraphrases 420 361Correct paraphrases 43% 41%higher precision (43% versus 39%)HTP and SBP respectively took 48 and 468 sec-onds per query on a 3 GHz machine.
The times arenot directly comparable because the systems are im-plemented in different languages (HTP in C# andSBP in Java), and use different data structures.HTP without Feature Nodes.
Both HTP and HTPminus feature nodes output paraphrases for each ofthe 100 query phrases.
Table 4 compares perfor-mance in the same manner as in Table 3, except thatthe ?top-1?
and ?top-k?
results are over all 100 queryphrases.
We see that feature nodes boost HTP?s per-formance, allowing HTP to return more correct para-phrases (420 versus 283), and at higher precision(43% versus 29%).HTP with Bipartite Graph.
Lastly, we investi-gate the contribution of alignments between foreignphrases to HTP?s performance.
HTP-Bipartite refersto HTP that is given a set consisting only of English-foreign phrase alignment as input.
HTP-Bipartitedoes not return paraphrases for 5 query phrases.Thus, in Table 5, the ?top-1?
and ?top-k?
results arefor the 95 query phrases for which both systems re-turn paraphrases.
From the better performance ofHTP, we see that the foreign phrase alignments helpin finding English paraphrases.5 Related WorkRandom walks and hitting times have been suc-cessfully applied to a variety of applications.Brand (2005) has used hitting times for collabora-tive filtering, in which product recommendations tousers are made based on purchase history.
In com-puter vision, hitting times have been used to de-termine object shape from silhouettes (Gorelick etal., 2004), and for image segmentation (Grady andSchwartz, 2006).
In social network analysis, Liben-Nowell and Kleinberg (2003) have investigated the151use of hitting times for predicting relationships be-tween entities.
Recently, Mei et al (2008) have usedthe hitting times of nodes in a bipartite graph cre-ated from search engine query logs to find relatedqueries.
They used an iterative algorithm to computethe hitting time, which converges slowly on largegraphs.
In HTP, we have sought to obviate this issueby using truncated hitting time that can be computedefficiently by sampling random walks.Several approaches have been proposed to learnparaphrases.
Barzilay and Mckeown (2001) acquireparaphrases from a monolingual parallel corpus us-ing a co-training algorithm.
Their co-trained classi-fier determines whether two phrases are paraphrasesof one another using their surrounding contexts.
Linand Pantel (2001) learn paraphrases using the dis-tributional similarity of paths in dependency trees.Ibrahim et al (2003) generalize syntactic paths inaligned monolingual sentence pairs in order to gen-erate paraphrases.
Pang et al (2003) merge parsetrees of monolingual sentence pairs, and then com-press the merged tree into a word lattice that can sub-sequently be used to generate paraphrases.
Recently,Zhao et al (2008) used dependency parses to learnparaphrase patterns that include part-of-speech slots.In other recent work, Das and Smith (2009) use agenerative model for paraphrase detection.
Ratherthan outputing paraphrases of an input phrase, theirsystem detects whether two input sentences are para-phrases of one another.6 Conclusion and Future WorkWe have introduced HTP, a novel approach basedon random walks and hitting times for the learningof paraphrases from bilingual parallel corpora.
HTPworks by converting aligned phrases into a graph,and finding paraphrases that are ?close?
to a phraseof interest.
Compared to previous approaches, HTPis able to find more paraphrases by traversing pathsof lengths longer than 2; utilizes information in theedges between foreign phrase nodes; and allows do-main knowledge to be easily incorporated.
Empir-ical results show its effectiveness in learning newparaphrases.As future work, we plan to learn the distributionof weights on edges to phrase nodes and featurenodes automatically from data, rather than tuningthem manually, and to develop a probabilistic modelsupporting HTP.
We intend also to apply HTP tolearning paraphrases in languages other than Englishand investigate the impact of the learned paraphraseson resource-sparse machine translation systems.AcknowledgmentsThis work was done while the first author was anintern at Microsoft Research.
We would like tothank Xiaodong He, Jianfeng Gao, Chris Quirk,Kristina Toutanova, Bob Moore, and other mem-bers of the MSR NLP group, along with DengyongZhou (TMSN) for their insightful comments and as-sistance in the course of this project.ReferencesDavid Aldous and Jim Fill.
2001.
ReversibleMarkov Chains and Random Walks on Graphs.http://www.stat.berkeley.edu/~aldous/RWG/book.html.Colin Bannard and Chris Callison-Burch.
2005.
Para-phrasing with bilingual parallel corpora.
In Proceed-ings of the 43rd Annual Meeting of the ACL, pages597?604.Regina Barzilay and Lillian Lee.
2003.
Learning toparaphrase: an unsupervised approach using multiple-sequence alignment.
In Proceedings of HLT/NAACL,pages 16?23.Regina Barzilay and Kathleen McKeown.
2001.
Extract-ing paraphrases from a parallel corpus.
In Proceedingsof the 39th Annual Meeting of the ACL, pages 50?57.Regina Barzilay, Kathleen McKeown, and Michael El-hadad.
1999.
Information fusion in the context ofmulti-document summarization.
In Proceedings of the37th Annual Meeting of the ACL, pages 550?557.Matthew Brand.
2005.
A random walks perspective onmaximizing satisfaction and profit.
In Proceedings ofthe 8th SIAM Conference on Optimization.Chris Callison-Burch, Philipp Koehn, and Miles Os-borne.
2006.
Improved statistical machine translationusing paraphrases.
In Proceedings of HLT/NAACL,pages 17?24.Chris Callison-Burch.
2008.
Syntactic constraints onparaphrases extracted from parallel corpora.
In Pro-ceedings of EMNLP, pages 196?205.Yu Chen, Martin Kay, and Andreas Eisele.
2009.
Inter-secting multilingual data for faster and better statisticaltranslations.
In Proceedings of HLT/NAACL.Dipanjan Das and Noah A. Smith.
2009.
Paraphraseidentification as probabilistic quasi-synchronousrecognition.
In Proceedings of the Joint Conference152of the Annual Meeting of the Association for Com-putational Linguistics and the International JointConference on Natural Language Processing.Lena Gorelick, Meirav Galun, Eitan Sharon, RonenBasri, and Achi Brandt.
2004.
Shape representationand classification using the Poisson equation.
In Pro-ceedings of the Conference on Computer Vision andPattern Recognition.Leo Grady and Eric L. Schwartz.
2006.
Isoperimet-ric graph partitioning for image segmentation.
IEEETransactions on Pattern Analysis and Machine Intelli-gence, 28:469?475.Shudong Huang, David Graff, and George Doddington.2002.
Multiple-translation Chinese corpus.
LinguisticData Consortium, Philadelphia.Ali Ibrahim, Boris Katz, and Jimmy Lin.
2003.
Ex-tracting structural paraphrases from aligned monolin-gual corpora.
In Proceedings of the 2nd InternationalWorkshop on Paraphrasing, pages 57?64.Lidija Iordanskaja, Richard Kittredge, and AlainPolgue`re.
1991.
Lexical selection and paraphrase ina meaning-text generation model.
In Ce?cile L. Paris,William R. Swartout, and William C. Mann, editors,Natural Language Generation in Artificial Intelligenceand Computational Linguistics.
Kluwer Academic.David Kauchak and Regina Barzilay.
2006.
Para-phrasing for automatic evaluation.
In Proceedings ofHLT/NAACL, pages 455?462.Philipp Koehn.
2005.
Europarl: A parallel corpus forstatistical machine translation.
In Proceedings of the10th Machine Translation Summit.Nils Lenke.
1994.
Anticipating the reader?s problemsand the automatic generation of paraphrases.
In Pro-ceedings of the 15th Conference on ComputationalLinguistics, pages 319?323.Percy Liang, Ben Taskar, and Dan Klein.
2006.
Align-ment by agreement.
In Proceedings of HLT/NAACL,pages 104?111.David Liben-Nowell and Jon Kleinberg.
2003.
The linkprediction problem for social networks.
In Proceed-ings of the 12th International Conference on Informa-tion and Knowledge, pages 556?559.Dekang Lin and Patrick Pantel.
2001.
Discovery of in-ference rules for question answering.
In Proceedingsof ACM SIGKDD Conference on Knowledge Discov-ery and Data Mining, pages 323?328.La?szlo?
Lova?sz.
1996.
Random walks on graphs: A sur-vey.
In D. Miklo?s, V. T. So?s, and T. Szo?nyi, editors,Combinatorics, Paul Erdo?s is Eighty, Vol.
2, pages353?398.Nitin Madnani, Necip Fazil Ayan, Philip Resnik, andBonnie J. Dorr.
2007.
Using paraphrases for param-eter tuning in statistical machine translation.
In Pro-ceedings of the 2nd Workshop on Statistical MachineTranslation, pages 120?127.Kathleen R. McKeown, Regina Barzilay, David Evans,Vasileios Hatzivassiloglou, Judith L. Klavans, AniNenkova, Carl Sable, Barry Schiffman, and SergeySigelman.
2002.
Tracking and summarizing news ona daily basis with Columbia?s Newsblaster.
In Pro-ceedings of the 2nd International Conference on HLTResearch, pages 280?285.Qiaozhu Mei, Dengyong Zhou, and Kenneth Church.2008.
Query suggestion using hitting time.
In Pro-ceeding of the 17th ACM Conference on Informationand Knowledge Management, pages 469?478.Franz J. Och and Hermann Ney.
2004.
The alignmenttemplate approach to statistical machine translation.Computational Linguistics, 30:417?449.Bo Pang, Kevin Knight, and Daniel Marcu.
2003.Syntax-based alignment of multiple translations: Ex-tracting paraphrases and generating new sentences.
InProceedings of HLT/NAACL, pages 102?109.Chris Quirk, Chris Brockett, and William B. Dolan.2004.
Monolingual machine translation for paraphrasegeneration.
In Proceedings of EMNLP, pages 142?149.Deepak Ravichandran and Eduard Hovy.
2002.
Learningsurface text patterns for a question answering system.In Proceedings of the 40th Annual Meeting of the ACL,pages 41?47.Stefan Reizler, Alexander Vasserman, Ioannis Tsochan-taridis, Vibhu Mittal, and Yi Liu.
2007.
Statisticalmachine translation for query expansion in answer re-trieval.
In Proceedings of the 45th Annual Meeting ofthe ACL.Purnamrita Sarkar and Andrew W. Moore.
2007.A tractable approach to finding closest truncated-commute-time neighbors in large graphs.
In Proceed-ings of the 23th Conference on Uncertainty in ArtificialIntelligence.Purnamrita Sarkar, Andrew W. Moore, and Amit Prakash.2008.
Fast incremental proximity search in largegraphs.
In Proceedings of the 25th International Con-ference on Machine Learning.Manfred Stede.
1999.
Lexical Semantics and Knowl-edge Representation in Multilingual Text Generation.Kluwer Academic Publishers.Shiqi Zhao, Haifeng Wang, Ting Liu, and Sheng Li.2008.
Pivot approach for extracting paraphrase pat-terns from bilingual corpora.
In Proceedings of ACL.153
