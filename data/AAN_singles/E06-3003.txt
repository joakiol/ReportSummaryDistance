An Approach to Summarizing Short StoriesAnna KazantsevaThe School of Information Technology and EngineeringUniversity of Ottawaankazant@site.uottawa.caAbstractThis paper describes a system that pro-duces extractive summaries of shortworks of literary fiction.
The ultimatepurpose of produced summaries is de-fined as helping a reader to determinewhether she would be interested in read-ing a particular story.
To this end, thesummary aims to provide a reader withan idea about the settings of a story (suchas characters, time and place) without re-vealing the plot.
The approach presentedhere relies heavily on the notion of as-pect.
Preliminary results show an im-provement over two na?ve baselines: alead baseline and a more sophisticatedvariant of it.
Although modest, the resultssuggest that using aspectual informationmay be of help when summarizing fic-tion.
A more thorough evaluation involv-ing human judges is under way.1 IntroductionIn the course of recent years the scientificcommunity working on the problem of automatictext summarization has been experiencing anupsurge.
A multitude of different techniques hasbeen applied to this end, some of the moreremarkable of them being (Marcu, 1997; Mani etal.
1998; Teufel and Moens, 2002; Elhadad et al,2005), to name just a few.
These researchersworked on various text genres: scientific andpopular scientific articles (Marcu, 1997; Mani etal., 1998), texts in computational linguistics(Teufel and Moens, 2002), and medical texts(Elhadad et al, 2002).
All these genres are ex-amples of texts characterized by rigid structure,relative abundance of surface markers andstraightforwardness.
Relatively few attemptshave been made at summarizing less structuredgenres, some of them being dialogue and speechsummarization (Zechner, 2002; Koumpis et al2001).
The issue of summarizing fiction remainslargely untouched, since a few very thoroughearlier works (Charniak, 1972; Lehnert, 1982).The work presented here seeks to fill in this gap.The ultimate objective of the project is statedas follows: to produce indicative summaries ofshort works of fiction such that they be helpful toa potential reader in deciding whether she wouldbe interested in reading a particular story or not.To this end, revealing the plot was deemed un-necessary and even undesirable.
Instead, the cur-rent approach relies on the following assumption:when a reader is presented with an extractedsummary outlining the general settings of a story(such as time, place and who it is about), she willhave enough information to decide how inter-ested she would be in reading a story.
For exam-ple, a fragment of such a summary, produced byan annotator for the story The Cost of Kindnessby Jerome K. Jerome is presented in Figure 1.The plot, which is a tale of how one local familydecides to bid a warm farewell to Rev.
Crackle-thorpe and causes the vicar to change his mindand remain in town, is omitted.The data used in the experiments consisted of23 short stories, all written in XIX ?
early XXcentury by main-stream authors such as Kathe-rine Mansfield, Anton Chekhov, O.Henry, Guyde Maupassant and others (13 authors in total).The genre can be vaguely termed social fictionwith the exception of a few fairy-tales.
Suchvagueness as far as genre is concerned was de-liberate, as the author wished to avoid producinga system relying on cues specific to a particulargenre.
Average length of a story in the corpus is3,333 tokens (approximately 4.5 letter-sizedpages) and the target compression rate is 6%.In order to separate the background of a storyfrom events, this project relies heavily on thenotion of aspect (the term is explained in Section3.1).
Each clause of every sentence is describedin terms of aspect-related features.
This represen-tation is then used to select salient descriptivesentences and to leave out those which describeevents.55The organization of the paper follows theoverall architecture of the system.
Section 2 pro-vides a generalized overview of the pre-processing stage of the project, during whichpronominal and nominal anaphoric references(the term is explained in Section 2) were re-solved and main characters were identified.
Sec-tion 3 briefly reviews the concept of aspect,gives an overview of the system and provides thelinguistic motivation behind it.
Section 4 de-scribes the classification procedures (machinelearning and manual rule creation) used to distin-guish between descriptive elements of a storyand passages that describe events.
It also reportsresults.
Section 5 draws some conclusions andoutlines possible directions in which this workmay evolve.2 Data Pre-ProcessingBefore working on selecting salient descriptivesentences, the stories of the training set were ana-lyzed for presence of surface markers denotingcharacters, locations and temporal anchors.
Tothis end, the GATE Gazetteer (Cunningham etal., 2002) was used, and only entities recognizedby it automatically were considered.The findings were as follows.
Each story con-tained multiple mentions of characters (an aver-age of 64 mentions per story).
Yet only 22 loca-tion markers were found, most of these beingstreet names.
The 22 markers were found in 10out of 14 stories, leaving 4 stories without anyidentifiable location markers.
Only 4 temporalanchors were identified in all 14 stories: 2 abso-lute (such as years) and 2 relative (names ofholidays).
These findings support the intuitiveidea that short stories revolve around their char-acters, even if the ultimate goal is to show a lar-ger social phenomenon.Due to this fact, the data was pre-processed insuch a way as to resolve pronominal and nominalanaphoric references to animate entities.
Theterm anaphora can be informally explained as away of mentioning a previously encountered en-tity without naming it explicitly.
Consider exam-ples 1a and 1b from The Gift of the Magi by O.Henri.
1a is an example of pronominal anaphora,where the noun phrase (further NP) Della is re-ferred to as an antecedent and both occurrencesof the pronoun her as anaphoric expressions orreferents.
Example 1b illustrates the concept ofnominal anaphora.
Here the NP Dell is the ante-cedent and my girl is the anaphoric expression(in the context of this story Della and the girl arethe same person).
(1a) Della finished her cry and attended toher cheeks with the powder rag.
(1b) "Don't make any mistake, Dell," he said,?about me.
I don't think there's anything[?]
that could make me like my girl anyless.The author created a system that resolved 1stand 3rd person singular pronouns (I, me, my, he,his etc.)
and singular nominal anaphoric expres-sions (e.g.
the man, but not men).
The systemwas implemented in Java, within the GATEframework, using Connexor Machinese Syntaxparser (Tapanainen and J?rvinen, 1997).A generalized overview of the system is pro-vided below.
During the first step, the docu-ments were parsed using Connexor MachineseSyntax parser.
The parsed data was then for-warded to the Gazetteer in GATE, which recog-nized nouns denoting persons.
The original ver-sion of the Gazetteer recognized only named en-tities and professions, but the Gazetteer was ex-tended to include common animate nouns such asman, woman, etc.
As the next step, an imple-mentation based on a classical pronoun resolu-tion algorithm (Lappin and Leass, 1994) was ap-plied to the texts.
Subsequently, anaphoric nounphrases were identified using the rules outlinedFigure 1.
A fragment of a desired summary for The Cost of Kindness by Jerome K. Jerome.The Cost of KindnessJerome K. Jerome (1859-1927)Augustus Cracklethorpe would be quitting Wychwood-on-the-Heath the following Monday, never to setfoot--so the Rev.
Augustus Cracklethorpe himself and every single member of his congregation hoped sin-cerely--in the neighbourhood again.
[?]
The Rev.
Augustus Cracklethorpe, M.A., might possibly have beenof service to his Church in, say, some East-end parish of unsavoury reputation, some mission station faradvanced amid the hordes of heathendom.
There his inborn instinct of antagonism to everybody and every-thing surrounding him, his unconquerable disregard for other people's views and feelings, his inspired con-viction that everybody but himself was bound to be always wrong about everything, combined with deter-mination to act and speak fearlessly in such belief, might have found their uses.
In picturesque littleWychwood-on-the-Heath [?]
these qualities made only for scandal and disunion.56in (Poesio and Vieira, 2000).
Finally, these ana-phoric noun phrases were resolved using a modi-fied version of (Lappin and Leass, 1994), ad-justed to finding antecedents of nouns.A small-scale evaluation based on 2 short sto-ries revealed results shown in Table 1.
After re-solving anaphoric expressions, characters that arecentral to the story were selected based on nor-malized frequency counts.3 Selecting Descriptive Sentences UsingAspectual Information3.1 Linguistic definition of aspectIn order to select salient sentences that set out thebackground of a story, this project relied on thenotion of aspect.
For the purposes of this paperthe author uses the term aspect to denote thesame concept as what (Huddleston and Pullum,2002) call the situation type.
Informally, it can beexplained as a characteristic of a clause thatgives an idea about the temporal flow of an eventor state being described.A general hierarchy of aspectual classifi-cation based on (Huddleston and Pullum, 2002)is shown in Figure 2 with examples for eachtype.
In addition, aspectual type of a clause maybe altered by multiplicity, e.g.
repetitions.
Con-sider examples 2a and 2b.
(2a) She read a book.
(2b) She usually read a book a day.
(e.g.
Sheused to read a book a day).Example 2b is referred to as serial situation(Huddleston and Pullum, 2002).
It is consideredto be a state, even though a single act of readinga book would constitute an event.Intuitively, stative situations (especially serialones) are more likely to be associated with de-scriptions; that is with things that are, or thingsthat were happening for an extended period oftime (consider He was a tall man.
vs.
He openedthe window.
).The rest of Section 3 describes theapproach used for identifying single and serialstative clauses and for using them to constructsummaries.3.2 Overall system designSelection of the salient background sentenceswas conducted in the following manner.
Firstly,the pre-processed data (as outlined in Section 2)was parsed using Connexor Machinese Syntaxparser.
Then, sentences were recursively splitinto clauses.
For the purposes of this project aclause is defined as a main verb with all its com-plements, including subject, modifiers and theirsub-trees.Subsequently, two different representationswere constructed for each clause: one fine-grained and one coarse-grained.
The main differ-ence between these two representations was inthe number of attributes and in the cardinality ofthe set of possible values, and not in how muchand what kind of information they carried.
Forinstance, the fine-grained dataset had 3 differentfeatures with 7 possible values to carry tense-related information: tense, is_progressive andis_perfect, while the coarse-grained dataset car-ried only one binary feature,is_simple_past_or_present.Two different approaches for selecting de-scriptive sentences were tested on each of therepresentations.
The first approach used machinelearning techniques, namely C5.0 (Quinlan,1992) implementation of decision trees.
The sec-ond approach consisted of applying a set ofmanually created rules that guided the classifica-tion process.
Motivation for features used in eachdataset is given in Section 3.3.
Both approachesand preliminary results are discussed in Sections4.1 - 4.4.The part of the system responsible for select-ing descriptive sentences was implemented inPython.3.3 Feature selection: description and moti-vationFigure 2.
Aspectual hierarchy after (Hud-dleston and Pullum, 2002).Table 1.
Results of anaphora resolution.Type ofanaphoraAll Correct Incor-rectErrorrate, %Pronominal 597 507 90 15.07Nominal 152 96 56 36.84Both 749 603 146 19.4957Features for both representations were selectedbased on one of the following criteria:(Criterion 1) a clause should ?talk?
about im-portant things, such as characters or locations(Criterion 2) a clause should contain back-ground descriptions rather then eventsThe number of features providing informationtowards each criterion, as well as the number ofpossible values, is shown in Table 2 for bothrepresentations.The attributes contributing towards Criterion 1can be divided into character-related and loca-tion-related.Character-related features were designed so asto help identify sentences that focused on charac-ters, not just mentioned them in passing.
Theseattributes described whether a clause contained acharacter mention and what its grammaticalfunction was (subject, object, etc.
), whether sucha mention was modified and what was the posi-tion of a parent sentence relative to the sentencewhere this character was first mentioned (intui-tively, earlier mentions of characters are morelikely to be descriptive).Location-related features in both datasets de-scribed whether a clause contained a locationmention and whether it was embedded in aprepositional phrase (further PP).
The rationalebehind these attributes is that location mentionsare more likely to occur in PPs, such as from theArc de Triomphe, to the Place de la Concorde.In order to meet Criterion 2 (that is, to selectdescriptive sentences) a number of aspect-relatedfeatures were calculated.
These features wereselected so as to model characteristics of a clausethat help determine its aspectual class.
The char-acteristics used were default aspect of the mainverb of a clause, tense, temporal expressions,semantic category of a verb, voice and someproperties of the direct object.
Each of thesecharacteristics is listed below, along with motiva-tion for it, and information about how it wascalculated.It must be mentioned that several researcherslooked into determining automatically varioussemantic properties of verbs, such as (Siegel,1998; Merlo et al, 2002).
Yet these approachesdealt with properties of verbs in general and notwith particular usages in the context of concretesentences.Default verbal aspect.
A set of verbs, referredto as stative verbs, tends to produce mostly sta-tive clauses.
Examples of such verbs include be,like, feel, love, hate and many others.
A commonproperty of such verbs is that they do not readilyyield a progressive form (Vendler, 1967; Dowty,1979).
Consider examples 3a and 3b.
(3a) She is talking.
(a dynamic verb talk)(3b) *She is liking the book.
(a stative verblike)The default aspectual category of a verb was ap-proximated using Longman Dictionary of Con-temporary English (LDOCE).
Verbs marked inLDOCE as not having a progressive form wereconsidered stative and all others ?
dynamic.
Thisinformation was expressed in both datasets as 1binary feature.Grammatical tense.
Usually, simple tensesare more likely to be used in stative or habitualsituations than progressive or perfect tenses.
Infact, it is considered to be a property of stativeclauses that they normally do not occur in pro-gressive (Vendler, 1967; Huddleston and Pullum,2002).
Perfect tenses are feasible with stativeclauses, yet less frequent.
Simple present is onlyfeasible with states and not with events (Huddle-ston and Pullum, 2002) (see examples 4a and4b).
(4a) She likes writing.
(4b) *She writes a book.
(e.g.
now)In the fine-grained dataset this information wasexpressed using 3 features with 7 possible valuesTable 2.
Description of the features in both datasetsFine-grained dataset Coarse-grained datasetType of features Number of fea-turesNumber of val-uesNumber of fea-turesNumber of valuesCharacter-related 9 16 4 6Aspect-related 12 92 8 16Location-related 2 4 2 4Others 4 9 3 4All 27 121 17 3058(whether a clause is in present, past or futuretense, whether it is progressive and whether it isperfective).
In the coarse-grained dataset, thisinformation was expressed using 1 binary fea-ture: whether a clause is in simple past or presenttense.Temporal expressions.
Temporal markers(often referred to as temporal adverbials), such asusually, never, suddenly, at that moment andmany others are widely employed to mark theaspectual type of a sentence (Dowty, 1982;Harkness, 1987; By, 2002).
Such markers pro-vide a wealth of information and often unambi-guously signal aspectual type.
For example:(5a) She read a lot tonight.
(5b) She always read a lot.
(Or She used toread a lot.
)Yet, such expressions are not easy to captureautomatically.
In order to use the informationexpressed in temporal adverbials, the author ana-lyzed the training data for presence of such ex-pressions and found 295 occurrences in 10 sto-ries.
It appears that this set could be reduced to95 templates in the following manner.
For exam-ple, the expressions this year, next year, that longyear could all be reduced to a template<some_expression> year.
Each template is char-acterized by 3 features: type of the temporal ex-pression (location, duration, frequency, enact-ment) (Harkness, 1987); magnitude (year, day,etc.
); and plurality (year vs. years).
The fine-grained dataset contained 3 such features with 14possible values (type of expression, its magni-tude and plurality).
The coarse-grained datasetcontained 1 binary feature (whether there was anexpression of a long period of time).Verbal semantics.
Inherent meaning of a verbalso influences the aspectual type of a givenclause.
(6a) She memorized that book by heart.
(anevent)(6b) She enjoyed that book.
(a state)Not surprisingly, this information is very difficultto capture automatically.
Hoping to leverage it,the author used semantic categorization of the3,000 most common English verbs as describedin (Levin, 1993).
The fine-grained dataset con-tained a feature with 49 possible values that cor-responded to the top-level categories described in(Levin, 1993).
The coarse-grained dataset con-tained 1 binary feature that carried this informa-tion.
Verbs that belong to more than one categorywere manually assigned to a single category thatbest captured their literal meaning.Voice.
Usually, clauses in passive voice onlyoccur with events (Siegel, 1998).
Both datasetscontained 1 binary feature to describe this infor-mation.Properties of direct object.
For some verbsproperties of direct object help determinewhether a given clause is stative or dynamic.
(7a) She wrote a book.
(event)(7b) She wrote books.
(state)The fine-grained dataset contained 2 binary fea-tures to describe whether direct object is definiteor indefinite and whether it is plural.
The coarse-grained dataset contained no such informationbecause it appeared that this information was notcrucial.Several additional features were present inboth datasets that described overall characteris-tics of a clause and its parent sentence, such aswhether these were affirmative, their index in thetext, etc.
The fine-grained dataset contained 4such features with 9 possible values and thecoarse-grained dataset contained 3 features with7 values.4 Experiments4.1 Experimental settingThe data used in the experiments consisted of 23stories split into a training set (14 stories) and atesting set (9 stories).
Each clause of every storywas annotated by the author of this paper assummary-worthy or not.
Therefore, the classifi-cation process occurred at the clause-level.
Yet,summary construction occurred at the sentence-level, that is if one clause in a sentence was con-sidered summary-worthy, the whole sentencewas also considered summary-worthy.
Becauseof this, results are reported at two levels: clauseand sentence.
The results at the clause-level aremore appropriate to judge the accuracy of theclassification process.
The results at the sentencelevel are better suited for giving an idea abouthow close the produced summaries are to theirannotated counterparts.The training set contained 5,514 clauses andthe testing set contained 4,196 clauses.
The targetcompression rate was set at 6% expressed interms of sentences.
This rate was selected be-cause it approximately corresponds to the aver-age compression rate achieved by the annotator59Table 3.
Results obtained using rules (summary-worthy class)Dataset Level Preci-sion,%Recall,%F-score,%Kappa Overall error rate,%(both classes)Baseline LEAD Clause 19.92 23.39 21.52 16.85 8.87BaselineLEAD CHARClause 8.93 25.69 13.25 6.01 17.47Fine-grained Clause 34.77 40.83 37.55 33.84 17.73Coarse-grained Clause 32.00 47.71 38.31 34.21 7.98Baseline LEAD Sent.
23.57 24.18 23.87 19.00 9.24BaselineLEAD CHARSent.
22.93 23.53 23.23 18.31 9.24Fine-grained Sent.
41.40 42.48 41.94 38.22 6.99Coarse-grained Sent.
40.91 41.18 41.04 37.31 7.03(5.62%).
The training set consisted of 310 posi-tive examples and 5,204 negative examples, andthe testing set included 218 positive and 3,978negative examples.Before describing the experiments and dis-cussing results, it is useful to define baselines.The author of this paper is not familiar with anycomparable summarization experiments and forthis reason was unable to use existing work forcomparison.
Therefore, a baseline needed to bedefined in different terms.
To this end, two na?vebaselines were computed.Intuitively, when a person wishes to decidewhether to read a certain book or not, he opens itand flips through several pages at the beginning.Imitating this process, a simple lead baselineconsisting of the first 6% of the sentences in astory was computed.
It is denoted LEAD in Ta-bles 3 and 4.
The second baseline is a slightlymodified version of the lead baseline and it con-sists of the first 6% of the sentences that containat least one mention of one of the importantcharacters.
It is denoted LEAD CHAR in Tables3 and 4.4.2 Experiments with the rulesThe first classification procedure consisted ofapplying a set of manually designed rules to pro-duce descriptive summaries.
The rules were de-signed using the same features that were used formachine learning and that are described in Sec-tion 3.3.Two sets of rules were created: one for thefine-grained dataset and another for the coarse-grained dataset.
Due to space restrictions it is notpossible to reproduce the rules in this paper.
Yet,several examples are given in Figure 4.
(If a rulereturns True, then a clause is considered to besummary-worthy.
)The results obtained using these rules are pre-sented in Table 3.
They are discussed along withthe results obtained using machine learning inSection 4.4.4.3 Experiments with machine learningAs an alternative to rule construction, the authorused C5.0 (Quilan, 1992) implementation of de-cision trees to select descriptive sentences.
Thealgorithm was chosen mainly because of thereadability of its output.
Both training and testingdatasets exhibited a 1:18 class imbalance, which,given a small size of the datasets, needed to becompensated.
Undersampling (randomly remov-ing instances of the majority class) was appliedto both datasets in order to correct class imbal-ance.This yielded altogether 4 different datasets(see Table 4).
For each dataset, the best modelwas selected using 10-fold cross-validation onthe training set.
The model was then tested on thetesting set and the results are reported in Table 4.Figure 4.
Examples of manually composedrules.Rule 1if a clause contains a character mention assubject or object and a temporal expressionof type enactment (ever, never, always)return TrueRule 2if a clause contains a character mention assubject or object and a stative verbreturn TrueRule 3if a clause is in progressive tensereturn False604.4 ResultsThe results displayed in Tables 3 and 4 showhow many clauses (and sentences) selected bythe system corresponded to those chosen by theannotator.
The columns Precision, Recall and F-score show measures for the minority class (sum-mary-worthy).
The columns Overall error rateand Kappa show measures for both classes.Although modest, the results suggest an im-provement over both baselines.
Statistical sig-nificance of improvements over baselines wastested for p = 0.001 for each dataset-approach.The improvements are significant in all cases.The columns F-score in Tables 3 and 4 showf-score for the minority class (summary-worthysentences), which is a measure combining preci-sion and recall for this class.
Yet, this measuredoes not take into account success rate on thenegative class.
For this reason, Cohen?s kappastatistic (Cohen, 1960) was also computed.
Itmeasures the overall agreement between the sys-tem and the annotator.
This measure is shown inthe column named Kappa.In order to see what features were the most in-formative in each dataset, a small experimentwas conducted.
The author removed one featureat a time from the training set and used the de-crease in F-score as a measure of informative-ness.
The experiment revealed that in the coarse-grained dataset the following features were themost informative: 1) the position of a sentencerelative to the first mention of a character; 2)whether a clause contained character mentions;3) voice and 4) tense.
In the fine-grained datasetthe findings were similar: 1) presence of a char-acter mention; 2) position of a sentence in thetext; 3) voice; and 4) tense were more importantthan the other features.It is not easy to interpret these results in anyconclusive way at this stage.
The main weakness,of course, is that the results are based solely onthe annotations of one person while it is gener-ally known that human annotators are likely toexhibit some disagreement.
The second issue liesin the fact that given the compression rate of 6%,and the objective that the summary be indicativeand not informative, more that one ?good?
sum-mary is possible.
It would therefore be desirablethat the results be evaluated not based on overlapwith an annotator (or annotators, for that matter),but on how well they achieve the stated objec-tive.5 ConclusionsIn the immediate future the inconclusiveness ofthe results will be addressed by means of askinghuman judges to evaluate the produced summa-ries.
During this process the author hopes to findout how informative the produced summaries areand how well they achieve the stated objective(help readers decide whether a story is poten-tially interesting to them).
The judges will alsobe asked to annotate their own version of a sum-mary to explore what inter-judge agreementmeans in the context of fiction summarization.More remote plans include possibly tacklingthe problem of summarizing the plot and dealingmore closely with the problem of evaluation inthe context of fiction summarization.Table 4.
Results obtained using machine learning (summary-worthy class)Dataset Training data-setLevel Preci-sion, %Recall,%F-score,%Kap-paOverallerror rate,%Baseline LEAD Clause 19.92 23.39 21.52 16.85 8.87BaselineLEAD CHARClause 8.93 25.69 13.25 6.01 17.47Fine-grained original Clause 28.81 31.19 29.96 25.96 7.58Fine-grained undersampled Clause 39.06 45.87 42.19 38.76 6.53Coarse-grained original Clause 34.38 30.28 32.22 28.73 6.63Coarse-grained undersampled Clause 28.52 33.49 30.80 26.69 7.82Baseline LEAD Sent.
23.57 24.18 23.87 19.00 9.24BaselineLEAD CHARSent.
22.93 23.53 23.23 18.31 9.24Fine-grained original Sent.
38.93 37.91 38.41 34.57 7.22Fine-grained undersampled Sent.
41.4 42.48 41.94 38.22 6.99Coarse-grained original Sent.
42.19 35.29 38.43 34.91 6.72Coarse-grained undersampled Sent.
37.58 38.56 38.06 34.10 7.4661AcknowledgementsThe author would like to express her gratitudeto Connexor Oy and especially to Atro Vouti-lainen for their kind permission to use ConnexorMachinese Syntax parser free of charge for re-search purposes.ReferencesTomas By.
2002.
Tears in the Rain.
Ph.D. thesis, Uni-versity of Sheffield.Eugene Charniak.
1972.
Toward a Model of Chil-dren?s Story Comprehension.
Ph.D. thesis, Massa-chusetts Institute of Technology.Jacob Cohen.
1960.
A coefficient of agreement fornominal scales, Educational and PsychologicalMeasurement, (20): 37?46.Hamish Cunningham, Diana Maynard, KalinaBontcheva, Valentin Tablan.
2002.
GATE: AFramework and Graphical Development Environ-ment for Robust NLP Tools and Applications.
Pro-ceedings of the 40th Anniversary Meeting of theAssociation for Computational Linguistics(ACL'02), Philadelphia.David Dowty.
1982.
Tense, Time Adverbs, and Com-positional Semantic Theory.
Linguistics and Phi-losophy, (5), p. 23-59.David Dowty.
1979.
Word Meaning and MontagueGrammar.
D. Reidel Publishing Company,Dordrecht.Noemie Elhadad, Min-Yen Kan, Judith Klavans, andKathleen McKeown.
2005.
Customization in a uni-fied framework for summarizing medical literature.Artificial Intelligence in Medicine 33(2): 179-198.Janet Harkness.
1987.
Time Adverbials in English andReference Time.
In Alfred Schopf (ed.
), Essays onTensing in English, Vol.
I: Reference Time, Tenseand Adverbs, p. 71-110.
T?bingen: Max Niemeyer.Rodney Huddleston and Geoffrey Pullum.
2002.
TheCambridge Grammar of the English Language Us-age, p. 74-210.
Cambridge University Press.Konstantinos Koumpis, Steve Renals, and MahesanNiranjan.
2001.
Extractive summarization ofvoicemail using lexical and prosodic feature subsetselection.
In Proeedings of Eurospeech, p. 2377?2380, Aalborg, Denmark.Herbert Leass and Shalom Lappin.
1994.
An algo-rithm for Pronominal Anaphora Resolution.
Com-putational Linguistics, 20(4): 535-561.Wendy Lehnert.
1982.
Plot Units: A Narrative Sum-marization Strategy.
In W. Lehnert and M.
Ringle(eds.).
Strategies for Natural Language Processing.Erlbaum, Hillsdale, NJ.Beth Levin.
1993.
English Verb Classes and Alterna-tions.
The University of Chicago Press.Longman Dictionary of Contemporary English.
2002.Pearson Education.Inderjeet Mani, Eric Bloedorn and Barbara Gates.1998.
Using Cohesion and Coherence Models forText Summarization.
In Working Notes of theWorkshop on Intelligent Text Summarization, p.69-76.
Menlo Park, California: American Associa-tion for Artificial Intelligence Spring SimposiumSeries.Daniel Marcu.
1997.
The Rhetorical Parsing, Summa-rization, and Generation of Natural LanguageTexts.
PhD Thesis, Department of Computer Sci-ence, University of Toronto.Paola Merlo, Suzanne Stevenson, Vivian Tsang andGianluca Allaria.
2002.
A Multilingual Paradigmfor Automatic Verb Classification.
Proceedings ofthe 40th Annual Meeting of the Association forComputational Linguistics.
(ACL?02), Philadelphia.Massimo Poesio and Renata Vieira.
2000 .
An Em-pirically Based System for Processing Definite De-scriptions.
Computational Linguistics, 26(4): 525-579.J.
Ross Quinlan, 1992: C4.5: Programs for MachineLearning.
Morgan Kaufmann Pub., San Mateo,CA.Eric V. Siegel.
1998.
Linguistic Indicators for Lan-guage Understanding: Using machine learningmethods to combine corpus-based indicators foraspectual classification of clauses.
Ph.D. Disserta-tion.
Columbia University.Pasi Tapanainen and Timo J?rvinen.
1997 A non-projective dependency parser.
In Proceedings ofthe 5th Conference on Applied Natural LanguageProcessing, p. 64-71.Simone Teufel and Marc Moens.
2002.
Summarizingscientific articles-experiments with relevance andrhetorical status.
Computational Linguistics, 28(4):409?445.Zeno Vendler.
1967.
Linguistics in Philosophy.
Cor-nell University Press, p. 97- 145.Klaus Zechner.
2002.
Automatic Summarization ofOpen-Domain Multiparty Dialogues in DiverseGenres.
Computational Linguistics 28(4):447-485.62
