Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 743?751,Beijing, August 2010Enhancing Morphological Alignment for TranslatingHighly Inflected Languages ?Minh-Thang LuongSchool of ComputingNational University of Singaporeluongmin@comp.nus.edu.sgMin-Yen KanSchool of ComputingNational University of Singaporekanmy@comp.nus.edu.sgAbstractWe propose an unsupervised approach uti-lizing only raw corpora to enhance mor-phological alignment involving highly in-flected languages.
Our method focuses onclosed-class morphemes, modeling theirinfluence on nearby words.
Our language-independent model recovers importantlinks missing in the IBM Model 4 align-ment and demonstrates improved end-to-end translations for English-Finnish andEnglish-Hungarian.1 IntroductionModern statistical machine translation (SMT)systems, regardless of whether they are word-,phrase- or syntax-based, typically use the word asthe atomic unit of translation.
While this approachworks when translating between languages withlimited morphology such as English and French,it has been found inadequate for morphologically-rich languages like Arabic, Czech and Finnish(Lee, 2004; Goldwater and McClosky, 2005;Yang and Kirchhoff, 2006).
As a result, a lineof SMT research has worked to incorporate mor-phological analysis to gain access to informationencoded within individual words.In a typical MT process, word aligned data isfed as training data to create a translation model.In cases where a highly inflected language isinvolved, the current word-based alignment ap-proaches produce low-quality alignment, as thestatistical correspondences between source and?This work was supported by a National Research Foun-dation grant ?Interactive Media Search?
(grant # R-252-000-325-279)target words are diffused over many morpholog-ical forms.
This problem has a direct impact onend translation quality.Our work addresses this shortcoming byproposing a morphologically sensitive approachto word alignment for language pairs involvinga highly inflected language.
In particular, ourmethod focuses on a set of closed-class mor-phemes (CCMs), modeling their influence onnearby words.
With the model, we correct er-roneous alignments in the initial IBM Model 4runs and add new alignments, which results in im-proved translation quality.After reviewing related work, we give a casestudy for morpheme alignment in Section 3.
Sec-tion 4 presents our four-step approach to constructand incorporate our CCM alignment model intothe grow-diag process.
Section 5 describes exper-iments, while Section 6 analyzes the system mer-its.
We conclude with suggestions for future work.2 Related WorkMT alignment has been an active research area.One can categorize previous approaches into thosethat use language-specific syntactic informationand those that do not.
Syntactic parse treeshave been used to enhance alignment (Zhang andGildea, 2005; Cherry and Lin, 2007; DeNeroand Klein, 2007; Zhang et al, 2008; Haghighi etal., 2009).
With syntactic knowledge, modelinglong distance reordering is possible as the searchspace is confined to plausible syntactic variants.However, they generally require language-specifictools and annotated data, making such approachesinfeasible for many languages.
Works that follownon-syntactic approaches, such as (Matusov et al,743i1 declare2 resumed3 the4 session5 of6 the7 european8 parliament9 adjourned10 on11 1312 december13 199614-1 julistan2 euroopan3 parlamentin4 perjantaina5 136 joulukuuta7 19968 keskeytyneen9 istuntokauden10 uudelleen11 avatuksi12Direct: 1-2 2-2 3-9 4-3 5-10 6-10 7-3 8-12 9-12 10-12 11-5 12-6 13-7 14-8Inverse: 1-1 2-2 8-3 9-4 10-5 12-6 13-7 14-8 10-9 10-10 10-11 10-12(a) Gloss: -1 declare2 european3 parliament 4 on-friday5 136 december7 19968 adjourned9 session10 resumed11,12i1 declare2 resume+3 d4 the5 session6 of7 the8 european9 parliament10 adjourn+11 ed12 on13 1314 december15 199616- julist+ a+ n euroopa+ n parlament+ in perjantai+ n+ a 13 joulukuu+ ta 1996 keskeyty+ neen istunto+ kauden uude+ lle+ en avatuksi1   2      3   4        5          6          7           8        9          10 11 12      13        14   15         16           17        18            19        20       21  22       23Direct: 1-23 2-23 3-23 4-23 5-22 6-23 7-22 8-6 9-5 10-7 11-16 12-16 13-9 14-12 15-13 16-15Inverse: 1-1 2-2 2-3 5-4 9-5 8-6 10-7 10-8 11-9 0-10 7-11 14-12 15-13 15-14 16-15 11-16 11-17 11-18 11-19 11-20 11-21 0-22 11-23(b)Figure 1: Sample English-Finnish IBM Model 4 alignments: (a) word-level and (b) morpheme-level.
Solid lines indicateintersection alignments, while the exhaustive asymmetric alignments are listed below.
In (a), translation glosses for Finnishare given; the dash-dot line is the incorrect alignment.
In (b), bolded texts are closed-class morphemes (CCM), while boldedindices indicate alignments involving CCMs.
The dotted lines are correct CCM alignments not found by IBM Model 4.2004; Liang et al, 2006; Ganchev et al, 2008),which aim to achieve symmetric word alignmentduring training, though good in many cases, arenot designed to tackle highly inflected languages.Our work differs from these by taking a middleroad.
Instead of modifying the alignment algo-rithm directly, we preprocess asymmetric align-ments to improve the input to the symmetrizingprocess later.
Also, our approach does not makeuse of specific language resources, relying only onunsupervised morphological analysis.3 A Case for Morpheme AlignmentThe notion that morpheme based alignment wouldbe useful in highly inflected languages is intu-itive.
Morphological inflections might indicatetense, gender or number that manifest as separatewords in largely uninflected languages.
Capturingthese subword alignments can yield better wordalignments that otherwise would be missed.Let us make this idea concrete with a case studyof the benefits of morpheme based alignment.
Weshow the intersecting alignments of an actual En-glish (source) ?
Finnish (target) sentence pair inFigure 1, where (a) word-level and (b) morpheme-level alignments are shown.
The morpheme-level alignment is produced by automatically seg-menting words into morphemes and running IBMModel 4 on the resulting token stream.Intersection links (i.e., common to both directand inverse alignments) play an important role increating the final alignment (Och and Ney, 2004).While there are several heuristics used in the sym-metrizing process, the grow-diag(onal) process iscommon and prevalent in many SMT systems,such as Moses (Koehn et al, 2007).
In the grow-diag process, intersection links are used as seedsto find other new alignments within their neigh-borhood.
The process continues iteratively, untilno further links can be added.In our example, the morpheme-level intersec-tion alignment is better as it has no misalignmentsand adds new alignments.
However it missessome key links.
In particular, the alignments ofclosed-class morphemes (CCMs; later formallydefined) as indicated by the dotted lines in (b) areoverlooked in the IBM Model 4 alignment.
Thisdifficulty in aligning CCMs is due to:1.
Occurrences of garbage-collector words(Moore, 2004) that attract CCMs to align tothem.
Examples of such links in (b) are 1?23or 11?21 with the occurrences of rare wordsadjourn+11 and avatuksi23.
We furthercharacterize such errors in Section 6.1.2.
Ambiguity among CCMs of the same surfacethat causes incorrect matchings.
In (b), weobserve multiple occurrence of the and non the source and target sides respectively.While the link 8?6 is correct, 5?4 is not as i1should be aligned to n4 instead.
To resolvesuch ambiguity, context information shouldbe considered as detailed in Section 4.3.The fact that rare words and multiple affixesoften occur in highly inflected languages exacer-bates this problem, motivating our focus on im-proving CCM alignment.
Furthermore, having ac-cess to the correct CCM alignments as illustrated744in Figure 1 guides the grow-diag process in find-ing the remaining correct alignments.
For exam-ple, the addition of CCM links i1?n4 and d4?lle21 helps to identify declare2?julist2and resume3?avatuksi23 as admissible align-ments, which would otherwise be missed.4 MethodologyOur idea is to enrich the standard IBM Model 4alignment by modeling closed-class morphemes(CCMs) more carefully using global statistics andcontext.
We realize our idea by proposing a four-step method.
First, we take the input parallel cor-pus and convert it into morphemes before trainingthe IBM Model 4 morpheme alignment.
Second,from the morpheme alignment, we induce auto-matically bilingual CCM pairs.
The core of ourapproach is in the third and fourth steps.
In Step 3,we construct a CCM alignment model, and applyit on the segmented input corpus to obtain an au-tomatic CCM alignment.
Finally, in Step 4, we in-corporate the CCM alignment into the symmetriz-ing process via our modified grow-diag process.4.1 Step 1: Morphological AnalysisThe first step presupposes morphologically seg-mented input to compute the IBM Model 4 mor-pheme alignment.
Following Virpioja et al(2007), we use Morfessor, an unsupervised an-alyzer which learns morphological segmentationfrom raw tokenized text (Creutz and Lagus, 2007).The tool segments input words into labeledmorphemes: PRE (prefix), STM (stem), and SUF(suffix).
Multiple affixes can be proposed foreach word; word compounding is allowed as well,e.g., uncarefully is analyzed as un/PRE+care/STM+ ful/SUF+ ly/SUF.
We append a?+?
sign to each nonfinal tag to distinguish word-internal morphemes from word-final ones, e.g.,?x/STM?
and ?x/STM+?
are considered differenttokens.
The ?+?
annotation enables the restorationof the original words, a key point to enforce wordboundary constraints in our work later.4.2 Step 2: Bilingual CCM PairsWe observe that low and highly inflected lan-guages, while intrinsically different, share moreen fi en fi en fithe1 -n?1 in6 -ssa?15 me166 -ni?60-s2 -t?9 is7 on?2 me166 minun?282to3 -a?6 that8 etta?
?7 why168 siksi?187to3 maan91 that8 ettei?283 view172 mielta?
?162of4 -a4 we10 -mme?10 still181 viela?
?108of4 -en?5 we10 meida?n?52 where183 jossa?209of4 -sta?19 we10 me?113 same186 samaa?334and5 ja?3 we10 emme123 he187 ha?n?184and5 seka?
?122 we10 meilla?
?231 good189 hyva?
?321and5 eika?203 .
.
.
.
.
.
over-408 yli-?391Table 1: English(en)-Finnish(fi) Bilingual CCM pairs(N=128).
Shown are the top 19 and last 10 of 168 bilingualCCM pairs extracted.
Subscript i indicates the ith most fre-quent morpheme in each language.
?
marks exact correspon-dence linguistically, whereas ?
suggests rough correspon-dence w.r.t http://en.wiktionary.org/wiki/.in common at the morpheme level.
The many-to-one relationships among words on both sidesis often captured better by one-to-one correspon-dences among morphemes.
We wish to modelsuch bilingual correspondence in terms of closed-class morphemes (CCM), similar to Nguyen andVogel (2008)?s work that removes nonaligned af-fixes during the alignment process.
Let us nowformally define CCM and an associative measureto gauge such correspondence.Definition 1.
Closed-class Morphemes (CCM)are a fixed set of stems and affixes that ex-hibit grammatical functions just like closed-classwords.
In highly inflected languages, we observethat grammatical meanings may be encoded inmorphological stems and affixes, rather than sep-arate words.
While we cannot formally identifyvalid CCMs in a language-independent way (asby definition they manifest language-dependentgrammatical functions), we can devise a good ap-proximation.
Following Setiawan et al (2007),we induce the set of CCMs for a language as thetop N frequent stems together with all affixes1.Definition 2.
Bilingual Normalized PMI(biPMI) is the averaged normalized PMI com-puted on the asymmetric morpheme alignments.Here, normalized PMI (Bouma, 2009), known tobe less biased towards low-frequency data, is de-fined as: nPMI(x, y) = ln p(x,y)p(x)p(y))/- ln p(x, y),where p(x), p(y), and p(x, y) follow definitionsin the standard PMI formula.
In our case, we only1Note that we employ length and vowel sequence heuris-tics to filter out corpus-specific morphemes.745compute the scores for x, y being morphemes fre-quently aligned in both asymmetric alignments.Given these definitions, we now consider a pairof source and target CCMs related and termed abilingual CCM pair (CCM pair, for short) if theyexhibit positive correlation in their occurrences(i.e., positive nPMI2 and frequent cooccurrences).We should note that relying on a hard thresh-old of N as in (Setiawan et al, 2007) is brittleas the CCM set varies in sizes across languages.Our method is superior in the use of N as a start-ing point only; the bilingual correspondence of thetwo languages will ascertain the final CCM sets.Take for example the en and fi CCM sets with154 and 214 morphemes initially (each consist-ing of N=128 stems).
As morphemes not havingtheir counterparts in the other language are spu-rious, we remove them by retaining only those inthe CCM pairs.
This effectively reduces the re-spective sizes to 91 and 114.
At the same time,these final CCMs cover a much larger range of topfrequent morphemes than N , up to 408 en and 391fi morphemes, as evidenced in Table 1.4.3 Step 3: The CCM Alignment ModelThe goal of this model is to predict when appear-ances of a CCM pair should be deemed as linking.With an identified set of CCM pairs, we knowwhen source and target morphemes correspond.However, in a sentence pair there can be many in-stances of both the source and target morphemes.In our example, the the?n pair corresponds todefinite nouns; there are two the and three -n in-stances, yielding 2?
3=6 possible links.Deciding which instances are aligned is a deci-sion problem.
To solve this, we inspect the IBMModel 4 morpheme alignment to construct a CCMalignment model.
The CCM model labels whetheran instance of a CCM pair is deemed semanticallyrelated (linked).
We cast the modeling problem assupervised learning, where we choose a maximumentropy (ME) formulation (Berger et al, 1996).We first discuss sample selection from the IBMModel 4 morpheme alignment, and then give de-tails on the features extracted.
The processes de-scribed below are done per sentence pair with fm1 ,2nPMI has a bounded range of [?1, 1] with values 1 and0 indicating perfect positive and no correlation, respectively.en1 and U denoting the source, target sentences andthe union alignments, respectively.Class labels.
We base this on the initial IBMModel 4 alignment to label each CCM pair in-stance as a positive or negative example: Positiveexamples are simply CCM pairs in U.
To be pre-cise, links j?i in U are positive examples if fj?eiis a CCM pair.
To find negative examples, we in-ventory other potential links that share the samelexical items with a positive one.
That is, a linkj??i?
not in U is a negative example, if a positivelink j?i such that fj = f ?j and ei = e?i exists.We stress that our collection of positive exam-ples contains high-precision but low-recall IBMModel 4 links, which connect the reliable CCMpairs identified before.
The model then general-izes from these samples to detect incorrect CCMlinks and to recover the correct ones, enhancingrecall.
We later detail this process in ?4.4.Feature Set.
Given a CCM pair instance, weconstruct three feature types: lexical, monolin-gual, and bilingual (See Table 2).
These featurescapture the global statistics and contexts of CCMpairs to decide if they are true alignment links.?
Lexical features reflect the tendency of theCCM pair being aligned to themselves.
We usebiPMI, which aggregates the global alignmentstatistics, to determine how likely source and tar-get CCMs are associated with each other.?
Monolingual context features measure theassociation among tokens of the same language,capturing what other stems and affixes co-occurwith the source/target CCM:1. within the same word (intra).
The aim is todisambiguate affixes as necessary in highlyinflected languages where same stems couldgenerate different roles or meanings.2.
outside the CCM?s word boundary (inter).This potentially capture cues such as tense,or number agreement.
For example, in En-glish, the 3sg agreement marker on verbs -soften co-occurs with nearby pronouns e.g.,he, she, it; whereas the same marker onnouns (-s), often appears with plural deter-miners e.g., these, those, many.To accomplish this, we compute two monolin-gual nPMI scores in the same spirit as biPMI, butusing the morphologically segmented input from746Feature Description ExamplesLexical ?
biPMI: None [?1, 0], Low (0, 1/3], Medium (1/3, 2/3], High (2/3, 1] pmid?lle=LowMonolingual Context ?
Capture morpheme cooccurrence with the src/tgt CCMIntra ?
Within the same word srcWd?lle=resume, tgtWd?lle=en, tgtWd?lle=uudeInter ?
To the Left & Right, in different words srcLd?lle=i, srcRd?lle=the, tgtRd?lle=avatuksiBilingual context ?
Capture neighbor links?
cooccurrence with the CCM pair linkbi0 ?
Most descriptive, capturing in terms of surface forms only ?
maybe sparse bi0d?lle=resume?avatuksibi1 ?
Generalizes morphemes into relative locations (Left, Within, Right) bi1d?lle=W?avatuksi, bi1d?lle=resume?Rbi2 ?
Most general, coupling token types (Close, Open) /w relative positions bi2d?lle=O?WRTable 2: Maximum entropy feature set.
Shown are feature types, descriptions and examples.
Most examples are given forthe alignment d4?lle+21 of the same running example in ?3.
Note that we only partially list the bilingual context features.each language separately.
Two morphemes are?linked?
if within a context window of wc words.?
Bilingual context features model cross-lingual reordering, capturing the relationships be-tween the CCM pair link and its neighbor3 links.Consider a simple translation between an Englishphrase of the form we ?verb?
and the Finnishone ?verb?
-mme, where -mme is the 1pl verbmarker.
We aim to capture movements such as?the open-class morphemes on the right of we andon the left of -mme are often aligned?.
These willfunction as evidence for the ME learner to alignthe CCM pair (we, -mme).
We encode the bilin-gual context at three different granularities, frommost specific to most general ones (cf Table 2).4.4 Step 4: Incorporate CCM AlignmentAt test time, we apply the trained CCM alignmentmodel to all CCM pairs occurring in each sentencepair to find CCM links.
On our running exam-ple in Figure 1, the CCM classifier tests 17 CCMpairs, identifying 6 positive CCM links of which4 are true positives (dotted lines in (b)).Though mostly correct, we note that some ofthe predicted links conflict: (d4?lle21, ed12?neen17 and ed12?lle21) share alignment end-points.
Such sharing in CCM alignments is rareand we believe should be disallowed.
This moti-vates us to resolve all CCM link conflicts beforeincorporating them into the symmetrizing process.Resolving link conflicts.
As CCM pairs areclassified independently, they possess classifica-tion probabilities which we use as evidence to re-solve the conflicts.
In our example, the classifica-tion probabilities for (d4?lle21, ed12?neen17,ed12?lle21) are (0.99, 0.93, 0.79) respectively.We use a simple, ?best-first?
greedy approach3Within a context window of wc words as in monolingual.to determine which links are kept and which aredropped to satisfy our assumption.
In our case,we pick the most confident link, d4?lle21 withprobability 0.99.
This precludes the incorrect link,ed12?lle21, but admits the other correct oneed12?neen17, probability 0.93.
As a result, thisresolution successfully removes the incorrect link.Modifying grow-diag.
We incorporate the setof conflict-resolved CCM links into the grow-diagprocess.
This step modifies the input alignmentsas well as the growing process.
U and I denote theIBM Model 4 union and intersection alignments.In our view, the resolved CCM links can serveas a quality mark to ?upgrade?
links before inputinto the grow-diag process.
We upgrade resolvedCCM links: (a) those ?
U ?
part of I , treatingthem as alignment seeds; (b) those /?
U ?
partof U , using them for exploration and growing.
Toreduce spurious alignments, we discarded links inU that conflict with the resolved CCM links.In the usual grow-diag, links immediately adja-cent to a seed link l are considered candidates tobe appended into the alignment seeds.
While suit-able for word-based alignment, we believe it is toosmall a context when the input are morphemes.For morpheme alignment, the candidate contextmakes more sense in terms of word units.
We thusenforce word boundaries in our modified grow-diag.
We derive word boundaries for end points inl using the morphological tags and the ?+?
word-end marker mentioned in ?4.1.
Using such bound-aries, we can then extend the grow-diag to con-sider candidate links within a neighborhood of wgwords; hence, enhancing the candidate coverage.5 ExperimentsWe use English-Finnish and English-Hungariandata from past shared tasks (WPT05 and WMT09)747to validate our approach.
Both Finnish and Hun-garian are highly inflected languages, with numer-ous verbal and nominal cases, exhibiting agree-ment.
Dataset statistics are given in Table 3.en-fi # en-hu #Train Europarl-v1 714K Europarl-v4 1,510KLM Europarl-v1-fi 714K News-hu 4,209KDev wpt05-dev 2000 news-dev2009 2051Test wpt05-test 2000 news-test2009 3027Table 3: Dataset Statistics: the numbers of parallel sen-tences for training, LM training, development and test sets.We use the Moses SMT framework for ourwork, creating both our CCM-based systems andthe baselines.
In all systems built, we obtainthe IBM Model 4 alignment via GIZA++ (Ochand Ney, 2003).
Results are reported using case-insensitive BLEU (Papineni et al, 2001).Baselines.
We build two SMT baselines:w-system: This is a standard phrase-basedSMT, which operates at the word level.
The sys-tem extracts phrases of maximum length 7 words,and uses a 4-gram word-based LM.wm-system: This baseline works at the wordlevel just like the w-system, but differs at thealignment stage.
Specifically, input to the IBMModel 4 training is the morpheme-level corpus,segmented by Morfessor and augmented with ?+?to provide word-boundary information (?4.1).
Us-ing such information, we constrain the alignmentsymmetrization to extract phrase pairs of 7 wordsor less in length.
The morpheme-based phrase ta-ble is then mapped back to word forms.
The pro-cess continues identically as in the w-system.CCM-based systems.
Our CCM-based sys-tems are similar in spirit to the wm system: train atthe morpheme, but decode at the word level.
Wefurther enhance the wm-system at the alignmentstage.
First, we train our CCM model based onthe initial IBM Model 4 morpheme alignment, andapply it to the morpheme corpus to obtain CCMalignment, which are input to our modified grow-diag process.
The CCM approach defines the set-ting of three parameters: ?N , wc, wg?
(Section 4).Due to our resource constraints, we set N=128,similar to (Setiawan et al, 2007), and wc=1 ex-perimentally.
We only focus on the choice of wg,testing wg={1, 2} to explore the effect of enforc-ing word boundaries in the grow-diag process.5.1 English-Finnish resultsWe test the translation quality of both directions(en-fi) and (fi-en).
We present results in Table 4 for7 systems, including: our baselines, three CCM-based systems with word-boundary knowledgewg={0, 1, 2} and two wm-systems wg={1, 2}.Results in Table 4 show that our CCM approacheffectively improves the performance.
Comparedto the wm-system, it chalks up a gain of 0.46BLEU points for en-fi, and a larger improvementof 0.93 points for the easier, reverse direction.Further using word boundary knowledge in ourmodified grow-diag process demonstrates that theadditional flexibility consistently enhances BLEUfor wg = 1, 2.
We achieve the best performanceat wg = 2 with improvements of 0.67 and 1.22BLEU points for en-fi and fi-en, respectively.en-fi fi-enw-system 14.58 23.56wm-system 14.47 22.89wm-system + CCM 14.93+0.46 23.82+0.93wm-system + CCM + wg = 1 15.01 23.95wm-system + CCM + wg = 2 15.14+0.67 24.11+1.22wm-system + wg = 1 14.44 22.92wm-system + wg = 2 14.28 23.01(Ganchev, 2008) - Base 14.72 22.78(Ganchev, 2008) - Postcat 14.74 23.43+0.65(Yang, 2006) - Base N/A 22.0(Yang, 2006) - Backoff N/A 22.3+0.3Table 4: English/Finnish results.
Shown are BLEUscores (in %) with subscripts indicating absolute improve-ments with respect to the wm-system baseline.Interestingly, employing the word boundaryheuristic alone in the original grow-diag does notyield any improvement for en-fi, and even worsensas wg is enlarged (as seen in Rows 6?7).
Thereare only slight improvements for fi-en with largerwg.This attests to the importance of combining theCCM model and the modified grow-diag process.Our best system outperforms the w-systembaseline by 0.56 BLEU points for en-fi, and yieldsan improvement of 0.55 points for fi-en.Compared to works experimenting en/fi trans-lation, we note the two prominent ones by Yangand Kirchhoff (2006) and recently by Ganchevet al (2008).
The former uses a simple back-offmethod experimenting only fi-en, yielding an im-provement of 0.3 BLEU points.
Work in the op-748posite direction (en-fi) is rare, with the latter pa-per extending the EM algorithm using posteriorconstraints, but showing no improvement; for fi-en, they demonstrate a gain of 0.65 points.
OurCCM method compares favorably against both ap-proaches, which use the same datasets as ours.5.2 English-Hungarian resultsTo validate our CCM method as language-independent, we also perform preliminary exper-iments on en-hu.
Table 5 shows the results usingthe same CCM setting and experimental schemesas in en/fi.
An improvement of 0.35 BLEU pointsis shown using the CCM model.
We further im-prove by 0.44 points with word boundary wg=1,but performance degrades for the larger window.Due to time constraints, we leave experimentsfor the reverse, easier direction as future work.Though modest, the best improvement for en-huis statistical significant at p<0.01 according toCollins?
sign test (Collins et al, 2005).System BLEUw-system 9.63wm-system 9.47wm-system + CCM 9.82 +0.35wm-system + CCM + wg = 1 9.91 +0.44wm-system + CCM + wg = 2 9.87Table 5: English/Hungarian results.
Subscripts indicateabsolute improvements with respect to the wm-system.We note that MT experiments for en/hu 4 arevery limited, especially for the en to hu direction.Nova?k (2009) obtained an improvement of 0.22BLEU with no distortion penalty; whereas Koehnand Haddow (2009) enhanced by 0.5 points us-ing monotone-at-punctuation reordering, mini-mum Bayes risk and larger beam size decoding.While not directly comparable in the exact set-tings, these systems share the same data sourceand splits similar to ours.
In view of these com-munity results, we conclude that our CCM modeldoes perform competitively in the en-hu task, andindeed seems to be language independent.6 Detailed AnalysisThe macroscopic evaluation validates our ap-proach as improving BLEU over both baselines,4Hungarian was used in the ACL shared task 2008, 2009.but how do the various components contribute?We first analyze the effects of Step 4 in produc-ing the CCM alignment, and then step backwardto examine the contribution of the different featureclasses in Step 3 towards the ME model.6.1 Quality of CCM alignmentTo evaluate the quality of the predicted CCMalignment, we address the following questions:Q1: What is the portion of CCM pairs beingmisaligned in the IBM Model 4 alignment?Q2: How does the CCM alignment differ fromthe IBM Model 4 alignment?Q3: To what extent do the new links introducedby our CCM model address Q1?Given that we do not have linguistic expertise inFinnish or Hungarian, it is not possible to exhaus-tively list all misaligned CCM pairs in the IBMModel 4 alignment.
As such, we need to find otherform of approximation in order to address Q1.We observe that correct links that do not existin the original alignment could be entirely miss-ing, or mistakenly aligned to neighboring words.With morpheme input, we can also classify mis-takes with respect to intra- or inter-word errors.Figure 2 characterizes errors T1, T2 and T3, eachbeing a more severe error class than the previous.Focusing on ei in the figure, links connecting eito fj?
or fj??
are deemed T1 errors (misalignmentshappen on one side).
A T2 error aligns f ?
?j withinthe same word, while a T3 error aligns it outsidethe current word but still within its neighborhood.This characterization is automatic, cheap and hasthe advantage of being language-independent.fj fj' fj?
?1 wordT1T2T31 wordei ei' ei?
?Figure 2: Categorization of CCM missing links.
Giventhat a CCM pair link (fj?ei) is not present in the IBM Model4, occurrences of any nearby link of the types T[1?3] can beconstrued as evidence of a potential misalignment.Statistics in Table 6(ii) answers Q1, suggest-ing a fairly large number of missing CCM links:3, 418K for en/fi and 6, 216K for en/hu, about12.35% and 12.06% of the IBM Model 4 unionalignment respectively.
We see that T1 errors con-749stitute the majority, a reasonable reflection of thegarbage- collector5 effect discussed in Section 3.General (i) Missing CCM links (ii)en/fi en/hu en/fi en/huDirect 17,632K 34,312K T1 2,215K 3,487KInverse 18,681K 34,676K T2 358K 690KD ?
I 8,643K 17,441K T3 845K 2,039KD ?
I 27,670K 51,547K Total 3,418K 6,216KTable 6: IBM Model 4 alignment statistics.
(i) Generalstatistics.
(ii) Potentially missing CCM links.Q2 is addressed by the last column in Ta-ble 7.
Our CCM model produces about 11.98%(1,035K/8,643K) new CCM links as compared tothe size of the IBM Model 4 intersection align-ment for en/fi, and similarly, 9.52% for en/hu.Orig.
Resolved I U\I Newen/fi 5,299K 3,433K 1065K 1,332K 1,035Ken/hu 9,425K 6,558K 2,752K 2,146K 1,660KTable 7: CCM vs IBM Model 4 alignments.
Orig.
andResolved give # CCM links predicted in Step 4 before andafter resolving conflicts.
Also shown are the number of re-solved links present in the Intersection, Union excluding I(U\I) of the IBM Model 4 alignment and New CCM links.Lastly, figures in Table 8 answer Q3, revealingthat for en/fi, 91.11% (943K/1,035K) of the newCCM links effectively cover the missing CCMalignments, recovering 27.59% (943K/3,418K) ofall missing CCM links.
Our modified grow-diagrealizes a majority 76.56% (722K/943K) of theselinks in the final alignment.We obtain similar results in the en/hu pair forlink recovery, but a smaller percentage 22.59%(330K/1,461K) are realized through the modifiedsymmetrization.
This partially explains why im-provements are modest for en/hu.New CCM Links (i) Modified grow-diag (ii)en/fi en/hu en/fi en/huT1 707K 1,002K 547K 228KT2 108K 146K 79K 22KT3 128K 313K 96K 80KTotal 943K 1,461K 722K 330KTable 8: Quality of the newly introduced CCM links.Shown are # new CCM links addressing the three error typesbefore (i) and after (ii) the modified grow-diag process.6.2 Contributions of ME Feature ClassesWe also evaluate the effectiveness the ME featuresindividually through ablation tests.
For brevity,5E.g., ei prefers f?j or f?
?j (garbage collectors) over fj .we only examine the more difficult translation di-rection, en to fi.
Results in Table 9 suggest thatall our features are effective, and that removingany feature class degrades performance.
Balanc-ing specificity and generality, bi1 is the mostinfluential feature in the bilingual context group.For monolingual context, inter, which captureslarger monolingual context, outperforms intra.The most important feature overall is pmi, whichcaptures global alignment preferences.
As featuregroups, bilingual and monolingual context fea-tures are important sources of information, as re-moving them drastically decreases system perfor-mance by 0.23 and 0.16 BLEU, respectively.System BLEUall (wm-system+CCM) 14.93?bi2 14.90 ?intra 14.89?bi1 14.84?
?0.09 ?pmi 14.81?
?0.12?bi0 14.89 ?bi{2/1/0} 14.70?
?0.23?inter 14.85 ?in{ter/tra} 14.77?
?0.16Table 9: ME feature ablation tests for English-Finnishexperiments.
?
mark results statistically significant at p <0.05, differences are subscripted.7 Conclusion and Future WorkIn this work, we have proposed a language-independent model that addresses morphemealignment problems involving highly inflectedlanguages.
Our method is unsupervised, requiringno language specific information or resources, yetits improvement on BLEU is comparable to muchsemantically richer, language-specific work.
Asour approach deals only with input word align-ment, any downstream modifications of the trans-lation model also benefit.As alignment is a central focus in this work, weplan to extend our work over different and mul-tiple input alignments.
We also feel that bettermethods for the incorporation of CCM alignmentsis an area for improvement.
In the en/hu pair, alarge proportion of discovered CCM links are dis-carded, in favor of spurious links from the unionalignment.
Automatic estimation of the correct-ness of our CCM alignments may improve endtranslation quality over our heuristic method.750ReferencesBerger, Adam L., Stephen D. Della Pietra, and Vin-cent J. D. Della Pietra.
1996.
A maximum entropyapproach to natural language processing.
Computa-tional Linguistics, 22(1):39?71.Bouma, Gerlof.
2009.
Normalized (pointwise) mutualinformation in collocation extraction.
In Proceed-ings of the Biennial GSCL Conference, Tu?bingen,Gunter Narr Verlag.Cherry, Colin and Dekang Lin.
2007.
Inversion trans-duction grammar for joint phrasal translation mod-eling.
In SSST.Collins, Michael, Philipp Koehn, and Ivona Kuc?erova?.2005.
Clause restructuring for statistical machinetranslation.
In ACL.Creutz, Mathias and Krista Lagus.
2007.
Unsuper-vised models for morpheme segmentation and mor-phology learning.
ACM Trans.
Speech Lang.
Pro-cess., 4(1):3.DeNero, John and Dan Klein.
2007.
Tailoring wordalignments to syntactic machine translation.
InACL.Ganchev, Kuzman, Joa?o V. Grac?a, and Ben Taskar.2008.
Better alignments = better translations?
InACL-HLT.Goldwater, Sharon and David McClosky.
2005.
Im-proving statistical mt through morphological analy-sis.
In HLT.Haghighi, Aria, John Blitzer, John DeNero, and DanKlein.
2009.
Better word alignments with super-vised itg models.
In ACL.Koehn, Philipp and Barry Haddow.
2009.
Edinburgh?ssubmission to all tracks of the WMT2009 sharedtask with reordering and speed improvements toMoses.
In EACL.Koehn, Philipp, Hieu Hoang, Alexandra Birch Mayne,Christopher Callison-Burch, Marcello Federico,Nicola Bertoldi, Brooke Cowan, Wade Shen, Chris-tine Moran, Richard Zens, Chris Dyer, Ondrej Bo-jar, Alexandra Constantin, and Evan Herbst.
2007.Moses: Open source toolkit for statistical machinetranslation.
In ACL, Demonstration Session.Lee, Young-Suk.
2004.
Morphological analysis forstatistical machine translation.
In HLT-NAACL.Liang, Percy, Ben Taskar, and Dan Klein.
2006.Alignment by agreement.
In HLT-NAACL.Matusov, Evgeny, Richard Zens, and Hermann Ney.2004.
Symmetric word alignments for statisticalmachine translation.
In COLING.Moore, Robert C. 2004.
Improving IBM word-alignment model 1.
In ACL.Nguyen, Thuy Linh and Stephan Vogel.
2008.Context-based Arabic morphological analysis formachine translation.
In CoNLL.Nova?k, Attila.
2009.
MorphoLogic?s submission forthe WMT 2009 shared task.
In EACL.Och, Franz Josef and Hermann Ney.
2003.
A sys-tematic comparison of various statistical alignmentmodels.
Computational Linguistics, 29(1):19?51.Och, Franz Josef and Hermann Ney.
2004.
The align-ment template approach to statistical machine trans-lation.
Computational Linguistics, 30(4):417?449.Papineni, Kishore, Salim Roukos, Todd Ward, andWei-Jing Zhu.
2001.
Bleu: a method for automaticevaluation of machine translation.
In ACL ?02: Pro-ceedings of the 40th Annual Meeting on Associa-tion for Computational Linguistics, pages 311?318,Morristown, NJ, USA.
Association for Computa-tional Linguistics.Setiawan, Hendra, Min-Yen Kan, and Haizhou Li.2007.
Ordering phrases with function words.
InACL.Virpioja, Sami, Jaakko J. Vyrynen, Mathias Creutz,and Markus Sadeniemi.
2007.
Morphology-awarestatistical machine translation based on morphs in-duced in an unsupervised manner.
In MT SummitXI.Yang, Mei and Katrin Kirchhoff.
2006.
Phrase-basedbackoff models for machine translation of highly in-flected languages.
In EACL.Zhang, Hao and Daniel Gildea.
2005.
Stochastic lex-icalized inversion transduction grammar for align-ment.
In ACL.Zhang, Hao, Chris Quirk, Robert C. Moore, andDaniel Gildea.
2008.
Bayesian learning of non-compositional phrases with synchronous parsing.
InACL-HLT.751
