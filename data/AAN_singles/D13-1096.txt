Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 935?945,Seattle, Washington, USA, 18-21 October 2013. c?2013 Association for Computational LinguisticsA Dataset for Research on Short-Text Conversation ?Hao Wang?
Zhengdong Lu?
Hang Li?
Enhong Chen??
xdwangh@mail.ustc.edu.cn ?lu.zhengdong@huawei.com?hangli.hl@huawei.com ?cheneh@ustc.edu.cn?Univ.
of Sci & Tech of China, China ?Noah?s Ark Lab, Huawei Technologies, Hong KongAbstractNatural language conversation is widely re-garded as a highly difficult problem, whichis usually attacked with either rule-based orlearning-based models.
In this paper wepropose a retrieval-based automatic responsemodel for short-text conversation, to exploitthe vast amount of short conversation in-stances available on social media.
For thispurpose we introduce a dataset of short-textconversation based on the real-world instancesfrom Sina Weibo (a popular Chinese mi-croblog service), which will be soon releasedto public.
This dataset provides rich collec-tion of instances for the research on findingnatural and relevant short responses to a givenshort text, and useful for both training and test-ing of conversation models.
This dataset con-sists of both naturally formed conversation-s, manually labeled data, and a large repos-itory of candidate responses.
Our prelimi-nary experiments demonstrate that the simpleretrieval-based conversation model performsreasonably well when combined with the richinstances in our dataset.1 IntroductionNatural language conversation is one of the holygrail of artificial intelligence, and has been taken asthe original form of the celebrated Turing test.
Pre-vious effort in this direction has largely focused onanalyzing the text and modeling the state of the con-versation through dialogue models, while in this pa-?The work is done when the first author worked as intern atNoah?s Ark Lab, Huawei Techologies.per we take one step back and focus on a much easi-er task of finding the response for a given short text.This task is in clear contrast with previous effort indialogue modeling in the following two aspects?
we do not consider the context or history ofconversations, and assume that the given shorttext is self-contained;?
we only require the response to be natural, rel-evant, and human-like, and do not require it tocontain particular opinion, content, or to be ofparticular style.This task is much simpler than modeling a completedialogue session (e.g., as proposed in Turing test),and probably not enough for real conversation sce-nario which requires often several rounds of interac-tions (e.g., automatic question answering system asin (Litman et al 2000)).
However it can shed impor-tant light on understanding the complicated mecha-nism of the interaction between an utterance and it-s response.
The research in this direction will notonly instantly help the applications of short sessiondialogue such as automatic message replying on mo-bile phone and the chatbot employed in voice assis-tant like Siri1, but also it will eventually benefit themodeling of dialogues in a more general setting.Previous effort in modeling lengthy dialogues fo-cused either on rule-based or learning-based models(Carpenter, 1997; Litman et al 2000; Williams andYoung, 2007; Schatzmann et al 2006; Misu et al2012).
This category of approaches require relative-ly less data (e.g.
reinforcement learning based) for1http://en.wikipedia.org/wiki/Siri935training or no training at all, but much manual ef-fort in designing the rules or the particular learningalgorithms.
In this paper, we propose to attack thisproblem using an alternative approach, by leverag-ing the vast amount of training data available fromthe social media.
Similar ideas have appeared in (Ja-farpour and Burges, 2010; Leuski and Traum, 2011)as an initial step for training a chatbot.With the emergence of social media, especiallymicroblogs such as Twitter, in the past decade, theyhave become an important form of communicationfor many people.
As the result, it has collected con-versation history with volume previously unthink-able, which brings opportunity for attacking the con-versation problem from a whole new angle.
Morespecifically, instead of generating a response to anutterance, we pick a massive suitable one from thecandidate set.
The hope is, with a reasonable re-trieval model and a large enough candidate set, thesystem can produce fairly natural and appropriate re-sponses.This retrieval-based model is somewhat like non-parametric model in machine learning communities,which performs well only when we have abundan-t data.
In our model, it needs only a relatively s-mall labeled dataset for training the retrieval model,but requires a rather large unlabeled set (e.g., onemillion instances) for candidate responses.
To fur-ther promote the research in similar direction, wecreate a dataset for training and testing the retrievalmodel, with a candidate responses set of reason-able size.
Sina Weibo is the most popular Twitter-like microblog service in China, hosting over 500million registered users and generating over 100million messages per day 2.
As almost all mi-croblog services, Sina Weibo allows users to com-ment on a published post3, which forms a naturalone-round conversation.
Due to the great abundanceof those (post, response) pairs, it provides an idealdata source and test bed for one-round conversation.We will make this dataset publicly available in thenear future.2http://en.wikipedia.org/wiki/Sina_Weibo3Actually it also allows users to comment on other users?comments, but we will not consider that in the dataset.2 The Dialogues on Sina WeiboSina Weibo is a Twitter-like microblog service, onwhich a user can publish short messages (will be re-ferred to as post in the remainder of the paper) visi-ble to public or a group specified by the user.
Simi-lar to Twitter, Sina Weibo has the word limit of 140Chinese characters.
Other users can comment on apublished post, with the same length limit, as shownin the real example given in Figure 6 (in Chinese).Those comments will be referred to as responses inthe remainder of the paper.Figure 1: An example of Sina Weibo post and the com-ments it received.We argue that the (post, response) pairs on SinaWeibo provide rather valuable resource for studyingone round dialogue between users.
The commentsto a post can be of rather flexible forms and diversetopics, as illustrated in the example in Table 1.
Witha post stating the user?s status (traveling to Hawaii),the comments can be of quite different styles andcontents, but apparently all appropriate.In many cases, the (post, response) pair is self-contained, which means one does not need any back-ground and contextual information to get the mainpoint of the conversation (Examples of that includethe responses from B, D, G and H).
In some cas-es, one may need extra knowledge to understand theconversation.
For example, the response from userE will be fairly elusive if taken out of the contextthat A?s Hawaii trip is for an international confer-ence and he is going to give a talk there.
We arguethat the number of self-contained (post, response)pairs is vast, and therefore the extracted (post, re-936PostUser A: The first day at Hawaii.
Watching sunset at the balcony with a big glass of wine in hand.ResponsesUser B: Enjoy it & don?t forget to share your photos!User C: Please take me with you next time!User D: How long are you going to stay there?User E: When will be your talk?User F: Haha, I am doing the same thing right now.
Which hotel are you staying in?User G: Stop showing-off, buddy.
We are still coding crazily right now in the lab.User H: Lucky you!
Our flight to Honolulu is delayed and I am stuck in the airport.
Chewing Frenchfries in MacDonald?s right now.Table 1: A typical example of Sina Weibo post and the comments it received.
The original text is in Chinese, and wetranslated it into English for easy access of readers.
We did the same thing for all the examples throughout this paper.sponse) pairs can serve as a rich resource for ex-ploring rather sophisticated patterns and structuresin natural language conversation.3 Content of the DatasetThe dataset consists of three parts, as illustrated inFigure 2.
Part 1 contains the original (post, re-sponse) pairs, indicated by the dark-grey section inFigure 2.
Part 2, indicated by the light-gray sectionin Figure 2, consists labeled (post, response) pairsfor some Weibo posts, including positive and nega-tive examples.
Part 3 collects all the responses, in-cluding but not limited to the responses in Part 1 and2.
Some of the basic statistics are summarized inTable 2.# posts # responses vocab.
# labeled pairs4,6345 1,534,874 105,732 12,427Table 2: Some statistics of the datasetOriginal (Post, Response) Pairs This part ofdataset gives (post, response) pairs naturally pre-sented in the microblog service.
In other words,we create a (post, response) pair there when the re-sponse is actually given to the post in Sina Weibo.The part of data is noisy since the responses givento a Weibo post could still be inappropriate for d-ifferent reasons, for example, they could be spamsor targeting some responses given earlier.
We have628, 833 pairs.Labeled Pairs This part of data contains the (post,response) pairs that are labeled by human.
Note that1) the labeling is only on a small subset of posts,and 2) for each selected post, the labeled responsesare not originally given to it.
The labeling is donein an active manner (see Section 4 for more detail-s), so the obtained labels are much more informativethan the those on randomly selected pairs (over 98%of which are negative).
This part of data can be di-rectly used for training and testing of retrieval-basedresponse models.
We have labeled 422 posts and foreach of them, about 30 candidate responses.Responses This part of dataset contains only re-sponses, but they are not necessarily for a certainpost.
These extra responses are mainly filtered outby our data cleaning strategy (see Section 4.2) fororiginal (post, response) pairs, including those fromfiltered-out Weibo posts and those addressing oth-er responses.
Nevertheless, those responses are stillvalid candidate for responses.
We have about 1.5million responses in the dataset.3.1 Using the Dataset for Retrieval-basedResponse ModelsOur data can be used for training and testing ofretrieval-based response model, or just as a bank ofresponses.
More specifically, it can be used in atleast the following three ways.Training Low-level Matching Features Therather abundant original (post, response) pairs pro-vide rather rich supervision signal for learning dif-ferent matching patterns between a post and a re-sponse.
These matching patterns could be of dif-937Figure 2: Content of the dataset.ferent levels.
For example, one may discover fromthe data that when the word ?Hawaii?
occurs in thepost, the response are more likely to contain word-s like ?trip?, ?flight?, or ?Honolulu?.
On a slight-ly more abstract level, one may learn that when anentity name is mentioned in the post, it tends to bementioned again in the response.
More complicatedmatching pattern could also be learned.
For exam-ple, the response to a post asking ?how to?
is statisti-cally longer than average responses.
As a particularcase, Ritter et al(2011) applied translation model(Brown et al 1993) on similar parallel data extract-ed from Twitter in order to extract the word-to-wordcorrelation.
Please note that with more sophisticat-ed natural language processing, we can go beyondbag-of-words for more complicated correspondencebetween post and response.Training Automatic Response Models Althoughthe original (post, response) pairs are rather abun-dant, they are not enough for discriminative trainingand testing of retrieval models, for the following rea-sons.
In the labeled pairs, both positive and negativeones are ranked high by some baseline models, andhence more difficult to tell apart.
This supervisionwill naturally tune the model parameters to find thereal good responses from the seemingly good ones.Please note that without the labeled negative pairs,we need to generate negative pairs with randomlychosen responses, which in most of the cases are tooeasy to differentiate by the ranking model and can-not fully tune the model parameters.
This intuitionhas been empirically verified by our experiments.Testing Automatic Response Models In testing aretrieval-based system, although we can simply usethe original responses associated with the query postas positive and treat all the others as negative, thisstrategy suffers from the problem of spurious neg-ative examples.
In other words, with a reasonablygood model, the retrieved responses are often goodeven if they are not the original ones, which bringssignificant bias to the evaluation.
With the labeledpairs, this problem can be solved if we limit the test-ing only in the small pool of labeled responses.3.2 Using the Dataset for Other PurposesOur dataset can also be used for other researches re-lated to short-text conversations, namely anaphoraresolution, sentiment analysis, and speech act anal-ysis, based on the large collection of original (post,response) pairs.
For example, to determine the sen-timent of a response, one needs to consider boththe original post as well as the observed interactionbetween the two.
In Figure 3, if we want to un-derstand user?s sentiment towards the ?invited talk?mentioned in the post, the two responses should betaken as positive, although the sentiment in the mereresponses is either negative or neutral.4 Creation of the DatasetThe (post, comment) pairs are sampled from theSina Weibo posts published by users in a looselyconnected community and the comments they re-ceived (may not be from this community).
Thiscommunity is mainly posed of professors, re-searchers, and students of natural language process-ing (NLP) and related areas in China, and the users938Figure 3: An example (original Chinese and the Englishtranslation) on the difficulty of sentiment analysis on re-sponses.commonly followed them.The creation process of the dataset, as illustratedin Figure 4, consists of three consecutive steps: 1)crawling the community of users, 2) crawling theirWeibo posts and their responses, 3) cleaning the da-ta, with more details described in the remainder ofthis section.4.1 Sampling StrategyWe take the following sampling strategy for collect-ing the (post, response) pairs to make the topic rel-atively focused.
We first locate 3,200 users from aloosely connected community of Natural LanguageProcessing (NLP) and Machine Learning (ML) inChina.
This is done through crawling followees4 often manually selected seed users who are NLP re-searchers active on Sina Weibo (with no less than 2posts per day on average) and popular enough (withno less than 100 followers).We crawl the posts and the responses they re-ceived (not necessarily from the crawled communi-ty) for two months (from April 5th, 2013, to June5th, 2013).
The topics are relatively limited due toour choice of the users, with the most saliently onesbeing:?
Research: discussion on research ideas, paper-s, books, tutorials, conferences, and researchersin NLP and machine learning, etc;?
General Arts and Science: mathematics,physics, biology, music, painting, etc;4When user A follows user B, A is called B?s follower, andB is called A?s followee.?
IT Technology: Mobile phones, IT companies,jobs opportunities, etc;?
Life: traveling (both touring or conference trip-s), food, photography, etc.4.2 Processing, Filtering, and Data CleaningOn the crawled posts and responses, we first performa four-step filtering on the post and responses?
We first remove the Weibo posts and their re-sponses if the length of post is less than 10 Chi-nese characters or the length of the response isless than 5 characters.
The reason for that istwo-fold: 1) if the text is too short, it can bare-ly contain information that can be reliably cap-tured, e.g.
the following exampleP: Three down, two to go.and 2) some of the posts or responses are toogeneral to be interesting for other cases, e.g.
theresponse in the example below,P: Nice restaurant.
I?d strong recommend it.Everything here is good except the longwaiting lineR: wow.?
In the remained posts, we only keep the first100 responses in the original (post, response)pairs, since we observe that after the first 100responses there will be a non-negligible propor-tion of responses addressing things other thanthe original Weibo post (e.g., the responses giv-en earlier).
We however will still keep the re-sponses in the bank of responses.?
The last step is to filter out the potential adver-tisements.
We will find the long responses thathave been posted more than twice on differentposts and scrub them out of both original (post,response) pairs and the response repository.For the remained posts and responses, we removethe punctuation marks and emoticons, and use ICT-CLAS (Zhang et al 2003) for Chinese word seg-mentation.939Figure 4: Diagram of the process for creating the dataset.4.3 LabelingWe employ a pooling strategy widely used in in-formation retrieval for getting the instance to label(Voorhees, 2002).
More specifically, for a givenpost, we use three baseline retrieval models to eachselect 10 responses (see Section 5 for the descrip-tion of the baselines), and merge them to form amuch reduced candidate set with size ?
30.
Thenwe label the reduced candidate set into ?suitable?and ?unsuitable?
categories.
Basically we considera response suitable for a given post if we cannot tellwhether it is an original response.
More specificallythe suitability of a response is judged based on thefollowing three criteria5:Semantic Relevance: This requires the content ofthe response to be semantically relevant to the post.As shown in the example right below, the post P isabout soccer, and so is response R1 (hence seman-tically relevant), whereas response R2 is about food(hence semantically irrelevant).P: There are always 8 English players in theirown penalty area.
Unbelievable!R1: Haha, it is still 0:0, no goal so far.R2: The food in England is horrible.Another important aspect of semantic relevance isthe entity association.
This requires the entities inthe response to be correctly aligned with those inthe post.
In other words, if the post is about entity5Note that although our criteria in general favor short andgeneral answers like ?Well said!?
or ?Nice?, most of these gen-eral answers have already been filtered out due to their length(see Section 4.2).A, while the response is about entity B, they are verylikely to be mismatched.
As shown in the followingexample, where the original post is about Paris, andthe response R2 talks about London:P: It is my last day in Paris.
So hard to saygoodbye.R1: Enjoy your time in Paris.R2: Man, I wish I am in London right now.This is however not absolute, since a response con-taining a different entity could still be sound, asdemonstrated by the following two responses to thepost aboveR1: Enjoy your time in France.R2: The fall of London is nice too.Logic Consistency: This requires the content ofthe response to be logically consistent with the post.For example, in the table right below, post P statesthat the Huawei mobile phone ?Honor?
is already inthe market of mainland China.
Response R1 talk-s about a personal preference over the same phonemodel (hence logically consistent), whereas R2 asksthe question the answer to which is already clearfrom P (hence logically inconsistent).P: HUAWEI?s mobile phone, Honor, sellswell in Chinese Mainland.R1: HUAWEI Honor is my favorite phoneR2: When will HUAWEI Honor get to themarket in mainland China?Speech Act Alignment: Another important factorin determining the suitability of a response is the940speech act.
For example, when a question is posed inthe Weibo post, a certain act (e.g., answering or for-warding it) is expected.
In the example below, postP asks a special question about location.
ResponseR1 and R2 either forwards or answers the question,whereas R3 is a negative sentence and therefore doesnot align well in speech act.P: Any one knows where KDD will be held theyear after next?R1: co-ask.
Hopefully EuropeR2: New York, as I heardR3: No, it is still in New York City5 Retrieval-based Response ModelIn a retrieval-based response model, for a given postx we pick from the candidate set the response withthe highest ranking score, where the score is the en-semble of several individual matching featuresscore(x, y) =?i?
?wi?i(x, y).
(1)with y stands for a candidate response.We perform a two-stage retrieval to handle the s-calability associated with the massive candidate set,as illustrated in Figure 5.
In Stage I, the system em-ploys several fast baseline matching models to re-trieve a number of candidate responses for the giv-en post x, forming a much reduced candidate setC(reduced)x .
In Stage II, the system uses a rankingfunction with more and sophisticated features to fur-ther evaluate all the responses in C(reduced)x , return-ing a matching score for each response.
Our re-sponse model then decides whether to respond andwhich candidate response to choose.In Stage II, we use the linear score function de-fined in Equation 1 with 15 features, trained withRankSVM (Joachims, 2002).
The training and test-ing are both performed on the 422 labeled posts,with about 12,000 labeled (post, response) pairs.
Weuse a 5-fold cross validation with a fixed penalty pa-rameter for slack variable.
65.1 Baseline Matching ModelsWe use the following matching models as the base-line model for Stage I fast retrieval.
Moreover, the6The performance is fairly insensitive to the choice of thepenalty, so we only report the result with a typical choice of it.matching features used in the ranking function inStage II are generated, directly or indirectly, fromthe those matching models:POST-RESPONSE SEMANTIC MATCHING:This particular matching function relies on a learnedmapping from the original sparse representation fortext to a low-dimensional but dense representationfor both Weibo posts and responses.
The level ofmatching score between a post and a response canbe measured as the inner product between theirimages in the low-dimensional spaceSemMatch(x, y) = x>LXL>Yy.
(2)where x and y are respectively the 1-in-N represen-tations of x and y.
This is to capture the seman-tic matching between a Weibo post and a response,which may not be well captured by a word-by-wordmatching.
More specifically, we find LX and LYthrough a large margin variant of (Wu et al 2013)arg minLX ,LY?imax(1?
?ix>i LXL>Yyi, 0)s.t.
?Ln,X ?1 ?
?1, n = 1, 2, ?
?
?
, Nx?Lm,Y?1 ?
?1, m = 1, 2, ?
?
?
, Ny?Ln,X ?2 = ?2, n = 1, 2, ?
?
?
, Nx?Lm,Y?2 = ?2m = 1, 2, ?
?
?
, Ny.where i indices the original (post, response) pairs.Our experiments (Section 6) indicate that this sim-ple linear model can learn meaningful patterns, dueto the massive training set.
For example, the im-age of the word ?Italy?
in the post in the latent s-pace matches well word ?Sicily?, ?Mediterraneansea?
and ?travel?.
Once the mapping LX and LYare learned, the semantic matching score x>LXL>Yywill be treated as a feature for modeling the overallsuitability of y as a response to post x.POST-RESPONSE SIMILARITY: Here we use asimple vector-space model for measuring the simi-larity between a post and a responsesimPR(x,y) =x>y?x??y?.
(3)Although it is not necessarily true that a good re-sponse has many common words as the post, but thismeasurement is often helpful in finding relevant re-sponses.
For example, when the post and response941Figure 5: Diagram of the retrieval-based automatic response system.both have ?National Palace Museum in Taipei?, itis a strong signal that they are about similar topic-s.
Unlike the semantic matching feature, this simplesimilarity requires no learning and works on infre-quent words.
Our empirical results show that it canoften capture the Post-Response relation failed withsemantic matching feature.POST-POST SIMILARITY: The basic idea here isto find posts similar to x and use their responses asthe candidates.
Again we use the vector space modelfor measuring the post-post similaritysimPP (x, x?)
=x>x??x??x??.
(4)The intuition here is that if a post x?
is similar to x itsresponses might be appropriate for x.
It however of-ten fails, especially when a response to x?
addressesparts of x not contained by x, which fortunately canbe alleviated when combined with other measures.5.2 Learning to Rank with Labeled DataWith all the matching features, we can learn a rank-ing model with the labeled (post, response) pairs,e.g., through off-the-shelf ranking algorithms.
Fromthe labeled data, we can extract triples (x, y+, y?
)to ensure that score(x, y+) > score(x, y?).
Appar-ently y+ can be selected from labeled positive re-sponse of x, while y?
can be sampled either fromlabeled negative negative or randomly selected ones.Since the manually labeled negative instances aretop-ranked candidates according to some individualretrieval model (see Section 5.1) and therefore gen-erally yield slightly better results.The matching features are mostly constructed bycombining the individual matching models, for ex-ample the following two?
?7(x, y): this feature measures the length ofthe longest common string in the post and theresponse;?
?12(x, y): this feature considers both seman-tic matching score between query post x andcandidate response y, as well as the similaritybetween x and y?s original post x?
:?12(x, y) = SemMatch(x, y)simPP (x, x?
).In addition to the matching features, we also havesimple features describing responses only, such asthe length of it.6 Experimental EvaluationWe perform experiments on the proposed dataset totest our retrieval-based model as an algorithm for au-tomatically generating response.6.1 Performance of ModelsWe evaluate the retrieved models based on the fol-lowing two metrics:MAP This one measures the mean average preci-sion (MAP)(Manning et al 2008) associatedwith the ranked list on C(reduced)x .P@1 This one simply measures the precision of thetop one response in the ranked list:P@1 =#good top-1 responses#posts942We perform a 5-fold cross-validation on the 422 la-beled posts, with the results reported in Table 1.
Asit shows, the semantic matching helps slightly im-prove the overall performance on P@1.Model MAP P@1P2R 0.565 0.489P2R + P2P 0.621 0.567P2R + MATCH 0.575 0.513P2R + P2P + MATCH 0.621 0.574Table 3: Comparison of different choices of features,where P2R stands for the features based on post-responsesimilarity, P2P stands for the features based on post-postsimilarity, and MATCH stands for the semantic match fea-ture.To mimic a more realistic scenario on automaticresponse model on Sina Weibo, we allow the systemto choose which post to respond to.
Here we simplyset the response algorithm to respond only when thehighest score of the candidate response passes a cer-tain threshold.
Our experiments show that when wechoose to respond only to 50% of the posts, the P@1increases to 0.76, while if the system only respondto 25% of the posts, P@1 keeps increasing to 81%.6.2 Case StudyAlthough our preliminary retrieval model does notconsider more complicated syntax, it is still able tocapture some useful coupling structure between theappropriate (post, response) pairs, as well as the sim-ilar (post, post) pairs.Figure 6: An actual instance (the original Chinese textand its English translation) of response returned by ourretrieval-based system.Case study shows that our retrieval is fairly ef-fective at capturing the semantic relevance (Section6.2.1), but relative weak on modeling the logic con-sistency (Section 6.2.2).
Also it is clear that the se-mantic matching feature (described in Section 5.1)helps find matched responses that do not share anywords with the post (Section 6.2.3).6.2.1 On Semantic RelevanceThe features employed in our retrieval model aremostly vector-space based, which are fairly good atcapturing the semantic relevance, as illustrated byExample 1 & 2.EXAMPLE 1:P: It is a small town on an Spanish with 500population, and guess what, they evenhave a casino!R: If you travel to Spain, you need to spendsome time there.EXAMPLE 2:P: One quote from Benjamin Franklin: ?Weare all born ignorant, but one mustwork hard to remain stupid.
?R: Benjamin Franklin is a wise man, andone of the founding fathers of USA.However our retrieval model also makes badchoice, especially when either the query post or theresponse is long, as shown in Example 3.
Here theresponse is picked up because 1) the correspondencebetween the word ?IT?
in the post and the word?mobile phone?
in the candidate, and 2) the Chineseword for ?lay off?
in the post and the word for ?out-dated?
in the response are the same.EXAMPLE 3:P: As to the laying-off, I haven?t heard anythingabout it.
?Elimination of the least competent?is kind-off conventional in IT, but the ratio isactually quite small.R: Please don?t speak that way, otherwise you canget outdated.
Mobile phones are very expensivewhen they were just out, but now they are fairlycheap.
Look forward, or you will be outdated.The entity association is only partially addressedwith features like post-response cosine similarity,treating entity name just as a word, which is appar-ently not enough for preventing the following type943of mistakes (see Example 4 & 5) when the post andresponse match well on other partsEXAMPLE 4:P: Professor Wang will give a curse on naturallanguage processing, starting next semester.R: Jealous..
I wish I can attend Prof. Li?scourse too some time in the future.EXAMPLE 5:P: The fine China from Exhibition at the NationalPalace Museum in TaipeiR: This drawing looks so nice.
National PalaceMuseum in Taipei is full of national treasures6.2.2 On Logic ConsistencyOur current model does not explicitly maintainthe logic consistency between the response and thepost, since Logic consistency requires a deeper anal-ysis of the text, and therefore hard to capture withjust a vector space model.
Below are two exampleswhich are semantically relevant, and correct with re-spect to speech act, but logically inappropriate.EXAMPLE 1:P: I checked.
Wang Fengyi is not my great grand-father, although they?ve done similar deedsand both were called ?Wang the Well-doer?.R: wow, Wang Fengyi is your great grand-fatherEXAMPLE 2:P: We are looking for summer interns.
We providebooks and lunch.
If you are in Wu Han andinterested, drop us an email.
Sorry we don?ttake any students outside Wu Han.R: Are you looking for summer intern?6.2.3 The Effect of Semantic MatchingThe experiments also show that we may find inter-esting and appropriate responses that have no com-mon words as the post, as shown in the example be-low.
Our bi-linear semantic matching model how-ever performs relatively poorly on long posts, wherethe topics of the sentence cannot be well capturedby the sum of the latent vectors associated with eachword.P: Eight England players stand in the penaltyarea.R1: What a classic matchR2: Haha, it is still 0:0, no goal so far7 SummaryIn this paper we propose a retrieval-based responsemodel for short-text based conversation, to leveragethe massive instances collected from social media.For research in similar directions, we create a datasetbased on the posts and comments from Sina Weibo.Our preliminary experiments show that our retrieval-based response model, when combined with a largecandidate set, can achieve fairly good performance.This dataset will be valuable for both training andtesting automatic response models for short texts.ReferencesPeter F. Brown, Vincent J. Della Pietra, Stephen A. DellaPietra, and Robert L. Mercer.
1993.
The mathematic-s of statistical machine translation: parameter estima-tion.
Comput.
Linguist., 19(2).Rollo Carpenter.
1997.
Cleverbot.Sina Jafarpour and Christopher J. C. Burges.
2010.
Fil-ter, rank, and transfer the knowledge: Learning to chat.Thorsten Joachims.
2002.
Optimizing search enginesusing clickthrough data.
In Proceedings of the eighthACM SIGKDD international conference on Knowl-edge discovery and data mining, KDD ?02, pages 133?142, New York, NY, USA.
ACM.Anton Leuski and David R. Traum.
2011.
Npceditor:Creating virtual human dialogue using information re-trieval techniques.
AI Magazine, 32(2):42?56.Diane Litman, Satinder Singh, Michael Kearns, and Mar-ilyn Walker.
2000.
Njfun: a reinforcement learningspoken dialogue system.
In Proceedings of the 2000ANLP/NAACL Workshop on Conversational systems -Volume 3, ANLP/NAACL-ConvSyst ?00, pages 17?20, Stroudsburg, PA, USA.
Association for Computa-tional Linguistics.Christopher D. Manning, Prabhakar Raghavan, and Hin-rich Schu?tze.
2008.
Introduction to Information Re-trieval.
Cambridge University Press, New York, NY,USA.Teruhisa Misu, Kallirroi Georgila, Anton Leuski, andDavid Traum.
2012.
Reinforcement learning ofquestion-answering dialogue policies for virtual muse-um guides.
In Proceedings of the 13th Annual Meeting944of the Special Interest Group on Discourse and Dia-logue, SIGDIAL ?12, pages 84?93.Alan Ritter, Colin Cherry, and William B. Dolan.
2011.Data-driven response generation in social media.
InProceedings of the Conference on Empirical Meth-ods in Natural Language Processing, EMNLP ?11,pages 583?593, Stroudsburg, PA, USA.
Associationfor Computational Linguistics.Jost Schatzmann, Karl Weilhammer, Matt Stuttle, andSteve Young.
2006.
A survey of statistical user sim-ulation techniques for reinforcement-learning of dia-logue management strategies.
Knowl.
Eng.
Rev., pages97?126.Ellen M Voorhees.
2002.
The philosophy of infor-mation retrieval evaluation.
In Evaluation of cross-language information retrieval systems, pages 355?370.
Springer.Jason D. Williams and Steve Young.
2007.
Partially ob-servable markov decision processes for spoken dialogsystems.
Comput.
Speech Lang., 21(2):393?422.Wei Wu, Zhengdong Lu, and Hang Li.
2013.
Learningbilinear model for matching queries and documents.Journal of Machine Learning Research (2013 to ap-pear).Hua-Ping Zhang, Hong-Kui Yu, De-Yi Xiong, and QunLiu.
2003.
Hhmm-based chinese lexical analyzer ict-clas.
SIGHAN ?03.945
