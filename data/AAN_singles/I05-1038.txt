Classification of Multiple-Sentence QuestionsAkihiro Tamura, Hiroya Takamura, and Manabu OkumuraPrecision and Intelligence Laboratory,Tokyo Institute of Technology, Japanaki@lr.pi.titech.ac.jp{takamura, oku}@pi.titech.ac.jpAbstract.
Conventional QA systems cannot answer to the questionscomposed of two or more sentences.
Therefore, we aim to construct aQA system that can answer such multiple-sentence questions.
As the firststage, we propose a method for classifying multiple-sentence questionsinto question types.
Specifically, we first extract the core sentence froma given question text.
We use the core sentence and its question focus inquestion classification.
The result of experiments shows that the proposedmethod improves F-measure by 8.8% and accuracy by 4.4%.1 IntroductionQuestion-Answering (QA) systems are useful in that QA systems return theanswer itself, while most information retrieval systems return documents thatmay contain the answer.QA systems have been evaluated at TREC QA-Track1 in U.S. and QAC(Question & Answering Challenge)2 in Japan.
In these workshops, the inputsto systems are only single-sentence questions, which are defined as the ques-tions composed of one sentence.
On the other hand, on the web there are alot of multiple-sentence questions (e.g., answer bank3, AskAnOwner4), whichare defined as the questions composed of two or more sentences: For example,?My computer reboots as soon as it gets started.
OS is Windows XP.
Is thereany homepage that tells why it happens??.
For conventional QA systems, thesequestions are not expected and existing techniques are not applicable or workpoorly to these questions.
Therefore, constructing QA systems that can handlemultiple-sentence questions is desirable.An usual QA system is composed of three components: question process-ing, document retrieval, and answer extraction.
In question processing, a givenquestion is analyzed, and its question type is determined.
This process is called?question classification?.
Depending on the question type, the process in the an-swer extraction component usually changes.
Consequently, the accuracy and theefficiency of answer extraction depend on the accuracy of question classification.1 http://trec.nist.gov/tracks.htm2 http://www.nlp.is.ritsumei.ac.jp/qac/3 http://www.theanswerbank.co.uk/4 http://www.askanowner.com/R.
Dale et al (Eds.
): IJCNLP 2005, LNAI 3651, pp.
426?437, 2005.c?
Springer-Verlag Berlin Heidelberg 2005Classification of Multiple-Sentence Questions 427Therefore, as a first step towards developing a QA system that can han-dle multiple-sentence questions, we propose a method for classifying multiple-sentence questions.
Specifically, in this work, we treat only questions which re-quire one answer.
For example, if the question ?The icon to return to desktop hasbeen deleted.
Please tell me how to recover it.?
is given, we would like ?WAY?to be selected as the question type.
We thus introduce core sentence extractioncomponent, which extracts the most important sentence for question classifica-tion.
This is because there are unnecessary sentences for question classificationin a multiple-sentence question, and we hope noisy features should be eliminatedbefore question classification with the component.
If a multiple-sentence questionis given, we first extract the most important sentence for question classificationand then classify the question using the only information in the sentence.In Section 2, we present the related work.
In Section 3, we explain our pro-posed method.
In Section 4, we describe our experiments and results, where wecan confirm the effectiveness of the proposed method.
Finally, in Section 5, wedescribe the summary of this paper and the future work.2 Related WorkThis section presents some existing methods for question classification.
Themethods are roughly divided into two groups: the ones based on hand-craftedrules and the ones based on machine learning.
The system ?SAIQA?
[1], Xu et al[2] used hand-crafted rules for question classification.
However, methods basedon pattern matching have the following two drawbacks: high cost of making rulesor patterns by hand and low coverage.Machine learning can be considered to solve these problems.
Li et al [3] usedSNoW for question classification.
The SNoW is a multi-class classifier that isspecifically tailored for learning in the presence of a very large number of fea-tures.
Zukerman et al [4] used decision tree.
Ittycheriah et al [5] used maximumentropy.
Suzuki [6] used Support Vector Machines (SVMs).
Suzuki [6] comparedquestion classification using machine learning methods (decision tree, maximumentropy, SVM) with a rule-based method.
The result showed that the accuracyof question classification with SVM is the highest of all.
According to Suzuki [6],a lot of information is needed to improve the accuracy of question classificationand SVM is suitable for question classification, because SVM can classify ques-tions with high accuracy even when the dimension of the feature space is large.Moreover, Zhang et al [7] compared question classification with five machinelearning algorithms and showed that SVM outperforms the other four methodsas Suzuki [6] showed.
Therefore, we also use SVM in classifying questions, as wewill explain later.However, please note that we treat not only usual single-sentence questions,but also multiple-sentence questions.
Furthermore, our work differs from previouswork in that we treat real data on the web, not artificial data prepared for theQA task.
From these points, the results in this paper cannot be compared withthe ones in the previous work.428 A.Tamura, H. Takamura, and M. Okumura3 Two-Step Approach to Multiple-Sentence QuestionClassificationThis section describes our method for classifying multiple-sentence questions.We first explain the entire flow of our question classification.
Figure 1 shows theproposed method.questionpreprocessingcore sentenceextraction componentquestion classificationcomponenta core sentencesingle-sentencequestionmultiple-sentencequestionquestion typepeculiar process tomultiple-sentence questionsFig.
1.
The entire flow of question classificationAn input question consisting of possibly multiple sentences is first prepro-cessed.
Parentheses parts are excluded in order to avoid errors in syntactic pars-ing.
The question is divided into sentences by punctuation marks.The next process changes depending on whether the given question is a single-sentence question or a multiple-sentence question.
If the question consists of asingle sentence, the question is sent directly to question classification component.If the question consists of multiple sentences, the question is sent to core sentenceextraction component.
In the component, a core sentence, which is defined asthe most important sentence for question classification, is extracted.
Then, thecore sentence is sent to the question classification component and the question isclassified using the information in the core sentence.
In Figure 1, ?core sentenceextraction?
is peculiar to multiple-sentence questions.3.1 Core Sentence ExtractionWhen a multiple-sentence question is given, the core sentence of the question isextracted.
For example, if the question ?I have studied the US history.
Therefore,I am looking for the web page that tells me what day Independence Day is.?
isgiven, the sentence ?Therefore, I am looking for the web page that tells me whatday Independence Day is.?
is extracted as the core sentence.With the core sentence extraction, we can eliminate noisy information beforequestion classification.
In the above example, the occurrence of the sentenceClassification of Multiple-Sentence Questions 429?I have studied the US history.?
would be a misleading information in terms ofquestion classification.Here, we have based our work on the following assumption: a multiple-sentence question can be classified using only the core sentence.
Please notethat we treat only questions which require one answer.We explain the method for extracting a core sentence.
Suppose we have aclassifier, which returns Score(Si) for each sentence Si of Question.
Question isthe set of sentences composing a given question.
Score(Si) indicates the likelinessof Si being the core sentence.
The sentence with the largest value is selected asthe core sentence:Core sentence = argmaxSi?QuestionScore(Si).
(1)We then extract features for constructing a classifier which returns Score(Si).We use the information on the words as features.
Only the features from thetarget sentence would not be enough for accurate classification.
This issue isexemplified by the following questions (core sentences are underlined).?
Question 1:Please advise a medication effective for hay fever.
I want to relieve myheadache and stuffy nose.
Especially my headache is severe.?
Question 2:I want to relieve my headache and stuffy nose.
Especially my head-ache is severe.While the sentence ?I want to relieve my headache and stuffy nose.?
written inbold-faced type is the core sentence in Question 2, the sentence is not suitable asthe core sentence in Question 1.
These examples show that the target sentencealone is sometimes not a sufficient evidence for core sentence extraction.Thus, in classification of a sentence, we use its preceding and following sen-tences.
For that purpose, we introduce a notion of window size.
?Window size isn?
means ?the preceding n sentences and the following n sentences in addition tothe target sentence are used to make a feature vector?.
For example, if windowsize is 0, we use only the target sentence.
If window size is ?, we use all thesentences in the question.We use SVM as a classifier.
We regard the functional distance from theseparating hyperplane (i.e., the output of the separating function) as Score(Si).Word unigrams and word bigrams of the target sentence and the sentences inthe window are used as features.
A word in the target sentence and the sameword in the other sentences are regarded as two different features.3.2 Question ClassificationAs discussed in Section 2, we use SVM in the classification of questions.
We usefive sets of features: word unigrams, word bigrams, semantic categories of nouns,question focuses, and semantic categories of question focuses.
The semantic cat-egories are obtained from a thesaurus (e.g., SHOP, STATION, CITY).430 A.Tamura, H. Takamura, and M. Okumura?Question focus?
is the word that determines the answer class of the ques-tion.
The notion of question focus was described by Moldovan et al [8].
Forinstance, in the question ?What country is ??
?, the question focus is ?coun-try?.
In many researches, question focuses are extracted with hand-crafted rules.However, since we treat all kinds of questions including the questions which arenot in an interrogative form, such as ?Please teach me ??
and ?I don?t know ?
?,it is difficult to manually create a comprehensive set of rules.
Therefore, in thispaper, we automatically find the question focus in a core sentence according tothe following steps :step 1 find the phrase5 including the last verb of the sentence or the phrasewith ???
at the end.step 2 find the phrase that modifies the phrase found in step 1.step 3 output the nouns and the unknown words in the phrase found in step 2.The output of this procedure is regarded as a question focus.
Although thisprocedure itself is specific to Japanese, we suppose that we can extract questionfocus for other languages with a similar simple procedure.4 ExperimentsWe designed experiments to confirm the effectiveness of the proposed method.In the experiments, we use data in Japanese.
We use a package for SVMcomputation, TinySVM 6, and a Japanese morphological analyzer, ChaSen 7 forword segmentation of Japanese text.
We use CaboCha 8 to obtain dependencyrelations, when a question focus is extracted from a question.
Semantic categoriesare obtained from a thesaurus ?Goitaikei?
[9].4.1 Experimental SettingsWe collect questions from two Japanese Q&A sites: hatena9 andYahoo!tiebukuro10.
2000 questions are extracted from each site and experimentaldata consist of 4000 questions in total.
A Q&A site is the site where a user puts aquestion on the site and other users answer the question on the site.
Such Q&Asites include many multiple-sentence questions in various forms.
Therefore, thosequestions are appropriate for our experiments where non-artificial questions arerequired.Here, we manually exclude the following three kinds of questions from thedataset: questions whose answers are only Yes or No, questions which require two5 Phrase here is actually Japanese bunsetsu phrase, which is the smallest meaningfulsequence consisting of an independent word and accompanying words.6 http://chasen.org/?taku/software/TinySVM/7 http://chasen.naist.jp/hiki/ChaSen/8 http://chasen.org/?taku/software/cabocha/9 http://www.hatena.ne.jp/10 http://knowledge.yahoo.co.jp/Classification of Multiple-Sentence Questions 431Table 1.
The types and the distribution of 2376 questionsNominal Answer Non-nominal AnswerQuestion Type Number Question Type NumberPERSON 64 REASON 132PRODUCT 238 WAY 500FACILITY 139 DEFINITION 73LOCATION 393 DESCRIPTION 228TIME 108 OPINION 173NUMBER 53 OTHERS (TEXT) 131OTHERS (NOUN) 1441139 1237TOTAL 2376or more answers, and questions which are not actually questions.
This deletionleft us 2376 questions.
The question types that we used and their numbers areshown in Table 111.
Question types requiring nominal answers are determinedreferring to the categories used by Sasaki et al [1].Of the 2376 questions, 818 are single-sentence questions and 1558 aremultiple-sentence questions.
The average number of sentences in a multiple-sentence question is 3.49.
Therefore, the task of core sentence extraction in oursetting is to decide a core sentence from 3.49 sentences on the average.
As an eval-uation measure for core sentence extraction, we use accuracy, which is definedas the number of multiple-sentence questions whose core sentence is correctlyidentified over the number of all the multiple-sentence questions.
To calculatethe accuracy, correct core sentence of the 2376 questions is manually tagged inthe preparation of the experiments.As an evaluation measure for question classification, we use F-measure, whichis defined as 2?Recall?Precision / (Recall+Precision).
As another evaluationmeasure for question classification, we use also accuracy, which is defined as thenumber of questions whose type is correctly classified over the number of thequestions.
All experimental results are obtained with two-fold cross-validation.4.2 Core Sentence ExtractionWe conduct experiments of core sentence extraction with four different windowsizes (0, 1, 2, and ?)
and three different feature sets (unigram, bigram, andunigram+bigram).
Table 2 shows the result.As this result shows, we obtained a high accuracy, more than 90% for thistask.
The accuracy is so good that we can use this result for the succeeding taskof question classification, which is our main target.
This result also shows thatlarge widow sizes are better for core sentence extraction.
This shows that goodclues for core sentence extraction are scattered all over the question.11 Although Sasaki et al [1] includes ORGANIZATION in question types, ORGA-NIZATION is integrated into OTHERS (NOUN) in our work because the size ofORGANIZATION is small.432 A.Tamura, H. Takamura, and M. OkumuraTable 2.
Accuracy of core sentence extraction with different window sizes and featuresWindow Size\ Features Unigram Bigram Unigram+Bigram0 1350/1558= 0.866 1378/1558= 0.884 1385/1558= 0.8891 1357/1558= 0.871 1386/1558= 0.890 1396/1558= 0.8962 1364/1558= 0.875 1397/1558= 0.897 1405/1558= 0.902?
1376/1558= 0.883 1407/1558= 0.903 1416/1558= 0.909Table 3.
Accuracy of core sentence extraction with simple methodologiesMethodology AccuracyFirst Sentence 743/1558= 0.477Last Sentence 471/1558= 0.302Interrogative Sentence 1077/1558= 0.691The result in Table 2 also shows that unigram+bigram features are mosteffective for any window size in core sentence extraction.To confirm the validity of our proposed method, we extract core sentenceswith three simple methodologies, which respectively extract one of the followingsentences as the core sentence : (1) the first sentence, (2) the last sentence,and (3) the last interrogative sentence (or the first sentence).
Table 3 shows theresult.
The result shows that such simple methodologies would not work in coresentence extraction.4.3 Question Classification: The Effectiveness of Core SentenceExtractionWe conduct experiments to examine whether the core sentence extraction iseffective for question classification or not.
For that purpose, we construct thefollowing three models:Plain question.
The given question is the input of question classification com-ponent without core sentence extraction process.Predicted core sentence.
The core sentence extracted by the proposedmethod in Section 3.1 is the input of question classification component.
Theaccuracy of core sentence extraction process is 90.9% as mentioned in Sec-tion 4.2.Correct core sentence.
The correct core sentence tagged by hand is the inputof question classification component.
This case corresponds to the case whenthe accuracy of core sentence extraction process is 100%.Word unigrams, word bigrams, and semantic categories of nouns are used asfeatures.
The features concerning question focus cannot be used for the plainquestion model, because the method for identifying the question focus requiresthat the input be one sentence.
Therefore, in order to clarify the effectiveness ofcore sentence extraction itself, through fair comparison we do not use questionfocus for each of the three models in these experiments.Classification of Multiple-Sentence Questions 433Table 4.
F-measure and Accuracy of the three models for question classificationModel Plain Question Predicted Core Sentence Correct Core SentenceAccuracy OfCore Sentence Extraction ?
0.909 1.000PERSON 0.462 0.434 0.505PRODUCT 0.381 0.467 0.480FACILITY 0.584 0.569 0.586LOCATION 0.758 0.780 0.824TIME 0.340 0.508 0.524NUMBER 0.262 0.442 0.421OTHERS (NOUN) 0.049 0.144 0.145REASON 0.280 0.539 0.579WAY 0.756 0.778 0.798DEFINITION 0.643 0.624 0.656DESCRIPTION 0.296 0.315 0.317OPINION 0.591 0.675 0.659OTHERS (TEXT) 0.090 0.179 0.186Average 0.423 0.496 0.514Accuracy 0.617 0.621 0.652Table 4 shows the result.
For most question types, the proposed methodwith a predicted core sentence improves F-measure.
This result shows that thecore sentence extraction is effective in question classification.
We can still expectsome more improvement of performance, by boosting accuracy of core sentenceextraction.In order to further clarify the importance of core sentence extraction, weexamine the accuracy for the questions whose core sentences are not correctlyextracted.
Of 142 such questions, 54 questions are correctly classified.
In short,the accuracy is 38% and very low.
Therefore, we can claim that without accuratecore sentence extraction, accurate question classification is quite hard.4.4 Question Classification: More Detailed Investigation of FeaturesHere we investigate the effectiveness of each set of features and the influenceof the preceding and the following sentences of the core sentence.
After that,we conduct concluding experiments.
In the first two experiments of this section,we use only the correct core sentence tagged by hand as the input of questionclassification.The Effectiveness of Each Feature SetFirst, to examine which feature set is effective in question classification, weexclude a feature set one by one from the five feature sets described in Section3.2 and conduct experiments of question classification.
Please note that the fivefeature sets can be used unlike the last experiment (Table 4), because the inputof question classification is one sentence.434 A.Tamura, H. Takamura, and M. OkumuraTable 5.
Experiments with each feature set being excluded.
Here ?sem.
noun?
meanssemantic categories of nouns.
?sem.
qf?
means semantic categories of question focuses.Excluded Feature SetAll Unigram Bigram Sem.
noun Qf Sem.
QfPERSON 0.574 0.571 0.620 0.536 0.505 0.505(-0.003) (+0.046) (-0.038) (-0.069) (-0.069)PRODUCT 0.506 0.489 0.579 0.483 0.512 0.502(-0.017) (+0.073) (-0.023) (+0.006) (-0.004)FACILITY 0.612 0.599 0.642 0.549 0.615 0.576(-0.013) (+0.03) (-0.063) (+0.003) (-0.036)LOCATION 0.832 0.826 0.841 0.844 0.825 0.833(-0.006) (+0.009) (+0.012) (-0.007) (+0.001)TIME 0.475 0.506 0.548 0.420 0.502 0.517(+0.031) (+0.073) (-0.055) (+0.027) (+0.042)NUMBER 0.442 0.362 0.475 0.440 0.466 0.413(-0.080) (+0.033) (-0.002) (+0.024) (-0.029)OTHERS (NOUN) 0.210 0.182 0.267 0.204 0.198 0.156(-0.028) (+0.057) (-0.006) (-0.012) (-0.054)REASON 0.564 0.349 0.622 0.603 0.576 0.582(-0.215) (+0.058) (+0.039) (+0.012) (+0.018)WAY 0.817 0.803 0.787 0.820 0.817 0.807(-0.014) (-0.030) (+0.003) (?0.000) (-0.010)DEFINITION 0.652 0.659 0.603 0.640 0.647 0.633(+0.007) (-0.049) (-0.012) (-0.005) (-0.019)DESCRIPTION 0.355 0.308 0.355 0.363 0.357 0.334(-0.047) (?0.000) (+0.008) (+0.002) (-0.021)OPINION 0.696 0.670 0.650 0.703 0.676 0.685(-0.026) (-0.046) (+0.007) (-0.020) (-0.011)OTHERS (TEXT) 0.183 0.176 0.179 0.154 0.190 0.198(-0.007) (-0.004) (-0.029) (+0.007) (+0.015)Average 0.532 0.500 0.551 0.520 0.530 0.518(-0.032) (+0.019) (-0.012) (-0.002) (-0.014)Accuracy 0.674 0.632 0.638 0.668 0.661 0.661Table 5 shows the result.
The numbers in parentheses are differences ofF-measure compared with its original value.
The decrease of F-measure suggeststhe effectiveness of the excluded feature set.We first discuss the difference of F-measure values in Table 5, by takingPRODUCT and WAY as examples.
The F-measure of PRODUCT is muchsmaller than that of WAY.
This difference is due to whether characteristic ex-pressions are present in the type or not.
In WAY, words and phrases such as?method?
and ?How do I - ??
are often used.
Such words and phrases work asgood clues for classification.
However, there is no such characteristic expressionsfor PRODUCT.
Although there is a frequently-used expression ?What is [noun] -?
?, this expression is often used also in other types such as LOCATION and FA-CILITY.
We have to rely on currently-unavailable world knowledge of whetherthe noun is a product name or not.
This is the reason of the low F-measure forPRODUCT.We next discuss the difference of effective feature sets according to questiontypes.
We again take PRODUCT and WAY as examples.
The most effectiveClassification of Multiple-Sentence Questions 435Table 6.
Experiments with different window sizesWindow Size0 1 2 ?PERSON 0.574 0.558 0.565 0.570PRODUCT 0.506 0.449 0.441 0.419FACILITY 0.612 0.607 0.596 0.578LOCATION 0.832 0.827 0.817 0.815TIME 0.475 0.312 0.288 0.302NUMBER 0.442 0.322 0.296 0.311OTHERS (NOUN) 0.210 0.123 0.120 0.050REASON 0.564 0.486 0.472 0.439WAY 0.817 0.808 0.809 0.792DEFINITION 0.652 0.658 0.658 0.641DESCRIPTION 0.355 0.358 0.357 0.340OPINION 0.696 0.670 0.658 0.635OTHERS (TEXT) 0.183 0.140 0.129 0.133Average 0.532 0.486 0.477 0.463Accuracy 0.674 0.656 0.658 0.653feature set is semantic categories of nouns for ?PRODUCT?
and bigrams for?WAY?.
Since whether a noun is a product name or not is important for PROD-UCT as discussed before, semantic categories of nouns are crucial to PRODUCT.On the other hand, important clues for WAY are phrases such as ?How do I?.Therefore, bigrams are crucial to WAY.Finally, we discuss the effectiveness of a question focus.
The result in Table5 shows that the F-measure does not change so much even if question focuses ortheir semantic categories are excluded.
This is because both question focuses andtheir semantic categories are redundantly put in the feature sets.
By comparingTables 4 and 5, we can confirm that question focuses improve question classifi-cation performance (F-measure increases from 0.514 to 0.532).
Please note againthat question focuses are not used in Table 4 for fair comparison.The Influence of Window SizeNext, we clarify the influence of window size.
As in core sentence extraction,?Window size is n?
means that ?the preceding n sentences and the followingn sentences in addition to the core sentence are used to make a feature vec-tor?.
We construct four models with different window sizes (0, 1, 2, and ?
)and compare their experimental results.
In this experiment, we use five sets offeatures and correct core sentence as the input of question classification like thelast experiment (Table 5).Table 6 shows the result of the experiment.
The result in Table 6 shows thatthe model with the core sentence alone is best.
Therefore, the sentences otherthan the core sentence are considered to be noisy for classification and wouldnot contain effective information for question classification.
This result suggeststhat the assumption (a multiple-sentence question can be classified using onlythe core sentence) described in Section 3.1 be correct.436 A.Tamura, H. Takamura, and M. OkumuraTable 7.
The result of concluding experimentsPlain Question The Proposed Methodcore sentence extraction No Yesfeature sets unigram, bigram unigram,bigram,qfsem.
noun sem.
noun,sem.
qfPERSON 0.462 0.492PRODUCT 0.381 0.504FACILITY 0.584 0.575LOCATION 0.758 0.792TIME 0.340 0.495NUMBER 0.262 0.456OTHERS (NOUN) 0.049 0.189REASON 0.280 0.537WAY 0.756 0.789DEFINITION 0.643 0.626DESCRIPTION 0.296 0.321OPINION 0.591 0.677OTHERS (TEXT) 0.090 0.189Average 0.423 0.511Accuracy 0.617 0.661Concluding ExperimentsWe have so far shown that core sentence extraction and question focuses workwell for question classification.
In this section, we conduct concluding experi-ments which show that our method significantly improves the classification per-formance.
In the discussion on effective features, we used correct core sentences.Here we use predicted core sentences.The result is shown in Table 7.
For comparison, we add to this table thevalues of F-measure in Table 4, which correspond to plain question (i.e., withoutcore sentence extraction).
The result shows that F-measure of most categoriesincrease, except for FACILITY and DEFINITION.
From comparison of ?All?in Table 5 with Table 7, the reason of decrease would be the low accuracies ofcore sentence extraction for these categories.
As shown in this table, in conclu-sion, we obtained 8.8% increase of average F-measure of all and 4.4% increase ofaccuracy, which is statistically significant in the sign-test with 1% significance-level.Someone may consider that the type of multiple-sentence questions can beidentified by ?one-step?
approach without core sentence extraction.
In a word,the question type of each sentence in the given multiple-sentence question isfirst identified by a classifier, and then the type of the sentence for which theclassifier outputs the largest score is selected as the type of the given question.The classifier?s output indicates the likeliness of being the question type of agiven question.
Therefore, we compared the proposed model with this modelin the preliminary experiment.
The accuracy of question classification with theproposed model is 66.1% (1570/2376), and that of the one-step approach is61.7% (1467/2376).
This result shows that our two-step approach is effective forclassification of multiple-sentence questions.Classification of Multiple-Sentence Questions 4375 ConclusionsIn this paper, we proposed a method for identifying the types of multiple-sentence questions.
In our method, the core sentence is first extracted from agiven multiple-sentence question and then used for question classification.We obtained accuracy of 90.9% in core sentence extraction and empiricallyshowed that larger window sizes are more effective in core sentence extraction.We also showed that the extracted core sentences and the question focuses aregood for question classification.
Core sentence extraction is quite important alsoin the sense that question focuses could not be introduced without core sentences.With the proposed method, we obtained the 8.8% increase of F-measure and4.4% increase of accuracy.Future work includes the following.
The question focuses extracted in theproposed method include nouns which might not be appropriate for questionclassification.
Therefore, we regard the improvement on the question focus detec-tion as future work.
To construct a QA system that can handle multiple-sentencequestion, we are also planning to work on the other components: document re-trieval, answer extraction.References1.
Yutaka Sasaki, Hideki Isozaki, Tsutomu Hirao, Koji Kokuryou, and Eisaku Maeda:NTT?s QA Systems for NTCIR QAC-1.
Working Notes, NTCIR Workshop 3, Tokyo,pp.
63?70, 2002.2.
Jinxi Xu, Ana Licuanan, and Ralph M.Weischedel: TREC 2003 QA at BBN: An-swering Definitional Questions.
TREC 2003, pp.
98?106, 2003.3.
Xin Li and Dan Roth: Learning Question Classifiers.
COLING 2002, Taipei, Taiwan,pp.
556?562, 2002.4.
Ingrid Zukerman and Eric Horvitz: Using Machine Learning Techniques to InterpretWH-questions.
ACL 2001, Toulouse, France, pp.
547?554, 2001.5.
Abraham Ittycheriah, Martin Franz, Wei-Jing Zhu, and Adwait Ratnaparkhi: Ques-tion Answering Using Maximum Entropy Components.
NAACL 2001, pp.
33?39,2001.6.
Jun Suzuki: Kernels for Structured Data in Natural Language Processing, DoctorThesis, Nara Institute of Science and Technology, 2005.7.
Dell Zhang and Wee Sun Lee: Question Classification using Support Vector Ma-chines.
SIGIR, Toronto, Canada, pp.
26?32, 2003.8.
Dan Moldovan, Sanda Harabagiu, Marius Pasca, Rada Mihalcea, Richard Goodrum,Roxana Girju, and Vasile Rus: Lasso: A Tool for Surfing the Answer Net.
TREC-8,pp.
175?184, 1999.9.
Satoru Ikehara, Masahiro Miyazaki, Satoshi Shirai, Akio Yokoo, Hiromi Nakaiwa,Kentaro Ogura, Yoshifumi Oyama, and Yoshihiko Hayashi, editors: The SemanticSystem, volume 1 of Goi-Taikei ?
A Japanese Lexicon.
Iwanami Shoten, 1997 (inJapanese).
