Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural LanguageProcessing (HLT/EMNLP), pages 299?306, Vancouver, October 2005. c?2005 Association for Computational LinguisticsUsing Question Series To Evaluate Question Answering System EffectivenessEllen M. VoorheesNational Institute of Standards and TechnologyGaithersburg, MD 20899AbstractThe original motivation for using ques-tion series in the TREC 2004 question an-swering track was the desire to model as-pects of dialogue processing in an evalu-ation task that included different questiontypes.
The structure introduced by the se-ries also proved to have an important ad-ditional benefit: the series is at an appro-priate level of granularity for aggregatingscores for an effective evaluation.
Theseries is small enough to be meaningfulat the task level since it represents a sin-gle user interaction, yet it is large enoughto avoid the highly skewed score distribu-tions exhibited by single questions.
Ananalysis of the reliability of the per-seriesevaluation shows the evaluation is stablefor differences in scores seen in the track.The development of question answering technol-ogy in recent years has been driven by tasks de-fined in community-wide evaluations such as TREC,NTCIR, and CLEF.
The TREC question answering(QA) track started in 1999, with the first several edi-tions of the track focused on factoid questions.
Afactoid question is a fact-based, short answer ques-tion such as How many calories are there in a BigMac?.
The track has evolved by increasing the typeand difficulty of questions that are included in thetest set.
The task in the TREC 2003 QA track wasa combined task that contained list and definitionquestions in addition to factoid questions (Voorhees,2004).
A list question asks for different instances ofa particular kind of information to be returned, suchas List the names of chewing gums.
Answering suchquestions requires a system to assemble an answerfrom information located in multiple documents.
Adefinition question asks for interesting informationabout a particular person or thing such as Who isVlad the Impaler?
or What is a golden parachute?.Definition questions also require systems to locateinformation in multiple documents, but in this casethe information of interest is much less crisply de-lineated.Like the NTCIR4 QACIAD challenge (Kato etal., 2004), the TREC 2004 QA track grouped ques-tions into series, using the series as abstractions ofinformation-seeking dialogues.
In addition to mod-eling a real user task, the series are a step toward in-corporating context-processing into QA evaluationsince earlier questions in a series provide some con-text for the current question.
In the case of the TRECseries, each series contained factoid and list ques-tions and had the target of a definition associatedwith it.
Each question in a series asked for someinformation about the target.
In addition, the finalquestion in each series was an explicit ?other?
ques-tion, which was to be interpreted as ?Tell me otherinteresting things about this target I don?t knowenough to ask directly?.
This last question wasroughly equivalent to the definition questions in theTREC 2003 task.This paper examines the efficacy of series-basedQA evaluation, and demonstrates that aggregatingscores over individual series provides a more mean-ingful evaluation than averages of individual ques-299tion scores.
The next section describes the questionseries that formed the basis of the TREC 2004 eval-uation.
Since TREC uses different evaluation proto-cols for different question types, the following sec-tion describes the way in which individual questiontypes were evaluated.
Section 3 contrasts the scoresobtained by aggregating individual question scoresby question type or by series, and shows the use ofseries leads to a reliable evaluation at differences inscores that are observed in practice.1 Question SeriesA question series as used in the TREC 2004 QAtrack consisted of several factoid questions, zero totwo list questions, and exactly one Other question.Associated with each series was a definition target.The series a question belonged to, the order of thequestion in the series, and the type of each question(factoid, list, or Other) were all explicitly encoded inthe XML format used to describe the test set.
Exam-ple series (minus the XML tags) are shown in fig-ure 1.
A target was a person, an organization, orthing that was a plausible match for the scenario as-sumed for the task: that the questioner was an ?aver-age?
adult reader of US newspapers who was look-ing for more information about a term encounteredwhile reading the paper.The TREC 2004 test set contains 65 series.
Ofthe 65 targets, 23 are PERSONs, 25 are ORGANI-ZATIONs, and 17 are THINGs.
The series containa total of 230 factoid questions, 56 list questions,and 65 (one per target) Other questions.
Each se-ries contains at least four questions, counting theOther question, with most series containing five orsix questions.
The maximum number of questionsin a series is ten.Question series were also the fundamental struc-ture used in the QACIAD challenge (Question An-swering Challenge for Information Access Dia-logue) of NTCIR4.
However, there are some impor-tant differences between the QACIAD and TRECseries.
The QACIAD series model a more natu-ral flow of questions in an information-seeking di-alogue.
Given other evaluation requirements (mostquestions need to have an answer in the source doc-uments, answers to earlier questions should not begiven in later questions, etc.
), the series in the TRECtest set are heavily edited versions of the series col-lected from the original information seekers.
Theresulting edited series appear as a stilted conversa-tional style when viewed from the perspective of truedialogue, and the series do not reflect the full rangeof information requested in the original series.
(Forexample, TREC requires list question answers to beconcrete entities such as cities or book titles whilethe information seekers often asked for fuzzier in-formation such as lists of descriptive qualities.)
TheQACIAD challenge contained two types of series,gathering series and browsing series.
In a gather-ing series, all of the questions are related to a singletarget (that was not explicitly given in QACIAD),while questions in a browsing series can refer to un-related targets.
The TREC series are all gatheringtype series with the target explicitly given.
Finally,the QACIAD series consist of list questions only,since factoid questions are treated as list questionswith a single answer.Systems participating in the TREC evaluationwere required to process series independently fromone another, and were required to process an individ-ual series in question order.
That is, systems wereallowed to use questions and answers from earlierquestions in a series to answer later questions in thatsame series, but could not ?look ahead?
and use laterquestions to help answer earlier questions.
The se-ries was the unit used to structure the test set, butthere was no requirement for systems to process aseries as a unit.
Some systems appended the targetto each of the questions in its series and then pro-cessed all resulting question strings independentlyas in earlier TREC evaluations.
Per-series evaluationis still valid since the task to be evaluated is definedin terms of the series and is independent of how sys-tems choose to process the questions.Sixty-three runs from 28 participants were sub-mitted to the TREC 2004 QA track.2 Scoring Question SeriesThe evaluation protocol for individual questions de-pends on the type of the question.
This sectionsummarizes the protocols for the individual questiontypes and for a series as a whole.3003 Hale Bopp comet3.1 FACTOID When was the comet discovered?3.2 FACTOID How often does it approach the earth?3.3 LIST In what countries was the comet visible on its last return?3.4 OTHER21 Club Med21.1 FACTOID How many Club Med vacation spots are there worldwide?21.2 LIST List the spots in the United States.21.3 FACTOID Where is an adults-only Club Med?21.4 OTHER22 Franz Kafka22.1 FACTOID Where was Franz Kafka born?22.2 FACTOID When was he born?22.3 FACTOID What is his ethnic background?22.4 LIST What books did he author?22.5 OTHERFigure 1: Sample question series from the test set.
Series 3 has a THING as a target, series 21 has anORGANIZATION as a target, and series 22 has a PERSON as a target.2.1 Factoid questionsThe system response for a factoid question is eitherexactly one [doc-id, answer-string] pair or the literalstring ?NIL?.
NIL is returned by a system when it be-lieves there is no answer to the question in the docu-ment collection.
Otherwise, answer-string is a stringcontaining precisely an answer to the question, anddoc-id is the id of a document in the collection thatsupports answer-string as an answer.Each response was assigned exactly one of thefollowing four judgments:incorrect: the answer string does not contain a rightanswer or the answer is not responsive;not supported: the answer string contains a rightanswer but the document returned does not sup-port that answer;not exact: the answer string contains a right answerand the document supports that answer, but thestring contains more than just the answer or ismissing bits of the answer;correct: the answer string consists of exactly theright answer and that answer is supported bythe document returned.To be responsive, an answer string is required tocontain appropriate units and to refer to the correct?famous?
entity (e.g., the Taj Mahal casino is not re-sponsive when the question asks about ?the Taj Ma-hal?).
NIL responses are correct only if there is noknown answer to the question in the collection andare incorrect otherwise.
NIL is correct for 22 of the230 factoid questions in the test setAn individual factoid question has a binary score,1 if the response is judged correct and 0 otherwise.The score for a set of factoid questions is accuracy,the fraction of questions in the set judged correct.2.2 List questionsA list question can be thought of as a shorthand forasking the same factoid question multiple times.
Theset of all correct, distinct answers in the documentcollection that satisfy the factoid question is the cor-rect answer for a list question.A system?s response for a list question is an un-ordered set of [doc-id, answer-string] pairs suchthat each answer-string is considered an instance ofthe requested type.
Judgments of incorrect, unsup-ported, not exact, and correct are made for individualresponse pairs as in the factoid judging.
The asses-sor is given one run?s entire list at a time, and whilejudging for correctness also marks a set of responsesas distinct.
The assessor chooses an arbitrary mem-ber of the equivalent responses to be marked distinct,301and the remainder are not marked as distinct.
Onlycorrect responses may be marked as distinct.The final correct answer set for a list question iscompiled from the union of the correct responsesacross all runs plus the instances the assessor foundduring question development.
For the 55 list ques-tions used in the evaluation (one list question wasdropped because the assessor decided there were nocorrect answers during judging), the average numberof answers per question is 8.8, with 2 as the smallestnumber of answers, and 41 as the maximum num-ber of answers.
A system?s response to a list ques-tion was scored using instance precision (IP) andinstance recall (IR) based on the list of known in-stances.
Let S be the number of known instances,D be the number of correct, distinct responses re-turned by the system, and N be the total number ofresponses returned by the system.
Then IP = D/Nand IR = D/S.
Precision and recall were thencombined using the F measure with equal weightgiven to recall and precision:F = 2?
IP ?
IRIP + IRThe score for a set of list questions is the mean ofthe individual questions?
F scores.2.3 Other questionsThe Other questions were evaluated using the samemethodology as the TREC 2003 definition ques-tions (Voorhees, 2003).
A system?s response foran Other question is an unordered set of [doc-id,answer-string] pairs as for list questions.
Each stringis presumed to be a facet in the definition of theseries?
target that had not yet been covered by ear-lier questions in the series.
The requirement to notrepeat information already covered by earlier ques-tions in the series made answering Other questionssomewhat more difficult than answering TREC 2003definition questions.Judging the quality of the systems?
responses isdone in two steps.
In the first step, all of the answerstrings from all of the systems?
responses are pre-sented to the assessor in a single list.
Using theseresponses and the searches done during question de-velopment, the assessor creates a list of informationnuggets about the target.
An information nugget isan atomic piece of information about the target thatis interesting (in the assessor?s opinion) and is notpart of an earlier question in the series or an answerto an earlier question in the series.
An informationnugget is atomic if the assessor can make a binarydecision as to whether the nugget appears in a re-sponse.
Once the nugget list is created for a target,the assessor marks some nuggets as vital, meaningthat this information must be returned for a responseto be good.
Non-vital nuggets act as don?t care con-ditions in that the assessor believes the informationin the nugget to be interesting enough that returningthe information is acceptable in, but not necessaryfor, a good response.In the second step of judging the responses, anassessor goes through each system?s response in turnand marks which nuggets appear in the response.
Aresponse contains a nugget if there is a conceptualmatch between the response and the nugget; that is,the match is independent of the particular wordingused in either the nugget or the response.
A nuggetmatch is marked at most once per response?if theresponse contains more than one match for a nugget,an arbitrary match is marked and the remainder areleft unmarked.
A single [doc-id, answer-string] pairin a system response may match 0, 1, or multiplenuggets.Given the nugget list and the set of nuggetsmatched in a system?s response, the nugget recallof a response is the ratio of the number of matchednuggets to the total number of vital nuggets in thelist.
Nugget precision is much more difficult to com-pute since there is no effective way of enumerat-ing all the concepts in a response.
Instead, a mea-sure based on length (in non-white space charac-ters) is used as an approximation to nugget preci-sion.
The length-based measure starts with an initialallowance of 100 characters for each (vital or non-vital) nugget matched.
If the total system responseis less than this number of characters, the value ofthe measure is 1.0.
Otherwise, the measure?s valuedecreases as the length increases using the function1 ?
length?allowancelength .
The final score for an Otherquestion is computed as the F measure with nuggetrecall three times as important as nugget precision:F (?
= 3) = 10?
precision ?
recall9?
precision + recall .Note that the Other question for series S7 was302mistakenly left unjudged, so the series was was re-moved from the TREC 2004 evaluation.
This meansfinal scores for runs were computed over 64 ratherthan 65 question series.2.4 Per-series scoresIn the TREC 2003 evaluation, the final score for arun was computed as a weighted average of the meanscores for different question types:FinalScore = .5FactoidAccuracy + .25ListAveF+.25DefinitionAveF.Since each of the component scores ranges between0 and 1, the final score is also in that range.
Theweights for the different components reflect the de-sire to emphasize factoid scores, since factoid tech-nology is the most mature, while still allowing othercomponents to affect the final score.
The specificweights used match this general objective, but areotherwise arbitrary.
No experiments have been runexamining the effect of different weights on the sta-bility of the final scores, but small perturbations inthe weights should have little effect on the results.An individual question series also contains a mix-ture of different question types, so the weighted av-erage can be computed for an individual series ratherthan the test set as a whole.
The mean of the per-series scores is then used as the final score for a run.We use the same weighted average as above tocompute the score for an individual series that con-tains all three question types, using only the scoresfor questions belonging to that series in the compu-tation and using the Other question?s score in placeof the average of definition questions scores.
Forthose series that did not contain any list questions,the score was computed as .67FactoidAccuracy +.33OtherF.
Figure 2 shows the average per-seriesscore for the best run for each of the top 10 groupsthat participated in TREC 2004.3 Analysis of Per-series EvaluationThe main purpose of evaluations such as TREC is toprovide system builders with the information neededto improve their systems.
An informative evaluationmust be reliable (i.e., the results must be trustwor-thy) as well as capture salient aspects of the realuser task.
This section first examines the user task0.00.20.40.6Averageper-series scoreFigure 2: Average per-series scores for top ten QAtrack runs.abstracted by the per-series evaluation, and then de-rives an empirical estimate of the reliability of theevaluation.3.1 Modeling a User TaskThe set of questions used to aggregate individualquestions?
scores determines the emphasis of a QAevaluation.
In the TREC 2003 combined task therewere no series but there were different questiontypes, so question scores were first averaged byquestion type and then those averages were com-bined.
This strategy emphasizes question-type anal-ysis in that it is easy to compare different systems?abilities for the different question types.
The QA-CIAD challenge contained only a single questiontype but introduced a series structure into the testset (Kato et al, 2004).
In QACIAD, the scoreswere aggregated over the series and the series scoresaveraged.
The QACIAD series were specificallyconstructed to be an abstraction of an informationseeker?s dialogue, and the aggregation of scores overseries supports comparing different series types.
Forexample, QACIAD results show browsing series tobe more difficult than gathering series.The TREC 2004 QA track contained both seriesstructure and different question types, so individualquestion scores could be aggregated either by seriesor by question type.
In general, the two methodsof aggregation lead to different final scores.
Ag-gregating by question type gives equal weight to303S1 S4 S8 S11 S15 S19 S23 S27 S31 S35 S39 S43 S47 S51 S55 S59 S630.00.20.40.60.8Figure 3: Box and whiskers plot of per-series scores across all TREC 2004 runs.
The x-axis shows the seriesnumber and the y-axis the score.each of the questions of the same type, while aggre-gating by series gives equal weight to each series.This is the same difference as between micro- andmacro-averaging of document retrieval scores.
Forthe set of runs submitted to TREC 2004, the abso-lute value of the final scores when aggregated by se-ries were generally somewhat greater than the finalscores when aggregated by question type, though itis possible for the question-type-aggregated score tobe the greater of the two.
The relative scores for dif-ferent runs (i.e., whether one run was better than an-other) were usually, but not always, the same regard-less of which aggregation method was used.
TheKendall ?
(Stuart, 1983) measure of correlation be-tween the system rankings produced by sorting theruns by final score for each of the two aggregationmethods was 0.971, where identical rankings wouldhave a correlation of 1.0.Despite the relatively minor differences in runs?final scores when aggregating by series or by ques-tion type, there is a strong reason to prefer the seriesaggregation.
An individual series is small enough tobe meaningful at the task level (it represents a sin-gle user?s interaction) yet large enough for a seriesscore to be meaningful.
Figure 3 shows a box-and-whiskers plot of the per-series scores across all runsfor each series.
A box in the plot shows the extentof the middle half of the scores for that series, withthe median score indicated by the line through thebox.
The dotted lines (the ?whiskers?)
extend toa point that is 1.5 times the interquartile distance,or the most extreme score, whichever is less.
Ex-treme scores that are greater than the 1.5 times theinterquartile distance are plotted as circles.
The plotshows that only a few series (S21, S25, S37, S39)have median scores of 0.0.
This is in sharp con-trast to the median scores of individual questions.For the TREC 2004 test set, 212 of the 230 factoidquestions (92.2%) have a zero median, 39 of 55 listquestions (70.9%) have a zero median, and 41 of 64Other questions (64.1%) have a zero median.Having a unit of evaluation that is at the appro-priate level of granularity is necessary for meaning-ful results from the methodology used to assess thereliability of an evaluation.
This methodology, de-scribed below, was originally created for documentretrieval evaluations (Voorhees and Buckley, 2002)where the topic is the unit of evaluation.
The distri-304bution of scores across runs for an individual topicis much the same as the distribution of scores forthe individual series as in figure 3.
Score distribu-tions that are heavily skewed toward zero make theevaluation look far more reliable than is likely to bethe case since the reliability methodology computesa measure of the variability in scores.3.2 ReliabilityTREC uses comparative evaluations: one system isconsidered to be more effective than another if theevaluation score computed for the output of the firstsystem is greater than the evaluation score computedfor the output of the second system.
Since all mea-surements have some (unknown) amount of error as-sociated with them, there is always a chance thatsuch a comparison can lead to the wrong result.
Ananalysis of the reliability of an evaluation establishesbounds for how likely it is for a single comparisonto be in error.The reliability analysis uses the runs submitted tothe track to empirically determine the relationshipamong the number of series in a test set, the ob-served difference in scores (?)
between two runs,and the likelihood that a single comparison of tworuns leads to the correct conclusion.
Once estab-lished, the relationship is used to derive the mini-mum difference in scores required for a certain levelof confidence in the results given that there are 64series in the test set.The core of the procedure for establishing the re-lationship is comparing the effectiveness of a pairruns on two disjoint, equal-sized sets of series to seeif the two sets disagree as to which of the runs isbetter.
We define the error rate as the percentage ofcomparisons that have such a disagreement.
Sincethe TREC 2004 track had 64 series, we can directlycompute the error rate for test sizes up to 32 series.The smallest test set used is five series since fewerthan five series in a test set is too noisy to be infor-mative.
By fitting curves to the values observed fortest set sizes between 5 and 32, we can extrapolatethe error rates to test sets up to 64 series.When calculating the error rate, the difference be-tween two runs?
scores is categorized into a set ofbins based on the size of the difference.
The first bincontains runs with a difference of less than 0.01 (in-cluding no difference at all).
The next bin containsruns whose difference is at least 0.01 but less than0.02.
The limits for the remaining bins increase byincrements of 0.01.Each test set size from 5 to 32 is treated as a sep-arate experiment.
Within an experiment, we ran-domly select two disjoint sets of series of the re-quired size.
We compute the average series scoreover both sets for all runs, then count the number oftimes we see a disagreement as to which run is bet-ter for all pairs of runs using the bins to segregate thecounts by size of the difference in scores.
The entireprocedure is repeated 50 times (i.e., we perform 50trials), with the counts of the number of disagree-ments kept as running totals over all trials.
The ratioof the number of disagreements observed in a bin tothe total number of cases that land in that bin is theerror rate for the bin.Figure 4 shows the error rate curves for five sep-arate bins.
In the figure the test set size is plot-ted on the x-axis and the error rate is plotted onthe y-axis.
The individual points in the graphs arethe data points actually computed by the procedureabove, while the lines are the best-fit exponentialcurve for the data points in the current bin and ex-trapolated to size 64.
The top curve is for the binwith 0.01 ?
?
< 0.02 and the bottom curve for thebin with 0.05 ?
?
< 0.06; the intervening curvesare for the intervening bins, in order with smaller?
?s having larger error rates.
An error rate no greaterthan 5%, requires a difference in scores of at least0.05, which can be obtained with a test set of 47 se-ries.
Score differences of between 0.04 and 0.05 (thefourth curve) have an error rate slightly greater than5% when there are 64 series in the test set.Having established the minimum size of the dif-ference in scores needed to be confident that tworuns are actually different, it is also important toknow whether differences of the required size actu-ally occur in practice.
If it is rare to observe a dif-ference in scores as large as the minimum, then theevaluation will be reliable but insensitive.
With 64runs submitted to the TREC 2004 QA track, thereare 1953 run pairs; 70% of the pairs have a dif-ference in average per-series score that is at least0.05.
Many of the pairs in the remaining 30% aretruly equivalent?for example, runs submitted bythe same group that had very small differences intheir processing.
In figure 2, the difference in scores30500.050.10.150.20.250.30.350.40.450.50 5 10 15 20 25 30 35 40 45 50 55 60 65Test Set Size (number of series)ErrorRate0.01   <0.020.02   < 0.030.03   < 0.040.04   < 0.050.05   < 0.06Figure 4: Extrapolated error rates for per-series scores for different test set sizes.between each of the first three runs and its next clos-est run is greater than 0.05, while the next five runsare all within 0.05 of one another.4 ConclusionQuestion series have been introduced into recentquestion answering evaluations as a means of mod-eling dialogues between questioners and systems.The abstraction allows researchers to investigatemethods for answering contextualized questions andfor tracking (some forms of) the way objects are re-ferred to in natural dialogues.
The series have animportant evaluation benefit as well.
The individualseries is at the correct level of granularity for aggre-gating scores for a meaningful evaluation.
Unlikeindividual questions that have heavily skewed scoredistributions across runs, per-series score distribu-tions resemble the distributions of per-topic scoresin document retrieval evaluations.
This allows themethodology developed for assessing the quality ofa document retrieval evaluation to be meaningfullyapplied to the per-series evaluation.
Such an analy-sis of the TREC 2004 QA track per-series evaluationshows the evaluation results to be reliable for differ-ences in scores that are often observed in practice.ReferencesTsuneaki Kato, Jun?ichi Fukumoto, Fumito Masui, andNoriko Kando.
2004.
Handling information access di-alogue through QA technologies?A novel challengefor open-domain question answering.
In Proceedingsof the HLT-NAACL 2004 Workshop on Pragmatics ofQuestion Answering, pages 70?77, May.Alan Stuart.
1983.
Kendall?s tau.
In Samuel Kotz andNorman L. Johnson, editors, Encyclopedia of Statisti-cal Sciences, volume 4, pages 367?369.
John Wiley &Sons.Ellen M. Voorhees and Chris Buckley.
2002.
The effectof topic set size on retrieval experiment error.
In Pro-ceedings of the 25th Annual International ACM SIGIRConference on Research and Development in Informa-tion Retrieval, pages 316?323.Ellen M. Voorhees.
2003.
Evaluating answers to defi-nition questions.
In Proceedings of the 2003 HumanLanguage Technology Conference of the North Ameri-can Chapter of the Association for Computational Lin-guistics (HLT-NAACL 2003), Volume 2, pages 109?111, May.Ellen M. Voorhees.
2004.
Overview of the TREC2003 question answering track.
In Proceedings ofthe Twelfth Text REtrieval Conference (TREC 2003),pages 54?68.306
