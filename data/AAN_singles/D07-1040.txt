Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and ComputationalNatural Language Learning, pp.
380?389, Prague, June 2007. c?2007 Association for Computational LinguisticsExplorations in Automatic Book SummarizationRada Mihalcea and Hakan CeylanDepartment of Computer ScienceUniversity of North Texasrada@cs.unt.edu, hakan@unt.eduAbstractMost of the text summarization research car-ried out to date has been concerned withthe summarization of short documents (e.g.,news stories, technical reports), and very lit-tle work if any has been done on the sum-marization of very long documents.
In thispaper, we try to address this gap and ex-plore the problem of book summarization.We introduce a new data set specifically de-signed for the evaluation of systems for booksummarization, and describe summarizationtechniques that explicitly account for thelength of the documents.1 IntroductionBooks represent one of the oldest forms of writtencommunication and have been used since thousandsof years ago as a means to store and transmitinformation.
Despite this fact, given that a largefraction of the electronic documents available onlineand elsewhere consist of short texts such as Webpages, news articles, scientific reports, and others,the focus of natural language processing techniquesto date has been on the automation of methods tar-geting short documents.
We are witnessing howevera change: an increasingly larger number of booksbecome available in electronic format, in projectssuch as Gutenberg (http://www.gutenberg.org),Google Book Search (http://books.google.com),or the Million Books project(http://www.archive.org/details/millionbooks).Similarly, a large number of the books published inrecent years are often available ?
for purchase orthrough libraries ?
in electronic format.
This meansthat the need for language processing techniquesable to handle very large documents such as booksis becoming increasingly important.In this paper, we address the problem of booksummarization.
While there is a significant bodyof research that has been carried out on the taskof text summarization, most of this work has beenconcerned with the summarization of short doc-uments, with a particular focus on news stories.However, books are different in both length andgenre, and consequently different summarizationtechniques are required.
In fact, the straight-forwardapplication of a current state-of-the-art summariza-tion tool leads to poor results ?
a mere 0.348 F-measure compared to the baseline of 0.325 (see thefollowing sections for details).
This is not surprisingsince these systems were developed specifically forthe summarization of short news documents.The paper makes two contributions.
First, weintroduce a new data set specifically designed forthe evaluation of book summaries.
We describethe characteristics of a new benchmark consistingof books with manually constructed summaries, andwe calculate and provide lower and upper perfor-mance bounds on this data set.
Second, after brieflydescribing a summarization system that has beensuccessfully used for the summarization of shortdocuments, we show how techniques that take intoaccount the length of the documents can be used tosignificantly improve the performance of this sys-tem.2 Related WorkAutomatic summarization has received a lot of atten-tion from the natural language processing commu-380nity, ever since the early approaches to automatic ab-straction that laid the foundations of the current textsummarization techniques (Luhn, 1958; Edmunson,1969).
The literature typically distinguishes be-tween extraction, concerned with the identificationof the information that is important in the input text;and abstraction, which involves a generation step toadd fluency to a previously compressed text (Hovyand Lin, 1997).
Most of the efforts to date have beenconcentrated on the extraction step, which is perhapsthe most critical component of a successful summa-rization algorithm, and this is the focus of our cur-rent work as well.To our knowledge, no research work to date wasspecifically concerned with the automatic summa-rization of books.
There is, however, a large andgrowing body of work concerned with the summa-rization of short documents, with evaluations typ-ically focusing on news articles.
In particular, asignificant number of summarization systems havebeen proposed during the recent Document Under-standing Conference exercises (DUC) ?
annual eval-uations that usually draw the participation of 20?30teams every year.There are two main trends that can be identifiedin the summarization literature: supervised systems,that rely on machine learning algorithms trained onpre-existing document-summary pairs, and unsuper-vised techniques, based on properties and heuristicsderived from the text.Among the unsupervised techniques, typical sum-marization methods account for both the weight ofthe words in sentences, as well as the sentence posi-tion inside a document.
These techniques have beensuccessfully implemented in the centroid approach(Radev et al, 2004), which extends the idea of tf.idfweighting (Salton and Buckley, 1997) by introduc-ing word centroids, as well as integrating other fea-tures such as position, first-sentence overlap andsentence length.
More recently, graph-based meth-ods that rely on sentence connectivity have also beenfound successful, using algorithms such as node de-gree (Salton et al, 1997) or eigenvector centrality(Mihalcea and Tarau, 2004; Erkan and Radev, 2004;Wolf and Gibson, 2004).In addition to unsupervised methods, supervisedmachine learning techniques have also been usedwith considerable success.
Assuming the avail-ability of a collection of documents and their cor-responding manually constructed summaries, thesemethods attempt to identify the key properties of agood summary, such as the presence of named enti-ties, positional scores, or the location of key phrases.Such supervised techniques have been successfullyused in the systems proposed by e.g.
(Teufel andMoens, 1997; Hirao et al, 2002; Zhou and Hovy,2003; D?Avanzo and Magnini, 2005).In addition to short news documents, which havebeen the focus of most of the summarization systemsproposed to date, work has been also carried out onthe summarization of other types of documents.
Thisincludes systems addressing the summarization of e-mail threads (Wan and McKeown, 2004), online dis-cussions (Zhou and Hovy, 2005), spoken dialogue(Galley, 2006), product reviews (Hu and Liu, 2004),movie reviews (Zhuang et al, 2006), or short literaryfiction stories (Kazantseva and Szpakowicz, 2006).As mentioned before, we are not aware of any workaddressing the task of automatic book summariza-tion.3 A Data Set for the Evaluation ofBook SummarizationA first challenge we encountered when we startedworking on the task of book summarization was thelack of a suitable data set, designed specifically forthe evaluation of summaries of long documents.
Un-like the summarization of short documents, whichbenefits from the data sets made available throughthe annual DUC evaluations, we are not aware ofany publicly available data sets that can be used forthe evaluation of methods for book summarization.The lack of such data sets is perhaps not sur-prising since even for humans the summarization ofbooks is more difficult and time consuming than thesummarization of short news documents.
Moreover,books are often available in printed format and aretypically protected by copyright laws that do not al-low their reproduction in electronic format, whichconsequently prohibits their public distribution.We constructed a data set starting from the ob-servation that several English and literature coursesmake use of books that are sometimes also avail-able in the form of abstracts ?
meant to ease theaccess of students to the content of the books.
In381particular, we have identified two main publish-ers that make summaries available online for booksstudied in the U.S. high-school and college sys-tems: Grade Saver (http://www.gradesaver.com) andCliff?s Notes (http://www.cliffsnotes.com/).
Fortu-nately, many of these books are classics that are al-ready in the public domain, and thus for most ofthem we were able to find the online electronic ver-sion of the books on sites such as Gutenberg or On-line Literature (http://www.online-literature.com).For instance, the following is an example drawnfrom Cliff?s Notes summary of Bleak House byCharles Dickens.On a raw November afternoon, London is en-shrouded in heavy fog made harsher by chimneysmoke.
The fog seems thickest in the vicinity ofthe High Court of Chancery.
The court, now in ses-sion, is hearing an aspect of the case of Jarndyceand Jarndyce.
A ?little mad old woman?
is, as al-ways, one of the spectators.
Two ruined men, onea ?sallow prisoner,?
the other a man from Shrop-shire, appear before the court ?
to no avail.
Towardthe end of the sitting, the Lord High Chancellor an-nounces that in the morning he will meet with ?thetwo young people?
and decide about making themwards of their cousin....Starting with the set of books that had a sum-mary available from Cliff?s Notes, we removed allthe books that did not have an online version, andfurther eliminated those that did not have a summaryavailable from Grade Saver.
This left us with a ?goldstandard?
data set of 50 books, each of them withtwo manually created summaries.02000400060008000100001200014000160000  100000  200000  300000  400000SummarylengthBook lengthFigure 1: Summary and book lengths for 50 booksThe books in this collection have an averagelength of 92,000 words, with summaries with anaverage length of 6,500 words (Cliff?s Notes) and7,500 words (Grade Saver).
Figure 1 plots the lengthof the summaries (averaged over the two manualsummaries) with respect to the length of the books.As seen in the plot, most of the books have a lengthof 50,000-150,000 words, with a summary of 2,000?6,000 words, corresponding to a compression rate ofabout 5-15%.
There are also a few very long books,with more than 150,000 words, for which the sum-maries tend to become correspondingly longer.3.1 Evaluation MetricsFor the evaluation, we use the ROUGE evaluationtoolkit.
ROUGE is a method based on Ngram statis-tics, found to be highly correlated with human eval-uations (Lin and Hovy, 2003).1 Throughout the pa-per, the evaluations are reported using the ROUGE-1 setting, which seeks unigram matches betweenthe generated and the reference summaries, andwhich was found to have high correlation with hu-man judgments at a 95% confidence level.
Addi-tionally, the final system is also evaluated using theROUGE-2 (bigram matches) and ROUGE-SU4 (non-contiguous bigrams) settings, which have been fre-quently used in the DUC evaluations.In most of the previous summarization evalua-tions, the data sets were constructed specifically forthe purpose of enabling system evaluations, and thusthe length of the reference and the generated sum-maries was established prior to building the data setand prior to the evaluations.
For instance, someof the previous DUC evaluations provided refer-ence summaries of 100-word each, and required theparticipating systems to generate summaries of thesame length.However, in our case we have to deal withpre-existing summaries, with large summary-lengthvariations across the 50 books and across the tworeference summaries.
To address this problem, wedecided to keep one manual summary as the mainreference (Grade Saver), and use the other summary(Cliff?s Notes) as a way to decide on the length ofthe generated summaries.
This means that for agiven book, the Cliff?s Notes summary and all the1ROUGE is available at http://haydn.isi.edu/ROUGE/382automatically generated summaries have the samelength, and they are all evaluated against the (pos-sibly with a different length) Grade Saver summary.This way, we can also calculate an upper bound bycomparing the two manual summaries against eachother, and at the same time ensure a fair comparisonbetween the automatically generated summaries andthis upper bound.23.2 Lower and Upper BoundsTo determine the difficulty of the task on the 50 bookdata set, we calculate and report lower and upperbounds.
The lower bound is determined by using abaseline summary constructed by including the firstsentences in the book (also known in the literatureas the lead baseline).3 As mentioned in the previ-ous section, all the generated summaries ?
includ-ing this baseline ?
have a length equal to the Cliff?sNotes manual summary.
The upper bound is calcu-lated by evaluating Cliff?s Notes manual summaryagainst the reference Grade Saver summary.
Table1 shows the precision (P), recall (R), and F-measure(F) for these lower and upper bounds, calculated asaverage across the 50 books.P R FLower bound (lead baseline) 0.380 0.284 0.325Upper bound (manual summary) 0.569 0.493 0.528Table 1: Lower and upper bounds for the book sum-marization task, calculated on the 50 book data setAn automatic system evaluated on this data set istherefore expected to have an F-measure higher thanthe lower bound of 0.325, and it is unlikely to exceedthe upper bound of 0.528 obtained with a human-generated summary.4 An Initial Summarization SystemOur first book summarization experiment was doneusing a re-implementation of an existing state-of-the-art summarization system.
We decided to use the2An alternative solution would be to determine the lengthof the generated summaries using a predefined compressionrate (e.g., 10%).
However, this again implies great variationsacross the lengths of the generated versus the manual sum-maries, which can result in large and difficult to interpret varia-tions across the ROUGE scores.3A second baseline that accounts for text segments is alsocalculated and reported in section 6.centroid-based method implemented in the MEADsystem (Radev et al, 2004), for three main reasons.First, MEAD was shown to lead to good perfor-mance in several DUC evaluations, e.g., (Radev etal., 2003; Li et al, 2005).
Second, it is an unsuper-vised method which, unlike supervised approaches,does not require training data (not available in ourcase).
Finally, the centroid-based techniques imple-mented in MEAD can be optimized and made veryefficient, which is an important aspect in the sum-marization of very long documents such as books.The latest version of MEAD4 uses features, clas-sifiers and re-rankers to determine the sentences toinclude in the summary.
The default features arecentroid, position and sentence length.
The centroidvalue of a sentence is the sum of the centroid val-ues of the words in the sentence.
The centroid valueof a word is calculated by multiplying the term fre-quency (tf) of a word by the word?s inverse docu-ment frequency (idf) obtained from the Topic Detec-tion and Tracking (TDT) corpus.
The tf of a wordis calculated by dividing the frequency of a word ina document cluster by the number of documents inthe cluster.
The positional value Pi of a sentence iscalculated using the formula (Radev et al, 2004):Pi =n?
i+ 1n ?
Cmax (1)where n represents the number of sentences in thedocument, i represents the position of the sentenceinside the text, and Cmax is the score of the sentencethat has the maximum centroid value.The summarizer combines these features to givea score to each sentence.
The default setting con-sists of a linear combination of features that assignsequal weights to the centroid and the positional val-ues, and only scores sentences that have more thannine words.
After the sentences are scored, the re-rankers are used to modify the scores of a sentencedepending on its relation with other sentences.
Thedefault re-ranker implemented in MEAD first ranksthe sentences by their scores in descending orderand iteratively adds the top ranked sentence if thesentence is not too similar to the already added sen-tences.
This similarity is computed as a cosine sim-ilarity and by default the sentences that exhibit a co-sine similarity higher than 0.7 are not added to the4MEAD 3.11, http://www.summarization.com/mead/383summary.
Note that although the MEAD distributionalso includes an optional feature calculated using theLexRank graph-based algorithm (Erkan and Radev,2004), this feature could not be used since it takesdays to compute for very long documents such asours, and thus its application was not tractable.Although the MEAD system is publicly availablefor download, in order to be able to make continu-ous modifications easily and efficiently to the systemas we develop new methods, we decided to writeour own implementation.
Our implementation dif-fers from the original one in certain aspects.
First,we determine document frequency counts using theBritish National Corpus (BNC) rather than the TDTcorpus.
Second, we normalize the sentence scoresby dividing the score of a sentence by the length ofthe sentence, and instead we eliminate the sentencelength feature used by MEAD.
Note also that we donot take stop words into account when calculatingthe length of a sentence.
Finally, since we are notdoing multi-document summarization, we do not usea re-ranker in our implementation.P R FMEAD (original download) 0.423 0.296 0.348MEAD (our implementation) 0.435 0.323 0.369Table 2: Summarization results using the MEADsystemTable 2 shows the results obtained on the 50 bookdata set using the original MEAD implementation,as well as our implementation.
Although the per-formance of this system is clearly better than thebaseline (see Table 1), it is nonetheless far below theupper bound.
In the following section, we exploretechniques for improving the quality of the gener-ated summaries by accounting for the length of thedocuments.5 Techniques for Book SummarizationWe decided to make several changes to our initialsystem, in order to account for the specifics of thedata set we work with.
In particular, our data setconsists of very large documents, and correspond-ingly the summarization of such documents requirestechniques that account for their length.5.1 Sentence Position In Very LargeDocumentsThe general belief in the text summarization litera-ture (Edmunson, 1969; Mani, 2001) is that the posi-tion of sentences in a text represents one of the mostimportant sources of information for a summariza-tion system.
In fact, a summary constructed usingthe lead sentences was often found to be a compet-itive baseline, with only few systems exceeding thisbaseline during the recent DUC summarization eval-uations.Although the position of sentences in a documentseems like a pertinent heuristic for the summariza-tion of short documents, and in particular for thenewswire genre as used in the DUC evaluations, ourhypothesis is that this heuristic may not hold forthe summarization of very long documents such asbooks.
The style and topic may change several timesthroughout a book, and thus the leading sentenceswill not necessarily overlap with the essence of thedocument.To test this hypothesis, we modified our initialsystem so that it does not account for the positionof the sentences inside a document, but it only ac-counts for the weight of the constituent words.
Cor-respondingly, the score of a sentence is determinedonly as a function of the word centroids, and ex-cludes the positional score.
Table 3 shows the av-erage ROUGE scores obtained using the summariza-tion system with and without the position scores.P R FWith positional scores 0.435 0.323 0.369Without positional scores 0.459 0.329 0.383Table 3: Summarization results with and without po-sitional scoresAs suspected, removing the position scores leadsto a better overall performance, with an increase ob-served in both the precision and the recall of thesystem.
Although the position in a document is aheuristic that helps the summarization of news sto-ries and other short documents, it appears that thesentences located toward the beginning of a book arenot necessarily useful for building the summary of abook.3845.2 Text SegmentationA major difference between short and long docu-ments stands in the frequent topic shifts typicallyobserved in the later.
While short stories are usu-ally concerned with one topic at a time, long doc-uments such as books often cover more than onetopic.
Thus, the intuition is that a summary shouldinclude content covering the important aspects ofall the topics in the document, as opposed to onlygeneric aspects relevant to the document as a whole.A system for the summarization of long documentsshould therefore extract key concepts from all thetopics in the document, and this task is better per-formed when the topic boundaries are known priorto the summarization step.To accomplish this, we augment our system witha text segmentation module that attempts to deter-mine the topic shifts, and correspondingly splitsthe document into smaller segments.
Note that al-though chapter boundaries are available in some ofthe books in our data set, this is not always the caseas there are also books for which the chapters are notexplicitly identified.
To ensure an uniform treatmentof the entire data set, we decided not to use chap-ter boundaries, and instead apply an automatic textsegmentation algorithm.While several text segmentation systems havebeen proposed to date, we decided to use a graph-based segmentation algorithm using normalized-cuts (Malioutov and Barzilay, 2006), shown to ex-ceed the performance of alternative segmentationmethods.
Briefly, the segmentation algorithm startsby modeling the text as a graph, where sentencesare represented as nodes in the graph, and inter-sentential similarities are used to draw weightededges.
The similarity between sentences is calcu-lated using cosine similarity, with a smoothing fac-tor that adds the counts of the words in the neighborsentences.
Words are weighted using an adaptationof the tf.idf metric, where a document is uniformlysplit into chunks that are used for the tf.idf computa-tion.
There are two parameters that have to be set inthis algorithm: (1) the length in words of the blocksapproximating sentences; and (2) the cut-off valuefor drawing edges between nodes.
Since the methodwas originally developed for spoken lecture segmen-tation, we were not able to use the same parametersas suggested in (Malioutov and Barzilay, 2006).
In-stead, we used a development set of three books, anddetermined the optimal sentence word-length as 20and the optimal cut-off value as 25, and these are thevalues used throughout our experiments.Once the text is divided into segments, we gener-ate a separate summary for each segment, and con-sequently create a final summary by collecting sen-tences from the individual segment summaries ina round-robin fashion.
That is, starting with theranked list of sentences generated by the summa-rization algorithm for each segment, we pick onesentence at a time from each segment summary untilwe reach the desired book-summary length.A useful property of the normalized-cut segmen-tation algorithm is that one can decide apriori thenumber of segments to be generated, and so we canevaluate the summarization algorithm for differentsegmentation granularities.
Figure 2 shows the av-erage ROUGE-1 F-measure score obtained for sum-maries generated using one to 50 segments.0.370.380.390.40  5  10  15  20  25  30  35  40  45  50Rouge-1F-measureNumber of segmentsFigure 2: Summarization results for different seg-mentation granularities.As seen in the figure, segmenting the text helpsthe summarization process.
The average ROUGE-1F-measure score raises to more than 0.39 F-measurefor increasingly larger number of segments, with aplateau reached at approximately 15?25 segments,followed by a decrease when more than 30 segmentsare used.In all the following evaluations, we segment eachbook into a constant number of 15 segments; in fu-ture work, we plan to consider more sophisticatedmethods for finding the optimal number of segmentsindividually for each book.3855.3 Modified Term WeightingAn interesting characteristic of documents withtopic shifts is that words do not have an uniform dis-tribution across the entire document.
Instead, theirdistribution can vary with the topic, and thus theweight of the words should change accordingly.To account for the distribution of the words in-side the entire book, as well as inside the individualtopics (segments), we devised a weighting schemethat accounts for four factors: the segment termfrequency (stf), calculated as the number of occur-rences of a word inside a segment; the book termfrequency (tf), determined as the number of occur-rences of a word inside a book; the inverse segmentfrequency (isf), measured as the inverse of the num-ber of segments containing the word; and finally, theinverse document frequency (idf), which takes intoaccount the distribution of a word in a large exter-nal corpus (as before, we use the BNC corpus).
Aword weight is consequently determined by multi-plying the book term frequency with the segmentterm frequency, and the result is then multiplied withthe inverse segment frequency and the inverse docu-ment frequency.
We refer to this weighting schemeas tf.stf.idf.isf.Using this weighting scheme, we prevent a wordfrom having the same score across the entire book,and instead we give a higher weight to its occur-rences in segments where the word has a high fre-quency.
For instance, the word doctor occurs 30times in one of the books in our data set, which leadsto a constant tf.idf score of 36.76 across the entirebook.
Observing that from these 30 occurrences, 19appear in just one segment, the tf.stf.idf.isf weight-ing scheme will lead to a weight of 698.49 for thatsegment, much higher than e.g.
the weight of 36calculated for other segments that have only a fewoccurrences of this word.P R Ftf.idf weighting 0.463 0.339 0.391tf.stf.idf.isf weighting 0.464 0.349 0.398Table 4: Summarization results using a weightingscheme accounting for the distribution of words in-side and across segmentsTable 4 shows the summarization results obtainedfor the new weighting scheme (recall that all the re-sults are calculated for a text segmentation into 15segments).5.4 Combining Summarization MethodsThe next improvement we made was to bring anadditional source of knowledge into the system, bycombining the summarization provided by our cur-rent system with the summarization obtained from adifferent method.We implemented a variation of a centrality graph-based algorithm for unsupervised summarization,which was successfully used in the past for thesummarization of short documents.
Very briefly,the TextRank system (Mihalcea and Tarau, 2004)?
similar in spirit with the concurrently proposedLexRank method (Erkan and Radev, 2004) ?
worksby building a graph representation of the text, wheresentences are represented as nodes, and weightededges are drawn using inter-sentential word overlap.An eigenvector centrality algorithm is then appliedon the graph (e.g., PageRank), leading to a rank-ing over the sentences in the document.
An imped-iment we encountered was the size of the graphs,which become intractably large and dense for verylarge documents such as books.
In our implemen-tation we decided to use a cut-off value for drawingedges between nodes, and consequently removed allthe edges between nodes that are farther apart thana given threshold.
We use a threshold value of 75,found to work best using the same development setof three books used before.P R FOur system 0.464 0.349 0.398TextRank 0.449 0.356 0.397COMBINED 0.464 0.363 0.407Table 5: Summarization results for individual andcombined summarization algorithmsUsing the same segmentation as before (15 seg-ments), the TextRank method by itself did not lead toimprovements over our current centroid-based sys-tem.
Instead, since we noticed that the summariesgenerated with our system and with TextRank cov-ered different sentences, we implemented a methodthat combines the top ranked sentences from thetwo methods.
Specifically, the combination methodpicks one sentence at a time from the summary gen-erated by our system for each segment, followed by386one sentence selected from the summary generatedby the TextRank method, and so on.
The combi-nation method also specifically avoids redundancy.Table 5 shows the results obtained with our currentcentroid-based system the TextRank method, as wellas the combined method.5.5 Segment RankingIn the current system, all the segments identified ina book have equal weight.
However, this might notalways be the case, as there are sometimes topicsinside the book that have higher importance, andwhich consequently should be more heavily repre-sented in the generated summaries.To account for this intuition, we implemented asegment ranking method that assigns to each seg-ment a score reflecting its importance inside thebook.
The ranking is performed with a method sim-ilar to TextRank, using a random-walk model overa graph representing segments and segment simi-larities.
The resulting segment scores are multi-plied with the sentence scores obtained from thecombined method described before, normalized overeach segment, resulting in a new set of scores.
Thetop ranked sentences over the entire book are thenselected for inclusion in the summary.
Table 6 showsthe results obtained by using segment ranking.P R FCOMBINED 0.464 0.363 0.407COMBINED + Segment Ranking 0.472 0.366 0.412Table 6: Summarization results using segment rank-ing6 DiscussionIn addition to the ROUGE-1 metric, the quality of thesummaries generated with our final summarizationsystem was also evaluated using the ROUGE-2 andthe ROUGE-SU4 metrics, which are frequently usedin the DUC evaluations.
Table 7 shows the figuresobtained with ROUGE-1, ROUGE-2 and ROUGE-SU4 for our final system, for the original MEADdownload, as well as for the lower and upper bounds.The table also shows an additional baseline deter-mined by selecting the first sentences in each seg-ment, using the segmentation into 15 segments asdetermined before.
As it can be seen from the F-P R FROUGE-1Lower bound 0.380 0.284 0.325 [0.306,0.343]Segment baseline 0.402 0.301 0.344 [0.328,0.366]MEAD 0.423 0.296 0.348 [0.329,0.368]Our system 0.472 0.366 0.412 [0.394,0.428]Upper bound 0.569 0.493 0.528 [0.507,0.548]ROUGE-2Lower bound 0.035 0.027 0.031 [0.027,0.035]Segment baseline 0.040 0.031 0.035 [0.031,0.038]MEAD 0.039 0.029 0.033 [0.028,0.037]Our system 0.069 0.054 0.061 [0.055,0.067]Upper bound 0.112 0.097 0.104 [0.096,0.111]ROUGE-SU4Lower bound 0.096 0.073 0.083 [0.076,0.090]Segment baseline 0.102 0.079 0.089 [0.082,0.093]MEAD 0.106 0.076 0.088 [0.081,0.095]Our system 0.148 0.115 0.129 [0.121,0.138]Upper bound 0.210 0.182 0.195 [0.183,0.206]Table 7: Evaluation of our final book summariza-tion system using different ROUGE metrics.
The ta-ble also shows: the lower bound (first sentences inthe book); the segment baseline (first sentences ineach segment); MEAD (original system download);the upper bound (manual summary).
Confidence in-tervals for F-measure are also included.measure confidence intervals also shown in the ta-ble, the improvements obtained by our system withrespect to both baselines and with respect to theMEAD system are statistically significant (as theconfidence intervals do not overlap).Additionally, to determine the robustness of theresults with respect to the number of reference sum-maries, we ran a separate evaluation where both theGrade Saver and the Cliff?s Notes summaries wereused as reference.
As before, the length of the gener-ated summaries was determined based on the Cliff?sNotes summary.
The F-measure figures obtainedin this case using our summarization system were0.402, 0.057 and 0.127 using ROUGE-1, ROUGE-2and ROUGE-SU4 respectively.
The F-measure fig-ures calculated for the baseline using the first sen-tences in each segment were 0.340, 0.033 and 0.085.These figures are very close to those listed in Table7 where only one summary was used as a reference,suggesting that the use of more than one referencesummary does not influence the results.Regardless of the evaluation metric used, the per-formance of our book summarization system is sig-nificantly higher than the one of an existing summa-rization system that has been designed for the sum-387marization of short documents (MEAD).
In fact, ifwe account for the upper bound of 0.528, the rela-tive error rate reduction for the ROUGE-1 F-measurescore obtained by our system with respect to MEADis a significant 34.44%.The performance of our system is mainly due tofeatures that account for the length of the document:exclusion of positional scores, text segmentation andsegment ranking, and a segment-based weightingscheme.
An additional improvement is obtained bycombining two different summarization methods.
Itis also worth noting that our system is efficient, tak-ing about 200 seconds to apply the segmentation al-gorithm, plus an additional 65 seconds to generatethe summary of one book.5To assess the usefulness of our system with re-spect to the length of the documents, we analyzedthe individual results obtained for books of differentsizes.
Averaging the results obtained for the shorterbooks in our collection, i.e., 17 books with a lengthbetween 20,000 and 50,000 words, the lead base-line gives a ROUGE-1 F-measure score of 0.337,our system leads to 0.378, and the upper bound ismeasured at 0.498, indicating a relative error ratereduction of 25.46% obtained by our system withrespect to the lead baseline (accounting for the max-imum achievable score given by the upper bound).Instead, when we consider only the books with alength over 100,000 words (16 books in our data setfall under this category), the lead baseline is deter-mined as 0.347, our system leads to 0.418, and theupper bound is calculated as 0.552, which results ina higher 34.64% relative error rate reduction.
Thissuggests that our system is even more effective forlonger books, due perhaps to the features that specif-ically take into account the length of the books.There are also cases where our system does notimprove over the baseline.
For instance, for the sum-marization of Candide by Franc?ois Voltaire, our sys-tem achieves a ROUGE-1 F-measure of 0.361, whichis slightly worse than the lead baseline of 0.368.
Inother cases however, the performance of our systemcomes close to the upper bound, as it is the case withthe summarization of The House of the Seven Gablesby Nathaniel Hawthorne, which has a lead baseline5Running times measured on a Pentium IV 3GHz, 2GBRAM.of 0.296, an upper bound of 0.457, and our systemobtains 0.404.
This indicates that a possible avenuefor future research is to account for the characteris-tics of a book, and devise summarization methodsthat can adapt to the specifics of a given book suchas length, genre, and others.7 ConclusionsAlthough there is a significant body of work that hasbeen carried out on the task of text summarization,most of the research to date has been concerned withthe summarization of short documents.
In this paper,we tried to address this gap and tackled the problemof book summarization.We believe this paper made two important con-tributions.
First, it introduced a new summariza-tion benchmark, specifically targeting the evalua-tion of systems for book summarization.6 Second,it showed that systems developed for the summa-rization of short documents do not fare well whenapplied to very long documents such as books, andinstead a better performance can be achieved witha system that accounts for the length of the docu-ments.
In particular, the book summarization sys-tem we developed was found to lead to more than30% relative error rate reduction with respect to anexisting state-of-the-art summarization tool.Given the increasingly large number of booksavailable in electronic format, and correspondinglythe growing need for tools for book summarization,we believe that the topic of automatic book sum-marization will become increasingly important.
Wehope that this paper will encourage and facilitate thedevelopment of an active line of research concernedwith book summarization.AcknowledgmentsThe authors are grateful to the Language and Infor-mation Technologies research group at the Univer-sity of North Texas for useful discussions and feed-back, and in particular to Carmen Banea for sug-gesting Cliff?s Notes as a source of book summaries.This work was supported in part by a research grantfrom Google Inc. and by a grant from the Texas Ad-vanced Research Program (#003594).6The data set is publicly available and can be downloadedfrom http://lit.csci.unt.edu/index.php/Downloads388ReferencesE.
D?Avanzo and B. Magnini.
2005.
A keyphrase-basedapproach to summarization: The Lake system at DUC2005.
In Proceedings of the Document UnderstandingConference (DUC 2005).H.P.
Edmunson.
1969.
New methods in automatic ex-tracting.
Journal of the ACM, 16(2):264?285.G.
Erkan and D. Radev.
2004.
Lexpagerank: Prestigein multi-document text summarization.
In Proceed-ings of the Conference on Empirical Methods in Nat-ural Language Processing (EMNLP 2004), Barcelona,Spain, July.M.
Galley.
2006.
Automatic summarization of conversa-tional multi-party speech.
In Proceedings of the 21stNational Conference on Artificial Intelligence (AAAI2006), AAAI/SIGART Doctoral Consortium, Boston.T.
Hirao, Y. Sasaki, H. Isozaki, and E. Maeda.
2002.NTT?s text summarization system for DUC-2002.
InProceedings of the Document Understanding Confer-ence 2002 (DUC 2002).E.
Hovy and C. Lin, 1997.
Automated text summarizationin SUMMARIST.
Cambridge Univ.
Press.M.
Hu and B. Liu.
2004.
Mining and summarizingcustomer reviews.
In Proceedings of the tenth ACMSIGKDD international conference on Knowledge dis-covery and data mining, Seattle, Washington.A.
Kazantseva and S. Szpakowicz.
2006.
Challenges inevaluating summaries of short stories.
In Proceedingsof the Workshop on Task-Focused Summarization andQuestion Answering, Sydney, Australia.W.
Li, W. Li, B. Li, Q. Chen, and M. Wu.
2005.
TheHong Kong Polytechnic University at DUC 2005.
InProceedings of the Document Understanding Confer-ence (DUC 2005), Vancouver, Canada.C.Y.
Lin and E.H. Hovy.
2003.
Automatic evaluation ofsummaries using n-gram co-occurrence statistics.
InProceedings of Human Language Technology Confer-ence (HLT-NAACL 2003), Edmonton, Canada, May.H.
Luhn.
1958.
The automatic creation of literature ab-stracts.
IBM Journal of Research and Development,2(2):159?165.I.
Malioutov and R. Barzilay.
2006.
Minimum cut modelfor spoken lecture segmentation.
In Proceedings of theAnnual Meeting of the Association for ComputationalLinguistics (COLING-ACL 2006), pages 9?16.I.
Mani.
2001.
Automatic Summarization.
John Ben-jamins.R.
Mihalcea and P. Tarau.
2004.
TextRank ?
bringingorder into texts.
In Proceedings of the Conference onEmpirical Methods in Natural Language Processing(EMNLP 2004), Barcelona, Spain.D.
Radev, J. Otterbacher, H. Qi, and D. Tam.
2003.MEAD ReDUCs: Michigan at DUC 2003.
In Pro-ceedings of the Document Understanding Conference(DUC 2003).D.
Radev, H. Jing, M. Stys, and D. Tam.
2004.
Centroid-based summarization of multiple documents.
Informa-tion Processing and Management, 40.G.
Salton and C. Buckley.
1997.
Term weighting ap-proaches in automatic text retrieval.
In Readings inInformation Retrieval.
Morgan Kaufmann Publishers,San Francisco, CA.G.
Salton, A. Singhal, M. Mitra, and C. Buckley.
1997.Automatic text structuring and summarization.
Infor-mation Processing and Management, 2(32).S.
Teufel and M. Moens.
1997.
Sentence extraction as aclassification task.
In ACL/EACL workshop on intelli-gent and scalable text summarization, Madrid, Spain.S.
Wan and K. McKeown.
2004.
Generating overviewsummaries of ongoing email thread discussions.
InProceedings of the 20th International Conference onComputational Linguistics, Geneva, Switzerland.F.
Wolf and E. Gibson.
2004.
Paragraph-, word-, andcoherence-based approaches to sentence ranking: Acomparison of algorithm and human performance.
InProceedings of the Annual Meeting of the Associationfor Computational Linguistics (ACL 2004), Barcelona,Spain, July.L.
Zhou and E. Hovy.
2003.
A Web-trained extrac-tion summarization system.
In Proceedings of Hu-man Language Technology Conference (HLT-NAACL2003), Edmonton, Canada, May.L.
Zhou and E. Hovy.
2005.
Digesting virtual ?geek?culture: The summarization of technical internet re-lay chats.
In Proceedings of Association for Computa-tional Linguistics (ACL 2005), Ann Arbor.L.
Zhuang, F. Jing, and X.Y.
Zhu.
2006.
Movie re-view mining and summarization.
In Proceedings ofthe ACM international conference on Information andknowledge management (CIKM 2006), Arlington, Vir-ginia.389
