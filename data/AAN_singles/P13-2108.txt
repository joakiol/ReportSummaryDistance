Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 610?616,Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational LinguisticsThe Effect of Higher-Order Dependency Features in DiscriminativePhrase-Structure ParsingGregory F. Coppola and Mark SteedmanSchool of InformaticsThe University of Edinburghg.f.coppola@sms.ed.ac.uksteedman@inf.ed.ac.ukAbstractHigher-order dependency features areknown to improve dependency parser ac-curacy.
We investigate the incorporationof such features into a cube decodingphrase-structure parser.
We find consid-erable gains in accuracy on the range ofstandard metrics.
What is especially in-teresting is that we find strong, statisti-cally significant gains on dependency re-covery on out-of-domain tests (Brown vs.WSJ).
This suggests that higher-order de-pendency features are not simply over-fitting the training material.1 IntroductionHigher-order dependency features encode morecomplex sub-parts of a dependency tree struc-ture than first-order, bigram head-modifier rela-tionships.1 The clear trend in dependency pars-ing has been that the addition of such higher-orderfeatures improves parse accuracy (McDonald &Pereira, 2006; Carreras, 2007; Koo & Collins,2010; Zhang & Nivre, 2011; Zhang & McDonald,2012).
This finding suggests that the same ben-efits might be observed in phrase-structure pars-ing.
But, this is not necessarily implied.
Phrase-structure parsers are generally stronger than de-pendency parsers (Petrov et al, 2010; Petrov &McDonald, 2012), and make use of more kindsof information.
So, it might be that the infor-mation modelled by higher-order dependency fea-tures adds less of a benefit in the phrase-structurecase.1Examples of first-order and higher-order dependencyfeatures are given in ?3.2.To investigate this issue, we experiment usingHuang?s (2008) cube decoding algorithm.
Thisalgorithm allows structured prediction with non-local features, as discussed in ?2.
Collins?s (1997)strategy of expanding the phrase-structure parser?sdynamic program to incorporate head-modifier de-pendency information would not scale to the com-plex kinds of dependencies we will consider.
Us-ing Huang?s algorithm, we can indeed incorporatearbitrary types of dependency feature, using a sin-gle, simple dynamic program.Compared to the baseline, non-local featureset of Collins (2000) and Charniak & Johnson(2005), we find that higher-order dependenciesdo in fact tend to improve performance signifi-cantly on both dependency and constituency ac-curacy metrics.
Our most interesting finding,though, is that higher-order dependency featuresshow a consistent and unambiguous contributionto the dependency accuracy, both labelled and un-labelled, of our phrase-structure parsers on out-of-domain tests (which means, here, trained onWSJ, but tested on BROWN).
In fact, the gains areeven stronger on out-of-domain tests than on in-domain tests.
One might have thought that higher-order dependencies, being rather specific by na-ture, would tend to pick out only very rare events,and so only serve to over-fit the training material,but this is not what we find.
We speculate as towhat this might mean in ?5.2.The cube decoding paradigm requires a first-stage parser to prune the output space.
For this, weuse the generative parser of Petrov et al (2006).We can use this parser?s model score as a fea-ture in our discriminative model at no additionalcost.
However, doing so conflates the contribu-tion to accuracy of the generative model, on theone hand, and the discriminatively trained, hand-610written, features, on the other.
Future systemsmight use the same or a similar feature set toours, but in an architecture that does not includeany generative parser.
On the other hand, somesystems might indeed incorporate this generativemodel?s score.
So, we need to know exactly whatthe generative model is contributing to the accu-racy of a generative-discriminative model combi-nation.
Thus, we conduct experiments in sets: insome cases the generative model score is used, andin others it is not used.Compared to the faster and more psycholog-ically plausible shift-reduce parsers (Zhang &Nivre, 2011; Zhang & Clark, 2011), cube decod-ing is a computationally expensive method.
But,cube decoding provides a relatively exact envi-ronment with which to compare different featuresets, has close connections with modern phrase-based machine translation methods (Huang & Chi-ang, 2007), and produces very accurate parsers.
Insome cases, one might want to use a slower, butmore accurate, parser during the training stage ofa semi-supervised parser training strategy.
For ex-ample, Petrov et al (2010) have shown that a fastparser (Nivre et al, 2007) can be profitably trainedfrom the output of a slower but more accurate one(Petrov et al, 2006), in a strategy they call uptrain-ing.We make the source code for these experimentsavailable.22 Phrase-Structure Parsing withNon-Local Features2.1 Non-Local FeaturesTo decode using exact dynamic programming (i.e.,CKY), one must restrict oneself to the use of onlylocal features.
Local features are those that fac-tor according to the individual rule productions ofthe parse.
For example, a feature indicating thepresence of the rule S ?
NP VP is local.3 But,a feature that indicates that the head word of thisS is, e.g., joined, is non-local, because the headword of a phrase cannot be determined by look-ing at a single rule production.
To find a phrase?shead word (or tag), we must recursively find the2See http://gfcoppola.net/code.php.
Thissoftware is available for free for non-profit research uses.3A feature indicating that, e.g., the first word dominatedby S is Pierre is also local, since the words of the sentenceare constant across hypothesized parses, and words can bereferred to by their position with respect to a given rule pro-duction.
See Huang (2008) for more details.head phrase of each local rule production, until wereach a terminal node (or tag node).
This recursionwould not be allowed in standard CKY.
Many dis-criminative parsers have used only local features(Taskar et al, 2004; Turian et al, 2007; Finkelet al, 2008).
However, Huang (2008) shows thatthe use of non-local features does in fact contributesubstantially to parser performance.
And, our de-sire to make heavy use of head-word dependencyrelations necessitates the use of non-local features.2.2 Cube DecodingWhile the use of non-local features destroys theability to do exact search, we can still do inex-act search using Huang?s (2008) cube decodingalgorithm.4 A tractable first-stage parser prunesthe space of possible parses, and outputs a forest,which is a set of rule production instances that canbe used to make a parse for the given sentence,and which is significantly pruned compared to theentire space allowed by the grammar.
The size ofthis forest is at most cubic in the length of the sen-tence (Billot & Lang, 1989), but implicitly repre-sents exponentially many parses.
To decode, wefix an beam width of k (an integer).
Then, whenparsing, we visit each node n in the same bottom-up order we would use for Viterbi decoding, andcompute a list of the top k parses to n, accordingto a global linear model (Collins, 2002), using thetrees that have survived the beam at earlier nodes.2.3 The First-Stage ParserAs noted, we require a first-stage parser to prunethe search space.5 As a by-product of this pruningprocedure, we are able to use the model score ofthe first-stage parser as a feature in our ultimatemodel at no additional cost.
As a first-stage parser,we use Huang et al?s (2010) implementation ofthe LA-PCFG parser of Petrov et al (2006), whichuses a generative, latent-variable model.3 Features3.1 Phrase-Structure FeaturesOur phrase-structure feature set is taken fromCollins (2000), Charniak & Johnson (2005), and4This algorithm is closely related to the algorithm forphrase-based machine translation using a language model(Huang & Chiang, 2007).5All work in this paradigm has used a generative parser asthe first-stage parser.
But, this is arguably a historical acci-dent.
We could just as well use a discriminative parser withonly local features, like Petrov & Klein (2007a).611Huang (2008).
Some features are omitted, withchoices made based on the ablation studies ofJohnson & Ural (2010).
This feature set, which wecall ?phrase, contains the following, mostly non-local, features, which are described and depictedin Charniak & Johnson (2005), Huang (2008), andJohnson & Ural (2010):?
CoPar The depth (number of levels) of par-allelism between adjacent conjuncts?
CoParLen The difference in length betweenadjacent conjuncts?
Edges The words or (part-of-speech) tags onthe outside and inside edges of a given XP6?
NGrams Sub-parts of a given rule production?
NGramTree An n-gram of the input sen-tence, or the tags, along with the minimal treecontaining that n-gram?
HeadTree A sub-tree containing the pathfrom a word to its maximal projection, alongwith all siblings of all nodes in that path?
Heads Head-modifier bigrams?
Rule A single rule production?
Tag The tag of a given word?
Word The tag of and first XP above a word?
WProj The tag of and maximal projection ofa wordHeads is a first-order dependency feature.3.2 Dependency Parsing FeaturesMcDonald et al (2005) showed that chart-baseddependency parsing, based on Eisner?s (1996) al-gorithm, could be successfully approached in adiscriminative framework.
In this earliest work,each feature function could only refer to a sin-gle, bigram head-modifier relationship, e.g., Mod-ifier, below.
Subsequent work (McDonald &Pereira, 2006; Carreras, 2007; Koo & Collins,2010) looked at allowing features to access morecomplex, higher-order relationships, including tri-gram and 4-gram relationships, e.g., all featuresapart from Modifier, below.
With the ability toincorporate non-local phrase-structure parse fea-tures (Huang, 2008), we can recognize depen-dency features of arbitrary order (cf.
Zhang &McDonald (2012)).
Our dependency feature set,which we call ?deps, contains:?
Modifier head and modifier6The tags outside of a given XP are approximated usingthe marginally most likely tags given the parse.?
Sibling head, modifier m, and m?s nearest in-ner sibling?
Grandchild head, modifier m, and one ofm?s modifiers?
Sibling+Grandchild head, modifier m, m?snearest inner sibling, and one of m?s modi-fiers?
Grandchild+Grandsibling head, modifierm, one of m?s modifiers g, and g?s inner sib-lingThese features are insensitive to arc labels in thepresent experiments, but future work will incorpo-rate arc labels.
Each feature class contains moreand less lexicalized versions.3.3 Generative Model Score FeatureFinally, we have a feature set, ?gen, contain-ing only one feature function.
This featuremaps a parse to the logarithm of the MAX-RULE-PRODUCT score of that parse according to the LA-PCFG parsing model, which is trained separately.This score has the character of a conditional like-lihood for the parse (see Petrov & Klein (2007b)).4 TrainingWe have two feature sets ?phrase and ?deps, forwhich we fix weights using parallel stochastic op-timization of a structured SVM objective (Collins,2002; Taskar et al, 2004; Crammer et al, 2006;Martins et al, 2010; McDonald et al, 2010).
Tothe single feature in the set ?gen (i.e.
the genera-tive model score), we give the weight 1.The combined models, ?phrase+deps, ?phrase+gen,and ?phrase+deps+gen, are then model combinationsof the first three.
The combination weightsfor these combinations are obtained using Och?s(2003) Minimum Error-Rate Training (MERT).The MERT stage helps to avoid feature under-training (Sutton et al, 2005), and avoids the prob-lem of scaling involved in a model that containsmostly boolean features, but one, real-valued, log-scale feature.
Training is conducted in three stages(SVM, MERT, SVM), so that there is no influence ofany data outside the given training set (WSJ2-21)on the combination weights.5 Experiments5.1 MethodsAll models are trained on WSJ2-21, with WSJ22used to pick the stopping iteration for online612Test SetWSJ BROWNType Model F1 UAS LAS F1 UAS LASG LA-PCFG 90.3 93.7 91.5 85.1 88.7 85.0D phrase 91.2 93.9 91.0 86.1 89.4 85.1deps ?
93.3 ?
?
89.3 ?phrase+deps 91.7 94.4 91.5 86.4 90.1 85.9G+D phrase+gen 92.1 94.7 92.6 87.0 90.0 86.5phrase+deps+gen 92.4 94.9 92.8 87.4 90.7 87.1Table 1: Performance of the various models in cube decoding experiments, on the WSJ test set (in-domain) and the BROWN test set (out-of-domain).
G abbreviates generative, D abbreviates discrim-inative, and G+D a combination.
Some cells are empty because ?deps features are only sensitive tounlabelled dependencies.
Best results in D and G+D conditions appear in bold face.Test SetHypothesis WSJ BROWNGreater Lesser F1 UAS LAS F1 UAS LASphrase+deps phrase .042 .029 .018 .140 .022 .009phrase+deps deps ?
<.001 ?
?
.012 ?phrase+gen phrase .013 .003 <.001 .016 .090 <.001phrase+deps+gen phrase+gen .030 .122 .151 .059 .008 .020phrase+deps+gen phrase+deps .019 .020 <.001 .008 .040 <.001Table 2: Results of statistical significance evaluations of hypotheses of the form X?s accuracy is greaterthan Y?s on the various test sets and metrics.
Bold face indicates p < .05.optimization, as is standard.
The test sets areWSJ23 (in-domain test set), and BROWN9 (out-of-domain test set) from the Penn Treebank (Mar-cus et al, 1993).7 We evaluate using harmonicmean between labelled bracket recall and preci-sion (EVALB F1), unlabelled dependency accuracy(UAS), and labelled dependency accuracy (LAS).Dependencies are extracted from full output treesusing the algorithm of de Marneffe & Manning(2008).
We chose this dependency extractor,firstly, because it is natively meant to be run onthe output of phrase-structure parsers, rather thanon gold trees with function tags and traces stillpresent, as is, e.g., the Penn-Converter of Johans-son & Nugues (2007).
Also, this is the extractorthat was used in a recent shared task (Petrov &McDonald, 2012).
We use EVALB and eval.pl tocalculate scores.For hypothesis testing, we used the paired boot-strap test recently empirically evaluated in the con-text of NLP by Berg-Kirkpatrick et al (2012).
This7Following Gildea (2001), the BROWN test set is usuallydivided into 10 parts.
If we start indexing at 0, then the last(test) section has index 9.
We received the BROWN data splitsfrom David McClosky, p.c.involves drawing b subsamples of size n with re-placement from the test set in question, and check-ing relative performance of the models on the sub-sample (see the reference).
We use b = 106 andn = 500 in all tests.5.2 ResultsThe performance of the models is shown in Table1, and Table 2 depicts the results of significancetests of differences between key model pairs.We find that adding in the higher-order depen-dency feature set, ?deps, makes a statistically sig-nificant improvement in accuracy on most met-rics, in most conditions.
On the in-domain WSJtest set, we find that ?phrase+deps is significantlybetter than either of its component parts on allmetrics.
But, ?phrase+deps+gen is significantly bet-ter than ?phrase+gen only on F1, but not on UASor LAS.
However, on the out-of-domain BROWNtests, we find that adding ?deps always adds con-siderably, and in a statistically significant way, toboth LAS and UAS.
That is, not only is ?phrase+depsbetter at dependency recovery than its componentparts, but ?phrase+deps+gen is also considerably bet-613ter on dependency recovery than ?phrase+gen, whichrepresents the previous state-of-the-art in this veinof research (Huang, 2008).
This result is perhapscounter-intuitive, in the sense that one might havesupposed that higher-order dependency features,being highly specific by nature, might only haveonly served to over-fit the training material.
How-ever, this result shows otherwise.
Note that thedependency features include various levels of lex-icalization.
It might be that the more unlexical-ized features capture something about the struc-ture of correct parses, that transfers well out-of-domain.
Future work should investigate this.
And,it of course remains to be seen how this result willtransfer to other train-test domain pairs.To our knowledge, this is the first work tospecifically separate the role of the generativemodel feature from the other features of Collins(2000) and Charniak & Johnson (2005).
We notethat, even without the ?gen feature, the discrimi-native parsing models are very strong, but adding?gen nevertheless yields considerable gains.
Thus,while a fully discriminative model, perhaps im-plemented using a shift-reduce algorithm, can beexpected to do very well, if the best accuracy isnecessary (e.g., in a semi-supervised training strat-egy), it still seems to pay to use the generative-discriminative model combination.
Note that theLAS scores of our models without ?gen are rela-tively weak.
This is presumably largely becauseour dependency features are, at present, not sen-sitive to arc labels, so our results probably under-estimate the capability of our general frameworkwith respect to labelled dependency recovery.Table 3 compares our work with Huang?s(2008).
Note that our model ?phrase+gen uses es-sentially the same features as Huang (2008), sothe fact that our ?phrase+gen is noticeably more ac-curate on F1 is presumably due to the benefitsin reduced feature under-training achieved by theMERT combination strategy.
Also, our ?phrase+depsmodel is as accurate as Huang?s, without even us-ing the generative model score feature.
Table 4compares our work to McClosky et al?s (2006)domain adaptation work with the Charniak &Johnson (2005) parser.
Their three models shownhave been trained on: i) the WSJ (supervised,out-of-domain), ii) the WSJ plus 2.5 million sen-tences of automatically labelled NANC newswiretext (semi-supervised, out-of-domain), and iii) theBROWN corpus (supervised, in-domain).
We testType Model WSJG+D Huang (2008) 91.7D phrase+deps 91.7G+D phrase+gen 92.1G+D phrase+deps+gen 92.4Table 3: Comparison of constituency parsing re-sults in the cube decoding framework, on the WSJtest set.
On G+D, D, see Table 1.Parser Training Data BROWN F1CJ WSJ 85.2CJ WSJ+NANC 87.8CJ BROWN 88.4Our Best WSJ 87.4Table 4: Comparison of our best model,?phrase+deps+gen, on BROWN, with the Charniak &Johnson (2005) parser, denoted CJ, as reported inMcClosky et al (2006).
Underline indicates besttrained on WSJ, bold face indicates best overall.on BROWN.
We see that our best (WSJ-trained)model is over 2% more accurate (absolute F1difference) than the Charniak & Johnson (2005)parser trained on the same data.
In fact, ourbest model is nearly as good as McClosky et al?s(2006) self-trained, semi-supervised model.
Ofcourse, the self-training strategy is orthogonal tothe improvements we have made.6 ConclusionWe have shown that the addition of higher-orderdependency features into a cube decoding phase-structure parser leads to statistically significantgains in accuracy.
The most interesting findingis that these gains are clearly observed on out-of-domain tests.
This seems to imply that higher-order dependency features do not merely over-fitthe training material.
Future work should look atother train-test domain pairs, as well as look at ex-actly which higher-order dependency features aremost important to out-of-domain accuracy.AcknowledgmentsThis work was supported by the Scottish Infor-matics and Computer Science Alliance, The Uni-versity of Edinburgh?s School of Informatics, andERC Advanced Fellowship 249520 GRAMPLUS.We thank Zhongqiang Huang for his extensivehelp in getting started with his LA-PCFG parser.614ReferencesBerg-Kirkpatrick, T., Burkett, D., & Klein, D.(2012).
An empirical investigation of statisticalsignificance in NLP.
In EMNLP, 995?1005.Billot, S., & Lang, B.
(1989).
The structure ofshared forests in ambiguous parsing.
In ACL,143?151.Carreras, X.
(2007).
Experiments with a higher-order projective dependency parser.
In Pro-ceedings of the CoNLL Shared Task Session ofEMNLP-CoNLL 2007, 957?961.Charniak, E., & Johnson, M. (2005).
Coarse-to-fine n-best parsing and MaxEnt discriminativereranking.
In ACL, 173?180.Collins, M. (1997).
Three generative, lexicalisedmodels for statistical parsing.
In ACL, 16?23.Collins, M. (2000).
Discriminative reranking fornatural language parsing.
In ICML, 175?182.Collins, M. (2002).
Discriminative training meth-ods for Hidden Markov Models: theory andexperiments with perceptron algorithms.
InEMNLP, 1?8.Crammer, K., Dekel, O., Keshet, J., Shalev-Shwartz, S., & Singer, Y.
(2006).
Onlinepassive-aggressive algorithms.
JMLR, 7, 551?585.Eisner, J.
(1996).
Three new probabilistic mod-els for dependency parsing: An exploration.
InCOLING, 340?345.Finkel, J. R., Kleeman, A., & Manning, C. D.(2008).
Efficient, feature-based, conditionalrandom field parsing.
In ACL, 959?967.Gildea, D. (2001).
Corpus variation and parserperformance.
In EMNLP, 167?202.Huang, L. (2008).
Forest reranking: Discrimina-tive parsing with non-local features.
In ACL,586?594.Huang, L., & Chiang, D. (2007).
Forest rescor-ing: Faster decoding with integrated languagemodels.
In ACL.Huang, Z., Harper, M., & Petrov, S. (2010).
Self-training with products of latent variable gram-mars.
In EMNLP, 12?22.Johansson, R., & Nugues, P. (2007).
Extendedconstituent-to-dependency conversion for En-glish.
In Proc.
of the 16th Nordic Conference onComputational Linguistics (NODALIDA), 105?112.Johnson, M., & Ural, A. E. (2010).
Reranking theBerkeley and Brown parsers.
In HLT-NAACL,665?668.Koo, T., & Collins, M. (2010).
Efficient third-order dependency parsers.
In ACL, 1?11.Marcus, M. P., Santorini, B., & Marcinkiewicz,M.
A.
(1993).
Building a large annotated corpusof English: The Penn Treebank.
ComputationalLinguistics, 19(2), 313?330.de Marneffe, M.-C., & Manning, C. D. (2008).The Stanford typed dependencies representa-tion.
In Coling 2008: Proceedings of the work-shop on Cross-Framework and Cross-DomainParser Evaluation, 1?8.Martins, A. F., Gimpel, K., Smith, N. A., Xing,E.
P., Figueiredo, M. A., & Aguiar, P. M.(2010).
Learning structured classifiers with dualcoordinate ascent.
Technical report, DTIC Doc-ument.McClosky, D., Charniak, E., & Johnson, M.(2006).
Reranking and self-training for parseradaptation.
In ACL, 337?344.McDonald, R., & Pereira, F. (2006).
Online learn-ing of approximate dependency parsing algo-rithms.
In EACL, 81?88.McDonald, R. T., Crammer, K., & Pereira, F. C. N.(2005).
Online large-margin training of depen-dency parsers.
In ACL, 91?98.McDonald, R. T., Hall, K., & Mann, G. (2010).Distributed training strategies for the structuredperceptron.
In HLT-NAACL, 456?464.Nivre, J., Hall, J., Nilsson, J., Chanev, A., Eryigit,G., Ku?bler, S., Marinov, S., & Marsi, E. (2007).Maltparser: A language-independent system fordata-driven dependency parsing.
Natural Lan-guage Engineering, 13(2), 95?135.Och, F. J.
(2003).
Minimum error rate trainingin statistical machine translation.
In ACL, 160?167.Petrov, S., Barrett, L., Thibaux, R., & Klein, D.(2006).
Learning accurate, compact, and inter-pretable tree annotation.
In ACL, 433?440.Petrov, S., Chang, P.-C., Ringgaard, M., & Al-shawi, H. (2010).
Uptraining for accurate deter-ministic question parsing.
In EMNLP, 705?713.Petrov, S., & Klein, D. (2007a).
Discriminativelog-linear grammars with latent variables.
InNIPS.615Petrov, S., & Klein, D. (2007b).
Improved infer-ence for unlexicalized parsing.
In HLT-NAACL,404?411.Petrov, S., & McDonald, R. (2012).
Overview ofthe 2012 shared task on parsing the web.
Notesof the First Workshop on Syntactic Analysis ofNon-Canonical Language (SANCL).Sutton, C., Sindelar, M., & McCallum, A.
(2005).Feature bagging: Preventing weight undertrain-ing in structured discriminative learning.
InHLT-NAACL.Taskar, B., Klein, D., Collins, M., Koller, D., &Manning, C. D. (2004).
Max-margin parsing.In EMNLP, 1?8.Turian, J., Wellington, B., & Melamed, I. D.(2007).
Scalable discriminative learning for nat-ural language parsing and translation.
In NIPS,1409?1416.Zhang, H., & McDonald, R. (2012).
General-ized higher-order dependency parsing with cubepruning.
In EMNLP, 238?242.Zhang, Y., & Clark, S. (2011).
Shift-reduce CCGparsing.
In ACL, 683?692.Zhang, Y., & Nivre, J.
(2011).
Transition-baseddependency parsing with rich non-local fea-tures.
In ACL, 188?293.616
