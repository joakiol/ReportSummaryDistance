Towards a More Careful Evaluat ion of Broad CoverageParsing SystemsWide It.
Hogenhout  and Yuji MatsumotoNara  Ins t i tu te  of Sc ience and  Techno logy8916-5 Takayama,  IkomaNara  630-01, Japan{ marc -  h, matsu  } @is.
a ist -  nara .
ac .
jpAbstractSince treebanks have become availableto researchers a wide variety of tech-niques has been used to make broad cov-erage parsing systems.
This makes quan-titative evaluation very important, butthe current evaluation methods have anumber of drawbacks uch as arbitrarychoices in the treebank and the difficultyin measuring statistical significance.
Wesuggest a more detailed method for test-ing a parsing system using constituentboundaries, with a number of measuresthat give more information than currentmeasures, and evaluate the quality of thetest.
We also show that statistical signif-icance cannot be calculated in a straight-forward way, and suggest a calculationmethod for the case of Bracket Recall.1 Introduct ionDuring the last few years large treebanks have be-come available to many researchers, which has re-sulted in researches applying a range of new tech-niques for parsing systems.
Most of the meth-ods that are being suggested include some kindof Machine Learning, such as history based gram-mars and decision tree models (Black et al, 1993;Magerman, 1995), training or inducing statisti-cal grammars (Black, Garside and Leech, 1993;Pereira and Schabes, 1992; Schabes et al, 1993),or other techniques (Bod, 1993).Consequently, syntactical analysis has becomean area with a wide variety of (a) algorithms andmethods for learning and parsing, and (b) type ofinformation used for learning and parsing (some-times referred to as feature set).
These meth-ods only could become popular through evalua-tion methods for parsing systems, such as BracketAccuracy, Bracket Recall, Sentence Accuracy andViterbi Score.
Some of them were introduced in(Black et al, 1991; Harrison et M., 1991).These evaluation metrics have a number ofproblems, and in this paper we argue that theyneed to be reconsidered, and give a number ofsuggestions either to overcome those problems orto gain a better understanding of those prob-lems.
Particular problems we look at are arbi-trary choices in the treebank, errors in the tree-bank, types of errors made by parsers, and thestatistical significance of differences in test scoresby parsers.2 Prob lems wi th  Evaluat ionMetr icsUntil now a number of problems with evaluationhave been pointed out.
One well known prob-lem is that measures based only on the absenceof crossing errors on sentence level, such as Sen-tence Accuracy and Viterbi Consistency, are notusable for parsing systems that apply a partialbracketing, since a sparse bracketing improves thescore.
For example (Lin, 1995) discusses someother problems, but suggests an alternative thatis difficult to apply.
It is based on transferringconstituency trees to dependency trees, but thatintroduces many ad hoc choices, and treebankswith dependency trees are hardly available.Also, a treebank usually contains arbitrarychoices (besides errors) made by humans, in caseswhere it was not clear what brackets correctly re-flect the syntactical structure of the sentence.We also mention some less discussed problems.First of all, given a test result such as BracketAccuracy, it is necessary to know the confidanceinterval.
In other words, if a parsing system scores81.2% on a test, in what range should we assumethe estimate to be?
Basically the same prob-lem arises with the statistical significance of tiledifference between the test score of two differentparsers.
If one scores 81.2% and the other 82.5%,should we conclude the second one is really doingbetter?This is particularly important when developinga parsing system by trying various modifications,and choosing the one that performs the best on atest set.
If the differences between scores becometoo small in relation to the test set, one will just562be making a parser for the test set and the per-formance will drop as soon as other data is used.There are several problems for deciding signif-icance for Bracket Accuracy and Bracket Recall.There is a strong variation between brackets, be-cause some brackets are very easy and SOlIle arevery hard.
Also one mistake may lead other mis-takes, making them not independent.
As an ex-ample of the last problem, think of the indicatedbracket pair in the sentence "The dog waited for/his master on the bridge\]."
This would probablyproduce a crossing error, since the treebank wouldprobably contain the pair "The dog/waited for hismaste~\] on the bridge."
The parser is now almostcertain to make a second mistake, namely "Thedog waited \[for his master on the bridge\]."
Conse-quently two crossing errors are counted, whereascorrecting one would imply correcting the other.In this article we will show that this makes it im-possible to calculate the significance in a straight-forward way and suggest wo solutions.Another 1)roblem is that we only get a very gen-eral picture, whereas it would be interesting toknow much more details.
For example, how manyof the bracket-pairs that constituted a crossing er-ror when compared to the treebank would be ac-ceptable to a human?
(In other words, how oftendo arbitrary choices influence the result?)
And,how many brackets that the parser produces arenot in the treebank nor constitute a crossing er-ror, and how many of those are not acceptable tohumans?Bracket Accuracy is often lower than it shouldbe when the treebank does not indicate all brack-ets (so-called skeleton parsing).
This may alsomake Bracket Recall seem too low.In this paper we suggest giving more specificinformation about test results, and develop meth-ods to estimate the statistical significance for testscores.3 More  Care fu l  MeasuresThe data resulting from the test may be (a) gen-eral data fl'om all bracket pairs, or (b) data on spe-cific structures (i.e.
prepositional phrases).
Themeasures we give can be applied to either one.We suggest perforlning two types of tests: regu-lar tests and tests with a human check.
The regu-lar test should include a nuinber of figures that wedescribe below, which are much more informativethan the usual Bracket Recall or Bracket Preci-sion.
The more elaborate one includes a humancheck on certain items, which not only gives moreexact information on the test result, but in partic-ular shows the quality of the regular test.
This isparticularly useful if the parsing system was madeindependently from the treebank.The items for the regular test are listed here.The last four items only apply to a comparisonof two parsing systems (for example two modifi-cations of the same system), here referred to as Aand B.
* TTB:  Total Treebank Brackets, number ofbrackets in the treebank.. TPB:  Total Parse Brackets, number of brack-ets produced by the parsing system.. EM: Exact Match, the nmnber of bracket-pairs produced by the parsing system thatare equal to a pair in the treebank.. CE: Crossing Error, the number of bracket-pairs produced by the parsing system thatconstitute a crossing error against the tree-bank.. SP: Spurious, number of bracket pairs pro-dated by the parsing system that were notin the treebank but also do not constitute acrossing error.
* PINH: Parse-error Inherited, the number ofbracket-pairs produced by the parsing systemthat constitute a crossing error and have a di-rect parent bracket-pair that also constitutesa crossing error.. PNINI I :  Parse-error Non-Inherited, the num-ber of bracket-pairs produced by the parsingsystem that constitute a crossing error, butwere not counted for PINH.?
TINH: Treebank Inherited, the number ofbracket-pairs in the treebank that were repro-duced by the parsing system and have a directparent bracket-pair n the treebank that wasalso reproduced.?
TNINH:  Treebank Non-Inherited, the nmn-ber of bracket-pairs in the treebank that werereproduced by the parsing system but werenot counted for TINH.. YY:  Number of brackets in the treebank thatwere reproduced by A and B.?
YN: Number of brackets in the treebank thatwere reproduced by A but not by B.?
NY :  Number of brackets in the treebank thatwere reproduced by B but not by A.
* NN: Number of brackets in the treebank thatwere not reproduced by both A and B.As an example, we take this 2 sentence test:Treebank:\[He \[walks to \[the house\[\]\]\[\[The president\] [gave \[a long speech\]\]\]Parser:\[He \[walks \[to \[the housel\]\]\]\[The \[\[president gavel \[a \[long speech\]\]l\]The number of exactly matching brackets (EM)is 3+2 = 5.
The number of crossing errors (CE) is5632, both in the second sentence.
The rest, 1-1-1 = 2is spurious (SP).
Further, TTB is 7, TPB is 9,PINH is 1 and PNINH is 1, TINH is 1 and TNINHis 4.This already gives more detailed information,but we can take things a step further by having ahuman evaluate the most important brackets.
Ifthe test set is large, it would be undesirable orimpossible to have a human evaluate very singlebracket, but we can seriously reduce the workloadby not considering the exact matching bracketpairs; they are simply marked as 'accepted.'
Theonly result of evaluating these brackets would bea few errors in the treebank, which is often notreally worth the trouble (unless the treebank issuspected to contain many errors).
This leavesonly the crossing errors and spurious brackets tobe evaluated.This leaves a much smaller amount of work, es-pecially if there are many exact matches.
Never-theless we suggest doing a human check only onimportant ests, such as final evaluations.In the human evaluation, crossing error andspurious bracket pairs are to be counted as 'ac-ceptable' if they would fit into the correct interpre-tation using the style of bracketing that the pars-ing system aims at, ignoring the style of bracket-ing of the treebank.The result of this process is that EM, CE andSP will be divided in accepted and rejected, givingsix groups.
We will refer to them as EMA, EMR,CEA, CER, SPA and SPR.
If the check on EM isnot performed, as we suggest, EMR will be 0.If YN and NY are both relatively high, thisshows that there are structures on which A is bet-ter than B and vice versa (the systems 'comple-ment' each other).
In that case we would rec-ommend testing on (more) specific structures, be-cause otherwise the general result will be mislead-ing.4 A Pract ical  ExampleTo show the difference between the usual evalu-ation and our evaluation method we give the re-sults for two parsing systems we evaluated in thecourse of our research.
We do not intend to makeany particular claims about these parsing systems,nor about the treebank we used (the test was notdesigned to draw conclusions about the treebank),we only use it to discuss the issues involved in eval-uation.The treebank we used was the EDR corpus(EDR, 1995), a Japanese treebank with mainlynewspaper sentences.
We compared two versionsof a grammar based parsing system developed atour laboratory, using a stochastical grammar toselect one parse for every sentence.
Having twovariations of the same parser, we were interestedin the difference between them.
We performed atest on 600 sentences from the corpus (which werenot used for training).Our evaluation was as follows:1.
Unrelevant elements uch as punctuation areeliminated fl'om both the treebank tree andthe parse tree.2.
Next, all (resulting) empty bracket-pairsare removed.
This was done recursively,therefore, if removing an empty bracket-paircaused its parent to become empty, the par-ent is also removed.3.
Double bracket-pairs are removed.
For exam-ple "The l/old man\]\]" is turned into "The \[oldmagi'.4.
The crossing error bracket-pairs and spuriousbracket-pairs were evaluated by hand.
Thistook about three person-hours.In this process one step is missing, we namelywanted to remove trivial brackets before evaluat-ing.
In English there is a simple strategy for this:remove all brackets that enclose only one word.In Japanese this is not so easy.
Since Japanese isan agglutinating language and words are not sep-arated, it is difficult to say what the 'words' arein the first place.
We decided on a certain level topermit brackets, and the tree from the treebankalso stopped at some level so that remaining, moreprecise bracket-pairs were amongst hose countedas spurious.The resulting figures are in table 1 and table 2gives the comparative items.Table 1: Sample Test ResultsItem System A System BTTBTPBEMAEMRCEACERSPASPRP\[NHPNINHTINHTNINH1140086716748 (77.8%)O (assumed)204 (2.4%)690 (8.0%)956 (11.0%)73 (0.8%)5233715212153611400877168 8 (78.2%)0 (assumed)182 (2.1%)611 (7.0%)1049 (12.0%)71 (0.8%)47032354261432Table 2: Comparative Measure ResultsYY 6516 (57.2%)YN 232 (2.0%)NY 343 (3.1%)NN 4309 (37.8%)5645 New MeasuresWe claim that the items listed in the previousparagraph allows a nmre flexible framework forewduatim~.
In this paragraph we will show someexamt)les of measures that can be used.
They canbe calculated with these items so there is no needto discuss every one of them all the time.
Table 3gives the measures and table 4 gives the resultsin percentages.
~l?he measures in the lower partof this tabh> are more directed at the test than atthe parsers.
'\['able 3: MeasuresMeasnreGeneration RateRecall-hardRecall-softPrecision-hm-dPrecision-softSpuriousnessSpurious RejectFalse ErrorTest Noiset)roblem RateP-inheritance'l?-inheritanceCalculationEMA / T"I'B( EMA 4- CEA -I-SPA ) ~ TTBEMA / TPB( EMA-t CEA-I-SPA ) / TPB(.gPA q-SPR) / fI't'BSPR?
(SPA q-SPR)P INH/  ( PIN H + I'NINIt)Table 4: Results for MeasuresMeasure A BGeneration RateP~ecall-hardRecall- softI'recision- hardPrecision-softSpuriousnessSpurious RejectFalse Error'Pest NoiseProblem Ratel Lin heritanceT-inheritance59.2% !60.2%69.4% 71.0%77.8% , 78.2%91.2% !92.2%11.9% 12.8%22.8% 23.0%14.2% 114.8%3.2% ~ 2.9%58.5% 59.3?/077.2% 179.1%The generation rate shows that both systemsarc rather modest in producing brackets.We give two types of recall.
We suggest usingrecall-lmrd, but when the treebank does not indi-cate all brackets recall-soft may give an indicationof the proper recall.We also present wo types of precision.
B scoresbetter on precision-soft, but there is not much dif-ference for precision-hard.
This shows that B isbetter at teeM1 but also generates nmre spnriousbrackets.
The spuriousness also indicates this.The other measures tell us more about the testitself.
A would have been treated slightly fa-vorable without a human cheek, since relativelymore errors go 'undetected.'
False Error showsthat almost I out of 4 crossing errors is not re-ally wrong, which indicates there is much differ-ence in bracketing-style between the treebank andthe parsing system.
'rest Noise shows how manybracket-pairs were not tested properly.
ProblemRate shows the real 'myopia'  of the test.3'he inheritance data shows that in our test;crossing errors are often related (P-inheritance).Also, reproducing a particular bracket-pair fromthe treebank increases the chances on reproducingits parent (T-inheritance).6 S ign i f i canceThings would be easy if we could assmne that thechance of apl)lying a bracket is correctly modeledas a binomial experiment.
We begin by mention-ing two reasons why that is not possible.?
Errors that are related, sneh a~s one wrongattachment hat causes a number of cross-ing errors, as was shown in our test by P-inheritance.?
For a binomial process we mast assume thatthe chance on success is the same for everybracket pair.
It is not, in fact there are bothvery easy and very hard bracket pairs, withchances w~rying from very small to very high.The significance levels of all differences areworth knowing, but our main interest is the disference between A and B in recall and precision.Because of space limitations we only discuss astrategy for estimating the significance level of themeasure recall-hard.Significance for Recall-Hard First we willcheck whether the distribution can be modeledproperly with a binomial experiment.
We do thisby looking at the comparative items YY, YN, NYand NN.From these values the problem is intuitivelyclear: there are many easy bracket pairs that bothalways produce correctly, and many that both al-most never produce because they are too hard, orthe parsing systems simply never produce a cer-tain type of bracket pair.
Also, we have testedtwo rather similar parsing systems often giving thesame answer, after all that is often just what oneis interested in because one wants to measnre im-provement.
We will use statistical distributions toconfirm this problem occurs, and to find a solutionto the significance problem.We do not have tile space to go into tile detailsof the relations between the distributions, but if Aand B would behave like a binomial variable withtest size N, with Pa and P~ as respective chanceon success, the distribution of YY should againbe a binomial variable for test size N, with chancePry = PaPb.
The expected value and variance ofYY would be565E(YY)  = NPyy = NPaPbV(YY)  = NPw(1  - Pyy) = N(PaPb)(1 - P..Pb)For NN the distril~ution is the same with theopposite probabilities, a binomial variable for testsize Nand P,~, = (1 -  Pa) (1 -  Pb).
If we takePa = l - Pa and Pb = 1 -- Pb, the expected valueand variance of NN becomeE(NN)  = NP,~.
= Nf i~gV(NN)  -- NP~,~(1 - Pnn) = N(Pa-Pb)(1 - P.Pb)We will later put this to more use, but for nowwe just use it to conclude that YY  is expected tobe around 4063, and NN is expected to be around1851.
Using the variation we find that the ob-served values are both extremely rare, so we canreject the hypothesis that we are comparing twobinomial variables.Our strategy to solve this problem is assumingthere are three types of brackets, namely brack-ets that are ahnost always reproduced, those thatare almost never reproduced, and those that aresometimes reproduced and therefore constitutethe 'real test' between the two parsing systems.Note that the first two types do not tell us any-thing about the difference between the parsingsystems.
By assuming the rest is similar to a bi-nomial distribution, we can calculate the signif-icance.
Of conrse this assumption simplifies thesituation, but it is closer to the truth than assum-ing the whole test can be modeled by a binominaldistribution.
And, if this assumption is not jus-tified the whole test is not appropriate withouttesting on more specific phenomena.Guess ing the Real  Test Size The idea behindthis method is that some brackets are almost al-ways produced, and some are never, and thoseshould be discarded so the real test remains.
Ig-noring certain bracket pairs corresponds with thefact that some constituents relate to little andsome to much ambiguity, making some suitable forcomparison and others not.
We look at the num-ber of equal answers to estimate the number ofbracket-pairs that were not too easy or too hard.This is a theoretical operation, thus there is noneed to do this in practice.
We only need to es-timate two parameters: M1 being the number ofbracket-pairs that is discarded because they arealways reproduced, and M2 being the number ofbracket-pairs discarded because they are not re-produced.
We reduce YY  by M1, and NN by M2(the test size is thus reduced by M1 4- M2).
Thisindicates an imaginary real test, namely the partof the test that really served to compare the pars-ing systems.We calculate these quantities by a~suming a bi-nomial distribution for the real test, and makingsure that the corrected values for YY  and NN be-come equal to their expected value.
Letobserved YY  in real test = E(YY)  in real test =real test size x Pa in real test x Pb in real testthen we getYY-M1 -- (YY  + YN - M1) (YY  + NY  - M1)TTB - M1We do not give the derivation, but when doingthe same for NN and combining the equations thefollowing relation between M 1 and M2 holds:M1 = NY?TTB+M2?YY-(NY++NN)(VY+NY~M2-NNThere are usually rnany values for M1 and M2that satisfy this condition.
In practice M1 andM2 have to be discrete values, so they often arenot satisfying the condition exactly, but are closeenough.It may seem logical to find the proper valuesfor M1 and M2 as a next step, in other wordsdeciding how many brackets were 'too easy' andhow many were 'too hard.'
But our experienceis that there is no need to do that, because weare only interested in the significance level of thedifference between A and B, and the significancelevel is practically the same for all values of M1and M2 that satisfy the condition.As for our test, M1 and M2 can be, for exam-ple, 6234 and 4027 respectively.
Whatever valuewe take, the significance l vel o{" the difference be-tween A and B corresponds to being 4.7 standardvariations away from the expected value.
Thismeans that we can safely conclude that B re-ally performs better than A.
The real test is alot smMler, only 1139 bracket pairs, but that isstill enough to be meaningful.
(If the nmnber ofeqnM answers would be extremely high, the realtest size ruay become too small, indicating the testis meaningless.
)7 Conc lus ionWe have pointed out that the measures which arecurrently in use have a number of weaknesses.
Tolist the most important ones, a number of aspectsof parsing systems are not measured, treebankscontain arbitrary choices, some errors are not de-tected and discovering statistical significance isdifficult.The test items and measures we have suggestedgive a better picture of the specific behaviors ofparsing systems.
Although not solving the prob-lem of arbitrary choices in the treebank, we can atleast find out how much influence this has on thetest results by using a human check on importanttests.
The same goes for other problems, such aserrors that are not detected by comparison withthe treebank.566We suggest giving the regular items on everytest, and sometimes doing a hmnan check to dis-cover the quality of the regular test.
The amountof work in the hmnan check can be made smallwhen the recall of the parsing systems is high, byassuming the exact matches are correct.We have also given a strategy for calculatingthe significance level of differences in scores onone particular measure, namely recall-hard.
Thisstrategy makes it possible to calculate the signif-icance level right away from the test items, notrequiring a haman check.This discussion will certainly not be the last onthis subject.
We have not mentioned some quanti-ties such as the number of sentences with 1 cross-ing error, 2 crossing errors, and those with manycrossing errors.
These are of course also usefultools for evaluation.
We have also not mentionedthe Parse Base (calculated as the geometric meanover all sentences of ~ ,  where n is the number ofwords in the sentence and p the number of parsestbr the sentence), because that relates to a gram-mar rather than a parsing system.
Neverthelesswe feel this will help to improve the evaluation ofbroad coverage parsing systems.Language Processing Systems, Association forComputational Linguistics.Japan Electronic Dictionary Research Institute,Ltd.
1995.
EDR Electronic Dictionary 7~chni-cal Guide.D.
Lin.
1995.
A dependency-based method forevaluating broad-coverage parsers.
In Proceed-ings of the 14th International Joint Conferenceon Artificial Intelligence, pages 1420 1425.D.
M. Magerman.
1995.
Statistical decision-treemodels for parsing, in Proceedings of the 33dAnnual Meeting of the Association for Compu-tational Linguistics, pages 276-283.F.
Pereira and Y. Schabes.
1992.
Inside Outsidereestimation from partially bracketed corpora.In Proceedings of the 30th Annual Meeting ofthe Association for Computational Linguistics,pages 128-\]35.Y.
Sehabes, M. Roth, and R. Osborne.
1993.Parsing the wall street journal with the inside-outside algorithm.
In Proceedings of the SixthConference of the European Chapter of the As-sociation for Computational Linguistics, pages34\]-347.ReferencesE.
Black, S. Abney, D. Flickenger, C. Gdaniec,R.
Grishman, P. Harrison, D. Hindle, R. Ingria,F.
Jelinek, J. Klavans, M. I,iberman, M. Mar-cus, S. t{oukos, B. Santorini, and T. Strza-lkowski.
1991.
A procedure for quantitativelycomparing the syntactic coverage of Englishgrammars.
In Proceedings of the Workshopon Spcech.
and Natural Language, Defense Ad-vanced Research Projects Agency, U.S. Govt.,pages 306-311.E.
Black, R. Garside, and G. Leech.
1993.
Statis-tically Driven Computer Grammars of English:The IBM/Lancaster Approach.
Rodopi.E.
Black, F. Jelinek, J. Lafferty, and D. M. Mager-man.
1993.
Towards history-based grammars:Using richer models for probabilistic parsing.In Proceedings of the 31st Annual Meeting ofthe Association for Computational Linguistics,pages 31--37.R.
Bod.
1993.
Using an annotated corpus as astochastic grammar.
In Proceedings of the SixthConference of the European Chapter of the As-sociation for Computational Linguistics, pages37-44.P.
Harrison, S. Abney, E. Black, D. Flickenger,C.
Gdanicc, R. Grishman, D. Hindle, R. In-gria, M. Marcus, B. Santorini, and T. Strza-lkowski.
1991.
Evaluating syntax performanceof parser/grammars of English.
In Proceed-ings of the Workshop on Evaluating Natural567
