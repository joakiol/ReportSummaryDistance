Identification of Patients with Congestive Heart Failure using abinary classifier: a case study.Serguei V. PakhomovDivision of MedicalInformatics ResearchMayo Foundationpakhomov@mayo.eduJames BuntrockDivision of MedicalInformatics ResearchMayo Foundationbuntrock@mayo.eduChristopher G. ChuteDivision of MedicalInformatics ResearchMayo Foundationchute@mayo.eduAbstractThis paper addresses a very specificproblem that happens to be  common inhealth science research.
We present amachine learning based method foridentifying patients diagnosed withcongestive heart failure and other relatedconditions by automatically classifyingclinical notes.
This method relies on aPerceptron neural network classifiertrained on comparable amounts ofpositive and negative samples of clinicalnotes previously categorized by humanexperts.
The documents are represented asfeature vectors where features are a mixof single words and concept mappings toMeSH and HICDA ontologies.
Themethod is designed and implemented tosupport a particular epidemiological studybut has broader implications for clinicalresearch.
In this paper, we describe themethod and present experimentalclassification results based onclassification accuracy and positivepredictive value.1 IntroductionEpidemiological research frequently has to dealwith collecting a comprehensive set of humansubjects that are deemed relevant for a particularstudy.
For example, the research focused onpatients with congestive heart failure needs toidentify all possible candidates for the study so thatthe candidates could be asked to participate.
Oneof the requirements of a study like that is thecompleteness of the subject pool.
In many cases,such as disease incidence or prevalence studies, itis not acceptable for the investigator to miss any ofthe candidates.
The identification of the candidatesrelies on a large number of sources some of whichdo not exist in an electronic format, but it may startwith the clinical notes dictated by the treatingphysician.Another aspect of candidate identification isprospective patient recruitment.
Prospectiverecruitment is based on inclusion or exclusioncriteria and is of great interest to physicians forenabling just-in-time treatment, clinic trialenrollment, or research study options for patients.At Mayo Clinic most clinical documents aretranscribed within 24 hours of patient consultation.This electronic narration serves as resource forenabling prospective recruitment based on criteriapresent in clinical document.Probably the most basic approach toidentification of candidates for recruitment is todevelop a set of terms whose presence in the notemay be indicative of the diagnoses of interest.This term set can be used as a filtering mechanismby either searching on an indexed collection ofclinical notes or simply by doing term spotting ifthe size of the collection would allow it.
Forexample, in case of congestive heart failure, onecould define the following set of search terms:?CHF?, ?heart failure?, ?cardiomyopathy?,?volume overload?, ?fluid overload?, ?pulmonaryedema?, etc.
The number of possible variants isvirtually unlimited, which is the inherent problemwith this approach.
It would be hard to guaranteethe completeness of this set to begin with, which isfurther complicated by morphological and spellingvariants.
This problem is serious because it affectsthe recall, which is especially important inepidemiological studies.Another problem is that such term spotting orindexing approach would have to be intelligentenough to identify the search terms in negated andother contexts that would render documentscontaining these terms irrelevant.
A notecontaining ?no evidence of heart failure?
shouldnot be retrieved, for example.
Identifying negationreliably and, more importantly, its scope is farfrom trivial and is in fact a notoriously difficultproblem in Linguistics [1].
This problem is slightlyless serious than the completeness problem since itonly affects precision which is less important in thegiven context than recall.In order to be able to correctly identify whethera given patient note contains evidence that thepatient is relevant to a congestive heart failurestudy, one has to ?understand?
the note.
Currently,there are no systems capable of human-like?understanding?
of natural language; however,there are methods that allow at least partialsolutions to the language understanding problemonce the problem is constrained in very specificways.
One such constraint is to treat languageunderstanding as a classification problem and touse available machine learning approaches toautomatic classification to solve the problem.Clearly, this is a very limited view of languageunderstanding but we hypothesize that it issufficient for the purposes referred to in this paper.2 Previous workThe classification problems that have beeninvestigated in the past are just as varied as themachine learning algorithms that have been used tosolve these problems.
Linear Least Squares Fit [2],Support Vector Machines, Decision trees,Bayesean learning [3], symbolic rule induction [4],maximum entropy [5], expert networks [6] are justa few that have been applied to classifying e-mail,Web pages, newswire articles, medical reportsamong other documents.Aronow et al [7] have investigated a problemvery similar to the one described in this papers.They developed an ad hoc classifier based on avariation of relevance feedback technique formammogram reports where the reports wereclassified into three ?bins?
: relevant, irrelevant andunsure.
One of the features of the text processingsystem they used had to do with the ability todetect and take into account negated elements ofthe reports.Wilcox et al [8] have experimented with anumber of classification algorithms for identifyingclinical conditions such as congestive heart failure,chronic obstructive pulmonary disease, etc.
inraidograph reports.
They found that using an NLPsystem such as MedLEE (Medical LanguageExtraction and Encoding System) and domainknowledge sources such as UMLS?
[9] for featureextraction can significantly improve classificationaccuracy over the baseline where single words areused to represent training samples.Jain and Friedman [10] have demonstrated thefeasibility of using MedLEE for classifyingmammogram reports.
Unlike Wilcox  [8], thiswork does not use an automatic classifier, instead,it uses the NLP system to identify findings that areconsidered suspicious for breast cancer.3 NaiveBayes vs. PerceptronWe experimented with two widely usedmachine learning algorithms, Perceptron and Na?veBayes, in order to train models capable ofdistinguishing between clinical notes that containsufficient evidence of the patient having thediagnosis of congestive heart failure (positiveexamples) from notes that do not contain suchevidence (negative examples).
The choice of theproblem was dictated by a specific grant aimed atstudying patients with congestive heart failure.The choice of the algorithms was largelydictated by efficiency considerations.
BothPerceptron and Na?ve Bayes belong to a family oflinear classifiers which tend to be computationallymore manageable on large feature sets like the onewe are addressing than other algorithms.
Damerauet al [11] show on the Reuters corpus that sparsefeature implementations of linear algorithms arecapable of handling large feature sets.
We used asparse feature implementation of these twoalgorithms available in the SNoW (SparseNetworks of Winnows) Version 2.1.2 package[12].
Perceptron and Na?ve Bayes classifiers.Perceptron is a simple iterative learningalgorithm that represents in its simplest form atwo-layer (input/output) neural network whereeach node in the input layer is connected to eachnode in the output layer.
A detailed description canbe found in [13] and [14].
There are several wellknown limitations of this algorithm.
The mostsignificant is that the simple Perceptron is unableto learn non-linearly separable problems.
In orderfor this algorithm to work, one should be able todraw a hyperplane in the training data featurespace that will linearly separate positive examplesfrom negative.
With large multidimensional featurespaces, it is hard to know a priori whether thespace is linearly separable; however, a goodindication of that can be gleaned from theclassification accuracy testing on several folds oftraining/testing data.
If the accuracy results showlarge fluctuations between folds, then that wouldbe a good indication that the space is not linearlyseparable.
On the other hand if the standarddeviation on such a cross-validation task isrelatively small, then one could be reasonablycertain that Perceptron is a usable technique for theproblem.The other less serious limitation is that there isa chance that the algorithm will falsely concludeconvergence in a local minimum on the errorfunction curve without reaching the globalminimum, which could also account for low orinconsistent accuracy results.
This limitation is lessserious because it can be controlled to some extentwith the learning rate parameter, which sets theamount by which the weights are adjusted eachtime Perceptron makes a classification error duringtraining [14].Na?ve Bayes does not have the limitations ofPerceptron, but does have limitations of its own.The Bayes decision rule chooses the class thatmaximizes the conditional probability of the classgiven the context in which it occurs:(1) C` = argmax  )|()(1CVPCPnjj?=Here, C` is the chosen category, C is the set ofall categories and Vj is the context.
Na?ve Bayesdecision algorithm makes a simplifyingassumption that the words in Vj are independent ofeach other.
A particular implementation of theNa?ve Bayes decision rule based on theindependence assumption to text categorizationand word sense disambiguation problems is alsoknown as ?bag of words?
approach [13].
Thisapproach does not attempt to take into account anysort of possible dependency between the individualwords in any given context, in fact it assumes thatthe word ?heart?
and the word ?failure?, forexample, occur completely independently of eachother.
Theoretically, such assumption makes Na?veBayes classifiers very unappealing for textcategorization problems, but in practice it has beenshown to perform well on a much greater range ofdomains than the theory would support.The common feature between the twotechniques is that both are linear classifiers and arerelatively efficient which makes them attractive forlearning from large feature sets with lots oftraining samples.4 CHF pilot studyAs part of preliminary grant work to investigateand evaluate incidence, outcome, and etiologytrends of heart failure, a pilot study for prospectiverecruitment using term spotting techniques wastested.
Prospective recruitment was needed forrapid case identification with 24 hours of newlydiagnosed heart failure patients.Within Mayo Clinic approximately 75% ofclinical dictations are electronically transcribed onthe date of diagnosis allowing them to beprocessed using natural language techniques.Using the terms ?cardiomyopathy, heart failure,congestive heart failure, pulmonary edema,decompensated heart failure, volume overload, andfluid overload?
all electronic outpatient,emergency department, and hospital dismissalnotes were processed.
These results were reviewedby trained nurse abstractors to determine if thistechnique could provide identification of patientswith clinically active heart failure.
Using the termspotting technique no cases were omitted ascompared to standard human diagnostic codingmethods of final diagnosis.
This pilot provided avalid basis for using term spotting for prospectiverecruitment; however, the nurse abstractorsreported filtering out a large number of documentsthat were irrelevant to the query, thus indicatingthat there was room for improvement especially inprecision.
These were not quantified at the time.The results derived from the test sets used for thestudy described in this paper display similartendencies.5 Human Expert AgreementFor testing a classifier, it is important to have a testbed that contains positive as well as negativeexamples that have been annotated by humanexperts.
It is also important to establish  some sortof an agreement between annotators.
For this studywe used a test bed created with a specific focus onthe diagnosis regarding the patient describedwithin the medical document for a separate pilotstudy of agreement between annotators (de Groenet al, p. c.).One of the topics selected for this test bedcreation study included congestive heart failure.For each topic, 90 documents were selected forevaluation.
Seventy of the 90 documents werechosen from documents with a high likelihood ofcontaining diagnostic information regarding thetopic of inquiry.
Specifically, thirty-fivedocuments were randomly selected from a pool ofdocuments based on a coded final diagnosis; thirty-five documents were randomly selected from apool of documents based on a textual retrieval oflexical surface forms (term spotting).
The finaltwenty documents were randomly selected fromthe remaining documents, not originally includedin the coded or text identified collections.
A groupof Emeritus physicians acted as the human expertsfor this annotation task.
The experts wereinstructed to determine whether the informationcontained in the clinical note could supportinclusion of the patient in a clinical/researchinvestigation, if such investigation was centered onpatients having - at the time the note was created -the topic of inquiry.Each document was judged by three physicianson the following scale: (confirmed-probable-indeterminate-probably not-definitely not).
For thepurposes of our study we collapsed ?confirmed?and ?probable?
categories into one ?positive?category.
We also collapsed ?probably not?
and?definitely not?
into a ?negative?
category.
The?indeterminate?
category happened to include suchartifacts as differential diagnosis as well asuncertain judgements and therefore was ignoredfor our purposes.
The agreement on this particulartopic happened to be low: only 31% of theinstances were agreed upon by all three experts;therefore, we decided to use the agreed uponsubset of the notes only for testing our approach.The low level of agreement was partly attributableto the breadth of the topic and, partly, to how theinstructions were interpreted by the experts.Despite the low level of agreement, we were ableto select a subset of 26 documents where all threeannotators agreed.
These were the documentswhere all three annotators assigned either the?positive?
or the ?negative?
category.
7 documentswere judged as ?positive?
and 19 were judged  as?negative?
by all three experts.6 Feature extractionArguably, the most important part of training anytext document classifier is extracting relevantfeatures from the training data.
The resulting dataset looks like a set of feature vectors where eachvector should represent all the relevant informationencoded in the document and as little as possible ofthe irrelevant information.
To capture the relevantinformation and give it more weight, we used twoclassification schemes: MeSH (Medical SubjectsHeadings) [15]and HICDA (Hospital InternationalClassification of Diseases Adaptation) [16].
TheMeSH classification is available as part of theUMLS (Unified Medical Language System)compiled and distributed by the National Libraryof Medicine (NLM) [9].
HICDA is a hierarchicalclassification with 19 root nodes and 4,334 leafnodes.
Since 1975, it has been loosely expanded  tocomprise 35,676 rubrics or leaf nodes.
It is anadaptations of ICD-8, which is the 8th edition of theInternational Classification of Diseases.
HICDAcontains primarily diagnostic statements, whereasMeSH is not limited to diagnostic statements andtherefore the two complement each other.
It shouldalso be noted that, for mapping the text of clinicalnotes to these two ontologies, in addition to thetext phrases present in HICDA and MeSH, somelexical and syntactic variants found empirically inmedical texts were also added.
For MeSH, thesevariants were derived from MEDLINE articles byUMLS developers and for HICDA, the variantscame from coded diagnoses.
Having these lexicaland syntactic variants in conjunction with textlemmatization made the job of mapping relativelyeasy.
Text lemmatization was done using theLexical Variant Generator?s (lvg1) ?norm?
functionalso developed at NLM.For the purposes of this experiment, werepresented each document as a mixed set offeatures of the following types: (MeSH codemappings, HICDA code mapping, Single wordtokens, Demographic data).
First, MeSH andHICDA mappings were identified by stemmingand lowercasing all words in the notes and finding1 umlslex.nlm.nih.govtheir matches in the two ontologies.
Next, stopwords were deleted from the text that remainedunmapped.
The remaining words were treated assingle word token features.
In addition to theselexical features, we used a set of demographicfeatures such as age, gender, service code (the typeof specialty provider where the patient was seen (e.g.
?cardiology?))
and death indicator (whether thepatient was alive at the time the note was created).Since age is a continuous feature, we had todiscretize it by introducing ranges A-N arbitrarilydistributed across 5 year intervals from 0 to over70 years old.
For this experiment, features thatoccurred less than 2 times were ignored.
Theextracted feature ?vocabulary?
consists of 11,118unique features.
Table 1 shows the breakdown ofthe feature vocabulary by type.Feature type N features ProportionMeSH headings 6631 60 %HICDA categories 2721 24 %Single words 1635 15 %Demographic features 131 01 %Totals 11,118 100 %Table 1 Breakdown of training features by type.7 Experimental SetupBoth Na?ve Bayes and Perceptron were trained onthe same data and tested using a 10-fold cross-validation technique as well as a held-out test setof 26 notes mentioned in section 4.7.1 DataTwo types of annotated testing/training data wereused in this study.
The first type (Type I) is thedata generated by medical coders for the purposeof conceptual indexing of the clinical notes.
Thesecond type (Type II) is the data annotated byEmeritus physicians (experts).For Type I data, a set of clinical notes for 6months of year 2001 was collected resulting in acorpus of 1,117,284 notes.
Most of these notescontain a set of final diagnoses established by thephysician and coded using the HICDAclassification by specially trained staff.
The codingmakes it easy to extract a set of notes whose finaldiagnoses suggests that the patient has congestiveheart failure or a closely related condition orsymptom like pulmonary edema.
Once thispositive set was extracted (2945 notes), theremainder was randomized and a similar set ofnegative samples was extracted (4675 notes).
Thetotal size of the corpus is 7620 notes.
Each notewas then run through feature extraction and theresulting set was split into 10 train/test folds byrandomly selecting 20% of the 7620 notes to setaside for testing for each fold.Type II data set was split into two subsets: acomplete agreement (TypeII-CA) set and a partialagreement set (TypeII-PA).
The completeagreement set was created by taking 26 notes thatwere reliably categorized by the experts withrespect to congestive heart failure specifically.These 26 notes represent a set where all threeannotators agreed at least to a large extent on thecategorization.
?A large extent?
here means that allthree annotators labeled the positive samples aseither ?confirmed?
or ?probable?
and the negativesamples as either ?probably not?
or ?definitelynot?.
The set contains 7 positive and 19 negativesamples.
The partial agreement set was created bylabeling all samples for which at least one expertmade a positive judgement and no experts made a?negative?
judgement as ?positive?
and thenlabeling all samples for which at least one expertmade a negative judgement and no experts made apositive judgements as ?negative?.
This procedureresulted in reducing the initial set of 90 samples to74 of which 21 were positive and 53 were negativefor congestive heart failure.
This partial agreementset is obviously weaker in its reliability but it doesprovide substantially more data to test on andwould enable us to judge, at the very least, theconsistency of the automatic classifiers beingtested.7.2 TrainingThe following parameters were used for trainingthe classifiers.
Na?ve Bayes was used with thedefault smoothing parameter of 15.
For Perceptron,the most optimal combination of parameters was tohave the learning rate set at 0.0001 (very smallincrements in weights), the error threshold was setat 15.
The algorithm with these settings was run for1000 iterations.7.3 ResultsStandard classifier accuracy computation [13] forbinary classifiers was used.
(2) FNFPTNTPTNTPAcc++++?= 100Where TP represents the number of times theclassifier guessed a correct positive value (truepositives), TN is the number of times the classifiercorrectly guessed a negative value (true negatives),FP is the number of times the classifier predicted apositive value but the correct value was negative(false positives) and the FN (false negatives) is theinverse of FP.In addition to standard accuracy, positivepredictive value was also used.
It is defined as:(3) FPTPTPPPV+= *100Where TP+FP constitute all positive samples inthe test data set.
We are interested in positivepredictive value because of the strong preferencetowards perfect recall in document retrieval forepidemiological studies, even if it comes at theexpense of precision.
The rule is that it is better toidentify irrelevant data that can be discarded uponreview than to miss any of the relevant patients.First, we established a baseline by running a avery simple term spotter that looked for the CHF-related terms mentioned in Section 2 (and theirnormalized variants) in the collection ofnormalized2 documents from the Type II data set.The accuracy of the term spotter is 56% on TypeII-CA set and 54% on Type II-PA set.
Positivepredictive value on Type II-CA set is 85% and onType II-PA set ?
71%.
The positive predictivevalue on Type II-CA set reflects the spottermissing only 1 document out of 7 identified aspositive by the experts.
The results are summarizedin Tables 3 and 4.The results of testing the two classifiers arepresented in Table 2.
Na?ve Bayes algorithmachieves 82.2% accuracy, whereas Perceptron gets86.5%.
The standard deviation on the Perceptronclassifier results appears to be relatively small,which leads us to believe that this particularclassification problem is linearly separable.
Thedifference of 4.3% happens to be statisticallysignificant as evidenced by a t-test at 0.012 normalization was done with the lvg stemmer(umlslex.nlm.nih.gov)Fold Na?ve Bayes Perceptron DeltaPPV (%) Acc (%) PPV (%) Acc (%) PPV (%) Acc (%)1 89.21 84.06 78.42 88.39 -10.79 4.332 88.16 82.41 74.88 85.30 -13.28 2.893 89.34 82.74 75.74 86.09 -13.61 3.354 90.77 82.02 79.62 87.07 -11.15 5.055 90.54 82.07 76.51 86.54 -14.03 4.476 89.55 82.74 80.27 87.40 -9.29 4.667 88.16 82.41 74.88 85.30 -13.28 2.898 88.10 81.16 78.62 86.28 -9.48 5.129 89.26 81.69 79.36 86.68 -9.90 4.9910 88.12 80.45 76.59 85.89 -11.53 5.44Mean 89.12 82.18 77.49 86.49 -11.63 4.32Stdev 0.99 0.009 2.01 0.02Table 2.
Classification test results illustrating the differences between Perceptron and Na?ve Bayes.confidence level.
The difference in the positivepredictive value is also significant, however, is itinversely related to the difference in accuracy.Perceptron models perform on average 11 absolutepercentage points worse than Na?ve Bayes models.Table 1 shows results that represent theaccuracy of the classifiers on classifying the Type Itest data that has been generated by medicalcoders.
Clearly, Type I data is not generated inexactly the same way as Type II.
Although Type Idata is captured reliably and is highly accurate,Type II data is classified specifically with respectto congestive heart failure only, by expertphysicians and, we believe, reflects the nature ofthe task at hand a little better.In order to test the classifiers on Type II data,we re-trained them on the full set of 7620 notes ofType I data using the same parameters as wereused for the 10-fold cross-validation test.
Theresults of testing the classifiers on Type II-CA data(complete agreement) are presented in Table 3.Classifier PPV (%) Acc (%)NaiveBayes 100 69.2Perceptron 85 76.92TermSpotter 85 56Table 3.
Test results for Type II-CA data(annotated by retired physicians with completeagreement).These results are consistent with the onesdisplayed in Table 2 in that Perceptron tends to bemore accurate overall but less so in predictingpositive samples.
Table 4 summarizes the sameresults for Type II-PA test set and the resultsappear to be oriented in the same general directionas the ones reported in Table 2 and 3.Classifier PPV (%) Acc (%)NaiveBayes 95 57Perceptron 86 65TermSpotter 71 54Table 4.
Test results for Type II-PA data(annotated by retired physicians with partialagreement).From a practical standpoint, the resultspresented here are interesting in that they suggestthat the most accurate classifier may not be themost useful for a given task.
In our case, if wewere to use these classifiers for routing a stream ofelectronic clinical notes, the gains in precision thatwould be attained with the more accurate classifierwould most likely be wiped out by the losses inrecall since recall is more important for ourparticular task than precision.
However, for adifferent task that may be more focused onprecision, obviously, Perceptron would be a betterchoice.Finally, both Perceptron and Na?ve Bayesperformance appears to be superior to the baselineperformance of the term spotter.
Clearly suchcomparison is only an indicator because the termspotter is very simple.
It is possible that a moresophisticated term spotting algorithm may be ableto infer semantic relations between various termsand be able to compensate for misspellings andcarry out other functions resulting possibly inbetter performance.
However, even the mostsophisticated term spotter will only be as good asthe initial list of terms supplied to it.
Theadvantage of automatic classification lies in thefact that classifiers encode the terminologicalinformation implicitly which alleviates the need torely on managing lists of terms and the risk of suchlists being incomplete.
The disadvantage ofautomatic classification is that the classifier?sperformance is heavily data dependent, whichraises the need for sufficient amounts of annotatedtraining data and limits this methodology toenvironments where such data is available.The error analysis of the misclassified notesshows that a more intelligent feature selectionprocess that takes into account discoursecharacteristics and semantics of negation in theclinical notes is required.
For example, one of themisclassified notes contained ?no evidence ofCHF?
as part of the History of Present Illness(HPI) section.
Clearly, the presence of a particularconcept in a clinical note is not always relevant.For example, various terms and concepts mayappear in the Review of Systems (ROS) section ofthe note; however, the ROS section is often used asa preset template and may have little to do with thepresent condition.
Same is true for other sectionssuch as Family History, Surgical History, etc.
It isnot clear at this point which sections are to beincluded in the feature selection process.
Thechoice will most likely be task specific.The current study did not use any negationidentification, which we think accounted for someof the errors.
As one of the future steps, we areplanning to implement a negation detector such asthe NegExpander used by Aronow et al[7].8 ConclusionIn this paper, we have presented a methodology forgenerating on-demand binary classifiers forfiltering clinical patient notes with respect to aparticular condition of interest to a clinicalinvestigator.
Implementation of this approach isfeasible in environments where some quantity ofcoded clinical notes can be used as training data.We have experimented with HICDA codes;however, other coding schemes may be usable oreven more usable as well.We do not claim that either Na?ve Bayes or thePerceptron are the best possible classifiers thatcould be used for the task of identifying patientswith certain conditions.
All we show is that eitherone of these two classifiers is reasonably suitablefor the task and has the benefits of computationalefficiency and simplicity.
The results of theexperiments with the classifiers suggest thatalthough Perceptron has higher accuracy than theNa?ve Bayes classifier overall, its positivepredictive value is significantly lower.
The latterresult makes it less usable for a practical binaryclassification task focused on identifying patientrecords that have evidence of congestive heartfailure.
It may be worth while pursuing anapproach that would use the two classifiers intandem.
The classifier with the highest PPV wouldbe used to make the first cut to maximize recalland the more accurate classifier would be used torank the output for subsequent review.AcknowledgementsWe are thankful to the investigators working on the?Heart Failure?
grant RO1-HL-72435 who haveprovided valuable input and recommendations forthis research.References1.
Horn, L., A Natural History of Negation.
1989,Chicago: University Of Chicago Press.2.
Yang, Y. and C. Chute.
A linear least squares fitmapping method for information retrieval fromnatural language texts.
in 14th InternationalConference on Computational Linguistics(COLING).
1992.3.
Lewis, D. Naive (Bayes) at forty: Theindependence assumption in informationretrieval.
in ECML-98.
1998.4.
Johnson, D., et al, A deci-sion-tree-basedsymbolic rule induction system for textcategorization.
IBM Systems Journal, 2002.41(3).5.
Nigam, K., J. Lafferty, and A. McCullum.
UsingMaximum Entropy for Text Classification.
inIJCAI-99 Workshop on Machine Learning forInformation Filtering.
1999.6.
Yang, Y.
Expert Network: Combining Word-based Matching and Human Experiences inText Categorization and Retrieval.
in SIGIR.1994.7.
Aronow, D., F. Fangfang, and, and B. Croft, AdHoc Classification of Radiology Reports.Journal of Medical Informatics Association,1999.
6(5).8.
Wilcox, A., et al Using Knowledge Sources toImprove Classification of Medical TextReports.
in KDD-2000.
2000.9.
NLM, UMLS.
2001, National Library ofMedicine.10.
Jain, N. and C. Friedman.
Identification offinding suspiciois for breast cancer based onnatural language processing of mammogramreports.
in AMIA.
1997.11.
Damerau, F., et al Experiments in HighDimensional Text Categorization.
in ACMSIGIR International Conference on InformationRe-trieval.
2002.12.
Carlson, A.J., et al, SNoW User's Guide,Cognitive Computations Group - University ofIllinois at Urbana/Champaign.13.
Manning, C. and H. Shutze, Foundations ofStatistical Natural Language Processing.
1999,Cambridge, MA: MIT Press.14.
Anderson, J., Introduction to Neural Networks.1995, Boston: MIT Press.15.
NLM, Fact sheet Medical Subject Headings(MeSH?).
2000.16.
Commission on Professional and HospitalActivities, Hospital Adaptation of ICDA.
2nded.
Vol.
1.
1973, Ann Arbor, MI: Commissionon Professional and Hospital Activities.
