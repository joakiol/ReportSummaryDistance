Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 396?404,Suntec, Singapore, 2-7 August 2009. c?2009 ACL and AFNLPReducing semantic drift with bagging and distributional similarityTara McIntosh and James R. CurranSchool of Information TechnologiesUniversity of SydneyNSW 2006, Australia{tara,james}@it.usyd.edu.auAbstractIterative bootstrapping algorithms are typ-ically compared using a single set of hand-picked seeds.
However, we demonstratethat performance varies greatly depend-ing on these seeds, and favourable seedsfor one algorithm can perform very poorlywith others, making comparisons unreli-able.
We exploit this wide variation withbagging, sampling from automatically ex-tracted seeds to reduce semantic drift.However, semantic drift still occurs inlater iterations.
We propose an integrateddistributional similarity filter to identifyand censor potential semantic drifts, en-suring over 10% higher precision when ex-tracting large semantic lexicons.1 IntroductionIterative bootstrapping algorithms have been pro-posed to extract semantic lexicons for NLP taskswith limited linguistic resources.
Bootstrappingwas initially proposed by Riloff and Jones (1999),and has since been successfully applied to extract-ing general semantic lexicons (Riloff and Jones,1999; Thelen and Riloff, 2002), biomedical enti-ties (Yu and Agichtein, 2003), facts (Pas?ca et al,2006), and coreference data (Yang and Su, 2007).Bootstrapping approaches are attractive becausethey are domain and language independent, re-quire minimal linguistic pre-processing and can beapplied to raw text, and are efficient enough fortera-scale extraction (Pas?ca et al, 2006).Bootstrapping is minimally supervised, as it isinitialised with a small number of seed instancesof the information to extract.
For semantic lexi-cons, these seeds are terms from the category of in-terest.
The seeds identify contextual patterns thatexpress a particular semantic category, which inturn recognise new terms (Riloff and Jones, 1999).Unfortunately, semantic drift often occurs whenambiguous or erroneous terms and/or patterns areintroduced into and then dominate the iterativeprocess (Curran et al, 2007).Bootstrapping algorithms are typically com-pared using only a single set of hand-picked seeds.We first show that different seeds cause these al-gorithms to generate diverse lexicons which varygreatly in precision.
This makes evaluation un-reliable ?
seeds which perform well on one algo-rithm can perform surprisingly poorly on another.In fact, random gold-standard seeds often outper-form seeds carefully chosen by domain experts.Our second contribution exploits this diversitywe have identified.
We present an unsupervisedbagging algorithm which samples from the ex-tracted lexicon rather than relying on existinggazetteers or hand-selected seeds.
Each sample isthen fed back as seeds to the bootstrapper and theresults combined using voting.
This both improvesthe precision of the lexicon and the robustness ofthe algorithms to the choice of initial seeds.Unfortunately, semantic drift still dominates inlater iterations, since erroneous extracted termsand/or patterns eventually shift the category?s di-rection.
Our third contribution focuses on detect-ing and censoring the terms introduced by seman-tic drift.
We integrate a distributional similarityfilter directly into WMEB (McIntosh and Curran,2008).
This filter judges whether a new term ismore similar to the earlier or most recently ex-tracted terms, a sign of potential semantic drift.We demonstrate these methods for extractingbiomedical semantic lexicons using two bootstrap-ping algorithms.
Our unsupervised bagging ap-proach outperforms carefully hand-picked seedsby ?
10% in later iterations.
Our distributionalsimilarity filter gives a similar performance im-provement.
This allows us to produce large lexi-cons accurately and efficiently for domain-specificlanguage processing.3962 BackgroundHearst (1992) exploited patterns for informationextraction, to acquire is-a relations using manuallydevised patterns like such Z as X and/or Y where Xand Y are hyponyms of Z. Riloff and Jones (1999)extended this with an automated bootstrapping al-gorithm, Multi-level Bootstrapping (MLB), whichiteratively extracts semantic lexicons from text.In MLB, bootstrapping alternates between twostages: pattern extraction and selection, and termextraction and selection.
MB is seeded with a smallset of user selected seed terms.
These seeds areused to identify contextual patterns they appear in,which in turn identify new lexicon entries.
Thisprocess is repeated with the new lexicon termsidentifying new patterns.
In each iteration, the top-n candidates are selected, based on a metric scor-ing their membership in the category and suitabil-ity for extracting additional terms and patterns.Bootstrapping eventually extracts polysemousterms and patterns which weakly constrain thesemantic class, causing the lexicon?s meaning toshift, called semantic drift by Curran et al (2007).For example, female firstnames may drift intoflowers when Iris and Rose are extracted.
Manyvariations on bootstrapping have been developedto reduce semantic drift.1One approach is to extract multiple semanticcategories simultaneously, where the individualbootstrapping instances compete with one anotherin an attempt to actively direct the categories awayfrom each other.
Multi-category algorithms out-perform MLB (Thelen and Riloff, 2002), and wefocus on these algorithms in our experiments.In BASILISK, MEB, and WMEB, each compet-ing category iterates simultaneously between theterm and pattern extraction and selection stages.These algorithms differ in how terms and patternsselected by multiple categories are handled, andtheir scoring metrics.
In BASILISK (Thelen andRiloff, 2002), candidate terms are ranked highly ifthey have strong evidence for a category and littleor no evidence for other categories.
This typicallyfavours less frequent terms, as they will match farfewer patterns and are thus more likely to belongto one category.
Patterns are selected similarly,however patterns may also be selected by differ-ent categories in later iterations.Curran et al (2007) introduced Mutual Exclu-1Komachi et al (2008) used graph-based algorithms toreduce semantic drift for Word Sense Disambiguation.sion Bootstrapping (MEB) which forces stricterboundaries between the competing categories thanBASILISK.
In MEB, the key assumptions are thatterms only belong to a category and that patternsonly extract terms of a single category.
Semanticdrift is reduced by eliminating patterns that collidewith multiple categories in an iteration and by ig-noring colliding candidate terms (for the currentiteration).
This excludes generic patterns that canoccur frequently with multiple categories, and re-duces the chance of assigning ambiguous terms totheir less dominant sense.2.1 Weighted MEBThe scoring of candidate terms and patterns inMEB is na??ve.
Candidates which 1) match the mostinput instances; and 2) have the potential to gen-erate the most new candidates, are preferred (Cur-ran et al, 2007).
This second criterion aims to in-crease recall.
However, the selected instances arehighly likely to introduce drift.Our Weighted MEB algorithm (McIntosh andCurran, 2008), extends MEB by incorporating termand pattern weighting, and a cumulative patternpool.
WMEB uses the ?2 statistic to identify pat-terns and terms that are strongly associated withthe growing lexicon terms and their patterns re-spectively.
The terms and patterns are then rankedfirst by the number of input instances they match(as in MEB), but then by their weighted score.In MEB and BASILISK2, the top-k patterns foreach iteration are used to extract new candidateterms.
As the lexicons grow, general patterns candrift into the top-k and as a result the earlier pre-cise patterns lose their extracting influence.
InWMEB, the pattern pool accumulates all top-k pat-terns from previous iterations, to ensure previouspatterns can contribute.2.2 Distributional SimilarityDistributional similarity has been used to ex-tract semantic lexicons (Grefenstette, 1994), basedon the distributional hypothesis that semanticallysimilar words appear in similar contexts (Harris,1954).
Words are represented by context vectors,and words are considered similar if their contextvectors are similar.Patterns and distributional methods have beencombined previously.
Pantel and Ravichandran2In BASILISK, k is increased by one in each iteration, toensure at least one new pattern is introduced.397TYPE (#) MEDLINETerms 1 347 002Contexts 4 090 4125-grams 72 796 760Unfiltered tokens 6 642 802 776Table 1: Filtered 5-gram dataset statistics.
(2004) used lexical-syntactic patterns to labelclusters of distributionally similar terms.
Mirkin etal.
(2006) used 11 patterns, and the distributionalsimilarity score of each pair of terms, to constructfeatures for lexical entailment.
Pas?ca et al (2006)used distributional similarity to find similar termsfor verifying the names in date-of-birth facts fortheir tera-scale bootstrapping system.2.3 Selecting seedsFor the majority of bootstrapping tasks, there islittle or no guidance on how to select seeds whichwill generate the most accurate lexicons.
Mostprevious works used seeds selected based on auser?s or domain expert?s intuition (Curran et al,2007), which may then have to meet a frequencycriterion (Riloff et al, 2003).Eisner and Karakos (2005) focus on this issueby considering an approach called strapping forword sense disambiguation.
In strapping, semi-supervised bootstrapping instances are used totrain a meta-classifier, which given a bootstrap-ping instance can predict the usefulness (fertility)of its seeds.
The most fertile seeds can then beused in place of hand-picked seeds.The design of a strapping algorithm is morecomplex than that of a supervised learner (Eisnerand Karakos, 2005), and it is unclear how wellstrapping will generalise to other bootstrappingtasks.
In our work, we build upon bootstrappingusing unsupervised approaches.3 Experimental setupIn our experiments we consider the task of extract-ing biomedical semantic lexicons from raw textusing BASILISK and WMEB.3.1 DataWe compared the performance of BASILISK andWMEB using 5-grams (t1, t2, t3, t4, t5) from rawMEDLINE abstracts3.
In our experiments, the can-didate terms are the middle tokens (t3), and thepatterns are a tuple of the surrounding tokens (t1,3The set contains all MEDLINE abstracts available up toOct 2007 (16 140 000 abstracts).CAT DESCRIPTIONANTI Antibodies: Immunoglobulin molecules that reactwith a specific antigen that induced its synthesisMAb IgG IgM rituximab infliximab (?1:0.89, ?2:1.0)CELL Cells: A morphological or functional form of a cellRBC HUVEC BAEC VSMC SMC (?1:0.91, ?2:1.0)CLNE Cell lines: A population of cells that are totally de-rived from a single common ancestor cellPC12 CHO HeLa Jurkat COS (?1:0.93, ?2: 1.0)DISE Diseases: A definite pathological process that affectshumans, animals and or plantsasthma hepatitis tuberculosis HIV malaria(?1:0.98, ?2:1.0)DRUG Drugs: A pharmaceutical preparationacetylcholine carbachol heparin penicillin tetracy-clin (?1:0.86, ?2:0.99)FUNC Molecular functions and processeskinase ligase acetyltransferase helicase binding(?1:0.87, ?2:0.99)MUTN Mutations: Gene and protein mutations, and mutantsLeiden C677T C282Y 35delG null (?1:0.89, ?2:1.0)PROT Proteins and genesp53 actin collagen albumin IL-6 (?1:0.99, ?2:1.0)SIGN Signs and symptoms of diseasesanemia hypertension hyperglycemia fever cough(?1:0.96, ?2:0.99)TUMR Tumors: Types of tumorslymphoma sarcoma melanoma neuroblastomaosteosarcoma (?1:0.89, ?2:0.95)Table 2: The MEDLINE semantic categories.t2, t4, t5).
Unlike Riloff and Jones (1999) andYangarber (2003), we do not use syntactic knowl-edge, as we aim to take a language independentapproach.The 5-grams were extracted from the MEDLINEabstracts following McIntosh and Curran (2008).The abstracts were tokenised and split into sen-tences using bio-specific NLP tools (Grover et al,2006).
The 5-grams were filtered to remove pat-terns appearing with less than 7 terms4.
The statis-tics of the resulting dataset are shown in Table 1.3.2 Semantic CategoriesThe semantic categories we extract from MED-LINE are shown in Table 2.
These are a subsetof the TREC Genomics 2007 entities (Hersh et al,2007).
Categories which are predominately multi-term entities, e.g.
Pathways and Toxicities, wereexcluded.5 Genes and Proteins were merged intoPROT as they have a high degree of metonymy,particularly out of context.
The Cell or Tissue Typecategory was split into two fine grained classes,CELL and CLNE (cell line).4This frequency was selected as it resulted in the largestnumber of patterns and terms loadable by BASILISK5Note that polysemous terms in these categories may becorrectly extracted by another category.
For example, allPathways also belong to FUNC.398The five hand-picked seeds used for each cat-egory are shown in italics in Table 2.
These werecarefully chosen based on the evaluators?
intuition,and are as unambiguous as possible with respect tothe other categories.We also utilised terms in stop categories whichare known to cause semantic drift in specificclasses.
These extra categories bound the lexi-cal space and reduce ambiguity (Yangarber, 2003;Curran et al, 2007).
We used four stop cate-gories introduced in McIntosh and Curran (2008):AMINO ACID, ANIMAL, BODY and ORGANISM.3.3 Lexicon evaluationThe evaluation involves manually inspecting eachextracted term and judging whether it was a mem-ber of the semantic class.
This manual evaluationis extremely time consuming and is necessary dueto the limited coverage of biomedical resources.To make later evaluations more efficient, all eval-uators?
decisions for each category are cached.Unfamiliar terms were checked using onlineresources including MEDLINE, Medical SubjectHeadings (MeSH), Wikipedia.
Each ambiguousterm was counted as correct if it was classified intoone of its correct categories, such as lymphomawhich is a TUMR and DISE.
If a term was un-ambiguously part of a multi-word term we consid-ered it correct.
Abbreviations, acronyms and typo-graphical variations were included.
We also con-sidered obvious spelling mistakes to be correct,such as nuetrophils instead of neutrophils (a typeof CELL).
Non-specific modifiers are marked asincorrect, for example, gastrointestinal may be in-correctly extracted for TUMR, as part of the entitygastrointestinal carcinoma.
However, the modi-fier may also be used for DISE (gastrointestinalinfection) and CELL.The terms were evaluated by two domain ex-perts.
Inter-annotator agreement was measuredon the top-100 terms extracted by BASILISK andWMEB with the hand-picked seeds for each cat-egory.
All disagreements were discussed, and thekappa scores, before (?1) and after (?2) the discus-sions, are shown in Table 2.
Each score is above0.8 which reflects an agreement strength of ?al-most perfect?
(Landis and Koch, 1977).For comparing the accuracy of the systems weevaluated the precision of samples of the lexiconsextracted for each category.
We report averageprecision over the 10 semantic categories on the1-200, 401-600 and 801-1000 term samples, andover the first 1000 terms.
In each algorithm, eachcategory is initialised with 5 seed terms, and thenumber of patterns, k, is set to 5.
In each itera-tion, 5 lexicon terms are extracted by each cate-gory.
Each algorithm is run for 200 iterations.4 Seed diversityThe first step in bootstrapping is to select a set ofseeds by hand.
These hand-picked seeds are typi-cally chosen by a domain expert who selects a rea-sonably unambiguous representative sample of thecategory with high coverage by introspection.To improve the seeds, the frequency of the po-tential seeds in the corpora is often considered, onthe assumption that highly frequent seeds are bet-ter (Thelen and Riloff, 2002).
Unfortunately, theseseeds may be too general and extract many non-specific patterns.
Another approach is to identifyseeds using hyponym patterns like, * is a [NAMEDENTITY] (Meij and Katrenko, 2007).This leads us to our first investigation of seedvariability and the methodology used to comparebootstrapping algorithms.
Typically algorithmsare compared using one set of hand-picked seedsfor each category (Pennacchiotti and Pantel, 2006;McIntosh and Curran, 2008).
This approach doesnot provide a fair comparison or any detailed anal-ysis of the algorithms under investigation.
Aswe shall see, it is possible that the seeds achievethe maximum precision for one algorithm and theminimum for another, and thus the single compar-ison is inappropriate.
Even evaluating on multiplecategories does not ensure the robustness of theevaluation.
Secondly, it provides no insight intothe sensitivity of an algorithm to different seeds.4.1 Analysis with random gold seedsOur initial analysis investigated the sensitivity andvariability of the lexicons generated using differ-ent seeds.
We instantiated each algorithm 10 timeswith different random gold seeds (Sgold) for eachcategory.
We randomly sample Sgold from twosets of correct terms extracted from the evalua-tion cache.
UNION: the correct terms extracted byBASILISK and WMEB; and UNIQUE: the correctterms uniquely identified by only one algorithm.The degree of ambiguity of each seed is unknownand term frequency is not considered during therandom selection.Firstly, we investigated the variability of the399506070809050  60  70  80  90  100BASILISK(precision)WMEB (precision)Hand-pickedAverageFigure 1: Performance relationship betweenWMEB and BASILISK on Sgold UNIONextracted lexicons using UNION.
Each extractedlexicon was compared with the other 9 lexiconsfor each category and the term overlap calcu-lated.
For the top 100 terms, BASILISK had anoverlap of 18% and WMEB 44%.
For the top500 terms, BASILISK had an overlap of 39% andWMEB 47%.
Clearly BASILISK is far more sensi-tive to the choice of seeds ?
this also makes thecache a lot less valuable for the manual evaluationof BASILISK.
These results match our annotators?intuition that BASILISK retrieved far more of theesoteric, rare and misspelt results.
The overlap be-tween algorithms was even worse: 6.3% for thetop 100 terms and 9.1% for the top 500 terms.The plot in Figure 1 shows the variation in pre-cision between WMEB and BASILISK with the 10seed sets from UNION.
Precision is measured onthe first 100 terms and averaged over the 10 cate-gories.
The Shand is marked with a square, as wellas each algorithms?
average precision with 1 stan-dard deviation (S.D.)
error bars.
The axes startat 50% precision.
Visually, the scatter is quiteobvious and the S.D.
quite large.
Note that onour Shand evaluation, BASILISK performed signif-icantly better than average.We applied a linear regression analysis to iden-tify any correlation between the algorithm?s per-formances.
The resulting regression line is shownin Figure 1.
The regression analysis identified nocorrelation between WMEB and BASILISK (R2 =0.13).
It is almost impossible to predict the per-formance of an algorithm with a given set of seedsfrom another?s performance, and thus compar-isons using only one seed set are unreliable.Table 3 summarises the results on Sgold, in-cluding the minimum and maximum averages overthe 10 categories.
At only 100 terms, lexiconSgold Shand Avg.
Min.
Max.
S.D.UNIONBASILISK 80.5 68.3 58.3 78.8 7.31WMEB 88.1 87.1 79.3 93.5 5.97UNIQUEBASILISK 80.5 67.1 56.7 83.5 9.75WMEB 88.1 91.6 82.4 95.4 3.71Table 3: Variation in precision with random goldseed setsvariations are already obvious.
As noted above,Shand on BASILISK performed better than average,whereas WMEB Sgold UNIQUE performed signifi-cantly better on average than Shand.
This clearlyindicates the difficulty of picking the best seedsfor an algorithm, and that comparing algorithmswith only one set has the potential to penalise analgorithm.
These results do show that WMEB issignificantly better than BASILISK.In the UNIQUE experiments, we hypothesizedthat each algorithm would perform well on itsown set, but BASILISK performs significantlyworse than WMEB, with a S.D.
greater than 9.7.BASILISK?s poor performance may be a direct re-sult of it preferring low frequency terms, which areunlikely to be good seeds.These experiments have identified previouslyunreported performance variations of these sys-tems and their sensitivity to different seeds.
Thestandard evaluation paradigm, using one set ofhand-picked seeds over a few categories, does notprovide a robust and informative basis for compar-ing bootstrapping algorithms.5 Supervised BaggingWhile the wide variation we reported in the pre-vious section is an impediment to reliable evalua-tion, it presents an opportunity to improve the per-formance of bootstrapping algorithms.
In the nextsection, we present a novel unsupervised baggingapproach to reducing semantic drift.
In this sec-tion, we consider the standard bagging approachintroduced by Breiman (1996).
Bagging was usedby Ng and Cardie (2003) to create committees ofclassifiers for labelling unseen data for retraining.Here, a bootstrapping algorithm is instantiatedn = 50 times with random seed sets selected fromthe UNION evaluation cache.
This generates n newlexicons L1, L2, .
.
.
, Ln for each category.
Thenext phase involves aggregating the predictions inL1?n to form the final lexicon for each category,using a weighted voting function.4001-200 401-600 801-1000 1-1000ShandBASILISK 76.3 67.8 58.3 66.7WMEB 90.3 82.3 62.0 78.6Sgold BAGBASILISK 84.2 80.2 58.2 78.2WMEB 95.1 79.7 65.0 78.6Table 4: Bagging with 50 gold seed setsOur weighting function is based on two relatedhypotheses of terms in highly accurate lexicons: 1)the more category lexicons in L1?n a term appearsin, the more likely the term is a member of thecategory; 2) terms ranked higher in lexicons aremore reliable category members.
Firstly, we rankthe aggregated terms by the number of lexiconsthey appear in, and to break ties, we take the termthat was extracted in the earliest iteration acrossthe lexicons.5.1 Supervised resultsTable 4 compares the average precisions of thelexicons for BASILISK and WMEB using just thehand-picked seeds (Shand) and 50 sample super-vised bagging (Sgold BAG).Bagging with samples from Sgold successfullyincreased the performance of both BASILISK andWMEB in the top 200 terms.
While the improve-ment continued for BASILISK in later sections, ithad a more variable effect for WMEB.
Overall,BASILISK gets the greater improvement in perfor-mance (a 12% gain), almost reaching the perfor-mance of WMEB across the top 1000 terms, whileWMEB?s performance is the same for both Shandand Sgold BAG.
We believe the greater variabilityin BASILISK meant it benefited from bagging withgold seeds.6 Unsupervised baggingA significant problem for supervised bagging ap-proaches is that they require a larger set of gold-standard seed terms to sample from ?
either anexisting gazetteer or a large hand-picked set.
Inour case, we used the evaluation cache which tookconsiderable time to accumulate.
This saddlesthe major application of bootstrapping, the quickconstruction of accurate semantic lexicons, with achicken-and-egg problem.However, we propose a novel solution ?
sam-pling from the terms extracted with the hand-picked seeds (Lhand).
WMEB already has veryhigh precision for the top extracted terms (88.1%BAGGING 1-200 401-600 801-1000 1-1000Top-100BASILISK 72.3 63.5 58.8 65.1WMEB 90.2 78.5 66.3 78.5Top-200BASILISK 70.7 60.7 45.5 59.8WMEB 91.0 78.4 62.2 77.0Top-500BASILISK 63.5 60.5 45.4 56.3WMEB 92.5 80.9 59.1 77.2PDF-500BASILISK 69.6 68.3 49.6 62.3WMEB 92.9 80.7 72.1 81.0Table 5: Bagging with 50 unsupervised seed setsfor the top 100 terms) and may provide an accept-able source of seed terms.
This approach nowonly requires the original 50 hand-picked seedterms across the 10 categories, rather than the2100 terms used above.
The process now uses tworounds of bootstrapping: first to create Lhand tosample from and then another round with the 50sets of randomly unsupervised seeds, Srand.The next decision is how to sample Srand fromLhand.
One approach is to use uniform randomsampling from restricted sections of Lhand.
Weperformed random sampling from the top 100,200 and 500 terms of Lhand.
The seeds from thesmaller samples will have higher precision, butless diversity.In a truly unsupervised approach, it is impossi-ble to know if and when semantic drift occurs andthus using arbitrary cut-offs can reduce the diver-sity of the selected seeds.
To increase diversity wealso sampled from the top n=500 using a proba-bility density function (PDF) using rejection sam-pling, where r is the rank of the term in Lhand:PDF(r) =?ni=r i?1?ni=1?nj=i j?1(1)6.1 Unsupervised resultsTable 5 shows the average precision of the lex-icons after bagging on the unsupervised seeds,sampled from the top 100 ?
500 terms from Lhand.Using the top 100 seed sample is much less effec-tive than Sgold BAG for BASILISK but nearly as ef-fective for WMEB.
As the sample size increases,WMEB steadily improves with the increasing vari-ability, however BASILISK is more effective whenthe more precise seeds are sampled from higherranking terms in the lexicons.Sampling with PDF-500 results in more accuratelexicons over the first 1000 terms than the other40100.511.522.530  100  200  300  400  500  600  700  800  900  1000DriftNumber of termsCorrectIncorrectFigure 2: Semantic drift in CELL (n=20, m=20)sampling methods for WMEB.
In particular, WMEBis more accurate with the unsupervised seeds thanthe Sgold and Shand (81.0% vs 78.6% and 78.6%).WMEB benefits from the larger variability intro-duced by the more diverse sets of seeds, and thegreater variability available out-weighs the poten-tial noise from incorrect seeds.
The PDF-500 dis-tribution allows some variability whilst still prefer-ring the most reliable unsupervised seeds.
In thecritical later iterations, WMEB PDF-500 improvesover supervised bagging (Sgold BAG) by 7% andthe original hand-picked seeds (Shand) by 10%.7 Detecting semantic driftAs shown above, semantic drift still dominates thelater iterations of bootstrapping even after bag-ging.
In this section, we propose distributionalsimilarity measurements over the extracted lexi-con to detect semantic drift during the bootstrap-ping process.
Our hypothesis is that semantic drifthas occurred when a candidate term is more sim-ilar to recently added terms than to the seed andhigh precision terms added in the earlier iterations.We experiment with a range of values of both.Given a growing lexicon of size N , LN , letL1...n correspond to the first n terms extracted intoL, and L(N?m)...N correspond to the last m termsadded to LN .
In an iteration, let t be the next can-didate term to be added to the lexicon.We calculate the average distributional similar-ity (sim) of t with all terms in L1...n and those inL(N?m)...N and call the ratio the drift for term t:drift(t, n,m) =sim(L1...n, t)sim(L(N?m)...N , t)(2)Smaller values of drift(t, n,m) correspond tothe current term moving further away from thefirst terms.
A drift(t, n,m) of 0.2 correspondsto a 20% difference in average similarity betweenL1...n and L(N?m)...N for term t.Drift can be used as a post-processing step to fil-ter terms that are a possible consequence of drift.However, our main proposal is to incorporate thedrift measure directly within the WMEB bootstrap-ping algorithm, to detect and then prevent drift oc-curing.
In each iteration, the set of candidate termsto be added to the lexicon are scored and rankedfor their suitability.
We now additionally deter-mine the drift of each candidate term before it isadded to the lexicon.
If the term?s drift is below aspecified threshold, it is discarded from the extrac-tion process.
If the term has zero similarity withthe last m terms, but is similar to at least one ofthe first n terms, the term is selected.
Preventingthe drifted term from entering the lexicon duringthe bootstrapping process, has a flow on effect asit will not be able to extract additional divergentpatterns which would lead to accelerated drift.For calculating drift we use the distributionalsimilarity approach described in Curran (2004).We extracted window-based features from thefiltered 5-grams to form context vectors foreach term.
We used the standard t-test weightand weighted Jaccard measure functions (Curran,2004).
This system produces a distributional scorefor each pair of terms presented by the bootstrap-ping system.7.1 Drift detection resultsTo evaluate our semantic drift detection we incor-porate our process in WMEB.
Candidate terms arestill weighted in WMEB using the ?2 statistic as de-scribed in (McIntosh and Curran, 2008).
Many ofthe MEDLINE categories suffer from semantic driftin WMEB in the later stages.
Figure 2 shows thedistribution of correct and incorrect terms appear-ing in the CELL lexicon extracted using Shand withthe term?s ranks plotted against their drift scores.Firstly, it is evident that incorrect terms begin todominate in later iterations.
Encouragingly, thereis a trend where low values of drift correspond toincorrect terms being added.
Drift also occurs inANTI and MUTN, with an average precision at 801-1000 terms of 41.5% and 33.0% respectively.We utilise drift in two ways with WMEB;as a post-processing filter (WMEB+POST) andinternally during the term selection phase(WMEB+DIST).
Table 6 shows the performance4021-200 401-600 801-1000 1000WMEB 90.3 82.3 62.0 78.6WMEB+POSTn:20 m:5 90.3 82.3 62.1 78.6n:20 m:20 90.3 81.5 62.0 76.9n:100 m:5 90.2 82.3 62.1 78.6n:100 m:20 90.3 82.1 62.1 78.1WMEB+DISTn:20 m:5 90.8 79.7 72.1 80.2n:20 m:20 90.6 80.1 76.3 81.4n:100 m:5 90.5 82.0 79.3 82.8n:100 m:20 90.5 81.5 77.5 81.9Table 6: Semantic drift detection resultsof drift detection with WMEB, using Shand.
Weuse a drift threshold of 0.2 which was selectedempirically.
A higher value substantially reducedthe lexicons?
size, while a lower value resultedin little improvements.
We experimented withvarious sizes of initial terms L1...n (n=20, n=100)and L(N?m)...N (m=5, m=20).There is little performance variation observedin the various WMEB+POST experiments.
Over-all, WMEB+POST was outperformed slightly byWMEB.
The post-filtering removed many incor-rect terms, but did not address the underlying driftproblem.
This only allowed additional incorrectterms to enter the top 1000, resulting in no appre-ciable difference.Slight variations in precision are obtained usingWMEB+DIST in the first 600 terms, but noticeablegains are achieved in the 801-1000 range.
This isnot surprising as drift in many categories does notstart until later (cf.
Figure 2).With respect to the drift parameters n and m, wefound values of n below 20 to be inadequate.
Weexperimented initially with n=5 terms, but this isequivalent to comparing the new candidate termsto the initial seeds.
Setting m to 5 was also lessuseful than a larger sample, unless n was alsolarge.
The best performance gain of 4.2% over-all for 1000 terms and 17.3% at 801-1000 termswas obtained using n=100 and m=5.
In differentphases of WMEB+DIST we reduce semantic driftsignificantly.
In particular, at 801-1000, ANTI in-crease by 46% to 87.5% and MUTN by 59% to92.0%.For our final experiments, we report the perfor-mance of our best performing WMEB+DIST sys-tem (n=100 m=5) using the 10 random GOLD seedsets from section 4.1, in Table 7.
On averageWMEB+DIST performs above WMEB, especially inthe later iterations where the difference is 6.3%.Shand Avg.
Min.
Max.
S.D.1-200WMEB 90.3 82.2 73.3 91.5 6.43WMEB+DIST 90.7 84.8 78.0 91.0 4.61401-600WMEB 82.3 66.8 61.4 74.5 4.67WMEB+DIST 82.0 73.1 65.2 79.3 4.52Table 7: Final accuracy with drift detection8 ConclusionIn this paper, we have proposed unsupervisedbagging and integrated distributional similarity tominimise the problem of semantic drift in itera-tive bootstrapping algorithms, particularly whenextracting large semantic lexicons.There are a number of avenues that require fur-ther examination.
Firstly, we would like to takeour two-round unsupervised bagging further byperforming another iteration of sampling and thenbootstrapping, to see if we can get a further im-provement.
Secondly, we also intend to experi-ment with machine learning methods for identify-ing the correct cutoff for the drift score.
Finally,we intend to combine the bagging and distribu-tional approaches to further improve the lexicons.Our initial analysis demonstrated that the outputand accuracy of bootstrapping systems can be verysensitive to the choice of seed terms and thereforerobust evaluation requires results averaged acrossrandomised seed sets.
We exploited this variabilityto create both supervised and unsupervised bag-ging algorithms.
The latter requires no more seedsthan the original algorithm but performs signifi-cantly better and more reliably in later iterations.Finally, we incorporated distributional similaritymeasurements directly into WMEB which detectand censor terms which could lead to semanticdrift.
This approach significantly outperformedstandard WMEB, with a 17.3% improvement overthe last 200 terms extracted (801-1000).
The resultis an efficient, reliable and accurate system for ex-tracting large-scale semantic lexicons.AcknowledgmentsWe would like to thank Dr Cassie Thornley, oursecond evaluator who also helped with the eval-uation guidelines; and the anonymous reviewersfor their helpful feedback.
This work was sup-ported by the CSIRO ICT Centre and the Aus-tralian Research Council under Discovery projectDP0665973.403ReferencesLeo Breiman.
1996.
Bagging predictors.
Machine Learning,26(2):123?140.James R. Curran, Tara Murphy, and Bernhard Scholz.
2007.Minimising semantic drift with mutual exclusion boot-strapping.
In Proceedings of the 10th Conference of thePacific Association for Computational Linguistics, pages172?180, Melbourne, Australia.James R. Curran.
2004.
From Distributional to SemanticSimilarity.
Ph.D. thesis, University of Edinburgh.Jason Eisner and Damianos Karakos.
2005.
Bootstrappingwithout the boot.
In Proceedings of the Conference onHuman Language Technology and Conference on Empiri-cal Methods in Natural Language Processing, pages 395?402, Vancouver, British Columbia, Canada.Gregory Grefenstette.
1994.
Explorations in Automatic The-saurus Discovery.
Kluwer Academic Publishers, USA.Claire Grover, Michael Matthews, and Richard Tobin.
2006.Tools to address the interdependence between tokeni-sation and standoff annotation.
In Proceedings of theMulti-dimensional Markup in Natural Language Process-ing Workshop, Trento, Italy.Zellig Harris.
1954.
Distributional structure.
Word,10(2/3):146?162.Marti A. Hearst.
1992.
Automatic acquisition of hyponymsfrom large text corpora.
In Proceedings of the 14th Inter-national Conference on Computational Linguistics, pages539?545, Nantes, France.William Hersh, Aaron M. Cohen, Lynn Ruslen, andPhoebe M. Roberts.
2007.
TREC 2007 Genomics TrackOverview.
In Proceedings of the 16th Text REtrieval Con-ference, Gaithersburg, MD, USA.Mamoru Komachi, Taku Kudo, Masashi Shimbo, and YujiMatsumoto.
2008.
Graph-based analysis of semantic driftin Espresso-like bootstrapping algorithms.
In Proceedingsof the Conference on Empirical Methods in Natural Lan-guage Processing, pages 1011?1020, Honolulu, USA.J.
Richard Landis and Gary G. Koch.
1977.
The measure-ment of observer agreement in categorical data.
Biomet-rics, 33(1):159?174.Tara McIntosh and James R. Curran.
2008.
Weighted mu-tual exclusion bootstrapping for domain independent lex-icon and template acquisition.
In Proceedings of the Aus-tralasian Language Technology Association Workshop,pages 97?105, Hobart, Australia.Edgar Meij and Sophia Katrenko.
2007.
Bootstrapping lan-guage associated with biomedical entities.
The AID groupat TREC Genomics 2007.
In Proceedings of The 16th TextREtrieval Conference, Gaithersburg, MD, USA.Shachar Mirkin, Ido Dagan, and Maayan Geffet.
2006.
In-tegrating pattern-based and distributional similarity meth-ods for lexical entailment acquistion.
In Proceedings ofthe 21st International Conference on Computational Lin-guisitics and the 44th Annual Meeting of the Associationfor Computational Linguistics, pages 579?586, Sydney,Australia.Vincent Ng and Claire Cardie.
2003.
Weakly supervisednatural language learning without redundant views.
InProceedings of the Human Language Technology Confer-ence of the North American Chapter of the Associationfor Computational Linguistics, pages 94?101, Edmonton,USA.Marius Pas?ca, Dekang Lin, Jeffrey Bigham, Andrei Lifchits,and Alpa Jain.
2006.
Names and similarities on the web:Fact extraction in the fast lane.
In Proceedings of the 21stInternational Conference on Computational Linguisiticsand the 44th Annual Meeting of the Association for Com-putational Linguistics, pages 809?816, Sydney, Australia.Patrick Pantel and Deepak Ravichandran.
2004.
Automati-cally labelling semantic classes.
In Proceedings of the Hu-man Language Technology Conference of the North Amer-ican Chapter of the Association for Computational Lin-guistics, pages 321?328, Boston, MA, USA.Marco Pennacchiotti and Patrick Pantel.
2006.
A bootstrap-ping algorithm for automatically harvesting semantic re-lations.
In Proceedings of Inference in Computational Se-mantics (ICoS-06), pages 87?96, Buxton, England.Ellen Riloff and Rosie Jones.
1999.
Learning dictionariesfor information extraction by multi-level bootstrapping.
InProceedings of the 16th National Conference on ArtificialIntelligence and the 11th Innovative Applications of Ar-tificial Intelligence Conference, pages 474?479, Orlando,FL, USA.Ellen Riloff, Janyce Wiebe, and Theresa Wilson.
2003.Learning subjective nouns using extraction pattern boot-strapping.
In Proceedings of the Seventh Conference onNatural Language Learning (CoNLL-2003), pages 25?32.Michael Thelen and Ellen Riloff.
2002.
A bootstrappingmethod for learning semantic lexicons using extractionpattern contexts.
In Proceedings of the Conference on Em-pirical Methods in Natural Language Processing, pages214?221, Philadelphia, USA.Xiaofeng Yang and Jian Su.
2007.
Coreference resolu-tion using semantic relatedness information from automat-ically discovered patterns.
In Proceedings of the 45th An-nual Meeting of the Association for Computational Lin-guistics, pages 528?535, Prague, Czech Republic.Roman Yangarber.
2003.
Counter-training in discovery ofsemantic patterns.
In Proceedings of the 41st AnnualMeeting of the Association for Computational Linguistics,pages 343?350, Sapporo, Japan.Hong Yu and Eugene Agichtein.
2003.
Extracting synony-mous gene and protein terms from biological literature.Bioinformatics, 19(1):i340?i349.404
