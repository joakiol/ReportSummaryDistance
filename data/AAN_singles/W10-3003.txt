Proceedings of the Fourteenth Conference on Computational Natural Language Learning: Shared Task, pages 18?25,Uppsala, Sweden, 15-16 July 2010. c?2010 Association for Computational LinguisticsDetecting Speculative Language Using Syntactic Dependenciesand Logistic RegressionAndreas Vlachos and Mark CravenDepartment of Biostatistics and Medical InformaticsUniversity of Wisconsin-Madison{vlachos,craven}@biostat.wisc.eduAbstractIn this paper we describe our approachto the CoNLL-2010 shared task on de-tecting speculative language in biomedicaltext.
We treat the detection of sentencescontaining uncertain information (Task1)as a token classification task since theexistence or absence of cues determinesthe sentence label.
We distinguish wordsthat have speculative and non-speculativemeaning by employing syntactic featuresas a proxy for their semantic content.
Inorder to identify the scope of each cue(Task2), we learn a classifier that predictswhether each token of a sentence belongsto the scope of a given cue.
The featuresin the classifier are based on the syntacticdependency path between the cue and thetoken.
In both tasks, we use a Bayesianlogistic regression classifier incorporatinga sparsity-enforcing Laplace prior.
Over-all, the performance achieved is 85.21%F-score and 44.11% F-score in Task1 andTask2, respectively.1 IntroductionThe term speculative language, also known ashedging, refers to expressions of uncertainty overstatements.
Recognition of such statements is im-portant for higher-level applications.
For exam-ple, a multi-document summarization system canassign different weights to speculative and non-speculative statements when aggregating informa-tion on a particular issue.The CoNLL-2010 shared task (Farkas et al,2010) formulates speculative language detectionas two subtasks.
In the first subtask (Task1), sys-tems need to determine whether a sentence con-tains uncertain information or not.
In the sec-ond subtask (Task2), systems need to identify thehedge cues and their scope in the sentence.
Table 1provides an example from the training data.The participants are provided with data fromtwo domains: biomedical scientific literature (bothabstracts and full articles) and Wikipedia.
Wechoose to focus on the former.
The training datafor this domain are nine full articles and 1,273 ab-stracts from the BioScope corpus (Szarvas et al,2008) and the test data are 15 full articles.Our approach to speculative language detectionrelies on syntactic parsing and machine learning.We give a description of the techniques used inSections 2 and 3.
We treat the detection of sen-tences containing uncertain information (Task1) asa token classification task in which we learn a clas-sifier to predict whether a token is a cue or not.
Inorder to handle words that have speculative andnon-speculative meaning (e.g.
?indicating?
in theexample of Table 1), we employ syntactic featuresas a proxy for their semantic content (Section 4).For scope identification (Task2), we learn a clas-sifier that predicts whether each token of the sen-tence belongs to the scope of a particular cue (Sec-tion 6).
The features used are based on the syntac-tic dependency path between the cue and the to-ken.
We report results and perform error analysisfor both tasks, pointing out annotation issues thatcould be ameliorated (Sections 5 and 7).
Based onour experience we suggest improvements on thetask definition taking into account work from thebroader field (Section 8).2 Syntactic parsing for the biomedicaldomainThe syntactic parser we chose for our experi-ments is the C&C Combinatory Categorial Gram-mar (CCG) parser adapted to the biomedical do-main (Rimell and Clark, 2009).
In this frame-work, parsing is performed in three stages: part-of-speech (PoS) tagging, CCG supertagging andparse selection.
The parse selection module de-18The Orthology and Combined modules both have states that achieve likelihood ratios above 400 (ashigh as 1207 for the Orthology module and 613 for the Combined module), {indicating that both thesemodules {can, on their own, predict some interacting protein pairs with a posterior odds ratio above 1}}.Table 1: Sentence annotated as speculative with two cues (in boldface) and their scopes (in brackets).rives the actual parse tree using the informationfrom the other two components.
The intermediateCCG supertagging stage assigns each token to alexical category which attempts to capture its syn-tactic role in the sentence.
Lexical categories con-tain more information than PoS tags (mainly onsubcategorization) and they are more numerous,thereby making their assignment a relatively dif-ficult task.
Therefore, the parse selection moduletakes into account multiple predictions per tokenwhich allows recovery from supertagging errorswhile still reducing the ambiguity propagated.
Aninteresting aspect of this three-stage parsing ap-proach is that, if the parse selection module fails toconstruct a parse tree for the sentence (a commonissue when syntactic parsers are ported to new do-mains), the lexical categories obtained by the su-pertagger preserve some of the syntactic informa-tion that would not be found in PoS tags.The adaptation to the biomedical domain byRimell and Clark (2009) involved re-training thePoS tagger and the CCG supertagger using in-domain resources, while the parse selection com-ponent was left intact.
As recent work in theBioNLP 2009 shared task has shown (Kim et al,2009), domain-adapted parsing benefits informa-tion extraction systems.The native output of the C&C parser is con-verted into the Stanford Dependency (SD) col-lapsed dependency format (de Marneffe and Man-ning, 2008).
These dependencies define binary re-lations between tokens and the labels of these re-lations are obtained from a hierarchy.
While theconversion is unlikely to be perfect given that thenative C&C output follows a different formalism,we made this choice because it allows for the useof different parsers with minimal adaptation.Finally, an important pre-processing step wetake is tokenization of the original text.
Since thePoS tagger is trained on the GENIA corpus whichfollows the Penn TreeBank tokenization scheme,we use the tokenization script provided by the tree-bank.11http://www.cis.upenn.edu/?treebank/tokenization.html3 Bayesian logistic regressionIn both tasks, we use a Bayesian logistic regres-sion classifier incorporating a sparsity-enforcingLaplace prior (Genkin et al, 2006).
Logistic re-gression models are of the form:p(y = +1|?, x) = exp(x?T )1 + exp(x?T ) (1)where y ?
{+1,?1} is a binary class label, xis the feature vector representation of the instanceto be classified and ?
is the feature weight vec-tor which is learnt from the training data.
Sincefeature interactions are not directly represented,the interactions that are expected to matter for thetask considered must be specified as additionalfeatures.
In Bayesian logistic regression, a priordistribution on ?
is used which encodes our priorbeliefs on the feature weights.
In this work, weuse the Laplace prior which encourages the fea-ture weight vector to be sparse, reflecting our be-lief that most features will be irrelevant to the task.4 Detecting sentences containingspeculationIn Task1, systems need to determine whether asentence contains uncertain information (labeleduncertain) or not (labeled certain).
A sentence isuncertain if one or more of its tokens denote un-certainty.
Such tokens are labeled as cues and theyare provided by the organizers for training.
If acue is a present, any other (potentially ?unhedg-ing?)
token becomes irrelevant to the task.
There-fore, we cast the task as a binary token classifi-cation problem and determine the sentence labelfrom the token-level decisions.Words used as speculative cues do not alwaysdenote speculation.
For example, in BioScope ?if?and ?appear?
are annotated as cues 4% and 83%of the times they are encountered.
In order togain better understanding of the task, we build adictionary-based cue extractor.
First we extract allthe cues from the training data and use their lem-mas, obtained using morpha (Minnen et al, 2001),to tag tokens in the test data.
We keep only single-token cues in order to avoid non-indicative lem-19token=indicating lemma=indicatePoS=VBG lemma+PoS=indicate+VBGCCG=(S[ng]\NP)/S[em]lemma+CCG=indicate+(S[ng]\NP)/S[em]Table 2: Features extracted for the token ?indicat-ing?
from the Example in Table 1.
CCG supertag(S[ng]\NP)/S[em] denotes that ?indicating?
ex-pects an embedded clause (S[em]) to its right (in-dicated by the forward slash /) and a noun phrase(NP) to its left (indicated by the backward slash \)to form a present participle (S[ng]).mas entering the dictionary (e.g.
?that?
in ?in-dicate that?).
Since the test data consist of fullarticles only, we evaluate the performance of thedictionary-based approach using four-fold cross-validation on the nine full articles of the trainingdata with the abstracts added as training data inevery fold, but not used as test data.
The recallachieved is 98.07%, but F-score is lower (59.53%)demonstrating that the single-token cues in thetraining data provide adequate coverage, but lowprecision.
The restricted domain helps precisionas it precludes some word meanings from appear-ing.
For example ?might?
is unlikely to be encoun-tered as a noun in the biomedical domain.
Never-theless, in order to achieve better performance itis important to further refine the cue identificationprocedure.Determining whether a token is used as a specu-lative cue or not resembles supervised word sensedisambiguation.
The main difference is that in-stead of having an inventory of senses for eachword, we have two senses applicable to all words.As in most word sense disambiguation tasks, theclassification of a word as cue or not is dependenton the other words in the sentence, which we takeinto account using syntax.
The syntactic contextof words is a useful proxy to their semantics, asshown in recent work on verb sense disambigua-tion (Chen and Palmer, 2009).
Furthermore, it iseasy to obtain syntactic information automaticallyusing a parser, even though there will be somenoise due to parsing errors.
Similar intuitions wereexploited by Kilicoglu and Bergler (2008) in refin-ing a dictionary of cues with syntactic rules.In what follows, we present the features ex-tracted for each token for our final system, alongwith an example of their application in Table 2.Where appropriate we give the relevant labels inthe Stanford Dependency (SD) scheme in paren-theses for reproducibility:?
We extract the token itself and its lemma asfeatures.?
To handle cases where word senses are identi-fiable by the PoS of a token (?might result?
vs?the might?
), we combine the latter with thelemma and add it as a feature.?
We combine the lemma with the CCG supertagand add it as a feature in order to capture caseswhere the hedging function of a word is de-termined by its syntactic role in the sentence.For example, ?indicating?
in the example ofTable 1 is followed by a clausal complement (avery reliable predictor of hedging function forepistemic verbs), which is captured by its CCGsupertag.
As explained in Section 2, this in-formation can be recovered even in sentenceswhere the parser fails to produce a parse.?
Passive voice is commonly employed to limitcommitment to the statement made, thereforewe add it as a feature combined with thelemma to verbs in that voice (nsubjpass).?
Modal auxiliaries are prime hedging devicesbut they are also among the most ambiguous.For example, ?can?
is annotated as a cue in16% of its occurrences and it is the fifth mostfrequent cue in the full articles.
To resolvethis ambiguity, we add as features the lemmaof the main verb the auxiliary is dependent on(aux) as well as the lemmas of any dependentsof the main verb.
Thus we can capture somestereotypical speculative expressions in scien-tific articles (e.g ?could be due?
), while avoid-ing false positives that are distinguished by theuse of first person plural pronoun and/or ref-erence to objective enabling conditions (Kil-icoglu and Bergler, 2008).?
Speculation can be expressed via negation ofa word expressing certainty (e.g.
?we do notknow?
), therefore we add the lemma of the to-ken prefixed with ?not?
(neg).?
In order to capture stereotypical hedging ex-pressions such as ?raises the question?
and?on the assumption?
we add as features the di-rect object of verbs combined with the lemmaof their object (dobj) and the preposition fornouns in a prepositional relation (prep *).?
In order to capture the effect of adverbs on thehedging function of verbs (e.g.
?theoretically20features Recall Precision F-scoretokens, lemmas 75.92 81.07 78.41+PoS, CCG 78.23 83.71 80.88+syntax 81.00 81.31 81.15+combs 79.58 84.98 82.19Table 3: Performance of various feature sets onTask1 using cross-validation on full articles.considered?)
we add the lemma of the adverbas a feature to the verb (advmod).?
To distinguish the probabilistic/numericalsense from the hedging sense of adjectivessuch as ?possible?, we add the lemma and thenumber of the noun they modify as features(amod), since plural number tends to be as-sociated with the probabilistic/numerical sense(e.g.
?all possible combinations?
).Finally, given that this stage is meant to identifycues in order to recover their scopes in Task2, weattempt to resolve multi-token cues in the train-ing data into single-token ones.
This agrees withthe minimal strategy for marking cues stated in thecorpus guidelines (Szarvas et al, 2008) and it sim-plifies scope detection.
Therefore, during train-ing multi-token cues are resolved to their syntactichead according to the dependency output, e.g.
inTable 1 ?indicate that?
is restricted to ?indicate?only.
There were two cases in which this processfailed; the cues being ?cannot?
(S3.167) and ?notclear?
(S3.269).
We argue that the former is in-consistently annotated (the sentence reads ?cannotbe defined.
.
.
?
and it would have been resolved to?defined?
), while the latter is headed syntacticallyby the verb ?be?
which is preceding it.5 Task1 results and error analysisInitially we experiment using the full-articles partof the training data only divided in four folds.
Thereason for this choice is that the language of theabstracts is relatively restricted and phenomenathat appear only in full papers could be obscuredby the abstracts, especially since the latter con-sist of more sentences in total (11,871 vs. 2,670).Such phenomena include language related to fig-ures and descriptions of probabilistic models.Each row in Table 3 is produced by addingextra features to the feature set represented onthe row directly above it.
First we consider us-ing only the tokens and their lemmas as featuresfeatures Recall Precision F-scoretokens, lemmas 79.19 80.43 79.81+PoS, CCG 81.12 85.22 83.12+syntax 83.43 84.57 84.00+combs 85.16 85.99 85.58Table 4: Performance of various feature sets onTask1 using cross-validation on full articles incor-porating the abstracts as training data.which amounts to a weighted dictionary but whichachieves reasonable performance.
The inclusionof PoS tags and CCG supertags improves perfor-mance, whereas syntactic context increases recallwhile decreasing precision slightly.
This is dueto the fact that logistic regression does not rep-resent feature interactions and the effect of thesefeatures varies across words.
For example, clausalcomplements affect epistemic verbs but not otherwords (?indicate?
vs. ?state?
in the example ofTable 1) and negation affects only words express-ing certainty.
In order to ameliorate this limitationwe add the lexicalized features described in Sec-tion 4, for example the combination of the lemmawith the negation syntactic dependency.
These ad-ditional features improved precision from 81.31%to 84.98%.Finally, we add the abstracts to the training datawhich improves recall but harms precision slightly(Table 4) when only tokens and lemmas are usedas features.
Nevertheless, we decided to keep themas they have a positive effect for all other featurerepresentations.A misinterpretation of the BioScope paper(Szarvas et al, 2008) led us to believe that five ofthe nine full articles in the training data were anno-tated using the guidelines of Medlock and Briscoe(2007).
After the shared task, the organizers clar-ified to us that all the full articles were annotatedusing the BioScope guidelines.
Due to our misin-terpretation, we change our experimental setup tocross-validate on the four full articles annotated inBioScope only, considering the other five full ar-ticles and the abstracts only as training data.
Wekeep this setup for the remainder of the paper.We repeat the cross-validation experiments withthe full feature set and this new experimental setupand report the results in Table 5.
Using the samefeature set, we experiment with the Gaussian priorinstead of the sparsity-enforcing Laplace priorwhich results in decreased precision and F-score,21Recall Precision F-scorecross-Laplace 80.33 84.21 82.23cross-Gaussian 81.59 80.58 81.08test 84.94 85.48 85.21Table 5: Performance of the final system in Task1.therefore confirming our intuition that most fea-tures extracted are irrelevant to the task and shouldhave zero weight.
Finally, we report our perfor-mance on the test data using the Laplace prior.6 Detecting the scope of the hedgesIn Task2, the systems need to identify speculativecues and their respective scopes.
Since our systemfor Task1 identifies cues, our discussion of Task2focuses on identifying the scope of a given cue.It is a non-trivial task, since scopes can be nestedand can span over a large number of tokens of thesentence.An initial approach explored was to associateeach cue with the token representing the syntactichead of its scope and then to infer the scope us-ing syntactic parsing.
In order to achieve this, weresolved the (almost always multi-token) scopesto their syntactic heads and then built a classi-fier whose features are based on syntactic depen-dency paths.
Multi-token scopes which were notheaded syntactically by a single token (accordingto the parser) were discarded in order to obtain acleaner dataset for training.
This phenomenon oc-curs rather frequently, therefore reducing the train-ing instances.
At testing, the classifier identifiesthe syntactic head of the scope for each cue andwe infer the scope from the syntactic parser?s out-put.
If more than one scope head is identified fora particular cue, then the scopes are concatenated.The performance of this approach turned out tobe very low, 10.34% in F-score.
We identified twoprincipal reasons for this.
First, relying on the syn-tactic parser?s output to infer the scope is unavoid-ably affected by parsing errors.
Second, the scopeannotation takes into account semantics instead ofsyntax.
For example bibliographic references areexcluded based on their semantics.In order to handle these issues, we developed anapproach that predicts whether each token of thesentence belongs to the scope of a given cue.
Theoverall scope for that cue becomes the string en-closed by the left- and right-most tokens predictedto belong to the scope.
The features used by theclassifier to predict whether a token belongs to thescope of a particular cue are based on the short-est syntactic dependency path connecting them,which is found using Dijkstra?s algorithm.
If nosuch path is found (commonly due to parsing fail-ure), then the token is classified as not belongingto the scope of that cue.
The features we use arethe following:?
The dependency path between the cue and thetoken, combined with both their lemmas.?
According to the guidelines, different cueshave different preferences in having theirscopes extended to their left or to their right.For example modal auxiliaries like ?can?
inTable 1 extend their scope to their right.
There-fore we add the dependency path feature de-fined above refined by whether the token is onthe left or the right of the cue in question.?
We combine the dependency path and the lem-mas of the cue and the token with their PoStags and CCG supertags, since these tags re-fine the syntactic function of the tokens.The features defined above are very sparse, espe-cially when longer dependency paths are involved.This can affect performance substantially, as thescopes can be rather long, in many cases spanningover the whole sentence.
An unseen dependencypath between a cue and a token during testing re-sults in the token being excluded from the scopeof that cue.
In turn, this causes predicted scopes tobe shorter than they should be.
We attempt to al-leviate this sparsity in two stages.
First, we makethe following simplifications to the labels of thedependencies:?
Adjectival, noun compound, relative clauseand participial modifiers (amod, nn, rcmod,partmod) are converted to generic modifiers(mod).?
Passive auxiliary (auxpass) and copula (cop)relations are converted to auxiliary relations(aux).?
Clausal complement relations with inter-nal/external subject (ccomp/xcomp) are con-verted to complement relations (comp).?
All subject relations in passive or active voice(nsubj, nsubjpass, csubj, csubjpass) are con-verted to subjects (subj).?
Direct and indirect object relations (iobj, dobj)are converted to objects (obj).22?
We de-lexicalize conjunct (conj *) and prepo-sitional modifier relations (prep *).Second, we shorten the dependency paths:?
Since the SD collapsed dependencies formattreats conjunctions asymmetrically (conj), wepropagate the subject and object dependenciesof the head of the conjunction to the depen-dent.
We process appositional and abbrevi-ation modifiers (appos, abbrev) in the sameway.?
Determiner and predeterminer relations (det,predet) in the end of the dependency path areremoved, since the determiners (e.g.
?the?
)and predeterminers (e.g.
?both?)
are includedin/excluded from the scope following theirsyntactic heads.?
Consecutive modifier and dependent relations(mod, dep) are replaced by a single relation ofthe same type.?
Auxiliary relations (aux) that are not in the be-ginning or the end of the path are removed.Despite these simplifications, it is still possibleduring testing to encounter dependency paths un-seen in the training data.
In order to amelioratethis issue, we implement a backoff strategy thatprogressively shortens the dependency path untilit matches a dependency path seen in the trainingdata.
For example, if the path from a cue to a tokenis subj-mod-mod and it has not been seen in thetraining data, we test if subj-mod has been seen.If it has, we consider it as the dependency path todefine the features described earlier.
If not, we testfor subj in the same way.
This strategy relies onthe assumption that tokens that are likely to be in-cluded in/excluded from the scope following thetokens they are syntactically dependent on.
Forexample, modifiers are likely to follow the tokenbeing modified.7 Task2 results and error analysisIn order to evaluate the performance of our ap-proach, we performed four-fold cross-validationon the four BioScope full articles, using the re-maining full articles and the abstracts as trainingdata only.
The performance achieved using thefeatures mentioned in Section 6 is 28.62% F-score,while using the simplified dependency paths in-stead of the path extracted from the parser?s out-put improves it to 34.35% F-score.
Applying theback-off strategy for unseen dependency paths tofeatures Recall Precision F-scorestandard 27.54 29.79 28.62simplified 33.11 35.69 34.35+backoff 34.10 36.75 35.37+post 40.98 44.17 42.52Table 6: Performance on Task2 using cross-validation on BioScope full articles.the simplified paths results in 35.37% F-score (Ta-ble 6).Our system predicts only single token cues.This agrees in spirit with the minimal cue an-notation strategy stated in the BioScope guide-lines.
The guidelines allow for multi-token cues,referred to as complex keywords, which are de-fined as cases where the tokens of a phrase cannotexpress uncertainty independently.
We argue thatthis definition is rather vague, and combined withthe requirement for contiguity, results in cue in-stances such as ?indicating that?
(multiple occur-rences), ?looks as?
(S4.232) and ?address a num-ber of questions?
(S4.36) annotated as cues.
It isunclear why ?suggesting that?
or ?appears that?are not annotated as cues as well, or why ?that?contributes to the semantic content of ?indicate?.?that?
does help determine the sense of ?indicate?,but we argue that it should not be part of the cue asit does not contribute to its semantic content.
?in-dicate that?
is the only consistent multi-token cuepattern in the training data.
Therefore, when oursystem identifies as a cue a token with the lemma?indicate?, if this token is followed by ?that?,?that?
is added to the cue.
Given the annotationdifficulties multi-token cues present, it would beuseful during evaluation to relax cue matching inthe same way as in the BioNLP 2009 shared task,i.e.
considering as correct those cues predictedwithin one token of the gold standard annotation.As explained in Section 6, bibliographic ref-erences are excluded from scopes and cannot berecognized by means of syntactic parsing only.Additionally, in some cases the XML formattingdoes not preserve the parentheses and/or brack-ets around numerical references.
We employ twopost-processing steps to deal with these issues.First, if the ultimate token of a scope happens tobe the penultimate token of the sentence and anumber, then it is removed from the scope.
Thisstep can have a negative effect when the last to-ken of the scope and penultimate token of the sen-23Recall Precision F-scoreCues cross 74.52 81.63 77.91test 74.50 81.85 78.00Task2 cross 40.98 44.17 42.52test 42.40 45.96 44.11Table 7: Performance on cue identification andcue/scope identification in Task2.tence happens to be a genuine number, as in Fig-ure 1.
In our experiments however, this heuristicalways increased performance.
Second, if a scopecontains an opening parenthesis but not its clos-ing one, then the scope is limited to the token im-mediately before the opening one.
Note that thetraining data annotation allows for partial paren-thetical statements to be included in scopes, as aresult of terminating scopes at bibliographic ref-erences which are not the only tokens in a paren-theses.
For example, in S7.259: ?expressed (ED,unpublished)?
the scope is terminated after ?ED?.These post-processing steps improved the perfor-mance substantially to 42.52% F-score (Table 6).The requirement for contiguous scope spanswhich include their cue(s) is not treated appropri-ately by our system, since we predict each token ofthe scope independently.
Combined with the factthat the guidelines frequently define scopes to ex-tend either to the left or to the right of the cue, anapproach based on sequential tagging and/or pre-dicting boundaries could perform better.
However,as mentioned in the guidelines, the contiguity re-quirement sometimes forced the inclusion of to-kens that should have been excluded given the pur-pose of the task.Our final performance on the test data is 44.11%in F-score (Table 7).
This is higher than the one re-ported in the official results (38.37%) because wesubsequently increased the coverage of the C&Cparser (parse failures resulted in 63 cues not re-ceiving a scope), the addition of the back-off strat-egy for unseen dependency paths and the clarifica-tion on the inclusion of bibliographic references inthe scopes which resulted in improving the paren-theses post-processing steps.8 Related workThe shared task uses only full articles for testingwhile both abstracts and full articles are used fortraining.
We argue that this represents a realisticscenario for system developers since annotated re-sources consist mainly of abstracts, while most in-formation extraction systems are applied to full ar-ticles.
Also, the shared task aimed at detecting thescope of speculation, while most previous work(Light et al, 2004; Medlock and Briscoe, 2007;Kilicoglu and Bergler, 2008) considered only clas-sification of sentences, possibly due to the lack ofappropriately annotated resources.The increasing interest in detecting speculativelanguage in scientific text resulted in a number ofguidelines.
Compared to the most recent previousdefinition by Medlock and Briscoe (2007), Bio-Scope differs in the following ways:?
BioScope does not annotate anaphoric hedgereferences.?
BioScope annotates indications of experimen-tally observed non-universal behaviour.?
BioScope annotates statements of explicitlyproposed alternatives.The first difference is due to the requirement thatthe scope of the speculation be annotated, whichis not possible when it is present in a different sen-tence.
The other two differences follow from thestated purpose which is the detection of sentencescontaining uncertain information.In related work, Hyland (1996) associates theuse of speculative language in scholarly publica-tions with the purpose for which they are em-ployed by the authors.
In particular, he dis-tinguishes content-oriented hedges from reader-oriented ones.
The former are used to calibratethe strength of the claims made, while the latterare employed in order to soften anticipated crit-icism on behalf of the reader.
Content-orientedhedges are further distinguished as accuracy-oriented ones, used to express uncertain claimsmore accurately, and writer-oriented ones, usedto limit the commitment of the author(s) to theclaims.
While the boundaries between these dis-tinctions are not clear-cut and instances of hedgingcan serve more than one of these purposes simulta-neously, it is worth bearing them in mind while ap-proaching the task.
With respect to the shared task,taking into account that hedging is used to ex-press statements more accurately can help resolvethe ambiguity when annotating certain statementsabout uncertainty.
Such statements, which involvewords such as ?estimate?, ?possible?, ?predict?,occur frequently in full articles.Wilson (2008) analyzes speculation detection24inside a general framework for sentiment analysiscentered around the notion of private states (emo-tions, thoughts, intentions, etc.)
that are not opento objective observation or verification.
Specu-lation is annotated with a spec-span/spec-targetscheme by answering the questions what the spec-ulation is and what the speculation is about.
Withrespect to the BioScope guidelines, spec-span issimilar to what scope attempts to capture.
spec-span and spec-target do not need to be presentat the same time, which could help annotatinganaphoric cues.9 ConclusionsThis paper describes our approach to the CoNLL-2010 shared task on speculative language detec-tion using logistic regression and syntactic depen-dencies.
We achieved competitive performance onsentence level uncertainty classification (Task1),but not on scope identification (Task2).
Motivatedby our error analysis we suggest refinements to thetask definition that could improve annotation.Our approach to detecting speculation cues suc-cessfully employed syntax as a proxy for the se-mantic content of words.
In addition, we demon-strated that performance gains can be obtained bychoosing an appropriate prior for feature weightsin logistic regression.
Finally, our performance inscope detection was improved substantially by thesimplification scheme used to reduce the sparsityof the dependency paths.
It was devised using hu-man judgment, but as information extraction sys-tems become increasingly reliant on syntax andeach task is likely to need a different scheme, fu-ture work should investigate how this could beachieved using machine learning.AcknowledgementsWe would like to thank the organizers for provid-ing the infrastructure and the data for the sharedtask, Laura Rimell for her help with the C&Cparser and Marina Terkourafi for useful discus-sions.
The authors were funded by NIH/NLMgrant R01/LM07050.ReferencesJinying Chen and Martha Palmer.
2009.
ImprovingEnglish Verb Sense Disambiguation Performancewith Linguistically Motivated Features and ClearSense Distinction Boundaries.
Language Resourcesand Evaluation, 43(2):143?172.Marie-Catherine de Marneffe and Christopher D. Man-ning.
2008.
The Stanford typed dependencies rep-resentation.
In CrossParser ?08: Coling 2008: Pro-ceedings of the Workshop on Cross-Framework andCross-Domain Parser Evaluation, pages 1?8.Richa?rd Farkas, Veronika Vincze, Gyo?rgy Mo?ra, Ja?nosCsirik, and Gyo?rgy Szarvas.
2010.
The CoNLL-2010 Shared Task: Learning to Detect Hedges andtheir Scope in Natural Language Text.
In Proceed-ings of CoNLL-2010 Shared Task, pages 1?12.Alexander Genkin, David D. Lewis, and David Madi-gan.
2006.
Large-scale Bayesian Logistic Re-gression for Text Classification.
Technometrics,49(3):291?304.Ken Hyland.
1996.
Writing Without Conviction?Hedging in Science Research Articles.
Applied Lin-guistics, 17(4):433?454.Halil Kilicoglu and Sabine Bergler.
2008.
Recog-nizing speculative language in biomedical researcharticles: a linguistically motivated perspective.
InBioNLP ?08: Proceedings of the Workshop on Cur-rent Trends in Biomedical Natural Language Pro-cessing, pages 46?53.Jin-Dong Kim, Tomoko Ohta, Sampo Pyysalo, Yoshi-nobu Kano, and Jun?ichi Tsujii.
2009.
Overviewof BioNLP?09 shared task on event extraction.
InBioNLP ?09: Proceedings of the Workshop onBioNLP, pages 1?9.Marc Light, Xin Ying Qiu, and Padmini Srinivasan.2004.
The Language of Bioscience: Facts, Specu-lations, and Statements In Between.
In HLT-NAACL2004 Workshop: BioLINK 2004, Linking BiologicalLiterature, Ontologies and Databases, pages 17?24.Ben Medlock and Ted Briscoe.
2007.
Weakly Super-vised Learning for Hedge Classification in ScientificLiterature.
In Proceedings of the 45th Annual Meet-ing of the Association of Computational Linguistics,pages 992?999.Guido Minnen, John Carroll, and Darren Pearce.
2001.Applied morphological processing of English.
Nat-ural Language Engineering, 7(3):207?223.Laura Rimell and Stephen Clark.
2009.
Port-ing a lexicalized-grammar parser to the biomedi-cal domain.
Journal of Biomedical Informatics,42(5):852?865.Gyo?rgy Szarvas, Veronika Vincze, Richa?rd Farkas, andJa?nos Csirik.
2008.
The BioScope corpus: anno-tation for negation, uncertainty and their scope inbiomedical texts.
In BioNLP ?08: Proceedings ofthe Workshop on Current Trends in Biomedical Nat-ural Language Processing, pages 38?45.Theresa Ann Wilson.
2008.
Fine-grained Subjectivityand Sentiment Analysis: Recognizing the Intensity,Polarity, and Attitudes of Private States.
Ph.D. the-sis, University of Pittsburgh.25
