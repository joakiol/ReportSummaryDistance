Expressing ConcernComputer Science DivisionUniversity of California t BerkeleyBerkeley, CA 94720U.S.A.AbstractA consultant system's main task is to provided helpful adviceto the user.
Consultant systems hould not only find solutionsto user problems, but should also inform the user of potentialproblems with these solutions.
Expressing such potentialcaveats is a difficult process due to the many potential planfailures for each particular plan in a particular planning situa-tion.
A commonsense planner, called KIP, Knowledge Inten-sive Planner, is described.
KIP is the planner for the UNIXConsultant system.
KIP detect potential plan failures using anew knowledge structure termed a concern.
Concerns allowKIP to detects plan failures due to unsatisfied conditions orgoal conflict.
KIP's concern algorithm also is able to provideinformation to the expression mechanism regarding potentialplan failures.
Concern information ispassed to the expressionmechanism when KIP's selected plan might not work.
In thiscase, KIP passes information regarding both the suggestedplan and the potential caveats in that plan to the expressionmechanism.
This is an efficient approach since KIP mustmake such decisions in the context of its planning process.
Aconcern's declarative structure makes it easier to express thanprocedural descriptions of plan failures used by earlier sys-tems.Marc LuriaComputer Science DepartmentTechnion, Israel Institute of TechnologyHalfaIsrael(a2) Let me know if the door is locked.
(a3) Be careful walking down the stairs.
(a4) Make sure to turn off the basement light.In (al), the mother has provided the child with informationabout the location of his shoe.
The mother has also impliedthe use of a plan: Walk down to the basement and get yourshoes.
However, there are a number of problems inherent inthis plan.
The mother might also inform her child of theseproblems.
The first problem, (a2), is that one of the condi-tions necessary to execute the plan might be unsatisfied.
Thedoor to the basement might be locked.
If it is locked addition-al steps in the plan will be necessary.
The second problem,(a3), is that executing the walk-down-the-stairs plan mightresult in a fall.
The mother knows that this outcome is likely,due to her experience of the child's previous attempts at thewalk-down-the-stairs plan.
The mother wishes to prevent thechild from falling, since this is a potentially dangerous andfrightening experience for the child.
The third problem, (a4),is that the child might forget to turn off the light in the base-ment.
This would threaten the mothers's goal of preventingthe basement light from burning out.However, the same parent might not add:I. IntroductionThe most important task of a consultant is to provideadvice to a user.
Human consultants are asked to provideanswers to user queries in domains within which they havemore expertise than the user.
In some cases, the answers pro-vided to the user are basic information about a particulardomain.
However, in many cases, the task of the consultant isprovide answers to user problems.
Furthermore, they are notonly asked to find solutions, they are also asked to use theirexpertise to anticipate potential problems with these solutions.Let us consider a very simple example of a consultant rela-tionship.
For example, suppose a child asks the followingquestion:(a) Where is my shoe?His mother might respond:(al) It's in the basement.However, his mother might also add:(a5) Let me know if the door needs to be oiled(a6) Be careful walking in the basement(a7) Make sure to close the basement doorThis second set of responses also provide advice that reflectsproblems due to unsatisfied conditions of the plan or potentialgoal conflicts.
However, the mother might not decide to ex-press these statements to the child since they are either unlike-ly or unimportant causes of potential plan failure.Therefore, the mother has made three decisions.
First,she has decided which plan to suggest o the child based onhis world knowledge.
Secondly, she has decided which partsof that plan should be expressed to the child.
Thirdly, she hasdecided which potential caveats in that plan should be ex-pressed to the child based on her experience.Previous research in intelligent user interfaces (Allen84,Appelt85, McDonald84) has focused on the second decision.Systems attempt not to violate Grice's second Maxim ofQuantity: Make your contribution as informative as is re-quired (Grice 1975).
These systems formulated a response221that would provide information or a plan to the user.
Allensought o discover obstacles in the user's plan.
He tried tohelp the user's plan by providing the user with the informationhe needed to execute that plan.
However, he did not provide amechanism for expressing plan failures.
In this paper, wefocus on the problem of making decisions regarding those po-tential problems which should be expressed to the user.
How-ever, rather than using a separate mechanism for this purpose,we propose that this decision be made using information pro-vided by the problem solving component of the system, theplanner.We describe a commonsense planner called KIP,Knowledge Intensive Planner.
KIP is being developed forUC, the UNIX Consultant system (Luria85, Wilensky 84a,86).
UC provides solutions to user problems in the UNIXoperating system domain.
KIP provides the informationnecessary in order to make decisions regarding which poten-tial plan failures should be expressed to the user.
KIP mustmake decisions regarding potential plan failures in order todevise a good plan for the user.
Rather than use a separateprocess to make decisions about those potential plan failureswhich should be expressed to the user, KIP provides this in-formation to the expression mechanism directly.
In the nextsection, we give some examples of KIP's interaction with ex-pression mechanism.
KIP provides information about poten-tial plan failures which will be expressed to the user.
We alsodescribe KIP's role in the UNIX Consultant system.
In thefollowing section, we describe a declarative representationthat allows KIP to detect and provide expression i formationabout potential plan failures.2.
KIP Examples(b) User:UC:How do I pr int J im's file onthe l inepr inter?Print J im's fi le foo bytyping ipr foo.
This planwil l  not work if you don't  haveread permiss ion  on J im's fileor if the pr inter  is out ofpaper.
(c) User:UC:How do I move a file namedpaul to the file named mary?To move the file paul to thefile named mary, type mv paulmary.
However, if the filemary exists, it wil l  bedeleted.
(d) User:UC:How do I change my password?To change your passwd use thepasswd command.
However,remember that if you changeyour password on one machine,it wil l  not be changed onother machines.In each of these examples, KIP has selected a knownplan for accomplishing the goals of the user.
However, ineach of these examples, KIP determines that the plan couldfail and therefore has decided to express this potential failureto the user.KIP has a large knowledge-base of information aboutthe UNIX operating system.
Decisions regarding UC's ownactions are made by UCEgo.
The parser and goal analyzer(Mayfield 86) of UC pass KIP a set of goals, and KIP wies tofind appropriate plans for those goals.
KIP determines a planfor the problem, and notes which potential plan failures houldbe expressed to the user.
KIP passes this decision-making in-formation to the UCExpression mechanism(Chin86, Wilen-sky86).
The expression mechanism decides how to expressthe plan to the user, given a model of the user's knowledgeabout UNIX.
The plan is then passed to the natural languagegenerator, which generates a natural language response to theuser.
UC is a conversational system, and if necessary KIP canquery the user for more information.
Nevertheless, KIP triesto provide the best plan it can with the information providedby user.3.
ConcernsIn the previous ections, we have described the impor-tance of informing the user about potential problems with aplan.
In this section, we describe a new concept which wehave introduced, termed a concern.
A concern allows KIP topredict potential plan failures and provide knowledge to ex-press potential plan failures to the user.A concern refers to those aspects of a plan which shouldbe considered because they are possible sources of planfailure.
A concern describes which aspects of a plan are likelyto cause failure.There are two major types of concerns, condition con-cerns, and goal conflict concerns.
These two types reflect hetwo major types of plan failure.
Condition concerns refer tothose aspects of a plan that are likely to cause plan failure dueto a condition of the plan that is needed for successful execu-tion.
The conditions about which KIP is concerned are alwaysconditions of a particular plan.
(These are fully described inLuria86, 87a).Goal conflict concerns refer to those aspects of a planwhich are likely to cause plan failure due to a potential goalconflict between an effect of a plan and a goal of the user.Goal conflict concerns relate plans to user goals and to otherpieces of knowledge that are not part of the plan.
Examples ofthis knowledge include background goals which may bethreatened by the plan.
Since these background goals are notusually inferred until such a threat is perceived, goal conflictconcerns often refer to conflicts between a potential plan and along-term interest of the user.
Interests are general states thatKIP assumes are important to the user.
An interest differsfrom a goal in that one can have interests about general statesof the world, while goals refer to a concrete state of the world.For example, preserving the contents of one's files is an in-terest, while preserving the contents of the file named filel is a222goal.
KIP's knowledge-base includes many interests that KIPassumes on the part of the user.
Goals are generated onlywhen expressed by the user, or by KIP itself during the plan-ning process.Stored goal conflict concerns refer to concerns aboutconflicts of interest.
These are concerns about the selectedplan conflicting with an interest of the user.
If KIP detects aconflict-of-interest concern, then KIP must determine if itshould infer an individual goal on the part of the user thatreflects this interest.
If KIP decides to infer this individualgoal, then a dynamic concern between the selected plan andthe individual goal is also instantiated.
(Goal conflict aredescribed more fully in Luria87b.
)Some plan failures are more likely to occur than others,and some plan failures are more important than others if theydo occur.
The representation f concerns reflects this differ-ence by assigning a varying degree of concern to the storedconcerns in the knowledge base.
The degree of a conditionconcern reflects both the likelihood that the condition will fail,and the importance of satisfying the condition for the success-ful execution of the plan.
There are many factors that deter-mine the degree of concern about a confiict-of-interes~.
Theplanning knowledge base designer needs to determine howlikely a conflicting effect is to occur, how likely it is that theuser holds the threatened goal, and how important this goal isto the user.In the present implementation f KIP, information re-garding concerns of potential plans is supplied by a human ex-pert with a great deal of UNIX experience.
Stored concernsare therefore, a way for the planner database designer to ex-press his personal experience regarding those aspects of astored plan that are most likely to fail.
In principle, however,the information might be supplied by an analysis of data of ac-tual UNIX interactions.4.
Concerns and ExpressionIn this section, we describe the problems that concernswere initially meant o address in plan failure detection.
Wealso describe how this same process has been used to expresspotential plan failures to the user.KIP is a a commonsense planner ONilensky83) - aplanner which is able to effectively use a large body ofknowledge about a knowledge-rich domain.
Such knowledgeincludes a general understanding of planning strategy, detaileddescriptions of plans, the conditions necessary for these plansto execute successfully, and descriptions of those potentialgoal conflicts that the plans might cause.
Due to the detailednature of this knowledge, it is difficult to detect potential planfailures.
Condition failures are hard to detect since there aremany conditions for any particular plan.
Goal conflict failuresare difficult to detect since any of the many effects couldconflict with any of the many goals of the user.
Furthermore,many of the user goals are not inferred until a threat o user in-terest is perceived.
Previous planning programs (Fikes71,Newel172, Sacerdoti74) searched exhaustively among everycondition and every potential goal conflict for potential planfailure.
This is a very inefficient process.
On the other hand,human consultants generally consider only a few potentialplan failures while assessing a particular plan.Additionally, KIP may not be aware of the values ofmany of the conditions of a particular plan.
Most previousplanning research assumed that the values for all the condi-tions is known.
However, in UC, when a user describes aplanning problem which is then passed to KIP, the values formany conditions are usually left out.
All users would believethat normal conditions, like the machine being up, would beassumed by the consultant.
A naive user might not be awareof the value of many conditions that require a more sophisti-cated knowledge of UNIX.
An expert user would believe thatthe consultant would make certain assumptions requiring thismore sophisticated knowledge of UNIX.
It would be undesir-able to prompt the user for this information, particularly forthose values which axe not important for the specific plonningsituation.Therefore, concerns were introduced in order to detectplan failures.
Concerns allow KIP to use information aboutthe likelihood and importance of potential plan failures.
Theyallow the planning database designer to store knowledge re-garding which conditions are most likely to be unsatisfied, andwhich goal conflicts are most likely to occur as a result of theexecution of a particular plan.Furthermore, the same concern information can be usedin order to determine which potential plan failures should beexpressed to the user.
When, KIP selects a potential plan, theconcerns of that particular plan are evaluated in the particularplanning situation.
Once the concerns of a plan are evaluatedthere are three possible scenarios.
In the first case, none of theconcerns are important in the particular planning situation.The plan is generated to the user without any concern infor-mation.
In the second case, there is a moderate degree of con-cern regarding the plan.
In this case, the plan is generatedalong with the concern information.
In cases where there is ahigh degree of concern, the plan is modified or a new plan isselected.
These scenarios will be fully explained in the fol-lowing section.
Before describing KIP's algorithm regardingdecisions about concerns, we first describe a simple exampleof the use of concerns.
For the purposes of this example, weconsider only condition concerns.5.
An Example of the Use of ConcernsThe simplest use of concerns addresses the problem ofspecifying which conditions of a particular plan are importantenough invoke the planner's concern.
For example, supposethe user asks the following question:(e) How do I pr int  out the fi le namedgeorge on the laser pr inter?KIP is passed the goal of printing the file named georgeon the laser printer.
In this case, KIP's knowledge-base con-223rains a stored plan for the goal of printing a file, namely, theUSE-LSPR-COI~gclAND plan.
KIP creates an instance of thisplan, which it calls USE-LSPR-COMMANDI.
KIP must thenevaluate the USE-LSPR-COMMAND1 plan in order to determineif the plan is appropriate for this particular planning situation.This process entails the examination of those conditions likelyto cause failure of this plan.In order to examine these conditions, KIP looks at thestored concerns of the stored plan, USE-LSPR-COMMAND.
Foreach of the stored concerns of the stored plan, it creates adynamic concern in this individual plan, USE-LSPR-COMMANDI.
KIP examines the USE-LSPR-COM~'IAND plan,and finds that two of its many conditions are cause for con-cern:(i) the pr inter  has paper(2) the pr inter  is onl ineThe most likely cause of plan failure involves (1), since thepaper runs out quite often.
Therefore, (1) has a moderate de-gree of concern, and (2) has a low degree of concern.
KIPconsiders the most likely concerns first.
These concerns arecalled stored condition concerns, because the failure of theseconditions" often causes the failure of USE-LSPR-COMMAND.KIP therefore creates dynamic concerns regarding the paper inthe printer, and the printer being online.KIP then must evaluate ach of these dynamic concerns.In this particular example, there is no explicit informationabout the paper in the printer or the printer being online.Therefore, KIP uses the default values for the concerns them-selves.
KIP's concern about paper in the printer is highenough to warrant further consideration.
Therefore, this con-tern is temporarily overlooked.
However, the concern aboutthe printer being online is disregarded.
Its degree of concernis low.
It is not a very likely source of plan failure.
Sincethere are no other dynamic concerns for this particular plan,KIP looks back at its overlooked concern.
Since this is theonly concern, and the degree of concern is moderate, KIP de-cides that this concern should not be elevated to a source ofplan failure.
Rather, KIP decides to express this concern tothe user.
KIP assumes that, except for this concern, the planwill execute successfully.
The plan is then suggested to theuser:(E) UC: To print the file george on thelaser printer, type lpr -Plpgeorge.
This plan will not workif the pr inter  is out of paper.There are many other conditions of the USE-LSPR-COMMAND plan that KIP might have considered.
For exam-ple, the condition that the file exists is an important conditionfor the lpr command.
However, KIP need not be concernedabout this condition in most planning situations, since it is un-likely that this condition will cause plan failure.
Hence suchconditions are not stored in the long term memory of KIP asstored concerns.6.
KIP 's  Concern Treatment AlgorithmIn the following section, we describe the part of KIP'salgorithm that decides what to do with concerns once theyhave been evaluated.
KIP's entire algorithm for determiningthe concerns of a particular plan is fully described in(Luria86) and CLuria87ab).Once KIP has evaluated a particular dynamic concern ofa particular plan, it can proceed in one of three ways, depend-ing on the degree of that particular concern.
If the degree ofconcern is low, KIP can choose to disregard the concern.Disregard means that the concern is no longer considered atall.
KIP can u'y to modify other parts of the plan, and suggestthe plan to the user with no reference to this particular con-tern.If the degree of concern is high, KIP can choose toelevate the concern to a source of plan failure.
In this case,KIP determines that it is very likely that the plan will fail.KIP tries to fix this plan in order to change the value of thiscondition, or tries to find another plan.The most complex case is when the degree of concern ismoderate.
In this case, KIP can choose to disregard the con-cern, or elevate it to a source of plan failure.
KIP can alsochoose to overlook the concern.KIP then evaluates each of the concerns of a particularplan.
It addresses all of the concerns which have been elevat-ed to a a source of plan failure.
KIP thus develops a completeplan for the problem by satisfying conditions about which itwas concerned, and resolving goal conflicts about which itwas concerned.
Once KIP has developed a complete plan, it isonce again faced with the need to deal with the overlookedconcerns.
If the plan will work, except for the overlookedconcerns, KIP can again choose to disregard the concern.
Ifthere are a number of overlooked concerns KIP may choose toelevate one or more of these overlooked concerns to a sourceof plan failure.
The plan is then modified accordingly, or anew plan is selected.At this point, KIP can also choose to suggest an answerto the user.
Any, overlooked concerns are then expressed tothe user in the answer.Furthermore, if the concern has been elevated to asource of plan failure, and no other acceptable plan has beenfound, KIP can choose to suggest he faulty plan to the user,along with the potential caveats.
The concern information isbased on default knowledge that assumed by KIP.
Therefore,the plans may work if these defaults are not correct even ifthere are concerns in the particular planning situation.
Also,the user may decide that he is not concerned about particularplan failure.
For example, KIP may have mid the user about apotential deleterious side effect.
The user may decide that thisside effect is not that important if it occurs.
This correspondsto a human consultant, who, when faced with a problem hecannot solve, gives the user a potentially faulty plan with theexplanation of the potential caveats.
This is more informative' 224for the user than just saying that he doesn't know.7.
Advantages of ConcernsThus, concerns are used by KIP to decide how the plan-ning process hould proceed and how to decide which answeris expressed.
In this section, we describe a few more exam-pies of KIP's behavior In these examples, we also refer to anew type of concern called a violated default concern.
Theseconcerns are accessed by KIP whenever it realizes that a de-fault has been violated.
In this way, KIP can use knowledgefrom default concerns when there is no knowledge that de-faults have been violated.
However, when planning in novelsituations, general violated default concerns are accessed.Consider the following examples:(f) How do I edit the file anyfi le?
(g) How do I edit J im's file j imfi le?
(h) How do I edit the file groupf i lewhich is shared by my group?One of KIP's main concerns in any of the possible edit-ing plans is the write permission of the file.
If the user tries toedit a file on which he does not have write permission, theplan will fall.
In (f), this concern is inherited from the editplan with a relatively low degree of concern.
According to thedefault case, the file belongs to the user and he has write per-mission on the file.
Since there is no infortnation about thewrite permission of the file, the default must be assumed andthis concern is disregarded.
KIP would therefore return a planof(F) To edit the file named anyfile, usevi anyfi le.In (g) ,  KIP realizes that the default of the file belong-ing to the user is violated.
Due to this default violation, aviolated default concern of having write permission on the fileis created.
This concern of write permission is thereforeevaluated by the default mechanism.
Since there is a verygood chance that the plan will not work, this concern aboutwrite permission of the file is elevated to a cause of planfailure.
Once a condition is a cause of plan failure, KIP mustdeal with the plan failure.
KIP c/n suggest a plan for chang-ing the condition or try some new plan.
In this case, sincethere is no way to change the write permission of J im's file,another plan is chosen.
(G) In order to edit J im's file, copy thefile to your d i rectory and then use vif i lename to edit the file.In (h) ,  KIP also realizes that the default of the file be-longing to the user has been violated.
However, the defaultvalue for write permission of this file is different because thefile belongs to the user's group.
There is a good chance thatthe user does have write permission on the file.
However,since there still is some chance that he does not have groupwrite permission, there is still some concern about the condi-tion.
In this case, since the degree of concern is moderate,KIP can choose to overlook the concern, and suggest he planto the user.
However, the concern is sdli high enough that theanswer expression mechanism (Luria 82ab), might choose toexpress the concern to the user.
The answer to (h) wouldtherefore be:(H) To edit the file groupfi le,  use vigroupf i le.
However, it might not work,if you don't have write permiss ion onthis par t i cu la r  group file.KIP can therefore use concerns to select a potential planwhich has a moderate likelihood of success.
KIP can expressthe plan and its reservations regarding the plan to the user.
Bytemporarily overlooking a concern, KIP may search for otherplan failures of a particular plan or other potential plans.
KIPcan accomplish this without completely disregarding a con-cern or elevating the concern to a source of certain planfailure.8.
Implementation and RepresentationKIP is implemented in Zetalisp on a Symbolics 3670.Concepts are represented in the KODIAK knowledge represen-tation language (Wilensky84b).
In particular, knowledgeabout UNIX commands has been organized in complex hierar-chies using multiple inheritance.
Therefore, when searchingfor stored default concerns of a particular plan that uses a par-ticular UNIX command, KIP must search through a hierarchyof these commands.
This is also true when looking for defaultviolations.
KIP searches up the hierarchy, and retrieves thestored concerns or default violations in this hierarchy.Stored condition concerns are presently implemented bycreating a different CONCERN concept for each concern.
Also,a HAS-CONCER~ relation is added between each concern andthose conditions which are cause for concern.
Degrees of con-cern are implemented by creating a HAS-CONCERN-LEVEL re-lation between the particular concern and the degree of con-cern.
Degrees of concerns are presently implemented asnumbers from one to ten.
Dynamic condition concerns areimplemented as instances of these stored concerns.Stored goal conflict concerns are presendy implementedby creating a different CONCERN concept for each concern.Also, a 3-way HAS-CONCERN relation is created between eachconcern, the conflicting effect and the threatened interest orgoal which are cause for concern.Defaults are implemented in the current version of KIPby attaching default values of conditions to the plans them-selves.
Context-dependent defaults are implemented by ex-ploiting the concretion mechanism of UC, which tries to findthe most specific concept in the hierarchy.
Therefore, sinceKIP retrieves the most specific plan in the knowledge-base, itautomatically retrieves the most specific defaults.Violated default concerns are implemented by creatinga different VIOLATED-DEFAULT-CONCERN concept for eachviolated default concern.
A HAS-VIOLATED-DEFAULT-225CONCERN relation is added between the concern and thestored default which is violated.
Therefore, when KIP hasfound the default hat has been violated, it looks for the violat-ed default concerns that are referenced by this default.Particular concerns have been entered into the databaseof UNIX plans through a KODIAK knowledge representationacquisition language called DEFABS.
These concerns are allbased on my experience using UNIX and on discussions Ihave had with other UNIX users in our research group.
Weare currently investigating a way to enter this concern infor-mation, using the UCTeacher program (Martin, 1985) a natur-al language knowledge acquisition system.
Eventually, KIPmay incorporate a learning component that would allow KIPto detect he frequency of certain plan failures and to storethese as concerns.9.
Previous Research9.1.
PlanningEarly planners uch as STRIPS (Fikes71) did not addressGoal Conflict Detection as a separate problem.
Conflicts weredetected by the resolution theorem prover, The theoremprover compares a small set of add or delete formulas, and asmall set of formulas that described the present state and thedesired state of the world.
If an action deleted the precondi-tion of another action in the plan sequence, backtracking al-lowed the planner to determine another ordering of the plansteps.
ABSTRIPS (Sacerdod74), modified STRIPS to avoid theseinteracting subgoal problems by solving goals in a hierarchicalfashion.
Conflicts in ABSTRIPS were also noticed by thetheorem prover.
However, since the most important parts ofthe plan were solved first, they occurred less often and fewerpaths were explored.
Thus, both these programs identified aplan failure as a failed path in the search tree.
Therefore, noinformation about he nature of a failed path could easily beextracted and expressed toa user of the planning system.Sacerdoti's NOAH (Sacerdoti77) program separated thedetection of conflicts from the rest of the planning process us-ing his Resolve-Conflicts critic.
This critic detects one partic-ular kind of conflict, in which one action deletes the precondi-tion of another action.
We refer to this type of conflict as adeleted precondition plan conflict.
The critic resolves theconflict by committing to an ordering of steps in which the ac-tion which requires the precondition is executed first.
Theordering of steps is usually possible since NOAH uses a leastcommitment strategy for plan step ordering.
By separating thedetection of goal conflicts from the rest of the planning pro-cess, NOAH needs to search fewer plan paths than earlierplanners.In order to detect conflicts NOAH computes a TOME, atable of multiple effects, each time a new action is added tothe plan.
This table includes all preconditions which are as-serted or denied by more than one step in the current plan.Conflicts are recognized when a precondition for one step isdenied in another step.
In order to construct this table, NOAHmust enter all the effects and preconditions for each of thesteps in the plan every time a new step is added to the plan.NOAH'S separation of the Goal Conflict Detection Phasefrom the rest of the planning process was an important addi-tion to planning research.
However, NOAH'S approach isproblematic in a number of ways.
First, it only detectsconflicts that occur as a result of deleted preconditions.
Otherconflicts, such as conflicts between effects of a plan and otherplanner goals, cannot be detected using this method.
Most ofthe examples in this paper are part of this category of conflict.If many planner goals were included in a TOME, as would benecessary in real world planning situations, this method wouldbe computationally inefficient, Therefore, the same problemsthat were discussed earlier in regard to exhaustive search alsoapply to this method.
A TOME is (1) computationallyinefficient, (2) not cognitively valid, (3) unable to deal withdefault knowledge, and (4) assumes that all user goals areknown, i.e.
would have to evaluate very planner interest in aparticular planning situation.Furthermore, information from a critic which is derivedfrom a TOME is very difficult to express.
The only thing thatNOAH knows regarding a potential plan failure is that onestep in a plan will delete the precondition of another step inthe plan.
A concern, on the other hand is very easy express tothe user.
Concerns connect the various objects that are effect-ed by a plan failure.
In addition, as in any part of the KO-DIAK knowledge base, additional expression i formation canbe attached to the concern itself.
This difference between aconcern and a TOME is another example of the advantage ofknowledge-rich declarative representations over proceduralrepresentation f knowledge.9.2.
ExpressionAs discussed earlier, work in intelligent userinterfaces(Allen84, Appelt85, McDonald84) has primarilyfocused on decisions regarding what aspects of a plan shouldbe expressed to the user.
Expressing concerns about potentialplan failures is a natural extension to these other user inter-faces.The texture of this work is very similar to work doneearlier by the author.
In earlier work on question answering ina text understanding system (Luria82ab), question-answeringwas divided into two separate processes.
According to earlierwork one question-answering process determined what wascontained in the answer and how that information was ex-pressed to the user.
The first of our two processes determinedwhich part of a causal chain was relevant for a particularanswer.
The second process determined which part of thatcausal chain should be generated into a natural anguageresponse for the user.
This resulted in one relatively simpleprocess that found that correct response, and another moregeneral expression process termed answer expression.In the present work, the process of expressing potentialcaveats in a plan was not divided into two new processes, In-stead, this process is divided into the preexisting planningcomponent, and a more general expression mechanism.
In sodoing, we have improved the ability of the planning com-ponent to deal with potential plan failures.22610.
ReferencesAllen, J.
1984.
Recognizing Intentions From NaturalLanguage Utterances.
In Michael Brady (ed.)
Computa-tional Models of Discourse Cambridge, Mass; MITPress.Appelt, D. 1982.
Planning Natural Utterances to Satisfy Mul-tiple Goals.
SRI International AI Center Technical Note259.Chin, D. N. 1987.
"KNOME: Modeling What the User Knowsin UC" to appear in User Modelling in Dialog Systems,Springer-Verlag series on Symbolic Computation.Ernst, G. and Newell, A.
1969.
GPS: A Case Study in Gen-erality and Problem Solving.
New York: AcademicPress.Fikes, R. E., and Nilsson, N. J.
STRIPS: A new approach tothe application of theorem proving to problem solving.Artificial Intelligence, Vol.
2, No.
3-4, pp.
189-208.1971.Grice, H. P. Logic and Conversation.
In P. Cole (ed.)
Syntaxand Semantics, Vol.
3: Speech Acts, New York: Academ-ic Press, pp.
41-58.Luria, M. "Question Answering: Two Separate Processes"Proceedings of the 4th National Conference of the Cog-nitive Science Society, Ann Arbor, MI August, 1982.Luria, M. "Dividing up the Question Answering Process"Proceedings of the National Conference on Artificial In-telligence, Pittsburgh, PA. August, 1982.Luria, M. "Commonsense Planning in a Consultant System"Proceedings of 9th Conference of the IEEE on Systems,Man, and Cybernetics, Tuscon, AZ.
November, 1985.Luria, M. "Concerns: How to Detect Plan Failures."
Proceed-ings of the Third Annual Conference on Theoretical ls-sues in Conceptual Information Processing.
Philadel-phia, PA. August, 1986.Luria, M. "Concerns: A Means of Identifying Potential PlanFailures."
Proceedings of the Third IEEE Conference onArtificial Intelligence Applications.
Orlando, Florida.February, 1987.Luria, M. "Goal Conflict Concerns" Proceedings of the TenthInternational Joint Conference on Artificial Inteligence.Milan, Italy.
August, 1987.McDonald, D. 1984.
Natural Language Generation as a com-putational problem.
In Michael Brady (ed.)
Computa-tional Models of Discourse Cambridge, Mass; MITPress.Martin, J., 1985.
Knowledge Acquisition Through NaturalLanguage Dialogue, Proceedings of the 2nd Conferenceon Artificial Intelligence Applications, Miami, Florida,1985.Mayfield, J., 1986.
When to Keep Thinking, Proceedings ofthe Third Annual Conference on Theoretical Issues inConceptual Information Processing.
Philadelphia, PA.1986.Newell, A., and Simon, H. A.
Human Problem Solving.Prentice-Hall, Englewood Cliffs, N. J.
1972.Sacerdoti, E., Planning in a Hierarchy of Abstraction Spaces,Artificial lnteUigence Vol.
5, pp.
115-135, 1974.Sacerdoti E. A Structure for Plans and Behavior ElsevierNorth-Holland, New York, N.Y. 1977.Wilensky, R. Planning and Understanding: A ComputationalApproach to Human Reasoning.
Addison-Wesley, Read-ing, Mass., 1983.Wilensky, R., "KODIAK: A Knowledge RepresentationLanguage".
Proceedings of the 6th National Conferenceof the Cognitive Science Society, Boulder, CO, June1984.Wilensky, R., Arens, Y., and Chin, D. Talking to Unix in En-glish: An Overview of UC.
Communications of the As-sociation for Computing Machinery, June, 1984.Wilensky, R., et.
al., UC - A Progress Report.
University ofCalifornia, Berkeley, Electronic Research LaboratoryMemorandum No.
UCB/CSD 87/303.
1986.Sponsored by the Defense Advanced Research ProjectsAgency (DoD), Arpa Order No.
4871, monitored by Spaceand Naval Warfare Systems Command under ContractN00039-84-C-0089.227
