Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, pages 766?777,Berlin, Germany, August 7-12, 2016.c?2016 Association for Computational LinguisticsSentence Rewriting for Semantic ParsingBo Chen Le Sun Xianpei Han Bo AnState Key Laboratory of Computer SciencesInstitute of Software, Chinese Academy of Sciences, China.
{chenbo, sunle, xianpei, anbo}@nfs.iscas.ac.cnAbstractA major challenge of semantic parsingis the vocabulary mismatch problem be-tween natural language and target ontol-ogy.
In this paper, we propose a sen-tence rewriting based semantic parsingmethod, which can effectively resolve themismatch problem by rewriting a sentenceinto a new form which has the same struc-ture with its target logical form.
Specifi-cally, we propose two sentence-rewritingmethods for two common types of mis-match: a dictionary-based method for 1-N mismatch and a template-based methodfor N-1 mismatch.
We evaluate our sen-tence rewriting based semantic parser onthe benchmark semantic parsing dataset ?WEBQUESTIONS.
Experimental resultsshow that our system outperforms the basesystem with a 3.4% gain in F1, and gen-erates logical forms more accurately andparses sentences more robustly.1 IntroductionSemantic parsing is the task of mapping natu-ral language sentences into logical forms whichcan be executed on a knowledge base (Zelle andMooney, 1996; Zettlemoyer and Collins, 2005;Kate and Mooney, 2006; Wong and Mooney,2007; Lu et al, 2008; Kwiatkowksi et al, 2010).Figure 1 shows an example of semantic parsing.Semantic parsing is a fundamental technique ofnatural language understanding, and has been usedin many applications, such as question answering(Liang et al, 2011; He et al, 2014; Zhang et al,2016) and information extraction (Krishnamurthyand Mitchell, 2012; Choi et al, 2015; Parikh et al,2015).Semantic parsing, however, is a challengingSentence:     What is the capital of Germany?Logical form: ?x.capital(Germany,x)Result:             {Berlin}(Semantic parsing)KB(Execution)Figure 1: An example of semantic parsing.task.
Due to the variety of natural language ex-pressions, the same meaning can be expressed us-ing different sentences.
Furthermore, because log-ical forms depend on the vocabulary of target-ontology, a sentence will be parsed into differentlogical forms when using different ontologies.
Forexample, in below the two sentences s1and s2express the same meaning, and they both can beparsed into the two different logical forms lf1andlf2using different ontologies.s1What is the population of Berlin?s2How many people live in Berlin?lf1?x.population(Berlin,x)lf2count(?x.person(x)?live(x,Berlin))Based on the above observations, one majorchallenge of semantic parsing is the structural mis-match between a natural language sentence andits target logical form, which are mainly raisedby the vocabulary mismatch between natural lan-guage and ontologies.
Intuitively, if a sentence hasthe same structure with its target logical form, it iseasy to get the correct parse, e.g., a semantic parsercan easily parse s1into lf1and s2into lf2.
Onthe contrary, it is difficult to parse a sentence intoits logic form when they have different structures,e.g., s1?
lf2or s2?
lf1.To resolve the vocabulary mismatch problem,766(a) An example using traditional methods0: What is the name of Sonia Gandhis daughter?l0: ?x.child(S.G.,x)r0: {Rahul Gandhi (Wrong answer), Priyanka Vadra}(b) An example using our methods0: What is the name of Sonia Gandhis daughter?s1: What is the name of Sonia Gandhis female child?l1: ?x.child(S.G.,x)?gender(x,female)r1: {Priyanka Vadra}Table 1: Examples of (a) sentences s0, possiblelogical form l0from traditional semantic parser,result r0for the logical form l0; (b) possible sen-tence s1from rewriting for the original sentences0, possible logical form l1for sentence s1, resultr1for l1.
Rahul Gandhi is a wrong answer, as heis the son of Sonia Gandhi.this paper proposes a sentence rewriting approachfor semantic parsing, which can rewrite a sen-tence into a form which will have the same struc-ture with its target logical form.
Table 1 givesan example of our rewriting-based semantic pars-ing method.
In this example, instead of parsingthe sentence ?What is the name of Sonia Gand-his daughter??
into its structurally different log-ical form childOf.S.G.
?gender.femaledirectly, our method will first rewrite the sentenceinto the form ?What is the name of Sonia Gand-his female child?
?, which has the same structurewith its logical form, then our method will getthe logical form by parsing this new form.
Inthis way, the semantic parser can get the correctparse more easily.
For example, the parse obtainedthrough traditional method will result in the wronganswer ?Rahul Gandhi?, because it cannot iden-tify the vocabulary mismatch between ?daughter?and child?female1.
By contrast, by rewriting?daughter?
into ?female child?, our method canresolve this vocabulary mismatch.Specifically, we identify two common types ofvocabulary mismatch in semantic parsing:1.
1-N mismatch: a simple word may corre-spond to a compound formula.
For example,the word ?daughter?
may correspond to thecompound formula child?female.2.
N-1 mismatch: a logical constant may cor-respond to a complicated natural languageexpression, e.g., the formula populationcan be expressed using many phrases such as?how many people?
and ?live in?.1In this paper, we may simplify logical forms for readabil-ity, e.g., female for gender.female.To resolve the above two vocabulary mismatchproblems, this paper proposes two sentence rewrit-ing algorithms: One is a dictionary-based sen-tence rewriting algorithm, which can resolve the1-N mismatch problem by rewriting a word us-ing its explanation in a dictionary.
The otheris a template-based sentence rewriting algorithm,which can resolve the N-1 mismatch problemby rewriting complicated expressions using para-phrase template pairs.Given the generated rewritings of a sentence,we propose a ranking function to jointly choosethe optimal rewriting and the correct logical form,by taking both the rewriting features and the se-mantic parsing features into consideration.We conduct experiments on the benchmarkWEBQUESTIONS dataset (Berant et al, 2013).Experimental results show that our method can ef-fectively resolve the vocabulary mismatch prob-lem and achieve accurate and robust performance.The rest of this paper is organized as follows.Section 2 reviews related work.
Section 3 de-scribes our sentence rewriting method for seman-tic parsing.
Section 4 presents the scoring func-tion which can jointly ranks rewritings and logicalforms.
Section 5 discusses experimental results.Section 6 concludes this paper.2 Related WorkSemantic parsing has attracted considerable re-search attention in recent years.
Generally, se-mantic parsing methods can be categorized intosynchronous context free grammars (SCFG) basedmethods (Wong and Mooney, 2007; Arthur et al,2015; Li et al, 2015), syntactic structure basedmethods (Ge and Mooney, 2009; Reddy et al,2014; Reddy et al, 2016), combinatory categor-ical grammars (CCG) based methods (Zettle-moyer and Collins, 2007; Kwiatkowksi et al,2010; Kwiatkowski et al, 2011; Krishnamurthyand Mitchell, 2014; Wang et al, 2014; Artzi et al,2015), and dependency-based compositional se-mantics (DCS) based methods (Liang et al, 2011;Berant et al, 2013; Berant and Liang, 2014; Be-rant and Liang, 2015; Pasupat and Liang, 2015;Wang et al, 2015).One major challenge of semantic parsing is howto scale to open-domain situation like Freebaseand Web.
A possible solution is to learn lexiconsfrom large amount of web text and a knowledgebase using a distant supervised method (Krishna-767murthy and Mitchell, 2012; Cai and Yates, 2013a;Berant et al, 2013).
Another challenge is how toalleviate the burden of annotation.
A possible so-lution is to employ distant-supervised techniques(Clarke et al, 2010; Liang et al, 2011; Cai andYates, 2013b; Artzi and Zettlemoyer, 2013), orunsupervised techniques (Poon and Domingos,2009; Goldwasser et al, 2011; Poon, 2013).There were also several approaches focusedon the mismatch problem.
Kwiatkowski et al(2013) addressed the ontology mismatch prob-lem (i.e., two ontologies using different vocabu-laries) by first parsing a sentence into a domain-independent underspecified logical form, and thenusing an ontology matching model to transformthis underspecified logical form to the target on-tology.
However, their method is still hard todeal with the 1-N and the N-1 mismatch prob-lems between natural language and target ontolo-gies.
Berant and Liang (2014) addressed the struc-ture mismatch problem between natural languageand ontology by generating a set of canonical ut-terances for each candidate logical form, and thenusing a paraphrasing model to rerank the candi-date logical forms.
Their method addresses mis-match problem in the reranking stage, cannot re-solve the mismatch problem when constructingcandidate logical forms.
Compared with thesetwo methods, we approach the mismatch prob-lem in the parsing stage, which can greatly reducethe difficulty of constructing the correct logicalform, through rewriting sentences into the formswhich will be structurally consistent with their tar-get logic forms.Sentence rewriting (or paraphrase generation)is the task of generating new sentences that havethe same meaning as the original one.
Sentencerewriting has been used in many different tasks,e.g., used in statistical machine translation to re-solve the word order mismatch problem (Collinset al, 2005; He et al, 2015).
To our best knowl-edge, this paper is the first work to apply sentencerewriting for vocabulary mismatch problem in se-mantic parsing.3 Sentence Rewriting for SemanticParsingAs discussed before, the vocabulary mismatch be-tween natural language and target ontology is abig challenge in semantic parsing.
In this section,we describe our sentence rewriting algorithm forWordLogical FormWiktionaryExplanationsonchild?male male childactressactor?femalefemale actorfather parent?male male parentgrandaprentparent?parentparent of one?sparentbrothersibling?malemale siblingTable 2: Several examples of words, their logicalforms and their explanations in Wiktionary.solving the mismatch problem.
Specifically, wesolve the 1-N mismatch problem by dictionary-based rewriting and solve the N-1 mismatch prob-lem by template-based rewriting.
The details areas follows.3.1 Dictionary-based RewritingIn the 1-N mismatch case, a word will correspondto a compound formula, e.g., the target logicalform of the word ?daughter?
is child?female(Table 2 has more examples).To resolve the 1-N mismatch problem, werewrite the original word (?daughter?)
into anexpression (?female child?)
which will havethe same structure with its target logical form(child?female).
In this paper, we rewritewords using their explanations in a dictionary.This is because each word in a dictionary willbe defined by a detailed explanation using sim-ple words, which often will have the same struc-ture with its target formula.
Table 2 shows howthe vocabulary mismatch between a word and itslogical form can be resolved using its dictionaryexplanation.
For instance, the word ?daughter?
isexplained as ?female child?
in Wiktionary, whichhas the same structure as child?female.In most cases, only common nouns will resultin the 1-N mismatch problem.
Therefore, in orderto control the size of rewritings, this paper onlyrewrite the common nouns in a sentence by replac-ing them with their dictionary explanations.
Be-cause a sentence usually will not contain too manycommon nouns, the size of candidate rewritingsis thus controllable.
Given the generated rewrit-ings of a sentence, we propose a sentence selectionmodel to choose the best rewriting using multiplefeatures (See details in Section 4).Table 3 shows an example of the dictionary-based rewriting.
In Table 3, the example sen-tence s contains two common nouns (?name?and ?daughter?
), therefore we will generate threerewritings r1, r2and r3.
Among these rewritings,768s : What is the name of Sonia Gandhis daughter?r1: What is the reputation of Sonia Gandhis daughter?r2: What is the name of Sonia Gandhis female child?r3: What is the reputation of Sonia Gandhis female child?Table 3: An example of the dictionary-based sen-tence rewriting.the candidate rewriting r2is what we expected,as it has the same structure with the target logicalform and doesn?t bring extra noise (i.e., replacing?name?
with its explanation ?reputation?
).For the dictionary used in rewriting, this paperuses Wiktionary.
Specifically, given a word, weuse its ?Translations?
part in the Wiktionary as itsexplanation.
Because most of the 1-N mismatchare caused by common nouns, we only collect theexplanations of common nouns.
Furthermore, forpolysomic words which have several explanations,we only use their most common explanations.
Be-sides, we ignore explanations whose length arelonger than 5.3.2 Template-based RewritingIn the N-1 mismatch case, a complicated natu-ral language expression will be mapped to a sin-gle logical constant.
For example, considering thefollowing mapping from the natural language sen-tence s to its logical form lf based on Freebaseontology:s: How many people live in Berlin?lf : ?x.population(Berlin,x)where the three words: ?how many?
(count),?people?
(people) and ?live in?
(live) willmap to the predicate population together.
Ta-ble 4 shows more N-1 examples.Expression Logical constanthow many, people, live inpopulationhow many, people, visit, annuallyannual-visitwhat money, usecurrencywhat school, go toeducationwhat language, speak, officiallyofficial-languageTable 4: Several N-1 mismatch examples.To resolve the N-1 mismatch problem, we pro-pose a template rewriting algorithm, which canrewrite a complicated expression into its sim-pler form.
Specifically, we rewrite sentencesbased on a set of paraphrase template pairs P ={(ti1, ti2)|i = 1, 2, ..., n}, where each template tTemplate 1 Template 2How many people live in $yWhat is the population of$yWhat money in $y is used What is the currency of $yWhat school did $y go toWhat is the education of$yWhat language does $yspeak officiallyWhat is the officiallanguage of $yTable 5: Several examples of paraphrase templatepairs.is a sentence with an argument slot $y, and ti1andti2are paraphrases.
In this paper, we only con-sider single-slot templates.
Table 5 shows severalparaphrase template pairs.Given the template pair database and a sentence,our template-based rewriting algorithm works asfollows:1.
Firstly, we generate a set of candidate tem-plates ST = {st1, st2, ..., stn} of the sen-tence by replacing each named entity withinit by ?$y?.
For example, we will gener-ate template ?How many people live in $y?from the sentence ?How many people live inBerlin?.2.
Secondly, using the paraphrase template pairdatabase, we retrieve all possible rewritingtemplate pairs (t1, t2) with t1?
ST , e.g., wecan retrieve template pair (?How many peo-ple live there in $y?, ?What is the populationof $y?
for t2) using the above ST .3.
Finally, we get the rewritings by replacingthe argument slot ?$y?
in template t2withthe corresponding named entity.
For exam-ple, we get a new candidate sentence ?Whatis the population of Berlin?
by replacing?$y?
in t2with Berlin.
In this way wecan get the rewriting we expected, since thisrewriting will match its target logical formpopulation(Berlin).To control the size and measure the quality ofrewritings using a specific template pair, we alsodefine several features and the similarity betweentemplate pairs (See Section 4 for details).To build the paraphrase template pair database,we employ the method described in Fader et al(2014) to automatically collect paraphrase tem-plate pairs.
Specifically, we use the WikiAnswersparaphrase corpus (Fader et al, 2013), which con-tains 23 million question-clusters, and all ques-769How many people live in chembakolli?How many people is in chembakolli?How many people live in chembakolli india?How many people live there chembakolli?How many people live there in chembakolli?What is the population of Chembakolli india?What currency is used on St Lucia?What is st lucia money?What is the money used in st lucia?What kind of money did st lucia have?What money do st Lucia use?Which money is used in St Lucia?Table 6: Two paraphrase clusters from theWikiAnswers corpus.tions in the same cluster express the same mean-ing.
Table 6 shows two paraphrase clusters fromthe WikiAnswers corpus.
To build paraphrasetemplate pairs, we first replace the shared nounwords in each cluster with the placeholder ?$y?,then each two templates in a cluster will form aparaphrase template pair.
To filter out noisy tem-plate pairs, we only retain salient paraphrase tem-plate pairs whose co-occurrence count is largerthan 3.4 Sentence Rewriting based SemanticParsingIn this section we describe our semantic rewritingbased semantic parsing system.
Figure 2 presentsthe framework of our system.
Given a sentence,we first rewrite it into a set of new sentences, thenwe generate candidate logical forms for each newsentence using a base semantic parser, finally wescore all logical forms using a scoring functionand output the best logical form as the final result.In following, we first introduce the used base se-mantic parser, then we describe the proposed scor-ing function.4.1 Base Semantic ParserIn this paper, we produce logical forms for eachsentence rewritings using an agenda-based seman-tic parser (Berant and Liang, 2015), which isbased on the lambda-DCS proposed by Liang(2013).
For parsing, we use the lexicons and thegrammars released by Berant et al (2013), wherelexicons are used to trigger unary and binary pred-icates, and grammars are used to conduct logicalforms.
The only difference is that we also use thecomposition rule to make the parser can handlecomplicated questions involving two binary pred-icates, e.g., child.obama?gender.female.Original sentenceNew sentencesLogical formsResults(Sentence rewriting)(Semantic parsing)(Executing)Figure 2: The framework of our sentence rewritingbased semantic parsing.For model learning and sentence parsing, thebase semantic parser learned a scoring functionby modeling the policy as a log-linear distributionover (partial) agenda derivations Q:p?
(a|s) =exp{?(a)T?)}?a??Aexp{?(a?)T?
)}(1)The policy parameters are updated as follows:?
?
?
+ ?R(htarget)?Tt=1?
(htarget) (2)?t(h) = ?
?log p?
(at|st)= ?(at)?
Ep?(a?t|st)[?
(a?t)](3)The reward function R(h) measures the compati-bility of the resulting derivation, and ?
is the learn-ing rate which is set using the AdaGrad algorithm(Duchi et al, 2011).
The target history htargetisgenerated from the root derivation d?with highestreward out of the K (beam size) root derivations,using local reweighting and history compression.4.2 Scoring FunctionTo select the best semantic parse, we propose ascoring function which can take both sentencerewriting features and semantic parsing featuresinto consideration.
Given a sentence x, a gener-ated rewriting x?and the derivation d of x?, wescore them using follow function:score(x, x?, d) = ?
?
?
(x, x?, d)= ?1?
?
(x, x?)
+ ?2?
?
(x?, d)This scoring function is decomposed into twoparts: one for sentence rewriting ?
?1?
?
(x, x?
)and the other for semantic parsing ?
?2?
?
(x?, d).Following Berant and Liang (2015), we update theparameters ?2of semantic parsing features as the770Input: Q/A pairs {(xi, yi) : i = 1...n}; KnowledgebaseK; Number of sentencesN ; Number of iterations T .Definitions: The function REWRITING(xi) returnsa set of candidate sentences by applying sentencerewriting on sentence x; PARSE(p?, x) parses thesentence x based on current parameters ?, using agenda-based parsing; CHOOSEORACLE(h0) choosesthe derivation with highest reward from the root ofh0; CHOOSEORACLE(Htarget) chooses thederivation with highest reward from a set of derivations.CHOOSEORACLE(h?target) chooses the new sen-tence that results in derivation with highest reward.Algorithm:?1?
0, ?2?
0for t = 1...T , i = 1...N :X = REWRITING(xi)for each x?i?
X :h0?
PARSE(p?, x?i)d??
CHOOSEORACLE(h0)htarget?
PARSE(p+cw?, x?i)h?target?
CHOOSEORACLE(Htarget)x??i?
CHOOSEORACLE(h?target)?2?
?2+ ?R(h?target)?Tt=1?(h?target)?1?
?1+ ?R(h?target)?
(xi, x?
?i)Output: Estimated parameters ?1and ?2.Table 7: Our learning algorithm for parameter es-timation from question-answer pairs.same as (2).
Similarly, the parameters ?1of sen-tence rewriting features are updated as follows:?1?
?1+ ?R(h?target)?
(x, x??)?
(x, x??)
= ?
log p?1(x?
?|x)= ?
(x, x??)?
Ep?1(x?|x)[?
(x, x?
)]where the learning rate ?
is set using the same al-gorithm in Formula (2).4.3 Parameter Learning AlgorithmTo estimate the parameters ?1and ?2, our learn-ing algorithm uses a set of question-answer pairs(xi, yi).
Following Berant and Liang (2015), ourupdates for ?1and ?2do not maximize reward northe log-likelihood.
However, the reward providesa way to modulate the magnitude of the updates.Specifically, after each update, our model resultsin making the derivation, which has the highest re-ward, to get a bigger score.
Table 7 presents ourlearning algorithm.4.4 FeaturesAs described in Section 4.3, our model uses twokinds of features.
One for the semantic parsingmodule which are simply the same features de-scribed in Berant and Liang (2015).
One for thesentence rewriting module these features are de-fined over the original sentence, the generated sen-tence rewritings and the final derivations:Features for dictionary-based rewriting.
Givena sentence s0, when the new sentence s1is gener-ated by replacing a word to its explanation w ?ex, we will generate four features: The first fea-ture indicates the word replaced.
The second fea-ture indicates the replacement w ?
ex we used.The final two features are the POS tags of the leftword and the right word of w in s0.Features for template-based rewriting.
Given asentence s0, when the new sentence s1is gener-ated through a template based rewriting t1?
t2,we generate four features: The first feature indi-cates the template pair (t1, t2) we used.
The sec-ond feature is the similarity between the sentences0and the template t1, which is calculated usingthe word overlap between s0and t1.
The thirdfeature is the compatibility of the template pair,which is the pointwise mutual information (PMI)between t1and t2in the WikiAnswers corpus.
Thefinal feature is triggered when the target logicalform only contains an atomic formula (or predi-cate), and this feature indicates the mapping fromtemplate t2to the predicate p.5 ExperimentsIn this section, we assess our method and compareit with other methods.5.1 Experimental SettingsDataset: We evaluate all systems on the bench-mark WEBQUESTIONS dataset (Berant et al,2013), which contains 5,810 question-answerpairs.
All questions are collected by crawlingthe Google Suggest API, and their answers areobtained using Amazon Mechanical Turk.
Thisdataset covers several popular topics and its ques-tions are commonly asked on the web.
Accordingto Yao (2015), 85% of questions can be answeredby predicting a single binary relation.
In our ex-periments, we use the standard train-test split (Be-rant et al, 2013), i.e., 3,778 questions (65%) fortraining and 2,032 questions (35%) for testing, anddivide the training set into 3 random 80%-20%splits for development.Furthermore, to verify the effectiveness of ourmethod on solving the vocabulary mismatch prob-lem, we manually select 50 mismatch test exam-ples from the WEBQUESTIONS dataset, where771all sentences have different structure with their tar-get logical forms, e.g., ?Who is keyshia cole dad?
?and ?What countries have german as the officiallanguage?
?.System Settings: In our experiments, we usethe Freebase Search API for entity lookup.
Weload Freebase using Virtuoso, and execute logicalforms by converting them to SPARQL and query-ing using Virtuoso.
We learn the parameters of oursystem by making three passes over the trainingdataset, with the beam size K = 200, the dictio-nary rewriting size KD= 100, and the templaterewriting size KT= 100.Baselines: We compare our method with sev-eral traditional systems, including semantic pars-ing based systems (Berant et al, 2013; Berant andLiang, 2014; Berant and Liang, 2015; Yih et al,2015), information extraction based systems (Yaoand Van Durme, 2014; Yao, 2015), machine trans-lation based systems (Bao et al, 2014), embed-ding based systems (Bordes et al, 2014; Yang etal., 2014), and QA based system (Bast and Hauss-mann, 2015).Evaluation: Following previous work (Berant etal., 2013), we evaluate different systems using thefraction of correctly answered questions.
Becausegolden answers may have multiple values, we usethe average F1 score as the main evaluation metric.5.2 Experimental ResultsTable 8 provides the performance of all base-linesand our method.
We can see that:1.
Our method achieved competitive perfor-mance: Our system outperforms all baselinesand get the best F1-measure of 53.1 on WE-BQUESTIONS dataset.2.
Sentence rewriting is a promising techniquefor semantic parsing: By employing sen-tence rewriting, our system gains a 3.4% F1improvement over the base system we used(Berant and Liang, 2015).3.
Compared to all baselines, our system getsthe highest precision.
This result indicatesthat our parser can generate more-accuratelogical forms by sentence rewriting.
Our sys-tem also achieves the second highest recall,which is a competitive performance.
Interest-ingly, both the two systems with the highestrecall (Bast and Haussmann, 2015; Yih et al,System Prec.
Rec.
F1 (avg)Berant et al, 2013 48.0 41.3 35.7Yao and Van-Durme, 2014 51.7 45.8 33.0Berant and Liang, 2014 40.5 46.6 39.9Bao et al, 2014 ?
?
37.5Bordes et al, 2014a ?
?
39.2Yang et al, 2014 ?
?
41.3Bast and Haussmann, 2015 49.8 60.4 49.4Yao, 2015 52.6 54.5 44.3Berant and Liang, 2015 50.5 55.7 49.7Yih et al, 2015 52.8 60.7 52.5Our approach 53.7 60.0 53.1Table 8: The results of our system and recentlypublished systems.
The results of other systemsare from either original papers or the standardevaluation web.2015) rely on extra-techniques such as entitylinking and relation matching.The effectiveness on mismatch problem.
To an-alyze the commonness of mismatch problem insemantic parsing, we randomly sample 500 ques-tions from the training data and do manually anal-ysis, we found that 12.2% out of the sampled ques-tions have mismatch problems: 3.8% out of themhave 1-N mismatch problem and 8.4% out of themhave N-1 mismatch problem.To verify the effectiveness of our method onsolving the mismatch problem, we conduct experi-ments on the 50 mismatch test examples and Table9 shows the performance.
We can see that our sys-tem can effectively resolve the mismatch betweennatural language and target ontology: compared tothe base system, our system achieves a significant54.5% F1 im-provement.SystemPrec.
Rec.F1 (avg)Base system31.4 43.9 29.4Our system83.3 92.3 83.9Table 9: The results on the 50 mismatch testdataset.When scaling a semantic parser to open-domainsituation or web situation, the mismatch problemwill be more common as the ontology and lan-guage complexity increases (Kwiatkowski et al,2013).
Therefore we believe the sentence rewrit-ing method proposed in this paper is an importanttechnique for the scalability of semantic parser.The effect of different rewriting algorithms.To analyze the contribution of different rewritingmethods, we perform experiments using differentsentence rewriting methods and the results are pre-sented in Table 10.
We can see that:772Method Prec.
Rec.F1 (avg)base 49.8 55.3 49.1+ dictionary SR (only)51.6 57.5 50.9+ template SR (only)52.9 59.0 52.3+ both 53.7 60.0 53.1Table 10: The results of the base system and oursystems on the 2032 test questions.1.
Both sentence rewriting methods improvedthe parsing performance, they resulted in1.8% and 3.2% F1 improvements respec-tively2.2.
Compared with the dictionary-based rewrit-ing method, the template-based rewritingmethod can achieve higher performance im-provement.
We believe this is because N-1mismatch problem is more common in theWEBQUESTIONS dataset.3.
The two rewriting methods are good comple-mentary of each other.
The semantic parsercan achieve a higher performance improve-ment when using these two rewriting meth-ods together.The effect on improving robustness.
We foundthat the template-based rewriting method cangreatly improve the robustness of the base se-mantic parser.
Specially, the template-basedmethod can rewrite similar sentences into auniform template, and the (template, predi-cate) feature can provide additional informa-tion to reduce the uncertainty during parsing.For example, using only the uncertain align-ments from the words ?people?
and ?speak?to the two predicates official languageand language spoken, the base parser willparse the sentence ?What does jamaican peo-ple speak??
into the incorrect logical formofficial language.jamaican in our ex-periments, rather than into the correct formlanguage spoken.jamaican (See the finalexample in Table 11).
By exploiting the alignmentfrom the template ?what language does $y peoplespeak?
to the predicate , our system can parse theabove sentence correctly.The effect on OOV problem.
We found that thesentence rewriting method can also provide extra2Our base system yields a slight drop in accuracy com-pared to the original system (Berant and Liang, 2015), as weparallelize the learning algorithm, and the order of the datafor updating the parameter is different to theirs.O Who is willow smith mom name?RWho is willow smith female parent name?LFparentOf.willow smith?gender.femaleOWho was king henry viii son?RWho was king henry viii male child?LFchildOf.king henry?gender.maleOWhat are some of the traditions of islam?RWhat is of the religion of islam?LFreligionOf.islamOWhat does jamaican people speak?RWhat language does jamaican people speak?LFlanguage spoken.jamaicaTable 11: Examples which our system generatesmore accurate logical form than the base seman-tic parser.
O is the original sentence; R is thegenerated sentence from sentence rewriting (withthe highest score for the model, including rewrit-ing part and parsing part); LF is the target logicalform.profit for solving the OOV problem.
Traditionally,if a sentence contains a word which is not coveredby the lexicon, it will cannot be correctly parsed.However, with the help of sentence rewriting, wemay rewrite the OOV words into the words whichare covered by our lexicons.
For example, in Table11 the 3rd question ?What are some of the tradi-tions of islam??
cannot be correctly parsed as thelexicons dont cover the word ?tradition?.
Throughsentence rewriting, we can generate a new sen-tence ?What is of the religion of islam?
?, whereall words are covered by the lexicons, in this waythe sentence can be correctly parsed.5.3 Error AnalysisTo better understand our system, we conduct er-ror analysis on the parse results.
Specifically, werandomly choose 100 questions which are not cor-rectly answered by our system.
We found that theerrors are mainly raised by following four reasons(See Table 12 for detail):Reason#(Ratio)Sample ExampleLabel issue 38What band was george clintonin?N-ary predi-cate(n > 2)31What year did the seahawkswin the superbowl?Temporalclause15Who was the leader of the usduring wwii?Superlative8Who was the first governor ofcolonial south carolina?Others 8What is arkansas statecapitol?Table 12: The main reasons of parsing errors, theratio and an example for each reason are also pro-vided.773The first reason is the label issue.
The main la-bel issue is incompleteness, i.e., the answers of aquestion may not be labeled completely.
For ex-ample, for the question ?Who does nolan ryan playfor?
?, our system returns 4 correct teams but thegolden answer only contain 2 teams.
One anotherlabel issue is the error labels.
For example, thegold answer of the question ?What state is barackobama from??
is labeled as ?Illinois?, however,the correct answer is ?Hawaii?.The second reason is the n-ary predicate prob-lem (n > 2).
Currently, it is hard for a parserto conduct the correct logical form of n-ary pred-icates.
For example, the question ?What year didthe seahawks win the superbowl??
describes an n-ary championship event, which gives the champi-onship and the champion of the event, and expectsthe season.
We believe that more research atten-tions should be given on complicated cases, suchas the n-ary predicates parsing.The third reason is temporal clause.
For ex-ample, the question ?Who did nasri play for be-fore arsenal??
contains a temporal clause ?be-fore?.
We found temporal clause is complicatedand makes it strenuous for the parser to understandthe sentence.The fourth reason is superlative case, which isa hard problem in semantic parsing.
For example,to answer ?What was the name of henry viii firstwife?
?, we should choose the first one from a listordering by time.
Unfortunately, it is difficult forthe current parser to decide what to be ordered andhow to order.There are also many other miscellaneous errorcases, such as spelling error in the question, e.g.,?capitol?
for ?capital?, ?mary?
for ?marry?.6 ConclusionsIn this paper, we present a novel semantic pars-ing method, which can effectively deal with themismatch between natural language and target on-tology using sentence rewriting.
We resolve twocommon types of mismatch (i) one word in natu-ral language sentence vs one compound formula intarget ontology (1-N), (ii) one complicated expres-sion in natural language sentence vs one formulain target ontology (N-1).
Then we present two sen-tence rewriting methods, dictionary-based methodfor 1-N mismatch and template-based method forN-1 mismatch.
The resulting system significantlyoutperforms the base system on the WEBQUES-TIONS dataset.Currently, our approach only leverages sim-ple sentence rewriting methods.
In future work,we will explore more advanced sentence rewrit-ing methods.
Furthermore, we also want to em-ploy sentence rewriting techniques for other chal-lenges in semantic parsing, such as the sponta-neous, unedited natural language input, etc.AcknowledgmentsWe sincerely thank the reviewers for their valu-able comments and suggestions.
This work is sup-ported by the National High Technology Devel-opment 863 Program of China under Grants no.2015AA015405, and the National Natural ScienceFoundation of China under Grants no.
61433015,612722324 and 61572477.ReferencesPhilip Arthur, Graham Neubig, Sakriani Sakti, TomokiToda, and Satoshi Nakamura.
2015.
Semantic pars-ing of ambiguous input through paraphrasing andverification.
Transactions of the Association forComputational Linguistics, 3:571?584.Yoav Artzi and Luke Zettlemoyer.
2013.
Weakly su-pervised learning of semantic parsers for mappinginstructions to actions.
Transactions of the Associa-tion for Computational Linguistics, 1(1):49?62.Yoav Artzi, Kenton Lee, and Luke Zettlemoyer.
2015.Broad-coverage ccg semantic parsing with amr.
InProceedings of the 2015 Conference on EmpiricalMethods in Natural Language Processing, pages1699?1710, Lisbon, Portugal, September.
Associa-tion for Computational Linguistics.Junwei Bao, Nan Duan, Ming Zhou, and Tiejun Zhao.2014.
Knowledge-based question answering as ma-chine translation.
In Proceedings of the 52nd An-nual Meeting of the Association for ComputationalLinguistics (Volume 1: Long Papers), pages 967?976, Baltimore, Maryland, June.
Association forComputational Linguistics.Hannah Bast and Elmar Haussmann.
2015.
More ac-curate question answering on freebase.
In Proceed-ings of the 24th ACM International on Conferenceon Information and Knowledge Management, CIKM2015, Melbourne, VIC, Australia, October 19 - 23,2015, pages 1431?1440.Jonathan Berant and Percy Liang.
2014.
Seman-tic parsing via paraphrasing.
In Proceedings of the52nd Annual Meeting of the Association for Compu-tational Linguistics (Volume 1: Long Papers), pages1415?1425, Baltimore, Maryland, June.
Associationfor Computational Linguistics.774Jonathan Berant and Percy Liang.
2015.
Imitationlearning of agenda-based semantic parsers.
Trans-actions of the Association for Computational Lin-guistics, 3:545?558.Jonathan Berant, Andrew Chou, Roy Frostig, and PercyLiang.
2013.
Semantic parsing on Freebase fromquestion-answer pairs.
In Proceedings of the 2013Conference on Empirical Methods in Natural Lan-guage Processing, pages 1533?1544, Seattle, Wash-ington, USA, October.
Association for Computa-tional Linguistics.Antoine Bordes, Sumit Chopra, and Jason Weston.2014.
Question answering with subgraph embed-dings.
In Proceedings of the 2014 Conference onEmpirical Methods in Natural Language Process-ing (EMNLP), pages 615?620, Doha, Qatar, Octo-ber.
Association for Computational Linguistics.Qingqing Cai and Alexander Yates.
2013a.
Large-scale semantic parsing via schema matching and lex-icon extension.
In Proceedings of the 51st AnnualMeeting of the Association for Computational Lin-guistics (Volume 1: Long Papers), pages 423?433,Sofia, Bulgaria, August.
Association for Computa-tional Linguistics.Qingqing Cai and Alexander Yates.
2013b.
Seman-tic parsing freebase: Towards open-domain seman-tic parsing.
In Second Joint Conference on Lexicaland Computational Semantics (*SEM), Volume 1:Proceedings of the Main Conference and the SharedTask: Semantic Textual Similarity, pages 328?338,Atlanta, Georgia, USA, June.
Association for Com-putational Linguistics.Eunsol Choi, Tom Kwiatkowski, and Luke Zettle-moyer.
2015.
Scalable semantic parsing with partialontologies.
In Proceedings of the 53rd Annual Meet-ing of the Association for Computational Linguisticsand the 7th International Joint Conference on Natu-ral Language Processing (Volume 1: Long Papers),pages 1311?1320, Beijing, China, July.
Associationfor Computational Linguistics.James Clarke, Dan Goldwasser, Ming-Wei Chang, andDan Roth.
2010.
Driving semantic parsing fromthe world?s response.
In Proceedings of the Four-teenth Conference on Computational Natural Lan-guage Learning, pages 18?27, Uppsala, Sweden,July.
Association for Computational Linguistics.Michael Collins, Philipp Koehn, and Ivona Kucerova.2005.
Clause restructuring for statistical machinetranslation.
In Proceedings of the 43rd AnnualMeeting of the Association for Computational Lin-guistics (ACL?05), pages 531?540, Ann Arbor,Michigan, June.
Association for Computational Lin-guistics.John Duchi, Elad Hazan, and Yoram Singer.
2011.Adaptive subgradient methods for online learningand stochastic optimization.
J. Mach.
Learn.
Res.,12:2121?2159, July.Anthony Fader, Luke Zettlemoyer, and Oren Etzioni.2013.
Paraphrase-driven learning for open questionanswering.
In Proceedings of the 51st Annual Meet-ing of the Association for Computational Linguis-tics (Volume 1: Long Papers), pages 1608?1618,Sofia, Bulgaria, August.
Association for Computa-tional Linguistics.Anthony Fader, Luke Zettlemoyer, and Oren Etzioni.2014.
Open question answering over curated andextracted knowledge bases.
In Proceedings of the20th ACM SIGKDD International Conference onKnowledge Discovery and Data Mining, KDD ?14,pages 1156?1165, New York, NY, USA.
ACM.Ruifang Ge and Raymond Mooney.
2009.
Learninga compositional semantic parser using an existingsyntactic parser.
In Proceedings of the Joint Confer-ence of the 47th Annual Meeting of the ACL and the4th International Joint Conference on Natural Lan-guage Processing of the AFNLP, pages 611?619,Suntec, Singapore, August.
Association for Compu-tational Linguistics.Dan Goldwasser, Roi Reichart, James Clarke, and DanRoth.
2011.
Confidence driven unsupervised se-mantic parsing.
In Proceedings of the 49th An-nual Meeting of the Association for ComputationalLinguistics: Human Language Technologies, pages1486?1495, Portland, Oregon, USA, June.
Associa-tion for Computational Linguistics.Shizhu He, Kang Liu, Yuanzhe Zhang, Liheng Xu, andJun Zhao.
2014.
Question answering over linkeddata using first-order logic.
In Proceedings of the2014 Conference on Empirical Methods in Natu-ral Language Processing (EMNLP), pages 1092?1103, Doha, Qatar, October.
Association for Com-putational Linguistics.He He, Alvin Grissom II, John Morgan, Jordan Boyd-Graber, and Hal Daum?e III.
2015.
Syntax-basedrewriting for simultaneous machine translation.
InProceedings of the 2015 Conference on EmpiricalMethods in Natural Language Processing, pages55?64, Lisbon, Portugal, September.
Association forComputational Linguistics.Rohit J. Kate and Raymond J. Mooney.
2006.
Us-ing string-kernels for learning semantic parsers.
InProceedings of the 21st International Conference onComputational Linguistics and 44th Annual Meet-ing of the Association for Computational Linguis-tics, pages 913?920, Sydney, Australia, July.
Asso-ciation for Computational Linguistics.Jayant Krishnamurthy and Tom Mitchell.
2012.Weakly supervised training of semantic parsers.
InProceedings of the 2012 Joint Conference on Empir-ical Methods in Natural Language Processing andComputational Natural Language Learning, pages754?765, Jeju Island, Korea, July.
Association forComputational Linguistics.775Jayant Krishnamurthy and Tom M. Mitchell.
2014.Joint syntactic and semantic parsing with combi-natory categorial grammar.
In Proceedings of the52nd Annual Meeting of the Association for Compu-tational Linguistics (Volume 1: Long Papers), pages1188?1198, Baltimore, Maryland, June.
Associationfor Computational Linguistics.Tom Kwiatkowksi, Luke Zettlemoyer, Sharon Goldwa-ter, and Mark Steedman.
2010.
Inducing probabilis-tic CCG grammars from logical form with higher-order unification.
In Proceedings of the 2010 Con-ference on Empirical Methods in Natural LanguageProcessing, pages 1223?1233, Cambridge, MA, Oc-tober.
Association for Computational Linguistics.Tom Kwiatkowski, Luke Zettlemoyer, Sharon Goldwa-ter, and Mark Steedman.
2011.
Lexical generaliza-tion in ccg grammar induction for semantic parsing.In Proceedings of the 2011 Conference on Empiri-cal Methods in Natural Language Processing, pages1512?1523, Edinburgh, Scotland, UK., July.
Asso-ciation for Computational Linguistics.Tom Kwiatkowski, Eunsol Choi, Yoav Artzi, and LukeZettlemoyer.
2013.
Scaling semantic parsers withon-the-fly ontology matching.
In Proceedings ofthe 2013 Conference on Empirical Methods in Natu-ral Language Processing, pages 1545?1556, Seattle,Washington, USA, October.
Association for Compu-tational Linguistics.Junhui Li, Muhua Zhu, Wei Lu, and Guodong Zhou.2015.
Improving semantic parsing with enrichedsynchronous context-free grammar.
In Proceed-ings of the 2015 Conference on Empirical Methodsin Natural Language Processing, pages 1455?1465,Lisbon, Portugal, September.
Association for Com-putational Linguistics.Percy Liang, Michael Jordan, and Dan Klein.
2011.Learning dependency-based compositional seman-tics.
In Proceedings of the 49th Annual Meeting ofthe Association for Computational Linguistics: Hu-man Language Technologies, pages 590?599, Port-land, Oregon, USA, June.
Association for Computa-tional Linguistics.P.
Liang.
2013.
Lambda dependency-based composi-tional semantics.
arXiv preprint arXiv:1309.4408.Wei Lu, Hwee Tou Ng, Wee Sun Lee, and Luke S.Zettlemoyer.
2008.
A generative model for pars-ing natural language to meaning representations.
InProceedings of the 2008 Conference on EmpiricalMethods in Natural Language Processing, pages783?792, Honolulu, Hawaii, October.
Associationfor Computational Linguistics.Ankur P. Parikh, Hoifung Poon, and KristinaToutanova.
2015.
Grounded semantic parsing forcomplex knowledge extraction.
In Proceedings ofthe 2015 Conference of the North American Chap-ter of the Association for Computational Linguis-tics: Human Language Technologies, pages 756?766, Denver, Colorado, May?June.
Association forComputational Linguistics.P.
Pasupat and P. Liang.
2015.
Compositional semanticparsing on semi-structured tables.
In Association forComputational Linguistics (ACL).Hoifung Poon and Pedro Domingos.
2009.
Unsuper-vised semantic parsing.
In Proceedings of the 2009Conference on Empirical Methods in Natural Lan-guage Processing, pages 1?10, Singapore, August.Association for Computational Linguistics.Hoifung Poon.
2013.
Grounded unsupervised se-mantic parsing.
In Proceedings of the 51st AnnualMeeting of the Association for Computational Lin-guistics (Volume 1: Long Papers), pages 933?943,Sofia, Bulgaria, August.
Association for Computa-tional Linguistics.Siva Reddy, Mirella Lapata, and Mark Steedman.2014.
Large-scale semantic parsing withoutquestion-answer pairs.
Transactions of the Associ-ation for Computational Linguistics, 2:377?392.Siva Reddy, Oscar T?ackstr?om, Michael Collins, TomKwiatkowski, Dipanjan Das, Mark Steedman, andMirella Lapata.
2016.
Transforming DependencyStructures to Logical Forms for Semantic Parsing.Transactions of the Association for ComputationalLinguistics, 4:127?140.Adrienne Wang, Tom Kwiatkowski, and Luke Zettle-moyer.
2014.
Morpho-syntactic lexical generaliza-tion for ccg semantic parsing.
In Proceedings ofthe 2014 Conference on Empirical Methods in Nat-ural Language Processing (EMNLP), pages 1284?1295, Doha, Qatar, October.
Association for Com-putational Linguistics.Yushi Wang, Jonathan Berant, and Percy Liang.
2015.Building a semantic parser overnight.
In Proceed-ings of the 53rd Annual Meeting of the Associationfor Computational Linguistics and the 7th Interna-tional Joint Conference on Natural Language Pro-cessing (Volume 1: Long Papers), pages 1332?1342,Beijing, China, July.
Association for ComputationalLinguistics.Yuk Wah Wong and Raymond Mooney.
2007.
Learn-ing synchronous grammars for semantic parsingwith lambda calculus.
In Proceedings of the 45thAnnual Meeting of the Association of ComputationalLinguistics, pages 960?967, Prague, Czech Repub-lic, June.
Association for Computational Linguis-tics.Min-Chul Yang, Nan Duan, Ming Zhou, and Hae-Chang Rim.
2014.
Joint relational embeddings forknowledge-based question answering.
In Proceed-ings of the 2014 Conference on Empirical Methodsin Natural Language Processing (EMNLP), pages645?650, Doha, Qatar, October.
Association forComputational Linguistics.776Xuchen Yao and Benjamin Van Durme.
2014.
Infor-mation extraction over structured data: Question an-swering with freebase.
In Proceedings of the 52ndAnnual Meeting of the Association for Computa-tional Linguistics (Volume 1: Long Papers), pages956?966, Baltimore, Maryland, June.
Associationfor Computational Linguistics.Xuchen Yao.
2015.
Lean question answering overfreebase from scratch.
In Proceedings of the 2015Conference of the North American Chapter of theAssociation for Computational Linguistics: Demon-strations, pages 66?70, Denver, Colorado, June.
As-sociation for Computational Linguistics.Wen-tau Yih, Ming-Wei Chang, Xiaodong He, andJianfeng Gao.
2015.
Semantic parsing via stagedquery graph generation: Question answering withknowledge base.
In Proceedings of the 53rd AnnualMeeting of the Association for Computational Lin-guistics and the 7th International Joint Conferenceon Natural Language Processing (Volume 1: LongPapers), pages 1321?1331, Beijing, China, July.
As-sociation for Computational Linguistics.John M. Zelle and Raymond J. Mooney.
1996.
Learn-ing to parse database queries using inductive logicprogramming.
In AAAI/IAAI, pages 1050?1055,Portland, OR, August.
AAAI Press/MIT Press.Luke S. Zettlemoyer and Michael Collins.
2005.Learning to map sentences to logical form: Struc-tured classification with probabilistic categorialgrammars.
In UAI ?05, Proceedings of the 21st Con-ference in Uncertainty in Artificial Intelligence, Ed-inburgh, Scotland, July 26-29, 2005, pages 658?666.Luke Zettlemoyer and Michael Collins.
2007.
On-line learning of relaxed CCG grammars for parsingto logical form.
In Proceedings of the 2007 JointConference on Empirical Methods in Natural Lan-guage Processing and Computational Natural Lan-guage Learning (EMNLP-CoNLL), pages 678?687,Prague, Czech Republic, June.
Association for Com-putational Linguistics.Yuanzhe Zhang, Shizhu He, Kang Liu, and Jun Zhao.2016.
A joint model for question answering overmultiple knowledge bases.
In Proceedings of theThirtieth AAAI Conference on Artificial Intelligence,February 12-17, 2016, Phoenix, Arizona, USA.,pages 3094?3100.777
