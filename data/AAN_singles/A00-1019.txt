Unit Completion for a Computer-aided TranslationSystemPh i l ippe  Lang la i s ,  George  Foster  and  Guy  Lapa lmeRAL I  / D IROUniversit6 de Montrea lC.P.
6128, succursale Centre-vi l leMontra l  (Qubec) ,  Canada,  H3C 3J7{ f elipe,f oster, lapalme }@iro.
umontreal, caTypingAbst ractThis work is in the context of TRANSTYPE, a sys-tem that observes its user as he or she types a trans-lation and repeatedly suggests completions for thetext already entered.
The user may either accept,modify, or ignore these suggestions.
We describe thedesign, implementation, and performance of a pro-totype which suggests completions of units of textsthat are longer than one word.1 I n t roduct ionTRANSTYPE is part of a project set up to explorean appealing solution to Interactive Machine Trans-lation (IMT).
In constrast to classical IMT systems,where the user's role consists mainly of assisting thecomputer to analyse the source text (by answeringquestions about word sense, ellipses, phrasal attach-ments, etc), in TRANSTYPE the interaction is direct-ly concerned with establishing the target ext.Our interactive translation system works as fol-lows: a translator selects a sentence and begins typ-ing its translation.
After each character typed bythe translator, the system displays a proposed com-pletion, which may either be accepted using a spe-cial key or rejected by continuing to type.
Thusthe translator remains in control of the translationprocess and the machine must continually adapt it-s suggestions in response to his or her input.
Weare currently undertaking a study to measure theextent o which our word-completion prototype canimprove translator productivity.
The conclusions ofthis study will be presented elsewhere.The first version of TrtANSTYPE (Foster et al,1997) only proposed completions for the currentword.
This paper deals with predictions which ex-tend to the next several words in the text.
The po-tential gain from multiple-word predictions can beappreciated in the one-sentence translation task re-ported in table 1, where a hypothetical user savesover 60% of the keystrokes needed to produce atranslation i a word completion scenario, and about85% in a "unit" completion scenario.In all the figures that follow, we use different fontsto differentiate he various input and output: italicsare used for the source text, sans-serif for characterstyped by the user and typewr i te r - l i ke  for charac-ters completed by the system.The first few lines of the table 1 give an idea ofhow TransType functions.
Let us assume the unit s-cenario (see column 2 of the table) and suppose thatthe user wants to produce the sentence "Ce projetde loi est examin~ ~ la chambre des communes" as atranslation for the source sentence "This bill is ex-amined in the house of commons".
The first hypoth-esis that the system produces before the user entersa character is lo i  (law).
As this is not a good guessfrom TRANSTYPE the user types the first character(c) of the words he or she wants as a translation.Taking this new input into account, TRANSTYPEthen modifies its proposal so that it is compatiblewhith what the translator has typed.
It suggeststhe desired sequence ce projet de Ioi, which the usercan simply validate by typing a dedicated key.
Con-tinuing in this way, the user and TRANSTYPE alter-nately contribute to the final translation.
A screencopy of this prototype is provided in figure 1.2 The  Core  Eng ineThe core of TRANSTYPE is a completion enginewhich comprises two main parts: an evaluator whichassigns probabilistic scores to completion hypothesesand a generator which uses the evaluation functionto select he best candidate for completion.2.1 The  Eva luatorThe evaluator is a function p(t\[t', s) which assigns toeach target-text unit t an estimate of its probabilitygiven a source text s and the tokens t' which precedet in the current ranslation of s. 1 Our approach tomodeling this distribution is based to a large extenton that of the IBM group (Brown et al, 1993), butit differs in one significant aspect: whereas the IB-M model involves a "noisy channel" decomposition,we use a linear combination of separate prediction-s from a language model p(tlt ~) and a translationmodel p(tls ).
Although the noisy channel technique1We assume the existence of a determinist ic  procedure fortokenizing the target text.135This bill is examined in the house of commonsword-completion task unit-completion taskceprojetdeIoiestexamin~chambredescommunespreL completionsce+ / lo i  ?
C/'p+ /es t ?
p / ro je td+ / t rbs  ?
d/eI+ / t=~s ?
I /o ie+ /de ?
e / s te+ /en ?
e/xamin6~+ /par ?
~/ 1~+ /chambrede+ /co,~unes ?
d/e+ /communes?
de/spref.
completionsc-l- /loJ.
?
c/e pro je t  de 1oie+ /de ?
e / s tex+ /~ la  chambre des communes.+ /b l a  chambre des con~unese/n ?
ex /min~Table 1: A one-sentence s ssion illustrating the word- and unit-completion tasks.
The first column indicatesthe target words the user is expected to produce.
The next two columns indicate respectively the prefixestyped by the user and the completions proposed by the system in a word-completion task.
The last twocolumns provide the same information for the unit-completion task.
The total number of keystrokes forboth tasks is reported in the last line.
+ indicates the acceptance key typed by the user.
A completion isdenoted by a/13 where a is the typed prefix and 13 the completed part.
Completions for different prefixesare separated by ?.is powerful, it has the disadvantage that p(slt' , t) ismore expensive to compute than p(tls ) when usingIBM-style translation models.
Since speed is cru-cial for our application, we chose to forego the noisychannel approach in the work described here.
Ourlinear combination model is described as follows:pCtlt',s) = pCtlt') a(t ' ,s)  + pCtls) \[1 - exit',s)\] (1)?
~ ?
?
v Jlanguage translationwhere a(t', s) E \[0, 1\] are context-dependent inter-polation coefficients.
For example, the translationmodel could have a higher weight at the start of asentence but the contribution of the language mod-el might become more important in the middle orthe end of the sentence?
A study of the weightingsfor these two models is described elsewhere?
In thework described here we did not use the contributionof the language model (that is, a(t' ,  s) = O, V t', s).Techniques for weakening the independence as-sumptions made by the IBM models 1 and 2 havebeen proposed in recent work (Brown et al, 1993;Berger et al, 1996; Och and Weber, 98; Wang andWaibel, 98; Wu and Wong, 98).
These studies reportimprovements on some specific tasks (task-orientedlimited vocabulary) which by nature are very differ-ent from the task TRANSTYPE is devoted to.
Fur-thermore, the underlying decoding strategies are tootime consuming for our application?
We thereforeuse a translation model based on the simple linear in-terpolation given in equation 2 which combines pre-dictions of two translation models - -  Ms and M~ - -both based on IBM-like model 2(Brown et al, 1993).Ms was trained on single words and Mu, describedin section 3, was trained on both words and units.- -  _ (2 )word unitwhere Ps and Pu stand for the probabilities given re-spectively by Ms and M~.
G(s) represents he newsequence of tokens obtained after grouping the to-kens of s into units.
The grouping operator G isillustrated in table 2 and is described in section 3.2.2  The  GeneratorThe task of the generator is to identify units thatmatch the current prefix typed by the user, and pickthe best candidate according to the evaluator.
Dueto time considerations, the generator introduces adivision of the target vocabulary into two parts: asmall active component whose contents are alwayssearched for a match to the current prefix, and amuch larger passive part over (380,000 word form-s) which comes into play only when no candidatesare found in the active vocabulary.
The active partis computed ynamically when a new sentence is s-elected by the translator.
It is composed of a fewentities (tokens and units) that are likely to appearin the translation.
It is a union of the best can-didates provided by each model Ms and M~ overthe set of all possible target tokens (resp.
units)that have a non-null translation probability of beingtranslated by any of the current source tokens (resp.units).
Table 2 shows the 10 most likely tokens andunits in the active vocabulary for an example sourcesentence.136that.
is ?
what .
the .
p r ime,  minister .
said?
and .
i ?
have.
outlined?
what .
has .happened .
since?
then .
.c' - est.
ce -que ,  le- premier - ministre, a-d i t .
, .e t .
j ' ,  ai.
r4sum4- ce.
qui .s ' -  est-produit - depuis ?
.g(s) that is what ?
the prime minister said ?
, and i?
have .
outlined ?
what has happened ?
sincethen ?
.AsA~?
?
?
es t  ?
ce  ?
m in i s t re  ?
que .
e t  ?
a ?
p remierl i ece  qu i  s' es t  p rodu i t  ?
e t  je  - c '  es t  ce  que .
vo i l~ce  que  ?
qu '  es t  - c '  es t  ?
,  e t  ?
le p remier  min is t red i sa i tTable 2: Role of the generator for a sample pair ofsentences (t is the translation of s in our corpus).G(s) is the sequence of source tokens recasted bythe grouping operator G. A8 indicates the 10 besttokens according to the word model, Au the 10 bestunits according to the unit model.3 Mode l ing  Un i t  Assoc ia t ionsAutomatically identifying which source words orgroups of words will give rise to which target wordsor groups of words is a fundamental problem whichremains open.
In this work, we decided to proceedin two steps: a) monolingually identifying roups ofwords that would be better handled as units in a giv-en context, and b) mapping the resulting source andtarget units.
To train our unit models, we used asegment of the Hansard corpus consisting of 15,377pairs of sentences, totaling 278,127 english token-s (13,543 forms) and 292,865 french tokens (16,399forms).3.1 F inding Monol ingual  Uni tsFinding relevant units in a text has been explored inmany areas of natural anguage processing.
Our ap-proach relies on distributional and frequency statis-tics computed on each sequence of words found in atraining corpus.
For sake of efficiency, we used thesuffix array technique to get a compact representa-tion of our training corpus.
This method allows theefficient retrieval of arbitrary length n-grams (Nagaoand Mori, 94; Haruno et al, 96; Ikehara et al, 96;Shimohata et al, 1997; Russell, 1998).The literature abounds in measures that can helpto decide whether words that co-occur are linguisti-cally significant or not.
In this work, the strength ofassociation of a sequence of words w\[ = w l , .
.
.
,  wnis computed by two measures: a likelihood-based onep(w'~) (where g is the likelihood ratio given in (Dun-ning, 93)) and an entropy-based one e(w'~) (Shimo-hata et al, 1997).
Letting T stand for the trainingtext and m a token:p(w~) = argming(w~, uS1  ) (3)ie\]l,n\[e(w'~) = 0.5x  +k~rnlw,~meT h ( Ireq(w'~ m) k Ir~q(wT) \]Intuitively, the first measurement accounts for thefact that parts of a sequence of words that shouldbe considered as a whole should not appear often bythemselves.
The second one reflects the fact that asalient unit should appear in various contexts (i.e.should have a high entropy score).We implemented a cascade filtering strategy basedon the likelihood score p, the frequency f ,  the lengthl and the entropy value e of the sequences.
Afirst filter (.~"1 (lmin, fmin, Pmin, emin)) removes anysequence s for which l (s) < lmin or p(s) < Pminor e(s) < e,nin or f ( s )  < fmin.
A second filter(~'2) removes sequences that are included in pre-ferred ones.
In terms of sequence reduction, apply-ing ~1 (2, 2, 5.0, 0.2) on the 81,974 English sequencesof at least two tokens een at least twice in our train-ing corpus, less than 50% of them (39,093) were fil-tered: 17,063 (21%) were removed because of theirlow entropy value, 25,818 (31%) because of their lowlikelihood value.3.2 MappingMapping the identified units (tokens or sequences) totheir equivalents in the other language was achievedby training a new translation model (IBM 2) us-ing the EM algorithm as described in (Brown et al,1993).
This required grouping the tokens in ourtraining corpus into sequences, on the basis of theunit lexicons identified in the previous tep (we willrefer to the results of this grouping as the sequence-based corpus).
To deal with overlapping possibilities,we used a dynamic programming scheme which opti-mized a criterion C given by equation 4 over a set Sof all units collected for a given language plus all sin-gle words.
G(w~) is obtained by returning the paththat maximized B(n) .
We investigated several C-criteria and we found C~--a length-based measurcto be the most satisfactory.
Table 2 shows an outputof the grouping function.Oi l  i=oB( i )  = argmax/~\[1,i\[ ,w~_les ) + B( i  - I - 1) (4)0 i f j<=iwith: Cl (w~)= j - - i  + l e lse137source unit (s)we have 1748we must 720this bill 640people of canada 282mr.
speaker : 269what is happening 190of course , 178is it the pleasure of the house to 14adopt thethe worldchild carethe free trade agreementpost-secondary educationthe first timethe canadian aviation safety boardthe next five yearsthe people of chinaf(8) target units (\[a,p\])\[nous,0.49\] \[avons,0.41\] \[, nous avons,0.07\]\[nous devons,0.61\] \[ilrant,0.19\] [nous,0.14\]\[ce projet de 1oi,0.35\] \[projet de loi .,0.21\] [projet de loi,0.18\]\[les canadiens,0.26\] \[des canadiens,0.21\] \[la population,0.07\]\[m. le prdsident :,0.80\] [a,0.07\] \[h la,0.06\]Ice qui se passe,0.21\] Ice qui se,0.16\] [et,0.15\]\[dvidemment ,0.26\] \[naturellement,0.08\] \[bien stir,0.08\]\[plait-il h la chambre d' adopter,0.49\] \[la motion ?,0.42\] [motion?,0.04\]201 \[le monde,O.46\] [du monde,O.33\] lie monde entier,O.19\]86 lies garderies,O.59\] \[la garde d' enfants,O.23\] \[des services degarde d' enfants,O.13\]75 \[1' accord de libre-dchange,O.96\] \[la ddcision du gatt,O.04\]66 \[1' euseignement postsecondaize,O.75\] \[1' dducation postsec-ondaire,O.15\] \[des fonds,O.06\]62 \[la premiere fois,l.00\]36 lie bureau canadien de la s~urit~ adrienne,O.55\] \[du bureau cana-dien de la sdcurit~ adrienne,O.31\] \[1'un,O.14\]26 \[au cours des cinq prochaines ann~es,O.53\] \[cinq prochaines an-ndes,O.27\] \[25 milliards de d ollars,O.lO\]17 \[le peuple chinois,0.38\] \[la population chinoise,0.25\] \[les chi-nois,O.13\]Table 3: Bilingual associations.
The first column indicates a source unit, the second one its frequency in thetraining corpus.
The third column reports its 3-best ranked target associations (a being a token or a unit,p being the translation probability).
The second half of the table reports NP-associations obtained after thefilter described in the text.We investigated three ways of estimating the pa-rameters of the unit model.
In the first one, El,the translation parameters are estimated by apply-ing the EM algorithm in a straightforward fashionover all entities (tokens and units) present at leasttwice in the sequence-based corpus 2.
The two nextmethods filter the probabilities obtained with the Ezmethod.
In E2, all probabilities p(tls ) are set to 0whenever s is a token (not a unit), thus forcing themodel to contain only associations between sourceunits and target entities (tokens or units).
In E3any parameter of the model that involves a tokenis removed (that is, p(tls ) = 0 if t or s is a token).The resulting model will thus contain only unit as-sociations.
In both cases, the final probabilities arerenormalized.
Table 3 shows a few entries from aunit model (Mu) obtained after 15 iterations of theEM-algorithm on a sequence corpus resulting fromthe application of the length-grouping criterion (dr)over a lexicon of units whose likelihood score is above5.0.
The probabilities have been obtained by appli-cation of the method E2.We found many partially correct associationsCover the years/au fils des, we have/nous, etc) thatillustrate the weakness of decoupling the unit iden-tification from the mapping problem.
In most cas-2The entities een only once are mapped to a special "un-known" wordes however, these associations have a lower proba-bility than the good ones.
We also found few er-ratic associations (the first time/e'dtait, some hon.members/t, etc) due to distributional rtifacts.
It isalso interesting to note that the good associationswe found are not necessary compositional in nature(we must/il Iaut, people of canada/les canadiens, ofeourse/6videmment, etc).3.3 F i l ter ingOne way to increase the precision of the mappingprocess is to impose some linguistic constraints onthe sequences such as simple noun-phrase contraints(Ganssier, 1995; Kupiec, 1993; hua Chen and Chen,94; Fung, 1995; Evans and Zhai, 1996).
It is alsopossible to focus on non-compositional compounds,a key point in bilingual applications (Su et al, 1994;Melamed, 1997; Lin, 99).
Another interesting ap-proach is to restrict sequences to those that do notcross constituent boundary patterns (Wu, 1995; Fu-ruse and Iida, 96).
In this study, we filtered for po-tential sequences that are likely to be noun phrases,using simple regular expressions over the associatedpart-of-speech tags.
An excerpt of the associationprobabilities of a unit model trained considering on-ly the NP-sequences i given in table 3.
Applyingthis filter (referred to as JrNp in the following) to the39,093 english sequences still surviving after previ-ous filters ~'1 and ~'2 removes 35,939 of them (92%).138model spared ok good nu u1 baseline - model 1 48.98 0 0 747 02 basel ine - model 2 51.83 0 0 747 03 E1 + ~'1(2, 2, 0, 0.2) 50.98 527 1702 5 6264 E1+~'1(2,2,5,0.2)  51.61 596 2149 5 6585 E1 + ~-~ (2, 2, 5, 0.2) + 9r2 51.72 633 2265 5 6576 E2 + ~'~(2,2,0,0.2) 51.39 514 1551 43 5787 ?2 + ~-~ (2, 2, 5, 0.2) 51.99 470 1889 46 6148 E2 + ~'~(2,2,5,0.2) + ~'2 52.12 493 1951 46 6069 E3 + ~-1(2, 2, 0, 0.2) 51.07 577 1699 43 58810 E2 + ~-1(2, 2, 5, 0.2) 51.47 629 2124 46 61811 E2+~'~(2 ,2 ,5 ,0 .2 )+~'2  51.68 665 2209 46 61512 ~1 -}- .~1 (2, 2, 5, 0.2) -}- .~2 -}- ~:NP 52.83 416 1302 4 56413 E2 + ~'1(2, 2, 5, 0.2) + ~NP 53.12 439 1031 228 42514 ?2 + ~'~ (2, 2, 5, 0.2) + 5r2 + ~'NP 53.16 458 1052 199 43915 ~3 -{- ~ : 0.4 -}- ~-1(2, 2, 5, 0.2) 4- .~NP 53.22 495 1031 228 425Table 4: Completion results of several translation models, spared: theoretical proportion of characterssaved; ok: number of target units accepted by the user; good: number of target units that matched theexpected whether they were proposed or not; nu: number of sentences for which no target unit was foundby the translation model; u: number of sentences for which at least one helpful unit has been found by themodel, but not necessarily proposed.More than half of the 3,154 remaining NP-sequencescontain only two words.4 Resu l t sWe collected completion results on a test corpusof 747 sentences (13,386 english tokens and 14,506french ones) taken from the Hansard corpus.
Thesesentences have been selected randomly among sen-tences that have not been used for the training.Around 18% of the source and target words are notknown by the translation model.The baseline models (line 1 and 2) are obtainedwithout any unit model (i.e.
/~ = 1 in equation 2).The first one is obtained with an IBM-like model 1while the second is an IBM-like model 2.
We observethat for the pair of languages we considered, model2 improves the amount of saved keystrokes of almost3% compared to model 1.
Therefore we made use ofalignment probabilities for the other models.The three next blocks in table 4 show how theparameter estimation method affects performance.Training models under the C1 method gives the worstresults.
This results from the fact that the word-to-word probabilities trained on the sequence basedcorpus (predicted by Mu in equation 2) are less ac-curate than the ones learned from the token basedcorpus.
The reason is simply that there are less oc-currences of each token, especially if many units areidentified by the grouping operator.In methods C2 and C3, the unit model of equation2 only makes predictions pu(tls ) when s is a source u-nit, thus lowering the noise compared to method ?1.We also observe in these three blocks the influenceof sequence filtering: the more we filter, the betterthe results.
This holds true for all estimation meth-ods tried.
In the fifth block of table 4 we observethe positive influence of the NP-filtering, especiallywhen using the third estimation method.The best combination we found is reported in line15.
It outperforms the baseline by around 1.5%.This model has been obtained by retaining all se-quences een at least two times in the training cor-pus for which the likelihood test value was above 5and the entropy score above 0.2 (5rl (2, 2, 5, 0.2)).
Interms of the coverage of this unit model, it is in-teresting to note that among the 747 sentences ofthe test session, there were 228 for which the modeldid not propose any units at all.
For 425 of the re-maining sentences, the model proposed at least onehelpful (good or partially good) unit.
The active vo-cabulary for these sentences contained an average ofaround 2.5 good units per sentence, of which onlyhalf (495) were proposed during the session.
Thefact that this model outperforms others despite it-s relatively poor coverage (compared to the others)may be explained by the fact that it also removespart of the noise introduced by decoupling the i-dentification of the salient units from the trainingprocedure.
Furthermore, as we mentionned earlier,the more we filter, the less the grouping scheemepresented in equation 4 remains necessary, thus re-ducing a possible source of noise.The fact that this model outperforms others, de-spite its relatively poor coverage, is due to the fact139E ich le r  C )pt lonsl am p leased  to  t~ lce  ]par t  in  th i s  debate  tod  W .Us ing  rod  W "s techno log ies ,  i t  i s  poss ib le  fo r  a l l  C~m~dia~s  toreg is ter  the i r  votes  on  i s s t les  of  pub l i c  spend ing  and  pub l i cI )o r ro~v ing .II me fa l t  p la le l r  de  prendre  la paro le  au Jourd 'hu i  dana  le cadre  de  ?ed~bat .Gr~ice  & la  techno log le  moderne ,  toue  lea  Canad len= peuvent  6eprononcer  sur  le=;  quest ion= de  d6pen=e== et  d" e rnprunta  de  I" I~tat  .Not re  pFigure 1: Example of an i teraction i  TRANSTYPE with the source text in the top half of the screen.
Thetarget text is typed in the bottom half with suggestions given by the menu at the insertion point.that it also removes part of the noise that is intro-duced by dissociating the identification ofthe salientunits from the training procedure.
~rthermore, aswe mentioned earlier, the more we filter, the less thegrouping scheme presented in equation 4 remainsnecessary, thus further reducing an other possiblesource of noise.5 ConclusionWe have described a prototype system calledTRANSTYPE which embodies an innovative ap-proach to interactive machine translation in whichthe interaction is directly concerned with establish-ing the target ext.
We proposed and tested a mech-anism to enhance TRANSTYPE by having it predic-t sequences of words rather than just completionsfor the current word.
The results show a modestimprovement in prediction performance which willserve as a baseline for our future investigations.
Oneobvious direction for future research is to revise ourcurrent strategy of decoupling the selection of unitsfrom their bilingual context.AcknowlegmentsTRANSTYPE is a project funded by the Natural Sci-ences and Engineering Research Council of Canada.We are undebted to Elliott Macklovitch and PierreIsabelle for the fruitful orientations they gave to thiswork.ReferencesAdam L. Berger, Stephen A. Della Pietra, and Vin-cent J. Della Pietra.
1996.
A maximum entropyapproach to natural language processing.
Compu-tational Linguistics, 22(1):39-71.Peter F. Brown, Stephen A. Della Pietra, Vincen-t Della J. Pietra, and Robert L. Mercer.
1993.The mathematics of machine trmaslation: Pa-rameter estimation.
Computational Linguistics,19(2):263-312, June.Ted Dunning.
93.
Accurate methods for the statis-tics of surprise and coincidence.
ComputationalLinguistics, 19(1):61-74.David A. Evans and Chengxiang Zhai.
1996.
Noun-phrase analysis in unrestricted text for informa-tion retrieval.
In Proceedings of the 34th Annu-al Meeting of the Association for ComputationalLinguistics, pages 17-24, Santa Cruz, California.George Foster, Pierre Isabelle, and Pierre Plamon-don.
1997.
Target-text Mediated Interactive Ma-chine Translation.
Machine Translation, 12:175-194.Pascale Fung.
1995.
A pattern matching method forfinding noun and proper noun translations fromnoisy parallel corpora.
In Proceedings ofthe 33rdAnnual Meeting of the Association for Compu-tational Linguistics, pages 236-243, Cambridge,Massachusetts.Osamu Furuse and Hitoshi Iida.
96.
Incremen-140tal translation utilizing constituent boundray pat-terns.
In Proceedings of the 16th InternationalConference On Computational Linguistics, pages412-417, Copenhagen, Denmark.Eric Gaussier.
1995.
Modles statistiques et patron-s morphosyntaxiques pour l'extraction de lcxiquesbilingues.
Ph.D. thesis, Universit de Paris 7, jan-vier.Masahiko Haruno, Satoru Ikehara, and TakefumiYamazaki.
96.
Learning bilingual collocations byword-level sorting.
In Proceedings of the 16th In-ternational Conference On Computational Lin-guistics, pages 525-530, Copenhagen, Denmark.Kuang hua Chen and Hsin-Hsi Chen.
94.
Extract-ing noun phrases from large-scale texts: A hybridapproach and its automatic evaluation.
In Pro-ceedings of the 32nd Annual Meeting of the Asso-ciation for Computational Linguistics, pages 234-241, Las Cruces, New Mexico.Satoru Ikehara, Satoshi Shirai, and Hajine Uchino.96.
A statistical method for extracting uinterupt-ed and interrupted collocations from very largecorpora.
In Proceedings of the 16th InternationalConference On Computational Linguistics, pages574-579, Copenhagen, Denmark.Julian Kupiec.
1993.
An algorithm for finding nounphrase correspondences in bilingual corpora.
InProceedings of the 31st Annual Meeting of theAssociation for Computational Linguistics, pages17-22, Colombus, Ohio.Dekang Lin.
99.
Automatic identification of non-compositional phrases.
In Proceedings of the 37thAnnual Meeting of the Association for Computa-tional Linguistics, pages 317-324, College Park,Maryland.I.
Dan Melamed.
1997.
Automatic discovery of non-compositional coumpounds in parallel data.
InProceedings of the 2nd Conference on EmpiricalMethods in Natural Language Processing, pages97-108, Providence, RI, August, lst-2nd.Makoto Nagao and Shinsuke Mori.
94.
A newmethod of n-gram statistics for large number ofn and automatic extraction of words and phrasesfrom large text data of japanese.
In Proceedingsof the 16th International Conference On Com-putational Linguistics, volume 1, pages 611-615,Copenhagen, Denmark.Franz Josef Och and Hans Weber.
98.
Improvingstatistical natural anguage translation with cate-gories and rules.
In Proceedings of the 36th Annu-al Meeting of the Association for ComputationalLinguistics, pages 985-989, Montreal, Canada.Graham Russell.
1998.
Identification of salient to-ken sequences.
Internal report, RALI, Universityof Montreal, Canada.Sayori Shimohata, Toshiyuki Sugio, and JunjiNagata.
1997.
Retrieving collocations by co-occurrences and word order constraints.
In Pro-ceedings of the 35th Annual Meeting of the Asso-ciation for Computational Linguistics, pages 476-481, Madrid Spain.Keh-Yih Su, Ming-Wen Wu, and Jing-Shin Chang.1994.
A corpus-based approach to automatic om-pound extraction.
In Proceedings of the 32nd An-nual Meeting of the Association for Computation-al Linguistics, pages 242-247, Las Cruces, NewMexico.Ye-Yi Wang and Alex Waibel.
98.
Modeling withstructures in statistical machine translation.
InProceedings of the 36th Annual Meeting of theAssociation for Computational Linguistics, vol-ume 2, pages 1357-1363, Montreal, Canada.Dekai Wu and Hongsing Wong.
98.
Machine trans-lation with a stochastic grammatical channel.
InProceedings of the 36th Annual Meeting of theAssociation for Computational Linguistics, pages1408-1414, Montreal, Canada.Dekai Wu.
1995.
Stochastic inversion transduc-tion grammars, with application to segmentation,bracketing, and alignment of parallel corpora.
InProceedings of the International Joint Conferenceon Artificial Intelligence, volume 2, pages 1328-1335, Montreal, Canada.141
