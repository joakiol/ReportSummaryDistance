Proceedings of the 8th International Natural Language Generation Conference, pages 74?82,Philadelphia, Pennsylvania, 19-21 June 2014. c?2014 Association for Computational LinguisticsExperimental Design to Improve Topic Analysis Based SummarizationJohn E. MillerComputer & Information SciencesUniversity of DelawareNewark, DE 19711jmiller@udel.eduKathleen F. McCoyComputer & Information SciencesUniversity of DelawareNewark, DE 19711mccoy@udel.eduAbstractWe use efficient screening experimentsto investigate and improve topic analysisbased multi-document extractive summa-rization.
In our summarization process,topic analysis determines the weightedtopic content vectors that characterize thecorpora, and then Jensen-Shannon diver-gence extracts sentences that best matchthe weighted content vectors to assemblethe summaries.
We use screening experi-ments to investigate several control param-eters in this process, gaining better under-standing of and improving the topic anal-ysis based summarization process.1 IntroductionWe use efficient experimental design to investi-gate and improve topic analysis based multipledocument extractive summarization.
Our processproceeds in two steps: Latent Dirichlet Anal-ysis (LDA) topic analysis determines the top-ics that characterize the multi-document corpus,and Jensen-Shannon divergence selects sentencesfrom the corpus.
This process offers many poten-tial control settings for understanding and improv-ing the summarization process.Figure 1 shows topic analysis with corpus input,control settings, and product outputs of topics andprobability estimates of topic compositions anddocument mixtures.
There are controls for doc-ument preparation (headlines) and analysis (num-ber of topics, initial ?
and ?, number of iterations,and whether to optimize ?
and ?
in process).Figure 2 shows summarization with corpus andtopic inputs, control settings, and the text summa-rization product.
There are controls for extractionof sentences (Extract ?
and JSD Divisor) and forcomposing the summary (Order policy).Topic analysis has become a popular choice fortext summarization as seen in Text Analysis Con-CorpusAnalyzeTopics# Iterations!Optimize !, "# Topics!Initial !, "Topics#$HeadlinesFigure 1: Topic AnalysisCorpusAnalyzeTopicsOrder policyExtract !
!JSD divisorSummaryTopicsFigure 2: Text Summarizationferences (TAC, 2010; TAC, 2011) with individualteam reports (Delort and Alfonseca, 2011; Lui etal., 2011; Mason and Charniak, 2011).
Nenkovaand McKeown (2012; 2011) included topic anal-ysis among standard methods in their surveys oftext summarization methodologies.
Haghighi andVanderwende (2009) explored extensions of LDAtopic analysis for use in multiple document sum-marization tasks.
Yet there are many control set-tings that can affect summarization that have notbeen explicitly studied or documented, and thatare important for reproducing research results.In this text summarization pilot study, we exper-iment with several control settings.
As in Masonand Charniak (2011) we do a general rather thanguided summarization.
Our primary contributionis illustrating the use of efficient experimental de-sign on control settings to help understand and im-prove the text summarization process.
We enjoysome success in this endeavor even as we are sur-prised by some of our results.742 Technical Background2.1 LDA Topic AnalysisLDA topic analysis uses a per document bag ofwords approach to determine topic compositionsof words and document mixtures of topics.
Anal-ysis constructs topic compositions and documentmixtures by assigning words to topics within doc-uments.
Weighted topic compositions can then beused as a basis for selecting the most informativetext to include in summarizations.LDA topic analysis is based on a generativeprobabilistic model.
Document mixtures of top-ics are generated by a multinomial distribution,?, and topic compositions of words are gener-ated by a multinomial distribution, ?.
Both ?
and?
in turn are generated by Dirichlet distributionswith parameters ?
and ?
respectively.
Figure 3(Steyvers and Griffiths, 2007) shows a corpus ex-plained as the product of topic word compositions(?)
and document topic mixtures (?
).Corpuswordsdocumentswordstopics documentstopics=x!
"Figure 3: Topic ModelThe joint distribution of words and topics (Grif-fiths and Steyvers (2004)) is given by P (w, z) =P (w|z)P (z) where in generating a document thetopics are generated with probability P (z) and thewords given the topics are generated with proba-bility P (w|z).
HereP (w|z) =(?
(??)?
(?
)V)Z Z?z=1?v ?
(nzv + ?)?
(nz?
+ ??
),(1)where nzv is the number of times word v occurs intopic z, nz?
is the number of times topic z occurs,??
is the sum of the ?
scalar over all word types,and ?
( ) is the gamma function (Knuth, 2004),andP (z) =(?
(??)?
(?
)Z)D D?d=1?z ?
(nzd + ?)?
(n?d + ??
), (2)where nzd is the number of times topic z occurs indocument d, n?d is the number of times documentd occurs, and ??
is the sum of ?s over topics.Analysis reverses the generative model.
Givena corpus, topic analysis identifies weighted topicword compositions and document topic mixturesfrom the corpus.
We assign topics to words inthe training corpus using Gibbs sampling (Gel-man et al., 2004) where each word is considered inturn in making the topic assignment.
We monitortraining progress by logP (w, z) where a greaterlogP (w, z) indicates better fit.
After sufficient it-erations through the corpus the logP (w, z) typi-cally converges to steady state.Analysis products are topic determinations forthe corpus as well as weighted estimates of topicword compositions ?
and document topic mix-tures ?.
The ?
and ?
priors are optimized (re-estimated) during training and the asymmetric ?which varies by topic can be used as a measure oftopic importance in our summarization step.The topic analysis implementation used in thispilot study borrows from the UMass Mallet topicanalysis (McCallum, 2002).2.2 Jensen-Shannon DivergenceFrom the topic word compositions and optimized?s, we form a weighted aggregate vector of theprominent topics, and select sentences from thecorpus that have minimal divergence from the ag-gregate topic.
The operating assumption is that theaggregate topic vector adequately represents thecontent of an ideal summary.
So the closer to zerodivergence from the aggregate topic, the closer weare to the ideal summary.We seek to minimize the Jensen-Shannon Di-vergence, JSD(C||T ), a symmetric Kullback-Liebler (KL) divergence, between the extractivesummary content, C, and the aggregate topic, T,using a greedy search method of adding at eachpass through the corpus the sentence that mostreduces the divergence.
Haghighi and Vander-wende (2009) made similar use of KL divergencein their Topic Sum method.In preliminary studies, this minimize JSD cri-terion seemed to give overly long sentences be-cause the greedy method favored the greatest re-duction in JSD regardless of the length of the sen-tence.
This affected readability and rapidly usedup all available target document size.
Thereforewe modified the greedy search method to considersentence length as well.11Global optimization of JSD(C||T ) could address bothof these issues; we will investigate this option in a future ef-fort.75In selecting each new sentence we seek to maxi-mize the reduction in divergence corrected for sen-tence length(JSD(Ct?1||T )?
JSD(St, Ct?1||T ))function(length(St)), (3)where St is the sentence under consideration andCt?1 is the content from the previously completediterations, and the function of length of St, is ei-ther the constant 1 (i.e.
no correction for sentencelength) or?length(St).3 Pilot Study Using TAC 2010 SamplesOur goal is to investigate and optimize factors thatimpact multi-document extractive summarization.We hope to subsequently extend our findings andexperience to abstractive summarization as well.For our pilot, we?ve chosen summarization ofthe 2010 Text Analysis Conference (2010) sam-ple themes, which are conveniently available andof a manageable size.
The three sample themesare from different summarization categories out ofa total of 46 news themes over five different cat-egories, with 10 original and 10 follow-up newsreports each.
In the original TAC 2010 task, par-ticipants were asked to do focused queries varyingwith the summarization category.
In our pilot weperform an undirected summarization of the orig-inal news reports.NIST provides 4 model summaries for eachnews theme annotated for the focused summary,and we use these model summaries in scoring ourextractive summarizations.2 We also include ameasure of fluency in our assessment.Our document summarization task is then: mul-tiple document extractive summarization using 10documents of less than 250 words each to con-struct summaries of 100 words.3.1 Preliminary Results of Topic AnalysisTopic analysis is such a complex methodology thatit makes sense to fix some parameters before usingit in the summarization process.We use the commonly accepted initial ?
valueof 1 for each topic giving a sum of ?
values equalto the number of topics.
Later, we experiment witha single individual topic initial ?
value, but we al-ways maintain an initial ?
sum equal to the num-ber of topics.
Likewise we use the scalar ?
value2Comparison of our summarization results versus theTAC 2010 task will necessarily be imprecise given the dif-ferences in focus of our pilot study from TAC 2010.0.1 typical of a modest number of word types (lessthan 1000 in this study).In prior studies, we found that re-estimating ?and ?
frequently adds little cost to topic analysisand drives better and more rapid convergence.
Weoptimize ?
and ?
every 5 iterations, starting at it-eration 50.How Many Topics to UseThe number of topics depends on the problem it-self.
The problem of size of ?
2000 words pernews theme would indicate a number of topics be-tween 3 and 20 as adequate to explain documentword use where the log(|Corpus|) is the mini-mum and?|Corpus| is the maximum number oftopics to use (Meila?, 2007).A common way to select the correct number oftopics is to optimize logP (w) on held-out doc-uments, where greater log likelihoods indicate abetter number of topics.
While it would be im-practical to do such a study for each news themeor each document summary, it is reasonable to doso on a few sample themes and then generalize tosimilar corpora.
We look at log likelihood for 3, 5,and 10 topics using the TAC 2010 sample themes.As there are only 10 documents for each theme,we use the TAC 2010 update documents as held-out documents for calculating the log likelihoods.Topic word distributions, ?, from training areused to infer document mixtures, ?, on the held-out data, and the log P (w) is calculated (Teh etal., 2007) as:P (w) =?d,i(?znzwi + ?nz?
+ ?
?nzd + ?n?d + ??
), (4)where the sum is over all possible topics for agiven word and the product is over all documentsand words.Table 1 shows mean log likelihoods for the newsthemes at 3, 5 and 10 topics each.
There is lit-tle practical difference between the log likelihoodmeasures even though the 3 topic model has a sig-nificantly lower log likelihood (p < 0.05) thanthe 5 and 10 topic models.
We assess topic qualitymore directly to see which model is better.3 Topics 5 Topics 10 Topics-6.00 -5.97 -5.96Table 1: Held-out Log Likelihood Number Topics.76Useful topic quality measures are:Importance measured by number of documents(or optimized ?s).
Low importance topics,with very few documents related to a topic,indicate that we have more topics than neces-sary.
While not a fatal flaw, the topic modelmay be over fit.Coherence measured as a log sum of co-occurrence proportions of each topic?s highfrequency words across multiple docu-ments (Mimno et al., 2011).
The more neg-ative the coherence measure, the poorer thecoherence.
A few poor coherence topics isnot fatal, but the topic model may be over fit.Similarity to other topics measured by cosinedistance between topic vectors is undesirable.The more similar the topics, the more diffi-cult it is to distinguish between them.
Manysimilar topics makes it difficult to discrimi-nate among topics over the corpus.Reviewing the document quality for 3, 5 and 10topics we find:?
More low importance topics in 10 versus 5and 3 topic models,?
Somewhat better topic coherence in 3 and 5topic models,?
Undesirable greater topic similarity for the 3versus 5 versus 10 topic models.We choose the 10 topic model giving higher pri-ority to the problem of undesirable topic similar-ity, recognizing that we may get some unimportantor less coherent topics.
As our summarization pro-cess only uses the most important topics for the ag-gregate topic, the occasional unimportant and lesscoherent topic should not matter.Document PreparationDocument cleaning removed all HTML, as wellas all header information not related to the articlesthemselves; document dates, references, and head-lines were saved for use in the document summa-rization step.
Document headlines were optionallyfolded into the document text.
Stop words were re-moved and remaining words lemmatized for topicanalysis.4 Design of ExperimentsAs our information about the various controls inthe process and the expected results is fairly rudi-mentary, we use efficient screening experimentaldesigns to evaluate several factors at the same timewith a minimum number of trials.
We define thefactors (control parameters) in our experiment, thedependent variables we will measure, and finallyselect the screening design itself.Most of the process of topic analysis will re-main fixed such as the use of 10 topics, initial ?sum of 10, initial scalar ?
of 0.1, optimization of?
and ?
every 5 iterations and 500 total iterationsbefore saving the final topic vector weights andcorresponding topic alphas.From our experimentation we hope to find:?
Factors impacting dependent variables,?
Gross magnitude of impact on dependentvariables,?
Factors to followup with in more detail.4.1 Experimental FactorsIn screening experiments, we chose factors aboutwhich we have crude information, and which wethink could impact intermediate or final productresults.
To learn as much as possible about factoreffects, we choose to vary them between defaultand extreme settings or between two extremeswhere we hope to see some positive impact.Our experimental factors are:Save headline text as part of document prepara-tion (Yes, No).
Headlines often contain im-portant summary information.
We test to seeif such information improves summaries.Single fixed ?
proportion of the ?
sum (*, 0.5).Topic analysis typically selects (weights) afew important topic vectors with substantialproportions of the ?
sum.
We want to see ifbiasing selection of a single important vec-tor at a 0.5 proportion of the ?
sum improvessummaries versus unbiased ?
weighting (*).Aggregate topic policy as a proportion of the ?sum for selecting the topic aggregate used insummarization (0.5, 0.75).
We order topicsbased on the optimized (re-estimated) ?s andaggregate topics summing and weighting bythe ?s until we reach the aggregate topic pol-icy proportion.
We want to see which policy(0.5 or 0.75) proportion of the ?
sum resultsin better summaries.JSD divisor to use with iterative greedy searchfor sentences (ONE, SQRT).
Prior workshows the JSD Divisor impacts the length of77sentences selected.
We test the impact on thesummaries themselves.Order policy for constructing the summaryfrom selected sentences (DATE-DOC,SALIENCE-DOC).
Ordering sentences bynews report date or by salience as measuredby reduction in JSD should impact thefluency of summaries.4.2 Dependent VariablesWe want readable and informative text that sum-marizes content of the input documents in the al-lowable space.
We measure several intermedi-ate process variables as well as evaluate the sum-maries themselves.Intermediate measures include:?
Initial selected sentence Jensen-Shannon di-vergence from the aggregate topic.
The firstsentence selected should substantially reducedivergence.?
Final selected sentence Jensen-Shannon di-vergence from the aggregate topic.
Diver-gence close to zero would indicate broad cov-erage of the aggregate topic; it may be relatedto summary content.?
Number of topics in the aggregate topic.?
Average sentence length.
This should be im-pacted by the JSD divisor; it may be relatedto summary fluency.ROUGE (Lin, 2011) is a package for auto-matic evaluation of summaries that compares sys-tem produced summaries to model (gold stan-dard) summaries and reports statistics such as R-2, bi-gram co-occurrence statistics between sys-tem and model summaries, and SU4, skip bi-gramco-occurrence statistics where word pairs no morethan 4 words apart may also be counted as bi-grams.
The R-2 and SU4 are automated contentmeasures reported for TAC 2010, and the goldstandard summaries are readily available for thesamples topics.
We use ROUGE R-2 and SU4 asreliable dependent measures and for comparisonto TAC 2010 results.We add a simple measure of fluency focused onacross sentence issues.
The fluency score startsat a value of 5 and then subtracts: 1 for eachnon sequitur or obvious out of order sentence, 1/2for each missing co-reference, non-informative,ungrammatical, or redundant sentence.
For sen-tences of less than 20 words, when more than onepenalty applies only the most severe penalty is ap-plied, so as not to penalize the same short phrasemultiple times.
Scoring is done by one of the au-thors without knowing the combination of experi-mental factors of the summary (blind scoring).Summary measures thus include: ROUGE R-2,ROUGE SU4, and Fluency.4.3 Select Experimental DesignScreening designs focus on detecting and assess-ing main effects and optionally low order inter-action effects.
When all experimental factors arecontinuous, center points may also be included insome designs.
In subsequent stages of experimen-tation, when factors have been reduced to a min-imum, one can use more fine grained factor set-tings to better map the response surface for thosefactors.
Two common families of screening de-signs (Montgomery, 1997) are:Two level fractional factorial Uses a power of1/2 fraction of a full two level factorial design.For example, instead of running all possiblecombinations of 5 factors (i.e.
32 trials), youcould choose a 1/2 or even 1/4 fraction of thedesign, based on how many experiments youcan run and how much confounding you arewilling to accept between main effects andvarious interaction effects.
The 1/2 fraction ofa 5 factor design would result in 16 trials be-ing run with the main effects estimated clearof any 2-way or 3-way interactions.Plackett-Burman These screening designs areavailable in multiples of 4 trials and can haveas many factors as the number of trials lessone.
Main effects are confounded with allother effects in the Plackett-Burman designand so not estimable, but the confounding isspread evenly among all main effects ratherthan concentrated in specific interactions asin the fractional factorial.We?ve chosen the 12 run Plackett-Burman de-sign with 5 factors and 6 degrees of freedom fromthe unassigned (dummy) factors available to esti-mate error.
Assuming sparsity of effects (or equiv-alently invoking the Pareto principal), there willlikely only be a few critical factors explainingmuch of the variation in dependent variables.Table 2 shows the resulting Plackett-Burmandesign excluding dummy factors.78Run Fixed Aggr JSD Order HeadAlpha Topic Div Line1 .5 .75 ONE SAL YES2 * .75 SQRT DATE YES3 .5 .5 SQRT SAL NO4 * .75 ONE SAL YES5 * .5 SQRT DATE YES6 * .5 ONE SAL NO7 .5 .5 ONE DATE YES8 .5 .75 ONE DATE NO9 .5 .75 SQRT DATE NO10 * .75 SQRT SAL NO11 .5 .5 SQRT SAL YES12 * .5 ONE DATE NOTable 2: Plackett-Burman 12 DOE.5 Experimental ResultsWe analyze our experiment using conventionalanalysis of variance (ANOVA) and show tables ofmeans for the various experimental conditions.
Asthis is a screening experiment, we treat a p-value<0.20 as informative and consider the correspond-ing factor worth further consideration.
To savespace, only significant p-values are reported ratherthan the full ANOVAs.5.1 Intermediate MeasuresNumber of topics in the aggregate topic is directlyimpacted by the AggrTopic setting; we simply re-port the mean number of topics selected by Aggr-Topic value (Table 3).
The 1.0 average number oftopics for AggrTopic set to 0.5 indicates that onlyone topic was ever selected for the aggregate topicat this setting.
This implies that the most impor-tant topic always had an ?
proportion > 0.50 ofthe ?
sum even when the FixedAlpha setting was* (for unbiased ?
weighting).
This is unexpectedin that we thought the most important topic ?
de-termined by topic analysis would be more variableand show some ?
values with proportions less than0.5 of the ?
sum.Aggr NumberTopic Topics0.50 1.000.75 4.55Table 3: Average Number Topics.Average sentence length in the summary may beaffected by any of the independent variables ex-cept sentence order policy.
JSD Divisor has a dra-matic impact (p < 0.0001) and AggrTopic a mod-est impact (p < 0.01) on average sentence length.Using a divisor of ONE in the JSD based sentenceselection results in much longer sentences whileusing AggrTopic of 0.5 results in shorter sentences(Table 4).Aggr Sentence JSD SentenceTopic Length Divisor Length0.5 20.3 ONE 26.80.75 23.9 SQRT 17.4Standard Error of the mean = 0.78Table 4: Average Sentence Length.Initial selected sentence Jensen-Shannon diver-gence (JSD) should be affected directly by JSDDivisor in iterative sentence selection, but mayalso be affected by any of the other independentvariables except for sentence order policy.
Aggr-Topic and JSD Divisor strongly impact initial sen-tence JSD (p < 0.00005).The table of JSD initial sentence means by Ag-grTopic and JSD Divisor is revealing (Table 5).The JSD for the initial sentence selected is lowerfor AggrTopic of 0.5.
We observed above that onlyone topic is selected for the aggregate topic whenAggrTopic is 0.5.
Thus we achieve a lower di-vergence of the initial sentence from the aggregatetopic when the aggregate is composed of only onetopic.
For initial sentence JSD, aggregating topicsseems ineffective.Similarly a JSD Divisor of ONE gives a lowerinitial divergence than using the SQRT as the di-visor.
The interpretation is problematic here inthat a divisor of ONE seems to give lower ini-tial divergence because it selects longer sentences,which means that less space remains in the sum-mary to select other sentences minimizing total di-vergence.Aggr JSD JSD JSDTopic Initial Divisor Initial0.5 0.665 ONE 0.6580.75 0.735 SQRT 0.742Standard Error of the mean = 0.0056Table 5: Average Initial JSD.Table 6 shows the impact of AggrTopic and JSDDivisor together on the JSD for the initial sen-tence.
There is still the issue of whether using aJSD Divisor of ONE is appropriate given the ef-fect on the remaining summary size, but the effectsappear additive.79Aggr JSD JSDTopic Divisor initial0.50 ONE 0.6270.50 SQRT 0.7030.75 ONE 0.6900.75 SQRT 0.780Standard Error of the mean = 0.0080Table 6: Average Initial JSD.Final sentence Jensen-Shannon Divergence(JSD) may be affected by any but the sentence or-der policy variable.
AggrTopic (p < 0.00001) andJSD Divisor (p < 0.001) strongly impact the finalsentence JSD; there is also a possible effect fromincluding headlines in the summary (p < 0.1).The effect of the JSD Divisor has reversed fromthe initial JSD; using a divisor of ONE results herein a less desirable higher divergence for the finalsentence.
The AggrTopic effect is about the sameas for initial JSD divergence; a single dominanttopic seems more effective than using an aggre-gate topic.Aggr JSD JSD JSDTopic Final Divisor Final0.5 0.422 ONE 0.4870.75 0.513 SQRT 0.448Standard Error of the mean = 0.0047Table 7: Average Initial JSD.Impact of AggrTopic and JSD Divisor togetheron the JSD for the initial sentence (Table 8) seemsadditive.Aggr JSD JSDTopic Divisor final0.50 ONE 0.4370.50 SQRT 0.4070.75 ONE 0.5370.75 SQRT 0.490Standard Error of the mean = 0.0066Table 8: Average Final JSD.5.2 Product MeasuresBased on the analysis of intermediate measures, itwould seem that using a JSD Divisor of the SQRTand selecting only the dominant topic gives lessdivergence from the aggregate topic.
However,we have to be careful here in drawing conclusionsbased on intermediate variables; selecting only thedominant topic may result in reduced divergence,but this does not necessarily mean that the domi-nant topic is representative of good summaries.We examine product variables to provide directsupport in our study, and so we ask how ROUGER-2 and SU4, and fluency evaluations vary withthe experimental factors.
This pilot studies un-guided summarization of initial stories from the 3sample news themes from 3 separate categories.While results are not directly comparable withthose of the full TAC 2010 test corpus, we will usethe TAC 2010 results as a reference point versusour own results.
The average of all experimentsare reported along with the TAC 2010 results (Ta-ble 9).
Our ROUGE R-2 and SU4 performanceseems reasonable showing results better than thebaseline but not as good as the best system.Reference System R-2 SU4Baseline - Lead sentences 5.4 8.6Baseline - MEAD?
5.9 9.1Best System 9.6 13.0Pilot Average 6.7 10.1Pilot Minimum 5.6 8.7Pilot Maximum 8.1 11.9?Text summarization system (Radevet al., 2004)Table 9: TAC 2010 ROUGE Scores.ROUGE R-2 results show no significant impactfrom our experimental factors.
This is disappoint-ing as it gives us no handle on how to improveperformance.ROUGE SU4 shows a modest impact for Aggr-Topic (p < 0.025) and the possible impact of JSDDivisor (p < 0.20).
Note that we dropped Or-der and FixedAlpha factors from the model; Or-der because it can only effect sentence order andFixedAlpha because the most important ?
deter-mined automatically by topic analysis did not varymuch from the 0.5 FixedAlpha.
A benefit of drop-ping terms from the model is that we have moredummy factors to estimate error.The ROUGE SU4 means (Table 10) show thesame pattern as for the JSD final sentence, but thedifferences are not as clear cut.
Box and whiskersplots for AggrTopic and JSD Divisor (Figures 4and 5) offer more insight into the AggrTopic andJSD Divisor effects.There is a clear distinction between AggrTopic80Aggr ROUGE JSD ROUGETopic SU4 Divisor SU40.5 10.75 ONE 9.700.75 9.48 SQRT 10.53Standard Error of the mean = 0.32Table 10: Average ROUGE SU4.levels 0.5 and 0.75 with better results at the 0.5level, except for an outlier value of 9.1.
Investiga-tion shows no data coding error and nothing spe-cial about the experimental conditions other thanif uses a JSD Divisor of ONE which also giveslower SU4 scores.
The box and whiskers plots forJSD Divisor effects also suggest a positive effectfor JSD Divisor of SQRT, but the whiskers over-lap the boxes indicating no strong effect.l0.5 0.759.09.510.010.511.011.512.0Alpha Extract ProportionROUGESU4Figure 4: ROUGESU4 by Aggr TopicONE SQRT9.09.510.010.511.011.512.0JSD DivisorROUGESU4Figure 5: ROUGESU4 by JSD DivisorWe had speculated that the final sentence diver-gence might be related to some of the end prod-uct measures.
Indeed, we find that JSD final sen-tence is strongly inversely related to ROUGE SU4as shown by regression analysis (Table 11).
Whilethe residual error of 0.73 indicates that we canonly reliably predict ROUGE SU4 within 1.5 units(for averages of 3 trials), this is still important.A 0.1 reduction in final sentence divergence cor-responds on the average to a 1.4 unit increase inROUGE SU4.Estimate StdErr t Pr(>|t|)Intercept 16.865 1.934 8.721 ?0.0JSDfinal -14.435 4.112 -3.510 0.006Residual standard error: 0.73 on 10 degrees of freedomF-statistic: 12.32 on 1 and 10 DF, p-value: 0.0056Table 11: Regression - ROUGE SU4.We thought Simple Fluency would show an ef-fect for sentence order policy and maybe other fac-tors.
Analysis shows an effect for JSD Divisor(p < 0.05) and possible effects of Order policyand Head lines (p < 0.20).Fluency means (Table 12) show that fluency isbetter for JSD Divisor ONE.
From our experienceof scoring Fluency, this would seem to be becausethe fewer and longer sentences with JSD Divisorof ONE offer fewer chances for disfluencies.
Thebetter Fluency with DATE ordering likely comesfrom fewer out of order or non sequitur sentences,and the better Fluency with NO headlines likelyresults from fewer short ungrammatical headlinesas part of the text.JSD Flu- Order Flu- Head Flu-Div ency ency Lines encyONE 3.95 DATE 3.80 NO 3.80SQRT 3.33 SAL 3.47 YES 3.47Standard Error of the mean = 0.16Table 12: Average Fluency.6 Summary and DiscussionOur pilot studied topic analysis based multi-document extractive summarization using the2010 TAC sample topics.
Our experimental designprocess identified control factors with their defaultand extreme settings, defined intermediate and fi-nal product dependent measures, designed the ex-periment, ran, and analyzed the experiment.We identified an intermediate variable, final se-lected sentence divergence, that could be used as astand-in for the product content measure, ROUGESU4.
We found that using a single dominant topic,instead of an aggregate topic, and using a divisorof the square root of sentence length in sentenceselection, improved final sentence divergence andROUGE SU4.
However, using a divisor of one insentence selection improved fluency of summarieswhich is at odds with the benefit of using squareroot of sentence length to improve content.Our planned experimentation has made obviousand objective the process of describing and im-proving our extractive summarization process.
Itis an extremely useful process and furthermore aprocess that when documented permits sharing ofresults and even duplicating of results by othersworking in this area.81ReferencesJean-Yves Delort and Enrique Alfonseca.
2011.
De-scription of the Google Update Summarizer.
2011TAC Proceedings.Andrew Gelman, John B. Carlin, Hal S. Stern, andDonald B. Rubin.
2004.
Bayesian Data Analysis.Chapman and Hall/CRC, New York, USA.Tom L. Griffiths and Mark Steyvers.
2004.
FindingScientific Topics.
PNAS, 101(Suppl.
1):5228-5235.Aria Haghighi and Lucy Vanderwalde.
2009.Exploring Content Models for Multi-DocumentSummarization.
2009 NACL Conference, HLTProceedings:362-370.Donald E. Knuth.
1997.
The Art of Computer Pro-gramming, Volume 1 (Fundamental Algorithms).Addison Wesley, New York, USA.Chin-Yew Lin.
204.
ROUGE: A Package for Auto-matic Evaluation of Summaries.
ACL 2004 Proceed-ings of Workshop: Text Summarization BranchesOut.Hongyan Liu, Pingan Liu, Wei Heng, and Lei Li.2011.
The CIST Summarization System at TAC2011.
2011 TAC Proceedings.Dragomir Radev, Timothy Allison, Sasha Blair-Goldensohn, John Blitzer, Arda Celebi,Stanko Dimitrov, Elliott Drabek, Ali Hakim,Wai Lam, Danyu Liu, Jahna Otterbacher, Hong Qi,Horacio Saggion, Simone Teufel, Michael Topper,Adam Winkel, and Zhu Zhang.
2004.
MEAD?
A platform for multidocument multilingualtext summarization.
Conference on LanguageResources and Evaluation LREC, Lisbon, Portugal,(May 2004).Rebecca Mason and Eugene Charniak.
2011.
BLLIPat TAC 2011: A General Summarization System fora Guided Summarization Task.
2011 TAC Proceed-ings.Andres K. McCallum.
2002.
MALLET: AMachine Learning for Language Toolkit.http://mallet.cs.umass.edu.Marina Meila?.
2007.
Comparing Clusterings ?
an in-formation based distance.
J. Multivariate Analysis,98(5):873-895.David Mimno, Hanna M. Wallach, Edmund Talley,Miriam Leenders, and Andrew McCallum.
2011.Optimizing Semantic Coherence in Topic Models.2011 EMNLP Conference, Proceedings:262-272.Douglas C. Montgomery.
1997.
Design and Analysisof Experiments.
John Wiley and Sons, New York,USA.Ani Nenkova and Kathleen McKeown.
2011.
Auto-matic Summarization.
Foundations and Trends inInformation Retrieval, 5(2-3):1003-233.Ani Nenkova and Kathleen McKeown.
2012.
A Sur-vey of Text Summarization Techniques.
Mining TextData.
In Charu C. Aggarwal and ChengXiang Zhai(eds.)
Springer.Mark Steyvers and Tom Griffiths.
2007.
ProbabilisiticTopic Models.
Latent Semantic Analysis: A road toMeaning.
In T. Landauer, S. D. McNamara & W.Kintsch (eds.)
Laurence Erlbaum.Task Analysis Conference 2010 ?Summarization Track.
2010.http://www.nist.gov/tac/2010/Summarization/.Task Analysis Conference 2011 ?Summarization Track.
2011.http://www.nist.gov/tac/2011/Summarization/.Yee Whye Teh, Dave Newman, and Max Welling.2007.
Collapsed Variational Inference for HDP.Advances in Neural Information ProcessingSystems:1481-1488.82
