Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the ACL, pages 353?361,Boulder, Colorado, June 2009. c?2009 Association for Computational LinguisticsHierarchical Text Segmentation from Multi-Scale Lexical CohesionJacob EisensteinBeckman Institute for Advanced Science and TechnologyUniversity of IllinoisUrbana, IL 61801jacobe@illinois.eduAbstractThis paper presents a novel unsupervisedmethod for hierarchical topic segmentation.Lexical cohesion ?
the workhorse of unsu-pervised linear segmentation ?
is treated asa multi-scale phenomenon, and formalizedin a Bayesian setting.
Each word token ismodeled as a draw from a pyramid of la-tent topic models, where the structure of thepyramid is constrained to induce a hierarchi-cal segmentation.
Inference takes the formof a coordinate-ascent algorithm, iterating be-tween two steps: a novel dynamic programfor obtaining the globally-optimal hierarchi-cal segmentation, and collapsed variationalBayesian inference over the hidden variables.The resulting system is fast and accurate, andcompares well against heuristic alternatives.1 IntroductionRecovering structural organization from unformat-ted texts or transcripts is a fundamental problemin natural language processing, with applications toclassroom lectures, meeting transcripts, and chat-room logs.
In the unsupervised setting, a varietyof successful systems have leveraged lexical cohe-sion (Halliday and Hasan, 1976) ?
the idea thattopically-coherent segments display consistent lex-ical distributions (Hearst, 1994; Utiyama and Isa-hara, 2001; Eisenstein and Barzilay, 2008).
How-ever, such systems almost invariably focus on linearsegmentation, while it is widely believed that dis-course displays a hierarchical structure (Grosz andSidner, 1986).
This paper introduces the concept ofmulti-scale lexical cohesion, and leverages this ideain a Bayesian generative model for hierarchical topicsegmentation.The idea of multi-scale cohesion is illustratedby the following two examples, drawn from theWikipedia entry for the city of Buenos Aires.There are over 150 city bus lines called Colec-tivos ... Colectivos in Buenos Aires do nothave a fixed timetable, but run from 4 to sev-eral per hour, depending on the bus line andtime of the day.The Buenos Aires metro has six lines, 74 sta-tions, and 52.3 km of track.
An expansionprogram is underway to extend existing linesinto the outer neighborhoods.
Track length isexpected to reach 89 km...The two sections are both part of a high-level seg-ment on transportation.
Words in bold are charac-teristic of the subsections (buses and trains, respec-tively), and do not occur elsewhere in the transporta-tion section; words in italics occur throughout thehigh-level section, but not elsewhere in the article.This paper shows how multi-scale cohesion can becaptured in a Bayesian generative model and ex-ploited for unsupervised hierarchical topic segmen-tation.Latent topic models (Blei et al, 2003) provide apowerful statistical apparatus with which to studydiscourse structure.
A consistent theme is the treat-ment of individual words as draws from multinomiallanguage models indexed by a hidden ?topic?
asso-ciated with the word.
In latent Dirichlet alocation(LDA) and related models, the hidden topic for eachword is unconstrained and unrelated to the hiddentopic of neighboring words (given the parameters).In this paper, the latent topics are constrained to pro-duce a hierarchical segmentation structure, as shownin Figure 1.353?
?w1?...
?wT?8?6 ?7?1 ?2 ?3 ?4 ?5Figure 1: Each word wt is drawn from a mixture of thelanguage models located above t in the pyramid.These structural requirements simplify inference,allowing the language models to be analyticallymarginalized.
The remaining hidden variables arethe scale-level assignments for each word token.Given marginal distributions over these variables, itis possible to search the entire space of hierarchicalsegmentations in polynomial time, using a novel dy-namic program.
Collapsed variational Bayesian in-ference is then used to update the marginals.
Thisapproach achieves high quality segmentation onmultiple levels of the topic hierarchy.Source code is available at http://people.csail.mit.edu/jacobe/naacl09.html.2 Related WorkThe use of lexical cohesion (Halliday and Hasan,1976) in unsupervised topic segmentation dates backto Hearst?s seminal TEXTTILING system (1994).Lexical cohesion was placed in a probabilistic(though not Bayesian) framework by Utiyama andIsahara (2001).
The application of Bayesian topicmodels to text segmentation was investigated firstby Blei and Moreno (2001) and later by Purver etal.
(2006), using HMM-like graphical models forlinear segmentation.
Eisenstein and Barzilay (2008)extend this work by marginalizing the languagemodels using the Dirichlet compound multinomialdistribution; this permits efficient inference to beperformed directly in the space of segmentations.All of these papers consider only linear topic seg-mentation; we introduce multi-scale lexical cohe-sion, which posits that the distribution of somewords changes slowly with high-level topics, whileothers change rapidly with lower-level subtopics.This gives a principled mechanism to model hier-archical topic segmentation.The literature on hierarchical topic segmentationis relatively sparse.
Hsueh et al (2006) describe asupervised approach that trains separate classifiersfor topic and sub-topic segmentation; more relevantfor the current work is the unsupervised methodof Yaari (1997).
As in TEXTTILING, cohesion ismeasured using cosine similarity, and agglomerativeclustering is used to induce a dendrogram over para-graphs; the dendrogram is transformed into a hier-archical segmentation using a heuristic algorithm.Such heuristic approaches are typically brittle, asthey include a number of parameters that must behand-tuned.
These problems can be avoided byworking in a Bayesian probabilistic framework.We note two orthogonal but related approachesto extracting nonlinear discourse structures fromtext.
Rhetorical structure theory posits a hierarchi-cal structure of discourse relations between spans oftext (Mann and Thompson, 1988).
This structure isricher than hierarchical topic segmentation, and thebase level of analysis is typically more fine-grained?
at the level of individual clauses.
Unsupervisedapproaches based purely on cohesion are unlikely tosucceed at this level of granularity.Elsner and Charniak (2008) propose the task ofconversation disentanglement from internet chat-room logs.
Unlike hierarchical topic segmentation,conversational threads may be disjoint, with un-related threads interposed between two utterancesfrom the same thread.
Elsner and Charniak present asupervised approach to this problem, but the devel-opment of cohesion-based unsupervised methods isan interesting possibility for future work.3 ModelTopic modeling is premised on a generative frame-work in which each word wt is drawn from a multi-nomial ?yt , where yt is a hidden topic indexing thelanguage model that generates wt.
From a modelingstandpoint, linear topic segmentation merely addsthe constraint that yt ?
{yt?1, yt?1 + 1}.
Segmen-tations that draw boundaries so as to induce com-pact, low-entropy language models will achieve a354high likelihood.
Thus topic models situate lexicalcohesion in a probabilistic setting.For hierarchical segmentation, we take the hy-pothesis that lexical cohesion is a multi-scale phe-nomenon.
This is represented with a pyramid of lan-guage models, shown in Figure 1.
Each word may bedrawn from any language model above it in the pyra-mid.
Thus, the high-level language models will berequired to explain words throughout large parts ofthe document, while the low-level language modelswill be required to explain only a local set of words.A hidden variable zt indicates which level is respon-sible for generating the word wt.Ideally we would like to choose the segmentationy?
= argmaxyp(w|y)p(y).
However, we must dealwith the hidden language models ?
and scale-levelassignments z.
The language models can be inte-grated out analytically (Section 3.1).
Given marginallikelihoods for the hidden variables z, the globallyoptimal segmentation y?
can be found using a dy-namic program (Section 4.1).
Given a segmentation,we can estimate marginals for the hidden variables,using collapsed variational inference (Section 4.2).We iterate between these procedures in an EM-likecoordinate-ascent algorithm (Section 4.4) until con-vergence.3.1 Language modelsWe begin the formal presentation of the model withsome notation.
Each word wt is modeled as a singledraw from a multinomial language model ?j .
Thelanguage models in turn are drawn from symmetricDirichlet distributions with parameter ?.
The num-ber of language models is written K; the number ofwords is W ; the length of the document is T ; andthe depth of the hierarchy is L.For hierarchical segmentation, the vector yt indi-cates the segment index of t at each level of the topichierarchy; the specific level of the hierarchy respon-sible for wt is given by the hidden variable zt.
Thus,y(zt)t is the index of the language model that gener-ates wt.With these pieces in place, we can write the ob-servation likelihood,p(w|y, z,?)
=T?tp(wt|?y(zt)t )=K?j?
{t:y(zt)t =j}p(wt|?j),where we have merely rearranged the product togroup terms that are drawn from the same languagemodel.
As the goal is to obtain the hierarchical seg-mentation and not the language models, the searchspace can be reduced by marginalizing ?.
Thederivation is facilitated by a notational convenience:xj represents the lexical counts induced by the setof words {wt : y(zt)t = j}.p(w|y, z, ?)
=K?j?d?jp(?j |?
)p(xj |?j)=K?jpdcm(xj ;?)=K?j?(W?)?
(?Wi xji + ?)W?i?
(xji + ?)?(?)
.
(1)Here, pdcm indicates the Dirichlet compoundmultinomial distribution (Madsen et al, 2005),which is the closed form solution to the integral overlanguage models.
Also known as the multivariatePolya distribution, the probability density functioncan be computed exactly as a ratio of gamma func-tions.
Here we use a symmetric Dirichlet prior ?,though asymmetric priors can easily be applied.Thus far we have treated the hidden variablesz as observed.
In fact we will compute approxi-mate marginal probabilities Qzt(zt), written ?t` ?Qzt(zt = `).
Writing ?x?Qz for the expectation of xunder distribution Qz , we approximate,?pdcm(xj ;?
)?Qz ?
pdcm(?xj?Qz ;?
)?xj(i)?Qz =?{t:j?yt}L?`?
(wt = i)?
(y(`)t = j)?t`,355where xj(i) indicates the count for word type i gen-erated from segment j.
In the outer sum, we con-sider all t for possibly drawn from segment j. Theinner sum goes over all levels of the pyramid.
Thedelta functions take the value one if the enclosedBoolean expression is true and zero otherwise, sowe are adding the fractional counts ?t` only whenwt = i and y(`)t = j.3.2 Prior on segmentationsMaximizing the joint probability p(w,y) =p(w|y)p(y) leaves the term p(y) as a prior on seg-mentations.
This prior can be used to favor segmen-tations with the desired granularity.
Consider a priorof the form p(y) = ?L`=1 p(y(`)|y(`?1)); for nota-tional convenience, we introduce a base level suchthat y(0)t = t, where every word is a segmentationpoint.
At every level ` > 0, the prior is a Markovprocess, p(y(`)|y(`?1)) = ?Tt p(y(`)t |y(`)t?1,y(`?1)).The constraint y(`)t ?
{y(`)t?1, y(`)t?1 + 1} ensures alinear segmentation at each level.
To enforce hierar-chical consistency, each y(`)t can be a segmentationpoint only if t is also a segmentation point at thelower level ` ?
1.
Zero probability is assigned tosegmentations that violate these constraints.To quantify the prior probability of legal segmen-tations, assume a set of parameters d`, indicatingthe expected segment duration at each level.
If tis a valid potential segmentation point at level `(i.e., y(`?1)t = 1 + y(`?1)t?1 ), then the prior probabil-ity of a segment transition is r` = d`?1/d`, withd0 = 1.
If there are N segments in level ` andM ?
N segments in level ` ?
1, then the priorp(y(`)|y(`?1)) = rN` (1 ?
r`)M?N , as long as thehierarchical segmentation constraint is obeyed.For the purposes of inference it will be prefer-able to have a prior that decomposes over levels andsegments.
In particular, we do not want to have tocommit to a particular segmentation at level ` be-fore segmenting level ` + 1.
The above prior canbe approximated by replacing M with its expecta-tion ?M?d`?1 = T/d`?1.
Then a single segmentranging from wu to wv (inclusive) will contributelog r` + v?ud`?1 log(1?
r`) to the log of the prior.4 InferenceThis section describes the inference for the segmen-tation y, the approximate marginals QZ , and the hy-perparameter ?.4.1 Dynamic programming for hierarchicalsegmentationWhile the model structure is reminiscent of a facto-rial hidden Markov model (HMM), there are impor-tant differences that prevent the direct application ofHMM inference.
Hidden Markov models assumethat the parameters of the observation likelihood dis-tributions are available directly, while we marginal-ize them out.
This has the effect of introducing de-pendencies throughout the state space: the segmentassignment for each yt contributes to lexical countswhich in turn affect the observation likelihoods formany other t?.
However, due to the left-to-right na-ture of segmentation, efficient inference of the opti-mal hierarchical segmentation (given the marginalsQZ) is still possible.Let B(`)[u, v] represent the log-likelihood ofgrouping together all contiguous words wu .
.
.
wv?1at level ` of the segmentation hierarchy.
Using xtto indicate a vector of zeros with one at the positionwt, we can express B more formally:B(`)[u, v] = log pdcm( v?t=uxt?t`)+ log r` + v ?
u?
1d`?1 log(1?
r`).The last two terms are from the prior p(y), as ex-plained in Section 3.2.
The value of B(`)[u, v] iscomputed for all u, all v > u, and all `.Next, we compute the log-likelihood of the op-timal segmentation, which we write as A(L)[0, T ].This matrix can be filled in recursively:A(`)[u, v] = maxu?t<vB(`)[t, v] +A(`?1)[t, v] +A(`)[u, t].The first term adds in the log probability of thesegment from t to v at level `.
The second term re-turns the best score for segmenting this same intervalat a more detailed level of segmentation.
The thirdterm recursively segments the interval from u to t atthe same level `.
The boundary case A(`)[u, u] = 0.3564.1.1 Computational complexityThe sizes of A and B are each O(LT 2).
The ma-trix A can be constructed by iterating through thelayers and then iterating: u from 1 to T ; v from u+1to T ; and t from u to v + 1.
Thus, the time cost forfilling A is O(LT 3).
For computing the observationlikelihoods in B, the time complexity isO(LT 2W ),where W is the size of the vocabulary ?
by keepingcumulative lexical counts, we can compute B[u, v]without iterating from u to v.Eisenstein and Barzilay (2008) describe a dy-namic program for linear segmentation with aspace complexity of O(T ) and time complexities ofO(T 2) to compute the A matrix and O(TW ) to fillthe B matrix.1 Thus, moving to hierarchical seg-mentation introduces a factor of TL to the complex-ity of inference.4.1.2 DiscussionIntuitively, efficient inference is possible becausethe location of each segment boundary affects thelikelihood of only the adjoining segments at thesame level of the hierarchy, and their ?children?
atthe lower levels of the hierarchy.
Thus, the observa-tion likelihood at each level decomposes across thesegments of the level.
This is due to the left-to-rightnature of segmentation ?
in general it is not possibleto marginalize the language models and still performefficient inference in HMMs.
The prior (Section 3.2)was designed to decompose across segments ?
if, forexample, p(y) explicitly referenced the total numberof segments, inference would be more difficult.A simpler inference procedure would be a greedyapproach that makes a fixed decision about the top-level segmentation, and then applies recursion toachieve segmentation at the lower levels.
The greedyapproach will not be optimal if the best top-levelsegmentation leads to unsatisfactory results at thelower levels, or if the lower levels could help todisambiguate high-level segmentation.
In contrast,the algorithm presented here maximizes the overallscore across all levels of the segmentation hierarchy.1The use of dynamic programming for linear topic segmen-tation goes back at least to (Heinonen, 1998); however, we areaware of no prior work on dynamic programming for hierarchi-cal segmentation.4.2 Scale-level marginalsThe hidden variable zt represents the level of thesegmentation hierarchy from which the word wt isdrawn.
Given language models ?, each wt canbe thought of as a draw from a Bayesian mixturemodel, with zt as the index of the component thatgenerates wt.
However, as we are marginalizingthe language models, standard mixture model infer-ence techniques do not apply.
One possible solu-tion would be to instantiate the maximum a posteri-ori language models after segmenting, but we wouldprefer not to have to commit to specific languagemodels.
Collapsed Gibbs sampling (Griffiths andSteyvers, 2004) is another possibility, but sampling-based solutions may not be ideal from a performancestandpoint.Recent papers by Teh et al (2007) and Sung etal.
(2008) point to an appealing alternative: col-lapsed variational inference (called latent-state vari-ational Bayes by Sung et al).
Collapsed variationalinference integrates over the parameters (in thiscase, the language models) and computes marginaldistributions for the latent variables, Qz.
However,due to the difficulty of computing the expectationof the normalizing term, these marginal probabili-ties are available only in approximation.More formally, we wish to compute the approx-imate distribution Qz(z) = ?Tt Qzt(zt), factoriz-ing across all latent variables.
As is typical in vari-ational approaches, we fit this distribution by opti-mizing a lower bound on the data marginal likeli-hood p(w, z|y) ?
we condition on the segmentationy because we are treating it as fixed in this part ofthe inference.
The lower bound can be optimized byiteratively setting,Qzt(zt) ?
exp{?logP (x, z|y)?
?Qzt},indicating the expectation under Qz?t for all t?
6= t.Due to the couplings across z, it is not possibleto compute this expectation directly, so we use thefirst-order approximation described in (Sung et al,2008).
In this approximation, the value Qzt(zt = `)?
which we abbreviate as ?t` ?
takes the form ofthe likelihood of the observation wt, given a mod-ified mixture model.
The parameters of the mixturemodel are based on the priors and the counts of w357and ?
for all t?
6= t:?t` ?
?`x?
?t` (wt)?Wi x?
?t` (i)(2)x?
?t` (i) = ?`(i) +?t?
6=t?t?`?(wt?
= i).
(3)The first term in equation 2 is the set of compo-nent weights ?`, which are fixed at 1/L for all `.
Thefraction represents the posterior estimate of the lan-guage models: standard Dirichlet-multinomial con-jugacy gives a sum of counts plus a Dirichlet prior(equation 3).
Thus, the form of the update is ex-tremely similar to collapsed Gibbs sampling, exceptthat we maintain the full distribution over zt ratherthan sampling a specific value.
The derivation of thisupdate is beyond the scope of this paper, but is sim-ilar to the mixture of Bernoullis model presented inSection 5 of (Sung et al, 2008).Iterative updates of this form are applied until thechange in the lower bound is less than 10?3.
Thisprocedure appears at step 5a of algorithm 1.4.3 Hyperparameter estimationThe inference procedure defined here includes twoparameters: ?, the symmetric Dirichlet prior on thelanguage models; and d, the expected segment du-rations.
The granularity of segmentation is consid-ered to be a user-defined characteristic, so there isno ?right answer?
for how to set this parameter.
Wesimply use the oracle segment durations, and pro-vide the same oracle to the baseline methods wherepossible.
As discussed in Section 6, this parameterhad little effect on system performance.The ?
parameter controls the expected sparsity ofthe induced language models; it will be set automat-ically.
Given a segmentation y and hidden-variablemarginals ?, we can maximize p(?,w|y, ?)
=pdcm(w|y, ?, ?)p(?)
through gradient descent.
TheDirichlet compound multinomial has a tractable gra-dient, which can be computed using scaled counts,x?j = ?t:y(zt)t =j ?tjxt (Minka, 2003).
The scaledcounts are taken for each segment j across the entiresegmentation hierarchy.
The likelihood p(x?|?)
thenhas the same form as equation 1, with the xji termsreplaced by x?ji.
The gradient of the log-likelihoodAlgorithm 1 Complete segmentation inference1.
Input text w; expected durations d.2.
?
?
INITIALIZE-GAMMA(w)3.
y??
EQUAL-WIDTH-SEG(w, d)4.
??
.15.
Do(a) ?
?
ESTIMATE-GAMMA(y?,w, ?, ?
)(b) ??
ESTIMATE-ALPHA(y?,w, ?
)(c) y?
SEGMENT(w, ?, ?, d)(d) If y = y?
then return y(e) Else y??
yis thus a sum across segments,d`/d?
=K?jW (?(W?)??(?))+W?i?
(x?ji + ?)??
(W?+W?ix?ji).Here, ?
indicates the digamma function, whichis the derivative of the log gamma function.
Theprior p(?)
takes the form of a Gamma distributionwith parameters G(1, 1), which has the effect of dis-couraging large values of ?.
With these parame-ters, the gradient of the Gamma distribution with re-spect to ?
is negative one.
To optimize ?, we inter-pose an epoch of L-BFGS (Liu and Nocedal, 1989)optimization after maximizing ?
(Step 5b of algo-rithm 1).4.4 Combined inference procedureThe final inference procedure alternates between up-dating the marginals ?, the Dirichlet prior ?, and theMAP segmentation y?.
Since the procedure makeshard decisions on ?
and the segmentations y, itcan be thought of as a form of Viterbi expectation-maximization (EM).
When a repeated segmentationis encountered, the procedure terminates.
Initializa-tion involves constructing a segmentation y?
in whicheach level is segmented uniformly, based on the ex-pected segment duration d`.
The hidden variablemarginals ?
are initialized randomly.
While thereis no guarantee of finding the global maximum, lit-tle sensitivity to the random initialization of ?
wasobserved in preliminary experiments.The dynamic program described in this sectionachieves polynomial time complexity, but O(LT 3)358can still be slow when T is the number of word to-kens in a large document such as a textbook.
Forthis reason, we only permit segment boundaries tobe placed at gold-standard sentence boundaries.
Theonly change to the algorithm is that the tables Aand B need contain only cells for each sentencerather than for each word token ?
hidden variablemarginals are still computed for each word token.Implemented in Java, the algorithm runs in roughlyfive minutes for a document with 1000 sentences ona dual core 2.4 GHz machine.5 Experimental SetupCorpora The dataset for evaluation is drawn froma medical textbook (Walker et al, 1990).2 The textcontains 17083 sentences, segmented hierarchicallyinto twelve high-level parts, 150 chapters, and 520sub-chapter sections.
Evaluation is performed sep-arately on each of the twelve parts, with the task ofcorrectly identifying the chapter and section bound-aries.
Eisenstein and Barzilay (2008) use the samedataset to evaluate linear topic segmentation, thoughthey evaluated only at the level of sections, givengold standard chapter boundaries.Practical applications of topic segmentation typ-ically relate to more informal documents such asblogs or speech transcripts (Hsueh et al, 2006), asformal texts such as books already contain segmen-tation markings provided by the author.
The premiseof this evaluation is that textbook corpora provide areasonable proxy for performance on less structureddata.
However, further clarification of this point isan important direction for future research.Metrics All experiments are evaluated in termsof the commonly-used Pk and WindowDiff met-rics (Pevzner and Hearst, 2002).
Both metrics pass awindow through the document, and assess whetherthe sentences on the edges of the window are prop-erly segmented with respect to each other.
Win-dowDiff is stricter in that it requires that the numberof intervening segments between the two sentencesbe identical in the hypothesized and the referencesegmentations, while Pk only asks whether the twosentences are in the same segment or not.
This eval-2The full text of this book is available for free download athttp://onlinebooks.library.upenn.edu.uation uses source code provided by Malioutov andBarzilay (2006).Experimental system The joint hierarchicalBayesian model described in this paper is calledHIERBAYES.
It performs a three-level hierarchicalsegmentation, in which the lowest level is for sub-chapter sections, the middle level is for chapters, andthe top level spans the entire part.
This top-level hasthe effect of limiting the influence of words that arecommon throughout the document.Baseline systems As noted in Section 2, there islittle related work on unsupervised hierarchical seg-mentation.
However, a straightforward baseline isa greedy approach: first segment at the top level,and then recursively feed each top-level segment tothe segmenter again.
Any linear segmenter can beplugged into this baseline as a ?black box.
?To isolate the contribution of joint inference, thegreedy framework can be combined with a one-levelversion of the Bayesian segmentation algorithm de-scribed here.
This is equivalent to BAYESSEG,which achieved the best reported performance on thelinear segmentation of this same dataset (Eisensteinand Barzilay, 2008).
The hierarchical segmenterbuilt by placing BAYESSEG in a greedy algorithmis called GREEDY-BAYES.To identify the contribution of the Bayesiansegmentation framework, we can plug in alter-native linear segmenters.
Two frequently-citedsystems are LCSEG (Galley et al, 2003) andTEXTSEG (Utiyama and Isahara, 2001).
LC-SEG optimizes a metric of lexical cohesion basedon lexical chains.
TEXTSEG employs a probabilis-tic segmentation objective that is similar to ours,but uses maximum a posteriori estimates of the lan-guage models, rather than marginalizing them out.Other key differences are that they set ?
= 1, anduse a minimum description length criterion to deter-mine segmentation granularity.
Both of these base-lines were run using their default parametrization.Finally, as a minimal baseline, UNIFORM pro-duces a hierarchical segmentation with the groundtruth number of segments per level and uniform du-ration per segment at each level.Preprocessing The Porter (1980) stemming algo-rithm is applied to group equivalent lexical items.
Aset of stop-words is also removed, using the same359chapter section average# segs Pk WD # segs Pk WD Pk WDHIERBAYES 5.0 .248 .255 8.5 .312 .351 .280 .303GREEDY-BAYES 19.0 .260 .372 19.5 .275 .340 .268 .356GREEDY-LCSEG 7.8 .256 .286 52.2 .351 .455 .304 .371GREEDY-TEXTSEG 11.5 .251 .277 88.4 .473 .630 .362 .454UNIFORM 12.5 .487 .491 43.3 .505 .551 .496 .521Table 1: Segmentation accuracy and granularity.
Both the Pk and WindowDiff (WD) metrics are penalties, so lowerscores are better.
The # segs columns indicate the average number of segments at each level; the gold standardsegmentation granularity is given in the UNIFORM row, which obtains this granularity by construction.list originally employed by several competitive sys-tems (Utiyama and Isahara, 2001).6 ResultsTable 1 presents performance results for the jointhierarchical segmenter and the three greedy base-lines.
As shown in the table, the hierarchical systemachieves the top overall performance on the harsherWindowDiff metric.
In general, the greedy seg-menters each perform well at one of the two levelsand poorly at the other level.
The joint hierarchicalinference of HIERBAYES enables it to achieve bal-anced performance at the two levels.The GREEDY-BAYES system achieves a slightlybetter average Pk than HIERBAYES, but has a verylarge gap between its Pk and WindowDiff scores.The Pk metric requires only that the system cor-rectly classify whether two points are in the sameor different segments, while the WindowDiff metricinsists that the exact number of interposing segmentsbe identified correctly.
Thus, the generation of spu-rious short segments may explain the gap betweenthe metrics.LCSEG and TEXTSEG use heuristics to deter-mine segmentation granularity; even though thesemethods did not score well in terms of segmentationaccuracy, they were generally closer to the correctgranularity.
In the Bayesian methods, granularityis enforced by the Markov prior described in Sec-tion 3.2.
This prior was particularly ineffective forGREEDY-BAYES, which gave nearly the same num-ber of segments at both levels, despite the differentsettings of the expected duration parameter d.The Dirichlet prior ?
was selected automatically,but informal experiments with manual settings sug-gest that this parameter exerts a stronger influenceon segmentation granularity.
Low settings reflect anexpectation of sparse lexical counts and thus encour-age short segments, while high settings reflect an ex-pectation of evenly-distributed counts and thus leadto long segments.
Further investigation is neededon how best to control segmentation granularity in aBayesian setting.7 DiscussionWhile it is widely agreed that language often dis-plays hierarchical topic structure (Grosz, 1977),there have been relatively few attempts to extractsuch structure automatically.
This paper showsthat the lexical features that have been successfullyexploited in linear segmentation can also be usedto extract a hierarchical segmentation, due to thephenomenon of multi-scale lexical cohesion.
TheBayesian methodology offers a principled proba-bilistic formalization of multi-scale cohesion, yield-ing an accurate and fast segmentation algorithm witha minimal set of tunable parameters.It is interesting to consider how multi-scale seg-mentation might be extended to finer-grain seg-ments, such as paragraphs.
The lexical counts at theparagraph level will be sparse, so lexical cohesionalone is unlikely to be sufficient.
Rather it may benecessary to model discourse connectors and lexicalsemantics explicitly.
The development of more com-prehensive Bayesian models for discourse structureseems an exciting direction for future research.Acknowledgments Thanks to Michel Galley, IgorMalioutov, and Masao Utiyama for making their topicsegmentation systems publicly available, and to theanonymous reviewers for useful feedback.
This researchis supported by the Beckman Postdoctoral Fellowship.360ReferencesDavid M. Blei and Pedro J. Moreno.
2001.
Topic seg-mentation with an aspect hidden markov model.
InSIGIR, pages 343?348.David M. Blei, Andrew Y. Ng, and Michael I. Jordan.2003.
Latent Dirichlet alocation.
Journal of MachineLearning Research, 3:993?1022.Jacob Eisenstein and Regina Barzilay.
2008.
Bayesianunsupervised topic segmentation.
In Proceedings ofEMNLP.Micha Elsner and Eugene Charniak.
2008.
You Talk-ing to Me?
A Corpus and Algorithm for ConversationDisentanglement.
In Proceedings of ACL.Michel Galley, Katheen McKeown, Eric Fosler-Lussier,and Hongyan Jing.
2003.
Discourse segmentation ofmulti-party conversation.
pages 562?569.T.L.
Griffiths and M. Steyvers.
2004.
Finding scientifictopics.Barbara Grosz and Candace Sidner.
1986.
Attention,intentions, and the structure of discourse.
Computa-tional Linguistics, 12(3):175?204.Barbara Grosz.
1977.
The representation and use of fo-cus in dialogue understanding.
Technical Report 151,Artificial Intelligence Center, SRI International.M.
A. K. Halliday and Ruqaiya Hasan.
1976.
Cohesionin English.
Longman.Marti A. Hearst.
1994.
Multi-paragraph segmentation ofexpository text.
In Proceedings of ACL, pages 9?16.Oskari Heinonen.
1998.
Optimal Multi-Paragraph TextSegmentation by Dynamic Programming.
In Proceed-ings of ACL, pages 1484?1486.P.Y.
Hsueh, J. Moore, and S. Renals.
2006.
Automaticsegmentation of multiparty dialogue.
In Proccedingsof EACL.Dong C. Liu and Jorge Nocedal.
1989.
On the limitedmemory BFGS method for large scale optimization.Mathematical Programming, 45:503?528.R.E.
Madsen, D. Kauchak, and C. Elkan.
2005.
Model-ing word burstiness using the Dirichlet distribution.
InProceedings of ICML.Igor Malioutov and Regina Barzilay.
2006.
Minimumcut model for spoken lecture segmentation.
In Pro-ceedings of ACL, pages 25?32.William C. Mann and Sandra A. Thompson.
1988.Rhetorical structure theory: Toward a functional the-ory of text organization.
Text, 8:243?281.Thomas P. Minka.
2003.
Estimating a dirichlet distri-bution.
Technical report, Massachusetts Institute ofTechnology.Lev Pevzner and Marti A. Hearst.
2002.
A critique andimprovement of an evaluation metric for text segmen-tation.
Computational Linguistics, 28(1):19?36.M.
F. Porter.
1980.
An algorithm for suffix stripping.Program, 14:130?137.M.
Purver, T.L.
Griffiths, K.P.
Ko?rding, and J.B. Tenen-baum.
2006.
Unsupervised topic modelling for multi-party spoken discourse.
In Proceedings of ACL, pages17?24.Jaemo Sung, Zoubin Ghahramani, and Sung-Yang Bang.2008.
Latent-space variational bayes.
IEEE Trans-actions on Pattern Analysis and Machine Intelligence,30(12):2236?2242, Dec.Y.W.
Teh, D. Newman, and M. Welling.
2007.
A Col-lapsed Variational Bayesian Inference Algorithm forLatent Dirichlet Allocation.
In NIPS, volume 19, page1353.Masao Utiyama and Hitoshi Isahara.
2001.
A statisticalmodel for domain-independent text segmentation.
InProceedings of ACL, pages 491?498.H.
Kenneth Walker, W. Dallas Hall, and J. Willis Hurst,editors.
1990.
Clinical Methods : The History, Physi-cal, and Laboratory Examinations.
Butterworths.Y.
Yaari.
1997.
Segmentation of Expository Texts byHierarchical Agglomerative Clustering.
In Recent Ad-vances in Natural Language Processing.361
