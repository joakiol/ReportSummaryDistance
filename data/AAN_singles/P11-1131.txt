Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 1308?1317,Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational LinguisticsGappy Phrasal Alignment by AgreementMohit Bansal?UC Berkeley, CS Divisionmbansal@cs.berkeley.eduChris QuirkMicrosoft Researchchrisq@microsoft.comRobert C. MooreGoogle Researchrobert.carter.moore@gmail.comAbstractWe propose a principled and efficient phrase-to-phrase alignment model, useful in machinetranslation as well as other related natural lan-guage processing problems.
In a hidden semi-Markov model, word-to-phrase and phrase-to-word translations are modeled directly bythe system.
Agreement between two direc-tional models encourages the selection of par-simonious phrasal alignments, avoiding theoverfitting commonly encountered in unsu-pervised training with multi-word units.
Ex-panding the state space to include ?gappyphrases?
(such as French ne ?
pas) makes thealignment space more symmetric; thus, it al-lows agreement between discontinuous align-ments.
The resulting system shows substantialimprovements in both alignment quality andtranslation quality over word-based HiddenMarkov Models, while maintaining asymptot-ically equivalent runtime.1 IntroductionWord alignment is an important part of statisti-cal machine translation (MT) pipelines.
Phrasetables containing pairs of source and target lan-guage phrases are extracted from word alignments,forming the core of phrase-based statistical ma-chine translation systems (Koehn et al, 2003).Most syntactic machine translation systems extractsynchronous context-free grammars (SCFGs) fromaligned syntactic fragments (Galley et al, 2004;Zollmann et al, 2006), which in turn are de-rived from bilingual word alignments and syntactic?Author was a summer intern at Microsoft Research duringthis project.FrenchEnglishvoudrais voyager par chemin de ferwould like traveling by railroadne pasnotFigure 1: French-English pair with complex word alignment.parses.
Alignment is also used in various other NLPproblems such as entailment, paraphrasing, questionanswering, summarization and spelling correction.A limitation to word-based alignment is undesir-able.
As seen in the French-English example in Fig-ure 1, many sentence pairs are naturally aligned withmulti-word units in both languages (chemin de fer;would ?
like, where ?
indicates a gap).
Much workhas addressed this problem: generative models fordirect phrasal alignment (Marcu and Wong, 2002),heuristic word-alignment combinations (Koehn etal., 2003; Och and Ney, 2003), models with pseudo-word collocations (Lambert and Banchs, 2006; Maet al, 2007; Duan et al, 2010), synchronous gram-mar based approaches (Wu, 1997), etc.
Most have alarge state-space, using constraints and approxima-tions for efficient inference.We present a new phrasal alignment model basedon the hidden Markov framework (Vogel et al,1996).
Our approach is semi-Markov: each state cangenerate multiple observations, representing word-to-phrase alignments.
We also augment the statespace to include contiguous sequences.
This cor-responds to phrase-to-word and phrase-to-phrasealignments.
We generalize alignment by agreement(Liang et al, 2006) to this space, and find that agree-ment discourages EM from overfitting.
Finally, wemake the alignment space more symmetric by in-cluding gappy (or non-contiguous) phrases.
This al-lows agreement to reinforce non-contiguous align-1308f1f2f3e1 e2 e3 f1 f2 f3e1e2e3Observations??
?States?HMM(E|F) HMM(F|E)Figure 2: The model of E given F can represent the phrasalalignment {e1, e2} ?
{f1}.
However, the model of F givenE cannot: the probability mass is distributed between {e1} ?
{f1} and {e2} ?
{f1}.
Agreement of the forward and back-ward HMM alignments tends to place less mass on phrasal linksand greater mass on word-to-word links.ments, such English not to French ne ?
pas.
Prun-ing the set of allowed phrases preserves the timecomplexity of the word-to-word HMM alignmentmodel.1.1 Related WorkOur first major influence is that of conditionalphrase-based models.
An early approach by Dengand Byrne (2005) changed the parameterization ofthe traditional word-based HMM model, modelingsubsequent words from the same state using a bi-gram model.
However, this model changes only theparameterization and not the set of possible align-ments.
More closely related are the approachesof Daume?
III and Marcu (2004) and DeNero etal.
(2006), which allow phrase-to-phrase alignmentsbetween the source and target domain.
As DeN-ero warns, though, an unconstrained model mayoverfit using unusual segmentations.
Interestingly,the phrase-based hidden semi-Markov model ofAndre?s-Ferrer and Juan (2009) does not seem toencounter these problems.
We suspect two maincauses: first, the model interpolates with Model 1(Brown et al, 1994), which may help prevent over-fitting, and second, the model is monotonic, whichscreens out many possible alignments.
Monotonic-ity is generally undesirable, though: almost all par-allel sentences exhibit some reordering phenomena,even when languages are syntactically very similar.The second major inspiration is alignment byagreement by Liang et al (2006).
Here, soft inter-section between the forward (F?E) and backward(E?F) alignments during parameter estimation pro-duces better word-to-word correspondences.
Thisunsupervised approach produced alignments withincredibly low error rates on French-English, thoughonly moderate gains in end-to-end machine transla-tion results.
Likely this is because the symmetricportion of the HMM space contains only single wordto single word links.
As shown in Figure 2, in orderto retain the phrasal link f1 ?
e1, e2 after agree-ment, we need the reverse phrasal link e1, e2 v f1in the backward direction.
However, this is not pos-sible in a word-based HMM where each observa-tion must be generated by a single state.
Agreementtends to encourage 1-to-1 alignments with very highprecision and but lower recall.
As each word align-ment acts as a constraint on phrase extraction, thephrase-pairs obtained from those alignments havehigh recall and low precision.2 Gappy Phrasal AlignmentOur goal is to unify phrasal alignment and align-ment by agreement.
We use a phrasal hidden semi-Markov alignment model, but without the mono-tonicity requirement of Andre?s-Ferrer and Juan(2009).
Since phrases may be used in both the stateand observation space of both sentences, agreementduring EM training no longer penalizes phrasal linkssuch as those in Figure 2.
Moreover, the benefits ofagreement are preserved: meaningful phrasal linksthat are likely in both directions of alignment will bereinforced, while phrasal links likely in only one di-rection will be discouraged.
This avoids segmenta-tion problems encountered by DeNero et al (2006).Non-contiguous sequences of words present anadditional challenge.
Even a semi-Markov modelwith phrases can represent the alignment betweenEnglish not and French ne ?
pas in one directiononly.
To make the model more symmetric, we ex-tend the state space to include gappy phrases aswell.1 The set of alignments in each model becomessymmetric, though the two directions model gappyphrases differently.
Consider not and ne ?
pas:when predicting French given English, the align-ment corresponds to generating multiple distinct ob-1We only allow a single gap with one word on each end.This is sufficient for the vast majority of the gapped phenomenathat we have seen in our training data.1309voudraisvoyagerparchemindeferwouldliketravelingby railroadCwouldliketravelingbyrailroadvoudraisvoyagerparcheminde fernotpasnenotne pasObservations?States?Observations?States?Figure 3: Example English-given-French and French-given-English alignments of the same sentence pair using the Hidden Semi-Markov Model (HSMM) for gapped-phrase-to-phrase alignment.
It allows the state side phrases (denoted by vertical blocks),observation side phrases (denoted by horizontal blocks), and state-side gaps (denoted by discontinuous blocks in the same columnconnected by a hollow vertical ?bridge?).
Note both directions can capture the desired alignment for this sentence pair.servations from the same state; in the other direction,the word not is generated by a single gappy phrasene ?
pas.
Computing posteriors for agreement issomewhat complicated, so we resort to an approx-imation described later.
Exact inference retains alow-order polynomial runtime; we use pruning to in-crease speed.2.1 Hidden Markov Alignment ModelsOur model can be seen as an extension of the stan-dard word-based Hidden Markov Model (HMM)used in alignment (Vogel et al, 1996).
Toground the discussion, we first review the struc-ture of that model.
This generative model hasthe form p(O|S) =?A p(A,O|S), where S =(s1, .
.
.
, sI) ?
??
is a sequence of words from avocabulary ?
; O = (o1, .
.
.
, oJ) ?
??
is a sequencefrom vocabulary ?
; and A = (a1, .
.
.
, aJ) is thealignment between the two sequences.
Since somewords are systematically inserted during translation,the target (state) word sequence is augmented witha special NULL word.
To retain the position of thelast aligned word, the state space contains I copiesof the NULL word, one for each position (Och andNey, 2003).
The alignment uses positive positionsfor words and negative positions for NULL states, soaj ?
{1..I} ?
{?1..?
I}, and si = NULL if i < 0.It uses the following generative procedure.
Firstthe length of the observation sequence is selectedbased on pl(J |I).
Then for each observation posi-tion, the state is selected based on the prior state: anull state with probability p0, or a non-null state atposition aj with probability (1 ?
p0) ?
pj(aj |aj?1)where pj is a jump distribution.
Finally the observa-tion word oj at that position is generated with prob-ability pt(oj |saj ), where pt is an emission distribu-tion:p(A,O|S) = pl(J |I)J?j=1pj(aj |aj?1)pt(oj |saj )pj(a|a?)
={(1?
p0) ?
pd(a?
|a?|) a > 0p0 ?
?
(|a|, |a?|) a < 0We pick p0 using grid search on the developmentset, pl is uniform, and the pj and pt are optimized byEM.22.2 Gappy Semi-Markov ModelsThe HMM alignment model identifies a word-to-word correspondence between the observation2Note that jump distances beyond -10 or 10 share a singleparameter to prevent sparsity.1310words and the state words.
We make two changesto expand this model.
First, we allow contiguousphrases on the observation side, which makes themodel semi-Markov: at each time stamp, the modelmay emit more than one observation word.
Next, wealso allow contiguous and gappy phrases on the stateside, leading to an alignment model that can retainphrasal links after agreement (see Section 4).The S and O random variables are unchanged.Since a single state may generate multiple observa-tion words, we add a new variable K representingthe number of states.
K should be less than J , thenumber of observations.
The alignment variable isaugmented to allow contiguous and non-contiguousranges of words.
We allow only a single gap, but ofunlimited length.
The null state is still present, andis again represented by negative numbers.A =(a1, .
.
.
, aK) ?
A(I)A(I) ={(i1, i2, g)|0 < i1 ?
i2 ?
I,g ?
{GAP, CONTIG}}?
{(?i,?i, CONTIG) | 0 < i ?
I}We add one more random variable to capture the to-tal number of observations generated by each state.L ?
{(l0, l1, .
.
.
, lK) | 0 = l0 < ?
?
?
< lK = J}The generative model takes the following form:p(A,L,O|S) =pl(J |I)pf (K|J)K?k=1pj(ak|ak?1)?pt(lk, olklk?1+1|S[ak], lk?1)First, the length of the observation sequence (J)is selected, based on the number of words in thestate-side sentence (I).
Since it does not affect thealignment, pl is modeled as a uniform distribution.Next, we pick the total number of states to use (K),which must be less than the number of observations(J).
Short state sequences receive an exponentialpenalty: pf (K|J) ?
?
(J?K) if 0 ?
K ?
J , or 0otherwise.
A harsh penalty (small positive value of?)
may prevent the systematic overuse of phrases.33We found that this penalty was crucial to prevent overfittingin independent training.
Joint training with agreement made itbasically unnecessary.Next we decide the assignment of each state.We retain the first-order Markov assumption: theselection of each state is conditioned only on theprior state.
The transition distribution is identicalto the word-based HMM for single word states.
Forphrasal and gappy states, we jump into the first wordof that state, and out of the last word of that state,and then pay a cost according to how many wordsare covered within that state.
If a = (i1, i2, g), thenthe beginning word of a is F (a) = i1, the end-ing word is L(a) = i2, and the length N(a) is 2for gapped states, 0 for null states, and last(a) ?first(a) + 1 for all others.
The transition probabil-ity is:pj(a|a?)
=????
?p0 ?
?
(|F (a)|, |L(a?
)|) if F (a) < 0(1?
p0)pd(F (a)?
|L(a?
)|)?pn(N(a)) otherwisewhere pn(c) ?
?c is an exponential distribution.
Asin the word HMM case, we use a mixture parameterp0 to determine the likelihood of landing in a NULLstate.
The position of that NULL state remembers thelast position of the prior state.
For non-null words,we pick the first word of the state according to thedistance from the last word of the prior state.
Finally,we pick a length for that final state according to anexponential distribution: values of ?
less than onewill penalize the use of phrasal states.For each set of state words, we maintain an emis-sion distribution over observation word sequences.Let S[a] be the set of state words referred to bythe alignment variable a.
For example, the Englishgiven French alignment of Figure 3 includes the fol-lowing state word sets:S[(2, 2, CONTIG)] = voudraisS[(1, 3, GAP)] = ne ?
pasS[(6, 8, CONTIG)] = chemin de ferFor the emission distribution we keep a multinomialover observation phrases for each set of state words:p(l, oll?
|S[a], l?)
?
c(oll?
|S[a])In contrast to the approach of Deng and Byrne(2005), this encourages greater consistency acrossinstances, and more closely resembles the com-monly used phrasal translation models.1311We note in passing that pf (K|J) may be movedinside the product: pf (K|J) ?
?
(J?K) =?Kk=1 ?(lk?lk?1?1).
The following form derived us-ing the above rearrangement is helpful during EM.p(A,L,O|S) ?K?k=1pj(ak|ak?1)?pt(lk, olklk?1+1|S[ak], lk?1)??
(lk?lk?1?1)where lk ?
lk?1 ?
1 is the length of the observationphrase emitted by state S[ak].2.3 MinimalityAt alignment time we focus on finding the minimalphrase pairs, under the assumption that composedphrase pairs can be extracted in terms of these min-imal pairs.
We are rather strict about this, allowingonly 1 ?
k and k ?
1 phrasal alignment edges(or links).
This should not cause undue stress, sinceedges of the form 2 ?
3 (say e1e2 ?
f1f2f3) cangenerally be decomposed into 1 ?
1 ?
1 ?
2 (i.e.,e1 ?
f1 ?
e2 ?
f2f3), etc.
However, the modeldoes not require this to be true: we will describe re-estimation for unconstrained general models, but usethe limited form for word alignment.3 Parameter EstimationWe use Expectation-Maximization (EM) to estimateparameters.
The forward-backward algorithm effi-ciently computes posteriors of transitions and emis-sions in the word-based HMM.
In a standard HMM,emission always advances the observation positionby one, and the next transition is unaffected bythe emission.
Neither of these assumptions holdin our model: multiple observations may be emit-ted at a time, and a state may cover multiple state-side words, which affects the outgoing transition.
Amodified dynamic program computes posteriors forthis generalized model.The following formulation of the forward-backward algorithm for word-to-word alignment isa good starting point.
?
[x, 0, y] indicates the totalmass of paths that have just transitioned into state yat observation x but have not yet emitted; ?
[x, 1, y]represents the mass after emission but before subse-quent transition.
?
is defined similarly.
(We omitNULL states for brevity; the extension is straightfor-ward.)?
[0, 0, y] = pj(y|INIT)?
[x, 1, y] = ?
[x, 0, y] ?
pt(ox|sy)?
[x, 0, y] =?y??[x?
1, 1, y?]
?
pj(y|y?)?
[n, 1, y] = 1?
[x, 0, y] = pt(ox|sy) ?
?
[x, 1, y]?
[x, 1, y] =?y?pj(y?|y) ?
?
[x+ 1, 0, y?
]Not only is it easy to compute posteriors of bothemissions (?
[x, 0, y]pt(ox|sy)?
[x, 1, y]) and transi-tions (?
[x, 1, y]pj(y?|y)?
[x+ 1, 0, y?])
with this for-mulation, it also simplifies the generalization tocomplex emissions.
We update the emission forwardprobabilities to include a search over the possiblestarting points in the state and observation space:?
[0, 0, y] =pj(y|INIT)?
[x, 1, y] =?x?<x,y??y?
[x?, 0, y?]
?
EMIT(x?
: x, y?
: y)?
[x, 0, y] =?y??[x?
1, 1, y?]
?
pj(y|y?)?
[n, 1, y] =1?
[x?, 0, y?]
=?x?<x,y??yEMIT(x?
: x, y?
: y) ?
?
[x, 1, y]?
[x, 1, y] =?y?pj(y?|y) ?
?
[x+ 1, 0, y?
]Phrasal and gapped emissions are pooled into EMIT:EMIT(w : x, y : z) =pt(oxw|szy) ?
?z?y+1 ?
?x?w+1+pt(oxw|sy ?
sz) ?
?2 ?
?x?w+1The transition posterior is the same as above.
Theemission is very similar: the posterior probabilitythat oxw is aligned to szy is proportional to ?
[w, 0, y] ?pt(oxw|szy) ?
?z?y+1 ?
?x?w+1 ??
[x, 1, z].
For a gappedphrase, the posterior is proportional to ?
[w, 0, y] ?pt(oxw|sy ?
sz) ?
?2 ?
?x?w+1 ?
?
[x, 1, z].Given an inference procedure for computing pos-teriors, unsupervised training with EM follows im-mediately.
We use a simple maximum-likelihoodupdate of the parameters using expected countsbased on the posterior distribution.13124 Alignment by AgreementFollowing Liang et al (2006), we quantify agree-ment between two models as the probability that thealignments produced by the two models agree on thealignment z of a sentence pair x = (S,O):?zp1(z|x; ?1)p2(z|x; ?2)To couple the two models, the (log) probability ofagreement is added to the standard log-likelihoodobjective:max?1,?2?x[log p1(x; ?1) + log p2(x; ?2)+log?zp1(z|x; ?1)p2(z|x; ?2)]We use the heuristic estimator from Liang et al(2006), letting q be a product of marginals:E : q(z; x) :=?z?zp1(z|x; ?1)p2(z|x; ?2)where each pk(z|x; ?k) is the posterior marginal ofsome edge z according to each model.
Such aheuristic E step computes the marginals for eachmodel separately, then multiplies the marginals cor-responding to the same edge.
This product ofmarginals acts as the approximation to the posteriorused in the M step for each model.
The intuition isthat if the two models disagree on a certain edge z,then the marginal product is small, hence that edgeis dis-preferred in each model.Contiguous phrase agreement.
It is simple toextend agreement to alignments in the absence ofgaps.
Multi-word (phrasal) links are assigned someposterior probability in both models, as shown in theexample in Figure 3, and we multiply the posteriorsof these phrasal links just as in the single word case.4?F?E(fi, ej) := ?E?F (ej , fi):= [?F?E(fi, ej)?
?E?F (ej , fi)]4Phrasal correspondences can be represented in multipleways: multiple adjacent words could be generated from thesame state either using one semi-Markov emission, or usingmultiple single word emissions followed by self-jumps.
Onlythe first case is reinforced through agreement, so the latter isimplicitly discouraged.
We explored an option to forbid same-state transitions, but found it made little difference in practice.Gappy phrase agreement.
When we introducegappy phrasal states, agreement becomes more chal-lenging.
In the forward direction F?E, if we have agappy state aligned to an observation, say fi ?
fj ?ek, then its corresponding edge in the backward di-rection E?F would be ek v fi ?
fj .
How-ever, this is represented by two distinct and unre-lated emissions.
Although it is possible the computethe posterior probability of two non-adjacent emis-sions, this requires running a separate dynamic pro-gram for each such combination to sum the mass be-tween these emissions.
For the sake of efficiencywe resort to an approximate computation of pos-terior marginals using the two word-to-word edgesek v fi and ek v fj .The forward posterior ?F?E for edge fi ?
fj ?ek is multiplied with the min of the backward pos-teriors of the edges ek v fi and ek v fj .
?F?E(fi ?
fj , ek) := ?F?E(fi ?
fj , ek)?min{?E?F (ek, fi), ?E?F (ek, fj)}Note that this min is an upper bound on the desiredposterior of edge ek v fi ?
fj , since every paththat passes through ek v fi and ek v fj must passthrough ek v fi, therefore the posterior of ek vfi ?
fj is less than that of ek v fi, and likewise lessthan that of ek v fj .The backward posteriors of the edges ek v fi andek v fj are also mixed with the forward posteriorsof the edges to which they correspond.
?E?F (ek, fi) := ?E?F (ek, fi)?
[?F?E(fi, ek)+?h<i<j{?F?E(fh ?
fi, ek) + ?F?E(fi ?
fj , ek)}]5 Pruned Lists of ?Allowed?
PhrasesTo identify contiguous and gapped phrases that aremore likely to lead to good alignments, we use word-to-word HMM alignments from the full training datain both directions (F?E and E?F).
We collect ob-servation phrases of length 2 toK aligned to a singlestate, i.e.
oji ?
s, to add to a list of allowed phrases.For gappy phrases, we find all non-consecutive ob-servation pairs oi and oj such that: (a) both are1313aligned to the same state sk, (b) state sk is aligned toonly these two observations, and (c) at least one ob-servation between oi and oj is aligned to a non-nullstate other than sk.
These observation phrases arecollected from F?E and E?F models to build con-tiguous and gappy phrase lists for both languages.Next, we order the phrases in each contiguous listusing the discounted probability:p?
(oji ?
s|oji ) =max(0, count(oji ?
s)?
?
)count(oji )where count(oji ?
s) is the count of occurrence ofthe observation-phrase oji , all aligned to some sin-gle state s, and count(oji ) is the count of occur-rence of the observation phrase oji , not all necessar-ily aligned to a single state.
Similarly, we rank thegappy phrases using the discounted probability:p?
(oi ?
oj ?
s|oi ?
oj) =max(0, count(oi ?
oj ?
s)?
?
)count(oi ?
oj)where count(oi ?
oj ?
s) is the count of occur-rence of the observations oi and oj aligned to a sin-gle state s with the conditions mentioned above, andcount(oi ?
oj) is the count of general occurrence ofthe observations oi and oj in order.
We find that 200gappy phrases and 1000 contiguous phrases workswell, based on tuning with a development set.6 Complexity AnalysisLet m be the length of the state sentence S and nbe the length of the observation sentence O.
In IBMModel 1 (Brown et al, 1994), with only a translationmodel, we can infer posteriors or max alignmentsin O(mn).
HMM-based word-to-word alignmentmodel (Vogel et al, 1996) adds a distortion model,increasing the complexity to O(m2n).Introducing phrases (contiguous) on the observa-tion side, we get a HSMM (Hidden Semi-MarkovModel).
If we allow phrases of length no greaterthan K, then the number of observation typesrises from n to Kn for an overall complexity ofO(m2Kn).
Introducing state phrases (contiguous)with length ?
K grows the number of state typesfrom m to Km.
Complexity further increases toO((Km)2Kn) = O(K3m2n).Finally, when we introduce gappy state phrases ofthe type si ?
sj , the number of such phrases isO(m2), since we may choose a start and end pointindependently.
Thus, the total complexity rises toO((Km + m2)2Kn) = O(Km4n).
Although thisis less than the O(n6) complexity of exact ITG (In-version Transduction Grammar) model (Wu, 1997),a quintic algorithm is often quite slow.The pruned lists of allowed phrases limit thiscomplexity.
The model is allowed to use observa-tion (contiguous) and state (contiguous and gappy)phrases only from these lists.
The number ofphrases that match any given sentence pair fromthese pruned lists is very small (?
2 to 5).
If thenumber of phrases in the lists that match the obser-vation and state side of a given sentence pair aresmall constants, the complexity remains O(m2n),equal to that of word-based models.7 ResultsWe evaluate our models based on both word align-ment and end-to-end translation with two languagepairs: English-French and English-German.
ForFrench-English, we use the Hansards NAACL 2003shared-task dataset, which contains nearly 1.1 mil-lion training sentence pairs.
We also evaluatedon German-English Europarl data from WMT2010,with nearly 1.6 million training sentence pairs.
Themodel from Liang et al (2006) is our word-basedbaseline.7.1 Training RegimenOur training regimen begins with both the forward(F?E) and backward (E?F) iterations of Model 1run independently (i.e.
without agreement).
Next,we train several iterations of the forward and back-ward word-to-word HMMs, again with independenttraining.
We do not use agreement during wordalignment since it tends to produce sparse 1-1 align-ments, which in turn leads to low phrase emissionprobabilities in the gappy model.Initializing the emission probabilities of the semi-Markov model is somewhat complicated, since theword-based models do not assign any mass tothe phrasal or gapped configurations.
Thereforewe use a heuristic method.
We first retrieve theViterbi alignments of the forward and backward1314word-to-word HMM aligners.
For phrasal corre-spondences, we combine these forward and back-ward Viterbi alignments using a common heuris-tic (Union, Intersection, Refined, or Grow-Diag-Final), and extract tight phrase-pairs (no unalignedwords on the boundary) from this alignment set.We found that Grow-Diag-Final was most effectivein our experiments.
The counts gathered from thisphrase extraction are used to initialize phrasal trans-lation probabilities.
For gappy states in a forward(F?E) model, we use alignments from the back-ward (E?F) model.
If a state sk is aligned to twonon-consecutive observations oi and oj such that skis not aligned to any other observation, and at leastone observation between oi and oj is aligned to anon-null state other than sk, then we reverse thislink to get oi ?
oj ?
sk and use it as a gapped-state-phrase instance for adding fractional counts.Given these approximate fractional counts, we per-form a standard MLE M-step to initialize the emis-sion probability distributions.
The distortion proba-bilities from the word-based model are used withoutchanges.7.2 Alignment Results (F1)The validation and test sentences have been hand-aligned (see Och and Ney (2003)) and are markedwith both sure and possible alignments.
For French-English, following Liang et al (2006), we lowercaseall words, and use the validation set plus the first100 test sentences as our development set and theremaining 347 test-sentences as our test-set for fi-nal F1 evaluation.5 In German-English, we have adevelopment set of 102 sentences, and a test set of258 sentences, also annotated with a set of sure andpossible alignments.
Given a predicted alignmentA,precision and recall are computed using sure align-ments S and possible alignments P (where S ?
P )as in Och and Ney (2003):Precision =|A ?
P ||A|?
100%Recall =|A ?
S||S|?
100%5We report F1 rather than AER because AER appears not tocorrelate well with translation quality.
(Fraser and Marcu, 2007)Language pair Word-to-word GappyFrench-English 34.0 34.5German-English 19.3 19.8Table 2: BLEU results on German-English and French-English.AER =(1?|A ?
S|+ |A ?
P ||A|+ |S|)?
100%F1 =2?
Precision?RecallPrecision+Recall?
100%Many free parameters were tuned to optimizealignment F1 on the development set, including thenumber of iterations of each Model 1, HMM, andGappy; the NULL weight p0, the number of con-tiguous and gappy phrases to include, and the max-imum phrase length.
Five iterations of all models,p0 = 0.3, using the top 1000 contiguous phrasesand the top 200 gappy phrases, maximum phraselength of 5, and penalties ?
= ?
= 1 producedcompetitive results.
Note that by setting ?
and ?
toone, we have effectively removed the penalty alto-gether without affecting our results.
In Table 1 wesee a consistent improvement with the addition ofcontiguous phrases, and some additional gains withgappy phrases.7.3 Translation Results (BLEU)We assembled a phrase-based system from the align-ments (using only contiguous phrases consistentwith the potentially gappy alignment), with 4 chan-nel models, word and phrase count features, dis-tortion penalty, lexicalized reordering model, and a5-gram language model, weighted by MERT.
Thesame free parameters from above were tuned to opti-mize development set BLEU using grid search.
Theimprovements in Table 2 are encouraging, especiallyas a syntax-based or non-contiguous phrasal system(Galley and Manning, 2010) may benefit more fromgappy phrases.8 Conclusions and Future WorkWe have described an algorithm for efficient unsu-pervised alignment of phrases.
Relatively straight-forward extensions to the base HMM allow for ef-ficient inference, and agreement between the two1315Data Decoding method Word-to-word +Contig phrases +Gappy phrasesFE 10K Viterbi 89.7 90.6 90.3FE 10K Posterior ?
0.1 90.1 90.4 90.7FE 100K Viterbi 93.0 93.6 93.8FE 100K Posterior ?
0.1 93.1 93.7 93.8FE All Viterbi 94.1 94.3 94.3FE All Posterior ?
0.1 94.2 94.4 94.5GE 10K Viterbi 76.2 79.6 79.7GE 10K Posterior ?
0.1 76.7 79.3 79.3GE 100K Viterbi 81.0 83.0 83.2GE 100K Posterior ?
0.1 80.7 83.1 83.4GE All Viterbi 83.0 85.2 85.6GE All Posterior ?
0.1 83.7 85.3 85.7Table 1: F1 scores of automatic word alignments, evaluated on the test set of the hand-aligned sentence pairs.models prevents EM from overfitting, even in the ab-sence of harsh penalties.
We also allow gappy (non-contiguous) phrases on the state side, which makesagreement more successful but agreement needs ap-proximation of posterior marginals.
Using prunedlists of good phrases, we maintain complexity equalto the baseline word-to-word model.There are several steps forward from this point.Limiting the gap length also prevents combinato-rial explosion; we hope to explore this in futurework.
Clearly a translation system that uses discon-tinuous mappings at runtime (Chiang, 2007; Gal-ley and Manning, 2010) may make better use ofdiscontinuous alignments.
This model can also beapplied at the morpheme or character level, allow-ing joint inference of segmentation and alignment.Furthermore the state space could be expanded andenhanced to include more possibilities: states withmultiple gaps might be useful for alignment in lan-guages with template morphology, such as Arabic orHebrew.
More exploration in the model space couldbe useful ?
a better distortion model might place astronger distribution on the likely starting and end-ing points of phrases.AcknowledgmentsWe would like to thank the anonymous reviewers fortheir helpful suggestions.
This project is funded byMicrosoft Research.ReferencesJesu?s Andre?s-Ferrer and Alfons Juan.
2009.
A phrase-based hidden semi-Markov approach to machine trans-lation.
In Proceedings of EAMT.Peter F. Brown, Stephen A. Della Pietra, Vincent J. DellaPietra, and Robert L. Mercer.
1994.
The mathematicsof statistical machine translation: Parameter estima-tion.
Computational Linguistics, 19:263?311.David Chiang.
2007.
Hierarchical phrase-based transla-tion.
Computational Linguistics.Hal Daume?
III and Daniel Marcu.
2004.
A phrase-basedHMM approach to document/abstract alignment.
InProceedings of EMNLP.John DeNero, Dan Gillick, James Zhang, and Dan Klein.2006.
Why generative phrase models underperformsurface heuristics.
In Proceedings of ACL.Yonggang Deng and William Byrne.
2005.
HMM wordand phrase alignment for statistical machine transla-tion.
In Proceedings of HLT-EMNLP.Xiangyu Duan, Min Zhang, and Haizhou Li.
2010.Pseudo-word for phrase-based machine translation.
InProceedings of ACL.Alexander Fraser and Daniel Marcu.
2007.
Measuringword alignment quality for statistical machine transla-tion.
Computational Linguistics, 33(3):293?303.Michel Galley and Christopher D. Manning.
2010.
Ac-curate non-hierarchical phrase-based translation.
InHLT/NAACL.Michel Galley, Mark Hopkins, Kevin Knight, and DanielMarcu.
2004.
What?s in a translation rule?
In Pro-ceedings of HLT-NAACL.Philipp Koehn, Franz Och, and Daniel Marcu.
2003.
Sta-tistical Phrase-Based Translation.
In Proceedings ofHLT-NAACL.Patrik Lambert and Rafael Banchs.
2006.
Groupingmulti-word expressions according to part-of-speech in1316statistical machine translation.
In Proc.
of the EACLWorkshop on Multi-Word-Expressions in a Multilin-gual Context.Percy Liang, Ben Taskar, and Dan Klein.
2006.
Align-ment by agreement.
In Proceedings of HLT-NAACL.Yanjun Ma, Nicolas Stroppa, and Andy Way.
2007.Boostrapping word alignment via word packing.
InProceedings of ACL.Daniel Marcu and Daniel Wong.
2002.
A phrase-based,joint probability model for statistical machine transla-tion.
In Proceedings of EMNLP.Franz Josef Och and Hermann Ney.
2003.
A system-atic comparison of various statistical alignment mod-els.
Computational Linguistics, 29:19?51.Stephan Vogel, Hermann Ney, and Christoph Tillmann.1996.
HMM-based word alignment in statistical trans-lation.
In Proceedings of COLING.Dekai Wu.
1997.
Stochastic inversion transductiongrammars and bilingual parsing of parallel corpora.Computational Linguistics, 23:377?404.Andreas Zollmann, Ashish Venugopal, and Stephan Vo-gel.
2006.
Syntax augmented machine translation viachart parsing.
In Processings of the Statistical Ma-chine Translation Workshop at NAACL.1317
