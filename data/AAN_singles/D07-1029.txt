Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and ComputationalNatural Language Learning, pp.
277?286, Prague, June 2007. c?2007 Association for Computational LinguisticsHierarchical System Combination for Machine TranslationFei HuangIBM T.J. Watson Research CenterYorktown Heights, NY 10562huangfe@us.ibm.comKishore Papineni ?Yahoo!
ResearchNew York, NY 10011kpapi@yahoo-inc.comAbstractGiven multiple translations of the samesource sentence, how to combine them toproduce a translation that is better than anysingle system output?
We propose a hier-archical system combination framework formachine translation.
This framework inte-grates multiple MT systems?
output at theword-, phrase- and sentence- levels.
Byboosting common word and phrase trans-lation pairs, pruning unused phrases, andexploring decoding paths adopted by otherMT systems, this framework achieves bet-ter translation quality with much less re-decoding time.
The full sentence translationhypotheses from multiple systems are addi-tionally selected based on N-gram languagemodels trained on word/word-POS mixedstream, which further improves the transla-tion quality.
We consistently observed sig-nificant improvements on several test sets inmultiple languages covering different gen-res.1 IntroductionMany machine translation (MT) frameworks havebeen developed, including rule-based transfer MT,corpus-based MT (statistical MT and example-basedMT), syntax-based MT and the hybrid, statisticalMT augmented with syntactic structures.
DifferentMT paradigms have their strengths and weaknesses.
?This work was done when the author was at IBM Research.Systems adopting the same framework usually pro-duce different translations for the same input, dueto their differences in training data, preprocessing,alignment and decoding strategies.
It is beneficialto design a framework that combines the decodingstrategies of multiple systems as well as their out-puts and produces translations better than any singlesystem output.
More recently, within the GALE1project, multiple MT systems have been developedin each consortium, thus system combination be-comes more important.Traditionally, system combination has been con-ducted in two ways: glass-box combination andblack-box combination.
In the glass-box combi-nation, each MT system provides detailed decod-ing information, such as word and phrase transla-tion pairs and decoding lattices.
For example, in themulti-engine machine translation system (Nirenburgand Frederking, 1994), target language phrases fromeach system and their corresponding source phrasesare recorded in a chart structure, together with theirconfidence scores.
A chart-walk algorithm is usedto select the best translation from the chart.
To com-bine words and phrases from multiple systems, it ispreferable that all the systems adopt similar prepro-cessing strategies.In the black-box combination, individual MT sys-tems only output their top-N translation hypothe-ses without decoding details.
This is particularlyappealing when combining the translation outputsfrom COTS MT systems.
The final translation maybe selected by voted language models and appropri-ate confidence rescaling schemes ((Tidhar and Kuss-1http://www.darpa.mil/ipto/programs/gale/index.htm277ner, 2000) and (Nomoto, 2004)).
(Mellebeek et al,2006) decomposes source sentences into meaning-ful constituents, translates them with component MTsystems, then selects the best segment translationand combine them based on majority voting, lan-guage models and confidence scores.
(Jayaraman and Lavie, 2005) proposed anotherblack-box system combination strategy.
Given sin-gle top-one translation outputs from multiple MTsystems, their approach reconstructs a phrase lat-tice by aligning words from different MT hypothe-ses.
The alignment is based on the surface formof individual words, their stems (after morphologyanalysis) and part-of-speech (POS) tags.
Alignedwords are connected via edges.
The algorithm findsthe best alignment that minimizes the number ofcrossing edges.
Finally the system generates a newtranslation by searching the lattice based on align-ment information, each system?s confidence scoresand a language model score.
(Matusov et al, 2006)and (Rosti et al, 2007) constructed a confusion net-work from multiple MT hypotheses, and a consen-sus translation is selected by redecoding the latticewith arc costs and confidence scores.In this paper, we introduce our hierarchical sys-tem combination strategy.
This approach allowscombination on word, phrase and sentence levels.Similar to glass-box combination, each MT sys-tem provides detailed information about the trans-lation process, such as which source word(s) gener-ates which target word(s) in what order.
Such in-formation can be combined with existing word andphrase translation tables, and the augmented phrasetable will be significantly pruned according to reli-able MT hypotheses.
We select an MT system to re-translate the test sentences with the refined models,and encourage search along decoding paths adoptedby other MT systems.
Thanks to the refined trans-lation models, this approach produces better transla-tions with a much shorter re-decoding time.
As inthe black-box combination, we select full sentencetranslation hypotheses from multiple system outputsbased on n-gram language models.
This hierarchicalsystem combination strategy avoids problems liketranslation output alignment and confidence scorenormalization.
It seamlessly integrates detailed de-coding information and translation hypotheses frommultiple MT engines, and produces better transla-tions in an efficient manner.
Empirical studies in alater section show that this algorithm improves MTquality by 2.4 BLEU point over the best baseline de-coder, with a 1.4 TER reduction.
We also observedconsistent improvements on several evaluation testsets in multiple languages covering different genresby combining several state-of-the-art MT systems.The rest of the paper is organized as follows: Insection 2, we briefly introduce several baseline MTsystems whose outputs are used in the system com-bination.
In section 3, we present the proposed hi-erarchical system combination framework.
We willdescribe word and phrase combination and pruning,decoding path imitation and sentence translation se-lection.
We show our experimental results in section4 and conclusions in section 5.2 Baseline MT System OverviewIn our experiments, we take the translation out-puts from multiple MT systems.
These includephrase-based statistical MT systems (Al-Onaizanand Papineni, 2006) (Block) and (Hewavitharana etal., 2005) (CMU SMT) , a direct translation model(DTM) system (Ittycheriah and Roukos, 2007) and ahierarchical phrased-based MT system (Hiero) (Chi-ang, 2005).
Different translation frameworks areadopted by different decoders: the DTM decodercombines different features (source words, mor-phemes and POS tags, target words and POS tags)in a maximum entropy framework.
These featuresare integrated with a phrase translation table forflexible distortion model and word selection.
TheCMU SMT decoder extracts testset-specific bilin-gual phrases on the fly with PESA algorithm.
TheHiero system extracts context-free grammar rulesfor long range constituent reordering.We select the IBM block decoder to re-translatethe test set for glass-box system combination.
Thissystem is a multi-stack, multi-beam search decoder.Given a source sentence, the decoder tries to findthe translation hypothesis with the minimum trans-lation cost.
The overall cost is the log-linear combi-nation of different feature functions, such as trans-lation model cost, language model cost, distortioncost and sentence length cost.
The translation cost278between a phrase translation pair (f, e) is defined asTM(e, f) =?i?i?
(i) (1)where feature cost functions ?
(i) includes:?
log p(f |e), a target-to-source word translationcost, calculated based on unnormalized IBM model1cost (Brown et al, 1994);p(f |e) =?j?it(fj|ei) (2)where t(fj|ei) is the word translation probabilities,estimated based on word alignment frequencies overall the training data.
i and j are word positions intarget and source phrases.?
log p(e|f), a source-to-target word translationcost, calculated similar to ?
log p(f |e);S(e, f), a phrase translation cost estimated ac-cording to their relative alignment frequency in thebilingual training data,S(e, f) = ?
log P (e|f) = ?
log C(f, e)C(f) .
(3)?
?s in Equation 1 are the weights of different fea-ture functions, learned to maximize development setBLEU scores using a method similar to (Och, 2003).The SMT system is trained with testset-specifictraining data.
This is not cheating.
Given a test set,from a large bilingual corpora we select parallel sen-tence pairs covering n-grams from source sentences.Phrase translation pairs are extracted from the sub-sampled alignments.
This not only reduces the sizeof the phrase table, but also improves topic relevancyof the extracted phrase pairs.
As a results, it im-proves both the efficiency and the performance ofmachine translation.3 Hierarchical System CombinationFrameworkThe overall system combination framework isshown in Figure 1.
The source text is translatedby multiple baseline MT systems.
Each system pro-duces both top-one translation hypothesis as well asphrase pairs and decoding path during translation.The information is shared through a common XMLfile format, as shown in Figure 2.
It demonstrateshow a source sentence is segmented into a sequenceof phrases, the order and translation of each sourcephrase as well as the translation scores, and a vectorof feature scores for the whole test sentence.
SuchXML files are generated by all the systems whenthey translate the source test set.We collect phrase translation pairs from each de-coder?s output.
Within each phrase pair, we iden-tify word alignment and estimate word translationprobabilities.
We combine the testset-specific wordtranslation model with a general model.
We aug-ment the baseline phrase table with phrase trans-lation pairs extracted from system outputs, thenprune the table with translation hypotheses.
We re-translate the source text using the block decoder withupdated word and phrase translation models.
Ad-ditionally, to take advantage of flexible reorderingstrategies of other decoders, we develop a word or-der cost function to reinforce search along decod-ing paths adopted by other decoders.
With the re-fined translation models and focused search space,the block decoder efficiently produces a better trans-lation output.
Finally, the sentence hypothesis se-lection module selects the best translation from eachsystems?
top-one outputs based on language modelscores.
Note that the hypothesis selection moduledoes not require detailed decoding information, thuscan take in any MT systems?
outputs.3.1 Word Translation CombinationThe baseline word translation model is too generalfor the given test set.
Our goal is to construct atestset-specific word translation model, combine itwith the general model to boost consensus wordtranslations.
Bilingual phrase translation pairs areread from each system-generated XML file.
Wordalignments are identified within a phrase pair basedon IBM Model-1 probabilities.
As the phrase pairsare typically short, word alignments are quite accu-rate.
We collect word alignment counts from thewhole test set translation, and estimate both source-to-target and target-to-source word translation prob-abilities.
We combine such testset-specific transla-tion model with the general model.t??
(e|f) = ?t?
(e|f) + (1 ?
?
)t(e|f); (4)where t?
(e|f) is the testset-specific source-to-targetword translation probability, and t(e|f) is the prob-279<tr engine="XXX"><s id="0"> <w>  </w><w> </w><w>  </w><w>  </w><w> </w><w>  </w><w>  </w><w>  </w><w>   </w><w> ! " </w><w> #$% </w></s><hyp r="0" c="2.15357"><t><p al="0-0" cost="0.0603734"> erdogan </p><p al="1-1" cost="0.367276"> emphasized </p><p al="2-2" cost="0.128066"> that </p><p al="3-3" cost="0.0179338"> turkey </p><p al="4-5" cost="0.379862"> would reject any </p><p al="6-6" cost="0.221536"> pressure </p><p al="7-7" cost="0.228264"> to urge them </p><p al="8-8" cost="0.132242"> to</p><p al="9-9" cost="0.113983"> recognize </p><p al="10-10" cost="0.133359"> Cyprus </p></t><sco>19.6796 8.40107 0.333514 0.00568583 0.223554 0 0.352681 0.01 -0.616 0.009 0.182052</sco></hyp></tr>Figure 2: Sample XML file format.
This includes a source sentence (segmented as a sequence of sourcephrases), their translations as well as a vector of feature scores (language model scores, translation modelscores, distortion model scores and a sentence length score).ability from general model.
?
is the linear combi-nation weight, and is set according to the confidenceon the quality of system outputs.
In our experiments,we set ?
to be 0.8.
We combine both source-to-target and target-to-source word translation models,and update the word translation costs, ?
log p(e|f)and ?
log p(f |e), accordingly.3.2 Phrase Translation Combination andPruningPhrase translation pairs can be combined in two dif-ferent ways.
We may collect and merge testset-specific phrase translation tables from each system,if they are available.
Essentially, this is similar tocombining the training data of multiple MT systems.The new phrase translation probability is calculatedaccording to the updated phrase alignment frequen-cies:P ?
(e|f) = Cb(f, e) +?
?mCm(f, e)Cb(f) +?
?mCm(f), (5)where Cb is the phrase pair count from the baselineblock decoder, and Cm is the count from other MTsystems.
?m is a system-specific linear combinationweight.
If not all the phrase tables are available, wecollect phrase translation pairs from system outputs,and merge them with Cb.
In such case, we may ad-just ?
to balance the small counts from system out-puts and large counts from Cb.The corresponding phrase translation cost is up-dated asS?
(e, f) = ?
log P ?(e|f).
(6)Another phrase combination strategy works onthe sentence level.
This strategy relies on the con-sensus of different MT systems when translating thesame source sentence.
It collects phrase translationpairs used by different MT systems to translate thesame sentence.
Similarly, it boosts common phrasepairs that are selected by multiple decoders.S??
(e, f) = ?|C(f, e)| ?
S?
(e, f), (7)where ?
is a boosting factor, 0 < ?
?
1 .
|C(f, e)|is the number of systems that use phrase pair (f, e)to translate the input sentence.
A phrase translationpair selected by multiple systems is more likely agood translation, thus costs less.The combined phrase table contains multipletranslations for each source phrase.
Many of them280are unlikely translations given the context.
Thesephrase pairs produce low-quality partial hypothe-ses during hypothesis expansion, incur unnecessarymodel cost calculation and larger search space, andreduce the translation efficiency.
More importantly,the translation probabilities of correct phrase pairsare reduced as some probability mass is distributedamong incorrect phrase pairs.
As a result, goodphrase pairs may not be selected in the final trans-lation.Oracle experiments show that if we prune thephrase table and only keep phrases that appear inthe reference translations, we can improve the trans-lation quality by 10 BLEU points.
This shows thepotential gain by appropriate phrase pruning.
Wedeveloped a phrase pruning technique based on self-training.
This approach reinforces phrase transla-tions learned from MT system output.
Assumingwe have reasonable first-pass translation outputs, weonly keep phrase pairs whose target phrase is cov-ered by existing system translations.
These phrasepairs include those selected in the final translations,as well as their combinations or sub-phrases.
Asa result, the size of the phrase table is reduced by80-90%, and the re-decoding time is reduced by80%.
Because correct phrase translations are as-signed higher probabilities, it generates better trans-lations with higher BLEU scores.3.3 Decoding Path ImitationBecause of different reordering models, words in thesource sentence can be translated in different orders.The block decoder has local reordering capabilitythat allows source words within a given window tojump forward or backward with a certain cost.
TheDTM decoder takes similar reordering strategy, withsome variants like dynamic window width depend-ing on the POS tag of the current source word.
TheHiero system allows for long range constituent re-ordering based on context-free grammar rules.
Tocombine different reordering strategies from vari-ous decoders, we developed a reordering cost func-tion that encourages search along decoding pathsadopted by other decoders.From each system?s XML file, we identify the or-der of translating source words based on word align-ment information.
For example, given the followinghypothesis path,<p al=?0-1?> izzat ibrahim </p> <p al=?2-2?> receives </p> <p al=?3-4?> an economicofficial </p> <p al=?5-6?> in </p> <p al=?7-7?> baghdad </p>We find the source phrase containing words [0,1]is first translated into a target phrase ?izzat ibrahim?,which is followed by the translation from sourceword 2 to a single target word ?receives?, etc.. Weidentify the word alignment within the phrase trans-lation pairs based on IBM model-1 scores.
As a re-sult, we get the following source word translationsequence from the above hypothesis (note: sourceword 5 is translated as NULL):0 < 1 < 2 < 4 < 3 < 6 < 7Such decoding sequence determines the transla-tion order between any source word pairs, e.g., word4 should be translated before word 3, 6 and 7.
Wecollect such ordered word pairs from all system out-puts?
paths.
When re-translating the source sen-tence, for each partially expanded decoding path, wecompute the ratio of word pairs that satisfy such or-dering constraints2 .Specifically, given a partially expanded path P ={s1 < s2 < ?
?
?
< sm}, word pair (si < sj) impliessi is translated before sj .
If word pair (si < sj) iscovered by a full decoding path Q (from other sys-tem outputs), we denote the relationship as (si <sj) ?
Q.For any ordered word pair (si < sj) ?
P , we de-fine its matching ratio as the percentage of full de-coding paths that cover it:R(si < sj) =|Q|N , {Q|(si < sj) ?
Q} (8)where N is the total number of full decoding paths.We define the path matching cost function:L(P ) = ?
log??
(si<sj)?P R(si < sj)??
(si<sj)?P 1(9)The denominator is the total number of orderedword pairs in path P .
As a result, partial paths areboosted if they take similar source word translationorders as other system outputs.
This cost function ismultiplied with a manually tuned model weight be-fore integrating into the log-linear cost model frame-work.2We set no constraints for source words that are translatedinto NULL.2813.4 Sentence Hypothesis SelectionThe sentence hypothesis selection module only takesthe final translation outputs from individual systems,including the output from the glass-box combina-tion.
For each input source sentence, it selects the?optimal?
system output based on certain featurefunctions.We experiment with two feature functions.
Oneis a typical 5-gram word language model (LM).
Theoptimal translation output E?
is selected among thetop-one hypothesis from all the systems accordingto their LM scores.
Let ei be a word in sentence E:E?
= arg minE?
log P5glm(E) (10)= arg minE?i?
log p(ei|ei?1i?4),where ei?1i?4 is the n-gram history,(ei?4, ei?3, ei?2, ei?1).Another feature function is based on the 5-gramLM score calculated on the mixed stream of wordand POS tags of the translation output.
We run POStagging on the translation hypotheses.
We keep theword identities of top N frequent words (N=1000in our experiments), and the remaining words are re-placed with their POS tags.
As a result, the mixedstream is like a skeleton of the original sentence, asshown in Figure 3.With this model, the optimal translation output E?is selected based on the following formula:E?
= arg minE?
log Pwplm(E) (11)= arg minE?i?
log p(T (ei)|T (e)i?1i?4)where the mixed stream token T (e) = e when e ?N , and T (e) = POS(e) when e > N .
Similar toa class-based LM, this model is less prone to datasparseness problems.4 ExperimentsWe experiment with different system combinationstrategies on the NIST 2003 Arabic-English MTevaluation test set.
Testset-specific bilingual dataare subsampled, which include 260K sentence pairs,10.8M Arabic words and 13.5M English words.
Wereport case-sensitive BLEU (Papineni et al, 2001)BLEUr4n4c TERsys1 0.5323 43.11sys4 0.4742 46.35Tstcom 0.5429 42.64Tstcom+Sentcom 0.5466 42.32Tstcom+Sentcom+Prune 0.5505 42.21Table 1: Translation results with phrase combinationand pruning.and TER (Snover et al, 2006) as the MT evaluationmetrics.
We evaluate the translation quality of dif-ferent combination strategies:?
WdCom: Combine testset-specific word trans-lation model with the baseline model, as de-scribed in section 3.1.?
PhrCom: Combine and prune phrase trans-lation tables from all systems, as describedin section 3.2.
This include testset-specificphrase table combination (Tstcom), sen-tence level phrase combination (Sentcom) andphrase pruning based on translation hypotheses(Prune).?
Path: Encourage search along the decodingpaths adopted by other systems via path match-ing cost function, as described in section 3.3.?
SenSel: Select whole sentence translation hy-pothesis among all systems?
top-one outputsbased on N-gram language models trained onword stream (word) and word-POS mixedstream(wdpos).Table 1 shows the improvement by combiningphrase tables from multiple MT systems using dif-ferent combination strategies.
We only show thehighest and lowest baseline system scores.
By com-bining testset-specific phrase translation tables (Tst-com), we achieved 1.0 BLEU improvement and 0.5TER reduction.
Sentence-level phrase combinationand pruning additionally improve the BLEU scoreby 0.7 point and reduce TER by 0.4 percent.Table 2 shows the improvement with differ-ent sentence translation hypothesis selection ap-proaches.
The word-based LM is trained with about1.75G words from newswire text.
A distributed282BLEUr4n4c TERsys1 0.5323 43.11sys2 0.5320 43.06SentSel-word: 0.5354 42.56SentSel-wpmix: 0.5380 43.06Table 2: Translation results with different sentencehypothesis selection strategies.BLEUr4n4c TERsys1 0.5323 43.11sys2 0.5320 43.06sys3 0.4922 46.03sys4 0.4742 46.35WdCom 0.5339 42.60WdCom+PhrCom 0.5528 41.98WdCom+PhrCom+Path 0.5543 41.75WdCom+PhrCom+Path+SenSel 0.5565 41.59Table 3: Translation results with hierarchical systemcombination strategy.large-scale language model architecture is devel-oped to handle such large training corpora3, as de-scribed in (Emami et al, 2007).
The word-based LMshows both improvement in BLEU scores and errorreduction in TER.
On the other hand, even thoughthe word-POS LM is trained with much less data(about 136M words), it improves BLEU score moreeffectively, though there is no change in TER.Table 3 shows the improvements from hierarchi-cal system combination strategy.
We find that word-based translation combination improves the baselineblock decoder by 0.16 BLEU point and reduce TERby 0.5 point.
Phrase-based translation combina-tion (including phrase table combination, sentence-level phrase combination and phrase pruning) fur-ther improves the BLEU score by 1.9 point (another0.6 drop in TER).
By encouraging the search alongother decoder?s decoding paths, we observed addi-tional 0.15 BLEU improvement and 0.2 TER reduc-tion.
Finally, sentence translation hypothesis selec-tion with word-based LM led to 0.2 BLEU pointimprovement and 0.16 point reduction in TER.
To3The same LM is also used during first pass decoding byboth the block and the DTM decoders.BLEUr4n4c TERsys1 0.3205 60.48sys2 0.3057 59.99sys3 0.2787 64.46sys4 0.2823 59.19sys5 0.3028 62.16syscom 0.3409 58.89Table 4: System combination results on Chinese-English translation.BLEUr1n4c TERsys1 0.1261 71.70sys2 0.1307 77.52sys3 0.1282 70.82sys4 0.1259 70.20syscom 0.1386 69.23Table 5: System combination results for Arabic-English web log translation.summarize, with the hierarchical system combina-tion framework, we achieved 2.4 BLEU point im-provement over the best baseline system, and reducethe TER by 1.4 point.Table 4 shows the system combination results onChinese-English newswire translation.
The test datais NIST MT03 Chinese-English evaluation test set.In addition to the 4 baseline MT systems, we alsoadd another phrase-based MT system (Lee et al,2006).
The system combination improves over thebest baseline system by 2 BLEU points, and reducethe TER score by 1.6 percent.
Thanks to the longrange constituent reordering capability of differentbaseline systems, the path imitation improves theBLEU score by 0.4 point.We consistently notice improved translation qual-ity with system combination on unstructured textand speech translations, as shown in Table 5 and 6.With one reference translation, we notice 1.2 BLEUpoint improvement over the baseline block decoder(with 2.5 point TER reduction) on web log transla-tion and about 2.1 point BLEU improvement (with0.9 point TER reduction) on Broadcast News speechtranslation.283BLEUr1n4c TERsys1 0.2011 61.46sys2 0.2211 66.32sys3 0.2074 61.21sys4 0.1258 85.45syscom 0.2221 60.54Table 6: System combination results for Arabic-English speech translation.5 Related WorkMany system combination research have been donerecently.
(Matusov et al, 2006) computes consen-sus translation by voting on a confusion network,which is created by pairwise word alignment of mul-tiple baseline MT hypotheses.
This is similar to thesentence- and word- level combinations in (Rostiet al, 2007), where TER is used to align multi-ple hypotheses.
Both approaches adopt black-boxcombination strategy, as target translations are com-bined independent of source sentences.
(Rosti et al,2007) extracts phrase translation pairs in the phraselevel combination.
Our proposed method incorpo-rates bilingual information from source and targetsentences in a hierarchical framework: word, phraseand decoding path combinations.
Such informationproves very helpful in our experiments.
We also de-veloped a path matching cost function to encouragedecoding path imitation, thus enable one decoder totake advantage of rich reordering models of otherMT systems.
We only combine top-one hypothesisfrom each system, and did not apply system confi-dence measure and minimum error rate training totune system combination weights.
This will be ourfuture work.6 ConclusionOur hierarchical system combination strategy effec-tively integrates word and phrase translation com-binations, decoding path imitation and sentence hy-pothesis selection from multiple MT systems.
Byboosting common word and phrase translation pairsand pruning unused ones, we obtain better transla-tion quality with less re-decoding time.
By imitat-ing the decoding paths, we take advantage of variousreordering schemes from different decoders.
Thesentence hypothesis selection based on N-gram lan-guage model further improves the translation qual-ity.
The effectiveness has been consistently provedin several empirical studies with test sets in differentlanguages and covering different genres.7 AcknowledgmentThe authors would like to thank Yaser Al-Onaizan,Abraham Ittycheriah and Salim Roukos for help-ful discussions and suggestions.
This work is sup-ported under the DARPA GALE project, contractNo.
HR0011-06-2-0001.ReferencesYaser Al-Onaizan and Kishore Papineni.
2006.
Dis-tortion Models for Statistical Machine Translation.In Proceedings of the 21st International Conferenceon Computational Linguistics and 44th Annual Meet-ing of the Association for Computational Linguistics,pages 529?536, Sydney, Australia, July.
Associationfor Computational Linguistics.Peter F. Brown, Stephen Della Pietra, Vincent J. DellaPietra, and Robert L. Mercer.
1994.
The Mathematicof Statistical Machine Translation: Parameter Estima-tion.
Computational Linguistics, 19(2):263?311.David Chiang.
2005.
A Hierarchical Phrase-BasedModel for Statistical Machine Translation.
In Pro-ceedings of the 43rd Annual Meeting of the Associ-ation for Computational Linguistics (ACL?05), pages263?270, Ann Arbor, Michigan, June.
Association forComputational Linguistics.Ahmad Emami, Kishore Papineni, and Jeffrey Sorensen.2007.
Large-scale Distributed Language Modeling.In Proceedings of the 2007 International Conferenceon Acoustics, Speech, and Signal Processing (ICASSP2007), Honolulu, Hawaii, April.Sanjika Hewavitharana, Bing Zhao, Almut Silja Hilde-brand, Matthias Eck, Chiori Hori, Stephan Vogel, andAlex Waibel.
2005.
The CMU Statistical MachineTranslation System for IWSLT2005.
In Proceedingsof IWSLT 2005, Pittsburgh, PA, USA, November.Arraham Ittycheriah and Salim Roukos.
2007.
Di-rect Translation Model2.
In Proceedings of the 2007Human Language Technologies: The Annual Confer-ence of the North American Chapter of the Associationfor Computational Linguistics (NAACL-HLT 2007),Rochester, NY, April.
Association for ComputationalLinguistics.284Shyamsundar Jayaraman and Alon Lavie.
2005.
Multi-Engine Machine Translation Guided by Explicit WordMatching.
In Proceedings of the ACL InteractivePoster and Demonstration Sessions, pages 101?104,Ann Arbor, Michigan, June.
Association for Compu-tational Linguistics.Y-S. Lee, S. Roukos, Y. Al-Onaizan, and K. Papineni.2006.
IBM Spoken Language Translation System.In Proc.
of TC-STAR Workshop on Speech-to-SpeechTranslation, Barcelona, Spain.Evgeny Matusov, Nicola Ueffing, and Hermann Ney.2006.
Computing Consensus Translation for Multi-ple Machine Translation Systems Using Enhanced Hy-pothesis Alignment.
In Proceedings of the 11th Con-ference of the European Chapter of the Associationfor Computational Linguistics (EACL ?06), pages 263?270, Trento, Italy, April.
Association for Computa-tional Linguistics.B.
Mellebeek, K. Owczarzak, J.
Van Genabith, andA.
Way.
2006.
Multi-Engine Machine Translation byRecursive Sentence Decomposition.
In Proceedingsof the 7th biennial conference of the Association forMachine Translation in the Americas, pages 110?118,Boston, MA, June.Sergei Nirenburg and Robert Frederking.
1994.
TowardMulti-engine Machine Translation.
In HLT ?94: Pro-ceedings of the workshop on Human Language Tech-nology, pages 147?151, Morristown, NJ, USA.
Asso-ciation for Computational Linguistics.Tadashi Nomoto.
2004.
Multi-Engine Machine Transla-tion with Voted Language Model.
In Proceedings ofACL, pages 494?501.Franz Josef Och.
2003.
Minimum Error Rate Trainingin Statistical Machine Translation.
In Proceedings ofACL, pages 160?167.Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu.
2001.
BLEU: a Method for AutomaticEvaluation of Machine Translation.
In ACL ?02: Pro-ceedings of the 40th Annual Meeting on Associationfor Computational Linguistics, pages 311?318, Mor-ristown, NJ, USA.
Association for Computational Lin-guistics.Antti-Veikko Rosti, Necip Fazil Ayan, Bing Xiang, Spy-ros Matsoukas, Richard Schwartz, and Bonnie J.Dorr.
2007.
Combining Translations from Mul-tiple Machine Translation Systems.
In Proceed-ings of the Conference on Human Language Technol-ogy and North American chapter of the Associationfor Computational Linguistics Annual Meeting (HLT-NAACL?2007), Rochester, NY, April.Matthew Snover, Bonnie Dorr, Richard Schwartz, Lin-nea Micciulla, and John Makhoul.
2006.
A Studyof Translation Edit Rate with Targeted Human An-notation.
In Proceedings of Association for MachineTranslation in the Americas.D.
Tidhar and U. Kussner.
2000.
Learning to Select aGood Translation.
In Proceedings of the InternationalConference on Computational Linguistics, pages 843?849.285System 1 System 2 System NWordCombinationPhraseCombination& PruningDecoderDecodingPathImitationSentenceHypothesisSelectionDecoderTargetTranslationSourceTextFigure 1: Hierarchical MT system combination ar-chitecture.
The top dot-line rectangle is similar tothe glass-box combination, and the bottom rectanglewith sentence selection is similar to the black-boxcombination.Original Sentence:Word-POS mixed stream:in short , making a good plan at thebeginning of the construction is the crucialmeasure for reducing haphazard economicdevelopment .in JJ , making a good plan at the NN of theconstruction is the JJ NN for VBG JJeconomic development .Figure 3: Sentence with Word-POS mixed stream.286
