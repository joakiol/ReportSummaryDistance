Maximal Consistent SubsetsRobert Malouf?San Diego State UniversityDefault unification operations combine strict information with information from one or moredefeasible feature structures.
Many such operations require finding the maximal subsets of a setof atomic constraints that are consistent with each other and with the strict feature structure,where a subset is maximally consistent with respect to the subsumption ordering if no constraintcan be added to it without creating an inconsistency.
Although this problem is NP-complete,there are a number of heuristic optimizations that can be used to substantially reduce the size ofthe search space.
In this article, we propose a novel optimization, leaf pruning, which in somecases yields an improvement in running time of several orders of magnitude over previouslydescribed algorithms.
This makes default unification efficient enough to be practical for a widerange of problems and applications.1.
IntroductionIn unification-based grammatical frameworks, it often desirable to combine informationfrom possibly inconsistent sources.
Over the years, a number of default unificationoperations have been proposed1, which combine a strict feature structure with one ormore defeasible feature structures.
These operations preserve all information in thestrict feature structure, while bringing in as much information as possible from thedefeasible structures.
Default unification has been used to address a wide variety oflinguistic knowledge representation problems, including lexical inheritance hierarchies(Copestake 1993; Ginzburg and Sag 2001), lexical semantics (Lascarides and Copestake1998), grammar induction (Briscoe 1999; Villavicencio 2002; Petersen 2004), anaphoraresolution (Grover et al 1994; Pru?st, Scha, and van den Berg 1994), and discourseprocessing (Gurevych et al 2003; Alexandersson, Becker, and Pfleger 2004), amongmany others.Although the various default unification operators differ in their particulars, mostinvolve something like Carpenter (1992)?s credulous default unification as one step.The result of credulously adding the default information in G to the strict informationin F is:cred-unify(F, G) ?
{unify(F, G?
), where G?
subsumes G andG?
is maximal such that unify(F, G?)
is defined}?
Department of Linguistics and Asian/Middle Eastern Languages, San Diego State University, 5500Campanile Drive, San Diego, CA 92182-7727 USA, E-mail: rmalouf@mail.sdsu.edu.1 For example, Bouma (1992), Carpenter (1992), Pru?st (1992), Calder (1993), Lascarides and Copestake(1999), Alexandersson and Becker (2001), and Ninomiya, Miyao, and Tsujii (2002).
See Bouma (2006) for arecent overview.?
2007 Association for Computational LinguisticsComputational Linguistics Volume 33, Number 2In other words, cred-unify(F, G) is the result of unifying F with the maximal consistentsubset(s) of the atomic constraints in G. A subset of constraints is maximally consistentwith respect to the subsumption ordering if no constraint can be added to it without cre-ating an inconsistency.
In general, there may be more than one subset of the constraintsin G that is consistent with F and maximal, so the result of credulous default unificationis a set of feature structures.One example of the use of credulous default unification for the resolution of dis-course anaphora comes from Grover et al (1994).
Consider the mini-discourse: Jessy likesher brother.
So does Hannah.
To resolve the anaphoric predicate in the second sentence, wecan set up meaning representations for the source and the target:Source:???
?REL likeAGENT 1 jessyPATIENT[REL brotherTHEME 1]???
?Target:[AGENT hannah]and credulously unify the source with the strict information in the target.To proceed, we first decompose the source feature structure into the following fiveatomic ground constraints:{[REL like],[AGENT jessy],[PATIENT|REL brother],[PATIENT|THEME jessy],[AGENT 1PATIENT|THEME 1]}Then, we find the maximal subsets of the remaining constraints which are mutuallyconsistent with the target.
This yields two solutions:???
?REL likeAGENT 1 hannahPATIENT[REL brotherTHEME 1]???????
?REL likeAGENT hannahPATIENT[REL brotherTHEME jessy]???
?corresponding to the sloppy identity and the strict identity readings of the anaphoricexpression.2.
AlgorithmA key step for applying most default unification operators is finding the maximal con-sistent subsets of a set of constraints C. Unfortunately, finding these maximal consistentsubsets is an expensive operation, and, in fact, is NP-complete.
Let T = {T1, .
.
.
, Tm} bethe set of conflicts in C, where a conflict is a minimal set of constraints that are mutuallyinconsistent with the target.
For this example, T consists of the two conflicts:{{[PATIENT|THEME jessy],[AGENT 1PATIENT|THEME 1]},{[AGENT jessy]}}Removing any one of the constraints from a conflict Ti would break that conflict, so if wecould remove from C at least one member of each Ti, the remaining constraints would be154Malouf Maximal Consistent Subsetsmutually consistent with the target information.
Finding the maximal consistent subsetsof C then is equivalent to finding the minimal subset C?
?
C such that each Ti containsat least one member of C?.
This is the hitting subset problem, a classic NP-completeproblem (Karp 1972).An algorithm to find the maximal consistent subsets of C must check each subset ofC for consistency.
One way to proceed is to construct a spanning tree of the booleanlattice of subsets of C. This takes the form of a binomial tree, as in Figure 1 (Birdand Hinze 2003).
At each node, we keep track of k, the index of the constraint thatwas removed from the parent set to create that subset.
For example, subset {c1, c3}was formed by removing c2 from {c1, c2, c3}, so k = 2.
The descendants of a node areconstructed by successively dropping each of the constraints ci where k < i ?
|C|.
Thisensures that we will visit every subset of C exactly once.The algorithm described by Grover et al (1994) performs a breadth-first searchof the subset lattice, with one important optimization.
Because the cardinality of thesubsets at each level is one greater than those on the level below it, a breadth-firstsearch of this tree will consider all larger sets before considering any smaller ones.Furthermore, because each subset is produced by removing constraints from its parentset, every node in a subtree is a subset of its root.
This means that once a consistent setis found, no descendants of that set can be maximal, and that subtree can be prunedfrom the search space.
However, consistent subsets that are maximal in their branch ofthe tree may turn out not to be globally maximal.
For example, in Figure 1, if {c1, c2}is consistent and {c1, c3} is not, a breadth-first search would identify both {c1, c2} and{c1} as consistent and (locally) maximal.
A final post-check for set inclusion can removepseudo-maximal results like {c1}.In addition to pruning branches rooted by a consistent subset (call this root prun-ing), the organization of the search space into a binomial tree allows another valuableoptimization.
The deepest leaf node in any subtree is the set formed from the root byremoving all constraints ck<i?|C|, and every set in the subset is a superset of that deepestleaf.
Because no superset of an inconsistent set of constraints can be consistent, if thefoot of a subtree is inconsistent then clearly no node in the tree can be consistent, andthe entire tree can be skipped (call this leaf pruning).
Taking both root pruning andleaf pruning together, the only subtrees that need to be explored are those whose root isFigure 1Boolean lattice and binomial spanning tree for |C| = 3.155Computational Linguistics Volume 33, Number 2inconsistent but whose deepest leaf is consistent.
No other subtrees can possibly containa solution.Figure 2 gives a breadth-first search algorithm that implements these optimiza-tions.
Like Grover et al (1994), this algorithm requires a post-check to remove pseudo-maximal subsets from results.
A queue is used to keep track of subsets S that are yet to bechecked for consistency, along with the index k of the constraint that was last dropped,and a flag leftmost that indicates whether that subset is the leftmost child.
Because thedeepest leaf node of the leftmost child is the same as the deepest leaf node of the parent,we are guaranteed that the deepest leaf of a leftmost child is consistent.
Keeping trackof leftmost children allows us to avoid a substantial number of redundant consistencychecks.3.
EvaluationThe graphs in Figure 3 and Figure 4 show an empirical comparison between a breadth-first search with root pruning (BFS-R) and a breadth-first search with root and leafpruning (BFS-RL) on randomly generated problems.
The graphs show the number ofsubsets that were checked for consistency, as it relates to |C|, the number of constraints,and p, the probability that two members of C are consistent.
Larger values for p generallylead to fewer but larger maximal consistent subsets.
All counts are averaged across 100randomly generated sets of ground constraints.
In generating random problems, wemake the simplifying assumptions that all constraints are consistent with any inde-feasible information, and that a subset of constraints that are pairwise consistent is aconsistent subset.The first thing to note in these graphs is that root pruning by itself provides verylittle benefit.
For most values of p, the number of subsets checked by BFS-R is veryclose to the worst case maximum 2|C|.
A possible reason for this is that root pruningFigure 2Find the maximal consistent subsets of C. Performs a breadth-first search of the subset tree, withroot and leaf pruning.156Malouf Maximal Consistent SubsetsFigure 3Comparison of breadth-first search using root pruning alone (BFS-R) and in combination withleaf pruning (BFS-RL).
|C| is the number of ground constraints, p is the fraction of the groundconstraints which are pairwise consistent, and ?Subsets visited?
is the number of subsets of Cwhich were checked for consistency (on a logarithmic scale).
All counts are based on the averageof 100 randomly generated problems.will have the greatest effect when consistent subsets are found in the interior nodes ofthe binomial search tree.
However, the configuration of the search space is such thatmost nodes are either leaves or very close to leaves, and only a few nodes have a largenumber of descendants.
Therefore, root pruning mostly removes very small subtreesand has only a small effect on the overall cost of the algorithm.The next observation to make is that for small values of |C| (in these experiments,less than 7), BFS-RL is very slightly more expensive than BFS-R.
In these cases, theadvantages of leaf pruning do not outweigh the cost of the extra consistency checksrequired to implement it.
As |C| increases, though, leaf pruning can offer substantialimprovements.
For |C| = 19 and p = 0.1, leaf pruning eliminates more than 99.5% ofthe search space, leading to a 185-fold improvement in running time.
As p increases,the benefits of leaf pruning do become more modest.
Larger values of p mean fewerinconsistent leaf nodes, so fewer subtrees are able to be eliminated.
Even so, the savings157Computational Linguistics Volume 33, Number 2Figure 4Comparison of subsets visited by BFS-R and BFS-RL as a function of p for |C| = 15.
All countsare based on the average of 100 randomly generated problems.from leaf pruning can still be dramatic: at |C| = 19 and p = 0.9, leaf pruning yields anearly five-fold improvement in speed.Values of |C| and p that can be realistically expected will vary widely from ap-plication to application.
An anonymous reviewer reports that in one application, theresolution of non-monotonic lexical inheritance for constraint-based grammars, p isgenerally greater than 0.7.
This may be due in part to the fact that most constraint-based grammar development platforms do not support defaults (Copestake?s [2002]LKB is a notable exception), and so grammar engineers tend to avoid the use of defaultoverriding.
Ginzburg and Sag (2001) propose a more comprehensive use of defaults, andgrammars written following these principles would likely have a much lower value ofp.
To my knowledge, however, these ideas have not yet made their way into any large-scale grammar implementations.Ninomiya, Miyao, and Tsujii (2002) describe experiments using default unificationfor robust parsing and automatic grammar augmentation via a kind of explanation-based learning.
For this application, all features of a rule in the base XHPSG grammar(Tateisi et al 1998) are considered defaults that can be overridden if necessary to get asuccessful parse of a sentence.
In this case, |C| is likely very large and grows quicklywith the length of the sentences being parsed.
The value of p will depend on thecoverage of the base grammar, but can be expected to be fairly close to 1.0 for mostdomains.
In situations such as this, where p is expected to fall close to the worst casefor leaf pruning, one could consider inverting the search direction of the algorithm inFigure 2.
Rather than beginning with C and removing constraints until a consistentsubset is found, we could instead begin with the empty subset and add constraintsuntil an inconsistency is found.
In either case, the frontier in the search space betweenconsistent and inconsistent subsets is where maximally consistent subsets will be found,158Malouf Maximal Consistent Subsetsand leaf pruning can be used to eliminate regions of the search space that contain onlyconsistent or inconsistent subsets.4.
ConclusionsFinding the maximal consistent subsets of a set of ground constraints is an impor-tant sub-problem for many natural language processing and knowledge representa-tion tasks.
Unfortunately, the problem is NP-complete, and in the worst case requireschecking all 2|C| subsets of C for consistency.
Previously proposed algorithms haveproduced approximate solutions (Boppana and Halldo?rsson 1992), or have weakenedthe requirements to make finding a solution easier (Ninomiya, Miyao, and Tsujii 2002).By using deepest leaf pruning, the algorithm described in the previous sectionsimproves on the method of Grover et al (1994) and is able to achieve substantial gainsover the worst case running time for a large class of problems.
An efficient method forfinding maximal consistent subsets can make default unification practical for problemssuch as large-scale lexical representation, on-line discourse processing, or ontologyconstruction.ReferencesAlexandersson, Jan and Tilman Becker.
2001.Overlay as the basic operation fordiscourse processing in a multimodaldialogue system.
In Proceedings of the IJCAIWorkshop ?Knowledge and Reasoning inPractical Dialogue Systems,?
pages 8?14,Seattle, WA.Alexandersson, Jan, Tilman Becker, andNorbert Pfleger.
2004.
Scoring for overlaybased on informational distance.
InProceedings of KONVENS 2004, pages 1?4,Vienna, Austria.Bird, Richard and Ralf Hinze.
2003.Functional pearl: Trouble shared is troublehalved.
In Proceedings of the 2003 ACMSIGPLAN Workshop on Haskell, pages 1?6,Uppsala, Sweden.Boppana, Ravi and Magnu?s M. Halldo?rsson.1992.
Approximating maximumindependent sets by excluding subgraphs.BIT Numerical Mathematics, 32(2):180?196.Bouma, Gosse.
1992.
Feature structures andnonmonotonicity.
ComputationalLinguistics, 18(2):183?204.Bouma, Gosse.
2006.
Unification: Classicaland default.
In Keith Brown, editor,Encyclopedia of Language and Linguistics.Elsevier, New York.Briscoe, E. J.
1999.
The acquisition ofgrammar in an evolving population oflanguage agents.
Electronic Transactions inArtificial Intelligence.
Special Issue: MachineIntelligence, 3(035):47?77.Calder, Jonathan.
1993.
Feature-value logics:Some limits on the role of defaults.
In C. J.Rupp, Mike Rosner, and Rod Johnson,editors, Constraints, Language andComputation.
Academic Press, London,pages 20?32.Carpenter, Bob.
1992.
Skeptical andcreduluous default unification withapplications to templates and inheritance.In Ted Briscoe, Anne Copestake, andValerie de Paiva, editors, DefaultInheritance within Unification-BasedApproaches to the Lexicon.
CambridgeUniversity Press, Cambridge, UK,pages 13?37.Copestake, Ann.
1993.
Defaults in lexicalrepresentation.
In E. J. Briscoe,A.
Copestake, and V. de Paiva, editors,Inheritance, Defaults and the Lexicon.Cambridge University Press, Cambridge,UK, pages 223?245.Copestake, Ann.
2002.
Implementing TypedFeature Structure Grammars.
CSLIPublications, Stanford, CA.Ginzburg, Jonathan and Ivan A.
Sag.
2001.Interrogative Investigations.
CSLIPublications, Stanford, CA.Grover, Claire, Chris Brew, Marc Moens,and Suresh Manandhar.
1994.
Priorityunion and generalisation in discoursegrammar.
In Proceedings of the 32nd AnnualMeeting of the Association for ComputationalLinguistics, pages 17?24, Las Cruces,New Mexico.Gurevych, Iryna, Robert Porzel, Elena Slinko,Norbert Pfleger, Jan Alexandersson,and Stefan Merten.
2003.
Less is more:Using a single knowledge representation159Computational Linguistics Volume 33, Number 2in dialog systems.
In Proceedings of theHLT-NAACL 2003 Workshop on TextMeaning, pages 14?21, Edmonton, Alberta.Karp, Richard.
1972.
Reducibility amongcombinatorial problems.
In R. E. Millerand J. W. Thatcher, editors, Complexity ofComputer Computations.
Plenum Press,New York, pages 85?103.Lascarides, Alex and Ann Copestake.
1998.Pragmatics and word meaning.
Journal ofLinguistics, 34:387?414.Lascarides, Alex and Ann Copestake.
1999.Default representation in constraint-basedframeworks.
Computational Linguistics,25:55?105.Ninomiya, Takashi, Yusuke Miyao, andJun?ichi Tsujii.
2002.
Lenient defaultunification for robust processing withinunification based grammar formalisms.In Proceedings of the 19th InternationalConference on Computational Linguistics(COLING), pages 1?7, Taipei, Taiwan.Petersen, Wiebke.
2004.
A set-theoreticapproach for the induction of inheritancehierarchies.
Electronic Notes in TheoreticalComputer Science, 53:296?308.Pru?st, Hub.
1992.
On Discourse Structuring,VP Anaphora and Gapping.
Ph.D. thesis,University of Amsterdam.Pru?st, Hub, Remko Scha, and Martinvan den Berg.
1994.
Discourse grammarand verb phrase anaphora.
Linguistics andPhilosophy, 17(3):261?327.Tateisi, Yuka, Kentaro Torisawa, YusukeMiyao, and Jun?ichi Tsujii.
1998.Translating the XTAG English grammarto HPSG.
In Proceedings of the FourthInternational Workshop on Tree AdjoiningGrammars and Related Frameworks (TAG+4),pages 172?175, Philadelphia, PA.Villavicencio, Aline.
2002.
The Acquisitionof a Unification-Based Generalised CategorialGrammar.
Ph.D. thesis, CambridgeUniversity.160
