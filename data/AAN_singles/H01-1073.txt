University of Colorado Dialog Systems forTravel and NavigationB.
Pellom, W. Ward, J. Hansen, R. Cole, K. Hacioglu, J. Zhang, X. Yu, S. PradhanCenter for Spoken Language Research, University of ColoradoBoulder, Colorado 80303, USA{pellom, whw, jhlh, cole, hacioglu, zjp, xiu, spradhan}@cslr.colorado.eduABSTRACTThis paper presents recent improvements in the development ofthe University of Colorado ?CU Communicator?
and ?CU-Move?
spoken dialog systems.
First, we describe the CUCommunicator system that integrates speech recognition,synthesis and natural language understanding technologies usingthe DARPA Hub Architecture.
Users are able to converse with anautomated travel agent over the phone to retrieve up-to-datetravel information such as flight schedules, pricing, along withhotel and rental car availability.
The CU Communicator hasbeen under development since April of 1999 and represents ourtest-bed system for developing robust human-computerinteractions where reusability and dialogue system portabilityserve as two main goals of our work.
Next, we describe our morerecent work on the CU Move dialog system for in-vehicle routeplanning and guidance.
This work is in joint collaboration withHRL and is sponsored as part of the DARPA Communicatorprogram.
Specifically, we will provide an overview of the task,describe the data collection environment for in-vehicle systemsdevelopment, and describe our initial dialog system constructedfor route planning.1.
CU COMMUNICATOR1.1 OverviewThe Travel Planning TaskThe CU Communicator system [1,2] is a Hub compliantimplementation of the DARPA Communicator task [3].
Thesystem combines continuous speech recognition, naturallanguage understanding and flexible dialogue control to enablenatural conversational interaction by telephone callers to accessinformation from the Internet pertaining to airline flights, hotelsand rental cars.
Specifically, users can describe a desired airlineflight itinerary to the Communicator and use natural dialog tonegotiate a flight plan.
Users can also inquire about hotelavailability and pricing as well as obtain rental car reservationinformation.System OverviewThe dialog system is composed of a Hub and several servers asshown in Fig.
1.
The Hub is used as a centralized message routerthrough which servers can communicate with one another [4].Frames containing keys and values are emitted by each server,routed by the hub, and received by a secondary server based onrules defined in a ?Hub script?.Figure 1.
Block diagram of the functional components thatcomprise the CU Communicator system1.1.2 Audio ServerThe audio server is responsible for answering the incoming call,playing prompts and recording user input.
Currently, our systemuses the MIT/MITRE audio server that was provided to DARPACommunicator program participants.
The telephony hardwareconsists of an external serial modem device that connects to themicrophone input and speaker output terminals on the hostcomputer.
The record process is pipelined to the speechrecognition server and the play process is pipelined the text-to-speech server.
This audio server does not support barge-in.Recently we have developed a new audio server that supportsbarge-in using the Dialogic hardware platform.
The new audioserver implements a Fast Normalized Least-Mean-Square (LMS)algorithm for software-based echo cancellation.
Duringoperation, the echo from the system speech is actively cancelledfrom the recorded audio to allow the user to cut through while1This work was supported by DARPA through SPAWAR underGrant No.
N66001-002-8906.
The ?CU Move?
system issupported in part through a joint collaboration with HRLLaboratories.LanguageGeneratorLanguageeneratorHubSpeechRecognizerSpeechRecognizerSpeechSynthesizerSpeechSynthesizerSemanticParserSe anticParserDialogueManagerDialogueanagerData Base /BackendData Base /BackendConfidenceServerConfidenceServerAudio ServerAudio Serverwwwthe system is speaking.
The new audio server operates in theLinux environment and is currently being field-tested at CSLR.Because the server implements software-based echo cancellation,it can work on virtually any low-cost Dialogic hardwareplatform.
This server will be made available to the researchcommunity as a resource in the near future.1.3 Speech RecognizerWe are currently using the Carnegie Mellon University Sphinx-IIsystem [5] in our speech recognition server.
This is a semi-continuous Hidden Markov Model recognizer with a classtrigram language model.
The recognition server receives theinput vectors from the audio server.
The recognition serverproduces a word lattice from which a single best hypothesis ispicked and sent to the hub for processing by the dialog manager.Acoustic ModelingDuring dialog interaction with the user, the audio server sendsthe acoustic samples to three Sphinx-II speech recognizers.While the language model is the same for each decoder, theacoustic models consist of (i) speaker independent analogtelephone, (ii) female adapted analog telephone, and (iii) cellulartelephone adapted acoustic model sets.
Each decoder outputs aword string hypothesis along with a word-sequence probabilityfor the best path.
An intermediate server is used to examine eachhypothesis and pass the most likely word string onto the naturallanguage understanding module.Language ModelingThe Communicator system is designed for end users to get up-to-date worldwide air travel, hotel and rental car information via thetelephone.
In the task there are word lists for countries, cities,states, airlines, etc.
To train a robust language model, names areclustered into different classes.
An utterance with class tagging isshown in Fig.2.
In this example, city, hour_number, and am_pmare class names.Figure 2.
Examples of class-based and grammar-basedlanguage modelingEach commonly used word takes one class.
The probability ofword Wi given class Ci is estimated from training corpora.
Afterthe corpora are correctly tagged, a back-off class-based trigramlanguage model can be computed from the tagged corpora.
Weuse the CMU-Cambridge Statistical Language Modeling Toolkitto compute our language models.More recently, we have developed a dialog context dependentlanguage model (LM) combining stochastic context freegrammars (SCFGs) and n-grams [6,7].
Based on a spokenlanguage production model in which a user picks a set ofconcepts with respective values and constructs word sequencesusing phrase generators associated with each concept inaccordance with the dialog context, this LM computes theprobability of a word, P(W), asP(W) = P(W/C) P(C/S)          (1)where W is the sequence of words, C is the sequence of conceptsand S is the dialog context.
Here, the assumptions are (i) S isgiven, (ii) W is independent of S but C, and (iii) W and Cassociations are unambiguous.
This formulation can beconsidered as a general extension of the standard class wordbased statistical language model as seen in Fig.
2.The first term in (1) is modeled by SCFGs, one for each concept.The concepts are classes of phrases with the same meaning.
EachSCFG is compiled into a stochastic recursive transition network(STRN).
Our grammar is a semantic grammar since thenonterminals correspond to semantic concepts instead ofsyntactic constituents.
The set of task specific concepts isaugmented with a single word, multiple word and a small numberof broad but unambigious part of speech (POS) classes toaccount for the phrases that are not covered by the grammar.These classes are considered as "filler" concepts within a unifiedframework.
The second term in (1) is modeled as a pool ofconcept n-gram LMs.
That is, we have a separate LM for eachdialog context.
At the moment, the dialog context is selected asthe last question prompted by the system, as it is very simple andyet strongly predictive and constraining.
SCFG and n-gramprobabilities are learned by simple counting and smoothing.
Oursemantic grammars have a low degree of ambiguity and thereforedo not require computationally intensive stochastic training andparsing techniques.Experimental results with N-best list rescoring were foundpromising (5-6% relative improvement in WER).
In addition, wehave shown that a dynamic combining of our new LM and thestandard class word n-gram (the LM currently in use in oursystem) should result in further improvements.
At the present, weare interfacing the grammar LM to the speech recognizer using aword graph.1.4 Confidence ServerOur prior work on confidence assessment has considereddetection and rejection of word-level speech recognition errorsand out-of-domain phrases using language model features [8].More recently [9], we have considered detection and rejection ofmisrecognized units at the concept level.
Because concepts areused to update the state of the dialog system, we believe thatconcept level confidence is vitally important to ensuring agraceful human-computer interaction.
Our current work onconcept error detection has considered language model features(e.g., LM back-off behavior, language model score) as well asacoustic features from the speech recognizer (e.g., normalizedacoustic score, lattice density, phone perplexity).
ConfidenceOriginal UtteranceI want to go from Boston to Portland around nine a_mClass-Tagged UtteranceI want to go from [city:Boston] to [city:Portland]around [hour_number:nine] [am_pm:a_m]Concept-Tagged Utterance[I_want: I want to go] [depart_loc: from Boston][arrive_loc: to Portland] [time:around nine a_m]features are combined to compute word-level, concept-level, andutterance-level confidence scores.1.5 Language UnderstandingWe use a modified version of the Phoenix [10] parser to map thespeech recognizer output onto a sequence of semantic frames.
APhoenix frame is a named set of slots, where the slots representrelated pieces of information.
Each slot has an associatedcontext-free semantic grammar that specifies word string patternsthat can fill the slot.
The grammars are compiled into RecursiveTransition Networks, which are matched against the recognizeroutput to fill slots.
Each filled slot contains a semantic parse treewith the slot name as root.Phoenix has been modified to also produce an extractedrepresentation of the parse that maps directly onto the taskconcept structures.
For example, the utterance?I want to go from Boston to Denver Tuesday morning?would produce the extracted parse:Flight_Constraint: Depart_Location.City.BostonFlight_Constraint: Arrive_Location.City.DenverFlight Constraints:[Date_Time].[Date].[Day_Name].tuesday[Time_Range].
[Period_Of_Day].morning1.6 Dialog ManagementThe Dialogue Manager controls the system?s interaction with theuser and the application server.
It is responsible for decidingwhat action the system will take at each step in the interaction.The Dialogue Manager has several functions.
It resolvesambiguities in the current interpretation; Estimates confidence inthe extracted information; Clarifies the interpretation with theuser if required; Integrates new input with the dialogue context;Builds database queries (SQL); Sends information to NLgeneration for presentation to user; and prompts the user formissing information.We have developed a flexible, event driven dialogue manager inwhich the current context of the system is used to decide what todo next.
The system does not use a dialogue network or adialogue script, rather a general engine operates on the semanticrepresentations and the current context to control the interactionflow.
The Dialogue Manager receives the extracted parse.
It thenintegrates the parse into the current context.
Context consists of aset of frames and a set of global variables.
As new extractedinformation arrives, it is put into the context frames andsometimes used to set global variables.
The system provides ageneral-purpose library of routines for manipulating frames.This ?event driven?
architecture functions similar to a productionsystem.
An incoming parse causes a set of actions to fire whichmodify the current context.
After the parse has been integratedinto the current context, the DM examines the context to decidewhat action to take next.
The DM attempts the following actionsin the order listed:?
Clarify if necessary?
Sign off if all done?
Retrieve data and present to user?
Prompt user for required informationThe rules for deciding what to prompt for next are verystraightforward.
The frame in focus is set to be the frameproduced in response to the user, or to the last system prompt.?
If there are unfilled required slots in the focus frame, thenprompt for the highest priority unfilled slot in the frame.?
If there are no unfilled required slots in the focus frame,then prompt for the highest priority missing piece ofinformation in the context.Our mechanism does not have separate ?user initiative?
and?system initiative?
modes.
If the system has enough informationto act on, then it does it.
If it needs information, then it asks forit.
The system does not require that the user respond to theprompt.
The user can respond with anything and the system willparse the utterance and set the focus to the resulting frame.
Thisallows the user to drive the dialog, but doesn?t require it.
Thesystem prompts are organized locally, at the frame level.
Thedialog manager or user puts a frame in focus, and the system triesto fill it.
This representation is easy to author, there is no separatedialog control specification required.
It is also robust in that ithas a simple control that has no state to lose track of.An additional benefit of Dialog Manager mechanism is that it isvery largely declarative.
Most of the work done by a developerwill be the creation of frames, forms and grammars.
The systemdeveloper creates a task file that specifies the system ontologyand templates for communicating about nodes in the hierarchy.The templates are filled in from the values in the frames togenerate output in the desired language.
This is the way wecurrently generate SQL queries and user prompts.
An exampletask frame specification is:Frame:Air[Depart_Loc]+Prompt: "where are you departing from"[City_Name]*Confirm: "You are departing from $([City_Name]).Is that correct?
"Sql: "dep_$[leg_num] in (select airport_code fromairport_codes where city like '!%' $(and state_provincelike '[Depart_Loc].
[State]' ) )"[Airport_Code]*This example defines a frame with name Air and slot[Depart_Loc].
The child nodes of Depart_Loc are are[City_Name] and [Airport_Code].
The ?+?
after [Depart_Loc]indicates that it is a mandatory field.
The Prompt string is thetemplate for prompting for the node information.
The ?*?
after[City_Name] and [Airport_Code] indicate that if either of them isfilled, the parent node [Depart_Loc] is filled.
The Confirm stringis a template to prompt the user to confirm the values.
The SQLstring is the template to use the value in an SQL query to thedatabase.The system will prompt for all mandatory nodes that haveprompts.
Users may specify information in any order, but thesystem will prompt for whatever information is missing until theframe is complete.1.7 Database & Internet InterfaceThe back-end interface consists of an SQL database and domain-specific Perl scripts for accessing information from the Internet.During operation, database requests are transmitted by the DialogManager to the database server via a formatted frame.The back-end consists of a static and dynamic informationcomponent.
Static tables contain data such as conversionsbetween 3-letter airport codes and the city, state, and country ofthe airport (e.g., BOS for Boston Massachusetts).
There are over8000 airports in our database, 200 hotel chains, and 50 car rentalcompanies.
The dynamic information content consists ofdatabase tables for car, hotel, and airline flights.When a database request is received, the Dialog Manager?s SQLcommand is used to select records in local memory.
If norecords are found to match, the back-end can submit an HTTP-based request for the information via the Internet.
Recordsreturned from the Internet are then inserted as rows into the localSQL database and the SQL statement is once again applied.1.8 Language GenerationThe language generation module uses templates to generate textbased on dialog speech acts.
Example dialog acts include?prompt?
for prompting the user for needed information,?summarize?
for summarization of flights, hotels, and rental cars,and ?clarify?
for clarifying information such as departure andarrival cities that share the same name.1.9 Text-to-Speech SynthesisFor audio output, we have developed a domain-dependentconcatenative speech synthesizer.
Our concatenative synthesizercan adjoin units ranging from phonemes, to words, to phrasesand sentences.
For domain modeling, we use a voice talent torecord entire task-dependent utterances  (e.g., ?What are yourtravel plans??)
as well as short phrases with carefully determinedbreak points (e.g., ?United flight?, ?ten?, ?thirty two?, ?departsAnchorage at?).
Each utterance is orthographically transcribedand phonetically aligned using a HMM-based recognizer.
Ourresearch efforts for data collection are currently focused onmethods for reducing the audible distortion at segmentboundaries, optimization schemes for prompt generation, as wellas tools for rapidly correcting boundary misalignments.
Ingeneral, we find that some degree of hand-correction is alwaysrequired in order to reduce distortions at concatenation points.During synthesis, the text is automatically divided into individualsentences that are then synthesized and pipelined to the audioserver.
A text-to-phoneme conversion is applied using aphonetic dictionary.
Words that do not appear in the phoneticdictionary are automatically pronounced using a multi-layerperceptron based pronunciation module.
Here, a 5-letter contextis extracted from the word to be pronounced.
The letter input isfed through the MLP and a phonetic symbol (or possibly epsilon)is output by the network.
By sliding the context window, we canextract the phonetic pronunciation of the word.
The MLP istrained using letter-context and symbol output pairs from a largephonetic dictionary.The selection of units to concatenate is determined using a hybridsearch algorithm that operates at the word or phoneme level.During synthesis, sections of word-level text that have beenrecorded are automatically concatenated.
Unrecorded words orword sequences are synthesized using a Viterbi beam searchacross all available phonetic units.
The cost function includesinformation regarding phonetic context, pitch, duration, andsignal amplitude.
Audio segments making up the best-path arethen concatenated to generate the final sentence waveform.2.
DATA COLLECTION & EVALUATION2.1 Data Collection EffortsLocal Collection EffortThe Center for Spoken Language Research maintains a dialupCommunicator system for data collection1.
Users wishing to usethe dialogue system can register at our web site [1] and receive aPIN code and system telephone number.
To date, our system hasfielded over 1750 calls totaling over 25,000 utterances fromnearly 400 registered users.NIST Multi-Site Data CollectionDuring the months of June and July of 2000, The NationalInstitute of Standards (NIST) conducted a multi-site datacollection effort for the nine DARPA Communicatorparticipants.
Participating sites included: AT&T, IBM, BBN,SRI, CMU, Colorado, MIT, Lucent, and MITRE.
In this datacollection, a pool of potential users was selected from variousparts of the United States by a market research firm.
Theselected subjects were native speakers of American English whowere possible frequent travelers.
Users were asked to performnine tasks.
The first seven tasks consisted of fixed scenarios forone-way and round-trip flights both within and outside of theUnited States.
The final two tasks consisted of users makingopen-ended business or vacation.2.2 System EvaluationTask CompletionA total of 72 calls from NIST participants were received by theCU Communicator system.
Of these, 44 callers were female and28 were male.
Each scenario was inspected by hand andcompared against the scenario provided by NIST to the subject.For the two open-ended tasks, judgment was made based on whatthe user asked for with that of the data provided to the user.
Intotal, 53/72 (73.6%) of the tasks were completed successfully.A detailed error analysis can be found in [11].Word Error Rate AnalysisA total of 1327 utterances were recorded from the 72 NIST calls.Of these, 1264 contained user speech.
At the time of the June2000 NIST evaluation, the CU Communicator system did notimplement voice-based barge-in.
We noticed that one source oferror was due to users who spoke before the recording processwas started.
Even though a tone was presented to the user tosignify the time to speak, 6.9% of the utterances containedinstances in which the user spoke before the tone.
Since all userswere exposed to several other Communicator systems that2The system can be accessed toll-free at 1-866-735-5189employed voice barge-in, there may be some effect fromexposure to those systems.
Table 3 summarizes the word errorrates for the system utilizing the June 2000 NIST data as the testset.
Overall, the system had a word error rate (WER) of 26.0%when parallel gender-dependent decoders were utilized.
SinceJune of 2000, we have collected an additional 15,000 task-dependent utterances.
With the extra data, we were able toremove our dependence on the CMU Communicator trainingdata [12].
When the language model was reestimated andlanguage model weights reoptimized using only CUCommunicator data, the WER dropped from 26.0% to 22.5%.This amounts to a 13.5% relative reduction in WER.Table 1: CU Communicator Word Error Rates for (A)Speaker Independent acoustic models and June 2000language model, (B) Gender-dependent parallel recognizerswith June 2000 Language Model, and (C) Language Modelretrained in December 2000.June 2000 NIST Evaluation Data, 1264utterances, 72 speakersWord ErrorRate(A) Speaker Indep.
HMMs (LM#1) 29.8%(B) Gender Dependent HMMs (LM#1) 26.0%(C) Gender Dependent HMMs (LM#2)  22.5%Core MetricsSites in the DARPA Communicator program agreed to log acommon set of metrics for their systems.
The proposed set ofmetrics was: Task Completion, Time to Completion, Turns toCompletion, User Words/Turn, System Words/Turn, UserConcepts/Turn, Concept Efficiency, State of Itinerary, ErrorMessages, Help Messages, Response Latency, User Words toCompletion, System Words to Completion, User Repeats, SystemRepeats/Reprompts, Word Error, Mean Length of SystemUtterance, and Mean Length of System Turn.Table 2: Dialogue system evaluation metricsItem Min Mean MaxTime to Completion (secs) 120.9 260.3 537.2Total Turns to Completion 23 37.6 61Response Latency (secs) 1.5 1.9 2.4User Words to Task End 19 39.4 105System Words to End 173 331.9 914Number of Reprompts 0 2.4 15Table 2 summarizes results obtained from metrics derivedautomatically from the logged timing markers for the calls inwhich the user completed the task assigned to them.
The averagetime to task completion is 260.
During this period there are anaverage of 19 user turns and 19 computer turns (37.6 averagetotal turns).
The average response latency was 1.86 seconds.The response latency also includes the time required to access thedata live from the Internet travel information provider.3.
CU MOVE3.1 Task OverviewThe ?CU Move?
system represents our work towards achievinggraceful human-computer interaction in automobileenvironments.
Initially, we have considered the task of vehicleroute planning and navigation.
As our work progresses, we willexpand our dialog system to new tasks such as informationretrieval and summarization and multimedia access.The problem of voice dialog within vehicle environments offerssome important speech research challenges.
Speech recognitionin car environments is in general fragile, with word-error-rates(WER) ranging from 30-65% depending on driving conditions.These changing environmental conditions include speakerchanges (task stress, emotion, Lombard effect, etc.)
as well as theacoustic environment (road/wind noise from windows, airconditioning, engine noise, exterior traffic, etc.
).In developing the CU-Move system [13,14], there are a numberof research challenges that must be overcome to achieve reliableand natural voice interaction within the car environment.
Sincethe speaker is performing a task (driving the vehicle), the driverwill experience a measured level of user task stress and thereforethis should be included in the speaker-modeling phase.
Previousstudies have clearly shown that the effects of speaker stress andLombard effect can cause speech recognition systems to failrapidly.
In addition, microphone type and placement for in-vehicle speech collection can impact the level of acousticbackground noise and speech recognition performance.3.2 Signal ProcessingOur research for robust recognition in automobile environmentsis concentrated on development of an intelligent microphonearray.
Here, we employ a Gaussian Mixture Model (GMM)based environmental classification scheme to characterize thenoise conditions in the automobile.
By integrating anenvironmental classification system into the microphone arraydesign, decisions can be made as to how best to utilize a noise-adaptive frequency-partitioned iterative enhancement algorithm[15,16] or model-based adaptation algorithms [17,18] duringdecoding to optimize speech recognition accuracy on the beam-formed signal.3.3 Data CollectionA five-channel microphone array was constructed using Knowlesmicrophones and a multi-channel data recorder housing built(Fostex) for in-vehicle data collection.
An additional referencemicrophone is situated behind the driver?s seat.
Fig.
3 shows theconstructed microphone array and data recorder housing.Figure 3: Microphone array and reference microphone (left),Fostex multi-channel data recorder (right).As part of the CU-Move system formulation, a two phase datacollection plan has been initiated.
Phase I focuses on collectingacoustic noise and probe speech from a variety of cars anddriving conditions.
Phase II focuses on a extensive speakercollection across multiple U.S. sites.
A total of eight vehicleshave been selected for acoustic noise analysis.
These include thefollowing: a compact car, minivan, cargo van, sport utilityvehicle (SUV), compact and full size trucks, sports car, full sizeluxury car.
A fixed 10 mile route through Boulder, CO was usedfor Phase I data collection.
The route consisted of city (25 &45mph) and highway driving (45 & 65mph).
The route includedstop-and-go traffic, and prescribed locations wheredriver/passenger windows, turn signals, wiper blades, airconditioning were operated.
Each data collection run per carlasted approximately 35-45 minutes.
A detailed acoustic analysisof Phase I data can be found in [13].
Our plan is to begin PhaseII speech/dialogue data collection during spring 2001, which willinclude (i) phonetically balanced utterances, (ii) task-specificvocabularies, (iii) natural extemporaneous speech, and (iv)human-to-human and Wizard-of-Oz (WOZ) interaction with CU-Communicator and CU-Move dialog systems.3.4 Prototype Dialog SystemFinally, we have developed a prototype dialog system for datacollection in the car environment.
The dialog system is based onthe MIT Galaxy-II Hub architecture with base systemcomponents derived from the CU Communicator system [1].Users interacting with the dialog system can enter their originand destination address by voice.
Currently, 1107 street namesfor Boulder, CO area are modeled.
The system can resolve streetaddresses by business name via interaction with an Internettelephone book.
This allows users to ask more natural routequeries (e.g., ?I need an auto repair shop?, or ?I need to get to theBoulder Marriott?).
The dialog system automatically retrievesthe driving instructions from the Internet using an online WWWroute direction provider.
Once downloaded, the drivingdirections are queried locally from an SQL database.
Duringinteraction, users mark their location on the route by providingspoken odometer readings.
Odometer readings are needed sinceGPS information has not yet been integrated into the prototypedialog system.
Given the odometer reading of the vehicle as anestimate of position, route information such as turn descriptions,distances, and summaries can be queried during travel (e.g.,"What's my next turn", "How far is it", etc.
).The prototype system uses the CMU Sphinx-II speech recognizerwith cellular telephone acoustic models along with the PhoenixParser [10] for semantic parsing.
The dialog manager is mixed-initiative and event driven.
For route guidance, the naturallanguage generator formats the driving instructions beforepresentation to the user by the text-to-speech server.
Forexample, the direction,  "Park Ave W. becomes 22nd St." isreformatted to, "Park Avenue West becomes Twenty SecondStreet".
Here, knowledge of the task-domain can be used tosignificantly improve the quality of the output text.
For speechsynthesis, we have developed a Hub-compliant server thatinterfaces to the AT&T NextGen speech synthesizer.3.5 Future WorkWe have developed a Hub compliant server that interfaces aGarmin GPS-III global positioning device to a mobile computervia a serial port link.
The GPS server reports vehicle velocity inthe X,Y,Z directions as well as real-time updates of  vehicleposition in latitude and longitude.
HRL Laboratories hasdeveloped a route server that interfaces to a major navigationcontent provider.
The HRL route server can take GPScoordinates as inputs and can describe route maneuvers in termsof GPS coordinates.
In the near-term, we will interface our GPSserver to the HRL route server in order to provide real-timeupdating of vehicle position.
This will eliminate the need forperiodic location update by the user and also will allow for moreinteresting dialogs to be established (e.g., the computer mightproactively tell the user about upcoming points of interest, etc.).4.
REFERENCES[1] http://communicator.colorado.edu[2] W. Ward, B. Pellom, "The CU Communicator System," IEEEWorkshop on Automatic Speech Recognition and Understanding,Keystone Colorado, December, 1999.
[3] http://fofoca.mitre.org[4] Seneff, S., Hurley, E., Lau, R., Pao, C., Schmid, P., Zue,  V.,?Galaxy-II: A Reference Architecture for Conversational SystemDevelopment,?
Proc.
ICSLP, Sydney Australia, Vol.
3, pp.
931-934, 1998.
[5] Ravishankar, M.K., ?Efficient Algorithms for SpeechRecognition?.
Unpublished Dissertation CMU-CS-96-138,Carnegie Mellon University, 1996[6] K. Hacioglu, W. Ward, "Dialog-Context Dependent LanguageModeling Using N-Grams and Stochastic Context-Free Grammars",Proc.
IEEE ICASSP, Salt Lake City, May 2001.
[7] K. Hacioglu, W. Ward, "Combining Language Models : OracleApproach", Proc.
Human Language Technology Conference, SanDiego, March 2001.
[8] R. San-Segundo, B. Pellom, W. Ward, J. M. Pardo, "ConfidenceMeasures for Dialogue Management in the CU CommunicatorSystem," Proc.
IEEE ICASSP, Istanbul Turkey, June 2000.
[9] R. San-Segundo, B. Pellom, K. Hacioglu, W. Ward, J.M.
Pardo,"Confidence Measures for Dialogue Systems," Proc.
IEEE ICASSP,Salt Lake City, May 2001.
[10] Ward, W., ?Extracting Information From Spontaneous Speech?,Proc.
ICSLP, September 1994.
[11] B. Pellom, W. Ward, S. Pradhan, "The CU Communicator: AnArchitecture for Dialogue Systems", Proc.
ICSLP, Beijing China,November 2000.
[12] Eskenazi,  M., Rudnicky, A., Gregory, K., Constantinides, P.,Brennan, R., Bennett, K., Allen, J., ?Data Collection andProcessing in the Carnegie Mellon Communicator,?
Proc.Eurospeech-99, Budapest, Hungary.
[13] J.H.L.
Hansen, J. Plucienkowski, S. Gallant, B.L.
Pellom, W. Ward,"CU-Move: Robust Speech Processing for In-Vehicle SpeechSystems," Proc.
ICSLP, vol.
1, pp.
524-527, Beijing, China, Oct.2000.
[14] http://cumove.colorado.edu/[15] J.H.L.
Hansen, M.A.
Clements, ?Constrained Iterative SpeechEnhancement with Application to Speech Recognition,?
IEEETrans.
Signal Proc., 39(4):795-805, 1991.
[16] B. Pellom, J.H.L.
Hansen, ?An Improved Constrained IterativeSpeech Enhancement Algorithm for Colored Noise Environments,"IEEE Trans.
Speech & Audio Proc., 6(6):573-79, 1998.
[17] R. Sarikaya, J.H.L.
Hansen, "Improved Jacobian Adaptation forFast Acoustic Model Adaptation in Noisy Speech Recognition,"Proc.
ICSLP, vol.
3, pp.
702-705, Beijing, China, Oct.
2000.
[18] R. Sarikaya, J.H.L.
Hansen, "PCA-PMC: A novel use of a prioriknowledge for fast model combination," Proc.
ICASSP, vol.
II, pp.1113-1116, Istanbul, Turkey, June 2000.
