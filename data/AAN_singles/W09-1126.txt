Proceedings of the Thirteenth Conference on Computational Natural Language Learning (CoNLL), pages 210?218,Boulder, Colorado, June 2009. c?2009 Association for Computational LinguisticsUsing Encyclopedic Knowledge for Automatic Topic IdentificationKino CourseyUniversity of North Texas and Daxtron Laboratories, Inc.kino@daxtron.comRada Mihalcea and William MoenUniversity of North Texasrada,wemoen@unt.eduAbstractThis paper presents a method for automatictopic identification using an encyclopedicgraph derived from Wikipedia.
The sys-tem is found to exceed the performance ofpreviously proposed machine learning algo-rithms for topic identification, with an annota-tion consistency comparable to human anno-tations.1 IntroductionWith exponentially increasing amounts of text be-ing generated, it is important to find methods thatcan annotate and organize documents in meaning-ful ways.
In addition to the content of the documentitself, other relevant information about a documentsuch as related topics can often enable a faster andmore effective search or classification.
Documenttopics have been used for a long time by librarians toimprove the retrieval of a document, and to providebackground or associated information for browsingby human users.
They can also assist search, back-ground information gathering and contextualizationtasks, and enhanced relevancy measures.The goal of the work described in this paper is toautomatically find topics that are relevant to an inputdocument.
We refer to this task as ?topic identifica-tion?
(Medelyan and Witten, 2008).
For instance,starting with a document on ?United States in theCold War,?
we want to identify relevant topics suchas ?history,?
?Global Conflicts,?
?Soviet Union,?
andso forth.
We propose an unsupervised method fortopic identification, based on a biased graph cen-trality algorithm applied to a large knowledge graphbuilt from Wikipedia.The task of topic identification goes beyond key-word extraction (Mihalcea and Csomai, 2007), sincerelevant topics may not be necessarily mentioned inthe document, and instead have to be obtained fromsome repositories of external knowledge.
The taskis also different from text classification (Gabrilovichand Markovitch, 2006), since the topics are eithernot known in advance or are provided in the form ofa controlled vocabulary with thousands of entries,and thus no classification can be performed.
In-stead, with topic identification, we aim to find topics(or categories1) that are relevant to the document athand, which can be used to enrich the content of thedocument with relevant external knowledge.2 Dynamic Ranking of Topic RelevanceOur method is based on the premise that externalencyclopedic knowledge can be used to identify rel-evant topics for a given document.The method consists of two main steps.
In the firststep, we build a knowledge graph of encyclopedicconcepts based on Wikipedia, where the nodes in thegraph are represented by the entities and categoriesthat are defined in this encyclopedia.
The edges be-tween the nodes are represented by their relation ofproximity inside the Wikipedia articles.
The graphis built once and then it is stored offline, so that itcan be efficiently use for the identification of topicsin new documents.In the second step, for each input document, wefirst identify the important encyclopedic concepts inthe text, and thus create links between the content ofthe document and the external encyclopedic graph.Next, we run a biased graph centrality algorithm onthe entire graph, so that all the nodes in the exter-nal knowledge repository are ranked based on theirrelevance to the input document.
We use a variation1Throughout the paper, we use the terms ?topic?
and ?cate-gory?
interchangeably.210of the PageRank (Brin and Page, 1998) algorithm,which accounts for both the relation between thenodes in the document and the encyclopedic graph,as well as the relation between the nodes in the en-cyclopedic graph itself.In the following, we first describe the structureof Wikipedia, followed by a brief description of theWikify!
system that automatically identifies the en-cyclopedic concepts in a text, and finally a descrip-tion of the dynamic ranking process on the encyclo-pedic graph.2.1 WikipediaWikipedia (http://wikipedia.org) is a free online en-cyclopedia, representing the outcome of a continu-ous collaborative effort of a large number of vol-unteer contributors.
Virtually any Internet user cancreate or edit a Wikipedia webpage, and this ?free-dom of contribution?
has a positive impact on boththe quantity (fast-growing number of articles) andthe quality (potential mistakes are quickly correctedwithin the collaborative environment) of this re-source.Wikipedia has grown to become one of the largestonline repositories of encyclopedic knowledge, withmillions of articles available for a large number oflanguages.
In fact, Wikipedia editions are availablefor more than 250 languages, with a number of en-tries varying from a few pages to close to three mil-lion articles per language.The basic entry in Wikipedia is an article (orpage), which defines an entity or an event, and con-sists of a hypertext document with hyperlinks toother pages within or outside Wikipedia.
The roleof the hyperlinks is to guide the reader to pagesthat provide additional information about the enti-ties or events mentioned in an article.
Each articlein Wikipedia is uniquely referenced by an identifier,which consists of one or more words separated byspaces or underscores, and occasionally a parenthet-ical explanation.
The current version of the EnglishWikipedia consists of about 2.75 million articles.In addition to articles, Wikipedia also includes alarge number of categories, which represent topicsthat are relevant to a given article (the July 2008 ver-sion of Wikipedia includes about 390,000 such cate-gories).
The category links are organized hierarchi-cally, and vary from broad topics such as ?history?or ?games?
to highly focused topics such as ?mili-tary history of South Africa during World War II?
orFigure 1: A snapshot from the encyclopedic graph.
?role-playing game publishing companies.
?We use the entire English Wikipedia to build anencyclopedic graph for use in the topic identifica-tion process.
The nodes in the graph are representedby all the article and category pages in Wikipedia,and the edges between the nodes are represented bytheir relation of proximity inside the articles.
Thegraph contains 5.8 million nodes, and 65.5 millionedges.
Figure 1 shows a small section of the knowl-edge graph, as built starting with the article on ?Cor-pus Linguistics?.2.2 Wikify!In order to automatically identify the important en-cyclopedic concepts in an input text, we use the un-supervised system Wikify!
(Mihalcea and Csomai,2007), which identifies the concepts in the text thatare likely to be highly relevant (i.e., ?keywords?
)for the input document, and links them to Wikipediaconcepts.Wikify!
works in three steps, namely: (1) candi-date extraction, (2) keyword ranking, and (3) wordsense disambiguation.
The candidate extraction stepparses the input document and extracts all the pos-sible n-grams that are also present in the vocabularyused in the encyclopedic graph (i.e., anchor texts forlinks inside Wikipedia or article or category titles).Next, the ranking step assigns a numeric value toeach candidate, reflecting the likelihood that a givencandidate is a valuable keyword.
Wikify!
uses a?keyphraseness?
measure to estimate the probabil-ity of a term W to be selected as a keyword in a211document, by counting the number of documentswhere the term was already selected as a keywordcount(Dkey) divided by the total number of docu-ments where the term appeared count(DW ).
Thesecounts are collected from all the Wikipedia articles.P (keyword|W ) ?
count(Dkey)count(DW )(1)This probability can be interpreted as ?the moreoften a term was selected as a keyword among itstotal number of occurrences, the more likely it is thatit will be selected again.
?Finally, a simple word sense disambiguationmethod is applied, which identifies the most likelyarticle in Wikipedia to which a concept shouldbe linked to.
This step is trivial for words orphrases that have only one corresponding article inWikipedia, but it requires an explicit disambiguationstep for those words or phrases that have multiplemeanings (e.g., ?plant?)
and thus multiple candidatepages to link to.
The algorithm is based on statisticalmethods that identify the frequency of meanings intext, combined with symbolic methods that attemptto maximize the overlap between the current docu-ment and the candidate Wikipedia articles.
See (Mi-halcea and Csomai, 2007) for more details.2.3 Biased Ranking of the Wikipedia GraphStarting with the graph of encyclopedic knowledge,and knowing the nodes that belong to the input doc-ument, we want to rank all the nodes in the graphso that we obtain a score that indicates their impor-tance relative to the given document.
We can do thisby using a graph-ranking algorithm biased towardthe nodes belonging to the input document.Graph-based ranking algorithms such as PageR-ank are a way of deciding the importance of a vertexwithin a graph, based on global information recur-sively drawn from the entire graph.
One formula-tion is in terms of a random walk through a directedgraph.
A ?random surfer?
visits nodes of the graph,and has some probability of jumping to some otherrandom node of the graph, and the remaining proba-bility of continuing their walk from the current nodeto one in its outdegree list.
The rank of a node is anindication of the probability that the surfer would befound at that node at any given time.Formally, let G = (V,E) be a directed graph withthe set of vertices V and set of edges E, where E isa subset of V ?
V .
For a given vertex Vi, let In(Vi)be the set of vertices that point to it (predecessors),and let Out(Vi) be the set of vertices that vertex Vipoints to (successors).
The score of a vertex Vi isdefined as follows (Brin and Page, 1998):S(Vi) = (1?
d) + d ?
?j?In(Vi)1|Out(Vj)|S(Vj) (2)where d is a damping factor usually set to 0.85.Given the ?random surfer?
interpretation of theranking process, the (1 ?
d) portion represents theprobability that a surfer will jump to a given nodefrom any other node at random, and the summationportion indicates that the process will enter the nodevia edges directly connected to it.We introduce a bias in this graph-based rank-ing algorithm by extending the framework of per-sonalization of PageRank proposed by (Haveliwala,2002).
We modify the formula so that the (1 ?
d)component also accounts for the importance of theconcepts found in the input document, and it is sup-pressed for all the nodes that are not found in theinput document.S(Vi) = (1?d)?Bias(Vi)+d?
?j?In(Vi)1|Out(Vj)|S(Vj)(3)where Bias(Vi) is only defined for those nodes ini-tially identified in the input document:Bias(Vi) = f(Vi)?j?InitalNodeSetf(Vj)and 0 for all other nodes in the graph.InitalNodeSet is the set of nodes belongingto the input document.Note that f(Vi) can vary in complexity from a de-fault value of 1 to a complex knowledge-based es-timation.
In our implementation, we use a combi-nation of the ?keyphraseness?
score assigned to thenode Vi and its distance from the ?Fundamental?category in Wikipedia.The use of the Bias assigned to each node meansthe surfer random jumps will be limited to only thosenodes connected to the original query.
Thus thegraph-ranking process becomes biased and focusedon those topics directly related to the input.
It alsoaccumulates activation at those nodes not directlyfound in the input text, but linked through indirectmeans, thus reinforcing the nodes where patterns ofactivation intersect and creating a constructive in-terference pattern in the network.
These reinforcednodes are the ?implied related topics?
of the text.2123 IllustrationTo illustrate the ranking process, consider as an ex-ample the following sentence ?The United Stateswas involved in the Cold War.
?First the text is passed through the Wikify!
sys-tem, which returns the articles ?United States?
and?Cold War.?
Taking into account their ?keyphrase-ness?
as calculated by Wikify!, the selections aregiven an initial bias of 0.5492 (?United States?)
and0.4508 (?Cold War?
).After the first iteration the initial activationspreads out into the encyclopedic graph, the nodesfind a direct connection to one another, and cor-respondingly their scores are changed to 0.3786(?United States?)
and 0.3107 (?Cold War?).
Afterthe second iteration, new nodes are identified fromthe encyclopedic graph, a subset of which is shownin Figure2.
The process will eventually continue forseveral iterations until the scores of the nodes do notchange.
The nodes with the highest scores in thefinal graph are considered to be the most closely re-lated to the input sentence, and thus selected as rel-evant topics.Figure 2: Sub-graph between ?United States?
and ?ColdWar?In order to see the effect of the initial bias, con-sider as an example the ranking of the nodes inthe encyclopedic graph when biased with the sen-tence ?The United States was involved in the ColdWar,?
versus the sentence ?Microsoft applies Com-puter Science.?
A comparison between the scores ofthe nodes when activated by each of these sentencesis shown in Table 1.Wikipedia entry US/CW MS/CS Diff.A: United States 0.393636 0.006578 0.387058C: Computer Science 0.000004 0.003576 -0.003571A: World War II 0.007102 0.003674 0.003428A: United Kingdom 0.005346 0.002670 0.002676C: Microsoft 0.000001 0.001839 -0.001837C: Cold War 0.001695 0.000006 0.001689C: Living People 0.000835 0.002223 -0.001387C: Mathematics 0.000029 0.001337 -0.001307C: Computing 0.000008 0.001289 -0.001280C: Computer Pioneers 0.000002 0.001238 -0.001235Table 1: Node ranking differences when the encyclo-pedic graph is biased with different inputs: (1) ?UnitedStates?
and ?Cold War?
(US/CW) vs. (2) ?Microsoft?and ?Computer Science?
(MS/CS).
The nodes are eitherarticle pages (A) or category pages (C).4 ExperimentsIn order to measure the effectiveness of the topicranking process, we run three sets of experiments,aimed at measuring the relevancy of the automati-cally identified topics with respect to manually an-notated gold standard data sets.In the first experiment, the identification of theimportant concepts in the input text (used to bias thetopic ranking process) is performed manually, by theWikipedia users.
In the second and third experiment,the identification of these important concepts is doneautomatically, by the Wikify!
system.
In all the ex-periments, the ranking of the concepts from the en-cyclopedic graph is done automatically by using thedynamic ranking process described in Section 2.In the first two experiments, we use a data setconsisting of 150 articles from Wikipedia, whichhave been explicitly removed from the encyclope-dic graph.
All the articles in this data set includemanual annotations of the relevant categories, as as-signed by the Wikipedia users, against which wecan measure the quality of the automatic topic as-signments.
The 150 articles have been randomly se-lected while following the constraint that they eachcontain at least three article links and at least threecategory links.
Our task is to rediscover the relevantcategories for each page.
Note that the task is non-trivial, since there are approximately 390,000 cate-gories to choose from.
We evaluate the quality ofour system through the standard measures of preci-213sion and recall.4.1 Manual Annotation of the Input TextIn this first experiment, the articles in the gold stan-dard data set alo include manual annotations of theimportant concepts in the text, i.e., the links to otherWikipedia articles as created by the Wikipedia users.Thus, in this experiment we only measure the accu-racy of the dynamic topic ranking process, withoutinterference from the Wikify!
system.There are two main parameters that can be set dur-ing a system run.
First, the set of initial nodes usedas bias in the ranking can include: (1) the initial setof articles linked to by the original document (viathe Wikipedia links); (2) the categories listed in thearticles linked to by the original document2; and (3)both.
Second, the dynamic ranking process can berun through propagation on an encyclopedic graphthat includes (1) all the articles from Wikipedia; (2)all the categories from Wikipedia; or (3) all the arti-cles and the categories from Wikipedia.Figures 3, 4 and 5 show the precision, recall andF-measure obtained for the various settings.
In theplots, Bias and Propagate indicate the selectionsmade for the two parameters, which can be each setto Articles, Categories, or Both.
Each of thesecorrespond to the options listed before.00.020.040.060.080.10.120.140.160.180  20  40  60  80  100PrecisionTop N topics returnedBiasArticles- PropCategoriesBiasCategories- PropArticles PropCategoriesBiasArticles BiasCategories- PropCategoriesBiasArticles- PropArticles PropCategoriesBiasArticles BiasCategories- PropArticlesBiasCategories- PropArticlesBiasArticles- PropArticlesBiasArticles BiasCategories- PropArticles PropCategoriesBiasCategories- PropCategoriesBaselineFigure 3: Precision for manual input text annotations.As seen in the figures, the best results are obtainedfor a setting where both the initial bias and the prop-agation include all the available nodes, i.e., both ar-ticles and categories.
Although the primary task is2These should not be confused with the categories includedin the document itself, which represent the gold standard anno-tations and are not used at any point.00.050.10.150.20.250.30.350.40.450.50  20  40  60  80  100RecallTop N topics returnedBiasArticles- PropCategoriesBiasCategories- PropArticles PropCategoriesBiasArticles BiasCategories- PropCategoriesBiasArticles- PropArticles PropCategoriesBiasArticles BiasCategories- PropArticlesBiasCategories- PropArticlesBiasArticles- PropArticlesBiasArticles BiasCategories- PropArticles PropCategoriesBiasCategories- PropCategoriesBaselineFigure 4: Recall for manual input text annotations.00.020.040.060.080.10.120.140.160.180  20  40  60  80  100F-MeasureTop N topics returnedBiasArticles- PropCategoriesBiasCategories- PropArticles PropCategoriesBiasArticles BiasCategories- PropCategoriesBiasArticles- PropArticles PropCategoriesBiasArticles BiasCategories- PropArticlesBiasCategories- PropArticlesBiasArticles- PropArticlesBiasArticles BiasCategories- PropArticles PropCategoriesBiasCategories- PropCategoriesBaselineFigure 5: F-measure when using Wikipedia article anno-tations.the identification of the categories, the addition ofthe article links improves the system performance.To place results in perspective, we also calculate abaseline (labeled as ?Baseline?
in the plots), whichselects by default all the categories listed in the arti-cles linked to by the original document.
Each base-line article assigns 1/N to each of its N possiblecategories, with categories pointed to by multiple ar-ticles receiving the summation.4.2 Automatic Annotation of the Input TextThe second experiment is similar to the first one, ex-cept that rather than using the manual annotationsof the important concepts in the input document,we use instead the Wikify!
system that automat-ically identifies these important concepts by usingthe method briefly described in Section 2.2.
The ar-ticle links identified by Wikify!
are treated in thesame way as the human anchor annotations from theprevious experiment.
In this experiment, we have214an additional parameter, which consists of the per-centage of links selected by Wikify!
out of the totalnumber of words in the document.
We refer to thisparameter as keyRatio.
The higher the keyRatio, themore terms are added, but also the higher the poten-tial of noise due to mis-disambiguation.Figures 6, 7 and 8 show the effect of varying thevalue of the keyRatio parameter used by Wikify!
hason the precision, recall and F-measure of the system.Note that in this experiment, we only use the bestsetting for the other two parameters as identified inthe previous experiment, namely an initial bias anda propagation step that include all available nodes,i.e., both articles and categories.00.050.10.150.20.250  20  40  60  80  100PrecisionTop N topics returnedkeyRatio= 0.02keyRatio= 0.04keyRatio= 0.08keyRatio= 0.16Baseline keyRatio= 0.04Figure 6: Precision for automatic input text annotations(Wikipedia data set)00.050.10.150.20.250.30.350  20  40  60  80  100RecallTop N topics returnedkeyRatio= 0.02keyRatio= 0.04keyRatio= 0.08keyRatio= 0.16Baseline keyRatio= 0.04Figure 7: Recall for automatic input text annotations(Wikipedia data set)The system?s best performance occurs for akeyRatio of 0.04 to 0.06, which coincides with theratio found optimal in previous experiments usingthe Wikify!
system (Mihalcea and Csomai, 2007).0.010.020.030.040.050.060.070.080.090.10.110  20  40  60  80  100F-MeasureTop N topics returnedkeyRatio= 0.02keyRatio= 0.04keyRatio= 0.08keyRatio= 0.16Baseline keyRatio= 0.04Figure 8: F-measure for automatic input text annotations(Wikipedia data set)As before, we also calculate a baseline, which se-lects by default all the categories listed in the articleslinked to by the original document, with the linksbeing automatically identified with the Wikify!
sys-tem.
The baseline is calculated for a keyRatio of0.04, which is one of the values that were found towork well for the ranking system itself and in previ-ous Wikify!
experiments.Overall, the system manages to find many relevanttopics for the documents in the evaluation data set,despite the large number of candidate topics (closeto 390,000).
Our system exceeds the baseline by alarge margin, demonstrating the usefulness of usingthe biased ranking on the encyclopedic graph.4.3 Article Selection for Computer ScienceTextsIn the third experiment, we use again the Wikify!system to annotate the input documents, but thistime we run the evaluations on a data set consist-ing of computer science documents.
We use the dataset introduced in previous work on topic identifica-tion (Medelyan and Witten, 2008), where 20 doc-uments in the field of computer science were inde-pendently annotated by 15 teams of two computerscience undergraduates.
The teams were asked toread the texts and assign to each of them the titleof the five Wikipedia articles they thought were themost relevant and the other groups would also se-lect.
Thus, the consistency of the annotations wasan important measure for this data set.
(Medelyanand Witten, 2008) define consistency as a measureof agreement:Consistency = 2CA+B215where A and B are the number of terms assignedby two indexing teams, and C is the number ofterms they have in common.
In the annotations ex-periments reported in (Medelyan and Witten, 2008),the human teams consistency ranged from 21.4% to37.1%, with 30.5% being the average.300.10.20.30.40.50.60.70.80.90  20  40  60  80  100PrecisionTop N topics returnedkeyRatio= 0.02keyRatio= 0.04keyRatio= 0.08keyRatio= 0.16Baseline keyRatio= 0.04Figure 9: Precision for automatic input text annotations(Waikato data set)00.050.10.150.20.250.30.350.40.450  20  40  60  80  100RecallTop N topics returnedkeyRatio= 0.02keyRatio= 0.04keyRatio= 0.08keyRatio= 0.16Baseline keyRatio= 0.04Figure 10: Recall for automatic input text annotations(Waikato data set)Figures 10, 9, 11 and 12 show the performanceof our system on this data set, by using the Wikify!annotations for the initial bias, and then propagat-ing to both articles and categories.
The plots alsoshow a baseline that selects all the articles automat-ically identified in the original document by usingthe Wikify!
system with a keyRatio set to 0.04.3The consistency for one team is measured as the average ofthe consistencies with the remaining 14 teams.00.050.10.150.20.250.30  20  40  60  80  100F-MeasureTop N topics returnedkeyRatio= 0.02keyRatio= 0.04keyRatio= 0.08keyRatio= 0.16Baseline keyRatio= 0.04Figure 11: F-measure for automatic input text annota-tions (Waikato data set)00.10.20.30.40.50.60.70  20  40  60  80  100ConsistencyTop N topics returnedkeyRatio= 0.02keyRatio= 0.04keyRatio= 0.08keyRatio= 0.16Baseline keyRatio= 0.04Figure 12: Consistency for automatic input text annota-tions (Waikato data set)When selecting the top five topics returned by oursystem (the same number of topics as provided bythe human teams), the average consistency with re-spect to the 15 human teams was measured at 34.5%,placing it between the 86% and 93% percentile ofthe human participants, with only two human teamsdoing better.
We can compare this result with theone reported in previous work for the same dataset.
Using a machine learning system, (Medelyanand Witten, 2008) reported a consistency of 30.5%.Thus, our result of 34.5% is significantly better, de-spite the fact that our method is unsupervised.In a second evaluation, we also considered theunion of all the terms assigned by the 15 teams.
Onaverage, each document was assigned 35.5 differ-ent terms by the human teams.
If allowed to pro-vide more annotations, our system peaks with a con-216sistency of 66.6% for the top 25 topics returned.The system has the ability to identify possible rele-vant alternative topics using the comprehensive cata-log of Wikipedia computer science articles and theirpossible associations.
A human team may not nec-essarily consider all of the possibilities or even beaware that some of the articles, possibly known andused by the other teams, exist.5 Related WorkThe work closest to ours is perhaps the one de-scribed in (Medelyan and Witten, 2008), where top-ics relevant to a given document are automaticallyselected by using a machine learning system.
Unlikeour unsupervised approach, (Medelyan and Witten,2008) learn what makes a good topic by training onpreviously annotated data.Also related is the Wikify!
system concernedwith the automatic annotation of documents withWikipedia links (Mihalcea and Csomai, 2007).However, Wikify!
is purely extractive, and thus itcannot identify important topics or articles that arenot explicitly mentioned in the input text.Explicit semantic analysis (Gabrilovich andMarkovitch, 2006) was also introduced as a way todetermine the relevancy of the Wikipedia articleswith respect to a given input text.
The resultingvector however is extremely large, and while it wasfound useful for the task of text classification with arelatively small number of categories, it would bedifficult to adapt for topic identification when thenumber of possible topics grows beyond the approx-imately 390,000 under consideration.
In a similarline of work, (Bodo et al, 2007) examined the useof Wikipedia and latent semantic analysis for thepurposes of text categorization, but reported nega-tive results when used for the categorization of theReuters-21578 dataset.Others are exploring the use of graph propagationfor deriving semantic information.
(Hughes and Ra-mage, 2007) described the use of a biased PageRankover the WordNet graph to compute word pair se-mantic relatedness using the divergence of the prob-ability values over the graph created by each word.
(Ollivier and Senellart, 2007) describes a method todetermine related Wikipedia article using a Markovchain derived value called the green measure.
Dif-ferences exist between the PageRank based meth-ods used as a baseline in their work and the methodproposed here, since our system can use the contentof the article, multiple starting points, and tightercontrol of the random jump probability via the biasvalue.
Finally, (Syed et al, 2008) reported positiveresults by using various methods for topic predictionincluding the use of text similarity and spreading ac-tivation.
The method was tested by using randomlyselected Wikipedia articles, where in addition to thecategories listed on a Wikipedia page, nearby sub-suming categories were also included as acceptable.6 Conclusions and Future WorkIn this paper, we introduced a system for automatictopic identification, which relies on a biased graphcentrality algorithm applied on a richly intercon-nected encyclopedic graph built from Wikipedia.Experiments showed that the integration of ency-clopedic knowledge consistently adds useful infor-mation when compared to baselines that rely exclu-sively on the text at hand.
In particular, when testedon a data set consisting of documents manually an-notated with categories by Wikipedia users, the top-ics identified by our system were found useful ascompared to the manual annotations.
Moreover, ina second evaluation on a computer science data set,the system exceeded the performance of previouslyproposed machine learning algorithms, which is re-markable given the fact that our system is unsuper-vised.
In terms of consistency with manual anno-tations, our system?s performance was found to becomparable to human annotations, with only two outof 15 teams scoring better than our system.The system provides a means to generate a dy-namic ranking of topics in Wikipedia within aframework that has the potential to utilize knowl-edge or heuristics through additional resources (likeontologies) converted to graph form.
This capabil-ity is not present in resources like search enginesthat provide access to a static ranking of Wikipedia.Future work will examine the integration of addi-tional knowledge sources and the application of themethod for metadata document annotations.AcknowledgmentsThis work has been partially supported by an award#CR72105 from the Texas Higher Education Coor-dinating Board and by an award from Google Inc.The authors are grateful to the Waikato group formaking their data set available.217ReferencesZ.
Bodo, Z. Minier, and L. Csato.
2007.
Text categoriza-tion experiments using Wikipedia.
In Proceedings ofthe International Conference on Knowledge Engineer-ing ,Principles and Techniques, Cluj-Napoca (Roma-nia).S.
Brin and L. Page.
1998.
The anatomy of a large-scalehypertextual Web search engine.
Computer Networksand ISDN Systems, 30(1?7).E.
Gabrilovich and S. Markovitch.
2006.
Overcomingthe brittleness bottleneck using Wikipedia: Enhancingtext categorization with encyclopedic knowledge.
InProceedings of the National Conference on ArtificialIntelligence (AAAI), Boston.T.
Haveliwala.
2002.
Topic-sensitive PageRank.
InProceedings of the Eleventh International World WideWeb Conference, May.T.
Hughes and D. Ramage.
2007.
Lexical semanticknowledge with random graph walks.
In Proceedingsof the Conference on Empirical Methods in NaturalLanguage Processing, Prague, Czech Republic.O.
Medelyan and I. H. Witten.
2008.
Topic indexingwith Wikipedia.
In Proceedings of the AAAI WikiAIworkshop.R.
Mihalcea and A. Csomai.
2007.
Wikify!
: linking doc-uments to encyclopedic knowledge.
In Proceedingsof the Sixteenth ACM Conference on Information andKnowledge Management, Lisbon, Portugal.Y.
Ollivier and P. Senellart.
2007.
Finding related pagesusing green measures: An illustration with wikipedia.In Association for the Advancement of Artificial In-telligence Conference on Artificial Intelligence (AAAI2007).Z.
Syed, T. Finin, and A. Joshi.
2008.
Wikipedia as anOntology for Describing Documents.
In Proceedingsof the Second International Conference on Weblogsand Social Media.
AAAI Press, March.218
