Coling 2010: Poster Volume, pages 223?231,Beijing, August 2010AbstractGlobal ranking, a new information re-trieval (IR) technology, uses a rankingmodel for cases in which there exist re-lationships between the objects to beranked.
In the ranking task, the rankingmodel is defined as a function of theproperties of the objects as well as therelations between the objects.
Existingglobal ranking approaches address theproblem by ?learning to rank?.
In thispaper, we propose a global rankingframework that solves the problem viadata fusion.
The idea is to take each re-trieved document as a pseudo-IR sys-tem.
Each document generates a pseu-do-ranked list by a global function.
Thedata fusion algorithm is then adapted togenerate the final ranked list.
Taking abiomedical information extraction task,namely, interactor normalization task(INT), as an example, we explain howthe problem can be formulated as aglobal ranking problem, and demon-strate how the proposed fusion-basedframework outperforms baseline me-thods.
By using the proposed frame-work, we improve the performance ofthe top 1 INT system by 3.2% usingthe official evaluation metric of theBioCreAtIvE challenge.
In addition, byemploying the standard ranking qualitymeasure, NDCG, we demonstrate that* Corresponding authorthe proposed framework can be cas-caded with different local rankingmodels and improve their ranking re-sults.1 IntroductionInformation Retrieval (IR) involves findingdocuments that are relevant to a given query ina large corpus.
The task is usually formulatedas a ranking problem.
When a user submits aquery, the IR system retrieves all documentsthat contain at least one query term, calculatesa ranking score for each of the documents us-ing a ranking model, and sorts the documentsaccording to the ranking scores.
The scoresrepresent the relevance, importance, and/ordiversity of the retrieved documents.
Thus, thequality of a search engine can be determinedby the accuracy of the ranking results.Recently, a machine learning technologycalled learning to rank has been applied exten-sively to the task.
Several state-of-the-art ma-chine learning-based ranking algorithms havebeen proposed, e.g., RankSVM and RankNet.These algorithms differ substantially in termsof the ranking models and optimization tech-niques employed, but most of them can be re-garded as ?local ranking?
approaches in thesense that each model is defined on a singledocument without considering the possiblerelations to other documents to be ranked.
Inmany applications, this is only a loose approx-imation as there is always relational informa-tion among documents.
For example, in somecases, users may prefer that two similar docu-ments have similar relevance scores; evenGlobal Ranking via Data FusionHong-Jie Dai1,2 Po-Ting Lai3 Richard Tzong-Han Tsai3* Wen-Lian Hsu1,2*1Department of Computer Science, National Tsing-Hua University,2Institute of Information Science, Academia Sinica,3Department of Computer Science & Engineering, Yuan Ze Universityhongjie@iis.sinica.edu.tws951416@mail.yzu.edu.twthtsai@saturn.yzu.edu.twhsu@iis.sinica.edu.tw223though one of the documents is not as relevantto the given query as the other; this problem issimilar to Pseudo Relevance Feedback (Kwok,1984).
In other cases, web pages from thesame site form a sitemap hierarchy in which aparent document should be ranked higher thanits child documents (referred to as Topic Dis-tillation at TREC (Chowdhury, 2007)).
To util-ize all available information, more advancedranking algorithms define a ranking model as afunction of all the documents to be ranked, i.e.,a global ranking model (Qin et al, 2008a; Qinet al, 2008b).Unlike conventional ranking and learning torank models, such as BM25 and RankSVM,whose ranking functions are defined on aquery and document pair, global ranking mod-els utilize both content information and rela-tion information.
Qin et al (2008) proposedthe first supervised learning framework for theglobal ranking problem.
They formulated theproblem as an optimization problem that in-volves finding an objective function to minim-ize the trade-off between local consistence andglobal consistence and implemented it onSVM.
Subsequently, they defined the globalranking problem formally in (Qin et al, 2008)and solved it by employing continuous condi-tional random fields (CRF).In this paper, we propose a new frameworkfor the global ranking problem.
The major dif-ference between our work and that of Qin et al(2008a; 2008b) is that we do not compile afeature vector of relational information directlyto construct a new machine-learned rankingmodel for global ranking.
Instead, we use theranking results generated by the original rank-ing model and then employ an algorithm withthe relational information to transform theglobal ranking problem into a data fusion prob-lem; that is also known as a rank aggregateproblem.
The proposed framework is flexibleand can be cascaded with conventional rankingmodels or learning to rank models.The remainder of this paper is organized asfollows.
In Section 2, we present a formal de-finition of global ranking.
In Section 3, we de-scribe the proposed framework and considerthree fusion algorithms that can be used withour framework.
We also explain how the algo-rithms can be adapted to solve the global rank-ing problem.
In Section 4, we introduce a bio-medical text mining task called the interactornormalization task (INT) (Krallinger et al,2009) and show why it should be formulatedas a global ranking problem.
In Section 5, wereport extensive experiments conducted on theINT dataset released by BioCreAtIvE(Krallinger et al, 2009).
Section 6 containssome concluding remarks.2 Global Ranking ProblemThe global ranking problem was first definedformally by Qin et al (2008).
In this paper, wepropose a new global ranking frameworkbased on their definition.
Although we devel-oped the framework independently, we adoptQin et al?s terminology.Let   denote a query.
In addition, letdenote the docu-ments retrieved by  , and letdenote the ranking scoresassigned to the documents.
Here,represents the number of documents retrievedby  .
Note that the numbers of documents va-ries according to different queries.
We assumethat      is determined by a ranking model.If a ranking model is defined on a singledocument, i.e., in the form of,it is called a ?local ranking?
model.Letbe a set ofreal-value functions defined on,, and(                ).
The functionsrepresents the relations between documents.Equation 2 is defined according to the re-quirements of different tasks.
For example, forthe Pseudo Relevance Feedback problem, Qinet al (2008) defined Equation 2 as the similari-ties between any two documents in their CRF-based model.If a ranking model takes all the documentsas its input and exploits both local and globalinformation (Equation 2) in the documents, i.e.,in the form of,it is called a ?global ranking?
approach.
(1)(2)2243 Fusion-based Global RankingFrameworkIt is usually difficult to develop a global rank-ing algorithm that can fully utilize all the localand global information in documents to pro-duce a document rank and also consider thescore ranks.
One example of a global rankingalgorithm that satisfied these criteria is the oneproposed in (Qin et al, 2008) in which themodified CRF algorithm handles context (local)features and relational (global) features in thedocuments.
Without solving a ranking problemdirectly, however, the modified CRF algorithmis more like a regression algorithm since it op-timizes the CRF parameters in a maximumlikelihood estimate without considering thescore ranks.
With respect to the ranking feature,in this section, we describe our frameworkbased on the idea of data fusion for solving theglobal ranking problem.3.1 Framework DescriptionThe flow chart of the proposed framework isillustrated in Figure 1.
The first step is thesame as that of the traditional local rankingmodel.
Given a query, the local ranking modeldefined in Equation 1 is used to calculatethe ranking score for each document, and re-turn a document list sorted according to thelocal scores.The second step transforms the global rank-ing problem into a data fusion problem.
Ouridea is to take each retrieved document as apseudo-IR system, and the pseudo-rankingmodel,, used by each system is the func-tion defined in Equation 2.
For each pseudo-IRsystem,, the pseudo-ranking model for adocumentis defined as follows:.There are totally      pseudo-IR systems,which generate      pseudo-ranked lists.
As aresult, the global ranking problem is trans-formed into a data fusion problem, that is toaggregate the pseudo-ranked lists.
Figure 2shows the steps of the transformation algo-rithm.The final step is to adapt fusion algorithmsto aggregate the pseudo-ranked lists.
A canoni-cal data fusion task is called meta-search(Aslam and Montague, 2001; Fox and Shaw,1994; Lee, 1997; Nuray and Can, 2006), whichaggregates Web search query results from sev-eral engines into a more accurate ranking.
Theorigin of research on data fusion can be tracedback to (Borda, 1781).
In recent years, theprocess has been used in many new applica-tions, including aggregating data from micro-array experiments to discover cancer-relatedgenes (Pihura et al, 2008), integration of re-sults from multiple mRNA studies (Lin andDing, 2008), and similarity searches acrossdatasets and information merging (Adler et al,2009; Zhao et al, 2010).Liu et al (2007) classified data fusion tech-nologies into two categories: order-based fu-sion and score-based fusion.
In the first catego-ry, the orders of the entities in individual rank-ing lists are used by the fusion algorithm.
Inthe second category, the entities in individualranking lists are assigned scores and the fusionalgorithm uses the scores.
In the followingsub-sections, we adapt three fusion algorithmsStep 3Step 2Step 1Local Ranking ModelDocument SetQueryLocal RankedDocument ListTransformationAlgorithmPseudo-IRSystem1Pseudo-IRSystem2Pseudo-IRSystemn(q)...FusionAlgorithmPseudo-Ranked List1Pseudo-Ranked List2Pseudo-Ranked Listn(q)...Global RankedDocument ListPseudo-Ranking Model1Pseudo-RankingModel2Pseudo-RankingModeln(q)Figure 1.
The Proposed Framework forGlobal Ranking.
(3)225for the proposed framework.
The first is theBorda-fuse model (Aslam and Montague,2001), an order-based fusion approach basedon an optimal voting procedure.
The second isa linear combination (LC) model (Vogt andCottrell, 1999), which is a score-based fusionapproach.3.2 Borda-fuseThe Borda-fuse model (Aslam and Montague,2001) is based on a political election strategycalled the Borda Count.
For our framework,the rationale behind the strategy is as follows.Each pseudo-IR systemis an analogy fora voter; and each voter ranks a fixed set ofdocuments in order of preference (Equation 3).For each voter, the top ranked document isgiven      points, the second ranked documentis given     -  points, and so on.
If some doc-uments left unranked by the voter, the remain-ing points are divided equally among the un-ranked documents.
The documents are rankedin descending order of the total points.In our framework, we implement two Bor-da-fuse-based models.
The first is the modifiedBorda-fuse (MBF) model.
In MBF, the numberof points given for a voter's first and subse-quent preferences is determined by the numberof documents they have actually ranked, ratherthan the total number of ranked.
Because theranking model,, used by the pseudo-IRsystem may only retrieve  documents whereis smaller than     , we penalize systemsthat do not rank a full document set by reduc-ing the number of points their vote distributesamong the documents.
In other words, if thereare ten documents, but the pseudo-IR systemonly retrieves five, then the first document willonly receive 5 points; the second will receive 4points, and so on.The second is the weighted Borda-fuse(WBF) model.
The original Borda-fuse modelreflects a democratic election in which eachvoter has equal weight.
However, in many cas-es, we prefer some voters because they aremore reliable.
We employ a simple weightingscheme that multiplies the points assigned to adocument determined by systemby aweight.3.3 LC ModelThe LC model has been used by many IR re-searchers with varying degrees of success(Bartell et al, 1994; Knaus et al, 1995; Vogtand Cottrell, 1999; Vogt and Cottrell, 1998).
Inour framework, it is defined as follows.
Givena query  , a document, the weightsforindividualpseudo-IR systems, and jth pseudo-IR sys-tem?s ranking score, the LC model cal-culates the ranking score   ofagainst allpseudo-IR systems as follows:This score is then used to rank the documents.For example, for two pseudo-IR systems, thisreduces to:Compared with MBF, Equation 4 requiresboth relevance scores and training data to de-function transform (    : the documents retrievedwith query  ){generate pseudo-ranked lists for     }# a dictionary that maps the pseudo-IR systems to# their corresponding pseudo-ranked lists1.
pseudoRankedLists = {}2. forin     :# a dictionary that maps the relation score (real# value) to a list of documents.3.
relation = {}forin     :4.         relation[].append()# relation.keys() returns all keys stored in the# dictionary relation.
The key of relation is the# relation score.5.
Sort relation.keys() in decreasing order# a dictionary that maps a new rank to a list of# documents.6.
pseudoRankedList = {}7.     newRank = 0for score in sorted relation.keys():# relation[score] returns the document list# corresponding to the given scorefor doc in relation[score]:8.             pseudoRankedList[1+newRank].append(doc)9.         newRank = newRank + 110.     pseudoRankedLists [] = pseudoRankedListreturn pseudoRankedListsFigure 2.
The Dependent Ranked List Gen-eration Algorithm (represented using pythonsyntax).
(4)226termine the weight   given to each pseudo-IRsystem.4 Case StudyIn this section, we describe the task examinedin our study.
We also explain how we formu-late the task as a global ranking problem.
Theexperiments results are detailed in Section 5.4.1 Interactor Normalization TaskThe interactor normalization task (INT) is acomplicated text mining task that involves thefollowing steps: (1) It recognizes gene men-tions in a full text article.
(2) It maps the rec-ognized gene mentions to correspondingunique database identifiers which is similar tothe word sense disambiguation task in compu-tational linguistics.
(3) It generates a rankedlist of the identifiers according to their impor-tance in the article and their probability ofplaying the interactor role in protein-proteininteractions (PPIs).
Such ranked lists are usefulfor human curators and can speed up PPI data-base curation.Dai et al (2010) won first place in the Bio-CreAtIvE II.5 INT challenge (Mardis et al,2009) by using a SVM-based local rankingmodel in which they treat gene mentions?
iden-tifiers in an article as the document set, and thequery is a constant string ?interactor?.
Basedon their feature sets and evaluation results, wecan find that their local ranking model tends torank focus genes higher (Dai et al, 2010).However, the primary objective of INT is togenerate a ranked list of interaction gene iden-tifiers.
According to (Jenssen et al, 2001), co-mentioned genes are usually related in someway.
For example, if two gene mentions fre-quently occur alongside each other in the samesentence in an article, they probably have anassociation and influence each other?s rank.Take a low-ranked interactor that is only men-tioned twice in an article as an example.
Ifboth mentions are next to the highest-rankedinteractor in the article, then the low-rankedinteractor?s rank should be boosted significant-ly.
Therefore, the ranking task for each articlecan be formulated as a global ranking problem;the global ranking algorithm should considerboth the local information from Dai et al?smodel and the global information from the as-sociations among identifiers.4.2 Global Ranking in INTLet   be a constant ?interactor.?
The identifierset generated by an INT system for a full-textarticle is analogous to the document set.
Here      denotesthe number of identifiers.
Note that the numberof identifiers varies for different articles.
Letdenote the rankingscores assigned to the identifiers given by alocal ranking model.
In this study, we used theINT system and SVM-based local rankingmodel released by (Dai et al, 2010) to gener-ate the identifier set and ranking scores.To obtain the global information, we con-sider the co-occurrence of identifiers and em-ploy mutual information (MI) to measure theassociation between two identifiers as follows:.In the above formula, the identifier probabili-ties       and       are estimated by countingthe number of occurrences in an article norma-lized by  , i.e., the number of sentences con-taining identifiers.
The joint probability,, is estimated by the number of timesco-occurs with    in a window of   wordsnormalized by  .
Note that, in practice, otheradvanced approaches can be used to calculatethe association score.For the proposed framework, each identifieris a pseudo-IR system with MI as itspseudo-ranking model.
The identifiersthat co-occur withbecome candidates on?s pseudo-ranked list.5 ExperimentsIn the following sub-sections, we introduce thedataset used in the experiments, describe theevaluation methods, report the results of theexperiments conducted to compare the perfor-mance of different methods, and discuss theefficiency of the proposed global rankingframework.2275.1 DatasetWe used the BioCreAtIvE II.5 Elsevier corpusreleased by BioCreAtIvE II.5 challenge in theexperiments.
The corpus contains 1,190 full-text journal articles selected mainly fromFEBS Letters.
Following the same format asthe BioCreAtIvE II.5 INT challenge, we usedarticles published in 2008 (61 articles) as ourtraining set and articles published in 2007 orearlier (61 articles) as our test set.5.2 A Fusion-based Global RankingFramework for INTBefore applying the proposed framework, wepreprocess the articles in the dataset to identifyall gene mentions, and map them to their cor-responding identifiers.
After preprocessing,each full-text article is associated with a list ofidentifiers (Step 1 in Figure 1).
The transformand fusion algorithm is then applied on eacharticle (Steps 2 and 3 in Figure 1).To apply the WBF and LC models, we needto determine the weight assigned to each pseu-do-IR system.
To obtain the weight, we calcu-late the precision of each rank of the rankedlists generated by Dai et al?s INT system.
Fig-ure 3 shows the precision of ranks 1 to 15 cal-culated by applying three-fold cross validationon the INT training set.
We observe that theprecision declines as the rank increases, whichimplies that the higher ranks predicted by theirSVM-based local ranking model are more reli-able than the lower ranks.5.3 Evaluation MetricsOur evaluations focus on two comparisons: thefirst compares the ranking of the proposedframework with the original local rankingmodel by using the area under the curve of theinterpolated precision/recall (iP/R) curve.
Thisis the evaluation metric used in the BioCreA-tIvE II.5 challenge and is a common way todepict the degradation of precision as one tra-verses the retrieved results by plotting interpo-lated precision numbers against percentagerecall.
The area under the iP/R function     isdefined as follows:,where   is the total number of correct identifi-ers and    is the highest interpolated precisionfor the correct identifier   at   , the recall forthat hit.
The interpolated precision    is calcu-lated for each recall   by taking the highestprecision at   or any     .In the second comparison, we use a standardquality measure in IR to estimate the rankingperformance of local ranking models and theproposed framework.
We adopt NormalizedDiscounted Cumulative Gain (NDCG) tomeasure the performance.
The NDCG score ofa ranking is computed based on DCG (Dis-counted Cumulative Gain) as follows:,where   is the rank position, andis the relevance grade of the  th identifier inthe ranked result set.
In our experiment,corresponds to an interaction iden-tifier, and        corresponds to other iden-tifiers.
NDCG is then computed as follows:,where      denotes the results of a perfectranking.
The NDCG values for all articles areaveraged to obtain the average performance ofthe proposed framework.5.4 INT Test Set PerformanceFigure 4 shows the Area_iPR scores of fourconfigurations.
In the baseline configuration(Local/Rank1), the SVM-based local rankingmodel released by Dai et al is employed.
Inthe configuration Global+LC, Global+MBF,and Global+WBF, the proposed global rankingframework is cascaded with the local rankingmodel and with three data fusion models: theLC model, the modified Borda-fuse (MBF)model, and the weighted Borda-fuse model.The figure also shows the Area_iPR scores ofFigure 3.
Precision of Different Ranks.00.20.40.60.81 2 3 4 5 6 7 8 9 10 11 12 13 14 15PrecisionRank228the top three teams and the average Area_iPRscore of all BioCreAtIvE II.5 INT participants(Average).The results show that under the global rank-ing framework, Area_iPR performance is im-proved in addition to Global+MBF.
The high-est Area_iPR (Global+LC: 46.7%) is 3.2%higher than the Rank 1 score in the BioCreA-tIvE II.5 INT challenge.
According to ouranalysis, before global ranking, identifierswhose feature values rarely appear in the train-ing set are often ranked incorrectly becausetheir feature values are under-estimated by theranking model.
However, if the identifiers co-occur with higher-ranked identifiers whosefeature values appear frequently, the proposedframework is very likely to increase their ranks.This results in an improved Area_iPR score.5.5 Global Ranking PerformanceTo illustrate the effectiveness of the proposedglobal ranking framework and assess its per-formance when it is cascaded with other con-ventional ranking models, we implement asimple term frequency-based ranking function,which is based on the identifier frequency inan article as another local ranking model.
Iftwo or more identifiers have the same frequen-cy, two heuristic rules are employed sequen-tially to rank them: (1) the identifier with thehighest frequency in the Results section of thearticle, and (2) the identifier mentioned first inthe article.Table 1 shows the NDCG percentage gainof different ranking models.
It compares theranked list generated by our global rankingframework and by the local ranking models.We observe that (1) irrespective of whether thelocal ranking model is a conventional model ora learning to rank model, Global+LC andGlobal+WBF models achieve NDCG gainsover the original rankings of the local rankingmodels; (2) the results show that our globalranking framework can improve the perfor-mance by only exploiting MI analysis.
Howev-er, it is expected that employing more ad-vanced relation extraction methods to deter-mine the global information (Equation 3)would yield more reliable pseudo-ranked listsand lead to a further improvement in the finalranking; and (3) similar to the results in Sec-tion 5.4, the performance of Global+MBF doesnot improve.
Global+MBF has a negativeNDCG gain and the Area_iPR decreases by2.61%.
We believe this is due to MBF givesequal weight to each pseudo-IR system.
Asmentioned in Section 4.1, the document set inINT is comprised of the identifiers of the genementions derived by Dai et al?s system.
Un-fortunately, there must be incorrect identifiers(the errors may be due to their gene mentionrecognition or identifier mapping processes).As in the meta-search, the best performance isoften achieved by weighting the input systemsunequally.
Reasonable weights allow the algo-rithm to concentrate on good feedback frompseudo-IR systems and ignore poor feedback.As shown by the average precision results inFigure 3, the identifiers (corresponding to thepseudo-IR systems in our framework) in thehigher ranks are more reliable; however, MBFcannot use this information, which leads to anegative NDCG gain and a lower Area_iPRscore.6 ConclusionWe have presented a new global rankingframework based on data fusion technology.Our approach solves the global ranking prob-lem in three stages: the first stage ranks thedocument set by the original local rankingmodel; the second stage transforms the prob-Based on Global Ranking NDCG1 NDCG3 NDCG5Local Ranking/Rank1Global+LC +0.908 +1.323 -0.003Global+MBF -3.279 -1.034 -0.020Global+WBF -0.016 +3.630 +2.071Freq Global+LCf +1.639 +3.152 +2.817Global+MBFf -6.860 -4.275 -4.839Global+WBFf +2.549 +2.390 +3.043Table 1.
The NDCG Gain (%) of DifferentRanking Models.Figure 4.
The Area_iPR Results of DifferentRanking Models0.22 0.27 0.32 0.37 0.42 0.47 0.52Global+WBFGlobal+MBFGlobal+LCLocal/Rank1Hakenberg/Rank 2S?
tre/Rank 3Average229lem into a data fusion task by using global in-formation, and the final stage adapts fusionalgorithms to solve the ranking problem.
Theframework is flexible and it can be combinedwith other mature ranking models and fusionalgorithms.
We also show how the BioCreA-tIvE INT can be formulated as a global rankingproblem and solved by the proposed frame-work.
Experiments on the INT dataset demon-strate the effectiveness of the proposed frame-work and its superior performance over otherranking models.In our future work, we will address the fol-lowing issues: (1) the use of advanced datafusion algorithms in the proposed framework;(2) assessing the performance of the proposedframework on other tasks, such as Pseudo Re-levance Feedback and Topic Distillation; and(3) design an advanced supervised learningrelation extraction algorithm to replace MI inINT to evaluate the system performance.ReferencesAdler, P., R. Kolde, M. Kull, A. Tkachenko, H.Peterson, J. Reimand and J. Vilo (2009).
Miningfor coexpression across hundreds of datasetsusing novel rank aggregation and visualizationmethods.
Genome Biology 10(R139).Aslam, J.
A. and M. Montague (2001).
Models formetasearch.
Proceedings of the 24th annualinternational ACM SIGIR conference onResearch and development in informationretrieval, New Orleans, Louisiana, United States,ACM.Bartell, B. T., G. W. Cottrell and R. K. Belew(1994).
Automatic combination of multipleranked retrieval systems.
Proceedings of the17th annual international ACM SIGIRconference on Research and development ininformation retrieval, Dublin, Ireland Springer-Verlag New York, Inc.Borda, J.
(1781).
M?moire sur les ?lections auscrutin.
Histoire del'Acad e?mie Royale desSciences 2: 13.Chowdhury, G. (2007).
TREC: Experiment andEvaluation in Information Retrieval.
OnlineInformation Review 31(5): 462.Dai, H.-J., P.-T. Lai and R. T.-H. Tsai (2010).Multi-stage gene normalization and SVM-basedranking for protein interactor extraction in full-text articles.
IEEE TRANSACTIONS ONCOMPUTATIONAL BIOLOGY ANDBIOINFORMATICS 14 May.
2010.
IEEEcomputer Society Digital Library.
IEEEComputer Society.Fox, E. A. and J.
A. Shaw (1994).
Combination ofMultiple Searches.
1994, Proceedings of theSecond Text REtrieval Conference (TREC 2)Jenssen, T.-K., A. Lagreid, J. Komorowski and E.Hovig (2001).
A literature network of humangenes for high-throughput analysis of geneexpression.
Nature Genetics 28(1): 21-28.Knaus, D., E. Mittendorf and P. Sch?uble (1995).Improving a basic retrieval method by links andpassage level evidence.
NIST SpecialPublication 500-225: Overview of the Third TextREtrieval Conference (TREC-3).Krallinger, M., F. Leitner and A. Valencia (2009).The BioCreative II.5 challenge overview.Proceedings of the BioCreative II.5 Workshop2009 on Digital Annotations, Madrid, Spain.Kwok, K. L. (1984).
A document-documentsimilarity measure based on cited titles andprobability theory, and its application torelevance feedback retrieval.
SIGIR'84.Lee, J. H. (1997).
Analyses of multiple evidencecombination.
Proceedings of the 20th annualinternational ACM SIGIR conference onResearch and development in informationretrieval, Philadelphia, Pennsylvania, UnitedStates, ACM.Lin, S. and J. Ding (2008).
Integration of RankedLists via Cross Entropy Monte Carlo withApplications to mRNA and microRNA Studies.Biometrics 65(1): 9-18.Liu, Y.-T., T.-Y.
Liu, T. Qin, Z.-M. Ma and H. Li(2007).
Supervised rank aggregation.Proceedings of the 16th international conferenceon World Wide Web, Banff, Alberta, Canada,ACM.Mardis, S., F. Leitner and L. Hirschman (2009).BioCreative II.5: Evaluation and ensemblesystem performance.
Proceedings of theBioCreative II.5 Workshop 2009 on DigitalAnnotations, Madrid, Spain.Nuray, R. and F. Can (2006).
Automatic ranking ofinformation retrieval systems using data fusion.Inf.
Process.
Manage.
42(3): 595-614.Pihura, V., S. Dattaa and S. Datta (2008).
Findingcommon genes in multiple cancer types throughmeta?analysis of microarray experiments: A230rank aggregation approach Genomics 92(6):400-403Qin, T., T.-Y.
Liu, X.-D. Zhang, D.-S. Wang and H.Li (2008).
Global Ranking Using ContinuousConditional Random Fields.
Proceedings of theTwenty-Second Annual Conference on NeuralInformation Processing Systems  (NIPS 2008),Vancouver, Canada.Qin, T., T. Liu, X. Zhang, D. Wang, W. Xiong andH.
Li (2008).
Learning to rank relational objectsand its application to web search, ACM.Vogt, C. and G. Cottrell (1999).
Fusion via a linearcombination of scores.
Information Retrieval1(3): 151-173.Vogt, C. C. and G. W. Cottrell (1998).
Predictingthe performance of linearly combined IR systems.Proceedings of the 21st annual internationalACM SIGIR conference on Research anddevelopment in information retrieval, Melbourne,Australia ACM.Zhao, Z., J. Wang, S. Sharma, N. Agarwal, H. Liuand Y. Chang (2010).
An Integrative Approachto Identifying Biologically Relevant Genes.Proceedings of SIAM International Conferenceon Data Mining (SDM).231
