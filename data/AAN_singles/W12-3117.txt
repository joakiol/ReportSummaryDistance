Proceedings of the 7th Workshop on Statistical Machine Translation, pages 138?144,Montre?al, Canada, June 7-8, 2012. c?2012 Association for Computational LinguisticsDCU-Symantec Submission for the WMT 2012 Quality Estimation TaskRaphael Rubino?
?, Jennifer Foster?, Joachim Wagner?,Johann Roturier?, Rasul Samad Zadeh Kaljahi?
?, Fred Hollowood?
?Dublin City University, ?Symantec, Ireland?firstname.lastname@computing.dcu.ie?firstname lastname@symantec.comAbstractThis paper describes the features and the ma-chine learning methods used by Dublin CityUniversity (DCU) and SYMANTEC for theWMT 2012 quality estimation task.
Two setsof features are proposed: one constrained, i.e.respecting the data limitation suggested by theworkshop organisers, and one unconstrained,i.e.
using data or tools trained on data that wasnot provided by the workshop organisers.
Intotal, more than 300 features were extractedand used to train classifiers in order to predictthe translation quality of unseen data.
In thispaper, we focus on a subset of our feature setthat we consider to be relatively novel: fea-tures based on a topic model built using theLatent Dirichlet Allocation approach, and fea-tures based on source and target language syn-tax extracted using part-of-speech (POS) tag-gers and parsers.
We evaluate nine featurecombinations using four classification-basedand four regression-based machine learningtechniques.1 IntroductionFor the first time, the WMT organisers this year pro-pose a Quality Estimation (QE) shared task, whichis divided into two sub-tasks: scoring and rankingautomatic translations.
The aim of this workshop isto define useful sets of features and machine learn-ing techniques in order to predict the quality of amachine translation (MT) output T (Spanish) givena source segment S (English).
Quality is measuredusing a 5-point likert scale which is based on post-editing effort, following the scoring scheme:1.
The MT output is incomprehensible2.
About 50-70% of the MT output needs to beedited3.
About 25-50% of the MT output needs to beedited4.
About 10-25% of the MT output needs to beedited5.
The MT output is perfectly clear and intelligi-bleThe final score is a combination of the scores as-signed by three evaluators.
The use of a 5-point scalemakes the scoring task more difficult than a binaryclassification task where a translation is consideredto be either good or bad.
However, if the task issuccessfully carried out, the score produced is moreuseful.Dublin City University and Symantec jointly ad-dress the scoring task.
For each pair (S, T ) of sourcesegment S and machine translation T , we train threeclassifiers and one classifier combination using thetraining data provided by the organisers to predict5-point Likert scores.
In this paper, we present theclassification results on the test set alng with addi-tional results obtained using regression techniques.We evaluate the usefulness of two new sets of fea-tures:1. topic-based features using Latent Dirichlet Al-location (LDA (Blei et al, 2003)),2. syntax-based features using POS taggers andparsers (Wagner et al, 2009)The remainder of this paper is organised as fol-lows.
In Section 2, we give an overview of all the138features employed in our QE system.
Then, in Sec-tion 3, we describe the topic and syntax-based fea-tures in more detail.
Section 4 presents the vari-ous classification and regression techniques we ex-plored.
Our results are presented and discussed inSection 5.
Finally, we summarise and outline ourplans in Section 6.2 Features OverviewIn this section, we describe the features used in ourQE system.
In the first subsection, the features in-cluded in our constrained system are presented.
Inthe second subsection, we detail the features in-cluded in our unconstrained system.
Both of thesesystems include the 17 baseline features providedfor the shared task.2.1 Constrained SystemThe constrained system is based only on the dataprovided by the organisers.
We extracted 70 fea-tures in total (including the baseline features) andwe present them here according to the type of infor-mation they capture.Word and Phrase-Level Features?
Ratio of source and target segment length:the number of source words divided by thenumber of target words?
Ratio of source and target number of punc-tuation marks: the number of source punctua-tion marks divided by the number of target ones?
Number of phrases comprising the MT out-put: given a phrase-table, we assume that asentence composed of several phrases indicatesuncertainty on the part of the MT system.?
Average length of source and target phrases:concatenating short phrases may result in lowerfluency compared to the use of longer ones.?
Ratio of source and target averaged phraselength?
Number of source prepositions and conjunc-tions word: our assumption here is that seg-ments containing a relatively high number ofprepositions and conjunctions may be morecomplex and difficult to translate.?
Number of source out-of-vocabulary wordsLanguage Model FeaturesAll the language models (LMs) used in our workare n-gram LMs with Kneser-Ney smoothing builtwith the SRI Toolkit (Stolcke, 2002).?
Backward 2-gram and 3-gram source andtarget log probabilities: as proposed byDuchateau et al (2002)?
Log probability of target segments on5-gram MT-output-based LM: usingMOSES (Koehn et al, 2007) trained on theprovided parallel corpus, we translated the En-glish side of this corpus into Spanish, assumingthat the MT output contains mistakes.
ThisMT output is used to build a LM that modelsthe behavior of the MT system.
We assumethat for a given MT output, a high n-gramprobability (or a low perplexity) of the LMindicates that the MT output contains mistakes.MT-system Features?
15 scores provided by Moses: phrase-table,language model, reordering model and wordpenalty (weighted and unweighted)?
Number of n-bests for each source segment?
MT output back-translation: from Spanish toEnglish using MOSES trained on the providedparallel corpus, scored with TER (Snover etal., 2006), BLEU (Papineni et al, 2002) andthe Levenshtein distance (Levenshtein, 1966),based on the source segments as a translationreferenceTopic Model Features?
Probability distribution over topics: Sourceand target segment probability distribution overtopics for a 10-dimension topic model?
Cosine distance between source and targettopic vectorsMore details about these two features are providedin Section 3.1.2.2 Unconstrained SystemIn addition to the features used for the constrainedsystem, a further 238 unconstrained features wereincluded in our unconstrained system.139MT System FeaturesAs for our constrained system, we use MT outputback-translation from Spanish to English, but thistime using Bing Translator1 in addition to Moses.Each back-translated segment is scored with TER,BLEU and the Levenshtein distance, based on thesource segments as a translation reference.Source Syntax FeaturesWagner et al (2007; 2009) propose a series offeatures to measure sentence grammaticality.
Thesefeatures rely on a part-of-speech tagger, a probabilis-tic parser and a precision grammar/parser.
We haveat our disposal these tools for English and so we ap-ply them to the source data.
The features themselvesare described in more detail in Section 3.2.Target Syntax FeaturesWe use a part-of-speech tagger trained on Spanishto extract from the target data the subset of grammat-icality features proposed by Wagner et al (2007;2009) that are based on POS n-grams.
In additionwe extract features which reflect the prevalence ofparticular POS tags in each target segment.
Theseare explained in more detail in Section 3.2 below.Grammar Checker FeaturesLANGUAGETOOL (based on (Naber, 2003)) is anopen-source grammar and style proofreading toolthat finds errors based on pre-defined, language-specific rules.
The latest version of the tool canbe run in server mode, so individual sentences canbe checked and assigned a total number of errors(which may or may not be true positives).2 Thisnumber is used as a feature for each source segmentand its corresponding MT output.3 Topic and Syntax-based FeaturesIn this section, we focus on the set of featuresthat aim to capture adequacy using topic modellingand grammaticality using POS tagging and syntacticparsing.1http://www.microsofttranslator.com/2The list of English and Spanish rules is available at:http://languagetool.org/languages.3.1 Topic-based FeaturesWe extract source and target features based on atopic model built using LDA.
The main idea in topicmodelling is to produce a set of thematic word clus-ters from a collection of documents.
Using the par-allel corpus provided for the task, a bilingual corpusis built where each line is composed of a source seg-ment and its translation separated by a space.
Eachpair of segments is considered as a bilingual docu-ment.
This corpus is used to train a bilingual topicmodel after stopwords removal.
The resulting modelis one set of bilingual topics z containing words wwith a probability p(wn|zn, ?)
(with n equal to thevocabulary size in the whole parallel corpus).
Thismodel can be used to infer the probability distri-bution of unseen source and target segments overbilingual topics.
During the test step, each sourcesegment and its translation are considered individu-ally, as two monolingual documents.
This methodallows us to compare the source and target topic dis-tributions.
We assume that a source segment and itstranslation share topic similarities.We propose two ways of using topic-based fea-tures for quality estimation: keeping source and tar-get topic vectors as two sets of k features, or com-puting a vector distance between these two vectorsand using one feature only.
To measure the prox-imity of two vectors, we decided to used the Co-sine distance, as it leads to the best results in termsof classification accuracy.
However, we plan tostudy different metrics in further experiments, likethe Manhattan or the Euclidean distances.
Someparameters related to LDA have to be studied morecarefully too, such as the number of topics (dimen-sions in the topic space), the number of words pertopic, the Dirichlet hyperparameter ?, etc.
In ourexperiments, we built a topic model composed of 10dimensions using Gibbs sampling with 1000 itera-tions.
We assume that a higher dimensionality canlead to a better repartitioning of the vocabulary overthe topics.Multilingual LDA has been used before in nat-ural language processing, e.g.
polylingual topicmodels (Mimno et al, 2009) or multilingual topicmodels for unaligned text (Boyd-Graber and Blei,2009).
In the field of machine translation, Tam etal.
(2007) propose to adapt a translation and a lan-140guage model to a specific topic using Latent Se-mantic Analysis (LSA, or Latent Semantic Index-ing, LSI (Deerwester et al, 1990)).
More recently,some studies were conducted on the use of LDA toadapt SMT systems to specific domains (Gong et al,2010; Gong et al, 2011) or to extract bilingual lexi-con from comparable corpora (Rubino and Linare`s,2011).
Extracting features from a topic model is, tothe best of our knowledge, the first attempt in ma-chine translation quality estimation.3.2 Syntax-based FeaturesSyntactic features have previously been used in MTfor confidence estimation and for building automaticevaluation measures.
Corston-Oliver et al (2001)build a classifier using 46 parse tree features to pre-dict whether a sentence is a human translation or MToutput.
Quirk (2004) uses a single parse tree featurein the quality estimation task with a 4-point scale,namely whether a spanning parse can be found, inaddition to LM perplexity and sentence length.
Liuand Gildea (2005) measure the syntactic similaritybetween MT output and reference translation.
Al-brecht and Hwa (2007) measure the syntactic simi-larity between MT output and reference translationand between MT output and a large monolingualcorpus.
Gimenez and Marquez (2007) explore lexi-cal, syntactic and shallow semantic features and fo-cus on measuring the similarity of MT output to ref-erence translation.
Owczarzak et al (2007) use la-belled dependencies together with WordNet to avoidpenalising valid syntactic and lexical variations inMT evaluation.
In what follows, we describe howwe make use of syntactic information in the QE task,i.e.
evaluating MT output without a reference trans-lation.Wagner et al (2007; 2009) use three sourcesof linguistic information in order to extract featureswhich they use to judge the grammaticality of En-glish sentences:1.
For each POS n-gram (with n ranging from 2 to7), a feature is extracted which represents thefrequency of the least frequent n-gram in thesentence according to some reference corpus.TreeTagger (Schmidt, 1994) is used to producePOS tags.2.
Features provided by a hand-crafted, broad-coverage precision grammar of English (Buttet al, 2002) and a Lexical Functional Grammarparser (Maxwell and Kaplan, 1996).
These in-clude whether or not a sentence could be parsedwithout resorting to robustness measures, thenumber of analyses found and the parsing time.3.
Features extracted from the output of threeprobabilistic parsers of English (Charniak andJohnson, 2005), one trained on Wall StreetJournal trees (Marcus et al, 1993), one trainedon a distorted version of the treebank obtainedby automatically creating grammatical errorand adjusting the parse trees, and the thirdtrained on the union of the original and dis-torted versions.These features were originally designed to distin-guish grammatical sentences from ungrammaticalones and were tested on sentences from learner cor-pora by Wagner et al (2009) and Wagner (2012).In this work we extract all three sets of featuresfrom the source side of our data and the POS-basedsubset from the target side.3 We use the publiclyavailable pre-trained TreeTagger models for Englishand Spanish4.
The reference corpus used to obtainPOS n-gram frequences is the MT translation modeltraining data.5In addition to the POS-based features described inWagner et al (2007; 2009), we also extract the fol-lowing features from the Spanish POS-tagged data:for each POS tag P and target segment T , we ex-tract a feature which is the proportion of words inT that are tagged as P .
Two additional features areextracted to represent the proportion of words in Tthat are assigned more than one tag by the tagger,3Unfortunately, due to time constraints, we were unable tosource a suitable probabilistic phrase-structure parser and a pre-cision grammar for Spanish and were thus unable to extractparser-based features for Spanish.
We expect that these featureswould be more useful on the target side than the source side.4http://www.ims.uni-stuttgart.de/projekte/corplex/TreeTagger/5To aid machine learning methods that linearly combine fea-ture values, we add binarised features derived from the raw XLEand POS n-gram features described above, for example we adda feature indicating whether the frequency of the least frequentPOS 5-gram is below 10.
We base the choice of binary fea-tures on (a) decision rules observed in decision trees trained fora binary scoring task and (b) decision rules of simple classifiers(decision trees with just one decision node and 2 leaf nodes)that form a convex hull of optimal classifiers in ROC space.141and the proportion of words in T that are unknownto the tagger.4 Machine LearningIn this section, we describe the machine learningmethods that we experimented with.
Our final sys-tems submitted for the shared task are based on clas-sification methods.
However, we also performedsome experiments with regression methods.We evaluate the systems on the test set using theofficial evaluation script and the reference scores.We report the evaluation results as Mean Aver-age Error (MAE) and Root Mean Squared Error(RMSE).4.1 ClassificationIn order to apply classification algorithms to theset of features associated with each source and tar-get segment, we rounded the training data scoresto the closest integer.
We tested several classifiersand empirically chose three algorithms: SupportVector Machine using sequential minimal optimiza-tion and RBF kernel (parameters optimized by grid-search) (Platt, 1999), Naive Bayes (John and Lang-ley, 1995) and Random Forest (Breiman, 2001) (thelatter two techniques were applied with default pa-rameters).
We use the Weka toolkit (Hall et al,2009) to train the classifiers and predict the scoreson the test set.
Each method is evaluated individu-ally and then combined by averaging the predictedscores.4.2 RegressionWe applied three different regression techniques:SVM epsilon-SVR with RBF kernel, Linear Regres-sion and M5P (Quinlan, 1992; Wang and Witten,1997).
The two latter algorithms were used withdefault parameters, whereas SVM parameters (?, cand ) were optimized by grid-search.
We also per-formed a combination of the three algorithms by av-eraging the predicted scores.
We apply a linear func-tion on the predicted scores S in order to keep themin the correct range (from 1 to 5) as detailed in (1),where S?
is the rescaled sentence score, Smin is thelowest predicted score and Smax is the highest pre-dicted score.S?
= 1 + 4?S ?
SminSmax ?
Smin(1)5 EvaluationTable 1 shows the results obtained by our classifi-cation approach on various feature subsets.
Notethat the two submitted systems used the combinedclassifier approach with the constrained and uncon-strained feature sets.
Table 2 shows the results forthe same feature combinations, this time using re-gression rather than classification.The results of quality estimation using classifica-tion methods show that the baseline and the syntax-based features with the classifier combination leadsto the best results with an MAE of 0.71 and anRMSE of 0.87.
However, these scores are substan-tially lower than the ones obtained using regression,where the unconstrained set of features with SVMleads to an MAE of 0.62 and an RMSE of 0.78.It seems that the classification methods are notsuitable for this task according to the different setsof features studied.
Furthermore, the topic-distancefeature is not correlated with the quality scores, ac-cording to the regression results.
On the other hand,the syntax-based features appear to be the most in-formative and lead to an MAE of 0.70.6 ConclusionWe presented in this paper our submission for theWMT12 Quality Estimation shared task.
We alsopresented further experiments using different ma-chine learning techniques and we evaluated the im-pact of two sets of features - one set which is basedon linguistic features extracted using POS taggingand parsing, and a second set which is based on topicmodelling.
The best results are obtained by our un-constrained system containing all features and us-ing an -SVR regression method with a Radial BasisFunction kernel.
This setup leads to a Mean Aver-age Error of 0.62 and a Root Mean Squared Errorof 0.78.
Unfortunately, we did not submit our bestconfiguration for the shared task.We plan to continue working on the task of ma-chine translation quality estimation.
Our immediatenext steps are to continue to investigate the contribu-tion of individual features, to explore feature selec-tion in a more detailed fashion and to apply our bestsystem to other types of data including sentencestaken from an online discussion forum.142SMO NAIVE BAYES RANDOM FOREST CombinationFeatures MAE RMSE MAE RMSE MAE RMSE MAE RMSEbaseline 0.74 0.89 0.85 1.10 0.84 1.06 0.71 0.88topic distribution 0.84 1.02 1.09 1.38 0.91 1.15 0.78 0.98topic distance 0.88 1.11 0.93 1.17 1.04 1.23 0.84 1.04syntax 0.78 0.97 1.01 1.27 0.83 1.05 0.72 0.90baseline + topic 0.82 1.01 1.00 1.31 0.84 1.05 0.75 0.95baseline + syntax 0.76 0.94 1.01 1.25 0.79 0.98 0.71 0.87baseline + topic + syntax 0.82 1.04 1.03 1.29 0.79 0.98 0.74 0.93all constrained 0.99 1.26 1.12 1.46 0.71 0.88 0.86 ?
1.12 ?all unconstrained 0.97 1.25 0.80 1.02 0.79 0.99 0.75 ?
0.97 ?Table 1: MAE and RMSE results for different sets of features using three classification methods.
The results with ?and ?
correspond to the DCU-SYMC constrained and the DCU-SYMC unconstrained systems respectively, submittedfor the shared task.SVM LINEAR REG.
M5P CombinationFeatures MAE RMSE MAE RMSE MAE RMSE MAE RMSEbaseline 0.78 0.93 0.80 0.99 0.73 0.91 0.72 0.88topic distribution 0.78 0.95 0.79 0.96 0.80 0.96 0.79 0.95topic distance 1.38 1.67 1.31 1.62 1.85 2.09 1.00 1.24syntax 0.70 0.88 0.97 1.22 1.41 1.65 0.76 0.92baseline + topic 0.78 0.96 1.06 1.31 1.16 1.42 0.88 1.10baseline + syntax 0.67 0.82 0.90 1.12 2.17 2.38 0.98 1.22baseline + topic + syntax 0.68 0.84 0.93 1.16 2.12 2.33 0.97 1.21all constrained 0.83 1.02 0.94 1.18 0.78 0.99 0.71 0.88all unconstrained 0.62 0.78 1.33 1.60 0.71 0.89 0.73 0.91Table 2: MAE and RMSE results for different sets of features using three regression methods.ReferencesJ.
Albrecht and R. Hwa.
2007.
A re-examination of ma-chine learning approaches for sentence-level MT eval-uation.
In Proceedings of the 45th Annual Meeting ofthe ACL, pages 880?887.D.M.
Blei, A.Y.
Ng, and M.I.
Jordan.
2003.
LatentDirichlet Allocation.
The Journal of Machine Learn-ing Research, 3:993?1022.Jordan Boyd-Graber and David M. Blei.
2009.
Multilin-gual topic models for unaligned text.
In Proceedingsof the 25th Conference on Uncertainty in Artificial In-telligence, pages 75?82.L.
Breiman.
2001.
Random forests.
Machine learning,45(1):5?32.M.
Butt, H. Dyvik, T. Holloway King, H. Masuichi, andC.
Rohrer.
2002.
The parallel grammar project.
InProceedings of the Coling Workshop on Grammar En-gineering and Evaluation.E.
Charniak and M. Johnson.
2005.
Course-to-fine n-best-parsing and maxent discriminative reranking.
InProceedings of the 43rd Annual Meeting of the ACL,pages 173?180, Ann Arbor.S.
Corston-Oliver, M. Gamon, and C. Brockett.
2001.A machine learning approach to the automatic evalu-ation of machine translation.
In Proceedings of 39thAnnual Meeting of the Association for ComputationalLinguistics, pages 148?155, Toulouse, France, July.S.
Deerwester, S.T.
Dumais, G.W.
Furnas, T.K.
Landauer,and R. Harshman.
1990.
Indexing by Latent SemanticAnalysis.
Journal of the American Society for Infor-mation Science, 41(6):391?407.J.
Duchateau, K. Demuynck, and P. Wambacq.
2002.Confidence scoring based on backward language mod-els.
In Proceedings IEEE international confer-ence on acoustics, speech, and signal processing,ICASSP?2002, volume 1, pages 221?224.J.
Gime?nez and L. Ma`rquez.
2007.
Linguistic featuresfor automatic evaluation of heterogenous MT systems.In Proceedings of the Second Workshop on StatisticalMachine Translation, pages 256?264, Prague, CzechRepublic, June.143Z.
Gong, Y. Zhang, and G. Zhou.
2010.
Statistical ma-chine translation based on lda.
In Universal Commu-nication Symposium (IUCS), 2010 4th International,pages 286?290.Z.
Gong, G. Zhou, and L. Li.
2011.
Improve smt withsource-side ?topic-document?
distributions.
In MTSummit, pages 496?501.M.
Hall, E. Frank, G. Holmes, B. Pfahringer, P. Reute-mann, and I.H.
Witten.
2009.
The weka data min-ing software: an update.
ACM SIGKDD ExplorationsNewsletter, 11(1):10?18.G.H.
John and P. Langley.
1995.
Estimating continuousdistributions in bayesian classifiers.
In Eleventh con-ference on uncertainty in artificial intelligence, pages338?345.
Morgan Kaufmann Publishers Inc.P.
Koehn, H. Hoang, A. Birch, C. Callison-Burch,M.
Federico, N. Bertoldi, B. Cowan, W. Shen,C.
Moran, R. Zens, et al 2007.
Moses: Open sourcetoolkit for statistical machine translation.
In ACL,pages 177?180.V.I.
Levenshtein.
1966.
Binary codes capable of cor-recting deletions, insertions, and reversals.
In SovietPhysics Doklady, volume 10-8, pages 707?710.D.
Liu and D. Gildea.
2005.
Syntactic features for eval-uation of machine translation.
In Proceedings of theACL Workshop on Intrinsic and Extrinsic EvaluationMeasures for Machine Translation and/or Summariza-tion, pages 25?32, Ann Arbor, Michigan.M.
P. Marcus, B. Santorini, and M. A. Marcinkiewicz.1993.
Building a large annotated corpus of en-glish: the penn treebank.
Computational Linguistics,19(2):313?330.John Maxwell and Ron Kaplan.
1996.
An EfficientParser for LFG.
In Proceedings of LFG-96, Grenoble.D.
Mimno, H.M. Wallach, J. Naradowsky, D.A.
Smith,and A. McCallum.
2009.
Polylingual topic models.In Proceedings of EMNLP: Volume 2-Volume 2, pages880?889.
Association for Computational Linguistics.D.
Naber.
2003.
A rule-based style and grammarchecker.
Technical report, Bielefeld University Biele-feld, Germany.K.
Owczarzak, J. van Genabith, and A.
Way.
2007.
La-belled dependencies in machine translation evaluation.In Proceedings of the Second Workshop on StatisticalMachine Translation, pages 104?111, Prague, CzechRepublic, June.K.
Papineni, S. Roukos, T. Ward, and W.J.
Zhu.
2002.Bleu: a method for automatic evaluation of machinetranslation.
In ACL, pages 311?318.J.C.
Platt.
1999.
Fast training of support vector machinesusing sequential minimal optimization.
In Advances inkernel methods, pages 185?208.
MIT Press.R.
J. Quinlan.
1992.
Learning with continuous classes.In 5th Australian Joint Conference on Artificial Intel-ligence, pages 343?348, Singapore.
World Scientific.C.
Quirk.
2004.
Training a sentence-level machine trans-lation confidence measure.
In Proceedings of LREC,Lisbon, June.R.
Rubino and G. Linare`s.
2011.
A multi-view approachfor term translation spotting.
Computational Linguis-tics and Intelligent Text Processing, 6609:29?40.H.
Schmidt.
1994.
Probabilistic part-of-speech taggingusing decision trees.
In Proceedings of the Interna-tional Conference on New Methods in Natural Lan-guage Processing.M.
Snover, B. Dorr, R. Schwartz, L. Micciulla, andJ.
Makhoul.
2006.
A study of translation edit rate withtargeted human annotation.
In Proceedings of Associ-ation for Machine Translation in the Americas, pages223?231.A.
Stolcke.
2002.
SRILM-an extensible language mod-eling toolkit.
In InterSpeech, volume 2, pages 901?904.Y.C.
Tam, I.
Lane, and T. Schultz.
2007.
Bilinguallsa-based adaptation for statistical machine translation.Machine Translation, 21(4):187?207.J.
Wagner, J.
Foster, and J. van Genabith.
2007.
A com-parative evaluation of deep and shallow approaches tothe automatic detection of common grammatical er-rors.
In Proceedings of EMNLP-CoNLL, pages 112?121, Prague, Czech Republic, June.J.
Wagner, J.
Foster, and J. van Genabith.
2009.
Judg-ing grammaticality: Experiments in sentence classifi-cation.
CALICO Journal, 26(3):474?490.J.
Wagner.
2012.
Detecting grammatical errors withtreebank-induced probabilistic parsers.
Ph.D. thesis,Dublin City University.Y.
Wang and I. H. Witten.
1997.
Induction of model treesfor predicting continuous classes.
In Poster papers ofthe 9th European Conference on Machine Learning.Springer.144
