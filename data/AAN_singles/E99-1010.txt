Proceedings ofEACL '99An Eff icient Method  for Determin ing  B i l ingua l  Word  ClassesFranz  J ose f  OchLehrstuhl ffir Informatik VIRWTH Aachen - University of TechnologyAhornstrai3e 5552056 AachenGERMANYoch@informatik.rwth-aachen.deAbstractIn statistical natural anguage process-ing we always face the problem of sparsedata.
One way to reduce this problem isto group words into equivalence classeswhich is a standard method in statisticallanguage modeling.
In this paper we de-scribe a method to determine bilingualword classes uitable for statistical ma-chine translation.
We develop an opti-mization criterion based on a maximum-likelihood approach and describe a clus-tering algorithm.
We will show that theusage of the bilingual word classes we getcan improve statistical machine transla-tion.1 In t roduct ionWord classes are often used in language modellingto solve the problem of sparse data.
Various clus-tering techniques have been proposed (Brown etal., 1992; Jardino and Adda, 1993; Martin et al,1998) which perform automatic word clusteringoptimizing a maximum-likelihood criterion withiterative clustering algorithms.In the field of statistical machine translationwe also face the problem of sparse data.
Ouraim is to use word classes in statistical machinetranslation to allow for more robust statisticaltranslation models.
A naive approach for doingthis would be the use of mono-lingually optimizedword classes in source and target language.
Un-fortunately we can not expect hese independentlyoptimized classes to be correspondent.
There-fore mono-lingually optimized word classes do notseem to be useful for machine translation (see also(Fhng and Wu, 1995)).
We define bilingual wordclustering as the process of forming correspond-ing word classes uitable for machine translationpurposes for a pair of languages using a paralleltraining corpus.The described method to determine bilingualword classes is an extension and improvementof the method mentioned in (Och and Weber,1998).
Our approach is simpler and computation-ally more efficient han (Wang et al, 1996).2 Monolingual Word ClusteringThe task of a statistical language model is to es-timate the probability Pr(w N) of a sequence ofwords w N = w l .
.
.wg .
A simple approximationof Pr(w N) is to model it as a product of bigramprobabilities: Pr (w~)  N = I-\[i=1P(WilWi-1) ?
If wewant to estimate the bigram probabilities p(w\[w')using a realistic natural anguage corpus we arefaced with the problem that most of the bigramsare rarely seen.
One possibility to solve this prob-lem is to partition the set of all words into equiv-alence classes.
The function C maps words w totheir classes C(w).
Rewriting the corpus probabil-ity using classes we arrive at the following proba-bility model p(wNlC):NP(wNIC) :=i=1(1)In this model we have two types of probabili-ties: the transition probability p(CIC ~) for classC given its predecessor class C' and the member-ship probability p(wlC ) for word w given class C.To determine the optimal classes C for a givennumber of classes M we perform a maximum-likelihood approach:C = arg mpx p(w lc) (2)We estimate the probabilities of Eq.
(1) byrelative frequencies: p(CIC' ) := n(CIC')/n(C'),p(wlC ) = n(w)/n(C) .
The function (-) providesthe frequency of a uni- or bigram in the trainingcorpus.
If we insert this into Eq.
(2) and applythe negative logarithm and change the summa-tion order we arrive at the following optimization71Proceedings of EACL '99criterion LP1 (Kneser and Ney, 1991):LPx(C,n) = - ~ h(n(C\]C'))C,C'+2 Zh(n(C) )  (3)C= argm~n LPI(C,n).
(4)The function h(n) is a shortcut for n .
log(n).It is necessary to fix the number of classes inC in advance as the optimum is reached if everyword is a class of its own.
Because of this it isnecessary to perform an additional optimizationprocess which determines the number of classes.The use of leaving-one-out in a modified optimiza-tion criterion as in (Kneser and Ney, 1993) couldin principle solve this problem.An efficient optimization algorithm for LP1 isdescribed in section 4.3 B i l ingua l  Word  C lus ter ingIn bilingual word clustering we are interested inclasses ~" and C which form partitions of the vo-cabulary of two languages.
To perform bilingualword clustering we use a maximum-likelihood ap-proach as in the monolingnal case.
We maximizethe joint probability of a bilingual training corpus(el, f J):= argma  (5)$,f i= argmax p(e/\[C) .p(fJ le~;C,3)(6) $,~"To perform the maximization ofEq.
(6) we have tomodel the monolingual  priori probability p(e I IE)and the translation probability p(fJte~; E, .T).
Forthe first we use the class-based bigram probabilityfrom Eq.
(1).To model p(fJle~;8,.T) we assume the exis-tence of an alignment aJ.
We assume that ev-ery word f j  is produced by the word e~j at posi-tion aj in the training corpus with the probabilityP(f~le,~i):Jp(f lc ') = 1\] p(L Icon)j= l(7)The word alignment aJ is trained automaticallyusing statistical translation models as described in(Brown et al, 1993; Vogel et al, 1996).
The ideais to introduce the unknown alignment al J as hid-den variable into a statistical model of the trans-lation probability p(fJle~).
By applying the EM-algorithm we obtain the model parameters.
Thealignment a J that we use is the Viterbi-Alignmentof an HMM alignment model similar to (Vogel etal., 1996).By rewriting the translation probability usingword classes, we obtain (corresponding to Eq.
(1)):Jp(f le '; E, = 1\]j= l(s)The variables F and E denote special classes in9 v and ~'.
We use relative frequencies to estimatep(FIE) and p(flF):p(F\[E) = nt(FIE)/ (~F hi(FIE))The function nt(FIE) counts how often the wordsin class F are aligned to words in class E. If weinsert these relative frequencies into Eq.
(8) andapply the same transformations as in the monolin-gual case we obtain a similar optimization crite-rion for the translation probability part of Eq.
(6).Thus the full optimization criterion for bilingualword classes is:- ~ h(n(E\[E')) - ~ h(nt(FIE))E,E' E,F+2Eh(n(E))E+ Z h(~-~' nt(FIE))+ ~--~ h(E  nt(FIE))F E E FThe two count functions n(EIE' ) and nt(FIE ) canbe combined into one count function ng(X\[Y ) :=n(XIY)+nt(X\[Y ) as for all words f and all wordse and e' holds n(fle ) = 0 and nt(ele' ) = O. Usingthe function ng we arrive at the following opti-mization criterion:LP2((C,~'),ng) = - ~ h(ng(ZlX')) +X,X'~h(ng , l (X ) )  + Eh(ng,2(X)) (9)X x(~,~) = argmin LP2((E,~-),ng) (10)Here we defined ng,l(X) = ~'~x, ng(X\[X') andng,2(X) = ~"~x' ng(X'\[X).
The variable X runsover the classes in E and Y.
In the optimiza-tion process it cannot be allowed that words of72Proceedings of EACL '99INPUT: Parallel corpus (e~,/~) and number of classes in 6 and ~.Determine the word alignment a~.Get some initial classes C and ~.UNTIL convergence riterion is met:FOR EACH word e:FOR EACH class E:\[Determine the change of LP((E, 9v), rig) if e is moved to E.Move e to the class with the largest improvement.FOR EACH 'word f:FOR EACH class F:~ the change of LP((C,.gv), rig) if f is moved to F.Move f to the class with the largest improvement.OUTPUT: Classes C and 5 r.Figure 1: Word Clustering Algorithm.different languages occur in one class.
It can beseen that Eq.
(3) is a special case of Eq.
(9) withg,1 ----- r ig ,2 .Another possibility to perform bilingual wordclustering is to apply a two-step approach.
In afirst step we determine classes ?
optimizing onlythe monolingual part of Eq.
(6) and secondly wedetermine classes 5~ optimizing the bilingual part(without changing C):= argm~n LP2(~,n) (11).~ = argm~n LP2((E, Sr),n~).
(12)By using these two optimization processes we en-force that the classes E are mono-lingually 'good'classes and that the classes fi- correspond to ~.Interestingly enough this results in a higher trans-lation quality (see section 5).4 Imp lementat ionAn efficient optimization algorithm for LPz is theexchange algorithm (Martin et al, 1998).
Forthe optimization of LP2 we can use the same al-gorithm with small modifications.
Our startingpoint is a random partition of the training corpusvocabulary.
This initial partition is improved it-eratively by moving a single word from one classto another.
The algorithm to determine bilingualclasses is depicted in Figure 1.If only one word w is moved between the parti-tions C and C' the change LP(C, ng) - LP(C', ng)can be computed efficiently looking only at classesC for which ng(w, C) > 0 or ng(C, w) > 0.
We de-fine M0 to be the average number of seen predeces-sor and successor word classes.
With the notationI for the number of iterations needed for conver-gence, B for the number of word bigrams, M forthe number of classes and V for the vocabulary?
?
.
, ?
?
?
.
.
.
?fifty-eight .
.
.
.
.
.
.
.
?six .
.
.
.
.
.
.
.
?
?
frca .
.
.
.
.
.
.
.
.Hanover  " " \ ]"  ?
.
.
.
.
.
.
.
.
.I ?
frn~ "hourlYtraingOesthe .
.
.
.
.
.
.
?
?
?
'I 4" .
.
.
.
.m 13Figure 2: Examples of alignment templates ~.size the computational complexity of this algo-rithm is roughly I.
(B. log 2 (B/V) + V.  M.  Mo).A detailed analysis of the complexity can be foundin (Martin et al, 1998).The algorithm described above provides only alocal optimum.
The quality of the resulting localoptima can be improved if we accept a short-termdegradation of the optimization criterion duringthe optimization process.
We do this in our imple-mentation by applying the optimization methodthreshold accepting (Dueck and Scheuer, 1990)which is an efficient simplification of simulated an-nealing.73Proceedings ofEACL '99Table 1: The EUTRANS-I corpus.Train:Test:SentencesWordsVocabulary SizeSentencesWordsBigr.
PerplexitySpanish English1000097131 99292686 5132 99635023 35590- 5.2Table 2: The EUTRANS-II corpus.German EnglishTrain: 16 226Test:SentencesWordsVocabulary SizeSentencesWordsBigr.
Perplexity266080 29994539511 257511872 556 2 853- 1575 Resu l tsThe statistical machine-translation method de-scribed in (Och and Weber, 1998) makes use ofbilingual word classes.
The key element of thisapproach are the alignment templates (originallyreferred to as translation rules)which are pairs ofphrases together with an alignment between thewords of the phrases.
Examples of alignment tem-plates are shown in Figure 2.
The advantage of thealignment template approach against word-basedstatistical translation models is that word contextand local re-orderings are explicitly taken into ac-count.The alignment emplates are automaticallytrained using a parallel training corpus.
Thetranslation of a sentence is done by a search pro-cess which determines the set of alignment tem-plates which optimally cover the source sentence.The bilingual word classes are used to general-ize the applicability of the alignment templates insearch.
If there exists a class which contains allcities in source and target language it is possiblethat an alignment template containing a specialcity can be generalized to all cities.
More detailsare given in (Och and Weber, 1998; Och and Ney,1999).We demonstrate r sults of our bilingual clus-tering method for two different bilingual corpora(see Tables 1 and 2).
The EUTRANS-I corpus isa subtask of the "Traveller Task" (Vidal, 1997)which is an artificially generated Spanish-Englishcorpus.
The domain of the corpus is a human-to-human communication situation at a receptionTable 3: Example of bilingual word classes (corpusEUTRANS-I, method BIL-2).El: how it pardon what when where which.who whyE2: my ourE3: today tomorrowE4: ask call makeE5: carrying changing giving lookingmoving putting sending showing wakingE6: full half quarter$1: c'omo cu'al cu'ando cu'anta d'ondedice dicho hace qu'e qui'en tiene$2: ll'eveme mi mis nuestra nuestrasnuestro nuestros s'ub~nme$3: hoy manana mismo$4: hacerme ll'ameme ll'amenos llamallamar llamarme llamarnos llame p'idamep'idanos pedir pedirme pedirnospida pide$5: cambiarme cambiarnos despertarmedespertarnos llevar llevarme llevarnossubirme subirnos usted ustedes$6: completa cuarto media menosdesk of a hotel.
The EUTRANS-II corpus is a natu-ral German-English corpus consisting of differenttext types belonging to the domain of tourism:bilingual Web pages of hotels, bilingual touristicbrochures and business correspondence.
The tar-get language of our experiments is English.We compare the three described methods togenerate bilingual word classes.
The classesMONO are determined by monolingually opti-mizing source and target language classes withEq.
(4).
The classes BIL are determined by bilin-gually optimizing classes with Eq.
(10).
Theclasses BIL-2 are determined by first optimiz-ing mono-lingually classes for the target language(English) and afterwards optimizing classes forthe source language (Eq.
(11) and Eq.
(12)).For EUTRANS-I we used 60 classes and forEUTRANS-II we used 500 classes.
We chose thenumber of classes in such a way that the final per-formance of the translation system was optimal.The CPU time for optimization of bilingual wordclasses on an Alpha workstation was under 20 sec-onds for EUTRANS-I and less than two hours forEUTRANS-II.Table 3 provides examples of bilingual wordclasses for the EUTRANS-I corpus.
It can be seenthat the resulting classes often contain words thatare similar in their syntactic and semantic func-tions.
The grouping of words with a different74Proceedings of EACL '99Table 4: Perplexity (PP) of different classes.Corpus MONO BIL BIL-2EUTRANS-I 2.13 197: 198:EUTRANS-II 13.2 .
.Table 5: Average -mirror of different classes.Corpus \[MONO \[BIL BIL-2EUTRANS-I I 3.5 I 2.6 2.6EUTRANS-II 2.2 1.8 2.0meaning like today and tomorrow does not im-ply that these words should be translated by thesame Spanish word, but it does imply that thetranslations of these words are likely to be in thesame Spanish word class.To measure the quality of our bilingual wordclasses we applied two different evaluation mea-sures:1.
Average e-mirror size (Wang et al, 1996):The e-mirror of a class E is the set of classeswhich have a translation probability greaterthan e. We use e = 0.05.2.
The perplexity of the class transition proba-bility on a bilingual test corpus:exp j -1 .
y~ maxi log (p (g (fj) Ig (ei)))j= lBoth measures determine the extent o which thetranslation probability is spread out.
A smallvalue means that the translation probability isvery focused and that the knowledge of the sourcelanguage class provides much information aboutthe target language class.Table 4 shows the perplexity of the obtainedtranslation lexicon without word classes, withmonolingual nd with bilingual word classes.
Asexpected the bilingually optimized classes (BIL,BIL-2) achieve asignificantly ower perplexity anda lower average -mirror than the mono-linguallyoptimized classes (MONO).The tables 6 and 7 show the translation qual-ity of the statistical machine translation systemdescribed in (Och and Weber, 1998) using noclasses (WORD) at all, mono-lingually, and bi-lingually optimized word classes.
The trans-lation system was trained using the bilingualtraining corpus without any further knowledgesources.
Our evaluation criterion is the word er-ror rate (WER) - -  the minimum number of in-Table 6: Word error rate (WER) and averagealignment template l ngth (AATL) on EUTRANS-I.MethodWORDMONOBILBIL-2WER \[70\] I AAq'L6.31 2.855.64 5.035.38 4.404.76 5.19Table 7: Word error rate (WER) and averagealignment template l ngth (AATL) on EUTRANS-II.MethodWORDMONOBILBIL-2WER \[%\] I AATL64.3 1.3663.5 1.7463.2 1.5362.5 1.54sertions/deletions/substitutions relative to a ref-erence translation.As expected the translation quality improvesusing classes.
For the small EuTRANS-I task theword error rates reduce significantly.
The word er-ror rates for the EUTRANS-II task are much largerbecause the task has a very large vocabulary and ismore complex.
The bilingual classes how betterresults than the monolingual c asses MONO.
Oneexplanation for the improvement in translationquality is that the bilingually optimized classesresult in an increased average size of used align-ment templates.
For example the average lengthof alignment templates with the EUTRANS-I cor-pus using WORD is 2.85 and using BIL-2 it is5.19.
The longer the average alignment templatelength, the more context is used in the translationand therefore the translation quality is higher.An explanation for the superiority of BIL-2over BIL is that by first optimizing the Englishclasses mono-lingually, it is much more probablethat longer sequences of classes occur more oftenthereby increasing the average alignment templatesize.6 Summary  and  fu ture  worksBy applying a maximum-likelihood approach tothe joint probability of a parallel corpus we ob-tained an optimization criterion for bilingual wordclasses which is very similar to the one used inmonolingual maximum-likelihood word clustering.For optimization we used the exchange algorithm.The obtained word classes give a low translationlexicon perplexity and improve the quality of sta-75Proceedings ofEACL '99tistical machine translation.We expect improvements in translation qualityby allowing that words occur in more than oneclass and by performing a hierarchical c ustering.Acknowledgements This work has been par-tialIy supported by the European Community un-der the ESPRIT project number 30268 (EuTrans).Re ferencesP.
F. Brown, V. J. Della Pietra, P. V. deSouza,J.
C. Lai, and R. L. Mercer.
1992.
Class-basedn-gram models of natural anguage.
Computa-tional Linguistics, 18(4):467-479.Peter F. Brown, Stephen A. Della Pietra, Vin-cent J. Della Pietra, and Robert L. Mercer.1993.
The mathematics of statistical machinetranslation: Parameter estimation.
Computa-tional Linguistics, 19 (2) :263-311.G.
Dueck and T. Scheuer.
1990.
Threshold ac-cepting: A general purpose optimization al-gorithm appearing superior to simulated an-nealing.
Journal of Computational Physics,90(1):161-175.Pascale Fung and Dekai Wu.
1995.
Coercedmarkov models for cross-lingual lexical-tag re-lations.
In The Sixth International Conferenceon Theoretical and Methodological Issues in Ma-chine Translation, pages 240-255, Leuven, Bel-gium, July.M.
Jardino and G. Adda.
1993.
Automatic WordClassification Using Simulated Annealing.
InProc.
Int.
Conf.
on Acoustics, Speech, and Sig-nal Processing, volume 2, pages 41-44, Min-neapolis.R.
Kneser and H. Ney.
1991.
Forming WordClasses by Statistical Clustering for StatisticalLanguage Modelling.
In 1.
Quantitative Lin-guistics Conference.R.
Kneser and H. Ney.
1993.
Improved Cluster-ing Techniques for Class-Based Statistical Lan-guage Modelling.
In European Conference onSpeech Communication and Technology, pages973-976.Sven Martin, JSrg Liermann, and Hermann Ney.1998.
Algorithms for bigram and trigram wordclustering.
Speech Communication, 24(1):19-37.Franz Josef Och and Hermann Ney.
1999.
Thealignment template approach to statistical ma-chine translation.
To appear.Franz Josef Och and Hans Weber.
1998.
Im-proving statistical natural language translationwith categories and rules.
In Proceedings of the35th Annual Conference of the Association forComputational Linguistics and the 17th Inter-national Conference on Computational Linguis-tics, pages 985-989, Montreal, Canada, August.Enrique Vidal.
1997.
Finite-state speech-to-speech translation.
In Proc.
Int.
Conf.
onAcoustics, Speech, and Signal Processing, vol-ume 1, pages 111-114.Stephan Vogel, Hermann Ney, and Christoph Till-mann.
1996.
HMM-based word alignment instatistical translation.
In COLING '96: The16th Int.
Conf.
on Computational Linguistics,pages 836-841, Copenhagen, August.Ye-Yi Wang, John Laiferty, and Alex Waibel.1996.
Word clustering with parallel spoken lan-guage corpora.
In Proceedings of the ~th Inter-national Conference on Spoken Language Pro-cesing (ICSLP'96), pages 2364-2367.76
