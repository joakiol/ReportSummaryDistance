Proceedings of the 4th International Workshop on Semantic Evaluations (SemEval-2007), pages 81?86,Prague, June 2007. c?2007 Association for Computational LinguisticsSemEval-2007 Task 16: Evaluation of Wide Coverage Knowledge ResourcesMontse CuadrosTALP Research CenterUniversitat Polite?cnica de CatalunyaBarcelona, Spaincuadros@lsi.upc.eduGerman RigauIXA NLP GroupEuskal Herriko UnibersitateaDonostia, Spaingerman.rigau@ehu.esAbstractThis task tries to establish the relative qual-ity of available semantic resources (derivedby manual or automatic means).
The qual-ity of each large-scale knowledge resourceis indirectly evaluated on a Word Sense Dis-ambiguation task.
In particular, we useSenseval-3 and SemEval-2007 English Lex-ical Sample tasks as evaluation bechmarksto evaluate the relative quality of each re-source.
Furthermore, trying to be as neu-tral as possible with respect the knowledgebases studied, we apply systematically thesame disambiguation method to all the re-sources.
A completely different behaviour isobserved on both lexical data sets (Senseval-3 and SemEval-2007).1 IntroductionUsing large-scale knowledge bases, such as Word-Net (Fellbaum, 1998), has become a usual, oftennecessary, practice for most current Natural Lan-guage Processing (NLP) systems.
Even now, build-ing large and rich enough knowledge bases forbroad?coverage semantic processing takes a greatdeal of expensive manual effort involving large re-search groups during long periods of development.In fact, dozens of person-years have been invested inthe development of wordnets for various languages(Vossen, 1998).
For example, in more than ten yearsof manual construction (from version 1.5 to 2.1),WordNet passed from 103,445 semantic relations to245,509 semantic relations1.
That is, around onethousand new relations per month.
But this datadoes not seems to be rich enough to support ad-vanced concept-based NLP applications directly.
Itseems that applications will not scale up to work-ing in open domains without more detailed and richgeneral-purpose (and also domain-specific) seman-tic knowledge built by automatic means.Fortunately, during the last years, the researchcommunity has devised a large set of innovativemethods and tools for large-scale automatic acqui-sition of lexical knowledge from structured and un-structured corpora.
Among others we can men-tion eXtended WordNet (Mihalcea and Moldovan,2001), large collections of semantic preferences ac-quired from SemCor (Agirre and Martinez, 2001;Agirre and Martinez, 2002) or acquired from BritishNational Corpus (BNC) (McCarthy, 2001), large-scale Topic Signatures for each synset acquired fromthe web (Agirre and de la Calle, 2004) or acquiredfrom the BNC (Cuadros et al, 2005).
Obviously,these semantic resources have been acquired using avery different set of methods, tools and corpora, re-sulting on a different set of new semantic relationsbetween synsets (or between synsets and words).Many international research groups are workingon knowledge-based WSD using a wide range of ap-proaches (Mihalcea, 2006).
However, less attentionhas been devoted on analysing the quality of eachsemantic resource.
In fact, each resource presentsdifferent volume and accuracy figures (Cuadros etal., 2006).In this paper, we evaluate those resources on the1Symmetric relations are counted only once.81SemEval-2007 English Lexical Sample task.
Forcomparison purposes, we also include the results ofthe same resources on the Senseval-3 English Lex-ical sample task.
In both cases, we used only thenominal part of both data sets and we also includedsome basic baselines.2 Evaluation FrameworkIn order to compare the knowledge resources, all theresources are evaluated as Topic Signatures (TS).That is, word vectors with weights associated to aparticular synset.
Normally, these word vectors areobtained by collecting from the resource under studythe word senses appearing as direct relatives.
Thissimple representation tries to be as neutral as possi-ble with respect to the resources studied.A common WSD method has been applied toall knowledge resources on the test examples ofSenseval-3 and SemEval-2007 English lexical sam-ple tasks.
A simple word overlapping counting isperformed between the Topic Signature and the testexample.
The synset having higher overlappingword counts is selected.
In fact, this is a very sim-ple WSD method which only considers the topicalinformation around the word to be disambiguated.Finally, we should remark that the results are notskewed (for instance, for resolving ties) by the mostfrequent sense in WN or any other statistically pre-dicted knowledge.As an example, table 1 shows a test example ofSemEval-2007 corresponding to the first sense of thenoun capital.
In bold there are the words that appearin its corresponding Topic Signature acquired fromthe web.Note that although there are several importantrelated words, the WSD process implements ex-act word form matching (no preprocessing is per-formed).2.1 Basic BaselinesWe have designed a number of basic baselines inorder to establish a complete evaluation frameworkfor comparing the performance of each semantic re-source on the English WSD tasks.RANDOM: For each target word, this method se-lects a random sense.
This baseline can be consid-ered as a lower-bound.Baselines P R F1TRAIN 65.1 65.1 65.1TRAIN-MFS 54.5 54.5 54.5WN-MFS 53.0 53.0 53.0SEMCOR-MFS 49.0 49.1 49.0RANDOM 19.1 19.1 19.1Table 2: P, R and F1 results for English Lexical Sam-ple Baselines of Senseval-3SemCor MFS (SEMCOR-MFS): This methodselects the most frequent sense of the target wordin SemCor.WordNet MFS (WN-MFS): This method selectsthe first sense in WN1.6 of the target word.TRAIN-MFS: This method selects the most fre-quent sense in the training corpus of the target word.Train Topic Signatures (TRAIN): This baselineuses the training corpus to directly build a Topic Sig-nature using TFIDF measure for each word sense.Note that this baseline can be considered as anupper-bound of our evaluation.Table 2 presents the precision (P), recall (R) andF1 measure (harmonic mean of recall and preci-sion) of the different baselines in the English LexicalSample exercise of Senseval-3.
In this table, TRAINhas been calculated with a vector size of at maxi-mum 450 words.
As expected, RANDOM baselineobtains the poorest result.
The most frequent sensesobtained from SemCor (SEMCOR-MFS) and WN(WN-MFS) are both below the most frequent senseof the training corpus (TRAIN-MFS).
However, allof them are far below the Topic Signatures acquiredusing the training corpus (TRAIN).Table 3 presents the precision (P), recall (R) andF1 measure (harmonic mean of recall and preci-sion) of the different baselines in the English LexicalSample exercise of SemEval-2007.
Again, TRAINhas been calculated with a vector size of at max-imum 450 words.
As before, RANDOM baselineobtains the poorest result.
The most frequent sensesobtained from SemCor (SEMCOR-MFS) and WN(WN-MFS) are both far below the most frequentsense of the training corpus (TRAIN-MFS), and allof them are below the Topic Signatures acquired us-ing the training corpus (TRAIN).Comparing both lexical sample sets, SemEval-2007 data appears to be more skewed and simple forWSD systems than the data set from Senseval-3: less82<instance id=?19:0@11@wsj/01/wsj 0128@wsj@en@on?
docsrc=?wsj?> <context>?
A sweeping restructuring of the industry is possible .
?
Standard & Poor ?s Corp. says First Boston , Shearsonand Drexel Burnham Lambert Inc. , in particular , are likely to have difficulty shoring up their credit standing inmonths ahead .
What worries credit-rating concerns the most is that Wall Street firms are taking long-term riskswith their own <head> capital </head> via leveraged buy-out and junk bond financings .
That ?s a departure fromtheir traditional practice of transferring almost all financing risks to investors .
Whereas conventional securitiesfinancings are structured to be sold quickly , Wall Street ?s new penchant for leveraged buy-outs and junk bonds isresulting in long-term lending commitments that stretch out for months or years .</context> </instance>Table 1: Example of test id for capital#n which its correct sense is 1Baselines P R F1TRAIN 87.6 87.6 87.6TRAIN-MFS 81.2 79.6 80.4WN-MFS 66.2 59.9 62.9SEMCOR-MFS 42.4 38.4 40.3RANDOM 27.4 27.4 27.4Table 3: P, R and F1 results for English Lexical Sam-ple Baselines of SemEval-2007polysemous (as shown by the RANDOM baseline),less similar than SemCor word sense frequency dis-tributions (as shown by SemCor-MFS), more simi-lar to the first sense of WN (as shown by WN-MFS),much more skewed to the first sense of the trainingcorpus (as shown by TRAIN-MFS), and much moreeasy to be learned (as shown by TRAIN).3 Large scale knowledge ResourcesThe evaluation presented here covers a wide rangeof large-scale semantic resources: WordNet (WN)(Fellbaum, 1998), eXtended WordNet (Mihalceaand Moldovan, 2001), large collections of seman-tic preferences acquired from SemCor (Agirre andMartinez, 2001; Agirre and Martinez, 2002) or ac-quired from the BNC (McCarthy, 2001), large-scaleTopic Signatures for each synset acquired from theweb (Agirre and de la Calle, 2004) or SemCor (Lan-des et al, 2006).Although these resources have been derived us-ing different WN versions, using the technology forthe automatic alignment of wordnets (Daude?
et al,2003), most of these resources have been integratedinto a common resource called Multilingual Cen-tral Repository (MCR) (Atserias et al, 2004) main-taining the compatibility among all the knowledgeresources which use a particular WN version as asense repository.
Furthermore, these mappings al-low to port the knowledge associated to a particularWN version to the rest of WN versions.The current version of the MCR contains 934,771semantic relations between synsets, most of themacquired by automatic means.
This represents al-most four times larger than the Princeton WordNet(245,509 unique semantic relations in WordNet 2.1).Hereinafter we will refer to each semantic re-source as follows:WN (Fellbaum, 1998): This resource uses thedirect relations encoded in WN1.6 or WN2.0 (forinstance, tree#n#1?hyponym?>teak#n#2).
We alsotested WN2 (using relations at distances 1 and 2),WN3 (using relations at distances 1 to 3) and WN4(using relations at distances 1 to 4).XWN (Mihalcea and Moldovan, 2001): This re-source uses the direct relations encoded in eXtendedWN (for instance, teak#n#2?gloss?>wood#n#1).WN+XWN: This resource uses the direct rela-tions included in WN and XWN.
We also tested(WN+XWN)2 (using either WN or XWN relationsat distances 1 and 2, for instance, tree#n#1?related?>wood#n#1).spBNC (McCarthy, 2001): This resource contains707,618 selectional preferences acquired for sub-jects and objects from BNC.spSemCor (Agirre and Martinez, 2002): This re-source contains the selectional preferences acquiredfor subjects and objects from SemCor (for instance,read#v#1?tobj?>book#n#1).MCR (Atserias et al, 2004): This resourceuses the direct relations included in MCR but ex-cluding spBNC because of its poor performance.Thus, MCR contains the direct relations fromWN (as tree#n#1?hyponym?>teak#n#2), XWN(as teak#n#2?gloss?>wood#n#1), and spSemCor(as read#v#1?tobj?>book#n#1) but not the indi-83Source #relationsPrinceton WN1.6 138,091Selectional Preferences from SemCor 203,546New relations from Princeton WN2.0 42,212Gold relations from eXtended WN 17,185Silver relations from eXtended WN 239,249Normal relations from eXtended WN 294,488Total 934,771Table 4: Semantic relations uploaded in the MCRrect relations of (WN+XWN)2 (tree#n#1?related?>wood#n#1).
We also tested MCR2 (using rela-tions at distances 1 and 2), which also integrates(WN+XWN)2 relations.Table 4 shows the number of semantic relationsbetween synset pairs in the MCR.3.1 Topic SignaturesTopic Signatures (TS) are word vectors related to aparticular topic (Lin and Hovy, 2000).
Topic Signa-tures are built by retrieving context words of a targettopic from large corpora.
In our case, we considerword senses as topics.For this study, we use two different large-scaleTopic Signatures.
The first constitutes one of thelargest available semantic resource with around 100million relations (between synsets and words) ac-quired from the web (Agirre and de la Calle, 2004).The second has been derived directly from SemCor.TSWEB2: Inspired by the work of (Leacock etal., 1998), these Topic Signatures were constructedusing monosemous relatives from WordNet (syn-onyms, hypernyms, direct and indirect hyponyms,and siblings), querying Google and retrieving up toone thousand snippets per query (that is, a wordsense), extracting the words with distinctive fre-quency using TFIDF.
For these experiments, weused at maximum the first 700 words of each TS.TSSEM: These Topic Signatures have been con-structed using the part of SemCor having all wordstagged by PoS, lemmatized and sense tagged ac-cording to WN1.6 totalizing 192,639 words.
Foreach word-sense appearing in SemCor, we gatherall sentences for that word sense, building a TS us-ing TFIDF for all word-senses co-occurring in thosesentences.2http://ixa.si.ehu.es/Ixa/resources/sensecorpuspolitical party#n#1 2.3219party#n#1 2.3219election#n#1 1.0926nominee#n#1 0.4780candidate#n#1 0.4780campaigner#n#1 0.4780regime#n#1 0.3414identification#n#1 0.3414government#n#1 0.3414designation#n#3 0.3414authorities#n#1 0.3414Table 5: Topic Signatures for party#n#1 obtainedfrom Semcor (11 out of 719 total word senses).In table 5, there is an example of the first word-senses we calculate from party#n#1.The total number of relations between WNsynsets acquired from SemCor is 932,008.4 Evaluating each resourceTable 6 presents ordered by F1 measure, the perfor-mance of each knowledge resource on Senseval-3and the average size of the TS per word-sense.
Theaverage size of the TS per word-sense is the numberof words associated to a synset on average.
Obvi-ously, the best resources would be those obtainingbetter performances with a smaller number of asso-ciated words per synset.
The best results for preci-sion, recall and F1 measures are shown in bold.
Wealso mark in italics those resources using non-directrelations.Surprisingly, the best results are obtained byTSSEM (with F1 of 52.4).
The lowest result is ob-tained by the knowledge directly gathered from WNmainly because of its poor coverage (R of 18.4 andF1 of 26.1).
Also interesting, is that the knowledgeintegrated in the MCR although partly derived byautomatic means performs much better in terms ofprecision, recall and F1 measures than using themseparately (F1 with 18.4 points higher than WN, 9.1than XWN and 3.7 than spSemCor).Despite its small size, the resources derived fromSemCor obtain better results than its counterpartsusing much larger corpora (TSSEM vs. TSWEB andspSemCor vs. spBNC).Regarding the basic baselines, all knowledge re-sources surpass RANDOM, but none achieves nei-ther WN-MFS, TRAIN-MFS nor TRAIN.
Only84KB P R F1 Av.
SizeTSSEM 52.5 52.4 52.4 103MCR2 45.1 45.1 45.1 26,429MCR 45.3 43.7 44.5 129spSemCor 43.1 38.7 40.8 56(WN+XWN)2 38.5 38.0 38.3 5,730WN+XWN 40.0 34.2 36.8 74TSWEB 36.1 35.9 36.0 1,721XWN 38.8 32.5 35.4 69WN3 35.0 34.7 34.8 503WN4 33.2 33.1 33.2 2,346WN2 33.1 27.5 30.0 105spBNC 36.3 25.4 29.9 128WN 44.9 18.4 26.1 14Table 6: P, R and F1 fine-grained results for theresources evaluated individually at Senseval-03 En-glish Lexical Sample Task.TSSEM obtains better results than SEMCOR-MFSand is very close to the most frequent sense of WN(WN-MFS) and the training (TRAIN-MFS).Table 7 presents ordered by F1 measure, the per-formance of each knowledge resource on SemEval-2007 and its average size of the TS per word-sense3.The best results for precision, recall and F1 mea-sures are shown in bold.
We also mark in italicsthose resources using non-direct relations.Interestingly, on SemEval-2007, all the knowl-edge resources behave differently.
Now, the bestresults are obtained by (WN+XWN)2 (with F1 of52.9), followed by TSWEB (with F1 of 51.0).
Thelowest result is obtained by the knowledge encodedin spBNC mainly because of its poor precision (P of24.4 and F1 of 20.8).Regarding the basic baselines, spBNC, WN (andalso WN2 and WN4) and spSemCor do not sur-pass RANDOM, and none achieves neither WN-MFS, TRAIN-MFS nor TRAIN.
Now, WN+XWN,XWN, TSWEB and (WN+XWN)2 obtain better re-sults than SEMCOR-MFS but far below the mostfrequent sense of WN (WN-MFS) and the training(TRAIN-MFS).5 Combination of Knowledge ResourcesIn order to evaluate deeply the contribution of eachknowledge resource, we also provide some resultsof the combined outcomes of several resources.
The3The average size is different with respect Senseval-3 be-cause the words selected for this task are differentKB P R F1 Av.
Size(WN+XWN)2 54.9 51.1 52.9 5,153TSWEB 54.8 47.8 51.0 700XWN 50.1 39.8 44.4 96WN+XWN 45.4 36.8 40.7 101MCR 40.2 35.5 37.7 149TSSEM 35.1 32.7 33.9 428MCR2 32.4 29.5 30.9 24,896WN3 29.3 26.3 27.7 584WN2 25.9 27.4 26.6 72spSemCor 31.4 23.0 26.5 51.0WN4 26.1 23.9 24.9 2,710WN 36.8 16.1 22.4 13spBNC 24.4 18.1 20.8 290Table 7: P, R and F1 fine-grained results for theresources evaluated individually at SemEval-2007,English Lexical Sample Task .KB RankMCR+(WN+XWN)2+TSWEB+TSSEM 55.5Table 8: F1 fine-grained results for the 4 system-combinations on Senseval-3combinations are performed following a very basicstrategy (Brody et al, 2006).Rank-Based Combination (Rank): Each se-mantic resource provides a ranking of senses of theword to be disambiguated.
For each sense, its place-ments according to each of the methods are summedand the sense with the lowest total placement (clos-est to first place) is selected.Table 8 presents the F1 measure result with re-spect this method when combining four different se-mantic resources on the Senseval-3 test set.Regarding the basic baselines, this combinationoutperforms the most frequent sense of SemCor(SEMCOR-MFS with F1 of 49.1), WN (WN-MFSwith F1 of 53.0) and, the training data (TRAIN-MFSwith F1 of 54.5).Table 9 presents the F1 measure result with re-spect the rank mthod when combining the same fourdifferent semantic resources on the SemEval-2007test set.KB RankMCR+(WN+XWN)2+TSWEB+TSSEM 38.9Table 9: F1 fine-grained results for the 4 system-combinations on SemEval-200785In this case, the combination of the four resourcesobtains much lower result.
Regarding the baselines,this combination performs lower than the most fre-quent senses from SEMCOR, WN or the trainingdata.
This could be due to the poor individual per-formance of the knowledge derived from SemCor(spSemCor, TSSEM and MCR, which integratesspSemCor).
Possibly, in this case, the knowledgecomming from SemCor is counterproductive.
Inter-estingly, the knowledge derived from other sources(XWN from WN glosses and TSWEB from theweb) seems to be more robust with respect corpuschanges.6 ConclusionsAlthough this task had no participants, we providethe performances of a large set of knowledge re-sources on two different test sets: Senseval-3 andSemEval-2007 English Lexical Sample task.
Wealso provide the results of a system combination offour large-scale semantic resources.
When evalu-ated on Senseval-3, the combination of knowledgesources surpass the most-frequent classifiers.
How-ever, a completely different behaviour is observedon SemEval-2007 data test.
In fact, both corporapresent very different characteristics.
The resultsshow that some resources seems to be less depen-dant than others to corpus changes.Obviously, these results suggest that much moreresearch on acquiring, evaluating and using large-scale semantic resources should be addressed.7 AcknowledgementsWe want to thank the valuable comments of theanonymous reviewers.
This work has been partiallysupported by the projects KNOW (TIN2006-15049-C03-01) and ADIMEN (EHU06/113).ReferencesE.
Agirre and O. Lopez de la Calle.
2004.
Publicly avail-able topic signatures for all wordnet nominal senses.In Proceedings of LREC, Lisbon, Portugal.E.
Agirre and D. Martinez.
2001.
Learning class-to-classselectional preferences.
In Proceedings of CoNLL,Toulouse, France.E.
Agirre and D. Martinez.
2002.
Integrating selectionalpreferences in wordnet.
In Proceedings of GWC,Mysore, India.J.
Atserias, L. Villarejo, G. Rigau, E. Agirre, J. Car-roll, B. Magnini, and Piek Vossen.
2004.
The mean-ing multilingual central repository.
In Proceedings ofGWC, Brno, Czech Republic.S.
Brody, R. Navigli, and M. Lapata.
2006.
Ensem-ble methods for unsupervised wsd.
In Proceedings ofCOLING-ACL, pages 97?104.M.
Cuadros, L.
Padro?, and G. Rigau.
2005.
Comparingmethods for automatic acquisition of topic signatures.In Proceedings of RANLP, Borovets, Bulgaria.M.
Cuadros, L.
Padro?, and G. Rigau.
2006.
An empiricalstudy for automatic acquisition of topic signatures.
InProceedings of GWC, pages 51?59.J.
Daude?, L.
Padro?, and G. Rigau.
2003.
Validation andTuning of Wordnet Mapping Techniques.
In Proceed-ings of RANLP, Borovets, Bulgaria.C.
Fellbaum, editor.
1998.
WordNet.
An Electronic Lexi-cal Database.
The MIT Press.S.
Landes, C. Leacock, and R. Tengi.
2006.
Build-ing a semantic concordance of english.
In WordNet:An electronic lexical database and some applications.MIT Press, Cambridge,MA., 1998, pages 97?104.C.
Leacock, M. Chodorow, and G. Miller.
1998.
Us-ing Corpus Statistics and WordNet Relations for SenseIdentification.
Computational Linguistics, 24(1):147?166.C.
Lin and E. Hovy.
2000.
The automated acquisition oftopic signatures for text summarization.
In Proceed-ings of COLING.
Strasbourg, France.D.
McCarthy.
2001.
Lexical Acquisition at the Syntax-Semantics Interface: Diathesis Aternations, Subcate-gorization Frames and Selectional Preferences.
Ph.D.thesis, University of Sussex.R.
Mihalcea and D. Moldovan.
2001. extended wordnet:Progress report.
In Proceedings of NAACL Workshopon WordNet and Other Lexical Resources, Pittsburgh,PA.R.
Mihalcea.
2006.
Knowledge based methods for wordsense disambiguation.
In E. Agirre and P.
Edmonds(Eds.)
Word Sense Disambiguation: Algorithms andapplications., volume 33 of Text, Speech and Lan-guage Technology.
Springer.P.
Vossen, editor.
1998.
EuroWordNet: A MultilingualDatabase with Lexical Semantic Networks .
KluwerAcademic Publishers .86
