Proceedings of the 12th Conference of the European Chapter of the ACL, pages 380?388,Athens, Greece, 30 March ?
3 April 2009. c?2009 Association for Computational LinguisticsRule Filtering by Pattern for Efficient Hierarchical TranslationGonzalo Iglesias?
Adria` de Gispert??
University of Vigo.
Dept.
of Signal Processing and Communications.
Vigo, Spain{giglesia,erbanga}@gts.tsc.uvigo.es?
University of Cambridge.
Dept.
of Engineering.
CB2 1PZ Cambridge, U.K.{ad465,wjb31}@eng.cam.ac.ukEduardo R. Banga?
William Byrne?AbstractWe describe refinements to hierarchicaltranslation search procedures intended toreduce both search errors and memory us-age through modifications to hypothesisexpansion in cube pruning and reductionsin the size of the rule sets used in transla-tion.
Rules are put into syntactic classesbased on the number of non-terminals andthe pattern, and various filtering strate-gies are then applied to assess the impacton translation speed and quality.
Resultsare reported on the 2008 NIST Arabic-to-English evaluation task.1 IntroductionHierarchical phrase-based translation (Chiang,2005) has emerged as one of the dominant cur-rent approaches to statistical machine translation.Hiero translation systems incorporate many ofthe strengths of phrase-based translation systems,such as feature-based translation and strong tar-get language models, while also allowing flexi-ble translation and movement based on hierarchi-cal rules extracted from aligned parallel text.
Theapproach has been widely adopted and reported tobe competitive with other large-scale data drivenapproaches, e.g.
(Zollmann et al, 2008).Large-scale hierarchical SMT involves auto-matic rule extraction from aligned parallel text,model parameter estimation, and the use of cubepruning k-best list generation in hierarchical trans-lation.
The number of hierarchical rules extractedfar exceeds the number of phrase translations typ-ically found in aligned text.
While this may leadto improved translation quality, there is also therisk of lengthened translation times and increasedmemory usage, along with possible search errorsdue to the pruning procedures needed in search.We describe several techniques to reduce mem-ory usage and search errors in hierarchical trans-lation.
Memory usage can be reduced in cubepruning (Chiang, 2007) through smart memoiza-tion, and spreading neighborhood exploration canbe used to reduce search errors.
However, searcherrors can still remain even when implementingsimple phrase-based translation.
We describe a?shallow?
search through hierarchical rules whichgreatly speeds translation without any effect onquality.
We then describe techniques to analyzeand reduce the set of hierarchical rules.
We dothis based on the structural properties of rules anddevelop strategies to identify and remove redun-dant or harmful rules.
We identify groupings ofrules based on non-terminals and their patterns andassess the impact on translation quality and com-putational requirements for each given rule group.We find that with appropriate filtering strategiesrule sets can be greatly reduced in size without im-pact on translation performance.1.1 Related WorkThe search and rule pruning techniques describedin the following sections add to a growing lit-erature of refinements to the hierarchical phrase-based SMT systems originally described by Chi-ang (2005; 2007).
Subsequent work has addressedimprovements and extensions to the search proce-dure itself, the extraction of the hierarchical rulesneeded for translation, and has also reported con-trastive experiments with other SMT architectures.Hiero Search Refinements Huang and Chiang(2007) offer several refinements to cube pruningto improve translation speed.
Venugopal et al(2007) introduce a Hiero variant with relaxed con-straints for hypothesis recombination during pars-ing; speed and results are comparable to those ofcube pruning, as described by Chiang (2007).
Liand Khudanpur (2008) report significant improve-ments in translation speed by taking unseen n-grams into account within cube pruning to mini-mize language model requests.
Dyer et al (2008)380extend the translation of source sentences to trans-lation of input lattices following Chappelier et al(1999).Extensions to Hiero Blunsom et al (2008)discuss procedures to combine discriminative la-tent models with hierarchical SMT.
The Syntax-Augmented Machine Translation system (Zoll-mann and Venugopal, 2006) incorporates targetlanguage syntactic constituents in addition to thesynchronous grammars used in translation.
Shenat al.
(2008) make use of target dependency treesand a target dependency language model duringdecoding.
Marton and Resnik (2008) exploit shal-low correspondences of hierarchical rules withsource syntactic constituents extracted from par-allel text, an approach also investigated by Chiang(2005).
Zhang and Gildea (2006) propose bina-rization for synchronous grammars as a means tocontrol search complexity arising from more com-plex, syntactic, hierarchical rules sets.Hierarchical rule extraction Zhang et al (2008)describe a linear algorithm, a modified version ofshift-reduce, to extract phrase pairs organized intoa tree from which hierarchical rules can be directlyextracted.
Lopez (2007) extracts rules on-the-flyfrom the training bitext during decoding, search-ing efficiently for rule patterns using suffix arrays.Analysis and Contrastive Experiments Zollmanet al (2008) compare phrase-based, hierarchicaland syntax-augmented decoders for translation ofArabic, Chinese, and Urdu into English, and theyfind that attempts to expedite translation by simpleschemes which discard rules also degrade transla-tion performance.
Lopez (2008) explores whetherlexical reordering or the phrase discontiguity in-herent in hierarchical rules explains improvementsover phrase-based systems.
Hierarchical transla-tion has also been used to great effect in combina-tion with other translation architectures (e.g.
(Simet al, 2007; Rosti et al, 2007)).1.2 OutlineThe paper proceeds as follows.
Section 2 de-scribes memoization and spreading neighborhoodexploration in cube pruning intended to reducememory usage and search errors, respectively.
Adetailed comparison with a simple phrase-basedsystem is presented.
Section 3 describes pattern-based rule filtering and various procedures to se-lect rule sets for use in translation with an aimto improving translation quality while minimizingrule set size.
Finally, Section 4 concludes.2 Two Refinements in Cube PruningChiang (2007) introduced cube pruning to applylanguage models in pruning during the generationof k-best translation hypotheses via the applicationof hierarchical rules in the CYK algorithm.
In theimplementation of Hiero described here, there isthe parser itself, for which we use a variant of theCYK algorithm closely related to CYK+ (Chap-pelier and Rajman, 1998); it employs hypothesisrecombination, without pruning, while maintain-ing back pointers.
Before k-best list generationwith cube pruning, we apply a smart memoiza-tion procedure intended to reduce memory con-sumption during k-best list expansion.
Within thecube pruning algorithm we use spreading neigh-borhood exploration to improve robustness in theface of search errors.2.1 Smart MemoizationEach cell in the chart built by the CYK algorithmcontains all possible derivations of a span of thesource sentence being translated.
After the parsingstage is completed, it is possible to make a very ef-ficient sweep through the backpointers of the CYKgrid to count how many times each cell will be ac-cessed by the k-best generation algorithm.
Whenk-best list generation is running, the number oftimes each cell is visited is logged so that, as eachcell is visited for the last time, the k-best list as-sociated with each cell is deleted.
This continuesuntil the one k-best list remaining at the top of thechart spans the entire sentence.
Memory reduc-tions are substantial for longer sentences: for thelongest sentence in the tuning set described later(105 words in length), smart memoization reducesmemory usage during the cube pruning stage from2.1GB to 0.7GB.
For average length sentences ofapprox.
30 words, memory reductions of 30% aretypical.2.2 Spreading Neighborhood ExplorationIn generation of a k-best list of translations fora source sentence span, every derivation is trans-formed into a cube containing the possible trans-lations arising from that derivation, along withtheir translation and language model scores (Chi-ang, 2007).
These derivations may contain non-terminals which must be expanded based on hy-potheses generated by lower cells, which them-381HIERO MJ1 HIERO HIERO SHALLOWX ?
?V2V1,V1V2?
X ?
??,??
X ?
?
?s,?s?X ?
?V ,V ?
?, ?
?
({X} ?T)+ X ?
?V ,V ?V ?
?s,t?
V ?
?s,t?s, t ?
T+ s, t ?
T+; ?s, ?s ?
({V } ?
T)+Table 1: Hierarchical grammars (not including glue rules).
T is the set of terminals.selves may contain non-terminals.
For efficiencyeach cube maintains a queue of hypotheses, calledhere the frontier queue, ranked by translation andlanguage model score; it is from these frontierqueues that hypotheses are removed to create thek-best list for each cell.
When a hypothesis is ex-tracted from a frontier queue, that queue is updatedby searching through the neighborhood of the ex-tracted item to find novel hypotheses to add; if nonovel hypotheses are found, that queue necessar-ily shrinks.
This shrinkage can lead to search er-rors.
We therefore require that, when a hypothe-sis is removed, new candidates must be added byexploring a neighborhood which spreads from thelast extracted hypothesis.
Each axis of the cubeis searched (here, to a depth of 20) until a novelhypothesis is found.
In this way, up to three newcandidates are added for each entry extracted froma frontier queue.Chiang (2007) describes an initialization pro-cedure in which these frontier queues are seededwith a single candidate per axis; we initialize eachfrontier queue to a depth of bNnt+1, where Nnt isthe number of non-terminals in the derivation andb is a search parameter set throughout to 10.
Bystarting with deep frontier queues and by forcingthem to grow during search we attempt to avoidsearch errors by ensuring that the universe of itemswithin the frontier queues does not decrease as thek-best lists are filled.2.3 A Study of Hiero Search Errors inPhrase-Based TranslationExperiments reported in this paper are basedon the NIST MT08 Arabic-to-English transla-tion task.
Alignments are generated over all al-lowed parallel data, (?150M words per language).Features extracted from the alignments and usedin translation are in common use: target lan-guage model, source-to-target and target-to-sourcephrase translation models, word and rule penalties,number of usages of the glue rule, source-to-targetand target-to-source lexical models, and three ruleFigure 1: Spreading neighborhood explorationwithin a cube, just before and after extractionof the item C. Grey squares represent the fron-tier queue; black squares are candidates alreadyextracted.
Chiang (2007) would only consideradding items X to the frontier queue, so the queuewould shrink.
Spreading neighborhood explo-ration adds candidates S to the frontier queue.count features inspired by Bender et al (2007).MET (Och, 2003) iterative parameter estimationunder IBM BLEU is performed on the develop-ment set.
The English language used model is a4-gram estimated over the parallel text and a 965million word subset of monolingual data from theEnglish Gigaword Third Edition.
In addition to theMT08 set itself, we use a development set mt02-05-tune formed from the odd numbered sentencesof the NIST MT02 through MT05 evaluation sets;the even numbered sentences form the validationset mt02-05-test.
The mt02-05-tune set has 2,075sentences.We first compare the cube pruning decoder tothe TTM (Kumar et al, 2006), a phrase-basedSMT system implemented with Weighted Finite-State Tansducers (Allauzen et al, 2007).
The sys-tem implements either a monotone phrase ordertranslation, or an MJ1 (maximum phrase jump of1) reordering model (Kumar and Byrne, 2005).Relative to the complex movement and translationallowed by Hiero and other models, MJ1 is clearlyinferior (Dreyer et al, 2007); MJ1 was developedwith efficiency in mind so as to run with a mini-mum of search errors in translation and to be eas-ily and exactly realized via WFSTs.
Even for the382large models used in an evaluation task, the TTMsystem is reported to run largely without pruning(Blackwood et al, 2008).The Hiero decoder can easily be made toimplement MJ1 reordering by allowing only arestricted set of reordering rules in addition tothe usual glue rule, as shown in left-hand columnof Table 1, where T is the set of terminals.Constraining Hiero in this way makes it possibleto compare its performance to the exact WFSTTTM implementation and to identify any searcherrors made by Hiero.Table 2 shows the lowercased IBM BLEUscores obtained by the systems for mt02-05-tunewith monotone and reordered search, and withMET-optimised parameters for MJ1 reordering.For Hiero, an N-best list depth of 10,000 is usedthroughout.
In the monotone case, all phrase-based systems perform similarly although Hierodoes make search errors.
For simple MJ1 re-ordering, the basic Hiero search procedure makesmany search errors and these lead to degradationsin BLEU.
Spreading neighborhood expansion re-duces the search errors and improves BLEU scoresignificantly but search errors remain a problem.Search errors are even more apparent after MET.This is not surprising, given that mt02-05-tune isthe set over which MET is run: MET drives up thelikelihood of good hypotheses at the expense ofpoor hypotheses, but search errors often increasedue to the expanded dynamic range of the hypoth-esis scores.Our aim in these experiments was to demon-strate that spreading neighborhood exploration canaid in avoiding search errors.
We emphasize thatwe are not proposing that Hiero should be used toimplement reordering models such as MJ1 whichwere created for completely different search pro-cedures (e.g.
WFST composition).
However theseexperiments do suggest that search errors may bean issue, particularly as the search space growsto include the complex long-range movement al-lowed by the hierarchical rules.
We next studyvarious filtering procedures to reduce hierarchi-cal rule sets to find a balance between translationspeed, memory usage, and performance.3 Rule Filtering by PatternHierarchical rules X ?
??,??
are composed ofsequences of terminals and non-terminals, whichMonotone MJ1 MJ1+METBLEU SE BLEU SE BLEU SEa 44.7 - 47.2 - 49.1 -b 44.5 342 46.7 555 48.4 822c 44.7 77 47.1 191 48.9 360Table 2: Phrase-based TTM and Hiero perfor-mance on mt02-05-tune for TTM (a), Hiero (b),Hiero with spreading neighborhood exploration(c).
SE is the number of Hiero hypotheses withsearch errors.we call elements.
In the source, a maximum oftwo non-adjacent non-terminals is allowed (Chi-ang, 2007).
Leaving aside rules without non-terminals (i.e.
phrase pairs as used in phrase-based translation), rules can be classed by theirnumber of non-terminals, Nnt, and their numberof elements, Ne.
There are 5 possible classes:Nnt.Ne= 1.2, 1.3, 2.3, 2.4, 2.5.During rule extraction we search each class sep-arately to control memory usage.
Furthermore, weextract from alignments only those rules which arerelevant to our given test set; for computation ofbackward translation probabilities we log generalcounts of target-side rules but discard unneededrules.
Even with this restriction, our initial rulesetfor mt02-05-tune exceeds 175M rules, of whichonly 0.62M are simple phrase pairs.The question is whether all these rules areneeded for translation.
If the rule set can be re-duced without reducing translation quality, bothmemory efficiency and translation speed can beincreased.
Previously published approaches to re-ducing the rule set include: enforcing a mini-mum span of two words per non-terminal (Lopez,2008), which would reduce our set to 115M rules;or a minimum count (mincount) threshold (Zoll-mann et al, 2008), which would reduce our setto 78M (mincount=2) or 57M (mincount=3) rules.Shen et al (2008) describe the result of filter-ing rules by insisting that target-side rules arewell-formed dependency trees.
This reduces theirrule set from 140M to 26M rules.
This filteringleads to a degradation in translation performance(see Table 2 of Shen et al (2008)), which theycounter by adding a dependency LM in translation.As another reference point, Chiang (2007) reportsChinese-to-English translation experiments basedon 5.5M rules.Zollmann et al (2008) report that filtering rules383en masse leads to degradation in translation per-formance.
Rather than apply a coarse filtering,such as a mincount for all rules, we follow a moresyntactic approach and further classify our rulesaccording to their pattern and apply different fil-ters to each pattern depending on its value in trans-lation.
The premise is that some patterns are moreimportant than others.3.1 Rule PatternsClass Rule PatternNnt.Ne ?source , target?
Types?wX1 , wX1?
11850281.2 ?wX1 , wX1w?
153130?wX1 , X1w?
978891.3 ?wX1w , wX1w?
32903522?wX1w , wX1?
9895402.3 ?X1wX2 , X1wX2?
1554656?X2wX1 , X1wX2?
39163?wX1wX2 , wX1wX2?
26901823?X1wX2w , X1wX2w?
260539692.4 ?wX1wX2 , wX1wX2w?
2534510?wX2wX1 , wX1wX2?
349176?X2wX1w , X1wX2w?
259459?wX1wX2w , wX1wX2w?
61704299?wX1wX2w , wX1X2w?
31495162.5 ?wX1wX2w , X1wX2w?
2330797?wX2wX1w , wX1wX2w?
275810?wX2wX1w , wX1X2w?
205801Table 3: Hierarchical rule patterns classed bynumber of non-terminals, Nnt, number of ele-ments Ne, source and target patterns, and types inthe rule set extracted for mt02-05-tune.Given a rule set, we define source patterns andtarget patterns by replacing every sequence ofnon-terminals by a single symbol ?w?
(indicatingword, i.e.
terminal string, w ?
T+).
Each hierar-chical rule has a unique source and target patternwhich together define the rule pattern.By ignoring the identity and the number of ad-jacent terminals, the rule pattern represents a nat-ural generalization of any rule, capturing its struc-ture and the type of reordering it encodes.
In to-tal, there are 66 possible rule patterns.
Table 3presents a few examples extracted for mt02-05-tune, showing that some patterns are much morediverse than others.
For example, patterns withtwo non-terminals (Nnt=2) are richer than pat-terns with Nnt=1, as they cover many more dis-tinct rules.
Additionally, patterns with two non-terminals which also have a monotonic relation-ship between source and target non-terminals aremuch more diverse than their reordered counter-parts.Some examples of extracted rules and their cor-responding pattern follow, where Arabic is shownin Buckwalter encoding.Pattern ?wX1 , wX1w?
:?w+ qAl X1 , the X1said?Pattern ?wX1w , wX1?
:?fy X1kAnwn Al>wl , on december X1?Pattern ?wX1wX2 , wX1wX2w?
:?Hl X1lAzmp X2 , a X1solution to the X2crisis?3.2 Building an Initial Rule SetWe describe a greedy approach to building a ruleset in which rules belonging to a pattern are addedto the rule set guided by the improvements theyyield on mt02-05-tune relative to the monotoneHiero system described in the previous section.We find that certain patterns seem not to con-tribute to any improvement.
This is particularlysignificant as these patterns often encompass largenumbers of rules, as with patterns with match-ing source and target patterns.
For instance, wefound no improvement when adding the pattern?X1w,X1w?, of which there were 1.2M instances(Table 3).
Since concatenation is already possibleunder the general glue rule, rules with this patternare redundant.
By contrast, the much less frequentreordered counterpart, i.e.
the ?wX1,X1w?
pat-tern (0.01M instances), provides substantial gains.The situation is analogous for rules with two non-terminals (Nnt=2).Based on exploratory analyses (not reportedhere, for space) an initial rule set was built byexcluding patterns reported in Table 4.
In to-tal, 171.5M rules are excluded, for a remainingset of 4.2M rules, 3.5M of which are hierarchi-cal.
We acknowledge that adding rules in this way,by greedy search, is less than ideal and inevitablyraises questions with respect to generality and re-peatability.
However in our experience this is arobust approach, mainly because the initial trans-lation system runs very fast; it is possible to runmany exploratory experiments in a short time.384Excluded Rules Typesa ?X1w,X1w?
, ?wX1,wX1?
2332604b ?X1wX2,??
2121594?X1wX2w,X1wX2w?
,c ?wX1wX2,wX1wX2?52955792d ?wX1wX2w,??
69437146e Nnt.Ne= 1.3 w mincount=5 32394578f Nnt.Ne= 2.3 w mincount=5 166969g Nnt.Ne= 2.4 w mincount=10 11465410h Nnt.Ne= 2.5 w mincount=5 688804Table 4: Rules excluded from the initial rule set.3.3 Shallow versus Fully HierarchicalTranslationIn measuring the effectiveness of rules in transla-tion, we also investigate whether a ?fully hierarchi-cal?
search is needed or whether a shallow searchis also effective.
In constrast to full Hiero, in theshallow search, only phrases are allowed to be sub-stituted into non-terminals.
The rules used in eachcase can be expressed as shown in the 2nd and 3rdcolumns of Table 1.
Shallow search can be con-sidered (loosely) to be a form of rule filtering.As can be seen in Table 5 there is no impact onBLEU, while translation speed increases by a fac-tor of 7.
Of course, these results are specific to thisArabic-to-English translation task, and need notbe expected to carry over to other language pairs,such as Chinese-to-English translation.
However,the impact of this search simplification is easy tomeasure, and the gains can be significant enough,that it may be worth investigation even for lan-guages with complex long distance movement.mt02-05- -tune -testSystem Time BLEU BLEUHIERO 14.0 52.1 51.5HIERO - shallow 2.0 52.1 51.4Table 5: Translation performance and time (in sec-onds per word) for full vs. shallow Hiero.3.4 Individual Rule FiltersWe now filter rules individually (not by class) ac-cording to their number of translations.
For eachfixed ?
/?
T+ (i.e.
with at least 1 non-terminal),we define the following filters over rules X ???,??:?
Number of translations (NT).
We keep theNT most frequent ?, i.e.
each ?
is allowed tohave at most NT rules.?
Number of reordered translations (NRT).We keep the NRT most frequent ?
withmonotonic non-terminals and the NRT mostfrequent ?
with reordered non-terminals.?
Count percentage (CP).
We keep the mostfrequent ?
until their aggregated number ofcounts reaches a certain percentage CP of thetotal counts of X ?
??,??.
Some ?
?s are al-lowed to have more ?
?s than others, depend-ing on their count distribution.Results applying these filters with variousthresholds are given in Table 6, including num-ber of rules and decoding time.
As shown, allfilters achieve at least a 50% speed-up in decod-ing time by discarding 15% to 25% of the base-line rules.
Remarkably, performance is unaffectedwhen applying the simple NT and NRT filterswith a threshold of 20 translations.
Finally, theCM filter behaves slightly worse for thresholds of90% for the same decoding time.
For this reason,we select NRT=20 as our general filter.mt02-05- -tune -testFilter Time Rules BLEU BLEUbaseline 2.0 4.20 52.1 51.4NT=10 0.8 3.25 52.0 51.3NT=15 0.8 3.43 52.0 51.3NT=20 0.8 3.56 52.1 51.4NRT=10 0.9 3.29 52.0 51.3NRT=15 1.0 3.48 52.0 51.4NRT=20 1.0 3.59 52.1 51.4CP=50 0.7 2.56 51.4 50.9CP=90 1.0 3.60 52.0 51.3Table 6: Impact of general rule filters on transla-tion (IBM BLEU), time (in seconds per word) andnumber of rules (in millions).3.5 Pattern-based Rule FiltersIn this section we first reconsider whether reintro-ducing the monotonic rules (originally excluded asdescribed in rows ?b?, ?c?, ?d?
in Table 4) affectsperformance.
Results are given in the upper rowsof Table 7.
For all classes, we find that reintroduc-ing these rules increases the total number of rules385mt02-05- -tune -testNnt.Ne Filter Time Rules BLEU BLEUbaseline NRT=20 1.0 3.59 52.1 51.42.3 +monotone 1.1 4.08 51.5 51.12.4 +monotone 2.0 11.52 51.6 51.02.5 +monotone 1.8 6.66 51.7 51.21.3 mincount=3 1.0 5.61 52.1 51.32.3 mincount=1 1.2 3.70 52.1 51.42.4 mincount=5 1.8 4.62 52.0 51.32.4 mincount=15 1.0 3.37 52.0 51.42.5 mincount=1 1.1 4.27 52.2 51.51.2 mincount=5 1.0 3.51 51.8 51.31.2 mincount=10 1.0 3.50 51.7 51.2Table 7: Effect of pattern-based rule filters.
Time in seconds per word.
Rules in millions.substantially, despite the NRT=20 filter, but leadsto degradation in translation performance.We next reconsider the mincount threshold val-ues for Nnt.Ne classes 1.3, 2.3, 2.4 and 2.5 origi-nally described in Table 4 (rows ?e?
to ?h?).
Resultsunder various mincount cutoffs for each class aregiven in Table 7 (middle five rows).
For classes2.3 and 2.5, the mincount cutoff can be reducedto 1 (i.e.
all rules are kept) with slight translationimprovements.
In contrast, reducing the cutoff forclasses 1.3 and 2.4 to 3 and 5, respectively, addsmany more rules with no increase in performance.We also find that increasing the cutoff to 15 forclass 2.4 yields the same results with a smaller ruleset.
Finally, we consider further filtering applied toclass 1.2 with mincount 5 and 10 (final two rowsin Table 7).
The number of rules is largely un-changed, but translation performance drops con-sistently as more rules are removed.Based on these experiments, we conclude that itis better to apply separate mincount thresholds tothe classes to obtain optimal performance with aminimum size rule set.3.6 Large Language Models and EvaluationFinally, in this section we report results of ourshallow hierarchical system with the 2.5 min-count=1 configuration from Table 7, after includ-ing the following N-best list rescoring steps.?
Large-LM rescoring.
We build sentence-specific zero-cutoff stupid-backoff (Brants etal., 2007) 5-gram language models, estimatedusing ?4.7B words of English newswire text,and apply them to rescore each 10000-bestlist.?
Minimum Bayes Risk (MBR).
We then rescorethe first 1000-best hypotheses with MBR,taking the negative sentence level BLEUscore as the loss function to minimise (Ku-mar and Byrne, 2004).Table 8 shows results for mt02-05-tune, mt02-05-test, the NIST subsets from the MT06 evalu-ation (mt06-nist-nw for newswire data and mt06-nist-ng for newsgroup) and mt08, as measured bylowercased IBM BLEU and TER (Snover et al,2006).
Mixed case NIST BLEU for this system onmt08 is 42.5.
This is directly comparable to offi-cial MT08 evaluation results1.4 ConclusionsThis paper focuses on efficient large-scale hierar-chical translation while maintaining good trans-lation quality.
Smart memoization and spreadingneighborhood exploration during cube pruning aredescribed and shown to reduce memory consump-tion and Hiero search errors using a simple phrase-based system as a contrast.We then define a general classification of hi-erarchical rules, based on their number of non-terminals, elements and their patterns, for refinedextraction and filtering.For a large-scale Arabic-to-English task, weshow that shallow hierarchical decoding is as good1Full MT08 results are available athttp://www.nist.gov/speech/tests/mt/2008/.
It is worthnoting that many of the top entries make use of systemcombination; the results reported here are for single systemtranslation.386mt02-05-tune mt02-05-test mt06-nist-nw mt06-nist-ng mt08HIERO+MET 52.2 / 41.6 51.5 / 42.2 48.4 / 43.6 35.3 / 53.2 42.5 / 48.6+rescoring 53.2 / 40.8 52.6 / 41.4 49.4 / 42.9 36.6 / 53.5 43.4 / 48.1Table 8: Arabic-to-English translation results (lower-cased IBM BLEU / TER) with large language mod-els and MBR decoding.as fully hierarchical search and that decoding timeis dramatically decreased.
In addition, we describeindividual rule filters based on the distribution oftranslations with further time reductions at no costin translation scores.
This is in direct contrastto recent reported results in which other filteringstrategies lead to degraded performance (Shen etal., 2008; Zollmann et al, 2008).We find that certain patterns are of much greatervalue in translation than others and that separateminimum count filters should be applied accord-ingly.
Some patterns were found to be redundantor harmful, in particular those with two monotonicnon-terminals.
Moreover, we show that the valueof a pattern is not directly related to the number ofrules it encompasses, which can lead to discardinglarge numbers of rules as well as to dramatic speedimprovements.Although reported experiments are only forArabic-to-English translation, we believe the ap-proach will prove to be general.
Pattern relevancewill vary for other language pairs, but we expectfiltering strategies to be equally worth pursuing.AcknowledgmentsThis work was supported in part by the GALE pro-gram of the Defense Advanced Research ProjectsAgency, Contract No.
HR0011- 06-C-0022.
G.Iglesias supported by Spanish Government re-search grant BES-2007-15956 (project TEC2006-13694-C03-03).ReferencesCyril Allauzen, Michael Riley, Johan Schalkwyk, Wo-jciech Skut, and Mehryar Mohri.
2007.
OpenFst: Ageneral and efficient weighted finite-state transducerlibrary.
In Proceedings of CIAA, pages 11?23.Oliver Bender, Evgeny Matusov, Stefan Hahn, SasaHasan, Shahram Khadivi, and Hermann Ney.
2007.The RWTH Arabic-to-English spoken languagetranslation system.
In Proceedings of ASRU, pages396?401.Graeme Blackwood, Adria` de Gispert, Jamie Brunning,and William Byrne.
2008.
Large-scale statisticalmachine translation with weighted finite state trans-ducers.
In Proceedings of FSMNLP, pages 27?35.Phil Blunsom, Trevor Cohn, and Miles Osborne.
2008.A discriminative latent variable model for statisticalmachine translation.
In Proceedings of ACL-HLT,pages 200?208.Thorsten Brants, Ashok C. Popat, Peng Xu, Franz J.Och, and Jeffrey Dean.
2007.
Large languagemodels in machine translation.
In Proceedings ofEMNLP-ACL, pages 858?867.Jean-Ce?dric Chappelier and Martin Rajman.
1998.
Ageneralized CYK algorithm for parsing stochasticCFG.
In Proceedings of TAPD, pages 133?137.Jean-Ce?dric Chappelier, Martin Rajman, Ramo?nAragu?e?s, and Antoine Rozenknop.
1999.
Latticeparsing for speech recognition.
In Proceedings ofTALN, pages 95?104.David Chiang.
2005.
A hierarchical phrase-basedmodel for statistical machine translation.
In Pro-ceedings of ACL, pages 263?270.David Chiang.
2007.
Hierarchical phrase-based trans-lation.
Computational Linguistics, 33(2):201?228.Markus Dreyer, Keith Hall, and Sanjeev Khudanpur.2007.
Comparing reordering constraints for SMTusing efficient BLEU oracle computation.
In Pro-ceedings of SSST, NAACL-HLT 2007 / AMTA Work-shop on Syntax and Structure in Statistical Transla-tion.Christopher Dyer, Smaranda Muresan, and PhilipResnik.
2008.
Generalizing word lattice translation.In Proceedings of ACL-HLT, pages 1012?1020.Liang Huang and David Chiang.
2007.
Forest rescor-ing: Faster decoding with integrated language mod-els.
In Proceedings of ACL, pages 144?151.Shankar Kumar and William Byrne.
2004.
MinimumBayes-risk decoding for statistical machine transla-tion.
In Proceedings of HLT-NAACL, pages 169?176.Shankar Kumar and William Byrne.
2005.
Lo-cal phrase reordering models for statistical machinetranslation.
In Proceedings of HLT-EMNLP, pages161?168.Shankar Kumar, Yonggang Deng, and William Byrne.2006.
A weighted finite state transducer translationtemplate model for statistical machine translation.Natural Language Engineering, 12(1):35?75.387Zhifei Li and Sanjeev Khudanpur.
2008.
A scal-able decoder for parsing-based machine translationwith equivalent language model state maintenance.In Proceedings of the ACL-HLT Second Workshopon Syntax and Structure in Statistical Translation,pages 10?18.Adam Lopez.
2007.
Hierarchical phrase-based trans-lation with suffix arrays.
In Proceedings of EMNLP-CONLL, pages 976?985.Adam Lopez.
2008.
Tera-scale translation modelsvia pattern matching.
In Proceedings of COLING,pages 505?512.Yuval Marton and Philip Resnik.
2008.
Soft syntac-tic constraints for hierarchical phrased-based trans-lation.
In Proceedings of ACL-HLT, pages 1003?1011.Franz J. Och.
2003.
Minimum error rate training instatistical machine translation.
In Proceedings ofACL, pages 160?167.Antti-Veikko Rosti, Necip Fazil Ayan, Bing Xiang,Spyros Matsoukas, Richard Schwartz, and BonnieDorr.
2007.
Combining outputs from multiple ma-chine translation systems.
In Proceedings of HLT-NAACL, pages 228?235.Libin Shen, Jinxi Xu, and Ralph Weischedel.
2008.
Anew string-to-dependency machine translation algo-rithm with a target dependency language model.
InProceedings of ACL-HLT, pages 577?585.Khe Chai Sim, William Byrne, Mark Gales, HichemSahbi, and Phil Woodland.
2007.
Consensus net-work decoding for statistical machine translationsystem combination.
In Proceedings of ICASSP,volume 4, pages 105?108.Matthew Snover, Bonnie J. Dorr, Richard Schwartz,Linnea Micciulla, and John Makhoul.
2006.
Astudy of translation edit rate with targeted human an-notation.
In Proceedings of AMTA, pages 223?231.Ashish Venugopal, Andreas Zollmann, and VogelStephan.
2007.
An efficient two-pass approach tosynchronous-CFG driven statistical MT.
In Pro-ceedings of HLT-NAACL, pages 500?507.Hao Zhang and Daniel Gildea.
2006.
Synchronousbinarization for machine translation.
In Proceedingsof HLT-NAACL, pages 256?263.Hao Zhang, Daniel Gildea, and David Chiang.
2008.Extracting synchronous grammar rules from word-level alignments in linear time.
In Proceedings ofCOLING, pages 1081?1088.Andreas Zollmann and Ashish Venugopal.
2006.
Syn-tax augmented machine translation via chart parsing.In Proceedings of NAACL Workshop on StatisticalMachine Translation, pages 138?141.Andreas Zollmann, Ashish Venugopal, Franz Och,and Jay Ponte.
2008.
A systematic comparisonof phrase-based, hierarchical and syntax-augmentedstatistical MT.
In Proceedings of COLING, pages1145?1152.388
