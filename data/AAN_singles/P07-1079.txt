Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 624?631,Prague, Czech Republic, June 2007. c?2007 Association for Computational LinguisticsHPSG Parsing with Shallow Dependency ConstraintsKenji Sagae1 and Yusuke Miyao1 and Jun?ichi Tsujii1,2,31Department of Computer ScienceUniversity of TokyoHongo 7-3-1, Bunkyo-ku, Tokyo, Japan2School of Computer Science, University of Manchester3National Center for Text Mining{sagae,yusuke,tsujii}@is.s.u-tokyo.ac.jpAbstractWe present a novel framework that com-bines strengths from surface syntactic pars-ing and deep syntactic parsing to increasedeep parsing accuracy, specifically by com-bining dependency and HPSG parsing.
Weshow that by using surface dependencies toconstrain the application of wide-coverageHPSG rules, we can benefit from a num-ber of parsing techniques designed for high-accuracy dependency parsing, while actu-ally performing deep syntactic analysis.
Ourframework results in a 1.4% absolute im-provement over a state-of-the-art approachfor wide coverage HPSG parsing.1 IntroductionSeveral efficient, accurate and robust approaches todata-driven dependency parsing have been proposedrecently (Nivre and Scholz, 2004; McDonald et al,2005; Buchholz and Marsi, 2006) for syntactic anal-ysis of natural language using bilexical dependencyrelations (Eisner, 1996).
Much of the appeal of theseapproaches is tied to the use of a simple formalism,which allows for the use of efficient parsing algo-rithms, as well as straightforward ways to train dis-criminative models to perform disambiguation.
Atthe same time, there is growing interest in pars-ing with more sophisticated lexicalized grammarformalisms, such as Lexical Functional Grammar(LFG) (Bresnan, 1982), Lexicalized Tree Adjoin-ing Grammar (LTAG) (Schabes et al, 1988), Head-driven Phrase Structure Grammar (HPSG) (Pollardand Sag, 1994) and Combinatory Categorial Gram-mar (CCG) (Steedman, 2000), which represent deepsyntactic structures that cannot be expressed in ashallower formalism designed to represent only as-pects of surface syntax, such as the dependencyformalism used in current mainstream dependencyparsing.We present a novel framework that combinesstrengths from surface syntactic parsing and deepsyntactic parsing, specifically by combining depen-dency and HPSG parsing.
We show that, by us-ing surface dependencies to constrain the applica-tion of wide-coverage HPSG rules, we can bene-fit from a number of parsing techniques designedfor high-accuracy dependency parsing, while actu-ally performing deep syntactic analysis.
From thepoint of view of HPSG parsing, accuracy can be im-proved significantly through the use of highly ac-curate discriminative dependency models, withoutthe difficulties involved in adapting these modelsto a more complex and linguistically sophisticatedformalism.
In addition, improvements in depen-dency parsing accuracy are converted directly intoimprovements in HPSG parsing accuracy.
From thepoint of view of dependency parsing, the applica-tion of HPSG rules to structures generated by a sur-face dependency model provides a principled andlinguistically motivated way to identify deep syntac-tic phenomena, such as long-distance dependencies,raising and control.We begin by describing our dependency andHPSG parsing approaches in section 2.
In section3, we present our framework for HPSG parsing withshallow dependency constraints, and in section 4 we624Figure 1: HPSG parsingevaluate this framework empirically.
Sections 5 and6 discuss related work and conclusions.2 Fast dependency parsing andwide-coverage HPSG parsing2.1 Data-driven dependency parsingBecause we use dependency parsing as a step indeep parsing, it is important that we choose a pars-ing approach that is not only accurate, but also effi-cient.
The deterministic shift/reduce classifier-baseddependency parsing approach (Nivre and Scholz,2004) has been shown to offer state-of-the-art accu-racy (Nivre et al, 2006) with high efficiency due toa greedy search strategy.
Our approach is based onNivre and Scholz?s approach, using support vectormachines for classification of shift/reduce actions.2.2 Wide-coverage HPSG parsingHPSG (Pollard and Sag, 1994) is a syntactic the-ory based on lexicalized grammar formalism.
InHPSG, a small number of schemas explain generalconstruction rules, and a large number of lexical en-tries express word-specific syntactic/semantic con-straints.
Figure 1 shows an example of the processof HPSG parsing.
First, lexical entries are assignedto each word in a sentence.
In Figure 1, lexicalentries express subcategorization frames and pred-icate argument structures.
Parsing proceeds by ap-plying schemas to lexical entries.
In this example,the Head-Complement Schema is applied to the lex-ical entries of ?tried?
and ?running?.
We then obtaina phrasal structure for ?tried running?.
By repeat-edly applying schemas to lexical/phrasal structures,Figure 2: Extracting HPSG lexical entries from thePenn Treebankwe finally obtain an HPSG parse tree that covers theentire sentence.In this paper, we use an HPSG parser developedby Miyao and Tsujii (2005).
This parser has a wide-coverage HPSG lexicon which is extracted from thePenn Treebank.
Figure 2 illustrates their methodfor extraction of HPSG lexical entries.
First, givena parse tree from the Penn Treebank (top), HPSG-style constraints are added and an HPSG-style parsetree is obtained (middle).
Lexical entries are then ex-tracted from the terminal nodes of the HPSG parsetree (bottom).
This way, in addition to a wide-coverage lexicon, we also obtain an HPSG treebank,which can be used as training data for disambigua-tion models.The disambiguation model of this parser is basedon a maximum entropy model (Berger et al, 1996).The probability p(T |W ) of an HPSG parse tree Tfor the sentence W = ?w1, .
.
.
, wn?
is given as:p(T |W ) = p(T |L,W )p(L|W )=1Zexp(?i?ifi(T ))?jp(lj |W ),where L = ?l1, .
.
.
, ln?
are lexical entries and625p(li|W ) is the supertagging probability, i.e., theprobability of assignining the lexical entry li to wi(Ninomiya et al, 2006).
The probability p(T |L,W )is a maximum entropy model on HPSG parse trees,where Z is a normalization factor, and feature func-tions fi(T ) represent syntactic characteristics, suchas head words, lengths of phrases, and appliedschemas.
Given the HPSG treebank as training data,the model parameters ?i are estimated so as to maxi-mize the log-likelihood of the training data (Malouf,2002).3 HPSG parsing with dependencyconstraintsWhile a number of fairly straightforward models canbe applied successfully to dependency parsing, de-signing and training HPSG parsing models has beenregarded as a significantly more complex task.
Al-though it seems intuitive that a more sophisticatedlinguistic formalism should be more difficult to pa-rameterize properly, we argue that the difference incomplexity between HPSG and dependency struc-tures can be seen as incremental, and that the useof accurate and efficient techniques to determine thesurface dependency structure of a sentence providesvaluable information that aids HPSG disambigua-tion.
This is largely because HPSG is based on a lex-icalized grammar formalism, and as such its syntac-tic structures have an underlying dependency back-bone.
However, HPSG syntactic structures includeslong-distance dependencies, and the underlying de-pendency structure described by and HPSG structureis a directed acyclic graph, not a dependency tree (asused by mainstream approaches to data-driven de-pendency parsing).
This difference manifests itselfin words that have multiple heads.
For example, inthe sentence I tried to run, the pronoun I is a depen-dent of tried and of run.
This makes it possible torepresent that I is the subject of both verbs, preciselythe kind of information that cannot be represented independency parsing.
If we ignore long-distance de-pendencies, however, HPSG structures can be seenas lexicalized trees that can be easily converted intodependency trees.Given that for an HPSG representation of the syn-tactic structure of a sentence we can determine adependency tree by removing long-distance depen-dencies, we can use dependency parsing techniques(such as the deterministic dependency parsing ap-proach mentioned in section 2.1) to determine theunderlying dependency trees in HPSG structures.This is the basis for the parsing framework presentedhere.
In this approach, deep dependency analysisis done in two stages.
First, a dependency parserdetermines the shallow dependency tree for the in-put sentence.
This shallow dependency tree corre-sponds to the underlying dependency graph of theHPSG structure for the input sentence, without de-pendencies that roughly correspond to deep syntax.The second step is to perform HPSG parsing, asdescribed in section 2.2, but using the shallow de-pendency tree to constrain the application of HPSGrules.
We now discuss these two steps in more detail.3.1 Determining shallow dependencies inHPSG structures using dependency parsingIn order to apply a data-driven dependency ap-proach to the task of identifying the shallow de-pendency tree in HPSG structures, we first need acorpus of such dependency trees to serve as train-ing data.
We created a dependency training corpusbased on the Penn Treebank (Marcus et al, 1993),or more specifically on the HPSG Treebank gener-ated from the Penn Treebank (see section 2.2).
Foreach HPSG structure in the HPSG Treebank, a de-pendency tree is extracted in two steps.
First, theHPSG tree is converted into a CFG-style tree, sim-ply by removing long-distance dependency links be-tween nodes.
A dependency tree is then extractedfrom the resulting lexicalized CFG-style tree, as iscommonly done for converting constituent trees intodependency trees after the application of a head-percolation table (Collins, 1999).Once a dependency training corpus is available,it is used to train a dependency parser as describedin section 2.1.
This is done by training a classifierto determine parser actions based on local featuresthat represent the current state of the parser (Nivreand Scholz, 2004; Sagae and Lavie, 2005).
Train-ing data for the classifier is obtained by applying theparsing algorithm over the training sentences (forwhich the correct dependency structures are known)and recording the appropriate parser actions that re-sult in the formation of the correct dependency trees,coupled with the features that represent the state of626the parser mentioned in section 2.1.
An evaluationof the resulting dependency parser and its efficacy inaiding HPSG parsing is presented in section 4.3.2 Parsing with dependency constraintsGiven a set of dependencies, the bottom-up processof HPSG parsing can be constrained so that it doesnot violate the given dependencies.
This can beachieved by a simple extension of the parsing algo-rithm, as follows.
During parsing, we store the lex-ical head of each partial parse tree.
In each schemaapplication, we can determine which child is thehead; for example, the left child is the head whenwe apply the Head-Complement Schema.
Given thisinformation and lexical heads, the parser can iden-tify the dependency produced by this schema appli-cation, and can therefore judge whether the schemaapplication violates the dependency constraints.This method forces the HPSG parser to produceparse trees that strictly conform to the output ofthe dependency parser.
However, this means thatthe HPSG parser outputs no successful parse resultswhen it cannot find the parse tree that is completelyconsistent with the given dependencies.
This situ-ation may occur when the dependency parser pro-duces structures that are not covered in the HPSGgrammar.
This is especially likely with a fully data-driven dependency parser that uses local classifica-tion, since its output may not be globally consistentgrammatically.
In addition, the HPSG grammar isextracted from the HPSG Treebank using a corpus-based procedure, and it does not necessarily coverall possible grammatical phenomena in unseen text(Miyao and Tsujii, 2005).We therefore propose an extension of this ap-proach that uses predetermined dependencies as softconstraints.
Violations of schema applications aredetected in the same way as before, but instead ofstrictly prohibiting schema applications, we penal-ize the log-likelihood of partial parse trees createdby schema applications that violate the dependen-cies constraints.
Given a negative value ?, we add?
to the log-probability of a partial parse tree whenthe schema application violates the dependency con-straints.
That is, when a parse tree violates n depen-dencies, the log-probability of the parse tree is low-ered by n?.
The meta parameter ?
is determined soas to maximize the accuracy on the development set.Soft dependency constraints can be implementedas explained above as a straightforward extension ofthe parsing algorithm.
In addition, it is easily inte-grated with beam thresholding methods of parsing.Because beam thresholding discards partial parsetrees that have low log-probabilities, we can ex-pect that the parser would discard partial parse treesbased on violation of the dependency constraints.4 ExperimentsWe evaluate the accuracy of HPSG parsing with de-pendency constraints on the HPSG Treebank (Miyaoet al, 2003), which is extracted from the Wall StreetJournal portion of the Penn Treebank (Marcus etal., 1993)1.
Sections 02-21 were used for training(for HPSG and dependency parsers), section 22 wasused as development data, and final testing was per-formed on section 23.
Following previous work onwide-coverage parsing with lexicalized grammarsusing the Penn Treebank, we evaluate the parser bymeasuring the accuracy of predicate-argument rela-tions in the parser?s output.
A predicate-argumentrelation is defined as a tuple ?
?,wh, a, wa?, where?
is the predicate type (e.g.
adjective, intransitiveverb), wh is the head word of the predicate, a is theargument label (MODARG, ARG1, ... , ARG4), andwa is the head word of the argument.
Labeled pre-cision (LP)/labeled recall (LR) is the ratio of tuplescorrectly identified by the parser.
These predicate-argument relations cover the full range of syntacticdependencies produced by the HPSG parser (includ-ing, long-distance dependencies, raising and control,in addition to surface dependencies).In the experiments presented in this section, in-put sentences were automatically tagged with parts-of-speech with about 97% accuracy, using a max-imum entropy POS tagger.
We also report resultson parsing text with gold standard POS tags, whereexplicitly noted.
This provides an upper-bound onwhat can be expected if a more sophisticated multi-tagging scheme (James R. Curran and Vadas, 2006)is used, instead of hard assignment of single tags ina preprocessing step as done here.1The extraction software can be obtained from http://www-tsujii.is.s.u-tokyo.ac.jp/enju.6274.1 BaselineHPSG parsing results using the same HPSG gram-mar and treebank have recently been reported byMiyao and Tsujii (2005) and Ninomia et al (2006).By running the HPSG parser described in section 2.2on the development data without dependency con-straints, we obtain similar values of LP (86.8%) andLR (85.6%) as those reported by Miyao and Tsu-jii (Miyao and Tsujii, 2005).
Using the extremelylexicalized framework of (Ninomiya et al, 2006) byperforming supertagging before parsing, we obtainsimilar accuracy as Ninomiya et al (87.1% LP and85.9% LR).4.2 Dependency constraints and the penaltyparameterParsing the development data with hard dependencyconstraints confirmed the intuition that these con-straints often describe dependency structures that donot conform to HPSG schema used in parsing, re-sulting in parse failures.
To determine the upper-bound on HPSG parsing with hard dependency con-straints, we set the HPSG parser to disallow the ap-plication of any rules that result in the creation ofdependencies that violate gold standard dependen-cies.
This results in high precision (96.7%), but re-call is low (82.3%) due to parse failures caused bylack of grammatical coverage 2.
Using dependen-cies produced by the shift-reduce SVM parser, weobtain 91.5% LP and 65.7% LR.
This represents alarge gain in precision over the baseline, but an evengreater loss in recall, which limits the usefulness ofthe parser, and severely hurts the appeal of hard con-straints.We focus the rest of our experiments on parsingwith soft dependency constraints.
As explained insection 3, this involves setting the penalty parame-ter ?.
During parsing, we subtract ?
from the log-probability of applying any schema that violates thedependency constraints given to the HPSG parser.Figure 3 illustrates the effect of ?
when gold stan-dard dependencies (and gold standard POS tags) areused.
We note that setting ?
= 0 causes the parser2Although the HPSG grammar does not have perfect cov-erage of unseen text, it supports complete and mostly correctanalyses for all sentences in the development set.
However,when we require completely correct analyses by using hard con-straints, lack of coverage may cause parse failures.8990919293949596 05101520253035PenaltyAccuracyPrecisionRecallF-scoreFigure 3: The effect of ?
on HPSG parsing con-strained by gold standard dependencies.to ignore dependency constraints, providing base-line performance.
Conversely, setting a high enoughvalue (?
= 30 is sufficient, in practice) causes anysubstructures that violate the dependency constraintsto be used only when they are absolutely neces-sary to produce a valid parse for the input sentence.In figure 3, this corresponds to an upper-bound onthe accuracy of parsing with soft dependency con-straints (94.7% f-score), since gold standard depen-dencies are used.We set ?
empirically with simple hill climbing onthe development set.
Because it is expected that theoptimal value of ?
depends on the accuracy of thesurface dependency parser, we set separate valuesfor parsing with a POS tagger or with gold standardPOS tags.
Figure 4 shows the accuracy of HPSGpredicate-argument relations obtained with depen-dency constraints determined by dependency pars-ing with gold standard POS tags.
With both au-tomatically assigned and gold standard POS tags,we observe an improvement of about 0.6% in pre-cision, recall and f-score, when the optimal ?
valueis used in each case.
While this corresponds to a rel-ative error reduction of over 6% (or 12%, if we con-sider the upper-bound dictated by imperfect gram-matical coverage), a more interesting aspect of thisframework is that it allows techniques designed forimproving dependency accuracy to improve HPSGparsing accuracy directly, as we illustrate next.62889.489.689.89090.290.490.690.891 00.511.522.533.5PenaltyAccuracyPrecisionRecallF-scoreFigure 4: The effect of ?
on HPSG parsing con-strained by the output of a dependency parser usinggold standard POS tags.4.3 Determining constraints with dependencyparser combinationParser combination has been shown to be a power-ful way to obtain very high accuracy in dependencyparsing (Sagae and Lavie, 2006).
Using dependencyconstraints allows us to improve HPSG parsing ac-curacy simply by using an existing parser combina-tion approach.
As a first step, we train two addi-tional parsers with the dependencies extracted fromthe HPSG Treebank.
The first uses the same shift-reduce framework described in section 2.1, but itprocess the input from right to left (RL).
This hasbeen found to work well in previous work on depen-dency parser combination (Zeman and Z?abokrtsky?,2005; Sagae and Lavie, 2006).
The second parseris MSTParser, the large-margin maximum spanningtree parser described in (McDonald et al, 2005)3.We examine the use of two combination schemes:one using two parsers, and one using three parsers.The first combination approach is to keep only de-pendencies for which there is agreement between thetwo parsers.
In other words, dependencies that areproposed by one parser but not the other are simplydiscarded.
Using the left-to-right shift-reduce parserand MSTParser, we find that this results in very highprecision of surface dependencies on the develop-ment data.
In the second approach, combination of3Downloaded from http://sourceforge.net/projects/mstparserthe three dependency parsers is done according tothe maximum spanning tree combination scheme ofSagae and Lavie (2006), which results in high accu-racy of surface dependencies.
For each of the com-bination approaches, we use the resulting dependen-cies as constraints for HPSG parsing, determiningthe optimal value of ?
on the development set inthe same way as done for a single parser.
Table 1summarizes our experiments on development datausing parser combinations to produce dependencyconstraints 4.
The two combination approaches aredenoted as C1 and C2.Parser Dep ?
HPSG Diffnone (baseline) ?
?
86.5 ?LR shift-reduce 91.2 1.5 87.1 0.6RL shift-reduce 90.1 ?
?MSTParser 91.0 ?
?C1 (agreement) 96.8* 2.5 87.4 0.9C2 (MST) 92.4 2.5 87.4 0.9Table 1: Summary of results on development data.
* The shallow accuracy of combination C1 corre-sponds to the dependency precision (no dependen-cies were reported for 8% of all words in the devel-opment set).4.4 ResultsHaving determined ?
values on development datafor the shift-reduce dependency parser, the two-parser agreement combination, and the three-parsermaximum spanning tree combination, we parse thetest data (section 23) using these three differentsources of dependency constraints for HPSG pars-ing.
Our final results are shown in table 2, wherewe also include the results published in (Ninomiyaet al, 2006) for comparison purposes, and the resultof using dependency constraints obtained with goldstandard POS tags.By using two unlabeled dependency parsers toprovide soft dependency constraints, we obtain a1% absolute improvement in precision and recall ofpredicate-argument identification in HPSG parsingover a strong baseline.
Our baseline approach out-performed previously published results on this test4The accuracy figures for the dependency parsers is ex-pressed as unlabeled accuracy of the surface dependencies only,and are not comparable to the HPSG parsing accuracy figures629Parser LP LR F-scoreHPSG Baseline 87.4 87.0 87.2Shift-Reduce + HPSG 88.2 87.7 87.9C1 + HPSG 88.5 88.0 88.2C2 + HPSG 88.4 87.9 88.1Baseline(gold) 89.8 89.4 89.6Shift-Reduce(gold) 90.62 90.23 90.42C1+HPSG(gold) 90.9 90.4 90.6C2+HPSG(gold) 90.8 90.4 90.6Miyao and Tsujii, 2005 85.0 84.3 84.6Ninomiya et al, 2006 87.4 86.3 86.8Table 2: Final results on test set.
The first set ofresults show our HPSG baseline and HPSG with softdependency constraints using three different sourcesof dependency constraints.
The second set of resultsshow the accuracy of the same parsers when goldpart-of-speech tags are used.
The third set of resultsis from existing published models on the same data.set, and our best performing combination schemeobtains an absolute improvement of 1.4% over thebest previously published results using the HPSGTreebank.
It is interesting to note that the results ob-tained with dependency parser combinations C1 andC2 were very similar, even though in C1 only twoparsers were used, and constraints were provided forabout 92% of shallow dependencies (with accuracyhigher than 96%).
Clearly, precision is crucial in de-pendency constraints.Finally, although it is necessary to perform de-pendency parsing to pre-compute dependency con-straints, the total time required to perform the en-tire process of HPSG parsing with dependency con-straints is close to that of the baseline HPSG ap-proach.
This is due to two reasons: (1) the de-pendency parsing approaches used to pre-computeconstraints are several times faster than the baselineHPSG approach, and (2) the HPSG portion of theprocess is significantly faster when dependency con-straints are used, since the constraints help sharpenthe search space, making search more efficient.
Us-ing the baseline HPSG approach, it takes approx-imately 25 minutes to parse the test set.
The to-tal time required to parse the test set using HPSGwith dependency constraints generated by the shift-reduce parser is 27 minutes.
With combination C1,parsing time increases to 30 minutes, since two de-pendency parsers are used sequentially.5 Related workThere are other approaches that combine shallowprocessing with deep parsing (Crysmann et al,2002; Frank et al, 2003; Daum et al, 2003) to im-prove parsing efficiency.
Typically, shallow parsingis used to create robust minimal recursion seman-tics, which are used as constraints to limit ambigu-ity during parsing.
Our approach, in contrast, usessyntactic dependencies to achieve a significant im-provement in the accuracy of wide-coverage HPSGparsing.
Additionally, our approach is in manyways similar to supertagging (Bangalore and Joshi,1999), which uses sequence labeling techniques asan efficient way to pre-compute parsing constraints(specifically, the assignment of lexical entries to in-put words).6 ConclusionWe have presented a novel framework for taking ad-vantage of the strengths of a shallow parsing ap-proach and a deep parsing approach.
We haveshown that by constraining the application of rulesin HPSG parsing according to results from a depen-dency parser, we can significantly improve the ac-curacy of deep parsing by using shallow syntacticanalyses.To illustrate how this framework allows for im-provements in the accuracy of dependency parsingto be used directly to improve the accuracy of HPSGparsing, we showed that by combining the results ofdifferent dependency parsers using the search-basedparsing ensemble approach of (Sagae and Lavie,2006), we obtain improved HPSG parsing accuracyas a result of the improved dependency accuracy.Although we have focused on the use of HPSGand dependency parsing, the general framework pre-sented here can be applied to other lexicalized gram-mar formalisms, such as LTAG, CCG and LFG.AcknowledgementsThis research was partially supported by Grant-in-Aid for Specially Promoted Research 18002007.630ReferencesSrinivas Bangalore and Aravind K. Joshi.
1999.
Su-pertagging: an approach to almost parsing.
Compu-tational Linguistics, 25(2):237?265.A.
Berger, S. A. Della Pietra, and V. J. Della Pietra.
1996.Amaximum entropy approach to natural language pro-cessing.
Computational Linguistics, 22(1):39?71.Joan Bresnan.
1982.
The mental representation of gram-matical relations.
MIT Press.Sabine Buchholz and Erwin Marsi.
2006.
Conll-x sharedtask on multilingual dependency parsing.
In Proceed-ings of the Tenth Conference on Natural LanguageLearning.
New York, NY.M.
Collins.
1999.
Head-Driven Models for Natural Lan-guage Parsing.
Phd thesis, University of Pennsylva-nia.Berthold Crysmann, Anette Frank, Bernd Kiefer, StefanMueller, Guenter Neumann, Jakub Piskorski, UlrichSchaefer, Melanie Siegel, Hans Uszkoreit, Feiyu Xu,Markus Becker, and Hans-Ulrich Krieger.
2002.
Anintegrated architecture for shallow and deep process-ing.
In Proceedings of the 40th Annual Meeting ofthe Association for Computational Linguistics (ACL2002).Michael Daum, Kilian A. Foth, and Wolfgang Menzel.2003.
Constraint-based integration of deep and shal-low parsing techniques.
In Proceedings of the 10thConference of the European Chapter of the Associa-tion for Computational Linguistics (EACL 2003).Jason Eisner.
1996.
Three new probabilistic models fordependency parsing: An exploration.
In Proceedingsof the International Conference on Computational Lin-guistics (COLING?96).
Copenhagen, Denmark.Anette Frank, Markus Becker, Berthold Crysmann,Bernd Kiefer, and Ulrich Schaefer.
2003.
Integratedshallow and deep parsing: TopP meets HPSG.
In Pro-ceedings of the 41st Annual Meeting of the Associa-tion for Computational Linguistics (ACL 2003), pages104?111.Stephen Clark James R. Curran and David Vadas.
2006.Multi-tagging for lexicalized-grammar parsing.
InProceedings of COLING/ACL 2006.
Sydney, Aus-tralia.Robert Malouf.
2002.
A comparison of algorithms formaximum entropy parameter estimation.
In Proceed-ings of the 2002 Conference on Natural LanguageLearning.M.
P. Marcus, B. Santorini, and M. A. Marcinkiewics.1993.
Building a large annotated corpus of english:The penn treebank.
Computational Linguistics, 19.Ryan McDonald, Fernando Pereira, K. Ribarov, andJ.
Hajic.
2005.
Non-projective dependency pars-ing using spanning tree algorithms.
In Proceedingsof the Conference on Human Language Technolo-gies/Empirical Methods in Natural Language Process-ing (HLT-EMNLP).
Vancouver, Canada.Yusuke Miyao and Jun?ichi Tsujii.
2005.
Probabilisticdisambiguation models for wide-coverage hpsg pars-ing.
In Proceedings of the 42nd Meeting of the Associ-ation for Computational Linguistics.
Ann Arbor, MI.Yusuke Miyao, Takashi Ninomiya, and Jun?ichi Tsu-jii.
2003.
Corpus oriented grammar development foraquiring a head-driven phrase structure grammar fromthe penn treebank.
In Proceedings of the Tenth Con-ference on Natural Language Learning.T.
Ninomiya, T. Matsuzaki, Y. Tsuruoka, Y. Miyao, andJ.
Tsujii.
2006.
Extremely lexicalized models for ac-curate and fast hpsg parsing.
In Proceedings of the2006 Conference on Empirical Methods for NaturalLanguage Processing (EMNLP 2006).Joakim Nivre and Mario Scholz.
2004.
Deterministicdependency parsing of english text.
In Proceedings ofthe 20th International Conference on ComputationalLinguistics, pages 64?70.
Geneva, Switzerland.J.
Nivre, J.
Hall, J. Nilsson, G. Eryigit, and S. Marinov.2006.
Labeled pseudo-projective dependency pars-ing with support vector machines.
In Proceedings ofthe Tenth Conference on Natural Language Learning.New York, NY.C.
Pollard and I.
A.
Sag.
1994.
Head-Driven PhraseStructure Grammar.
University of Chicago Press.Kenji Sagae and Alon Lavie.
2005.
A classifier-basedparser with linear run-time complexity.
In Proceed-ings of the Ninth International Workshop on ParsingTechnologies.
Vancouver, BC.Kenji Sagae and Alon Lavie.
2006.
Parser combinationby reparsing.
In Proceedings of the 2006 Meeting ofthe North American ACL.
New York, NY.Yves Schabes, Anne Abeille, and Aravind Joshi.
1988.Parsing strategies with lexicalized grammars: Appli-cation to tree adjoining grammars.
In Proceedings of12th COLING.Mark Steedman.
2000.
The Syntactic Process.
MITPress.Daniel Zeman and Zdenek Z?abokrtsky?.
2005.
Improvingparsing accuracy by combining diverse dependencyparsers.
In Proceedings of the International Workshopon Parsing Technologies.
Vancouver, Canada.631
