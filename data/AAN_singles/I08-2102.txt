Summarization by Analogy:An Example-based Approach for News ArticlesMegumi Makino and Kazuhide YamamotoDept.
of Electrical Engineering, Nagaoka University of Technology1603-1 Kamitomioka, Nagaoka, Niigata 940-2188 Japan{makino,ykaz}@nlp.nagaokaut.ac.jpAbstractAutomatic summarization is an importanttask as a form of human support technology.We propose in this paper a new summariza-tion method that is based on example-basedapproach.
Using example-based approachfor the summarization task has the followingthree advantages: high modularity, absenceof the necessity to score importance for eachword, and high applicability to local con-text.
Experimental results have proven thatthe summarization system attains approxi-mately 60% accuracy by human judgment.1 IntroductionThe example-based approach generates language byimitating instances, which originated in the machinetranslation method based on the analogy (Nagao,1984).
The idea is derived from the observation thata human being translates according to past transla-tion experiences.
In the machine translation task,this approach has been implemented, and has so farachieved efficient results (Sumita, 1998; Imamura,2004).In summarization, a human being also summa-rizes with his own knowledge and experiences.
Forthis reason, we focus on a summarization methodwhich is based on analogy, example-based summa-rization.
The example-based method summarizesthe input text in three steps.
First, it retrieves a simi-lar instance to the input text.
Second, it links equiv-alent phrases between the input text and the similarinstance.
Finally, a summary is acquired with com-bination of some corresponding phrases.
Here, weemployed a Japanese news article as the input textand utilized news headlines as the instances.
Thenews headline consists of one brief sentence whichdescribes the main point.We assert that the example-based summarizationhas the following advantages:(1)High modularityEasy improvement and maintenance are requiredto formulate a useful system in general.
Anexample-based framework makes it easy for us toimprove a system by only adding instances.
Besides,the addition of instances causes few side-effects.
(2)Use of similarity rather than importanceAlmost all previous work on summarization hasfocused on a sentence extraction.
These works com-pute importance for each word to extract a sentence.However, it is difficult to compute the importancewhich correlates with human sense.
Example-basedsummarization means there is no need to measurethe importance, and it computes the similarity in-stead.
We think it is easier to assess the similaritybetween two expressions rather than the importanceof one expression.
(3)High applicability to local contextThe statistical method, in general, attempts tocompute the probability of each word appearing inthe summary corpus (Knight and Marcu, 2002; Wit-brock and Mittal, 1999).
This may increase difficul-ties in maintaining local context, since the statisticalapproach focuses on the global probability.
How-ever, the example-based approach attempts to findmost locally similar instance out of the instance col-lection, which may increase the fitness of input con-texts.For the three reasons given above, this paperexplains the system which summarizes a Japanesenews article to a one-sentence summary by imitat-ing the similar instance.739As related work, Nguyen et al (2004) have pro-posed an example-based sentence reduction model.They deal with the compression of one sentence,while we summarize some sentences into a one-sentence summary.
Thus, our summarization ratiois inevitably lower than theirs, as it is considered tobe more difficult as a summarization task.Many studies have summarized some sentences,such as a news article, into a one-sentence summary.Most of them extract the important sentence andcontract it.
In contrast, our method generates a one-sentence summary by combining phrases in somesentences.
Consequently, we can obtain high com-pression summaries that include information frommany positions of the source.2 Instance CollectionOur example-based summarization regards newsheadlines as the instance collection.
A news head-line is a short sentence in which the primary pointis written.
The following example is Japanese newsheadlines:Example (1) :????????????????????
(Mitsubishi Motors Corp. produces passenger carsin China.
)We use Japanese news headlines, like the aboveexamples, as instances.
Besides, as we have noted,only news headlines are used as instances; that is,the pairs formed by an original sentence and its sum-marized sentence are not used.3 Example-based Summarization3.1 OverviewOur example-based summarization system summa-rizes a lengthy news article into a one-sentence sum-mary by using instances.
The overall process is il-lustrated in figure 1.
The system is composed of thefollowing three processes in this order:1.
Retrieve a similar instance to an input news ar-ticle from the instance collection.2.
Align corresponding phrases between the inputnews article and the similar instance.3.
Combine the corresponding phrases to form asummary.Detail of each process is described hereafter.3.2 Retrieval of Similar InstanceThe system measures a similarity between the inputand each instance in the instance collection whenit retrieves a similar instance.
If many words areshared between two expressions, we regard two ex-pressions as similar.
Hence, the similarity is calcu-lated on basis of the overlaps of content words be-tween the input news article I and the instance E ,defined as follows:Sim(E, I)=n?i=1Score(i)?
{w ?
||T v1(E)?Tvi(I)||+||To1(E)?Toi(I)||} (1)where,- n : the number of sentences in input,- Tvi(?)
: the verbs set in the last phrase of the i-thsentence,- Toi(?)
: the set of content words in the i-th sen-tence,- ||Tv1(E)?
Tvi(I)|| : the number of overlaps be-tween Tv1(E) and Tvi(I).In the equation, Score(i) and w are designed to givea higher score if words indicating the main topic ofthe input article are matched with words in the in-stance.
We have found that words have differentcontributions, depending on the sentence position,to the main topic.
Therefore, we apply Score(i)which depends on the sentence position i, and weuse the following experimentally-determined scoreas Score(i).Score(i) ={5.15 if i = 12.78/i0.28 otherwise (2)The score indicates an agreement rate of contentwords depending on the sentence position, which iscalculated by using 5000 pairs of newspaper?s bodyand its title1 We have also found that the verbs inthe last phrase are appropriate for the main topic ofthe input article.
For that reason, we determine theweight w=3 by our experiment.Example 2 shows the similar instance obtained bymeasuring the similarity.Example (2) :Input news article???????????????????????????
24??????????
(skip the1We used the same kind of newspaper as data set in section4.1 for calculating Score(i).740Figure 1: Overview of example-based summarizationrest.
)(The Manufacturing Council held a meeting on the24th, which discusses the hard-hitting strategy forquality management.
...)Obtained similar instance?????????
18???????????????
(The committee for the privatization of the PublicRoads Administration held the first meeting on the18th at the prime minister?s office.
)3.3 Phrase AlignmentWe compare the phrases in the input with those inthe similar instance, and the system aligns the corre-sponding phrases.
Here, the correspondence refersto the link of the equivalent phrases between the in-put and its similar instance.
The detail of phrasealignment procedures are shown in the following.To begin with, sentences both in the input and inthe similar instance are analyzed using a Japanesesyntactic parser CaboCha1).
The sentences are splitinto phrases and named entities (NEs), such as PER-SON, LOCATION, DATE, are recognized by thetool.Then the adnominal phrases in the similar in-stance are deleted.
This is because the adnomi-nal phrases are of many types, depending on themodified noun; accordingly, the adnominal phraseshould be used only if the modified nouns are ex-actly matched between the input and the similar in-stance.Finally, the system links the correspondingphrases.
Here, phrase correspondence is one-to-many, not one-to-one, and therefore a phrase in asimilar instance has some corresponding phrases inthe input.
In order to compare phrases, the followingfour measures are employed: (i) agreement of gram-matical case, (ii) agreement of NE, (iii) similaritywith enhanced edit distance, and (iv) similarity bymeans of mutual information.
The measure of (i)focuses on functional words, whereas the measuresof (ii)-(iv) note content words.
Let us explain themeasures using example 2.
(i) Agreement of Grammatical CaseIf there is a phrase which has the same grammati-cal case2 in the input and in the similar instance, weregard the phrase as the corresponding phrase.
Inexample 2, for example, the phrases ?????
?
(the hard-hitting strategy obj3), ???
(the meet-ing obj)?
in the input corresponds the phrase ?????
(the first meeting obj)?
in the similar instance.
(ii) Agreement of Named EntityProvided the input has the same NE tag as the sim-ilar instance, the phrase involving its tag links thecorresponding phrase.
For example, in example 2,the phrase ?24?
[DATE] (on the 24th.)?
in the in-put corresponds the phrase ?18?
[DATE] (on the18th.)?
in the similar instance.
(iii) Similarity with Enhanced Edit DistanceWe adopt the enhanced edit distance to link phrasesincluding the same characters, because Japanese ab-breviation tends to include the same characters asthe original.
For example, the abbreviation of ?
?2Comma is also regarded as grammatical case (i.e., nullcase) here.3?obj?
is an object case marker.741???
(Bank of Japan)?
is ????.
The enhancededit distance is proposed by Yamamoto et al (2003).The distance is a measure of similarity by countingmatching characters between two phrases.
More-over, the distance is assigned a different similarityweight according to the type of matched characters.We apply 1.0 to the weight only if Chinese-derivedcharacters (Kanji) are matched.
We link phrases ascorresponding phrases, where the phrases are the topthree similar to a phrase in the similar instance.
(iv) Similarity with Mutual InformationWe finally compute the similarity with mutual in-formation to link syntactically similar phrases.
Forexample, given the following two expressions: ??????
(to hold a meeting)?
and ??????
(tohold a convention)?, we regard??
(a meeting) and??
(a convention) as similar.
We use the similar-ity proposed by Lin (1998).
The method uses mu-tual information and dependency relationships as thephrase features.
We extend the method to Japaneseby using a particle as the dependency relationships.We link phrases as corresponding phrases, where thephrases are the top three similar to a phrase in thesimilar instance.3.4 Combination of the Corresponding PhrasesOur system forms the one-sentence summary bycombining the corresponding phrases.
Let us ex-plain this process by using figure 2.
We arrange thephrase of the input on the node, where the phrasesis judged as the correspondence to the phrase in thesimilar instance.
For example, in figure 2, the sec-ond nodes e and d denote the corresponding phrasesin the input, which correspond to the second phrasehad in the similar instance.We assign the similarity between correspondingphrases as the weight of node.
In addition to this,we employ phrase connection score to the weight ofedge.
The score indicates the connectivity of con-secutive two phrases, e.g.
two nodes such as noded and node e in figure 2.
If you want to obtain afine summary, i.e., a summary that contains similarphrases to the similar instance, and that is correctgrammatically, you have to search the best path ?Wpfor path sequence Wp = {w0,w1,w2, ?
?
?
,wm}, wherethe best path maximizes the score.
?Wp =Wp s.t.
argmaxpScorep(Wp) (3)Figure 2: Optimal path problem that depends oncombination of the corresponding phrases4.The best path ?Wp is a one-sentence summary whichis generated by our system.
Take the case of thethick line in figure 2, ?Wp is indicated as ?Wp ={a,d,e,g,k,m,n}, namely, generated summary isformed the phrases {a,d,e,g,k,m,n}.
In eq.3,Scorep(Wp) is given byScorep(Wp)=?m?i=0N(wi)+(1??
)m?i=1E(wi?1,wi) (4)where ?
is the balancing factor among theweights of node and edge.
We score ?
= 0.6 byour experiment.
m indicates the last number of thephrase in the similar instance, N(wi) is given as fol-lows:N(wi)=max{ 0.5 if (grammatical case orNE tag is matched)1/rank otherwise(5)where, rank indicates the rank order of the similaritywith the enhanced edit distance or mutual informa-tion to the phrase wi.
N(wi) illustrates the similar-ity between corresponding two phrases.
The nodescore, shown above, is determined by the prelim-inary experiment.
The edge score E(wi?1,wi) isgiven byE(wi?1,wi) =1|loc(wi?1)?
loc(wi)|+1(6)where, loc(wi) denotes where the location of thesentence contains the phrase wi in the input.
Theedge score means that if wi?1 and wi are locatedclosely to each other, a higher score is given, since agood connection is expected in this case.4The nodes, a, b, c,?
?
?
, n, indicate the correspondingphrases to the phrase in the similar sentence.
For example, thenodes, b, c, d correspond to ?The PRA Committee.?
i is a phrasenumber in the similar sentence.7424 Evaluation and Discussion4.1 The CorpusWe used 26,784 news headlines as instances, whichwere collected from the Nikkei-goo mail service2)for 2001-2006.
In order to adjust the weight w in theeq.1 and the balancing parameter ?
in eq.4, 150 in-put news articles were used as the tuning set.
A dif-ferent group of 134 news articles were used for eval-uation.
We used Nihon Keizai Shimbun, a Japanesenewspaper 3) , from 1999 through 2000 as tuning andtest data.4.2 Summarization RatioTo calculate summarization ratio, we have comparedthe number of characters in the input news articleswith that in the output summary.
As the result,we obtained a summarization ratio of 5%; namely,95% characters in the input were reduced.
From thesummarization ratio, our approach made it possibleto summarize sentences into one-sentence summarywith high compression.4.3 Sectional EvaluationWe evaluated each part of our system by humanjudgment5.
We first evaluated the process by retriev-ing similar instance.
Next, we evaluated the pro-cesses of phrase alignment and the combination byassessing whether the output summaries were appro-priate.?
Retrieving ProcessAn examinee evaluated the similar instances ob-tained.
Given an input news article and the similarinstance to the input, the examinee rates the follow-ing scale from one to four, based on how similar thesimilar instance obtained is to the summary whichthe examinee generated from the input news article:1) quite similar 2) slightly similar3) not very similar 4) not similarOut of 134 input articles, 77 inputs were rankedeither 1) quite similar or 2) slightly similar.
As aconsequence, the accuracy of similar instance ob-tained is approximately 57%, which indicates thatthe similarity calculation for obtaining similar in-stance is feasible.5One examinee judged the parts of our system.?
Phrase Alignment and CombinationWe also evaluated parts of phrase alignment andthe combination by human judgment.
The exami-nee compared 77 output summaries with their input.Here, we limited 77 outputs judged as good similarinstances in evaluation of the process of retrievingsimilar instance, because we evaluate specificallythe parts of phrase alignment and combination.The examinee categorized them based on howproper the output summary is to the input news arti-cle:1) quite proper 2) slightly proper3) not very proper 4) not properAs a result of judgment, 48 outputs out of 77 areevaluated either 1) quite proper or 2) slightly proper.Both a statistical method by Knight andMarcu (2002) and an example-based method byNguyen et al (2004) contracted one-sentence witha summarization ratio of approximately 60-70%.Both papers indicated that a score of 7-8 on a scalefrom one to ten was obtained.
They deal with thecompression of one sentence, while we summarizesome sentences into a one-sentence summary.
Thus,our summarization ratio is lower than theirs, as it isconsidered to be more difficult as a summarizationtask.
Despite this, we obtained the ratio that 62%(48 out of 77 results) were judged proper.
Althoughdirect comparison of the performance is impossible,it is considered that our proposed method obtains acompetitive accuracy.4.4 Discussions?
Examples of Output SummaryFigure 3 shows some examples of the output sum-mary.From figure 3, we can see that the similar in-stances were effectively used, and the appropriatesummaries to the input are generated.
For example,the second summary in the figure is judged as a finesummary contracting information of two sentencesaccording to the similar instance.?
Analysis of Summarization ErrorsIn the course of our summarization, we have ob-served errors due to erroneous correspondences.
InJapanese, sometimes two or more phrases are con-tracted into one phrase, as in the example below.
Wenow only attempt to correspond two phrases one by743Input news article?????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????
(skip the rest.
)(The prosecution made Kawano?s closing arguments on the 21stin the trial at the Yokohama District Court.
The ex-sergeantSuguru Kawano is accused of gang-bashing by Atsugi PoliceStation?s patrol group in a string of scandals of Kanagawa Pre-fectural Police.
The prosecutors demanded one and half year ina prison.
...)Obtained similar instance??????
22??8??????????????????????????????????????????????
(The prosecution made Takuma?s closing arguments on the22nd in the trial at the Osaka District Court, and asked for thedeath penalty.
)Output summary???????????????????????????????????????????
(The prosecution made Kawano?s closing arguments on the 21stin the trial and demanded one and half years in prison.
)Figure 3: The examples of generated summaryone, and we thus can not deal with many-to-one cor-respondences.Example (3) :?????/6???????
?/(compare with the same month last year)5??/????
5??
?/(in May)We expect that this kind of phenomenon canbe solved by paraphrasing an input summary aswell as summary instance.
Recently, several workson paraphrasing techniques have been proposed inJapanese, hence such pre-processing before align-ment would be feasible.5 Conclusion and Future WorkWe have presented an example-based technique thathas been applied to the summarization task.
Theessence of the proposed method is to generate a one-sentence summary by combining instances each ofwhich imitates the given input.As the result of human judgment, the retrievalprocess of a similarity sentence attained 57% accu-racy.
And our method generated summary in which62% were judged proper.
We have confirmed byour observation that the summaries were generatedby combining the phrases in many positions of theinput, while those summaries are not given just by6?/?
indicates a phrase boundary.common methods such as sentence extraction meth-ods and sentence compression methods.The sectional evaluation and the inspection ofexample output show that this system works well.However, larger scale evaluation and comparison ofits accuracy remain to be future work.Tools and language resources1) CaboCha, Ver.0.53, Matsumoto Lab., Nara Institute ofScience and Technology.http://chasen.org/?taku/software/cabocha/2) Nikkei News Mail, NIKKEI-goo,http://nikkeimail.goo.ne.jp/3) Nihon Keizai Shimbun Newspaper Corpus, years 1999?2000, Nihon Keizai Shimbun, Inc.ReferencesKenji Imamura.
2004.
Automatic Construction of Trans-lation Knowledge for Corpus-based Machine Transla-tion.
Ph.D. thesis, Nara Institute of Science and Tech-nology.Kevin Knight and Daniel Marcu.
2002.
Summariza-tion Beyond Sentence Extraction: A Probabilistic Ap-proach to Sentence Compression.
Artificial Intelli-gence, 139(1):91?107.Dekang Lin.
1998.
Automatic Retrieval and Clusteringof Similar Words.
In Proceedings of COLING-ACL98,pages 768?773.Makoto Nagao.
1984.
A Framework of a MechanicalTranslation Between Japanese and English By Anal-ogy Principle.
In Artificial and Human Intelligence,pages 173?180.Minh Le Nguyen, Susumu Horiguchi, Akira Shimazu,and Bao Tu Ho.
2004.
Example-Based SentenceReduction Using the Hidden Markov Model.
ACMTransactions on Asian Language Information Process-ing, 3(2):146?158.Eiichiro Sumita.
1998.
An Example-Based Approachto Transfer and Structural Disambiguation within Ma-chine Translation.
Ph.D. thesis, Kyoto University.Michael J. Witbrock and Vibhu O. Mittal.
1999.
Ultra-Summarization: A Statistical Approach to Generat-ing Highly Condensed Non-Extractive Summaries.
InResearch and Development in Information Retrieval,pages 315?316.Eiko Yamamoto, Masahiro Kishida, Yoshinori Takenami,Yoshiyuki Takeda, and Kyoji Umemura.
2003.
Dy-namic Programming Matching for Large Scale Infor-mation Retrieval.
In Proceedings of the 6th Interna-tional Workshop on Information Retrieval with AsianLanguages, pages 100?108.744
