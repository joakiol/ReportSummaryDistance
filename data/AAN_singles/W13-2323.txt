Proceedings of the 7th Linguistic Annotation Workshop & Interoperability with Discourse, pages 187?195,Sofia, Bulgaria, August 8-9, 2013. c?2013 Association for Computational LinguisticsThe Benefits of a Model of AnnotationRebecca J. PassonneauCenter for Computational Learning SystemsColumbia Universitybecky@ccls.columbia.eduBob CarpenterDepartment of StatisticsColumbia Universitycarp@alias-i.comAbstractThis paper presents a case study of adifficult and important categorical anno-tation task (word sense) to demonstratea probabilistic annotation model appliedto crowdsourced data.
It is argued thatstandard (chance-adjusted) agreement lev-els are neither necessary nor sufficientto ensure high quality gold standard la-bels.
Compared to conventional agree-ment measures, application of an annota-tion model to instances with crowdsourcedlabels yields higher quality labels at lowercost.1 IntroductionThe quality of annotated data for computationallinguistics is generally assumed to be good enoughif a few annotators can be shown to be consistentwith one another.
Metrics such as pairwise agree-ment and agreement coefficients measure consis-tency among annotators.
These descriptive statis-tics do not support inferences about corpus qualityor annotator accuracy, and the absolute values oneshould aim for are debatable, as in the review byArtstein and Poesio (2008).
We argue that highchance-adjusted inter-annotator agreement is nei-ther necessary nor sufficient to ensure high qual-ity gold-standard labels.
Agreement measures re-veal little about differences among annotators, andnothing about the certainty of the true label, giventhe observed labels from annotators.
In contrast, aprobabilistic model of annotation supports statis-tical inferences about the quality of the observedand inferred labels.This paper presents a case study of a particu-larly thorny annotation task that is of widespreadinterest, namely word-sense annotation.
The itemsthat were annotated are occurrences of selectedwords in their sentence contexts, and the annota-tion labels are WordNet senses (Fellbaum, 1998).The annotations, collected through crowdsourc-ing, consist of one WordNet sense for each itemfrom up to twenty-five different annotators, giv-ing each word instance a large set of labels.
Notethat application of an annotation model does notrequire this many labels for each item, and crowd-sourced annotation data does not require a prob-abilistic model.
This case study, however, doesdemonstrate a mutual benefit.A highly certain ground truth label for each an-notated instance is the ultimate goal of data anno-tation.
Many issues, however, make this compli-cated for word sense annotation.
The number ofdifferent senses defined for a word varies acrosslexical resources, and pairs of senses within a sin-gle sense inventory are not equally distinct (Ideand Wilks, 2006; Erk and McCarthy, 2009).
Aprevious annotation effort using WordNet sense la-bels demonstrates a great deal of variation acrosswords (Passonneau et al 2012b).
On over 116words, chance-adjusted agreement ranged fromvery high to chance levels.
As a result, the groundtruth labels for many words are questionable.
On arandom subset of 45 of the same words, the crowd-sourced data presented here (available as noted be-low) yields a certainty measure for each groundtruth label indicating high certainty for most in-stances.2 Chance-Adjusted AgreementCurrent best practice for collecting and curatingannotated data involves iteration over four steps,or variations of them: 1) design or redesign theannotation task, 2) write or revise guidelines in-187structing annotators how to carry out the task, pos-sibly with some training, 3) have two or more an-notators work independently to annotate a sampleof data, and 4) measure the interannotator agree-ment on the data sample.
Once the desired agree-ment has been obtained, a gold standard datasetis created where each item is annotated by oneannotator.
As noted in the introduction, howmuch agreement is sufficient has been much dis-cussed (Artstein and Poesio, 2008; di Eugenio andGlass, 2004; di Eugenio, 2000; Bruce and Wiebe,1998).
The quality of the gold standard is not ex-plicitly measured.
Nor is the accuracy of the an-notators.
Since there are many ways to be inaccu-rate, and only one way to be accurate, it is assumedthat if annotators agree, then the annotation mustbe accurate.
This is often but not always correct.If two annotators do not agree well, this methoddoes not identify whether one annotator is moreaccurate than the other.
For the individual itemsthey disagree on, no information is gained aboutthe true label.To get a high level sense of the limitations ofagreement metrics, we briefly discuss how theyare computed and what they tell us.
For a com-mon notation, let i ?
1:I represent the set of allitems, j ?
1:J all the annotators, k ?
1:K all thelabel classes in a categorical labeling scheme (e.g.,word senses), and yi,j ?
1:K the observed labelsfrom annotator j for item i (assuming every anno-tator labels every item exactly once; we relax thisrestriction later).Agreement: Pairwise agreement Am,n betweentwo annotators m,n ?
1:J is defined as the pro-portion of items 1:I for which the annotators sup-plied the same label,Am,n = 1I?Ii=1 I(yi,m = yi,n),where the indicator function I(s) = 1 if s is trueand 0 otherwise.
Am,n is thus the maximum like-lihood estimate that annotator m and n will agree.Pairwise agreement can be extended to the en-tire pool of annotators by averaging over all(J2)pairs,A = 1(J2)?Jm=1?Jn=m+1Am,n.By construction, Am,n ?
[0, 1] and A ?
[0, 1].Pairwise agreement does not take into accountthe proportion of observed annotation values from1:K. As a simple expected chance of agreement, itprovides little information about the resulting dataquality.Chance-Adjusted Agreement: An agreementcoefficient, such as Cohen?s ?
(Cohen, 1960) orKrippendorff?s ?
(Krippendorff, 1980), measuresthe proportion of observed agreements that areabove the proportion expected by chance.
Givenan estimate Am,n of the probability that two an-notators m,n ?
1:J will agree on a label andan estimate of the probability Cm,n that theywill agree by chance, the chance-adjusted inter-annotator agreement coefficient IAm,n ?
[?1, 1]is defined byIAm,n =Am,n?Cm,n1?Cm,n.For Cohen?s ?
statistic, chance agreement is de-fined to take into account the prevalence of theindividual labels in 1:K. Specifically, it is de-fined to be the probability that a pair of labelsdrawn at random for two annotators agrees.
Thereare two common ways to define this draw.
Thefirst assumes each annotator draws uniformly atrandom from her set of labels.
Letting ?j,k =1I?Ii=1 I(yi,j = k) be the proportion of the label kin annotator j?s labels, this notion of chance agree-ment for a pair of annotators m,n is estimated asthe sum over 1:K of the products of their propor-tions ?
:Cm,n =?Kk=1 ?m,k ?
?n,k.Another computation of chance agreement in wideuse assumes each annotator draws uniformly atrandom from the pooled set of labels from all an-notators (Krippendorff, 1980).
Letting ?k be theproportion of label k in the entire set of labels, thisalternative estimate, C ?m,n =?Kk=1 ?2k, does notdepend on the identity of the annotators m and n.An inter-annotator agreement statistic like ?suffers from multiple shortcomings.
(1) Agree-ment statistics are intrinsically pairwise, althoughone can compare to a voted consensus or aver-age over multiple pairwise agreements.
(2) Inagreement-based analyses, two wrongs make aright; if two annotators both make the same mis-take, they agree.
If annotators are 80% accurateon a binary task, chance agreement on the wrongcategory occurs at a 4% rate.
(3) Chance-adjustedagreement reduces to simple agreement as chanceagreement approaches zero.
When chance agree-ment is high, even high-accuracy annotators can188have low chance-adjusted agreement.
For ex-ample, in a binary task with 95% prevalence ofone category, two 90% accurate annotators havea chance-adjusted agreement of 0.9?(.952+.052)1?
(.952+.052) =?.053.
Thus high chance-adjusted inter-annotatoragreement is not a necessary condition for a high-quality corpus.
(4) Inter-annotator agreementstatistics implicitly assume annotators are unbi-ased; if they are biased in the same direction, as weshow they are for the sense data considered here,then agreement is an overestimate of their accu-racy.
In the extreme case, in a binary labeling task,two adversarial annotators who always provide thewrong answer have a chance-adjusted agreementof 100%.
(5) Item-level effects such as difficultycan inflate levels of agreement-in-error.
For ex-ample, hard-to-identify names in a named-entitycorpus have correlated false negatives among an-notators, leading to higher agreement-in-error thanwould otherwise be expected.
(6) Inter-annotatoragreement statistics are rarely computed with con-fidence intervals, which can be quite wide evenunder optimistic assumptions of no annotator biasor item-level effects.
In a sample of MASC wordsense data, 100 annotations by 80% accurate an-notators produce a 95% interval for accuracy of+/- 6%.
Agreement statistics have even wider er-ror bounds.
This introduces enough uncertainty tospan the rather arbitrary decision boundaries foracceptable agreement.Model-Based Inference: In contrast to agreementmetrics, application of a model of annotation canprovide information about the certainty of param-eter estimates.
The model of annotation presentedin the next section includes as parameters the truecategories of items in the corpus, and also theprevalence of each label in the corpus and eachannotator?s accuracies and biases by category.3 A Probabilistic Annotation ModelA probabilistic model provides a recipe to ran-domly ?generate?
a dataset from a set of modelparameters and constants.1 The utility of a math-ematical model lies in its ability to support mean-ingful inferences from data, such as the true preva-lence of a category.
Here we apply the probabilis-tic model of annotation introduced in (Dawid andSkene, 1979); space does not permit detailed dis-1In a Bayesian setting, the model parameters are them-selves modeled as randomly generated from a prior distribu-tion.n iin jjn yn1 1 1 42 1 3 13 192 17 5............Table 1: Table of annotations y indexed by wordinstance ii and annotator jj.cussion here of the inference process (this will beprovided in a separate paper that is currently inpreparation).
Dawid and Skene used their modelto determine a consensus among patient historiestaken by multiple doctors.
We use it to estimatethe consensus judgement of category labels basedon word sense annotations provided by multipleMechanical Turkers.
Inference is driven by accu-racies and biases estimated for each annotator ona per-category basis.Let K be the number of possible labels or cate-gories for an item, I the number of items to anno-tate, J the number of annotators, and N the totalnumber of labels provided by annotators, whereeach annotator may label each instance zero ormore times.
Each annotation is a tuple consist-ing of an item ii ?
1:I , an annotator jj ?
1:J ,and a label y ?
1:K. As illustrated in Table 1, weassemble the annotations in a database-like tablewhere each row is an annotation, and the values ineach column are indices over the item, annotator,and label.
For example, the first two rows showthat on item 1, annotators 1 and 3 assigned labels4 and 1, respectively.
The third row says that foritem 192 annotator 17 provided label 5.Dawid and Skene?s model includes parameters?
zi ?
1:K for the true category of item i,?
pik ?
[0, 1] for the probability that an item isof category k, subject to?Kk=1 pik = 1, and?
?j,k,k?
?
[0, 1] for the probabilty that annota-tor j will assign the label k?
to an item whosetrue category is k, subject to?Kk?=1 ?j,k,k?
=1.The generative model first selects the true cate-gory for item i according to the prevalence of cat-egories, which is given by a Categorical distribu-tion,2zi ?
Categorical(pi).2The probability of n successes inm trials has a binomialdistribution, with each trial (m=1) having a Bernoulli dis-tribution.
Data with more than two values has a multinomial189Word Pos Senses ?
Agreementcurious adj 3 0.94 0.97late adj 7 0.84 0.89high adj 7 0.77 0.91different adj 4 0.13 0.60severe adj 6 0.05 0.32normal adj 4 0.02 0.38strike noun 7 0.89 0.93officer noun 4 0.85 0.91player noun 5 0.83 0.93date noun 8 0.48 0.58island noun 2 0.10 0.78success noun 4 0.09 0.39combination noun 7 0.04 0.73entitle verb 3 0.99 0.99mature verb 6 0.86 0.96rule verb 7 0.85 0.90add verb 6 0.55 0.72help verb 8 0.26 0.58transfer verb 9 0.22 0.42ask verb 7 0.10 0.37justify verb 5 0.04 0.82Table 2: Agreement results for MASC words withthe three highest and lowest ?
scores, by part ofspeech, along with additional words discussed inthe text (boldface).The observed labels yn are generated based onannotator jj[n]?s responses ?jj[n], z[ii[n]] to itemsii[n] whose true category is zz[ii[n]],yn ?
Categorical(?jj[n], z[ii[n]]).We use additively smoothed maximum likelihoodestimation (MLE) to stabilize inference.
This isequivalent to maximum a posteriori (MAP) estima-tion in a Bayesian model with Dirichlet priors,?j,k ?
Dirichlet(?k) pi ?
Dirichlet(?
).The unsmoothed MLE is equivalent to the MAP es-timate when ?k and ?
are unit vectors.
For ourexperiments, we added a tiny fractional count tounit vectors, corresponding to a very small degreeof additive smoothing applied to the MLE.4 MASC Word Sense Sentence CorpusMASC (Manually Annotated SubCorpus) is a veryheterogeneous 500,000 word subset of the OpenAmerican National Corpus (OANC) with 16 typesof annotation.3 MASC contains a separate wordsense sentence corpus for 116 words nearly evenlydistribution (a generalization of the binomial).
Each trial thenresults in one of k outcomes with a categorical distribution.3Both corpora are available from http://www.anc.org.
The crowdsourced MASC words and labels will alsobe available for download.balanced among nouns, adjectives and verbs (Pas-sonneau et al 2012a).
Each sentence is drawnfrom the MASC corpus, and exemplifies a partic-ular word form annotated for a WordNet sense.To motivate our aim, which is to compare MASCword sense annotations with the annotations wecollected through crowdsourcing, we review theMASC word sense corpus and some of its limita-tions.College students from Vassar, Barnard, andColumbia were trained to carry out the MASC wordsense annotation (Passonneau et al 2012a).
Mostannotators stayed with the project for two to threeyears.
Along with general training in the anno-tation process, annotators trained for each wordon a sample of fifty sentences to become famil-iar with the sense inventory through discussionwith Christiane Fellbaum, one of the designersof WordNet, and if needed, to revise the senseinventory for inclusion in subsequent releases ofWordNet.
After the pre-annotation sample, an-notators worked independently to label 1,000 sen-tences for each word using an annotation tool thatpresented the WordNet senses and example us-ages, plus four variants of none of the above.
Pas-sonneau et aldescribe the training and annotationtools in (2012b; 2012a).
For each word, 100 of thetotal sentences were annotated by three or four an-notators for assessment of inter-annotator reliabil-ity using pairwise agreement and Krippendorff?s?.The MASC agreement measures varied widelyacross words.
Table 2 shows for each part ofspeech the words with the three highest and threelowest ?
scores, along with additional words ex-emplified below (boldface).4 The ?
values in col-umn 2 range from a high of 0.99 (for entitle, verb,3 senses) to a low of 0.02 (normal, adjective, 3senses).
Pairwise agreement (column 3) has simi-larly wide variation.
Passonneau et al(2012b) ar-gue that the differences were due in part to the dif-ferent words: each word is a new annotation task.The MASC project deviated from the best prac-tices described in section 2 in that there was noiteration to achieve some threshold of agreement.All annotators, however, had at least two phasesof training.
Table 2 illustrates that annotators canagree on words with many senses, but at the sametime, there are many words with low agreement.4This table differs from a similar one Passonneau et algive in (2012b) due to completion of more words and otherupdates.190Even with high agreement, the measures reportedin Table 2 provide no information about word in-stance quality.5 Crowdsourced Word Sense AnnotationAmazon Mechanical Turk is a venue for crowd-sourcing tasks that is used extensively in the NLPcommunity (Callison-Burch and Dredze, 2010).Human Intelligence Tasks (HITs) are presented toturkers by requesters.
For our task, we used 45randomly selected MASC words, with the samesentences and WordNet senses the trained MASCannotators used.
Given our 1,000 instances perword, for a category whose prevalence is as lowas 0.10 (100 examples expected), the 95% intervalfor observed examples, assuming examples are in-dependent, will be 0.10 ?
0.06.
One of our futuregoals for this data is to build item difficulty into theannotation model, so we collected 20 to 25 labelsper item to get reasonable confidence intervals forthe true label.
This will also sharpen our estimatesof the true category significantly, as estimated er-ror goes down as 1/?n with n independent anno-tations; confidence intervals must be expanded ascorrelation among annotator responses increasesdue to annotator bias or item-level effects such asdifficulty or subject matter.In each HIT, turkers were presented with tensentences for each word, with the word?s senseslisted below each sentence.
Each HIT had a shortparagraph of instructions indicating that turkerscould expect their time per HIT to decrease as theirfamiliarity with a word?s senses increased (wewanted multiple annotations per turker per wordfor tighter estimates of annotator accuracies andbiases).To insure a high proportion of instances withhigh quality inferred labels, we piloted the HIT de-sign and payment regimen with two trials of twoand three words each, and discussed both withturkers on the Turker Nation message board.
Thefinal procedure and payment were as follows.
Toavoid spam workers, we required turkers to havea 98% lifetime approval rating and to have suc-cessfully completed 20,000 HITs.
Our HITs wereautomatically approved after fifteen minutes.
Weconsidered manual approval and programming amore sophisticated approval procedure, but bothwere deemed too onerous given the scope ofour task.
Instead, we monitored performance ofturkers across HITs by comparing each individ-ual turker?s labels to the current majority labels.Turkers with very poor performance were warnedto take more care, or be blocked from doing fur-ther HITs.
Of 228 turkers, five were blocked, withone subsequently unblocked.
The blocked turkerdata is included in our analyses and in the fulldataset, which will be released in the near future;the model-based approach to annotation is effec-tive at adjusting for inaccurate annotators.6 Annotator Accuracy and BiasThrough maximum likelihood estimation of theparameters of the Dawid and Skene model, an-notators?
accuracies and error biases can be esti-mated.
Figure 1a) shows confusion matrices in theform of heatmaps that plot annotator responses bythe estimated true labels for four of the 57 annota-tors who contributed labels for add-v (the affixes-v and -n represent part of speech).
This wordhad a reliability of ?=0.56 for four trained MASCannotators on 100 sentences and pairwise agree-ment=0.73.
Figure 1b) shows heatmaps for four ofthe 49 annotators on help-v, which had a reliabilityof ?=0.26 for the MASC annotators, with pairwiseagreement=0.58.
As indicated in the figure keys,darker cells have higher probabilities.
Perfect ac-curacy of annotator responses (agreement with theinferred reference label) would yield black squareson the diagonal, with all the off-diagonal squaresin white.The two figures show that the turkers weregenerally more accurate on add-v than on help-v, which is consistent with the differences in theMASC agreement on these two words.
In contrastto the knowledge gained from agreement metrics,inference based on the annotation model providesestimates of bias towards specific category values.Figure 1a shows the bias of these annotators tooveruse WordNet sense 1 for help-v; bias appearsin the plots as an uneven distribution of grey boxesoff the main diagonal.
Further, there were no as-signments of senses 6 or 8 for this word.
The fig-ures provide a succinct visual summary that therewere more differences across the four annotatorsfor help-v than for add-v, with more bias towardsoveruse of not only sense 1, but also senses 2 (an-notators 8 and 41) and 3 (annotator 9).
When an-notator 8 uses sense 1, the true label is often sense6, thus illustrating how annotators provide infor-mation about the true label even from inaccurateresponses.191(a) Four of 57 annotators for add-v(b) Four of 49 annotators for help-vFigure 1: Heatmaps of annotators?
accuracies and biasesFor the 45 words, average accuracies per wordranged from 0.05 to 0.86, with most words show-ing a large spread.
Examination of accuracies bysense shows that accuracy was often highest forthe more frequent senses.
Accuracy for add-vranged from 0.25 to 0.73, but was 0.90 for sense1, 0.79 for sense 2, and much lower for senses6 (0.29) and 7 (0.19).
For help-v, accuracy wasbest on sense 1 (0.73), which was also the mostfrequent, but it was also quite good on sense 4(0.64), which was much less frequent.
Accuracieson senses of help-v ranged from 0.11 (senses 5, 7,and other) to 0.73 (sense 1).7 Estimates for Prevalence and LabelsThat the Dawid and Skene model allows an-notators to have distinct biases and accuraciesshould match the intuitions of anyone who hasperformed annotation or collected annotated data.The power of their parameterization, however,shows up in the estimates their model yields forcategory prevalence (rate of each category) and forthe true labels on each instance.
Figure 2 con-trasts five ways to estimate the sense prevalenceof MASC words, two of which are based on modelsestimated via MLE.
The MLE estimates each havean associated probability, thus a degree of cer-tainty, with more certain estimates derived fromthe larger sets of crowdsourced labels (AMT MLE).MASC Freq is a simple ratio.
Majority voted labelstend to be superior to single labels, but do not takeannotators?
biases into account.The plots for the four words in Figure 2 are or-dered by their ?
scores from four trained MASCannotators (see Table 2).
There is a slight trendfor the various estimates to diverge less on wordswhere agreement is higher.
The notable result,however, is that for each word, the plot demon-strates one or more senses where the AMT MLE es-timate differs markedly from all other estimates.For add-v, the AMT MLE estimate for sense 1 ismuch lower (0.51) than any of the other measures(0.61-0.64).
For date-n, the AMT MLE estimate forsense 4 is much closer to the other estimates thanAMT Maj, which sugggests that some AMT an-notators are baised against sense 4.
The AMT MLEestimates for senses 6 and 7 are quite distinct.
Forhelp-v, the AMT MLE estimates for senses 1 and 6are also very distinct.
For ask-v, there are moredifferences across all estimates for senses 2 and 4,with the AMT MLE estimate neither the highest northe lowest.The estimates of label quality on each item areperhaps the strongest reason for turning to model-based approaches to assess annotated data.
For thesame four words discussed above, Table 3 showsthe proportion of all instances that had an esti-mated true label where the label probability wasgreater than or equal to 0.99.
For these words with?
scores ranging from 0.10 (ask-v) to 0.55 (add-v),the proportion of very high quality inferred truelabels ranges from 81% to 94%.
Even for help-v, of the remaining 19% of instances, 13% haveprobabilities greater than 0.75.
Table 3 also shows1920.000.100.200.300.400.500.60Other Sense 1 Sense 2 Sense 3 Sense 4 Sense 5 Sense 6add-v MASC FreqMASC MajMASC MLEAMT MajAMT MLE(a) add-v (?
= 0.55, agreement=0.72)0.000.100.200.300.400.500.60Other Sense1Sense2Sense3Sense Sense5Sense6Sense7Sense8date -n MASC FreqMASC MajMASC MLEAMT MajAMT MLE(b) date-n (?
= 0.48, agreement=0.58)0.000.100.200.300.400.500.60Other Sense1Sense2Sense3Sense4Sense5Sense6Sense7Sense8hel p -v MASC FreqMASC MajMASC MLEAMT MajAMT MLE(c) help-v (?
= 0.26, agreement=0.58)0.000.100.200.300.400.500.60Other Sense 1 Sense 2 Sense 3 Sense 4 Sense 5 Sense 6 Sense 7ask -v MASC FreqMASC MajMASC MLEAMT MajAMT MLE(d) ask-v (?
= 0.10, agreement=0.37)Figure 2: Prevalence estimates for 4 MASC words; (MASC Freq) frequency of each sense in ?
1, 000singly-annotated instances from the trained MASC annotators; (MASC Maj) frequency of majority votesense in ?100 instances annotated by four trained MASC annotators; (MASC MLE) estimated probabilityof each sense in the same 100 instances annotated by four MASC annotators, using MLE; (AMT Maj)frequency of each majority vote sense for ?
1000 instances annotated by ?
25 turkers; (AMT MLE)estimated probability of each sense in the same ?1000 instances annotated by ?25 turkers, using MLESense k ?
0.99 Prop.0 9 0.011 461 0.482 135 0.143 107 0.114 50 0.055 50 0.056 93 0.10SubTot 905 0.94Rest 62 0.06(a) add-v: 94%Sense k ?
0.99 Prop.0 19 0.021 68 0.072 19 0.023 83 0.094 173 0.185 190 0.206 133 0.147 236 0.258 5 0.01SubTot 926 0.97Rest 33 0.03(b) date-n: 97%Sense k ?
0.99 Prop.0 0 0.001 279 0.302 82 0.093 201 0.214 24 0.035 0 0.006 169 0.187 0 0.008 5 0.01SubTot 760 0.81Rest 180 0.19(c) help-v: 81%Sense k ?
0.99 Prop.0 6 0.011 348 0.362 177 0.183 9 0.014 251 0.265 0 06 0 07 6 0.018 6 0.01SubTot 803 0.83Rest 163 0.17(d) ask-v: 83%Table 3: Proportion of high quality labels per word193that the high quality labels for each word are dis-tributed across many of the senses.
Of the 45words studied here, 22 had ?
scores less than 0.50from the trained annotators.
For 42 of the same45 words, 80% of the inferred true labels have aprobability higher than 0.99.In contrast to current best practices, an annota-tion model yields far more information about themost essential aspect of annotation efforts, namelyhow much uncertainty is associated with each goldstandard label, and how the uncertainty is dis-tributed across other possible label categories foreach instance.
An equally important benefit comesfrom a comparison of the cost per gold standardlabel.
Over the course of a five-year period thatincluded development of the infrastructure, theundergraduates who annotated MASC words werepaid an estimated total of $80,000 for 116 words?
1000 sentences per word, which comes to a unitcost of $0.70 per ground truth label.
In a 12 monthperiod with 6 months devoted to infrastructure andtrial runs, we paid 224 turkers a total of $15,000for 45 words?
1000 sentences per word, for a unitcost of $0.33 per ground truth label.
In short, theAMT data cost less than half the trained annotatordata.8 Related WorkThe model proposed by Dawid and Skene (1979)comes out of a long practice in epidemiologyto develop gold-standard estimation.
Albert andDodd (2008) give a relevant discussion of dis-ease prevalence estimation adjusted for accuracyand bias of diagnostic tests.
Like Dawid andSkene (1979), Smyth (1995) used unsupervisedmethods to model human annotation of craters onimages of Venus.
In the NLP literature, Bruceand Wiebe (1999) and Snow et al(2008) usegold-standard data to estimate Dawid and Skene?smodel via maximum likelihood; Snow et alshowthat combining noisy crowdsourced annotationsproduced data of equal quality to five distinct pub-lished gold standards.
Rzhetsky et al(2009) andWhitehill et al(2009) estimate annotation mod-els without gold-standard supervision, but nei-ther models annotator biases, which are criti-cal for estimating true labels.
Klebanov andBeigman (2009) discuss censoring uncertain itemsfrom gold-standard corpora.
Sheng et al(2008)apply similar models to actively select the next la-bel to elicit from annotators.
Smyth et al(1995),Rogers et al(2010), and Raykar et al(2010)all discuss the advantages of learning and evalu-ation with probabilistically annotated corpora.
Bynow crowdsourcing is so widespread that NAACL2010 sponsored a workshop on ?Creating Speechand Language Data With Amazons MechanicalTurk?
and in 2011, TREC added a crowdsourcingtrack.9 ConclusionThe case study of word sense annotation presentedhere demonstrates that in comparison to currentpractice for assessment of annotated corpora, anannotation model applied to crowdsourced labelsprovides more knowledge and higher quality goldstandard labels at lower cost.
Those who woulduse the corpus for training benefit because theycan differentiate high from low confidence la-bels.
Cross-site evaluations of word sense dis-ambiguation systems could benefit because thereare more evaluation options.
Where the mostprobable label is relatively uncertain, systems canbe penalized less for an incorrect but close re-sponse (e.g., log loss).
Systems that produce senserankings for each instance could be scored us-ing metrics that compare probability distributions,such as Kullbach-Leibler divergence (Resnik andYarowsky, 2000).
Wider use of annotation mod-els should lead to more confidence from users incorpora for training or evaluation.AcknowledgmentsThe first author was partially supported by fromNSF CRI 0708952 and CRI 1059312, and thesecond by NSF CNS-1205516 and DOE DE-SC0002099.
We thank Shreya Prasad for datacollection, Mitzi Morris for feedback on the paper,Marilyn Walker for advice on Mechanical Turk,and Nancy Ide, Keith Suderman, Tim Brown andMitzi Morris for help with the sentence data.ReferencesPaul S. Albert and Lori E. Dodd.
2008.
On esti-mating diagnostic accuracy from studies with mul-tiple raters and partial gold standard evaluation.Journal of the American Statistical Association,103(481):61?73.Ron Artstein and Massimo Poesio.
2008.
Inter-coderagreement for computational linguistics.
Computa-tional Linguistics, 34(4):555?596.194Rebecca F. Bruce and Janyce M. Wiebe.
1998.
Word-sense distinguishability and inter-coder agreement.In Proceedings of Empirical Methods in NaturalLanguage Processing.Rebecca F. Bruce and Janyce M. Wiebe.
1999.
Recog-nizing subjectivity: a case study of manual tagging.Natural Language Engineering, 1(1):1?16.Chris Callison-Burch and Mark Dredze.
2010.
Cre-ating speech and language data with Amazon?s Me-chanical Turk.
In Proceedings of the NAACL HLT2010 Workshop on Creating Speech and LanguageData with Amazon?s Mechanical Turk, pages 1?12.Jacob Cohen.
1960.
A coefficient of agreementfor nominal scales.
Educational and PsychologicalMeasurement, 20:37?46.A.
P. Dawid and A. M. Skene.
1979.
Maximum likeli-hood estimation of observer error-rates using the EMalgorithm.
Journal of the Royal Statistical Society.Series C (Applied Statistics), 28(1):20?28.Barbara di Eugenio and Michael Glass.
2004.
Thekappa statistic: A second look.
Computational Lin-guistics, 30(1):95?101.Barbara di Eugenio.
2000.
On the usage of kappato evaluate agreement on coding tasks.
In Proceed-ings of the Second International Conference on Lan-guage Resources and Evaluation (LREC).Katrin Erk and Diana McCarthy.
2009.
Graded wordsense assignment.
In Proceedings of the Conferenceon Empirical Methods in Natural Language Pro-cessing.Christiane Fellbaum.
1998.
WordNet: An ElectronicLexical Database.
MIT Press, Cambridge, MA.Nancy Ide and Yorick Wilks.
2006.
Making senseabout sense.
In Word Sense Disambiguation: Al-gorithms and Applications, pages 47?74.
SpringerVerlag.Beata Beigman Klebanov and Eyal Beigman.
2009.From annotator agreement to noise models.
Com-putational Linguistics, 35(4):495?503.Klaus Krippendorff.
1980.
Content analysis: An in-troduction to its methodology.
Sage Publications,Beverly Hills, CA.Rebecca J. Passonneau, Collin F. Baker, ChristianeFellbaum, and Nancy Ide.
2012a.
The MASCword sense corpus.
In Nicoletta Calzolari (Con-ference Chair), Khalid Choukri, Thierry Declerck,Mehmet Uur Doan, Bente Maegaard, Joseph Mar-iani, Jan Odijk, and Stelios Piperidis, editors, Pro-ceedings of the Eight International Conference onLanguage Resources and Evaluation (LREC?12), Is-tanbul, Turkey.
European Language Resources As-sociation (ELRA).Rebecca J. Passonneau, Vikas Bhardwaj, Ansaf Salleb-Aouissi, and Nancy Ide.
2012b.
Multiplicity andword sense: evaluating and learning from multi-ply labeled word sense annotations.
Language Re-sources and Evaluation, 46(2):219?252.Vikas C. Raykar, Shipeng Yu, Linda H. Zhao, Ger-ardo Hermosillo Valadez, Charles Florin, Luca Bo-goni, and Linda Moy.
2010.
Learning from crowds.Journal of Machine Learning Research, 11:1297?1322.Philip Resnik and David Yarowsky.
2000.
Distinguish-ing systems and distinguishing senses: New evalua-tion methods for word sense disambiguation.
Natu-ral Language Engineering, 5(3):113?133.Simon Rogers, Mark Girolami, and Tamara Polajnar.2010.
Semi-parametric analysis of multi-rater data.Statistical Computing, 20:317?334.Andrey Rzhetsky, Hagit Shatkay, and W. John Wilbur.2009.
How to get the most out of your curation ef-fort.
PLoS Computational Biology, 5(5):1?13.Victor S. Sheng, Foster Provost, and Panagiotis G.Ipeirotis.
2008.
Get another label?
improving dataquality and data mining using multiple, noisy label-ers.
In Proceedings of the Fourteenth ACM Inter-national Conference on Knowledge Discovery andData Mining (KDD).Padhraic Smyth, Usama Fayyad, Michael Burl, PietroPerona, and Pierre Baldi.
1995.
Inferring groundtruth from subjectively-labeled images of Venus.
InAdvances in Neural Information Processing Systems7, pages 1085?1092.
MIT Press.Rion Snow, Brendan O?Connor, Daniel Jurafsky, andAndrew Y. Ng.
2008.
Cheap and fast - butis it good?
evaluating non-expert annotations fornatural language tasks.
In Proceedings of Em-pirical Methods in Natural Language Processing(EMNLP), pages 254?263, Honolulu.Jacob Whitehill, Paul Ruvolo, Tingfan Wu, JacobBergsma, and Javier Movellan.
2009.
Whose voteshould count more: Optimal integration of labelsfrom labelers of unknown expertise.
In Proceedingsof the 24th Annual Conference on Advances in Neu-ral Information Processing Systems.195
