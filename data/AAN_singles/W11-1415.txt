Proceedings of the Sixth Workshop on Innovative Use of NLP for Building Educational Applications, pages 120?129,Portland, Oregon, 24 June 2011. c?2011 Association for Computational LinguisticsReadability Annotation: Replacing the Expert by the CrowdPhilip van OostenLT3, Language and Translation Technology Team, University College GhentGroot-Brittannie?laan 45, 9000 Ghent, BelgiumDepartment of Applied Mathematics and Computer Science, Ghent UniversityKrijgslaan 281 (S9), 9000 Ghent, Belgiumphilip.vanoosten@hogent.beVe?ronique HosteLT3, Language and Translation Technology Team, University College GhentGroot-Brittannie?laan 45, 9000 Ghent, BelgiumDepartment of Linguistics, Ghent UniversityBlandijnberg 2, 9000 Ghent, Belgiumveronique.hoste@hogent.beAbstractThis paper investigates two strategies forcollecting readability assessments, an Ex-pert Readers application intended to collectfine-grained readability assessments from lan-guage experts and a Sort by Readability ap-plication designed to be intuitive and open foreveryone having internet access.
We showthat the data sets resulting from both annota-tion strategies are very similar.
We concludethat crowdsourcing is a viable alternative tothe opinions of language experts for readabil-ity prediction.1 IntroductionThe task of automatically determining the readabil-ity of texts has a long and rich tradition.
This has notonly resulted in a large number of readability formu-las (Flesch, 1948; Brouwer, 1963; Dale and Chall,1948; Gunning, 1952; McLaughlin, 1969), but alsoto the more recent tendency of using insights fromNLP for automatic readability prediction (Schwarmand Ostendorf, 2005; Collins-Thompson and Callan,2004; Pitler and Nenkova, 2008).
Potential appli-cations include the selection of reading material forlanguage learners, automatic essay scoring, the se-lection of online text material for automatic summa-rization, etc.One of the well-known bottlenecks in data-drivenNLP research is the lack of sufficiently large datasets for which annotators provided labels with suffi-cient agreement.
Also readability research is facedwith the crucial obstacle that very few corpora ofgeneric texts exist of which reliable readability in-formation is available (Tanaka-Ishii et al, 2010).When constructing such a corpus, the inherent sub-jectivity of the concept of readability cannot be ig-nored.
The ease with which a given reader cancorrectly identify the message conveyed in a textis, among other things, inextricably related to thereader?s background knowledge of the subject athand (McNamara et al, 1993).
The construction ofa corpus, which can serve as a gold standard againstwhich new scoring or ranking systems can be tested,thus requires a multifaceted approach taking into ac-count both the properties of the text under evaluationand those of the readers.
In recent years, a tendencyseems to have arisen to also explicitly address thissubjective aspect of readability.
Pitler and Nenkova(2008), for example, base their readability predic-tion method exclusively on the extent to which read-ers found a text to be ?well-written?
and Kate et al(2010) take the assessments supplied by a numberof experts as their gold standard, and test their read-ability prediction method as well as assessments bynovices against these expert opinions.In this paper, we report on two methodologiesto construct a corpus of readability assessments,which can serve as a gold standard against whichnew scoring or ranking systems can be tested.
Bothmethodologies were used for collecting readabil-ity assessments of Dutch and English texts.
Sincethese data collection experiments for English onlyrecently started, the focus in this paper will be on120Dutch.
By collecting multiple assessments per text,the goal was to level out the reader?s backgroundknowledge and attitude.
We will both report ona data collection experiment designed for languageexperts and a simple crowdsourcing experiment.We will introduce inter-annotator agreement andcalculate K scores in different settings.
We willshow that from the two readability assessment appli-cations, two very similar data sets are obtained, withcalculations of Pearson correlations of at least 87 %,and conclude that the simple crowdsourcing resultsare a viable alternative to the assessments resultingfrom expert labelings.In section 2, we describe the data from languageexperts and how those data can be converted to rela-tive assessments.
Section 3 outlines a simpler crow-sourcing application and its correspondences withthe experts.
Finally, in section 4, we draw conclu-sions and give a short summary of future work.2 Readability assessment by the expertreaderSince readability prediction was initially primarilydesigned to identify reading material suited to thereading competence of a given individual, most ofthe existing data sets are drawn from textbooks andother sources intended for different compentencelevels (Franc?ois, 2009; Heilman et al, 2008).
ForDutch, for example, the only large-scale experi-mental readability research (Staphorsius and Krom,1985; Staphorsius, 1994) is limited to texts for el-ementary school children.1 For English, the situa-tion is similar as for Dutch, viz.
a predominant focuson educational corpora.
Recently, an evaluation wasdesigned by LDC in the framework of the DARPAMachine Reading Program (Kate et al, 2010).
Forthis purpose a more general corpus was assembledwhich was not tailored to a specific audience, genreor domain.
Unfortunately, the data are not availablefor further use.
Our research focus is similar and wereport on the collection of readability assessments1Staphorsius (1994), for instance, who conducted the onlylarge-scale experimental readability research in the Dutch-speaking regions, based his research entirely on cloze-testing.
Acloze-test is a reading comprehension test introduced by Rankin(1959) in which test subjects are required to fill in automaticallydeleted words in an unseen text.
It is unclear whether such tasksare actually suitable to estimate the readability of a text.for a corpus of Dutch text, which will be used fortraining and evaluating a readability prediction sys-tem.2.1 Source dataIn order to acquire useful data for the construction ofa gold standard, we implemented the Expert Read-ers application intended for language experts.
Thetexts for the application were chosen from the Lassycorpus (van Noord, 2009), which is syntactically an-notated, and which is currently being enhanced withseveral layers of semantic annotations (Schuurmanet al, 2009).
These annotations will allow us in thefuture to determine the impact of various semantic,syntactic and pragmatic factors on text readability.The small subcorpus consists of 105 texts of be-tween about 100 and 200 words.
Most of the textsare extracted from a larger context, but all are mean-ingful by themselves.
All texts are in Dutch andmost of them originate from Wikipedia or newspa-pers.
Further, the corpus contains parts of domain-specific and official documents, manuals, patient in-formation leaflets and others.
The texts in the sub-corpus have no readability levels assigned, but theyare carefully selected in order to obtain texts with amultitude of readability levels.
Because of the lackof a prior readability assessment, the selection waspurely based on careful, yet intuitive judgment.2.2 Application set-upThe Expert Readers application2 is designed to col-lect readability assessments from language experts.They can express their opinion by ranking texts ona scale of 0 (easy) to 100 (difficult), which allowsthem to compare the texts with each other whileat the same time assigning absolute scores.
Thesefine-grained assessments committed by experts aregrouped into submission batches, holding a num-ber of texts which have been ranked and to whicha score has been assigned.
For each submitted text,we know who sent it when, with which score andalong with which other texts in the same submissionbatch.
The experts can also make use of a so-calledframe of reference, in which texts are kept avail-able over different submission batches.
The same2The Expert Readers application is accessible at thepassword-protected link http://lt3.hogent.be/tools/expert-readers-nl/.121text can occur only once per batch, but can be pre-sented again to the same expert in other batches.Apart from the readability scores and the rankingsin the batches, the experts can also enter commentson what makes each text more or less readable.That allows for qualitative analysis.
We did notask more detailed questions about certain aspectsof readability, because we wanted to avoid influenc-ing the text properties experts pay attention to.
Nei-ther did we inform the experts in any way how theyshould judge readability.
Any presumption aboutwhich features are important readability indicatorswas thus avoided.
Our main interest is to design asystem that is robust enough to model readability asgenerally as possible.In the context of our experiments, we regard peo-ple as language experts if they are native readers pro-fessionally involved with the Dutch language.
Ourcurrent pool of active experts consists of 34 teach-ers, writers and linguists, who have contributed a to-tal of 1862 text scores over 108 submission batches.The experts were all volunteers and were not paidfor their work.
Their instructions consisted of an ex-planation of how the application works on paper andan instruction movie of a couple of minutes.
Thesizes of the submission batches range from 5 to allavailable texts.
Batches with less than 5 texts wereomitted from the data.2.3 Text scores converted to text pairsThe Expert Readers application provided a rich, buthighly fine-grained output.
At first sight, a straight-forward and intuitive way to work with the ExpertReaders data would be to use, for example, the meanreadability score assigned to each text.
Pitler andNenkova (2008) and Kate et al (2010), for example,average out results collected from different readers.However, problems with this approach immediatelyarise.
Results from Anderson and Davison (1986),for example, show for their data set that if the dataon which readability formulas are based, were notaggregated on the school grade level but consideredat the individual level, their predictive power woulddrop from around 80% to an estimated 10%.We observed a similar tendency in the results ofthe expert readers application: Figure 1 illustratesthat different experts employ different standards toassign readability scores to texts.
Being given thellllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllll lllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllll lllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllll0 5 10 15 20 25 30 35020406080100userscoreFigure 1: Different scoring strategies for a subset of ex-perts, showing all text scores aggregated across batcheschoice to label texts with marks between 0 and100, some annotators decided to use a more coarse-grained labeling strategy (e.g.
by using multiples of10 or 20), whereas others used a fine-grained scoring(all marks between 0 and 100).
Furthermore, somepeople seem to be reluctant to assign either high orlow scores, or both, while some others use the fullrange of possible scores.Moreover, the experts delivered their data in sev-eral batches.
The texts presented in each submis-sion batch were selected randomly, which impliesthat the annotator could have been confronted withpredominantly less readable or predominantly morereadable texts, which may have affected his scoring.Furthermore, since each text being added to abatch makes it increasingly difficult for an annota-tor to position this text to the already scored texts,we can assume that the greater the number of textsin a batch, the more effort the annotator did to posi-tion each text correctly in the batch.
We decided toonly take into account submission batches in whichat least 5 texts were compared to each other.
Figure 2clearly shows the variability in the scores assigned tothe texts.There is by no means a notion of a single statis-tical distribution that allows for a useful interpre-tation of the means of the scores.
Since it is farfrom trivial to use the absolute scores assigned bythe experts, we transformed their assessments to arelative scale.
A resulting text pair then consists122Figure 2: Box plots showing the minimum, first quantile,median, third quantile, the maximum and the outliers forthe scores assigned to each textof two texts, accompanied with an assessment thatdesignates which of the two texts is easier than theother one, and to what degree.
The identificationof text pairs is straightforward, since in each batch,each pair of distinct texts presents a text pair, leadingto n?
(n?1)2 pairs per batch.
For the transformationfrom the position of the texts in a batch to a relativeassessment for each text pair, we need to fit the batchsize and number of texts scored in between two textsin the same batch to a measure that indicates the dif-ference in readability between two texts.
In order todo so, a possible formula to map the significance ofthe difference in readability is the following:S =(tB)2?(1?
exp(?B10))in which S is the significance of the difference inreadability, B is the batch size and t is the numberof texts scored in between two texts.The quadratic function(tB)2in the first factor ex-presses that, in order to achieve a greater signifi-cance, the value of t must be more than proportion-ally higher.
Because of the quadratic function, moretexts must be scored in between two texts in order toget a higher significance estimate.
If the quadraticpart would be the only factor, the two outer textsin each batch would always get the highest possiblesignificance estimate.
However, the second factor,1 ?
exp(?
B10)ensures that small batches are less0 20 40 60 80 1000.00.20.40.60.81.0tSignificance estimateB = 5B = 25B = 45B = 65B = 85B = 105Figure 3: S as a function of t for 6 different values of Blikely to result in text pairs with a great difference inreadability.
Figure 3 illustrates S as a function of tfor different batch sizes.0 20 40 60 80 1000.00.20.40.60.81.0PercentileEstimated significanceFigure 4: The relative cumulative frequency of the esti-mated significance scoresA plot of which percentile of the text scores gen-erated from the batches results in which significanceof the difference in readability is shown in Figure 4.The text pairs plotted on the lower left of the figurewill be regarded as text pairs for which the annota-tors assess the readability of both texts in the pairas equal.
The text pairs plotted in the middle of thefigure will be regarded as assessed with a somewhatdifferent readability and those plotted in the upper123right part will be interpreted as text pairs with muchdifference in readability.3 From the expert to the crowdBased on the assumption that the readability of atext can be conceptualized as the extent to which thetext is perceived to be readable by the communityof language users, we also investigated whether acrowdsourcing approach could be a viable alterna-tive to expert labeling.
Crowdsourcing has alreadybeen used with success for NLP applications such asWSD (Snow et al, 2008) or anaphora resolution3.By redesigning readability assessment as a crowd-sourcing application, we hypothesize that no back-ground in linguistics is required to judge the read-ability of a given text.
The Sort by Readability ap-plication4 is designed as a simple crowdsourcing ap-plication to be used by as many users as possible.The site is accessible to anyone having internet ac-cess and very inutitive; the users are not required toprovide personal data.
A screenshot of the crowd-sourcing application is shown in Figure 5.Two texts are displayed simultaneously and theuser is asked to tick one of the following statements?Left: much more difficult ?
Right: much easier?,?Left: somewhat more difficult ?
Right: somewhateasier?, ?Both equally difficult?, ?Left: somewhateasier ?
Right: somewhat more difficult?, ?Left:much easier Right: much more difficult?.
The as-sessments were performed on the same data set thatwas used for the Expert readers application.
The re-spondents were not paid for their work and initiallyrecruited among friends and students.
The only in-structions they were given were the following twosentences on the landing page of the application:Using this tool, you can help us composea readability corpus.
You are shown twotexts of which you can decide which is themore difficult and which is the easier one.We assume that most respondents are native speak-ers of the Dutch language.At the time of writing, 8568 comparisons wereperformed.3http://www.phrasedetectives.org4The Sort by Readability application can be accessedthrough the following link: http://lt3.hogent.be/tools/sort-by-readability-nl/.Figure 6: The number of times each button is pressedin the Sort by Readability application.
The buttons fromleft to right are LME (?Left: much easier ?
Right: muchmore difficult?
), LSE (?Left: somewhat easier ?
Right:somewhat more difficult?
), ED (?Both equally difficult?
),RSE (?Left: somewhat more difficult ?
Right: somewhateasier?)
and RME (?Left: much more difficult ?
Right:much easier?
).The number of times each button in the crowd-sourcing application was pressed is displayed in Fig-ure 6.
The number of times the text on the left wasfound easier is almost exactly the same as the num-ber of times for the right one.
That means that usersof the crowdsourcing application are generally notbiased towards finding texts on one side easier thanon the other side.
Most of the times two texts werecompared, people found that there was a differencein readability.
Only in 28.2% of the cases, people as-sessed both texts as equally difficult.
In 53.6% of thecases, the crowd assigned a slight difference in read-ability and in 18.2%, the readability was assessed asvery different.
Note that not everyone evaluated thesame text pairs.
Moreover, nobody evaluated all thepossible text pairs.Figure 7 shows for both the Expert readers andSort by Readability application the relationship be-tween the proportions with which each text is as-sessed as easier (both much and somewhat easier),equally readable or more difficult (both much andsomewhat more difficult) than any other text.
Inall scatter plots, the texts occur in a sickle-shapedform.
The plots for both data sets look very simi-lar, but there is less variability for the Expert Read-124Figure 5: A screenshot of the Sort by Readability application.ers data.
That may indicate that the Expert Readersapplication actually helps people to provide assess-ments more consistently than the Sort by Readabilityapplication.
Despite these small variations, we canconclude that from the two readability assessmentapplications, two very similar data sets are obtained.3.1 Inter-annotator agreementFor most NLP tasks, there is a tradition to calculatesome measure of inter-annotator agreement (IAA).If this measure is high enough, the data are deemedacceptable to serve as a gold standard.
If not, the un-derlying annotation guidelines can be adapted or fur-ther specified in order to improve the future agree-ment between annotators.
In readability research,however, this practice does not seem to have gainedmuch ground.
Given that many readability pre-diction methods (e.g.
(Flesch, 1948; Staphorsius,1994)) were developed before it became common-place, it is not surprising that inter-annotator agree-ment played no great part in the development ofthose readability formulas.
However, also in themore recent classification-based work on readabilityprediction, we are not aware of such efforts.
Deter-mining inter-annotator agreement for both our an-notation tasks is far from trivial.
In both appli-cations, not all texts received an equal number ofassessments, as shown in Figures 8 and 9.
Sincethis evidently leads to a varying number of assess-ments per text pair (ranging from 1 to 25 for ExpertReaders and from 1 to 8 for Sort by Readability),we took this into account in the calculation of theinter-annotator agreement.
Further, our definitionof readability does not allow annotation guidelines.We explicitly avoided to influence people on whattheir view on readability should be, because we as-sume that their collective view is what defines thereadability of a given text.
Annotation guidelineswould make the definition recursive.
Inter-annotatoragreement is therefore implemented as a descriptivestatistic.
It is not used to further guide the annotationprocess.We calculated the IAA both for the text pairs fromthe Sort by Readability application and the mappedtext pairs resulting from the Expert Readers data.
Toconvert the significance levels of the Expert Read-125easier0.0 0.4 0.8llllllllllllllllllllllllllllllllllllllllllllllllllllll llllllllllllllllllllllllllllllllllllllllll0.00.40.8llllllllllllllllllllllllll lllllllll llll llllllllll lllllllllllllllllllllllllllllllllllllllllllllllll0.00.40.8llllll llllllll l llllll lllllllllllllllllllllll ll llll lllllll llllllll lllllllllll llllllllllllllllllllsamellll lllllllllllllllll ll llllllllllllll lllll ll llllll llllllllll llllllllll llll lllllllllllllllllll0.0 0.4 0.8llllllllllllllllll llllllllllllllllllllllllllllllllllllllllll llllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllll lllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllll lll0.0 0.4 0.80.00.40.8more.difficulteasier0.0 0.4 0.8llllllllllllllllllllllllllllllllllllll llllllllllllllllllllllllllllllllllllllll0.00.40.8llllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllll0.00.40.8lllll lllllllllll llllllllll lllll l lll ll llll llll ll ll lllll ll l llll l ll l ll lll llll lllllsamel lll ll lllllllll ll l l ll lll ll llll llllll lllllllllll llllll ll lll ll ll ll lll lllll ll ll lll0.0 0.4 0.8llllllllllllllllllllllllllll llllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllll llllllllllllllllllllllllllllllllllllllllllll0.0 0.4 0.80.00.40.8more.difficult(a) (b)Figure 7: Proportion of times each text was assessed as easier, equally difficult or more difficult than any other text:(a) for the Sort by Readability data and (b) for the Expert Readers data.Data set # text pairs Setup KExperts 1 ?
10 standard 30 %Experts 11 ?
25 standard 31 %Experts 1 ?
25 standard 30 %Experts 1 ?
10 no same 56 %Experts 11 ?
25 no same 75 %Experts 1 ?
25 no same 60 %Experts 1 ?
10 much difference 95 %Experts 11 ?
25 much difference 98 %Experts 1 ?
25 much difference 96 %Experts 1 ?
10 adjacent 50 %Experts 11 ?
25 adjacent 65 %Experts 1 ?
25 adjacent 54 %Experts 1 ?
10 merged 35 %Experts 11 ?
25 merged 41 %Experts 1 ?
25 merged 37 %Crowd 1 ?
8 standard 44 %Crowd 1 ?
8 no same 66 %Crowd 1 ?
8 much difference 88 %Crowd 1 ?
8 adjacent 59 %Crowd 1 ?
8 merged 50 %Table 1: Kappa statistics for all the different setups.
Thesecond column shows the number of times a text pairmust have been labeled in order to be taken into account.Number of occurrences in a batchNumberoftexts10 20 30 4005102030Figure 8: The distribution of the texts, according to thenumber of submission batches in which they occurred.Only batches with >5 texts were taken into account.ers text pairs as shown in Figure 4 to classes of textpairs like in the Sort by Readability data, we canchoose boundary values for the classes.
As bound-ary values, we chose the significance estimates lead-ing to equal proportions of equally difficult, some-what different or much different text pairs for bothdata sets.
The only possible alternative would be tochoose ad hoc boundaries.
Projection of the num-ber of times each button is pressed in the Sort by126Number of sessions in which text was comparedNumberoftexts90 100 110 12005102030Figure 9: Distribution of the number of sessions each textwas seen in for the Sort by Readability applicationReadability application5 on the Expert Readers dataset, leads to the boundary values displayed as dashedlines in Figure 4.
28 % of the text pairs in both ap-plications are thus labeled as equally readable, while18 % of the pairs are labeled with much difference indifficulty.
Those partitions correspond with bound-ary values of 0.016 and 0.29 for S, respectively.We used K as proposed by Carletta (1996) as ameasure for the agreement between annotators.
Kis given by the following formula:K =P (A)?
P (E)1?
P (E)in which P (A) is the probability that two annotatorsmake the same decision and P (E) is the probabil-ity that the same decision is made coincidently.
ForP (A), we take into account the number of times twoannotators agree about a text pair and the number oftimes they disagree.
The trivial case, when there istotal agreement, simply because a text pair is anno-tated only once, was not taken into account for thecalculation of the kappa statistic.
P (E) is empiri-cally estimated in the standard way.We calculate K in 5 different settings.
In the stan-dard setting, each of the five possible assessmentsfor a text in a text pair is regarded as a separate class,without ordering of the classes.In a second calculation of inter-annotator agree-ment, we considered a click on an adjacent button5See Figure 6for the same text pair as agreement.
By doing so,we took into account that the choice between ?eas-ier?
and ?much easier?
and between ?more diffi-cult?
?much more difficult?
, respectively, is lessstraightforward than the distinction between ?eas-ier?
and ?more difficult?.
Furthermore, the bound-ary between ?both equally difficult?
and ?somewhateasier/more difficult?
could also be considered lesstransparent.In a third calculation, named merged, the classes?easier?
and ?much easier?
on one hand, and ?moredifficult?
and ?much more difficult?
on the otherhand are merged, resulting in three different classes.Finally, we examine two cases in which a part ofthe text pairs are omitted, viz.
no same and muchdifference.
In both cases, a binary classification isperformed.
P (E) now equals 0.5 for both classes,because there are two possible outcomes, with equalprobability.
For no same, the button in the mid-dle was discarded.
The ?easier?
and ?much easier?classes were merged, as well as the ?more difficult?and ?much more difficult?
classes.
In the much dif-ference setting, only the texts labeled as much easieror much more difficult were taken into account.The results of all these calculations are shown inTable 1.
The second column indicates a range of anumber of text pairs, which determines how manytimes a text pair must have been labeled in orderto be taken into account for the calculation of K.The results are variable, depending on how K wascalculated.
For the Expert Readers, we consistentlyobserve higher K values when more labelings arerequired per text pair.One possibility to get an idea of how similar thetwo data sets are is by calculating correlation met-rics, such as the Pearson correlation coefficient.
Inorder to calculate that, a numerical value acquiredfrom both data sets must be attached to each text.For each text, we attached two values per data set,viz.
the proportions of times the text was assessedeither as easier or as more difficult than any othertext.
The correlations between the 4 resulting valuesper text are shown in Table 2.
From those results, itis clear that the data sets are very similar.There are different viable alternatives to constructa gold standard from the data sets.
The type of goldstandard that is needed depends on the learning taskto be performed.
For regression, for example, the127Crowd Crowd Experts Expertseasier more difficult easier more difficultCrowd ?
easier 100 % -93 % 88 % -87 %Crowd ?
more difficult -93 % 100 % -87 % 89 %Experts ?
easier 88 % -87 % 100 % -99 %Experts ?
more difficult -87 % 89 % -99 % 100 %Table 2: Pearson correlations between 4 different metrics calculated based on the assessments by experts or the crowd.The metrics are the proportions of times a text is assessed either as easier or as more difficult than any other text.most suitable gold standard consists of an assign-ment of a readability score to each individual text.Those readability scores can for example be the pro-portion of times each text was assessed as easier thanany other text.
Other possibilities to assign scorescan also lead to a gold standard for regression.
Bi-nary classification is an example of a different learn-ing task, for which the data set doesn?t need to betransformed.
For two texts, a binary classifier at-tempts to determine which is the easiest and whichthe most difficult one.
Further research will focuson how the data sets resulting from both annotationstrategies can be transformed into gold standards.4 Concluding remarksWe have implemented two web applications to col-lect assessments about the readability of texts in aselected corpus: an application intended for lan-guage experts and a crowdsourcing tool.
Althoughboth English and Dutch are targeted, we focused onthe results that were obtained for Dutch.
In orderto compare the resulting readability assessments, weviewed the data as text pairs, for which a relative as-sessment is given.
A comparison of both data setsrevealed that they are very similar, a similarity whichwas numerically confirmed by an analysis with Pear-son?s correlation coefficient.
Finally, we gave ex-amples of how gold standards for different learningtasks canbe constructed from the data sets.We introduced the problem of inter-annotatoragreement into the field of readability prediction andcalculated inter-annotator agreement for both datasets in five different ways.
We show that for thetext pairs which were assessed > 10 times, higherK scores are obtained in each of the different set-tings, which strengthens our confidence that read-ability can be learned from our data sets.We conclude that both data sets are valuable andthat crowdsourcing is a viable alternative to read-ability assessments by language experts.Future work includes a further extension and anal-ysis of the data sets.
Further analysis could also re-veal the ideal way to extract a gold standard from thedata sets.
We will also continue to investigate theimpact of different linguistic features on automaticreadability prediction (van Oosten et al, 2010).AcknowledgmentsThis research was funded by the University CollegeGhent Research Fund.We would like to thank all volunteers who coop-erated with our research by using our web applica-tions.ReferencesRichard C. Anderson and Alice Davison.
1986.
Con-ceptual and Empirical Bases of Readability Formulas.Technical Report 392, University of Illinois at Urbana-Champaign, October.R.
H. M. Brouwer.
1963.
Onderzoek naar de leesmoeili-jkheden van Nederlands proza.
Pedagogische Studie?n,40:454?464.Jean Carletta.
1996.
Assessing Agreement on Classi-fication Tasks: The Kappa Statistic.
ComputationalLinguistics, 22(2):249?254.Kevin Collins-Thompson and Jamie Callan.
2004.
Alanguage modeling approach to predicting reading dif-ficulty.
In Proceedings of HLT / NAACL 2004, Boston,USA, May.Edgar Dale and Jeanne S. Chall.
1948.
A formula forpredicting readability.
Educational research bulletin,27:11?20.Rudolph Flesch.
1948.
A new readability yardstick.Journal of Applied Psychology, 32(3):221?233, June.Thomas Franc?ois.
2009.
Combining a Statistical Lan-guage Model with Logistic Regression to Predict theLexical and Syntactic Difficulty of Texts for FFL.128In Proceedings of the EACL 2009 Student ResearchWorkshop.Robert Gunning.
1952.
The technique of clear writing.McGraw-Hill, New York.Michael Heilman, Kevyn Collins-Thompson, and Max-ine Eskenazi.
2008.
An Analysis of Statistical Modelsand Features for Reading Difficulty Prediction.
In TheThird Workshop on Innovative Use of NLP for Build-ing Educational Applications.Rohit J. Kate, Xiaoqiang Luo, Siddharth Patwardhan,Martin Franz, Radu Florian, Raymond J. Mooney,Salim Roukos, and Chris Welty.
2010.
Learning toPredict Readability using Diverse Linguistic Features.In 23rd International Conference on ComputationalLinguistics.G.
Harry McLaughlin.
1969.
SMOG grading ?
a newreadability formula.
Journal of Reading, pages 639?646.Danielle S. McNamara, Eileen Kintsch, Nancy ButlerSonger, and Walter Kintsch.
1993.
Are good textsalways better?
Interactions of text coherence, back-ground knowledge, and levels of understanding inlearning from text.
Technical report, Institute of Cog-nitive Science, University of Colorado.Emily Pitler and Ani Nenkova.
2008.
Revisiting Read-ability: A Unified Framework for Predicting TextQuality.
In EMNLP, pages 186?195.
ACL.Earl F. Rankin.
1959.
The cloze procedure: its validityand utility.
Eighth Yearbook of the National ReadingConference, 8:131?144.Ineke Schuurman, Ve?ronique Hoste, and Paola Monach-esi.
2009.
Cultivating Trees: Adding Several Se-mantic Layers to the Lassy Treebank in SoNaR.
InProceedings of the 7th International Workshop onTreebanks and Linguistic Theories, Groningen, TheNetherlands.Sarah E. Schwarm and Mari Ostendorf.
2005.
Read-ing Level Assessment Using Support Vector Machinesand Statistical Language Models.
In Proceedings ofthe 43rd Annual Meeting of the ACL, pages 523?530,Ann Arbor, June.
Association of Computational Lin-guistics.Rion Snow, Brendan O?Connor, Daniel Jurafsky, and An-drew Y. Ng.
2008.
Cheap and fast?but is it good?
:evaluating non-expert annotations for natural languagetasks.
In Proceedings of the Conference on EmpiricalMethods in Natural Language Processing, EMNLP?08, pages 254?263, Stroudsburg, PA, USA.
Associ-ation for Computational Linguistics.Gerrit Staphorsius and Ronald S.H.
Krom.
1985.
Citoleesbaarheidsindex voor het basisonderwijs: verslagvan een leesbaarheidsonderzoek.
Number 36 in Spe-cialistisch bulletin.
Cito Arnhem, april.Gerrit Staphorsius.
1994.
Leesbaarheid enleesvaardigheid.
De ontwikkeling van een domein-gericht meetinstrument.
Cito, Arnhem.Kumiko Tanaka-Ishii, Satoshi Tezuka, and Hiroshi Ter-ada.
2010.
Sorting Texts by Readability.
Computa-tional Linguistics, 36(2):203?227.Gertjan J.M.
van Noord.
2009.
Large Scale SyntacticAnnotation of written Dutch (LASSY), January.Philip van Oosten, Dries Tanghe, and Ve?ronique Hoste.2010.
Towards an Improved Methodology for Auto-mated Readability Prediction.
In Nicoletta Calzolari,Khalid Choukri, Bente Maegaard, Joseph Mariani, JanOdijk, Stelios Piperidis, and Daniel Tapias, editors,Proceedings of the seventh International Conferenceon Language Resources and Evaluation (LREC?10),Valletta, Malta, May.
European Language ResourcesAssociation (EL.129
