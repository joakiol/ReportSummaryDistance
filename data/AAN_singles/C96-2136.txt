Context -Based Spell ing Correction for Japanese OCRMasaak i  NAGATANTT In fo rmat ion  and  Communicat ion  Systems I ,aborator i cs1-2356 Take ,  Yokosuka-Sh i ,  Kanag~wa,  238-03 Japannaga%aent tn ly ,  i s l .
n t t .
j pAbst ractWe present a novel spelling correctionmethod \['or those languages that haveno delimiter between words, such ~rs,lap;mese, (.
',hinese, ,~nd ThM.
It con-sists of an al)proximate word match-ing method and an N-best word segmental|on Mgorithm using a statisticalla.nguage model.
For OCR errors, theproposed word-based correction methodoutperf.ornrs the conventional charactm'-b`ased correction method.
When thebmselme character ecognition accuracyis 90%, it achieves 96.0% characterrecognition accuracy and 96.3% wordsegmentation accuracy, while the cilar-acter recognition accuracy of cilaracter-b,ased correction is1 In t roduct ionAutomatic spelling correction research dates t)ackin the 1960s.
~lbday, there are some excellentacademic ~nd commercial spell checkers available\['or English (Kukich, 1992).
However, for thoselanguages that have a different morphology andwriting system from English, spelling correctionremMns one of the signillcant unsolved researcilproblems in computational linguistics.
'\['he b,asic strategy for English spelling correc-tion is sitnple: Word boundaries are defined bywhite space characters.
If the tokenized string isnot found in the dictionary, it, is either a non-word or an unknown word.
For a. non word,correction candidates axe generated t)y approxi-nm.tely matching the string with the dictionary,using context independent word dis|mice mea-sures such ,as edit distance (Wagner and l,'ischer,1974; Kernighan et M., 19q0).It is impossible to apply these "isolated worderror correction" techniques to Japanese in twore`asons: First, in noisy texts, word tokenizationis difficult because there are no delimiters be-tween words.
Second, context-independent worddistance measures ~re useless because the averageword length is very short (< 2), and the chnra.cterset is huge (> 3000).
There are a large number ofone edit distaalce height)ors for a ,lapanese word.In English spelling correction, "word bounda.ry problem", such as splits (forgot -~ .lot gol)a.nd run-ons (in form --+ in.lbrm), mad "short wordproblem'(ot  -~ on, or, of, at, it, to, etc.)
arealso known to I)e very dilIicult.
Context infofmat|on, such as word N-gram, is used to sup-plement he underlying context-independent co>reel|on tbr these problematic examples (GMe and(~hurch, 1990; Mays et aJ., 1991).
To the contra.ry,Japanese spelling correction must be essentiallycontext-dependent, because Japanese sentence is,as it were, a. run on sequence of short words, pos-sibly including some typos, something like (lfor-.qololinfo'mnyou --~ I forgot to inibrtn you).In this pa.per, we present a novel ~t)proach forspelling correction, which is suite.hie for those l~n-guages that have no delimiter between words, suchf~s aN)anese.
It consists of two stages: First,MI substrings in the input sentence are hypoth-esized ms words, and those words that approxi-mately matched with the substrings axe retrievedfrom the dictionary ms well ,as those that exactlymatched, l{,ased on the statisticM language model,the Nd)est word sequences are then selected ascorrection ca,ndidates from all combinations of ex-actly and approximately matched words.
Figure 1 illustrates this ~pproach.
Out of the listof character ecognition candidates for the inputsentence "~ b~R~7-~,~Y2~g)k~ 79o "which means "to hill out the necessary items in theapplication form.
", the system searches the eombi-n,~tion of exactly matched words (solid boxes) andapl)roximately matched words (dashed boxes) 1The major contribution of this paper is itssolutions of the word boundary problem andshort word problem in Japanese spelling coffee.tion.
lly introducing a statistical model of wordtOCR output tm,ds to be very noisy, est)e(:ially forhand writing.
To (:omt)ensate for this bd,avior, OC'RsusuMly output at, ordered list of tit(!
best N elutra(:ters.The list of the (:~uMid~tes for an int)ut string is calledetl~*ra(:ter m~mix.806input sentencecharacter matrix171 i~ d H }\[:?~ K ,i~, -~ dt fJlitt4 1 D p\] X.
-~t7 ~- t7j~ h.~ ( 7 b ,,7forward search\[\] T_~K-}\[\]A 5~J 3exactly matched wordi .
.
.
.
.
I approximately matched word .
.
.
.
.
.
?I" igure 1: l 'oss ih le ( 1oinl)hia,l,ioliS or l:,XalCl, ly a, nd Atf l )roxhua,l ,ely ~l\] n,l,ched WordsImigl;h a,iid s l /e l lh ig , l,lie proposed sysLei\[l a,Cctira, t, ely phi,ces word  bounda, rics in noisy LexLs 1,1u/,Li l ic lude l iOl l -words n,nd tl l lkl/OWll words.
I ly usingt, he c|la,ra,cl,or I)ased CC~l\]l;0xl; l i lode\],  il; a,c(:ura,1;elyselecl, s (:orr0?1,iOll (umdid~l.es \['or shorL words \['i'Ol\[iLhe h~rge n t l l l l l )cr o\[' .~pproxini;~l;ely ill~l,L(tiletI worctswiLh Lhe slmlc edit, disl;n,nco.The  gold o\[  our  project, is l,o in ip lenienl ,  a, iI h'li,~r~ci.ive word  correcl,or for ~ lia,ndwriLi, m~ FAX OCI{,sysl, eil i .
ldT~ a,re especia, l ly inl;eresl;ed in 1;exl, s t,lia,l,inc lude a,ddresses, IHI,IIICS, ~l,lld \ [ i i essa ,~es ,  such asorder fOrlllS, quesLionnn,ires, a,nd t, ch~gi'ig)h.2 No isy  Channe l  Mode l  forCharacLer  Recogn i t ionFirsL, we (\]lrniula,l,e i;hc, spellinT; c(~rrcct;ion o\[' O(2 I~<~i'rors in I,he noisy cha, nncl  pa, r;~div;ln.
I,el, (< ropresenL 1,tie inpuL sLrhlg a,ncl , \  rct>resenl, l;tie ()(Jlt,oul;Imt, st, rhig.
l "h idh ig  Llie fllosL tm~lmble sf, i 'hig C <given iJle O( ill, oul,ptll, <\" a,nioulil,s I;o ma, xini iz i l i~;u,e l;,,, ,~,io,, r (x  IC ) / - ' ( ( ; ) ,?
: = ,H~ ,,, ,~ n (c ' lX  ) = ,~,.~ ,IID, X \])(X\[~.))\])({.
; ) ( i )(3 {'lieca, use lia,yes' rule sl;;d,es l;ha, L,#'(~'l.V) = s'(xl(:.')#'(~:.')
(.~)/'( X )/ ' (C)  is ca, tied the hmgu;Lge model.
II, is c.nipuled \[l'Oilt l,lw l,ra,ii lhig CoI'\])CiS.
I,et, us cMIP(XI( .
<) l,lte O(', l l ,  n iodel .
IL ca, it |)o conttmLedl 'roni t;iic a, pr ior i  l ike l ihood o,gLilnn,Les for ind iv idua, I cila, ra,cl;ers,isp(x  IC) = l-i/'{":' I<, ) (:~)where 'n is l, he sl,rhlv; leu~>l,h, t'(:l:ilcT) is c;i,IN'dL|ic COlifUsion lun,l, r ix  o\[ ch;i,ra,(tl,or<~.
IL i,g t,i';i,hicdtishlg l,lie hipuL a, nd oul,1)ul, sl;rhigs o\[' Lhe ( ) ( i l lThe  coii\['tiSiOll \ [ l i a l r i x  is h ig | l ly  depentionL elil:iie chn,r;l,ct,er r0co~niLion a,l~orit, iun  a,nd Lhe qua, IiLy of the hlput, docunionl , .
IL is a, l ld)or inl;ensive1;aM~ Lo prepa, re a, con\[tiSiOl/ niaJ;rix lbr ca,oh cha,ra,cl,er recognil Jon sys|,ellt, sillce .I a,p<~liose ha.s lilore1;hail 3,0()() c|lai','l, cl;ers.
'l'here\[7)re, we used a, shnpD ()(\]11, model  wl iere l, l ie confus ion ni~t, r i x  is a>pproximai ,  ed by t, tie correcl; cha,r;~cl,er dist, r ibul ; ionovm" t, he r&nk of Ltie ca,ndicl&Lcs.
We asstttlle t;ll~|;l, hera,  nk erder disl, i;ibul, ion of l,|io correcL clia,ra,cLore is a, geonl0Lric disf, r ibuL ion whoso pa,ra,niet,eris l;he a:CCtll';%cy OI" Lhe firM; ca,ndida, Le.l,ei, c~ be tha 7-i,h c|la, ra,cl,er in Lhe inpuL sl, rin~4,:l: G \])e l,|ie j t,h ca,ndida,t,e \[or (:~, ;uid p 1)o Life prol>idfil i i , y l, ha,i, l, he lh'sl; ca,ndida,Le is correct,. '
l ' l ie COltf'tiSiOll pt'olml>ilil~y \['(;v U \[r:i) is a,ppro?inla,t, od as,r(~:,~l<,,) ~ P(:,:,~ i,~ <:<,,.,.<:<:c) .~ ~,(l p)~ ' (,1)I'klua,l:ion (d) a, hus t,o a,Pl~roxhna,i:e l,he a,ccura,cyof t,hc firsL ca,ndida.im, a,nd tJle l,endency tJmt, t j .-relia.bilit;y of Lhe ca.ndhla.l;e ch'cramses al, 'uptly asits ranlc incr(m.~es.
For exa.mple, ill the recognition~ccura.cy of t, he lirsi; candida.t.e p is 0.75, we wi l l  assign i,he prob;dfilii~y of Lllo Iirst,, secmid~ ;rod i, hirdcn, ndida.i,cs l,o 0.75, 0.\]9, a,i/d (I.05, respect.ively,regli, i'dloss ~lI' Lhe h ipuL a,iid Ollt;pul, cha,r~cl;ers.
(-)11o ~.
)J' Lhe I:lelietil,s o\[  usin<g a, s i l i ip lc  ( ) ( I t{nlodel  is Lha,L Lhe spel l ing correct, ion s.yM,eni becoiiles hi~4hly imh3)endenl, of l,lie underlying; ()( i1{cha,raxq, crisl,ics.
Obv ious ly ,  a, more sophislficaLedO(\]11, n iodol  wou ld  in ip rove  OI'FOF Col'rect;ioli /)el"retina, liCe, hut, eVeli l,his shnlHe O( I I / i l i t>d<q worksfa,h'iy wdl in our eXllerinient,s 72( )11( '  ( i f  (,lit!
I)i'ax:l,iciil r ( ' ,a ,St l l lb  ~'(Jr ilSill,~ Lhc ~tXJ l l lC IL -rh: di~lri lml, hJli i~ ilia, i, we ll~cd l,hc cuid'H,HO, nlal, ri?for ilnl)h!iiicnl, in ~ the O(;R silnlila, l,Ol-.
\'\"c fccl h, i,~i lnfl l ir  I,t) ii~t!
I, hc slii l lc con\[llblon lilaA, rix btJLh ftJl' t!lr()ll'~Clicr&l, ioli illld error corrc, d, lon.
{7 0 73 Word  Segmentat ion  A lgor i thm3.1 Statistical Language ModelFor the language model in Equation (1), we usedthe part of speech trigram nlodel (POS trigranl or2nd-order HMM).
It is used ,as tagging mode\[ inEnglish (Church, 1988; Cutting et al, 1992) andmorphological nalysis nlodel (word segmentationand tagging) in Japanese (Nagata, 1994).Let the input character sequence be (/ =c \ ]c .e .
.
.
c  .... We approxinlate P (C)by  P(W, 7'),the joint prol>ability of' word sequence W =wlw2...'u),~ and part of speech sequence '\[' =t l t .e .
.
,  t,,.
P (W,T)  is then approximated t>y theproduct of parts of speech trigram probabilitiesP(t i \ ]t i - '2,  | i - l )  and word output probabilities forgiven part of speech P(w i l t l ) ,71p(c)  _- p(w,~') --= IX  p(t, l t , -=, t , - , )p(~, l t , )  (5)i=1P( t i l t i - ,e , t i -~)  and /-'(w~lti ) are estimated \[>ycomputing the relative frequencies of the corre-sponding events in training corpus a3.2 Forward-DP  Backward-A* Algorithm\[/sing the language model (5), .Japanese morp\[lo-logical analysis can be detined ,as finding tile setof word segmentation and parts of speech (1~/, 7'')that maximizes the joint probability of word se-quence and tag sequence P(W,  7').
(V?, ~') = ,~,-g,,,~?
P(w,'J') (~)W~ TThis maxinfization search can be efficiently im-plemented t>y using the forward-DP backward-A*algorithm (Nagata, 1994).
It is a natural exten-sion of the Viteri>i algorithm (Church, 1<,)88; Cut-ting et al, 1992) for those languages that do nothave delimiters between words, and it can gener-ate N-best morphological nalysis hypotheses, liketree trellis search (Soong and l\[uang, 1991).The algorithm consists of a forward dynamicprogramming search and a backward A* search.The fbrward search starts from tile beginning ofthe input sentence, and proceeds character bycharacter.
At each point in tile sentence, it looksup the combination of the best partial parses end-ing at the point and word hypotheses starting atthe point.
If the connection between a partialparse and a word hypothesis i allowed by the lan-guage model, that is, the corresponding part ofspeech trigram probability is positive, a new con-tinuation parse is made and registered in the bestpartial path table.
\[,'or example, at point 4 in Fig-ure 1, tile final word of the partial parses ending at4 are ga b~ ('application'), .
~  ('prospect'),SAs a word segmeotal, ion nmdel, the advantage ofthe POS trigram model is that it can be trained usinga smaller <:orpus, than the word bigram mode.1.and ~ ('inclusive'), while tile word hypothe-ses starting at 4 are m?~ ('form'), ~ ('s~ne'), Y\]('moon'), and Fq ('circle').In tile backward A* search, we consider a par-tial parse recorded in the best partial path tat>lc `asa state in A* seareiL 'File backward search startsat tile end of the input sentence, and backtracksto tile beginning of the sentence.
Since the probabilities of the best possible remaining paths areexactly known by the forward search, the back-ward search is admissible.We made two extensions to tile original fbrward-DP backward-A* algorithm to handle OCR out-puts.
First, it retrieves all words in tile dictionarythat match the strings which consist of a combina-tion of the characters in the matrix.
Second, thepath probability is changed to the product of thelanguage model probability and the OCR modelprobability, so as to get the most likely charactersequence, according to Equation (1).4 Word  Mode l  for Non-Words  andUnknown WordsThe identification of non:words and unknownwords is a key to implement Japanese spelling cot-rector, because word identilication error severelyatDets the segmentation of neighboring words.We take tile following approach for this wordboundary problem.
We first tlypothesize all sub:strings in the input sentence as words, and assigna reasonable non-zero probal>ility.
\[,'or example,at point 7 in Figure 1, other than the exactly andapproximately matched words starting at 7 such,as ,.g,~ ('necessary'), ~,'~ ('necessarily'), and alZ('pond'), we tlypothesize the sut>strings ~,, ~,~,~,@~, , .g ,@~,  ... as words.
We then locate themost likely word boundaries using the forward-I)P backward-A* algorithm, taking into accountthe entire sentence.We use a statistical word model to assign aprobat>ility to each subs|ring (Nagata, 1996).
Itis defined as tile joint probability of tile charactersequence if it is an unknown word.
Without lossof generality, we can write,P(~ I<~z>) = p(c~... ,:~ I<~z>)= r(k)P(, : , .
.
.
, ;~lk)  (7)where <'.1 ?
?
?
<'+ is the character sequence of lengthk that constitutes word wi.
We call P(k )  theword length model, and P(c l  ?
.. ck \]k) the spellingnmdel.We assume that word length probability P(k )obeys a Poisson distribution whose parameter isthe average word length A,(.~ __ \] )kThis means that we think word length is the interval between hidden word boundary markers,which are randomly placed where tile average in-terval equals tile average word length.
Although808this word length model is very simple, it plays akey role in making tile word segmentation algorithm rot>ust.We al)proximate the spelling probability givenword length P(el ... ck \]k) |>y tile word-t)a~ed char-acter trigram model, regardless of word length.Since there are more than 3,000 characters inJapanese, tile amount of training data would betoo small if we divided them by word length.@:~-..
"~) -- P(c~ I#, #)P(c= I#, q )kz=3where "#" indicates the word t>oundary marker.Note that tile word-I>,%sed character trigrammodel is different from tile sentence-b~Lsed char-acter trigram model.
'l'he tbrmer is estimatedfrom tile corpus which is segmented into words.
Ita,ssigns large probabilities to character sequencesthat appear within a word, and small probat>ilitiesto those that appear across word boundaries.5 Approximate Match forCorrection CandidatesAs described t>elBre, we hypothesize all sul>stringsin the input sentence ,as words, and retrieve ap:proximately matched words from the dictionaryas correction candidates.
For a word hypoth-.esis, correction candidates are generated basedon tile minimmn edit distance technique (Wag-net anti l!
'ischer, 1974).
Edit distance is definedas the ntiniulum number of editing operations (insertions, deletions, and substitutions) required totransform one string into another.
If tile targetis OCIL output ,  we can restrict tile type of errorsto substitutions only.
Thus, the similarity of twowords can be computed as c/n, where c is tilenund)er of matched characters and n is tile lengthof the misspelled (and dictionary) word.For longer words (._> 3 characters), it is rea:sonable to generate correction candidates t>y re-trieving all words in the dictionary with similarityabove a certain threshold (eta >_ 0.5).
For exam-pie, at point 0 in Figure 1, g+ b~ ('application')is retrieved by approximately ntatching the stringItt L~;9- with the dictionary (c/n = 3/4 = 0.75).Ilowever, tbr short words (1 or 2 characterword), this strategy is unrealistic because thereare a large numt>cr of words with one edit dis-lance.
Since the total nund)er of one characterwords and two <:haracter words an lounts  to  luorethan 80% of the total word tokens in Japanese, wecannot neglect hese short words.It is natural to resort to context-dependentword correction methods to overcome tile shortword prol>lem.
In English, ((-;ale and (\]hurch,199(t) achieved good spelling check performanceusing word bigranLs, llowever, in ,lapanese, wecannot use word bigram to rank correction can-didates, because we have to rank them betbre wepertbrm word segnmntation.Therefbre, we used character context instead ofword context.
For a short word, correction candi-dates with the same edit distance are ranked bytile joint probability of tile previous and tile fol-lowing two characters in the context.
This probwbility is computed using the sentence-based char-acter trigram model.
For 2 character words, forexample, we first retrieve a set of words in thedictionary that match exactly one character withthe one in the input string.
We then compute the6 grant probability Ibr all candidate words .siSi+l,and rank them according to the prot>ability.P(c,_2,  c i - l ,  .sl, si+.t , ci+:~, ci+a ) :P(.s'i lci-~, c l - t  ) P(s i+ l  \]ci 4, .
'~i)P(ci+=lsl,.si+l)P(ci+al.si+t,ci+.2) (10)For example, at point 12 in Figure 1, there aremany two character words whose first characteris ~g, such ~s -gEil~ ('mention'), ~E~4$ ('article'), ~0..~ ('journalist'), gg.zX.
('entry'), g0,,&~, ('commen>oration'), etc.
By using character contexts, tilesystem selects gg)k. anti ~t\]fti;~ as approximatelymatched word hypotheses.6 Experiments6.1 Language Data  and  OCR S imulatorWe used tile NI 'R Dialogue Database (Ehara etel., 1990) to train and test tile spelling correc-tion method.
It is a corpus of approximately800,000 words whose word segmentation anti partok' speech tagging were laboriously performed byhmu\[.
In this experiment, we used one lburth oftile ATR, Corpus, a portion of tile keyboard dia-logues in the conference registration domain.
'l'a-ble 1 shows the nmnber of sentences, words, andcharacters for training anti test data.
The testdata is not included in the training data.
That is,open data were tested in the experiment.Tat>le it: The Amount of 'l?aining and '\[>st DataTraining set Test setSenten<:es 10945 l O0Words 150039 1134C, haracters 268830 2097For the spelling correction experiment, we usedan OC, R simulator because it is very difficult toobtain a large amount of test data with arbitraryrecognition accuracies.
The OCR, simulator takesan input string anti generates a character matrixusing a conflmion matrix for Japanese handwritingOCI{,, developed in our laboratory.
The parame-ters of the OCR sinmlator are tile recognition ac-curacy of the lirst candidate (lirst candklate cor-rect rate), anti tile percentage of tile correct the.r-809acters included in tile character matrix (correctcandidate included rate).In general, the accuracy of current Japanesehandwriting OCR is around 90%.
It is lower thanthat of printed characters (around 98%) due to thewide variability in handwriting.
When the inputcomes from FAX, it degrades another 10% to 15%,because tile resolution of most FAX machines is200dpi, while that of scanners is 400dpi.
There-\['ore, we made \[bur test sets of' character matri-ces whose first candidate correct rates and correctcandidate included rates were (70%, 90%), (80%,95%), (90%, 98%), and (95%, 98%), respectively.The average numt>er of candidates ibr a characterw~s 8.9 in these character matrices 46.2 Character  Recogn i t ion  AccuracyFirst, we compared the proposed word-basedspelling corrector using the POS trigram model(POSe) with tile conventional character I)msedspelling eorreetor using tile character trigrammodel (Char3).
Table 2 shows tile characterrecognition accuracies after error correction \['orvarious b~seline OCR accuracies.
We also changedthe condition of the approximate word match.
InTat)le 2, Matehl, Match2, and Match3 representthat tilt approximate mM;ch fbr substrings whoselengths were more than or equal to one, two, andthree characters, respectively.In generM, tile approximate match for shortwords improves character recognition accuracy byabout one percent.
When the lirst candidate cor-rect rate is low (70% and 80%), tile word basedcorrector significantly outperIbrnL~ tile character-based corrector.
This is because, by approximateword matching, tile word-based corrector can cor-rect words even if the correct, characters are notpresent in the matrix.
When the first candidatecorrect rate is high (90% and 95%), the word-I>~sed corrector still outperl`orms tile characterbased eorrector, although the ditDrenee is small.This is because most correct characters are already included in the ma.trix.Table 2: Comparison of Character RecognitionAccuracy (Character Trigram vs. POS trigra.m)OCR (thou'370% (90%) 74.4%80% (9a%) 8~.0%~),~% (98%) !
)5.o%M~m:h l84.6%~)2..5%96.0%,~)6.~%POSeMatch2 Mateh38a.9% 83.1%92.0% 90.6%95.9% 95.6%96.7% 95.9%~The par~m/eters ~rre sc|ected considering the filetthat the corre.ct candidate included r~ttc increases a.sthe tirst candi(hm~ correct rate incrc~Lscs, a.nd thatNOllle correct characters ~re l|ev(:r \[)resellt ill tile Illg--trix ewm if the first candidate correct ,:~Lt(~ is high.6.3 Word  Segmentat ion  and WordCorrect ion  AccuracyFirst, we deline the performance mea,sures ofJ apanese word segmentation and word correction.We will think of' tile output of tile spelling eor-rector ~ a set of 2-tuples, word segmentation andorthography.
We then compare tile tuples contained in the system's output to tile tuptes con-tained in the standard analysis.
For tile N-bestcandidate, we will make the union of tile tuplescontained in each candidate, in other words, wewill make a word lattice from N-best candidates,and compare them to tile tuples in the standard.For comparison, we count tile number of tuples intile standard (Std), the number of tuples in thesystem output (Sys), and tile number of matchingtuples (M).
We' then calculate recall (M/Std) andprecision (M/Sys) as accuracy measures.We define two degrees of equality among tuplesfor counting the number of matching tuples.
Forword segmentation accuracy, two tuples are equalif they have tile same word segmentation regardless of orthography.
For word correction accuracy,two tuples are equal if they have the same wordsegmentation and orthography.Table 5 shows the words segmentation accuracyand word correction accuracy.
The word segmenration accuracy of tile spelling eorrector is sig-nitieantly high, even if the input is very noisy.For example, when the accuracy of the baselineOCI{.
is 80%, since tile a.verage numlmr of characters and words in the test sentences are 20.1and 11.3, there are 4.0 (=20.1'(1-0.80)) chm'ac-tee errors in the sentence, in average.
Ilowever,94.5% word segmentation recall means that thereare only 0.62 (=11.3'(1-0.945)) word segmentations that are not found in the first candidate.Moreover, we t>el the word correction accuracyin Table 3 is satisfactory \['or an interactive spellingcorrector.
For example, when the accuracy of theb~seline OCI{ is 90%, there are 2.0 (=20.1"(10.90)) cha.racter errors in the test sentence, llowever, 92.8% reca.ll for the first candidate and 95.6%recall for tile top 5 candidates means that thereare only 0.81 (11.3"0-0.928)) words that are notfound in the lirst candidate, and if you exa.minethe top 5 candidates, this wdue is reduced to 0.50(~1.3'(1-0.9S@).
That is, about half of the errors in the lirst candidate are corrected by simplyselecting tile alternatives in the word lattice.7 Discuss ionPrevious works oil Japanese OCR error correctionare l)ased on either the character trigram model ortile part of speech t)igram model.
Their targets areprinted characters, not handwritten characters.That is, they assutne the underlying OCI{.
's accuracy is over 90%.
Moreover, their treatment ofunknown words and short words is rather ad hoe.810'l'a,ble 3: Word  Segmenta.t ion Accura,cy a, nd Word ( Jorrect ion Accuracy for Noisy TextsO(:117o% (9o%)8o% (9~%)9o% (:)8%)95% (98%)Wor(l Segtnent;~tionR(x:M1 (l lest-5) l)re<:ision (l \]est-5)89.o% (9e.1%) ~.a% (752%)94.5% (97.4%) 90.5% (81.7%)96.a% (97.9%) 9a.
(~% (85.s%)97.3% (98.6%) !\]4.8% (86.8%)Wet(| (-',or e(:t h) n1{ c(:all ( l \]est-5) P rc.
(:ision (l}t:st-577.1% (82.4%) 71.a% (58.2%87.9% (92.6%) 84.2% (67.2%!\]2.8% (95.6%) 90.1% (72.1%94.a% (!\]7.0%) !
}1.8% (74.0%('l 'Mmo and Nishino, 1989) used 1)~u't of speech bigra, m a,nd best  \[irsl, sea+rob for ()C,I{, correct ion.They  used heur ist ic  templal ;es \[Lr ttnkllown words.
( 11;o a,nd M a,rtty,'tma,, 1.
()92) used pa,rt of speech I)igraan a, nd \]lea,In search ill order  to get, n iu l t ip lec,'mdidaJ, es in their  int;eracl;ive 0(-:11, correcter  r,The  proposed  Ja,paa\]ese spel l ing correct ionmeLh.od uses pa,rt of speech tr igra,m ;rod N bestsea,reh, Th is  (:oml>ina,l,ion is l, heoretica, lly a, ndpra,ctica,lly i i lore ;l,CCtlr;l, Le (;\[liLII prev ious reel, hods.In add i t ion ,  t>y using sl,a,t;istiea,I word ntodel,  a,ndcc)llteXt; I>a,sed n,l)lm)xin\]a,l,c word \[na, l, ch, il, t)ecomes robust  enottgh |;o }tm~dle very noisy texts,such a,s the ottl,put o\[' FAX O(111, systetns.To improve the word correct ion a,ccuraey, morepowerful  hmgua,ge models ,  stteh as word b igram,are required.
( Je l inek, 1.(.
)85) po in ted  out  that"I)()S (pa,rt of speech)  elassil iea,tion is too crudea,nd not  necessa,rily su i ted  1,o la+ngtutge model ing" .I lowever,  il; is 1;c)o expens ive  to prepa, re a, la,rgem,~nua, lly segmented  (:ort~,tts ()f e;tch l,a, rget doIlia,ill L()(:O\[llpute the word 1)igra,m.
'l'her<q'ore, wea,re th ink ing o\[' ran,king a, set\[" orga,tfized word segmeni;aJ, ion method I)y generMiz ing the l "orwm'dIbtekwa,rd a, lgor i t lml  \['or those hmgua,ges tha, t ha,veno de l imi ter  between words (Na,gaJ, a,, 199(i),8 ConclusionWe h;tve present;ed ~ spe l l ing  eorrecl, ion met,hodtbr noisy ,la,pa,nese texts .
We a,re current lyI>uilding a,n intera,ctive Ja,pa,nese spel l ing corrector jspcll, where words are the I)msic object: ma.nipuhtt, ed 1)y t, he user in ope\]'~l;ions such as repla.ee, a,ceept, and edit .
I t  is someth ing  like theJ a,pa,nese countert)a,rt of I Jnix's spel l ing cor recterispell, with a, user interf~tce s imi lar  to kan(t-lo-ka'njZ converter ,  a, popu\[a,r J a,pa, nese inpul, method~A(:<:ording to Fig.
6 ill (~\]'a,k;to and NMtim), 1989),they achieved iti)Olll, 95(~1 (:ha, ra<:tcr I'C~:Og, tlil, ioII &(:CII-r;t(:y when |,Ira I)ms('.llnr.
~L(:cllr,~l(:y iS 9\[% for ill;tga.-7,ines ~tnd int, ro(\]ll(:lA)ry t(!xl,1)ooks of scien(:(!
and t,e(:ll-nology dmu;tiu.
According to TM)Ic.
I in (11,o ~tndMa, ruya, tn~t, 1992), they itchicvcd 94.61% ,<ha, la,<.:tcrI't?
(:O,~,ll\[LiOll a+c(:tu'+t<:y when |,}it |)a, selinc ~tc(:ur+~<:y is87.46% \[m" pal, elLS h, uhx:tri<: c.gilme.rlng, dora;tin.
We~t(:hit:vcd 9fi.0% c\]l.+trltci, cr J'e(:og.il, ion +~(:c.ra(:y, whenthe I)+~ell.c a+(:c.r+~cy i~ 90% in thu cunf('.rcn<:c roy<istr~tLion doma.i..
It is very (l if l ic,lt to c,.)nlt)a+v.!
ourrusu\]ts with thu previous rcsUlll\[,~ I>(:ca, ust!
t,\]l(', expuri-merit <:onditio.s a, rc (:Oml)h:Lt:ly dill'rxenL.for the AS(3 l  keyt>oaa'd.|{e \ [ 'e re l \ ]  cesKt!rltl(!th W. (.~,l|llr(:h. 1988.
A Sto<:ha.sti(: P+u't~ Pro-g,,'am +u.l No .n  Phra.se P~H's,.
:I" for IJlll'(!s|,rlcIA'dq'ext,, \]n lJrocc(:d{ng.~ (ff A NI, tLSb;, t)~tges 136- 143.I)oug (',utting, Julhul Kut)ie(:, J~ul Pudersen~, +tn(lIJcllelot)c Sibun.
1992.
A t)llajcth:+tl \[)a, nlt-o\[-Sl)eu(:h~l'a+gger, In t)roc('+:ding.~ (ff A N1,1)-92, I)a+gcs 13;1- 140.q'erumasa, lC}l;u% Kunl, aJu Ogura, Tsuyoshl Mori-mol,u.
19!}0.
ATI{ I)ia,\]oguc \[)atal>asu.
1, lq'o('ccd-i.,g.~ of ICSI, P, \[m;q, es 1(193-109(J.Wi\]li:-lln A.
(\]a\]l.~ a+nd I<r.n.etll W. (~lillr(:h. 199(\].
Pooll'~sthna, tcs of C, ontcxt  are Worse.
th;Ln Nora:.
lit Pro-ccc(liT*ys of I)AHPA A'atur, I Lan(j'u,gc m*d St)etchWorkshop, 1)+tgcs 28a-287.M~trk D. I<crnigha, n Kenneth W. Chur(:h, andWilli~un A.
(~th!.
1990.
A Spelling Correction Pro-gr~un Based o .
a, Noi.sy (-lh.
;utnel Model.
In l'roccc(l-in(js of (701,1N(;-90, l)~tgus 2(-15-210.K~u'r.n Kuki<:h. 111!12.
'l'ccllniqurs For Aut,omnth:a.llyCol'reeLing Words in Text.
A( :M (/omlmlin 9 ,%+r-O(:g.'?~ VoI.L)4, No.4, I)~t~,(:s ;\]77-4119.Nol)uyms.
\[to ;?lld l\[iroshi M :?rilya, lllat.
19!12.
A, Methodof \])u.te(:ting aim (hJrrecLing l?t'rors in the t{esults ofJitl)a+ne:.+e OCR.
In Tra.nsaclion o\] In\]otto+ilion Pro-f'?
?S,'fi~L~(J ~'OC+dll I of ,/f+pfllZ, Vc)l.;\],~, No.5, J)a.gC?S {J{J4-670 ( in J+q)+tnese).I"re(h!rh:k Je.linek+ 1985.
Self-org+uli/,(!d l,a,.
:e,u+~gcMo(h!ling, \[o17 St)et!
(:h Rtx:ognition.
IBM t{,.
:l)Od,.I",rh: M+tys, I"rcd J. J)+uncratlt, aJM l/obt:rt I,.
Mcr-(:er.
1991.
C, tmtcxt l}+Ls(!
(l Spelling (',orn't!ctio.. /n-Jormalion l)ro(:cssing ~',; M,nagcmcnl, V.I.
27, Nt).5,I)~g(!s 517-522.Masa~tki N +tgati,~t.
1!}!t4.
A S tO(:}U a,M,i(: ,\];~\[)}ttlcSCMort)hoh)glcal Amdyzer Using it l"orwa, r(l-l)l ~l}ackw~r(1-/t* N-Best Se+tr(:h Algorithm.
In t?occ(:d-inys of (JOI,1N(7-9~, l)a#.e,s 201-207.Ma,,,+t+tki N itgiLL+L 1996. alui, onl~tti<: l~Xl, l';-:t(:lion of NewWords from J~tl)+tnese q'exl, s u,,,ing Gc,uraJizcdI"orwa.r(1-1btckw~u'd ~qe~tr<:h. q'o att)l)(,.itr ill l)r.'wccd -my.~ of EMNI,I'.I"r~nk K. ,qt)onZ ~tnd l'\]n,e--l"oulg llu~tng.
1991.
A Ti'cc-Trellis lbk,~ud l"+tst Suar<:h for I"indi,g the N l}uslSenten(:e. l lyl)otheses h!
(hmtinuous Speech I{e(:og-nil, ion.
l .
/)roccc<ling* of 1(7A SSILOI, I)a,g{:sT05:7(IS.
"l'el,suya,su Ta, k~m +~l~(l FunlihiLo NiMfino.
1989. lml)h>m(,.nt;tl, ion atit(I Ewdui~tlon of Post-processiug forJa.l)aJl(!St!
I)O(:lltncnL \]{.(!a,(l,.:u's.
lit ~}'nns.
(:li,n* o/hffo,'m++liou t'l'occ.
't,?i?+fj ,qo(:icly of Japeln, Vol.30,No.l I, l)a, ges 13!
)4 1401 (1. ,\];Ltla.llt:Se ).i{ol)erl, A. W;~z,ur +ul(I Miclut(!l .I.
Fis(:her.
1974. q'he~l, rlng-l,tr.,qtring C,orrr.cl,ion Probh!m.
In ,h)mrud ofthc ,4UM, Vol.21, No.I, t)a+Zes 168-173.811
