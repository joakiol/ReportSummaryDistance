Teaching a Weaker Classifier:Named Entity Recognition on Upper Case TextHai Leong ChieuDSO National Laboratories20 Science Park DriveSingapore 118230chaileon@dso.org.sgHwee Tou NgDepartment of Computer ScienceSchool of ComputingNational University of Singapore3 Science Drive 2Singapore 117543nght@comp.nus.edu.sgAbstractThis paper describes how a machine-learning named entity recognizer (NER)on upper case text can be improved by us-ing a mixed case NER and some unlabeledtext.
The mixed case NER can be used totag some unlabeled mixed case text, whichare then used as additional training mate-rial for the upper case NER.
We show thatthis approach reduces the performancegap between the mixed case NER and theupper case NER substantially, by 39% forMUC-6 and 22% for MUC-7 named en-tity test data.
Our method is thus usefulin improving the accuracy of NERs on up-per case text, such as transcribed text fromautomatic speech recognizers where caseinformation is missing.1 IntroductionIn this paper, we propose using a mixed case namedentity recognizer (NER) that is trained on labeledtext, to further train an upper case NER.
In theSixth and Seventh Message Understanding Confer-ences (MUC-6, 1995; MUC-7, 1998), the namedentity task consists of labeling named entities withthe classes PERSON, ORGANIZATION, LOCA-TION, DATE, TIME, MONEY, and PERCENT.
Weconducted experiments on upper case named entityrecognition, and showed how unlabeled mixed casetext can be used to improve the results of an up-per case NER on the official MUC-6 and MUC-7Mixed Case: Consuela Washington, a longtimeHouse staffer and an expert in securities laws,is a leading candidate to be chairwoman of theSecurities and Exchange Commission in the Clintonadministration.Upper Case: CONSUELA WASHINGTON, ALONGTIME HOUSE STAFFER AND AN EX-PERT IN SECURITIES LAWS, IS A LEADINGCANDIDATE TO BE CHAIRWOMAN OF THESECURITIES AND EXCHANGE COMMIS-SION IN THE CLINTON ADMINISTRATION.Figure 1: Examples of mixed and upper case texttest data.
Besides upper case text, this approachcan also be applied on transcribed text from auto-matic speech recognizers in Speech Normalized Or-thographic Representation (SNOR) format, or fromoptical character recognition (OCR) output.
For theEnglish language, a word starting with a capital let-ter often designates a named entity.
Upper caseNERs do not have case information to help themto distinguish named entities from non-named en-tities.
When data is sparse, many named entities inthe test data would be unknown words.
This makesupper case named entity recognition more difficultthan mixed case.
Even a human would experiencegreater difficulty in annotating upper case text thanmixed case text (Figure 1).We propose using a mixed case NER to ?teach?
anupper case NER, by making use of unlabeled mixedcase text.
With the abundance of mixed case un-Computational Linguistics (ACL), Philadelphia, July 2002, pp.
481-488.Proceedings of the 40th Annual Meeting of the Association forlabeled texts available in so many corpora and onthe Internet, it will be easy to apply our approachto improve the performance of NER on upper casetext.
Our approach does not satisfy the usual as-sumptions of co-training (Blum and Mitchell, 1998).Intuitively, however, one would expect some infor-mation to be gained from mixed case unlabeled text,where case information is helpful in pointing outnew words that could be named entities.
We showempirically that such an approach can indeed im-prove the performance of an upper case NER.In Section 5, we show that for MUC-6, this wayof using unlabeled text can bring a relative reduc-tion in errors of 38.68% between the upper case andmixed case NERs.
For MUC-7 the relative reductionin errors is 22.49%.2 Related WorkConsiderable amount of work has been done inrecent years on NERs, partly due to the Mes-sage Understanding Conferences (MUC-6, 1995;MUC-7, 1998).
Machine learning methods suchas BBN?s IdentiFinder (Bikel, Schwartz, andWeischedel, 1999) and Borthwick?s MENE (Borth-wick, 1999) have shown that machine learningNERs can achieve comparable performance withsystems using hand-coded rules.
Bikel, Schwartz,and Weischedel (1999) have also shown how mixedcase text can be automatically converted to uppercase SNOR or OCR format to train NERs to workon such formats.
There is also some work on un-supervised learning for mixed case named entityrecognition (Collins and Singer, 1999; Cucerzanand Yarowsky, 1999).
Collins and Singer (1999)investigated named entity classification using Ad-aboost, CoBoost, and the EM algorithm.
However,features were extracted using a parser, and perfor-mance was evaluated differently (the classes wereperson, organization, location, and noise).
Cucerzanand Yarowsky (1999) built a cross language NER,and the performance on English was low comparedto supervised single-language NER such as Identi-Finder.
We suspect that it will be hard for purelyunsupervised methods to perform as well as super-vised ones.Seeger (2001) gave a comprehensive summary ofrecent work in learning with labeled and unlabeleddata.
There is much recent research on co-training,such as (Blum and Mitchell, 1998; Collins andSinger, 1999; Pierce and Cardie, 2001).
Most co-training methods involve using two classifiers builton different sets of features.
Instead of using distinctsets of features, Goldman and Zhou (2000) used dif-ferent classification algorithms to do co-training.Blum and Mitchell (1998) showed that in orderfor PAC-like guarantees to hold for co-training, fea-tures should be divided into two disjoint sets satis-fying: (1) each set is sufficient for a classifier tolearn a concept correctly; and (2) the two sets areconditionally independent of each other.
Each set offeatures can be used to build a classifier, resulting intwo independent classifiers, A and B. Classificationsby A on unlabeled data can then be used to furthertrain classifier B, and vice versa.
Intuitively, the in-dependence assumption is there so that the classifi-cations of A would be informative to B.
When theindependence assumption is violated, the decisionsof A may not be informative to B.
In this case, thepositive effect of having more data may be offset bythe negative effect of introducing noise into the data(classifier A might not be always correct).Nigam and Ghani (2000) investigated the differ-ence in performance with and without a feature split,and showed that co-training with a feature split givesbetter performance.
However, the comparison theymade is between co-training and self-training.
Inself-training, only one classifier is used to tag unla-beled data, after which the more confidently taggeddata is reused to train the same classifier.Many natural language processing problems donot show the natural feature split displayed by theweb page classification task studied in previous co-training work.
Our work does not really fall underthe paradigm of co-training.
Instead of co-operationbetween two classifiers, we used a stronger classi-fier to teach a weaker one.
In addition, it exhibitsthe following differences: (1) the features are notat all independent (upper case features can be seenas a subset of the mixed case features); and (2) Theadditional features available to the mixed case sys-tem will never be available to the upper case system.Co-training often involves combining the two differ-ent sets of features to obtain a final system that out-performs either system alone.
In our context, how-ever, the upper case system will never have accessto some of the case-based features available to themixed case system.Due to the above reason, it is unreasonable toexpect the performance of the upper case NER tomatch that of the mixed case NER.
However, we stillmanage to achieve a considerable reduction of errorsbetween the two NERs when they are tested on theofficial MUC-6 and MUC-7 test data.3 System DescriptionWe use the maximum entropy framework to buildtwo classifiers: an upper case NER and a mixedcase NER.
The upper case NER does not have ac-cess to case information of the training and test data,and hence cannot make use of all the features usedby the mixed case NER.
We will first describe howthe mixed case NER is built.
More details of thismixed case NER and its performance are given in(Chieu and Ng, 2002).
Our approach is similarto the MENE system of (Borthwick, 1999).
Eachword is assigned a name class based on its features.Each name class is subdivided into 4 classes, i.e.,N begin, N continue, N end, and N unique.
Hence,there is a total of 29 classes (7 name classes  4sub-classes  1 not-a-name class).3.1 Maximum EntropyThe maximum entropy framework estimates proba-bilities based on the principle of making as few as-sumptions as possible, other than the constraints im-posed.
Such constraints are derived from trainingdata, expressing some relationship between featuresand outcome.
The probability distribution that sat-isfies the above property is the one with the high-est entropy.
It is unique, agrees with the maximum-likelihood distribution, and has the exponential form(Della Pietra, Della Pietra, and Lafferty, 1997):	fffiffifl "!$# %'& (where  refers to the outcome,the history (or con-text), and   is a normalization function.
In addi-tion, each feature function )  ( $ is a binary func-tion.
For example, in predicting if a word belongs toa word class,  is either true or false, andrefers tothe surrounding context:)(*,+if  = true, previous word = the-otherwiseThe parameters are estimated by a procedurecalled Generalized Iterative Scaling (GIS) (Darrochand Ratcliff, 1972).
This is an iterative method thatimproves the estimation of the parameters at eachiteration.3.2 Features for Mixed Case NERThe features we used can be divided into 2 classes:local and global.
Local features are features that arebased on neighboring tokens, as well as the tokenitself.
Global features are extracted from other oc-currences of the same token in the whole document.Features in the maximum entropy framework arebinary.
Feature selection is implemented using a fea-ture cutoff: features seen less than a small count dur-ing training will not be used.
We group the featuresused into feature groups.
Each group can be madeup of many binary features.
For each token .
, zero,one, or more of the features in each group are set to1.The local feature groups are:Non-Contextual Feature: This feature is set to1 for all tokens.
This feature imposes constraintsthat are based on the probability of each name classduring training.Zone: MUC data contains SGML tags, and a doc-ument is divided into zones (e.g., headlines and textzones).
The zone to which a token belongs is usedas a feature.
For example, in MUC-6, there are fourzones (TXT, HL, DATELINE, DD).
Hence, for eachtoken, one of the four features zone-TXT, zone-HL,zone-DATELINE, or zone-DD is set to 1, and theother 3 are set to 0.Case and Zone: If the token .
starts with a cap-ital letter (initCaps), then an additional feature (init-Caps, zone) is set to 1.
If it is made up of all capitalletters, then (allCaps, zone) is set to 1.
If it containsboth upper and lower case letters, then (mixedCaps,zone) is set to 1.
A token that is allCaps will also beinitCaps.
This group consists of (3  total numberof possible zones) features.Case and Zone of .0/  and .21  : Similarly,if .0/  (or .31  ) is initCaps, a feature (initCaps,Token satisfies Example FeatureStarts with a capital Mr. InitCap-letter, ends with a period PeriodContains only one A OneCapcapital letterAll capital letters and CORP. AllCaps-period PeriodContains a digit AB3, Contain-747 DigitMade up of 2 digits 99 TwoDMade up of 4 digits 1999 FourDMade up of digits 01/01 Digit-and slash slashContains a dollar sign US$20 DollarContains a percent sign 20% PercentContains digit and period $US3.20 Digit-PeriodTable 1: Features based on the token stringzone) 457698 (or (initCaps, zone):7;<5= ) is set to 1,etc.Token Information: This group consists of 10features based on the string .
, as listed in Table 1.For example, if a token starts with a capital letterand ends with a period (such as Mr.), then the featureInitCapPeriod is set to 1, etc.First Word: This feature group contains only onefeature firstword.
If the token is the first word of asentence, then this feature is set to 1.
Otherwise, itis set to 0.Lexicon Feature: The string of the token .
isused as a feature.
This group contains a large num-ber of features (one for each token string present inthe training data).
At most one feature in this groupwill be set to 1.
If .
is seen infrequently duringtraining (less than a small count), then .
will not se-lected as a feature and all features in this group areset to 0.Lexicon Feature of Previous and Next Token:The string of the previous token .
1  and the nexttoken .>/  is used with the initCaps informationof .
.
If .
has initCaps, then a feature (initCaps,.
?/ ) 4<5768 is set to 1.
If .
is not initCaps, then (not-initCaps, .>/  ) 45ff68 is set to 1.
Same for .01  .
Inthe case where the next token ./  is a hyphen, then.
?/A@ is also used as a feature: (initCaps, .B/A@ ) 457698is set to 1.
This is because in many cases, the useof hyphens can be considered to be optional (e.g.,?third-quarter?
or ?third quarter?
).Out-of-Vocabulary: We derived a lexicon listfrom WordNet 1.6, and words that are not found inthis list have a feature out-of-vocabulary set to 1.Dictionaries: Due to the limited amount of train-ing material, name dictionaries have been found tobe useful in the named entity task.
The sourcesof our dictionaries are listed in Table 2.
A token.
is tested against the words in each of the fourlists of location names, corporate names, person firstnames, and person last names.
If .
is found in a list,the corresponding feature for that list will be set to 1.For example, if Barry is found in the list of personfirst names, then the feature PersonFirstName willbe set to 1.
Similarly, the tokens .C/  and .D1  aretested against each list, and if found, a correspond-ing feature will be set to 1.
For example, if .B/  isfound in the list of person first names, the featurePersonFirstName 4<57698 is set to 1.Month Names, Days of the Week, and Num-bers: If .
is one of January, February, .
.
.
, Decem-ber, then the feature MonthName is set to 1.
If .
isone of Monday, Tuesday, .
.
.
, Sunday, then the fea-ture DayOfTheWeek is set to 1.
If .
is a numberstring (such as one, two, etc), then the feature Num-berString is set to 1.Suffixes and Prefixes: This group contains onlytwo features: Corporate-Suffix and Person-Prefix.Two lists, Corporate-Suffix-List (for corporate suf-fixes) and Person-Prefix-List (for person prefixes),are collected from the training data.
For a token .that is in a consecutive sequence of initCaps tokens.21	E(GFGFGFH(.(GFGFGFH(.
?/I, if any of the tokens from.
?/ to .0/I is in Corporate-Suffix-List, then a fea-ture Corporate-Suffix is set to 1.
If any of the to-kens from .
?1	E?1  to .31  is in Person-Prefix-List,then another feature Person-Prefix is set to 1.
Notethat we check for .>1	E?1  , the word preceding theconsecutive sequence of initCaps tokens, since per-son prefixes like Mr., Dr. etc are not part of personnames, whereas corporate suffixes like Corp., Inc.etc are part of corporate names.The global feature groups are:InitCaps of Other Occurrences: There are 2 fea-tures in this group, checking for whether the first oc-currence of the same word in an unambiguous posi-Description SourceLocation Names http://www.timeanddate.comhttp://www.cityguide.travel-guides.comhttp://www.worldtravelguide.netCorporate Names http://www.fmlx.comPerson First Names http://www.census.gov/genealogy/namesPerson Last NamesTable 2: Sources of Dictionariestion (non first-words in the TXT or TEXT zones) inthe same document is initCaps or not-initCaps.
Fora word whose initCaps might be due to its positionrather than its meaning (in headlines, first word of asentence, etc), the case information of other occur-rences might be more accurate than its own.Corporate Suffixes and Person Prefixes ofOther Occurrences: With the same Corporate-Suffix-List and Person-Prefix-List used in local fea-tures, for a token .
seen elsewhere in the same docu-ment with one of these suffixes (or prefixes), anotherfeature Other-CS (or Other-PP) is set to 1.Acronyms: Words made up of all capitalized let-ters in the text zone will be stored as acronyms (e.g.,IBM).
The system will then look for sequences ofinitial capitalized words that match the acronymsfound in the whole document.
Such sequences aregiven additional features of A begin, A continue, orA end, and the acronym is given a feature A unique.For example, if ?FCC?
and ?Federal Communica-tions Commission?
are both found in a document,then ?Federal?
has A begin set to 1, ?Communica-tions?
has A continue set to 1, ?Commission?
hasA end set to 1, and ?FCC?
has A unique set to 1.Sequence of Initial Caps: In the sentence ?EvenNews Broadcasting Corp., noted for its accurate re-porting, made the erroneous announcement.
?, a NERmay mistake ?Even News Broadcasting Corp.?
asan organization name.
However, it is unlikely thatother occurrences of ?News Broadcasting Corp.?
inthe same document also co-occur with ?Even?.
Thisgroup of features attempts to capture such informa-tion.
For every sequence of initial capitalized words,its longest substring that occurs in the same docu-ment is identified.
For this example, since the se-quence ?Even News Broadcasting Corp.?
only ap-pears once in the document, its longest substring thatoccurs in the same document is ?News BroadcastingCorp.?.
In this case, ?News?
has an additional fea-ture of I begin set to 1,?Broadcasting?
has an addi-tional feature of I continue set to 1, and ?Corp.?
hasan additional feature of I end set to 1.Unique Occurrences and Zone: This group offeatures indicates whether the word .
is unique inthe whole document.
.
needs to be in initCaps tobe considered for this feature.
If .
is unique, then afeature (Unique, Zone) is set to 1, where Zone is thedocument zone where .
appears.3.3 Features for Upper Case NERAll features used for the mixed case NER are usedby the upper case NER, except those that requirecase information.Among local features, Case and Zone, InitCap-Period, and OneCap are not used by the upper caseNER.
Among global features, only Other-CS andOther-PP are used for the upper case NER, sincethe other global features require case information.For Corporate-Suffix and Person-Prefix, as the se-quence of initCaps is not available in upper casetext, only the next word (previous word) is testedfor Corporate-Suffix (Person-Prefix).3.4 TestingDuring testing, it is possible that the classifierproduces a sequence of inadmissible classes (e.g.,person begin followed by location unique).
Toeliminate such sequences, we define a transitionprobability between word classes J KLM K   to beequal to 1 if the sequence is admissible, and 0otherwise.
The probability of the classes K  (GFGFGFN( K Iassigned to the words in a sentence O in a documentPis defined as follows:Figure 2: The whole process of re-training the upper case NER.
Q signifies that the text is converted toupper case before processing.JK(GFGFGFN(KIO(PILJKLO(PRJKL KL1(where J K L  O (Pis determined by the maximumentropy classifier.
A dynamic programming algo-rithm is then used to select the sequence of wordclasses with the highest probability.4 Teaching ProcessThe teaching process is illustrated in Figure 2.
Thisprocess can be divided into the following steps:Training NERs.
First, a mixed case NER(MNER) is trained from some initial corpus S , man-ually tagged with named entities.
This corpus is alsoconverted to upper case in order to train another up-per case NER (UNER).
UNER is required by ourmethod of example selection.Baseline Test on Unlabeled Data.
Apply thetrained MNER on some unlabeled mixed case textsto produce mixed case texts that are machine-taggedwith named entities (text-mner-tagged).
Convertthe original unlabeled mixed case texts to uppercase, and similarly apply the trained UNER on thesetexts to obtain upper case texts machine-tagged withnamed entities (text-uner-tagged).Example Selection.
Compare text-mner-taggedand text-uner-tagged and select tokens in which theclassification by MNER differs from that of UNER.The class assigned by MNER is considered to becorrect, and will be used as new training data.
Thesetokens are collected into a set SUT .Retraining for Final Upper Case NER.
Both Sand S3T are used to retrain an upper case NER.
How-ever, tokens from S are given a weight of 2 (i.e.,each token is used twice in the training data), and to-kens from SDT a weight of 1, since S is more reliablethan S T (human-tagged versus machine-tagged).5 Experimental ResultsFor manually labeled data (corpus C), we used onlythe official training data provided by the MUC-6and MUC-7 conferences, i.e., using MUC-6 train-ing data and testing on MUC-6 test data, and us-ing MUC-7 training data and testing on MUC-7 testdata.1 The task definitions for MUC-6 and MUC-7 are not exactly identical, so we could not com-bine the training data.
The original MUC-6 trainingdata has a total of approximately 160,000 tokens and1MUC data can be obtained from the Linguistic Data Con-sortium: http://www.ldc.upenn.eduFigure 3: Improvements in F-measure on MUC-6plotted against amount of selected unlabeled datausedMUC-7 a total of approximately 180,000 tokens.The unlabeled text is drawn from the TREC (TextREtrieval Conference) corpus, 1992 Wall StreetJournal section.
We have used a total of 4,893 ar-ticles with a total of approximately 2,161,000 to-kens.
After example selection, this reduces the num-ber of tokens to approximately 46,000 for MUC-6and 67,000 for MUC-7.Figure 3 and Figure 4 show the results for MUC-6and MUC-7 obtained, plotted against the number ofunlabeled instances used.
As expected, it increasesthe recall in each domain, as more names or theircontexts are learned from unlabeled data.
However,as more unlabeled data is used, precision drops dueto the noise introduced in the machine tagged data.For MUC-6, F-measure performance peaked at thepoint where 30,000 tokens of machine labeled dataare added to the original manually tagged 160,000tokens.
For MUC-7, performance peaked at 20,000tokens of machine labeled data, added to the originalmanually tagged 180,000 tokens.The improvements achieved are summarized inTable 3.
It is clear from the table that this method ofusing unlabeled data brings considerable improve-ment for both MUC-6 and MUC-7 named entitytask.The result of the teaching process for MUC-6 is alot better than that of MUC-7.
We think that this isFigure 4: Improvements in F-measure on MUC-7plotted against amount of selected unlabeled datausedSystems MUC-6 MUC-7Baseline Upper Case NER 87.97% 79.86%Best Taught Upper Case NER 90.02% 81.52%Mixed case NER 93.27% 87.24%Reduction in relative error 38.68% 22.49%Table 3: F-measure on MUC-6 and MUC-7 test datadue to the following reasons:Better Mixed Case NER for MUC-6 thanMUC-7.
The mixed case NER trained on the MUC-6 officially released training data achieved an F-measure of 93.27% on the official MUC-6 test data,while that of MUC-7 (also trained on only the offi-cial MUC-7 training data) achieved an F-measure ofonly 87.24%.
As the mixed case NER is used as theteacher, a bad teacher does not help as much.Domain Shift in MUC-7.
Another possible causeis that there is a domain shift in MUC-7 for the for-mal test (training articles are aviation disasters arti-cles and test articles are missile/rocket launch arti-cles).
The domain of the MUC-7 test data is alsovery specific, and hence it might exhibit differentproperties from the training and the unlabeled data.The Source of Unlabeled Data.
The unlabeleddata used is from the same source as MUC-6, butdifferent for MUC-7 (MUC-6 articles and the un-labeled articles are all Wall Street Journal articles,whereas MUC-7 articles are New York Times arti-cles).6 ConclusionIn this paper, we have shown that the performance ofNERs on upper case text can be improved by usinga mixed case NER with unlabeled text.
Named en-tity recognition on mixed case text is easier than onupper case text, where case information is unavail-able.
By using the teaching process, we can reducethe performance gap between mixed and upper caseNER by as much as 39% for MUC-6 and 22% forMUC-7.
This approach can be used to improve theperformance of NERs on speech recognition output,or even for other tasks such as part-of-speech tag-ging, where case information is helpful.
With theabundance of unlabeled text available, such an ap-proach requires no additional annotation effort, andhence is easily applicable.This way of teaching a weaker classifier can alsobe used in other domains, where the task is to in-fer V W X , and an abundance of unlabeled dataPZYV(\[is available.
If one possesses a secondclassifier  V (W X such thatprovides addi-tional ?useful?
information that can be utilized bythis second classifier, then one can use this secondclassifier to automatically tag the unlabeled data P ,and select from P examples that can be used to sup-plement the training data for training V]W^X .ReferencesDaniel M. Bikel, Richard Schwartz, and RalphM.
Weischedel.
1999.
An Algorithm that LearnsWhat?s in a Name.
Machine Learning, 34(1/2/3):211-231.Avrim Blum and Tom Mitchell.
1998.
Combining La-beled and Unlabeled Data with Co-Training.
In Pro-ceedings of the Eleventh Annual Conference on Com-putational Learning Theory, 92-100.Andrew Borthwick.
1999.
A Maximum Entropy Ap-proach to Named Entity Recognition.
Ph.D. disserta-tion.
Computer Science Department.
New York Uni-versity.Hai Leong Chieu and Hwee Tou Ng.
2002.
NamedEntity Recognition: A Maximum Entropy ApproachUsing Global Information.
To appear in Proceedingsof the Nineteenth International Conference on Compu-tational Linguistics.Michael Collins and Yoram Singer.
1999.
UnsupervisedModels for Named Entity Classification.
In Proceed-ings of the 1999 Joint SIGDAT Conference on Empiri-cal Methods in Natural Language Processing and VeryLarge Corpora, 100-110.Silviu Cucerzan and David Yarowsky.
1999.
Lan-guage Independent Named Entity Recognition Com-bining Morphological and Contextual Evidence.
InProceedings of the 1999 Joint SIGDAT Conference onEmpirical Methods in Natural Language Processingand Very Large Corpora, 90-99.J.
N. Darroch and D. Ratcliff.
1972.
Generalized Iter-ative Scaling for Log-Linear Models.
The Annals ofMathematical Statistics, 43(5):1470-1480.Stephen Della Pietra, Vincent Della Pietra, and John Laf-ferty.
1997.
Inducing Features of Random Fields.IEEE Transactions on Pattern Analysis and MachineIntelligence, 19(4):380-393.Sally Goldman and Yan Zhou.
2000.
Enhancing Super-vised Learning with Unlabeled Data.
In Proceedingsof the Seventeenth International Conference on Ma-chine Learning, 327-334.MUC-6.
1995.
Proceedings of the Sixth Message Un-derstanding Conference (MUC-6).MUC-7.
1998.
Proceedings of the Seventh MessageUnderstanding Conference (MUC-7).Kamal Nigam and Rayid Ghani.
2000.
Analyzingthe Effectiveness and Applicability of Co-training.
InProceedings of the Ninth International Conference onInformation and Knowledge Management, 86-93.David Pierce and Claire Cardie.
2001.
Limitationsof Co-Training for Natural Language Learning fromLarge Datasets.
In Proceedings of the 2001 Confer-ence on Empirical Methods in Natural Language Pro-cessing, 1-9.Matthias Seeger.
2001.
Learning with Labeled and Un-labeled Data.
Technical Report, University of Edin-burgh.
