Human Language Technologies: The 2015 Annual Conference of the North American Chapter of the ACL, pages 376?386,Denver, Colorado, May 31 ?
June 5, 2015.c?2015 Association for Computational LinguisticsThe Geometry of Statistical Machine TranslationAurelien Waite, William ByrneDepartment of Engineering, University of Cambridge, UKaaw35@cam.ac.uk, wjb31@cam.ac.ukAbstractMost modern statistical machine translationsystems are based on linear statistical models.One extremely effective method for estimatingthe model parameters is minimum error ratetraining (MERT), which is an efficient form ofline optimisation adapted to the highly non-linear objective functions used in machinetranslation.
We describe a polynomial-timegeneralisation of line optimisation that com-putes the error surface over a plane embeddedin parameter space.
The description of this al-gorithm relies on convex geometry, which isthe mathematics of polytopes and their faces.Using this geometric representation of MERTwe investigate whether the optimisation of lin-ear models is tractable in general.
Previouswork on finding optimal solutions in MERT(Galley and Quirk, 2011) established a worst-case complexity that was exponential in thenumber of sentences, in contrast we showthat exponential dependence in the worst-casecomplexity is mainly in the number of fea-tures.Although our work is framed with respect toMERT, the convex geometric description isalso applicable to other error-based trainingmethods for linear models.
We believe ouranalysis has important ramifications because itsuggests that the current trend in building sta-tistical machine translation systems by intro-ducing a very large number of sparse featuresis inherently not robust.1 IntroductionThe linear model of Statistical Machine Translation(SMT) (Och and Ney, 2002) casts translation as asearch for translation hypotheses under a linear com-bination of weighted features: a source languagesentence f is translated as?e(f ;w) = argmaxe{wh(e, f)} (1)where translation scores are a linear combination ofthe D ?
1 feature vector h(e, f) ?
RDunder the1?D model parameter vector w.Convex geometry (Ziegler, 1995) is the math-ematics of such linear equations presented as thestudy of convex polytopes.
We use convex geom-etry to show that the behaviour of training methodssuch as MERT (Och, 2003; Macherey et al, 2008),MIRA (Crammer et al, 2006), PRO (Hopkins andMay, 2011), and others converge with a high fea-ture dimension.
In particular we analyse how robust-ness decreases in linear models as feature dimensionincreases.
We believe that severe overtraining is aproblem in many current linear model formulationsdue to this lack of robustness.In the process of building this geometric represen-tation of linear models we discuss algorithms suchas the Minkowski sum algorithm (Fukuda, 2004)and projected MERT (Section 4.2) that could be use-ful for designing new and more robust training algo-rithms for SMT and other natural language process-ing problems.2 Training Linear ModelsLet f1.
.
.
fSbe a set of S source language sentenceswith reference translations r1.
.
.
rS.
The goal is toestimate the model parameter vector w so as to min-imize an error count based on an automated metric,such as BLEU (Papineni et al, 2002), assumed to be376additive over sentences:?w = argminwS?s=1E(?e(fs;w), rs) (2)Optimisation can be made tractable by restrictingthe search to rescoring of K-best lists of translationhypotheses, {es,i, 1 ?
i ?
K}Ss=1.
For fs, leths,i= h(es,i, fs) be the feature vector associatedwith hypothesis es,i.
Restricted to these lists, thegeneral decoder of Eqn.
1 becomes?e(fs;w) = argmaxes,i{wh(es,i, fs)} (3)Although the objective function in Eqn.
(2) cannotbe solved analytically, MERT as described by Och(2003) can be performed over the K-best lists.
Theline optimisation procedure considers a subset of pa-rameters defined by the line w(0)+ ?d, where w(0)corresponds to an initial point in parameter spaceand d is the direction along which to optimise.
Eqn.
(3) can be rewritten as:?e(fs; ?)
= argmaxes,i{w(0)hs,i+ ?dhs,i)} (4)Line optimisation reduces the D-dimensional pro-cedure in Eqn.
(2) to a 1-Dimensional problem thatcan be easily solved using a geometric algorithm formany source sentences (Macherey et al, 2008).More recently, Galley and Quirk (2011) have in-troduced linear programming MERT (LP-MERT) asan exact search algorithm that reaches the global op-timum of the training criterion.
A hypothesis es,ifrom the sth K-best list can be selected by the de-coder only ifw(hs,j?
hs,i) ?
0 for 1 ?
j ?
K (5)for some parameter vector w 6= 0.
If such a solutionexists then the system of inequalities is feasible, anddefines a convex region in parameter space withinwhich any parameter w will yield es,i.
Testing thesystem of inequalities in (5) and finding a parametervector can be cast as a linear programming feasibil-ity problem (Galley and Quirk, 2011), and this canbe extended to find a parameter vector that optimizesEqn.
2 over a collection of K-best lists.
We discussthe complexity of this operation in Section 4.1.Hopkins and May (2011) note that for the sthsource sentence, the parameter w that correctlyranks its K-best list must satisfy the following setof constraints for 1 ?
i, j ?
K:w(hs,j?
hs,i) ?
0 if ?
(es,i, es,j) ?
0 (6)where ?
computes the difference in error betweentwo hypotheses.
The difference vectors (hs,j?hs,i)associated with each constraint can be used as inputvectors for a binary classification problem in whichthe aim is to predict whether the the difference inerror ?
(es,i, es,j) is positive or negative.
Hopkinsand May (2011) call this algorithm Pairwise Rank-ing Optimisation (PRO).
Because there are SK2dif-ference vectors across all source sentences, a subsetof constraints is sampled in the original formulation;with effcient calculation of rankings, sampling canbe avoided (Dreyer and Dong, 2015).The online error based training algorithm MIRA(Crammer et al, 2006) is also used for SMT (Watan-abe et al, 2007; Chiang et al, 2008; Chiang, 2012).Using a sentence-level error function, a set of S or-acle hypotheses are indexed with the vector?i:?is= argminiE(es,i, rs) for 1 ?
s ?
SFor a given s the objective at iteration n + 1 is :minimisew(n+1)12?w(n+1)?w(n)?2+ CK?j=1?j(7)subject to ?j?
0 and for 1 ?
j ?
K,?is6= j :w(n+1)(hs,j?
hs,?is) + ?
(es,?is, es,j)?
?j?
0where {?}
are slack variables added to allow infea-sible solutions, and C controls the trade-off betweenerror minimisation and margin maximisation.
Theonline nature of the optimiser results in compleximplementations, therefore batch versions of MIRAhave been proposed (Cherry and Foster, 2012; Gim-pel and Smith, 2012).Although MERT, LP-MERT, PRO, and MIRAcarry out their search in very different ways, we cancompare them in terms of the constraints they areattempting to satisfy.
A feasible solution for LP-MERT is also an optimal solution for MERT, andvice versa.
The constraints (Eqn.
(5)) that defineLP-MERT are a subset of the constraints (Eqn.
(6))377that define PRO and so a feasible solution for PROwill also be feasible for LP-MERT; however the con-verse is not necessarily true.
The constraints thatdefine MIRA (Eqn.
(7)) are similar to the LP-MERTconstraints (5), although with the addition of slackvariables and the ?
function to handle infeasible so-lutions.
However, if a feasible solution is availablefor MIRA, then these extra quantities are unneces-sary.
With these quantities removed, then we re-cover a ?hard-margin?
optimiser, which utilises thesame constraint set as in LP-MERT.
In the feasiblecase, the solution found by MIRA is also a solutionfor LP-MERT.2.1 Survey of Recent WorkOne avenue of SMT research has been to add asmany features as possible to the linear model, es-pecially in the form of sparse features (Chiang etal., 2009; Hopkins and May, 2011; Cherry and Fos-ter, 2012; Gimpel and Smith, 2012; Flanigan et al,2013; Galley et al, 2013; Green et al, 2013).
Theassumption is that the addition of new features willimprove translation performance.
It is interestingto read the justification for many of these works asstated in their abstracts.
For example Hopkins andMay (2011) state that:We establish PRO?s scalability and effec-tiveness by comparing it to MERT andMIRA and demonstrate parity on bothphrase-based and syntax-based systemsCherry and Foster (2012) state:Among other results, we find that a simpleand efficient batch version of MIRA per-forms at least as well as training online.Along similar lines Gimpel and Smith (2012) state:[We] present a training algorithm that iseasy to implement and that performs com-parable to others.In defence of MERT, Galley et al (2013) state:Experiments with up to 3600 featuresshow that these extensions of MERT yieldresults comparable to PRO, a learner oftenused with large feature sets.Green et al (2013) also note that feature-richmodels are rarely used in annual MT evaluations,an observation they use to motivate an investigationinto adaptive learning rate algorithms.Why do such different methods give such remark-ably ?comparable?
performance in research settings?And why is it so difficult to get general and unam-biguous improvements through the use of high di-mensional, sparse features?
We believe that the ex-planation is in feasibility.
If the oracle index vector?i is feasible then all training methods will find verysimilar solutions.
Our belief is that as the featuredimension increases, the chance of an oracle indexvector being feasible also increases.3 Convex GeometryWe now build on the description of LP-MERT togive a geometric interpretation to training linearmodels.
We first give a concise summary of thefundamentals of convex geometry as presented by(Ziegler, 1995) after which we work through the ex-ample in Cer et al (2008) to provide an intuition be-hind these concepts.3.1 Convex Geometry FundamentalsIn this section we reference definitions from convexgeometry (Ziegler, 1995) in a form that allows us todescribe SMT model parameter optimisation.Vector Space The real valued vector space RDrep-resents the space of all finite D-dimensional featurevectors.Dual Vector Space The dual vector space (RD)?arethe real linear functions RD?
R.Polytope The polytope Hs?
RDis the convex hullof the finite set of feature vectors associated withthe K hypotheses for the sth sentence, i.e.
Hs=conv(hs,1, .
.
.
,hs,K).Faces in RDSuppose for w ?
(RD)?that wh ?maxh?
?Hswh?, ?h ?
Hs.
A face is defined asF = {h ?
Hs: wh = maxh??Hswh?}
(8)Vertex A face consisting of a single point is calleda vertex.
The set of vertices of a polytope is denotedvert(Hs).Edge An edge is a face in the form of a line seg-ment between two vertices hs,iand hs,jin the poly-tope Hs.
The edge can be written as [hs,i,hs,j] =conv(hs,i,hs,j).
If an edge exists then the following378hLM: log(PLM(e)) hTM: log(PTM(f |e))e1-0.1 -1.2e2-1.2 -0.2e3-0.9 -1.6e4-0.9 -0.1e5-0.8 -0.9Table 1: An example set of two dimensional fea-ture vectors (after Cer et al (2008), Table 1) withlanguage model (hLM) and translation model (hTM)components.
A fifth feature vector has been addedto illustrate redundancy.modified system from (5) is feasiblew(hj?
hi) = 0 (9)w(hk?
hi) < 0, 1 ?
k ?
K, k 6= i, k 6= jw(hl?
hj) < 0, 1 ?
l ?
K, l 6= i, l 6= jwhich implies that [hs,i,hs,j] defines a decisionboundary in (RD)?between the parameters thatmaximise hs,iand those that maximise hs,j.Normal Cone For the face F in polytope Hsthenormal cone NFtakes the form.NF= {w : w(hs,j?
hs,i) ?
0,?hs,i?
vert(F ),?hs,j?
vert(Hs)} (10)If the face is a vertex F = {hs,i} then its normalcone N{hs,i}is the set of feasible parameters thatsatisfy the system in (5).Normal Fan The set of all normal cones associatedwith the faces ofHsis called the normal fanN (Hs).3.2 Drawing a Normal FanFollowing the example in Cer et al (2008) we an-alyze a system based on two features: the transla-tion PTM(f |e) and language PLM(e) models.
Forbrevity we omit the common sentence index, sothat hi= hs,i.
The system produces a set offour hypotheses which yield four feature vectors{h1,h2,h3,h4} (Table 1).
To this set of four hy-potheses, we add a fifth hypothesis and feature vec-tor h5to illustrate an infeasible solution.
These fea-ture vectors are plotted in Figure 1.The feature vectors form a polytope H shaded inlight blue.
From Figure 1 we see that h4satisfies thehLMhTMh1h2h3h4h5Figure 1: A geometric interpretation of LP-MERT(after Cer et al (2008) and Galley and Quirk(2011)).
The decision boundary represented by thedashed line intersects the polytope at only h4, mak-ing it a vertex.
No decision boundary intersectsh5without intersecting other points in the polytope,making h5redundant.conditions for a vertex in Eqn.
(8), because we candraw a decision boundary that interests the vertexand no other h ?
H .
We also note h5is not a vertex,and is redundant to the description of H.Figure 1 of Cer et al (2008) actually shows a nor-mal fan, although it is not described as such.
We nowdescribe how this geometric object is constructedstep by step in Figure 2.
In Part (a) we identify theedge [h4,h1] in R2with a decision boundary rep-resented by a dashed line.
We have also drawn avector w normal to the decision boundary that satis-fies Eqn.
(8).
This parameter would result in a tiedmodel score such that wh4= wh1.
When movingto (R2)?we see that the normal cone N[h4,h1]is aray parallel to w. This ray can be considered as theset of parameter vectors that yield the edge [h4,h1].The ray is also a decision boundary in (R2)?, withparameters on either side of the decision boundarymaximising either h4or h1.
Any vector parallel tothe edge [h4,h1], such as (h1?
h4), can be used todefine this decision boundary in (R2)?.Next in Part (b), with the same procedure we de-fine the normal cone for the edge [h3,h1].
Now boththe edges from parts (a) and (b) share the the vertexh1.
This implies that any parameter vector that liesbetween the two decision boundaries (i.e.
betweenthe two rays N[h3,h1]and N[h4,h1]) would maximisethe vertex h1: this is the set of vectors that comprise379h1h2h3h4h5wwLMwTMN[h4,h1](h1?
h4)R2(R2)?(a)h1h2h3h4h5wLMwTMN[h4,h1]N[h3,h1]R2(R2)?(b)h1h2h3h4h5wLMwTMN[h4,h1]N[h3,h1]N{h1}R2(R2)?(c)h1h2h3h4h5N[h4,h1]N[h3,h1]N[h4,h2]N[h3,h2]N{h1}N{h3}N{h4}N{h2}R2(R2)?
(d)Figure 2: Drawing the Normal Fan.
See the description in Section 3.2.
The end result in the r.h.s.
of Part(d) reproduces Figure 1 from Cer et al (2008), identifying the normal cones for all vertices.normal cone of the vertex N{h1}.In Part (c) we have shaded and labelled N{h1}.Note that no other edges are needed to define thisnormal cone; these other edges are redundant to thenormal cone?s description.Finally in Part (d) we draw the full fan.
We haveomitted the axes in (R2)?for clarity.
The normalcones for all 4 vertices have been identified.4 Training Set GeometryThe previous discussion treated only a single sen-tence.
For a training set of S input sentences, let ibe an index vector that contains S elements.
Eachelement is an index isto a hypothesis and a featurevector for the sth sentence.
A particular i specifiesa set of hypotheses drawn from each of the K-bestlists.
LP-MERT builds a set of KSfeature vectorsassociated with S dimensional index vectors i of theform hi= h1,i1+ .
.
.+hS,iS.
The polytope of thesefeature vectors is then constructed.In convex geometry this operation is called theMinkowski sum and for the polytopes Hsand Ht,is defined as (Ziegler, 1995)Hs+ Ht:= {h + h?
: h ?
Hs,h??
Ht} (11)We illustrate this operation in the top part of Figure3.
The Minkowski sum is commutative and asso-ciative and generalises to more than two polytopes(Gritzmann and Sturmfels, 1992).For the polytopes Hsand Htthe common refine-ment (Ziegler, 1995) isN (Hs) ?N (Ht) := {N ?N?
:N ?
N (Hs), N??
N (Ht)} (12)Each cone in the common refinement is the set ofparameter vectors that maximise two faces inHsandHt.
This operation is shown in the bottom part ofFigure 3.As suggested by Figure 3 the Minkowski sum andcommon refinement are linked by the followingProposition 1.
N (Hs+ Ht) = N (Hs) ?N (Ht)Proof.
See Gritzmann and Sturmfels (1992)This implies that, with hidefined for the indexvector i, the Minkowski sum defines the parametervectors that satisfy the following (Tsochantaridis etal., 2005, Eqn.
3)w(hs,j?
hs,is) ?
0, 1 ?
s ?
S, 1 ?
j ?
K (13)380h1,1h1,2h1,3h1,4h2,1h2,2h2,3h1,1+ h2,1h1,2+ h2,1h1,1+ h2,2h1,2+ h2,2h1,3+ h2,2h1,4+ h2,2{h1,4+ h2,1,h1,1+ h2,3}{h1,3+ h2,1,h1,2+ h2,3}h1,3+ h2,3h1,4+ h2,3H1H2H1+ H2R2(R2)?N[h1,4,h1,3]N[h1,2,h1,1]N[h1,4,h1,1]N[h1,3,h1,2]N[h2,3,h2,1]N[h2,3,h2,2]N[h2,1,h2,2]N[h1,4,h1,3]N[h1,2,h1,1]N[h1,4,h1,1]N[h1,3,h1,2]N[h2,3,h2,1]N[h2,3,h2,2]N[h2,1,h2,2]N (H1) N (H2) N (H1) ?N (H2)Figure 3: An example of the equivalence between the Minkowski sum and the common refinement.4.1 Computing the Minkowski SumIn the top part of the Figure 3 we see that computingtheMinkowski sum directly gives 12 feature vectors,10 of which are unique.
Each feature vector wouldhave to be tested under LP-MERT.
In general thereareKSsuch feature vectors and exhaustive testing isimpractical.
LP-MERT performs a lazy enumerationof feature vectors as managed through a divide andconquer algorithm.
We believe that in the worst casethe complexity of this algorithm could be O(KS).The lower part of Figure 3 shows the computationof the common refinement.
The common refinementappears as if one normal fan was superimposed onthe other.
We can see there are six decision bound-aries associated with the six edges of the Minkowskisum.
Even in this simple example, we can see thatthe common refinement is an easier quantity to com-pute than the Minkowski sum.We now briefly describe the algorithm of Fukuda(2004) that computes the common refinement.
Con-sider the example in Figure 3.
For H1and H2wehave drawn an edge in each polytope with a dashedline.
The corresponding decision boundaries in theirnormal fans have also been drawn with dashed lines.Now consider the vertex h1,3+ h2,2in H =H1+ H2and note it has two incident edges.
Theseedges are parallel to edges in the summand poly-topes and correspond to decision boundaries in thenormal cone N{h1,3+h2,2}.We can find the redundant edges in theMinkowski sum by testing the edges suggested bythe summand polytopes.
If a decision boundary in(RD)?is redundant, then we can ignore the featurevector that shares the decision boundary.
For exam-ple h1,4+h2,2is redundant and the decision bound-ary N[h1,3,h1,4]is also redundant to the descriptionof the normal cone N{h1,3+h2,2}.
The test for redun-dant edges can be performed by a linear program.Given aMinkowski sumH we can define an undi-rected cyclic graph G(H) = (vert(H), E) where Eis the set of edges.
The degree of a vertex in G(H)is the number of edges incident to a vertex; ?
is de-noted as the maximum degree of the vertices.The linear program for testing redundancy ofdecision boundaries has a runtime of O(D3.5?
)(Fukuda, 2004).
Enumerating the vertices of graphG(H) is not trivial due to it being an undirectedand cyclic graph.
The solution is to use a reversesearch algorithm (Avis and Fukuda, 1993).
Essen-381BLEU-0.2-0.100.10.2-0.2 -0.1 0 0.1 0.2WIPParameter?2UtoV Parameter ?10.331 0.334 0.337w(0)Figure 4: The BLEU score over a 1502 sentencetune set for the CUED Russian-to-English (Pino etal., 2013) system over two parameters.
Enumer-ated vertices of the Minkowski sum are shown in theshaded regions.tially reverse search transforms the graph into a tree.The vertex associated with w(0)is denoted as theroot of the tree, and from this root vertices are enu-merated in reverse order of model score under w(0).Each branch of the tree can be enumerated indepen-dently, which means that the enumeration can beparallelised.The complexity of the full algorithm isO(?(D3.5?
)| vert(H)|) (Fukuda, 2004).
In compar-ison with the O(KS) for LP-MERT the worst casecomplexity of the reverse search algorithm is linearwith respect to the size of vert(H).4.2 Two Dimensional Projected MERTWe now explore whether the reverse search algo-rithm is a practical method for performing MERTusing an open source implementation of the algo-rithm (Weibel, 2010).
For reasons discussed in thenext section, we wish to reduce the feature dimen-sion.
For M < D, we can define a projection ma-trix AM+1,Dthat maps hi?
RDinto RM+1asAM+1,Dhi=?hi,?hi?
RM+1.
There are tech-nical constraints to be observed, discussed in Waite(2014).
We note that when M = 1 we obtain Eqn.
(4).For our demonstration, we plot the error countover a plane in (RD)?.
Using the CUED Russian-to-English (Pino et al, 2013) entry to WMT?13 (Bojaret al, 2013) we build a tune set of 1502 sentences.The system uses 12 features which we initially tunewith lattice MERT (Macherey et al, 2008) to get aparameter w(0).
Using this parameter we generate1000-best lists.
We then project the feature functionsin the 1000-best lists to a 3-dimensional representa-tion that includes the source-to-target phrase proba-bility (UtoV), the word insertion penalty (WIP), andthe model score due to w(0).
We use the Minkowskisum algorithm to compute BLEU as ?
?
(R2)?isapplied to the parameters from w(0).Figure 4 displays some of the characteristics ofthe algorithm1.
This plot can be interpreted as a3-dimensional version of Figure 3 in Macherey etal.
(2008) where we represent the BLEU score asa heatmap instead of a third axis.
Execution wason 12 CPU cores, leading to the distinct search re-gions, demonstrating the parallel nature of the algo-rithm.
Weibel (2010) uses a depth-first enumerationorder of G(H), hence the narrow and deep explo-ration of (RD)?.
A breadth-first ordering would fo-cus on cones closer to w(0).
To our knowledge, thisis the first description of a generalised line optimi-sation algorithm that can search all the parametersin a plane in polynomial time.
Extensions to higherdimensional search are straightforward.5 Robustness of Linear ModelsIn the previous section we described the Minkowskisum polytope.
Let us consider the following upperbound theoremTheorem 1.
Let H1, .
.
.
., HSbe polytopes in RDwith at most N vertices each.
Then for D > 2 theupper bound on number of vertices ofH1+ .
.
.+HSis O(SD?1K2(D?1)).Proof.
See Gritzmann and Sturmfels (1992)Each vertex hicorresponds to a single indexvector i, which itself corresponds to a single setof selected hypotheses.
Therefore the numberof distinct sets of hypotheses that can be drawn1A replication of this experiment forms part of the UCAM-SMT tutorial at http://ucam-smt.github.io382from the S K-best lists in bounded above byO(min(KS, SD?1K2(D?1))).For low dimension features, i.e.
for D :SD?1K2(D?1)KS, the optimiser is thereforetightly constrained.
It cannot pick arbitrarily fromthe individual K-best lists to optimise the overallBLEU score.
We believe this acts as an inherentform of regularisation.For example, in the system of Section 4.2 (D=12,S=1502, K=1000), only 10?4403percent of the KSpossible index vectors are feasible.
However, if thefeature dimension D is increased to D = 493, thenSD?1K2(D?1)KSand this inherent regularisa-tion is no longer at work: any index vector is feasi-ble, and sentence hypotheses can chosen arbitrarilyto optimise the overall BLEU score.This exponential relationship of feasible solutionswith respect to feature dimension can be seen in Fig-ure 6 of Galley and Quirk (2011).
At low feature di-mension, they find that the LP-MERT algorithm canrun to completion for a training set size of hundredsof sentences.
As feature dimension increases, theruntime increases exponentially.PRO and other ranking methods are similarly con-strained for low dimensional feature vectors.Theorem 2.
If H is a D-dimensional polytope, thenfor D ?
3 the following is an upper bound on thenumber of edges |E||E| ?
(| vert(H)|2)(14)Proof.
This is a special case of the upper bound the-orem.
See Ziegler (1995, Theorem 8.23).Each feasible pairwise ranking of pairs of hy-potheses corresponds to an edge in the Minkowskisum polytope.
Therefore in low dimension rankingmethods also benefit from this inherent regularisa-tion.For higher dimensional feature vectors, these up-per bounds no longer guarantee that this inherentregularisation is at work.
The analysis suggests - butdoes not imply - that index vectors, and their cor-responding solutions, can be picked arbitrarily fromthe K-best lists.
For MERT overtraining is clearly arisk.MIRA and related methods have a regularisationmechanism due to the margin maximisation term inN[h4,h2]N[h4,h1]N{h4}N{h2}N{h1}w(0)w(2)w(1)?wFigure 5: We redraw the normal fan from Figure 2with potential optimal parameters under the `2reg-ularisation scheme of Galley et al (2013) marked.The thick red line is the subspace of (R2)?opti-mised.
The dashed lines mark the distances betweenthe decision boundaries and w(0).their objective functions.
Although this form of reg-ularisation may be helpful in practice, there is noguarantee that it will prevent overtraining due to theexponential increase in feasible solutions.
For ex-ample the adaptive learning rate method of Green etal.
(2013) finds gains of over 13 BLEU points in thetraining set with the addition of 390,000 features, yetonly 2 to 3 BLEU points are found in the test set.5.1 A Note on RegularisationThe above analysis suggest a need for regularisa-tion in training with high dimensional feature vec-tors.
Galley et al (2013) note that regularisation ishard to apply to linear models due to the magnitudeinvariance of w in Eqn.
(1).
Figure 2 makes the dif-ficulty clear: the normal cones are determined en-tirely by the feature vectors of the training samples,and within any particular normal cone a parametervector can be chosen with arbitrary magnitude.
Thisrenders schemes such as L1 or L2 normalisation in-effective.
To avoid this, Galley et al (2013) de-scribe a regularisation scheme for line optimisationthat encourages the optimal parameter to be foundclose to w(0).
The motivation is that w(0)should bea trusted initial point, perhaps taken from a lower-dimensional model.
We briefly discuss the chal-lenges of doing this sort of regularisation in MERT.In Figure 5 we reproduce the normal fan from Fig-ure 2.
In this diagram we represent the set of pa-rameters considered by a line optimisation as a thickred line.
Let us assume that both e1and e2have a383similarly low error count.
Under the regularisationscheme of Galley et al (2013) we have a choice ofw(1)or w(2), which are equidistant from w(0).
Inthis affine projection of parameter space it is unclearwhich one is the optimum.
However, if we considerthe normal fan as a whole we can clearly see that?w ?
N{hi}is the optimal point under the regular-isation.
However, it is not obvious in the projectedparameter space that?w is the better choice.
Thisanalysis suggests that direct intervention, e.g.
mon-itoring BLEU on a held-out set, may be more effec-tive in avoiding overtraining.6 DiscussionThe main contribution of this work is to presenta novel geometric description of MERT.
We showthat it is possible to enumerate all the feasible so-lutions of a linear model in polynomial time usingthis description.
The immediate conclusion fromthis work is that the current methods for estimatinglinear models as done in SMT works best for lowdimensional feature vectors.We can consider the SMT linear model as a mem-ber of a family of linear models where the outputvalues are highly structured, and where each inputyields a candidate space of possible output values.We have already noted that the constraints in (13) areshared with the structured-SVM (Tsochantaridis etal., 2005), and we can also see the same constraintsin Eqn.
3 of Collins (2002).
It is our belief that ouranalysis is applicable to all models in this family andextends far beyond the discussion of SMT here.We note that the upper bound on feasible solu-tions increases polynomially in training set size S,whereas the number of possible solutions increasesexponentially in S. The result is that the ratio offeasible to possible solutions decreases with S. Ouranalysis suggests that inherent regularisation shouldbe improved by increasing training set size.
Thisconfirms most researchers intuition, with perhapseven larger training sets needed than previously be-lieved.Another avenue to prevent overtraining would beto project high-dimensional feature sets to low di-mensional feature sets using the technique describedin Section 4.1.
We could then use existing trainingmethods to optimise over the projected feature vec-tors.We also note that non-linear models methods,such as neural networks (Schwenk et al, 2006;Kalchbrenner and Blunsom, 2013; Devlin et al,2014; Cho et al, 2014) and decision forests (Crimin-isi et al, 2011) are not bound by these analyses.
Inparticular neural networks are non-linear functionsof the features, and decision forests actively reducethe number of features for individual trees in the for-rest.
From the perspective of this paper, the recentimprovements in SMT due to neural networks arewell motivated.AcknowledgmentsThis research was supported by a doctoral trainingaccount from the Engineering and Physical SciencesResearch Council.ReferencesDavid Avis and Komei Fukuda.
1993.
Reverse searchfor enumeration.
Discrete Applied Mathematics,65:21?46.Ond?rej Bojar, Christian Buck, Chris Callison-Burch,Christian Federmann, Barry Haddow, Philipp Koehn,Christof Monz, Matt Post, Radu Soricut, and LuciaSpecia.
2013.
Findings of the 2013 Workshop onStatistical Machine Translation.
In Proceedings of theEighth Workshop on Statistical Machine Translation,pages 1?44, Sofia, Bulgaria, August.
Association forComputational Linguistics.Daniel Cer, Dan Jurafsky, and Christopher D. Manning.2008.
Regularization and search for minimum errorrate training.
In Proceedings of the Third Workshop onStatistical Machine Translation, pages 26?34, Colum-bus, Ohio, June.
Association for Computational Lin-guistics.Colin Cherry and George Foster.
2012.
Batch tuningstrategies for statistical machine translation.
In Pro-ceedings of the 2012 Conference of the North Amer-ican Chapter of the Association for ComputationalLinguistics: Human Language Technologies, pages427?436, Montr?eal, Canada, June.
Association forComputational Linguistics.David Chiang, Yuval Marton, and Philip Resnik.
2008.Online large-margin training of syntactic and struc-tural translation features.
In Proceedings of the 2008Conference on Empirical Methods in Natural Lan-guage Processing, pages 224?233, Honolulu, Hawaii,October.
Association for Computational Linguistics.384David Chiang, Kevin Knight, and Wei Wang.
2009.11,001 new features for statistical machine transla-tion.
In Proceedings of Human Language Technolo-gies: The 2009 Annual Conference of the North Ameri-can Chapter of the Association for Computational Lin-guistics, pages 218?226, Boulder, Colorado, June.
As-sociation for Computational Linguistics.David Chiang.
2012.
Hope and fear for discriminativetraining of statistical translation models.
The Journalof Machine Learning Research, 13(1):1159?1187.Kyunghyun Cho, Bart vanMerrienboer, Caglar Gulcehre,Dzmitry Bahdanau, Fethi Bougares, Holger Schwenk,and Yoshua Bengio.
2014.
Learning phrase repre-sentations using RNN encoder?decoder for statisticalmachine translation.
In Proceedings of the 2014 Con-ference on Empirical Methods in Natural LanguageProcessing (EMNLP), pages 1724?1734, Doha, Qatar,October.
Association for Computational Linguistics.Michael Collins.
2002.
Discriminative training meth-ods for hidden Markov models: Theory and experi-ments with perceptron algorithms.
In Proceedings ofthe 2002 Conference on Empirical Methods in Natu-ral Language Processing, pages 1?8.
Association forComputational Linguistics, July.Koby Crammer, Ofer Dekel, Joseph Keshet, Shai Shalev-Shwartz, and Yoram Singer.
2006.
Online passive-aggressive algorithms.
The Journal of Machine Learn-ing Research, 7:551?585.A.
Criminisi, J. Shotton, and E. Konukoglu.
2011.
Deci-sion forests for classification, regression, density esti-mation, manifold learning and semi-supervised learn-ing.
Technical report, Microsoft Research.Jacob Devlin, Rabih Zbib, Zhongqiang Huang, ThomasLamar, Richard Schwartz, and John Makhoul.
2014.Fast and robust neural network joint models for sta-tistical machine translation.
In Proceedings of the52nd Annual Meeting of the Association for Compu-tational Linguistics (Volume 1: Long Papers), pages1370?1380, Baltimore, Maryland, June.
Associationfor Computational Linguistics.Markus Dreyer and Yuanzhe Dong.
2015.
APRO: All-pairs ranking optimization for MT tuning.
In Proceed-ings of the 2015 Conference of the North AmericanChapter of the Association for Computational Linguis-tics: Human Language Technologies.Jeffrey Flanigan, Chris Dyer, and Jaime Carbonell.
2013.Large-scale discriminative training for statistical ma-chine translation using held-out line search.
In Pro-ceedings of the 2013 Conference of the North Amer-ican Chapter of the Association for ComputationalLinguistics: Human Language Technologies, pages248?258, Atlanta, Georgia, June.
Association forComputational Linguistics.Komei Fukuda.
2004.
From the zonotope construction tothe Minkowski addition of convex polytopes.
Journalof Symbolic Computation, 38(4):1261?1272.Michel Galley and Chris Quirk.
2011.
Optimal searchfor minimum error rate training.
In Proceedings ofthe 2011 Conference on Empirical Methods in NaturalLanguage Processing, pages 38?49, Edinburgh, Scot-land, UK., July.
Association for Computational Lin-guistics.Michel Galley, Chris Quirk, Colin Cherry, and KristinaToutanova.
2013.
Regularized minimum error ratetraining.
In Proceedings of the 2013 Conference onEmpirical Methods in Natural Language Processing,pages 1948?1959, Seattle, Washington, USA, October.Association for Computational Linguistics.Kevin Gimpel and Noah A. Smith.
2012.
Structuredramp loss minimization for machine translation.
InProceedings of the 2012 Conference of the NorthAmerican Chapter of the Association for Computa-tional Linguistics: Human Language Technologies,pages 221?231, Montr?eal, Canada, June.
Associationfor Computational Linguistics.Spence Green, Sida Wang, Daniel Cer, and Christo-pher D. Manning.
2013.
Fast and adaptive onlinetraining of feature-rich translation models.
In Pro-ceedings of the 51st Annual Meeting of the Associa-tion for Computational Linguistics (Volume 1: LongPapers), pages 311?321, Sofia, Bulgaria, August.
As-sociation for Computational Linguistics.Peter Gritzmann and Bernd Sturmfels.
1992.
Minkowskiaddition of polytopes: Computational complexity andapplications to Gr?obner bases.
SIAM Journal on Dis-crete Mathematics, 6(2).Mark Hopkins and Jonathan May.
2011.
Tuning as rank-ing.
In Proceedings of the 2011 Conference on Empir-ical Methods in Natural Language Processing, pages1352?1362, Edinburgh, Scotland, UK., July.
Associa-tion for Computational Linguistics.Nal Kalchbrenner and Phil Blunsom.
2013.
Recur-rent continuous translation models.
In Proceedings ofthe 2013 Conference on Empirical Methods in Natu-ral Language Processing, pages 1700?1709, Seattle,Washington, USA, October.
Association for Computa-tional Linguistics.Wolfgang Macherey, Franz Och, Ignacio Thayer, andJakob Uszkoreit.
2008.
Lattice-based minimum errorrate training for statistical machine translation.
In Pro-ceedings of the 2008 Conference on Empirical Meth-ods in Natural Language Processing, pages 725?734,Honolulu, Hawaii, October.
Association for Computa-tional Linguistics.Franz Josef Och and Hermann Ney.
2002.
Discrimi-native training and maximum entropy models for sta-tistical machine translation.
In Proceedings of 40th385Annual Meeting of the Association for ComputationalLinguistics, pages 295?302, Philadelphia, Pennsylva-nia, USA, July.
Association for Computational Lin-guistics.Franz Josef Och.
2003.
Minimum error rate training instatistical machine translation.
In Proceedings of the41st Annual Meeting of the Association for Compu-tational Linguistics, pages 160?167, Sapporo, Japan,July.
Association for Computational Linguistics.Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu.
2002.
Bleu: a method for automatic eval-uation of machine translation.
In Proceedings of 40thAnnual Meeting of the Association for ComputationalLinguistics, pages 311?318, Philadelphia, Pennsylva-nia, USA, July.
Association for Computational Lin-guistics.Juan Pino, Aurelien Waite, Tong Xiao, Adri`a de Gis-pert, Federico Flego, and William Byrne.
2013.
TheUniversity of Cambridge Russian-English system atWMT13.
In Proceedings of the Eighth Workshop onStatistical Machine Translation, pages 200?205, Sofia,Bulgaria, August.
Association for Computational Lin-guistics.Holger Schwenk, Daniel Dechelotte, and Jean-Luc Gau-vain.
2006.
Continuous space language models forstatistical machine translation.
In Proceedings of theCOLING/ACL 2006 Main Conference Poster Sessions,pages 723?730, Sydney, Australia, July.
Associationfor Computational Linguistics.Ioannis Tsochantaridis, Thorsten Joachims, Thomas Hof-mann, and Yasemin Altun.
2005.
Large marginmethods for structured and interdependent output vari-ables.
In Journal of Machine Learning Research,pages 1453?1484.Aurelien Waite.
2014.
The Geometry of Statistical Ma-chine Translation.
Ph.D. thesis, University of Cam-bridge, Cambridge, United Kingdom.Taro Watanabe, Jun Suzuki, Hajime Tsukada, and HidekiIsozaki.
2007.
Online large-margin training for sta-tistical machine translation.
In Proceedings of the2007 Joint Conference on Empirical Methods in Natu-ral Language Processing and Computational NaturalLanguage Learning, pages 764?773.Christophe Weibel.
2010.
Implementation and paral-lelization of a reverse-search algorithm for minkowskisums.
In Proceedings of the 12th Workshop on Algo-rithm Engineering and Experiments (ALENEX 2010),pages 34?42.
SIAM.G Ziegler.
1995.
Lectures on Polytopes.
Springer-Verlag.386
