Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 840?848,Los Angeles, California, June 2010. c?2010 Association for Computational LinguisticsLearning about Voice Search for Spoken Dialogue SystemsRebecca J. Passonneau1, Susan L. Epstein2,3, Tiziana Ligorio2,Joshua B. Gordon4, Pravin Bhutada41Center for Computational Learning Systems, Columbia University2Department of Computer Science, Hunter College of The City University of New York3Department of Computer Science, The Graduate Center of The City University of New York4Department of Computer Science, Columbia Universitybecky@cs.columbia.edu, susan.epstein@hunter.cuny.edu, tligorio@gc.cuny.edu,joshua@cs.columbia.edu, pravin.bhutada@gmail.comAbstractIn a Wizard-of-Oz experiment with multiplewizard subjects, each wizard viewed automatedspeech recognition (ASR) results for utteranceswhose interpretation is critical to task success:requests for books by title from a library data-base.
To avoid non-understandings, the wizarddirectly queried the application database withthe ASR hypothesis (voice search).
To learnhow to avoid misunderstandings, we investi-gated how wizards dealt with uncertainty invoice search results.
Wizards were quite suc-cessful at selecting the correct title from queryresults that included a match.
The most suc-cessful wizard could also tell when the queryresults did not contain the requested title.
Ourlearned models of the best wizard?s behaviorcombine features available to wizards withsome that are not, such as recognition confi-dence and acoustic model scores.1 IntroductionWizard-of-Oz (WOz) studies have long been usedfor spoken dialogue system design.
In a relativelynew variant, a subject (the wizard) is presentedwith real or simulated automated speech recogni-tion (ASR) to observe how people deal with incor-rect speech recognition output (Rieser, Kruijff-Korbayov?, & Lemon, 2005; Skantze, 2003;Stuttle, Williams, & Young, 2004; Williams &Young, 2003, 2004; Zollo, 1999).
In these experi-ments, when a wizard could not interpret the ASRoutput (non-understanding), she rarely asked usersto repeat themselves.
Instead, the wizard foundother ways to continue the task.This paper describes an experiment that pre-sented wizards with ASR results for utteranceswhose interpretation is critical to task success: re-quests for books from a library database, identifiedby title.
To avoid non-understandings, wizardsused voice search (Wang et al, 2008): they direct-ly queried the application database with ASR out-put.
To investigate how to avoid errors inunderstanding (misunderstandings), we examinedhow wizards dealt with uncertainty in voice searchresults.
When the voice search results included therequested title, all seven of our wizards were likelyto identify it.
One wizard, however, recognized farbetter than the others when the voice search resultsdid not contain the requested title.
The experimentemployed a novel design that made it possible toinclude system features in models of wizard beha-vior.
The principal result is that our learned modelsof the best wizard?s behavior combine features thatare available to wizards with some that are not,such as recognition confidence and acoustic modelscores.The next section of the paper motivates our ex-periment.
Subsequent sections describe relatedwork, the dialogue system and embedded wizardinfrastructure, experimental design, learning me-thods, and results.
We then discuss how to general-ize from the results of our study for spokendialogue system design.
We conclude with a sum-mary of results and their implications.2 MotivationRather than investigate full dialogues, we ad-dressed a single type of turn exchange or adjacencypair (Sacks et al, 1974): a request for a book by its840title.
This allowed us to collect data exclusivelyabout an utterance type critical for task success inour application domain.
We hypothesized that low-level features from speech recognition, such asacoustic model fit, could independently affectvoice search confidence.
We therefore applied anovel approach, embedded WOz, in which a wizardand the system together interpret noisy ASR.To address how to avoid misunderstandings, weinvestigated how wizards dealt with uncertainty invoice search returns.
To illustrate what we meanby uncertainty, if we query our book title databasewith the ASR hypothesis:ROLL DWELLour voice search procedure returns, in this order:CROMWELLROBERT LOWELLROAD TO WEALTHThe correct title appears last because of the score itis assigned by the string similarity metric we use.Three factors motivated our use of voice searchto interpret book title requests: noisy ASR, un-usually long query targets, and high overlap of thevocabulary across different query types (e.g., au-thor and title) as well as with non-query words incaller utterances (e.g., ?Could you look up .
.
.?
).First, accurate speech recognition for a real-world telephone application can be difficult toachieve, given unpredictable background noise andtransmission quality.
For example, the 68% worderror rate (WER) for the fielded version of Let?sGo Public!
(Raux et al, 2005) far exceeded its17% WER under controlled conditions.
Our appli-cation handles library requests by telephone, andwould benefit from robustness to noisy ASR.Second, the book title field in our database dif-fers from the typical case for spoken dialogue sys-tems that access a relational database.
Suchsystems include travel booking (Levin et al, 2000),bus route information (Raux et al, 2006), restau-rant guides (Johnston et al, 2002; Komatani et al,2005), weather (Zue et al, 2000) and directoryservices (Georgila et al, 2003).
In general for thesesystems, a few words are sufficient to retrieve thedesired attribute value, such as a neighborhood, astreet, or a surname.
Mean utterance length in asample of 40,000 Let?s Go Public!
utterances, forexample, is 2.4 words.
The average book titlelength in our database is 5.4 words.Finally, our dialogue system, CheckItOut, al-lows users to choose whether to request books bytitle, author, or catalogue number.
The databaserepresents 5028 active patrons (with real borrow-ing histories and preferences but fictitious personalinformation), 71,166 book titles and 28,031 au-thors.
Though much smaller than a database for adirectory service application (Georgila et al,2003), this is much larger than that of many currentresearch systems.
For example, Let?s Go Public!accesses a database with 70 bus routes and 1300place names.
Titles and author names contribute50,394 words to the vocabulary, of which 57.4%occur only in titles, 32.1% only in author names,and 10.5% in both.
Many book titles (e.g., You SeeI Haven?t Forgotten, You Never Know) have a highpotential for confusability with non-title phrases inusers?
book requests.
Given the longer databasefield and the confusability of the book title lan-guage, integrating voice search is likely to have arelatively larger impact in CheckItOut.We seek to minimize non-understandings andmisunderstandings for several reasons.
First, usercorrections in both situations have been shown tobe more poorly recognized than non-correction ut-terances (Litman et al, 2006).
Non-understandingstypically result in re-prompting the user for thesame information.
This often leads to hyper-articulation and concomitant degradation in recog-nition performance.
Second, users seem to prefersystems that minimize non-understandings and mi-sunderstandings, even at the expense of dialogueefficiency.
Users of the TOOT train informationspoken dialogue system preferred system-initiativeto mixed- or user-initiative, and preferred explicitconfirmation to implicit or no confirmation(Litman & Pan, 1999).
This was true despite thefact that a mixed-initiative, implicit confirmationstrategy led to fewer turns for the same task.
Mostof the more recent work on spoken dialogue sys-tems focuses on mixed-initiative systems in labora-tory settings.
Still, recent work suggests that whilemixed- or user-initiative is rated highly in usabilitystudies, under real usage it ?fails to provide [a] ro-bust enough interface?
(Turunen et al, 2006).
In-corporating accurate voice search into spokendialogue systems could lead to fewer non-understandings and fewer misunderstandings.3 Related WorkOur approach to noisy ASR contrasts with manyother information-seeking and transaction-baseddialogue systems.
Those systems typically perform841natural language understanding on ASR output be-fore database query with techniques that try to im-prove or expand ASR output.
None that we knowof use voice search.
For one directory service ap-plication, users spell the first three letters of sur-names, and then ASR results are expanded usingfrequently confused phones (Georgila et al, 2003).A two-pass recognition architecture added to Let?sGo Public!
improved concept recognition in post-confirmation user utterances (Stoyanchev & Stent,2009).
In (Komatani et al, 2005), a shallow se-mantic interpretation phase was followed by deci-sion trees to classify utterances as relevant either toquery type or to specific query slots, to narrow theset of possible interpretations.
CheckItOut is mostsimilar in spirit to the latter approach, but relies onthe database earlier, and only for semantic interpre-tation, not to also guide the dialogue strategy.Our approach to noisy ASR is inspired by pre-vious WOz studies with real (Skantze, 2003; Zollo,1999) or simulated ASR (Kruijff-Korbayov?
et al,2005; Rieser et al, 2005; Williams & Young,2004).
Simulation makes it possible to collect di-alogues without building a speech recognizer, andto control for WER.
In the studies that involvedtask-oriented dialogues, wizards typically focusedmore on the task and less on resolving ASR errors(Williams & Young, 2004; Skantze, 2003; Zollo,1999).
In studies more like the information-seekingdialogues addressed here, an entirely different pat-tern is observed (Kruijff-Korbayov?
et al, 2005;Rieser et al, 2005).Zollo collected seven dialogues with differenthuman-wizard pairs to develop an evacuation plan.The overall WER was 30%.
Of the 227 cases ofincorrect ASR, wizard utterances indicated a fail-ure to understand for only 35% of them.
Wizardsignored words not salient in the domain and hy-pothesized words based on phonetic similarity.
In(Skantze, 2003), both users and wizards knewthere was no dialogue system; 44 direction-findingdialogues were collected with 16 subjects.
Despitea WER of 43%, the wizard operators signaled mis-understanding only 5% of the time, in part becausethey often ignored ASR errors and continued thedialogue.
For the 20% of non-understandings, op-erators continued a route description, asked a task-related question, or requested a clarification.Williams and Young collected 144 dialoguessimulating tourist requests for directions and othernegotiations.
WER was constrained to be high,medium, or low.
Under medium WER, a task-related question in response to a non-understandingor misunderstanding led to full understanding moreoften than explicit repairs.
Under high WER, how-ever, the reverse was true.
Misunderstandings sig-nificantly increased when wizards followed non-understandings or misunderstandings with a task-related question instead of a repair.In (Rieser et al, 2005), wizards simulated amultimodal MP3 player application with access toa database of 150K music albums.
Responsescould be presented verbally or graphically.
In thenoisy transcription condition, wizards made clarifi-cation requests about twice as often as that foundin similar human-human dialogue.In a system like CheckItOut, user utterances thatrequest database information must be understood.We seek an approach that would reduce the rate ofmisunderstandings observed for high WER in(Williams & Young, 2004) and the rate of clarifi-cation requests observed in (Rieser et al, 2005).4 CheckItOut and Embedded WizardsCheckItOut is modeled on library transactions atthe Andrew Heiskell Braille and Talking Book Li-brary, a branch of the New York Public Libraryand part of the National Library of Congress.
Bor-rowing requests are handled by telephone.
Books,mainly in a proprietary audio format, travel bymail.
In a dialogue with CheckItOut, a user identi-fies herself, requests books, and is told which areavailable for immediate shipment or will go on re-serve.
The user can request a book by cataloguenumber, title, or author.CheckItOut builds on the Olympus/RavenClawframework (Bohus & Rudnicky, 2009) that hasbeen the basis for about a dozen dialogue systemsin different domains, including Let?s Go Public!
(Raux et al, 2005).
Speech recognition relies onPocketSphinx.
Phoenix, a robust context-freegrammar (CFG) semantic parser, handles naturallanguage understanding (Ward & Issar, 1994).
TheApollo interaction manager (Raux & Eskenazi,2007) detects utterance boundaries using informa-tion from speech recognition, semantic parsing,and Helios, an utterance-level confidence annotator(Bohus & Rudnicky, 2002).
The dialogue manageris implemented in RavenClaw.842To design CheckItOut?s dialogue manager, werecorded 175 calls (4.5 hours) from patrons to li-brarians.
We identified 82 book request calls, tran-scribed them, aligned the utterances with thespeech signal, and annotated the transcripts for di-alogue acts.
Because active patrons receivemonthly newsletters listing new titles in the desiredformats, patrons request specific items with ad-vance knowledge of the author, title, or cataloguenumber.
Most book title requests accurately repro-duce the exact title, the title less an initial deter-miner (?the,?
?a?
), or a subtitle.We exploited the Galaxy message passing archi-tecture of Olympus/RavenClaw to insert a wizardserver into CheckItOut.
The hub passes messagesbetween the system and a wizard?s graphical userinterface (GUI), allowing us to collect runtime in-formation that can be included in models of wi-zards?
actions.For speech recognition, CheckItOut relies onPocketSphinx 0.5, a Hidden Markov Model-basedrecognizer.
Speech recognition for this experiment,relied on the freely available Wall Street Journal?read speech?
acoustic models.
We did not adaptthe models to our population or to spontaneousspeech, thus insuring that wizards would receiverelatively noisy recognition output.We built trigram language models from thebook titles using the CMU Statistical LanguageModeling Toolkit.
Pilot tests with one male andone female native speaker indicated that a lan-guage model based on 7500 titles would yieldWER in the desired range.
(Average WER for thebook title requests in our experiment was 71%.)
Tomodel one aspect of the real world useful for an ac-tual system, titles with below average circulationwere eliminated.
An offline pilot study had demon-strated that one-word titles were easy for wizards,so we eliminated those as well.
A random sampleof 7,500 was chosen from the remaining 19,708titles to build the trigram language model.We used Ratcliff/Obersherhelp (R/O) to meas-ure the similarity of an ASR string to book titles inthe database (Ratcliff & Metzener, 1988).
R/O cal-culates the ratio r of the number of matching cha-racters to the total length of both strings, butrequires O(r2) time on average and O(r3) time inthe worst case.
We therefore computed an upperbound on the similarity of a title/ASR pair prior tofull R/O to speed processing.5 Experimental DesignIn this experiment, a user and a wizard sat in sepa-rate rooms where they could not overhear oneanother.
Each had a headset with microphone and aGUI.
Audio input on the wizard?s headset was dis-abled.
When the user requested a title, the ASRhypothesis for the title appeared on the wizard?sGUI.
The wizard then selected the ASR hypothesisto execute a voice search against the database.Given the ASR and the query return, the wi-zard?s task was to guess which candidate in thequery return, if any, matched the ASR hypothesis.Voice search accessed the full backend of 71,166titles.
The custom query designed for the experi-ment produced four types of return, in real time,based on R/O scores:?
Singleton: a single best candidate (R/O ?
0.85)?
AmbiguousList: two to five moderately goodcandidates (0.85 > R/O ?
0.55)?
NoisyList: six to ten poor but non-random can-didates (0.55 > R/O ?
0.40)?
Empty: No candidate titles (max R/O < 0.40)In pilot tests, 5%-10% of returns were empty ver-sus none in the experiment.
The distribution ofother returns was: 46.7% Singleton, 50.5% Ambi-guousList, and 2.8% NoisyList.Seven undergraduate computer science majorsat Hunter College participated.
Two were non-native speakers of English (one Spanish, one Ro-manian).
Each of the possible 21 pairs of studentsmet for five trials.
During each trial, one studentserved as wizard and the other as user for a sessionof 20 title cycles.
They immediately reversed rolesfor a second session, as discussed further below.The experiment yielded 4172 title cycles ratherthan the full 4200, because users were permitted toend sessions early.
All titles were selected from the7500 used to construct the language model.Each user received a printed list of 20 titles anda brief synopsis of each book.
The acoustic qualityof titles read individually from a list is unlikely toapproximate that of a patron asking for a specifictitle.
Therefore, immediately before each session,the user was asked to read a synopsis of each book,and to reorder the titles to reflect some logicalgrouping, such as genre or topic.
Users requestedtitles in this new order that they had created.Participants were encouraged to maximize a ses-sion score, with a reward for the experiment win-ner.
Scoring was designed to foster cooperative843strategies.
The wizard scored +1 for a correctlyidentified title, +0.5 for a thoughtful question, and-1 for an incorrect title.
The user scored +0.5 for asuccessfully recognized title.
User and wizardtraded roles for the second session, to discourageparticipants from sabotaging the others?
scores.The wizard?s GUI presented a real-time livefeed of ASR hypotheses, weighted by grayscale toreflect acoustic confidence.
Words in each candi-date title that matched a word in the ASR appeareddarker: dark black for Singleton or AmbiguousList,and medium black for NoisyList.
All other wordswere in grayscale in proportion to the degree ofcharacter overlap.
The wizard queried the databasewith a recognition hypothesis for one utterance at atime, but could concatenate successive utterances,possibly with some limited editing.After a query, the wizard?s GUI displayed can-didate matches in descending order of R/O score.The wizard had four options: make a firm choice ofa candidate, make a tentative choice, ask a ques-tion, or give up to end the title cycle.
Questionswere recorded.
The wizard?s GUI showed the suc-cess or failure of each title cycle before the nextone began.
The user?s GUI posted the 20 titles tobe read during the session.
On the GUI, the userrated the wizard?s title choices as correct or incor-rect.
Titles were highlighted green if the userjudged a wizard?s offered title correct, red if incor-rect, yellow if in progress, and not highlighted ifstill pending.
The user also rated the wizard?squestions.
Average elapsed time for each 20-titlesession was 15.5 minutes.A questionnaire similar to the type used inPARADISE evaluations (Walker et al, 1998) wasadministered to wizards and users for each pair ofsessions.
On a 5-point Likert scale, the average re-sponse to the question ?I found the system easy touse this time?
was 4 (sd=0; 4=Agree), indicatingthat participants were comfortable with the task.All other questions received an average score ofNeutral (3) or Disagree (2).
For example, partici-pants were neutral (3) regarding confidence inguessing the correct title, and disagreed (2) thatthey became more confident as time went on.6 Learning Method and GoalsTo model wizard actions, we assembled 60 fea-tures that would be available at run time.
Part ofour task was to detect their relative independence,meaningfulness, and predictive ability.
Featuresdescribed the wizard?s GUI, the current title ses-sion, similarity between ASR and candidates, ASRrelevance to the database, and recognition and con-fidence measures.
Because the number of voicesearch returns varied from one title to the next, fea-tures pertaining to candidates were averaged.We used three machine-learning techniques topredict wizards?
actions: decision trees, linear re-gression, and logistic regression.
All models wereproduced with the Weka data mining package, us-ing 10-fold cross-validation (Witten & Frank,2005).
A decision tree is a predictive model thatmaps feature values to a target value.
One applies adecision tree by tracing a path from the root (thetop node) to a leaf, which provides the target value.Here the leaves are the wizard actions: firm choice,tentative choice, question, or give up.
The algo-rithm used is a version of C4.5 (Quinlan, 1993),where gain ratio is the splitting criterion.To confirm the learnability and quality of thedecision tree models, we also trained logistic re-gression and linear regression models on the samedata, normalized in [0, 1].
The logistic regressionmodel predicts the probability of wizards?
actionsby fitting the data to a logistic curve.
It generalizesthe linear model to the prediction of categorical da-ta; here, categories correspond to wizards?
actions.The linear regression models represent wizards?actions numerically, in decreasing value: firmchoice, tentative choice, question, give up.Although analysis of individual wizards has notbeen systematic in other work, we consider thevariation in human performance significant.
Be-cause we seek excellent, not average, teachers forCheckItOut, our focus is on understanding goodwizardry.
Therefore, we learned two kinds of mod-els with each of the three methods: the overallmodel using data from all of our wizards, and indi-vidual wizard models.Preliminary cross-correlation confirmed thatmany of the 60 features were heavily interdepen-dent.
Through an initial manual curation phase, weisolated groups of features with R2 > 0.5.
Whenthese groups referenced semantically similar fea-tures, we selected a single representative from thegroup and retained only that one.
For example, thefeatures that described similarity between hypo-theses and candidates were highly correlated, sowe chose the most comprehensive one: the numberof exact word matches.
We also grouped together844and represented by a single feature: three featuresthat described the gaps between exact wordmatches, three that described the data presented tothe wizard, nine that described various system con-fidence scores, and three that described the user?sspeaking rate.
This left 28 features.Next we ran CfsSubsetEval, a supervisedattribute selection algorithm for each model(Witten & Frank, 2005).
This greedy, hill-climbingalgorithm with backtracking evaluates a subset ofattributes by the predictive ability of each featureand the degree of redundancy among them.
Thisprocess further reduced the 28 features to 8-12 fea-tures per model.
Finally, to reduce overfitting fordecision trees, we used pruning and subtree rising.For linear regression we used the M5 method, re-peatedly removing the attribute with the smalleststandardized coefficient until there was no furtherimprovement in the error estimate given by theAkaike information criterion.7 ResultsTable 1 shows the number of title cycles per wi-zard, the raw session score according to the formu-la given to the wizards, and accuracy.
Accuracy isthe proportion of title cycles where the wizardfound the correct title, or correctly guessed that thecorrect title was not present (asked a question orgave up).
Note that score and accuracy are highlycorrelated (R=0.91, p=0.0041), indicating that theinstructions to participants elicited behavior con-sistent with what we wanted to measure.Wizards clearly differed in performance, large-ly due to their response when the candidate list didnot include the correct title.
Analysis of variancewith wizard as predictor and accuracy as the de-pendent variable is highly significant (p=0.0006);significance is somewhat greater (p=0.0001) wheresession score is the dependent variable.
Table 2shows the distribution of correct actions: to offer acandidate at a given position in the query return(Returns 1 through 9), or to ask a question or giveup.
As reflected in Table 2, a baseline accuracy ofabout 65% could be achieved by offering the firstreturn.
The fifth column of Table 1 shows how of-ten wizards did that (Offered Return 1), and clearlyillustrates that those who did so most often (W3and W6) had accuracy results closest to the base-line.
The wizard who did so least often (W4) hadthe highest accuracy, primarily because she moreoften correctly offered no title, as shown in the lastcolumn of Table 1.
We conclude that a spoken di-alogue system would do well to emulate W4.Overall, our results in modeling wizards?
actionswere uniform across the three learning methods,gauged by accuracy and F measure.
For the com-bined wizard data, logistic regression had an accu-racy of 75.2%, and F measures of 0.83 for firmchoices and 0.72 for tentative choices; the decisiontree accuracy was 82.2%, and the F measures forfirm versus tentative choices were respectively0.82 and 0.71.
The decision tree had a root meansquared error of 0.306, linear regression 0.483.
Ta-ble 3 shows the accuracy and F measures on firmchoices for the decision trees by individual wizard,along with the numbers of attributes and nodes perTable 1.
Raw session score, accuracy, proportion of offered titles that were listed first in the query return, andfrequency of correct non-offers for seven participants.Participant Cycles Session Score Accuracy Offered Return 1 Correct Non-OffersW4 600 0.7585 0.8550 0.70 0.64W5 600 0.7584 0.8133 0.76 0.43W7 599 0.6971 0.7346 0.76 0.14W1 593 0.6936 0.7319 0.79 0.16W2 599 0.6703 0.7212 0.74 0.10W3 581 0.6648 0.6954 0.81 0.20W6 600 0.6103 0.6950 0.86 0.03Table 2.
Distribution of correct actionsCorrect Action N %Return 1 2722 65.2445Return 2 126 3.0201Return 3 56 1.3423Return 4 46 1.1026Return 5 26 0.6232Return 7 7 0.1678Return 8 1 0.0002Return 9 2 0.0005Question or Giveup 1186 28.4276Total 4172 1.0000845tree.
Although relatively few attributes appeared inany one tree, most attributes appeared in multiplenodes.
W1 was the exception, with a very smallpruned tree of 7 nodes.Accuracy of the decision trees does not correlatewith wizard rank.
In general, the decision treescould consistently predict a confident choice (0.80?
F ?
0.87), but were less consistent on a tentativechoice (0.60 ?
F ?
0.89), and could predict a ques-tion only for W4, the wizard with the highest accu-racy and greatest success at detecting when thecorrect title was not in the candidates.What wizards saw on the GUI, their recent suc-cess, and recognizer confidence scores were keyattributes in the decision trees.
The five featuresthat appeared most often in the root and top-levelnodes of all tree models reported in Table 3 were:?
DisplayType of the return (Singleton, Ambi-guous List, NoisyList)?
RecentSuccess, how often the wizard chose thecorrect title within the last three title cycles?
ContiguousWordMatch, the maximum numberof contiguous exact word matches between acandidate and the ASR hypothesis (averagedacross candidates)?
NumberOfCandidates, how many titles were re-turned by the voice search?
Confidence, the Helios confidence scoreDisplayType, NumberOfCandidates and Conti-guousWordMatch pertain to what the wizard couldsee on her GUI.
(Recall that DisplayType is distin-guished by font darkness, as well as by number ofcandidates.)
The impact of RecentSuccess mightresult not just from the wizard?s confidence in hercurrent strategy, but also from consistency in theuser?s speech characteristics.
The Helios confi-dence annotation uses a learned model based onfeatures from the recognizer, the parser, and the di-alogue state.
Here confidence primarily reflectsrecognition confidence; due to the simplicity of ourgrammar, parse results only indicate whether thereis a parse.
In addition to these five features, everytree relied on at least one measure of similarity be-tween the hypothesis and the candidates.W4 achieved superior accuracy: she knew whento offer a title and when not to.
In the learned treefor W4, if the DisplayType was NoisyList, W4asked a question; if DisplayType was Ambiguous-List, the features used to predict W4?s action in-cluded the five listed above, along with the acous-tic model score, word length of the ASR, numberof times the wizard had asked the user to repeat,and the maximum size of the gap between words inthe candidates that matched the ASR hypothesis.To focus on W4?s questioning behavior, wetrained an additional decision tree to learn how W4chose between two actions: offering a title versusasking a question.
This 37-node, 8-attribute treewas based on 600 data points, with F=0.91 formaking an offer and F=0.68 for asking a question.The tree is distinctive in that it splits at the root onthe number of frames in the ASR.
If the ASR isshort (as measured both by the number of recogni-tion frames and the words), W4 asks a questionwhen DisplayType = AmbiguousList or NoisyList,either RecentSuccess ?
1 or ContiguousWord-Match = 0, and the acoustic model score is low.Note that shorter titles are more confusable.
If theASR is long, W4 asks a question when Conti-guousWordMatch ?
1, RecentSuccess ?
2, and ei-ther CandidateDisplay = NoisyList, or Confidenceis low, and there is a choice of titles.8 DiscussionOur experiment addressed whether voice searchcan compensate for incorrect ASR hypotheses andpermit identification of a user?s desired book, giv-en a request by title.
The results show that withhigh WER, a baseline dialogue strategy that alwaysoffers the highest-ranked database return can nev-ertheless achieve moderate accuracy.
This is trueeven with the relatively simplistic measure of simi-larity between the ASR hypothesis and candidatetitles used here.
As a result, we have integratedvoice search into CheckItOut, along with a linguis-tically motivated grammar for book titles.
Our cur-rent Phoenix grammar relies on CFG rulesautomatically generated from dependency parsesof the book titles, using the MICA parserTable 3.
Learning results for wizardsTree Rank Nodes Attributes Accuracy F firmW4 1  55 12 75.67 0.85W5 2  21 10 76.17 0.85W1 3  7 8 80.44 0.87W7 4  45 11 73.62 0.83W3 5  33 10 77.42 0.84W2 6  35 10 78.49 0.85W6 7  23 10 85.19 0.80846(Bangalore et al, 2009).
As described in (Gordon& Passonneau, 2010), a book title parse can con-tain multiple title slots that consume discontinuoussequences of words from the ASR hypothesis, thusaccommodating noisy ASR.
For the voice searchphase, we now concatenate the words consumed bya sequence of title slots.
We are also experimentingwith a statistical machine learning approach thatwill replace or complement the semantic parsing.Computers clearly do some tasks faster andmore accurately than people, including databasesearch.
To benefit from such strengths, a dialoguesystem should also accommodate human prefe-rences in dialogue strategy.
Previous work hasshown that user satisfaction depends in part on tasksuccess, but also on minimizing behaviors that canincrease task success but require the user to correctthe system (Litman et al, 2006).The decision tree that models W4 has lower ac-curacy than other models?
(see Table 3), in part be-cause her decisions had finer granularity.
A spokendialogue system could potentially do as well as orbetter than the best human at detecting when thetitle is not present, given the proper training data.To support this, a dataset could be created that wasbiased toward a larger proportion of cases wherenot offering a candidate is the correct action.9 Conclusion and Current WorkThis paper presents a novel methodology that em-beds wizards in a spoken dialogue system, and col-lects data for a single turn exchange.
Our resultsillustrate the merits of ranking wizards, and learn-ing from the best.
Our wizards were uniformlygood at choosing the correct title when it waspresent, but most were overly eager to identify atitle when it was not among the candidates.
In thisrespect, the best wizard (W4) achieved the highestaccuracy because she demonstrated a much greaterability to know when not to offer a title.
We haveshown that it is feasible to replicate this ability in amodel learned from features that include the pres-entation of the search results (length of the candi-date list, amount of word overlap of candidateswith the ASR hypothesis), recent success at select-ing the correct candidate, and measures pertainingto recognition results (confidence, acoustic modelscore, speaker rate).
If replicated in a spoken di-alogue system, such a model could support integra-tion of voice search in a way that avoidsmisunderstandings.
We conclude that learningfrom embedded wizards can exploit a wider rangeof relevant features, that dialogue managers canprofit from access to more fine-grained representa-tions of user utterances, and that machine learnersshould be selective about which people to model.That wizard actions can be modeled using sys-tem features bodes well for future work.
Our nextexperiment will collect full dialogues with embed-ded wizards whose actions will again be restrictedthrough an interface.
This time, NLU will integratevoice search with the linguistically motivated CFGrules for book titles described earlier, and a largerlanguage model and grammar for database entities.We will select wizards who perform well duringpilot tests.
Again, the goal will be to model themost successful wizards, based upon data fromrecognition results, NLU, and voice search results.AcknowledgementsThis research was supported by the NationalScience Foundation under IIS-0745369, IIS-084966, and IIS-0744904.
We thank the anonym-ous reviewers, the Heiskell Library, our CMU col-laborators, our statistical wizard Liana Epstein, andour enthusiastic undergraduate research assistants.ReferencesBangalore, Srinivas; Bouillier, Pierre; Nasr, Alexis;Rambow, Owen; Sagot, Benoit (2009).
MICA: aprobabilistic dependency parser based on treeinsertion grammars.
Application Note.
HumanLanguage Technology and North American Chapterof the Association for Computational Linguistics,pp.
185-188.Bohus, D.; Rudnicky, A.I.
(2009).
The RavenClawdialog management framework: Architecture andsystems.
Computer Speech and Language, 23(3),332-361.Bohus, Daniel; Rudnicky, Alex (2002).
Integratingmultiple knowledge sources for utterance-levelconfidence annotation in the CMU Communicatorspoken dialog system (Technical Report No.
CS-190): Carnegie Mellon University.Georgila, Kallirroi; Sgarbas, Kyrakos; Tsopanoglou,Anastasios; Fakotakis, Nikos; Kokkinakis, George(2003).
A speech-based human-computer interactionsystem for automating directory assistance services.International Journal of Speech Technology, SpecialIssue on Speech and Human-Computer Interaction,6(2), 145-59.847Gordon, Joshua, B.; Passonneau, Rebecca J.
(2010).
Anevaluation framework for natural languageunderstanding in spoken dialogue systems.
SeventhInternational Conference on Language Resourcesand Evaluation (LREC).Johnston, Michael; Bangalore, Srinivas; Vasireddy,Gunaranjan; Stent, Amanda; Ehlen, Patrick; Walker,Marilyn A., et al (2002).
MATCH--An architecturefor multimodal dialogue systems.
Proceedings of the40th Annual Meeting of the Association forComputational Linguistics, pp.
376-83.Komatani, Kazunori; Kanda, Naoyuki; Ogata, Tetsuya;Okuno, Hiroshi G. (2005).
Contextual constraintsbased on dialogue models in database search taskfor spoken dialogue systems.
The Ninth EuropeanConference on Speech Communication andTechnology (Eurospeech), pp.
877-880.Kruijff-Korbayov?, Ivana; Blaylock, Nate;Gerstenberger, Ciprian; Rieser, Verena; Becker,Tilman; Kaisser, Michael, et al (2005).
Anexperiment setup for collecting data for adaptiveoutput planning in a multimodal dialogue system.10th European Workshop on Natural LanguageGeneration (ENLG), pp.
191-196.Levin, Esther; Narayanan, Shrikanth; Pieraccini,Roberto; Biatov, Konstantin; Bocchieri, E.; DeFabbrizio, Giuseppe, et al (2000).
The AT&T-DARPA Communicator Mixed-Initiative SpokenDialog System.
Sixth International Conference onSpoken Dialogue Processing (ICLSP), pp.
122-125.Litman, Diane; Hirschberg, Julia; Swerts, Marc (2006).Characterizing and predicting corrections in spokendialogue systems.
Computational Linguistics, 32(3),417-438.Litman, Diane; Pan, Shimei (1999).
Empiricallyevaluating an adaptable spoken dialogue system.
7thInternational Conference on User Modeling (UM),pp.
55-46.Quinlan, J. Ross (1993).
C4.5: Programs for MachineLearning.
San Mateo, CA: Morgan Kaufmann.Ratcliff, John W.; Metzener, David (1988).
PatternMatching: The Gestalt Approach.
Dr. Dobb'sJournal, 46Raux, Antoine; Bohus, Dan; Langner, Brian; Black,Alan W.; Eskenazi, Maxine (2006).
Doing researchon a deployed spoken dialogue system: one year ofLet's Go!
experience.
Ninth InternationalConference on Spoken Language Processing(Interspeech/ICSLP).Raux, Antoine; Eskenazi, Maxine (2007).
A Multi-layerarchitecture for semi-synchronous event-drivendialogue management.IEEE Workshop onAutomatic Speech Recognition and Understanding(ASRU 2007), Kyoto, Japan.Raux, Antoine; Langner, Brian; Black, Alan W.;Eskenazi, Maxine (2005).
Let's Go Public!
Taking aspoken dialog system to the real world.Interspeech2005 (Eurospeech), Lisbon, Portugal.Rieser, Verena; Kruijff-Korbayov?, Ivana; Lemon,Oliver (2005).
A corpus collection and annotationframework for learning multimodal clarificationstrategies.
Sixth SIGdial Workshop on Discourseand Dialogue, pp.
97-106.Sacks, Harvey; Schegloff, Emanuel A.; Jefferson, Gail(1974).
A simplest systematics for the organizationof turn-taking for conversation.
Language, 50(4),696-735.Skantze, Gabriel (2003).
Exploring human errorhandling strategies: Implications for SpokenDialogue Systems.
Proceedings of ISCA Tutorialand Research Workshp on Error Handling in SpokenDialogue Systems, pp.
71-76.Stoyanchev, Svetlana; Stent, Amanda (2009).Predicting concept types in user corrections indialog.
Proceedings of the EACL Workshop SRSL2009, the Second Workshop on SemanticRepresentation of Spoken Language, pp.
42-49.Turunen, Markku; Hakulinen, Jaakko; Kainulainen,Anssi (2006).
Evaluation of a spoken dialoguesystem with usability tests and long-term pilotstudies.
Ninth International Conference on SpokenLanguage Processing (Interspeech 2006 - ICSLP).Walker, M A.; Litman, D, J.; Kamm, C. A.; Abella, A.(1998).
Evaluating Spoken Dialogue Agents withPARADISE: Two Case Studies.
Computer Speechand Language, 12, 317-348.Wang, Ye-Yi; Yu, Dong; Ju, Yun-Cheng; Acero, Alex(2008).
An introduction to voice search.
IEEESignal Process.
Magazine, 25(3).Ward, Wayne; Issar, Sunil (1994).
Recent improvementsin the CMU spoken language understandingsystem.ARPA Human Language TechnologyWorkshop, Plainsboro, NJ.Williams, Jason D.; Young, Steve (2004).Characterising Task-oriented Dialog using aSimulated ASR Channel.
Eight InternationalConference on Spoken Language Processing(ICSLP/Interspeech), pp.
185-188.Witten, Ian H.; Frank, Eibe (2005).
Data Mining:Practical Machine Learning Tools and Techniques(2nd ed.).
San Francisco: Morgan Kaufmann.Zollo, Teresa (1999).
A study of human dialoguestrategies in the presence of speech recognitionerrors.
Proceedings of AAAI Fall Symposium onPsychological Models of Communication inCollaborative Systems, pp.
132-139.Zue, Victor; Seneff, Stephanie; Glass, James; Polifroni,Joseph; Pao, Christine; Hazen, Timothy J., et al(2000).
A Telephone-based conversational interfacefor weather information.
IEEE Transactions onSpeech and Audio Processing, 8, 85-96.848
