Proceedings of the Third Linguistic Annotation Workshop, ACL-IJCNLP 2009, pages 121?124,Suntec, Singapore, 6-7 August 2009. c?2009 ACL and AFNLPUsing Parallel Propbanks to enhance Word-alignmentsJinho D. ChoiDept.
of Computer ScienceUniv.
of Colorado at Boulderchoijd@colorado.eduMartha PalmerDept.
of LinguisticsUniv.
of Colorado at Bouldermpalmer@colorado.eduNianwen XueDept.
of Computer ScienceBrandeis Universityxuen@brandeis.eduAbstractThis short paper describes the use of thelinguistic annotation available in paral-lel PropBanks (Chinese and English) forthe enhancement of automatically derivedword alignments.
Specifically, we sug-gest ways to refine and expand wordalignments for verb-predicates by usingpredicate-argument structures.
Evalua-tions demonstrate improved alignment ac-curacies that vary by corpus type.1 IntroductionSince verbs tend to be the roots of dependency re-lations in a sentence (Palmer et al, 2005), when itcomes down to translations, finding correct map-pings between verbs in a source and a target lan-guage is very important.
Many machine transla-tion systems (Fraser and Marcu, 2007) use word-alignment tools such as GIZA++ (Och and Ney,2003) to retrieve word mappings between a sourceand a target language.
Although GIZA++ giveswell-structured alignments, it has limitations inseveral ways.
First, it is hard to verify if align-ments generated by GIZA++ are correct.
Second,GIZA++ may not find alignments for low-frequentwords.
Third, GIZA++ does not account for anysemantic information.In this paper, we suggest a couple of ways toenhance word-alignments for predicating expres-sions such as verbs1.
We restricted the sourceand the target language to Chinese and English,respectively.
The goal is to use the linguisticannotation available in parallel PropBanks (Xueand Palmer, 2009) to refine and expand automaticword-alignments.
First, we check if the alignmentfor each Chinese predicate, generated by GIZA++,is also a predicate in English (Section 3).
If it is,we verify if the alignment is correct by matching1Throughout the paper, all predicates refer to verbs.their arguments (Section 4.1).
If it is not, we findan English predicate that has the maximum argu-ment matching with the Chinese predicate (Sec-tion 4.2).
Finally, we evaluate the potential of theenhanced word-alignments for providing a signif-icant improvement over the GIZA++ baseline.2 Parallel CorpusWe used the ?English Chinese Translation Tree-bank?
(ECTB), a parallel English-Chinese cor-pus.
In addition to the treebank syntactic struc-ture, the corpus has also been annotated withsemantic role labels in the standard PropBankstyle of Arg0, Arg1, etc., based on verb specificframe file definitions (Xue and Palmer, 2009).The corpus is divided into two parts: the Xin-hua Chinese newswire with literal English trans-lations (4,363 parallel sentences) and the Sino-rama Chinese news magazine with non-literal En-glish translations (12,600 parallel sentences).
Weexperimented with the two parts separately tosee how literal and non-literal translations affectword-alignments.3 Predicate MatchingFor preprocessing, we ran GIZA++ on ECTB toget word-alignments between Chinese and En-glish.
Then, for each Chinese predicate, wechecked if it is aligned to an English predicate byusing the gold-standard parallel Propbanks.
Ta-ble 1 shows how many Chinese predicates werealigned to what kind of English words.Only (45.3%-Xinhua, 19.1%-Sinorama) of Chi-nese predicates were aligned to words that arepredicates in English.
It is true that not all Chi-nese verbs are supposed to be translated to verbsin English, but that does not account for the num-bers in Table 1.
We therefore assume that thereare opportunities to enhance word-alignments forChinese and English predicates.121Alignment Xinhua SinoramaCh.pred?
En.pred 5,842 7,643Ch.pred?
En.be 386 1,229Ch.pred?
En.else 2,489 8,726Ch.pred?
En.none 4,178 22,488Total 12,895 40,086Table 1: Results of predicate matching (Ch: Chi-nese, En: English, pred: predicates, be: be-verbs,else: non-verbs, none: no word).
The numbers in-dicate the amount of verb-tokens, not verb-types.4 Argument MatchingFor Chinese predicates aligned to English predi-cates, we can verify the alignments by ?Top-downargument matching?
: given Chinese and Englishpredicates that are aligned, check if their argu-ments are also aligned (arguments are found fromparallel Propbanks).
The intuition is that if thepredicates are correctly aligned across the lan-guages, their arguments should be aligned as well.For Chinese predicates not aligned to any En-glish words, we can find their potential Englishalignments by ?Bottom-up argument matching?
:given a set of arguments for a such Chinese predi-cate, find some English predicate whose set of ar-guments has the most words aligned to words inthe Chinese arguments.
If the words in the argu-ments are mostly aligned (above a certain thresh-old) across the languages, we suspect that thepredicates should be aligned as well.4.1 Top-down Argument Matching (T-D)Given a Chinese predicate pc aligned to an Englishpredicate pe, let Sc and Se be a set of argumentsfor pc and pe, respectively.
For each cai ?
Sc, wematch it with some eaj ?
Se that has the mostwords aligned to words in cai.
If such eaj ex-ists, we count the number of aligned words, say|cai ?
eaj |; otherwise, the count is 0.
Once thematchings are done, we average the proportionsof the counts and if the average is above a certainthreshold, we consider the alignment is correct.Let us look at the example in Table 2.
Af-ter the preprocessing, a Chinese predicate ???
?is aligned to an English predicate ?set up?
byGIZA++.
????
has two arguments, Ch.Arg0 andCh.Arg1, retrieved from the Chinese Propbank.For each Chinese argument, we search for someargument of ?set?
(from the English Propbank) that?
Chinese Sentence ?
: ????????????????????
?- Predicate: ??.01?
set up- Ch.Arg0: ?????
those municipalities- Ch.Arg1: ???????????
fourteen border economic cooperation zones?
English Sentence ?
: At the same time it also sanctioned those municipalitiesto set up fourteen border economic cooperation zones- Predicate: set.03 (set up)- En.Arg0: those municipalities- En.Arg1: fourteen border economic cooperation zonesTable 2: Parallel sentences labelled with their se-mantic roleshas the most words aligned.
For instance, wordsin Ch.Arg0, ???
??
?, are aligned to ?thosemunicipalities?
by GIZA++ so Ch.Arg0 findsEn.Arg0 as the one maximizes word-interscetions(similar for Ch.Arg1 and En.Arg1).
In this case,the argument matchings for all pairs of argumentsare 100%, so we consider the alignment is correct.Table 3 shows the average argument matchingscores for all pairs of Chinese and English predi-cates.
For each pair of predicates, ?macro-average?measures the proportion of word-intersections foreach pair of Chinese and English arguments (withthe most words aligned) and averages the pro-portions whereas ?micro-average?
counts word-intersections for all pairs of arguments (each pairwith the most words aligned) and divides it by thetotal number of words in Chinese arguments.?
Sc = a set of Chinese arguments, cai ?
Sc?
Se = a set of English arguments, eaj ?
Se?
Macro average argument matching score= 1|Sc|?
?cai(argmax(|cai ?
eaj |)|cai| )?
Micro average argument matching score=?
?cai argmax(|cai ?
eaj |)?
?cai |cai|Xinhua SinoramaMacro Avg.
80.55% 53.56%Micro Avg.
83.91% 52.62%Table 3: Average argument matching scores fortop-down argument matching122It is not surprising that Xinhua?s scores arehigher because the English sentences in Xinhuaare more literally translated than ones in Sinoramaso that it is easier to find correct alignments in Xin-hua.4.2 Bottom-Up Argument Matching (B-U)A large portion of Chinese predicates are alignedto no English words.
For such Chinese predicate,say pc, we check to see if there exists an Englishpredicate within the parallel sentence, say pe, thatis not aligned to any Chinese word and gives themaximum micro-average score (Section 4.1) com-pare to all other predicates in the English sen-tence.
If the micro-average score is above a certainthreshold, we align pc to pe.The thresholds we used are 0.7 and 0.8.
Thresh-olds below 0.7 assumes too many alignments thatare incorrect and ones above 0.8 assumes too fewalignments to be useful.
Table 4 shows the averageargument matching scores for alignments found bybottom-up argument matching.Xinhua SinoramaThresh.
0.7 0.8 0.7 0.8Macro 80.74 83.99 77.70 82.86Micro 82.63 86.46 79.45 85.07Table 4: Average argument matching scores inpercentile for bottom-up argument matching5 EvaluationsEvaluations are done by a Chinese-English bilin-gual.
We used a different English-Chinese paral-lel corpus for evaluations.
There are 100 paral-lel sentences, 365 Chinese verb-tokens, and 273Chinese verb-types in the corpus.
We testedword-alignments, refined and expanded by our ap-proaches, on verb-types rather than verb-tokensto avoid over-emphasizing multiple appearancesof a single type.
Furthermore, we tested word-alignments from Xinhua and Sinorama separatelyto see how literal and non-literal translations affectthe outcomes.5.1 Refining word-alignmentWe used three kinds of measurements for compar-isons: term coverage, term expansion, and align-ment accuracy.
?Term coverage?
shows how manysource terms (Chinese verb-types) are covered byword-alignments found in each corpus.
Out of273 Chinese verb-types in the test corpus, (79-Xinhua, 129-Sinorama) were covered by word-alignments generated by GIZA++.
?Term expan-sion?
shows how many target terms (English verb-types) are suggested for each of the covered sourceterms.
There are on average (1.77-Xinhua, 2.29-Sinorama) English verb-types suggested for eachcovered Chinese verb-type.
?Alignment accuracy?shows how many of the suggested target terms arecorrect.
Among the suggested English verb-types,(83.35%-Xinhua, 57.76%-Sinorama) were correcton average.The goal is to improve the alignment accu-racy with minimum reduction of the term cov-erage and expansion.
To accomplish the goal,we set a threshold for the T-D?s macro-averagescore: for Chinese predicates aligned to Englishpredicates, we kept only alignments whose macro-average scores meet or exceed a certain threshold.The thresholds we chose are 0.4 and 0.5; lowerthresholds did not have much effect and higherthresholds threw out too many alignments.
Table 5shows the results of three measurements with re-spect to the thresholds (Note that all these align-ments were generated by GIZA++).Xinhua SinoramaTH TC ATE AAA TC ATE AAA0.0 79 1.77 83.35 129 2.29 57.760.4 76 1.72 83.54 93 1.8 65.880.5 76 1.68 83.71 62 1.58 78.09Table 5: Results for alignment refinement (TH:threshold, TC: term coverage, ATE: average termexpansion, AAA: average alignment accuracy inpercentage).
The highest score for each measure-ment is marked as bold.As you can see, thresholds did not have mucheffect on alignments found in Xinhua.
This isunderstandable because the translations in Xin-hua are so literal that it was relatively easy forGIZA++ to find correct alignments; in otherwords, the alignments generated by GIZA++ werealready very accurate.
However, for alignmentsfound in Sinorama, the average alignment accu-racy increases radically as the threshold increases.This implies that it is possible to refine word-alignments found in a corpus containing manynon-literal translations by using T-D.Notice that the term coverage for Sinorama de-creases as the threshold increases.
Considering123how much improvement it made for the averagealignment accuracy, we suspect that it filtered outmostly ones that were incorrect alignments.5.2 Expanding word-alignmentWe used B-U to expand word-alignments for Chi-nese predicates aligned to no English words.
Wedecided not to expand alignments for Chinesepredicates aligned to non-verb English words be-cause GIZA++ generated alignments are more ac-curate than ones found by B-U in general.There are (22-Xinhua, 20-Sinorama) additionalverb-types covered by the expanded-alignments.Note that these alignments are already filtered bythe micro-average score (Section 4.2).
To refinethe alignments even more, we set a threshold onthe macro-average score as well.
The thresholdswe used for the macro-average score are 0.6 and0.7.
Table 6 shows the results of the expanded-alignments found in Xinhua and Sinorama.Mac - 0.7 Mac - 0.8TC ATE AAA TC ATE AAAMic Xinhua0.0 22 4.27 50.38 20 3.35 57.500.6 21 3.9 54.76 18 3.39 63.890.7 19 3.47 55.26 17 3.12 61.76Mic Sinorama0.0 37 3.59 18.01 29 3.14 14.950.6 31 3.06 15.11 27 2.93 14.460.7 21 2.81 11.99 25 2.6 11.82Table 6: Results for expanded-alignments found inXinhua and Sinorama (Mac: threshold on macro-average score, Mic: threshold on micro-averagescore)The average alignment accuracy for Xinhua isencouraging; it shows that B-U can expand word-alignments for a corpus with literal translations.The average alignment accuracy for Sinorama issurprisingly low; it shows that B-U cannot func-tion effectively given non-literal translations.6 Summary and Future WorksWe have demonstrated the potential for using par-allel Propbanks to improve statistical verb transla-tions from Chinese to English.
Our B-U approachshows promise for expanding the term-coverageof GIZA++ alignments that are based on literaltranslations.
In contrast, our T-D is most effec-tive with non-literal translations for verifying thealignment accuracy, which has been proven diffi-cult for GIZA++.This is still a preliminary work but in the fu-ture, we will try to enhance word-alignmentsby using automatically labelled Propbanks, Nom-banks (Meyers et al, 2004), Named-entity tag-ging, and test the enhancement on bigger corpora.Furthermore, we will also evaluate the integrationof our enhanced alignments with statistical ma-chine translation systems.AcknowledgmentsSpecial thanks to Daniel Gildea, Ding Liu(University of Rochester) who provided word-alignments, Wei Wang (Information Sciences In-stitute at University of Southern California) whoprovided the test-corpus, and Hua Zhong (Uni-versity of Colorado at Boulder) who performedthe evaluations.
We gratefully acknowledgethe support of the National Science FoundationGrants IIS-0325646, Domain Independent Seman-tic Parsing, CISE-CRI-0551615, Towards a Com-prehensive Linguistic Annotation, and a grantfrom the Defense Advanced Research ProjectsAgency (DARPA/IPTO) under the GALE pro-gram, DARPA/CMO Contract No.
HR0011-06-C-0022, subcontract from BBN, Inc. Any contentsexpressed in this material are those of the authorsand do not necessarily reflect the views of the Na-tional Science Foundation.ReferencesAlexander Fraser and Daniel Marcu.
2007.
Measuringword alignment quality for statistical machine trans-lation.
Computational Linguistics, 33(3):293?303.A.
Meyers, R. Reeves, C. Macleod, R. Szekely,V.
Zielinska, B.
Young, and R. Grishman.
2004.The nombank project: An interim report.
In HLT-NAACL 2004 Workshop: Frontiers in Corpus Anno-tation, pages 24?31.Franz Josef Och and Hermann Ney.
2003.
A sys-tematic comparison of various statistical alignmentmodels.
Computational Linguistics, 29(1):19?51.Martha Palmer, Daniel Gildea, and Paul Kingsbury.2005.
The proposition bank: An annotated cor-pus of semantic roles.
Computational Linguistics,31(1):71?106.Nianwen Xue and Martha Palmer.
2009.
Adding se-mantic roles to the chinese treebank.
Natural Lan-guage Engineering, 15(1):143?172.124
