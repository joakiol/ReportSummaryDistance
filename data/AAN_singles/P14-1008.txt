Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 79?89,Baltimore, Maryland, USA, June 23-25 2014.c?2014 Association for Computational LinguisticsLogical Inference on Dependency-based Compositional SemanticsRan Tian Yusuke Miyao Takuya MatsuzakiNational Institute of Informatics, Japan{tianran,yusuke,takuya-matsuzaki}@nii.ac.jpAbstractDependency-based Compositional Se-mantics (DCS) is a framework of naturallanguage semantics with easy-to-processstructures as well as strict semantics.
Inthis paper, we equip the DCS frameworkwith logical inference, by defining ab-stract denotations as an abstraction ofthe computing process of denotations inoriginal DCS.
An inference engine is builtto achieve inference on abstract denota-tions.
Furthermore, we propose a way togenerate on-the-fly knowledge in logicalinference, by combining our frameworkwith the idea of tree transformation.Experiments on FraCaS and PASCALRTE datasets show promising results.1 IntroductionDependency-based Compositional Semantics(DCS) provides an intuitive way to model seman-tics of questions, by using simple dependency-liketrees (Liang et al, 2011).
It is expressive enoughto represent complex natural language queries ona relational database, yet simple enough to belatently learned from question-answer pairs.
Inthis paper, we equip DCS with logical inference,which, in one point of view, is ?the best wayof testing an NLP system?s semantic capacity?
(Cooper et al, 1996).It should be noted that, however, a frameworkprimarily designed for question answering is notreadily suited for logical inference.
Because, an-swers returned by a query depend on the specificdatabase, but implication is independent of anydatabases.
For example, answers to the question?What books are read by students?
?, should al-ways be a subset of answers to ?What books areever read by anyone?
?, no matter how we store thedata of students and how many records of booksare there in our database.Thus, our first step is to fix a notation which ab-stracts the calculation process of DCS trees, so asto clarify its meaning without the aid of any exist-ing database.
The idea is to borrow a minimal setof operators from relational algebra (Codd, 1970),which is already able to formulate the calculationin DCS and define abstract denotation, which isan abstraction of the computation of denotationsguided by DCS trees.
Meanings of sentences thencan be represented by primary relations amongabstract denotations.
This formulation keeps thesimpleness and computability of DCS trees mostlyunaffected; for example, our semantic calculationfor DCS trees is parallel to the denotation compu-tation in original DCS.An inference engine is built to handle inferenceon abstract denotations.
Moreover, to compensatethe lack of background knowledge in practical in-ference, we combine our framework with the ideaof tree transformation (Bar-Haim et al, 2007), topropose a way of generating knowledge in logicalrepresentation from entailment rules (Szpektor etal., 2007), which are by now typically consideredas syntactic rewriting rules.We test our system on FraCaS (Cooper et al,1996) and PASCAL RTE datasets (Dagan et al,2006).
The experiments show: (i) a competi-tive performance on FraCaS dataset; (ii) a bigimpact of our automatically generated on-the-flyknowledge in achieving high recall for a logic-based RTE system; and (iii) a result that outper-forms state-of-the-art RTE system on RTE5 data.Our whole system is publicly released and canbe downloaded from http://kmcs.nii.ac.jp/tianran/tifmo/.2 The IdeaIn this section we describe the idea of represent-ing natural language semantics by DCS trees, andachieving inference by computing logical relationsamong the corresponding abstract denotations.79SUBJreadstudent bookOBJARG ARGFigure 1: The DCS tree of ?students read books?studentARGMarkJohnEmily...bookARGA Tale of Two CitiesUlysses...readSUBJ OBJMark New York TimesMary A Tale of Two CitiesJohn Ulysses... ...Table 1: Databases of student, book, and read2.1 DCS treesDCS trees has been proposed to represent natu-ral language semantics with a structure similar todependency trees (Liang et al, 2011) (Figure 1).For the sentence ?students read books?, imaginea database consists of three tables, namely, a setof students, a set of books, and a set of ?reading?events (Table 1).
The DCS tree in Figure 1 is in-terpreted as a command for querying these tables,obtaining ?reading?
entries whose ?SUBJ?
fieldis student and whose ?OBJ?
field is book.
Theresult is a set {John reads Ulysses, .
.
.
}, which iscalled a denotation.DCS trees can be extended to represent linguis-tic phenomena such as quantification and coref-erence, with additional markers introducing addi-tional operations on tables.
Figure 2 shows an ex-ample with a quantifier ?every?, which is markedas ???
on the edge (love)OBJ-ARG(dog) and in-terpreted as a division operator qOBJ?(?2.2).
Op-timistically, we believe DCS can provide a frame-work of semantic representation with sufficientlywide coverage for real-world texts.The strict semantics of DCS trees brings us theidea of applying DCS to logical inference.
This isnot trivial, however, because DCS works under theassumption that databases are explicitly available.Obviously this is unrealistic for logical inferenceon unrestricted texts, because we cannot preparea database for everything in the world.
This factfairly restricts the applicable tasks of DCS.Our solution is to redefine DCS trees withoutthe aid of any databases, by considering each nodeof a DCS tree as a content word in a sentence (butmay no longer be a table in a specific database),while each edge represents semantic relations be-tween two words.
The labels on both ends ofan edge, such as SUBJ (subject) and OBJ (ob-ject), are considered as semantic roles of the cor-SUBJreadstu enbueoOBJARG ARGotadARGOBJSUBJotadke??
?t?OBJARG ARGstu SUBJread ?
?OBJARG ARG ke?
?SUBJARGT: H:?Figure 2: DCS trees of ?Mary loves every dog?
(Left-Up), ?Tom has a dog?
(Left-Down), and?Tom has an animal that Mary loves?
(Right).responding words1.
To formulate the databasequerying process defined by a DCS tree, we pro-vide formal semantics to DCS trees by employingrelational algebra (Codd, 1970) for representingthe query.
As described below, we represent mean-ings of sentences with abstract denotations, andlogical relations among sentences are computedas relations among their abstract denotations.
Inthis way, we can perform inference over formulasof relational algebra, without computing databaseentries explicitly.2.2 Abstract denotationsAbstract denotations are formulas constructedfrom a minimal set of relational algebra (Codd,1970) operators, which is already able to formu-late the database queries defined by DCS trees.For example, the semantics of ?students readbooks?
is given by the abstract denotation:F1= read ?
(studentSUBJ?
bookOBJ),where read, student and book denote sets repre-sented by these words respectively, and wrrepre-sents the set w considered as the domain of thesemantic role r (e.g.
bookOBJis the set of booksconsidered as objects).
The operators?
and?
rep-resent intersection and Cartesian product respec-tively, both borrowed from relational algebra.
Itis not hard to see the abstract denotation denotesthe intersection of the ?reading?
set (as illustratedby the ?read?
table in Table 1) with the product of?student?
set and ?book?
set, which results in thesame denotation as computed by the DCS tree inFigure 1, i.e.
{John reads Ulysses, .
.
.}.
However,the point is that F1itself is an algebraic formulathat does not depend on any concrete databases.Formally, we introduce the following constants:?
W : a universal set containing all entities.1The semantic role ARG is specifically defined for denot-ing nominal predicate.80example phrase abstract denotation / statementcompound noun pet fish pet ?
fishmodification nice day day ?
(WARG?
niceMOD)temporal relation boys study at night study ?
(boySUBJ?
nightTIME)relative clause books that book ?
piOBJ(readstudents read ?
(studentSUBJ?WOBJ))quantification all men die man ?
piSUBJ(die)hypernym dog ?
animalderivation all criminals commit criminal ?
piSUBJ(commit?a crime (WSUBJ?
crimeOBJ))antonym rise ?
fallnegation no dogs are hurt dog ?
piOBJ(hurt)Table 2: Abstract denotations and statements?
Content words: a content word (e.g.
read)defines a set representing the word (e.g.read={(x, y) | read(x, y)}).In addition we introduce following functions:?
?
: the Cartesian product of two sets.?
?
: the intersection of two sets.?
pir: projection onto domain of semantic roler (e.g.
piOBJ(read) = {y | ?x; read(x, y)}).Generally we admit projections onto multiplesemantics roles, denoted by piRwhere R is aset of semantic roles.?
?r: relabeling (e.g.
?OBJ(book) = bookOBJ).?
qr?
: the division operator, where qr?
(A,B) isdefined as the largest set X which satisfiesBr?X ?
A.2This is used to formulate uni-versal quantifiers, such as ?Mary loves everydog?
and ?books read by all students?.An abstract denotation is then defined as finite ap-plications of functions on either constants or otherabstract denotations.2.3 StatementsAs the semantics of DCS trees is formulated byabstract denotations, the meanings of declarativesentences are represented by statements on ab-stract denotations.
Statements are declarationsof some relations among abstract denotations, forwhich we consider the following set relations:Non-emptiness A 6= ?
: the set A is not empty.Subsumption A ?
B: set A is subsumed by B.3Roughly speaking, the relations correspond to thelogical concepts satisfiability and entailment.2If A and B has the same dimension, q?
(A,B) is either?
or {?}
(0-dimension point set), depending on if A ?
B.3Using division operator, subsumption can be representedby non-emptiness, since for setsA,B of the same dimension,q?
(A,B) 6= ?
?
A ?
B.Abstract denotations and statements are conve-nient for representing semantics of various typesof expressions and linguistic knowledge.
Someexamples are shown in Table 2.42.4 Logical inference on DCSBased on abstract denotations, we briefly describeour process to apply DCS to textual inference.2.4.1 Natural language to DCS treesTo obtain DCS trees from natural language, weuse Stanford CoreNLP5for dependency parsing(Socher et al, 2013), and convert Stanford depen-dencies to DCS trees by pattern matching on POStags and dependency labels.6Currently we usethe following semantic roles: ARG, SUBJ, OBJ,IOBJ, TIME and MOD.
The semantic role MODis used for any restrictive modifiers.
Determinerssuch as ?all?, ?every?
and ?each?
trigger quanti-fiers, as shown in Figure 2.2.4.2 DCS trees to statementsA DCS tree T = (N , E) is defined as a rooted tree,where each node ?
?
N is labeled with a contentword w(?)
and each edge (?, ??)
?
E ?
N ?N is labeled with a pair of semantic roles (r, r?
)7.Here ?
is the node nearer to the root.
Furthermore,for each edge (?, ??)
we can optionally assign aquantification marker.Abstract denotation of a DCS tree can be cal-culated in a bottom-up manner.
For example, theabstract denotation of H in Figure 2 is calculatedfrom the leaf node Mary, and then:Node love (Mary loves):F2= love ?
(MarySUBJ?WOBJ)Node animal (Animal that Mary loves):F3= animal ?
piOBJ(F2)Node have (Tom has an animal that Mary loves):F4= have ?
(TomSUBJ?
(F3)OBJ).Formally, suppose the root ?
of a DCS tree T haschildren ?1, .
.
.
, ?n, and edges (?, ?1), .
.
.
, (?, ?n)labeled by (r1, r?1), .
.
.
, (rn, r?n), respectively.
Theabstract denotation of T is defined as:[[T ]]=w(?)
?
(n?i=1?ri(pir?i([[T?i]]))?WR?\ri),4Negation and disjointness (???)
are explained in ?2.5.5http://nlp.stanford.edu/software/corenlp.shtml6In (Liang et al, 2011) DCS trees are learned from QApairs and database entries.
We obtain DCS trees from depen-dency trees, to bypass the need of a concrete database.7The definition differs slightly from the original Liang etal.
(2011), mainly for the sake of simplicity and clarity.81piOBJ(F4) = F3?
F7piOBJ(F6) = dog ?
F7TF66= ?
Axiom 4dog ?
F76= ?Tdog ?
piOBJ(F2) dog ?
animal Axiom 8dog ?
F3dog ?
F7?
F3?
F7Axiom 6F3?
F76= ?
Axiom 4F46= ?Figure 3: An example of proof using abstract denotations1.
W 6= ?2.
A ?
B ?
A3.
Br?
qr?
(A,B) ?
A4.
piR(A) 6= ?
?
A 6= ?5.
(A ?
B & B ?
C)?
A ?
C6.
(A ?
B & A 6= ?)?
B 6= ?7.
A ?
B ?
piR(A) ?
piR(B)8.
(C ?
A & C ?
B)?
C ?
A ?
BTable 3: An excerpt of axiomswhere T?iis the subtree of T rooted at ?i, andR?is the set of possible semantic roles for con-tent word w(?)
(e.g.
Rlove= {SUBJ,OBJ}), andWR?\riis the product of W which has dimensionR?\ ri(e.g.
W{SUBJ,OBJ}\SUBJ= WOBJ).When universal quantifiers are involved, weneed to add division operators to the formula.If (?, ?i) is assigned by a quantification marker??
?8, then the abstract denotation is9[[T ]]=qri?(piR?\{r1,...,ri?1}([[T?
]]), pir?i([[T?i]])),where T?is the same tree as T except that theedge (?, ?i) is removed.
For example, the ab-stract denotation of the first sentence of T in Fig-ure 2 (Mary loves every dog) is calculated from F2(Mary loves) asF5= qOBJ?
(piOBJ(F2),dog).After the abstract denotation [[T ]] is calcu-lated, the statement representing the meaning ofthe sentence is defined as [[T ]] 6= ?.
For ex-ample, the statement of ?students read books?is read ?
(studentSUBJ?
bookOBJ) 6= ?, andthe statement of ?Mary loves every dog?
isqOBJ?
(piOBJ(F2),dog) 6= ?, which is logicallyequivalent to dog ?
piOBJ(F2).102.4.3 Logical inferenceSince meanings of sentences are represented bystatements on abstract denotations, logical infer-ence among sentences is reduced to deriving newrelations among abstract denotations.
This is doneby applying axioms to known statements, and ap-proximately 30 axioms are implemented (Table 3).8Multiple quantifiers can be processed similarly.9The result of [[T ]] depends on the order of the children?1, .
.
.
, ?n.
Different orders correspond to readings of differ-ent quantifier scopes.10See Footnote 2,3.These are algebraic properties of abstract denota-tions, among which we choose a set of axioms thatcan be handled efficiently and enable most com-mon types of inference seen in natural language.For the example in Figure 2, by constructing thefollowing abstract denotations:Tom has a dog:F6= have ?
(TomSUBJ?
dogOBJ)Objects that Tom has:F7= piOBJ(have ?
(TomSUBJ?WOBJ)),we can use the lexical knowledge dog ?
animal,the statements of T (i.e.
dog ?
piOBJ(F2) andF66= ?
), and the axioms in Table 3,11to provethe statement of H (i.e.
F46= ?)
(Figure 3).We built an inference engine to perform logicalinference on abstract denotations as above.
In thislogical system, we treat abstract denotations asterms and statements as atomic sentences, whichare far more easier to handle than first order pred-icate logic (FOL) formulas.
Furthermore, all im-plemented axioms are horn clauses, hence we canemploy forward-chaining, which is very efficient.2.5 ExtensionsFurther extensions of our framework are madeto deal with additional linguistic phenomena, asbriefly explained below.Negation To deal with negation in our forward-chaining inference engine, we introduce one morerelation on abstract denotations, namely disjoint-ness A ?
B, meaning that A and B are dis-joint sets.
Using disjointness we implemented twotypes of negations: (i) atomic negation, for eachcontent word w we allow negation w?
of that word,characterized by the property w ?
w?
; and (ii) rootnegation, for a DCS tree T and its denotation [[T ]],the negation of T is represented by T ?
T , mean-ing that T = ?
in its effect.Selection Selection operators in relational alge-bra select a subset from a set to satisfy some spe-11Algebraic identities, such as piOBJ(F4) = F3?
F7andpiOBJ(F6) = dog ?
F7, are also axioms.82cific properties.
This can be employed to rep-resent linguistic phenomena such as downwardmonotonicity and generalized quantifiers.
In thecurrent system, we implement (i) superlatives,e.g.
shighest(mountain?
(WARG?AsiaMOD)) (thehighest mountain in Asia) and (ii) numerics, e.g.stwo(pet ?
fish) (two pet fish), where sfis a se-lection marker.
Selection operators are imple-mented as markers assigned to abstract denota-tions, with specially designed axioms.
For ex-ample superlatives satisfy the following property:A ?
B & shighest(B) ?
A ?
shighest(B) =shighest(A).
New rules can be added if necessary.Coreference We use Stanford CoreNLP to re-solve coreferences (Raghunathan et al, 2010),whereas coreference is implemented as a specialtype of selection.
If a node ?
in a DCS tree T be-longs to a mention cluster m, we take the abstractdenotation [[T?]]
and make a selection sm([[T?
]]),which is regarded as the abstract denotation of thatmention.
Then all selections of the same mentioncluster are declared to be equal.3 Generating On-the-fly KnowledgeRecognizing textual entailment (RTE) is the taskof determining whether a given textual statementH can be inferred by a text passage T. For this,our primary textual inference system operates as:1.
For a T-H pair, apply dependency parsingand coreference resolution.2.
Perform rule-based conversion from depen-dency parses to DCS trees, which are trans-lated to statements on abstract denotations.3.
Use statements of T and linguistic knowledgeas premises, and try to prove statements of Hby our inference engine.However, this method does not work for real-world datasets such as PASCAL RTE (Dagan etal., 2006), because of the knowledge bottleneck:it is often the case that the lack of sufficient lin-guistic knowledge causes failure of inference, thusthe system outputs ?no entailment?
for almost allpairs (Bos and Markert, 2005).The transparent syntax-to-semantics interfaceof DCS enables us to back off to NLP techniquesduring inference for catching up the lack of knowl-edge.
We extract fragments of DCS trees as para-phrase candidates, translate them back to linguis-T/H DCS trees AbstractdenotationsParsingCoreference InferenceYes/NoOn-the-flyknowledge AxiomsLanguageresourcesFigure 4: RTE systemtic expressions, and apply distributional similar-ity to judge their validity.
In this way, our frame-work combines distributional and logical seman-tics, which is also the main subject of Lewis andSteedman (2013) and Beltagy et al (2013).As follows, our full system (Figure 4) addition-ally invokes linguistic knowledge on-the-fly:4.
If H is not proven, compare DCS trees of Tand H, and generate path alignments.5.
Aligned paths are evaluated by a similar-ity score to estimate their likelihood of be-ing paraphrases.
Path alignments with scoreshigher than a threshold are accepted.6.
Convert accepted path alignments into state-ments on abstract denotations, use them inlogical inference as new knowledge, and tryto prove H again.3.1 Generating path alignmentsOn-the-fly knowledge is generated by aligningpaths in DCS trees.
A path is considered as joiningtwo germs in a DCS tree, where a germ is definedas a specific semantic role of a node.
For example,Figure 5 shows DCS trees of the following sen-tences (a simplified pair from RTE2-dev):T: Tropical storm Debby is blamed for deaths.H: A storm has caused loss of life.The germ OBJ(blame) and germ ARG(death) inDCS tree of T are joined by the underscored path.Two paths are aligned if the joined germs arealigned, and we impose constraints on alignedgerms to inhibit meaningless alignments, as de-scribed below.3.2 Aligning germs by logical cluesTwo germs are aligned if they are both at leafnodes (e.g.
ARG(death) in T and ARG(life) in H,Figure 5), or they already have part of their mean-ings in common, by some logical clues.83readsT?
: H?
: ARGtunsb obnek?btt?ARG ARGOBJreadsARG ARGIOBJeda?
?nuARG MOD?
?b uarru?bARGSUBJARGMODOBJFigure 5: Aligned paths (underscored by the solidlines) and aligned germs (joined by the dotted line)To formulate this properly, we define the ab-stract denotation of a germ, which, intuitively, rep-resents the meaning of the germ in the specific sen-tence.
The abstract denotation of a germ is definedin a top-down manner: for the root node ?
of aDCS tree T , we define its denotation [[?
]]Tas thedenotation of the entire tree [[T ]]; for a non-rootnode ?
and its parent node ?, let the edge (?, ?)
belabeled by semantic roles (r, r?
), then define[[?
]]T= [[T?]]
?
(?r?(pir([[?]]T))?WR?\r?
).Now for a germ r(?
), the denotation is defined asthe projection of the denotation of node ?
onto thespecific semantic role r: [[r(?
)]]T= pir([[?
]]T).For example, the abstract denotation of germARG(book) in Figure 1 is defined as piARG(book?piOBJ(read?
(studentSUBJ?bookOBJ))), meaning?books read by students?.
Similarly, denotationof germ OBJ(blame) in T of Figure 5 indicatesthe object of ?blame?
as in the sentence ?Tropi-cal storm Debby is blamed for death?, which isa tropical storm, is Debby, etc.
Technically, eachgerm in a DCS tree indicates a variable when theDCS tree is translated to a FOL formula, and theabstract denotation of the germ corresponds to theset of consistent values (Liang et al, 2011) of thatvariable.The logical clue to align germs is: if there existsan abstract denotation, other than W , that is a su-perset of both abstract denotations of two germs,then the two germs can be aligned.
A simple ex-ample is that ARG(storm) in T can be alignedto ARG(storm) in H, because their denotationshave a common superset other than W , namelypiARG(storm).
A more complicated example is thatOBJ(blame) and SUBJ(cause) can be aligned,because inference can induce [[OBJ(blame)]]T=[[ARG(Debby)]]T= [[ARG(storm)]]T, as well as[[SUBJ(cause)]]H= [[ARG(storm)]]H, so they alsohave the common superset piARG(storm).
How-ever, for example, logical clues can avoid align-ing ARG(storm) to ARG(loss), which is obviouslyT?
: T'?
:What is tropical storm, Debby,       and is blamed for death ]][[ What is tropical storm, Debby,           and cause loss of life ]][[?blame deathDebbyARG ARGOBJstormARG ARGIOBJtropicalARG MODcause losslifeARGSUBJARGMODOBJDebbyARGstormARG ARGtropicalARG MODFigure 6: Tree transformation and generated on-the-fly knowledge (subsumption of denotationsshown above the trees)meaningless.3.3 Scoring path alignments by similarityAligned paths are evaluated by a similarity score,for which we use distributional similarity of thewords that appear in the paths (?4.1).
Only pathalignments with high similarity scores can be ac-cepted.
Also, we only accept paths of length ?
5,to prevent too long paths to be aligned.3.4 Applying path alignmentsAccepted aligned paths are converted into state-ments, which are used as new knowledge.
Theconversion is done by first performing a DCS treetransformation according to the aligned paths, andthen declare a subsumption relation between thedenotations of aligned germs.
For example, to ap-ply the aligned path pair generated in Figure 5,we use it to transform T into a new tree T?
(Fig-ure 6), and then the aligned germs, OBJ(blame)in T and SUBJ(cause) in T?, will generatethe on-the-fly knowledge: [[OBJ(blame)]]T?
[[SUBJ(cause)]]T?.Similar to the tree transformation based ap-proach to RTE (Bar-Haim et al, 2007), this pro-cess can also utilize lexical-syntactic entailmentrules (Szpektor et al, 2007).
Furthermore, sincethe on-the-fly knowledge is generated by trans-formed pairs of DCS trees, all contexts are pre-served: in Figure 6, though the tree transformationcan be seen as generated from the entailment rule?X is blamed for death?
X causes loss of life?, thegenerated on-the-fly knowledge, as shown abovethe trees, only fires with the additional conditionthat X is a tropical storm and is Debby.
Hence,the process can also be used to generate knowl-edge from context sensitive rules (Melamud et al,2013), which are known to have higher quality(Pantel et al, 2007; Clark and Harrison, 2009).However, it should be noted that using on-the-fly knowledge in logical inference is not a trivial84task.
For example, the FOL formula of the rule ?Xis blamed for death?
X causes loss of life?
is:?x; (?a; blame(x, a) & death(a))?
(?b, c; cause(x, b) & loss(b, c) & life(c)),which is not a horn clause.
The FOL formula forthe context-preserved rule in Figure 6 is even moreinvolved.
Still, it can be efficiently treated by ourinference engine because as a statement, the for-mula [[OBJ(blame)]]T?
[[SUBJ(cause)]]T?is anatomic sentence, more than a horn clause.4 ExperimentsIn this section, we evaluate our system on FraCaS(?4.2) and PASCAL RTE datasets (?4.3).4.1 Language ResourcesThe lexical knowledge we use are synonyms, hy-pernyms and antonyms extracted from WordNet12.We also add axioms on named entities, stopwords,numerics and superlatives.
For example, namedentities are singletons, so we add axioms such as?x; (x ?
Tom & x 6= ?)?
Tom ?
x.To calculate the similarity scores of path align-ments, we use the sum of word vectors of thewords from each path, and calculate the cosinesimilarity.
For example, the similarity score of thepath alignment ?OBJ(blame)IOBJ-ARG(death)?
SUBJ(cause)OBJ-ARG(loss)MOD-ARG(life)?
iscalculated as the cosine similarity of vectorsblame+death and cause+loss+life.
Other struc-tures in the paths, such as semantic roles, are ig-nored in the calculation.
The word vectors weuse are from Mikolov et al (2013)13(Mikolov13),and additional results are also shown using Turianet al (2010)14(Turian10).
The threshold for ac-cepted path alignments is set to 0.4, based on pre-experiments on RTE development sets.4.2 Experiments on FraCaSThe FraCaS test suite contains 346 inference prob-lems divided into 9 sections, each focused on a cat-egory of semantic phenomena.
We use the data byMacCartney and Manning (2007), and experimenton the first section, Quantifiers, following Lewisand Steedman (2013).
This section has 44 singlepremise and 30 multi premise problems.
Most of12http://wordnet.princeton.edu/13http://code.google.com/p/word2vec/14http://metaoptimize.com/projects/wordreprs/Single Prem.
Multi Prem.Lewis13 70 50MacCartney07 84.1 -MacCartney08 97.7 -Our Sys.
79.5 80.0Table 4: Accuracy (%) on FraCaSthe problems do not require lexical knowledge, sowe use our primary textual inference system with-out on-the-fly knowledge nor WordNet, to test theperformance of the DCS framework as formal se-mantics.
To obtain the three-valued output (i.e.yes, no, and unknown), we output ?yes?
if H isproven, or try to prove the negation of H if H isnot proven.
To negate H, we use the root negationas described in ?2.5.
If the negation of H is proven,we output ?no?, otherwise we output ?unknown?.The result is shown in Table 4.
Since our sys-tem uses an off-the-shelf dependency parser, andsemantic representations are obtained from sim-ple rule-based conversion from dependency trees,there will be only one (right or wrong) interpre-tation in face of ambiguous sentences.
Still, oursystem outperforms Lewis and Steedman (2013)?sprobabilistic CCG-parser.
Compared to MacCart-ney and Manning (2007) and MacCartney andManning (2008), our system does not need a pre-trained alignment model, and it improves by mak-ing multi-sentence inferences.
To sum up, the re-sult shows that DCS is good at handling universalquantifiers and negations.Most errors are due to wrongly generated DCStrees (e.g.
wrongly assigned semantic roles) orunimplemented quantifier triggers (e.g.
?neither?
)or generalized quantifiers (e.g.
?at least a few?
).These could be addressed by future work.4.3 Experiments on PASCAL RTE datasetsOn PASCAL RTE datasets, strict logical inferenceis known to have very low recall (Bos and Markert,2005), so on-the-fly knowledge is crucial in thissetting.
We test the effect of on-the-fly knowledgeon RTE2, RTE3, RTE4 and RTE5 datasets, andcompare our system with other approaches.4.3.1 Impact of on-the-fly knowledgeResults on test data are shown in Table 5.
Whenonly primary knowledge is used in inference (thefirst row), recalls are actually very low; After weactivate the on-the-fly knowledge, recalls jump toover 50%, with a moderate fall of precision.
As aresult, accuracies significantly increase.85RTE2 RTE3 RTE4 RTE5Prec.
Rec.
Acc.
Prec.
Rec.
Acc.
Prec.
Rec.
Acc.
Prec.
Rec.
Acc.Primary 70.9 9.8 52.9 73.2 7.3 51.1 89.7 5.2 52.3 82.6 6.3 52.5+On-the-fly 57.6 66.5 58.8 63.7 64.6 63.0 60.0 57.4 59.6 69.9 55.7 65.8Table 5: Impact of on-the-fly knowledgeRTE2 RTE3 RTE4 RTE5Bos06 60.6 - - -MacCartney08 - 59.4 - -Clark08 - - 56.5 -Wang10 63.0 61.1 - -Stern11 61.6 67.1 - 63.5Stern12 - - - 64.0Our Sys.
58.8 63.0 59.6 65.8Table 6: Comparison with other systems4.3.2 Comparison to other RTE systemsA comparison between our system and other RTEsystems is shown in Table 6.
Bos06 (Bos andMarkert, 2006) is a hybrid system combiningdeep features from a theorem prover and a modelbuilder, together with shallow features such as lex-ical overlap and text length.
MacCartney08 (Mac-Cartney and Manning, 2008) uses natural logic tocalculate inference relations between two superfi-cially aligned sentences.
Clark08 (Clark and Har-rison, 2008) is a logic-based system utilizing vari-ous resources including WordNet and DIRT para-phrases (Lin and Pantel, 2001), and is tolerant topartially unproven H sentences in some degree.All of the three systems pursue a logical approach,while combining various techniques to achieve ro-bustness.
The result shows that our system hascomparable performance.
On the other hand,Wang10 (Wang and Manning, 2010) learns a tree-edit model from training data, and captures entail-ment relation by tree edit distance.
Stern11 (Sternand Dagan, 2011) and Stern12 (Stern et al, 2012)extend this framework to utilize entailment rulesas tree transformations.
These are more tailoredsystems using machine learning with many hand-crafted features.
Still, our unsupervised systemoutperforms the state-of-the-art on RTE5 dataset.4.3.3 AnalysisSumming up test data from RTE2 to RTE5, Fig-ure 7 shows the proportion of all proven pairs andtheir precision.
Less than 5% pairs can be provenprimarily, with a precision of 77%.
Over 40%pairs can be proven by one piece of on-the-flyknowledge, yet pairs do exist in which more than2 pieces are necessary.
The precisions of 1 and 2pieces on-the-fly knowledge application are over0 1 2 >=300.10.20.30.40.50.60.70.80.9 Proportion of proven pairsPrecisionApplied on-the-fly knowledgeFigure 7: Proportion of proven pairs and their pre-cision, w.r.t.
pieces of on-the-fly knowledge.60%, which is fairly high, given our rough estima-tion of the similarity score.
As a comparison, Dinuand Wang (2009) studied the proportion of provenpairs and precision by applying DIRT rules to treeskeletons in RTE2 and RTE3 data.
The proportionis 8% with precision 65% on RTE2, and propor-tion 6% with precision 72% on RTE3.
Appliedby our logical system, the noisy on-the-fly knowl-edge can achieve a precision comparable to higherquality resources such as DIRT.A major type of error is caused by the igno-rance of semantic roles in calculation of simi-larity scores.
For example, though ?Italy beatsKazakhstan?
is not primarily proven from ?Italyis defeated by Kazakhstan?, our system doesproduce the path alignment ?SUBJ(beat)OBJ ?OBJ(defeat)SUBJ?
with a high similarity score.The impact of such errors depends on the datamaking methodology, though.
It lowers precisionsin RTE2 and RTE3 data, particularly in ?IE?
sub-task (where precisions drop under 0.5).
On theother hand, it occurs less often in ?IR?
subtask.Finally, to see if we ?get lucky?
on RTE5 datain the choice of word vectors and thresholds, wechange the thresholds from 0.1 to 0.7 and drawthe precision-recall curve, using two types of wordvectors, Mikolov13 and Turian10.
As shown inFigure 8, though the precision drops for Turian10,both curves show the pattern that our system keepsgaining recall while maintaining precision to a cer-tain level.
Not too much ?magic?
in Mikolov13 ac-tually: for over 80% pairs, every node in DCS treeof H can be covered by a path of length ?
5 that860 012 01> 01= 013 01.
014 01501.01..014014.015015.016016.
What tisropchlms,789Prrop89titnFigure 8: Precision-Recall curve.has a corresponding path of length ?
5 in T witha similarity score > 0.4.5 Conclusion and DiscussionWe have presented a method of deriving abstractdenotation from DCS trees, which enables logi-cal inference on DCS, and we developed a textualinference system based on the framework.
Exper-imental results have shown the power of the rep-resentation that allows both strict inference as onFraCaS data and robust reasoning as on RTE data.Exploration of an appropriate meaning repre-sentation for querying and reasoning on knowl-edge bases has a long history.
Description logic,being less expressive than FOL but featuring moreefficient reasoning, is used as a theory base for Se-mantic Web (W3C, 2012).
Ideas similar to ourframework, including the use of sets in a repre-sentation that benefits efficient reasoning, are alsofound in description logic and knowledge repre-sentation community (Baader et al, 2003; Sowa,2000; Sukkarieh, 2003).
To our knowledge, how-ever, their applications to logical inference beyondthe use for database querying have not been muchexplored in the context of NLP.The pursue of a logic more suitable for naturallanguage inference is not new.
For instance, Mac-Cartney and Manning (2008) has implemented amodel of natural logic (Lakoff, 1970).
Whilebeing computationally efficient, various inferencepatterns are out of the scope of their system.Much work has been done in mapping natu-ral language into database queries (Cai and Yates,2013; Kwiatkowski et al, 2013; Poon, 2013).Among these, the (?-)DCS (Liang et al, 2011;Berant et al, 2013) framework defines algorithmsthat transparently map a labeled tree to a databasequerying procedure.
Essentially, this is becauseDCS trees restrict the querying process to a verylimited subset of possible operations.
Our maincontribution, the abstract denotation of DCS trees,can thus be considered as an attempt to charac-terize a fragment of FOL that is suited for bothnatural language inference and transparent syntax-semantics mapping, through the choice of opera-tions and relations on sets.We have demonstrated the utility of logical in-ference on DCS through the RTE task.
A widevariety of strategies tackling the RTE task havebeen investigated (Androutsopoulos and Malaka-siotis, 2010), including the comparison of surfacestrings (Jijkoun and De Rijke, 2005), syntactic andsemantic structures (Haghighi et al, 2005; Snowet al, 2006; Zanzotto et al, 2009; Burchardt etal., 2009; Heilman and Smith, 2010; Wang andManning, 2010), semantic vectors (Erk and Pad?o,2009) and logical representations (Bos and Mark-ert, 2005; Raina et al, 2005; Tatu and Moldovan,2005).
Acquisition of basic knowledge for RTEis also a huge stream of research (Lin and Pantel,2001; Shinyama et al, 2002; Sudo et al, 2003;Szpektor et al, 2004; Fujita et al, 2012; Weis-man et al, 2012; Yan et al, 2013).
These previ-ous works include various techniques for acquir-ing and incorporating different kinds of linguisticand world knowledge, and further fight against theknowledge bottleneck problem, e.g.
by back-offto shallower representations.Logic-based RTE systems employ various ap-proaches to bridge knowledge gaps.
Bos andMarkert (2005) proposes features from a modelbuilder; Raina et al (2005) proposes an abductionprocess; Tatu and Moldovan (2006) shows hand-crafted rules could drastically improve the perfor-mance of a logic-based RTE system.As such, our current RTE system is at a proof-of-concept stage, in that many of the above tech-niques are yet to be implemented.
Nonetheless,we would like to emphasize that it already showsperformance competitive to state-of-the-art sys-tems on one data set (RTE5).
Other directions ofour future work include further exploitation of thenew semantic representation.
For example, sinceabstract denotations are readily suited for dataquerying, they can be used to verify newly gen-erated assumptions by fact search in a database.This may open a way towards a hybrid approachto RTE wherein logical inference is intermingledwith large scale database querying.Acknowledgments This research was supportedby the Todai Robot Project at National Institute ofInformatics.87ReferencesIon Androutsopoulos and Prodromos Malakasiotis.2010.
A survey of paraphrasing and textual entail-ment methods.
J. Artif.
Int.
Res., 38(1).Franz Baader, Diego Calvanese, Deborah L. McGuin-ness, Daniele Nardi, and Peter F. Patel-Schneider,editors.
2003.
The Description Logic Handbook:Theory, Implementation, and Applications.
Cam-bridge University Press, New York, NY, USA.Roy Bar-Haim, Ido Dagan, Iddo Greental, and EyalShnarch.
2007.
Semantic inference at the lexical-syntactic level.
In Proceedings of AAAI 2007.Islam Beltagy, Cuong Chau, Gemma Boleda, Dan Gar-rette, Katrin Erk, and Raymond Mooney.
2013.Montague meets markov: Deep semantics withprobabilistic logical form.
In Second Joint Con-ference on Lexical and Computational Semantics(*SEM).Jonathan Berant, Andrew Chou, Roy Frostig, and PercyLiang.
2013.
Semantic parsing on Freebase fromquestion-answer pairs.
In Proceedings of EMNLP2013.Johan Bos and Katja Markert.
2005.
Recognising tex-tual entailment with logical inference.
In Proceed-ings of EMNLP 2005.Johan Bos and Katja Markert.
2006.
When logicalinference helps determining textual entailment (andwhen it doesnt).
In Proceedings of the 2nd PASCALRTE Challenge Workshop.Aljoscha Burchardt, Marco Pennacchiotti, StefanThater, and Manfred Pinkal.
2009.
Assessing theimpact of frame semantics on textual entailment.Nat.
Lang.
Eng., 15(4).Qingqing Cai and Alexander Yates.
2013.
Large-scalesemantic parsing via schema matching and lexiconextension.
In Proceedings of ACL 2013.Peter Clark and Phil Harrison.
2008.
Recognizing tex-tual entailment with logical inference.
In Proceed-ings of 2008 Text Analysis Conference (TAC?08).Peter Clark and Phil Harrison.
2009.
Large-scale ex-traction and use of knowledge from text.
In Pro-ceedings of the Fifth International Conference onKnowledge Capture (K-CAP?09).E.
F. Codd.
1970.
A relational model of data for largeshared data banks.
Commun.
ACM, 13(6).Robin Cooper, Dick Crouch, Jan Van Eijck, ChrisFox, Johan Van Genabith, Jan Jaspars, Hans Kamp,David Milward, Manfred Pinkal, Massimo Poesio,and et al 1996.
Using the framework.
FraCaS De-liverable D, 16.Ido Dagan, O. Glickman, and B. Magnini.
2006.
Thepascal recognising textual entailment challenge.
InMachine Learning Challenges.
Evaluating Predic-tive Uncertainty, Visual Object Classification, andRecognising Tectual Entailment.Georgiana Dinu and Rui Wang.
2009.
Inference rulesand their application to recognizing textual entail-ment.
In Proceedings of EACL 2009.Katrin Erk and Sebastian Pad?o.
2009.
Paraphrase as-sessment in structured vector space: Exploring pa-rameters and datasets.
In Proceedings of the Work-shop on Geometrical Models of Natural LanguageSemantics.Atsushi Fujita, Pierre Isabelle, and Roland Kuhn.2012.
Enlarging paraphrase collections throughgeneralization and instantiation.
In Proceedings ofEMNLP 2012.Aria Haghighi, Andrew Ng, and Christopher Manning.2005.
Robust textual inference via graph matching.In Proceedings of EMNLP 2005.Michael Heilman and Noah A. Smith.
2010.
Tree editmodels for recognizing textual entailments, para-phrases, and answers to questions.
In Proceedingsof NAACL 2010.Valentin Jijkoun and Maarten De Rijke.
2005.
Rec-ognizing textual entailment: Is word similarityenough?
In Machine Learning Challenge Work-shop, volume 3944 of LNCS, Springer.Tom Kwiatkowski, Eunsol Choi, Yoav Artzi, and LukeZettlemoyer.
2013.
Scaling semantic parsers withon-the-fly ontology matching.
In Proceedings ofEMNLP 2013.George Lakoff.
1970.
Linguistics and natural logic.Synthese, 22(1-2).Mike Lewis and Mark Steedman.
2013.
Combineddistributional and logical semantics.
Transactionsof ACL, 1.Percy Liang, Michael Jordan, and Dan Klein.
2011.Learning dependency-based compositional seman-tics.
In Proceedings of ACL 2011.Dekang Lin and Patrick Pantel.
2001.
Discovery ofinference rules for question-answering.
Nat.
Lang.Eng., 7(4).Bill MacCartney and Christopher D. Manning.
2007.Natural logic for textual inference.
In Proceedingsof the ACL-PASCAL Workshop on Textual Entail-ment and Paraphrasing.Bill MacCartney and Christopher D. Manning.
2008.Modeling semantic containment and exclusion innatural language inference.
In Proceedings of Col-ing 2008.88Oren Melamud, Jonathan Berant, Ido Dagan, JacobGoldberger, and Idan Szpektor.
2013.
A two levelmodel for context sensitive inference rules.
In Pro-ceedings of ACL 2013.Tomas Mikolov, Wen-tau Yih, and Geoffrey Zweig.2013.
Linguistic regularities in continuous spaceword representations.
In Proceedings of NAACL2013.Patrick Pantel, Rahul Bhagat, Bonaventura Coppola,Timothy Chklovski, and Eduard Hovy.
2007.
ISP:Learning inferential selectional preferences.
In Pro-ceedings of NAACL 2007.Hoifung Poon.
2013.
Grounded unsupervised seman-tic parsing.
In Proceedings of ACL 2013.Karthik Raghunathan, Heeyoung Lee, Sudarshan Ran-garajan, Nate Chambers, Mihai Surdeanu, Dan Ju-rafsky, and Christopher Manning.
2010.
A multi-pass sieve for coreference resolution.
In Proceed-ings of EMNLP 2010.Rajat Raina, Andrew Y. Ng, and Christopher D. Man-ning.
2005.
Robust textual inference via learningand abductive reasoning.
In Proceedings of AAAI2005.Yusuke Shinyama, Satoshi Sekine, and Kiyoshi Sudo.2002.
Automatic paraphrase acquisition from newsarticles.
In Proceedings of HLT 2002.Rion Snow, Lucy Vanderwende, and Arul Menezes.2006.
Effectively using syntax for recognizing falseentailment.
In Proceedings of NAACL 2006.Richard Socher, John Bauer, Christopher D. Manning,and Ng Andrew Y.
2013.
Parsing with compo-sitional vector grammars.
In Proceedings of ACL2013.John F. Sowa.
2000.
Knowledge Representation:Logical, Philosophical and Computational Founda-tions.
Brooks/Cole Publishing Co., Pacific Grove,CA, USA.Asher Stern and Ido Dagan.
2011.
A confidence modelfor syntactically-motivated entailment proofs.
InProceedings of RANLP 2011.Asher Stern, Roni Stern, Ido Dagan, and Ariel Felner.2012.
Efficient search for transformation-based in-ference.
In Proceedings of ACL 2012.Kiyoshi Sudo, Satoshi Sekine, and Ralph Grishman.2003.
An improved extraction pattern representa-tion model for automatic ie pattern acquisition.
InProceedings of ACL 2003.JanaZ.
Sukkarieh.
2003.
An expressive efficient rep-resentation: Bridging a gap between nlp and kr.In Vasile Palade, RobertJ.
Howlett, and LakhmiJain, editors, Knowledge-Based Intelligent Informa-tion and Engineering Systems.
Springer Berlin Hei-delberg.Idan Szpektor, Hristo Tanev, Ido Dagan, and Bonaven-tura Coppola.
2004.
Scaling web-based acquisitionof entailment relations.
In Proceedings of EMNLP2004.Idan Szpektor, Eyal Shnarch, and Ido Dagan.
2007.Instance-based evaluation of entailment rule acqui-sition.
In Proceedings of ACL 2007.Marta Tatu and Dan Moldovan.
2005.
A semantic ap-proach to recognizing textual entailment.
In Pro-ceedings of EMNLP 2005.Marta Tatu and Dan Moldovan.
2006.
A logic-based semantic approach to recognizing textual en-tailment.
In Proceedings of the COLING/ACL 2006.Joseph Turian, Lev-Arie Ratinov, and Yoshua Bengio.2010.
Word representations: A simple and generalmethod for semi-supervised learning.
In Proceed-ings of ACL 2010.W3C.
2012.
Owl 2 web ontology language documentoverview (second edition).
www.w3.org/TR/owl2-overview/.Mengqiu Wang and Christopher Manning.
2010.Probabilistic tree-edit models with structured latentvariables for textual entailment and question answer-ing.
In Proceedings of Coling 2010.Hila Weisman, Jonathan Berant, Idan Szpektor, and IdoDagan.
2012.
Learning verb inference rules fromlinguistically-motivated evidence.
In Proceedings ofEMNLP 2012.Yulan Yan, Chikara Hashimoto, Kentaro Torisawa,Takao Kawai, Jun?ichi Kazama, and Stijn De Saeger.2013.
Minimally supervised method for multilin-gual paraphrase extraction from definition sentenceson the web.
In Proceedings of NAACL 2013.Fabio massimo Zanzotto, Marco Pennacchiotti, andAlessandro Moschitti.
2009.
A machine learn-ing approach to textual entailment recognition.
Nat.Lang.
Eng., 15(4).89
