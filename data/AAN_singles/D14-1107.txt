Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 990?1000,October 25-29, 2014, Doha, Qatar.c?2014 Association for Computational LinguisticsA?CCG Parsing with a Supertag-factored ModelMike LewisSchool of InformaticsUniversity of EdinburghEdinburgh, EH8 9AB, UKmike.lewis@ed.ac.ukMark SteedmanSchool of InformaticsUniversity of EdinburghEdinburgh, EH8 9AB, UKsteedman@inf.ed.ac.ukAbstractWe introduce a new CCG parsing modelwhich is factored on lexical category as-signments.
Parsing is then simply a de-terministic search for the most probablecategory sequence that supports a CCGderivation.
The parser is extremely simple,with a tiny feature set, no POS tagger, andno statistical model of the derivation ordependencies.
Formulating the model inthis way allows a highly effective heuris-tic for A?parsing, which makes parsingextremely fast.
Compared to the standardC&C CCG parser, our model is more ac-curate out-of-domain, is four times faster,has higher coverage, and is greatly simpli-fied.
We also show that using our parserimproves the performance of a state-of-the-art question answering system.1 IntroductionCCG is a strongly lexicalized grammatical formal-ism, in which the vast majority of the decisionsmade during interpretation involve choosing thecorrect definitions of words.
We explore the ef-fect of modelling this explicitly in a parser, byonly using a probabilistic model of lexical cate-gories (based on a local context window), ratherthan modelling the derivation or dependencies.Existing state-of-the-art CCG parsers use com-plex pipelines of POS-tagging, supertagging andparsing?each with its own feature sets and pa-rameters (and sources of error)?together with fur-ther parameters governing their integration (Clarkand Curran, 2007).
We show that much simplermodels can achieve high performance.
Our modelpredicts lexical categories based on a tiny fea-ture set of word embeddings, capitalization, and 2-character suffixes?with no parsing model beyonda small set of CCG combinators, and no POS-tagger.
Simpler models are easier to implement,replicate and extend.Another goal of our model is to parse CCGoptimally and efficiently, without using excessivepruning.
CCG?s large set of lexical categories,and generalized notion of constituency, mean thatsentences can have a huge number of potentialparses.
Fast existing CCG parsers rely on aggres-sive pruning?for example, the C&C parser usesa supertagger to dramatically cut the search spaceconsidered by the parser.
Even the loosest beamsetting for their supertagger discards the correctparse for 20% of sentences.
The structure of ourmodel allows us to introduce a simple but power-ful heuristic for A?parsing, meaning it can parsealmost 50 sentences per second exactly, with nobeam-search or pruning.
Adding very mild prun-ing increases the speed to 186 sentences per sec-ond with minimal loss of accuracy.Our approach faces two obvious challenges.Firstly, categories are assigned based on a localwindow, which may not contain the necessary con-text for resolving some attachment decisions.
Forexample, in I saw a squirrel 2 weeks ago with anut, the model cannot make an informed decisionon whether to assign with an adverbial or adnomi-nal preposition category, as the crucial words sawand squirrel fall outside the local context window.Secondly, even if the supertagger makes all lexicalcategory decisions correctly, then the parser canstill make erroneous decisions.
One example isin coordination-scope ambiguities, such as cleverboys and girls, where the two interpretations usethe same assignment of categories.We hypothesise that such decisions are rela-tively rare, and are challenging for any parsingmodel, so a weak model is unlikely to result insubstantially lower accuracy.
Our implementationof this model1, which we call EASYCCG, has high1Available from https://github.com/mikelewis0/easyccg990accuracy?suggesting that most parsing decisionscan be made accurately based on a local contextwindow.Of course, there are many parsing decisions thatcan only be made accurately with more complexmodels.
However, exploring the power and lim-itations of simpler models may help focus futureresearch on the more challenging cases.2 Background2.1 Combinatory Categorial GrammarCCG (Steedman, 2000) is a strongly lexicalizedgrammatical formalism.
Words have categoriesrepresenting their syntactic role, which are eitheratomic, or functions from one category to another.Phrase-structure grammars have a relativelysmall number of lexical categories types (e.g.POS-tags), and a large set of rules used to builda syntactic analysis of a complete sentence (e.g.an adjective and noun can combine into a noun).In contrast, CCG parsing has many lexical cate-gory types (we use 425), but a small set of combi-natory rule types (we use 10 binary and 13 unaryrule schemata).
This means that, aside from thelexicon, the grammar is small enough to be hand-coded?which allows us, in this paper, to confinethe entire statistical model to the lexicon.CCG?s generalized notion of constituencymeans that many derivations are possible fora given a set of lexical categories.
However,most of these derivations will be semanticallyequivalent?for example, deriving the same de-pendency structures?in which case the actualchoice of derivation is unimportant.
Such ambi-guity is often called spurious.2.2 Existing CCG Parsing ModelsThe seminal C&C parser is by far the most pop-ular choice of CCG parser (Clark and Curran,2007).
It showed that it was possible to parse toan expressive linguistic formalism with high speedand accuracy.
The performance of the parser hasenabled large-scale logic-based distributional re-search (Harrington, 2010; Lewis and Steedman,2013a; Lewis and Steedman, 2013b; Reddy et al.,2014), and it is a key component of Boxer (Bos,2008).The C&C parser uses CKY chart parsing, with alog-linear model to rank parses.
The vast numberof possible parses means that computing the com-plete chart is impractical.
To resolve this prob-lem, a supertagger is first run over the sentence toprune the set of lexical categories considered bythe parser for each word.
The initial beam out-puts an average of just 1.2 categories per word,rather than the 425 possible categories?makingthe standard CKY parsing algorithm very efficient.If the parser fails to find any analysis of the com-plete sentence with this set of supertags, the su-pertagger re-analyses the sentence with a more re-laxed beam (adaptive supertagging).2.3 A?ParsingKlein and Manning (2003a) introduce A?parsingfor PCFGs.
The parser maintains a chart and anagenda, which is a priority queue of items to add tothe chart.
The agenda is sorted based on the items?inside probability, and a heuristic upper-bound onthe outside probability?to give an upper boundon the probability of the complete parse.
The chartis then expanded in best-first order, until a com-plete parse for the sentence is found.Klein and Manning calculate an upper bound onthe outside probability of a span based on a sum-mary of the context.
For example, the summaryfor the SX heuristic is the category of the span, andthe number of words in the sentence before and af-ter the span.
The value of the heuristic is the prob-ability of the best possible sentence meeting theserestrictions.
These probabilities are pre-computedfor every non-terminal symbol and for every pos-sible number of preceding and succeeding words,leading to large look-up tables.Auli and Lopez (2011b) find that A?CCG pars-ing with this heuristic is very slow.
However,they achieve a modest 15% speed improvementover CKY when A?is combined with adaptive su-pertagging.
One reason is that the heuristic esti-mate is rather coarse, as it deals with the best pos-sible outside context, rather than the actual sen-tence.
We introduce a new heuristic which gives atighter upper bound on the outside probability.3 Model3.1 Lexical Category ModelAs input, our parser takes a distribution over allCCG lexical categories for each word in the sen-tence.
These distributions are assigned usingLewis and Steedman (2014)?s semi-supervised su-pertagging model.
The supertagger is a unigramlog-linear classifier that uses features of the ?3word context window surrounding a word.
The991key feature is word embeddings, initialized withthe 50-dimensional embeddings trained in Turianet al.
(2010), and fine-tuned during supervisedtraining.
The model also uses 2-character suffixesand capitalization features.The use of word embeddings, which are trainedon a large unlabelled corpus, allows the supertag-ger to generalize well to words not present in thelabelled data.
It does not use a POS-tagger, whichavoids problems caused by POS-tagging errors.Our methods could be applied to any supertag-ging model, but we find empirically that thismodel gives higher performance than the C&C su-pertagger.3.2 Parsing ModelLet a CCG parse y of a sentence S be a list oflexical categories c1.
.
.
cnand a derivation.
If weassume all derivations licensed by our grammarare equally likely, and that lexical category assign-ments are conditionally independent given the sen-tence, we can compute the optimal parse y?
as:y?
= argmaxy?ni=1p(ci|S)As discussed in Section 2.1, many derivationsare possible given a sequence of lexical categories,some of which may be semantically distinct.
How-ever, our model will assign all of these an equalscore, as they use the same sequence of lexicalcategories.
Therefore we extend our model witha simple deterministic heuristic for ranking parsesthat use the same lexical categories.
Given a set ofderivations with equal probability, we output theone maximizing the sums of the length of all arcsin the corresponding dependency tree.The effect of this heuristic is to prefer non-local attachments in cases of ambiguity, whichwe found worked better on development data thanfavouring local attachments.
In cases of spuriousambiguity, all parses will have the same value ofthis heuristic, so one is chosen arbitrarily.
Forexample, one of the parses in Figures 1a and 1bwould be selected over the parse in Figure 1c.Of course, we could use any function of theparses in place of this heuristic, for example ahead-dependency model.
However, one aim ofthis paper is to demonstrate that an extremely sim-ple parsing model can achieve high performance,so we leave more sophisticated alternatives to fu-ture work.a house in Paris in FranceNP (NP\NP)/NP NP (NP\NP)/NP NP> >NP\NP NP\NP<NP<NP(a) A standard derivation of a house in Paris in France, with adependency from in France to housea house in Paris in FranceNP (NP\NP)/NP NP (NP\NP)/NP NP> >NP\NP NP\NP>BNP\NP<NP(b) A derivation of a house in Paris in France, which is spu-riously equivalent to Figure 1a.
A composition combinator isused to compose the predicates in Paris and in France, creatinga constituent which creates dependencies to its argument fromboth in Paris and in France.a house in Paris in FranceNP (NP\NP)/NP NP (NP\NP)/NP NP>NP\NP<NP>NP\NP<NP(c) A derivation of a house in Paris in France, which yieldsdifferent dependencies to Figures 1a and 1b: here, there is adependency from in France to Paris, not house.Figure 1: Three CCG parses of a house in Parisin France, given the same set of supertags.
Thefirst two are spuriously equivalent, but the third issemantically distinct.3.3 A?SearchFor parsing, we use an A?search for the most-probable complete CCG derivation of a sentence.A key advantage of A?parsing over CKY parsingis that it does not require us to prune the searchspace first with a supertagger, allowing the parserto consider the complete distribution of 425 cate-gories for each word (in contrast to an average of3.57 categories per word considered by the C&Cparser?s most relaxed beam).
This is possible be-cause A?only searches for the Viterbi parse ofa sentence, rather than building a complete chartwith every possible category per word (another al-ternative, used by Hockenmaier (2003), is to use ahighly aggressive beam search in the parser).In A?parsing, items on the agenda are sorted bytheir cost; the product of their inside probabilityand an upper bound on their outside probability.992For a span wi.
.
.
wjwith lexical categoriesci.
.
.
cjin a sentence S = w1.
.
.
wn, the insideprobability is simply:?jk=ip(ck|S)The factorization of our model lets us give thefollowing upper-bound on the outside probability:h(wi.
.
.
wj) =?k<ik=1maxckp(ck|S)?
?k?nk=j+1maxckp(ck|S)This heuristic assumes that all words outside thespan will take their highest-probability supertag.Because the model is factored on lexical cate-gories, this estimate is clearly an upper bound.As supertagging is over 90% accurate, the upperbound will often be exact, and in Section 4.3 weshow empirically that it is extremely efficient.
Thevalues of the heuristic can be computed once foreach sentence and cached.To implement the preference for non-local at-tachment described in Section 3.2, if two agendaitems have the same cost, the one with the longerdependencies is preferred.Intuitively, the parser first attempts to find aparse for the sentence using the 1-best category foreach word, by building as complete a chart as pos-sible.
If it fails to find a parse for the completesentence, it adds one more supertag to the chart(choosing the most probable tag not already in thechart), and tries again.
This strategy allows theparser to consider an unbounded number of cate-gories for each word, as it does not build a com-plete chart with all supertags.3.4 GrammarHere, we describe the set of combinators andunary rules in the EASYCCG grammar.
Becausewe do not have any probabilistic model of thederivation, all rules can apply with equal probabil-ity.
This means that some care needs to be takenin designing the grammar to ensure that all therules are generally applicable.
We also try to limitspurious ambiguity, and build derivations whichare compatible with the C&C parser?s scripts forextracting dependencies (for evaluation).
We de-scribe the grammar in detail, to ensure replicabil-ity.Our parser uses the following binary combi-nators from Steedman (2012): forward applica-tion, backward application, forward composition,backward crossed composition, generalized for-ward composition, generalized backward crossedcomposition.
These combinators are posited tobe linguistically universal.
The generalized rulesInitial Result UsageN NP BarenounphrasesNP S/(S\NP ) TypeNP (S\NP )/((S\NP )/NP ) raisingPP (S\NP )/((S\NP )/PP )Spss\NP NP\NPSng\NP NP\NP ReducedSadj\NP NP\NP relativeSto\NP NP\NP clausesSto\NP N\NSdcl/NP NP\NPSpss\NP S/S VPSng\NP S/S SentenceSto\NP S/S ModifiersTable 1: Set of unary rules used by the parser.are generalized to degree 2.
Following Steedman(2000) and Clark and Curran (2007), backwardcomposition is blocked where the argument of theright-hand category is anN orNP .
The unhelpful[nb] feature is ignored.As in the C&C parser, we add a special Con-junction rule:Y X>X \XWhere Y ?
{conj, comma, semicolon}.
Weblock conjunctions where the right-hand categoryis type-raised, punctuation, N , or NP\NP .
Thisrule (and the restrictions) could be removed bychanging CCGBank to analyse conjunctions with(X\X)/X categories.We also add syntagmatic rules for removing anypunctuation to the right, and for removing open-brackets and open-quotes to the leftThe grammar also contains 13 unary rules,listed in Table 1.
These rules were chosen basedon their frequency in the training data, and theirclear semantic interpretations.Following Clark and Curran (2007), we also adda (switchable) constraint that only category com-binations that have combined in the training datamay combine in the test data.
We found that thiswas necessary for evaluation, as the C&C conver-sion tool for extracting predicate-argument depen-dencies had relatively low coverage on the CCGderivations produced by our parser.
While thisrestriction is theoretically inelegant, we found itdid increase parsing speed without lowering lexi-993cal category accuracy.We also use Eisner Normal Form Constraints(Eisner, 1996), and Hockenmaier and Bisk?s(2010) Constraint 5, which automatically rule outcertain spuriously equivalent derivations, improv-ing parsing speed.We add a hard constraint that the root categoryof the sentence must be a declarative sentence, aquestion, or a noun-phrase.This grammar is smaller and cleaner than thatused by the C&C parser, which uses 32 unaryrules (some of which are semantically dubious,such as S[dcl]?
NP\NP ), and non-standard bi-nary combinators such as merging two NP s intoan NP .
The C&C parser also has a large num-ber of special case rules for handling punctua-tion.
Our smaller grammar reduces the grammarconstant, eases implementation, and simplifies thejob of building downstream semantic parsers suchas those of Bos (2008) or Lewis and Steedman(2013a) (which must implement semantic analogsof each syntactic rule).3.5 Extracting Dependency StructuresThe parsing model defined in Section 3.2 re-quires us to compute unlabelled dependency treesfrom CCG derivations (to prefer non-local attach-ments).
It is simple to extract an unlabelled depen-dency tree from a CCG parse, by defining one ar-gument of each binary rule instantiation to be thehead.
For forward application and (generalized)forward composition, we define the head to be theleft argument, unless the left argument is an endo-centric head-passing modifier category X/X .
Wedo the inverse for the corresponding ?backward?combinators.
For punctuation rules, the head is theargument which is not punctuation, and the headof a Conjunction rule is the right-hand argument.The standard CCG parsing evaluation uses adifferent concept of dependencies, correspond-ing to the predicate-argument structure defined byCCGBank.
These dependencies capture a deeperinformation?for example by assigning both boyand girl as subjects of talk in a boy and a girltalked.
We extract these dependencies usingthe generate program supplied with the C&Cparser.3.6 PruningOur parsing model is able to efficiently and op-timally search for the best parse.
However,we found that over 80% of the run-time of ourpipeline was spent during supertagging.
Naively,the log-linear model needs to output a probabilityfor each of the 425 categories.
This is expensiveboth in terms of the number of dot products re-quired, and the cost of building the initial priority-queue for the A?parsing agenda.
It is also largelyunnecessary?for example, periods at the end ofsentences always have the same category, but oursupertagger calculates a distribution over all pos-sible categories.Note that the motivation for introducing prun-ing here is fundamentally different from for theC&C pipeline.
The C&C supertagger prunes thethe categories so that the parser can build the com-plete set of derivations given those categories.
Incontrast, our parser can efficiently search large (orinfinite) spaces of categories, but pruning is help-ful for making supertagging itself more efficient,and for building the initial agenda.We therefore implemented the following strate-gies to improve efficiency:?
Only allowing at most 50 categories perword.
The C&C parser takes on average 1.27tags per word (and an average of 3.57 at itsloosest beam setting), so this restriction is avery mild one.
Nevertheless, it considerablyreduces the potential size of the agenda.?
Using a variable-width beam ?
which prunescategories less likely than ?
times the prob-ability of the best category.
We set ?
=0.00001, which is two orders-of-magnitudesmaller than the equivalent C&C beam.Again, this heuristic is useful for reducing thelength of the agenda.?
Using a tag dictionary of possible categoriesfor each word, so that weights are only cal-culated for a subset of the categories.
Unlikethe other methods, this approach does affectthe probabilities which are calculated, as thenormalizing constant is only computed for asubset of the categories.
However, the proba-bility mass contained in the pruned categoriesis small, and it only slightly decreases pars-ing accuracy.
To build the tag dictionary, weparsed 42 million sentences of Wikipedia us-ing our parser, and for all words occurring atleast 500 times, we stored the set of observedword-category combinations.
When parsingnew sentences, these words are only allowedto occur with one of these categories.994Supertagger Parser CCGBank Wikipedia BioinferF1 COV F1 Time F1 COV F1 F1 COV F1(cov) (all) (cov) (all) (cov) (all)C&C C&C 85.47 99.63 85.30 54s 81.19 99.0 80.64 76.08 97.2 74.88EASYCCG EASYCCG 83.37 99.96 83.37 13s 81.75 100 81.75 77.24 100 77.24EASYCCG C&C 86.14 99.96 86.11 69s 82.46 100 82.46 78.00 99.8 77.88Table 2: Parsing F1-scores for labelled dependencies across a range of domains.
F1 (cov) refers toresults on sentences which the parser is able to parse, and F1 (all) gives results over all sentences.
Forthe EASYCCG results, scores are only over parses where the C&C dependency extraction script wassuccessful, which was 99.3% on CCGBank, 99.5% on Wikipedia, and 100% on Bioinfer.4 Experiments4.1 Experimental SetupWe trained our model on Sections 02-21 of CCG-Bank (Hockenmaier and Steedman, 2007), usingSection 00 for development.
For testing, we usedSection 23 of CCGBank, a Wikipedia corpus an-notated by Honnibal and Curran (2009), and theBioinfer corpus of biomedical abstracts (Pyysaloet al., 2007).
The latter two are out-of-domain, soare more challenging for the parsers.We compare the performance of our modelagainst both the C&C parser, and the system de-scribed in Lewis and Steedman (2014).
Thismodel uses the same supertagger as used in EASY-CCG, but uses the C&C parser for parsing, usingadaptive supertagging with the default values.All timing experiments used the same 1.8GhzAMD machine.4.2 Parsing AccuracyResults are shown in Table 2.
Our parser per-forms competitively with a much more complexparsing model, and outperforms the C&C pipelineon both out-of-domain datasets.
This result con-firms our hypothesis that the majority of parsingdecisions can be made accurately with a simpletagging model and a deterministic parser.We see that the combination of the EASYCCGsupertagger and the C&C parser achieves the bestaccuracy across all domains.
This result showsthat, unsurprisingly, there is some value to hav-ing a statistical model of the dependencies that theparser is evaluated on.
However, the difference isnot large, particularly out-of-domain, consideringthat a sophisticated and complex statistical parseris being compared with a deterministic one.
Ourparser is also far faster than this baseline.It is interesting that the performance gap isSpeed (sentences/second)System Tagger Parser TotalC&C 343 52 45EASYCCG tagger +C&C parser299 58 49EASYCCG baseline 56 222 45+Tag Dictionary 185 217 99+Max 50 tags/word 238 345 141+?=0.00001 299 493 186EASYCCG ?
nullheuristic300 221 127Table 3: Effect of our optimizations of parsingspeed.much lower on out-of-domain datasets (2.8 pointsin domain, but only 0.65-0.75 out-of-domain),suggesting that much of the C&C parser?s depen-dency model is domain specific, and does not gen-eralize well to other domains.We also briefly experimented using the C&Csupertagger (with a beam of ?
= 10?5) with theEASYCCG parser.
Performance was much worse,with an F-score of 79.63% on the 97.8% of sen-tences it parsed on CCGBank Section 23.
Thisshows that our model is reliant on the accuracy ofthe supertagger.4.3 Parsing SpeedCCG parsers have been used in distributionalapproaches to semantics (Lewis and Steedman,2013a; Lewis and Steedman, 2013b), which bene-fit from large corpora.
However, even though theC&C parser is relatively fast, it will still take over40 CPU-days to parse the Gigaword corpus on ourhardware, which is slow enough to be an obstacleto scaling distributional semantics to larger cor-9950 20 40 60 80 1001020304050Sentence LengthAverageParseTime(ms)Figure 2: Average parse times in milliseconds, bysentence length.pora such as ClueWeb.
Therefore, it is importantto be able to parse sentences at a high speed.We measured parsing times on Section 23 ofCCGBank (after developing against Section 00),using the optimizations described in Section 4.3.We also experimented with the null heuristic,which always estimates the outside probability asbeing 1.0.
Times exclude the time taken to loadmodels.Results are shown in Table 3.
The best EASY-CCG model is roughly four times faster than theC&C parser2.
Adding the tag dictionary causedaccuracy to drop slightly from 83.46 to 83.37, andmeant the parser failed to parse a single sentencein the test set (?Among its provisions :?)
but otherchanges did not affect accuracy.
The pruning inthe supertagger improves parsing speed, by limit-ing the length of the priority queue it builds for theagenda.
Of course, we could use a backoff modelto ensure full coverage (analogously to adaptivesupertagging), but we leave that to future work.Using our A?heuristic doubles the speed of pars-ing (excluding supertagging).To better understand the properties of ourmodel, we also investigate how parsing time varieswith sentence length.
Unlike the cubic CKY al-gorithm typically used by chart parsers, our A?search potentially takes exponential time in thesentence length.
For this experiment, we used theSections 02-21 of CCGBank.
Sentences were di-vided into bins of width 10, and we calculated theaverage parsing time for sentences in each bin.Results are shown in Figure 2, and demon-2It is worth noting that the C&C parser code is written inhighly-optimized C++, compared to our simple Java imple-mentation.
It seems likely that our parser could be made sub-stantially faster with a similar level of engineering effort.strate that while parsing is highly efficient for sen-tences of up to 50 words (over 95% of CCGBank),it scales super-linearly with long sentences.
Infact, Section 00 contains a sentence of 249 words,which took 37 seconds to parse (3 times longerthan the other 1912 sentences put together).
Inpractice, this scaling is unlikely to be problematic,as long sentences are typically filtered when pro-cessing large corpora.4.4 Semantic ParsingA major motivation for CCG parsing is to exploitits transparent interface to the semantics, allowingsyntactic parsers to do much of the work of seman-tic parsers.
Therefore, perhaps the most relevantmeasure of the performance of a CCG parser is itseffect on the accuracy of downstream applications.We experimented with a supervised versionof Reddy et al.
(2014)?s model for question-answering on Freebase (i.e.
without using Reddyet al.
?s lexicon derived from unlabelled text), us-ing the WEBQUESTIONS dataset (Berant et al.,2013)3.
The model learns to map CCG parses todatabase queries.
We compare the performance ofthe QA system using both our parser and C&C,taking the 10-best parses from each parser foreach sentence.
Syntactic question parsing modelswere trained from the combination of 10 copiesof Rimell and Clark (2008)?s question dataset andone copy of the CCGBankThe accuracy of Reddy et al.
(2014)?s modelvaries significantly between iterations of the train-ing data.
Rather than tune the number of iterations,we instead measure the accuracy after each iter-ation.
We experimented with the models?
1-bestanswers, and the oracle accuracy of their 100 bestanswers.
The oracle accuracy gives a better indi-cation of the performance of the parser, by miti-gating errors caused by the semantic component.Results are shown in Figure 3, and demonstratethat using EASYCCG can lead to better down-stream performance than the C&C parser.
The im-provement is particularly large on oracle accuracy,increasing the upper bound on the performance ofthe semantic parser by around 4 points.5 Related WorkCCG parsing has been the subject of much re-search.
We have already described the C&C pars-3Using the Business, Film and People domains, with 1115questions for training and 570 for testing.9962 4 6 8 10404244464850IterationF1-scoreC&CEASYCCG2 4 6 8 10545658606264Iteration100-bestOracleF1-scoreC&CEASYCCGFigure 3: Question Answering accuracy per iteration of Reddy et al.
(2014)?s supervised model.ing model.
Kummerfeld et al.
(2010) showed thatthe speed of the C&C parser can be improvedwith domain-specific self-training?similar im-provements may be possible applying this tech-nique to our model.
Auli and Lopez (2011a)have achieved the best CCG parsing accuracy, byallowing the parser and supertagger to performjoint inference (though there is a significant speedpenalty).
Auli and Lopez (2011b) were the first touse A?parsing for CCG, but their system is bothmuch slower and less accurate than ours (due to adifferent model and a different A?heuristic).
Kr-ishnamurthy and Mitchell (2014) show how CCGparsing can be improved by jointly modelling thesyntax and semantics.
Fowler and Penn (2010)apply the Petrov parser to CCG, making a smallimprovement in accuracy over the C&C parser,at the cost of a 300-fold speed decrease.
Zhangand Clark (2011) and Xu et al.
(2014) exploredshift-reduce CCG parsing, but despite the use of alinear-time algorithm, parsing speed in practice issignificantly slower than the C&C parser.Parsers based on supertagging models have pre-viously been applied to other strongly lexical-ized formalisms, such as to LTAG (Bangalore andJoshi, 1999) and to HPSG (Ninomiya et al., 2006).A major contribution of our work over these isshowing that factoring models on lexical cate-gories allows fast and exact A?parsing, withoutthe need for beam search.
Our parsing approachcould be applied to any strongly lexicalized for-malism.Our work fits into a tradition of attempting tosimplify complex models without sacrificing per-formance.
Klein and Manning (2003b) showedthat unlexicalized parsers were only slightly lessaccurate than their lexicalized counterparts.
Col-lobert et al.
(2011) showed how a range of NLPtagging tasks could be performed at high accu-racy using a small feature set based on vector-space word embeddings.
However, the extensionof this work to phrase-structure parsing (Collobert,2011) required a more complex model, and did notmatch the performance of traditional parsing tech-niques.
We achieve state-of-the-art results usingthe same feature set and a simpler model by ex-ploiting CCG?s lexicalized nature, which makes itmore natural to delegate parsing decisions to a tag-ging model.Other parsing research has focused on build-ing fast parsers for web-scale processing, typicallyusing dependency grammars (e.g.
Nivre (2003)).CCG has some advantages over dependency gram-mars, such as supporting surface-compositionalsemantics.
The fastest dependency parsers usean easy-first strategy, in which edges are addedgreedily in order of their score, with O(nlog(n))complexity (Goldberg and Elhadad, 2010; Tratzand Hovy, 2011).
This strategy is reminiscent ofour A?search, which expands the chart in a best-first order.
A?has higher asymptotic complexity,but finds a globally optimal solution.6 Future WorkWe believe that our model opens several interest-ing directions for future research.One interesting angle would be to increase theamount of information in CCGBank?s lexical en-tries, to further reduce the search space for theparser.
For example, PP categories could be dis-tinguished with the relevant preposition as a fea-ture; punctuation and coordination could be givenmore detailed categories to avoid needing theirown combinators, and slashes could be extended997with Baldridge and Kruijff (2003)?s multi-modalextensions to limit over-generation.
Honnibal andCurran (2009) show how unary rules can be lexi-calized in CCG.
Such improvements may improveboth the speed and accuracy of our model.Because our parser is factored on a unigram tag-ging model, it can be trained from isolated anno-tated words, and does not require annotated parsetrees or full sentences.
Reducing the requirementsfor training data eases the task for human annota-tors.
It may also make the model more amenableto semi-supervised approaches to CCG parsing,which have typically focused on extending the lex-icon (Thomforde and Steedman, 2011; Deoskar etal., 2014).
Finally, it may make it easier to convertother annotated resources, such as UCCA (Abendand Rappoport, 2013) or AMR (Banarescu et al.,2013), to CCG training data?as only specificwords need to be converted, rather than full sen-tences.Our model is weak at certain kinds of deci-sions, e.g.
coordination-scope ambiguities or non-local attachments.
Incorporating specific modelsfor such decisions may improve accuracy, whilestill allowing fast and exact search?for example,we intend to try including Coppola et al.
(2011)?smodel for prepositional phrase attachment.7 ConclusionsWe have shown that a simple, principled, deter-ministic parser combined with a tagging modelcan parse an expressive linguistic formalism withhigh speed and accuracy.
Although accuracyis not state-of-the-art on CCGBank, our modelgives excellent performance on two out-of-domaindatasets, and improves the accuracy of a question-answering system.
We have shown that this modelallows an efficient heuristic for A?parsing, whichmakes parsing extremely fast, and may enablelogic-based distributional semantics to scale tolarger corpora.
Our methods are directly applica-ble to other lexicalized formalisms, such as LTAG,LFG and HPSG.AcknowledgmentsWe would like to thank Tejaswini Deoskar, BharatRam Ambati, Michael Roth and the anonymousreviewers for helpful feedback on an earlier ver-sion of this paper, and Siva Reddy for running thesemantic parsing experiments.ReferencesOmri Abend and Ari Rappoport.
2013.
Universal con-ceptual cognitive annotation (ucca).
In Proceedingsof ACL.Michael Auli and Adam Lopez.
2011a.
A compari-son of loopy belief propagation and dual decompo-sition for integrated CCG supertagging and parsing.In Proceedings of the 49th Annual Meeting of theAssociation for Computational Linguistics: HumanLanguage Technologies-Volume 1, pages 470?480.Association for Computational Linguistics.Michael Auli and Adam Lopez.
2011b.
Efficient CCGparsing: A* versus adaptive supertagging.
In Pro-ceedings of the 49th Annual Meeting of the Associ-ation for Computational Linguistics: Human Lan-guage Technologies-Volume 1, pages 1577?1585.Association for Computational Linguistics.Jason Baldridge and Geert-Jan M Kruijff.
2003.
Multi-modal combinatory categorial grammar.
In Pro-ceedings of the tenth conference on European chap-ter of the Association for Computational Linguistics-Volume 1, pages 211?218.
Association for Compu-tational Linguistics.Laura Banarescu, Claire Bonial, Shu Cai, MadalinaGeorgescu, Kira Griffitt, Ulf Hermjakob, KevinKnight, Philipp Koehn, Martha Palmer, and NathanSchneider.
2013.
Abstract Meaning Representationfor sembanking.
In Proceedings of the 7th Linguis-tic Annotation Workshop and Interoperability withDiscourse, Sofia, Bulgaria, August.
Association forComputational Linguistics.Srinivas Bangalore and Aravind K Joshi.
1999.
Su-pertagging: An approach to almost parsing.
Com-putational linguistics, 25(2):237?265.Jonathan Berant, Andrew Chou, Roy Frostig, and PercyLiang.
2013.
Semantic parsing on freebase fromquestion-answer pairs.
In Proceedings of EMNLP.Johan Bos.
2008.
Wide-coverage semantic analy-sis with boxer.
In Johan Bos and Rodolfo Del-monte, editors, Semantics in Text Processing.
STEP2008 Conference Proceedings, Research in Compu-tational Semantics, pages 277?286.
College Publi-cations.Stephen Clark and James R Curran.
2007.
Wide-coverage efficient statistical parsing with CCGand log-linear models.
Computational Linguistics,33(4):493?552.Ronan Collobert, Jason Weston, L?on Bottou, MichaelKarlen, Koray Kavukcuoglu, and Pavel Kuksa.2011.
Natural language processing (almost) fromscratch.
The Journal of Machine Learning Re-search, 12:2493?2537.Ronan Collobert.
2011.
Deep learning for efficient dis-criminative parsing.
In International Conference onArtificial Intelligence and Statistics, number EPFL-CONF-192374.998Gregory F Coppola, Alexandra Birch, Tejaswini De-oskar, and Mark Steedman.
2011.
Simple semi-supervised learning for prepositional phrase attach-ment.
In Proceedings of the 12th International Con-ference on Parsing Technologies, pages 129?139.Association for Computational Linguistics.Tejaswini Deoskar, Christos Christodoulopoulos,Alexandra Birch, and Mark Steedman.
2014.Generalizing a Strongly Lexicalized Parser usingUnlabeled Data.
In Proceedings of the 14th Con-ference of the European Chapter of the Associationfor Computational Linguistics.
Association forComputational Linguistics.Jason Eisner.
1996.
Efficient normal-form parsing forcombinatory categorial grammar.
In Proceedings ofthe 34th annual meeting on Association for Com-putational Linguistics, pages 79?86.
Association forComputational Linguistics.Timothy AD Fowler and Gerald Penn.
2010.
Accu-rate context-free parsing with combinatory catego-rial grammar.
In Proceedings of the 48th AnnualMeeting of the Association for Computational Lin-guistics, pages 335?344.
Association for Computa-tional Linguistics.Yoav Goldberg and Michael Elhadad.
2010.
An effi-cient algorithm for easy-first non-directional depen-dency parsing.
In Human Language Technologies:The 2010 Annual Conference of the North AmericanChapter of the Association for Computational Lin-guistics, pages 742?750.
Association for Computa-tional Linguistics.Brian Harrington.
2010.
A semantic network ap-proach to measuring relatedness.
In Proceedingsof the 23rd International Conference on Compu-tational Linguistics: Posters, COLING ?10, pages356?364.
Association for Computational Linguis-tics.Julia Hockenmaier and Mark Steedman.
2007.
CCG-bank: a corpus of CCG derivations and dependencystructures extracted from the Penn Treebank.
Com-putational Linguistics, 33(3):355?396.Julia Hockenmaier.
2003.
Data and models for statis-tical parsing with combinatory categorial grammar.Matthew Honnibal and James R Curran.
2009.
Fullylexicalising CCGbank with hat categories.
In Pro-ceedings of the 2009 Conference on Empirical Meth-ods in Natural Language Processing: Volume 3-Volume 3, pages 1212?1221.
Association for Com-putational Linguistics.Dan Klein and Christopher D Manning.
2003a.
A*parsing: fast exact viterbi parse selection.
InProceedings of the 2003 Conference of the NorthAmerican Chapter of the Association for Computa-tional Linguistics on Human Language Technology-Volume 1, pages 40?47.
Association for Computa-tional Linguistics.Dan Klein and Christopher D Manning.
2003b.
Ac-curate unlexicalized parsing.
In Proceedings of the41st Annual Meeting on Association for Computa-tional Linguistics-Volume 1, pages 423?430.
Asso-ciation for Computational Linguistics.Jayant Krishnamurthy and Tom M Mitchell.
2014.Joint syntactic and semantic parsing with combina-tory categorial grammar.
June.Jonathan K. Kummerfeld, Jessika Roesner, Tim Daw-born, James Haggerty, James R. Curran, andStephen Clark.
2010.
Faster parsing by supertag-ger adaptation.
In Proceedings of the 48th AnnualMeeting of the Association for Computational Lin-guistics, ACL ?10, pages 345?355.
Association forComputational Linguistics.Mike Lewis and Mark Steedman.
2013a.
CombinedDistributional and Logical Semantics.
Transactionsof the Association for Computational Linguistics,1:179?192.Mike Lewis and Mark Steedman.
2013b.
Unsuper-vised induction of cross-lingual semantic relations.In Proceedings of the 2013 Conference on Empiri-cal Methods in Natural Language Processing, pages681?692, Seattle, Washington, USA, October.
Asso-ciation for Computational Linguistics.Mike Lewis and Mark Steedman.
2014.
ImprovedCCG parsing with Semi-supervised Supertagging.Transactions of the Association for ComputationalLinguistics (to appear).Takashi Ninomiya, Takuya Matsuzaki, Yoshimasa Tsu-ruoka, Yusuke Miyao, and Jun?ichi Tsujii.
2006.Extremely lexicalized models for accurate and fasthpsg parsing.
In Proceedings of the 2006 Confer-ence on Empirical Methods in Natural LanguageProcessing, EMNLP ?06, pages 155?163, Strouds-burg, PA, USA.
Association for Computational Lin-guistics.Joakim Nivre.
2003.
An efficient algorithm for pro-jective dependency parsing.
In Proceedings of the8th International Workshop on Parsing Technologies(IWPT.Sampo Pyysalo, Filip Ginter, Juho Heimonen, JariBj?rne, Jorma Boberg, Jouni J?rvinen, and TapioSalakoski.
2007.
Bioinfer: a corpus for informationextraction in the biomedical domain.
BMC bioinfor-matics, 8(1):50.Siva Reddy, Mirella Lapata, and Mark Steedman.2014.
Large-scale Semantic Parsing withoutQuestion-Answer Pairs.
Transactions of the Asso-ciation for Computational Linguistics (to appear).Laura Rimell and Stephen Clark.
2008.
Adapt-ing a lexicalized-grammar parser to contrasting do-mains.
In Proceedings of the Conference on Em-pirical Methods in Natural Language Processing,pages 475?484.
Association for Computational Lin-guistics.999Mark Steedman.
2000.
The Syntactic Process.
MITPress.Mark Steedman.
2012.
Taking Scope: The NaturalSemantics of Quantifiers.
MIT Press.Emily Thomforde and Mark Steedman.
2011.
Semi-supervised CCG lexicon extension.
In Proceedingsof the Conference on Empirical Methods in Natu-ral Language Processing, pages 1246?1256.
Asso-ciation for Computational Linguistics.Stephen Tratz and Eduard Hovy.
2011.
A fast, effec-tive, non-projective, semantically-enriched parser.In Proceedings of EMNLP.Joseph Turian, Lev Ratinov, and Yoshua Bengio.
2010.Word representations: a simple and general methodfor semi-supervised learning.
In Proceedings of the48th Annual Meeting of the Association for Compu-tational Linguistics, pages 384?394.
Association forComputational Linguistics.Wenduan Xu, Stephen Clark, and Yue Zhang.
2014.Shift-reduce ccg parsing with a dependency model.In Proceedings of the 52nd Annual Meeting ofthe Association for Computational Linguistics (ACL2014).
Association for Computational Linguistics,June.Yue Zhang and Stephen Clark.
2011.
Shift-reduceCCG parsing.
In Proceedings of the 49th AnnualMeeting of the Association for Computational Lin-guistics: Human Language Technologies-Volume 1,pages 683?692.
Association for Computational Lin-guistics.1000
