Coling 2008: Companion volume ?
Posters and Demonstrations, pages 135?138Manchester, August 2008Robust and Efficient Chinese Word Dependency Analysis withLinear Kernel Support Vector MachinesYu-Chieh WuDept.
of Computer Science and In-formation EngineeringNational Central UniversityTaoyuan, Taiwanbcbb@db.csie.ncu.edu.twJie-Chi YangGraduate Institute of NetworkLearning TechnologyNational Central UniversityTaoyuan, Taiwanyang@cl.ncu.edu.twYue-Shi LeeDept.
of Computer Science and Infor-mation EngineeringMing Chuan UniversityTaoyuan, Taiwan{leeys}@mcu.edu.twAbstractData-driven learning based on shift reduce pars-ing algorithms has emerged dependency parsingand shown excellent performance to many Tree-banks.
In this paper, we investigate the extensionof those methods while considerably improvedthe runtime and training time efficiency via L2-SVMs.
We also present several properties andconstraints to enhance the parser completeness inruntime.
We further integrate root-level and bot-tom-level syntactic information by using sequen-tial taggers.
The experimental results show thepositive effect of the root-level and bottom-levelfeatures that improve our parser from 81.17% to81.41% and 81.16% to 81.57% labeled attach-ment scores with modified Yamada?s and Nivre?smethod, respectively on the Chinese Treebank.
Incomparison to well-known parsers, such as Malt-Parser (80.74%) and MSTParser (78.08%), ourmethods produce not only better accuracy, butalso drastically reduced testing time in 0.07 and0.11, respectively.1 IntroductionWith the late development of Chinese Treebank(Xue et al 2005), parsing Chinese is still an ongo-ing research issue.
The goal of dependency parsingis to find the head-modifier (labeled) relations intexts.
Though some of the parsing algorithms arelanguage independent and show state-of-the-art per-formance on multilingual dependency Treebanks(Nivre et al, 2007; Buchholz and Marsi, 2006), theyare often too slow for online purpose.
Therefore, todevelop an efficient and effective dependencyparser is indispensable.Over the past few years, several research studieshad addressed the use of shift-reduce and edge-factored-based approaches attend fairly accurateperformance in Chinese (Cheng et al, 2005; Hall,?
2008.
Licensed under the Creative Commons Attribution-Noncommercial-Share Alike 3.0 Unported license(http://creativecommons.org/licenses/by-nc-sa/3.0/).
Some rightsreserved.2005; Wang et al, 2006).
The former (shift-reduce)is a linear time algorithm, while the latter involvesn3 for decoding where n is the length of sentence.Even the shift-reduce approaches seems to be veryefficient, most studies (Hall et al, 2007; Nivre et al,2006) yet employ nonlinear kernel methods such aspolynomial kernel support vector machines (SVMs).Furthermore, there is no research work directlycompare with the two methods with Chinese Tree-bank.
Nevertheless, the empirical training and test-ing time comparisons of those methods has not beenreported yet.In this paper, we present an efficient and robustparser for Chinese based on linear classifiers andshift-reduce parsing algorithms.
We propose severaluseful properties to enhance the completeness of thetwo well-known shift-reduce algorithms, namelyNivre?s shift reduce (NSR) (Nivre, 2003) andYamada?s shift reduce (YSR) (Yamada andMatsumoto, 2003) algorithms.
To enhance theperformance, we add root and bottom (neighbor)information by adopting sequential taggers.
We alsoperform experiments on the Chinese Treebank andcompare with two of the state-of-the-art parsers.2 Parsing AlgorithmsAt the beginning, we briefly review the selected twoparsing algorithm as follows.
The NSR makes useof four parse actions to incrementally construct thedependency graph.
By following the same notationsas (Nivre, 2003), NSR initializes with (S, I, A) = (?
,W, ? )
where S is the stack (represented as a list), Iis the queue initiated with all words, and A is the setof directed and labeled edges for the dependencygraph.
The stack is a list of words whose parent orchild has not been found entirely.
NSR incremen-tally parses a pair of words (one is the top of thestack and the other is the first word of the queue)and uses four parse actions to construct the depend-ency graph.
The four parse actions are: {Left-Arc(LA), Right-Arc (RA), Shift, Reduce}.
Both LA andRA could be parameterized with a dependence rela-135tion type.
By parsing a pair of words step-by-step,the parser terminates when the queue is empty.Similar to NSR, YSR constructs the dependencygraph by incrementally parse a pair of no_headwords.
The original YSR algorithm (Yamada andMatsumoto, 2003) makes use of three parse actionsto parse a sentence left-to-right and involves n2parser transitions.
Recently, Chang et al (2006)showed that by adding an extra parse action Wait-Left and performing the ?step-back?
operation canaccomplish parse in linear time.
The step-backmeans that after an action determined, the parse pairmoves back with except for Shift action.
Wait-Left ismainly proposed to wait the next word until all itsright children having attached to heads.
In this paper,we employ such modification to form our basicYSR algorithm.2.1 Useful PropertiesWe give more formal definitions of the dependencygraph as follows.Let R = {r1, r2, r3,?, rN} be the finite set of de-pendency arc labels with N types.
A dependencygraph G =(W, A) where W is the string of words W =w1, w2, w3, etc.
and A is the set of directed labeledarcs (wx, r, wy) where r?R, and wx, wy?W.
For aparse pair wx and wy in a sentence, we introduce thefollowing notation:1. wx?wy: wx is the head of wy, and wx?wy: wy isthe head of wx.2.
wx < wy: word wx is on the left hand side ofword wy in the sentence.3.
(wx, r, wy): denotes the word wy is the head ofwx with relation r.Definition 1: Valid dependency graphA dependency graph G is well formed and valid iffthe following conditions are true.1.
G is connected2.
G is acyclic (cycle free)3.
G is projective4.
For each node in G, there is only one parent ex-cept the root word5.
G is a single rooted graphDefinition 2: Parse pairWhen the parsing algorithm considers a pair ofword (wx, wy), we name the pair ?parse pair?.Definition 1 gives the formal definition of a validdependency graph.
Condition 3 and condition 5 arenot always true for all languages.
For example,there are multiple roots in Arabic dependency Tree-bank, while the dependency graph is usually non-projective according to the linguistic characteristics.Fortunately the dependency graphs in Chinese arefully projective and single rooted and thus compati-ble with Definition 1.However, we can not always assume the classi-fier is perfect.
During run-time, the classifier mightmake incorrect decision which leads to incorrectparse graph and even constructs an incomplete andinvalid parse graph.
For example, for NSR it is usu-ally retain more than two words that are not at-tached to their heads in the stack.
To solve it, wepropose the following properties to enhance thecompleteness of the original NSR/YSR parsers.Definition 3: One word sentenceIf there is only one unparsed word, then it must bethe root.Proposition 4: Constrained parsing IFor a parse pair (wx, wy), if the head of wx is notfound previously, then the parse action Reduce isinvalid.Proof.
The parse action Reduce will remove wxfrom the stack and leads to an unconnected andmultiple roots dependency graph (violates definition1).Proposition 5: Unique pair parsingIf there are only two unparsed words in G, then theparse action of this pair of words is limited to be{LA, RA}.Proof.
Clearly if the parse action is Reduce, then itviolates Proposition 4 (unconnected graph).
Simi-larly when applying Shift, the state does not change,i.e., there are still two unparsed words.
Nonetheless,by applying either LA or RA, the two isolated wordswill be linked and gives a connected graph.Proposition 6: Constrained parsing IIFor a parse pair (wx, wy), if the head of wx is found,then the parse action RA is invalid.Proof.
Assume the head of wx is wm.
If the parserpredicts RA, then it regards wy as the head of wx.Therefore it violates Definition 1 (for each wordthere should be at most one head in the sentence).On the other hand, actions Reduce and Shift do notchange the structure of G and is intuitively validparsing actions.
In the case of LA, by adding theedge from wx to wy, the dependency graph does notviolate definition 1.
Thus, LA is also a valid action.Definition 3 is very common and intuitively seenin the case of one word parsing at the final stage.The Proposition 4 limits the parser actions thatbring about a single-rooted dependency graph.Proposition 5 is particularly useful when there areonly two unparsed words in the stack for the NSR.On the basis of the original NSR algorithm, theparse work is done when the input queue is empty.However, words will be shift and put onto the stackif their heads are not found currently.
Finally if thequeue is empty and these words are still retained inthe stack, then it will produce multiple roots andlead to an unconnected graph.
Proposition 6 is pro-posed to avoid the case of multiple heads in the sen-tence when there are two no-head words.
To handlemore than two words on the stack, the ?step-back?operation is used.136Some of the above properties can also be appliedto YSR with slightly modifications.
We skip thedetails here owing to the space constraints.?(chiuan)?(ace)?(class)??(heros),(,)??(game)????(cliffhanger)?(.)??
(league cup)??
(struggle)PPSubVModVModSubNMod AMod NModNMod?
(guo)Figure 1: An example of Chinese dependency graph3 Root and Neighbor InformationIn general, the shift-reduce parsing algorithm in-crementally parse a pair of words until the finalparse graph has built.
However, it is usually the casethat when an error decision made at earlier stage,the real heads of the following words will be mis-attached.
In particular the head is nearby the currentpair of words.
Similarly if the root word is misclas-sified as a child of other word, then the all nodesimmediately modified by the root will attached tothe wrong root.One solution to improve this problem is to en-hance the root and bottom (neighbor) informationduring parse.
To obtain such information, we em-ploy the sequential taggers to predict.
That is onesequential tagger learns to determine whether thecurrent word is the child of its left/right word ornone of them while the other is to recognize the rootword.
For example, if the word is labeled as ?left-Mod?, then it means its left word is the head of itand the relation tag is ?Mod?.Finding root in Chinese is even simpler, sincethere is only one root word in the same sentence inChinese Treebank.
Here, we adopt the same tech-nology to label the root by using sequential taggers.Such solution had also been applied to EnglishTreebank where a polynomial kernel SVM was used(Isozaki et al, 2004).
However, there are two differ-ences to our method.
First, we enable our root tag-ger to incorporate bottom-level features.
More pre-cisely, the two taggers are cascaded combined.
Sec-ond, to enhance the top-level syntactic information,our root tagger does not only recognize the rootword, but also the words which belong to the im-mediate child of it.
We give the following propertyto prove that attaching all root children to the rootstill leads to a valid dependency graph.Proposition 7: Cycle-free for root taggerThe dependency graph is a cycle-free graph by link-ing root child to the root words.Proof.
The minimum cycle length in a valid de-pendency graph is two (two edges for two words bylinked each other).
Assume there are K children fora root.
By attaching all children to the root, it leadsto the out-degree of each child is 1, while the in-degree of the root is K. According to the Definition1, the root word does not have any parent (out-degree of the root is exactly zero) and will neverattach to any word in the sentence (include its chil-dren).Figure 2: Attaching neighbor relations with sequentaltaggersFigure 3: Attaching root words with sequential taggersThe sequential tagger used in this paper isCRF++ (Kudo et al, 2004).
One advantage of con-ditional random fields (CRF) is that it is a structurallearning method and can search optimal tag se-quence with efficient Viterbi search algorithm.Features used for the two taggers include word,part-of-speech tag, and prefix/suffix Chinese char-acters with context window = 3.
Features that oc-curs less than twice in the training data is removed.Figure 1 shows an example of Chinese dependencygraph.
Figure 2 illustrates the sample of attachingneighbors with CRF++ by using the same sentenceas in Figure 1.
Figure 3 shows the example of iden-tifying root and its children with CRF++.Table 1: Feature set used for NSR and YSRFeature type Stack position Queue positionWordPOSBiWordBiPOSNeighbor (NSR)Root (NSR)Neighbor (YSR)Root (YSR)HistoryChild (Word)Child (POS)-1,0-1,0(-2,-1),(-1,0),(-2,0),(-1,+1)(-2,-1),(-1,0),(-2,0),(-1,+1)-2,-1,0-1,000-2,-1000,+1,+2,+30,+1,+2,+3(0,1),(1,2),(2,3),(0,2),(1,3)(0,1),(1,2),(2,3),(0,2),(1,3)0,+1,+20004 ExperimentsWe randomly select 90% of the Chinese Treebank5.1 corpus for training and the remaining 10% isused for testing.
Totally there are 0.45 millionwords in the training data and 50144 words for test-ing.
By following (Hall et al, 2006), we use thesame headword table to convert the CTB into de-pendency structure.
The gold-tagged POS tags are137used in the experiments.
All experimental resultsare evaluated by LAS (label attachment score), UAS(unlabeled attachment score), and root accuracy.4.1 SettingsIn this paper, we employ the MSTParser (McDon-ald et al, 2006) and MaltParser (Nivre, 2003) forcomparison.
We adopt the best settings for Malt-Parser with SVM and MBL learners as reported by(Hall et al, 2006)2.
For MSTParser, the Eisner?sdecoding algorithm is used.The learner we used in this paper is L2-SVM withlinear kernel (Keerthi and DeCoste, 2005).
The one-versus-all (OVA) strategy is applied to handle mul-ticlass problems.
Features that appear less thantwice are removed from the feature set.
Table 4 liststhe feature set for the NSR and YSR.4.2 ResultsTable 2 summarizes overall experimental results.The final two rows list the entire training and testingtime of the corresponding methods.
From this table,we can see that our method (both NSR and YSR)achieve the best and second best parsing accuracy interms of LAS, UAS, and root accuracy.
For testingtime efficiency, both our NSR and YSR also outper-form the other methods.
Meanwhile there is no sig-nificant difference between NSR and YSR from theaspect of run time efficiency view.
In comparison toMaltParser, NSR yields 14 times faster in parsingspeed.Next, we analyze the effect of the two sequentialtaggers.
The pure system performance of theneighbor tagger is 88.54 in F(?)
rate, while the roottagger only achieves 61.67 F(?)
score.
The entiretraining time of the two taggers takes about 10hours.
Table 3 shows the compared results.
It isclear that adding the two taggers leads better pars-ing accuracy than pure NSR and YSR.
For example,it enhances the LAS score from 81.17 to 81.41 forNSR.
Furthermore, the pure NSR and YSR stillproduce better parsing accuracy than MaltParser andMSTParser.5 ConclusionThis paper presents an efficient and robust Chinesedependency parsing based on shift reduce parsingalgorithms.
We employ two sequential taggers tolabel the root and neighbor information as features.Experimental results show that our methods outper-form two top-performed parsers, MaltParser andMSTParser in both accuracy and run-time efficiency.In the future, we will to investigate the effect of fullparsing Chinese by applying shift-reduce-like ap-proaches.2 http://w3.msi.vxu.se/~nivre/research/chiMaltSVM.htmlTable 2: Parsing accuracy of each parsing algorithmthis paper EvaluationMetricsMaltParser(SVM)MaltParser(MBT) MST NSR YSRLAS 80.74 73.53 78.08 81.41 81.57UAS 81.98 75.40 79.53 82.60 82.76LAC 91.28 86.26 89.21 92.26 92.37RootA65.88 69.36 73.71 74.93 77.61SentenceA33.12 25.67 24.07 32.85 33.44TrainTime 6.74hr 3.42min 7.51hr 2.76hr 2.24hrTestTime 15.92min 3.22min 10.15min 1.12min 1.15minTable 3: Effective of the additional root and neighborinformationImprovement rate NSR YSRLAS 81.17?81.41 81.16?81.57UAS 82.33?82.60 82.37?82.76LAC 92.07?92.26 92.15?92.37Root_ Accuracy 74.14?74.93 76.24?77.61ReferencesSabine Buchholz and Erwin Marsi.
2006.
CoNLL-X Shared Task onMultilingual Dependency Parsing.
In Proc.
of CoNLL, pp.
149-164.Ming-Wei Chang, Quang Do, and Dan Roth.
2006.
Multilingual depend-ency parsing: a pipeline approach.
Recent Advances in Natural Lan-guage Processing, pp.
195-204.Yuchang Cheng, Masayuki Asahara, and Yuji Matsumoto.
2005.
Chinesedeterministic dependency analyzer: examining effects of global fea-tures and root node finder.
In Proc.
of SIGHAN, pp.
17-24.Yuchang Cheng, Masayuki  Asahara, and Yuji Matsumoto.
2006.
Multi-lingual Dependency Parsing at NAIST.
In Proc.
of CoNLL, pp.
191-195.Jason Eisner.
1996.
Three New Probabilistic Models for DependencyParsing: An Exploration.
In Proc.
of COLING, pp.
340-345.Johan Hall, Joakim Nivre, and Jens Nilsson.
2006.
Discriminative Classi-fiers for Deterministic Dependency Parsing.
In Proc.
of COLING-ACL Main Conference Poster Sessions, pp.
316-323.Hideki Isozaki, Hideto Kazawa, and Tsutomu Hirao.
2004.
A determinis-tic word dependency analyzer enhanced with preference learning.
InProc.
of COLING, pp.
275-281.Sathiya Keerthi and Dennis DeCoste.
2005.
A modified finite Newtonmethod for fast solution of large scale linear SVMs, JMLR, 6: 341-361.Taku Kudo and Yuji Matsumoto.
2003.
Fast methods for kernel-basedtext analysis.
In Proc.
of ACL, pp.
24-31.Taku Kudo, Kaoru Yamamoto, and Yuji Matsumoto.
2004.
Applyingconditional random fields to Japanese morphological analysis, InProc.
of EMNLP, pp.
230-237.Ryan McDonald, Kevin Lerman, and Fernando Pereira.
2006.
Multilin-gual dependency analysis with a two-stage discriminative.
In Proc.
ofCoNLL, pp.
216-220.Joakim Nivre.
2003.
An efficient algorithm for projective dependencyparsing.
In Proc.
of IWPT, pp.
149-160.Joakim Nivre, Johan Hall, Jens Nilsson, G?lsen Eryigit, and SvetoslavMarinov.
2006.
Labeled pseudo-projective dependency parsing withsupport vector machines.
In Proc.
of CoNLL, pp.
221-226.Joakim Nivre, Johan Hall, Sandra K?bler, Ryan McDonald, Jens Nilsson,Sebastian Riedel, and Deniz Yuret.
2007.
The CoNLL 2007 sharedtask on dependency parsing.
In Proc.
of EMNLP-CoNLL, pp.
915-932.Qin Iris Wang, Dekang Lin, and Dale Schuurmans.
2007.
Simple trainingof dependency parsers via structured boosting.
In Proc.
of IJCAI, pp.1756-1762.Yu-Chieh Wu, Jie-Chi Yang, and Yue-Shi Lee.
2007.
Multilingual de-terministic dependency parsing framework using modified finiteNewton method support vector machines.
In Proc.
of EMNLP-CoNLL, pp.
1175-1181.Nianwen Xue, Fei Xia, Fu-Dong Chiou and Martha Palmer.
2005.
ThePenn Chinese Treebank: phrase structure annotation of a large corpus.Natural Language Engineering, 11(2):207-238.Hiroyasu Yamada and Yuji Matsumoto.
2003.
Statistical dependencyanalysis with support vector machines.
In Proc.
of IWPT, pp.
195-206.138
