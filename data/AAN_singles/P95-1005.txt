Discourse Processing of Dialogues with Mult iple ThreadsCaro lyn  Penste in  Ros~ t, Barbara  D i  Eugen io  t, Lor i  S. Lev in  t,Caro l  Van  Ess -Dykema tt Computat iona l  L ingu is t ics  P rogramCarneg ie  Mel lon  Un ivers i tyP i t t sburgh ,  PA ,  15213{cprose, dieugeni}@icl, cmu.
eduIsl?cs.
cmu.
edu* Depar tment  of DefenseMai l  stop:  R5259800 Savage RoadFt .
George  G. Meade,  MD 20755-6000cj vanes?afterl ife, ncsc.
milAbst ractIn this paper we will present our ongoingwork on a plan-based iscourse processordeveloped in the context of the EnthusiastSpanish to English translation system aspart of the JANUS multi-lingual speech-to-speech translation system.
We will demon-strate that theories of discourse which pos-tulate a strict tree structure of discourseon either the intentional or attentionallevel are not totally adequate for handlingspontaneous dialogues.
We will presentour extension to this approach along withits implementation i our plan-based is-course processor.
We will demonstrate hatthe implementation of our approach out-performs an implementation based on thestrict tree structure approach.1 In t roduct ionIn this paper we will present our ongoing work on aplan-based iscourse processor developed in the con-text of the Enthusiast Spanish to English translationsystem (Suhm et al 1994) as part of the JANUSmulti-lingual speech-to-speech translation system.The focus of the work reported here has been to drawupon techniques developed recently in the compu-tational discourse processing community (Lambert1994; Lambert 1993; Hinkelman 1990), developing adiscourse processor flexible enough to cover a largecorpus of spontaneous dialogues in which two speak-ers attempt o schedule a meeting.There are two main contributions of the work wewill discuss in this paper.
From a theoretical stand-point, we will demonstrate that theories which pos-tulate a strict tree structure of discourse (henceforth,Tree Structure Theory, or TST) on either the inten-tional level or the attentional level (Grosz and Sidner1986) are not totally adequate for covering sponta-neous dialogues, particularly negotiation dialogueswhich are composed of multiple threads.
Theseare negotiation dialogues in which multiple propo-sitions are negotiated in parallel.
We will discussour proposea extension to TST which handles thesestructures in a perspicuous manner.
From a prac-tical standpoint, our second contribution will be adescription of our implemented iscourse processorwhich makes use of this extension of TST, taking asinput the imperfect result of parsing these sponta-neous dialogues.We will also present a comparison of the perfor-mance of two versions of our discourse processor,one based on strict TST, and one with our extendedversion of TST, demonstrating that our extensionof TST yields an improvement in performance onspontaneous scheduling dialogues.A strength of our discourse processor is thatbecause it was designed to take a language-independent meaning representation (interlingua) asits input, it runs without modification on either En-glish or Spanish input.
Development of our dis-course processor was based on a corpus of 20 spon-taneous Spanish scheduling dialogues containing atotal of 630 sentences.
Although development andinitial testing of the discourse processor was donewith Spanish dialogues, the theoretical work on themodel as well as the evaluation presented in this pa-per was done with spontaneous English dialogues.In section 2, we will argue that our proposed ex-tension to Standard TST  is necessary for makingcorrect predictions about patterns of referring ex-pressions found in dialogues where multiple alter-natives are argued in parallel.
In section 3 we willpresent our implementation of Extended TST.
Fi-nally, in section 4 we will present an evaluationof the performance of our discourse processor withExtended TST compared to its performance usingStandard TST.2 D iscourse  S t ructureOur discourse model is based on an analysis of nat-urally occurring scheduling dialogues.
Figures 1 and2 contain examples which are adapted from natu-rally occurring scheduling dialogues.
These exam-ples contain the sorts of phenomena we have foundin our corpus but have been been simplified for the31(1)(2)(3) $2:(4)(5) SI:(6)(7)(8)(9) $2:(lO)(11)(12)(13)(14)(15)(16)(17)(18)FigureS 1: We need to set up a schedule for the meeting.How does your schedule look for next week?Well, Monday and Tuesday both mornings are good.Wednesday afternoon is good also.It looks like it will have to be Thursday then.Or Friday would also possibly work.Do you have time between twelve and two on Thursday?Or do you think sometime Friday afternoon you could meet?No.Thursday I have a class.And Friday is really tight for me.How is the next week?If all else fails there is always video conferencing.S 1: Monday, Tuesday, and Wednesday I am out of town.But Thursday and Friday are both good.How about Thursday at twelve?$2: Sounds good.See you then.1: Example  of  Del iberat ing Over A Meet ing T imepurpose of making our argument easy to follow.
No-tice that in both of these examples, the speakersnegotiate over multiple alternatives in parallel.We challenge an assumption underlying the bestknown theories of discourse structure (Grosz andSidner 1986; Scha and Polanyi 1988; Polanyi 1988;Mann and Thompson 1986), namely that discoursehas a recursive, tree-like structure.
Webber (1991)points out that Attentional State i is modeled equiv-alently as a stack, as in Grosz and Sidner's approach,or by constraining the current discourse segment toattach on the rightmost frontier of the discoursestructure, as in Polanyi and Scha's approach.
Thisis because attaching a leaf node corresponds topush-ing a new element on the stack; adjoining a node Dito a node Dj corresponds to popping all the stackelements through the one corresponding to Dj andpushing Di on the stack.
Grosz and Sider (1986),and more recently Lochbaum (1994), do not for-mally constrain their intentional structure to a stricttree structure, but they effectively impose this lim-itation in cases where an anaphoric link must bemade between an expression inside of the currentdiscourse segment and an entity evoked in a different1Attentional State is the representation which is usedfor computing which discourse ntities are most salient.segment.
If the expression can only refer to an entityon the stack, then the discourse segment purpose 2of the current discourse segment must be attachedto the rightmost frontier of the intentional structure.Otherwise the entity which the expression refers towould have already been popped from the stack bythe time the reference would need to be resolved.We develop our theory of discourse structure inthe spirit of (Grosz and Sidner 1986) which hasplayed an influential role in the analysis of discourseentity saliency and in the development of dialogueprocessing systems.
Before we make our argument,we will argue for our approach to discourse segmen-tation.
In a recent extension to Grosz and Sidner'soriginal theory, described in (Lochbaum 1994), eachdiscourse segment purpose corresponds to a partialor full shared plan 3 (Grosz and Kraus 1993).
Thesediscourse segment purposes are expressed in termsof the two intention operators described in (Groszand Kraus 1993), namely Int.
To which representsan agent's intention to perform some action and2A discourse segment purpose denotes the goal whichthe speaker(s) attempt to accomplish in engaging in theassociated segment of talk.3A Shared Plan is a plan which a group of two ormore participants intend to accomplish together.32Sl:S2:SI:DS 01.
When can you meet next week?
SI:DS 12.
Tuesday afternoon looks good.
S2:.... DS23.
I could do it Wednesday morning too.DS 34.
Tuesday I have a class from 12:00-1:30.
Sl:.
DS 45.
But the other day sounds good.DSA1.
When can you meet next week?r - - -  DSB!'
2.
Tuesday afternoon looks good.
!i DS C .
.
.
.
!
.
.
.
.
.
.3.
I could do it Wednesday morning too.DS D ~-- -, 4.
Tuesday I have aclass from 12:00-1:30..- .... DSEi 5.
But the other day sounds good.Simple Stack based Structure Proposed StructureFigure 2: Sample  Ana lys i sInt.
That which represents an agent's intention thatsome proposition hold.
Potential intentions are usedto account for an agent's process of weighing differ-ent means for accomplishing an action he is com-mitted to performing (Bratman, Israel, & Pollack1988).
These potential intentions, Pot.Int.
To andPot.Int.
That, are not discourse segment purposes inLochbaum's theory since they cannot form the ba-sis for a shared plan having not been decided uponyet and being associated with only one agent.
It isnot until they have been decided upon that they be-come Int.
To's and Int.
That's which can then becomediscourse segment purposes.
We argue that poten-tial intentions must be able to be discourse segmentpurposes.Potential intentions are expressed within portionsof dialogues where speakers negotiate over how toaccomplish a task which they are committed to com-pleting together.
For example, deliberation overhow to accomplish a shared plan can be repre-sented as an expression of multiple Pot.Int.
To's andPot.Int.
That's, each corresponding to different alter-natives.
As we understand Lochbaum's theory, foreach factor distinguishing these alternatives, the po-tential intentions are all discussed inside of a singlediscourse segment whose purpose is to explore theoptions so that the decision can be made.The stipulation that Int.
To's and Int.
That's canbe discourse segment purposes but Pot.Int.
To's andPot.Int.
That's cannot has a major impact on theanalysis of scheduling dialogues uch as the one inFigure 1 since the majority of the exchanges inscheduling dialogues are devoted to deliberating overwhich date and at which time to schedule a meet-ing.
This would seem to leave all of the delibera-tion over meeting times within a single monolithicdiscourse segment, leaving the vast majority of thedialogue with no segmentation.
As a result, we areleft with the question of how to account for shiftsin focus which seem to occur within the deliberationsegment as evidenced by the types of pronominal ref-erences which occur within it.
For example, in thedialogue presented in Figure 1, how would it be pos-sible to account for the differences in interpretationof "Monday" and "Tuesday" in (3) with "Monday"and "Tuesday" in (14)?
It cannot simply be a matterof immediate focus since the week is never mentionedin (13).
And there are no semantic lues in the sen-tences themselves to let the hearer know which weekis intended.
Either there is some sort of structure inthis segment more fine grained than would be ob-tained if Pot.Int.
To's and Pot.Int.
That's cannot bediscourse segment purposes, or another mechanismmust be proposed to account for the shift in focuswhich occurs within the single segment.
We arguethat rather than propose an additional mechanism,it is more perspicuous to lift the restriction thatPot.Int.
To's and Pot.Int.
That's cannot be discoursesegment purposes.
In our approach a separate dis-course segment is allocated for every potential plandiscussed in the dialogue, one corresponding to eachparallel potential intention expressed.Assuming that potential intentions form the ba-sis for discourse segment purposes just as intentions33do, we present two alternative analyses for an ex-ample dialogue in Figure 2.
The one on the leftis the one which would be obtained if AttentionalState were modeled as a stack.
It has two shortcom-ings.
The first is that the suggestion for meeting onWednesday in DS 2 is treated like an interruption.Its focus space is pushed onto the stack and thenpopped off when the focus space for the responseto the suggestion for Tuesday in DS 3 is pushed 4.Clearly, this suggestion is not an interruption how-ever.
Furthermore, since the focus space for DS 2 ispopped off when the focus space for DS 4 is pushedon, 'Wednesday is nowhere on the focus stack when"the other day", from sentence 5, must be resolved.The only time expression on the focus stack at thatpoint would be "next week".
But clearly this ex-pression refers to Wednesday.
So the other problemis that it makes it impossible to resolve anaphoricreferring expressions adequately in the case wherethere are multiple threads, as in the case of parallelsuggestions negotiated at once.We approach this problem by modeling Atten-tional State as a graph structured stack rather thanas a simple stack.
A graph structured stack is astack which can have multiple top elements at anypoint.
Because it is possible to maintain more thanone top element, it is possible to separate multiplethreads in discourse by allowing the stack to branchout, keeping one branch for each thread, with theone most recently referred to more strongly in fo-cus than the others.
The analysis on the right handside of Figure 2 shows the two branches in differentpatterns.
In this case, it is possible to resolve thereference for "the other day" since it would still beon the stack when the reference would need to beresolved.
Implications of this model of AttentionalState are explored more fully in (Rosd 1995).3 Discourse ProcessingWe evaluated the effectiveness ofour theory of dis-course structure in the context of our implementeddiscourse processor which is part of the EnthusiastSpeech translation system.
Traditionally machinetranslation systems have processed sentences in iso-lation.
Recently, however, beginning with work atATR, there has been an interest in making use of dis-course information in machine translation.
In (Iidaand Arita 1990; Kogura et al 1990), researchersat ATR advocate an approach to machine transla-tion called illocutionary act based translation, argu-ing that equivalent sentence forms do not necessar-ily carry the same illocutionary force between lan-guages.
Our implementation is described more fullyin (Rosd 1994).
See Figure 4 for the discourse rep-4Alternatively, DS 2 could not be treated like an in-terruption, in which case DS 1 would be popped beforeDS 2 would be pushed.
The result would be the same.DS 2 would be popped before DS 3 would be pushed.
((when((frame *simple-time)(day-of-week wednesday)(time-of-day morning)))(a-speech-act(*multiple* *suggest *accept))(who((frame *i)))(frame *free)(sentence-type *state)))Sentence:  I could do it Wednesday morning too.Figure 3: Sample  In ter l ingua  Representat ionw i th  Poss ib le  Speech  Acts  Notedresentation our discourse processor obtains for theexample dialogue in Figure 2.
Note that although acomplete tripartite structure (Lambert 1993) is com-puted, only the discourse level is displayed here.Development of our discourse processor was basedon a corpus of 20 spontaneous Spanish scheduling di-alogues containing a total of 630 sentences.
Thesedialogues were transcribed and then parsed with theGLR* skipping parser (Lavie and Tomita 1993).
Theresulting interlingua structures (See Figure 3 for anexample) were then processed by a set of matchingrules which assigned a set of possible speech actsbased on the interlingua representation returned bythe parser similar to those described in (Hinkelman1990).
Notice that the list of possible speech actsresulting from the pattern matching process are in-serted in the a-speech-act slot ('a' for ambiguous).It is the structure resulting from this pattern match-ing process which forms the input to the discourseprocessor.
Our goals for the discourse processor in-clude recognizing speech acts and resolving ellipsisand anaphora.
In this paper we focus on the task ofselecting the correct speech act.Our discourse processor is an extension of Lam-bert's implementation (Lambert 1994; Lambert1993; Lambert and Carberry 1991).
We have chosento pattern our discourse processor after Lambert'srecent work because of its relatively broad coveragein comparison with other computational discoursemodels and because of the way it represents rela-tionships between sentences, making it possible torecognize actions expressed over multiple sentences.We have left out aspects of Lambert's model whichare too knowledge intensive to get the kind of cov-erage we need.
We have also extended the set ofstructures recognized on the discourse level in orderto identify speech acts such as Suggest, Accept, andReject which are common in negotiation discourse.There are a total of thirteen possible speech actswhich we identify with our discourse processor.
SeeFigure 5 for a complete list.34Request- SuggtSuggestlon(S2,S 1,...)Request-Suggestion-Form(S1,S2,...)Argument-Segment(S2,S 1,...)Suggest- Suggest- Response(S 1,$2,...)Form(S2,S1,...) Form(S2,S1,...) /Ask_ief(S 1,$2,...) InfoT(S2,S1,...) Infon~(S2,S 1,...)Ref-Request(S1,S2,...) Tell(S2,S1,...) Tell(S2,S1,...)I I /Surface- Surface- Surface-Query- State(S2,S 1,...) State(s2,s 1,...)Ref(S 1,$2,...)Respon\]e(S 1 ,$2,...)lReJect(S 1,$2,...) I Accelt(S 1'$2'"')Reject- Accept-Form/S1,S2,...) Fo7(S1,$2,...)/ /Inform(S1,S2,...) Inform(S1,S2,...)J ITell(S 1 ,S2,...) Tell(Si 1 ,$2,...)Surface- Surface-State(S 1 ,S2,...) State(S 1 ,S2,...)(1) When can... (2) Tuesday... (3) I could... (4) Tuesday...Figure 4: Sample  Discourse S t ructure(5) But the other...It is commonly impossible to tell out of contextwhich speech act might be performed by some ut-terances ince without the disambiguating contextthey could perform multiple speech acts.
For exam-ple, "I'm free Tuesday."
could be either a Suggestor an Accept.
"Tuesday I have a class."
could be aState-Constraint or a Reject.
And "So we can meetTuesday at 5:00."
could be a Suggest or a Confirm-Appointment.
That is why it is important o con-struct a discourse model which makes it possible tomake use of contextual information for the purposeof disambiguating.Some speech acts have weaker forms associatedwith them in our model.
Weaker and strongerforms very roughly correspond to direct and indirectspeech acts.
Because very suggestion, rejection, ac-ceptance, or appointment confirmation is also giv-ing information about the schedule of the speaker,State-Constraint is considered to be a weaker form ofSuggest, Reject, Accept, and Confirm-Appointment.Also, since every acceptance xpressed as "yes" isalso an affirmative answer, Affirm is considered tobe a weaker form of Accept.
Likewise Negate is con-sidered a weaker form of Reject.
This will come intoplay in the next section when we discuss our evalu-ation.When the discourse processor computes a chain ofinference for the current input sentence, it attachesit to the current plan tree.
Where it attaches de-termines which speech act is assigned to the inputsentence.
For example, notice than in Figure 4, be-cause sentences 4 and 5 attach as responses, they areassigned speech acts which are responses (i.e.
eitherAccept or Reject).
Since sentence 4 chains up to aninstantiation of the Response operator from an in-stantiation of the Reject operator, it is assigned thespeech act Reject.
Similarly, sentence 5 chains up toan instantiation of the Response operator from aninstantiation of the Accept operator, sentence 5 isassigned the speech act Accept.
After the discourse35Speech  ActOpeningClosingSuggestRejectAcceptState-ConstraintConfirm-AppointmentNegateAffirmRequest-ResponseRequest-SuggestionRequest-ClarificationRequest-ConfirmationExampleHi, Cindy.See you then.Are you free on the morningof the eighth?Tuesday I have a class.Thursday I'm free the wholeday.This week looks pretty busyfor me.So Wednesday at 3:00 then?no.yes.What do you think?What looks good for you?What did you say aboutWednesday?You said Monday was free?Figure 5: Speech  Acts  covered  by the  sys temprocessor attaches the current sentence to the plantree thereby selecting the correct speech act in con-text, it inserts the correct speech act in the speech-act slot in the interlingua structure.
Some speechacts are not recognized by attaching them to theprevious plan tree.
These are speech acts such asSuggest which are not responses to previous peechacts.
These are recognized in cases where the planinferencer chooses not to attach the current inferencechain to the previous plan tree.When the chain of inference for the current sen-tence is attached to the plan tree, not only is thespeech act selected, but the meaning representationfor the current sentence is augmented from context.Currently we have only a limited version of this pro-cess implemented, namely one which augments thetime expressions between previous time expressionsand current ime expressions.
For example, considerthe case where Tuesday, April eleventh as been sug-gested, and then the response only makes referenceto Tuesday.
When the response is attached to thesuggestion, the rest of the time expression can befilled in.The decision of which chain of inference to selectand where to attach the chosen chain, if anywhere, ismade by the focusing heuristic which is a version ofthe one described in (Lambert 1993) which has beenmodified to reflect our theory of discourse structure.In Lambert's model, the focus stack is representedimplicitly in the rightmost frontier of the plan treecalled the active path.
In order to have a focus stackwhich can branch out like a graph structured stackin this framework, we have extended Lambert's planoperator formalism to include annotations on the ac-tions in the body of decomposition plan operatorswhich indicate whether that action should appear 0or 1 times, 0 or more times, 1 or more times, or ex-actly 1 time.
When an attachment to the activepath is attempted, a regular expression evaluatorchecks to see that it is acceptable to make that at-tachment according to the annotations in the planoperator of which this new action would become achild.
If an action on the active path is a repeat-ing action, rather than only the rightmost instancebeing included on the active path, all adjacent in-stances of this repeating action would be included.For example, in Figure 4, after sentence 3, not onlyis the second, rightmost suggestion in focus, alongwith its corresponding inference chain, but both sug-gestions are in focus, with the rightmost one beingslightly more accessible than the previous one.
Sowhen the first response is processed, it can attach tothe first suggestion.
And when the second responseis processed, it can be attached to the second sug-gestion.
Both suggestions remain in focus as long asthe node which immediately dominates the parallelsuggestions i on the rightmost frontier of the plantree.
Our version of Lambert's focusing heuristic isdescribed in more detail in (Ros~ 1994).4 Eva luat ionThe evaluation was conducted on a corpus of 8 pre-viously unseen spontaneous English dialogues con-taining a total of 223 sentences.
Because spokenlanguage is imperfect to begin with, and because theparsing process is imperfect as well, the input to thediscourse processor was far from ideal.
We are en-couraged by the promising results presented in figure6, indicating both that it is possible to successfullyprocess a good measure of spontaneous dialogues ina restricted domain with current technology, 5 andthat our extension of TST  yields an improvement inperformance.The performance of the discourse processor wasevaluated primarily on its ability to assign the cor-rect speech act to each sentence.
We are not claim-ing that speech act recognition is the best way toevaluate the validity of a theory of discourse, butbecause speech act recognition is the main aspect ofthe discourse processor which we have implemented,and because recognizing the discourse structure ispart of the process of identifying the correct speechact, we believe it was the best way to evaluate thedifference between the two different focusing mech-anisms in our implementation at this time.
Prior tothe evaluatic.n, the dialogues were analyzed by handsit should be noted that we do not claim to havesolved the problem of discourse processing of spon-taneous dialogues.
Our approach is coursely grainedand leaves much room for future development in everyrespect.36Vers ion  Good Acceptab le  Incor rectExtended TSTStandard  TST171 total(77%)144 basedplan-inferenceon161 total(72%)116 based on planinference27 total(12%)22 based on planinference33 total(15%)25 based on planinference25 total(11%)20 based on planinference28 total(13%)23 based on planinferenceFigure 6: Per fo rmance  Eva luat ion  Resu l tsand sentences were assigned their correct speech actfor comparison with those eventually selected by thediscourse processor.
Because the speech acts for thetest dialogues were coded by one of the authors andwe do not have reliability statistics for this encoding,we would draw the attention of the readers more tothe difference in performance between the two focus-ing mechanisms rather than to the absolute perfor-mance in either case.For each sentence, if the correct speech act, or ei-ther of two equally preferred best speech acts wererecognized, it was counted as correct.
If a weakerform of a correct speech act was recognized, it wascounted as acceptable.
See the previous section formore discussion about weaker forms of speech acts.Note that if a stronger form is recognized when onlythe weaker one is correct, it is counted as wrong.And all other cases were counted as wrong as well,for example recognizing a suggestion as an accep-tance.In each category, the number of speech acts de-termined based on plan inference is noted.
In somecases, the discourse processor is not able to assign aspeech act based on plan inference.
In these cases, itrandomly picks a speech act from the list of possiblespeech acts returned from the matching rules.
Thenumber of sentences which the discourse processorwas able to assign a speech act based on plan infer-ence increases from 164 (74%) with Standard TSTto 186 (83%) with Extended TST.
As Figure 6 indi-cates, in many of these cases, the discourse processorguesses correctly.
It should be noted that althoughthe correct speech act can be identified without planinference in many cases, it is far better to recognizethe speech act by first recognizing the role the sen-tence plays in the dialogue with the discourse pro-cessor since this makes it possible for further pro-cessing to take place, such as ellipsis and anaphoraresolution.
6You will notice that Figure 6 indicates that the6Ellipsis and anaphora resolution are areas for futuredevelopment.biggest difference in terms of speech act recognitionbetween the two mechanisms i that Extended TSTgot more correct where Standard TST got more ac-ceptable.
This is largely because of cases like the onein Figure 4.
Sentence 5 is an acceptance to the sug-gestion made in sentence 3.
With Standard TST, theinference chain for sentence 3 would no longer be onthe active path when sentence 5 is processed.
There-fore, the inference chain for sentence 5 cannot attachto the inference chain for sentence 3.
This makes itimpossible for the discourse processor to recognizesentence 5 as an acceptance.
It will try to attach it tothe active path.
Since it is a statement informing thelistener of the speaker's chedule, a possible speechact is State-Constraint.
And any State-Constraintcan attach to the active path as a confirmation be-cause the constraints on confirmation attachmentsare very weak.
Since State-Constraint is weaker thanAccept, it is counted as acceptable.
While this is ac-ceptable for the purposes of speech act recognition,and while it is better than failing completely, it isnot the correct discourse structure.
If the reply, sen-tence 5 in this example, contains an abbreviated oranaphoric expression referring to the date and timein question, and if the chain of inference attaches tothe wrong place on the plan tree as in this case, thenormal procedure for augmenting the shortened re-ferring expression from context could not take placecorrectly as the attachment is made.In a separate valuation with the same set of di-alogues, performance in terms of attaching the cur-rent chain of inference to the correct place in theplan tree for the purpose of augmenting temporalexpressions from context was evaluated.
The resultswere consistent with what would have been expectedgiven the results on speech act recognition.
Stan-dard TST  achieved 64.3% accuracy while ExtendedTST achieved 70.4%.While the results are less than perfect, they indi-cate that Extended TST outperforms Standard TSTon spontaneous scheduling dialogues.
In summary,Figure 6 makes clear, with the extended version ofTST, the number of speech acts identified correctly37increases from 161 (72%) to 171 (77%), and the num-ber of sentences which the discourse processor wasable to assign a speech act based on plan inferenceincreases from 164 (74%) to 186 (83%).5 Conc lus ions  and  Future  D i rec t ionsIn this paper we have demonstrated one way inwhich TST is not adequate for describing the struc-ture of discourses with multiple threads in a per-spicuous manner.
While this study only exploresthe structure of negotiation dialogues, its resultshave implications for other types of discourse aswell.
This study indicates that it is not a struc-tural property of discourse that Attentional State isconstrained to exhibit stack like behavior.
We in-tend to extend this research by exploring more fullythe implications of our extension to TST in terms ofdiscourse focus more generally.
It is clear that it willneed to be limited by a model of resource bounds ofattentional capacity (Walker 1993) in order to avoidovergenerating.We have also described our extension to TST interms of a practical application of it in our imple-mented iscourse processor.
We demonstrated thatour extension to TST yields an increase in perfor-mance in our implementation.6 AcknowledgementsThis work was made possible in part by funding fromthe U.S. Department of Defense.Re ferencesM.
E. Bratman, D. J. Israel and M. E. Pollack.
1988.Plans and resource-bounded practical reasoning.Computational Intelligence, 3, pp 117-136.B.
J. Grosz and S. Kraus.
1993.
Collaborative plansfor group activities.
In Proceedings of IJCAI-93,pp 367-373, Chambery, Savoie, France.B.
Grosz and C. Sidner.
1986.
Attention, Intentions,and the Structure of Discourse.
ComputationalLinguistics 12, 175-204.E.
A. Hinkelman.
1990.
Linguistic and PragmaticConstraints on Utterance Interpretation.
PhDdissertation, University of Rochester, Departmentof Computer Science.
Technical Report 288.Hitoshi Iida and Hidekazu Arita.
1990.
NaturalLanguage Dialog Understanding on a Four-LayerPlan Recognition Model.
Transactions of IPSJ31:6, pp 810-821.K.
Kogura, M. Kume, and H. Iida.
1990.
II-locutionary Act Based Translation of Dialogues.The Third International Conference on Theoreti-cal and Methodological Issues in Machine Trans-lation of Natural Language.L.
Lambert and S. Carberry.
1994.
A Process Modelfor Recognizing Communicative Acts and Model-ing Negotiation Subdialogues.
under review forjournal publication.L.
Lambert.
Recognizing Complex Discourse Acts:A Tripartite Plan-Based Model of Dialogue.
PhDdissertation.
Tech.
Rep. 93-19, Department ofComputer and Information Sciences, University ofDelaware.
1993.L.
Lambert and S. Carberry.
A Tripartite, PlanRecognition Model for Structuring Discourse.Discourse Structure in NL Understanding andGeneration.
AAAI Fall Symposium.
Nov 1991.A.
Lavie and M. Tomita.
1993.
GLR* - An Effi-cient Noise Skipping Parsing Algorithm for Con-text Free Grammars.
in the Proceedings of theThird International Workshop on Parsing Tech-nologies.
Tilburg, The Netherlands.K.
E. Lochbaum.
1994.
Using Collaborative Plansto Model the Intentional Structure of Discourse.PhD dissertation, Harvard University.
TechnicalReport TR-25-94..W. C. Mann and S. A. Thompson.
1986.
Rela-tional Propositions in Discourse.
Technical Re-port RR-83-115.
Information Sciences Institute,Marina del Rey, CA, November.L.
Polanyi.
1988.
A Formal Model of the Structureof Discourse.
Journal of Pragmatics 12, pp 601-638.C.
P. Ros& 1994.
Plan-Based Discourse Pro-cessor for Negotiation Dialogues.
unpublishedmanuscriptC.
P. Ros~.
1995.
The Structure of Multiple HeadedNegotiations.
unpublished manuscript.R.
Scha and L. Polanyi.
1988.
An Augmented Con-text Free Grammar for Discourse.
Proceedings ofthe 12th International Conference on Computa-tional Linguistics, Budapest.B.
Suhm, L. Levin, N. Coccaro, J. Carbonell, K.Horiguchi, R. Isotani, A. Lavie, L. Mayfield,C.
P. Ros~, C. Van Ess-Dykema, A. Waibel.1994.
Speech-language integration in multi-lingual speech translation system, in WorkingNotes of the Workshop on Integration of NaturalLanguage and Speech Processing, Paul McKevitt(chair).
AAAI-94, Seattle.M.
A. Walker.
Information Redundancy and Re-source Bounds in Dialogue.
PhD Dissertation,Computer and Information Science, University ofPennsylvania.B.
L. Webber.
1991.
Structure and Ostension in theInterpretation of Discourse Deixis.
Language andCognitive Prvcesses, 6(2), pp 107-135.38
