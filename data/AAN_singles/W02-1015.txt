Handling noisy training and testing dataDon BlahetaDepartment of Computer ScienceBrown Universitydpb@cs.brown.eduAbstractIn the eld of empirical natural languageprocessing, researchers constantly deal withlarge amounts of marked-up data; whetherthe markup is done by the researcher orsomeone else, human nature dictates that itwill have errors in it.
This paper will morefully characterise the problem and discusswhether and when (and how) to correct theerrors.
The discussion is illustrated withspecic examples involving function taggingin the Penn treebank.1 Introduction: ErrorsNobody?s perfect.
A cliche, but in the eld of empir-ical natural language processing, we know it to betrue: on a daily basis, we work with large corporacreated by, and often marked up by, humans.
Falli-ble as ever, these humans have made errors.
For theerrors in content, be they spelling, syntax, or some-thing else, we can hope to build more robust systemsthat will be able to handle them.
But what of theerrors in markup?In this paper, we propose a system for cataloguingcorpus errors, and discuss some strategies for dealingwith them as a research community.
Finally, we willpresent an example (function tagging) that demon-strates the appropriateness of our methods.2 An error taxonomy2.1 Type A: Detectable errorsThe easiest errors, which we have dubbed \Type A",are those that can be automatically detected andxed.
These typically come up when there wouldbe multiple reasonable ways of tagging a certain in-teresting situation: the markup guidelines arbitrarilychoose one, and the human annotator unthinkinglyuses the other.hhhXXXX9the dealersbyNP...should go hereThis LGS tag...VBN PP-LGSINstartedVPFigure 1: A function tag error of Type AThe canonical example of this sort of thing is thetreebank?s LGS tag, representing the \logical sub-ject" of a passive construction.
It makes a greatdeal of sense to put this tag on the NP object ofthe ?by?
construction; it makes almost as much senseto tag the PP itself, especially since (given a choice)most other function tags are put there.
The tree-bank guidelines specically choose the former: \Itattaches to the NP object of by and not to the PPnode itself."
(Bies et al, 1995) Nevertheless, in sev-eral cases the annotators put the tag on the PP, asshown in Figure 1.
We can automatically correct thiserror by algorithmically removing the LGS tag fromany such PP and adding it to the object thereof.The unifying feature of all Type A errors is thatthe annotator?s intent is still clear.
In the LGS case,the annotator managed to clearly indicate the pres-ence of a passive construction and its logical subject.Since the transformation from what was marked towhat ought to have been marked is straightforwardand algorithmic, we can easily apply this correctionto all data.2.2 Type B: Fixable errorsNext, we come to the Type B errors, those whichare xable but require human intervention at somepoint in the process.
In theory, this category couldinclude errors that could be found automatically butrequire a human to x; this doesn?t happen in prac-tice, because if an error is suciently systematic thatAssociation for Computational Linguistics.Language Processing (EMNLP), Philadelphia, July 2002, pp.
111-116.Proceedings of the Conference on Empirical Methods in Natural????
(((hhh PPVPADVPhardNPcompanyNPMistag?should be VBNVBDhitPPby ...Figure 2: A part-of-speech error of Type B1an algorithm can detect it and be certain that it isin fact an error, it can usually be corrected with cer-tainty as well.
In practice, the instances of this classof error are all cases where the computer can?t detectthe error for certain.
However, for all Type B errors,once detected, the correction that needs to be madeis clear, at least to a human observer with access tothe annotation guidelines.Certain Type B errors are moderately easy tond.
When annotators misunderstand a complicatedmarkup guideline, they mismark in a somewhat pre-dictable way.
While not being totally systematicallydetectable, an algorithm can leverage these patternsto extract a list of tags or parses that might be incor-rect, which a human can then examine.
Some errorsof this type (henceforth \Type B1") include: VBD / VBN.
Often the past tense form of a verb(VBD) and its past participle (VBN) have thesame form, and thus annotators sometimes mis-take one for the other, as in Figure 2.
Some suchcases are not detectable, which is why this is notType A.1 IN / RB / RP.
There are specic tests and guide-lines for telling these three things apart, but fre-quently a preposition (IN) is marked when anadverb (RB) or particle (PRT) would be moreappropriate.
If an IN is occurring somewhereother than under a PP, it is likely to be a mistag.Occasionally, an extracted list of maybe-errors willbe \perfect", containing only instances that are ac-tually corpus errors.
This happens when the pat-tern is a very good heuristic, though not necessarilyvalid (which is why the errors are Type B1, and notType A).
When ling corrections for these, it is stillbest to annotate them individually, as the correc-tions may later be applied to an expanded or modi-1There is a subclass of this error which is Type A:when we find a VBD whose grandparent is a VP headedby a form of ?have?, we can deterministically retag it asVBN.ed data set, for which the heuristic would no longerbe perfect.Other xable errors are pretty much isolated.Within section 24 of the treebank, for instance, wehave: the word ?long?
tagged as an adjective (JJ) whenclearly used as a verb (VB) the word ?that?
parsed into a noun phrase insteadof heading a subordinate clause, as in Figure 3 a phrase headed by ?about?, as in ?think about?,tagged as a location (LOC)These isolated errors (resulting, presumably, froma typo or a moment of inattention on the part ofthe annotator) are not in any way predictable, andcan be found essentially only by examining the out-put of one?s algorithm, analysing the \errors", andnoticing that the treebank was incorrect, rather than(or in addition to) the algorithm.
We will call theseType B2.2.3 Type C: Systematic inconsistencySometimes, there is a construction that the markupguidelines writers didn?t think about, didn?t writeup, or weren?t clear about.
In these cases, annota-tors are left to rely on their own separate intuitions.This leaves us with markup that is inconsistent andtherefore clearly partially in error, but with no obvi-ous correction.
There is really very little to be doneabout these, aside from noting them and perhapscontrolling for them in the evaluation.Some Type C errors in the treebank include: ?ago?.
English?s sole postposition seems to havegiven annotators some diculty.
Lacking apostposition tag, many tagged such occurrencesof ?ago?
as a preposition (IN); others used theadverb tag (RB) exclusively.2 Since some occur-rences really are adverbs, this just makes a bigmess. ADVP-MNR.
The MNR tag is meant to be ap-plied to constituents denoting manner or instru-ment.
Some annotators (but not all) seemedto decide that any adverbial phrase (ADVP)headed by an ?-ly?
word must get a MNR tag, ap-plying it to words like ?suddenly?, ?significantly?,and ?clearly?.2In particular, the annotators of sections 05, 09, 12,17, 20, and 24 used IN sometimes, while the others taggedall occurrences of ?ago?
as adverbs.??
?hhhPP PP*NONE*0SNP VPwere ...SBARDT NNSthat subsidiesshould be??
?hhh PPIN SNP VPwere ...SBARthatNNSsubsidiesFigure 3: A parse error of Type B2The hallmark of a Type C error is that even whatought to be correct isn?t always clear, and as a result,any plan to correct a group of Type C errors willhave to rst include discussion on what the correctmarkup guideline should be.3 tsedIn order to eect these changes in some communi-cable way, we have implemented a program calledtsed, by analogy with and inspired by the alreadyprevalent tgrep search program.3 It takes a searchpattern and a replacement pattern, and after nd-ing the constituent(s) that match the search pattern,modies them and prints the result.
For those al-ready familiar with tgrep search syntax, this shouldbe moderately intuitive.To the basic pattern-matching syntax of tgrep, wehave added a few extra restriction patterns (for spec-ifying sentence number and head word), as well as away of marking nodes for later reference in the re-placement pattern (by simply wrapping a constituentin square brackets instead of parentheses).The replacement syntax is somewhat more com-plicated, because wherever possible we want to beable to construct the new trees by reference to theold tree, in order to preserve modiers and structurewe may not know about when we write the pattern.For full details of the program?s abilities, consult theprogram documentation, but here are the main ones: Relabelling.
Constituents can be relabelled withno change to any of their modiers or children. Tagging.
A tag can be added to or removed froma constituent, without changing any modiers orchildren. Reference.
Constituents in the search patterncan be included by reference in the replacementpattern.3tgrep was written by Richard Pito of the Universityof Pennsylvania, and comes with the treebank. Construction.
New structure can be built byspecifying it in the usual S-expression format,e.g.
(NP (NN snork)).
Usually used in combi-nation with Reference patterns.Along with tsed itself, we distribute a Perl pro-gram wsjsed to process treebank change scripts likethe following:{2429#0-b}<<EOFNP $ [ADJP] > (VP / keep) (S \0 \1)NP <<, markets - SBJEOFThis script would make a batch modication to thezeroth sentence of the 29th le in section 24.
Thebatch includes two corrections: the rst matchesa noun phrase (NP) whose sister is an ADJP andwhose parent is a VP headed by the word ?keep?.
Thematched NP node is replaced by a (created) S nodewhose children will be that very NP and its sisterADJP.
The second correction then nds an NP thatends in the word ?markets?
and marks it with the SBJfunction tag.Distributing changes in this form is important fortwo reasons.
First of all, by giving changes in theirminimal, most general forms, they are small and easyto transmit, and easy to merge.
Perhaps more im-portantly, since corpora are usually copyrighted andcan only be used by paying a fee to the controllingbody (usually LDC or ELDA), we need a way to dis-tribute only the changes, in a form that is uselesswithout having bought the original corpus.
Scriptsfor tsed, or for wsjsed, serve this purpose.These programs are available from our website.44 When to correctNow that we have analysed the dierent types oferrors that can occur and how to correct them, wecan discuss when and whether to do so.4http://www.cs.brown.edu/~dpb/tsed/4.1 TrainingIn virtually all empirical NLP work, the training setis going to encompass the vast majority of the data.As such, it is usually impractical for a human (oreven a whole lab of humans) to sit down and revisethe training.
Type A errors can be corrected easilyenough, as can some Type B1errors whose heuristicshave a high yield.
Purely on grounds of practicality,though, it would be dicult to eect signicant cor-rection on a training set of any signicant size (suchas for the treebank).Practicality aside, correcting the training set is abad idea anyway.
After expending an enormous ef-fort to perfect one training set, the net result is justone correct training set.
While it might make certainthings easier and probably will improve the resultsof most algorithms, those improved results will notbe valid for those same algorithms trained on other,non-perfect data; the vast majority of corpora willstill be noisy.
If a user of an algorithm, e.g.
an ap-plication developer, chooses to perfect a training setto improve the results, that would be helpful, but itis important that researchers report results that arelikely to be applicable more generally, to more thanone training set.
Furthermore, robustness to errorsin the training, via smoothing or some other mech-anism, will also make an algorithm robust to sparsedata, the ever-present spectre that haunts nearly ev-ery problem in the eld; thus eliminating all errorsin the training ought not to have as much of an eecton a strong algorithm.4.2 TestingTesting data is another story, however.
In terms ofpracticality, it is more feasible, as the test set is usu-ally at least an order of magnitude smaller than thetraining.
More important, though, is the issue offairness.
We need to continue using noisy trainingdata in order to better model real-world use, but itis unfair and unreasonable to have noise in the goldstandard5, which causes an algorithm to be penalisedwhere it is more correct than the human annotation.As performance on various tasks improves, it be-comes ever more important to be able to correct thetesting data.
A ?mere?
1% improvement on a result of75% is not impressive, as it represents just a 4% re-duction in apparent error, but the same 1% improve-ment on a result of 95% represents a 20% reductionin apparent error!
In the end, a noisy gold standardsets an upper bound of less than 100% on perfor-mance, which is if nothing else counterintuitive.5Sometimes more of a pyrite standard, really.4.3 Ethical considerationsOf course, we cannot simply go about changing thecorpus willy-nilly.
We refer the reader to chapter 7of David Magerman?s thesis (1994) for a cogent dis-cussion of why changing either the training or thetesting data is a bad idea.
However, we believe thatthere are now some changed circumstances that war-rant a modication of this ethical dictum.First, we are not allowed to look at testing data.How to correct it, then?
An initial reaction mightbe to \promise" to forget everything seen while cor-recting the test corpus; this is not reasonable.Another solution exists, however, which is nearlyas good and doesn?t raise any ethical questions.Many research groups already use yet another sec-tion, separate from both the training and testing,as a sort of development corpus.6 When developingan algorithm, we must look at some output for de-bugging, preliminary evaluation, and parameter esti-mation; so this development section is used for test-ing until a piece of work is ready for publication, atwhich point the \true" test set is used.
Since we areall reading this development output already anyway,there is no harm in reading it to perform correctionsthereon.
In publication, then, one can publish theresults of an algorithm on both the unaltered andcorrected versions of the development section, in ad-dition to the results on the unaltered test section.We can then presume that a corrected version of thetest corpus would result in a perceived error reduc-tion comparable to that on the development corpus.Another problem mentioned in that chapter is of aresearcher quietly correcting a test corpus, and pub-lishing results on the modied data (without evennoting that it was modied).
The solution to thisis simple: any results on modied data will need toacknowledge that the data is modied (to be hon-est), and those modications need to be made public(to facilitate comparisons by later researchers).
ForType A errors xed by a simple rule, it may be rea-sonable to publish them directly in the paper thatgives the results.7 For Type B errors, it would bemore reasonable to simply publish them on a web-site, since there are bound to be a large number ofthem.86In the treebank, this is usually section 24.7The rule we used to fix the LGS problem noted insection 2.1 is as follows:{24*-bg}<<EOFNP !- LGS > (PP - LGS) - LGSPP - LGS !
LGSEOF8The 235 corrections made to section 24 are availableat http://www.cs.brown.edu/~dpb/tbfix/.Finally, we would like to note that one of the rea-sons Magerman was ready to dismiss error in thetesting was that the test data had \a consistencyrate much higher than the accuracy rate of state-of-the-art parsers".
This is no longer true.4.4 Practical considerationsAs multiple researchers each begin to impose theirown corrections, there are several new issues thatwill come up.
First of all, even should everyone pub-lish their own corrections, and post comparisons toprevious researchers?
corrected results, there is somedanger that a variety of dierent correction sets willexist concurrently.
To some extent this can be mit-igated if each researcher posted both their own cor-rections by themselves, and a full list of all correc-tions they used (including their own).
Even so, fromtime to time these varied correction sets will need tobe collected and merged for the whole community touse.More dicult to deal with is the fact that, in-evitably, there will be disputes as to what is correct.Sometimes these will be between the treebank ver-sion and a proposed correction; there will probablyalso be cases where multiple competing correctionsare suggested.
There really is no good systematicpolicy for dealing with this.
Disputes will have tobe handled on a case-by-case basis, and researchersshould probably note any disputes to their correc-tions that they know of when publishing results, butbeyond that it will have to be up to each researcher?spersonal sense of ethics.In all cases, a search-and-replace pattern shouldbe made as general as possible (without being toogeneral, of course), so that it interacts well withother modications.
Various researchers are alreadyworking with (deterministically) dierent versions ofcorpora|with new tags added, or empty nodes re-moved, or some tags collapsed, for instance, not tomention other corrections already performed|and itwould be a bad idea to distribute corrections that arespecic to one version of these.
When in doubt, oneshould favour the original form of the corpus, natu-rally.The nal issue is not a practical problem, but anAlgorithm error 44%Parse error 20%Treebank error 18%Type C error 13%Dubious 6%Table 1: Analysis of reported errorsobservation: once a researcher publishes a correctionset, any further corrections by other researchers arelikely to decrease the results of the rst researcher?salgorithm, at least somewhat.
This is due to the factthat that researcher is usually not going to noticecorpus errors when the algorithm errs in the sameway.
This unfortunate consequence is inevitable, andhopefully will prove minor.5 Experimental resultsAs a sort of case study in the meta-algorithms pre-sented in the previous sections, we will look atthe problem of function tagging in the treebank.Blaheta and Charniak (2000) describe an algorithmfor marking sentence constituents with function tagssuch as SBJ (for sentence subjects) and TMP (fortemporal phrases).
We trained this algorithm on sec-tions 02{21 of the treebank and ran it on section 24(the development corpus), then analysed the output.First, we printed out every constituent with a func-tion tag error.
We then examined the sentence inwhich each occurred, and determined whether theerror was in the algorithm or in the treebank, orelsewhere, as reported in Table 1.
Of the errors weexamined, less than half were due solely to an algo-rithmic failure in the function tagger itself.
The nextlargest category was parse error: this function tag-ging algorithm requires parsed input, and in thesecases, that input was incorrect and led the functiontagger astray; had the tagger received the treebankparse, it would have given correct output.
In justunder a fth of the reported \errors", the algorithmwas correct and the treebank was denitely wrong.The remainder of cases we have identied either asType C errors|wherein the tagger agreed with manytraining examples, but the \correct" tag agreed withmany others|or at least \dubious", in the cases thatweren?t common enough to be systematic inconsis-tencies but where the guidelines did not clearly pre-fer the treebank tag over the tagger output, or viceversa.Next, we compiled all the noted treebank errorsand their corrections.
The most common correc-tion involved simply adding, removing, or changinga function tag to what the algorithm output (with anet eect of improving our score).
However, it shouldbe noted that when classifying reported errors, weexamined their contexts, and in so doing discoveredother sorts of treebank error.
Mistags and misparsesdid not directly aect us; some function tag correc-tions actually decreased our score.
All correctionswere applied anyway, in the hope of cleaner evalua-tions for future researchers.
In total, we made 235corrections, including about 130 simple retags.Grammatical tags Form/function tags Topicalisation tagsP R F P R F P R FTreebank 96.37% 95.04% 95.70% 81.61% 76.44% 78.94% 96.74% 94.68% 95.70%Fixed 97.08% 95.27% 96.16% 85.75% 77.51% 81.42% 97.85% 95.79% 96.81%False error 19.56% 4.64% 10.70% 22.51% 4.54% 11.78% 34.05% 20.86% 25.81%Table 2: Function tagging results, adjusted for treebank errorFinally, we re-evaluated the algorithm?s output onthe corrected development corpus.
Table 2 showsthe resulting improvements.9 Precision, recall, andF-measure are calculated as in (Blaheta and Char-niak, 2000).
The false error rate is simply the percentby which the error is reduced; in terms of the per-formance on the treebank version (t) and the xedversion (f),False error =f ?
t1.0?
t 100%This is the percentage of the reported errors that aredue to treebank error.The topicalisation result is nice, but since the TPCtag is fairly rare (121 occurrences in section 24), thesenumbers may not be robust.
It is interesting, though,that the false error rate on the two major tag groupsis so similar|roughly 20% in precision and 5% inrecall for each, leading to 10% in F-measure.
Firstof all, this parallelism strengthens our assertion thatthe false error rate, though calculated on a devel-opment corpus, can be presumed to apply equallyto the test corpus, since it indicates that the hu-man missed tag and mistag rates may be roughlyconstant.
Second, the much higher improvement onprecision indicates that the majority of treebank er-ror (at least in the realm of function tagging) is dueto human annotators forgetting a tag.6 ConclusionIn this paper, we have given a new characterisationof the sorts of noise one nds in empirical NLP, anda roadmap for dealing with it in the future.
Formany of the problems in the eld, the state of theart is now suciently advanced that evaluation erroris becoming a signicant factor in reported results;we show that it is correctable within the constraintsof practicality and ethics.Although our examples all came from the Penntreebank, the taxonomy presented is applicable to9We did not run corrections on, nor do we show re-sults for, Blaheta and Charniak?s ?misc?
grouping, bothbecause there were very many of them in the reportederror list and because they are very frequently wrong inthe treebank.any corpus annotation project.
As long as there aretypographical errors, there will be Type B errors;and unclear or counterintuitive guidelines will foreverengender Type A and Type C errors.
Furthermore,we expect that the experimental improvement shownin Section 5 will be reflected in projects on other an-notated corpora|perhaps to a lesser or greater de-gree, depending on the diculty of the annotationtask and the prior performance of the computer sys-tem.An eect of the continuing improvement of thestate of the art is that researchers will begin (orhave begun) concentrating on specic subproblems,and will naturally report results on those subprob-lems.
These subproblems are likely to involve thecomplicated cases, which are presumably also moresubject to annotator error, and are certain to involvesmaller test sets, thus increasing the performance ef-fect of each individual misannotation.
As the sizesof the subproblems decrease and their complexity in-creases, the ability to correct the evaluation corpuswill become increasingly important.ReferencesAnn Bies, Mark Ferguson, Karen Katz, and RobertMacIntyre, 1995.
Bracketing Guidelines for Tree-bank II Style Penn Treebank Project, January.Don Blaheta and Eugene Charniak.
2000.
Assign-ing function tags to parsed text.
In Proceedingsof the 1st Annual Meeting of the North AmericanChapter of the Association for Computational Lin-guistics, pages 234{240.David M. Magerman.
1994.
Natural language pars-ing as statistical pattern recognition.
Ph.D. thesis,Stanford University, February.
