Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 860?865,Baltimore, Maryland, USA, June 23-25 2014. c?2014 Association for Computational Linguistics___________________*Corresponding authorCross-lingual Opinion Analysis via Negative Transfer DetectionLin Gui1,2, Ruifeng Xu1*, Qin Lu2, Jun Xu1, Jian Xu2, Bin Liu1, Xiaolong Wang11Key Laboratory of Network Oriented Intelligent Computation, Shenzhen Graduate School,Harbin Institute of Technology, Shenzhen 5180552Department Of Computing, the Hong Kong Polytechnic Universityguilin.nlp@gmail.com, xuruifeng@hitsz.edu.cn, csluqin@comp.polyu.edu.hk, xujun@hitsz.edu.cn,csjxu@comp.polyu.edu.hk,{bliu,wangxl}@insun.hit.edu.cnAbstractTransfer learning has been used in opin-ion analysis to make use of available lan-guage resources for other resource scarcelanguages.
However, the cumulativeclass noise in transfer learning adverselyaffects performance when more trainingdata is used.
In this paper, we propose anovel method in transductive transferlearning to identify noises through thedetection of negative transfers.
Evalua-tion on NLP&CC 2013 cross-lingualopinion analysis dataset shows that ourapproach outperforms the state-of-the-artsystems.
More significantly, our systemshows a monotonic increase trend in per-formance improvement when more train-ing data are used.1 IntroductionMining opinions from text by identifying theirpositive and negative polarities is an importanttask and supervised learning methods have beenquite successful.
However, supervised methodsrequire labeled samples for modeling and thelack of sufficient training data is the performancebottle-neck in opinion analysis especially for re-source scarce languages.
To solve this problem,the transfer leaning method (Arnold et al, 2007)have been used to make use of samples from aresource rich source language to a resourcescarce target language, also known as cross lan-guage opinion analysis (CLOA).In transductive transfer learning (TTL) wherethe source language has labeled data and the tar-get language has only unlabeled data, an algo-rithm needs to select samples from the unlabeledtarget language as the training data and assignthem with class labels using some estimated con-fidence.
These labeled samples in the target lan-guage, referred to as the transferred samples, alsohave a probability of being misclassified.
Duringtraining iterations, the misclassification introduc-es class noise which accumulates, resulting in aso called negative transfer that affects the classi-fication performance.In this paper, we propose a novel methodaimed at reducing class noise for TTL in CLOA.The basic idea is to utilize transferred sampleswith high quality to identify those negative trans-fers and remove them as class noise to reducenoise accumulation in future training iterations.Evaluations on NLP&CC 2013 CLOA evalua-tion data set show that our algorithm achieves thebest result, outperforming the current state-of-the-art systems.
More significantly, our systemshows a monotonic increasing trend in perfor-mance when more training data are used beatingthe performance degradation curse of most trans-fer learning methods when training data reachescertain size.The rest of the paper is organized as follows.Section 2 introduces related works in transferlearning, cross lingual opinion analysis, and classnoise detection technology.
Section 3 presentsour algorithm.
Section 4 gives performance eval-uation.
Section 5 concludes this paper.2 Related worksTTL has been widely used before the formalconcept and definition of TTL was given in (Ar-nold, 2007).
Wan introduced the co-trainingmethod into cross-lingual opinion analysis (Wan,2009; Zhou et al, 2011), and Aue et al intro-duced transfer learning into cross domain analy-sis (Aue, 2005) which solves similar problems.In this paper, we will use the terms source lan-guage and target language to refer to all crosslingual/domain analysis.Traditionally, transfer learning methods focuson how to estimate the confidence score of trans-ferred samples in the target language or domain(Blitzer et al 2006, Huang et al, 2007; Sugiya-ma et al, 2008, Chen et al 2011, Lu et al, 2011).In some tasks, researchers utilize NLP tools suchas alignment to reduce the bias towards that of860the source language in transfer learning (Meng etal., 2012).
However, detecting misclassificationin transferred samples (referred to as class noise)and reducing negative transfers are still an unre-solved problem.There are two basic methods for class noisedetection in machine learning.
The first is theclassification based method (Brodley and Friedl,1999; Zhu et al 2003; Zhu 2004; Sluban et al,2010) and the second is the graph based method(Zighed et al 2002; Muhlenbach et al 2004;Jiang and Zhou, 2004).
Class noise detection canalso be applied to semi-supervised learning be-cause noise can accumulate in iterations too.
Liemployed Zighed?s cut edge weight statisticmethod in self-training (Li and Zhou, 2005) andco-training (Li and Zhou, 2011).
Chao used Li?smethod in tri-training (Chao et al 2008).
(Fuku-moto et al 2013) used the support vectors to de-tect class noise in semi-supervised learning.In TTL, however, training and testing samplescannot be assumed to have the same distributions.Thus, noise detection methods used in semi-supervised learning are not directly suited inTTL.
Y. Cheng has tried to use semi-supervisedmethod (Jiang and Zhou, 2004) in transfer learn-ing (Cheng and Li, 2009).
His experimentshowed that their approach would work when thesource domain and the target domain share simi-lar distributions.
How to reduce negative trans-fers is still a problem in transfer learning.3 Our ApproachIn order to reduce negative transfers, we pro-pose to incorporate class noise detection intoTTL.
The basic idea is to first select high qualitylabeled samples after certain iterations as indica-tor to detect class noise in transferred samples.We then remove noisy samples that cause nega-tive transfers from the current accumulated train-ing set to retain an improved set of training datafor the remainder of the training phase.
This neg-ative sample reduction process can be repeatedseveral times during transfer learning.
Two ques-tions must be answered in this approach: (1) howto measure the quality of transferred samples,and (2) how to utilize high quality labeled sam-ples to detect class noise in training data.3.1 Estimating Testing ErrorTo determine the quality of the transferredsamples that are added iteratively in the learningprocess, we cannot use training error to estimatetrue error because the training data and the test-ing data have different distributions.
In this work,we employ the Probably Approximately Correct(PAC) learning theory to estimate the errorboundary.
According to the PAC learning theory,the least error boundary ?
is determined by thesize of the training set m and the class noise rate?, bound by the following relation:?
(   )                      ( )In TTL, m increases linearly, yet ?
is multi-plied in each iteration.
This means the signifi-cance of m to performance is higher at the begin-ning of transfer learning and gradually slowsdown in later iterations.
On the contrary, the in-fluence of class noise increases.
That is why per-formance improves initially and gradually falls tonegative transfer when noise accumulation out-performs the learned information as shown inFig.1.
In TTL, transferred samples in both thetraining data and test data have the same distribu-tion.
This implies that we can apply the PACtheory to analyze the error boundary of the ma-chine learning model using transferred data.Figure 1 Negative transfer in the learning processAccording to PAC theorem with an assumedfixed probability ?
(Angluin and Laird, 1988),the least error boundary ?
is given by:?
(   ? )
( (   ) )       ( )where N is a constant decided by the hypothesisspace.
In any iteration during TTL, the hypothe-sis space is the same and the probability ?
isfixed.
Thus the least error boundary is deter-mined by the size of the transferred sample mand the class noise of transferred samples ?.
Ac-cording to (2), we apply a manifold assumptionbased method to estimate ?.
Let T be the numberof iterations to serve as one period.
We then es-timate the least error boundary before and aftereach T to measure the quality of transferred sam-ples during each T. If the least error boundary isreduced, it means that transferred samples usedin this period are of high quality and can improvethe performance.
Otherwise, the transfer learningalgorithm should stop.8613.2 Estimating Class NoiseFor formula (2) to work, we need to know theclass noise rate ?
to calculate the error boundary.Obviously, we cannot use conditional probabili-ties from the training data in the source languageto estimate the noise rate ?
of the transferredsamples because the distribution of source lan-guage is different from that of target language.Consider a KNN graph on the transferredsamples using any similarity metric, for example,cosine similarity, for any two connected vertex(     )and (     ) in the graph from samples toclasses, the edge weight is given by:(     )                         ( )Furthermore, a sign function for the two vertices(     )and (     ), is defined as:{( )According to the manifold assumption, theconditional probability  (  |  ) can be approxi-mated by the frequency of  (     ) which isequal to  (     ).
In opinion annotations, theagreement of two annotators is often no largerthan 0.8.
This means that for the best cases(     )=0.2.
Hence     follows a Bernoullidistribution with p=0.2 for the best cases inmanual annotations.Let      (     )  be the vertices that areconnected to the     vertex, the statistical magni-tude of the     vertex can be defined as:?
( )where j refers to the     vertex that is connectedto the     vertex.From the theory of cut edge statics, we knowthat the expectation of    is:(     )  ?
( )And the variance of    is:(     ) (     )  ?
( )By the Center Limit Theorem (CLT),    fol-lows the normal distribution:(     )(   )                    ( )To detect the noise rate of a sample (     ) ,we can use (8) as the null hypothesis to test thesignificant level.
Let    denotes probability ofthe correct classification for a transferred sample.should follow a normal distribution,??
(    )( )Note that experiments (Li and Zhou, 2011;Cheng and Li, 2009; Brodley and Friedl, 1999)have shown that     is related to the error rate ofthe example (     ), but it does not reflect theground-truth probability in statistics.
Hence weassume the class noise rate of example (     ) is:(  )We take the general significant level of 0.05to reject the null hypothesis.
It means that if    of(     ) is larger than 0.95, the sample will beconsidered as a class noisy sample.
Furthermore,can be used to estimate the average class noiserate of a transferred samples in (2).In our proposed approach, we establish thequality estimate period T to conduct class noisedetection to estimate the class noise rate of trans-ferred samples.
Based on the average class noisewe can get the least error boundary so as to tell ifan added sample is of high quality.
If the newlyadded samples are of high quality, they can beused to detect class noise in transferred trainingdata.
Otherwise, transfer learning should stop.The flow chart for negative transfer is in Fig.2.SLS(labeled)TLS(unlabeled)ClassifierTop kTSperiod 1TSperiod 2TSperiod nKNNgraphEstimate ?i and ?n?n ?
?n-1?Output SLS and TS(period 1 to n-1)NoYesDel te TS?i?
0.95period 1 to n-1InputInputT iterations per periodTransferprocessNegativetransferdetectionFigure 2 Flow charts of negative transfer detectionIn the above flow chart, SLS and TLS refer tothe source and target language samples, respec-tively.
TS refers to the transferred samples.
Let Tdenote quality estimate period T in terms of itera-tion numbers.
The transfer process select k sam-ples in each iteration.
When one period of trans-fer process finishes, the negative transfer detec-tion will estimate the quality by comparing andeither select the new transferred samples or re-move class noise accumulated up to this iteration.4 Experiment4.1 Experiment SettingThe proposed approach is evaluated on theNLP&CC 2013 cross-lingual opinion analysis (in862short, NLP&CC) dataset 1 .
In the training set,there are 12,000 labeled English Amazon.comproducts reviews, denoted by Train_ENG, and120 labeled Chinese product reviews, denoted asTrain_CHN, from three categories, DVD, BOOK,MUSIC.
94,651 unlabeled Chinese products re-views from corresponding categories are used asthe development set, denoted as Dev_CHN.
Inthe testing set, there are 12,000 Chinese productreviews (shown in Table.1).
This dataset is de-signed to evaluate the CLOA algorithm whichuses Train_CHN, Train_ENG and Dev_CHN totrain a classifier for Test_CHN.
The performanceis evaluated by the correct classification accuracyfor each category in Test_CHN2:where c is either DVD, BOOK or MUSIC.Team DVD Book MusicTrain_CHN 40 40 40Train_ENG 4000 4000 4000Dev_CHN 17814 47071 29677Test_CHN 4000 4000 4000Table.1 The NLP&CC 2013 CLOA datasetIn the experiment, the basic transfer learningalgorithm is co-training.
The Chinese word seg-mentation tool is ICTCLAS (Zhang et al 2003)and Google Translator3 is the MT for the sourcelanguage.
The monolingual opinion classifier isSVMlight4, word unigram/bigram features are em-ployed.4.2 CLOA Experiment ResultsFirstly, we evaluate the baseline systemswhich use the same monolingual opinion classi-fier with three training dataset includingTrain_CHN, translated Train_ENG and their un-ion, respectively.DVD Book Music AccuracyTrain_CHN 0.552 0.513 0.500 0.522Train_ENG 0.729 0.733 0.722 0.728Train_CHN+Train_ENG0.737 0.722 0.742 0.734Table.2 Baseline performancesIt can be seen that using the same method, theclassifier trained by Train_CHN are on avergage20% worse than the English counter parts.Thecombined use of Train_CHN and translatedTrain_ENG, however, obtained similar1http://tcci.ccf.org.cn/conference/2013/dldoc/evdata03.zip2http://tcci.ccf.org.cn/conference/2013/dldoc/evres03.pdf3https://translate.google.com4http://svmlight.joachims.org/performance to the English counter parts.
Thismeans the predominant training comes from theEnglish training data.In the second set of experiment, we compareour proposed approach to the official results inNLP&CC 2013 CLOA evaluation and the resultis given in Table 3.
Note that in Table 3, the topperformer of NLP&CC 2013 CLOA evaluationis the HLT-HITSZ system(underscored in thetable), which used the co-training method intransfer learning (Gui et al 2013), proving thatco-training is quite effective for cross-lingualanalysis.
With the additional negative transferdetection, our proposed approach achieves thebest performance on this dataset outperformedthe top system (by HLT-HITSZ) by a 2.97%which translate to 13.1% error reduction im-provement to this state-of-the-art system asshown in the last row of Table 3.Team DVD Book Music AccuracyBUAA 0.481 0.498 0.503 0.494BISTU 0.647 0.598 0.661 0.635HLT-HITSZ 0.777 0.785 0.751 0.771THUIR 0.739 0.742 0.733 0.738SJTU 0.772 0.724 0.745 0.747WHU 0.783 0.770 0.760 0.771Our approach 0.816 0.801 0.786 0.801ErrorReduction0.152 0.072 0.110 0.131Table.3 Performance compares with NLP&CC2013 CLOA evaluation resultsTo further investigate the effectiveness of ourmethod, the third set of experiments evaluate thenegative transfer detection (NTD) compared toco-training (CO) without negative transferdetection as shown in Table.4 and Fig.3 Here, weuse the union of Train_CHN and Train_ENG aslabeled data and Dev_CHN as unlabeled data tobe transferred in the learning algorithms.DVD Book Music MeanNTDBest case 0.816 0.801 0.786 0.801Best period 0.809 0.798 0.782 0.796Mean 0.805 0.795 0.781 0.794COBest case 0.804 0.796 0.783 0.794Best period 0.803 0.794 0.781 0.792Mean 0.797 0.790 0.775 0.787Table.4 CLOA performancesTaking all categories of data, our proposedmethod improves the overall average precision(the best cases) from 79.4% to 80.1% whencompared to the state of the art system whichtranslates to error reduction of 3.40% (p-value?0.01 in Wilcoxon signed rank test).
Alt-hough the improvement does not seem large, our863Figure 3 Performance of negative transfer detection vs. co-trainingalgorithm shows a different behavior in that itcan continue to make use of available trainingdata to improve the system performance.
In otherwords, we do not need to identify the tippingpoint where the performance degradation canoccur when more training samples are used.
Ourapproach has also shown the advantage of stableimprovement.In the most practical tasks, co-training basedapproach has the difficulty to determine when tostop the training process because of the negativetransfer.
And thus, there is no sure way to obtainthe above best average precision.
On the contrary,the performance of our proposed approach keepsstable improvement with more iterations, i.e.
ourapproach has a much better chance to ensure thebest performance.
Another experiment is con-ducted to compare the performance of our pro-posed transfer learning based approach with su-pervised learning.
Here, the achieved perfor-mance of 3-folder cross validation are given inTable 5.DVD Book Music AverageSupervised 0.833 0.800 0.801 0.811Our approach 0.816 0.801 0.786 0.801Table.5 Comparison with supervised learningThe accuracy of our approach is only 1.0%lower than the supervised learning using 2/3 ofTest_CHN.
In the BOOK subset, our approachachieves match result.
Note that the performancegap in different subsets shows positive correla-tion to the size of Dev_CHN.
The more samplesare given in Dev_CHN, a higher precision isachieved even though these samples are unla-beled.
According to the theorem of PAC, weknow that the accuracy of a classifier trainingfrom a large enough training set with confinedclass noise rate will approximate the accuracy ofclassifier training from a non-class noise trainingset.
This experiment shows that our proposednegative transfer detection controls the classnoise rate in a very limited boundary.
Theoreti-cally speaking, it can catch up with the perfor-mance of supervised learning if enough unla-beled samples are available.
In fact, such an ad-vantage is the essence of our proposed approach.5 ConclusionIn this paper, we propose a negative transferdetection approach for transfer learning methodin order to handle cumulative class noise andreduce negative transfer in the process of transferlearning.
The basic idea is to utilize high qualitysamples after transfer learning to detect classnoise in transferred samples.
We take cross lin-gual opinion analysis as the data set to evaluateour method.
Experiments show that our proposedapproach obtains a more stable performance im-provement by reducing negative transfers.
Ourapproach reduced 13.1% errors than the top sys-tem on the NLP&CC 2013 CLOA evaluationdataset.
In BOOK category it even achieves bet-ter result than the supervised learning.
Experi-mental results also show that our approach canobtain better performance when the transferredsamples are added incrementally, which in pre-vious works would decrease the system perfor-mance.
In future work, we plan to extend thismethod into other language/domain resources toidentify more transferred samples.AcknowledgementThis research is supported by NSFC 61203378,61300112, 61370165, Natural Science Founda-tion of GuangDong S2013010014475, MOESpecialized Research Fund for the Doctoral Pro-gram of Higher Education 20122302120070,Open Projects Program of National Laboratoryof Pattern Recognition?Shenzhen FoundationalResearch Funding JCYJ20120613152557576,JC201005260118A, Shenzhen International Co-operation Research FundingGJHZ20120613110641217 and Hong Kong Pol-ytechnic University Project code Z0EP.DVD Book Music864ReferenceAngluin, D., Laird, P. 1988.
Learning from NoisyExamples.
Machine Learning, 2(4): 343-370.Arnold, A., Nallapati, R., Cohen, W. W. 2007.
AComparative Study of Methods for TransductiveTransfer Learning.
In Proc.
7th IEEE ICDM Work-shops, pages 77-82.Aue, A., Gamon, M. 2005.
Customizing SentimentClassifiers to New Domains: a Case Study, In Proc.of t RANLP.Blitzer, J., McDonald, R., Pereira, F. 2006.
DomainAdaptation with Structural Correspondence Learn-ing.
In Proc.
EMNLP, 120-128.Brodley, C. E., Friedl, M. A.
1999.
Identifying andEliminating Mislabeled Training Instances.
Journalof Artificial Intelligence Research, 11:131-167.Chao, D., Guo, M. Z., Liu, Y.,  Li, H. F. 2008.
Partic-ipatory Learning based Semi-supervised Classifica-tion.
In Proc.
of 4th ICNC, pages 207-216.Cheng, Y., Li, Q. Y.
2009.
Transfer Learning withData Edit.
LNAI, pages 427?434.Chen, M., Weinberger, K. Q.,  Blitzer, J. C. 2011.Co-Training for Domain Adaptation.
In Proc.
of23th NIPS.Fukumoto, F., Suzuki, Y., Matsuyoshi, S. 2013.
TextClassification from Positive and Unlabeled Datausing Misclassified Data Correction.
In Proc.
of51st ACL, pages 474-478.Gui, L., Xu, R.,  Xu, J., et al 2013.
A Mixed Modelfor Cross Lingual Opinion Analysis.
In CCIS, 400,pages 93-104.Huang, J., Smola, A., Gretton, A., Borgwardt, K.M.,Scholkopf, B.
2007.
Correcting Sample SelectionBias by Unlabeled Data.
In Proc.
of 19th NIPS,pages 601-608.Jiang, Y., Zhou, Z. H. 2004.
Editing Training Data forkNN Classifiers with Neural Network Ensemble.
InLNCS, 3173,  pages 356-361.Li, M., Zhou, Z. H. 2005.
SETRED: Self-Trainingwith Editing.
In Proc.
of PAKDD, pages 611-621.Li, M., Zhou, Z. H. 2011.
COTRADE: Confident Co-Training With Data Editing.
IEEE Transactions onSystems, Man, and Cybernetics?Part B: Cyber-netics, 41(6):1612-1627.Lu, B., Tang, C. H., Cardie, C., Tsou, B. K. 2011.Joint Bilingual Sentiment Classification with Un-labeled Parallel Corpora.
In Proc.
of 49th ACL,pages 320-330.Meng, X. F., Wei, F. R., Liu, X. H., et al 2012.Cross-Lingual Mixture Model for Sentiment Clas-sification.
In Proc.
of 50th ACL, pages 572-581.Muhlenbach, F., Lallich, S., Zighed, D. A.
2004.Identifying and Handling Mislabeled Instances.Journal of Intelligent Information System, 22(1):89-109.Pan, S. J., Yang, Q.
2010.
A Survey on TransferLearning, IEEE Transactions on Knowledge andData Engineering, 22(10):1345-1360.Sindhwani, V., Rosenberg, D. S. 2008.
An RKHS forMulti-view Learning and Manifold Co-Regularization.
In Proc.
of 25th  ICML, pages 976?983.Sluban, B., Gamberger, D., Lavra, N. 2010.
Advanc-es in Class Noise Detection.
In Proc.19th ECAI,pages 1105-1106.Sugiyama, M.,  Nakajima, S., Kashima, H., Buenau,P.V., Kawanabe, M. 2008.
Direct Importance Es-timation with Model Selection and its Applicationto Covariate Shift Adaptation.
In Proc.
20th NIPS.Wan, X.
2009.
Co-Training for Cross-Lingual Senti-ment Classification, In Proc.
of the 47th AnnualMeeting of the ACL and the 4th IJCNLP of theAFNLP,  235?243.Zhang, H. P., Yu, H. K., Xiong, D. Y., and Liu., Q.2003.
HHMM-based Chinese Lexical AnalyzerICTCLAS.
In 2nd SIGHAN workshop affiliatedwith 41th ACL, pages 184-187.Zhou, X., Wan X., Xiao, J.
2011.
Cross-LanguageOpinion Target Extraction in Review Texts.
InProc.
of IEEE 12th ICDM, pages 1200-1205.Zhu, X. Q., Wu, X. D., Chen, Q. J.
2003.
EliminatingClass Noise in Large Datasets.
In Proc.
of 12thICML, pages 920-927.Zhu, X. Q.
2004.
Cost-guided Class Noise Handlingfor Effective Cost-sensitive Learning In Proc.
of 4thIEEE ICDM,  pages 297-304.Zighed, D. A., Lallich, S., Muhlenbach, F. 2002.Separability Index in Supervised Learning.
In Proc.of PKDD, pages 475-487.865
