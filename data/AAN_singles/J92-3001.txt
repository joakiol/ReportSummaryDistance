Making DATR Work for Speech: LexiconCompilation in SUNDIALFrancois Andry*Cap Gemini InnovationScott McGlashantUniversity of SurreyNick J. Youd~Logica Cambridge Ltd.Norman M. Fraser ~University of SurreySimon Thornton ~Logica Cambridge Ltd.We present DIALEX, an inheritance-based tool that facilitates the rapid construction of linguis-tic knowledge bases.
Simple lexical entries are added to an application-specific DATR lexiconthat inherits morphosyntactic, syntactic, and lexico-semantic constraints from an application-independent set of structured base definitions.
A lexicon generator expands the DATR lexi-con out into a disjunctive normal form lexicon.
This is then encoded either as an accep-tance lexicon (in which the constraining features are bit-encoded for use in pruning wordlattices), or as a full lexicon (which is used for assigning interpretations or for generatingmessages).1.
IntroductionIn this paper we describe DIALEX, a modular inheritance-based tool for the construc-tion of lexicalized grammar knowledge bases.
DIALEX has been developed as partof the SUNDIAL (Speech UNderstanding and DIALogue) project--currently one ofEurope's largest collaborative research projects in speech and language technology, lSUNDIAL's main project goal is to produce four prototype systems that support rela-tively unconstrained telephone dialogs for limited domains in each of English, French,German, and Italian (Peckham 1991).
This paper reports work carried out in the devel-opment of the English and French systems.
These share a common application domain,namely flight enquiries and reservations.
* Cap Gemini Innovation, 118 Rue de Tocqueville, 75017 Paris, France (andry@capsogeti.fr).t Social and Computer Sciences Research Group, University of Surrey, Guildford, Surrey, GU2 5XH, U.K.(norman@soc.surrey.ac.uk; s ott@soc.surrey.ac.uk).
:~ Logica Cambridge Ltd, Betjeman House, 104 Hills Road, Cambridge, CB2 1LQ, U.K.(simont@logcam.co.uk; nic @logcam.co.uk).1 The work reported here was supported in part by the Commission ofthe European Communities apart of ESPRIT project P2218, SUNDIAL.
Partners in the project are Cap Gemini Innovation, CNET,IRISA (France); Daimler-Benz, Siemens, University of Erlangen (Germany); CSELT, Saritel (Italy);Logica Cambridge Ltd, University of Surrey (U.K).
We acknowledge with gratitude the helpfulcomments and suggestions ofthe editors and reviewers of this special issue of Computational Linguisticsand of Lionel Moser.
(~) 1992 Association for Computational LinguisticsComputational Linguistics Volume 18, Number 3The process of writing linguistic knowledge bases has been guided by a numberof design requirements on the SUNDIAL project as a whole......First of all, prototype systems must be capable of understanding speech.Therefore grammars must be appropriate for the purposes of speechprocessing.
For example, they must reflect he fact that input to theparser is a word lattice or graph from which some of the spoken words(typically short words such as function words) may be missing.Each prototype system must be capable of producing speech.
Speechgeneration takes place in two stages.
In the first stage, text is generated.In the second stage, a text-to-speech system outputs the message.Therefore the linguistic knowledge must also be structured appropriatelyfor the purposes of text generation.Each system must run in real time or near real time.
Therefore thelinguistic knowledge must be structured so as to allow rapid access andmanipulation.Portability to new applications hould be simple; work required to writenew linguistic knowledge bases should therefore be kept to a minimum.Duplication of effort must be avoided.
This must be true in respect of thecomponents of each separate prototype system.
For example, the samedialog manager software module has been used in each prototype withminor customizations for each language (Bilange 1991; McGlashan et al1992).
The same principle should apply to the design of tools for theconstruction of knowledge bases, including lexical knowledge bases.Thus, the task of adding a new lexical item should only require theaddition of knowledge that is idiosyncratic to that lexical item and notpredictable from what is already present in the knowledge base.Section 2 of this paper presents an overview of the SUNDIAL DIALEX tool.
Section3 describes the way in which linguistic knowledge is initially expressed in terms ofdeclarative DATR theories.
Section 4 explains how a compact DATR knowledge baseis expanded out into a fully specified lexicon.
Section 5 relates how the lexicon canbe customized for the purposes of real-time speech parsing.
Practical experiences ofconstructing and using DIALEX are recounted in Section 6.
Concluding observationsare drawn in Section 7.2.
Overview of the SystemIn common with contemporary generative theories that are unification based and forwhich information is concentrated in the lexicon (Pollard and Sag 1987; Calder et al1988), we adopt the sign as our basic unit of linguistic representation.
For a given lexi-cal entry, a sign describes the constraints--morphological, syntactic, and semantic--it246Francois Andry et al Making DATR Work for Speechintroduces.
The sign for intransitive arrives, for example, is:mor :syn :sem :root : arrive \]form :arrives Jmajor : vvform : finhead : tense : presperson : thirdnumber : sgargs : Isyn : I head : \[ maj?r : prep l \ ] case :a tsem : ~ type: time I RestT \]\[ opt: opt \]order : | dir : post J \[ adj: anyr r  as?r:  I , , I case:nomsyn : I neaa : \[ person: thirdL \[ number: sgsem : \[ type:object I RestA \]\[ opt:oblig \]order : | dir : pre?
\[ adj :anytype : amve \]thetime : \[ type: time I RestT \]theagent : \[ type:object I RestA \]The lexical sign for arrives combines yntactic head features that help to determinethe inflected form, with an args list that constrains its environment within the phraseof which it is the head; the sere feature represents the semantic structure that willbe assigned to that phrase.
The sign shows that the verb may optionally be followedby a prepositional phrase whose semantics will fill the semantic role thetime.
2 Theargument preceding the verb is constrained to be third person singular nominative(i.e.
not object-marked), and supplies the filler for the semantic role theagent.In the interests of linguistic parsimony and sensible knowledge ngineering, it isnecessary for lexicalist approaches to factor away at the lexicon-encoding interface asmany as possible of the commonalities between lexical items.
To this end, we adoptthe principles of default inheritance (Gazdar 1987), as embodied in the DATR language(Evans and Gazdar 1989).
Areas where abstractions may be made over the lexicon aremorphosyntax (Gazdar 1990), transitivity (Charniak and McDermott 1985; Flickingeret al 1985; Hudson 1990), and combinations of these leading to lexical rules suchas passive.
To this we have added the area of lexico-semantic relations.
In order togeneralize over semantic roles, it is necessary to tie these to functional-syntactic roles,such as subject, direct object, etc.
These in turn are related to order marked argumentsin the args frame.
Only the latter appear in the final version of the lexicon.2 In our representation of feature structures we follow Prolog conventions, whereby variables areidentified by initial capitals, and a vertical bar introduces the tail of a list.247Computational Linguistics Volume 18, Number 3A major issue for approaches such as ours is whether or not regularities in thelexicon should be expanded out off-line, or remain for lazy evaluation during pars-ing.
We are sympathetic with the latter approach, for reasons of the economies thatcan be achieved in lexicon size.
However, we believe that a precompiled lexicon ismore appropriate to current speech recognition technology.
Parsing typically involvesextremely large lattices of lexical hypotheses with imprecise boundaries, and is thuscomputationally expensive.
Our experience suggests that the trade-off between lexi-con size and the cost of online inference is such as to favor lexicon size, in the caseof application-specific lexicons of the size required in the SUNDIAL systems (around2000 words).
For inflection-impoverished English and (somewhat richer) French, whichform the basis of our work, limited morphological decomposition during parsing isavoided; instead the parser lexicon consists of fully inflected forms.The parser lexicon we have developed has the following two properties...It is indexed by surface forms, i.e.
fully inflected words that are uniqueat the phonological level.
Efficiency of access is achieved by allowingsome internal disjunctions within entries in cases where the surface formcan be derived from a number of morphosyntactic feature combinations.It consists of two separate knowledge bases: an acceptance lexicon and afull lexicon.
The former is designed for efficient parsing.
Only thosefeatures that constrain the ability of a sign to combine are represented.These include syntactic head features and semantic types.
The encodingtechnique uses bit-vectors to achieve conomy of representation a d fastunification.
The full lexicon contains igns with no information missing;the information i  a full lexical entry is therefore a superset of thecorresponding acceptance l xicon entry.Parsing takes place in two phases: lattice parsing using the acceptance l xicon, involvingheuristic search with intensive computation; and structure building, which operates onthe analysis tree produced by the first phase, using term unification to combine theentries from the full lexicon corresponding to the lexical entries found by the firstphase.The lexicon compilation architecture that we present in this paper is outlined inFigure 1.At the lexical encoding interface, a human lexicon builder builds an application-and sublanguage-specific lexicon, using a set of structured base definitions, which gen-eralize over commonalities and provide macros with which to structure ntries (Sec-tion 3).
Both of these are written in DATR; we refer to the output of this as the DATRlexicon.
The lexicon generator then compiles this into a lexicon for which the entriesare directed acyclic graphs (DAGs) indexed by surface forms.
For this a set of closuredefinitions is used.
These constitute a knowledge base forming a set of meta-definitionsto complement the DATR lexicon, as well as rendering explicit what may be implicit inthe latter (Section 4).
The resulting entries are encoded in two ways: for the full lexiconvia Prolog term encoding and for the acceptance l xicon via bit coding (Section 5).3.
Encoding Linguistic Knowledge3.1 DATRDATR is a declarative language for representing inheritance networks that supportmultiple default inheritance.
Knowledge is expressed in DATR in terms of path equa-248Frangois Andry et al Making DATR Work for SpeechLEXICONBUILDERIIII\]ILappl icat ionspecif iclexiconDATR lexiconstructuredbasedefinitionslexicongenerat ion?osure f "-.JTacceptance fulllexicon lexiconFigure 1DIALEX lexicon compilation architecture.tions.
The syntax of paths is a superset of that found in the PATR-II language (Shieber1986).
For example, (1) identifies two different paths in the DAG rooted at Node1 inan inheritance network.
(1) Node1: <syn head case>Node1: <syn head number>The path equations we present in this paper take the following forms:(2) a. Nodel: <> == Node2b.
Nodel: Path1 == Value1c.
Nodel: Path1 == "Path2"d. Nodel: Pathl == Node2:Path2e.
Nodel: Pathl == Node2:<>The form shown in (2a) is the special case in which the path at Node1 is empty.
Thisallows Node1 to inherit all equations available at Node2, except hose incompatible withequations at Node 1.
Two equations are incompatible if they both make assignments othe same path.
The form shown in (2b) is used to assign values to paths, e.g.
<syn headnumber> =-- sg.
Alternatively, a value may be copied from elsewhere in the DAG.
(2c)is used to assign to Path1 whatever value is found for Path2 at the original querynode.
The double quotes are significant here because they indicate that Path2 must beevaluated globally.
If the quotes were not present, Path1 would be evaluated locally249Computational Linguistics Volume 18, Number 3and assigned the value of Path2 at Nodel if such a value existed.
The form shown in(2d) assigns to Node l :Path l  whatever value is found at Node2:Path2.
A special caseof this is (2e), which allows extensions of Pathl to be specified at Node2.
For example,evaluating the DATR theory in (3) yields the theorems for node Ex2 shown in (4).
(3) Exl: <head major> == n<head case> == nom.
(4)Ex2: <syn> == Ex l :<>.Ex2: <syn  head major> = n.Ex2: <syn  head case> = nom.For a more detailed escription of DATR see Evans and Gazdar (1990).3.2 The Linguistic FrameworkLinguistic knowledge is structured in terms of a simple unification categorial grammar(Calder et al 1988) in which featural constraints at the levels of morphology, syntax,and semantics may all occur in a lexical sign.
The basic sign structure of lexical entriesis shown in (5).
(5)morphology : \[...\] \]syntax: \[...\]semantics : \[...\]The basic sign structure of the syntax feature value is shown in (6).
(6) s ntax\[ ea :llar slThe head feature includes attribute-value structures for such things as tense, person,number, and definiteness.
The args feature is stack-valued, with stack position deter-mining the order in which arguments may be combined by functional application.The basic sign structure of the semantics feature value is shown in (7).
(7) semantics :id :< value >type :< value >modus : \[.
:.\]role, :\[...\]Each semantic object has a unique index (id).
The type feature locates the object in asortal hierarchy.
The modus feature specifies a number of constraints imposed on theinterpretation f semantic objects, such as polarity, aspect, and tense.
Semantic roles(such as theagent, thetime, theinstrument) are specified within the inheritance-baseddefinitions for semantic types.The signs are defined in terms of a dual-component DATR lexicon.
The base defini-tions represent an application-independent account of morphosyntax, transitivity, andlexico-semantic constraints.
They define what can be thought of as most of the highernodes of an inheritance hierarchy.
The base definitions as a whole are, of course, lan-guage specific, although significant exchange of definitions has been possible during250Frangois Andry et al Making DATR Work for Speechthe parallel development of our English and French DATR theories.
The application-specific lexicon can be thought of as a collection of lower nodes that hook onto thebottom of the hierarchy defined by the structured base definitions.
Whereas the struc-tured base definitions provide a general morphological, syntactic, and lexico-semanticaccount of a language fragment, the application-specific lexicon provides a vocabu-lary and a task-related lexical semantics.
Ideally, a change of application should onlynecessitate a change of an application-specific lexicon.
Naturally, application-specificlexicons take much less time to construct than the base lexicon.
Much of our discussionin the rest of this section will focus on the structured base definitions.3.3 MorphosyntaxSince the requirements of speech processing in real time rule out online morpholog-ical parsing, a full-form lexicon must be produced.
However, the task of entering allpossible forms of a word into the lexicon by hand would be both time consumingand repetitive.
We therefore provide a subtheory of morphology in the DATR basedefinitions o that the grammar writer need only specify exceptional morphology foreach lexeme, leaving the lexicon generator to expand out all of the regular forms.The surface form of an English verb encodes information relating to finiteness,tense, number, and person.
What is required in the DATR theory is a number ofcondition-action statements hat say things like:IF a verb is finiteTHEN IF it is present tense AND singular AND third per-sonTHEN its form is <root>+sELSE its form is <root>.The desired effect is achieved by means of DATR's evaluable paths.
The following pathequation is included at the VERB node.
(8) VERB: <mor form> == VERB_MOR:<>The VERB_MOR node looks like this:(9) VERB340R: <bse> == "<mor root>"<prp> == ("<mor root>" ing)<psp> == ("<mor root>" ed)<f in> == "< "<syn head tense>" "<syn head number>""<syn head person>" >"<pres> == "<mor root>"<pres sg th ird> == ("<mor root>" s)<past> == "<mor form psp>".251Computational Linguistics Volume 18, Number 3The base, present participle, and past participle forms are immediately available.
If theverb is finite it is necessary to construct an evaluable path consisting of tense, number,and person values.
If the tense is past (last line), the form is copied from the form ofthe past participle.
If the form is present singular third person (second last line), theform is <root> +s.
Otherwise, the present tense form is copied from the root form.Exceptional forms are stated explicitly, thus overriding default forms.
For example,the following entry for hear specifies that it has exceptional past forms.
(10) HEAR: <> == VERB<mor root> == hear<mor form psp> == heard.The evaluable path mechanism is also used to set the value of an agreement feature agrto tps (third person singular) or not_tps.
The path equation shown in (11), augmentedby the information at the V_AGREE node (12) then requires ubject and verb to sharethe same agr feature value.
The subject's agr feature is set by the definitions in (13).
3(11) VERB: <syn args gr_subject syn head agr> ==V_AGREE:< "<syn head tense> .... <syn head number>""<syn head person>" >.
(12) V_AGREE: <pres> == not_tps<pres sg th i rd> == tps.
(13) NOUN: <syn head agr> ==N_AGREE:<agr "<syn head number>" "<syn head person>">.N_AGREE: <agr> == not~ps<agr  sg th i rd> == tps.English verb morphology presents no real problems; noun morphology is even simpler.French morphology is rather more complicated.
However, it can be handled by meansof the same general technique of allowing evaluable paths to act as case statementsthat select he appropriate morphological form.
Instead of a unified account of Frenchverb morphology there are a number of distinct inflectional paradigms from whichdifferent verbs inherit.
A more sophisticated account of subject-verb agreement is alsorequired.3.4 TransitivityConsider the relationship between verbs of different ransitivity.
An intransitive verbtakes a subject only.
A transitive verb takes a subject and an object.
A ditransitive verbtakes a subject and two objects, one direct and the other indirect.
This information3 In a few exceptional c ses (e.g.
am~are~is in the singular of BE) more complex constraints onagreementare stated in the relevant lexical entries.252Francois Andry et al Making DATR Work for Speechis easily expressible in terms of an inheritance hierarchy.
Facts about subjects areassociated with a top node, for example a node called VERB.
Facts about direct objectsare associated with another node, for example, a node called TRANS_V.
By saying thatTRANS_V is a VERB, the general information about subjects is inherited at the TRANS_Vnode.
This relationship can be expressed simply in DATR.
A similar treatment can beadopted for ditransitive verbs (DTRANS_V):(14) VERB: <syn head major> == v<syn args gr_subject> == GR_SUBJECT:<>.TRANS_V: <> == VERB<syn args gr_direct> == GR_DIRECT:<>.DTRANS_V: <> == TRANS_V<syn args gr_indirect> == GR_INDIRECT:<>.Entries of the form <syn args gr_subject> == GR_SUBJECT:<> represent a convenientway of packaging up all information relating to an argument type at a single node(part of the information stored at this node can be found in (18) below; notice thatdifferent arguments are identified by unique labels such as gr_subj ect and gr_direct).We have already noted that in our sign representation, arguments are distinguishedby their position in a stack.
This ought o render unique argument labels superfluous.In fact, there are a number of reasons why it is desirable to use unique labels in theDATR theory.
Firstly, they allow individual arguments of a word to be picked out (seeSection 3.4.1 below).
Secondly, they allow classes of argument to be identified andgeneralizations to be made where appropriate.
For example, we show in Section 3.4.2how order and optionality generalizations can be made over argument types, and howa system organized around named arguments can be mapped within DATR into anorder-marked system.
Finally, grammatical relation labels are much easier for grammarwriters to remember and manipulate than positionally encoded argument structures.Consider the following partial DATR entry for English infinitival complementverbs.
(15) INF_COMP_V: <> == VERB<syn args gr_comp> == GR_COMP:<><syn args gr_comp syn args gr_subject> =="Ksyn args gr_subject>"The first line states that an infinitival complement verb inherits from the VERB node,i.e., it is a verb that must have a subject.
The second line introduces a number ofconstraints on the complement.
These constraints-----collected at the GR_COMP node--include the fact that the complement must be the infinitive form of a verb.
The nextline enables the complement to share the subject of the matrix verb, i.e., in a sentencelike Amy wants to fly, Amy is the subject of both want and fly.253Computational Linguistics Volume 18, Number 33.4.1 Unevaluated Path Equations.
Consider the relationship between the semantics ofa verb and the semantics of its subject.
The semantics of the subject must be coindexedwith a semantic role of the verb such as theagent, as shown in (16).
(16) \[syn:\[args:\[gr~ubject:\[sem:Asem: [ theagent:A \] \] \] \] \]This reentrancy can be expressed in DATR as follows:(17) <sem theagent> == "<syn args gr_subject sere>".The argument labeled gr_subj ect is typically underspecified in the lexicon and awaitsfull specification at parse time.
Because of this, the constraint is carried over to theDAG-encoding phase of lexicon compilation, where it becomes a reentrancy, as de-scribed in Section 4.3.4.2 Argument Order and Optionality.
While arguments in the structured base def-initions are identified by grammatical relation labels, such as gr_subj ect, the lexicongeneration process requires arguments encoding order and optionality constraints hatare identified by relative position in an args list.
Two techniques are used to produceDATR theories with arguments structured in this way.The first technique is to define featural constraints of order and optionality foreach grammatical relation label.
Three types of constraint are defined:din indicating whether the argument precedes or follows the functor (pre orpost);adj: indicating whether the argument is adjacent to the functor or not (next orany); andopt: indicating whether the argument is optional or obligatory (opt or oblig).Arguments identified as gr_subject and gr_oblique, for example, inherit the follow-ing ordering constraints:(18) GR_SUBJECT: <order  d i r> == pre<order  adj> == next<order  opt> == oblig.GR_0BLIQUE: <order  d i r> == post<order  ad j> == any<order  opt> == optWhereas the subject is obligatory, and precedes the functor and allows for interven-ing constituents, the oblique argument is optional and may appear in any positionfollowing the functor.The second technique maps arguments identified by relation labels onto argumentsidentified by position in a linked list.
Relative position is encoded in terms of the254Franqois Andry et al Making DATR Work for Speechfeatures first and rest: first identifies the first argument in a list, and rest identifies thelinked list of remaining arguments.Consider part of the base definition for transitive verbs, as shown in (19).
(19) TRANS_V: <> == VERB<syn args gr_direct> == GR_DIRECT:<><syn args> == TVARGS:<>.Part of the collection of nodes devoted to mapp ing  named arguments onto order-marked arguments is shown in (20).
(20) TVARGS: <> == DTVARGS<rest> == DARGS:<>.DTVARGS: <f i r s t> == "<syn args gr_subject>"<rest> == ARGSI :<>.DARGS: <f i r s t> == "<syn args gr_direct>"<rest> == ARGS3:<>.ARGS3: <f i rs t> == "<syn args obl iquel>"<rest> == ARGS4: <>.TVARGS inherits the value of <f i r s t> from DTVARGS, which finds it by evaluatingthe path "<syn args gr_subject>."
The <rest> is then inherited from DTVARGSwhere the <f i r s t> argument of <rest> inherits from "<syn args gr_direct>.
"The <rest> of <res t> then inherits from ARGS3, which specifies the position ofoblique arguments within the args list of transitive verbs.3.5 Lexico-Semantic ConstraintsA word lattice is likely to include numerous semantically anomalous but syntacticallywell-formed constructions.
In a system that aims toward real time speech understand-ing it is vital that semantic selectional restrictions be introduced as early as possiblein order to eliminate false phrasal hypotheses atan early stage.Selectional restrictions are typically associated with lexemes.
Each content wordhas a semantic type, and many words specify the semantic types of their arguments.For example, the semantic type of tell is inform and the type of its role theexperiencer(associated with the indirect object) is almost always human in our trial domain.
Thiscan be expressed as follows.
(21) TELL: <> == DTRANS_V<mor root> == tell<sem type> == inform<sem theexperiencer type> == human.255Computational Linguistics Volume 18, Number 3Certain argument types can be assigned efault semantic types.
For example, by de-fault the semantic type of subjects must be sentient (a superclass of human).
Thisworks for a large majority of verbs.
Of course, defaults of this kind can be overriddenfor individual lexemes (such as the verb rain) or word classes (such as copular verbs).3.6 Example: The French Noun PhraseBy way of example, we show how two entries from the French SUNDIAL lexicon, thedeterminer le ('the.MASC') and the common noun passager ('passenger'), are encodedin DATR; to put the following section in context, we also show the DIALEX output.In a simple French noun phrase, we treat the common oun as the head, with thedeterminer as an optional argument.
Therefore, most of the information is associatedwith the common oun.A common oun inherits from the node NOUN:(22) NOUN: <> == WOKD<syn head major> == n<syn head gender> == masc<syn head case> == nom<syn args gr_determiner> == GK_DETERMINER: <><syn args gr_determiner syn head gender> =="<syn head gender>"<syn args> == NOUNARGS: <><syn head number> =="<syn args gr_determiner syn head number>"<syn head def> =="<syn args gr_determiner syn head def>"<sem type> == entity.NOUN itself inherits general word features, such as default morphology, from the nodeWORD:(23) WORD: <mor form> == "<mor root>" .Syntactic and semantic default values such as category (n), gender (mast), case (nom),and semantic type (ent i ty)  are given at the NOUN node.
Some of these values may beoverridden, for example in the definition of passager:(24) Passager: <> == NOUN<mor root> == passager<sem type> == passenger.The number  and definiteness of the noun phrase are specified by the determiner when256Francois Andry et al Making DATR Work for SpeechWORDARGS.../_common-nounsFigure 2Inheritance graph for French common ouns.present, whereas the gender of the determiner is copied from the common noun.Where a feature value is already specified for both noun and determiner at parsetime, the values must be the same if combination is to take place.
The definitions forGR_DETERMINER and NOUNARGS are shown in (25):(25) GR_DETERMINER: <syn head major> == det<order adj> == any<order opt> == opt<order d i r> == pre.NOUNARGS: <f i rs t> == "<syn args gr_determiner>"<rest> == ARGS: <>.The definition of GR_DETERMINER specifies order and optionality information as well asthe syntactic ategory (det).
NOUNARGS defines the mapping of case-marked to order-marked arguments, for simple determiner-noun NPs.The inheritance graph for this set of DATR sentences i illustrated in Figure 2.In fact, common nouns may be more complex than our example suggests; theymay have several obliques, for example.
Fortunately, DATR allows the creation ofintermediate nodes between the NOUN node and the common ouns, and these nodesspecify distinctive properties of each distinct class of nouns.
For example, a RELDAYnode has been created for French in order to describe common grammatical propertiesfor relative day references such as lendemain ('tomorrow') and veille ('the day before').In the same spirit, NPs with genitive postmodifiers such as le numero du vol ('the numberof the flight/the flight number'), where two nouns are combined, use the node GNOUN,which specifies general features of the arguments of the head noun.The definition of the determiner node, DET, is simple in comparison with the NOUNnode, inheriting only from the WORD node.
Example (26) shows the definition of DET,257Computational Linguistics Volume 18, Number 3mor :syn :mor :syn :sem : modusroot : passager \].
form :passager \]headargsroot : le \]form : le \]major: dethead : gender : mascdef : thenumber : sg\[ de f : the  \] \]major : ncase : noragender : mascdef : Anumber : Bsyn :first :ordersere : " type : passenger-\]Figure 3DAG lexicon entries for le and passager.major : detgender" maschead : def : A"number : B\[ opt:  opt \]| dir:  pre |\[ adj : any Jtogether with entries for le and la ('the.FEM').
(26) DET: <> == WORD<syn  head major> == det<syn  head de f> == the<syn  head number> == sg<syn  head gender> == masc<sem modus de f> == the.le: <> == DET<mor  root> == le.la: <> == DET<mor  root> == la<syn  head gender> == fem.The lexical entries for le and passager produced by the DAG-encoding phase of com-pilation (see Section 4) are shown in Figure 3.258Francois Andry et al Making DATR Work for Speech4.
Lexicon Generation4.1 Obtaining the DNF LexiconIn order to generalize across morphological instantiations, a DATR theory makes use ofnodes at the level of the lexeme.
In general, the constraints ina lexeme cannot be simplyrepresented asa union of paths.
This is due to the fact that the sentences making upthe definition of a lexeme for which morphosyntactic variations exist implicitly containdisjunctions.
Because we require the lexicon to be disjoint, our strategy is to cash outall embedded disjunctions that reference ach surface form.
The lexicon thus obtainedcan be described as being in disjunctive normal form (DNF).
This DNF-lexicon willcontain all lexical signs, where a lexical sign incorporates both the surface form andthe corresponding lexeme.In order to govern the expansion of information i the DATR lexicon, it is necessaryto make a closed world of the feature space defined there.
The values that featuresmay take may be implicit in the DATR lexicon; however such implicit knowledge isnotnecessarily complete.
Nothing prevents arbitrary extension of the lexicon by additionalfeatures and values, and this may lead to unwanted interactions with existing rules.We therefore numerate he possible values of features in a knowledge base known asthe closure definitions.
This enumeration is designed to be recursive, to take into accountcategory-valued features uch as the args list.
Figure 4 gives an example of closuredefinitions, for a sign with only syn and mor attributes.
These state the features thatmake up a sign; the definition is recursively introduced at the level of <syn args>.A closure definition takes the form:cdef (Feature, Fields, FieldVals, FCRs ).A complex feature is composed of fields either these are atomic valued, and enumer-ated or declared as open class in FieldVals; or they are complex and their definitionsare to be found elsewhere.cdef(sign,\[syn,mor\],_,\[mor:form=>syn:vform\]).cdef(syn,\[head,args\]  .
.
.
.
).cdef(head,\[major,type,vform,tense,number,person\],\[major==\[n,v,det,prep\],vform==\[f in,bse,prp,psp\],tense==\[pres,past\] ,number==\[sg,pl\],person==\[first,second,third\]\],\[vform:fin => \[tense,person,number\]\]).cdef(args,setof(sign) .... ).cdeI(mor,\[form,root\],\[open(form),open(root)\],_).Figure 4Example closure definitions.259Computational Linguistics Volume 18, Number 3Besides providing closure for DNF lexicon expansion, these definitions have anumber of uses:1. they are used to determine which possible paths the compiler should tryto evaluate in order to build a DAG representation f a lexical sign.
Thesearch proceeds depth-first through the closure definitions, ignoringthose fringes of the search space for which no evaluation is possible.Values of paths, constraints representing unevaluable r entrancies, andconsistent combinations of these are returned;2. they provide a filter on the output of the DATR lexicon.
Only thosefeatures present in the closure definitions are output.
Constraintsincorporating functional labels such as gr_subject are no longer needed;3. they include a complete set of knowledge definitions for our semanticrepresentation language (SIL), which is inheritance based.
Theinheritance hierarchy for semantic types, for example, is used in bitcoding (Section 5), so that semantic selectional restrictions can be testedduring parsing;4. they furnish a set of declarative templates against which bit coding andDAG-term conversion may be carried out.In addition to an enumeration f feature values, the closure definitions contain FeatureCooccurrence Restrictions (FCRs) (Gazdar et al 1985).
In principle these could beencoded in the DATR lexicon, for example, using the feature-value nspec to representnegative occurrence.
Their presence here is not only to restrict he possible featurecombinations that can appear at the head of a sign, but also to detect dependenciesthat govern DNF expansion.The DNF lexicon is obtained as follows.
Those features on which the surface formof a full lexical sign depend, which we shall refer to as its surface form dependencyfeatures, may be derived from the FCRs contained in the closure definitions.
Then foreach pair consisting of a DATR node A and a possible assignment to its unassignedsurface form dependency features ~, generate a new DATR node A ~, which inheritsfrom A and contains the feature assignments in ~.
The DATR theory for A ~ is then usedto produce the set of evaluated and unevaluated constraint sentences that describe it.For example, the base lexical entry for arrive is defined at the DATR node Arrivei,which is underspecified for the paths <syn head tense>, <syn head person>, and<syn head number>.
For the assignment of values pres, third,  sg (respectively) tothese paths, the node Arrivel_presthirdsg is created.4.2 Producing Unevaluated PathsAs we have shown, reentrancies can be specified in DATR using global inheritance; see,for example, (15) in Section 3.4.1.
However, such sentences may not appear directlyin the DAG representation, either because they include paths not derivable within theclosure definitions, or because interaction with higher-ranking exceptions may leadto weaker equivalences being derived.
Any DATR sentence that does not explicitlyintroduce a value is treated as a candidate reentrancy constraint; at the stage whereconstraint sentences are being derived from a DATR theory, all unevaluated constraintsentences are retained.
In the case of Arrivel_presthirdsg, the following constraintsentences are derived by inheritance from Verb:(27) <syn args first sem> = <syn args gr_subject sem>.<sere theagent> = <syn args gr_subject sere>.260Frangois Andry et al Making DATR Work for SpeechDATR inference takes the form of successive reduction of right-hand sides; in (27),neither sentence is further reducible--both would be ignored by a standard DATRtheorem-prover.
By passing both constraints o the DAG-building procedure however,where equality is reflexive as well as transitive (Section 4.3), the two constraints may becombined to derive the reentrancy between <sem theagent> and <syn args firstsere>.4.3 DAG Building and Disjunction OptimizationThe constraint sentences derived for a DATR node A or for an extension of it A ~are of the form Path = Value or Pathl = Path2.
If consistent, hey can be used tobuild a DAG corresponding to A ~.
Our DAG-building procedure is based on onedescribed in Gazdar and Mellish (1989).
It builds DAGs by unification of constraints,so that directionality is irrelevant.
For this to succeed, the input constraints must notcontain inconsistencies.
This property of correctness i only partially guaranteed bythe constraint-derivation stage, which will tolerate an unevaluated constraint whoseleft-hand side is a proper prefix of an evaluated one (but not vice versa), as in (28).
(28) <sem theagent type> = object.<sem theagent> = <syn args gr_subject sem>.This will work so long as a contradictory t pe is not derivable lsewhere.
The form ofencoded DAGs is known as normal form (Bouma 1990); that is, if two DAGs share a com-mon sub-DAG, this is explicitly represented in both, with the exception of unevaluatedsharing sub-DAGs that are represented as Prolog variables.
Once the DAG is built,any remaining unwanted paths are filtered out.
In the case of Arrivel_presthirdsg,this amounts to removing those sub-DAGs introduced at paths containing r_subj ectand gr_oblique 1.Although the closure definitions ensure that the number of surface form depen-dency feature assignments for each lexeme is finite, in practice for languages likeEnglish where a number of morphosyntactic feature combinations map onto a smallerset of surface forms, the DNF lexicon will have more entries than there are distinct sur-face forms.
In cases where a number of entries differ only in a single feature, a phaseof disjunction optimization serves to reduce these, according to the simple equivalence:(41 /~42 /k...4n) V (4~ /N42 A...4n) -~-- (41 V4~)/k42/k...4n.Apart from this optimization, the lexicon produced is in DNF form.5.
Bit Coding5.1 Motivation and RequirementsThe last step toward the production of data structures for efficient parsing and gen-eration is the construction of two separate lexicons: a Prolog term encoding of theDAGs and a compact bit-encoded lexicon.
The motivation for two separate lexiconsis the decision to split the task of parsing into its two aspects: determining grammat-icality and assigning an interpretation.
Since in speech recognition there is also theadded complication of identifying the best-scoring sequence of words from a lattice ofhypotheses, and since an interpretation is only needed for the best sequence, not forevery acceptable one, it is more efficient o separate these tasks.
This involves sepa-rating lexical entries into those features that are constraining (i.e.
which affect a sign's261Computational Linguistics Volume 18, Number 3capacity to combine with others) and those that simply contribute to its eventual in-terpretation.
The former set is used to produce the bit-coded 'acceptance' l xicon, thelatter to form a term-encoded 'full' lexicon.As well as being used in sentence interpretation, the full lexicon is also used insentence generation.
However, we shall concentrate h re on the bit-encoded acceptancelexicon.Since the search space when parsing a typical word hypothesis lattice is potentiallygreat, the acceptance l xicon must be both compact and suitable for processing by ef-ficient low-level operations.
Bit encoding allows unification of feature structures to beperformed by Boolean operations on bit strings, which enables a parser to be imple-mented in an efficient programming language such as C; it also provides a convenientrepresentation f disjunctions and negations of feature values.Two distinct kinds of bit coding are used to represent semantic types and syntactichead features: both produce vectors of bits that can be stored as integers or lists ofintegers.5.2 Semantic Type CodingThe principal semantic type of a lexical entry is a node in a tree-structured (single-inheritance) sortal hierarchy.
Coding for types in the hierarchy is straightforward:?
a terminal node has one unique bit set;?
a nonterminal node is represented by the bitwise Boolean OR of thecodings for the nodes it dominates.This scheme requires as many bits as there are terminal nodes in the tree and, assumingthat every nonterminal node dominates at least two subnodes, assigns a unique bitvector to every node.
(A simple example is given in Figure 5).
The most specific typesare represented by a bit vector containing a single '1,' and the most general by a vectorwith all its bits set.
Unification of two types is performed by bitwise AND; since thehierarchy is tree structured the result of this will be the coding of the more specifictype, or 0 indicating failure if the types are incompatible.
The same coding schemewould also serve if the hierarchy were extended to a multiple-inheritance graph, theonly difference being that bitwise AND could then result in a type distinct from eitherof its arguments.5.3 Syntactic Feature-Value CodingOur approach to the encoding of the feature structures used to represent syntacticcategories i very similar to that proposed in Nakazawa et al (1988) for implementingGPSG-style grammars.A set of features i  represented by a bit vector in which for every n-valued feature,n + 1 bits are assigned, one associated with each value and one bit indicating that thefeature is not present.
A value of '0' for a bit means that the feature does not have thecorresponding value; a '1' indicates that the value is a possible one.
If the value of afeature can be specified precisely, the corresponding bit is set, and all the others forthat feature are cleared.
Hence the negation of a feature-value pair can be representedby clearing a bit, and a disjunction of values by setting more than one bit in therepresentation f a feature.
This fact can be utilized to pack lexical entries together:if two entries differ only in one atomic-valued feature, they can be combined into asingle entry by this method.
Unification is again performed by bitwise AND; failureis indicated if all the bits for some feature are turned off, meaning that the structures262Francois Andry et al Making DATR Work for Speech% @QType Bit VectorA 1111111B 1110000C 0001111D 11O000OE 0010000F 0001000Figure 5Bit coding of the semantic type hierarchy.Type Bit VectorG 0000110H OOOOO01I 1O0OO0OJ O100000K 0000100L 0000010being unified have no common value for this feature.
Since this operation only turnsbits off, unification of bit vectors is order-independent (commutative and associative).The bit vector representation is straightforward for encoding flat feature-valuestructures, but presents difficulties when features have categories as values, given therequirement that the possible values for all features can be enumerated in order toproduce bit vectors of finite size.
Although a general solution can be proposed thatuses some pattern of bits to indicate a recursive feature and associates with this featureanother bit vector of the same length (the solution adopted by Nakazawa et al 1988),we have chosen a more ad hoc encoding, specifying in advance which features can berecursive and representing them by pointers to similarly coded structures.
The featuresthat are recursive are the list of arguments of a functor sign and the slash feature usedto handle long-distance dependencies2 (This approach enables the parser to process4 We follow GPSG in the use of the category-valued f ature slash as a propagating device to handleextraction phenomena.
For example in the question 'what did you say?
', the phrase "did you say?'
can263Computational Linguistics Volume 18, Number 3major v ~-~ I major nmajor detmajor *vform finvform bsevform pspvform prp1000100001110001case *case gencase objcase noratense *tense pasttense presvform *Figure 6Sample bit vector for head features major, vform, tense, and case.signs more efficiently, but unfortunately makes it partly dependent on their structure).The bit encoding procedure takes as input a DAG representation f individuallexical entries and is guided in its translation by the closure definitions.
A set ofdeclarations i used to indicate which features are to be included in the acceptancelexicon, and how they are to be encoded: using either of the bit representations di -cussed above, or simply as a list of encodings of their constituents.
If no coding typeis specified for a feature, then it is simply ignored.As a simple example, consider the following partially specified feature structure:(29) \ [head:\[  maj?r : vvform:fin \] \]Assume that the closure definitions pecify values for the head features major, vform,tense and case, and the FCR"case ~ major : nThen if the node head is declared for bit coding, the vector shown in Figure 6 willbe produced.
(The symbol "*' stands for 'not present').
Note that bits have been setfor all values of the unspecified feature tense, indicating that nothing is known aboutits value, but that only the '*' bit is set for the feature case, since the FCR blocks itspresence for entries whose major feature is not n.5.4 Variable SharingAlthough the representation f variables and their instantiation to(more or fully) spec-ified values is straightforward, the implementation f variable sharing or reentrancypresents a serious problem for bit coding schemes, since there is no means of rep-resenting identifiable variables.
We have adopted a two-fold solution, depending onthe type of the variable.
For sign-valued variables, and other large scale structures,sharing is achieved by means of pointers to common data objects.This approach cannot be extended own to the level of bit-coded features, sincethese involve data below the level of the machine word.
Instead a solution based onbe partially characterized, in our notation, assyn : args : \[1slash: first: \[ syn: \[ head: \[ major:n \] \] \] \]indicating that it is a sentence from which a noun phrase has been extracted.264Frangois Andry et al Making DATR Work for Speechthe use of bit masks has been adopted.
The key to this is the recognition that variablesharing between structures i  a limited form of unification, carried out between arestricted set of their features.
If two feature structures represented by bit vectors flland t2 share a variable for the feature ?, a mask # is constructed in which all thebits representing ?
are cleared, and all the rest are set.
The values for q~ in the two bitvectors are unified in the result of the expression:~, A (f12 V ~)Note that a single mask may represent more than one variable shared between twostructures.A disadvantage of this technique is that it requires the construction of masks forall possible feature structures within a sign between which variables may be shared.In practice we can assume that this means only the recursively nested signs of theargs list and slash, and so need relatively few masks.A description of the two-stage parsing procedure can be found in Andry andThornton (1991).6.
Implementation and CoverageDIALEX is implemented in Quintus Prolog; benchmark tests indicate that compilationtime is linear in the size of the lexicon.
Development of very large scale lexicons issomewhat hindered by the current lack of effective debugging tools.
We have, how-ever, succeeded in constructing lexicons that cover a broad range of syntactic phe-nomena in both French and English.
For example, the English DATR lexicon covers alldistinctive l xical forms in our corpus gathered from simulations of flight enquiry dia-logues (Fraser and Gilbert 1991).
Furthermore, one of the major advantages of DATR'sinheritance-based approach is ease of adding new lexical entries.
For example, a largenumber of entries for cities is required in the flight information domain.
With the def-inition of a CITY_PROP node to specify general properties of proper nouns identifyingcities, individual cities such as Paris are simple and quick to define:(30) Par is :  <> == C ITY_PROP<mor  root> == par i s<sem thec i ty  va lue> == par is .Extending the lexicon to include new verbs, especially verbs with idiosyncratic prop-erties like try, takes more time and effort.This paper has been mainly concerned with the definition and compilation oflexicons for understanding.
In fact, SUNDIAL applications are such that a produc-tion lexicon shares a considerable portion with its recognition counterpart.
To thisend, DIALEX has been adapted for compilation of a generation lexicon (Youd andMcGlashan 1992).
This is derived from the same DATR definitions but differs from theparser lexicons in that indexing is based on semantic type and complexity, rather thanthe surface string, and inflection is factored away from the lexical entries.7.
ConclusionIn the design of our lexicon compilation tool, we have shown how linguistic knowl-edge can be arranged in terms of a set of DATR structured base definitions that are265Computational Linguistics Volume 18, Number 3portable across applications.
Knowledge at the levels of morphology, syntax, and se-mantics combines in a single reusable DATR database.
The fact that this knowledgeis expressed in a high-level representation language does not limit its usefulness.
Onthe contrary, it allows the designers of base definitions or application lexicons to thinkclearly about the structural relations that hold between objects in the representationand to maximize generalizations.
Default inheritance allows generalizations to trickledown to specific instances, unless overridden.
As a consequence, very good gener-alization captured uring the design of structured base definitions represents laborsaved during subsequent application-specific work.We have also shown how high-level knowledge can be entered by the lexiconbuilder at the appropriate conceptual level and then compiled into a lower level formappropriate for a chosen application.
The system we describe produces two kinds ofoutput: a term-encoded full lexicon for use in sentence interpretation a d generation,and a lower level bit-encoded acceptance l xicon for use in lattice pruning duringspeech processing.
The modular design of our system makes it particularly easy toexchange xisting coding modules for new ones, thus allowing linguistic knowledgeto be customized for a wide variety of speech or language applications.ReferencesAndry, Francois, and Thornton, Simon.(1991).
"A parser for speech lattices usinga UCG grammar."
In Proceedings, 2ndEuropean Conference on SpeechCommunication a d Technology.
Genova,September 1991, 219-222.Bilange, Eric.
(1991).
"A task independentoral dialogue model."
In Proceedings, 5thMeeting of the European Chapter of theAssociation for Computational Linguistics.Berlin, April 1991, 83--88.Bouma, Gosse.
(1990).
"Defaults inunification grammar."
In Proceedings, 28thAnnual Meeting of the Association forComputational Linguistics.
Pittsburgh, June1990, 165-172.Calder, Jo; Klein, Ewan; and Zeevat, Henk.(1988).
"Unification Categorial Grammar:a consise xtendable grammar for naturallanguage processing."
In Proceedings,COLING-88.
Budapest, August 1988,83-86.Charniak, Eugene, and McDermott, Drew.(1985).
An Introduction to ArtificialIntelligence.
Lawrence Erlbaum Associates.Evans, Roger, and Gazdar, Gerald.
(1989).
"Inference in DATR."
In Proceedings, 4thMeeting of the European Chapter of theAssociation for Computational Linguistics.Manchester, April 1989, 66-71.Evans, Roger, and Gazdar, Gerald, eds.(1990).
The DATR Papers.
Research ReportCSRP 139, School of Cognitive andComputing Science, University of Sussex.Flickinger, Daniel P.; Pollard, Carl J.; andWasow, Thomas.
(1985).
"Structure-sharing  lexicalrepresentation."
In Proceedings, 23rdAnnual Meeting of the Association forComputational Linguistics.
Chicago,262-267.Fraser, Norman M., and Gilbert, G.
Nigel.(1991).
"Effects of system voice quality onuser utterances in speech dialoguesystems."
In Proceedings, 2nd EuropeanConference on Speech Communication a dTechnology.
Genova, September 1991,57-60.Gazdar, Gerald.
(1987).
"Linguisticapplications of default inheritancemechanisms."
In Linguistic Theory andComputer Applications, edited by PeterWhitelock; Harold Somers; Rod Johnson;and Mary McGee Wood.
Academic Press.Gazdar, Gerald.
(1990).
"An introduction toDATR."
In The DATR Papers, edited byRoger Evans and Gerald Gazdar.Research Report CSRP 139, School ofCognitive and Computing Science,University of Sussex: 1-14.Gazdar, Gerald; Klein, Ewan; Pullum,Geoffry; and Sag, Ivan.
(1985).
GeneralizedPhrase Structure Grammar.
HarvardUniversity Press.
Cambridge, MA.Gazdar, Gerald, and Mellish, Chris.
(1989).Natural Language Processing in Prolog.Addison Wesley.Hudson, Richard A.
(1990).
English WordGrammar.
Basil Blackwell.McGlashan, Scott; Fraser, Norman M.;Gilbert, G. Nigel; Bilange, Eric;Heisterkamp, Paul; and Youd, Nick J.(1992).
Dialogue management fortelephone information systems.
InProceedings ofthe 3rd Conference on AppliedNatural Language Processing.
Trento, April:245-246.266Francois Andry et al Making DATR Work for SpeechNakazawa, Tsuneko; Neher, Laura; andHinrichs, Erhard W. (1988).
"Unificationwith disjunctive and negative values forGPSG grammars."
In Proceedings, 8thEuropean Conference on Artificial Intelligence.Munich, August 1990, 467-472.Peckham, Jeremy.
(1991).
"Speechunderstanding and dialogue over thetelephone: an overview of the SUNDIALproject."
In Proceedings, 2nd EuropeanConference on Speech Communication a dTechnology.
Genova, September 1991,1469-1472.Pollard, Carl, and Sag, Ivan A.
(1987).Information-Based Syntax and Semantics.CSLI, Stanford, CA.Shieber, Stuart M. (1986).
An Introduction toUnification-Based Approaches toGrammar.CSLI, Stanford, CA.Youd, Nick J.; and McGlashan, Scott.
(1992).Generating utterances in dialoguesystems.
In Aspects of Automated NaturalLanguage Generation: Proceedings ofthe 6thInternational Workshop on Natural LanguageGeneration, edited by Robert Dale; EduardHovy; Dietmar R6sner; and Olivero Stock.Academic Press, London.267
