Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 374?378,Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational LinguisticsDetection of Agreement and Disagreement in Broadcast ConversationsWen Wang1 Sibel Yaman2y Kristin Precoda1 Colleen Richey1 Geoffrey Raymond31SRI International, 333 Ravenswood Avenue, Menlo Park, CA 94025, USA2IBM T. J. Watson Research Center P.O.Box 218, Yorktown Heights, NY 10598, USA3University of California, Santa Barbara, CA, USAfwwang,precoda,colleeng@speech.sri.com, syaman@us.ibm.com, graymond@soc.ucsb.eduAbstractWe present Conditional Random Fieldsbased approaches for detecting agree-ment/disagreement between speakers inEnglish broadcast conversation shows.
Wedevelop annotation approaches for a varietyof linguistic phenomena.
Various lexical,structural, durational, and prosodic featuresare explored.
We compare the performancewhen using features extracted from au-tomatically generated annotations againstthat when using human annotations.
Weinvestigate the efficacy of adding prosodicfeatures on top of lexical, structural, anddurational features.
Since the training datais highly imbalanced, we explore two sam-pling approaches, random downsamplingand ensemble downsampling.
Overall, ourapproach achieves 79.2% (precision), 50.5%(recall), 61.7% (F1) for agreement detectionand 69.2% (precision), 46.9% (recall), and55.9% (F1) for disagreement detection, on theEnglish broadcast conversation data.1 IntroductionIn this work, we present models for detectingagreement/disagreement (denoted (dis)agreement)between speakers in English broadcast conversationshows.
The Broadcast Conversation (BC) genre dif-fers from the Broadcast News (BN) genre in thatit is more interactive and spontaneous, referring tofree speech in news-style TV and radio programsand consisting of talk shows, interviews, call-inprograms, live reports, and round-tables.
PreviousyThis work was performed while the author was at ICSI.work on detecting (dis)agreements has been focusedon meeting data.
(Hillard et al, 2003), (Galleyet al, 2004), (Hahn et al, 2006) used spurt-levelagreement annotations from the ICSI meeting cor-pus (Janin et al, 2003).
(Hillard et al, 2003) ex-plored unsupervised machine learning approachesand on manual transcripts, they achieved an over-all 3-way agreement/disagreement classification ac-curacy as 82% with keyword features.
(Galley etal., 2004) explored Bayesian Networks for the de-tection of (dis)agreements.
They used adjacencypair information to determine the structure of theirconditional Markov model and outperformed the re-sults of (Hillard et al, 2003) by improving the 3-way classification accuracy into 86.9%.
(Hahn et al,2006) explored semi-supervised learning algorithmsand reached a competitive performance of 86.7%3-way classification accuracy on manual transcrip-tions with only lexical features.
(Germesin and Wil-son, 2009) investigated supervised machine learn-ing techniques and yields competitive results on theannotated data from the AMI meeting corpus (Mc-Cowan et al, 2005).Our work differs from these previous studies intwo major categories.
One is that a different def-inition of (dis)agreement was used.
In the cur-rent work, a (dis)agreement occurs when a respond-ing speaker agrees with, accepts, or disagrees withor rejects, a statement or proposition by a firstspeaker.
Second, we explored (dis)agreement de-tection in broadcast conversation.
Due to the dif-ference in publicity and intimacy/collegiality be-tween speakers in broadcast conversations vs. meet-ings, (dis)agreement may have different character-374istics.
Different from the unsupervised approachesin (Hillard et al, 2003) and semi-supervised ap-proaches in (Hahn et al, 2006), we conducted su-pervised training.
Also, different from (Hillard etal., 2003) and (Galley et al, 2004), our classifica-tion was carried out on the utterance level, insteadof on the spurt-level.
Galley et al extended Hillardet al?s work by adding features from previous spurtsand features from the general dialog context to in-fer the class of the current spurt, on top of fea-tures from the current spurt (local features) used byHillard et al Galley et al used adjacency pairs todescribe the interaction between speakers and the re-lations between consecutive spurts.
In this prelim-inary study on broadcast conversation, we directlymodeled (dis)agreement detection without using ad-jacency pairs.
Still, within the conditional randomfields (CRF) framework, we explored features frompreceding and following utterances to consider con-text in the discourse structure.
We explored a widevariety of features, including lexical, structural, du-rational, and prosodic features.
To our knowledge,this is the first work to systematically investigatedetection of agreement/disagreement for broadcastconversation data.
The remainder of the paper is or-ganized as follows.
Section 2 presents our data andautomatic annotation modules.
Section 3 describesvarious features and the CRF model we explored.Experimental results and discussion appear in Sec-tion 4, as well as conclusions and future directions.2 Data and Automatic AnnotationIn this work, we selected English broadcast con-versation data from the DARPA GALE pro-gram collected data (GALE Phase 1 Release4, LDC2006E91; GALE Phase 4 Release 2,LDC2009E15).
Human transcriptions and manualspeaker turn labels are used in this study.
Also,since the (dis)agreement detection output will beused to analyze social roles and relations of an inter-acting group, we first manually marked soundbitesand then excluded soundbites during annotation andmodeling.
We recruited annotators to provide man-ual annotations of speaker roles and (dis)agreementto use for the supervised training of models.
We de-fined a set of speaker roles as follows.
Host/chairis a person associated with running the discussionsor calling the meeting.
Reporting participant is aperson reporting from the field, from a subcommit-tee, etc.
Commentator participant/Topic participantis a person providing commentary on some subject,or person who is the subject of the conversation andplays a role, e.g., as a newsmaker.
Audience par-ticipant is an ordinary person who may call in, askquestions at a microphone at e.g.
a large presenta-tion, or be interviewed because of their presence at anews event.
Other is any speaker who does not fit inone of the above categories, such as a voice talent,an announcer doing show openings or commercialbreaks, or a translator.Agreements and disagreements are com-posed of different combinations of initiatingutterances and responses.
We reformulated the(dis)agreement detection task as the sequencetagging of 11 (dis)agreement-related labels foridentifying whether a given utterance is initiatinga (dis)agreement opportunity, is a (dis)agreementresponse to such an opportunity, or is neither ofthese, in the show.
For example, a Negative tagquestion followed by a negation response forms anagreement, that is, A: [Negative tag] This is notblack and white, is it?
B: [Agreeing Response]No, it isn?t.
The data sparsity problem is serious.Among all 27,071 utterances, only 2,589 utterancesare involved in (dis)agreement as initiating orresponse utterances, about 10% only among alldata, while 24,482 utterances are not involved.These annotators also labeled shows with a va-riety of linguistic phenomena (denoted languageuse constituents, LUC), including discourse mark-ers, disfluencies, person addresses and person men-tions, prefaces, extreme case formulations, and dia-log act tags (DAT).
We categorized dialog acts intostatement, question, backchannel, and incomplete.We classified disfluencies (DF) into filled pauses(e.g., uh, um), repetitions, corrections, and falsestarts.
Person address (PA) terms are terms that aspeaker uses to address another person.
Person men-tions (PM) are references to non-participants in theconversation.
Discourse markers (DM) are wordsor phrases that are related to the structure of thediscourse and express a relation between two utter-ances, for example, I mean, you know.
Prefaces(PR) are sentence-initial lexical tokens serving func-tions close to discourse markers (e.g., Well, I think375that...).
Extreme case formulations (ECF) are lexi-cal patterns emphasizing extremeness (e.g., This isthe best book I have ever read).
In the end, we man-ually annotated 49 English shows.
We preprocessedEnglish manual transcripts by removing transcriberannotation markers and noise, removing punctuationand case information, and conducting text normal-ization.
We also built automatic rule-based and sta-tistical annotation tools for these LUCs.3 Features and ModelWe explored lexical, structural, durational, andprosodic features for (dis)agreement detection.
Weincluded a set of ?lexical?
features, including n-grams extracted from all of that speaker?s utter-ances, denoted ngram features.
Other lexical fea-tures include the presence of negation and acquies-cence, yes/no equivalents, positive and negative tagquestions, and other features distinguishing differ-ent types of initiating utterances and responses.
Wealso included various lexical features extracted fromLUC annotations, denoted LUC features.
These ad-ditional features include features related to the pres-ence of prefaces, the counts of types and tokensof discourse markers, extreme case formulations,disfluencies, person addressing events, and personmentions, and the normalized values of these countsby sentence length.
We also include a set of featuresrelated to the DAT of the current utterance and pre-ceding and following utterances.We developed a set of ?structural?
and ?dura-tional?
features, inspired by conversation analysis,to quantitatively represent the different participationand interaction patterns of speakers in a show.
Weextracted features related to pausing and overlapsbetween consecutive turns, the absolute and relativeduration of consecutive turns, and so on.We used a set of prosodic features includingpause, duration, and the speech rate of a speaker.
Wealso used pitch and energy of the voice.
Prosodicfeatures were computed on words and phoneticalignment of manual transcripts.
Features are com-puted for the beginning and ending words of an ut-terance.
For the duration features, we used the aver-age and maximum vowel duration from forced align-ment, both unnormalized and normalized for vowelidentity and phone context.
For pitch and energy, wecalculated the minimum, maximum, range, mean,standard deviation, skewness and kurtosis values.
Adecision tree model was used to compute posteriorsfrom prosodic features and we used cumulative bin-ning of posteriors as final features , similar to (Liu etal., 2006).As illustrated in Section 2, we reformulated the(dis)agreement detection task as a sequence taggingproblem.
We used the Mallet package (McCallum,2002) to implement the linear chain CRF model forsequence tagging.
A CRF is an undirected graph-ical model that defines a global log-linear distribu-tion of the state (or label) sequence E conditionedon an observation sequence, in our case includingthe sequence of sentences S and the correspondingsequence of features for this sequence of sentencesF .
The model is optimized globally over the en-tire sequence.
The CRF model is trained to maxi-mize the conditional log-likelihood of a given train-ing set P (EjS; F ).
During testing, the most likelysequence E is found using the Viterbi algorithm.One of the motivations of choosing conditional ran-dom fields was to avoid the label-bias problem foundin hidden Markov models.
Compared to Maxi-mum Entropy modeling, the CRF model is opti-mized globally over the entire sequence, whereas theME model makes a decision at each point individu-ally without considering the context event informa-tion.4 ExperimentsAll (dis)agreement detection results are based on n-fold cross-validation.
In this procedure, we heldout one show as the test set, randomly held out an-other show as the dev set, trained models on therest of the data, and tested the model on the held-out show.
We iterated through all shows and com-puted the overall accuracy.
Table 1 shows the re-sults of (dis)agreement detection using all featuresexcept prosodic features.
We compared two condi-tions: (1) features extracted completely from the au-tomatic LUC annotations and automatically detectedspeaker roles, and (2) features from manual speakerrole labels and manual LUC annotations when man-ual annotations are available.
Table 1 showed thatrunning a fully automatic system to generate auto-matic annotations and automatic speaker roles pro-376duced comparable performance to the system usingfeatures from manual annotations whenever avail-able.Table 1: Precision (%), recall (%), and F1 (%) of(dis)agreement detection using features extracted frommanual speaker role labels and manual LUC annota-tions when available, denoted Manual Annotation, andautomatic LUC annotations and automatically detectedspeaker roles, denoted Automatic Annotation.AgreementP R F1Manual Annotation 81.5 43.2 56.5Automatic Annotation 79.5 44.6 57.1DisagreementP R F1Manual Annotation 70.1 38.5 49.7Automatic Annotation 64.3 36.6 46.6We then focused on the condition of using fea-tures from manual annotations when available andadded prosodic features as described in Section 3.The results are shown in Table 2.
Adding prosodicfeatures produced a 0.7% absolute gain on F1 onagreement detection, and 1.5% absolute gain on F1on disagreement detection.Table 2: Precision (%), recall (%), and F1 (%) of(dis)agreement detection using manual annotations with-out and with prosodic features.AgreementP R F1w/o prosodic 81.5 43.2 56.5with prosodic 81.8 44.0 57.2DisagreementP R F1w/o prosodic 70.1 38.5 49.7with prosodic 70.8 40.1 51.2Note that only about 10% utterances among alldata are involved in (dis)agreement.
This indicatesa highly imbalanced data set as one class is moreheavily represented than the other/others.
We sus-pected that this high imbalance has played a ma-jor role in the high precision and low recall resultswe obtained so far.
Various approaches have beenstudied to handle imbalanced data for classifications,trying to balance the class distribution in the train-ing set by either oversampling the minority class ordownsampling the majority class.
In this prelimi-nary study of sampling approaches for handling im-balanced data for CRF training, we investigated twoapproaches, random downsampling and ensembledownsampling.
Random downsampling randomlydownsamples the majority class to equate the num-ber of minority and majority class samples.
Ensem-ble downsampling is a refinement of random down-sampling which doesn?t discard any majority classsamples.
Instead, we partitioned the majority classsamples into N subspaces with each subspace con-taining the same number of samples as the minorityclass.
Then we train N CRF models, each basedon the minority class samples and one disjoint parti-tion from the N subspaces.
During testing, the pos-terior probability for one utterance is averaged overthe N CRF models.
The results from these two sam-pling approaches as well as the baseline are shownin Table 3.
Both sampling approaches achieved sig-nificant improvement over the baseline, i.e., train-ing on the original data set, and ensemble downsam-pling produced better performance than downsam-pling.
We noticed that both sampling approachesdegraded slightly in precision but improved signif-icantly in recall, resulting in 4.5% absolute gain onF1 for agreement detection and 4.7% absolute gainon F1 for disagreement detection.Table 3: Precision (%), recall (%), and F1 (%) of(dis)agreement detection without sampling, with randomdownsampling and ensemble downsampling.
Manual an-notations and prosodic features are used.AgreementP R F1Baseline 81.8 44.0 57.2Random downsampling 78.5 48.7 60.1Ensemble downsampling 79.2 50.5 61.7DisagreementP R F1Baseline 70.8 40.1 51.2Random downsampling 67.3 44.8 53.8Ensemble downsampling 69.2 46.9 55.9In conclusion, this paper presents our work ondetection of agreements and disagreements in En-377glish broadcast conversation data.
We explored avariety of features, including lexical, structural, du-rational, and prosodic features.
We experimentedthese features using a linear-chain conditional ran-dom fields model and conducted supervised train-ing.
We observed significant improvement fromadding prosodic features and employing two sam-pling approaches, random downsampling and en-semble downsampling.
Overall, we achieved 79.2%(precision), 50.5% (recall), 61.7% (F1) for agree-ment detection and 69.2% (precision), 46.9% (re-call), and 55.9% (F1) for disagreement detection, onEnglish broadcast conversation data.
In future work,we plan to continue adding and refining features, ex-plore dependencies between features and contextualcues with respect to agreements and disagreements,and investigate the efficacy of other machine learn-ing approaches such as Bayesian networks and Sup-port Vector Machines.AcknowledgmentsThe authors thank Gokhan Tur and Dilek Hakkani-Tu?r for valuable insights and suggestions.
Thiswork has been supported by the Intelligence Ad-vanced Research Projects Activity (IARPA) viaArmy Research Laboratory (ARL) contract num-ber W911NF-09-C-0089.
The U.S. Government isauthorized to reproduce and distribute reprints forGovernmental purposes notwithstanding any copy-right annotation thereon.
The views and conclusionscontained herein are those of the authors and shouldnot be interpreted as necessarily representing the of-ficial policies or endorsements, either expressed orimplied, of IARPA, ARL, or the U.S. Government.ReferencesM.
Galley, K. McKeown, J. Hirschberg, and E. Shriberg.2004.
Identifying agreement and disagreement in con-versational speech: Use of bayesian networks to modelpragmatic dependencies.
In Proceedings of ACL.S.
Germesin and T. Wilson.
2009.
Agreement detectionin multiparty conversation.
In Proceedings of Interna-tional Conference on Multimodal Interfaces.S.
Hahn, R. Ladner, and M. Ostendorf.
2006.
Agree-ment/disagreement classification: Exploiting unla-beled data using constraint classifiers.
In Proceedingsof HLT/NAACL.D.
Hillard, M. Ostendorf, and E. Shriberg.
2003.
De-tection of agreement vs. disagreement in meetings:Training with unlabeled data.
In Proceedings ofHLT/NAACL.A.
Janin, D. Baron, J. Edwards, D. Ellis, D. Gelbart,N.
Morgan, B. Peskin, T. Pfau, E. Shriberg, A. Stolcke,and C. Wooters.
2003.
The ICSI Meeting Corpus.
InProc.
ICASSP, Hong Kong, April.Yang Liu, Elizabeth Shriberg, Andreas Stolcke, DustinHillard, Mari Ostendorf, and Mary Harper.
2006.Enriching speech recognition with automatic detec-tion of sentence boundaries and disfluencies.
IEEETransactions on Audio, Speech, and Language Pro-cessing, 14(5):1526?1540, September.
Special Issueon Progress in Rich Transcription.Andrew McCallum.
2002.
Mallet: A machine learningfor language toolkit.
http://mallet.cs.umass.edu.I.
McCowan, J. Carletta, W. Kraaij, S. Ashby, S. Bour-ban, M. Flynn, M. Guillemot, T. Hain, J. Kadlec,V.
Karaiskos, M. Kronenthal, G. Lathoud, M. Lincoln,A.
Lisowska, W. Post, D. Reidsma, and P. Wellner.2005.
The AMI meeting corpus.
In Proceedings ofMeasuring Behavior 2005, the 5th International Con-ference on Methods and Techniques in Behavioral Re-search.378
