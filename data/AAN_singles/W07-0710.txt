Proceedings of the Second Workshop on Statistical Machine Translation, pages 72?79,Prague, June 2007. c?2007 Association for Computational LinguisticsTraining Non-Parametric Features for Statistical Machine TranslationPatrick Nguyen, Milind Mahajan and Xiaodong HeMicrosoft Corporation1 Microsoft Way,Redmond, WA 98052{panguyen,milindm,xiaohe}@microsoft.comAbstractModern statistical machine translation sys-tems may be seen as using two components:feature extraction, that summarizes informa-tion about the translation, and a log-linearframework to combine features.
In this pa-per, we propose to relax the linearity con-straints on the combination, and hence relax-ing constraints of monotonicity and indepen-dence of feature functions.
We expand fea-tures into a non-parametric, non-linear, andhigh-dimensional space.
We extend empir-ical Bayes reward training of model param-eters to meta parameters of feature genera-tion.
In effect, this allows us to trade awaysome human expert feature design for data.Preliminary results on a standard task showan encouraging improvement.1 IntroductionIn recent years, statistical machine translation haveexperienced a quantum leap in quality thanks to au-tomatic evaluation (Papineni et al, 2002) and error-based optimization (Och, 2003).
The conditionallog-linear feature combination framework (Berger,Della Pietra and Della Pietra, 1996) is remarkablysimple and effective in practice.
Therefore, re-cent efforts (Och et al, 2004) have concentrated onfeature design ?
wherein more intelligent featuresmay be added.
Because of their simplicity, how-ever, log-linear models impose some constraints onhow new information may be inserted into the sys-tem to achieve the best results.
In other words,new information needs to be parameterized care-fully into one or more real valued feature functions.Therefore, that requires some human knowledge andunderstanding.
When not readily available, thisis typically replaced with painstaking experimenta-tion.
We propose to replace that step with automatictraining of non-parametric agnostic features instead,hopefully relieving the burden of finding the optimalparameterization.First, we define the model and the objective func-tion training framework, then we describe our newnon-parametric features.2 ModelIn this section, we describe the general log-linearmodel used for statistical machine translation, aswell as a training objective function and algorithm.The goal is to translate a French (source) sentenceindexed by t, with surface string ft.
Among a set ofKt outcomes, we denote an English (target) a hy-pothesis with surface string e(t)k indexed by k.2.1 Log-linear ModelThe prevalent translation model in modern systemsis a conditional log-linear model (Och and Ney,2002).
From a hypothesis e(t)k , we extract featuresh(t)k , abbreviated hk, as a function of e(t)k and ft. Theconditional probability of a hypothesis e(t)k given asource sentence ft is:pk , p(e(t)k |ft) ,exp[?
?
hk]Zft;?,72where the partition function Zft;?
is given by:Zft;?
=?jexp[?
?
hj ].The vector of parameters of the model ?, gives arelative importance to each feature function compo-nent.2.2 Training CriteriaIn this section, we quickly review how to adjust ?to get better translation results.
First, let us definethe figure of merit used for evaluation of translationquality.2.2.1 BLEU EvaluationThe BLEU score (Papineni et al, 2002) was de-fined to measure overlap between a hypothesizedtranslation and a set of human references.
n-gramoverlap counts {cn}4n=1 are computed over the testset sentences, and compared to the total counts ofn-grams in the hypothesis:cn,(t)k , max.
# of matching n-grams for hyp.
e(t)k ,an,(t)k , # of n-grams in hypothesis e(t)k .Those quantities are abbreviated ck and ak to sim-plify the notation.
The precision ratio Pn for an n-gram order n is:Pn ,?t cn,(t)k?t an,(t)k.A brevity penalty BP is also taken into account, toavoid favoring overly short sentences:BP , min{1; exp(1 ?
ra)},where r is the average length of the shortest sen-tence1, and a is the average length of hypotheses.The BLEU score the set of hypotheses {e(t)k } is:B({e(t)k }) , BP ?
exp( 4?n=114 logPn).1As implemented by NIST mteval-v11b.pl.Oracle BLEU hypothesis: There is no easy wayto pick the set hypotheses from an n-best list thatwill maximize the overall BLEU score.
Instead, tocompute oracle BLEU hypotheses, we chose, foreach sentence independently, the hypothesis with thehighest BLEU score computed for a sentence itself.We believe that it is a relatively tight lower boundand equal for practical purposes to the true oracleBLEU.2.2.2 Maximum LikelihoodUsed in earlier models (Och and Ney, 2002), thelikelihood criterion is defined as the likelihood of anoracle hypothesis e(t)k?
, typically a single referencetranslation, or alternatively the closest match whichwas decoded.
When the model is correct and infi-nite amounts of data are available, this method willconverge to the Bayes error (minimum achievableerror), where we define a classification task of se-lecting k?
against all others.2.2.3 Regularization SchemesOne can convert a maximum likelihood probleminto maximum a posteriori using Bayes?
rule:argmax?
?tp(?|{e(t)k , ft}) = argmax??tpkp0(?
),where p0(?)
is the prior distribution of ?.
Themost frequently used prior in practice is the normalprior (Chen and Rosenfeld, 2000):log p0(?)
, ?||?||22?2 ?
log |?|,where ?2 > 0 is the variance.
It can be thought ofas the inverse of a Lagrange multiplier when work-ing with constrained optimization on the Euclideannorm of ?.
When not interpolated with the likeli-hood, the prior can be thought of as a penalty term.The entropy penalty may also be used:H , ?
1TT?t=1Kt?k=1pk log pk.Unlike the Gaussian prior, the entropy is indepen-dent of parameterization (i.e., it does not depend onhow features are expressed).732.2.4 Minimum Error Rate TrainingA good way of training ?
is to minimize empiricaltop-1 error on training data (Och, 2003).
Comparedto maximum-likelihood, we now give partial creditfor sentences which are only partially correct.
Thecriterion is:argmax??tB({e(t)k?})
: e(t)k?= argmaxe(t)jpj.We optimize the ?
so that the BLEU score of themost likely hypotheses is improved.
For that reason,we call this criterion BLEU max.
This function isnot convex and there is no known exact efficient op-timization for it.
However, there exists a linear-timealgorithm for exact line search against that objec-tive.
The method is often used in conjunction withcoordinate projection to great success.2.2.5 Maximum Empirical Bayes RewardThe algorithm may be improved by giving partialcredit for confidence pk of the model to partially cor-rect hypotheses outside of the most likely hypothe-sis (Smith and Eisner, 2006):1TT?t=1Kt?k=1pk logB({ek(t)}).Instead of the BLEU score, we use its logrithm, be-cause we think it is exponentially hard to improveBLEU.
This model is equivalent to the previousmodel when pk give all the probability mass to thetop-1.
That can be reached, for instance, when ?has a very large norm.
There is no known methodto train against this objective directly, however, ef-ficient approximations have been developed.
Again,it is not convex.It is hoped that this criterion is better suited forhigh-dimensional feature spaces.
That is our mainmotivation for using this objective function through-out this paper.
With baseline features and on ourdata set, this criterion also seemed to lead to resultssimilar to Minimum Error Rate Training.We can normalize B to a probability measureb({e(t)k }).
The empirical Bayes reward also coin-cides with a divergence D(p||b).2.3 Training AlgorithmWe train our model using a gradient ascent methodover an approximation of the empirical Bayes re-ward function.2.3.1 ApproximationBecause the empirical Bayes reward is definedover a set of sentences, it may not be decomposedsentence by sentence.
This is computationally bur-densome.
Its sufficient statistics are r, ?t ck and?t ak.
The function may be reconstructed in a first-order approximation with respect to each of thesestatistics.
In practice this has the effect of commut-ing the expectation inside of the functional, and forthat reason we call this criterion BLEU soft.
This ap-proximation is called linearization (Smith and Eis-ner, 2006).
We used a first-order approximation forspeed, and ease of interpretation of the derivations.The new objective function is:J , log B?P +4?n=114 log?t Ecn,(t)k?t Ean,(t)k,where the average bleu penalty is:log B?P , min{0; 1 ?
rEk,ta1,(t)k}.The expectation is understood to be under the cur-rent estimate of our log-linear model.
Because B?P isnot differentiable, we replace the hard min functionwith a sigmoid, yielding:log B?P ?
u(r ?
Ek,ta1,(t)k )(1?
rEk,ta1,(t)k),with the sigmoid function u(x) defines a soft stepfunction:u(x) , 11 + e?
?x ,with a parameter ?
?
1.2.3.2 Gradients and Sufficient StatisticsWe can obtain the gradients of the objective func-tion using the chain rule by first differentiating withrespect to the probability.
First, let us decomposethe log-precision of the expected counts:log P?n = log Ecn,(t)k ?
log Ean,(t)k .74Each n-gram precision may be treated separately.For each n-gram order, let us define sufficient statis-tics ?
for the precision:?c?
,?t,k(?
?pk)ck; ?a?
,?t,k(?
?pk)ak,where the gradient of the probabilities is given by:?
?pk = pk(hk ?
h?),with:h?
,Kt?j=1pjhj .The derivative of the precision P?n is:?
?log P?n =1T[ ?c?Eck?
?a?Eak]For the length, the derivative of log B?P is:u(r?Ea)[(ra ?
1)[1 ?
u(r ?
Ea)]?
+r(Ea)2]?a1?
,where ?a1?
is the 1-gram component of ?a?.
Finally,the derivative of the entropy is:?
?H =?k,t(1 + log pk)?
?pk.2.3.3 RPropFor all our experiments, we chose RProp (Ried-miller and Braun, 1992) as the gradient ascent al-gorithm.
Unlike other gradient algorithms, it is onlybased on the sign of the gradient components at eachiteration.
It is relatively robust to the objective func-tion, requires little memory, does not require metaparameters to be tuned, and is simple to implement.On the other hand, it typically requires more iter-ations than stochastic gradient (Kushner and Yin,1997) or L-BFGS (Nocedal and Wright, 1999).Using fairly conservative stopping criteria, we ob-served that RProp was about 6 times faster than Min-imum Error Rate Training.3 Adding FeaturesThe log-linear model is relatively simple, and is usu-ally found to yield good performance in practice.With these considerations in mind, feature engineer-ing is an active area of research (Och et al, 2004).Because the model is fairly simple, some of the in-telligence must be shifted to feature design.
Afterhaving decided what new information should go inthe overall score, there is an extra effort involvedin expressing or parameterizing features in a waywhich will be easiest for the model learn.
Experi-mentation is usually required to find the best config-uration.By adding non-parametric features, we proposeto mitigate the parameterization problem.
We willnot add new information, but rather, propose a wayto insulate research from the parameterization.
Thesystem should perform equivalently invariant of anycontinuous invertible transformation of the originalinput.3.1 Existing FeaturesThe baseline system is a syntax based machinetranslation system as described in (Quirk, Menezesand Cherry, 2005).
Our existing feature set includes11 features, among which the following:?
Target hypothesis word count.?
Treelet count used to construct the candidate.?
Target language models, based on the Giga-word corpus (5-gram) and target side of paralleltraining data (3-gram).?
Order models, which assign a probability to theposition of each target node relative to its head.?
Treelet translation model.?
Dependency-based bigram language models.3.2 Re-ranking FrameworkOur algorithm works in a re-ranking framework.In particular, we are adding features which are notcausal or additive.
Features for a hypothesis maynot be accumulating by looking at the English (tar-get) surface string words from the left to the rightand adding a contribution per word.
Word count,for instance, is causal and additive.
This propertyis typically required for efficient first-pass decod-ing.
Instead, we look at a hypothesis sentence as awhole.
Furthermore, we assume that the Kt-best listprovided to us contains the entire probability space.75In particular, the computation of the partition func-tion is performed over all Kt-best hypotheses.
Thisis clearly not correct, and is the subject of furtherstudy.
We use the n-best generation scheme inter-leaved with ?
optimization as described in (Och,2003).3.3 Issues with ParameterizationAs alluded to earlier, when designing a new featurein the log-linear model, one has to be careful to findthe best embodiment.
In general, a set of featuresmust satisfy the following properties, ranked fromstrict to lax:?
Linearity (warping)?
Monotonicity?
Independence (conjunction)Firstly, a feature should be linearly correlated withperformance.
There should be no region were itmatters less than other regions.
For instance, in-stead of a word count, one might consider addingits logarithm instead.
Secondly, the ?goodness?
of ahypothesis associated with a feature must be mono-tonic.
For instance, using the signed difference be-tween word count in the French (source) and En-glish (target) does not satisfy this.
(In that case, onewould use the absolute value instead.)
Lastly, thereshould be no inter-dependence between features.
Asan example, we can consider adding multiple lan-guage model scores.
Whether we should considerratios those of, globally linearly or log-linearly in-terpolating them, is open to debate.
When featuresinteract across dimensions, it becomes unclear whatthe best embodiment should be.3.4 Non-parametric FeaturesA generic solution may be sought in non-parametricprocessing.
Our method can be derived from a quan-tized Parzen estimate of the feature density function.3.4.1 Parzen WindowThe Parzen window is an early empirical kernelmethod (Duda and Hart, 1973).
For an observationhm, we extrapolate probability mass around it witha smoothing window ?(?).
The density function is:p(h) = 1MK?m=1?(h?
hm),assuming ?(?)
is a density function.
Parzen win-dows converge to the true density estimate, albeitslowly, under weak assumptions.3.4.2 Bin FeaturesOne popular way of using continuous features inlog-linear models is to convert a single continuousfeature into multiple ?bin?
features.
Each bin featureis defined as the indicator function of whether theoriginal continuous feature was in a certain range.The bins were selected so that each bin collects anequal share of the probability mass.
This is equiva-lent to the maximum likelihood estimate of the den-sity function subject to a fixed number of rectangulardensity kernels.
Since that mapping is not differen-tiable with respect to the original features, one mayuse sigmoids to soften the boundaries.Bin features are useful to relax the requirementsof linearity and monotonicity.
However, becausethey work on each feature individually, they do notaddress the problem of inter-dependence betweenfeatures.3.4.3 Gaussian Mixture Model FeaturesBin features may be generalized to multi-dimensional kernels by using a Gaussian smoothingwindow instead of a rectangular window.
The directanalogy is vector quantization.
The idea is to weightspecific regions of the feature space differently.
As-suming that we have M Gaussians each with meanvector ?m and diagonal covariance matrix Cm, andprior weight wm.
We will add m new features, eachdefined as the posterior in the mixture model:hm , wmN (h;?m, Cm)?r wrN (h;?r, Cr).It is believed that any reasonable choice of kernelswill yield roughly equivalent results (Povey et al,2004), if the amount of training data and the numberof kernels are both sufficiently large.
We show twomethods for obtaining clusters.
In contrast with bins,lossless representation becomes rapidly impossible.ML kernels: The canonical way of obtaining clus-ter is to use the standard Gaussian mixture training.First, a single Gaussian is trained on the whole dataset.
Then, the Gaussian is split into two Gaussians,with each mean vector perturbed, and the Gaus-sians are retrained using maximum-likelihood in an76expectation-maximization framework (Rabiner andHuang, 1993).
The number of Gaussians is typicallyincreased exponentially.Perceptron kernels: We also experimented withanother quicker way of obtaining kernels.
Wechose an equal prior and a global covariance matrix.Means were obtained as follows: for each sentencein the training set, if the top-1 candidate was differ-ent from the approximate maximum oracle BLEUhypothesis, both were inserted.
It is a quick wayto bootstrap and may reach the oracle BLEU scorequickly.In the limit, GMMs will converge to the oracleBLEU.
In the next section, we show how to re-estimate these kernels if needed.3.5 Re-estimation Formul?Features may also be trained using the same empir-ical maximum Bayes reward.
Let ?
be the hyper-parameter vector used to generate features.
In thecase of language models, for instance, this could bebackoff weights.
Let us further assume that the fea-ture values are differentiable with respect to ?.
Gra-dient ascent may be applied again but this time withrespect to ?.
Using the chain rule:?
?J = (?
?h)(?hpk)(?pkJ),with ?hpk = pk(1 ?
pk)?.
Let us take the exampleof re-estimating the mean of a Gaussian kernel ?m:?
?mhm = ?wmhm(1 ?
hm)C?1m (?m ?
h),for its own feature, and for other posteriors r 6= m:?
?mhr = ?wrhrhmC?1m (?m ?
h),which is typically close to zero if no two Gaussiansfire simultaneously.4 Experimental ResultsFor our experiments, we used the standard NISTMT-02 data set to evaluate our system.4.1 NIST SystemA relatively simple baseline was used for our exper-iments.
The system is syntactically-driven (Quirk,Menezes and Cherry, 2005).
The system was trainedon 175k sentences which were selected from theNIST training data (NIST, 2006) to cover words insource language sentences of the MT02 develop-ment and evaluation sets.
The 5-gram target lan-guage model was trained on the Gigaword mono-lingual data using absolute discounting smoothing.In a single decoding, the system generated 1000 hy-potheses per sentence whenever possible.4.2 Leave-one-out TrainingIn order to have enough data for training, we gen-erated our n-best lists using 10-fold leave-one-outtraining: base feature extraction models were trainedon 9/10th of the data, then used for decoding theheld-out set.
The process was repeated for all 10parts.
A single ?
was then optimized on the com-bined lists of all systems.
That ?
was used for an-other round of 10 decodings.
The process was re-peated until it reached convergence after 7 iterations.Each decoding generated about 100 hypotheses, andthere was relatively little overlap across decodings.Therefore, there were about 1M hypotheses in total.The combined list of all iterations was used for allsubsequent experiments of feature expansion.4.3 BLEU Training ResultsWe tried training systems under the empirical Bayesreward criterion, and appending either bin or GMMfeatures.
We will find that bin features are es-sentially ineffective while GMM features show amodest improvement.
We did not retrain hyper-parameters.4.3.1 Convexity of the Empirical Bayes RewardThe first question to ask is how many local op-tima does the cost surface have using the standardfeatures.
A complex cost surface indicates that somegain may be had with non-linear features, but it alsoshows that special care should be taken during op-timization.
Non-convexity is revealed by sensitivityto initialization points.
Thus, we decided to initial-ize from all vertices of the unit hypercube, and sincewe had 11 features, we ran 211 experiments.
Thehistogram of BLEU scores on dev data after conver-gence is shown on Figure 1.
We also plotted the his-togram of an example dimension in Figure 2.
Therange of BLEU scores and lambdas is reasonablynarrow.
Even though ?
seems to be bimodal, we see77that this does not seriously affect the BLEU score.This is not definitive evidence but we provisionallypretend that the cost surface is almost convex forpractical purposes.24.8 24.9 25 25.1 25.2 25.3 25.40200400600800BLEU scorenumber of trainedmodelsFigure 1: Histogram of BLEU scores after trainingfrom 211 initializations.
?60 ?40 ?20 00100200300400500600700?
valuenumber of trainedmodelsFigure 2: Histogram of one ?
parameter after train-ing from 211 initializations.4.3.2 Bin FeaturesA log-linear model can be converted into a binfeature model nearly exactly by setting ?
valuesin such a way that scores will be equal.
Equiva-lent weights (marked as ?original?
in Figure 3) havethe shape of an error function (erf): this is becausethe input feature is a cummulative random variable,which quickly converges to a Gaussian (by the cen-tral limit theorem).
After training the ?
weights forthe log-linear model, weights may be converted intobins and re-trained.
On Figure 3, we show that relax-ing the monotonicity constraint leads to rough val-ues for ?.
Surprisingly, the BLEU score and ob-jective on the training set only increases marginally.Starting from ?
= 0, we obtained nearly exactly thesame training objective value.
By varying the num-ber of bins (20-50), we observed similar behavior aswell.0 10 20 30 40 50?1.5?1?0.500.51bin idvalueoriginal weightstrained weightsFigure 3: Values before and after training bin fea-tures.
Monotonicity constraint has been relaxed.BLEU score is virtually unchanged.4.3.3 GMM FeaturesExperiments were carried out with GMM fea-tures.
The summary is shown on Table 1.
Thebaseline was the log-linear model trained with thebaseline features.
The baseline features are includedin all systems.
We trained GMM models using theiterative mixture splitting interleaved with EM re-estimation, split up to 1024 and 16384 Gaussians,which we call GMM-ML-1k and GMM-ML-16k re-spectively.
We also used the ?perceptron?
selec-tion features on the training set to bootstrap quicklyto 300k Gaussians (GMM-PCP-300k), and ran thesame algorithm on the development set (GMM-PCP-2k).
Therefore, GMM-PCP-300k had 300kfeatures, and was trained on 175k sentences (eachwith about 700 hypotheses).
For all experiments but?unreg?
(unregularized), we chose a prior Gaussianprior with variance empirically by looking at the de-velopment set.
For all but GMM-PCP-300k, regu-larization did not seem to have a noticeably positiveeffect on development BLEU scores.
All systemswere seeded with the baseline log-linear model, and78all additional weights set to zero, and then trainedwith about 50 iterations, but convergence in BLEUscore, empirical reward, and development BLEUscore occurred after about 30 iterations.
In that set-ting, we found that regularized empirical Bayes re-ward, BLEU score on training data, and BLEU scoreon development and evaluation to be well corre-lated.
Cursory experiments revealed that using mul-tiple initializations did not significantly alter the fi-nal BLEU score.System Train Dev EvalOracle 14.10 N/A N/ABaseline 10.95 35.15 25.95GMM-ML-1k 10.95 35.15 25.95GMM-ML-16k 11.09 35.25 25.89GMM-PCP-2k 10.95 35.15 25.95GMM-PCP-300k-unreg 13.00 N/A N/AGMM-PCP-300k 12.11 35.74 26.42Table 1: BLEU scores for GMM features vs the lin-ear baseline, using different selection methods andnumber of kernels.Perceptron kernels based on the training set im-proved the baseline by 0.5 BLEU points.
We mea-sured significance with the Wilcoxon signed ranktest, by batching 10 sentences at a time to producean observation.
The difference was found to be sig-nificant at a 0.9-confidence level.
The improvementmay be limited due to local optima or the fact thatoriginal feature are well-suited for log-linear mod-els.5 ConclusionIn this paper, we have introduced a non-parametricfeature expansion, which guarantees invariance tothe specific embodiment of the original features.Feature generation models, including feature ex-pansion, may be trained using maximum regular-ized empirical Bayes reward.
This may be used asan end-to-end framework to train all parameters ofthe machine translation system.
Experimentally, wefound that Gaussian mixture model (GMM) featuresyielded a 0.5 BLEU improvement.Although this is an encouraging result, furtherstudy is required on hyper-parameter re-estimation,presence of local optima, use of complex originalfeatures to test the effectiveness of the parameteri-zation invariance, and evaluation on a more compet-itive baseline.ReferencesK.
Papineni, S. Roukos, T. Ward, W.-J.
Zhu.
2002.BLEU: a method for automatic evaluation of machinetranslation.
ACL?02.A.
Berger, S. Della Pietra, and V. Della Pietra.
1996.A Maximum Entropy Approach to Natural LanguageProcessing.
Computational Linguistics, vol 22:1, pp.39?71.S.
Chen and R. Rosenfeld.
2000.
A survey of smoothingtechniques for ME models.
IEEE Trans.
on Speech andAudio Processing, vol 8:2, pp.
37?50.R.
O. Duda and P. E. Hart.
1973.
Pattern Classificationand Scene Analysis.
Wiley & Sons, 1973.H.
J. Kushner and G. G. Yin.
1997.
Stochastic Approxi-mation Algorithms and Applications.
Springer-Verlag,1997.National Institute of Standards and Technology.
2006.The 2006 Machine Translation Evaluation Plan.J.
Nocedal and S. J. Wright.
1999.
Numerical Optimiza-tion.
Springer-Verlag, 1999.F.
J. Och.
2003.
Minimum Error Rate Training in Statis-tical Machine Translation.
ACL?03.F.
J. Och, D. Gildea, S. Khudanpur, A. Sarkar, K. Ya-mada, A. Fraser, S. Kumar, L. Shen, D. Smith, K. Eng,V.
Jain, Z. Jin, and D. Radev.
2004.
A Smorgas-bord of Features for Statistical Machine Translation.HLT/NAACL?04.F.
J. Och and H. Ney.
2002.
Discriminative Trainingand Maximum Entropy Models for Statistical MachineTranslation.
ACL?02.D.
Povey, B. Kingsbury, L. Mangu, G. Saon, H. Soltauand G. Zweig.
2004. fMPE: Discriminatively trainedfeatures for speech recognition.
RT?04 Meeting.C.
Quirk, A. Menezes and C. Cherry.
2005.
De-pendency Tree Translation: Syntactically InformedPhrasal SMT.
ACL?05.L.
R. Rabiner and B.-H. Huang.
1993.
Fundamentals ofSpeech Recognition.
Prentice Hall.M.
Riedmiller and H. Braun.
1992.
RPROP: A FastAdaptive Learning Algorithm.
Proc.
of ISCIS VII.D.
A. Smith and J. Eisner.
2006.
Minimum-RiskAnnealing for Training Log-Linear Models.
ACL-COLING?06.79
