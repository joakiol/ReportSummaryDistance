Proceedings of SSST-5, Fifth Workshop on Syntax, Semantics and Structure in Statistical Translation, pages 79?87,ACL HLT 2011, Portland, Oregon, USA, June 2011. c?2011 Association for Computational LinguisticsA Dependency Based Statistical Translation ModelGiuseppe AttardiUniversit?
di PisaDipartimento di Informaticaattardi@di.unipi.itAtanas ChanevUniversit?
di PisaDipartimento di Informaticachanev@di.unipi.itAntonio Valerio Miceli BaroneUniversit?
di PisaDipartimento di Informaticamiceli@di.unipi.itAbstractWe present a translation model based ondependency trees.
The model adopts a tree-to-string approach and extends Phrase-Based translation (PBT) by using the de-pendency tree of the source sentence forselecting translation options and for reor-dering them.
Decoding is done by translat-ing each node in the tree and combining itstranslations with those of its head in alter-native orders with respect to its siblings.Reordering of the siblings exploits a heu-ristic based on the syntactic informationfrom the parse tree which is learned fromthe corpus.
The decoder uses the samephrase tables produced by a PBT systemfor looking up translations of single wordsor of partial sub-trees.
A mathematicalmodel is presented and experimental re-sults are discussed.1 IntroductionSeveral efforts are being made to incorporate syn-tactic analysis into phrase-base statistical transla-tion (PBT) (Och 2002; Koehn et.
al.
2003), whichrepresents the state of the art in terms of robust-ness in modeling local word reordering and effi-ciency in decoding.
Syntactic analysis is meant toimprove some of the pitfalls of PBT:Translation options selection: candidate phrasesfor translation are selected as consecutive n-grams.
This may miss to consider certain syn-tactic phrases if their component words are farapart.Phrase reordering: especially for languageswith different word order, e.g.
subject-verb-object (SVO) and subject-object-verb (SVO)languages, long distance reordering is a prob-lem.
This has been addressed with a distancebased distortion model (Och 2002; Koehn et al2003), lexicalized phrase reordering (Tillmann,2004; Koehn, et.al., 2005; Al-Onaizan and Pa-pineni, 2006), by hierarchical phrase reorderingmodel (Galley and Manning, 2008) or by reor-dering the nodes in a dependency tree (Xu etal., 2009)Movement of translations of fertile words: aword with fertility higher than one can be trans-lated into several words that do not occur con-secutively.
For example, the Italian sentence?Lui partir?
domani?
translates into German as?Er wird morgen abreisen?.
The Italian word?partir??
(meaning ?will leave?)
translates into?wird gehen?
in German, but the infinite ?ab-reisen?
goes to the end of the sentence with amovement that might be quite long.Reordering of phrases is necessary because of dif-ferent word order typologies of languages: consti-tuent word order like SOV for Hindi vs. SVO forEnglish; order of modifiers like noun?adjective forFrench, Italian vs. adjective-noun in English.
Xu etal.
(2009) tackle this issue by introducing a reor-dering approach based on manual rules that areapplied to the parse tree produced by a dependen-cy parser.However the splitting phenomenon mentionedabove requires more elaborate solutions than sim-ple reordering grammatical rules.Several schemes have been proposed for im-proving PBMT systems based on dependencytrees.
Our approach extends basic PBT as de-79scribed in (Koehn et.
al., 2003) with the followingdifferences:we perform tree-to-string translation.
The de-pendency tree of the source language sentenceallows identifying syntactically meaningfulphrases as translation options, instead of n-grams.
However these phrases are then stilllooked up in a Phrase Translation Table (PT)quite similarly to PBT.
Thus we avoid thesparseness problem that other methods basedon treelets suffer (Quirk et al, 2005).reordering of phrases is carried out traversingthe dependency tree and selecting as optionsphrases that are children of each head.
Hence afar away but logically connected portion of aphrase can be included in the reordering.phrase combination is performed by combiningthe translations of a node with those of its head.Hence only phrases that have a syntactic rela-tion are connected.
The Language Model (LM)is still consulted to ensure that the combinationis proper, and the overall score of each transla-tion is carried along.when all the links in the parse tree have beenreduced, the root node contains candidate trans-lations for the whole sentencesalternative visit orderings of the tree may pro-duce different translations so the final transla-tion is the one with the highest score.Some of the benefits of our approach include:1) reordering is based on syntactic phrases ratherthan arbitrary chunks2) computing the future cost estimation can beavoided, since the risk of choosing an easier n-gram is mitigated by the fact that phrases arechosen according to the dependency tree3) since we are translating from tree to string, wecan directly exploit the standard phrase tablesproduced by PBT tools such as giza++ (Ochand Ney, 2000) and Moses (Koehn, 2007)4) integration with the parser: decoding can beperformed incrementally while a dependencyShift/Reduce parser builds the parse tree (At-tardi, 2006).2 The  Dependency Based DecoderWe describe in more detail the approach by pre-senting a simple example.The translation of an input sentence is generatedby reducing the dependency tree one link at a time,i.e.
merging one node with its parent and combin-ing their translations, until a single node remains.Links must be chosen in an order that preservesthe connectivity of the dependency tree.
Sincethere is a one-to-one correspondence betweenlinks and nodes (i.e.
the link between a node andits head), we can use any ordering that corres-ponds to a topological ordering of the nodes of thetree.A sentence is a sequence of words (w1, ?
, wn),so we can use their index to identify words andhence each ordering is a permutation of those in-dexes.Consider for example the dependency tree forthe Italian sentence: Il ragazzo alto (?The tallboy?
).There are only two possible topological orderingsfor this tree: 1-3-2 and 3-1-2.In principle the decoding process should ex-plore all possible topological orderings for gene-rating translations, but their number is too big,being proportional to the factorial of the number ofwords, so we will introduce later a criterion forselecting a subset of these, which conform bestwith the rules of the languages.Given a permutation we obtain a translation bymerging in that order each node with its parent.The initialization step of the decoder createsnodes corresponding to the parse tree and collectstranslations for each individual word from the PT.ragazzoboyaltotallhighIlTheIl   ragazzo   alto80Case 1: Permutation 1-3-2The first merge step is applied to the nodes for w1and its head w2, performing the concatenation ofthe translations of nodes il (the) and ragazzo (boy),both in normal and reverse order.
Hence expansionof this hypothesis reduces the tree to the follow-ing, where we show also the partial translationsassociated to each node.
Each translation has asso-ciated weights (i.e.
the LM weight, the translationmodel weight, etc.)
and a cumulative score.
Thescore is the dot product of the weights for the sen-tence and the vector of tuning parameters for themodel.
The score is used to rank the sentences andalso to limit how many of them are kept accordingto the beam size parameter of the algorithm.The second step merges the node for word w3 (?al-to?)
with that of its head w2 (?ragazzo?)
producinga single node with four translations: ?the boy tall?,?boy the tall?, ?tall the boy?
and ?tall boy the?.Case 2: Permutation 3-1-2The first merge between w3 and w2 generates twotranslation fragments: ?boy tall?
and ?tall boy?.The second one creates four translations: ?the boytall?, ?boy tall the?, ?the tall boy?, ?tall boy the?.When the tree has been reduced to a single rootnode and the results of both permutations are col-lected, the node will contain all eight alternativetranslations ranked according to the languagemodel, so that the best one, possibly ?the tall boy?,can be selected as overall sentence translation.3 Node MergeThe operation of node merge consists of taking allpossible translations for the two nodes and conca-tenating them in either sequential or reverse order,adding them to the translation of the parent nodeand dropping the child.In certain cases though, for example idiomaticphrases, the best translation is not obtained bycombining the individual translations of eachword, but instead a proper translation might befound in the Phrase Translation Table (PT).
Hencebesides performing combination of translations,we also consider the sub-tree rooted at the headnode hri of node ri.
We consider the phrase corres-ponding to the leaves of the sub-tree rooted at hriand all children already merged into it, includingri: if this phrase is present in the PT, then its trans-lations are also added to the node.This is sometimes useful, since it allows the de-coder to exploit phrases that only correspond topartial sub-trees that it will otherwise miss.4 Reordering RulesIn order to restrict the number of permutations toconsider, we introduce a reordering step based onrules that examine the dependency tree of thesource sentence.The rules are dependent on the language pairand they can be learned automatically from thecorpus.We report first a simple set of hand crafted rulesdevised for the pair Italian-English that we used asa baseline.The default ordering is to start numbering theleft children of a node backwards, i.e.
the nodecloser to the head comes first, then continuingwith the right children in sequential order.Special rules handle these cases:1) The head is a verb: move an adverb child tofirst position.
This lets a sequence of VA VMV R be turned into VA VM R V, where VA isthe POS for auxiliary verbs, VM for modals,V for main verb and R for adverbs.2) The head is a noun: move adjectives or prepo-sitions immediately following the head to thebeginning.Il ragazzo altothe boy tallboy the talltall the boytall boy theIl ragazzothe boyboy thealtotallhigh814.1 Learning Reordering RulesIn order to learn the reordering rules we created aword-aligned parallel corpus from 1.3 millionsource sentences selected from the parallel corpus.The corpus is parsed and each parse tree is ana-lyzed using the giza++ word alignments of itstranslation to figure out node movements.For each source-language word, we estimate aunique alignment to a target-language word.
If thesource word is aligned to more than one targetword we select the first one appearing in thealignment file.
If a source word is not aligned toany word, we choose the first alignment in its des-cendants in the dependency tree.
If no alignmentcan be found in the descendants, we assume thatthe word stays in its original position.We reorder the source sentence according tothis alignment, putting it in target-language order.We produce a training event consisting of a pair(context, offset) for each non-root word.
The con-text of the event consists of a set of features (thePOS tag of a word, its dependency tag and thePOS of its head) extracted for the word and itschildren.
The outcome of the event is the offset ofthe word relative to its parent (negative for wordsthat appear on the left of their parent in target-language order, positive otherwise).We calculate the relative frequency of eachevent conditioned on the context, deriving rules ofthe form:(context, offset, Pr[Offset = offset | Context =context]).During decoding, we compute a reordering posi-tion for each source word by adding to the wordposition to the offset predicted by the most likelyreordering rule matching the word context (or 0 ifno matching context is found).The reordering position drives the childrencombination procedure in the decoder.Our reordering rules are similar to those pro-posed by Xu at al.
(2009), except that we derivethem automatically from the training set, ratherthan being hand-coded.4.2 Beam SearchSearch through the space of hypotheses generatedis performed using beam search that keeps in eachnode the list of the top best translations for thenode.
The score for the translation is computedusing the weights of the individual phrases thatmake up the translation and the overall LM proba-bility of the combination.The scores are computed querying the standardMoses Phrase Table and the LM for the target lan-guage; other weights uses by moses such as thereordering weights or the future cost estimates arediscarded or not computed.5 The ModelA mathematical model of the dependency basedtranslation process can be formulated as follows.Consider the parse of a sentence f of length n.Let R denote all topological ordering of the nodesaccording to the dependency tree.Let fr denote the parse tree along with a consis-tent node ordering r. Each ordering gives rise toseveral different translations.
Let Er denote the setof translations corresponding to fr.
We assign toeach translation er  Er a probability according tothe formula below.
The final translation is the bestresult obtained through combinations over all or-derings.Error!
Objects cannot be created from editing fieldcodes.Where er denotes any of the translations of f ob-tained when nodes are combined according tonode ordering r.The probability of a translation er correspondingto a node ordering r for a phrase f, p(er | f ) is de-fined as:Error!
Objects cannot be created from editing fieldcodes.whereError!
Objects cannot be created from editingfield codes.
andError!
Objects cannot becreated from editing field codes.denote the leafwords from node ri and those of its head node hri,respectively.Error!
Objects cannot be created from edit-ing field codes.is either Error!
Objects cannot82be created from editing field codes.or Error!Objects cannot be created from editing fieldcodes.p(f, e) = pPT(str(f), e) if str(f)  PTstr(f) is the sentence at the leaves of node ripLM is the Language Model probabilitypPT is the Phrase Table probability6 Related WorkYamada and Knight (2001) introduced a syntax-based translation model that incorporated source-language syntactic knowledge within statisticaltranslation.
Many similar approaches are based onconstituent grammars, among which we mention(Chiang, 2005) who introduced hierarchical trans-lation models.The earliest approach based on dependencygrammars is the work by Ashlawi et al (2000),who developed a tree-to-tree translation model,based on middle-out string transduction capable ofphrase reordering.
It translated transcribed spokenutterances from English to Spanish and from Eng-lish to Japanese.
Improvements were reported overa word-for-word baseline.Ambati (2008) presents a survey of other ap-proaches based on dependency trees.Quirk et.
al.
(2005) explore a tree-to-tree ap-proach, called treelet translation, that extracts tree-lets, i.e.
sub-trees, from both source and targetlanguage by means of a dependency parser.
Aword aligner is used to align the parallel corpus.The source dependency is projected onto the targetlanguage sentence in order to extract treelet trans-lation pairs.
Given a foreign input sentence, theirsystem first generates its dependency tree made oftreelets.
These treelets are translated into treeletsof the target language, according to the dependen-cy treelet translation model.
Translated treelets arethen reordered according to a reorder model.The ordering model is trained on the parallelcorpus.
Treelet translation pairs are used for de-coding.
The reordering is done at the treelet levelwhere all the child nodes of a node are allowed allpossible orders.
The results show marginal im-provements in the BLEU score (40.66) in compar-ison with Pharaoh and MSR-MT.
But the treelettranslation algorithm is more than an order ofmagnitude slower.Shen et.
al.
(2008) present a hierarchical ma-chine translation method from string to trees.
Thescheme uses the dependency structure of the targetlanguage to use transfer rules while generating atranslation.
The scheme uses well-formed depen-dency structure which involves fixed and floatingtype structures.
The floating structures allow thetranslation scheme to perform different concatena-tion, adjoining and unification operations still be-ing within the definition of well-formed structures.While decoding the scheme uses the probability ofa word being the root, and also the left-side, right-side generative probabilities.
The number of rulesused varies from 27 M (for a string to dependencysystem) to 140 M (baseline system).
The perfor-mance reached 37.25% for the system with 3-grams, 39.47% for 5-grams.Marcu and Wong (2002) propose a joint- prob-ability model.
The model establishes a correspon-dence between a source phrase and a target phrasethrough some concept.
The reordering is inte-grated into the joint probability model with thehelp of:3) Phrase translation probabilities Error!
Ob-jects cannot be created from editing fieldcodes.
denoting the probability that concept cigenerates the translation Error!
Objects can-not be created from editing field codes.
forthe English and Error!
Objects cannot becreated from editing field codes.
for the for-eign language inputs.4) Distortion probabilities based on absolute po-sitions of the phrases.Decoding uses a hill-climbing algorithm.
Perfor-mance wise the approach records an averageBLEU score of 23.25%, with about 2% of im-provement over the baseline IBM system.Zhang et.
al.
(2007) present a reordering modelthat uses linguistic knowledge to guide bothphrase reordering and translation between linguis-tically correct phrases by means of rules.
Rules areencoded in the form of weighted synchronousgrammar and express transformations on the parsetrees.
They experiment also mixing constituencyand dependency trees achieving some improve-83ments in BLEU score (27.37%) over a baselinesystem (26.16%).Cherry (2008) introduces a cohesion feature in-to a traditional phrase based decoder.
It is imple-mented as a soft constraint which is based on thedependency syntax of the source language.
Hereports a BLEU score improvement on French-English translation.The work by Xu et al (2009) is the closest toour approach.
They perform preprocessing of theforeign sentences by parsing them with a depen-dency parser and applying a set of hand writtenrules to reorder the children of certain nodes.
Thepreprocessing is applied to both the training cor-pus and to the sentences to translate, hence afterreordering a regular hierarchical system can beapplied.
Translation experiments between Englishand five non SVO Asian languages show signifi-cant improvements in accuracy in 4 out of 5 lan-guages.
With respect to our approach the solutionby Xu et al does not require any intervention onthe translation tools, since the sentences are rewrit-ten before being passed to the processing chain: onthe other hand the whole collection has to undergofull parsing with higher performance costs andhigher dependency on the accuracy of the parser.Dyer and Resnik (2010) introduce a translationmodel based on a Synchronous Context FreeGrammar (SCFG).
In their model, translationexamples are stored as a context-free forest.
Theprocess of translation comprise two steps: tree-based reordering and phrase transduction.
Whilereordering is modeled with the context-free forest,the reordered source is transduced into the targetlanguage by a Finite State Transducer (FST).
Theimplemented model is trained on those portions ofthe data which it is able to generate.
An increaseof BLEU score is achieved for Chinese-Englishwhen compared to the phrase based baseline.Our approach is a true tree-to-string model anddiffers from (Xu et al, 2009), which uses treesonly as an intermediate representation to rearrangethe original sentences.
We perform parsing andreordering only on the phrases to be translated.The training collection is kept in the original form,and this has two benefits: training is not subject toparsing errors and our system can share the samemodel of a regular hierarchical system.Another difference is in the selection of transla-tion options: our method exploits the parse tree toselect grammatical phrases as translation options.7 ImplementationThe prototype decoder consists of the followingcomponents:1) A specialized table lookup server, providingan XML-RPC interface for querying both thephrase table and the LM2) A parser engine based on DeSR (DeSR, 2009)3) A reordering algorithm that adds orderingnumbers to the output produced by DeSR inCoNLL-X format.
Before reordering, this stepalso performs a restructuring of the parse tree,converting from the conventions of the ItalianTanl Treebank to a structure that helps theanalysis.
In particular it converts conjunctions,which are represented as chains, where eachconjunct connects to the previous, to a treewhere they are all dependent of the same headword.
Compound verbs are also revised: in thedependency tree each auxiliary of a verb is adirect child of the main verb.
For example in?avrebbe potuto vedere?, both the auxiliary?avrebbe?
and the modal ?potuto?
depend onthe verb ?vedere?.
This steps groups all aux-iliaries of a verb under the first one, i.e.
?potu-to?.
This helps so that the full auxiliary can belooked up separately from the verb in thephrase table.4) A decoder that uses the output produced bythe reordering algorithm, queries the phrasetable and performs a beam search on the hypo-theses produced according to the suggestedreordering.8 Experimental Setup and ResultsMoses (Koehn et al, 2007) is used as a baselinephrase-based SMT system.
The following toolsand data were used in our experiments:1) the IRSTLM toolkit (Marcello and Cettolo,2007) is used to train a 5-gram language mod-84el with Kneser-Ney smoothing on a set of 4.5million sentences from the Italian Wikipedia.2) the Europarl version 6 corpus, consisting of1,703,886 sentence pairs, is used for training.A tuning set of 2000 sentences from ACLWMT 2007 is used to tune the parameters.3) the model is trained with lexical reordering.4) the model is tuned with mert (Bertoldi, et al )5) the official test set from ACL WMT 2008(Callison-Burch et al, 2008), consisting of2000 sentences, is used as test set.6) the open-source parser DeSR (DeSR, 2009) isused to parse Italian sentences, trained on theEvalita 2009 corpus (Bosco et al, 2009).
Pars-er domain adaptation is obtained by adding tothis corpus a set of 1200 sentences from theACL WMT 2005 test set, parsed by DeSR andthen corrected by hand.Both the training corpora and the test set had to becleaned in order to normalize tokens: for examplethe English versions contained possessives splitlike this ?Florence' s?.
We applied the same toke-nizer used by the parser which conforms to thePTB standard.DeSR achieved a Labeled Accuracy Score of88.67% at Evalita 2009, but for the purpose oftranslation, just the Unlabeled Accuracy is rele-vant, which was 92.72%.The table below shows the results of our decod-er (Desrt) in the translation from Italian to English,compared to a baseline Moses system trained onthe same corpora and to the online version ofGoogle translate.Desrt was run with a beam size of 10, since ex-periments showed no improvements with a largerbeam size.We show two versions of Desrt, one with parsetrees as obtained by the parser and one (Desrtgold) where the trees were corrected by hand.
Thedifference is minor and this confirms that the de-coder is robust and not much affected by parsingerrors.System BLEU NISTMoses 29.43 7.22Moses tree phrases 28.55 7.10Desrt gold 26.26 6.88Desrt 26.08 6.86Google Translate 24.96 6.86Desrt learned 24.37 6.76Table 1.
Results of the experiments.Since we used the same phrase table produced byMoses also for Desrt, Moses has an advantage,because it can look up n-grams that do not corres-pond to grammatical phrases, which Desrt neverconsiders.
In order to determine how this affectsthe results, we tested Moses restricting its choiceto phrases corresponding to treelets form the parsetree.
The result is shown in the row in the tablelabeled as ?Moses tree phrases?.
The score is low-er, as expected, but this confirms that Desrt makesquite good use of the portion of the phrase table ituses.Since the version of the reordering algorithm weused produces a single reordering, the Desrt de-coder has linear complexity on the length of thesentence.
Indeed, despite being written in Pythonand having to query the PT as a network service, itis quite faster than Moses.9 Error AnalysisDespite that fact that Desrt is driven by the parsetree, it is capable of selecting fairly good and evenlong sentences for look up in the phrase table.How close is the Desrt translation from those ofthe Moses baseline can be seen from this table:1-gram 2-gram 3-gram 4-gram 5-gramNIST 7.28 3.05 1.0 0.27 0.09BLEU 84.73 67.69 56.94 48.59 41.78Sometimes Desrt fails to select a better translationfor a verb, since it looks up prepositional phrasesseparately from the verb, while Moses often con-nects the preposition to the verb.This could be improved by performing a checkand scoring higher translations which include thetranslation of the preposition dependent on theverb.Another improvement could come from creatingphrase tables limited to treelet phrases, i.e.
phrasescorresponding to treelets from the parser.8510 EnhancementsThe current algorithm needs to be improved tofully deal with certain aspects of long distancedependencies.
Consider for example the sentence?The grass around the house is wet?.
The depen-dency tree of the sentence contains the non-contiguous phrases ?The grass?
and ?wet?, whoseItalian translation must obey a morphologicalgender agreement between the subject ?grass?
(?erba?, feminine), and the adjective ?wet?
(?bag-nata?
).However, the current combination algorithmdoes not exploit this dependence, because the lastphases of node merge will occur when the tree hasbeen reduced to this:The PT however could tell us that ?erba bagnata?is more likely than ?erba bagnato?
and allow us toscore the former higher.11 ConclusionsWe have described a decoding algorithm guidedby the dependency tree of the source sentence.
Byexploiting the dependency tree and deterministicreordering rules among the children of a node, thedecoder is fast and can be kept simple by avoidingto consider multiple reorderings, to use reorderingweights and to estimate future costs.There is still potential for improving the algo-rithm exploiting information implicit in the PT interms of morphological constraints, while main-taining a simple decoding algorithm that does notinvolve complex grammatical transformationrules.The experiments show encouraging results withrespect to state of the art PBT systems.
We plan totest the system on other language pairs to see howit generalizes to other situations where phrasereordering is relevant.AcknowledgmentsZauhrul Islam helped setting up our baseline sys-tem and Niladri Chatterjie participated in the earlydesign of the model.ReferencesG.
Attardi.
2006.
Experiments with a MultilanguageNon-Projective Dependency Parser.
Proc.
of theTenth Conference on Natural Language Learning,New York, (NY).H.
Alshawi, S. Douglas and S. Bangalore.
2000.Learning Dependency Translation Models asCollections of Finite State Head Transducers.Computational Linguistics 26(1), 45?60.N.
Bertoldi, B. Haddow, J-B.
Fouet.
2009.
ImprovedMinimum Error Rate Training in Moses.
In Proc.
of3rd MT Marathon, Prague, Czech Republic.V.
Ambati.
2008.
Dependency Structure Trees in Syn-tax Based Machine Translation.
Adv.
MT SeminarCourse Report.C.
Bosco, S. Montemagni, A. Mazzei, V. Lombardo, F.Dell?Orletta and A. Lenci.
2009.
Evalita?09 ParsingTask: comparing dependency parsers and treebanks.Proc.
of Evalita 2009.P.
F. Brown, V. J. Della Pietra, S. A. and  R. L. Mercer.1993.
The Mathematics of Statistical Machine Trans-lation: Parameter Estimation.
Computational Lin-guistics, 19(2), 263?311.Callison-Burch et al 2008.
Further Meta-Evaluation ofMachine Translation.
Proc.
of ACL WMT 2008.C.
Cherry.
2008.
Cohesive phrase-based decoding forstatistical machine translation.
Proc.
of ACL 2008:HLT.D.
Chiang.
2005.
A hierarchical phrase-based model forstatistical machine translation.
In Proc.
of ACL 2005.DeSR.
Dependency Shift Reduce parser.http://sourceforge.net/projects/desr/Y.
Ding, and M. Palmer.
2005.
Machine Translationusing Probabilistic Synchronous Dependency Inser-tion Grammar.
Proc.
of ACL?05, 541?548.C.
Dyer and P. Resnik.
2010.
Context-free reordering,finite-state translation.
Proc.
of HLT: The 2010Annual Conference of the North American Chapterof the ACL, 858?866.grassL?
erba intorno alla casais?wetbagnatabagnato86F.
Marcello, M. Cettolo.
2007.
Efficient Handling of N-gram Language Models for Statistical MachineTranslation.
Workshop on Statistical Machine Trans-lation 2007.M.
Galley and C. D. Manning.
2008.
A Simple andEffective Hierarchical Phrase Reordering Model.
InProc.
of EMNLP 2008.R.
Hwa, P. Resnik, A. Weinberg, C. Cabezas and O.Kolak, 2005.
Bootstrapping Parsers via SyntacticProjection across Parallel texts.
Natural LanguageEngineering 11(3), 311-325.P.
Koehn, F. J. Och and D. Marcu.
2003.
StatisticalPhrase-Based Translation.
Proc.
of Human Lan-guage Technology and North American Associationfor Computational Linguistics Conference(HLT/NAACL), 127?133.P.
Koehn, H. Hoang, A. Birch, C. Callison-Burch, M.Federico, N. Bertoldi, B. Cowan, W. Shen, C. Mo-ran, R. Zens, C. Dyer, O. Bojar, A. Constantin, andE.
Herbst.
2007.
Moses: Open source toolkit for sta-tistical machine translation.
In Proc.
of the 45th An-nual Meeting of the ACL, demonstration session,177?180, Prague, Czech Republic.P.
Koehn.
2009.
Statistical Machine Translation.Cambridge University Press.Y.
Liu, Q. Liu and S. Lin.
2006.
Tree-to-string Align-ment Template for Statistical Machine Translation,In Proc.
of COLING-ACL.D.
Marcu and W. Wong.
2002.
A Phrase-Based JointProbability Model for Statistical Machine Transla-tion.
Proc.
Empirical Methods in Natural LanguageProcessing (EMNLP), 133?139.C.
Quirk, A. Menzes and C. Cherry.
2005.
DependencyTreelet Translation: Syntactically Informed PhrasalSMT.
Proc.
43rd Annual Meeting of the ACL, 217?279.S.
Libin, J. Xu and R. Weischedel.
2008.
A New String-to-Dependency Machine Translation Algorithm witha Target Dependency Language Model.
Proc.
ACL-08, 577?585.F.
J. Och 2002.
Statistical Machine Translation: FromSingle Word Models to Alignment Template.
Ph.D.Thesis, RWTH Aachen, Germany.F.J.
Och, H. Ney.
2000.
Improved Statistical AlignmentModels.
Proc.
of the 38th Annual Meeting of theACL.
Hong Kong, China.
440-447.K.
Yamada and K. Knight.
2001.
A Syntax-Based Sta-tistical Translation Model.
Proc.
39th Annual Meet-ing of ACL (ACL-01), 6?11.P.
Xu, J. Kang, M. Ringgaard and F. Och.
2009.
Usinga Dependency Parser to Improve SMT for Subject-Object-Verb Languages.
Proc.
of NAACL 2009, 245?253, Boulder, Colorado.D.
Zhang, Mu Li, Chi-Ho Li and M. Zhou.
2007.Phrase Reordering Model Integrating SyntacticKnowledge for SMT.
Proc.
Joint Conference onEmpirical Methods in Natural Language Processingand Computational  Natural Language Processing:533?540.87
