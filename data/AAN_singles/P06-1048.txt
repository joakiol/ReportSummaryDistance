Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL, pages 377?384,Sydney, July 2006. c?2006 Association for Computational LinguisticsModels for Sentence Compression: A Comparison across Domains,Training Requirements and Evaluation MeasuresJames Clarke and Mirella LapataSchool of Informatics, University of Edinburgh2 Bucclecuch Place, Edinburgh EH8 9LW, UKjclarke@ed.ac.uk, mlap@inf.ed.ac.ukAbstractSentence compression is the task of pro-ducing a summary at the sentence level.This paper focuses on three aspects ofthis task which have not received de-tailed treatment in the literature: train-ing requirements, scalability, and auto-matic evaluation.
We provide a novel com-parison between a supervised constituent-based and an weakly supervised word-based compression algorithm and exam-ine how these models port to different do-mains (written vs. spoken text).
To achievethis, a human-authored compression cor-pus has been created and our study high-lights potential problems with the auto-matically gathered compression corporacurrently used.
Finally, we assess whetherautomatic evaluation measures can beused to determine compression quality.1 IntroductionAutomatic sentence compression has recently at-tracted much attention, in part because of its affin-ity with summarisation.
The task can be viewedas producing a summary of a single sentence thatretains the most important information while re-maining grammatically correct.
An ideal compres-sion algorithm will involve complex text rewritingoperations such as word reordering, paraphrasing,substitution, deletion, and insertion.
In default ofa more sophisticated compression algorithm, cur-rent approaches have simplified the problem to asingle rewriting operation, namely word deletion.More formally, given an input sentence of wordsW = w1,w2, .
.
.
,wn, a compression is formed bydropping any subset of these words.
Viewing thetask as word removal reduces the number of pos-sible compressions to 2n; naturally, many of thesecompressions will not be reasonable or grammati-cal (Knight and Marcu 2002).Sentence compression could be usefully em-ployed in wide range of applications.
For exam-ple, to automatically generate subtitles for televi-sion programs; the transcripts cannot usually beused verbatim due to the rate of speech being toohigh (Vandeghinste and Pan 2004).
Other applica-tions include compressing text to be displayed onsmall screens (Corston-Oliver 2001) such as mo-bile phones or PDAs, and producing audio scan-ning devices for the blind (Grefenstette 1998).Algorithms for sentence compression fall intotwo broad classes depending on their training re-quirements.
Many algorithms exploit parallel cor-pora (Jing 2000; Knight and Marcu 2002; Riezleret al 2003; Nguyen et al 2004a; Turner and Char-niak 2005; McDonald 2006) to learn the corre-spondences between long and short sentences ina supervised manner, typically using a rich featurespace induced from parse trees.
The learnt ruleseffectively describe which constituents should bedeleted in a given context.
Approaches that donot employ parallel corpora require minimal orno supervision.
They operationalise compressionin terms of word deletion without learning spe-cific rules and can therefore rely on little linguisticknowledge such as part-of-speech tags or merelythe lexical items alone (Hori and Furui 2004).
Al-ternatively, the rules of compression are approxi-mated from a non-parallel corpus (e.g., the PennTreebank) by considering context-free grammarderivations with matching expansions (Turner andCharniak 2005).Previous approaches have been developed andtested almost exclusively on written text, a no-table exception being Hori and Furui (2004) whofocus on spoken language.
While parallel cor-pora of original-compressed sentences are not nat-urally available in the way multilingual corporaare, researchers have obtained such corpora auto-matically by exploiting documents accompaniedby abstracts.
Automatic corpus creation affordsthe opportunity to study compression mechanisms377cheaply, yet these mechanisms may not be repre-sentative of human performance.
It is unlikely thatauthors routinely carry out sentence compressionwhile creating abstracts for their articles.
Collect-ing human judgements is the method of choice forevaluating sentence compression models.
How-ever, human evaluations tend to be expensive andcannot be repeated frequently; furthermore, com-parisons across different studies can be difficult,particularly if subjects employ different scales, orare given different instructions.In this paper we examine some aspects of thesentence compression task that have received lit-tle attention in the literature.
First, we provide anovel comparison of supervised and weakly su-pervised approaches.
Specifically, we study howconstituent-based and word-based methods port todifferent domains and show that the latter tend tobe more robust.
Second, we create a corpus ofhuman-authored compressions, and discuss somepotential problems with currently used compres-sion corpora.
Finally, we present automatic evalu-ation measures for sentence compression and ex-amine whether they correlate reliably with be-havioural data.2 Algorithms for Sentence CompressionIn this section we give a brief overview of the algo-rithms we employed in our comparative study.
Wefocus on two representative methods, Knight andMarcu?s (2002) decision-based model and Horiand Furui?s (2004) word-based model.The decision-tree model operates over parallelcorpora and offers an intuitive formulation of sen-tence compression in terms of tree rewriting.
Ithas inspired many discriminative approaches tothe compression task (Riezler et al 2003; Nguyenet al 2004b; McDonald 2006) and has beenextended to languages other than English (seeNguyen et al 2004a).
We opted for the decision-tree model instead of the also well-known noisy-channel model (Knight and Marcu 2002; Turnerand Charniak 2005).
Although both models yieldcomparable performance, Turner and Charniak(2005) show that the latter is not an appropriatecompression model since it favours uncompressedsentences over compressed ones.1Hori and Furui?s (2004) model was originallydeveloped for Japanese with spoken text in mind,1The noisy-channel model uses a source model trainedon uncompressed sentences.
This means that the most likelycompressed sentence will be identical to the original sen-tence as the likelihood of a constituent deletion is typicallyfar lower than that of leaving it in.SHIFT transfers the first word from the input list ontothe stack.REDUCE pops the syntactic trees located at the topof the stack, combines them into a new tree and thenpushes the new tree onto the top of the stack.DROP deletes from the input list subsequences of wordsthat correspond to a syntactic constituent.ASSIGNTYPE changes the label of the trees at the topof the stack (i.e., the POS tag of words).Table 1: Stack rewriting operationsit requires minimal supervision, and little linguis-tic knowledge.
It therefor holds promise for lan-guages and domains for which text processingtools (e.g., taggers, parsers) are not readily avail-able.
Furthermore, to our knowledge, its perfor-mance on written text has not been assessed.2.1 Decision-based Sentence CompressionIn the decision-based model, sentence compres-sion is treated as a deterministic rewriting processof converting a long parse tree, l, into a shorterparse tree s. The rewriting process is decomposedinto a sequence of shift-reduce-drop actions thatfollow an extended shift-reduce parsing paradigm.The compression process starts with an emptystack and an input list that is built from the orig-inal sentence?s parse tree.
Words in the input listare labelled with the name of all the syntactic con-stituents in the original sentence that start with it.Each stage of the rewriting process is an operationthat aims to reconstruct the compressed tree.
Thereare four types of operations that can be performedon the stack, they are illustrated in Table 1.Learning cases are automatically generatedfrom a parallel corpus.
Each learning case is ex-pressed by a set of features and represents one ofthe four possible operations for a given stack andinput list.
Using the C4.5 program (Quinlan 1993)a decision-tree model is automatically learnt.
Themodel is applied to a parsed original sentence ina deterministic fashion.
Features for the currentstate of the input list and stack are extracted andthe classifier is queried for the next operation toperform.
This is repeated until the input list isempty and the stack contains only one item (thiscorresponds to the parse for the compressed tree).The compressed sentence is recovered by travers-ing the leaves of the tree in order.2.2 Word-based Sentence CompressionThe decision-based method relies exclusively onparallel corpora; the caveat here is that appropri-ate training data may be scarce when porting thismodel to different text domains (where abstracts378are not available for automatic corpus creation) orlanguages.
To alleviate the problems inherent withusing a parallel corpus, we have modified a weaklysupervised algorithm originally proposed by Horiand Furui (2004).
Their method is based on worddeletion; given a prespecified compression length,a compression is formed by preserving the wordswhich maximise a scoring function.To make Hori and Furui?s (2004) algorithmmore comparable to the decision-based model, wehave eliminated the compression length parameter.Instead, we search over all lengths to find the com-pression that gives the maximum score.
This pro-cess yields more natural compressions with vary-ing lengths.
The original score measures the sig-nificance of each word (I) in the compression andthe linguistic likelihood (L) of the resulting wordcombinations.2 We add some linguistic knowledgeto this formulation through a function (SOV ) thatcaptures information about subjects, objects andverbs.
The compression score is given in Equa-tion (1).
The lambdas (?I , ?SOV , ?L) weight thecontribution of the individual scores:S(V ) =M?i=1?II(vi)+?sovSOV (vi)+?LL(vi|vi?1,vi?2) (1)The sentence V = v1,v2, .
.
.
,vm (of M words)that maximises the score S(V ) is the best com-pression for an original sentence consisting of Nwords (M < N).
The best compression can befound using dynamic programming.
The ?
?s inEquation (1) can be either optimised using a smallamount of training data or set manually (e.g., ifshort compressions are preferred to longer ones,then the language model should be given a higherweight).
Alternatively, weighting could be dis-pensed with by including a normalising factor inthe language model.
Here, we follow Hori and Fu-rui?s (2004) original formulation and leave the nor-malisation to future work.
We next introduce eachmeasure individually.Word significance score The word signifi-cance score I measures the relative importance ofa word in a document.
It is similar to tf-idf, a termweighting score commonly used in information re-trieval:I(wi) = fi log FAFi (2)2Hori and Furui (2004) also have a confidence score basedupon how reliable the output of an automatic speech recog-nition system is.
However, we need not consider this scorewhen working with written text and manual transcripts.Where wi is the topic word of interest (topic wordsare either nouns or verbs), fi is the frequency of wiin the document, Fi is the corpus frequency of wiand FA is the sum of all topic word occurrences inthe corpus (?i Fi).Linguistic score The linguistic score?sL(vi|vi?1,vi?2) responsibility is to select somefunction words, thus ensuring that compressionsremain grammatical.
It also controls which topicwords can be placed together.
The score mea-sures the n-gram probability of the compressedsentence.SOV Score The SOV score is based on the in-tuition that subjects, objects and verbs should notbe dropped while words in other syntactic rolescan be considered for removal.
This score is basedsolely on the contents of the sentence consideredfor compression without taking into account thedistribution of subjects, objects or verbs, acrossdocuments.
It is defined in (3) where fi is the doc-ument frequency of a verb, or word bearing thesubject/object role and ?default is a constant weightassigned to all other words.SOV (wi) =??
?fi if wi in subject, objector verb role?default otherwise (3)The SOV score is only applied to the head word ofsubjects and objects.3 CorporaOur intent was to assess the performance of thetwo models just described on written and spo-ken text.
The appeal of written text is understand-able since most summarisation work today fo-cuses on this domain.
Speech data not only pro-vides a natural test-bed for compression applica-tions (e.g., subtitle generation) but also poses ad-ditional challenges.
Spoken utterances can be un-grammatical, incomplete, and often contain arte-facts such as false starts, interjections, hesitations,and disfluencies.
Rather than focusing on sponta-neous speech which is abundant in these artefacts,we conduct our study on the less ambitious do-main of broadcast news transcripts.
This lies in-between the extremes of written text and sponta-neous speech as it has been scripted beforehandand is usually read off an autocue.One stumbling block to performing a compara-tive study between written data and speech datais that there are no naturally occurring parallel379speech corpora for studying compression.
Auto-matic corpus creation is not a viable option ei-ther, speakers do not normally create summariesof their own utterances.
We thus gathered our owncorpus by asking humans to generate compres-sions for speech transcripts.In what follows we describe how the manualcompressions were performed.
We also brieflypresent the written corpus we used for our exper-iments.
The latter was automatically constructedand offers an interesting point of comparison withour manually created corpus.Broadcast News Corpus Three annotatorswere asked to compress 50 broadcast news sto-ries (1,370 sentences) taken from the HUB-41996 English Broadcast News corpus provided bythe LDC.
The HUB-4 corpus contains broadcastnews from a variety of networks (CNN, ABC,CSPAN and NPR) which have been manually tran-scribed and split at the story and sentence level.Each document contains 27 sentences on averageand the whole corpus consists of 26,151 tokens.3The Robust Accurate Statistical Parsing (RASP)toolkit (Briscoe and Carroll 2002) was used to au-tomatically tokenise the corpus.Each annotator was asked to perform sentencecompression by removing tokens from the originaltranscript.
Annotators were asked to remove wordswhile: (a) preserving the most important infor-mation in the original sentence, and (b) ensuringthe compressed sentence remained grammatical.
Ifthey wished they could leave a sentence uncom-pressed by marking it as inappropriate for com-pression.
They were not allowed to delete wholesentences even if they believed they contained noinformation content with respect to the story asthis would blur the task with abstracting.Ziff-Davis Corpus Most previous work (Jing2000; Knight and Marcu 2002; Riezler et al 2003;Nguyen et al 2004a; Turner and Charniak 2005;McDonald 2006) has relied on automatically con-structed parallel corpora for training and evalua-tion purposes.
The most popular compression cor-pus originates from the Ziff-Davis corpus ?
a col-lection of news articles on computer products.
Thecorpus was created by matching sentences that oc-cur in an article with sentences that occur in anabstract (Knight and Marcu 2002).
The abstractsentences had to contain a subset of the originalsentence?s words and the word order had to remainthe same.3The compression corpus is available at http://homepages.inf.ed.ac.uk/s0460084/data/.A1 A2 A3 Av.
Ziff-DavisComp% 88.0 79.0 87.0 84.4 97.0CompR 73.1 79.0 70.0 73.0 47.0Table 2: Compression Rates (Comp% measuresthe percentage of sentences compressed; CompRis the mean compression rate of all sentences)1 2 3 4 5 6 7 8 9 10Length of word span dropped00.10.20.30.40.5RelativenumberofdropsAnnotator 1Annotator 2Annotator 3Ziff-Davis+Figure 1: Distribution of span of words droppedComparisons Following the classificationscheme adopted in the British National Corpus(Burnard 2000), we assume throughout this paperthat Broadcast News and Ziff-Davis belong to dif-ferent domains (spoken vs. written text) whereasthey represent the same genre (i.e., news).
Table 2shows the percentage of sentences which werecompressed (Comp%) and the mean compressionrate (CompR) for the two corpora.
The annota-tors compress the Broadcast News corpus to asimilar degree.
In contrast, the Ziff-Davis corpusis compressed much more aggressively with acompression rate of 47%, compared to 73% forBroadcast News.
This suggests that the Ziff-Daviscorpus may not be a true reflection of humancompression performance and that humans tendto compress sentences more conservatively thanthe compressions found in abstracts.We also examined whether the two corpora dif-fer with regard to the length of word spans be-ing removed.
Figure 1 shows how frequently wordspans of varying lengths are being dropped.
As canbe seen, a higher percentage of long spans (fiveor more words) are dropped in the Ziff-Davis cor-pus.
This suggests that the annotators are remov-ing words rather than syntactic constituents, whichprovides support for a model that can act on theword level.
There is no statistically significant dif-ference between the length of spans dropped be-tween the annotators, whereas there is a signif-icant difference (p < 0.01) between the annota-tors?
spans and the Ziff-Davis?
spans (using the380Wilcoxon Test).The compressions produced for the BroadcastNews corpus may differ slightly to the Ziff-Daviscorpus.
Our annotators were asked to performsentence compression explicitly as an isolatedtask rather than indirectly (and possibly subcon-sciously) as part of the broader task of abstracting,which we can assume is the case with the Ziff-Davis corpus.4 Automatic Evaluation MeasuresPrevious studies relied almost exclusively onhuman judgements for assessing the well-formedness of automatically derived com-pressions.
Although human evaluations ofcompression systems are not as large-scale as inother fields (e.g., machine translation), they aretypically performed once, at the end of the de-velopment cycle.
Automatic evaluation measureswould allow more extensive parameter tuningand crucially experimentation with larger datasets.
Most human studies to date are conductedon a small compression sample, the test portionof the Ziff-Davis corpus (32 sentences).
Largersample sizes would expectedly render humanevaluations time consuming and generally moredifficult to conduct frequently.
Here, we reviewtwo automatic evaluation measures that holdpromise for the compression task.Simple String Accuracy (SSA, Bangalore et al2000) has been proposed as a baseline evaluationmetric for natural language generation.
It is basedon the string edit distance between the generatedoutput and a gold standard.
It is a measure of thenumber of insertion (I), deletion (D) and substi-tution (S) errors between two strings.
It is definedin (4) where R is the length of the gold standardstring.Simple String Accuracy = (1?
I +D+SR) (4)The SSA score will assess whether appropriatewords have been included in the compression.Another stricter automatic evaluation methodis to compare the grammatical relations found inthe system compressions against those found in agold standard.
This allows us ?to measure the se-mantic aspects of summarisation quality in termsof grammatical-functional information?
(Riezleret al 2003).
The standard metrics of precision,recall and F-score can then be used to measurethe quality of a system against a gold standard.Our implementation of the F-score measure usedthe grammatical relations annotations provided byRASP (Briscoe and Carroll 2002).
This parser isparticularly appropriate for the compression tasksince it provides parses for both full sentencesand sentence fragments and is generally robustenough to analyse semi-grammatical compres-sions.
We calculated F-score over all the relationsprovided by RASP (e.g., subject, direct/indirectobject, modifier; 15 in total).Correlation with human judgements is an im-portant prerequisite for the wider use of automaticevaluation measures.
In the following section wedescribe an evaluation study examining whetherthe measures just presented indeed correlate withhuman ratings of compression quality.5 Experimental Set-upIn this section we present our experimental set-up for assessing the performance of the two al-gorithms discussed above.
We explain how differ-ent model parameters were estimated.
We also de-scribe a judgement elicitation study on automaticand human-authored compressions.Parameter Estimation We created two vari-ants of the decision-tree model, one trained onthe Ziff-Davis corpus and one on the BroadcastNews corpus.
We used 1,035 sentences from theZiff-Davis corpus for training; the same sentenceswere previously used in related work (Knight andMarcu 2002).
The second variant was trained on1,237 sentences from the Broadcast News corpus.The training data for both models was parsed us-ing Charniak?s (2000) parser.
Learning cases wereautomatically generated using a set of 90 featuressimilar to Knight and Marcu (2002).For the word-based method, we randomlyselected 50 sentences from each training setto optimise the lambda weighting parame-ters4.
Optimisation was performed using Pow-ell?s method (Press et al 1992).
Recall from Sec-tion 2.2 that the compression score has threemain parameters: the significance, linguistic, andSOV scores.
The significance score was calcu-lated using 25 million tokens from the BroadcastNews corpus (spoken variant) and 25 million to-kens from the North American News Text Cor-pus (written variant).
The linguistic score was es-timated using a trigram language model.
The lan-guage model was trained on the North Ameri-4To treat both models on an equal footing, we attemptedto train the decision-tree model solely on 50 sentences.
How-ever, it was unable to produce any reasonable compressions,presumably due to insufficient learning instances.381can corpus (25 million tokens) using the CMU-Cambridge Language Modeling Toolkit (Clarksonand Rosenfeld 1997) with a vocabulary size of50,000 tokens and Good-Turing discounting.
Sub-jects, objects, and verbs for the SOV score wereobtained from RASP (Briscoe and Carroll 2002).All our experiments were conducted on sen-tences for which we obtained syntactic analyses.RASP failed on 17 sentences from the Broadcastnews corpus and 33 from the Ziff-Davis corpus;Charniak?s (2000) parser successfully parsed theBroadcast News corpus but failed on three sen-tences from the Ziff-Davis corpus.Evaluation Data We randomly selected40 sentences for evaluation purposes, 20 fromthe testing portion of the Ziff-Davis corpus (32sentences) and 20 sentences from the BroadcastNews corpus (133 sentences were set aside fortesting).
This is comparable to previous studieswhich have used the 32 test sentences from theZiff-Davis corpus.
None of the 20 BroadcastNews sentences were used for optimisation.
Weran the decision-tree system and the word-basedsystem on these 40 sentences.
One annotator wasrandomly selected to act as the gold standard forthe Broadcast News corpus; the gold standardfor the Ziff-Davis corpus was the sentence thatoccurred in the abstract.
For each original sen-tence we had three compressions; two generatedautomatically by our systems and a human au-thored gold standard.
Thus, the total number ofcompressions was 120 (3x40).Human Evaluation The 120 compressionswere rated by human subjects.
Their judgementswere also used to examine whether the automaticevaluation measures discussed in Section 4 corre-late reliably with behavioural data.
Sixty unpaidvolunteers participated in our elicitation study, allwere self reported native English speakers.
Thestudy was conducted remotely over the Internet.Participants were presented with a set of instruc-tions that explained the task and defined sentencecompression with the aid of examples.
They firstread the original sentence with the compressionhidden.
Then the compression was revealed bypressing a button.
Each participant saw 40 com-pressions.
A Latin square design prevented sub-jects from seeing two different compressions ofthe same sentence.
The order of the sentences wasrandomised.
Participants were asked to rate eachcompression they saw on a five point scale takinginto account the information retained by the com-pression and its grammaticality.
They were told allo: Apparently Fergie very much wants to have a career intelevision.d: A career in television.w: Fergie wants to have a career in television.g: Fergie wants a career in television.o: Many debugging features, including user-defined breakpoints and variable-watching and message-watchingwindows, have been added.d: Many debugging features.w: Debugging features, and windows, have been added.g: Many debugging features have been added.o: As you said, the president has just left for a busy threedays of speeches and fundraising in Nevada, Californiaand New Mexico.d: As you said, the president has just left for a busy threedays.w: You said, the president has left for three days ofspeeches and fundraising in Nevada, California andNew Mexico.g: The president left for three days of speeches andfundraising in Nevada, California and New Mexico.Table 3: Compression examples (o: original sen-tence, d: decision-tree compression, w: word-based compression, g: gold standard)compressions were automatically generated.
Ex-amples of the compressions our participants saware given in Table 3.6 ResultsOur experiments were designed to answer threequestions: (1) Is there a significant differencebetween the compressions produced by super-vised (constituent-based) and weakly unsuper-vised (word-based) approaches?
(2) How welldo the two models port across domains (writtenvs.
spoken text) and corpora types (human vs. au-tomatically created)?
(3) Do automatic evaluationmeasures correlate with human judgements?One of our first findings is that the the decision-tree model is rather sensitive to the style of trainingdata.
The model cannot capture and generalise sin-gle word drops as effectively as constituent drops.When the decision-tree is trained on the BroadcastNews corpus, it is unable to create suitable com-pressions.
On the evaluation data set, 75% of thecompressions produced are the original sentenceor the original sentence with one word removed.It is possible that the Broadcast News compres-sion corpus contains more varied compressionsthan those of the Ziff-Davis and therefore a largeramount of training data would be required to learna reliable decision-tree model.
We thus used theZiff-Davis trained decision-tree model to obtaincompressions for both corpora.Our results are summarised in Tables 4 and 5.Table 4 lists the average compression rates for382Broadcast News CompR SSA F-scoreDecision-tree 0.55 0.34 0.40Word-based 0.72 0.51 0.54gold standard 0.71 ?
?Ziff-Davis CompR SSA F-scoreDecision-tree 0.58 0.20 0.34Word-based 0.60 0.19 0.39gold standard 0.54 ?
?Table 4: Results using automatic evaluation mea-suresCompression Broadcast News Ziff-DavisDecision-tree 2.04 2.34Word-based 2.78 2.43gold standard 3.87 3.53Table 5: Mean ratings from human evaluationeach model as well as the models?
performance ac-cording to the two automatic evaluation measuresdiscussed in Section 4.
The row ?gold standard?displays human-produced compression rates.
Ta-ble 5 shows the results of our judgement elicitationstudy.The compression rates (CompR, Table 4) indi-cate that the decision-tree model compresses moreaggressively than the word-based model.
This isdue to the fact that it mostly removes entire con-stituents rather than individual words.
The word-based model is closer to the human compres-sion rate.
According to our automatic evaluationmeasures, the decision-tree model is significantlyworse than the word-based model (using the Stu-dent t test, SSA p < 0.05, F-score p < 0.05) onthe Broadcast News corpus.
Both models are sig-nificantly worse than humans (SSA p < 0.05, F-score p < 0.01).
There is no significant differencebetween the two systems using the Ziff-Davis cor-pus on both simple string accuracy and relationF-score, whereas humans significantly outperformthe two systems.We have performed an Analysis of Variance(ANOVA) to examine whether similar results areobtained when using human judgements.
Statisti-cal tests were done using the mean of the ratings(see Table 5).
The ANOVA revealed a reliable ef-fect of compression type by subjects and by items(p < 0.01).
Post-hoc Tukey tests confirmed thatthe word-based model outperforms the decision-tree model (?
< 0.05) on the Broadcast news cor-pus; however, the two models are not significantlyMeasure Ziff-Davis Broadcast NewsSSA 0.171 0.348*F-score 0.575** 0.532***p < 0.05 **p < 0.01Table 6: Correlation (Pearson?s r) between evalu-ation measures and human ratings.
Stars indicatelevel of statistical significance.different when using the Ziff-Davis corpus.
Bothsystems perform significantly worse than the goldstandard (?
< 0.05).We next examine the degree to which the auto-matic evaluation measures correlate with humanratings.
Table 6 shows the results of correlatingthe simple string accuracy (SSA) and relation F-score against compression judgements.
The SSAdoes not correlate on both corpora with humanjudgements; it thus seems to be an unreliable mea-sure of compression performance.
However, the F-score correlates significantly with human ratings,yielding a correlation coefficient of r = 0.575 onthe Ziff-Davis corpus and r = 0.532 on the Broad-cast news.
To get a feeling for the difficulty ofthe task, we assessed how well our participantsagreed in their ratings using leave-one-out resam-pling (Weiss and Kulikowski 1991).
The techniquecorrelates the ratings of each participant with themean ratings of all the other participants.
The aver-age agreement is r = 0.679 on the Ziff-Davis cor-pus and r = 0.746 on the Broadcast News corpus.This result indicates that F-score?s agreement withthe human data is not far from the human upperbound.7 Conclusions and Future WorkIn this paper we have provided a comparison be-tween a supervised (constituent-based) and a min-imally supervised (word-based) approach to sen-tence compression.
Our results demonstrate thatthe word-based model performs equally well onspoken and written text.
Since it does not relyheavily on training data, it can be easily extendedto languages or domains for which parallel com-pression corpora are scarce.
When no parallel cor-pora are available the parameters can be manu-ally tuned to produce compressions.
In contrast,the supervised decision-tree model is not partic-ularly robust on spoken text, it is sensitive to thenature of the training data, and did not produce ad-equate compressions when trained on the human-authored Broadcast News corpus.
A comparisonof the automatically gathered Ziff-Davis corpus383with the Broadcast News corpus revealed impor-tant differences between the two corpora and thussuggests that automatically created corpora maynot reflect human compression performance.We have also assessed whether automatic eval-uation measures can be used for the compressiontask.
Our results show that grammatical relations-based F-score (Riezler et al 2003) correlates re-liably with human judgements and could thus beused to measure compression performance auto-matically.
For example, it could be used to assessprogress during system development or for com-paring across different systems and system config-urations with much larger test sets than currentlyemployed.In its current formulation, the only functiondriving compression in the word-based modelis the language model.
The word significanceand SOV scores are designed to single out im-portant words that the model should not drop.
Wehave not yet considered any functions that encour-age compression.
Ideally these functions should beinspired from the underlying compression process.Finding such a mechanism is an avenue of futurework.
We would also like to enhance the word-based model with more linguistic knowledge; weplan to experiment with syntax-based languagemodels and more richly annotated corpora.Another important future direction lies in apply-ing the unsupervised model presented here to lan-guages with more flexible word order and richermorphology than English (e.g., German, Czech).We suspect that these languages will prove chal-lenging for creating grammatically acceptablecompressions.
Finally, our automatic evaluationexperiments motivate the use of relations-based F-score as a means of directly optimising compres-sion quality, much in the same way MT systemsoptimise model parameters using BLEU as a mea-sure of translation quality.AcknowledgementsWe are grateful to our annotators Vasilis Karaiskos, BeataKouchnir, and Sarah Luger.
Thanks to Jean Carletta, FrankKeller, Steve Renals, and Sebastian Riedel for helpful com-ments and suggestions.
Lapata acknowledges the support ofEPSRC (grant GR/T04540/01).ReferencesBangalore, Srinivas, Owen Rambow, and Steve Whittaker.2000.
Evaluation metrics for generation.
In Proceedingsof the 1st INLG.
Mitzpe Ramon, Israel, pages 1?8.Briscoe, E. J. and J. Carroll.
2002.
Robust accurate statisti-cal annotation of general text.
In Proceedings of the 3rdLREC.
Las Palmas, Spain, pages 1499?1504.Burnard, Lou.
2000.
The Users Reference Guide for theBritish National Corpus (World Edition).
British NationalCorpus Consortium, Oxford University Computing Ser-vice.Charniak, Eugene.
2000.
A maximum-entropy-inspiredparser.
In Proceedings of the 1st NAACL.
San Francisco,CA, pages 132?139.Clarkson, Philip and Ronald Rosenfeld.
1997.
Statistical lan-guage modeling using the CMU?cambridge toolkit.
InProceedings of Eurospeech.
Rhodes, Greece, pages 2707?2710.Corston-Oliver, Simon.
2001.
Text Compaction for Displayon Very Small Screens.
In Proceedings of the NAACLWorkshop on Automatic Summarization.
Pittsburgh, PA,pages 89?98.Grefenstette, Gregory.
1998.
Producing Intelligent Tele-graphic Text Reduction to Provide an Audio Scanning Ser-vice for the Blind.
In Proceedings of the AAAI Symposiumon Intelligent Text Summarization.
Stanford, CA, pages111?117.Hori, Chiori and Sadaoki Furui.
2004.
Speech summariza-tion: an approach through word extraction and a methodfor evaluation.
IEICE Transactions on Information andSystems E87-D(1):15?25.Jing, Hongyan.
2000.
Sentence Reduction for Automatic TextSummarization.
In Proceedings of the 6th ANLP.
Seat-tle,WA, pages 310?315.Knight, Kevin and Daniel Marcu.
2002.
Summarization be-yond sentence extraction: a probabilistic approach to sen-tence compression.
Artificial Intelligence 139(1):91?107.McDonald, Ryan.
2006.
Discriminative sentence compres-sion with soft syntactic constraints.
In Proceedings of the11th EACL.
Trento, Italy, pages 297?304.Nguyen, Minh Le, Susumu Horiguchi, Akira Shimazu, andBao Tu Ho.
2004a.
Example-based sentence reduction us-ing the hidden Markov model.
ACM TALIP 3(2):146?158.Nguyen, Minh Le, Akira Shimazu, Susumu Horiguchi,Tu Bao Ho, and Masaru Fukushi.
2004b.
Probabilisticsentence reduction using support vector machines.
In Pro-ceedings of the 20th COLING.
Geneva, Switzerland, pages743?749.Press, William H., Saul A. Teukolsky, William T. Vetterling,and Brian P. Flannery.
1992.
Numerical Recipes in C: TheArt of Scientific Computing.
Cambridge University Press,New York, NY, USA.Quinlan, J. R. 1993.
C4.5 ?
Programs for Machine Learn-ing.
The Morgan Kaufmann series in machine learning.Morgan Kaufman Publishers.Riezler, Stefan, Tracy H. King, Richard Crouch, and AnnieZaenen.
2003.
Statistical sentence condensation usingambiguity packing and stochastic disambiguation meth-ods for lexical-functional grammar.
In Proceedings of theHLT/NAACL.
Edmonton, Canada, pages 118?125.Turner, Jenine and Eugene Charniak.
2005.
Supervised andunsupervised learning for sentence compression.
In Pro-ceedings of the 43rd ACL.
Ann Arbor, MI, pages 290?297.Vandeghinste, Vincent and Yi Pan.
2004.
Sentence compres-sion for automated subtitling: A hybrid approach.
In Pro-ceedings of the ACL Workshop on Text Summarization.Barcelona, Spain, pages 89?95.Weiss, Sholom M. and Casimir A. Kulikowski.
1991.
Com-puter systems that learn: classification and predictionmethods from statistics, neural nets, machine learning,and expert systems.
Morgan Kaufmann Publishers Inc.,San Francisco, CA, USA.384
