Decision Tree Models Applied tothe Labeling of Text with Parts-of-SpeechEzra Black Fred Jelinek John LaffertyRobert Mercer Salim RoukosIBM Thomas J. Watson Research CenterABSTRACTWe describe work which uses decision trees to estimatemarginal probabilities in a maximum entropy model for pre-dicting the part-of-speech of a word given the context inwhich it appears.
Two experiments are presented which ex-hibit improvements over the usual hidden Markov model ap-proach.1.
In t roduct ionIn this paper we describe work which uses decision treesto estimate probabilities of words appearing with variousparts-of-speech, given the context in which the words ap-pear.
In principle, this approach affords the optimal so-lution to the problem of predicting the correct sequenceof parts-of-speech.
In practice, the method is limited bythe lack of large, hand-labeled training corpora, as wellas by the difficulty inherent in constructing a set of ques-tions to be used in the decision procedure.
Nevertheless,decision trees provide a powerful mechanism for tacklingthe problem of modeling long-distance dependencies.The following sentence is typical of the difficulties facinga tagging program:The new energy policy announced in December bythe Pr ime Minister will guarantee sufficien~ oil sup-plies at one price only.structed a complete set of binary questions to be asked ofwords, using a mutual information clustering procedure\[2\].
We then extracted a set of events from a 2-millionword corpus of hand-labeled text.
Using an algorithmsimilar to that described in \[1\], the set of contexts wasdivided into equivalence classes using a decision proce-dure which queried the binary questions, splitting thedata based upon the principle of maximum mutual in-formation between tags and questions.
The resultingtree was then smoothed using the forward-backward al-gorithm \[6\] on a set of held-out events, and tested on aset of previously unseen sentences from the hand-labeledcorpus.The results showed a modest improvement over the usualhidden Markov model approach.
We present explana-tions and examples of the results obtained and suggestideas for obtaining further improvements.2.
Dec is ion  TreesThe problem at hand is to predict a tag for a given wordin a sentence, taking into consideration the tags assignedto previous words, as well as the remaining words in thesentence.
Thus, if we wish to predict tag S,~ for word w~in a sentence S -- wl, w2, ?
?
?, wN, then we must form anestimate of the probabilityThe usual hidden Markov model, trained as describedthe last section of this paper, incorrectly labeled the verbannounced as having the active rather than the passiveaspect.
If, however, a decision procedure is used to re-solve the ambiguity, the context may be queried to de-termine the nature of the verb as well its agent.
We caneasily imagine, for example, that if the battery of avail-able questions is rich enough to include such queries as"Is the previous noun inanimate?"
and "Does the prepo-sition by appear within three words of the word beingtagged?"
then such ambiguities may be probabilisticallyresolved.
Thus it is evident that the success of the de-cision approach will rely in the questions as well as themanner in which they are asked.In the experiments described in this paper, we con-P(S,~ \[ $1, $2,...S,~-1 and wl,  w2 .
.
.
,  war).We will refer to a sequence ($1,... ,  t,~-l; w l , .
.
.
,  wN) asa history.
A generic history is denoted as H, or as H =(HT, Hw), when we wish to separate it into its tag andword components.
The set of histories is denoted by 7-/,and a pair (t, H) is called an event.A tag is chosen from a fixed tag vocabulary VT, andwords are chosen from a word vocabulary Vw.
Givena training corpus E of events, the decision tree methodproceeds by placing the observed histories into equiv-alence classes by asking binary questions about them.Thus, a tree is grown with each node labeled by a ques-tion q : 7-/ --~ {True, False}.
The  entropy of tags at a117leaf L of the tree q- is given byH(T I L) = - ~ P(t I L) log P(41 L)t?Tand the average ntropy of tags in the tree is given byH~(T) = ~ P(L) H(T \] L).L6TThe method of growing trees that we have employedadopts a greedy algorithm, described in \[1\], to minimizethe average ntropy of tags.Specifically, the tree is grown in the following manner.Each node n is associated with a subset E, C E of train-ing events.
For a given node n, we compute for eachquestion q, the conditional entropy of tags at n, givenby~r(Tln, q)  =P(q(H) = True In) H(TIn, q(H) = True) +P(q(H) = False In) H(TIn, q(H) = False).The node n is then assigned the question q with thelowest conditional entropy.
The reduction in entropy atnode n resulting in asking question q isH'(T In) - B(TIn, q).If this reduction is significant, as determined by evaluat-ing the question on held-out data, then two descendentnodes of n are created, corresponding to the equivalenceclasses of events{E = (t, H) I E 6 E., q(H) = True}and{E = (t, H) I E 6 ?., q(H) = False}.The algorithm continues to split nodes by choosing thequestions which maximize the reduction in entropy, untileither no further splits are possible, or until a maximumnumber of leaves is obtained.3.
Max imum Ent ropy  Mode lsThe above algorithm for growing trees has as its ob-jective function the entropy of the joint distribution oftags and histories.
More generally, if we suppose thattags and histories arise according to some distribution~(4, HT, Hw) in textual data, the coding theory point-of-view encourages us to try to construct a model forgenerating tags and histories according to a distributionp(4, HT, Hw) which minimizes the Kullback informationD(P ll )= P(t, HT, Hw)Iog p(4'HT'HW)t,H~,Hw ~(4, HT, Hw) "Typically, one may be able to obtain estimates for certainmarginals ofp.
In the case of tagging, we have estimatesof the marginals q(t, HT) = ~H- p(4, HT, Hw) from theEM algorithm applied to label~'ed or partially labeledtext.
The marginals r(4, Hw) = ~ HTp(4, HT, Hw)might be estimated using decision trees applied to la-belled text.
To minimize D(p II q) subject o knowingthese marginals, introducing Lagrange multipliers a andfl leads us to minimize the function~ p(4, HT, Hw)log HT, HW)+t,HT,Hwt,H=t,Hw H~Differentiating with respect o p and solving this equa-tion, we find that the maximum en4ropy solution p takesthe formp(4, HT, Hw) = 7f(4, HT)g(4, Hw)p(4, HT, Hw)for some normalizing constant 7.
In particular, in thecase where we know no better than to take ~ equal tothe uniform distribution, we obtain the solutionp(4, HT, HW) = q(t, HT) r(t, Hw)q(4)where the marginal q(4) is assumed to satisfyq(t)= ~q(t ,  HT)= ~r(4 ,  Hw).Hz HwNote that the usual HMM tagging model is given byP(4.,t.-2, t._l) P(w.,4)p(t., HT, Hw) = P(t.)which has the form of a maximum entropy model, eventhough the marginals P(wn, in) and P(4., 4.-2, 4,~-1) aremodelled as bigram and trigram statistics, estimated ac-cording to the maximum likelihood criterion using theEM algorithm.In principle, growing a decision tree to estimate the fulldensity p(4n, HT, Hw) will provide a model with smallerKullback information.
In practice, however, the quantityof training data is severely limited, and the statistics atthe leaves will be unreliable.
In the model describedabove we assume that we are able to construct morereliable estimates of the marginals eparating the word118and tag components of the history, and we then com-bine these marginals according to the maximum entropycriterion.
In the experiments that we performed, suchmodels performed slightly better than those for whichthe full distribution p(tn, HT, Hw) was modeled with atree.4.
Constructing QuestionsThe method of mutual information clustering, describedin \[2\], can be used to obtain a set of binary features to as-sign to words, which may in turn be employed as binaryquestions in growing decision trees.
Mutual informationclustering proceeds by beginning with a vocabulary V,and initially assigning each word to a distinct class.
Ateach step, the average mutual information between adja-cent classes in training text is computed using a bigrammodel, and two classes are chosen to be merged basedupon the criterion of minimizing the loss in average mu-tual information that the merge affects.
If this processis continued until only one class remains, the result is abinary tree, the leaves of which are labeled by the wordsin the original vocabulary.
By labeling each branch by 0or 1, we obtain a bit string assigned to each word.Like all methods in statistical language modeling, thisapproach is limited by the problems of statistical signif-icance imposed by the lack of sufficient raining data.However, the method provides a powerful way of au-tomatically extracting both semantic and syntactic fea-tures of large vocabularies.
We refer to \[2\] for examplesof the features which this procedure yields.5.
Smoothing the Leaf DistributionsAfter growing a decision tree according to the proce-dures outlined above, we obtain an equivalence class ofhistories together with an empirical distribution of tagsat each leaf.
Because the training data, which is in anycase limited, is split exponentially in the process of grow-ing the tree, many of the leaves are invariably associatedwith a small number of events.
Consequently, the em-pirical distributions at such leaves may not be reliable,and it is desirable to smooth them against more reliablestatistics.One approach is to form the smoothed distributionsP(.
\[ n) from the empirical distributions P(.
\[ n) fora node n by settingP(t  I n) = An P(t  I n) + (1 - An) P(t  I parent(n))where parent(n) is the parent node of n (with the con-vention that parent(root) -- root), and 0 _< An _< 1 canbe thought of as the confidence placed in the empiricaldistribution at the node.In order to optimize the coefficients An, we seek to max-imize the probability that the correct prediction is madefor every event in a corpus ?g held-out from the train-ing corpus used to grow the tree.
That is, we attemptto maximize the objective functionO = 11 P(t l L(H))(t,H) E?Has a function of the coefficients A = (A1, A2,...) whereL(H) is the leaf of the history H. While finding the max-imizing A is generally an intractable problem, the EMalgorithm can be adopted to estimate coefficients whichlocally maximize the above objective function.
Since thisis a straightforward application of the EM algorithm wewill not present he details of the calculation here.6.
Experimental ResultsIn this section we report on two experiments in part-of-speech labeling using decision trees.
In the first ex-periment, we created a model for tagging text using aportion of the Lancaster treebank.
In the second exper-iment, we tagged a portion of the Brown corpus using amodel derived from the University of Pennsylvania cor-pus of hand-corrected labeled text.
In each case we com-pared the standard HMM model to a maximum entropymodel of the formP(tn  \] ~1, ~2, ' ' '  ~'n,--1 and 1,01, w2.
.
.
, wN) =: P(t,~ \[ t,~-2,t.~_1 ;Wn-2, W,~-i, Wn, Wn+1, Wn+2)= P(tn \[ w. -2 ,  w. -1 ,  w,~, w,~+1, Wn+2) xX P(~n i tn-~,~n-~) P(~.
)-~where the parameters P(tn \[ tn- l , tn-1)  were ob-tained Using the usual HMM method, and the parame-ters P(tn \[ wn-2, w,~-l, wn, wn+l, wn+2) were obtainedfrom a smoothed ecision tree as described above.
Thetrees were grown to have from 30,000 to 40,000 leaves.The relevant data of the experiments i  tabulated in Ta-bles 2 and 3.
The word and tag vocabularies were derivedfrom the data, as opposed to being obtained from on-linedictionaries or other sources.
In the case of the Lancastertreebank, however, the original set of approximately 350tags, many of which were special tags for idioms, wascompressed to a set of 163 tags.
A rough categorizationof these parts-of-speech appears in Table 1.For training the model we had at our disposal approxi-mately 1.9 million words of hand-labeled text.
This cor-pus is approximately half AP newswire text and half En-glish Hansard text, and was labeled by the team of Lan-caster linguists.
To construct our model, we divided thedata into three sections, to be used for training, smooth-11929 Nouns27 Verbs20 Pronouns17 Determiners16 Adverbs12 Punctuation10 Conjunctions8 Adjectives4 Prepositions20 OtherTable 1: Lancaster parts-of-speeching, and testing, consisting of 1,488,271 words, 392,732words, and 51,384 words respectively.We created an initial lexicon with the word-tag pairs thatappear in the training, smoothing, and test portions ofthis data.
We then filled out this lexicon using a statis-tical procedure which combines information from wordspellings together with information derived from wordbigram statistics in English text.
This technique can beused both to discover parts-of-speech for words which donot occur in the hand-labeled text, as well as to discoveradditional parts-of-speech for those that do.
In both ex-periments multiword expressions, such as "nineteenth-century" and "stream-of-consciousness," which were as-signed a single tag in the hand-labelled text, were brokenup into single words in the training text, with each wordreceiving no tag.The parameters of the HMM model were estimated fromthe training section of the hand-labeled text, withoutany use of the forward-backward algorithm.
Subse-quently, we used the smoothing section of the data toconstruct an interpolated model as described by Meri-aldo \[4, 6\].We evaluated the performance of the interpolated hiddenMarkov model by tagging the 2000 sentences which makeup the testing portion of the data.
We then comparedthe resultant ags with those produced by the Lancasterteam, and found the error rate to be 3.03%.We then grew and smoothed a decision tree using thesame division of training and smoothing data, and com-bined the resulting marginals for predicting tags fromthe word context with the marginals for predicting tagsfrom the tag context derived from the HMM model.
Theresulting error rate was 2.61%, a 14% reduction from theHMM model figure.Tag vocabulary size: 163 tagsWord vocabulary size: 41471 wordsTraining data: 1,488,271 wordsHeld-out data: 392,732 wordsTest data: 51,384 words(2000 sentences)Source of data: HansardsAP newswireDictionary: no unknown wordsMultiword expressions: broken upHMM errors: 1558 (3.03%)Decision tree errors: 1341 (2.61%)Error reduction: 13.9%Table 2: Lancaster Treebank ExperimentIn the case of the experiment with the UPenn corpus,the word vocabulary and dictionary were derived fromthe training and smoothing data only, and the dictio-nary was not statistically filled out.
Thus, there wereunknown words in the test data.
The tag set used in thesecond experiment was comprised of the 48 tags chosenby the UPenn project.
For training the model we hadat our disposal approximately 4.4 million words of hand-labeled text, using approximately half the Brown corpus,with the remainder coming from the Wall Street Jour-nal texts labelled by the UPenn team.
For testing themodel we used the remaining half of the Brown corpus,which was not used for any other purpose.
To constructour model, we divided the data into a training sectionof 4,113,858 words, and a smoothing section of 292,731words.
The error rate on 8,000 sentences from the Browncorpus test set was found to be 4.57%.
The correspond-ing error rate for the model using a decision tree grownonly on the Brown corpus portion of the training datawas 4.37%, representing only a 4.31% reduction in theerror rate.7.
ConclusionsIn two experiments we have seen how decision trees pro-vide modest improvements over HMM's for the problemof labeling unrestricted text with parts-of-speech.
In ex-amining the errors made by the models which incorpo-rate the decision tree marginals, we find that the errorsmay be attributed to two primary problems: bad ques-120Tag vocabulary size: 48 tagsWord vocabulary size: 86456 wordsTraining data: 4,113,858 wordsHeld-out data: 292,731 wordsTest data: 212,064 words(8000 sentences)Source of data: Brown corpusWall Street JournalDictionary: unknown test wordsMultiword expressions: broken upHMM errors: 9683 (4.57%)Decision tree errors: 9265 (4.37%)Error reduction: 4.31%References1.
L. Bahl, P. Brown, P. deSouza, and R. Mercer.
A tree-based statistical language model for natural languagespeech recognition.
IEEE Transactions on Acoustics,Speech, and Signal Processing, 37, pp.
1001-1008, 1989.2.
P. Brown, V. Della Pietra, P. deSouza, and R. Mercer.Class-based n-gram models of natural language.
Pro-ceedings o\] the IBM Natural Language ITL, pp.
283-298,Paris, France, 1990.3.
K. Church.
A stochastic parts program and noun phraseparser for unrestricted text.
Proceedings of the SecondConference on Applied Natural Language Processing,Austin, Texas, 1988.4.
B. Merialdo.
Tagging text with a probabilistic model.IBM Research Report, RC 1597~, 1990.5.
M. Meteer, R. Schwartz, and It.
Weischedel.
Studies inpart of speech labelling.
In Proceedings of the February1991 DAItPA Speech and Natural Language Workshop.Asflomar, California.6.
S. Katz.
Estimation of probabilities from sparse datafor the language model component of a speech recog-nizer.
IEEE Transactions on Acoustics, Speech, and Sig-nal Processing, ASSP-35, Number 3, pp.
400-401, 1987.Table 3: UPenn Brown Corpus Experimenttions and insufficient raining data.
Consider the wordlack, for example, which may be either a noun or a verb.The mutual information clustering procedure tends toclassify such words as either nouns or verbs, rather thanas words which may be both.
In the case of lack asit appeared in the Lancaster data, the binary featuresemphasized the nominal apects of the word, relating itto such words as scarcity, number, amount and portion.This resulted in errors when it occurred as a verb in thetest data.Clearly an improvement in the binary questions asked ofthe histories is called for.
In a preliminary set of exper-iments we augmented the automatically-derived ques-tions with a small set of hand-constructed questionswhich were intended to resolve the ambiguity of the la-bel for verbs which may have either the active or pas-sive aspect.
The resulting decision trees, however, didnot significantly improve the error rate on this partic-ular problem, which represents inherently long-distancelinguistic phenomena.
Nevertheless, it appears that thebasic approach can be made to prosper through a com-bination of automatic and linguistic efforts.121
