First Joint Conference on Lexical and Computational Semantics (*SEM), pages 408?412,Montre?al, Canada, June 7-8, 2012. c?2012 Association for Computational LinguisticsEMNLP@CPH: Is frequency all there is to simplicity?Anders Johannsen, H?ctor Mart?nez, Sigrid Klerke?, Anders S?gaardCentre for Language TechnologyUniversity of Copenhagen{ajohannsen|alonso|soegaard}@hum.ku.dksigridklerke@gmail.com?AbstractOur system breaks down the problem of rank-ing a list of lexical substitutions according tohow simple they are in a given context into aseries of pairwise comparisons between can-didates.
For this we learn a binary classifier.As only very little training data is provided,we describe a procedure for generating artifi-cial unlabeled data from Wordnet and a corpusand approach the classification task as a semi-supervised machine learning problem.
We usea co-training procedure that lets each classi-fier increase the other classifier?s training setwith selected instances from an unlabeled dataset.
Our features include n-gram probabilitiesof candidate and context in a web corpus, dis-tributional differences of candidate in a cor-pus of ?easy?
sentences and a corpus of normalsentences, syntactic complexity of documentsthat are similar to the given context, candidatelength, and letter-wise recognizability of can-didate as measured by a trigram character lan-guage model.1 IntroductionThis paper describes a system for the SemEval 2012English Lexical Simplification shared task.
Thetask description uses a loose definition of simplic-ity, defining ?simple words?
as ?words that can beunderstood by a wide variety of people, including forexample people with low literacy levels or some cog-nitive disability, children, and non-native speakers ofEnglish?
(Specia et al, 2012).Feature rN???
?sf 0.33N???
?sf+1 0.27N???
?sf?1 0.27L?
?sf -0.26L?
?max -0.26RIproto(l) -0.18S?
?cn -0.17S?
?w -0.17S?
?cp -0.17Feature rRIproto(f) -0.15C??
?max -0.14RIorig(l) -0.11L?
?tokens -0.10C??
?min 0.10SWfreq 0.08SWLLR 0.07C??
?avg -0.04Table 1: Pearson?s r correlations.
The table showsthe three highest correlated features per group, all ofwhich are significant at the p < 0.01 level2 FeaturesWe model simplicity with a range of features dividedinto six groups.
Five of these groups make use ofthe distributional hypothesis and rely on external cor-pora.
We measure a candidate?s distribution in termsof its lexical associations (RI), participation in syn-tactic structures (S??
), or corpus presence in order toassess its simplicity (N???
?, SW, C???).
A singlegroup, L?
?, measures intrinsic aspects of the substi-tution candidate, such as its length.The substitution candidate is either an adjective,an adverb, a noun, or a verb, and all candidates withina list share the same part of speech.
Because wordclass might influence simplicity, we allow our modelto fit parameters specific to the candidate?s part ofspeech by making a copy of the features for each partof speech which is active only when the candidate isin the given part of speech.408Simple Wikipedia (SW) These two features con-tain relative frequency counts of the substitutionform in Simple English Wikipedia (SWfreq), and thelog likelihood ratio of finding the word in the simplecorpus to finding it in regular Wikipedia (SWLLR)1.Word length (L??)
This set of three features de-scribes the length of the substitution form in char-acters (L?
?sf ), the length of the longest token(L?
?max), and the length of the substitution form intokens (L??tokens).
Word length is an integral partof common measures of text complexity, e.g in theEnglish Flesch?Kincaid (Kincaid et al, 1975) in theform of syllable count, and in the Scandinavian LIX(Bjornsson, 1983).Character trigram model (C???)
These threefeatures approximate the reading difficulty of a wordin terms of the probabilities of its forming charactertrigrams, with special characters to mark word be-ginning and end.
A word with an unusual combi-nation of characters takes longer to read and is per-ceived as less simple (Ehri, 2005).We calculate the minimum, average, and maxi-mum trigram probability (C??
?min, C??
?avg, andC??
?max).2Web corpus N-gram (N????)
These 12 featureswere obtained from a pre-built web-scale languagemodel3.
Features of the form N???
?sf?i, where0 < i < 4, express the probability of seeing thesubstitution form together with the following (or pre-vious) unigram, bigram, or trigram.
N???
?sf isthe probability of substitution form itself, a featurewhich also is the backbone of our frequency base-line.Random Indexing (RI) These four features areobtained from measures taken from a word-to-worddistributional semantic model.
Random Indexing(RI) was chosen for efficiency reasons (Sahlgren,2005).
We include features describing the seman-tic distances between the candidate and the original1Wikipedia dump obtained March 27, 2012.
Date on theSimple Wikipedia dump is March 22, 2012.2Trigram probabilities derived from Google T1 unigramcounts.3The ?jun09/body?
trigram model from Microsoft Web N-gram Services.form (RIorig), and between the candidate and a proto-type vector (RIproto).
For the distance between can-didate and original, we hypothesize that annotatorswould prefer a synonym closer to the original form.A prototype distributional vector of a set of words isbuilt by summing the individual word vectors, thusobtaining a representation that approximates the be-havior of that class overall (Turney and Pantel, 2010).Longer distances indicate that the currently exam-ined substitution is far from the shared meaning ofall the synonyms, making it a less likely candidate.The features are included for both lemma and surfaceforms of the words.Syntactic complexity (S??)
These 23 featuresmeasure the syntactic complexity of documentswhere the substitution candidate occurs.
We usedmeasures from (Lu, 2010) in which they describe 14automatic measures of syntactic complexity calcu-lated from frequency counts of 9 types of syntacticstructures.
This group of syntax-metric scores buildson two ideas.First, syntactic complexity and word difficulty gotogether.
A sentence with a complicated syntax ismore likely to be made up of difficult words, andconversely, the probability that a word in a sentenceis simple goes up when we know that the syntax ofthe sentence is uncomplicated.
To model this wesearch for instances of the substitution candidates inthe UKWAC corpus4 and measure the syntactic com-plexity of the documents where they occur.Second, the perceived simplicity of a word maychange depending on the context.
Consider the ad-jective ?frigid?, which may be judged to be sim-pler than ?gelid?
if referring to temperature, but per-haps less simple than ?ice-cold?
when characterizingsomeone?s personality.
These differences in wordsense are taken into account by measuring the sim-ilarity between corpus documents and substitutioncontexts and use these values to provide a weightedaverage of the syntactic complexity measures.3 Unlabeled dataThe unlabeled data set was generated by a three-step procedure involving synonyms extracted fromWordnet5 and sentences from the UKWAC corpus.4http://wacky.sslmit.unibo.it/5http://wordnet.princeton.edu/4091) Collection: Find synsets for unambigious lem-mas in Wordnet.
The synsets must have more thanthree synonyms.
Search for the lemmas in the cor-pus.
Generate unlabeled instances by replacing thelemma with each of its synonyms.
2) Sampling: Inthe unlabeled corpus, reduce the number of rankingproblems per lemma to a maximum of 10.
Samplefrom this pool while maintaining a distribution ofpart of speech similar to that of the trial and test set.3) Filtering: Remove instances for which there aremissing values in our features.The unlabeled part of our final data set containsn = 1783 problems.4 RankingWe are given a number of ranking problems (n =300 in the trial set and n = 1710 for the test data).Each of these consists of a text extract with a posi-tion marked for substitution, and a set of candidatesubstitutions.4.1 Linear orderLet X (i) be the substitution set for the i-th problem.We can then formalize the ranking problem by as-suming that we have access to a set of (weighted)preference judgments, w(a ?
b) for all a, b ?
X (i)such that w(a ?
b) is the value of ranking item aahead of b.
The values are the confidence-weightedpair-wise decisions from our binary classifier.
Ourgoal is then to establish a total order on X (i) thatmaximizes the value of the non-violated judgments.This is an instance of the Linear Ordering Problem(Mart?
and Reinelt, 2011), which is known to be NP-hard.
However, with problems of our size (maximumten items in each ranking), we escape these complex-ity issues by a very narrow margin?10!
?
3.6 mil-lion means that the number of possible orderings issmall enough to make it feasible to find the optimalone by exhaustive enumeration of all possibilities.4.2 Binary classicationIn order to turn our ranking problem into binary clas-sification, we generate a new data set by enumerat-ing all point-wise comparisons within a problem andfor each apply a transformation function ?
(a,b) =a ?
b.
Thus each data point in the new set is thedifference between the feature values of two candi-dates.
This enables us to learn a binary classifier forthe relation ?ranks ahead of?.We use the trial set for labeled training data L and,in a transductive manner, treat the test set as unla-beled data Utest.
Further, we supplement the pool ofunlabeled data with artificially generated instancesUgen, such that U = Utest ?
Ugen.Using a co-training setup (Blum and Mitchell,1998), we divide our features in two independent setsand train a large margin classifier6 on each split.
Theclassifiers then provide labels for data in the unla-beled set, adding the k most confidently labeled in-stances to the training data for the other classifier, aniterative process which continues until there is no un-labeled data left.
At the end of the training we havetwo classifiers.
The classification result is a mixture-of-experts: the most confident prediction of the twoclassifiers.
Furthermore, as an upper-bound of theco-training procedure, we define an oracle that re-turns the correct answer whenever it is given by atleast one classifier.4.3 TiesIn many cases we have items a and b that tie?inwhich case both a ?
b and b ?
a are violated.
Wedeal with these instances by omitting them from thetraining set and setting w(a ?
b) = 0.
For the fi-nal ranking, our system makes no attempt to produceties.5 ExperimentsIn our experiments we vary feature-split, size of un-labeled data, and number of iterations.
The first fea-ture split, S??
?SW, pooled all syntactic complexityfeatures and Wikipedia-based features in one view,with the remaining feature groups in another view.Our second feature split, S???C????L?
?, combinedthe syntactic complexity features with the charactertrigram language model features and the basic wordlength features.
Both splits produced a pair of classi-fiers with similar performance?each had an F-scoreof around .73 and an oracle score of .87 on the trialset on the binary decision problem, and both splitsperformed equally on the ranking task.6Liblinear with L1 penalty and L2 loss.
Parameter settingswere default.
http://www.csie.ntu.edu.tw/?cjlin/liblinear/410System All N V R AM????????F???
0.449 0.367 0.456 0.487 0.493S??
?SWf 0.377 0.283 0.269 0.271 0.421S??
?SWl 0.425 0.355 0.497 0.408 0.425S???C????L?
?f 0.377 0.284 0.469 0.270 0.421S???C????L?
?l 0.435 0.362 0.481 0.465 0.439Table 2: Performance on part of speech.
Unlabeledset was Utest.
Subscripts tell whether the scores arefrom the first or last iterationWith a large unlabeled data set available, the clas-sifiers can avoid picking and labeling data pointswith a low certainty, at least initially.
The assump-tion is that this will give us a higher quality trainingset.
However, as can be seen in Figure 1, none of oursystems are benefitting from the additional data.
Infact, the systems learn more when the pool of unla-beled data is restricted to the test set.Our submitted systems, O?
?1 and O?
?2 scored0.405 and 0.393 on the test set, and 0.494 and 0.500on the trial set.
Following submission we adjusteda parameter7 and re-ran each split with both U andUtest.We analyzed the performance by part of speechand compared them to the frequency baseline asshown in Table 2.
For the frequency baseline, per-formance is better on adverbs and adjectives alone,and somewhat worse on nouns.
Both our sys-tems benefit from co-training on all word classes.S???C????L?
?, our best performing system, no-tably has a score reduction (compared to the base-line) of only 5% on adverbs, eliminates the score re-duction on nouns, and effectively beats the baselinescore on verbs with a 6% increase.6 DiscussionThe frequency baseline has proven very strong, and,as witnessed by the correlations in Table 1, frequencyis by far the most powerful signal for ?simplicity?.But is that all there is to simplicity?
Perhaps it is.For a person with normal reading ability, a sim-ple word may be just a word with which the per-son is well-acquainted?one that he has seen be-fore enough times to have a good idea about whatit means and in which contexts it is typically used.7In particular, we selected a larger value for the C parameterin the liblinear classifier.0 5000 10000 15000 20000 25000Unlabeled datapoints0.380.400.420.440.460.48ScoreSYN-SW(Utest)SYN-CHAR-LEN(Utest)SYN-CHAR-LEN(U)Figure 1: Test set kappa score vs. number of datapoints labeled during co-trainingAnd so an n-gram model might be a fair approxi-mation.
However, lexical simplicity in English maystill be something very different to readers with lowliteracy.
For instance, the highly complex letter-to-sound mapping rules are likely to prevent such read-ers from arriving at the correct pronunciation of un-seen words and thus frequent words with exceptionalspelling patterns may not seem simple at all.A source of misclassifications discovered in ourerror analysis is the fact that substituting candidatesinto the given contexts in a straight-forward mannercan introduce syntactic errors.
Fixing these can re-quire significant revisions of the sentence, and yetthe substitutions resulting in an ungrammatical sen-tence are sometimes still preferred to grammatical al-ternatives.8 Here, scoring the substitution and theimmediate context in a language model is of littleuse.
Moreover, while these odd grammatical errorsmay be preferable to many non-native English speak-ers with adequate reading skills, such errors can bemore obstructing to reading impaired users and be-ginning language learners.AcknowledgmentsThis research is partially funded by the European Commission?s7th Framework Program under grant agreement n?
238405(CLARA).8For example sentence 1528: ?However, it appears they in-tend to pull out all stops to get what they want.?
Gold: {try ev-erything} {do everything it takes} {pull} {stop at nothing} {goto any length} {yank}.411ReferencesC.
H. Bjornsson.
1983.
Readability of Newspa-pers in 11 Languages.
Reading Research Quarterly,18(4):480?497.A Blum and T Mitchell.
1998.
Combining labeled andunlabeled data with co-training.
In Proceedings of theeleventh annual conference on Computational learningtheory, pages 92?100.
ACM.Linnea C. Ehri.
2005.
Learning to read words: The-ory, findings, and issues.
Scientific Studies of Reading,9(2):167?188.J P Kincaid, R P Fishburne, R L Rogers, and B S Chissom.1975.
Derivation of New Readability Formulas (Auto-mated Readability Index, Fog Count and Flesch Read-ing Ease Formula) for Navy Enlisted Personnel.Xiaofei Lu.
2010.
Automatic analysis of syntactic com-plexity in second language writing.
International Jour-nal of Corpus Linguistics, 15(4):474?496.Rafael Mart?
and Gerhard Reinelt.
2011.
The Lin-ear Ordering Problem: Exact and Heuristic Methodsin Combinatorial Optimization (Applied MathematicalSciences).
Springer.Magnus Sahlgren.
2005.
An introduction to randomindexing.
In Methods and Applications of Seman-tic Indexing Workshop at the 7th International Con-ference on Terminology and Knowledge Engineering,TKE, volume 5.Lucia Specia, Sujay K. Jauhar, and Rada Mihalcea.
2012.SemEval-2012 Task 1: English Lexical Simplifica-tion.
In Proceedings of the 6th International Work-shop on Semantic Evaluation (SemEval 2012), Mon-treal, Canada.P.
D Turney and P. Pantel.
2010.
From frequency tomeaning: Vector space models of semantics.
Journalof Artificial Intelligence Research, 37:141?188.412
