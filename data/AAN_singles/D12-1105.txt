Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational NaturalLanguage Learning, pages 1146?1156, Jeju Island, Korea, 12?14 July 2012. c?2012 Association for Computational LinguisticsTraining Factored PCFGs with Expectation PropagationDavid Hall and Dan KleinComputer Science DivisionUniversity of California, Berkeley{dlwh,klein}@cs.berkeley.eduAbstractPCFGs can grow exponentially as additionalannotations are added to an initially simplebase grammar.
We present an approach wheremultiple annotations coexist, but in a factoredmanner that avoids this combinatorial explo-sion.
Our method works with linguistically-motivated annotations, induced latent struc-ture, lexicalization, or any mix of the three.We use a structured expectation propagationalgorithm that makes use of the factored struc-ture in two ways.
First, by partitioning the fac-tors, it speeds up parsing exponentially overthe unfactored approach.
Second, it minimizesthe redundancy of the factors during training,improving accuracy over an independent ap-proach.
Using purely latent variable annota-tions, we can efficiently train and parse withup to 8 latent bits per symbol, achieving F1scores up to 88.4 on the Penn Treebank whileusing two orders of magnitudes fewer parame-ters compared to the na?
?ve approach.
Combin-ing latent, lexicalized, and unlexicalized anno-tations, our best parser gets 89.4 F1 on all sen-tences from section 23 of the Penn Treebank.1 IntroductionMany high-performance PCFG parsers take an ini-tially simple base grammar over treebank labels likeNP and enrich it with deeper syntactic features toimprove accuracy.
This broad characterization in-cludes lexicalized parsers (Collins, 1997), unlexical-ized parsers (Klein and Manning, 2003), and latentvariable parsers (Matsuzaki et al 2005).
Figures1(a), 1(b), and 1(c) show small examples of context-free trees that have been annotated in these ways.When multi-part annotations are used in the samegrammar, systems have generally multiplied theseannotations together, in the sense that an NP thatwas definite, possessive, and VP-dominated wouldhave a single unstructured PCFG symbol that en-coded all three facts.
In addition, modulo backoffor smoothing, that unstructured symbol would of-ten have rewrite parameters entirely distinct from,say, the indefinite but otherwise similar variant ofthe symbol (Klein and Manning, 2003).
Therefore,when designing a grammar, one would have to care-fully weigh new contextual annotations.
Should adefiniteness annotation be included, doubling thenumber of NPs in the grammar and perhaps overlyfragmenting statistics?
Or should it be excluded,thereby losing important distinctions?
Klein andManning (2003) discuss exactly such trade-offs andomit annotations that were helpful on their own be-cause they were not worth the combinatorial or sta-tistical cost when combined with other annotations.In this paper, we argue for grammars with fac-tored annotations, that is, grammars with annota-tions that have structured component parts that arepartially decoupled.
Our annotated grammars caninclude both latent and explicit annotations, as illus-trated in Figure 1(d), and we demonstrate that thesefactored grammars outperform parsers with unstruc-tured annotations.After discussing the factored representation, wedescribe a method for parsing with factored anno-tations, using an approximate inference techniquecalled expectation propagation (Minka, 2001).
Ouralgorithm has runtime linear in the number of an-notation factors in the grammar, improving on thena?
?ve algorithm, which has runtime exponential inthe number of annotations.
Our method, the Ex-pectation Propagation for Inferring Constituency(EPIC) parser, jointly trains a model over factoredannotations, where each factor naturally leveragesinformation from other annotation factors and im-proves on their mistakes.1146(a) NP[agenda]NN[agenda]agendaNP[?s]The president?s(b) NP[?S]NN[?NP]agendaNP[?NP-Poss-Det]The president?s(c) NP[1]NN[0]agendaNP[1]The president?s(d) NP[agenda,?S,1]NN[agenda,?NP,0]agendaNP[?s,?NP-Poss-Det,1]The president?sFigure 1: Parse trees using four different annotation schemes: (a) Lexicalized annotation like that in Collins (1997);(b) Unlexicalized annotation like that in Klein and Manning (2003); (c) Latent annotation like that in Matsuzaki et al(2005); and (d) the factored, mixed annotations we argue for in our paper.We demonstrate the empirical effectiveness of ourapproach in two ways.
First, we efficiently traina latent-variable grammar with 8 disjoint one-bitlatent annotation factors, with scores as high as89.7 F1 on length ?40 sentences from the PennTreebank (Marcus et al 1993).
This latent vari-able parser outscores the best of Petrov and Klein(2008a)?s comparable parsers while using two or-ders of magnitude fewer parameters.
Second, wecombine our latent variable factors with lexicalizedand unlexicalized annotations, resulting in our bestF1 score of 89.4 on all sentences.2 IntuitionsModern theories of grammar such as HPSG (Pollardand Sag, 1994) and Minimalism (Chomsky, 1992)do not ascribe unstructured conjunctions of anno-tations to phrasal categories.
Rather, phrasal cat-egories are associated with sequences of metadatathat control their function.
For instance, an NPmight have annotations to the effect that it is sin-gular, masculine, and nominative, with perhaps fur-ther information about its animacy or other aspectsof the head noun.
Thus, it is appealing for a gram-mar to be able to model these (somewhat) orthog-onal notions, but most models have no mechanismto encourage this.
As a notable exception, Dreyerand Eisner (2006) tried to capture this kind of insightby allowing factored annotations to pass unchangedfrom parent label to child label, though they were notable to demonstrate substantial gains in accuracy.Moreover, there has been to our knowledge no at-tempt to employ both latent and non-latent annota-tions at the same time.
There is good reason for this:lexicalized or highly annotated grammars like thoseof Collins (1997) or Klein and Manning (2003) havea very large number of states and an even largernumber of rules.
Further annotating these rules withlatent annotations would produce an infeasibly largegrammar.
Nevertheless, it is a shame to sacrifice ex-pert annotation just to get latent annotations.
Thus,it makes sense to combine these annotation methodsin a way that does not lead to an explosion of thestate space or a fragmentation of statistics.3 Parsing with AnnotationsSuppose we have a raw (binarized) treebank gram-mar, with productions of the form A ?
B C.The typical process is to then annotate these ruleswith additional information, giving rules of the formA[x] ?
B[y] C[z].
In the case of explicit annota-tions, an x might include information about the par-ent category, or a head word, or a combination ofthings.
In the case of latent annotations, x will bean integer that may or may not correspond to somelinguistic notion.
We are interested in the specificcase where each x is actually factored into M dis-joint parts: A[x1, x2, .
.
.
, xM ].
(See Figure 1(d).
)We call each component of x an annotation factoror an annotation component.11473.1 Annotation ClassesIn this paper, we consider three kinds of annotationmodels, representing three of the major traditions inconstituency parsing.
Individually, none of our mod-els are state-of-the-art, instead achieving F1 scoresin the mid-80?s on the Penn Treebank.The first model is a relatively simple lexicalizedparser.
We are not aware of a prior discriminativelexicalized constituency parser, and it is quite dif-ferent from the generative models of Collins (1997).Broadly, it considers features over a binary rule an-notated with head words: A[h] ?
B[h] C[d] andA[h] ?
B[d] C[h], focusing on monolexical rulefeatures and bilexical dependency features.
It is ourbest individual model, scoring 87.3 F1 on the devel-opment set.The second is similar to the unlexicalized modelof Klein and Manning (2003).
This parser startsfrom a grammar with labels annotated with siblingand parent information, and then adds specific an-notations, such as whether an NP is possessive orwhether a symbol rewrites as a unary.
This parsergets 86.3, tying the original generative version ofKlein and Manning (2003).Finally, we use a straightforward discriminativelatent variable model much like that of Petrov andKlein (2008a).
Here, each symbol is given a la-tent annotation, referred to as a substate.
Typically,these substates correlate at least loosely with linguis-tic phenomena.
For instance, NP-1 might be associ-ated with possessive NPs, while NP-3 might be foradjuncts.
Often, these latent integers are consideredas bit strings, with each bit indicating one latent an-notation.
Prior work in this area has considered theeffect of splitting and merging these states (Petrov etal., 2006; Petrov and Klein, 2007), as well as ?mul-tiscale?
grammars (Petrov and Klein, 2008b).
Withtwo states (or one bit of annotation), our version ofthis parser gets 81.7 F1, edging out the compara-ble parser of Petrov and Klein (2008a).
On the otherhand, our parser gets 83.2 with four states (two bits),short of the performance of prior work.11Much of the difference stems from the different binariza-tion scheme we employ.
We use head-outward binarization,rather than the left-branching binarization they employed.
Thischange was to enable integrating lexicalization with our othermodels.3.2 Model RepresentationWe employ a general exponential family representa-tion of our grammar.
This representation is fairlygeneral, and?in its generic form?by no meansnew, save for the focus on annotation components.Formally, we begin with a parse tree T over basesymbols for some sentence w, and we decorate thetree with annotations X , giving a parse tree T [X].We focus on the case whenX partitions into disjointcomponents X = [X1, X2, .
.
.
, XM ].
These com-ponents are decoupled in the sense that, conditionedon the coarse tree T , each column of the annota-tion is independent of every other column.
How-ever, they are crucially not independent conditionedonly on the sentence w. This model is representedschematically in Figure 2(a).The conditional probability P(T [X]|w, ?)
of anannotated tree given words is:P(T [X]|w, ?
)=?m fm(T [Xm];w, ?m)?T ?,X?
?m fm(T ?
[X ?m];w, ?m)= 1Z(w, ?
)?mfm(T [Xm];w, ?m)(1)where the factors fm for each model take the form:fm(T [Xm];w, ?m) = exp(?Tm?m(T,Xm,w))Here, Xm is the annotation associated with a partic-ular model m. ?
is a feature function that projectsthe raw tree, annotations, and words into a featurevector.
The features ?
need to decompose into fea-tures for each factor fm; we do not allow featuresthat take into account the annotation from two dif-ferent components.We further add a pruning filter that assigns zeroweight to any tree with a constituent that a baselineunannotated grammar finds sufficiently unlikely, anda weight of one to any other tree.
This filter is similarto that used in Petrov and Klein (2008a) and allowsfor much more efficient training and inference.Because our model is discriminative, trainingtakes the form of maximizing the probability of thetraining trees given the words.
This objective is con-vex for deterministic annotations, but non-convexfor latent annotations.
We (locally) optimize the1148flex funlflat f?lat f?unl(a) (b) (c)Full product model Approximate modelP (T [X]|w; ?)
q(T |w)qlex qlatqunlFigure 2: Schematic representation of our model, its approximation, and expectation propagation.
(a) The full jointdistribution consists of a product of three grammars with different annotations, here lexicalized, latent, and unlexi-calized.
This model is described in Section 3.2.
(b) The core approximation is an anchored PCFG with one factorcorresponding to each annotation component, described in Section 5.1.
(c) Fitting the approximation with expectationpropagation, as described in Section 5.3.
At the center is the core approximation.
During each step, an ?augmented?distribution qm is created by taking one annotation factor from the full grammar and the rest from the approximategrammar.
For instance, in upper left hand corner the full fLEX is substituted for f?LEX.
This new augmented distributionis projected back to the core approximation.
This process is repeated for each factor until convergence.
(non-convex) log conditional likelihood of the ob-served training data (T (d),w(d)):`(?)
=?dlog P(T (d)|w(d), ?
)=?dlog?XP(T (d)[X]|w(d), ?
)(2)Using standard results, the derivative takes the form:?`(?)
=?dE[?
(T,X,w)|T (d),w(d)]??dE[?
(T,X,w)|w(d)](3)The first half of this derivative can be obtained by theforward/backward-like computation defined by Mat-suzaki et al(2005), while the second half requiresan inside/outside computation (Petrov and Klein,2008a).
The partition function Z(w, ?)
is computedas a byproduct of the latter computation.
Finally,this objective is regularized, using the L2 norm of ?as a penalty.We note that we omit from our parser one majorfeature class found in other discriminative parsers,namely those that use features over the words in thespan (Finkel et al 2008; Petrov and Klein, 2008b).These features might condition on words on eitherside of the split point of a binary rule or take intoaccount the length of the span.
While such featureshave proven useful in previous work, they are not thefocus of our current work and so we omit them.4 The Complexity of AnnotatedGrammarsNote that the first term of Equation 3?which isconditioned on the coarse tree T?factors into Mpieces, one for each of the annotation components.However, the second term does not factor because itis conditioned on just the words w. Indeed, na?
?velycomputing this term requires parsing with the fullyarticulated grammar, meaning that inference wouldbe no more efficient than parsing with non-factoredannotations.Standard algorithms for parsing run in timeO(G|w|3), where |w| is the length of the sentence,and G is the size of the grammar, measured in thenumber of (binary) rules.
Let G0 be the numberof binary rules in the unannotated ?base?
grammar.1149Suppose that we have M annotation components.Each annotation component can have up to A primi-tive annotations per rule.
For instance, a latent vari-able grammar will have A = 8b where b is the num-ber of bits of annotation.
If we compile all annota-tion components into unstructured annotations, wecan end up with a total grammar size of O(AMG0),and so in general parsing time scales exponentiallywith the number of annotation components.
Thus, ifwe use latent annotations and the hierarchical split-ting approach of Petrov et al(2006), then the gram-mar has size O(8SG0), where S is the number oftimes the grammar was split in two.
Therefore, thesize of annotated grammars can reach intractablelevels very quickly, particularly in the case of latentannotations, where all combinations of annotationsare possible.Petrov (2010) considered an approach to slowingthis growth down by using a set of M independentlytrained parsers Pm, and parsed using the productof the scores from each parser as the score for thetree.
This approach worked largely because train-ing was intractable: if the training algorithm couldreach the global optimum, then this approach mighthave yielded no gain.
However, because the opti-mization technique is local, the same algorithm pro-duced multiple grammars.In what follows, we propose another solution thatexploits the factored structure of our grammar withexpectation propagation.
Crucially, we are able tojointly train and parse with all annotation factors,minimizing redundancy across the models.
Whilenot exact, we will see that expectation propagationis indeed effective.5 Factored InferenceThe key insight behind the approximate inferencemethods we consider here is that the full model isa product of complex factors that interact in compli-cated ways, and we will approximate it with a prod-uct of corresponding simple factors that interact insimple ways.
Since each annotation factor is a rea-sonable model in both power and complexity on itsown, we can consider them one at a time, replac-ing all others with their approximations, as shown inFigure 2(c).The way we will build these approximations iswith expectation propagation (Minka, 2001).
Ex-pectation propagation (EP) is a general method forapproximate inference that generalizes belief propa-gation.
We describe it here, but we first try to pro-vide an intuition for how it functions in our system.We also describe a simplified version of EP, calledassumed density filtering (Boyen and Koller, 1998),which is somewhat easier to understand and rhetori-cally convenient.
For a more detailed introduction toEP in general, we direct the reader to either Minka(2001) or Wainwright and Jordan (2008).
Our treat-ment most resembles the former.5.1 Factored ApproximationsOur goal is to build an approximation that takes in-formation from all components into account.
To be-gin, we note that each of these components capturesdifferent phenomena: an unlexicalized grammar isgood at capturing structural relationships in a parsetree (e.g.
subject noun phrases have different dis-tributions than object noun phrases), while a lexi-calized grammar captures preferred attachments fordifferent verbs.
At the same time, each of these com-ponent grammars can be thought of as a refinementof the raw unannotated treebank grammar.
By itself,each of these grammars induces a different poste-rior distribution over unannotated trees for each sen-tence.
If we can approximate each model?s contri-bution by using only unannotated symbols, we candefine an algorithm that avoids the exponential over-head of parsing with the full grammar, and insteadworks with each factor in turn.To do so, we define a sentence specific coreapproximation over unannotated trees q(T |w) =?m f?m(T,w).
Figure 2(b) illustrates this approx-imation.
Here, q(T ) is a product of M structurallyidentical factors, one for each of the annotated com-ponents.
We will approximate each model fm byits corresponding f?m.
Thus, there is one color-coordinated approximate factor for each componentof the model in Figure 2(a).There are multiple choices for the structure ofthese factors, but we focus on anchored PCFGs.
An-chored PCFGs have productions of the form iAj ?iBk kCj , where i, k, and j are indexes into the sen-tence.
Here, iAj is a symbol representing buildingthe base symbol A over the span [i, j].Billott and Lang (1989) introduced anchored1150CFGs as ?shared forests,?
and Matsuzaki et al(2005) have previously used these grammars forfinding an approximate one-best tree in a latent vari-able parser.
Note that, even though an anchoredgrammar is unannotated, because it is sentence spe-cific it can represent many complex properties of thefull grammar?s posterior distribution for a given sen-tence.
For example, it might express a preferencefor whether a PP token attaches to a particular verbor to that verb?s object noun phrase in a particularsentence.Before continuing, note that a pointwise productof anchored grammars is still an anchored gram-mar.
The complexity of parsing with a product ofthese grammars is therefore no more expensive thanparsing with just one.
Indeed, anchoring adds noinferential cost at all over parsing with an unanno-tated grammar: the anchored indices i, j, k have tobe computed just to parse the sentence at all.
Thisproperty is crucial to EP?s efficiency in our setting.5.2 Assumed Density FilteringWe now describe a simplified version of EP: parsingwith assumed density filtering (Boyen and Koller,1998).
We would like to train a sequence ofM mod-els, where each model is trained with knowledgeof the posterior distribution induced by the previousmodels.
Much as boosting algorithms (Freund andSchapire, 1995) work by focusing learning on as-yet-unexplained data points, this approach will en-courage each model to improve on earlier models,albeit in a different formal way.At a high level, assumed density filtering (ADF)proceeds as follows.
First, we have an initially un-informative q: it assigns the same probability to allunpruned trees for a given sentence.
Then, we fac-tor in one of the annotated grammars and parse withthis new augmented grammar.
This gives us a newposterior distribution for this sentence over trees an-notated with just that annotation component.
Then,we can marginalize out the annotations, giving us anew q that approximates the annotated grammar asclosely as possible without using any annotations.Once we have incorporated the current model?s com-ponent, we move on to the next annotated grammar,augmenting it with the new q, and repeating.
Inthis way, information from all grammars is incor-porated into a final posterior distribution over treesusing only unannotated symbols.
The algorithm isthen as follows:?
Initialize q(T ) uniformly.?
For each m in sequence:1.
Create the augmented distributionqm(T[Xm]) ?
q(T) ?
fm(T[Xm]) andcompute inside and outside scores.2.
Minimize DKL(qm(T )||f?m(T )q(T ))byfitting an anchored grammar f?m.3.
Set q(T ) =?mm?=1 f?m?
(T ).Step 1 of the inner loop forms an approximate pos-terior distribution using fm, which is the parsingmodel associated with component m, and q, whichis the anchored core approximation to the poste-rior induced by the first m ?
1 models.
Then, themarginals are computed, and the new posterior dis-tribution is projected to an anchored grammar, cre-ating f?m.
More intuitively, we create an anchoredPCFG that makes the approximation ?as close aspossible?
to the augmented grammar.
(We describethis procedure more precisely in Section 5.4.)
Thus,each term fm is approximated in the context of theterms that come before it.
This contextual approx-imation is essential: without it, ADF would ap-proximate the terms independently, meaning that noinformation would be shared between the models.This method would be, in effect, a simple methodfor parser combination, not all that dissimilar to themethod proposed by Petrov (2010).
Finally, notethat the same inside and outside scores computed inthe loop can be used to compute the expected countsneeded in Equation 3.Now we consider the runtime complexity of thisalgorithm.
If the maximum number of annotationsper rule for any factor is A, ADF has complex-ity O(MAG0|w|3)when using M factors.
Incontrast, parsing with the fully annotated grammarwould have complexityO(AMG0|w|3).
Critically,for a latent variable parser with M annotation bits,the exact algorithm takes time exponential in M ,while this approximate algorithm takes time linearin M .It is worth pausing to consider what this algo-rithm does during training.
At each step, we have1151in q an approximation to what the posterior distribu-tion looks like with the first m?
1 models.
In someplaces, q will assign high probabilities to spans in thegold tree, and in some places it will not be so accu-rate.
?m will be particularly motivated to correct thelatter, because they are less like the gold tree.
On theother hand, ?m will ignore the other ?correct?
seg-ments, because q has already sufficiently capturedthem.5.3 Expectation PropagationWhile this sequential algorithm gives us a way to ef-ficiently combine many kinds of annotations, it isnot a fully joint algorithm: there is no backwardpropagation of information from later models to ear-lier models.
Ideally, no model should be privilegedover any other.
To correct that, we use EP, which isessentially the iterative generalization of ADF.Intuitively, EP cycles among the models, updat-ing the approximation for that model in turn so thatit closely resembles the predictions made by fm inthe context of all other approximations, as in Fig-ure 2(c).
Thus, each approximate term f?m is cre-ated using information from all other f?m?
, meaningthat the different annotation factors can still ?talk?to each other.
The product of these approximationsq will therefore come to act as an approximation tothe true posterior: it takes into account joint infor-mation about all annotation components, all withinone tractable anchored grammar.With that intuition in mind, EP is defined as fol-lows:?
Initialize contributions f?m to the approximateposterior q.?
At each step, choose m.1.
Include approximations to all factors otherthan m: q\m(T ) =?m?
6=m f?m?
(T ).2.
Create the augmented distribution by in-cluding the actual factor for component mqm(T [Xm]) ?
fm(T [Xm])q\m(T )and compute inside and outside scores.3.
Create a new f?m(T ) that minimizesDKL(qm(T )||f?m(T )q\m(T )).?
Finally, set q(T ) ?
?m f?m(T ).Step 2 creates the augmented distribution qm, whichincludes fm along with the approximate factors forall models except the current model.
Step 3 createsa new anchored f?m that has the same marginal dis-tribution as the true model fm in the context of theother approximations, just as we did in ADF.In practice, it is usually better to not recomputethe product of all f?m each time, but instead to main-tain the full product q(T ) ?
?m f?m and to removethe appropriate f?m by division.
This optimization isanalogous to belief propagation, where messages areremoved from beliefs by division, instead of recom-puting beliefs on the fly by multiplying all messages.Schematically, the whole process is illustrated inFigure 2(c).
At each step, one piece of the coreapproximation is replaced with the correspondingcomponent from the full model.
This augmentedmodel is then reapproximated by a new core approx-imation q after updating the corresponding f?m.
Thisprocess repeats until convergence.5.4 EPIC ParsingIn our parser, EP is implemented as follows.
qand each of the f?m are anchored grammars that as-sign weights to unannotated rules.
The product ofanchored grammars with the annotated factor fmneed not be carried out explicitly.
Instead, notethat an anchored grammar is just a function q(A ?B C, i, k, j) ?
R+ that returns a score for every an-chored binary rule.
This function can be easily in-tegrated into the CKY algorithm for a single anno-tated grammar by simply multiplying in the valueof q whenever computing the score of the respectiveproduction over some span.
The modified inside re-currence takes the form:INSIDE(A[x], i, j)=?B,y,C,z?T?(A[x]?
B[y] C[z],w)?
?i<k<jINSIDE(B[y], i, k) ?
INSIDE(C[z], k, j)?
q(A?
B C, i, k, j)(4)Thus, parsing with a pointwise product of an an-chored grammar and an annotated grammar has noincreased combinatorial cost over parsing with justthe annotated grammar.1152To actually perform the projection in step 3 of EP,we create an anchored grammar from inside and out-side probabilities.
First, we compute the expectednumber of times the rule iAj ?
iBk kCj occurs,and then then we locally normalize for each sym-bol iAj .
This actually creates the new q distribution,and so we have to divide out q\m This process mini-mizes KL divergence subject to the local normaliza-tion constraints.All in all, this gives an algorithm that takes timeO(IMAG0|w|3), where I is the maximum num-ber of iterations, M is the number of models, andA is the maximum number of annotations for anygiven rule.5.5 Other Inference AlgorithmsTo our knowledge, expectation propagation has beenused only once in the NLP community; Daume?
IIIand Marcu (2006) employed an unstructured ver-sion in a Bayesian model of extractive summariza-tion.
Therefore, it is worth describing how EP dif-fers from more familiar techniques.EP can be thought of as a more flexible gen-eralization of belief propagation, which has beenused several times in NLP (Smith and Eisner, 2008;Niehues and Vogel, 2008; Cromie`res and Kurohashi,2009; Burkett and Klein, 2012).
In particular, EP al-lows for the arbitrary choice of messages (the f?m),meaning that we can use structured messages likeanchored PCFGs.Mean field (Saul and Jordan, 1996) is another ap-proximate inference technique that allows for struc-tured approximations (Xing et al 2003; Burkett etal., 2010), but here the natural version of mean fieldfor our model would still be intractable.
However,it is possible to adapt mean field into allowing fortractable updates that are similar to the ones we pro-posed.
We do not pursue that approach here.Dual decomposition (Dantzig and Wolfe, 1960;Komodakis et al 2007) has recently become pop-ular in the community (Rush et al 2010; Koo etal., 2010).
In fact, EP can be seen as a particularkind of dual decomposition of the log normalizationconstant logZ(w, ?)
that is optimized with messagepassing rather than (sub-)gradient descent or LP re-laxations.
Indeed, Minka (2001) argues that the EPobjective is more efficiently optimized with messagepassing than with gradient updates.
This assertionshould be examined for the structured models com-mon in NLP, but that is beyond the scope of this pa-per.Finally, note that EP, like belief propagation butunlike mean field, is not guaranteed to converge,though in practice it usually seems to.
In our exper-iments, typically three or four iterations are enoughfor almost all sentences to reach convergence, andwe found no loss in cutting off the number of itera-tions to four.6 ExperimentsIn what follows, we describe three experiments.First, in a small experiment, we examine how effec-tive the different inference algorithms are for bothtraining and testing.
Second, we scale up our latentvariable model into successively larger products.
Fi-nally, we present a selection of the many possiblemodel combinations, showing that combining latentand expert annotation can be quite effective.6.1 Experimental SetupFor our experiments, we trained and tested on thePenn Treebank using the standard splits: sections 2-21 were training, 22 development, and 23 testing.In preliminary experiments, we report developmentset F1 on sentences up to length 40.
For our finaltest set experiment, we report F1 on sentences fromsection 23 up to length 40, as well as all sentencesfrom that section.
Scores reported are computed us-ing EVALB (Sekine and Collins, 1997).
We binarizetrees using Collins?
head rules (Collins, 1997).Each discriminative parser was trained using theAdaptive Gradient variant of Stochastic GradientDescent (Duchi et al 2010).
Smaller models wereseeded from larger models.
That is, before traininga grammar of 5 models with 1 latent bit each, westarted with weights from a parser with 4 factoredbits.
Initial experiments suggested this step did notaffect final performance, but greatly decreased to-tal training time, especially for the latent variableparsers.
For extracting a one-best tree, we use aversion of the Max-Recall algorithm of Goodman(1996).
When using EP or ADF, we initializedthe core approximation q to the uniform distributionover unpruned trees.1153ParsingTraining ADF EP Exact PetrovADF 84.3 84.5 84.5 82.5EP 84.1 84.6 84.5 78.7Exact 83.8 84.5 84.9 81.5Indep.
82.3 82.1 82.2 82.6Table 1: The effect of algorithm choice for training andparsing on a product of two 2-state parsers on F1.
Petrovis the product parser of Petrov (2010), and Indep.
refersto independently trained models.
For comparison, a four-state parser achieves a score of 83.2.When counting parameters, we consider the num-ber of parameters per binary rule.
Hence, a singlefour-state latent model would have 64 (= 43) param-eters per rule, while a product of 5 two-state modelswould have just 40 (= 5 ?
23).6.2 Comparison of Inference AlgorithmsIn our first experiment, we test the relative perfor-mance of the various approximate inference meth-ods at both train and test time.
In order to includeexact inference, we necessarily need to look at asmaller scale example for which exact inference isstill feasible.
We examined development perfor-mance for training and inference on a small productof two parsers, each with two latent states per sym-bol.During training, we have several options.
We canuse exact training by parsing with the fully articu-lated product of both grammars, or, we can insteaduse EP, ADF, or independent training.
At test time,we can parse using the full product of both gram-mars, or, we can instead use EP, ADF, or we can usethe method of Petrov (2010) wherein we multiplythe parsers together in an ad hoc fashion.The results are in Table 1.
The best reported score,unsurprisingly, is for using exact training and pars-ing, but using EP for training and parsing results ina relatively small loss of 0.3 F1.
ADF, however, suf-fers a loss of 0.6 F1 over Exact when used for train-ing and parsing.
Otherwise, Exact and EP seem toperform fairly similarly at parse time for all trainingconditions.In general, there seems to be a gain for using thesame method for training and testing.
Each test-ing method performs at its best when using modelstrained with the same method.
Moreover, except forADF, the converse holds true: the grammars trained8082848688901 2 3 4 5 6 7 8F1Number of ModelsFigure 3: Development F1 plotted against the number Mof one-bit latent annotation components.
The best gram-mar has 6 one-bit annotations, with 89.7 F1.with a given parsing method are best decoded usingthe same method.Oddly, using Petrov (2010)?s method does notseem to work well at all for jointly trained models,except for ADF.
Similarly, joint parsing underper-forms Petrov (2010)?s method when using indepen-dently trained models.
Likely, the joint parsing al-gorithms are miscalibrating the redundant informa-tion present in the two independently-trained mod-els, while the two jointly-trained components cometo depend on each other.
In fact, the F1 scores forthe two separate models of the EP parser are in the60?s.As expected, ADF does not perform as well asEP.
Therefore, we exclude it from our subsequentexperiments, focusing exclusively on EP.6.3 Latent Variable ExperimentsMost of the previous work in latent variable parsinghas focused on splitting smaller unstructured anno-tations into larger unstructured annotations.
Here,we consider training a joint model consisting of alarge number of disjoint one-bit (i.e.
two-state) la-tent variable annotations.
Specifically, we considerthe performance of products of up to 8 one-bit anno-tations.In Figure 3, we show development F1 as a func-tion of the number of latent bits.
Improvement isroughly linear up to 3 components.
Performancelevels off afterwards, with the top performing sys-tem scoring 89.7 F1.
Nevertheless, these parsersoutperform the comparable parsers of Petrov andKlein (2008a) (89.3), even though our six-bit parserhas many fewer effective parameters per binary rule:1154Models F1, ?
40 F1, AllLexicalized 87.3 86.5Unlexicalized 86.3 85.43xLatent 88.6 87.6Lex+Unlex 90.2 89.5Lex+Lat 90.0 89.4Unlex+Lat 90.0 89.4Lex+Unlex+Lat 90.2 89.7Table 2: Development F1 score for various model com-binations for sentences less than length 40 and all sen-tences.
3xLatent refers to a latent annotation model with3 factored latent bits.48 instead of the 4096 in their best parser.
We alsoran our best system on Section 23, where it gets 89.1and 88.4 on sentences less than length 40 and on allsentences, respectively.
This result compares favor-ably to the 88.8/88.3 of Petrov and Klein (2008a).6.4 Heterogeneous ModelsWe now consider factored models with differentkinds of annotations.
Specifically, we tested gram-mars comprising all subsets of {Lexicalized, Unlex-icalized, Latent}.
We used a model with 3 factoredbits as our representative of the latent variable class,because it was closest in performance to the othermodels.
Of course, other smaller and larger combi-nations are possible, but we found this selection tobe representative.The development results are in Table 2.
Unsur-prisingly, adding more kinds of annotations helps forthe most part, though the combination of all threecomponents is not much better than a combinationof just the lexicalized and unlexicalized models.
In-deed, our best systems involved combining the lexi-calized model with some other model.
This is proba-bly because the lexicalized model can represent verydifferent syntactic relationships than the latent andunlexicalized models, meaning there is more diver-sity in the joint model?s capacity when using combi-nations involving the lexicalized annotations.Finally, we ran our best system (the fully com-bined one) on Section 23 of the Penn Treebank.
Itscored 90.1/89.4 F1 on length 40 and all sentencesrespectively, slightly edging out the 90.0/89.3 F1of Petrov and Klein (2008a).
However, it is notquite as good at exact match: 37.7/35.3 vs 40.1/37.7.Note, though, that their parser makes use of spanfeatures, which deliver a gain of +0.3/0.2F1 respec-tively, while ours does not.
We suspect that similargains could be had by incorporating these features,but we leave that for future work.7 ConclusionFactored representations capture a fundamental lin-guistic insight: grammatical categories are notmonolithic, unanalyzable entities.
Instead, they arecomposed of numerous facets that together governhow categories combine into parse trees.We have developed a new model for grammarswith factored annotations and presented two meth-ods for parsing with these grammars.
Our ex-periments have demonstrated that our approachproduces higher performance parsers with manyfewer parameters.
Moreover, our model workswith both latent and explicit annotations, allowingus to combine linguistic knowledge with machinelearning.
Finally, our source code is available athttp://nlp.cs.berkeley.edu/Software.shtml.AcknowledgmentsWe would like to thank Slav Petrov, David Burkett,Adam Pauls, Greg Durrett and the anonymous re-viewers for helpful comments.
We would also liketo thank Daphne Koller for originally suggesting theassumed density filtering approach.
This work waspartially supported by BBN under DARPA contractHR0011-12-C-0014, and by an NSF fellowship tothe first author.ReferencesSylvie Billott and Bernard Lang.
1989.
The structureof shared forests in ambiguous parsing.
In Proceed-ings of the 27th Annual Meeting of the Association forComputational Linguistics, pages 143?151, Vancou-ver, British Columbia, Canada, June.Xavier Boyen and Daphne Koller.
1998.
Tractable in-ference for complex stochastic processes.
In Proceed-ings of the 14th Conference on Uncertainty in ArtificialIntelligence?UAI 1998, pages 33?42.
San Francisco:Morgan Kaufmann.David Burkett and Dan Klein.
2012.
Fast inference inphrase extraction models with belief propagation.
InNAACL.David Burkett, John Blitzer, and Dan Klein.
2010.Joint parsing and alignment with weakly synchronizedgrammars.
In NAACL.1155Noam Chomsky.
1992.
A minimalist program for lin-guistic theory, volume 1.
MIT Working Papers in Lin-guistics, MIT, Cambridge Massachusetts.Michael Collins.
1997.
Three generative, lexicalisedmodels for statistical parsing.
In ACL, pages 16?23.Fabien Cromie`res and Sadao Kurohashi.
2009.
Analignment algorithm using belief propagation and astructure-based distortion model.
In EACL.G.
B. Dantzig and P. Wolfe.
1960.
Decompositionprinciple for linear programs.
Operations Research,8:101?111.Hal Daume?
III and Daniel Marcu.
2006.
Bayesian query-focused summarization.
In Proceedings of the Confer-ence of the Association for Computational Linguistics(ACL), Sydney, Australia.Markus Dreyer and Jason Eisner.
2006.
Better informedtraining of latent syntactic features.
In EMNLP, pages317?326, July.John Duchi, Elad Hazan, and Yoram Singer.
2010.Adaptive Subgradient Methods for Online Learningand Stochastic Optimization.
COLT.Jenny Rose Finkel, Alex Kleeman, and Christopher D.Manning.
2008.
Efficient, feature-based, conditionalrandom field parsing.
In ACL 2008, pages 959?967.Yoav Freund and Robert E. Schapire.
1995.
A decision-theoretic generalization of on-line learning and an ap-plication to boosting.Joshua Goodman.
1996.
Parsing algorithms and metrics.In ACL, pages 177?183.Dan Klein and Christopher D. Manning.
2003.
Accurateunlexicalized parsing.
In ACL, pages 423?430.Nikos Komodakis, Nikos Paragios, and Georgios Tziri-tas.
2007.
Mrf optimization via dual decomposition:Message-passing revisited.
In ICCV, pages 1?8.Terry Koo, Alexander M. Rush, Michael Collins, TommiJaakkola, and David Sontag.
2010.
Dual decompo-sition for parsing with non-projective head automata.In Proceedings of the 2010 Conference on EmpiricalMethods in Natural Language Processing (EMNLP).Mitchell P. Marcus, Beatrice Santorini, and Mary AnnMarcinkiewicz.
1993.
Building a large annotated cor-pus of English: The Penn Treebank.
ComputationalLinguistics, 19(2):313?330.Takuya Matsuzaki, Yusuke Miyao, and Jun?ichi Tsujii.2005.
Probabilistic CFG with latent annotations.
InACL, pages 75?82, Morristown, NJ, USA.Thomas P. Minka.
2001.
Expectation propagation forapproximate Bayesian inference.
In UAI, pages 362?369.Jan Niehues and Stephan Vogel.
2008.
Discriminativeword alignment via alignment matrix modeling.
InProceedings of the Third Workshop on Statistical Ma-chine Translation, pages 18?25, June.Slav Petrov and Dan Klein.
2007.
Improved inferencefor unlexicalized parsing.
In NAACL-HLT, April.Slav Petrov and Dan Klein.
2008a.
Discriminative log-linear grammars with latent variables.
In NIPS, pages1153?1160.Slav Petrov and Dan Klein.
2008b.
Sparse multi-scalegrammars for discriminative latent variable parsing.
InEMNLP, pages 867?876, Honolulu, Hawaii, October.Slav Petrov, Leon Barrett, Romain Thibaux, and DanKlein.
2006.
Learning accurate, compact, and inter-pretable tree annotation.
In Proceedings of the 21st In-ternational Conference on Computational Linguisticsand 44th Annual Meeting of the Association for Com-putational Linguistics, pages 433?440, Sydney, Aus-tralia, July.Slav Petrov.
2010.
Products of random latent variablegrammars.
In Human Language Technologies: The2010 Annual Conference of the North American Chap-ter of the Association for Computational Linguistics,pages 19?27, Los Angeles, California, June.Carl Pollard and Ivan A.
Sag.
1994.
Head-Driven PhraseStructure Grammar.
University of Chicago Press,Chicago.Alexander M Rush, David Sontag, Michael Collins, andTommi Jaakkola.
2010.
On dual decomposition andlinear programming relaxations for natural languageprocessing.
In EMNLP, pages 1?11, Cambridge, MA,October.Lawrence Saul and Michael Jordan.
1996.
Exploit-ing tractable substructures in intractable networks.
InNIPS 1995.Satoshi Sekine and Michael J. Collins.
1997.
Evalb ?bracket scoring program.David A. Smith and Jason Eisner.
2008.
Dependencyparsing by belief propagation.
In EMNLP, pages 145?156, Honolulu, October.Martin J Wainwright and Michael I Jordan.
2008.Graphical Models, Exponential Families, and Varia-tional Inference.
Now Publishers Inc., Hanover, MA,USA.Eric P. Xing, Michael I. Jordan, and Stuart J. Russell.2003.
A generalized mean field algorithm for varia-tional inference in exponential families.
In UAI, pages583?591.1156
