Competitive Self-Trained Pronoun InterpretationAndrew Kehler?UC San Diegoakehler@ucsd.eduDouglas AppeltSRI Internationalappelt@ai.sri.comLara Taylor?UC San Diegolmtaylor@ucsd.eduAleksandr Simma?UC San Diegoasimma@ucsd.eduAbstractWe describe a system for pronoun interpre-tation that is self-trained from raw data,that is, using no annotated training data.The result outperforms a Hobbsian baselinealgorithm and is only marginally inferior toan essentially identical, state-of-the-art su-pervised model trained from a substantialmanually-annotated coreference corpus.1 IntroductionThe last several years have seen a number of feature-based systems for pronoun interpretation in whichthe feature weights are determined via manual exper-imentation or supervised learning (see Mitkov (2002)for a useful survey).
Reliable estimation of theweights in both paradigms requires a substantialmanually-annotated corpus of examples.
In thisshort paper we describe a system for (third-person)pronoun interpretation that is self-trained from rawdata, that is, using no annotated training data what-soever.
The result outperforms a Hobbsian baselinealgorithm and is only marginally inferior (2.3%) toan essentially identical, state-of-the-art supervisedmodel trained from a manually-annotated corefer-ence corpus.
This result leaves open the possibil-ity that systems self-trained on very large datasetswith more finely-grained features could eventuallyoutperform supervised models that rely on manually-annotated datasets.The remainder of the paper is organized as fol-lows.
We first briefly describe the supervised system(described in more detail in Kehler et al (2004)) towhich we will compare the self-trained system.
Bothsystems use the same learning algorithm and featureset; they differ with respect to whether the data they?Department of Linguistics.
?Department of Computer Science and Engineering.are trained on is annotated by a human or the algo-rithm itself.
We then describe our Hobbsian baselinealgorithm, and present the results of all three sys-tems.2 The Supervised AlgorithmThe supervised model was trained using the im-proved iterative scaling algorithm for MaximumEntropy (MaxEnt) models described by Berger etal.
(1996) with binary-valued features.
As is stan-dard, the model was trained as a binary coreferenceclassifier: for each possible antecedent of each pro-noun, a training instance was created that consistedof the pronoun, the possible antecedent phrase, anda binary coreference outcome.
(Such a model canbe seen as providing a probabilistic measure of an-tecedent salience.)
Because we are ultimately inter-ested in identifying the correct antecedent among aset of possible ones, during testing the antecedentassigned the highest probability is chosen.The algorithm receives as input the results of SRI?sTextpro system, a shallow parser that recognizeslow-level constituents (noun groups, verb groups,etc.).
No difficult syntactic attachments are at-tempted, and the results are errorful.
There was nohuman-annotated linguistic information in the input.The training corpus consists of 2773 annotatedthird-person pronouns from the newspaper andnewswire segments of the Automatic Content Ex-traction (ACE) program training corpus.
The an-notated blind corpus used for evaluation consists of762 annotated third-person pronouns from the ACEFebruary 2002 evaluation set.
The annotated pro-nouns in both sets include only those that are ACE?markables?, i.e., ones that refer to entities of the fol-lowing types: Persons, Organizations, GeoPo-liticalEntities (politically defined geographicalregions, their governments, or their people), Loca-tions, and Facilities.The system employs a set of hard constraintsand soft features.
The hard constraints filter outthose noun groups that fail conservative number andgender agreement checks before training, whereas thesoft features are used by the MaxEnt algorithm.
Aset of forty soft features were developed and opti-mized manually; they fall into five categories thathave become fairly standard in the literature:Gender Agreement: Includes features to test astrict match of gender (e.g., a masculine pro-noun and a masculine antecedent), as well asmere compatibility (e.g., a masculine pronounwith an antecedent of unknown gender).
Thesefeatures are more liberal than the gender-basedhard constraint mentioned above.Number Agreement: Includes features to test astrict match of number (e.g., a singular pronounand a singular antecedent), as well as mere com-patibility (e.g., a singular pronoun with an an-tecedent of unknown number).
These featuresare likewise more liberal than the number-basedhard constraint mentioned above.Distance: Includes features pertaining to the dis-tance between the pronoun and the potential an-tecedent.
Examples include the number of sen-tences between them and the ?Hobbs distance?,that is, the number of noun groups that haveto be skipped before the potential antecedent isfound per the search order used by the Hobbsalgorithm (Hobbs, 1978; Ge et al, 1998).Grammatical Role: Includes features pertainingto the syntactic position of the potential an-tecedent.
Examples include whether the poten-tial antecedent appears to be the subject or ob-ject of a verb, and whether the potential an-tecedent is embedded in a prepositional phrase.Linguistic Form: Includes features pertaining tothe referential form of the potential antecedent,e.g., whether it is a proper name, definite de-scription, indefinite NP, or a pronoun.The values of these features ?
computed fromTextPro?s errorful shallow constituent parses ?comprised the input to the learning algorithm, alongwith the outcome as indicated by the annotated key.3 The Self-Trained AlgorithmThe self-trained algorithm likewise uses MaxEnt,with the same feature set and shallow parser.
Thetwo systems differ in the training data utilized.Instead of the training corpus of 2773 annotatedpronouns used in the supervised experiments, theself-trained algorithm creates training data frompronouns found in a raw corpus, particularly thenewswire segment of the Topic Detection and Track-ing (TDT-2) corpus.
The system was evaluated onthe same annotated set of 762 pronouns as the su-pervised system; the performance statistics reportedherein are from the only time an evaluation with thisdata was carried out.The self-trained system embeds the MaxEnt algo-rithm in an iterative loop during which the trainingexamples are acquired.
The first phase of the algo-rithm builds an initial model as follows:1.
For each third-person pronoun:(a) Collect possible antecedents, that is, all ofthe noun groups found in the previous twosentences and to the left of the pronoun inthe current sentence.
(b) Filter them by applying the hard con-straints.
(c) If only one possible antecedent remains,create a pronoun-antecedent pair and labelthe coreference outcome as True.
(d) Otherwise, with some probability (0.2in our experiments1), create a pronoun-antecedent pair for each possible antecedentand label the coreference outcome as False.2.
Train a MaxEnt classifier on this training data.The simplification assumed above ?
that corefer-ence holds for all and only those pronouns for whichTextPro and hard constraints find a single possi-ble antecedent ?
is obviously false, but it nonethelessyields a model to seed the iterative part of the algo-rithm, which goes as follows:3.
For each pronoun in the training data acquiredin step 1:(a) Apply the current MaxEnt model to eachpronoun-antecedent pair.
(b) Label the pair to which the model assignsthe highest probability the coreference out-come of True.
Label all other pairs (if any)for that pronoun the outcome of False.4.
Retrain the MaxEnt model with this new train-ing data.5.
Repeat steps 3 and 4 until the training datareaches a steady state, that is, there are nopronouns for which the current model changesits preference to a different potential antecedentthan it favored during the previous iteration.1This choice will be explained in Section 5.The hope is that improved predictions about whichpotential antecedents of ambiguous pronouns are cor-rect will yield iteratively better models (note that the?unambiguous?
pronoun-antecedent pairs collectedin step 1c will be considered to be correct through-out).
This hope is notwithstanding the fact that thealgorithm is based on a simplifying assumption ?
thateach pronoun is associated with exactly one correctantecedent ?
that is clearly false for a variety of rea-sons: (i) there will be cases in which there is morethan one coreferential antecedent in the search win-dow, all but one of which will get labeled as not coref-erential during any given iteration, (ii) there will becases in which the (perhaps only) correct antecedentwas misparsed or incorrectly weeded out by hard con-straints, and thus not seen by the learning algorithm(presumably some of the ?unambiguous?
cases iden-tified in step 1c will be incorrect because of this),and (iii) some of the pronouns found will not even bereferential, e.g.
pleonastic pronouns.
The empiricalquestion remains, however, of how good of a systemcan be trained under such an assumption.
After all,the model probabilities need not necessarily be accu-rate in an absolute sense, but only in a relative one:that is, good enough so that the antecedent assignedthe highest probability tends to be correct.4 Hobbs BaselineFor comparison purposes, we also implemented a ver-sion of Hobbs?s (1978) well-known pronoun interpre-tation algorithm, in which no machine learning isinvolved.
This algorithm takes the syntactic repre-sentations of the sentences up to and including thecurrent sentence as input, and performs a search foran antecedent noun phrase on these trees.
SinceTextPro does not build full syntactic trees for theinput, we developed a version that does a simplesearch through the list of noun groups recognized.In accordance with Hobbs?s search procedure, noungroups are searched in the following order: (i) in thecurrent sentence from right-to-left, starting with thefirst noun group to the left of the pronoun, (ii) in theprevious sentence from left-to-right, (iii) in two sen-tences prior from left-to-right, (iv) in the current sen-tence from left-to-right, starting with the first noungroup to the right of the pronoun (for cataphora).The first noun group encountered that agrees withthe pronoun with respect to number, gender, andperson is chosen as the antecedent.5 ResultsReporting on the results of a self-trained systemmeans only evaluating the system against annotateddata once, since any system reconfiguration and re-evaluation based on the feedback received would con-stitute a form of indirectly supervised training.
Thuswe had to select a configuration as representing our?reportable?
system before doing any evaluation.
Toallow for the closest comparison with our supervisedsystem, we opted to train the system with the samenumber of pronouns that we had in our supervisedtraining set (2773), and sought to have approxi-mately the same ratio of positive to negative traininginstances, which meant randomly including one-fifthof the pronouns in the raw data that had more thanone possible antecedent (see step 1d).
Later we re-port on post-hoc experiments to assess the effect oftraining data size on performance.The self-trained system was trained fourteentimes, once using each of fourteen different segmentsof the TDT-2 data that we had arbitrarily appor-tioned at the inception of the project.
The scoresreported below and in Table 1 for the self-trainedsystem are averages of the fourteen correspondingevaluations.
The final results are as follows:?
Hobbs Baseline: 68.8%?
Self-Trained: 73.4%?
Supervised: 75.7%The self-trained system beats the competitive Hobbsbaseline system by 4.6% and comes within 2.3% ofthe supervised system trained on the same numberof manually-annotated pronouns.2Convergence for the self-trained system was fairlyrapid, taking between 8 and 14 iterations.
The num-ber of changes in the current model?s predictionsstarted off relatively high in early iterations (aver-aging approximately 305 pronouns or 11% of thedataset) and then steadily declined (usually, but notalways, monotonically) until convergence.
Post-hoc2All results are reported here in terms of accuracy,that is, the number of pronouns correctly resolved dividedby the total number of pronouns read in from the key.
Anantecedent is considered correct if the ACE keys place thepronoun and antecedent in the same coreference class.In the case of 64 of the 762 pronouns in the evaluationset, none of the antecedents input to the learning algo-rithms were coreferential.
Thus, 91.6% accuracy is thebest that these algorithms could have achieved.In Kehler et al (2004) we describe two ways in whichour supervised system was augmented to use predicate-argument frequencies, one which used them in a post-processor and another which modeled them with featuresalongside our morphosyntactic ones.
In our self-trainedsystem, the first of these methods improved performanceto 75.1% (compared to 76.8% for the supervised system)and the second to 74.1% (compared to 75.7% for the su-pervised system).Number of Pronouns Blind Test Performance55 71.4%138 72.3%277 72.5%554 72.6%1386 73.5%2773 73.4%5546 73.5%Full Segment 73.7%Table 1: Effect of Training Data Size on Blind TestPerformanceanalysis showed that the iterative phase contributeda gradual (although again not completely monotonic)improvement in performance during the course oflearning.We then performed a set of post-hoc experimentsto measure the effect of training data size on perfor-mance for the self-trained system.
The results aregiven in Table 1, which show a gradual increase inperformance as the number of pronouns grows.
Thefinal row includes the results when all of the ?un-ambiguous?
pronouns in each TDT segment are uti-lized (again, along with approximately one-fifth ofthe ambiguous pronouns), which amounted to be-tween 7,212 and 11,245 total pronouns.3 (Note thatsince most pronouns have more than one possibleantecedent, the number of pronoun-antecedent train-ing examples fed to MaxEnt is considerably higherthan the numbers of pronouns shown in the table.
)Perhaps one of the more striking facts is how wellthe algorithm performs with relatively few pronouns,which suggests that the generality of the featuresused allow for fairly reliable estimation without muchdata.6 ConclusionTo conclude, a pronoun interpretation system canbe trained solely on raw data using a standard setof morphosyntactic features to achieve performancethat approaches that of a state-of-the-art supervisedsystem.
Although the self-acquired training data isno doubt highly noisy, the resulting model is stillaccurate enough to perform well at selecting correctantecedents.
As a next step, we will take a closerlook at the training data acquired to try to ascertain3TDT segment 14, which is smaller than the others,provided only about 3800 pronouns in the runs corre-sponding to the last two rows of Table 1.
The overallaverage performance figures are the same to the first dec-imal place whether or not the results from this segmentare included.the underlying reasons for this success.There are also a number of variants of the algo-rithm that could be pursued.
For instance, whereasour algorithm uses the current model?s probabilitiesin a winner-take-all strategy for positive example se-lection, these probabilities could instead be used todictate the likelihood that examples are assigned apositive outcome, or they could be thresholded invarious ways to create a more discerning positive out-come assignment mechanism.
Such strategies wouldavoid the current simplification of assigning a posi-tive outcome to exactly one potential antecedent foreach pronoun.The relative generality of our feature set was ap-propriate given the size of the data sets used.
Theavailability of very large raw corpora, however, cre-ates the prospect of using self-training with consider-ably more fine-grained features than is possible in asupervised scenario, due to the relative infrequencywith which they would be found in any corpus of asize that could be feasibly annotated manually.
Itis thus at least conceivable that a self-trained ap-proach, coupled with a large set of features and alarge corpus of raw data, could eventually overtakethe performance of the best supervised models.AcknowledgmentsThis work was supported by the ACE program(www.nist.gov/speech/tests/ACE/).ReferencesAdam Berger, Stephen A. Della Pietra, and Vin-cent J. Della Pietra.
1996.
A maximum entropyapproach to natural language processing.
Compu-tational Linguistics, 22(1):39?71.Niyu Ge, John Hale, and Eugene Charniak.
1998.A statistical approach to anaphora resolution.
InProceedings of the Sixth Workshop on Very LargeCorpora, Montreal, Quebec.Jerry R. Hobbs.
1978.
Resolving pronoun references.Lingua, 44:311?338.Andrew Kehler, Douglas Appelt, Lara Taylor, andAleksandr Simma.
2004.
The (non)utility ofpredicate-argument frequencies for pronoun inter-pretation.
In Proceedings of HLT/NAACL-04,Boston, MA.Ruslan Mitkov.
2002.
Anaphora Resolution.
Long-man, London.
