Grammar Factorization byTree DecompositionDaniel Gildea?University of RochesterWe describe the application of the graph-theoretic property known as treewidth to the problem offinding efficient parsing algorithms.
This method, similar to the junction tree algorithm used ingraphical models for machine learning, allows automatic discovery of efficient algorithms suchas the O(n4) algorithm for bilexical grammars of Eisner and Satta.
We examine the complexityof applying this method to parsing algorithms for general Linear Context-Free Rewriting Sys-tems.
We show that any polynomial-time algorithm for this problem would imply an improvedapproximation algorithm for the well-studied treewidth problem on general graphs.1.
IntroductionIn this article, we describe meta-algorithms for parsing: algorithms for finding theoptimal parsing algorithm for a given grammar, with the constraint that rules in thegrammar are considered independently of one another.
In order to have a commonrepresentation for our algorithms to work with, we represent parsing algorithmsas weighted deduction systems (Shieber, Schabes, and Pereira 1995; Goodman 1999;Nederhof 2003).
Weighted deduction systems consist of axioms and rules for buildingitems or partial results.
Items are identified by square brackets, with their weightswritten to the left.
Figure 1 shows a rule for deducing a new item when parsing acontext free grammar (CFG) with the rule S?
AB.
The item below the line, calledthe consequent, can be derived if the two items above the line, called the antecedents,have been derived.
Items have types, corresponding to grammar nonterminals in thisexample, and variables, whose values range over positions in the string to be parsed.We restrict ourselves to items containing position variables directly as arguments; noother functions or operations are allowed to apply to variables.
The consequent?s weightis the product of the weights of the two antecedents and the rule weight w0.
Implicit inthe notation is the fact that we take themaximumweight over all derivations of the sameitem.
Thus, the weighted deduction system corresponds to the Viterbi or max-productalgorithm for parsing.
Applications of the same weighted deduction system with othersemirings are also possible (Goodman 1999).The computational complexity of parsing depends on the total number of instanti-ations of variables in the system?s deduction rules.
If the total number of instantiationsis M, parsing is O(M) if there are no cyclic dependencies among instantiations, or,equivalently, if all instantiations can be sorted topologically.
In most parsing algorithms,?
Computer Science Department, University of Rochester, Rochester NY 14627.E-mail: gildea@cs.rochester.edu.Submission received: 28 June 2010; revised submission received: 20 September 2010; accepted for publication:21 October 2010.?
2011 Association for Computational LinguisticsComputational Linguistics Volume 37, Number 1w1: [A, x0, x1]w2: [B, x1, x2]w0w1w2: [S, x0, x2]Figure 1CFG parsing in weighted deduction notation.variables range over positions in the input string.
In order to determine complexity inthe length n of the input string, it is sufficient to count the number of unique positionvariables in each rule.
If all rules have at most k position variables, M = O(nk), andparsing takes timeO(nk) in the length of the input string.
In the remainder of this article,we will explore methods for minimizing k, the largest number of position variables inany rule, among equivalent deduction systems.
These methods directly minimize theparsing complexity of the resulting deduction system.
Although we will assume nocyclic dependencies among rule instantiations for the majority of the article, we willdiscuss the cyclic case in Section 2.2.It is often possible to improve the computational complexity of a deduction ruleby decomposing the computation into two or more new rules, each having a smallernumber of variables than the original rule.
We refer to this process as factorization.
Onestraightforward example of rule factorization is the binarization of a CFG, as shown inFigure 2.
Given a deduction rule for a CFG rule with r nonterminals on the righthandside, and a total of r+ 1 variables, an equivalent set of rules can be produced, each withthree variables, storing intermediate results that indicate that a substring of the originalrule?s righthand side has been recognized.
This type of rule factorization produces anO(n3) parser for any input CFG.Another well-known instance of rule factorization is the hook trick of Eisner andSatta (1999), which reduces the complexity of parsing for bilexicalized CFGs fromO(n5) to O(n4).
The basic rule for bilexicalized parsing combines two CFG constituentsmarked with lexical heads as shown in Figure 3a.
Here items with type C indicateconstituents, with [C, x0, h, x1] indicating a constituent extending from position x0 toposition x1, headed by the word at position h. The item [D,m?
h] is used to indicatethe weight assigned by the grammar to a bilexical dependency headed by the word ata)w1: [A, x0, x1]w2: [B, x1, x2]w3: [C, x2, x3]w4: [D, x3, x4]w0w1w2w3w4: [S, x0, x4]b)w1: [A, x0, x1]w2: [B, x1, x2]w1w2: [X, x0, x2]w5: [X, x0, x2]w3: [C, x2, x3]w3w5: [Y, x0, x3]w6: [Y, x0, x3]w3: [D, x3, x4]w0w3w6: [S, x0, x3]Figure 2Binarization of the CFG rule S?
ABCD as rule factorization: The deduction rule above can befactored into the three equivalent rule below.232Gildea Grammar Factorization by Tree Decompositiona)w: [D,m?
h]w1: [C, x0, h, x1]w2: [C, x1,m, x2]ww1w2: [C, x0, h, x2]b)w: [D,m?
h]w2: [C, x1,m, x2]ww2: [H, h, x1, x2]wh: [H, h, x1, x2]w1: [C, x0, h, x1]whw1: [C, x0, h, x2]Figure 3Rule factorization for bilexicalized parsing.position h with the word at position m as a modifier.
The deduction rule is broken intotwo steps, one which includes the weight for the bilexical grammar rule, and anotherwhich identifies the boundaries of the new constituent, as shown in Figure 3b.
The hooktrick has also been applied to Tree Adjoining Grammar (TAG; Eisner and Satta 2000),and has been generalized to improve the complexity of machine translation decodingunder synchronous context-free grammars (SCFGs) with an n-gram language model(Huang, Zhang, and Gildea 2005).Rule factorization has also been studied in the context of parsing for SCFGs.
Unlikemonolingual CFGs, SCFGs cannot always be binarized; depending on the permutationbetween nonterminals in the two languages, it may or may not be possible to reduce therank, or number of nonterminals on the righthand side, of a rule.
Algorithms for findingthe optimal rank reduction of a specific rule are given by Zhang and Gildea (2007).
Thecomplexity of synchronous parsing for a rule of rank r is O(n2r+2), so reducing rankimproves parsing complexity.Rule factorization has also been applied to Linear Context-Free Rewriting Systems(LCFRS), which generalize CFG, TAG, and SCFG to define a rewriting system wherenonterminals may have arbitrary fan-out, which indicates the number of continuousspans that a nonterminal accounts for in the string (Vijay-Shankar, Weir, and Joshi 1987).Recent work has examined the problem of factorization of LCFRS rules in order toreduce rank without increasing grammar fan-out (Go?mez-Rodr?
?guez et al 2009), as wellas factorization with the goal of directly minimizing the parsing complexity of the newgrammar (Gildea 2010).We define factorization as a process which applies to rules of the input grammarindependently.
Individual rules are replaced with an equivalent set of new rules, whichmust derive the same set of consequent items as the original rule given the same an-tecedent items.
While new intermediate items of distinct types may be produced, the setof items and weights derived by the original weighted deduction system is unchanged.This definition of factorization is broad enough to include all of the previous examples,but does not include, for example, the fold/unfold operation applied to grammars byJohnson (2007) and Eisner and Blatz (2007).
Rule factorization corresponds to the unfoldoperation of fold/unfold.If we allow unrestricted transformations of the input deduction system, finding themost efficient equivalent system is undecidable; this follows from the fact that it is un-decidable whether a CFG generates the set of all strings (Bar-Hillel, Perles, and Shamir1961), and would therefore be recognizable in constant time.
Whereas the fold/unfoldoperation of Johnson (2007) and Eisner and Blatz (2007) specifies a narrower class of233Computational Linguistics Volume 37, Number 1grammar transformations, no general algorithms are known for identifying an optimalseries of transformations in this setting.
Considering input rules independently allowsus to provide algorithms for optimal factorization.In this article, we wish to provide a general framework for factorization of deduc-tive parsing systems in order to minimize computational complexity.
We show howto apply the graph-theoretic property of treewidth to the factorization problem, andexamine the question of whether efficient algorithms exist for optimizing the parsingcomplexity of general parsing systems in this framework.
In particular, we show thatthe existence of a polynomial time algorithm for optimizing the parsing complexity ofgeneral LCFRS rules would imply an improved approximation algorithm for the well-studied problem of treewidth of general graphs.2.
Treewidth and Rule FactorizationIn this section, we introduce the graph-theoretic property known as treewidth, andshow how it can be applied to rule factorization.A tree decomposition of a graph G = (V,E) is a type of tree having a subset of G?svertices at each node.
We define the nodes of this tree T to be the set I, and its edgesto be the set F. The subset of V associated with node i of T is denoted by Xi.
A treedecomposition is therefore defined as a pair ({Xi | i ?
I},T = (I,F)) where each Xi, i ?
Iis a subset of V, and tree T has the following properties: Vertex cover: The nodes of the tree T cover all the vertices of G:?i?I Xi = V. Edge cover: Each edge in G is included in some node of T. That is, for alledges (u, v) ?
E, there exists an i ?
I with u, v ?
Xi. Running intersection: The nodes of T containing a given vertex of G form aconnected subtree.
Mathematically, for all i, j, k ?
I, if j is on the (unique)path from i to k in T, then Xi?Xk ?
Xj.The treewidth of a tree decomposition ({Xi},T) is maxi |Xi| ?
1.
The treewidth of agraph is the minimum treewidth over all tree decompositions:tw(G) = min({Xi},T)?TD(G)maxi|Xi| ?
1where TD(G) is the set of valid tree decompositions of G. We refer to a tree decomposi-tion achieving the minimum possible treewidth as being optimal.In general, more densely interconnected graphs have higher treewidth.
Any treehas treewidth = 1; a graph consisting of one large cycle has treewidth = 2, and a fullyconnected graph of n vertices has treewidth= n?
1.
Low treewidth indicates some tree-like structure in the graph, as shown by the example with treewidth = 2 in Figure 4.
Asan example of the running intersection property, note that the vertex N appears in threeadjacent nodes of the tree decomposition.
Finding the treewidth of a graph is an NP-complete problem (Arnborg, Corneil, and Proskurowski 1987).
However, given a graphof n vertices and treewidth k, a simple algorithm finds the optimal tree decomposition intime O(nk+2) (Arnborg, Corneil, and Proskurowski 1987), and a variety of approxima-tion algorithms and heuristics are known for the treewidth problem (Bodlaender et al1995; Amir 2001; Feige, Hajiaghayi, and Lee 2005).
Furthermore, for fixed k, optimal treedecompositions can be computed in linear time (Bodlaender 1996).234Gildea Grammar Factorization by Tree DecompositionFigure 4A tree decomposition of a graph is a set of overlapping clusters of the graph?s vertices, arrangedin a tree.
This example has treewidth = 2.We can factorize a deduction rule by representing the rule as a graph, which wecall a dependency graph, and searching for tree decompositions of this graph.
For arule r having n variables V = {vi | i ?
{1, .
.
.
,n}}, m antecedent items Ai, i ?
{1, .
.
.
,m},and consequent C, let V(Ai) ?
V be the variables appearing in antecedent Ai, and V(C)be the variables appearing in the consequent.
The dependency graph representation ofthe rule is Gr = (V,E =?S:A1,...,Am,C{(vi, vj) | vi, vj ?
V(S)}).
That is, we have a vertex foreach variable in the rule, and connect any two vertices that appear together in the sameantecedent, or that appear together in the consequent.The dependency graph representation allows us to prove the following result con-cerning parsing complexity:Theorem 1Given a deduction rule r for parsing where the input string is referenced only throughposition variables appearing as arguments of antecedent and consequent items, the opti-mal complexity of any factorization of rule r isO(ntw(Gr )+1), where Gr is the dependencygraph derived from r.ProofOne consequence of the definition of a tree decomposition is that, for any clique appear-ing in the original graph Gr, there must exist a node in the tree decomposition T whichcontains all the vertices in the clique.
We use this fact to show that there is a one-to-one correspondence between tree decompositions of a rule?s dependency graph Gr andfactorizations of the rule.First, we need to show that any tree decomposition of Gr can be used as a factoriza-tion of the original deduction rule.
By our earlier definition, a factorization must derivethe same set of consequent items from a given set of antecedent items as the originalrule.
Because Gr includes a clique connecting all variables in the consequent C, the treedecomposition Tmust have a node Xc such that V(C) ?
Xc.
We consider this node to bethe root of T. The original deduction rule can be factorized into a new set of rules, onefor each node in T. For node Xc, the factorized rule has C as a consequent, and all othernodes Xi have a new partial result as a consequent, consisting of the variables Xi ?
Xj,where Xj is Xi?s neighbor on the path to the root node Xc.
We must guarantee that thefactorized rule set yields the same result as the original rule, namely, the semiring sum235Computational Linguistics Volume 37, Number 1over all variable values of the semiring product of the antecedents?
weights.
The treestructure of T corresponds to a factorization of this semiring expression.
For example, ifwe represent the CFG rule of Figure 2a with the generalized semiring expression:?x1x2x3A(x0, x1)?
B(x1, x2)?
C(x2, x3)?D(x3, x4)the factorization of this expression corresponding to the binarized rule is?x3(?x2(?x1A(x0, x1)?
B(x1, x2))?
C(x2, x3))?D(x3, x4)where semiring operations ?
and ?
have been interchanged as allowed by the depen-dency graph for this rule.Because each antecedent Ai is represented by a clique in the graph Gr, the treedecomposition T must contain at least one node which includes all variables V(Ai).We can choose one such node and multiply in the weight of Ai, given the values ofvariables V(Ai), at this step of the expression.
The running intersection property of thetree decomposition guarantees that each variable has a consistent value at each pointwhere it is referenced in the factorization.The same properties guarantee that any valid rule factorization corresponds to atree decomposition of the graph Gr.
We consider the tree decomposition with a set Xifor each new rule ri, consisting of all variables used in ri, and with tree edges T definedby the producer/consumer relation over intermediate results in the rule factorization.Each antecedent of the original rule must appear in some new rule in the factorization,as must the consequent of the original rule.
Therefore, all edges in the original rule?sdependency graph Gr appear in some tree node Xi.
Any variable that appears in tworules in the factorizationmust appear in all intermediate rules in order to ensure that thevariable has a consistent value in all rules that reference it.
This guarantees the runningintersection property of the tree decomposition ({Xi},T).
Thus any rule factorization,when viewed as a tree of sets of variables, has the properties that make it a valid treedecomposition of Gr.The theorem follows as a consequence of the one-to-one correspondence betweenrule factorizations and tree decompositions.
2.1 Computational ComplexityFactorization produces, for each input rule having m antecedents, at most m?
1 newrules, each containing at most the same number of nonterminals and the same numberof variables as the input rule.
Hence, the size of the new factorized grammar is O(|G|2),andwe avoid any possibility of an exponential increase in grammar size.
Tighter boundscan be achieved for specific classes of input grammars.The computational complexity of optimal factorization with tree decomposition isexponential in the size of the input rules.
However, optimal factorization is generallyfeasible whenever parsing with the unfactorized grammar is feasible.
This is because,for an input rule with  variables, parsing is O(n) in the sentence length n. Thetreewidth of this rule is at most ?
1, and can be computed in timeO(+1); generally weexpect n to be greater than .
One may also wish to accept only rules having treewidthk and disregard the remainder, for example, when factorizing rules automatically236Gildea Grammar Factorization by Tree Decompositionextracted from word-aligned bitext (Wellington, Waxmonsky, and Melamed 2006;Huang et al 2009) or from dependency treebanks (Kuhlmann and Nivre 2006; Gildea2010).
In this setting, the rules having treewidth k can be identified in timeO(k+2) usingthe simple algorithm of Arnborg, Corneil, and Proskurowski (1987), (where again is the number of variables in the input rules), or in time O() using the algorithm ofBodlaender (1996).2.2 Cyclic DependenciesAlthough this article primarily addresses the case where there are no cyclic dependen-cies between rule instantiations, we note here that our techniques carry over to thecyclic case under certain conditions.
If there are cycles in the rule dependencies, butthe semiring meets Knuth?s (1977) definition of a superior function, parsing takes timeO(M logM), where M is the number of rule instantiations, and the extra logM termaccounts for maintaining an agenda as a priority queue (Nederhof 2003).
Cycles inthe rule dependencies may arise, for example, from chains of unary productions in aCFG; the properties of superior functions guarantee that unbounded chains need notbe considered.
The max-product semiring used in Viterbi parsing has this property,assuming that all rule weights are less than one, whereas for exact computation withthe sum-product semiring, unbounded chains must be considered.
As in the acycliccase, M = O(nk) for parsing problems where rules have at most k variables.
Underthe assumption of superior functions, parsing takes time O(nkk log n) with Knuth?salgorithm.
In this setting, as in the acyclic case, minimizing k with tree decompositionminimizes parsing complexity.2.3 Related Applications of TreewidthThe technique of using treewidth to minimize complexity has been applied to constraintsatisfaction (Dechter and Pearl 1989), graphical models in machine learning (Jensen,Lauritzen, and Olesen 1990; Shafer and Shenoy 1990), and query optimization fordatabases (Chekuri and Rajaraman 1997).
Our formulation of parsing is most closelyrelated to logic programming; in this area treewidth has been applied to limit complex-ity in settings where either the deduction rules or the input database of ground factshave fixed treewidth (Flum, Frick, and Grohe 2002).
Whereas Flum, Frick, and Grohe(2002) apply treewidth to nonrecursive datalog programs, our parsing programs haveunbounded recursion, as the depth of the parse tree is not fixed in advance.
Our resultsfor parsing can be seen as a consequence of the fact that, even in the case of unboundedrecursion, the complexity of (unweighted) datalog programs is linear in the number ofpossible rule instantiations (McAllester 2002).3.
Examples of Treewidth for ParsingIn this section, we show how a few well-known parsing algorithms can be derivedautomatically by finding the optimal tree decomposition of a dependency graph.To aid in visualization of the graphical representation of deduction rules, we use afactor graph representation based on that of Kschischang, Frey, and Loeliger (2001) forMarkov Random Fields.
Our graphs have three types of nodes: variables, antecedents,and consequents.
Each antecedent node is connected to the variables it contains, andrepresents the antecedent?s weight as a function of those variables.
Antecedent nodesare analogous to the factor nodes of Kschischang, Frey, and Loeliger (2001), and237Computational Linguistics Volume 37, Number 1Figure 5Factor graph for the binary CFG deduction rule of Figure 1.consequent nodes are a new feature of this representation.
We can think of consequentsas factors with weight = 1; they do not affect the weights computed, but serve toguarantee that the consequent of the original rule can be found in one node of the treedecomposition.
We refer to both antecedent and consequent nodes as factor nodes.
Re-placing each factor node with a clique over its neighbor variables yields the dependencygraph Gr defined earlier.
We represent variables with circles, antecedents with squareslabeled with the antecedent?s weight, and consequents with diamonds labeled c. Anexample factor graph for the simple CFG rule of Figure 1 is shown in Figure 5.3.1 CFG BinarizationFigure 6a shows the factor graph derived from the monolingual CFG rule with fourchildren in Figure 2a.
The dependency graph obtained by replacing each factor witha clique of size 2 (a single edge) is a graph with one large cycle, shown in Figure 6b.Finding the optimal tree decomposition yields a tree with nodes of size 3, {x0, xi, xi+1}for each i, shown in Figure 6c.
Each node in this tree decomposition corresponds to oneof the factored deduction rules in Figure 2b.
Thus, the tree decomposition shows us howFigure 6Treewidth applied to CFG binarization.238Gildea Grammar Factorization by Tree Decompositionto parse in time O(n3); finding the tree decomposition of a long CFG rule is essentiallyequivalent to converting to Chomsky Normal Form.3.2 The Hook TrickThe deduction rule for bilexicalized parsing shown in Figure 3a translates into the factorgraph shown in Figure 7a.
Factor nodes are created for the two existing constituentsfrom the chart, with the first extending from position x0 in the string to x1, and thesecond from x1 to x2.
Both factor nodes are connected not only to the start and endpoints, but also to the constituent?s head word, h for the first constituent and m forthe second (we show the construction of a left-headed constituent in the figure).
Anadditional factor is connected only to h and m to represent the bilexicalized rule weight,expressed as a function of h and m, which is multiplied with the weight of the twoexisting constituents to derive the weight of the new constituent.
The new constituentis represented by a consequent node at the top of the graph?the variables that will berelevant for its further combination with other constituents are its end points x0 and x2and its head word h.Placing an edge between each pair of variable nodes that share a factor, we get Fig-ure 7b.
If we compute the optimal tree decomposition for this graph, shown in Figure 7c,each of the two nodes corresponds to one of the factored rules in Figure 3b.
The largestnode of the tree decomposition has four variables, giving the O(n4) algorithm of Eisnerand Satta (1999).3.3 SCFG Parsing StrategiesSCFGs generalize CFGs to generate two strings with isomorphic hierarchical structuresimultaneously, and have become widely used as statistical models of machine transla-tion (Galley et al 2004; Chiang 2007).
We write SCFG rules as productions with oneFigure 7Treewidth applied to bilexicalized parsing.239Computational Linguistics Volume 37, Number 1lefthand side nonterminal and two righthand side strings.
Nonterminals in the twostrings are linkedwith superscript indices; symbols with the same indexmust be furtherrewritten synchronously.
For example,X ?
A(1) B(2) C(3)D(4), A(1) B(2) C(3)D(4) (1)is a rule with four children and no reordering, whereasX ?
A(1) B(2) C(3)D(4), B(2)D(4) A(1) C(3) (2)expresses a more complex reordering.
In general, we can take indices in the firstrighthand-side string to be consecutive, and associate a permutation ?
with the secondstring.
If we use Xi for 0 ?
i ?
n as a set of variables over nonterminal symbols (forexample,X1 andX2 may both stand for nonterminalA), we canwrite rules in the generalform:X0 ?
X(1)1 ?
?
?X(n)n , X(?(1))?
(1) ?
?
?X(?(n))?
(n)Unlike monolingual CFGs, SCFGs cannot always be binarized.
In fact, the lan-guages of string pairs generated by a synchronous grammar can be arranged in aninfinite hierarchy, with each rank?
4 producing languages not possible with grammarsrestricted to smaller rules (Aho and Ullman 1972).
For any grammar with maximumrank r, converting each rule into a single deduction rule yields an O(n2r+2) parsingalgorithm, because there are r+ 1 boundary variables in each language.
More efficientparsing algorithms are often possible for specific permutations, and, by Theorem 1, thebest algorithm for a permutation can be found by computing the minimum-treewidthtree decomposition of the graph derived from the SCFG deduction rule for a specificpermutation.
For example, for the non-binarizable rule of Equation (2), the resultingfactor graph is shown in Figure 8a, where variables x0, .
.
.
, x4 indicate position variablesin one language of the synchronous grammar, and y0, .
.
.
, y4 are positions in the otherlanguage.
The optimal tree decomposition for this rule is shown in Figure 8c.
For thispermutation, the optimal parsing algorithm takes time O(n8), because the largest nodein the tree decomposition of Figure 8c includes eight position variables.
This result isintermediate between the O(n6) for binarizable SCFGs, also known as Inversion Trans-duction Grammars (Wu 1997), and theO(n10) that we would achieve by recognizing therule in a single deduction step.Gildea and S?tefankovic?
(2007) use a combinatorial argument to show that as thenumber of nonterminals r in an SCFG rule grows, the parsing complexity grows as?
(ncr) for some constant c. In other words, some very difficult permutations exist of alllengths.It is interesting to note that although applying the tree decomposition techniqueto long CFG rules results in a deduction system equivalent to a binarized CFG, theindividual deduction steps in the best parsing strategy for an SCFG rule do not ingeneral correspond to SCFG rules.
This is because the intermediate results may includemore than one span in each language.
These intermediate deduction steps do, however,correspond to LCFRS rules.
We now turn to examine LCFRS in more detail.240Gildea Grammar Factorization by Tree DecompositionFigure 8Treewidth applied to the SCFG rule of Equation (2).4.
LCFRS Parsing StrategiesLCFRS provides a generalization of a number of widely used formalisms in naturallanguage processing, including CFG, TAG, SCFG, and synchronous TAG.
LCFRS hasalso been used to model non-projective dependency grammars, and the LCFRS rulesextracted from dependency treebanks can be quite complex (Kuhlmann and Satta2009), making factorization important.
Similarly, LCFRS can model translation relationsbeyond the power of SCFG (Melamed, Satta, and Wellington 2004), and grammarsextracted from word-aligned bilingual corpora can also be quite complex (Wellington,Waxmonsky, and Melamed 2006).
An algorithm for factorization of LCFRS rules ispresented by Gildea (2010), exploiting specific properties of LCFRS.
The tree decompo-sition method achieves the same results without requiring analysis specific to LCFRS.In this section, we examine the complexity of rule factorization for general LCFRSgrammars.The problem of finding the optimal factorization of an arbitrary deduction rule isNP-complete.
This follows from the NP-completeness of treewidth using the followingconstruction: Given a graph, create a deduction rule with a variable for each vertex inthe graph and an antecedent for each edge, containing the two variables associated withthe edge?s endpoints.
The graphs produced by LCFRS grammar rules, however, havecertain properties which may make more efficient factorization algorithms possible.
Wefirst define LCFRS precisely before examining the properties of these graphs.241Computational Linguistics Volume 37, Number 1An LCFRS is defined as a tuple G = (VT,VN,P,S), where VT is a set of terminalsymbols, VN is a set of nonterminal symbols, P is a set of productions, and S ?
VN is adistinguished start symbol.
Associatedwith each nonterminal B is a fan-out?
(B), whichtells how many continuous spans B covers.
Productions p ?
P take the form:p : A?
g(B1,B2, .
.
.
,Br) (3)where A,B1, .
.
.
,Br ?
VN, and g is a functiong : (V?T )?
(B1) ?
?
?
?
?
(V?T )?
(Br ) ?
(V?T )?
(A)which specifies how to assemble the?ri=1?
(Bi) spans of the righthand side nontermi-nals into the?
(A) spans of the lefthand side nonterminal.
The function gmust be linearand non-erasing, which means that if we writeg(?s1,1, .
.
.
, s1,?
(B1 )?, .
.
.
, ?s1,1, .
.
.
, s1,?
(Br )?)
= ?t1, .
.
.
, t?
(A)?the tuple of strings ?t1, .
.
.
, t?(A)?
on the righthand side contains each variable si,j fromthe lefthand side exactly once, and may also contain terminals from VT.
The process ofgenerating a string from an LCFRS grammar can be thought of as first choosing, top-down, a production to expand each nonterminal, and then, bottom?up, applying thefunctions associated with each production to build the string.
As an example, the CFGS?
ABA?
aB?
bcorresponds to the following grammar in LCFRS notation:S?
gS(A,B) gS(?sA?, ?sB?)
= ?sAsB?A?
gA() gA() = ?a?B?
gB() gB() = ?b?Here, all nonterminals have fan-out = 1, reflected in the fact that all tuples defining theproductions?
functions contain just one string.
As CFG is equivalent to LCFRS with fan-out = 1, SCFG and TAG can be represented as LCFRS with fan-out = 2.
Higher values offan-out allow strictly more powerful grammars (Rambow and Satta 1999).
Polynomial-time parsing is possible for any fixed LCFRS grammar, but the degree of the polynomialdepends on the grammar.
Parsing general LCFRS grammars, where the grammar isconsidered part of the input, is NP-complete (Satta 1992).4.1 Graphs Derived from LCFRS RulesGiven an LCFRS rule as defined previously, a weighted deduction rule for a bottom?up parser can be derived by creating an antecedent for each righthand nonterminal,a consequent for the lefthand side, and variables for all the boundaries of the non-terminals in the rule.
A nonterminal of fan-out f has 2f boundaries.
Each boundary242Gildea Grammar Factorization by Tree Decompositionvariable will occur exactly twice in the deduction rule: either in two antecedents, if twononterminals on the rule?s righthand side are adjacent, or once in an antecedent andonce in the consequent, if the variable indicates a boundary of any segment of the rule?slefthand side.Converting such deduction rules into dependency graphs, we see that the cliquesof the dependency graph may be arbitrarily large, due to the unbounded fan-out ofLCFRS nonterminals.
However, each vertex appears in only two cliques, because eachboundary variable in the rule is shared by exactly two nonterminals.
In the remainder ofthis section, we consider whether the problem of finding the optimal tree decompositionof this restricted set of graphs is also NP-complete, or whether efficient algorithms maybe possible in the LCFRS setting.4.2 Approximation of Treewidth for General GraphsWe will show that an efficient algorithm for finding the factorization of an arbitraryLCFRS production that optimizes parsing complexity would imply the existence of analgorithm for treewidth that returns a result within a factor of 4?
(G) of the optimum,where ?
(G) is the maximum degree of the input graph.
Although such an approxima-tion algorithm may be possible, it would require progress in fundamental problems ingraph theory.Consider an arbitrary graph G = (V,E), and define k to be its treewidth, k = tw(G).We wish to construct a new graph G?
= (V?,E?)
from G in such a way that tw(G?)
=tw(G) and every vertex in G?
has even degree.
This can be accomplished by doublingthe graph?s edges in the manner shown in Figure 9.
To double the edges, for every edgee = (u, v) in E, we add a new vertex e?
to G?
and add edges (u, e?)
and (v, e?)
to G?.
We alsoinclude every edge in the original graph G in G?.
Now, every vertex v in G?
has degree =2, if it is a newly created vertex, or twice the degree of v in G otherwise, and therefore?(G?)
= 2?
(G) (4)We now show that tw(G?)
= tw(G), under the assumption that tw(G) ?
3.
Any treedecomposition of G can be adapted to a tree decomposition of G?
by adding a nodecontaining {u, v, e?}
for each edge e in the original graph, as shown in Figure 10.
The newnode can be attached to a node containing u and v; because u and v are connected byan edge in G, such a node must exist in G?s tree decomposition.
The vertex e?
will notoccur anywhere else in the tree decomposition, and the occurrences of u and v still forma connected subtree.
For each edge e = (u, v) in G?, the tree decomposition must have anode containing u and v; this is the case because, if e is an original edge from G, thereis already a node in the tree decomposition containing u and v, whereas if e is an edgeto a newly added vertex in G?, one of the newly added nodes in the tree decompositionFigure 9An example graph Gex and the result G?ex of doubling Gex?s edges.243Computational Linguistics Volume 37, Number 1Figure 10Tree decompositions of Gex and G?ex.will contain its endpoints.
We constructed the new tree decomposition by adding nodesof size 3.
Therefore, as long as the treewidth of G was at least 3, tw(G?)
?
tw(G).
Inthe other direction, because G is a subgraph of G?, any tree decomposition of G?
forms avalid tree decomposition of G after removing the vertices in G?
?
G, and hence tw(G?)
?tw(G).
Therefore,tw(G?)
= tw(G) (5)Because every vertex in G?
has even degree, G?
has an Eulerian tour, that is, apath visiting every edge exactly once, beginning and ending at the same vertex.
Let?
= ?
?1, .
.
.
,?n?
be the sequence of vertices along such a tour, with ?1 = ?n.
Note thatthe sequence ?
contains repeated elements.
Let ?i, i ?
{1, .
.
.
,n} indicate how manytimes we have visited ?i on the ith step of the tour: ?i = |{j | ?j = ?i, j ?
{1, .
.
.
, i}}|.We now construct an LCFRS production P with |V?| righthand side nonterminals fromthe Eulerian tour:P : X ?
g(B1, .
.
.
,B|V?|)g(?s1,1, .
.
.
, s1,?
(B1)?, .
.
.
, ?s|V?|,1, .
.
.
, s|V?|,?
(B|V?| )?)
= ?s?1,?1 ?
?
?
s?n,?n?The fan-out ?
(Bi) of each nonterminal Bi is the number of times vertex i is visited on theEulerian tour.
The fan-out of the lefthand side nonterminal X is one, and the lefthandside is constructed by concatenating the spans of each nonterminal in the order specifiedby the Eulerian tour.For the example graph in Figure 9, one valid tour is?ex = ?A,B,C,D,F,C,E,A,G,B,H,C,A?This tour results in the following LCFRS production:Pex : X ?
gex(A,B,C,D,E,F,G,H)gex(?sA,1, sA,2, sA,3?, ?sB,1, sB,2?, ?sC,1, sC,2, sC,3?, ?sD,1?, ?sE,1?, ?sF,1?, ?sG,1?, ?sH,1?)
=?sA,1sB,1sC,1sD,1sF,1sC,2sE,1sA,2sG,1sB,2sH,1sC,3sA,3?We now construct dependency graph G??
from the LCFRS production P by applyingthe technique of Section 2.
G??
has n+ 1 vertices, corresponding to the beginning and244Gildea Grammar Factorization by Tree Decompositionend points of the nonterminals in P. The edges in G??
are formed by adding a clique foreach nonterminal in P connecting all its beginning and end points, that is,(2f2)edgesfor a nonterminal of fan-out f .
We must include a clique for X, the lefthand side ofthe production.
However, because the righthand side of the production begins andends with the same nonterminal, the vertices for the beginning and end points of Xare already connected, so the lefthand side does not affect the graph structure for theentire production.
By Theorem 1, the optimal parsing complexity of P is tw(G??
)+ 1.The graphs G?
and G??
are related in the following manner: Every edge in G?corresponds to a vertex in G?
?, and every vertex in G?
corresponds to a clique in G?
?.We can identify vertices in G??
with unordered pairs of vertices {u, v} in G?.
The edges inG??
are ({u, v}, {u,w}) ?u, v,w : u = v,u = w, v = w. An example of G??
derived from ourexample production Pex is shown in Figure 11.Any tree decomposition T??
of G??
can be transformed into a valid tree decomposi-tion T?
ofG?
by simply replacing each vertex in each node of T??
with both correspondingvertices in G?.
If T??
witnesses a tree decomposition of optimal width k??
= tw(G??
), eachnode in T??
will produce a node of size at most 2k??
in T?.
For any vertex v in G?, onenode in T??
must contain the clique corresponding to v in G??.
Each vertex {v,w} in G?
?must be found in a contiguous subtree of T?
?, and these subtrees all include the nodecontaining the clique for v. The occurrences of v in T?
are the union of these contiguoussubtrees, which must itself form a contiguous subtree.
Furthermore, each edge (u, v) inG?
corresponds to some vertex in G?
?, so u and v must occur together in some node ofT?.
Combining these two properties, we see that T?
is a valid tree decomposition of G?.From the construction, if SOL is the treewidth of T?, we are guaranteed thatSOL ?
2tw(G??)
(6)In the other direction, any tree decomposition T?
of G?
can be transformed into atree decomposition T??
of G??
by simply replacing each occurrence of vertex v in a nodeof T?
with all vertices {v,w} in T??.
The number of such vertices is the degree of v, ?
(v).Figure 11Dependency graph G?
?ex derived from the example of Figure 9.
Vertex #A corresponds to thebeginning of the Eulerian tour through G?ex and A# corresponds to the end of the tour; all othervertices correspond to edges in G?ex.245Computational Linguistics Volume 37, Number 1Each vertex {v,w} occurs in a contiguous subtree of T??
because v and w occurred incontiguous subtrees of T?, and had to co-occur in at least one node of T?.
Each edgein G??
comes from a clique for some vertex v in G?, so the edge has both its endpointsin any node of T??
corresponding to a node of T?
that contained v. Thus T??
is a valid treedecomposition of G??.
We expand each node in the tree decomposition by at most themaximum degree of the graph?(G?
), and thereforetw(G??)
?
?(G?)tw(G?)
(7)Assume that we have an efficient algorithm for computing the optimal parsingstrategy of an arbitrary LCFRS rule.
Consider the following algorithm for finding a treedecomposition of an input graph G: Transform G to G?
of even degree, and construct LCFRS production P froman Eulerian tour of G?. Find the optimal parsing strategy for P. Translate this strategy into a tree decomposition of G??
of treewidth k?
?, andmap this into a tree decomposition of G?, and then remove all new nodes e?to obtain a tree decomposition of G of treewidth SOL.If tw(G??)
= k?
?, we have SOL ?
2k??
from Equation (6), and k??
?
?(G?)tw(G?)
fromEquation (7).
Putting these together:SOL ?
2?(G?)tw(G?
)and using Equations (4) and (5) to relate our result to the original graph G,SOL ?
4?
(G)tw(G)This last inequality proves the main result of this sectionTheorem 2An algorithm for finding the optimal parsing strategy of an arbitrary LCFRS productionwould imply a 4?
(G) approximation algorithm for treewidth.Whether such an approximation algorithm for treewidth is possible is an open prob-lem.
The best-known result is the O(?log k) approximation result of Feige, Hajiaghayi,and Lee (2005), which improves on theO(log k) result of Amir (2001).
This indicates that,although polynomial-time factorization of LCFRS rules to optimize parsing complexitymay be possible, it would require progress on general algorithms for treewidth.5.
ConclusionWe have demonstrated that a number of techniques used for specific parsing prob-lems can be found algorithmically from declarative specifications of the grammar.Our method involves finding the optimal tree decomposition of a graph, which is ingeneral an NP-complete problem.
However, the relation to tree decomposition allowsus to exploit existing algorithms for this problem, such as the linear time algorithmof Bodlaender (1996) for graphs of bounded treewidth.
In practice, grammar rules are246Gildea Grammar Factorization by Tree Decompositiontypically small, and finding the tree decomposition is not computationally expensive,and in fact is trivial in comparison to the original parsing problem.
Given the specialstructure of the graphs derived from LCFRS productions, however, we have exploredwhether finding optimal tree decompositions of these graphs, and therefore optimalparsing strategies for LCFRS productions, is also NP-complete.
Although a polynomialtime algorithm for this problem would not necessarily imply that P = NP, it wouldrequire progress on fundamental, well-studied problems in graph theory.
Therefore, itdoes not seem possible to exploit the special structure of graphs derived from LCFRSproductions.AcknowledgmentsThis work was funded by NSF grantsIIS-0546554 and IIS-0910611.
We are gratefulto Giorgio Satta for extensive discussions ongrammar factorization, as well as forfeedback on earlier drafts from Mehdi HafeziManshadi, Matt Post, and four anonymousreviewers.ReferencesAho, Albert V. and Jeffery D. Ullman.
1972.The Theory of Parsing, Translation, andCompiling, volume 1.
Prentice-Hall,Englewood Cliffs, NJ.Amir, Eyal.
2001.
Efficient approximationfor triangulation of minimum treewidth.In 17th Conference on Uncertainty inArtificial Intelligence, pages 7?15,Seattle, WA.Arnborg, Stefen, Derek G. Corneil, andAndrzej Proskurowski.
1987.
Complexityof finding embeddings in a k-tree.
SIAMJournal of Algebraic and Discrete Methods,8:277?284.Bar-Hillel, Yehoshua, M. Perles, andE.
Shamir.
1961.
On formal properties ofsimple phrase structure grammars.Zeitschrift fu?r Phonetik, Sprachwissenschaftund Kommunikationsforschung, 14:143?172.Reprinted in Y. Bar-Hillel.
(1964).
Languageand Information: Selected Essays on TheirTheory and Application, Addison-WesleyReading, MA, pages 116?150.Bodlaender, H. L. 1996.
A linear timealgorithm for finding tree decompositionsof small treewidth.
SIAM Journal onComputing, 25:1305?1317.Bodlaender, Hans L., John R. Gilbert,Hja?lmty?r Hafsteinsson, and Ton Kloks.1995.
Approximating treewidth,pathwidth, frontsize, and shortestelimination tree.
Journal of Algorithms,18(2):238?255.Chekuri, Chandra and Anand Rajaraman.1997.
Conjunctive query containmentrevisited.
In Database Theory ?
ICDT ?97,volume 1186 of Lecture Notes in ComputerScience.
Springer, Berlin, pages 56?70.Chiang, David.
2007.
Hierarchicalphrase-based translation.
ComputationalLinguistics, 33(2):201?228.Dechter, Rina and Judea Pearl.
1989.
Treeclustering for constraint networks.Artificial Intelligence, 38(3):353?366.Eisner, Jason and John Blatz.
2007.
Programtransformations for optimization ofparsing algorithms and other weightedlogic programs.
In Shuly Wintner, editor,Proceedings of FG 2006: The 11th Conferenceon Formal Grammar.
CSLI Publications,pages 45?85, Malaga.Eisner, Jason and Giorgio Satta.
1999.Efficient parsing for bilexical context-freegrammars and head automaton grammars.In Proceedings of the 37th Annual Conferenceof the Association for ComputationalLinguistics (ACL-99), pages 457?464,College Park, MD.Eisner, Jason and Giorgio Satta.
2000.
A fasterparsing algorithm for lexicalizedtree-adjoining grammars.
In Proceedings ofthe 5th Workshop on Tree-AdjoiningGrammars and Related Formalisms (TAG+5),pages 14?19, Paris.Feige, Uriel, MohammadTaghi Hajiaghayi,and James R. Lee.
2005.
Improvedapproximation algorithms forminimum-weight vertex separators.
InSTOC ?05: Proceedings of the thirty-seventhannual ACM symposium on Theory ofcomputing, pages 563?572, Baltimore, MD.Flum, Jo?rg, Markus Frick, and Martin Grohe.2002.
Query evaluation viatree-decompositions.
Journal of the ACM,49(6):716?752.Galley, Michel, Mark Hopkins, Kevin Knight,and Daniel Marcu.
2004.
What?s in atranslation rule?
In Proceedings of the 2004Meeting of the North American Chapter of theAssociation for Computational Linguistics(NAACL-04), pages 273?280, Boston, MA.Gildea, Daniel.
2010.
Optimal parsingstrategies for Linear Context-Free247Computational Linguistics Volume 37, Number 1Rewriting Systems.
In Proceedings of the2010 Meeting of the North American Chapterof the Association for ComputationalLinguistics (NAACL-10), pages 769?776,Los Angeles, CA.Gildea, Daniel and Daniel S?tefankovic?.
2007.Worst-case synchronous grammar rules.
InProceedings of the 2007 Meeting of the NorthAmerican Chapter of the Association forComputational Linguistics (NAACL-07),pages 147?154, Rochester, NY.Go?mez-Rodr?
?guez, Carlos, MarcoKuhlmann, Giorgio Satta, and David Weir.2009.
Optimal reduction of rule length inLinear Context-Free Rewriting Systems.
InProceedings of the 2009 Meeting of the NorthAmerican Chapter of the Association forComputational Linguistics (NAACL-09),pages 539?547, Boulder, CO.Goodman, Joshua.
1999.
Semiring parsing.Computational Linguistics, 25(4):573?605.Huang, Liang, Hao Zhang, and DanielGildea.
2005.
Machine translation aslexicalized parsing with hooks.
InInternational Workshop on ParsingTechnologies (IWPT05), pages 65?73,Vancouver.Huang, Liang, Hao Zhang, Daniel Gildea,and Kevin Knight.
2009.
Binarization ofsynchronous context-free grammars.Computational Linguistics, 35(4):559?595.Jensen, Finn V., Steffen L. Lauritzen, andKristian G. Olesen.
1990.
Bayesianupdating in causal probabilistic networksby local computations.
ComputationalStatistics Quarterly, 4:269?282.Johnson, Mark.
2007.
Transformingprojective bilexical dependency grammarsinto efficiently-parsable CFGs withunfold-fold.
In Proceedings of the 45thAnnual Meeting of the Association ofComputational Linguistics, pages 168?175,Prague.Knuth, D. 1977.
A generalization of Dijkstra?salgorithm.
Information Processing Letters,6(1):1?5.Kschischang, F. R., B. J. Frey, and H. A.Loeliger.
2001.
Factor graphs and thesum-product algorithm.
IEEE Transactionson Information Theory, 47(2):498?519.Kuhlmann, Marco and Joakim Nivre.
2006.Mildly non-projective dependencystructures.
In Proceedings of the InternationalConference on ComputationalLinguistics/Association for ComputationalLinguistics (COLING/ACL-06),pages 507?514, Sydney.Kuhlmann, Marco and Giorgio Satta.
2009.Treebank grammar techniques fornon-projective dependency parsing.
InProceedings of the 12th Conference of theEuropean Chapter of the ACL (EACL-09),pages 478?486, Athens.McAllester, David.
2002.
On the complexityanalysis of static analyses.
Journal of theACM, 49(4):512?537.Melamed, I. Dan, Giorgio Satta, and BenWellington.
2004.
Generalized multitextgrammars.
In Proceedings of the 42ndAnnual Conference of the Association forComputational Linguistics (ACL-04),pages 661?668, Barcelona.Nederhof, M.-J.
2003.
Weighted deductiveparsing and Knuth?s algorithm.Computational Linguistics, 29(1):135?144.Rambow, Owen and Giorgio Satta.
1999.Independent parallelism in finitecopying parallel rewriting systems.Theoretical Computer Science,223(1-2):87?120.Satta, Giorgio.
1992.
Recognition of LinearContext-Free Rewriting Systems.
InProceedings of the 30th Annual Conference ofthe Association for Computational Linguistics(ACL-92), pages 89?95, Newark, DE.Shafer, G. and P. Shenoy.
1990.
Probabilitypropagation.
Annals of Mathematics andArtificial Intelligence, 2:327?353.Shieber, Stuart M., Yves Schabes, andFernando C. N. Pereira.
1995.
Principlesand implementation of deductive parsing.The Journal of Logic Programming,24(1-2):3?36.Vijay-Shankar, K., D. L. Weir, and A. K. Joshi.1987.
Characterizing structuraldescriptions produced by variousgrammatical formalisms.
In Proceedings ofthe 25th Annual Conference of the Associationfor Computational Linguistics (ACL-87),pages 104?111, Stanford, CA.Wellington, Benjamin, Sonjia Waxmonsky,and I. Dan Melamed.
2006.
Empiricallower bounds on the complexity oftranslational equivalence.
In Proceedings ofthe International Conference on ComputationalLinguistics/Association for ComputationalLinguistics (COLING/ACL-06),pages 977?984, Sydney.Wu, Dekai.
1997.
Stochastic inversiontransduction grammars and bilingualparsing of parallel corpora.
ComputationalLinguistics, 23(3):377?403.Zhang, Hao and Daniel Gildea.
2007.Factorization of synchronous context-freegrammars in linear time.
In NAACLWorkshop on Syntax and Structure inStatistical Translation (SSST), pages 25?32,Rochester, NY.248
