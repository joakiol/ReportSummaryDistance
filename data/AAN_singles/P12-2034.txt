Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 171?175,Jeju, Republic of Korea, 8-14 July 2012. c?2012 Association for Computational LinguisticsSyntactic Stylometry for Deception DetectionSong Feng Ritwik Banerjee Yejin ChoiDepartment of Computer ScienceStony Brook UniversityStony Brook, NY 11794-4400songfeng, rbanerjee, ychoi@cs.stonybrook.eduAbstractMost previous studies in computerized de-ception detection have relied only on shal-low lexico-syntactic patterns.
This pa-per investigates syntactic stylometry fordeception detection, adding a somewhatunconventional angle to prior literature.Over four different datasets spanning fromthe product review to the essay domain,we demonstrate that features driven fromContext Free Grammar (CFG) parse treesconsistently improve the detection perfor-mance over several baselines that are basedonly on shallow lexico-syntactic features.Our results improve the best published re-sult on the hotel review data (Ott et al,2011) reaching 91.2% accuracy with 14%error reduction.1 IntroductionPrevious studies in computerized deception de-tection have relied only on shallow lexico-syntactic cues.
Most are based on dictionary-based word counting using LIWC (Pennebakeret al, 2007) (e.g., Hancock et al (2007), Vrij etal.
(2007)), while some recent ones explored theuse of machine learning techniques using sim-ple lexico-syntactic patterns, such as n-gramsand part-of-speech (POS) tags (Mihalcea andStrapparava (2009), Ott et al (2011)).
Theseprevious studies unveil interesting correlationsbetween certain lexical items or categories withdeception that may not be readily apparent tohuman judges.
For instance, the work of Ottet al (2011) in the hotel review domain resultsin very insightful observations that deceptive re-viewers tend to use verbs and personal pronouns(e.g., ?I?, ?my?)
more often, while truthful re-viewers tend to use more of nouns, adjectives,prepositions.
In parallel to these shallow lexicalpatterns, might there be deep syntactic struc-tures that are lurking in deceptive writing?This paper investigates syntactic stylometryfor deception detection, adding a somewhat un-conventional angle to prior literature.
Over fourdifferent datasets spanning from the product re-view domain to the essay domain, we find thatfeatures driven from Context Free Grammar(CFG) parse trees consistently improve the de-tection performance over several baselines thatare based only on shallow lexico-syntactic fea-tures.
Our results improve the best published re-sult on the hotel review data of Ott et al (2011)reaching 91.2% accuracy with 14% error reduc-tion.
We also achieve substantial improvementover the essay data of Mihalcea and Strapparava(2009), obtaining upto 85.0% accuracy.2 Four DatasetsTo explore different types of deceptive writing,we consider the following four datasets spanningfrom the product review to the essay domain:I. TripAdvisor?Gold: Introduced in Ott etal.
(2011), this dataset contains 400 truthful re-views obtained from www.tripadviser.com and400 deceptive reviews gathered using AmazonMechanical Turk, evenly distributed across 20Chicago hotels.171TripAdvisor?Gold TripAdvisor?HeuristicDeceptive Truthful Deceptive TruthfulNP?PP ?
DT NNP NNP NNP S?ROOT ?
VP .
NP?S ?
PRP VP?S ?
VBZ NPSBAR?NP ?
S NP?NP ?
$ CD SBAR?S ?
WHADVP S NP?NP ?
NNSNP?VP ?
NP SBAR PRN?NP ?
LRB NP RRB VP?S ?
VBD PP WHNP?SBAR ?
WDTNP?NP ?
PRP$ NN NP?NP ?
NNS S?SBAR ?
NP VP NP?NP ?
NP PP PPNP?S ?
DT NNP NNP NNP NP?S ?
NN S?ROOT ?
PP NP VP .
NP?S ?
EXVP?S ?
VBG PP NP?PP ?
DT NNP VP?S ?
VBD S NX?NX ?
JJ NNNP?PP ?
PRP$ NN NP?PP ?
CD NNS NP?S ?
NP CC NP NP?NP ?
NP PPVP?S ?
MD ADVP VP NP?NP ?
NP PRN NP?S ?
PRP$ NN VP?S ?
VBZ RB NPVP?S ?
TO VP PRN?NP ?
LRB PP RRB NP?PP ?
DT NNP PP?NP ?
IN NPADJP?NP ?
RBS JJ NP?NP ?
CD NNS NP?PP ?
PRP$ NN PP?ADJP ?
TO NPTable 1: Most discriminative rewrite rules (r?
): hotel review datasetsFigure 1: Parsed treesII.
TripAdvisor?Heuristic: This datasetcontains 400 truthful and 400 deceptive reviewsharvested from www.tripadviser.com, basedon fake review detection heuristics introducedin Feng et al (2012).1III.
Yelp: This dataset is our own creationusing www.yelp.com.
We collect 400 filtered re-views and 400 displayed reviews for 35 Italianrestaurants with average ratings in the range of[3.5, 4.0].
Class labels are based on the metadata, which tells us whether each review is fil-tered by Yelp?s automated review filtering sys-tem or not.
We expect that filtered reviewsroughly correspond to deceptive reviews, anddisplayed reviews to truthful ones, but not with-out considerable noise.
We only collect 5-starreviews to avoid unwanted noise from varying1Specifically, using the notation of Feng et al (2012),we use data created by Strategy-dist?
heuristic, withHS ,S as deceptive and H ?S , T as truthful.degree of sentiment.IV.
Essays: Introduced in Mihalcea andStrapparava (2009), this corpus contains truth-ful and deceptive essays collected using AmazonMechanic Turk for the following three topics:?Abortion?
(100 essays per class), ?Best Friend?
(98 essays per class), and ?Death Penalty?
(98essays per class).3 Feature EncodingWords Previous work has shown that bag-of-words are effective in detecting domain-specificdeception (Ott et al, 2011; Mihalcea and Strap-parava, 2009).
We consider unigram, bigram,and the union of the two as features.Shallow Syntax As has been used in manyprevious studies in stylometry (e.g., Argamon-Engelson et al (1998), Zhao and Zobel (2007)),we utilize part-of-speech (POS) tags to encodeshallow syntactic information.
Note that Ottet al (2011) found that even though POS tagsare effective in detecting fake product reviews,they are not as effective as words.
Therefore, westrengthen POS features with unigram features.Deep syntax We experiment with four differ-ent encodings of production rules based on theProbabilistic Context Free Grammar (PCFG)parse trees as follows:?
r: unlexicalized production rules (i.e., allproduction rules except for those with ter-minal nodes), e.g., NP2 ?
NP3 SBAR.?
r?
: lexicalized production rules (i.e., allproduction rules), e.g., PRP ?
?you?.?
r?
: unlexicalized production rules combinedwith the grandparent node, e.g., NP2 ?VP172TripAdvisor Yelp EssayGold Heur Abort BstFr Deathunigram 88.4 74.4 59.9 70.0 77.0 67.4words bigram 85.8 71.5 60.7 71.5 79.5 55.5uni + bigram 89.6 73.8 60.1 72.0 81.5 65.5pos(n=1) + unigram 87.4 74.0 62.0 70.0 80.0 66.5shallow syntax pos(n=2) + unigram 88.6 74.6 59.0 67.0 82.0 66.5+words pos(n=3) + unigram 88.6 74.6 59.3 67.0 82.0 66.5r 78.5 65.3 56.9 62 67.5 55.5deep syntax r?
74.8 65.3 56.5 58.5 65.5 56.0r?
89.4 74.0 64.0 70.1 77.5 66.0r??
90.4 75 63.5 71.0 78 67.5r + unigram 89.0 74.3 62.3 76.5 82.0 69.0deep syntax r?
+ unigram 88.5 74.3 62.5 77.0 81.5 70.5+words r?
+ unigram 90.3 75.4 64.3 74.0 85.0 71.5r??
+ unigram 91.2 76.6 62.1 76.0 84.5 71.0Table 2: Deception Detection Accuracy (%).1 ?
NP3 SBAR.?
r??
: lexicalized production rules (i.e., allproduction rules) combined with the grand-parent node, e.g., PRP?NP 4 ?
?you?.4 Experimental ResultsFor all classification tasks, we use SVM classi-fier, 80% of data for training and 20% for test-ing, with 5-fold cross validation.2 All featuresare encoded as tf-idf values.
We use BerkeleyPCFG parser (Petrov and Klein, 2007) to parsesentences.
Table 2 presents the classificationperformance using various features across fourdifferent datasets introduced earlier.34.1 TripAdvisor?GoldWe first discuss the results for the TripAdvisor?Gold dataset shown in Table 2.
As reported inOtt et al (2011), bag-of-words features achievesurprisingly high performance, reaching upto89.6% accuracy.
Deep syntactic features, en-coded as r??
slightly improves this performance,achieving 90.4% accuracy.
When these syntacticfeatures are combined with unigram features, weattain the best performance of 91.2% accuracy,2We use LIBLINEAR (Fan et al, 2008) with L2-regulization, parameter optimized over the 80% trainingdata (3 folds for training, 1 fold for testing).3Numbers in italic are classification results reportedin Ott et al (2011) and Mihalcea and Strapparava (2009).yielding 14% error reduction over the word-onlyfeatures.Given the power of word-based features, onemight wonder, whether the PCFG driven fea-tures are being useful only due to their lexi-cal production rules.
To address such doubts,we include experiments with unlexicalized rules,r and r?.
These features achieve 78.5% and74.8% accuracy respectively, which are signifi-cantly higher than that of a random baseline(?50.0%), confirming statistical differences indeep syntactic structures.
See Section 4.4 forconcrete exemplary rules.Another question one might have is whetherthe performance gain of PCFG features aremostly from local sequences of POS tags, indi-rectly encoded in the production rules.
Compar-ing the performance of [shallow syntax+words]and [deep syntax+words] in Table 2, we find sta-tistical evidence that deep syntax based featuresoffer information that are not available in simplePOS sequences.4.2 TripAdvisor?Heuristic & YelpThe performance is generally lower than that ofthe previous dataset, due to the noisy natureof these datasets.
Nevertheless, we find similartrends as those seen in the TripAdvisor?Golddataset, with respect to the relative performancedifferences across different approaches.
The sig-173TripAdvisor?Gold TripAdvisor?HeurDecep Truth Decep TruthVP PRN VP PRNSBAR QP WHADVP NXWHADVP S SBAR WHNPADVP PRT WHADJP ADJPCONJP UCP INTJ WHPPTable 3: Most discriminative phrasal tags in PCFGparse trees: TripAdvisor data.nificance of these results comes from the factthat these two datasets consists of real (fake)reviews in the wild, rather than manufacturedones that might invite unwanted signals thatcan unexpectedly help with classification accu-racy.
In sum, these results indicate the exis-tence of the statistical signals hidden in deepsyntax even in real product reviews with noisygold standards.4.3 EssayFinally in Table 2, the last dataset Essay con-firms the similar trends again, that the deep syn-tactic features consistently improve the perfor-mance over several baselines based only on shal-low lexico-syntactic features.
The final results,reaching accuracy as high as 85%, substantiallyoutperform what has been previously reportedin Mihalcea and Strapparava (2009).
How ro-bust are the syntactic cues in the cross topic set-ting?
Table 4 compares the results of Mihalceaand Strapparava (2009) and ours, demonstrat-ing that syntactic features achieve substantiallyand surprisingly more robust results.4.4 Discriminative Production RulesTo give more concrete insights, we provide10 most discriminative unlexicalized productionrules (augmented with the grand parent node)for each class in Table 1.
We order the rulesbased on the feature weights assigned by LIB-LINEAR classifier.
Notice that the two produc-tion rules in bolds ?
[SBAR?NP?
S] and [NP?VP?
NP SBAR] ?
are parts of the parse treeshown in Figure 1, whose sentence is taken froman actual fake review.
Table 3 shows the mostdiscriminative phrasal tags in the PCFG parsetraining: A & B A & D B & Dtesting: DeathPen BestFrn AbortionM&S 2009 58.7 58.7 62.0r?
66.8 70.9 69.0Table 4: Cross topic deception detection accuracy:Essay datatrees for each class.
Interestingly, we find morefrequent use of VP, SBAR (clause introducedby subordinating conjunction), and WHADVPin deceptive reviews than truthful reviews.5 Related WorkMuch of the previous work for detecting de-ceptive product reviews focused on related, butslightly different problems, e.g., detecting dupli-cate reviews or review spams (e.g., Jindal andLiu (2008), Lim et al (2010), Mukherjee et al(2011), Jindal et al (2010)) due to notable dif-ficulty in obtaining gold standard labels.4 TheYelp data we explored in this work shares a sim-ilar spirit in that gold standard labels are har-vested from existing meta data, which are notguaranteed to align well with true hidden la-bels as to deceptive v.s.
truthful reviews.
Twoprevious work obtained more precise gold stan-dard labels by hiring Amazon turkers to writedeceptive articles (e.g., Mihalcea and Strappa-rava (2009), Ott et al (2011)), both of whichhave been examined in this study with respectto their syntactic characteristics.
Although weare not aware of any prior work that dealtwith syntactic cues in deceptive writing directly,prior work on hedge detection (e.g., Greene andResnik (2009), Li et al (2010)) relates to ourfindings.6 ConclusionWe investigated syntactic stylometry for decep-tion detection, adding a somewhat unconven-tional angle to previous studies.
Experimentalresults consistently find statistical evidence ofdeep syntactic patterns that are helpful in dis-criminating deceptive writing.4It is not possible for a human judge to tell with fullconfidence whether a given review is a fake or not.174ReferencesS.
Argamon-Engelson, M. Koppel, and G. Avneri.1998.
Style-based text categorization: Whatnewspaper am i reading.
In Proc.
of the AAAIWorkshop on Text Categorization, pages 1?4.Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh,Xiang-Rui Wang, and Chih-Jen Lin.
2008.
LIB-LINEAR: A library for large linear classification.Journal of Machine Learning Research, 9:1871?1874.S.
Feng, L. Xing, Gogar A., and Y. Choi.
2012.Distributional footprints of deceptive product re-views.
In Proceedings of the 2012 InternationalAAAI Conference on WebBlogs and Social Media,June.S.
Greene and P. Resnik.
2009.
More thanwords: Syntactic packaging and implicit senti-ment.
In Proceedings of Human Language Tech-nologies: The 2009 Annual Conference of theNorth American Chapter of the Association forComputational Linguistics, pages 503?511.
Asso-ciation for Computational Linguistics.J.T.
Hancock, L.E.
Curry, S. Goorha, and M. Wood-worth.
2007.
On lying and being lied to: A lin-guistic analysis of deception in computer-mediatedcommunication.
Discourse Processes, 45(1):1?23.Nitin Jindal and Bing Liu.
2008.
Opinion spamand analysis.
In Proceedings of the internationalconference on Web search and web data mining,WSDM ?08, pages 219?230, New York, NY, USA.ACM.Nitin Jindal, Bing Liu, and Ee-Peng Lim.
2010.Finding unusual review patterns using unexpectedrules.
In Proceedings of the 19th ACM Confer-ence on Information and Knowledge Management,pages 1549?1552.X.
Li, J. Shen, X. Gao, and X. Wang.
2010.
Ex-ploiting rich features for detecting hedges andtheir scope.
In Proceedings of the FourteenthConference on Computational Natural LanguageLearning?Shared Task, pages 78?83.
Associationfor Computational Linguistics.Ee-Peng Lim, Viet-An Nguyen, Nitin Jindal, BingLiu, and Hady Wirawan Lauw.
2010.
Detectingproduct review spammers using rating behaviors.In Proceedings of the 19th ACM international con-ference on Information and knowledge manage-ment, CIKM ?10, pages 939?948, New York, NY,USA.
ACM.R.
Mihalcea and C. Strapparava.
2009.
The lie de-tector: Explorations in the automatic recognitionof deceptive language.
In Proceedings of the ACL-IJCNLP 2009 Conference Short Papers, pages309?312.
Association for Computational Linguis-tics.Arjun Mukherjee, Bing Liu, Junhui Wang, Natalie S.Glance, and Nitin Jindal.
2011.
Detecting groupreview spam.
In Proceedings of the 20th Interna-tional Conference on World Wide Web (Compan-ion Volume), pages 93?94.Myle Ott, Yejin Choi, Claire Cardie, and Jeffrey T.Hancock.
2011.
Finding deceptive opinion spamby any stretch of the imagination.
In Proceed-ings of the 49th Annual Meeting of the Associa-tion for Computational Linguistics: Human Lan-guage Technologies, pages 309?319, Portland, Ore-gon, USA, June.
Association for ComputationalLinguistics.J.W.
Pennebaker, C.K.
Chung, M. Ireland, A. Gon-zales, and R.J. Booth.
2007.
The developmentand psychometric properties of liwc2007.
Austin,TX, LIWC.
Net.S.
Petrov and D. Klein.
2007.
Improved inference forunlexicalized parsing.
In Proceedings of NAACLHLT 2007, pages 404?411.A.
Vrij, S. Mann, S. Kristen, and R.P.
Fisher.
2007.Cues to deception and ability to detect lies as afunction of police interview styles.
Law and hu-man behavior, 31(5):499?518.Ying Zhao and Justin Zobel.
2007.
Searching withstyle: authorship attribution in classic literature.In Proceedings of the thirtieth Australasian confer-ence on Computer science - Volume 62, ACSC ?07,pages 59?68, Darlinghurst, Australia, Australia.Australian Computer Society, Inc.175
