Proceedings of the Fifteenth Conference on Computational Natural Language Learning, pages 78?86,Portland, Oregon, USA, 23?24 June 2011. c?2011 Association for Computational LinguisticsGender Attribution: Tracing Stylometric Evidence Beyond Topic and GenreRuchita Sarawgi, Kailash Gajulapalli, and Yejin ChoiDepartment of Computer ScienceStony Brook UniversityNY 11794, USA{rsarawgi, kgajulapalli, ychoi}@cs.stonybrook.eduAbstractSociolinguistic theories (e.g., Lakoff (1973))postulate that women?s language styles differfrom that of men.
In this paper, we explorestatistical techniques that can learn to iden-tify the gender of authors in modern Englishtext, such as web blogs and scientific papers.Although recent work has shown the efficacyof statistical approaches to gender attribution,we conjecture that the reported performancemight be overly optimistic due to non-stylisticfactors such as topic bias in gender that canmake the gender detection task easier.
Ourwork is the first that consciously avoids genderbias in topics, thereby providing stronger evi-dence to gender-specific styles in language be-yond topic.
In addition, our comparative studyprovides new insights into robustness of var-ious stylometric techniques across topic andgenre.1 IntroductionSociolinguistic theories (e.g., Lakoff (1973)) postu-late that women?s language styles differ from thatof men with respect to various aspects of communi-cation, such as discourse behavior, body language,lexical choices, and linguistic cues (e.g., Crosbyand Nyquist (1977), Tannen (1991), Argamon et al(2003), Eckert and McConnell-Ginet (2003), Arga-mon et al (2007)).
In this paper, we explore statis-tical techniques that can learn to identify the gen-der of authors in modern English text, such as webblogs and scientific papers, motivated by sociolin-guistic theories for gender attribution.There is a broad range of potential applicationsacross computational linguistics and social sciencewhere statistical techniques for gender attributioncan be useful: e.g., they can help understanding de-mographic characteristics of user-created web texttoday, which can provide new insight to social sci-ence as well as intelligent marketing and opinionmining.
Models for gender attribution can also helptracking changes to gender-specific styles in lan-guage over different domain and time.
Gender de-tectors can be useful to guide the style of writing aswell, if one needs to assume the style of a specificgender for imaginative writing.Although some recent work has shown the effi-cacy of machine learning techniques to gender at-tribution (e.g., Koppel et al (2002), Mukherjee andLiu (2010)), we conjecture that the reported perfor-mance might be overly optimistic under scrutiny dueto non-stylistic factors such as topic bias in genderthat can make the gender detection task easier.
In-deed, recent research on web blogs reports that thereis substantial gender bias in topics (e.g., Janssen andMurachver (2004), Argamon et al (2007)) as wellas in genre (e.g., Herring and Paolillo (2006)).In order to address this concern, we perform thefirst comparative study of machine learning tech-niques for gender attribution after deliberately re-moving gender bias in topics and genre.
Further-more, making the task even more realistic (and chal-lenging), we experiment with cross-topic and cross-genre gender attribution, and provide statistical ev-idence to gender-specific styles in language beyondtopic and genre.
Five specific questions we aim toinvestigate are:78Q1 Are there truly gender-specific characteristicsin language?
or are they confused with genderpreferences in topics and genre?Q2 Are there deep-syntactic patterns in women?slanguage beyond words and shallow patterns?Q3 Which stylometric analysis techniques are ef-fective in detecting characteristics in women?slanguage?Q4 Which stylometric analysis techniques are ro-bust against domain change with respect to top-ics and genre?Q5 Are there gender-specific language characteris-tics even in modern scientific text?From our comparative study of various techniquesfor gender attribution, including two publicly avail-able systems - Gender Genie1 and Gender Guesser2we find that (1) despite strong evidence for deepsyntactic structure that characterizes gender-specificlanguage styles, such deep patterns are not as robustas shallow morphology-level patterns when facedwith topic and genre change, and that (2) there areindeed gender-specific linguistic signals that go be-yond topics and genre, even in modern and scientificliterature.2 Related WorkThe work of Lakoff (1973) initiated the research onwomen?s language, where ten basic characteristicsof women?s language were listed.
Some exemplaryones are as follows:1 Hedges: e.g., ?kind of?, ?it seems to be?2 Empty adjectives: e.g., ?lovely?, ?adorable?,?gorgeous?3 Hyper-polite: e.g., ?would you mind ...?, ?I?dmuch appreciate if ...?4 Apologetic: e.g., ?I am very sorry, but I thinkthat ...?5 Tag questions: e.g., ?you don?t mind, do you?
?1http://bookblog.net/gender/genie.php2Available at http://www.hackerfactor.com/GenderGuesser.phpMany sociolinguists and psychologists consequentlyinvestigated on the validity of each of the aboveassumptions and extended sociolinguistic theo-ries on women?s language based on various con-trolled experiments and psychological analysis (e.g.,Crosby and Nyquist (1977), McHugh and Ham-baugh (2010)).While most theories in socioliguistics and psy-chology focus on a small set of cognitively identi-fiable patterns in women?s language (e.g., the use oftag questions), some recent studies in computer sci-ence focus on investigating the use of machine learn-ing techniques that can learn to identify women?slanguage from a bag of features (e.g., Koppel et al(2002), Mukherjee and Liu (2010)).Our work differs from most previous work inthat we consciously avoid gender bias in topics andgenre in order to provide more accurate analysisof statistically identifiable patterns in women?s lan-guage.
Furthermore, we compare various techniquesin stylometric analysis within and beyond topics andgenre.3 Dataset without Unwanted Gender BiasIn this section, we describe how we prepared ourdataset to avoid unwanted gender bias in topics andgenre.
Much of previous work has focused on for-mal writings, such as English literature, newswirearticles and the British Natural Corpus(BNC) (e.g.,Argamon et al (2003)), while recent studies ex-panded toward more informal writing such as webblogs (e.g., Mukherjee and Liu (2010)).
In thiswork, we chose two very different and prominentgenre electronically available today: web blogs andscientific papers.Blogs: We downloaded blogs from popular blogsites for 7 distinctive topics:3 education, travel, spir-ituality, entertainment, book reviews, history andpolitics.
Within each topic, we find 20 articles writ-ten by male authors, and additional 20 articles writ-ten by female authors.
We took the effort to matcharticles written by different gender even at the sub-topic level.
For example, if we take a blog writtenabout the TV show ?How I met your mother?
by afemale author, then we also find a blog written by a3wordpress.com, blogspot.com & nytimes.com/interactive/blogs/directory.html79male author on the same show.
Note that previousresearch on web blogs does not purposefully main-tain balanced topics between gender, thereby bene-fiting from topic bias inadvertently.
From each blog,we keep the first 450 (+/- 20) words preserving thesentence boundaries.4 We plan to make this datapublically available.Scientific Papers: Scientific papers have not beenstudied in previous research on gender attribution.Scientific papers correspond to very formal writ-ing where gender-specific language styles are notlikely to be conspicuous (e.g., Janssen and Mu-rachver (2004)).For this dataset, we collected papers from the re-searchers in our own Natural Language Processingcommunity.
We randomly selected 5 female and 5male authors, and collected 20 papers from each au-thor.
We tried to select these authors across a varietyof subtopics within NLP research, so as to reducepotential topic-bias in gender even in research.
It isalso worthwhile to mention that authors in our selec-tion are highly established ones who have publishedover multiple subtopics in NLP.Similarly as the blog dataset, we keep the first450 (+/- 20) words preserving the sentence bound-aries.
Some papers are co-authored by researchersof mixed gender.
In those cases, we rely on the gen-der of the advisory person as she or he is likely toinfluence on the abstract and intro the most.4 Statistical TechniquesIn this section, we describe three different types ofstatistical language models that learn patterns at dif-ferent depth.
The first kind is based on probabilis-tic context-free grammars (PCFG) that learn deeplong-distance syntactic patterns (Section 4.1).
Thesecond kind is based on token-level language mod-els that learn shallow lexico-syntactic patterns (Sec-tion 4.2).
The last kind is based on character-levellanguage models that learn morphological patternson extremely short text spans (Section 4.3).
Fi-nally, we describe the bag-of-word approach usingthe maximum entropy classifier (Section 4.4).4Note that existing gender detection tools require a mini-mum 300 words for appropriate identification.4.1 Deep Syntactic Patterns usingProbabilistic Context free GrammarA probabilistic context-free grammar (PCFG) cap-tures syntactic regularities beyond shallow ngram-based lexico-syntactic patterns.
Raghavan et al(2010) recently introduced the use of PCFG for au-thorship attribution for the first time, and demon-strated that it is highly effective for learning stylisticpatterns for authorship attribution.
We therefore ex-plore the use of PCFG for gender attribution.
Wegive a very concise description here, referring toRaghavan et al (2010) for more details.
(1) Train a generic PCFG parser Go on manuallytree-banked corpus such as WSJ or Brown.
(2) Given training corpus D for gender attribution,tree-bank each training document di ?
D usingthe PCFG parser Go.
(3) For each gender ?, train a new gender-specificPCFG parser G?
using only those tree-bankeddocuments in D that correspond to gender ?.
(4) For each test document, compare the likelihoodof the document determined by each gender-specific PCFG parser G?
, and the gender cor-responding to the higher score.Note that PCFG models can be considered as a kindof language models, where probabilistic context-free grammars are used to find the patterns in lan-guage, rather than n-grams.
We use the implementa-tion of Klein and Manning (2003) for PCFG models.4.2 Shallow Lexico-Syntactic Patterns usingToken-level Language ModelsToken-based (i.e.
word-based) language modelshave been employed in a wide variety of NLP ap-plications, including those that require stylometricanalysis, e.g., authorship attribution (e.g., Uzner andKatz (2005)), and Wikipedia vandalism detection(Wang and McKeown, 2010).
We expect that token-based language models will be effective in learningshallow lexico-syntactic patterns of gender specificlanguage styles.
We therefore experiment with un-igram, bigram, and trigram token-level models, andname them as TLM(n=1), TLM(n=2), TLM(n=3),respectively, where TLM stands for Token-based80lexicon based deep syntax morphology b.o.w.
shallow lex-syntaxGender Gender PCFG CLM CLM CLM ME TLM TLM TLMData Type Genie Guesser n=1 n=2 n=3 n=1 n=2 n=3Male Only 72.1 68.6 53.4 65.8 69.0 63.4 57.6 67.1 67.8 66.2Female Only 27.1 06.4 74.8 57.6 73.6 76.8 73.8 60.1 64.2 64.2All 50.0 37.5 64.1 61.70 71.3 70.3 65.8 63.7 66.1 65.4Table 1: Overall Accuracy of Topic-Balanced Gender Attribution on Blog Data (Experiment-I)Language Models.
We use the LingPipe package5for experiments.4.3 Shallow Morphological Patterns usingCharacter-level Language ModelsNext we explore the use of character-level lan-guage models to investigate whether there are mor-phological patterns that characterize gender-specificstyles in language.
Despite its simplicity, previ-ous research have reported that character-level lan-guage models are effective for authorship attribu-tion (e.g., Peng et al (2003b)) as well as genreclassification (e.g., Peng et al (2003a), Wu et al(2010)).
We experiment with unigram, bigram,and trigram character-level models, and name themas CLM(n=1), CLM(n=2), CLM(n=3), respectively,where CLM stands for Character-based LanguageModels.
We again make use of the LingPipe pack-age for experiments.Note that there has been no previous researchthat directly compares the performance of character-level language models to that of PCFG based modelsfor author attribution, not to mention for gender at-tribution.4.4 Bag of Words usingMaximum Entropy (MaxEnt) ClassifierWe include Maximum Entropy classifier using sim-ple unigram features (bag-of-words) for comparisonpurposes, and name it as ME.
We use the MALLETpackage (McCallum, 2002) for experiments.5 Experimental ResultsNote that our two datasets are created to specificallyanswer the following question: are there gender-specific characteristics in language beyond gender5Available at http://alias-i.com/lingpipe/preferences in topics and genre?
One way to answerthis question is to test whether statistical models candetect gender attribution on a dataset that is dras-tically different from the training data in topic andgenre.
Of course, it is a known fact that machinelearning techniques do not transfer well across dif-ferent domains (e.g., Blitzer et al (2006)).
However,if they can still perform considerably better than ran-dom prediction, then it would prove that there is in-deed gender-specific stylometric characteristics be-yond topic and genre.
In what follows, we presentfive different experimental settings across two differ-ent dataset to compare in-domain and cross-domainperformance of various techniques for gender attri-bution.5.1 Experiments with Blog DatasetFirst we conduct two different experiments using theblog data in the order of increasing difficulty.
[Experiment-I: Balanced Topic] Using the webblog dataset introduced in Section 3, we performgender attribution (classification) task on balancedtopics.
For each topic, 80% of the documents areused for training and remaining ones are used fortesting, yielding 5-fold cross validation.
Both train-ing and test data have balanced class distributionsso that random guess would yield 50% of accuracy.The results are given in Table 1.
Note that the ?over-all accuracy?
corresponds to the average across thefive folds.The PCFG model achieves prediction accuracy64.1%, demonstrating statistical evidence to gender-specific characteristics in syntactic structure.
ThePCFG model outperforms two publicly availablesystems - Gender Genie and Gender Guesser, whichare based on a fixed list of indicator words.
The dif-ference is statistically significant (p = 0.01 < 0.05)81lexicon based deep syntax morphology b.o.w.
shallow lex-syntaxGender Gender PCFG CLM CLM CLM ME TLM TLM TLMTopic Genie Guesser n=1 n=2 n=3 n=1 n=2 n=3Per Topic Accuracy (%) for All AuthorsEntertain 50.0 42.5 50.0 52.5 67.5 67.5 60.0 57.5 57.5 57.5Book 50.0 42.5 65.0 57.5 67.5 72.5 55.0 60.0 67.5 67.5Politics 35.0 30.0 50.0 47.5 52.5 50.0 45.0 52.5 52.5 52.5History 40.0 35.0 77.5 65.0 80.0 80.0 55.0 65.0 65.0 65.0Education 62.5 42.5 55.0 63.0 65.0 70.0 63.0 55.0 57.5 52.5Travel 62.5 37.5 63.0 65.0 63.0 63.0 63.0 62.5 65.0 65.0Spirituality 50.0 32.5 53.0 78.0 78.0 78.0 50.0 65.0 70.0 72.5Avg 50.0 37.5 59.0 61.2 68.3 68.3 55.87 60.0 61.3 61.5Per Topic Accuracy (%) for Female AuthorsEntertain 25.0 10.0 85.0 70.0 50.0 85.0 70.0 75.0 75.0 75.0Book 15.0 15.0 95.0 80.0 95.0 90.0 85.0 75.0 90.0 90.0Politics 10.0 05.0 65.0 00.0 05.0 00.0 35.0 30.0 30.0 25.0History 10.0 05.0 90.0 70.0 80.0 75.0 70.0 50.0 50.0 50.0Education 45.0 10.0 80.0 95.0 85.0 90.0 100.0 50.0 55.0 50.0Travel 65.0 00.0 85.0 90.0 100.0 100.0 100.0 85.0 95.0 90.0Spirituality 20.0 00.0 60.0 65.0 65.0 70.0 45.0 50.0 50.0 50.0Avg 27.1 06.4 80.0 67.1 68.6 72.9 72.1 59.3 63.6 61.4Per Topic Accuracy (%) for Male AuthorsEntertain 75.0 75.0 15.0 35.0 85.0 50.0 50.0 40.0 40.0 40.0Book 80.0 70.0 35.0 35.0 40.0 55.0 25.0 45.0 45.0 45.0Politics 60.0 55.0 35.0 95.0 100.0 100.0 55.0 75.0 75.0 80.0History 70.0 65.0 65.0 60.0 80.0 85.0 40.0 80.0 80.0 80.0Education 80.0 75.0 30.0 30.0 45.0 50.0 25.0 60.0 60.0 55.0Travel 60.0 75.0 40.0 40.0 25.0 25.0 25.0 40.0 35.0 40.0Spirituality 80.0 65.0 45.0 90.0 90.0 85.0 55.0 80.0 90.0 95.0Avg 72.1 68.6 37.9 55.0 66.4 64.2 39.3 60.0 60.8 62.1Table 2: Per-Topic & Per-Gender Accuracy of Cross-Topic Gender Attribution on Blog Data (Experiment-II)using paired student?s t-test.6Interestingly, the best performing approaches arecharacter-level language models, performing sub-stantially better (71.30% for n=2) than both thetoken-level language models (66.1% for n=2) andthe PCFG model (64.10%).
The difference betweenCLM(n=2) and PCFG is statistically significant (p =0.015 < 0.05) using paired student?s t-test, while thedifference between TLM(n=2) and PCFG is not.6We also experimented with the interpolated PCFG modelfollowing Raghavan et al (2010) using various interpolationdataset, but we were not able to achieve a better result in ourexperiments.
We omit the results of interpolated PCFG modelsfor brevity.As will be seen in the following experiment(Experiment-II) using the Blog dataset as well, theperformance of PCFG models is very close to thatof unigram language models.
As a result, one mightwonder whether PCFG models are learning any use-ful syntactic pattern beyond terminal productionsthat can help discriminating gender-specific styles inlanguage.
This question will be partially answeredin the fourth experiment (Experiment-IV) usingthe Scientific Paper dataset, where PCFG modelsdemonstrate considerably better performance overthe unigram language models.Following Raghavan et al (2010), we also exper-82lexicon based deep syntax morphology b.o.w.
shallow lex-syntaxGender Gender PCFG CLM CLM CLM ME TLM TLM TLMData Type Genie Guesser n=1 n=2 n=3 n=1 n=2 n=3Male Only 85.0 63.0 59.0 96.0 94.0 99.0 62.0 68.0 68.0 68.0Female Only 9.0 0.0 36.0 10.0 8.0 18.0 61.0 34.0 32.0 32.0All 47.0 31.5 47.5 53.0 51.0 58.5 61.5 51.0 50.0 50.0Table 3: Overall Accuracy of Cross-Topic /Cross-Genre Gender Attribution on Scientific Papers (Experiment-III)imented with ensemble methods that linearly com-bine the output of different classifiers, but we omitthe results in Table 1, as we were not able to ob-tain consistently higher performance than the simplecharacter-level language models in our dataset.
[Experiment-II: Cross-Topic] Next we performcross-topic experiments using the same blog dataset,in order to quantify the robustness of different tech-niques against topic change.
We train on 6 topics,and test on the remaining 1 topic, making 7-foldcross validation.
The results are shown in Table 2,where the top one third shows the performance forall authors, the next one third shows the performancewith respect to only female authors, the bottom onethird shows the performance with respect to onlymale authors.Again, the best performing approaches are basedon character-level language models, achieving upto68.3% in accuracy.
PCFG models and token-levellanguage models achieve substantially lower accu-racy of 59.0% and 61.5% respectively.
Per-genderanalysis in Table 1 reveals interesting insights intodifferent approaches.
In particular, we find thatGender Genie and Gender Guesser are biased to-ward male authors, attributing the majority authorsas male.
PCFG and ME on the other hand are bi-ased toward female authors.
Both character-leveland token-level language models show balanced dis-tribution between gender.
We also experimentedwith ensemble methods, but omit the results as wewere not able to obtain higher scores than simplecharacter-level language models.From these two experiments so far, we find thatPCFG models and word-level language models areneither as effective, nor as robust as character-levellanguage models for gender attribution.
Despiteoverall low performance of PCFG models, this re-sult suggests that PCFG models are able to learngender-specific syntactic patterns, albeit the signalsfrom deep syntax seem much weaker than those ofvery shallow morphological patterns.5.2 Experiments with Scientific PapersNext we present three different experiments usingthe scientific data, in the order of decreasing diffi-culty.
[Experiment-III: Cross-Topic & Cross-Genre]In this experiment, we challenge statistical tech-niques for gender attribution by changing both top-ics and genre across training and testing.
To do so,we train models on the blog dataset and test on thescientific paper dataset.
Notice that this is a dramati-cally harder task than the previous two experiments.Note also that previous research thus far has notreported experiments such as this, or even like theprevious one.
It is worthwhile to mention that ourgoal in this paper is not domain adaptation for gen-der attribution, but merely to quantify to what degreethe gender-specific language styles can be tracedacross different topics and genre, and which tech-niques are robust against domain change.The results are shown in Table 5.
Precisely as ex-pected, the performance of all models drop signif-icantly in this scenario.
The two baseline systems?
Gender Genie and Gender Guesser, which are notdesigned for formal scientific writings also performworse in this dataset.
Table 4 discussed in the nextexperiment will provide more insight into this byproviding per-gender accuracy of these baseline sys-tems.From this experiment, we find a rather surprisingmessage: although the performance of most statis-tical approaches decreases significantly, notice thatmost approaches perform still better than random(50%) prediction, achieving upto 61.5% accuracy.83lexicon based deep syntax morphology b.o.w.
shallow lex-syntaxGender Gender PCFG CLM CLM CLM ME TLM TLM TLMData Type Genie Guesser n=1 n=2 n=3 n=1 n=2 n=3Per Author Accuracy (%) for All AuthorsAll 47.0 31.5 76.0 73.0 72.0 76.0 70.50 63.5 62.5 62.5Per Author Accuracy (%) for Male AuthorsA 80.0 55.0 75.0 100.0 100.0 100.0 45.0 45.0 40.0 40.0B 90.0 75.0 75.0 80.0 70.0 85.0 55.0 45.0 40.0 40.0C 95.0 55.0 85.0 85.0 90.0 95.0 90.0 90.0 90.0 90.0D 85.0 65.0 100.0 95.0 100.0 100.0 100.0 100.0 100.0 100.0E 75.0 65.0 90.0 70.0 85.0 80.0 70.0 70.0 70.0 60.0Avg 85.0 63.0 85.0 86.0 89.0 92.0 72.0 70.0 68.0 66.0Per Author Accuracy (%) for Female AuthorsF 15.0 0.0 95.0 05.0 30.0 75.0 100.0 85.0 85.0 85.0G 5.0 0.0 25.0 55.0 70.0 85.0 75.0 80.0 85.0 85.0H 10.0 0.0 65.0 70.0 45.0 35.0 40.0 35.0 30.0 30.0I 15.0 0.0 80.0 85.0 45.0 50.0 65.0 35.0 35.0 35.0J 0.0 0.0 70.0 85.0 85.0 85.0 65.0 50.0 50.0 60.0Avg 9.0 0.0 67.0 60.0 55.0 66.0 69.0 57.0 57.0 59.0Table 4: Per-Author Accuracy of Cross-Topic Gender Attribution for Scientific Papers (Experiment-IV)Considering that the models are trained on dras-tically different topics and genre, this result sug-gests that there are indeed gender-specific linguis-tic signals beyond different topics and genre.
Thisis particularly interesting given that scientific paperscorrespond to very formal writing where gender-specific language styles are not likely to be conspic-uous (e.g., Janssen and Murachver (2004)).
[Experiment-IV: Cross-Topic] Next we performcross-topic experiment, only using the scientific pa-per dataset.
Because the stylistic difference in genreis significantly more prominent than the stylistic dif-ference in topics, this should be a substantially eas-ier task than the previous experiment.
Nevertheless,previous research to date has not attempted to eval-uate gender attribution techniques across differenttopics.
Here we train on 4 authors per gender (8 au-thors in total), and test on the remaining 2 authors,making 5-fold cross validation.
As before, the classdistributions are balanced in both training and testdata.The experimental results are shown in Table 4,where we report per-author, per-gender, and overallaverage accuracy.
As expected, the overall perfor-mance increase dramatically, as models are trainedon articles in the same genre.
It is interesting tosee how Gender Genie and Gender Guesser are ex-tremely biased toward male authors, achieving al-most zero accuracy with respect to articles writtenby female authors.
Here the best performing modelsare PCFG and CLM(n=3), both achieving 76.0% inaccuracy.
Token-level language models on the otherhand achieve significantly lower performance.Remind that in the first two experiments basedon the blog data, PCFG models and token-level lan-guage models performed similarly.
Given that, it isvery interesting that PCFG models now perform justas good as character-level language models, whileoutperforming token-level language models signifi-cantly.
We conjecture following two reasons to ex-plain this:?
First, scientific papers use very formal lan-guage, thereby suppressing gender-specific lex-ical cues that are easier to detect (e.g., emptywords such as ?lovely?, ?gorgeous?
(Lakoff,1973)).
In such data, deep syntactic patternsplay a much stronger role in detecting genderspecific language styles.
This also indirectly84lexicon based deep syntax morphology b.o.w.
shallow lex-syntaxGender Gender PCFG CLM CLM CLM ME TLM TLM TLMData Type Genie Guesser n=1 n=2 n=3 n=1 n=2 n=3Male Only 85.0 63.0 86.0 92.0 92.0 91.0 86.0 86.0 87.0 88.0Female Only 9.0 0.0 84.0 88.0 87.0 92.0 91.0 83.0 84.0 86.0All 47.0 31.5 85.0 90.0 88.50 91.50 88.50 85.0 85.5 87.0Table 5: Overall Accuracy of Topic-Balanced Gender Attribution on Scientific Papers (Experiment-V)addresses the concern raised in Experiment-I& II as to whether the PCFG models are learn-ing any syntactic pattern beyond terminal pro-ductions that are similar to unigram languagemodels.?
Second, our dataset is constructed in such away that the training and test data do not sharearticles written by the same authors.
Further-more, the authors are chosen so that the mainresearch topics are substantially different fromeach other.
Therefore, token-based languagemodels are likely to learn topical words andphrases, and suffer when the topics change dra-matically between training and testing.
[Experiment-V: Balanced Topic] Finally, wepresent the conventional experimental set up, wheretopic distribution is balanced between training andtest dataset.
This is not as interesting as the previoustwo scenarios, however, we include this experimentin order to provide a loose upper bound.
Becausewe choose each different author from each differentsub-topic of research, we need to split articles by thesame author into training and testing to ensure bal-anced topic distribution.
We select 80% of articlesfrom each author as training data, and use the re-maining 20% as test data, resulting in 5-fold crossvalidation.This is the easiest task among the three exper-iments using the scientific paper data, hence theperformance increases substantially.
As before,character-level language models perform the best,with CLM n=3 reaching extremely high accuracyof 91.50%.
All other statistical approaches performvery well achieving at least 85% or higher accuracy.Note that token-level language models performvery poorly in the previous experimental setting,while performing close to the top performer in thisexperiment.
We make the following two conclusionsbased on the last two experiments:?
Token-level language models have the ten-dency of learning topics words, rather than juststylometric cues.?
When performing cross-topic gender attribu-tion (as in Experiment-IV), PCFG models aremore robust than token-level language models.6 ConclusionsWe postulate that previous study in gender attribu-tion might have been overly optimistic due to gen-der specific preference on topics and genre.
We per-form the first comparative study of machine learn-ing techniques for gender attribution consciously re-moving gender bias in topics.
Rather unexpect-edly, we find that the most robust approach is basedon character-level language models that learn mor-phological patterns, rather than token-level languagemodels that learn shallow lexico-syntactic patterns,or PCFG models that learn deep syntactic patterns.Another surprising finding is that we can trace sta-tistical evidence of gender-specific language stylesbeyond topics and genre, and even in modern scien-tific papers.AcknowledgmentsWe thank reviewers for giving us highly insightfuland valuable comments.ReferencesShlomo Argamon, Moshe Koppel, Jonathan Fine, andAnat Rachel Shimoni.
2003.
Gender, genre, and writ-ing style in formal written texts.
Text, 23.Shlomo Argamon, Moshe Koppel, James W. Pennebaker,and Jonathan Schler.
2007.
Mining the blogosphere:85Age, gender and the varieties of selfexpression.
InFirst Monday, Vol.
12, No.
9.John Blitzer, Ryan McDonald, and Fernando Pereira.2006.
Domain adaptation with structural correspon-dence learning.
In Conference on Empirical Methodsin Natural Language Processing, Sydney, Australia.Faye Crosby and Linda Nyquist.
1977.
The female reg-ister: an empirical study of lakoff?s hypotheses.
InLanguage in Society, 6, pages 313 ?
322.Penelope Eckert and Sally McConnell-Ginet.
2003.
Lan-guage and gender.
Cambridge University Press.Susan C. Herring and John C. Paolillo.
2006.
Genderand genre variations in weblogs.
In Journal of Soci-olinguistics, Vol.
10, No.
4., pages 439 ?459.Anna Janssen and Tamar Murachver.
2004.
The relation-ship between gender and topic in gender-preferentiallanguage use.
In Written Communication, 21, pages344?
367.Dan Klein and Christopher D. Manning.
2003.
Accurateunlexicalized parsing.
In Proceedings of the 41st An-nual Meeting on Association for Computational Lin-guistics, pages 423?430.
Association for Computa-tional Linguistics.Moshe Koppel, Shlomo Argamon, and Anat Shimoni.2002.
Automatically categorizing written texts byauthor gender.
Literary and Linguistic Computing,17(4):401?412, June.Robin T. Lakoff.
1973.
Language and woman?s place.
InLanguage in Society, Vol.
2, No.
1, pages 45 ?
80.Andrew Kachites McCallum.
2002.
Mal-let: A machine learning for language toolkit.http://www.cs.umass.edu/ mccallum/mallet.Maureen C. McHugh and Jennifer Hambaugh.
2010.She said, he said: Gender, language, and power.
InHandbook of Gender Research in Psychology.
Volume1: Gender Research in General and Experimental Psy-chology, pages 379 ?
410.Arjun Mukherjee and Bing Liu.
2010.
Improving gen-der classification of blog authors.
In Proceedings ofthe 2010 Conference on Empirical Methods in NaturalLanguage Processing, EMNLP ?10, pages 207?217,Stroudsburg, PA, USA.
Association for ComputationalLinguistics.Fuchun Peng, Dale Schuurmans, Vlado Keselj, and Shao-jun Wang.
2003a.
Language independent authorshipattribution with character level n-grams.
In EACL.Funchun Peng, Dale Schuurmans, and Shaojun Wang.2003b.
Language and task independent text catego-rization with simple language models.
In Proceedingsof the 2003 Human Language Technology Conferenceof the North American Chapter of the Association forComputational Linguistics.Sindhu Raghavan, Adriana Kovashka, and RaymondMooney.
2010.
Authorship attribution using proba-bilistic context-free grammars.
In Proceedings of theACL, pages 38?42, Uppsala, Sweden, July.
Associa-tion for Computational Linguistics.Deborah Tannen.
1991.
You just don?t understand:Women and men in conversation.
Ballantine Books.Ozlem Uzner and Boris Katz.
2005.
A Compara-tive Study of Language Models for Book And Au-thor Recognition.
In Second International Joint Con-ference on Natural Language Processing:Full Papers,pages 1969?980.
Association for Computational Lin-guistics.William Yang Wang and Kathleen R. McKeown.2010.
?got you!?
: Automatic vandalism detec-tion in wikipedia with web-based shallow syntactic-semantic modeling.
In 23rd International Conferenceon Computational Linguistics (Coling 2010), page1146?1154.Zhili Wu, Katja Markert, and Serge Sharoff.
2010.
Fine-grained genre classification using structural learningalgorithms.
In Proceedings of the 48th Annual Meet-ing of the Association for Computational Linguistics,pages 749?759, Uppsala, Sweden, July.
Associationfor Computational Linguistics.86
