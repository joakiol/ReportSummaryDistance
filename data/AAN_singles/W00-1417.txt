Content Aggregation in Natural Language HypertextSummarization of OLAP and Data Mining DiscoveriesJacques RobinUniversidade Federal de Pernambuco (UFPE)Centro de Informfitica (CIn)Caixa Postal :785150732-970 - Recife, Braziljr@di.ufpe.brEloi L. FaveroUniversidade Federal do Parfi (UFPA)Departamento deInform~itica (DI)- , , =.=.
~ampus doGuarna66075-900 - Bel6m, Patti,ellf@di.ufpe.brAbstractWe present a new approach to paratacticcontent aggregation in the context ofgenerating hypertext summaries of OLAPand data mining discoveries.
Two keyproperties make this approach innovativeand interesting: (1) it encapsulatesaggregation inside the sentence planningcomponent, and (2) it relies on a domainindependent algorithm working on a datastructure that abstracts from lexical andsyntactic knowledge.1 Research context: hypertextexecutive summary generation forintelligent decision-supportIn this paper, we present a new approach tocontent aggregation in Natural NanguageGeneration (NLG).
This approach has beendeveloped for the NLG system HYSSOP(HYpertext Summary System of On-lineanalytical Processing) which summarizes OLAP(On-Line Analytical Processing) and DataMining discoveries into an hypevtext report.HYSSOP is itself part of the IntelligentDecision-Support System (IDSS) MATRIKS(Multidimensional Ana lys i s  and TextualReporting for Insight Knowledge Search), whichaims to provide a comprehensive knowledgediscovery environment through seamlessintegration of data warehousing, OLAP, datamining, expert system and NLG technologies.1.1 The MATRIKS intelligent decision-support systemThe architecture of MATRIKS is given in Fig.1.
It extends previous cutting-edge environmentsfor Knowledge Discovery in Databases (KDD)such as DBMiner (Han et al 1997) by theintegration of:?
a data warehouse hypercube explorationexpert system allowing automation andexpertise legacy of dimensional datawarehouse xploration strategies developedby human data analyst using OLAP queriesand data mining tools;?
an hypertext executive summary generatorreporting data hypercube exploration insightsin the most concise and familiar way: a fewweb pages of natural language.These two extensions allow an IDSS to be useddirectly by decision makers without constantmediation of a data analyst.1.2 The HYSSOP natural languagehypertext summary generatorTo our knowledge, the development ofHYSSOP is pioneer work in coupling OLAPand data mining with natural languagegeneration, Fig.
2.
We view such coupling as asynergetic fit with tremendous potential for awide range of practical applications.
In anutshell', while NLG is the only technology ableto completely fulfill the reporting needs ofi See Favero (2000) for further justification for thisview, as well as for details on the motivation andtechnology underlying MATRIKS.124OLAP and data mining, these two technologiesare reciprocally the only ones able to completelyfulfill the content determination needs of a keyNLG application sub-class: textualsummarization ofquantitative data.Decision .
~r - "  Data .
?
:.maker ?
L." a'n.&ly s t LUiHYSSOP:  ~ /NL hypertext L isummary - .
.
.
.
.
, o r )  ~.t .
w are,, .
.
.
.
"~hypercubel e,p,oration l- \[~.
Expert  System j }  ,~Fig.
1 - The architecture of  MA TRIKSGenerators that summarize large amount ofquantitative data by a short natural anguage text(such as ANA (Kukich 1988), GOSSIP(Carcagno and Iordanskaja 1993), PLANDoc(McKeown, Kukich and Shaw 1994) amongothers) generally perform content determinationby relying on a fixed set of domain-dependentheuristic rules.
Such an approach suffers fromtwo severe limitations that prevent it fromreporting the most interesting content from anunderlying database:o it does not scale up for analytical contextswith high dimensionality and which take intoaccount the .hi-smrical.
:.e~olution .,of datathrough time; such complex context wouldrequire a combinatorially explosive numberof summary content determination heuristicrules;o it can only select facts whose class have beenthought ahead by the rule base author, whilein most cases, it is its very unexpectednessthat makes a fact interesting to report;/.-~war~ho~--'-.,~hypercube explorat|on ).
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.I !
I el*course *!
JT I Hypertext ) planner \] | Sentence |L pla .
.
.
.
/  +rl/ { GrammatlcaZ\]L i ~ , ~  Izer 1-1 QC- C :ot0.d Web pages .
~Fig.
2 - The architecture of HYSSOPCLAP and data mining are the two technologiesthat emerged to tackle precisely these twoissues: for OLAP, efficient search in a highdimensionality, historical data search space, andfor data mining, automatic discovery in suchspaces, of hitherto unsuspected regularities orsingularities.
In the MATRIKS architecture,heuristic rules are not used to define contentworth reporting in a data warehouse xecutivesummary.
Instead, they are used to guide theprocess of searching the warehouse forunexpected facts using OLAP and data miningoperators.A data warehouse hypercube xploration expertsystem encapsulates such rules in its knowledgebase to perform content determination.
Anexample outPUt of such expert system, and inputtO HYSSOP, is ,given in Fig.
3:.
the data cellsselected for inclusion in the output textualsummary are passed along with their CLAPcontext and the data mining annotations thatjustify their relevance.
One output generated byHYSSOP from this input is given in Fig.
4. andFig.
5.125Celll c2c3c4c5C6c7c8c9c10c11c12C13cOLAP contextDimensionsproduct placeBirch Beer nation ....Jolt Cola nationBirch Beer nationBirch Beer nationColaDiet SodaDiet SodacentraleasteastDiet Soda eastDiet Soda southDiet Soda westCola ColoradoCola ColoradoCola WisconsinMeas.time ANov -10Aug +6Jun -12Sep +42Aug -30Aug.
+.1,0Sep -33Jul -40Jul +19Au 9 -17Sep -32Jul -40Jul -11Data mining annotationsDiscoveryexceptionlowlowlowhighlowroll up context (avg%) ..-prod place- 71tim e3 2 40 3 -72 5 3-2 1 17 -5 -1Drillde~nplacenationnationnationnationregion?
,low- :-.
,-..mediumhighlowlowmediummediumlow-5..- .... 7 ...... -.-.,8 ,, ,,region.-1 0 7 region-1 5 8 region1 -1 -11 region2 4 1 region-2 2 2 state-1 4 0 state0 13 ;7  stateFig.
3 - An example input of  HYSSOP, derived from an example retailing database taken from,(Sarawagi, Agrawal and Megiddo, 1998).
The part inside the bold sub-frame is the input to thesentence plannerLast year, the most atypical sales variations from one month to the next occurred for:?
Birch Beer with a 42% national increase from September to October;?
Diet Soda with a 40% decrease in the Eastern region from July to August.At the next level of idiosyncrasy came:?
Cola "s Colorado sales, falling 40% from July to August and then a further 32% from September to October;?
again Diet Soda Eastern sales, falling 33% from September to October.Less aberrant but still notably atypical were:?
again nationwide Birch Beer sales'-12% from June to July and -10% from November to December;?
Cola's 11% fall from July to August in the Central region and 30% dive in Wisconsin from August toSeptember;?
Diet Soda sales" 19% increase in the Southern region from July to August, followed by its two oppositeregional variations from August to September, +10% in the East but -17% in the West;.
national Jolt Cola sales' +6._% from August to September.To know what makes one of these variations unusual in the context of this year's sales, click on it.Fig.
4 - Example of HYSSOP front-page outputThe 40% decrease in Diet Soda sales was very atypical mostly due to the combination of the two following facts.?
across the rest of the regions, the July to August average variation for that product was 9% increase,o over the rest of the year, the average monthly decrease in Eastern sales for that product was only 7%.
"o across the rest of the product line, the Eastern sales variations from July to August was 2%Fig.
5 - Example of HYSSOP follow-up page output (behind the 40%front page anchor link)The architecture of HYSSOP is given in Fig.
2.
Robin 1997), while surface syntactic realizationHYSSOP is entirely implemented in LIFE (Ait- follows the approach described in (Favero andKaci and Lincoln.
198.9), a languagethat .extends.. ..... Robin.,2000b),:- H?-SSOP~-makes -two innovativeProlog with functional programming, aritylessfeature structure unification and hierarchicaltype constraint inheritance.
For contentrealization, HYSSOP relies on feature structureunification.
Lexicalization is inspired from theapproach described in (Elhadad, McKeown andcontributions to NLG research: one to hypertextcontent planning presented in (Favero andRobin 2000a) and one to content aggregationpresented in the rest of this paper.2 Research focus: content  aggregat ion126in natural language generationNatural language generation system istraditionally decomposed in the followingsubtasks: content determination, discourse-levelcontent organization, sentence-level contentorganization, lexical content realization andgrammatical content realization.
The first three......................... subtasks together ate_often=referred toas.Jzontentplanning, and the last two together as linguisticrealization.
This separation is now fairlystandard and most implementations encapsulateeach task in a separate module (Robin 1995),(Reiter 1994).Another generation subtask that has recentlyreceived much attention is content aggregation.However, there is still no consensus on the exactscope of aggregation and on its precise relationwith the five standard generation tasks listedabove.
To avoid ambiguity, we defineaggregation here as: grouping several contentunits, sharing various semantic features, insidea single linguistic structure, in such a way thatthe shared features are maximally factored outand minimally repeated in the generated text.Defined as above, aggregation is essentially akey subtask of sentence planning.
As such,aggregation choices are constrained bydiscourse planning decisions and they in turnconstrain lexical choices.In HYSSOP, aggregation is carried out by thesentence planner in three steps:1. content factorization, which is performed ona tabular data structure called a FactorizationMatrix (FM) ;2. generation from the FM of a discourse treerepresenting the hypertext plan to pass downto the lexicalizer;3. top-down traversal of the discourse tree todetect content units with shared featuresoccurring in non-adjacent sentences and2.1 Content faetorization i,iHYSSOPThe key properties of the factorization matrixthat sets it apart from previously proposed atastructures on which to perform aggregation arethat:?
it fully abstracts from lexical and syntacticinformation;q.
~...it.
focuses, on, two =types,:ofAnformation.
keptseparate in most generators, (1) the semanticfeatures of each sentence constituent(generally represented only beforelexicalization), and (2) the linear precedenceconstraints between them (generallyrepresented only late during syntacticrealization);?
it visually captures the interaction betweenthe two, which underlies the factorizationphenomenon at the core of aggregation.In HYSSOP, the sentence planner eceives asinput from the discourse planner an FMrepresenting the yet unaggregated content to beconveyed, together with an ordered list ofcandidate semantic dimensions to consider foroutermost factoring.
The pseudo-code ofHYSSOP's aggregation algorithm is given inFig.
10.
We now illustrate this algorithm on theinput example FM that appears inside the boldsub-frame of the overall HYSSOP input given inFig.
3.
For this example, we assume that thediscourse planner directive is to factor out firstthe exception dimension, followed by theproduct dimension, i.e., FactoringStrategy =\[except,product\].
This example illustrates themixed initiative choice of the aggregationstrategy: part of it is dictated by the discourseplanner to ensure that aggregation will notadversely affect the high-level textualorganization that it carefully planned.The remaining part, in our example factoringalong the place and time dimensions, is left toannotate them as anaphora.Such annotations are then used by thelexicalizer to choose the appropriate cue word toinsert near or in place of the anaphoric item.- : : - : .
: the:initiative.~f'~the:-sentence pla n r. The.
firststep of HYSSOP's aggregation algorithm is toshift the priority dimension D of the factoringstrategy to the second leftmost column of theFM.
The second step is to sort the FM rows in(increasing or decreasing) order of their D cellvalues.
The third step is to horizontally slice the127?
FM into row groups withidentical D cellvalues.The fourth step is to merge these identical cellsand annotate the merged cell with the number ofcells that it replaced.
The FM resulting fromthese four first steps on the input FM inside thebold sub-frame of Fig.
3 using exception asfactoring dimension is given in Fig.
6.?
The fifth step consists,.,oPreetlrsi~vely'eaHingtheentire aggregation algorithm inside each rowgroup on the sub-FM to the right of D, using theremaining dimensions of the factoring strategy.Let us now follow one such recursive call: theone on the sub-FM inside a bold sub-frame inFig.
6 to the right of the exception column in thethird row group.
The result of the first fouraggregation steps of this recursive call is givenin Fig.
7.
This time it is the product dimensionthat has been left-shifted and that provided thebasis for row sorting, row grouping and cellmerging.
Further recursive calls are nowtriggered.
These calls are different from thepreceding ones, however, in that at this point allthe input constraints provided by the discourseplanner have already been satisfied.
It is thusnow up to the sentence planner to choose alongwhich dimension to perform the nextfactorization step.
In the currentimplementation, the column with the lowestnumber of distinct values is always chosen.
Inour example, this translates as factoring alongthe time dimension for some row groups andalong the space dimension for the others.
Theresult of the recursive aggregation call on thesub-FM inside the bold frame of Fig.
7 is givenin Fig.
8.
In this case, factoring occurred alongthe time dimension.
The fully aggregated FMresulting from all the recursive calls is given inFig.
9.
Note how the left to right embedding ofits cells reflects exactly the left to rightembedding of the phrases in the naturallanguage summary of Fig.
4 generated from it.2.2 Cue word generation in HYSSOPOnce content factorization is completed, thesentence planner builds in two passes thediscourse tree that the lexicalizer expects asinput.
In the first pass.
the sentence plannerpatterns the recursive structure of the tree (thatitself-prefigures the output-text linguisticconstituent structure) after the left to right andnarrowing embedding of  sub-matrices inside theFM.cell except4c8c11c12c ?l c2c3c5c6c9c10c13chigh*2~rRed '- ," .-~*3low*8!product  placeBirch Beer ?
nationDiet Soda east.
:~Dietr~.oela.- ;., ~., ~ast~ .-Cola Colora.Cola Colora.Birch Beer nationJolt Cola nationBirch Beer nationCola centrali Diet Soda eastDiet Soda southDiet Soda westCola WiscontimeSepJul~Sep~SepJulNovAugJunAugAugJulAugJulA+42-40:-33 .
: .
.
.
.
.
.
.
.
.
.
.
.
.-32-40-10+6-12-30+10+19-17-11Fig.
6 - Left shift, row grouping and cellmerging along the exception dimensionCell "l c3c5c13c6c9c10c2cproduct placeBirch Beer nation*2 nationCola central*2 WisconsinDiet Soda east*3 southwestJolt Cola ; nationtime ANov -10Jun -12Aug -30Jul -11Aug +10Jul +19Aug -17Aug +6Fig.
7 - Recursion along the product dimensionCell time i place ,A I9c Jul I south +19 I6c Aug east + 1010c *2 ,,, west -17Fig.
8 - Recursion along the time dimensioncell except4c high8c *211c med..12c *37clc  low3c *85c13c9c6c10c2cproductBirch BeerDiet SodaCola*2Diet SodaBirch Beer"2 .. :Cola*2Diet Soda*3Jolt Colaplace X timetime X placenation I Sep +42east I Jul -40Colorad Sep I -32I 0 *2 Jul -40east I Sep 1-33nation I Nov 1-10.~2, .- .
.
.
.
?
Jun, -12central I Aug -30Wiscon \] Jul -11Jul south + 19Aug east +10*2 west -17nation Aug +6128Fig.
9 - Final ful ly aggregated FM after allrecursive callsIn the second pass, the sentence plannertraverses this initial discourse tree to enrich itwith anaphoric annotations that the lexicalizerneeds to generated cue words such as "again","both", "neither", "except" etc.
Planning cueplanner output discourse tree built fo rmthe  .....aggregated FM of Fig.
9 is given in Fig.
12.
Thediscourse tree spans horizontally with .its root tothe left of  the feature structure and its leaves tothe right.
Note in Fig.
12 the cue word directive:\[anaph=loccur=2 ~a, repeated=\[product, region\]\]\].It indicates that this is the second mention in thetext of a content unit with produc( .= .
"Birchwords can be considered?art--of-,aggregation ......... Beer"~afrd:~regiow=:tiation.~T-heqexica:i~zer~useg ....since it makes the aggregation structures explicit this annotation to generate the cue word "again"to the reader and prevents ambiguities that may before the second reference to "nationwideotherwise be introduced by aggressive content Birch Beer sales".factorization.
A fragment of the sentencefactor(Matrix, FactoringStrategy)variables: Matrix = a factorization matrixFactoringStrategy =a list of pairs (Dimension, Order) where Dimension ~ dimensions(Matrix)and Order E {increasing, decreasing}RowGroups = list of sub-rnatrices of Matnxbeginff FactoringStrategy =ernptyListthen FactoringStrategy <- buildFactodngStrategy(Matrix) ;(Dim l, 0rderl  ) <- first( FactoringStrategy) ;RernainingFactoringStrategy <- rest(FactoringStrategy) ;Matrix <- leftShiftColumn(Matrix, Diml);Matrix <- sortRows(Matnx, Dim 1, Order1) ;RowGroups <- horizSlice(Matrix, Dim 1);for each RowGroup in RowGroups do:RowGroup <- mergeCells(RowGroup, Dim 1) ;(LeftSubMatrix, RighSubMatrix) <- cut(RowGroup,Diml) ;FactoredRightSubMatnx <- factor(RightSubMatrix, RernainingFactoringStrategy) ;RowGreup <- paste(LeftSubMatrix,FactoredRightSubMatrix,Dim 1) ;Matrix <- update(Matrix,RowGroup);endfor;return Matrix ;end.buildFactoringStrategy(Matrix): returns inside a list a pair (Dim, increasing) where Dim is the matrix's dimension (i.e.,column) with the lowest number of distinct values.leftShiftColumn (Matrix, Dim1): moves Dirn I to the second leftrnost column next to the cell id co/urnn.sortRows(Matrix, Diml ,0rder) :  sorts the Matrix's rows in order of their Dim1 cell value; Order specifies whether the ordershould be increasing or decreasing.horizSlice(Matrix, Dim 1): horizontally slices the Matrix into row groups with equal value along Dim I.rnergeCetls(RowGroup,Diml): merges (by definition equal valued) cells of Dim1 in RowGroup.cut(RowGroup,Diml):  cuts RowGroup into two sub-rnatrices, one to the/eft of Dim1 (including Dim1) and the other to theright of Dim1paste(LeftSubMatrix, FactoredRightSubMatrix, Diml) :  pastes together/eft and right sub-matrices.update(Matrix, RowGroup): identifies the rows R~ of Matrix whose cell ids match those of RowGroup RG and substitutethose RM by RG inside MatrixFig.
10 - HYSSOP's aggregation algorithmA special class of aggregation-related cue mentioning the group's cardinal.
An examplephrases involves not only the sentence planner summary front page generated using such aand the lexicalizer but also the discourse strategy is given in Fig.
11.
The count annotationplanner.
One discourse strategy option that in the cell merging function of HYSSOP'sHYSSOP implements is to precede each aggregation algorithm are computed for thataggregation group by a cue phrase explicitly purpose.
While the decision to use an explicit129count discourse strategy lies within the discourseplanner, the counts are computed by the sentenceplanner and their realization as cue phrases arecarried out by the lexicalizer.Last year, there were 13 exceptions in the beverage product line.The most striking was Birch Beer's 42% national fall from Sep to Oct.The remaining exceptions clustered around four products were:?
Again, Birch Beers sales accounting for other two national exceptions, both decreasing mild values:1. a 12% from Jun to Jul;2. a 10% from Nov to Dec;?
.Cola's sales accountingofor.four.exceptions: .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.  "
: " .
.
.
.
.
.
.
.
.
.
.
: .......... - ' - .
..... = ......1. two medium in Colorado, a 40% from Jul to Aug and a 32% from Aug to Sep;2. two mild, a 11% in Wisconsin from Jul to Aug and a 30% in Central region from Aug to Sep;?
Diet Soda accounting for 5 exceptions:1. one strong, a 40% slump in Eastern region from Jul to Aug;2. one medium, a 33% slump in Eastern region from Sep to Oct;3. three mild: two increasing, a 10 % in Eastern region from Aug to Sep and a 19% in Southern regionfrom Jul to Aug; and one falling, a 17% in Westem region from Aug to Sep;?
Finally, Jolt Cola's sales accounting for one mild exception, a 6% national fall from Aug to Sep.Fig.
11 HYSSOP's frontpage output using discourse strategy with explicit countscat = aggr, level =1, ngroup =2, nmsg =2common I Exceptionallity = high %% The most atypical sales variations from one moth to the next occurred= I fordistinct = I cat =msg, attr =\[product ="Birch beer", time =9, place =nation, vat=+42\]I %%Birch Beer with a 42% national increase from Sept to Octcat =msg, attr =\[product ="Diet Soda", time =7, place -=east, var=-40\]%%Diet Soda with a 40% decrease in the Eastern region from Jul to Augcat =aggr, level=l, ngroup=2, nmsg=3common I exceptionallity = medium %%At next level of idiosyncrasy came:= Idistinct = \] cat =aggr, level =2, ngroup =2, nmsg=2,I common /pr?duct=C?la '  place=Colorado %% Cola's sales =distinct = i I cat=msg, attr=\[time=7, var =-40\] %% failing 40% from Jun to JulI I cat=msg, attr=\[time=9 var =-32 %% and then a further32 from Sep to Octl cat =msg, attr =\[product ="Diet Soda", time =9, place =east, var=-33anaph \[occurr =2nd, repeated=\[product, place\]%% again Diet Soda Eastern sales, falling 33% from Sep to OctI cat =aggr .... %% Less aberrant but still notably a~/pical were: ...Fig.
12 - Fragment of LIFE feature structure representing the discourse tree output of  the sentenceplanner and input to the lexicalizer.3 Related work in content aggregationThe main previous works on contentaggregation are due to:o (Dalianis 1995, 1996), whose ASTROGENsystem generates natural languageparaphrases of formal software specificationfor validation purposes;(Huang and Fiedler 1997), whose PROVERBsystem generates natural languagemathematical proofs from a theorem proverreasoning trace;(Robin and McKeown, 1996), whoseSTREAK system generates basketball gamesummaries from a semantic network130representing the key game statistics and theirhistorical context;(Shaw 1998), whose CASPER discourse andsentence planner has been used both in thePLANDoc system that generatestelecommunication equipment installationplan documentation from an expert systemtrace and the MAGIC system that generatesextracted from a dimensional data warehousehypercube.
In contrast, he other systems all takeas input either a semantic network extractedfrom a knowledge base or a pre-linguisticrepresentation f the text to generate such asMeteer's text structure (Meteer 1992) orJackendoffs semantic structure (Jackendoff1985).
Such natural language processingICUmeasurements.In this section, we briefly compare theseresearch efforts with ours along fourdimensions: (1) the definition of aggregationand the scope of the aggregation taskimplemented in the generator, (2) the type ofrepresentation the generator takes as input andthe type of output ext that it produces, (3) thegenerator's architecture and the localization ofthe aggregation task within it, and (4) the datastructures and algorithms used to implementaggregation.3.1 Definition of the aggregation taskThe definition of aggregation that we gave at thebeginning of previous ection is similar to thoseprovided by Dalianis and Huang, although itfocuses on common feature factor izat ion toinsure aggregation remains a proper  subset ofsentence planning.
By viewing aggregation onlyas a process of combining clauses, Shaw'sdefinition is more restrictive.
In our view,aggregation is best handled prior to commit ospecific syntactic categories and the sameabstract process, such the algorithm of Fig.
10,can be used to aggregate content units insidelinguistic constituents of any syntactic ategory(clause, nominal, prepositional phrases,adjectival phrases, etc.).
In terms of aggregationtask coverage, HYSSOP focuses on paratacticforms of  aggregation.
In contrast, ASTROGEN,PROVERB and STREAK alsohypotactic and paradigmaticCASPER,performaggregation.3.2 Inputoutput textpatient status :.~.
:br~efs .~r~in~ .
: m6d i  ~a~.
~ .~  `:.~6iiented~:inputk:tend `t~~ gi.mp~ify~..the: ~vera~ .
textrepresentation and generatedA second characteristic that sets HYSSOP apartfrom other generators perfornfing aggregation isthe nature of its input: a set of data cellsgeneration task and hide important issues thatcome up in real life applications for which rawdata is often the only available input.
In terms ofoutput, HYSSOP differs from most othersystems in that it generates hypertext instead oflinear text.
It thus tackles the contentaggregation problem in a particularly demandingapplication requiring the generator tosimultaneously start from raw data, producehypertext output and enforce concisenessconstraints.3.3 Generation architecture andaggregation localizationWhile its overall architecture is a conventionalpipeline, HYSSOP is unique in encapsulating allaggregation processing in the sentence plannerand carrying it out entirely on a deep semanticrepresentation.
I  contrast, most other systemsdistribute aggregation over several processingcomponents and across several levels of internalrepresentations: deep semantic, thematic andeven surface syntactic for some of them.3.4 Data structures and algorithms foraggregationAll previous approaches to aggregations reliedon rules that included some domain-specificsemantic or lexical information.
In contrast, theaggregation algorithm used by HYSSOP isdomain independent since it relies only on (1)generic matrix row and column shufflingoperations, and (2) on a generic similarity.
=:meas ure.betveeen-arbi trary data cells.4 ConclusionWe presented a new approach to contentaggregation i the context of a very challengingand practical generation application:summarizing OLAP and data mining discoveries13tas a few linked web pages of  fluent and concisenatural language.
We believe that the keycontribution to our work is to show thefeasibility to perform effective paratacticaggregation:?
encapsulated within a single generationcomponent ( he sentence planner)@Proc.
of 5 th International.
Con/brence onApplications of Natural Language to InformationSystems, NLDB'2000, 28-30 June, VersaillesFrance.Favero E. L. and Robin J.
(2000b).
ImplementingFunctional Unification Grammars for TextGeneration as Featured Definite Clause Grammars.Submitted to Natural Language Engineering.using a domain-independent algorithm and a ~.
.
.
.
.
.
~,-~- .-,._.~,.
....... ........... DBMiner~ ..... (20.00~:....~http-ltdl~,sfia~du/DBMiner/ .
.
.
.
.
.
.
.
.
.simple data .structure,'- me -~- raetonzauon .
.
.
.
.
.
.
.
.
.
.
index.htmlmatrix, that captures the key structural and Huang G. and Fiedler A (1996) Paraphrasing andordering constraints on paratacticaggregation while completely abstractingfrom domain semantic idiosyncrasies as wellas from lexicai and syntactic details.This is a first success towards the developmentof a plug-in content aggregation component fortext generation, reusable across applicationdomains.
In future work, we intend toempirically evaluate the summaries generated byHYSSOP.ReferencesA'it-Kaci H. and Lincoln P. (1989) LIFE - A naturallanguage for natural anguage.
T.A .
Informations,30(1-2):37-67, Association pour le TraitementAutomatique des Langues, Paris France.Carcagno D. and Iordanskaja L. (1993) Contentdetermination a d text structuring; two interrelatedprocesses.
In H Horacek (ed.)
New concepts inNLG: Planning.
realisation and systems.
London:Pinter Publishers, pp 10-26.Dalianis H. (1995) Aggregation, Formal specificationand Natural Language Generation.
In Proc.
of theNLDB'95 First International Workshop on theapplication of NL to Databases, 135-149,Versailles, France.Dalianis H. (1996) Aggregation as a subtask of textand sentence planning.
In Proc.
of Florida AIResearch symposium, FLAIRS-96, Florida, pp 1-5.Elhadad M., McKeown K. and Robin J.
(1997)Floating constraints in lexica\[ choice.Computational Linguistics, 23(2).Favero E. L. (2000).
Generating hypertext summariesof data mining discoveries in multidimensionaldatabases.
PhD Thesis.
Centro de lnform~tica,UFPE, Recife, Brazil.Favero E. L. and Robin J.
(2000a).
Using OLAP anddata mining for content planning in naturallanguage generation.
Accepted for publication inaggregation argumentative text using text structure.In Proc.
of the 8th International NLG Workshop,pages 21-3, Sussex, UK.Jackendoff R. (1985) Semantics and Cognition.
MITPress, Cambridge, MA, June 15-17.Kukich K. (1988) Fluency in Natural LanguageReports in Natural Language Generation Systems,McDonald, D. & Bloc, L.
(Eds.
), Springer-Verlag.McKeown K., Kukich, K. and Shaw J.
(1994)Practical issues in automatic document generation.In Proc.
of ANLP '94, pages 7-14, Stuttgart, Oct.Meteer M. (1992) Expressibility and the problem ofefficient text planning.
Communication i  ArtificialIntelligence.
Pinter Publisher Limited, London,Reiter E. 0994) Has a Consensus NL GenerationArchitecture Appeared, and is itPsycholinguistically Plausible?
In Proc of theSeventh International Workshop on NaturalLanguage Generation (INLGW-I994), pages 163-170.
Kennebunkport, Maine, USA.Robin J.
(1995) Revision-based generation of naturallanguage summaries providing historicalbackground: corpus-based analysis, design,implementation and evaluation.
Ph.D. Thesis.CUCS-034-94, Columbia University, ComputerScience Department, New York, USA.
357p.Robin J. and McKeown K. (1996) Empiricallydesigning and evaluating a new revision-basedmodel for summary generation.
ArtificialhTtelligence, 85(1-2).
57p.Sarawagi--S.-Agrawal R and Megiddo N. (\[998)Discovery-driven exploration of MDDB datacubes.
In Proc.
Int.
Cotf of Extending Database.
.
.
.
Technology (ED2BT'98), March.Shaw J.
(1998) Segregatory coordination and ellipsisin text generation.
In Proc.
of the 17 'h COLING '98.132
