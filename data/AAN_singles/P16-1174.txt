Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, pages 1848?1858,Berlin, Germany, August 7-12, 2016.c?2016 Association for Computational LinguisticsA Trainable Spaced Repetition Model for Language LearningBurr Settles?DuolingoPittsburgh, PA USAburr@duolingo.comBrendan Meeder?Uber Advanced Technologies CenterPittsburgh, PA USAbmeeder@cs.cmu.eduAbstractWe present half-life regression (HLR), anovel model for spaced repetition practicewith applications to second language ac-quisition.
HLR combines psycholinguis-tic theory with modern machine learningtechniques, indirectly estimating the ?half-life?
of a word or concept in a student?slong-term memory.
We use data fromDuolingo ?
a popular online languagelearning application ?
to fit HLR models,reducing error by 45%+ compared to sev-eral baselines at predicting student recallrates.
HLR model weights also shed lighton which linguistic concepts are system-atically challenging for second languagelearners.
Finally, HLR was able to im-prove Duolingo daily student engagementby 12% in an operational user study.1 IntroductionThe spacing effect is the observation that peopletend to remember things more effectively if theyuse spaced repetition practice (short study periodsspread out over time) as opposed to massed prac-tice (i.e., ?cramming?).
The phenomenon was firstdocumented by Ebbinghaus (1885), using himselfas a subject in several experiments to memorizeverbal utterances.
In one study, after a day ofcramming he could accurately recite 12-syllablesequences (of gibberish, apparently).
However,he could achieve comparable results with half asmany practices spread out over three days.The lag effect (Melton, 1970) is the related ob-servation that people learn even better if the spac-ing between practices gradually increases.
For ex-ample, a learning schedule might begin with re-?Corresponding author.
?Research conducted at Duolingo.view sessions a few seconds apart, then minutes,then hours, days, months, and so on, with eachsuccessive review stretching out over a longer andlonger time interval.The effects of spacing and lag are well-established in second language acquisition re-search (Atkinson, 1972; Bloom and Shuell, 1981;Cepeda et al, 2006; Pavlik Jr and Anderson,2008), and benefits have also been shown for gym-nastics, baseball pitching, video games, and manyother skills.
See Ruth (1928), Dempster (1989),and Donovan and Radosevich (1999) for thoroughmeta-analyses spanning several decades.Most practical algorithms for spaced repetitionare simple functions with a few hand-picked pa-rameters.
This is reasonable, since they werelargely developed during the 1960s?80s, whenpeople would have had to manage practice sched-ules without the aid of computers.
However, therecent popularity of large-scale online learningsoftware makes it possible to collect vast amountsof parallel student data, which can be used to em-pirically train richer statistical models.In this work, we propose half-life regression(HLR) as a trainable spaced repetition algorithm,marrying psycholinguistically-inspired models ofmemory with modern machine learning tech-niques.
We apply this model to real student learn-ing data from Duolingo, a popular language learn-ing app, and use it to improve its large-scale, op-erational, personalized learning system.2 DuolingoDuolingo is a free, award-winning, online lan-guage learning platform.
Since launching in 2012,more than 150 million students from all over theworld have enrolled in a Duolingo course, eithervia the website1or mobile apps for Android, iOS,1https://www.duolingo.com1848(a) skill tree screen (b) skill screen (c) correct response (d) incorrect responseFigure 1: Duolingo screenshots for an English-speaking student learning French (iPhone app, 2016).
(a) A course skill tree: golden skills have four bars and are ?at full strength,?
while other skills havefewer bars and are due for practice.
(b) A skill screen detail (for the Gerund skill), showing which wordsare predicted to need practice.
(c,d) Grading and explanations for a translation exercise.
?etant un enfant il est petit?etre.V.GER un.DET.INDF.M.SG enfant.N.SG il.PN.M.P3.SG ?etre.V.PRES.P3.SG petit.ADJ.M.SGFigure 2: The French sentence from Figure 1(c,d) and its lexeme tags.
Tags encode the root lexeme, partof speech, and morphological components (tense, gender, person, etc.)
for each word in the exercise.and Windows devices.
For comparison, that ismore than the total number of students in U.S. el-ementary and secondary schools combined.
Atleast 80 language courses are currently availableor under development2for the Duolingo platform.The most popular courses are for learning English,Spanish, French, and German, although there arealso courses for minority languages (Irish Gaelic),and even constructed languages (Esperanto).More than half of Duolingo students live indeveloping countries, where Internet access hasmore than tripled in the past three years (ITU andUNESCO, 2015).
The majority of these studentsare using Duolingo to learn English, which cansignificantly improve their job prospects and qual-ity of life (Pinon and Haydon, 2010).2.1 System OverviewDuolingo uses a playfully illustrated, gamified de-sign that combines point-reward incentives withimplicit instruction (DeKeyser, 2008), masterylearning (Block et al, 1971), explanations (Fahy,2https://incubator.duolingo.com2004), and other best practices.
Early researchsuggests that 34 hours of Duolingo is equivalentto a full semester of university-level Spanish in-struction (Vesselinov and Grego, 2012).Figure 1(a) shows an example skill tree forEnglish speakers learning French.
This specifiesthe game-like curriculum: each icon representsa skill, which in turn teaches a set of themati-cally or grammatically related words or concepts.Students tap an icon to access lessons of newmaterial, or to practice previously-learned mate-rial.
Figure 1(b) shows a screen for the Frenchskill Gerund, which teaches common gerund verbforms such as faisant (doing) and ?etant (being).This skill, as well as several others, have alreadybeen completed by the student.
However, the Mea-sures skill in the bottom right of Figure 1(a) hasone lesson remaining.
After completing each rowof skills, students ?unlock?
the next row of moreadvanced skills.
This is a gamelike implementa-tion of mastery learning, whereby students mustreach a certain level of prerequisite knowledge be-fore moving on to new material.1849Each language course also contains a corpus(large database of available exercises) and a lex-eme tagger (statistical NLP pipeline for automat-ically tagging and indexing the corpus; see theAppendix for details and a lexeme tag reference).Figure 1(c,d) shows an example translation exer-cise that might appear in the Gerund skill, and Fig-ure 2 shows the lexeme tagger output for this sen-tence.
Since this exercise is indexed with a gerundlexeme tag (?etre.V.GER in this case), it is availablefor lessons or practices in this skill.The lexeme tagger also helps to provide correc-tive feedback.
Educational researchers maintainthat incorrect answers should be accompanied byexplanations, not simply a ?wrong?
mark (Fahy,2004).
In Figure 1(d), the student incorrectly usedthe 2nd-person verb form es (?etre.V.PRES.P2.SG)instead of the 3rd-person est (?etre.V.PRES.P3.SG).If Duolingo is able to parse the student responseand detect a known grammatical mistake such asthis, it provides an explanation3in plain language.Each lesson continues until the student masters allof the target words being taught in the session, asestimated by a mixture model of short-term learn-ing curves (Streeter, 2015).2.2 Spaced Repetition and PracticeOnce a lesson is completed, all the target wordsbeing taught in the lesson are added to the studentmodel.
This model captures what the student haslearned, and estimates how well she can recall thisknowledge at any given time.
Spaced repetition isa key component of the student model: over time,the strength of a skill will decay in the student?slong-term memory, and this model helps the stu-dent manage her practice schedule.Duolingo uses strength meters to visualize thestudent model, as seen beneath each of the com-pleted skill icons in Figure 1(a).
These metersrepresent the average probability that the studentcan, at any moment, correctly recall a random tar-get word from the lessons in this skill (more onthis probability estimate in ?3.3).
At four bars, theskill is ?golden?
and considered fresh in the stu-dent?s memory.
At fewer bars, the skill has grownstale and may need practice.
A student can tap theskill icon to access practice sessions and target herweakest words.
For example, Figure 1(b) shows3If Duolingo cannot parse the precise nature of the mis-take ?
e.g., because of a gross typographical error ?
it pro-vides a ?diff?
of the student?s response with the closest ac-ceptable answer in the corpus (using Levenshtein distance).some weak words from the Gerund skill.
Practicesessions are identical to lessons, except that theexercises are taken from those indexed with words(lexeme tags) due for practice according to studentmodel.
As time passes, strength meters continu-ously update and decay until the student practices.3 Spaced Repetition ModelsIn this section, we describe several spaced repeti-tion algorithms that might be incorporated into ourstudent model.
We begin with two common, estab-lished methods in language learning technology,and then present our half-life regression modelwhich is a generalization of them.3.1 The Pimsleur MethodPimsleur (1967) was perhaps the first to makemainstream practical use of the spacing and lag ef-fects, with his audio-based language learning pro-gram (now a franchise by Simon & Schuster).
Hereferred to his method as graduated-interval re-call, whereby new vocabulary is introduced andthen tested at exponentially increasing intervals,interspersed with the introduction or review ofother vocabulary.
However, this approach is lim-ited since the schedule is pre-recorded and can-not adapt to the learner?s actual ability.
Consideran English-speaking French student who easilylearns a cognate like pantalon (pants), but strug-gles to remember manteau (coat).
With the Pim-sleur method, she is forced to practice both wordsat the same fixed, increasing schedule.3.2 The Leitner SystemLeitner (1972) proposed a different spaced repeti-tion algorithm intended for use with flashcards.
Itis more adaptive than Pimsleur?s, since the spac-ing intervals can increase or decrease dependingon student performance.
Figure 3 illustrates a pop-ular variant of this method.1 2 4 8 16correctly-remembered cardsincorrectly-remembered cardsFigure 3: The Leitner System for flashcards.The main idea is to have a few boxes that corre-spond to different practice intervals: 1-day, 2-day,18504-day, and so on.
All cards start out in the 1-daybox, and if the student can remember an item afterone day, it gets ?promoted?
to the 2-day box.
Twodays later, if she remembers it again, it gets pro-moted to the 4-day box, etc.
Conversely, if she isincorrect, the card gets ?demoted?
to a shorter in-terval box.
Using this approach, the hypotheticalFrench student from ?3.1 would quickly promotepantalon to a less frequent practice schedule, butcontinue reviewing manteau often until she canregularly remember it.Several electronic flashcard programs use theLeitner system to schedule practice, by organiz-ing items into ?virtual?
boxes.
In fact, when it firstlaunched, Duolingo used a variant similar to Fig-ure 3 to manage skill meter decay and practice.The present research was motivated by the needfor a more accurate model, in response to studentcomplaints that the Leitner-based skill meters didnot adequately reflect what they had learned.3.3 Half-Life Regression: A New ApproachWe now describe half-life regression (HLR), start-ing from psychological theory and combining itwith modern machine learning techniques.Central to the theory of memory is the Ebbing-haus model, also known as the forgetting curve(Ebbinghaus, 1885).
This posits that memory de-cays exponentially over time:p = 2??/h.
(1)In this equation, p denotes the probability of cor-rectly recalling an item (e.g., a word), which isa function of ?, the lag time since the item waslast practiced, and h, the half-life or measure ofstrength in the learner?s long-term memory.Figure 4(a) shows a forgetting curve (1) withhalf-life h = 1.
Consider the following cases:1. ?
= 0.
The word was just recently practiced,so p = 20= 1.0, conforming to the idea thatit is fresh in memory and should be recalledcorrectly regardless of half-life.2.
?
= h. The lag time is equal to the half-life,so p = 2?1= 0.5, and the student is on theverge of being unable to remember.3.
?
h. The word has not been practiced fora long time relative to its half-life, so it hasprobably been forgotten, e.g., p ?
0.Let x denote a feature vector that summarizesa student?s previous exposure to a particular word,and let the parameter vector ?
contain weights thatcorrespond to each feature variable in x. Underthe assumption that half-life should increase expo-nentially with each repeated exposure (a commonpractice in spacing and lag effect research), we let?h?denote the estimated half-life, given by:?h?= 2??x.
(2)In fact, the Pimsleur and Leitner algorithms canbe interpreted as special cases of (2) using a fewfixed, hand-picked weights.
See the Appendix forthe derivation of ?
for these two methods.For our purposes, however, we want to fit ?
em-pirically to learning trace data, and accommodatean arbitrarily large set of interesting features (wediscuss these features more in ?3.4).
Suppose wehave a data set D = {?p,?,x?i}Di=1made up ofstudent-word practice sessions.
Each data instanceconsists of the observed recall rate p4, lag time ?since the word was last seen, and a feature vectorx designed to help personalize the learning expe-rience.
Our goal is to find the best model weights?
?to minimize some loss function `:?
?= arg min?D?i=1`(?p,?,x?i; ?)
.
(3)To illustrate, Figure 4(b) shows a student-wordlearning trace over the course of a month.
Each6 indicates a data instance: the vertical position isthe observed recall rate p for each practice session,and the horizontal distance between points is thelag time ?
between sessions.
Combining (1) and(2), the model prediction p?
?= 2?
?/?h?is plot-ted as a dashed line over time (which resets to 1.0after each exposure, since ?
= 0).
The trainingloss function (3) aims to fit the predicted forget-ting curves to observed data points for millions ofstudent-word learning traces like this one.We chose the L2-regularized squared loss func-tion, which in its basic form is given by:`(6; ?)
= (p?
p??
)2+ ???
?22,where 6 = ?p,?,x?
is shorthand for the trainingdata instance, and ?
is a parameter to control theregularization term and help prevent overfitting.4In our setting, each data instance represents a full lessionor practice session, which may include multiple exercises re-viewing the same word.
Thus p represents the proportion oftimes a word was recalled correctly in a particular session.185100.20.40.60.810  1  2  3  4  5  6  7???
(a) Ebbinghaus model (h = 1)00.20.40.60.810  5  10  15  20  25  30??????
(b) 30-day student-word learning trace and predicted forgetting curveFigure 4: Forgetting curves.
(a) Predicted recall rate as a function of lag time ?
and half-life h = 1.
(b) Example student-word learning trace over 30 days: 6 marks the observed recall rate p for eachpractice session, and half-life regression aims to fit model predictions p??
(dashed lines) to these points.In practice, we found it useful to optimize forthe half-life h in addition to the observed recallrate p. Since we do not know the ?true?
half-lifeof a given word in the student?s memory ?
thisis a hypothetical construct ?
we approximate italgebraically from (1) using p and ?.
We solvefor h =?
?log2(p)and use the final loss function:`(6; ?)
= (p?
p??
)2+ ?(h??h?
)2+ ???
?22,where ?
is a parameter to control the relative im-portance of the half-life term in the overall train-ing objective function.
Since ` is smooth with re-spect to ?, we can fit the weights to student-wordlearning traces using gradient descent.
See the Ap-pendix for more details on our training and opti-mization procedures.3.4 Feature SetsIn this work, we focused on features that were eas-ily instrumented and available in the productionDuolingo system, without adding latency to thestudent?s user experience.
These features fall intotwo broad categories:?
Interaction features: a set of counters sum-marizing each student?s practice history witheach word (lexeme tag).
These include thetotal number of times a student has seen theword xn, the number of times it was correctlyrecalled x?, and the number of times incor-rect x.
These are intended to help the modelmake more personalized predictions.?
Lexeme tag features: a large, sparse set ofindicator variables, one for each lexeme tagin the system (about 20k in total).
These areintended to capture the inherent difficulty ofeach particular word (lexeme tag).recall rate lag (days) feature vector xp (?/n) ?
xnx?xx?etre.V.GER1.0 (3/3) 0.6 3 2 1 10.5 (2/4) 1.7 6 5 1 11.0 (3/3) 0.7 10 7 3 10.8 (4/5) 4.7 13 10 3 10.5 (1/2) 13.5 18 14 4 11.0 (3/3) 2.6 20 15 5 1Table 1: Example training instances.
Each rowcorresponds to a data point in Figure 4(b) above,which is for a student learning the French word?etant (lexeme tag ?etre.V.GER).To be more concrete, imagine that the trace inFigure 4(b) is for a student learning the Frenchword ?etant (lexeme tag ?etre.V.GER).
Table 1 showswhat ?p,?,x?
would look like for each sessionin the student?s history with that word.
The inter-action features increase monotonically5over time,and x?etre.V.GERis the only lexeme feature to ?fire?for these instances (it has value 1, all other lexemefeatures have value 0).
The model also includes abias weight (intercept) not shown here.4 ExperimentsIn this section, we compare variants of HLR withother spaced repetition algorithms in the context ofDuolingo.
First, we evaluate methods against his-torical log data, and analyze trained model weightsfor insight.
We then describe two controlled userexperiments where we deployed HLR as part ofthe student model in the production system.5Note that in practice, we found that using the square rootof interaction feature counts (e.g.,?x?)
yielded better re-sults than the raw counts shown here.1852Model MAE?
AUC?
CORh?HLR 0.128* 0.538* 0.201*HLR -lex 0.128* 0.537* 0.160*HLR -h 0.350 0.528* -0.143*HLR -lex-h 0.350 0.528* -0.142*Leitner 0.235 0.542* -0.098*Pimsleur 0.445 0.510* -0.132*LR 0.211 0.513* n/aLR -lex 0.212 0.514* n/aConstant p?
= 0.859 0.175 n/a n/aTable 2: Evaluation results using historical logdata (see text).
Arrows indicate whether lower (?
)or higher (?)
scores are better.
The best methodfor each metric is shown in bold, and statisticallysignificant effects (p < 0.001) are marked with *.4.1 Historical Log Data EvaluationWe collected two weeks of Duolingo log data,containing 12.9 million student-word lesson andpractice session traces similar to Table 1 (for allstudents in all courses).
We then compared threecategories of spaced repetition algorithms:?
Half-life regression (HLR), our model from?3.3.
For ablation purposes, we consider fourvariants: with and without lexeme features(-lex), as well as with and without the half-life term in the loss function (-h).?
Leitner and Pimsleur, two established base-lines that are special cases of HLR, usingfixed weights.
See the Appendix for a deriva-tion of the model weights we used.?
Logistic regression (LR), a standard machinelearning6baseline.
We evaluate two variants:with and without lexeme features (-lex).We used the first 1 million instances of the datato tune the parameters for our training algorithm.After trying a handful of values, we settled on?
= 0.1, ?
= 0.01, and learning rate ?
= 0.001.We used these same training parameters for HLRand LR experiments (the Leitner and Pimsleurmodels are fixed and do not require training).6For LR models, we include the lag time x?as an addi-tional feature, since ?
unlike HLR ?
it isn?t explicitly ac-counted for in the model.
We experimented with polynomialand exponential transformations of this feature, as well, butfound the raw lag time to work best.Table 2 shows the evaluation results on the fulldata set of 12.9 million instances, using the first90% for training and remaining 10% for testing.We consider several different evaluation measuresfor a comprehensive comparison:?
Mean absolute error (MAE) measures howclosely predictions resemble their observedoutcomes:1D?Di=1|p?
p??|i.
Since thestrength meters in Duolingo?s interface arebased on model predictions, we use MAE asa measure of prediction quality.?
Area under the ROC curve (AUC) ?
or theWilcoxon rank-sum test ?
is a measure ofranking quality.
Here, it represents the proba-bility that a model ranks a random correctly-recalled word as more likely than a randomincorrectly-recalled word.
Since our model isused to prioritize words for practice, we useAUC to help evaluate these rankings.?
Half-life correlation (CORh) is the Spearmanrank correlation between?h?and the alge-braic estimate h described in ?3.3.
We usethis as another measure of ranking quality.For all three metrics, HLR with lexeme tag fea-tures is the best (or second best) approach, fol-lowed closely by HLR -lex (no lexeme tags).
Infact, these are the only two approaches with MAElower than a baseline constant prediction of the av-erage recall rate in the training data (Table 2, bot-tom row).
These HLR variants are also the onlymethods with positive CORh, although this seemsreasonable since they are the only two to directlyoptimize for it.
While lexeme tag features madelimited impact, the h term in the HLR loss func-tion is clearly important: MAE more than doubleswithout it, and the -h variants are generally worsethan the other baselines on at least one metric.As stated in ?3.2, Leitner was the spaced repeti-tion algorithm used in Duolingo?s production stu-dent model at the time of this study.
The Leitnermethod did yield the highest AUC7values amongthe algorithms we tried.
However, the top twoHLR variants are not far behind, and they also re-duce MAE compared to Leitner by least 45%.7AUC of 0.5 implies random guessing (Fawcett, 2006),so the AUC values here may seem low.
This is due in partto an inherently noisy prediction task, but also to a range re-striction: p?
= 0.859, so most words are recalled correctlyand predictions tend to be high.
Note that all reported AUCvalues are statistically significantly better than chance usinga Wilcoxon rank sum test with continuity correction.1853Lg.
Word Lexeme Tag ?kEN camera camera.N.SG 0.77EN ends end.V.PRES.P3.SG 0.38EN circle circle.N.SG 0.08EN rose rise.V.PST -0.09EN performed perform.V.PP -0.48EN writing write.V.PRESP -0.81ES liberal liberal.ADJ.SG 0.83ES como comer.V.PRES.P1.SG 0.40ES encuentra encontrar.V.PRES.P3.SG 0.10ES est?a estar.V.PRES.P3.SG -0.05ES pensando pensar.V.GER -0.33ES quedado quedar.V.PP.M.SG -0.73FR visite visiter.V.PRES.P3.SG 0.94FR suis ?etre.V.PRES.P1.SG 0.47FR trou trou.N.M.SG 0.05FR dessous dessous.ADV -0.06FR ceci ceci.PN.NT -0.45FR fallait falloir.V.IMPERF.P3.SG -0.91DE Baby Baby.N.NT.SG.ACC 0.87DE sprechen sprechen.V.INF 0.56DE sehr sehr.ADV 0.13DE den der.DET.DEF.M.SG.ACC -0.07DE Ihnen Sie.PN.P3.PL.DAT.FORM -0.55DE war sein.V.IMPERF.P1.SG -1.10Table 3: Lexeme tag weights for English (EN),Spanish (ES), French (FR), and German (DE).4.2 Model Weight AnalysisIn addition to better predictions, HLR can cap-ture the inherent difficulty of concepts that are en-coded in the feature set.
The ?easier?
conceptstake on positive weights (less frequent practice re-sulting from longer half-lifes), while the ?harder?concepts take on negative weights (more frequentpractice resulting from shorter half-lifes).Table 3 shows HLR model weights for sev-eral English, Spanish, French, and German lexemetags.
Positive weights are associated with cog-nates and words that are common, short, or mor-phologically simple to inflect; it is reasonable thatthese would be easier to recall correctly.
Negativeweights are associated with irregular forms, rarewords, and grammatical constructs like past orpresent participles and imperfective aspect.
Thesemodel weights can provide insight into the aspectsof language that are more or less challenging forstudents of a second language.Daily Retention ActivityExperiment Any Lesson PracticeI.
HLR (v. Leitner) +0.3 +0.3 -7.3*II.
HLR -lex (v. HLR) +12.0* +1.7* +9.5*Table 4: Change (%) in daily student retention forcontrolled user experiments.
Statistically signifi-cant effects (p < 0.001) are marked with *.4.3 User Experiment IThe evaluation in ?4.1 suggests that HLR is a bet-ter approach than the Leitner algorithm originallyused by Duolingo (cutting MAE nearly in half).To see what effect, if any, these gains have on ac-tual student behavior, we ran controlled user ex-periments in the Duolingo production system.We randomly assigned all students to one oftwo groups: HLR (experiment) or Leitner (con-trol).
The underlying spaced repetition algorithmdetermined strength meter values in the skill tree(e.g., Figure 1(a)) as well as the ranking of targetwords for practice sessions (e.g., Figure 1(b)), butotherwise the two conditions were identical.
Theexperiment lasted six weeks and involved just un-der 1 million students.For evaluation, we examined changes in dailyretention: what percentage of students who en-gage in an activity return to do it again the fol-lowing day?
We used three retention metrics: anyactivity (including contributions to crowdsourcedtranslations, online forum discussions, etc.
), newlessons, and practice sessions.Results are shown in the first row of Table 4.The HLR group showed a slight increase in overallactivity and new lessons, but a significant decreasein practice.
Prior to the experiment, many stu-dents claimed that they would practice instead oflearning new material ?just to keep the tree gold,?but that practice sessions did not review what theythought they needed most.
This drop in practice?
plus positive anecdotal feedback about stengthmeter quality from the HLR group ?
led us tobelieve that HLR was actually better for studentengagement, so we deployed it for all students.4.4 User Experiment IISeveral months later, active students pointed outthat particular words or skills would decay rapidly,regardless of how often they practiced.
Uponcloser investigation, these complaints could be1854traced to lexeme tag features with highly negativeweights in the HLR model (e.g., Table 3).
This im-plied that some feature-based overfitting had oc-curred, despite the L2regularization term in thetraining procedure.
Duolingo was also preparingto launch several new language courses at the time,and no training data yet existed to fit lexeme tagfeature weights for these new languages.Since the top two HLR variants were virtuallytied in our ?4.1 experiments, we hypothesized thatusing interaction features alone might alleviateboth student frustration and the ?cold-start?
prob-lem of training a model for new languages.
In afollow-up experiment, we randomly assigned allstudents to one of two groups: HLR -lex (experi-ment) and HLR (control).
The experiment lastedtwo weeks and involved 3.3 million students.Results are shown in the second row of Ta-ble 4.
All three retention metrics were signifi-cantly higher for the HLR -lex group.
The mostsubstantial increase was for any activity, althoughrecurring lessons and practice sessions also im-proved (possibly as a byproduct of the overall ac-tivity increase).
Anecdotally, vocal students fromthe HLR -lex group who previously complainedabout rapid decay under the HLR model were alsopositive about the change.We deployed HLR -lex for all students, and be-lieve that its improvements are at least partially re-sponsible for the consistent 5% month-on-monthgrowth in active Duolingo users since the modelwas launched.5 Other Related WorkJust as we drew upon the theories of Ebbinghausto derive HLR as an empirical spaced repetitionmodel, there has been other recent work drawingon other (but related) theories of memory.ACT-R (Anderson et al, 2004) is a cognitivearchitecture whose declarative memory module8takes the form of a power function, in contrast tothe exponential form of the Ebbinghaus model andHLR.
Pavlik and Anderson (2008) used ACT-Rpredictions to optimize a practice schedule forsecond-language vocabulary, although their set-ting was quite different from ours.
They assumedfixed intervals between practice exercises withinthe same laboratory session, and found that theycould improve short-term learning within a ses-8Declarative (specifically semantic) memory is widely re-garded to govern language vocabulary (Ullman, 2005).sion.
In contrast, we were concerned with mak-ing accurate recall predictions between multiplesessions ?in the wild?
on longer time scales.
Ev-idence also suggests that manipulation betweensessions can have greater impact on long-termlearning (Cepeda et al, 2006).Motivated by long-term learning goals, the mul-tiscale context model (MCM) has also been pro-posed (Mozer et al, 2009).
MCM combines twomodern theories of the spacing effect (Staddon etal., 2002; Raaijmakers, 2003), assuming that eachtime an item is practiced it creates an additionalitem-specific forgetting curve that decays at a dif-ferent rate.
Each of these forgetting curves is ex-ponential in form (similar to HLR), but are com-bined via weighted average, which approximatesa power law (similar to ACT-R).
The authorswere able to fit models to controlled laboratorydata for second-language vocabulary and a fewother memory tasks, on times scales up to severalmonths.
We were unaware of MCM at the time ofour work, and it is unclear if the additional compu-tational overhead would scale to Duolingo?s pro-duction system.
Nevertheless, comparing to andintegrating with these ideas is a promising direc-tion for future work.There has also been work on more heuris-tic spaced repetition models, such as Super-Memo (Wo?zniak, 1990).
Variants of this algo-rithm are popular alternatives to Leitner in someflashcard software, leveraging additional parame-ters with complex interactions to determine spac-ing intervals for practice.
To our knowledge, theseadditional parameters are hand-picked as well, butone can easily imagine fitting them empirically toreal student log data, as we do with HLR.6 ConclusionWe have introduced half-life regression (HLR), anovel spaced repetition algorithm with applica-tions to second language acquisition.
HLR com-bines a psycholinguistic model of human mem-ory with modern machine learning techniques, andgeneralizes two popular algorithms used in lan-guage learning technology: Leitner and Pimsleur.We can do this by incorporating arbitrarily richfeatures and fitting their weights to data.
This ap-proach is significantly more accurate at predict-ing student recall rates than either of the previousmethods, and is also better than a conventional ma-chine learning approach like logistic regression.1855One result we found surprising was that lexemetag features failed to improve predictions much,and in fact seemed to frustrate the student learn-ing experience due to over-fitting.
Instead of thesparse indicator variables used here, it may be bet-ter to decompose lexeme tags into denser and moregeneric features of tag components9(e.g., part ofspeech, tense, gender, case), and also use corpusfrequency, word length, etc.
This representationmight be able to capture useful and interesting reg-ularities without negative side-effects.Finally, while we conducted a cursory analy-sis of model weights in ?4.2, an interesting nextstep would be to study such weights for evendeeper insight.
(Note that using lexeme tag com-ponent features, as suggested above, should makethis anaysis more robust since features would beless sparse.)
For example, one could see whetherthe ranking of vocabulary and/or grammar compo-nents by feature weight is correlated with externalstandards such as the CEFR (Council of Europe,2001).
This and other uses of HLR hold the poten-tial to transform data-driven curriculum design.Data and CodeTo faciliatate research in this area, we have pub-licly released our data set and code from ?4.1:https://github.com/duolingo/halflife-regression.AcknowledgmentsThanks to our collaborators at Duolingo, particu-larly Karin Tsai, Itai Hass, and Andr?e Horie forhelp gathering data from various parts of the sys-tem.
We also thank the anonymous reviewers forsuggestions that improved the final manuscript.ReferencesJ.R.
Anderson, D. Bothell, M.D.
Byrne, S. Douglass,C.
Libiere, and Y. Qin.
2004.
An intergrated theoryof mind.
Psychological Review, 111:1036?1060.R.C.
Atkinson.
1972.
Optimizing the learning of asecond-language vocabulary.
Journal of Experimen-tal Psychology, 96(1):124?129.J.H.
Block, P.W.
Airasian, B.S.
Bloom, and J.B. Car-roll.
1971.
Mastery Learning: Theory and Practice.Holt, Rinehart, and Winston, New York.9Engineering-wise, each lexeme tag (e.g., ?etre.V.GER) isrepresented by an ID in the system.
We used indicator vari-ables in this work since the IDs are readily available; the over-head of retreiving all lexeme components would be inefficientin the production system.
Of course, we could optimize forthis if there were evidence of a significant improvement.K.C.
Bloom and T.J. Shuell.
1981.
Effects of massedand distributed practice on the learning and retentionof second language vocabulary.
Journal of Educa-tional Psychology, 74:245?248.N.J.
Cepeda, H. Pashler, E. Vul, J.T.
Wixted, andD.
Rohrer.
2006.
Distributed practice in verbal re-call tasks: A review and quantitative synthesis.
Psy-chological Bulletin, 132(3):354.Council of Europe.
2001.
Common European Frame-work of Reference for Languages: Learning, Teach-ing, Assessment.
Cambridge University Press.R.
DeKeyser.
2008.
Implicit and explicit learning.In The Handbook of Second Language Acquisition,chapter 11, pages 313?348.
John Wiley & Sons.F.N.
Dempster.
1989.
Spacing effects and their im-plications for theory and practice.
Educational Psy-chology Review, 1(4):309?330.J.J.
Donovan and D.J.
Radosevich.
1999.
A meta-analytic review of the distribution of practice effect:Now you see it, now you don?t.
Journal of AppliedPsychology, 84(5):795?805.J.
Duchi, E. Hazan, and Y.
Singer.
2011.
Adaptive sub-gradient methods for online learning and stochas-tic optimization.
Journal of Machine Learning Re-search, 12(Jul):2121?2159.H.
Ebbinghaus.
1885.
Memory: A Contributionto Experimental Psychology.
Teachers College,Columbia University, New York, NY, USA.P.J.
Fahy.
2004.
Media characteristics and onlinelearning technology.
In T. Anderson and F. Elloumi,editors, Theory and Practice of Online Learning,pages 137?171.
Athabasca University.T.
Fawcett.
2006.
An introduction to ROC analysis.Pattern Recognition Letters, 27:861?874.M.L.
Forcada, M.
Ginest?
?-Rosell, J. Nordfalk,J.
O?Regan, S. Ortiz-Rojas, J.A.
P?erez-Ortiz,F.
S?anchez-Mart?
?nez, G.
Ram?
?rez-S?anchez, andF.M.
Tyers.
2011.
Apertium: A free/open-source platform for rule-based machine trans-lation.
Machine Translation, 25(2):127?144.http://wiki.apertium.org/wiki/Main Page.ITU and UNESCO.
2015.
The state of broadband2015.
Technical report, September.S.
Leitner.
1972.
So lernt man lernen.
AngewandteLernpsychologie ?
ein Weg zum Erfolg.
VerlagHerder, Freiburg im Breisgau, Germany.A.W.
Melton.
1970.
The situation with respect to thespacing of repetitions and memory.
Journal of Ver-bal Learning and Verbal Behavior, 9:596?606.M.C.
Mozer, H. Pashler, N. Cepeda, R.V.
Lindsey,and E. Vul.
2009.
Predicting the optimal spacingof study: A multiscale context model of memory.In Advances in Neural Information Processing Sys-tems, volume 22, pages 1321?1329.1856P.I.
Pavlik Jr and J.R. Anderson.
2008.
Using amodel to compute the optimal schedule of prac-tice.
Journal of Experimental Psychology: Applied,14(2):101?117.P.
Pimsleur.
1967.
A memory schedule.
Modern Lan-guage Journal, 51(2):73?75.R.
Pinon and J. Haydon.
2010.
The benefits of the En-glish language for individuals and societies: Quanti-tative indicators from Cameroon, Nigeria, Rwanda,Bangladesh and Pakistan.
Technical report, Eu-romonitor International for the British Council.J.G.W.
Raaijmakers.
2003.
Spacing and repetitioneffects in human memory: Application of the sammodel.
Cognitive Science, 27(3):431?452.T.C.
Ruth.
1928.
Factors influencing the relative econ-omy of massed and distributed practice in learning.Psychological Review, 35:19?45.J.E.R.
Staddon, I.M.
Chelaru, and J.J. Higa.
2002.
Ha-bituation, memory and the brain: The dynamics ofinterval timing.
Behavioural Processes, 57(2):71?88.M.
Streeter.
2015.
Mixture modeling of individuallearning curves.
In Proceedings of the InternationalConference on Educational Data Mining (EDM).M.T.
Ullman.
2005.
A cognitive neuroscienceperspective on second language acquisition: Thedeclarative/procedural model.
In C. Sanz, editor,Mind and Context in Adult Second Language Acqui-sition: Methods, Theory, and Practice, pages 141?178.
Georgetown University Press.R.
Vesselinov and J. Grego.
2012.
Duolingo effective-ness study.
Technical report, Queens College, CityUniversity of New York.Wikimedia Foundation.
2002.
Wiktionary: A wiki-based open content dictionary, retrieved 2012?2015.https://www.wiktionary.org.P.A.
Wo?zniak.
1990.
Optimization of learning.
Mas-ter?s thesis, University of Technology in Pozna?n.A AppendixA.1 Lexeme Tagger DetailsWe use a lexeme tagger, introduced in ?2, to ana-lyze and index the learning corpus and student re-sponses.
Since Duolingo courses teach a moderateset of words and concepts, we do not necessarilyneed a complete, general-purpose, multi-lingualNLP stack.
Instead, for each language we use a fi-nite state transducer (FST) to efficiently parse can-didate lexeme tags10for each word.
We then use a10The lexeme tag set is based on a large morphology dictio-nary created by the Apertium project (Forcada et al, 2011),which we supplemented with entries from Wiktionary (Wiki-media Foundation, 2002) and other sources.
Each Duolingocourse teaches about 3,000?5,000 lexeme tags.Abbreviation MeaningACC accusative caseADJ adjectiveADV adverbDAT dative caseDEF definiteDET determinerFORM formal registerF feminineGEN genitive caseGER gerundIMPERF imperfective aspectINDF indefiniteINF infinitiveM masculineN nounNT neuterP1/P2/P3 1st/2nd/3rd personPL pluralPN pronounPP past participlePRESP present participlePRES present tensePST past tenseSG singularV verbTable 5: Lexeme tag component abbreviations.hidden Markov model (HMM) to determine whichtag is correct in a given context.Consider the following two Spanish sentences:?Yo como manzanas?
(?I eat apples?)
and ?Corrocomo el viento?
(?I run like the wind?).
For bothsentences, the FST parses the word como intothe lexeme tag candidates comer.V.PRES.P1.SG([I] eat) and como.ADV.CNJ (like/as).
The HMMthen disambiguates between the respective tags foreach sentence.
Table 5 contains a reference of theabbreviations used in this paper for lexeme tags.A.2 Pimsleur and Leitner ModelsAs mentioned in ?3.3, the Pimsleur and Leitneralgorithms are special cases of HLR using fixed,hand-picked weights.
To see this, consider theoriginal practice interval schedule used by Pim-sleur (1967): 5 sec, 25 sec, 2 min, 10 min, 1 hr,5 hr, 1 day, 5 days, 25 days, 4 months, and 2 years.If we interpret this as a sequence of?h?half-lifes(i.e., students should practice when p?
?= 0.5), wecan rewrite (2) and solve for log2(?h?)
as a linear1857equation.
This yields ?
= {xn: 2.4, xb: -16.5},where xnand xbare the number of practices anda bias weight (intercept), respectively.
This modelperfectly reconstructs Pimsleur?s original schedulein days (r2= 0.999, p  0.001).
Analyzing theLeitner variant from Figure 3 is even simpler: thiscorresponds to ?
= {x?
: 1, x: -1}, where x?is the number of past correct responses (i.e., dou-bling the interval), and xis the number of incor-rect responses (i.e., halving the interval).A.3 Training and Optimization DetailsThe complete objective function given in ?3.3 forhalf-life regression is:`(?p,?,x?
; ?)
= (p?
p??
)2+ ?(h??h?
)2+ ???
?22.Substituting (1) and (2) into this equation producesthe following more explicit formulation:`(?p,?,x?
; ?)
=(p?
2??2?
?x)2+ ?(??log2(p)?
2?
?x)2+ ???
?22.In general, the search for ?
?weights to minimize` cannot be solved in closed form, but since it is asmooth function, it can be optimized using gradi-ent methods.
The partial gradient of ` with respectto each ?kweight is given by:?`?
?k= 2(p???
p) ln2(2)p??(??h?
)xk+ 2?
(?h?+?log2(p))ln(2)?h?xk+ 2?
?k.In order to fit ?
to a large amount of studentlog data, we use AdaGrad (Duchi et al, 2011),an online algorithm for stochastic gradient descent(SGD).
AdaGrad is typically less sensitive to thelearning rate parameter ?
than standard SGD, bydynamically scaling each weight update as a func-tion of how often the corresponding feature ap-pears in the training data:?
(+1)k:= ?k?
?[c(xk)?12]?`?
?k.Here c(xk) denotes the number of times featurexkhas had a nonzero value so far in the SGD passthrough the training data.
This is useful for train-ing stability when using large, sparse feature sets(e.g., the lexeme tag features in this study).
Notethat to prevent computational overflow and under-flow errors, we bound p???
[0.0001, 0.9999] and?h??
[15 min, 9 months] in practice.1858
