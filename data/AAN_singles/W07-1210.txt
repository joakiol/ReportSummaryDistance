Proceedings of the 5th Workshop on Important Unresolved Matters, pages 73?80,Ann Arbor, June 2005. c?2005 Association for Computational LinguisticsSemantic composition with (Robust) Minimal Recursion SemanticsAnn CopestakeComputer Laboratory, University of CambridgeJJ Thomson Avenue, Cambridge, UKaac@cl.cam.ac.ukAbstractWe discuss semantic composition in Mini-mal Recursion Semantics (MRS) and RobustMinimal Recursion Semantics (RMRS).
Wedemonstrate that a previously defined for-mal algebra applies to grammar engineeringacross a much greater range of frameworksthan was originally envisaged.
We showhow this algebra can be adapted to compo-sition in grammar frameworks where a lex-icon is not assumed, and how this underliesa practical implementation of semantic con-struction for the RASP system.1 IntroductionMinimal Recursion Semantics (MRS: Copestake etal.
(2005)) is a flat semantic representation whichfactors semantics into elementary predications (EPs)and allows for underspecification of scope.
It hasbeen widely used, especially for HPSG.
RobustMinimal Recursion Semantics (RMRS: Copestake(2003)) is a variant of MRS which takes this fac-torisation further to allow underspecification of re-lational information as well.
While MRS has gen-erally been used with hand-built HPSG grammars,RMRS is also suitable for use with shallower ap-proaches to analysis, including part-of-speech tag-ging, noun phrase chunking and stochastic parserswhich operate without detailed lexicons.
MRSs canbe converted into RMRSs: RMRS output from shal-lower systems is less fully specified than the out-put from deeper systems, but in principle fully com-patible.
In our work, the semantics produced by adeep grammar is taken as normative when devel-oping semantic representations from shallower pro-cessing.
For English, the target semantic represen-tations are those produced by the English ResourceGrammar (ERG, Flickinger (2000)).
The MRS/RMRSapproach has been adopted as a common frameworkfor the DELPH-IN initiative (Deep Linguistic Pro-cessing with HPSG: http://www.delph-in.net).An algebra for MRS was defined by Copestake etal.
(2001) (henceforth CLF) and forms the startingpoint for the work reported here.The aim of CLF was to formalise the notion of se-mantic composition within grammars expressed in atyped feature structure (TFS) logic.
Here, we ex-tend that work to non-lexicalist approaches and alsodescribe how the formal principles of compositionused in MRS can be adapted to produce a formalismfor RMRS composition.
Thus we demonstrate thatthe algebra applies to grammar engineering acrossa much wider range of frameworks than was origi-nally envisaged.
Besides its theoretical interest, thisresult has practical benefits when combining multi-ple processing systems in that it allows compatiblesemantic representations at a phrasal level as well asat a sentence level.The next section (?2) describes the most impor-tant features of MRS, RMRS and the earlier work onthe algebra.
We then outline how the algebra canbe used for implementing deep non-TFS approaches(?3) and explain how it works with RMRS (?4).
Thisis followed by discussion of the extension to gram-mars without a detailed lexicon (?5).
To briefly illus-trate the practical applications, section (?6) outlineshow RMRS semantics is constructed from RASP (Ro-bust accurate domain-independent statistical pars-ing: Briscoe and Carroll (2002)).2 MRS, RMRS and the algebraDetails of MRS, RMRS and the algebra are given inthe cited papers, but we will briefly introduce themhere for convenience.
Fig.
1 illustrates an MRS froma deep grammar (based on the ERG output, but sim-plified for expository purposes), an equivalent RMRSand a very underspecified RMRS, derived from aPOS tagger.MRS achieves a flat representation via the use oflabels on EPs, thus factoring out scopal relation-ships.
Scope constraints (HCONS) are shown as qeqrelationships (=q equality modulo quantifiers: the73MRS representation:l0: the q(x0, h01, h02), l1: fat j(x1), l2: cat n(x2), l3: sit v 1(e3, x3), l4: on p(e4, e41, x4),l5: a q(x5, h51, h52), l6: mat n 1(x6),h01 =q l1, h51 =q l6x0 = x1 = x2 = x3, e3 = e41, x4 = x5 = x6, l1 = l2, l3 = l4RMRS equivalent to the MRS above:l0: a0: the q(x0), l0: a0: RSTR(h01), l0: a0: BODY(h02), l1: a1: fat j(x1), l2: a2: cat n(x2),l3: a3: sit v 1(e3), l3: a3: ARG1(x31), l4: a4: on p(e4, e41, x4), l4: a4: ARG1(e41), l4: a4: ARG2(x4),l5: a5: a q(x5), l5: a5: RSTR(h51), l5: a5: BODY(h52), l6: a6: mat n 1(x6),h01 =q l1, h51 =q l6x0 = x1 = x2 = x3, e3 = e41, x4 = x5 = x6, l1 = l2, l3 = l4Highly underspecified RMRS output:l0: a0: the q(x0), l1: a1: fat j(x1), l2: a2: cat n(x2), l3: a3: sit v(e3), l4: a4: on p(e4),l5: a5: a q(x5), l6: a6: mat n(x6)Figure 1: MRS and RMRS for the fat cat sat on a matdetails are not important to understand this paper).In MRS, implicit conjunction is indicated by equalitybetween labels.
For instance, the labels on l1: fat(x)and l2: cat1(x) are equated.
In this figure, we showMRS using explicit equalities (eqs: =) rather thancoindexation of variables since this corresponds tothe formalism used in the algebra.RMRS uses the same approach to scope butadopts a variant of a neo-Davidsonian representa-tion, where arguments (ARGs) are represented asdistinct elements.
In the very underspecified RMRSat the bottom of Fig.1, no relational information isknown so there are no ARGs.
Separating out ARGsfrom the EPs and allowing them to be omitted per-mits a straightforward notion of a specificity hierar-chy in terms of information content.
ARGs may alsobe underspecified: e.g., ARGn indicates that thereis some argument relationship, but it is unknownwhether it is an ARG1, ARG2 or ARG3.
In theversion of RMRS described in this paper, the ARGsare related to the main EPs via an ?anchor?
element.An EP and its associated ARGs share a unique an-chor.
This version of RMRS uses exactly the samemechanism for conjunction as does MRS: the anchorelements are required so that ARGs can still be asso-ciated with a single EP even if the label of the EP hasbeen equated with another EP.
This is a change fromCopestake (2003): the reasons for this proposal arediscussed in ?4, below.
The conjunction informa-tion is not available from a POS tagger alone and sois not present in the second RMRS in Fig.1.The naming convention adopted for the relations(e.g., sit v) allows them to be constructed withoutaccess to a lexicon.
?
v?
etc are indications of thecoarse-grained sense distinctions which can be in-ferred from part-of-speech information.
Deep gram-mars can produce finer-grained sense distinctions,indicated by ?
1?
etc, and there is an implicit hier-archy such that sit v 1 is taken as being more spe-cific than sit v. However, in what follows, we willuse simple relation names for readability.
MRS andRMRS both assume an inventory of features on vari-ables which are used to represent tense etc, but thesewill not be discussed in this paper.2.1 The MRS algebraIn the algebra introduced by CLF, semantic struc-tures (SEMENTS) for phrases consist of five parts:1.
Hooks: can be thought of as pointers into therelations list.
In a full grammar, hooks consistof three parts: a label (l), an index (i) and anexternal argument (omitted here for simplicity).2.
Slots: structures corresponding to syntac-tic/semantic unsaturation ?
they specify howthe semantics is combined.
A slot in one sign isinstantiated by being equated with the hook ofanother sign.
(CLF use the term ?hole?
insteadof ?slot?.)
For the TFS grammars consideredin CLF, the slot corresponds to the part of theTFS accessed via a valence feature.
The inven-tory of slot labels given by CLF is SUBJ, SPR,SPEC, COMP1, COMP2, COMP3 and MOD.743.
rels: The bag of EPs.4.
hcons: qeq constraints (=q).5. eqs: the variable equivalences which are the re-sults of equating slots and hooks.SEMENTs are: [l, i]{slots}[eps][hcons]{eqs}.Some rules contribute their own semantics (con-struction semantics: e.g., compound nouns).
How-ever, the MRS approach requires that this can al-ways be treated as equivalent to having an additionaldaughter in the rule.
Thus construction semanticsneed not be considered separately in the formal al-gebra, although it does result in some syntacticallybinary rules being semantically ternary (and so on).The principles of composition are:1.
A (syntactically specified) slot in one structure(the daughter which corresponds to the seman-tic head) is filled by the hook of the other struc-ture (by adding equalities).2.
The hook of the phrase is the semantic head?shook.3.
The eps of the phrase is equal to appending theeps of the daughters.4.
The eqs of the phrase is equal to appending theeqs of the daughters plus any eqs contributedby the filling of the slot.5.
The slots of the phrase are the unfilled slots ofthe daughters (although see below).6.
The hcons of the phrase is equal to appendingthe hcons of the daughters.Formally, the algebra is defined in terms of a se-ries of binary operations, such as opspec, whicheach correspond to the instantiation of a particularlabelled slot.Fig.
2 illustrates this.
The hook of cat instanti-ates the SPEC slot of a, which is the semantic head(though not the syntactic head in the ERG).
Thisleads to the equalities between the variables in theresult.
Since the SPEC slot has been filled, it is notcarried up to the phrase.
Thus, abstractly at least,the semantics of the HPSG specifier-head rule cor-responds to opspec.11As usual in MRS, in order to allow scope underspecifica-tion, the label l4 of the quantifier?s hook is not coindexed withany EP.The MRS algebra was designed to abstract awayfrom the details of the syntax and of the syntax-semantics interface, so that it can be applied togrammars with differing feature geometry.
The as-sumption in CLF is simply that the syntax selectsthe appropriate op and its arguments for each ap-plication.
i.e., semantic operations are associatedwith HPSG constructions so that there is a mappingfrom the daughters of the construction to the argu-ments of the operation.
The algebra does not attemptto completely replicate all aspects of semantic con-struction: e.g., the way that the features (represent-ing tense and so on) are instantiated on variables isnot modelled.
However, it does constrain semanticconstruction compared with the possibilities for TFSsemantic compositional in general.
For instance, asdiscussed by CLF, it enforces a strong monotonic-ity constraint.
The algebra also contributes to limit-ing the possibilities for specification of scope.
Theseproperties can be exploited by algorithms that oper-ate on MRS: e.g., generation, scope resolution.2.2 The MRS algebra and the syntax-semanticsinterfaceCLF did not discuss the syntax-semantics interfacein detail, but we do so here for two reasons.
Firstly,it is a preliminary for discussing the use of the al-gebra in frameworks other than HPSG in the fol-lowing sections.
Secondly, as CLF discuss, the con-straints that the algebra imposes cannot be fully im-plemented in a TFS.
Thus, for grammar engineeringin TFS frameworks, an additional automatic checkeris needed to determine whether a grammar meets thealgebra?s constraints.
This requires specification ofthe syntax-semantics interface so that the checkercan extract the slots from the TFSs and determinethe slot operation(s) corresponding to a rule.Unfortunately, CLF are imprecise about the alge-bra in several respects.
One problem is that theygloss over the issue of slot propagation in real gram-mars.
CLF state that for an operation opx, the slotcorresponding to opx on the semantic head is instan-tiated and all other slots appear on the result.
Forinstance, the definition of opspec states that for alllabels l 6= spec: slotl(opspec(a1, a2)) = slotl(a1)?slotl(a2).
However, this is inadequate for real gram-mars, if a simple correspondence between the slotnames and the valence paths in the feature structure75hook slots rels eqs hconscat : [l1, x1] {} [l1 : cat(x1)] {} []a : [l4, x2] {[l3, x2]spec} [l2 : a(x2, h2, h3)] {} [h2 =q l3]a cat : [l4, x2] {} [l2 : a(x2, h2, h3), l1 : cat(x1)] {l3 = l1, x2 = x1} [h2 =q l3]Figure 2: Example of the MRS algebrais assumed.
For instance, the passive rule involvescoindexing a COMP in the original lexical sign withthe SUBJ of the passive (informally, the complement?becomes?
the subject).There are two ways round this problem.
The firstis to keep the algebra unchanged, but to assume that,for instance, the subject-head grammar rule corre-sponds to opsubj in the algebra for non-passivizedcases and to opcomp1 for passives of simple tran-sitives and so on.
Though possible formally, this isnot in accord with the spirit of the approach sinceselection of the appropriate algebra operation in thesyntax-semantics interface would require non-localinformation.
Practically, it also precludes the im-plementation of an algebra checker, since keepingtrack of the slot uses would be both complex andgrammar-specific.
The alternative is to extend thealgebra to allow for slot renaming.
For instance,opcomp1-subj can be defined so that the COMP1 sloton the daughter is a SUBJ slot on the mother.1.
For all labels l 6= comp1, l 6= subj:slotl(opcomp1-subj(a)) = slotl(a)2. slotsubj(opcomp1-subj(a)) = slotcomp1(a)This means extending the inventory of operations,but the choice of operation is then locally deter-minable from the rule (e.g., the passive rule wouldspecify opcomp1-subj to be its operation).Another issue arises in grammars which allow foroptional complements.
For instance, one approachto a verb like eat is to give it a single lexical en-try which corresponds to both transitive and intran-sitive uses.
The complement is marked as optionaland the corresponding variable in the semantics isassumed to be discourse bound if there is no syn-tactic complement in the phrase.
Optional comple-ments can be discharged by a construction.
This ap-proach is (arguably) appropriate for eat because theintransitive use involves an implicit patient (e.g., Ialready ate means I already ate something), in con-trast to a verb like kick.
CLF do not discuss op-tionality but it can be formalised in the algebra interms of a construction-specified sement which hasa hook containing the discourse referent and is oth-erwise empty.
For instance, an optional complementconstruction corresponds to opcomp1(a1, a2) wherea1 is the head (and the only daughter appearing inthe TFS for the construction) and a2 is stipulated bythe rule to be [l, d]{}[][]{}, where d is the discourse-bound referent.3 The algebra in non-lexicalist grammarsCLF motivate the MRS algebra in terms of formalisa-tion of the semantics of constraint-based grammars,such as HPSG, but, as we outline here, it is equallyapplicable to non-lexicalist frameworks.
With a suit-able definition of the syntax-semantics interface, thealgebra can be used with non-TFS-based grammars.Fig.
3 sketches an example of MRS semantics for aCFG.
A syntax-semantic interface component of therule (shown in the second line of the figure) specifiesthe ops and their daughters: the IOBJ slot of the verbis instantiated with the first NP?s hook and the OBJslot of the result is instantiated with the hook of thesecond NP.
The idea is extremely similar to the useof the algebra with TFS but note that with the ad-dition of this syntax-semantic interface, the algebracan be used directly to implement semantic compo-sition for a CFG.This still relies on the assumption that all slotsare known for every lexical item: semantically thegrammar is lexicalist even though it is not syntacti-cally.
In fact this is analogous to semantic compo-sition in GPSG (Gazdar et al, 1985) in that conven-tional lambda calculus also assumes that the seman-tic properties are known at the lexical level.4 RMRS composition with deep grammarsThe use of the CLF algebra in RMRS compositionwith deep lexicalist grammars is reasonably straight-76VP -> Vditrans NP1 NP2opobj(opiobj(Vditrans, NP1), NP2)MRSs for application of the rule to give a cat a rat.hook slots rels eqsgive : [l1, e1] {[l1, x12]subj , [l1 : give(e1, x12, x13, x14)] {}[l1, x13]obj , [l1, x14]iobj}a cat : [l4, x2] {} [l2 : a(x2, h2, h3), l1 : cat(x1)] {l3 = l1, x2 = x1}a rat : [l7, x5] {} [l5 : a(x5, h5, h6), l4 : rat(x4)] {l6 = l4, x5 = x4}iobj : [l1, e1] {[l1, x12]subj , [l1, x13]obj} [l1 : give(e1, x12, x13, x14), {l3 = l1, x2 = x1,l2 : a(x2, h2, h3), l1 : cat(x1)] l1 = l4, x14 = x2}obj : [l1, e1] {[l1, x12]subj} [l1 : give(e1, x12, x13, x14), {l3 = l1, x2 = x1,l2 : a(x2, h2, h3), l1 : cat(x1), l1 = l4, x14 = x2,l5 : a(x5, h5, h6), l4 : rat(x4)] l1 = l7, x13 = x5}Figure 3: MRS algebra with a CFG (hcons omitted for clarity)forward.2 The differences between MRS and RMRSare that RMRS uses anchors and factors out theARGs.
Thus for RMRS, we need to redefine the no-tion of a semantic entity from the MRS algebra toadd anchors.
An RMRS EP thus contains:1. a handle, which is the label of the EP2.
an anchor (a)3. a relation4.
up to one argument of the relationHooks also include anchors: {[l, a, i]} is a hook.Instead of the rels list only containing EPs, suchas l1:chase(e,x,y), it contains a mixture of EPsand ARGs, with associated anchors, such asl1:a1:chase(e), l1:a1:ARG1(x), l1:a1:ARG2(y).
Butformally ARGs are EPs according to the definitionabove, so this requires no amendment of the alge-bra.
Fig.
4 shows the RMRS version of Fig.
2.As mentioned above, earlier forms of RMRS usedan explicit representation for conjunction: the in-group, or in-g. Reasons to avoid explicit binaryconjunction were discussed with respect to MRS byCopestake et al (2005) and readers are referred tothat paper for an explanation: essentially the prob-lem is that the syntactic assumptions influence thesemantic representation.
e.g., the order of combi-nation of intersective modifiers affects the semantic2Current DELPH-IN grammars generally construct MRSswhich may be converted into RMRSs.
However, RMRS haspotential advantages, for instance in allowing more extensivelexical underspecification than is possible with MRS: e.g.,(Haugereid, 2004).representation, though it has no effect on denotation.The binary in-g suffers from this problem.One alternative would be to use an n-ary conjunc-tion symbol.
However such representations cannotbe constructed compositionally if modification is bi-nary branching as there is no way of incrementallyadding the conjuncts.
Another option we consideredwas the use of, possibly redundant, conjunction re-lations associated with each element which could becombined to produce a flat conjunction.
This leadsto a spurious in-g in the case where there is no mod-ifier.
This looks ugly, but more importantly, doesnot allow for incremental specialisation, althoughthe demonstration of this would take us too far fromthe main point of this paper.We therefore assume a modified version of RMRSwhich drops in-g symbols but uses anchors instead.This means that RMRS and MRS TFS grammars canbe essentially identical apart from lexical types.
Fur-thermore, it turns out that, for composition withouta lexicon, an anchor is needed in the hook regardlessof the treatment of conjunction (see below).5 RMRS composition without a lexiconWe now discuss the algebra for grammars whichdo not have access to subcategorization informationand thus are neither syntactically nor semanticallylexicalist.
We concentrate in particular on composi-tion for the grammar used in the RASP system.
RASPconsists of a tokenizer, POS tagger, lemmatizer, tagsequence grammar and statistical disambiguator.
Ofthe robust analysers we have looked at, RASP pro-77hook slots rels eqs hconscat : [l1, a1, x1] {} [l1 : a1 : cat(x1)] {} []a : [l4, a2, x2] {[l3, a2, x2]spec} [l2 : a2 : a(x2), {} [h2 =q l3]l2 : a2 : rstr(h2), l2 : a2 : body(h2)]a cat : [l4, a4, x2] {} [l1 : a1 : cat(x1), l2 : a2 : a(x2), {l3 = l1, [h2 =q l3]l2 : a2 : rstr(h2), l2 : a2 : body(h2) x2 = x1]Figure 4: Example of the RMRS algebra.vides the biggest challenge for the RMRS approachbecause it provides quite detailed syntactic analy-ses which are somewhat dissimilar to the ERG: itis an intermediate rather than a shallow processor.The RMRS approach can only be fully successful tothe extent that it abstracts away from the differencesin syntactic analyses assumed by different systems,so intermediate processors are more difficult to dealwith than shallow ones.Instead of normal lexical entries, RASP uses thePOS tags for the words in the input.
For the exam-ple in Fig.
1, the output of the POS tagging phase is:the AT fat JJ cat NN1 sit+ed VVD on II a AT1mat NN1The semantics associated with the individual wordsin the sentence can be derived from a ?lexicon?
ofPOS tags, which defines the EPs.
Schematically:AT lexrel q(x) NN1 lexrel n(x)AT1 lexrel q(x) VVD lexrel v(epast)JJ lexrel j(x) II lexrel p(e)Here, ?lexrel?
is a special symbol, which is tobe replaced by the individual lemma (with aleading underscore) ?
e.g., lexrel v(epast) yieldsl1:a1: sit v(e).
Producing the semantics from thetagger output and this lexicon is a simple matter ofsubstitution.
All EPs are labelled with unique labelsand all variables are different unless repeated in thesame lexical entry.If the analysis were to stop at POS tagging, thesemantic composition rules would apply trivially.There are no slots, the hooks are irrelevant and thereare no equalities.
The composition principle of ac-cumulation of elementary predications holds, so thesemantics of the result involves an accumulation ofthe rels (see the example at the bottom of Fig.
1).When using the full RASP parser, although wecannot expect to obtain all the details available fromdeep grammars, we can derive some relational struc-ture.
For instance, given a sentence such as thecat chased the rat, it should be possible to derivethe ARG1 and ARG2 for chase by associating theARG1 with the application of the S/np_vp RASPrule (i.e., S->NP VP) and the ARG2 with the appli-cation of the V1/v_np rule.
But since there can beno slot information in the lexical structures (at leastnot for open-class words), it is necessary to modifythe lexicalist approach to semantics taken so far.We assume that both the ARGs and the slots arespecified at a phrasal level rather than lexically.
Asmentioned in ?2.1, the MRS algebra allows for rulesto contribute semantics as though they were normalphrases.
The central idea in the application of thealgebra to RASP is to make use of construction se-mantics in all rules.
Fig.
5 illustrates this with theV1/v_np rule (the NP has been simplified for clar-ity) assuming the same sort of syntax-semantics in-terface specification as shown earlier for the CFG.This is semantically ternary because of the rule se-mantics.
The rule has an ARG2 slot plus a slot Rwhich is instantiated by the verb?s hook.
In effect,the rule adds a slot to the verb.It is necessary for the anchor of the argument-taking structure to be visible at all points where ar-guments may be attached.
For instance, in the ex-ample above, the anchor of the verb chase has tobe accessible when the ARG1 relation is introduced.Although generally the anchor will correspond to theanchor of the semantic head daughter, this is not thecase if there is a scopal modifier (consider a cat didnot chase a rat: the ARG1 must be attached to chaserather than to not).
This is illustrated by not sleepin Fig.
6.
Because not is associated with a uniquetag in RASP, it can be assigned a slot and an ARG1directly.
The anchor of the result is equated withthe label of sleep and thus the subject ARG1 can beappropriately attached.
So the hook would have toinclude an anchor even if explicit conjunction wereused instead of equating labels.78VP -> V NPoparg2(opr(rule, V), NP)chase : [l1, a1, e1] {} [l1 : a1 : chase(e1)] {}rule : [l2, a2, e2] {[l2, a2, e2]r, [l2 : a2 : ARG2(x2)] {}[l4, a4, x2]arg2}(rule V)/r : [l2, a2, e2] {[l4, x2]arg2} [l2 : a1 : ARG2(x2), l1 : a1 : chase(e1)] {l1 = l2, e2 = e1}it : [l3, a3, x3] {} [l3 : a3 : pron(x3)] {}chase it : [l2, a2, e2] {} [l2 : a2 : ARG2(x2), l1 : a1 : chase(e1), {l1 = l2, e2 = e1,l3 : a3 : pron(x3)] l4 = l3, x2 = x3}Figure 5: RASP-RMRS algebra (hcons omitted)not : [l1, a2, e2] {[l2, a3, e2]mod} [l1 : a1 : not(e2), l1 : a1 : ARG1(h4)] {} [h4 =q l2]sleep : [l2, a2, e2] {} [l2 : a2 : sleep(e2)] {} []not sleep : [l1, a2, e2] {} [l1 : a1 : not(e2), l1 : a1 : ARG1(h4), {} [h4 =q l3]l2 : a2 : sleep(e2)]Figure 6: RASP-RMRS illustrating the use of the anchor6 Experiments with RASP-RMRSIn this section, we outline the practical implementa-tion of the algebra for RASP-RMRS.
The RASP tagsequence grammar is formally equivalent to a CFG:it uses phrase structure rules augmented with fea-tures.
As discussed, the algebra requires that opsare specified for each rule application, and the eas-iest way of achieving this is to associate semanticcomposition rules with each rule name.
Composi-tion operates on the tree output from RASP, e.g.,:(|T/txt-sc1/----|(|S/np_vp|(|NP/det_n1| |Every:1_AT1|(|N1/n| |cat:2_NN1|))(|V1/v| |bark+ed:3_VVD|)))Composition operates bottom-up: the semanticstructures derived from the tags are combined ac-cording to the semantics associated with the rule.The implementation corresponds very directly to thealgebra, although the transitive closure of the equali-ties is computed on the final structure, since nothingrequires that it be available earlier.The notation used to specify semantics associatedwith the rules incorporates some simplifications toavoid having to explicitly specify the slot and ops.The specification of equalities between variables andcomponents of the individual daughters?
hooks is aconvenient shorthand for the full algebra.rule V1/v_npdaughters V NPsemhead Vhook [l,a,e] rels {l:a:ARG2(x)}eqs {x=NP.index,l=V.label,a=V.anchor}If no semantic rule is specified corresponding toa rule used in a tree, the rels are simply appended.Semantic composition is thus robust to omissions inthe semantic component of the grammar.
In fact, se-mantic rules can be constructed semi-automatically,rather than fully manually, although we do not havespace to discuss this in detail here.There are cases of incompatibility between RASP-RMRS and ERG-RMRS.
For example, the ERG treatsit as expletive in it rains: the lexical entry for rainspecifies an expletive subject (i.e., a semanticallyempty it).
RASP makes no such distinction, sinceit lacks the lexical information and thus the sentencehas extraneous relations for the pronoun and an in-correct ARG1 for rain.
This is an inevitable conse-quence of the lack of lexical information in RASP.However, from the perspective of the evaluation ofthe revised algebra, the issue is whether there are anycases where compositional construction of RASP-RMRSs which match ERG-RMRSs is impossible dueto the restrictions imposed by the algebra.
No suchcases have been found.797 Related workBos et al (2004) and Bos (2005) derive semantic in-terpretations from a wide-coverage categorial gram-mar.
There are several differences between this andRASP-RMRS, but the most important arise from thedifferences between CCG and RASP.
The CCG parserrelies on having detailed subcategorization infor-mation (automatically derived from the CCG Bankwhich was semi-automatically constructed from thePenn Treebank), and thus semantic construction canassume that the arity of the predicate is lexicallyavailable.
However, because CCG is purely lexical-ist, phenomena that we expect to have constructionsemantics (e.g., compound nouns, larger numbers)have to be dealt with in a post-parsing phase ratherthan compositionally.Spreyer and Frank (2005) demonstrate RMRS con-struction from TIGER dependencies, but do not at-tempt to match a deep parser output.8 ConclusionWe have demonstrated that the MRS algebra, orig-inally intended as a formalisation of some aspectsof semantic composition in constraint-based gram-mars, can be extended to RMRS and other types ofgrammar framework and can be used as the basis ofa full implementation of composition.
The algebracan thus be used much more widely than originallyenvisaged and could be exploited by a wide range ofparsers.
Useful properties concerning monotonicityand scope (see Fuchss et al (2004)) are thus guaran-teed for a range of grammars.
Phrasal-level com-patibility of RMRS (to the extent that this is syn-tactically possible) is also an important result.
Themain practical outcome of this work so far has beena semantic component for the RASP system whichproduces representations compatible with that of theERG without compromising RASP speed or robust-ness.
RASP-RMRSs have already been used in sys-tems for question answering, information extraction,email response, creative authoring and ontology ex-traction (e.g., Uszkoreit et al (2004), Watson et al(2003), Herbelot and Copestake (2006)).AcknowledgementsThis work was funded by EPSRC (EP/C010035/1).I am very grateful to the anonymous reviewers fortheir insightful comments, which sadly I have hadto ignore due to the constraints of time and space.
Ihope to address them in a later paper.ReferencesJohan Bos, Stephen Clark, Mark Steedman, James R.Curran and Julia Hockenmaier 2004.
Wide-CoverageSemantic Representations from a CCG Parser.
COL-ING ?04, Geneva.Johan Bos.
2005.
Towards Wide-Coverage Semantic In-terpretation.
Sixth International Workshop on Compu-tational Semantics IWCS-6.
42?53.Ted Briscoe and John Carroll.
2002.
Robust accuratestatistical annotation of general text.
LREC-2002, LasPalmas, Gran Canaria.Ann Copestake, Alex Lascarides, and Dan Flickinger.2001.
An Algebra for Semantic Construction inConstraint-based Grammars.
ACL-01, Toulouse.Ann Copestake.
2003.
Report on the design of RMRS.DeepThought project deliverable.Ann Copestake, Dan Flickinger, Ivan Sag, and Carl Pol-lard.
2005.
Minimal Recursion Semantics: An in-troduction.
Research in Language and Computation3(2?3), 281?332.Dan Flickinger 2000.
On building a more efficientgrammar by exploiting types.
Natural LanguageEngineering,6:1,15?28.Ruth Fuchss, Alexander Koller, Joachim Niehren, andStefan Thater 2004.
Minimal Recursion Semantics asDominance Constraints: Translation, Evaluation, andAnalysis.
Proceedings of the 42nd ACL, Barcelona.Gerald Gazdar, Ewan Klein, Geoffrey Pullum and IvanSag 1985.
Generalized Phrase Structure Grammar.Basil Blackwell, OxfordPetter Haugereid 2004.
Linking in Constructions.HPSG2004, Leuven.Aurelie Herbelot and Ann Copestake 2006.
Acquir-ing Ontological Relationships from Wikipedia Us-ing RMRS.
ISWC 2006 Workshop on Web ContentMining with Human Language Technologies, Athens,Georgia.Kathrin Spreyer and Anette Frank.
2005 ProjectingRMRS from TIGER Dependencies.
HPSG 2005, Lis-bon.
354?363.Hans Uszkoreit, Ulrich Callmeier, Andreas Eisele, Ul-rich Schfer, Melanie Siegel, Jakob Uszkoreit.
2004.Hybrid Robust Deep and Shallow Semantic Process-ing for Creativity Support in Document Production.KONVENS 2004, Vienna, Austria, 209?216.Rebecca Watson, Judita Preiss and EJ Briscoe.
2003.The Contribution of Domain-independent RobustPronominal Anaphora Resolution to Open-DomainQuestion-Answering.
Int.
Symposium on ReferenceResolution and its Application to Question-Answeringand Summarisation, Venice.80
