Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 358?363,Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational LinguisticsA Novel Graph-based Compact Representation of Word AlignmentQun Liu??
Zhaopeng Tu?
Shouxun Lin?
?Centre for Next Generation Locolisation ?Key Lab.
of Intelligent Info.
ProcessingDublin City University Institute of Computing Technology, CASqliu@computing.dcu.ie {tuzhaopeng,sxlin}@ict.ac.cnAbstractIn this paper, we propose a novel compactrepresentation called weighted bipartitehypergraph to exploit the fertility model,which plays a critical role in word align-ment.
However, estimating the probabili-ties of rules extracted from hypergraphs isan NP-complete problem, which is com-putationally infeasible.
Therefore, we pro-pose a divide-and-conquer strategy by de-composing a hypergraph into a set of inde-pendent subhypergraphs.
The experimentsshow that our approach outperforms both1-best and n-best alignments.1 IntroductionWord alignment is the task of identifying trans-lational relations between words in parallel cor-pora, in which a word at one language is usuallytranslated into several words at the other language(fertility model) (Brown et al, 1993).
Given thatmany-to-many links are common in natural lan-guages (Moore, 2005), it is necessary to pay atten-tion to the relations among alignment links.In this paper, we have proposed a novel graph-based compact representation of word alignment,which takes into account the joint distribution ofalignment links.
We first transform each align-ment to a bigraph that can be decomposed into aset of subgraphs, where all interrelated links arein the same subgraph (?
2.1).
Then we employa weighted partite hypergraph to encode multiplebigraphs (?
2.2).The main challenge of this research is to effi-ciently calculate the fractional counts for rules ex-tracted from hypergraphs.
This is equivalent to thedecision version of set covering problem, which isNP-complete.
Observing that most alignments arenot connected, we propose a divide-and-conquerstrategy by decomposing a hypergraph into a setFigure 1: A bigraph constructed from an align-ment (a), and its disjoint MCSs (b).of independent subhypergraphs, which is compu-tationally feasible in practice (?
3.2).
Experimen-tal results show that our approach significantly im-proves translation performance by up to 1.3 BLEUpoints over 1-best alignments (?
4.3).2 Graph-based Compact Representation2.1 Word Alignment as a BigraphEach alignment of a sentence pair can be trans-formed to a bigraph, in which the two disjoint ver-tex sets S and T are the source and target words re-spectively, and the edges are word-by-word links.For example, Figure 1(a) shows the correspondingbigraph of an alignment.The bigraph usually is not connected.
A graphis called connected if there is a path between everypair of distinct vertices.
In an alignment, words ina specific portion at the source side (i.e.
a verbphrase) usually align to those in the correspondingportion (i.e.
the verb phrase at the target side), andwould never align to other words; and vice versa.Therefore, there is no edge that connects the wordsin the portion to those outside the portion.Therefore, a bigraph can be decomposed intoa unique set of minimum connected subgraphs(MCSs), where each subgraph is connected anddoes not contain any other MCSs.
For example,the bigraph in Figure 1(a) can be decomposed into358thebookisonthedesk?????Dthebookisonthedesk?????e1Fthebookisonthedesk????
?Ee2 e3e4e5Figure 2: (a) One alignment of a sentence pair; (b) another alignment of the same sentence pair; (c) theresulting hypergraph that takes the two alignments as samples.the MCSs in Figure 1(b).
We can see that all in-terrelated links are in the same MCS.
These MCSswork as fundamental units in our approach to takeadvantage of the relations among the links.
Here-inafter, we use bigraph to denote the alignment ofa sentence pair.2.2 Weighted Bipartite HypergraphWe believe that offering more alternatives to ex-tracting translation rules could help improve trans-lation quality.
We propose a new structure calledweighted bipartite hypergraph that compactly en-codes multiple alignments.We use an example to illustrate our idea.
Fig-ures 2(a) and 2(b) show two bigraphs of the samesentence pair.
Intuitively, we can encode theunion set of subgraphs in a bipartite hypergraph,in which each MCS serves as a hyperedge, as inFigure 2(c).
Accordingly, we can calculate howwell a hyperedge is by calculating its relative fre-quency, which is the probability sum of bigraphsin which the corresponding MCS occurs dividedby the probability sum of all possible bigraphs.Suppose that the probabilities of the two bigraphsin Figures 2(a) and 2(b) are 0.7 and 0.3, respec-tively.
Then the weight of e1 is 1.0 and e2 is0.7.
Therefore, each hyperedge is associated witha weight to indicate how well it is.Formally, a weighted bipartite hypergraph H isa triple ?S, T,E?
where S and T are two sets ofvertices on the source and target sides, and E arehyperedges associated with weights.
Currently,we estimate the weights of hyperedges from an n-best list by calculating relative frequencies:w(ei) =?BG?N p(BG) ?
?
(BG, gi)?BG?N p(BG)Here N is an n-best bigraph (i.e., alignment) list,p(BG) is the probability of a bigraph BG in the n-best list, gi is the MCS that corresponds to ei, and?
(BG, gi) is an indicator function which equals 1when gi occurs in BG, and 0 otherwise.It is worthy mentioning that a hypergraph en-codes much more alignments than the input n-bestlist.
For example, we can construct a new align-ment by using hyperedges from different bigraphsthat cover all vertices.3 Graph-based Rule ExtractionIn this section we describe how to extract transla-tion rules from a hypergraph (?
3.1) and how toestimate their probabilities (?
3.2).3.1 Extraction AlgorithmWe extract translation rules from a hypergraphfor the hierarchical phrase-based system (Chiang,2007).
Chiang (2007) describes a rule extrac-tion algorithm that involves two steps: (1) extractphrases from 1-best alignments; (2) obtain vari-able rules by replacing sub-phrase pairs with non-terminals.
Our extraction algorithm differs at thefirst step, in which we extract phrases from hyper-graphs instead of 1-best alignments.
Rather thanrestricting ourselves by the alignment consistencyin the traditional algorithm, we extract all possiblecandidate target phrases for each source phrase.To maintain a reasonable rule table size, we fil-ter out less promising candidates that have a frac-tional count lower than a threshold.3.2 Calculating Fractional CountsThe fractional count of a phrase pair is the proba-bility sum of the alignments with which the phrasepair is consistent (?3.2.2), divided by the probabil-ity sum of all alignments encoded in a hypergraph(?3.2.1) (Liu et al, 2009).359Intuitively, our approach faces two challenges:1.
How to calculate the probability sum of allalignments encoded in a hypergraph (?3.2.1)?2.
How to efficiently calculate the probabilitysum of all consistent alignments for eachphrase pair (?3.2.2)?3.2.1 Enumerating All AlignmentsIn theory, a hypergraph can encode all possiblealignments if there are enough hyperedges.
How-ever, since a hypergraph is constructed from an n-best list, it can only represent partial space of allalignments (p(A|H) < 1) because of the limitingsize of hyperedges learned from the list.
There-fore, we need to enumerate all possible align-ments in a hypergraph to obtain the probabilitysum p(A|H).Specifically, generating an alignment from a hy-pergraph can be modelled as finding a completehyperedge matching, which is a set of hyperedgeswithout common vertices that matches all vertices.The probability of the alignment is the product ofhyperedge weights.
Thus, enumerating all possi-ble alignments in a hypergraph is reformulated asfinding all complete hypergraph matchings, whichis an NP-complete problem (Valiant, 1979).Similar to the bigraph, a hypergraph is also usu-ally not connected.
To make the enumeration prac-tically tractable, we propose a divide-and-conquerstrategy by decomposing a hypergraph H into a setof independent subhypergraphs {h1, h2, .
.
.
, hn}.Intuitively, the probability of an alignment is theproduct of hyperedge weights.
According to thedivide-and-conquer strategy, the probability sumof all alignments A encoded in a hypergraph H is:p(A|H) =?hi?Hp(Ai|hi)Here p(Ai|hi) is the probability sum of all sub-alignments Ai encoded in the subhypergraph hi.3.2.2 Enumerating Consistent AlignmentsSince a hypergraph encodes many alignments, it isunrealistic to enumerate all consistent alignmentsexplicitly for each phrase pair.Recall that a hypergraph can be decomposedto a list of independent subhypergraphs, and analignment is a combination of the sub-alignmentsfrom the decompositions.
We observe that aphrase pair is absolutely consistent with the sub-alignments from some subhypergraphs, while pos-sibly consistent with the others.
As an example,Ethebookisonthedesk????
?e1De2 e3e4e5thebookisonthedesk????
?e1e2 e3e4e5h1h3h2Figure 3: A hypergraph with a candidate phrasein the grey shadow (a), and its independent subhy-pergraphs {h1, h2, h3}.consider the phrase pair in the grey shadow in Fig-ure 3(a), it is consistent with all sub-alignmentsfrom both h1 and h2 because they are outside andinside the phrase pair respectively, while not con-sistent with the sub-alignment that contains hyper-edge e2 from h3 because it contains an alignmentlink that crosses the phrase pair.Therefore, to calculate the probability sum of allconsistent alignments, we only need to considerthe overlap subhypergraphs, which have at leastone hyperedge that crosses the phrase pair.
Givena overlap subhypergraph, the probability sum ofconsistent sub-alignments is calculated by sub-tracting the probability sum of the sub-alignmentsthat contain crossed hyperedges, from the proba-bility sum of all sub-alignments encoded in a hy-pergraph.Given a phrase pair P , let OS and NS de-notes the sets of overlap and non-overlap subhy-pergraphs respectively (NS = H ?OS).
Thenp(A|H,P ) =?hi?OSp(Ai|hi, P )?hj?NSp(Aj|hj)Here the phrase pair is absolutely consistent withthe sub-alignments from non-overlap subhyper-graphs (NS), and we have p(A|h, P ) = p(A|h).Then the fractional count of a phrase pair is:c(P |H) = p(A|H,P )p(A|H) =?hi?OS p(A|hi, P )?hi?OS p(A|hi)After we get the fractional counts of transla-tion rules, we can estimate their relative frequen-cies (Och and Ney, 2004).
We follow (Liu et al,2009; Tu et al, 2011) to learn lexical tables fromn-best lists and then calculate the lexical weights.360Rules from.
.
.
Rules MT03 MT04 MT05 Avg.1-best 257M 33.45 35.25 33.63 34.1110-best 427M 34.10 35.71 34.04 34.62Hypergraph 426M 34.71 36.24 34.41 35.12Table 1: Evaluation of translation quality.4 Experiments4.1 SetupWe carry out our experiments on Chinese-Englishtranslation tasks using a reimplementation of thehierarchical phrase-based system (Chiang, 2007).Our training data contains 1.5 million sentencepairs from LDC dataset.1 We train a 4-gramlanguage model on the Xinhua portion of theGIGAWORD corpus using the SRI LanguageToolkit (Stolcke, 2002) with modified Kneser-NeySmoothing (Kneser and Ney, 1995).
We use min-imum error rate training (Och, 2003) to optimizethe feature weights on the MT02 testset, and teston the MT03/04/05 testsets.
For evaluation, case-insensitive NIST BLEU (Papineni et al, 2002) isused to measure translation performance.We first follow Venugopal et al (2008) to pro-duce n-best lists via GIZA++.
We produce 10-bestlists in two translation directions, and use ?grow-diag-final-and?
strategy (Koehn et al, 2003) togenerate the final n-best lists by selecting thetop n alignments.
We re-estimated the probabil-ity of each alignment in the n-best list using re-normalization (Venugopal et al, 2008).
Finally weconstruct weighted alignment hypergraphs fromthese n-best lists.2 When extracting rules from hy-pergraphs, we set the pruning threshold t = 0.5.4.2 Tractability of Divide-and-ConquerStrategyFigure 4 shows the distribution of vertices (hy-peredges) number of the subhypergraphs.
We cansee that most of the subhypergraphs have just lessthan two vertices and hyperedges.3 Specifically,each subhypergraph has 2.0 vertices and 1.4 hy-1The corpus includes LDC2002E18, LDC2003E07,LDC2003E14, Hansards portion of LDC2004T07,LDC2004T08 and LDC2005T06.2Here we only use 10-best lists, because the alignmentsbeyond top 10 have very small probabilities, thus have negli-gible influence on the hypergraphs.3It?s interesting that there are few subhypergraphs thathave exactly 2 hyperedges.
In this case, the only two hy-peredges fully cover the vertices and they differ at the word-by-word links, which is uncommon in n-best lists.00.20.40.60.811 2 3 4 5 6 7 8 9 10percentagenumber of vertices (hyperedges)verticeshyperedgesFigure 4: The distribution of vertices (hyperedges)number of the subhypergraphs.peredges on average.
This suggests that the divide-and-conquer strategy makes the extraction compu-tationally tractable, because it greatly reduces thenumber of vertices and hyperedges.
For computa-tional tractability, we only allow a subhypergraphhas at most 5 hyperedges.
44.3 Translation PerformanceTable 1 shows the rule table size and transla-tion quality.
Using n-best lists slightly improvesthe BLEU score over 1-best alignments, but atthe cost of a larger rule table.
This is in ac-cord with intuition, because all possible transla-tion rules would be extracted from different align-ments in n-best lists without pruning.
This largerrule table indeed leads to a high rule coverage, butin the meanwhile, introduces translation errors be-cause of the low-quality rules (i.e., rules extractedonly from low-quality alignments in n-best lists).By contrast, our approach not only significantlyimproves the translation performance over 1-bestalignments, but also outperforms n-best lists witha similar-scale rule table.
The absolute improve-ments of 1.0 BLEU points on average over 1-bestalignments are statistically significant at p < 0.01using sign-test (Collins et al, 2005).4If a subhypergraph has more than 5 hyperedges, weforcibly partition it into small subhypergraphs by iterativelyremoving lowest-probability hyperedges.361Rules from.
.
.
Shared Non-shared AllRules BLEU Rules BLEU Rules BLEU10-best 1.83M 32.75 2.81M 30.71 4.64M 34.62Hypergraph 1.83M 33.24 2.89M 31.12 4.72M 35.12Table 2: Comparison of rule tables learned from n-best lists and hypergraphs.
?All?
denotes the full ruletable, ?Shared?
denotes the intersection of two tables, and ?Non-shared?
denotes the complement.
Notethat the probabilities of ?Shared?
rules are different for the two approaches.Why our approach outperforms n-best lists?
Intheory, the rule table extracted from n-best listsis a subset of that from hypergraphs.
In prac-tice, however, this is not true because we prunedthe rules that have fractional counts lower than athreshold.
Therefore, the question arises as to howmany rules are shared by n-best and hypergraph-based extractions.
We try to answer this ques-tion by comparing the different rule tables (filteredon the test sets) learned from n-best lists and hy-pergraphs.
Table 2 gives some statistics.
?All?denotes the full rule table, ?Shared?
denotes theintersection of two tables, and ?Non-shared?
de-notes the complement.
Note that the probabil-ities of ?Shared?
rules are different for the twoapproaches.
We can see that both the ?Shared?and ?Non-shared?
rules learned from hypergraphsoutperform n-best lists, indicating: (1) our ap-proach has a better estimation of rule probabili-ties because we estimate the probabilities from amuch larger alignment space that can not be rep-resented by n-best lists, (2) our approach can ex-tract good rules that cannot be extracted from anysingle alignments in the n-best lists.5 Related WorkOur research builds on previous work in the fieldof graph models and compact representations.Graph models have been used before in wordalignment: the search space of word alignment canbe structured as a graph and the search problemcan be reformulated as finding the optimal paththough this graph (e.g., (Och and Ney, 2004; Liu etal., 2010)).
In addition, Kumar and Byrne (2002)define a graph distance as a loss function forminimum Bayes-risk word alignment, Riesa andMarcu (2010) open up the word alignment task toadvances in hypergraph algorithms currently usedin parsing.
As opposed to the search problem, wepropose a graph-based compact representation thatencodes multiple alignments for machine transla-tion.Previous research has demonstrated that com-pact representations can produce improved re-sults by offering more alternatives, e.g., usingforests over 1-best trees (Mi and Huang, 2008;Tu et al, 2010; Tu et al, 2012a), word latticesover 1-best segmentations (Dyer et al, 2008),and weighted alignment matrices over 1-best wordalignments (Liu et al, 2009; Tu et al, 2011; Tu etal., 2012b).
Liu et al, (2009) estimate the linkprobabilities from n-best lists, while Gispert etal., (2010) learn the alignment posterior probabil-ities directly from IBM models.
However, both ofthem ignore the relations among alignment links.By contrast, our approach takes into account thejoint distribution of alignment links and exploresthe fertility model past the link level.6 ConclusionWe have presented a novel compact representa-tion of word alignment, named weighted bipar-tite hypergraph, to exploit the relations amongalignment links.
Since estimating the probabil-ities of rules extracted from hypergraphs is anNP-complete problem, we propose a computation-ally tractable divide-and-conquer strategy by de-composing a hypergraph into a set of independentsubhypergraphs.
Experimental results show thatour approach outperforms both 1-best and n-bestalignments.AcknowledgementThe authors are supported by 863 State KeyProject No.
2011AA01A207, National Key Tech-nology R&D Program No.
2012BAH39B03 andNational Natural Science Foundation of China(Contracts 61202216).
Qun Liu?s work is partiallysupported by Science Foundation Ireland (GrantNo.07/CE/I1142) as part of the CNGL at DublinCity University.
We thank Junhui Li, Yifan Heand the anonymous reviewers for their insightfulcomments.362ReferencesPeter E. Brown, Stephen A. Della Pietra, VincentJ.
Della Pietra, and Robert L. Mercer.
1993.The mathematics of statistical machine translation:Parameter estimation.
Computational linguistics,19(2):263?311.David Chiang.
2007.
Hierarchical phrase-based trans-lation.
Computational Linguistics, 33(2):201?228.M.
Collins, P. Koehn, and I. Kuc?erova?.
2005.
Clauserestructuring for statistical machine translation.
InProceedings of the 43rd Annual Meeting on Associa-tion for Computational Linguistics, pages 531?540.Adria` de Gispert, Juan Pino, and William Byrne.
2010.Hierarchical phrase-based translation grammars ex-tracted from alignment posterior probabilities.
InProceedings of the 2010 Conference on EmpiricalMethods in Natural Language Processing, pages545?554.Christopher Dyer, Smaranda Muresan, and PhilipResnik.
2008.
Generalizing word lattice translation.In Proceedings of ACL-08: HLT, pages 1012?1020.Reinhard Kneser and Hermann Ney.
1995.
Improvedbacking-off for m-gram language modeling.
In Pro-ceedings of the International Conference on Acous-tics, Speech, and Signal Processing, volume 1, pages181?184.Philipp Koehn, Franz Joseph Och, and Daniel Marcu.2003.
Statistical phrase-based translation.
In Pro-ceedings of the 2003 Conference of the North Amer-ican Chapter of the Association for ComputationalLinguistics, pages 48?54.Shankar Kumar and William Byrne.
2002.
Mini-mum Bayes-risk word alignments of bilingual texts.In Proceedings of the 2002 Conference on Empiri-cal Methods in Natural Language Processing, pages140?147.Yang Liu, Tian Xia, Xinyan Xiao, and Qun Liu.
2009.Weighted alignment matrices for statistical machinetranslation.
In Proceedings of the 2009 Conferenceon Empirical Methods in Natural Language Pro-cessing, pages 1017?1026.Yang Liu, Qun Liu, and Shouxun Lin.
2010.
Discrim-inative word alignment by linear modeling.
Compu-tational Linguistics, 36(3):303?339.Haitao Mi and Liang Huang.
2008.
Forest-based trans-lation rule extraction.
In Proceedings of the 2008Conference on Empirical Methods in Natural Lan-guage Processing, pages 206?214.Robert C. Moore.
2005.
A discriminative frameworkfor bilingual word alignment.
In Proceedings of theConference on Empirical Methods in Natural Lan-guage Processing, pages 81?88, October.Franz J. Och and Hermann Ney.
2004.
The alignmenttemplate approach to statistical machine translation.Computational Linguistics, 30(4):417?449.Franz Josef Och.
2003.
Minimum error rate trainingin statistical machine translation.
In Proceedings ofthe 41st Annual Meeting of the Association for Com-putational Linguistics, pages 160?167.Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu.
2002.
Bleu: a method for automaticevaluation of machine translation.
In Proceedingsof 40th Annual Meeting of the Association for Com-putational Linguistics, pages 311?318.Jason Riesa and Daniel Marcu.
2010.
Hierarchicalsearch for word alignment.
In Proceedings of the48th Annual Meeting of the Association for Compu-tational Linguistics, pages 157?166.Andreas Stolcke.
2002.
Srilm - an extensible languagemodeling toolkit.
In Proceedings of Seventh Inter-national Conference on Spoken Language Process-ing, volume 3, pages 901?904.
Citeseer.Zhaopeng Tu, Yang Liu, Young-Sook Hwang, QunLiu, and Shouxun Lin.
2010.
Dependency forestfor statistical machine translation.
In Proceedingsof the 23rd International Conference on Computa-tional Linguistics, pages 1092?1100.Zhaopeng Tu, Yang Liu, Qun Liu, and Shouxun Lin.2011.
Extracting hierarchical rules from a weightedalignment matrix.
In Proceedings of 5th Interna-tional Joint Conference on Natural Language Pro-cessing, pages 1294?1303.Zhaopeng Tu, Wenbin Jiang, Qun Liu, and ShouxunLin.
2012a.
Dependency forest for sentiment anal-ysis.
In Springer-Verlag Berlin Heidelberg, pages69?77.Zhaopeng Tu, Yang Liu, Yifan He, Josef van Genabith,Qun Liu, and Shouxun Lin.
2012b.
Combining mul-tiple alignments to improve machine translation.
InProceedings of the 24th International Conference onComputational Linguistics, pages 1249?1260.Leslie G Valiant.
1979.
The complexity of comput-ing the permanent.
Theoretical Computer Science,8(2):189?201.Ashish Venugopal, Andreas Zollmann, Noah A. Smith,and Stephan Vogel.
2008.
Wider pipelines: n-bestalignments and parses in mt training.
In Proceed-ings of AMTA, pages 192?201.363
