Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 165?171,October 25-29, 2014, Doha, Qatar.c?2014 Association for Computational LinguisticsSyntax-Augmented Machine Translation using Syntax-Label ClusteringHideya Mino, Taro Watanabe and Eiichiro SumitaNational Institute of Information and Communications Technology3-5 Hikaridai, Seika-cho, Soraku-gun, Kyoto, JAPAN{hideya.mino, taro.watanabe, eiichiro.sumita}@nict.go.jpAbstractRecently, syntactic information has helpedsignificantly to improve statistical ma-chine translation.
However, the use of syn-tactic information may have a negative im-pact on the speed of translation because ofthe large number of rules, especially whensyntax labels are projected from a parser insyntax-augmented machine translation.
Inthis paper, we propose a syntax-label clus-tering method that uses an exchange algo-rithm in which syntax labels are clusteredtogether to reduce the number of rules.The proposed method achieves clusteringby directly maximizing the likelihood ofsynchronous rules, whereas previous workconsidered only the similarity of proba-bilistic distributions of labels.
We testedthe proposed method on Japanese-Englishand Chinese-English translation tasks andfound order-of-magnitude higher cluster-ing speeds for reducing labels and gainsin translation quality compared with pre-vious clustering method.1 IntroductionIn recent years, statistical machine translation(SMT) models that use syntactic information havereceived significant research attention.
Thesemodels use syntactic information on the sourceside (Liu et al., 2006; Mylonakis and Sima?an,2011), the target side (Galley et al., 2006; Huangand Knight, 2006) or both sides (Chiang, 2010;Hanneman and Lavie, 2013) produce syntacticallycorrect translations.
Zollmann and Venugopal(2006) proposed syntax-augmented MT (SAMT),which is a MT system that uses syntax labels of aparser.
The SAMT grammar directly encodes syn-tactic information into the synchronous context-free grammar (SCFG) of Hiero (Chiang, 2007),which relies on two nonterminal labels.
One prob-lem in adding syntax labels to Hiero-style rulesis that only partial phrases are assigned labels.It is common practice to extend labels by us-ing the idea of combinatory categorial grammar(CCG) (Steedman, 2000) on the problem.
Al-though this extended syntactical information mayimprove the coverage of rules and syntactic cor-rectness in translation, the increased grammar sizecauses serious speed and data-sparseness prob-lems.
To address these problems, Hanneman andLavie (2013) coarsen syntactic labels using thesimilarity of the probabilistic distributions of la-bels in synchronous rules and showed that perfor-mance improved.In the present work, we follow the idea of label-set coarsening and propose a new method to groupsyntax labels.
First, as an optimization criterion,we use the logarithm of the likelihood of syn-chronous rules instead of the similarity of prob-abilistic distributions of syntax labels.
Second,we use exchange clustering (Uszkoreit and Brants,2008), which is faster than the agglomerative-clustering algorithm used in the previous work.We tested our proposed method on Japanese-English and Chinese-English translation tasks andobserved gains comparable to those of previouswork with similar reductions in grammar size.2 Syntax-Augmented MachineTranslationSAMT is an instance of SCFG G, which can beformally defined asG = (N , S, T?, T?,R)where N is a set of nonterminals, S ?
N is astart label, T?and T?are the source- and target-side terminals, andR is a set of synchronous rules.Each synchronous rule in R takes the formX ?
?
?, ?,?
?165where X ?
N is a nonterminal, ?
?
(N ?
T?
)?is a sequence of nonterminals or source-side ter-minals, and ?
?
(N ?
T?)?
is a sequence ofnonterminals or target-side terminals.
The num-ber #NT (?)
of nonterminals in ?
is equal tothe number #NT (?)
of nonterminals in ?, and?
: {1, ...,#NT (?)}
?
{1, ...,#NT (?)}
is aone-to-one mapping from nonterminals in ?
tononterminals in ?.
For each synchronous rule, anonnegative real-value weight w(X ?
?
?, ?,??
)is assigned and the sum of the weights of all rulessharing the same left-hand side in a grammar isunity.Hierarchical phrase-based SMT (Hiero) (Chi-ang, 2007) translates by using synchronous rulesthat only have two nonterminal labelsX and S buthave no linguistic information.
SAMT augmentsthe Hiero-style rules with syntax labels from aparser and extends these labels based on CCG.Although the use of extended syntax labels mayincrease the coverage of rules and improve thepotential for syntactically correct translations, thegrowth of the nonterminal symbols significantlyaffects the speed of decoding and causes a seriousdata-sparseness problem.To address these problems, Hanneman andLavie (2013) proposed a label-collapsing algo-rithm, in which syntax labels are clustered by us-ing the similarity of the probabilistic distributionsof clustered labels in synchronous rules.
First,Hanneman and Lavie defined the label-alignmentdistribution asP (s|t) =#(s, t)#(t)(1)where N?and N?are the source- and target-sidenonterminals in synchronous rules, s ?
N?andt ?
N?are syntax labels from the source and tar-get sides, #(s, t) denotes the number of left-hand-side label pairs, and #(t) denotes the number oftarget-side labels.
Second, for each target-side la-bel pair (ti, tj), we calculate the total distance d ofthe absolute differences in the likelihood of labelsthat are aligned to a source-side label s:d(ti, tj) =?s?N?|P (s|ti)?
P (s|tj)| (2)Next, the closest syntax-label pair of?t and?t?iscombined into a new single label.
The agglomera-tive clustering is applied iteratively until the num-ber of the syntax labels reaches a given value.The clustering of Hanneman and Lavie provedsuccessful in decreasing the grammar size and pro-viding a statistically significant improvement intranslation quality.
However, their method relieson an agglomerative clustering with a worst-casetime complexity of O(|N |2log |N |).
Also, clus-tering based on label distributions does not al-ways imply higher-quality rules, because it doesnot consider the interactions of the nonterminalson the left-hand side and the right-hand side ineach synchronous rule.3 Syntax-Label ClusteringAs an alternative to using the similarity of proba-bilistic distributions as a criterion for syntax-labelclustering, we propose a clustering method basedon the maximum likelihood of the synchronousrules in a training data D. We uses the ideaof maximizing the Bayesian posterior probabilityP (M |D) of the overall model structure M givendata D (Stolcke and Omohundro, 1994).
Whiletheir goal is to maximize the posteriorP (M |D) ?
P (M)P (D|M) (3)we omit the prior term P (M) and directly max-imize the P (D|M).
A model M is a clusteringstructure1.
The synchronous rule in the data Dfor SAMT with target-side syntax labels is repre-sented asX ?
?a1Y(1)a2Z(2)a3, b1Y(1)b2Z(2)b3?
(4)where a1, a2, a3and b1, b2, b3are the source- andtarget-side terminals, respectively X , Y , Z arenonterminal syntax labels, and the superscriptnumber indicates alignment between the source-and target-side nonterminals.
Using Equation (4)we maximize the posterior probability P (D|M)which we define as the probability of right-handside given the syntax label X of the left-hand siderule in the training data as follows:?X???,?,???DlogPr(?
?, ?,?
?|X) (5)For the sake of simplicity, we assume that thegenerative probability for each rule does not de-pend on the existence of terminal symbols and thatthe reordering in the target side may be ignored.Therefore, Equation (5) simplifies to?X?
?a1Y(1)a2Z(2)a3,b1Y(1)b2Z(2)b3?log p(Y, Z|X) (6)1P (M) is reflected by the number of clusters.1663.1 Optimization CriterionThe generative probability in each rule of the formof Equation (6) can be approximated by clusteringnonterminal symbols as follows:p(Y, Z|X) ?
p(Y |c(Y )) ?
p(Z|c(Z))?p(c(Y ), c(Z)|c(X)) (7)where we map a syntax label X to its equivalencecluster c(X).
This can be regarded as the cluster-ing criterion usually used in a class-based n-gramlanguage model (Brown et al., 1992).
If each labelon the right-hand side of a synchronous rule (4) isindependent of each other, we can factor the jointmodel as follows:p(Y, Z|X) ?
p(Y |c(Y )) ?
p(Z|c(Z))?p(c(Y )|c(X))?p(c(Z)|c(X)) (8)We introduce the predictive idea of Uszkoreit andBrants (2008) to Equation (8), which doesn?t con-dition on the clustered label c(X), but directly onthe syntax label X:p(Y, Z|X) ?
p(Y |c(Y )) ?
p(Z|c(Z))?p(c(Y )|X) ?
p(c(Z)|X) (9)The objective in Equation (9) is represented usingthe frequency in the training data asN(Y )N(c(Y ))?N(X, c(Y ))N(X)?N(Z)N(c(Z))?N(X, c(Z))N(X)(10)where N(X) and N(c(X)) denote the frequency2of X and c(X), and N(X,K) denotes the fre-quency of cluster K in the right-hand side of asynchronous rule whose left-hand side syntax la-bel is X .
By replacing the rule probabilities inEquation (9) with Equation (10) and plugging theresult into Equation (6), our objective becomesF (C) =?Y ?NN(Y ) ?
logN(Y )N(c(Y ))+?X?N ,K?CN(X,K) ?
logN(X,K)N(X)=?Y ?NN(Y ) ?
logN(Y )?
?Y ?NN(Y ) ?
logN(c(Y ))+?X?N ,K?CN(X,K) ?
logN(X,K)?
?X?N ,K?CN(X,K) ?
logN(X)(11)2We use a fractional count (Chiang, 2007) which adds upto one as a frequency.start with the initial mapping (labelX ?
c(X))compute objective function F (C)for each labelX doremove labelX from c(X)for each clusterK domove labelX tentatively to clusterKcompute F (C) for this exchangemove labelX to cluster with maximum F (C)do until the cluster mapping does not changeTable 1: Outline of syntax-label clustering methodwhere C denotes all clusters and N denotes allsyntax labels.
For Equation (11), the last summa-tion is equivalent to the sum of the occurrencesof all syntax labels, and canceled out by the firstsummation.
K in the third summation consid-ers clusters in a synchronous rule whose left-handside label is X , and we let ch(X) denote a setof those clusters.
The second summation equals?K?CN(K) ?
logN(K).
As a result, Equation(11) simplifies toF (C) =?X?N ,K?ch(X)N(X,K) ?
logN(X,K)?
?K?CN(K) ?
logN(K) (12)3.2 Exchange ClusteringWe used an exchange clustering algorithm(Uszkoreit and Brants, 2008) which was provento be very efficient in word clustering with a vo-cabulary of over 1 million words.
The exchangeclustering for words begins with the initial cluster-ing of words and greedily exchanges words fromone cluster to another such that an optimizationcriterion is maximized after the move.
While ag-glomerative clustering requires recalculation forall pair-wise distances between words, exchangeclustering only demands computing the differenceof the objective for the word pair involved in a par-ticular movement.
We applied this exchange clus-tering to syntax-label clustering.
Table 1 showsthe outline.
For initial clustering, we partitionedall the syntax labels into clusters according to thefrequency of syntax labels in synchronous rules.
Ifremove and move are as computationally inten-sive as computing the change in F (C) in Equation(12), then the time complexity of remove andmove is O(K) (Martin et al., 1998), where K isthe number of clusters.
Since the remove proce-dure is called once for each label and, for a givenlabel, the move procedure is called K ?
1 times167Data Lang Training Development Testsent src-tokens tgt-tokens sent tgt-tokens sent tgt-tokensIWSLT07 J to E 40 K 483 K 369 K 500 7.4 K 489 3.7 KFBIS C to E 302 K 2.7 M 3.4 M 1,664 47 K 919 30 KNIST08 1 M 15 M 17 MTable 2: Data sets: The ?sent?
column indicates the number of sentences.
The ?src-tokens?
and ?tgt-tokens?
columns indicate the number of words in the source- and the target-side sentences.to find the maximum F (C), the worst-time com-plexity for one iteration of the syntax-label clus-tering is O(|N |K2).
The exchange procedure iscontinued until the cluster mapping is stable or thenumber of iterations reaches a threshold value of100.4 Experiments4.1 DataWe conducted experiments on Japanese-English(ja-en) and Chinese-English (zh-en) translationtasks.
The ja-en data comes from IWSLT07(Fordyce, 2007) in a spoken travel domain.
Thetuning set has seven English references and the testset has six English references.
For zh-en data weprepared two kind of data.
The one is extractedfrom FBIS3, which is a collection of news arti-cles.
The other is 1 M sentences extracted ron-domly from NIST Open MT 2008 task (NIST08).We use the NIST Open MT 2006 for tuning andthe MT 2003 for testing.
The tuning and test setshave four English references.
Table 2 shows thedetails for each corpus.
Each corpus is tokenized,put in lower-case, and sentences with over 40 to-kens on either side are removed from the trainingdata.
We use KyTea (Neubig et al., 2011) to to-kenize the Japanese data and Stanford Word Seg-menter (Tseng et al., 2005) to tokenize the Chinesedata.
We parse the English data with the Berkeleyparser (Petrov and Klein, 2007).4.2 Experiment designWe did experiments with the SAMT (Zollmannand Venugopal, 2006) model with the Moses(Koehn et al., 2007).
For the SAMT model, weconducted experiments with two label sets.
Oneis extracted from the phrase structure parses andthe other is extended with CCG4.
We applied theproposed method (+clustering) and the baselinemethod (+coarsening), which uses the Hanneman3LDC2003E144Using the relax-parse with option SAMT 4 for IWSLT07and FBIS and SAMT 2 for NIST08 in the MosesLabel set Label Rule F(C) SDparse 63 0.3 K - -CCG 3,147 4.2 M - -+ coarsening 80 2.4 M -3.8 e+08 249+ clustering 80 3.8 M -7.2 e+07 73Table 3: SAMT grammars on ja-en experimentsLabel set Label Rule F(C) SDFBISparse 70 2.1 M - -CCG 5,460 60 M - -+ coarsening 80 32 M -1.5 e+10 526+ clustering 80 38 M -7.9 e+09 154NIST08parse 70 12 M - -CCG 7,328 120 M - -+ clustering 80 100 M -2.6 e+10 218Table 4: SAMT grammars on zh-en experimentslabel-collapsing algorithm described in Section 2,for syntax-label clustering to the SAMT modelswith CCG.
The number of clusters for each clus-tering was set to 80.
The language models werebuilt using SRILM Toolkits (Stolcke, 2002).
Thelanguage model with the IWSLT07 is a 5-grammodel trained on the training data, and the lan-guage model with the FBIS and NIST08 is a 5-gram model trained on the Xinhua portion of En-glish GigaWord.
For word alignments, we usedMGIZA++ (Gao and Vogel, 2008).
To tune theweights for BLEU (Papineni et al., 2002), we usedthe n-best batch MIRA (Cherry and Foster, 2012).5 Results and analysisTables 3 and 4 present the details of SAMT gram-mars with each label set learned by the exper-iments using the IWSLT07 (ja-en), FBIS andNIST08 (zh-en), which include the number of syn-tax labels and synchronous rules, the values of theobjective (F (C)), and the standard deviation (SD)of the number of labels assigned to each cluster.For NIST08 we applied only the + clustering be-cause the + coarsening needs a huge amount ofcomputation time.
Table 5 shows the differencesbetween the BLEU score and the rule number for168each cluster number when using the IWSLT07dataset.Since the +clustering maximizes the likelihoodof synchronous rules, it can introduce appropriaterules adapted to training data given a fixed numberof clusters.
For each experiment, SAMT gram-mars with the +clustering have a greater numberof rules than with the +coarsening and, as shownin Table 5, the number of synchronous rules with+clustering increase with the number of clusters.For +clustering with eight clusters and +coars-ening with 80 clusters, which have almost 2.4Mrules, the BLEU score of +clustering with eightclusters is higher.
Also, the SD of the numberof labels, which indicates the balance of the num-ber of labels among clusters, with +clustering issmaller than with +coarsening.
These results sug-gest that +clustering maintain a large-scale varia-tion of synchronous rules for high performance bybalancing the number of labels in each cluster.The number of synchronous rules grows as youprogress from +coarsening to +clustering and fi-nally to raw label with CCG.
To confirm the ef-fect of the number of rules, we measured the de-coding time per sentence for translating the testset by taking the average of ten runs with FBIScorpus.
+coarsening takes 0.14 s and +clusteringtakes 0.16 s while raw label with CCG takes 0.37s.Thus the increase in the number of synchronousrules adversely affects the decoding speed.Table 6 presents the results for the experiments5using ja-en and zh-en with the BLEU metric.SAMT with parse have the lowest BLEU scores.It appears that the linguistic information of theraw syntax labels of the phrase structure parsesis not enough to improve the translation perfor-mance.
Hiero has the higher BLEU score thanSAMT with CCG on zh-en.
This is likely due tothe low accuracy of the parses, on which SAMTrelies while Hiero doesn?t.
SAMT with + clus-tering have the higher BLEU score than raw labelwith CCG.
For SAMT with CCG using IWSLT07and FBIS, though the statistical significance testswere not significant when p < 0.05, +clusteringhave the higher BLEU scores than +coarsening.For these results, the performance of +clusteringis comparable to that of +coarsening.
For thecomplexity of both clustering algorithm, though itis difficult to evaluate directly because the speed5As another baseline, we also used Phrase-based SMT(Koehn et al., 2003) and Hiero (Chiang, 2007).+clustering +coarseningCluster 80 40 8 4 80BLEU 50.21 49.49 49.96 50.25 49.54Rule 3.8 M 3.5 M 2.4 M 2.2 M 2.4 MTable 5: BLEU score and rule number for eachcluster number using IWSLT07ja-en zh-enModel parse CCG parse CCG parse CCGSAMT 42.58 48.77 23.66 26.97 24.67 27.28+coarsening - 49.54 - 27.12 - -+clustering - 50.21 - 27.47 - 27.29Hiero 48.91 28.31 27.62PB-SMT 49.14 26.88 26.71Table 6: BLEU scores on each experimentsdepends on how each algorithm is implemented,+clustering is an order of magnitude faster than+coarsening.
For the clustering experiment thatgroups 5460 raw labels with CCG into 80 clus-ters using FBIS corpus, +coarsening takes about1 week whereas +clustering takes about 10 min-utes.6 ConclusionIn this paper, we propose syntax-label clusteringfor SAMT, which uses syntax-label information togenerate syntactically correct translations.
One ofthe problems of SAMT is the large grammar sizewhen a CCG-style extended label set is used in thegrammar, which make decoding slower.
We clus-ter syntax labels with a very fast exchange algo-rithm in which the generative probabilities of syn-chronous rules are maximized.
We demonstratethe effectiveness of the proposed method by us-ing it to translate Japanese-English and Chinese-English tasks and measuring the decoding speed,the accuracy and the clustering speed.
Future workinvolves improving the optimization criterion.
Weexpect to make a new objective that includes theterminal symbols and the reordering of nontermi-nal symbols that were ignored in this work.
An-other interesting direction is to determine the ap-propriate number of clusters for each corpus andthe initialization method for clustering.AcknowledgmentsWe thank the anonymous reviewers for their sug-gestions and helpful comments on the early ver-sion of this paper.169ReferencesPeter F. Brown, Vincent J. Della Pietra, Peter V. deS-ouza, Jenifer C. Lai, and Robert L. Mercer.
1992.Class-based n-gram models of natural language.Computational Linguistics, 18(4):467?479.Colin Cherry and George Foster.
2012.
Batch tuningstrategies for statistical machine translation.
In Pro-ceedings of the 2012 Conference of the North Amer-ican Chapter of the Association for ComputationalLinguistics: Human Language Technologies, pages427?436, Montr?eal, Canada, June.
Association forComputational Linguistics.David Chiang.
2007.
Hierarchical phrase-based trans-lation.
Computational Linguistics, pages 201?228,June.David Chiang.
2010.
Learning to translate with sourceand target syntax.
In Proceedings of the 48th AnnualMeeting of the Association for Computational Lin-guistics, pages 1443?1452, Uppsala, Sweden, July.Association for Computational Linguistics.Cameron Shaw Fordyce.
2007.
Overview of the 4thinternational workshop on spoken language transla-tion iwslt 2007 evaluation campaign.
In In Proceed-ings of IWSLT 2007, pages 1?12, Trento, Italy, Oc-tober.Michel Galley, Jonathan Graehl, Kevin Knight, DanielMarcu, Steve DeNeefe, Wei Wang, and IgnacioThayer.
2006.
Scalable inference and training ofcontext-rich syntactic translation models.
In Pro-ceedings of the 21st International Conference onComputational Linguistics and 44th Annual Meet-ing of the Association for Computational Linguis-tics, pages 961?968, Sydney, Australia, July.
Asso-ciation for Computational Linguistics.Qin Gao and Stephan Vogel.
2008.
Parallel implemen-tations of word alignment tool.
In Software Engi-neering, Testing, and Quality Assurance for Natu-ral Language Processing, pages 49?57, Columbus,Ohio, June.
Association for Computational Linguis-tics.Greg Hanneman and Alon Lavie.
2013.
Improvingsyntax-augmented machine translation by coarsen-ing the label set.
In Proceedings of the 2013 Con-ference of the North American Chapter of the Asso-ciation for Computational Linguistics: Human Lan-guage Technologies, pages 288?297, Atlanta, Geor-gia, June.
Association for Computational Linguis-tics.Bryant Huang and Kevin Knight.
2006.
Relabelingsyntax trees to improve syntax-based machine trans-lation quality.
In Proceedings of the Human Lan-guage Technology Conference of the NAACL, MainConference, pages 240?247, New York City, USA,June.
Association for Computational Linguistics.Phillip Koehn, Franz Josef Och, and Daniel Marcu.2003.
Statistical phrase-based translation.
In InProceedings of HLT-NAACL, pages 48?54, Edmon-ton, Canada, May/July.Philipp Koehn, Hieu Hoang, Alexandra Birch, ChrisCallison-Burch, Marcello Federico, Nicola Bertoldi,Brooke Cowan, Wade Shen, Christine Moran,Richard Zens, Chris Dyer, Ondrej Bojar, Alexan-dra Constantin, and Evan Herbst.
2007.
Moses:Open source toolkit for statistical machine transla-tion.
In Proceedings of the 45th Annual Meeting ofthe Association for Computational Linguistics Com-panion Volume Proceedings of the Demo and PosterSessions, pages 177?180, Prague, Czech Republic,June.
Association for Computational Linguistics.Yang Liu, Qun Liu, and Shouxun Lin.
2006.
Tree-to-string alignment template for statistical machinetranslation.
In Proceedings of the 21st Interna-tional Conference on Computational Linguistics and44th Annual Meeting of the Association for Compu-tational Linguistics, pages 609?616, Sydney, Aus-tralia, July.
Association for Computational Linguis-tics.Sven Martin, Jorg Liermann, and Hermann Ney.
1998.Algorithms for bigram and trigram word clustering.In Speech Communication, pages 19?37.Markos Mylonakis and Khalil Sima?an.
2011.
Learn-ing hierarchical translation structure with linguis-tic annotations.
In Proceedings of the 49th An-nual Meeting of the Association for ComputationalLinguistics: Human Language Technologies, pages642?652, Portland, Oregon, USA, June.
Associationfor Computational Linguistics.Graham Neubig, Yosuke Nakata, and Shinsuke Mori.2011.
Pointwise prediction for robust, adaptablejapanese morphological analysis.
In Proceedings ofthe 49th Annual Meeting of the Association for Com-putational Linguistics: Human Language Technolo-gies, pages 529?533, Portland, Oregon, USA, June.Association for Computational Linguistics.Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu.
2002.
Bleu: a method for automaticevaluation of machine translation.
In Proceedingsof 40th Annual Meeting of the Association for Com-putational Linguistics, pages 311?318, Philadelphia,Pennsylvania, USA, July.
Association for Computa-tional Linguistics.Slav Petrov and Dan Klein.
2007.
Improved infer-ence for unlexicalized parsing.
In Human LanguageTechnologies 2007: The Conference of the NorthAmerican Chapter of the Association for Computa-tional Linguistics; Proceedings of the Main Confer-ence, pages 404?411, Rochester, New York, April.Association for Computational Linguistics.Mark Steedman.
2000.
The syntactic process, vol-ume 27.
MIT Press.Andreas Stolcke and Stephen Omohundro.
1994.
In-ducing probabilistic grammars by bayesian model170merging.
In R. C. Carrasco and J. Oncina, editors,Grammatical Inference and Applications (ICGI-94),pages 106?118.
Berlin, Heidelberg.Andreas Stolcke.
2002.
Srilm an extensible languagemodeling toolkit.
In In Proceedings of the SeventhInternational Conference on Spoken Language Pro-cessing, pages 901?904.Huihsin Tseng, Pichuan Chang, Galen Andrew, DanielJurafsky, and Christopher Manning.
2005.
A con-ditional random field word segmenter.
In FourthSIGHAN Workshop on Chinese Language Process-ing, pages 168?171.
Jeju Island, Korea.Jakob Uszkoreit and Thorsten Brants.
2008.
Dis-tributed word clustering for large scale class-basedlanguage modeling in machine translation.
In Pro-ceedings of ACL-08: HLT, pages 755?762, Colum-bus, Ohio, June.
Association for Computational Lin-guistics.Andreas Zollmann and Ashish Venugopal.
2006.
Syn-tax augmented machine translation via chart parsing.In Proceedings on the Workshop on Statistical Ma-chine Translation, pages 138?141, New York City,June.
Association for Computational Linguistics.171
