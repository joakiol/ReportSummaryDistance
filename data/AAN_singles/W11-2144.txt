Proceedings of the 6th Workshop on Statistical Machine Translation, pages 372?378,Edinburgh, Scotland, UK, July 30?31, 2011. c?2011 Association for Computational LinguisticsThe Uppsala-FBK systems at WMT 2011Christian HardmeierJo?rg TiedemannUppsala universitetInst.
fo?r lingvistik och filologiUppsala, Swedenfirst.last@lingfil.uu.seMarkus SaersHuman LanguageTechnology CenterHong Kong Univ.
ofScience & Technologymasaers@cs.ust.hkMarcello FedericoMathur PrashantFondazione Bruno KesslerHuman Language TechnologiesTrento, Italylastname@fbk.euAbstractThis paper presents our submissions to theshared translation task at WMT 2011.
Wecreated two largely independent systemsfor English-to-French and Haitian Creole-to-English translation to evaluate different fea-tures and components from our ongoing re-search on these language pairs.
Key featuresof our systems include anaphora resolution,hierarchical lexical reordering, data selectionfor language modelling, linear transductiongrammars for word alignment and syntax-based decoding with monolingual dependencyinformation.1 English to FrenchOur submission to the English-French task was aphrase-based Statistical Machine Translation basedon the Moses decoder (Koehn et al, 2007).
Phrasetables were separately trained on Europarl, newscommentary and UN data and then linearly inter-polated with uniform weights.
For language mod-elling, we used 5-gram models trained with theIRSTLM toolkit (Federico et al, 2008) on the mono-lingual News corpus and parts of the English-French109 corpus.
More unusual features of our systemincluded a special component to handle pronomi-nal anaphora and the hierarchical lexical reorderingmodel by Galley and Manning (2008).
Selected fea-tures of our system will be discussed in depth in thefollowing sections.1.1 Handling pronominal anaphoraPronominal anaphora is the use of pronominal ex-pressions to refer to ?something previously men-tioned in the discourse?
(Strube, 2006).
It is a verycommon phenomenon found in almost all kinds oftexts.
Anaphora can be local to a sentence, or it cancross sentence boundaries.
Standard SMT methodsdo not handle this phenomenon in a satisfactory wayat present: For sentence-internal anaphora, they de-pend on the n-gram language model with its lim-ited history, while cross-sentence anaphora is leftto chance.
We therefore added a word-dependencymodel (Hardmeier and Federico, 2010) to our sys-tem to handle anaphora explicitly.Our processing of anaphoric pronouns followsthe procedure outlined by Hardmeier and Federico(2010).
We use the open-source coreference resolu-tion system BART (Broscheit et al, 2010) to linkpronouns to their antecedents in the text.
Coref-erence links are handled differently depending onwhether or not they cross sentence boundaries.
Ifa coreference link points to a previous sentence, weprocess the sentence containing the antecedent withthe SMT system and look up the translation of theantecedent in the translated output.
If the corefer-ence link is sentence-internal, the translation lookupis done dynamically by the decoder during search.In either case, the word-dependency model adds afeature function to the decoder score representingthe probability of a particular pronoun choice giventhe translation of the antecedent.In our English-French system, this model wasonly applied to the inanimate pronouns it and they,which seemed to be the most promising candidatesfor improvement since their French equivalents re-quire gender marking.
It was trained on data au-tomatically annotated for anaphora taken from thenews-commentary corpus, and the vocabulary of thepredicted pronouns was limited to words recognisedas pronouns by the POS tagger.3721.2 Hierarchical lexical reorderingThe basic word order model of SMT penalises anydivergence between the order of the words in the in-put sentence and the order of their translation equiv-alents in the MT output.
All reordering must thus bedriven by the language model when no other reorder-ing model is present.
Lexical reordering modelsmaking certain word order choices in the MT out-put conditional on the identity of the words involvedhave been a standard component in SMT for someyears.
The lexical reordering model usually em-ployed in the Moses decoder was implemented byKoehn et al (2005).
Adopting the perspective of theSMT decoder, which produces the target sentencefrom left to right while covering source phrases infree order, the model distinguishes between three or-dering classes, monotone, swap and discontinuous,depending on whether the source phrases giving riseto the two last target phrases emitted were adjacentin the same order, adjacent in swapped order or sep-arated by other source words.
Probabilities for eachordering class given source and target phrase areestimated from a word-aligned training corpus andintegrated into MT decoding as extra feature func-tions.In our submission, we used the hierarchical lexi-cal reordering model proposed by Galley and Man-ning (2008) and recently implemented in the Mosesdecoder.1 This model uses the same approach ofclassifying movements as monotone, swap or dis-continuous, but unlike the phrase-based model, itdoes not require the source language phrases to bestrictly adjacent in order to be counted as monotoneor swap.
Instead, a phrase can be recognised as ad-jacent to, or swapped with, a contiguous block ofsource words that has been segmented into multi-ple phrases.
Contiguous phrase blocks are recog-nised by the decoder with a shift-reduce parsing al-gorithm.
As a result, fewer jumps are labelled withthe uninformative discontinuous class.1.3 Data selection from the WMT Giga corpusOne of the supplied language resources for this eval-uation is the French-English WMT Giga corpus,1The hierarchical lexical reordering model was imple-mented in Moses during MT Marathon 2010 by Christian Hard-meier, Gabriele Musillo, Nadi Tomeh, Ankit Srivastava, SaraStymne and Marcello Federico.60 80 100 120 140 160 180 200 220 240 260 280100  150  200  250  300  350  400 60 80 100 120 140 160 180 200 220 240 260 280LM Perplexity LM size (million 5-grams)Data Selection ThresholdThreshold vs PerplexityThreshold vs LM SizeFigure 1: Perplexity and size of language models trainedon data of the WMT Giga corpus that were selected usingdifferent perplexity thresholds.aka 109 corpus, a large collection of parallel sen-tences crawled from Canadian and European Unionsources.
While this corpus was too large to be usedfor model training with the means at our disposal,we exploited it as a source of parallel data for trans-lation model training as well as monolingual Frenchdata for the language model by filtering it down to amanageable size.
In order to extract sentences closeto the news translation task, we applied a simpledata selection procedure based on perplexity.
Sen-tence pairs were selected from the WMT Giga cor-pus if the perplexity of their French part with respectto a language model (LM) trained on French newsdata was below a given threshold.
The rationale isthat text sentences which are better predictable bythe LM should be closer to the news domain.
Thethreshold was set in a way to capture enough noveln-grams, from one side, but also to avoid adding toomany irrelevant n-grams.
It was tuned by traininga 5-gram LM on the selected data and checking itssize and its perplexity on a development set.
In fig-ure 1 we plot perplexity and size of the WMT GigaLM for different values of the data-selection thresh-old.
Perplexities are computed on the newstest2009set.
As a good perplexity-size trade-off, the thresh-old 250 was chosen to estimate an additional 5-gramLM (WMT Giga 250) that was interpolated withthe original News LM.
The resulting improvementin perplexity is reported in table 1.
For translationmodel data, a perplexity threshold of 159 was ap-plied.373LM Perplexity OOV rateNews 146.84 0.82News + WMT Giga 250 130.23 0.71Table 1: Perplexity reduction after interpolating the NewsLM with data selected from the 109 corpus.newstest2009 2010 2011Primary submission 0.246 0.286 0.284w/o Anaphora handling 0.246 0.286 0.284WMT Giga dataw/o LM 0.244 0.289 0.280w/o TM 0.247 0.286 0.282w/o LM and TM 0.247 0.289 0.278Lexical reorderingphrase-based reo 0.239 0.281 0.275no lexical reo 0.239 0.281 0.275with LDC data 0.254 0.293 0.291Table 2: Ablation test results (case-sensitive BLEU)1.4 Results and Ablation testsOwing to time constraints, we were not able to runthorough tests on our system before submitting it tothe evaluation campaign.
We therefore evaluated thevarious components included in a post hoc fashionby running ablation tests.
In each test, we left outone of the system components to identify its effecton the overall performance.
The results of these testsare reported in table 2.Performance-wise, the most important particular-ity of our SMT system was the hierarchical lexicalreordering model, which led to a sizeable improve-ment of 0.7, 0.5 and 0.9 BLEU points for the 2009,2010 and 2011 test sets, respectively.
We had previ-ously seen negative results when trying to apply thesame model to English-German SMT, so its perfor-mance seems to be strongly dependent on the lan-guage pair it is used with.Compared to the scores obtained using the fullsystem, the anaphora handling system did not haveany effect on the BLEU scores.
This result issimilar to our result for English-German transla-tion (Hardmeier and Federico, 2010).
Unfortu-nately, for English-French, the negative results ex-tends to the pronoun translation scores (not reportedhere), where slightly higher recall with the word-dependency model was overcompensated by de-graded precision, so the outcome of the experimentsclearly suggests that the anaphora handling proce-dure is in need of improvement.The effect of the WMT Giga language model dif-fers among the test sets.
For the 2009 and 2011test sets, it results in an improvement of 0.2 and 0.4BLEU points, respectively, while the 2010 test setfares better without this additional language model.However, it should be noted that there may be aproblem with the 2010 test set and the News lan-guage model, which was used as a component in allour systems.
In particular, upgrading the News LMdata from last year?s to this year?s release led to animprovement of 4 BLEU points on the 2010 test setand an unrealistically low perplexity of 73 as com-pared to 130 for the 2009 test set, which makes ussuspect that the latest News LM data may be taintedwith data from the 2010 test corpus.
If this is thecase, the 2010 test set should be considered unreli-able for LM evaluation.
The benefit of adding WMTGiga data to the translation model is less clear.
Forthe 2009 and 2010 test sets, this leads to a slightdegradation, but for the 2011 corpus, we obtaineda small improvement.Our shared task submission did not use the FrenchGigaword corpus from the Linguistic Data Consor-tium (LDC2009T28), which is not freely availableto sites without LDC membership.
After the sub-mission, we ran a contrastive experiment includinga 5-gram model trained on this corpus, which ledto a sizeable improvement of 0.7?0.8 BLEU pointsacross all test sets.2 Haitian Creole to EnglishOur experiments with the Haitian Creole-Englishdata are independent of the system presented for theEnglish to French task above.
We experimented withboth phrase-based SMT and syntax-based SMT.
Themain questions we investigated were i) whether wecan improve word alignment and phrase extractionfor phrase-based SMT and ii) whether we can in-tegrate dependency parsing into a syntax-based ap-proach.
All our experiments were conducted on theclean data set using Moses for training and decod-ing.
In the following we will first describe the exper-iments with phrase-based models and linear trans-374duction grammars for word alignment and, there-after, our findings from integrating English depen-dency parses into a syntax-based approach.2.1 Phrase-based SMTThe phrase-based system that we used in this seriesof experiments uses a rather traditional setup.
Forthe translations into English we used the news dataprovided for the other translations tasks in WMT2011 to build a large scale-background languagemodel.
The English data from the Haitian Creoletask were used as a separate domain-specific lan-guage model.
For the other translation direction weonly used the in-domain data provided.
We usedstandard 5-gram models with Witten-Bell discount-ing and backoff interpolation for all language mod-els.
For the translation model we applied standardtechniques and settings for phrase extraction andscore estimations.
However, we applied two differ-ent systems for word alignment: One is the standardGIZA++ toolbox implementing the IBM alignmentmodels (Och and Ney, 2003) and extensions and theother is based on transduction grammars which willbriefly be introduced in the next section.2.1.1 Alignment with PLITGsBy making the assumption that the parallel cor-pus constitutes a linear transduction (Saers, 2011)2we can induce a grammar that is the most likely tohave generated the observed corpus.
The grammarinduced will generate a parse forest for each sen-tence pair in the corpus, and each parse tree in thatforest will correspond to an alignment between thetwo sentences.
Following Saers et al (2010), thealignment corresponding to the best parse can be ex-tracted and used instead of other word alignment ap-proaches such as GIZA++.
There are several gram-mar types that generate linear transductions, and inthis work, stochastic bracketing preterminalized lin-ear inversion transduction grammars (PLITG) wereused (Saers and Wu, 2011).
Since we were mainlyinterested in the word alignments, we did not inducephrasal grammars.Although alignments from PLITGs may not reachthe same level of translation quality as GIZA++,they make different mistakes, so both complement2A transduction is a set of pairs of strings, and thus repre-sents a relation between two languages.each other.
By duplicating the training corpus andaligning each copy of the corpus with a differentalignment tool, the phrase extractor seems to be ableto pick the best of both worlds, producing a phrasetable that is superior to one produced with either ofthe alignments tools used in isolation.2.1.2 ResultsIn the following we present our results on the pro-vided test set3 for translating into both languageswith phrase-based systems trained on different wordalignments.
Table 3 summarises the BLEU scoresobtained.English-Haitian BLEU phrase-tableGIZA++ 0.2567 3,060,486PLITG 0.2407 5,007,254GIZA++ & PLITG 0.2572 7,521,754Haitian-English BLEU phrase-tableGIZA++ 0.3045 3,060,486PLITG 0.2922 5,049,280GIZA++ & PLITG 0.3105 7,561,043Table 3: Phrase-based SMT (pbsmt) on the HaitianCreole-English test set with different word alignments.From the table we can see that phrase-based sys-tems trained on PLITG alignments performs slightlyworse than the ones trained on GIZA++.
Howevercombining both alignments with the simple data du-plication technique mentioned earlier produces theoverall best scores in both translation directions.The fact that both alignments lead to complemen-tary information can be seen in the size of the phrasetables extracted (see table 3).2.2 Syntax-based SMTWe used Moses and its syntax-mode for our exper-iments with hierarchical phrase-based and syntax-augmented models.
Our main interest was to in-vestigate the influence of monolingual parsing onthe translation performance.
In particular, we triedto integrate English dependency parses created byMaltParser (Nivre et al, 2007) trained on the WallStreet Journal section of the Penn Treebank (Mar-cus et al, 1993) extended with about 4000 questions3We actually swapped the development set and the test setby mistake.
But, of course, we never mixed development andtest data in any result reported.375from the Question Bank (Judge et al, 2006).
Theconversion to dependency trees was done using theStanford Parser (de Marneffe et al, 2006).
Again,we ran both translation directions to test our settingsin more than just one task.
Interesting here is alsothe question whether there are significant differenceswhen integrating monolingual parses on the sourceor on the target side.The motivation for applying dependency parsingin our experiments is to use the specific informationcarried by dependency relations.
Dependency struc-tures encode functional relations between words thatcan be seen as an interface to the semantics of asentence.
This information is usually not avail-able in phrase-structure representations.
We believethat this type of information can be beneficial formachine translation.
For example, knowing that anoun acts as the subject of a sentence is more in-formative than just marking it as part of a nounphrase.
Whether or not this information can be ex-plored by current syntax-based machine translationapproaches that are optimised for phrase-structurerepresentations is a question that we liked to inves-tigate.
For comparison we also trained hierarchicalphrase-based models without any additional annota-tion.2.2.1 Converting projective dependency treesFirst we needed to convert dependency parses toa tree representation in order to use our data inthe standard models of syntax-based models imple-mented in Moses.
In our experiments, we useda parser model that creates projective dependencygraphs that can be converted into tree structures ofnested segments.
We used the yield of each word(referring to that word and its transitive dependents)to define spans of phrases and their dependency rela-tions are used as span labels.
Furthermore, we alsodefined pre-terminal nodes that encode the part-of-speech information of each word.
These tags wereobtained using the HunPos tagger (Hala?csy et al,2007) trained on the Wall Street Journal section ofthe Penn Treebank.
Figure 2 illustrates the conver-sion process.
Tagging and parsing is done for all En-glish data without any manual corrections or optimi-sation of parameters.
After the conversion, we wereable to use the standard training procedures imple-mented in Moses.-ROOT- andCC howWRB oldJJ isVBZ yourPRP$ nephewNN ?.advmoddep possnsubjccpunctnull<tree label="null"><tree label="cc"><tree label="CC">and</tree></tree><tree label="dep"><tree label="advmod"><tree label="WRB">how</tree></tree><tree label="JJ">old</tree></tree><tree label="VBZ">is</tree><tree label="nsubj"><tree label="poss"><tree label="PRP$">your</tree></tree><tree label="NN">nephew</tree></tree><tree label="punct"><tree label=".
">?</tree></tree></tree>Figure 2: A dependency graph from the training corpusand its conversion to a nested tree structure.
The yield ofeach word in the sentence defines a span with the labeltaken from the relation of that word to its head.
Part-of-speech tags are used as additional pre-terminal nodes.2.2.2 Experimental ResultsWe ran several experiments with slightly differ-ent settings.
We used the same basic setup forall of them including the same language modelsand GIZA++ word alignments that we have usedfor the phrase-based models already.
Further, weused Moses for extracting rules of the syntax-basedtranslation model.
We use standard settings forthe baseline system (=hiero) that does not employany linguistic markup.
For the models that includedependency-based trees we changed the maximumspan threshold to a high value of 999 (default: 15)in order to extract as many rules as possible.
Thislarge degree of freedom is possible due to the oth-erwise strong constraints on rule flexibility imposedby the monolingual syntactic markup.
Rule tablesare dramatically smaller than for the unrestricted hi-erarchical models (see table 4).However, rule restriction by linguistic constraintsusually hurts performance due to the decreased cov-erage of the rule set.
One common way of improving376reference Are you going to let us die on Ile a` Vaches which is located close the city of Les Cayes.
I am ...pbsmt Do you are letting us die in Ilavach island?s on in Les Cayes.
I am ...hiero do you will let us die in the island Ilavach on the in Les Cayes .
I am ...samt2 Are you going to let us die in the island Ilavach the which is on the Les.
My name is ...reference I?m begging you please help me my situation is very critical.pbsmt Please help me please.
Because my critical situation very much.hiero please , please help me because my critical situation very much .samt2 Please help me because my situation very critical.reference I don?t have money to go and give blood in Port au Prince from La Gonave.pbsmt I don?t have money, so that I go to give blood Port-au-Prince since lagonave.hiero I don ?t have any money , for me to go to give blood Port-au-Prince since lagonave .samt2 I don?t have any money, to be able to go to give blood Port-au-Prince since Gona?ve Island.Figure 3: Example translations for various models.English-Haitian BLEU number of ruleshiero 0.2549 34,118,622malt (source) 0.2180 1,628,496- binarised 0.2327 9,063,933- samt1 0.2311 11,691,279- samt2 0.2366 29,783,694Haitian-English BLEU number of ruleshiero 0.3034 33,231,535malt (target) 0.2739 1,922,688- binarised 0.2857 8,922,343- samt1 0.2952 11,073,764- samt2 0.2954 24,554,317Table 4: Syntax-based SMT on the Haitian Creole-English test set with (=malt) or without (=hiero) Englishparse trees and various parse relaxation strategies.
Thefinal system submitted to WMT11 is malt(target)-samt2.rule extraction is based on tree manipulation and re-laxed extraction algorithms.
Moses implements sev-eral algorithms that have been proposed in the lit-erature.
Tree binarisation is one of them.
This canbe done in a left-branching and in a right-branchingmode.
We used a combination of both in the set-tings denoted as binarised.
The other relaxation al-gorithms are based on methods proposed for syntax-augmented machine translation (Zollmann et al,2008).
We used two of them: samt1 combines pairsof neighbouring children nodes into combined com-plex nodes and creates additional complex nodes ofall children nodes except the first child and similarcomplex nodes for all but the last child.
samt2 com-bines any pair of neighbouring nodes even if they arenot children of the same parent.
All of these relax-ation algorithms lead to increased rule sets (table 4).In terms of translation performance there seems tobe a strong correlation between rule table size andtranslation quality as measured by BLEU.
None ofthe dependency-based models beats the unrestrictedhierarchical model.
Both translation directions be-have similar with slightly worse performances ofthe dependency-based models (relative to the base-line) when syntax is used on the source languageside.
Note also that all syntax-based models (includ-ing hiero) are below the corresponding phrase-basedSMT systems.
Of course, automatic evaluation hasits limits and interesting qualitative differences maybe more visible in manual assessments.
The use oflinguistic information certainly has an impact on thetranslation hypotheses produced as we can see in theexamples in figure 3.
In the future, we plan to inves-tigate the effect of dependency information on gram-maticality of translated sentences in more detail.3 ConclusionsIn our English-French and Haitian Creole-Englishshared task submissions, we investigated the useof anaphora resolution, hierarchical lexical reorder-ing and data selection for language modelling(English-French) as well as LTG word alignmentand syntax-based decoding with dependency infor-mation (Haitian Creole-English).
While the re-sults for the systems with anaphora handling weresomewhat disappointing and the effect of data fil-tering was inconsistent, hierarchical lexical reorder-ing brought substantial improvements.
We also ob-tained consistent gains by combining informationfrom different word aligners, and we presented asimple way of including dependency parses in stan-dard tree-based decoding.377AcknowledgementsMost of the features used in our English-French sys-tem were originally developed while Christian Hard-meier was at FBK.
Activities at FBK were supportedby the EuroMatrixPlus project (IST-231720) and theT4ME network of excellence (IST-249119), bothfunded by the DG INFSO of the European Commis-sion through the Seventh Framework Programme.ReferencesSamuel Broscheit, Massimo Poesio, Simone PaoloPonzetto, Kepa Joseba Rodriguez, Lorenza Romano,Olga Uryupina, Yannick Versley, and Roberto Zanoli.2010.
BART: A multilingual anaphora resolution sys-tem.
In Proceedings of the 5th International Workshopon Semantic Evaluations (SemEval-2010), Uppsala.Marie-Catherine de Marneffe, Bill MacCartney, andChristopher D. Manning.
2006.
Generating typed de-pendency parses from phrase structure parses.
In Pro-ceedings of LREC.Marcello Federico, Nicola Bertoldi, and Mauro Cettolo.2008.
IRSTLM: an open source toolkit for handlinglarge scale language models.
In Interspeech 2008,pages 1618?1621.
ISCA.Michel Galley and Christopher D. Manning.
2008.
Asimple and effective hierarchical phrase reorderingmodel.
In Proceedings of the 2008 Conference onEmpirical Methods in Natural Language Processing,pages 847?855, Honolulu, Hawaii, October.
Associa-tion for Computational Linguistics.Pe?ter Hala?csy, Andra?s Kornai, and Csaba Oravecz.
2007.Hunpos: an open source trigram tagger.
In Proceed-ings of the 45th Annual Meeting of the ACL on Inter-active Poster and Demonstration Sessions, pages 209?212.Christian Hardmeier and Marcello Federico.
2010.
Mod-elling Pronominal Anaphora in Statistical MachineTranslation.
In Marcello Federico, Ian Lane, MichaelPaul, and Franc?ois Yvon, editors, Proceedings of theseventh International Workshop on Spoken LanguageTranslation (IWSLT), pages 283?289.John Judge, Aoife Cahill, and Josef van Genabith.
2006.Questionbank: creating a corpus of parse-annotatedquestions.
In Proceedings of the 21st InternationalConference on Computational Linguistics and the 44thannual meeting of the Association for ComputationalLinguistics, pages 497?504.Philipp Koehn, Amittai Axelrod, AlexandraBirch Mayne, et al 2005.
Edinburgh systemdescription for the 2005 iwslt speech translationevaluation.
In International workshop on spokenlanguage translation, Pittsburgh.Philipp Koehn, Hieu Hoang, Alexandra Birch, et al2007.
Moses: open source toolkit for Statistical Ma-chine Translation.
In Annual meeting of the Associ-ation for Computational Linguistics: Demonstrationsession, pages 177?180, Prague.Mitchell P. Marcus, Mary Ann Marcinkiewicz, and Beat-rice Santorini.
1993.
Building a large annotated cor-pus of english: The Penn Treebank.
ComputationalLinguistics, 19:313?330, June.Joakim Nivre, Johan Hall, Jens Nilsson, Atanas Chanev,Gu?lsen Eryigit, Sandra Ku?bler, Svetoslav Marinov,and Erwin Marsi.
2007.
MaltParser: A language-independent system for data-driven dependency pars-ing.
Natural Language Engineering, 13(2):95?135.Franz Josef Och and Hermann Ney.
2003.
A system-atic comparison of various statistical alignment mod-els.
Computational linguistics, 29:19?51.Markus Saers and Dekai Wu.
2011.
Principled inductionof phrasal bilexica.
In Proceedings of the 15th AnnualConference of the European Association for MachineTranslation, Leuven, Belgium, May.Markus Saers, Joakim Nivre, and Dekai Wu.
2010.
Wordalignment with stochastic bracketing linear inversiontransduction grammar.
In Human Language Technolo-gies: The 2010 Annual Conference of the North Amer-ican Chapter of the Association for ComputationalLinguistics, pages 341?344, Los Angeles, California,June.Markus Saers.
2011.
Translation as Linear Transduc-tion: Models and Algorithms for Efficient Learning inStatistical Machine Translation.
Ph.D. thesis, UppsalaUniversity, Department of Linguistics and Philology.M.
Strube.
2006.
Anaphora and coreference resolution,Statistical.
In Encyclopedia of language and linguis-tics, pages 216?222.
Elsevier.Andreas Zollmann, Ashish Venugopal, Franz Och, andJay Ponte.
2008.
A systematic comparison of phrase-based, hierarchical and syntax-augmented statisticalmt.
In Proceedings of the 22nd International Confer-ence on Computational Linguistics - Volume 1, pages1145?1152.378
