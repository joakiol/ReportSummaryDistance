A High-Performance Coreference Resolution Systemusing a Constraint-based Multi-Agent StrategyZHOU GuoDong            SU JianInstitute for Infocomm Research21 Heng Mui Keng TerraceSingapore 119613Email: zhougd@i2r.a-star.edu.sgAbstractThis paper presents a constraint-based multi-agent strategy to coreference resolution ofgeneral noun phrases in unrestricted Englishtext.
For a given anaphor and all the precedingreferring expressions as the antecedentcandidates, a common constraint agent is firstpresented to filter out invalid antecedentcandidates using various kinds of generalknowledge.
Then, according to the type of theanaphor, a special constraint agent is proposed tofilter out more invalid antecedent candidatesusing constraints which are derived from variouskinds of special knowledge.
Finally, a simplepreference agent is used to choose an antecedentfor the anaphor form the remaining antecedentcandidates, based on the proximity principle.One interesting observation is that the mostrecent antecedent of an anaphor in thecoreferential chain is sometimes indirectlylinked to the anaphor via some other antecedentsin the chain.
In this case, we find that the mostrecent antecedent always contains littleinformation to directly determine the coreferencerelationship with the anaphor.
Therefore, for agiven anaphor, the corresponding specialconstraint agent can always safely filter out theseless informative antecedent candidates.
In thisway, rather than finding the most recentantecedent for an anaphor, our system tries tofind the most direct and informative antecedent.Evaluation shows that our system achievesPrecision / Recall / F-measures of 84.7% /65.8% / 73.9 and 82.8% / 55.7% / 66.5 on MUC-6 and MUC-7 English coreference tasksrespectively.
This means that our systemachieves significantly better precision rates byabout 8 percent over the best-reported systemswhile keeping recall rates.1 IntroductionCoreference accounts for cohesion in texts.Especially, a coreference denotes an identity ofreference and holds between two expressions,which can be named entities, definite nounphrases, pronouns and so on.
Coreferenceresolution is the process of determining whethertwo referring expressions refer to the same entityin the world.
The ability to link referringexpressions both within and across the sentence iscritical to discourse and language understanding ingeneral.
For example, coreference resolution is akey task in natural language interfaces, machinetranslation, text summarization, informationextraction and question answering.
In particular,information extraction systems like those built inthe DARPA Message Understanding Conferences(MUC) have revealed that coreference resolution issuch a crucial component of an informationextraction system that a separate coreference taskhas been defined and evaluated in MUC-6 (1995)and MUC-7 (1998).There is a long tradition of work oncoreference resolution within computationallinguistics.
Many of the earlier works incoreference resolution heavily exploited domainand linguistic knowledge (Carter 1987; Rich andLuperFoy 1988; Carbonell and Brown 1988).However, the pressing need for the development ofrobust and inexpensive solutions encouraged thedrive toward knowledge-poor strategies (Daganand Itai 1990; Lappin and Leass 1994; Mitkov1998; Soon, Ng and Lim 2001; Ng and Cardie2002), which was further motivated by theemergence of cheaper and more reliable corpus-based NLP tools such as part-of-speech taggersand shallow parsers alongside the increasingavailability of corpora and other resources (e.g.ontology).Approaches to coreference resolution usuallyrely on a set of factors which include gender andnumber agreements, c-command constraints,semantic consistency, syntactic parallelism,semantic parallelism, salience, proximity, etc.These factors can be either ?constraints?
whichdiscard invalid ones from the set of possiblecandidates (such as gender and numberagreements, c-command constraints, semanticconsistency), or ?preferences?
which gives morepreference to certain candidates and less to others(such as syntactic parallelism, semanticparallelism, salience, proximity).
While a numberof approaches use a similar set of factors, thecomputational strategies (the way antecedents aredetermined, i.e.
the algorithm and formula forassigning antecedents) may differ, i.e.
from simpleco-occurrence rules (Dagan and Itai 1990) todecision trees (Soon, Ng and Lim 2001; Ng andCardie 2002) to pattern induced rules (Ng andCardie 2002) to centering algorithms (Grosz andSidner 1986; Brennan, Friedman and Pollard 1987;Strube 1998; Tetreault 2001).This paper proposes a simple constraint-basedmulti-agent system to coreference resolution ofgeneral noun phrases in unrestricted English text.For a given anaphor and all the preceding referringexpressions as the antecedent candidates, acommon constraint agent is first presented to filterout invalid antecedent candidates using variouskinds of general knowledge.
Then, according tothe type of the anaphor, a special constraint agentis proposed to filter out more invalid antecedentcandidates using constraints which are derivedfrom various kinds of special knowledge.
Finally, asimple preference agent is used to choose anantecedent for the anaphor form the remainingantecedent candidates, based on the proximityprinciple.
One interesting observation is that themost recent antecedent of an anaphor in thecoreferential chain is sometimes indirectly linkedto the anaphor via some other antecedents in thechain.
In this case, we find that the most recentantecedent always contains little information todirectly determine the coreference relationshipwith the anaphor.
Therefore, for a given anaphor,the corresponding special constraint agent canalways safely filter out these less informativeantecedent candidates.
In this way, rather thanfinding the most recent antecedent for an anaphor,our system tries to find the most direct andinformative antecedent.In this paper, we focus on the task ofdetermining coreference relations as defined inMUC-6 (1995) and MUC-7 (1998).
In order toevaluate the performance of our approach oncoreference resolution, we utilize the annotatedcorpus and the scoring programs from MUC-6 andMUC-7.
For MUC-6, 30 dry-run documentsannotated with coreference information are used asthe training data.
There are also 30 annotatedtraining documents from MUC-7.
The total size of30 training documents is close 12,400 words forMUC-6 and 19,000 for MUC-7.
For testing, weutilize the 30 standard test documents from MUC-6 and the 20 standard test documents from MUC-7.The layout of this paper is as follows: inSection 2, we briefly describe the preprocessing:determination of referring expressions.
In Section3, we differentiate coreference types and discusshow to restrict possible types of direct andinformative antecedent candidates according toanaphor types.
In Section 4, we describe theconstraint-based multi-agent system.
In Section 5,we evaluate the multi-agent algorithm.
Finally, wepresent our conclusions.2 Preprocessing: Determination ofReferring ExpressionsThe prerequisite for automatic coreferenceresolution is to obtain possible referringexpressions in an input document.
In our system,the possible referring expressions are determinedby a pipeline of NLP components:?
Tokenization and sentence segmentation?
Named entity recognition?
Part-of-speech tagging?
Noun phrase chunkingAmong them, named entity recognition, part-of-speech tagging and noun phrase chunking applythe same Hidden Markov Model (HMM) basedengine with error-driven learning capability (Zhouand Su 2000).
The named entity recognitioncomponent (Zhou and Su 2002) recognizes varioustypes of MUC-style named entities, that is,organization, location, person, date, time, moneyand percentage.
The HMM-based noun phrasechunking component (Zhou and Su 2000)determines various noun phrases based on theresults of named entity recognition and part-of-speech tagging.3 Coreference TypesSince coreference is a symmetrical and transitiverelation, it leads to a simple partitioning of a set ofreferring expressions and each partition forms acoreference chain.
Although any two referringexpressions in the coreference chain iscoreferential, some of conference pairs may bedirect while others may be indirect since they onlybecome conferential via other referring expressionsin the same coreference chain.
This indicates thatthe most recent antecedent of an anaphor in thecoreferential chain is sometimes indirectly linkedto the anaphor via some other antecedents in thechain.
In these indirect cases, we find that the mostrecent antecedent always contains littleinformation to directly determine the coreferencerelationship with the anaphor.
Generally, direct andinformative coreference pairs are much easier toresolve than indirect and less informative ones.
Inthe following example1,Microsoft Corp. (i) announced its (i) new CEOyesterday.
Microsoft (i) said ?1 The italic markables with the same identificationsymbol are coreferential.
?Microsoft Corp.?, ?its?
and ?Microsoft?
form acoreference chain.
Among the three coreferencepairs in the chain,1) The coreference pair between ?MicrosoftCorp.?
and ?Microsoft?
is direct.2) The coreference pair between ?MicrosoftCorp.?
and ?its?
is direct.3) The coreference pair between ?its?
and?Microsoft?
is indirect.
This coreference paironly becomes coreferential via anotherreferring expression ?Microsoft Corp.?
Directresolution of this coreference pair is error-prone and not necessary since it can beindirectly linked by the other two coreferencepairs in the coreference chain.Therefore, for a given anaphor, we can alwayssafely filter out these less informative antecedentcandidates.
In this way, rather than finding themost recent antecedent for an anaphor, our systemtries to find the most direct and informativeantecedent.
This also suggests that we can classifycoreference types according to the types ofanaphors and restrict the possible types ofantecedent candidates for a given anaphor type asfollows:?
Name alias coreferenceThis is the most widespread type of coreferencewhich is realised by the name alias phenomenon.The success of name alias coreference resolution islargely conditional on success at determining whenone referring expression is a name alias of anotherreferring expression.
Here, the direct antecedentcandidate of a named entity anaphor can only bethe type of named entity.
For example,Microsoft Corp. (i) announced its new CEOyesterday.
Microsoft (i) said ??
Apposition coreferenceThis is the easiest type of coreference.
A typicaluse of an appositional noun phrase is to provide analternative description for a named entity.
ForexampleJulius Caesar (i), the well-known emperor (i),was born in 100 BC.?
Predicate nominal coreferencePredicate nominal is typically coreferential withthe subject.
For example,George W. Bush (i) is the president of theUnited States (i).?
Pronominal coreferenceThis is the second widespread type of coreferencewhich is realised by pronouns.
Pronominalcoreference has been widely studied in literature oftraditional anaphora resolution.
The directantecedent candidate of a pronoun anaphor can beany type of referring expressions.
For example,Computational linguistics (i) from differentcountries attended the tutorial.
They (i) tookextensive note.?
Definite noun phrase coreferenceThis is the third widespread type of coreferencewhich is realised by definite noun phrases.
It hasalso been widely studied in the literature oftraditional anaphora resolution.
A typical case ofdefinite noun phrase coreference is when theantecedent is referred by a definite noun phraseanaphor representing either same concept(repetition) or semantically close concept (e.g.synonyms, super-ordinates).
The direct antecedentcandidate of a definite noun phrase anaphor can beany type of referring expressions except pronouns.For example,Computational linguistics (i) from differentcountries attended the tutorial.
Theparticipants (i) took extensive note.?
Demonstrative noun phrase coreferenceThis type of coreference is not widespread.
Similarto that of definite noun phrase coreference, thedirect antecedent candidate of a demonstrativenoun phrase anaphor can be any type of referringexpressions except pronouns.
For example,Boorda wants to limit the total number ofsailors on the arsenal ship (i) to between 50and 60.
Currently, this ship (i) has about 90sailors.?
Bare noun phrase coreferenceThe direct antecedent candidate of a bare nounphrase anaphor can be any type of referringexpressions except pronouns.
For example,The price of aluminium (i) siding has steadilyincreased, as the market for aluminium (i)reacts to the strike in Chile.4 Constraint-based Multi-Agent Systemfor Coreference ResolutionIn accordance with the above differentiation ofcoreference types according to the anaphor types, aconstraint-based multi-agent system is developed.4.1 Common Constraint AgentFor all coreference types described in Section 3, acommon constraint agent is applied first usingfollowing constraints:Morphological agreementsThese constraints require that an anaphor and itsantecedent candidate should agree in gender andnumber.
These kinds of morphological agreementshas been widely used in the literature of anaphoraresolutionSemantic consistencyThis constraint stipulates that the anaphor and itsantecedent candidate must be consistent insemantics.
For example, the anaphor and itsantecedent candidate should contain the samesense or the anaphor contains a sense which isparental to the antecedent candidate.
In this paper,WordNet (Miller 1990) is used for semanticconsistency check.For example,IBM (i) announced its new CEO yesterday.The company (i) said ?4.2 Special Constraint AgentsFor each coreference type described in Section 3, aspecial constraint agent is applied next using someheuristic rules mainly based on the accessibilityspace, which is learnt from the training data asfollows:For a given coreference type and a given validantecedent type, all the anaphors of the givencoreference type are identified first from left toright as they appear in the sentences.
For eachanaphor, its antecedent is then determined usingthe principle of proximity.
If the most recentantecedent candidate has the given antecedenttype, meet the morphological agreements andsemantic consistency and is in the samecoreference chain as the anaphor, this coreferencepair is counted as a correct instance for the givenconference type and the given antecedent type.Otherwise, it is counted as an error instance.
In thisway, the precision rates of the coreference typeover different valid antecedent types and differentaccessibility spaces are computed as the percentageof the correct instances among all the correct anderror instances.
Finally, the accessibility space fora given coreference type and a given antecedenttype is decided using a precision rate threshold(e.g.
95%).?
Agent for name alias coreferenceA named entity is co-referred with another namedentity when the formal is a name alias of the latter.This type of coreference has an accessibility spaceof the whole document.
In this paper, it is tackledby a named entity recognition component, as inZhou and Su (2002), using the following namealias algorithm in the ascending order ofcomplexity:1) The simplest case is to recognize full identityof strings.
This applies to all types of entitynames.2) The next simplest case is to recognize thevarious forms of location names.
Normally,various acronyms are applied, e.g.
?NY?
vs.?New York?
and ?N.Y.?
vs. ?New York?.Sometime, partial mention is also applied, e.g.?Washington?
vs. ?Washington D.C.?.3) The third case is to recognize the variousforms of personal proper names.
Thus anarticle on Microsoft may include ?Bill Gates?,?Bill?
and ?Mr.
Gates?.
Normally, the fullpersonal name is mentioned first in a documentand later mention of the same person isreplaced by various short forms such asacronym, the last name and, to a less extent,the first name, of the full person name.4) The most difficult case is to recognize thevarious forms of organizational names.
Forvarious forms of company names, consider a)?International Business Machines Corp.?,?International Business Machines?
and ?IBM?
;b) ?Atlantic Richfield Company?
and?ARCO?.
Normally, various abbreviationforms (e.g.
contractions and acronym) anddropping of company suffix are applied.
Forvarious forms of other organizational names,consider a) ?National University ofSingapore?, ?National Univ.
of Singapore?
and?NUS?
; b) ?Ministry of Education?
and?MOE?.
Normally, acronyms andabbreviations are applied.?
Agent for apposition coreferenceIf the anaphor is in apposition to the antecedentcandidate, they are coreferential.
The MUC-6 andMUC-7 coreference task definitions are slightlydifferent.
In MUC-6, the appositive should be adefinite noun phrase while both indefinite anddefinite noun phrases are acceptable in MUC-7.?
Agent for predicate nominal coreferenceIf the anaphor is the predicate nominal and theantecedent candidate is the subject, they arecoreferential.
This agent is still under construction.?
Agent for pronominal coreferenceThis agent is applied to the most widely studiedcoreference: pronominal coreference.
6 heuristicrules are learnt and applied depending on theaccessibility space and the types of the antecedentcandidates:1) If the anaphor is a person pronoun and theantecedent candidate is a person named entity,they are coreferential over the wholedocument.2) If the anaphor is a neuter pronoun and theantecedent candidate is an organization namedentity, they are coreferential when they are inthe same sentence.3) If the anaphor is a neuter plural pronoun andthe antecedent candidate is a plural nounphrase, they are coreferential over the wholedocument.4) If both the anaphor and the antecedentcandidate are third person pronouns, they arecoreferential over the whole document.5) If both the anaphor and the antecedentcandidate are first or second person pronouns,they are coreferential when they are in thesame paragraph.6) If both the anaphor and the antecedentcandidate are neuter pronouns, they arecoreferential when they are in the sameparagraph or the antecedent candidate is in theprevious paragraph of the anaphor.?
Agent for definite noun phrase coreferenceThe agent for definite noun phrase coreference ismainly based on the accessibility space.
This agentis based on the following 3 heuristic rules:1) The definite noun phrase will be coreferentialwith a named entity if they are in sameparagraph or the entity name is in the previousparagraph of the definite noun phrase.2) The definite noun phrase will be coreferentialwith a named entity if the head word of thedefinite noun phrase is only modified by thedeterminer ?the?.
That is, the definite nounphrase is of type ?the HEADWORD?, e.g.
?thecompany?.3) The definite noun phrase will be coreferentialwith a definite/demonstrative/indefinite nounphrase if they string-match2.?
Agent for demonstrative noun phrasecoreferenceThe agent for demonstrative noun phrasecoreference is similar to the agent for definite nounphrase coreference except that the anaphor is ademonstrative noun phrase.?
Agent for base noun phrase coreferenceThis is the most complicated and confusingcoreference in MUC coreference task definitions.Although this type of coreference occupies a largeportion, it is hard to find heuristic rules to dealwith it.
In our system, only one heuristic rule isapplied: If the anaphor and the antecedentcandidate string-match and include at least twowords except the determiner, they are coreferentialover the whole document.2 The determiners, e.g.
?a?, ?an?
and ?the?, are removedfrom the strings before comparison.
Therefore, ?thecompany?
string-matches ?a company?.4.3 Common Preference AgentFor a given anaphor, invalid antecedents are firstfiltered out using the above common constraintagent and the special constraint agent.
Then, thestrategy has to choose which of the remainingcandidates, if any, is the most likely antecedentcandidate.
In our strategy, this is done through acommon preference agent based on the principle ofproximity.
That is, our common preference agenttakes advantages of the relative locations of theremaining antecedent candidates in the text.Among the antecedent candidates:1) First it looks for those occurring earlier in thecurrent sentence, preferring the one that occursearliest in the natural left-to-right order.2) If there are no antecedent candidates occurringearlier in the current sentence, look to thoseoccurring in the immediately precedingsentence of the same paragraph, againpreferring the one that occurs earliest in thatsentence in left-to-right order.3) If nothing comes up, look back at thoseoccurring in the earlier sentences of the sameparagraph, moving back a sentence at a time,but now, within a given sentence preferring themost rightward candidate that occurs later inthe sentence.4) Finally, if the scope extends back beyond aparagraph boundary, it looks to those thatoccur in the sentences of the precedingparagraph, again preferring later to earlieroccurrences.4.4 Multi-Agent AlgorithmThe coreference resolution algorithm isimplemented based on the previous multi-agents.First, all the anaphors are identified from left toright as they appear in the sentences.
Then, for agiven anaphor,1) All the referring expressions occurred beforethe anaphor are identified as antecedentcandidates.2) The common constraint agent is applied tofilter out the invalid antecedent candidatesusing various general constraints, such asmorphological agreements and semanticconsistency constraints.3) The corresponding special constraint agent (ifexists) is recalled to first filter out indirect andless informative antecedent candidates andthen check the validity of the remainingantecedent candidates by using some heuristicrules.
In this way, more invalid antecedentcandidates are discarded using various specialconstraints, such as the accessibility space.4) The antecedent is chosen from the remainingantecedent candidates, if any, using thecommon preference agent based on theprinciple of proximity.5 ExperimentationTable 1 shows the performance of our constraint-based multi-agent system on MUC-6 and MUC-7standard test data using the standard MUCevaluation programs while Table 2 gives thecomparisons of our system with others using thesame MUC test data and the same MUC evaluationprograms.
Here, the precision (P) measures thenumber of correct coreference pairs in the answerfile over the total number of coreference pairs inthe answer file and the recall (R) measures thenumber of correct coreference pairs in the answerfile over the total number of coreference pairs inthe key file while F-measure is the weightedharmonic mean of precision and recall:PRRPF ++= 22 )1(?
?with =1.
2?Table 1: Results of our baseline multi-agent coreference resolution system on MUC-6 and MUC-7MUC-6 MUC-7 PerformanceR P F R P FOverall 65.8 84.7 73.9 55.7 82.8 66.5?
Agent for name alias coreference 32.7 (35) 92.3 - 33.6 (36) 89.0 -?
Agent for apposition coreference  4.3 (5) 95.5 -   2.6 (3) 84.6 -?
Agent for predicate nominal coreference3 - (2) - - - (3) - -?
Agent for pronominal coreference 18.6 (22) 77.5 - 10.8 (16) 72.3 -?
Agent for definite noun phrase coreference  9.4 (15) 80.0 -   7.0 (20) 85.0 -?
Agent for demonstrative noun phrase coreference  0.1 (2) 50.0 -   0.2 (2) 66.7 -?
Agent for bare noun phrase coreference  1.9 (19) 63.0    1.7 (20) 61.1 -Table 2: Comparison of our system with the best-reported systems on MUC-6 and MUC-7MUC-6 MUC-7 Performance ComparisonR P F R P FOurs 65.8 84.7 73.9 55.7 82.8 66.5Ng and Cardie 2002 (C4.5) 64.1 74.9 69.1 57.4 70.8 63.4Ng and Cardie 2002 (RIPPER) 64.2 78.0 70.4 55.7  72.8 63.1Table 1 shows that our system achieves F-measures of 73.9 and 66.5 on MUC-6 and MUC-7standard test data, respectively.
The figures outsidethe parentheses show the contributions of variousagents to the overall recall while the figures insidethe parentheses show the frequency distribution ofvarious coreference types in the answer file.
Itshows that the performance difference betweenMUC-6 and MUC-7 mainly comes from thesignificant distribution variation of pronominalcoreference.
It also shows that there are muchroom for improvement, especially for the types ofpronominal coreference and definite noun pronounresolution.
Table 2 shows that our system achievessignificantly better F-measures by 3.1~4.8 percentover the best-reported systems (Ng and Cardie2002).
Most of the contributions come formprecision gains.
Our system achieves significantlybetter precision rates by 6.7~10.0 percent over thebest-reported systems (Ng and Cardie 2002) whilekeeping recall rates.
One reason behind such highperformance is the restriction of indirect and lessinformative antecedent candidates according to thetype of the anaphor.
Another reason isdifferentiation of various types of coreference andthe use of multi-agents.
In this way, various typesof coreference are dealt with effectively bydifferent agents according to their characteristics.The recall difference between our system and theRIPPER system in (Ng and Cardie 2002) maybecome from the predicate nominal coreference,which can be easily resolved using a machinelearning algorithm, e.g.
(Cohen 1995).
Completionof the agent for predicate nominal coreference caneasily fill the difference.6 ConclusionsThis paper presents a constraint-based multi-agentstrategy to coreference resolution of general nounphrases in unrestricted English text.The first contribution of this paper comes fromthe high performance of our system and its easy3 The agent for predicate nominal coreference is still under construction.implementation.
The second contribution is tofilter out indirect and less informative antecedentcandidates according to the anaphor type.
The thirdcontribution is the differentiation of variouscoreference types according to the anaphor typesand the use of multi-agents.Future work includes:?
The exploration of new constraints to improvethe precision and new coreference types toincrease the recall.?
The problem of type coercion or metonymywhich is a general problem and accounts formuch of the overall missing recall.?
The problem of cataphora, which is nothandled in the current mechanism.ReferencesBrennan S. E. Friedman M. W. and Pollard C. J.1987.
A centering approach to pronouns.Proceedings of the 25th Annual Meeting of theAssociation for Computational Linguistics(ACL?1987), pages 155-162.Carbonell J. and Brown R. 1988.
Anaphoraresolution: a multi-strategy approach.Proceedings of the 12th International Conferenceon Computational Linguistics (COLING?1988),pages 96-101, Budapest, Hungary.Carter D. M. 1987.
Interpreting Anaphors inNatural Language Texts.
Ellis Horwood,Chichester, UK.Cohen W. 1995.
Fast effective rule induction.Proceedings of the Twelfth InternationalConference on Machine Learning (ICML?1995).pages 115-123.
Tahoe City, CA.Dagan I. and Itai A.
1990.
Automatic processing oflarge corpora for the resolution of anaphorareferences.
Proceedings of the 13th InternationalConference on Computational Linguistics(COLING?1990), pages 1-3, Helsinki, Finland.Grosz B. J. and Sidner C. L. 1986.
Attention,intentions and the structure of discourse.Computational Linguistics, 12(3):175-204.Lappin S. and Leass H. 1994.
An algorithm forpronominal anaphora resolution.
ComputationalLinguistics.
20(4):535-561.Miller G.A.
1990.
WordNet: An online lexicaldatabase.
International Journal of Lexicography.3(4):235-312.Mitkov R. 1998.
Robust pronoun resolution withlimited knowledge.
Proceedings of the 36thAnnual Meeting for Computational Linguisticsand the 17th International Conference onComputational Linguistics(COLING/ACL?1998), pages 869-875,Montreal, Canada.MUC-6.
1995.
Proceedings of the 6th MessageUnderstanding Conference (MUC-6).
MorganKaufmann, San Francisco, CA.MUC-7.
1998.
Proceedings of the 7th MessageUnderstanding Conference (MUC-7).
MorganKaufmann, San Mateo, CA.Ng V. and Cardie C. 2002.
Improving machinelearning approaches to coreference resolution.Proceedings of the 40th Annual Meeting of theAssociation for Computational Linguistics(ACL?2002), pages 104-111, Philadelphia, Penn.Rich E. and LuperFoy S. 1988.
An architecture foranaphora resolution.
Proceedings of the 2ndConference on Applied Natural LanguageProcessing (ANLP?1988), pages 18-24, Austin,TX.Soon W. M.., Ng H. T. and Lim C. Y.
2001.
Amachine learning approach to coreferenceresolution of noun phrases.
ComputationalLinguistics, 27(4):521-544.Strube M. 1998.
Never look back: An alternative tocentering.
Proceedings of the 36th AnnualMeeting of the Association for ComputationalLinguistics and the 17th International Conferenceon Computational Linguistics, pages 1251-1257.Tetreault J. R. 2001.
A corpus-based evaluation ofcentering and pronoun resolution.
ComputationLinguistics, 27(4):507-520.Zhou G. D. and Su Jian, 2000.
Error-driven HMM-based chunk tagger with context-dependentlexicon.
Proceedings of the Joint Conference onEmpirical Methods on Natural LanguageProcessing and Very Large Corpus (EMNLP/VLC'2000).
Hong Kong.Zhou G. D. and Su Jian.
2002.
Named EntityRecognition Using a HMM-based ChunkTagger, Proceedings of the 40th Annual Meetingof the Association for Computational Linguistics(ACL?2002).
Philadelphia.
