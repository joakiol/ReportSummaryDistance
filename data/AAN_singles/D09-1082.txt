Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 784?792,Singapore, 6-7 August 2009.c?2009 ACL and AFNLPRecognizing Textual Relatedness with Predicate-Argument StructuresRui WangDept of Computational LinguisticsSaarland University66123 Saarbr?ucken, Germanyrwang@coli.uni-sb.deYi ZhangDept of Computational LinguisticsSaarland UniversityLT-Lab, DFKI GmbHD-66123 Saarbr?ucken, Germanyyzhang@coli.uni-sb.deAbstractIn this paper, we first compare severalstrategies to handle the newly proposedthree-way Recognizing Textual Entailment(RTE) task.
Then we define a new mea-surement for a pair of texts, called TextualRelatedness, which is a weaker conceptthan semantic similarity or paraphrase.
Weshow that an alignment model based on thepredicate-argument structures using thismeasurement can help an RTE system torecognize the Unknown cases at the firststage, and contribute to the improvementof the overall performance in the RTE task.In addition, several heterogeneous lexicalresources are tested, and different contri-butions from them are observed.1 IntroductionRecognizing Textual Entailment (RTE) (Dagan etal., 2006) is a task to detect whether one Hypoth-esis (H) can be inferred (or entailed) by a Text(T).
Being a challenging task, it has been shownthat it is helpful to applications like question an-swering (Harabagiu and Hickl, 2006).
The recentresearch on RTE extends the two-way annotationinto three-way1 2, making it even more difficult,but more linguistic-motivated.The straightforward strategy is to treat it as athree-way classification task, but the performancesuffers a significant drop even when using thesame classifier and the same feature model.
Infact, it can also be dealt with as an extension to thetraditional two-way classification, e.g., by identi-1http://nlp.stanford.edu/RTE3-pilot/2http://www.nist.gov/tac/tracks/2008/rte/rte.08.guidelines.htmlfying the Entailment (E) cases first and then fur-ther label the Contradiction (C) and Unknown (U)T-H pairs.
Some other researchers also work ondetecting negative cases, i.e.
contradiction, in-stead of entailment (de Marneffe et al, 2008).However, according to our best knowledge, thedetailed comparison between these strategies hasnot been fully explored, let alne the impact of thelinguistic motivation behind the strategy selection.This paper will address this issue.Take the following example from the RTE-4 testset (Giampiccolo et al, 2009) as an example,T: At least five people have been killed ina head-on train collision in north-easternFrance, while others are still trapped in thewreckage.
All the victims are adults.H: A French train crash killed children.This is a pair of two contradicting texts, thementioning of events (i.e.
train crash) in both Tand H are assumed to refer the same event3.
Infact, the only contradicting part lies in the sec-ond sentence of T against H, that is, whetherthere are children among the victims.
Therefore,this pair could also be classified as a Known (K)pair (=E?C) against Unknown (U) pairs, insteadof being classified as a Non-entailment (N) case(=C?U) against E case in the traditional two-wayannotation.Furthermore, many state-of-the-art RTE ap-proaches which are based on overlapping informa-tion or similarity functions between T and H, infact over-cover the E cases, and sometimes, coverthe C cases as well.
Therefore, in this paper, we3See more details about the annotation guideline athttp://www.nist.gov/tac/tracks/2008/rte/rte.08.guidelines.html784would like to test whether applying this style ofapproaches to capture the K cases instead of Ecases is more effective.
While in lexical seman-tics, semantic relatedness is a weaker concept thansemantic similarity, there is no counterpart at thesentence or text level.
Therefore, in this paper, wepropose a Recognizing Textual Relatedness (RTR)task as a subtask or the first step of RTE.
By doingso, we choose predicate-argument structure (PAS)as the feature representation, which has alreadybeen shown quite useful in the previous RTE chal-lenges (Wang and Neumann, 2007).In order to obtain the PAS, we utilize a SemanticRole Labeling (SRL) system developed by Zhanget al (2008).
Although SRL has been shown to beeffective for many tasks, e.g.
information extrac-tion, question answering, etc., it has not been suc-cessfully used for RTE, mainly due to the low cov-erage of the verb frame or semantic role resourcesor the low performance of the automatic SRL sys-tems.
The recent CoNLL shared tasks (Surdeanuet al, 2008; Haji?c et al, 2009) have been focus-ing on semantic dependency parsing along withthe traditional syntactic dependency parsing.
ThePAS from the system output is almost ready foruse to build applications based on it.
Therefore,another focus of this paper will be to apply SRL tothe RTE task.
In particular, it can improve the firststage binary classification (K vs. U), and the finalresult improves as well.The rest of the paper will be organized as fol-lows: Section 2 will give a brief literature reviewon both RTE and SRL; Section 3 describes the se-mantic parsing system, which includes a syntacticdependency parser and an SRL system; Section 4presents an algorithm to align two PASs to recog-nize textual relatedness between T and H, usingseveral lexical resources; The experiments will bedescribed in Section 5, followed by discussions;and the final section will conclude the paper andpoint out directions to work on in the future.2 Related WorkAlthough the term of Textual Relatedness has notbeen widely used by the community (as far aswe know), many researchers have already incor-porated modules to tackle it, which are usuallyimplemented as an alignment module before theinference/learning module is applied.
For exam-ple, Pado et al (2009) mentioned two alignmentmodules, one is a phrase-based alignment systemcalled MANLI (MacCartney et al, 2008), and theother is a stochastic aligner based on dependencygraphs.
Siblini and Kosseim (2009) performed thealignment on top of two ontologies.
In this paper,we would like to follow this line of research but onanother level of representation, i.e.
the predicate-argument structures (PAS), together with differentlexical semantic resources.As for the whole RTE task, many people di-rectly do the three-way classification with selec-tive features (e.g.
Agichtein et al (2009)) or dif-ferent inference rules to identify entailment andcontradiction simultaneously (e.g.
Clark and Har-rison (2009)); while other researchers also extendtheir two-way classification system into three-wayby performing a second-stage classification after-wards.
An interesting task proposed by de Marn-effe et al (2008) suggested an alternative way todeal with the three-way classification, that is, tosplit out the contradiction cases first.
However,it has been shown to be more difficult than theentailment recognition.
Based on these previousworks and our experimental observations, we pro-pose an alternative two-stage binary classificationapproach, i.e.
to identify the unknown cases fromthe known cases (entailment and contradiction)first.
And the results show that due to the natureof these approaches based on overlapping infor-mation or similarity between T and H, this way ofsplitting is more reasonable.However, RTE systems using semantic role la-belers has not shown very promising results, al-though SRL has been successfully used in manyother NLP tasks, e.g.
information extraction,question answering, etc.
According to our anal-ysis of the data, there are mainly three reasons: a)the limited coverage of the verb frames or predi-cates; b) the undetermined relationships betweentwo frames or predicates; and c) the unsatisfy-ing performance of an automatic SRL system.For instance, Burchardt et al (2007) attempted touse FrameNet (Baker et al, 1998) for the RTE-3challenge, but did not show substantial improve-ment.
With the recent CoNLL challenges, moreand more robust and accurate SRL systems areready for use, especially for the PAS identifica-tion.
For the lexical semantics, we also discoverthat, if we relax the matching criteria (from simi-larity to relatedness), heterougeous resources cancontribute to the coverage differently and then theeffectiveness of PAS will be shown as well.7853 Semantic ParsingIn order to obtain the predicate-argument struc-tures for the textual entailment corpus, we use thesemantic role labeler described in (Zhang et al,2008).
The SRL system is trained on the WallStreet Journal sections of the Penn Treebank us-ing PropBank and NomBank annotation of ver-bal and nominal predicates, and relations to theirarguments, and produces as outputs the semanticdependencies.
The head words of the arguments(including modifiers) are annotated as a direct de-pendent of the corresponding predicate words, la-beled with the type of the semantic relation (Arg0,Arg1 .
.
.
, and various ArgMs).
Note that for theapplication of SRL in RTE task, the PropBank andNomBank notation appears to be more accessibleand robust than the the FrameNet notation (withmuch more detailed roles or frame elements bondto specific verb frames).As input, the SRL system requires syntacticdependency analysis.
We use the open sourceMST Parser (McDonald et al, 2005), trained alsoon the Wall Street Journal Sections of the PennTreebank, using a projective decoder with second-order features.
Then the SRL system goes througha pipeline of 4-stage processing: predicate identifi-cation (PI) identifies words that evokes a semanticpredicate; argument identification (AI) identifiesthe arguments of the predicates; argument classifi-cation (AC) labels the argument with the semanticrelations (roles); and predicate classification (PC)further differentiate different use of the predicateword.
All components are built as maximal en-tropy based classifiers, with their parameters es-timated by the open source TADM system4, fea-ture sets selected on the development set.
Evalu-ation results from previous years?
CoNLL sharedtasks show that the system achieves state-of-the-art performance, especially for its out-domain ap-plications.4 Textual RelatednessAs we mentioned in the introduction, we breakdown the three-way classification into a two-stagebinary classification.
Furthermore, we treat thefirst stage as a subtask of the main task, whichdetermines whether H is relevant to T. Similar tothe probabilistic entailment score, we use a relat-edness score to measure such relationship.
Due4http://tadm.sourceforge.net/to the nature of the entailment recognition thatH should be fully entailed by T, we also makethis relatedness relationship asymmetric.
Roughlyspeaking, this Relatedness function R(T,H) canbe described as whether or how relevant H is tosome part of T. The relevance can be realized asstring similarity, semantic similarity, or being co-occurred in similar contexts.Before we define the relatedness function for-mally, let us look at the representation again.
Aftersemantic parsing described in the previous section,we obtain a PAS for each sentence.
On top of it,we define a predicate-argument graph (PAG), thenodes of which are predicates, arguments or some-times both, and the edges of which are labeled se-mantic relations.
Notice that each predicate candominate zero, one, or more arguments, and eachargument have one or more predicates which dom-inate it.
Furthermore, the graph is not necessar-ily fully connected.
Thus, the R(T,H) functioncan be defined on the dependency representationas follows: if the PAG of H is semantically rel-evant to part of the PAG of T, H is semanticallyrelevant to T.In order to compare the two graphs, we furtherreduce the alignment complexity by breaking thegraphs into sets of trees.
Two types of decomposedtrees are considered: one is to take each predicateas the root of a tree and arguments as child nodes,and the other is on the contrary, to take each ar-gument as root and their governing predicates aschild nodes.
We name them as Predicate Trees (P-Trees) and Argument Trees (A-Trees) respectively.To obtain the P-Trees, we enumerate each predi-cate, find all the arguments which it directly dom-inates, and then construct a P-Tree.
The algorithmto obtain A-Trees works in the similar way.
Fi-nally, we will have a set of P-Trees and a set of A-Trees for each PAG, both of which are simple treeswith depth of one.
Figure 1 shows an example ofsuch procedures.
Notice that we do not considercross-sentential inference, instead, we simply takethe union of tree sets from all the sentences.
Figure2 illustrates the PAG for both T and H after seman-tic parsing, and the resulting P-Trees and A-Treesafter applying the decomposition algorithm.Formally, we define the relatedness function fora T-H pair as the maximum value of the related-ness scores of all pairs of trees in T and H (P-treesand A-trees).786ad e fb c egead ea b afege fb cadgA?Tree(s)P?Tree(s)Figure 1: Decomposition of predicate-argument graphs (left) into P-Trees (right top) and A-Trees (rightbottom)R(T,H) = max1?i?r,1?j?s{R(TreeTi, T reeHj)}In order to compare two P-Trees or A-Trees,we further define each predicate-argument paircontained in a tree as a semantic dependencytriple.
Each semantic dependency triple con-tains a predicate, an argument, and the seman-tic dependency label in between, in the formof ?Predicate,Dependency,Argument?.
Thenwe define the relatedness function between twotrees as the minimum value of the relatednessscores of all the triple pairs from the two trees.R(TreeT, T reeH) = min1?i?n,1?j?m{R(?PT, DTi, ATi?, ?PH, DHj, AHj?
)}For the relatedness function between two se-mantic dependency triples, we define the follow-ing two settings: the FULL match and the NOT-FULL match.
Either match requires that the pred-icates are related at the first place.
The formermeans both the dependencies and the argumentsare related; while the latter only requires the de-pendencies to be related.R(?PT, DT, AT?, ?PH, DH, AH?)
=??
?Full R(PT,PH)=R(DT,DH)=R(AT,AH)=1NotFull R(PT,PH)=R(DT,DH)=1Other OtherwiseNow, the only missing components in our defi-nition is the relatedness functions between pred-icates, arguments, and semantic dependencies.Fortunately, many people have done research onsemantic relatedness in lexical semantics that wecould use.
Therefore, these functions can berealized by different string matching algorithmsand/or lexical resources.
Since the meaning of rel-evance is rather wide, apart from the string match-ing of the lemmas, we also incorporate variousresources, from distributionally collected ones tohand-crafted ontologies.
We choose VerbOcean(Chklovski and Pantel, 2004) to obtain the relat-edness between predicates (after using WordNet(Fellbaum, 1998) to change all the nominal pred-icates into verbs) and use WordNet for the argu-ment alignment.
For the verb relations in Ver-bOcean, we consider all of them as related; andfor WordNet, we not only use the synonyms, hy-ponyms, and hypernyms, but antonyms as well.Consequently, we simplify these basic relatednessfunctions into a binary decision.
If the correspond-ing strings are matched or the relations mentionedabove exist, the two predicates, arguments, or de-pendencies are related; otherwise, not.In addition, the Normalized Google Distance(NGD) (Cilibrasi and Vitanyi, 2007) is applied toboth cases5.
As for the comparison between de-pendencies, we simply apply the string matching,except for modifier labels, which we treat them asthe same6.
In all, the main idea here is to incorpo-rate both distributional semantics and ontologicalsemantics in order to see whether their contribu-tions are overlapping or complementary.
In prac-tice, we use empirical value 0.5 as the threshold.Below the threshold means they are related, oth-5You may find the NGD values of all the con-tent word pairs in RTE-3 and RTE-4 datasets athttp://www.coli.uni-sb.de/?rwang/resources/RTE3_RTE4_NGD.txt.6This is mainly because it is more difficult for the SRLsystem to differentiate modifier labels than the complements.787crashkilledtrainchildrenA0 A1A1whilepeopleA1killed... ... traincollisionstillA1trapped... ...AM?ADVAM?ADVA1crashtrainkilledchildrenA0 A1crashP?TreesA?TreeswhilepeopleA1killed... ... traincollisionstillA1trapped... ...AM?ADVAM?ADVkilled killed killed collision trapped trappedpeople while ... ... train still ... ...A1 AM?ADV A1 AM?ADVkilled killed crashtraincrash childrenA1A0 A1T HPAGFigure 2: Predicate-argument graphs and corresponding P-Trees and A-trees of the T-H pairerwise not.
In order to achieve a better coverage,we use the OR operator to connect all the related-ness functions above, which means, if any of themholds, the two items are related.Notice that, although we define only the relat-edness between T and H, in principle, the graphrepresentation can also be used for the entailmentrelationship.
However, since it needs more fine-grained analysis and resources, we will leave it asthe future work.5 ExperimentsIn order to evaluate our method, we setup severalexperiments.
The baseline system here is a simpleNaive Bayes classifier with a feature set contain-ing the Bag-of-Words (BoW) overlapping ratio be-tween T and H, and also the syntactic dependencyoverlapping ratio.
The feature model combinestwo baseline systems proposed by previous work,which gives out quite competitive performance.Since the main goal of this paper is to show theimpact of the PAS-based alignment module, wewill not compare our results with other RTE sys-tems (In fact, the baseline system already outper-forms the average accuracy score of the RTE-4challenge).The main data set used for testing here is theRTE-4 data set with three-way annotations (500entailment T-H pairs (E), 150 contradiction pairs(C), and 350 unknown pairs (U)).
The results onRTE-3 data set (combination of the developmentset and test set, in all, 822 E pairs, 161 C pairs,and 617 U pairs) is also shown, although the origi-nal annotation is two-way and the three-way anno-tation was done by different researchers after thechallenge7.We will first show the performance of the base-line systems, followed by the results of our PAS-based alignment module and its impact on thewhole task.
After that, we will also give more de-tailed analysis of our alignment module, accordingto different lexical relatedness measurements.7The annotation of the development set was done by stu-dents at Stanford, and the annotation of the test set was doneas double annotation by NIST assessors, followed by adjudi-cation of disagreements.
Answers were kept consistent withthe two-way decisions in the main task gold answer file.7885.1 BaselinesThe baseline systems used here are based on over-lapping ratio of words and syntactic dependenciesbetween T and H. For the word overlapping ratio,we calculate the number of overlapping tokens be-tween T and H and normalize it by dividing it bythe number of tokens in H. The syntactic depen-dency overlapping ratio works similarly: we cal-culate the number of overlapping syntactic depen-dencies and divide it by the number of syntacticdependencies in H, i.e.
the same as the numberof tokens.
Enlightened by the relatedness func-tion, we also allow either FULL match (meaningboth the dependencies and the parent tokens arematched), and NOTFULL match (meaning only thedependencies are matched).
Here we only usestring match between lemmas and syntactic de-pendencies.
Table 1 presents the performance ofthe baseline system.The results show that, even with the same clas-sifier and the same feature model, with a propertwo-stage strategy, it can already achieve betterresults than the three-way classification.
Notethat, the first strategy is not so successful, andthat is the traditional two-way annotation of theRTE task.
Our explanation here is that the BoWmethod (even with syntactic dependency features)is based on overlapping information shared by Tand H, which essentially means the more informa-tion they share, the more relevant they are, insteadof being more similar or the same.
Therefore, forthe ?ECU ?
E/CU?
setting, methods based onoverlapping information are not the best choice,while for ?ECU ?
U/EC?, they are more ap-propriate.In addition, the upper bound numbers show theaccuracy when the first-stage classification is per-fect, which give us an indication of how far wecould go.
The lower upper bound for the secondstrategy is mainly due to the low proportion of theC cases (15%) in the data set; while the other twoboth show large space for improvement.5.2 The PAS-based Alignment ModuleIn this subsection, we present a separate evalua-tion of our PAS-based alignment module.
As wementioned before (cf.
Section 4), there are sev-eral parameters to be tuned in our alignment algo-rithm: a) whether the relatedness function betweenP-Trees asks for the FULL match; b) whether thefunction for A-Trees asks for the FULL match; andc) whether both P-Trees and A-Trees being relatedare required or either of them holds is enough.Since they are all binary values, we use the 3-digitcode to represent each setting, e.g.
[FFO]8meanseither P-Trees are FULL matched or A-Trees areFULL matched.
The performances of different set-tings of the module are shown in the followingPrecision-Recall figure 3,0102030405060708068707274767880828486Recall (%)Precision(%)[FFA][NFA][FNA][FFO][NNA][NFO][FNO][NNO]Figure 3: Precision and recall of different align-ment settingsSince we will combine this module with thebaseline system and it will be integrated as thefirst-stage classification, the F1 scores are not in-dicative for selecting the best setting.
Intuitively,we may prefer higher precision than recall.One limitation of our method we need to pointout here is that, if some important predicates or ar-guments in H are not (correctly) identified by theSRL system, fewer P-Trees and A-Trees are re-quired to be related to some part of T, thus, therelatedness of the whole pair could easily be satis-fied, leading to false positive cases.5.3 Impact on the Final ResultsThe best settings for RTE-3 data set is [NNA] andfor RTE-4 data set is [NFO], which are both in themiddle of the setting range shown in the previousfigure 3.As for the integration of the PAS-based align-ment model with our BoW-based baseline, weonly consider the third two-stage classificationstrategy in Table 1.
Other strategies would also beinteresting to try, however, the proposed alignmentalgorithm exploits relatedness between T and H,which might not be fine-grained enough to detect8F stands for FULL, and O stands for OR.
Other lettersare, N stands for NOTFULL, and A stands for AND.789Strategies Three-Way Two-StageE/C/U E/CU ?
E/C/U C/EU ?
C/E/U U/EC ?
U/E/CAccuray 53.20% 50.00% 53.50% 54.20%Upper Bound / 82.80% 68.70% 84.90%Table 1: Performances of the Baselinesentailment or contradiction.
New alignment algo-rithm has to be designed to explore other strate-gies.
Thus, in this work, we believe that the align-ment algorithm based on PAS (and other methodsbased on overlapping information between T andH) is suitable for the U/EC ?
U/E/C classifi-cation strategy.Table 2 shows the final results.The first observation is that the improvement ofaccuracy on the first stage of the classification canbe preserved to the final results.
And our PAS-based alignment module can help, though thereis still large space for improvement.
Comparedwith the significantly improved results on RTE-4,the improvement on RTE-3 is less obvious, mainlydue to the relatively lower precision (70.33% vs.79.67%) of the alignment module itself.Also, we have to say that the improvement is notas big as we expected.
There are several reasonsfor this.
Besides the limitation of our approachmentioned in the previous section, the predicatesand arguments themselves might be too sparse toconvey all the information we need for the en-tailment detection.
In addition, in some sense,the baseline is quite strong for this comparison,since the PAS-based alignment module relies onthe overlapping words at the first place, there arequite a few pairs solved by both the main approachand the baseline.
Then, it would be interestingto take a closer look at the lexical resources usedin the main system, which is another additionalknowledge it has, comparing with the baseline.5.4 Impact of the Lexical ResourcesWe did an ablation test of the lexical resourcesused in our alignment module.
Recall that wehave applied three lexical resources, VerbOceanfor the predicate relatedness function, WordNetfor the argument relatedness function, and Nor-malized Google Distance for both.
Table 3 showsthe performances of the system without each of theresources,The results clearly show that each lexical re-source does contribute some improvement to thefinal performance of the system and it confirmsthe idea of combining lexical resources being ac-quired in different ways.
For instance, at thebeginning, we expected that the relationship be-tween ?people?
and ?children?
could be capturedby WordNet, but in fact not.
Fortunately, the NGDhas a quite low value of this pair of words (0.21),which suggests that they occur together quite of-ten, or in other words, they are relevant.One interesting future work on this aspect is tosubstitute the OR connector between these lexicalresources with an AND operator.
Thus, instead ofusing them to achieve a higher coverage, whetherthey could be filters for each other to increase theprecision will also be interesting to know.6 Conclusion and Future WorkIn this paper, we address the motivation and issuesof casting the three-way RTE problem into a two-stage binary classification task.
We apply an SRLsystem to derive the predicate-argument structureof the input sentences, and propose ways of cal-culating semantic relatedness between the shallowsemantic structures of T and H. The experimentsshow improvements in the first-stage classifica-tion, which accordingly contribute to the final re-sults of the RTE task.For future work, we would like to see whetherthe PAS can help the second-stage classificationas well, e.g.
the semantic dependency of negation(AM-NEG) could be helpful for the contractionrecognition.
Furthermore, since the PAS is usu-ally a bag of unconnected graphs, we could finda way to joint them together, in order to considerboth inter- and intra- sentential inferences basedon it.In addition, this approach has the potential tobe integrated with other RTE modules.
For in-stance, for the predicate alignment, we may con-sider to use DIRT rules (Lin and Pantel, 2001)or other paraphrase resources (Callison-Burch,2008), and for the argument alignment, exter-nal named-entity recognizer and anaphora resolverwould be very helpful.
Even more, we also plan tocompare/combine it with other methods which arenot based on overlapping information between Tand H.790Systems Baseline1 Baseline2 SRL+Baseline2 The First StageData Sets Three-Way Two-Stage Two-Stage Baseline2 SRL+Baseline2 SRLRTE-3 [NNA] 52.19% 52.50% 53.69%(2.87%?)
59.50% 60.56%(1.78%?)
70.33%RTE-4 [NFO] 53.20% 54.20% 56.60%(6.39%?)
67.10% 70.20%(4.62%?)
79.67%Table 2: Results on the Whole DatasetsData Sets SRL+Baseline SRL+Baseline - VO SRL+Baseline - NGD SRL+Baseline - WNRTE-3 [NNA] 53.69% 53.19%(0.93%?)
53.50%(0.35%?)
52.88%(1.51%?
)RTE-4 [NFO] 56.60% 56.00%(1.06%?)
56.10%(0.88%?)
55.70%(1.59%?
)Table 3: Impact of the Lexical ResourcesAcknowledgmentsThe first author is funded by the PIRE PhDscholarship program sponsored by the GermanResearch Foundation (DFG).
The second authorthanks the German Excellence Cluster of Multi-modal Computing and Interaction for the supportof the work.ReferencesEugene Agichtein, Walt Askew, and Yandong Liu.2009.
Combining Lexical, Syntactic, and SemanticEvidence for Textual Entailment Classification.
InProceedings of the First Text Analysis Conference(TAC 2008).Collin F. Baker, Charles J. Fillmore, and John B. Lowe.1998.
The berkeley framenet project.
In Proceed-ings of the 36th Annual Meeting of the Associa-tion for Computational Linguistics and 17th Inter-national Conference on Computational Linguistics,Volume 1, pages 86?90, Montreal, Canada.Aljoscha Burchardt, Nils Reiter, Stefan Thater, andAnette Frank.
2007.
A semantic approach to textualentailment: System evaluation and task analysis.
InProceedings of the ACL-PASCAL Workshop on Tex-tual Entailment and Paraphrasing, Prague, CzechRepublic.Chris Callison-Burch.
2008.
Syntactic constraints onparaphrases extracted from parallel corpora.
In Pro-ceedings of EMNLP.Timothy Chklovski and Patrick Pantel.
2004.
VerbO-cean: Mining the Web for Fine-Grained SemanticVerb Relations.
In Proceedings of Conference onEmpirical Methods in Natural Language Processing(EMNLP-04), Barcelona, Spain.Rudi Cilibrasi and Paul M. B. Vitanyi.
2007.
TheGoogle Similarity Distance.
IEEE/ACM Trans-actions on Knowledge and Data Engineering,19(3):370?383.Peter Clark and Phil Harrison.
2009.
RecognizingTextual Entailment with Logical Inference.
In Pro-ceedings of the First Text Analysis Conference (TAC2008).Ido Dagan, Oren Glickman, and Bernardo Magnini.2006.
The PASCAL Recognising Textual Entail-ment Challenge.
In Machine Learning Challenges,volume 3944 of Lecture Notes in Computer Science,pages 177?190.
Springer.Marie-Catherine de Marneffe, Anna N. Rafferty, andChristopher D. Manning.
2008.
Finding contradic-tions in text.
In Proceedings of ACL-08.Christiane Fellbaum.
1998.
WordNet: An ElectronicLexical Database.
MIT Press.Danilo Giampiccolo, Hoa Trang Dang, BernardogMagnini, Ido Dagan, Elena Cabrio, and Bill Dolan.2009.
The Fourth PASCAL Recognizing TextualEntailment Challenge.
In Proceedings of the FirstText Analysis Conference (TAC 2008).Jan Haji?c, Massimiliano Ciaramita, Richard Johans-son, Daisuke Kawahara, Maria Ant`onia Mart?
?, Llu?
?sM`arquez, Adam Meyers, Joakim Nivre, SebastianPad?o, Jan?St?ep?anek, Pavel Stra?n?ak, Mihai Surdeanu,Nianwen Xue, and Yi Zhang.
2009.
The CoNLL-2009 shared task: Syntactic and semantic depen-dencies in multiple languages.
In Proceedings ofthe 13th Conference on Computational Natural Lan-guage Learning, Boulder, CO, USA.Sanda Harabagiu and Andrew Hickl.
2006.
Meth-ods for Using Textual Entailment in Open-DomainQuestion Answering.
In Proceedings of COLING-ACL 2006, pages 905?912, Sydney, Australia.Dekang Lin and Patrick Pantel.
2001.
DIRT - Dis-covery of Inference Rules from Text.
In In Proceed-ings of the ACM SIGKDD Conference on KnowledgeDiscovery and Data Mining.Bill MacCartney, Michel Galley, and Christopher D.Manning.
2008.
A phrase-based alignment modelfor natural language inference.
In Proceedings ofEMNLP 2008.Ryan McDonald, Fernando Pereira, Kiril Ribarov, andJan Hajic.
2005.
Non-Projective Dependency Pars-ing using Spanning Tree Algorithms.
In Proceed-ings of hlt-emnlp 2005, pages 523?530, Vancouver,Canada.Sebastian Pado, Marie-Catherine de Marneffe, BillMacCartney, Anna N. Rafferty, Eric Yeh, and791Christopher D. Manning.
2009.
Deciding en-tailment and contradiction with stochastic and editdistance-based alignment.
In Proceedings of theFirst Text Analysis Conference (TAC 2008).Reda Siblini and Leila Kosseim.
2009.
Using Ontol-ogy Alignment for the TAC RTE Challenge.
In Pro-ceedings of the First Text Analysis Conference (TAC2008).Mihai Surdeanu, Richard Johansson, Adam Meyers,Llu?
?s M`arquez, and Joakim Nivre.
2008.
TheCoNLL-2008 shared task on joint parsing of syntac-tic and semantic dependencies.
In Proceedings ofthe 12th conference on computational natural lan-guage learning (CoNLL-2008), Manchester, UK.Rui Wang and G?unter Neumann.
2007.
Recog-nizing textual entailment using a subsequence ker-nel method.
In Proceedings of the Twenty-SecondAAAI Conference on Artificial Intelligence (AAAI-07), pages 937?942, Vancouver, Canada.Yi Zhang, Rui Wang, and Hans Uszkoreit.
2008.
Hy-brid Learning of Dependency Structures from Het-erogeneous Linguistic Resources.
In Proceedings ofthe Twelfth Conference on Computational NaturalLanguage Learning (CoNLL 2008), pages 198?202,Manchester, UK.792
