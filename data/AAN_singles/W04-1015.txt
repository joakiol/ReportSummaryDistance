Sentence Compression for Automated Subtitling: A Hybrid ApproachVincent Vandeghinste and Yi PanCentre for Computational LinguisticsKatholieke Universiteit LeuvenMaria Theresiastraat 21BE-3000 LeuvenBelgiumvincent.vandeghinste@ccl.kuleuven.ac.be, yi.pan@ccl.kuleuven.ac.beAbstractIn this paper a sentence compression tool is de-scribed.
We describe how an input sentence getsanalysed by using a.o.
a tagger, a shallow parserand a subordinate clause detector, and how, basedon this analysis, several compressed versions of thissentence are generated, each with an associated es-timated probability.
These probabilities were esti-mated from a parallel transcript/subtitle corpus.
Toavoid ungrammatical sentences, the tool also makesuse of a number of rules.
The evaluation was doneon three different pronunciation speeds, averagingsentence reduction rates of 40% to 17%.
The num-ber of reasonable reductions ranges between 32.9%and 51%, depending on the average estimated pro-nunciation speed.1 IntroductionA sentence compression tool has been built withthe purpose of automating subtitle generation forthe deaf and hard-of-hearing.
Verbatim transcrip-tions cannot be presented as the subtitle presentationtime is between 690 and 780 characters per minute,which is more or less 5.5 seconds for two lines (ITC,1997), (Dewulf and Saerens, 2000), while the aver-age speech rate contains a lot more than the equiva-lent of 780 characters per minute.The actual amount of compression needed de-pends on the speed of the speaker and on the amountof time available after the sentence.
In documen-taries, for instance, there are often large silent in-tervals between two sentences, the speech is oftenslower and the speaker is off-screen, so the avail-able presentation time is longer.
When the speakeris off-screen, the synchrony of the subtitles withthe speech is of minor importance.
When subti-tling the news the speech rate is often very highso the amount of reduction needed to allow thesynchronous presentation of subtitles and speech ismuch greater.
The sentence compression rate is aparameter which can be set for each sentence.Note that the sentence compression tool de-scribed in this paper is not a subtitling tool.
Whensubtitling, only when a sentence needs to be re-duced, and the amount of reduction is known, thesentence is sent to the sentence compression tool.So the sentence compression tool is a module of anautomated subtitling tool.
The output of the sen-tence compression tool needs to be processed ac-cording to the subtitling guidelines like (Dewulf andSaerens, 2000), in order to be in the correct lay-outwhich makes it usable for actual subtitling.
Manu-ally post-editing the subtitles will still be required,as for some sentences no automatic compression isgenerated.In real subtitling it often occurs that the sentencesare not compressed, but to keep the subtitles syn-chronized with the speech, some sentences are en-tirely removed.In section 2 we describe the processing of a sen-tence in the sentence compressor, from input to out-put.
In section 3 we describe how the system wasevaluated and the results of the evaluation.
Section4 contains the conclusions.2 From Full Sentence to CompressedSentenceThe sentence compression tool is inspired by (Jing,2001).
Although her goal is text summarizationand not subtitling, her sentence compression systemcould serve this purpose.She uses multiple sources of knowledge on whichher sentence reduction is based.
She makes use ofa corpus of sentences, aligned with human-writtensentence reductions which is similar to the parallelcorpus we use (Vandeghinste and Tjong Kim Sang,2004).
She applies a syntactic parser to analyse thesyntactic structure of the input sentences.
As therewas no syntactic parser available for Dutch (Daele-mans and Strik, 2002), we created ShaRPA (Van-deghinste, submitted), a shallow rule-based parserwhich could give us a shallow parse tree of theinput sentence.
Jing uses several other knowl-edge sources, which are not used (not available forDutch) or not yet used in our system (like WordNet).In figure 1 the processing flow of an input sen-tence is sketched.Input SentenceTaggerAbbreviatorNumbers to DigitsChunkerSubordinate Clause DetectorShallow ParseTreeCompressorWord ReducerCompressedSentenceRemoval,Non?removal,ReductionDatabaseGrammarRulesFigure 1: Sentence Processing Flow ChartFirst we describe how the sentence is analysed(2.1), then we describe how the actual sentencecompression is done (2.2), and after that we de-scribe how words can be reduced for extra compres-sion (2.3).
The final part describes the selection ofthe ouput sentence (2.4).2.1 Sentence AnalysisIn order to apply an accurate sentence compression,we need a syntactic analysis of the input sentence.In a first step, the input sentence gets tagged forparts-of-speech.
Before that, it needs to be trans-formed into a valid input format for the part-of-speech tagger.
The tagger we use is TnT (Brants,2000) , a hidden Markov trigram tagger, which wastrained on the Spoken Dutch Corpus (CGN), Inter-nal Release 6.
The accuracy of TnT trained on CGNis reported to be 96.2% (Oostdijk et al, 2002).In a second step, the sentence is sent to theAbbreviator.
This tool connects to a databaseof common abbreviations, which are often pro-nounced in full words (E.g.
European Union be-comes EU) and replaces the full form with its ab-breviation.
The database can also contain the tagof the abbreviated part (E.g.
the tag for EU isN(eigen,zijd,ev,basis,stan) [E: singular non-neuterproper noun]).In a third step, all numbers which are written inwords in the input are replaced by their form in dig-its.
This is done for all numbers which are smallerthan one million, both for cardinal and ordinal nu-merals.In a fourth step, the sentence is sent to ShaRPa,which will result in a shallow parse-tree of the sen-tence.
The chunking accuracy for noun phrases(NPs) has an F-value of 94.7%, while the chunk-ing accuracy of prepositional phrases (PPs) has anF-value of 95.1% (Vandeghinste, submitted).A last step before the actual sentence compres-sion consists of rule-based clause-detection: Rel-ative phrases (RELP), subordinate clauses (SSUB)and OTI-phrases (OTI is om ... te + infinitive1) aredetected.
The accuracy of these detections was eval-uated on 30 files from the CGN component of read-aloud books, which contained 7880 words.
Theevaluation results are presented in table 1.Type of S Precision Recall F-valueOTI 71.43% 65.22% 68.18%RELP 69.66% 68.89% 69.27%SSUB 56.83% 60.77% 58.74%Table 1: Clause Detection AccuracyThe errors are mainly due to a wrong analysisof coordinating conjunctions, which is not only theweak point in the clause-detection module, but alsoin ShaRPa.
A full parse is needed to accuratelysolve this problem.2.2 Sentence CompressionFor each chunk or clause detected in the previoussteps, the probabilities of removal, non-removal andreduction are estimated.
This is described in moredetail in 2.2.1.Besides the statistical component in the compres-sion, there are also a number of rules in the com-pression program, which are described in more de-tail in 2.2.2.The way the statistical component and the rule-based component are combined is described in2.2.3.1There is no equivalent construction in English.
OTI is aVP-selecting complementizer.2.2.1 Use of StatisticsChunk and clause removal, non-removal and reduc-tion probabilities are estimated from the frequenciesof removal, non-removal and reduction of certaintypes of chunks and clauses in the parallel corpus.The parallel corpus consists of transcripts of tele-vision programs on the one hand and the subti-tles of these television programs on the other hand.A detailed description of how the parallel corpuswas collected, and how the sentences and chunkswere aligned is given in (Vandeghinste and TjongKim Sang, 2004).All sentences in the source corpus (transcripts)and the target corpus (subtitles) are analysed in thesame way as described in section 2.1, and are chunkaligned.
The chunk alignment accuracy is about95% (F-value).We estimated the removal, non-removal and re-duction probabilities for the chunks of the types NP,PP, adjectival phrase (AP), SSUB, RELP, and OTI,based on their chunk removal, non-removal and re-duction frequencies.For the tokens not belonging to either of thesetypes, the removal and non-removal probabilitieswere estimated based on the part-of-speech tag forthose words.
A reduced tagset was used, as the orig-inal CGN-tagset (Van Eynde, 2004) was too fine-grained and would lead to a multiplication of thenumber of rules which are now used in ShaRPa.
Thefirst step in SharPa consists of this reduction.For the PPs, the SSUBs and the RELPs, as wellas for the adverbs, the chunk/tag information wasconsidered as not fine-grained enough, so the es-timation of the removal, non-removal and reduc-tion probabilities for these types are based on thefirst word of those phrases/clauses and the reduc-tion, removal and non-removal probabilities of suchphrases in the parallel corpus, as the first words ofthese chunk-types are almost always the heads ofthe chunk.
This allows for instance to make thedistinction between several adverbs in one sentence,so they do not all have the same removal and non-removal probabilities.
A disadvantage is that thisapproach leads to sparse data concerning the lessfrequent adverbs, for which a default value (averageover all adverbs) will be employed.An example : A noun phrase.de grootste Belgische bank[E: the largest Belgian bank]After tagging and chunking the sentence and af-ter detecting subordinate clauses, for every non-terminal node in the shallow parse tree we retrievethe measure of removal (X), of non-removal (=) andof reduction2 (   ).
For the terminal nodes, only themeasures of removal and of non-removal are used.NP= 0.54X 0.27 0.05DET= 0.68X 0.28deADJ= 0.56X 0.35groot-steADJ= 0.56X 0.35Bel-gischeN= 0.65X 0.26bankFor every combination the probability estimateis calculated.
So if we generate all possible com-pressions (including no compression), the phrasede grootste Belgische bank will get theprobability estimate 		fiffflffi  .
For the phrase de Belgischebank the probability estimate is !
fl"#$	$	%ff& ## , and so on for theother alternatives.In this way, the probability estimate of all possi-ble alternatives is calculated.2.2.2 Use of RulesAs the statistical information allows the generationof ungrammatical sentences, a number of rules wereadded to avoid generating such sentences.
The pro-cedure keeps the necessary tokens for each kind ofnode.
The rules were built in a bootstrapping man-nerIn some of these rules, this procedure is appliedrecursively.
These are the rules implemented in oursystem:' If a node is of type SSUB or RELP, keep thefirst word.'
If a node is of type S, SSUB or RELP, keep?
the verbs.
If there are prepositions whichare particles of the verb, keep the prepo-sitions.
If there is a prepositional phrasewhich has a preposition which is in thecomplements list of the verb, keep thenecessary tokens3 of that prepositionalphrase.2These measures are estimated probabilities and do not needto add up to 1, because in the parallel training corpus, some-times a match was detected with a chunk which was not a re-duction of the source chunk or which was not identical to thesource chunk: the chunk could be paraphrased, or even havebecome longer.3Recursive use of the rules?
each token which is in the list of nega-tive words.
These words are kept to avoidaltering the meaning of the sentence bydropping words which negate the mean-ing.?
the necessary tokens of the te + infinitives(TI).?
the conjunctions.?
the necessary tokens of each NP.?
the numerals.?
the adverbially used adjectives.'
If a node is of type NP, keep?
each noun.?
each nominalised adjectival phrase.?
each token which is in the list of negativewords.?
the determiners.?
the numerals.?
the indefinite prenominal pronouns.'
If a node is of type PP, keep?
the preposition.?
the determiners.?
the necessary tokens of the NPs.'
If the node is of type adjectival phrase, keep?
the head of the adjectival phrase.?
the prenominal numerals.?
each word which is in the list of negativewords.'
If the node is of type OTI, keep?
the verbs.?
the te + infinitives.'
If the node is of type TI, keep the node.'
If the node is a time phrase4, keep it.These rules are chosen because in tests on earlierversions of the system, using a different test set, un-grammatical output was generated.
By using theserules the output should be grammatical, providedthat the input sentence was analysed correctly.4A time phrase, as defined in ShaRPa is used for specialphrases, like dates, times, etc.
E.g.
27 september 1998, kwartvoor drie [E: quarter to three].2.2.3 Combining Statistics and RulesIn the current version of the system, in a first stageall variations on a sentence are generated in the sta-tistical part, and they are ranked according to theirprobability.
In a second stage, all ungrammaticalsentences are (or should be) filtered out, so the onlysentence alternatives which remain should be gram-matical ones.This is true, only if tagging as well as chunkingwere correct.
If errors are made on these levels, thegeneration of an ungrammatical alternative is stillpossible.For efficiency reasons, a future version of the sys-tem should combine the rules and statistics in onestage, so that the statistical module only generatesgrammatically valid sentence alternatives, althoughthere is no effect on correctness, as the resulting sen-tence alternatives would be the same if statistics andrules were better integrated.2.3 Word ReductionAfter the generation of several grammatical reduc-tions, which are ordered according to their prob-ability estimated by the product of the removal,non-removal and reduction probabilities of all itschunks, for every word in every compressed alterna-tive of the sentence it is checked whether the wordcan be reduced.The words are sent to a WordSplitter-module,which takes a word as its input and checks if it isa compound by trying to split it up in two parts:the modifier and the head.
This is done by lexiconlookup of both parts.
If this is possible, it is checkedwhether the modifier and the head can be recom-pounded according to the word formation rules forDutch (Booij and van Santen, 1995), (Haeseryn etal., 1997).
This is done by sending the modifierand the head to a WordBuilding-module, which isdescribed in more detail in (Vandeghinste, 2002).This is a hybrid module combining the compound-ing rules with statistical information about the fre-quency of compounds with the samen head, the fre-quency of compounds with the same modifier, andthe number of different compounds with the samehead.Only if this module allows the recomposition ofthe modifier and the head, the word can be consid-ered to be a compound, and it can potentially be re-duced to its head, removing the modifier.If the words occur in a database which containsa list of compounds which should not be split up,the word cannot be reduced.
For example, theword voetbal [E: football] can be split up and re-compounded according to the word formation rulesfor Dutch (voet [E: foot] and bal [E: ball]), butwe should not replace the word voetbal with theword bal if we want an accurate compression, withthe same meaning as the original sentence, as thiswould alter the meaning of the sentence too much.The word voetbal has (at least) two different mean-ings: soccer and the ball with which soccer isplayed.
Reducing it to bal would only keep the sec-ond meaning.
The word gevangenisstraf [E: prisonsentence] can be split up and recompounded (gevan-genis [E: prison] and straf [E: punishment]).
Wecan replace the word gevangenisstraf by the wordstraf.
This would still alter the meaning of the sen-tence, but not to the same amount as it would havebeen altered in the case of the word voetbal.2.4 Selection of the Compressed SentenceApplying all the steps described in the previous sec-tions results in an ordered list of sentence alterna-tives, which are supposedly grammatically correct.When word reduction was possible, the word-reduced alternative is inserted in this list, just afterits full-words equivalent.The first sentence in this list with a length smallerthan the maximal length (depending on the availablepresentation time) is selected.In a future version of the system, the word reduc-tion information can be integrated in a better waywith the rest of the module, by combining the proba-bility of reduction/non-reduction of a word with theprobability of the sentence alternative.
The reduc-tion probability of a word would then play its rolein the estimated probability of the compressed sen-tence alternative containing this reduced word.3 EvaluationThe evaluation of a sentence compression module isnot an easy task.
The output of the system needs tobe judged manually for its accuracy.
This is a verytime consuming task.
Unlike (Jing, 2001), we donot compare the system results with the human sen-tence reductions.
Jing reports a succes rate of 81.3%for her program, but this measure is calculated as thepercentage of decisions on which the system agreeswith the decisions taken by the human summarizer.This means that 81.3% of all system decisions arecorrect, but does not say anything about how manysentences are correctly reduced.In our evaluation we do not expect the compres-sor to simulate human summarizer behaviour.
Theresults presented here are calculated on the sentencelevel: the amount of valid reduced sentences, be-ing those reductions which are judged by humanraters to be accurate reductions: grammatical sen-tences with (more or less) the same meaning as theinput sentence, taking into account the meaning ofthe previous sentences on the same topic.3.1 MethodTo estimate the available number of characters in asubtitle, it is necessary to estimate the average pro-nunciation time of the input sentence, provided thatit is unknown.
We estimate sentence duration bycounting the number of syllables in a sentence andmultiplying this with the average duration per sylla-ble (ASD).The ASD for Dutch is reported to be about 177ms (Koopmans-van Beinum and van Donzel, 1996),which is the syllable speed without including pausesbetween words or sentences.We did some similar research on CGN using theASD as a unit of analysis, while we consider boththe situation without pauses and the situation withpauses.
Results of this research are presented in ta-ble 2.ASD no pauses pauses includedAll files 186 237One speaker 185 239Read-aloud 188 256Table 2: Average Syllable Duration (ms)We extract the word duration from all the filesin each component of CGN.
A description of thecomponents can be found in (Oostdijk et al, 2002).We created a syllable counter for Dutch words,which we evaluated on all words in the CGN lexi-con.
For 98.3% of all words in the lexicon, syllablesare counted correctly.
Most errors occur in very lowfrequency words or in foreign words.By combining word duration information and thenumber of syllables we can calculate the averagespeaking speed.We evaluated sentence compression in three dif-ferent conditions:The fastest ASD in our ASD-research was 185 ms(one speaker, no pauses), which was used for Con-dition A.
We consider this ASD as the maximumspeed for Dutch.The slowest ASD (256 ms) was used for Condi-tion C. We consider this ASD to be the minimumspeed for Dutch.We created a testset of 100 sentences mainly fo-cused on news broadcasts in which we use the realpronunciation time of each sentence in the testsetwhich results in an ASD of 192ms.
This ASD wasused for Condition B, and is considered as the realspeed for news broadcasts.We created a testset of 300 sentences, of which200 were taken from transcripts of television news,and 100 were taken from the ?broadcast news?
com-ponent of CGN.To evaluate the compressor, we estimate the du-ration of each sentence, by counting the number ofsyllables and multiplying that number with the ASDfor that condition.
This leads to an estimated pro-nunciation time.
This is converted to the number ofcharacters, which is available for the subtitle.We know the average time for subtitle presenta-tion at the VRT (Flemish Broadcasting Coorpora-tion) is 70 characters in 6 seconds, which gives usan average of 11.67 characters per second.So, for example, if we have a test sentence of15 syllables, this gives us an estimated pronunci-ation time of 2.775 seconds (15 syllables 	 185ms/syllable) in condition A.
When converting this tothe available characters, we multiply 2.775 secondsby 11.67 characters/second, resulting in 32 (2.775s11.67 ch/s = 32.4 ch) available characters.In condition B (considered to be real-time) forthe part of the test-sentences coming from CGN,the pronunciation time was not estimated, as it wasavailable in CGN.3.2 ResultsThe results of our experiments on the sentence com-pression module are presented in table 3.Condition A B CNo output (0) 44.33% 41.67% 15.67%Avg Syllable speed(msec/syllable) 185 192 256Avg Reduction Rate 39.93% 37.65% 16.93%Interrater Agreement 86.2% 86.9% 91.7%Accurate Compr.
4.8% 8.0% 28.9%+/- Acc.
Compr.
28.1% 26.3% 22.1%Reasonable Compr.
32.9% 34.3% 51%Table 3: Sentence Compression Evaluation on theSentence LevelThe sentence compressor does not generate out-put for all test sentences in all conditions: In thosecases where no output was generated, the sentencecompressor was not able to generate a sentencealternative which was shorter than the maximumnumber of characters available for that sentence.The cases where no output is generated are not con-sidered as errors because it is often impossible, evenfor humans, to reduce a sentence by about 40%,without changing the content too much.
The amountof test sentences where no output was generatedis presented in table 3.
The high percentage ofsentences where no output was generated in condi-tions A and B is most probably due to the fact thatthe compression rates in these conditions are higherthan they would be in a real life application.
Condi-tion C seems to be closer to the real life compressionrate needed in subtitling.Each condition has an average reduction rate overthe 300 test sentences.
This reduction rate is basedon the available amount of characters in the subtitleand the number of characters in the source sentence.A rater scores a compressed sentence as + whenit is grammatically correct and semantically equiva-lent to the input sentence.
No essential informationshould be missing.
A sentence is scored as +/-when it is grammatically correct, but some infor-mation is missing, but is clear from the context inwhich the sentence occurs.
All other compressedsentences get scored as -.Each sentence is evaluated by two raters.
Thelowest score of the two raters is the score which thesentence gets.
Interrater agreement is calculated ona 2 point score: if both raters score a sentence as +or +/- or both raters score a sentence as -, it is con-sidered an agreed judgement.
Interrater agreementresults are presented in table 3.Sentence compression results are presented in ta-ble 3.
We consider both the + and +/- results asreasonable compressions.The resulting percentages of reasonable compres-sions seem to be rather low, but one should keepin mind that these results are based on the sentencelevel.
One little mistake in one sentence can leadto an inaccurate compression, although the majorpart of the decisions taken in the compression pro-cess can still be correct.
This makes it very hardto compare our results to the results presented byJing (2001), but we presented our results on sen-tence evaluations as it gives a clearer idea on howwell the system would actually perform in a real lifeapplication.As we do not try to immitate human subtitling be-haviour, but try to develop an equivalent approach,our system is not evaluated in the same way as thesystem deviced by Jing.4 ConclusionWe have described a hybrid approach to sentencecompression which seems to work in general.
Thecombination of using statistics and filtering out in-valid results because they are ungrammatical by us-ing a set of rules is a feasible way for automatedsentence compression.The way of combining the probability-estimatesof chunk removal to get a ranking in the generatedsentence alternatives is working reasonably well,but could be improved by using more fine-grainedchunk types for data collection.A full syntactic analysis of the input sentencewould lead to better results, as the current sentenceanalysis tools have one very weak point: the han-dling of coordinating conjunction, which leads tochunking errors, both in the input sentence as in theprocessing of the used parallel corpus.
This leads tomisestimations of the compression probabilities andcreates noise in the behaviour of our system.Making use of semantics would most probablylead to better results, but a semantic lexicon andsemantic analysis tools are not available for Dutch,and creating them would be out of the scope of thecurrent project.In future research we will check the effects ofimproved word-reduction modules, as word reduc-tions often seem to lead to inaccurate compres-sions.
Leaving out the word-reduction modulewould probably lead to an even bigger amount ofno output-cases.
This will also be checked in futureresearch.5 AcknowledgementsResearch funded by IWT (Institute for Innovationin Science and Technology) in the STWW pro-gram, project ATraNoS (Automatic Transcriptionand Normalisation of Speech).
For more informa-tion visit http://atranos.esat.kuleuven.ac.be/.We would like to thank Ineke Schuurman for rat-ing the reduced sentences.ReferencesG.
Booij and A. van Santen.
1995.
Morfologie.
Dewoordstructuur van het Nederlands.
AmsterdamUniversity Press, Amsterdam, Netherlands.T.
Brants.
2000.
TnT - A Statistical Part-of-SpeechTagger.
Published online at http://www.coli.uni-sb.de/thorsten/tnt.W.
Daelemans and H. Strik.
2002.
Het Neder-lands in Taal- en Spraaktechnologie: Prioriteitenvoor Basisvoorzieningen.
Technical report, Ne-derlandse Taalunie.B.
Dewulf and G. Saerens.
2000.
StijlboekTeletekst Ondertiteling.
Technical report, VRT,Brussel.
Internal Subtitling Guidelines.W.
Haeseryn, G. Geerts, J de Rooij, andM.
van den Toorn.
1997.
Algemene Neder-landse Spraakkunst.
Martinus Nijhoff Uitgevers,Groningen.ITC.
1997.
Guidance on standardsfor subtitling.
Technical report,ITC.
Online at http://www.itc.org.uk/codes guidelines/broadcasting/tv/sub signaudio/subtitling stnds/.H.
Jing.
2001.
Cut-and-Paste Text Summarization.Ph.D.
thesis, Columbia University.F.J.
Koopmans-van Beinum and M.E.
van Donzel.1996.
Relationship Between Discourse Structureand Dynamic Speech Rate.
In Proceedings IC-SLP 1996, Philadelphia, USA.N.
Oostdijk, W. Goedertier, F. Van Eynde, L. Boves,J.P.
Marters, M. Moortgat, and H. Baayen.
2002.Experiences from the Spoken Dutch Corpus.
InProceedings of LREC 2002, volume I, pages 340?347, Paris.
ELRA.F.
Van Eynde.
2004.
Part-of-speech Taggingen Lemmatisering.
Internal manual of Cor-pus Gesproken Nederlands, published online athttp://www.ccl.kuleuven.ac.be/Papers/POSmanual febr2004.pdf.V.
Vandeghinste and E. Tjong Kim Sang.
2004.
Us-ing a parallel transcript/subtitle corpus for sen-tence compression.
In Proceedings of LREC2004, Paris.
ELRA.V.
Vandeghinste.
2002.
Lexicon optimization:Maximizing lexical coverage in speech recogni-tion through automated compounding.
In Pro-ceedings of LREC 2002, volume IV, pages 1270?1276, Paris.
ELRA.V.
Vandeghinste.
submitted.
ShaRPa: ShallowRule-based Parsing, focused on Dutch.
In Pro-ceedings of CLIN 2003.
