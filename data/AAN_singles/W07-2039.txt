Proceedings of the 4th International Workshop on Semantic Evaluations (SemEval-2007), pages 187?190,Prague, June 2007. c?2007 Association for Computational LinguisticsILK: Machine learning of semantic relations with shallow featuresand almost no dataIris HendrickxCNTS / Language Technology GroupUversity of Antwerp,Universiteitsplein 12610 Wilrijk, Belgiumiris.hendrickx@ua.ac.beRoser Morante, Caroline Sporleder,Antal van den BoschILK / Communication and Information SciencesTilburg University, P.O.
Box 90153,5000 LE Tilburg, The Netherlands{R.Morante,C.Sporleder,Antal.vdnBosch}@uvt.nlAbstractThis paper summarizes our approach to theSemeval 2007 shared task on ?Classifica-tion of Semantic Relations between Nom-inals?.
Our overall strategy is to developmachine-learning classifiers making use ofa few easily computable and effective fea-tures, selected independently for each clas-sifier in wrapper experiments.
We train twotypes of classifiers for each of the seven re-lations: with and without WordNet informa-tion.1 IntroductionWe interpret the task of determining semantic rela-tions between nominals as a classification problemthat can be solved, per relation, by machine learningalgorithms.
We aim at using straightforward featuresthat are easy to compute and relevant to preferablyall of the seven relations central to the task.The starting conditions of the task provide us witha very small amount of training data, which furtherstresses the need for robust, generalizable features,that generalize beyond surface words.
We there-fore hypothesize that generic information on the lex-ical semantics of the entities involved in the rela-tion is crucial.
We developed two systems, basedon two sources of semantic information.
Since theentities in the provided data were word-sense dis-ambiguated, an obvious way to model their lexicalsemantics was by utilizing WordNet3.0 (Fellbaum,1998) (WN).
One of the systems followed this route.We also entered a second system, which did notrely on WN but instead made use of automaticallygenerated semantic clusters (Decadt and Daelemans,2004) to model the semantic classes of the entities.For both systems we trained seven binary clas-sifiers; one for each relation.
From a pool of eas-ily computable features, we selected feature subsetsfor each classifier in a number of wrapper exper-iments, i.e.
repeated cross-validation experimentson the training set to test out subset selections sys-tematically.
Along with feature subsets we alsochose the machine-learning method independentlyfor each classifier.Section 2 presents the system description, Sec-tion 3, the results, and Section 4, the conclusions.2 System DescriptionThe development of the system consists of a prepro-cessing phase to extract the features, and the classi-fication phase.2.1 PreprocessingEach sentence is preprocessed automatically in thefollowing steps.
First, the sentence is tokenized witha rule-based tokenizer.
Next a part-of-speech tag-ger and text chunker that use the memory-based tag-ger MBT (Daelemans et al, 1996) produces part-of-speech tags and NP chunk labels for each token.Then a memory-based shallow parser predicts gram-matical relations between verbs and NP chunks suchas subject, object or modifier (Buchholz, 2002).
Thetagger, chunker and parser were all trained on theWSJ Corpus (Marcus et al, 1993).
We also usea memory-based lemmatizer (Van den Bosch et al,1996) trained on Celex (Baayen et al, 1993) to pre-dict the lemma of each word.187The features extracted are of three types: seman-tic, lexical, and morpho-syntactic.
The features thatapply to the entities in a relation (e1,e2) are extractedfor term 1 (t1) and term 2 (t2) of the relation, wheret1 is the first term in the relation name, and t2 is thesecond term.
For example, in the relation CAUSE?EFFECT, t1 is CAUSE and t2 is EFFECT.The semantic features are the following:WN semantic class of t1 and t2.
The WN seman-tic class of each entity in the relation.
For the WN-based system, we determined the semantic class ofthe entities on the basis of the lexicographer filenumbers (LFN) in WN3.0.
The LFN are encoded inthe synset number provided in the annotation of thedata.
For nouns there are 25 file numbers that corre-spond to suitably abstract semantic classes, namely:noun.Tops(top concepts for nouns), act, animal, artifact, at-tribute, body, cognition, communication event, feeling, food,group, location, motive, object, person, phenomenon, plant,possession, process, quantity, relation, shape, state, substance,time.Is container (is C).
Exclusively for theCONTENT?CONTAINER relation we furthermoreincluded two binary features that test whether thetwo entities in the relation are hyponyms of thesynset container in WN.
For the PART?WHOLErelation we also experimented with binary featuresexpressing whether the two entities in the relationhave some type of meronym and holonym relation,but these features did not prove to be predictive.Cluster class of t1 and t2.
A cluster class iden-tifier for each entity in the relation.
This informa-tion is drawn from automatically generated clustersof semantically similar nouns (Decadt and Daele-mans, 2004) generated on the British National Cor-pus (Clear, 1993).
The corpus was first prepro-cessed by a lemmatizer and the memory-based shal-low parser, and the found verb?object relations wereused to cluster nouns in groups.
We used the top-5000 lemmatized nouns, that are clustered into 250groups.
This is an example of two of these clusters:?
{can pot basin tray glass container bottle tin pan mug cupjar bowl bucket plate jug vase kettle}?
{booth restaurant bath kitchen hallway toilet bedroomhall suite bathroom interior lounge shower compartmentoven lavatory room}The lexical features are the following:Lemma of t1 and t2 (lem1, lem2).
The lemmas ofthe entities involved in the relation.
In case an entityconsisted of multiple words (e.g.
storage room) weuse the lemma of the head noun (i.e.
room).Main verb (verb).
The main verb of the sentencein which the entities involved in the relation appear,as predicted by the shallow parser.The morpho-syntactic features are:GramRel (gr1, gr2).
The grammatical relationtags of the entities.Suffixes of t1 and t2 (suf1, suf2).
The suffixes ofthe entity lemmas.
We implemented a rule-basedsuffix guesser, which determines whether the nounsinvolved in the relation end in a derivational suffix,such as -ee, -ment etc.
Suffixes often provide cuesfor semantic properties of the entities.
For exam-ple, the suffix -ee usually indicates animate (and typ-ically human) referents (e.g.
detainee etc.
), whereas(-ment) points at abstract entities (e.g.
statement).While the features were selected independentlyfor all relations, the seven classifiers in the WN-based system all make use of the WN semantic classfeatures; in the system that did not use WN, theseven classifiers make use of the cluster class fea-tures instead.2.2 ClassificationWe experimented with several machine learningframeworks and different feature (sub-)sets.
Forrapid testing of different learners and feature sets,and given the size of the training data (140 exam-ples for each relation), we made use of the Weka ma-chine learning software1 (Witten and Frank, 1999).We systematically tested the following algorithms:NaiveBayes (NB) (Langley et al, 1992), BayesNet(BN) (Cooper and Herskovits, 1992), J48 (Quinlan,1993), Jrip (Cohen, 1995), IB1 and IBk (Aha et al,1991), LWL (Atkeson et al, 1997), and Decision-Stumps (DS) (Iba and Langley, 1992), all with de-fault algorithm settings.The classifiers for all seven relations were opti-mized independently in a number of 10-fold cross-validation (CV) experiments on the provided train-1http://www.cs.waikato.ac.nz/ml/weka/188ing sets.
The feature sets and learning algorithmswhich were found to obtain the highest accuraciesfor each relation were then used when applying theclassifiers to the unseen test data.The classifiers of the cluster-based system (A) alluse the two cluster class features.
The other se-lected features and the chosen algorithms (CL) aredisplayed in Table 1.
Knowledge of the identity ofthe lemmas was found to be beneficial for all clas-sifiers.
With respect to the machine learning frame-work, Naive Bayes was selected most frequently.Relation CL lem1 lem2 verb gr1 gr2 suf1 suf2Cause-Effect DS + + + + + + +Instr-Agency LWL + + + + +Product-Producer NB + + + + + + +Origin-Entity IBk + + + + + +Theme-Tool NB + + + + +Part-Whole NB + + + + + +Content-Container NB + + + + + +Table 1: The final selected algorithms and featuresfor each relation by the cluster-based system (A).The classifiers of the WN-based system (B) alluse at least the WN semantic class features.
Ta-ble 2 shows the other selected features and algorithmfor each relation.
None of the classifiers use all thefeatures.
For the part-whole relation no extra fea-tures besides the WN class are selected.
Also theclassifiers for the relations cause-effect and content-container only use two additional features.
The listof best found algorithms shows that ?like with thecluster-based system?
a Bayesian approach is fa-vorable, as it is selected in four of seven cases.Relation CL lem1 lem2 verb gr1 gr2 suf1 suf2 is CCause-Effect BN + +Instr-Agency NB + + +Product-Producer IB1 + + + +Origin-Entity IBk + + + + +Theme-Tool NB + + + + + +Part-Whole J48Content-Container BN + +Table 2: The final selected algorithms and featuresfor each relation by the WN-based system (B).
(is Cis the CONTENT-CONTAINER specific feature.
)3 ResultsIn Table 3 we first present the best results computedon the training set using 10-fold CV for the cluster-based system (A) and the WN-based system (B).These results are generally higher than the officialtest set results, shown in Tables 4 and 5, possiblyshowing a certain amount of overfitting on the train-ing sets.Relation A BCause-Effect 56.4 72.9Instrument-Agency 71.4 75.7Product-Producer 65.0 67.9Origin-Entity 70.7 78.6Theme-Tool 75.7 79.3Part-Whole 65.7 73.6Content-Container 70.0 75.4Avg 67.9 74.8Table 3: Average accuracy on the training set com-puted in 10-fold CV experiments of the cluster-based system (A) and the WN-based system (B).The official scores on the test set are computedby the task organizers: accuracy, precision, recalland F1 score.
Table 4 presents the results of thecluster-based system.
Table 5 presents the resultsof the WN-based system.
(The column Total showsthe number of instances in the test set.)
Markable isthe high accuracy for the PART-WHOLE relation asthe classifier was only trained on two features cod-ing the WN classes.A4 Pre Rec F Acc TotalCause?Effect 53.3 97.6 69.0 55.0 80Instrument?Agency 56.1 60.5 58.2 57.7 78Product?Producer 69.1 75.8 72.3 61.3 93Origin?Entity 60.7 47.2 53.1 63.0 81Theme?Tool 64.5 69.0 66.7 71.8 71Part?Whole 48.4 57.7 52.6 62.5 72Content?Container 71.4 78.9 75.0 73.0 74Avg 60.5 69.5 63.8 63.5 78.4Table 4: Test scores for the seven relations of thecluster-based system trained on 140 examples (A4).The system using all training data with WordNetfeatures, B4 (Table 5), performs better in terms of F-score on six out of the seven subtasks as comparedto the system that does not use the WordNet featuresbut the semantic cluster information instead, A4 (Ta-ble 4).
This is largely due to a lower precision of theA4 system.
The WordNet features appear to be di-rectly responsible for a relatively higher precision.In contrast, the semantic cluster features of sys-tem A sometimes boost recall.
A4?s recall on the189B4 Pre Rec F Acc TotalCause?Effect 69.0 70.7 69.9 68.8 80Instrument?Agency 69.8 78.9 74.1 73.1 78Product?Producer 79.7 75.8 77.7 71.0 93Origin?Entity 71.0 61.1 65.7 71.6 81Theme?Tool 69.0 69.0 69.0 74.6 71Part?Whole 73.1 73.1 73.1 80.6 72Content?Container 78.1 65.8 71.4 73.0 74Avg 72.8 70.6 71.5 73.2 78.4Table 5: Test scores for the seven relations of theWN-based system trained on 140 examples (B4).CAUSE?EFFECT relation is 97.6% (the classifier pre-dicts the class ?true?
for 75 of the 80 examples),and on CONTENT?CONTAINER the system attains78.9%, markedly better than B4.4 ConclusionWe have shown that a machine learning approach us-ing shallow and easily computable features performsquite well on this task.
The system using Word-Net features based on the provided disambiguatedword senses outperforms the cluster-based system.It would be interesting to compare both systems to amore realistic WN-based system that uses predictedword senses by a Word Sense Disambiguation sys-tem.However we end by noting that the amount oftraining and test data in this shared task should beconsidered too small to base any reliable conclu-sions on.
In a realistic scenario (e.g.
when high-precision relation classification would be needed asa component of a question-answering system), moretraining material would have been gathered, and theexamples would not have been seeded by a limitednumber of queries ?
especially the negative exam-ples are very artificial now due to their similarity tothe positive cases, and the fact that they are down-sampled very unrealistically.
Rather, the focus of thetask should be on detecting positive instances of therelations in vast amounts of text (i.e.
vast amounts ofimplicit negative examples).
Positive training exam-ples should be as randomly sampled from raw textas possible.
The seven relations are common enoughto warrant a focused effort to annotate a reasonableamount of randomly selected text, gathering severalhundreds of positive cases of each relation.ReferencesD.
W. Aha, D. Kibler, M. Albert.
1991.
Instance-basedlearning algorithms.
Machine Learning, 6:37?66.C.
Atkeson, A. Moore, S. Schaal.
1997.
Locallyweighted learning.
Artificial Intelligence Review,11(1?5):11?73.R.
H. Baayen, R. Piepenbrock, H. van Rijn.
1993.
TheCELEX lexical data base on CD-ROM.
LinguisticData Consortium, Philadelphia, PA.S.
Buchholz.
2002.
Memory-Based Grammatical Rela-tion Finding.
PhD thesis, University of Tilburg.J.
H. Clear.
1993.
The British national corpus.
MITPress, Cambridge, MA, USA.W.
Cohen.
1995.
Fast effective rule induction.
In Pro-ceedings of the 12th International Conference on Ma-chine Learning, 115?123.
Morgan Kaufmann.G.
F. Cooper, E. Herskovits.
1992.
A bayesian methodfor the induction of probabilistic networks from data.Machine Learning, 9(4):309?347.W.
Daelemans, J. Zavrel, P. Berck, S. Gillis.
1996.Mbt: A memory-based part of speech tagger genera-tor.
In Proceedings of the 4th ACL/SIGDAT Workshopon Very Large Corpora, 14?27.B.
Decadt, W. Daelemans.
2004.
Verb classification -machine learning experiments in classifying verbs intosemantic classes.
In Proceedings of the LREC 2004Workshop Beyond Named Entity Recognition: Seman-tic Labeling for NLP Tasks, 25?30, Lisbon, Portugal.C.
Fellbaum, ed.
1998.
WordNet: An ElectronicDatabase.
MIT Press, Cambridge, MA.W.
Iba, P. Langley.
1992.
Induction of one-level decisiontrees.
Proceedings of the Ninth International Confer-ence on Machine Learning, 233?240.P.
Langley, W. Iba, K. Thompson.
1992.
An analysis ofBayesian classifiers.
In Proceedings of the Tenth An-nual Conference on Artificial Intelligence, 223?228.AAAI Press and MIT Press.M.
Marcus, S. Santorini, M. Marcinkiewicz.
1993.Building a Large Annotated Corpus of English:the Penn Treebank.
Computational Linguistics,19(2):313?330.J.
Quinlan.
1993.
C4.5: Programs for Machine Learn-ing.
Morgan Kaufmann, San Mateo, CA.A.
Van den Bosch, W. Daelemans, A. Weijters.
1996.Morphological analysis as classification: an inductive-learning approach.
In K. Oflazer, H. Somers, eds.,Proceedings of the Second International Conferenceon New Methods in Natural Language Processing,NeMLaP-2, Ankara, Turkey, 79?89.I.
H. Witten, E. Frank.
1999.
Data Mining: PracticalMachine Learning Tools and Techniques with Java Im-plementations.
Morgan Kaufman, San Francisco, CA.190
