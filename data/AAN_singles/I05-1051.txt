Phrase-Based Statistical Machine Translation:A Level of Detail ApproachHendra Setiawan1,2, Haizhou Li1, Min Zhang1, and Beng Chin Ooi21 Institute for Infocomm Research,21 Heng Mui Keng Terrace,Singapore 119613{stuhs, hli, mzhang}@i2r.a-star.edu.sg2 School of Computing,National University of Singapore,Singapore 117543{hendrase, ooibc}@comp.nus.edu.sgAbstract.
The merit of phrase-based statistical machine translation isoften reduced by the complexity to construct it.
In this paper, we ad-dress some issues in phrase-based statistical machine translation, namely:the size of the phrase translation table, the use of underlying transla-tion model probability and the length of the phrase unit.
We presentLevel-Of-Detail (LOD) approach, an agglomerative approach for learn-ing phrase-level alignment.
Our experiments show that LOD approachsignificantly improves the performance of the word-based approach.
LODdemonstrates a clear advantage that the phrase translation table growsonly sub-linearly over the maximum phrase length, while having a per-formance comparable to those of other phrase-based approaches.1 IntroductionEarly approach to statistical machine translation relies on the word-based trans-lation model to describe the translation process [1].
However, the underlying as-sumption of word-to-word translation often fails to capture all properties of thelanguage, i.e.
the existence of the phrase where a group of words often functiontogether as a unit.
Many researchers have proposed to move from the word-basedto the phrase-based translation model [2] [3] [4].
A phrase-based approach offersmany advantages as a phrase translation captures word context and local re-ordering inherently [3].
It has become popular in statistical machine translationapplications.There are typically two groups of approaches to constructing the phrase-based model.
The first group learns phrase translation directly from the sen-tence pair.
It learns both word and phrase units simultaneously.
Although theseapproaches appear intuitive, it usually suffers from a prohibitive computationalcost.
It might have to consider all possible multi-word sequences as phrase can-didates and all possible pairings as phrase translations at the same time.R.
Dale et al (Eds.
): IJCNLP 2005, LNAI 3651, pp.
576?587, 2005.c?
Springer-Verlag Berlin Heidelberg 2005Phrase-Based Statistical Machine Translation: A Level of Detail Approach 577The second group of approaches learns phrase translations through word-levelalignment: alignment template [2] and projection extension [6], just to name afew.
In general, these approaches take the word-level alignment, a by-product ofthe word-based translation model, as their input and then utilize a heuristic mea-surement to learn the phrase translation.
The heuristic measurement containsall possible configurations of word-level alignment on a phrase translation.It is noted that the underlying word-level alignment is just an approximationto the exact alignment.
The approximation is reflected by a probability producedby the word-based translation model.
The majority of approaches do not makeuse of this probability, whereas it may provide a valuable clue leading to a betterphrase translation from a statistical point of view.
Koehn, et.
al [8] compared therepresentative of both groups and reported that learning phrase translation usinga simple heuristic from word alignment yields a better translation performancethan learning phrase translation directly from the sentence pair.Many approaches try to learn all phrase translations in one step, either di-rectly from the sentence pair or through word alignment.
As a result, they mayencounter a huge amount of phrase translation candidates at once.
Usually, theylimit the maximum phrase length to reduce the choice of candidates.
Althoughthis method is sufficient to satisfy the computational requirement, it comes withthe cost of not finding the good phrases longer than the imposed limit.
Addition-ally, to reduce the candidates, those approaches use a threshold to separate goodphrase translation from the rest.
The threshold is ad-hoc and often not capableof making a clear separation.
Therefore, the use of threshold often comes withthe cost of the inclusion of undesired phrase translations and the absence of goodphrase translations in the phrase translation table.
The cost may be reflectedfrom the size of the phrase translation table that often grows almost linearly overthe phrase length limit [6][8].
The growth implies a non-intuitive behavior: twophrases with different length introduce an equal number of additional entries tothe phrase translation table.
As longer phrases occur less often, there should befewer entries introduced into the phrase translation table.We propose an agglomerative approach to learn phrase translations.
Ourapproach is motivated by the second group, which is to learn phrase translationthrough word-alignment, while addressing the common issues: the size of thephrase translation table, the use of underlying translation model probabilityand the length of the phrase unit.Only a few approaches move away from one-step learning.
Melamed [13]presented an agglomerative approach to learn the phrases progressively froma parallel corpus by using sub-phrase bigram statistics.
Moore [14] proposeda similar approach which identifies the phrase candidates by parsing the rawtraining data.
Our idea differs from these approaches in that we look into theassociation of the alignments rather than the association of the words to discoverthe phrases.In this paper, we propose the Level of Detail (LOD) approach for learningof phrase translations in phrase-based statistical machine translation.
Section 2discusses the background and motivation and then formulates the LOD approach578 H. Setiawan et alwhile section 3 describes the learning process in details.
Section 4 describesthe experimental results.
In this section, we compare LOD with state-of-the-artword-based approach in translation tasks.
Finally, section 5 concludes this paperby providing some discussion in comparison with other related works.2 Statistical Machine Translation: A Level of Detail2.1 Motivation and BackgroundIt is often not intuitive to model the translation of a phrase using the word-basedtranslation model.
First, the literal translation of phrase constituents is often in-appropriate from a linguistic point of view.
The word-based translation modeltreats a phrase as a multi-word.
One such example is the case where a phraseappears as an idiom.
The translation of an idiom cannot be synthesized fromthe literal translation of its constituents but rather from the semantic trans-lation of the whole.
Besides, the literal translation of an idiom detracts fromthe intended meaning.
In one such example, the literal translation of French?manger sur le pouce?
is ?to eat on the thumb?.
This detracts from the correcttranslation ?to grab a bite to eat ?.
In addition, to produce the correct trans-lation, the word-based translation model might have to learn that ?manger?is translated as ?eat?
or ?pouce?
is translated as ?thumb?.
Although it mayserve the translation purpose, it will introduce many non-intuitive entries to thedictionary.Second, even if it is possible to translate a phrase verbatim, modeling phrasetranslation using the word-based translation model suffers from a disadvantage:the number of word alignments required to synthesize the phrase translation islarge.
It requires four word alignments to model the translation between ?uneminute de silence?
and ?one minute of silence?, whereas one phrase alignmentis adequate.
The introduction of more alignments also implies the requirementto estimate more parameters for the translation model.
The implication oftencomes with the cost of learning wrong word alignments.Third, a phrase often constitutes some spurious words.
The word-based trans-lation model often has trouble in modeling spurious words, such as functionwords.
Function words may appear freely in any position and often may notbe translated to any word.
We observe that many of these function words ap-pear inside a phrase.
It is beneficial to realize these spurious words inside aphrase unit so as to improve statistical machine translation performance andalso to remove the necessity to model them explicitly.
All these suggest that,ideally, a phrase translation should be realized as a phrase alignment, wherethe lexical correspondence is established on phrase level rather than on its wordconstituents.The discussion above suggests that phrase-based translation is a wise choice.Practically, as a phrase is not a well defined lexical entry, a mechanism is neededto judge what constitutes a phrase in the context of statistical machine transla-tion.
In this paper, we advocate an approach to look into the phrase discoveryprocess at different level of details.
The level of detail refers to the size of aPhrase-Based Statistical Machine Translation: A Level of Detail Approach 579phrase unit.
At its finest level of detail, a phrase translation uses the word-basedtranslation model where a phrase is modeled through its word constituent.
Ata coarser level of detail, a sub-phrase unit is introduced as a sequence of words,making it a constituent of the phrase.
The coarsest level of detail refers to thestatus of a phrase where all word constituents converge into a whole unit.Our Level-Of-Detail (LOD) approach views the problem of phrase-basedtranslation modeling through a LOD process.
It starts from the finest word-level alignment and transforms the phrase translation into its coarsest level ofdetail.2.2 FormulationLet < e, f > be a sentence pair of two sequences of words with e as an Englishsentence and f as its translation in French1.
Let < e?, f?
> represents the samesentence pair but with the phrase as its atomic unit rather than the word.
Togeneralize the notation, we treat word and phrase unit similarly by consideringa word as a phrase of length one.
Therefore, < e, f > hereafter will be referred as< e?, f?
>(0), which represents the finest level of detail, and < e?, f?
> as < e?, f?
>(N),which represents the coarsest level of detail.
Let each tuple in the sentence pairof any level of detail n, < e?, f?
>(n) be e?
(n) = {e?
(n)0 , e?
(n)1 , .
.
.
, e?
(n)i , .
.
.
, e?
(n)l(n)} andf?
(n)= {f?
(n)0 , f?
(n)1 , .
.
.
, f?
(n)j , .
.
.
, f?
(n)m(n)} where e?
(n)0 ,f?
(n)0 represent the special tokenNULL as suggested in [1] and l(n),m(n) represent the length of the correspondingsentence.
Let T (n) be a set of alignment defined over the sentence pair < e?, f?
>(n)with t(n)ij = [e?
(n)i , f?
(n)j ] as its member.
The superscript in all notations denotesthe level of detail where 0 represents the finest and N represents the coarsestlevel of detail.LOD algorithm iteratively transforms < e?, f?
>(0) to < e?, f?
>(N) throughre-alignment of phrases and re-estimation of phrase translation probability.
Atn-th iteration, LOD harvests all bi-directional alignments from the sentence pair< e?, f?
>(n).
The alignment is obtained by a typical word-based translation model,such as the IBM model, while treating a sub-phrase at n-th iteration as a word.We refer to those alignments as B(n), a pool of sub-phrase alignments unique tothe particular iteration.
Afterwards, LOD generates all possible phrase alignmentcandidates C(n) for a coarser level of detail from these sub-phrase alignments.A resulting phrase alignment candidate is basically a joining of two adjacentsub-phrase alignments subject to a certain criterion.
It represents the futurecoarser level alignment.
Up to this point, two sets of alignment are obtainedover< e?, f?
>(n): a pool of sub-phrase alignments B(n) at the current level and apool of phrase alignment candidates C(n) at a coarser level.
From these two setsof alignments B(n) ?C(n), we would like to derive a new set of alignments T (n+1)that best describes the training corpus with the re-estimated statistics obtainedat n-th iteration.
LOD constructs < e?, f?
>(n+1) from the new set of alignment.Algorithm 1 provides the general overview of LOD algorithm.1 Subsequently, we will refer e as source sentence and f as target sentence, but theterm does not always reflect the translation direction.580 H. Setiawan et alAlgorithm 1.
An overview of LOD approach in learning phrase translation.
The LODapproach takes a sentence pair at its finest level of detail as its input, learns the phrase-level alignment iteratively and outputs the same sentence pair at its coarsest level ofdetail along with its phrase translation table.input ?e?, f??
(0)for n = 0 to (N ?
1) do- Generate bi-directional sub-phrase level alignments B(n) from ?e?, f??
(n)- Identify phrase-level alignment candidates C(n) from B(n)- Estimate the alignment probability in B(n) and C(n)- Learn coarser level alignment T (n+1) from B(n) ?
C(n) and construct ?e?, f??
(n+1)output ?e?, f??
(N) and T (N)3 Learning Phrase TranslationIn this section, we discuss the steps of LOD algorithm in detail.
As presentedin Algorithm 1, moving from one level of alignment to its coarser level, LODfollows four simple steps:1.
Generation of bi-directional sub-phrase level alignments 22.
Identification of phrase level alignment candidates3.
Estimation of alignment probability4.
Learning coarser level alignment3.1 Generation of Bi-directional Sub-phrase Level AlignmentsLOD follows the common practice to utilize the IBM translation model for learn-ing the phrase translation.
That is to harvest all alignments from both translationdirections.
For the sake of clarity, LOD defines the following notation for thesealignments, as follows:Let ?
(n)ef : e?
(n)i ??
f?
(n)j be an alignment function represents all alignmentsfrom translating the source English sentence to the target French sentence, and?
(n)fe : f?
(n)j ??
e?
(n)i be the reversed translation direction.
Then, bi-directionalsub-phrase alignment B(n) includes all possible alignment by both functions:B(n) = {t(n)ij = [e?
(n)i , f?
(n)j ]|(?
(n)ef (e?
(n)i ) = f?
(n)j ) ?
(?
(n)fe (f?
(n)j ) = e?
(n)i )}Let us denote NULL alignments, N (n), a subset of alignments in B(n) inwhich the special token NULL is involved.2 The process starts with word level alignment.
A word here is also referred to as asub-phrase.Phrase-Based Statistical Machine Translation: A Level of Detail Approach 5813.2 Identification of Phrase Alignment CandidatesLOD applies a simple heuristic to identify a phrase alignment candidate.
First,LOD considers every combination of two distinct sub-phrase alignments and as-sesses its candidacy.
Here, we define a phrase alignment candidate < t(n)ij , t(n)i?j?
>?C(n) as follows:Let < t(n)ij , t(n)i?j?
> be a set of two tuples, where t(n)ij ?
B(n) and t(n)i?j?
?
B(n).Then < t(n)ij , t(n)i?j?
> is a phrase aligment candidate if and only if1.
not ((i, i?
)= 0) or (|i ?
i?| = 1)2. not ((t(n)ij ?
N (n)) and (t(n)i?j?
?
N (n)))In the definition above, the first clause defines a candidate as a set of two whosesource sub-phrases are adjacent.
The second clause forbids the consideration oftwo NULL alignments.As LOD considers only two alignments for each phrase alignment candidate,it implies that, at the n-th iteration, the length of the longest possible phraseis bounded by 2n.
Apparently, we do not have to examine sub-phrase alignmenttrunks of more than two sub-phrases because the iteration process guaranteesLOD to explore phrases of any length given sufficient iteration.
This way, thesearch space at each iteration can be manageable at each iteration.3.3 Estimation of Alignment ProbabilityJoining the alignment set B(n) derived in Section 3.1 and the coarser level align-ment C(n) derived in Section 3.2, we form a candidate alignment set B(n) ?
C(n).Assuming that there are two alignments x ?
B(n), y ?
B(n), and a candidatealignment < x, y >?
C(n), we derive the probability p(x) and p(y) from thestatistics as the count of x and y normalized by the number of alignments in thecorpus, and we derive the joint probability p(< x, y >) in a similar way.If there is a genuine association between the two alignments, x and y, thenwe expect that p(< x, y >)  p(x)p(y).
If there is no interesting relationshipbetween x and y, then p(< x, y >) ?
p(x)p(y) where we say that x and y areindependent.
If x and y are in a complementary relationship, then we expect tosee that p(< x, y >)  p(x)p(y).
These statistics allow us to discover a genuinesub-phrase association.The probability is estimated by the count of observed events normalized bythe corpus size.
Note that the alignment from the IBM translation model isderived using a Viterbi-like decoding scheme.
Each observed event is counted asone.
This is referred to as hard-counting.
As the alignment is done according toprobability distribution, another way of counting the event is to use the fractionalcount that can be derived from the translation model.
We refer to it as soft-counting.3.4 Learning a Coarser Level AlignmentFrom section 3.1 to 3.3, we have prepared all the necessary alignments with theirprobability estimates.
The next step is to re-align < e?, f?
>(n) into < e?, f?
>(n+1)582 H. Setiawan et alusing alignment phrases in B(n) ?
C(n) with their newly estimated probabilitydistribution.
The re-alignment is considered as a constrained search process.
Letp(t(n)ij ) be the probability of a phrase alignment t(n)ij ?
(B(n) ?
C(n)) as definedin Section 3.3, T (n) be the potential new alignment sequence for < e?, f?
>(n), wehave the likelihood for T (n) aslog P (< e?, f?
>(n) |T (n)) =?t(n)ij ?T(n)log p(t(n)ij ) (1)The constrained search is to decode an alignment sequence that producesthe highest likelihood possible in the current iteration, subject to the followingconstraints:1. to preserve the phrase ordering of the source and target languages2.
to preserve the completeness of word or phrase coverage in the sentence pair3.
to ensure the mutual exclusion between alignments (except for the specialNULL tokens)The constrained search can be formulated as follows:T (n+1) = argmax?T (n)log P (< e?, f?
>(n) |T (n)) (2)In Eq.
(2), we have T (n+1) as the best alignment sequence to re-align sentencepair < e?, f?
>(n) to < e?, f?
>(n+1) .The constraints are to ensure that the search leads to a valid alignment re-sult.
The search is essentially a decoding process, which traverses the sentencepair along the source language and explores all the possible phrase alignmentswith the target language.
In practice, LOD tries to find a phrase translationtable that maximizes Eq.
(2) as formulated in Algorithm 2.
As the existing align-ment for < e?, f?
>(n) in the n-th iteration is a valid alignment subject to threeAlgorithm 2.
A stack decoding algorithm to explore the best alignment path betweensource and target languages by considering all alignment candidates in B(n) ?
C(n) atn-th iteration.1.
Initialize a lattice of l(n) slots for l(n) sub-phrase in source language.2.
Starting from i=1, for all phrases in source language ei;1) Register all the alignments t(n)ij that map source phrases ending with ei,including ei itself, into slot i in the lattice;2) Register the probability of alignment p(t(n)ij ) together withthe alignment entry t(n)ij3) Repeat 1) and 2) until i=l(n)3.
Apply stack decoding [15] process to find the top n-best paths subject to thethree constraints.
During the decoding processing, the extension of partial pathis subject to a connectivity test to enforce the three constraints.4.
Output the top best alignment result as the final result.Phrase-Based Statistical Machine Translation: A Level of Detail Approach 583constraints, it also serves as one resolution to the search.
In the worst case, if theconstrained search can not discover any new alignment other than the existingone, then the existing alignment in the current iteration will stand through thenext iteration.In Algorithm 2, we establish the lattice along the source language.
In thecase of English to French translation, we follow the phrases in the English order.However, it can be done along the target language as well since our approachfollows a symmetric many-to-many word alignment strategy.This step ends with the promotion of all phrase alignment candidates in thebest alignment sequence T (n+1).
The promotion includes the merging of the twosub-phrase alignments and the concerning sub-phrases.
The merged unit will beconsidered as a unit in the next iteration.4 ExperimentsThe objective of our experiments is to validate our LOD approach in ma-chine translation task.
Additionally, we are interested in investigating the fol-lowing: the effect of soft-counting in probability estimation, and the behav-ior of LOD approach in every iteration, in terms of the length of the phraseunit and the size of the phrase translation table.
We report all our experi-ments using BLEU metrics [10].
Furthermore, we report confidence intervalswith 95% statistical significance level of each experiments, as suggested byKoehn [16].We validate our approach through several experiments using English andFrench language pairs from the Hansard corpus.
We restrict the sentence lengthto at most 20 words to obtain around 110 thousands sentence pairs.
Then werandomly select around 10 thousands sentence pair as our own testing set.
Intotal, the French corpus consists of 994,564 words and 29,360 unique words; whilethe English corpus consists of 1,055,167 words and 20,138 unique words.
Ourexperiment is conducted on both English-to-French (e2f) and French-to-English(f2e) tasks under open testing set-up.
We use these available tools: GIZA++3for word-based IBM 4 model training and ISI ReWrite4 for translation test.
Formeasuring the BLEU score and deriving the confidence intervals, we use thepublicly available tools5.4.1 Soft-Counting vs. Hard-CountingTable 1 summarizes our experiments in analyzing the effect of soft-countingand hard-counting in the probability estimation on the BLEU score.
Case Idemonstrates the BLEU score of the experiment using the underlying transla-tion model probability or soft-counting, while Case II demonstrates the score of3 http://www.fjoch.com/4 http://www.isi.edu/licensed-sw/rewrite-decoder/5 http://www.nist.gov/speech/tests/mt/resources/scoring.htm andhttp://projectile.is.cs.cmu.edu/research/public/tools/bootStrap/tutorial.htm584 H. Setiawan et alTable 1.
Summary of experiment showing the contribution of using the translationmodel probability.
The experiments are conducted on English-to-French task.
Case Iindicates the BLEU score of the LOD approach using soft-counting whereas Case IIindicates the BLEU score of hard-counting.
The value in the column indicates theBLEU score.
The range inside the bracket indicates the confidence intervals with 95%statistical significance level.iteration Case I Case II1 29.60 (29.01-30.14) 28.80 (28.20-29.38)2 30.72 (30.09-31.29) 30.11 (29.48-30.67)3 31.52 (30.87-32.06) 30.70 (30.05-31.32)4 31.93 (31.28-32.50) 30.93 (30.30-31.51)5 31.90 (31.45-32.68) 31.07 (30.39-31.62)hard-counting.
The experimental results suggest that the use of the underlyingtranslation model probability is beneficial as it gives consistently higher BLEUscores in all the iterations.
The comparison using paired bootstrap resampling[16] also confirms the conclusion.4.2 LOD Behavior over IterationTable 2 summarizes the performance of our LOD approach for the first 10 itera-tions in comparison with the baseline IBM 4 word-based approach.
The resultsshow that the LOD approach produces a significant improvement over IBM 4consistently.
The first iteration yields the biggest improvement.
We achieve anabsolute BLEU score improvement of 5.01 for the English-to-French task and5.48 for the French-to-English task from the first iteration.
The subsequent im-provement is obtained by performing more iterations and capturing longer phrasetranslation, however, the improvement gained is less significant compared to thatof the first iteration.Table 2 also summarizes the maximum phrase length and the behavior ofthe phrase translation table: its size and its increment over iteration.
It showsthat the phrase length is soft-constrained by the maximum likelihood criterionin Eq.
(2) rather than limited.
As iteration goes on, longer phrases are learntbut their probabilities are less probable than shorter one.
Consequently, longerphrases introduce fewer entries to the phrase translation table.
Table 2 capturesthe behavior of the phrase translation table.
The first iteration contributes thehighest increment of 12.5 % to the phrase translation table while the accumulatedincrement of table size up to 10th iteration only contributes 27.5% incrementover the original size.
It suggests that as iteration goes and longer phrases arecaptured, fewer additional entries are introduced to the phrase translation table.The results also show the growth of the size of the phrase translation table issub-linear and it converges after reasonable number of iterations.
This representsa clear advantage of LOD over other related work [6][8].Phrase-Based Statistical Machine Translation: A Level of Detail Approach 585Table 2.
Summary of experiments showing the behavior of LOD approach and thecharacteristics of the phrase translation table in each iteration.
The table shows thetranslation performance of the word-based IBM 4 approach and the first 10 iteration ofLOD approach in BLEU score.
The value in the columns indicate the BLEU score whilethe range inside the bracket represents the confidence intervals with 95% statisticalsignificance level.
The table also shows the trend of the phrase translation table: themaximum phrase length, its size, and its increase over iterations.Max Table BLEU with confidence intervalsIteration Phrase Size IncreaseLength e2f f2eIBM 4 1 216,852 - 24.59 (24.12-25.21) 26.76 (26.15-27.33)1 2 244,097 27,245 29.60 (29.01-30.14) 32.24 (31.58-32.83)2 4 258,734 14,637 30.72 (30.09-31.29) 32.93 (32.28-33.57)3 7 266,209 7,475 31.52 (30.87-32.06) 33.88 (33.22-34.49)4 7 270,531 4,322 31.93 (31.28-32.50) 34.14 (33.46-34.76)5 10 271,793 1,262 31.90 (31.45-32.68) 34.26 (33.56-34.93)6 11 273,589 1,796 32.14 (31.48-32.72) 34.50 (33.78-35.16)7 12 274,641 1,052 32.09 (31.43-32.68) 34.55 (33.81-35.18)8 12 275,399 758 32.07 (31.39-32.60) 34.43 (33.71-35.09)9 13 275,595 196 31.98 (31.32-32.55) 34.65 (33.93-35.29)10 14 276,508 913 32.22 (31.55-32.79) 34.61 (33.91-35.26)5 DiscussionIn this paper, we propose LOD approach to phrase-based statistical machinetranslation.
The LOD approach addresses three issues in the phrase-based trans-lation framework: the size of phrase translation table, the use of underlyingtranslation model probability and the length of the phrase unit.In terms of the size of the phrase translation table, our LOD approachpresents a sub-linear growth of the phrase translation table.
It demonstrates aclear advantage over other reported attempts, such as in [6][8] where the phrasetranslation table grows almost linearly over the phrase length limit.
The LODapproach manages the phrase translation table size in a systematic way as aresult of the incorporation of maximum likelihood criterion into the phrase dis-covery process.In terms of the use of underlying translation model probability, we proposeto use soft-counting instead of hard-counting in the re-estimation processing ofprobability estimation.
In the projection extension algorithm [6], the phrases arelearnt based on the presence of alignment in certain configurations.
In alignmenttemplate[2], two phrases are considered to be translation of each other, if theword alignments exist within the phrases and not to the words outside.
Bothmethods are based on hard-counting of translation event.
Our experiment resultssuggest the use of soft-counting.586 H. Setiawan et alIn terms of the length of the phrase unit, we move away from the window-likelimit for phrase candidacy [4][9].
The LOD approach is shown to be more flexiblein capturing phrases of different length.
It gradually explores longer phrases asiteration goes, leading any reasonable length given sufficient iteration as long asthey are statistically credible.It is known that statistical machine translation relies very much on thetraining corpus.
A larger phrase translation table means more training dataare needed for the translation model to be statistically significant.
In this paper,we successfully introduce the LOD approach to control the process of new phrasediscovery process.
The results are encouraging.References1.
Peter F. Brown, Stephen A. Della Pietra, Vincent J. Della Pietra, and RobertL.
Mercer.
1993.
The mathematics of statistical machine translation: parameterestimation.
Computational Linguistics, 19(2), pp.
263-311.2.
Franz Josef Och, Christoph Tillmann, and Hermann Ney.
1999.
Improved alignmentmodels for statistical machine translation.
In Proc of the Joint SIGDAT Conferenceon Empirical Methods in Natural Language Processing and Very Large Corpora,pp.
20-28, University of Maryland, College Park, MD, June.3.
Franz Josef Och and Hermann Ney.
2000.
A Comparison of alignment models forstatistical machine translation.
In Proc of the 18th International Conference ofComputational Linguistics, Saarbruken, Germany, July.4.
Daniel Marcu and William Wong.
2002.
A phrase-Based, joint probability model forstatistical machine translation.
In Proc.
of the Conference on Empirical Methodsin Natural Language Processing, pp.
133-139, Philadelphia, PA, July.5.
Stephan Vogel, Hermann Ney, and Christoph Tillmann.
1996.
HMM-based wordalignment in statistical translation, Proc.
of COLING ?96: The 16th InternationalConference of Computational Linguistics.
pp.
836-841.
Copenhagen, Denmark.6.
Christoph Tillmann.
2003.
A projection extension algorithm for statistical machinetranslation.
in Proc.
of the Conference on Empirical Methods in Natural LanguageProcessing, Sapporo, Japan.7.
Ying Zhang, Stephan Vogel, Alex Waibel.
2003.
Integrated phrase segmentationand alignment algorithm for statistical machine translation.
in Proc.
of the Confer-ence on Natural Language Processing and Knowledge Engineering, Beijing, China.8.
Philipp Koehn, Franz Josef Och, Daniel Marcu.
2003.
Statistical Phrase-basedTranslation.
In Proc.
of the Human Language Technology Conference, pp.
127-133, Edmonton, Canada, May/June.9.
Ashish Venugopal, Stephan Vogel, Alex Waibel.
2004.
Effective phrase translationextraction from alignment models.
in Proc.
of 41st Annual Meeting of Associationof Computational Linguistics, pp.
319-326, Sapporo, Japan, July.10.
K. Papineni, S. Roukos, T. Ward and W. J. Zhu.
2001.
BLEU: A method forautomatic evaluation of machine translation.
Technical Report RC22176 (W0109-022), IBM Research Report.11.
G. Doddington.
2002.
Automatic evaluation of machine translation quality usingN-gram co-occurence statistics.
In Proc.
of the Conference on Human LanguageTechnology, pp.
138-135, San Diego, CA, USA.Phrase-Based Statistical Machine Translation: A Level of Detail Approach 58712.
Richard Zens, Hermann Ney.
2004.
Improvements in phrase-Based statistical ma-chine translation.
in Proc.
of Conference on Human Language Technology, pp.257-264, Boston, MA, USA.13.
I. D. Melamed.
1997.
Automatic discovery of non-compositional compounds in par-allel data.
In Proc.
of 2nd Conference on Empirical Methods in Natural LanguageProcessing, Provicence, RI.14.
Robert C Moore.
2001.
Towards a simple and accurate statistical approach tolearning translation relationships among words.
In Proc of Workshop on Data-driven Machine Translation, 39th Annual Meeting and 10th Conference of theEuropean Chapter, Association for Computational Linguistics, pp.
79-86, Toulouse,France.15.
R Schwartz and Y. L. Chow .
1990.
The N-best algorithm: An efficient and exactprocedure for finding the N most likely sentence hypothesis.
In Proc.
of ICASSP1990, pp.
81-84.
Albuquerque, CA.16.
Philipp Koehn.
2004.
Statistical significance tests for machine translation evalua-tion.
In Proc.
of the 2004 Conference on Empirical Methods in Natural LanguageProcessing, pp.
388-395.
