Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 53?57,Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational LinguisticsLatent Class Transliteration based on Source Language OriginMasato HagiwaraRakuten Institute of Technology, New York215 Park Avenue South, New York, NYmasato.hagiwara@mail.rakuten.comSatoshi SekineRakuten Institute of Technology, New York215 Park Avenue South, New York, NYsatoshi.b.sekine@mail.rakuten.comAbstractTransliteration, a rich source of proper nounspelling variations, is usually recognized byphonetic- or spelling-based models.
How-ever, a single model cannot deal with dif-ferent words from different language origins,e.g., ?get?
in ?piaget?
and ?target.?
Li etal.
(2007) propose a method which explicitlymodels and classifies the source language ori-gins and switches transliteration models ac-cordingly.
This model, however, requires anexplicitly tagged training set with languageorigins.
We propose a novel method whichmodels language origins as latent classes.
Theparameters are learned from a set of translit-erated word pairs via the EM algorithm.
Theexperimental results of the transliteration taskof Western names to Japanese show that theproposed model can achieve higher accuracycompared to the conventional models withoutlatent classes.1 IntroductionTransliteration (e.g., ???????
baraku obama /Barak Obama?)
is phonetic translation between lan-guages with different writing systems.
Words areoften transliterated when imported into differet lan-guages, which is a major cause of spelling variationsof proper nouns in Japanese and many other lan-guages.
Accurate transliteration is also the key torobust machine translation systems.Phonetic-based rewriting models (Knight andJonathan, 1998) and spelling-based supervised mod-els (Brill and Moore, 2000) have been proposed forrecognizing word-to-word transliteration correspon-dence.
These methods usually learn a single modelgiven a training set.
However, single models cannotdeal with words from multiple language origins.
Forexample, the ?get?
parts in ?piaget /????
piaje?
(French origin) and ?target / ?????
ta?getto?
(English origin) may differ in how they are translit-erated depending on their origins.Li et al (2007) tackled this issue by proposing aclass transliteration model, which explicitly modelsand classifies origins such as language and genders,and switches corresponding transliteration model.This method requires training sets of transliteratedword pairs with language origin.
However, it is diffi-cult to obtain such tagged data, especially for propernouns, a rich source of transliterated words.
In ad-dition, the explicitly tagged language origins are notnecessarily helpful for loanwords.
For example, theword ?spaghetti?
(Italian origin) can also be foundin an English dictionary, but applying an Englishmodel can lead to unwanted results.In this paper, we propose a latent class transliter-ation model, which models the source language ori-gin as unobservable latent classes and applies appro-priate transliteration models to given transliterationpairs.
The model parameters are learned via the EMalgorithm from training sets of transliterated pairs.We expect that, for example, a latent class which ismostly occupied by Italian words would be assignedto ?spaghetti /?????supageti?
and the pair willbe correctly recognized.In the evaluation experiments, we evaluated theaccuracy in estimating a corresponding Japanesetransliteration given an unknown foreign word,53flextimefurekkusutaimus:t:?i?iFigure 1: Minimum edit operation sequence in the alpha-beta model (Underlined letters are match operations)using lists of Western names with mixed lan-guages.
The results showed that the proposed modelachieves higher accuracy than conventional modelswithout latent classes.Related researches include Llitjos and Black(2001), where it is shown that source language ori-gins may improve the pronunciation of proper nounsin text-to-speech systems.
Another one by Ahmadand Kondrak (2005) estimates character-based errorprobabilities from query logs via the EM algorithm.This model is less general than ours because it onlydeals with character-based error probability.2 Alpha-Beta ModelWe adopted the alpha-beta model (Brill and Moore,2000), which directly models the string substitu-tion probabilities of transliterated pairs, as the basemodel in this paper.
This model is an extension tothe conventional edit distance, and gives probabil-ities to general string substitutions in the form of?
?
?
(?, ?
are strings of any length).
The wholeprobability of rewriting word s with t is given by:PAB(t|s) = maxT?Part(t),S?Part(s)|S|?i=1P (?i ?
?i), (1)where Part(x) is all the possible partitions of wordx.
Taking logarithm and regarding ?
logP (?
?
?
)as the substitution cost of ?
?
?, this maximiza-tion is equivalent to finding a minimum of total sub-stitution costs, which can be solved by normal dy-namic programming (DP).
In practice, we condi-tioned P (?
?
?)
by the position of ?
in words,i.e., at the beginning, in the middle, or at the end ofthe word.
This conditioning is simply omitted in theequations in this paper.The substitution probabilities P (?
?
?)
arelearned from transliterated pairs.
Firstly, we obtainan edit operation sequence using the normal DP foredit distance computation.
In Figure 1 the sequenceis f?f, ?
?u, l?r, e?e,?
?k, x?k, ... and so on.Secondly, non-match operations are merged with ad-jacent edit operations, with the maximum length ofsubstitution pairs limited to W .
When W = 2,for example, the first non-match operation ?
?u ismerged with one operation on the left and right, pro-ducing f?fu and l?ur.
Finally, substitution prob-abilities are calculated as relative frequencies of allsubstitution operations created in this way.
Note thatthe minimum edit operation sequence is not unique,so we take the averaged frequencies of all the possi-ble minimum sequences.3 Class Transliteration ModelThe alpha-beta model showed better performance intasks such as spelling correction (Brill and Moore,2000), transliteration (Brill et al, 2001), and queryalteration (Hagiwara and Suzuki, 2009).
However,the substitution probabilities learned by this modelare simply the monolithic average of training setstatistics, and cannot be switched depending on thesource language origin of given pairs, as explainedin Section 1.Li et al (2007) pointed out that similar problemsarise in Chinese.
Transliteration of Indo-Europeannames such as ?????
/ Alexandra?
can be ad-dressed by Mandarin pronunciation (Pinyin) ?Ya-Li-Shan-Da,?
while Japanese names such as ???
/Yamamoto?
can only be addressed by consideringthe Japanese pronunciation, not the Chinese pro-nunciation ?Shan-Ben.?
Therefore, Li et al tookinto consideration two additional factors, i.e., sourcelanguage origin l and gender / first / last names g,and proposed a model which linearly combines theconditioned probabilities P (t|s, l, g) to obtain thetransliteration probability of s ?
t as:P (t|s)soft =?l,gP (t, l, g|s)=?l,gP (t|s, l, g)P (l, g|s) (2)We call the factors c = (l, g) as classes in this paper.This model can be interpreted as firstly computing54the class probability distribution given P (c|s) thentaking a weighted sum of P (t|s, c) with regard tothe estimated class c and the target t.Note that this weighted sum can be regardedas doing soft-clustering of the input s into classeswith probabilities.
Alternatively, we can employhard-clustering by taking one class such that c?
=argmaxl,g P (l, g|s) and compute the transliterationprobability by:P (t|s)hard ?
P (t|s, c?).
(3)4 Latent Class Transliteration ModelThe model explained in the previous section inte-grates different transliteration models for words withdifferent language origins, but it requires us to buildclass detection model c from training pairs explicitlytagged with language origins.Instead of assigning an explicit class c to eachtransliterated pair, we can introduce a random vari-able z and consider a conditioned string substitutionprobability P (?
?
?|z).
This latent class z cor-responds to the classes of transliterated pairs whichshare the same transliteration characteristics, such aslanguage origins and genders.
Although z is not di-rectly observable from sets of transliterated words,we can compute it via EM algorithm so that it max-imizes the training set likelihood as shown below.Due to the space limitation, we only show the up-date equations.
Xtrain is the training set consistingof transliterated pairs {(sn, tn)|1 ?
n ?
N}, N isthe number of training pairs, and K is the number oflatent classes.Parameters: P (z = k) = pik, P (?
?
?|z)(4)E-Step: ?nk =pikP (tn|sn, z = k)?Kk=1 pikP (tn|sn, z = k), (5)P (tn|sn, z) = maxT?Part(tn),S?Part(sn)|S|?i=1P (?i ?
?i|z)M-Step: pi?k =NkN, Nk =N?n=1?nk (6)P (?
?
?|z = k)?
= 1NkN?n=1?nkfn(?
?
?)????
fn(?
?
?
)Here, fn(?
?
?)
is the frequency of substitutionpair ?
?
?
in the n-th transliterated pair, whosecalculation method is explained in Section 2.
Thefinal transliteration probability is given by:Platent(t|s) =?zP (t, z|s) =?zP (z|s)P (t|s, z)?
?zpikP (s|z)P (t|s, z) (7)The proposed model cannot explicitly modelP (s|z), which is in practice approximated byP (t|s, z).
Even omitting this factor only has amarginal effect on the performance (within 1.1%).5 ExperimentsHere we evaluate the performance of the transliter-ation models as an information retrieval task, wherethe model ranks target t?
for a given source s?, basedon the model P (t?|s?).
We used all the t?n in thetest set Xtest = {(s?n, t?n)|1 ?
n ?
M} as targetcandidates and s?n for queries.
Five-fold cross vali-dation was adopted when learning the models, thatis, the datasets described in the next subsections areequally splitted into five folds, of which four wereused for training and one for testing.
The mean re-ciprocal rank (MRR) of top 10 ranked candidateswas used as a performance measure.5.1 Experimental SettingsDataset 1: Western Person Name List Thisdataset contains 6,717 Western person names andtheir Katakana readings taken from an Europeanname website ??????
1, consisting of Ger-man (de), English (en), and French (fr) person namepairs.
The numbers of pairs for these languages are2,470, 2,492, and 1,747, respectively.
Accent marksfor non-English languages were left untouched.
Up-percase was normalized to lowercase.Dataset 2: Western Proper Noun List Thisdataset contains 11,323 proper nouns and theirJapanese counterparts extracted from Wikipedia in-terwiki.
The languages and numbers of pairs con-tained are: German (de): 2,003, English (en): 5,530,Spanish (es): 781, French (fr): 1,918, Italian (it):1http://www.worldsys.org/europe/55Language de en frPrecision(%) 80.4 77.1 74.7Table 1: Language Class Detection Result (Dataset 1)1,091.
Linked English and Japanese titles are ex-tracted, unless the Japanese title contains any othercharacters than Katakana, hyphen, or middle dot.The language origin of titles were detectedwhether appropriate country names are included inthe first sentence of Japanese articles.
If they con-tain ?????
(of Germany),?
??????
(ofFrance),?
??????
(of Italy),?
they are markedas German, French, and Italian origin, respectively.If the sentence contains any of Spain, Argentina,Mexico, Peru, or Chile plus ???
(of), it is markedas Spanish origin.
If they contain any of Amer-ica, England, Australia or Canada plus ???
(of), itis marked as English origin.
The latter parts ofJapanese/foreign titles starting from ?,?
or ?(?
wereremoved.
Japanese and foreign titles were split intochunks by middle dots and ?
?, respectively, and re-sulting chunks were aligned.
Titles pairs with differ-ent numbers of chunks, or ones with foreign char-acter length less than 3 were excluded.
All accentmarks were normalized (German ???
was convertedto ?ss?
).Implementation Details P (c|s) of the classtransliteration model was calculated by a charac-ter 3-gram language model with Witten-Bell dis-counting.
Japanese Katakanas were all convertedto Hepburn-style Roman characters, with minorchanges so as to incorporate foreign pronunciationssuch as ?wi / ???
and ?we / ??.?
The hyphens???
were replaced by the previous vowels (e.g., ?????????
is converted to ?supagettii.?
)The maximum length of substitution pairs W de-scribed in Section 2 was set W = 2.
The EM al-gorithm parameters P (?
?
?|z) were initialized tothe probability P (?
?
?)
of the alpha-beta modelplus Gaussian noise, and pik were uniformly initial-ized to 1/K.
Based on the preliminary results, werepeated EM iterations for 40 times.5.2 ResultsLanguage Class Detection We firstly show theprecision of language detection using the classLanguage de en es fr itPrecision(%) 65.4 83.3 48.2 57.7 66.1Table 2: Language Class Detection Result (Dataset 2)Model Dataset 1 Dataset 2AB 94.8 90.9HARD 90.3 89.8SOFT 95.7 92.4LATENT 95.8 92.4Table 3: Model Performance Comparison (MRR; %)transliteration model P (c|s) and Equation (3) (Table5.2, 5.2).
The overall precision is relatively lowerthan, e.g., Li et al (2007), which is attributed to thefact that European names can be quite ambiguous(e.g., ?Charles?
can read ??????
cha?ruzu?
or?????
sharuru?)
The precision of Dataset 2 iseven worse because it has more classes.
We can alsouse the result of the latent class transliteration forclustering by regarding k?
= argmaxk ?nk as theclass of the pair.
The resulting cluster purity waywas 0.74.Transliteration Model Comparison We showthe evaluation results of transliteration candidate re-trieval task using each of PAB(t|s) (AB), Phard(t|s)(HARD), Psoft(t|s) (SOFT), and Platent(t|s) (LA-TENT) (Table 5.2).
The number of latent classeswas K = 3 for Dataset 1 and K = 5 for Dataset 2,which are the same as the numbers of language ori-gins.
LATENT shows comparable performance ver-sus SOFT, although it can be higher depending onthe value of K, as stated below.
HARD, on the otherhand, shows lower performance, which is mainlydue to the low precision of class detection.
The de-tection errors are alleviated in SOFT by consideringthe weighted sum of transliteration probabilities.We also conducted the evaluation based on thetop-1 accuracy of transliteration candidates.
Be-cause we found out that the tendency of the resultsis the same as MRR, we simply omitted the result inthis paper.The simplest model AB incorrectly reads ?Felix/ ??????,?
?Read / ????
as ?????Firisu?
and ?????
Rea?do.?
This may be becauseEnglish pronunciation ?x / ???
kkusu?
and ?ea /56??
?i?
are influenced by other languages.
SOFTand LATENT can find correct candidates for thesepairs.
Irregular pronunciation pairs such as ?Caen/ ???
ka?n?
(French; misread ?????
sha?n?
)and ?Laemmle /???
Remuri?
(English; misread????
Riamu?)
were misread by SOFT but not byLATENT.
For more irregular cases such as ?Hilda????
Iruda?
(English), it is difficult to find correctcounterparts even by LATENT.Finally, we investigated the effect of the numberof latent classes K. The performance is higher whenK is slightly smaller than the number of languageorigins in the dataset (e.g., K = 4 for Dataset 2) butthe performance gets unstable for larger values of Kdue to the EM algorithm initial values.6 ConclusionIn this paper, we proposed a latent class translitera-tion method which models source language originsas latent classes.
The model parameters are learnedfrom sets of transliterated words with different ori-gins via the EM algorithm.
The experimental re-sult of Western person / proper name transliterationtask shows that, even though the proposed modeldoes not rely on explicit language origins, it achieveshigher accuracy versus conventional methods usingexplicit language origins.
Considering sources otherthan Western languages as well as targets other thanJapanese is the future work.ReferencesFarooq Ahmad and Grzegorz Kondrak.
2005.
Learning aspelling error model from search query logs.
In Proc.of EMNLP-2005, pages 955?962.Eric Brill and Robert C. Moore.
2000.
An improvederror model for noisy channel spelling.
In Proc.
ACL-2000, pages 286?293.Eric Brill, Gary Kacmarcik, and Chris Brockett.
2001.Automatically harvesting katakana-english term pairsfrom search engine query logs.
In Proc.
NLPRS-2001,pages 393?399.Masato Hagiwara and Hisami Suzuki.
2009.
Japanesequery alteration based on semantic similarity.
In Proc.of NAACL-2009, page 191.Kevin Knight and Graehl Jonathan.
1998.
Machinetransliteration.
Computational Linguistics, 24:599?612.Haizhou Li, Khe Chai Sum, Jin-Shea Kuo, and MinghuiDong.
2007.
Semantic transliteration of personalnames.
In Proc.
of ACL 2007, pages 120?127.Ariadna Font Llitjos and Alan W. Black.
2001.
Knowl-edge of language origin improves pronunciation accu-racy.
In Proc.
of Eurospeech, pages 1919?1922.57
