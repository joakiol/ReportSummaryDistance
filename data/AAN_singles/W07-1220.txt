Proceedings of the 5th Workshop on Important Unresolved Matters, pages 152?159,Ann Arbor, June 2005. c?2005 Association for Computational LinguisticsThe Corpus and the Lexicon: Standardising Deep Lexical AcquisitionEvaluationYi Zhang?
and Timothy Baldwin?
and Valia Kordoni??
Dept of Computational Linguistics, Saarland University and DFKI GmbH, Germany?
Dept of Computer Science and Software Engineering, University of Melbourne, Australia{yzhang,kordoni}@coli.uni-sb.detim@csse.unimelb.edu.auAbstractThis paper is concerned with the standard-isation of evaluation metrics for lexical ac-quisition over precision grammars, whichare attuned to actual parser performance.Specifically, we investigate the impact thatlexicons at varying levels of lexical itemprecision and recall have on the perfor-mance of pre-existing broad-coverage pre-cision grammars in parsing, i.e., on theircoverage and accuracy.
The grammars usedfor the experiments reported here are theLinGO English Resource Grammar (ERG;Flickinger (2000)) and JACY (Siegel andBender, 2002), precision grammars of En-glish and Japanese, respectively.
Our re-sults show convincingly that traditional F-score-based evaluation of lexical acquisitiondoes not correlate with actual parsing per-formance.
What we argue for, therefore, is arecall-heavy interpretation of F-score in de-signing and optimising automated lexical ac-quisition algorithms.1 IntroductionDeep processing is the process of applying rich lin-guistic resources within NLP tasks, to arrive at adetailed (=deep) syntactic and semantic analysis ofthe data.
It is conventionally driven by deep gram-mars, which encode linguistically-motivated predic-tions of language behaviour, are usually capable ofboth parsing and generation, and generate a high-level semantic abstraction of the input data.
Whileenjoying a resurgence of interest due to advancesin parsing algorithms and stochastic parse prun-ing/ranking, deep grammars remain an underutilisedresource predominantly because of their lack of cov-erage/robustness in parsing tasks.
As noted in previ-ous work (Baldwin et al, 2004), a significant causeof diminished coverage is the lack of lexical cover-age.Various attempts have been made to amelioratethe deficiencies of hand-crafted lexicons.
Morerecently, there has been an explosion of interestin deep lexical acquisition (DLA; (Baldwin, 2005;Zhang and Kordoni, 2006; van de Cruys, 2006))for broad-coverage deep grammars, either by ex-ploiting the linguistic information encoded in thegrammar itself (in vivo), or by using secondary lan-guage resources (in vitro).
Such approaches provide(semi-)automatic ways of extending the lexicon withminimal (or no) human interference.One stumbling block in DLA research has beenthe lack of standardisation in evaluation, withcommonly-used evaluation metrics including:?
Type precision: the proportion of correctly hy-pothesised lexical entries?
Type recall: the proportion of gold-standardlexical entries that are correctly hypothesised?
Type F-measure: the harmonic mean of thetype precision and type recall?
Token Accuracy: the accuracy of the lexical en-tries evaluated against their token occurrencesin gold-standard corpus dataIt is often the case that the different measures leadto significantly different assessments of the qualityof DLA, even for a given DLA approach.
Addi-tionally, it is far from clear how the numbers gen-erated by these evaluation metrics correlate with ac-tual parsing performance when the output of a givenDLA method is used.
This makes standardised com-parison among the various different approaches toDLA very difficult, if not impossible.
It is far fromclear which evaluation metrics are more indicative ofthe true ?goodness?
of the lexicon.
The aim of thisresearch, therefore, is to analyse how the differentevaluation metrics correlate with actual parsing per-formance using a given lexicon, and to work towards152a standardised evaluation framework for future DLAresearch to ground itself in.In this paper, we explore the utility of differentevaluation metrics at predicting parse performancethrough a series of experiments over two broad cov-erage grammars: the English Resource Grammar(ERG; Flickinger (2000)) and JACY (Siegel andBender, 2002).
We simulate the results of DLAby generating lexicons at different levels of preci-sion and recall, and test the impact of such lexiconson grammar coverage and accuracy related to gold-standard treebank data.
The final outcome of thisanalysis is a proposed evaluation framework for fu-ture DLA research.The remainder of the paper is organised as fol-lows: Section 2 reviews previous work on DLA forthe robust parsing task; Section 3 describes the ex-perimental setup; Section 4 presents the experimentresults; Section 5 analyses the experiment results;Section 6 concludes the paper.2 Lexical Acquisition in Deep ParsingHand-crafted large-scale grammars are error-prone.An error can be roughly classified as undergenerat-ing (if it prevents a grammatical sentence from be-ing generated/parsed) or overgenerating (if it allowsan ungrammatical sentence to be generated/parsed).Hence, errors in deep grammar lexicons can be clas-sified into two categories: i) a lexical entry is miss-ing for a specific lexeme; and ii) an erroneous lexicalentry enters the lexicon.
The former error type willcause the grammar to fail to parse/generate certainsentences (i.e.
undergenerate), leading to a loss incoverage.
The latter error type will allow the gram-mar to parse/generate inappropriate sentences (i.e.overgenerate), potentially leading to a loss in ac-curacy.
In the first instance, we will be unable toparse sentences involving a given lexical item if it ismissing from our lexicon, i.e.
coverage will be af-fected assuming the lexical item of interest occursin a given corpus.
In the second instance, the im-pact is indeterminate, as certain lexical items mayviolate constraints in the grammar and never be li-cenced, whereas others may be licenced more lib-erally, generating competing (incorrect) parses for agiven input and reducing parse accuracy.
It is thesetwo competing concerns that we seek to quantify inthis research.Traditionally, errors in the grammar are detectedmanually by the grammar developers.
This is usu-ally done by running the grammar over a carefullydesigned test suite and inspecting the outputs.
Thisprocedure becomes less reliable as the grammar getslarger.
Also we can never expect to attain completelexical coverage, due to language evolution and theeffects of domain/genre.
A static, manually com-piled lexicon, therefore, becomes inevitably insuffi-cient when faced with open domain text.In recent years, some approaches have been de-veloped to (semi-)automatically detect and/or repairthe lexical errors in linguistic grammars.
Such ap-proaches can be broadly categorised as either sym-bolic or statistical.Erbach (1990), Barg and Walther (1998) andFouvry (2003) followed a unification-based sym-bolic approach to unknown word processing forconstraint-based grammars.
The basic idea is touse underspecified lexical entries, namely entrieswith fewer constraints, to parse whole sentences,and generate the ?real?
lexical entries afterwards bycollecting information from the full parses.
How-ever, lexical entries generated in this way may be ei-ther too general or too specific.
Underspecified lex-ical entries with fewer constraints allow more gram-mar rules to be applied while parsing, and fully-underspecified lexical entries are computationallyintractable.
The whole procedure gets even morecomplicated when two unknown words occur nextto each other, potentially allowing almost any con-stituent to be constructed.
The evaluation of theseproposals has tended to be small-scale and some-what brittle.
No concrete results have been pre-sented relating to the improvement in grammar per-formance, either for parsing or for generation.Baldwin (2005) took a statistical approach to au-tomated lexical acquisition for deep grammars.
Fo-cused on generalising the method of deriving DLAmodels on various secondary language resources,Baldwin used a large set of binary classifiers to pre-dict whether a given unknown word is of a particularlexical type.
This data-driven approach is grammarindependent and can be scaled up for large gram-mars.
Evaluation was via type precision, type recall,type F-measure and token accuracy, resulting in dif-ferent interpretations of the data depending on theevaluation metric used.Zhang and Kordoni (2006) tackled the robustnessproblem of deep processing from two aspects.
Theyemployed error mining techniques in order to semi-automatically detect errors in deep grammars.
Theythen proposed a maximum entropy model based lex-153ical type predictor, to generate new lexical entrieson the fly.
Evaluation focused on the accuracy ofthe lexical type predictor over unknown words, notthe overall goodness of the resulting lexicon.
Simi-larly to Baldwin (2005), the methods are applicableto other constraint-based lexicalist grammars, but nodirect measurement of the impact on grammar per-formance was attempted.van de Cruys (2006) took a similar approach overthe Dutch Alpino grammar (cf.
Bouma et al (2001)).Specifically, he proposed a method for lexical ac-quisition as an extension to automatic parser errordetection, based on large amounts of raw text (cf.van Noord (2004)).
The method was evaluated us-ing type precision, type recall and type F-measure.Once again, however, these numbers fail to give usany insight into the impact of lexical acquisition onparser performance.Ideally, we hope the result of DLA to be both ac-curate and complete.
However, in reality, there willalways be a trade-off between coverage and parseraccuracy.
Exactly how these two concerns should bebalanced up depends largely on what task the gram-mar is applied to (i.e.
parsing or generation).
In thispaper, we focus exclusively on the parsing task.13 Experimental SetupIn this research, we wish to evaluate the impactof different lexicons on grammar performance.
Bygrammar performance, we principally mean cov-erage and accuracy.
However, it should be notedthat the efficiency of the grammar?e.g.
the aver-age number of edges in the parse chart, the averagetime to parse a sentence and/or the average numberof analyses per sentence?is also an important per-formance measurement which we expect the qualityof the lexicon to impinge on.
Here, however, weexpect to be able to call on external processing opti-misations2 to dampen any loss in efficiency, in a waywhich we cannot with coverage and accuracy.3.1 ResourcesIn order to get as representative a set of results aspossible, we choose to run the experiment over two1In generation, we tend to have a semantic representationas input, which is linked to pre-existing lexical entries.
Hence,lexical acquisition has no direct impact on generation.2For example, (van Noord, 2006) shows that a HMM POStagger trained on the parser outputs can greatly reduce the lexi-cal ambiguity and enhance the parser efficiency, without an ob-servable decrease in parsing accuracy.large-scale HPSGs (Pollard and Sag, 1994), basedon two distinct languages.The LinGO English Resource Grammar (ERG;Flickinger (2000)) is a broad-coverage, linguis-tically precise HPSG-based grammar of English,which represents the culmination of more than 10person years of (largely) manual effort.
We use thejan-06 version of the grammar, which contains about23K lexical entries and more than 800 leaf lexicaltypes.JACY (Siegel and Bender, 2002) is a broad-coverage linguistically precise HPSG-based gram-mar of Japanese.
In our experiment, we use theNovember 2005 version of the grammar, which con-tains about 48K lexical entries and more than 300leaf lexical types.It should be noted in HPSGs, the grammar ismade up of two basic components: the grammarrules/type hierarchy, and the lexicon (which inter-faces with the type hierarchy via leaf lexical types).This is different to strictly lexicalised formalismslike LTAG and CCG, where essentially all linguisticdescription resides in individual lexical entries in thelexicon.
The manually compiled grammars in ourexperiment are also intrinsically different to gram-mars automatically induced from treebanks (e.g.
thatused in the Charniak parser (Charniak, 2000) or thevarious CCG parsers (Hockenmaier, 2006)).
Thesedifferences sharply differentiate our work from pre-vious research on the interaction between lexical ac-quisition and parse performance.Furthermore, to test the grammar precision andaccuracy, we use two treebanks: Redwoods (Oepenet al, 2002) for English and Hinoki (Bond et al,2004) for Japanese.
These treebanks are so-calleddynamic treebanks, meaning that they can be (semi-)automatically updated when the grammar is up-dated.
This feature is especially useful when wewant to evaluate the grammar performance with dif-ferent lexicon configurations.
With conventionaltreebanks, our experiment is difficult (if not impos-sible) to perform as the static trees in the treebankcannot be easily synchronised to the evolution of thegrammar, meaning that we cannot regenerate gold-standard parse trees relative to a given lexicon (es-pecially when for reduced recall where there is noguarantee we will be able to produce all of the parsesin the 100% recall gold-standard).
As a result, it isextremely difficult to faithfully update the statisticalmodels.The Redwoods treebank we use is the 6th growth,154which is synchronised with the jan-06 version of theERG.
It contains about 41K test items in total.The Hinoki treebank we use is updated for theNovember 2005 version of the JACY grammar.
The?Rei?
sections we use in our experiment contains45K test items in total.3.2 Lexicon GenerationTo simulate the DLA results at various levels of pre-cision and recall, a random lexicon generator is used.In order to generate a new lexicon with specific pre-cision and recall, the generator randomly retains aportion of the gold-standard lexicon, and generates apre-determined number of erroneous lexical entries.More specifically, for each grammar we first ex-tract a subset of the lexical entries from the lexicon,each of which has at least one occurrence in the tree-bank.
This subset of lexical entries is considered tobe the gold-standard lexicon (7,156 entries for theERG, 27,308 entries for JACY).Given the gold-standard lexicon L, the target pre-cision P and recall R, a new lexicon L?
is created,which is composed of two disjoint subsets: the re-tained part of the gold-standard lexicon G, and theerroneous entries E. According to the definitions ofprecision and recall:P = |G||L?| (1) R =|G||L| (2)and the fact that:|L?| = |G| + |E| (3)we get:|G| = |L| ?
R (4)|E| = |L| ?
R ?
( 1P ?
1) (5)To retain a specific number of entries from thegold-standard lexicon, we randomly select |G| en-tries based on the combined probabilistic distribu-tion of the corresponding lexeme and lexical types.3We obtain the probabilistic distribution of lexemesfrom large corpora (BNC for English and MainichiShimbun [1991-2000] for Japanese), and the distri-bution of lexical types from the corresponding tree-banks.
For each lexical entry e(l, t) in the gold-standard lexicon with lexeme l and lexical type t,3For simplicity, we assume mutual independence of the lex-emes and lexical types.the combined probability is:p(e(l, t)) = CL(l) ?
CT (t)?e?(l?,t?
)?L CL(l?)
?
CT (t?
)(6)The erroneous entries are generated in the sameway among all possible combinations of lexemesand lexical types.
The difference is that only opencategory types and less frequent lexemes are usedfor generating new entries (e.g.
we wouldn?t expectto learn a new lexical item for the lexeme the or thelexical type d - the le in English).
In our ex-periment, we consider lexical types with more thana predefined number of lexical entries (20 for theERG, 50 for JACY) in the gold-standard lexicon tobe open-class lexical types; the upper-bound thresh-old on token frequency is set to 1000 for English and537 for Japanese, i.e.
lexemes which occur more fre-quently than this are excluded from lexical acquisi-tion under the assumption that the grammar develop-ers will have attained full coverage of lexical itemsfor them.For each grammar, we then generate 9 differ-ent lexicons at varying precision and recall levels,namely 60%, 80%, and 100%.3.3 Parser CoverageCoverage is an important grammar performancemeasurement, and indicates the proportion of inputsfor which a correct parse was obtained (adjudgedrelative to the gold-standard parse data in the tree-banks).
In our experiment, we adopt a weak defini-tion of coverage as ?obtaining at least one spanningtree?.
The reason for this is that we want to obtainan estimate for novel data (for which we do not havegold-standard parse data) of the relative number ofstrings for which we can expect to be able to produceat least one spanning parse.
This weak definition ofcoverage actually provides an upper bound estimateof coverage in the strict sense, and saves the effort tomanually evaluate the correctness of the parses.
Pastevaluations (e.g.
Baldwin et al (2004)) have shownthat the grammars we are dealing with are relativelyprecise.
Based on this, we claim that our results forparse coverage provide a reasonable estimate indica-tion of parse coverage in the strict sense of the word.In principle, coverage will only decrease whenthe lexicon recall goes down, as adding erroneousentries should not invalidate the existing analy-ses.
However, in practice, the introduction of er-roneous entries increases lexical ambiguity dramati-1550.6 0.8 1.0P \ R C E A C E A C E A0.6 4294 2862 7156 5725 3817 9542 7156 4771 119270.8 4294 1073 5367 5725 1431 7156 7156 1789 89451.0 4294 0 4294 5725 0 5725 7156 0 7156Table 1: Different lexicon configurations for the ERG with the number of correct (C), erroneous (E) andcombined (A) entries at each level of precision (P) and recall (R)0.6 0.8 1.0P \ R C E A C E A C E A0.6 16385 10923 27308 21846 14564 36410 27308 18205 455130.8 16385 4096 20481 21846 5462 27308 27308 6827 341351.0 16385 0 16385 21846 0 21846 27308 0 27308Table 2: Different lexicon configurations for JACY with the number of correct (C), erroneous (E) andcombined (A) entries at each level of precision (P) and recall (R)cally, readily causing the parser to run out of mem-ory.
Moreover, some grammars use recursive unaryrules which are triggered by specific lexical types.Here again, erroneous lexical entries can lead to ?failto parse?
errors.Given this, we run the coverage tests for the twogrammars over the corresponding treebanks: Red-woods and Hinoki.
The maximum number of pas-sive edges is set to 10K for the parser.
We used[incr tsdb()] (Oepen, 2001) to handle the dif-ferent lexicon configurations and data sets, and PET(Callmeier, 2000) for parsing.3.4 Parser AccuracyAnother important measurement of grammar perfor-mance is accuracy.
Deep grammars often generatehundreds of analyses for an input, suggesting theneed for some means of selecting the most probableanalysis from among them.
This is done with theparse disambiguation model proposed in Toutanovaet al (2002), with accuracy indicating the proportionof inputs for which we are able to accurately selectthe correct parse.The disambiguation model is essentially a maxi-mum entropy (ME) based ranking model.
Given aninput sentence s with possible analyses t1 .
.
.
tk, theconditional probability for analysis ti is given by:P (ti|s) =exp ?mj=1 fj(ti)?j?ki?=1 exp?mj=1 fj(ti?
)?j(7)where f1 .
.
.
fm are the features and ?1 .
.
.
?mare the corresponding parameters.
When rankingparses,?mj=1 fj(ti)?j is the indicator of ?good-ness?.
Drawing on the discriminative nature of theME models, various feature types can be incor-porated into the model.
In combination with thedynamic treebanks where the analyses are (semi-)automatically disambiguated, the models can beeasily re-trained when the grammar is modified.For each lexicon configuration, after the cover-age test, we do an automatic treebank update.
Dur-ing the automatic treebank update, only those newparse trees which are comparable to the active treesin the gold-standard treebank are marked as cor-rect readings.
All other trees are marked as in-active and deemed as overgeneration of the gram-mar.
The ME-based parse disambiguation modelsare trained/evaluated using these updated treebankswith 5-fold cross validation.
Since we are only in-terested in the difference between different lexiconconfigurations, we use the simple PCFG-S modelfrom (Toutanova et al, 2002), which incorporatesPCFG-style features from the derivation tree of theparse.
The accuracy of the disambiguation modelis calculated by top analysis exact matching (i.e.
aranking is only considered correct if the top rankedanalysis matches the gold standard prefered readingin the treebank).All the Hinoki Rei noun sections (about 25Kitems) were used in the accuracy evaluation forJACY.
However, due to technical limitations, onlythe jh sections (about 6K items) of the RedwoodsTreebank were used for training/testing the disam-biguation models for the ERG.4 Experiment ResultsThe experiment consumes a considerable amount ofcomputational resources.
For each lexicon config-156P \ R 0.6 0.8 1.00.6 44.56% 66.88% 75.51%0.8 42.18% 65.82% 75.86%1.0 40.45% 66.19% 76.15%Table 3: Parser coverage of JACY with different lex-iconsP \ R 0.6 0.8 1.00.6 27.86% 39.17% 79.66%0.8 27.06% 37.42% 79.57%1.0 26.34% 37.18% 79.33%Table 4: Parser coverage of the ERG with differentlexiconsuration of a given grammar, we need to i) process(parse) all the items in the treebank, ii) compare theresulting trees with the gold-standard trees and up-date the treebank, and iii) retrain the disambiguationmodels over 5 folds of cross validation.
Given thetwo grammars with 9 configurations each, the en-tire experiment takes over 1 CPU month and about120GB of disk space.The coverage results are shown in Table 3 andTable 4 for JACY and the ERG, respectively.4 Asexpected, we see a significant increase in grammarcoverage when the lexicon recall goes up.
This in-crease is more significant for the ERG than JACY,mainly because the JACY lexicon is about twice aslarge as the ERG lexicon; thus, the most frequententries are still in the lexicons even with low recall.When the lexicon recall is fixed, the grammar cov-erage does not change significantly at different lev-els of lexicon precision.
Recall that we are not eval-uating the correctness of such parses at this stage.It is clear that the increase in lexicon recall booststhe grammar coverage, as we would expect.
Theprecision of the lexicon does not have a large in-fluence on coverage.
This result confirms that withDLA (where we hope to enhance lexical coveragerelative to a given corpus/domain), the coverage ofthe grammar can be enhanced significantly.The accuracy results are obtained with 5-foldcross validation, as shown in Table 5 and Table 64Note that even with the lexicons at 100% precision and re-call level, there is no guarantee of 100% coverage.
As the con-tents of the Redwoods and Hinoki treebanks were determinedindependently of the respective grammars, rather than the gram-mars being induced from the treebanks e.g., they both still con-tain significant numbers of strings for which the grammar can-not produce a correct analysis.P-R #ptree Avg.
?060-060 13269 62.65% 0.89%060-080 19800 60.57% 0.83%060-100 22361 59.61% 0.63%080-060 14701 63.27% 0.62%080-080 23184 60.97% 0.48%080-100 27111 60.04% 0.56%100-060 15696 63.91% 0.64%100-080 26859 61.47% 0.68%100-100 31870 60.48% 0.71%Table 5: Accuracy of disambiguation models forJACY with different lexiconsP-R #ptree Avg.
?060-060 737 71.11% 3.55%060-080 1093 63.94% 2.75%060-100 3416 60.92% 1.23%080-060 742 70.07% 1.50%080-080 1282 61.81% 3.60%080-100 3842 59.05% 1.30%100-060 778 69.76% 4.62%100-080 1440 60.59% 2.64%100-100 4689 57.03% 1.36%Table 6: Accuracy of disambiguation models for theERG with different lexiconsfor JACY and the ERG, respectively.
When the lex-icon recall goes up, we observe a small but steadydecrease in the accuracy of the disambiguation mod-els, for both JACY and ERG.
This is generally a sideeffect of change in coverage: as the grammar cover-age goes up, the parse trees become more diverse,and are hence harder to discriminate.When the recall is fixed and the precision of thelexicon goes up, we observe a very small accuracygain for JACY (around 0.5% for each 20% increasein precision).
This shows that the grammar accu-racy gain is limited as the precision of the lexiconincreases, i.e.
that the disambiguation model is re-markably robust to the effects of noise.It should be noted that for the ERG we failed toobserve any accuracy gain at all with a more pre-cise lexicon.
This is partly due to the limited sizeof the updated treebanks.
For the lexicon config-uration 060 ?
060, we obtained only 737 preferredreadings/trees to train/test the disambiguation modelover.
The 5-fold cross validation results vary withina margin of 10%, which means that the models arestill not converging.
However, the result does con-firm that there is no significant gain in grammar ac-curacy with a higher precision lexicon.Finally, we combine the coverage and accuracyscores into a single F-measure (?
= 1) value.
Theresults are shown in Figure 1.
Again we see that157the difference in lexicon recall has a more signif-icant impact on the overall grammar performancethan precision.0.40.50.60.7F-score(JaCY)R=0.6R=0.8R=1.0P=0.6P=0.8P=1.00.40.50.60.70.6 0.8 1.0F-score(ERG)Lex.
PrecisionR=0.6R=0.8R=1.00.6 0.8 1.0Lex.
RecallP=0.6P=0.8P=1.0Figure 1: Grammar performance (F-score) with dif-ferent lexicons5 Discussion5.1 Is F-measure a good metric for DLAevaluation?As mentioned in Section 2, a number of relevant ear-lier works have evaluated DLA results via the un-weighted F-score (relative to type precision and re-call).
This implicitly assumes that the precision andrecall of the lexicon are equally important.
How-ever, this is clearly not the case as we can see in theresults of the grammar performance.
For example,the lexicon configurations 060 ?
100 and 100 ?
060of JACY (i.e.
60% precision, 100% recall vs. 100%precision, 60% recall, respectively) have the sameunweighted F-scores, but their corresponding over-all grammar performance (parser F-score) differs byup to 17%.5.2 Does precision matter?The most interesting finding in our experiment isthat the precision of the deep lexicon does not ap-pear to have a significant impact on grammar accu-racy.
This is contrary to the earlier predominant be-lief that deep lexicons should be as accurate as pos-sible.
This belief is derived mainly from observa-tion of grammars with relatively small lexicons.
Insuch small lexicons, the closed-class lexical entriesand frequent entries (which comprise the ?core?
ofthe lexicon) make up a large proportion of lexicalentries.
Hence, any loss in precision means a signif-icant degradation of the ?core?
lexicon, which leadsto performance loss of the grammar.
For example,we find that the inclusion of one or two erroneousentries for frequent closed-class lexical type words(such as the, or of in English, for instance) may eas-ily ?break?
the parser.However, in state-of-the-art broad-coverage deepgrammars such as JACY and ERG, the lexicons aremuch larger.
They usually have more or less similar?cores?
to the smaller lexicons, but with many moreopen-class lexical entries and less frequent entries,which compose the ?peripheral?
parts of the lexi-cons.
In our experiment, we found that more than95% of the lexical entries belong to the top 5% ofthe open-class lexical types.
The bigger the lexiconis, the larger the proportion of lexical entries that be-long to the ?peripheral?
lexicon.In our experiment, we only change the ?periph-eral?
lexicon by creating/removing lexical entriesfor less frequent lexemes and open-class lexicaltypes, leaving the ?core?
lexicon intact.
Therefore, amore accurate interpretation of the experimental re-sults is that the precision of the open type and lessfrequent lexical entries does not have a large impacton the grammar performance, but their recall has acrucial effect on grammar coverage.The consequence of this finding is that the bal-ance between precision and recall in the deep lexi-con should be decided by their impact on the task towhich the grammar is applied.
In research on auto-mated DLA, the motivation is to enhance the robust-ness/coverage of the grammars.
This work showsthat grammar performance is very robust over theinevitable errors introduced by the DLA, and thatmore emphasis should be placed on recall.Again, caution should be exercised here.
Wedo not mean that by blindly adding lexical entrieswithout worrying about their correctness, the per-formance of the grammar will be monotonically en-hanced ?
there will almost certainly be a point atwhich noise in the lexicon swamps the parse chartand/or leads to unacceptable levels of spurious am-biguity.
Also, the balance between precision and re-call of the lexicon will depend on various expecta-tions of the grammarians/lexicographers, i.e.
the lin-guistic precision and generality, which is beyond thescope of this paper.As a final word of warning, the absolute gram-mar performance change that a given level of lexi-158con type precision and recall brings about will obvi-ously depend on the grammar.
In looking across twogrammars from two very different languages, we areconfident of the robustness of our results (at least forgrammars of the same ilk) and the conclusions thatwe have drawn from them.
For any novel grammarand/or formalism, however, the performance changeshould ideally be quantified through a set of exper-iments with different lexicon configurations, basedon the procedure outlined here.
Based on this, itshould be possible to find the optimal balance be-tween the different lexicon evaluation metrics.6 ConclusionIn this paper, we have investigated the relationshipbetween evaluation metrics for deep lexical acquisi-tion and grammar performance in parsing tasks.
Theresults show that traditional DLA evaluation basedon F-measure is not reflective of grammar perfor-mance.
The precision of the lexicon appears to haveminimal impact on grammar accuracy, and thereforerecall should be emphasised more greatly in the de-sign of deep lexical acquisition techniques.ReferencesTimothy Baldwin, Emily Bender, Dan Flickinger, Ara Kim, andStephan Oepen.
2004.
Road-testing the English ResourceGrammar over the British National Corpus.
In Proc.
of thefourth international conference on language resources andevaluation (LREC 2004), pages 2047?2050, Lisbon, Portu-gal.Timothy Baldwin.
2005.
Bootstrapping deep lexical resources:Resources for courses.
In Proc.
of the ACL-SIGLEX 2005workshop on deep lexical acquisition, pages 67?76, Ann Ar-bor, USA.Petra Barg and Markus Walther.
1998.
Processing unknownwords in HPSG.
In Proc.
of the 36th Conference of theACL and the 17th International Conference on Computa-tional Linguistics, pages 91?95, Montreal, Canada.Francis Bond, Sanae Fujita, Chikara Hashimoto, KanameKasahara, Shigeko Nariyama, Eric Nichols, Akira Ohtani,Takaaki Tanaka, and Shigeaki Amano.
2004.
The Hinokitreebank: a treebank for text understanding.
In Proc.
of thefirst international joint conference on natural language pro-cessing (IJCNLP04), pages 554?562, Hainan, China.Gosse Bouma, Gertjan van Noord, and Robert Malouf.
2001.Alpino: wide-coverage computational analysis of Dutch.
InComputational linguistics in the Netherlands 2000, pages45?59, Tilburg, the Netherlands.Ulrich Callmeier.
2000.
PET ?
a platform for experimentationwith efficient HPSG processing techniques.
Natural Lan-guage Engineering, 6(1):99?107.Eugene Charniak.
2000.
A maximum entropy-based parser.In Proc.
of the 1st Annual Meeting of the North Ameri-can Chapter of Association for Computational Linguistics(NAACL2000), Seattle, USA.Gregor Erbach.
1990.
Syntactic processing of unknown words.IWBS Report 131, IBM, Stuttgart, Germany.Dan Flickinger.
2000.
On building a more efficient grammar byexploiting types.
Natural Language Engineering, 6(1):15?28.Frederik Fouvry.
2003.
Lexicon acquisition with a large-coverage unification-based grammar.
In Proc.
of the 10thConference of the European Chapter of the Association forComputational Linguistics (EACL 2003), pages 87?90, Bu-dapest, Hungary.Julia Hockenmaier.
2006.
Creating a CCGbank and a wide-coverage CCG lexicon for German.
In Proc.
of the 21stInternational Conference on Computational Linguistics and44th Annual Meeting of the Association for ComputationalLinguistics, pages 505?512, Sydney, Australia.Stephan Oepen, Kristina Toutanova, Stuart Shieber, ChristopherManning, Dan Flickinger, and Thorsten Brants.
2002.
TheLinGO Redwoods treebank: Motivation and preliminary ap-plications.
In Proc.
of the 17th international conference oncomputational linguistics (COLING 2002), Taipei, Taiwan.Stephan Oepen.
2001.
[incr tsdb()] ?
competence and perfor-mance laboratory.
User manual.
Technical report, Compu-tational Linguistics, Saarland University, Saarbru?cken, Ger-many.Carl Pollard and Ivan Sag.
1994.
Head-Driven Phrase Struc-ture Grammar.
University of Chicago Press, Chicago, USA.Melanie Siegel and Emily Bender.
2002.
Efficient deep pro-cessing of Japanese.
In Proc.
of the 3rd Workshop onAsian Language Resources and International Standardiza-tion, Taipei, Taiwan.Kristina Toutanova, Christoper Manning, Stuart Shieber, DanFlickinger, and Stephan Oepen.
2002.
Parse ranking fora rich HPSG grammar.
In Proc.
of the First Workshop onTreebanks and Linguistic Theories (TLT2002), pages 253?263, Sozopol, Bulgaria.Tim van de Cruys.
2006.
Automatically extending the lexiconfor parsing.
In Proc.
of the eleventh ESSLLI student session,pages 180?191, Malaga, Spain.Gertjan van Noord.
2004.
Error mining for wide-coveragegrammar engineering.
In Proc.
of the 42nd Meeting of theAssociation for Computational Linguistics (ACL?04), MainVolume, pages 446?453, Barcelona, Spain.Gertjan van Noord.
2006.
At Last Parsing Is Now Operational.In Actes de la 13e conference sur le traitement automatiquedes langues naturelles (TALN06), pages 20?42, Leuven, Bel-gium.Fei Xia, Chung-Hye Han, Martha Palmer, and Aravind Joshi.2001.
Automatically extracting and comparing lexicalizedgrammars for different languages.
In Proc.
of the 17th Inter-national Joint Conference on Artificial Intelligence (IJCAI-2001), pages 1321?1330, Seattle, USA.Yi Zhang and Valia Kordoni.
2006.
Automated deep lexicalacquisition for robust open texts processing.
In Proc.
ofthe fifth international conference on language resources andevaluation (LREC 2006), pages 275?280, Genoa, Italy.159
