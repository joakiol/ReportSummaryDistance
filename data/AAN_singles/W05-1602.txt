Interactive Authoring of Logical Forms for Multilingual Generation?Ofer Biller, Michael Elhadad, Yael NetzerDepartment of Computer ScienceBen Gurion UniversityBe?er-Sheva, 84105, Israel{billero, elhadad, yaeln}@cs.bgu.ac.ilAbstractWe present an authoring system for logical formsencoded as conceptual graphs (CG).
The systembelongs to the family of WYSIWYM (What YouSee Is What You Mean) text generation systems:logical forms are entered interactively and the cor-responding linguistic realization of the expressionsis generated in several languages.
The systemmaintains a model of the discourse context corre-sponding to the authored documents.The system helps users author documents formu-lated in the CG format.
In a first stage, a domain-specific ontology is acquired by learning from ex-ample texts in the domain.
The ontology acquisi-tion module builds a typed hierarchy of conceptsand relations derived from the WordNet and Verb-net.The user can then edit a specific document, by en-tering utterances in sequence, and maintaining arepresentation of the context.
While the user en-ters data, the system performs the standard stepsof text generation on the basis of the authored log-ical forms: reference planning, aggregation, lexi-cal choice and syntactic realization ?
in several lan-guages (we have implemented English and Hebrew- and are exploring an implementation using theBliss graphical language).
The feedback in naturallanguage is produced in real-time for every singlemodification performed by the author.We perform a cost-benefit analysis of the applica-tion of NLG techniques in the context of authoringcooking recipes in English and Hebrew.
By com-bining existing large-scale knowledge resources(WordNet, Verbnet, the SURGE and HUGG real-ization grammars) and techniques from modern in-tegrated software development environment (suchas the Eclipse IDE), we obtain an efficient tool forthe generation of logical forms, in domains wherecontent is not available in the form of databases.
?Research supported by the Israel Ministry of Science - Knowl-edge Center for Hebrew Computational Linguistics and by theFrankel Fund1 IntroductionNatural language generation techniques can be applied topractical systems when the ?input?
data to be rendered in textcan be obtained in a cost-effective manner, and when the ?out-put?
requires such variability (multiple styles or languages,or customization to specific users or classes) that producingdocuments manually becomes prohibitively expensive.The input data can be either derived from an existing appli-cation database or it can be authored specifically to producedocuments.
Applications where the data is available in a data-base include report generators (e.g., ANA [Kukich, 1983],PlanDoc [Shaw et al, 1994], Multimeteo [Coch, 1998], FOG[Goldberg et al, 1994]).
In other cases, researchers identi-fied application domains where some of the data is available,but not in sufficient detail to produce full documents.
The?WYSIWYM?
approach was proposed ([Power and Scott,1998], [Paris and Vander Linden, 1996]) as a system designmethodology where users author and manipulate an underly-ing logical form through a user interface that provides feed-back in natural language text.The effort invested in authoring logical forms ?
either fromscratch or from a partial application ontology ?
is justifiedwhen the logical form can be reused.
This is the case whendocuments must be generated in several languages.
The fieldof multilingual generation (MLG) has addressed this need([Bateman, 1997], [Stede, 1996]).
When documents must beproduced in several versions, adapted to various contexts orusers, the flexibility resulting from generation from logicalforms is also valuable.
Another motivation for authoring logi-cal forms (as opposed to textual documents) is that the logicalform can be used for other applicative requirements: search,summarization of multiple documents, inference.
This con-cern underlies the research programme of the Semantic Web,which promotes the encoding in standardized forms of on-tological knowledge such as KIF [Berners-Lee et al, 2001],[Genesereth and Fikes, 1992].In this paper, we analyze an application of the WYSIWYMmethod to author logical forms encoded in Sowa?s Concep-tual Graphs (CG) format [Sowa, 1987].
In a first stage, userssubmit sample texts in a domain to the system.
The systemlearns from the samples a hierarchy of concepts and relations.Given this ontology, the author then enters expressions usinga simple variant of the CG Interchange Format (CGIF) whichwe have designed to speed editing operations.
The systemprovides realtime feedback to the author in English and He-brew.We evaluate the specific features of such a system whichmake it cost-effective as a tool to author logical forms.
Weselect the CG formalism as one of the representatives ofthe family of knowledge encoding formalisms, which bene-fits from well-established inference and quantification mech-anisms and standard syntax encodings in graphical and linearformats.The editing system we developed can be seen as CG ed-itor motivated and expanded by natural language generation(NLG) techniques.
The mixing of a practical ontology edit-ing perspective with NLG techniques yielded the followingbenefits:?
Generation tasks such as aggregation and reference plan-ning are easily expressed as operations upon CGs.?
The construction and maintenance of context accordingto models of text planning [Reiter and Dale, 1992], allowthe author to break a complex CG into a manageablecollection of small utterances.
Each utterance links to aglobal context in a natural manner.?
We designed a compact form to edit a textual encodingof CGs taking into account defaults, knowledge of typesof concepts, sets and individual instances and context.This format syntactically looks like a simple object-oriented programming language with objects, methodsand attributes.
We use an editing environment similar toa modern programming language development environ-ment ?
with a browser of types and instances, intelligenttyping completion based on type analysis, and context-specific tooltip assistance.?
The simultaneous generation of text in two languages(Hebrew and English) is important to distinguish be-tween un-analyzed terms in the ontology and their lin-guistic counterpart.We evaluate the overall effectiveness of the authoring en-vironment in the specific domain of cooking recipes (inspiredby [Dale, 1990]).
We perform various usability studies toevaluate the overall cost of authoring cooking recipes as log-ical forms and evaluate the relative contribution of each com-ponent of the system: ontology, natural language feedback,user interface.
We conclude that the combination of thesethree factors results in an effective environment for authoringlogical forms.In the paper, we first review the starting points upon whichthis study builds in generation and knowledge editing.
Wethen present the tool we have implemented ?
its architecture,the knowledge acquisition module and the editor, we finallypresent the evaluation experiments and their results, and con-clude with their analysis.2 Related WorkOur work starts from several related research traditions: mul-tilingual generation systems; WYSIWYM systems; knowl-edge and ontology editors.
We review these in this section inturn.2.1 Multilingual GenerationMultilingual texts generation (MLG) is a well motivatedmethod for the automatic production of technical documentsin multiple languages.
The benefits of MLG over translationfrom single language source were documented in the past andinclude the high cost of human translation and the inaccu-racy of automatic machine translation [Stede, 1996], [Coch,1998], [Bateman, 1997].
In an MLG system, users enter datain an interlingua, from which the target languages are gener-ated.MLG Systems aim to be as domain independent as pos-sible (since development is expensive) but usually refer to anarrow domain, since the design of the interlingua refers todomain information.
MLG systems share a common archi-tecture consisting of the following modules:?
A language-independent underlying knowledge repre-sentation: knowledge represented as AI plans [Ro?snerand Stede, 1994] [Delin et al, 1994], [Paris and Van-der Linden, 1996], knowledge bases (or ontologies)such as LOOM, the Penman Upper-model and other(domain-specific) concepts and instances [Ro?sner andStede, 1994].?
Micro-structure planning (rhetorical structure) - lan-guage independent - this is usually done by the humanwriters using the MLG application GUI.?
Sentence planning - different languages can express thesame content in various rhetorical structures, and plan-ning must take it into consideration: either by avoidingthe tailoring of structure to a specific language [Ro?snerand Stede, 1994] or by taking advantage of knowledgeon different realizations of rhetorical structures in differ-ent languages at the underlying representation [Delin etal., 1994].?
Lexical and syntactic realization resources (e.g., Eng-lish PENMAN/German NIGEL in [Ro?sner and Stede,1994])As an MLG system, our system also includes similar mod-ules.
We have chosen to use Conceptual Graphs as an inter-lingua for encoding document data [Sowa, 1987].
We use ex-isting generation resources for English ?
SURGE [Elhadad,1992] for syntactic realization and the lexical chooser de-scribed in [Jing et al, 2000] and the HUGG grammar forsyntactic realization in Hebrew [Netzer, 1997].
For micro-planning, we have implemented the algorithm for referenceplanning described in [Reiter and Dale, 1992] and the ag-gregation algorithm described in [Shaw, 1995].
The NLGcomponents rely on the C-FUF implementation of the FUFlanguage [Kharitonov, 1999] [Elhadad, 1991] ?
which is fastenough to be used interactively in realtime for every singleediting modification of the semantic input.2.2 WYSIWYMIn an influential series of papers [Power and Scott, 1998],WYSIWYM (What You See Is What You Mean) was pro-posed as a method for the authoring of semantic informationthrough direct manipulation of structures rendered in naturallanguage text.
A WYSIWYM editor enables the user to editinformation at the semantic level.
The semantic level is a di-rect controlled feature, and all lower levels which are derivedfrom it, are considered as presentational features.
While edit-ing content, the user gets a feedback text and a graphical rep-resentation of the semantic network.
These representationscan be interactively edited, as the visible data is linked backto the underlying knowledge representation.Using this method, a domain expert produces data by edit-ing the data itself in a formal way, using a tool that requiresonly knowledge of the writer?s natural language.
Knowledgeediting requires less training, and the natural language feed-back strengthens the confidence of users in the validity of thedocuments they prepare.The system we have developed belongs to the WYSIWYMfamily.
The key aspects of the WYSIWYM method we in-vestigate are the editing of the semantic information.
Textis generated as a feedback for every single editing operation.Specifically, we evaluate how ontological information helpsspeed up semantic data editing.2.3 Controlled LanguagesA way to ensure that natural language text is unambiguousand ?easy to process?
is to constrain its linguistic form.
Re-searchers have designed ?controlled languages?
to ensure thatwords in a limited vocabulary and simple syntactic struc-tures are used (see for example [Pulman, 1996]).
This notionis related to that of sublanguage [Kittredge and Lehrberger,1982], which has been used to analyze and generate text inspecific domains such as weather reports.With advances in robust methods for text analysis, it is be-coming possible to parse text with high accuracy and recoverpartial semantic information.
For example, the DIRT system[Lin and Pantel, 2001] recovers thematic structures from freetext in specific domains.
Combined with lexical resources(WordNet [Miller, 1995] and Verbnet [Kipper et al, 2000]),it is now possible to confirm the thesis that controlled lan-guages are easy to process automatically.Complete semantic interpretation of text remains howevertoo difficult for current systems.
In our system, we rely onautomatic interpretation of text samples in a specific sublan-guage to assist in the acquisition of a domain-specific ontol-ogy, as described below.2.4 Graphical Editors for Logical FormsSince many semantic encodings are described as graphs,knowledge editing tools have traditionally been proposed asgraphical editors ?
where concepts are represented as nodesand relations as edges.
Such a ?generic graphical editor?
ispresented for example in [Paley et al, 1997].Conceptual graphs have also been traditionally representedgraphically, and there is a standard graphical encoding forCGs.
Graphical editors for CGs are available (e.g., [Delu-gach, 2001]).While graphical editors are attractive, they suffer fromknown problems of visual languages: they do not scale well(large networks are particularly difficult to edit and under-stand).
Editing graphical representations is often slower thanediting textual representations.
Finally, graphical representa-tions convey too much information, as non-meaningful datamay be inferred from graphical features such as layout offont, which is not constrained by the underlying visual lan-guage.2.5 Generation from CGCGs have been used as an input to text generation in a varietyof systems in the past [Cote and Moulin, 1990], [Bontcheva,1995] and others.In our work, we do not view the CG level as a direct in-put to a generation system.
Instead, we view the CG levelas an ontological representation, lacking communicative in-tention levels, and not linked directly to linguistic considera-tions.
The CG level is justified by its inferencing and queryretrieval capabilities, while taking into account sets, quantifi-cation and nested contexts.Processing is required to link the CG representation level(see Fig.
1) to linguistically motivated rhetorical structures,sentence planning and lexical choice.
In our work, CGs areformally converted to an input to a generation system by atext planner and a lexical chooser, as described below.
Ex-isting generation components for lexical choice and syntac-tic realization based on functional unification are used on theoutput of the text planner.Figure 1: Conceptual Graph in a linear representation.3 Method and ArchitectureWe now present the system we have implemented, which wehave called SAUT (Semantic AUthoring Tool).
Our objectiveis to perform usability studies to evaluate:?
How ontological knowledge in the form of concept andrelation hierarchies is useful for semantic authoring;?
How natural language feedback improves the authoring?
and how feedback in two languages modifies the au-thoring process;?
How user interface functionality improves the speed andaccuracy of the authoring.The architecture of the system is depicted in Fig.
2.The two key components of the system are the knowledgeacquisition system and the editing component.
The knowl-edge acquisition system is used to derive an ontology fromsample texts in a specific domain.
In the editing component,users enter logical expressions on the basis of the ontology.3.1 Knowledge AcquisitionFor the acquisition of the concepts/relations database, we usetwo main sources: Verbnet [Kipper et al, 2000] and WordNet[Miller, 1995].We use the information for bootstrapping concept and rela-tion hierarchies.
Given sample texts in the target domain, weFigure 2: Architecture of the SAUT systemperform shallow syntactic analysis and extract nouns, verbsand adjectives from the text.
Dependency structures for verbsand nouns are also extracted.
We currently perform manuallyanaphora resolution and word sense disambiguation, sinceautomatic methods do not produce accurate enough results.Given the set of nouns and adjectives, we induce the hyper-nym hierarchy from WordNet, resulting in a tree of concepts?
one for each synset appearing in the list of words in thesample texts.1In addition to the concept hierarchy, we derive relationsamong the concepts and predicates by using the Verbnet lexi-cal database [Kipper et al, 2000].
Verbnet supplies informa-tion on the conceptual level, in the form of selectional restric-tions for the thematic roles.These relations allow us to connect the concepts and rela-tions in the derived ontology to nouns, verbs and adjectives.The selectional restrictions in Verbnet refer to the WordNetconceptual hierarchy.
In Verbnet, verbs are classified fol-lowing Levin?s classes [Levin, 1993] and thus its represen-tation is easily adjustable with our verb lexicon [Jing et al,2000], which combined information on argument structure ofverbs from Levin, Comlex [Macleod and Grishman, 1995]and WordNet.
The rich information on argument structureand selectional restrictions can be automatically adopted tothe domain concepts database.
Thus, by connecting a con-cept to a verb, given all the concepts that stand in relation toit in a specific CG (the verb?s arguments and circumstantials)?
our lexical chooser finds the suitable structure (alternation)to map the CG to a syntactic structure.The outcome of this process is useful in the lexical and syn-tactic module of the system due to the flexibility it offers tothe lexical chooser (a general word can be used instead of a1Although hypernym relations in WordNet define a forest oftrees, we connect all trees with a general node.specific word i.e.
vehicles instead of cars, and for the gener-ality of selectional restrictions on verb/adjective arguments.Since there are no Hebrew parallels to WordNet/verbnet,we use a ?naive?
scheme of translating the English LC to He-brew, with manual corrections of specific structures when er-rors are found.Once the knowledge is acquired, we automatically updateda lexical chooser adopted to the domain.
The lexical choosermaps the ontological concepts and relations to nouns, verbsand adjectives in the domain.3.2 The SAUT EditorTo describe the SAUT editor, we detail the process of author-ing a document using the tool.
When the authoring tool isinitiated, the next windows are presented (see Fig.
3):?
Input window?
Global context viewer?
Local context viewer?
CG feedback viewer?
Feedback text viewer?
Generated document viewer.The user operates in the input window.
This window in-cludes three panels:?
Defaults: rules that are enforced by default on the rest ofthe document.
The defaults can be changed while edit-ing.
Defaults specify attribute values which are auto-matically copied to the authored CGs according to theirtype.?
Participants: a list of objects to which the documentrefers.
Each participant is described by an instance (or ageneric) CG, and is given an alias.
The system providesFigure 3: Snapshot of editing state in the SAUT systeman automatic identifier for participants, but these can bechanged by the user to a meaningful identifier.?
Utterances: editing information proposition by proposi-tion.The system provides suggestions to complete expressionsaccording to the context in the form of popup windows.
Inthese suggestion windows, the user can either scroll or choosewith the mouse or by entering the first letters of the desiredword, when the right word is marked by the system, the usercan continue, and the word will be automatically completedby the system.
For example, when creating a new participant,the editor presents a selection window with all concepts inthe ontology that can be instantiated.
If the user chooses theconcept type ?Dog?
the system creates a new object of typedog, with the given identifier.
The user can further enrich thisobject with different properties.
This is performed using the?.?
notation to modify a concept with an attribute.
While theuser enters the instance specification and its initial properties,a feedback text and a conceptual graph in linear form are gen-erated simultaneously.
When the user moves to the next line,the new object is updated on the global context view.
Eachobject is placed in a folder corresponding to its concept type,and will include its instance name and its description in CGlinear form.In the Utterances panel, the author enters propositions in-volving the objects he declared in the participants section.
Tocreate an utterance, the user first specifies the object which isthe topic of the utterance.
The user can choose one of the par-ticipants declared earlier from an identifiers list, or by choos-ing a concept type from a list.
Choosing a concept type willresult in creating a new instance of this concept type.
Everyinstance created in the system will be viewed in the contextviewer.
After choosing an initial object, the user can add ex-pressions in order to add information concerning this object.After entering the initial object in an utterance, the user canpress the dot key which indicates that he wants to enrich thisobject with information.
The system will show the user listof expressions that can add information on this object.
In CGterms, the system will fill the list with items which fall in oneof the following three categories:?
Relations that can be created by the system and their se-lectional restrictions are such that they allow the modi-fied object as a source for the relation.?
Properties that can be added to the concept object suchas name and quantity.?
Concept types that expect relations, the first of whomcan connect to the new concept.
For example the con-cept type ?Eat?
expects a relation ?Agent?
and a relation?Patient.?
The selectional restriction on the destinationof ?Agent?
will be for example ?Animate?.
Thereforethe concept ?Eat?
will appear on the list of an object oftype ?Dog?.The author can modify and add information to the activeobject by pressing the dot key.
An object which itself mod-ifies an object previously entered, can be modified with newrelations, properties and concepts in the same manner.
Theglobal context is updated whenever a new instance is createdin the utterances.
When the author has finished composingthe utterance, the system will update the local context andwill add this information to the generated natural languagedocument.The comma operator (?,?)
is used to define sets in exten-sion.
For example, in Fig.3, the set ?salt and pepper?
iscreated by entering the expression #sa,#pe.
The set itself be-comes an object in the context and is assigned its own identi-fier.The dot notation combined with named variables allowsfor easy and intuitive editing of the CG data.
In addition,the organization of the document as defaults, participants andcontext (local and global) ?
provides an intuitive manner toorganize documents.Propositions, after they are entered as utterances, can alsobe named, and therefore can become arguments for furtherpropositions.
This provides a natural way to cluster large con-ceptual graphs into smaller chunks.The text generation component proceeds from this infor-mation, according to the following steps:?
Pronouns are generated when possible using the localand global context information.?
Referring expression are planned using the competingexpressions from the context information, excluding andincluding information and features of the object in thegenerated text, so the object identity can be resolved bythe reader, but without adding unnecessary information.?
Aggregation of utterances which share certain featuresusing the aggregation algorithm described in [Shaw,1995].Consider the example cooking recipe in Fig.3.
The authoruses the participants section in order to introduce the ingre-dients needed for this recipe.
One of the ingredients is ?sixlarge eggs?.
The author first chooses an identifier name forthe eggs, for example ?eg?.
From the initial list of conceptstypes proposed by the system, we choose the concept type?egg?.
Pressing the dot key will indicate we want to pro-vide the system with further information about the newly cre-ated object.
We choose ?quantity?
from a given list by typ-ing ?qu?.
seeing that the word ?quantity?
was automaticallymarked in the list.
Pressing the space key will automaticallyopen brackets, which indicates we have to provide the systemwith an argument.
A tool tip text will pop to explain the userwhat is the function of the required argument.
After enteringnumber, we will hit the space bar to indicate we have no moreinformation to supply about the ?quantity?
; the brackets willbe automatically closed.
After the system has been told nomore modification will be made on the quantity, the ?egg?object is back to be the active one.
The system marks the ac-tive object in any given time by underline the related word inthe input text.Pressing the dot will pop the list box with the possible mod-ifications for the object.
We will now choose ?attribute?.Again the system will open brackets, and a list of possibleconcepts will appear.
The current active node in the graph is?attribute?.
Among the possible concepts we will choose the?big?
concept, and continue by clicking the enter key (thelexical chooser will map the ?big?
concept to the collocation?large?
appropriate for ?eggs?).
A new folder in the globalcontext view will be added with the title of ?egg?
and willcontain the new instance with its identifier and description asa CG in linear form.Each time a dot or an identifier is entered, the system con-verts the current expression to a CG, maps the CG to a FUFFunctional Description which serves as input to the lexicalchooser; lexical choice and syntactic realization is performed,and feedback is provided in both English and Hebrew.The same generated sentence is shown without context (inthe left part of the screen), and in context (after referenceplanning and aggregation).When generating utterances, the author can refer to an ob-ject from the context by clicking on the context view.
Thisenters the corresponding identifier in the utterance graph.4 EvaluationThe objectives of the SAUT authoring system are to pro-vide the user with a fast, intuitive and accurate way to com-pose semantic structures that represent meaning s/he wants toconvey, then presenting the meaning in various natural lan-guages.
Therefore, an evaluation of these aspects (speed, in-tuitiveness, accuracy and coverage) is required, and we haveconducted an experiment with human subjects to measurethem.
The experiment measures a snapshot of these parame-ters at a given state of the implementation.
In the error analy-sis we have isolated parameters which depend on specificsof the implementation and those which require essential revi-sions to the approach followed by SAUT.4.1 User ExperimentWe have conducted a user experiment, in which ten subjectswere given three to four recipes in English (all taken fromthe Internet) from a total pool of ten.
The subjects had tocompose semantic documents for these recipes using SAUT2.
The ontology and lexicon for the specific domain of cook-ing recipes were prepared in advance, and we have tested thetool by composing these recipes with the system.
The docu-ments the authors prepared are later used as a ?gold standard?
(we refer to them as ?reference documents?).
The experi-ment was managed as follows: first, a short presentation ofthe tool (20 minutes) was given.
Then, each subject recieveda written interactive tutorial which took approximately halfan hour to process.
Finally, each subject composed a set of 3to 4 documents.
The overall time taken for each subject was2.5 hours.4.2 EvaluationWe have measured the following aspects of the system duringthe experiment.Coverage - answers the questions ?can I say everything Imean?
and ?how much of the possible meanings that can beexpressed in natural language can be expressed using the in-put language?.
In order to check the coverage of the tool,we examined the reference documents.
We compared thetext generated from the reference documents with the orig-inal recipes and checked which parts of the information were2All subjects were computer science students.included, excluded or expressed in a partial way with respectto the original.
We counted each of these in number of wordsin the original text, and expressed these 3 counts as a per-centage of the words in the original recipe.
We summed upthe result as a coverage index which combined the 3 counts(correct, missing, partial) with a factor of 70% for the partialcount.The results were checked by two authors independentlyand we report here the average of these two verifications.
Ona total of 10 recipes, containing 1024 words overall, the cov-erage of the system is 91%.
Coverage was uniform acrossrecipes and judges.
We performed error analysis for the re-maining 9% of the un-covered material below.Intuitiveness - to assess the ease of use of the tool,we measured the ?learning curve?
for users first using thesystem, and measuring the time it takes to author a recipe foreach successive document (1st, 2nd, 3rd, 4th).
For 10 usersfirst facing the tool, the time it took to author the documentsis as follows:Document # Average Time to author1st 36 mn2nd 28 mn3rd 22 mn4th 14 mnThe time distribution among 10 users was extremely uni-form.
We did not find variation in the quality of the authoreddocuments across users and across number of document.The tool is mastered quickly, by users with no prior train-ing in knowledge representation or natural language process-ing.
Composing the reference documents (approximately100-words recipes) by the authors took an average of 12minutes.Speed - we measured the time required to compose a docu-ment as a semantic representation, and compare it to the timetaken to translate the same document in a different language.We compare the average time for trained users to author arecipe (14 minutes) with that taken by 2 trained translators totranslate 4 recipes (from English to Hebrew).Semantic Authoring Time Translation Time14 (minutes) 6 (minutes)The comparison is encouraging - it indicates that a tool forsemantic authoring could become cost-effective if it is usedto generate in 2 or 3 languages.Accuracy - We analyzed the errors in the documents pre-pared by the 10 users according to the following breakup:?
Words in the source document not present in the seman-tic form?
Words in the source document presented inaccurately inthe semantic form?
Users?
errors in semantic form that are not included inthe former two parameters.We calculated the accuracy for each document producedby the subjects during the experiment.
Then we comparedeach document with the corresponding reference document(used here as a gold standard).
Relative accuracy of thisform estimates a form of confidence ?
?how sure can theuser be that s/he wrote what s/he meant??
This measurementdepends on the preliminary assumption that for a givenrecipe, any two readers (in the experiment environment ?including the authors), will extract similar information.
Thisassumption is warranted for cooking recipes.
This measuretakes into account the limitations of the tool and reflects thesuccess of users to express all that the tool can express:Document # Accuracy1st 93%2nd 92%3rd 95%4th 90%Accuracy is quite consistent during the experiment ses-sions, i.e., it does not change as practice increases.
The aver-age 92.5% accuracy is quite high.We have categorized the errors found in subjects?
docu-ments in the following manner:?
Content can be accurately expressed with SAUT (usererror)?
Content will be accurately expressed with changes in theSAUT?s lexicon and ontology (ontology deficit)?
Content cannot be expressed in the current implemen-tation, and requires further investigation of the concept(implementation and conceptual limitations)Document # AccuracyUser error 44%Ontology deficit 23%Tool limitations 33%This breakdown indicates that the tool can be improved byinvesting more time in the GUI and feedback quality and byextending the ontology.
The difficult conceptual issues (thosewhich will require major design modifications, or put in ques-tion our choice of formalism for knowledge encoding) repre-sent 33% of the errors ?
overall accounting for 2.5% of thewords in the word count of the generated text.5 AnalysisThe current prototype of SAUT proves the feasibility of se-mantic authoring combined with natural language generation.The system includes a lexical chooser of several hundredverbs and nouns derived from WordNet in a specific domain.The system is easy to use and requires training of less thanone hour.
User interface features make it very fast to enterCGs of the type required for a recipe.
If the documents aregenerated in more than 2 languages, the tool can even becomecost effective at its current level of ergonomy.The current prototype indicates that combining techniquesfrom NLG with User Interfaces techniques from program-ming languages editors results in an efficient knowledge edi-tor.
In future work, we intend to evaluate how to use semanticforms for summarization and inferencing.
We also will evalu-ate how rhetorical information can be managed in the system,by applying the tool to different domains.References[Bateman, 1997] John Bateman.
Enabling technology formultilingual natural language generation: the KPML de-velopment.
Natural Language Engineering, 1(1):1 ?
42,1997.
[Berners-Lee et al, 2001] Tim Berners-Lee, James Hendler,and Ora Lassila.
Semantic web.
Scientific American, 2001.
[Bontcheva, 1995] Kalina Bontcheva.
Generation of multi-lingual eplanations from conceptual graphs.
In Proc.
ofRANLP?97, Batak, Bulgaria, 1995.
[Coch, 1998] J. Coch.
Interactive generation and knowledgeadministration in multimeteo.
In Proc.
of the 9th WorkshopINLG, pages 300?303, Canada, 1998.
[Cote and Moulin, 1990] D. Cote and B. Moulin.
Refin-ing sowa?s con-ceptual graph theory for text genera-tion.
In Proc.
of IEA/AIE90, volume 1, pages 528?537,Charleston, SC, 1990.
[Dale, 1990] Robert Dale.
Generating recipes: An overviewof epicure.
In Michael Zock Robert Dale, Chris Mellish,editor, Current Research in Natural Language Generation,pages 229?255.
Academic Press, New York, 1990.
[Delin et al, 1994] Judy Delin, Anthony Hartley, Ce?cile L.Paris, Donia Scott, and Keith Vander Linden.
ExpressingProcedural Relationships in Multilingual Instructions.
InProc.
of the 7th.
Int.
Workshop on NLG, pages 61 ?
70,1994.
[Delugach, 2001] Harry Delugach.
Charger: A graphicalconceptual graph editor.
In Proc.
of ICCS 2001 CGToolsWorkshop, 2001.
[Elhadad, 1991] Michael Elhadad.
FUF user manual - ver-sion 5.0.
Technical Report CUCS-038-91, University ofColumbia, 1991.
[Elhadad, 1992] Michael Elhadad.
Using Argumentation toControl Lexical Choice: A Functional Unification Imple-mentation.
PhD thesis, Columbia University, 1992.
[Genesereth and Fikes, 1992] M.R.
Genesereth and R.E.Fikes.
Knowledge interchange format, version 3.0 ref-erence manual.
Technical Report Logic-92-1, ComputerScience Department, Stanford University, 1992.
[Goldberg et al, 1994] E. Goldberg, N. Driedger, andR.
Kittredge.
Using natural-language processing to pro-duce weather forecasts.
IEEE Expert, 9(2):45?53, 1994.
[Jing et al, 2000] Hongyan Jing, Yael Dahan Netzer,Michael Elhadad, and Kathleen McKeown.
Integratinga large-scale, reusable lexicon with a natural languagegenerator.
In Proceedings of the 1st INLG, pages 209?216,Mitzpe Ramon, Israel, 2000.
[Kharitonov, 1999] Mark Kharitonov.
Cfuf: A fast inter-preter for the functional unification formalism.
Master?sthesis, BGU, Israel, 1999.
[Kipper et al, 2000] K. Kipper, H. Trang Dang, andM.
Palmer.
Class-based construction of a verb lexicon.In Proceeding of AAAI-2000, 2000.
[Kittredge and Lehrberger, 1982] R. Kittredge andJ.
Lehrberger.
Sublanguage: Studies of Language inRestricted Semantic Domains.
De Gruyter, Berlin, 1982.
[Kukich, 1983] Karen Kukich.
Knowledge-based report gen-eration: A technique for automatically generating naturallanguage reports from databases.
In Proc.
of the 6th Inter-national ACM SIGIR Conference, 1983.
[Levin, 1993] Beth Levin.
English Verb Classes and VerbAlternations: A Preliminary Investigation.
University ofChicago Press, 1993.
[Lin and Pantel, 2001] Dekang Lin and Patrick Pantel.
DIRT@SBT@discovery of inference rules from text.
In Knowl-edge Discovery and Data Mining, pages 323?328, 2001.
[Macleod and Grishman, 1995] C. Macleod and R. Grish-man.
COMLEX Syntax Reference Manual.
ProteusProject, NYU, 1995.
[Miller, 1995] George A. Miller.
Wordnet: a lexical databasefor english.
Commun.
ACM, 38(11):39?41, 1995.
[Netzer, 1997] Yael Netzer.
Design and evaluation of a func-tional input specification language for the generation ofbilingual nominal expressions (hebrew/english).
Master?sthesis, BGU, Israel, 1997.
[Paley et al, 1997] S.M.
Paley, Lowrance, J.D., and P.D.Karp.
A generic knowledge-base browser and editor.
InProc.
of the 1997 National Conference on AI, 1997.
[Paris and Vander Linden, 1996] Ce?cile Paris and Keith Van-der Linden.
DRAFTER: An interactive support toolfor writing multilingual instructions.
IEEE Computer,29(7):49?56, 1996.
[Power and Scott, 1998] Roger Power and Donia Scott.
Mul-tilingual authoring using feedback texts.
In Proc.
ofCOLING-ACL 98, Montreal, Canada, 1998.
[Pulman, 1996] Stephen Pulman.
Controlled language forknowledge representation.
In Proc.
of the 1st Int.
Work-shop on Controlled Language Applications, pages 233 ?242, 1996.
[Reiter and Dale, 1992] Ehud Reiter and Robert Dale.
Afast algorithm for the generation of referring expressions.In Proc.
of the 14th COLING, pages 232?238, Nantes,France, 1992.
[Ro?sner and Stede, 1994] D. Ro?sner and M. Stede.
Generat-ing multilingual documents from a knowledge base: Thetechdoc project.
In Proc.
of COLING?94, pages 339?346,Kyoto, 1994.
[Shaw et al, 1994] J. Shaw, K. Kukich, and K. Mckeown.Practical issues in automatic documentation generation.
InProceeding of the 4th ANLP, pages 7?14, 1994.
[Shaw, 1995] James Shaw.
Conciseness through aggregationin text generation.
In Proc.
of the 33rd conference on ACL,pages 329 ?
331, Morristown, NJ, USA, 1995.
[Sowa, 1987] J. F. Sowa.
Semantic networks.
In S. C.Shapiro, editor, Encyclopedia of Artificial Intelligence 2.John Wiley & Sons, New York, 1987.
[Stede, 1996] Manfred Stede.
Lexical semantics and knowl-edge representation in multilingual sentence generation.PhD thesis, University of Toronto, 1996.
