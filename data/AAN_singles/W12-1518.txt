INLG 2012 Proceedings of the 7th International Natural Language Generation Conference, pages 110?114,Utica, May 2012. c?2012 Association for Computational LinguisticsPlanning Accessible Explanations for Entailments in OWL OntologiesTu Anh T. Nguyen, Richard Power, Paul Piwek, Sandra WilliamsThe Open UniversityMilton Keynes, United Kingdom{t.nguyen,r.power,p.piwek,s.h.williams}@open.ac.ukAbstractA useful enhancement of an NLG system forverbalising ontologies would be a module ca-pable of explaining undesired entailments ofthe axioms encoded by the developer.
Thistask raises interesting issues of content plan-ning.
One approach, useful as a baseline, issimply to list the subset of axioms relevantto inferring the entailment; however, in manycases it will still not be obvious, even to OWLexperts, why the entailment follows.
We sug-gest an approach in which further statementsare added in order to construct a proof tree,with every step based on a relatively simplededuction rule of known difficulty; we also de-scribe an empirical study through which thedifficulty of these simple deduction patternshas been measured.1 IntroductionA practical problem in developing ontologies forthe semantic web is that mistakes are hard to spot.One reason for this lies in the opacity of the stan-dard OWL formalisms, such as OWL/RDF, whichare designed for efficient processing by computerprograms and not for fast comprehension by peo-ple.
Various tools have been proposed to addressthis problem, including not only graphical interfacessuch as Prote?ge?, but NLG (Natural Language Gener-ation) programs that verbalise the axioms of an on-tology as text (Kaljurand and Fuchs, 2007; Schwit-ter and Meyer, 2007; Hart et al, 2008).
Using such atool, a mistaken axiom presented through a sentencelike ?Every person is a movie?
immediately leaps tothe eye.Although there is evidence that verbalisationhelps developers to check individual axioms(Stevens et al, 2011), there remains a more subtleproblem of undesired entailments, often based on in-teractions among axioms.
The difference betweenaxioms and entailments is that whereas axioms arestatements encoded by the developer, entailmentsare statements inferred from axioms by automatedreasoners such as FaCT++ (Tsarkov and Horrocks,2006).
Because reasoning systems interpret state-ments absolutely literally, it is quite common for ap-parently innocuous axioms to lead to absurd conclu-sions such as ?Everything is a person?, ?Nothing isa person?, or indeed ?Every person is a movie?.
Thestandard reasoning algorithms, based on tableau al-gorithms, will compute these entailments efficiently,but they provide no information that helps explainwhy an undesired conclusion was drawn, and hencewhich axiom or axioms need to be corrected.To provide an explanation of an entailment, thefirst step is obviously to determine which axioms arerelevant to the inference.
A set of relevant axiomsis known technically as a justification of the entail-ment, defined as any minimal subset of the ontologyfrom which the entailment can be drawn (Kalyan-pur, 2006).
The minimality requirement here meansthat if any axiom is removed from a justification, theentailment will no longer be inferable.Drawing on Kalyanpur?s work, the most directstrategy for planning an explanation is simply toverbalise the axioms in the justification, followedby the entailment, with no additional content.
Thisstrategy serves as a useful baseline for comparison,and might even be effective for some simple justi-110Entailment Person v Movie Every person is a movie.1.
GoodMovie ?
?hasRating.FourStars 1.
A good movie is anything that only has ratings of four stars.Justification 2.
Domain(hasRating) = Movie 2.
Anything that has a rating is a movie.3.
GoodMovie v StarRatedMovie 3.
Every good movie is a star-rated movie.4.
StarRatedMovie v Movie 4.
Every star-rated movie is a movie.Table 1: An example justification that requires further explanationfications; however, user studies have shown that inmany cases even OWL experts are unable to workout how the conclusion follows from the premiseswithout further explanation (Horridge et al, 2009).This raises two problems of content planning thatwe now address: (a) how we can ascertain that fur-ther explanation is needed, and (b) what form suchexplanation should take.2 Explaining complex justificationsAn example of a justification requiring further ex-planation is shown in Table 1.
Statements are pre-sented in mathematical notation in the middle col-umn (rather than in OWL, which would take up alot more space), with a natural language gloss in theright column.
Since these sentences are handcraftedthey should be more fluent than the output of a ver-baliser, but even with this benefit, it is extremelyhard to see why the entailment follows.The key to understanding this inference lies in thefirst axiom, which asserts an equivalence betweentwo classes: good movies, and things that only haveratings of four stars.
The precise condition for an in-dividual to belong to the second class is that all of itsratings should be four star, and this condition wouldbe trivially satisfied if the individual had no ratingsat all.
From this it follows that people, parrots,parsnips, or in general things that cannot have a rat-ing, all belong to the second class, which is assertedto be equivalent to the class of good movies.
If in-dividuals with no rating are good movies, then byaxioms 3 and 4 they are also movies, so we are leftwith two paradoxical statements: individuals with arating are movies (axiom 2), and individuals withouta rating are movies (the intermediate conclusion justderived).
Since everything that exists must eitherhave some rating or no rating, we are driven to theconclusion that everything is a movie, from which itfollows that any person (or parrot, etc.)
must also bea movie: hence the entailment.
Our target explana-tion for this case is as follows:Every person is a movie because the ontologyimplies that everything is a movie.Everything is a movie because (a) anything thathas a rating is a movie, and (b) anything that hasno rating at all is a movie.Statement (a) is stated in axiom 2 in the justifica-tion.
Statement (b) is inferred because the ontologyimplies that (c) anything that has no rating at allis a good movie, and (d) every good movie is amovie.Statement (d) is inferred from axioms 3 and 4 inthe justification.
Statement (c) is inferred fromaxiom 1, which asserts an equivalence betweentwo classes: ?good movie?
and ?anything that hasas rating only four stars?.
Since the second classtrivially accepts anything that has no rating at all,we conclude that anything that has no rating at allis a good movie.Note that in this or any other intelligible explana-tion, a path is traced from premises to conclusion byintroducing a number of intermediate statements, orlemmas.
Sometimes a lemma merely unpacks partof the meaning of an axiom ?
the part that actuallycontributes to the entailment.
This is clearly whatwe are doing when we draw from axiom 1 the im-plication that all individuals with no ratings are goodmovies.
Alternatively a lemma could be obtained bycombining two axioms, or perhaps even more.
Byintroducing appropriate lemmas of either type, wecan construct a proof tree in which the root node isthe entailment, the terminal nodes are the axioms inthe justification, and the other nodes are lemmas.
Anexplanation based on a proof tree should be easier tounderstand because it replaces a single complex in-ference step with a number of simpler ones.Assuming that some kind of proof tree is needed,the next question is how to construct proof trees thatprovide effective explanations.
Here two conditionsneed to be met: (1) the proof tree should be correct,in the sense that all steps are valid; (2) it should be111accessible, in the sense that all steps are understand-able.
As can be seen, one of these conditions is logi-cal, the other psychological.
Several research groupshave proposed methods for producing logically cor-rect proof trees for description logic (McGuinness,1996; Borgida et al, 1999; Horridge et al, 2010),but explanations planned in this way will not nec-essarily meet our second requirement.
In fact theycould fail in two ways: either they might employ asingle reasoning step that most people cannot fol-low, or they might unduly complicate the text byincluding multiple steps where a single step wouldhave been understood equally well.
We believe thisproblem can be addressed by constructing the prooftree from deduction rules for which the intuitive dif-ficulty has been measured in an empirical study.13 Collecting Deduction RulesFor our purposes, a deduction rule consists of aconclusion (i.e., an entailment) and up to threepremises from which the conclusion logically fol-lows.
Both conclusion and premises are generalisedby using variables that abstract over class and prop-erty names, as shown in Table 2, where for examplethe second rule corresponds to the well-known syl-logism that from ?Every A is a B?
and ?Every B is aC?, we may infer ?Every A is a C?.Our deduction rules were derived through a cor-pus study of around 500 OWL ontologies.
Firstwe computed entailment-justification pairs using themethod described in Nguyen et al (2010), andcollated them to obtain a list of deduction patternsranked by frequency.
From this list, we selected pat-terns that were simple (in a sense that will be ex-plained shortly) and frequent, subsequently addingsome further rules that occurred often as parts ofmore complex deduction patterns, but were not com-puted as separate patterns because of certain limi-tations of the reasoning algorithm.2 The deductionrules required for the previous example are shown1Deduction rules were previously used by Huang for re-constructing machine-generated mathematical proofs; however,these rules were not for description logic based proofs andassumed to be intuitive to people (Huang, 1994).
The out-put proofs were then enhanced (Horacek, 1999) and verbalised(Huang, 1994).2Reasoning services for OWL typically compute only somekinds of entailment, such as subclass and class membershipstatements, and ignore others.in Table 2.
So far, 41 deduction rules have been ob-tained in this way; these are sufficient to generateproof trees for 48% of the justifications of subsump-tion entailments in the corpus (i.e., over 30,000 jus-tifications).As a criterion of simplicity we considered thenumber of premises (we stipulated not more thanthree) and also what is called the ?laconic?
property(Horridge et al, 2008) ?
that an axiom should notcontain information that is not required for the en-tailment to hold.
We have assumed that deductionrules that are simple in this sense are more likely tobe understandable by people; we return to this issuein section 5, which describes an empirical test of theunderstandability of the rules.4 Constructing Proof TreesA proof tree can be defined as any tree linking theaxioms of a justification (terminal nodes) to an en-tailment (root node), in such a way that every localtree (i.e., every node and its children) correspondsto a deduction rule.
This means that if the entail-ment and justification already correspond to a de-duction rule, no further nodes (i.e., lemmas) needto be added.
Otherwise, a proof can be sought byapplying the deduction rules, where possible, to theterminal nodes, so introducing lemmas and grow-ing the tree bottom-up towards the root.
Exhaus-tive search using this method may yield zero, one ormultiple solutions ?
e.g., for our example two prooftrees were generated, as depicted in Figure 1.35 Measuring understandabilityTo investigate the difficulty of deduction rules em-pirically, we have conducted a survey in which 43participants (mostly university staff and students un-familiar with OWL) were shown the premises of therule, expressed as English sentences concerning fic-titious entities, and asked to choose the correct con-clusion from four alternatives.
They were also askedto rate the difficulty of this choice on a five-pointscale.
For instance, in one problem the premises3In the current implementation, the proof tree can also be de-veloped by adding lemmas that unpack part of the meaning ofan axiom, using the method proposed by Horridge et al(2008).These steps in the proof are not always obvious, so their under-standability should also be measured.112ID Deduction Rule Example Success Rate1 ?r.?
v C Anything that has no ratings at all is a movie.
65%?r.> v C Anything that has a rating is a movie.?
> v C ?
Everything is a movie.2 C v D Anything that has no ratings at all is a good movie.
88%D v E Every good movie is a movie.?
C v E ?
Anything that has no ratings at all is a movie.3 C ?
?r.D A good movie is anything that only has ratings of four stars.
??
?r.?
v C ?
Anything that has no ratings at all is a good movie.Table 2: Deduction rules for the example in Table 1Figure 1: Proof trees generated by our current systemFigure 2: Results of the empirical study.
In our difficultyscale, 1 means ?very easy?
and 5 means ?very difficult?were ?Every verbeeg is a giantkin; no giantkin isa verbeeg.?
; to answer correctly, participants had totick ?Nothing is a verbeeg?
and not ?Nothing is a gi-antkin?.So far 9/41 deduction rules have been measuredin this way.
Figure 2 shows the success rates and themeans of difficulty of those rules.
For most prob-lems the success rates were around 80%, confirm-ing that the rules were understandable, although ina few cases performance fell to around 50%, sug-gesting that further explanation would be needed.The study also indicates a statistically significant re-lationship between the accuracy of the participants?performance and their perceptions of difficulty (r =0.82, p < 0.01).
Two of the three rules in Table 2were measured in this way.
The third rule has notbeen tested yet; however, its success rate is expectedto be very low as it was proved to be a very difficultinference (Horridge et al, 2009).6 ConclusionThis paper has reported our work in progress on con-tent planning for explanations of entailments.
Themain steps involved in the planning process are sum-113Figure 3: Our approach for the content planning.
E, J, Pnare entailments, justifications and proofs respectively; d1and d2 are difficulty scores and d2 ?
d1marised in Figure 3.
We have focused on one as-pect: the introduction of lemmas that mediate be-tween premises and conclusion, so organising theproof into manageable steps.
Lemmas are derivedby applying deduction rules collected through a cor-pus study on entailments and their justifications.Through a survey we have measured the difficulty ofsome of these rules, as evidenced by performance onthe task of choosing the correct conclusion for givenpremises.
These measures should indicate whichsteps in a proof are relatively hard, and thus perhapsin need of further elucidation, through special strate-gies that can be devised for each problematic rule.Our hypothesis is that these measures will also allowan accurate assessment of the difficulty of a candi-date proof tree, so providing a criterion for choos-ing among alternatives ?
e.g., by using the successrates as an index of difficulty, we can sum the in-dex over a proof tree to obtain a simple measureof its difficulty.
Our verbaliser currently translatesOWL statements literally, and needs to be improvedto make sure any verbalisations do not give rise tounwanted presuppositions and Gricean implicatures.AcknowledgmentsThis research was undertaken as part of the ongo-ing SWAT project (Semantic Web Authoring Tool),which is supported by the UK Engineering andPhysical Sciences Research Council (EPSRC).
Wethank our colleagues and the anonymous viewers.ReferencesAlexander Borgida, Enrico Franconi, Ian Horrocks, Deb-orah L. McGuinness, and Peter F. Patel-Schneider.1999.
Explaining ALC Subsumption.
In DL 1999,International Workshop on Description Logics.Glen Hart, Martina Johnson, and Catherine Dolbear.2008.
Rabbit: developing a control natural languagefor authoring ontologies.
In ESWC 2008, EuropeanSemantic Web Conference, pages 348?360.Helmut Horacek.
1999.
Presenting Proofs in a Human-Oriented Way.
In CADE 1999, International Confer-ence on Automated Deduction, pages 142?156.Matthew Horridge, Bijan Parsia, and Ulrike Sattler.2008.
Laconic and Precise Justifications in OWL.
InISWC 2008, International Semantic Web Conference,pages 323?338.Matthew Horridge, Bijan Parsia, and Ulrike Sattler.2009.
Lemmas for Justifications in OWL.
In DL 2009,International Workshop on Description Logics.Matthew Horridge, Bijan Parsia, and Ulrike Sattler.2010.
Justification Oriented Proofs in OWL.
In ISWC2010, International Semantic Web Conference, pages354?369.Xiaorong Huang.
1994.
Human Oriented Proof Presen-tation: A Reconstructive Approach.
Ph.D. thesis, TheUniversity of Saarbru?cken, Germany.Kaarel Kaljurand and Norbert Fuchs.
2007.
Verbaliz-ing OWL in Attempto Controlled English.
In OWLED2007, International Workshop on OWL: Experiencesand Directions.Aditya Kalyanpur.
2006.
Debugging and repair of OWLontologies.
Ph.D. thesis, The University of Maryland,US.Deborah Louise McGuinness.
1996.
Explaining reason-ing in description logics.
Ph.D. thesis, The State Uni-versity of New Jersey, US.Tu Anh T. Nguyen, Paul Piwek, Richard Power, and San-dra Williams.
2010.
Justification Patterns for OWLDL Ontologies.
Technical Report TR2011/05, TheOpen University, UK.Rolf Schwitter and Thomas Meyer.
2007.
Sydney OWLSyntax - towards a Controlled Natural Language Syn-tax for OWL 1.1.
In OWLED 2007, InternationalWorkshop on OWL: Experiences and Directions.Robert Stevens, James Malone, Sandra Williams,Richard Power, and Allan Third.
2011.
Automatinggeneration of textual class definitions from OWL toEnglish.
Journal of Biomedical Semantics, 2(S 2:S5).Dmitry Tsarkov and Ian Horrocks.
2006.
FaCT++ De-scription Logic Reasoner: System Description.
In IJ-CAR 2006, International Joint Conference on Auto-mated Reasoning, pages 292?297.114
