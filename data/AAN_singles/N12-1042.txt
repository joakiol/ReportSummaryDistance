2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 382?385,Montre?al, Canada, June 3-8, 2012. c?2012 Association for Computational LinguisticsTowards Using EEG to Improve ASR AccuracyYun-Nung Chen, Kai-Min Chang, and Jack MostowProject LISTEN (http://www.cs.cmu.edu/?listen)School of Computer Science, Carnegie Mellon University5000 Forbes Avenue, Pittsburgh, PA 15213-3891, USA{yvchen,kkchang,mostow}@cs.cmu.eduAbstractWe report on a pilot experiment to improve the per-formance of an automatic speech recognizer (ASR)by using a single-channel EEG signal to classify thespeaker?s mental state as reading easy or hard text.We use a previously published method (Mostow etal., 2011) to train the EEG classifier.
We use its prob-abilistic output to control weighted interpolation ofseparate language models for easy and difficult read-ing.
The EEG-adapted ASR achieves higher accu-racy than two baselines.
We analyze how its perfor-mance depends on EEG classification accuracy.
Thispilot result is a step towards improving ASR moregenerally by using EEG to distinguish mental states.1 IntroductionHumans use speech to communicate what?s on theirmind.
However, until now, automatic speech recogniz-ers (ASR) and dialogue systems have had no direct wayto take into account what is going on in a speaker?smind.
Some work has attempted to infer cognitivestates from volume and speaking rate to adapt languagemodeling (Ward and Vega, 2009) or from query clicklogs (Hakkani-Tu?r et al, 2011) to detect domains.
Anew way to address this limitation is to infer mental statesfrom electroencephalogram (EEG) signals.EEG is a voltage signal that can be measured on thesurface of the scalp, arising from large areas of coordi-nated neural activity.
This neural activity varies as a func-tion of development, mental state, and cognitive activity,and EEG can measurably detect such variation.Recently, a few companies have scaled back medicalgrade EEG technology to create portable EEG headsetsthat are commercially available and simple to use.
TheNeuroSky MindSetTM (2009), for example, is an audioheadset equipped with a single-channel EEG sensor.
Itmeasures the voltage between an electrode that rests onthe forehead and electrodes in contact with the ear.
Un-like the multi-channel electrode nets worn in labs, thesensor requires no gel or saline for recording, and re-quires no expertise to wear.
Even with the limitationsof recording from only a single sensor and working withuntrained users, Furthermore, Mostow et al(2011) usedits output signal to distinguish easy from difficult reading,achieving above-chance accuracy.
Here we build on thatwork by using the output of such classifiers to adapt lan-guage models for ASR and thereby improve recognitionaccuracy.The most similar work is Jou and Schultz?s (2008) useof electromyographic (EMG) signals generated by humanarticulatory muscles in producing speech.
They showedthat augmenting acoustic features with these EMG fea-tures can achieve rudimentary silent speech detection.Pasley et al (2012) used electrocorticographic (ECoG)recordings from nonprimary auditory cortex in the humansuperior temporal gyrus to reconstruct acoustic informa-tion in speech sounds.
Our work differs from these effortsin that we use a consumer-grade single-channel EEG sen-sor measuring frontal lobe activities, and that we use thedetected mental state just to help improve ASR perfor-mance rather than to dictate or reconstruct speech, whichare much harder tasks.Section 2 describes how to use machine learning to dis-tinguish mental states associated with easy and difficultreadings.
Section 3 describes how we use EEG classifieroutput to adapt ASR language models.
Section 4 uses anoracle simulation to show how increasing EEG classifieraccuracy will affect ASR accuracy.
Section 5 concludes.2 Mental State Classification Using EEGWe use training and testing data from Mostow et al?s(2011) experiment, which presented text passages, onesentence at a time, to 10 adults and 11 nine- to ten-year-olds wearing a Neurosky MindsetTM (2009).
They readthree easy and three difficult texts aloud, in alternating382order.
The ?easy?
passages were from texts classified bythe Common Core Standards1 at the K-1 level.
The ?diffi-cult?
passages were from practice materials for the Grad-uate Record Exam2 and the ACE GED test3.
Across thereading conditions, passages ranged from 62 to 83 wordslong.
Although instructed to read the text aloud, the read-ers (especially children) did not always read correctly orfollow the displayed sentences.Following Mostow et al (2011), we trained binary lo-gistic regression classifiers to estimate the probability thatan EEG signal is associated with reading an easy (or diffi-cult) sentence.
As features for logistic regression we usedthe streams of values logged by the MindSet:1.
The raw EEG signal, sampled at 512 Hz2.
A filtered version of the raw signal, also sampled at512 Hz, which is raw signal smoothed over a win-dow of 2 seconds3.
Proprietary ?attention?
and ?meditation?
measures,reported at 1 Hz4.
A power spectrum of 1Hz bands from 1-256 Hz, re-ported at 8 Hz5.
An indicator of signal quality, reported at 1 HzHead movement or system instability led to missing orpoor-quality EEG data for some utterances, which we ex-cluded in order to focus on utterances with clear acous-tic and EEG signals.
The features for each utteranceconsisted of measures 1-4, averaged over the utterance,excluding the 15% of observations where measure 5 re-ported poor signals.
After filtering, the data includes 269utterances from adults and 243 utterances from children,where 327 utterances are for the easy passages and 185utterances are for the difficult passages.
To balance theclasses, we used the undersampling method for training.We trained a reader-specific classifier on each reader?sdata from all but one text passage, tested it on eachsentence in the held-out passage, performed this proce-dure for each passage, and averaged the results to cross-validate accuracy within readers.
We computed classifi-cation accuracy as the percentage of utterances classifiedcorrectly.
Classification accuracy for adults?, children?s,and total oral reading was 71.49%, 58.74%, and 65.45%respectively.
A one-tailed t-test, with classification accu-racy on an utterance as the random variable, showed thatEEG classification was significantly better than chance.3 Language Model Adaptation for ASRTraditional ASR decodes a word sequence W ?
from theacoustic model and language model as below:1http://www.corestandards.org2http://majortests.com/gre/reading comprehension.php3http://college.cengage.com:80/devenglish/resources/readingace/studentsW ?
= argmaxW P (W | A) (1)= argmaxWP (A | W ) ?
P (W )P (A)To incorporate EEG, we include mental state N as an ad-ditional observation in the decoding procedure:W ?
= argmaxW P (W | A,N) (2)= argmaxWP (A | W ) ?
P (W | N)P (A)The six passages use a vocabulary of 430 distinctwords.
To evaluate the impact on ASR accuracy of us-ing EEG to adapt language models, we needed acousticmodels appropriate for the speakers.
For adult speech, weused the US English HUB4 Acoustic Model from CMUSphinx.
For children?s speech, we used Project LISTEN?sacoustic models trained on children?s oral reading.We used separate trigram language models (with bi-gram and unigram backoff) for easy and difficult text ?EasyLM, trained on the three easy passages, and Diffi-cultLM, trained on the three difficult passages.
Both lan-guage models used the same lexicon, consisting of the430 words in all six target passages.
All experiments usedthe same ASR parameter values.As a gold standard, all utterances were manually tran-scribed by a native English speaker.
To measure ASR per-formance, we computed Word Accuracy (WACC) as thenumber of words recognized correctly minus insertionsdivided by number of words in the reference transcriptsfor each reader, and averaged them.Then we can adapt the language model to estimateP (W | N) using mental state information.
Using theEEG classifier described in Section 2, we adapted the lan-guage model separately for each utterance, using threetypes of language model adaptation: hard selection, softselection, and combination with ASR output.3.1 Hard Selection of Language ModelsGiven the probabilistic estimate that a given utterancewas easy or difficult (SEasy(N) and SDifficult(N)), hard se-lection simply picks EasyLM if the utterance was likelierto be easy, or DifficultLM otherwise:PHard(W | N) = IC(N) ?
PEasy(W ) (3)+ (1 ?
IC(N)) ?
PDiff(W ).Here IC(N) = 1 if SEasy(N) > SDifficult(N), andPEasy(W ) and PDiff(W ) are the probability of word W inEasyLM and DifficultLM, respectively.
For comparison,the Random Pick baseline randomly picks either EasyLMor DifficultLM:383WACCAdult ChildEasy Difficult All Easy Difficult All(a) Baseline 1: Random Pick 54.5 51.2 53.8 32.8 14.7 30.6(b) EEG-based: Hard Selection 57.6 49.4 52.7 36.4 17.0 32.8(c) Baseline 2: Equal Weight 63.2 59.9 56.5 37.3 19.5 33.4(d) EEG-based: Soft Selection w/o smoothing 57.2 48.8 52.4 35.8 17.2 32.5(e) EEG-based: Soft Selection w/ smoothing 66.0 62.3 64.2 39.8 22.7 36.2(f) Baseline 3: Weight from ASR (?
= 0) 63.8 60.6 61.5 39.2 20.0 35.0(g) Weight from ASR and EEG (?
= 0.5) 64.5 63.4 63.5 39.2 21.9 36.0Table 1: ASR performance of proposed approaches using EEG-based classification of mental states.PRandom(W ) = IR ?
PEasy(W ) (4)+ (1 ?
IRandom) ?
PDiff(W ).Here IR is randomly set to 0 or 1.3.2 Soft Selection of Language ModelsMental state classification based on EEG is imperfect,and using only the corresponding language model (Ea-syLM or DifficultLM) to decode the target utterance is li-able to perform worse when the classifier is wrong.
Thus,we use the classifier?s probabilistic estimate that the ut-terance is easy (or difficult) as interpolation weights tolinearly combine EasyLM and DifficultLM:PSoft(W | N) = wEasy(N) ?
PEasy(W ) (5)+ wDiff(N) ?
PDiff(W ).Here wEasy(N) and wDiff(N) are from classifier?s output.wEasy(N) = SEasy(N), wDiff(N) = SDiff(N) (6)Additionally, we can adjust the range of weights bysmoothing the probability outputted by the EEG classi-fier:wEasy(N) =?
+ SEasy(N)2?
+ 1, (7)wDiff(N) =?
+ SDiff(N)2?
+ 1Here SEasy(N) (or SDiff(N)) is the classifier?s probabilis-tic estimate that the sentence is easy (or difficult) and?
is the smoothing weight, which we set to 0.5.
Af-ter smoothing the probabilities, wEasy(N) and wDiff(N)each lie within the interval [0.25, 0.75], and wEasy(N) +wDiff(N) = 1.
That is, Soft Selection with smoothing in-terpolates the two language models, but assigns a weightof at least 0.25 to each one to reduce the impact of EEGclassifier errors.
Notice that ?
= 0 is equivalent to EEGSoft Selection without smoothing.For comparison, the Equal Weight baseline interpo-lates EasyLM and DifficultLM with equal weights:PEqual(W ) = 0.5 ?
PEasy(W ) + 0.5 ?
PDiff(W ) (8)3.3 Combination with ASR OutputGiven the ASR results from the Equal Weight baseline,we can derive S?Easy(N) as:S?Easy(N) = ?
?
SEasy(N) (9)+ (1 ?
?)
?PEasy(W0)PEasy(W0) + PDiff(W0)Here we can estimate S?Easy(N) based on the classifier?soutput and the probability of the recognized words W0 inEasyLM.
We can derive S?Diff(N) in the same way.
Thenwe can use (5) and (7) to re-decode the utterances by us-ing S?Easy(N) and S?Diff(N).
Here ?
is a linear interpola-tion weight, where we set to 0.5 to give equal weights toASR output and EEG.
For comparison, the ASR baselineuses weights from only the ASR results, where ?
= 0.Notice that the case of ?
= 1 is equivalent to EEG SoftSelection with smoothing.3.4 Results of Proposed ApproachesTable 1 shows the performance of our proposed ap-proaches and the corresponding baselines as measured byWACC.
According to one-tailed t-tests with word accu-racy of an utterance as the random variable, the resultsin boldface are significantly better tgan their respectivebaselines (p ?
0.05).Hard Selection (row b) outperforms the Random Pickbaseline (row a).
Soft Selection without smoothing (rowd) has similar performance as Hard Selection because theclassifier often outputs probability estimates that are ei-ther 1 or 0.
However, Soft Selection with smoothing (rowe) outperforms the Equal Weight baseline (row c).
TheWeight from ASR baseline (row f) is better than the otherbaselines.
Weight from ASR and EEG (row g) can fur-ther improve performance, but it?s not better than SoftSelection with smoothing (row e) - evidence that EEGgives good estimation for choosing language models.
Inshort, Table 1 shows that using EEG to choose betweenEasyLM and DifficultLM achieves higher ASR accuracythan the baselines that do not use EEG.Comparing the first two baselines, the Equal Weightbaseline (row c) outperforms the Random Pick baseline3840102030405060700 10 20 30 40 50 60 70 80 90 100Easy utt.Difficult utt.All utt.Predicted WACC (%)Simulated Accuracy of Classification (%)(a) Adult0102030405060700 10 20 30 40 50 60 70 80 90 100Easy utt.Difficult utt.All utt.Predicted WACC (%)Simulated Accuracy of Classification (%)(b) ChildFigure 1: The simulated accuracy graphs plot the predicted ASR word accuracy against the level of EEG classification accuracysimulated by an oracle.
(row a) in every column, because the loss in ASR accu-racy from picking the wrong language model outweighsthe improvement from picking the right one.
Similarly,EEG-based Soft Selection with smoothing (row e) out-performs EEG-based Hard Selection (row b) in every col-umn because the interpolated language model is morerobust to EEG classification error.
The third base-line,Weight from ASR (row f) depends solely on ASR resultsto estimate weights; it performs better than other base-lines, but not as well as EEG-based Soft Selection withsmoothing (row e).
That is, using EEG alone can weightthe two language models better than ASR alone.4 Oracle SimulationTo explore the relationship between EEG classifier ac-curacy and the effect of EEG-based adaptation on ASRaccuracy, we simulate different classification accuraciesand used Hard Selection to predict the resulting ASR ac-curacy by selecting between the ASR output from Ea-syLM and DifficultLM according to the simulated clas-sifier accuracy.
We use the resulting Word Accuracy topredict ASR performance at that level of EEG classifieraccuracy.Figure 1 plots predicted ASR WACC against simulatedEEG classification accuracy.
As expected, the predictedASR accuracy increases as EEG classification accuracyincreases, for both groups (adults and children) and bothlevels of difficulty (easy and difficult).
However, Figure1a and 1b shows that WACC was much lower for childrenthan for adults, especially on difficult utterances, whereeven 100% simulated EEG classifier accuracy achievesbarely 20% WACC.
One explanation is that on difficultsentences, children produced reading mistakes and/ oroff-task speech.
In contrast, adults read better and stayedon task.
Not only is predicted ASR accuracy higher onadults?
reading, it improves substantially as simulatedEEG classifier accuracy increases.5 ConclusionThis paper shows that classifying EEG signals from an in-expensive single-channel device can help adapt languagemodels to significantly improve ASR performance.
Aninterpolated language model smoothed to compensate forclassification errors yielded the best performance.
ASRperformance depended on the accuracy of mental stateclassification.
Future work includes improving EEG clas-sification accuracy, detecting other relevant mental states,such as emotion, and improving ASR by using word-levelEEG classification.
A neurologically-informed ASR maybetter capture what people intend to communicate, andaugment acoustic input with non-verbal cues to ASR ordialogue systems.AcknowledgementsThis work was supported by the Institute of EducationSciences, U.S. Department of Education, through GrantR305A080628 to Carnegie Mellon University.
Any opin-ions, findings, and conclusions or recommendations ex-pressed in this publication are those of the authors and donot necessarily reflect the views or official policies, eitherexpressed or implied of the Institute or the U.S. Depart-ment of Education.
We thank the students, educators, andLISTENers who helped create our data, and the reviewersfor their helpful comments.ReferencesHakkani-Tn?r, D., Tur, G., Heck, L., and Shriberg, E. 2011.Bootstrapping domain detection using query click logs fornew domains Proceedings of InterSpeech, 709-712.Jou, S.-C. S. and Schultz, T.. 2008.
Ears: ElectromyograpicalAutomatic Recognition of Speech.
Proceedings of Biosig-nals, 3-12.Mostow, J., Chang, K.-M., and Nelson, J.
2011.
Toward Ex-ploiting EEG Input in a Reading Tutor.
Proceedings of the15th International Conference on Artificial Intelligence inEducation, 230-237.NeuroSky 2009.
NeuroSky?s SenseTM Meters and Detection ofMental State: Neurisky, Inc.Pasley, B. N. and et al 2012.
Reconstructing speech from au-ditory cortex.
PLos Biology, 10(1), 1-13.Ward, N. G. and Vega, A.
2009.
Towards the use of cognitivestates in language modeling.
Proceedings of ASRU, 323-326.385
