From Visual Attributes to Adjectivesthrough Decompositional Distributional SemanticsAngeliki Lazaridou Georgiana Dinu?
Adam Liska Marco BaroniCenter for Mind/Brain SciencesUniversity of Trento{angeliki.lazaridou|georgiana.dinu|adam.liska|marco.baroni}@unitn.itAbstractAs automated image analysis progresses, thereis increasing interest in richer linguistic an-notation of pictures, with attributes of ob-jects (e.g., furry, brown.
.
. )
attracting mostattention.
By building on the recent ?zero-shot learning?
approach, and paying atten-tion to the linguistic nature of attributes asnoun modifiers, and specifically adjectives,we show that it is possible to tag imageswith attribute-denoting adjectives even whenno training data containing the relevant an-notation are available.
Our approach relieson two key observations.
First, objects canbe seen as bundles of attributes, typically ex-pressed as adjectival modifiers (a dog is some-thing furry, brown, etc.
), and thus a functiontrained to map visual representations of ob-jects to nominal labels can implicitly learnto map attributes to adjectives.
Second, ob-jects and attributes come together in pictures(the same thing is a dog and it is brown).We can thus achieve better attribute (and ob-ject) label retrieval by treating images as ?vi-sual phrases?, and decomposing their linguis-tic representation into an attribute-denotingadjective and an object-denoting noun.
Ourapproach performs comparably to a methodexploiting manual attribute annotation, it out-performs various competitive alternatives inboth attribute and object annotation, and it au-tomatically constructs attribute-centric repre-sentations that significantly improve perfor-mance in supervised object recognition.?
Current affiliation: Thomas J. Watson Research Center,IBM, gdinu@us.ibm.com1 IntroductionAs the quality of image analysis algorithms im-proves, there is increasing interest in annotating im-ages with linguistic descriptions ranging from sin-gle words describing the depicted objects and theirproperties (Farhadi et al., 2009; Lampert et al.,2009) to richer expressions such as full-fledged im-age captions (Kulkarni et al., 2011; Mitchell et al.,2012).
This trend has generated wide interest in lin-guistic annotations beyond concrete nouns, with therole of adjectives in image descriptions receiving, inparticular, much attention.Adjectives are of special interest because of theircentral role in so-called attribute-centric image rep-resentations.
This framework views objects as bun-dles of properties, or attributes, commonly ex-pressed by adjectives (e.g., furry, brown), and usesthe latter as features to learn higher-level, seman-tically richer representations of objects (Farhadi etal., 2009).1 Attribute-based methods achieve bettergeneralization of object classifiers with less train-ing data (Lampert et al., 2009), while at the sametime producing semantic representations of visualconcepts that more accurately model human se-1In this paper, we assume that, just like nouns are the lin-guistic counterpart of visual objects, visual attributes are ex-pressed by adjectives.
An informal survey of the relevant litera-ture suggests that, when attributes have linguistic labels, theyare indeed mostly expressed by adjectives.
There are someattributes, such as parts, that are more naturally expressed byprepositional phrases (PPs: with a tail).
Interestingly, Dinu andBaroni (2014) showed that the decomposition function we willadopt here can derive both adjective-noun and noun-PP phrases,suggesting that our approach could be seamlessly extended tovisual attributes expressed by noun-modifying PPs.183Transactions of the Association for Computational Linguistics, vol.
3, pp.
183?196, 2015.
Action Editor: Patrick Pantel.Submission batch: 9/2014; Revision batch 1/2015; Revision batch 3/2015; Published 3/2015.c?2015 Association for Computational Linguistics.
Distributed under a CC-BY-NC-SA 4.0 license.mantic intuition (Silberer et al., 2013).
Moreover,automated attribute annotation can facilitate finer-grained image retrieval (e.g., searching for a rockybeach rather than a sandy beach) and provide thebasis for more accurate image search (for examplein cases of visual sense disambiguation (Divvala etal., 2014), where a user disambiguates their query bysearching for images of wooden cabinet as furnitureand not just cabinet, which can also mean council).Classic attribute-centric image analysis requires,however, extensive manual and often domain-specific annotation of attributes (Vedaldi et al.,2014), or, at best, complex unsupervised image-and-text-mining procedures to learn them (Berg etal., 2010).
At the same time, resources with high-quality per-image attribute annotations are limited;to the best of our knowledge, coverage of all pub-licly available datasets containing non-class specificattributes does not exceed 100 attributes,2 ordersof magnitude smaller than the equivalent object-annotated datasets (Deng et al., 2009).
Moreover,many visual attributes currently available (e.g., 2D-boxy, furniture leg), albeit visually meaningful, donot have straightforward linguistic equivalents, ren-dering them inappropriate for applications requir-ing natural linguistic expressions, such as the searchscenarios considered above.A promising way to limit manual attribute anno-tation effort is to extend recently proposed zero-shotlearning methods, until now applied to object recog-nition, to the task of labeling images with attribute-denoting adjectives.
The zero-shot approach relieson the possibility to extract, through distributionalmethods, semantically effective vector-based wordrepresentations from text corpora, on a large scaleand without supervision (Turney and Pantel, 2010).In zero-shot learning, training images labeled withobject names are also represented as vectors (of fea-tures extracted with standard image-analysis tech-niques), which are paired with the vectors repre-senting the corresponding object names in language-based distributional semantic space.
Given such2The attribute datasets we are aware of are the ones ofFarhadi et al.
(2010), Ferrari and Zisserman (2007) and Rus-sakovsky and Fei-Fei (2010), containing annotations for 64, 7and 25 attributes, respectively.
(This count excludes the SUNAttributes Database (Patterson et al., 2014), whose attributescharacterize scenes rather than concrete objects.
)?400 ?300 ?200 ?100 0 100 200 300?150?100?50050100150 puppycutefurrycarmetallicplastic birdfeatheredwild?150 ?100 ?50 0 50 100?250?200?150?100?50050100150200 puppycutefurrycarmetallicplasticbirdfeatheredwildFigure 1: t-SNE (Van der Maaten and Hinton, 2008) visu-alization of 3 objects together with the 2 nearest attributesin our visual space (left), and of the corresponding nounsand adjectives in linguistic space (right).paired training data, various algorithms (Socher etal., 2013; Frome et al., 2013; Lazaridou et al., 2014)can be used to induce a cross-modal projection ofimages onto linguistic space.
This projection is thenapplied to map previously unseen objects to the cor-responding linguistic labels.
The method takes ad-vantage of the similarities in the vector space topolo-gies of the two modalities, allowing informationpropagation from the limited number of objects seenin training to virtually any object with a vector-basedlinguistic representation.To adapt zero-shot learning to attributes, we relyon their nature as (salient) properties of objects, andon how this is reflected linguistically in modifier re-lations between adjectives and nouns.
We build onthe observation that visual and linguistic attribute-adjective vector spaces exhibit similar structures:The correlation ?
between the pairwise similari-ties in visual and linguistic space of all attributes-adjectives from our experiments is 0.14 (significantat p < 0.05).3 While the correlation is smallerthan for object-noun data (0.23), we conjecture itis sufficient for zero-shot learning of attributes.
Wewill confirm this by testing a cross-modal projectionfunction from attributes, such as colors and shapes,onto adjectives in linguistic semantic space, trainedon pre-existing annotated datasets covering less than100 attributes (Experiment 1).We proceed to develop an approach achievingequally good attribute-labeling performance withoutmanual attribute annotation.
Inspired by linguisticand cognitive theories that characterize objects as at-tribute bundles (Murphy, 2002), we hypothesize thatwhen we learn to project images of objects to thecorresponding noun labels, we implicitly learn to3In this paper, we report significance at ?
= 0.05 threshold.184?800 ?600 ?400 ?200 0 200 400 600 800?1000?800?600?400?2000200400600800only fruitsparkling winesmooth pastevibrant redmobile visitherbal liqueurrich chocolateclear honeyorange liqueurmixed fruitnew orangeorangeirish creamliqueurwhite rumFigure 2: Images tagged with orange and liqueur aremapped in linguistic space closer to the vector of thephrase orange liqueur than to the orange or liqueur vec-tors (t-SNE visualization) (the figure also shows the near-est neighbours of phrase, adjective and noun in linguis-tic space).
The mapping is trained using solely noun-annotated images.associate the visual properties/attributes of the ob-jects to the corresponding adjectives.
As an exam-ple, Figure 1 (left) displays the nearest attributes ofcar, bird and puppy in the visual space and, inter-estingly, the relative distance between the noun de-noting objects and the adjective denoting attributesis also preserved in the linguistic space (right).We further observe that, as also highlighted byrecent work in object recognition, any object in animage is, in a sense, a visual phrase (Sadeghi andFarhadi, 2011; Divvala et al., 2014), i.e., the objectand its attributes are mutually dependent.
For exam-ple, we cannot visually isolate the object drum fromattributes such as wooden and round.
Indeed, withinour data, in 80% of the cases the projected imageof an object is closer to the semantic representationof a phrase describing it than to either the object orattribute labels.
See Figure 2 for an example.Motivated by this observation, we turn to recentwork in distributional semantics defining a vectordecomposition framework (Dinu and Baroni, 2014)which, given a vector encoding the meaning of aphrase, aims at decoupling its constituents, produc-ing vectors that can then be matched to a sequenceof words best capturing the semantics of the phrase.We adopt this framework to decompose image rep-resentations projected onto linguistic space into anadjective-noun phrase.
We show that the methodyields results comparable to those obtained when us-ing attribute-labeled training data, while only requir-ing object-annotated data.
Interestingly, this decom-positional approach also doubles the performanceof object/noun annotation over the standard zero-shot approach (Experiment 2).
Given the positiveresults of our proposed method, we conclude withan extrinsic evaluation (Experiment 3); we showthat attribute-centric representations of images cre-ated with the decompositional approach boost per-formance in an object classification task, supportingclaims about its practical utility.In addition to contributions to image annotation,our work suggests new test beds for distributionalsemantic representations of nouns and associatedadjectives, and provides more in-depth evidence ofthe potential of the decompositional approach.2 General experimental setup2.1 Cross-Modal MappingOur approach relies on cross-modal mappingfrom a visual semantic space V, populated withvector-based representations of images, onto alinguistic (distributional semantic) space W of wordvectors.
The mapping is performed by first inducinga function fproj : Rd1 ?
Rd2 from data points(vi, wi), where vi ?
Rd1 is a vector representationof an image tagged with an object or an attribute(such as dog or metallic), and wi ?
Rd2 is thelinguistic vector representation of the correspondingword.
The mapping function can subsequently beapplied to any given image vi ?
V to obtain itsprojection w?i ?W onto linguistic space:w?i = fproj(vi)Specifically, we consider two mapping methods.
Inthe RIDGE regression approach, we learn a linearfunction Fproj ?
Rd2?d1 by solving the Tikhonov-Phillips regularization problem, which minimizesthe following objective:||W Tr ?
FprojV Tr||22 ?
||?Fproj ||22,where W Tr and V Tr are obtained by stacking theword vectors wi and corresponding image vectorsvi, from the training set.44The parameter ?
is determined through cross-validation onthe training data.185Second, motivated by the success of CanonicalCorrelations Analysis (CCA) (Hotelling, 1936) inseveral vision-and-language tasks, such as imageand caption retrieval (Gong et al., 2014; Hardoon etal., 2004; Hodosh et al., 2013), we adapt normalizedCanonical Correlations Analysis (NCCA) to oursetup.
Given two paired observation matrices Xand Y , in our case W Tr and V Tr, CCA seeks twoprojection matrices A and B that maximize thecorrelation between ATX and BTY .
This can besolved efficiently by applying SVD toC?1/2XXC?XY C?1/2Y Y = U?V Twhere C?
stands for the covariance matrix.
Finally,the projection matrices are defined as A = C?1/2XXUand B = C?1/2Y Y V .
Gong et al.
(2014) propose a nor-malized variant of CCA, in which the projection ma-trices are further scaled by some power ?
of the sin-gular values ?
returned by the SVD solution.
In ourexperiments, we tune the choice of ?
on the trainingdata.
Trivially, if ?
= 0, NCCA reduces to CCA.Note that other mapping functions could also beused.
We leave a more extensive exploration of pos-sible alternatives to further research, since the detailsof how the vision-to-text conversion is conducted arenot crucial for the current study.
As increasinglymore effective mapping methods are developed, wecan easily plug them into our architecture.Through the selected cross-modal mapping func-tion, any image can be projected onto linguisticspace, where the word (possibly of the appropriatepart of speech) corresponding to the nearest vectoris returned as a candidate label for the image (fol-lowing standard practice in distributional semantics,we measure proximity by the cosine measure).2.2 DecompositionDinu and Baroni (2014) have recently proposed ageneral decomposition framework that, given a dis-tributional vector encoding a phrase meaning andthe syntactic structure of that phrase, decomposesit into a set of vectors expected to express the se-mantics of the words that composed the phrase.
Inour setup, we are interested in a decomposition func-tion fDec : Rd2 ?
R2d2 which, given a visual vec-tor projected onto the linguistic space, assumes itrepresents the meaning of an adjective-noun phrase,and decomposes it into two vectors corresponding tothe adjective and noun constituents [wadj ;wnoun] =fDec(wAN ).
We take fDec to be a linear functionand, following Dinu and Baroni (2014), we use astraining data vectors of adjective-noun bigrams di-rectly extracted from the corpus together with theconcatenation of the corresponding adjective andnoun word vectors.
We estimate fDec by solving aridge regression problem minimizing the followingobjective:||[W Tradj ;W Trnoun]?
FdecW TrAN ||22 ?
||?Fdec||22whereW Tradj ,W Trnoun,W TrAN are the matrices obtainedby stacking the training data vectors.
The ?
param-eter is tuned through generalized cross-validation(Hastie et al., 2009).2.3 Representational SpacesLinguistic Space We construct distributional vec-tors from text through the method recently proposedby Mikolov et al.
(2013), to which we feed a cor-pus of 2.8 billion words obtained by concatenatingEnglish Wikipedia, ukWaC and BNC.5 Specifically,we used the CBOW algorithm, which induces vec-tors by predicting a target word given the words sur-rounding it.
We construct vectors of 300 dimensionsconsidering a context window of 5 words to eitherside of the target, setting the sub-sampling option to1e-05 and the negative sampling parameter to 5.6Visual Spaces Following standard practice, im-ages are represented as bags of visual words(BoVW) (Sivic and Zisserman, 2003).7 Local low-level image features are clustered into a set of visualwords that act as higher-level descriptors.
In ourcase, we use PHOW-color image features, a vari-ant of dense SIFT (Bosch et al., 2007), and a vi-sual vocabulary of 600 words.
Spatial informationis preserved with a two-level spatial pyramid rep-resentation (Lazebnik et al., 2006), achieving a fi-nal dimensionality of 12,000.
The entire pipelineis implemented using the VLFeat library (Vedaldiand Fulkerson, 2010), and its setup is identical to the5http://wacky.sslmit.unibo.it, http://www.natcorp.ox.ac.uk6The parameters are tuned on the MEN word similaritydataset (Bruni et al., 2014).7In future research, we might obtain a performance boostsimply by using the more advanced visual features recently in-troduced by Krizhevsky et al.
(2012).186Category AttributesColor black, blue, brown, gray, green,orange, pink, red, violet, white, yellowPattern spotted, stripedShape long, round, rectangular, squareTexture furry, smooth, rough, shiny, metallic,vegetation, wooden, wetTable 1: List of attributes in the evaluation dataset.Image Attributes Objectfurry catwhitesmoothgreen cocktailshinyTable 2: Sample annotations from the evaluation dataset.toolkit?s basic recognition sample application.8 Weapply Positive Pointwise Mutual Information (Evert,2005) to the BoVW counts, and reduce the resultingvectors to 300 dimensions using SVD.2.4 Evaluation DatasetFor evaluation purposes, we use the dataset consist-ing of images annotated with adjective-noun phrasesintroduced in Russakovsky and Fei-Fei (2010),which pertains to 384 WordNet/ImageNet synsetswith 25 images per synset.
The images were manu-ally annotated with 25 attribute-denoting adjectivesrelated to texture, color, pattern and shape, respect-ing the constraints that a color must cover a signifi-cant part of the target object, and all other attributesmust pertain to the object as a whole (as opposedto parts).
Table 1 lists the 25 attributes and Table 2illustrates sample annotations.9In order to increase annotation quality, we onlyconsider attributes with full annotator consensus, fora total of 8,449 annotated images, with 2.7 attributesper-image on average.
Furthermore, to make the lin-guistic annotation more natural and avoid sparsityproblems, we renamed excessively specific objectswith a noun denoting a more general category, fol-lowing recent work on entry-level categories (Or-8http://www.vlfeat.org/applications/apps.html9Although vegetation is a noun, we have kept it in the eval-uation set, treating it as an adjective.Training Evaluation#im.
#attr.
#obj.
#im.
#attr.
#obj.Exp.
1 10,749 97 - leave-one-attribute-outExp.
2 23,000 - 750 8,449 25 203Table 3: Summary of training and evaluation sets.donez et al., 2013); e.g., colobus guereza was re-labeled as monkey.
The final evaluation dataset con-tains 203 distinct objects.3 Experiment 1: Zero-shot attributelearningIn Section 1, we showed that there is a signifi-cant correlation between pairwise similarities of ad-jectives in a language-based distributional seman-tic space and those of visual feature vectors ex-tracted from images labeled with the correspondingattributes.
In the first experiment, we test whetherthis correspondence in attribute-adjective similar-ity structure across modalities suffices to success-fully apply zero-shot labeling.
We learn a cross-modal function from an annotated dataset and useit to label images from an evaluation dataset withattributes outside the training set.
We will refer tothis approach as DIRA, for Direct Retrieval usingAttribute annotation.
Note that this is the first timethat zero-shot techniques are used in the attributedomain.
In the present evaluation, we distinguishDIRA-RIDGE and DIRA-NCCA, according to thecross-modal function used to project from images tolinguistic representations (see Section 2.1 above).3.1 Cross-modal training and evaluationTo gather sufficient data to train a cross-modalmapping function for attributes/adjectives, we com-bine the publicly available datasets of Farhadi et al.
(2009) and Ferrari and Zisserman (2007) with at-tributes and associated images extracted from MIR-FLICKR (Huiskes and Lew, 2008).10 The resultingdataset contains 72 distinct attributes and 2,300 im-ages.
Each image-attribute pair represents a trainingdata point (v, wadj), where v is the vector represen-tation of the image, and wadj is the linguistic vectorof the attribute (corresponding to an adjective).
Noinformation about the depicted object is needed.10We filtered out attributes not expressed by adjectives, suchas wheel or leg.1870.40.50.60.70.80.91Dir?-Ridge DecDir?-nCCA Russakovsky and Fei-Fei (2010)AttributesRocAreaFigure 3: Performance of zero-shot attribute classification (as measured by AUC) compared to the supervised methodof Russakovsky and Fei-Fei (2010), where available.
The dark-red horizontal line marks chance performance.To further maximize the amount of training datapoints, we conduct a leave-one-attribute-out evalua-tion, in which the cross-modal mapping function isrepeatedly learned on all 72 attributes from the train-ing set, as well as all but one attribute from the eval-uation set (Section 2.4), and the associated images.This results in 72+(25?1) = 96 training attributesin total.
On average, 45 images per attribute areused.
The performance is measured for the singleattribute that was excluded from training.
A numeri-cal summary of the experiment setup is presented inthe first row of Table 3.3.2 Results and discussionRussakovsky and Fei-Fei (2010) trained separateSVM classifiers for each attribute in the evaluationdataset in a cross-validation setting.
This fully su-pervised approach can be seen as an ambitious up-per bound for zero-shot learning, and we directlycompare our performance to theirs using their figureof merit, namely area under the ROC curve (AUC),which is commonly used for binary classificationproblems.11 A perfect classifier achieves an AUC of1, whereas an AUC of 0.5 indicates random guess-ing.
For purposes of AUC computation, DIRA isconsidered to label test images with a given adjec-tive if the linguistic-space distance between theirmapped representation and the adjective is belowa certain threshold.
AUC measures the aggregatedperformance over all thresholds.
To get a sense of11Table 4 reports hit@k results for DIRA, which will be dis-cussed below in the context of Experiment 2.what AUC compares to in terms of precision and re-call, the AUC of DIRA for furry is 0.74, while theprecision is 71% and the corresponding recall 14%.For the more difficult blue case, AUC is at 0.5, pre-cision and recall are 2% and 55%, respectively.The AUC results are presented in Figure 3 (ig-nore red bars for now).
We observe first that, of thetwo mapping functions we considered, RIDGE (bluebars) clearly outperforms NCCA (yellow bars).
Ac-cording to a series of paired permutation tests,RIDGE has a significantly larger AUC in 13/25cases, NCCA in only 2.
This is somewhat surpris-ing given the better performance of NCCA in theexperiments of Gong et al.
(2014).
However, oursetup is quite different from theirs: They performall retrieval tasks by projecting the input visual andlanguage data onto a common multimodal space dif-ferent from both input spaces.
NCCA is a well-suited algorithm for this.
We aim instead at produc-ing linguistic annotations of images, which is moststraightforwardly accomplished by projecting visualrepresentations onto linguistic space.
Regression-based learning (in our case, via RIDGE) is a morenatural choice for this purpose.Coming now to a more general analysis of the re-sults, as expected, and analogously to the supervisedsetting, DIRA-RIDGE performance varies across at-tributes.
Some achieve performance close to thesupervised model (e.g., rectangular or wooden)and, for 18 out of 25, the performance is wellabove chance (bootstrap test).
The exceptions are:blue, square, round, vegetation, smooth, spotted andstriped.
Interestingly, for the last 4 attributes in188this list, Russakovsky and Fei-Fei (2010) achievedtheir lowest performance, attributing it to the lower-quality of the corresponding image annotations.Furthermore, Russakovsky and Fei-Fei (2010) ex-cluded 5 attributes due to insufficient training data.Of these, our performance for blue, vegetation andsquare is not particularly encouraging, but for violetand pink we achieve more than 0.7 AUC, at the levelof the supervised classifiers, suggesting that the pro-posed method can complement the latter when an-notated data are not available.For a different perspective on the performanceof DIRA, we took several objects and queried themodel for their most common attribute, based on theaverage attribute rank across all images of the objectin the dataset.
Reassuringly, we learn that sunflow-ers are on average yellow (mean rank 2.3), fields aregreen (4.4), cabinets are wooden (4) and vans metal-lic (6.6) (strawberries are, suspiciously, blue, 2.7).Overall, this experiment shows that, just like ob-ject classification, attribute classifiers benefit fromknowledge transfer between the visual and linguis-tic modalities, and zero-shot learning can achievereasonable performance on attributes and the corre-sponding adjectives.
This conclusion is based on theassumption that per-image annotations of attributesare available; in the following section, we show howequal and even better performance can be attainedusing data sets annotated with objects only, there-fore without any hand-coded attribute information.4 Experiment 2: Learning attributes fromobjects and visual phrasesHaving shown that reasonably accurate annotationsof unseen attributes can be obtained with zero-shotlearning when a small amount of manual annota-tion is available, we now proceed to test the intu-ition, preliminarily supported by the data in Figure1, that, since objects are bundles of attributes, at-tributes are implicitly learned together with objects.We thus try to induce attribute-denoting adjective la-bels by exploiting only widely-available object-noundata.
At the same time, building on the observa-tion illustrated in Figure 2 that pictures of objectsare pictures of visual phrases, we experiment witha vector decomposition model which treats imagesas composite and derives adjective and noun anno-tations jointly.
We compare it with standard zero-shot learning using direct label retrieval as well asagainst a number of challenging alternatives that ex-ploit gold-standard information about the depictedobjects.
The second row of Table 3 gives a numeri-cal summary of the setup for this experiment.4.1 Cross-modal trainingWe now assume object annotations only, in the formof training data (v, wnoun), where v is the vectorrepresentation of an image tagged with an object andwnoun is the linguistic vector of the correspondingnoun.
To ensure high imageability and diversity, weuse as training object labels those appearing in theCIFAR-100 dataset (Krizhevsky, 2009), combinedwith those previously used in the work of Farhadiet al.
(2009), as well as the most frequent nouns inour corpus that also exist in ImageNet, for a totalof 750 objects-nouns.
For each object label, we in-clude at most 50 images from the corresponding Im-ageNet synset, resulting in ?
23, 000 training datapoints.
Images containing objects from the evalua-tion dataset are excluded, so that both adjective andnoun retrieval adhere to the zero-shot paradigm.4.2 Object-agnostic modelsDIRO The Direct Retrieval using Object annota-tion approach projects an image onto the linguisticspace and retrieves the nearest adjectives as candi-date attribute labels.
The only difference with DIRA(more precisely, DIRA-RIDGE), the zero-shot ap-proach we tested above, is that the mapping functionhas been trained on object-noun data only.DEC The Decomposition method uses the fDecfunction inspired by Dinu and Baroni (2014) (seeSection 2.2), to associate the image vector projectedonto linguistic space to an adjective and a noun.
Wetrain fDec with about ?
50, 000 training instances,selected based on corpus frequency.
These dataare further balanced by not allowing more than 100training samples for any adjective or noun in orderto prevent very frequent words such as other or newfrom dominating the training data.
No image dataare used, and there is no need for manual annota-tion, as the adjective-noun tuples are automaticallyextracted from the corpus.At test time, given an image to be labeled,189we project its visual representation onto the lin-guistic space and decompose the resulting vectorw?
into two candidate adjective and noun vectors:[w?adj ;w?noun] = fDec(w?).
We then search the lin-guistic space for adjectives and nouns whose vectorsare nearest to w?adj and w?noun, respectively.4.3 Object-informed modelsA cross-modal function trained exclusively onobject-noun data might be able to capture only pro-totypical characteristics of an object, as inducedfrom text, independently of whether they are de-picted in an image.
Although the gold annotationof our dataset should already penalize this image-independent labeling strategy (see Section 2.4), wecontrol for this behaviour by comparing againstthree models that have access to the gold noun an-notations of the image and favor adjectives that aretypical modifiers of the nouns.LM We build a bigram Language Model by usingthe Berkeley LM toolkit (Pauls and Klein, 2012)12on the one-trillion-token Google Web1T corpus13and smooth probabilities with the ?Stupid?
back-off technique (Brants et al., 2007).
Given animage with object-noun annotation, we score allattributes-adjectives based on the language-model-derived conditional probability p(adjective|noun).All images of the same object produce identicalrankings.
As an example, among the top attributesof cocktail we find heady, creamy and fruity.VLM LM does not exploit visual informationabout the image to be annotated.
A natural way toenhance it is to combine it with DIRO, our cross-modal mapping adjective retrieval method.
In thevisually-enriched Language Model, we interpolate(using equal weights) the ranks produced by thetwo models.
In the resulting combination, attributesthat are both linguistically sensible and likely to bepresent in the given image should be ranked high-est.
We expect this approach to be challenging tobeat.
MacKenzie (2014) recently introduced a simi-lar model in a supervised setting, where it improvedover standard attribute classifiers.12https://code.google.com/p/berkeleylm/13https://catalog.ldc.upenn.edu/LDC2006T13LM SP VLM DIRO DEC DIRA@1 2 0 5 1 10 7@5 5 7 16 4 31 23@10 8 9 29 9 44 37@20 18 17 50 19 59 51@50 33 32 72 43 81 68@100 56 55 82 67 89 77Table 4: Percentage hit@k attribute retrieval scores.SP The Selectional Preference model robustlycaptures semantic restrictions imposed by a noun onthe adjectives modifying it (Erk et al., 2010).
Con-cretely, for each noun denoting a target object, weidentify a set of adjectives ADJnoun that co-occurwith it in a modifier relation more that 20 times.By averaging the linguistic vectors of these adjec-tives, we obtain a vector wprototypicalnoun , which shouldcapture the semantics of the prototypical adjectivesfor that noun.
Adjectives that have higher similar-ity with this prototype vector are expected to denotetypical attributes of the corresponding noun and willbe ranked as more probable attributes.
Similarly toLM, all images of the same object produce identicalrankings.
As an example, among the top attributesof cocktail we find fantastic, delicious and perfect.4.4 ResultsWe evaluate the performance of the models onattribute-denoting adjective retrieval, using a searchspace containing the top 5,000 most frequent ad-jectives in our corpus.
Tables 4 and 5 presenthit@k and recall@k results, respectively (k ?
{1, 5, 10, 20, 50, 100}).
Hit@k measures the per-centage of images for which at least one gold at-tribute exists among the top k retrieved attributes.Recall@k measures the proportion of gold attributesretrieved among the top k, relative to the total num-ber of gold attributes for each image.14First of all, we observe that LM and SP ?
the twomodels that have access to gold object-noun annota-tion and are entirely language-based ?
although wellabove the random baseline (k/5,000), achieve ratherlow performance.
This confirms that to model ourtest set accurately, it is not sufficient to predict typi-cal attributes of the depicted objects.14Due to the leave-one-attribute-out approach used to trainand test DIRA (see Section 3), it is not possible to computerecall results for this model.190LM SP VLM DIRO DEC@1 1 0 2 0 4@5 2 3 7 2 15@10 3 5 15 4 23@20 9 10 30 9 35@50 20 20 49 22 59@100 35 34 61 44 70Table 5: Percentage recall@k attribute retrieval scores.DIRO DEC DIRA@1 1 2 0@5 3 10 0@10 5 14 1@20 9 20 2@50 20 29 6@100 33 41 12Table 6: Percentage hit@k noun retrieval scores.The DIRO method, which exploits visual in-formation, performs numerically similarly to theobject-informed models LM and SP, with betterhit and recall at high ranks.
Although worse thanDIRA, the relatively high performance of DIRO isa promising result, suggesting object annotations to-gether with linguistic knowledge extracted in an un-supervised manner from large corpora can replace,to some extent, manual attribute annotations.
How-ever, DIRO does not directly model any semanticcompatibility constraints between the retrieved ad-jectives and the object present in the image (see ex-amples below).
Hence, the object-informed modelVLM, which combines visual information wit lin-guistic co-occurrence statistics, doubles the perfor-mance of DIRO, LM and SP.Our DEC model, which treats images as visualphrases and jointly decouples their semantics, out-performs even VLM by a large margin.
It also out-performs DIRA, the standard zero-shot learning ap-proach using attribute-adjective annotated data (seealso the attribute-by-attribute AUC comparison be-tween DEC, DIRA and the fully-supervised ap-proach of Russakovsky and Fei-Fei in Figure 3).Interestingly, accounting for the phrasal nature ofvisual information leads to substantial performanceimprovement in object recognition through zero-shot learning (i.e., tagging images with the depictednouns) as well.
Table 6 provides the hit@k resultsobtained with the DIRO and DEC methods for thenoun retrieval task in a search space of 10,000 mostImage Model Top item Top hit (Rank)A: white, brownN: dogDEC A: white white (1)N: dog dog (1)DIRO A: animal white (27)N: goat dog (25)LM A: stray brown (74)VLM A: pet brown (17)A: shiny, roundN: syrupDEC A: shiny shiny (1)N: flan syrup (170)DIRO A: crunchy shiny (15)N: ramekin syrup (113)LM A: chocolate shiny (84)VLM A: chocolate shiny (17)Table 7: Images with gold attribute-adjective and object-noun labels, and highest-ranked items for each model(Top item), as well as highest-ranked correct item andrank (Top hit).
Noun results for (V)LM are omitted sincethese models have access to the gold noun label.frequent nouns from our corpus.
Note that DIROrepresents the label retrieval technique that has beenstandardly used in conjunction with zero-shot learn-ing for objects: The cross-modal function is trainedon images annotated with nouns that denote the ob-jects they depict, and it is then used for noun labelretrieval of unseen objects through a nearest neigh-bor search of the mapped image representation (theDIRA column shows that zero-shot noun retrievalusing the mapping function trained on adjectivesworks very poorly).
DEC decomposes instead themapped image representation into two vectors de-noting adjective and noun semantics, respectively,and uses the latter to perform the nearest neigh-bor search for a noun label.
Although not directlycomparable, the results of DEC reported here are inthe same range of state-of-the-art zero-shot learningmodels for object recognition (Frome et al., 2013).Annotation examples Table 7 presents some in-teresting patterns we observed in the results.
Thefirst example illustrates the case in which conductingadjective and noun retrieval independently results inmixing information, which damages the DIRO ap-proach: Adjectival and nominal properties are notdecoupled properly, since the animal property of thedepicted dog is reflected in both the animal adjec-tive and the goat noun.
At the same time, the white-191ness of the object (an adjectival property) influencesnoun selection, since goats tend to be white.
Instead,DEC unpacks the visual semantics in an accurateand meaningful way, producing correct attribute andnoun annotations that form acceptable phrases.
LMand VLM are negatively affected by co-occurrencestatistics and guess stray and pet as adjectives, bothtypical but generic and abstract dog properties.In the next example, DIRO predicts a reason-able noun label (ramekin), focusing on the containerrather than the liquid it contains.
By ignoring therelation between the adjective and the noun, the re-sulting adjective annotation (crunchy) is semanti-cally incompatible with the noun label, emphasizingthe inability of this method to account for semanticrelations between attributes-adjectives and object-nouns.
DEC, on the other hand, mistakenly anno-tates the object as flan instead of syrup.
However,having captured the right general category of the ob-ject (?smooth gelatinous items that reflect light?
),it ranks a semantically appropriate and correct at-tribute (shiny) at the top.
Finally, LM and VLMchoose chocolate, an attribute semantically appro-priate for syrup but irrelevant for the target image.Semantic plausibility of phrases The examplesabove suggest that one fundamental way in whichDEC improves over DIRO is by producing seman-tically coherent adjective-noun combinations.
Moresystematic evidence for this conjecture is providedby a follow-up experiment on the linguistic qual-ity of the generated phrases.
We randomly sampled2 images for each of the 203 objects in our dataset.
For each image, we let the two models gen-erate 9 descriptive phrases by combining their re-spective top 3 adjective and noun predictions.
Fromthe resulting lists of 3,654 phrases, we picked the200 most common ones for each model, with only1/8 of these common phrases being shared by both.The selected phrases were presented (in random or-der and concealing their origin) to two linguistically-sophisticated annotators, who were asked to ratetheir degree of semantic plausibility on a 1-3 scale(the annotators were not shown the correspondingimages and had to evaluate phrases purely on lin-guistic/semantic grounds).
Since the two judgeswere largely in agreement (?=0.63), we averagedtheir ratings.
The mean averaged plausibility scoreDEC LM vLM DIRO DIRA0.250.30.350.40.450.50.55SPFigure 4: Distributions of (per-image) concretenessscores across different models.
Red line marks medianvalues, box edges correspond to 1st and 3rd quartiles, thewiskers extend to the most extreme data points and out-liers are plotted individually.for DIRO phrases was 1.74 (s.d.
: 0.76), for DEC itwas 2.48 (s.d.
: 0.64), with the difference significantaccording to a Mann-Whitney test.
The two anno-tators agreed in assigning the lowest score (?com-pletely implausible?)
to more than 1/3 of the DIROphrases (74/200; e.g., tinned tostada, animal bird,hollow hyrax), but they unanimously assigned thelowest score to only 7/200 DEC phrases (e.g., cylin-drical bed-sheet, sweet ramekin, wooden meat).
Wethus have solid quantitative support that the superior-ity of DEC is partially due to how it learns to jointlyaccount for adjective and noun semantics, producingphrases that are linguistically more meaningful.Adjective concreteness We can gain further in-sight into the nature of the adjectives chosen bythe models by considering the fact that phrases thatare meant to describe an object in a picture shouldmostly contain concrete adjectives, and thus the de-gree of concreteness of the adjectives produced by amodel is an indirect measure of its quality.
Follow-ing Hill and Korhonen (2014), we define the con-creteness of an adjective as the average concretenessscore of the nouns it modifies in our text corpus.Noun concreteness scores are taken, in turn, fromTurney et al.
(2011).
For each test image and model,we obtain a concreteness score by averaging the con-creteness of the top 5 adjectives that the model se-lected for the image.
Figure 4 reports the distribu-tions of the resulting scores across models.
We con-192firm that the purely language-based models (LM,SP) are producing generic abstract adjectives thatare not appropriate to describe images (e.g., crypto-graphic key, homemade bread, Greek salad, beatenyolk).
The image-informed VLM and DIRO modelsproduce considerably more concrete adjectives.
Notsurprisingly, DIRA, that was directly trained on con-crete adjectives, produces the most concrete ones.Importantly, DEC, despite being based on a cross-modal function that was not explicitly exposed toadjectives, produced adjectives that are approachingthe concreteness level of those of DIRA (both differ-ences between DEC and DIRO, DEC and DIRA aresignificant as by paired Mann-Whitney tests).5 Using DEC for attribute-based objectclassificationAs discussed in the introduction, attributes can ef-fectively be used for attribute-based object clas-sification.
In this section, we show that clas-sifiers trained on attribute representations createdwith DEC ?
which does not require any attribute-annotated training data nor training a battery of at-tribute classifiers ?
outperform (and are complemen-tary to) standard BoVW features.We use a subset of the Pascal VOC 2008 dataset.15Specifically, following Farhadi et al.
(2009), we usethe original VOC training set for training/validation,and the VOC validation set for testing.
One-vs-alllinear-SVM classifiers are trained for all VOC ob-jects, using 3 alternative image representations.First, we train directly on BoVW features(PHOW, see Section 2.3), as in the classic objectrecognition pipeline.
We compare PHOW to anattribute-centric approach with attribute labels auto-matically generated by DEC. All VOC images areprojected onto the linguistic space using the cross-modal mapping function trained with object-noundata only (see Section 4.1), from which we furtherremoved all images depicting a VOC object.
Eachimage projection is then decomposed through DECinto two vectors representing adjective and noun in-formation.
The final attribute-centric vector repre-senting an image is created by recording the cosinesimilarities of the DEC-generated adjective vector15http://pascallin.ecs.soton.ac.uk/challenges/VOC/voc2008/Image Object PredictedAttributesaeroplane thick, wet, dry,cylindrical,motionless,translucentdog cuddly, wild,cute, furry,white, colouredTable 8: Two VOC images with some top attributes as-signed by DEC: these attributes, together with their co-sine similarities to the mapped image vectors, serve asattribute-centric representations.with all the adjectives in our linguistic space.
Infor-mally, this representation can be thought of as a vec-tor of weights describing the appropriateness of eachadjective as an annotation for the image.16 This iscomparable to standard attribute-based classification(Farhadi et al., 2009), in which images are repre-sented as distributions over attributes estimated witha set of ad hoc supervised attribute-specific classi-fiers.
Table 8 show examples of top attributes auto-matically assigned by DEC.
While not nearly as ac-curate as manual annotation, many attributes are rel-evant to the objects, both as specifically depicted inthe image (the aeroplane is wet), but also more pro-totypically (aeroplanes are cylindrical in general).We also perform feature-level fusion (FUSED) byconcatenating the PHOW and DEC features, and re-ducing the resulting vector to 100 dimensions withSVD (Bruni et al., 2014) (SVD dimensionality de-termined by cross-validation on the training set).5.1 ResultsThere is an improvement over PHOW visual featureswhen using DEC-based attribute vectors, with accu-racy raising from 30.49% to 32.76%.
The confusionmatrices in Figure 5 show that PHOW and DEC donot only differ in quantitative performance, but makedifferent kinds of errors, in part pointing at the dif-ferent modalities the two models tap into.
PHOW,for example, tends to confuse cats with sofas, prob-ably because the former are often pictured lying on16Given that the resulting representations are very dense, wesparsify them by setting to zeros all adjective dimensions withcosine below the global mean cosine value.193Figure 5: Confusion matrices for PHOW (top) and DEC(bottom).
Warmer-color cells correspond to higher pro-portions of images with gold row label tagged by an algo-rithm with the column label (e.g., the first cells show thatDEC tags a larger proportion of aeroplanes correctly).the latter.
DEC, on the other hand, tends to con-fuse chairs with TV monitors, partially misguidedby the taxonomic information encoded in language(both are pieces of furniture).
Indeed, the combinedFUSED approach outperforms both representationsby a large margin (35.81%), confirming that thelinguistically-enriched information brought by DECis to a certain extent complementary to the lower-level visual evidence directly exploited by PHOW.Overall, the performance of our system is quite closeto the one obtained by Farhadi et al.
(2009) with en-sembles of supervised attribute classifiers trained onmanually annotated data (the most comparable ac-curacy from their Table 1 is at 34.3%).1717Farhadi and colleagues reduce the bias for the people cat-egory by reporting mean per-class accuracy; we directly ex-cluded people from our version of the data set.6 ConclusionWe extended zero-shot image labeling beyond ob-jects, showing that it is possible to tag images withattribute-denoting adjectives that were not seen dur-ing training.
For some attributes, performance wascomparable to that of per-attribute supervised classi-fiers.
We further showed that attributes are implicitlyinduced when learning to map visual vectors of ob-jects to their linguistic realizations as nouns, and thatimprovements in both attribute and noun retrievalare attained by treating images as visual phrases,whose linguistic representations must be decom-posed into a coherent word sequence.
The resultingmodel outperformed a set of strong rivals.
While theperformance of the zero-shot decompositional ap-proach in the adjective-noun phrase labeling alonemight still be low for practical applications, thismodel can still produce attribute-based representa-tions that significantly improve performance in asupervised object recognition task, when combinedwith standard visual features.By mapping attributes and objects to phrases ina linguistic space, we are also likely to producemore natural descriptions than those currently usedin computer vision (fluffy kittens rather than 2-boxytables).
In future work, we want to delve moreinto the linguistic and pragmatic naturalness of at-tributes: Can we predict not just which attributesof a depicted object are true, but which are moresalient and thus more likely to be mentioned (redcar over metal car)?
Can we pick the most appro-priate adjective to denote an attribute given the ob-ject in the picture (moist, rather than damp lips)?We should also address attribute dependencies: byignoring them, we currently get undesired results,such as the aeroplane in Table 8 being tagged as bothwet and dry.
More ambitiously, inspired by Karpa-thy et al.
(2014), we plan to associate image frag-ments with phrases of arbitrary syntactic structures(e.g., PPs for backgrounds, a VPs for main events),paving the way to full-fledged caption generation.AcknowledgmentsWe thank the TACL reviewers for their feedback.We were supported by ERC 2011 Starting Indepen-dent Research Grant n. 283554 (COMPOSES).194ReferencesTamara Berg, Alexander Berg, and Jonathan Shih.
2010.Automatic attribute discovery and characterizationfrom noisy Web data.
In Proceedings of ECCV, pages663?676, Crete, Greece.Anna Bosch, Andrew Zisserman, and Xavier Munoz.2007.
Image classification using random forests andferns.
In Proceedings of ICCV, pages 1?8, Rio deJaneiro, Brazil.Thorsten Brants, Ashok Popat, Peng Xu, Franz Och, andJeffrey Dean.
2007.
Large language models in ma-chine translation.
In Proceedings of EMNLP, pages858?867, Prague, Czech Republic.Elia Bruni, Nam Khanh Tran, and Marco Baroni.
2014.Multimodal distributional semantics.
Journal of Arti-ficial Intelligence Research, 49:1?47.Jia Deng, Wei Dong, Richard Socher, Lia-Ji Li, andLi Fei-Fei.
2009.
Imagenet: A large-scale hierarchi-cal image database.
In Proceedings of CVPR, pages248?255, Miami Beach, FL.Georgiana Dinu and Marco Baroni.
2014.
How to makewords with vectors: Phrase generation in distributionalsemantics.
In Proceedings of ACL, pages 624?633,Baltimore, MD.Santosh Divvala, Ali Farhadi, and Carlos Guestrin.2014.
Learning everything about anything: Webly-supervised visual concept learning.
In Proceedings ofCVPR, Columbus, OH.Katrin Erk, Sebastian Pado?, and Ulrike Pado?.
2010.
Aflexible, corpus-driven model of regular and inverseselectional preferences.
Computational Linguistics,36(4):723?763.Stefan Evert.
2005.
The Statistics of Word Cooccur-rences.
Ph.D dissertation, Stuttgart University.Ali Farhadi, Ian Endres, Derek Hoiem, and DavidForsyth.
2009.
Describing objects by their attributes.In Proceedings of CVPR, pages 1778?1785, MiamiBeach, FL.Ali Farhadi, Mohsen Hejrati, Mohammad A. Sadeghi,Peter Young, Cyrus Rashtchian, Julia Hockenmaier,and David Forsyth.
2010.
Every picture tells a story:Generating sentences from images.
In Proceedings ofECCV, Crete, Greece.Vittorio Ferrari and Andrew Zisserman.
2007.
Learningvisual attributes.
In Proceedings of NIPS, pages 433?440, Vancouver, Canada.Andrea Frome, Greg Corrado, Jon Shlens, Samy Ben-gio, Jeff Dean, Marc?Aurelio Ranzato, and TomasMikolov.
2013.
DeViSE: A deep visual-semantic em-bedding model.
In Proceedings of NIPS, pages 2121?2129, Lake Tahoe, NV.Yunchao Gong, Liwei Wang, Micah Hodosh, Julia Hock-enmaier, and Svetlana Lazebnik.
2014.
Improvingimage-sentence embeddings using large weakly an-notated photo collections.
In Proceedings of ECCV,pages 529?545, Zurich, Switzerland.David R Hardoon, Sandor Szedmak, and John Shawe-Taylor.
2004.
Canonical correlation analysis: Anoverview with application to learning methods.
Neu-ral Computation, 16(12):2639?2664.Trevor Hastie, Robert Tibshirani, and Jerome Friedman.2009.
The Elements of Statistical Learning, 2nd edi-tion.
Springer, New York.Felix Hill and Anna Korhonen.
2014.
Concreteness andsubjectivity as dimensions of lexical meaning.
In Pro-ceedings of ACL, pages 725?731, Baltimore, Mary-land.Micah Hodosh, Peter Young, and Julia Hockenmaier.2013.
Framing image description as a ranking task:Data, models and evaluation metrics.
Journal of Arti-ficial Intelligence Research, 47:853?899.Harold Hotelling.
1936.
Relations between two sets ofvariates.
Biometrika, 28(3/4):321?377.Mark Huiskes and Michael Lew.
2008.
The MIR Flickrretrieval evaluation.
In Proceedings of MIR, pages 39?43, New York, NY.Andrej Karpathy, Armand Joulin, and Li Fei-Fei.
2014.Deep fragment embeddings for bidirectional imagesentence mapping.
In Proceedings of NIPS, pages1097?1105, Montreal, Canada.Alex Krizhevsky, Ilya Sutskever, and Geoffrey Hinton.2012.
ImageNet classification with deep convolutionalneural networks.
In Proceedings of NIPS, pages 1097?1105, Lake Tahoe, Nevada.Alex Krizhevsky.
2009.
Learning multiple layers of fea-tures from tiny images.
Master?s thesis.Girish Kulkarni, Visruth Premraj, Sagnik Dhar, SimingLi, Yejin Choi, Alexander Berg, and Tamara Berg.2011.
Baby talk: Understanding and generating sim-ple image descriptions.
In Proceedings of CVPR,pages 1601?1608, Colorado Springs, CO.Christoph H Lampert, Hannes Nickisch, and StefanHarmeling.
2009.
Learning to detect unseen objectclasses by between-class attribute transfer.
In Pro-ceedings of CVPR, pages 951?958, Miami Beach, FL.Angeliki Lazaridou, Elia Bruni, and Marco Baroni.
2014.Is this a wampimuk?
cross-modal mapping betweendistributional semantics and the visual world.
In Pro-ceedings of ACL, pages 1403?1414, Baltimore, MD.Svetlana Lazebnik, Cordelia Schmid, and Jean Ponce.2006.
Beyond bags of features: Spatial pyramidmatching for recognizing natural scene categories.
InProceedings of CVPR, pages 2169?2178, Washington,DC.195Calvin MacKenzie.
2014.
Integrating visual and linguis-tic information to describe properties of objects.
Un-dergraduate Honors Thesis, Computer Science Depart-ment, University of Texas at Austin.Tomas Mikolov, Kai Chen, Greg Corrado, and JeffreyDean.
2013.
Efficient estimation of word representa-tions in vector space.
http://arxiv.org/abs/1301.3781/.Margaret Mitchell, Xufeng Han, Jesse Dodge, AlyssaMensch, Amit Goyal, Alex Berg, Kota Yamaguchi,Tamara Berg, Karl Stratos, and Hal Daume?
III.
2012.Midge: Generating image descriptions from computervision detections.
In Proceedings of EACL, pages747?756, Avignon, France.Gregory Murphy.
2002.
The Big Book of Concepts.
MITPress, Cambridge, MA.Vicente Ordonez, Jia Deng, Yejin Choi, Alexander Berg,and Tamara Berg.
2013.
From large scale image cate-gorization to entry-level categories.
In Proceedings ofICCV, pages 1?8, Sydney, Australia.Genevieve Patterson, Chen Xu, Hang Su, and JamesHays.
2014.
The SUN attribute database: Beyond cat-egories for deeper scene understanding.
InternationalJournal of Computer Vision, 108(1-2):59?81.Adam Pauls and Dan Klein.
2012.
Large-scale syntacticlanguage modeling with treelets.
In Proceedings ofACL, pages 959?968, Jeju Island, Korea.Olga Russakovsky and Li Fei-Fei.
2010.
Attribute learn-ing in large-scale datasets.
In Proceedings of ECCV,pages 1?14.Mohammad Sadeghi and Ali Farhadi.
2011.
Recognitionusing visual phrases.
In Proceedings of CVPR, pages1745?1752, Colorado Springs, CO.Carina Silberer, Vittorio Ferrari, and Mirella Lapata.2013.
Models of semantic representation with visualattributes.
In Proceedings of ACL, pages 572?582,Sofia, Bulgaria.Josef Sivic and Andrew Zisserman.
2003.
Video Google:A text retrieval approach to object matching in videos.In Proceedings of ICCV, pages 1470?1477, Nice,France.Richard Socher, Milind Ganjoo, Christopher Manning,and Andrew Ng.
2013.
Zero-shot learning throughcross-modal transfer.
In Proceedings of NIPS, pages935?943, Lake Tahoe, NV.Peter Turney and Patrick Pantel.
2010.
From frequencyto meaning: Vector space models of semantics.
Jour-nal of Artificial Intelligence Research, 37:141?188.Peter Turney, Yair Neuman, Dan Assaf, and Yohai Co-hen.
2011.
Literal and metaphorical sense identifi-cation through concrete and abstract context.
In Pro-ceedings of EMNLP, pages 680?690, Edinburgh, UK.Laurens Van der Maaten and Geoffrey Hinton.
2008.Visualizing data using t-SNE.
Journal of MachineLearning Research, 9(2579-2605).Andrea Vedaldi and Brian Fulkerson.
2010.
VLFeat ?an open and portable library of computer vision al-gorithms.
In Proceedings of ACM Multimedia, pages1469?1472, Firenze, Italy.Andrea Vedaldi, Siddarth Mahendran, Stavros Tsogkas,Subhransu Maji, Ross Girshick, Juho Kannala, EsaRahtu, Iasonas Kokkinos, Matthew Blaschko, DavidWeiss, Ben Taskar, Karen Simonyan, Naomi Saphra,and Sammy Mohamed.
2014.
Understanding objectsin detail with fine-grained attributes.
In Proceedingsof CVPR, Columbus, OH.196
