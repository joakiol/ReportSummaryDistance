Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 799?809,MIT, Massachusetts, USA, 9-11 October 2010. c?2010 Association for Computational LinguisticsLatent-Descriptor Clustering for Unsupervised POS InductionMichael LamarDepartment of Mathematics and Computer ScienceSaint Louis University220 N. Grand Blvd.St.Louis, MO 63103, USAmlamar@slu.eduYariv MaronGonda Brain Research CenterBar-Ilan UniversityRamat-Gan 52900, Israelsyarivm@yahoo.comElie BienenstockDivision of Applied Mathematicsand Department of NeuroscienceBrown UniversityProvidence, RI 02912, USAelie@brown.eduAbstractWe present a novel approach to distributional-only, fully unsupervised, POS tagging, based onan adaptation of the EM algorithm for the esti-mation of a Gaussian mixture.
In this approach,which we call Latent-Descriptor Clustering(LDC), word types are clustered using a seriesof progressively more informative descriptorvectors.
These descriptors, which are computedfrom the immediate left and right context ofeach word in the corpus, are updated based onthe previous state of the cluster assignments.The LDC algorithm is simple and intuitive.
Us-ing standard evaluation criteria for unsupervisedPOS tagging, LDC shows a substantial im-provement in performance over state-of-the-artmethods, along with a several-fold reduction incomputational cost.1 IntroductionPart-of-speech (POS) tagging is a fundamentalnatural-language-processing problem, and POStags are used as input to many important appli-cations.
While state-of-the-art supervised POStaggers are more than 97% accurate (Toutanovaet al, 2003; Tsuruoka and Tsujii, 2005), unsu-pervised POS taggers continue to lag far behind.Several authors addressed this gap using limitedsupervision, such as a dictionary of tags for eachword (Goldwater and Griffiths, 2007; Ravi andKnight, 2009), or a list of word prototypes foreach tag (Haghighi and Klein, 2006).
Even inlight of all these advancements, there is still in-terest in a completely unsupervised method forPOS induction for several reasons.
First, mostlanguages do not have a tag dictionary.
Second,the preparation of such resources is error-prone.Third, while several widely used tag sets do ex-ist, researchers do not agree upon any specificset of tags across languages or even within onelanguage.
Since tags are used as basic featuresfor many important NLP applications (e.g.Headden et al 2008), exploring new, statisticallymotivated, tag sets may also be useful.
For thesereasons, a fully unsupervised induction algo-rithm has both a practical and a theoretical val-ue.In the past decade, there has been a steadyimprovement on the completely unsupervisedversion of POS induction (Sch?tze, 1995; Clark,2001; Clark, 2003; Johnson, 2007; Gao andJohnson, 2008; Gra?a et al, 2009; Abend et al,2010; Lamar et al, 2010; Reichart et al, 2010;Berg-Kirkpatrick et al, 2010).
Some of thesemethods use morphological cues (Clark, 2001;Clark, 2003; Abend et al, 2010; Reichart et al,2010; Berg-Kirkpatrick et al, 2010), but all relyheavily on distributional information, i.e., bi-799gram statistics.
Two recent papers advocate non-disambiguating models (Abend et al, 2010;Lamar et al, 2010): these assign the same tag toall tokens of a word type, rather than attemptingto disambiguate words in context.
Lamar et al(2010) motivate this choice by showing howremoving the disambiguation ability from astate-of-the-art disambiguating model results inincreasing its accuracy.In this paper, we present a novel approach tonon-disambiguating, distributional-only, fullyunsupervised, POS tagging.
As in all non-disambiguating distributional approaches, thegoal, loosely stated, is to assign the same tag towords whose contexts in the corpus are similar.Our approach, which we call Latent-DescriptorClustering, or LDC, is an iterative algorithm, inthe spirit of the K-means clustering algorithmand of the EM algorithm for the estimation of amixture of Gaussians.In conventional K-means clustering, one isgiven a collection of N objects described as Ndata points in an r-dimensional Euclidean space,and one seeks a clustering that minimizes thesum of intra-cluster squared distances, i.e., thesum, over all data points, of the squared distancebetween that point and the centroid of its as-signed cluster.
In LDC, we similarly state ourgoal as one of finding a tagging, i.e., cluster as-signment, A, that minimizes the sum of intra-cluster squared distances.
However, unlike inconventional K-means, the N objects to be clus-tered are themselves described by vectors?in asuitable manifold?that depend on the clusteringA.
We call these vectors latent descriptors.Specifically, each object to be clustered, i.e.,each word type w, is described in terms of itsleft-tag context and right-tag context.
These con-text vectors are the counts of the K different tagsoccurring, under tagging A, to the left and rightof tokens of word type w in the corpus.
We nor-malize each of these context vectors to unitlength, producing, for each word type w, twopoints LA(w) and RA(w) on the (K?1)-dimensional unit sphere.
The latent descriptorfor w consists of the pair (LA(w), RA(w))?moredetails in Section 2.A straightforward approach to this latent-descriptor K-means problem is to adapt the clas-sical iterative K-means algorithm so as to handlethe latent descriptors.
Specifically, in each itera-tion, given the assignment A obtained from theprevious iteration, one first computes the latentdescriptors for all word types as defined above,and then proceeds in the usual way to updatecluster centroids and to find a new assignment Ato be used in the next iteration.For reasons to be discussed in Section 5, weinstead prefer a soft-assignment strategy, in-spired from the EM algorithm for the estimationof a mixture of Gaussians.
Thus, rather than thehard assignment A, we use a soft-assignmentmatrix P. Pwk, interpreted as the probability ofassigning word w to cluster k, is, essentially,proportional to exp{?
dwk2/2?2}, where dwk is thedistance between the latent descriptor for w andthe centroid, i.e., Gaussian mean, for k. Unlikethe Gaussian-mixture model however, we usethe same mixture coefficient and the same Gaus-sian width for all k. Further, we let the Gaussianwidth ?
?decrease gradually during the iterativeprocess.
As is well-known, the EM algorithm forGaussian mixtures reduces in the limit of small ?to the simpler K-means clustering algorithm.
Asa result, the last few iterations of LDC effec-tively implement the hard-assignment K-means-style algorithm outlined in the previous para-graph.
The soft assignment used earlier in theprocess lends robustness to the algorithm.The LDC approach is shown to yield substantialimprovement over state-of-the-art methods forthe problem of fully unsupervised, distributionalonly, POS tagging.
The algorithm is conceptu-ally simple and easy to implement, requiring lessthan 30 lines of Matlab code.
It runs in a fewseconds of computation time, as opposed tohours or days for the training of HMMs.2 Notations and Statement of ProblemThe LDC algorithm is best understood in thecontext of the latent-descriptor K-means optimi-zation problem.
In this section, we set up ournotations and define this problem in detail.
Forsimplicity, induced tags are henceforth referredto as labels, while tags will be reserved for thegold-standard tags, to be used later for evalua-tion.Let W denote the set of word types w1,?,wN,and let T denote the set of labels, i.e., induced800tags.
The sizes of these sets are |W| = N and |T| =K.
In the experiments presented in Section 4, Nis 43,766 and K is either 50 or 17.
For any wordtoken t in the corpus, we denote the word type oft by w(t).
The frequency of word type w in thecorpus is denoted f(w); thus, ?w f(w) = 1.For a word type w1, the left-word context ofw1, L(w1), is defined as the N-dimensional vectorwhose n-th component is the number of bigrams,i.e., pairs of consecutive tokens (ti?1, ti) in thecorpus, such that w(ti) = w1 and w(ti?1) = n. Simi-larly, we define the right-word context of w1,R(w1), as the N-dimensional vector whose n-thcomponent is the number of bigrams (ti, ti+1)such that w(ti) = w1 and w(ti+1) = n. We let L(resp.
R) be the N?N matrix whose w-th row isL(w) (resp.
R(w)).SK?1 is the unit sphere in the K-dimensionalEuclidean space ?K.
For any x?
?K, we denoteby ?
(x) the projection of x on SK?1, i.e., ?
(x) =x/||x||.A labeling is a map A: W ?
T. Given a labelingA, we define )(~ 1wLA , the left-label context ofword type w1, as the K-dimensional vectorwhose k-th component is the number of bigrams(ti?1, ti) in the corpus such that w(ti) = w1 andA(w(ti?1)) = k. We define the left descriptor ofword type w as:))(~()( wLwL AA ??
.We similarly define the right-label context of w1,)(~ 1wRA , as the K-dimensional vector whose k-th component is the number of bigrams (ti, ti+1)such that w(ti) = w1 and A(w(ti+1)) = k, and wedefine the right descriptor of word type w as:))(~()( wRwR AA ??
.In short, any labeling A defines two maps, LAand RA, each from W to SK?1.For any function g(w) defined on W, ?g(w)?
willbe used to denote the average of g(w) weightedby the frequency of word type w in the corpus:?g(w)????
?w f(w)g(w).For any label k, we define:CL(k) = ?(?
LA(w): A(w) = k ?
).Thus, CL(k) is the projection on SK?1 of theweighted average of the left descriptors of theword types labeled k. We sometimes refer toCL(k) as the left centroid of cluster k. Note thatCL(k) depends on A in two ways, first in that theaverage is taken on words w such that A(w) = k,and second through the global dependency of LAon A.
We similarly define the right centroids:CR(k)= ?(?
?RA(w): A(w) = k ?
).Informally, we seek a labeling A such that, forany two word types w1 and w2 in W, w1 and w2are labeled the same if and only if LA(w1) andLA(w2) are close to each other on SK?1 and so areRA(w1) and RA(w2).
Formally, our goal is to finda labeling A that minimizes the objective func-tion:F(A)=?||LA(w)?CL(A(w))||2+||RA(w)?CR(A(w))||2?.Note that, just as in conventional K-means clus-tering, F(A) is the sum of the intra-clustersquared distances.
However, unlike conventionalK-means clustering, the descriptors of the ob-jects to be clustered depend themselves on theclustering.
We accordingly refer to LA and RA aslatent descriptors, and to the method describedin the next section as Latent-Descriptor Clus-tering, or LDC.Note, finally, that we do not seek the globalminimum of F(A).
This global minimum, 0, isobtained by the trivial assignment that maps allword types to a unique label.
Instead, we seek aminimum under the constraint that the labelingbe non-trivial.
As we shall see, this constraintneed not be imposed explicitly: the iterativeLDC algorithm, when suitably initialized andparameterized, converges to non-trivial localminima of F(A)?and these are shown to pro-vide excellent taggers.3 MethodsRecall that a mixture of Gaussians is a genera-tive model for a random variable taking values801in a Euclidean space ?r.
With K Gaussians, themodel is parameterized by:?
K mixture parameters, i.e., K non-negative numbers adding up to 1;?
K means, i.e., K points ?1,?,?K in ?r;?
K variance-covariance d?d matrices.The collection of all parameters defining themodel is denoted by ?.
EM is an iterative algo-rithm used to find a (local) maximizer of thelikelihood of N observed data points x1,?,xN ??r.
Each iteration of the algorithm includes an Ephase and an M phase.
The E phase consists ofcomputing, based on the current ?, a probabilis-tic assignment of each of the N observations tothe K Gaussian distributions.
These probabilisticassignments form an N?K stochastic matrix P,i.e., a matrix of non-negative numbers in whicheach row sums to 1.
The M phase consists ofupdating the model parameters ?, based on thecurrent assignments P. For more details, see,e.g., Bishop (2006).The structure of the LDC algorithm is very simi-lar to that of the EM algorithm.
Thus, each itera-tion of LDC consists of an E phase and an Mphase.
As observations are replaced by latentdescriptors, an iteration of LDC is best viewedas starting with the M phase.
The M phase firststarts by building a pair of latent-descriptor ma-trices LP and RP, from the soft assignments ob-tained in the previous iteration.
Note that thesedescriptors are now indexed by P, the matrix ofprobabilistic assignments, rather than by hardassignments A as in the previous section.LP and RP are obtained by a straightforward ad-aptation of the definition given in the previoussection to the case of probabilistic assignments.Thus, the latent descriptors consist of the left-word and right-word contexts (recall that theseare given by matrices L and R), mapped intoleft-label and right-label contexts through multi-plication by the assignment matrix P, and scaledto unit length:LP = ?
(LP)RP = ?
(RP).With these latent descriptors in hand, we pro-ceed with the M phase of the algorithm as usual.Thus, the left mean ?Lk for Gaussian k is theweighted average of the left latent descriptorsLP(w), scaled to unit length.
The weight used inthis weighted average is Pwk?f(w) (rememberthat f(w) is the frequency of word type w in thecorpus).
Note that the definition of the Gaussianmean ?Lk parallels the definition of the clustercentroid CL(k) given in the previous section; ifthe assignment P happens to be a hard assign-ment, ?Lk is actually identical to CL(k).
The rightGaussian mean ?Rk is computed in a similarfashion.
As mentioned, we do not estimate anymixture coefficients or variance-covariance ma-trices.The E phase of the iteration takes the latent de-scriptors and the Gaussian means, and computesa new N?K matrix of probabilistic assignmentsP.
These new assignments are given by:}2/]||)(||||)([||exp{1 222 ???
RkPLkPwk wRwLZP ????
?with Z a normalization constant such that?k Pwk = 1. ?
is a parameter of the model, which,as mentioned, is gradually decreased to enforceconvergence of P to a hard assignment.The description of the M phase given abovedoes not apply to the first iteration, since the Mphase uses P from the previous iteration.
To ini-tialize the algorithm, i.e., create a set of left andright descriptor vectors in the M phase of thefirst iteration, we use the left-word and right-word contexts L and R. These matrices howeverare of very high dimension (N?N), and thussparse and noisy.
We therefore reduce their di-mensionality, using reduced-rank singular-valuedecomposition.
This yields two N?r1 matrices,L1 and R1.
A natural choice for r1 is r1 = K, andthis was indeed used for K = 17.
For K = 50, wealso use r1 = 17.
The left and right descriptorsfor the first iteration are obtained by scalingeach row of matrices L1 and R1 to unit length.The Gaussian centers ?Lk and ?Rk, k = 1,?,K, areset equal to the left and right descriptors of the K802most frequent words in the corpus.
This com-pletes the description of the LDC algorithm.1While this algorithm is intuitive and simple, itdoes not easily lend itself to mathematicalanalysis; indeed there is no a priori guaranteethat it will behave as desired.
Even for the sim-pler, hard-assignment, K-means-style version ofLDC outlined in the previous section, there is noequivalent to the statement?valid for the con-ventional K-means algorithm?that each itera-tion lowers the intra-cluster sum of squared dis-tances F(A); this is a mere consequence of thefact that the descriptors themselves are updatedon each iteration.
The soft-assignment version ofLDC does not directly attempt to minimize F(A),nor can it be viewed as likelihood maximiza-tion?as is EM for a Gaussian mixture?sincethe use of latent descriptors precludes the defini-tion of a generative model for the data.
Thistheoretical difficulty is compounded by the useof a variable ?.Empirically however, as shown in the next sec-tion, we find that the LDC algorithm is very wellbehaved.
Two simple tools will be used to aid inthe description of the behavior of LDC.The first tool is an objective function G(P)that parallels the definition of F(A) for hard as-signments.
For a probabilistic assignment P, wedefine G(P) to be the weighted average, over allw and all k, of ||LP(w) ?
?Lk||2 + ||RP(w) ?
?Rk||2;the weight used in this average is Pwk?f(w), justas in the computation of the Gaussian means.Clearly, G is identical to F on any P that hap-pens to be a hard assignment.
Thus, G is actuallyan extension of the objective function F to softassignments.The second tool will allow us to compute atagging accuracy for soft assignments.
For thispurpose, we simply create, for any probabilisticassignment P, the obvious labeling A = A*(P)that maps w to k with highest Pwk.4 ResultsIn order to evaluate the performance of LDC, weapply it to the Wall Street Journal portion of the1 The LDC code, including tagging accuracy evaluation, isavailable at http://www.dam.brown.edu/people/elie/code/.Penn Treebank corpus (1,173,766 tokens, alllower-case, resulting in N = 43,766 word types).We compare the induced labels with two gold-standard tagsets:?
PTB45, the standard 45-tag PTB tagset.When using PTB45 as the gold standard,models induce 50 labels, to allow com-parison with Gao and Johnson (2008)and Lamar et al (2010).?
PTB17, the PTB tagset coarse-grainedto 17 tags (Smith and Eisner 2005).When using PTB17 as the gold standard,models induce 17 labels.In order to compare the labels generated by theunsupervised model with the tags of each tagset,we use two map-based criteria:?
MTO: many-to-one tagging accuracy,i.e., fraction of correctly-tagged tokensin the corpus under the so-called many-to-one mapping, which takes each in-duced tag to the gold-standard POS tagwith which it co-occurs most frequently.This is the most prevalent metric in usefor unsupervised POS tagging, and wefind it the most reliable of all criteriacurrently in use.
Accordingly, the studypresented here emphasizes the use ofMTO.?
OTO: best tagging accuracy achievableunder a so-called one-to-one mapping,i.e., a mapping such that at most one in-duced tag is sent to any POS tag.
Theoptimal one-to-one mapping is foundthrough the Hungarian algorithm2.2 Code by Markus Beuhren is available athttp://www.mathworks.com/matlabcentral/fileexchange/6543-functions-for-the-rectangular-assignment-problem803Figures 1 and 2 show the behavior of the LDCalgorithm for K = 17 and K = 50 respectively.From the G curves as well as from the MTOscoring curves (using the labeling A*(P) definedat the end of Section 3), it is clear that the algo-rithm converges.
The figures show only the first15 iterations, as little change is observed afterthat.
The schedule of the ?
parameter was giventhe simple form ?
(t) = ?1exp{?c(t?1)}, t =1,2,?, and the parameters ?1 and c were ad-justed so as to get the best MTO accuracy.
Withthe ?-schedules used in these experiments, Ptypically converges to a hard assignment inabout 45 iterations, ?
being then 10?5.Figure 1: Convergence of LDC with K = 17.
Bottomcurve: ?
-schedule, i.e., sequence of Gaussian widthsemployed.
Middle curve: Objective function G(P)(see Section 3).
Top curve: Many-to-one taggingaccuracy of labeling A*(P), evaluated againstPTB17.While the objective function G(P) mostly de-creases, it does show a hump for K = 50 arounditeration 9.
This may be due to the use of latentdescriptors, or of a variable ?, or both.
TheMTO score sometimes decreases by a smallfraction of a percent, after having reached itspeak around the 15th iteration.Note that we start ?
at 0.4 for K = 17, and at0.5 for K = 50.
Although we chose two slightlydifferent ?
schedules for the two tagsets in orderto achieve optimal performance on each tagset,an identical sequence of ?
can be used for bothwith only a 1% drop in PTB17 score.Figure 2: Same as Figure 1 but with K = 50.
Top curveshows the MTO accuracy of the labeling evaluatedagainst PTB45.As the width of the Gaussians narrows, eachvector is steadily pushed toward a single choiceof cluster.
This forced choice, in turn, producesmore coherent descriptor vectors for all wordtypes, and yields a steady increase in taggingaccuracy.804Table 1 compares the tagging accuracy of LDCwith several recent models of Gao and Johnson(2008) and Lamar et al (2010).The LDC results shown in the top half of thetable, which uses the MTO criterion, were ob-tained with the same ?-schedules as used in Fig-ures 1 and 2.
Note that the LDC algorithm isdeterministic.
However, the randomness in thesparse-matrix implementation of reduced-rankSVD used in the initialization step causes asmall variability in performance (the standarddeviation of the MTO score is 0.0004 for PTB17and 0.003 for PTB45).
The LDC results reportedare averages over 20 runs.
Each run was haltedat iteration 15, and the score reported uses thelabeling A*(P) defined at the end of Section 3.The LDC results shown in the bottom half ofthe table, which uses the OTO criterion, wereobtained with a variant of the LDC algorithm, inwhich the M phase estimates not only the Gaus-sian means but also the mixture coefficients.Also, different ?-schedules were used,3For both PTB17 and PTB45, and under bothcriteria, LDC's performance nearly matches orexceeds (often by a large margin) the resultsachieved by the other models.
We find the large3 All details are included in the code available athttp://www.dam.brown.edu/people/elie/code/.increase achieved by LDC in the MTO perform-ance under the PTB45 tagset particularly com-pelling.
It should be noted that Abend et al(2010) report 71.6% MTO accuracy for PTB45,but they treat all punctuation tags differently intheir evaluation and therefore these results can-not be directly compared.
Berg-Kirkpatrick et al(2010) report 75.5% MTO accuracy for PTB45by incorporating other features such as mor-phology; Table 1 is limited to distributional-onlymethods.Criterion  Model  PTB17 PTB45MTO LDC  0.751 0.708SVD2 0.740 0.658HMM-EM 0.647 0.621HMM-VB 0.637 0.605HMM-GS 0.674 0.660OTO LDC 0.593 0.483SVD2 0.541 0.473HMM-EM 0.431 0.405HMM-VB 0.514 0.461HMM-GS 0.466 0.499Table 1.
Tagging accuracy comparison betweenseveral models for two tagsets and two mappingcriteria.
Note that LDC significantly outperformsall HMMs (Gao and Johnson, 2008) in every caseexcept PTB45 under the OTO mapping.
LDC alsooutperforms SVD2 (Lamar et al, 2010) in allcases.Figure 3:  Mislabeled words per tag, using thePTB17 tagset.
Black bars indicate mislabeled wordswhen 17 clusters are used.
Gray bars indicate wordsthat continue to be mislabeled even when every wordtype is free to choose its own label, as if each typewere in its own cluster?which defines the theoreti-cally best possible non-disambiguating model.
Top:fraction of the corpus mislabeled, broken down bygold tags.
Bottom: fraction of tokens of each tag thatare mislabeled.
Many of the infrequent tags are100% mislabeled because no induced label ismapped to these tags under MTO.Figure 3 demonstrates the mistakes made byLDC under the MTO mapping.
From the topgraph, it is clear that the majority of the missedtokens are open-class words ?
most notably ad-jectives and adverbs.
Over 8% of the tokens inthe corpus are mislabeled adjectives ?
roughlyone-third of all total mislabeled tokens (25.8%).Furthermore, the corresponding bar in the bot-tom graph indicates that over half of the adjec-tives are labeled incorrectly.
Similarly, nearly4% of the mislabeled tokens are adverbs, butevery adverb in the corpus is mislabeled becauseno label is mapped to this tag ?
a common oc-805currence under MTO, shared by seven of theseventeen tags.Figure 4: The confusion matrix for LDC's labeling under PTB17.
The area of a black square indicates the numberof tokens in each element of the confusion matrix.
The diamonds indicate the induced tag under the MTO map-ping.
Several labels are mapped to N (Noun), and one of these labels causes appreciable confusion between nounsand adjectives.
Because multiple labels are dedicated to a single tag (N, V and PREP), several tags (in this case 7)are left with no label.To further illuminate the errors made by LDC,we construct the confusion matrix (figure 4).Element (i,j) of this matrix stores the fraction ofall tokens of POS tag i that are given label j bythe model.
In a perfect labeling, exactly oneelement of each row and each column would benon-zero.
As illustrated in figure 4, the confu-sion matrices produced by LDC are far fromperfect.
LDC consistently splits the Nouns intoseveral labels and often confuses Nouns and Ad-jectives under a single label.
These types ofmistakes have been observed as well in modelsthat use supervision (Haghighi and Klein, 2006).5 DiscussionWhen devising a model for unsupervised POSinduction, one challenge is to choose a model ofadequate complexity, this choice being related tothe bias-variance dilemma ubiquitous in statisti-cal estimation problems.
While large datasets areavailable, they are typically not large enough toallow efficient unsupervised learnability in mod-els that are powerful enough to capture complexfeatures of natural languages.
Ambiguity is oneof these features.
Here we propose a new ap-proach to this set of issues: start with a modelthat explicitly entertains ambiguity, and gradu-ally constrain it so that it eventually convergesto an unambiguous tagger.Thus, although the algorithm uses probabilis-tic assignments, of Gaussian-mixture type, thegoal is the construction of hard assignments.
By806requiring the Gaussians to be isotropic with uni-form width and by allowing that width to shrinkto zero, the algorithm forces the soft assign-ments to converge to a set of hard assignments.Based on its performance, this simulated-annealing-like approach appears to provide agood compromise in the choice of model com-plexity.LDC bears some similarities with the algorithmof Ney, Essen and Kneser (1994), further im-plemented, with extensions, by Clark (2003).Both models use an iterative approach to mini-mize an objective function, and both initializewith frequent words.
However, the model ofNey et al is, in essence, an HMM where eachword type is constrained to belong to a singleclass (i.e., in HMM terminology, be emitted by asingle hidden state).
Accordingly, the objectivefunction is the data likelihood under this con-strained HMM.
This takes into account only therightward transition probabilities.
Our approachis conceptually rather different from an HMM.
Itis more similar to the approach of Sch?tze(1995) and Lamar et al (2010), where eachword type is mapped into a descriptor vectorderived from its left and right tag contexts.
Ac-cordingly, the objective function is that of the K-means clustering problem, namely a sum of in-tra-cluster squared distances.
This objectivefunction, unlike the likelihood under an HMM,takes into account both left and right contexts.
Italso makes use in a crucial way of cluster cen-troids (or Gaussian means), a notion that has nocounterpart in the HMM approach.
We note thatLDC achieves much better results (by about10%) than a recent implementation of the Ney etal.
approach (Reichart et al 2010).The only parameters in LDC are the two pa-rameters used to define the ?
schedule, and r1used in the first iteration.
Performance was gen-erally found to degrade gracefully with changesin these parameters away from their optimal val-ues.
When ?
was made too large in the first fewiterations, it was found that the algorithm con-verges to the trivial minimum of the objectivefunction F(A), which maps all word types to aunique label (see section 2).
An alternativewould be to estimate the variance for each Gaus-sian separately, as is usually done in EM forGaussian mixtures.
This would not necessarilypreclude the use of an iteration-dependent scal-ing factor, which would achieve the goal ofgradually forcing the tagging to become deter-ministic.
Investigating this and related options isleft for future work.Reduced-rank SVD is used in the initializationof the descriptor vectors, for the optimization toget off the ground.
The details of this initializa-tion step do not seem to be too critical, as wit-nessed by robustness against many parameterchanges.
For instance, using only the 400 mostfrequent words in the corpus?instead of allwords?in the construction of the left-word andright-word context vectors in iteration 1 causesno appreciable change in performance.The probabilistic-assignment algorithm wasfound to be much more robust against parameterchanges than the hard-assignment version ofLDC, which parallels the classical K-meansclustering algorithm (see Section 1).
We ex-perimented with this hard-assignment latent-descriptor clustering algorithm (data not shown),and found that a number of additional deviceswere necessary in order to make it work prop-erly.
In particular, we found it necessary to usereduced-rank SVD on each iteration of the algo-rithm?as opposed to just the first iteration inthe version presented here?and to graduallyincrease the rank r. Further, we found it neces-sary to include only the most frequent words atthe beginning, and only gradually incorporaterare words in the algorithm.
Both of these de-vices require fine tuning.
Provided they are in-deed appropriately tuned, the same level of per-formance as in the probabilistic-assignment ver-sion could be achieved.
However, as mentioned,the behavior is much less robust with hard clus-tering.Central to the success of LDC is the dynamicinterplay between the progressively harder clus-ter assignments and the updated latent descriptorvectors.
We operate under the assumption that ifall word types were labeled optimally, wordsthat share a label should have similar descriptorvectors arising from this optimal labeling.These similar vectors would continue to be clus-tered together, producing a stable equilibrium in807the dynamic process.
The LDC algorithm dem-onstrates that, despite starting far from this op-timal labeling, the alternation between vectorupdates and assignment updates is able to pro-duce steadily improving clusters, as seen by thesteady increase of tagging accuracy.We envision the possibility of extending thisapproach in several ways.
It is a relatively sim-ple matter to extend the descriptor vectors toinclude context outside the nearest neighbors,which may well improve performance.
In viewof the computational efficiency of LDC, whichruns in under one minute on a desktop PC, theadded computational burden of working with theextended context is not likely to be prohibitive.LDC could also be extended to include morpho-logical or other features, rather than relying ex-clusively on context.
Again, we would antici-pate a corresponding increase in accuracy fromthis additional linguistic information.ReferencesOmri Abend, Roi Reichart and Ari Rappoport.
Im-proved Unsupervised POS Induction through Pro-totype Discovery.
2010.
In Proceedings of the48th Annual Meeting of the ACL.Christopher M. Bishop.
2006.
Pattern Recognitionand Machine Learning.
Springer-Verlag, NewYork, LLC.Taylor Berg-Kirkpatrick, Alexandre Bouchard-C?t?,John DeNero, and Dan Klein.
2010.
Painless Un-supervised Learning with Features.
In proceedingsof NAACL 2010.Alexander Clark.
2001.
The unsupervised inductionof stochastic context-free grammars using distribu-tional clustering.
In CoNLL.Alexander Clark.
2003.
Combining distributional andmorphological information for part of speech in-duction.
In 10th Conference of the EuropeanChapter of the Association for Computational Lin-guistics, pages 59?66.Jianfeng Gao and Mark Johnson.
2008.
A comparisonof bayesian estimators for unsupervised HiddenMarkov Model POS taggers.
In Proceedings of the2008 Conference on Empirical Methods in NaturalLanguage Processing, pages 344?352.Sharon Goldwater and Tom Griffiths.
2007.
A fullyBayesian approach to unsupervised part-of-speechtagging.
In Proceedings of the 45th Annual Meet-ing of the Association of Computational Linguis-tics, pages 744?751.Jo?o V. Gra?a, Kuzman Ganchev, Ben Taskar, andFernando Pereira.
2009.
Posterior vs. ParameterSparsity in Latent Variable Models.
Neural Infor-mation Processing Systems Conference (NIPS).Michael Lamar, Yariv Maron, Mark Johnson, ElieBienenstock.
2010.
SVD and Clustering for Unsu-pervised POS Tagging.
In Proceedings of the 48thAnnual Meeting of the ACL.Aria Haghighi and Dan Klein.
2006.
Prototype-driven learning for sequence models.
In Proceed-ings of the Human Language Technology Confer-ence of the NAACL, Main Conference, pages 320?327, New York City, USA, June.
Association forComputational Linguistics.William P. Headden, David McClosky, and EugeneCharniak.
2008.
Evaluating unsupervised part-of-speech tagging for grammar induction.
In Pro-ceedings of the International Conference on Com-putational Linguistics (COLING ?08).Mark Johnson.
2007.
Why doesn?t EM find goodHMM POS-taggers?
In Proceedings of the 2007Joint Conference on Empirical Methods in NaturalLanguage Processing and Computational NaturalLanguage Learning (EMNLP-CoNLL), pages296?305.Hermann Ney, Ute Essen, and Reinhard Kneser.1994.
On structuring probabilistic dependences instochastic language modelling.
Computer Speechand Language, 8, 1-38.Roi Reichart, Raanan Fattal and Ari Rappoport.2010.
Improved Unsupervised POS Induction Us-ing Intrinsic Clustering Quality and a Zipfian Con-straint.
CoNLL.Sujith Ravi and Kevin Knight.
2009.
Minimizedmodels for unsupervised part-of-speech tagging.
InProceedings of the 47th Annual Meeting of theACL and the 4th IJCNLP of the AFNLP, pages504?512.Hinrich Sch?tze.
1995.
Distributional part-of-speechtagging.
In Proceedings of the seventh conferenceon European chapter of the Association for Com-putational Linguistics, pages 141?148.Noah A. Smith and Jason Eisner.
2005.
Contrastiveestimation: Training log-linear models on unla-beled data.
In Proceedings of the 43rd Annual808Meeting of the Association for Computational Lin-guistics (ACL?05), pages 354?362.Kristina Toutanova, Dan Klein, Christopher D. Man-ning and Yoram Singer.
2003.
Feature-rich part-of-speech tagging with a cyclic dependency net-work.
In Proceedings of HLT-NAACL 2003, pages252-259.Yoshimasa Tsuruoka and Jun'ichi Tsujii.
2005.
Bidi-rectional Inference with the Easiest-First Strategyfor Tagging Sequence Data.
In Proceedings ofHLT/EMNLP, pp.
467-474.809
