Proceedings of the 15th Conference on Computational Natural Language Learning: Shared Task, pages 112?116,Portland, Oregon, 23-24 June 2011. c?2011 Association for Computational LinguisticsUBIU: A Robust System for Resolving Unrestricted CoreferenceDesislava ZhekovaUniversity of Bremenzhekova@uni-bremen.deSandra Ku?blerIndiana Universityskuebler@indiana.eduAbstractIn this paper, we discuss the applicationof UBIU to the CONLL-2011 shared taskon ?Modeling Unrestricted Coreference?
inOntoNotes.
The shared task concentrates onthe detection of coreference not only in nounphrases but also involving verbs.
The infor-mation provided for the closed track includedWordNet as well as corpus generated numberand gender information.
Our system showsno improvement when using WordNet infor-mation, and the number information provedless reliable than the information in the partof speech tags.1 IntroductionCoreference Resolution is the process of identify-ing the linguistic expressions in a discourse that re-fer to the same real world entity and to divide thoseexpressions into equivalence classes that representeach discourse entity.
For this task, a deeper knowl-edge of the discourse is often required.
However,such knowledge is difficult to acquire.
For this rea-son, many systems use superficial information suchas string match.
The CoNLL shared task on ?Model-ing Unrestricted Coreference in OntoNotes?
(Prad-han et al, 2011) presents challenges that go be-yond previous definitions of the task.
On the onehand, mention extraction is part of the task whilemany previous approaches assumed gold standardmentions.
On the other hand, coreference is notrestricted to noun phrases, verbs are also included.Thus, in Sales of passenger cars grew 22%.
Thestrong growth followed year-to-year increases., theverb grew has an identity relation with the nounphrase The strong growth.The system that we used for the shared task isthe memory-based machine learning system UBIU(Zhekova and Ku?bler, 2010).
We describe the mostimportant components of the system in section 2.The system was originally developed for robust,multilingual coreference resolution, and thus had tobe adapted to this shared task.
We investigate thequality of our mention extraction in section 2.1 andthe quality of the features used in the classifier insection 2.2.
In section 3, we present UBIU?s resultson the development set, and in section 4, UBIU?sfinal results in the shared task.2 UBIUUBIU (Zhekova and Ku?bler, 2010) was developedas a multilingual coreference resolution system.
Arobust approach is necessary to make the system ap-plicable for a variety of languages.
For this rea-son, we use a machine learning approach to clas-sify mention pairs.
We use TiMBL (Daelemans etal., 2007), a memory-based learner (MBL) that la-bels the feature vectors from the test set based onthe k nearest neighbors in the training instances.We chose TiMBL since MBL has been shown towork well with small training sets.
A non-exhaustiveparameter optimization on the development set ledus to use the IB1 algorithm, similarity is computedbased on weighted overlap, the relevance weightsare computed using gain ratio and the number ofnearest neighbors is set to k = 3 (for a descriptionof the algorithm and parameters cf.
(Daelemans etal., 2007)).
The classifier is preceded by a mentionextractor, which identifies possible mentions, and afeature extractor.
The latter creates a feature vec-tor for each possible pair of a potentially coreferring112mention and all possible antecedents in a context of3 sentences.
Another important step is to separatesingleton mentions from coreferent ones since onlythe latter are annotated in OntoNotes.
Our markableextractor overgenerates in that it extracts all possi-ble mentions, and only after classification, the sys-tem can decide which mentions are singletons.
Weinvestigate the performance of the mention and fea-ture extraction modules in more detail below.2.1 Mention ExtractionUBIU?s mention extractor uses part-of-speech(POS), syntactic, and lemma information providedin the OntoNotes data set to detect mentions.
Themodule defines a mention for each noun phrase,based on syntactic information, as well as for allpossessive pronouns and all proper nouns, based ontheir POS tags.
Since for the shared task, verbs arealso potentially coreferent, we included a mentionfor each of the verbs with a predicate lemma.
An ex-ample of the output of the mention extraction mod-ule is shown in table 1.
Each mention is numberedwith an individual number and thus still represents adistinct entity.
Since singleton mentions are not an-notated in the OntoNotes data set, mentions withoutcoreference relations after classification need to beremoved from the answer set, which can only be per-formed after coreference resolution when all coref-erent pairs are identified.
For this reason, the mark-able extractor is bound to overgenerate.
The lattercan clearly be seen when the mention extraction out-put is compared to the provided gold mentions (cf.the last column in table 1).We conducted a simple experiment on the devel-opment data in order to gain insight into the per-formance of the mention extraction module.
Usingthe scorer provided by the shared task, we evaluatedthe output of the module, without performing coref-erence resolution and without removing singletonmentions.
This led to a recall of 96.55 % and a preci-sion of 18.55%, resulting in an F-score of 31.12.
Thehigh recall shows that the system is very reliable infinding mentions with the correct boundaries.
How-ever, since we do not remove any singletons, UBIUovergenerates and thus the system identified a con-siderable number of singletons, too.
Nevertheless,the fact that UBIU identified 96.55% of all mentionsshows that the performance of the mention extrac-# Word POS Parse bit ME output Gold0 Devastating VBG (TOP(NP(NP* (1)|(2|(3 -1 Critique NN *) 3) -2 of IN (PP* - -3 the DT (NP* (4 (324 Arab JJ * - -5 World NN *)) 4) 32)6 by IN (PP* - -7 One CD (NP(NP*) (5|(6) -8 of IN (PP* - -9 Its PRP$ (NP* (7)|(8 (32)10 Own JJ *)))))) 8)|5)|2) -Table 1: The output of the mention extractor for a samplesentence.tion module is close to optimal.2.2 Feature ExtractionFeature extraction is the second important subtaskfor the UBIU pipeline.
Since mentions are repre-sented by their syntactic head, the feature extractoruses a heuristic that selects the rightmost noun in anoun phrase.
However, since postmodifying prepo-sitional phrases may be present in the mention, thenoun may not be followed by a preposition.
For eachmention, a feature vector is created for all of its pre-ceding mentions in a window of 3 sentences.
Af-ter classification, a filter can optionally be appliedto filter out mention pairs that disagree in number,and another filter deletes all mentions that were notassigned an antecedent in classification.
Note thatthe number information was derived from the POStags and not from the number/gender data providedby the shared task since the POS information provedmore reliable in our system.Initially, UBIU was developed to use a wide set offeatures (Zhekova and Ku?bler, 2010), which consti-tutes a subset of the features described by Rahmanand Ng (2009).
For the CONLL-2011 shared task,we investigated the importance of various additionalfeatures that can be included in the feature set usedby the memory-based classifier.
Thus, we conductedexperiments with a base set and an extended featureset, which makes use of lexical semantic features.Base Feature Set Since the original feature set inZhekova and Ku?bler (2010) contained informationthat is not easily accessible in the OntoNotes dataset (such as grammatical functions), we had to re-strict the feature set to information that can be de-rived solely from POS annotations.
Further infor-113# Feature Description1 mj - the antecedent2 mk - the mention to be resolved3 Y ifmj is a pronoun; else N4 number - S(ingular) or P(lural)5 Y ifmk is a pronoun; else N6 C if the mentions are the same string; else I7 C if one mention is a substring of the other; else I8 C if both mentions are pronominal and are the samestring; else I9 C if the two mentions are both non-pronominal andare the same string; else I10 C if both mentions are pronominal and are either thesame pronoun or different only w.r.t.
case;NA if at least one of them is not pronominal; else I11 C if the mentions agree in number; I if they disagree;NA if the number for one orboth mentions cannot be determined12 C if both mentions are pronouns; I if neither arepronouns; else NA13 C if both mentions are proper nouns; I if neither areproper nouns; else NA14 sentence distance between the mentionsTable 2: The pool of features used in the base feature set.mation as sentence distance, word overlap etc.
wasincluded as well.
The list of used features is shownin table 2.Extended Feature Set Since WordNet informa-tion was provided for the closed setting of theCONLL-2011 shared task, we also used an ex-tended feature set, including all features from thebase set alng with additional features derived fromWordNet.
The latter features are shown in table 3.2.3 SingletonsIn section 2.1, we explained that singletons need tobe removed after classification.
However, this leadsto a drastic decrease in system performance for tworeasons.
First, if a system does not identify a coref-erence link, the singleton mentions will be removedfrom the coreference chains, and consequently, thesystem is penalized for the missing link as well asfor the missing mentions.
If singletons are included,the system will still receive partial credit for themfrom all metrics but MUC.
For this reason, we in-vestigated filtered and non-filtered results in combi-nation with the base and the extended feature sets.3 Results on the Development SetThe results of our experiment on the developmentset are shown in table 4.
Since the official scoresof the shared task are based on an average of MUC,# Feature Description15 C if both are nouns andmk is hyponym ofmj ; I if bothare nouns butmk is not a hyponym ofmj ; NA otherwise16 C if both are nouns andmj is hyponym ofmk; I if bothare nouns butmj is not a hyponym ofmk; NA otherwise17 C if both are nouns andmk is a partial holonym ofmj ;I if both are nouns butmk is not a partial holonym ofmj ;NA otherwise18 C if both are nouns andmj is a partial holonym ofmk;I if both are nouns butmj is not a partial holonym ofmk;NA otherwise19 C if both are nouns andmk is a partial meronym ofmj ;I if both are nouns butmk is not a partial meronym ofmj ;NA otherwise20 C if both are nouns andmj is a partial meronym ofmk;I if both are nouns butmj is not a partial meronym ofmk;NA otherwise21 C if both are verbs andmk entailsmj ; I if both areverbs butmk does not entailmj ; NA otherwise22 C if both are verbs andmj entailsmk; I if both areverbs butmj does not entailmk; NA otherwise23 C if both are verbs andmk is a hypernym ofmj ;I if both are verbs butmk is not a hypernym ofmj ;NA otherwise24 C if both are verbs andmj is a hypernym ofmk;I if both are verbs butmj is not a hypernym ofmk;NA otherwise25 C if both are verbs andmk is a troponym ofmj ;I if both are verbs butmk is not a troponym ofmj ;NA otherwise26 C if both are verbs andmj is a troponym ofmk;I if both are verbs butmj is not a troponym ofmk;NA otherwiseTable 3: The features extracted from WordNet.B3, and CEAFE, we report these measures and theiraverage.
All the results in this section are based onautomatically annotated linguistic information.
Thefirst part of the table shows the results for the basefeature set (UBIUB), the second part for the ex-tended feature set (UBIUE).
We also report resultsif we keep all singletons (& Sing.)
and if we filterout coreferent pairs that do not agree in number (&Filt.).
The results show that keeping the singletonsresults in lower accuracies on the mention and thecoreference level.
Only recall on the mention levelprofits from the presence of singletons.
Filtering fornumber agreement with the base set has a detrimen-tal effect on mention recall but increases mentionprecision so that there is an increase in F-score of1%.
However, on the coreference level, the effect isnegligible.
For the extended feature set, filtering re-sults in a decrease of approximately 2.0% in mentionprecision, which also translates into lower corefer-ence scores.
We also conducted an experiment inwhich we filter before classification (& Filt.
BC),following a more standard approach.
The reasoning114IM MUC B3 CEAFE AverageR P F1 R P F1 R P F1 R P F1 F1UBIUB 62.71 38.66 47.83 30.59 24.65 27.30 67.06 62.65 64.78 34.19 40.16 36.94 43.01UBIUB & Sing.
95.11 18.27 30.66 30.59 24.58 27.26 67.10 62.56 64.75 34.14 40.18 36.92 42.97UBIUB & Filt.
61.30 40.58 48.83 29.10 25.77 27.33 64.88 64.63 64.76 35.38 38.74 36.98 43.02UBIUB & Filt.
BC 61.33 40.49 48.77 28.96 25.54 27.14 64.95 64.48 64.71 35.23 38.71 36.89 42.91UBIUE 62.72 39.09 48.16 30.63 24.94 27.49 66.72 62.76 64.68 34.19 39.90 36.82 43.00UBIUE & Sing.
95.11 18.27 30.66 29.87 20.96 24.64 69.13 57.71 62.91 32.28 42.24 36.59 41.38UBIUE & Filt.
63.01 36.62 46.32 28.65 21.05 24.27 68.10 58.72 63.06 32.91 41.53 36.72 41.35Gold ME 100 100 100 38.83 82.97 52.90 39.99 92.33 55.81 66.73 26.75 38.19 48.97Table 4: UBIU system results on the development set.is that the training set for the classifier is biased to-wards not assuming coreference since the majorityof mention pairs does not have a coreference rela-tion.
Thus filtering out non-agreeing mention pairsbefore classification reduces not only the number oftest mention pairs to be classified but also the num-ber of training pairs.
However, in our system, thisapproach leads to minimally lower results, which iswhy we decided not to pursue this route.
We alsoexperimented with instance sampling in order to re-duce the bias towards non-coreference in the trainingset.
This also did not improve results.Contrary to our expectation, using ontological in-formation does not improve results.
Only on themention level, we see a minimal gain in precision.But this does not translate into any improvement onthe coreference level.
Using filtering in combinationwith the extended feature set results in a more pro-nounced deterioration than with the base set.The last row of table 4 (Gold ME) shows re-sults when the system has access to the gold stan-dard mentions.
The MUC and B3 results show thatthe classifier reaches an extremely high precision(82.97% and 92.33%), from which we conclude thatthe coreference links that our system finds are re-liable, but it is also too conservative in assumingcoreference relations.
For the future, we need toinvestigate undersampling the negative examples inthe training set and more efficient methods for filter-ing out singletons.4 Final ResultsIn the following, we present the UBIU system re-sults in two separate settings: using the test set withautomatically extracted mentions (section 4.1) andusing a test set with gold standard mentions, includ-ing singletons (section 4.2).
An overview of all sys-tems participating in the CONLL-2011 shared taskand their results is provided by Pradhan et al (2011).4.1 Automatic Mention IdentificationThe final results of UBIU for the test set withoutgold standard mentions are shown in the first partof table 5.
They are separated into results for thecoreference resolution module based on automati-cally annotated linguistic information and the goldannotations.
Again, we report results for both thebase feature set (UBIUB) and the extended featureset usingWordNet features (UBIUE).
A comparisonof the system results on the test and the developmentset in the UBIUB setting shows that the average F-score is considerably lower for the test set, 40.46 vs.43.01 although the quality of the mentions remainsconstant with an F-score of 48.14 on the test set and47.83 on the development set.The results based on the two data sets show thatUBIU?s performance improves when the system hasaccess to gold standard linguistic annotations.
How-ever, the difference between the results is in the areaof 2%.
The improvement is due to gains of 3-5%in precision for MUC and B3, which are counter-acted by smaller losses in recall.
In contrast, CEAFEshows a loss in precision and a similar gain in recall,resulting in a minimal increase in F-score.A comparison of the results for the experimentswith the base set as opposed to the extended set in5 shows that the extended feature set using Word-Net information is detrimental to the final results av-eraged over all metrics while it led to a slight im-provement on the mention level.
Our assumptionis that while in general, the ontological informationis useful, the additional information may be a mix-ture of relevant and irrelevant information.
Mihalcea(2002) showed for word sense disambiguation that115IM MUC B3 CEAFE AverageR P F1 R P F1 R P F1 R P F1 F1Automatic Mention IdentificationautoUBIUB 67.27 37.48 48.14 28.75 20.61 24.01 67.17 56.81 61.55 31.67 41.22 35.82 40.46UBIUE 67.49 37.60 48.29 28.87 20.66 24.08 67.14 56.67 61.46 31.57 41.21 35.75 40.43goldUBIUB 65.92 40.56 50.22 31.05 25.57 28.04 64.94 62.23 63.56 33.53 39.08 36.09 42.56UBIUE 66.11 40.37 50.13 30.84 25.14 27.70 65.07 61.83 63.41 33.23 39.05 35.91 42.34Gold Mention BoundariesautoUBIUB 67.57 58.66 62.80 34.14 40.43 37.02 54.24 71.09 61.53 39.65 33.73 36.45 45.00UBIUE 69.19 57.27 62.67 33.48 37.15 35.22 55.47 68.23 61.20 38.29 34.65 36.38 44.27goldUBIUB 67.64 58.75 62.88 34.37 40.68 37.26 54.28 71.18 61.59 39.69 33.76 36.49 45.11UBIUE 67.72 58.66 62.87 34.18 40.40 37.03 54.30 71.04 61.55 39.64 33.78 36.47 45.02Table 5: Final system results for the coreference resolution module on automatically extracted mentions on the goldstandard mentions for the base and extended feature sets.memory-based learning is extremely sensitive to ir-relevant features.
For the future, we are planningto investigate this problem by applying forward-backward feature selection, as proposed by Mihal-cea (2002) and Dinu and Ku?bler (2007).4.2 Gold Mention BoundariesUBIU was also evaluated in the experimental set-ting in which gold mention boundaries were pro-vided in the test set, including for singletons.
Theresults of the setting using both feature sets are re-ported in the second part of table 5.
The results showthat overall the use of gold standard mentions re-sults in an increase of the average F-score of approx.4.5%.
Where mention quality and MUC are con-cerned, gold standard mentions have a significantpositive influence on the average F-score.
For B3and CEAFE, however, there is no significant changein scores.
The increase in performance is most no-ticeable in mention identification, for which the F-score increases from 48.14 to 62.80.
But this im-provement has a smaller effect on the overall coref-erence system performance leading to a 5% increaseof results.
In contrast to the gold mention results inthe development set, we see lower precision valuesin the test set.
This is due to the fact that the test setcontains singletons.
Detecting singletons reliably isa difficult problem that needs further investigation.5 Conclusion and Future WorkIn the current paper, we presented the results ofUBIU in the CONLL-2011 shared task.
We showedthat for a robust system for coreference resolutionsuch as UBIU, automatically annotated linguisticdata is sufficient for mention-pair based coreferenceresolution.
We also showed that ontological infor-mation as well as filtering non-agreeing mentionpairs leads to an insignificant improvement of theoverall coreference system performance.
The treat-ment of singletons in the data remains a topic thatrequires further investigation.ReferencesWalter Daelemans, Jakub Zavrel, Ko van der Sloot, andAntal van den Bosch.
2007.
TiMBL: Tilburg memorybased learner ?
version 6.1 ?
reference guide.
Techni-cal Report ILK 07-07, Induction of Linguistic Knowl-edge, Computational Linguistics, Tilburg University.Georgiana Dinu and Sandra Ku?bler.
2007.
Sometimesless is more: Romanian word sense disambiguationrevisited.
In Proceedings of the International Confer-ence on Recent Advances in Natural Language Pro-cessing, RANLP 2007, Borovets, Bulgaria.Rada Mihalcea.
2002.
Instance based learning withautomatic feature selection applied to word sensedisambiguation.
In Proceedings of the 19th Inter-national Conference on Computational Linguistics,COLING?02, Taipeh, Taiwan.Sameer Pradhan, Lance Ramshaw, Mitchell Marcus,Martha Palmer, Ralph Weischedel, and Nianwen Xue.2011.
CoNLL-2011 Shared Task: Modeling unre-stricted coreference in OntoNotes.
In Proceedings ofthe Fifteenth Conference on Computational NaturalLanguage Learning (CoNLL), Portland, Oregon.Altaf Rahman and Vincent Ng.
2009.
Supervised modelsfor coreference resolution.
In Proceedings of EMNLP2009, Singapore.Desislava Zhekova and Sandra Ku?bler.
2010.
UBIU: Alanguage-independent system for coreference resolu-tion.
In Proceedings of the 5th International Workshopon Semantic Evaluation (SemEval), pages 96?99, Up-psala, Sweden.116
