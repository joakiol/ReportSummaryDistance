Proceedings of the 53rd Annual Meeting of the Association for Computational Linguisticsand the 7th International Joint Conference on Natural Language Processing (Short Papers), pages 346?351,Beijing, China, July 26-31, 2015.c?2015 Association for Computational LinguisticsLearning Topic Hierarchies for Wikipedia CategoriesLinmei Hu?, Xuzhong Wang?, Mengdi Zhang?, Juanzi Li?, Xiaoli Li?,Chao Shao?, Jie Tang?, Yongbin Liu??Dept.
of Computer Sci.
and Tech.
Tsinghua University, China?State Key Laboratory of Math.
Eng.
and Advanced Computing, China?Institute for Infocomm Research(I2R), A*STAR, Singapore{hulinmei1991,koodoneko,mdzhangmd,lijuanzi2008}@gmail.comxlli@i2r.a-star.edu.sg, birdlinux@gmail.comjietang@tsinghua.edu.cn, yongbinliu03@gmail.comAbstractExisting studies have utilized Wikipediafor various knowledge acquisition tasks.However, no attempts have been made toexplore multi-level topic knowledge con-tained in Wikipedia articles?
Contents ta-bles.
The articles with similar subjectsare grouped together into Wikipedia cat-egories.
In this work, we propose novelmethods to automatically construct com-prehensive topic hierarchies for given cat-egories based on the structured Contentstables as well as corresponding unstruc-tured text descriptions.
Such a hierarchy isimportant for information browsing, doc-ument organization and topic prediction.Experimental results show our proposedapproach, incorporating both the structuraland textual information, achieves highquality category topic hierarchies.1 IntroductionAs a free-access online encyclopedia, writtencollaboratively by people all over the world,Wikipedia (abbr.
to Wiki) offers a surplus of richinformation.
Millions of articles cover variousconcepts and instances1.
Wiki has been wide-ly used for various knowledge discovery tasks.Some good examples include knowledge miningfrom Wiki infoboxes (Lin et al, 2011; Wang et al,2013), and taxonomy deriving from Wiki categorysystem (Zesch and Gurevych, 2007).We observe that, in addition to Wiki?s infobox-es and category system, Wiki articles?
Contentstables (CT for short) also provide valuable struc-tured topic knowledge with different levels ofgranularity.
For example, in the article ?2010Haiti Earthquake?, shown in Fig.1, the left Con-tents zone is a CT formed in a topic hierarchy for-1http://en.wikipedia.org/wiki/EncyclopediaFigure 1: The Wiki article ?2010 Haiti Earth-quake?
with structured Contents table and corre-sponding unstructured text descriptions.mat.
If we view ?2010 Haiti earthquake?
as theroot topic, the first-level ?Geology?
and ?Dam-age to infrastructure?
tags can be viewed as it-s subtopics, and the second-level ?Tsunami?
and?Aftershocks?
tags underneath ?Geology?
are thesubtopics of ?Geology?.
Clicking any of the tagsin Contents, we can jump to the corresponding textdescription.
Wiki articles contain a wealth of thiskind of structured and unstructured information.However, to our best knowledge, little work hasbeen done to leverage the knowledge in CT.In Wiki, similar articles (each with their ownCT) belonging to the same subject are groupedtogether into a Wiki category.
We aim to inte-grate multiple topic hierarchies represented by C-T (from the articles under the same Wiki catego-ry) into a comprehensive category topic hierar-chy (CTH).
While there also exist manually builtCTH represented by CT in corresponding Wik-i articles, they are still too high-level and incom-plete.
Take the ?Earthquake?
category as an exam-ple, its corresponding Wiki article2only contains2http://en.wikipedia.org/wiki/Earthquake346some major and common topics.
It does not in-clude the subtopic ?nuclear power plant?, whichis an important subtopic of the ?2011 Japan earth-quake?.
A comprehensive CTH is believed to bemore useful for information browsing, documentorganization and topic extraction in new text cor-pus (Veeramachaneni et al, 2005).
Thus, we pro-pose to investigate the Wiki articles of the samecategory to automatically build a comprehensiveCTH to enhance the manually built CTH.Clearly, it is very challenging to learn a CTHfrommultiple topic hierarchies in different articlesdue to the following 3 reasons: 1) A topic can bedenoted by a variety of tags in different articles(e.g., ?foreign aids?
and ?aids from other coun-tries?
); 2) Structural/hierarchical information canbe inconsistent (or even opposite) across differen-t articles (e.g., ?response subtopicOf aftermath?and ?aftermath subtopicOf response?
in differentearthquake event articles); 3) Intuitively, text de-scriptions of the topics in Wiki articles are sup-posed to be able to help determine subtopic rela-tions between topics.
However, how can we modelthe textual correlation?In this study, we propose a novel approach tobuild a high-quality CTH for any given Wiki cate-gory.
We use a Bayesian network to model a CTH,and map the CTH learning problem as a structurelearning problem.
We leverage both structural andtextual information of topics in the articles to in-duce the optimal tree structure.
Experiments on 3category data demonstrate the effectiveness of ourapproach for CTH learning.2 PreliminariesOur problem is to learn a CTH for a Wiki catego-ry from multiple topic hierarchies represented byCT in the Wiki articles of the category.
For ex-ample, consider the category ?earthquake?.
Thereare a lot of Wikipedia articles about earthquakeevents which are manually created by human ex-perts.
In these articles, the CTs imply hierarchi-cal topic knowledge in the events.
However, dueto crowdsourcing nature, these knowledge is het-erogeneous across different articles.
We want tointegrate these knowledge represented by CTs indifferent earthquake event articles to form a com-prehensive understanding of the category ?earth-quake?
(CTH).Specifically, our input consists of a set of Wikiarticles Ac= {a}, belonging to a Wiki categoryc.
As shown in Fig.1, each article a ?
Accon-tains a CT (topic tree hierarchy) Ha= {Ta, Ra},where Tais a set of topics, each denoted by a tagg and associated with a text description dg, andRa= {(gi, gj)}, gi, gj?
Tais a set of subtopicrelations (gjis a subtopic of gi).
The output isan integrated comprehensive CTH Hc= {Tc, Rc}where Tc= {t} is a set of topics, each denot-ed by a set of tags t = {g} and associated bya text description dtaggregated by {dg}g?t, andRc= {(ti, tj)}, ti, tj?
Tcis a set of subtopicrelations (tjis a subtopic of ti).We map the problem of learning the output Hcfrom the input {Ha}, a ?
Ac, as a structure learn-ing problem.
We first find clusters of similar tagsTc(each cluster represents a topic) and then derivehierarchical relations Rcamong these clusters.Particularly, given a category c, we first col-lect relevant Wiki articles Ac= {a}.
This canbe done automatically since each Wiki article haslinks to its categories.
We can also manually findthe Wikipage which summarizes the links of Ac(e.g., http://en.wikipedia.org/wiki/Lists_of_earthquakes) and then collec-t Acaccording to the links.Then we can get a global tag set G = {g} con-taining all the tags including titles in the articlesAc.
We cluster the same or similar tags from dif-ferent articles using single-pass incremental clus-tering (Hammouda and Kamel, 2003) to constructthe topic set Tc, with cosine similarity computedbased on the names of tags g and their text de-scriptions dg.
Note that titles of all the articles be-longing to the same cluster corresponds to a roottopic.Next, the issue is how to induce a CTH Hc={Tc, Rc} from a set of topics Tc.3 Topic Hierarchy ConstructionWe first present a basic method to learn Hcandthen describe a principled probabilistic model in-corporating both structural and textual informationfor CTH learning.3.1 Basic MethodAfter replacing the tags in a CT (see Fig.1) withthe topics they belong to, we can then get a top-ic hierarchy Ha= {Ta, Ra} for each articlea.
For each subtopic relation (ti, tj) ?
Ra, wecan calculate a count/weight n(ti, tj), represent-ing the number of articles in Accontaining the347relation.
We then construct a directed completegraph with a weight w(ti, tj) = n(ti, tj) on eachedge (ti, tj).
Finally, we apply the widely usedChu-Liu/Edmonds algorithm (Chu and Liu, 1965;Edmonds, 1967) to find an optimal tree with thelargest sum of weights from our constructed graph,meaning that the overall subtopic relations in thetree is best supported by all the CT/articles.
TheChu-Liu/Edmonds algorithm works as follows.First, it selects, for each node, the maximum-weight incoming edge.
Next, it recursively breakscycles with the following idea: nodes in a cycle arecollapsed into a pseudo-node and the maximum-weight edge entering the pseudo-node is selectedto replace the other incoming edge in the cycle.During backtracking, pseudo-nodes are expandedinto an acyclic directed graph, i.e., our final cate-gory topic hierarchy Hc.However, the basic method has a problem.Consider if n(?earthquake?, ?damages to hos-pitals?
)=10 and n(?earthquake?)
=100, whilen(?damages?, ?damages to hospitals?
)=5 andn(?damages?)=5.
We would prefer ?damages?
tobe the parent topic of ?damages to hospitals?
witha higher confidence level (5/5=1 vs 10/100=0.1).However, the above basic method will choose?earthquake?
which maximizes the weight sum.An intuitive solution is to normalize the weights.In Subsection 3.2, we present our proposed princi-pled probabilistic model which can derive normal-ized structure based weights.
In addition, it canbe easily used to incorporate and combine textualinformation of topics into CTH learning.3.2 Probabilistic Model for CTH LearningWe first describe the principled probabilistic mod-el for a CTH.
Then we present how to encodestructural dependency and textual correlation be-tween topics.
Last, we present our final approachcombining both structural dependency and textualcorrelation for CTH construction.3.2.1 Modeling a Category Topic HierarchyIn a topic hierarchy, each node represents a top-ic.
We consider each node as a variable and thetopic hierarchy as a Bayesian network.
Then thejoint probability distribution of nodes N given aparticular tree H isP (N |H) = P (root)?n?N\rootP (n|parH(n)) ,where P (n|parH(n)) is the conditional probabili-ty of node n given its parent node parH(n) in H .Given the nodes, this is actually the likelihood ofH .
Maximizing the likelihood with respect to thetree structure gives the optimal tree:H?= argmaxHP (N |H)= argmaxHP (root)?n?N\rootP (n|parH(n))= argmaxH?n?NlogP (n|parH(n))(1)Encoding Structural Dependency.
Consider-ing tjis a subtopic of ti, we define the structuralconditional probability:Pstruc(tj|ti) =n(ti, tj) + ?n(ti) + ?
?
|Tc?
1|, (2)where n(ti, tj) is the count of articles containingrelation (ti, tj) and n(ti) is the count of articlescontaining topic ti.
The parameter ?
= 1.0 is theLaplace smoothing factor, and |Tc?
1| is the to-tal number of possible relations taking tias theirparent topic.Encoding Textual Correlation.
Consideringa topic text description dtas a bag of words,we use the normalized word frequencies ?t={?t,w}w?Vs.t.
?w?V?t,w= 1 to represent a top-ic t. To capture the subtopic relationship (ti, tj),we prefer a model where the expectation of thedistribution for the child is exactly same with thedistribution for its parent, i.e., E(?tj) = ?ti.This naturally leads to the hierarchical Dirichletmodel (Wang et al, 2014; Veeramachaneni et al,2005), formally, ?tj|?ti?
Dir(?
?ti) in which ?3is the concentration parameter which determineshow concentrated the probability density is likelyto be.
Thus we have:Ptext(tj|ti) =1Z?w?V??
?ti,w?1tj,v,(3)where Z =?w?V?(??ti,w)?(?w?V?
?ti,w)is a normalization fac-tor and ?(?)
is the standard Gamma distribution.We note that for the root node we have the uniformprior instead of the prior coming from the parent.3.2.2 Combining Structural and TextualInformationSubstituting Eq.2 into Eq.1, we can solvethe optimal tree structure by applying Chu-3Experimental results are insensitive to ?, we set ?=5348Liu/Edmonds algorithm to the directed com-plete graph with structure based weightswstruc=log(Pstruc(tj|ti) = logn(ti,tj)+?n(ti)+?
?|Tc?1|on the edges (ti, tj).
While this solves theproblem of the basic method, it only considersstructural dependency and does not considertextual correlation which is supposed to be useful.Therefore, we calculate text based weightswtext=log(Ptext(tj|ti) =?w?Vlog??
?ti,w?1tj,v?logZ similarly.
Then we combine structural in-formation and textual information by definingthe weights w(ti, tj) of the edges (ti, tj) as asimple weighted average of wstruc(ti, tj) andwtext(ti, tj).
Specifically, we define:w(ti, tj) = ?wtext(ti, tj) + (1?
?
)wstruc(ti, tj) ,where ?
controls the impacts of text correla-tion and structure dependency in optimal structurelearning.
Note that wtextand wstrucshould be s-caled4first before applying Chu-Liu/Edmonds al-gorithm to find an optimal topic hierarchy.4 ExperimentsWe evaluate the CTH automatically generated byour proposed methods via comparing it with amanually constructed ground-truth CTH.4.1 Data and Evaluation MetricData.
We evaluate our methods on 3 categories,i.e., English ?earthquake?
and ?election?
cate-gories containing 293 and 60 articles, and Chi-nese ?earthquake?
category containing 48 articles5.
After removing noisy tags such as ?references?and ?see also?, they contain 463, 79 and 426 u-nique tags respectively.
After tag clustering6, wecan get 176, 57 and 112 topics for each category.Evaluation Metric.
We employ the precisionmeasure to evaluate the performance of our meth-ods.
Let R and Rsbe subtopic relation sets ofour generated result and ground-truth result re-spectively, then precison=|R ?
Rs|/|R|.
Due tothe number of relations |R|=|Rs| = |Tc?
1|, wehave precison=recall=F1=|R ?
Rs|/|R|.We compare three methods, including our basicmethod (Basic) which uses only non-normalizedstructural information, our proposed probabilis-tic method considering only structural information4We use min-max normalization x?=x?minmax?min5We filter articles with little information in Contents.6We use an incremental clustering algorithm(?
= 0) (Pro+S), and considering both structuraland textual information (0 < ?
< 1) (Pro+ST).4.2 Results and AnalysisQuantitative Analysis.
From Table 1, we observethat our approach Pro+ST (with best ?
values asshown in Fig.2) significantly outperforms Basicand Pro+S which only utilize the structural infor-mation (+24.3% and +5.1% on average, p <0.025with t-test).
Pro+S which normalizes structural in-formation also achieves significant higher preci-sion than Basic (+19.2% on average, p <0.025).Earth.
(En) Elect.
(En) Earth.
(Ch)Basic 0.5965 0.7719 0.7143Pro+S 0.8971 0.8596 0.9017Pro+ST 0.9543 0.9298 0.9286Table 1: Precision of different methods on 3 cate-goriesFigure 2: The precision of CTH with different ?valuesTo examine the influence of ?, we show theperformance of our approach Pro+ST with dif-ferent ?
values on 3 categories in Fig.2.
All thecurves grow up first and then decrease dramatical-ly as we emphasize more on textual information.They can always get consistent better results when0.2?
?
?0.3.
When ?
approaches 1, the precisiondeclines fast to near 0.
The reason is that the top-ics with short (or null) text descriptions are likelyto be a parent node of all other nodes and influ-ence the results dramatically, but if we rely mostlyon structural information and use the textual infor-mation as auxiliary for correcting minor errors insome ambiguous cases, we can improve the preci-sion of the resultant topic hierarchy.Qualitative Analysis.
Due to space limitation,we only show the topic hierarchy for ?Election?with smaller topic size in Fig.3.
As we can see,349Figure 3: The category topic hierarchy for presi-dential elections.
Topics are labeled by tags sepa-rated by ?#?.the root topic ?presidential elections?
includessubtopics ?results?, ?vote?, ?official candidates?,etc.
Furthermore, ?official candidates?
contain-s subtopics ?debates, ?rejected candidates?, ?un-successful candidates?, etc.
The above mentionedexamples are shown with red edges.
However,there are also a few (7%) mistaken relations (blackedges) such as ?comparison?
(should be ?officialcandidates?
instead) ?
?official candidate web-sites?.
Overall, the above hierarchy aligns wellwith human knowledge.5 Related WorkTo our best knowledge, our overall problem set-ting is novel and there is no previous work usingWiki articles?
contents tables to learn topic hier-archies for categories.
Existing work mainly fo-cused on learning topic hierarchies from texts onlyand used traditional hierarchical clustering meth-ods (Chuang and Chien, 2004) or topic modelssuch as HLDA (Griffiths and Tenenbaum, 2004),HPAM (Mimno et al, 2007), hHDP (Zavitsanoset al, 2011), and HETM (Hu et al, 2015).
Differ-ently, we focus on structured contents tables withcorresponding text descriptions.Our work is also different from ontology (tax-onomy) construction (Li et al, 2007; Tang et al,2009; Zhu et al, 2013; Navigli et al, 2011; Wuet al, 2012) as their focus is concept hierarchies(e.g.
isA relation) rather than thematic topic hier-archies.
For example, given the ?animals?
cate-gory, they may derive ?cats?
and ?dogs?, etc.
assubcategories, while our work aims to derive the-matic topics ?animal protection?
and ?animal ex-tinction?, etc.
as subtopics.
Our work enables afresher to quickly familiarize himself/herself withany new category, and is very useful for informa-tion browsing, organization and topic extraction.6 ConclusionIn this paper, we propose an innovative problem,i.e., to construct high quality comprehensive top-ic hierarchies for different Wiki categories usingtheir associated Wiki articles.
Our novel approachis able to model a topic hierarchy and to incorpo-rate both structural dependencies and text correla-tions into the optimal tree learning.
Experimentalresults demonstrate the effectiveness of our pro-posed approach.
In future work, we will inves-tigate how to update the category topic hierarchyincrementally with the creation of new related ar-ticles.AcknowledgmentsThe work is supported by 973 Program(No.
2014CB340504), NSFC-ANR (No.61261130588), Tsinghua University InitiativeScientific Research Program (No.
20131089256),Science and Technology Support Program (No.2014BAK04B00), China Postdoctoral ScienceFoundation (No.
2014M550733), National Natu-ral Science Foundation of China (No.
61309007)and THU-NUS NExT Co-Lab.ReferencesYoeng-Jin Chu and Tseng-Hong Liu.
1965.
On short-est arborescence of a directed graph.
Scientia Sini-ca, 14(10):1396.Shui-Lung Chuang and Lee-Feng Chien.
2004.
Apractical web-based approach to generating topic hi-erarchy for text segments.
In Proceedings of thethirteenth ACM international conference on Infor-mation and knowledge management, pages 127?136.
ACM.Jack Edmonds.
1967.
Optimum branchings.
Journalof Research of the National Bureau of Standards B,71(4):233?240.DMBTL Griffiths and MIJJB Tenenbaum.
2004.
Hier-archical topic models and the nested chinese restau-rant process.
Advances in NIPS, 16:17.Khaled M Hammouda and Mohamed S Kamel.
2003.Incremental document clustering using cluster sim-ilarity histograms.
In Web Intelligence, 2003.
WI2003.
Proceedings.
IEEE/WIC International Con-ference on, pages 597?601.
IEEE.350Linmei Hu, Juanzi Li, Jing Zhang, and Chao Shao.2015.
o-hetm: An online hierarchical entity topicmodel for news streams.
In Advances in KnowledgeDiscovery and Data Mining - 19th Pacific-Asia Con-ference, PAKDD 2015, Proceedings, Part I, pages696?707.Rui Li, Shenghua Bao, Yong Yu, Ben Fei, and ZhongSu.
2007.
Towards effective browsing of large s-cale social annotations.
In Proceedings of the 16thinternational conference on World Wide Web, pages943?952.
ACM.Wen-Pin Lin, Matthew Snover, and Heng Ji.
2011.
Un-supervised language-independent name translationmining from wikipedia infoboxes.
In Proceedingsof the First workshop on Unsupervised Learning inNLP, pages 43?52.
Association for ComputationalLinguistics.David Mimno, Wei Li, and Andrew McCallum.
2007.Mixtures of hierarchical topics with pachinko allo-cation.
In Proceedings of the 24th ICML, pages633?640.
ACM.Roberto Navigli, Paola Velardi, and Stefano Faralli.2011.
A graph-based algorithm for inducing lexi-cal taxonomies from scratch.
In IJCAI, pages 1872?1877.Jie Tang, Ho-fung Leung, Qiong Luo, Dewei Chen,and Jibin Gong.
2009.
Towards ontology learn-ing from folksonomies.
In IJCAI, volume 9, pages2089?2094.Sriharsha Veeramachaneni, Diego Sona, and PaoloAvesani.
2005.
Hierarchical dirichlet model fordocument classification.
In Proceedings of the 22ndICML, pages 928?935.
ACM.Zhigang Wang, Zhixing Li, Juanzi Li, Jie Tang, andJeff Z Pan.
2013.
Transfer learning based cross-lingual knowledge extraction for wikipedia.
In ACL(1), pages 641?650.JingjingWang, Changsung Kang, Yi Chang, and JiaweiHan.
2014.
A hierarchical dirichlet model for tax-onomy expansion for search engines.
In Proceed-ings of the 23rd international conference on WWW,pages 961?970.
International World WideWeb Con-ferences Steering Committee.WentaoWu, Hongsong Li, HaixunWang, and Kenny QZhu.
2012.
Probase: A probabilistic taxonomy fortext understanding.
In Proceedings of the 2012 ACMSIGMOD International Conference on Managementof Data, pages 481?492.
ACM.Elias Zavitsanos, Georgios Paliouras, and George AVouros.
2011.
Non-parametric estimation of top-ic hierarchies from texts with hierarchical dirichletprocesses.
The Journal of Machine Learning Re-search, 12:2749?2775.Torsten Zesch and Iryna Gurevych.
2007.
Analysisof the wikipedia category graph for nlp application-s.
In Proceedings of the TextGraphs-2 Workshop(NAACL-HLT 2007), pages 1?8.Xingwei Zhu, Zhao-Yan Ming, Xiaoyan Zhu, and Tat-Seng Chua.
2013.
Topic hierarchy construction forthe organization of multi-source user generated con-tents.
In Proceedings of the 36th international ACMSIGIR conference on Research and development ininformation retrieval, pages 233?242.
ACM.351
