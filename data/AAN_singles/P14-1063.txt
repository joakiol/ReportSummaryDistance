Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 666?675,Baltimore, Maryland, USA, June 23-25 2014.c?2014 Association for Computational LinguisticsOnline Learning in Tensor SpaceYuan Cao Sanjeev KhudanpurCenter for Language & Speech Processing and Human Language Technology Center of ExcellenceThe Johns Hopkins UniversityBaltimore, MD, USA, 21218{yuan.cao, khudanpur}@jhu.eduAbstractWe propose an online learning algorithmbased on tensor-space models.
A tensor-space model represents data in a compactway, and via rank-1 approximation theweight tensor can be made highly struc-tured, resulting in a significantly smallernumber of free parameters to be estimatedthan in comparable vector-space models.This regularizes the model complexity andmakes the tensor model highly effective insituations where a large feature set is de-fined but very limited resources are avail-able for training.
We apply with the pro-posed algorithm to a parsing task, andshow that even with very little trainingdata the learning algorithm based on a ten-sor model performs well, and gives signif-icantly better results than standard learn-ing algorithms based on traditional vector-space models.1 IntroductionMany NLP applications use models that try to in-corporate a large number of linguistic features sothat as much human knowledge of language canbe brought to bear on the (prediction) task as pos-sible.
This also makes training the model param-eters a challenging problem, since the amount oflabeled training data is usually small compared tothe size of feature sets: the feature weights cannotbe estimated reliably.Most traditional models are linear models, inthe sense that both the features of the data andmodel parameters are represented as vectors in avector space.
Many learning algorithms appliedto NLP problems, such as the Perceptron (Collins,2002), MIRA (Crammer et al, 2006; McDonaldet al, 2005; Chiang et al, 2008), PRO (Hop-kins and May, 2011), RAMPION (Gimpel andSmith, 2012) etc., are based on vector-space mod-els.
Such models require learning individual fea-ture weights directly, so that the number of param-eters to be estimated is identical to the size of thefeature set.
When millions of features are used butthe amount of labeled data is limited, it can be dif-ficult to precisely estimate each feature weight.In this paper, we shift the model from vector-space to tensor-space.
Data can be representedin a compact and structured way using tensors ascontainers.
Tensor representations have been ap-plied to computer vision problems (Hazan et al,2005; Shashua and Hazan, 2005) and informationretrieval (Cai et al, 2006a) a long time ago.
Morerecently, it has also been applied to parsing (Cohenand Collins, 2012; Cohen and Satta, 2013) and se-mantic analysis (Van de Cruys et al, 2013).
Alinear tensor model represents both features andweights in tensor-space, hence the weight tensorcan be factorized and approximated by a linearsum of rank-1 tensors.
This low-rank approxi-mation imposes structural constraints on the fea-ture weights and can be regarded as a form ofregularization.
With this representation, we nolonger need to estimate individual feature weightsdirectly but only a small number of ?bases?
in-stead.
This property makes the the tensor modelvery effective when training a large number of fea-ture weights in a low-resource environment.
Onthe other hand, tensor models have many more de-grees of ?design freedom?
than vector space mod-els.
While this makes them very flexible, it alsocreates much difficulty in designing an optimaltensor structure for a given training set.We give detailed description of the tensor space666model in Section 2.
Several issues that comewith the tensor model construction are addressedin Section 3.
A tensor weight learning algorithmis then proposed in 4.
Finally we give our exper-imental results on a parsing task and analysis inSection 5.2 Tensor Space RepresentationMost of the learning algorithms for NLP problemsare based on vector space models, which representdata as vectors ?
?
Rn, and try to learn featureweight vectors w ?
Rnsuch that a linear modely = w ?
?
is able to discriminate between, say,good and bad hypotheses.
While this is a naturalway of representing data, it is not the only choice.Below, we reformulate the model from vector totensor space.2.1 Tensor Space ModelA tensor is a multidimensional array, and is a gen-eralization of commonly used algebraic objectssuch as vectors and matrices.
Specifically, a vec-tor is a 1storder tensor, a matrix is a 2ndordertensor, and data organized as a rectangular cuboidis a 3rdorder tensor etc.
In general, a Dthordertensor is represented as T ?
Rn1?n2?...nD, and anentry in T is denoted by Ti1,i2,...,iD.
Different di-mensions of a tensor 1, 2, .
.
.
, D are named modesof the tensor.Using a Dthorder tensor as container, we canassign each feature of the task a D-dimensionalindex in the tensor and represent the data as ten-sors.
Of course, shifting from a vector to a tensorrepresentation entails several additional degrees offreedom, e.g., the order D of the tensor and thesizes {nd}Dd=1of the modes, which must be ad-dressed when selecting a tensor model.
This willbe done in Section 3.2.2 Tensor DecompositionJust as a matrix can be decomposed as a lin-ear combination of several rank-1 matrices viaSVD, tensors also admit decompositions1into lin-ear combinations of ?rank-1?
tensors.
A Dthor-der tensor A ?
Rn1?n2?...nDis rank-1 if it can be1The form of tensor decomposition defined here is namedas CANDECOMP/PARAFAC(CP) decomposition (Koldaand Bader, 2009).
Another popular form of tensor decom-position is called Tucker decomposition, which decomposesa tensor into a core tensor multiplied by a matrix along eachmode.
We focus only on the CP decomposition in this paper.written as the outer product of D vectors, i.e.A = a1?
a2?, .
.
.
,?aD,where ai?
Rnd, 1 ?
d ?
D. A Dthorder tensorT ?
Rn1?n2?...nDcan be factorized into a sum ofcomponent rank-1 tensors asT =R?r=1Ar=R?r=1a1r?
a2r?, .
.
.
,?aDrwhere R, called the rank of the tensor, is the mini-mum number of rank-1 tensors whose sum equalsT .
Via decomposition, one may approximate atensor by the sum of H major rank-1 tensors withH ?
R.2.3 Linear Tensor ModelIn tensor space, a linear model may be written (ig-noring a bias term) asf(W ) = W ?
?,where ?
?
Rn1?n2?...nDis the feature tensor, Wis the corresponding weight tensor, and ?
denotesthe Hadamard product.
If W is further decom-posed as the sum of H major component rank-1tensors, i.e.
W ?
?Hh=1w1h?w2h?, .
.
.
,?wDh,thenf(w11, .
.
.
,wD1, .
.
.
,w1h, .
.
.
,wDh)=H?h=1??1w1h?2w2h.
.
.
?DwDh, (1)where ?lis the l-mode product operator betweena Dthorder tensor T and a vector a of dimensionnd, yielding a (D ?
1)thorder tensor such that(T ?la)i1,...,il?1,il+1,...,iD=nd?il=1Ti1,...,il?1,il,il+1,...,iD?
ail.The linear tensor model is illustrated in Figure 1.2.4 Why Learning in Tensor Space?So what is the advantage of learning with a ten-sor model instead of a vector model?
Consider thecase where we have defined 1,000,000 features forour task.
A vector space linear model requires es-timating 1,000,000 free parameters.
However ifwe use a 2ndorder tensor model, organize the fea-tures into a 1000 ?
1000 matrix ?, and use just667Figure 1: A 3rdorder linear tensor model.
Thefeature weight tensor W can be decomposed asthe sum of a sequence of rank-1 component ten-sors.one rank-1 matrix to approximate the weight ten-sor, then the linear model becomesf(w1,w2) = wT1?w2,where w1,w2?
R1000.
That is to say, now weonly need to estimate 2000 parameters!In general, if V features are defined for a learn-ing problem, and we (i) organize the feature setas a tensor ?
?
Rn1?n2?...nDand (ii) use Hcomponent rank-1 tensors to approximate the cor-responding target weight tensor.
Then the totalnumber of parameters to be learned for this ten-sor model is H?Dd=1nd, which is usually muchsmaller than V =?Dd=1ndfor a traditional vec-tor space model.
Therefore we expect the tensormodel to be more effective in a low-resource train-ing environment.Specifically, a vector space model assumes eachfeature weight to be a ?free?
parameter, and es-timating them reliably could therefore be hardwhen training data are not sufficient or the fea-ture set is huge.
By contrast, a linear tensor modelonly needs to learn H?Dd=1nd?bases?
of the mfeature weights instead of individual weights di-rectly.
The weight corresponding to the feature?i1,i2,...,iDin the tensor model is expressed aswi1,i2,...,iD=H?h=1w1h,i1w2h,i2.
.
.
wDh,iD, (2)where wjh,ijis the ithjelement in the vector wjh.In other words, a true feature weight is now ap-proximated by a set of bases.
This reminds usof the well-known low-rank matrix approximationof images via SVD, and we are applying similartechniques to approximate target feature weights,which is made possible only after we shift fromvector to tensor space models.This approximation can be treated as a form ofmodel regularization, since the weight tensor isrepresented in a constrained form and made highlystructured via the rank-1 tensor approximation.
Ofcourse, as we reduce the model complexity, e.g.
bychoosing a smaller and smaller H , the model?s ex-pressive ability is weakened at the same time.
Wewill elaborate on this point in Section 3.1.3 Tensor Model ConstructionTo apply a tensor model, we first need to con-vert the feature vector into a tensor ?.
Once thestructure of ?
is determined, the structure of Wis fixed as well.
As mentioned in Section 2.1, atensor model has many more degrees of ?designfreedom?
than a vector model, which makes theproblem of finding a good tensor structure a non-trivial one.3.1 Tensor OrderThe order of a tensor affects the model in twoways: the expressiveness of the model and thenumber of parameters to be estimated.
We assumeH = 1 in the analysis below, noting that one canalways add as many rank-1 component tensors asneeded to approximate a tensor with arbitrary pre-cision.Obviously, the 1storder tensor (vector) modelis the most expressive, since it is structureless andany arbitrary set of numbers can always be repre-sented exactly as a vector.
The 2ndorder rank-1tensor (rank-1 matrix) is less expressive becausenot every set of numbers can be organized intoa rank-1 matrix.
In general, a Dthorder rank-1tensor is more expressive than a (D + 1)thorderrank-1 tensor, as a lower-order tensor imposes lessstructural constraints on the set of numbers it canexpress.
We formally state this fact as follows:Theorem 1.
A set of real numbers that can be rep-resented by a (D + 1)thorder tensor Q can alsobe represented by a Dthorder tensor P , providedP andQ have the same volume.
But the reverse isnot true.Proof.
See appendix.On the other hand, tensor order also affects thenumber of parameters to be trained.
Assumingthat a Dthorder has equal size on each mode (wewill elaborate on this point in Section 3.2) andthe volume (number of entries) of the tensor isfixed as V , then the total number of parameters668of the model is DV1D.
This is a convex func-tion of D, and the minimum2is reached at eitherD?= blnV c or D?= dlnV e.Therefore, as D increases from 1 to D?, welose more and more of the expressive power of themodel but reduce the number of parameters to betrained.
However it would be a bad idea to chooseaD beyondD?.
The optimal tensor order dependson the nature of the actual problem, and we tunethis hyper-parameter on a held-out set.3.2 Mode SizeThe size ndof each tensor mode, d = 1, .
.
.
, D,determines the structure of feature weights a ten-sor model can precisely represent, as well as thenumber of parameters to estimate (we also as-sume H = 1 in the analysis below).
For exam-ple, if the tensor order is 2 and the volume V is12, then we can either choose n1= 3, n2= 4or n1= 2, n2= 6.
For n1= 3, n2= 4, thenumbers that can be precisely represented are di-vided into 3 groups, each having 4 numbers, thatare scaled versions of one another.
Similarly forn1= 2, n2= 6, the numbers can be divided into2 groups with different scales.
Obviously, the twopossible choices of (n1, n2) also lead to differentnumbers of free parameters (7 vs. 8).GivenD and V , there are many possible combi-nations of nd, d = 1, .
.
.
, D, and the optimal com-bination should indeed be determined by the struc-ture of target features weights.
However it is hardto know the structure of target feature weights be-fore learning, and it would be impractical to try ev-ery possible combination of mode sizes, thereforewe choose the criterion of determining the modesizes as minimization of the total number of pa-rameters, namely we solve the problem:minn1,...,nDD?d=1nds.tD?d=1nd= VThe optimal solution is reached when n1= n2=.
.
.
= nD= V1D.
Of course it is not guaran-teed that V1Dis an integer, therefore we choosend= bV1Dc or dV1De, d = 1, .
.
.
, D such that?Dd=1nd?
V and[?Dd=1nd]?
V is minimized.The[?Dd=1nd]?
V extra entries of the tensorcorrespond to no features and are used just for2The optimal integer solution can be determined simplyby comparing the two function values.padding.
Since for each ndthere are only twopossible values to choose, we can simply enumer-ate all the possible 2D(which is usually a smallnumber) combinations of values and pick the onethat matches the conditions given above.
This wayn1, .
.
.
, nDare fully determined.Here we are only following the principle of min-imizing the parameter number.
While this strat-egy might work well with small amount of train-ing data, it is not guaranteed to be the best strategyin all cases, especially when more data is avail-able we might want to increase the number of pa-rameters, making the model more complex so thatthe data can be more precisely modeled.
Ideallythe mode size needs to be adaptive to the amountof training data as well as the property of targetweights.
A theoretically guaranteed optimal ap-proach to determining the mode sizes remains anopen problem, and will be explored in our futurework.3.3 Number of Rank-1 TensorsThe impact of using H > 1 rank-1 tensors is ob-vious: a larger H increases the model complexityand makes the model more expressive, since weare able to approximate target weight tensor withsmaller error.
As a trade-off, the number of param-eters and training complexity will be increased.
Tofind out the optimal value of H for a given prob-lem, we tune this hyper-parameter too on a held-out set.3.4 Vector to Tensor MappingFinally, we need to find a way to map the orig-inal feature vector to a tensor, i.e.
to associateeach feature with an index in the tensor.
Assum-ing the tensor volume V is the same as the numberof features, then there are in all V !
ways of map-ping, which is an intractable number of possibili-ties even for modest sized feature sets, making itimpractical to carry out a brute force search.
How-ever while we are doing the mapping, we hope toarrange the features in a way such that the corre-sponding target weight tensor has approximately alow-rank structure, this way it can be well approx-imated by very few component rank-1 tensors.Unfortunately we have no knowledge about thetarget weights in advance, since that is what weneed to learn after all.
As a way out, we first runa simple vector-model based learning algorithm(say the Perceptron) on the training data and es-timate a weight vector, which serves as a ?surro-669gate?
weight vector.
We then use this surrogatevector to guide the design of the mapping.
Ide-ally we hope to find a permutation of the surro-gate weights to map to a tensor in such a way thatthe tensor has a rank as low as possible.
How-ever matrix rank minimization is in general a hardproblem (Fazel, 2002).
Therefore, we follow anapproximate algorithm given in Figure 2a, whosemain idea is illustrated via an example in Figure2b.Basically, what the algorithm does is to di-vide the surrogate weights into hierarchical groupssuch that groups on the same level are approx-imately proportional to each other.
Using thesegroups as units we are able to ?fill?
the tensor in ahierarchical way.
The resulting tensor will have anapproximate low-rank structure, provided that thesorted feature weights have roughly group-wiseproportional relations.For comparison, we also experimented a trivialsolution which maps each entry of the feature ten-sor to the tensor just in sequential order, namely?0is mapped to ?0,0,...,0, ?1is mapped to ?0,0,...,1etc.
This of course ignores correlation betweenfeatures since the original feature order in the vec-tor could be totally meaningless, and this strategyis not expected to be a good solution for vector totensor mapping.4 Online Learning AlgorithmWe now turn to the problem of learning the featureweight tensor.
Here we propose an online learningalgorithm similar to MIRA but modified to accom-modate tensor models.Let the model be f(T ) = T ?
?
(x, y), whereT =?Hh=1w1h?
w2h?, .
.
.
,?wDhis the weighttensor, ?
(x, y) is the feature tensor for an input-output pair (x, y).
Training samples (xi, yi), i =1, .
.
.
,m, where xiis the input and yiis the ref-erence or oracle hypothesis, are fed to the weightlearning algorithm in sequential order.
A predic-tion ztis made by the model Ttat time t from aset of candidatesZ(xt), and the model updates theweight tensor by solving the following problem:minT?Rn1?n2?...nD12?T ?
Tt?2+ C?
(3)s.t.Lt?
?, ?
?
0where T is a decomposed weight tensor andLt= T ??
(xt, zt)?
T ??
(xt, yt) + ?
(yt, zt)Input:Tensor order D, tensor volume V , mode sizend, d = 1, .
.
.
, D, surrogate weight vector vLetv+= [v+1, .
.
.
, v+p] be the non-negative part ofvv?= [v?1, .
.
.
, v?q] be the negative part of vAlgorithm:?v+= sort(v+) in descending order?v?= sort(v?)
in ascending orderu = V/nDe = p?mod(p, u), f = q ?mod(q, u)Construct vectorX = [v?+1, .
.
.
, v?+e, v?
?1, .
.
.
, v?
?f,v?+e+1, .
.
.
, v?+p, v?
?f+1, .
.
.
, v?
?q]Map Xa, a = 1, .
.
.
, p + q to the tensor entryTi1,...,iD, such thata =D?d=1(id?
1)ld?1+ 1where ld= ld?1nd, and l0= 1(a) Mapping a surrogate weight vector to a tensor(b) Illustration of the algorithmFigure 2: Algorithm for mapping a surrogateweight vector X to a tensor.
(2a) provides the al-gorithm; (2b) illustrates it by mapping a vector oflength V = 12 to a (n1, n2, n3) = (2, 2, 3) ten-sor.
The bars Xirepresent the surrogate weights?
after separately sorting the positive and nega-tive parts ?
and the labels along a path of the treecorrespond to the tensor-index of the weight rep-resented by the leaf resulting from the mapping.670is the structured hinge loss.This problem setting follows the same ?passive-aggressive?
strategy as in the original MIRA.
Tooptimize the vectors wdh, h = 1, .
.
.
,H, d =1, .
.
.
, D, we use a similar iterative strategy as pro-posed in (Cai et al, 2006b).
Basically, the idea isthat instead of optimizing wdhall together, we op-timize w11,w21, .
.
.
,wDHin turn.
While we are up-dating one vector, the rest are fixed.
For the prob-lem setting given above, each of the sub-problemsthat need to be solved is convex, and accordingto (Cai et al, 2006b) the objective function valuewill decrease after each individual weight updateand eventually this procedure will converge.We now give this procedure in more detail.Denote the weight vector of the dthmode ofthe hthtensor at time t as wdh,t.
We will up-date the vectors in turn in the following order:w11,t, .
.
.
,wD1,t,w12,t, .
.
.
,wD2,t, .
.
.
,w1H,t, .
.
.
,wDH,t.Once a vector has been updated, it is fixed forfuture updates.By way of notation, defineWdh,t= w1h,t+1?, .
.
.
,?wd?1h,t+1?wdh,t?, .
.
.
,?wDh,t(and letWD+1h,t, w1h,t+1?, .
.
.
,?wDh,t+1),?Wdh,t= w1h,t+1?, .
.
.
,?wd?1h,t+1?wd?, .
.
.
,?wDh,t(where wd?
Rnd),Tdh,t=h?1?h?=1WD+1h?,t+Wdh,t+H?h?=h+1W1h?,t(4)?Tdh,t=h?1?h?=1WD+1h?,t+?Wdh,t+H?h?=h+1W1h?,t?dh,t(x, y)= ?
(x, y)?2w2h,t+1.
.
.?d?1wd?1h,t+1?d+1wd+1h,t.
.
.
?DwDh,t(5)In order to update from wdh,tto get wdh,t+1, thesub-problem to solve is:minwd?Rnd12??Tdh,t?
Tdh,t?2+ C?= minwd?Rnd12?
?Wdh,t?Wdh,t?2+ C?= minwd?Rnd12?1h,t+1.
.
.
?d?1h,t+1?d+1h,t.
.
.
?Dh,t?wd?wdh,t?2+ C?s.t.
Ldh,t?
?, ?
?
0.where?dh,t= ?wdh,t?2Ldh,t=?Tdh,t??
(xt, zt)??Tdh,t??
(xt, yt)+?
(yt, zt)= wd?
(?dh,t(xt, zt)?
?dh,t(xt, yt))?(h?1?h?=1WD+1h?,t+H?h?=h+1W1h?,t)?(?
(xt, yt)??
(xt, zt))+?
(yt, zt)Letting?
?dh,t, ?dh,t(xt, yt)?
?dh,t(xt, zt)andsdh,t,(h?1?h?=1WD+1h?,t+H?h?=h+1W1h?,t)?(?
(xt, yt)??
(xt, zt))we may compactly writeLdh,t= ?
(yt, zt)?
sdh,t?wd??
?dh,t.This convex optimization problem is just like theoriginal MIRA and may be solved in a similar way.The updating strategy for wdh,tis derived aswdh,t+1= wdh,t+ ???dh,t?
= (6)min{C,?
(yt, zt)?
Tdh,t?
(?
(xt, yt)??
(xt, zt))??
?dh,t?2}The initial vectors wih,1cannot be made all zero,since otherwise the l-mode product in Equation(5) would yield all zero ?dh,t(x, y) and the modelwould never get a chance to be updated.
There-fore, we initialize the entries of wih,1uniformlysuch that the Frobenius-norm of the weight tensorW is unity.We call the algorithm above ?Tensor-MIRA?and abbreviate it as T-MIRA.6715 ExperimentsIn this section we shows empirical results of thetraining algorithm on a parsing task.
We used theCharniak parser (Charniak et al, 2005) for our ex-periment, and we used the proposed algorithm totrain the reranking feature weights.
For compari-son, we also investigated training the reranker withPerceptron and MIRA.5.1 Experimental SettingsTo simulate a low-resource training environment,our training sets were selected from sections 2-9of the Penn WSJ treebank, section 24 was used asthe held-out set and section 23 as the evaluationset.
We applied the default settings of the parser.There are around V = 1.33 million features inall defined for reranking, and the n-best size forreranking is set to 50.
We selected the parse withthe highest f -score from the 50-best list as the or-acle.We would like to observe from the experimentshow the amount of training data as well as dif-ferent settings of the tensor degrees of freedomaffects the algorithm performance.
Therefore wetried all combinations of the following experimen-tal parameters:Parameters SettingsTraining data (m) Sec.
2, 2-3, 2-5, 2-9Tensor order (D) 2, 3, 4# rank-1 tensors (H) 1, 2, 3Vec.
to tensor mapping approximate, sequentialHere ?approximate?
and ?sequential?
means us-ing, respectively, the algorithm given in Figure 2and the sequential mapping mentioned in Section3.4.
According to the strategy given in 3.2, oncethe tensor order and number of features are fixed,the sizes of modes and total number of parametersto estimate are fixed as well, as shown in the tablesbelow:D Size of modes Number of parameters2 1155?
1155 23103 110?
110?
111 3314 34?
34?
34?
34 1365.2 Results and AnalysisThe f -scores of the held-out and evaluation setgiven by T-MIRA as well as the Perceptron andMIRA baseline are given in Table 1.
From the re-sults, we have the following observations:1.
When very few labeled data are available fortraining (compared with the number of fea-tures), T-MIRA performs much better thanthe vector-based models MIRA and Percep-tron.
However as the amount of training dataincreases, the advantage of T-MIRA fadesaway, and vector-based models catch up.This is because the weight tensors learnedby T-MIRA are highly structured, which sig-nificantly reduces model/training complex-ity and makes the learning process very ef-fective in a low-resource environment, butas the amount of data increases, the morecomplex and expressive vector-based modelsadapt to the data better, whereas further im-provements from the tensor model is impededby its structural constraints, making it insen-sitive to the increase of training data.2.
To further contrast the behavior of T-MIRA,MIRA and Perceptron, we plot the f -scoreson both the training and held-out sets givenby these algorithms after each training epochin Figure 3.
The plots are for the exper-imental setting with mapping=surrogate, #rank-1 tensors=2, tensor order=2, trainingdata=sections 2-3.
It is clearly seen that bothMIRA and Perceptron do much better than T-MIRA on the training set.
Nevertheless, witha huge number of parameters to fit a limitedamount of data, they tend to over-fit and givemuch worse results on the held-out set thanT-MIRA does.As an aside, observe that MIRA consistentlyoutperformed Perceptron, as expected.3.
Properties of linear tensor model: The heuris-tic vector-to-tensor mapping strategy givenby Figure 2 gives consistently better resultsthan the sequential mapping strategy, as ex-pected.To make further comparison of the two strate-gies, in Figure 4 we plot the 20 largest sin-gular values of the matrices which the surro-gate weights (given by the Perceptron afterrunning for 1 epoch) are mapped to by bothstrategies (from the experiment with trainingdata sections 2-5).
From the contrast betweenthe largest and the 2nd-largest singular val-ues, it can be seen that the matrix generated672by the first strategy approximates a low-rankstructure much better than the second strat-egy.
Therefore, the performance of T-MIRAis influenced significantly by the way featuresare mapped to the tensor.
If the correspond-ing target weight tensor has internal struc-ture that makes it approximately low-rank,the learning procedure becomes more effec-tive.The best results are consistently given by 2ndorder tensor models, and the differences be-tween the 3rdand 4thorder tensors are notsignificant.
As discussed in Section 3.1, al-though 3rdand 4thorder tensors have less pa-rameters, the benefit of reduced training com-plexity does not compensate for the loss ofexpressiveness.
A 2ndorder tensor has al-ready reduced the number of parameters fromthe original 1.33 million to only 2310, and itdoes not help to further reduce the number ofparameters using higher order tensors.4.
As the amount of training data increases,there is a trend that the best results come frommodels with more rank-1 component tensors.Adding more rank-1 tensors increases themodel?s complexity and ability of expression,making the model more adaptive to largerdata sets.6 Conclusion and Future WorkIn this paper, we reformulated the traditional lin-ear vector-space models as tensor-space models,and proposed an online learning algorithm namedTensor-MIRA.
A tensor-space model is a com-pact representation of data, and via rank-1 ten-sor approximation, the weight tensor can be madehighly structured hence the number of parame-ters to be trained is significantly reduced.
Thiscan be regarded as a form of model regular-ization.Therefore, compared with the traditionalvector-space models, learning in the tensor spaceis very effective when a large feature set is defined,but only small amount of training data is available.Our experimental results corroborated this argu-ment.As mentioned in Section 3.2, one interestingproblem that merits further investigation is howto determine optimal mode sizes.
The challengeof applying a tensor model comes from finding aproper tensor structure for a given problem, and95.5 9696.5 9797.5 9898.5 99  12 34 56 78 910f-scoreIterationsTraining set f-score T-MIRA MIRA Perceptron(a) Training set8787.5 8888.5 8989.5 90  12 34 56 78 910f-scoreIterationsTraining set f-score T-MIRA MIRA Perceptron(b) Held-out setFigure 3: f -scores given by three algorithms ontraining and held-out set (see text for the setting).the key to solving this problem is to find a bal-ance between the model complexity (indicated bythe order and sizes of modes) and the number ofparameters.
Developing a theoretically guaran-teed approach of finding the optimal structure fora given task will make the tensor model not onlyperform well in low-resource environments, butadaptive to larger data sets.7 AcknowledgementsThis work was partially supported by IBM viaDARPA/BOLT contract number HR0011-12-C-0015 and by the National Science Foundation viaaward number IIS-0963898.ReferencesDeng Cai , Xiaofei He , and Jiawei Han.
2006.
TensorSpace Model for Document Analysis Proceedingsof the 29th Annual International ACM SIGIR Con-ference on Research and Development in Informa-tion Retrieval(SIGIR), 625?626.Deng Cai, Xiaofei He, and Jiawei Han.
2006.
Learn-ing with Tensor Representation Technical Report,Department of Computer Science, University of Illi-nois at Urbana-Champaign.673Mapping Approximate SequentialRank-1 tensors 1 2 3 1 2 3Tensor order 2 3 4 2 3 4 2 3 4 2 3 4 2 3 4 2 3 4Held-out score 89.43 89.16 89.22 89.16 89.21 89.24 89.27 89.14 89.24 89.21 88.90 88.89 89.13 88.88 88.88 89.15 88.87 88.99Evaluation score 89.83 89.69MIRA 88.57Percep 88.23(a) Training data: Section 2 onlyMapping Approximate SequentialRank-1 tensors 1 2 3 1 2 3Tensor order 2 3 4 2 3 4 2 3 4 2 3 4 2 3 4 2 3 4Held-out score 89.26 89.06 89.12 89.33 89.11 89.19 89.18 89.14 89.15 89.2 89.01 88.82 89.24 88.94 88.95 89.19 88.91 88.98Evaluation score 90.02 89.82MIRA 89.00Percep 88.59(b) Training data: Section 2-3Mapping Approximate SequentialRank-1 tensors 1 2 3 1 2 3Tensor order 2 3 4 2 3 4 2 3 4 2 3 4 2 3 4 2 3 4Held-out score 89.40 89.44 89.17 89.5 89.37 89.18 89.47 89.32 89.18 89.23 89.03 88.93 89.24 88.98 88.94 89.16 89.01 88.85Evaluation score 89.96 89.78MIRA 89.49Percep 89.10(c) Training data: Section 2-5Mapping Approximate SequentialRank-1 tensors 2 3 4 2 3 4Tensor order 2 3 4 2 3 4 2 3 4 2 3 4 2 3 4 2 3 4Held-out score 89.43 89.23 89.06 89.37 89.23 89.1 89.44 89.22 89.06 89.21 88.92 88.94 89.23 88.94 88.93 89.23 88.95 88.93Evaluation score 89.95 89.84MIRA 89.95Percep 89.77(d) Training data: Section 2-9Table 1: Parsing f -scores.
Tables (a) to (d) correspond to training data with increasing size.
The upper-part ofeach table shows the T-MIRA results with different settings, the lower-part shows the MIRA and Perceptronbaselines.
The evaluation scores come from the settings indicated by the best held-out scores.
The best resultson the held-out and evaluation data are marked in bold.01002003004005002 46 810 12 1416 18 20Singular valueApproximate SequentialFigure 4: The top 20 singular values of the surro-gate weight matrices given by two mapping algo-rithms.Eugene Charniak, and Mark Johnson 2005.
Coarse-to-fine n-Best Parsing and MaxEnt DiscriminativeReranking Proceedings of the 43th Annual Meetingon Association for Computational Linguistics(ACL)173?180.David Chiang, Yuval Marton, and Philip Resnik.2008.
Online Large-Margin Training of Syntacticand Structural Translation Features Proceedings ofEmpirical Methods in Natural Language Process-ing(EMNLP), 224?233.Shay Cohen and Michael Collins.
2012.
Tensor De-composition for Fast Parsing with Latent-VariablePCFGs Proceedings of Advances in Neural Infor-mation Processing Systems(NIPS).Shay Cohen and Giorgio Satta.
2013.
ApproximatePCFG Parsing Using Tensor Decomposition Pro-ceedings of NAACL-HLT, 487?496.Michael Collins.
2002.
Discriminative training meth-ods for hidden Markov Models: Theory and Exper-iments with Perceptron.
Algorithms Proceedings ofEmpirical Methods in Natural Language Process-ing(EMNLP), 10:1?8.Koby Crammer, Ofer Dekel, Joseph Keshet, ShaiShalev-Schwartz, and Yoram Singer.
2006.
OnlinePassive-Aggressive Algorithms Journal of MachineLearning Research(JMLR), 7:551?585.Maryam Fazel.
2002.
Matrix Rank Minimization withApplications PhD thesis, Stanford University.Kevin Gimpel, and Noah A. Smith 2012.
StructuredRamp Loss Minimization for Machine TranslationProceedings of North American Chapter of the As-sociation for Computational Linguistics(NAACL),221-231.674Tamir Hazan, Simon Polak, and Amnon Shashua 2005.Sparse Image Coding using a 3D Non-negative Ten-sor Factorization Proceedings of the InternationalConference on Computer Vision (ICCV).Mark Hopkins and Jonathan May.
2011.
Tuningas Reranking Proceedings of Empirical Methodsin Natural Language Processing(EMNLP), 1352-1362.Tamara Kolda and Brett Bader.
2009.
Tensor Decom-positions and Applications SIAM Review, 51:455-550.Ryan McDonald, Koby Crammer, and FernandoPereira.
2005.
Online Large-Margin Training ofDependency Parsers Proceedings of the 43rd An-nual Meeting of the ACL, 91?98.Amnon Shashua, and Tamir Hazan.
2005.
Non-Negative Tensor Factorization with Applications toStatistics and Computer Vision Proceedings ofthe International Conference on Machine Learning(ICML).Tim Van de Cruys, Thierry Poibeau, and Anna Korho-nen.
2013.
A Tensor-based Factorization Model ofSemantic Compositionality Proceedings of NAACL-HLT, 1142?1151.A Proof of Theorem 1Proof.
For D = 1, it is obvious that if a set ofreal numbers {x1, .
.
.
, xn} can be represented bya rank-1 matrix, it can always be represented by avector, but the reverse is not true.For D > 1, if {x1, .
.
.
, xn} can be repre-sented by P = p1?
p2?
.
.
.
?
pD, namelyxi= Pi1,...,iD=?Dd=1pdid, then for any compo-nent vector in mode d,[pd1, pd2, .
.
.
, pdnd] = [sd1pd1, sd2pd1, .
.
.
, sdnpdpd1]where npdis the size of mode d of P , sdjis a con-stant and sdj=pi1,...,id?1,j,id+1,...,iDpi1,...,id?1,1,id+1,...,iDThereforexi= Pi1,...,iD= x1,...,1D?d=1sdid(7)and this representation is unique for a given D(upto the ordering of pjand sdjin pj, which simplyassigns {x1, .
.
.
, xn} with different indices in thetensor), due to the pairwise proportional constraintimposed by xi/xj, i, j = 1, .
.
.
, n.If xican also be represented by Q, then xi=Qi1,...,iD+1= x1,...,1?D+1d=1tdid, where tdjhas asimilar definition as sdj.
Then it must be the casethat?d1, d2?
{1, .
.
.
, D + 1}, d ?
{1, .
.
.
, D}, d16= d2s.t.td1id1td2id2= sdid, (8)tdaida= sdbidb, da6= d1, d2, db6= dsince otherwise {x1, .
.
.
, xn} would be repre-sented by a different set of factors than those givenin Equation (7).Therefore, in order for tensor Q to representthe same set of real numbers that P represents,there needs to exist a vector [sd1, .
.
.
, sdnd] that canbe represented by a rank-1 matrix as indicated byEquation (8), which is in general not guaranteed.On the other hand, if {x1, .
.
.
, xn} can be rep-resented by Q, namelyxi= Qi1,...,iD+1=D+1?d=1qdidthen we can just pick d1?
{1, .
.
.
, D}, d2= d1+1 and letq?= [qd11qd21, qd11qd22, .
.
.
, qd1nqd2qd2nqd1]andQ?= q1?
.
.
.?qd1?1?q??qd2+1?
.
.
.
?qD+1Hence {x1, .
.
.
, xn} can also be represented by aDthorder tensor Q?.675
