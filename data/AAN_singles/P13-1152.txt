Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 1547?1557,Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational LinguisticsCombining Referring Expression Generation and Surface Realization:A Corpus-Based Investigation of ArchitecturesSina Zarrie?
Jonas KuhnInstitut fu?r maschinelle SprachverarbeitungUniversity of Stuttgart, Germanysina.zarriess,jonas.kuhn@ims.uni-stuttgart.deAbstractWe suggest a generation task that inte-grates discourse-level referring expressiongeneration and sentence-level surface re-alization.
We present a data set of Ger-man articles annotated with deep syntaxand referents, including some types of im-plicit referents.
Our experiments compareseveral architectures varying the order of aset of trainable modules.
The results sug-gest that a revision-based pipeline, with in-termediate linearization, significantly out-performs standard pipelines or a parallelarchitecture.1 IntroductionGenerating well-formed linguistic utterances froman abstract non-linguistic input involves makinga multitude of conceptual, discourse-level as wellas sentence-level, lexical and syntactic decisions.Work on rule-based natural language generation(NLG) has explored a number of ways to com-bine these decisions in an architecture, rangingfrom integrated systems where all decisions hap-pen jointly (Appelt, 1982) to strictly sequentialpipelines (Reiter and Dale, 1997).
While inte-grated or interactive systems typically face issueswith efficiency and scalability, they can directlyaccount for interactions between discourse-levelplanning and linguistic realization.
For instance,Rubinoff (1992) mentions Example (1) where thesentence planning component needs to have ac-cess to the lexical knowledge that ?order?
and not?home?
can be realized as a verb in English.
(1) a.
*John homed him with an order.b.
John ordered him home.In recent data-driven generation research, thefocus has somewhat shifted from full data-to-textsystems to approaches that isolate well-definedsubproblems from the NLG pipeline.
In particular,the tasks of surface realization and referring ex-pression generation (REG) have received increas-ing attention using a number of available anno-tated data sets (Belz and Kow, 2010; Belz et al,2011).
While these single-task approaches havegiven rise to many insights about algorithms andcorpus-based modelling for specific phenomena,they can hardly deal with aspects of the architec-ture and interaction between generation levels.This paper suggests a middle ground betweenfull data-to-text and single-task generation, com-bining two well-studied NLG problems.
We in-tegrate a discourse-level approach to REG withsentence-level surface realization in a data-drivenframework.
We address this integrated task with aset of components that can be trained on flexibleinputs which allows us to systematically exploredifferent ways of arranging the components in ageneration architecture.
Our main goal is to inves-tigate how different architectural set-ups accountfor interactions between generation decisions atthe level of referring expressions (REs), syntaxand word order.Our basic set-up is inspired from the GeneratingReferring Expressions in Context (GREC) tasks,where candidate REs have to be assigned to in-stances of a referent in a Wikipedia article (Belzand Kow, 2010).
We have created a dataset of Ger-man texts with annotations that extend this stan-dard in three substantial ways: (i) our domain con-sists of articles about robbery events that mainlyinvolve two main referents, a victim and a per-petrator (perp), (ii) annotations include deep andshallow syntactic relations similar to the repre-sentations used in (Belz et al, 2011) (iii) anno-tations include empty referents, as e.g.
in passivesand nominalizations directing attention to the phe-nomenon of implicit reference, which is largelyunderstudied in NLG.
Figure 1 presents an exam-ple for a deep syntax tree with underspecified RE1547(Tree) beagentperpmodonpobjtrialmodbecausesubattackagentperpthemevictimperp italianstwomenthe twothey <empty>victim mana youngvictimthehe <empty>Figure 1: Underspecified tree with RE candidatesslots and lists of candidates REs for each referent.Applying a strictly sequential pipeline on ourdata, we observe incoherent system output thatis related to an interaction of generation levels,very similar to the interleaving between sentenceplanning and lexicalization in Example (1).
Apipeline that first inserts REs into the underspec-ified tree in Figure 1, then generates syntax and fi-nally linearizes, produces inappropriate sentenceslike (2-a).
(2) a.
*[The two men]p are on trial because of an attackby [two italians]p on [a young man]v .b.
[Two italians]p are on trial because of an attack on[a young man]v .Sentence (2-a) is incoherent because the syntac-tic surface obscurs the intended meaning that ?twoitalians?
and ?the two men?
refer to the same ref-erent.
In order to generate the natural Sentence(2-b), the RE component needs information aboutlinear precedence of the two perp instances and thenominalization of ?attack?.
These types of inter-actions between referential and syntactic realiza-tion have been thoroughly discussed in theoreticalaccounts of textual coherence, as e.g.
CenteringTheory (Grosz et al, 1995).The integrated modelling of REG and surfacerealization leads to a considerable expansion ofthe choice space.
In a sentence with 3 referentsthat each have 10 RE candidates and can be freelyordered, the number of surface realizations in-creases from 6 to 6?103, assuming that the remain-ing words can not be syntactically varied.
Thus,even when the generation problem is restricted tothese tasks, a fully integrated architecture facesscalability issues on realistic corpus data.In this work, we assume a modular set-up ofthe generation system that allows for a flexibleordering of the single components.
Our experi-ments vary 3 parameters of the generation archi-tecture: 1) the sequential order of the modules,2) parallelization of modules, 3) joint vs. sepa-rate modelling of implicit referents.
Our resultssuggest that the interactions between RE and syn-tax can be modelled in sequential generation ar-chitecture where the RE component has accessto information about syntactic realization and anapproximative, intermediate linearization.
Sucha system is reminiscent of earlier work in rule-based generation that implements an interactive orrevision-based feedback between discourse-levelplanning and linguistic realisation (Hovy, 1988;Robin, 1993).2 Related WorkDespite the common view of NLG as a pipelineprocess, it is a well-known problem that high-level, conceptual knowledge and low-level lin-guistic knowledge are tightly interleaved (Danlos,1984; Mellish et al, 2000).
In rule-based, strictlysequential generators these interactions can leadto a so-called generation gap, where a down-stream module cannot realize a text or sentenceplan generated by the preceding modules (Meteer,1991; Wanner, 1994).
For this reason, a num-ber of other architectures has been proposed, seeDe Smedt et al (1996) for an overview.
For rea-sons of tractability and scalability, many prac-tical NLG systems still have been designed assequential pipelines that follow the basic layoutof macroplanning-microplanning-linguistic real-ization (Reiter, 1994; Cahill et al, 1999; Batemanand Zock, 2003).In recent data-driven research on NLG, manysingle tasks have been addressed with corpus-based methods.
For surface realization, the stan-dard set-up is to regenerate from syntactic rep-resentations that have been produced for realis-tic corpus sentences.
The first widely known sta-tistical approach by Langkilde and Knight (1998)used language-model n-gram statistics on a wordlattice of candidate realisations to guide a ranker.Subsequent work explored ways of exploiting lin-guistically annotated data for trainable generationmodels (Ratnaparkhi, 2000; Belz, 2005).
Work ondata-driven approaches has led to insights aboutthe importance of linguistic features for sentence1548linearization decisions (Ringger et al, 2004; Filip-pova and Strube, 2007; Cahill and Riester, 2009).(Zarrie?
et al, 2012) have recently argued thatthe good performance of these linguistically mo-tivated word order models, which exploit morpho-syntactic features of noun phrases (i.e.
refer-ents), is related to the fact that these morpho-syntactic features implicitly encode a lot of knowl-edge about the underlying discourse or informa-tion structure.A considerable body of REG research has beendone in the paradigm established by Dale (1989;1995).
More closely related to our work are ap-proaches in the line of Siddarthan and Copes-take (2004) or Belz and Varges (2007) who gener-ate contextually appropriate REs for instances of areferent in a text.
Belz and Varges (2007)?s GRECdata set includes annotations of implicit subjectsin coordinations.
Zarrie?
et al (2011) deal withimplicit subjects in passives, proposing a set ofheuristics for adding these agents to the genera-tion input.
Roth and Frank (2012) acquire au-tomatic annotations of implicit roles for the pur-pose of studying coherence patterns in texts.
Im-plicit referents have also received attention for theanalysis of semantic roles (Gerber and Chai, 2010;Ruppenhofer et al, 2010).Statistical methods for data-to-text generationhave been explored only recently.
Belz (2008)trains a probabilistic CFG to generate weatherforecasts, Chen et al (2010) induce a synchronousgrammar to generate sportcaster text.
Both ad-dress a restricted domain where a direct align-ment between units in the non-linguistic represen-tation and the linguistic utterance can be learned.Marciniak and Strube (2005) propose an ILPmodel for global optimization in a generation taskthat is decomposed into a set of classifiers.
Bohnetet al (2011) deal with multi-level generation in astatistical framework and in a less restricted do-main.
They adopt a standard sequential pipelineapproach.Recent corpus-based generation approachesfaced the problem that existing standard treebankrepresentations for parsing or other analysis tasksdo not necessarily fit the needs of generation(Bohnet et al, 2010; Wanner et al, 2012).
Zarrie?et al (2011) discuss the problem of an input rep-resentation that is appropriately underspecified forthe realistic generation of voice alternations.3 The Data SetThe data set for our generation experiments con-sists of 200 newspaper articles about robberyevents.
The articles were extracted from a largeGerman newspaper corpus.
A complete exampletext with RE annotations is given in Figure 2, Ta-ble 1 summarizes some data set statistics.3.1 RE annotationThe RE annotations mark explicit and implicitmentions of referents involved in the robbery eventdescribed in an article.
Explicit mentions aremarked as spans on the surface sentence, labeledwith the referent?s role and an ID.
We annotate thefollowing referential roles: (i) perpetrator (perp),(ii) victim, (iii) source, according to the core rolesof the Robbery frame in English FrameNet.
Weinclude source since some texts do not mention aparticular victim, but rather the location of the rob-bery (e.g.
a bank, a service station).
The ID distin-guishes referents that have the same role, e.g.
?thehusband?
and the ?young family?
in Sentences(3-a) and (3-d) in Figure 2.
Each RE is linked toits syntactic head.
This complies with the GRECdata sets, and is also useful for further annotationof the deep syntax level (see Section 3.2).The RE implicit mentions of victim, perp, andsource are annotated as attributes of their syntac-tic heads in the surface sentence.
We consider thefollowing types of implicit referents: (i) agents inpassives (e.g.
?robbed?
in (3-a)), (ii) arguments ofnominalizations (e.g.
?resistance?
in (3-e)), (iii)possessives (e.g.
?watch?
in (3-f)), (iv) missingsubjects in coordinations.
(e.g.
?flee?
in (3-f))The brat tool (Stenetorp et al, 2012) was usedfor annotation.
We had 2 annotators with a compu-tational linguistic background, provided with an-notation guidelines.
They were trained on a set of20 texts.
We measure a good agreement on anotherset of 15 texts: the simple pairwise agreement forexplicit mentions is 95.14%-96.53% and 78.94%-76.92% for implicit mentions.13.2 Syntax annotationThe syntactic annotation of our data includes twolayers: shallow and deep, labeled dependencies,similar to the representation used in surface real-ization shared tasks (Belz et al, 2011).
We use1Standard measures for the ?above chance annotatoragreement?
are only defined for task where the set of anno-tated items is pre-defined.1549(3) a. Junge Familie v:0 Young family aufon demthe Heimwegposs:vway homeposs:v ausgeraubtag:probbedag:pb.
DieThePolizeipolicesuchtlooksnachforzwei ungepflegt wirkenden jungen Ma?nnern im Alter von etwa 25 Jahren p:0.two shabby-looking young men of about 25 years .c.
Sie p:0Theysollenare said toamonMontagMondaygegenaround2020Uhro?clock eine junge Familie mit ihrem sieben Monate alten Baby v:0 a young family with their seven month old baby aufondemtheHeimwegposs:vway homeposs:vvonfromeinemaEinkaufsbummelshopping touru?berfallenattackedundandausgeraubtrobbedhaben.have.d.
WieAsdiethePolizeipoliceberichtet,reports,drohtenthreateneddie zwei Ma?nner p:0the two men  dem Ehemann v:1,  the husband  ihn v:1  him zusammenzuschlagen.beat up.e.  Er v:1  He gabgave deshalbtherefore  seine v:1  his Brieftaschewallet ohnewithout Gegenwehrag:v,the:presistanceag:v,the:p heraus.out.f.
Anschlie?endAfterwardsnahmentook  ihm v:1  him die Ra?uber p:0the robbers nochalso diethe Armbanduhrposs:vwatchposs:v aboff undand flu?chtetenag:p.fleedag:p.Figure 2: Example text with RE annotations, oval boxes mark victim mentions, square boxes mark perpmentions, heads of implicit arguments are underlinedthe Bohnet (2010) dependency parser to obtain anautomatic annotation of shallow or surface depen-dencies for the corpus sentences.The deep syntactic dependencies are derivedfrom the shallow layer by a set of hand-writtentransformation rules.
The goal is to link referentsto their main predicate in a uniform way, indepen-dently of the surface-syntactic realization of theverb.
We address passives, nominalizations andpossessives corresponding to the contexts wherewe annotated implicit referents (see above).
Thetransformations are defined as follows:1. remove auxiliary nodes, verb morphology and finite-ness, a tense feature distinguishes past and present, e.g.
?haben:AUX u?berfallen:VVINF?
(have attacked) mapsto ?u?berfallen:VV:PAST?
(attack:PAST)2. map subjects in actives and oblique agents in passivesto ?agents?
; objects in actives and subjects in passive to?themes?, e.g.
victim/subj was attacked by perp/obl-agmaps to perp/agent attack victim/theme3.
attach particles to verb lemma, e.g.
?gab?
... ?heraus?in (3-e) is mapped to ?herausgeben?
(give to)4. map nominalized to verbal lemmas, their prepositionaland genitive arguments to semantic subjects and ob-jects, e.g.
attack on victim is mapped to attack vic-tim/theme5.
normalize prenominal and genitive postnominal poses-sives, e.g.
?seine Brieftasche?
(his wallet) and ?dieBrieftasche des Opfers?
(the wallet of the victim) mapto ?die Brieftasche POSS victim?
(the wallet of victim),only applies if possessive is an annotated RENominalizations are mapped to their verbalbase forms on the basis of lexicalized rules for thenominalized lemmas observed in the corpus.
Theother transformations are defined on the shallowdependency annotation.# sentences 2030# explicit REs 3208# implicit REs 1778# passives 383# nominalizations 393# possessives 1150Table 1: Basic annotation statistics3.3 Multi-level RepresentationIn the final representation of our data set, we inte-grate the RE and deep syntax annotation by replac-ing subtrees corresponding to an RE span.
The REslot in the tree of the sentence is labeled with itsreferential role and its ID.
All RE subtrees for areferent in a text are collected in a candidate listwhich is initialized with three default REs: (i) apronoun, (ii) a default nominal (e.g.
?the victim?
),(iii) the empty RE.
In contrast to the GREC datasets, our RE candidates are not represented as theoriginal surface strings, but as non-linearized sub-trees.
The resulting multi-layer representation foreach text is structured as follows:1. unordered deep trees with RE slots (deepSyn?re)2. unorderd shallow trees with RE slots(shallowSyn?re)3. unordered RE subtrees4.
linearized, fully specified surface trees (linSyn+re)5. alignments between nodes in 1., 2., 4.The generation components in Section 4 alsouse intermediate layers where REs are insertedinto the deep trees (deepSyn+re) or shallow trees(shallowSyn+re).Nodes in unordered trees are deterministicallysorted by their : 1. distance to the root, 2. label,15503.
PoS tag, 4. lemma.
The generation componentstraverse the nodes in this the order.4 Generation SystemsOur main goal is to investigate different architec-tures for combined surface realization and refer-ring expression generation.
We assume that thistask is split into three main modules: a syntax gen-erator, an REG component, and a linearizer.
Thecomponents are implemented in a way that theycan be trained and applied on varying inputs, de-pending on the pipeline.
Section 4.1 describes thebasic set-up of our components.
Section 4.2 de-fines the architectures that we will compare in ourexperiments (Section 5).
Section 4.3 presents theimplementation of the underlying feature models.4.1 Components4.1.1 SYN: Deep to Shallow SyntaxFor mapping deep to shallow dependency trees,the syntax generator induces a probabilistic treetransformation.
The transformations are restrictedto verb nodes in the deep tree (possessives arehandled in the RE module) and extracted fromthe alignments between the deep and shallowlayer in the training input.
As an example, thedeep node ?attack:VV?
aligns to ?have:AUX at-tacked:VVINF?, ?attacks:VVFIN?, ?the:ART at-tack:NN on:PRP?.
The learner is implementedas a ranking component, trained with SVMrank(Joachims, 2006).
During training, each instanceof a verb node has one optimal shallow depen-dency alignment and a set of distractor candidates.During testing, the module has to pick the bestshallow candidate according to its feature model.In our crossvalidation set-up (see Section 5),we extract, on average, 374 transformations fromthe training sets.
This set subdivides into non-lexicalized and lexicalized transformations.
Themapping rule in (4-a) that simply rewrites the verbunderspecified PoS tag to the finite verb tag in theshallow tree illustrates the non-lexicalized case.Most transformation rules (335 out of 374 on aver-age) are lexicalized for a specific verb lemma andmostly transform nominalizations as in rule (4-b)and particles (see Section 3.2).
(4) a.
(x,lemma,VV,y)?
(x,lemma,VVFIN,y)b.
(x,u?berfallen/attack,VV,y) ?
(x,bei/at,PREP,y),(z,U?berfall/attack,NN,x),(q,der/the,ART,z)The baseline for the verb transformation com-ponent is a two-step procedure: 1) pick a lexical-ized rule if available for that verb lemma, 2) pickthe most frequent transformation.4.1.2 REG: Realizing Referring ExpressionsSimilar to the syntax component, the REG mod-ule is implemented as a ranker that selects surfaceRE subtrees for a given referential slot in a deepor shallow dependency tree.
The candidates forthe ranking correspond to the entire set of REsused for that referential role in the original text(see Section 3.1).
The basic RE module is a jointmodel of all RE types, i.e.
nominal, pronominaland empty realizations of the referent.
For the ex-periment in Section 5.4, we use an additional sep-arate classifier for implicit referents, also trainedwith SVMrank.
It uses the same feature modelas the full ranking component, but learns a binarydistinction for implicit or explicit mentions of areferent.
The explicit mentions will be passed tothe RE ranking component.The baseline for the REG component is definedas follows: if the preceding and the current REslot are instances of the same referent, realize apronoun, else realize the longest nominal RE can-didate that has not been used in the preceding text.4.1.3 LIN: LinearizationFor linearization, we use the state-of-the-artdependency linearizer described in Bohnet etal.
(2012).
We train the linearizer on an auto-matically parsed version of the German TIGERtreebank (Brants et al, 2002).
This versionwas produced with the dependency parser byBohnet (2010), trained on the dependency conver-sion of TIGER by Seeker and Kuhn (2012).4.2 ArchitecturesDepending on the way the generation componentsare combined in an architecture, they will have ac-cess to different layers of the input representation.The following definitions of architectures recur tothe layers introduced in Section 3.3.4.2.1 First PipelineThe first pipeline corresponds most closely to astandard generation pipeline in the sense of (Reiterand Dale, 1997).
REG is carried out prior to sur-face realization such that the RE component doesnot have access to surface syntax or word orderwhereas the SYN component has access to fullyspecified RE slots.?
training15511.
train REG: (deepSyn?re, deepSyn+re)2. train SYN: (deepSyn+re, shallowSyn+re)?
prediction1.
apply REG: deepSyn?re ?
deepSyn+re2.
apply SYN: deepSyn+re ?
shallowSyn+re3.
linearize: shallowSyn+re ?
linSyn+re4.2.2 Second PipelineIn the second pipeline, the order of the RE andSYN component is switched.
In this case, REGhas access to surface syntax without word orderbut the surface realization is trained and appliedon trees with underspecified RE slots.?
training1.
train SYN: (deepSyn?re, shallowSyn?re)2. train REG: (shallowSyn?re, shallowSyn+re)?
prediction1.
apply SYN: deepSyn?re ?
shallowSyn?re2.
apply REG: shallowSyn?re ?shallowSyn+re3.
linearize: shallowSyn+re ?
linSyn+re4.2.3 Parallel SystemA well-known problem with pipeline architecturesis the effect of error propagation.
In our parallelsystem, the components are trained independentlyof each other and applied in parallel on the deepsyntactic input with underspecified REs.?
training1.
train SYN: (deepSyn?re, shallowSyn?re)2. train REG: (deepSyn?re, deepSyn+re)?
prediction1.
apply REG and SYN:deepSyn?re ?
shallowSyn+re2.
linearize: shallowSyn+re ?
linSyn+re4.2.4 Revision-based SystemIn the revision-based system, the RE componenthas access to surface syntax and a preliminary lin-earization, called prelinSyn.
In this set-up, we ap-ply the linearizer first on trees with underspeci-fied RE slots.
For this step, we insert the defaultREs for the referent into the respective slots.
AfterREG, the tree is linearized once again.?
training1.
train SYN on gold pairs of(deepSyn?re, shallowSyn?re)2. train REG on gold pairs of(prelinSyn?re, prelinSyn+re)?
prediction1.
apply SYN: deepSyn?re ?
shallowSyn?re2.
linearize: shallowSyn?re ?
prelinSyn?re3.
apply REG: prelinSyn?re ?
prelinSyn+re4.
linearize: prelinSyn+re ?
linSyn+re4.3 Feature ModelsThe implementation of the feature models is basedon a general set of templates for the SYN and REGcomponent.
The exact form of the models dependson the input layer of a component in a given ar-chitecture.
For instance, when SYN is trained ondeepSyn?re, the properties of the children nodesare less specific for verbs that have RE slots astheir dependents.
When the SYN component istrained on deepSyn+re, lemma and POS of thechildren nodes are always specified.The feature templates for SYN combine prop-erties of the shallow candidate nodes (label, PoSand lemma for top node and its children) with theproperties of the instance in the tree: (i) lemma,tense, (ii) sentence is a header, (iii) label, PoS,lemma of mother node, children and grandchil-dren nodes (iv) number, lemmas of other verbs inthe sentence.The feature templates for REG combine proper-ties of the candidate RE (PoS and lemma for topnode and its children, length) with properties ofthe RE slot in the tree: lemma, PoS and labels forthe (i) mother node, (ii) grandmother node, (iii)uncle and sibling nodes.
Additionally, we imple-ment a small set of global properties of a referentin a text: (i) identity is known, (ii) plural or sin-gular referent, (iii) age is known, and a number ofcontextual properties capturing the previous refer-ents and their predicted REs: (i) role and realiza-tion of the preceding referent, (ii) last mention ofthe current referent, (iii) realization of the referentin the header.5 ExperimentsIn this experimental section, we provide a corpus-based evaluation of the generation componentsand architectures introduced in Section 4.
In thefollowing, Section 5.1 presents the details of ourevaluation methodology.
In Section 5.2, we dis-cuss the first experiment that evaluates the pipelinearchitectures and the single components on oracleinputs.
Section 5.3 describes an experiment whichcompares the parallel and the revision-based ar-chitecture against the pipeline.
In Section 5.4, wecompare two methods for dealing with the implicitreferents in our data.
Section 5.5 provides somegeneral discussion of the results.1552Sentence overlap SYN Accuracy RE AccuracyInput System BLEU NIST BLEUr String Type String Type ImpldeepSyn?re Baseline 42.38 9.9 47.94 35.66 44.81 33.3 36.03 50.43deepSyn?re 1st pipeline 54.65 11.30 59.95 57.09 68.15 54.61 71.51 84.72deepSyn?re 2nd pipeline 54.28 11.25 59.62 59.14 68.58 52.24 68.2 82gold deepSyn+re SYN?LIN 63.9 12.7 62.86 60.83 69.74 100 100 100gold shallowSyn?re REG?LIN 60.57 11.87 68.06 100 100 60.53 75.86 88.86gold shallowSyn+re LIN 79.17 13.91 72.7 100 100 100 100 100Table 2: Evaluating pipeline architectures against the baseline and upper bounds5.1 Evaluation MeasuresWe split our data set into 10 splits of 20 articles.We use one split as the development set, and cross-validate on the remaining splits.
In each case,the downstream modules of the pipeline will betrained on the jackknifed training set.Text normalization: We carry out automaticevaluation calculated on lemmatized text with-out punctuation, excluding additional effects thatwould be introduced from a morphology genera-tion component.Measures: First, we use a number of evalua-tion measures familiar from previous generationshared tasks:1.
BLEU, sentence-level geometric mean of 1- to 4-gramprecision, as in (Belz et al, 2011)2.
NIST, sentence-level n-gram overlap weighted infavour of less frequent n-grams, as in (Belz et al, 2011)3.
RE Accuracy on String, proportion of REs selected bythe system with a string identical to the RE string in theoriginal corpus, as in (Belz and Kow, 2010)4.
RE Accuracy on Type, proportion of REs selected bythe system with an RE type identical to the RE type inthe original corpus, as in (Belz and Kow, 2010)Second, we define a number of measures moti-vated by our specific set-up of the task:1.
BLEUr , sentence-level BLEU computed on post-processed output where predicted referring expressionsfor victim and perp are replaced in the sentences (bothgold and predicted) by their original role label, thisscore does not penalize lexical mismatches betweencorpus and system REs2.
RE Accuracy on Impl, proportion of REs predicted cor-rectly as implicit/non-implicit3.
SYN Accuracy on String, proportion of shallow verbcandidates selected by the system with a string identicalto the verb string in the original corpus4.
SYN Accuracy on Type, proportion of shallow verbcandidates selected by the system with a syntactic cat-egory identical to the category in the original corpus5.2 Pipelines and Upper BoundsThe first experiment addresses the first and sec-ond pipeline introduced in Section 4.2.1 and 4.2.2.The baseline combines the baseline version ofthe SYN component (Section 4.1.1) and the REGcomponent (Section 4.1.2) respectively.
As we re-port in Table 2, both pipelines largely outperformthe baseline.
Otherwise, they obtain very similarscores in all measures with a small, weakly signif-icant tendency for the first pipeline.
The only re-markable difference is that the accuracy of the in-dividual components is, in each case, lower whenthey are applied as the second step in the pipeline.Thus, the RE accuracy suffers from mistakes fromthe predicted syntax in the same way that the qual-ity of syntax suffers from predicted REs.The three bottom rows in Table 2 report the per-formance of the individual components and lin-earization when they are applied to inputs with anREG and SYN oracle, providing upper bounds forthe pipelines applied on deepSyn?re.
When REGand linearization are applied on shallowSyn?rewith gold shallow trees, the BLEU score islower (60.57) as compared to the system that ap-plies syntax and linearization on deepSyn+re,deep trees with gold REs (BLEU score of 63.9).However, the BLEUr score, which generalizesover lexical RE mismatches, is higher for theREG?LIN components than for SYN?LIN.Moreover, the BLEUr score for the REG?LINsystem comes close to the upper bound that ap-plies linearization on linSyn+re, gold shallowtrees with gold REs (BLEUr of 72.4), whereasthe difference in standard BLEU and NIST ishigh.
This effect indicates that the RE predic-tion mostly decreases BLEU due to lexical mis-matches, whereas the syntax prediction is morelikely to have a negative impact on final lineariza-tion.The error propagation effects that we find in thefirst and second pipeline architecture clearly showthat decisions at the levels of syntax, referenceand word order interact, otherwise their predic-1553Input System BLEU NIST BLEUrdeepSyn?re 1st pipeline 54.65 11.30 59.95deepSyn?re Parallel 54.78 11.30 60.05deepSyn?re Revision 56.31 11.42 61.30Table 3: Architecture evaluationtion would not affect each other.
In particular, theREG module seems to be affected more seriously,the String Accuracy decreases from 60.53 on goldshallow trees to 52.24 on predicted shallow treeswhereas the Verb String Accuracy decreases from60.83 on gold REs to 57.04 on predicted REs.5.3 Revision or parallelism?The second experiment compares the first pipelineagainst the parallel and the revision-based ar-chitecture introduced in Section 4.2.3 and 4.2.4.The evaluation in Table 3 shows that the paral-lel architecture improves only marginally over thepipeline.
By contrast, we obtain a clearly signifi-cant improvement for the revision-based architec-ture on all measures.
The fact that this architec-ture significantly improves the BLEU, NIST andthe BLEUr score of the parallel system indicatesthat the REG benefits from the predicted syntaxwhen it is approximatively linearized.
The factthat also the BLEUr score improves shows that ahigher lexical quality of the REs leads to better fi-nal linearizations.Table 4 shows the performance of the REGmodule on varying input layers, providing a moredetailed analysis of the interaction between RE,syntax and word order.
In order to produce thedeeplinSyn?re layer, deep syntax trees with ap-proximative linearizations, we preprocessed thedeep trees by inserting a default surface trans-formation for the verb nodes.
We compare thisinput for REG against the prelinSyn?re layerused in the revision-based architecture, and thedeepSyn?re layer used in the pipeline and the par-allel architecture.
The REG module benefits fromthe linearization in the case of deeplinSyn?reand prelinSyn?re, outperforming the compo-nent trained applied on the non-linearized deepsyntax trees.
However, the REG module ap-plied on prelinSyn?re, predicted shallow and lin-earized trees, clearly outperforms the module ap-plied on deeplinSyn?re.
This shows that theRE prediction can actually benefit from the pre-dicted shallow syntax, but only when the predictedtrees are approximatively linearized.
As an up-per bound, we report the performance obtained onRE AccuracyInput System String Type ImpldeepSyn?re RE 54.61 71.51 84.72deeplinSyn?re RE 56.78 72.23 84.71prelinSyn?re RE 58.81 74.34 86.37gold linSyn?re RE 68.63 83.63 94.74Table 4: RE generation from different input layerslinSyn?re, gold shallow trees with gold lineariza-tions.
This set-up corresponds to the GREC tasks.The gold syntax leads to a huge increase in perfor-mance.These results strengthen the evidence from theprevious experiment that decisions at the level ofsyntax, reference and word order are interleaved.A parallel architecture that simply ?circumvents?error propagation effects by making decisions in-dependent of each other is not optimal.
Instead,the automatic prediction of shallow syntax canpositively impact on RE generation if these shal-low trees are additionally processed with an ap-proximative linearization step.5.4 A joint treatment of implicit referents?The previous experiments have pursued a jointapproach for modeling implicit referents.
Thehypothesis for this experiment is that the SYNcomponent and the intermediate linearization ina revision-based architecture could benefit from aseparate treatment of implicit referents since verbalternations like passive or nominalization ofteninvolve referent deletions.The evaluation in Table 5 provides contradic-tory results depending on the evaluation measure.For the first pipeline, the system with a separatetreatment of implicit referents significantly outper-forms the joint system in terms of BLEU.
How-ever, the BLEUr score does not improve.
In therevision-based architecture, we do not find a clearresult for or against a joint modelling approach.The revision-based system with disjoint modellingof implicits shows a slight, non-significant in-crease in BLEU score.
By contrast, the BLEUrscore is signficantly better for the joint approach.We experimented with parallelization of syntaxgeneration and prediction of implicit referents ina revision-based system.
This has a small positiveeffect on the BLEUr score and a small negativeeffect on the plain BLEU and NIST score.
Thesecontradictory scores might indicate that the auto-matic evaluation measures cannot capture all as-pects of text quality, an issue that we discuss inthe following.1554(5) Generated by sequential system:a. DeshalbThereforegabgavedem Ta?terto the robber  seine  his Brieftaschewallet ohnewithout da?that das Opfer  the victim Widerstandresistance leistetshows heraus.out.b.
ErHenahmtakesanschlie?endafterwards dem Opfer  the victim diethe Armbanduhrwatch aboff undand der Ta?terthe robber flu?chtete.fleed.
(6) Generated by revision-based system:a. Das Opfer  The victim gibtgave deshalbtherefore  seine  his Brieftaschewallet ohnewithout Widerstandresistance zuto leistenshow heraus.out.b.
Anschlie?endAfterwardsnahmtookder Ta?terthe robber dem Opfer  the victim diethe Armbanduhrwatch aboff undand flu?chtete.fleed.Figure 3: Two automatically generated outputs for the Sentences (3e-f) in Figure 2.Joint System BLEU NIST BLEUr+ 1st pipeline 54.65 11.30 59.95- 1st pipeline 55.38 11.48 59.52+ Revision 56.31 11.42 61.30- Revision 56.42 11.54 60.52- Parallel+Revision 56.29 11.51 60.63Table 5: Implicit reference and architectures5.5 DiscussionThe results presented in the preceding evaluationsconsistenly show the tight connections betweendecisions at the level of reference, syntax and wordorder.
These interactions entail highly interde-pendent modelling steps: Although there is a di-rect error propagation effect from predicted verbtransformation on RE accuracy, predicted syntaxstill leads to informative intermediate lineariza-tions that improve the RE prediction.
Our optimalgeneration architecture thus has a sequential set-up, where the first linearization step can be seenas an intermediate feedback that is revised in thefinal linearization.
This connects to work in, e.g.
(Hovy, 1988; Robin, 1993).In Figure 3, we compare two system outputs forthe last two sentences of the text in Figure 2.
Theoutput of the sequential system is severely inco-herent and would probably be rejected by a hu-man reader: In sentence (5a) the victim subject ofan active verb is deleted, and the relation betweenthe possessive and the embedded victim RE is notclear.
In sentence (5b) the first conjunct realizesa pronominal perp RE and the second conjunct anominal perp RE.
The output of the revision-basedsystem reads much more natural.
This exampleshows that the extension of the REG problem totexts with more than one main referent (as in theGREC data set) yields interesting inter-sententialinteractions that affect textual coherence.We are aware of the fact that our automatic eval-uation might only partially render certain effects,especially with respect to textual coherence.
Itis likely that the BLEU scores do not capture themagnitude of the differences in text quality illus-trated by the Examples (5-6).
Ultimately, a hu-man evaluation for this task is highly desirable.We leave this for future work since our integratedset-up rises a number of questions with respect toevaluation design.
In a preliminary analysis, wenoticed the problem that human readers find it dif-ficult to judge discourse-level properties of a textlike coherence or naturalness when the generationoutput is not perfectly grammatical or fluent at thesentence level.6 ConclusionWe have presented a data-driven approach for in-vestigating generation architectures that addressdiscourse-level reference and sentence-level syn-tax and word order.
The data set we created for ourexperiments basically integrates standards fromprevious research on REG and surface realizationand extends the annotations to further types of im-plicit referents.
Our results show that interactionsbetween the different generation levels are bestcaptured in a sequential, revision-based pipelinewhere the REG component has access to predic-tions from the syntax and the linearization mod-ule.
These empirical findings obtained from ex-periments with generation architectures have clearconnections to theoretical accounts of textual co-herence.AcknowledgementsThis work was supported by the DeutscheForschungsgemeinschaft (German ResearchFoundation) in SFB 732 Incremental Specificationin Context, project D2.1555ReferencesDouglas Edmund Appelt.
1982.
Planning natural lan-guage utterances to satisfy multiple goals.
Ph.D.thesis, Stanford, CA, USA.John Bateman and Michael Zock.
2003.
NaturalLanguage Generation.
In Ruslan Mitkov, editor,The Oxford Handbook of Computational Linguis-tics.
Oxford University Press.Anja Belz and Eric Kow.
2010.
The GREC Challenges2010: overview and evaluation results.
In Proc.
ofthe 6th International Natural Language GenerationConference, INLG ?10, pages 219?229, Strouds-burg, PA, USA.Anja Belz and Sebastian Varges.
2007.
Generation ofrepeated references to discourse entities.
In Proc.
ofthe 11th European Workshop on Natural LanguageGeneration, ENLG ?07, pages 9?16, Stroudsburg,PA, USA.
Association for Computational Linguis-tics.Anja Belz, Mike White, Dominic Espinosa, Eric Kow,Deirdre Hogan, and Amanda Stent.
2011.
The firstsurface realisation shared task: Overview and evalu-ation results.
In Proc.
of the Generation ChallengesSession at the 13th European Workshop on Natu-ral Language Generation, pages 217?226, Nancy,France, September.
Association for ComputationalLinguistics.Anja Belz.
2005.
Statistical generation: Three meth-ods compared and evaluated.
In Proc.
of the 10thEuropean Workshop on Natural Language Genera-tion, pages 15?23.Anja Belz.
2008.
Automatic generation ofweather forecast texts using comprehensive proba-bilistic generation-space models.
Nat.
Lang.
Eng.,14(4):431?455, October.Bernd Bohnet, Leo Wanner, Simon Milles, and Ali-cia Burga.
2010.
Broad coverage multilingual deepsentence generation with a stochastic multi-level re-alizer.
In Proc.
of the 23rd International Conferenceon Computational Linguistics, Beijing, China.Bernd Bohnet, Simon Mille, Beno?
?t Favre, and LeoWanner.
2011.
<stumaba >: From deep repre-sentation to surface.
In Proc.
of the GenerationChallenges Session at the 13th European Workshopon Natural Language Generation, pages 232?235,Nancy, France, September.Bernd Bohnet, Anders Bjo?rkelund, Jonas Kuhn, Wolf-gang Seeker, and Sina Zarriess.
2012.
Generatingnon-projective word order in statistical linearization.In Proc.
of the 2012 Joint Conference on EmpiricalMethods in Natural Language Processing and Com-putational Natural Language Learning, pages 928?939, Jeju Island, Korea, July.Bernd Bohnet.
2010.
Top accuracy and fast de-pendency parsing is not a contradiction.
In Proc.of the 23rd International Conference on Computa-tional Linguistics, pages 89?97, Beijing, China, Au-gust.Sabine Brants, Stefanie Dipper, Silvia Hansen, Wolf-gang Lezius, and George Smith.
2002.
The TIGERTreebank.
In Proc.
of the Workshop on Treebanksand Linguistic Theories.Aoife Cahill and Arndt Riester.
2009.
Incorporat-ing Information Status into Generation Ranking.
InProc.
of the 47th Annual Meeting of the ACL, pages817?825, Suntec, Singapore, August.Lynne Cahill, Christy Doran, Roger Evans, Chris Mel-lish, Daniel Paiva, Mike Reape, Donia Scott, andNeil Tipper.
1999.
In search of a reference architec-ture for nlg systems.
In Proc.
of the European Work-shop on Natural Language Generation (EWNLG),pages 77?85.David L. Chen, Joohyun Kim, and Raymond J.Mooney.
2010.
Training a multilingual sportscaster:Using perceptual context to learn language.
Journalof Artificial Intelligence Research, 37:397?435.Robert Dale and Ehud Reiter.
1995.
Computationalinterpretations of the gricean maxims in the gener-ation of referring expressions.
Cognitive Science,19(2):233?263.Robert Dale.
1989.
Cooking up referring expressions.In Proc.
of the 27th Annual Meeting of the Associ-ation for Computational Linguistics, pages 68?75,Vancouver, British Columbia, Canada, June.Laurence Danlos.
1984.
Conceptual and linguistic de-cisions in generation.
In Proc.
of the 10th Interna-tional Conference on Computational Linguistics and22nd Annual Meeting of the Association for Compu-tational Linguistics, pages 501?504, Stanford, Cali-fornia, USA, July.Koenraad De Smedt, Helmut Horacek, and MichaelZock.
1996.
Architectures for natural language gen-eration: Problems and perspectives.
In Trends InNatural Language Generation: An Artifical Intelli-gence Perspective, pages 17?46.
Springer-Verlag.Katja Filippova and Michael Strube.
2007.
Generatingconstituent order in german clauses.
In Proc.
of the45th Annual Meeting of the Association for Compu-tational Linguistics, Prague, Czech Republic.Matthew Gerber and Joyce Chai.
2010.
Beyond nom-bank: A study of implicit arguments for nominalpredicates.
In Proc.
of the 48th Annual Meetingof the Association for Computational Linguistics,pages 1583?1592, Uppsala, Sweden, July.Barbara J. Grosz, Aravind Joshi, and Scott Weinstein.1995.
Centering: A framework for modeling the lo-cal coherence of discourse.
Computational Linguis-tics, 21(2):203?225.1556Eduard H. Hovy.
1988.
Planning coherent multisen-tential text.
In Proc.
of the 26th Annual Meetingof the Association for Computational Linguistics,pages 163?169, Buffalo, New York, USA, June.Thorsten Joachims.
2006.
Training linear SVMsin linear time.
In Proc.
of the ACM Conferenceon Knowledge Discovery and Data Mining (KDD),pages 217?226.Irene Langkilde and Kevin Knight.
1998.
Gener-ation that exploits corpus-based statistical knowl-edge.
In Proc.
of the 36th Annual Meeting ofthe Association for Computational Linguistics and17th International Conference on ComputationalLinguistics, Volume 1, pages 704?710, Montreal,Quebec, Canada, August.
Association for Compu-tational Linguistics.Tomasz Marciniak and Michael Strube.
2005.
Be-yond the pipeline: discrete optimization in nlp.
InProc.
of the 9th Conference on Computational Nat-ural Language Learning, CONLL ?05, pages 136?143, Stroudsburg, PA, USA.Chris Mellish, Roger Evans, Lynne Cahill, Christy Do-ran, Daniel Paiva, Mike Reape, Donia Scott, andNeil Tipper.
2000.
A representation for complexand evolving data dependencies in generation.
InProc.
of the 6th Conference on Applied Natural Lan-guage Processing, pages 119?126, Seattle, Wash-ington, USA, April.Marie Meteer.
1991.
Bridging the generation gap be-tween text planning and linguistic realization.
InComputational Intelligence, volume 7 (4).Adwait Ratnaparkhi.
2000.
Trainable methods forsurface natural language generation.
In Proc.
ofthe 1st North American chapter of the Associationfor Computational Linguistics conference, NAACL2000, pages 194?201, Stroudsburg, PA, USA.Ehud Reiter and Robert Dale.
1997.
Building appliednatural language generation systems.
Nat.
Lang.Eng., 3(1):57?87, March.Ehud Reiter.
1994.
Has a Consensus NL Genera-tion Architecture Appeared, and is it Psycholinguis-tically Plausible?
pages 163?170.Eric K. Ringger, Michael Gamon, Robert C. Moore,David Rojas, Martine Smets, and Simon Corston-Oliver.
2004.
Linguistically Informed Statisti-cal Models of Constituent Structure for Ordering inSentence Realization.
In Proc.
of the 2004 Inter-national Conference on Computational Linguistics,Geneva, Switzerland.Jacques Robin.
1993.
A revision-based generation ar-chitecture for reporting facts in their historical con-text.
In New Concepts in Natural Language Gener-ation: Planning, Realization and Systems.
FrancesPinter, London and, pages 238?265.
Pinter Publish-ers.Michael Roth and Anette Frank.
2012.
Aligning predi-cate argument structures in monolingual comparabletexts: A new corpus for a new task.
In Proc.
of the1st Joint Conference on Lexical and ComputationalSemantics (*SEM), Montreal, Canada.Robert Rubinoff.
1992.
Integrating text planning andlinguistic choice by annotating linguistic structures.In Robert Dale, Eduard H. Hovy, Dietmar Ro?sner,and Oliviero Stock, editors, NLG, volume 587 ofLecture Notes in Computer Science, pages 45?56.Springer.Josef Ruppenhofer, Caroline Sporleder, RoserMorante, Collin Baker, and Martha Palmer.
2010.Semeval-2010 task 10: Linking events and theirparticipants in discourse.
In Proc.
of the 5thInternational Workshop on Semantic Evaluation,pages 45?50, Uppsala, Sweden, July.Wolfgang Seeker and Jonas Kuhn.
2012.
Making El-lipses Explicit in Dependency Conversion for a Ger-man Treebank.
In Proc.
of the 8th conference onInternational Language Resources and Evaluation,Istanbul, Turkey, May.Advaith Siddharthan and Ann Copestake.
2004.
Gen-erating referring expressions in open domains.
InProceedings of the 42nd Meeting of the Associationfor Computational Linguistics (ACL?04), Main Vol-ume, pages 407?414, Barcelona, Spain, July.Pontus Stenetorp, Sampo Pyysalo, Goran Topic?,Tomoko Ohta, Sophia Ananiadou, and Jun?ichi Tsu-jii.
2012. brat: a web-based tool for nlp-assisted textannotation.
In Proc.
of the Demonstrations at the13th Conference of the European Chapter of the As-sociation for Computational Linguistics, pages 102?107, Avignon, France, April.Leo Wanner, Simon Mille, and Bernd Bohnet.
2012.Towards a surface realization-oriented corpus anno-tation.
In Proc.
of the 7th International Natural Lan-guage Generation Conference, pages 22?30, Utica,IL, May.Leo Wanner.
1994.
Building another bridge over thegeneration gap.
In Proc.
of the 7th InternationalWorkshop on Natural Language Generation, INLG?94, pages 137?144, Stroudsburg, PA, USA.Sina Zarrie?, Aoife Cahill, and Jonas Kuhn.
2011.
Un-derspecifying and predicting voice for surface real-isation ranking.
In Proc.
of the 49th Annual Meet-ing of the Association for Computational Linguis-tics: Human Language Technologies, pages 1007?1017, Portland, Oregon, USA, June.Sina Zarrie?, Aoife Cahill, and Jonas Kuhn.
2012.To what extent does sentence-internal realisation re-flect discourse context?
a study on word order.
InProc.
of the 13th Conference of the European Chap-ter of the Association for Computational Linguistics,pages 767?776, Avignon, France, April.1557
