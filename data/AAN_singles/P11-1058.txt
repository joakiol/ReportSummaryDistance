Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 570?580,Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational LinguisticsWhich Noun Phrases Denote Which Concepts?Jayant KrishnamurthyCarnegie Mellon University5000 Forbes AvenuePittsburgh, PA 15213jayantk@cs.cmu.eduTom M. MitchellCarnegie Mellon University5000 Forbes AvenuePittsburgh, PA 15213tom.mitchell@cmu.eduAbstractResolving polysemy and synonymy is re-quired for high-quality information extraction.We present ConceptResolver, a component forthe Never-Ending Language Learner (NELL)(Carlson et al, 2010) that handles both phe-nomena by identifying the latent concepts thatnoun phrases refer to.
ConceptResolver per-forms both word sense induction and synonymresolution on relations extracted from text us-ing an ontology and a small amount of la-beled data.
Domain knowledge (the ontology)guides concept creation by defining a set ofpossible semantic types for concepts.
Wordsense induction is performed by inferring a setof semantic types for each noun phrase.
Syn-onym detection exploits redundant informa-tion to train several domain-specific synonymclassifiers in a semi-supervised fashion.
WhenConceptResolver is run on NELL?s knowledgebase, 87% of the word senses it creates cor-respond to real-world concepts, and 85% ofnoun phrases that it suggests refer to the sameconcept are indeed synonyms.1 IntroductionMany information extraction systems constructknowledge bases by extracting structured assertionsfrom free text (e.g., NELL (Carlson et al, 2010),TextRunner (Banko et al, 2007)).
A major limi-tation of many of these systems is that they fail todistinguish between noun phrases and the underly-ing concepts they refer to.
As a result, a polysemousphrase like ?apple?
will refer sometimes to the con-cept Apple Computer (the company), and other timesto the concept apple (the fruit).
Furthermore, twosynonymous noun phrases like ?apple?
and ?Apple?apple?
?apple computer?apple (the fruit)Apple ComputerFigure 1: An example mapping from noun phrases (left)to a set of underlying concepts (right).
Arrows indicatewhich noun phrases can refer to which concepts.
[eli lilly, lilly][kaspersky labs, kaspersky lab, kaspersky][careerbuilder, careerbuilder.com][l 3 communications, level 3 communications][cellular, u.s. cellular][jc penney, jc penny][nielsen media research, nielsen company][universal studios, universal music group, universal][amr corporation, amr][intel corp, intel corp., intel corporation, intel][emmitt smith, chris canty][albert pujols, pujols][carlos boozer, dennis martinez][jason hirsh, taylor buchholz][chris snyder, ryan roberts][j.p. losman, losman, jp losman][san francisco giants, francisco rodriguez][andruw jones, andruw][aaron heilman, bret boone][roberto clemente, clemente]Figure 2: A random sample of concepts created by Con-ceptResolver.
The first 10 concepts are from company,while the second 10 are from athlete.Computer?
can refer to the same underlying con-cept.
The result of ignoring this many-to-many map-ping between noun phrases and underlying concepts(see Figure 1) is confusion about the meaning of ex-tracted information.
To minimize such confusion, asystem must separately represent noun phrases, theunderlying concepts to which they can refer, and themany-to-many ?can refer to?
relation between them.The relations extracted by systems like NELL ac-tually apply to concepts, not to noun phrases.
Say570the system extracts the relation ceoOf(x1, x2) be-tween the noun phrases x1 and x2.
The correct in-terpretation of this extracted relation is that there ex-ist concepts c1 and c2 such that x1 can refer to c1,x2 can refer to c2, and ceoOf(c1, c2).
If the orig-inal relation were ceoOf(?steve?, ?apple?
), then c1would be Steve Jobs, and c2 would be Apple Com-puter.
A similar interpretation holds for one-placecategory predicates like person(x1).
We define con-cept discovery as the problem of (1) identifying con-cepts like c1 and c2 from extracted predicates likeceoOf(x1, x2) and (2) mapping noun phrases likex1, x2 to the concepts they can refer to.The main input to ConceptResolver is a set ofextracted category and relation instances over nounphrases, like person(x1) and ceoOf(x1, x2), pro-duced by running NELL.
Here, any individual nounphrase xi can be labeled with multiple categoriesand relations.
The output of ConceptResolver isa set of concepts, {c1, c2, ..., cn}, and a mappingfrom each noun phrase in the input to the set ofconcepts it can refer to.
Like many other systems(Miller, 1995; Yates and Etzioni, 2007; Lin and Pan-tel, 2002), ConceptResolver represents each outputconcept ci as a set of synonymous noun phrases,i.e., ci = {xi1, xi2, ..., xim}.
For example, Figure 2shows several concepts output by ConceptResolver;each concept clearly reveals which noun phrases canrefer to it.
Each concept also has a semantic type thatcorresponds to a category in ConceptResolver?s on-tology; for instance, the first 10 concepts in Figure 2belong to the category company.Previous approaches to concept discovery use lit-tle prior knowledge, clustering noun phrases basedon co-occurrence statistics (Pantel and Lin, 2002).In comparison, ConceptResolver uses a knowledge-rich approach.
In addition to the extracted relations,ConceptResolver takes as input two other sources ofinformation: an ontology, and a small number of la-beled synonyms.
The ontology contains a schemafor the relation and category predicates found inthe input instances, including properties of predi-cates like type restrictions on its domain and range.The category predicates are used to assign semantictypes to each concept, and the properties of relationpredicates are used to create evidence for synonymresolution.
The labeled synonyms are used as train-ing data during synonym resolution, where they are1.
Induce Word Sensesi.
Use extracted category instances to createone or more senses per noun phrase.ii.
Use argument type constraints to produce re-lation evidence for synonym resolution.2.
Cluster Synonymous SensesFor each category C defined in the ontology:i.
Train a semi-supervised classifier to predictsynonymy.ii.
Cluster word senses with semantic type Cusing classifier?s predictions.iii.
Output sense clusters as concepts with se-mantic type C.Figure 3: High-level outline of ConceptResolver?s algo-rithm.used to train a semi-supervised classifier.ConceptResolver discovers concepts using theprocess outlined in Figure 3.
It first performs wordsense induction, using the extracted category in-stances to create one or more unambiguous wordsenses for each noun phrase in the knowledge base.Each word sense is a copy of the original nounphrase paired with a semantic type (a category) thatrestricts the concepts it can refer to.
ConceptRe-solver then performs synonym resolution on theseword senses.
This step treats the senses of each se-mantic type independently, first training a synonymclassifier then clustering the senses based on theclassifier?s decisions.
The result of this process isclusters of synonymous word senses, which are out-put as concepts.
Concepts inherit the semantic typeof the word senses they contain.We evaluate ConceptResolver using a subset ofNELL?s knowledge base, presenting separate resultsfor the concepts of each semantic type.
The eval-uation shows that, on average, 87% of the wordsenses created by ConceptResolver correspond toreal-world concepts.
We additionally find that, onaverage, 85% of the noun phrases in each conceptrefer to the same real-world entity.2 Prior WorkPrevious work on concept discovery has focusedon the subproblems of word sense induction andsynonym resolution.
Word sense induction is typ-ically performed using unsupervised clustering.
Inthe SemEval word sense induction and disambigua-571tion task (Agirre and Soroa, 2007; Manandhar et al,2010), all of the submissions in 2007 created sensesby clustering the contexts each word occurs in, andthe 2010 event explicitly disallowed the use of exter-nal resources like ontologies.
Other systems clusterwords to find both word senses and concepts (Panteland Lin, 2002; Lin and Pantel, 2002).
ConceptRe-solver?s category-based approach is quite differentfrom these clustering approaches.
Snow et al (2006)describe a system which adds new word senses toWordNet.
However, Snow et al assume the exis-tence of an oracle which provides the senses of eachword.
In contrast, ConceptResolver automaticallydetermines the number of senses for each word.Synonym resolution on relations extracted fromweb text has been previously studied by Resolver(Yates and Etzioni, 2007), which finds synonyms inrelation triples extracted by TextRunner (Banko etal., 2007).
In contrast to our system, Resolver is un-supervised and does not have a schema for the re-lations.
Due to different inputs, ConceptResolverand Resolver are not precisely comparable.
How-ever, our evaluation shows that ConceptResolver hashigher synonym resolution precision than Resolver,which we attribute to our semi-supervised approachand the known relation schema.Synonym resolution also arises in record link-age (Winkler, 1999; Ravikumar and Cohen, 2004)and citation matching (Bhattacharya and Getoor,2007; Bhattacharya and Getoor, 2006; Poon andDomingos, 2007).
As with word sense induction,many approaches to these problems are unsuper-vised.
A problem with these algorithms is that theyrequire the authors to define domain-specific simi-larity heuristics to achieve good performance.
Othersynonym resolution work is fully supervised (Singlaand Domingos, 2006; McCallum and Wellner, 2004;Snow et al, 2007), training models using manuallyconstructed sets of synonyms.
These approaches uselarge amounts of labeled data, which can be difficultto create.
ConceptResolver?s approach lies betweenthese two extremes: we label a small number of syn-onyms (10 pairs), then use semi-supervised trainingto learn a similarity function.
We think our tech-nique is a good compromise, as it avoids much ofthe manual effort of the other approaches: tuning thesimilarity function in one case, and labeling a largeamount of data in the otherConceptResolver uses a novel algorithm for semi-supervised clustering which is conceptually similarto other work in the area.
Like other approaches(Basu et al, 2004; Xing et al, 2003; Klein et al,2002), we learn a similarity measure for clusteringbased on a set of must-link and cannot-link con-straints.
Unlike prior work, our algorithm exploitsmultiple views of the data to improve the similar-ity measure.
As far as we know, ConceptResolveris the first application of semi-supervised cluster-ing to relational data ?
where the items being clus-tered are connected by relations (Getoor and Diehl,2005).
Interestingly, the relational setting also pro-vides us with the independent views that are benefi-cial to semi-supervised training.Concept discovery is also related to coreferenceresolution (Ng, 2008; Poon and Domingos, 2008).The difference between the two problems is thatcoreference resolution finds noun phrases that referto the same concept within a specific document.
Wethink the concepts produced by a system like Con-ceptResolver could be used to improve coreferenceresolution by providing prior knowledge about nounphrases that can refer to the same concept.
Thisknowledge could be especially helpful for cross-document coreference resolution systems (Haghighiand Klein, 2010), which actually represent conceptsand track mentions of them across documents.3 Background: Never-Ending LanguageLearnerConceptResolver is designed as a component for theNever-Ending Language Learner (NELL) (Carlsonet al, 2010).
In this section, we provide some per-tinent background information about NELL that in-fluenced the design of ConceptResolver 1.NELL is an information extraction system thathas been running 24x7 for over a year, using coupledsemi-supervised learning to populate an ontologyfrom unstructured text found on the web.
The ontol-ogy defines two types of predicates: categories (e.g.,company and CEO) and relations (e.g., ceoOf-Company).
Categories are single-argument pred-icates, and relations are two-argument predicates.1More information about NELL, including browsable anddownloadable versions of its knowledge base, is available fromhttp://rtw.ml.cmu.edu.572NELL?s knowledge base contains both definitionsfor predicates and extracted instances of each pred-icate.
At present, NELL?s knowledge base definesapproximately 500 predicates and contains over halfa million extracted instances of these predicates withan accuracy of approximately 0.85.Relations between predicates are an importantcomponent of NELL?s ontology.
For ConceptRe-solver, the most important relations are domain andrange, which define argument types for each rela-tion predicate.
For example, the first argument ofceoOfCompany must be a CEO and the second ar-gument must be a company.
Argument type restric-tions inform ConceptResolver?s word sense induc-tion process (Section 4.1).Multiple sources of information are used to popu-late each predicate with high precision.
The systemruns four independent extractors for each predicate:the first uses web co-occurrence statistics, the sec-ond uses HTML structures on webpages, the thirduses the morphological structure of the noun phraseitself, and the fourth exploits empirical regularitieswithin the knowledge base.
These subcomponentsare described in more detail by Carlson et al (2010)and Wang and Cohen (2007).
NELL learns usinga bootstrapping process, iteratively re-training theseextractors using instances in the knowledge base,then adding some predictions of the learners to theknowledge base.
This iterative learning process canbe viewed as a discrete approximation to EM whichdoes not explicitly instantiate every latent variable.As in other information extraction systems, thecategory and relation instances extracted by NELLcontain polysemous and synonymous noun phrases.ConceptResolver was developed to reduce the im-pact of these phenomena.4 ConceptResolverThis section describes ConceptResolver, our newcomponent which creates concepts from NELL?s ex-tractions.
It uses a two-step procedure, first creatingone or more senses for each noun phrase, then clus-tering synonymous senses to create concepts.4.1 Word Sense InductionConceptResolver induces word senses using a sim-ple assumption about noun phrases and concepts.
Ifa noun phrase has multiple senses, the senses shouldbe distinguishable from context.
People can deter-mine the sense of an ambiguous word given just afew surrounding words (Kaplan, 1955).
We hypoth-esize that local context enables sense disambigua-tion by defining the semantic type of the ambiguousword.
ConceptResolver makes the simplifying as-sumption that all word senses can be distinguishedon the basis of semantic type.
As the category pred-icates in NELL?s ontology define a set of possiblesemantic types, this assumption is equivalent to theone-sense-per-category assumption: a noun phraserefers to at most one concept in each category ofNELL?s ontology.
For example, this means that anoun phrase can refer to a company and a fruit, butnot multiple companies.ConceptResolver uses the extracted category as-sertions to define word senses.
Each word sense isrepresented as a tuple containing a noun phrase anda category.
In synonym resolution, the category actslike a type constraint, and only senses with the samecategory type can be synonymous.
To create senses,the system interprets each extracted category predi-cate c(x) as evidence that category c contains a con-cept denoted by noun phrase x.
Because it assumesthat there is at most one such concept, Concept-Resolver creates one sense of x for each extractedcategory predicate.
As a concrete example, saythe input assertions contain company(?apple?)
andfruit(?apple?).
Sense induction creates two sensesfor ?apple?
: (?apple?, company) and (?apple?, fruit).The second step of sense induction produces ev-idence for synonym resolution by creating relationsbetween word senses.
These relations are createdfrom input relations and the ontology?s argumenttype constraints.
Each extracted relation is mappedto all possible sense relations that satisfy the ar-gument type constraints.
For example, the nounphrase relation ceoOfCompany(?steve jobs?, ?ap-ple?)
would map to ceoOfCompany((?steve jobs?,ceo), (?apple?, company)).
It would not map to asimilar relation with (?apple?, fruit), however, as(?apple?, fruit) is not in the range of ceoOfCom-pany.
This process is effective because the relationsin the ontology have restrictive domains and ranges,so only a small fraction of sense pairs satisfy the ar-gument type restrictions.
It is also not vital that thismapping be perfect, as the sense relations are only573used as evidence for synonym resolution.
The finaloutput of sense induction is a sense-disambiguatedknowledge base, where each noun phrase has beenconverted into one or more word senses, and rela-tions hold between pairs of senses.4.2 Synonym ResolutionAfter mapping each noun phrase to one or moresenses (each with a distinct category type), Con-ceptResolver performs semi-supervised clusteringto find synonymous senses.
As only senses withthe same category type can be synonymous, oursynonym resolution algorithm treats senses of eachtype independently.
For each category, ConceptRe-solver trains a semi-supervised synonym classifierthen uses its predictions to cluster word senses.Our key insight is that semantic relations andstring attributes provide independent views of thedata: we can predict that two noun phrases are syn-onymous either based on the similarity of their textstrings, or based on similarity in the relations NELLhas extracted about them.
As a concrete example,we can decide that (?apple computer?, company)and (?apple?, company) are synonymous becausethe text string ?apple?
is similar to ?apple computer,?or because we have learned that (?steve jobs?, ceo)is the CEO of both companies.
ConceptResolver ex-ploits these two independent views using co-training(Blum and Mitchell, 1998) to produce an accuratesynonym classifier using only a handful of labels.4.2.1 Co-Training the Synonym ClassifierFor each category, ConceptResolver co-trains a pairof synonym classifiers using a handful of labeledsynonymous senses and a large number of automat-ically created unlabeled sense pairs.
Co-training isa semi-supervised learning algorithm for data setswhere each instance can be classified from two (ormore) independent sets of features.
That is, the fea-tures of each instance xi can be partitioned into twoviews, xi = (x1i , x2i ), and there exist functions ineach view, f1, f2, such that f1(x1i ) = f2(x2i ) = yi.The co-training algorithm uses a bootstrapping pro-cedure to train f1, f2 using a small set of labeled ex-amples L and a large pool of unlabeled examples U .The training process repeatedly trains each classifieron the labeled examples, then allows each classifierto label some examples in the unlabeled data pool.Co-training also has PAC-style theoretical guaran-tees which show that it can learn classifiers with ar-bitrarily high accuracy under appropriate conditions(Blum and Mitchell, 1998).Figure 4 provides high-level pseudocode for co-training in the context of ConceptResolver.
In Con-ceptResolver, an instance xi is a pair of senses (e.g.,<(?apple?, company), (?microsoft?, company)>),the two views x1i and x2i are derived from stringattributes and semantic relations, and the output yiis whether the senses are synonyms.
(The featuresof each view are described later in this section.)
Lis initialized with a small number of labeled sensepairs.
Ideally, U would contain all pairs of sensesin the category, but this set grows quadratically incategory size.
Therefore, ConceptResolver uses thecanopies algorithm (McCallum et al, 2000) to ini-tialize U with a subset of the sense pairs that aremore likely to be synonymous.Both the string similarity classifier and the rela-tion classifier are trained using L2-regularized lo-gistic regression.
The regularization parameter ?
isautomatically selected on each iteration by search-ing for a value which maximizes the loglikelihoodof a validation set, which is constructed by ran-domly sampling 25% of L on each iteration.
?
isre-selected on each iteration because the initial la-beled data set is extremely small, so the initial vali-dation set is not necessarily representative of the ac-tual data.
In our experiments, the initial validationset contains only 15 instances.The string similarity classifier bases its decisionon the original noun phrase which mapped to eachsense.
We use several string similarity measures asfeatures, including SoftTFIDF (Cohen et al, 2003),Level 2 JaroWinkler (Cohen et al, 2003), Fellegi-Sunter (Fellegi and Sunter, 1969), and Monge-Elkan(Monge and Elkan, 1996).
The first three algorithmsproduce similarity scores by matching words in thetwo phrases and the fourth is an edit distance.
Wealso use a heuristic abbreviation detection algorithm(Schwartz and Hearst, 2003) and convert its outputinto a score by dividing the length of the detectedabbreviation by the total length of the string.The relation classifier?s features capture severalintuitive ways to determine that two items are syn-onyms from the items they are related to.
The re-lation view contains three features for each relation574For each category C:1.
Initialize labeled data L with 10 positive and 50negative examples (pairs of senses)2.
Initialize unlabeled data U by running canopies(McCallum et al, 2000) on all senses in C.3.
Repeat 50 times:i.
Train the string similarity classifier on Lii.
Train the relation classifier on Liii.
Label U with each classifieriv.
Add the most confident 5 positive and 25negative predictions of both classifiers to LFigure 4: The co-training algorithm for learning synonymclassifiers.r whose domain is compatible with the current cat-egory.
Consider the sense pair (s, t), and let r(s)denote s?s values for relation r (i.e., r(s) = {v :r(s, v)}).
For each relation r, we instantiate the fol-lowing features:?
(Senses which share values are synonyms)The percent of values of r shared by both s and t,that is |r(s)?r(t)||r(s)?r(t)| .?
(Senses with different values are not synonyms)The percent of values of r not shared by s and t, or1 ?
|r(s)?r(t)||r(s)?r(t)| .
The feature is set to 0 if either r(s)or r(t) is empty.
This feature is only instantiated ifthe ontology specifies that r has at most one valueper sense.?
(Some relations indicate synonymy) A booleanfeature which is true if t ?
r(s) or s ?
r(t).The output of co-training is a pair of classifiers foreach category.
We combine their predictions usingthe assumption that the two views X1, X2 are con-ditionally independent given Y .
As we trained bothclassifiers using logistic regression, we have modelsfor the probabilities P (Y |X1) and P (Y |X2).
Theconditional independence assumption implies thatwe can combine their predictions using the formula:P (Y = 1|X1, X2) =P (Y = 1|X1)P (Y = 1|X2)P (Y = 0)?y=0,1 P (Y = y|X1)P (Y = y|X2)(1?
P (Y = y))The above formula involves a prior term, P (Y ),because the underlying classifiers are discrimina-tive.
We set P (Y = 1) = .5 in our experi-ments as this setting reduces our dependence on the(typically poorly calibrated) probability estimates oflogistic regression.
We also limited the probabil-ity predictions of each classifier to lie in [.01, .99]to avoid divide-by-zero errors.
The probabilityP (Y |X1, X2) is the final synonym classifier whichis used for agglomerative clustering.4.2.2 Agglomerative ClusteringThe second step of our algorithm runs agglomera-tive clustering to enforce transitivity constraints onthe predictions of the co-trained synonym classifier.As noted in previous works (Snow et al, 2006), syn-onymy is a transitive relation.
If a and b are syn-onyms, and b and c are synonyms, then a and c mustalso be synonyms.
Unfortunately, co-training is notguaranteed to learn a function that satisfies thesetransitivity constraints.
We enforce the constraintsby running agglomerative clustering, as clusteringsof instances trivially satisfy the transitivity property.ConceptResolver uses the clustering algorithmdescribed by Snow et al (2006), which defines aprobabilistic model for clustering and a procedure to(locally) maximize the likelihood of the final cluster-ing.
The algorithm is essentially bottom-up agglom-erative clustering of word senses using a similarityscore derived from P (Y |X1, X2).
The similarityscore for two senses is defined as:logP (Y = 0)P (Y = 1|X1, X2)P (Y = 1)P (Y = 0|X1, X2)The similarity score for two clusters is the sum ofthe similarity scores for all pairs of senses.
The ag-glomerative clustering algorithm iteratively mergesthe two most similar clusters, stopping when thescore of the best possible pair is below 0.
The clus-ters of word senses produced by this process are theconcepts for each category.5 EvaluationWe perform several experiments to measure Con-ceptResolver?s performance at each of its respectivetasks.
The first experiment evaluates word sense in-duction using Freebase as a canonical set of con-cepts.
The second experiment evaluates synonymresolution by comparing ConceptResolver?s senseclusters to a gold standard clustering.For both experiments, we used a knowledge basecreated by running 140 iterations of NELL.
We pre-processed this knowledge base by removing all noun575phrases with zero extracted relations.
As Concept-Resolver treats the instances of each category pred-icate independently, we chose 7 categories fromNELL?s ontology to use in the evaluation.
The cat-egories were selected on the basis of the number ofextracted relations that ConceptResolver could useto detect synonyms.
The number of noun phrasesin each category is shown in Table 2.
We manuallylabeled 10 pairs of synonymous senses for each ofthese categories.
The system automatically synthe-sized 50 negative examples from the positive exam-ples by assuming each pair represents a distinct con-cept, so senses in different pairs are not synonyms.5.1 Word Sense Induction EvaluationOur first experiment evaluates the performance ofConceptResolver?s category-based word sense in-duction.
We estimate two quantities: (1) sense pre-cision, the fraction of senses created by our systemthat correspond to real-world entities, and (2) senserecall, the fraction of real-world entities that Con-ceptResolver creates senses for.
Sense recall is onlymeasured over entities which are represented by anoun phrase in ConceptResolver?s input assertions ?it is a measure of ConceptResolver?s ability to cre-ate senses for the noun phrases it is given.
Senseprecision is directly determined by how frequentlyNELL?s extractors propose correct senses for nounphrases, while sense recall is related to the correct-ness of the one-sense-per-category assumption.Precision and recall were evaluated by comparingthe senses created by ConceptResolver to conceptsin Freebase (Bollacker et al, 2008).
We sampled100 noun phrases from each category and matchedeach noun phrase to a set of Freebase concepts.
Weinterpret each matching Freebase concept as a senseof the noun phrase.
We chose Freebase because ithad good coverage for our evaluation categories.To align ConceptResolver?s senses with Freebase,we first matched each of our categories with a set ofsimilar Freebase categories2.
We then used a com-bination of Freebase?s search API and MechanicalTurk to align noun phrases with Freebase concepts:we searched for the noun phrase in Freebase, thenhad Mechanical Turk workers label which of the2In Freebase, concepts are called Topics and categories arecalled Types.
For clarity, we use our terminology throughout.FreebaseCategory Precision Recall conceptsper Phraseathlete 0.95 0.56 1.76city 0.97 0.25 3.86coach 0.86 0.94 1.06company 0.85 0.41 2.41country 0.74 0.56 1.77sportsteam 0.89 0.30 3.28stadiumoreventvenue 0.83 0.61 1.63Table 1: ConceptResolver?s word sense induction perfor-manceFigure 5: Empirical distribution of the number of Free-base concepts per noun phrase in each categorytop 10 resulting Freebase concepts the noun phrasecould refer to.
After obtaining the list of matchingFreebase concepts for each noun phrase, we com-puted sense precision as the number of noun phrasesmatching ?
1 Freebase concept divided by 100, thetotal number of noun phrases.
Sense recall is the re-ciprocal of the average number of Freebase conceptsper noun phrase.
Noun phrases matching 0 Freebaseconcepts were not included in this computation.The results of the evaluation in Table 1 showthat ConceptResolver?s word sense induction worksquite well for many categories.
Most categories havehigh precision, while recall varies by category.
Cat-egories like coach are relatively unambiguous, withalmost exactly 1 sense per noun phrase.
Other cate-gories have almost 4 senses per noun phrase.
How-ever, this average is somewhat misleading.
Figure5 shows the distribution of the number of conceptsper noun phrase in each category.
The distributionshows that most noun phrases are unambiguous, buta small number of noun phrases have a large num-ber of senses.
In many cases, these noun phrases576are generic terms for many items in the category; forexample, ?palace?
in stadiumoreventvenue refersto 10 Freebase concepts.
Freebase?s category def-initions are also overly technical in some cases ?for example, Freebase?s version of company has aconcept for each registered corporation.
This defi-nition means that some companies like Volkswagenhave more than one concept (in this case, 9 con-cepts).
These results suggest that the one-sense-per-category assumption holds for most noun phrases.An important footnote to this evaluation is that thecategories in NELL?s ontology are somewhat arbi-trary, and that creating subcategories would improvesense recall.
For example, we could define subcat-egories of sportsteam for various sports (e.g., foot-ball team); these new categories would allow Con-ceptResolver to distinguish between teams with thesame name that play different sports.
Creating sub-categories could improve performance in categorieswith a high level of polysemy.5.2 Synonym Resolution EvaluationOur second experiment evaluates synonym resolu-tion by comparing the concepts created by Concept-Resolver to a gold standard set of concepts.
Al-though this experiment is mainly designed to eval-uate ConceptResolver?s ability to detect synonyms,it is somewhat affected by the word sense induc-tion process.
Specifically, the gold standard cluster-ing contains noun phrases that refer to multiple con-cepts within the same category.
(It is unclear howto create a gold standard clustering without allowingsuch mappings.)
The word sense induction processproduces only one of these mappings, which limitsmaximum possible recall in this experiment.For this experiment, we report two different mea-sures of clustering performance.
The first measureis the precision and recall of pairwise synonym de-cisions, typically known as cluster precision and re-call.
We dub this the clustering metric.
We alsoadopt the precision/recall measure from Resolver(Yates and Etzioni, 2007), which we dub the Re-solver metric.
The Resolver metric aligns each pro-posed cluster containing ?
2 senses with a goldstandard cluster (i.e., a real-world concept) by se-lecting the cluster that a plurality of the senses in theproposed cluster refer to.
Precision is then the frac-tion of senses in the proposed cluster which are alsoin the gold standard cluster; recall is computed anal-ogously by swapping the roles of the proposed andgold standard clusters.
Resolver precision can be in-terpreted as the probability that a randomly sampledsense (in a cluster with at least 2 senses) is in a clus-ter representing its true meaning.
Incorrect senseswere removed from the data set before evaluatingprecision; however, these senses may still affect per-formance by influencing the clustering process.Precision was evaluated by sampling 100 randomconcepts proposed by ConceptResolver, then manu-ally scoring each concept using both of the metricsabove.
This process mimics aligning each sampledconcept with its best possible match in a gold stan-dard clustering, then measuring precision with re-spect to the gold standard.Recall was evaluated by comparing the system?soutput to a manually constructed set of concepts foreach category.
To create this set, we randomly sam-pled noun phrases from each category and manuallymatched each noun phrase to one or more real-worldentities.
We then found other noun phrases which re-ferred to each entity and created a concept for eachentity with at least one unambiguous reference.
Thisprocess can create multiple senses for a noun phrase,depending on the real-world entities represented inthe input assertions.
We only included concepts con-taining at least 2 senses in the test set, as singletonconcepts do not contribute to either recall metric.The size of each recall test set is listed in Table 2;we created smaller test sets for categories where syn-onyms were harder to find.
Incorrectly categorizednoun phrases were not included in the gold standardas they do not correspond to any real-world entities.Table 2 shows the performance of ConceptRe-solver on each evaluation category.
For each cat-egory, we also report the baseline recall achievedby placing each sense in its own cluster.
Concept-Resolver has high precision for several of the cate-gories.
Other categories like athlete and city havesomewhat lower precision.
To make this differenceconcrete, Figure 2 (first page) shows a random sam-ple of 10 concepts from both company and athlete.Recall varies even more widely across categories,partly because the categories have varying levels ofpolysemy, and partly due to differences in averageconcept size.
The differences in average conceptsize are reflected in the baseline recall numbers.577Resolver Metric Clustering MetricCategory# of RecallPrecision Recall F1BaselinePrecision Recall F1BaselinePhrases Set Size Recall Recallathlete 3886 80 0.69 0.69 0.69 0.46 0.41 0.45 0.43 0.00city 5710 50 0.66 0.52 0.58 0.42 0.30 0.10 0.15 0.00coach 889 60 0.90 0.93 0.91 0.43 0.83 0.88 0.85 0.00company 3553 60 0.93 0.71 0.81 0.39 0.79 0.44 0.57 0.00country 693 60 0.98 0.50 0.66 0.30 0.94 0.15 0.26 0.00sportsteam 2085 100 0.95 0.48 0.64 0.29 0.87 0.15 0.26 0.00stadiumoreventvenue 1662 100 0.84 0.73 0.78 0.39 0.65 0.49 0.56 0.00Table 2: Synonym resolution performance of ConceptResolverWe attribute the differences in precision acrosscategories to the different relations available foreach category.
For example, none of the relations forathlete uniquely identify a single athlete, and there-fore synonymy cannot be accurately represented inthe relation view.
Adding more relations to NELL?sontology may improve performance in these cases.We note that the synonym resolution portion ofConceptResolver is tuned for precision, and that per-fect recall is not necessarily attainable.
Many wordsenses participate in only one relation, which maynot provide enough evidence to detect synonymy.As NELL continually extracts more knowledge, itis reasonable for ConceptResolver to abstain fromthese decisions until more evidence is available.6 DiscussionIn order for information extraction systems to ac-curately represent knowledge, they must representnoun phrases, concepts, and the many-to-many map-ping from noun phrases to concepts they denote.
Wepresent ConceptResolver, a system which takes ex-tracted relations between noun phrases and identifieslatent concepts that the noun phrases refer to.
Twolessons from ConceptResolver are that (1) ontolo-gies aid word sense induction, as the senses of pol-ysemous words tend to have distinct semantic types,and (2) redundant information, in the form of stringsimilarity and extracted relations, helps train accu-rate synonym classifiers.An interesting aspect of ConceptResolver is thatits performance should improve as NELL?s ontol-ogy and knowledge base grow in size.
Definingfiner-grained categories will improve performanceat word sense induction, as more precise categorieswill contain fewer ambiguous noun phrases.
Bothextracting more relation instances and adding newrelations to the ontology will improve synonym res-olution.
These scaling properties allow manual ef-fort to be spent on high-level ontology operations,not on labeling individual instances.
We are inter-ested in observing ConceptResolver?s performanceas NELL?s ontology and knowledge base grow.For simplicity of exposition, we have implicitlyassumed thus far that the categories in NELL?s on-tology are mutually exclusive.
However, the ontol-ogy contains compatible categories like male andpolitician, where a single concept can belong toboth categories.
In these situations, the one-sense-per-category assumption may create too many wordsenses.
We currently address this problem with aheuristic post-processing step: we merge all pairs ofconcepts that belong to compatible categories andshare at least one referring noun phrase.
This heuris-tic typically works well, however there are prob-lems.
An example of a problematic case is ?obama,?which NELL believes is a male, female, and politi-cian.
In this case, the heuristic cannot decide which?obama?
(the male or female) is the politician.
Assuch cases are fairly rare, we have not developed amore sophisticated solution to this problem.ConceptResolver has been integrated into NELL?scontinual learning process.
NELL?s current set ofconcepts can be viewed through the knowledge basebrowser on NELL?s website, http://rtw.ml.cmu.edu.AcknowledgmentsThis work is supported in part by DARPA (undercontract numbers FA8750-08-1-0009 and AF8750-09-C-0179) and by Google.
We also gratefully ac-knowledge the contributions of our colleagues on theNELL project, Jamie Callan for the ClueWeb09 webcrawl and Yahoo!
for use of their M45 computingcluster.
Finally, we thank the anonymous reviewersfor their helpful comments.578ReferencesEneko Agirre and Aitor Soroa.
2007.
Semeval-2007 task02: Evaluating word sense induction and discrimina-tion systems.
In Proceedings of the 4th InternationalWorkshop on Semantic Evaluations, pages 7?12.Michele Banko, Michael J. Cafarella, Stephen Soderland,Matt Broadhead, and Oren Etzioni.
2007.
Open infor-mation extraction from the web.
In Proceedings of theTwentieth International Joint Conference on ArtificialIntelligence, pages 2670?2676.Sugato Basu, Mikhail Bilenko, and Raymond J. Mooney.2004.
A probabilistic framework for semi-supervisedclustering.
In Proceedings of the Tenth ACM SIGKDDInternational Conference on Knowledge Discoveryand Data Mining, pages 59?68.Indrajit Bhattacharya and Lise Getoor.
2006.
A latentdirichlet model for unsupervised entity resolution.
InProceedings of the 2006 SIAM International Confer-ence on Data Mining, pages 47?58.Indrajit Bhattacharya and Lise Getoor.
2007.
Collectiveentity resolution in relational data.
ACM Transactionson Knowledge Discovery from Data, 1(1).Avrim Blum and Tom Mitchell.
1998.
Combining la-beled and unlabeled data with co-training.
In Proceed-ings of the Eleventh Annual Conference on Computa-tional Learning Theory, pages 92?100.Kurt Bollacker, Colin Evans, Praveen Paritosh, TimSturge, and Jamie Taylor.
2008.
Freebase: a col-laboratively created graph database for structuring hu-man knowledge.
In Proceedings of the 2008 ACMSIGMOD International Conference on Management ofData, pages 1247?1250.Andrew Carlson, Justin Betteridge, Bryan Kisiel, BurrSettles, Estevam R. Hruschka Jr., and Tom M.Mitchell.
2010.
Toward an architecture for never-ending language learning.
In Proceedings of theTwenty-Fourth AAAI Conference on Artificial Intelli-gence.William W. Cohen, Pradeep Ravikumar, and Stephen E.Fienberg.
2003.
A Comparison of String DistanceMetrics for Name-Matching Tasks.
In Proceedingsof the IJCAI-03 Workshop on Information Integration,pages 73?78, August.Ivan P. Fellegi and Alan B. Sunter.
1969.
A theory forrecord linkage.
Journal of the American Statistical As-sociation, 64:1183?1210.Lise Getoor and Christopher P. Diehl.
2005.
Link min-ing: a survey.
SIGKDD Explorations Newsletter, 7:3?12.Aria Haghighi and Dan Klein.
2010.
Coreference res-olution in a modular, entity-centered model.
In Pro-ceedings of the 2010 Annual Conference of the NorthAmerican Chapter of the Association for Computa-tional Linguistics, pages 385?393, June.Abraham Kaplan.
1955.
An experimental study of ambi-guity and context.
Mechanical Translation, 2:39?46.Dan Klein, Sepandar D. Kamvar, and Christopher D.Manning.
2002.
From instance-level constraintsto space-level constraints: Making the most of priorknowledge in data clustering.
In Proceedings ofthe Nineteenth International Conference on MachineLearning, pages 307?314.Dekang Lin and Patrick Pantel.
2002.
Concept discoveryfrom text.
In Proceedings of the 19th InternationalConference on Computational linguistics - Volume 1,pages 1?7.Suresh Manandhar, Ioannis P. Klapaftis, Dmitriy Dli-gach, and Sameer S. Pradhan.
2010.
Semeval-2010task 14: Word sense induction & disambiguation.
InProceedings of the 5th International Workshop on Se-mantic Evaluation, pages 63?68.Andrew McCallum and Ben Wellner.
2004.
Conditionalmodels of identity uncertainty with application to nouncoreference.
In Advances in Neural Information Pro-cessing Systems 18.Andrew McCallum, Kamal Nigam, and Lyle H. Un-gar.
2000.
Efficient clustering of high-dimensionaldata sets with application to reference matching.
InProceedings of the Sixth ACM SIGKDD InternationalConference on Knowledge Discovery and Data Min-ing, pages 169?178.George A. Miller.
1995.
Wordnet: A lexical database forenglish.
Communications of the ACM, 38:39?41.Alvaro Monge and Charles Elkan.
1996.
The fieldmatching problem: Algorithms and applications.
InProceedings of the Second International Conferenceon Knowledge Discovery and Data Mining, pages267?270.Vincent Ng.
2008.
Unsupervised models for coreferenceresolution.
In Proceedings of the 2008 Conference onEmpirical Methods in Natural Language Processing,pages 640?649.Patrick Pantel and Dekang Lin.
2002.
Discovering wordsenses from text.
In Proceedings of ACM SIGKDDConference on Knowledge Discovery and Data Min-ing, pages 613?619.Hoifung Poon and Pedro Domingos.
2007.
Joint infer-ence in information extraction.
In Proceedings of the22nd AAAI Conference on Artificial Intelligence - Vol-ume 1, pages 913?918.Hoifung Poon and Pedro Domingos.
2008.
Joint un-supervised coreference resolution with markov logic.In Proceedings of the 2008 Conference on EmpiricalMethods in Natural Language Processing, EMNLP?08, pages 650?659.579Pradeep Ravikumar and William W. Cohen.
2004.
A hi-erarchical graphical model for record linkage.
In Pro-ceedings of the 20th Conference on Uncertainty in Ar-tificial Intelligence, pages 454?461.Ariel S. Schwartz and Marti A. Hearst.
2003.
A sim-ple algorithm for identifying abbreviation definitionsin biomedical text.
In Proceedings of the Pacific Sym-posium on BIOCOMPUTING 2003, pages 451?462.Parag Singla and Pedro Domingos.
2006.
Entity reso-lution with markov logic.
In Proceedings of the SixthInternational Conference on Data Mining, pages 572?582.Rion Snow, Daniel Jurafsky, and Andrew Y. Ng.
2006.Semantic taxonomy induction from heterogenous evi-dence.
In Proceedings of the 21st International Con-ference on Computational Linguistics and the 44th An-nual Meeting of the Association for ComputationalLinguistics, pages 801?808, Morristown, NJ, USA.Rion Snow, Sushant Prakash, Daniel Jurafsky, and An-drew Y. Ng.
2007.
Learning to merge word senses.In Proceedings of the 2007 Joint Conference on Em-pirical Methods in Natural Language Processing andComputational Natural Language Learning, pages1005?1014, June.Richard C. Wang and William W. Cohen.
2007.Language-independent set expansion of named enti-ties using the web.
In Proceedings of the Seventh IEEEInternational Conference on Data Mining, pages 342?350.William E. Winkler.
1999.
The state of record linkageand current research problems.
Technical report, Sta-tistical Research Division, U.S. Census Bureau.Eric P. Xing, Andrew Y. Ng, Michael I. Jordan, and StuartRussell.
2003.
Distance metric learning, with applica-tion to clustering with side-information.
In Advancesin Neural Information Processing Systems 17, pages505?512.Alexander Yates and Oren Etzioni.
2007.
Unsupervisedresolution of objects and relations on the web.
In Pro-ceedings of the 2007 Annual Conference of the NorthAmerican Chapter of the Association for Computa-tional Linguistics.580
