KnowNet:A Proposal for Building HighlyConnected and DenseKnowledge Bases from the WebMontse CuadrosTALP Research Center, UPC, Barcelona (Spain)email: cuadros@lsi.upc.eduGerman RigauIXA NLP Group, UPV/EHU, Donostia (Spain)email: german.rigau@ehu.esAbstractThis paper presents a new fully automatic method for building highlydense and accurate knowledge bases from existing semantic resources.Basically, the method uses a wide-coverage and accurate knowledge-based Word Sense Disambiguation algorithm to assign the most appro-priate senses to large sets of topically related words acquired from theweb.
KnowNet, the resulting knowledge-base which connects large setsof semantically-related concepts is a major step towards the autonomousacquisition of knowledge from raw corpora.
In fact, KnowNet is severaltimes larger than any available knowledge resource encoding relationsbetween synsets, and the knowledge that KnowNet contains outperformany other resource when empirically evaluated in a common multilingualframework.7172 Cuadros and Rigau1 IntroductionUsing large-scale knowledge bases, such as WordNet (Fellbaum, 1998), has become ausual, often necessary, practice for most current Natural Language Processing (NLP)systems.
Even now, building large and rich enough knowledge bases for broad?coverage semantic processing takes a great deal of expensive manual effort involv-ing large research groups during long periods of development.
In fact, hundredsof person-years have been invested in the development of wordnets for various lan-guages (Vossen, 1998).
For example, in more than ten years of manual construction(from 1995 to 2006, that is from version 1.5 to 3.0), WordNet passed from 103,445 to235,402 semantic relations1.
But this data does not seems to be rich enough to supportadvanced concept-based NLP applications directly.
It seems that applications will notscale up to working in open domains without more detailed and rich general-purpose(and also domain-specific) semantic knowledge built by automatic means.
Obviously,this fact has severely hampered the state-of-the-art of advanced NLP applications.However, the Princeton WordNet is by far the most widely-used knowledge base(Fellbaum, 1998).
In fact, WordNet is being used world-wide for anchoring differ-ent types of semantic knowledge including wordnets for languages other than English(Atserias et al, 2004), domain knowledge (Magnini and Cavagli?, 2000) or ontolo-gies like SUMO (Niles and Pease, 2001) or the EuroWordNet Top Concept Ontology(?lvez et al, 2008).
It contains manually coded information about nouns, verbs, ad-jectives and adverbs in English and is organised around the notion of a synset.
Asynset is a set of words with the same part-of-speech that can be interchanged in a cer-tain context.
For example, <party, political_party> form a synset because they canbe used to refer to the same concept.
A synset is often further described by a gloss, inthis case: "an organisation to gain political power" and by explicit semantic relationsto other synsets.Fortunately, during the last years the research community has devised a large set ofinnovative methods and tools for large-scale automatic acquisition of lexical knowl-edge from structured and unstructured corpora.
Among others we can mention eX-tended WordNet (Mihalcea and Moldovan, 2001), large collections of semantic pref-erences acquired from SemCor (Agirre and Martinez, 2001, 2002) or acquired fromBritish National Corpus (BNC) (McCarthy, 2001), large-scale Topic Signatures foreach synset acquired from the web (Agirre and de la Calle, 2004) or knowledge aboutindividuals from Wikipedia (Suchanek et al, 2007).
Obviously, all these semantic re-sources have been acquired using a very different set of processes (Snow et al, 2006),tools and corpora.
In fact, each semantic resource has different volume and accuracyfigures when evaluated in a common and controlled framework (Cuadros and Rigau,2006).However, not all available large-scale resources encode semantic relations betweensynsets.
In some cases, only relations between synsets and words have been acquired.This is the case of the Topic Signatures (Agirre et al, 2000) acquired from the web(Agirre and de la Calle, 2004).
This is one of the largest semantic resources ever builtwith around one hundred million relations between synsets and semantically related1Symmetric relations are counted only once.KnowNet: A Proposal for Building Knowledge Bases from the Web 73words.2A knowledge net or KnowNet, is an extensible, large and accurate knowledgebase, which has been derived by semantically disambiguating the Topic Signaturesacquired from the web.
Basically, the method uses a robust and accurate knowledge-based Word Sense Disambiguation algorithm to assign the most appropriate sensesto the topic words associated to a particular synset.
The resulting knowledge-basewhich connects large sets of topically-related concepts is a major step towards the au-tonomous acquisition of knowledge from raw text.
In fact, KnowNet is several timeslarger than WordNet and the knowledge contained in KnowNet outperformsWordNetwhen empirically evaluated in a common framework.Table 1 compares the different volumes of semantic relations between synset pairsof available knowledge bases and the newly created KnowNets3.Table 1: Number of synset relationsSource #relationsPrinceton WN3.0 235,402Selectional Preferences from SemCor 203,546eXtended WN 550,922Co-occurring relations from SemCor 932,008New KnowNet-5 231,163New KnowNet-10 689,610New KnowNet-15 1,378,286New KnowNet-20 2,358,927Varying from five to twenty the number of processed words from each Topic Signa-ture, we created automatically four different KnowNets with millions of new semanticrelations between synsets.After this introduction, Section 2 describes the Topic Signatures acquired from theweb.
Section 3 presents the approach we plan to follow for building highly dense andaccurate knowledge bases.
Section 4 describes the methods we followed for buildingKnowNet.
In Section 5, we present the evaluation framework used in this study.
Sec-tion 6 describes the results when evaluating different versions of KnowNet and finally,Section 7 presents some concluding remarks and future work.2 Topic SignaturesTopic Signatures (TS) are word vectors related to a particular topic (Lin and Hovy,2000).
Topic Signatures are built by retrieving context words of a target topic fromlarge corpora.
In our case, we consider word senses as topics.
Basically, the acquisi-tion of TS consists of:?
acquiring the best possible corpus examples for a particular word sense (usuallycharacterising each word sense as a query and performing a search on the corpus2Available at http://ixa.si.ehu.es/Ixa/resources/sensecorpus3These KnowNet versions can be downloaded from http://adimen.si.ehu.es74 Cuadros and RigauTable 2: TS of party#n#1 (first 10 out of 12,890 total words)tammany#n 0.0319alinement#n 0.0316federalist#n 0.0315whig#n 0.0300missionary#j 0.0229Democratic#n 0.0218nazi#j 0.0202republican#n 0.0189constitutional#n 0.0186organization#n 0.0163for those examples that best match the queries)?
building the TS by deriving the context words that best represent the word sensefrom the selected corpora.The Topic Signatures acquired from the web (hereinafter TSWEB) constitutes oneof the largest available semantic resources with around 100 million relations (betweensynsets and words) (Agirre and de la Calle, 2004).
Inspired by the work of Leacocket al (1998), TSWEB was constructed using monosemous relatives from WN (syn-onyms, hypernyms, direct and indirect hyponyms, and siblings), querying Google andretrieving up to one thousand snippets per query (that is, a word sense), extractingthe salient words with distinctive frequency using TFIDF.
Thus, TSWEB consist of alarge ordered list of words with weights associated to each of the senses of the poly-semous nouns of WordNet 1.6.
The number of constructed topic signatures is 35,250with an average size per signature of 6,877 words.
When evaluating TSWEB, we usedat maximum the first 700 words while for building KnowNet we used at maximum thefirst 20 words.For example, Table 2 present the first words (lemmas and part-of-speech) andweights of the Topic Signature acquired for party#n#1.3 Building highly connected and dense knowledge basesIt is our belief, that accurate semantic processing (such as WSD) would rely notonly on sophisticated algorithms but on knowledge intensive approaches.
In fact,the cycling arquitecture of the MEANING4 project demonstrated that acquiring bet-ter knowledge allow to perform better Word Sense Disambiguation (WSD) and thathaving improvedWSD systems we are able to acquire better knowledge (Rigau et al,2002).Thus, we plan to acquire by fully automatic means highly connected and denseknowledge bases from large corpora or the web by using the knowledge already avail-able, increasing the total number of relations from less than one million (the currentnumber of available relations) to millions.4http://www.lsi.upc.edu/~nlp/meaningKnowNet: A Proposal for Building Knowledge Bases from the Web 75The current proposal consist of:?
to follow Cuadros et al (2005) and Cuadros and Rigau (2006) for acquiringhighly accurate Topic Signatures for all monosemous words in WordNet (forinstance, using InfoMap (Dorow and Widdows, 2003)).
That is, to acquireword vectors closely related to a particular monosemousword (for instance, air-port#n#1) from BNC or other large text collections like GigaWord, Wikipediaor the web.?
to apply a very accurate knowledge?based all?words disambiguation algorithmto the Topic Signatures in order to obtain sense vectors instead of word vectors(for instance, using a version of Structural Semantic Interconnections algorithm(SSI) (Navigli and Velardi, 2005)).For instance, consider the first ten weighted words (with Part-of-Speech) appear-ing in the Topic Signature (TS) of the word sense airport#n#1 corresponding to themonosemous word airport, as shown in Table 3.
This TS has been obtained fromBNC using InfoMap.
From the ten words appearing in the TS, two of them do notappear in WN (corresponding to the proper names heathrow#n and gatwick#n), fourwords are monosemous (airport#n, airfield#n, travelling#n and passenger#n) and fourother are polysemous (flight#n, train#n, station#n and ferry#n).Table 3: First ten words with weigths and number of senses in WN of the TopicSignature for airport#n#1 obtained from BNC using InfoMapword+pos weight #sensesairport#n 1.000000 1heathrow#n 0.843162 0gatwick#n 0.768215 0flight#n 0.765804 9airfield#n 0.740861 1train#n 0.739805 6travelling#n 0.732794 1passenger#n 0.722912 1station#n 0.722364 4ferry#n 0.717653 2SSI-DijkstraWe have implemented a version of the Structural Semantic Interconnections algorithm(SSI), a knowledge-based iterative approach to Word Sense Disambiguation (Navigliand Velardi, 2005).
The SSI algorithm is very simple and consists of an initialisationstep and a set of iterative steps.
GivenW, an ordered list of words to be disambiguated,the SSI algorithm performs as follows.
During the initialisation step, all monosemouswords are included into the set I of already interpreted words, and the polysemouswords are included in P (all of them pending to be disambiguated).
At each step, the76 Cuadros and RigauTable 4: Minimum distances from airport#n#1Synsets Distance4 64530 564713 429767 3597 220 11 0set I is used to disambiguate one word of P, selecting the word sense which is closerto the set I of already disambiguated words.
Once a sense is disambiguated, the wordsense is removed from P and included into I.
The algorithm finishes when no morepending words remain in P.Initially, the list I of interpretedwords should include the senses of the monosemouswords inW, or a fixed set of word senses5.
However, in this case, when disambiguatinga TS derived from a monosemous word m, the list I includes since the beginning atleast the sense of the monosemous word m (in our example, airport#n#1).In order to measure the proximity of one synset (of the word to be disambiguated ateach step) to a set of synsets (those word senses already interpreted in I), the originalSSI uses an in-house knowledge base derived semi-automatically which integrates avariety of online resources (Navigli, 2005).
This very rich knowledge-base is used tocalculate graph distances between synsets.
In order to avoid the exponential explosionof possibilities, not all paths are considered.
They used a context-free grammar ofrelations trained on SemCor to filter-out inappropriate paths and to provide weights tothe appropriate paths.Instead, we use part of the knowledge already available to build a very large con-nected graph with 99,635 nodes (synsets) and 636,077 edges (the set of direct relationsbetween synsets gathered from WordNet and eXtended WordNet).
On that graph, weused a very efficient graph library to compute the Dijkstra algorithm.6 The Dijkstraalgorithm is a greedy algorithm that computes the shortest path distance between onenode an the rest of nodes of a graph.
In that way, we can compute very efficientlythe shortest distance between any two given nodes of a graph.
This version of the SSIalgorithm is called SSI-Dijkstra.For instance, Table 4 shows the volumes of the minimumdistances from airport#n#1to the rest of the synsets of the graph.
Interestingly, from airport#n#1 all synsets ofthe graph are accessible following paths of at maximum six edges.
While there isonly one synset at distance zero (airport#n#1) and twenty synsets directly connectedto airport#n#1, 95% of the total graph is accessible at distance four or less.SSI-Dijkstra has very interesting properties.
For instance, SSI-Dijkstra always pro-5If no monosemous words are found or if no initial senses are provided, the algorithm could make aninitial guess based on the most probable sense of the less ambiguous word of W.6See http://www.boost.orgKnowNet: A Proposal for Building Knowledge Bases from the Web 77vides an answer when comparing the distances between the synsets of a word and allthe synsets already interpreted in I.
That is, the Dijkstra algorithm always providesan answer being the minimum distance close or far7.
At each step, the SSI-Dijkstraalgorithm selects the synset which is closer to I (the set of already interpreted words).Table 5 presents the result of the word?sense disambiguation process with the SSI-Dijkstra algorithm on the TS presented in Table 38.
Now, part of the TS obtainedfrom BNC using InfoMap have been disambiguated at a synset level resulting on aword?sense disambiguated TS.
Those words not present in WN1.6 have been ignored(heathrow and gatwick).
Some others, being monosemous in WordNet were consid-ered already disambiguated (travelling, passenger, airport and airfield).
But the rest,have been correctly disambiguated (flight with nine senses, train with six senses, sta-tion with four and ferry with two).Table 5: Sense disambiguated TS for airport#n#1 obtained from BNC using InfoMapand SSI-Dijkstra.Word Offset-WN Weight Glossflight#n 00195002n 0.017 a scheduled trip by plane between designatedairportstravelling#n 00191846n 0 the act of going from one place to anothertrain#n 03528724n 0.012 a line of railway cars coupled together and drawnby a locomotivepassenger#n 07460409n 0 a person travelling in a vehicle (a boat or bus orcar or plane or train etc) who is not operating itstation#n 03404271n 0.019 a building equipped with special equipment andpersonnel for a particular purposeairport#n 02175180n 0 an airfield equipped with control tower and hangersas well as accommodations for passengers and cargoferry#n 02671945n 0.010 a boat that transports people or vehicles across abody of water and operates on a regular scheduleairfield#n 02171984n 0 a place where planes take off and landThis sense disambiguated TS represents seven direct new semantic relations be-tween airport#n#1 and the first words of the TS.
It could be directly integrated into anew knowledge base (for instance, airport#n#1 ?related?> flight#n#9), but also all theindirect relations of the disambiguated TS (for instance, flight#n#9 ?related?> trav-elling#n#1).
In that way, having n disambiguated word senses, a total of (n2 ?
n)/2relations could be created.
That is, for the ten initial words of the TS of airport#n#1,twenty-eight new direct relations between synsets could be created.This process could be repeated for all monosemous words of WordNet appearingin the selected corpus.
The total number of monosemous words in WN1.6 is 98,953.Obviously, not all these monosemous words are expected to appear in the corpus.However, we expect to obtain in that way several millions of new semantic relationsbetween synsets.
This method will allow to derive by fully automatic means a hugeknowledge base with millions of new semantic relations.7In contrast, the original SSI algorithm not always provides a path distance because it depends on thegrammar.8It took 4.6 seconds to disambiguate the TS on a modern personal computer.78 Cuadros and RigauFurthermore, this approach is completely language independent.
It could be re-peated for any language having words connected to WordNet.It remains for further study and research, how to convert the relations created inthat way to more specific and informed relations.4 Building KnowNetAs a proof of concept, we developed KnowNet (KN), a large-scale and extensibleknowledge base obtained by applying the SSI-Dijkstra algorithm to each topic signa-ture from TSWEB.
That is, instead of using InfoMap and a large corpora for acquiringnew Topic Signatures for all the monosemous terms in WN, we used the already avail-able TSWEB.
We have generated four different versions of KonwNet applying SSI-Dijkstra to the first 5, 10, 15 and 20 words for each TS.
SSI-Dijkstra used only theknowledge present in WordNet and eXtended WordNet which consist of a very largeconnected graph with 99,635 nodes (synsets) and 636,077 edges (semantic relations).We generated each KnowNet by applying the SSI-Dijkstra algorithm to the wholeTSWEB (processing the first words of each of the 35,250 topic signatures).
For eachTS, we obtained the direct and indirect relations from the topic (a word sense) to thedisambiguated word senses of the TS.
Then, as explained in Section 3, we also gen-erated the indirect relations for each TS.
Finally, we removed symmetric and repeatedrelations.Table 6 shows the percentage of the overlapping between each KnowNet with re-spect the knowledge contained intoWordNet and eXtendedWordNet, the total numberof relations and synsets of each resource.
For instance, only an 8,6% of the total rela-tions included into WN+XWN are also present in KN-20.
This means that the rest ofrelations from KN-20 are new.
This table also shows the different KnowNet volumes.As expected, each KnowNet is very large, ranging from hundreds of thousands tomillions of new semantic relations between synsets among increasing sets of synsets.Surprisingly, the overlapping between the semantic relations of KnowNet and theknowledge bases used for building the SSI-Dijkstra graph (WordNet and eXtendedWordNet) is very small, possibly indicating disjunct types of knowledge.Table 6: Size and percentage of overlapping relations between KnowNet versions andWN+XWNKB WN+XWN #relations #synsetsKN-5 3.2% 231,164 39,837KN-10 5.4% 689,610 45,770KN-15 7.0% 1,378,286 48,461KN-20 8.6% 2,358,927 50,705Table 7 presents the percentage of overlapping relations between KnowNet ver-sions.
The upper triangular part of the matrix presents the overlapping percentagecovered by larger KnowNet versions.That is, most of the knowledge from KN-5 isalso contained in larger versions of KnowNet.
Interestingly, the knowledge containedinto KN-10 is only partially covered by KN-15 and KN-20.
The lower triangularKnowNet: A Proposal for Building Knowledge Bases from the Web 79part of the matrix presents the overlapping percentage covered by smaller KnowNetversions.Table 7: Percentage of overlapping relations between KnowNet versionsoverlapping KN-5 KN-10 KN-15 KN-20KN-5 100 93,3 97,7 97,2KN-10 31,2 100 88,5 88,9KN-15 16,4 44,4 100 97.14KN-20 9,5 26,0 56,7 1005 Evaluation frameworkIn order to empirically establish the relative quality of these KnowNet versions withrespect already available semantic resources, we used the noun-set of Senseval-3 En-glish Lexical Sample task which consists of 20 nouns.Trying to be as neutral as possible with respect to the resources studied, we appliedsystematically the same disambiguation method to all of them.
Recall that our maingoal is to establish a fair comparison of the knowledge resources rather than providingthe best disambiguation technique for a particular resource.
Thus, all the semantic re-sources studied are evaluated as Topic Signatures.
That is, word vectors with weightsassociated to a particular synset (topic) which are obtained by collecting those wordsenses appearing in the synsets directly related to the topics.A common WSD method has been applied to all knowledge resources.
A simpleword overlapping counting is performed between the Topic Signature and the testexample9.
The synset having higher overlapping word counts is selected.
In fact, thisis a very simple WSD method which only considers the topical information aroundthe word to be disambiguated.
All performances are evaluated on the test data usingthe fine-grained scoring system provided by the organisers.
Finally, we should remarkthat the results are not skewed (for instance, for resolving ties) by the most frequentsense in WN or any other statistically predicted knowledge.5.1 BaselinesWe have designed a number of baselines in order to establish a complete evaluationframework for comparing the performance of each semantic resource on the EnglishWSD task.RANDOM: For each target word, this method selects a random sense.
This base-line can be considered as a lower-bound.SEMCOR-MFS: This baseline selects the most frequent sense of the target wordin SemCor.WN-MFS: This baseline is obtained by selecting the most frequent sense (the firstsense in WN1.6) of the target word.
WordNet word-senses were ranked using SemCorand other sense-annotated corpora.
Thus, WN-MFS and SemCor-MFS are similar, butnot equal.9We also consider the multiword terms.80 Cuadros and RigauTRAIN-MFS: This baseline selects the most frequent sense in the training corpusof the target word.TRAIN: This baseline uses the training corpus to directly build a Topic Signatureusing TFIDF measure for each word sense.
Note that in WSD evaluation frameworks,this is a very basic baseline.
However, in our evaluation framework, this "WSD base-line" could be considered as an upper-bound.
We do not expect to obtain better topicsignatures for a particular sense than from its own annotated corpus.5.2 Large-scale Knowledge ResourcesIn order to measure the relative quality of the new resources, we include in the evalu-ation a wide range of large-scale knowledge resources connected to WordNet.WN (Fellbaum, 1998): This resource uses the different direct relations encoded inWN1.6 and WN2.0.
We also tested WN2 using relations at distance 1 and 2, WN3using relations at distances 1 to 3 and WN4 using relations at distances 1 to 4.XWN (Mihalcea and Moldovan, 2001): This resource uses the direct relations en-coded in eXtended WN.WN+XWN: This resource uses the direct relations included in WN and XWN.
Wealso tested (WN+XWN)2 (using either WN or XWN relations at distances 1 and 2).spBNC (McCarthy, 2001): This resource contains 707,618 selectional preferencesacquired for subjects and objects from BNC.spSemCor (Agirre and Martinez, 2002): This resource contains the selectionalpreferences acquired for subjects and objects from SemCor.MCR (Atserias et al, 2004): This resource uses the direct relations of WN, XWNand spSemCor (we excluded spBNC because of its poor performance).TSSEM (Cuadros et al, 2007): These Topic Signatures have been constructedusing the part of SemCor having all words tagged by PoS, lemmatized and sensetagged according to WN1.6 totalizing 192,639 words.
For each word-sense appearingin SemCor, we gather all sentences for that word sense, building a TS using TFIDFfor all word-senses co-occurring in those sentences.6 KnowNet EvaluationWe evaluated KnowNet using the framework of Section 5, that is, the noun part of thetest set from the Senseval-3 English lexical sample task.Table 8 presents ordered by F1 measure, the performance in terms of precision(P), recall (R) and F1 measure (F1, harmonic mean of recall and precision) of eachknowledge resource on Senseval-3 and its average size of the TS per word-sense.
Thedifferent KnowNet versions appear marked in bold and the baselines appear in italics.In this table, TRAIN has been calculated with a vector size of at maximum 450 words.As expected, RANDOM baseline obtains the poorest result.
The most frequent sensesobtained from SemCor (SEMCOR-MFS) and WN (WN-MFS) are both below themost frequent sense of the training corpus (TRAIN-MFS).
However, all of them arefar below to the Topic Signatures acquired using the training corpus (TRAIN).The best resources would be those obtaining better performances with a smallernumber of related words per synset.
The best results are obtained by TSSEM (withF1 of 52.4).
The lowest result is obtained by the knowledge directly gathered fromWN mainly because of its poor coverage (R of 18.4 and F1 of 26.1).
Interestingly,KnowNet: A Proposal for Building Knowledge Bases from the Web 81the knowledge integrated in the MCR although partly derived by automatic meansperforms much better in terms of precision, recall and F1 measures than using themseparately (F1 with 18.4 points higher than WN, 9.1 than XWN and 3.7 than spSem-Cor).Despite its small size, the resources derived from SemCor obtain better results thanits counterparts using much larger corpora (TSSEM vs. TSWEB and spSemCor vs.spBNC).Regarding the baselines, all knowledge resources surpass RANDOM, but noneachieves neither WN-MFS, TRAIN-MFS nor TRAIN.
Only TSSEM obtains betterresults than SEMCOR-MFS and is very close to the most frequent sense of WN (WN-MFS) and the training (TRAIN-MFS).The different versions of KnowNet consistently obtain better performances as theyincrease the window size of processed words of TSWEB.
As expected, KnowNet-5 obtain the lower results.
However, it performs better than WN (and all its ex-tensions) and spBNC.
Interestingly, from KnowNet-10, all KnowNet versions sur-pass the knowledge resources used for their construction (WN, XWN, TSWEB andWN+XWN).Furthermore, the integration of WN+XWN+KN?20 performs better than MCRand similarly to MCR2 (having less than 50 times its size).
It is also interesting to notethat WN+XWN+KN?20 has a better performance than their individual resources,indicating a complementary knowledge.
In fact, WN+XWN+KN?20 performs muchbetter than the resources from which it derives (WN, XWN and TSWEB).These initial results seem to be very promising.
If we do not consider the re-sources derived frommanually sense annotated data (spSemCor, MCR, TSSEM, etc.
),KnowNet-10 performs better that any knowledge resource derived by manual or au-tomatic means.
In fact, KnowNet-15 and KnowNet-20 outperforms spSemCor whichwas derived from manually annotated corpora.
This is a very interesting result sincethese KnowNet versions have been derived only with the knowledge coming fromWN and the web (that is, TSWEB), and WN and XWN as a knowledge source forSSI-Dijkstra (eXtended WordNet only has 17,185 manually labelled senses).7 Conclusions and future researchThe initial results obtained for the different versions of KnowNet seem to be verypromising, since they seem to be of a better quality than other available knowledgeresources encoding relations between synsets derived from non-annotated sense cor-pora.We tested all these resources and the different versions of KnowNet on SemEval-2007 English Lexical Sample Task (Cuadros and Rigau, 2008a).
When comparingthe ranking of the different knowledge resources, the different versions of KnowNetseem to be more robust and stable across corpora changes than the rest of resources.Furthermore, we also tested the performance of KnowNet when ported to Spanish (asthe Spanish WordNet is also integrated into the MCR).
Starting from KnowNet-10,all KnowNet versions perform better than any other knowledge resource on Spanishderived by manual or automatic means (including the MCR) (Cuadros and Rigau,2008b).82 Cuadros and RigauTable 8: P, R and F1 fine-grained results for the resources evaluated at Senseval-3,English Lexical Sample TaskKB P R F1 Av.
SizeTRAIN 65.1 65.1 65.1 450TRAIN-MFS 54.5 54.5 54.5WN-MFS 53.0 53.0 53.0TSSEM 52.5 52.4 52.4 103SEMCOR-MFS 49.0 49.1 49.0MCR2 45.1 45.1 45.1 26,429WN+XWN+KN-20 44.8 44.8 44.8 671MCR 45.3 43.7 44.5 129KnowNet-20 44.1 44.1 44.1 610KnowNet-15 43.9 43.9 43.9 339spSemCor 43.1 38.7 40.8 56KnowNet-10 40.1 40.0 40.0 154(WN+XWN)2 38.5 38.0 38.3 5,730WN+XWN 40.0 34.2 36.8 74TSWEB 36.1 35.9 36.0 1,721XWN 38.8 32.5 35.4 69KnowNet-5 35.0 35.0 35.0 44WN3 35.0 34.7 34.8 503WN4 33.2 33.1 33.2 2,346WN2 33.1 27.5 30.0 105spBNC 36.3 25.4 29.9 128WN 44.9 18.4 26.1 14RANDOM 19.1 19.1 19.1In sum, this is a preliminary step towards improved KnowNets we plan to obtainexploiting the Topic Signatures derived from monosemous words as explained in Sec-tion 3.AcknowledgmentsWe want to thank Aitor Soroa for his technical support and the anonymous reviewersfor their comments.
This work has been supported by KNOW (TIN2006-15049-C03-01) and KYOTO (ICT-2007-211423).ReferencesAgirre, E., O. Ansa, D. Martinez, and E. Hovy (2000).
Enriching very large ontologieswith topic signatures.
In Proceedings of ECAI?00 workshop on Ontology Learning,Berlin, Germany.Agirre, E. and O. L. de la Calle (2004).
Publicly available topic signatures for allwordnet nominal senses.
In Proceedings of LREC, Lisbon, Portugal.KnowNet: A Proposal for Building Knowledge Bases from the Web 83Agirre, E. and D. Martinez (2001).
Learning class-to-class selectional preferences.
InProceedings of CoNLL, Toulouse, France.Agirre, E. and D. Martinez (2002).
Integrating selectional preferences in wordnet.
InProceedings of GWC, Mysore, India.
?lvez, J., J. Atserias, J. Carrera, S. Climent, A. Oliver, and G. Rigau (2008).
Consis-tent annotation of eurowordnet with the top concept ontology.
In Proceedings ofFourth International WordNet Conference (GWC?08).Atserias, J., L. Villarejo, G. Rigau, E. Agirre, J. Carroll, B. Magnini, and P. Vossen(2004).
The meaning multilingual central repository.
In Proceedings of GWC,Brno, Czech Republic.Cuadros, M., L.
Padr?, and G. Rigau (2005).
Comparing methods for automatic ac-quisition of topic signatures.
In Proceedings of RANLP, Borovets, Bulgaria.Cuadros, M. and G. Rigau (2006).
Quality assessment of large scale knowledge re-sources.
In Proceedings of the EMNLP.Cuadros, M. and G. Rigau (2008a).
KnowNet: Building a Ln?arge Net of Knowledgefrom the Web.
In Proceedings of COLING.Cuadros, M. and G. Rigau (2008b).
Multilingual Evaluation of KnowNet.
In Pro-ceedings of SEPLN.Cuadros, M., G. Rigau, and M. Castillo (2007).
Evaluating large-scale knowledgeresources across languages.
In Proceedings of RANLP.Dorow, B. and D. Widdows (2003).
Discovering corpus-specific word senses.
InEACL, Budapest.Fellbaum, C. (1998).
WordNet.
An Electronic Lexical Database.
The MIT Press.Leacock, C., M. Chodorow, and G. Miller (1998).
Using Corpus Statistics and Word-Net Relations for Sense Identification.
Computational Linguistics 24(1), 147?166.Lin, C. and E. Hovy (2000).
The automated acquisition of topic signatures for textsummarization.
In Proceedings of COLING.
Strasbourg, France.Magnini, B. and G. Cavagli?
(2000).
Integrating subject field codes into wordnet.
InProceedings of LREC, Athens.
Greece.McCarthy, D. (2001).
Lexical Acquisition at the Syntax-Semantics Interface: Diathe-sis Aternations, Subcategorization Frames and Selectional Preferences.
Ph.
D.thesis, University of Sussex.Mihalcea, R. and D. Moldovan (2001).
extended wordnet: Progress report.
In Pro-ceedings of NAACL Workshop on WordNet and Other Lexical Resources, Pitts-burgh, PA.84 Cuadros and RigauNavigli, R. (2005).
Semi-automatic extension of large-scale linguistic knowledgebases.
In Proc.
of 18th FLAIRS International Conference (FLAIRS), ClearwaterBeach, Florida.Navigli, R. and P. Velardi (2005).
Structural semantic interconnections: a knowledge-based approach to word sense disambiguation.
IEEE Transactions on Pattern Anal-ysis and Machine Intelligence (PAMI) 27(7), 1063?1074.Niles, I. and A. Pease (2001).
Towards a standard upper ontology.
In C. Welty andB.
Smith (Eds.
), Proc.
of the 2nd International Conference on Formal Ontology inInformation Systems (FOIS-2001), pp.
17?19.Rigau, G., B. Magnini, E. Agirre, P. Vossen, and J. Carroll (2002).
Meaning: Aroadmap to knowledge technologies.
In Proceedings of COLING?2002 Workshopon A Roadmap for Computational Linguistics, Taipei, Taiwan.Snow, R., D. Jurafsky, and A. Y. Ng (2006).
Semantic taxonomy induction fromheterogenous evidence.
In Proceedings of COLING-ACL.Suchanek, F. M., G. Kasneci, and G. Weikum (2007).
Yago: A Core of SemanticKnowledge.
In 16th international World Wide Web conference (WWW 2007), NewYork, NY, USA.
ACM Press.Vossen, P. (1998).
EuroWordNet: A Multilingual Database with Lexical SemanticNetworks.
Kluwer Academic Publishers.
