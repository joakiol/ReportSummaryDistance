A Rapid Match Algorithm for ContinuousSpeech RecognitionLaurence S. Gillick and Robert RothDragon Systems, Inc.90 Bridge St.Newton MA.
02158AbstractThis paper describes an algorithm for performing rapidmatch on continuous speech that makes it possible to rec-ognize sentences from an 842 word vocabulary on a desk-top 33 megahertz 80486 computer in near eal time.
Thisalgorithm relies on a combination of smoothing and linearsegmentation together with the notion of word start groups.It appears that the total computation required grows moreslowly than linearly with the vocabulary size, so that largervocabularies appear feasible, with only moderately enhancedhardware.
The rapid match algorithm described here isclosely related to the one that is used in DragonDictate,Dragon's commercial 30,000 word discrete utterance recog-nizer.rapid match module to obtain a short list of plausible xten-sions.The key ideas that the algorithm relies on are linearsegmentation, smoothing, acoustic lustering, and word startgroupings.
In subsequent sections we shall elaborate onthese ideas and explain their role in rapid match.
We shallthen report on some empirical results, having to do with aparticular task that Dragon has chosen to use for develop-ment purposes: the dictation of mammography reports, us-ing a vocabulary of 842 words.Other apid match algorithms that are quite different incharacter have also been described in the literature \[2\], \[3\],and \[4\].1.
IntroductionFor it to be feasible to perform large vocabulary con-tinuous peech recognition in near real time on currentlyavailable, moderately priced hardware, computational com-promises appear to be essential.
One obvious compromise isto avoid detailed consideration f certain hypotheses that"cursory" inspection would reveal to be exceedingly un-likely.
But what constitutes cursory inspection?
To put itsomewhat differently, how can we perform a very quickcomputation that would allow us to throw out many of thehypotheses that are "obviously" false?
In particular, needdynamic programming play a role in such a "rapid match"algorithm?In this paper we shall describe astrategy for performingthis kind of computation i  the context of continuousspeech recognition.
The algorithm is an extension of therapid match procedure that is used in DragonDictate,Dragon's commercially available 30,000 word discrete utter-ance recognizer.
In that system, the interface between therapid match module and the recognizer is very straightfor-ward.
Each time the user says something, the rapid matchalgorithm provides the recognizer with a relatively short listof words that it thinks might have been said -- typically be-tween 100 and 200 words -- and in fact, this list is usuallysupplied to the recognizer before the speaker has finishedsaying the word.The nature of the interface between the rapid match mod-ule and the recognizer isdifferent when continuous speech isinvolved, because the recognizer must contemplate hypothe-ses that represent sequences of words, in the course of rec-ognizing an utterance.
In the system we describe \[1\], thefundamental act that the rapid match module has been de-signed to carry out is to provide a short list of words thatmight begin at a particular time based on looking at speechdata beginning at that ime and extending a fixed (and short)duration into the future.
Thus, whenever the recognizer isconsidering a partial sentence hypothesis that involves fin-ishing a word at a certain time and needs to know what wordhypotheses to consider as possible xtensions, it can call the2.
Description of the AlgorithmWe begin by describing the kind of data that the algo-rithm works with and then move on to describe the modelsto which the data are compared.
Finally, we describe the ac-tual way in which the computation is done.We suppose that an utterance isconverted by a front endto a sequence of k dimensional vectors, one per frame:Xl,X2 ..... Xn.
At any time (i.e.
frame) t the rapid matchmodule is capable of hypothesizing a short list of words thatmight begin at that time, based on looking at the vectors?t,Xt+l,...,Xt+w-1 , where w is the window size.
In ourcurrent implementation, a frame of 8 parameters is computedonce every 20 milliseconds, and the window size is 12; thusthe analysis i  based on 240 milliseconds of speech data.The algorithm begins with the computation of a se-quence of (k dimensional) smooth frames Yl,Y2,...,Ys ,based on the x's in the window.
Thus we haveb-1Y l=I  aixt+ii=Ob-1Y2=I  aixt+c+ii=Ob-1Y3=~,~,,fi=Oaixt+2c+ietc.where b is the smooth frame window width, the a's are thesmoothing weights (and are assumed to sum to 1), c is theoffset of successive smooth frame windows, and s is thenumber of smooth frames.
A little thought reveals that w =170b + (s - 1)c. At the present time the smoothing weights areall equal, there are three smooth frames, each smooth frameis computed from a window encompassing four regularframes, and successive smooth frame windows are offset by4 frames.
Hence the smooth frame windows are nonoverlap-ping.
In the DragonDictate isolated word rapid match wealso make use of linear segmentation a d smoothing.
Inthat sytem, 5 smooth frames are computed using overlap-ping windows with nonuniform weights.
We have not yetoptimized the choice of the smoothing algorithm for contin-uous speech.In this way we therefore convert 240 milliseconds ofspeech data into 3 smooth frames, or 24 numbers.
Thesmoothing that is done is intended on the one hand to pro-duce a very parsimonious representation f the speech datato make further processing very quick, and on the other, torepresent the data in a way that is not too sensitive to varia-tions in the duration of phonemes, thus obviating the needA word start grouping (WSG) consists of a collection ofwords whose beginnings are acoustically similar.
A wordmay appear in several different WSGs since, depending onthe context in which the word finds itself, its acoustic real-ization may vary considerably.
At the present ime, wehave at most 4 different word start groups for a given word,relating to whether or not the word emerges from silence orspeech, and whether or not the word ends in silence or inmore speech.
It may prove to be desirable to expand thenumber of different possible representations of a word be-yond this, to include prior phonetic ontext, but to do somight incre~e the necessary computation.
The generationof the word start groups is automatic and relies on a special-ized clustering algorithm.
Here are some typical groupsfrom the mammography vocabulary:A. medial, medially, mediasdnum, ediasfinal,mediolateral, needle, neededB.
severe, suggest, suggested, severely, suggests,suggestiveC.
dense, density, denser, densitiesEach word start group (WSG) also consists of a sequenceof acoustic models for smooth frames that might begenerated from that group.
More specifically a WSG is rep-resented by a sequence of r probability distributions in kdimensional space, where r is no greater than s (the numberof smooth frames computed): fl,f2,.--,fr.
For mostWSG's, we would have r = s, but for WSG's that are to rep-resent words that are so short that they may not last longenough for all s smooth frames to be computed, we allow r< s. For example, short function words like "of", "in","the", etc., when embedded in speech spoken continuously,often last less than 240 milliseconds.Currently, we assume that each probability density f isthe product of k univariate probability densities (i.e.
weassume that he k elements of each smooth frame y are inde-pendent).
Furthermore, we assume that each univariate den-sity is a double exponential distribution (Laplacian).
Thus,a single element of a smooth frame vector y is assumed tobe a random variable with a probability density of the formf(z) = exp(-Iz -glm)where g is the mean (or median) and a is the mean absolutedeviation (MAD).If we wish to assess the evidence for whether the currentsequence of smooth frames represents words in a particularWSG, we compute the scoreS = ~, .
, i  -log fi(yi)i=1which is the average negative log likelihood of the smoothframes evaluated using the probability model for the WSG.Let us suppose that there are M word start groups; then itwould be necessary to compute a score for each of these:S I ,S2, .
.
.
,SM.
A considerable computational saving canbe achieved by having a particlar probability density f appearas part of the model for multiple different WSG's.
Then, thevery same value of-log f can be added into multiple differentWSG scores.
Obtaining a representation f the probabilitydistributions of word start groups in terms of a small num-ber of probability densities f is again a job for a specializedclustering algorithm, one that clusters probability distribu-tions.Once we have computed the scores for each of theWSG's, we throw out all those groups with scores worsethan a threshold T1.
Then we look up all of the wordscontained in the surviving word start groups (throwing outany duplicates) and to each such word w, we attach the sumof its WSG score and a language model score:Sw =SwsG + SLMFinally we find all words w for whichSw -< T2where T2 is a second (combined) threshold.
At this pointwe have a list of words for the recognizer to contemplate inmore detail.
If the recognizer has asked us to return no morethan p words, where p happens to be less than the number ofsurvivors, we would prune the list further by throwing outthe worst scoring candidates.3.
Some Results on theMammography TaskThe recognition task that has been used during thedevelopment of our rapid match algorithm has been an 842word vocabulary drawn from mammography reports, manyof which are actually transcriptions oforal dictations of radi-ologists.
Since many of the words in the vocabulary can bepronounced in more than one way, we have addedl81 addi-tional prounciations; thus the effective vocabulary size is1023.
With this vocabulary size, we have found that run-ning our continuous peech recognizer with rapid matchspeeds up the system by a factor of 5 to 10, relative to run-ning without it.One obvious way of assessing the quality of the al-gorithm is simply to run the continuous speech recognizerwith and without rapid match, and observe how many errorsare introduced (or removed ) by virtue of its use.
On a test171set of 1000 sentences,spoken by one person, consisting of atotal of 8571 words, the word error rate was 3.7% runningwith rapid match returning a list of around 40 words perframe, and it was also 3.7% running without rapid match;in this particular experiment the total number of errors hap-pened to be exactly the same, although the actual errors weresomewhat different.
This strategy for assessing perfor-mance is rather global, however, and it proved to be usefulto have an assessment tool which provides more detail onwhere rapid match makes mistakes.By running the recognizer in a mode where it know sthe correct ranscription of each of the utterances, it ispossible to compute a reasonable segmentation f each ut-terance; that is to say, we can compute in which frame eachword in the transcription is most likely to begin.
Then,we can ask thisquestion for each word in each utterance:what is the rank of the score of the correct word among thecandidates returned by the rapid match module in the framein which the word begins (according to the segmenter)?
AtDragon, there is an interactive program that has been writtenfor the purpose of studying this question; by using it, theinvestigator finds it easy to detect words which have bad orinadequate rapid match models.
Its basic functionality is todisplay and record the words that rapid match passes throughto the recognizer in given frames.
Table 1 displays the per-centage of the time that the rapid match algorithm passesthrough the correct word in the correct frame as a function ofthe list size that the recognizer requests.
Beyond a fist size of40, there are diminishing returns.Size of list10Percentage passedthrough84%20 91%30 94%40 96%Table 1.
Percentage of words for which rapid match passes through the correct word in the frame inwhich the word begins, based on 700 tokensOne of the important features of the algorithm that theevaluation program highfights is that even if a word is notpassed through in the "ideal" frame (ideal from the HiddenMarkov Model's point of view), it is very likely to bepassed through in a nearby flame.
Because of the flexibilityof dynamic programming, the inflexibility of our linearsegmentation based rapid matcher does not prove to be asmuch of a disadvantage asone might have guessed.
Thisobservation is reinforced by the fact that even though 4% ofthe time the correct word does not appear in the rapid matchlist in the correct frame, running with the rapid match pro-cedure (in a sufficiently conservative mode) produces nodegradation i overall recognition performance, relative torunning without it.
In many cases where rapid match failsto pass back the word in the correct frame but does pass itthrough on a nearby frame, one sees upon inspecting aspectrogram of the utterance that the segmentation is hardto do by eye, and that the segmenter has made a reasonablechoice for itself but not for the rapid matcher.
In those casesthe recognizer very often gets the word correct.4.
ConclusionsThe rapid match procedure that we have described hereappears to be a very promising method.
On an approx-imately 1000 word continuous peech recognition task--atask that is not artificial, although another 1500 wordswould need to be added to give adequate coverage of realmammography reports in actual practice--it has enabled usto obtain near real time performance on a 33 megahertz80486 processor.We have done some preliminary work on adapting tonew speakers based on several hundred mammographysentences, and we are optimistic that our rapid match modelswill adapt successfully, as they have inside ofDragonDictate.
A future paper will discuss the general ques-tion of the training and adaptation of these models.REFERENCES\[1\] P. Bamberg, Y. Chow, L. Gillick, R. Roth, D.Sturtevant, "The Dragon Continuous Speech RecognitionSystem: A Real-Time Implementation", Proceedings ofDARPA Speech and Natural Language Workshop, June1990 Hidden Valley, Pennsylvania.\[2\] Lalit Bahl, Raimo Bakis, Peter V. de Souza and RobertL.
Mercer, "Obtaining Candidate Words by Polling in aLarge Vocabulary Speech Recognition System", ICASSP88, New York City, April 1988.\[3\] Xavier L. Aubert, "Fast Look-Ahead Pruning Strategiesin Continuous Speech Recognition", ICASSP 89, Glasgow,May 1989.\[4\] Lalit Bahl, P. S. Gopalakrishnan, D. Kanevsky, D.Nahamoo, "Matrix Fast Match: A Fast Method forIdentifying a Short List of Candidate Words for Decoding",ICASSP 89, Glasgow, May 1989.172
