Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 680?686,Baltimore, Maryland, USA, June 23-25 2014.c?2014 Association for Computational LinguisticsNormalizing tweets with edit scripts and recurrent neural embeddingsGrzegorz Chrupa?aTilburg Center for Cognition and CommunicationTilburg Universityg.chrupala@uvt.nlAbstractTweets often contain a large proportion ofabbreviations, alternative spellings, novelwords and other non-canonical language.These features are problematic for stan-dard language analysis tools and it canbe desirable to convert them to canoni-cal form.
We propose a novel text nor-malization model based on learning editoperations from labeled data while incor-porating features induced from unlabeleddata via character-level neural text embed-dings.
The text embeddings are generatedusing an Simple Recurrent Network.
Wefind that enriching the feature set with textembeddings substantially lowers word er-ror rates on an English tweet normaliza-tion dataset.
Our model improves on state-of-the-art with little training data and with-out any lexical resources.1 IntroductionA stream of posts from Twitter contains text writ-ten in a large variety of languages and writing sys-tems, in registers ranging from formal to inter-net slang.
Substantial effort has been expendedin recent years to adapt standard NLP process-ing pipelines to be able to deal with such con-tent.
One approach has been text normaliza-tion, i.e.
transforming tweet text into a morecanonical form which standard NLP tools ex-pect.
A multitude of resources and approacheshave been used to deal with normalization: hand-crafted and (semi-)automatically induced dictio-naries, language models, finite state transduc-ers, machine translation models and combinationsthereof.
Methods such as those of Han and Bald-win (2011), Liu et al (2011), Gouws et al (2011)or Han et al (2012) are unsupervised but theytypically use many adjustable parameters whichneed to be tuned on some annotated data.
In thiswork we suggest a simple, supervised character-level string transduction model which easily incor-porates features automatically learned from largeamounts of unlabeled data and needs only a lim-ited amount of labeled training data and no lexicalresources.Our model learns sequences of edit operationsfrom labeled data using a Conditional RandomField (Lafferty et al, 2001).
Unlabeled datais incorporated following recent work on usingcharacter-level text embeddings for text segmen-tation (Chrupa?a, 2013), and word and sentenceboundary detection (Evang et al, 2013).
Wetrain a recurrent neural network language model(Mikolov et al, 2010; Mikolov, 2012b) on a largecollection of tweets.
When run on new strings, theactivations of the units in the hidden layer at eachposition in the string are recorded and used as fea-tures for training the string transduction model.The principal contributions of our work are: (i)we show that a discriminative sequence labelingmodel is apt for text normalization and performsat state-of-the-art levels with small amounts of la-beled training data; (ii) we show that character-level neural text embeddings can be used to effec-tively incorporate information from unlabeled datainto the model and can substantially boost text nor-malization performance.2 MethodsMany approaches to text normalization adopt thenoisy channel setting, where the model normaliz-ing source string s into target canonical form t isfactored into two parts:?t = argmaxtP (t)P (s|t).The error term P (s|t) models how canonicalstrings are transformed into variants such as e.g.misspellings, emphatic lengthenings or abbrevia-tions.
The language model P (t) encodes whichtarget strings are probable.We think this decomposition is less appropriate680Input c w a tEdit DEL INS(see) NIL INS(h) NILOutput see w ha tTable 1: Example edit script.in the context of text normalization than in appli-cations from which it was borrowed such as Ma-chine Translations.
This is because it is not obvi-ous what kind of data can be used to estimate thelanguage model: there is plentiful text from thesource domain, but little of it is in normalized tar-get form.
There is also much edited text such asnews text, but it comes from a very different do-main.
One of the main advantages of the noisychannel decomposition is that is makes it easy toexploit large amounts of unlabeled data in the formof a language model.
This advantage does not holdfor text normalization.We thus propose an alternative approach wherenormalization is modeled directly, and which en-ables easy incorporation of unlabeled data fromthe source domain.2.1 Learning to transduce stringsOur string transduction model works by learningthe sequence of edits which transform the inputstring into the output string.
Given a pair of stringssuch a sequence of edits (known as the shortestedit script) can be found using the DIFF algorithm(Miller and Myers, 1985; Myers, 1986).
Our ver-sion of DIFF uses the following types of edits:?
NIL ?
no edits,?
DEL ?
delete character at this position,?
INS(?)
?
insert specified string before charac-ter at this position.1Table 1 shows a shortest edit script for the pairof strings (c wat, see what).We use a sequence labeling model to learn tolabel input strings with edit scripts.
The train-ing data for the model is generated by comput-ing shortest edit scripts for pairs of original andnormalized strings.
As a sequence labeler we useConditional Random Fields (Lafferty et al, 2001).Once trained the model is used to label new stringsand the predicted edit script is applied to the in-put string producing the normalized output string.Given source string s the predicted target string?t1The input string is extended with an empty symbol toaccount for the cases where an insertion is needed at the endof the string.is:?t = argmaxtP (ses(s, t)|s)where e = ses(s, t) is the shortest edit script map-ping s to t. P (e|s) is modeled with a linear-chainConditional Random Field.2.2 Character-level text embeddingsSimple Recurrent Networks (SRNs) were intro-duced by Elman (1990) as models of temporal, orsequential, structure in data, including linguisticdata (Elman, 1991).
More recently SRNs wereused as language models for speech recognitionand shown to outperform classical n-gram lan-guage models (Mikolov et al, 2010; Mikolov,2012b).
Another version of recurrent neural netshas been used to generate plausible text with acharacter-level language model (Sutskever et al,2011).
We use SRNs to induce character-level textrepresentations from unlabeled Twitter data to useas features in the string transduction model.The units in the hidden layer at time t receiveconnections from input units at time t and alsofrom the hidden units at the previous time stept ?
1.
The hidden layer predicts the state of theoutput units at the next time step t + 1.
The inputvector w(t) represents the input element at currenttime step, here the current character.
The outputvector y(t) represents the predicted probabilitiesfor the next character.
The activation sjof a hid-den unit j is a function of the current input and thestate of the hidden layer at the previous time step:t?
1:sj(t) = ?(I?i=1wi(t)Uji+L?l=1sj(t?
1)Wjl)where ?
is the sigmoid function and Ujiis theweight between input component i and hidden unitj, while Wjlis the weight between hidden unit lat time t ?
1 and hidden unit j at time t. Therepresentation of recent history is stored in a lim-ited number of recurrently connected hidden units.This forces the network to make the representationcompressed and abstract rather than just memo-rize literal history.
Chrupa?a (2013) and Evanget al (2013) show that these text embeddings canbe useful as features in textual segmentation tasks.We use them to bring in information from unla-beled data into our string transduction model andthen train a character-level SRN language modelon unlabeled tweets.
We run the trained model on681Figure 1: Tweets randomly generated with an SRNnew tweets and record the activation of the hid-den layer at each position as the model predicts thenext character.
These activation vectors form ourtext embeddings: they are discretized and used asinput features to the supervised sequence labeleras described in Section 3.4.3 Experimental SetupWe limit the size of the string alphabet by alwaysworking with UTF-8 encoded strings, and usingbytes rather than characters as basic units.3.1 Unlabeled tweetsIn order to train our SRN language model we col-lected a set of tweets using the Twitter samplingAPI.
We use the raw sample directly without fil-tering it in any way, relying on the SRN to learnthe structure of the data.
The sample consists of414 million bytes of UTF-8 encoded in a varietyof languages and scripts text.
We trained a 400-hidden-unit SRN, to predict the next byte in thesequence using backpropagation through time.
In-put bytes were encoded using one-hot representa-tion.
We modified the RNNLM toolkit (Mikolov,2012a) to record the activations of the hidden layerand ran it with the default learning rate schedule.Given that training SRNs on large amounts of texttakes a considerable amount of time we did notvary the size of the hidden layer.
We did try tofilter tweets by language and create specific em-beddings for English but this had negligible effecton tweet normalization performance.The trained SRN language model can be usedto generate random text by sampling the next bytefrom its predictive distribution and extending thestring with the result.
Figure 1 shows examplestrings generated in this way: the network seemsto prefer to output pseudo-tweets written consis-tently in a single script with words and pseudo-words mostly from a single language.
The gener-ated byte sequences are valid UTF-8 strings.In Table 2 in the first column we show the suf-fix of a string for which the SRN is predicting thelast byte.
The rest of each row shows the nearestneighbors of this string in embedding space, i.e.should h should d will s will m should a@justth @neenu @raven @lanae @despicmaybe u maybe y cause i wen i when iTable 2: Nearest neighbors in embedding space.strings for which the SRN is activated in a similarway when predicting its last byte as measured bycosine similarity.3.2 Normalization datasetsA difficulty in comparing approaches to tweet nor-malization is the sparsity of publicly availabledatasets.
Many authors evaluate on private tweetcollections and/or on the text message corpus ofChoudhury et al (2007).For English, Han and Baldwin (2011) createda small tweet dataset annotated with normalizedvariants at the word level.
It is hard to inter-pret the results from Han and Baldwin (2011),as the evaluation is carried out by assuming thatthe words to be normalized are known in ad-vance: Han et al (2012) remedy this shortcomingby evaluating a number of systems without pre-specifying ill-formed tokens.
Another limitationis that only word-level normalization is covered inthe annotation; e.g.
splitting or merging of wordsis not allowed.
The dataset is also rather small:549 tweets, which contain 2139 annotated out-of-vocabulary (OOV) words.
Nevertheless, weuse it here for training and evaluating our model.This dataset does not specify a development/testsplit.
In order to maximize the size of the trainingdata while avoiding tuning on test data we use asplit cross-validation setup: we generate 10 cross-validation folds, and use 5 of them during devel-opment to evaluate variants of our model.
The bestperforming configuration is then evaluated on theremaining 5 cross-validation folds.3.3 Model versionsThe simplest way to normalize tweets with a stringtransduction model is to treat whole tweets as in-put sequences.
Many other tweet normalizationmethods work in a word-wise fashion: they firstidentify OOV words and then replace them withnormalized forms.
Consequently, publicly avail-able normalization datasets are annotated at wordlevel.
We can emulate this setup by training the se-quence labeler on words, instead of whole tweets.This approach sacrifices some generality, sincetransformations involving multiple words cannot682be learned.
However, word-wise models are morecomparable with previous work.
We investigatedthe following models:?
OOV-ONLY is trained on individual words andin-vocabulary (IV) words are discarded fortraining, and left unchanged for prediction.2?
ALL-WORDS is trained on all words and al-lowed to change IV words.?
DOCUMENT is trained on whole tweets.Model OOV-ONLY exploits the setting when thetask is constrained to only normalize words absentfrom a reference dictionary, while DOCUMENT isthe one most generally applicable but does notbenefit from any constraints.
To keep model sizewithin manageable limits we reduced the label setfor models ALL-WORDS and DOCUMENT by re-placing labels which occur less than twice in thetraining data with NIL.
For OOV-ONLY we wereable to use the full label set.
As our sequence la-beling model we use the Wapiti implementationof Conditional Random Fields (Lavergne et al,2010) with the L-BFGS optimizer and elastic netregularization with default settings.3.4 FeaturesWe run experiments with two feature sets: N-GRAM and N-GRAM+SRN.
N-GRAM are char-acter n-grams of size 1?3 in a window of(?2,+2) around the current position.
For the N-GRAM+SRN feature set we augment N-GRAM withfeatures derived from the activations of the hiddenunits as the SRN is trying to predict the currentcharacter.
In order to use the activations in theCRF model we discretize them as follows.
Foreach of the K = 10 most active units out oftotal J = 400 hidden units, we create features(f(1) .
.
.
f(K)) defined as f(k) = 1 if sj(k)>0.5 and f(k) = 0 otherwise, where sj(k)returnsthe activation of the kthmost active unit.3.5 Evaluation metricsAs our evaluation metric we use word error rate(WER) which is defined as the Levenshtein editdistance between the predicted word sequence?tand the target word sequence t, normalized by thetotal number of words in the target string.
A moregenerally applicable metric would be character er-ror rate, but we report WERs to make our resultseasily comparable with previous work.
Since the2We used the IV/OOV annotations in the Han et al (2012)dataset, which are automatically derived from the aspell dic-tionary.Model Features WER (%)NO-OP 11.7DOCUMENT NGRAM 6.8DOCUMENT NGRAM+SRN 5.7ALL WORDS NGRAM 7.2ALL WORDS NGRAM+SRN 5.0OOV-ONLY NGRAM 5.1OOV-ONLY NGRAM+SRN 4.5Table 3: WERs on development data.9 cont continued 5 gon gonna4 bro brother 4 congrats congratulations3 yall you 3 pic picture2 wuz what?s 2 mins minutes2 juss just 2 fb facebookTable 4: Improvements from SRN features.English dataset is pre-tokenized and only coversword-to-word transformations, this choice has lit-tle importance here and character error rates showa similar pattern to word error rates.4 ResultsTable 3 shows the results of our development ex-periments.
NO-OP is a baseline which leaves textunchanged.
As expected the most constrainedmodel OOV-ONLY outperforms the more genericmodels on this dataset.
For all model variations,adding SRN features substantially improves per-formance: the relative error reductions range from12% for OOV-ONLY to 30% for ALL-WORDS.
Ta-ble 4 shows the non-unique normalizations madeby the OOV-ONLY model with SRN features whichwere missed without them.
SRN features seemto be especially useful for learning long-range,multi-character edits, e.g.
fb for facebook.Table 5 shows the non-unique normalizationswhich were missed by the best model: they area mixture of relatively standard variations whichhappen to be infrequent in our data, like tonite orgf, and a few idiosyncratic respellings like uu orbhee.
Our supervised approach makes it easy toaddress the first type of failure by simply annotat-ing additional training examples.Table 6 presents evaluation results of several ap-proaches reported in Han et al (2012) as well asthe model which did best in our development ex-periments.
HB-dict is the Internet slang dictio-nary from Han and Baldwin (2011).
GHM-dictis the automatically constructed dictionary from6834 1 one 2 withh with2 uu you 2 tonite tonight2 thx thanks 2 thiis this2 smh somehow 2 outta out2 n in 2 m am2 hmwrk homework 2 gf girlfriend2 fxckin fucking 2 dha the2 de the 2 d the2 bhee be 2 bb babyTable 5: Missed transformations.Method WER (%)NO-OP 11.2HB-dict 6.6GHM-dict 7.6S-dict 9.7Dict-combo 4.9Dict-combo+HB-norm 7.9OOV-ONLY NGRAM+SRN (test) 4.8Table 6: WERs compared to previous work.Gouws et al (2011); S-dict is the automaticallyconstructed dictionary from (Han et al, 2012);Dict-combo are all the dictionaries combined andDict-combo+HB-norm are all dictionaries com-bined with approach of Han and Baldwin (2011).The WER reported for OOV-ONLY NGRAM+SRNis on the test folds only.
The score on the fulldataset is a bit better: 4.66%.
As can be seen ourapproach it the best performing approach overalland in particular it does much better than all of thesingle dictionary-based methods.
Only the combi-nation of all the dictionaries comes close in per-formance.5 Related workIn the field of tweet normalization the approachof Liu et al (2011, 2012) shows some similaritiesto ours: they gather a collection of OOV wordstogether with their canonical forms from the weband train a character-level CRF sequence labeleron the edit sequences computed from these pairs.They use this as the error model in a noisy-channelsetup combined with a unigram language model.In addition to character n-gram features they usephoneme and syllable features, while we rely onthe SRN embeddings to provide generalized rep-resentations of input strings.Kaufmann and Kalita (2010) trained a phrase-based statistical translation model on a paralleltext message corpus and applied it to tweet nor-malization.
In comparison to our first-order linear-chain CRF, an MT model with reordering is moreflexible but for this reason needs more trainingdata.
It also suffers from language model mis-match mentioned in Section 2: optimal resultswere obtained by using a low weight for the lan-guage model trained on a balanced text corpus.Many other approaches to tweet normalizationare more unsupervised in nature (e.g.
Han andBaldwin, 2011; Gouws et al, 2011; Xue et al,2011; Han et al, 2012).
They still require an-notated development data for tuning parametersand a variety of heuristics.
Our approach workswell with similar-sized training data, and unlikeunsupervised approaches can easily benefit frommore if it becomes available.
Further afield,our work has connections to research on mor-phological analysis: for example Chrupa?a et al(2008) use edit scripts to learn lemmatization ruleswhile Dreyer et al (2008) propose a discrimina-tive model for string transductions and apply itto morphological tasks.
While Chrupa?a (2013)and Evang et al (2013) use character-level SRNtext embeddings for learning segmentation, andrecurrent nets themselves have been used for se-quence transduction (Graves, 2012), to our knowl-edge neural text embeddings have not been previ-ously applied to string transduction.6 ConclusionLearning sequences of edit operations from exam-ples while incorporating unlabeled data via neu-ral text embeddings constitutes a compelling ap-proach to tweet normalization.
Our results are es-pecially interesting considering that we trained ononly a small annotated data set and did not useany other manually created resources such as dic-tionaries.
We want to push performance furtherby expanding the training data and incorporatingexisting lexical resources.
It will also be impor-tant to check how our method generalizes to otherlanguage and datasets (e.g.
de Clercq et al, 2013;Alegria et al, 2013).The general form of our model can be usedin settings where normalization is not limited toword-to-word transformations.
We are planningto find or create data with such characteristics andevaluate our approach under these conditions.684ReferencesI?naki Alegria, Nora Aranberri, V?
?ctor Fresno, PabloGamallo, Lluis Padr?o, I?naki San Vicente, JordiTurmo, and Arkaitz Zubiaga.
2013.
Introducci?on a latarea compartida Tweet-Norm 2013: Normalizaci?onl?exica de tuits en espa?nol.
In Workshop on TweetNormalization at SEPLN (Tweet-Norm), pages 36?45.Monojit Choudhury, Rahul Saraf, Vijit Jain, AnimeshMukherjee, Sudeshna Sarkar, and Anupam Basu.2007.
Investigation and modeling of the struc-ture of texting language.
International Journal ofDocument Analysis and Recognition (IJDAR), 10(3-4):157?174.Grzegorz Chrupa?a.
2013.
Text segmentation withcharacter-level text embeddings.
In ICML Workshopon Deep Learning for Audio, Speech and LanguageProcessing.Grzegorz Chrupa?a, Georgiana Dinu, and JosefVan Genabith.
2008.
Learning morphology withMorfette.
In Proceedings of the 6th edition of theLanguage Resources and Evaluation Conference.Orph?ee de Clercq, Bart Desmet, Sarah Schulz, ElsLefever, and V?eronique Hoste.
2013.
Normaliza-tion of Dutch user-generated content.
In 9th Inter-national Conference on Recent Advances in NaturalLanguage Processing (RANLP-2013), pages 179?188.
INCOMA Ltd.Markus Dreyer, Jason R Smith, and Jason Eisner.2008.
Latent-variable modeling of string transduc-tions with finite-state methods.
In Proceedings ofthe conference on empirical methods in natural lan-guage processing, pages 1080?1089.
Associationfor Computational Linguistics.Jeffrey L Elman.
1990.
Finding structure in time.
Cog-nitive science, 14(2):179?211.Jeffrey L Elman.
1991.
Distributed representations,simple recurrent networks, and grammatical struc-ture.
Machine learning, 7(2-3):195?225.Kilian Evang, Valerio Basile, Grzegorz Chrupa?a, andJohan Bos.
2013.
Elephant: Sequence labelingfor word and sentence segmentation.
In EmpiricalMethods in Natural Language Processing.Stephan Gouws, Dirk Hovy, and Donald Metzler.
2011.Unsupervised mining of lexical variants from noisytext.
In Proceedings of the First workshop on Unsu-pervised Learning in NLP, pages 82?90.
Associationfor Computational Linguistics.Alex Graves.
2012.
Sequence transduction with recur-rent neural networks.
arXiv:1211.3711.Bo Han and Timothy Baldwin.
2011.
Lexical normali-sation of short text messages: Makn sens a# twitter.In Proceedings of the 49th Annual Meeting of theAssociation for Computational Linguistics: HumanLanguage Technologies-Volume 1, pages 368?378.Association for Computational Linguistics.Bo Han, Paul Cook, and Timothy Baldwin.
2012.
Au-tomatically constructing a normalisation dictionaryfor microblogs.
In Proceedings of the 2012 JointConference on Empirical Methods in Natural Lan-guage Processing and Computational Natural Lan-guage Learning, pages 421?432.
Association forComputational Linguistics.Max Kaufmann and Jugal Kalita.
2010.
Syntacticnormalization of Twitter messages.
In Interna-tional conference on natural language processing,Kharagpur, India.John D. Lafferty, Andrew McCallum, and FernandoC.
N. Pereira.
2001.
Conditional Random Fields:Probabilistic models for segmenting and labeling se-quence data.
In Proceedings of the Eighteenth Inter-national Conference on Machine Learning, ICML?01, pages 282?289.
Morgan Kaufmann PublishersInc., San Francisco, CA, USA.Thomas Lavergne, Olivier Capp?e, and Franc?ois Yvon.2010.
Practical very large scale CRFs.
In Proceed-ings of the 48th Annual Meeting of the Associationfor Computational Linguistics, pages 504?513.
As-sociation for Computational Linguistics.Fei Liu, Fuliang Weng, and Xiao Jiang.
2012.
A broad-coverage normalization system for social media lan-guage.
In Proceedings of the 50th Annual Meet-ing of the Association for Computational Linguis-tics: Long Papers-Volume 1, pages 1035?1044.
As-sociation for Computational Linguistics.Fei Liu, Fuliang Weng, Bingqing Wang, and Yang Liu.2011.
Insertion, deletion, or substitution?
: normaliz-ing text messages without pre-categorization nor su-pervision.
In Proceedings of the 49th Annual Meet-ing of the Association for Computational Linguis-tics: Human Language Technologies: short papers-Volume 2, pages 71?76.
Association for Computa-tional Linguistics.Tom?a?s Mikolov.
2012a.
Recurrent neural network lan-guage models.
http://rnnlm.org.Tom?a?s Mikolov.
2012b.
Statistical language modelsbased on neural networks.
Ph.D. thesis, Brno Uni-versity of Technology.Tom?a?s Mikolov, Martin Karafi?at, Luk?a?s Burget, Jan?Cernock?y, and Sanjeev Khudanpur.
2010.
Recur-rent neural network based language model.
In Inter-speech, pages 1045?1048.Webb Miller and Eugene W Myers.
1985.
A file com-parison program.
Software: Practice and Experi-ence, 15(11):1025?1040.Eugene W Myers.
1986.
An O(ND) difference algo-rithm and its variations.
Algorithmica, 1(1-4):251?266.685Ilya Sutskever, James Martens, and Geoffrey E Hin-ton.
2011.
Generating text with recurrent neuralnetworks.
In Proceedings of the 28th InternationalConference on Machine Learning (ICML-11), pages1017?1024.Zhenzhen Xue, Dawei Yin, and Brian D Davison.
2011.Normalizing microtext.
In Proceedings of the AAAI-11 Workshop on Analyzing Microtext, pages 74?79.686
