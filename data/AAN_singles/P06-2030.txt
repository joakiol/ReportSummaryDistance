Proceedings of the COLING/ACL 2006 Main Conference Poster Sessions, pages 231?238,Sydney, July 2006. c?2006 Association for Computational LinguisticsUsing Bilingual Comparable Corpora and Semi-supervised Clustering forTopic TrackingFumiyo FukumotoInterdisciplinary GraduateSchool of Medicine and EngineeringUniv.
of Yamanashifukumoto@yamanashi.ac.jpYoshimi SuzukiInterdisciplinary GraduateSchool of Medicine and EngineeringUniv.
of Yamanashiysuzuki@yamanashi.ac.jpAbstractWe address the problem dealing withskewed data, and propose a method forestimating effective training stories for thetopic tracking task.
For a small number oflabelled positive stories, we extract storypairs which consist of positive and its as-sociated stories from bilingual comparablecorpora.
To overcome the problem of alarge number of labelled negative stories,we classify them into some clusters.
Thisis done by using k-means with EM.
Theresults on the TDT corpora show the ef-fectiveness of the method.1 IntroductionWith the exponential growth of information on theInternet, it is becoming increasingly difficult tofind and organize relevant materials.
Topic Track-ing defined by the TDT project is a research areato attack the problem.
It starts from a few samplestories and finds all subsequent stories that discussthe target topic.
Here, a topic in the TDT con-text is something that happens at a specific placeand time associated with some specific actions.
Awide range of statistical and ML techniques havebeen applied to topic tracking(Carbonell et.
al,1999; Oard, 1999; Franz, 2001; Larkey, 2004).The main task of these techniques is to tune theparameters or the threshold to produce optimal re-sults.
However, parameter tuning is a tricky issuefor tracking(Yang, 2000) because the number ofinitial positive training stories is very small (oneto four), and topics are localized in space and time.For example, ?Taipei Mayoral Elections?
and ?U.S.Mid-term Elections?
are topics, but ?Elections?
isnot a topic.
Therefore, the system needs to esti-mate whether or not the test stories are the sametopic with few information about the topic.
More-over, the training data is skewed data, i.e.
thereis a large number of labelled negative stories com-pared to positive ones.
The system thus needs tobalance the amount of positive and negative train-ing stories not to hamper the accuracy of estima-tion.In this paper, we propose a method for esti-mating efficient training stories for topic track-ing.
For a small number of labelled positive sto-ries, we use bilingual comparable corpora (TDT1-3 English and Japanese newspapers, Mainichi andYomiuri Shimbun).
Our hypothesis using bilin-gual corpora is that many of the broadcasting sta-tion from one country report local events more fre-quently and in more detail than overseas?
broad-casting stations, even if it is a world-wide famousones.
Let us take a look at some topic fromthe TDT corpora.
A topic, ?Kobe Japan quake?from the TDT1 is a world-wide famous one, and89 stories are included in the TDT1.
However,Mainichi and Yomiuri Japanese newspapers havemuch more stories from the same period of time,i.e.
5,029 and 4,883 stories for each.
These obser-vations show that it is crucial to investigate the useof bilingual comparable corpora based on the NLtechniques in terms of collecting more informationabout some specific topics.
We extract Japanesestories which are relevant to the positive Englishstories using English-Japanese bilingual corpora,together with the EDR bilingual dictionary.
Theassociated story is the result of alignment of aJapanese term association with an English term as-sociation.For a large number of labelled negative sto-ries, we classify them into some clusters us-ing labelled positive stories.
We used a semi-supervised clustering technique which combines231labeled and unlabeled stories during clustering.Our goal for semi-supervised clustering is to clas-sify negative stories into clusters where each clus-ter is meaningful in terms of class distributionprovided by one cluster of positive training sto-ries.
We introduce k-means clustering that can beviewed as instances of the EM algorithm, and clas-sify negative stories into clusters.
In general, thenumber of clusters k for the k-means algorithm isnot given beforehand.
We thus use the BayesianInformation Criterion (BIC) as the splitting crite-rion, and select the proper number for k.2 Related WorkMost of the work which addresses the small num-ber of positive training stories applies statisticaltechniques based on word distribution and MLtechniques.
Allan et.
al explored on-line adaptivefiltering approaches based on the threshold strat-egy to tackle the problem(Allan et.
al, 1998).
Thebasic idea behind their work is that stories closertogether in the stream are more likely to discuss re-lated topics than stories further apart.
The methodis based on unsupervised learning techniques ex-cept for its incremental nature.
When a trackingquery is first created from the Nt training stories,it is also given a threshold.
During the trackingphase, if a story S scores over that threshold, Sis regarded to be relevant and the query is regen-erated as if S were among the Nt training sto-ries.
This method was tested using the TDT1 cor-pus and it was found that the adaptive approachis highly successful.
But adding more than fourtraining stories provided only little help, althoughin their approach, 12 training stories were added.The method proposed in this paper is similar toAllan?s method, however our method for collect-ing relevant stories is based on story pairs whichare extracted from bilingual comparable corpora.The methods for finding bilingual story pairsare well studied in the cross-language IR task,or MT systems/bilingual lexicons(Dagan, 1997).Much of the previous work uses cosine similar-ity between story term vectors with some weight-ing techniques(Allan et.
al, 1998) such as TF-IDF,or cross-language similarities of terms.
However,most of them rely on only two stories in questionto estimate whether or not they are about the sametopic.
We use multiple-links among stories toproduce optimal results.In the TDT tracking task, classifying negativestories into meaningful groups is also an im-portant issue to track topics, since a large num-ber of labelled negative stories are available inthe TDT context.
Basu et.
al.
proposed amethod using k-means clustering with the EM al-gorithm, where labeled data provides prior infor-mation about the conditional distribution of hid-den category labels(Basu, 2002).
They reportedthat the method outperformed the standard randomseeding and COP-k-means(Wagstaff, 2001).
Ourmethod shares the basic idea with Basu et.
al.
Animportant difference with their method is that ourmethod does not require the number of clusters kin advance, since it is determined during cluster-ing.
We use the BIC as the splitting criterion, andestimate the proper number for k. It is an impor-tant feature because in the tracking task, no knowl-edge of the number of topics in the negative train-ing stories is available.3 System DescriptionThe system consists of four procedures: extractingbilingual story pairs, extracting monolingual storypairs, clustering negative stories, and tracking.3.1 Extracting Bilingual Story PairsWe extract story pairs which consist of positiveEnglish story and its associated Japanese storiesusing the TDT English and Mainichi and Yomi-uri Japanese corpora.
To address the optimal pos-itive English and their associated Japanese stories,we combine the output of similarities(multiple-links).
The idea comes from speech recognitionwhere two outputs are combined to yield a betterresult in average.
Fig.1 illustrates multiple-links.The TDT English corpus consists of training andtest stories.
Training stories are further dividedinto positive(black box) and negative stories(dotedbox).
Arrows in Fig.1 refer to an edge with simi-larity value between stories.
In Fig.1, for example,whether the story J2discusses the target topic, andis related to E1or not is determined by not only thevalue of similarity between E1and J2, but also thesimilarities between J2and J4, E1and J4.Extracting story pairs is summarized as follows:Let initial positive training stories E1, ?
?
?, Em beinitial node, and each Japanese stories J1, ?
?
?, Jm?be node or terminal node in the graph G. We cal-culate cosine similarities between Ei(1 ?
i ?
m)and Jj(1 ?
j ?
m?)1.
In a similar way, we calcu-1m?
refers to the difference of dates between English and232training storiestest stories time linesTDT English corpusE1 E2 E3edge(E1,J1)edge(E1,J4)time linesMainichi and Yomiuri Japanese corpora topicJ1 J2 J3 J4 J5 J6 Jm?edge(J2,J4)not topicFigure 1: Multiple-links among storieslate similarities between Jk and Jl(1 ?
k, l ?
m?
).If the value of similarity between nodes is largerthan a certain threshold, we connect them by anedge(bold arrow in Fig.1).
Next, we delete an edgewhich is not a constituent of maximal connectedsub-graph(doted arrow in Fig.1).
After eliminat-ing edges, we extract pairs of initial positive En-glish story Ei and Japanese story Jj as a linkedstory pair, and add associated Japanese story Jjto the training stories.
In Fig.1, E1, J2, and J4are extracted.
The procedure for calculating co-sine similarities between Ei and Jj consists of twosub-steps: extracting terms, and estimating bilin-gual term correspondences.Extracting termsThe first step to calculate similarity betweenEi and Jj is to align a Japanese term with itsassociated English term using the bilingual dic-tionary, EDR.
However, this naive method suf-fers from frequent failure due to incompletenessof the bilingual dictionary.
Let us take a look atthe Mainichi Japanese newspaper stories.
The to-tal number of terms(words) from Oct. 1, 1998 toDec.
31, 1998, was 528,726.
Of these, 370,013terms are not included in the EDR bilingual dic-tionary.
For example, ??????
(Endeavour)?which is a key term for the topic ?Shuttle Endeav-our mission for space station?
from the TDT3 cor-pus is not included in the EDR bilingual dictio-nary.
New terms which fail to segment by dur-ing a morphological analysis are also a problem incalculating similarities between stories in mono-lingual data.
For example, a proper noun ????????
(Tokyo Metropolitan Univ.)
is divided intothree terms, ????
(Metropolitan), ???
(Univ.
)?,Japanese story pairs.Table 1: tE and tJ matrixtEtE ?
siE tE ?
siEtJtJ ?
S?iJ a btJ ?
S?iJ c dand ???
(Tokyo)?.
To tackle these problems, weconducted term extraction from a large collectionof English and Japanese corpora.
There are severaltechniques for term extraction(Chen, 1996).
Weused n-gram model with Church-Gale smoothing,since Chen reported that it outperforms all existingmethods on bigram models produced from largetraining data.
The length of the extracted termsdoes not have a fixed range2.
We thus applied thenormalization strategy which is shown in Eq.
(1)to each length of the terms to bring the probabil-ity value into the range [0,1].
We extracted termswhose probability value is greater than a certainthreshold.
Words from the TDT English(Japanesenewspaper) corpora are identified if they match theextracted terms.simnew =simold ?
simminsimmax ?
simmin(1)Bilingual term correspondencesThe second step to calculate similarity betweenEi and Jj is to estimate bilingual term correspon-dences using ?2 statistics.
We estimated bilingualterm correspondences with a large collection ofEnglish and Japanese data.
More precisely, let Eibe an English story (1 ?
i ?
n), where n is thenumber of stories in the collection, and SiJ denotethe set of Japanese stories with cosine similaritieshigher than a certain threshold value ?
: SiJ = {Jj| cos(Ei, Jj) ?
?}.
Then, we concatenate con-stituent Japanese stories of SiJ into one story S?iJ ,and construct a pseudo-parallel corpus PPCEJ ofEnglish and Japanese stories: PPCEJ = { { Ei,S?iJ } | SiJ = 0 }.
Suppose that there are two crite-ria, monolingual term tE in English story and tJ inJapanese story.
We can determine whether or not aparticular term belongs to a particular story.
Con-sequently, terms are divided into four classes, asshown in Table 1.
Based on the contingency tableof co-occurence frequencies of tE and tJ , we esti-mate bilingual term correspondences according tothe statistical measure ?2.
?2(tE, tJ ) =(ad ?
bc)2(a + b)(a + c)(b + d)(c + d)(2)2We set at most five noun words.233We extract term tJ as a pair of tE which satisfiesmaximum value of ?2, i.e.
maxtJ?TJ ?2(tE ,tJ ),where TJ = {tJ | ?2(tE ,tJ )}.
For the extracted En-glish and Japanese term pairs, we conducted semi-automatic acquisition, i.e.
we manually selectedbilingual term pairs, since our source data is nota clean parallel corpus, but an artificially gener-ated noisy pseudo-parallel corpus, it is difficult tocompile bilingual terms full-automatically(Dagan,1997).
Finally, we align a Japanese term with itsassociated English term using the selected bilin-gual term correspondences, and again calculatecosine similarities between Japanese and Englishstories.3.2 Extracting Monolingual Story PairsWe noted above that our source data is not a cleanparallel corpus.
Thus the difference of dates be-tween bilingual stories is one of the key factors toimprove the performance of extracting story pairs,i.e.
stories closer together in the timeline are morelikely to discuss related subjects.
We therefore ap-plied a method for extracting bilingual story pairsfrom stories closer in the timelines.
However, thisoften hampers our basic motivation for using bilin-gual corpora: bilingual corpora helps to collectmore information about the target topic.
We there-fore extracted monolingual(Japanese) story pairsand added them to the training stories.
Extract-ing Japanese monolingual story pairs is quite sim-ple: Let Jj (1 ?
j ?
m?)
be the extracted Japanesestory in the procedure, extracting bilingual storypairs.
We calculate cosine similarities between Jjand Jk(1 ?
k ?
n).
If the value of similarity be-tween them is larger than a certain threshold, weadd Jk to the training stories.3.3 Clustering Negative StoriesOur method for classifying negative stories intosome clusters is based on Basu et.
al.
?smethod(Basu, 2002) which uses k-means with theEM algorithm.
K-means is a clustering algo-rithm based on iterative relocation that partitionsa dataset into the number of k clusters, locallyminimizing the average squared distance betweenthe data points and the cluster centers(centroids).Suppose we classify X = { x1, ?
?
?, xN}, xi ?Rd into k clusters: one is the cluster which con-sists of positive stories, and other k-1 clustersconsist of negative stories.
Here, which clustersdoes each negative story belong to?
The EM isa method of finding the maximum-likelihood es-timate(MLE) of the parameters of an underlyingdistribution from a set of observed data that hasmissing value.
K-means is essentially an EM ona mixture of k Gaussians under certain assump-tions.
In the standard k-means without any initialsupervision, the k-means are chosen randomly inthe initial M-step and the stories are assigned tothe nearest means in the subsequent E-step.
Forpositive training stories, the initial labels are keptunchanged throughout the algorithm, whereas theconditional distribution for the negative stories arere-estimated at every E-step.
We select the num-ber of k initial stories: one is the cluster center ofpositive stories, and other k-1 stories are negativestories which have the top k-1 smallest value be-tween the negative story and the cluster center ofpositive stories.
In Basu et.
al?s method, the num-ber of k is given by a user.
However, for negativetraining stories, the number of clusters is not givenbeforehand.
We thus developed an algorithm forestimating k. It goes into action after each run ofk means3, making decisions about which sets ofclusters should be chosen in order to better fit thedata.
The splitting decision is done by comput-ing the Bayesian Information Criterion which isshown in Eq.
(3).BIC(k = l) = ?lll(X)?pl2?
log N (3)where l?ll(X) is the log-likelihood of X accordingto the number of k is l, N is the total number oftraining stories, and pl is the number of parame-ters in k = l. We set pl to the sum of k class prob-abilities,?km=1 l?l(Xm) , the number of n ?
k cen-troid coordinates, and the MLE for the variance,??2.
Here, n is the number of dimensions.
?
?2, un-der the identical spherical Gaussian assumption,is:?
?2 =1N ?
k?i(xi ?
?i)2 (4)where ?i denotes i-th partition center.
The proba-bilities are:?P (xi) =RiN?1?2???nexp(?12?
?2|| xi ?
?i ||2) (5)Ri is the number of stories that have ?i as theirclosest centroid.
The log-likelihood of ll(X)3We set the maximum number of k to 100 in the experi-ment.234cluster of positive training datacluster of negative training datatest datacenter of gravityminimum distance between test data and the center of gravityFigure 2: Each cluster and a test storyis log?i P (xi).
It is taken at the maximum-likelihood point(story), and thus, focusing just onthe set Xm ?
X which belongs to the centroid mand plugging in the MLE yields:?ll(Xm) = ?Rm2log(2?
)?Rm ?
n2log(?
?2)?Rm ?
k2+Rm log Rm ?
Rm log N (1 ?
m ?
k) (6)We choose the number of k whose value of BICis highest.3.4 TrackingEach story is represented as a vector of termswith tf ?
idf weights in an n dimensional space,where n is the number of terms in the collection.Whether or not each test story is positive is judgedusing the distance (measured by cosine similarity)between a vector representation of the test storyand each centroid g of the clusters.
Fig.2 illus-trates each cluster and a test story in the trackingprocedure.
Fig.2 shows that negative training sto-ries are classified into three groups.
The centroidg for each cluster is calculated as follows:g = (g1, ?
?
?
, gn) = (1pp?i=1xi1, ?
?
?
,1pp?i=1xin)(7)where xij (1?
j ?
n) is the tf ?idf weighted valueof term j in the story xi.
The test story is judgedby using these centroids.
If the value of cosinesimilarity between the test story and the centroidwith positive stories is smallest among others, thetest story is declared to be positive.
In Fig.2, thetest story is regarded as negative, since the valuebetween them is smallest.
This procedure, is re-peated until the last test story is judged.4 Experiments4.1 Creating Japanese CorpusWe chose the TDT3 English corpora as our goldstandard corpora.
TDT3 consists of 34,600 sto-ries with 60 manually identified topics.
We thencreated Japanese corpora (Mainichi and Yomiurinewspapers) to evaluate the method.
We annotatedthe total number of 66,420 stories from Oct.1, toDec.31, 1998, against the 60 topics.
Each storywas labelled according to whether the story dis-cussed the topic or not.
Not all the topics werepresent in the Japanese corpora.
We therefore col-lected 1 topic from the TDT1 and 2 topics from theTDT2, each of which occurred in Japan, and addedthem in the experiment.
TDT1 is collected fromthe same period of dates as the TDT3, and the firststory of ?Kobe Japan Quake?
topic starts from Jan.16th.
We annotated 174,384 stories of Japanesecorpora from the same period for the topic.
Ta-ble 2 shows 24 topics which are included in theJapanese corpora.
?TDT?
refers to the evaluationdata, TDT1, 2, or 3.
?ID?
denotes topic number de-fined by the TDT.
?OnT.?
(On-Topic) refers to thenumber of stories discussing the topic.
Bold fontstands for the topic which happened in Japan.
Theevaluation of annotation is made by three humans.The classification is determined to be correct if themajority of three human judges agree.4.2 Experiments Set UpThe English data we used for extracting termsis Reuters?96 corpus(806,791 stories) includingTDT1 and TDT3 corpora.
The Japanese datawas 1,874,947 stories from 14 years(from 1991to 2004) Mainichi newspapers(1,499,936 stories),and 3 years(1994, 1995, and 1998) Yomiurinewspapers(375,011 stories).
All Japanese sto-ries were tagged by the morphological analysisChasen(Matsumoto, 1997).
English stories weretagged by a part-of-speech tagger(Schmid, 1995),and stop word removal.
We applied n-gram modelwith Church-Gale smoothing to noun words, andselected terms whose probabilities are higher thana certain threshold4.
As a result, we obtained338,554 Japanese and 130,397 English terms.
Weused the EDR bilingual dictionary, and translatedJapanese terms into English.
Some of the wordshad no translation.
For these, we estimated termcorrespondences.
Each story is represented as avector of terms with tf ?idf weights.
We calcu-lated story similarities and extracted story pairsbetween positive and its associated stories5.
In4The threshold value for both English and Japanese was0.800.
It was empirically determined.5The threshold value for bilingual story pair was 0.65, andthat for monolingual was 0.48.
The difference of dates be-tween bilingual stories was ?4.235Table 2: Topic NameTDT ID Topic name OnT.
TDT ID Topic name OnT.1 15 Kobe Japan quake 9,9122 31015 Japan Apology to Korea 28 2 31023 Kyoto Energy Protocol 403 30001 Cambodian government coalition 48 3 30003 Pinochet trial 1653 30006 NBA labor disputes 44 3 30014 Nigerian gas line fire 63 30017 North Korean food shortages 23 3 30018 Tony Blair visits China in Oct. 73 30022 Chinese dissidents sentenced 21 3 30030 Taipei Mayoral elections 3533 30031 Shuttle Endeavour mission for space station 17 3 30033 Euro Introduced 1523 30034 Indonesia-East Timor conflict 34 3 30038 Olympic bribery scandal 353 30041 Jiang?s Historic Visit to Japan 111 3 30042 PanAm lockerbie bombing trial 133 30047 Space station module Zarya launched 30 3 30048 IMF bailout of Brazil 283 30049 North Korean nuclear facility?
111 3 30050 U.S. Mid-term elections 1233 30053 Clinton?s Gaza trip 74 3 30055 D?Alema?s new Italian government 373 30057 India train derailment 12the tracking, we used the extracted terms togetherwith all verbs, adjectives, and numbers, and repre-sented each story as a vector of these with tf ?idfweights.We set the evaluation measures used in the TDTbenchmark evaluations.
?Miss?
denotes Miss rate,which is the ratio of the stories that were judgedas YES but were not evaluated as such for the runin question.
?F/A?
shows false alarm rate, which isthe ratio of the stories judged as NO but were eval-uated as YES.
The DET curve plots misses andfalse alarms, and better performance is indicatedby curves more to the lower left of the graph.
Thedetection cost function(CDet) is defined by Eq.
(8).CDet = (CMiss ?
PMiss ?
PTarget +CFa ?
PFa ?
(1?
PTarget))PMiss = #Misses/#TargetsPFa = #FalseAlarms/#NonTargets (8)CMiss, CFa, and PTarget are the costs of a misseddetection, false alarm, and priori probability offinding a target, respectively.
CMiss, CFa, andPTarget are usually set to 10, 1, and 0.02, respec-tively.
The normalized cost function is defined byEq.
(9), and lower cost scores indicate better per-formance.
(CDet)Norm = CDet/MIN(CMiss ?
PTarget, CFa?(1?
PTarget)) (9)4.3 Basic ResultsTable 3 summaries the tracking results.
MINdenotes MIN(CDet)Norm which is the value of(CDet)Norm at the best possible threshold.
Ntis the number of initial positive training stories.We recall that we used subset of the topics de-fined by the TDT.
We thus implemented Allan?smethod(Allan et.
al, 1998) which is similar toour method, and compared the results.
It is based125102040608090.01   .02   .05  0.1   0.2   0.5    1     2     5    10    20    40    60    80    90MissProbability (in %)False Alarm Probability (in %)random performanceWith story pairsBaselineFigure 3: Tracking result(23 topics)on a tracking query which is created from the top10 most commonly occurring features in the Ntstories, with weight equal to the number of timesthe term occurred in those stories multiplied by itsincremental idf value.
They used a shallow tag-ger and selected all nouns, verbs, adjectives, andnumbers.
We added the extracted terms to thesepart-of-speech words to make their results compa-rable with the results by our method.
?Baseline?in Table 3 shows the best result with their methodamong varying threshold values of similarity be-tween queries and test stories.
We can see that theperformance of our method was competitive to thebaseline at every Nt value.Fig.3 shows DET curves by both our methodand Allan?s method(baseline) for 23 topics fromthe TDT2 and 3.
Fig.4 illustrates the results for 3topics from TDT2 and 3 which occurred in Japan.To make some comparison possible, only the Nt =4 is given for each.
Both Figs.
show that we havean advantage using bilingual comparable corpora.4.4 The Effect of Story PairsThe contribution of the extracted story pairs, es-pecially the use of two types of story pairs, bilin-gual and monolingual, is best explained by look-ing at the two results: (i) the tracking results withtwo types of story pairs, with only English and236Table 3: Basic resultsTDT1 (Kobe Japan Quake)Baseline Bilingual corpora & clusteringNt Miss F/A Recall Precision F MIN Nt Miss F/A Recall Precision F MIN1 27% .15% 73% 67% .70 .055 1 10% .42% 90% 74% .81 .0232 20% .12% 80% 73% .76 .042 2 6% .27% 93% 76% .83 .0134 9% .09% 91% 80% .85 .039 4 5% .18% 96% 81% .88 .012TDT2 & TDT3(23 topics)Baseline Bilingual corpora & clusteringNt Miss F/A Recall Precision F MIN Nt Miss F/A Recall Precision F MIN1 41% .17% 59% 60% .60 .089 1 29% .25% 71% 54% .61 .0592 40% .16% 60% 62% .61 .072 2 27% .25% 73% 55% .63 .0544 29% .12% 71% 72% .71 .057 4 20% .13% 80% 73% .76 .041125102040608090.01   .02   .05  0.1   0.2   0.5    1     2     5    10    20    40    60    80    90MissProbability (in %)False Alarm Probability (in %)random performanceWith story pairs(Japan)Baseline(Japan)Figure 4: 3 topics concerning to Japan125102040608090.01   .02   .05  0.1   0.2   0.5    1     2     5    10    20    40    60    80    90MissProbability (in %)False Alarm Probability (in %)random performancetwo types of story pairsWith only J-E story pairsWithout story pairsFigure 5: With and without story pairsJapanese stories in question, and without storypairs, and (ii) the results of story pairs by vary-ing values of Nt.
Fig.5 illustrates DET curves for23 topics, Nt=4.As can be clearly seen from Fig.5, the re-sult with story pairs improves the overall perfor-mance, especially the result with two types ofstory pairs was better than that with only EnglishTable 4: Performance of story pairs(24 topics)Two types of story pairs J-E story pairsNt Rec.
Prec.
F Rec.
Prec.
F1 30% 82% .439 28% 80% .4152 36% 85% .506 33% 82% .4714 45% 88% .595 42% 79% .548and Japanese stories in question.
Table 4 showsthe performance of story pairs which consist ofpositive and its associated story.
Each result de-notes micro-averaged scores.
?Rec.?
is the ratioof correct story pair assignments by the system di-vided by the total number of correct assignments.?Prec.?
is the ratio of correct story pair assign-ments by the system divided by the total numberof system?s assignments.
Table 4 shows that thesystem with two types of story pairs correctly ex-tracted stories related to the target topic even for asmall number of positive training stories, since theratio of Prec.
in Nt = 1 is 0.82.
However, each re-call value in Table 4 is low.
One solution is to usean incremental approach, i.e.
by repeating storypairs extraction, new story pairs that are not ex-tracted previously may be extracted.
This is a richspace for further exploration.The effect of story pairs for the tracking taskalso depends on the performance of bilingual termcorrespondences.
We obtained 1,823 English andJapanese term pairs in all when a period of dayswas ?4.
Fig.6 illustrates the result using differ-ent period of days(?1 to ?10).
For example, ?
?1?shows that the difference of dates between Englishand Japanese story pairs is less than ?1.
Y-axisshows the precision which is the ratio of correctterm pairs by the system divided by the total num-ber of system?s assignments.
Fig.6 shows that thedifference of dates between bilingual story pairs,affects the overall performance.4.5 The Effect of k-means with EMThe contribution of k-means with EM for classi-fying negative stories is explained by looking atthe result without classifying negative stories.
Wecalculated the centroid using all negative trainingstories, and a test story is judged to be negative or237??????????????
??
??
??
??
??
??
??
??
???Prec.
(%)1.4218.339.853.037.234.033.732.020.819.6Figure 6: Prec.
with different period of days125102040608090.01   .02   .05  0.1   0.2   0.5    1     2     5    10    20    40    60    80    90MissProbability (in %)False Alarm Probability (in %)Random PerformanceBIC (with classifying)k=0k=100Figure 7: BIC v.s.
fixed k for k-means with EMpositive by calculating cosine similarities betweenthe test story and each centroid of negative andpositive stories.
Further, to examine the effect ofusing the BIC, we compared with choosing a pre-defined k, i.e.
k=10, 50, and 100.
Fig.7 illustratespart of the result for k=100.
We can see that themethod without classifying negative stories(k=0)does not perform as well and results in a high missrate.
This result is not surprising, because the sizeof negative training stories is large compared withthat of positive ones, and therefore, the test story iserroneously judged as NO.
Furthermore, the resultindicates that we need to run BIC, as the result wasbetter than the results with choosing any numberof pre-defined k, i.e.
k=10, 50, and 100.
We alsofound that there was no correlation between thenumber of negative training stories for each of the24 topics and the number of clusters k obtained bythe BIC.
The minimum number of clusters k was44, and the maximum was 100.5 ConclusionIn this paper, we addressed the issue of the differ-ence in sizes between positive and negative train-ing stories for the tracking task, and investigatedthe use of bilingual comparable corpora and semi-supervised clustering.
The empirical results wereencouraging.
Future work includes (i) extend-ing the method to an incremental approach forextracting story pairs, (ii) comparing our cluster-ing method with the other existing methods suchas X-means(Pelleg, 2000), and (iii) applying themethod to the TDT4 for quantitative evaluation.AcknowledgmentsThis work was supported by the Grant-in-aid forthe JSPS, Support Center for Advanced Telecom-munications Technology Research, and Interna-tional Communications Foundation.ReferencesJ.Allan and R.Papka and V.Lavrenko, On-line new eventdetection and tracking, Proc.
of the DARPA Workshop,1998.J.Allan and V.Lavrenko and R.Nallapti, UMass at TDT2002, Proc.
of TDT Workshop, 2002.S.Basu and A.Banerjee and R.Mooney, Semi-supervisedclustering by seeding, Proc.
of ICML?02, 2002.J.Carbonell et.
al, CMU report on TDT-2: segmentation,detection and tracking, Proc.
of the DARPA Workshop,1999.S.F.Chen and J.Goodman, An empirical study of smoothingtechniques for language modeling, Proc.
of the ACL?96,pp.
310-318, 1996.N.Collier and H.Hirakawa and A.Kumano, Machine trans-lation vs. dictionary term translation - a comparison forEnglish-Japanese news article alignment, Proc.
of COL-ING?02, pp.
263-267, 2002.I.Dagan and K.Church, Termight: Coordinating humans andmachines in bilingual terminology acquisition, Journal ofMT, Vol.
20, No.
1, pp.
89-107, 1997.M.Franz and J.S.McCarley, Unsupervised and supervisedclustering for topic tracking, Proc.
of SIGIR?01, pp.
310-317, 2001.L.S.Larkey et.
al, Language-specific model in multilingualtopic tracking, Proc.
of SIGIR?04, pp.
402-409, 2004.Y.Matsumoto et.
al, Japanese morphological analysis systemchasen manual, NAIST Technical Report, 1997.D.W.Oard, Topic tracking with the PRISE information re-trieval system, Proc.
of the DARPA Workshop, pp.
94-101, 1999.D.Pelleg and A.Moore, X-means: Extending K-means withefficient estimation of the number of clusters, Proc.
ofICML?00, pp.
727-734, 2000.H.Schmid, Improvements in part-of-speech tagging with anapplication to german, Proc.
of the EACL SIGDAT Work-shop, 1995.K.Wagstaff et.
al, Constrained K-means clustering withbackground knowledge, Proc.
of ICML?01, pp.
577-584,2001.Y.Yang et.
al, Improving text categorization methods forevent tracking, Proc.
of SIGIR?00, pp.
65-72, 2000.238
