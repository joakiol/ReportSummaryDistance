Proceedings of the Human Language Technology Conference of the North American Chapter of the ACL, pages 173?176,New York, June 2006. c?2006 Association for Computational LinguisticsEvolving optimal inspectable strategies for spoken dialogue systemsDave ToneySchool of InformaticsEdinburgh University2 Buccleuch PlaceEdinburgh EH8 9LWdave@cstr.ed.ac.ukJohanna MooreSchool of InformaticsEdinburgh University2 Buccleuch PlaceEdinburgh EH8 9LWjmoore@inf.ed.ac.ukOliver LemonSchool of InformaticsEdinburgh University2 Buccleuch PlaceEdinburgh EH8 9LWolemon@inf.ed.ac.ukAbstractWe report on a novel approach to gener-ating strategies for spoken dialogue sys-tems.
We present a series of experimentsthat illustrate how an evolutionary rein-forcement learning algorithm can producestrategies that are both optimal and easilyinspectable by human developers.
Our ex-perimental strategies achieve a mean per-formance of 98.9% with respect to a pre-defined evaluation metric.
Our approachalso produces a dramatic reduction instrategy size when compared with conven-tional reinforcement learning techniques(87% in one experiment).
We concludethat this algorithm can be used to evolveoptimal inspectable dialogue strategies.1 IntroductionDeveloping a dialogue management strategy for aspoken dialogue system is often a complex and time-consuming task.
This is because the number ofunique conversations that can occur between a userand the system is almost unlimited.
Consequently,a system developer may spend a lot of time antic-ipating how potential users might interact with thesystem before deciding on the appropriate system re-sponse.Recent research has focused on generating dia-logue strategies automatically.
This work is basedon modelling dialogue as a markov decision process,formalised by a finite state space S, a finite actionset A, a set of transition probabilities T and a re-ward function R. Using this model an optimal dia-logue strategy pi?
is represented by a mapping be-tween the state space and the action set.
That is, foreach state s ?
S this mapping defines its optimal ac-tion a?s .
How is this mapping constructed?
Previousapproaches have employed reinforcement learning(RL) algorithms to estimate an optimal value func-tion Q?
(Levin et al, 2000; Frampton and Lemon,2005).
For each state this function predicts the fu-ture reward associated with each action available inthat state.
This function makes it easy to extract theoptimal strategy (policy in the RL literature).Progress has been made with this approach butsome important challenges remain.
For instance,very little success has been achieved with the largestate spaces that are typical of real-life systems.Similarly, work on summarising learned strategiesfor interpretation by human developers has so faronly been applied to tasks where each state-actionpair is explicitly represented (Lec?uche, 2001).This tabular representation severely limits the sizeof the state space.We propose an alternative approach to finding op-timal dialogue policies.
We make use of XCS, anevolutionary reinforcement learning algorithm thatseeks to represent a policy as a compact set of state-action rules (Wilson, 1995).
We suggest that this al-gorithm could overcome both the challenge of largestate spaces and the desire for strategy inspectability.In this paper, we focus on the issue of inspectabil-ity.
We present a series of experiments that illustratehow XCS can be used to evolve dialogue strategiesthat are both optimal and easily inspectable.1732 Learning Classifier Systems and XCSLearning Classifier Systems were introduced byJohn Holland in the 1970s as a framework for learn-ing rule-based knowledge representations (Holland,1976).
In this model, a rule base consists of a popu-lation of N state-action rules known as classifiers.The state part of a classifier is represented by aternary string from the set {0,1,#} while the actionpart is composed from {0,1}.
The # symbol acts asa wildcard allowing a classifier to aggregate states;for example, the state string 1#1 matches the states111 and 101.
Classifier systems have been appliedto a number of learning tasks, including data mining,optimisation and control (Bull, 2004).Classifier systems combine two machine learningtechniques to find the optimal rule set.
A geneticalgorithm is used to evaluate and modify the popu-lation of rules while reinforcement learning is usedto assign rewards to existing rules.
The search forbetter rules is guided by the strength parameter as-sociated with each classifier.
This parameter servesas a fitness score for the genetic algorithm and as apredictor of future reward (payoff ) for the RL algo-rithm.
This evolutionary learning process searchesthe space of possible rule sets to find an optimal pol-icy as defined by the reward function.XCS (X Classifier System) incorporates a num-ber of modifications to Holland?s original frame-work (Wilson, 1995).
In this system, a classifier?sfitness is based on the accuracy of its payoff predic-tion instead of the prediction itself.
Furthermore, thegenetic algorithm operates on actions instead of thepopulation as a whole.
These aspects of XCS resultin a more complete map of the state-action spacethan would be the case with strength-based classi-fier systems.
Consequently, XCS often outperformsstrength-based systems in sequential decision prob-lems (Kovacs, 2000).3 Experimental MethodologyIn this section we present a simple slot-filling sys-tem based on the hotel booking domain.
The goal ofthe system is to acquire the values for three slots: thecheck-in date, the number of nights the user wishesto stay and the type of room required (single, twinetc.).
In slot-filling dialogues, an optimal strategy isone that interacts with the user in a satisfactory waywhile trying to minimise the length of the dialogue.A fundamental component of user satisfaction is thesystem?s prevention and repair of any miscommuni-cation between it and the user.
Consequently, ourhotel booking system focuses on evolving essentialslot confirmation strategies.We devised an experimental framework for mod-elling the hotel system as a sequential decision taskand used XCS to evolve three behaviours.
Firstly,the system should execute its dialogue acts in a log-ical sequence.
In other words, the system shouldgreet the user, ask for the slot information, presentthe query results and then finish the dialogue, in thatorder (Experiment 1).
Secondly, the system shouldtry to acquire the slot values as quickly as possiblewhile taking account of the possibility of misrecog-nition (Experiments 2a and 2b).
Thirdly, to increasethe likelihood of acquiring the slot values correctly,each one should be confirmed at least once (Experi-ments 3 and 4).The reward function for Experiments 1, 2a and2b was the same.
During a dialogue, each non-terminal system action received a reward value ofzero.
At the end of each dialogue, the final rewardcomprised three parts: (i) -1000 for each systemturn; (ii) 100,000 if all slots were filled; (iii) 100,000if the first system act was a greeting.
In Experiments3 and 4, an additional reward of 100,000 was as-signed if all slots were confirmed.The transition probabilities were modelled usingtwo versions of a handcoded simulated user.
A verylarge number of test dialogues are usually requiredfor learning optimal dialogue strategies; simulatedusers are a practical alternative to employing humantest users (Scheffler and Young, 2000; Lopez-Cozaret al, 2002).
Simulated user A represented a fullycooperative user, always giving the slot informationthat was asked.
User B was less cooperative, givingno response 20% of the time.
This allowed us toperform a two-fold cross validation of the evolvedstrategies.For each experiment we allowed the system?sstrategy to evolve over 100,000 dialogues with eachsimulated user.
Dialogues were limited to a maxi-mum of 30 system turns.
We then tested each strat-egy with a further 10,000 dialogues.
We logged thetotal reward (payoff) for each test dialogue.
Eachexperiment was repeated ten times.174In each experiment, the presentation of the queryresults and closure of the dialogue were combinedinto a single dialogue act.
Therefore, the dialogueacts available to the system for the first experi-ment were: Greeting, Query+Goodbye, Ask(Date),Ask(Duration) and Ask(RoomType).
Four booleanvariables were used to represent the state of the di-alogue: GreetingFirst, DateFilled, DurationFilled,RoomFilled.Experiment 2 added a new dialogue act: Ask(All).The goal here was to ask for all three slot valuesif the probability of getting the slot values was rea-sonably high.
If the probability was low, the sys-tem should ask for the slots one at a time as be-fore.
This information was modelled in the sim-ulated users by 2 variables: Prob1SlotCorrect andProb3SlotsCorrect.
The values for these variablesin Experiments 2a and 2b respectively were: 0.9 and0.729 (=0.93); 0.5 and 0.125 (=0.53).Experiment 3 added three new dialogue acts: Ex-plicit Confirm(Date), Explicit Confirm(Duration),Explicit Confirm(RoomType) and three new statevariables: DateConfirmed, DurationConfirmed,RoomConfirmed.
The goal here was for the sys-tem to learn to confirm each of the slot val-ues after the user has first given them.
Experi-ment 4 sought to reduce the dialogue length fur-ther by allowing the system to confirm one slotvalue while asking for another.
Two new di-alogue acts were available in this last experi-ment: Implicit Confirm(Date)+Ask(Duration) andImplicit Confirm(Duration)+Ask(RoomType).4 Experimental ResultsTable 1 lists the total reward (payoff) averaged overthe 10 cross-validated test trials for each experiment,expressed as a percentage of the maximum payoff.In these experiments, the maximum payoff repre-sents the shortest possible successful dialogue.
Forexample, the maximum payoff for Experiment 1 is195,000: 100,000 for filling the slots plus 100,000for greeting the user at the start of the dialogue mi-nus 5000 for the minimum number of turns (five)taken to complete the dialogue successfully.
The av-erage payoff for the 10 trials trained on simulateduser A and tested on user B was 193,877 ?
approxi-mately 99.4% of the maximum possible.
In light ofExp.
Training/Test Users Payoff (%)1 A, B 99.4B, A 99.82a A, B 99.1B, A 99.42b A, B 96.8B, A 97.23 A, B 98.8B, A 99.34 A, B 99.3B, A 99.7Table 1: Payoff results for the evolved strategies.these results and the stochastic user responses, wesuggest that these evolved strategies would comparefavourably with any handcoded strategies.It is instructive to compare the rate of convergencefor different strategies.
Figure 1 shows the averagepayoff for the 100,000 dialogues trained with sim-ulated user A in Experiments 3 and 4.
It showsthat Experiment 3 approached the optimal policyafter approximately 20,000 dialogues whereas Ex-periment 4 converged after approximately 5000 dia-logues.
This is encouraging because it suggests thatXCS remains focused on finding the shortest suc-cessful dialogue even when the number of availableactions increases.0 25,000 50,000 75,000 100,00000.511.522.53x 105DialoguesAveragePayoffExp.
3Exp.
4Figure 1: Convergence towards optimality duringtraining in Experiments 3 and 4 (simulated user A).Finally, we look at how to represent an optimalstrategy.
From the logs of the test dialogues we ex-tracted the state-action rules (classifiers) that wereexecuted.
For example, in Experiment 4, the op-175State ActionGreetingFirstDateFilledDurationFilledRoomFilledDateConfirmedDurationConfirmedRoomConfirmed0 0 # # # # # Greeting1 0 0 0 # # # Ask(Date)1 1 # # 0 # # Implicit Confirm(Date) + Ask(Duration)1 1 1 # 1 0 0 Implicit Confirm(Duration) + Ask(RoomType)1 1 1 1 1 1 0 Explicit Confirm(RoomType)1 1 1 1 1 1 1 Query + GoodbyeTable 2: A summary of the optimal strategy for Experiment 4.timal strategy is represented by 17 classifiers.
Bycomparison, a purely RL-based strategy would de-fine an optimal action for every theoretically pos-sible state (i.e.
128).
In this example, the evolu-tionary approach has reduced the number of rulesfrom 128 to 17 (a reduction of 87%) and is thereforemuch more easily inspectable.
In fact, the size of theoptimal strategy can be reduced further by select-ing the most general classifier for each action (Table2).
These rules are sufficient since they cover the 60states that could actually occur while following theoptimal strategy.5 Conclusions and future workWe have presented a novel approach to generatingspoken dialogue strategies that are both optimal andeasily inspectable.
The generalizing ability of theevolutionary reinforcement learning (RL) algorithm,XCS, can dramatically reduce the size of the opti-mal strategy when compared with conventional RLtechniques.
In future work, we intend to exploit thisgeneralization feature further by developing systemsthat require much larger state representations.
Wealso plan to investigate other approaches to strategysummarisation.
Finally, we will evaluate our ap-proach against purely RL-based methods.ReferencesLarry Bull, editor.
2004.
Applications of Learning Clas-sifier Systems.
Springer.Matthew Frampton and Oliver Lemon.
2005.
Reinforce-ment learning of dialogue strategies using the user?slast dialogue act.
In IJCAI Workshop on Knowledgeand Reasoning in Practical Dialogue Systems, Edin-burgh, UK, July.John Holland.
1976.
Adaptation.
In Rosen R.and F. Snell, editors, Progress in theoretical biology.Plenum, New York.Tim Kovacs.
2000.
Strength or accuracy?
Fitness cal-culation in learning classifier systems.
In Pier LucaLanzi, Wolfgang Stolzmann, and Stewart Wilson, edi-tors, Learning Classifier Systems.
From Foundations toApplications, Lecture Notes in Artificial Intelligence1813, pages 143?160.
Springer-Verlag.Renaud Lec?uche.
2001.
Learning optimal dialoguemanagement rules by using reinforcement learningand inductive logic programming.
In 2nd Meetingof the North American Chapter of the Association ofComputational Linguistics, Pittsburgh, USA, June.Esther Levin, Roberto Pieraccini, and Wieland Eckert.2000.
A stochastic model of human-machine inter-action for learning dialogue strategies.
IEEE Transac-tions on Speech and Audio Processing, 8(1):11?23.R.
Lopez-Cozar, A.
De la Torre, J. Segura, A. Rubio, andV.
Sa?nchez.
2002.
Testing dialogue systems by meansof automatic generation of conversations.
Interactingwith Computers, 14(5):521?546.Konrad Scheffler and Steve Young.
2000.
Probabilis-tic simulation of human-machine dialogues.
In Inter-national Conference on Acoustics, Speech and SignalProcessing, pages 1217?1220, Istanbul, Turkey, June.Stewart Wilson.
1995.
Classifier fitness based on accu-racy.
Evolutionary Computation, 3(2):149?175.176
