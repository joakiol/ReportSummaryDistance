Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 761?769,Beijing, August 2010Controlling Listening-oriented Dialogue using Partially ObservableMarkov Decision ProcessesToyomi Meguro?, Ryuichiro Higashinaka?, Yasuhiro Minami?, Kohji Dohsaka?
?NTT Communication Science Laboratories, NTT Corporation?NTT Cyber Space Laboratories, NTT Corporationmeguro@cslab.kecl.ntt.co.jphigashinaka.ryuichiro@lab.ntt.co.jp{minami,dohsaka}@cslab.kecl.ntt.co.jpAbstractThis paper investigates how to automat-ically create a dialogue control compo-nent of a listening agent to reduce the cur-rent high cost of manually creating suchcomponents.
We collected a large numberof listening-oriented dialogues with theiruser satisfaction ratings and used them tocreate a dialogue control component usingpartially observable Markov decision pro-cesses (POMDPs), which can learn a pol-icy to satisfy users by automatically find-ing a reasonable reward function.
A com-parison between our POMDP-based com-ponent and other similarly motivated sys-tems using human subjects revealed thatPOMDPs can satisfactorily produce a dia-logue control component that can achievereasonable subjective assessment.1 IntroductionAlthough task-oriented dialogue systems havebeen actively researched (Hirshman, 1989; Fer-guson et al, 1996; Nakano et al, 1999; Walkeret al, 2002), recently non-task-oriented functionsare starting to attract attention, and systems with-out a specific task that deal with more casual di-alogues, such as chats, are being actively investi-gated from their social and entertainment aspects(Bickmore and Cassell, 2001; Higashinaka et al,2008; Higuchi et al, 2008).In the same vein, we have been working onlistening-oriented dialogues in which one conver-sational participant attentively listens to the other(hereafter, listening-oriented dialogue).
Our aimis to build listening agents that can implementsuch a listening process so that users can satisfytheir desire to speak and be heard.
Figure 1 showsan excerpt from a typical listening-oriented dia-logue.
In the literature, dialogue control compo-nents for less (or non-) task-oriented dialogue sys-tems, such as listening agents, have typically usedhand-crafted rules for dialogue control, whichcan be problematic because completely coveringall dialogue states by hand-crafted rules is diffi-cult when the dialogue has fewer task restrictions(Wallace, 2004; Isomura et al, 2009).To solve this problem, this paper aims to auto-matically build a dialogue control component of alistening agent using partially observable Markovdecision processes (POMDPs).
POMDPs, whichmake it possible to learn a policy that can max-imize the averaged reward in partially observableenvironments (Pineau et al, 2003), have been suc-cessfully adopted in task-oriented dialogue sys-tems for learning a dialogue control module fromdata (Williams and Young, 2007).
However, nowork has attempted to use POMDPs for less (ornon-) task-oriented dialogue systems, such as lis-tening agents, because user goals are not as well-defined as task-oriented ones, complicating thefinding of a reasonable reward function.We apply POMDPs to listening-oriented dia-logues by having the system learn a policy that si-multaneously maximizes how well users feel thatthey are being listened to (hereafter, user satis-faction) and how smoothly the system generatesdialogues (hereafter, smoothness).
This formu-lation is new; no work has considered both usersatisfaction and smoothness using POMDPs.
Wecollected a large amount of listening-oriented di-alogues and annotated them with dialogue actsand also obtained subjective evaluation results forthem.
From them, we calculated the rewards andlearned the POMDP policies.
We evaluated thedialogue-act tag sequences of our POMDPs usinghuman subjects.761Utterance Dialogue actS: Good evening.
GREETINGThe topic is ?food,?
nice tomeet you.GREETINGL: Nice to meet you, too.
GREETINGS: I had curry for dinner.
S-DISC (sub: fact)Do you like curry?
QUESTION (sub: pref)L: Yes, I do.
SYMPATHYS: Really?
REPEATMe, too.
SYMPATHYL: Do you usually go out to eat?
QUESTION (sub: habit)S: No, I always cook at home.
S-DISC (sub: habit)I don?t use any special spices,but I sometimes cook noodlesusing soup and curry.S-DISC (sub: habit)L: That sounds good!
S-DISC (sub: pref (pos-itive))Figure 1: Excerpt of a typical listening-orienteddialogue.
Dialogue topic is ?food.?
Dialogue actscorresponding to utterances are shown in paren-theses (See Table 1 for meanings): S-DISC standsfor SELF-DISCLOSURE; PREF for PREFERENCE;S for speaker; and L for listener.
The dialoguewas translated from Japanese by the authors.The next section introduces related work.
Sec-tion 3 describes our approach.
Section 4 de-scribes our collection of listening-oriented dia-logues.
This is followed in Section 5 by an evalua-tion experiment that compared our POMDP-baseddialogue control with other similarly motivatedsystems.
The last section summarizes the mainpoints and mentions future work.2 Related workWith increased attention on social dialogues andsenior peer counseling, work continues to emergeon listening-oriented dialogues.
One early workis (Maatman et al, 2005), which showed that vir-tual agents can give users the sense of being heardusing such gestures as nodding and head shak-ing.
Recently, Meguro et al (2009a) analyzedthe characteristics of listening-oriented dialogues.They compared listening-oriented dialogues andcasual conversations between humans, revealingthat the two types of dialogues have significantlydifferent flows and that listeners actively ques-tion with frequently inserted self-disclosures; thespeaker utterances were mostly concerned withself-disclosure.Shitaoka et al (2010) also investigated thefunctions of listening agents, focusing on theirresponse generation components.
Their systemtakes the confidence score of speech recognitioninto account and changes the system response ac-cordingly; it repeats the user utterance or makesan empathic utterance for high-confidence user ut-terances and makes a backchannel when the con-fidence is low.
The system?s empathic utterancescan be ?I?m happy?
or ?That?s too bad,?
depend-ing on whether a positive or negative expressionis included in the user utterances.
Their system?sresponse generation only uses the speech recogni-tion confidence and the polarity of user utterancesas cues to choose its actions.
Currently, it doesnot consider the utterance content or the user in-tention.In order for listening agents to achieve highsmoothness, a switching mechanism between the?active listening mode,?
in which the system isa listener, and the ?topic presenting mode,?
inwhich the system is a speaker, has been proposed(Yokoyama et al, 2010; Kobayashi et al, 2010).Here, the system uses a heuristic function to main-tain a high user interest level and to keep the sys-tem in an active listening mode.
Dialogue con-trol is done by hand-crafted rules.
Our motivationbears some similarity to theirs in that we want tobuild a listening agent that gives users a sense ofbeing heard; however, we want to automaticallymake such an agent from dialogue data.POMDPs have been introduced for robot actioncontrol (Pineau et al, 2003).
Here, the systemlearns to make suitable movements for complet-ing a certain task.
Over the years, POMDPs havebeen actively studied for applications to spokendialogue systems.
Williams et al (2007) suc-cessfully used a POMDP for dialogue control in aticket-buying domain in which the objective wasto fix the departure and arrival places for tickets.Recent work on POMDPs indicates that it is pos-sible to train a dialogue control module in task-oriented dialogues when the user goal is obvious.In contrast, in this paper, we aim to verify whetherPOMDPs can be applied to less task-oriented di-alogues (i.e., listening-oriented dialogues) whereuser goals are not as obvious.In a recent study, Minami et al (2009) ap-plied POMDPs to non-task-orientedman-machineinteraction.
Their system learned suitable ac-tion control of agents that can act smoothly byobtaining rewards from the statistics of artifi-cially generated data.
Our work is different be-cause we use real human-human dialogue data to762train POMDPs for dialogue control in listening-oriented dialogues.3 ApproachA typical dialogue system has utterance under-standing, dialogue control, and utterance gen-eration modules.
The utterance understandingmodule comprehends user natural-language utter-ances, whose output (i.e., a user dialogue act) ispassed to the dialogue control module.
The dia-logue control module chooses the best system di-alogue act at every dialogue point using the userdialogue act as input.
The utterance generationmodule generates natural-language utterances andsays them to users by realizing the system dia-logue acts as surface forms.This paper focuses on the dialogue controlmodule of a listening agent.
Since a listening-oriented dialogue has a characteristic conversationflow (Meguro et al, 2009a), focusing on this mod-ule is crucial because it deals with the dialogueflow.
Our objective is to train from data a dialoguecontrol module that achieves a smooth dialogueflow that makes users feel that they are being lis-tened to attentively.3.1 Dialogue control using POMDPsThe purpose of our dialogue control is to simulta-neously create situations in which users feel lis-tened to (i.e., user satisfaction) and to generatesmooth action sequences (i.e., smoothness).
Todo this, we automatically and statistically trainthe reward and the policy of the POMDP using alarge amount of listening-oriented dialogue data.POMDP is a reinforcement learning frameworkthat can learn a policy to select an action sequencethat maximizes average future rewards.
Setting areward is crucial in POMDPs.For our purpose, we introduce two different re-wards: one for user satisfaction and the other forsmoothness.
Before creating a POMDP structure,we used the dynamic Bayesian network (DBN)structure (Fig.
2) to obtain the statistical structureof the data and the two rewards.The random values in the DBN are as follows:so and sa are the dialogue state and action state,o is a speaker observation, a is a listener action,and d is a random variable for an evaluation scorethat indicates the degree of the user being listenedto.
This evaluation score can be obtained by ques-'aaosros'oasaso??a'aosdos'oasaso?
?DBN structurePOMDP structureFigure 2: DBN and POMDP structures employedin this paper.
Note that a in the POMDP is isolatedfrom other states because it is decided by a learnedpolicy.tionnaires, and the variable is used for calculat-ing a user satisfaction reward for the POMDP.The DBN arcs in Fig.
2 define the emission andtransition probabilities.
Pr(o?|s?o) is the emissionprobability of o?
given s?o.
Pr(d|so) is the emis-sion probability of d given so.
Pr(s?o|so, a) is atransition probability from so to s?o given a. TheDBN is trained using the EM algorithm.
Usingthe obtained variables, we calculate the two re-ward functions as follows:(1) Reward for user satisfaction This reward isobtained from the d variable byr1((so, ?
), a) =max?d=mind ?
Pr(d|so, a),where * is arbitrary sa and min and max are min-imum and maximum evaluation scores.
(2) Reward for smoothness For smoothness,we maximize the action predictive probabilitygiven the history of actions and observations.
Theprobability is calculated from listening-orienteddialogue data.
sa is introduced for estimating thepredictive probability of action a and for selectinga to maximize the predictive probability.We set Pr(a|sa) = 1 when a = sa so that sacorresponds one-on-one with a.
Then, if at = saat time t is given, we obtainPr(at|o1, a1, .
.
.
, at?1, ot)=?s?aPr(at|s?a) Pr(s?a|o1, a1, .
.
.
, at?1, ot)= Pr(sa|o1, a1, .
.
.
, ot?1, at?1, ot)Consequently, maximizing the predictive proba-bility of a equals maximizing that of sa.
If we763set 1.0 to reward r2((?, sa), a) when sa = a, thePOMDP will generate actions that maximize theirpredictive probabilities.
We believe that this re-ward should increase the smoothness of a systemaction sequence since the sequence is generatedaccording to the statistics of human-human dia-logues.Converting a DBN into a POMDP The DBNis converted into a POMDP (Fig.
2), while main-taining the transition and output probabilities.
Weconvert d to r as described above.The system is in a partially observed state.Since the state is not known exactly, we use a dis-tribution called ?belief state?
bt with which we ob-tain the average reward that will be gained in thefuture at time t by:Vt =???=0??
?sb?+t((so, sa))r((so, sa), a?+t),where ?
is a discount factor; namely, the futurereward is decreased by ?
.
A policy is learned byvalue iteration so that the action that maximizesVt can be chosen.
We define r((so, sa), a) as fol-lows:r((so, sa), a) = r1((so, ?
), a) + r2((?, sa), a).By balancing these two rewards, we can choosean action that satisfies both user satisfaction andsmoothness.4 Data collectionWe collected listening-oriented dialogues usinghuman subjects who consisted of ten listeners(five males and five females) and 37 speakers (18males and 19 females).
The listeners and speak-ers ranged from 20 to 60 years old and were allnative Japanese speakers.
Listeners and speakerswere matched to form a listener-speaker pair andcommunicated over the Internet with our chat in-terface.
They used only text; they were not al-lowed to use voice, video, or facial expressions.The speakers chose their own listener and freelyparticipated in dialogues from 7:00 pm to mid-night for a period of 15 days.
One conversationwas restricted to about ten minutes.
The subjectstalked about a topic chosen by the speaker.
Therewere 20 predefined topics: money, sports, TV andradio, news, fashion, pets, movies, music, house-work and childcare, family, health, work, hob-bies, food, human relationships, reading, shop-ping, beauty aids, travel, and miscellaneous.
Thelisteners were instructed to make it easy for thespeakers to say what the speakers wanted to say.We collected 1260 listening-oriented dialogues.4.1 Dialogue-act annotationWe labeled the collected dialogues using thedialogue-act tag set shown in Table 1.
We madethese tags by selecting, extending, and modifyingthose from previous studies that concerned humanlistening behaviors in some way (Meguro et al,2009a; Jurafsky et al, 1997; Ivey and Ivey, 2002).In our tag set, only question and self-disclosuretags have sub-category tags.
Two annotators (notthe authors) labeled each sentence of our collecteddialogues using these 32 tags.
In dialogue-act an-notation, since there can be several sentences inone utterance, one annotator first split the utter-ances into sentences, and then both annotators la-beled each sentence with a single dialogue act.4.2 Obtaining evaluation scoresPOMDPs need evaluation scores (i.e., d) for dia-logue acts (i.e., a) for training a reward function.Therefore, we asked a third-party participant, whowas neither a listener nor a speaker in our dialoguedata collection, to evaluate the user satisfactionlevels of the collected dialogues.
She rated eachdialogue in terms of how she would have felt ?be-ing heard?
after the dialogue if she had been thespeaker of the dialogue in question.
She providedratings on the 7-point Likert scale for each dia-logue.
Since she rated the whole dialogue with asingle rating, we set the evaluation score of eachaction within a dialogue using the evaluation scorefor that dialogue.We used a third-person?s evaluation and not theoriginal person?s to avoid the fact that the eval-uative criterion is too different between humans;identical evaluation scores from two people donot necessarily reflect identical user satisfactionlevels.
We highly valued the reliability and con-sistency of the third-person scores.
This way, atleast, we can train a policy that maximizes its av-erage reward function for the rater, which we needto verify first before considering adaptation to twoor more individuals.5 Experiment5.1 Experimental setupThe experiment followed three steps.764GREETING Greeting and confirmation of dialoguetheme.
e.g., Hello.
Let?s talk aboutlunch.INFORMATION Delivery of objective information.
e.g.,My friend recommended a restaurant.SELF-DISCLOSUREDisclosure of preferences and feelings.sub: fact e.g., I live in Tokyo.sub: experience e.g., I had a hamburger for lunch.sub: habit e.g., I always go out for dinner.sub: preference e.g., I like hamburgers.
(positive)sub: preference e.g., I don?t really like hamburgers.
(negative)sub: preference e.g., Its taste is near my homemade(neutral) taste.sub: desire e.g., I want to try it.sub: plan e.g., I?m going there next week.sub: otherACKNOWLEDGM-ENTEncourage the conversational partner tospeak.
e.g., Well.
Aha.QUESTION Utterances that expect answers.sub: information e.g., Please tell me how to cook it.sub: fact e.g., What kind of curry?sub: experience e.g., What did you have for dinner?sub: habit e.g., Did you cook it yourself?sub: preference e.g., Do you like it?sub: desire e.g., Don?t you want to eat rice?sub: plan e.g., What are you going to have fordinner?sub: otherSYMPATHY Sympathetic utterances and praises.e.g., Me, too.NON-SYMPATHY Negative utterances.
e.g., Not really.CONFIRMATION Confirm what the conversation partnersaid.
e.g., Really?PROPOSAL Encourage the partner to act.
e.g., Tryit.REPEAT Repeat the partner?s utterance.PARAPHRASE Paraphrase the partner?s utterance.APPROVAL Broach or show goodwill toward thepartner.
e.g., Absolutely!THANKS Express thanks e.g., Thank you.APOLOGY Express regret e.g., I?m sorry.FILLER Filler between utterances.
e.g., Uh.
Letme see.ADMIRATION Express affection.
e.g., Ha-ha.OTHER Other utterances.Table 1: Definition and example of dialogue actsIn the first step, we created our POMDP sys-tem using our approach (See Section 3.1).
Wealso made five other systems for comparison thatwe describe in Section 5.2.
Each system outputsdialogue-act tag sequences for evaluation.
Thedialogue theme was ?food?
because it was themost frequent theme and accounted for 20% ofour data (See Table 2 for the statistics); we trainedour POMDP using the ?food?
dialogues.
We re-stricted the dialogue topic to verify that our ap-proach at least works with a small set.
Since thereis no established measure for automatically eval-uating a dialogue-act tag sequence, we evaluatedAll Food (subset of All)# dialogues 1260 250# words 479881 94867# utterances per dialogue 28.2 29.1# dialogues per listener 126 25# dialogues per speaker 34 6.8# dialogue acts 67801 13376inter-annotator agreement 0.57 0.55Table 2: Statistics of collected dialogues anddialogue-act annotation.
Inter-annotator agree-ment means agreement of dialogue-act annotationusing Cohen?s ?.our dialogue control module using human subjec-tive evaluations.
However, this is very difficult todo because dialogue control modules only outputdialogue acts, not natural language utterances.In the second step, we recruited participantswho created natural language utterances fromdialogue-act tag sequences.
In their creating dia-logues, we provided them with situations to stim-ulate their imaginations.
Table 3 shows the situ-ations, which were deemed common in everydayJapanese life; we let the participants create utter-ances that fit the situations.
These situations werenecessary because, without restrictions, the evalu-ation scores could be influenced by dialogue con-tent rather than by dialogue flow.For this dialogue-imagining exercise, we re-cruited 16 participants (eight males and eight fe-males) who ranged from 19 to 39 years old.
Eachparticipant made twelve dialogues using two situ-ations.
For assigning the situations, we first cre-ated four conditions: (1) a student and living withfamily, (2) working and living with family, (3) astudent and living alone, and (4) working and liv-ing alone.
Then the participants were categorizedinto one of these conditions on the basis of theiractual lifestyle and assigned two of the situationsmatching the condition.For each situation, each participant created siximaginary dialogues from the six dialogue-act se-quences output by the six systems: our POMDPand the other five systems for comparison.
Thisprocess produced such dialogues as shown inFigs.
5 and 6.
The dialogue in Fig.
5 was madefrom a dialogue-act tag sequence of a human-human conversation using No.
1 of Table 3.
Thedialogue in Fig.
6 was made from the sequence ofour POMDP using No.
2 of Table 3.In the third step, we additionally recruited threejudges (one male and two females) to evalu-765ate the imagined 192 (16 ?
2 ?
6) dialogues.The judges were neither the participants whomade dialogues nor those who rated the collectedlistening-oriented dialogues.
Six dialogues madefrom one situation were randomly shown to thejudges one-by-one, who then filled out question-naires to indicate their user satisfaction levels byanswering this question on a 7-point Likert scale:?If you had been the speaker, would you have feltthat you were listened to?
?5.2 Systems for comparisonWe created our POMDP-based dialogue controland five other systems for comparison.POMDP We learned a policy based on our ap-proach.
We used ?food?
dialogues (See Section4), and the evaluation scores were those describedin Section 4.2.
This system used the policy togenerate sequences of dialogue-act tags by sim-ulation; user observations were generated basedon emission probability, and system actions weregenerated based on the policy.In this paper, the total number of observationsand actions was 33 because we have 32 dialogue-act tags (See Table 1) plus a ?skip?
tag.
In learningthe policy, an observation and an actionmust indi-vidually take turns, but our data can include mul-tiple dialogue-act tags in one utterance.
There-fore, if there is more than one dialogue-act tagin one utterance, a ?skip?
is inserted between thetags.
The state numbers for So and Sa were 16and 33, respectively.
In this experiment, we set 10to r2((?, sa), a).EvenPOMDP We arranged a POMDP usingonly the smoothness reward (hereafter, Even-POMDP) by creating a POMDP system with afixed evaluation score; hence user satisfactionis not incorporated in the reward.
When usingfixed (even) evaluation scores for all dialogues,the effect of the user satisfaction reward is de-nied, and the system only generates highly fre-quent sequences.
We have EvenPOMDP to clarifywhether user satisfaction is necessary.
The otherconditions are identical as in the POMDP system.HMM We modeled our dialogue-act tag se-quences using a Speaker HMM (SHMM) (Me-guro et al, 2009a), which has been utilized tomodel two-party listening-orienteddialogues.
In aSHMM, half the states emit listener dialogue acts,Listener?GREETINGSpeaker?GREETINGListener?QUESTIONSpeaker?S-DISCListener?S-DISCSYMPATHYor12345Figure 3: Structure of rule-based systemand the other half emit speaker dialogue acts.
Allstates are connected to each other.
We modeledthe ?food?
dialogues using an SHMM, and madethemodel generate themost probable dialogue-acttag sequences.
More specifically, first, a dialogue-act tag was generated randomly based on the ini-tial state.
If the state was that of a listener, wegenerated a maximum likelihood action and thestate was randomly transited based on the transi-tion probability.
If the state was that of a speaker,we randomly generated an action based on theemission probability and the state was transitedusing the maximum likelihood transition proba-bility.Rule-based system This system createsdialogue-act tag sequences using hand-craftedrules that are based on the findings in (Meguro etal., 2009a) and are realized as shown in Fig.
3.A sequence begins at state 1?
in Fig.
3, and onedialogue act is generated at each state.
At state3?, a sub-category tag under QUESTION is chosenrandomly, and at state 4?, a matched sub-categorytag under SELF-DISCLOSURE is chosen.
Atstate 5?, the listener?s SELF-DISCLOSURE orSYMPATHY is generated randomly.Human dialogue sequence This system createddialogue-act tag sequences by randomly choosingdialogues between humans from the collected dataand used their annotated tag sequences.Random This system simply created dialogue-act tag sequences at random.5.3 Experimental resultsFigure 4 shows the average subjective evaluationscores.
Except between HMM and EvenPOMDP,there was a significant difference (p<0.01) be-tween all systems in a non-parametric multiplecomparison test (Steel-Dwass test).
The dialoguesshown in Figs.
5 and 6 were generated by the sys-tems.
The dialogue in Fig.
5 was made from hu-man dialogue sequences, and the one in Fig.
6 wasmade from POMDP.766With whom What day What time What Where Who made1 family weekday around 6:00 pm grilled salmon home mother2 family weekend around 7:00 pm potato and meat home mother3 co-workers weekday at noon boiled seaweed lunch box myself... ... ... ... ... ... ...32 friend weekday at noon hamburger school cafeteria N/ATable 3: Dialogue situations relating to everyday Japanese lifeWe qualitatively analyzed the dialogues of eachsystem and observed the following characteristics:POMDP At a dialogue?s beginning, the systemgreets several times and shifts to a different phasein which listeners ask questions and self-discloseto encourage speakers to reciprocate.Rule-based The output of this system seemsvery natural and easy to read.
The dialogue-acttags followed reasonable rules, making it easierfor the participants to create natural utterancesfrom them.Human conversation The dialogues betweenhumans were obviously natural before they werechanged to tags from the natural-language ut-terances.
However, human dialogues have ran-domness, which makes it difficult for the partic-ipants to create natural-language utterances fromthe tags.
Hence, the evaluation score for this sys-tem was lower than the ?Rule-based.
?HMM, EvenPOMDP Since these systems con-tinually output the same action tags, their outputwas very unnatural.
For example, greetings neverstopped because GREETING is most frequentlyfollowed by GREETING in the data.
These sys-tems have no mechanism to stop this loop.POMDP successfully avoided such continua-tion because its actions have more varied rewards.For example, GREETING is repeated in Even-POMDP because its smoothness reward is high;however, in POMDP, although the smoothness re-ward remains high, its user satisfaction reward isnot that high.
This is because greetings appearin all dialogues and their user satisfaction rewardconverges to the average.
Therefore, such actionsas greetings do not get repeated in POMDP.
InPOMDP, some states have high user satisfactionrewards, and the POMDP policy generated actionsto move to such states.Random Since this system has more varietyof tags than HMM, its evaluation scores out-performed HMM, but were outperformed byPOMDP, which learned statistically from the data.Rule-based6.07Humandialogue5.22POMDP?Proposed?3.76Random2.67HMM1.17EvenPOMDP1.1601234567AveragedEvaluationScoresFigure 4: System scores.
Except betweenPOMDP and EvenPOMDP, significant differencesexist among all systems (p<0.01).From our qualitative analysis, we found thatPOMDP can generate more satisfying sequencesthan HMM/EvenPOMDP because it does not fallinto the loop of frequent dialogue-act tag se-quences.
This suggests the usefulness of incor-porating two kinds of rewards into the policy andthat our approach for setting a reward is promis-ing.However, with the proposed POMDP, unnaturalsequences remain; for example, the system sud-denly output THANKS, as shown in Fig.
6.
Thenumber of states may have been too small.
Weplan to investigate what caused this in the future.In our qualitative analysis, we observed thatrandomness in dialogues might hold a clue forimproving evaluation scores.
Therefore, wemeasured the perplexity of each system outputusing dialogue-act trigrams and obtained 72.8for ?Random,?
27.4 for ?Human dialogue,?
7.4for ?POMDP,?
3.2 for ?HMM,?
2.5 for ?Even-POMDP,?
and 1.7 for ?Rule-based.
?The perplexity of the human dialogues is lessthan that of the random system, but humans alsoexhibit a certain degree of freedom.
On the otherhand, POMDP?s perplexity is less than the humandialogues; they still have some freedom, whichprobably led to their reasonable evaluation scores.Considering that HMM and EvenPOMDP, whichcontinually output the same dialogue acts, had low767Utterance Dialogue actS: Hello.
GREETINGL: Nice to meet you GREETINGS: I had dinner at home today.
S-DISC (sub: fact)Do you like grilled salmon?
QUESTION, PREFL: Yes, I think so.
SYMPATHYI sometimes want to have afancy meal.S-DISC (sub: de-sire)S: Deluxe.
REPEATMe too.
SYMPATHYL: Do you usually do your owncooking?QUESTION (sub:habit)S: No, I don?t.
S-DISC, HABITI always buy my meals at theconvenience store.S-DISC (sub:habit)L: I like the lunch boxes of conve-nience storesS-DISC (sub: pref(positive))Figure 5: Excerpt of listening-oriented dialoguethat participant imagined from tag sequences ofhuman conversations.
Dialogue was translatedfrom Japanese by the authors.Utterance Dialogue actL: Nice to meet you.
GREETINGWhere and who did you havedinner with today?QUESTION (sub:fact)S: I had ?niku-jaga?
(meat andbeef) with my family at home.S-DISC (sub: fact)L: Oh.
ADMIRATIONS: I think it is normal to eat withyour family at home.S-DISC (sub: pref(neutral))L: Thanks.
THANKSDo you have any brothers or sis-ters?QUESTION (sub:fact)Soon, my brother and his wifewill visit my home.S-DISC (sub: plan)S: I see.
SYMPATHYL: I want to use expensive meat inmy ?niku-jaga.
?S-DISC (sub: de-sire)Oh.
ADMIRATIONPlease give me your recipe.
QUESTION (sub:information)S: My friends claim that my?niku-jaga?
is as good as arestaurant?s.INFORMATIONL: I?d love to try it S-DISC (sub: de-sire)Figure 6: Excerpt of a listening-oriented dialoguemade from tag sequences of POMDPevaluation scores, we conclude that randomness isnecessary in non-task-oriented dialogues and thatsome randomness can be included with our ap-proach.
We do not discuss ?Rule-based?
here be-cause its tag sequence was meant to have smallperplexity.6 Conclusion and Future workThis paper investigated the possibility of automat-ically building a dialogue control module from di-alogue data to create automated listening agents.With a POMDP as a learning framework,a dialogue control module was learned fromthe listening-oriented dialogues we collected andcompared with five different systems.
OurPOMDP system showed higher performance insubjective evaluations than other statistically mo-tivated systems, such as an HMM-based one, thatwork by selecting the most likely subsequent ac-tion in the dialogue data.
When we investigatedthe output sequences of our POMDP system, thesystem frequently chose to self-disclose and ques-tion, which corresponds to human listener be-havior, as revealed in the literature (Meguro etal., 2009a).
This suggests that learning dialoguecontrol by POMDPs is achievable for listening-oriented dialogues.The main contribution of this paper is thatwe successfully showed that POMDPs can beused to train dialogue control policies for lesstask-oriented dialogue systems, such as listeningagents, where the user goals are not as clear astask-oriented ones.
We also revealed that the re-ward function can be learned effectively by ourformulation that simultaneously maximizes usersatisfaction and smoothness.
Finding an appro-priate reward function is a real challenge for lesstask-oriented dialogue systems; this work has pre-sented the first workable solution.Much work still remains.
Even though weconducted an evaluation experiment by simula-tion (i.e, offline evaluation), human dialogues ob-viously do not necessarily proceed as in simula-tions.
Therefore, we plan to evaluate our sys-tem using online evaluation, which also forces usto implement utterance understanding and gener-ation modules.
We also want to incorporate theidea of topic shift into our policy learning becausewe observed in our data that listeners frequentlychange topics to keep speakers motivated.
We arealso considering adapting the system behavior tousers.
Specifically, we want to investigate dia-logue control that adapts to the personality traitsof users because it has been found that the flowof listening-oriented dialogues differs dependingon the personality traits of users (Meguro et al,2009b).
Finally, although we only dealt with text,we also want to extend our approach to speech andother modalities, such as gestures and facial ex-pressions.768ReferencesBickmore, Timothy and Justine Cassell.
2001.
Rela-tional agents: a model and implementation of build-ing user trust.
In Proc.
SIGCHI conference onhuman factors in computing systems (CHI), pages396?403.Ferguson, George, James F. Allen, and Brad Miller.1996.
TRAINS-95: towards a mixed-initiativeplan-ning assistant.
In Proc.
Third Artificial IntelligencePlanning Systems Conference (AIPS), pages 70?77.Higashinaka, Ryuichiro, Kohji Dohsaka, and HidekiIsozaki.
2008.
Effects of self-disclosure and em-pathy in human-computer dialogue.
In Proc.
IEEEWorkshop on Spoken Language Technology (SLT),pages 108?112.Higuchi, Shinsuke, Rafal Rzepka, and Kenji Araki.2008.
A casual conversation system using modal-ity and word associations retrieved from the web.In Proc.
2008 conference on Empirical Methodsin Natural Language Processing (EMNLP), pages382?390.Hirshman, Lynette.
1989.
Overview of the DARPAspeech and natural language workshop.
In Proc.DARPA Speech and Natural Language Workshop1989, pages 1?2.Isomura, Naoki, Fujio Toriumi, and Kenichiro Ishii.2009.
Evaluation method of non-task-oriented di-alogue system using HMM.
IEICE Transactions onInformation and Systems, J92-D(4):542?551.Ivey, Allen E. and Mary Bradford Ivey.
2002.
In-tentional Interviewing and Counseling: Facilitat-ing Client Development in a Multicultural Society.Brooks/Cole Publishing Company.Jurafsky, Dan, Elizabeth Shriberg, and Debra Bi-asca, 1997.
Switchboard SWBD-DAMSL Shallow-Discourse-Function Annotation Coders Manual.Kobayashi, Yuka, Daisuke Yamamoto, ToshiyukiKoga, Sachie Yokoyama, and Miwako Doi.
2010.Design targeting voice interface robot capable ofactive listening.
In Proc.
5th ACM/IEEE inter-national conference on Human-robot interaction(HRI), pages 161?162,Maatman, R. M., Jonathan Gratch, and Stacy Marsella.2005.
Natural behavior of a listening agent.
LectureNotes in Computer Science, 3661:25?36.Meguro, Toyomi, Ryuichiro Higashinaka, KohjiDohsaka, Yasuhiro Minami, and Hideki Isozaki.2009a.
Analysis of listening-oriented dialogue forbuilding listening agents.
In Proc.
10th Annual SIG-DIAL Meeting on Discourse and Dialogue (SIG-DIAL), pages 124?127.Meguro, Toyomi, Ryuichiro Higashinaka, KohjiDohsaka, Yasuhiro Minami, and Hideki Isozaki.2009b.
Effects of personality traits on listening-oriented dialogue.
In Proc.
International Workshopon Spoken Dialogue Systems Technology (IWSDS),pages 104?107.Minami, Yasuhiro, Akira Mori, Toyomi Meguro,Ryuichiro Higashinaka, Kohji Dohsaka, and EisakuMaeda.
2009.
Dialogue control algorithm forambient intelligence based on partially observablemarkov decision processes.
In Proc.
InternationalWorkshop on Spoken Dialogue Systems Technology(IWSDS), pages 254?263.Nakano, Mikio, Noboru Miyazaki, Jun ichi Hirasawa,Kohji Dohsaka, and Takeshi Kawabata.
1999.
Un-derstanding unsegmented user utterances in real-time spoken dialogue systems.
In Proc.
37th An-nual Meeting of the Association for ComputationalLinguistics (ACL), pages 200?207.Pineau, Joelle., Geoff.
Gordon, and Sebastian Thrun.2003.
Point-based value iteration: An anytime al-gorithm for POMDPs.
In Proc.
International JointConference on Artificial Intelligence (IJCAI), pages1025?1032.Shitaoka, Kazuya, Ryoko Tokuhisa, TakayoshiYoshimura, Hiroyuki Hoshino, and NarimasaWatanabe.
2010.
Active listening system for dia-logue robot.
In JSAI SIG-SLUD Technical Report,volume 58, pages 61?66.
(in Japanese).Walker, Marilyn, Alex Rudnicky, John Aberdeen, Eliz-abeth Owen Bratt, Rashmi Prasad, Salim Roukos,Greg S, and Seneff Dave Stallard.
2002.
DARPAcommunicator evaluation: progress from 2000 to2001.
In Proc.
International Conference on SpokenLanguage Processing (ICSLP), pages 273?276.Wallace, Richard S. 2004.
The Anatomy of A.L.I.C.E.A.L.I.C.E.
Artificial Intelligence Foundation, Inc.Williams, Jason D. and Steve Young.
2007.
Par-tially observable markov decision processes for spo-ken dialog systems.
Computer Speech & Language,21(2):393?422.Yokoyama, Sachie, Daisuke Yamamoto, YukaKobayashi, and Miwako Doi.
2010.
Developmentof dialogue interface for elderly people ?switchingthe topic presenting mode and the attentive listeningmode to keep chatting?.
In IPSJ SIG TechnicalReport, volume 2010-SLP-80, pages 1?6.
(inJapanese).769
