Induction of Greedy Controllers for Deterministic TreebankParsersTom KaltDepartment of Computer ScienceUniversity of MassachusettsAmherst, MA 01003-9264kalt@cs.umass.eduAbstractMost statistical parsers have used the gram-mar induction approach, in which a stochasticgrammar is induced from a treebank.
An alter-native approach is to induce a controller for agiven parsing automaton.
Such controllers maybe stochastic; here, we focus on greedy con-trollers, which result in deterministic parsers.We use decision trees to learn the controllers.The resulting parsers are surprisingly accurateand robust, considering their speed and simplic-ity.
They are almost as fast as current part-of-speech taggers, and considerably more accuratethan a basic unlexicalized PCFG parser.
Wealso describe Markov parsing models, a generalframework for parser modeling and control, ofwhich the parsers reported here are a specialcase.1 IntroductionA fundamental result of formal language the-ory is that the languages defined by context-freegrammars are the same as those accepted bypush-down automata.
This result was recentlyextended to the stochastic case (Abney, et al,1999).
There are thus two main approaches totraining a statistical parser: inducing stochasticgrammars and inducing stochastic automata.Most recent work has employed grammar induc-tion (Collins, 1999; Charniak, 2000).
Examplesof the automaton-induction approach are Her-mjakob (1997), which described a determinis-tic parser, and Ratnaparkhi (1998), which de-scribed a stochastic parser.The deterministic parsers reported in this pa-per are greedy versions of stochastic parsersbased on Markov parsing models, described insection 3.3.
A greedy parser takes the singlemost probable action at every choice point.
Itthus does the minimum amount of search pos-sible.
There will always be a tradeoff betweenspeed on the one hand and accuracy and robust-ness on the other.
Our aim, in studying greedyparsers, is to find out what levels of coverageand accuracy can be attained at the high-speedextreme of this tradeoff.
There is no guaranteethat a greedy parser will find the best parse, orindeed any complete parse.
So the accuracy andcoverage of greedy parsers are both interestingempirical questions.
We find that they are al-most as fast as current part-of-speech taggers,and they outperform basic unlexicalized PCFGparsers.
While coverage is a concern, it is quitehigh (over 99%) for some of our parsers.2 Previous workMarkov parsing models are an example ofthe history-based parsing approach (Black, etal., 1992).
History-based parsing, broadlyinterpreted, includes most statistical parsers.Markov parsing models take a more automaton-oriented (or control-oriented) view of what his-tory means, compared to the more grammar-oriented view of the original paper and mostsubsequent work.Hermjakob (1997) described a deterministicshift-reduce parser.
The control is learned by ahybrid of decision trees and decision lists.
Thiswork also used a rich hand-crafted semantic on-tology.
The state representation contained over200 features, although it still worked rather wellwhen this was reduced to 12.
A notable featurewas that good performance was achieved withvery little training data (256 sentences).
Hisresults are not directly comparable with mostother experiments, for several reasons, includ-ing the use of a subset of the Wall St. Journalcorpus that used a closed lexicon of 3000 words.Ratnaparkhi (1999) used a maximum entropymodel to compute action probabilities for abottom-up parser.
His score function is an in-stance of a Markov parsing model, as definedin this paper (although he did not interpret hisscore as a probability).
His parser performedat a level very close to state-of-the-art.
His ap-proach was similar to ours in a number of ways.He used a beam search to find multiple parses.Wong and Wu (1999) implemented a deter-ministic shift-reduce parser, using a novel vari-ation on shift-reduce which employed a separatechunker-like phase as well as a base NP recog-nizer.
They used decision trees to learn control.Their state representation was restricted to un-lexicalized syntactic information.The approach described here combines manyelements which have previously been found use-ful.
Left-corner parsing is discussed in Man-ning and Carpenter (1997) and Roark (2001).For search, Roark used beam search with non-backtracking top down automata, rather thanthe more usual chart-based search.
Magerman(1994) used a parsing model based on decisiontree techniques.
Numerous papers (Manningand Carpenter, 1997; Johnson, 1998; Charniaket al, 1998; Roark, 2001) report that treebankbinarization is advantageous.3 Automaton Induction3.1 General ConceptsOur approach to automaton induction is to viewparsing as a control problem.
The parsing au-tomaton is a discrete dynamical system whichwe want to learn to control.
At any given time, aparser is in some state.
We use the term state asin control theory, and not as in automata theory.The state consists of the contents of the parser?sdata structures: a buffer holding the input sen-tence, a push-down stack, and a set of parsetree fragments, consisting of current or previ-ous stack items connected by arcs.
A parserhas a finite set of actions, which perform simpleoperations on its data structures (pushing andpopping the stack, dequeuing from the inputbuffer, and creating arcs between stack items).Performing an action changes the state.
Mostimportant, the parser has a controller, whichchooses an action at each time step based onthe current state.
A map from states to ac-tions is called a policy.
If this map is a func-tion, then the policy is deterministic, and re-sults in a deterministic parser.
If the map is astate-conditional distribution over actions, thenthe policy is stochastic, resulting in a stochasticparser.
A deterministic parser returns a singleparse for a given input, while a stochastic parsermay return more than one.
Thus the problemof inducing a stochastic parsing automaton con-sists in specifying the automaton?s data struc-tures, its dynamics (what the actions do), andthen learning a stochastic policy.In this paper, we assume that the parsing au-tomaton always halts and outputs a tree.
Wecan easily modify any parser so that this is thecase.
A parser fails when it is asked to per-form an action that is impossible (e.g.
shiftingwhen there is no more input).
When this hap-pens, the parser terminates by creating a rootnode labeled fail, whose children are all com-plete constituents constructed so far, along withany unused input.
Parsers can also fail by go-ing into a cycle.
This must be detected, whichcan be done by limiting the number of actionsto some multiple of the input length.
In prac-tice, we have found that cycles are rare, andnot difficult to handle.
We also assume that theparsing automaton is reversible; that is, for agiven input string, there is a one-to-one corre-spondence between parse trees and the sequenceof actions that produces that tree.
Because ofreversibility, given a parser and a tree, we caneasily determine the unique sequence of actionsthat the parser would use to produce that tree;we call this unparsing.3.2 Deterministic ControlThe parsers reported in this paper used deter-ministic controllers created as described below.Induction of deterministic control for a givenparser, using a treebank, is a straightforwardclassification problem.
We use the parser andthe treebank to create a training set of (state,action) pairs, and we induce a function fromstates to actions, which is the deterministic con-troller.
To create training instances from a tree-bank, we first unparse each tree to get an actionsequence.
We then use these actions to controlthe parser as it processes the corresponding in-put.
At each time step, we create a traininginstance, consisting of the parser?s current stateand the action it takes from that state.State: We said above that the parser?s stateconsists of the contents of its data structures;we will call this the complete state.
The statespace of the complete state is infinite, as statesinclude the contents of unbounded stacks andinput buffers.
We need to map this into a man-ageable number of equivalence classes.
This isdone in two stages.
First, we restrict attentionto a finite part of the complete state.
That is,we map particular elements of the parser?s datastructures onto a feature vector.
We refer tothis as the state representation, and the choiceof representation is a critical element of parserdesign.
All the work reported here uses twelvefeatures, which will be detailed in section 4.1.With twelve features and around 75 categori-cal feature values, the state space is still huge.The second state-space reduction is to use a de-cision tree to learn a mapping from state rep-resentations to actions.
Each leaf of the treeis an equivalence class over feature vectors andtherefore also over complete states.
The actionassigned to the leaf is the highest-frequency ac-tion found in the training instances that map tothe leaf.
Thus we have three different notionsof state: the complete state, the state represen-tation, and the state equivalence classes at theleaves of the decision tree.We use a CART-style decision tree algorithm(Brieman, et al, 1984) as our main machinelearning tool.
The training sets used here con-tained over two million instances.
The CARTalgorithm was modified to handle large train-ing sets by using samples, rather than all in-stances, to choose tests at each node of the tree.All features used were categorical, and all testswere binary.
Our decision trees had roughly 20kleaves.
Tree induction took around twenty min-utes.3.3 Markov Parsing ModelsWe define a class of conditional distributionsover parse trees which we call Markov parsingmodels (MPMs).
Consider a reversible parsingautomaton which takes a sequence of n actions(a1, a2, ...an) on an input string ?
to producea parse t. At each step, the automaton is insome state si, and in every state the automatonchooses actions according to a stochastic policyP (ai|si).
Because of reversibility, for a given in-put ?, there is an isomorphism between parsetrees and action sequences:t ?
(a1, a2, ...an)Taking probabilities,P (t|?)
= P (a1, a2, ...an|?)
(1)=n?i=1P (ai|ai?1...a1, ?)
(2)=n?i=1P (ai|si) (3)The second step merely rewrites equation1 using a probailistic identity.
In the thirdstep, replacing the history at the ith time step(ai?1, ...a1, ?)
with the state si is an expressionof the Markov property.
This is justified sincefor a reversible automaton, the action sequencedefines a unique state, and that state could onlybe reached by that action sequence.Equation 3 defines a Markov parsing model.Generative models, such as PCFGs, define ajoint distribution P (t, ?)
over trees and strings.By contrast, a parsing model defines P (t|?
),conditioned on the input string.
Assuming thatthe input string is given, a potential advantageof a conditional model over a generative one isthat it makes better use of limited training data,as it doesn?t need to model the string proba-bility.
The string probability is useful in someapplications, such as speech recognition, but itrequires extra parameters and training data tomodel it.An MPM plays two roles.
First, as in moststatistical parsers, it facilitates syntactic disam-biguation by specifying the relative likelihood ofthe various structures which might be assignedto a sentence.
Second, it is directly useful forcontrol; it tells us how to parse and search ef-ficiently.
By contrast, in some recent models(Collins, 1999; Charniak et al 1998), someevents used in the model are not available untilafter decisions are made; therefore a separate?figure of merit?
must be engineered to guidesearch.3.4 ML Estimation of MPMparametersThe parameters of an MPM can be estimatedusing a treebank.
Consider the decision-treeinduction procedure described in section 3.2.Each leaf of the tree corresponds to a state sin the model, and contains a set of training in-stances.
For each action a, the ML estimate ofP (a|s) is simply the relative frequency of thataction in the training instances at that leaf.A similar distribution can be defined for anynode in the tree, not just for leaves.
If necessary,the ML estimates can be smoothed by ?backingoff?
to the distribution at the next-higher levelin the tree.
Other smoothing methods are pos-sible as well.4 Description of parsersWe now describe the parsers we implemented.Three parsing strategies (top-down, left-corner,and shift-reduce) have been discussed exten-sively in the literature.
As there is no consensuson which is best for parsing natural language,we tried all three.
Our goal was not to directlycompare the strategies, but simply to find theone that worked best in our system.
Direct com-parison would be difficult, in particular becausethe choice of state representation has a big in-fluence on performance; and there is no obviousway of choosing the best state representation fora particular parsing strategy.The input sentences were pre-tagged usingthe MAXPOST tagger (Ratnaparkhi, 1996).All parsers here are unlexicalized, so they usepreterminals (part-of-speech tags) as their inputsymbols.
Each parser has an input (or looka-head) buffer, organized as a FIFO queue.
Eachparser also has a stack.
Stack items are labeledwith either a preterminal symbol or a nontermi-nal (a syntactic category).
The ?completeness?of a stack item is different in the three parsingstrategies (a node is considered complete if it isconnected to its yield).
Below, for conciseness,we describe some actions as having arguments;this is shorthand for the set of actions contain-ing each distinct combination of arguments.
Allthree parsers handle failure as described in sec-tion 3.1, that is, by returning a fail node whosechildren are the constituents completed so far,plus any remaining input.Even within a parsing strategy, we have con-siderable latitude in designing the dynamics ofa parser.
For example, Roark (2001) describeshow a top-down parser can be aggressive orlazy.
It is advantageous to be lazy, since de-layed predictions are made when there is bet-ter evidence for the correct prediction.
For thisand other reasons, the parsers described belowdepart somewhat from the usual textbook defi-nitions.Shift-Reduce: The SR parser?s shift ac-tion dequeues an input item and pushes it onthe stack.
The reduce(n, cat) action popsn stack symbols (n ?
1), makes them childrenof a new symbol labeled cat, and pushes thatsymbol on the stack.
The SR parser terminateswhen the input is consumed and the stack con-tains the special symbol top.
In the SR parser,all stack items are always complete; the tree un-der a stack node is not modified further.Top-Down: The TD parser has a pre-dict(list) action, where the elements of list areeither terminals or nonterminals.
The predictaction pops the stack, makes a new item for eachlist element, pushes each of these on the stackin reverse order, and makes each new item achild of the popped item.
The other action ismatch.
This action is performed if and onlyif the top-of-stack item is a preterminal.
Thestack is popped, one input symbol is dequeued,and the popped stack item is replaced in thetree by the input item.
(Our match coerces theprediction to use the input label, rather thanrequiring a match, which causes too many fail-ures.)
In the TD parser, all stack items are pre-dictions, and are incomplete in the sense thattheir yield has not been matched yet.Left-Corner: Unlike the other two strate-gies, the LC parser?s stack may contain bothcomplete and incomplete items.
Every incom-plete item is marked as such.
Also, every in-complete item has a complete left-corner (thatis, left-most child).
The LC parser has threeactions.
Shift is the same as for SR. Theproject(cat) action pops a completed itemfrom the stack, and makes it the left corner ofa new incomplete node labeled cat, which ispushed onto the stack.
Finally, the attach ac-tion finds the first incomplete item on the stack,pops all items above it, makes them its children,and marks the stack node, which is now at thetop of the stack, as complete.4.1 RepresentationTreebank Representation: Following manyprevious researchers, we binarize the treebank,as illustrated in Fig.
4.1.
There are severalreasons for doing this.
The Penn Treebank em-ploys a very flat tree style, particularly for nounphrases.
Some nodes have eight or more chil-dren.
For the SR and LC parsers, this meansthat many words must be shifted onto the stackbefore a reduce or attach action.
Binariza-tion breaks a single decision into a sequence ofsmaller ones.
Also, the parser?s data structuresare used in a more uniform way, allowing for im-provements in state representation.
For exam-ple, in binarized SR parsing, the top two stacknodes are the only candidates for reduction, andthe previous stack node always represents thephrase preceding the one being built.
For TD,binarization has the effect of delaying predic-tions.
Roark (2001) showed that this is a big ad-vantage for top-down parsing, particularly rightbinarization to nullary.
We tried several bina-rization transformations.
Unlike previous work,we labeled all nodes introduced by binarizationas e.g.
NP*, simply noting that this is a ?syn-thetic?
child of an NP.
These binarizations arereversible, and we convert back to Penn Tree-bank style before evaluation.State representation: The state represen-tation for each parser consisted of twelve cate-oldtheDT JJ NNdogNPtheDToldJJNNdogNP *NPoldJJ NNdogtheDT NP *NPoldJJtheDTNNdogNP *NPNP * theDToldJJNNdogNP *NPNP *NP *?
(a) (b) (c) (d) (e)Figure 1: Tree binarizations: (a) original; (b) left binarized (L); (c) right binarized to binary (R2);(d) right binarized to unary (R1); (e) right binarized to nullary (R0)gorical features.
Each feature is a node label,either a non-terminal (POS tag) or a terminalsymbol (syntactic category).
There are 49 dis-tinct POS tags and 28 distinct category labels.We attempted to choose the items that wouldbe the most relevant to the parsing decisions.The choices represented here are based on in-tuition along with trial and error; no system-atic attempt has been made so far to determinethe best set of features for the state representa-tions.
This is an area for future work.
We usedthe same number (twelve) for each of the threeparsers to make them roughly comparable.Each parser?s state representation containedfeatures for the first four input symbols and thetop two stack items.
The remaining features areas follows:SR: the third and fourth stack items, and theleft and right children of the first two stackitems.LC: the third and fourth stack items, the leftand right children of the first stack item,and the left children of the second and thirdstack items.TD: the first four ancestors of the first stackitem, and the first two completed phrasespreceding the first stack item (found by go-ing to the parent, then to its left child, re-turning it if the child is complete, otherwiserecursing on the parent).The choice of items to include in the state rep-resentation corresponds to choosing events forthe probabilistic models used in other statisti-cal parsers.
The different parsing strategies pro-vide different opportunities for conditioning oncontext.
This is a very rich topic which unfor-tunately we can?t explore further here.5 ResultsAll experiments were done on the standardPenn Treebank Wall St. Journal task (Marcuset al, 1993), for comparison with other work.We used sections 2-21 for training, section 0for development testing, and section 23 for finaltesting.
All preliminary experiments used thedevelopment set for testing.
We evaluated per-formance of each parser with several treebanktransforms.
Results are in Table 1.
We reportrecall and precision for all sentences with length?
100 and for all sentences with length ?
40 to-kens.
For a treebank parse T and a parse t tobe evaluated, these measures are defined asrecall = # correct constituents in t# constituents in Tprecision = # correct constituents in t# constituents in tWe followed the standard practice of ignor-ing punctuation and conflating ADVP and PRNfor purposes of evaluation.
The results reportedare for all results, not just complete parses.
Forfail nodes, the evaluation measures give par-tial credit for whatever has been completed cor-rectly.
Including incomplete parses in the re-sults tends to lower recall and precision, com-pared to the results for the complete parses only.Coverage: Coverage is the fraction of thetest set for which the parser found a completeparse.
The parsers here always return a parsetree, but some of those trees represent parsefailure, as noted earlier.
The SR-L and LC-R2 parsers have almost complete coverage, withlength ?
100 length ?
40 Words perParser Transform Coverage Recall Precision Recall Precision secondSR L 99.8 76.7 75.8 77.8 77.0 33,740SR R2 94.9 75.9 77.2 77.1 78.2 33,560SR R1 90.8 75.6 77.3 76.9 78.3 28,398LC L 95.6 71.9 71.9 72.9 72.8 25,812LC R2 99.9 73.9 74.0 74.9 75.0 24,948LC R1 96.2 74.4 74.3 75.6 75.4 21,610TD L 31.0 38.7 57.1 41.3 58.3 41,740TD R2 42.3 47.6 61.6 50.2 62.6 45,274TD R1 72.0 61.5 66.8 62.9 68.2 30,739TD R0 98.4 69.3 72.1 70.6 73.2 21,341Table 1: Parser performance on section 23 of the Penn Treebank.
Coverage, recall, and precisionare given as percentages.TD-R0 lagging slightly behind.
As in Roark(2001), increasingly aggressive binarization isbeneficial for top-down parsing, because deci-sions are delayed.
For greedy parsers with cov-erage in the high nineties, complete coveragecould be attained at minimal additional cost byusing search only for sentences where the greedyparse produced a parse failure.Accuracy: The best recall and precision re-ported here are better than a basic treebankPCFG, for which Johnson (1998) gives 69.7%and 73.5% respectively (for length ?
100), un-der identical conditions.
Our results are consid-erably below the state of the art for this task,currently around 90%, which is achieved withmuch more sophisticated probabilistic models.Considering their speed and the simplicity oftheir representations, it is remarkable that ourparsers achieve the levels of accuracy and cov-erage reported here.
Even at these speeds, im-provements in accuracy may be possible by im-proving the representation.
And of course, ac-curacy could be improved at the expense ofspeed by adding search (see section 6).
TheTD parser lags substantially behind SR in ac-curacy.
The accuracy problem for TD and itsslightly worse coverage are probably due to thesame cause.
We suspect that predictive pars-ing is inherently riskier than bottom-up pars-ing.
Unlike the other two strategies, predictionsmust sometimes be made when there is no im-mediately adjacent complete node in the tree.However, these comparisons are not conclusive,because the choice of features for the state rep-resentation may also have an important role inthe differences.Speed: Parsing speeds are reported in wordsper second.
This is exclusive of tagging time(recall that we pre-tagged our input), and alsoexclusive of IO.
Experiments were done on a1.2 GHz Athlon CPU with 1.25 GB of mem-ory running Linux.
The parsers were imple-mented in Java, including the decision tree mod-ule.
The JVM version was 1.4.2, and the JVMwas warmed up before testing for speed.
No ad-ditional effort was spent on speed optimization.Clearly, these speeds are quite fast.
A fast con-temporary tagger, TnT (Brants, 2000), whichis implemented in C, tags between 30,000 and60,000 words per second running on a Pentium500 MHz CPU.Our LC parser is slightly slower than our SRand TD parsers because LC inherently makesmore decisions per sentence than the others do.Speeds for the low-accuracy TD runs are highdue to the fact that the parser stops early whenit encounters a failure.
Comparing these speedswith other statistical parsers is somewhat prob-lematic.
Differences in CPU speeds and imple-mentation languages obscure the comparison.Moreover, many authors simply report accuracymeasures, and don?t report timing results.
Anydeterministic parser will have running time thatis linear in the size of the input, and the amountof work per input word that needs to be done issmall, dominated by the decision tree module,which is not expensive.
By contrast, most cur-rent statistical parsers lean towards the otherend of the speed-accuracy tradeoff spectrum.One paper that focuses on efficiency of sta-tistical parsing is Charniak et al (1998).
Theyused a chart parser, and measured speed in unitsof popped edges per sentence.
This correspondsclosely to the number of actions per sentencetaken by a parsing automaton.
They reportthat on average the minimum number of poppededges to create a correct parse would be 47.5.By this measure, our greedy parsers would takeon average very close to 47 actions.
They re-port 95% coverage and 75% average recall andprecision on sentences of length ?
40 with 490popped edges; this is ten times the minimumnumber of steps.
However, to get complete cov-erage, they required 1760 popped edges, whichis a factor of 37 greater than the minimum.Wong and Wu (1999) report recall and pre-cision of 78.9% and 77.7% respectively for theirdeterministic shift-reduce parser on sentences oflength ?
40, which is very similar to the accu-racy of our SR-L run.
They reported a rate of528 words per second, but did not specify thehardware configuration.6 Future workThe approach described here can be extendedin a number of ways.
As noted, a Markovparsing model can be used to guide search.We plan to add a beam search to explore thespeed-accuracy tradeoff.
Improvements in thestate representation are possible, particularlyalong the lines of linguistically-motivated tree-bank transformations, as in Klein and Man-ning (2003).
Adding a lexical component to themodel is another extension we intend to inves-tigate.7 ConclusionsDeterministic unlexicalized statistical parsershave surprisingly good accuracy and coverage,considering their speed and simplicity.
Thebest parsers reported here have almost com-plete coverage, outperform basic PCFGs, andare roughly as fast as taggers.
We described anapproach to statistical parsing based on induc-tion of stochastic automata.
We defined Markovparsing models, described how to estimate pa-rameters for them, and showed how the deter-ministic parsers we implemented are greedy ver-sions of MPM parsers.
We found that for greedyparsing, bottom-up parsing strategies seem tohave a small advantage over top-down.AcknowledgementsThanks to Brian Roark for helpful comments onthis paper.ReferencesSteven Abney, David McAllester, and FernandoPereira.
1999.
Relating Probabilistic Gram-mars and Automata.
37th Annual Meeting ofthe Association for Computational Linguis-tics: Proceedings of the Conference, pp.
542-549.E.
Black, F. Jelinek, J. Lafferty, D. M. Mager-man, R. Mercer, and S. Roukos.
1992.Towards History-based Grammars: UsingRicher Models for Probabilistic Parsing.
Pro-ceedings of the DARPA Speech and NaturalLanguage Workshop.Thorsten Brants.
2000.
TnT ?
A StatisticalPart-of-Speech Tagger.
Proceedings of theSixth Applied Natural Language ProcessingConference.Leo Brieman, Jerome H. Friedman, Richard A.Olshen, and Charles J.
Stone.
1984.
Classi-fication and Regression Trees.
Chapman &Hall.Eugene Charniak.
2000.
A Maximum-Entropy-Inspired Parser.
In Proceedings of the 1stConference of the North American Chapter ofthe Association for Computational Linguis-tics.Eugene Charniak, Sharon Goldwater, and MarkJohnson.
1998.
Edge-based best-first chartparsing.
In Proceedings of the Fourteenth Na-tional Conference on Artificial Intelligence,pages 127?133Michael Collins.
1999.
Head-Driven StatisticalModels for Natural Language Parsing.
Ph.D.Dissertation, University of Pennsylvania.Jesus Gimenez and Luis Marquez.
2003.
Fastand Accurate Part-of-Speech Tagging: TheSVM Approach Revisited .
Recent Advancesin Natural Language Processing.Ulf Hermjakob.
1997.
Learning Parse andTranslation Decisions from Examples withRich Context.
Ph.D. Dissertation, Universityof Texas.Mark Johnson.
1998.
PCFG models of linguistictree representations.
Computational Linguis-tics, 24(4):617-636.Dan Klein and Christopher D. Manning.
2002.Fast Exact Natural Language Parsing witha Factored Model.
Advances in Neural Infor-mation Processing Systems 15.Dan Klein and Christopher D. Manning.
2003.Accurate Unlexicalized Parsing.
41st AnnualMeeting of the Association for ComputationalLinguistics: Proceedings of the Conference.David M. Magerman.
1994.
Natural LanguageParsing as Statistical Pattern Recognition.Ph.D.
Dissertation, Stanford University.Mitchell P. Marcus, Beatrice Santorini, andMary Ann Marcinkiewicz.
1993.
Buildinga large annotated corpus of English: ThePenn Treebank.
Computational Linguistics,19:313?330.Adwait Ratnaparkhi.
1996.
A Maximum En-tropy Part-Of-Speech Tagger.
In Proceedingsof the 1st Conference on Empirical Methodsin Natural Language Processing.Adwait Ratnaparkhi.
1998.
Maximum EntropyModels for Natural Language Ambiguity Res-olution.
Ph.D. Dissertation.
University ofPennsylvania.Brian Roark.
2001.
Robust Probabilistic Pre-dictive Syntactic Processing: Motivations,Models, and Applications.
Ph.D. dissertation.Brown University.Aboy Wong and Dekai Wu.
1999.
Learning alightweight robust deterministic parser.
SixthEuropean Conference on Speech Communica-tion and Technology.
