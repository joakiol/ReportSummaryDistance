Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 932?937,Austin, Texas, November 1-5, 2016. c?2016 Association for Computational LinguisticsHuman Attention in Visual Question Answering:Do Humans and Deep Networks Look at the Same Regions?Abhishek Das1?
Harsh Agrawal1?
C. Lawrence Zitnick2 Devi Parikh1,3 Dhruv Batra1,31Virginia Tech 2Facebook AI Research 3Georgia Institute of Technology{abhshkdz, harsh92, parikh, dbatra}@vt.edu, zitnick@fb.comAbstractWe conduct large-scale studies on ?human at-tention?
in Visual Question Answering (VQA)to understand where humans choose to lookto answer questions about images.
We de-sign and test multiple game-inspired novelattention-annotation interfaces that require thesubject to sharpen regions of a blurred im-age to answer a question.
Thus, we in-troduce the VQA-HAT (Human ATtention)dataset.
We evaluate attention maps generatedby state-of-the-art VQA models against hu-man attention both qualitatively (via visualiza-tions) and quantitatively (via rank-order cor-relation).
Overall, our experiments show thatcurrent VQA attention models do not seem tobe looking at the same regions as humans.1 IntroductionIt helps to pay attention.
Humans have the abilityto quickly perceive a scene by selectively attendingto parts of the image instead of processing the wholescene in its entirety (Rensink, 2000).
Inspired by hu-man attention, a recent trend in computer vision anddeep learning is to build computational models of at-tention.
Given an input signal, these models learnto attend to parts of it for further processing andhave been successfully applied in machine transla-tion (Bahdanau et al, 2015; Firat et al, 2016), ob-ject recognition (Ba et al, 2015; Mnih et al, 2014;Sermanet et al, 2014), image captioning (Xu et al,2015; Cho et al, 2015) and visual question answer-ing (Yang et al, 2016; Lu et al, 2016; Xu andSaenko, 2015; Xiong et al, 2016).In this work, we study attention for the task of Vi-sual Question Answering (VQA).
Unlike image cap-tioning, where a coarse understanding of an image?Denotes equal contribution.Figure 1: Different human attention regions basedon question.
(best viewed in color)is often sufficient for producing generic descriptions(Devlin et al, 2015), visual questions selectively tar-get different areas of an image including backgrounddetails and underlying context.
This suggests that aVQA model may benefit from an explicit or implicitattention mechanism to answer a question correctly.In this work, we are interested in the following ques-tions: 1) Which image regions do humans choose tolook at in order to answer questions about images?2) Do deep VQA models with attention mechanismsattend to the same regions as humans?We design and conduct studies to collect ?humanattention maps?.
Figure 1 shows human attentionmaps on the same image for two different ques-tions.
When asked ?What type is the surface?
?, hu-mans choose to look at the floor, while attentionfor ?Which game is being played??
is concentratedaround the player and racket.These human attention maps can be used both forevaluating machine-generated attention maps andfor explicitly training attention-based models.932(a) (b) (c)Figure 2: (a-c): Column 1 shows deblurred image, and column 2 shows human attention map.Contributions.
First, we design game-inspirednovel interfaces for collecting human attention mapsof where humans choose to look to answer ques-tions from the large-scale VQA dataset (Antol et al,2015); this VQA-HAT (Human ATtention) datasetis publicly available at our project webpage1 Sec-ond, we perform qualitative and quantitative com-parison of the maps generated by state-of-the-artattention-based VQA models (Yang et al, 2016; Luet al, 2016) and a task-independent saliency base-line (Judd et al, 2009) against our human atten-tion maps through visualizations and rank-order cor-relation.
We find that machine-generated attentionmaps from the most accurate VQA model have amean rank-correlation of 0.26 with human atten-tion maps, which is worse than task-independentsaliency maps that have a mean rank-correlation of0.49.
It is well understood that task-independentsaliency maps have a ?center bias?
(Tatler, 2007;Judd et al, 2009).
After we control for this centerbias, we find that the correlation of task-independentsaliency is poor (as expected), while trends formachine-generated VQA-attention maps remain thesame, which confirms our key finding that currentVQA attention models do not seem to be looking atthe same regions as humans.2 Related WorkOur work draws on recent work in attention-basedVQA and human studies in saliency prediction.We work with the free-form and open-ended VQAdataset released by (Antol et al, 2015).VQA Models.
Attention-based models for VQAtypically use convolutional neural networks to high-1http://computing.ece.vt.edu/?abhshkdz/vqa-hatlight relevant regions of image given a question.Stacked Attention Networks (SAN) proposed in(Yang et al, 2016) use LSTM encodings of ques-tion words to produce a spatial attention distributionover the convolutional layer features of the image.Hierarchical Co-Attention Network (Lu et al, 2016)generates multiple levels of image attention basedon words, phrases and complete questions, and isthe top entry on the VQA Challenge2 as of the timeof this submission.
Another interesting approachuses question parsing to compose the neural networkfrom modules, attention being one of the sub-tasksaddressed by these modules (Andreas et al, 2016).Note that all these works are unsupervised attentionmodels, where ?attention?
is simply an intermedi-ate variable (a spatial distribution) that is producedby the model to optimize downstream loss (VQAcross-entropy).
The fact that some (it?s unclear howmany) of these spatial distributions end up beinginterpretable is simply fortuitous.
In contrast, westudy where humans choose to look to answer vi-sual questions.
These human attention maps can beused to evaluate unsupervised maps.Human Studies.
There?s a rich history of work incollecting eye tracking data from human subjectsto gain an understanding of image saliency and vi-sual perception (Jiang et al, 2014; Judd et al, 2009;Fei-Fei et al, 2007; Yarbus, 1967).
Eye trackingdata to study natural visual exploration (Jiang etal., 2014; Judd et al, 2009) is useful but difficultand expensive to collect on a large scale.
(Jiang etal., 2015) established mouse tracking as an accu-rate alternative to eye tracking for collecting atten-tion maps.
They collected large-scale attention an-notations for MS COCO (Lin et al, 2014) on Ama-2http://visualqa.org/challenge.html933zon Mechanical Turk (AMT).
While (Jiang et al,2015) studies natural exploration and collects task-independent human annotations by asking subjectsto freely move the mouse cursor to anywhere theywanted to look on a blurred image, our approach istask-driven.
(Jia Deng and Jonathan Krause and LiFei-Fei, 2013; Deng et al, 2015) leverage crowd-sourcing to help computers select discriminative fea-tures for fine-grained recognition.
They introduce anovel gamified setting where the humans can revealregions with certain penalty which ensures discrim-inative regions with assured quality.
Related to thisis the work of (von Ahn and Dabbish, 2004) who ex-plore gamification to locate objects in an image.
Tothe best of our knowledge, this is the first work tocollect human attention maps for VQA.Specifically, as described in Section 3, we collectground truth attention annotations by instructingsubjects to sharpen parts of a blurred image that areimportant for answering the questions accurately.Section 4 covers evaluation of unsupervised atten-tion maps generated by VQA models against our hu-man attention maps.3 VQA-HAT (Human ATtention) DatasetWe design and test multiple game-inspired novel in-terfaces for conducting large-scale human studies onAMT.
Our basic interface design consists of a ?de-blurring?
exercise for answering visual questions.Specifically, we present subjects with a blurred im-age and a question about the image, and ask subjectsto sharpen regions of the image that will help themanswer the question correctly, in a smooth, click-and-drag, ?coloring?
motion with the mouse.
Thesharpening is gradual: successively scrubbing thesame region progressively sharpens it.We experiment with multiple variants of the datacollection interface.
Analysis of the interfaces aswell as details of the human evaluation studies con-ducted to converge on the final interface used for re-sults in this main document have been included inthe supplement.
The human evaluation studies con-sisted of showing these attention-sharpened imagesto humans and asking them to answer the question.Based on these human studies, we pick the ?BlurredImage with Answer?
interface, where subjects wereshown the correct answer in addition to the ques-tion and blurred image, and asked to deblur as fewregions as possible such that someone can answerthe question just by looking at the sharpened re-gions.
Since the payment structure on AMT encour-age completing tasks as quickly as possible, this im-plicitly incentivizes subjects to deblur as few regionsas possible.
Our followup human studies on thesecollected maps show that other subjects are able toanswer questions based on these collected maps (de-tails in supplement).
Thus, overall we achieve a bal-ance between highlighting too little or too much.Note that the ?Blurred Image with Answer?
inter-face used to collect attention maps is a verificationtask as opposed to actual question answering.
Weshow subjects an answer and ask them to sharpenregions that will help them answer the question cor-rectly, as opposed to showing them just the ques-tion and asking them for the answer as well as rel-evant sharpened regions in the image (?Blurred Im-age without Answer?
interface).
Attention maps col-lected via this verification task ?Blurred Image withAnswer?
are more informative (in terms of humanVQA accuracy) than those collected for ?BlurredImage without Answer?
?
78.7% vs. 75.2%.We collected human attention maps for 58475 train(out of 248349 total) and 1374 val (out of 121512total) question-image pairs from the VQA dataset.This dataset is publicly available1.
Overall, we con-ducted approximately 20000 Human IntelligenceTasks (HITs) on AMT, among 800 unique workers.Figure 2 shows examples of collected human atten-tion maps.Figure 3To visualize the collected dataset, we cluster the hu-man attention maps and visualize the average atten-tion map and example questions falling in each ofthem for 6 selected clusters in Figure 3.9344 Human Attention Maps vs UnsupervisedAttention ModelsNow that we have collected these human attentionmaps, we can ask the following question ?
do unsu-pervised attention models learn to predict attentionmaps that are similar to human attention maps?
Torephrase, do neural networks look at the same re-gions as humans to answer a visual question?VQA Attention Models.
We evaluate maps gener-ated by the following unsupervised models:?
Stacked Attention Network (SAN) (Yang et al,2016) with two attention layers (SAN-2)3.?
Hierarchical Co-Attention Network(HieCoAtt) (Lu et al, 2016) with word-level(HieCoAtt-W), phrase-level (HieCoAtt-P) andquestion-level (HieCoAtt-Q) attention maps;we evaluate all three maps4.Comparison Metric: Rank Correlation.
We firstscale both the machine-generated and human atten-tion maps to 14x14, rank the pixels according totheir spatial attention and then compute correlationbetween these two ranked lists.
We choose an order-based metric so as to make the evaluation invariantto absolute spatial probability values which can bemade peaky or diffuse by tweaking a ?temperature?parameter.Table 1 shows rank-order correlation averaged overall image-question pairs on the validation set.
Wecompare with random attention maps and task-independent saliency maps generated by a modeltrained to predict human eye fixation locationswhere subjects are asked to freely view an imagefor 3 seconds (Judd et al, 2009).
Both SAN-2and HieCoAtt attention maps are positively corre-lated with human attention maps, but not as stronglyas task-independent Judd saliency maps.
Our find-ings lead to two take-away messages with signifi-cant potential impact on future research in this ac-tive field.
First, current VQA attention models donot seem to be ?looking?
at the same regions as hu-mans to produce an answer.
Second, as attention-based VQA models become more accurate (58.9%SAN?
62.1% HieCoAtt), they seem to be (slightly)better correlated with humans in terms of where they3https://github.com/zcyang/imageqa-san4https://github.com/jiasenlu/HieCoAttenVQAModel Rank-correlationSAN-2 (Yang et al, 2016) 0.249 ?
0.004HieCoAtt-W (Lu et al, 2016) 0.246 ?
0.004HieCoAtt-P (Lu et al, 2016) 0.256 ?
0.004HieCoAtt-Q (Lu et al, 2016) 0.264 ?
0.004Random 0.000 ?
0.001Judd et al (Judd et al, 2009) 0.497 ?
0.004Human 0.623 ?
0.003Table 1: Mean rank-correlation coefficients (higheris better); error bars show standard error of means.We can see that both SAN-2 and HieCoAtt attentionmaps are positively correlated with human attentionmaps, but not as strongly as task-independent Juddsaliency maps.Model Rank-correlationSAN-2 (Yang et al, 2016) 0.038 ?
0.011HieCoAtt-W (Lu et al, 2016) 0.062 ?
0.012HieCoAtt-P (Lu et al, 2016) 0.048 ?
0.010HieCoAtt-Q (Lu et al, 2016) 0.114 ?
0.012Judd et al (Judd et al, 2009) -0.063 ?
0.009Table 2: Correlation on the reduced set without cen-ter bias goes down significantly for Judd saliencysince they have a strong center bias.
Relative trendsamong SAN-2 & HieCoAtt are similar to those overthe whole validation set (reported in Table 1).look.
Our dataset will allow for a more thorough val-idation of this observation as future attention-basedVQA models are proposed.
Figure 4 shows ex-amples of human and machine-generated attentionmaps with their rank-correlation coefficients.To put these numbers in perspective, we computedinter-human agreement on the validation set by col-lecting 3 human attention maps per image-questionpair and computing mean rank-correlation, whichis 0.623.
Lastly, all reported correlations are aver-aged over 3 trials by adding random noise (order of10?14) to human attention maps to account for rank-ing variations in case of uniformly weighted regions.Center Bias.
Judd saliency maps aim to predict hu-man eye fixations during natural visual exploration.These tend to have a strong center bias (Tatler, 2007;Judd et al, 2009).
Although our human attentionmaps dataset is not an eye tracking study, the cen-935Figure 4: Random example of human attention (column 2) v/s machine-generated attention (columns 3-5)ter bias still exists albeit not as severely as in eye-tracking.
A potential source of center bias is the factthat the VQA dataset was human-generated by sub-jects looking at images.
Thus, salient objects in thecenter of the image are likely to be potential subjectsof questions.
We compute rank-correlation of a syn-thetically generated central attention map with Juddsaliency and human attention maps.
Judd saliencymaps have a mean rank-correlation of 0.877 and hu-man attention maps have a mean rank-correlation of0.458 on the validation set.To eliminate the effect of center bias in this evalua-tion, we removed human attention maps that havepositive rank-correlation with the center attentionmap.
We compute rank-correlation of machine-generated attention with human attention on this re-duced set.
See Table 2.
Mean correlation goes downsignificantly for Judd saliency maps since they havea strong center bias.
Relative trends among SAN-2& HieCoAtt are similar to those over the whole val-idation set (reported in Table 1).
HieCoAtt-Q nowhas higher correlation with human attention mapsthan Judd saliency.
Thus discounting the center bias,VQA-specific machine attention maps correlate bet-ter with VQA-specific human attention maps thantask-independent machine saliency maps.5 Conclusion & DiscussionWe introduce and release the VQA-HAT dataset1.This dataset can be used to evaluate attentionmaps generated in an unsupervised manner byattention-based VQA models, or to explicitly trainmodels with attention supervision for VQA.
Wequantify whether current attention-based VQAmodels are ?looking?
at the same regions of theimage as humans do to produce an answer.Necessary vs Sufficient Maps.
Are human atten-tion maps ?necessary?
and/or ?sufficient??
If regionshighlighted by the human attention maps are suffi-cient to answer the question accurately, then so isany region that is a superset.
For example, if atten-tion mass is concentrated on a ?cat?
for ?What animalis present in the picture?
?, then an attention map thatassigns weights to any arbitrary-sized region that in-cludes the ?cat?
is sufficient as well.
On the contrary,a necessary and sufficient attention map would bethe smallest visual region sufficient for answeringthe question accurately.
It is an ill-posed problem todefine a necessary attention map in the space of pix-els; random pixels can be blacked out and chancesare that humans would still be able to answer thequestion given the resulting subset attention map.Our work thus poses an interesting question for fu-ture work ?
what is the right semantic space in whichit is meaningful to talk about necessary and suffi-cient attention maps for humans?AcknowledgementsWe thank Jiasen Lu and Rama Vedantam for helpful sug-gestions.
This work was supported in part by the NationalScience Foundation CAREER awards to DB & DP, ArmyResearch Office YIP awards to DB & DP, ICTAS JuniorFaculty awards at VT to DB & DP, Army Research Labgrant W911NF-15-2-0080 to DP & DB, Office of NavalResearch (ONR) YIP award to DP, ONR grant N00014-14-1-0679 to DB, Alfred P. Sloan Fellowship to DP, PaulG.
Allen Family Foundation Allen Distinguished Inves-tigator award to DP, Google Faculty Research award toDP & DB, AWS in Education Research grant to DB, andNVIDIA GPU donation to DB.
The views and conclu-sions contained herein are those of the authors and shouldnot be interpreted as necessarily representing the officialpolicies or endorsements, either expressed or implied, ofthe US Government or any sponsor.936References[Andreas et al2016] Jacob Andreas, Marcus Rohrbach,Trevor Darrell, and Dan Klein.
2016.
Learning to com-pose neural networks for question answering.
In NAACLHLT.
2[Antol et al2015] Stanislaw Antol, Aishwarya Agrawal,Jiasen Lu, Margaret Mitchell, Dhruv Batra, C. LawrenceZitnick, and Devi Parikh.
2015.
VQA: Visual QuestionAnswering.
In ICCV.
2[Ba et al2015] Jimmy Lei Ba, Volodymyr Mnih, and Ko-ray Kavukcuoglu.
2015.
Multiple Object RecognitionWith Visual Attention.
In ICLR.
1[Bahdanau et al2015] Dzmitry Bahdanau, KyunghyunCho, and Yoshua Bengio.
2015.
Neural Machine Trans-lation by Jointly Learning to Align and Translate.
InICLR.
1[Cho et al2015] KyungHyun Cho, Aaron C. Courville,and Yoshua Bengio.
2015.
Describing Multimedia Con-tent using Attention-based Encoder-Decoder Networks.volume abs/1507.01053.
1[Deng et al2015] Jia Deng, Jonathan Krause, MichaelStark, and Li Fei-Fei.
2015.
Leveraging the Wisdomof the Crowd for Fine-Grained Recognition.
PAMI.
3[Devlin et al2015] Jacob Devlin, Saurabh Gupta, Ross B.Girshick, Margaret Mitchell, and C. Lawrence Zitnick.2015.
Exploring nearest neighbor approaches for imagecaptioning.
volume abs/1505.04467.
1[Fei-Fei et al2007] Li Fei-Fei, Asha Iyer, Christof Koch,and Pietro Perona.
2007.
What do we perceive in aglance of a real-world scene?
Journal of Vision, 7(1):10.2[Firat et al2016] Orhan Firat, KyungHyun Cho, andYoshua Bengio.
2016.
Multi-way, multilingual neuralmachine translation with a shared attention mechanism.volume abs/1601.01073.
1[Jia Deng and Jonathan Krause and Li Fei-Fei2013] JiaDeng and Jonathan Krause and Li Fei-Fei.
2013.
Fine-Grained Crowdsourcing for Fine-Grained Recognition.In CVPR.
3[Jiang et al2014] Ming Jiang, Juan Xu, and Qi Zhao.2014.
Saliency in Crowd.
In ECCV.
2[Jiang et al2015] Ming Jiang, Shengsheng Huang, Juany-ong Duan, and Qi Zhao.
2015.
Salicon: Saliency in con-text.
In CVPR.
2, 3[Judd et al2009] Tilke Judd, Krista Ehinger, Fre?do Du-rand, and Antonio Torralba.
2009.
Learning to predictwhere humans look.
In ICCV.
2, 4[Lin et al2014] Tsung-Yi Lin, Michael Maire, Serge Be-longie, James Hays, Pietro Perona, Deva Ramanan, PiotrDollr, and C. Lawrence Zitnick.
2014.
Microsoft COCO:Common Objects in Context.
In ECCV.
2[Lu et al2016] Jiasen Lu, Jianwei Yang, Dhruv Batra, andDevi Parikh.
2016.
Hierarchical Question-Image Co-Attention for Visual Question Answering.
In NIPS.
1, 2,4[Mnih et al2014] Volodymyr Mnih, Nicolas Heess, AlexGraves, and Koray Kavukcuoglu.
2014.
Recurrent Mod-els of Visual Attention.
In NIPS.
1[Rensink2000] Ronald A. Rensink.
2000.
The dynamicrepresentation of scenes.
Visual Cognition, 7(1-3):17?42.
1[Sermanet et al2014] Pierre Sermanet, Andrea Frome,and Esteban Real.
2014.
Attention for Fine-Grained Cat-egorization.
volume abs/1412.7054.
1[Tatler2007] Benjamin W. Tatler.
2007.
The central fixa-tion bias in scene viewing: Selecting an optimal viewingposition independently of motor biases and image featuredistributions.
Journal of Vision, 7(14):4.
2, 4[von Ahn and Dabbish2004] Luis von Ahn and LauraDabbish.
2004.
Labeling images with a computer game.In CHI.
3[Xiong et al2016] Caiming Xiong, Stephen Merity, andRichard Socher.
2016.
Dynamic memory networks forvisual and textual question answering.
In ICML.
1[Xu and Saenko2015] Huijuan Xu and Kate Saenko.2015.
Ask, attend and answer: Exploring question-guided spatial attention for visual question answering.volume abs/1511.05234.
1[Xu et al2015] Kelvin Xu, Jimmy Ba, Ryan Kiros,Kyunghyun Cho, Aaron C. Courville, Ruslan Salakhutdi-nov, Richard S. Zemel, and Yoshua Bengio.
2015.
Show,Attend and Tell: Neural Image Caption Generation withVisual Attention.
In ICML.
1[Yang et al2016] Zichao Yang, Xiaodong He, JianfengGao, Li Deng, and Alexander J. Smola.
2016.
StackedAttention Networks for Image Question Answering.
InCVPR.
1, 2, 4[Yarbus1967] A. L. Yarbus.
1967.
Eye Movements andVision.
Plenum.
New York.
2937
