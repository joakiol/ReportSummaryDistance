Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 41?50,Honolulu, October 2008. c?2008 Association for Computational LinguisticsOne-Class Clustering in the Text DomainRon BekkermanHP LaboratoriesPalo Alto, CA 94304, USAron.bekkerman@hp.comKoby CrammerUniversity of PennsylvaniaPhiladelphia, PA 19104, USAcrammer@cis.upenn.eduAbstractHaving seen a news title ?Alba denies weddingreports?, how do we infer that it is primar-ily about Jessica Alba, rather than about wed-dings or reports?
We probably realize that, in arandomly driven sentence, the word ?Alba?
isless anticipated than ?wedding?
or ?reports?,which adds value to the word ?Alba?
if used.Such anticipation can be modeled as a ratiobetween an empirical probability of the word(in a given corpus) and its estimated proba-bility in general English.
Aggregated over allwords in a document, this ratio may be usedas a measure of the document?s topicality.
As-suming that the corpus consists of on-topicand off-topic documents (we call them thecore and the noise), our goal is to determinewhich documents belong to the core.
We pro-pose two unsupervised methods for doing this.First, we assume that words are sampled i.i.d.,and propose an information-theoretic frame-work for determining the core.
Second, werelax the independence assumption and usea simple graphical model to rank documentsaccording to their likelihood of belonging tothe core.
We discuss theoretical guarantees ofthe proposed methods and show their useful-ness for Web Mining and Topic Detection andTracking (TDT).1 IntroductionMany intelligent applications in the text domain aimat determining whether a document (a sentence, asnippet etc.)
is on-topic or off-topic.
In some appli-cations, topics are explicitly given.
In binary textclassification, for example, the topic is describedin terms of positively and negatively labeled docu-ments.
In information retrieval, the topic is imposedby a query.
In many other applications, the topicis unspecified, however, its existence is assumed.Examples of such applications are within text sum-marization (extract the most topical sentences), textclustering (group documents that are close topi-cally), novelty detection (reason whether or not testdocuments are on the same topic as training docu-ments), spam filtering (reject incoming email mes-sages that are too far topically from the content of apersonal email repository), etc.Under the (standard) Bag-Of-Words (BOW) rep-resentation of a document, words are the functionalunits that bear the document?s topic.
Since somewords are topical and some are not, the problem ofdetecting on-topic documents has a dual formulationof detecting topical words.
This paper deals with thefollowing questions: (a) Which words can be con-sidered topical?
(b) How can topical words be de-tected?
(c) How can on-topic documents be detectedgiven a set of topical words?The BOW formalism is usually translated intothe generative modeling terms by representing doc-uments as multinomial word distributions.
For theon-topic/off-topic case, we assume that words in adocument are sampled from a mixture of two multi-nomials: one over topical words and another oneover general English (i.e.
the background).
Obvi-ously enough, the support of the ?topic?
multinomialis significantly smaller than the support of the back-ground.
A document?s topicality is then determinedby aggregating the topicality of its words (see belowfor details).
Note that by introducing the backgrounddistribution we refrain from explicitly modeling theclass of off-topic documents?a document is sup-posed to be off-topic if it is ?not topical enough?.Such a formulation of topicality prescribes us-ing the one-class modeling paradigm, as opposedto sticking to the binary case.
Besides being much41Figure 1: The problem of hyperspherical decision bound-aries in one-class models for text, as projected on 2D:(left) a too small portion of the core is captured; (right)too much space around the core is captured.less widely studied and therefore much more attrac-tive from the scientific point of view, one-class mod-els appear to be more adequate for many real-worldtasks, where negative examples are not straightfor-wardly observable.
One-class models separate thedesired class of data instances (the core) from otherdata instances (the noise).
Structure of noise is eitherunknown, or too complex to be explicitly modeled.One-class problems are traditionally approachedusing vector-space methods, where a convex deci-sion boundary is built around the data instances ofthe desired class, separating it from the rest of theuniverse.
In the text domain, however, those vector-space models are questionably applicable?unlikeeffective binary vector-space models.
In binarymodels, decision boundaries are linear1, whereas in(vector-space) one-class models, the boundaries areusually hyperspherical.
Intuitively, since core docu-ments tend to lie on a lower-dimensional manifold(Lebanon, 2005), inducing hyperspherical bound-aries may be sub-optimal as they tend to either cap-ture just a portion of the core, or capture too muchspace around it (see illustration in Figure 1).
Herewe propose alternative ways for detecting the core,which work well in text.One-class learning problems have been studied aseither outlier detection or identifying a small coher-ent subset.
In one-class outlier detection (Tax andDuin, 2001; Scho?lkopf et al, 2001), the goal is toidentify a few outliers from the given set of exam-ples, where the vast majority of the examples areconsidered relevant.
Alternatively, a complementarygoal is to distill a subset of relevant examples, in thespace with many outliers (Crammer and Chechik,1As such, or after applying the kernel trick (Cristianini andShawe-Taylor, 2000)2004; Gupta and Ghosh, 2005; Crammer et al,2008).
Most of the one-class approaches employ ge-ometrical concepts to capture the notion of relevancy(or irrelevancy) using either hyperplanes (Scho?lkopfet al, 2001) or hyperspheres (Tax and Duin, 2001;Crammer and Chechik, 2004; Gupta and Ghosh,2005).
In this paper we adopt the latter approach:we formulate one-class clustering in text as an opti-mization task of identifying the most coherent subset(the core) of k documents drawn from a given poolof n > k documents.2Given a collection D of on-topic and off-topicdocuments, we assume that on-topic documentsshare a portion of their vocabulary that consists of?relatively rare?
words, i.e.
words that are used in Dmore often than they are used in general English.
Wecall them topical words.
For example, if some doc-uments in D share words such as ?Bayesian?, ?clas-sifier?, ?reinforcement?
and other machine learningterms (infrequent in general English), whereas otherdocuments do not seem to share any subset of words(besides stopwords), then we conclude that the ma-chine learning documents compose the core of D,while non-machine learning documents are noise.We express the level of topicality of a word win terms of the ratio ?
(w) = p(w)q(w) , where p(w) isw?s empirical probability (in D), and q(w) is its es-timated probability in general English.
We discussan interesting characteristic of ?
(w): if D is largeenough, then, with high probability, ?
(w) values aregreater for topical words than for non-topical words.Therefore, ?
(w) can be used as a mean to measurethe topicality of w.Obviously, the quality of this measure depends onthe quality of estimating q(w), i.e.
the general En-glish word distribution, which is usually estimatedover a large text collection.
The larger the collec-tion is, the better would be the estimation.
Recently,Google has released the Web 1T dataset3 that pro-vides q(w) estimated on a text collection of one tril-lion tokens.
We use it in our experimentation.We propose two methods that use the ?
ratio to2The parameter k is analogous to the number of clusters in(multi-class) clustering, as well as to the number of outliers (Taxand Duin, 2001) or the radius of Bregmanian ball (Crammer andChechik, 2004)?in other formulations of one-class clustering.3http://www.ldc.upenn.edu/Catalog/CatalogEntry.jsp?catalogId=LDC2006T1342gzr |d| nw gzyr |d| nwFigure 2: (left) A simple generative model; (right) LatentTopic/Background model (Section 4).solve the one-class clustering problem.
First, we ex-press documents?
topicality in terms of aggregatingtheir words?
?
ratios into an information-theoretic?topicality measure?.
The core is then composedof k documents with the highest topicality measure.We show that the proposed measure is optimal forconstructing the core cluster among documents ofequal length.
However, our method is not usefulin a setup where some long documents have a top-ical portion: such documents should be consideredon-topic, but their heavy tail of background wordsovercomes the topical words?
influence.
We gener-alize our method to non-equally-long documents byfirst extracting words that are supposed to be topi-cal and then projecting documents over those words.Such projection preserves the optimality characteris-tic and results in constructing a more accurate corecluster in practice.
We call such a method of choos-ing both topical words and core documents One-Class Co-Clustering (OCCC).It turns out that our OCCC method?s performancedepends heavily on choosing the number of topicalwords.
We propose a heuristic for setting this num-ber.
As another alternative, we propose a methodthat does not require tuning this parameter: weuse words?
?
ratios to initialize an EM algorithmthat computes the likelihood of documents to be-long to the core?we then choose k documents ofmaximal likelihood.
We call this model the LatentTopic/Background (LTB) model.
LTB outperformsOCCC in most of our test cases.Our one-class clustering models have interestingcross-links with models applied to other Informa-tion Retrieval tasks.
For example, a model thatresembles our OCCC, is proposed by Zhou andCroft (2007) for query performance prediction.
Taoand Zhai (2004) describe a pseudo-relevance feed-back model that is similar to our LTB.
These typesof cross-links are common for the models that areFigure 3: (left) Words?
p(w) values when sorted by theirq(w) values; (right) words?
?
(w) values.general enough and relatively simple.
In this paperwe put particular emphasis on the simplicity of ourmodels, such that they are feasible for theoreticalanalysis as well as for efficient implementation.2 Motivation for using ?
ratiosRecall that we use the ?
(w) = p(w)q(w) ratios to expressthe level of our ?surprise?
of seeing the word w. Ahigh value of ?
(w) means that w is used in the cor-pus more frequently than in general English, which,we assume, implies that w is topical.
The more top-ical words a document contains, the more ?topical?it is?k most topical documents compose the coreDk ?
D.An important question is whether or not the ?
ra-tios are sufficient to detecting the actually topicalwords.
To address this question, let us model thecorpus D using a simple graphical model (Figure 2left).
In this model, the word distribution p(w) isrepresented as a mixture of two multinomial distri-butions: pr over a set R of topical words, and pgover all the words G ?
R in D. For each word wijin a document di, we toss a coin Zij , such that, ifZij = 1, then wij is sampled from pr, otherwise itis sampled from pg.
Define pi , p(Zij = 1).If |G| ?
|R| ?
0, and if pi ?
0, then top-ical words would tend to appear more often thannon-topical words.
However, we cannot simply baseour conclusions on word counts, as some words arenaturally more frequent than others (in general En-glish).
Figure 3 (left) illustrates this observation: itshows words?
p(w) values sorted by their q(w) val-ues.
It is hard to fit a curve that would separate be-tween R and G \R.
We notice however, that we can?flatten?
this graph by drawing ?
(w) values instead(see Figure 3 right).
Here, naturally frequent wordsare penalized by the q factor, so we can assume that,when re-normalized, ?
(w) behaves as a mixture oftwo discrete uniform distributions.
A simple thresh-old can then separate between R and G \ R.43Proposition 1 Under the uniformity assumption, itis sufficient to have a log-linear size sample (in |G|)in order to determine the setRwith high probability.See Bekkerman (2008) for the proof.
The proposi-tion states that in corpora of practical size4 the set oftopical words can be almost perfectly detected, sim-ply by taking words with the highest ?
ratios.
Con-sequently, the core Dk will consist of k documents,each of which contains more topical words than anydocument from D \ Dk.To illustrate this theoretical result, we followedthe generative process as described above, and con-structed an artificial dataset with characteristics sim-ilar to those of our WAD dataset (see Section 5.1).In particular, we fixed the size of the artificial datasetto be equal to the size of the WAD dataset (N =330, 000).
We set the ratio of topical words to 0.2and assumed uniformity of the ?
values.
In thissetup, we were able to detect the set of topical wordswith a 98.5% accuracy.2.1 Max-KL AlgorithmIn this section, we propose a simple information-theoretic algorithm for identifying the core Dk, andshow that it is optimal under the uniformity assump-tion.
Given the ?
ratios of words, the aggregatedtopicality of the corpus D can be expressed in termsof the KL-divergence:KL(p||q) =?w?Gp(w) log p(w)q(w)=?d?D,w?Gp(d,w) log p(w)q(w) .A document d?s contribution to the aggregated topi-cality measure will assess the topicality of d:KLd(p||q) =?w?Gp(d,w) log p(w)q(w) .
(1)The core Dk will be composed of documents withthe highest topicality scores.
A simple, greedy algo-rithm for detecting Dk is then:1.
Sort documents according to their topicalityvalue (1), in decreasing order.2.
Select the first k documents.4N = O(m logm), where N is the number of word tokensin D, and m = |G| is the size of the vocabulary.Since the algorithm chooses documents with highvalues of the KL divergence we call it the Max-KLalgorithm.
We now argue that it is optimal underthe uniformity assumption.
Indeed, if the corpusD is large enough, then according to Proposition 1(with high probability) any topical word w has alower ?
ratio than any non-topical word.
Assumethat all documents are of the same length (|d| is con-stant).
The Max-KL algorithm chooses documentsthat contain more topical words than any other doc-ument in the corpus?which is exactly the definitionof the core, as presented in Section 1.
We summarizethis observation in the following proposition:Proposition 2 If the corpus D is large enough, andall the documents are of the same length, then theMax-KL algorithm is optimal for the one-class clus-tering problem under the uniformity assumption.In contrast to the (quite natural) uniformity assump-tion, the all-the-same-length assumption is quite re-strictive.
Let us now propose an algorithm that over-comes this issue.3 One-Class Co-Clustering (OCCC)As accepted in Information Retrieval, we decide thata document is on-topic if it has a topical portion, nomatter how long its non-topical portion is.
There-fore, we decide about documents?
topicality basedon topical words only?non-topical words can becompletely disregarded.
This observation leads us toproposing a one-class co-clustering (OCCC) algo-rithm: we first detect the set R of topical words, rep-resent documents over R, and then detect Dk basedon the new representation.5We reexamine the document?s topicality score (1)and omit non-topical words.
The new score is then:KLrd(p||q) =?w?Rp?
(d,w) log p(w)q(w) , (2)where p?
(d,w) = p(d,w)/(?w?R p(d,w)) is ajoint distribution of documents and (only) topicalwords.
The OCCC algorithm first uses ?
(w) to5OCCC is the simplest, sequential co-clustering algorithm,where words are clustered prior to clustering documents (see,e.g., Slonim and Tishby (2000)).
In OCCC, word clustering isanalogous to feature selection.
More complex algorithms canbe considered, where this analogy is less obvious.44choose the most topical words, then it projects doc-uments on these words and apply the Max-KL algo-rithm, as summarized below:1.
Sort words according to their ?
ratios, in de-creasing order.2.
Select a subset R of the first mr words.3.
Represent documents as bags-of-words over R(delete counts of words from G \ R).4.
Sort documents according to their topicalityscore (2), in decreasing order.5.
Select a subset Dk of the first k documents.Considerations analogous to those presented in Sec-tion 2.1, lead us to the following result:Proposition 3 If the corpus D is large enough, theOCCC algorithm is optimal for one-class clusteringof documents, under the uniformity assumption.Despite its simplicity, the OCCC algorithm showsexcellent results on real-world data (see Section 5).OCCC?s time complexity is particularly appealing:O(N), where N is the number of word tokens in D.3.1 Choosing size mr of the word clusterThe choice of mr = |R| can be crucial.
We proposea useful heuristic for choosing it.
We assume thatthe distribution of ?
ratios for w ?
R is a Gaussianwith a mean ?r ?
1 and a variance ?2r , and that thedistribution of ?
ratios for w ?
G \ R is a Gaussianwith a mean ?nr = 1 and a variance ?2nr.
We alsoassume that all the words with ?
(w) < 1 are non-topical.
Since Gaussians are symmetric, we furtherassume that the number of non-topical words with?
(w) < 1 equals the number of non-topical wordswith ?
(w) ?
1.
Thus, our estimate of |G\R| is twicethe number of words with ?
(w) < 1, and then thenumber of topical words can be estimated as mr =|G| ?
2 ?#{words with ?
(w) < 1}.4 Latent Topic/Background (LTB) modelInstead of sharply thresholding topical and non-topical words, we can have them all, weighted with aprobability of being topical.
Also, we notice that ouroriginal generative model (Figure 2 left) assumesthat words are i.i.d.
sampled, which can be relaxedby deciding on the document topicality first.
In ournew generative model (Figure 2 right), for each doc-ument di, Yi is a Bernoulli random variable whereAlgorithm 1 EM algorithm for one-class clusteringusing the LTB model.Input:D ?
the dataset?
(wl) = p(wl)q(wl) ?
?
scores for each word wl|ml=1T ?
number of EM iterationsOutput: Posteriors p(Yi = 1|di,?T ) for each doc di|ni=1Initialization:for each document di initialize pi1ifor each word wl initialize p1r(wl) = ?r?
(wl);p1g(wl) = ?g?
(wl) , s.t.
?r and ?g are normalization factorsMain loop:for all t = 1, .
.
.
, T doE-step:for each document di compute ?ti = p(Yi = 1|di,?t)for each word token wij compute?tij = p(Zij = 1|Yi = 1, wij ,?t)M-step:for each document di update pit+1 = 1|di|?j ?tijfor each word wl updatept+1r (wl) =?i ?ti?j ?
(wij = wl) ?tij?i ?ti?j ?tijpt+1g (wl) =Nw ?
?i ?ti?j ?
(wij = wl) ?tijN ?
?i ?ti?j ?tijYi = 1 corresponds to di being on-topic.
As be-fore, Zij decides on the topicality of a word tokenwij , but now given Yi.
Since not all words in acore document are supposed to be topical, then foreach word of a core document we make a separatedecision (based on Zij) whether it is sampled frompr(W ) or pg(W ).
However, if a document does notbelong to the core (Yi = 0), each its word is sampledfrom pg(W ), i.e.
p(Zij = 0|Yi = 0) = 1.Inspired by Huang and Mitchell (2006), we usethe Expectation-Maximization (EM) algorithm toexactly estimate parameters of our model from thedataset.
We now describe the model parameters ?.First, the probability of any document to belong tothe core is denoted by p(Yi = 1) = kn = pd (thisparameter is fixed and will not be learnt from data).Second, for each document di, we maintain a proba-bility of each its word to be topical given that thedocument is on-topic, p(Zij = 1|Yi = 1) = piifor i = 1, .
.
.
, n. Third, for each word wl (fork = 1...m), we let p(wl|Zl = 1) = pr(wl) andp(wl|Zl = 0) = pg(wl).
The overall number of pa-45rameters is n+ 2m+ 1, one of which (pd) is preset.The dataset likelihood is then:p(D) =n?i=1[pd p(di|Yi = 1) + (1?
pd)p(di|Yi = 0)]=n?i=1?
?pd|di|?j=1[piipr(wij) + (1?
pii)pg(wij)]+(1?
pd)|di|?j=1pg(wij)??
.At each iteration t of the EM algorithm, we firstperform the E-step, where we compute the poste-rior distribution of hidden variables {Yi} and {Zij}given the current parameter values ?t and the dataD.
Then, at the M-step, we compute the new pa-rameter values ?t+1 that maximize the model log-likelihood given ?t,D and the posterior distribution.The initialization step is crucial for the EM al-gorithm.
Our pilot experimentation showed that ifdistributions pr(W ) and pg(W ) are initialized asuniform, the EM performance is close to random.Therefore, we decided to initialize word probabili-ties using normalized ?
scores.
We do not proposethe optimal way to initialize pii parameters, however,as we show later in Section 5, our LTB model ap-pears to be quite robust to the choice of pii.The EM procedure is presented in Algorithm 1.For details, see Bekkerman (2008).
After T itera-tions, we sort the documents according to ?i in de-creasing order and choose the first k documents tobe the core.
The complexity of Algorithm 1 is lin-ear: O(TN).
To avoid overfitting, we set T to be asmall number: in our experiments we fix T = 5.5 ExperimentationWe evaluate our OCCC and LTB models on two ap-plications: a Web Mining task (Section 5.1), and aTopic Detection and Tracking (TDT) (Allan, 2002)task (Section 5.2).To define our evaluation criteria, let C be the con-structed cluster and let Cr be its portion consistingof documents that actually belong to the core.
Wedefine precision as Prec = |Cr|/|C|, recall as Rec =|Cr|/k and F-measure as (2 Prec Rec)/(Prec+Rec).Unless stated otherwise, in our experiments we fix|C| = k, such that precision equals recall and is thencalled one-class clustering accuracy, or just accu-racy.We applied our one-class clustering methods infour setups:?
OCCC with the heuristic to choose mr (fromSection 3.1).?
OCCC with optimal mr. We unfairly choosethe number mr of topical words such that theresulting accuracy is maximal.
This setupcan be considered as the upper limit of theOCCC?s performance, which can be hypotheti-cally achieved if a better heuristic for choosingmr is proposed.?
LTB initialized with pii = 0.5 (for each i).As we show in Section 5.1 below, the LTBmodel demonstrates good performance withthis straightforward initialization.?
LTB initialized with pii = pd.
Quite naturally,the number of topical words in a dataset de-pends on the number of core documents.
Forexample, if the core is only 10% of a dataset, itis unrealistic to assume that 50% of all wordsare topical.
In this setup, we condition the ratioof topical words on the ratio of core documents.We compare our methods with two existing al-gorithms: (a) One-Class SVM clustering6 (Tax andDuin, 2001); (b) One-Class Rate Distortion (OC-RD) (Crammer et al, 2008).
The later is considereda state-of-the-art in one-class clustering.
Also, to es-tablish the lowest baseline, we show the result of arandom assignment of documents to the core Dk.The OC-RD algorithm is based on rate-distortiontheory and expresses the one-class problem as alossy coding of each instance into a few possibleinstance-dependent codewords.
Each document isrepresented as a distribution over words, and the KL-divergence is used as a distortion function (gener-ally, it can be any Bregman function).
The algo-rithm also uses an ?inverse temperature?
parameter(denoted by ?)
that represents the tradeoff betweencompression and distortion.
An annealing processis employed, in which the algorithm is applied witha sequence of increasing values of ?, when initial-ized with the result obtained at the previous itera-6We used Chih-Jen Lin?s LibSVM with the -s 2 parame-ter.
We provided the core size using the -n parameter.46Method WAD TWRandom assignment 38.7% 34.9?
3.1%One-class SVM 46.3% 45.2?
3.2%One-class rate distortion 48.8% 63.6?
3.5%OCCC with the mr heuristic 80.2% 61.4?
4.5%OCCC with optimal m 82.4% 68.3?
3.6%LTB initialized with pii = 0.5 79.8% 65.3?
7.3%LTB initialized with pii = pd 78.3% 68.0?
5.9%Table 1: One-class clustering accuracy of our OCCC andLTB models on the WAD and the TW detection tasks, ascompared to OC-SVM and OC-RD.
For TW, the accura-cies are macro-averaged over the 26 weekly chunks, withthe standard error of the mean presented after the ?
sign.tion.
The outcome is a sequence of cores with de-creasing sizes.
The annealing process is stoppedonce the largest core size is equal to k.5.1 Web appearance disambiguationWeb appearance disambiguation (WAD) is proposedby Bekkerman and McCallum (2005) as the problemof reasoning whether a particular mention of a per-son name in the Web refers to the person of interestor to his or her unrelated namesake.
The problem issolved given a few names of people from one socialnetwork, where the objective is to construct a clusterof Web pages that mention names of related people,while filtering out pages that mention their unrelatednamesakes.WAD is a classic one-class clustering task, thatis tackled by Bekkerman and McCallum with simu-lated one-class clustering: they use a sophisticatedagglomerative/conglomerative clustering method toconstruct multiple clusters, out of which one clusteris then selected.
They also use a simple link struc-ture (LS) analysis method that matches hyperlinksof the Web pages in order to compose a cloud ofpages that are close to each other in the Web graph.The authors suggest that the best performance canbe achieved by a hybrid of the two approaches.We test our models on the WAD dataset,7 whichconsists of 1085 Web pages that mention 12 peoplenames of AI researchers, such as Tom Mitchell andLeslie Kaelbling.
Out of the 1085 pages, 420 areon-topic, so we apply our algorithms with k = 420.At a preprocessing step, we binarize document vec-tors and remove low frequent words (both in terms7http://www.cs.umass.edu/?ronb/name_disambiguation.html# OCCC LTB1 cheyer artificial2 kachites learning3 quickreview cs4 adddoc intelligence5 aaai98 machine6 kaelbling edu7 mviews algorithms8 mlittman proceedings9 hardts computational10 meuleau reinforcement11 dipasquo papers12 shakshuki cmu13 xevil aaai14 sangkyu workshop15 gorfu kaelblingTable 2: Most highly ranked words by OCCC and LTB,on the WAD dataset.of p(w) and q(w)).
The results are summarized inthe middle column of Table 1.
We can see that bothOCCC and LTB dramatically outperform their com-petitors, while showing practically indistinguishableresults compared to each other.
Note that when thesize of the word cluster in OCCC is unfairly set toits optimal value, mr = 2200, the OCCC methodis able to gain a 2% boost.
However, for obviousreasons, the optimal value of mr may not always beobtained in practice.Table 2 lists a few most topical words accordingto the OCCC and LTB models.
The OCCC algo-rithm sorts words according to their ?
scores, suchthat words that often occur in the dataset but rarely inthe Web, are on the top of the list.
These are mostlylast names or login names of researchers, venues etc.The EM algorithm of LTB is the given ?
scores as aninput to initialize p1r(w) and p1g(w), which are thenupdated at each M-step.
In the LTB columns, wordsare sorted by p5r(w).
High quality of the LTB listis due to conditional dependencies in our generativemodel (via the Yi nodes).Solid lines in Figure 4 demonstrate the robustnessof our models to tuning their main parameters (mrfor OCCC, and the pii initialization for LTB).
As canbe seen from the left panel, OCCC shows robustperformance: the accuracy above 80% is obtainedwhen the word cluster is of any size in the 1000?3000 range.
The heuristic from Section 3.1 suggestsa cluster size of 1000.
The LTB is even more robust:practically any value of pii (besides the very largeones, pii ?
1) can be chosen.470 2500 5000 7500 100000.660.70.740.780.820.86size of word clusteraccuracy of doc clusterOCCC methodOCCCOCCC+link0 0.2 0.4 0.6 0.8 10.660.70.740.780.820.86pii parameter initializationaccuracy of doc clusterLTB methodLTBLTB+linkFigure 4: Web appearance disambiguation: (left)OCCC accuracy as a function of the word cluster size;(right) LTB accuracy over various initializations of pii pa-rameters.
The red dotted lines show the accuracy of eachmethod?s results combined with the Link Structure modelresults.
On the absolute scale, OCCC outperforms LTB,however LTB shows more robust behavior than OCCC.To perform a fair comparison of our resultswith those obtained by Bekkerman and McCal-lum (2005), we construct hybrids of their link struc-ture (LS) analysis model with our OCCC and LTB,as follows.
First, we take their LS core cluster,which consists of 360 documents.
Second, we passover all the WAD documents in the order as theywere ranked by either OCCC or LTB, and enlargethe LS core with 60 most highly ranked documentsthat did not occur in the LS core.
In either case, weend up with a hybrid core of 420 documents.Dotted lines in Figure 4 show accuracies of theresulting models.
As the F-measure of the hy-brid model proposed by Bekkerman and McCal-lum (2005) is 80.3%, we can see that it is signifi-cantly inferior to the results of either OCCC+LS orLTB+LS, when their parameters are set to a smallvalue (mr < 3000 for OCCC, pii < 0.06 forLTB).
Such a choice of parameter values can beexplained by the fact that we need only 60 docu-ments to expand the LS core cluster to the requiredsize k = 420.
When the values of mr and pii aresmall, both OCCC and LTB are able to build verysmall and very precise core clusters, which is exactlywhat we need here.
The OCCC+LS hybrid is par-ticularly successful, because it uses non-canonicalwords (see Table 2) to compose a clean core that al-most does not overlap with the LS core.
Remark-ably, the OCCC+LS model obtains 86.4% accuracywith mr = 100, which is the state-of-the-art resulton the WAD dataset.200 400 600 800 10000.60.70.80.90.5 document cluster sizeF?measureOCCCOCCLTBFigure 5: Web appearance disambiguation: F-measureas a function of document cluster size: a vertical line in-dicates the point where precision equals recall (and there-fore equals accuracy).
?OCC?
refers to the OCCC modelwhere all the words are taken as the word cluster (i.e.
noword filtering is done).To answer the question how much our models aresensitive to the choice of the core size k, we com-puted the F-measure of both OCCC and LTB as afunction of k (Figure 5).
It turns out that our meth-ods are quite robust to tuning k: choosing any valuein the 300?500 range leads to good results.5.2 Detecting the topic of the weekReal-world data rarely consists of a clean core anduniformly distributed noise.
Usually, the noise hassome structure, namely, it may contain coherentcomponents.
With this respect, one-class clusteringcan be used to detect the largest coherent compo-nent in a dataset, which is an integral part of manyapplications.
In this section, we solve the problem ofautomatically detecting the Topic of the Week (TW)in a newswire stream, i.e.
detecting all articles in aweekly news roundup that refer to the most broadlydiscussed event.We evaluate the TW detection task on the bench-mark TDT-5 dataset8, which consists of 250 newsevents spread over a time period of half a year, and9,812 documents in English, Arabic and Chinese(translated to English), annotated by their relation-ship to those events.9 The largest event in TDT-5dataset (#55106, titled ?Bombing in Riyadh, SaudiArabia?)
has 1,144 documents, while 66 out of the250 events have only one document each.
We splitthe dataset to 26 weekly chunks (to have 26 full8http://projects.ldc.upenn.edu/TDT5/9We take into account only labeled documents, while ignor-ing unlabeled documents that can be found in the TDT-5 data.481 2 3 4 5 6 7 8 9 10 11 12 1300.51weekaccuracyPerformance of OCCC and LTB on the "topic of the week" task14 15 16 17 18 19 20 21 22 23 24 25 2600.51weekaccuracyOCCC with the mr heuristicOCCC with the optimal mrLTB initialized with pii = 0.5LTB initialized with pii = pdFigure 6: ?Topic of the week?
detection task: Accuracies of two OCCC methods and two LTB methods.weeks, we delete all the documents dated with thelast day in the dataset, which decreases the dataset?ssize to 9,781 documents).
Each chunk contains from138 to 1292 documents.The one-class clustering accuracies, macro-averaged over the 26 weekly chunks, are presentedin the right column of Table 1.
As we can see, bothLTB models, as well as OCCC with the optimal mr,outperform our baselines.
Interestingly, even the op-timal choice of mr does not lead OCCC to signif-icantly superior results while compared with LTB.The dataset-dependent initialization of LTB?s pii pa-rameters (pii = pd) appears to be preferable over thedataset-independent one (pii = 0.5).Accuracies per week are shown in Figure 6.
Theseresults reveal two interesting observations.
First,OCCC tends to outperform LTB only on data chunkswhere the results are quite low in general (less than60% accuracy).
Specifically, on weeks 2, 4, 11,and 16 the LTB models show extremely poor per-formance.
While investigating this phenomenon, wediscovered that in two of the four cases LTB wasable to construct very clean core clusters, however,those clusters corresponded to the second largesttopic, while we evaluate our methods on the firstlargest topic.10 Second, the (completely unsuper-10For example, on the week-4 data, topic #55077 (?Riverferry sinks on Bangladeshi river?)
was discovered by LTB asthe largest and most coherent one.
However, in that dataset,topic #55077 is represented by 20 documents, while topic#55063 (?SARS Quarantined medics in Taiwan protest?)
isrepresented by 27 documents, such that topic #55077 is in factthe second largest one.vised) LTB model can obtain very good results onsome of the data chunks.
For example, on weeks 5,8, 19, 21, 23, 24, and 25 the LTB?s accuracy is above90%, with a striking 100% on week-23.6 ConclusionWe have developed the theory and proposed practi-cal methods for one-class clustering in the text do-main.
The proposed algorithms are very simple,very efficient and still surprisingly effective.
Moresophisticated algorithms (e.g.
an iterative11 versionof OCCC) are emerging.7 AcknowledgementsWe thank Erik Learned-Miller for the inspirationon this project.
We also thank Gunjan Gupta,James Allan, and Fernando Diaz for fruitful dis-cussions.
This work was supported in part by theCenter for Intelligent Information Retrieval and inpart by the Defense Advanced Research ProjectsAgency (DARPA) under contract number HR0011-06-C-0023.
Any opinions, findings and conclusionsor recommendations expressed in this material arethe authors?
and do not necessarily reflect those ofthe sponsor.ReferencesJ.
Allan, editor.
2002.
Topic detection and tracking:event-based information organization.
Kluwer Aca-demic Publishers.11See, e.g., El-Yaniv and Souroujon (2001)49R.
Bekkerman and A. McCallum.
2005.
Disambiguat-ing web appearances of people in a social network.
InProceedings of WWW-05, the 14th International WorldWide Web Conference.R.
Bekkerman.
2008.
Combinatorial Markov RandomFields and their Applications to Information Organi-zation.
Ph.D. thesis, University of Massachusetts atAmherst.K.
Crammer and G. Chechik.
2004.
A needle in ahaystack: local one-class optimization.
In Proceed-ings of the 21st International Conference on MachineLearning.K.
Crammer, P. Talukdar, and F. Pereira.
2008.
A rate-distortion one-class model and its applications to clus-tering.
In Proceedings of the 25st International Con-ference on Machine Learning.N.
Cristianini and J. Shawe-Taylor.
2000.
An In-troduction to Support Vector Machines and OtherKernel-based Learning Methods.
Cambridge Univer-sity Press.R.
El-Yaniv and O. Souroujon.
2001.
Iterative doubleclustering for unsupervised and semi-supervised learn-ing.
In Advances in Neural Information ProcessingSystems (NIPS-14).G.
Gupta and J. Ghosh.
2005.
Robust one-class cluster-ing using hybrid global and local search.
In Proceed-ings of the 22nd International Conference on MachineLearning, pages 273?280.Y.
Huang and T. Mitchell.
2006.
Text clustering with ex-tended user feedback.
In Proceedings of the 29th an-nual international ACM SIGIR conference, pages 413?420.G.
Lebanon.
2005.
Riemannian Geometry and StatisticalMachine Learning.
Ph.D. thesis, CMU.B.
Scho?lkopf, J. C. Platt, J. C. Shawe-Taylor, A. J. Smola,and R. C. Williamson.
2001.
Estimating the supportof a high-dimensional distribution.
Neural Computa-tion, 13(7):1443?1471.N.
Slonim and N. Tishby.
2000.
Document cluster-ing using word clusters via the information bottleneckmethod.
In Proceedings of the 23rd annual interna-tional ACM SIGIR conference, pages 208?215.T.
Tao and C. Zhai.
2004.
A two-stage mixture model forpseudo feedback.
In Proceedings of the 27th annualinternational ACM SIGIR conference, pages 486?487.D.
M. J.
Tax and R. P. W. Duin.
2001.
Outliers anddata descriptions.
In Proceedings of the 7th AnnualConference of the Advanced School for Computing andImaging, pages 234?241.Y.
Zhou and W. B. Croft.
2007.
Query performance pre-diction in web search environments.
In Proceedingsof the 30th Annual International ACM SIGIR Confer-ence.50
