Proceedings of the Workshop on Textual Entailment and Paraphrasing, pages 1?9,Prague, June 2007. c?2007 Association for Computational LinguisticsThe Third PASCAL Recognizing Textual Entailment ChallengeDanilo GiampiccoloCELCTVia alla Cascata 56/c38100 POVO TNgiampiccolo@celct.itBernardo MagniniFBK-ITCVia Sommarive 18,38100 Povo TNmagnini@itc.itIdo DaganComputer Science DepartmentBar-Ilan UniversityRamat Gan 52900, Israeldagan@macs.biu.ac.ilBill DolanMicrosoft ResearchRedmond, WA, 98052, USAbilldol@microsoft.comAbstractThis paper presents the Third PASCALRecognising Textual Entailment Chal-lenge (RTE-3), providing an overview ofthe dataset creating methodology and thesubmitted systems.
In creating thisyear?s dataset, a number of longer textswere introduced to make the challengemore oriented to realistic scenarios.
Ad-ditionally, a pool of resources was of-fered so that the participants could sharecommon tools.
A pilot task was also setup, aimed at differentiating unknown en-tailments from identified contradictionsand providing justifications for overallsystem decisions.
26 participants submit-ted 44 runs, using different approachesand generally presenting new entailmentmodels and achieving higher scores thanin the previous challenges.1.1 The RTE challengesThe goal of the RTE challenges has been to cre-ate a benchmark task dedicated to textual en-tailment ?
recognizing that the meaning of onetext is entailed, i.e.
can be inferred, by another1.In the recent years, this task has raised great in-terest since applied semantic inference concernsmany practical Natural Language Processing(NLP) applications, such as Question Answering(QA), Information Extraction (IE), Summariza-tion, Machine Translation and Paraphrasing, andcertain types of queries in Information Retrieval(IR).
More specifically, the RTE challengeshave aimed to focus research and evaluation onthis common underlying semantic inference taskand separate it from other problems that differ-ent NLP applications need to handle.
For exam-ple, in addition to textual entailment, QA sys-tems need to handle issues such as answer re-trieval and question type recognition.By separating out the general problem of tex-tual entailment from these task-specific prob-lems, progress on semantic inference for manyapplication areas can be promoted.
Hopefully,research on textual entailment will finally lead tothe development of entailment ?engines?, whichcan be used as a standard module in many appli-cations (similar to the role of part-of-speech tag-gers and syntactic parsers in current NLP appli-cations).In the following sections, a detailed descrip-tion of RTE-3 is presented.
After a quick review1The task was first defined by Dagan and Glickman(2004).1of the previous challenges (1.2), section 2 de-scribes the preparation of the dataset.
In section3 the evaluation process and the results are pre-sented, together with an analysis of the perform-ance of the participating systems.1.2  The First and Second RTE ChallengesThe first RTE challenge2 aimed to provide theNLP community with a new benchmark to testprogress in recognizing textual entailment, andto compare the achievements of different groups.This goal proved to be of great interest, and thecommunity's response encouraged the gradualexpansion of the scope of the original task.The Second RTE challenge3 built on the suc-cess of the first, with 23 groups from around theworld (as compared to 17 for the first challenge)submitting the results of their systems.
Repre-sentatives of participating groups presented theirwork at the PASCAL Challenges Workshop inApril 2006 in Venice, Italy.
The event was suc-cessful and the number of participants and theircontributions to the discussion demonstrated thatTextual Entailment is a quickly growing field ofNLP research.
In addition, the workshopsspawned an impressive number of publicationsin major conferences, with more work in pro-gress.
Another encouraging sign of the growinginterest in the RTE challenge was represented bythe increase in the number of downloads of thechallenge datasets, with about 150 registereddownloads for the RTE-2 development set.1.3 The Third ChallengeRTE-3 followed the same basic structure of theprevious campaigns, in order to facilitate theparticipation of newcomers and to allow "veter-ans" to assess the improvements of their systemsin a comparable test exercise.
Nevertheless,some innovations were introduced, on the onehand to make the challenge more stimulatingand, on the other, to encourage collaborationbetween system developers.
In particular, a lim-ited number of longer texts, i.e.
up to a para-graph in length, were incorporated in order tomove toward more comprehensive scenarios,2http://www.pascal-network.org/Challenges/RTE/.3http://www.pascal-network.org/Challenges/RTE2./which incorporate the need for discourse analy-sis.
However, the majority of examples re-mained similar to those in the previous chal-lenges, providing pairs with relatively shorttexts.Another innovation was represented by a re-source pool4, where contributors had the possi-bility to share the resources they used.
In fact,one of the key conclusions at the second RTEChallenge Workshop was that entailment model-ing requires vast knowledge resources that cor-respond to different types of entailment reason-ing.
Moreover, entailment systems also utilizegeneral NLP tools such as POS taggers, parsersand named-entity recognizers, sometimes posingspecialized requirements to such tools.
In re-sponse to these demands, the RTE ResourcePool was built, which may serve as a portal andforum for publicizing and tracking resources,and reporting on their use.In addition, an optional pilot task, called "Ex-tending the Evaluation of Inferences from Texts"was set up by the US National Institute of Stan-dards and Technology (NIST), in order to ex-plore two other sub-tasks closely related to tex-tual entailment: differentiating unknown entail-ments from identified contradictions and provid-ing justifications for system decisions.
In thefirst sub-task, the idea was to drive systems tomake more precise informational distinctions,taking a three-way decision between "YES","NO" and "UNKNOWN?, so that a hypothesisbeing unknown on the basis of a text would bedistinguished from a hypothesis being shownfalse/contradicted by a text.
As for the other sub-task, the goal for providing justifications for de-cisions was to explore how eventual users oftools incorporating entailment can be made tounderstand how decisions were reached by asystem, as users are unlikely to trust a systemthat gives no explanation for its decisions.
Thepilot task exploited the existing RTE-3 Chal-lenge infrastructure and evaluation process byusing the same test set, while utilizing humanassessments for the new sub-tasks.4 http://aclweb.org/aclwiki/index.php?title=Textual_Entailment_Resource_Pool.2Table 1: Some examples taken from the Development Set.2 The RTE-3 Dataset2.1 OverviewThe textual entailment recognition task required theparticipating systems to decide, given two textsnippets t and h, whether t entails h. Textual en-tailment is defined as a directional relation betweentwo text fragments, called text (t, the entailingtext), and hypothesis (h, the entailed text), so that ahuman being, with common understanding of lan-guage and common background knowledge, caninfer that h is most likely true on the basis of thecontent of t.As in the previous challenges, the RTE-3 datasetconsisted of 1600 text-hypothesis pairs, equallydivided into a development set and a test set.
Whilethe length of the hypotheses (h) was  the same as inthe past datasets, a certain number of texts (t) werelonger than in previous datasets, up to a paragraph.The longer texts were marked as L, after being se-lected automatically when exceeding 270 bytes.
Inthe test set they were about 17% of the total.As in RTE-2, four applications ?
namely IE, IR,QA and SUM ?
were considered as settings or con-texts for the pairs generation (see 2.2 for a detaileddescription).
200 pairs were selected for each ap-plication in each dataset.
Although the datasetswere supposed to be perfectly balanced, the num-ber of negative examples were slightly higher inboth development and test sets (51.50% and51.25% respectively; this was unintentional).
Posi-tive entailment examples, where t entailed h, wereannotated YES; the negative ones, where entailmentdid not hold, NO.
Each pair was annotated with itsTASK TEXT HYPOTHESIS ENTAILMENTIE At the same time the Italian digital rights group, Elec-tronic Frontiers Italy, has asked the nation's governmentto investigate Sony over its use of anti-piracy software.Italy's govern-ment investigatesSony.NOIE Parviz Davudi was representing Iran at a meeting of theShanghai Co-operation Organisation (SCO), the fledg-ling association that binds Russia, China and four for-mer Soviet republics of central Asia together to fightterrorismChina is a mem-ber of SCO.YESIR Between March and June, scientific observers say, up to300,000 seals are killed.
In Canada, seal-hunting meansjobs, but opponents say it is vicious and endangers thespecies, also threatened by global warmingHunting endan-gers seal species.YESIR The Italian parliament may approve a draft law allow-ing descendants of the exiled royal family to returnhome.
The family was banished after the Second WorldWar because of the King's collusion with the fascistregime, but moves were introduced this year to allowtheir return.Italian royal fam-ily returns home.NOQA Aeschylus is often called the father of Greek tragedy;he wrote the earliest complete plays which survive fromancient Greece.
He is known to have written more than90 plays, though only seven survive.
The most famousof these are the trilogy known as Orestia.
Also well-known are The Persians and Prometheus Bound.
"The Persians"was written byAeschylus.YESSUM A Pentagon committee and the congressionally char-tered Iraq Study Group have been preparing reports forBush, and Iran has asked the presidents of Iraq andSyria to meet in Tehran.Bush will meetthe presidents ofIraq and Syria inTehran.NO3related task (IE/IR/QA/SUM) and entailmentjudgment (YES/NO, obviously released only in thedevelopment set).
Table 1 shows some examplestaken from the development set.The examples in the dataset were based mostlyon outputs (both correct and incorrect) of Web-based systems.
In order to avoid copyright prob-lems, input data was limited to either what had al-ready been publicly released by official competi-tions or else was drawn from freely availablesources such as WikiNews and Wikipedia.In choosing the pairs, the following judgmentcriteria and guidelines were considered:?
As entailment is a directional relation, thehypothesis must be entailed by the giventext, but the text need not be entailed bythe hypothesis.?
The hypothesis must be fully entailed bythe text.
Judgment must be NO if the hy-pothesis includes parts that cannot be in-ferred from the text.?
Cases in which inference is very probable(but not completely certain) were judged asYES.?
Common world knowledge was assumed,e.g.
the capital of a country is situated inthat country, the prime minister of a state isalso a citizen of that state, and so on.2.2 Pair CollectionAs in RTE-2, human annotators generated t-h pairswithin 4 application settings.The IE task was inspired by the Information Ex-traction (and Relation Extraction) application,where texts and structured templates were replacedby t-h pairs.
As in the 2006 campaign, the pairswere generated using four different approaches:1) Hypotheses were taken from the relationstested in the ACE-2004 RDR task, whiletexts were extracted from the outputs of ac-tual IE systems, which were provided withrelevant news articles.
Correctly extractedinstances were used to generate positiveexamples and incorrect instances to gener-ate negative examples.2) The same procedure was followed usingoutput of IE systems on the dataset of theMUC-4 TST3 task, in which the events areacts of terrorism.3) The annotated MUC-4 dataset and thenews articles were also used to manuallygenerate entailment pairs based on ACE re-lations.4) Hypotheses corresponding to relations notfound in the ACE and MUC datasets  wereused both to be given to IE systems and tomanually generate t-h pairs from collectednews articles.
Examples of these relations,taken from various semantic fields, were?X beat Y?, ?X invented Y?, ?X steal Y?etc.The common aim of all these processes was tosimulate the need of IE systems to recognize thatthe given text indeed entails the semantic relationthat is expected to hold between the candidate tem-plate slot fillers.In the IR (Information Retrieval) application set-ting, the hypotheses were propositional IR queries,which specify some statement, e.g.
?robots areused to find avalanche victims?.
The hypotheseswere adapted and simplified from standard IRevaluation datasets (TREC and CLEF).
Texts (t)that did or did not entail the hypotheses were se-lected from documents retrieved by different searchengines (e.g.
Google, Yahoo and MSN) for eachhypothesis.
In this application setting it was as-sumed that relevant documents (from an IR per-spective) should entail the given propositional hy-pothesis.For the QA (Question Answering) task, annotatorsused questions taken from the datasets of officialQA competitions, such as TREC QA andQA@CLEF datasets, and the corresponding an-swers extracted from the Web by actual QA sys-tems.
Then they transformed the question-answerpairs into t-h pairs as follows:?
An answer term of the expected answertype was picked from the answer passage -either a correct or an incorrect one.?
The question was turned into an affirma-tive sentence plugging in the answer term.?
t-h pairs were generate, using the affirma-tive sentences as hypotheses (h?s) and theoriginal answer passages as texts (t?s).4For example, given the question ?How high isMount Everest??
and a text (t) ?The above men-tioned expedition team comprising of 10 memberswas permitted to climb 8848m.
high Mt.
Everestfrom Normal Route for the period of 75 days from15 April, 2007 under the leadership of Mr. WolfHerbert of Austria?, the annotator, extracting thepiece of information ?8848m.?
from the text,would turn the question into an the affirmative sen-tence ?Mount Everest is 8848m high?, generating apositive entailment pair.
This process simulated theneed of a QA system to verify that the retrievedpassage text actually entailed the provided answer.In the SUM (Summarization) setting, theentailment pairs were generated using two proce-dures.In the first one, t?s and h?s were sentences takenfrom a news document cluster, a collection of newsarticles that describe the same news item.
Annota-tors were given the output of multi-documentsummarization systems -including the documentclusters and the summary generated for each clus-ter.
Then they picked sentence pairs with high lexi-cal overlap, preferably where at least one of thesentences was taken from the summary (this sen-tence usually played the role of t).
For positive ex-amples, the hypothesis was simplified by removingsentence parts, until it was fully entailed by t.Negative examples were simplified in a similarmanner.
In alternative, ?pyramids?
produced forthe experimental evaluation mehod in DUC 2005(Passonneau et al 2005) were exploited.
In thisnew evaluation method, humans select sub-sentential content units (SCUs) in several manuallyproduced summaries on a subject, and collocatethem in a ?pyramid?, which has at the top theSCUs with the higher frequency, i.e.
those whichare present in most summaries.
Each SCU is identi-fied by a label, a sentence in natural languagewhich expresses the content.
Afterwards, the anno-tators individuate the SCUs present in summariesgenerated automatically (called peers), and linkthem to the ones present in the pyramid, in order toassign each peer a weight.
In this way, the SCUs inthe automatic summaries linked to the SCUs in thehigher tiers of the pyramid are assigned a heavierweight than those at the bottom.
For the SUM set-ting, the RTE-3 annotators selected relevant pas-sages from the peers and used them as T?s, mean-while the labels of the corresponding SCUs wereused as H?s.
Small adjustments were allowed,whenever the texts were not grammatically accept-able.
This process simulated the need of a summa-rization system to identify information redundancy,which should be avoided in the summary.2.3 Final datasetEach pair of the dataset was judged by three anno-tators.
As in previous challenges, pairs on whichthe annotators disagreed were filtered-out.On the test set, the average agreement betweeneach pair of annotators who shared at least 100 ex-amples was 87.8%, with an average Kappa level of0.75, regarded as substantial agreement accordingto Landis and Koch (1997).19.2 % of the pairs in the dataset were removedfrom the test set due to disagreement.
The dis-agreement was generally due to the fact that the hwas more specific than the t, for example becauseit contained more information, or made an absoluteassertion where t proposed only a personal opinion.In addition, 9.4 % of the remaining pairs were dis-carded, as they seemed controversial, too difficult,or too similar when compared to other pairs.As far as the texts extracted from the web areconcerned, spelling and punctuation errors weresometimes fixed by the annotators, but no majorchange was allowed, so that the language could begrammatically and stylistically imperfect.
The hy-potheses were finally double-checked by a nativeEnglish speaker.3 The RTE-3 Challenge3.1 Evaluation measuresThe evaluation of all runs submitted in RTE-3 wasautomatic.
The judgments (classifications) returnedby the system were compared to the Gold Standardcompiled by the human assessors.
The mainevaluation measure was accuracy, i.e.
the percent-age of matching judgments.For systems that provided a confidence-rankedlist of the pairs, in addition to the YES/NO judg-ment, an Average Precision measure was alsocomputed.
This measure evaluates the ability ofsystems to rank all the T-H pairs in the test set ac-cording to their entailment confidence (in decreas-ing order from the most certain entailment to theleast certain).
Average precision is computed as the5average of the system's precision values at allpoints in the ranked list in which recall increases,that is at all points in the ranked list for which thegold standard annotation is YES, or, more for-mally:?=?ni iiUpToPairEntailmentiER 1)(#)(1(1)where n is the number of the pairs in the test set, Ris the total number of positive pairs in the test set,E(i) is 1 if the i-th pair is positive and 0 otherwise,and i ranges over the pairs, ordered by their rank-ing.In other words, the more the system was confi-dent that t entails h, the higher was the ranking ofthe pair.
A perfect ranking would have placed allthe positive pairs (for which the entailment holds)before all the negative ones, yielding an averageprecision value of 1.3.2 Submitted systemsTwenty-six teams participated in the third chal-lenge, three more than in previous year.
Table 2presents the list of the results of each submittedruns and the components used by the systems.Overall, we noticed a move toward deep ap-proaches, with a general consolidation of ap-proaches based on the syntactic structure of Textand Hypothesis.
There is an evident increase ofsystems using some form of logical inferences (atleast seven systems).
However, these approaches,with few notably exceptions, do not seem to beconsolidated enough, as several systems show re-sults  not still at the state of art (e.g.
Natural Logicintroduced by Chambers et al).
For many systemsan open issue is the availability and integration ofdifferent and complex semantic resources-A more extensive and fine grained use of spe-cific semantic phenomena is also emerging.
As anexample, Tatu and Moldovan carry on a sophisti-cated analysis of named entities, in particular Per-son names, distinguishing first names from lastnames.
Some form of relation extraction, eitherthrough manually built patterns (Chambers et al)or through the use of an information extraction sys-tem (Hickl and Bensley) have been introduced thisyear, even if still on a small scale (i.e.
few rela-tions).On the other hand, RTE-3 confirmed that bothmachine learning using lexical-syntactic featuresand transformation-based approaches on depend-ency representations are well consolidated tech-niques to address textual entailment.
The extensionof transformation-based approaches toward prob-abilistic settings is an interesting direction investi-gated by some systems (e.g.
Harmeling).
On theside of ?light?
approaches to textual entailment,Malakasiotis and Androutpoulos provide a usefulbaseline for the task (0.61%) using only POS tag-ging and then applying string-based measures toestimate the similarity between Text and Hypothe-sis.As far as resources are concerned, lexical data-bases (mostly WordNet and DIRT) are still widelyused.
Extended WordNet is also a common re-source (for instance in Iftene and Balahur-Dobrescu) and the Extended Wordnet KnowledgeBase has been successfully used in (Tatu andMoldovan).
Verb-oriented resources are alsolargely present in several systems, including Fra-menet (e.g.
Burchardt et al), Verbnet (Bobrow etal.)
and Propbank (e.g.
Adams et al).
It seems thatthe use of the Web as a resource is more limitedwhen compared to the previous RTE workshop.However, as in RTE-2, the use of large semanticresources is still a crucial factor affecting the per-formance of systems (see, for instance, the use of alarge corpus of entailment examples in Hickl andBensley).Finally, an interesting aspect is that, stimulatedby the percentage of longer texts included this year,a number of participating systems addressed anaph-ora resolution (e.g.
Delmonte, Bar-Haim et al,Iftene and Balahur-Dobrescu).3.3 ResultsThe accuracy achieved by the participating sys-tems ranges from 49% to 80% (considering the bestrun of each group), while most of the systems ob-tained a score in between 59% and 66%.
One sub-mission, Hickl and Bensley achieved 80% accu-racy, scoring 8% higher than the second system(Tatu and Moldovan, 72%), and obtaining the bestabsolute result achieved in the three RTE chal-lenges.6Table 2: Submission results and components of the systems..System ComponentsFirst Author AccuracyAverageprecision LexicalRelation,WordNetn-gram\wordsimilaritySyntacticMatch-ing\AligningSemanticRoleLabeling\Framenet\Probank,VerbnetLogical InferenceCorpus/Web-basedStatistics,LSAMLClassificationAnaphoraresolutionEntailmentCorpora?DIRTBackgroundKnowledgeAdams 0.6700  X X    X X0.6112 0.6118 X  X   X  X X Bar-Haim0.5837 0.6093  X  X   X  XBaral 0.4963 0.5364 X    X    X0.6050 0.5897 X  X    X   Blake0.6587 0.6096 X  X    X0.5112 0.5720  X   X X     Bobrow0.5150 0.5807 X   X X0.6250  X  X X      Burchardt0.62620.5500   X    X    Burek0.5500 0.55140.6050 0.6341 X  X  X  X X  Chambers0.6362 0.6527 X  X  X  X X0.5088 0.4961  XX    X Clark0.4725 0.4961  X    X    XDelmonte 0.5875 0.5830 X  X X X   X0.6563  X X X       Ferrandez0.63750.6062  X X     X   Ferr?s0.6150  X X     X0.5600 0.5813 X  X    X   Harmling0.5775 0.5952 X  X    XHickl 0.8000 0.8815 X X   X  X X X0.6913  X  X      X Iftene0.6913  X  X      X0.6400  X X     X   Li0.6488Litkowski   0.6125Malakasiotis  0.6175 0.6808  X     XMarsi 0.5913    X      X0.5888  X X X    X   Montejo-R?ez0.6038  X X X    X0.6238  X X X    X   Rodrigo0.6312  X X X    X0.6262  X X       X Roth0.5975    X     X0.6100 0.6195 X X     X   Settembre0.6262 0.6274 X X     X0.7225 0.6942 X    X   X X Tatu0.7175 0.6797 X    X   X0.6650    X    X   Wang0.66870.6675 0.6674 X  X    X   Zanzotto0.6575 0.6732 X  X    X7As far as the per-task results are concerned, thetrend registered in RTE-2 was confirmed, in thatthere was a marked difference in the performancesobtained in different task settings.In fact, the average accuracy achieved in the QAsetting (0.71) was 20 points higher than thatachieved in the IE setting (0.52); the average accu-racy in the IR and Sum settings was 0.66 and 0.58respectively.
In RTE-2 the best results wereachieved in SUM, while the lower score was al-ways recorded in IE.
As already pointed out byBar-Haim (2006), these differences should be fur-ther investigated, as they could lead to a sensibleimprovement of the performance.As for the LONG pairs, which represented anew element of this year?s challenge, no substan-tial difference was noted in the systems?
perform-ances: the average accuracy over the long pairswas 58.72%, compared to 61.93% over the shortones.4 Conclusions and future workAt its third round, the Recognizing Textual En-tailment task has reached a noticeable level of ma-turity, as the very high interest in the NLP commu-nity and the continuously increasing number ofparticipants in the challenges demonstrate.
Therelevance of Textual Entailment Recognition todifferent applications, such as the AVE5 track atQA at CLEF6, has also been acknowledged.
Fur-thermore, the debates and the numerous publica-tions about the Textual Entailment have contrib-uted to the better understanding the task and itsnature.To keep a good balance between the consoli-dated main task and the need for moving forward,longer texts were introduced in the dataset, in orderto make the task more challenging, and a pilot taskwas proposed.
The Third RTE Challenge have alsoconfirmed that the methodology for the creation ofthe datasets, developed in the first two campaigns,is robust.
Overall, the transition of the challengecoordination from Bar-Ilan ?which organized thefirst two challenges- to CELCT was successful,though some problems were encountered, espe-cially in the preparation of the data set.
The sys-5http://nlp.uned.es/QA/ave/.6http://clef-qa.itc.it/.tems which took part in RTE-3 showed that thetechnology applied to Entailment Recognition hasmade significant progress, confirmed by the results,which were generally better than last year.
In par-ticular, visible progress in defining several newprincipled scenarios for RTE was represented, suchas Hickl?s commitment-based approach, BarHaim?s proof system, Harmeling?s probabilisticmodel, and Standford?s use of Natural Logic.If, on the one hand, the success that RTE hashad so far is very encouraging, on the other, it in-cites to overcome certain current limitations, and toset realistic and, at the same time, stimulating goalsfor the future.
First at all, theoretical refinementsboth of the task and the models applied to it needto be developed.
In particular, more efforts are re-quired to improve knowledge acquisition, as littleprogress has been made on this front so far.
Alsothe data set generation and the evaluation method-ology  need to be refined and extended.
A majorproblem in the current setting of the data collectionis that the distribution of the examples is arbitraryto a large extent, being determined by manual se-lection.
Therefore new evaluation methodologies,which can reflect realistic distributions should beinvestigated, as well as the possibility of evaluatingTextual Entailment Recognition within additionalconcrete application scenarios, following the spiritof the QA Answer Validation Exercise.AcknowledgmentsThe following sources were used in the preparationof the data:?
PowerAnswer question answering system, fromLanguage Computer Corporation, provided by DanMoldovan and Marta Tatu.http://www.languagecomputer.com/solutions/question answer-ing/power answer/?
Cicero Custom and Cicero Relation informationextraction systems, from Language Computer Cor-poration, provided by Sanda M. Harabagiu, An-drew Hickl, John Lehmann and  and Paul Aarseth.http://www.languagecomputer.com/solutions/information_extaction/cicero/index.html?
Columbia NewsBlaster multi-document summa-rization system, from the Natural Language Proc-8essing group at Columbia University?s Departmen-tof Computer Science.http://newsblaster.cs.columbia.edu/?
NewsInEssence multi-document summarizationsystem provided by Dragomir R. Radev and JahnaOtterbacher from the Computational Linguisticsand Information Retrieval research group, Univer-sity of Michigan.http://www.newsinessence.com?
New York University?s information extractionsystem, provided by Ralph Grishman, Departmentof Computer Science, Courant Institute of Mathe-matical Sciences, New York University.?
MUC-4 information extraction dataset, from theNational Institute of Standards and Technology(NIST).http://www.itl.nist.gov/iaui/894.02/related projects/muc/?
ACE 2004 information extraction templates,from the National Institute of Standards and Tech-nology (NIST).http://www.nist.gov/speech/tests/ace/?
TREC IR queries and TREC-QA question collec-tions, from the National Institute of Standards andTechnology (NIST).http://trec.nist.gov/?
CLEF IR queries and CLEF-QA question collec-tions, from DELOS Network of Excellencefor Digital Libraries.http://www.clef-campaign.org/, http://clef-qa.itc.it/?
DUC 2005 annotated peers, from Columbia Uni-versity, NY, provided by Ani Nenkova.http://www1.cs.columbia.edu/~ani/DUC2005/We would like to thank the people and organiza-tions that made these sources available for thechallenge.
In addition, we thank Idan Szpektor andRoy Bar Haim from Bar-Ilan University  for theirassistance and advice, and Valentina Bruseghinifrom CELCT for managing the RTE-3 website.We would also like to acknowledge the peopleand organizations involved in creating and annotat-ing the data: Pamela Forner, Errol Hayman, Cam-eron Fordyce from CELCT and CourtenayHendricks, Adam Savel and Annika Hamalainenfrom the Butler Hill Group, which was funded byMicrosoft Research.This work was supported in part by the IST Pro-gramme of the European Community, under thePASCAL Network of Excellence, IST-2002-506778.
We wish to thank the managers of thePASCAL challenges program, Michele Sebag andFlorence d?Alche-Buc, for their efforts and sup-port, which made this challenge possible.
We alsothank David Askey, who helped manage the RTE 3website.ReferencesRoy Bar-Haim, Ido Dagan, Bill Dolan, Lisa Ferro,Danilo Giampiccolo, Bernardo Magnini and IdanSzpektor.
2006.
The Second PASCAL RecognizingTextual Entailment Challenge.
In Proceedings of theSecond PASCAL Challenges Workshop on Recog-nizing Textual Entailment, Venice, Italy.Ido Dagan, Oren Glickman, and Bernardo Magnini.2006.
The PASCAL Recognizing Textual EntailmentChallenge.
In Qui?onero-Candela et al, editors,MLCW 2005, LNAI Volume 3944, pages 177-190.Springer-Verlag.J.
R. Landis and G. G. Koch.
1997.
The measurementsof observer agreement for categorical data.
Biomet-rics, 33:159?174.Rebecca Passonneau, Ani Nenkova., Kathleen McKe-own, and Sergey Sigleman.
2005.
Applying thepyramid method in DUC 2005.
In Proceedings of theDocument Understanding Conference (DUC 05),Vancouver, B.C., Canada.Ellen M. Voorhees and Donna Harman.
1999.
Overviewof the seventh text retrieval conference.
In Proceed-ings of the Seventh Text Retrieval Conference(TREC-7).
NIST Special Publication.9
