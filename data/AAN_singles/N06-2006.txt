Proceedings of the Human Language Technology Conference of the North American Chapter of the ACL, pages 21?24,New York, June 2006. c?2006 Association for Computational LinguisticsClass Model Adaptation for Speech SummarisationPierre Chatain, Edward W.D.
Whittaker, Joanna Mrozinski and Sadaoki FuruiDept.
of Computer ScienceTokyo Institute of Technology2-12-1 Ookayama, Meguro-ku, Tokyo 152-8552, Japan{pierre, edw, mrozinsk, furui}@furui.cs.titech.ac.jpAbstractThe performance of automatic speechsummarisation has been improved in pre-vious experiments by using linguisticmodel adaptation.
We extend such adapta-tion to the use of class models, whose ro-bustness further improves summarisationperformance on a wider variety of objec-tive evaluation metrics such as ROUGE-2and ROUGE-SU4 used in the text sum-marisation literature.
Summaries madefrom automatic speech recogniser tran-scriptions benefit from relative improve-ments ranging from 6.0% to 22.2% on allinvestigated metrics.1 IntroductionTechniques for automatically summarising writtentext have been actively investigated in the field ofnatural language processing, and more recently newtechniques have been developed for speech sum-marisation (Kikuchi et al, 2003).
However it isstill very hard to obtain good quality summaries.Moreover, recognition accuracy is still around 30%on spontaneous speech tasks, in contrast to speechread from text such as broadcast news.
Spontaneousspeech is characterised by disfluencies, repetitions,repairs, and fillers, all of which make recognitionand consequently speech summarisation more diffi-cult (Zechner, 2002).
In a previous study (Chatainet al, 2006), linguistic model (LiM) adaptation us-ing different types of word models has proved use-ful in order to improve summary quality.
Howeversparsity of the data available for adaptation makes itdifficult to obtain reliable estimates of word n-gramprobabilities.
In speech recognition, class modelsare often used in such cases to improve model ro-bustness.
In this paper we extend the work previ-ously done on adapting the linguistic model of thespeech summariser by investigating class models.We also use a wider variety of objective evaluationmetrics to corroborate results.2 Summarisation MethodThe summarisation system used in this paper is es-sentially the same as the one described in (Kikuchiet al, 2003), which involves a two step summarisa-tion process, consisting of sentence extraction andsentence compaction.
Practically, only the sentenceextraction part was used in this paper, as prelimi-nary experiments showed that compaction had littleimpact on results for the data used in this study.Important sentences are first extracted accord-ing to the following score for each sentenceW = w1, w2, ..., wn, obtained from the automaticspeech recognition output:S(W ) =1NN?i=1{?CC(wi)+?II(wi)+?LL(wi)},(1)where N is the number of words in the sentenceW , and C(wi), I(wi) and L(wi) are the confidencescore, the significance score and the linguistic scoreof word wi, respectively.
?C , ?I and ?L are therespective weighting factors of those scores, deter-mined experimentally.For each word from the automatic speech recogni-21tion transcription, a logarithmic value of its posteriorprobability, the ratio of a word hypothesis probabil-ity to that of all other hypotheses, is calculated usinga word graph obtained from the speech recogniserand used as a confidence score.For the significance score, the frequencies of oc-currence of 115k words were found using the WSJand the Brown corpora.In the experiments in this paper we modified thelinguistic component to use combinations of dif-ferent linguistic models.
The linguistic componentgives the linguistic likelihood of word strings inthe sentence.
Starting with a baseline LiM (LiMB)we perform LiM adaptation by linearly interpolat-ing the baseline model with other component mod-els trained on different data.
The probability of agiven n-gram sequence then becomes:P (wi|wi?n+1..wi?1) = ?1P1(wi|wi?n+1..wi?1)+... + ?nPn(wi|wi?n+1..wi?1), (2)where?k ?k = 1 and ?k and Pk are the weight andthe probability assigned by model k.In the case of a two-sided class-based model,Pk(wi|wi?n+1..wi?1) = Pk(wi|C(wi)) ?Pk(C(wi)|C(wi?n+1)..C(wi?1)), (3)where Pk(wi|C(wi)) is the probability of theword wi belonging to a given class C, andPk(C(wi)|C(wi?n+1)..C(wi?1)) the probability ofa certain word class C(wi) to appear after a historyof word classes, C(wi?n+1), ..., C(wi?1).Different types of component LiM are built, com-ing from different sources of data, either as wordor class models.
The LiMB and component LiMsare then combined for adaptation using linear inter-polation as in Equation (2).
The linguistic score isthen computed using this modified probability as inEquation (4):L(wi) = logP (wi|wi?n+1..wi?1).
(4)3 Evaluation Criteria3.1 Summarisation AccuracyTo automatically evaluate the summarised speeches,correctly transcribed talks were manually sum-marised, and used as the correct targets for evalua-tion.
Variations of manual summarisation results aremerged into a word network, which is considered toapproximately express all possible correct summari-sations covering subjective variations.
The word ac-curacy of automatic summarisation is calculated asthe summarisation accuracy (SumACCY) using theword network (Hori et al, 2003):Accuracy = (Len?Sub?Ins?Del)/Len?100[%],(5)where Sub is the number of substitution errors, Insis the number of insertion errors, Del is the numberof deletion errors, and Len is the number of wordsin the most similar word string in the network.3.2 ROUGEVersion 1.5.5 of the ROUGE scoring algorithm(Lin, 2004) is also used for evaluating results.ROUGE F-measure scores are given for ROUGE-2 (bigram), ROUGE-3 (trigram), and ROUGE-SU4(skip-bigram), using the model average (averagescore across all references) metric.4 Experimental SetupExperiments were performed on spontaneousspeech, using 9 talks taken from the TranslanguageEnglish Database (TED) corpus (Lamel et al, 1994;Wolfel and Burger, 2005), each transcribed andmanually summarised by nine different humans forboth 10% and 30% summarization ratios.
Speechrecognition transcriptions (ASR) were obtained foreach talk, with an average word error rate of 33.3%.A corpus consisting of around ten years of con-ference proceedings (17.8M words) on the subjectof speech and signal processing is used to generatethe LiMB and word classes using the clustering al-gorithm in (Ney et al, 1994).Different types of component LiM are built andcombined for adaptation as described in Section 2.The first type of component linguistic models arebuilt on the small corpus of hand-made summariesdescribed above, made for the same summarisationratio as the one we are generating.
For each talkthe hand-made summaries of the other eight talks(i.e.
72 summaries) were used as the LiM trainingcorpus.
This type of LiM is expected to help gener-ate automatic summaries in the same style as thosemade manually.22Baseline AdaptedSumACCY R-2 R-3 R-SU4 SumACCY R-2 R-3 R-SU410% Random 34.4 0.104 0.055 0.142 - - - -Word 63.1 0.186 0.130 0.227 67.8 0.193 0.140 0.228Class 65.1 0.195 0.131 0.226 72.6 0.210 0.143 0.234Mixed 63.6 0.186 0.128 0.218 71.8 0.211 0.139 0.23130% Random 71.2 0.294 0.198 0.331 - - - -Word 81.6 0.365 0.271 0.395 83.3 0.365 0.270 0.392Class 83.1 0.374 0.279 0.407 92.9 0.415 0.325 0.442Mixed 83.1 0.374 0.279 0.407 92.9 0.415 0.325 0.442Table 1: TRS baseline and adapted results.The second type of component linguistic modelsare built from the papers in the conference proceed-ings for the talk we want to summarise.
This typeof LiM, used for topic adaptation, is investigated be-cause key words and important sentences that appearin the associated paper are expected to have a highinformation value and should be selected during thesummarisation process.Three sets of experiments were made: in the firstexperiment (referred to as Word), LiMB and bothcomponent models are word models, as introducedin (Chatain et al, 2006).
For the second one (Class),both LiMB and the component models are classmodels built using exactly the same data as the wordmodels.
For the third experiment (Mixed), the LiMBis an interpolation of class and word models, whilethe component LiMs are class models.To optimise use of the available data, a rotatingform of cross-validation (Duda and Hart, 1973) isused: all talks but one are used for development, theremaining talk being used for testing.
Summariesfrom the development talks are generated automati-cally by the system using different sets of parametersand the LiMB .
These summaries are evaluated andthe set of parameters which maximises the develop-ment score for the LiMB is selected for the remain-ing talk.
The purpose of the development phase isto choose the most effective combination of weights?C , ?I and ?L.
The summary generated for eachtalk using its set of optimised parameters is thenevaluated using the same metric, which gives us ourbaseline for this talk.
Using the same parameters asthose that were selected for the baseline, we gener-ate summaries for the lectures in the development setfor different LiM interpolation weights ?k.
Valuesbetween 0 and 1 in steps of 0.1, were investigatedfor the latter, and an optimal set of ?k is selected.Using these interpolation weights, as well as the setof parameters determined for the baseline, we gen-erate a summary of the test talk, which is evaluatedusing the same evaluation metric, giving us our fi-nal adapted result for this talk.
Averaging those re-sults over the test set (i.e.
all talks) gives us our finaladapted result.This process is repeated for all evaluation metrics,and all three experiments (Word, Class, and Mixed).Lower bound results are given by random sum-marisation (Random) i.e.
randomly extracting sen-tences and words, without use of the scores presentin Equation (1) for appropriate summarisation ratios.5 Results5.1 TRS ResultsInitial experiments were made on the human tran-scriptions (TRS), and results are given in Table 1.Experiments on word models (Word) show relativeimprovements in terms of SumACCY of 7.5% and2.1% for the 10% and 30% summarisation ratios, re-spectively.
ROUGE metrics, however, do not showany significant improvement.Using class models (Class and Mixed), for allROUGE metrics, relative improvements range from3.5% to 13.4% for the 10% summarisation ratio, andfrom 8.6% to 16.5% on the 30% summarisation ra-tio.
For SumACCY, relative improvements between11.5% to 12.9% are observed.5.2 ASR ResultsASR results for each experiment are given in Ta-ble 2 for appropriate summarisation ratios.
As for23Baseline AdaptedSumACCY R-2 R-3 R-SU4 SumACCY R-2 R-3 R-SU410% Random 33.9 0.095 0.042 0.140 - - - -Word 48.6 0.143 0.064 0.182 49.8 0.129 0.060 0.173Class 50.0 0.133 0.063 0.170 55.1 0.156 0.077 0.193Mixed 48.5 0.134 0.068 0.176 56.2 0.142 0.077 0.19130% Random 56.1 0.230 0.124 0.283 - - - -Word 66.7 0.265 0.157 0.314 68.7 0.271 0.161 0.328Class 66.1 0.277 0.165 0.324 71.1 0.300 0.180 0.348Mixed 64.9 0.268 0.160 0.312 70.5 0.304 0.192 0.351Table 2: ASR baseline and adapted results.the TRS, LiM adaptation showed improvements interms of SumACCY, but ROUGE metrics do not cor-roborate those results for the 10% summarisation ra-tio.
Using class models, for all ROUGE metrics, rel-ative improvements range from 6.0% to 22.2% andfrom 7.4% to 20.0% for the 10% and 30% summari-sation ratios, respectively.
SumACCY relative im-provements range from 7.6% to 15.9%.6 DiscussionCompared to previous experiments using only wordmodels, improvements obtained using class modelsare larger and more significant for both ROUGE andSumACCY metrics.
This can be explained by thefact that the data we are performing adaptation onis very sparse, and that the nine talks used in theseexperiments are quite different from each other, es-pecially since the speakers also vary in style.
Classmodels are more robust to this spontaneous speechaspect than word models, since they generalise bet-ter to unseen word sequences.There is little difference between the Class andMixed results, since the development phase assignedmost weight to the class model component in theMixed experiment, making the results quite similarto those of the Class experiment.7 ConclusionIn this paper we have investigated linguistic modeladaptation using different sources of data for an au-tomatic speech summarisation system.
Class mod-els have proved to be much more robust than wordmodels for this process, and relative improvementsranging from 6.0% to 22.2% were obtained on a va-riety of evaluation metrics on summaries generatedfrom automatic speech recogniser transcriptions.Acknowledgements: The authors would like tothank M. Wo?lfel for the recogniser transcriptionsand C. Hori for her work on two stage summarisa-tion and gathering the TED corpus data.
This workis supported by the 21st Century COE Programme.ReferencesP.
Chatain, E.W.D.
Whittaker, J. Mrozinski, and S. Fu-rui.
2006.
Topic and Stylistic Adaptation for SpeechSummarization.
Proc.
ICASSP, Toulouse, France.R.
Duda and P. Hart.
1973.
Pattern Classification andScene Analysis.
Wiley, New York.C.
Hori, T. Hori, and S. Furui.
2003.
EvaluationMethod for Automatic Speech Summarization.
Proc.Eurospeech, Geneva, Switzerland, 4:2825?2828.T.
Kikuchi, S. Furui, and C. Hori.
2003.
AutomaticSpeech Summarization based on Sentence Extractionand Compaction.
Proc.
ICASSP, Hong Kong, China,1:236?239.L.
Lamel, F. Schiel, A. Fourcin, J. Mariani, and H. Till-mann.
1994.
The Translanguage English Database(TED).
Proc.
ICSLP, Yokohama, Japan, 4:1795?1798.Chin-Yew Lin.
2004.
ROUGE: a Package for AutomaticEvaluation of Summaries.
Proc.
WAS, Barcelona,Spain.H.
Ney, U. Essen, and R. Kneser.
1994.
On Structur-ing Probabilistic Dependences in Stochastic LanguageModelling.
Computer Speech and Language, (8):1?38.M.
Wolfel and S. Burger.
2005.
The ISL Baseline Lec-ture Transcription System for the TED Corpus.
Tech-nical report, Karlsruhe University.K.
Zechner.
2002.
Summarization of Spoken Language-Challenges, Methods, and Prospects.
Speech Technol-ogy Expert eZine, Issue.6.24
