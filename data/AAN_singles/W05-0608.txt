Proceedings of the 9th Conference on Computational Natural Language Learning (CoNLL),pages 56?63, Ann Arbor, June 2005. c?2005 Association for Computational LinguisticsDomain Kernels for Text CategorizationAlfio Gliozzo and Carlo StrapparavaITC-Irstvia Sommarive, I-38050, Trento, ITALY{gliozzo,strappa}@itc.itAbstractIn this paper we propose and evaluatea technique to perform semi-supervisedlearning for Text Categorization.
Inparticular we defined a kernel function,namely the Domain Kernel, that allowedus to plug ?external knowledge?
into thesupervised learning process.
Externalknowledge is acquired from unlabeleddata in a totally unsupervised way, and itis represented by means of Domain Mod-els.We evaluated the Domain Kernel in twostandard benchmarks for Text Categoriza-tion with good results, and we comparedits performance with a kernel function thatexploits a standard bag-of-words featurerepresentation.
The learning curves showthat the Domain Kernel allows us to re-duce drastically the amount of trainingdata required for learning.1 IntroductionText Categorization (TC) deals with the problem ofassigning a set of category labels to documents.
Cat-egories are usually defined according to a varietyof topics (e.g.
SPORT vs.
POLITICS) and a set ofhand tagged examples is provided for training.
In thestate-of-the-art TC settings supervised classifiers areused for learning and texts are represented by meansof bag-of-words.Even if, in principle, supervised approaches reachthe best performance in many Natural LanguageProcessing (NLP) tasks, in practice it is not alwayseasy to apply them to concrete applicative settings.In fact, supervised systems for TC require to betrained a large amount of hand tagged texts.
Thissituation is usually feasible only when there is some-one (e.g.
a big company) that can easily provide al-ready classified documents to train the system.In most of the cases this scenario is quite unprac-tical, if not infeasible.
An example is the task ofcategorizing personal documents, in which the cate-gories can be modified according to the user?s inter-ests: new categories are often introduced and, pos-sibly, the available labeled training for them is verylimited.In the NLP literature the problem of providinglarge amounts of manually annotated data is knownas the Knowledge Acquisition Bottleneck.
Cur-rent research in supervised approaches to NLP oftendeals with defining methodologies and algorithms toreduce the amount of human effort required for col-lecting labeled examples.A promising direction to solve this problem is toprovide unlabeled data together with labeled textsto help supervision.
In the Machine Learning lit-erature this learning schema has been called semi-supervised learning.
It has been applied to theTC problem using different techniques: co-training(Blum and Mitchell, 1998), EM-algorithm (Nigamet al, 2000), Transduptive SVM (Joachims, 1999b)and Latent Semantic Indexing (Zelikovitz and Hirsh,2001).In this paper we propose a novel technique to per-form semi-supervised learning for TC.
The under-lying idea behind our approach is that lexical co-56herence (i.e.
co-occurence in texts of semanticallyrelated terms) (Magnini et al, 2002) is an inherentproperty of corpora, and it can be exploited to help asupervised classifier to build a better categorizationhypothesis, even if the amount of labeled trainingdata provided for learning is very low.Our proposal consists of defining a DomainKernel and exploiting it inside a Support VectorMachine (SVM) classification framework for TC(Joachims, 2002).
The Domain Kernel relies on thenotion of Domain Model, which is a shallow repre-sentation for lexical ambiguity and variability.
Do-main Models can be acquired in an unsupervisedway from unlabeled data, and then exploited to de-fine a Domain Kernel (i.e.
a generalized similarityfunction among documents)1 .We evaluated the Domain Kernel in two stan-dard benchmarks for TC (i.e.
Reuters and 20News-groups), and we compared its performance with akernel function that exploits a more standard Bag-of-Words (BoW) feature representation.
The use ofthe Domain Kernel got a significant improvement inthe learning curves of both tasks.
In particular, thereis a notable increment of the recall, especially withfew learning examples.
In addition, F1 measure in-creases by 2.8 points in the Reuters task at full learn-ing, achieving the state-of-the-art results.The paper is structured as follows.
Section 2 in-troduces the notion of Domain Model and describesan automatic acquisition technique based on LatentSemantic Analysis (LSA).
In Section 3 we illustratethe SVM approach to TC, and we define a DomainKernel that exploits Domain Models to estimate sim-ilarity among documents.
In Section 4 the perfor-mance of the Domain Kernel are compared with astandard bag-of-words feature representation, show-ing the improvements in the learning curves.
Section5 describes the previous attempts to exploit semi-supervised learning for TC, while section 6 con-cludes the paper and proposes some directions forfuture research.1The idea of exploiting a Domain Kernel to help a super-vised classification framework, has been profitably used also inother NLP tasks such as word sense disambiguation (see for ex-ample (Strapparava et al, 2004)).2 Domain ModelsThe simplest methodology to estimate the similar-ity among the topics of two texts is to representthem by means of vectors in the Vector Space Model(VSM), and to exploit the cosine similarity.
Moreformally, let T = {t1, t2, .
.
.
, tn} be a corpus, letV = {w1, w2, .
.
.
, wk} be its vocabulary, let T bethe k ?
n term-by-document matrix representing T ,such that ti,j is the frequency of word wi into the texttj .
The VSM is a k-dimensional space Rk, in whichthe text tj ?
T is represented by means of the vec-tor ~tj such that the ith component of ~tj is ti,j.
Thesimilarity among two texts in the VSM is estimatedby computing the cosine.However this approach does not deal well withlexical variability and ambiguity.
For example thetwo sentences ?he is affected by AIDS?
and ?HIV isa virus?
do not have any words in common.
In theVSM their similarity is zero because they have or-thogonal vectors, even if the concepts they expressare very closely related.
On the other hand, the sim-ilarity between the two sentences ?the laptop hasbeen infected by a virus?
and ?HIV is a virus?
wouldturn out very high, due to the ambiguity of the wordvirus.To overcome this problem we introduce the notionof Domain Model (DM), and we show how to use itin order to define a domain VSM, in which texts andterms are represented in a uniform way.A Domain Model is composed by soft clusters ofterms.
Each cluster represents a semantic domain(Gliozzo et al, 2004), i.e.
a set of terms that oftenco-occur in texts having similar topics.
A DomainModel is represented by a k ?
k?
rectangular matrixD, containing the degree of association among termsand domains, as illustrated in Table 1.MEDICINE COMPUTER SCIENCEHIV 1 0AIDS 1 0virus 0.5 0.5laptop 0 1Table 1: Example of Domain MatrixDomain Models can be used to describe lexicalambiguity and variability.
Lexical ambiguity is rep-57resented by associating one term to more than onedomain, while variability is represented by associat-ing different terms to the same domain.
For examplethe term virus is associated to both the domainCOMPUTER SCIENCE and the domain MEDICINE(ambiguity) while the domain MEDICINE is associ-ated to both the terms AIDS and HIV (variability).More formally, let D = {D1, D2, ..., Dk?}
bea set of domains, such that k?
k. A DomainModel is fully defined by a k ?
k?
domain matrixD representing in each cell di,z the domain rele-vance of term wi with respect to the domain Dz .The domain matrix D is used to define a functionD : Rk ?
Rk?
, that maps the vectors ~tj , expressedinto the classical VSM, into the vectors ~t?j in the do-main VSM.
D is defined by2D(~tj) = ~tj(IIDFD) = ~t?j (1)where IIDF is a diagonal matrix such that iIDFi,i =IDF (wi), ~tj is represented as a row vector, andIDF (wi) is the Inverse Document Frequency of wi.Vectors in the domain VSM are called DomainVectors.
Domain Vectors for texts are estimated byexploiting formula 1, while the Domain Vector ~w?i,corresponding to the word wi ?
V , is the ith row ofthe domain matrix D. To be a valid domain matrixsuch vectors should be normalized (i.e.
?
~w?i, ~w?i?
=1).In the Domain VSM the similarity among DomainVectors is estimated by taking into account secondorder relations among terms.
For example the simi-larity of the two sentences ?He is affected by AIDS?and ?HIV is a virus?
is very high, because the termsAIDS, HIV and virus are highly associated to thedomain MEDICINE.In this work we propose the use of Latent Se-mantic Analysis (LSA) (Deerwester et al, 1990) toinduce Domain Models from corpora.
LSA is anunsupervised technique for estimating the similar-ity among texts and terms in a corpus.
LSA is per-formed by means of a Singular Value Decomposi-tion (SVD) of the term-by-document matrix T de-scribing the corpus.
The SVD algorithm can be ex-ploited to acquire a domain matrix D from a large2In (Wong et al, 1985) a similar schema is adopted to definea Generalized Vector Space Model, of which the Domain VSMis a particular instance.corpus T in a totally unsupervised way.
SVD de-composes the term-by-document matrix T into threematrixes T ' V?k?UT where ?k?
is the diagonalk ?
k matrix containing the highest k ?
k eigen-values of T, and all the remaining elements set to0.
The parameter k?
is the dimensionality of the Do-main VSM and can be fixed in advance3 .
Under thissetting we define the domain matrix DLSA4 asDLSA = INV??k?
(2)where IN is a diagonal matrix such that iNi,i =1??
~w?i, ~w?i?, ~w?i is the ith row of the matrix V??k?
.3 The Domain KernelKernel Methods are the state-of-the-art supervisedframework for learning, and they have been success-fully adopted to approach the TC task (Joachims,1999a).The basic idea behind kernel methods is to embedthe data into a suitable feature space F via a map-ping function ?
: X ?
F , and then use a linearalgorithm for discovering nonlinear patterns.
Kernelmethods allow us to build a modular system, as thekernel function acts as an interface between the dataand the learning algorithm.
Thus the kernel functionbecomes the only domain specific module of the sys-tem, while the learning algorithm is a general pur-pose component.
Potentially a kernel function canwork with any kernel-based algorithm, such as forexample SVM.During the learning phase SVMs assign a weight?i ?
0 to any example xi ?
X .
All the labeledinstances xi such that ?i > 0 are called support vec-tors.
The support vectors lie close to the best sepa-rating hyper-plane between positive and negative ex-amples.
New examples are then assigned to the classof its closest support vectors, according to equation3.3It is not clear how to choose the right dimensionality.
Inour experiments we used 400 dimensions.4When DLSA is substituted in Equation 1 the Domain VSMis equivalent to a Latent Semantic Space (Deerwester et al,1990).
The only difference in our formulation is that the vectorsrepresenting the terms in the Domain VSM are normalized bythe matrix IN, and then rescaled, according to their IDF value,by matrix IIDF.
Note the analogy with the tf idf term weightingschema (Salton and McGill, 1983), widely adopted in Informa-tion Retrieval.58f(x) =n?i=1?iK(xi, x) + ?0 (3)The kernel function K returns the similarity be-tween two instances in the input space X , and canbe designed in order to capture the relevant aspectsto estimate similarity, just by taking care of satis-fying set of formal requirements, as described in(Scho?lkopf and Smola, 2001).In this paper we define the Domain Kernel and weapply it to TC tasks.
The Domain Kernel, denotedby KD, can be exploited to estimate the topic simi-larity among two texts while taking into account theexternal knowledge provided by a Domain Model(see section 2).
It is a variation of the Latent Seman-tic Kernel (Shawe-Taylor and Cristianini, 2004), inwhich a Domain Model is exploited to define an ex-plicit mapping D : Rk ?
Rk?
from the classicalVSM into the domain VSM.
The Domain Kernel isdefined byKD(ti, tj) =?D(ti),D(tj)???D(tj),D(tj)??D(ti),D(ti)?
(4)where D is the Domain Mapping defined in equa-tion 1.
To be fully defined, the Domain Kernel re-quires a Domain Matrix D. In principle, D can beacquired from any corpora by exploiting any (soft)term clustering algorithm.
Anyway, we belive thatadequate Domain Models for particular tasks can bebetter acquired from collections of documents fromthe same source.
For this reason, for the experi-ments reported in this paper, we acquired the matrixDLSA, defined by equation 2, using the whole (un-labeled) training corpora available for each task, sotuning the Domain Model on the particular task inwhich it will be applied.A more traditional approach to measure topic sim-ilarity among text consists of extracting BoW fea-tures and to compare them in a vector space.
TheBoW kernel, denoted by KBoW , is a particular caseof the Domain Kernel, in which D = I, and I is theidentity matrix.
The BoW Kernel does not requirea Domain Model, so we can consider this settingas ?purely?
supervised, in which no external knowl-edge source is provided.4 EvaluationWe compared the performance of both KD andKBoW on two standard TC benchmarks.
In sub-section 4.1 we describe the evaluation tasks and thepreprocessing steps, in 4.2 we describe some algo-rithmic details of the TC system adopted.
Finallyin subsection 4.3 we compare the learning curves ofKD and KBoW .4.1 Text Categorization tasksFor the experiments reported in this paper, we se-lected two evaluation benchmarks typically used inthe TC literature (Sebastiani, 2002): the 20news-groups and the Reuters corpora.
In both the data setswe tagged the texts for part of speech and we consid-ered only the noun, verb, adjective, and adverb partsof speech, representing them by vectors containingthe frequencies of each disambiguated lemma.
Theonly feature selection step we performed was to re-move all the closed-class words from the documentindex.20newsgroups.
The 20Newsgroups data set5 isa collection of approximately 20,000 newsgroupdocuments, partitioned (nearly) evenly across 20different newsgroups.
This collection has becomea popular data set for experiments in text appli-cations of machine learning techniques, such astext classification and text clustering.
Some ofthe newsgroups are very closely related to eachother (e.g.
comp.sys.ibm.pc.hardware/ comp.sys.mac.hardware), while othersare highly unrelated (e.g.
misc.forsale /soc.religion.christian).
We removedcross-posts (duplicates), newsgroup-identifyingheaders (i.e.
Xref, Newsgroups, Path, Followup-To,Date), and empty documents from the originalcorpus, so to obtain 18,941 documents.
Then werandomly divided it into training (80%) and test(20%) sets, containing respectively 15,153 and3,788 documents.Reuters.
We used the Reuters-21578 collec-tion6, and we splitted it into training and test5Available at http://www.ai.mit.edu-/people/jrennie/20Newsgroups/.6Available at http://kdd.ics.uci.edu/databases/-reuters21578/reuters21578.html.59partitions according to the standard ModAptesplit.
It includes 12,902 documents for 90 cat-egories, with a fixed splitting between trainingand test data.
We conducted our experiments byconsidering only the 10 most frequent categories,i.e.
Earn, Acquisition, Money-fx,Grain, Crude, Trade, Interest,Ship, Wheat and Corn, and we included inour dataset al the non empty documents labeledwith at least one of those categories.
Thus the finaldataset includes 9295 document, of which 6680 areincluded in the training partition, and 2615 are inthe test set.4.2 Implementation detailsAs a supervised learning device, we used the SVMimplementation described in (Joachims, 1999a).The Domain Kernel is implemented by defining anexplicit feature mapping according to formula 1, andby normalizing each vector to obtain vectors of uni-tary length.
All the experiments have been per-formed on the standard parameter settings, using alinear kernel.We acquired a different Domain Model for eachcorpus by performing the SVD processes on theterm-by-document matrices representing the wholetraining partitions, and we considered only the first400 domains (i.e.
k?
= 400)7.As far as the Reuters task is concerned, the TCproblem has been approached as a set of binary fil-tering problems, allowing the TC system to pro-vide more than one category label to each document.For the 20newsgroups task, we implemented a one-versus-all classification schema, in order to assign asingle category to each news.4.3 Domain Kernel versus BoW KernelFigure 1 and Figure 2 report the learning curves forboth KD and KBoW , evaluated respectively on theReuters and the 20newgroups task.
Results clearlyshow that KD always outperforms KBoW , espe-cially when very limited amount of labeled data isprovided for learning.7To perform the SVD operation we adoptedLIBSVDC, an optimized package for sparse ma-trix that allows to perform this step in few minuteseven for large corpora.
It can be downloaded fromhttp://tedlab.mit.edu/?dr/SVDLIBC/.0.20.30.40.50.60.70.80.910 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1F1measureFraction of labeled training dataDomain KernelBoW KernelFigure 1: Micro-F1 learning curves for Reuters0.10.20.30.40.50.60.70.80.910 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1F1measureFraction of labeled training dataDomain KernelBoW KernelFigure 2: Micro-F1 learning curves for 20news-groupsTable 2 compares the performances of the twokernels at full learning.
KD achieves a better micro-F1 than KBoW in both tasks.
The improvement isparticularly significant in the Reuters task (+ 2.8 %).Tables 3 shows the number of labeled examplesrequired by KD and KBoW to achieve the samemicro-F1 in the Reuters task.
KD requires only146 examples to obtain a micro-F1 of 0.84, whileKBoW requires 1380 examples to achieve the sameperformance.
In the same task, KD surpass the per-formance of KBoW at full learning using only the10% of the labeled data.
The last column of the ta-ble shows clearly that KD requires 90% less labeleddata than KBoW to achieve the same performances.A similar behavior is reported in Table 4 for the60F1 Domain Kernel Bow KernelReuters 0.928 0.90020newsgroups 0.886 0.880Table 2: Micro-F1 with full learningF1 Domain Kernel Bow Kernel Ratio.54 14 267 5%.84 146 1380 10%.90 668 6680 10%Table 3: Number of training examples needed byKD and KBoW to reach the same micro-F1 on theReuters task20newsgroups task.
It is important to notice that thenumber of labeled documents is higher in this corpusthan in the previous one.
The benefits of using Do-main Models are then less evident at full learning,even if they are significant when very few labeleddata are provided.Figures 3 and 4 report a more detailed analysisby comparing the micro-precision and micro-recalllearning curves of both kernels in the Reuters task8.It is clear from the graphs that the main contributeof KD is about increasing recall, while precision issimilar in both cases9.
This last result confirms ourhypothesis that the information provided by the Do-main Models allows the system to generalize in amore effective way over the training examples, al-lowing to estimate the similarity among texts even ifthey have just few words in common.Finally, KD achieves the state-of-the-art in theReuters task, as reported in section 5.5 Related WorksTo our knowledge, the first attempt to apply thesemi-supervised learning schema to TC has beenreported in (Blum and Mitchell, 1998).
Their co-training algorithm was able to reduce significantlythe error rate, if compared to a strictly supervised8For the 20-newsgroups task both micro-precision andmicro-recall are equal to micro-F1 because a single categorylabel has been assigned to every instance.9It is worth noting that KD gets a F1 measure of 0.54 (Preci-sion/Recall of 0.93/0.38) using just 14 training examples, sug-gesting that it can be profitably exploited for a bootstrappingprocess.F1 Domain Kernel Bow Kernel Ratio.50 30 500 6%.70 98 1182 8%.85 2272 7879 29%Table 4: Number of training examples needed byKD and KBoW to reach the same micro-F1 on the20newsgroups task0.20.30.40.50.60.70.80.910 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1PrecisionFraction of labeled training dataDomain KernelBoW KernelFigure 3: Learning curves for Reuters (Precision)0.10.20.30.40.50.60.70.80.910 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1RecallFraction of labeled training dataDomain KernelBoW KernelFigure 4: Learning curves for Reuters (Recall)classifier.
(Nigam et al, 2000) adopted an Expectation Max-imization (EM) schema to deal with the same prob-lem, evaluating extensively their approach on sev-eral datasets.
They compared their algorithm witha standard probabilistic approach to TC, reportingsubstantial improvements in the learning curve.61A similar evaluation is also reported in (Joachims,1999b), where a transduptive SVM is comparedto a state-of-the-art TC classifier based on SVM.The semi-supervised approach obtained better re-sults than the standard with few learning data, whileat full learning results seem to converge.
(Bekkerman et al, 2002) adopted a SVM classi-fier in which texts have been represented by their as-sociations to a set of Distributional Word Clusters.Even if this approach is very similar to ours, it is nota semi-supervised learning schema, because authorsdid not exploit any additional unlabeled data to in-duce word clusters.In (Zelikovitz and Hirsh, 2001) backgroundknowledge (i.e.
the unlabeled data) is exploited to-gether with labeled data to estimate document sim-ilarity in a Latent Semantic Space (Deerwester etal., 1990).
Their approach differs from the one pro-posed in this paper because a different categoriza-tion algorithm has been adopted.
Authors comparedtheir algorithm with an EM schema (Nigam et al,2000) on the same dataset, reporting better resultsonly with very few labeled data, while EM performsbetter with more training.All the semi-supervised approaches in the liter-ature reports better results than strictly supervisedones with few learning, while with more data thelearning curves tend to converge.A comparative evaluation among semi-supervisedTC algorithms is quite difficult, because the useddata sets, the preprocessing steps and the splittingpartitions adopted affect sensibly the final results.Anyway, we reported the best F1 measure on theReuters corpus: to our knowledge, the state-of-the-art on the 10 top most frequent categories of theModApte split at full learning is F1 92.0 (Bekker-man et al, 2002) while we obtained 92.8.
It is im-portant to notice here that this results has been ob-tained thanks to the improvements of the DomainKernel.
In addition, on the 20newsgroups task, ourmethods requires about 100 documents (i.e.
fivedocuments per category) to achieve 70% F1, whileboth EM (Nigam et al, 2000) and LSI (Zelikovitzand Hirsh, 2001) requires more than 400 to achievethe same performance.6 Conclusion and Future WorksIn this paper a novel technique to perform semi-supervised learning for TC has been proposed andevaluated.
We defined a Domain Kernel that allowsus to improve the similarity estimation among docu-ments by exploiting Domain Models.
Domain Mod-els are acquired from large collections of non anno-tated texts in a totally unsupervised way.An extensive evaluation on two standard bench-marks shows that the Domain Kernel allows us to re-duce drastically the amount of training data requiredfor learning.
In particular the recall increases sen-sibly, while preserving a very good accuracy.
Weexplained this phenomenon by showing that the sim-ilarity scores evaluated by the Domain Kernel takesinto account both variability and ambiguity, beingable to estimate similarity even among texts that donot have any word in common.As future work, we plan to apply our semi-supervised learning method to some concrete ap-plicative scenarios, such as user modeling and cat-egorization of personal documents in mail clients.In addition, we are going deeper in the direction ofsemi-supervised learning, by acquiring more com-plex structures than clusters (e.g.
synonymy, hyper-onymy) to represent domain models.
Furthermore,we are working to adapt the general framework pro-vided by the Domain Models to a multilingual sce-nario, in order to apply the Domain Kernel to a CrossLanguage TC task.AcknowledgmentsThis work has been partially supported by the ON-TOTEXT (From Text to Knowledge for the Se-mantic Web) project, funded by the AutonomousProvince of Trento under the FUP-2004 researchprogram.ReferencesR.
Bekkerman, R. El-Yaniv, N. Tishby, and Y. Win-ter.
2002.
Distributional word clusters vs. words fortext categorization.
Journal of Machine Learning Re-search, 1:1183?1208.A.
Blum and T. Mitchell.
1998.
Combining labeled andunlabeled data with co-training.
In COLT: Proceed-ings of the Workshop on Computational Learning The-ory, Morgan Kaufmann Publishers.62S.
Deerwester, S. Dumais, G. Furnas, T. Landauer, andR.
Harshman.
1990.
Indexing by latent semantic anal-ysis.
Journal of the American Society of InformationScience.A.
Gliozzo, C. Strapparava, and I. Dagan.
2004.
Unsu-pervised and supervised exploitation of semantic do-mains in lexical disambiguation.
Computer Speechand Language, 18:275?299.T.
Joachims.
1999a.
Making large-scale SVM learningpractical.
In B. Scho?lkopf, C. Burges, and A. Smola,editors, Advances in kernel methods: support vectorlearning, chapter 11, pages 169 ?
184.
MIT Press,Cambridge, MA, USA.T.
Joachims.
1999b.
Transductive inference for textclassification using support vector machines.
In Pro-ceedings of ICML-99, 16th International Conferenceon Machine Learning, pages 200?209.
Morgan Kauf-mann Publishers, San Francisco, US.T.
Joachims.
2002.
Learning to Classify Text using Sup-port Vector Machines.
Kluwer Academic Publishers.B.
Magnini, C. Strapparava, G. Pezzulo, and A. Gliozzo.2002.
The role of domain information in wordsense disambiguation.
Natural Language Engineer-ing, 8(4):359?373.K.
Nigam, A. K. McCallum, S. Thrun, and T. M.Mitchell.
2000.
Text classification from labeled andunlabeled documents using EM.
Machine Learning,39(2/3):103?134.G.
Salton and M.H.
McGill.
1983.
Introduction to mod-ern information retrieval.
McGraw-Hill, New York.B.
Scho?lkopf and A. J. Smola.
2001.
Learning with Ker-nels.
Support Vector Machines, Regularization, Opti-mization, and Beyond.
MIT Press.F.
Sebastiani.
2002.
Machine learning in automated textcategorization.
ACM Computing Surveys, 34(1):1?47.J.
Shawe-Taylor and N. Cristianini.
2004.
Kernel Meth-ods for Pattern Analysis.
Cambridge University Press.C.
Strapparava, A. Gliozzo, and C. Giuliano.
2004.
Pat-tern abstraction and term similarity for word sensedisambiguation: Irst at senseval-3.
In Proc.
ofSENSEVAL-3 Third International Workshop on Eval-uation of Systems for the Semantic Analysis of Text,pages 229?234, Barcelona, Spain, July.S.K.M.
Wong, W. Ziarko, and P.C.N.
Wong.
1985.
Gen-eralized vector space model in information retrieval.In Proceedings of the 8th ACM SIGIR Conference.S.
Zelikovitz and H. Hirsh.
2001.
Using LSI for text clas-sification in the presence of background text.
In Hen-rique Paques, Ling Liu, and David Grossman, editors,Proceedings of CIKM-01, 10th ACM InternationalConference on Information and Knowledge Manage-ment, pages 113?118, Atlanta, US.
ACM Press, NewYork, US.63
