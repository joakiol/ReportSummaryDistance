Proceedings of the NAACL HLT Workshop on Semantic Evaluations: Recent Achievements and Future Directions, pages 146?154,Boulder, Colorado, June 2009. c?2009 Association for Computational LinguisticsAutomatic Recognition of Logical Relations for English, Chinese andJapanese in the GLARF FrameworkAdam Meyers?, Michiko Kosaka?, Nianwen Xue?, Heng Ji?, Ang Sun?, Shasha Liao?
and Wei Xu??
New York University, ?Monmouth University, ?Brandeis University, ?
City University of New YorkAbstractWe present GLARF, a framework for repre-senting three linguistic levels and systems forgenerating this representation.
We focus on alogical level, like LFG?s F-structure, but com-patible with Penn Treebanks.
While less fine-grained than typical semantic role labeling ap-proaches, our logical structure has several ad-vantages: (1) it includes all words in all sen-tences, regardless of part of speech or seman-tic domain; and (2) it is easier to produce ac-curately.
Our systems achieve 90% for En-glish/Japanese News and 74.5% for ChineseNews ?
these F-scores are nearly the same asthose achieved for treebank-based parsing.1 IntroductionFor decades, computational linguists have paired asurface syntactic analysis with an analysis represent-ing something ?deeper?.
The work of Harris (1968),Chomsky (1957) and many others showed that onecould use these deeper analyses to regularize differ-ences between ways of expressing the same idea.For statistical methods, these regularizations, in ef-fect, reduce the number of significant differences be-tween observable patterns in data and raise the fre-quency of each difference.
Patterns are thus easierto learn from training data and easier to recognize intest data, thus somewhat compensating for the spare-ness of data.
In addition, deeper analyses are oftenconsidered semantic in nature because conceptually,two expressions that share the same regularized formalso share some aspects of meaning.
The specific de-tails of this ?deep?
analysis have varied quite a bit,perhaps more than surface syntax.In the 1970s and 1980s, Lexical Function Gram-mar?s (LFG) way of dividing C-structure (surface)and F-structure (deep) led to parsers such as (Hobbsand Grishman, 1976) which produced these two lev-els, typically in two stages.
However, enthusiasmfor these two-stage parsers was eclipsed by the ad-vent of one stage parsers with much higher accu-racy (about 90% vs about 60%), the now-populartreebank-based parsers including (Charniak, 2001;Collins, 1999) and many others.
Currently, manydifferent ?deeper?
levels are being manually anno-tated and automatically transduced, typically usingsurface parsing and other processors as input.
Oneof the most popular, semantic role labels (annota-tion and transducers based on the annotation) char-acterize relations anchored by select predicate typeslike verbs (Palmer et al, 2005), nouns (Meyers etal., 2004a), discourse connectives (Miltsakaki et al,2004) or those predicates that are part of particularsemantic frames (Baker et al, 1998).
The CONLLtasks for 2008 and 2009 (Surdeanu et al, 2008;Hajic?
et al, 2009) has focused on unifying many ofthese individual efforts to produce a logical structurefor multiple parts of speech and multiple languages.Like the CONLL shared task, we link surface lev-els to logical levels for multiple languages.
How-ever, there are several differences: (1) The logicalstructures produced automatically by our system canbe expected to be more accurate than the compara-ble CONLL systems because our task involves pre-dicting semantic roles with less fine-grained distinc-tions.
Our English and Japanese results were higherthan the CONLL 2009 SRL systems.
Our English F-scores range from 76.3% (spoken) to 89.9% (News):146the best CONLL 2009 English scores were 73.31%(Brown) and 85.63% (WSJ).
Our Japanese systemscored 90.6%: the best CONLL 2009 Japanese scorewas 78.35%.
Our Chinese system 74.5%, 4 pointslower than the best CONLL 2009 system (78.6%),probably due to our system?s failings, rather than thecomplexity of the task; (2) Each of the languagesin our system uses the same linguistic framework,using the same types of relations, same analyses ofcomparable constructions, etc.
In one case, this re-quired a conversion from a different framework toour own.
In contrast, the 2009 CONLL task putsseveral different frameworks into one compatible in-put format.
(3) The logical structures produced byour system typically connect all the words in the sen-tence.
While this is true for some of the CONLL2009 languages, e.g., Czech, it is not true aboutall the languages.
In particular, the CONLL 2009English and Chinese logical structures only includenoun and verb predicates.In this paper, we will describe the GLARF frame-work (Grammatical and Logical RepresentationFramework) and a system for producing GLARFoutput (Meyers et al, 2001; Meyers, 2008).
GLARFprovides a logical structure for English, Chinese andJapanese with an F-score that is within a few per-centage points of the best parsing results for thatlanguage.
Like LFG?s (LFG) F-structure, our log-ical structure is less fine-grained than many of thepopular semantic role labeling schemes, but also hastwo main advantages over these schemes: it is morereliable and it is more comprehensive in the sensethat it covers all parts of speech and the resultinglogical structure is a connected graph.
Our approachhas proved adequate for three genetically unrelatednatural languages: English, Chinese and Japanese.It is thus a good candidate for additional languageswith accurate parsers.2 The GLARF frameworkOur system creates a multi-tiered representation inthe GLARF framework, combining the theory un-derlying the Penn Treebank for English (Marcus etal., 1994) and Chinese (Xue et al, 2005) (Chom-skian linguistics of the 1970s and 1980s) with: (2)Relational Grammar?s graph-based way of repre-senting ?levels?
as sequences of relations; (2) Fea-ture structures in the style of Head-Driven PhraseStructure Grammar; and (3) The Z. Harris style goalof attempting to regularize multiple ways of sayingthe same thing into a single representation.
Ourapproach differs from LFG F-structure in severalways: we have more than two levels; we have adifferent set of relational labels; and finally, our ap-proach is designed to be compatible with the PennTreebank framework and therefore, Penn-Treebank-based parsers.
In addition, the expansion of our the-ory is governed more by available resources than bythe underlying theory.
As our main goal is to useour system to regularize data, we freely incorporateany analysis that fits this goal.
Over time, we havefound ways of incorporating Named Entities, Prop-Bank, NomBank and the Penn Discourse Treebank.Our agenda also includes incorporating the results ofother research efforts (Pustejovsky et al, 2005).For each sentence, we generate a feature structure(FS) representing our most complete analysis.
Wedistill a subset of this information into a dependencystructure governed by theoretical assumptions, e.g.,about identifying functors of phrases.
Each GLARFdependency is between a functor and an argument,where the functor is the head of a phrase, conjunc-tion, complementizer, or other function word.
Wehave built applications that use each of these tworepresentations, e.g., the dependency representationis used in (Shinyama, 2007) and the FS represen-tation is used in (K. Parton and K. R. McKeownand R. Coyne and M. Diab and R. Grishman andD.
Hakkani-Tu?r and M. Harper and H. Ji and W. Y.Ma and A. Meyers and S. Stolbach and A.
Sun andG.
Tu?r and W. Xu and S. Yarman, 2009).In the dependency representation, each sentenceis a set of 23 tuples, each 23-tuple characterizing upto three relations between two words: (1) a SUR-FACE relation, the relation between a functor and anargument in the parse of a sentence; (2) a LOGIC1relation which regularizes for lexical and syntac-tic phenomena like passive, relative clauses, deletedsubjects; and (3) a LOGIC2 relation correspondingto relations in PropBank, NomBank, and the PennDiscourse Treebank (PDTB).
While the full outputhas all this information, we will limit this paper toa discussion of the LOGIC1 relations.
Figure 1 isa 5 tuple subset of the 23 tuple GLARF analysis ofthe sentence Who was eaten by Grendel?
(The full147L1 Surf L2 Func ArgNIL SENT NIL Who wasPRD PRD NIL was eatenCOMP COMP ARG0 eaten byOBJ NIL ARG1 eaten WhoNIL OBJ NIL by GrendelSBJ NIL NIL eaten GrendelFigure 1: 5-tuples: Who was eaten by GrendelWhoeatenwasbyPRDS?OBJL?OBJARG1COMPARG0S?SENTL?SBJGrendelFigure 2: Graph of Who was eaten by Grendel23 tuples include unique ids and fine-grained lin-guistic features).
The fields listed are: logic1 label(L1), surface label (Surf), logic2 label (L2), func-tor (Func) and argument (Arg).
NIL indicates thatthere is no relation of that type.
Figure 2 repre-sents this as a graph.
For edges with two labels,the ARG0 or ARG1 label indicates a LOGIC2 re-lation.
Edges with an L- prefix are LOGIC1 la-bels (the edges are curved); edges with S-prefixesare SURFACE relations (the edges are dashed); andother (thick) edges bear unprefixed labels represent-ing combined SURFACE/LOGIC1 relations.
Delet-ing the dashed edges yields a LOGIC1 representa-tion; deleting the curved edges yields a SURFACErepresentation; and a LOGIC2 consists of the edgeslabeled ARGO and ARG1 relations, plus the sur-face subtrees rooted where the LOGIC2 edges ter-minate.
Taken together, a sentence?s SURFACE re-lations form a tree; the LOGIC1 relations form adirected acyclic graph; and the LOGIC2 relationsform directed graphs with some cycles and, due toPDTB relations, may connect sentences to previousones, e.g., adverbs like however, take the previoussentence as one of their arguments.LOGIC1 relations (based on Relational Gram-mar) regularize across grammatical and lexical al-ternations.
For example, subcategorized verbal ar-guments include: SBJect, OBJect and IND-OBJ (in-direct Object), COMPlement, PRT (Particle), PRD(predicative complement).
Other verbal modifiersinclude AUXilliary, PARENthetical, ADVerbial.
Incontrast, FrameNet and PropBank make finer dis-tinctions.
Both PP arguments of consulted in Johnconsulted with Mary about the project bear COMPrelations with the verb in GLARF, but would havedistinct labels in both PropBank and FrameNet.Thus Semantic Role Labeling (SRL) should be moredifficult than recognizing LOGIC1 relations.Beginning with Penn Treebank II, Penn Treebankannotation includes Function tags, hyphenated addi-tions to phrasal categories which indicate their func-tion.
There are several types of function tags:?
Argument Tags such as SBJ, OBJ, IO (IND-OBJ), CLR (COMP) and PRD?These are lim-ited to verbal relations and not all are used inall treebanks.
For example, OBJ and IO areused in the Chinese, but not the English tree-bank.
These labels can often be directly trans-lated into GLARF LOGIC1 relations.?
Adjunct Tags such as ADV, TMP, DIR, LOC,MNR, PRP?These tags often translate into asingle LOGIC1 tag (ADV).
However, some ofthese also correspond to LOGIC1 arguments.In particular, some DIR and MNR tags are re-alized as LOGIC1 COMP relations (based ondictionary entries).
The fine grained seman-tic distinctions are maintained in other featuresthat are part of the GLARF description.In addition, GLARF treats Penn?s PRN phrasalcategory as a relation rather than a phrasal category.For example, given a sentence like, Banana ketchup,the agency claims, is very nutritious, the phrasethe agency claims is analyzed as an S(entence) inGLARF bearing a (surface) PAREN relation to themain clause.
Furthermore, the whole sentence is aCOMP of the verb claims.
Since PAREN is a SUR-FACE relation, not a LOGIC1 relation, there is noLOGIC1 cycle as shown by the set of 5-tuples inFigure 3?
a cycle only exists if you include bothSURFACE and LOGIC1 relations in a single graph.Another important feature of the GLARF frame-work is transparency, a term originating from N.148L1 Surf L2 Func ArgNIL SBJ ARG1 is ketchupPRD PRD ARG2 is nutritiousSBJ NIL NIL nutritious KetchupADV ADV NIL nutritious veryN-POS N-POS NIL ketchup BananaNIL PAREN NIL is claimsSBJ SBJ ARG0 claims agencyQ-POS Q-POS NIL agency theCOMP NIL ARG1 claims isFigure 3: 5-tuples: Banana Ketchup, the agency claims,is very nutritiousL1 Surf L2 Func ArgSBJ SBJ ARG0 ate andOBJ OBJ ARG1 ate boxCONJ CONJ NIL and JohnCONJ CONJ NIL and MaryCOMP COMP NIL box ofQ-POS Q-POS NIL box theOBJ OBJ NIL of cookiesFigure 4: 5-tuples: John and Mary ate the box of cookiesSager?s unpublished work.
A relation between twowords is transparent if: the functor fails to character-ize the selectional properties of the phrase (or sub-graph in a Dependency Analysis), but its argumentdoes.
For example, relations between conjunctions(e.g., and, or, but) and their conjuncts are transparentCONJ relations.
Thus although and links togetherJohn and Mary, it is these dependents that deter-mine that the resulting phrase is noun-like (an NPin phrase structure terminology) and sentient (andthus can occur as the subject of verbs like ate).
An-other common example of transparent relations arethe relations connecting certain nouns and the prepo-sitional objects under them, e.g., the box of cookiesis edible, because cookies are edible even thoughboxes are not.
These features are marked in theNOMLEX-PLUS dictionary (Meyers et al, 2004b).In Figure 4, we represent transparent relations, byprefixing the LOGIC1 label with asterisks.The above description most accurately describesEnglish GLARF.
However, Chinese GLARF hasmost of the same properties, the main exception be-ing that PDTB arguments are not currently marked.For Japanese, we have only a preliminary represen-tation of LOGIC2 relations and they are not derivedfrom PropBank/NomBank/PDTB.2.1 Scoring the LOGIC1 StructureFor purposes of scoring, we chose to focus onLOGIC1 relations, our proposed high-performancelevel of semantics.
We scored with respect to: theLOGIC1 relational label, the identity of the functorand the argument, and whether the relation is trans-parent or not.
If the system output differs in any ofthese respects, the relation is marked wrong.
Thefollowing sections will briefly describe each systemand present an evaluation of its results.The answer keys for each language were createdby native speakers editing system output, as repre-sented similarly to the examples in this paper, al-though part of speech is included for added clar-ity.
In addition, as we attempted to evaluate logi-cal relation (or dependency) accuracy independentof sentence splitting.
We obtained sentence divi-sions from data providers and treebank annotationfor all the Japanese and most of the English data, butused automatic sentence divisions for the EnglishBLOG data.
For the Chinese, we omitted severalsentences from our evaluation set due to incorrectsentence splits.
The English and Japanese answerkeys were annotated by single native speakers ex-pert in GLARF.
The Chinese data was annotated byseveral native speakers and may have been subjectto some interannotator agreement difficulties, whichwe intend to resolve in future work.
Currently, cor-recting system output is the best way to create an-swer keys due to certain ambiguities in the frame-work, some of which we hope to incorporate into fu-ture scoring procedures.
For example, consider theinterpretation of the phrase five acres of land in Eng-land with respect to PP attachment.
The differencein meaning between attaching the PP in Englandto acres or to land is too subtle for these authors?we have difficulty imagining situations where onestatement would be accurate and the other wouldnot.
This ambiguity is completely predictable be-cause acres is a transparent noun and similar ambi-guities hold for all such cases where a transparentnoun takes a complement and is followed by a PPattachment.
We believe that a more complex scor-ing program could account for most of these cases.149Similar complexities arise for coordination and sev-eral other phenomena.3 English GLARFWe generate English GLARF output by applying aprocedure that combines:1.
The output of the 2005 version of the Charniakparser described in (Charniak, 2001), whichlabel precision and recall scores in the 85%range.
The updated version of the parser seemsto perform closer to 90% on News data and per-form lower on other genres.
That performancewould reflect reports on other versions of theCharniak parser for which statistics are avail-able (Foster and van Genabith, 2008).2.
Named entity (NE) tags from the JET NE sys-tem (Ji and Grishman, 2006), which achievesF-scores ranging 86%-91% on newswire forboth English and Chinese (depending onEpoch).
The JET system identifies sevenclasses of NEs: Person, GPE, Location, Orga-nization, Facility, Weapon and Vehicle.3.
Machine Readable dictionaries: COMLEX(Macleod et al, 1998), NOMBANK dictio-naries (from http://nlp.cs.nyu.edu/meyers/nombank/) and others.4.
A sequence of hand-written rules (citationsomitted) such that: (1) the first set of rules con-vert the Penn Treebank into a Feature Structurerepresentation; and (2) each rule N after thefirst rule is applied to an entire Feature Struc-ture that is the output of rule N ?
1.For this paper, we evaluated the English output forseveral different genres, all of which approximatelytrack parsing results for that genre.
For writtengenres, we chose between 40 and 50 sentences.For speech transcripts, we chose 100 sentences?wechose this larger number because a lot of so-calledsentences contained text with empty logical de-scriptions, e.g., single word utterances contain norelations between pairs of words.
Each text comesfrom a different genre.
For NEWS text, we used 50sentences from the aligned Japanese-English datacreated as part of the JENAAD corpus (UtiyamaGenre Prec Rec FNEWS 731815 = 89.7%715812 = 90.0% 89.9%BLOG 704844 = 83.4%704899 = 78.3% 80.8%LETT 392434 = 90.3%392449 = 87.3% 88.8%TELE 472604 = 78.1%472610 = 77.4% 77.8%NARR 732959 = 76.3%732964 = 75.9% 76.1%Table 1: English Aggregate ScoresCorpus Prec Rec F SentsNEWS 90.5% 90.8% 90.6% 50BLOG 84.1% 79.6% 81.7% 46LETT 93.9% 89.2% 91.4% 46TELE 81.4% 83.2% 84.9% 103NARR 77.1% 78.1% 79.5% 100Table 2: English Score per Sentenceand Isahara, 2003); the web text (BLOGs) wastaken from some corpora provided by the LinguisticData Consortium through the GALE (http://projects.ldc.upenn.edu/gale/) pro-gram; the LETTer genre (a letter from Good Will)was taken from the ICIC Corpus of FundraisingTexts (Indiana Center for Intercultural Communi-cation); Finally, we chose two spoken languagetranscripts: a TELEphone conversation fromthe Switchboard Corpus (http://www.ldc.upenn.edu/Catalog/readme_files/switchboard.readme.html) and one NAR-Rative from the Charlotte Narrative and Conversa-tion Collection (http://newsouthvoices.uncc.edu/cncc.php).
In both cases, weassumed perfect sentence splitting (based on PennTreebank annotation).
The ICIC, Switchboardand Charlotte texts that we used are part of theOpen American National Corpus (OANC), inparticular, the SIGANN shared subcorpus of theOANC (http://nlp.cs.nyu.edu/wiki/corpuswg/ULA-OANC-1) (Meyers et al, 2007).Comparable work for English includes: (1) (Gab-bard et al, 2006), a system which reproduces thefunction tags of the Penn Treebank with 89% accu-racy and empty categories (and their antecedents)with varying accuracies ranging from 82.2% to96.3%, excluding null complementizers, as these aretheory-internal and have no value for filling gaps.
(2) Current systems that generate LFG F-structure150such as (Wagner et al, 2007) which achieve an Fscore of 91.1 on the F-structure PRED relations,which are similar to our LOGIC1 relations.4 Chinese GLARFThe Chinese GLARF program takes a ChineseTreebank-style syntactic parse and the output of aChinese PropBanker (Xue, 2008) as input, and at-tempts to determine the relations between the headand its dependents within each constituent.
It doesthis by first exploiting the structural informationand detecting six broad categories of syntactic rela-tions that hold between the head and its dependents.These are predication, modification, complementa-tion, coordination, auxiliary, and flat.
Predicationholds at the clause level between the subject and thepredicate, where the predicate is considered to bethe head and the subject is considered to the depen-dent.
Modification can also hold mainly within NPsand VPs, where the dependents are modifiers of theNP head or adjuncts to the head verb.
Coordinationholds almost for all phrasal categories where eachnon-punctuation child within this constituent is ei-ther conjunction or a conjunct.
The head in a co-ordination structure is underspecified and can be ei-ther a conjunct or a conjunction depending on thegrammatical framework.
Complementation holdsbetween a head and its complement, with the com-plement usually being a core argument of the head.For example, inside a PP, the preposition is the headand the phrase or clause it takes is the dependent.
Anauxiliary structure is one where the auxiliary takesa VP as its complement.
This structure is identi-fied so that the auxiliary and the verb it modifies canform a verb group in the GLARF framework.
Flatstructures are structures where a constituent has nomeaningful internal structure, which is possible in asmall number of cases.
After these six broad cate-gories of relations are identified, more fine-grainedrelation can be detected with additional information.Figure 5 is a sample 4-tuple for a Chinese translationof the sentence in figure 3.For the results reported in Table 3, we used theHarper and Huang parser described in (Harper andHuang, Forthcoming) which can achieve F-scoresas high as 85.2%, in combination with informa-tion about named entities from the output of theFigure 5: Agency claims, Banana Ketchup is very havenutrition DE.JET Named Entity tagger for Chinese (86%-91% F-measure as per section 3).
We used the NE tags toadjust the parts of speech and the phrasal boundariesof named entities (we do the same with English).As shown in Table 3, we tried two versions of theHarper and Huang parser, one which adds functiontags to the output and one that does not.
The ChineseGLARF system scores significantly (13.9% F-score)higher given function tagged input, than parser out-put without function tags.
Our current score is about10 points lower than the parser score.
Our initial er-ror analysis suggests that the most common formsof errors involve: (1) the processing of long NPs;(2) segmentation and POS errors; (3) conjunctionscope; and (4) modifier attachment.5 Japanese GLARFFor Japanese, we process text with the KNP parser(Kurohashi and Nagao, 1998) and convert the outputinto the GLARF framework.
The KNP/Kyoto Cor-pus framework is a Japanese-specific Dependencyframework, very different from the Penn Treebankframework used for the other systems.
Process-ing in Japanese proceeds as follows: (1) we pro-cess the Japanese with the Juman segmenter (Kuro-151Type Prec Rec FNo Function Tags VersionAggr 8431374 = 61.4%8431352 = 62.4% 61.8%Aver 62.3% 63.5% 63.6%Function Tags VersionAggr 10311415 = 72.9%10311352 = 76.3% 74.5%Aver 73.0% 75.3% 74.9%Table 3: 53 Chinese Newswire Sentences: Aggregate andAverage Sentence Scoreshashi et al, 1994) and KNP parser 2.0 (Kurohashiand Nagao, 1998), which has reported accuracy of91.32% F score for dependency accuracy, as re-ported in (Noro et al, 2005).
As is standard inJapanese linguistics, the KNP/Kyoto Corpus (K)framework uses a dependency analysis that has somefeatures of a phrase structure analysis.
In partic-ular, the dependency relations are between bun-setsu, small constituents which include a head wordand some number of modifiers which are typicallyfunction words (particles, auxiliaries, etc.
), but canalso be prenominal noun modifiers.
Bunsetsu canalso include multiple words in the case of names.The K framework differentiates types of dependen-cies into: the normal head-argument variety, coor-dination (or parallel) and apposition.
We convertthe head-argument variety of dependency straight-forwardly into a phrase consisting of the head andall the arguments.
In a similar way, appositive re-lations could be represented using an APPOSITIVErelation (as is currently done with English).
In thecase of bunsetsu, the task is to choose a head andlabel the other constituents?This is very similar toour task of labeling and subdividing the flat nounphrases of the English Penn Treebank.
Conjunctionis a little different because the K analysis assumesthat the final conjunct is the functor, rather than aconjunction.
We automatically changed this analy-sis to be the same as it is for English and Chinese.When there was no actual conjunction, we created atheory-internal NULL conjunction.
The final stagesinclude: (1) processing conjunction and apposition,including recognizing cases that the parser does notrecognize; (2) correcting parts of speech; (3) label-ing all relations between arguments and heads; (4)recognizing and labeling special constituent typesFigure 6: It is the state?s duty to protect lives and assets.Type Prec Rec FAggr 764843 = 91.0%764840 = 90.6% 90.8%Aver 90.7% 90.6% 90.6%Table 4: 40 Japanese Sentences from JENAA Corpus:Aggregate and Average Sentence Scoressuch as Named Entities, double quote constituentsand number phrases (twenty one); (5) handling com-mon idioms; and (6) processing light verb and cop-ula constructions.Figure 6 is a sample 4-tuple for a Japanesesentence meaning It is the state?s duty to protectlives and assets.
Conjunction is handled as dis-cussed above, using an invisible NULL conjunctionand transparent (asterisked) logical CONJ relations.Copulas in all three languages take surface subjects,which are the LOGIC1 subjects of the PRD argu-ment of the copula.
We have left out glosses for theparticles, which act solely as case markers and helpus identify the grammatical relation.We scored Japanese GLARF on forty sentences ofthe Japanese side of the JENAA data (25 of whichare parallel with the English sentences scored).
Likethe English, the F score is very close to the parsingscores achieved by the parser.1526 Concluding Remarks and Future WorkIn this paper, we have described three systemsfor generating GLARF representations automati-cally from text, each system combines the out-put of a parser and possibly some other processor(segmenter, Named Entity Recognizer, PropBanker,etc.)
and creates a logical representation of the sen-tence.
Dictionaries, word lists, and various otherresources are used, in conjunction with hand writ-ten rules.
In each case, the results are very close toparsing accuracy.
These logical structures are in thesame annotation framework, using the same labelingscheme and the same analysis for key types of con-structions.
There are several advantages to our ap-proach over other characterizations of logical struc-ture: (1) our representation is among the most accu-rate and reliable; (2) our representation connects allthe words in the sentence; and (3) having the samerepresentation for multiple languages facilitates run-ning the same procedures in multiple languages andcreating multilingual applications.The English system was developed for the Newsgenre, specifically the Penn Treebank Wall StreetJournal Corpus.
We are therefore consideringadding rules to better handle constructions that ap-pear in other genres, but not news.
The experi-ments describe here should go a long way towardsachieving this goal.
We are also considering ex-periments with parsers tailored to particular genresand/or parsers that add function tags (Harper et al,2005).
In addition, our current GLARF system usesinternal Propbank/NomBank rules, which have goodprecision, but low recall.
We expect that we achievebetter results if we incorporate the output of stateof the art SRL systems, although we would have toconduct experiments as to whether or not we can im-prove such results with additional rules.We developed the English system over the courseof eight years or so.
In contrast, the Chinese andJapanese systems are newer and considerably lesstime was spent developing them.
Thus they cur-rently do not represent as many regularizations.
Oneobstacle is that we do not currently use subcate-gorization dictionaries for either language, whilewe have several for English.
In particular, thesewould be helpful in predicting and filling relativeclause and others gaps.
We are considering auto-matically acquiring simple dictionaries by recordingfrequently occurring argument types of verbs overa larger corpus, e.g., along the lines of (Kawaharaand Kurohashi, 2002).
In addition, existing Japanesedictionaries such as the IPAL (monolingual) dictio-nary (technology Promotion Agency, 1987) or previ-ously acquired case information reported in (Kawa-hara and Kurohashi, 2002).Finally, we are investigating several avenues forusing this system output for Machine Translation(MT) including: (1) aiding word alignment for otherMT system (Wang et al, 2007); and (2) aiding thecreation various MT models involving analyzed text,e.g., (Gildea, 2004; Shen et al, 2008).AcknowledgmentsThis work was supported by NSF Grant IIS-0534700 Structure Alignment-based MT.ReferencesC.
F. Baker, C. J. Fillmore, and J.
B. Lowe.
1998.
TheBerkeley FrameNet Project.
In Coling-ACL98, pages86?90.E.
Charniak.
2001.
Immediate-head parsing for languagemodels.
In ACL 2001, pages 116?123.N.
Chomsky.
1957.
Syntactic Structures.
Mouton, TheHague.M.
Collins.
1999.
Head-Driven Statistical Models forNatural Language Parsing.
Ph.D. thesis, Universityof Pennsylvania.J.
Foster and J. van Genabith.
2008.
Parser Evaluationand the BNC: 4 Parsers and 3 Evaluation Metrics.
InLREC 2008, Marrakech, Morocco.R.
Gabbard, M. Marcus, and S. Kulick.
2006.
Fully pars-ing the penn treebank.
In NAACL/HLT, pages 184?191.D.
Gildea.
2004.
Dependencies vs.
Constituents forTree-Based Alignment.
In EMNLP, Barcelona.J.
Hajic?, M. Ciaramita, R. Johansson, D. Kawahara,M.
A.
Mart?
?, L. Ma`rquez, A. Meyers, J. Nivre, S. Pado?,J.
?Ste?pa?nek, P. Stran?a?k, M. Surdeanu, N. Xue, andY.
Zhang.
2009.
The CoNLL-2009 shared task:Syntactic and semantic dependencies in multiple lan-guages.
In CoNLL-2009, Boulder, Colorado, USA.M.
Harper and Z. Huang.
Forthcoming.
Chinese Statis-tical Parsing.
In J. Olive, editor, Global AutonomousLanguage Exploitation.
Publisher to be Announced.M.
Harper, B. Dorr, J. Hale, B. Roark, I. Shafran,M.
Lease, Y. Liu, M. Snover, L. Yung, A. Krasnyan-skaya, and R. Stewart.
2005.
Parsing and Spoken153Structural Event.
Technical Report, The John-HopkinsUniversity, 2005 Summer Research Workshop.Z.
Harris.
1968.
Mathematical Structures of Language.Wiley-Interscience, New York.J.
R. Hobbs and R. Grishman.
1976.
The AutomaticTransformational Analysis of English Sentences: AnImplementation.
International Journal of ComputerMathematics, 5:267?283.H.
Ji and R. Grishman.
2006.
Analysis and Repair ofName Tagger Errors.
In COLING/ACL 2006, Sydney,Australia.K.
Parton and K. R. McKeown and R. Coyne and M. Diaband R. Grishman and D. Hakkani-Tu?r and M. Harperand H. Ji and W. Y. Ma and A. Meyers and S. Stol-bach and A.
Sun and G. Tu?r and W. Xu and S. Yarman.2009.
Who, What, When, Where, Why?
ComparingMultiple Approaches to the Cross-Lingual 5W Task.In ACL 2009.D.
Kawahara and S. Kurohashi.
2002.
Fertilizationof Case Frame Dictionary for Robust Japanese CaseAnalysis.
In Proc.
of COLING 2002.S.
Kurohashi and M. Nagao.
1998.
Building a Japaneseparsed corpus while improving the parsing system.
InProceedings of The 1st International Conference onLanguage Resources & Evaluation, pages 719?724.S.
Kurohashi, T. Nakamura, Y. Matsumoto, and M. Na-gao.
1994.
Improvements of Japanese Morpholog-ical Analyzer JUMAN.
In Proc.
of InternationalWorkshop on Sharable Natural Language Resources(SNLR), pages 22?28.C.
Macleod, R. Grishman, and A. Meyers.
1998.
COM-LEX Syntax.
Computers and the Humanities, 31:459?481.M.
P. Marcus, B. Santorini, and M. A. Marcinkiewicz.1994.
Building a large annotated corpus of en-glish: The penn treebank.
Computational Linguistics,19(2):313?330.A.
Meyers, M. Kosaka, S. Sekine, R. Grishman, andS.
Zhao.
2001.
Parsing and GLARFing.
In Proceed-ings of RANLP-2001, Tzigov Chark, Bulgaria.A.
Meyers, R. Reeves, C. Macleod, R. Szekely, V. Zielin-ska, B.
Young, and R. Grishman.
2004a.
The Nom-Bank Project: An Interim Report.
In NAACL/HLT2004 Workshop Frontiers in Corpus Annotation,Boston.A.
Meyers, R. Reeves, Catherine Macleod, Rachel Szeke-ley, Veronkia Zielinska, and Brian Young.
2004b.
TheCross-Breeding of Dictionaries.
In Proceedings ofLREC-2004, Lisbon, Portugal.A.
Meyers, N. Ide, L. Denoyer, and Y. Shinyama.
2007.The shared corpora working group report.
In Pro-ceedings of The Linguistic Annotation Workshop, ACL2007, pages 184?190, Prague, Czech Republic.A.
Meyers.
2008.
Using treebank, dictionaries andglarf to improve nombank annotation.
In Proceedingsof The Linguistic Annotation Workshop, LREC 2008,Marrakesh, Morocco.E.
Miltsakaki, A. Joshi, R. Prasad, and B. Webber.
2004.Annotating discourse connectives and their arguments.In A. Meyers, editor, NAACL/HLT 2004 Workshop:Frontiers in Corpus Annotation, pages 9?16, Boston,Massachusetts, USA, May 2 - May 7.
Association forComputational Linguistics.T.
Noro, C. Koike, T. Hashimoto, T. Tokunaga, andHozumi Tanaka.
2005.
Evaluation of a Japanese CFGDerived from a Syntactically Annotated corpus withRespect to Dependency Measures.
In 2005 Workshopon Treebanks and Linguistic theories, pages 115?126.M.
Palmer, D. Gildea, and P. Kingsbury.
2005.
TheProposition Bank: An annotated corpus of semanticroles.
Computational Linguistics, 31(1):71?106.J.
Pustejovsky, A. Meyers, M. Palmer, and M. Poe-sio.
2005.
Merging PropBank, NomBank, TimeBank,Penn Discourse Treebank and Coreference.
In ACL2005 Workshop: Frontiers in Corpus Annotation II:Pie in the Sky.L.
Shen, J. Xu, and R. Weischedel.
2008.
A New String-to-Dependency Machine Translation Algorithm with aTarget Dependency Language Model.
In ACL 2008.Y.
Shinyama.
2007.
Being Lazy and Preemptive atLearning toward Information Extraction.
Ph.D. the-sis, NYU.M.
Surdeanu, R. Johansson, A. Meyers, Ll.
Ma?rquez,and J. Nivre.
2008.
The CoNLL-2008 Shared Taskon Joint Parsing of Syntactic and Semantic Dependen-cies.
In Proceedings of the CoNLL-2008 Shared Task,Manchester, GB.Information technology Promotion Agency.
1987.
IPALexicon of the Japanese Language for ComputersIPAL (Basic Verbs).
(in Japanese).M.
Utiyama and H. Isahara.
2003.
Reliable Measuresfor Aligning Japanese-English News Articles and Sen-tences.
In ACL-2003, pages 72?79.J.
Wagner, D. Seddah, J.
Foster, and J. van Genabith.2007.
C-Structures and F-Structures for the BritishNational Corpus.
In Proceedings of the Twelfth In-ternational Lexical Functional Grammar Conference,Stanford.
CSLI Publications.C.
Wang, M. Collins, and P. Koehn.
2007.
Chinese syn-tactic reordering for statistical machine translation.
InEMNLP-CoNLL 2007, pages 737?745.N.
Xue, F. Xia, F. Chiou, and M. Palmer.
2005.
ThePenn Chinese TreeBank: Phrase Structure Annotationof a Large Corpus.
Natural Language Engineering,11:207?238.N.
Xue.
2008.
Labeling Chinese Predicates with Seman-tic roles.
Computational Linguistics, 34:225?255.154
