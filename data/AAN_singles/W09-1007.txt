Proceedings of the EACL 2009 Workshop on Computational Linguistic Aspects of Grammatical Inference, pages 41?48,Athens, Greece, 30 March 2009. c?2009 Association for Computational LinguisticsLanguage models for contextual error detection and correctionHerman StehouwerTilburg Centre for Creative ComputingTilburg UniversityTilburg, The Netherlandsj.h.stehouwer@uvt.nlMenno van ZaanenTilburg Centre for Creative ComputingTilburg UniversityTilburg, The Netherlandsmvzaanen@uvt.nlAbstractThe problem of identifying and correctingconfusibles, i.e.
context-sensitive spellingerrors, in text is typically tackled usingspecifically trained machine learning clas-sifiers.
For each different set of con-fusibles, a specific classifier is trained andtuned.In this research, we investigate a moregeneric approach to context-sensitive con-fusible correction.
Instead of using spe-cific classifiers, we use one generic clas-sifier based on a language model.
Thismeasures the likelihood of sentences withdifferent possible solutions of a confusiblein place.
The advantage of this approachis that all confusible sets are handled bya single model.
Preliminary results showthat the performance of the generic clas-sifier approach is only slightly worse thatthat of the specific classifier approach.1 IntroductionWhen writing texts, people often use spellingcheckers to reduce the number of spelling mis-takes in their texts.
Many spelling checkers con-centrate on non-word errors.
These errors can beeasily identified in texts because they consist ofcharacter sequences that are not part of the lan-guage.
For example, in English woord is is notpart of the language, hence a non-word error.
Apossible correction would be word .Even when a text does not contain any non-word errors, there is no guarantee that the text iserror-free.
There are several types of spelling er-rors where the words themselves are part of thelanguage, but are used incorrectly in their context.Note that these kinds of errors are much harderto recognize, as information from the context inwhich they occur is required to recognize and cor-rect these errors.
In contrast, non-word errors canbe recognized without context.One class of such errors, called confusibles,consists of words that belong to the language, butare used incorrectly with respect to their local,sentential context.
For example, She owns to carscontains the confusible to.
Note that this word isa valid token and part of the language, but usedincorrectly in the context.
Considering the con-text, a correct and very likely alternative would bethe word two.
Confusibles are grouped together inconfusible sets.
Confusible sets are sets of wordsthat are similar and often used incorrectly in con-text.
Too is the third alternative in this particularconfusible set.The research presented here is part of alarger project, which focusses on context-sensitivespelling mistakes in general.
Within this projectall classes of context-sensitive spelling errors aretackled.
For example, in addition to confusibles,a class of pragmatically incorrect words (wherewords are incorrectly used within the document-wide context) is considered as well.
In this arti-cle we concentrate on the problem of confusibles,where the context is only as large as a sentence.2 ApproachA typical approach to the problem of confusiblesis to train a machine learning classifier to a specificconfusible set.
Most of the work in this area hasconcentrated on confusibles due to homophony(to , too , two) or similar spelling (desert , dessert ).However, some research has also touched upon in-flectional or derivational confusibles such as I ver-sus me (Golding and Roth, 1999).
For instance,when word forms are homophonic, they tend toget confused often in writing (cf.
the situation withto, too , and two, affect and effect , or there , their ,and they?re in English) (Sandra et al, 2001; Vanden Bosch and Daelemans, 2007).41Most work on confusible disambiguation usingmachine learning concentrates on hand-selectedsets of notorious confusibles.
The confusible setsare typically very small (two or three elements)and the machine learner will only see trainingexamples of the members of the confusible set.This approach is similar to approaches used in ac-cent restoration (Yarowsky, 1994; Golding, 1995;Mangu and Brill, 1997; Wu et al, 1999; Even-Zohar and Roth, 2000; Banko and Brill, 2001;Huang and Powers, 2001; Van den Bosch, 2006).The task of the machine learner is to decide, us-ing features describing information from the con-text, which word taken from the confusible set re-ally belongs in the position of the confusible.
Us-ing the example above, the classifier has to decidewhich word belongs on the position of the X inShe owns X cars , where the possible answers forX are to , too , or two.
We call X, the confusiblethat is under consideration, the focus word.Another way of looking at the problem of con-fusible disambiguation is to see it as a very spe-cialized case of word prediction.
The problem isthen to predict which word belongs at a specificposition.
Using similarities between these cases,we can use techniques from the field of languagemodeling to solve the problem of selecting the bestalternative from confusible sets.
We will investi-gate this approach in this article.Language models assign probabilities to se-quences of words.
Using this information, itis possible to predict the most likely word ina certain context.
If a language model givesus the probability for a sequence of n wordsPLM (w1, .
.
.
, wn), we can use this to predict themost likely word w following a sequence of n?
1words arg maxw PLM (w1, .
.
.
, wn?1, w).
Obvi-ously, a similar approach can be taken with w inthe middle of the sequence.Here, we will use a language model as a classi-fier to predict the correct word in a context.
Sincea language model models the entire language, it isdifferent from a regular machine learning classifiertrained on a specific set of confusibles.
The advan-tage of this approach to confusible disambiguationis that the language model can handle all potentialconfusibles without any further training and tun-ing.
With the language model it is possible to takethe words from any confusible set and compute theprobabilities of those words in the context.
Theelement from the confusible set that has the high-est probability according to the language model isthen selected.
Since the language model assignsprobabilities to all sequences of words, it is pos-sible to define new confusible sets on the fly andlet the language model disambiguate them with-out any further training.
Obviously, this is notpossible for a specialized machine learning clas-sifier approach, where a classifier is fine-tuned tothe features and classes of a specific confusible set.The expected disadvantage of the generic (lan-guage model) classifier approach is that the accu-racy is expected to be less than that of the specific(specialized machine learning classifier) approach.Since the specific classifiers are tuned to each spe-cific confusible set, the weights for each of thefeatures may be different for each set.
For in-stance, there may be confusibles for which the cor-rect word is easily identified by words in a specificposition.
If a determiner, like the , occurs in the po-sition directly before the confusible, to or too arevery probably not the correct answers.
The spe-cific approach can take this into account by assign-ing specific weights to part-of-speech and positioncombinations, whereas the generic approach can-not do this explicitly for specific cases; the weightsfollow automatically from the training corpus.In this article, we will investigate whether it ispossible to build a confusible disambiguation sys-tem that is generic for all sets of confusibles usinglanguage models as generic classifiers and investi-gate in how far this approach is useful for solvingthe confusible problem.
We will compare thesegeneric classifiers against specific classifiers thatare trained for each confusible set independently.3 ResultsTo measure the effectiveness of the generic clas-sifier approach to confusible disambiguation, andto compare it against a specific classifier approachwe have implemented several classification sys-tems.
First of these is a majority class baseline sys-tem, which selects the word from the confusibleset that occurs most often in the training data.1We have also implemented several generic classi-fiers based on different language models.
We com-pare these against two machine learning classi-fiers.
The machine learning classifiers are trainedseparately for each different experiment, whereas1This baseline system corresponds to the simplest lan-guage model classifier.
In this case, it only uses n-grams withn = 1.42the parameters and the training material of the lan-guage model are kept fixed throughout all the ex-periments.3.1 System descriptionThere are many different approaches that can betaken to develop language models.
A well-knownapproach is to use n-grams, or Markov models.These models take into account the probabilitythat a word occurs in the context of the previousn ?
1 words.
The probabilities can be extractedfrom the occurrences of words in a corpus.
Proba-bilities are computed by taking the relative occur-rence count of the n words in sequence.In the experiments described below, we will usea tri-gram-based language model and where re-quired this model will be extended with bi-gramand uni-gram language models.
The probabilityof a sequence is computed as the combination ofthe probabilities of the tri-grams that are found inthe sequence.Especially when n-grams with large n are used,data sparseness becomes an issue.
The trainingdata may not contain any occurrences of the par-ticular sequence of n symbols, even though thesequence is correct.
In that case, the probabilityextracted from the training data will be zero, eventhough the correct probability should be non-zero(albeit small).
To reduce this problem we can ei-ther use back-off or smoothing when the probabil-ity of an n-gram is zero.
In the case of back-off,the probabilities of lower order n-grams are takeninto account when needed.
Alternatively, smooth-ing techniques (Chen and Goodman, 1996) redis-tribute the probabilities, taking into account previ-ously unseen word sequences.Even though the language models provide uswith probabilities of entire sequences, we areonly interested in the n-grams directly around theconfusible when using the language models inthe context of confusible disambiguation.
Theprobabilities of the rest of the sequence will re-main the same whichever alternative confusibleis inserted in the focus word position.
Fig-ure 1 illustrates that the probability of for exampleP (analysts had expected ) is irrelevant for the de-cision between then and than because it occurs inboth sequences.The different language models we will considerhere are essentially the same.
The differences liein how they handle sequences that have zero prob-ability.
Since the probabilities of the n-grams aremultiplied, having a n-gram probability of zero re-sults in a zero probability for the entire sequence.There may be two reasons for an n-gram to haveprobability zero: there is not enough training data,so this sequence has not been seen yet, or this se-quence is not valid in the language.When it is known that a sequence is not validin the language, this information can be used todecide which word from the confusible set shouldbe selected.
However, when the sequence simplyhas not been seen in the training data yet, we can-not rely on this information.
To resolve the se-quences with zero probability, we can use smooth-ing.
However, this assumes that the sequence isvalid, but has not been seen during training.
Theother solution, back-off, tries not to make this as-sumption.
It checks whether subsequences of thesequence are valid, i.e.
have non-zero probabili-ties.
Because of this, we will not use smoothing toreach non-zero probabilities in the current exper-iments, although this may be investigated furtherin the future.The first language model that we will investi-gate here is a linear combination of the differ-ent n-grams.
The probability of a sequence iscomputed by a linear combination of weighted n-gram probabilities.
We will report on two differentweight settings, one system using uniform weight-ing, called uniform linear, and one where uni-grams receive weight 1, bi-grams weight 138, andtri-grams weight 437.2 These weights are normal-ized to yield a final probability for the sequence,resulting in the second system called weighted lin-ear.The third system uses the probabilities of thedifferent n-grams separately, instead of using theprobabilities of all n-grams at the same time as isdone in the linear systems.
The continuous back-off method uses only one of the probabilities ateach position, preferring the higher-level probabil-ities.
This model provides a step-wise back-off.The probability of a sequence is that of the tri-grams contained in that sequence.
However, if theprobability of a trigram is zero, a back-off to theprobabilities of the two bi-grams of the sequenceis used.
If that is still zero, the uni-gram probabil-ity at that position is used.
Note that this uni-gramprobability is exactly what the baseline system2These weights are selected by computing the accuracy ofall combinations of weights on a held out set.43.
.
.
much stronger most analysts had expected .than thenP (much stronger than) P (much stronger then)?P (stronger than most) ?P (stronger then most)?P (than most analysts) ?P (then most analysts)Figure 1: Computation of probabilities using the language model.uses.
With this approach it may be the case thatthe probability for one word in the confusible setis computed based on tri-grams, whereas the prob-ability of another word in the set of confusibles isbased on bi-grams or even the uni-gram probabil-ity.
Effectively, this means that different kinds ofprobabilities are compared.
The same weights asin the weighted linear systems are used.To resolve the problem of unbalanced probabil-ities, a fourth language model, called synchronousback-off, is proposed.
Whereas in the case of thecontinuous back-off model, two words from theconfusible set may be computed using probabil-ities of different level n-grams, the synchronousback-off model uses probabilities of the same levelof n-grams for all words in the confusible set, withn being the highest value for which at least one ofthe words has a non-zero probability.
For instance,when word a has a tri-gram probability of zero andword b has a non-zero tri-gram probability, b is se-lected.
When both have a zero tri-gram probabil-ity, a back-off to bi-grams is performed for bothwords.
This is in line with the idea that if a proba-bility is zero, the training data is sufficient, hencethe sequence is not in the language.To implement the specific classifiers, we usedthe TiMBL implementation of a k-NN classifier(Daelemans et al, 2007).
This implementation ofthe k-NN algorithm is called IB1.
We have tunedthe different parameter settings for the k-NN clas-sifier using Paramsearch (Van den Bosch, 2004),which resulted in a k of 35.3 To describe the in-stances, we try to model the data as similar as pos-sible to the data used by the generic classifier ap-proach.
Since the language model approaches usen-grams with n = 3 as the largest n, the featuresfor the specific classifier approach use words oneand two positions left and right of the focus word.3We note that k is handled slightly differently in TiMBLthan usual, k denotes the number of closest distances consid-ered.
So if there are multiple instances that have the same(closest) distance they are all considered.The focus word becomes the class that needs tobe predicted.
We show an example of both train-ing and testing in figure 2.
Note that the featuresfor the machine learning classifiers could be ex-panded with, for instance, part-of-speech tags, butin the current experiments only the word forms areused as features.In addition to the k-NN classifier, we also runthe experiments using the IGTree classifier, whichis denoted IGTree in the rest of the article, which isalso contained in the TiMBL distribution.
IGTreeis a fast, trie based, approximation of k-nearestneighbor classification (Knuth, 1973; Daelemanset al, 1997).
IGTree allows for fast training andtesting even with millions of examples.
IGTreecompresses a set of labeled examples into a deci-sion tree structure similar to the classic C4.5 algo-rithm (Quinlan, 1993), except that throughout onelevel in the IGTree decision tree, the same featureis tested.
Classification in IGTree is a simple pro-cedure in which the decision tree is traversed fromthe root node down, and one path is followed thatmatches the actual values of the new example tobe classified.
If a leaf is found, the outcome storedat the leaf of the IGTree is returned as the clas-sification.
If the last node is not a leaf node, butthere are no outgoing arcs that match a feature-value combination of the instance, the most likelyoutcome stored at that node is produced as the re-sulting classification.
This outcome is computedby collating the outcomes of all leaf nodes that canbe reached from the node.IGTree is typically able to compress a largeexample set into a lean decision tree with highcompression factors.
This is done in reasonablyshort time, comparable to other compression al-gorithms.
More importantly, IGTree?s classifica-tion time depends only on the number of features(O(f)).
Indeed, in our experiments we observehigh compression rates.
One of the unique char-acteristics of IGTree compared to basic k-NN isits resemblance to smoothing of a basic language44Training .
.
.
much stronger than most analysts had expected .
?much, stronger, most, analysts?
?thanTesting .
.
.
much stronger most analysts had expected .
?much, stronger, most, analysts?
?
?Figure 2: During training, a classified instance (in this case for the confusible pair {then , than}) aregenerated from a sentence.
During testing, a similar instance is generated.
The classifier decides whatthe corresponding class, and hence, which word should be the focus word.model (Zavrel and Daelemans, 1997), while stillbeing a generic classifier that supports any numberand type of features.
For these reasons, IGTree isalso included in the experiments.3.2 Experimental settingsThe probabilities used in the language models ofthe generic classifiers are computed by looking atoccurrences of n-grams.
These occurrences areextracted from a corpus.
The training instancesused in the specific machine learning classifiersare also extracted from the same data set.
Fortraining purposes, we used the Reuters news cor-pus RCV1 (Lewis et al, 2004).
The Reuters cor-pus contains about 810,000 categorized newswirestories as published by Reuters in 1996 and 1997.This corpus contains around 130 million tokens.For testing purposes, we used the Wall StreetJournal part of the Penn Treebank corpus (Marcuset al, 1993).
This well-known corpus contains ar-ticles from the Wall Street Journal in 1987 to 1989.We extract our test-instances from this corpus inthe same way as we extract our training data fromthe Reuters corpus.
There are minor tokenizationdifferences between the corpora.
The data is cor-rected for these differences.Both corpora are in the domain of English lan-guage news texts, so we expect them to have simi-lar properties.
However, they are different corporaand hence are slightly different.
This means thatthere are also differences between the training andtesting set.
We have selected this division to cre-ate a more realistic setting.
This should allow for amore to real-world use comparison than when bothtraining and testing instances are extracted fromthe same corpus.For the specific experiments, we selected anumber of well-known confusible sets to testthe different approaches.
In particular, welook at {then, than}, {its, it?s}, {your, you?re},{their, there, they?re}.
To compare the difficultyof these problems, we also selected two words atrandom and used them as a confusible set.The random category consists of two words thatwhere randomly selected from all words in theReuters corpus that occurred more than a thousandtimes.
The words that where chosen, and used forall experiments here are refugees and effect .
Theyoccur around 27 thousand times in the Reuters cor-pus.3.3 Empirical resultsTable 1 sums up the results we obtained with thedifferent systems.
The baseline scores are gen-erally very high, which tells us that the distribu-tion of classes in a single confusible set is severelyskewed, up to a ten to one ratio.
This also makesthe task hard.
There are many examples for oneword in the set, but only very few training in-stances for the other(s).
However, it is especiallyimportant to recognize the important aspects of theminority class.The results clearly show that the specific clas-sifier approaches outperform the other systems.For instance, on the first task ({then, than}) theclassifier achieves an accuracy slightly over 98%,whereas the language model systems only yieldaround 96%.
This is as expected.
The classifieris trained on just one confusible task and is there-fore able to specialize on that task.Comparing the two specific classifiers, we seethat the accuracy achieved by IB1 and IGTree isquite similar.
In general, IGTree performs a bitworse than IB1 on all confusible sets, which isas expected.
However, in general it is possiblefor IGTree to outperform IB1 on certain tasks.
Inour experience this mainly happens on tasks wherethe usage of IGTree, allowing for more compactinternal representations, allows one to use muchmore training data.
IGTree also leads to improved45{then, than} {its, it?s} {your, you?re} {their, there, they?re} randomBaseline 82.63 92.42 78.55 68.36 93.16IB1 98.01 98.67 96.36 97.12 97.89IGTree 97.07 96.75 96.00 93.02 95.79Uniform linear 68.27 50.70 31.64 32.72 38.95Weighted linear 94.43 92.88 93.09 93.25 88.42Continuous back-off 81.49 83.22 74.18 86.01 63.68Synchronous back-off 96.42 94.10 92.36 93.06 87.37Number of cases 2,458 4,830 275 3,053 190Table 1: This table shows the performance achieved by the different systems, shown in accuracy (%).The Number of cases denotes the number of instances in the testset.performance in cases where the features have astrong, absolute ordering of importance with re-spect to the classification problem at hand.The generic language model approaches per-form reasonably well.
However, there are cleardifferences between the approaches.
For instancethe weighted linear and synchronous back-off ap-proaches work well, but uniform linear and con-tinuous back-off perform much worse.
Especiallythe synchronous back-off approach achieves de-cent results, regardless of the confusible problem.It is not very surprising to see that the contin-uous back-off method performs worse than thesynchronous back-off method.
Remember thatthe continuous back-off method always uses lowerlevel n-grams when zero probabilities are found.This is done independently of the probabilities ofthe other words in the confusible set.
The contin-uous back-off method prefers n-grams with largern, however it does not penalize backing off to ann-gram with smaller n. Combine this with the factthat n-gram probabilities with large n are compar-atively lower than those for n-grams with smallern and it becomes likely that a bi-gram contributesmore to the erroneous option than the correct tri-gram does to the correct option.
Tri-grams aremore sparse than bi-grams, given the same data.The weighted linear approach outperforms theuniform linear approach by a large margin on allconfusible sets.
It is likely that the contributionfrom the n-grams with large n overrules the prob-abilities of the n-grams with smaller n in the uni-form linear method.
This causes a bias towards themore frequent words, compounded by the fact thatbi-grams, and uni-grams even more so, are lesssparse and therefore contribute more to the totalprobability.We see that the both generic and specific clas-sifier approaches perform consistently across thedifferent confusible sets.
The synchronous back-off approach is the best performing generic clas-sifier approach we tested.
It consistently outper-forms the baseline, and overall performs betterthan the weighted linear approach.The experiments show that generic classifiersbased on language model can be used in the con-text of confusible disambiguation.
However, then in the different n-grams is of major importance.Exactly which n grams should be used to com-pute the probability of a sequence requires moreresearch.
The experiments also show that ap-proaches that concentrate on n-grams with largern yield more encouraging results.4 Conclusion and future workConfusibles are spelling errors that can only be de-tected within their sentential context.
This kindof errors requires a completely different approachcompared to non-word errors (errors that can beidentified out of context, i.e.
sequences of char-acters that do not belong to the language).
Inpractice, most confusible disambiguation systemsare based on machine learning classification tech-niques, where for each type of confusible, a newclassifier is trained and tuned.In this article, we investigate the use of languagemodels in the context of confusible disambigua-tion.
This approach works by selecting the wordin the set of confusibles that has the highest prob-ability in the sentential context according to thelanguage model.
Any kind of language model canbe used in this approach.The main advantage of using language modelsas generic classifiers is that it is easy to add newsets of confusibles without retraining or adding ad-ditional classifiers.
The entire language is mod-46eled, which means that all the information onwords in their context is inherently present.The experiments show that using generic clas-sifiers based on simple n-gram language modelsyield slightly worse results compared to the spe-cific classifier approach, where each classifier isspecifically trained on one confusible set.
How-ever, the advantage of the generic classifier ap-proach is that only one system has to be trained,compared to different systems for each confusiblein the specific classifier case.
Also, the exact com-putation of the probabilities using the n-grams, inparticular the means of backing-off, has a largeimpact on the results.As future work, we would like to investigate theaccuracy of more complex language models usedas classifiers.
The n-gram language models de-scribed here are relatively simple, but more com-plex language models could improve performance.In particular, instead of back-off, smoothing tech-niques could be investigated to reduce the impactof zero probability problems (Chen and Goodman,1996).
This assumes that the training data we arecurrently working with is not enough to properlydescribe the language.Additionally, language models that concentrateon more structural descriptions of the language,for instance, using grammatical inference tech-niques (de la Higuera, 2005), or models that ex-plicitly take long distance dependencies into ac-count (Griffiths et al, 2005) can be investigated.This leads to much richer language models thatcould, for example, check whether there is alreadya verb in the sentence (which helps in cases suchas {its, it?s}).A different route which we would also like to in-vestigate is the usage of a specific classifier, suchas TiMBL?s IGTree, as a language model.
If aclassifier is trained to predict the next word in thesentence or to predict the word at a given positionwith both left and right context as features, it canbe used to estimate the probability of the words ina confusible set, just like the language models wehave looked at so far.
Another type of classifiermight estimate the perplexity at a position, or pro-vide some other measure of ?surprisedness?.
Ef-fectively, these approaches all take a model of theentire language (as described in the training data)into account.ReferencesBanko, M. and Brill, E. (2001).
Scaling to very verylarge corpora for natural language disambiguation.In Proceedings of the 39th Annual Meeting of the As-sociation for Computational Linguistics, pages 26?33.
Association for Computational Linguistics.Chen, S. and Goodman, J.
(1996).
An empirical studyof smoothing techniques for language modelling.
InProceedings of the 34th Annual Meeting of the ACL,pages 310?318.
ACL.Daelemans, W., Van den Bosch, A., and Weijters, A.(1997).
IGTree: using trees for compression andclassification in lazy learning algorithms.
ArtificialIntelligence Review, 11:407?423.Daelemans, W., Zavrel, J., Van der Sloot, K., and Vanden Bosch, A.
(2007).
TiMBL: Tilburg MemoryBased Learner, version 6.1, reference guide.
Techni-cal Report ILK 07-07, ILK Research Group, TilburgUniversity.de la Higuera, C. (2005).
A bibliographical studyof grammatical inference.
Pattern Recognition,38(9):1332 ?
1348.
Grammatical Inference.Even-Zohar, Y. and Roth, D. (2000).
A classificationapproach to word prediction.
In Proceedings of theFirst North-American Conference on ComputationalLinguistics, pages 124?131, New Brunswick, NJ.ACL.Golding, A. and Roth, D. (1999).
A Winnow-BasedApproach to Context-Sensitive Spelling Correction.Machine Learning, 34(1?3):107?130.Golding, A. R. (1995).
A Bayesian hybrid method forcontext-sensitive spelling correction.
In Proceed-ings of the 3rd workshop on very large corpora,ACL-95.Griffiths, T. L., Steyvers, M., Blei, D. M., and Tenen-baum, J.
B.
(2005).
Integrating topics and syntax.
InIn Advances in Neural Information Processing Sys-tems 17, pages 537?544.
MIT Press.Huang, J. H. and Powers, D. W. (2001).
Large scale ex-periments on correction of confused words.
In Aus-tralasian Computer Science Conference Proceed-ings, pages 77?82, Queensland AU.
Bond Univer-sity.Knuth, D. E. (1973).
The art of computer program-ming, volume 3: Sorting and searching.
Addison-Wesley, Reading, MA.Lewis, D. D., Yang, Y., Rose, T. G., Dietterich, G., Li,F., and Li, F. (2004).
Rcv1: A new benchmark col-lection for text categorization research.
Journal ofMachine Learning Research, 5:361?397.Mangu, L. and Brill, E. (1997).
Automatic rule ac-quisition for spelling correction.
In Proceedings ofthe International Conference on Machine Learning,pages 187?194.47Marcus, M., Santorini, S., and Marcinkiewicz, M.(1993).
Building a Large Annotated Corpus of En-glish: the Penn Treebank.
Computational Linguis-tics, 19(2):313?330.Quinlan, J.
(1993).
C4.5: Programs for MachineLearning.
Morgan Kaufmann, San Mateo, CA.Sandra, D., Daems, F., and Frisson, S. (2001).
Zohelder en toch zoveel fouten!
wat leren we uit psy-cholingu?
?stisch onderzoek naar werkwoordfoutenbij ervaren spellers?
Tijdschrift van de Verenigingvoor het Onderwijs in het Nederlands, 30(3):3?20.Van den Bosch, A.
(2004).
Wrapped progressivesampling search for optimizing learning algorithmparameters.
In Verbrugge, R., Taatgen, N., andSchomaker, L., editors, Proceedings of the SixteenthBelgian-Dutch Conference on Artificial Intelligence,pages 219?226, Groningen, The Netherlands.Van den Bosch, A.
(2006).
Scalable classification-based word prediction and confusible correction.Traitement Automatique des Langues, 46(2):39?63.Van den Bosch, A. and Daelemans, W. (2007).
TussenTaal, Spelling en Onderwijs, chapter Dat gebeurdmei niet: Computationele modellen voor verwarbarehomofonen, pages 199?210.
Academia Press.Wu, D., Sui, Z., and Zhao, J.
(1999).
An information-based method for selecting feature types for wordprediction.
In Proceedings of the Sixth EuropeanConference on Speech Communication and Technol-ogy, EUROSPEECH?99, Budapest.Yarowsky, D. (1994).
Decision lists for lexical ambi-guity resolution: application to accent restoration inSpanish and French.
In Proceedings of the AnnualMeeting of the ACL, pages 88?95.Zavrel, J. and Daelemans, W. (1997).
Memory-basedlearning: Using similarity for smoothing.
In Pro-ceedings of the 35th Annual Meeting of the Associa-tion for Computational Linguistics, pages 436?443.48
