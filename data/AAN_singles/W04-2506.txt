A Novel Approach to Focus Identification in Question/Answering SystemsAlessandro Moschitti and Sanda HarabagiuUniversity of Texas at DallasHuman Language Technology Research InstituteRichardson, TX 75083-0688, USAalessandro.moschitti@utdallas.edusanda@utdallas.eduAbstractModern Question/Answering systems rely onexpected answer types for processing ques-tions.
The answer type is a semantic cate-gory provided by Named Entity recognizer orby semantic hierarchies.
We argue in this pa-per that Q/A systems should take advantageof the topic information by exploiting severalmodels of question and answer categorization.The matching of the question category with theanswer category allows the system to filter outmany incorrect answers.1 IntroductionOne method of retrieving information from vast docu-ment collections is by using textual Question/Answering.Q/A is an Information Retrieval (IR) paradigm that re-turns a short list of answers, extracted from relevant doc-uments, to a question formulated in natural language.
An-other, different method to find the desired information isby navigating along subject categories assigned hierar-chically to groups of documents, in a style made popularby Yahoo.com among others.
When the defined categoryis reached, documents are inspected and the informationis eventually retrieved.Q/A systems incorporate a paragraph retrieval engine,to find paragraphs that contain candidate answers, as re-ported in (Clark et al, 1999; Pasca and Harabagiu, 2001).To our knowledge no information on the text categories ofthese paragraphs is currently employed in any of the Q/Asystems.
Instead, another semantic information, such asthe semantic classes of the expected answers, derivedfrom the question processing, is used to retrieve para-graphs and later to extract answers.
Typically, the se-mantic classes of answers are organized in hierarchicalontologies and do not relate in any way to the categoriesassociated with documents.The ontology of expected answer classes containsconcepts like PERSON, LOCATION or PRODUCT,whereas categories associated with documents are moresimilar to topics than concepts, e.g., acquisitions, tradingor earnings.
Given that text categories indicate differentsemantic information than the classes of the expected an-swers, we argue in this paper that text categories can beused to improve the quality of textual Q/A.
In fact, by as-signing text categories to both questions and answers, wehave additional information on their similarity, which al-lows systems to perform a first level of word disambigua-tion.
For example, if a user asks about the Apple charac-teristics, two type of answers may be retrieved: (a) aboutthe apple company and (b) related to the agricultural do-main.
Instead, if the computer subject is selected, onlythe answers involving the Apple company will be consid-ered.
Thus, topic categories allows Q/A systems to detectthe correct focus and consequently filter out many incor-rect answers.In order to assign categories to questions and answers,the set of documents, on which the Q/A systems oper-ate, has to be pre-categorized.
For our experiments wetrained our basic Q/A system on the well-known text cat-egorization benchmark, Reuters-21578.
This allows us toassume as categories of an answer the categories of thedocuments, which contain such answer.
More difficult,instead, is assigning categories to questions as: (a) theyare not known in advance and (b) their reduced size (interm of number of words) often prevents the detection oftheir categories.The article is organized as follows: Section 2 describesour Q/A system whereas Section 3 shows the questioncategorization problem and the solutions adopted.
Sec-tion 4 presents the filtering and the re-ranking methodsthat combine the basic Q/A with the question classifica-tion models.
Section 5 reports the experiments on ques-tion categorization, basic Question Answering and Ques-tion Answering based on Text Categorization (TC).
Fi-nally, Section 6 derives the conclusions.2 Textual Question AnsweringThe typical architecture of a Q/A system is illustrated inFigure 1:First the target question is processed to derive (a) thesemantic class of the expected answer and (b) what key-words constitute the queries used to retrieve relevantparagraphs.
Question processing relies on external re-sources to identify the class of the expected answer, typ-ically in the form of semantic ontologies (Answer TypeOntology).Second, the semantic class of the expected answer islater used to (1) filter out paragraphs that do not containany word that can be cast in the same class as the expectedanswer, and (2) locate and extract the answers from theparagraphs.
Finally, the answers are extracted and rankedbased on their unification with the question.Question Query Relevant Passag s AnswerAnswer Type OntologiesSemantic Class of expected AnswersQuestion Processing Paragraph Retrieval Answer extraction and formulationDocument CollectionFigure 1: Architecture of a Q/A system.2.1 Question ProcessingTo determine what a question asks about, several forms ofinformation can be used.
Since questions are expressedin natural language, sometimes their stems, e.g., who,what or where indicate the semantic class of the expectedanswer, i.e.
PERSON, ORGANIZATION or LO-CATION, respectively.
To identify words that belongto such semantic classes, Name Entity Recognizers areused, since most of these words represent names.
NameEntity (NE) recognition is a natural language technologythat identifies names of people, organizations, locationsand dates or monetary values.However, most of the time the question stems are ei-ther ambiguous or they simply do not exist.
For example,questions having what as their stem may ask about any-thing.
In this case another word from the question needsto be used to determine the semantic class of the expectedanswer.
In particular, the additional word is semanti-cally classified against an ontology of semantic classes.To determine which word indicates the semantic classof the expected answer, the syntactic dependencies1 be-tween the question words may be employed (Harabagiu1Syntactic parsers publicly available, e.g., (Charniak, 2000;et al, 2000; Pasca and Harabagiu, 2001; Harabagiu et al,2001).Sometimes the semantic class of the expected answerscannot be identified or is erroneously identified causingthe selection of erroneous answers.
The use of text clas-sification aims to filter out the incorrect set of answersthat Q/A systems provide.2.2 Paragraph RetrievalOnce the question processing has chosen the relevantkeywords of questions, some term expansion techniquesare applied: all nouns and adjectives as well as morpho-logical variations of nouns are inserted in a list.
To findthe morphological variations of the nouns, we used theCELEX (Baayen et al, 1995) database.
The list of ex-panded keywords is then used in the boolean version ofthe SMART system to retrieve paragraphs relevant to thetarget question.
Paragraph retrieval is preferred over fulldocument retrieval because (a) it is assumed that the an-swer is more likely to be found in a small text containingthe question keywords and at least one other word thatmay be the exact answer; and (b) it is easier to processsyntactically and semantically a small text window forunification with the question than processing a full docu-ment.2.3 Answer ExtractionThe procedure for answer extraction that we used is re-ported in (Pasca and Harabagiu, 2001), it has 3 steps:Step 1) Identification of Relevant Sentences:The Knowledge about the semantic class of the expectedanswer generates two cases: (a) When the semantic classof the expected answers is known, all sentences from eachparagraph, that contain a word identified by the NamedEntity recognizer as having the same semantic classes asthe expected answers, are extracted.
(b) The semanticclass of the expected answer is not known, all sentences,that contain at least one of the keywords used for para-graph retrieval, are selected.Step 2) Sentence Ranking:We compute the sentence ranks as a by product of sortingthe selected sentences.
To sort the sentences, we may useany sorting algorithm, e.g., the quicksort, given that weprovide a comparison function between each pair of sen-tences.
To learn the comparison function we use a sim-ple neural network, namely, the perceptron, to computea relative comparison between any two sentences.
Thisscore is computed by considering four different featuresfor each sentence as explained in (Pasca and Harabagiu,2001).Step 3) Answer Extraction:We select the top 5 ranked sentences and return them asCollins, 1997), can be used to capture the binary dependenciesbetween the head of each phrase.answers.
If we lead fewer than 5 sentences to select from,we return all of them.Once the answers are extracted we can apply an addi-tional filter based on text categories.
The idea is to matchthe categories of the answers against those of the ques-tions.
Next section addresses the problem of question andanswer categorization.3 Text and Question CategorizationTo exploit category information for Q/A we categorizeboth answers and questions.
For the former, we define ascategories of an answer a the categories of the documentthat contain a.
For the latter, the problem is more criticalas it is not clear what can be considered as categories ofa question.To define question categories we assume that usershave a specific domain in mind when they formulatetheir requests.
Although, this can be considered a strongassumption, it is verified in practical cases.
In fact, toformulate a sound question about a topic, the questionerneeds to know some basic concepts about that topic.
Asan example consider a random question from TREC-92:"How much folic acid should an expectantmother get daily?
"The folic acid and get daily concepts are related tothe expectant mother concept since medical expertsprescribe such substance to pregnant woman with acertain frequency.
The hypothesis that the questionwas generated without knowing the relations amongthe above concepts is unlikely.
Additionally, suchspecific relations are frequent and often they characterizedomains.
Thus, the user, by referring to some relations,automatically determines specific domains or categories.In summary, the idea of question categorization is: (a)users cannot formulate a consistent question on a domainthat do not know, and (b) specific questions that expressrelation among concepts automatically define domains.It is worth noting that the specificity of the questionsdepends on the categorization schemes which documentsare divided in.
For example the following TREC ques-tion:"What was the name of the first Russianastronaut to do a spacewalk?
"may be considered generic, but if a categorizationscheme includes categories like Space Conquest Historyor Astronaut and Spaceship the above question is clearlyspecific on the above categories.The same rationale cannot be applied to very shortquestions like: Where is Belize located?, Who2TREC-9 questions are available at http://trec.nist.gov/qa questions 201-893.invented the paper clip?
or How far away isthe moon?
In these cases we cannot assume that aquestion category exists.
However, our aim is to providean additional answer filtering mechanism for stand-aloneQ/A systems.
This means that when question categoriza-tion is not applicable, we can deactivate such a mecha-nism.The automatic models that we have study to classifyquestions and answers are: Rocchio (Ittner et al, 1995)and SVM (Vapnik, 1995) classifiers.
The former is a veryefficient TC that can be used for real scenario applica-tions.
This is a very appealing property considering thatQ/A systems are designed to operate on the web.
Thesecond is one of the best figure TC that provides goodaccuracy with a few training data.3.1 Rocchio and SVM Text ClassifiersRocchio and Support Vector Machines are both based onthe Vector Space Model.
In this approach, the documentd is described as a vector ~d =<wdf1 , .., wdf|F |> in a |F |-dimensional vector space, where F is the adopted set offeatures.
The axes of the space, f1, .., f|F | ?
F , are thefeatures extracted from the training documents and thevector components wdfj ?
< are weights that can be eval-uated as described in (Salton, 1989).The weighing methods that we adopted are based onthe following quantities: M , the number of documents inthe training-set, Mf , the number of documents in whichthe features f appears and ldf , the logarithm of the termfrequency defined as:ldf ={ 0 if odf = 0log(odf ) + 1 otherwise(1)where, odf are the occurrences of the features f in thedocument d (TF of features f in document d).Accordingly, the document weights is:wdf =ldf ?
IDF (f)?
?r?F (ldr ?
IDF (r))2where the IDF (f) (the Inverse Document Frequency) isdefined as log( MMf ).Given a category C and a set of positive and negativeexamples, P and P?
, Rocchio and SVM learning algo-rithms use the document vector representations to derivea hyperplane3, ~a ?
~d + b = 0.
This latter separates thedocuments that belong to C from those that do not be-long to C in the training-set.
More precisely, ?~d positiveexamples (~d ?
P ), ~a ?
~d + b ?
0, otherwise (~d ?
P?
)~a ?
~d + b < 0.
~d is the equation variable, while the gra-dient ~a and the constant b are determined by the targetlearning algorithm.
Once the above parameters are avail-able, it is possible to define the associated classification3The product between vectors is the usual scalar product.function, ?c : D ?
{C, ?
}, from the set of documents Dto the binary decision (i.e., belonging or not to C).
Suchdecision function is described by the following equation:?c(d) ={C ~a?
~d+ b ?
0?
otherwise (2)Eq.
2 shows that a category is accepted only if the product~a ?
~d overcomes the threshold ?b.
Rocchio and SVMare characterized by the same decision function4.
Theirdifference is the learning algorithm to evaluate the b andthe~a parameters: the former uses a simple heuristic whilethe second solves an optimization problem.3.1.1 Rocchio LearningThe learning algorithm of the Rocchio text classifier isthe simple application of the Rocchio?s formula (Eq.
3)(Rocchio, 1971).
The parameters ~a is evaluated by theequation:~af = max{0, 1|P |?d?Pwdf ??|P?
|?d?P?wdf}(3)where P is the set of training documents that belongs toC and ?
is a parameter that emphasizes the negative in-formation.
This latter can be estimated by picking-up thevalue that maximizes the classifier accuracy on a train-ing subset called evaluation-set.
A method, named theParameterized Rocchio Classifier, to estimate good pa-rameters has been given in (Moschitti, 2003b).The above learning algorithm is based on a simple andefficient heuristic but it does not ensure the best separa-tion of the training documents.
Consequently, the accu-racy is lower than other TC algorithms.3.1.2 Support Vector Machine LearningThe major advantage of SVM model is that the param-eters ~a and b are evaluated applying the Structural RiskMinimization principle (Vapnik, 1995), stated in the sta-tistical learning theory.
This principle provides a boundfor the error on the test-set.
Such bound is minimized ifthe SVMs are chosen in a way that |~a| is minimal.
Moreprecisely the parameters ~a and b are a solution of the fol-lowing optimization problem:??
?Minimize |~a|~a?
~d+ b ?
1 ?d ?
P~a?
~d+ b < ?1 ?d ?
P?
(4)It can be proven that the minimum |~a| leads to a maxi-mal margin5 (i.e.
distance) between negative and positiveexamples.4This is true only for linear SVM.
In the polynomial versionthe decision function is a polynomial of support vectors.5The software to carry out both the learning and clas-sification algorithm for SVM is described in (Joachims,1999) and it can be downloaded from the web sitehttp://svmlight.joachims.org/.In summary, SVM provides a better accuracy thanRocchio but this latter is better suited for real applica-tions.3.2 Question CategorizationIn (Moschitti, 2003b; Joachims, 1999), Rocchio andSVM text classifiers have reported to generate good ac-curacy.
Therefore, we use the same models to classifyquestions.
These questions can be considered as a partic-ular case of documents, in which the number of words issmall.
Due to the small number of words, a large collec-tion of questions needs to be used for training the clas-sifiers when reaching a reliable statistical word distribu-tion.
Practically, large number of training questions is notavailable.
Consequently, we approximate question wordstatistics using document statistics and we learn questioncategorization functions on category documents.We define for each question q a vector ~q =<wq1, .., wq|Fq|>, where wqi ?
< are the weights associ-ated to the question features in the feature set Fq , e.g.the set of question words.
Then, we evaluate four differ-ent methods computing the weights of question features,which in turn determine five models of question catego-rization:Method 1: We use lqf , the logarithm (evaluated simi-larly to Eq.
1) of the word frequency f in the questionq, together with the IDF derived from training documentsas follows:wqf =lqf ?
IDF (f)?
?r?Fq (lqr ?
IDF (r))2(5)This weighting mechanism uses the Inverse DocumentFrequency (IDF) of features instead of computing the In-verse Question Frequency.
The rationale is that ques-tion word statistics can be estimated from the word doc-ument distributions.
When this method is applied to theRocchio-based Text Categorization model, by substitut-ing wdf with wqf we obtain a model call the RTC0 model.When it is applied to the SVM model, by substituting wdfwith wqf , we call it SVM0.Method 2: The weights of the question features arecomputed by the formula 5 employed in Method 1, butthey are used in the Parameterized Rocchio Model (Mos-chitti, 2003b).
This entails that ?
from formula 3 as wellas the threshold b are chosen to maximize the catego-rization accuracy of the training questions.
We call thismodel of categorization PRTC.Method 3: The weights of the question features arecomputed by formula 5 employed in Method 1, but theyare used in an extended SVM model, in which two ad-ditional conditions enhance the optimization problem ex-pressed by Eq.
4.
The two new conditions are:??
?Minimize |~a|~a?
~q + b ?
1 ?q ?
Pq~a?
~q + b < ?1 ?q ?
P?q(6)where Pq and P?q are the set of positive and negative ex-amples of training questions for the target category C.We call this question categorization model QSVM.Method 4: We use the output of the basic Q/A systemto assign a category to questions.
Each question has as-sociated up to five answer sentences.
In turn, each of theanswers is extracted from a document, which is catego-rized.
The category of the question is chosen as the mostfrequent category of the answers.
In case that more thanone category has the maximal frequency, the set of cat-egories with maximal frequency is returned.
We namedthis ad-hoc question categorization method QATC (Q/Aand TC based model).4 Answer Filtering and Re-Ranking Basedon Text CategorizationMany Q/A systems extract and rank answers successfully,without employing any TC information.
For such sys-tems, it is interesting to evaluate if TC information im-proves the ranking of the answers they generate.
Thequestion category can be used in two ways: (1) to re-rankthe answers by pushing down in the list any answer thatis labeled with a different category than the question; or(2) to simply eliminate answers labeled with categoriesdifferent than the question category.First, a basic Q/A system has to be trained on docu-ments that are categorized (automatically or manually)in a predefined categorization scheme.
Then, the targetquestions as well as the answers provided by the basicQ/A system are categorized.
The answers receive thecategorization directly from the categorization scheme,as they are extracted from categorized documents.
Thequestions are categorized using one of the models de-scribed in the previous section.
Two different impactsof question categorization on Q/A are possible:?
Answers that do not match at least one of the cate-gories of the target questions are eliminated.
In thiscase the precision of the system should increase ifthe question categorization models are enough accu-rate.
The drawback is that some important answerscould be lost because of categorization errors.?
Answers that do not match the target questions (asbefore) get lowered ranks.
For example, if the firstanswer has categories different from the target ques-tion, it could shift to the last position in case of allother answers have (at least) one category in com-mon with the question.
In any case, all questionswill be shown to the final users, preventing the lostof relevant answers.An example of the answer elimination and answer re-ranking is given in the following.
As basic Q/A systemwe adopted the model described in Section 2.
We trained6it with the entire Reuters-21578 corpus7.
In particularwe adopted the collection Apte?
split.
It includes 12,902documents for 90 classes, with a fixed splitting betweentest-set and learning data (3,299 vs. 9,603).
A descriptionof some categories of this corpus is given in Table 1.Table 1: Description of some Reuters categoriesCategory DescriptionAcq Acquisition of shares and companiesEarn Earns derived by acquisitions or sellsCrude Crude oil events: market, Opec decision,..Grain News about grain productionTrade Trade between companiesShip Economic events that involve shipsCocoa Market and events related to Cocoa plantsNat-gas Natural Gas marketVeg-oil Vegetal Oil marketTable 2 shows the five answers generated (with theircorresponding rank) by the basic Q/A system, for oneexample question.
The category of the document fromwhich the answer was extracted is displayed in column1.
The question classification algorithm automatically as-signed the Crude category to the question.The processing of the question identifies the wordsay as indicating the semantic class of the expected an-swer and for paragraph retrieval it used the keywordsk1 = Director, k2 = General, k3 = energy,k4 = floating, k5 = production and k6 = plantsas well as all morphological variations for the nouns.For each answer from Table 2, we have underlined thewords matched against the keywords and emphasizedthe word matched in the class of the expected answer,whenever such a word was recognized (e.g., for an-swers 1 and 3 only).
For example, the first answerwas extracted because words producers, product anddirectorate general could be matched against the key-words production, Director and General from the ques-tion and moreover, the word said has the same semanticclass as the word say, which indicates the semantic classof the expected answer.The ambiguity of the word plants cause the basicQ/A system to rank the answers related to Cocoa andGrain plantations higher than the correct answer, which isranked as the third one.
If the answer re-ranking or elim-ination methods are adopted, the correct answer reaches6We could not use the TREC conference data-set becausetexts and questions are not categorized.7Available athttp://kdd.ics.uci.edu/databases/reuters21578/.Table 2: Example of question labeled in the Crude category and its five answers.Rank Category Question: What did the Director General say about the energy floating production plants?1 Cocoa ?
Leading cocoa producers are trying to protect their market from our product , ?
said a spokesman for Indonesia?s directorate general of plantations.2 Grain Hideo Maki , Director General of the ministry ?s Economic Affairs Bureau , quoted Lyng as telling AgricultureMinister Mutsuki Kato that the removal of import restrictions would help Japan as well as the United States.3 Crude Director General of Mineral and Energy Affairs Louw Alberts announced the strike earlier but said it wasuneconomic .4 Veg-oil Norbert Tanghe, head of division of the Commission?s Directorate General for Agriculture, told the 8th AntwerpOils and Fats Contact Days ?
the Commission firmly believes that the sacrifices which would be undergone byCommunity producers in the oils and fats sector...5 Nat-gas Youcef Yousfi, director - general of Sonatrach , the Algerian state petroleum agency , indicated in a televisioninterview in Algiers that such imports.the top as it was assigned the same category as the ques-tion, namely the Crude category.Next section describes in detail our experiments toprove that question categorization add some important in-formation to select relevant answers.5 ExperimentsThe aim of the experiments is to prove that category in-formation used, as described in the previous section, isuseful for Q/A systems.
For this purpose we have to showthat the performance of a basic Q/A system is improvedwhen the question classification is adopted.
To imple-ment our Q/A and filtering system we used: (1) A stateof the art Q/A system: improving low accurate systems isnot enough to prove that TC is useful for Q/A.
The basicQ/A system that we employed is based on the architec-ture described in (Pasca and Harabagiu, 2001), which isthe current state-of-the-art.
(2) The Reuters collection ofcategorized documents on which training our basic Q/Asystem.
(3) A set of questions categorized according tothe Reuters categories.
A portion of this set is used totrain PRTC and QSVM models, the other disjoint portionis used to measure the performance of the Q/A systems.Next section, describes the technique used to producethe question corpus.5.1 Question Set GenerationThe idea of PRTC and QSVM models is to exploit aset of questions for each category to improve the learn-ing of the PRC and SVM classifiers.
Given the com-plexity of producing any single question, we decided totest our algorithms on only 5 categories.
We chose Acq,Earn, Crude, Grain, Trade and Ship categories sincefor them is available the largest number of training doc-uments.
To generate questions we randomly selected anumber of documents from each category, then we triedto formulate questions related to the pairs <document,category>.
Three cases were found: (a) The documentTable 3: Some training/testing QuestionsAcq Which strategy aimed activities on core busi-nesses?How could the transpacific telephone cable be-tween the U.S. and Japan contribute to forminga join venture?Earn What was the most significant factor for the lackof the distribution of assets?What do analysts think about public compa-nies?Crude What is Kuwait known for?What supply does Venezuela give to another oilproducer?Grain Why do certain exporters fear that China mayrenounce its contract?Why did men in port?s grain sector stop work?Trade How did the trade surplus and the reservesweaken Taiwan?s position?What are Spain?s plans for reaching EuropeanCommunity export level?Ship When did the strikes start in the ship sector?Who attacked the Saudi Arabian supertanker inthe United Arab Emirates sea?does not contain general questions about the target cat-egory.
(b) The document suggests general questions, inthis case some of the question words that are in the an-swers are replaced with synonyms to formulate a new(more general) question.
(c) The document suggests gen-eral questions that are not related to the target category.We add these questions in our data-set associated withtheir true categories.Table 3 lists a sample of the questions we derived fromthe target set of categories.
It is worth noting that weincluded short queries also to maintain general our ex-perimental set-up.We generated 120 questions and we used 60 for thelearning and the other 60 for testing.
To measure the im-pact that TC has on Q/A, we first evaluated the questioncategorization models presented in Section 3.1.
Then wecompared the performance of the basic Q/A system withthe extended Q/A systems that adopt the answer elimina-tion and re-ranking methods.5.2 Performance MeasurementsIn sections 3 and 4 we have introduced several models.From the point of view of the accuracy, we can dividedthem in two categories: the (document and question) cat-egorization models and the Q/A models.
The formerare usually measured by using Precision, Recall, and f-measure (Yang, 1999); note that questions can be con-sidered as small documents.
The latter often provide asoutput a list of ranked answers.
In this case, a good mea-sure of the system performance should take into accountthe order of the correct and incorrect questions.One method employed in TREC is the reciprocal valueof the rank (RAR) of the highest-ranked correct answergenerated by the Q/A system.
Its value is 1 if the firstanswer is correct, 0.5 if the second answer is correctbut not the first one, 0.33 when the correct answer wason the third position, 0.25 if the fourth answer was cor-rect, and 0.1 when the fifth answer was correct and soon.
If none of the answers are corrects, RAR=0.
TheMean Reciprocal Answer Rank (MRAR) is used to com-pute the overall performance of Q/A systems8, defined asMRAR = 1n?i1ranki , where n is the number of ques-tions and ranki is the rank of the answer i.Since we believe that TC information is meaningful toprefer out incorrect answers, we defined a second mea-sure to evaluate Q/A.
For this purpose we designed theSigned Reciprocal Answer Rank (SRAR), which is de-fined as 1n?j?A1srankj , where A is the set of answersgiven for the test-set questions, |srankj | is the rank posi-tion of the answer j and srankj is positive if j is correctand negative if it is not correct.
The SRAR can be evalu-ated over a set of questions as well as over only one ques-tion.
SRAR for a single question is 0 only if no answerwas provided for it.For example, given the answer ranking of Table 2 andconsidering that we have just one question for testing, theMRAR score is 0.33 while the SRAR is -1 -.5 +.33 -.25 -.1 = -1.52.
If the answer re-ranking is adopted the MRARimprove to 1 and the SRAR becomes +1 -.5 -.33 -.25 -.1= -.18.
The answer elimination produces a MRAR and aSRAR of 1.5.3 Evaluation of Question CategorizationTable 4 lists the performance of question categorizationfor each of the models described in Section 3.1.
We no-ticed better results when the PRTC and QSVM modelswere used.
In the overall, we find that the performance of8The same measure was used in all TREC Q/A evaluations.question categorization is not as good as the one obtainedfor TC in (Moschitti, 2003b).Table 4: f1 performances of question categorization.RTC0 SVM0 PRTC QSVM QATCf1 f1 f1 f1 f1acq 18.19 54.02 62.50 56.00 46.15crude 33.33 54.05 53.33 66.67 66.67earn 0.00 55.32 40.00 13.00 26.67grain 50.00 52.17 75.00 66.67 50.00ship 80.00 47.06 75.00 90.00 85.71trade 40.00 57.13 66.67 58.34 45.455.4 Evaluation of Question AnsweringTo evaluate the impact of our filtering methods on Q/Awe first scored the answers of a basic Q/A system for thetest set, by using both the MRAR and the SRAR mea-sures.
Additionally, we evaluated (1) the MRAR whenanswers were re-ranked based on question and answercategory information; and (2) the SRAR in the case whenanswers extracted from documents with different cate-gories were eliminated.
Rows 1 and 2 of Table 5 reportthe MRAR and SRAR performances of the basic Q/A.Column 2,3,4,5 and 6 show the MRAR and SRAR accu-racies (rows 4 and 5) of Q/A systems that eliminate orre-rank the answer by using the RTC0, SVM0, PRTC,QSVM and QATC question categorization models.The basic Q/A results show that answering the Reutersbased questions is a quite difficult task9 as the MRAR is.662, about 15 percent points under the best system resultobtained in the 2003 TREC competition.
Note that thebasic Q/A system, employed in these experiments, usesthe same techniques adopted by the best figure Q/A sys-tem of TREC 2003.The quality of the Q/A results is strongly affected bythe question classification accuracy.
In fact, RTC0 andQATC that have the lowest classification f1 (see Table4) produce very low MRAR (i.e.
.622% and .607%) andSRAR (i.e.
-.189 and -.320).
When the best questionclassification model QSVM is used, the basic Q/A perfor-mance improves with respect to both the MRAR (66.35%vs 66.19%) and the SRAR (-.077% vs -.372%) scores.In order to study how the number of answers impactsthe accuracy of the proposed models, we have evaluatedthe MRAR and the SRAR score varying the maximumnumber of answers, provided by the basic Q/A system.We adopted as filtering policy the answer re-ranking.Figure 2 shows that as the number of answers increasesthe MRAR score for QSVM, PRTC and the basic Q/A in-9Past TREC competition results have shown that Q/A per-formances strongly depend on the questions/domains used forthe evaluation.
For example, the more advanced systems of2001 performed lower than the systems of 1999 as they wereevaluate on a more difficult test-set.Table 5: Performance comparisons between basic Q/A andQ/A using answer re-ranking or elimination policies.MRAR .662SRAR -.372Model RTC0 SVM0 PRTC QSVM QATCMRAR .622 .649 .658 .664 .607(re-rank.
)SRAR -.189 -.135 -.036 -.077 -.320(elimin.
)0.570.590.610.630.650.671 2 3 4 5 6 7 8 9 10# AnswersMRARscorebasic Q/APRTCQSVMFigure 2: The MRAR results for basic Q/A and Q/A with an-swer re-ranking based on question categorization via the PRTCand QSVM models.-0.6-0.5-0.4-0.3-0.2-0.100.10.21 2 3 4 5 6 7 8 9 10# AnswersSRAR scorebasic Q/APRTCQSVM)Figure 3: The SRAR results for basic Q/A and Q/A with an-swer re-ranking based on question categorization via the PRTCand QSVM models.creases, for the first four answers and it reaches a plateauafterwards.
We also notice that the QSVM outperformsboth PRTC and the basic Q/A.
This figure also shows thatquestion categorization per se does not greatly impact theMRAR score of Q/A.Figure 3 illustrates the SRAR curves by consideringthe answer elimination policy.
The figure clearly showsthat the QSVM and PRTC models for question catego-rization determine a higher SRAR score, thus indicatingthat fewer irrelevant answers are left.
Figure 3 shows thatquestion categorization can greatly improve the qualityof Q/A when irrelevant answers are considered.
It alsoshows that perhaps, when evaluating Q/A systems withthe MRAR scoring method, the ?optimistic?
view of Q/Ais taken, in which erroneous results are ignored for thesake of emphasizing that an answer was obtained afterall, even if it was ranked below several incorrect answers.In contrast, the SRAR score that we have describedin Section 5.2 produce a ?harsher?
score, in which errorsare given the same weight as the correct results, but affectnegatively the overall score.
This explains why, even for abaseline Q/A, we obtained a negative score, as illustratedin Table 5.
This shows that the Q/A system generatesmore erroneous answers then correct answers.
If onlythe MRAR scores would be considered we may assessthat TC does not bring significant information to Q/A forprecision enhancement by re-ranking answers.
However,the results obtained with the SRAR scoring scheme, in-dicate that text categorization impacts on Q/A results, byeliminating incorrect answers.
We plan to further studythe question categorization methods and empirically findwhich weighting scheme is ideal.6 ConclusionsQuestion/Answering and Text Categorization have been,traditionally, applied separately, even if category infor-mation should be used to improve the answer search-ing.
In this paper, it has been, firstly, presented a Ques-tion Answering system that exploits the category infor-mation.
The methods that we have designed are based onthe matching between the question and the answer cate-gories.
Depending on positive or negative matching twostrategies allow to affect the Q/A performances: answerre-ranking and answer elimination.We have studied five question categorization modelsbased on two traditional TC approaches: Rocchio andSupport Vector Machines.
Their evaluation confirms thedifficulty of automated question categorization as the ac-curacies are lower than those reachable for document cat-egorization.The impact of question classification in Q/A has beenevaluated using the MRAR and the SRAR scores.
Whenthe SRAR, which considers the number of incorrect an-swers, is used to evaluate the enhanced Q/A system aswell as the basic Q/A system, the results show a greatimprovement.ReferencesR.
H. Baayen, R. Piepenbrock, and L. Gulikers, editors.1995.
The CELEX Lexical Database (Release 2) [CD-ROM].
Philadelphia, PA: Linguistic Data Consortium, Uni-versity of Pennsylvania.Eugene Charniak.
2000.
A maximum-entropy-inspired parser.In In Proceedings of the 1st Meeting of the North AmericanChapter of the ACL, pages 132?139.P.
Clark, J. Thompson, and B. Porter.
1999.
A knowledge-based approach to question-answering.
In proceeding ofAAAI?99 Fall Symposium on Question-Answering Systems.AAAI.Michael Collins.
1997.
Three generative, lexicalized modelsfor statistical parsing.
In Proceedings of the ACL and EA-CLinguistics, pages 16?23, Somerset, New Jersey.S.
Harabagiu, M. Pasca, and S. Maiorano.
2000.
Experimentswith open-domain textual question answering.
In Proceed-ings of the COLING-2000.Sanda M. Harabagiu, Dan I. Moldovan, Marius Pasca, RadaMihalcea, Mihai Surdeanu, Razvan C. Bunescu, RoxanaGirju, Vasile Rus, and Paul Morarescu.
2001.
The role oflexico-semantic feedback in open-domain textual question-answering.
In Meeting of the ACL, pages 274?281.David J. Ittner, David D. Lewis, and David D. Ahn.
1995.Text categorization of low quality images.
In Proceedingsof SDAIR-95, pages 301?315, Las Vegas, US.T.
Joachims.
1999.
T. joachims, making large-scale svm learn-ing practical.
In B. Schlkopf, C. Burges, and MIT-Press.A.
Smola (ed.
), editors, Advances in Kernel Methods - Sup-port Vector Learning.Alessandro Moschitti.
2003a.
Natural Language Processingand Automated Text Categorization: a study on the recipro-cal beneficial interactions.
Ph.D. thesis, Computer ScienceDepartment, Univ.
of Rome ?Tor Vergata?.Alessandro Moschitti.
2003b.
A study on optimal parametertuning for Rocchio text classifier.
In Fabrizio Sebastiani, ed-itor, Proceedings of ECIR-03, 25th European Conference onInformation Retrieval, Pisa, IT.
Springer Verlag.Marius A. Pasca and Sandra M. Harabagiu.
2001.
High per-formance question/answering.
In Proceedings ACM SIGIR2001, pages 366?374.
ACM Press.J.J.
Rocchio.
1971.
Relevance feedback in information re-trieval.
In G. Salton, editor, The SMART Retrieval System?Experiments in Automatic Document Processing, pages 313-323 Englewood Cliffs, NJ, Prentice Hall, Inc.G.
Salton.
1989.
Automatic text processing: the transfor-mation, analysis and retrieval of information by computer.Addison-Wesley.V.
Vapnik.
1995.
The Nature of Statistical Learning Theory.Springer.Y.
Yang.
1999.
An evaluation of statistical approaches to textcategorization.
Information Retrieval Journal.
