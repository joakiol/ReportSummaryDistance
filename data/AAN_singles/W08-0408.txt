Proceedings of the Second ACL Workshop on Syntax and Structure in Statistical Translation (SSST-2), pages 61?68,ACL-08: HLT, Columbus, Ohio, USA, June 2008. c?2008 Association for Computational LinguisticsMultiple Reorderings in Phrase-based Machine TranslationNiyu Ge,  Abe IttycheriahIBM T.J.Watson Research1101 Kitchawan Rd.Yorktown Heights, NY 10598(niyuge, abei)@us.ibm.comKishore PapineniYahoo!
Research45 West 18th St.New York, NY 10011kpapi@yahoo-inc.comAbstractThis paper presents a method to integratemultiple reordering strategies inphrase-based statistical machinetranslation.
Recently there has been muchresearch effort in reordering problems inmachine translation.
State-of-the-artdecoders incorporate sophisticated localreordering strategies, but there is littleresearch on a unified approach toincorporate various kinds of reorderingmethods.
We present a phrase-baseddecoder which easily allows multiplereordering schemes.
We show how to usethis framework to perform distance-basedreordering and HIERO-style (Chiang2005) hierarchical reordering.
We alsopresent two novel syntax-based reorderingmethods, one built on part-of-speech tagsand the other based on parse trees.
We willgive experimental results using theserelatively easy to implement methods onstandard tests.1 Introduction and Previous WorkGiven an input source sentence and guided by atranslation model, language model, distortionmodel, etc., a machine translation decodersearches for a target sentence that is the besttranslation of the source.
There are usually twoaspects of the search.
One tries to find targetwords for a given source segment.
The othersearches for the order in which the sourcesegments are to be translated.
A source segmenthere means a contiguous part of the sourcesentence.
The former is largely controlled bylanguage models and translation models and thelatter by language models and distortion models.It is, in most cases, the latter, the search for thecorrect word order (which source segment to betranslated next) that results in a largecombinatorial search space.
State-of-the-artdecoders use dynamic programming basedbeam-search with local reordering (Och 1999,Tillmann 2000).
Although local reordering tosome degree is implicit in phrase-baseddecoding, the kind of reordering is very limited.The simplest distance-based reordering, from thecurrent source position i, tries to defer thetranslation of the next n words (1 ?
n ?
N, N themaximum number of words to be delayed).
N isbounded by the computational requirements.Recent work on reordering has been on trying tofind ?smart?
ways to decide word order, usingsyntactic features such as POS tags (Lee and Ge2005) , parse trees (Zhang et.al, 2007, Wang et.al.2007,  Collins et.al.
2005, Yamada and Knight2001) to name just a few, and synchronized CFG(Wu 1997, Chiang 2005), again to name just afew.
These efforts have shown promisingimprovements in translation quality.
However,to use these features during decoding requireseither a separate decoder to be written or somead-hoc mechanisms to be invented to incorporatethem into an existing decoder, or in some cases(Wang et.
al.
2007) the input source ispre-ordered to be decoded monotonically.
(Kanthak et.
al.
2005) described a framework  inwhich different reordering methods arerepresented as search constraints to a finite stateautomata.
It is able to compute distance-basedand ITG-style reordering automata.
We differfrom that approach in a couple of ways.
One isthat in (Kanthak et.
al.
2005), an on-demand61reordering graph is pre-computed which is thentaken as a input for monotonic decoding.
Wecompute the reordering as the sentence is beingdecoded.
The second is that it is not clear how togenerate the permutation graphs under, sayHIERO-type hierarchical constraints,  or othersyntax-inspired reorderings such as those basedon part-of-speech patterns.
Our approach differsin that we allow greater flexibility in capturing awider range of reordering strategies.We will first give an overview of  the framework(?2).
We then describe how to implement fourreordering methods in a single decoder in ?3.
?4presents some Chinese-English results on theNIST MT test sets.
It also shows results on weblog and broadcast news data.2 Reordering in Decoding2.1 HypothesisThe process of MT decoding can be thought of asa process of hypothesizing target translations.Given an input source sentence of length L, thedecoding is done segment by segment.
Asegment is simply an n-word source chunk,where 1 ?
n ?
L.  Decoding finishes when allsource chunks are translated (some source wordsthat have no target translations can be thought ofas being translated into a special token NULL).The decoder at this point outputs its besthypothesis.2.2 Hypothesis with reorderingsIn order to facilitate various search strategies, aseparation of duty is called for.
The decoder iscomposed of two major modules, a reorderingmodule and a production module.
The reorderingmodule decides which source segment to betranslated next.
The production moduleproduces the actual translations for a givensegment.
Although most of the start-of-the-artdecoders have these two modules, they arenevertheless tightly coupled.
Here they areseparated.
This separation does not compromisethe search space of the decoder.
Hypotheses thatare explored in the traditional way are stillexplored in this framework.
This separation isessential if one were to design a decoder thatincorporates phrase-based, syntax-based, andother types of decoding in a unified anddisciplined way.
In the decoder, each hypothesiscarries with it a sequence of source segments tobe decoded at the current time step.
After theproduction module translates these segments andafter beam pruning is applied to all thehypotheses produced at this time step, thehypotheses go back to the reordering modulewhich determines the next source segments to betranslated.
This process continues until all sourcewords are translated.One can think of the reordering module as a blackbox whose sole responsibility is to determine thenext sequence of source segments to be translated.Given this separation, the reordering module canbe implemented in whichever way and thechanges in it do not require changes to any othermodules in the decoder.
There can be a suite ofsuch modules, each exploring different featuresand implementing different search schemes.
Areordering module that implement basicdistance-based reordering will take twoparameters, the number of source words to beskipped and the window size that determineswhen the skipped words must be translated.
Areordering module that is based on HIERO ruleswill take the library of HIERO rules and selectthe subset that fire on a given input sentence.
Themodule will use this subset of rules to determinethe source translation order.
A parse-inspiredreordering module will take an input parse treeand based on either a trained model orhand-written rules  decide the next sourcesequence to be translated.
As long as all thereordering modules are written to a commoninterface,  they can be separately written andmaintained.Figure 1 shows an example of how threereordering modules can be incorporated into asingle decoder.
The input source is S1?Sn.Moduleskip = 2window = 3S1 S2 X ?> T1 T2 XS1S2 S3Sn?1 Sn.....Distance?basedReorderingS1 X Sn ?> Tn X T1 HIERO?basedReorderingParse?basedReorderingS1S2S3SnS1S1S2Sn?1ProductionFigure 1.
Reordering module example62Each reordering module has its own resourcesand parameters which are shown on the left side.Each reordering module produces a vector ofnext source positions.
The production moduletakes these positions and produces translationsfor them.3  Reordering ModulesIn this section, we describe four reorderingmodules implementing different reorderingstrategies.
The framework is not limited to thesefour methods.
We present these four todemonstrate the ability of the framework toincorporate a wide variety of reordering methods.3.1 Distance-based Skip ReorderingThis is the type of reordering first presented by(Brown et.al.
1993) and was briefly alluded to inthe above Introduction section.
This method iscontrolled by 2 parameters:Skip = number of words whosetranslations are to be delayed.
Let us call thesewords skipped words.WindowWidth (ww) = maximumnumber of words allowed to be translated beforetranslating the skipped words.This reordering module outputs all the possiblenext source words to be translated according tothese two parameters.
For illustration purposes,let us use a bit vector B to represent which sourcewords have been translated.
Thus those that havebeen translated have value 1 in the bit vector, andthose un-translated have 0.
As an example, letskip = 2 and ww = 3, and an input sentence oflength = 10.
Initially, all 10 entries of B are 0.
Atthe first time step, only the following are possiblenext positions:a) 1 0 0 0 0 0 0 0 0 0 :  translate 1st wordb) 0 1 0 0 0 0 0 0 0 0 :  skip 1st wordc) 0 0 1 0 0 0 0 0 0 0 :  skip 1st and 2nd wordsAt the next time step,  if  we want to continue thepath of c),  we have these choices:1) we can leave the first 2 words open andcontinue until we reach 3 words (because ww=3)c1) 0 0 1 1 0 0 0 0 0 0c2) 0 0 1 1 1 0 0 0 0 02) or we can go back and translate either of thefirst 2 skipped words:c3) 1 0 1 0 0 0 0 0 0 0c4) 0 1 1 0 0 0 0 0 0 0It is clear that the search space easily blows upwith large skip and window-width values.Therefore, a beam pruning step is performed afterpartial hypotheses are produced at every timestep.3.2 HIERO Hierarchical ReorderingIn this section we show an example of how theHiero decoding method (Chiang 2005) can beimplemented as a reordering module in thisframework.
This is not meant to show that ourMT decoder is a synchronous CFG parser.
Thisis a conceptual demonstration of how the Hierorules can be used in a reordering module todecide the source translation order and thus usedin a traditional phrase-based decoder.
Thismodule uses the Hiero rules to determine the nextsource segment to be translated.
The example isChinese-English translation.
Consider thefollowing Chinese sentence (word position andEnglish gloss are shown in parentheses):(1.Australia) (2. is)  (3. with) (4.
North Korea) 	(5. have)(6. diplomaticrelation)  (7.
NULL) (8. few)  (9.country) (10. one of)Suppose we have two following Hiero rules: X ?
Australia X  (1) X  ?
is one of X   (2)The left-hand-side of Hiero rules are sourcephrases and the right-hand-side is their Englishtranslation and the Xs are the non-terminalswhose extent is determined by the source inputagainst which the rules are tested for matching.A rule fires if its left-hand-side matches certainsegments of the input.Given the above Chinese input and the two Hierorules, the Hiero decoder as described in (Chiang2005) will produce a partial hypothesis?Australia is one of?
by firing the two rulesduring parsing (see Chiang 2005 for decodingdetails).
We will show how to decode in theHiero paradigm using the framework.63The reordering module first decides a sourcesegment based on rule (1).
Rule (1) generates asequence of source segments in term of sourceranges: <[1,1],[2,10]>.
This means the sourcesegment spanning range [1,1] (word 1, /Australia) is to be translated first, and then theremaining segment spanning range [2,10] is to betranslated next.
This is exactly what rule (1)dictates where  corresponds to source[1,1] in the reordering module?s output and the Xis [2,10].
The range [1,1], after being given to theproduction module,  results in the production of apartial hypothesis where the target ?Australia?
isproduced.
The task now is to translate the nextsource range [2,10].
At this point,  the reorderingmodule generates another source segmentaccording to rule (2) where the left-hand-side ?X ?
is matched against the input and threecorresponding source ranges are found which are[2,2] (/is), [4,9] (X), and [10,10] (/one of).According to rule (2), this source sequence is tobe translated in the order of [2,2] (is), [10,10](one of), and then [4,9] (X).
Therefore the outputof the reordering module at this stage is<[2,2],[10,10],[4,9]>.
This would then go on tobe translated and results in a partial hypothesis to?Australia is one of?.
Thus ?Australia is one of?is a partial production which covers sourcesegments [1,1] [2,2] and [10,10] in that order.Note that the source segments decoded so far arenot contiguous and this is the effect of long-rangereordering imposed by rule (2).
The next stage is<[4,9]> which is what the X in rule (2)corresponds to.
From here onwards, other ruleswill fire and the decoding sequence these rulesdictate will be realized by the reordering modulein the form of source ranges.
This process canalso be viewed hierarchically in Figure 2.In Figure 2 the ranges (the bracketed numbers)are source segments and the leaves are Englishproductions.
Initially we have the whole inputsentence as one range [1,10].
According to rule(1), this initial range is refined to be<[1,1],[2,10]>,  the 2nd level in Figure 2.
The[2,10] is further refined by rule (2)  to generatethe 3rd level ranges <[2,2],[10,10],[4,9]> and theprocess goes on.
Ranges that cannot be furtherrefined go into the production module which...[1,10][1,1]Australia[2,10][2,2] [10,10] [4,9]is one ofFigure 2.
Hiero-style decodinggenerates partial hypotheses which are the leavesin the figure.
In other words, the partialhypotheses are generated by traversing the tree inFigure 2 in a left-to-right depth-first fashion.3.3 Generalized Part-Of-Speech-basedReorderingThe aim of a generalized part-of-speech-basedreordering method is to tackle the problem oflong-range word movement.
Chinese is apre-modification language in which the modifiersprecede the head.
The following is an examplewith English gloss   in parentheses.
Theprepositional modifier ?on the table'' follows thehead ?the book'' in English (3.3b), but precedes itin Chinese (3.3a).
When the modifiers are long,word-based local reordering is inadequate tohandle the movement.3.3a.
(table)  (on)  (NULL) (book)3.3b.
the book on the tableThere have been several approaches to theproblem some of which are mentioned in ?1.Compared to these methods, this approach islightweight in that it requires only part-of-speech(POS) tagging on the source side.
The idea is tocapture general long-distance distortionphenomena by extracting reordering patternsusing a mixture of words and part-of-speech tagson the source side.
The reordering patterns areextracted for every contiguously aligned sourcesegment in the following form:source  sequence ?
target sequenceBoth the source sequence and the targetsequence are expressed using a combination ofsource words and POS tags.
The patterns are?generalized?
not only because POS tags are usedbut also because variables or place-holders are64allowed.
Given a pair of source and targettraining sentences, their word alignments andPOS tags on the source, we look for anycontiguously aligned source segment and extractword reordering patterns around it.
Figure 3shows an example.Shown in Figure 3 are a pair of Chinese andEnglish sentence, the Chinese POS tags and theword alignment indicated by the lines.
Whenmultiple English words  are aligned to a singleChinese word, they are grouped by a rectangle foreasy viewing.
Here we have a contiguouslyaligned source segment from position 3 to 8.Using the range notation, we say that sourcerange [3,8] is aligned to target range [6, 14].
LetX denote the source  segment [3,8].
The sourceverb phrase (at positions 9 and 10) occur after Xwhereas the corresponding target verb phrase(target words 2,3, and 4) occur before thetranslation of X (which is target [6,14]).
We thusextract the following pattern: X V N ?
V N  X       (1)where the left-hand side ? X V N?
is the sourceword sequence and the right-hand side ?V N  X?is the target word sequence.
The X  in the patternis meant to represent a variable, to be matched bya sequence of source words in the test data whenthis pattern fires during decoding.
Note that thepattern is a mixture of words and POS tags.Specifically, the word identity of the preposition (position 2) is retained whereas the contentwords (the verb and the noun) are substituted bytheir POS tags.
This is because in general, for thereordering purpose the POS tags are good classrepresentations for content words whereasdifferent prepositions may have different wordorder patterns so that mapping them all to a singlePOS P masks the difference.
Examples ofpatterns are shown in Table 1.In Chinese-English translation, the majority ofthe reorderings occur around verb modifiers(prepositions) and noun modifiers (usuallyaround the Chinese part-of-speech DEG as inposition 6).
Therefore we choose to extract onlythese 2 kinds of patterns that involve apreposition and/or a DEG.
In the example above,there are only 2 such patterns: X V N ?
V N  X              (1)X1 DEG X2 ?
X2 DEG X1           (2)Figure 3.
Chinese/English Alignment ExampleSource Seq.
Target Seq.
Count P(tseq|sseq)1 X DEG NN X DEG NN 861 0.1982 X DEG NN X NN DEG 1322 0.3053 X DEG NN NN DEG X 2070 0.4774 X DEG NN NN X DEG 10 0.0025 X DEG NN DEG NN X 52 0.0126 X DEG NN DEG X NN 22 0.0057  X VV  X VV 15 0.1188  X VV VV  X 112 0.8829 X VV VV  X 2 0.04110 X VV X VV 47 0.959Table 1.
Pattern examplesIn the table, we see that when the preposition is  (rows 7 and 8, translation: by), then theswapping is more likely (0.882 in row 8).
Whenthe preposition is  (rows 9 and 10 translation:because), then the target most often stays thesame order as the source (prob 0.959, last row).3.4 Parse-based Lexicalized ReorderingPart-of-speech reordering patterns as described in?3.3 are crude approximation to the structure ofthe source sentence.
For example, in the sourcepattern ?X DEG NN?, the variable X can match asource segment of arbitrary length which isfollowed by ?DEG NN?.
Although it doescapture very long range movement as a result ofSrcPOS  Source              Target1.NNP   1.WTO2.P                2.made3.NNP                 3.a4.CC    4.decision5.NNP     5.on6.DEG    6.the7.NN !
"#                   7.anti-dumping8.NN $%   8.dispute9.V &'   9.between10.NN ()   10.Canada11.and12.the13.United14.States65X matching a long segment, it often searchesunnecessarily for those segments that areimplausible matches to X.
The goal of thepattern ?X DEG NN?
is to capture thepre-modification phenomenon in Chinese whereX  is to match a modifier.
Parse trees are good atcapturing these structures.
A parse tree is shownin Figure 4a using notation from ChineseTreebank CHTB5 (nodes with same label arenumbered for easy reference).The node CP has 2 children, first of which is anIP and second is the word whose POS is DEG.This tree denotes a big NP (top node NP1) whosehead is the rightmost NP (NP2).
The IP under theCP is the modifier.
Given this tree, we can easilytell the span of the modifier IP.IPNP1NP2CPDEGVP1PP VP24a.
NP rule         4b.
VP ruleLCPPP LCPL*VP2BA   IPNPVP14c.
PP rule         4d.
BA ruleFigure 4.
Source parse trees to be reorderedParse trees represent the whole structure of theentire sentence.
Not every structure is of interestto the reordering problem.
In a way similar tothat used in part-of-speech-pattern extraction(?3.3), we restrict our attention to four kinds ofstructures, the first of which is NP involving aDEG (as in Figure 4a.)
The other three are inFigure 4b, 4c, and 4d.
In Figure 4c, the label L*means any node, sometimes it is a CP, sometimesan IP, and so on.Figure 4b captures the pre-modification in case ofa VP where PP modifies VP2 in Chinese andneeds to be swapped when translating intoEnglish.
Figure 4c is the case where there areboth pre-position (P) and post-position (LC) inthe Chinese.
In English, there are onlypre-positions and therefore something must bedone to the post-position LC.
Figure 4d is theconstruction in Chinese that turns an SVO wordorder into SOV and here we want VP2 to precedeits object NP.The reordering rules are written using the leavesin the parse tree,  in other words, the lexical items.In the rules below, we use the bracketed label [L]to mean the leaves it covers,  so [NP] means theleaves under NP.
The reordering rules for the 4structures are:NP (Figure 4a): [NP2] [DEG] [IP]VP (Figure 4b): [VP2] [PP]PP (Figure 4c):  [P] [LC] [L*]BA (Figure 4d): [VP2] [NP]Figure 5 is an example of rule 4a.Figure 5.
Lexical example of NP ruleChinese words and their English gloss are writtenat the leaves.
The correct English translation is?cases of malicious violation of consumerinterests?.
The DEG in the tree signals that thepreceding IP is the modifier of the head NP2.Given this tree, the reordering rule is [NP2][DEG] [IP] (see 4a) which will be written in theformsource  sequence ?
target sequencewhich is realized as the following (the indices arefor easy reference and are not in the actual rule)1.
*+ 2.,-.
3./01 4.2 5.
6.34  ?6.34 5.
1.
*+ 2.,-.
3./01 4.2The first three of these structures are explored in(Wang et.al.
2007).
The crucial difference is thatin (Wang et.al.
2007), the reordering rules forIP DEGADVP VPNP3./01consumer4.
2interest1.
*+malicious5.nullNP1CP NP26.
34case2.
,-violate66these structures are used as a hard decision topre-order the source.
Here the rules are used toextract reorder patterns which are used as anintegral part of the decoder.
The reorderingmodule not only proposes the next sourcesegment according to the reordering patterns butalso proposes monotone choices.
This is becausefirst, the parser is errorful.
In this work, we usethe Stanford Parser (Levy and Manning 2003).On the last 929 sentences of CHTB5, the parserachieves 81% label F-measure on true CHTB5word segmentation and drops to 65% on systemsegmentation using the Stanford CRF Segmenter(Tseng et.al.
2005).
The second reason to let thedecoder choose between reordering andmonotone is other modules such as phrase tablesand target LM can have an influence on the orderchoice too, especially when both reorder andmonotone are acceptable as in the followingexample:CH: 	(my/mine/I/me)(DEG/null)  (book)English1:  my book (monotone)English2: the book of mine (reorder)Since the Chinese has a DEG, our reordering rulewill prefer to swap but monotone is often correct .In cases like these we let the other models, suchas TM and LM, to also have a say in deciding theoutcome.
The reordering module will presentboth choices to be produced.4 Experiment ResultsWe run our experiments on NISTChinese-English MT03 and MT04 and also onweblog  (WL) and broadcast news (BN) data.The WL and BN test sets are held-out data fromLDC-released parallel training data.
WL data isfrom LDC2006E34 and BN is fromLDC2006E10.
The metric reported is casedBLEUn4 4-gram BLEU (Papineni et.al.
2001) .We train HMM alignments in both direction toget source-to-target and target-to-sourceprobabilities.
We have a smoothed 5-gramEnglish LM built on the English Gigawordcorpus and the English side of theChinese-English parallel corpora distributed byLDC from year 2000 to 2005.For distance-based skip reordering (?3.1) weexperimented with four sets of skip andWindowWidth values.For part-of-speech reordering patterns, we usethe  3259 hand alignments contained inLDC2006E93.
We build a MaxEnt Chinese POStagger and tagged the Chinese side of this data.The tagger achieves 92% F-measure on the 10%heldout data of CHTB5.
We then extractedreordering patterns according to the proceduredescribed in ?3.3.
A total of 788 source patternswere extracted.
It is a small pattern set becauseof our specific extraction criteria described in?3.3.
At decoding time, an average of 15-20patterns fire on a single sentence.
We use theunigram probabilities of the rules as shown inTable 1 to score the rules.For parse-based lexical reordering rules, we runthe Stanford parser on the test set and extract thelexicalized patterns.
The number of patterns ofeach test set is shown in Table 2.
The reorderedrules are assigned a value of 0.9 and themonotones are assigned a value of 0.1.Test Set # Sentences # Lex.PatternsMT03 919 4,824MT04 1,788 13,639WL (LDC2006E34) 550 3,261BN (LDC2006E10) 2,069 12,492Table 2.
Test data statisticsThe results on the NIST MT test sets MT03 andMT04 utilizing 4 references are in shown inTable 3.
The results of the weblog and broadcastnews data are shown in Table 4 where there is 1reference for each set.
The confidence intervalsin these experiments are between ?0.l2 and ?0.16.This means the variations in rows 1-5 of Table 3are not statistically significant.
Thepart-of-speech based reordering shows marginalimprovement.
We see significant improvementin using parse-based reordering rules.Cased-BLEUr4n4 MT03 MT041 Skip0 (monotone) 0.2817 0.30232 Skip = 1; WW=2 0.2854 0.30243 Skip = 2; WW = 3 0.2878 0.30614 Skip = 3; WW = 4 0.2903 0.30815 Skip = 4; WW = 5 0.2833 0.30906 Generalized POS 0.3066  0.31827 Parse-based Lex 0.3231 0.3250Table 3.
NIST MT03 and MT04 Results67Cased-BLEUr1n4 Weblog Broadcast NewsSkip0 (monotone) 0.0656 0.0858Generalized POS 0.0694 0.0878Parse-based Lex 0.0862 0.1135Table 4.
Weblog and BN results5.
ConclusionsWe have presented a decoding framework thatgreatly facilitates the incorporation of variousreordering strategies that are necessary to put thewords in the right order during translation.
Thismodularized framework abstracts away thereordering phase from the rest of the decodercomponents.
This not only makes the decodereasier to maintain but also allows rapidexperimentation of a variety of reorderingmethods.
Instead of using one reorderingmodule, multiple reordering modules are used tocome up with a list of next possible sourcesegment choices.
So far we have not seen anysignificant improvement using combination ofreordering modules.
This warrants furtherresearch since intuitively the knowledge-richmodules and the distance-based methods ought tocomplement each other.
The POS andparse-based methods are very targeted and workquite well when the source structure is correctlyunderstood, but cannot correct itself when errorsoccur in the tagging or the parsing process.
Thedistance-based methods pay no attention tostructure and is thus immune from sourceprocessing errors.Although we present the POS-based andparse-based reordering modules in the context ofChinese to English translation, they can be usedfor other languages as well.
For example, inArabic to English translation,  we  extractpatterns that capture the VSO word order ofArabic (English is SVO) and also the adjectivalpost-modification of noun.The framework greatly reduces the amount ofwork needed to experiment with drasticallydifferent ways of reordering.
All these can nowbe done in one single decoder.AcknowledgementsThis work was partially supported by theDepartment of the Interior, National BusinessCenter under  contract No.
NBCH2030001 andDefense Advanced Research Projects Agencyunder contract No.
HR0011-06-2-0001.
Theviews and findings contained in this material arethose of the authors and do not necessarily reflectthe position or policy of the U.S. government andno official endorsement should be inferred.ReferencesP.F.Brown, S.A.Della Pietra, V.J.Della Pietra, andR.L.Mercer.
The Mathematics of Statistical MachineTranslation.
Computation Linguistics, 19(2).D.
Chiang 2005 A hierarchical phrase-based modelfor statistical machine translation.
2005 ACL.Y.Lee and N.Ge 2006 Local reordering in statisticalmachine translation.
Workshop of TCStar 2006R.
Levy and C. Manning.
2003.
Is it harder to parseChinese, or the Chinese Treebank?
ACL 2003S.
Kanthak, D. Vilar, E. Matusov, R. Zens, and H. Ney.Novel Reordering Approaches in Phrase-BasedStatistical Machine Translation.
In ACL Workshopon Building and Using Parallel Texts 2005F.Och, P. Koehn,  and D. Marcu.
2003.
Statisticalphrase-based translation.
HLT-NAACL 2003F.Och, C.Tillmann, H.Ney 1999 Improved alignmentmodels for statistical machine ranslation, EMNLPF.
Och.
2003.
Minimum error rate training instatistical machine translation.
ACL2003K.Papineni, S.Roukos, T.Ward, W.Zhu 2001.
Amethod for automatic evaluation for MT, ACL 2001C.Tillmann, H. Ney 2000 Word reordering andDP-based search in SMT, COLING 2000H.
Tseng, P. Chang, G. Andrew, D. Jurafsky and C.Manning.
A Conditional Random Field WordSegmenter.
In Fourth SIGHAN Workshop 2005.Dekai Wu.
1997 Stochastic inversion transductiongrammars and bilingual parsing of parallel corpora.Computational Linguistics,  Vol.
23, pp.
377-404Kenji Yamada and Kevin Knight 2001 A syntax-basedstatistical translation model.
ACL  2001D.Zhang,  M. Li, C. Li, and M. Zhou.
PhraseReordering Model Integrating Syntactic Knowledgefor SMT.
Proceedings of  EMNLP 2007C.
Wang, M.Collins, and P.Koehn.
Chinese SyntacticReordering for Statistical Machine Translation.Proceedings of  EMNLP 200768
