Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 551?555,Dublin, Ireland, August 23-24, 2014.SentiKLUE: Updating a Polarity Classifier in 48 HoursStefan Evert and Thomas Proisl and Paul Greiner and Besim KabashiFriedrich-Alexander-Universit?t Erlangen-N?rnbergDepartment Germanistik und KomparatistikProfessur f?r KorpuslinguistikBismarckstr.
6, 91054 Erlangen, Germany{stefan.evert,thomas.proisl,paul.greiner,besim.kabashi}@fau.deAbstractSentiKLUE is an update of the KLUE po-larity classifier ?
which achieved good androbust results in SemEval-2013 with a sim-ple feature set ?
implemented in 48 hours.1 IntroductionThe SemEval-2014 shared task on ?SentimentAnalysis in Twitter?
(Rosenthal et al., 2014) is a re-run of the corresponding shared task from SemEval-2013 (Nakov et al., 2013) with new test data.It focuses on polarity classification in computer-mediated communication such as Twitter, othermicro-blogging services, and SMS.
There are twosubtasks: the goal of Message Polarity Classifica-tion (B) is to classify an entire SMS, tweet or othermessage as positive (pos), negative (neg) or neutral(ntr); in the subtask on Contextual Polarity Disam-biguation (A), a single word or short phrase has tobe classified in the context of the whole message.The training data are the same as in SemEval-2013.
The test data from 2013 are used as a devel-opment set in order to select features and tune ma-chine learning algorithms, but may not be includedin the training data.
The 2014 test set comprisesthe development data, new Twitter messages, Live-Journal entries as out-of-domain data, and a smallnumber of tweets containing sarcasm (see Rosen-thal et al.
(2014) for further details).
For subtask B,there are 10,239 training items, 5,907 items in thedevelopment set, and 3,861 additional unseen itemsin the new test set.
For subtask A, there are 9,505training items, 6,769 items in the development set,and 3,912 additional items in the test set.Our team participated in the SemEval-2013shared task with a relatively simple, but robustThis work is licensed under a Creative Commons Attribution4.0 International License.
Page numbers and proceedingsfooter are added by the organisers.
Licence details: http://creativecommons.org/licenses/by/4.0/system (KLUE) based on a maximum entropy clas-sifier and a small set of features (Proisl et al., 2013).Despite its simplicity, KLUE performed very wellin subtask B, ranking 5th out of 36 constrainedsystems on the Twitter data and 3rd out of 28 onthe SMS data.
Results for contextual polarity dis-ambiguation (subtask A) were less encouraging,with rank 14 out of 21 constrained systems on theTwitter data and rank 12 out of 19 on the SMS data.This paper describes our efforts to bring theKLUE system up to date within a period of 48hours.
The results obtained by the new SentiKLUEsystem are summarised in Table 1, showing that theupdate was successful.
The ranking of the systemhas improved substantially in subtask A, making itone of the best-performing systems in the sharedtask.
Rankings in subtask B are similar to thoseof the previous year, showing that SentiKLUE haskept up with recent developments.
Moreover, dif-ferences to the best-performing systems are muchsmaller than in SemEval-2013.2 Updating the KLUE polarity classifierThe KLUE polarity classifier is described in de-tail by Proisl et al.
(2013).
It used the followingfeatures as input for a maximum entropy classifier:?
The AFINN sentiment lexicon (Nielsen, 2011),which provides numeric polarity scores rangingfrom ?5 to +5 for 2,476 English word forms,extended with distributionally similar words.For each input message, the number of positiveand negative words as well as their averagepolarity score were computed.?
Emoticons and Internet slang expressions thatwere manually classified as positive, negativeor neutral.
Features were generated in the sameway as for the sentiment lexicon.?
A bag-of-words representation that generates aseparate feature for each word form that occursin at least 5 different messages ( f ?
5).
Only551task subset rank score bestB LJ14 3 / 42 73.99 74.84B SMS13 4 / 42 67.40 70.28B Twit13 6 / 42 69.06 72.12B Twit14 10 / 42 67.02 70.96B Sarcasm 24 / 42 43.36 58.16A LJ14 1 / 20 85.61 85.61A SMS13 6 / 20 85.16 89.31A Twit13 2 / 20 90.11 90.14A Twit14 2 / 20 84.83 86.63A Sarcasm 2 / 20 79.32 82.75Table 1: SentiKLUE results in SemEval 2014Task 9 (among constrained systems).
See Rosen-thal et al.
(2014) for further details and rankingsincluding the unconstrained systems.single words (unigrams) were used, since ex-periments with additional bigram features didnot lead to a clear improvement.?
A negation heuristic, which inverts the polar-ity score of the first sentiment word within 4tokens after a negation marker.
In the bag-of-words representation, the next 3 tokens after anegation marker are prefixed with not_.?
For subtask A, these features were computedboth for the marked word or phrase and for therest of the message.In order to improve the KLUE classifier, we drewinspiration from two other systems participatingin the SemEval-2013 task: NRC-Canada (Moham-mad et al., 2013), which won the task by a largemargin over competing systems, and GU-MLT-LT(G?nther and Furrer, 2013), which used similar fea-tures to our classifier, but obtained better resultsdue to careful selection and tuning of the machinelearning algorithm.Mohammad et al.
(2013) used a huge set of fea-tures, including several sentiment lexica (both man-ually and automatically created), word n-grams (upto 4-grams with low frequency threshold), charac-ter n-grams (3-grams to 5-grams), Twitter-derivedword clusters and a negation heuristic similar toour approach.
Features with the largest impactin subtask B were sentiment lexica (esp.
large au-tomatically generated word lists), word n-grams,character n-grams and the negation heuristic, in thisorder.
NRC-Canada achieved F-scores of 68.46(SMS) and 69.02 (Twitter) in task B, as well as88.00 (SMS) and 88.93 (Twitter) in task A.G?nther and Furrer (2013) claim that state-of-the-art results can be obtained with a small fea-ture set if a suitable machine learning algorithmis chosen.
They used stochastic gradient descent(SGD) and tuned its parameters by grid search.
GU-MLT-LT achieved scores of 62.15 (SMS) and 65.27(Twitter) in task B, as well as 88.37 (SMS) and85.19 (Twitter) in task A.We therefore decided to make use of a widerrange of sentiment lexica, extend the bag-of-wordsrepresentation to bigrams, implement character n-gram features, and experiment with different ma-chine learning algorithms, resulting in the Senti-KLUE system described in the following section.3 The SentiKLUE systemSentiKLUE is an improved version of the KLUEsystem and uses the same tokenisation, preprocess-ing and negation heuristics; see Proisl et al.
(2013)for details.
The features described below are usedas input for a machine learning classifier that pre-dicts the polarity categories positive (pos), nega-tive (neg) or neutral (ntr).
As in KLUE and GU-MLT-LT, the implementations of the Python libraryscikit-learn (Pedregosa et al., 2011)1are used.
Wetested four different learning algorithms: logisticregression (MaxEnt), stochastic gradient descent(SGD), linear SVM (LinSVM) and SVM with aRBF kernel (SVM).
Parameters were tuned by gridsearch and the best-performing algorithm was cho-sen for each subtask.
SentiKLUE makes use of thefollowing features:?
Several sentiment lexica, which are treated aslists of positive and negative polarity words.Numerical scores are converted by setting ap-propriate cutoff thresholds.
For each lexicon,we compute the number of positive and neg-ative words occurring in a message as fea-tures, with separate counts for negated and non-negated contexts.?
AFINN (Nielsen, 2011)2?
Bing Liu lexicon (Hu and Liu, 2004)3?
MPQA (Wilson et al., 2005)4?
SentiWords (Guerini et al., 2013)5; we cre-1http://scikit-learn.org/2http://www2.imm.dtu.dk/pubdb/views/publication_de-tails.php?id=60103http://www.cs.uic.edu/~liub/FBS/sentiment-analysis.html4http://mpqa.cs.pitt.edu/lexicons/subj_lexicon/5http://hlt.fbk.eu/technologies/sentiwords552ated two word lists with score thresholds of0.3 and 0.1?
Sentiment140 (Mohammad et al., 2013)6,which was compiled from a corpus of 1.6million tweets for NRC-Canada; we createdseparate lists for normal words and hashtagswith a score threshold of 1.0?
NRC Hashtag Sentiment Lexicon (Moham-mad et al., 2013)7, which contains wordsthat exhibit a strong statistical association(PMI score) to positive or negative hashtags,also compiled for NRC-Canada; again, wecreated separate lists for normal words andhashtags with a score threshold of 0.8?
a manual extension including synonyms,antonyms and several word lists from on-line sources, compiled by the SNAP team(Schulze Wettendorf et al., 2014)?
an automatic extension with distributionallysimilar words (DSM extension), using a strat-egy similar to Proisl et al.
(2013)?
Word form unigrams and bigrams.
Aftersome experimentation, the document frequencythreshold was set to f ?
5 for subtask B andf ?
2 for subtask A.?
In order to include information from charactern-grams, we used a Perl implementation of n-gram language models (Evert, 2008) that hasalready been applied successfully to text cat-egorization tasks (boilerplate detection in theCLEANEVAL 2007 competition).
We trainedthree separate models on positive, negative andneutral messages.
We selected a 5-gram model(n = 5) with strong smoothing (q = 0.7), whichminimized cross-entropy on the training data(measured by cross-validation).
For each mes-sage in the training and test data, three featureswere generated, specifying per-character cross-entropy for each of the three n-gram models.8?
Counts of positive and negative emoticons us-ing the same lists as in the KLUE system.?
The same negation heuristic as in KLUE.96http://www.umiacs.umd.edu/~saif/WebPages/Abstracts/NRC-SentimentAnalysis.htm7ibid.8Note that these features had to be generated by cross-validation on the training data to avoid catastrophic overfitting.9The full list of negation markers is not, don?t, doesn?t,won?t, can?t, mustn?t, isn?t, aren?t, wasn?t, weren?t, couldn?t,shouldn?t, wouldn?t.
To our surprise, including further nega-tion markers such as none, ain?t or hasn?t led to a decrease inclassification quality.For subtask A, we chose a simplistic strategy andcomputed the same set of features for the markedword or phrase instead of the entire message.
Inorder to take context into account, the three classprobabilities assigned to the complete message bya MaxEnt classifier were included as additionalfeatures.
No other features describing the contextof the marked expression were used.Optionally, features were standardized and priorclass weights (2?
for positive, 4?
for negative)were used in order to balance the predicted labels.The best-performing machine learning algorithmson the development set were MaxEnt for subtask B(L1 penalty, C = 0.3) and linear SVM for subtask A(L1 penalty, L2 loss, C = 0.5), as shown in Table 2.4 Experiments and conclusionIn order to determine the importance of individ-ual features, ablation experiments were carried outfor both subtasks by deactivating one group of fea-tures at a time.
Tables 3 and 4 show the resultingchanges in the official criterion Fp/nseparately foreach subset of the development and test sets, aswell as micro-averaged across the full developmentset (DEV) and test set (GOLD).
Rows are orderedby feature impact on the full gold standard.
Posi-tive values indicate that a feature group has a neg-ative impact on classification quality: results areimproved by omitting the features (which is oftenthe case for the Sarcasm subset).The most important features are bag-of-wordsunigrams and bigrams, closely followed by senti-ment lexica.
Training class weights had a strongpositive impact in subtask B, but decreased per-formance in subtask A.
In our official submission,they were only used for subtask B. Full-messagepolarity is the third most important feature in sub-task A.
Other features contributed relatively smallindividual effects, but were necessary to achievestate-of-the-art performance in combination.
Theyare often specific to one of the subtasks or to aparticular subset of the gold standard.The bottom half of each table shows ablationresults for individual sentiment lexica, with allother features active.
Key resources are the stan-dard lexica (AFINN, Liu, MPQA) as well asTwitter-specific lexica (Sentiment140, NRC Hash-tag).
Noisy word lists (DSM extension, SNAP,SentiWords) have a small or even a negative effect.Surprisingly, the standard lexica seem to give mis-leading cues on the Twitter 2014 subset (Table 3).553CV development set test set (gold standard)task classifier FallFposFnegFntrFallFp/nacc.
FposFnegFntrFallFp/nacc.B MaxEnt .727 .724 .651 .772 .735 .688 .734 .731 .650 .750 .726 .691 .725B SGD .725 .728 .645 .773 .736 .686 .734 .733 .656 .749 .727 .695 .726B LinSVM .702 .687 .604 .743 .700 .646 .701 .699 .599 .716 .689 .649 .690B SVM .702 .721 .631 .742 .716 .676 .712 .729 .636 .720 .709 .683 .706A MaxEnt .864 .890 .872 .179 .849 .881 .863 .893 .853 .171 .841 .873 .856A SGD .864 .889 .867 .223 .849 .878 .860 .891 .847 .188 .839 .869 .852A LinSVM .860 .892 .876 .064 .847 .884 .865 .895 .856 .064 .838 .875 .857A SVM .855 .890 .873 .024 .842 .881 .862 .892 .853 .014 .832 .872 .854Table 2: Performance of different machine learning algorithms on the training data (CV), development setand test set (Fall= weighted average F-score; Fp/n= official score; best results highlighted in bold font).Task B SMS Twitter DEV LJ14 SMS13 Twit13 Twit14 Sarcasm GOLD?
bag of words ?.0837 ?.0322 ?.0502 ?.0344 ?.0807 ?.0316 ?.0335 +.0511 ?.0430?
sentiment lexica ?.0445 ?.0354 ?.0389 ?.0690 ?.0422 ?.0372 ?.0092 +.0750 ?.0363?
training weights ?.0033 ?.0413 ?.0266 ?.0275 ?.0077 ?.0482 ?.0204 ?.0342 ?.0294?
emoticons ?.0071 ?.0107 ?.0087 ?.0006 ?.0067 ?.0105 +.0004 +.0492 ?.0048?
bow bigrams ?.0074 ?.0005 ?.0035 +.0010 ?.0105 ?.0012 ?.0096 +.0956 ?.0028?
feature scaling ?.0027 ?.0010 ?.0014 ?.0021 ?.0030 ?.0026 ?.0004 ?.0034 ?.0020?
character n-grams +.0029 ?.0068 ?.0033 +.0012 +.0040 ?.0044 ?.0056 +.0056 ?.0015?
negation ?.0098 +.0019 ?.0014 ?.0016 ?.0049 +.0002 ?.0012 +.0351 ?.0002?
bow f ?
2 +.0017 +.0026 +.0022 +.0004 +.0021 ?.0003 +.0021 +.0171 +.0013sentiment lexica:?
standard lexica ?.0206 ?.0135 ?.0152 ?.0245 ?.0234 ?.0124 +.0035 +.0586 ?.0124?
Twitter lexica ?.0026 +.0000 ?.0019 ?.0118 ?.0073 ?.0007 ?.0094 +.0034 ?.0066?
SentiWords ?.0008 ?.0010 ?.0009 ?.0034 ?.0015 ?.0005 ?.0075 +.0165 ?.0017?
hashtag lexica ?.0011 +.0021 +.0005 ?.0045 ?.0039 +.0035 +.0011 ?.0302 ?.0005?
DSM extension +.0047 ?.0032 ?.0002 ?.0070 +.0039 +.0022 ?.0025 +.0392 +.0002?
manual extension ?.0008 ?.0018 ?.0011 ?.0015 ?.0019 +.0000 +.0041 +.0361 +.0009only standard lexica ?.0124 ?.0119 ?.0120 ?.0088 ?.0101 ?.0108 ?.0095 +.0439 ?.0094only DSM extension ?.0303 ?.0260 ?.0262 ?.0427 ?.0287 ?.0251 +.0021 +.0183 ?.0230Table 3: Results of feature ablation experiments for subtask B.
Values show change in Fp/n-score if featureis excluded.
Rows are sorted by impact of features on the full SemEval-2014 test data (GOLD).Task A SMS Twitter DEV LJ14 SMS13 Twit13 Twit14 Sarcasm GOLD?
bag of words ?.0283 ?.0252 ?.0256 ?.0207 ?.0292 ?.0249 ?.0411 ?.0041 ?.0273?
sentiment lexica ?.0027 ?.0231 ?.0151 ?.0078 ?.0023 ?.0245 ?.0144 ?.0109 ?.0141?
context (class probs) +.0027 ?.0050 ?.0022 ?.0105 +.0017 ?.0057 ?.0171 +.0390 ?.0062?
negation ?.0081 ?.0041 ?.0052 ?.0064 ?.0063 ?.0024 ?.0058 +.0000 ?.0043?
bow bigrams ?.0045 ?.0009 ?.0022 ?.0014 ?.0046 +.0007 ?.0033 +.0208 ?.0014?
character n-grams ?.0015 +.0003 ?.0004 +.0003 ?.0038 +.0001 ?.0012 +.0085 ?.0012?
feature scaling +.0001 +.0001 +.0001 +.0009 +.0005 ?.0002 ?.0029 ?.0041 ?.0004?
emoticons +.0023 +.0026 +.0025 +.0016 +.0038 +.0012 ?.0062 +.0000 +.0004bow f ?
5 +.0027 +.0000 +.0009 +.0082 +.0027 +.0006 ?.0025 +.0243 +.0015?
training weights +.0046 +.0072 +.0059 +.0104 +.0037 +.0050 +.0000 ?.0145 +.0040sentiment lexica:?
standard lexica ?.0100 ?.0024 ?.0050 +.0014 ?.0086 ?.0035 ?.0055 +.0000 ?.0044?
Twitter lexica ?.0039 ?.0016 ?.0024 ?.0009 ?.0038 ?.0024 ?.0052 ?.0085 ?.0031?
hashtag lexica ?.0023 ?.0007 ?.0012 +.0000 ?.0014 ?.0019 ?.0030 ?.0126 ?.0017?
manual extensions ?.0016 +.0003 ?.0004 +.0021 ?.0025 ?.0009 +.0002 +.0000 ?.0007?
SentiWords +.0017 +.0005 +.0010 +.0001 +.0013 ?.0013 +.0001 +.0000 ?.0002?
DSM extensions +.0099 +.0011 +.0044 ?.0008 +.0098 ?.0006 ?.0004 ?.0085 +.0019only standard lexica +.0030 ?.0038 ?.0011 ?.0019 +.0035 ?.0048 ?.0027 ?.0168 ?.0019only DSM lexica ?.0114 ?.0085 ?.0094 ?.0035 ?.0117 ?.0104 ?.0057 ?.0338 ?.0089Table 4: Results of feature ablation experiments for subtask A.
Values show change in Fp/n-score if featureis excluded.
Rows are sorted by impact of features on the full SemEval-2014 test data (GOLD).554ReferencesStefan Evert.
2008.
A lightweight and efficient tool forcleaning Web pages.
In Proceedings of the 6th In-ternational Conference on Language Resources andEvaluation (LREC 2008), Marrakech, Morocco.Marco Guerini, Lorenzo Gatti, and Marco Turchi.2013.
Sentiment analysis: How to derive prior po-larities from SentiWordNet.
In Proceedings of the2013 Conference on Empirical Methods in NaturalLanguage Processing (EMNLP 2013), pages 1259?1269, Seattle, WA, October.Tobias G?nther and Lenz Furrer.
2013.
GU-MLT-LT:Sentiment analysis of short messages using linguis-tic features and stochastic gradient descent.
In Sec-ond Joint Conference on Lexical and ComputationalSemantics (*SEM), Volume 2: Proceedings of theSeventh International Workshop on Semantic Evalu-ation (SemEval 2013), pages 328?332, Atlanta, GA.Minqing Hu and Bing Liu.
2004.
Mining and summa-rizing customer reviews.
In Proceedings of the 10thACM SIGKDD International Conference on Knowl-edge Discovery and Data Mining (KDD ?04), pages168?177, Seattle, WA.Saif Mohammad, Svetlana Kiritchenko, and XiaodanZhu.
2013.
NRC-Canada: Building the state-of-the-art in sentiment analysis of tweets.
In Second JointConference on Lexical and Computational Seman-tics (*SEM), Volume 2: Proceedings of the SeventhInternational Workshop on Semantic Evaluation (Se-mEval 2013), pages 321?327, Atlanta, GA.Preslav Nakov, Sara Rosenthal, Zornitsa Kozareva,Veselin Stoyanov, Alan Ritter, and Theresa Wilson.2013.
SemEval-2013 task 2: Sentiment analysisin Twitter.
In Proceedings of the 7th InternationalWorkshop on Semantic Evaluation (SemEval-2013).Finn ?rup Nielsen.
2011.
A new ANEW: Evaluationof a word list for sentiment analysis in microblogs.In Proceedings of the ESWC2011 Workshop on Mak-ing Sense of Microposts: Big things come in smallpackages, number 718 in CEUR Workshop Proceed-ings, pages 93?98, Heraklion, Greece, May.F.
Pedregosa, G. Varoquaux, A. Gramfort, V. Michel,B.
Thirion, O. Grisel, M. Blondel, P. Prettenhofer,R.
Weiss, V. Dubourg, J. Vanderplas, A. Passos,D.
Cournapeau, M. Brucher, M. Perrot, and E. Duch-esnay.
2011.
Scikit-learn: Machine learning inPython.
Journal of Machine Learning Research,12:2825?2830.Thomas Proisl, Paul Greiner, Stefan Evert, and BesimKabashi.
2013.
KLUE: Simple and robust meth-ods for polarity classification.
In Second JointConference on Lexical and Computational Seman-tics (*SEM), Volume 2: Proceedings of the Sev-enth International Workshop on Semantic Evalua-tion (SemEval 2013), pages 395?401, Atlanta, Geor-gia, USA, June.
Association for Computational Lin-guistics.Sara Rosenthal, Preslav Nakov, Alan Ritter, andVeselin Stoyanov.
2014.
SemEval-2014 Task 9:Sentiment analysis in Twitter.
In Proceedings of the8th International Workshop on Semantic Evaluation(SemEval-2014), Dublin, Ireland.Clemens Schulze Wettendorf, Robin Jegan, AllanK?rner, Julia Zerche, Nataliia Plotnikova, JulianMoreth, Tamara Schertl, Verena Obermeyer, Su-sanne Streil, Tamara Willacker, and Stefan Evert.2014.
SNAP: A multi-stage XML pipeline for as-pect based sentiment analysis.
In Proceedings of the8th International Workshop on Semantic Evaluation(SemEval-2014), Dublin, Ireland.Theresa Wilson, Janyce Wiebe, and Paul Hoffmann.2005.
Recognizing contextual polarity in phrase-level sentiment analysis.
In Proceedings of the Hu-man Language Technology Conference and Confer-ence on Empirical Methods in Natural LanguageProcessing (HLT-EMNLP 2005), pages 347?354,Vancouver, BC, Canada.555
