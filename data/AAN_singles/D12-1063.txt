Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational NaturalLanguage Learning, pages 688?698, Jeju Island, Korea, 12?14 July 2012. c?2012 Association for Computational LinguisticsThree Dependency-and-Boundary Models for Grammar InductionValentin I. SpitkovskyStanford University and Google Inc.valentin@cs.stanford.eduHiyan AlshawiGoogle Inc., Mountain View, CA, 94043hiyan@google.comDaniel JurafskyStanford University, Stanford, CA, 94305jurafsky@stanford.eduAbstractWe present a new family of models for unsu-pervised parsing, Dependency and Boundarymodels, that use cues at constituent bound-aries to inform head-outward dependency treegeneration.
We build on three intuitions thatare explicit in phrase-structure grammars butonly implicit in standard dependency formu-lations: (i) Distributions of words that oc-cur at sentence boundaries ?
such as Englishdeterminers ?
resemble constituent edges.
(ii) Punctuation at sentence boundaries fur-ther helps distinguish full sentences fromfragments like headlines and titles, allow-ing us to model grammatical differences be-tween complete and incomplete sentences.
(iii) Sentence-internal punctuation boundarieshelp with longer-distance dependencies, sincepunctuation correlates with constituent edges.Our models induce state-of-the-art depen-dency grammars for many languages withoutspecial knowledge of optimal input sentencelengths or biased, manually-tuned initializers.1 IntroductionNatural language is ripe with all manner of bound-aries at the surface level that align with hierarchicalsyntactic structure.
From the significance of func-tion words (Berant et al2006) and punctuationmarks (Seginer, 2007; Ponvert et al2010) as sepa-rators between constituents in longer sentences ?
tothe importance of isolated words in children?s earlyvocabulary acquisition (Brent and Siskind, 2001)?
word boundaries play a crucial role in languagelearning.
We will show that boundary informationcan also be useful in dependency grammar induc-tion models, which traditionally focus on head ratherthan fringe words (Carroll and Charniak, 1992).DT NN VBZ IN DT NN[The check] is in [the mail].?
??
?Subject?
??
?ObjectFigure 1: A partial analysis of our running example.Consider the example in Figure 1.
Because thedeterminer (DT) appears at the left edge of the sen-tence, it should be possible to learn that determinersmay generally be present at left edges of phrases.This information could then be used to correctlyparse the sentence-internal determiner in the mail.Similarly, the fact that the noun head (NN) of the ob-ject the mail appears at the right edge of the sentencecould help identify the noun check as the right edgeof the subject NP.
As with jigsaw puzzles, workinginwards from boundaries helps determine sentence-internal structures of both noun phrases, neither ofwhich would be quite so clear if viewed separately.Furthermore, properties of noun-phrase edges arepartially shared with prepositional- and verb-phraseunits that contain these nouns.
Because typical head-driven grammars model valence separately for eachclass of head, however, they cannot see that the leftfringe boundary, The check, of the verb-phrase isshared with its daughter?s, check.
Neither of theseinsights is available to traditional dependency for-mulations, which could learn from the boundariesof this sentence only that determiners might have noleft- and that nouns might have no right-dependents.We propose a family of dependency parsing mod-els that are capable of inducing longer-range im-plications from sentence edges than just fertilitiesof their fringe words.
Our ideas conveniently lendthemselves to implementations that can reuse muchof the standard grammar induction machinery, in-cluding efficient dynamic programming routines forthe relevant expectation-maximization algorithms.6882 The Dependency and Boundary ModelsOur models follow a standard generative story forhead-outward automata (Alshawi, 1996a), restrictedto the split-head case (see below),1 over lexical wordclasses {cw}: first, a sentence root cr is chosen, withprobability PATTACH(cr | ?
; L); ?
is a special startsymbol that, by convention (Klein and Manning,2004; Eisner, 1996), produces exactly one child, toits left.
Next, the process recurses.
Each (head)word ch generates a left-dependent with probability1 ?
PSTOP( ?
| L; ?
?
?
), where dots represent additionalparameterization on which it may be conditioned.
Ifthe child is indeed generated, its identity cd is cho-sen with probability PATTACH(cd | ch; ?
?
?
), influencedby the identity of the parent ch and possibly other pa-rameters (again represented by dots).
The child thengenerates its own subtree recursively and the wholeprocess continues, moving away from the head, un-til ch fails to generate a left-dependent.
At that point,an analogous procedure is repeated to ch?s right, thistime using stopping factors PSTOP( ?
| R; ?
?
?
).
All parsetrees derived in this way are guaranteed to be projec-tive and can be described by split-head grammars.Instances of these split-head automata have beenheavily used in grammar induction (Paskin, 2001b;Klein and Manning, 2004; Headden et al2009,inter alia), in part because they allow for efficientimplementations (Eisner and Satta, 1999, ?8) ofthe inside-outside re-estimation algorithm (Baker,1979).
The basic tenet of split-head grammars isthat every head word generates its left-dependentsindependently of its right-dependents.
This as-sumption implies, for instance, that words?
left-and right-valences ?
their numbers of childrento each side ?
are also independent.
But it doesnot imply that descendants that are closer to thehead cannot influence the generation of fartherdependents on the same side.
Nevertheless, manypopular grammars for unsupervised parsing behaveas if a word had to generate all of its children(to one side) ?
or at least their count ?
beforeallowing any of these children themselves to recurse.For example, Klein and Manning?s (2004) depen-dency model with valence (DMV) could be imple-1Unrestricted head-outward automata are strictly more pow-erful (e.g., they recognize the language anbn in finite state) thanthe split-head variants, which process one side before the other.mented as both head-outward and head-inward au-tomata.
(In fact, arbitrary permutations of siblingsto a given side of their parent would not affect thelikelihood of the modified tree, with the DMV.)
Wepropose to make fuller use of split-head automata?shead-outward nature by drawing on information inpartially-generated parses, which contain useful pre-dictors that, until now, had not been exploited evenin featurized systems for grammar induction (Cohenand Smith, 2009; Berg-Kirkpatrick et al2010).Some of these predictors, including the identity?
or even number (McClosky, 2008) ?
of already-generated siblings, can be prohibitively expensive insentences above a short length k. For example, theybreak certain modularity constraints imposed by thecharts used in O(k3)-optimized algorithms (Paskin,2001a; Eisner, 2000).
However, in bottom-up pars-ing and training from text, everything about the yield?
i.e., the ordered sequence of all already-generateddescendants, on the side of the head that is in theprocess of spawning off an additional child ?
is notonly known but also readily accessible.
Taking ad-vantage of this availability, we designed three newmodels for dependency grammar induction.2.1 Dependency and Boundary Model OneDBM-1 conditions all stopping decisions on adja-cency and the identity of the fringe word ce ?
thecurrently-farthest descendant (edge) derived by headch in the given head-outward direction (dir ?
{L, R}):PSTOP( ?
| dir; adj, ce).In the adjacent case (adj = T), ch is deciding whetherto have any children on a given side: a first child?ssubtree would be right next to the head, so the headand the fringe words coincide (ch = ce).
In the non-adjacent case (adj = F), these will be different wordsand their classes will, in general, not be the same.2Thus, non-adjacent stopping decisions will be madeindependently of a head word?s identity.
Therefore,all word classes will be equally likely to continue togrow or not, for a specific proposed fringe boundary.For example, production of The check is involvestwo non-adjacent stopping decisions on the left: oneby the noun check and one by the verb is, both ofwhich stop after generating a first child.
In DBM-1,2Fringe words differ also from other standard dependencyfeatures (Eisner, 1996, ?2.3): parse siblings and adjacent words.689DT NN VBZ IN DT NN ?The check is in the mail .P = (1?0?
??
?PSTOP(?
| L; T)) ?
PATTACH(VBZ | ?
; L)?
(1?
PSTOP( ?
| L; T, VBZ)) ?
PATTACH(NN | VBZ; L)?
(1?
PSTOP( ?
| R; T, VBZ)) ?
PATTACH(IN | VBZ; R)?
PSTOP( ?
| L; F, DT) // VBZ ?
PSTOP( ?
| R; F, NN) // VBZ?
(1?
PSTOP( ?
| L; T, NN))2 ?
P2ATTACH(DT | NN; L)?
(1?
PSTOP( ?
| R; T, IN)) ?
PATTACH(NN | IN; R)?
P2STOP( ?
| R; T, NN) ?
P2STOP( ?
| L; F, DT) // NN?
PSTOP( ?
| L; T, IN) ?
PSTOP( ?
| R; F, NN) // IN?
P2STOP( ?
| L; T, DT) ?
P2STOP( ?
| R; T, DT)?
PSTOP(?
| L; F)?
??
?1?
PSTOP(?
| R; T)?
??
?1.Figure 2: Our running example ?
a simple sentence andits unlabeled dependency parse structure?s probability, asfactored by DBM-1; highlighted comments specify headsassociated to non-adjacent stopping probability factors.this outcome is captured by squaring a shared pa-rameter belonging to the left-fringe determiner The:PSTOP( ?
| L; F, DT)2 ?
instead of by a product of twofactors, such as PSTOP( ?
| L; F, NN) ?
PSTOP( ?
| L; F, VBZ).In these grammars, dependents?
attachment prob-abilities, given heads, are additionally conditionedonly on their relative positions ?
as in traditionalmodels (Klein and Manning, 2004; Paskin, 2001b):PATTACH(cd | ch; dir).Figure 2 shows a completely factored example.2.2 Dependency and Boundary Model TwoDBM-2 allows different but related grammars to co-exist in a single model.
Specifically, we presupposethat all sentences are assigned to one of two classes:complete and incomplete (comp ?
{T, F}, for nowtaken as exogenous).
This model assumes that word-word (i.e., head-dependent) interactions in the twodomains are the same.
However, sentence lengths?
for which stopping probabilities are responsible?
and distributions of root words may be different.Consequently, an additional comp parameter isadded to the context of two relevant types of factors:PSTOP( ?
| dir; adj, ce, comp);and PATTACH(cr | ?
; L, comp).For example, the new stopping factors could capturethe fact that incomplete fragments ?
such as thenoun-phrases George Morton, headlines Energy andOdds and Ends, a line item c - Domestic car, dollarquantity Revenue: $3.57 billion, the time 1:11am,and the like ?
tend to be much shorter than com-plete sentences.
The new root-attachment factorscould further track that incomplete sentences gener-ally lack verbs, in contrast to other short sentences,e.g., Excerpts follow:, Are you kidding?, Yes, hedid., It?s huge., Indeed it is., I said, ?NOW?
?, ?Ab-solutely,?
he said., I am waiting., Mrs. Yeargin de-clined., McGraw-Hill was outraged., ?It happens.
?,I?m OK, Jack., Who cares?, Never mind.
and so on.All other attachment probabilities PATTACH(cd | ch; dir)remain unchanged, as in DBM-1.
In practice, compcan indicate presence of sentence-final punctuation.2.3 Dependency and Boundary Model ThreeDBM-3 adds further conditioning on punctuationcontext.
We introduce another boolean parameter,cross, which indicates the presence of interveningpunctuation between a proposed head word ch andits dependent cd.
Using this information, longer-distance punctuation-crossing arcs can be modeledseparately from other, lower-level dependencies, viaPATTACH(cd | ch; dir, cross).For instance, in Continentals believe that thestrongest growth area will be southern Europe., fourwords appear between that and will.
Conditioningon (the absence of) intervening punctuation couldhelp tell true long-distance relations from impostors.All other probabilities, PSTOP( ?
| dir; adj, ce, comp) andPATTACH(cr | ?
; L, comp), remain the same as in DBM-2.2.4 Summary of DBMs and Related ModelsHead-outward automata (Alshawi, 1996a; Alshawi,1996b; Alshawi et al2000) played a central part asgenerative models for probabilistic grammars, start-ing with their early adoption in supervised split-headconstituent parsers (Collins, 1997; Collins, 2003).Table 1 lists some parameterizations that have sincebeen used by unsupervised dependency grammar in-ducers sharing their backbone split-head process.3 Experimental Set-Up and MethodologyWe first motivate each model by analyzing the WallStreet Journal (WSJ) portion of the Penn EnglishTreebank (Marcus et al1993),3 before delving into3We converted labeled constituents into unlabeled depen-dencies using deterministic ?head-percolation?
rules (Collins,690Split-Head Dependency Grammar PATTACH (head-root) PATTACH (dependent-head) PSTOP (adjacent and not)GB (Paskin, 2001b) 1 / |{w}| d | h; dir 1 / 2DMV (Klein and Manning, 2004) cr | ?
; L cd | ch; dir ?
| dir; adj, chEVG (Headden et al2009) cr | ?
; L cd | ch; dir, adj ?
| dir; adj, chDBM-1 (?2.1) cr | ?
; L cd | ch; dir ?
| dir; adj, ceDBM-2 (?2.2) cr | ?
; L, comp cd | ch; dir ?
| dir; adj, ce, compDBM-3 (?2.3) cr | ?
; L, comp cd | ch; dir, cross ?
| dir; adj, ce, compTable 1: Parameterizations of the split-head-outward generative process used by DBMs and in previous models.grammar induction experiments.
Although motivat-ing solely from this treebank biases our discussiontowards a very specific genre of just one language,it has the advantage of allowing us to make concreteclaims that are backed up by significant statistics.In the grammar induction experiments that follow,we will test each model?s incremental contributionto accuracies empirically, across many disparate lan-guages.
We worked with all 23 (disjoint) train/testsplits from the 2006/7 CoNLL shared tasks (Buch-holz and Marsi, 2006; Nivre et al2007), span-ning 19 languages.4 For each data set, we induceda baseline grammar using the DMV.
We excludedall training sentences with more than 15 tokens tocreate a conservative bias, because in this set-up thebaseline is known to excel (Spitkovsky et al2009).Grammar inducers were initialized using (the same)uniformly-at-random chosen parse trees of trainingsentences (Cohen and Smith, 2010); thereafter, weapplied ?add one?
smoothing at every training step.To fairly compare the models under considera-tion ?
which could have quite different startingperplexities and ensuing consecutive relative like-lihoods ?
we experimented with two terminationstrategies.
In one case, we blindly ran each learnerthrough 40 steps of inside-outside re-estimation, ig-noring any convergence criteria; in the other case,we ran until numerical convergence of soft EM?s ob-jective function or until the likelihood of resultingViterbi parse trees suffered ?
an ?early-stopping la-teen EM?
strategy (Spitkovsky et al2011a, ?2.3).We evaluated against all sentences of the blind testsets (except one 145-token item in Arabic ?07 data).Table 2 shows experimental results, averaged over1999), discarding any empty nodes, etc., as is standard practice.4We did not test on WSJ data because such evaluation wouldnot be blind, as parse trees from the PTB are our motivating ex-amples; instead, performance on WSJ serves as a strong base-line in a separate study (Spitkovsky et al2012a): bootstrappingof DBMs from mostly incomplete inter-punctuation fragments.all 19 languages, for the DMV baselines and DBM-1and 2.
We did not test DBM-3 in this set-up becausemost sentence-internal punctuation occurs in longersentences; instead, DBM-3 will be tested later (see?7), using most sentences,5 in the final training stepof a curriculum strategy (Bengio et al2009) that wewill propose for DBMs.
For the three models testedon shorter inputs (up to 15 tokens) both terminatingcriteria exhibited the same trend; lateen EM consis-tently scored slightly higher than 40 EM iterations.Termination Criterion DMV DBM-1 DBM-240 steps of EM 33.5 38.8 40.7early-stopping lateen EM 34.0 39.0 40.9Table 2: Directed dependency accuracies, averaged overall 2006/7 CoNLL evaluation sets (all sentences), for theDMV and two new dependency-and-boundary grammarinducers (DBM-1,2) ?
using two termination strategies.64 Dependency and Boundary Model OneThe primary difference between DBM-1 and tradi-tional models, such as the DMV, is that DBM-1 con-ditions non-adjacent stopping decisions on the iden-tities of fringe words in partial yields (see ?2.1).4.1 Analytical MotivationTreebank data suggests that the class of the fringeword ?
its part-of-speech, ce ?
is a better predic-tor of (non-adjacent) stopping decisions, in a givendirection dir, than the head?s own class ch.
A statis-tical analysis of logistic regressions fitted to the datashows that the (ch, dir) predictor explains only about7% of the total variation (see Table 3).
This seemslow, although it is much better compared to directionalone (which explains less than 2%) and slightly bet-ter than using the (current) number of the head?s de-5Results for DBM-3 ?
given only standard input sentences,up to length fifteen ?
would be nearly identical to DBM-2?s.6We down-weighed the four languages appearing in bothCoNLL years (see Table 8) by 50% in all reported averages.691Non-Adjacent Stop Predictor R2adj AICc(dir) 0.0149 1,120,200(n, dir) 0.0726 1,049,175(ch, dir) 0.0728 1,047,157(ce, dir) 0.2361 904,102.4(ch, ce, dir) 0.3320 789,594.3Table 3: Coefficients of determination (R2) and Akaikeinformation criteria (AIC), both adjusted for the numberof parameters, for several single-predictor logistic modelsof non-adjacent stops, given direction dir; ch is the classof the head, n is its number of descendants (so far) to thatside, and ce represents the farthest descendant (the edge).scendants on that side, n, instead of the head?s class.In contrast, using ce in place of ch boosts explanatorypower to 24%, keeping the number of parameters thesame.
If one were willing to roughly square the sizeof the model, explanatory power could be improvedfurther, to 33% (see Table 3), using both ce and ch.Fringe boundaries thus appear to be informativeeven in the supervised case, which is not surprising,since using just one probability factor (and its com-plement) to generate very short (geometric coin-flip)sequences is a recipe for high entropy.
But as sug-gested earlier, fringes should be extra attractive inunsupervised settings because yields are observable,whereas heads almost always remain hidden.
More-over, every sentence exposes two true edges (Ha?nig,2010): integrated over many sample sentence begin-nings and ends, cumulative knowledge about suchmarkers can guide a grammar inducer inside long in-puts, where structure is murky.
Table 4 shows distri-butions of all part-of-speech (POS) tags in the tree-bank versus in sentence-initial, sentence-final andsentence-root positions.
WSJ often leads with deter-miners, proper nouns, prepositions and pronouns ?all good candidates for starting English phrases; andits sentences usually end with various noun types,again consistent with our running example.4.2 Experimental ResultsTable 2 shows DBM-1 to be substantially more ac-curate than the DMV, on average: 38.8 versus 33.5%after 40 steps of EM.7 Lateen termination improvedboth models?
accuracies slightly, to 39.0 and 34.0%,respectively, with DBM-1 scoring five points higher.7DBM-1?s 39% average accuracy with uniform-at-randominitialization is two points above DMV?s scores with the ?ad-hoc harmonic?
strategy, 37% (Spitkovsky et al2011a, Table 5).% of All First Last Sent.
Frag.POS Tokens Tokens Tokens Roots RootsNN 15.94 4.31 36.67 0.10 23.40IN 11.85 13.54 0.57 0.24 4.33NNP 11.09 20.49 12.85 0.02 32.02DT 9.84 23.34 0.34 0.00 0.04JJ 7.32 4.33 3.74 0.01 1.15NNS 7.19 4.49 20.64 0.15 17.12CD 4.37 1.29 6.92 0.00 3.27RB 3.71 5.96 3.88 0.00 1.50VBD 3.65 0.09 3.52 46.65 0.93VB 3.17 0.44 1.67 0.48 6.81CC 2.86 5.93 0.00 0.00 0.00TO 2.67 0.37 0.05 0.02 0.44VBZ 2.57 0.17 1.65 28.31 0.93VBN 2.42 0.61 2.57 0.65 1.28PRP 2.08 9.04 1.34 0.00 0.00VBG 1.77 1.26 0.64 0.10 0.97VBP 1.50 0.05 0.61 14.33 0.71MD 1.17 0.07 0.05 8.88 0.57POS 1.05 0.00 0.11 0.01 0.04PRP$ 1.00 0.90 0.00 0.00 0.00WDT 0.52 0.08 0.00 0.01 0.13JJR 0.39 0.18 0.43 0.00 0.09RP 0.32 0.00 0.42 0.00 0.00NNPS 0.30 0.20 0.56 0.00 2.96WP 0.28 0.42 0.01 0.01 0.04WRB 0.26 0.78 0.02 0.01 0.31JJS 0.23 0.27 0.06 0.00 0.00RBR 0.21 0.20 0.54 0.00 0.04EX 0.10 0.75 0.00 0.00 0.00RBS 0.05 0.06 0.01 0.00 0.00PDT 0.04 0.08 0.00 0.00 0.00FW 0.03 0.01 0.05 0.00 0.09WP$ 0.02 0.00 0.00 0.00 0.00UH 0.01 0.08 0.05 0.00 0.62SYM 0.01 0.11 0.01 0.00 0.18LS 0.01 0.09 0.00 0.00 0.00Table 4: Empirical distributions for non-punctuation part-of-speech tags in WSJ, ordered by overall frequency, aswell as distributions for sentence boundaries and for theroots of complete and incomplete sentences.
(A uniformdistribution would have 1/36 = 2.7% for all POS-tags.)?1?
?x?pxqx All First Last Sent.
Frag.Uniform 0.48 0.58 0.64 0.79 0.65All 0.35 0.40 0.79 0.42First 0.59 0.94 0.57Last 0.83 0.29Sent.
0.86Table 5: A distance matrix for all pairs of probability dis-tributions over POS-tags shown in Table 4 and the uni-form distribution; the BC- (or Hellinger) distance (Bhat-tacharyya, 1943; Nikulin, 2002) between discrete distri-butions p and q (over x ?
X ) ranges from zero (iff p = q)to one (iff p ?
q = 0, i.e., when they do not overlap at all).6925 10 15 20 25 30 35 40 45 50 55 60 65 70 752505007501,0001,2501,5001,7502,000(Box-and-whiskers quartile diagrams.
)13717761142027171lDistributions of Sentence Lengths (l) in WSJFigure 3: Histograms of lengths (in tokens) for 2,261 non-clausal fragments (red) and other sentences (blue) in WSJ.5 Dependency and Boundary Model TwoDBM-2 adapts DBM-1 grammars to two classesof inputs (complete sentences and incomplete frag-ments) by forking off new, separate multinomials forstopping decisions and root-distributions (see ?2.2).5.1 Analytical MotivationUnrepresentative short sentences ?
such as head-lines and titles ?
are common in news-style dataand pose a known nuisance to grammar inducers.Previous research sometimes took radical measuresto combat the problem: for example, Gillenwateret al2009) excluded all sentences with three orfewer tokens from their experiments; and Marec?ekand Zabokrtsky?
(2011) enforced an ?anti-noun-root?policy to steer their Gibbs sampler away from theundercurrents caused by the many short noun-phrasefragments (among sentences up to length 15, inCzech data).
We refer to such snippets of text as?incomplete sentences?
and focus our study of WSJon non-clausal data (as signaled by top-level con-stituent annotations whose first character is not S).8Table 4 shows that roots of incomplete sentences,which are dominated by nouns, barely resemble theother roots, drawn from more traditional verb andmodal types.
In fact, these two empirical root dis-tributions are more distant from one another than ei-ther is from the uniform distribution, in the space ofdiscrete probability distributions over POS-tags (seeTable 5).
Of the distributions we considered, onlysentence boundaries are as or more different from8I.e., separating top-level types {S, SINV, SBARQ, SQ, SBAR}from the rest (ordered by frequency): {NP, FRAG, X, PP, .
.
.}.
(complete) roots, suggesting that heads of fragmentstoo may warrant their own multinomial in the model.Further, incomplete sentences are uncharacteris-tically short (see Figure 3).
It is this property thatmakes them particularly treacherous to grammar in-ducers, since by offering few options of root posi-tions they increase the chances that a learner willincorrectly induce nouns to be heads.
Given that ex-pected lengths are directly related to stopping deci-sions, it could make sense to also model the stoppingprobabilities of incomplete sentences separately.5.2 Experimental ResultsSince it is not possible to consult parse trees duringgrammar induction (to check whether an input sen-tence is clausal), we opted for a proxy: presence ofsentence-final punctuation.
Using punctuation to di-vide input sentences into two groups, DBM-2 scoredhigher: 40.9, up from 39.0% accuracy (see Table 2).After evaluating these multi-lingual experiments,we checked how well our proxy corresponds to ac-tual clausal sentences in WSJ.
Table 6 shows the bi-nary confusion matrix having a fairly low (but posi-tive) Pearson correlation coefficient.
False positivesr?
?
0.31 Clausal non-Clausal TotalPunctuation 46,829 1,936 48,765no Punctuation 118 325 443Total 46,947 2,261 49,208Table 6: A contingency table for clausal sentences andtrailing punctuation in WSJ; the mean square contingencycoefficient r?
signifies a low degree of correlation.
(Fortwo binary variables, r?
is equivalent to Karl Pearson?sbetter-known product-moment correlation coefficient, ?.
)693include parenthesized expressions that are markedas noun-phrases, such as (See related story: ?FedReady to Inject Big Funds?
: WSJ Oct. 16, 1989);false negatives can be headlines having a main verb,e.g., Population Drain Ends For Midwestern States.Thus, our proxy is not perfect but seems to be toler-able in practice.
We suspect that identities of punc-tuation marks (Collins, 2003, Footnote 13) ?
bothsentence-final and sentence-initial ?
could be of ex-tra assistance in grammar induction, specifically forgrouping imperatives, questions, and so forth.6 Dependency and Boundary Model ThreeDBM-3 exploits sentence-internal punctuation con-texts by modeling punctuation-crossing dependencyarcs separately from other attachments (see ?2.3).6.1 Analytical MotivationMany common syntactic relations, such as betweena determiner and a noun, are unlikely to hold overlong distances.
(In fact, 45% of all head-percolateddependencies in WSJ are between adjacent words.
)However, some common constructions are more re-mote: e.g., subordinating conjunctions are, on av-erage, 4.8 tokens away from their dependent modalverbs.
Sometimes longer-distance dependencies canbe vetted using sentence-internal punctuation marks.It happens that the presence of punctuation be-tween such conjunction (IN) and verb (MD) typesserves as a clue that they are not connected (see Ta-ble 7a); by contrast, a simpler cue ?
whether thesewords are adjacent ?
is, in this case, hardly of anyuse (see Table 7b).
Conditioning on crossing punc-tuation could be of help then, playing a role simi-lar to that of comma-counting (Collins, 1997, ?2.1)?
and ?verb intervening?
(Bikel, 2004, ?5.1) ?
inearly head-outward models for supervised parsing.a) r?
?
?0.40 Attached not Attached TotalPunctuation 337 7,645 7,982no Punctuation 2,144 4,040 6,184Total 2,481 11,685 14,166non-Adjacent 2,478 11,673 14,151Adjacent 3 12 15b) r?
?
+0.00 Attached not Attached TotalTable 7: Contingency tables for IN right-attaching MD,among closest ordered pairs of these tokens in WSJ sen-tences with punctuation, versus: (a) presence of interven-ing punctuation; and (b) presence of intermediate words.6.2 Experimental Results PostponedAs we mentioned earlier (see ?3), there is little pointin testing DBM-3 with shorter sentences, since mostsentence-internal punctuation occurs in longer in-puts.
Instead, we will test this model in a final step ofa staged training strategy, with more data (see ?7.3).7 A Curriculum Strategy for DBMsWe propose to train up to DBM-3 iteratively ?by beginning with DBM-1 and gradually increasingmodel complexity through DBM-2, drawing on theintuitions of IBM translation models 1?4 (Brown etal., 1993).
Instead of using sentences of up to 15 to-kens, as in all previous experiments (?4?5), we willnow make use of nearly all available training data:up to length 45 (out of concern for efficiency), dur-ing later stages.
In the first stage, however, we willuse only a subset of the data with DBM-1, in a pro-cess sometimes called curriculum learning (Bengioet al2009; Krueger and Dayan, 2009, inter alia).Our grammar inducers will thus be ?starting small?in both senses suggested by Elman (1993): simulta-neously scaffolding on model- and data-complexity.7.1 Scaffolding Stage #1: DBM-1We begin by training DBM-1 on sentences with-out sentence-internal punctuation but with at leastone trailing punctuation mark.
Our goal is to avoid,when possible, overly specific arbitrary parameterslike the ?15 tokens or less?
threshold used to selecttraining sentences.
Unlike DBM-2 and 3, DBM-1does not model punctuation or sentence fragments,so we instead explicitly restrict its attention to thiscleaner subset of the training data, which takes ad-vantage of the fact that punctuation may generallycorrelate with sentence complexity (Frank, 2000).9Aside from input sentence selection, our exper-imental set-up here remained identical to previoustraining of DBMs (?4?5).
Using this new input data,DBM-1 averaged 40.7% accuracy (see Table 8).This is slightly higher than the 39.0% when usingsentences up to length 15, suggesting that our heuris-tic for clean, simple sentences may be a useful one.9More incremental training strategies are the subject of anunpublished companion manuscript (Spitkovsky et al2012a).694Directed Dependency Accuracies for: Best of State-of-the-Art SystemsCoNLL Year this Work (@10) Monolingual; POS- Cross-Lingual& Language DMV DBM-1 DBM-2 DBM-3 +inference (i) Agnostic (ii) Identified (iii) TransferArabic 2006 12.9 10.6 11.0 11.1 10.9 (34.5) 33.4 SCAJ6 ?
50.2 Sbg?7 36.6 43.9 44.0 44.4 44.9 (48.8) 55.6 RF 54.6 RFH1 ?Basque ?7 32.7 34.1 33.0 32.7 33.3 (36.5) 43.6 SCAJ5 34.7 MZNR ?Bulgarian ?7 24.7 59.4 63.6 64.6 65.2 (70.4) 44.3 SCAJ5 53.9 RFH1&2 70.3 SptCatalan ?7 41.1 61.3 61.1 61.1 62.1 (78.1) 63.8 SCAJ5 56.3 MZNR ?Chinese ?6 50.4 63.1 63.0 63.2 63.2 (65.7) 63.6 SCAJ6 ?
?
?7 55.3 56.8 57.0 57.1 57.0 (59.8) 58.5 SCAJ6 34.6 MZNR ?Czech ?6 31.5 51.3 52.8 53.0 55.1 (61.8) 50.5 SCAJ5 ?
?
?7 34.5 50.5 51.2 53.3 54.2 (67.3) 49.8 SCAJ5 42.4 RFH1&2 ?Danish ?6 22.4 21.3 19.9 21.8 22.2 (27.4) 46.0 RF 53.1 RFH1&2 56.5 SarDutch ?6 44.9 45.9 46.5 46.0 46.6 (48.6) 32.5 SCAJ5 48.8 RFH1&2 65.7 MPHm:pEnglish ?7 32.3 29.2 28.6 29.0 29.6 (51.4) 50.3 SAJ 23.8 MZNR 45.7 MPHelGerman ?6 27.7 36.3 37.9 38.4 39.1 (52.1) 33.5 SCAJ5 21.8 MZNR 56.7 MPHm:dGreek ?6 36.3 28.1 26.1 26.1 26.9 (36.8) 39.0 MZ 33.4 MZNR 65.1 MPHm:pHungarian ?7 23.6 43.2 52.1 57.4 58.2 (68.4) 48.0 MZ 48.1 MZNR ?Italian ?7 25.5 41.7 39.8 39.9 40.7 (41.8) 57.5 MZ 60.6 MZNR 69.1 MPHptJapanese ?6 42.2 22.8 22.7 22.7 22.7 (32.5) 56.6 SCAJ5 53.5 MZNR ?Portuguese ?6 37.1 68.9 72.3 71.1 72.4 (80.6) 43.2 MZ 55.8 RFH1&2 76.9 SbgSlovenian ?6 33.4 30.4 33.0 34.1 35.2 (36.8) 33.6 SCAJ5 34.6 MZNR ?Spanish ?6 22.0 25.0 26.7 27.1 28.2 (51.8) 53.0 MZ 54.6 MZNR 68.4 MPHitSwedish ?6 30.7 48.6 50.3 50.0 50.7 (63.2) 50.0 SCAJ6 34.3 RFH1&2 68.0 MPHm:pTurkish ?6 43.4 32.9 33.7 33.4 34.4 (38.1) 40.9 SAJ 61.3 RFH1 ?
?7 58.5 44.6 44.2 43.7 44.8 (44.4) 48.8 SCAJ6 ?
?Average: 33.6 40.7 41.7 42.2 42.9 (51.9) 38.2 SCAJ6 (best average, not an average of bests)Table 8: Average accuracies over CoNLL evaluation sets (all sentences), for the DMV baseline and DBM1?3 trainedwith a curriculum strategy, and state-of-the-art results for systems that: (i) are also POS-agnostic and monolingual,including SCAJ (Spitkovsky et al2011a, Tables 5?6) and SAJ (Spitkovsky et al2011b); (ii) rely on gold POS-tagidentities to discourage noun roots (Marec?ek and Zabokrtsky?, 2011, MZ) or to encourage verbs (Rasooli and Faili,2012, RF); and (iii) transfer delexicalized parsers (S?gaard, 2011a, S) from resource-rich languages with transla-tions (McDonald et al2011, MPH).
DMV and DBM-1 trained on simple sentences, from uniform; DBM-2 and 3trained on most sentences, from DBM-1 and 2, respectively; +inference is DBM-3 with punctuation constraints.7.2 Scaffolding Stage #2: DBM-2?
DBM-1Next, we trained on all sentences up to length 45.Since these inputs are punctuation-rich, in both re-maining stages we used the constrained Viterbi EMset-up suggested by Spitkovsky et al2011b) in-stead of plain soft EM; we employ an early termina-tion strategy, quitting hard EM as soon as soft EM?sobjective suffers (Spitkovsky et al2011a).
Punc-tuation was converted into Viterbi-decoding con-straints during training using the so-called loosemethod, which stipulates that all words in an inter-punctuation fragment must be dominated by a single(head) word, also from that fragment ?
with onlythese head words allowed to attach the head wordsof other fragments, across punctuation boundaries.To adapt to full data, we initialized DBM-2 usingViterbi parses from the previous stage (?7.1), plusuniformly-at-random chosen dependency trees forthe new complex and incomplete sentences, subjectto punctuation-induced constraints.
This approachimproved parsing accuracies to 41.7% (see Table 8).7.3 Scaffolding Stage #3: DBM-3?
DBM-2Next, we repeated the training process of the pre-vious stage (?7.2) using DBM-3.
To initialize thismodel, we combined the final instance of DBM-2with uniform multinomials for punctuation-crossingattachment probabilities (see ?2.3).
As a result, av-erage performance improved to 42.2% (see Table 8).Lastly, we applied punctuation constraints also ininference.
Here we used the sprawl method ?
amore relaxed approach than in training, allowing ar-bitrary words to attach inter-punctuation fragments(provided that each entire fragment still be derived695by one of its words) ?
as suggested by Spitkovskyet al2011b).
This technique increased DBM-3?saverage accuracy to 42.9% (see Table 8).
Our fi-nal result substantially improves over the baseline?s33.6% and compares favorably to previous work.108 Discussion and the State-of-the-ArtDBMs come from a long line of head-outward mod-els for dependency grammar induction yet their gen-erative processes feature important novelties.
Oneis conditioning on more observable state ?
specifi-cally, the left and right end words of a phrase beingconstructed ?
than in previous work.
Another is al-lowing multiple grammars ?
e.g., of complete andincomplete sentences ?
to coexist in a single model.These improvements could make DBMs quick-and-easy to bootstrap directly from any available partialbracketings (Pereira and Schabes, 1992), for exam-ple capitalized phrases (Spitkovsky et al2012b).The second part of our work ?
the use of a cur-riculum strategy to train DBM-1 through 3 ?
elim-inates having to know tuned cut-offs, such as sen-tences with up to a predetermined number of tokens.Although this approach adds some complexity, wechose conservatively, to avoid overfitting settingsof sentence length, convergence criteria, etc.
: stageone?s data is dictated by DBM-1 (which ignorespunctuation); subsequent stages initialize additionalpieces uniformly: uniform-at-random parses for newdata and uniform multinomials for new parameters.Even without curriculum learning ?
trained withvanilla EM ?
DBM-2 and 1 are already strong.Further boosts to accuracy could come from em-ploying more sophisticated optimization algorithms,e.g., better EM (Samdani et al2012), constrainedGibbs sampling (Marec?ek and Zabokrtsky?, 2011) orlocally-normalized features (Berg-Kirkpatrick et al2010).
Other orthogonal dependency grammar in-duction techniques ?
including ones based on uni-versal rules (Naseem et al2010) ?
may also ben-efit in combination with DBMs.
Direct comparisonsto previous work require some care, however, asthere are several classes of systems that make dif-ferent assumptions about training data (see Table 8).10Note that DBM-1?s 39% average accuracy with standardtraining (see Table 2) was already nearly a full point higher thanthat of any single previous best system (SCAJ6 ?
see Table 8).8.1 Monolingual POS-Agnostic InducersThe first type of grammar inducers, including ourown approach, uses standard training and test datasets for each language, with gold part-of-speech tagsas anonymized word classes.
For the purposes ofthis discussion, we also include in this group trans-ductive learners that may train on data from the testsets.
Our DBM-3 (decoded with punctuation con-straints) does well among such systems ?
for whichaccuracies on all sentence lengths of the evaluationsets are reported ?
attaining highest scores for 8 of19 languages; the DMV baseline is still state-of-the-art for one language; and the remaining 10 bests aresplit among five other recent systems (see Table 8).11Half of the five came from various lateen EM strate-gies (Spitkovsky et al2011a) for escaping and/oravoiding local optima.
These heuristics are compat-ible with how we trained our DBMs and could po-tentially provide further improvement to accuracies.Overall, the final scores of DBM-3 were better, onaverage, than those of any other single system: 42.9versus 38.2% (Spitkovsky et al2011a, Table 6).The progression of scores for DBM-1 through 3without using punctuation constraints in inference?
40.7, 41.7 and 42.2% ?
fell entirely above thisprevious state-of-the-art result as well; the DMVbaseline ?
also trained on sentences without inter-nal but with final punctuation ?
averaged 33.6%.8.2 Monolingual POS-Identified InducersThe second class of techniques assumes knowledgeabout identities of part-of-speech tags (Naseem etal., 2010), i.e., which word tokens are verbs, whichones are nouns, etc.
Such grammar inducers gener-ally do better than the first kind ?
e.g., by encour-aging verbocentricity (Gimpel and Smith, 2011) ?though even here our results appear to be compet-itive.
In fact, to our surprise, only in 5 of 19 lan-guages a ?POS-identified?
system performed betterthan all of the ?POS-agnostic?
ones (see Table 8).8.3 Multi-Lingual Semi-Supervised ParsersThe final broad class of related algorithms we con-sidered extends beyond monolingual data and uses11For Turkish ?06, the ?right-attach?
baseline outperformseven the DMV, at 65.4% (Rasooli and Faili, 2012, Table 1); animportant difference between 2006 and 2007 CoNLL data setshas to do with segmentation of morphologically-rich languages.696both identities of POS-tags and/or parallel bitextsto transfer (supervised) delexicalized parsers acrosslanguages.
Parser projection is by far the most suc-cessful approach to date and we hope that it toomay stand to gain from our modeling improvements.Of the 10 languages for which we found resultsin the literature, transferred parsers underperformedthe grammar inducers in only one case: on En-glish (see Table 8).
The unsupervised system thatperformed better used a special ?weighted?
initial-izer (Spitkovsky et al2011b, ?3.1) that worked wellfor English (but less so for many other languages).DBMs may be able to improve initialization.
Forexample, modeling of incomplete sentences couldhelp in incremental initialization strategies like babysteps (Spitkovsky et al2009), which are likely sen-sitive to the proverbial ?bum steer?
from unrepresen-tative short fragments, pace Tu and Honavar (2011).8.4 Miscellaneous Systems on Short SentencesSeveral recent systems (Cohen et al2011; S?gaard,2011b; Naseem et al2010; Gillenwater et al2010;Berg-Kirkpatrick and Klein, 2010, inter alia) are ab-sent from Table 8 because they do not report perfor-mance for all sentence lengths.
To facilitate com-parison with this body of important previous work,we also tabulated final accuracies for the ?up-to-tenwords?
task under heading @10: 51.9%, on average.9 ConclusionAlthough a dependency parse for a sentence can bemapped to a constituency parse (Xia and Palmer,2001), the probabilistic models generating them usedifferent conditioning: dependency grammars focuson the relationship between arguments and heads,constituency grammars on the coherence of chunkscovered by non-terminals.
Since redundant views ofdata can make learning easier (Blum and Mitchell,1998), integrating aspects of both constituency anddependency ought to be able to help grammar in-duction.
We have shown that this insight is correct:dependency grammar inducers can gain from mod-eling boundary information that is fundamental toconstituency (i.e., phrase-structure) formalisms.DBMs are a step in the direction towards mod-eling constituent boundaries jointly with head de-pendencies.
Further steps must involve more tightlycoupling the two frameworks, as well as showingways to incorporate both kinds of information inother state-of-the art grammar induction paradigms.AcknowledgmentsWe thank Roi Reichart and Marta Recasens, for many helpfulcomments on draft versions of this paper, and Marie-Catherinede Marneffe, Roy Schwartz, Mengqiu Wang and the anonymousreviewers, for their apt recommendations.
Funded, in part, byDefense Advanced Research Projects Agency (DARPA) Ma-chine Reading Program, under Air Force Research Labora-tory (AFRL) prime contract no.
FA8750-09-C-0181.
Any opin-ions, findings, and conclusion or recommendations expressedin this material are those of the authors and do not necessarilyreflect the view of the DARPA, AFRL, or the US government.First author is grateful to Cindy Chan for her friendship andsupport over many long months leading up to this publication.ReferencesH.
Alshawi, S. Bangalore, and S. Douglas.
2000.
Learningdependency translation models as collections of finite-statehead transducers.
Computational Linguistics, 26.H.
Alshawi.
1996a.
Head automata for speech translation.
InICSLP.H.
Alshawi.
1996b.
Method and apparatus for an improvedlanguage recognition system.
US Patent 1999/5870706.J.
K. Baker.
1979.
Trainable grammars for speech recognition.In Speech Communication Papers for the 97th Meeting of theAcoustical Society of America.Y.
Bengio, J. Louradour, R. Collobert, and J. Weston.
2009.Curriculum learning.
In ICML.J.
Berant, Y.
Gross, M. Mussel, B. Sandbank, E. Ruppin, andS.
Edelman.
2006.
Boosting unsupervised grammar induc-tion by splitting complex sentences on function words.
InBUCLD.T.
Berg-Kirkpatrick and D. Klein.
2010.
Phylogenetic gram-mar induction.
In ACL.T.
Berg-Kirkpatrick, A.
Bouchard-Co?te?, J. DeNero, andD.
Klein.
2010.
Painless unsupervised learning with fea-tures.
In NAACL-HLT.A.
Bhattacharyya.
1943.
On a measure of divergence betweentwo statistical populations defined by their probability distri-butions.
BCMS, 35.D.
M. Bikel.
2004.
Intricacies of Collins?
parsing model.
Com-putational Linguistics, 30.A.
Blum and T. Mitchell.
1998.
Combining labeled and unla-beled data with co-training.
In COLT.M.
R. Brent and J. M. Siskind.
2001.
The role of exposure toisolated words in early vocabulary development.
Cognition,81.P.
F. Brown, V. J. Della Pietra, S. A. Della Pietra, and R. L. Mer-cer.
1993.
The mathematics of statistical machine transla-tion: Parameter estimation.
Computational Linguistics, 19.S.
Buchholz and E. Marsi.
2006.
CoNLL-X shared task onmultilingual dependency parsing.
In CoNLL.697G.
Carroll and E. Charniak.
1992.
Two experiments on learningprobabilistic dependency grammars from corpora.
Technicalreport, Brown University.S.
B. Cohen and N. A. Smith.
2009.
Shared logistic normal dis-tributions for soft parameter tying in unsupervised grammarinduction.
In NAACL-HLT.S.
B. Cohen and N. A. Smith.
2010.
Viterbi training for PCFGs:Hardness results and competitiveness of uniform initializa-tion.
In ACL.S.
B. Cohen, D. Das, and N. A. Smith.
2011.
Unsupervisedstructure prediction with non-parallel multilingual guidance.In EMNLP.M.
Collins.
1997.
Three generative, lexicalised models for sta-tistical parsing.
In ACL.M.
Collins.
1999.
Head-Driven Statistical Models for NaturalLanguage Parsing.
Ph.D. thesis, University of Pennsylvania.M.
Collins.
2003.
Head-driven statistical models for naturallanguage parsing.
Computational Linguistics, 29.J.
Eisner and G. Satta.
1999.
Efficient parsing for bilexicalcontext-free grammars and head-automaton grammars.
InACL.J.
M. Eisner.
1996.
An empirical comparison of probabilitymodels for dependency grammar.
Technical report, IRCS.J.
Eisner.
2000.
Bilexical grammars and their cubic-timeparsing algorithms.
In H. C. Bunt and A. Nijholt, editors,Advances in Probabilistic and Other Parsing Technologies.Kluwer Academic Publishers.J.
L. Elman.
1993.
Learning and development in neural net-works: The importance of starting small.
Cognition, 48.R.
Frank.
2000.
From regular to context-free to mildly context-sensitive tree rewriting systems: The path of child languageacquisition.
In A. Abeille?
and O. Rambow, editors, TreeAdjoining Grammars: Formalisms, Linguistic Analysis andProcessing.
CSLI Publications.J.
Gillenwater, K. Ganchev, J. Grac?a, B. Taskar, and F. Pereira.2009.
Sparsity in grammar induction.
In NIPS: Gram-mar Induction, Representation of Language and LanguageLearning.J.
Gillenwater, K. Ganchev, J. Grac?a, F. Pereira, and B. Taskar.2010.
Posterior sparsity in unsupervised dependency pars-ing.
Technical report, University of Pennsylvania.K.
Gimpel and N. A. Smith.
2011.
Concavity and initializationfor unsupervised dependency grammar induction.
Technicalreport, CMU.C.
Ha?nig.
2010.
Improvements in unsupervised co-occurrencebased parsing.
In CoNLL.W.
P. Headden, III, M. Johnson, and D. McClosky.
2009.
Im-proving unsupervised dependency parsing with richer con-texts and smoothing.
In NAACL-HLT.D.
Klein and C. D. Manning.
2004.
Corpus-based induction ofsyntactic structure: Models of dependency and constituency.In ACL.K.
A. Krueger and P. Dayan.
2009.
Flexible shaping: Howlearning in small steps helps.
Cognition, 110.M.
P. Marcus, B. Santorini, and M. A. Marcinkiewicz.
1993.Building a large annotated corpus of English: The PennTreebank.
Computational Linguistics, 19.D.
Marec?ek and Z. Zabokrtsky?.
2011.
Gibbs sampling withtreeness constraint in unsupervised dependency parsing.
InROBUS.D.
McClosky.
2008.
Modeling valence effects in unsupervisedgrammar induction.
Technical report, Brown University.R.
McDonald, S. Petrov, and K. Hall.
2011.
Multi-source trans-fer of delexicalized dependency parsers.
In EMNLP.T.
Naseem, H. Chen, R. Barzilay, and M. Johnson.
2010.
Usinguniversal linguistic knowledge to guide grammar induction.In EMNLP.M.
S. Nikulin.
2002.
Hellinger distance.
In M. Hazewinkel,editor, Encyclopaedia of Mathematics.
Kluwer AcademicPublishers.J.
Nivre, J.
Hall, S. Ku?bler, R. McDonald, J. Nilsson, S. Riedel,and D. Yuret.
2007.
The CoNLL 2007 shared task on de-pendency parsing.
In EMNLP-CoNLL.M.
A. Paskin.
2001a.
Cubic-time parsing and learning algo-rithms for grammatical bigram models.
Technical report,UCB.M.
A. Paskin.
2001b.
Grammatical bigrams.
In NIPS.F.
Pereira and Y. Schabes.
1992.
Inside-outside reestimationfrom partially bracketed corpora.
In ACL.E.
Ponvert, J. Baldridge, and K. Erk.
2010.
Simple unsuper-vised identification of low-level constituents.
In ICSC.M.
S. Rasooli and H. Faili.
2012.
Fast unsupervised depen-dency parsing with arc-standard transitions.
In ROBUS-UNSUP.R.
Samdani, M.-W. Chang, and D. Roth.
2012.
Unified expec-tation maximization.
In NAACL-HLT.Y.
Seginer.
2007.
Learning Syntactic Structure.
Ph.D. thesis,University of Amsterdam.A.
S?gaard.
2011a.
Data point selection for cross-languageadaptation of dependency parsers.
In ACL.A.
S?gaard.
2011b.
From ranked words to dependency trees:two-stage unsupervised non-projective dependency parsing.In TextGraphs.V.
I. Spitkovsky, H. Alshawi, and D. Jurafsky.
2009.
BabySteps: How ?Less is More?
in unsupervised dependencyparsing.
In NIPS: Grammar Induction, Representation ofLanguage and Language Learning.V.
I. Spitkovsky, H. Alshawi, and D. Jurafsky.
2011a.
LateenEM: Unsupervised training with multiple objectives, appliedto dependency grammar induction.
In EMNLP.V.
I. Spitkovsky, H. Alshawi, and D. Jurafsky.
2011b.
Punctu-ation: Making a point in unsupervised dependency parsing.In CoNLL.V.
I. Spitkovsky, H. Alshawi, and D. Jurafsky.
2012a.
Boot-strapping dependency grammar inducers from incompletesentence fragments via austere models ?
the ?wabi-sabi?
ofunsupervised parsing.
In submission.V.
I. Spitkovsky, H. Alshawi, and D. Jurafsky.
2012b.
Capi-talization cues improve dependency grammar induction.
InWILS.K.
Tu and V. Honavar.
2011.
On the utility of curricula inunsupervised learning of probabilistic grammars.
In IJCAI.F.
Xia and M. Palmer.
2001.
Converting dependency structuresto phrase structures.
In HLT.698
