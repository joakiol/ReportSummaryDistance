Proceedings of the Human Language Technology Conference of the North American Chapter of the ACL, pages 128?135,New York, June 2006. c?2006 Association for Computational LinguisticsUnknown word sense detection as outlier detectionKatrin ErkComputational LinguisticsSaarland UniversitySaarbru?cken, Germanyerk@coli.uni-sb.deAbstractWe address the problem of unknown wordsense detection: the identification of cor-pus occurrences that are not covered bya given sense inventory.
We model thisas an instance of outlier detection, usinga simple nearest neighbor-based approachto measuring the resemblance of a newitem to a training set.
In combination witha method that alleviates data sparseness bysharing training data across lemmas, theapproach achieves a precision of 0.77 andrecall of 0.82.1 IntroductionIf a system has seen only positive examples, howdoes it recognize a negative example?
This isthe problem addressed by outlier detection, alsocalled novelty detection1 (Markou and Singh, 2003a;Markou and Singh, 2003b; Marsland, 2003): to de-tect novel or unknown items that differ from all theseen training data.
Outlier detection approaches typ-ically derive some model of ?normal?
objects fromthe training set and use a distance measure and athreshold to detect abnormal items.In this paper, we apply outlier detection tech-niques to the task of unknown sense detection: theidentification of corpus occurrences that are not cov-ered by a given sense inventory.
The training set1The term novelty detection is also used for the distinctionof novel and repeated information in information retrieval, adifferent if related topic.Figure 1: Wrong assignment due to missing sense:from the Hound of the Baskervilles, Ch.
14against which new occurrences are compared willconsist of sense-annotated text.Unknown sense detection is related to word sensedisambiguation (WSD) and to word sense discrim-ination (Schu?tze, 1998), but differs from both.
InWSD all senses are assumed known, and the task isto select one of them, while in unknown sense detec-tion the task is to decide whether a given occurrencematches any of the known senses or none of them,and all training instances, regardless of the sense towhich they belong, are modeled as one group ofknown data.
Unknown sense detection also differsfrom word sense discrimination, where no sense in-ventory is given and the task is to group occurrencesinto senses.
In unknown sense detection the modelrespects the given word senses.The main motivation for this study comes fromshallow semantic parsing, by which we mean a com-bination of WSD and the automatic assignment of128semantic roles to free text.
In cases where a senseis missing from the inventory, WSD will wronglyassign one of the existing senses.
Figure 1 showsan example, a sentence from the Hound of theBaskervilles, analyzed by the SHALMANESER (Erkand Pado, 2006) shallow semantic parser.
The anal-ysis is based on FrameNet (Baker et al, 1998), aresource that lists senses and semantic roles for En-glish expressions.
FrameNet is lacking a sense of?expectation?
or ?being mentally prepared?
for theverb prepare, so prepared has been assigned thesense COOKING CREATION, a possible but improb-able analysis2.
Such erroneous labels can be fa-tal when further processing builds on the results ofshallow semantic parsing, e.g.
for drawing infer-ences.
Unknown sense detection can prevent suchmistakes.All sense inventories face the problem of missingsenses, either because of their small overall size (asis the case for some non-English WordNets) or whenthey encounter domain-specific senses.
Our studywill be evaluated on FrameNet because of our mainaim of improving shallow semantic parsing, but themethod we propose is applicable to any sense inven-tory that has annotated data; in particular, it is alsoapplicable to WordNet.In this paper we model unknown sense detec-tion as outlier detection, using a simple NearestNeighbor-based method (Tax and Duin, 2000) thatcompares the local probability density at each testitem with that of its nearest training item.To our knowledge, there exists no other approachto date to the problem of detecting unknown senses.There are, however, approaches to the complemen-tary problem of determining the closest known sensefor unknown words (Widdows, 2003; Curran, 2005;Burchardt et al, 2005), which can be viewed as thelogical next step after unknown sense detection.Plan of the paper.
After a brief sketch ofFrameNet in Section 2, we describe the experimen-tal setup used throughout this paper in Section 3.Section 4 tests whether a very simple model sufficesfor detecting unknown senses: a threshold on confi-dence scores returned by the SHALMANESER WSD2Unfortunately, the semantic roles have been mis-assignedby the system.
The word I should fill the FOOD role, while fora hound could be assigned the optional RECEIVER role.system.
The result is that recall is much too low.Section 5 introduces the NN-based outlier detectionapproach that we use in section 6 for unknown sensedetection, with better results than in the first experi-ment but still low recall.
Section 7 repeats the exper-iment of section 6 with added training data, makinguse of the fact that one semantic class in FrameNettypically pertains to several lemmas and achieving amarked improvement in results.2 FrameNetFrame Semantics (Fillmore, 1982) models the mean-ings of a word or expression by reference toframes which describe the background and situa-tional knowledge necessary for understanding whatthe predicate is ?about?.
Each frame provides itsspecific set of semantic roles.The Berkeley FrameNet project (Baker et al,1998) is building a semantic lexicon for English de-scribing the frames and linking them to the wordsand expressions that can evoke them.
These canbe verbs as well as nouns, adjectives, preposi-tions, adverbs, and multiword expressions.
Framesare linked by IS-A and other relations.
Currently,FrameNet contains 609 frames with 8,755 lemma-frame pairs, of which 5,308 are exemplified in an-notated sentences from the British National Corpus.The annotation comprises 133,846 sentences.As FrameNet is a growing resource, many lem-mas are still lacking senses, and many senses are stilllacking annotation.
This is problematic for the useof FrameNet analyses as a basis for inferences overtext, as e.g.
in Tatu and Moldovan (2005).For example, the verb prepare from Figure 1 isassociated with the framesCOOKING CREATION: prepare foodACTIVITY PREPARE: get ready for an activityACTIVITY READY STATE: be ready for an activityWILLINGNESS: be willingof which only the COOKING CREATION sense hasbeen annotated.
The sense in Figure 1 is not cov-ered yet: ACTIVITY READY STATE would be moreappropriate than COOKING CREATION, but still notoptimal, since the sentence refers to a mental staterather than the preparation of an activity.1293 Experimental setup and dataExperimental setup.
To evaluate an unknownsense detection system, we need occurrences that areguaranteed not to belong to any of the seen senses.To that end we use sense-annotated data, in our casethe FrameNet annotated sentences, simulating un-known senses by designating one sense of each am-biguous lemma as unknown.
All occurrences of thatsense are placed in the test set, while occurrencesof all other senses are split randomly between train-ing and test set, using 5-fold cross-validation.
Werepeat the experiment with each of the senses of anambiguous lemma playing the part of the unknownsense once.
Viewing each cross-validation run foreach unknown sense as a separate experiment, wethen report precision and recall averaged over un-known senses and cross-validation runs.It may seem questionable that in this experimen-tal setup, the unknown sense occurrences of eachlemma all belong to the same sense.
However, thisdoes not bias the experiment since none of the mod-els we study take advantage of the shape of the testset in any way.
Rather, each test item is classified in-dividually, without recourse to the other test items.Data.
All experiments in this paper were per-formed on the FrameNet 1.2 annotated data per-taining to ambiguous lemmas.
After removal ofinstances that were annotated with more than onesense, we obtain 26,496 annotated sentences for the1,031 ambiguous lemmas.
They were parsed withMinipar (Lin, 1993); named entities were computedusing Heart of Gold (Callmeier et al, 2004).4 Experiment 1: WSD confidence scoresfor unknown sense detectionIn this section we test a very simple model of un-known sense detection: Classifiers often return aconfidence score along with the assigned label.
Wewill try to detect unknown senses by a thresholdon confidence scores, declaring anything below thethreshold as unknown.
Note that this method canonly be applied to lemmas that have more than onesense, since for single-sense lemmas the system willalways return the maximum confidence score.Data.
While the approach that we follow in thissection is applicable to all lemmas with at least twoher and upwardsSheShewavehand outwardss subj obj modgen punc conj(1): subj, obj, mod (since s and subj corefer,we use only one of them)(2): she, hand, outwards(3): subj-she, obj-hand, mod-outwards(4): mod-obj-subjFigure 2: Sample Minipar parse and extracted gram-matical function featuressenses, we need lemmas with at least three sensesto evaluate it: One of the senses of each lemma istreated as unknown, which for lemmas with three ormore senses leaves at least two senses for the train-ing set.
This reduces our data set to 125 lemmaswith 7,435 annotated sentences.Modeling.
We test whether the WSD system builtinto SHALMANESER (Erk, 2005) can distinguishknown sense items from unknown sense items reli-ably by its confidence scores.
The system extractsa rich feature set, which forms the basis of all threeexperiments in this paper:?
a bag-of-words context, with a window size ofone sentence;?
bi- and trigrams centered on the target word;?
grammatical function information: for each de-pendent of the target, (1) its function label, (2)its headword, and (3) a combination of both areused as features.
(4) The concatenation of allfunction labels constitutes another feature.
ForPPs, function labels are extended by the prepo-sition.
As an example, Figure 2 shows a BNCsentence and its grammatical function features.?
for verb targets, the target voice.The feature set is based on Florian et al (2002) butcontains additional syntax-related features.
Eachword-related feature is represented as four featuresfor word, lemma, part of speech, and named entity.SHALMANESER trains one Naive Bayes classifierper lemma to be disambiguated.
For this experiment,130?
Precision Recall0.5 0.6524 (?
0.115) 0.0011 (?
0.0004)0.75 0.7855 (?
0.0086) 0.0527 (?
0.0013)0.9 0.7855 (?
0.0093) 0.1006 (?
0.0021)0.98 0.7847 (?
0.0073) 0.1744 (?
0.0025)Table 1: Experiment 1: Results for label unknownsense, WSD confidence level approach.
?
: confi-dence threshold.
?
: std.
dev.all system parameters were set to their default set-tings.
To detect unknown senses building on thisWSD system, we use a fixed confidence thresholdand label all items below the threshold as unknown.Results and discussion.
Table 1 shows precisionand recall for labeling instances as unknown usingdifferent confidence thresholds ?, averaged over un-known senses and 5-fold cross-validation3.
We seethat while the precision of this method is acceptableat 0.74 to 0.765, recall is extremely low, i.e.
almostno items were labeled unknown, even at a thresholdof 0.98.
However, SHALMANESER has very highconfidence values overall: Only 14.5% of all in-stances in this study had a confidence value of 0.98or below (7,697 of 53,206).We conclude that with the given WSD system and(rather standard) features, this simple method cannotdetect items with an unknown sense reliably.
Thismay be due to the indiscriminately high confidencescores; or it could indicate that classifiers, whichare geared at distinguishing between known classesrather than detecting objects that differ from all seendata, are not optimally suited to the task.
However,one further disadvantage of this approach is that, asmentioned above, it can only be applied to lemmaswith more than one annotated sense.
For FrameNet1.2, this comprises only 19% of the lemmas.5 A nearest neighbor-based method foroutlier detectionIn the previous section we have tested a simple ap-proach to unknown sense detection using WSD con-fidence scores.
Our conclusion was that it was not aviable approach, given its low recall and given that3Note that the minimum confidence score is 0.5 if 2 sensesare present in the training set, 0.33 for 3 present senses etc.t t?dtt?x dxtFigure 3: Outlier detection by comparing distancesbetween nearest neighborsit is only applicable to lemmas with more than oneknown sense.
In this section we introduce an al-ternative approach, which uses distances to nearestneighbors to detect outliers.In general, the task of outlier detection is to de-cide whether a new object belongs to a given trainingset or not.
Typically, outlier detection approachesderive some boundary around the training set, orthey derive from the set some model of ?normal-ity?
to which new objects are compared (Markouand Singh, 2003a; Markou and Singh, 2003b; Mars-land, 2003).
Applications of outlier detection in-clude fault detection (Hickinbotham and Austin,2000), hand writing deciphering (Tax and Duin,1998; Scho?lkopf et al, 2000), and network intru-sion detection (Yeung and Chow, 2002; Dasguptaand Forrest, 1999).
One standard approach to out-lier detection estimates the probability density of thetraining set, such that a test object can be classifiedas an outlier or non-outlier according to its probabil-ity of belonging to the set.Rather than estimating the complete density func-tion, Tax and Duin (2000) approximate local densityat the test object by comparing distances betweennearest neighbors.
Given a test object x, the ap-proach considers the training object t nearest to xand compares the distance dxt between x and t to thedistance dtt?
between t and its own nearest trainingdata neighbor t?.
Then the quotient between the dis-tances is used as an indicator of the (ab-)normalityof the test object x:pNN (x) =dxtdtt?When the distance dxt is much larger than dtt?
, x isconsidered an outlier.
Figure 3 illustrates the idea.The normality or abnormality of test objects is de-cided by a fixed threshold ?
on pNN .
The lowest131threshold that makes sense is 1.0, which rejects anyx that is further apart from its nearest training neigh-bor t than t is from its neighbor.
Tax and Duin useEuclidean distance, i.e.dxt =?
?i(xi ?
ti)2Applied to feature vectors with entries either 0 or 1,this corresponds to the size of the symmetric differ-ence of the two feature sets.6 Experiment 2: NN-based outlierdetectionIn this section we use the NN-based outlier detectionapproach of the previous section for an experimentin unknown sense detection.
Experimental setup anddata are as described in Section 3.Modeling.
We model unknown sense detection asan outlier detection task, using Tax and Duin?s out-lier detection approach that we have outlined inthe previous section.
Nearest neighbors (by Eu-clidean distance) were computed using the ANNtool (Mount and Arya, 2005).
We compute one out-lier detection model per lemma.
With training andtest sets constructed as described in Section 3, theaverage training set comprises 22.5 sentences.We use the same features as in Section 4, with fea-ture vector entries of 1 for present and 0 for absentfeatures.
For a more detailed analysis of the contri-bution of different feature types, we test on reducedas well as full feature vectors:All: full feature vectorsCx: only bag-of-word context features (words, lem-mas, POS, NE)Syn: function labels of dependentsSyn-hw: Syn plus headwords of dependentsWe compare the NN-based model to that ofExperiment 1, but not to any simpler baseline.While for WSD it is possible to formulate simplefrequency-based methods that can serve as a base-line, this is not so in unknown sense detection be-cause the frequency of unknown senses is, by def-inition, unknown.
Furthermore, the number of an-notated sentences per sense in FrameNet dependsFeatures Precision RecallAll 0.7072 (?
0.0088) 0.2683 (?
0.0043)Cx 0.7016 (?
0.0041) 0.3511 (?
0.0035)Syn 0.8333 (?
0.0085) 0.2099 (?
0.0042)Syn-hw 0.7784 (?
0.0029) 0.2368 (?
0.0022)Table 2: Experiment 2: Results for label unknownsense, NN-based outlier detection, ?
= 1.0.
?
: stan-dard deviationPrecision RecallFeatures all ?
10 ?
20 all ?
10 ?
20All 0.71 0.70 0.67 0.27 0.35 0.45Cx 0.70 0.70 0.67 0.35 0.47 0.58Syn 0.83 0.81 0.77 0.21 0.22 0.21Syn-hw 0.78 0.76 0.73 0.24 0.28 0.31Table 3: Experiment 2: Results by training set size,?
= 1.0on the number of subcategorization frames of thelemma rather than the frequency of the sense, whichmakes frequency calculations meaningless.Results.
Table 2 shows precision and recall for la-beling instances as unknown using a distance quo-tient threshold of ?=1.0, averaged over unknownsenses and over 5-fold cross-validation.
We see thatrecall is markedly higher than in Experiment 1, es-pecially for the two conditions that include contextwords, All and Cx.
The syntax-based conditionsSyn and Syn-hw show a higher precision, with aless pronounced increase in recall.Raising the distance quotient threshold results inlittle change in precision, but a large drop in recall.For example, All vectors with a threshold of ?
=1.1 achieve a recall of 0.14 in comparison to 0.27for ?
= 1.0 .Training set size is an important factor in sys-tem results.
Table 3 lists precision and recall for alltraining sets, for training sets of size ?
10, and fortraining sets of size ?
20.
Especially in conditionsAll and Cx, recall rises steeply when we only con-sider cases with larger training sets.
However notethat precision does not rise with larger training sets,rather it shows a slight decline.Another important factor is the number of sensesthat a lemma has, as the upper part of Table 7 shows.For lemmas with a higher number of senses, preci-132Figure 4: ?Acceptance radius?
of an outlier withinthe training set (left) and a more ?normal?
trainingset object (right)sion is much lower, while recall is much higher.Discussion.
While results in this experiment arebetter than in Experiment 1 ?
in particular recall hasrisen by 19 points for Cx ?, system performance isstill not high enough to be usable in practice.The uniformity of the training set has a large in-fluence on performance, as Table 7 shows.
The moresenses a lemma has, the harder it seems to be for themodel to identify known sense occurrences.
Preci-sion for the assignment of the unknown label drops,while recall rises.
We see a tradeoff between preci-sion and recall, in this table as well as in Table 3.There, we see that many more unknown test objectsare identified when training sets are larger, but alarger training set does not translate into universallyhigher results.One possible explanation for this lies in a prop-erty of Tax and Duin?s approach.
If a training item tis situated at distance d from its nearest neighbor inthe training set, then any test item within a radius ofd around t will be considered known.
Thus we couldterm d the ?acceptance radius?
of t. Now if t is anoutlier within the training set, then d will be large, asillustrated in Figure 4.
The sparser the training set is,the more training outliers we are likely to find, withlarge acceptance radii that assign a label of knowneven to more distanced test items.
Thus a sparsetraining set could lead to lower recall of unknownsense assignment and at the same time higher pre-cision, as the items labeled unknown would be theones at great distance from any items on the trainingset ?
conforming to the pattern in Tables 3 and 7.7 Experiment 3: NN-based outlierdetection with added training dataWhile the NN-based outlier detection model weused in the previous experiment showed better re-Target lemma: putSenses: ENCODING, PLACINGSense currently treated as unknown: PLACINGExtend training set by: all annotated sentences forlemmas other than put in the sense ENCODING:couch.v, expression.n, formulate.v, formulation.n,frame.v, phrase.v, word.v, wording.nTable 4: Extending training sets: an exampleFeatures Precision RecallAll 0.7709 (?
0.001) 0.7243 (?
0.0018)Cx 0.7727 (?
0.0027) 0.8172 (?
0.0035)Syn 0.8571 (?
0.0045) 0.1694 (?
0.0012)Syn-hw 0.8025 (?
0.0041) 0.3383 (?
0.0025)Syn 0.8587 (?
0.0081) 0.1748 (?
0.0015)Syn-hw 0.8055 (?
0.0056) 0.3516 (?
0.0015)Table 5: Experiment 3: Results for label unknownsense, NN-based outlier detection, ?
= 1.0.
?
: stan-dard deviationsults than the WSD confidence model, its recall isstill low.
We have suggested that data sparsenessmay be responsible for the low performance.
Con-sequently, we repeat the experiment of the previoussection with more, but less specific, training data.Like WordNet synsets, FrameNet frames are se-mantic classes that typically comprise several lem-mas or expressions.
So, assuming that words withsimilar meaning occur in similar contexts, the con-text features for lemmas in the same frame shouldbe similar.
Following this idea, we supplement thetraining data for a lemma by all the other annotateddata for the senses that are present in the trainingset, where by ?other data?
we mean data with othertarget lemmas.
Table 4 shows an example4.Modeling.
Again, we use Tax and Duin?s outlierdetection approach for unknown sense detection.The experimental design and evaluation are the sameas in Experiment 2, the only difference being thetraining set extension.
Training set extension raisesthe average training set size from 22.5 to 374.Results.
Table 5 shows precision and recall for la-beling instances as unknown, with a distance quo-tient threshold of 1.0, averaged over unknown senses4Conditions Syn and Syn-hw were also tested using onlyother target lemmas with the same part of speech.
Results werevirtually unchanged.133Precision RecallFeatures all ?
50 ?
200 all ?
50 ?
200All 0.77 0.77 0.73 0.72 0.80 0.87Cx 0.77 0.77 0.73 0.82 0.89 0.94Syn 0.86 0.85 0.82 0.17 0.16 0.13Syn-hw 0.80 0.79 0.76 0.38 0.36 0.38Syn 0.86 0.85 0.82 0.17 0.17 0.14Syn-hw 0.81 0.80 0.76 0.35 0.37 0.38Table 6: Experiment 3: Results by training set size,?
= 1.0Number of senses2 3 4 5Exp.
2 Prec.
0.78 0.68 0.59 0.55Rec.
0.21 0.38 0.47 0.59Exp.
3 Prec.
0.83 0.71 0.63 0.56Rec.
0.68 0.81 0.89 0.88Table 7: Experiments 2 and 3: Results by the num-ber of senses of a lemma, condition All, ?
= 1.0and 5-fold cross-validation.
In comparison to Exper-iment 2, precision has risen slightly, and for condi-tions All, Cx and Syn-hw, recall has risen steeply;the maximum recall is achieved by Cx at 0.82.As before, increasing the distance quotient thresh-old leads to little change in precision but a sharpdrop in recall.
For All vectors, recall is 0.72 forthreshold 1.0, 0.56 for ?
= 1.1, and 0.41 for ?
= 1.2.Table 6 shows system performance by training setsize.
As the average training set in this experimentis much larger than in Experiment 2, we are nowinspecting sets of minimum size 50 and 200 ratherthan 10 and 20.
We find the same effect as in Ex-periment 2, with noticeably higher recall for lemmaswith larger training sets, but slightly lower precision.Table 7 breaks down system performance by thedegree of ambiguity of a lemma.
Here, too, we seethe same effect as in Experiment 2: the more sensesa lemma has, the lower the precision and the higherthe recall of unknown label assignment.Discussion.
In comparison to Experiment 2, Ex-periment 3 shows a dramatic increase in recall, andeven some increase in precision.
Precision and re-call for conditions All and Cx are good enough forthe system to be usable in practice.Of the four conditions, the three that involve con-text words, All, Cx and Syn-hw, show consid-erably higher recall than Syn.
Furthermore, thetwo conditions that do not involve syntactic fea-tures, All and Cx, have markedly higher resultsthan Syn-hw.
This could mean that syntactic fea-tures are not as helpful as context features in detect-ing unknown senses; however in Experiment 2 theperformance difference between Syn and the otherconditions was not by far as large as in this experi-ment.
It could also mean that frames are not as uni-form in their syntactic structure as they are in theircontext words.
This seems plausible as FrameNetframes are constructed mostly on semantic grounds,without recourse to similarity in syntactic structure.Table 6 points to a sparse data problem, even withtraining sets extended by additional items.
It alsoshows that the more a test condition relies on contextword information, the more it profits from additionaldata.
So it may be worthwhile to explore methodsfor a further alleviation of data sparseness, e.g.
bygeneralizing over context words.Table 7 underscores the large influence of train-ing set uniformity: the more senses a lemma has, themore likely the model is to classify a test instance asunknown.
This is the case even for extended trainingsets.
One possible way of addressing this problemwould be to take into account more than a singlenearest neighbor in NN-based outlier detection inorder to compute more precise boundaries betweenknown and unknown instances.8 Conclusion and outlookWe have defined and addressed the problem ofunknown word sense detection: the identificationof corpus occurrences that are not covered by agiven sense inventory, using a training set of sense-annotated data as a basis.
We have modeled thisproblem as an instance of outlier detection, usingthe simple nearest neighbor-based approach of Taxand Duin to measure the resemblance of a new oc-currence to the training data.
In combination witha method that alleviates data sparseness by sharingtraining data across lemmas, the approach achievesgood results that make it usable in practice: Withitems represented as vectors of context words (in-cluding lemma, POS and NE), the system achieves0.77 precision and 0.82 recall in an evaluation onFrameNet 1.2.
The training set extension method,134which proved crucial to our approach, relies solelyon a grouping of annotated data by semantic simi-larity.
As such, the method is applicable to any re-source that groups words into semantic classes, forexample WordNet.For this first study on unknown sense detection,we have chosen a maximally simple outlier detec-tion method; many extensions are possible.
One ob-vious possibility is the extension of Tax and Duin?smethod to more than one nearest training neigh-bor for a more accurate estimate of local density.Furthermore, more sophisticated feature vectors canbe employed to generalize over context words, andother outlier detection approaches (Markou andSingh, 2003a; Markou and Singh, 2003b; Marsland,2003) can be tested on this task.Our immediate goal is to use unknown sense de-tection in combination with WSD, to filter out itemsthat the WSD system cannot handle due to missingsenses.
Once items have been identified as unknown,they are available for further processing: If possibleone would like to assign some measure of sense in-formation even to these items.
Possibilities includeassociating items with similar existing senses (Wid-dows, 2003; Curran, 2005; Burchardt et al, 2005) orclustering them into approximate senses.ReferencesC.
Baker, C. Fillmore, and J. Lowe.
1998.
The BerkeleyFrameNet Project.
In Proc.
ACL-98, Montreal.A.
Burchardt, K. Erk, and A. Frank.
2005.
A WordNetdetour to FrameNet.
In Proc.
GLDV 2005 WorkshopGermaNet II, Bonn.U.
Callmeier, A. Eisele, U. Scha?fer, and M. Siegel.
2004.The DeepThought core architecture framework.
InProc.
LREC-04, Lisbon.James Curran.
2005.
Supersense tagging of unknownnouns using semantic similarity.
In Proc.
ACL-05,Ann Arbor.D.
Dasgupta and S. Forrest.
1999.
Novelty detectionin time series data using ideas from immunology.
InProc.
International Conference on Intelligent Systems.Katrin Erk and Sebastian Pado.
2006.
Shalmaneser -a toolchain for shallow semantic parsing.
In Proc.LREC-06, Genoa.K.
Erk.
2005.
Frame assignment as word sense disam-biguation.
In Proc.
IWCS 2005, Tilburg.C.
Fillmore.
1982.
Frame Semantics.
Linguistics in theMorning Calm.R.
Florian, S. Cucerzan, C. Schafer, and D. Yarowsky.2002.
Combining classifiers for word sense disam-biguation.
Journal of Natural Language Engineering,8(4):327?431.S.
Hickinbotham and J. Austin.
2000.
Neural networksfor novelty detection in airframe strain data.
In Proc.International Joint Conference on Neural Networks.D.
Lin.
1993.
Principle-based parsing without overgen-eration.
In Proc.
ACL-93, Columbus, OH.M.
Markou and S. Singh.
2003a.
Novelty detection:A review.
part 1: Statistical approaches.
ACM SignalProcessing, 83(12):2481 ?
2497.M.
Markou and S. Singh.
2003b.
Novelty detection:A review.
part 2: Neural network based approaches.ACM Signal Processing, 83(12):2499 ?
2521.S.
Marsland.
2003.
Novelty detection in learning sys-tems.
Neural computing surveys, 3:157?195.D.
Mount and S. Arya.
2005.
ANN: A library for approx-imate nearest neighbor searching.
Download fromhttp://www.cs.umd.edu/?mount/ANN/.B.
Scho?lkopf, R. Williamson, A. Smola, J. Shawe-Taylor,and J. Platt.
2000.
Support vector method for noveltydetection.
Advances in neural information processingsystems, 12.H.
Schu?tze.
1998.
Automatic word sense discrimination.Computational Linguistics, 24(1):97 ?
123.M.
Tatu and D. Moldovan.
2005.
A semantic approach torecognizing textual entailment.
In Proc.
HLT/EMNLP2005, Vancouver.D.
Tax and R. Duin.
1998.
Outlier detection using clas-sifier instability.
In Advances in Pattern Recognition:the Joint IAPR International Workshops.D.
Tax and R. Duin.
2000.
Data description in sub-spaces.
In International Conference on Pattern recog-nition, volume 2, Barcelona.Dominic Widdows.
2003.
Unsupervised methods for de-veloping taxonomies by combining syntactic and sta-tistical information.
In Proc.
HLT/NAACL-03, Ed-monton.D.
Yeung and C. Chow.
2002.
Parzen-window networkintrusion detectors.
In Proc.
International Conferenceon Pattern Recognition.135
