Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers,pages 1238?1247, Dublin, Ireland, August 23-29 2014.Online Gaming for Crowd-sourcing Phrase-equivalentsA.
KumaranMicrosoft ResearchBangalore, Indiaa.kumaran@microsoft.comMelissa DensmoreUniversity of Cape TownCape Town, South Africamdensmore@acm.orgShaishav KumarMicrosoft ResearchBangalore, Indiav-shaisk@microsoft.comAbstractWe propose the use of a game with a purpose (GWAP) to facilitate crowd-sourcing of phrase-equivalents, as an alternative to expert or paid crowd-sourcing.
Doodling is an online multi-player game, in which one player (drawer), draws pictures on a shared board to get the otherplayers (guessers) to guess the meaning behind an assigned phrase.
In this paper we describethe system and results from several experiments intended to improve the quality of informationgenerated by the play.
In addition, we describe the mechanism by which we take candidatephrases generated during the games and filter out true phrase equivalents.
We expect that, atscale, this game will be more cost-efficient than paid mechanisms for a similar task, and demon-strate this by comparing the productivity of an hour of game play to an equivalent crowd-sourcedAmazon Mechanical Turk task to produce phrase-equivalents over one week.1  IntroductionWhile it is fairly well known when individual words have the same meaning, it is far more difficult todetermine when phrases or even sentences carry the same basic idea.
While it might be possible toaddress this task with machine learning techniques, building a corpus of sentences from which to seeda database requires human intelligence.
We suggest a game with a purpose (GWAP) that will serve togenerate phrases with similar meanings, while simultaneously providing meta-information about thequality of the match.
In this drawing game, called Doodling, individuals compete in groups to guess themeaning behind a given drawing that is being drawn by one designated drawer trying to convey a givenphrase or a short sentence.
The designated drawer decides when a guessed phrase matches the sourcephrase.
For example ?How far is the airport??
might match semantically ?What is the distance to theairport??
In addition, the drawer can indicate for each partial guess how close it is on a scale of 1-3 tohelp the guessers converge on phrases that will match the given phrase or sentence.
We then pass all ofthe guesses and annotations through an SVM classifier to automatically identify potential phrase-equiv-alents.
In this study we examine several techniques for using this system to generate high quality datawhile also making the game more enjoyable.
We measure the efficacy of each technique by comparingour results to a gold standard: using human evaluators to rate the phrase matches generated through thegame manually.
We also compare Doodling to a paid crowdsourcing paradigm ?
Amazon?s MechanicalTurk ?
to source phrase equivalents for the same set of phrases, and we show that our approach mightbe cost effective for large scale sourcing of paraphrases of equivalent quality.2 BackgroundIn this section we define the problem we are trying to address, and discuss the various ways it has beenapproached in the past.2.1 Phrase-equivalents & Evaluation of QualityIn this paper, we define phrase equivalents (PEs) as text elements ?
phrases or short sentences ?
thathave same or similar semantic content, but with surface structure different from each other.
PEs aresimilar to paraphrases, but broader in scope, inclusive of partial matches in meaning as well as completeThis work is licensed under a Creative Commons Attribution 4.0 International License.
Page numbers and proceedings footerare added by the organizers.
License details: http://creativecommons.org/licenses/by/4.0/1238paraphrases.
PEs are useful for many NLP systems from simple language modelling and smoothing, tocomplex Machine Translation technology for generation of a surface form in the target language.
Mostexisting corpora are hand-created, and hence they tend to be small in size, and available only in limitedlanguages and domains.
Other data driven approaches ?
such as, creation of paraphrases using mono-lingual machine translation (Quirk et al., 2004), mining inference rules from text corpora (Lin & Pantel,2001), or paraphrase extraction from parallel corpora (Dolan et al., 2004) (Barzilay & McKeown, 2001)?
were shown to be effective, but such approaches require significant seed corpora which are availableonly in limited domains and languages.
In addition, the (Lin & Pantel, 2001) approach can generateequivalents using user defined patterns, and may not be appropriate for generating loosely related con-ceptual paraphrases like the human generated ones that Doodling may generate.The criteria used for evaluating phrase equivalents differ vastly in research literature, ranging fromconceptual equivalence (Barzilay & McKeown, 2001), to interchangeability (Ibrahim et al, 2003), topreservation of grammatical correctness and semantic equivalence (Callison-Burch, 2005), and thestandard metric of BLEU score (Callison-Burch, 2005; Papineni et al., 2002).
In general, there is noaccepted standard model for measuring quality, hence we adopted manual annotation by experts.2.2 Crowdsourcing & Games with a Purpose (GWAP) for Computational LinguisticsMany flavors of crowdsourcing paradigms exist for the creation of language data.
From the for-paymodel where the contribution is for monetary rewards (Callison-Burch, 2009; Irvine & Klementiev,2010; Chen & Dolan, 2011), to the for-recognition model, where the contribution is made for individu-als?
visibility in a community (e.g., SourceForge), and the common-good model, value is produced forthe benefit of some community (Kumaran et al., 2009).
In this paper, we explore the for-fun model(Cooper et al., 2010; Law et al., 2007; Von Ahn & Dabbish, 2004; Von Ahn et al., 2006), in which datais a by-product of some gameplay, often referred to as ?Games with a Purpose?
(Von Ahn & Dabbish,2008), which have been shown to be very successful in many domains.Specifically with respect to generation of paraphrases or phrase equivalents, (Chen & Dolan, 2011)present their paraphrase collection using video annotations, focusing primarily on viability of establish-ing Mechanical Turk for providing paraphrases in a productive way.
(Barzilay & McKeown, 2003) pos-ited that multiple translations of a foreign text may be a naturally occurring source for paraphrases aseach is authored by a different translator; our approach is analogous to this approach, though our sourcephrases/sentences are not from a foreign language.
(Chklovski, 2005) presents an online paraphrasecollection tool and studies the incentive model for responsible contributions by volunteers.
Paraphrasesgenerated by Doodling would be similar to paraphrases labelled under class ?Phrasal?
and to a lesserextent class ?Elaboration?
in (Chen & Dolan, 2011).
In our earlier work (Kumaran et al., 2012) wefocused on a proof-of-concept methodology using a Pictionary-based approach for generation of para-phrases.
In this paper, we expand our concept for generating phrase equivalents in scale inexpensively,using several game and UI/UX features, and also compare it with a realistic for-pay baseline using Me-chanical Turk.
The power of our methodology is its self-verification mechanisms (by drawer annotatingthe response for convergence, and the final acceptance) that validates the generated paraphrases.3 Doodling as a GameIn this section, we present the design elements and the game flow of the Doodling game.3.1 Game DesignIn the Doodling game, the games are played in rooms with one player (designated as the Drawer)sketches an assigned concept - as phrase or sentence - while other players in the game room (Guessers)attempt to guess the assigned concept from the drawing that is being replicated to all screens.
TheGuessers typically start guessing the words first (based on the concept that the Drawer starts sketchingon the screen); while the game will automatically indicate exact partial matches (for example, ?Taxi?
asa guess for the given phrase, ?Taxi Driver?
), the drawer also has the ability to provide feedback usingannotations.
The Drawer may annotate partial guesses as incorrect (red), on the right track (orange), orpartially correct (green), to guide the convergence.
All the guessers?
guesses and the drawer?s annotationare broadcast to all the players in the room.
Such broadcasting provides a mechanism in which players1239can build on the top of other?s guesses, gradually building up the phrase or the sentence.
At some point,if one of the guessers guess the right phrase exactly, the game is closed automatically.
In addition, ifthe drawer judges the guess as having the same meaning as the assigned concept (for example, ?Cabbie?for ?Taxi Driver?
), he/she can end the round by marking the guess as correct, rewarding the guesserwith game points.
If the timer runs out before a correct guess happens, then the game times out.
Figure1 shows the UI during the progress of a game (the given text element being ?taxi driver?
).Figure 1: Doodling GameOur primary intuition is that the sketches provide a language-independent means of communicationof concepts that is effectively employed for the generation of phrase equivalents.
Thus, we leverage afun drawing-guessing game to fulfill the linguistic purpose of generating phrase-equivalents.
An im-portant aspect of making Doodling effective was to make it engaging to play.
We underwent multipleuser studies followed by changes to the game?s UI/UX.
Earlier trials had revealed the need for additionalfeedback from the drawer, leading to the introduction of 3-stage annotations of guessed phrases.
Froma usability standpoint, the UI and gestures were optimized for use with touchscreen capable devices,including of the use of swipe gestures for annotating incoming guesses.The Doodling game subscribes to the Inversion Problem (Von Ahn & Dabbish, 2008), where one ofthe players produces an output in the form of a sketch for a given input phrase.
The other players attemptto guess the given input.
The game may produce multiple surface forms of a single semantic intent thathave a relationship similar to that of the input-output pair in the ?noisy-channel?
model.3.2 Game ElementsWhile the game dynamics promote the resolution of the underlying computational problem (i.e., thegeneration of phrase equivalents), we made certain modifications to the basic sketch-and-convey meta-phor ?
in the formation and constitution of the game rooms, in the assignment of roles to players in around-robin fashion, and the drawer?s feedback using annotation, in exposing every player?s guess tothe entire game room, and the winning strategy that encourages building on each other?s guesses ?
inorder to help the rounds finish successfully, converge faster, and be more competitive.
Above all, thegame dynamics and the UI were designed to make Doodling enjoyable as a game.Roles: Users may join existing game rooms, or can create a new private game room after logging into the Doodling portal.
In a game room, one of the users is assigned ?
randomly ?
the role of Drawer(D), and the others the role of Guessers (G).
At the end of a given game round, the role of drawer cyclesamong the game room participants.
All G?s both compete (the first guesser to guess right ?
either fullyor partially ?
is rewarded), as well as collaborate (each builds on other?s guesses to build longer phrasesfor bigger rewards) in guessing the text element being conveyed by the D.1240Game Round: Like the sketches, the individual guesses of a given G are broadcast to the entire room,along with any annotation from D on each of the guesses (red/orange/green).
While the right guesses(either lexicographic match, or as judged by D) gives the game point to the specific G, the broadcast ofguesses and feedbacks from D to the entire game room provides a transparent mechanism to help eachplayer build on the guesses of the others.
The game round closes with exact reproduction of the sourcephrase by one of the G?s, or by D accepting a full semantic equivalent by double tapping a tile.
As anincentive for the role of the drawer, the D is also rewarded with some game points.Data: In our current experiments, we used standard phrases from a generic WikiTravel (http://wik-itravel.org/en/wikitravel:phrasebook_template) tourism phrase book as input elements.
The authorssubjectively classified each text element as Easy or Hard, depending on the potential difficulty to expressit as a sketch; though such annotation implies additional preparatory work, it may be well worth theinvestment as such tagged corpora forms the seed for many variations.
We plan to add text elements inmany domains (Celebrities, Movies and Idioms), to provide diversity to the players.Text Element Diff.
GranularityCheese Omelette Easy PhraseMuseum of Modern Art Hard PhraseI would like a bowl of soup.
Easy SentenceI am not feeling well!
Hard SentenceTable 1: Sample of text elements used in the initial seed corpusIn order to understand the dynamics of the game, and to improve the quality and quantity of the phraseequivalents generated in Doodling, we incorporated many features.Number of Players: The application supports 2-4 players per game room, to measure the effect ofroom size on convergence rate and the player enjoyment.
We hypothesize that those game rooms withmore players will lead to better completion primarily due to higher productivity in phrase generation.Hints & Reminders: we provided hints to all guessers at the beginning of the game to prime then onwhat to expect about the guess phrase.
Hints are simple text elements, such as ?Short Phrase?
or ?HardSentence?, etc.
In addition, we also provided some reminders periodically for improving the game dy-namics, especially for the new players, including a reminder to the drawer that they can accept non-exact phrases with the same meaning by double-tapping on the guess tile.
Reminders appear on thescreen, and fade away unobtrusively.
Some game rooms were provided the hints, while others are not,in order to measure how helpful the hints are for game completion.Soft Matches:  Exact lexicographic guesses (full or partial) are automatically rewarded by the gameengine.
However, as the primary mechanism for gathering paraphrases, soft matches were allowed andrewarded at the discretion of the drawer (either by the double-tap action that accepts a guess as a correctphrase equivalent, or by the swipe-right action that which indicates a potential partial match).
Yet, todiscourage collusion or cheating, a reporting mechanism is provided: The final accepted guess alongwith the input text element are shown to all participants, to report any unsatisfactory acceptance.Metrics: For measuring the effectiveness of the Doodling game, we define many metrics rangingfrom completion statistics (completion rate and completion time), to quality by comparison with golddata (true positives as compared with user-annotated data, precision and accuracy of automatically clas-sified data), to qualitative user feedback (fun factor).4 Doodling: Experimental EvaluationDoodling is an HTML5 app that is accessible from most devices - touchscreen laptop or tablets - anddeployed in the cloud (http://doodle1.cloudapp.net/).
After deployment, we recruited volunteers (pri-marily graduate students) to log in and play the game for one hour.
As the volunteers entered the gameserver, they were assigned to different game rooms; each room was instrumented for a specific config-uration (game room size - between 2 and 4 players, and availability of hints and reminders).
Each roomwas given the same set of 38 phrases in the same sequence, to keep the variability to a minimum.
Afteran hour, the games were closed and the players asked to fill in an online questionnaire.In these trials, the 14 volunteers played a total of 112 games, in different game rooms.
Most playershad previously been exposed to the sketch-and-convey metaphor through Pictionary-type games.12414.1 Quality of the Generated DataBasis for evaluation: We first extracted all of the text elements annotated as a potential match (green,orange, or winning) by the Drawer.
Each of the three authors then independently classified according tothe relevance of the match.
The following five classes were used for annotating every annotated textelement: EF (Exact Full Match), EP (Exact Partial Match), TF (True Full Match), TP (True PartialMatch) or NM (Not a Match).
Partial matches entailed guesses which captured some sub-element of theseed text, but not the entire meaning.
We then measured inter-annotator agreement of author?s annota-tions using a Fleiss Kappa measure (Fleiss, 1971), which stood at 0.7424, indicating substantial agree-ment among our annotations.
Hence, we used our annotation (using majority voting for resolving anyconflicts) as the gold data set for validating automatically the user generated paraphrases, in subsequentsections.Quality of the generated data:  Of the 112 games played, 98 of them completed successfully.
Gameswere considered incomplete if the timer expired before successful completion.
Of the 98 completedgames, 15 of the final guesses were false positives (i.e.
NM, wrong answers accepted erroneously), 42games closed with guessers reproducing the exact text element given to the drawer (i.e.
EF), and 19games closed with Drawer correctly accepting a guess that is semantically equivalent to the given textelement (i.e.
TF, a true phrase-equivalent), and the remaining producing various degrees partial semanticmatches (i.e.
TP, true partial phrase-equivalents).
The average time of completion for successfully com-pleted games was 160 seconds.In addition, most of the games, irrespective of whether closed correctly or not, produced partial equiv-alents to the given text element as intermediate guesses, thus providing valuable data for research.
Theseinclude all the potential matches which were not accepted as the final answer for a game, but weremarked as green or orange via the drawers?
swipe-based annotation.
Table 3 shows the breakdown ofthe gold classification of all of the potential matches.4.2 From Game to CorpusOnce assured the quality of the generated data, we devised a methodology for automatically detectingphrase equivalents (full or partial) from the user generated data, so that the game would be able to scalewithout the need for human annotators to verify individual guesses.
We designed a classifier for auto-matically validating phrase equivalents (partial or full), based only on the game meta data, and veryshallow text level features, and not based on any linguistic (such as, dictionaries, thesauri, etc.)
or otherspecialized corpora (such as, parallel or paraphrase corpora).
Our basic premise is that if such a classifiercan identify good paraphrases with simple features, then we will be able to identify the phrase equiva-lents automatically, in new domains or languages.Our classifier uses only simple game and text-level features: hardness of the input text element(easy/medium/hard), status of completion flag and cheating flag at completion, order and time of theguess, drawer?s annotation (green/orange/red), cross-game evidence, substring similarities to the inputtext element and orthographic overlap with the input text.
First, we extracted any exact matches (EF orEP) by removing any text elements that were a substring of the original guess, leaving us with a trainingcorpus of 122 potential phrase-equivalents.
We trained the classifier using a 5-fold cross-validation thiscorpus.
Some paraphrases thus extracted are shown in Table 2.Source phrase Paraphrases extractedPolice Officer Policeman, Police Inspector, Police SuperintendI lost my luggage.
I need to find my bag at the lost-and-found counter, Lost-and-found luggage counter.School Teacher Class Teacher, Teacher teaching in school.Railway Station  Railroad Station, Railway PlatformTable 2: Automatically Extracted ParaphrasesDoodling Doodling  + SVM MTurkRaw Corpus Training Corpus SVM = NM SVM = TF|TP CorpusSize 234 122 73 49 92Exact Full (EF) 42 EF and EP Data automatically removed fromCorpus using String and Substring Match0Exact Partial (EP) 71 211242True Full (TF) 30 30 2 28 53True Partial (TP) 11 11 5 6 13Not a Match (NM) 81 81 66 15 5Precision (TF+TP/Size) 17% 34% 10% 69% 72%Table 3: Comparison of corpora produced by Doodling and MTurk to gold data.
SVM numbers are an average of the resultsgenerated during the 5-fold cross-validation.The classifier reduces the burden on expert hand-annotators, by automatically filtering out text ele-ments that are likely to not be a match.
As can be seen in Table 3, only 17% of the raw corpus constitutesuseful data.
Removing exact and substring matches (EF and EP) increases the precision to 34%.
Theusable corpus produced by the classifier (SVM=TF|TP) has a precision of 69%, with only 10% of theremaining corpus (SVM=NM) constituting false negative, or ?lost?
data.
The overall accuracy of theclassifier (% of true positives + true negatives) is 82%.This methodology provides a viable means of generating paraphrase corpora, with a small amount ofhand-crafted corpus in a new domain.
The classifier can be fine-tuned either for accuracy of prediction(precision) or productivity (recall); in our experiments we fine-tuned it for precision.
Also, we believethat given that these features used are devoid of linguistic or domain information, our results may pro-vide a lower bound on the quality of automatic identification of phrase equivalents; this may be im-proved substantially by use of appropriate linguistic resources or specialized corpora.In addition to phrase-equivalent data, many of the guesses relate semantically to the input text ele-ment, in varying degrees.
Using similar features as used in the classifier, the annotation data can beused for identifying sets of related words for given input text elements, creating valuable resources forsearch query expansion.5 Mechanical Turk ExperimentsTo understand the quantitative difference between Doodling and a paid crowdsourcing model for gen-erating paraphrases we designed a ?Data Collection?
Mechanical Turk task using the same phrases thatwere used in our user experiments.
Based on previous work relating to designing of Turk experimentsand accepted best practices, we kept the task description simple: Each task asked a respondent to gen-erate five unique and semantically equivalent phrases for a given source phrase.
The respondents werechosen based on their familiarity with English as their first language, and each phrase was to be anno-tated by 20 respondents over one week duration; this duration was chosen to keep the respondent popu-lation size roughly equal to that of our user experiment.
Reward for completing the generation of fivephrase-equivalents for a single given phrase was fixed at $0.10USD, in line with the rewards given outfor tasks with similar levels of difficulty as cited in published literature (Callison-Burch et al.
2009;Dolan et al, 2011).
Though the time frame was a larger than the duration of our experiments (one hour)significantly, the overall time taken for task is comparable to the time spent in gameplay.At the end of the one-week duration of the experiment, 14 out of 38 phrases got at least one set ofvalid paraphrases, leading to a completion percentage of 37%.
Most of the submitted phrases wereannotated only by one respondent; the average number of respondents per phrase was 1.23.
The anno-tation data was judged by the authors in the same scale as outlined in Section 5.1, and the Fleiss Kappameasure for the annotation was 0.74, signifying significant agreement between their judgments.
Overall,72% of the MTurker generated paraphrases were accepted as full or partial alternatives (See Table 3).While the quality of data is very good, any misunderstanding of the task generated results that are sig-nificantly off the mark:  For example, ?How do I get to the nearest international airport??
was generatedfor ?International Airport?
as the source phrase.
Since the participation and completion was low, weextended the duration of the task by another week, but the second week yielded only 2 additional com-pleted tasks indicating that the duration of the experiment was not the sole factor in the relative low rateof task completion; perhaps it is the nature of the task that did not attract significant participation.12436 Discussions6.1 Viability of Doodling as a GameThe 85% successful completion (98 out of 112) of the games is encouraging, and indicates the viabilityof the game to complete successfully.
At the end of the experimental session (wherein 30 rounds of thegame had been completed by each player on an average), the players were asked to fill in an onlinesurvey to measure various qualitative metrics on effectiveness of Doodling as a game.
A wide varietyof questions were asked, ranging from specific input (How did [a specific feature] affect your ability toguess the right phrase?)
to generic qualitative measures (Would you play this game again?).
Amongthe questions were three specific questions on how much the players enjoyed the game as a drawer, asa guesser and overall, in a scale of 1 (Hated it.)
to 5 (Loved it!).
From the 10 respondents, the enjoymentfactor averaged at 4.7 overall.
Such high score validates the game design and UX as a viable mechanismfor an enjoyable game.
Further, 9 out of 10 respondents said that they would definitely play the gameagain, with comments such as ?It was very interesting and fun?
and ?This game is kind of addictive?,indicating attraction of the game for subsequent engagement6.2 Use of Hints & RemindersWe find no evidence for the hints or reminders to be valuable either in improving the quality of theresult, or helping the time for convergence/completion.
We note that several gamers resorted to othermeans of indicating the structure of the guess phrases, such as drawing out a number of dashes to indicatethe size of the guess phrase, with some of them requesting us to do the same.6.3 Scaling Up: Comparison with Mechanical Turk for crowd-sourcing phrase equivalentsGWAPs have been criticized for their complexity, long time-to-market, and hidden running costs (Wanget al., 2012).
Paid crowd-sourcing methods, by comparison, are simpler to set up, and have lower initialcosts.
While a concrete, direct comparison is not possible, Table 3 lays out some of the differencesbetween the two methods, especially with reference to our metrics.Mechanical Turk (MT) DoodlingExperimental Operating Costs US$82 US$90Ongoing Costs US$0.10/source phrase US$90/monthSetup Costs Minimal 3 man-monthsPlayers/Workers 9 14Time 2 weeks  1 hourCompletion (Games with ?
1 TF generated) 14/38 (37%) 38/38 (100%)Quantity (# of Unique TFs) 53 28Precision (% of usable data) 72% 69% (with Classifier)Table 4: Comparison of MTurk and Doodling experiments for generation of phrase-equivalentsIn the case of the Doodling game, the development of the game took 3 man-months, while MechanicalTurk?s (MT) setup time was minimal.
Both the Doodling and MT experiments had similar operationalcosts, at US$90 and US$82 respectively.
This cost of $90 for Doodling consists of hosting and band-width charges incurred for two virtual servers running on a commercial cloud platform.
However, oncewe scale Doodling up to permit more users and higher productivity, we expect the costs to remain fixed,whereas MT costs will scale proportionally to the productivity at US$0.10 per source.
In addition, evenwith approximately equal investment, one hour of Doodling game play is more productive than the twoweeks of MT task.
As discussed in Section 5, we encountered a significant limitation of paid crowd-sourcing: workers may not choose to do tasks they consider uninteresting.
While it is possible to in-crease the pay rate to increase the completion rate, this entails additional costs, with deteriorating com-pletion rates.
While we expect the productivity of Doodling to scale with the number of users, MT?sproductivity is low even for our limited experiment, and may not scale at all.1244To put this in perspective, the time taken to generate useful data using Mechanical Turk varies highlydepending on the task: (Chen and Dolan, 2011) reported a duration of 2 months, whereas (Callison-Burch et al., 2009) reported 2 days for their experiments.
In our Doodling experiment, the task comple-tion rate for the game (one hour, 14 players) is faster than the equivalent Mechanical Turk task (twoweeks, 9 workers).
We argue that for scalable data collection, a fixed recurring cost for a reliable com-pletion rate may be preferable over a variable recurring cost.
Furthermore, the Doodling game setup iseasily scalable to large user base with little marginal cost, and hence we hypothesize that the economyof scale will make Doodling cheaper than MT for diverse domains.
Finally, while MT workers tend tobe transient, gamers tend to be loyal, particularly if the game is perceived to be interesting.
Such a userbase may be likely to participate and be productive in other (perhaps related) GWAPs for the generationof useful language data.6.4 CheatingDoodling depends on fair gameplay in order to generate reliable phrase-equivalent.
Although we did nothave many cases of cheating during the trials, cases of cheating will be unavoidable as the game scalesto more users.
The drawer scribbling answers to the canvas is a most obvious form of cheating, whichmay require sophisticated image recognition algorithms to weed out automatically.
However, we optedfor a low-cost approach of allowing any guesser to mark a certain game round as cheating, if they findthe drawer scribbling on the canvas.
Any guesser can also mark a game round as cheating, if he/shefinds the drawer concluding a game round with guesses that are not equivalent phrases.
All guessers ina room other than the guesser who provided the accepted guess, are given three seconds to report cheat-ing in case the guess was not found as a suitable equivalent phrase.
While this methodology may notwork in a two player room, we expect that in larger rooms the competitive nature of the players willkeep a game honest.
Frequent offenders may be penalized.
Proposed penalties would be banning fromgame rooms, disabling certain roles or introducing harder authentication protocol to prune out offendingplayers.Along the same lines, we intend to introduce an ?inappropriate or offending?
flag, to be flagged for adrawing or a guess, by any of the players in the room.
Such flags, once set, may need to be investigatedoffline, and the players penalized in order to discourage misuse or abuse of the game environment.7 Conclusions and Future WorkIn this paper, we explored gaming as a methodology for generating paraphrase data that is useful forNLP or IR research and development of practical systems.
Specifically, we outlined a game-with-a-purpose ?
Doodling ?
that is based on sketch-and-convey metaphor, where a sketch by a Drawer wasused as a mechanism for abstracting a concept (the source phrase) which was then surfaced by differentguessers in the game room, potentially producing paraphrases.
We showed that our online multiplayergame was effective in generating paraphrase data, by mining user guesses in the familiar sketch-and-convey paradigm, and rewarding phrase-equivalents in addition to exact phrase guesses.
Our experi-ments for just one hour with volunteers have shown that this game can generate high quality data inscale.
Most importantly, our volunteers rated the game ?very enjoyable?, even after an hour of continualplay.
In addition, we presented a classification mechanism to automatically identify good partial or fullphrase-equivalents from the user guesses, using only the meta-level features of the game and shallowtext features, opening an avenue for data generation in diverse domains, with a small seed corpora.
Webelieve the quality of such identification may be improved significantly with addition of linguistic re-sources, such as, dictionaries or thesauri.
Finally, our experiments with Amazon?s Mechanical Turkindicated that our game is comparable to and potentially more scalable than paid crowd-sourcing.
Webelieve such a game may be a viable mechanism for generating paraphrase data in diverse domains andlanguages, cheaply.7.1 Future WorkCurrently, we are in the process of developing and releasing Doodling as a multiplayer game app,providing a potential opportunity to study its uptake in the Internet, and the quality of data generated.In our experiments we measured, through a post-game survey, the potential for Doodling being a fungame, and we obtained a score of 4.7 out of 5 for ?fun-factor?, in addition to many verbal comments on1245how enjoyable the game was.
Such user feedback amply indicate Doodling?s potential for scaling wellas a game in diverse domains, such as sports, entertainment and idioms.
Also, while the current imple-mentation of Doodling game works well for phrases, we have ample evidence that it works for shortsentences (such as, ?My luggage is lost?, ?Where is the nearest post office??
etc.).
We hope to extendit to complex sentences as future work.One of our goals long term is to explore the game?s potential for generating parallel data ?
perhapsthrough a game being played between two players conversant in two different languages.
While thismulti- and cross-lingual game poses significant challenges, it provides for an interesting exploration intogeneration of parallel data through games.
Significantly, it may also provide opportunities for languagelearning and/or cross-cultural awareness, as many of the idioms and culture-specific phrases are notreadily conveyed by the surface forms in one language or another.
If successful, this may pave way forcost-effective generation of parallel data between many languages of the world.1246ReferenceBarzilay, R. 2003.
Information Fusion for Mutli-document summarization: Paraphrasing and Generation.
Ph.D.thesis @ Columbia University.Barzilay, R., and McKeown, K. 2001.
Extracting paraphrases from a parallel corpus.
39th Annual Meeting of theAssociation for Computational Linguistics.Callison-Burch, C. 2009.
Fast, cheap, and creative: evaluating translation quality using Amazon?s MechanicalTurk.
EMNLP?09.Callison-Burch, C., Cohn, T., and Lapata, M.  2008.
ParaMetric: An Automatic Evaluation Metric for Paraphras-ing, International Conference on Computational Linguistics, 2008.Chen, D.L.
and Dolan, W. 2011.
Collecting highly parallel data for paraphrase evaluation.
49th Annual Meetingof the Association for Computational Linguistics.Lin, D., and Pantel, P. DIRT - Discovery of inference rules from text.
Proceedings of the seventh ACM SIGKDDInternational conference on Knowledge discovery and data mining.
ACM, 2001.Chklovski, T. Collecting paraphrase corpora from volunteer contributors.
Proceedings of the 3rd K-CAP.
Interna-tional conference on Knowledge Capture, ACM, 2005.Cooper, S., Khatib, F., Treuille, A., Barbero, J., Lee, J., Beenen, M., Leaver-Fey, A., Baker, D., Popovic, Z. andFoldit Players.
2010.
Predicting protein structures with a multiplayer online game.
Nature (466), Aug 2010.Dolan, W., Quirk, C., and Brockett, C. 2004.
Unsupervised construction of large paraphrase corpora: Exploitingmassively parallel news sources.
20th International Conference on Computational Linguistics.Fleiss, J. L. 1971.
Measuring nominal scale agreement among many raters.
Psychological Bulletin, 76, 378?382.Ibrahim, A., Katz, B., and Lin, J.
2003.
Extracting structural paraphrases from aligned monolingual corpora.
Sec-ond International Workshop on Paraphrasing (collocated with ACL 2003).Irvine, A. and Klementiev, A.
2010.
Using Mechanical Turk to annotate lexicons for less commonly used lan-guages.
NAACL-HLT 2010 Workshop on Creating Speech and Language Data with Amazon?s Mechanical Turk.Kumaran, A., Jauhar, S. K., and Basu, S. 2012.
Doodling: A Gaming Paradigm for Generating Language Data.Human Computation Workshop 2012.Kumaran, A., Saravanan, K., Datha, N., Ashok, B. and Dendi, V. 2009.
WikiBABEL: a wiki-style platform forcreation of parallel data.
ACL-IJCNLP 2009.Law, E.L.M., Von Ahn, L., Dannenberg, R. B. and Crawford, M. 2007.
Tagatune: A game for music and soundannotation.
ISMIR?07.Papineni, K., Roukos, S., Ward, T., and Zhu, W. J.
2002.
Bleu: A method for automatic evaluation of machinetranslation.
40th Annual Meeting of the Association for Computational Linguistics.Quirk, C., Brockett, C., and Dolan, W. 2004.
Monolingual machine translation for paraphrase generation.
Empir-ical Methods in Natural Language Processing (EMNLP-2004).Von Ahn, L. and Dabbish, L. 2004.
Labeling images with a computer game.
CHI?04.Von Ahn, L., Kedia, M. and Blum, M. 2006.
Verbosity: a game for collecting common-sense facts.
CHI?06.Von Ahn, L. and Dabbish, L. 2008.
Designing games with a purpose.
Communications of the ACM, Vol 51.Wang, A., Hoang, C. D. V. and Kan, M.  2012.
Perspectives on crowdsourcing annotations for natural languageprocessing.
Language Resources & Evaluation Conference, 2012.1247
