Proceedings of the 2nd Workshop on Cognitive Modeling and Computational Linguistics, pages 88?96,Portland, Oregon, June 2011. c?2011 Association for Computational LinguisticsClassification of atypical language in autismEmily T. Prud?hommeaux, Brian Roark, Lois M. Black, and Jan van SantenCenter for Spoken Language UnderstandingOregon Health & Science University20000 NW Walker Rd., Beaverton, Oregon 97006{emily,roark,lmblack,vansanten}@cslu.ogi.eduAbstractAtypical or idiosyncratic language is a char-acteristic of autism spectrum disorder (ASD).In this paper, we discuss previous work iden-tifying language errors associated with atyp-ical language in ASD and describe a proce-dure for reproducing those results.
We de-scribe our data set, which consists of tran-scribed data from a widely used clinical di-agnostic instrument (the ADOS) for childrenwith autism, children with developmental lan-guage disorder, and typically developing chil-dren.
We then present methods for automati-cally extracting lexical and syntactic featuresfrom transcripts of children?s speech to 1)identify certain syntactic and semantic errorsthat have previously been found to distinguishASD language from that of children with typ-ical development; and 2) perform diagnosticclassification.
Our classifiers achieve resultswell above chance, demonstrating the poten-tial for using NLP techniques to enhance neu-rodevelopmental diagnosis and atypical lan-guage analysis.
We expect further improve-ment with additional data, features, and clas-sification techniques.1 IntroductionAtypical language and communication have been as-sociated with autism spectrum disorder (ASD) sinceKanner (1943) first gave the name autism to the dis-order.
The Autism Diagnostic Observation Sched-ule (ADOS) (Lord et al, 2002) and other widelyused diagnostic instruments include unusual worduse as a diagnostic criterion.
The broad and con-flicting definitions used in diagnostic instruments forASD, however, can lead to difficulty distinguishingthe language peculiarities associated with autism.The most recent and the most systematic study ofunusual word use in ASD (Volden and Lord, 1991)found that certain types of atypical word use weresignificantly more prevalent in ASD speech thanin the speech of children with typical development(TD).
Although the results provided interesting in-formation about unusual language in ASD, the pro-cess of coding these types of errors was laboriousand required substantial linguistic and clinical ex-pertise.In this paper, we first use our own data to repro-duce a subset of the results reported in Volden andLord (1991).
We then present a method of automat-ically identifying the types of errors associated withASD using spoken language features and machinelearning techniques.
These same features are thenused to differentiate subjects with ASD or a devel-opmental language disorder (DLD) from those withTD.
Although these linguistic features yield strongclassification results, they also reveal a number ofobstacles to distinguishing language characteristicsassociated with autism from those associated withlanguage impairment.2 Previous WorkSince it was first recognized as a neurodevelop-mental disorder, autism has been associated withlanguage described variously as: ?seemingly non-sensical and irrelevant?, ?peculiar and out of placein ordinary conversation?
(Kanner, 1946); ?stereo-typed?, ?metaphorical?, ?inappropriate?
(Bartak etal., 1975); and characterized by ?a lack of ease in88the use of words?
(Rutter, 1965) and ?the use ofstandard, familiar words or phrases in idiosyncraticbut meaningful way?
(Volden and Lord, 1991).
Thethree most common instruments used in ASD diag-nosis ?
the Autism Diagnostic Observation Sched-ule (ADOS) (Lord et al, 2002), the Autism Di-agnostic Interview-Revised (ADI-R) (Lord et al,1994), and the Social Communication Questionnaire(SCQ) (Rutter et al, 2003) ?
make reference tothese language particularities in their scoring algo-rithms.
Unfortunately, the guidelines for identify-ing this unusual language are often vague (SCQ:?odd?, ADI-R: ?idiosyncratic?, ADOS: ?unusual?
)and sometimes contradictory (ADOS: ?appropriate?vs.
ADI-R: ?inappropriate?
; ADOS: ?phrases...theycould not have heard?
vs. SCQ: ?phrases that he/shehas heard other people use?
).In what is one of the only studies focused specif-ically on unusual word use in ASD, Volden andLord (1991) transcribed two 10-minute speech sam-ples from the ADOS for 20 school-aged, high-functioning children with autism and 20 with typi-cal development.
Utterances containing non-Englishwords or the unusual use of a word or phrase wereflagged by student workers and then categorized bythe authors into one of three classes according to thetype of error:?
Developmental syntax error: a violation of asyntactic rule normally acquired in early child-hood, such as the use of object pronoun in sub-ject position or an overextension of a regularmorphological rule, e.g., What does cows do??
Non-developmental syntax error: a syntacticerror not commonly observed in the speech ofchildren acquiring language, e.g., But in the carit?s some.?
Semantic error: a syntactically intact sentencewith an odd or unexpected word given the con-text and intended meaning, e.g., They?re sidingthe table.The authors found that high-functioning chil-dren with ASD produced significantly more non-developmental and semantic errors than childrenwith typical development.
The number of develop-mental syntax errors was not significantly differentbetween these two groups.Although there has been virtually no previouswork on automated analysis of unannotated tran-scripts of the speech of children with ASD, auto-matically extracted language features have shownpromise in the identification of other neurologicaldisorders such as language impairment and cogni-tive impairment.
Gabani et al (2009) used part-of-speech language models to derive perplexity scoresfor transcripts of the speech of children with andwithout language impairment.
These scores offeredsignificant diagnostic power, achieving an F1 mea-sure of roughly 70% when used within an supportvector machine (SVM) for classification.
Roark etal.
(in press) extracted a much larger set of lan-guage complexity features derived from syntacticparse trees from transcripts of narratives producedby elderly subjects for the diagnosis of mild cogni-tive impairment.
Selecting a subset of these featuresfor classification with an SVM yielded accuracy, asmeasured by the area under the receiver operatingcharacteristic curve, of 0.73.Language models have also been applied to thetask of error identification, but primarily in writ-ing samples of ESL learners.
Gamon et al (2008)used word-based language models to detect andcorrect common ESL errors, while Leacock andChodorow (2003) used part-of-speech bigram lan-guage models to identify potentially ungrammaticaltwo-word sequences in ESL essays.
Although thesetasks differ in a number of ways from our tasks, theydemonstrate the utility of using both word and part-of-speech language models for error detection.3 Data Collection3.1 SubjectsOur first objective was to gather data in order repro-duce the results reported in Volden and Lord (1991).As shown in Table 1, the participants in our studywere 50 children ages 4 to 8 with a performanceIQ greater than 80 and a diagnosis of either typicalDiagnosis Count Age (s.d.)
IQ (s.d.
)TD 17 6.24 (1.38) 125.7 (11.63)ASD 20 6.38 (1.25) 108.9 (16.41)DLD 13 7.01 (1.10) 100.6 (10.95)Table 1: Count, mean age and IQ by subject group.89development (TD, n=17), autism spectrum disorder(ASD, n=20), or developmental language disorder(DLD, n=13).Developmental language disorder (DLD), alsosometimes known as specific language impairment(SLI), is generally defined as the delayed or im-paired acquisition of language without accompany-ing comparable delays or deficits in hearing, cogni-tion, and socio-emotional development (McCauley,2001).
The language impairments that characterizeDLD are not related to articulation or ?speech im-pediments?
but rather are associated with more pro-found problems producing and often comprehend-ing language in terms of its pragmatics, syntax, se-mantics, and phonology.
The DSM-IV-TR (Ameri-can Psychiatric Association, 2000) includes neitherDLD nor SLI as a disorder, but for the purposesof this work, DLD corresponds to the DSM?s des-ignations Expressive Language Disorder and MixedExpressive-Receptive Language Disorder.For this study, a subject received a diagnosis ofDLD if he or she met one of two commonly usedcriteria: 1) The Tomblin Epi-SLI criteria (Tomblin,et al, 1996), in which diagnosis of language im-pairment is indicated when scores in two out of fivedomains (vocabulary, grammar, narrative, receptive,and expressive) are greater than 1.25 standard devia-tions below the mean; and 2) The CELF-Preschool-2/CELF-4 criteria, in which diagnosis of languageimpairment is indicated when one out of three indexscores and one out of three spontaneous languagescores are more than one standard deviation belowthe mean.A diagnosis of ASD required a previous medi-cal, educational, or clinical diagnosis of ASD, whichwas then confirmed by our team of clinicians ac-cording to the criteria of the DSM-IV-TR (Ameri-can Psychiatric Association, 2000), the revised al-gorithm of the ADOS (Lord et al, 2002), and theSCQ parental interview (Rutter et al, 2003).
Fifteenof the 20 ASD subjects participating in this studyalso met at least one of the above described criteriafor DLD.3.2 Data PreparationThe ADOS (Lord et al, 2002), a semi-structured se-ries of activities designed to reveal behaviors asso-ciated with autism, was administered to all 50 sub-jects.
Five of the ADOS activities that require sig-nificant amounts spontaneous speech (Make-BelievePlay, Joint Interactive Play, Description of a Pic-ture, Telling a Story From a Book, and Conversa-tion and Reporting) were then transcribed at the ut-terance level for all 50 speakers.
All utterances fromthe transcripts longer than four words (11,244) werepresented to individuals blind to the purposes of thestudy, who were asked to flag any sentence withatypical or unusual word use.
Those sentences werethen classified by the authors as having no errors orone of the three error types described in Volden andLord.
Examples from our data are given in Table 2.3.3 Reproducing Previous ResultsIn order to compare our results to those reported inVolden and Lord, we calculated the rates of the threetypes of errors for each subject, as shown in Ta-ble 2.
With a two-sample (TD v. ASD) t-test, therates of nondevelopmental and semantic errors weresignificantly higher in the ASD group than in theTD group, while there was no significant differencein developmental errors between the two groups.These results reflect the same trends observed inVolden and Lord, in which the raw counts of bothdevelopmental and semantic errors were higher inthe ASD group.Using ANOVA for significance testing over allthree diagnostic groups, we found that the rate ofdevelopmental errors was significantly higher in theDLD group than in the other groups.
The differencein semantic error rate between TD and ASD usingthe t-test was preserved, but the difference in nonde-velopmental error rate was lost when comparing allthree diagnostic groups with ANOVA, as shown inFigure 1.Error ExampleDev.I have a games.The baby drinked it.The frogs was watching TV.Nondev.He locked him all of out.Would you like to be fall down?He got so the ball went each way.Sem.Something makes my eyes poke.It smells like it?s falling on your head.All the fish are leaving in the air.Table 2: Examples of error types.9000.020.040.060.08Dev.
Nondev.
Sem.TDASDDLD***Figure 1: Error rates by diagnostic group (*p <0.05).The process of manually identifying sentenceswith atypical or unusual language was relativelypainless, but determining the specific error types issubjective and time-consuming, and requires a greatdeal of expertise.
In addition, although we do ob-serve significant differences between groups, it isnot clear whether the differences are sufficient fordiagnostic classification or discrimination.We now propose automatically extracting fromthe transcripts various measures of linguistic likeli-hood, complexity, and surprisal that have the poten-tial to objectively capture qualities that differentiate1) the three types of errors described above, and 2)the three diagnostic groups discussed above.
In thenext three sections, we will discuss the various lin-guistic features we extract; methods for using thesefeatures to classify each sentence according to its er-ror type for the purpose of automatic error-detection;and methods for using these features, calculated foreach subject, for diagnostic classification.4 FeaturesN-gram cross entropy.
Following previous workin both error detection (Gamon et al, 2008; Leacockand Chodorow, 2003) and neurodevelopmental di-agnostic classification (Gabani et al, 2009), we be-gin with simple bigram language model features.
Abigram language model provides information aboutthe likelihood of a given item (e.g., a word or partof speech) in a sentence given the previous item inthat sentence.
We suspect that some of the typesof unusual language investigated here, in particularthose seen in the syntactic errors shown in Table 2,are characterized by unlikely words (drinked) andword or part-of-speech sequences (a games, all ofout) and hence might be distinguished by languagemodel-based scores.We build a word-level bigram language model anda part-of-speech level bigram language model fromthe Switchboard (Godfrey et al, 1992) corpus.
Wethen automatically generate part-of-speech tags foreach sentence (where the tags were derived fromthe best scoring output of the full syntactic parsermentioned below), and then apply the two modelsto each sentence.
For each sentence, we calculateits cross entropy and perplexity.
For a word stringw1 .
.
.
wn of length n, the cross entropy H isH(w1 .
.
.
wn) = ?1nlog P(w1 .
.
.
wn) (1)where P(w1 .
.
.
wn) is calculated as the product ofthe n-gram probabilities of each word in the string.The corresponding measure can be calculated for thePOS-tag sequence, based on an n-gram model oftags.
Perplexity is simply 2H .While we would prefer to use a corpus that iscloser to the child language that we are attemptingto model, we found the conversational style of theSwitchboard corpus to be the most effective largecorpus that we had at our disposal for this study.As the size of our small corpus grows, we intend tomake use of the text to assist with model building,but for this study, we used all out-of-domain datafor n-gram language models and parsing models.Using Switchboard also allowed us to use the samecorpus to train both n-gram and parsing models.Surprisal-based features.
Surprisal, or the unex-pectedness of a word or syntactic category in a givencontext, is often used as a psycholinguistic mea-sure of sentence-processing difficulty (Hale, 2001;Boston et al, 2008).
Although surprisal is usuallydiscussed in the context of cognitive load for lan-guage processing, we hoped that it might also cap-ture some of the language characteristics of the se-mantic errors like those in Table 2, which often con-tain common words used in surprising ways, andthe nondevelopmental syntax errors, which often in-clude strings of function words presented in an orderthat would be difficult to anticipate.To derive surprisal-based features, each sentenceis parsed using the Roark (2001) incrementaltop-down parser relying on a model built again on91the Switchboard corpus.
The incremental output ofthe parser shows the surprisal for each word, as wellas other scores, as presented in Roark et al (2009).For each sentence, we collected the mean surprisal(equivalent to the cross entropy given the model);the mean syntactic surprisal; and the mean lexicalsurprisal.
The lexical and syntactic surprisal are adecomposition of the total surprisal into that portiondue to probability mass associated with buildingnon-terminal structure (syntactic surprisal) and thatportion due to probability mass associated withbuilding terminal lexical items in the tree (lexicalsurprisal).
We refer the reader to that paper forfurther details.Other linguistic complexity measures The non-developmental syntax errors in Table 2 are charac-terized by their ill-formed syntactic structure.
Fol-lowing Roark et al (in press), in which the authorsexplored the relationship between linguistic struc-tural complexity and cognitive decline, and Sagae(2005), in which the authors used automatic syntac-tic annotation to assess syntactic development, wealso investigated the following measures of linguis-tic complexity: words per clause, tree nodes perword, dependency length per word, and Ygnve andFrazier scores per word.
Each of these scores canbe calculated from a provided syntactic parse tree,and to generate these we made use of the Charniakparser (Charniak, 2000), also trained on the Switch-board treebank.Briefly, words per clause is the total number ofwords divided by the total number of clauses; andtree nodes per word is the total number of nodesin the parse tree divided by the number of words.The dependency length for a word is the distance (inword tokens) between that word and its governor,as determined through standard head-percolationmethods from the output of the Charniak parser.
Wecalculate the mean of this length over all words inthe utterance.
The Yngve score of a word is thesize of the stack of a shift-reduce parser after thatword; and the Frazier score essentially counts howmany intermediate nodes exist in the tree betweenthe word and its lowest ancestor that is either theroot or has a left sibling in the tree.
We calculatethe mean of both of these scores over the utterance.We refer the reader to the above cited paper for moredetails on these measures.As noted in Roark et al (in press), some of thesemeasures are influenced by particular characteristicsof the Penn Treebank style trees ?
e.g., flat nounphrases, etc.
?
and measures vary in the degree towhich they capture divergence from typical struc-tures.
Some (including Yngve) are sensitive to thebreadth of trees (e.g., flat productions with manychildren); others (including Frazier) are sensitive todepth of trees.
This variability is a key reason forincluding multiple, complementary features, such asboth Frazier and Yngve scores, to capture more sub-tle syntactic characteristics than would be availablefrom any of these measures alone.Although we were not able to measure parsing ac-curacy on our data set and how it might affect the re-liability of these features, Roark et al (in press) didinvestigate this very issue.
They found that all of theabove described syntactic measures, when they werederived from automatically generated parse trees,correlated very highly (greater than 0.9) with thosemeasures when they were derived from manuallygenerated parse trees.
For the moment, we assumethat the same principle holds true for our data set,though we do intend both to verify this assump-tion and to supplement our parsing models with datafrom child speech.
Based on manual inspection ofparser output, the current parsing model does seemto be recovering largely valid structures.5 Error ClassificationThe values for 8 of the 12 features were significantlydifferent over the three error classes, as measuredby one-way ANOVA: words per clause, Yngve, de-pendency, word cross-entropy all significant at p <0.001; Frazier, nodes per word at p < 0.01; overallsurprisal and lexical surprisal at p < 0.05.
We builtclassification and regression trees (CART) using theWeka data mining software (Hall et al, 2009) us-ing all of the 12 features described above to predictwhich error each sentence contained, and we reportthe accuracy, weighted F measure, and area underthe receiver operating characteristic curve (AUC).Including all 12 features in the CART using 10-fold cross validation resulted in an AUC of 0.68,while using only those features with significantbetween-group differences yielded an AUC of 0.65.92Classifier Acc.
F1 AUCBaseline 1 41% 0.24 0.5Baseline 2 33% 0.32 0.5All features 53% 0.53 0.68Feature subset 49% 0.49 0.65Table 3: Error-type classification results.These are both substantial improvements over abaseline with an unbalanced corpus in which themost frequent class is chosen for all input items(Baseline 1) or a baseline with a balanced corpus inwhich class is chosen at random (Baseline 2), whichboth have an AUC of 0.5.
The results for each ofthese classifiers, provided in Table 3, show potentialfor automating the identification of error type.6 Diagnostic ClassificationIn Section 3, we found a number of significant dif-ferences in error type production rates across ourthree diagnostic groups.
Individual rates of errorproduction, however, provide almost no classifica-tion power within a CART (AUC = 0.51).
Perhapsthe phenomena being observed in ASD and DLDlanguage are related to subtle language features thatare less easily identified than simply the membershipof a sentence in one of these three error categories.Given the ability of our language features to dis-criminate error types moderately well, as shown inSection 5, we decided to extract these same 12 fea-tures from every sentence longer than 4 words fromthe entire transcript for each of the subjects.
Wethen took the mean of each feature over all of thesentences for each speaker.
These per-speaker fea-ture vectors were used for diagnostic classificationwithin a CART.We first performed classification over the three di-agnostic groups using the full set of 12 features de-scribed in Section 4.
This results in only modestgains in performance over the baseline that uses er-ror rates as the only features.
We then used ANOVAto determine which of the 12 features differed sig-nificantly across the three groups.
Only four fea-tures were found to be significantly different acrossthe three groups (words per clause, Yngve, depen-dency, word cross entropy), and none of them dif-ferent significantly between the ASD group and theDLD group.
As expected, classification did not im-Features Acc.
F1 AUCError rates 33% 0.32 0.51All features 42% 0.38 0.59Feature subset 40% 0.37 0.6Table 4: All subjects: Diagnostic classification results.prove with this feature subset, as reported in Table 4.Recall that 15 of the 20 ASD subjects also met atleast one criterion for a developmental language dis-order.
Perhaps the language peculiarities we observein our subjects with ASD are related in part to lan-guage characteristics of DLD rather than ASD.
Wenow attempt to tease apart these two sources of un-usual language by investigating three separate clas-sification tasks: TD vs. ASD, TD vs. DLD, andASD vs. DLD.6.1 TD vs. ASDWe perform classification of the TD and ASD sub-jects with three feature sets: 1) per-subject errorrates; 2) all 12 features described in Section 4; and3) the subset of significantly different features.
Wefound that 7 of the 12 features explored in Section 4differed significantly between the TD group and theASD group: words per clause, Yngve, dependency,word cross-entropy, overall surprisal, syntactic sur-prisal, and lexical surprisal.
Classification results areshown in Table 5.
We see that using the automati-cally derived linguistic features improves classifica-tion substantially over the baseline using per-subjecterror rates, particularly when we use the feature sub-set.
Note that the best classification accuracy resultsare comparable to those reported in related work onlanguage impairment and mild cognitive impairmentdescribed in Section 2.6.2 TD vs. DLDWe perform classification of TD and DLD subjectswith the same three feature sets used for the TDvs.
ASD classification.
We found that 6 of the 12Features Acc.
F1 AUCError rates 62% 0.62 0.56All features 62% 0.62 0.65Feature subset 68% 0.67 0.72Table 5: TD vs. ASD: Diagnostic classification results.93Features Acc.
F1 AUCError rates 67% 0.67 0.72All features 80% 0.79 0.75Feature subset 77% 0.75 0.66Table 6: TD vs. DLD: Diagnostic classification results.features explored in Section 4 different significantlybetween the TD group and the ASD group: wordsper clause, Yngve, dependency, word cross-entropy,overall surprisal, and lexical surprisal.
Note that thisis a subset of the features that differed between theTD group and ASD group.
Classification results areshown in Table 6.
Interestingly, using per-subject er-ror rates for classification of TD and DLD subjectswas quite robust.
Using all of the features improvedclassification somewhat, while using only a subsetresulted in degraded performance.
We see that thediscriminative power of these features is superior tothat reported in earlier work using LM-based fea-tures for classification of specific language impair-ment (Gabani et al, 2009).6.3 ASD vs. DLDFinally, we perform classification of the ASD andDLD subjects using only the first two featuressets, since there were no features found to be evenmarginally significantly different between these twogroups.
Classification results, which are dismal forboth feature sets, are shown in Table 7.6.4 DiscussionIt seems quite clear that the error rates, feature val-ues, and classification performance are all being in-fluenced by the fact that a majority of the ASD sub-jects also meet at least one criterion for a develop-mental language disorder.
Neither error rates norfeature values could discriminate between the ASDand DLD group.
Nevertheless we see that our ASDgroup and DLD group do not follow the same pat-terns in their error production or language featurescores.
Clearly there are differences in the languageFeatures Acc.
F1 AUCError rates 55% 0.52 0.48All features 58% 0.44 0.40Table 7: ASD vs. DLD: Diagnostic classification results.patterns of the two groups that are not being cap-tured with any of the methods discussed here.We also observe that the error rates them-selves, while sometimes significantly differentacross groups as originally observed in Volden andLord, do not perform well as diagnostic featuresfor ASD in our framework.
Volden and Lord didnot attempt classification in their study, so it is notknown whether the authors would have encounteredthe same problem.
There are, however, a numberof possible explanations for a discrepancy betweenour results and theirs.
First, our data was gath-ered from pre-school and young school-aged chil-dren, while the Volden and Lord subjects were gen-erally teenagers and young adults.
The way in whichtheir spoken language samples were elicited allowedVolden and Lord to use raw error counts rather thanerror rates.
There may also have been important dif-ferences in the way we carried out the manual er-ror identification process, despite our best efforts toreplicate their procedure.
Further development ofour classification methods and additional data col-lection are needed to determine the utility of errortype identification for diagnostic purposes.7 Future WorkAlthough our classifiers using automatically ex-tracted features were generally robust, we expectthat including additional classification techniques,subjects (especially ASD subjects without DLD),and features will further improve our results.
Inparticular, we would like to explore semantic andlexical features that are less dependent on linear or-der and syntactic structure, such as Resnik similarityand features derived using latent semantic analysis.We also plan to expand the training input forthe language model and parser to include children?sspeech.
The Switchboard corpus is conversationalspeech, but it may fail to adequately model many lin-guistic features characteristic of small children.
TheCHILDES database of children?s speech, althoughit is not large enough to be used on its own for ouranalysis and would require significant manual syn-tactic annotation, might provide enough data for usto adapt our models to the child language domain.Finally, we would like to investigate how infor-mative the error types are and whether they can be94reliably coded by multiple judges.
When we exam-ined the output of our error-type classifier, we no-ticed that many of the misclassified examples couldbe construed, upon closer inspection, as belongingto multiple error classes.
The sentence He?s flyingin a lily-pond, for instance, could contain a devel-opmental error (i.e., the child has not yet acquiredthe correct meaning of in) or a semantic error (i.e.,the child is using the word flying instead of swim-ming).
Without knowing the context in which thesentence was uttered, it is not possible to determinethe type of error through any manual or automaticmeans.
The seemingly large number of misclassifi-cations of sentences like this indicates the need forfurther investigation of the existing coding proce-dure and in-depth classification error analysis.8 ConclusionsOur method of automatically identifying error typeshows promise as a supplement to, or substitute for,the time-consuming and subjective manual codingprocess described in Volden and Lord (Volden andLord, 1991).
However, the superior performance ofour automatically extracted language features sug-gests that perhaps it may not be the errors them-selves that characterize the speech of children withASD and DLD but rather a preference for certainstructures and word sequences that sometimes mani-fest themselves as clear language errors.
Such varia-tions in complexity and likelihood might be too sub-tle for humans to reliably observe.In summary, the methods explored in this papershow potential for improving diagnostic discrimina-tion between typically developing children and thosewith these neurodevelopmental disorders.
Furtherresearch is required, however, in finding the most re-liable markers that can be derived from such spokenlanguage samples.AcknowledgmentsThis work was supported in part by NSF Grant#BCS-0826654; an Innovative Technology forAutism grant from Autism Speaks; and NIH NIDCDgrant #1R01DC007129-01.
Any opinions, findings,conclusions or recommendations expressed in thispublication are those of the authors and do not nec-essarily reflect the views of the NSF, Autism Speaks,or the NIH.
Thanks to Meg Mitchell and CherylGreene for their assistance with this project.ReferencesAmerican Psychiatric Association.
2000.
DSM-IV-TR:Diagnostic and Statistical Manual of Mental Disor-ders.
American Psychiatric Publishing, Washington,DC, 4th edition.Laurence Bartak, Michael Rutter, and Anthony Cox.1975.
A comparative study of infantile autismand specific developmental receptive language disor-der.
I.
The children.
British Journal of Psychiatry,126:27145.Mariss Ferrara Boston, John Hale, Reinhold Kliegl, andShravan Vasishth.
2008.
Surprising parser actionsand reading difficulty.
In Proceedings of ACL-08:HLT,Short Papers.Eugene Charniak.
2000.
A maximum-entropy-inspiredparser.
In Proceedings of the 1st Conference of theNorth American Chapter of the ACL, pages 132?139.Keyur Gabani, Melissa Sherman, Thamar Solorio, andYang Liu.
2009.
A corpus-based approach for theprediction of language impairment in monolingual en-glish and spanish-english bilingual children.
In Pro-ceedings of NAACL-HLT, pages 46?55.Michael Gamon, Jianfeng Gao, Chris Brockett, andRe Klementiev.
2008.
Using contextual speller tech-niques and language modeling for ESL error correc-tion.
In Proceedings of IJCNLP.John J. Godfrey, Edward Holliman, and Jane McDaniel.1992.
SWITCHBOARD: telephone speech corpus forresearch and development.
In Proceedings of ICASSP,volume 1, pages 517?520.John T. Hale.
2001.
A probabilistic Earley parser asa psycholinguistic model.
In Proceedings of the 2ndmeeting of NAACL.Mark Hall, Eibe Frank, Geoffrey Holmes, BernhardPfahringer, Peter Reutemann, and Ian H. Witten.2009.
The WEKA data mining software: An update.SIGKDD Explorations, 11(1).Leo Kanner.
1943.
Autistic disturbances of affectivecontent.
Nervous Child, 2:217?250.Leo Kanner.
1946.
Irrelevant and metaphorical lan-guage.
American Journal of Psychiatry, 103:242?246.Claudia Leacock and Martin Chodorow.
2003.
Auto-mated grammatical error detection.
In M.D.
Shermisand J. Burstein, editors, Automated essay scoring: Across-disciplinary perspective.
Lawrence Erlbaum As-sociates, Inc., Hillsdale, NJ.Catherine Lord, Michael Rutter, and Anne LeCouteur.1994.
Autism diagnostic interview-revised: A revised95version of a diagnostic interview for caregivers of in-dividuals with possible pervasive developmental disor-ders.
Journal of Autism and Developmental Disorders,24:659?685.Catherine Lord, Michael Rutter, Pamela DiLavore, andSusan Risi.
2002.
Autism Diagnostic ObservationSchedule (ADOS).
Western Psychological Services,Los Angeles.Rebecca McCauley.
2001.
Assessment of language dis-orders in children.
Lawrence Erlbaum Associates,Mahwah, NJ.Brian Roark, Asaf Bachrach, Carlos Cardenas, andChristophe Pallier.
2009.
Deriving lexical and syn-tactic expectation-based measures for psycholinguisticmodeling via incremental top-down parsing.
In Pro-ceedings of EMNLP, pages 324?333.Brian Roark, Margaret Mitchell, John-Paul Hosom,Kristina Hollingshead, and Jeffrey Kaye.
in press.Spoken language derived measures for detecting mildcognitive impairment.
IEEE Transactions on Audio,Speech and Language Processing.Brian Roark.
2001.
Probabilistic top-down parsingand language modeling.
Computational Linguistics,27(2):249?276.Michael Rutter, Anthony Bailey, and Catherine Lord.2003.
Social Communication Questionnaire (SCQ).Western Psychological Services, Los Angeles.Michael Rutter.
1965.
Speech disorders in a series ofautistic children.
In A. Franklin, editor, Children withcommunication problems, pages 39?47.
Pitman.Kenji Sagae, Alon Lavie, and Brian MacWhinney.
2005.Automatic measurement of syntactic development inchild language.
In Proceedings of the 43rd AnnualMeeting of the ACL.Joanne Volden and Catherine Lord.
1991.
Neologismsand idiosyncratic language in autistic speakers.
Jour-nal of Autism and Developmental Disorders, 21:109?130.96
