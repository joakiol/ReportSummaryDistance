Proceedings of the ACL-IJCNLP 2009 Student Research Workshop, pages 63?71,Suntec, Singapore, 4 August 2009. c?2009 ACL and AFNLPOptimizing Language Model Information Retrieval System withExpectation Maximization AlgorithmJustin Liang-Te ChiuDepartment of Computer Scienceand Information Engineering,National Taiwan University#1 Roosevelt Rd.
Sec.
4, Taipei,Taiwan 106, ROCb94902009@ntu.edu.twJyun-Wei HuangDepartment of Computer Scienceand Engineering,Yuan Ze University#135 Yuan-Tung Road, Chungli,Taoyuan,Taiwan,ROCs976017@mail.yzu.edu.twAbstractStatistical language modeling (SLM) hasbeen used in many different domains for dec-ades and has also been applied to informationretrieval (IR) recently.
Documents retrievedusing this approach are ranked accordingtheir probability of generating the givenquery.
In this paper, we present a novel ap-proach that employs the generalized Expecta-tion Maximization (EM) algorithm to im-prove language models by representing theirparameters as observation probabilities ofHidden Markov Models (HMM).
In the expe-riments, we demonstrate that our method out-performs standard SLM-based and tf.idf-based methods on TREC 2005 HARD Trackdata.1 IntroductionIn 1945, soon after the computer was invented,Vannevar Bush wrote a famous article---?As wemay think?
(V. Bush, 1996), which formed thebasis of research into Information Retrieval (IR).The pioneers in IR developed two models forranking: the vector space model (G. Salton andM.
J. McGill, 1986) and the probabilistic model(S. E. Robertson and S. Jones, 1976).
Since then,the research of classical probabilistic models ofrelevance has been widely studied.
For example,Robertson (S. E. Robertson and S. Walker, 1994;S. E. Robertson, 1977) modeled word occur-rences into relevant or non-relevant classes, andranked documents according to the probabilitiesthey belong to the relevant one.
In 1998, Ponteand Croft (1998) proposed a language modelingframework which opens a new point of view inIR.
In this approach, they gave up the model ofrelevance; instead, they treated query generationas random sampling from every document model.The retrieval results were based on the probabili-ties that a document can generate the query string.Several improvements were proposed after theirwork.
Song and Croft (1999), for example, wasthe first to bring up a model with bi-grams andGood Turing re-estimation to smooth the docu-ment models.
Latter, Miller et al (1999) usedHidden Markov Model (HMM) for ranking,which also included the use of bigrams.HMM, firstly introduced by Rabiner and Juain(1986) in 1986, has been successfully appliedinto many domains, such as named entity recog-nition (D. M. Bikel et al, 1997), topic classifica-tion (R. Schwartz et al, 1997), or speech recog-nition (J. Makhoul and R. Schwartz, 1995).
Inpractice, the model requires solving three basicproblems.
Given the parameters of the model,computing the probability of a particular outputsequence is the first problem.
This process is of-ten referred to as decoding.
Both Forward andBackward procedure are solutions for this prob-lem.
The second problem is finding the mostpossible state sequence with the parameters ofthe model and a particular output sequence.
Thisis usually completed with Viterbi algorithm.
Thethird problem is the learning problem of HMMmodels.
It is often solved by Baum-Welch algo-rithm (L. E. Bmjm et al, 1970).
Given training63data, the algorithm computes the maximum like-lihood estimates and posterior mode estimate.
Itis in essence a generalized Expectation Maximi-zation (EM) algorithm which was first explainedand given name by Dempster, Laird and Rubin(1977) in 1977.
EM can estimate the maximumlikelihood of parameters in probabilistic modelswhich has unseen variables.
Nonetheless, in ourknowledge, the EM procedure in HMM has nev-er been used in IR domain.In this paper, we proposed a new languagemodel approach which models the user queryand documents as HMM models.
We then usedEM algorithm to maximize the probability ofquery words in our model.
Our assumption isthat if the word?s probability in a document ismaximized, we can estimate the probability ofgenerating the query word from documents moreconfidently.
Because they not only been calcu-lated by language modeling view features, butalso been maximized with statistical methods.Therefore the imprecise cases caused by specialdistribution in language modeling approach canbe further prevented in this way.The remainders of this paper are organized asfollows.
We review two related works in Section2.
In Section 3, we introduce our EM IR ap-proach.
Section 4 compares our results to twoother approaches proposed by Song and Corft(1999) and Robertson (1995) based on the datafrom TREC HARD track (J. Allan, 2005).
Sec-tion 5 discusses the effectiveness of our EMtraining and the EM-based document weightingwe proposed.
Finally, we conclude our paper inSection 6 and provide some future directions atSection 7.2 Related WorksEven if we only focus on the probabilistic ap-proach to IR, it is still impossible to discuss allup-to-date research.
Instead we focus on twoprevious works which have inspired the workreported in this paper: the first is a general lan-guage model approach proposed by Song andCroft (1999) and the second is a HMM approachby Miller et al (1999).2.1 A General Language Model for IRIn 1999, Song and Croft (1999) introduced a lan-guage model based on a range of data smoothingtechnique.
The following are some of the fea-tures they used:Good-Turing estimate: Since the effect ofGood-Turing estimate was verified as one of thebest discount methods (C. D. Manning and H.Schutze, 1999), Song and Croft used Good-Turing estimate for allocating proper probabilityfor the missing terms in the documents.
Thesmoothed probability for term t in document dcan be obtained with the following formula:| 	 1where Ntf is the number of terms with frequencytf in a document.
Nd is the total number of termsoccurred in document d, and a powerful smooth-ing function S(Ntf), which is used for calculatingthe expected value of Ntf regardless of the Ntf ap-pears in the corpus or not.Expanding document model: The documentmodel can be viewed as a smaller part of wholecorpus.
Due to its limited size, there is a largenumber of missing terms in documents, and canlead to incorrect distributions of known terms.For dealing with the problem, documents can beexpanded with the following weightedsum/product approach:| 	   |  1     | 	 |  where  is a weighting parameter between 0 and1.Modeling Query as a Sequence of Terms:Treating a query as a set of terms is commonlyseen in IR researches.
Song and Croft treatedqueries as a sequence of terms, and obtained theprobability of generating the query by multiply-ing the individual term probabilities. ! " #| 	 $%|%&where t1, t2 ?, tm is the sequence of terms in aquery Q.Combining the Unigram Model with theBigram Model: This is commonly implementedwith interpolation in statistical language model-ing:%, %| 	 (  %|  ()  )%, %|where ( and () are two parameters, and ( + ()= 1.
Such interpolation can be modeled by HMM,and can learn the appropriate value from the cor-pus through EM procedure.
A similar procedureis described in Hiemstra and Vries (2000).2.2 A HMM Information Retrieval System64Miller et al demonstrated an IR system based onHMM.
With a query Q, Miller et al tried to rankthe documents according to the probability thatD is relevant (R) with it, which can be written asP(D is R|Q).
With Baye?s rule, the core formulaof their approach is:* is .|# 	 #|* is . ?
* is .#where P(Q|D is R) is the probability of query Qbeing posed by a relevant document D; P(D is R)is the prior probability that D is relevant; P(Q) isthe prior probability of Q.
Because P(Q) will beidentical, and the P(D is R) is assumed to be con-stant across all documents, they place their focuson  P(Q|D is R).To figure out the value of P(Q|D is R), theyestablished a HMM.
The union of all words ap-pearing in the corpus is taken as the observation,and each different mechanism of query wordgeneration represent a state.
So the observationprobability from different states is according tothe output distribution of the state.Figure 1.
HMM proposed in ?A Hidden MarkovModel Information Retrieval System?To estimate the transition and observationprobabilities of HMM, EM algorithm is the stan-dard method for parameter estimation.
However,due to some difficulty, they make two practicalsimplifications.
First, they assume the transitionprobabilities are same for all documents, sincethey establish an individual HMM for each doc-ument.
Second, they completely abandon the EMalgorithm for the estimation of observation prob-abilities.
Instead, they use simple maximum like-lihood estimates for each documents.
So theprobabilities which their HMM generate term qfrom their HMM states become:Pq|D3 	 number of times q appears in D3length of D3Pq|GE 	 ?
number of times q appears in D33 ?
length of D33with these estimated parameters, they state theformula for P(Q|D is R) corresponding to Figure1 as:PQ|D3 is R 	  $aGPq|GE   aPq|D3HIJthe probabilities obtained through this formulais then used for calculating the P(D is R|Q).
Thedocument is then ranked according to the valueof P(D is R|Q).The HMM model we proposed is far differentfrom Miller et al (1999).
They build HMM forevery document, and treat all words in the docu-ment as one state?s observation, and word that isunrelated to the document, but occurs commonlyin natural language queries as another state?s ob-servation.
Hence, their approach requires infor-mation about the words which appears common-ly in natural language.
The content of the pro-vided information will also affect the IR result,hence it is unstable.
We assume that every doc-ument is an individual state, and the probabilitiesof query words generated by this document asthe observation probabilities.
Our HMM modelis built on the corpus we used and does not needfurther information.
This will make our IR resultfit on our corpus and not affected by outside in-formation.
It will be detailed introduced at Sec-tion 3.3 Our EM IR approachWe formulate the IR problem as follows: given aquery string and a set of documents, we rank thedocuments according to the probability of eachdocument for generating the query terms.
Sincethe EM procedure is very sensitive to the numberof states, while a large number of states takemuch time for one run, we firstly apply a basiclanguage modeling method to reduce our docu-ment set.
This language modeling method will bedetailed at Section 3.1.
Based on the reduceddocument set, we then describe how to build ourHMM model, and demonstrate how to obtain thespecial-designed observance sequence for ourHMM training in Section 3.2 and 3.3, respective-ly.
Finally, Section 3.4 introduces the evaluationmechanism to the probability of generating thequery for each document.3.1 The basic language modeling methodfor document reduction65Suppose we have a huge document set D, and aquery Q, we firstly reduce the document set toobtain the document Dr. We require the reducingmethod can be efficiently computed, thereforetwo methods proposed by Song and Croft (1999)are consulted with some modifications: Good-Turing estimation and modeling query as a se-quence of terms.In our modified Good-Turing estimation, wegathered the number of terms to calculate theterm frequency (tf) information in our documentset.
Table 1 shows the term distribution of theAQUAINT corpus which is used in the TREC2005 HARD Track (J. Allan, 2005).
The detail ofthe dataset is described in Section 4.1.tf Ntf tf Ntf0 1,140,854,966,460 5 3,327,6331 166,056,563 6 2,163,5382 29,905,324 7 1,491,2443 11,191,786 8 1,089,4904 5,668,929 9 819,517Table 1.
Term distribution in AQUAINT corpusIn this table, Ntf is the number of terms withfrequency tf in a document.
The tf = 0 case in thetable means the number of words not appear in adocument.
If the number of all word in our cor-pus is W, and the number of word in a documentd is wd, then for each document, the tf = 0 willadd W ?
wd.
By listing all frequency in our doc-ument set, we adapt the formula defined in (Songand Croft, 1999) as follows:| 	 1In our formula, the Nd means the number of wordtokens in the document d. Moreover, the smooth-ing function is replaced with accurate frequencyinformation, Ntf and Ntf+1.
Obviously, there couldbe two problems in our method: First, while inhigh frequency, there might be some missingNtf+1, because not all frequency is continuouslyappear.
Second, the Ntf+1 for the highest tf is zero,this will lead to its PmGT become zero.
Therefore,we make an assumption to solve these problems:If the Ntf+1 is missing, then its value is the sameas Ntf.
According to Table 1, we can find out thatthe difference between tf and tf+1 is decreasingwhen the tf becomes higher.
So we assume thedifference becomes zero when we faced themissing frequency at a high number.
This as-sumption can help us ensure the completeness ofour frequency distribution.Aside from our Good-Turing estimation de-sign, we also treat query as a sequence of terms.There are two reasons to make us made this deci-sion.
By doing so, we will be able to handle theduplicate terms in the query.
Furthermore, it willenable us to model query phrase with local con-texts.
So our document score with this basic me-thod can be calculated by multiplying PmGT(q|d)for every q in Q.
We can obtain Dr with the top50 scores in this scoring method.3.2 HMM model for EM IROnce we have the reduced document set Dr, wecan start to establish our HMM model for EM IR.This HMM is designed to use the EM procedureto modify its parameters, and its original parame-ters are given by the basic language modelingapproach calculation.Figure 2.
HMM model for EM IRWe define our HMM model as a four-tuple,{S,A,B,?
}, where S is a set of N states, A is aNN matrix of state transition probabilities, B isa set of N probability functions, each describingthe observation probability with respect to a stateand pi is the vector of the initial state probabili-ties.In our HMM model, it composes of |Dr|+1states.
Every document in the document set istreated as an individual state in our HMM model.Aside from these document states, we add a spe-cial state called ?Initial State?.
This state is theonly one not associate with any document in ourdocument sets.
Figure 2 illustrates the proposedHMM IR model.The transition probabilities in our HMM canbe classified into two types.
For the ?InitialState?, the transition to the other state can be re-gard as the probability of choosing that docu-ment.
We assume that every document has thesame probability to be chosen at the beginning,so the transition probabilities for ?Initial State?are 1/|Dr| to every document state.
For the docu-66ment states, their transition probabilities arefixed: 100% to the ?Initial State?.
Since the tran-sition between documents has no statisticalmeaning, we make the state transition after thedocument state back to the Initial State.
This de-sign helps us to keep the independency betweenthe query words.
We will detail this part at Sec-tion 3.3.The observation probabilities for each state aresimilar with the concept of language modeling.There are three types of observations in ourHMM model.Firstly, for every document, we can obtain theobservation probability for each query term ac-cording to our basic language modeling method.Even if the query term is not in the document, itwill be assigned a small value according to themethod described in Section 3.1.Secondly, for the terms in a document, whichis not part of our query terms, are treating asanother observation.
Since we mainly focus onthe probability of generating the query termsfrom the documents, the rest terms are treated asthe same type which means ?not the query term?.The last type of observation is a special im-posed token ?$?
which has 100% observationprobability at the Initial State.Figure 3 shows a complete built HMM modelfor EM IR.
The transition probability from InitialState is labeled with trans(dn), and the observa-tion probability in the document state and InitialState is showed with ?ob?.
The ?N?
symbolrepresents the ?not the query term?.
Summing allthe token mentioned above, all possible observa-tions for our HMM model are |Q|+2.
The possi-ble observation for each state is bolded, so wecan see the difference between Initial State andDocument State.Figure 3.
A complete built HMM model for EMIR with parametersFor Initial State, the observations are fixed with100% for $ token.
This special token help weensure the independency between the queryterms.
The effect of this token will be discussedin Section 3.3.
For the document states, the prob-abilities for the query terms are calculated withthe simple language modeling approach.
Even ifthe query term is not in the document, it will beassigned a small value according to the basiclanguage modeling method.
The rest of the termsin a document are treating as another kind of ob-servation, which is the ?N?
symbol in the Figure3.
Since we mainly focus on the probability ofgenerating the query terms from the documents,the rest of the words are treated as the same kindwhich means ?not the query term?.
Additionally,each document state represents a document, sothe $ token will never been observed in them.3.3 The observance sequence and HMMtraining procedureAfter establishing the HMM model, the observa-tion sequence is another necessary part for ourHMM training procedure.
The observation se-quence used in HMM training means the trendfor the observation while running HMM.
In ourapproach, since we want to find out the docu-ment which is more related with our query, so weuse the query terms as our observation sequence.During the state transition with query, we canmaximize the probability for each document togenerate our query.
This will help us figure outwhich document is more related with our query.Due to the state transitions in the proposedHMM model are required to go back to the Ini-tial State after transiting to the document state,generating the pure query terms observation se-quence is impossible, because the Initial Statewon?t produce any query term.
Therefore, weadd the $ token into our observation sequencebefore each query terms.
For instance, if we arerunning a HMM training with query ?a b c?, theexact observation sequence for our HMM train-ing becomes ?$ a $ b $ c?.
Additionally, eachdocument state represents a document, so the $token will never been observed in them.
By tun-ing our HMM model with the data from ourquery instead of other validation data, we canfocus on the document we want more precisely.The reason why we use this special setting forEM training procedure is because we are tryingto maintain the independency assumption forquery terms in HMM.
The HMM observancesequence not only shows the trend of this mod-el?s observation, but also indicate the dependen-cy between these observations.
However, theindependency between all query terms is a com-mon assumption for IR system (F. Song and W.B.
Croft, 1999; V. Lavrenko and W. B. Croft,672001; A. Berger and J. Lafferty, 1999).
To en-sure this assumption still works in our HMMsystem, we use the Initial State to separate eachtransition to the document state and observe thequery terms.
No matter the early or late the queryterm t occurs, the training procedure is fixed as?Starting from the Initial state and observed $,transit to a document state, and observe t?.We?ve made experiments to verify the indepen-dency assumption still work, and the result re-mains the same no matter how we change theorder of our query terms.After constructing the HMM model and theobservance sequence, we can start our EM train-ing procedure.
EM algorithm is used for findingmaximum likelihood estimates of parameters inprobabilistic models, where the model dependson unobserved latent variables.
In our experi-ment, we use EM algorithm to find the parame-ters of our HMM model.
These parameters willbe used for information retrieval.
The detail im-plementation information can be found in (C. D.Manning and H. Schutze, 1999), which introduceHMM and the training procedure very well.3.4 Scoring the documents with EM-trainedHMM modelWhen the training procedure is completed, eachdocument will have new parameters for theword?s observation probability.
Moreover, thetransition probabilities from Initial State to thedocument state are no longer uniform due to theEM training.
So the probability for a document dto generate the query Q becomes:#| 	 trans K$L|!IMIn this formula, the trans(d) means the transi-tion probability from the Initial State to the doc-ument state of d, which we called ?EM-baseddocument weighting?.
The P(q|d) means the ob-servation probability for query term q in docu-ment  state of d, which is also tuned in our EMtraining procedure.
With this formula, we canrank the IR result according to this probability.This performs better than the GLM when thedocument size is relatively small, since GLMgives those documents as with too high score.4 Experiment Results4.1 Data SetWe use the AQUAINT corpus as our trainingdata set.
It is used in the TREC 2005 HARDTrack (J. Allan, 2005).
The AQUAINT corpus isprepared by the LDC for the AQUAINT Project,and is used in official benchmark evaluationsconducted by National Institute of Standards andTechnology (NIST).
It contains news from threesources: the Xinhua News Service (People's Re-public of China), the New York Times NewsService, and the Associated Press WorldstreamNews Service.The topics we used are the same as the TRECRobust track (E. M. Voorhees, 2005), which arethe topics from number 303 to number 689 of theTREC topics.
Each topic is described in threeformats including titles, descriptions and narra-tives.
In our experiment, due to the fact that ourobservation sequence is very sensitive to thequery terms, we only focus on the title part of thetopic.
In this way, we can avoid some commonlyappeared words in narratives or descriptions,which may reduce the precision of our trainingprocedure for finding the real document.
Table 2shows the detail about the corpus.Datasize 2.96GB#Documents 1,030,561#Querys 50Term Types 2,002,165Term Tokens 431,823,255Table 2.
Statistics of the AQUAINT corpus4.2 Experiment Design and ResultsBy using the AQUAINT corpus, two differenttraditional IR methods are implemented for com-paring.
The two IR methods which we use asbaselines are the General Language Modeling(GLM) proposed by Song and Croft (1999) andthe tf.idf measure proposed by Robertson (1995).The GLM has been introduced in Section 2.
Thefollowing formulas show the core of tf.idf:tf.
idf#, * 	 P wtfL%, * ?
idfL%!RIMwtfL, * 	 tfL, *tfL, *  0.5  1.5 U*VUidfLlog W!  1N is the number of documents in the corpus; nq isthe number of documents in the corpus contain-ing q; tf(q, D) is the number of times q appears inD; l(D) is the length of D in words and the al isthe average length in words of a D in the corpus.68For the proposed EM IR approach, two confi-gurations are listed to compare.
The first (Con-fig.1) is the proposed HMM model without mak-ing use of the EM-based document weightingthat is don?t multiply the transition probability,trans(d), in equation (2).
The second (Config.2)is the HMM model with EM-based documentweighting.
The comparison is based on precision.For each problem, we retrieved the documentswith the highest 20 scores, and divided the num-ber of correct answer with the number of re-trieved document to obtain precision.
If there aredocuments with same score at the rank of 20, allof them will be retrieved.Methods Precision %Change %Changetf.idf 29.7% -GLM 30.5% 2.69% -Config.1 28.8% -5.58% -3.14%Config.2 32.2% 8.41% 5.57%Table 3.
Experiment Results of three IR methodson the AQUAINT corpusAs shown in Table 3, our EM IR system out-performs tf.idf method 8.41% and GLM method5.57%.5 DiscussionIn this section, we will discuss the effective-ness of the EM-based document weighting andthe EM procedure.
Both of them rely on theHMM design we have proposed.5.1 The effectiveness of EM-based docu-ment weightingWhen we establish our HMM model, the transi-tion probability from Initial State to the docu-ment state is assigned as uniform, since we don?thave any information about the importance ofevery document.
These transition probabilitiesrepresent the probability of choosing the docu-ment with the given observation sequence.During EM training procedure, the transitionprobability, exclusive the transition probabilityfrom document states which is fixed to 100% tothe Initial State, will be re-estimated according tothe observation sequence (the query) and the ob-servation probabilities of each state.
As shown inTable 3, two configurations (Config.1 and Con-fig.2) are conducted to verify the effectiveness ofusing the transition probability.The transition probability works due to theEM training procedure.
The training procedureworks for maximizing the probability for gene-rating the query words, so the weight for eachdocument will be given according to mathemati-cal formula.
The advantage of this mechanism isit will use the same formula regardless of differ-ent content of document.
Yet other statistical me-thods will have to fix the content or formula pre-viously to avoid the noise or other disturbance.Some researches employee the number of termsin the document to calculate the documentweighting.
Since the observation probability al-ready use the number of words in a document Ndas a parameter, using number of words as docu-ment weight will make it affect too much in oursystem.The experiment results show an improvementof 11.80% by using the transition probability ofInitial State.
Accordingly, we can understand thatthe EM procedure helps our HMM model notonly on the observation probability of generatingquery words, but also suggests a useful weightfor each document.5.2 The effectiveness of EM trainingIn HMM model training, the iteration numbers ofEM procedure is always a tricky issue for expe-riment design.
While training with too much ite-ration will lead to overfitting for the observationsequence, to less iteration will weaken the effectof EM training.For our EM IR system, we?ve made a series ofexperiments with different iterations for examin-ing the effect of EM training.
Figure 3 shows theresults.Figure 4.
The precision change with the EMtraining iterationsAs you can see in Figure 4, the precision in-creased with the iteration numbers.
Still, thegrowing rate of precision becomes very slowafter 2 iterations.
We have analysis this resultand find out two possible causes for this evi-dence.
First, the training document sets are li-mited in a small size due to the computation timecomplexity for our approach.
Therefore we canonly retrieve correct document with high score in30.430.630.83131.231.431.631.83232.232.40 1 2 3 4 5Precision(%)Iterations69basic language modeling, which is used for doc-ument reduction.
So the precision is also limitedwith the performance of our reducing methods.The number of correct answer is limited by thebasic language modeling, so as the highest preci-sion our system can achieve.
Second, our obser-vation only composed query terms, which gives alimited improving space.6 ConclusionWe have proposed a method for using EM algo-rithm to improve the precision in informationretrieval.
This method employees the concept oflanguage model approach, and merge it with theHMM.
The transition probability in HMM istreated as the probability of choosing the docu-ment, and the observation probability in HMM istreated as the probability of generating the termsfor the document.
We also implement this me-thod, and compare it with two existing IR me-thods with the dataset from TREC 2005 HARDTrack.
The experiment results show that the pro-posed approach outperforms two existing me-thods by 2.4% and 1.6% in precision, which are8.08% and 5.24% increasing for the existing me-thod.
The effectiveness of using the tuned transi-tion probability and EM training procedure isalso discussed, and been proved can work effec-tively.7 Future WorkSince we have achieved such improvement withEM algorithm, other kinds of algorithm withsimilar functions can also be tried in IR system.It might be work in the form of parameter re-estimation, tuning or even generating parametersby statistical measure.For the method we have proposed, we alsohave some part can be done in the future.
Findinga better observance sequence will be an impor-tant issue.
Since we use the exact query terms asour observance sequence, it?s possible to use themethod like statistical translation to generatemore words which are also related with the doc-uments we want and used as observance se-quence.Another possible issue is to integrate the bi-gram or trigram information into our trainingprocedure.
Corpus information might be used inmore delicate way to improve the performance.ReferencesA.
Berger and J. Lafferty, "Information retrieval asstatistical translation," 1999, pp.
222-229.A.
P. Dempster, N. M. Laird, and D. B. Rubin, "Max-imum likelihood from incomplete data via the EMalgorithm," Journal of the Royal Statistical Society,vol.
39, pp.
1-38, 1977.C.
D. Manning and H. Schutze, Foundations of statis-tical natural language processing: MIT Press,1999.D.
Hiemstra and A. P. de Vries, Relating the new lan-guage models of information retrieval to the tradi-tional retrieval models: University of Twente[Host]; University of Twente, Centre for Telemat-ics and Information Technology, 2000.D.
M. Bikel, S. Miller, R. Schwartz, and R. Weische-del, "Nymble: a high-performance learning name-finder," 1997, pp.
194-201.D.
R. H. Miller, T. Leek, and R. M. Schwartz, "Ahidden Markov model information retrieval sys-tem," 1999, pp.
214-221.E.
M. Voorhees, "The TREC robust retrieval track,"2005, pp.
11-20.F.
Song and W. B. Croft, "A general language modelfor information retrieval," 1999, pp.
316-321.G.
Salton and M. J. McGill, Introduction to ModernInformation Retrieval: McGraw-Hill, Inc. NewYork, NY, USA, 1986.J.
Allan, "HARD track overview in TREC 2005: Highaccuracy retrieval from documents," 2005.J.
Makhoul and R. Schwartz, "State of the Art in Con-tinuous Speech Recognition," Proceedings of theNational Academy of Sciences, vol.
92, pp.
9956-9963, 1995.J.
M. Ponte and W. B. Croft, "A language modelingapproach to information retrieval," 1998, pp.
275-281.L.
E. Bmjm, T. Petrie, G. Soules, and N. Weiss, "AMAXIMIZATION TECHNIQUE OCCURRINGIN THE STATISTICAL ANALYSIS OF PROB-ABILISTIC FUNCTIONS OF MARKOVCHAINS," The Annals of Mathematical Statistics,vol.
41, pp.
164-171, 1970.L.
Rabiner and B. Juang, "An introduction to hiddenMarkov models," ASSP Magazine, IEEE [see alsoIEEE Signal Processing Magazine], vol.
3, pp.
4-16, 1986.R.
Schwartz, T. Imai, F. Kubala, L. Nguyen, and J.Makhoul, "A Maximum Likelihood Model forTopic Classification of Broadcast News," 1997.S.
E. Robertson, "The probability ranking principle inIR," Journal of Documentation, vol.
33, pp.
294-304, 1977.70S.
E. Robertson and S. Jones, "Relevance Weightingof Search Terms," Journal of the American Societyfor Information Science, vol.
27, pp.
129-46, 1976.S.
E. Robertson and S. Walker, "Some simple effec-tive approximations to the 2-Poisson model forprobabilistic weighted retrieval," 1994, pp.
232-241.S.
E. Robertson, S. Walker, and S. Jones, "M. Han-cock-Beaulieu, M., and Gatford, M.(1995).
Okapiat TREC-3," pp.
109?126.V.
Bush, "As we may think," interactions, vol.
3, pp.35-46, 1996.V.
Lavrenko and W. B. Croft, "Relevance based lan-guage models," 2001, pp.
120-127.71
