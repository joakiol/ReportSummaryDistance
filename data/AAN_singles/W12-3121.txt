Proceedings of the 7th Workshop on Statistical Machine Translation, pages 163?170,Montre?al, Canada, June 7-8, 2012. c?2012 Association for Computational LinguisticsCombining Quality Prediction and System Selection for ImprovedAutomatic Translation OutputRadu SoricutSDL Language Weaver6060 Center Drive, Suite 150Los Angeles, CA 90045rsoricut@sdl.comSushant Narsale?Google Inc1600 Amphitheatre ParkwayMountain View, CA 94043snarsale@google.comAbstractThis paper presents techniques for reference-free, automatic prediction of Machine Trans-lation output quality at both sentence- anddocument-level.
In addition to helping withdocument-level quality estimation, sentence-level predictions are used for system selection,improving the quality of the output transla-tions.
We present three system selection tech-niques and perform evaluations that quantifythe gains across multiple domains and lan-guage pairs.1 IntroductionAside from improving the performance of core-translation models, there additionally exist twoorthogonal approaches via which fully-automatictranslations can achieve increased acceptance andbetter integration in real-world use cases.
These twoapproaches are: improved translation accuracy viasystem combination (Rosti et al, 2008; Karakos etal., 2008; Hildebrand and Vogel, 2008), and auto-matic quality-estimation techniques used as an ad-ditional layer on top of MT systems, which presentthe user only with translations that are predicted asbeing accurate (Soricut and Echihabi, 2010; Specia,2011).In this paper, we describe new contributions toboth these approaches.
First, we present a noveland superior technique for performing quality esti-mation at document level.
We achieve this by chang-?Research was completed before the author started in hiscurrent role at Google Inc.
The opinions stated are his own andnot of Google Inc.ing the granularity of the prediction mechanismfrom document-level (Soricut and Echihabi, 2010)to sentence-level, and predicting BLEU scores viadirectly modeling the sufficient statistics for BLEUcomputation.
A document-level score is then recre-ated based on the predicted sentence-level sufficientstatistics.
A second contribution is related to systemcombination (or, to be more precise, system selec-tion).
This is an intended side-effect of the granular-ity change: since the sentence-level statistics allowus to make quality predictions at sentence level, wecan use these predictions to perform system com-bination by selecting among various sentence-leveltranslations produced by different MT systems.
Thatis, instead of presenting the user with a documentwith sentences translated entirely by a single system,we can present documents for which, say, 60% ofthe sentences were translated by system A, and 40%were translated by system B.
We contribute a novelset of features and several techniques for choos-ing between competing machine translation outputs.The evaluation results show better output quality,across multiple domains and language pairs.2 Related WorkSeveral approaches to reference-free automatic MTquality assessment have been proposed, using classi-fication (Kulesza and Shieber, 2004), regression (Al-brecht and Hwa, 2007), and ranking (Ye et al, 2007;Duh, 2008).
The focus of these approaches is on sys-tem performance evaluation, as they use a constanttest set and measure various MT systems against it.In contrast, we are interested in evaluating thequality of the translations themselves, while treat-163ing the MT components as constants.
In this re-spect, the goal is more related to the area of con-fidence estimation for MT (Blatz et al, 2004).
Con-fidence estimation is usually concerned with iden-tifying words/phrases for which one can be confi-dent in the quality of the translation.
A sentence-level approach to quality estimation is taken on theclassification-based work of Gamon et al (2005) andregression-based work of Specia et al (2009).Our approach to quality estimation focuses onboth sentence-level and document-level estimation.We improve on the quality estimation technique thatis proposed for document-level estimation in (Sori-cut and Echihabi, 2010).
Furthermore, we exploitthe availability of multiple translation hypotheses toperform system combination.
Our system combina-tion methods are based on generic Machine Learn-ing techniques, applied on 1-best output strings.
Incontrast, most of the approaches to MT system com-bination combine N-best lists from multiple MT sys-tems via confusion network decoding (Karakos etal., 2008; Rosti et al, 2008).
The closest systemcombination approach to our work is (Hildebrandand Vogel, 2008), where an ensemble of hypothe-ses is generated by combining N-best lists from allthe participating systems, and a log-linear model istrained to select the best translation from all the pos-sible candidates.In our work, we show that it is possible to gainsignificant translation quality by taking advantageof only two participating systems.
This makes thesystem-combination proposition much more palat-able in real production deployment scenarios forMachine Translation, as opposed to pure researchscenarios as the ones used in the previous NIST andDARPA/GALE MT efforts (Olive et al, 2011).
Asour evaluations show, the two participating systemscan be at very similar performance levels, and yet asystem-selection procedure using Machine Learningtechniques can achieve significant translation im-provements in quality.
In addition, in a scenariowhere quality estimation needs to happen as a re-quirement for MT integration in large applications,having two translation systems producing transla-tions for the same inputs is part of the deploymentset-up (Soricut and Echihabi, 2010).
The improve-ment in overall translation quality comes in thesecases at near-zero cost.3 Sentence-level Quality PredictionsThe requirement for document-level quality esti-mation comes from the need to present a fully-automated translation solution, in which translateddocuments are either good enough to be directlypublished (or otherwise must undergo, say, a human-driven post-processing pipeline).
In the proposal ofSoricut and Echihabi (2010), regression models pre-dict BLEU-like scores for each document, based ondocument-level features.However, even if the predicted value is atdocument-level, the actual feature computation andmodel prediction does not necessarily need to hap-pen at document-level.
It is one of the goals of thiswork to determine if the models of prediction workbetter at a coarser granularity (such as documentlevel) or finer granularity (such as sentence-level).We describe here a mechanism for predictingBLEU scores at sentence level, and then combin-ing these scores into document-level scores.
Tomake explicit our prediction mechanism, we presenthere in detail the formula for computing BLEUscores (Papineni et al, 2002).
First, n-gram preci-sion scores Pn are computed as follows:Pn =?C?Candidates?n-gram?C Countclip(n-gram)?C?Candidates?n-gram?C Count(n-gram)(1)where Countclip(n-gram) is the maximum numberof n-grams co-occurring in a candidate translationand a reference translation, and Count(n-gram) isthe number of n-grams in the candidate translation.To prevent very short translations that try to max-imize their precision scores, BLEU adds a brevitypenalty, BP, to the formula:BP ={1 if |c| > |r|e(1?|r|/|c|) if |c| ?
|r|(2)where |c| is the length of the candidate translationand |r| is the length of the reference translation.
TheBLEU formula is then written as follows:BLEU = BP ?
exp(N?n=1wn log pn) (3)where the weighting factors wn are set to 1/N , forall 1 ?
n ?
4.1643.1 The learning methodThe results we report in this section are ob-tained using the freely-available Weka engine.
1For both sentence-level and document-level qual-ity prediction, we report all the results usingWeka implementation of M5P regression trees(weka.classifiers.trees.M5P).We use the components of the BLEU score (Equa-tions 1 and 2) to train fine-granularity M5P modelsusing our set of features (Section 3.2), for a total offive individual regression-tree models (four for thesentence-level precision scores Pn, 1 ?
n ?
4 fac-tors, and one for the BP factor).
The numbers pro-duced individually by our models are then combinedusing the BLEU equation 3 into a sentence-levelBLEU score.
The sentence-level predicted BLEUscores play an important role in our system combi-nation mechanism (see Section 4).At the same time, we sum up the sufficientstatistics for the sentence-level precision scores Pn(Equation 1) over all the sentences in a document,thus obtaining document-level precision scores.
Adocument-level BP score (Equation 2) is similarlyobtained by summing over all sentences.
Finally,we plug the predicted document-level Pn and BPscores in the BLEU formula (Equation 3) and arriveat a document-level predicted BLEU score.3.2 The featuresMost of the features we use in this work are notinternal features of the MT system, but rather de-rived starting from input/output strings.
Therefore,they can be applied for a large variety of MT ap-proaches, from statistical-based to rule-based ap-proaches.
The features we use can be dividedinto text-based, language-model?based, pseudo-reference?based, example-based, and training-data?based feature types (these latter features assume thatthe engine is statistical and one has access to thetraining data).
These feature types can be computedboth on the source-side (MT input) and on the target-side (MT output).Text-based featuresThese features compute the length of the input interms of (tokenized) number of words.
The source-1Weka software at http://www.cs.waikato.ac.nz/ml/weka/.side text feature is computed on the input string,while the target-side text feature is computed to theoutput translation string.
These two features are use-ful in modeling the relationship between the numberof words in the input and output and the expectedBLEU score for these sizes.Language-model?based featuresThese features are among the ones that were firstproposed as possible differentiators between goodand bad translations (Gamon et al, 2005).
They area measure of how likely a collection of strings is un-der a language model trained on monolingual data(either on the source or target side).The language-model?based feature values we usehere are computed as perplexity numbers using a 5-gram language model trained on the MT trainingset.
This can be achieved, for instance, by usingthe publicly-available SRILM toolkit 2.
These twofeatures are useful in modeling the relationship be-tween the likelihood of a string (or set of strings)under an n-gram language model and the expectedBLEU score for that input/output pair.Pseudo-reference?based featuresPrevious work has shown that, in the ab-sence of human-produced references, automatically-produced ones are still helpful in differentiating be-tween good and bad translations (Albrecht and Hwa,2008).
When computed on the target side, thistype of features requires one (or possibly more)secondary MT system(s), used to generate transla-tions starting from the same input.
These pseudo-references are useful in gauging translation conver-gence, using BLEU scores as feature values.
In in-tuitive terms, their usefulness can be summarized asfollows: ?if system X produced a translation A andsystem Y produced a translation B starting from thesame input, and A and B are similar, then A is prob-ably a good translation?.An important property here is that systems X andY need to be as different as possible from each other.This property ensures that a convergence on sim-ilar translations is not just an artifact of the sys-tems sharing the same translation model/resources,but a true indication that the translations converge.The secondary systems we use in this work are2Available at www-speech.sri.com/projects/srilm.165still phrase-based, but equipped with linguistically-oriented modules similar with the ones proposedin (Collins et al, 2005; Xu et al, 2009).
Our exper-iments indicate that this single feature is one of themost powerful ones in terms of its predictive power.Example-based featuresFor example-based features, we use a develop-ment set of parallel sentences, for which we pro-duce translations and compute sentence-level BLEUscores.
We set aside the top BLEU scoring sen-tences and bottom BLEU scoring sentences.
Thesesets are used as positive examples (with better-than-average BLEU) and negative examples (with worse-than-average BLEU), respectively.
We define apositive-example?based feature function as a geo-metric mean of 1-to-4?gram precision scores (i.e.,the BLEU equation 3 with the BP term set to 1) be-tween a string (on either source or target side) andthe positive examples used as references.
That is,we compute precision scores against all the positiveexamples at the same time, similar with how mul-tiple references are used to increase the precisionof the BLEU metric.
(The negative-example?basedfeatures are defined in an analogous way.)
The set ofpositive and negative examples is a fixed set that isused in the same manner both at training-time (tocompute the example-based feature values for thetraining examples) and at test-time (to compute theexample-based feature values for the test examples).The intuition behind these features can be sum-marized as follows: ?if system X translated Awell/poorly, and A and B are similar, then system Xprobably translates B well/poorly?.
The total num-ber of features on this type is 4 (2 for positive ex-amples against source/target strings, 2 for negativeexamples against source/target strings).Training-data?based featuresIf the system for which we make the predictions istrained on a parallel corpus, the data in this corpuscan be exploited towards assessing translation qual-ity (Specia et al, 2009; Soricut and Echihabi, 2010;Specia, 2011).
In our context, the documents thatmake up this corpus can be used in a fashion simi-lar with the positive examples.
One type of training-data?based features operates by computing the num-ber of out-of-vocabulary (OOV) tokens with respectto the training data (on source side).A more powerful type of training-data?based fea-tures operates by computing a geometric mean of 1-to-4?gram precision score between a string (sourceor target side) and the training-data strings used asreferences.
Intuitively, these features assess the cov-erage of the candidate strings with respect to thetraining data: ?if the n-grams of input string A arewell covered by the source-side of the training data,then the translation of A is probably good?
(on thesource side); ?if the n-grams in the output translationB are well covered by the target-side of the paralleltraining data, then B is probably a good translation?
(on the target side).
The total number of features onthis type is 3 (1 for the OOV counts, and 2 for thesource/target-side n-gram coverage).Given the described 12 feature functions, thetraining for our five M5P prediction models is doneusing the feature-function values at sentence-level,and associating these values with reference labelsthat are automatically-produced from parallel-textusing the sufficient-statistics of the BLEU score(Equations 1 and 2).3.3 Metrics for Quality PredictionPerformanceThe metrics we use here are designed to answer thefollowing question: how well can we automaticallyseparate better translations from worse translations(in the absence of human-produced references)?A first metric we use is Ranking Accuracy (rAcc),see (Gunawardana and Shani, 2009; Soricut andEchihabi, 2010).
In the general case, it measureshow well N elements are assigned into n quantilesas a result of a ranking procedure.
The formula is:rAcc[n] = Avgni=1TPiNn=1N?n?i=1TPiwhere TPi (True-Positivei) is the number ofcorrectly-assigned documents in quantile i. Intu-itively, this formula is an average of the ratio of ele-ments correctly assigned in each quantile.
For sim-plicity, we present here results using only 2 quan-tiles (n = 2), which effectively makes the rAcc[2]metric equivalent with binary classification accuracywhen the two sets are required to have equal size.That is, we measure the accuracy of placing the 50%166Training BLEU Ranking rAcc[2] DeltaAvg[2]Size Sys1 Sys2 Test Size Doc Sent Doc SentWMT09 Hungarian-English 26 Mw 26.9 26.9 510 Kw 88% 89% +8.3 +8.4Travel English-French 30 Mw 32.3 34.6 282 Kw 77% 80% +9.1 +10.1Travel English-German 44 Mw 40.6 43.4 186 Kw 74% 79% +9.8 +11.7HiTech English-French 0.4 Mw 44.1 44.7 69 Kw 75% 77% +4.4 +6.0HiTech English-Korean 16 Mw 37.4 36.1 80 Kw 78% 79% +9.3 +10.0Table 1: MT system performance and ranking performance using BLEU prediction at Doc- and Sent-level.best-translated documents (as measured by BLEUagainst human reference) in the top 50% of rankeddocuments.
Note that a random assignment gives aperformance lower bound of 50% accuracy.A second metric we use here is the DeltaAvg met-ric (Callison-Burch et al, 2012).
The goal of theDeltaAvg metric is to measure how valuable a pro-posed ranking (hypothesis) is from the perspectiveof an extrinsic metric associated with the test en-tries (in our case, the BLEU scores).
The follow-ing notations are used: for a given entry sentence s,V (s) represents the function that associates an ex-trinsic value to that entry; we extend this notationto a set S, with V (S) representing the average ofall V (s), s ?
S. Intuitively, V (S) is a quantitativemeasure of the ?quality?
of the set S, as induced bythe extrinsic values associated with the entries in S.For a set of ranked entries S and a parameter n, wedenote by S1 the first quantile of set S (the highest-ranked entries), S2 the second quantile, and so on,for n quantiles of equal sizes.3 We also use the no-tation Si,j =?jk=i Sk.
Using these notations, themetric is defined as:DeltaAvgV [n] =?n?1k=1 V (S1,k)n?
1?
V (S) (4)When the valuation function V is clear from the con-text, we write DeltaAvg[n] for DeltaAvgV [n].
Theparameter n represents the number of quantiles wewant to split the set S into.
For simplicity, we con-sider there only the case for n = 2, which givesDeltaAvg[2] = V (S1) ?
V (S).
This measures thedifference between the quality of the top quantile(top half) S1 and the overall quality (represented by3If the size |S| is not divisible by n, then the last quantileSn is assumed to contain the rest of the entries.V (S)).
For the results presented here, the valuationfunction V is taken to be the BLEU function (Equa-tion 3).3.4 Experimental ResultsWe measure the impact in ranking accuracy using avariety of European and Asian language pairs, usingparallel data from various domains.
One domain weuse is the publicly available WMT09 data (Koehnand Haddow, 2009), a combination of European par-liament and news data.
Another domain, calledTravel, consists of user-generated reviews and de-scriptions; and a third domain, called HiTech, con-sists of parallel data from customer support for thehigh-tech industry.
Using these parallel data sets,we train statistical phrase-based MT system similarto (Och and Ney, 2004) as primary systems (Sys1).As secondary systems (Sys2) we use phrase-basedsystems equipped with linguistically-oriented mod-ules similar with the ones proposed in (Collins etal., 2005; Xu et al, 2009).
Table 1 lists the size ofthe parallel training data on which the MT systemswere trained in the first column, and BLEU scoresfor the primary and secondary systems on held-out1000-sentence test sets in the next two columns.The training material for the regression-tree mod-els consists of 1000-document held-out sets.
(Forparallel data for which we do not have documentboundaries, we simply simulate document bound-aries after every 10 consecutive sentences.)
Simi-larly, the Ranking test sets we use consist of 1000-document held-out sets (see column 4 in Table 1 forsize).
In the last four columns of Table 1, we showthe results for ranking the translations produced bythe primary MT system (Sys1).
We measure theranking performance for the two granularity cases.The one labeled as ?Doc?
is an implementation of167the work described in (Soricut and Echihabi, 2010),where the BLEU prediction is done using document-level feature values and models.
The one labeledas ?Sent?
is the novel one proposed in this paper,where the BLEU prediction is done using sentence-level feature values and models, which are then ag-gregated into document-level BLEU scores.Both rAcc[2] and DeltaAvg[2] numbers supportthe choice of making document-level BLEU pre-diction at a finer, sentence-based granularity level.For Travel English-French, for instance, the accu-racy of the ranking improves from 77% to 80%.
Toput some intuition behind these numbers, it meansthat 4 out of every 5 sentences that the ranker placesin the top 50% do belong there.
At the same time,the DeltaAvg[2] numbers for Travel English-Frenchindicate that the translation quality of the top 50%of the 1000 Ranking Test documents exceeds by10.1 BLEU points the overall quality of the trans-lations (up from 9.1 BLEU points for the document-level prediction).
This large gap in the BLEU scoreof the top 50% ranked sentences and the overall-corpus BLEU indicates that these top-ranked trans-lations are indeed of much better quality (closer tothe human-produced references).
The same largenumbers are measured on the WMT09 data forHungarian-English.
This is a set for which it is hardto obtain significant improvements via core-modeltranslation improvements.
Our quality-estimationmethod allows one to automatically identify the top50% of the sentences with 89% accuracy.
This set oftop 50% sentences also has an overall BLEU scoreof 35.3, which is better by +8.4 BLEU-points com-pared to the overall BLEU score of 26.9 (we onlyshow the base overall BLEU score and the BLEU-point gain in Table 2 to avoid displaying redundantinformation).4 System Combination at Sentence LevelSince we produce two translations for every inputsentence for the purpose of quality estimation, weexploit the availability of these competing hypothe-ses in order to choose the best one.
In this sectionwe describe three system combination schemes thatchoose between the output of the primary and sec-ondary MT systems.4.1 System Combination using RegressionThis combination scheme makes use of theregression-based sentence-level BLEU predictionmechanism described in Section 3.
It requires thatwe also train and use an additional BLEU predic-tion mechanism for which the secondary MT sys-tem is now considered primary, and vice-versa.
As aconsequence, we can predict a sentence-level BLEUscore for each of the two competing hypotheses.
Wethen simply choose the hypothesis with the highestpredicted BLEU score.4.2 System Combination using RankingThis approach is based on ranking the candidatetranslations and then selecting the highest-rankedtranslation as the final output.
To this end we useSVM-rank (Joachims, 1999), a ranking algorithmbuilt on SVM.
We use SVM-rank with a linear ker-nel and the same feature set as the regression-basedmethod (we make the observation here that only thetarget-based features have discriminative power inthis context).4.3 System Combination using ClassificationIn this approach, we model the problem of select-ing the best output from the two candidate transla-tions into a binary classification problem.
We use thesame feature set as before for each candidate transla-tion (again, only the target-based features have dis-criminative power in this context).The final feature vectors are obtained by subtract-ing the values of the primary-system feature vec-tor from the values of the secondary-system featurevector.
The binary classifier is trained to predict?0?
if the primary-system is better, and ?1?
if thesecondary-system is better.4.4 Experimental ResultsIn Table 2, we summarize the results for the threesystem combination techniques discussed beforeacross our domains (WMT09, Travel, and Hi-Tech).To get an upper bound on the performance of thesesystem combination techniques, we also computean oracle function which selects the translationwith highest BLEU score computed against human-produced references.The results in Table 2 indicate that the BLEUimprovements obtained by our system combina-168BLEU Oracle Regression Rank ClassifySys1 Sys2WMT09 Hungarian-English 26.9 26.9 30.7(+3.8) 29.0(+2.1) 29.0(+2.1) 28.9(+2.0)Travel English-French 32.3 34.6 38.7(+3.9) 36.2(+1.6) 36.0(+1.4) 35.7(+1.1)Travel English-German 40.6 43.4 47.2(+3.8) 44.5(+1.1) 44.0(+0.6) 44.9(+1.5)HiTech English-French 44.1 44.7 49.8(+5.1) 46.1(+1.4) 46.3(+1.7) 45.3(+0.6)HiTech English-Korean 37.4 36.1 42.2(+4.8) 39.4(+2.0) 39.1(+1.7) 38.8(+1.4)Table 2: BLEU scores for the proposed system combination techniques across domains and language pairs.Travel Eng-Fra Hi-Tech Eng-FraSys1 Sys2 KL Sys1 Sy2 KLBLEU score 32.3 34.6 - 44.1 44.7 -Oracle distr.
34.9% 65.1% 0.00 34.5% 65.5% 0.00Regression distr.
31.2% 68.9% 0.68 32.3% 67.7% 0.11Rank distr.
43.4% 56.6% 1.92 47.0% 53.0% 3.31Classify distr.
47.4% 52.7% 3.78 63.9% 36.1% 17.88Table 3: Distribution of sentences selected from the participating system for Eng-Fra, across domains (Travel andHi-Tech).tion techniques are significant.
For instance,both the Regression-based system combination andthe Ranking-based system combination achieve aBLEU score of 29.0 on the WMT09 Hungarian-English test set, an increase of +2.1 BLEU points.In the case of Travel English-French, an increase of+1.6 BLEU points is obtained by the Regression-based system combination, in spite of the fact thatone of the systems is measured to be 2.3 BLEUpoints lower in translation accuracy.
Increases in therange of +1.5-2.0 BLEU points are obtained acrossall the experimental conditions that we tried: threedifferent domains, various language pairs (both inand out of English), and various training data sizes(from 0.4Mw to 40Mw).Since our system-combination methods chose onesystem translation over another system translation,we can also measure the distribution of choicesmade between the two participating systems.
Thesebimodal distributions can help us gauge the perfor-mance of various methods, when compared againstthe BLEU Oracle distribution.In Table 3, we report the percentages of sentencesselected from each system in the oracle combina-tion and each of the described system combinationmethods.
We also report the Kullback-Liebler di-vergence (KL) between the BLEU Oracle distribu-tion and the distribution induced by each of the sys-tem combination methods.
The results indicate that,for both English-French cases that we considered(in the Travel and HiTech domains), the choice dis-tribution of the Regression-based system combina-tion method is much closer to the oracle distribution(KL of 0.68 and 0.11, respectively), compared to theother two methods.
Note that this does not neces-sarily correlate with the evaluation based on over-all BLEU score of the system-combination meth-ods (Table 2).
For instance, for HiTech English-French the best BLEU improvement is obtained bythe Rank-based method with +1.7 BLEU points, butthe KL divergence score of 3.31 is higher than theone for the Regression-based method (KL score of0.11).
Nevertheless, the choice distributions are animportant factor in judging the performance of agiven system selection method.5 ConclusionsDocument-level quality estimation is an importantcomponent for building fully-automated translationsolutions where the translated documents are di-rectly published, without the need for human inter-vention.
Such approaches are the only possible solu-169tion to mitigate the imperfection of current MT tech-nology and the need to translate large volumes ofdata on a continuous basis.We show in this paper that sentence-level predic-tions, when aggregated to document-level predic-tions, outperform previously-proposed document-level quality estimation algorithms.
In addition tothat, these finer-granularity, sentence-level predic-tions can be used as part of a system selectionscheme.
The three alternative system selection tech-niques we describe here are intuitive, computation-ally cheap, and bring significant BLEU gains acrossmultiple domains and language pairs.
The findingthat the regression-based system selection techniqueperforms as well (or sometimes better) compared tothe discriminative methods fits well with the overalltheme of using two systems for both improved qual-ity estimation and improved MT performance.ReferencesJoshua Albrecht and Rebecca Hwa.
2007.
Regression forsentence-level MT evaluation with pseudo references.In Proceedings of ACL.Joshua Albrecht and Rebecca Hwa.
2008.
The role ofpseudo references in MT evaluation.
In Proceedingsof ACL.John Blatz, Erin Fitzgerald, George Foster, Simona Gan-drabur, Cyril Gouette, Alex Kulesza, Alberto Sanchis,and Nicola Ueffing.
2004.
Confidence estimation formachine translation.
In Proceedings of COLING.Chris Callison-Burch, Philipp Koehn, Christof Monz,Matt Post, Radu Soricut, and Lucia Specia.
2012.Findings of the 2012 workshop on statistical machinetranslation.
In Proceedings of the Seventh Workshopon Statistical Machine Translation, Montreal, Canada,June.
Association for Computational Linguistics.Michael Collins, Philipp Koehn, and Ivona Kucerova.2005.
Clause restructuring for statistical machinetranslation.
In Proceedings of ACL.Kevin Duh.
2008.
Ranking vs. regression in machinetranslation evaluation.
In Proceedings of the ACLThird Workshop on Statistical Machine Translation.Michael Gamon, Anthony Aue, and Martine Smets.2005.
Sentence-level MT evaluation without referencetranslations: Beyond language modeling.
In Proceed-ings of EAMT.Asela Gunawardana and Guy Shani.
2009.
A sur-vey of accuracy evaluation metrics of recommenda-tion tasks.
Journal of Machine Learning Research,10:2935?2962.Almut Silja Hildebrand and Stephan Vogel.
2008.
Com-bination of machine translation systems via hypothesisselection.
In Proceedings of AMTA.T.
Joachims.
1999.
Making large-Scale SVM LearningPractical.
M.I.T.
Press.Damianos Karakos, Jason Eisner, Sanjeev Khudanpur,and Markus Dreyer.
2008.
Machine translation sys-tem combination using ITG-based alignments.
In Pro-ceedings of ACL.Philipp Koehn and Barry Haddow.
2009.
Edinburgh?ssubmission to all tracks of the WMT2009 shared taskwith reordering and speed improvements to Moses.In Proceedings of EACL Workshop on Statistical Ma-chine Translation.Alex Kulesza and Stuart M. Shieber.
2004.
A learningapproach to improving sentence-level MT evaluation.In Proceedings of the 10th International Conferenceon Theoretical and Methodological Issues in MachineTranslation.Franz Josef Och and Hermann Ney.
2004.
The align-ment template approach to statistical machine transla-tion.
Computational Linguistics, 30(4):417?449.Joseph Olive, Caitlin Christianson, and John McCary, ed-itors.
2011.
Handbook of Natural Language Pro-cessing and Machine Translation: DARPA Global Au-tonomous Language Exploitation.
Springer.Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu.
2002.
BLEU: a method for automatic eval-uation of machine translation.
In Proceedings of ACL.Antti-Veikko Rosti, Bing Zhang, Spyros Matsoukas, andRichard Schwartz.
2008.
Incremental hypothesisalignment for building confusion networks with appli-cation to machine translation system combination.
InProceedings of Third Workshop on Statistical MachineTranslation.Radu Soricut and Abdessamad Echihabi.
2010.Trustrank: Inducing trust in automatic translations viaranking.
In Proceedings of ACL.Lucia Specia, Nicola Cancedda, Marc Dymetman, Mar-cho Turchi, and Nello Cristianini.
2009.
Estimatingthe sentence-level quality of machine translation.
InProceedings of EAMT.Lucia Specia.
2011.
Exploiting objective annotations formeasuring translation post-editing effort.
In Proceed-ings of EAMT.Peng Xu, Jaeho Kang, Michael Ringaard, and Franz Och.2009.
Using a dependency parser to improve SMTfor Subject-Object-Verb languages.
In Proceedings ofACL.Yang Ye, Ming Zhou, and Chin-Yew Lin.
2007.
Sen-tence level machine translation evaluation as a rank-ing.
In Proceedings of the ACL Second Workshop onStatistical Machine Translation.170
