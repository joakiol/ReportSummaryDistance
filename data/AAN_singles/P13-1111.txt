Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 1127?1136,Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational LinguisticsHandling Ambiguities of Bilingual Predicate-Argument Structures forStatistical Machine TranslationFeifei Zhai, Jiajun Zhang, Yu Zhou and Chengqing ZongNational Laboratory of Pattern Recognition, Institute of Automation,Chinese Academy of Sciences, Beijing, China{ffzhai,jjzhang,yzhou,cqzong}@nlpr.ia.ac.cnAbstractPredicate-argument structure (PAS) has beendemonstrated to be very effective in improvingSMT performance.
However, since a source-side PAS might correspond to multiple differ-ent target-side PASs, there usually exist manyPAS ambiguities during translation.
In this pa-per, we group PAS ambiguities into two types:role ambiguity and gap ambiguity.
Then wepropose two novel methods to handle the twoPAS ambiguities for SMT accordingly: 1) in-side context integration; 2) a novel maximumentropy PAS disambiguation (MEPD) model.In this way, we incorporate rich context in-formation of PAS for disambiguation.
Thenwe integrate the two methods into a PAS-based translation framework.
Experimentsshow that our approach helps to achieve sig-nificant improvements on translation quality.1 IntroductionPredicate-argument structure (PAS) depicts therelationship between a predicate and its associat-ed arguments, which indicates the skeleton struc-ture of a sentence on semantic level.
Basically,PAS agrees much better between two languagesthan syntax structure (Fung et al, 2006; Wu andFung, 2009b).
Considering that current syntax-based translation models are always impaired bycross-lingual structure divergence (Eisner, 2003;Zhang et al, 2010), PAS is really a better repre-sentation of a sentence pair to model the bilin-gual structure mapping.However, since a source-side PAS mightcorrespond to multiple different target-side PASs,there usually exist many PAS ambiguities duringtranslation.
For example, in Figure 1, (a) and (b)carry the same source-side PAS <[A0]1[Pred(?
)]2 [A1]3> for Chinese predicate ??
?.However, in Figure 1(a), the correspondingtarget-side-like PAS is <[X1] [X2] [X3]>, while inFigure 1(b), the counterpart target-side-like PAS1is <[X2] [X3] [X1]>.
This is because the twoPASs play different roles in their correspondingsentences.
Actually, Figure 1(a) is an independ-ent PAS, while Figure 1(b) is a modifier of thenoun phrase ???
?
????.
We call this kindof PAS ambiguity role ambiguity.??
?
???
??
???
[           A0         ]1 [     A1    ]3[Pred]2?being , should  ?two major countries[           X3            ][X2]China and Russia[          X1           ]?
???
??
?
???
[ A0 ]1 [          A1         ]3[Pred]2flood  prevention is the  primary  mission[           X1          ] [ X2 ] [              X3              ]???
?
??
?
???
?
?
?
?
[      A0      ]1 [    A1   ]3[Pred]2the location of the olympic village for athletesis the best[     X3    ][X2][                    X1                     ](a)(c)(b)Figure 1.
An example of ambiguous PASs.Meanwhile, Figure 1 also depicts another kindof PAS ambiguity.
From Figure 1, we can seethat (a) and (c) get the same source-side PAS andtarget-side-like PAS.
However, they are differentbecause in Figure 1(c), there is a gap string ??????
between [A0] and [Pred].
Generally, thegap strings are due to the low recall of automaticsemantic role labeling (SRL) or complex sen-tence structures.
For example, in Figure 1(c), thegap string ??
????
is actually an argument?AM-PRP?
of the PAS, but the SRL system has1We use target-side-like PAS to refer to a list of generalnon-terminals in target language order, where a non-terminal aligns to a source argument.1127ignored it.
We call this kind of PAS ambiguitygap ambiguity.During translation, these PAS ambiguities willgreatly affect the PAS-based translation models.Therefore, in order to incorporate the bilingualPAS into machine translation effectively, weneed to decide which target-side-like PAS shouldbe chosen for a specific source-side PAS.
Wecall this task PAS disambiguation.In this paper, we propose two novel methodsto incorporate rich context information to handlePAS ambiguities.
Towards the gap ambiguity,we adopt a method called inside contextintegration to extend PAS to IC-PAS.
In terms ofIC-PAS, the gap strings are combined effectivelyto deal with the gap ambiguities.
As to the roleambiguity, we design a novel maximum entropyPAS disambiguation (MEPD) model to combinevarious context features, such as context wordsof PAS.
For each ambiguous source-side PAS,we build a specific MEPD model to selectappropriate target-side-like PAS for translation.We will detail the two methods in Section 3 and4 respectively.Finally, we integrate the above two methodsinto a PAS-based translation framework (Zhai etal.
2012).
Experiments show that the two PASdisambiguation methods significantly improvethe baseline translation system.
The maincontribution of this work can be concluded asfollows:1) We define two kinds of PAS ambiguities:role ambiguity and gap ambiguity.
To ourbest knowledge, we are the first to handlethese PAS ambiguities for SMT.2) Towards the two different ambiguities, wedesign two specific methods for PASdisambiguation: inside context integrationand the novel MEPD model.2 PAS-based Translation FrameworkPAS-based translation framework is to performtranslation based on PAS transformation (Zhai etal., 2012).
In the framework, a source-side PASis first converted into target-side-like PASs byPAS transformation rules, and then performtranslation based on the obtained target-side-likePASs.2.1 PAS Transformation RulesPAS transformation rules (PASTR) are used toconvert a source-side PAS into a target one.Formally, a PASTR is a triple <Pred, SP, TP>:?
Pred means the predicate where the rule isextracted.?
SP denotes the list of source elements insource language order.?
TP refers to the target-side-like PAS, i.e., alist of general non-terminals in targetlanguage order.For example, Figure 2 shows the PASTRextracted from Figure 1(a).
In this PASTR, Predis Chinese verb ??
?, SP is the source elementlist <[A0]1 [Pred]2 [A1]3>, and TP is the list ofnon-terminals <X1 X2 X3>.
The same subscript inSP and TP means a one-to-one mapping betweena source element and a target non-terminal.
Here,we utilize the source element to refer to thepredicate or argument of the source-side PAS.
[X3] [X2] [A0]1 [Pred]2 [A1]3 [X1]source-side PAS(?)
target-side-like PASFigure 2.
An example PASTR.2.2 PAS DecodingThe PAS decoding process is divided into 3 steps:(1) PAS acquisition: perform semantic rolelabeling (SRL) on the input sentences to achievetheir PASs, i.e., source-side PASs;(2) Transformation: use the PASTR to matchthe source-side PAS i.e., the predicate Pred andthe source element list SP.
Then by the matchingPASTRs, transform source-side PASs to target-side-like PASs.
(3) Translation: in this step, the decoder firsttranslates each source element respectively, andthen a CKY-style decoding algorithm is adoptedto combine the translation of each element andget the final translation of the PAS.2.3 Sentence Decoding with the PAS-basedtranslation frameworkSometimes, the source sentence cannot be fullycovered by the PAS, especially when there areseveral predicates.
Thus to translate the wholesentence, Zhai et al (2012) further designed analgorithm to decode the entire sentence.In the algorithm, they organized the space oftranslation candidates into a hypergraph.
For thespan covered by PAS (PAS span), a multiple-branch hyperedge is employed to connect it tothe PAS?s elements.
For the span not covered byPAS (non-PAS span), the decoder considers allthe possible binary segmentations of it and uti-lizes binary hyperedges to link them.1128During translation, the decoder fills the spanswith translation candidates in a bottom-up man-ner.
For the PAS span, the PAS-based translationframework is adopted.
Otherwise, the BTG sys-tem (Xiong et al, 2006) is used.
When the spancovers the whole sentence, we get the final trans-lation result.Obviously, PAS ambiguities are notconsidered in this framework at all.
The target-side-like PAS is selected only according to thelanguage model and translation probabilities,without considering any context information ofPAS.
Consequently, it would be difficult for thedecoder to distinguish the source-side PAS fromdifferent context.
This harms the translationquality.
Thus to overcome this problem, we de-sign two novel methods to cope with the PASambiguities: inside-context integration and amaximum entropy PAS disambiguation (MEPD)model.
They will be detailed in the next two sec-tions.3 Inside Context IntegrationIn this section, we integrate the inside context ofthe PAS into PASTRs to do PAS disambiguation.Basically, a PAS consists of several elements (apredicate and several arguments), which are ac-tually a series of continuous spans.
For a specificPAS <E1,?, En>, such as the source-side PAS<[A0][Pred][A1]> in Figure 2, its controlled rangeis defined as:( ) { ( ), [1, ]}irange PAS s E i n= ?
?where s(Ei) denotes the span of element Ei.
Fur-ther, we define the closure range of a PAS.
Itrefers to the shortest continuous span covered bythe entire PAS:0( ) ( )_ min , maxnj s E j s Eclosure range j j?
??
?= ?
??
?Here, E0 and En are the leftmost and rightmostelement of the PAS respectively.
The closurerange is introduced here because adjacent sourceelements in a PAS are usually separated by gapstrings in the sentence.
We call these gap stringsthe inside context (IC) of the PAS, which satisfy:_ ( ) ( ( ) ( ) )closure range PAS IC PAS range PAS= ?
?The operator ?
takes a list of neighboring spansas input2, and returns their combined continuousspan.
As an example, towards the PAS ?<[A0][Pred][A1]>?
(the one for Chinese predicate ??(shi)?)
in Figure 3, its controlled range is{[3,5],[8,8],[9,11]} and its closure range is [3,11].The IC of the PAS is thus {[6,7]}.To consider the PAS?s IC during PAS trans-formation process, we incorporate its IC into theextracted PASTR.
For each gap string in IC, weabstract it by the sequence of highest node cate-gories (named as s-tag sequence).
The s-tag se-quence dominates the corresponding syntactictree fragments in the parse tree.
For example, inFigure 3, the s-tag sequence for span [6,8] is ?PPVC?.
Thus, the sequence for the IC (span [6,7])in Figure 3 is ?PP?.
We combine the s-tag se-quences with elements of the PAS in order.
Theresulting PAS is called IC-PAS, just like the leftside of Figure 4(b) shows.
[           A0           ] [        PP        ]??
?3 ??
?7 ?8 ?10de wei-zhiao-yun-cun?
?5?4 ?6dui yun-dong-yuan shi?9 ?11zui hao deNN DEC NNNPP NNPPVC AD VA DECCPVPIP?
?1VVbiao-shiVP,2PU?0PNta?PUIPDNP[Pred] [      A1     ]Figure 3.
The illustration of inside context (IC).
Thesubscript in each word refers to its position in sen-tence.Differently, Zhai et al (2012) attached the ICto its neighboring elements based on parse trees.For example, in Figure 3, they would attach thegap string ??
(dui) ???(yun-dong-yuan)?
to thePAS?s element ?Pred?, and then the span of?Pred?
would become [6,8].
Consequently, thespan [6,8] will be translated as a whole sourceelement in the decoder.
This results in a badtranslation because the gap string ??
(dui) ???(yun-dong-yuan)?
and predicate ??(shi)?
shouldbe translated separately, just as Figure 4(a)shows.
Therefore, we can see that the attachmentdecision in (Zhai et al, 2012) is sometimes un-reasonable and the IC also cannot be used forPAS disambiguation at all.
In contrast, our meth-2 Here, two spans are neighboring means that the beginningof the latter span is the former span?s subsequent word inthe sentence.
For example, span [3,6] and [7,10] are neigh-boring spans.1129od of inside context integration is much flexibleand beneficial for PAS disambiguation.
(a)(b)[X1] [X2] [X4] [A0]1 [PP]2 [Pred]3 [A1]4 [X3]source-side PAS(?)
target-side-like PAS???
???
?
?
[            A0            ]1 [      A1     ]4[Pred]3[the location of the olympic village]1 [for athletes]2[is]3 [the best]4[         PP         ]2de wei-zhiao-yun-cun???
?dui yun-dong-yuan shi?
?zui hao deFigure 4.
Example of IC-PASTR.
(a) The alignedspan of each element of the PAS in Figure 3; (b) Theextracted IC-PASTR from (a).Using the IC-PASs, we look for the alignedtarget span for each element of the IC-PAS.
Wedemand that every element and its correspondingtarget span must be consistent with word align-ment.
Otherwise, we discard the IC-PAS.
After-wards, we can easily extract a rule for PAS trans-formation, which we call IC-PASTR.
As an ex-ample, Figure 4(b) is the extracted IC-PASTRfrom Figure 4(a).Note that we only apply the source-side PASand word alignment for IC-PASTR extraction.By contrast, Zhai et al (2012) utilized the resultof bilingual SRL (Zhuang and Zong, 2010b).Generally, bilingual SRL could give a betteralignment between bilingual elements.
However,bilingual SRL usually achieves a really low re-call on PASs, about 226,968 entries in our train-ing set while it is 882,702 by using monolingualSRL system.
Thus to get a high recall for PASs,we only utilize word alignment instead of captur-ing the relation between bilingual elements.
Inaddition, to guarantee the accuracy of IC-PASTRs, we only retain rules with more than 5occurrences.4 Maximum Entropy PAS Disambigua-tion (MEPD) ModelIn order to handle the role ambiguities, in thissection, we concentrate on utilizing a maximumentropy model to incorporate the context infor-mation for PAS disambiguation.
Actually, thedisambiguation problem can be considered as amulti-class classification task.
That is to say, fora source-side PAS, every corresponding target-side-like PAS can be considered as a label.
Forexample, in Figure 1, for the source-side PAS?
[A0]1[Pred]2[A1]3?, the target-side-like PAS?
[X1] [X2] [X3]?
in Figure 1(a) is thus a label and?
[X2] [X3] [X1]?
in Figure 1(b) is another label ofthis classification problem.The maximum entropy model is the classicalway to handle this problem:exp( ( , , ( ), ( )))( | , ( ), ( ))exp( ( , , ( ), ( )))i i itp i i ih sp tp c sp c tpP tp sp c sp c tph sp tp c sp c tp???
?= ??
?where sp and tp refer to the source-side PAS (notincluding the predicate) and the target-side-likePAS respectively.
c(sp) and c(tp) denote the sur-rounding context of sp and tp.
hi is a binary fea-ture function and ?i is the weight of hi.We train a maximum entropy classifier foreach sp via the off-the-shelf MaxEnt toolkit 3 .Note that to avoid sparseness, sp does not in-clude predicate of the PAS.
Practically, the pred-icate serves as a feature of the MEPD model.
Asan example, for the rule illustrated in Figure 4(b),we build a MEPD model for its source elementlist sp <[A0] [PP] [Pred] [A1]>, and integrate thepredicate ??(shi)?
into the MEPD model as afeature.In detail, we design a list of features for eachpair <sp, tp> as follows:?
Lexical Features.
These features includethe words immediately to the left and right of sp,represented as w-1 and w+1.
Moreover, the headword of each argument also serves as a lexicalfeature, named as hw(Ei).
For example, Figure 3shows the context of the IC-PASTR in Figure4(b), and the extracted lexical features of the in-stance are: w-1=?
, w+1=?
, hw([A0]1)=??
(wei-zhi), hw([A1]4)=?(hao).?
POS Features.
These features are definedas the POS tags of the lexical features, p-1, p+1and phw(Ei) respectively.
Thus, the correspond-ing POS features of Figure 4 (b) are: p-1=PU,p+1=PU, phw([A0]1)=NN, phw([A1]4)=VA.?
Predicate Feature.
It is the pair of sourcepredicate and its corresponding target predicate.For example, in Figure 4(b), the source and tar-get predicate are ??(shi)?
and ?is?
respectively.The predicate feature is thus ?PredF=?
(shi)+is?.The target predicate is determined by:_ ( )- arg max ( | - )jj t range PASt pred p t s pred?=where s-pred is the source predicate and t-predis the corresponding target predicate.3http://homepages.inf.ed.ac.uk/lzhang10/maxent_toolkit.html1130t_range(PAS) refers to the target range coveringall the words that are reachable from the PAS viaword alignment.
tj refers to the jth word int_range(PAS).
The utilized lexical translationprobabilities are from the toolkit in Moses(Koehn et al, 2007).?
Syntax Features.
These features includest(Ei), i.e., the highest syntax tag for each argu-ment, and fst(PAS) which is the lowest fathernode of sp in the parse tree.
For example, for therule shown in Figure 4(b), syntax features arest([A0]1)=NP, st([A1]4)=CP, and fst(PAS)=IPrespectively.Using these features, we can train the MEPDmodel.
We set the Gaussian prior to 1.0 and per-form 100 iterations of the L-BFGS algorithm foreach MEPD model.
At last, we build 160 and215 different MEPD classifiers, respectively, forthe PASTRs and IC-PASTRs.
Note that since thetraining procedure of maximum entropy classifi-er is really fast, it does not take much time totrain these classifiers.5 Integrating into the PAS-based Trans-lation FrameworkIn this section, we integrate our method of PASdisambiguation into the PAS-based translationframework when translating each test sentence.For inside context integration, since the formatof IC-PASTR is the same to PASTR4, we canuse the IC-PASTR to substitute PASTR forbuilding a PAS-based translation system directly.We use ?IC-PASTR?
to denote this system.
Inaddition, since our method of rule extraction isdifferent from (Zhai et al, 2012), we also usePASTR to construct a translation system as thebaseline system, which we call ?PASTR?.On the basis of PASTR and IC-PASTR, wefurther integrate our MEPD model into transla-tion.
Specifically, we take the score of the MEPDmodel as another informative feature for the de-coder to distinguish good target-side-like PASsfrom bad ones.
The weights of the MEPD featurecan be tuned by MERT (Och, 2003) togetherwith other translation features, such as languagemodel.6 Related WorkThe method of PAS disambiguation for SMT isrelevant to the previous work on context depend-4 The only difference between IC-PASTR and PASTR isthat there are many syntactic labels in IC-PASTRs.ent translation.Carpuat and Wu (2007a, 2007b) and Chan etal.
(2007) have integrated word sense disambig-uation (WSD) and phrase sense disambiguation(PSD) into SMT systems.
They combine richcontext information to do disambiguation forwords or phrases, and achieve improved transla-tion performance.Differently, He et al (2008), Liu et al (2008)and Cui et al (2010) designed maximum entropy(ME) classifiers to do better rule section for hier-archical phrase-based model and tree-to-stringmodel respectively.
By incorporating the richcontext information as features, they chose betterrules for translation and yielded stable improve-ments on translation quality.Our work differs from the above work in thefollowing two aspects: 1) in our work, we focuson the problem of disambiguates on PAS; 2) wedefine two kinds of PAS ambiguities: roleambiguity and gap ambiguity.
3) towards the twodifferent ambiguities, we design two specificmethods for PAS disambiguation: inside contextintegration and the novel MEPD model.In addition, Xiong et al (2012) proposed anargument reordering model to predict the relativeposition between predicates and arguments.
Theyalso combine the context information in themodel.
But they only focus on the relation be-tween the predicate and a specific argument, ra-ther than the entire PAS.
Different from theirwork, we incorporate the context information todo PAS disambiguation based on the entire PAS.This is very beneficial for global reordering dur-ing translation (Zhai et al, 2012).7 Experiment7.1 Experimental SetupWe perform Chinese-to-English translation todemonstrate the effectiveness of our PAS disam-biguation method.
The training data containsabout 260K sentence pairs5.
To get accurate SRLresults, we ensure that the length of each sen-tence in the training data is among 10 and 30words.
We run GIZA++ and then employ thegrow-diag-final-and (gdfa) strategy to producesymmetric word alignments.
The developmentset and test set come from the NIST evaluationtest data (from 2003 to 2005).
Similar to thetraining set, we also only retain the sentences5 It is extracted from the LDC corpus.
The LDC categorynumber : LDC2000T50, LDC2002E18, LDC2003E07,LDC2004T07, LDC2005T06, LDC2002L27, LDC2005T10and LDC2005T34.1131whose lengths are among 10 and 30 words.
Fi-nally, the development set includes 595 sentenc-es from NIST MT03 and the test set contains1,786 sentences from NIST MT04 and MT05.We train a 5-gram language model with theXinhua portion of English Gigaword corpus andtarget part of the training data.
The translationquality is evaluated by case-insensitive BLEU-4with shortest length penalty.
The statistical sig-nificance test is performed by the re-samplingapproach (Koehn, 2004).We perform SRL on the source part of thetraining set, development set and test set by theChinese SRL system used in (Zhuang and Zong,2010b).
To relieve the negative effect of SRLerrors, we get the multiple SRL results byproviding the SRL system with 3-best parse treesof Berkeley parser (Petrov and Klein, 2007), 1-best parse tree of Bikel parser (Bikel, 2004) andStanford parser (Klein and Manning, 2003).Therefore, at last, we can get 5 SRL result foreach sentence.
For the training set, we use theseSRL results to do rule extraction respectively.We combine the obtained rules together to get acombined rule set.
We discard the rules withfewer than 5 appearances.
Using this set, we cantrain our MEPD model directly.As to translation, we match the 5 SRL resultswith transformation rules respectively, and thenapply the resulting target-side-like PASs for de-coding.
As we mentioned in section 2.3, we usethe state-of-the-art BTG system to translate thenon-PAS spans.source-side PAS counts number of classes[A0] [Pred(?)]
[A1] 245 6[A0] [Pred(?)]
[A1] 148 6[A0] [AM-ADV] [Pred(?)]
[A1] 68 20[A0] [Pred(??)]
[A1] 66 6[A0] [Pred(?)]
[A1] 42 6[A0] [Pred(??)]
[A1] 32 4[A0] [AM-ADV] [Pred(?)]
[A1] 32 19[A0] [Pred(??)]
[A1] 29 4[AM-ADV] [Pred(?)]
[A1] 26 6[A2] [Pred(?)]
[A1] 16 5Table 1.
The top 10 frequent source-side PASs in thedev and test set.7.2 Ambiguities in Source-side PASsWe first give Table 1 to show some examples ofrole ambiguity.
In the table, for instance, the se-cond line denotes that the source-side PAS ?[A0][Pred(?)]
[A1]?
appears 148 times in the devel-opment and test set al together, and it corre-sponds to 6 different target-side-like PASs in thetraining set.As we can see from Table 1, all the top 10PASs correspond to several different target-side-like PASs.
Moreover, according to our statistics,among all PASs appearing in the developmentset and test set, 56.7% of them carry gap strings.These statistics demonstrate the importance ofhandling the role ambiguity and gap ambiguity inthe PAS-based translation framework.
Therefore,we believe that our PAS disambiguation methodwould be helpful for translation.7.3 Translation ResultWe compare the translation result using PASTR,IC-PASTR and our MEPD model in this section.The final translation results are shown in Table 2.As we can see, after employing PAS for transla-tion, all systems outperform the baseline BTGsystem significantly.
This comparison verifiesthe conclusion of (Zhai et al, 2012) and thus alsodemonstrates the effectiveness of PAS.MT system Test setn-gram precision1 2 3 4BTG 32.75 74.39 41.91 24.75 14.91PASTR 33.24* 75.28 42.62 25.18 15.10PASTR+MEPD 33.78* 75.32 43.08 25.75 15.58IC-PASTR 33.95*# 75.62 43.36 25.92 15.58IC-PASTR+MEPD 34.19*# 75.66 43.40 26.15 15.92Table 2.
Result of baseline system and the MT sys-tems using our PAS-based disambiguation method.The ?*?
and ?#?
denote that the result is significantlybetter than BTG and PASTR respectively (p<0.01).Specifically, after integrating the inside con-text information of PAS into transformation, wecan see that system IC-PASTR significantly out-performs system PASTR by 0.71 BLEU points.Moreover, after we import the MEPD model intosystem PASTR, we get a significant improve-ment over PASTR (by 0.54 BLEU points).
Thesecomparisons indicate that both the inside contextintegration and our MEPD model are beneficialfor the decoder to choose better target-side-likePAS for translation.On the basis of IC-PASTR, we further add ourMEPD model into translation and get system IC-PASTR+MEPD.
We can see that this systemfurther achieves a remarkable improvement oversystem PASTR (0.95 BLEU points).However, from Table 2, we find that systemIC-PASTR+MEPD only outperforms system IC-PASTR slightly (0.24 BLEU points).
The resultseems to show that our MEPD model is not such1132useful after using IC-PASTR.
We will explorethe reason in section 7.5.7.4 Effectiveness of Inside Context Integra-tionThe method of inside context integration is usedto combine the inside context (gap strings) intoPAS for translation, i.e., extend the PASTR toIC-PASTR.
In order to demonstrate the effec-tiveness of inside context integration, we firstgive Table 3, which illustrates statistics on thematching PASs.
The statistics are conducted onthe combination of development set and test set.TransformationRulesMatching PASNone Gap PAS Gap PAS TotalPASTR 1702 1539 3241IC-PASTR 1546 832 2378Table 3.
Statistics on the matching PAS.In Table 3, for example, the line for PASTRmeans that if we use PASTR for the combinedset, 3241 PASs (column ?Total?)
can matchPASTRs in total.
Among these matching PASs,1539 ones (column ?Gap PAS?)
carry gap strings,while 1702 ones do not (column ?None GapPAS?).
Consequently, since PASTR does notconsider the inside context during translation, theGap PASs, which account for 47% (1539/3241)of all matching PASs, might be handled inappro-priately in the PAS-based translation framework.Therefore, integrating the inside context intoPASTRs, i.e., using the proposed IC-PASTRs,would be helpful for translation.
The translationresult shown in Table 2 also demonstrates thisconclusion.
(a) reference(c) translation result using IC-PASTR[for economic recovery , especially of investment confidence is][  A0  ] [                              PP                               ] [Pred] [      A1      ]?
?
?
?
????
??
??
?
???
??
??
??
[ a good sign ] [ for economic recovery , especially of investment confidence ]this is?
?
?
?
???
??
??
?
???
??
??
??
?
[a good sign]this(b) translation result using PASTR[  A0  ] [                              PP                               ] [Pred] [      A1      ]?
?
?
?
????
??
??
?
???
??
??
??
[a good sign]this is [for economic recovery and the restoration of investors ' confidence][  A0  ] [                            Pred                             ] [      A1      ]Figure 5.
Translation examples to verify the effec-tiveness of inside context.From Table 3, we can also find that the num-ber of matching PASs decreases after using IC-PASTR.
This is because IC-PASTR is more spe-cific than PASTR.
Therefore, for a PAS withspecific inside context (gap strings), even if thematched PASTR is available, the matched IC-PASTR might not.
This indicates that comparingwith PASTR, IC-PASTR is more capable of dis-tinguishing different PASs.
Therefore, based onthis advantage, although the number of matchingPASs decreases, IC-PASTR still improves thetranslation system using PASTR significantly.
Ofcourse, we believe that it is also possible to inte-grate the inside context without decreasing thenumber of matching PASs and we plan this asour future work.We further give a translation example in Fig-ure 5 to illustrate the effectiveness of our insidecontext integration method.
In the example, theautomatic SRL system ignores the long preposi-tion phrase ??
????
????
??
?????
for the PAS.
Thus, the system using PASTRscan only attach the long phrase to the predicate???
according to the parse tree, and meanwhile,make use of a transformation rule as follows:[X3] [X2] [A0]1 [Pred]2 [A1]3 [X1]source-side PAS(?)
target-side-like PASIn this way, the translation result is very bad, justas Figure 5(b) shows.
The long prepositionphrases are wrongly positioned in the translation.In contrast, after inside context integration, wematch the inside context during PAS transfor-mation.
As Figure 5(c) shows, the inside contexthelps to selects a right transformation rule as fol-lows and gets a good translation result finally.
[X1] [X2] [X4] [A0]1 [PP]2 [Pred]3 [A1]4 [X3]source-side PAS(?)
target-side-like PAS7.5 Effectiveness of the MEPD ModelThe MEPD model incorporates various contextfeatures to select better target-side-like PAS fortranslation.
On the basis of PASTR and IC-PASTR, we build 160 and 215 different MEPDclassifies, respectively, for the frequent source-side PASs.In Table 2, we have found that our MEPDmodel improves system IC-PASTR slightly.
Weconjecture that this phenomenon is due to twopossible reasons.
On one hand, sometimes, manyPAS ambiguities might be resolved by both in-side context and the MEPD model.
Therefore,the improvement would not be such significant1133when we combine these two methods together.On the other hand, as Table 3 shows, the numberof matching PASs decreases after using IC-PASTR.
Since the MEPD model works on PASs,its effectiveness would also weaken to some ex-tent.
Future work will explore this phenomenonmore thoroughly.PASTRRefPASTR+ MEPD...  ,  [??
]A0    [?
]Pred    [?
??
??
]A1  ?...
[the hague]     [is]      [the last leg]  ....  ,  [??]
[?]
[?
??
??]
?...
[the hague]   [is]   [his last stop]  ....  ,  [??
]A0    [?
]Pred    [?
??
??
]A1  ?...
[is]    [his last leg of]    [the hague] .Figure 6.
Translation examples to demonstrate theeffectiveness of our MEPD model.Now, we give Figure 6 to demonstrate the ef-fectiveness of our MEPD model.
From the Fig-ure, we can see that the system using PASTRsselects an inappropriate transformation rule fortranslation:[X1] [X3] [A0]1 [Pred]2 [A1]3 [X2]source-side PAS(?)
target-side-like PASThis rule wrongly moves the subject ???(Hague)?
to the end of the translation.
We do notgive the translation result of the BTG systemhere because it makes the same mistake.Conversely, by considering the context infor-mation, the PASTR+MEPD system chooses acorrect rule for translation:[X3] [X2] [A0]1 [Pred]2 [A1]3 [X1]source-side PAS(?)
target-side-like PASAs we can see, the used rule helps to keep theSVO structure unchanged, and gets the correcttranslation.8 Conclusion and Future WorkIn this paper, we focus on the problem of ambi-guities for PASs.
We first propose two ambigui-ties: gap ambiguity and role ambiguity.
Accord-ingly, we design two novel methods to do effi-cient PAS disambiguation: inside-context inte-gration and a novel MEPD model.
For insidecontext integration, we abstract the inside con-text and combine them into the PASTRs for PAStransformation.
Towards the MEPD model, wedesign a maximum entropy model for each ambi-tious source-side PASs.
The two methods suc-cessfully incorporate the rich context informationinto the translation process.
Experiments showthat our PAS disambiguation methods help toimprove the translation performance significantly.In the next step, we will conduct experimentson other language pairs to demonstrate the effec-tiveness of our PAS disambiguation method.
Inaddition, we also will try to explore more usefuland representative features for our MEPD model.AcknowledgmentsThe research work has been funded by the Hi-Tech Research and Development Program (?863?Program) of China under Grant No.2011AA01A207, 2012AA011101, and2012AA011102 and also supported by the KeyProject of Knowledge Innovation Program ofChinese Academy of Sciences under GrantNo.KGZD-EW-501.
We thank the anonymousreviewers for their valuable comments and sug-gestions.ReferencesWilker Aziz, Miguel Rios, and Lucia Specia.
(2011).Shallow semantic trees for smt.
In Proceedings ofthe Sixth Workshop on Statistical Machine Trans-lation, pages 316?322, Edinburgh, Scotland, July.Daniel Bikel.
(2004).
Intricacies of Collins parsingmodel.
Computational Linguistics, 30(4):480-511.David Chiang, (2007).
Hierarchical phrase-basedtranslation.
Computational Linguistics, 33 (2):201?228.Marine Carpuat and Dekai Wu.
2007a.
How phrase-sense disambiguation outperforms word sense dis-ambiguation for statistical machine translation.
In11th Conference on Theoretical and Methodologi-cal Issues in Machine Translation, pages 43?52.Marine Carpuat and Dekai Wu.
2007b.
Improvingstatistical machine translation using word sensedisambiguation.
In Proceedings of EMNLP-CoNLL2007, pages 61?72.Yee Seng Chan, Hwee Tou Ng, and David Chiang.2007.
Word sense disambiguation improves statis-tical machine translation.
In Proc.
ACL 2007, pag-es 33?40.Lei Cui, Dongdong Zhang, Mu Li, Ming Zhou andTiejun Zhao.
A Joint Rule Selection Model forHierarchical Phrase-Based Translation.
In Proc.of ACL 2010.1134Jason Eisner.
(2003).
Learning non-isomorphic treemappings for machine translation.
In Proc.
of ACL2003.Pascale Fung, Wu Zhaojun, Yang Yongsheng, andDekai Wu.
(2006).
Automatic learning of chineseenglish semantic structure mapping.
In IEEE/ACL2006 Workshop on Spoken Language Technology(SLT 2006), Aruba, December.Pascale Fung, Zhaojun Wu, Yongsheng Yang andDekai Wu.
(2007).
Learning bilingual semanticframes: shallow semantic sarsing vs. semantic soleprojection.
In Proceedings of the 11th Conferenceon Theoretical and Methodological Issues in Ma-chine Translation, pages 75-84.Qin Gao and Stephan Vogel.
(2011).
Utilizing target-side semantic role labels to assist hierarchicalphrase-based machine translation.
In Proceedingsof Fifth Workshop on Syntax, Semantics and Struc-ture in Statistical Translation, pages 107?115,Portland, Oregon, USA, June 2011.
Association forComputational LinguisticsZhongjun He, Qun Liu, and Shouxun Lin.
2008.
Im-proving statistical machine translation using lexi-calized rule selection.
In Proc.
of Coling 2008,pages 321?328.Franz Josef Och.
(2003).
Minimum error rate trainingin statistical machine translation.
In Proc.
of ACL2003, pages 160?167.Franz Josef Och and Hermann Ney.
(2004).
Thealignment template approach to statistical machinetranslation.
Computational Linguistics, 30:417?449.Dan Klein and Christopher D. Manning.
(2003).
Ac-curate unlexicalized parsing.
In Proc.
of ACL-2003,pages 423-430.Philipp Koehn, Franz Joseph Och, and Daniel Marcu.(2003).
Statistical phrase-based translation.
In Pro-ceedings of NAACL 2003, pages 58?54, Edmonton,Canada, May-June.Philipp Koehn.
(2004).
Statistical significance testsfor machine translation evaluation.
In Proceedingsof EMNLP 2004, pages 388?395, Barcelona, Spain,July.P Koehn, H Hoang, A Birch, C Callison-Burch, MFederico, N Bertoldi, B Cowan, W Shen, C Moranand R Zens, (2007).
Moses: Open source toolkit forstatistical machine translation.
In Proc.
of ACL2007.
pages 177?180, Prague, Czech Republic,June.
Association for Computational Linguistics.Mamoru Komachi and Yuji Matsumoto.
(2006).Phrase reordering for statistical machine translationbased on predicate-argument structure.
In Proceed-ings of the International Workshop on SpokenLanguage Translation: Evaluation Campaign onSpoken Language Translation, pages 77?82.Ding Liu and Daniel Gildea.
(2008).
Improved tree-to-string transducer for machine Translation.
InProceedings of the Third Workshop on StatisticalMachine Translation, pages 62?69, Columbus,Ohio, USA, June 2008.Ding Liu and Daniel Gildea.
(2010).
Semantic rolefeatures for machine translation.
In Proc.
of Coling2010, pages 716?724, Beijing, China, August.Qun Liu, Zhongjun He, Yang Liu, and Shouxun Lin.Maximum Entropy based Rule Selection Model forSyntax-based Statistical Machine Translation.
InProc.
of EMNLP 2008.Yang Liu, Qun Liu and Shouxun Lin.
(2006).
Tree-to-string alignment template for statistical machinetranslation.
In Proc.
of ACL-COLING 2006.Daniel Marcu, Wei Wang, Abdessamad Echihabi andKevin Knight.
(2006).
SPMT: Statistical machinetranslation with syntactified target languagephrases.
In Proc.
of EMNLP 2006, pages 44-52.Kishore Papineni, Salim Roukos, Todd Ward, andWei-Jing Zhu.
(2002).
Bleu: a method for automat-ic evaluation of machine translation.
In Proc.
ACL2002, pages 311?318, Philadelphia, Pennsylvania,USA, July.Slav Petrov, Leon Barrett, Romain Thibaux, and DanKlein.
(2006).
Learning accurate, compact, and in-terpretable tree annotation.
In Proceedings of the21st International Conference on ComputationalLinguistics and 44th Annual Meeting of the Asso-ciation for Computational Linguistics, pages 433?440, Sydney, Australia, July.
Association for Com-putational Linguistics.Andreas Stolcke.
(2002).
Srilm ?
an extensible lan-guage modelling toolkit.
In Proceedings of the 7thInternational Conference on Spoken LanguageProcessing, pages 901?904, Denver, Colorado,USA, September.Dekai Wu and Pascale Fung.
(2009a).
Can semanticrole labelling improve smt.
In Proceedings of the13th Annual Conference of the EAMT, pages 218?225, Barcelona, May.Dekai Wu and Pascale Fung.
(2009b).
Semantic rolesfor smt: A hybrid two-pass model.
In Proc.
NAACL2009, pages 13?16, Boulder, Colorado, June.ShuminWu and Martha Palmer.
(2011).
Semanticmapping using automatic word alignment and se-mantic role labelling.
In Proceedings of FifthWorkshop on Syntax, Semantics and Structure inStatistical Translation, pages 21?30, Portland, Or-egon, USA, June 2011.Xianchao Wu, Katsuhito Sudoh, Kevin Duh, HajimeTsukada, and Masaaki Nagata.
(2011).
Extractingpreordering rules from predicate-argument struc-tures.
In Proc.
IJCNLP 2011, pages 29?37, ChiangMai, Thailand, November.1135Deyi Xiong, Qun Liu, and Shouxun Lin.
(2006).
Max-imum entropy based phrase reordering model forstatistical machine translation.
In Proceedings ofthe 21st International Conference on Computa-tional Linguistics and 44th Annual Meeting of theAssociation for Computational Linguistics, pages521?528, Sydney, Australia, July.Deyi Xiong, Min Zhang, and Haizhou Li.
(2012).Modelling the translation of predicate-argumentstructure for smt.
In Proc.
of ACL 2012, pages902?911, Jeju, Republic of Korea, 8-14 July 2012.Nianwen Xue.
(2008).
Labelling chinese predicateswith semantic roles.
Computational Linguistics,34(2): 225-255.Feifei Zhai, Jiajun Zhang, Yu Zhou and ChengqingZong.
Machine Translation by Modeling Predicate-Argument Structure Transformation.
In Proc.
ofCOLING 2012.Hui Zhang, Min  Zhang, Haizhou Li and Eng SiongChng.
(2010).
Non-isomorphic Forest Pair Transla-tion.
In Proceedings of EMNLP 2010, pages 440-450, Massachusetts, USA, 9-11 October 2010.Tao Zhuang, and Chengqing Zong.
(2010a).
A mini-mum error weighting combination strategy for chi-nese semantic role labelling.
In Proceedings ofCOLING-2010, pages 1362-1370.Tao Zhuang and Chengqing Zong.
(2010b).
Joint in-ference for bilingual semantic role labelling.
InProceedings of EMNLP 2010, pages 304?314,Massachusetts, USA, 9-11 October 2010.1136
