Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers,pages 2018?2029, Dublin, Ireland, August 23-29 2014.Generating Acrostics via Paraphrasing and Heuristic SearchBenno Stein Matthias Hagen Christof Br?autigamBauhaus-Universit?at Weimar, Germany<first name>.<last name>@uni-weimar.deAbstractWe consider the problem of automatically paraphrasing a text in order to find an equivalent text thatcontains a given acrostic.
A text contains an acrostic, if the first letters of a range of consecutivelines form a word or phrase.
Our approach turns this paraphrasing task into an optimizationproblem: we use various existing and also new paraphrasing techniques as operators applicable tointermediate versions of a text (e.g., replacing synonyms), and we search for an operator sequencewith minimum text quality loss.
The experiments show that many acrostics based on commonEnglish words can be generated in less than a minute.
However, we see our main contribution inthe presented technology paradigm: a novel and promising combination of methods from NaturalLanguage Processing and Artificial Intelligence.
The approach naturally generalizes to relatedparaphrasing tasks such as shortening or simplifying a given text.1 IntroductionGiven some text, paraphrasing means to rewrite it in order to improve readability or to achieve otherdesirable properties while preserving the original meaning (Androutsopoulos and Malakasiotis, 2010).The paper in hand focuses on a specific paraphrasing problem: rewriting a given text such that it encodesa given acrostic.
A text contains an acrostic if the first letters of a range of consecutive lines form aword or phrase read from top to bottom.
A prominent and very explicit example of former GovernorSchwarzenegger is shown in Figure 1 (see the third and fourth paragraphs).
Schwarzenegger himselfcharacterized the appearance of that acrostic a ?wild coincidence?.1However, such a coincidence ishighly unlikely: Using the simplistic assumption that first letters of words are independent of each otherif more than ten words are in between (line length in the Schwarzenegger letter) and calculating withthe relative frequencies of first letters in the British National Corpus (Aston and Burnard, 1998), theprobability for the acrostic in Figure 1 can be estimated at 1.15 ?
10?12.
Typically, a given text will notcontain a given acrostic but has to be reformulated using different wording or formatting to achieve thedesired effect.
Thus we consider the purposeful generation of acrostics a challenging benchmark problemfor paraphrasing technology, which is subject to soft and hard constraints of common language usage.The paper shows how heuristic search techniques are applied to solve the problem.
Different paraphras-ing techniques are modeled as operators applicable to paraphrased versions of a text.
By pruning theso-formed search space and by employing a huge corpus of text n-grams for the possible operators, weare able to generate acrostics in given texts.
Our algorithmic solution is a novel combination of techniquesfrom Natural Language Processing and Artificial Intelligence.
We consider such combinations as a verypromising research direction (Stein and Curatolo, 2006; Sturtevant et al., 2012), and we point out thatthe problem of acrostic generation serves as a serious demonstration object: the presented model alongwith the heuristic search approach generalizes easily to other paraphrasing tasks such as text shortening,improving readability, or e-journalism.2 Related Work and Problem DefinitionRewriting a given text in order to ?encode?
an acrostic is a paraphrasing problem, which in turn is studiedin the domain of computational linguistics and natural language processing.
We review relevant literatureThis work is licenced under a Creative Commons Attribution 4.0 International License.
Page numbers and proceedings footerare added by the organizers.
License details: http://creativecommons.org/licenses/by/4.0/1www.huffingtonpost.com/2009/10/30/schwarzenegger-f-bomb-in_n_340579.html, last accessed: June 12, 2014.2018To the Members of the Californian State Assembly:I am returning Assembly Bill 1178 without my signature.F or some time now I have lamented the fact that major issues are overlooked while manyu nnecessary bills come to me for consideration.
Water reform, prison reform, and healthc are are major issues my Administration has brought to the table, but the Legislature justk icks the can down the alley.Y et another legislative year has come and gone without the major reforms Californianso verwhelmingly deserve.
In light of this, and after careful consideration, I believe it isu nnecessary to sign this measure at this time.Sincerely,Arnold SchwarzeneggerFigure 1: Excerpt from a letter of former Governor Arnold Schwarzenegger to the Californian StateAssembly in October 2009.
The third and fourth paragraphs contain the acrostic ?F???
You?.of the topic and highlight techniques that will be employed in our work.An important branch of the paraphrasing literature focuses on analyses with fixed corpora.
Such corporatypically are parallel in the sense that they contain different formulations of the same facts (Barzilay andMcKeown, 2001; Barzilay and Lee, 2003; Callison-Burch, 2008).
These ?facts?
can be news articles onthe same event (Clough et al., 2002; Dolan and Brockett, 2005), different translations of a source text to atarget language (Pang et al., 2003), or cross-lingual parallel corpora (Bannard and Callison-Burch, 2005).As most of the early parallel corpora were constructed manually (especially the judgments of whether apair of sentences forms a paraphrase), there are two shortcomings.
First, the obtained paraphrases areusually specific to the domain covered in the corpus (e.g., showbiz news) and often do not generalize well.Second and probably more severe is the fact that manually building parallel corpora is very expensive,such that the available ones are rather small: the METER corpus contains only 1717 texts on legal issuesand showbiz (Clough et al., 2002), the MSRP corpus contains only 5801 sentence pairs (Dolan andBrockett, 2005).
Recently, new methods employ machine learning techniques to automatically build largerparaphrase collections from parallel corpora (Ganitkevitch et al., 2011; Ganitkevitch et al., 2013; Metzleret al., 2011; Metzler and Hovy, 2011).
We include the paraphrase database (Ganitkevitch et al., 2013)?adatabase of extracted patterns from such large scale corpora?as one source of potential paraphrases inour algorithm.Compared to the large body of literature that ?extracts?
paraphrases from (parallel) corpora, there isrelatively little work on automatically paraphrasing a given text.
Some of the early generation methodsare based on rules that encode situations wherein a reformulation is possible (Barzilay and Lee, 2003).
Aproblem with rules is that often the rather complicated patterns extracted from text corpora are hardlyapplicable to a given to-be-paraphrased text: manually created corpora are simply too small and machinegenerated paraphrasing rules often do not match in a given text.
Other early methods use machinetranslation (Quirk et al., 2004).
However, the need for large and expensive parallel manual translationcorpora cannot be circumvented by using multiple resources (Zhao et al., 2008).Another branch of paraphrasing methods is based on large thesaurus resources such as WordNet (Fell-baum, 1998).
The idea is to insert synonyms into a text when the context fits (Bolshakov and Gelbukh,2004; Kauchak and Barzilay, 2006).
The most recent approaches are statistics-based (Chevelu et al., 2009;Chevelu et al., 2010; Zhao et al., 2009; Burrows et al., 2013).Compared to the existing research, we have a more difficult use case here.
Existing paraphrasegeneration focuses on sentence paraphrasing, while we have to consider a complete text that has to berewritten/reformatted in order to contain a given acrostic.
We will employ the above shown state-of-the-artparaphrasing procedures as ?operators?
in our approach.
This new problem setting of applying differentoperators to a complete text forms a search problem with a huge search space.
In order to deal withthis search space, we apply powerful search heuristics from Artificial Intelligence.
The combination ofheuristic search with established text level paraphrasing techniques represents a new approach to tackleproblems in computational linguistics.
The acrostic generation problem is defined as follows:2019ACROSTIC GENERATIONGiven: (1) A text T and an acrostic x .
(2) A lower bound lminand an upper bound lmaxon the desired line length.Task: Find a paraphrased version T?of T in monospaced font that encodes x in someof its lines when possible.
Each line of T?has to meet the length constraints.3 Modeling Paraphrasing as Search ProblemThis section shows how to model paraphrasing in general and ACROSTIC GENERATION in particular asa search problem (Pearl, 1984).
The search space is a universe T of candidate texts for which we candevise, at least theoretically, a systematic search strategy: if T is finite, each element n ?
T is analyzedexactly once.
The elements in T represent states (nodes), and there is a limited and a-priori known set ofpossibilities (edges, paraphrasing operators) to get from a node n to an adjacent or successor node ni.
Aparaphrasing operator ?
provides a number of parameters that control its application.
Each state n ?
Tis considered an acrostic (sub)problem; the dedicated state s ?
T represents the original problem while?
?
T is the set of solution nodes that have no problem associated with.
The following subsectionswill outline important properties of the search space and introduce a suited cost measure to control theexploration of T .3.1 Search Space StructureSolving an instance of ACROSTIC GENERATION under a so-called state-space representation means tofind a path from s, which represents the original text T , to some goal state ?
?
?.
The problem of findingan acrostic consists of tightly connected subproblems (finding subsequences of the acrostic) that cannotbe solved independently of each other.
Most puzzles such as Rubik?s cube are of this nature: changing adecision somewhere on the solution path will affect all subsequent decisions.
By contrast, a so-calledproblem-reduction representation will exploit the fact that subproblems can be solved independently ofeach other.
Many tasks of logical reasoning and theorem proving give rise to such a structure: given a setof axioms, the lemmas required for a proof can be derived independently, which in turn means that thesought solution (a plan or proof) is ideally represented by a tree.Searching T under a state-space representation means to unfold a tree whose inner nodes link tosuccessor nodes that encode alternative decisions; hence these inner nodes are also called OR-nodes.
Eachpath from s that can be extended towards a goal state ?
forms a solution candidate.
Similarly, searchingT under a problem-reduction representation also means to unfold a tree?however, the tree?s inner nodesmust be distinguished as AND-nodes and OR-nodes, whereas the successors of an AND-node encodesubproblems all of which have to be solved.
A solution candidate then is a tree comprised of (1) the root s,(2) OR-nodes with a single successor, and (3) AND-nodes with as many successors as subproblems all ofwhich are characterized by the fact of being extensible towards goal states in ?.
Figure 2 contrasts bothsearch space structures.Under either representation, OR-graphs as well as AND-OR-graphs, the application of a sequence of??
[T, x] Problem specificationDead end: node with unsolvable problemGoal state: no problem associated with nodeOR-node AND-node(a) (b)Legend:Solutioncandidate...n4 n6n5n1sn3n2[T, x][T', y][T*]...... ?
?n4 n6n5n1sn2[T, x][Ty , y]?...
[Tz , z]??
[Ty*] *][Tz...Figure 2: (a) State-space representation (OR-graph) versus (b) Problem-reduction representation (AND-OR-graph).
OR-nodes encode alternative decisions, while AND-nodes decompose a problem into sub-problems all of which are to be solved.2020operators will easily lead to situations where states are revisited?precisely: are generated again.
Searchalgorithms maintain so-called OPEN- and CLOSED-lists to manage the exploration status of generatednodes.
However, because of the intricate state encoding, which must inform about the effect of all appliedoperators from s to an arbitrary node, the exponentially growing number of nodes during search, and thenecessity of efficiently querying these lists, sophisticated data structures such as externalized hash tablesand key value stores are employed.
Typically, these data structures are tailored to the problem domain(here: to paraphrasing), and they model heuristic access strategies to operationalize a probabilisticallycontrolled trade-off between false positive and true negative answers to state queries.We have outlined the different search space structures since ACROSTIC GENERATION may show an OR-graph puzzle nature at first sight: paraphrasing at some position will affect all following text.
Interestingly,there is a limited possibility to introduce ?barriers?
in the text, which allows for an AND-OR-graphmodeling and hence for an isolated subproblem treatment.
Examples include paraphrasing operators thatdo not affect line breaks, or acrostics consisting of several words and thus spanning several paragraphs.Since in general the underlying linguistic considerations for the construction and maintenance of suchbarriers are highly intricate and complex, we capture this structural constraint probabilistically as shownin Equation (1).
The equation models the problem difficulty or effort for acrostic generation, E, andintroduces Py?z(|x|), which quantifies the probability for the event that an acrostic x can be treatedindependently as two partial acrostics y and z, where x = yz.E(x) =??
?e(x) If |x| = 1.Py?z(|x|) ?
(E(y) + E(z))+ (1?
Py?z(|x|)) ?
E(y) ?
E(z) If |x| > 1.(1)Remarks.
The effort for generating a single-letter acrostic of length 1 is e(x), with e(x) ?
1.
Based one(x), we recursively model the true effort E(x) for generating an acrostic x = yz as follows: as additiveeffort if the generation of the acrostics y and z can be done independently, and as multiplicative effortotherwise.
Observe how Py?zcontrols the search space structure: if Py?z= 0 for all partitionings ofx into y and z, one obtains a pure state-space representation for ACROSTIC GENERATION.
Similarly,the other extreme with Py?z= 1 results in a series of |x| letter generation problems that can be solvedindependently of each other.As an estimate e?
(x) for e(x) we suggest the multiplicative inverse of the occurrence probabilitiesof the first letters in the English language, as computed from the British National Corpus (BNC).
TheBNC is a 100 million word collection of written and spoken language from a wide range of sources,designed to represent a wide cross-section of current British English (Aston and Burnard, 1998).
TheBNC probabilities vary between 0.115719 for the letter ?s?
and 0.00005 for the letter ?z?.
As an estimatefor Py?zwe suggest the N(5, 0.5) distribution to model the paragraph lengths in T or, equivalently, thenumber of characters of the (English) words in x.
These choices give rise to?E(x), the estimated effort forgenerating an acrostic x.?E(x) is used to assess the (residual) problem complexity and, under a maximumlikelihood approach, models the expected search effort.
There is a close relation between the effortestimate?E and the quality estimate?Q introduced below, which will be exploited later on, in Equation (3).3.2 Cost Measure StructureCost measures?equivalently: merit measures?form the heart of systematic search strategies and de-termine whether an acceptable solution of a complex problem can be heuristically constructed withinreasonable time.
Here, we refer to general best-first search strategies as well as variants that relax strictadmissibility.
As a working example consider the following text T about Alan Turing taken from theEnglish Wikipedia, where the task is to generate the acrostic x = Turing with lmin= 55 and lmax= 60.Alan Mathison Turing was a British mathematician, logician,cryptanalyst and computer scientist.
He was highly influential inthe development of computer science, giving a formalization of theconcepts of algorithm and computation with the Turing machine,which can be considered a model of a general purpose computer.2021A possible solution T?
(a paragraph?s last line may be shorter than lmin) :T he British mathematician Alan Mathison Turing was also anu nrivaledlogician,cryptanalystandcomputerscientist.Her evolutionized the development of computer science, giv-i ng a formalization of the concepts of algorithm and defi-n ite computation with the Turing machine, which can be re-g arded a model of a general purpose computer.T?is of a high quality though it introduces an exaggerating tone, this way violating Wikipedia?sneutrality standard.
Also note that the applied paraphrasing operators vary in their quality, which is rootedin both the kind and the context of the operators.
Table 1 (left) shows a selection of the operators, some ofwhich are applied in a combined fashion.
Section 4 introduces the operators in greater detail.To further formalize the quantification of a cost measure C or a merit measure Q, we stipulate on thefollowing properties:1.
The quality of the original text T cannot be improved.
Each paraphrasing operator ?
introducesunavoidable deficiencies in T .2.
The overall quality of a solution T?depends on the quality of all applied paraphrasing operators.3.
Following readability theory and relevant research, the severity of text deficiencies?here introducedby a paraphrasing operator ?
?has a disproportionate impact on the text quality (Meyer, 2003).4.
To render different problems and solutions comparable, the achieved quality of a solution T?has tobe normalized.Equation (2) below shows the basic structure of Q, the proposed, unnormalized merit measure.
Itsoptimization yields Q?.
Q?
(n) assigns to a node n ?
T the maximum paraphrasing quality of a text T?that contains the partial acrostic associated with n. Likewise, Q?
(s) characterizes the quality of theoptimum solution for solving ACROSTIC GENERATION.1Q?(n)=??
?0 If n ?
?.mini{1q(n, ni)+1Q?(ni)}Otherwise.(2)Remarks.
The state (node) nidenotes a direct successor of the state (node) n in the search space T .Associated with niis a text resulting from the application of a paraphrasing operator ?
to the text associatedwith n, whereas q(n, ni) quantifies the local quality achieved with ?.
The measure in Equation (2) isboth of an additive form and formulated as a minimization problem.
As shown in the following, it canbe reformulated for a best-first algorithm scheme, ensuring admissibility under a delayed terminationcondition.
Also note that the merit measure operationalizes the above Property 3 via the harmonic meancomputation.
Accordingly, we obtain a normalized overall quality?Q?given an acrostic x as?Q?= |x|?Q?.To turn Equation (2) into actionable knowledge, the quality q(n, ni) of a paraphrasing operator ?
whenmoving from n to a successor nineeds to be quantified.
We employ for q the domain [0; 1], where 0 and 1encode the worst and best achievable quality respectively.
By construction the normalized quality?Q?willthen lie in the interval [0; 1] as well, thus greatly simplifying the interpretation of the measure.Table 1 (right) shows values for the local quality of the operators in the Alan Turing example, whichare derived from linguistic quality considerations and the experimental analysis detailed in Section 5.
Thecomment column argues the linguistic meaningfulness.
If we agree on q = 1.0 for the first two lines ofthe generated acrostic x = Turing and recursively apply the merit measure defined in Equation (2), weobtain Q = 0.127 as unnormalized and?Q = |x| ?Q = 0.76 as normalized overall quality.To make Equation (2) applicable as cost estimation heuristic f(n) in a best-first algorithm scheme,Equation (3) below unravels its recursive structure in the usual way as f(n) = g(n)+h(n).
The semanticsis as follows: under an optimistic estimate h(n) (= underestimating costs or overestimating merits) the2022Table 1: Left: Paraphrasing operators in the Alan Turing example.
Right: Values for the local quality ofthe respective operators, which entail the normalized overall quality?Q = 0.76 for the example.Line Operator ?
Text?
paraphrased text3 synonym highly influential?revolutionized4 hyphenation giving?
giv-ing5 tautology computation?
defi-nite computation6 synonym considered?
re-gardedq(n, ni) Comment0.9 stylistically well, exaggerating tone0.6 unexpected hyphen for a short word0.6 tautology arguable, hyphen unusual0.7 synonym suited, hyphen acceptabletotal cost (the overall quality) for solving ACROSTIC GENERATION via a path along node n is alwayslarger (smaller) than f(n).
In particular, g(n) accumulates the true cost (the achieved quality) for thepartial acrostic via a concrete path s = n0, n1, .
.
.
, nk= n, while h(n) gives an underestimation of thecost (overestimation of the quality) for the remaining part of the acrostic.
Observe that the additive formof Equation (2) guarantees the parent discarding property (Pearl, 1984), which states that no decision on apath from n to a goal state ?
can change the value for g(n).A tricky part is the construction of h(n), which, on the one hand, may ensure admissibility, while, onthe other hand, should be as close as possible to the real cost.
Here, the measure E(x) for the problemdifficulty from Equation (1) comes into play, which models the problem decomposability and whichinforms us about the largest remaining subproblem (= the depth of the deepest remaining OR-graph) whensolving x.
Without loss of generality, admissibility is ensured if (a) the probability Py?zused in?E(x)is biased towards decomposability, and if (b) we assume that the remaining acrostic x can be solved byalways applying the cheapest (maximum quality-preserving) operator qmax.1?Q(n)?
??
?f(n)=k?i=11q(ni?1, ni)?
??
?g(n)+ logK(?E(?(n)))?1qmax?
??
?h(n), where n0= s, nk= n (3)Remarks.
?
(n) denotes the remaining acrostic x that is associated with node n ?
T .
The logarithmbase K serves for normalization purposes with regard to the BNC letter frequencies e?
(x), |x| = 1,which are used within?E(x) in Equation (1).
We define K as the multiplicative inverse of the occurrenceprobability of the least frequent letter in the remaining acrostic x = ?
(n), which gives rise to the inequalitylogK(?E(x)) ?
|x|.
This choice entails two properties: (1) it underestimates the remaining acrostic lengthand hence ensures the admissibility characteristic of h(n), and, (2) it yields an increasing accuracy ofh(n) when approaching a goal state in ?.
Finally, we can substitute 1.0 as an upper bound for qmax, againpreserving the admissibility of h(n).Admissibility, i.e., the guarantee of optimality during best-first search, may not be the ultimate goal: ifh(n) underestimates costs (overestimates merits) too rigorously, best-first search degenerates to a kindof breadth-first search?precisely: to uniform-cost search.
Especially if computing power is a scarceresource, we may be better off with a depth-preferring strategy.
Observe that the logarithm base K inEquation (3) provides us a means to smoothly vary between the two extremes, namely by choosing Kfrom [Kmin;Kmax], where Kmin(Kmax) specifies the multiplicative inverse of the occurrence probabilityof the most (least) frequent letter in the remaining acrostic x = ?
(n).4 Paraphrasing OperatorsMost of the following operators used in our heuristic search process employ state-of-the-art linguistictools or are based on standard knowledge from Wikipedia.
Table 2 shows information about the role ofindividual operators in our experiments from Section 5; the table illustrates also the effort for preparing(offline) and applying (online) the operators.20234.1 Context-Independent OperatorsLine break Since we are dealing with text that should spread over several lines, breaking between linesis one of the most basic operators.
Similarly, it is the most efficient operator, and Column 4 of Table 2illustrates the performance of the others in relation to this operator.
Line breaks are possible at the end ofsentences (i.e., a paragraph break), while a line break in between words is only possible if it falls in the[lmin; lmax]-window given by the line length constraints.Hyphenation Related to line breaks are hyphenations.
We re-implemented and employ the standardTEX hyphenation algorithm (Knuth, 1986).
Analogous to line breaks, hyphenation is applicable if the lineafter hyphenating (and line breaking) has a length in the [lmin; lmax]-window.Function word synonym Specific groups of so-called synsemantic words can often be replaced byeach other without changing a text?s meaning.
We have identified 40 such groups from a list of SequencePublishing2and the Paraphrase Database (Ganitkevitch et al., 2013).
Examples are {can, may}, {so, thus,therefore, consequently, as a result}, and {however, nevertheless, yet}.Contraction and expansion Some local text changes can be achieved by contracting or expanding for-mulations like ?he?ll?
or ?won?t?.
We have identified 240 such pairs from Wikipedia.3Other possibilitiesare to spell out / contract standard abbreviations and acronyms.
We have mined a list of several thousandsuch acronyms from the Web.4Finally, also small numbers can be spelled out or be written as numerals(e.g., ?five?
instead of ?5?).
It is interesting to note that this operator was hardly ever used on successfulpaths in our experiments.Spelling In principle, we want to generate text that is correctly spelled.
In certain situations, however, itcan nevertheless be beneficial to introduce some slight mistakes in order to change word lengths or togenerate letters not present in the correctly spelled text.
We employ a list of 3 000 common misspellingsmined from Wikipedia5(e.g., ?accidently?
instead of ?accidentally?).
We also include several standardtypos related to computer keyboards (e.g., an ?m?
is often typed as an ?n?
and vice versa) as well asphonetic misspellings (e.g., ?f?
and ?ph?
often sound similar).
Since the quality score of wrong spellingstends to be low, this operator has to be treated with care.
Especially at the beginning of words, typos areless common than within words such that we allow typos only within words.Wrong hyphenation Similar to wrong spellings is the purposeful choice of a wrong hyphenation.
Aswith wrong spellings the quality score is typically low.
We thus employ this operator very carefully,avoiding for instance syllables on a new line with just two letters.
Analogous to correct hyphenation, theline length has to be in the [lmin; lmax]-window to apply wrong hyphenation.
Despite its questionablequality, this operator is used pretty often in the experiments since it has a very high probability of?generating?
a desired letter.4.2 Context-Dependent OperatorsSynonym For identifying synonyms, WordNet?s synsets (Fellbaum, 1998) is used.
Since only a smallsubset of the synset members of a to-be-replaced word w is reasonable in the context around w in T ,we check in the Google n-gram corpus (Brants and Franz, 2006) whether the synonym in fact fits in thesame context.
In this regard the public and highly efficient Netspeak API (Stein et al., 2010; Riehmannet al., 2012) is employed.
For example, given ?hello world?, the most frequent phrase with a synonymfor world is ?hello earth?.
The Google n-grams are up to five words long, such that at most four contextwords can be checked before or after w. Previous studies showed that more context yields higher qualitysynonyms (Metzler and Hovy, 2011; Pasca and Dienes, 2005), so that we use at least two words before orafter w. Higher quality scores are achieved if the context is matched before as well as after w.2sequencepublishing.com/academic.html\#function-words, last accessed: June 12, 20143en.wikipedia.org/wiki/List_of_English_contractions\#English, last accessed: June 12, 2014,en.wikipedia.org/wiki/English_auxiliaries_and_contractions, last accessed: June 12, 20144www.acronymfinder.com, last accessed: June 12, 20145en.wikipedia.org/wiki/Wikipedia:Lists_of_common_misspellings/For_machines, last accessed: June 12, 20142024Table 2: Statistics for the applicability, usage, and effort of single operators.
?Application probability?reports whether an operator is applicable at all at some node, ?Usage?
reports the application probabilityon a solution path, ?Effort?
reports the (online) application effort as multiple of the fastest operator (theLine break operator), and ?Offline time?
reports the preprocessing time in ms per word before the actualsearch is started.
All numbers are profiled within the experiment setup described in Section 5.Paraphrasing operator Application probability Usage probability Effort Offline time(in %) (in %) (multiple of Line break) (in ms per word)Line break 16.14 21.13 1.00 0.00Hyphenation 5.48 9.38 1.22 1.38Function word synonym 1.43 2.57 1.33 0.01Contraction and expansion 0.29 0.00 1.93 0.02Spelling 6.98 1.57 1.18 0.11Wrong hyphenation 9.53 37.63 1.07 1.38Synonyms 16.69 2.47 1.66 23.24Word insertion or deletion 43.46 25.24 2.46 42.75Average 12.50 12.50 1.49 14.35Word insertion or deletion Similar to the synonym replacement, the insertion or deletion of shortphrases is handled.
For all positions of the given text, the Google n-grams are checked with Netspeak(see above) for a word w that sufficiently often appears within the context of the text.
Similarly, for eachword w in the text, it is checked whether there are sufficiently many n-grams without the word but thesame surrounding context.
In both cases, w is a candidate to be inserted or deleted.
Given ?hello world?,the most frequently used intermediate word is ?cruel?, yielding the phrase ?hello cruel world.?
Again,as with synonyms, context size and quality are positively correlated.
We thus use at least two words ascontext and favor variants that match more context.4.3 Further Operator IdeasIn pilot experiments, also the three operator ideas discussed below were analyzed.
The ideas showpromising results for specific cases, but they easily lead to unexpected text flows due the introduction ofodd sentences or names.
The operators require future work to better fit them in the given text?s context,and they are not employed within the experiments in Section 5.Tautology It is often possible to introduce entirely new phrases or sentences in a text, which mayconfirm a previous sentence or which introduce a (nearly) arbitrary but true statement.
We tested a smalllist, including among others ?As a matter of fact this is true.?
or ?I didn?t know that until now.?
However,due to improper context such tautologies may mess up a text significantly.Sentence beginning The beginning of a sentence can often be modified without changing its meaning.Possibilities include the addition of function words like ?in general?
or ?actually?, but also the additionof a prefix like ??someone?
said that .
.
.
?
or ??time?
?someone?
said that .
.
.
?
where ?someone?
is to bereplaced by a person?s name or {I, he, she} depending on the full context of the text (e.g., author?s nameor gender of name mentioned before).
The ?time?
expression may expand to ?yesterday?
or ?last week?,etc.
Especially with the usage of names, a whole bunch of letters can be generated.
However, context ismore subtle for this operator compared to the usage of Google n-grams.Full PPDB The paraphrase database (Ganitkevitch et al., 2013) comes in different sizes and qualitylevels.
Many synonymity relations for nouns are already covered by WordNet, and function wordreplacements are already an operator on their own.
Still, the rich variety of the full data set can form asemantically strong operator.
However, in our pilot experiments, the full PPDB patterns often decreasedtext quality unacceptably, such that we refrained to use PPDB as a single operator in our experiments.20255 Experimental EvaluationGoal of the evaluation is to show that our approach is able to efficiently generate acrostics in differentsituations.
In this regard, we analyze the general success of acrostic generation, the influence of differentoperators, and effects on the text quality.5.1 Experiment SetupTo model different ?use cases?
in which acrostics have to be inserted, we use texts of different genres:newspaper articles, emails, and Wikipedia articles.
We sample 50 newspaper articles from the ReutersCorpus Volume 1 (Lewis et al., 2004), 50 emails from the Enron corpus (Klimt and Yang, 2004), and50 articles from the English Wikipedia.
Each text contains at least 150 words excluding tables and lists.As target acrostics for all of the above text types, the 25 most common adjectives, nouns, prepositions,verbs, and 50 other common English words are chosen (in total 150 words).6This scenario reflects theinclusion of arbitrary words.
Other target acrostics are formed by the 100 most common male and femalefirst names from the US (in total 200 words).7This models the standard poetry usage of acrostics whereoften a writer?s name is encoded.
For all input texts, we also model self-referentiality by using a text?s firstphrases as the target acrostics (in total 150 phrases for which at least the first word has to be generated).
Inthese cases, the first letter of the acrostic is also the first letter of the text?a fact that enables the controlledevaluation of the importance of the producibility of the first letter.The evaluation system is a standard quad-core PC running Ubuntu 12.04 with 16 GB of RAM.
Arelevant subset of all operator application possibilities is preprocessed and stored in-memory (e.g., thesynonym n-gram frequencies for every word), whereas the preprocessing time (about one minute in totalper run) is not counted for the search process.
We then conduct an A?search using the preprocessedoperator tables and an admissible instance of Equation (3).
To safe runtime, we slightly transform theproblem setting and require the acrostic to start at the beginning of the given text.
Pilot experiments showthat a good choice for line lengths is lmin= 50 and lmax= 70.
Note that this is only slightly more flexiblethan a standard line length between 55 and 65 characters (i.e., about 10-12 words) but eases acrosticgeneration.
The experiments also reveal that a successful run (the acrostic can be generated) usually takesless than 30 seconds for the search part.
An unsuccessful run (the acrostic cannot be generated) takes fiveto ten minutes until its termination caused by the memory constraints for the open list.5.2 Experiment DiscussionGiven our hardware and time restrictions, about 20% of the runs are successful altogether.
The producibil-ity of the first letter is critical for the overall success: we observe an almost 90% success rate for theself-referential acrostics compared to the about 20% for all others.
Statistics for the successful runs aregiven in Table 3.
As can be seen, our system is able to generate about 90 000 nodes with 550 goal checksper second.
This yields reasonable answer times on the test acrostics: the average number of goal checksneeded when the acrostic can be generated is below 10 000 (about 20 seconds of runtime).
Only very fewsuccessful runs took more than 40 seconds; the self-referential acrostics that often are two or three wordslong form the main exception.
Not that surprisingly, shorter acrostics are on average generated faster thanlonger ones.
Interestingly, besides self-referential acrostics, male first names seem to be the most difficultacrostics when taking the required runtime into account.
Note in this regard that many of the (longer)female names start with a more common first letter, which can be generated faster.Since our approach is the first attempt at the problem of acrostic generation, we cannot compare to othersystems from the literature.
Instead, we compare to a baseline system that can only use line breaking andhyphenation as its operators.
This also helps to further examine the effect of the producibility of the firstletter.
Whenever the acrostic?s first letter is not the first letter of the text, the baseline fails right from thestart: recall that for our experiments we require the acrostic to start at the text?s beginning.
For less than1% of our test cases, the baseline can generate the acrostic.
Most of these few cases are self-referentialfirst words.
Even if the first letter is already present, usually the second or third one are not producible by6en.wikipedia.org/wiki/Common_English_words, last accessed: June 12, 2014.7www.ssa.gov/OACT/babynames/decades/century.html, last accessed: June 12, 2014.2026Table 3: Experimental results for complete acrostic generations.
For each of the acrostic types (Col-umn 1), several thousand runs were conducted for which we report averaged values of the acrosticlengths in letters (Column 2).
The columns 3-10 relate to successful generations and report averagedvalues for the runtime in seconds, size of the explored search tree, generated nodes and goal checks persecond, used main memory, and the quality change according to the introduced measures.Acrostic type Length Runtime Nodes Nodes Goal checks Memory Quality-related measures(in letters) (total in s) (total) (per s) (per s) (in MByte)?
WFC ?
ARI ?
SMOGCommon English wordsAdjective 4.36 3.25 286 960 88 269 578 270 -0.99 -1.61 -0.91Noun 4.47 3.40 285 016 83 837 576 277 -0.39 -0.96 -0.50Preposition 3.44 3.16 280 593 88 853 556 243 -1.59 -2.28 -1.29Verb 3.59 2.76 251 161 90 898 595 236 -0.95 -1.60 -0.92Other 3.29 2.41 218 974 90 755 601 206 -1.10 -2.05 -1.11Common US first namesMale 6.00 9.32 851 665 91 368 554 649 -0.74 -1.87 -0.93Female 6.07 7.82 740 418 94 693 546 575 -0.60 -1.77 -0.93Self-referentialFirst words 10.33 36.09 3 164 873 87 690 518 1 985 -0.31 -0.09 0.20Average 5.19 8.53 759 957 89 545 565 372 -0.83 -1.53 -0.80the simple operators.
Using all operators, our approach is able to produce a self-referential acrostic ofmore than seven characters in 80% of the cases.
On average, acrostics of ten characters are possible for theself-referential cases.
This further highlights the importance of the first letter: whenever it is producible,the success ratio is much higher.To compare the importance of the different operators, we count for the successful generations in theexperiments of Table 3, how often operators are used and how long the search paths are.
Table 2 containsinformation on the applicability of the different operators.
About 21% of the operator applications are linebreaks, another 9% are hyphenations.
Interestingly, about 38% of the operator applications are wronghyphenations despite the low quality of this operator.
Even though our heuristic tries to avoid wronghyphenations, there are a lot of situations where all other operators fail.
Although not reflected by standardquality metrics (see next subsection), a wrong hyphenation usually is eye-catching for human readers,which gives rise to a desirable further quality improvement that should be aimed for in future work.
Theother operator usages are mostly word insertions and deletions (about 25%), synonym replacements (3%),and function words (3%).
The context-independent operators of contractions and spelling correspond toonly about 1% of all operator applications.5.3 Quality-Related AnalysisTable 3 also contains information about the text quality before and after generating the acrostic.
Toalgorithmically measure text quality-related effects, we employ a word frequency class analysis and areadability analysis.The frequency class WFC(w) of a word w relates to its customariness in written language and hassuccessfully been employed for text genre analysis (Stein and Meyer zu Ei?en, 2008).
Let ?
(w) denotethe frequency of a word in a given corpus; then the Wortschatz8defines WFC(w) as blog2(?(w?)/?
(w))c,where w?denotes the most frequently used word in the respective corpus.
Here we use as reference theGoogle n-gram corpus (Brants and Franz, 2006) whose most frequent word is ?the?, which corresponds tothe word frequency class 0; the most uncommonly used words within this corpus have a word frequencyclass of 26.
The readability of the text before and after acrostic generation is quantified according tothe standard ARI (Smith and Senter, 1967) and SMOG (McLaughlin, 1969) measures, implementedin the Phantom Readability Library.9Both measures have been designed to estimate the U.S. grade8wortschatz.uni-leipzig.de, last accessed: June 12, 2014.9http://niels.drni.de/s9y/pages/phantom.html, last accessed: June 12, 20142027level equivalent to the education required for understanding a given text.
Hence, larger readabilityscores indicate more difficult texts.
The Automated Readability Index ARI (Smith and Senter, 1967) isdesigned for being easily automatable and uses only the number of characters (excluding whitespace andpunctuation), words, and sentences in the text (delimited by a period, an exclamation mark, a questionmark, a colon, a semicolon, or an ellipsis).
The Simple Measure of Gobbledygook SMOG (McLaughlin,1969) includes the number of words with more than three syllables, so-called polysyllables.ARI = 4.71 ?CharactersWords+ 0.5 ?WordsSentences?
21.43 SMOG = 1.0430 ?
?30 ?PolysyllablesSentences+ 3.1291Of course, the above three measures cannot capture text quality as human judges would perceive it.
Still,they have their merits and can indicate interesting trends: On average, the texts after acrostic generationuse more common words (cf.
the negative ?
WFC) and are easier to read (cf.
the negative ?
ARI and?
SMOG) for almost all acrostic types.
Thus, one may argue that the quality is not harmed too much; stillsome issues like wrong hyphenation are ignored by the metrics (cf.
the above discussion of individualoperators).
A deeper analysis of operator quality and improved quality of paraphrased texts (e.g., furtheroperators or avoiding wrong hyphenations) constitute very promising directions for future work.6 Conclusion and OutlookWe have presented the first algorithmic approach to acrostic generation.
The experiments show that theheuristic search approach is able to generate about 20% of the target acrostics in reasonable time, whereasthe producibility of the first letter plays a key role.
Our solution successfully combines paraphrasingtechniques from Natural Language Processing with a heuristic search strategy from Artificial Intelligence.This way, our problem modeling opens a novel and very promising research direction, and the applicationof our framework to other paraphrasing problems is the most interesting line of future work.
As for theacrostic use case, the resulting text?s quality gives the most obvious possibility for improvements.
We planto further analyze better quality measures for the individual operators and to develop more sophisticatedoperators like changing a text?s tense or even anaphora exploitation (Schmolz et al., 2012).ReferencesIon Androutsopoulos and Prodromos Malakasiotis.
2010.
A Survey of Paraphrasing and Textual EntailmentMethods.
Journal of Artificial Intelligence Research, 38(1):135?187.Guy Aston and Lou Burnard.
1998.
The BNC Handbook.
http://www.natcorp.ox.ac.uk.Colin J. Bannard and Chris Callison-Burch.
2005.
Paraphrasing with Bilingual Parallel Corpora.
In Proceedingsof ACL 2005, pages 597?604.Regina Barzilay and Lillian Lee.
2003.
Learning to Paraphrase: An Unsupervised Approach Using Multiple-Sequence Alignment.
In Proceedings of HLT 2003, pages 16?23.Regina Barzilay and Kathleen McKeown.
2001.
Extracting Paraphrases From a Parallel Corpus.
In Proceedingsof ACL 2001, pages 50?57.Igor A. Bolshakov and Alexander F. Gelbukh.
2004.
Synonymous Paraphrasing Using WordNet and Internet.
InProceedings of NLDB 2004, pages 312?323.Thorsten Brants and Alex Franz.
2006.
Web 1T 5-gram Version 1.
Linguistic Data Consortium LDC2006T13.Steven Burrows, Martin Potthast, and Benno Stein.
2013.
Paraphrase Acquisition via Crowdsourcing and MachineLearning.
ACM Transactions on Intelligent Systems and Technology, 4(3):43:1?43:21.Chris Callison-Burch.
2008.
Syntactic Constraints on Paraphrases Extracted from Parallel Corpora.
In Proceed-ings of EMNLP 2008, pages 196?205.Jonathan Chevelu, Thomas Lavergne, Yves Lepage, and Thierry Moudenc.
2009.
Introduction of a New Para-phrase Generation Tool Based on Monte-Carlo Sampling.
In Proceedings of ACL 2009, pages 249?252.Jonathan Chevelu, Ghislain Putois, and Yves Lepage.
2010.
The True Score of Statistical Paraphrase Generation.In Proceedings of COLING 2010 (Posters), pages 144?152.2028Paul Clough, Robert Gaizauskas, Scott S. L. Piao, and Yorick Wilks.
2002.
METER: MEasuring TExt Reuse.
InProceedings of ACL 2002, pages 152?159.William B. Dolan and Chris Brockett.
2005.
Automatically Constructing a Corpus of Sentential Paraphrases.
InProceedings of the Third International Workshop on Paraphrasing 2005, pages 1?8.Christiane Fellbaum.
1998.
WordNet: An Electronic Lexical Database.
MIT Press.Juri Ganitkevitch, Chris Callison-Burch, Courtney Napoles, and Benjamin Van Durme.
2011.
Learning SententialParaphrases From Bilingual Parallel Corpora for Text-to-Text Generation.
In Proceedings of EMNLP 2011,pages 1168?1179.Juri Ganitkevitch, Benjamin Van Durme, and Chris Callison-Burch.
2013.
PPDB: The Paraphrase Database.
InProceedings of HLT 2013, pages 758?764.David Kauchak and Regina Barzilay.
2006.
Paraphrasing for Automatic Evaluation.
In Proceedings of HLT 2006.Bryan Klimt and Yiming Yang.
2004.
The Enron Corpus: A New Dataset for Email Classification Research.
InProceedings of ECML 2004, pages 217?226.Donald E. Knuth.
1986.
The TEXbook.
Addison-Wesley.David D. Lewis, Yiming Yang, Tony G. Rose, and Fan Li.
2004.
RCV1: A New Benchmark Collection for TextCategorization Research.
The Journal of Machine Learning Research, 5:361?397.G.
Harry McLaughlin.
1969.
SMOG Grading: A New Readability Formula.
Journal of Reading, 12(8):639?646.Donald Metzler and Eduard Hovy.
2011.
Mavuno: A Scalable and Effective Hadoop-Based Paraphrase Acquisi-tion System.
In Proceedings of the Third Workshop on Large Scale Data Mining 2011, pages 3:1?3:8.Donald Metzler, Eduard H. Hovy, and Chunliang Zhang.
2011.
An Empirical Evaluation of Data-Driven Para-phrase Generation Techniques.
In Proceedings of ACL 2011 (Short Papers), pages 546?551.Bonnie J. F. Meyer.
2003.
Text Coherence and Readability.
Topics in Language Disorders, 23(3):204?224.Bo Pang, Kevin Knight, and Daniel Marcu.
2003.
Syntax-Based Alignment of Multiple Translations: ExtractingParaphrases and Generating New Sentences.
In Proceedings of HLT 2003, pages 102?109.Marius Pasca and P?eter Dienes.
2005.
Aligning Needles in a Haystack: Paraphrase Acquisition Across the Web.In Proceedings of IJCNLP 2005, pages 119?130.Judea Pearl.
1984.
Heuristics.
Addison-Wesley.Chris Quirk, Chris Brockett, and William B. Dolan.
2004.
Monolingual Machine Translation for ParaphraseGeneration.
In Proceedings of EMNLP 2004, pages 142?149.Patrick Riehmann, Henning Gruendl, Martin Potthast, Martin Trenkmann, Benno Stein, and Bernd Froehlich.2012.
WORDGRAPH: Keyword-in-Context Visualization for NETSPEAK?s Wildcard Search.
IEEE Transac-tions on Visualization and Computer Graphics, 18(9):1411?1423.Helene Schmolz, David Coquil, and Mario D?oller.
2012.
In-Depth Analysis of Anaphora Resolution Require-ments.
In Proceedings of TIR 2012, pages 174?179.Edgar A. Smith and R. J. Senter.
1967.
Automated Readability Index.
Technical Report AMRL-TR-6620, Wright-Patterson Air Force Base.Benno Stein and Daniel Curatolo.
2006.
Phonetic Spelling and Heuristic Search.
In Proceedings of ECAI 2006,pages 829?830.Benno Stein and Sven Meyer zu Ei?en.
2008.
Retrieval Models for Genre Classification.
Scandinavian Journalof Information Systems (SJIS), 20(1):91?117.Benno Stein, Martin Potthast, and Martin Trenkmann.
2010.
Retrieving Customary Web Language to AssistWriters.
In Proceedings of ECIR 2010, pages 631?635.Nathan Sturtevant, Ariel Felner, Maxim Likhachev, and Wheeler Ruml.
2012.
Heuristic Search Comes of Age.
InProceedings of AAAI 2012.Shiqi Zhao, Cheng Niu, Ming Zhou, Ting Liu, and Sheng Li.
2008.
Combining Multiple Resources to ImproveSMT-Based Paraphrasing Model.
In Proceedings of ACL 2008, pages 1021?1029.Shiqi Zhao, Xiang Lan, Ting Liu, and Sheng Li.
2009.
Application-Driven Statistical Paraphrase Generation.
InProceedings of ACL 2009, pages 834?842.2029
