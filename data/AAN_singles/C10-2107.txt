Coling 2010: Poster Volume, pages 928?936,Beijing, August 2010Evaluating FrameNet-style semantic parsing:the role of coverage gaps in FrameNetAlexis Palmer and Caroline SporlederComputational LinguisticsSaarland University{apalmer, csporled}@coli.uni-saarland.deAbstractSupervised semantic role labeling (SRL)systems are generally claimed to have ac-curacies in the range of 80% and higher(Erk and Pado?, 2006).
These numbers,though, are the result of highly-restrictedevaluations, i.e., typically evaluating onhand-picked lemmas for which trainingdata is available.
In this paper we con-sider performance of such systems whenwe evaluate at the document level ratherthan on the lemma level.
While it is well-known that coverage gaps exist in the re-sources available for training supervisedSRL systems, what we have been lackinguntil now is an understanding of the pre-cise nature of this coverage problem andits impact on the performance of SRL sys-tems.
We present a typology of five differ-ent types of coverage gaps in FrameNet.We then analyze the impact of the cov-erage gaps on performance of a super-vised semantic role labeling system on fulltexts, showing an average oracle upperbound of 46.8%.1 IntroductionA lot of progress has been made in semanticrole labeling over the past years, but the per-formance of state-of-the-art systems is still rel-atively low, especially for deep, FrameNet-stylesemantic parsing.
Furthermore, many of the re-ported performance figures are somewhat unre-alistic because system performance is evaluatedon hand-selected lemmas, usually under the im-plicit assumptions that (i) all relevant word senses(frames) of each lemma are known, and (ii) thereis a suitable amount of training data for eachsense.
This approach to evaluation arises from thelimited coverage of the available hand-coded dataagainst which to evaluate.
More realistic evalua-tions test systems on full text, but these same cov-erage limitations mean that the assumptions madein more restricted evaluations do not necessarilyhold for full text.
This paper provides an analysisof the extent and nature of the coverage gaps inFrameNet.
A more precise understanding of thelimitations of existing resources with respect torobust semantic analysis of texts is an importantfoundational component both for improving ex-isting systems and for developing future systems,and it is in this spirit that we make our analysis.Full-text semantic analysisAutomated frame-semantic analysis aims to ex-tract from text the key event-denoting predicatesand the semantic argument structure for thosepredicates.
The semantic argument structure ofa predicate describing an event encodes relation-ships between the participants involved in theevent, e.g.
who did what to whom.
Knowledge ofsemantic argument structure is essential for lan-guage understanding and thus important for ap-plications such as information extraction (Mos-chitti et al, 2003; Surdeanu et al, 2003), ques-tion answering (Shen and Lapata, 2007), or recog-nizing textual entailment (Burchardt et al, 2009).Evaluating an existing system for its ability to aidsuch tasks is unrealistic if the evaluation is lemma-based rather than text-based.
Consequently, therecontinues to be significant interest in developingsemantic role labeling (SRL) systems able to au-tomatically compute the semantic argument struc-tures in an input text.Performance on the full text task, though, istypically much lower than for the more restrictedevaluations.
The SemEval 2007 Task on ?FrameSemantic Structure Extraction,?
for example, re-quired systems to identify key predicates in texts,928assign a semantic frame to the relevant predi-cates, identify the semantic arguments for thepredicates, and finally label those arguments withtheir semantic roles.
The systems participatingin this task only obtained F-Scores between 55%and 78% for frame assignment, despite the factthat the task organizers adopted a lenient evalu-ation scheme which gave partial credit for near-misses (Baker et al, 2007).
For the combined taskof frame assigment and role labeling the perfor-mance was even lower, ranging from 35% to 54%F-Score.Note that this distinction between evaluationschemes for SRL systems corresponds to the dis-tinction between ?lexical sample?
and ?all words?evaluations in word sense disambiguation, whereresults for the latter scheme are also typicallylower (McCarthy, 2009).The low performances are at least partly dueto coverage problems.
For example, Baker etal.
(2007) annotated three new texts for theirSemEval 2007 task.
Although these new textsoverlap in domain with existing FrameNet data,the task organizers had to create 40 new framesin order to complete annotation.
The new frameswere for word senses found in the test set butmissing from FrameNet.
The test set containedonly 272 frames (types), meaning that nearly 15%of the frames therein were not yet defined inFrameNet.
Obviously, coverage issues of this de-gree make full SRL a difficult task, but this is arealistic scenario that will be encountered in realapplications as well.As mentioned above, for many tasks it is neces-sary to compute the semantic argument structuresfor the whole text, or at least for multi-sentencepassages.
Due to non-local relations between ar-gument structures this is also true for tasks likequestion answering, where it might be possibleto automatically determine a subset of lemmaswhich are relevant for the task.
For example, in (1)it might be possible to determine that the secondsentence contains the answer to the question ?WasThomas Preston acquitted of theft??
However,to correctly answer this question, it is necessaryto resolve the null instantiation of the CHARGESrole of the VERDICT frame.
This null instantiationlinks back to the previous sentence, and resolvingit might require obtaining an analysis of the wordtried.
(1) [Captain Thomas Preston]Defendantiwas triedTry defendanti for[murder]Chargesi,j .In the end [he]Defendantj wasacquittedVerdictj [?
]Chargesj .Performance levels obtained for full text areusually not sufficient for this kind of real-worldtask.
FrameNet-style semantic role labeling hasbeen shown to, in principle, be beneficial for ap-plications that need to generalise over individuallemmas, such as recognizing textual entailment orquestion answering.
However, studies also foundthat state-of-the-art FrameNet-style SRL systemsperform too poorly to provide any substantial ben-efit to real applications (Burchardt et al, 2009;Shen and Lapata, 2007).Extending the value of automated semanticparsing for a variety of applications requires im-proving the ability of systems to process unre-stricted text.
Several methods have been pro-posed to address different aspects of the cover-age problem, ranging from automatic data expan-sion and semi-supervised semantic role labelling(Fu?rstenau and Lapata, 2009b; Fu?rstenau and La-pata, 2009a; Deschacht and Moens, 2009; Gordonand Swanson, 2007; Pado?
et al, 2008) to systemswhich can infer missing word senses (Pennac-chiotti et al, 2008b; Pennacchiotti et al, 2008a;Cao et al, 2008; Burchardt et al, 2005).
How-ever, so far there has not been a detailed analysisof the problem.
In this paper we provide that de-tailed analysis, by defining different types of cov-erage problems and performing analysis of bothcoverage and performance of an automated SRLsystem on three different data sets.Section 2 of the paper provides an introductionto FrameNet and introduces the basic terminol-ogy.
Section 4 describes our approach to coverageevaluation, Section 3 discusses the texts analyzed,and the analysis itself appears in Section 5.
Sec-tion 6 then looks at one possibility for addressingthe coverage problem.
The final section presentssome discussion and conclusions.929(a) (b)Figure 1: Terminology: (a) Frame with core frame elements (FEs) and frame-evoking elements (FEEs)(b) Target with possible frame assignments and resultant lexical units (LUs)2 FrameNetManual annotation of corpora with semantic ar-gument structure information has enabled the de-velopment of statistical and supervised machinelearning techniques for semantic role labeling(Toutanova et al, 2008; Moschitti et al, 2008;Gildea and Jurafsky, 2002).The two main resources are PropBank (Palmeret al, 2005) and FrameNet (Ruppenhofer et al,2006).
PropBank aims to provide a semantic roleannotation for every verb in the Penn TreeBank(Marcus et al, 1994) and assigns roles on a verb-by-verb basis, without making higher-level gener-alizations.
Whether two distinct usages of a givenverb are viewed as different senses or not is thusdriven by both syntax (namely, differences in syn-tactic argument structure) and semantics (via ba-sic, easily-discernable differences in meaning).FrameNet1 is a lexicographic project whoseaim it is to create a lexical resource documentingvalence structures for different word senses andtheir possible mappings to underlying semanticargument structure (Ruppenhofer et al, 2006).
Incontrast to PropBank, FrameNet is primarily se-mantically driven; word senses (frames)2 are de-fined mainly based on sometimes-subtle meaningdifferences and can thus generalise across individ-ual lemmas, and often also across different parts-of-speech.
Because FrameNet focusses on seman-tics it is not restricted to verbs but also provides1http://framenet.icsi.berkeley.edu/2We follow Erk (2005) in treating frame assignment as aword sense disambiguation task.
Thus in this paper we usethe terms frame and sense interchangeably.semantic argument annotations for nouns, adjec-tives, adverbs, prepositions and even multi-wordexpressions.
For example, the sentence in (2) andthe NP in (3) have identical argument structuresbecause the verb speak and the noun commentevoke the same frame STATEMENT.
(2) [The politician]Speaker spokeStatement[about recent developments on the labourmarket]Topic.
(3) [The politician?s]Speaker com-mentsStatement [on recent developmentson the labour market]TopicSince FrameNet annotations are semanti-cally driven they are considerably more time-consuming to create than PropBank annotations.However, FrameNet alo provides ?deeper?
andmore informative annotations than PropBankanalyses (Ellsworth et al, 2004).
For instance,the fact that (2) and (3) refer to the same state-of-affairs is not captured by PropBank sense dis-tinctions.FrameNet TerminologyThe English FrameNet data consist of an inven-tory of frames (i.e.
word senses), a set of lexi-cal entries, and a set of annotated examples ex-emplifying different syntactic realizations for se-lected frames (known as the lexicographic anno-tations).
Frames are conceptual structures thatdescribe types of situations or events togetherwith their participants.
Frame-evoking elements(FEEs) are predicate usages which evoke a par-ticular frame.
A given lemma can evoke different930frames in different contexts; each instance of thelemma is a separate target for semantic analysis.For example, (4) and (5) illustrate two differentframes of the lemma speak.
(4) [The politician]Speaker spokeStatement[about recent developments on the labourmarket]Topic.
(5) [She]Interlocutor1 doesn?t speakChattingto [anyone]Interlocutor2 .In this paper we follow standard use ofFrameNet terminology, with the possible excep-tion of the term lexical unit.
Figure 1 illus-trates our use of FrameNet-related terminology,focussing on (a) the CAUSE TO MAKE NOISEframe and (b) the target verb lemma ring.The definition of a frame determines the avail-able roles (frame elements or FEs) of the se-mantic argument structure for the particular useof the predicate, as well as the status?core orperipheral?of those roles.
For example, the FETOPIC is a core role under the STATEMENT frame,but a peripheral role under the CHATTING frame.The lexical entry of a lemma in FrameNet spec-ifies a list of frames which the lemma can evoke,and the pairing of a word with a particular frame iscalled a lexical unit (LU).
Ideally there should beannotated examples for each lexical unit, exem-plifying different syntactic constructions whichcan realize this LU.
However, as we will seelater (Section 5) annotated examples can be miss-ing.
Also, because FrameNet is a lexicographicproject, the examples were extracted to illustrateparticular usages, i.e., they are not meant to be sta-tistically representative.3 DataHaving introduced the basic FrameNet terminol-ogy, we now describe in more detail the datasets used in the analysis.
FrameNet Release 1.3(FN1.3), the latest release from the BerkeleyFrameNet project, includes both a corpus of lex-icographic annotations (FNL), which we referredto in Section 2, and a corpus of texts fully-annotated with frames and semantic role labels(FNF).
Annotations in the two corpora of coursecover different sets of predicates and frames, andFNL is the corpus commonly used as the basis fortraining supervised FrameNet-based SRL systems(Erk and Pado?, 2006).In our analysis, we look at three data sets: thelexicographic annotations from FN1.3, the fulltext annotations from FN1.3, and a new data setof running text that was annotated for the SemEval2010 Task-10 (see Table 1 for details).FrameNet Lexicographic (FNL) FrameNetstarted as a lexicographic project, aiming to drawup an inventory of frames and lexical units, sup-ported by corpus evidence, to document the rangeof syntactic and semantic usages of each lexicalunit.
The annotated example sentences in this partof FN1.3 are taken from the British National Cor-pus (BNC).
BNC is a balanced corpus, hence FNLcovers, in principle, a variety of domains.For each LU, a subset of the sentences in whichit occurs was selected for annotation, and in eachextracted sentence, only the target LU was anno-tated.
The sentences were not chosen randomlybut with a set of lexicographic constraints in mind.In particular the sentences should exemplify dif-ferent usage.
Thus ideally selected sentenceswould be easy to understand and not too long orcomplex.
As a consequence of this linguistically-driven selection procedure, the annotated sen-tences are not statistically representative in anyway.
FNL provides annotations for just under140,000 FEEs (tokens).
On average, around 20sentences are annotated for each LU.
FrameNet?sframe inventory contains 722 frames.3FrameNet Full Texts (FNF) Starting with re-lease 1.3, FrameNet alo provides annotations ofrunning texts.
In this annotation mode, all LUsin a sentence and all sentences in a text are an-notated.
FN1.3 contains two subsets of full textannotations.
The first of these (PB) contains fivetexts which were also annotated by the PropBankproject.
While all texts come from the Wall StreetJournal, they are not prototypical examples of thefinancial domain, rather they are longer essayscovering a wide variety of general interest topics3Only lexical frames are included in this number.
In addi-tion to those, FrameNet 1.3 defines another 74 frames whichcannot be lexicalised but are included because they provideuseful generalisations in the frame hierarchy.931FEEs FramesData Genre / Domain Tokens Types TypesFNL mixed 139,439 8370 722PB essays, general interest 1580 680 319NTI reports, foreign affairs 8271 1305 434SE fiction, crime 1530 680 320Table 1: Statistics for the three data sets(ranging from ?Bell Ringing?
to ?Earthquakes?
).The second subset (NTI) contains 12 texts fromthe Nuclear Threat Initiative website.4 These textsare intelligence reports which summarize and dis-cuss the status of various countries with regard tothe development of weapons and missile systems.Statistics for both data sets are given in Table 1.SemEval 2010 Task-10 Full Texts (SE) Whilethe FrameNet full texts allow us to estimate cover-age gaps that arise from limited training data, theydo not allow us to gauge coverage problems aris-ing from missing frames in the FN1.3 inventory.The reason for this is that the frame inventory re-flects the annotations of both the lexicographicand the full text part of FN1.3, i.e., every frameannotated in one of these subsets will also be partof the inventory.
To estimate the frame coverageproblem on completely new texts, we therefore in-cluded a third (full text) data set that was anno-tated for the SemEval 2010 Task 10 on ?LinkingEvents and Their Participants in Discourse?
(Rup-penhofer et al, 2009).5 The text is taken fromArthur Conan Doyle?s ?The Adventure of Wiste-ria Lodge?.
It thus comes from the fiction domain.The text was manually annotated with frame-semantic argument structure by two experiencedannotators.
Similar to the FNF texts, the annota-tors aimed to annotate all LUs in the text.
To doso, some new frames had to be created for pre-viously un-encountered LUs.
These new framesare not part of FN1.3 and we can thus use them toestimate coverage problems arising from missingframes.
Details for the data set can be found inTable 1.
This data set is very similar to the PB setin terms of size, FEE type-token ratio and numberof frames (types).4http://www.nti.org5The data set is available from http://semeval2.fbk.eu/semeval2.php?location=data.4 Types of Coverage GapsSemantic role labelling systems have to performtwo sub-tasks: (i) identifying the correct framefor a given lemma and context, and (ii) identifyingand labeling the frame elements.
The most severecoverage problems typically arise with the firstsubtask.
Furthermore, coverage problems relatedto frame identification have a knock-on effect onrole identification and labeling because the choiceof the correct frame determines which roles areavailable.
Therefore, we focus on the frame iden-tification task in this paper.Attempts to do automated frame assignmenton unrestricted text invariably encounter prob-lems associated with limited coverage of frame-evoking elements in FrameNet.
However, not ev-ery coverage gap is the same, and the precise na-ture of a coverage gap influences potential strate-gies for addressing it.
In this section we describethe different types of coverage gaps.
We pro-ceed from less problematic coverage gaps to moreproblematic ones, in the sense that the former canbe addressed more straighforwardly by automatedsystems than can the latter.4.1 NOTR gapsSome coverage gaps occur when lexical units(LUs) defined in FrameNet lack correspondingannotated examples; these gaps are the result oflacking training data, hence we call them NOTRgaps.
To give a sense of the abundance of suchgaps, of the 10,191 LUs defined in FN1.3, anno-tated examples are available for only 6727.NOTR-LU: lexical unit with no training data.In many cases, an LU ?
a specific pairing of atarget lemma with one frame ?
may be definedin FrameNet, thus potentially accessible to anautomated system, but lacking labeled trainingmaterial.
For example, FrameNet defines twoLUs for the noun ringer: with the frames CAUSETO MAKE NOISE and SIMILARITY.
It is clearthat the occurrence of ringer in (6) belongs tothe former LU, even given a very limited context.The lexicographic annotations, though, providetraining material only for the SIMILARITY frame.932(6) Then, at a signal, the ringers begin vary-ing the order in which the bells soundwithout altering the steady rhythm of thestriking.NOTR-LU gaps pose particular problems to afully-supervised SRL system, because such a sys-tem cannot learn anything about the context inwhich the CAUSE TO MAKE NOISE frame is moreappropriate.
A NOTR-LU gap is identified foran LU even if training data is available for othersenses (i.e.
other LUs) of the target lemma.NOTR-TGT: target with no training data.
Inother cases, a target lemma may be defined as par-ticipating in one or more LUs, but with no trainingdata available for any of them.
In other words, asupervised automated system trained only on theavailable annotated examples will fail to learn anypotential frame assignments for the target lemma.Such is the case for art, which in FrameNet isassigned the single frame CRAFT, but for whichFNL contains no training data.
(7) The art of change-ringing is peculiar tothe English, and, like most English pe-culiarities, unintelligible to the rest of theworld.Whereas a NOTR-LU gap obscures a particularframe assignment for a target lemma, a NOTR-TGT gap indicates a complete absence in the lexi-cographic corpus of annotated data for the lemma.4.2 UNDEF gapsThe previous coverage problems arise from a lackof annotated data, an issue which conceivablycould be addressed through further annotation.More serious problems arise when a text containsword senses, words, or frames not contained inFrameNet.
We call such elements ?undefined?
;specifically, they receive no treatment in FN1.3.UNDEF-LU: lexical unit not defined.
Cover-age gaps of this sort occur when the frame inven-tory for a given lemma is not complete.
In otherwords, at least one LU for the lemma exists inFrameNet, but one or more other LUs are miss-ing.
For example, the noun installation occursin FrameNet with the frames LOCALE BY USEand INSTALLING.
The sense of an art installation,which is an instance of the frame PHYSICAL ART-WORKS, is missing.UNDEF-TGT: target not addressed.
In theworst case, all LUs for a target lemma might bemissing, i.e., the lemma does not occur in theFrameNet lexicon at all.
The noun fabric is an ex-ample.
Though it has at least two distinct senses?that of cloth or material and that of a framework(e.g.
the fabric of society)?FrameNet providesno help for determining appropriate frames for in-stances of this lemma.UNDEF-FR: frame not defined.
Finally, itmay be not only that the LU is missing, but thatthere is no definition in FrameNet for the cor-rect frame given the context.
For example, inthe sports domain the lemma ringer can have thesense of (a horseshoe thrown so that it encirclesthe peg); to our knowledge, this sense is not avail-able in FrameNet.5 Coverage gaps and automatedprocessingWith the exception of work on extending cov-erage, most FrameNet-style semantic role label-ing studies draw both training and evaluation datafrom FNL.
This is an unrealistic evaluation sce-nario for full-text semantic analysis, as such eval-uation limits the domain for which prediction canoccur to those lexical entries treated in FNL.
Forsystems which do not attempt any generalizationbeyond those lexical entries with training data,this limits the system to 5864 lemmas for which itcan make predictions regarding frame assignmentand role labeling.Disregarding whether annotations have yetbeen provided for the lexical units in FNL stilllimits us to 8370 frame-evoking elements (tar-gets).
To better understand the potential of cur-rent frame-semantic resources for semantic anal-ysis of unrestricted text, we evaluate coverage ofthe FNL annotations against the texts in FNF, aswell as against the SemEval text.
We then analyzethe performance of an off-the-shelf, supervisedSRL system, Shalmaneser (Erk and Pado?, 2006),on the same texts, with a focus on the types of933Dataset TR-LU NOTR-LU NOTR-TGT UNDEF-LU UNDEF-FRPB 42.66 9.56 47.78 ?
?NTI 46.77 7.77 45.46 ?
?SE 51.64 6.86 26.01 3.40 12.09Table 2: FrameNet coverage for analyzed textserrors made and the upper bound on performancefor this system.5.1 FrameNet coverageAs described in Section 4, in many cases a lex-ical unit, a frame-evoking element, or a framemay simply not be represented in FrameNet.
Inother cases, the entity may be in FN1.3 but lack-ing training data.
Of the 722 frames defined inFN1.3, for example, annotations exist for 502.For the three data sets analyzed, Table 2 showsthe degree of coverage provided by FNL for thegold-standard frame annotations.
First, the TR-LU column shows the non-problematic cases, forwhich the correct frame annotation is availablein FrameNet, with training data.
The next twocolumns represent training gaps related to lackof training data: NOTR-LU are cases for whichtraining data exists for the target, but not for thecorrect sense of the target, and NOTR-TGT in-stances are those for which no training data at allexists for the target.Because all targets annotated in the FNF texts(i.e.
PB and NTI above) are incorporated inFN1.3, gaps due to missing LUs, targets, orframes do not exist for those texts.
The samedoes not hold for the SemEval (SE) text.
For3.4% of the annotated SemEval targets, an LU isentirely missing from the lemma?s frame inven-tory in FrameNet, and in just over 12% of casesboth the lemma and the frame are missing.
In to-tal, more than 15% of LUs appearing in the gold-standard SemEval annotations are not defined atall within FrameNet.
This figure accords with thatfound by Baker et al (2007).5.2 Error analysis of full-text frameassignmentHere we examine the errors made by Shalmaneserfor frame assignment on the three data sets.
Theupper bound on apparent performance is fixed byDataset Correct Type(i) Type(ii) Type(iii)PB 36.71 5.95 9.56 47.78NTI 41.22 5.55 7.77 45.46SE 46.67 4.97 6.86 41.50Table 3: Shalmaneser performance on textsthe number of targets for which Shalmaneser hasseen training data, namely the sum of TR-LU andNOTR-LU in Table 2.6We consider three categories of errors: (i) nor-mal or true errors are misclassifications when thecorrect label has been seen in the training data.
Inthis category we also count errors resulting fromincorrect lemmatization.
(ii) label-not-seen errorsare misclassifications when the correct label doesnot appear in the training data and thus is unavail-able to the classifier.
Finally, (iii) no-chance er-rors occur when the system has no informationfor either a given target or a given frame.
Ta-ble 3 shows the prevalence of each error type foreach data set, given as the percentage of all frame-assignment targets.It can be seen that the frame assignment accu-racy is relatively low for all three texts (between37% and 47%).
However, only a relatively smallproportion of the misclassifications are due to trueerrors made by the system.
Furthermore, a largeamount of errors (41% to 48%, with an averageof 46.8%) is due to cases where important infor-mation is missing from FrameNet (Type (iii) er-rors).
Consequently, improving the semantic rolelabeller by optimising the feature space or the ma-chine learning framework is going to have verylittle effect.
A much more promising path wouldbe to investigate methods which might enable theSRL system to deal gracefully with unseen data.One possible strategy is discussed in the next sec-tion.6By ?apparent performance?
we mean the system?s ownevaluation of its accuracy on frame assignment.9346 Frame and lemma overlapOne potential strategy for improving full-text se-mantic analysis without performing additional an-notation is to take advantage of semantic overlapas it is represented in FrameNet.
We can lookat two different types of overlap in FrameNet:lemma overlap and frame overlap.6.1 Lemma overlapThe approach of treating frame assignment as aword sense disambiguation task (as, e.g., by Shal-maneser) relies on the overlap of LUs with thesame lemma and trains lemma-based classifierson all training instances for all LUs involving thatlemma.
One way to consider using labeled mate-rial in FrameNet to improve performance on tar-gets for which we have no labeled material is togeneralize over lemmas associated with the sameframe.
The idea is to use training instances fromrelated lemmas to build a larger training set forlemmas with little or no annotated data.Of the 8370 lemmas in FN, 8358 share a singleframe with at least one other lemma.
890 overlapon two frames with at least one other lemma, and111 have 3-frame overlap with at least one otherlemma.
Only 16 lemmas show an overlap of fouror more frames.
These groupings are:1. clang.v, clatter.v, click.v, thump.v2.
hit.v, smack.v, swing.v, turn.v3.
drop.v, rise.v4.
remember.v, forget.v5.
examine.v, examination.n6.
withdraw.v, withdrawal.nThe first two groupings are sets of words thatare closely semantically related, the second twoare opposite pairs, and the third two are verb-nominalization pairs.The lemma overlap groups differ with respectto how much training data they make accessible.6.2 Frame overlapAnother possibility to be considered is general-ization over all instances of a given frame.
Forthe 502 frames with annotated examples, the num-ber of annotated instances ranges from one (SAFESITUATION, BOARD VEHICLE, and ACTIVITYSTART to 6233 (SELF MOTION), with an averageof 278 training instances per frame.In future work we will examine the effective-ness of binary frame-based classifiers, abstract-ing away from individual predicates to predictwhether a given lemma belongs to the frame inquestion (for a related study see Johansson andNugues (2007)).
A potential drawback to this ap-proach is the loss of predicate-specific informa-tion.
We know, for example, about verbs that theytend to have typical argument structures and typi-cal syntactic realizations of those argument struc-tures.In addition to this frame-overlap approach, wewill consider the impact on coverage of usingcoarser-grained versions of FrameNet in whichframes have been merged according to frame rela-tions defined over the FrameNet hierarchy, usingthe FrameNet Transformer tool described in (Rup-penhofer et al, 2010).7 ConclusionsAlthough it is clear that the capability to do shal-low semantic analysis on unrestricted text, and oncomplete documents or text passages, would helpperformance on a number of key tasks, currently-available resources seriously limit our potentialfor achieving this with supervised systems.
Theanalysis in this paper aims for a better understand-ing of the precise nature of these limitations inorder to address them more deliberately and witha principled understanding of the coverage prob-lems faced by current systems.To this end, we outline a typology of coveragegaps and analyze both coverage of FrameNet andperformance of a supervised semantic role label-ing system on three different full-text data sets, to-taling over 150,000 frame-assignment targets.
Wefind that, on average, 46.8% of targets are not cov-ered under straight supervised-classification ap-proaches to frame assignment.AcknowledgmentsThis research has been funded by the German Re-search Foundation DFG under the MMCI Clusterof Excellence.
Thanks to the anonymous review-ers, Josef Ruppenhofer, Ines Rehbein, and HagenFu?rstenau for interesting and helpful commentsand discussions, and to Collin Baker for assistancewith data.935ReferencesC.
Baker, M. Ellsworth, K. Erk.
2007.
Semeval-2007task 19: Frame semantic structure extraction.
InProceedings of SemEval-2007.A.
Burchardt, K. Erk, A. Frank.
2005.
A WordNetDetour to FrameNet.
In Proceedings of the GLDV-05 Workshop GermaNet II.A.
Burchardt, M. Pennacchiotti, S. Thater, M. Pinkal.2009.
Assessing the impact of frame semantics ontextual entailment.
Journal of Natural LanguageEngineering, Special Issue on Textual Entailment,15(4):527?550.D.
D. Cao, D. Croce, M. Pennacchiotti, R. Basili.2008.
Combining word sense and usage for mod-eling frame semantics.
In Proceedings of STEP-08.K.
Deschacht, M.-F. Moens.
2009.
Semi-supervisedSemantic Role Labeling Using the Latent WordsLanguage Model.
In Proceedings of EMNLP-09.M.
Ellsworth, K. Erk, P. Kingsbury, S. Pado?.
2004.PropBank, SALSA, and FrameNet: How DesignDetermines Product.
In Proceedings LREC 2004Workshop on Building Lexical Resources from Se-mantically Annotated Corpora.K.
Erk, S. Pado?.
2006.
Shalmaneser ?
a toolchain forshallow semantic parsing.
In Proceedings of LREC-06.K.
Erk.
2005.
Frame assignment as word sense disam-biguation.
In Proceedings of IWCS 6.H.
Fu?rstenau, M. Lapata.
2009a.
Graph alignment forsemi-supervised semantic role labeling.
In Proceed-ings of EMNLP 2009.H.
Fu?rstenau, M. Lapata.
2009b.
Semi-supervised se-mantic role labeling.
In Proceedings of EACL 2009.D.
Gildea, D. Jurafsky.
2002.
Automatic label-ing of semantic roles.
Computational Linguistics,28(3):245?288.A.
Gordon, R. Swanson.
2007.
Generalizing semanticrole annotations across syntactically similar verbs.In Proceedings of ACL 2007.R.
Johansson, P. Nugues.
2007.
Using WordNet toextend FrameNet coverage.
In Proceedings of theWorkshop on Building Frame-semantic Resourcesfor Scandinavian and Baltic Languages, NODAL-IDA.M.
Marcus, G. Kim, M. A. Marcinkiewicz, R. MacIn-tyre, A. Bies, M. Ferguson, K. Katz, B. Schasberger.1994.
The Penn Treebank: Annotating predicate ar-gument structure.
In ARPA Human Language Tech-nology Workshop.D.
McCarthy.
2009.
Word Sense Disambiguation:An Overview.
Language and Linguistics Compass,3(2):537?558.A.
Moschitti, P. Morarescu, S. Harabagiu.
2003.Open-domain information extraction via automaticsemantic labeling.
In Proceedings of FLAIRS.A.
Moschitti, D. Pighin, R. Basili.
2008.
Tree Kernelsfor Semantic Role Labeling.
Computational Lin-guistics, 34(2).S.
Pado?, M. Pennacchiotti, C. Sporleder.
2008.
Se-mantic role assignment for event nominalisations byleveraging verbal data.
In Proceedings of Coling2008.M.
Palmer, D. Gildea, P. Kingsbury.
2005.
The Propo-sition Bank: An Annotated Corpus of SemanticRoles.
Computational Linguistics, 31(1):71?105.M.
Pennacchiotti, D. D. Cao, R. Basili, D. Croce,M.
Roth.
2008a.
Automatic induction of FrameNetlexical units.
In Proceedings of EMNLP-08.M.
Pennacchiotti, D. D. Cao, P. Marocco, R. Basili.2008b.
Towards a Vector Space Model forFrameNet-like Resources.
In Proceedings of LREC-08.J.
Ruppenhofer, M. Ellsworth, M. R. L. Petruck, C. R.Johnson, J. Scheffczyk.
2006.
FrameNet II: Ex-tended Theory and Practice.J.
Ruppenhofer, C. Sporleder, R. Morante, C. Baker,M.
Palmer.
2009.
SemEval-2010 Task 10: Link-ing Events and Their Participants in Discourse.
InProceedings of SEW-2009.J.
Ruppenhofer, M. Pinkal, J. Sunde.
2010.
Generat-ing FrameNets of various granularities.
In Proceed-ings of LREC 2010.D.
Shen, M. Lapata.
2007.
Using semantic rolesto improve question answering.
In Proceedings ofEMNLP-2007.M.
Surdeanu, S. Harabagiu, J. Williams, P. Aarseth.2003.
Using predicate-argument structures for in-formation extraction.
In Proceedings of ACL 2003.K.
Toutanova, A. Haghighi, C. D. Manning.
2008.A Global Joint Model for Semantic Role Labeling.Computational Linguistics, 34(2).936
