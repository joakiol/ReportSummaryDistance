Efficient parsing strategies for syntactic analysis of closed captionsKrzysz to f  Czubakczuba@cs, cmu.
eduLanguage Technologies Inst i tuteCarnegie Mellon UniversityP i t tsburgh,  PA 15213, USAAbst ractWe present an efficient multi-level chart parser thatwas designed for syntactic analysis of closed captions(subtitles) in a real-time Machine Translation (MT)system.
In order to achieve high parsing speed, wedivided an existing English grammar into multiplelevels.
The parser proceeds in stages.
At each stage,rules corresponding to only one level are used.
Aconstituent pruning step is added between levels toinsure that constituents not likely to be part of thefinal parse are removed.
This results in a significantparse time and ambiguity reduction.
Since the do-main is unrestricted, out-of-coverage sentences are tobe expected and the parser might not produce a sin-gle analysis panning the whole input.
Despite theincomplete parsing strategy and the radical prun-ing, the initial evaluation results how that the lossof parsing accuracy is acceptable.
The parsing timefavorable compares with a Tomita parser and a chartparser parsing time when run on the same grammarand lexicon.1 In t roduct ionIn this paper we present on-going research on pars-ing closed captions (subtitles) from a news broad-cast.
The research as been conducted as part of aneffort to build a prototype of a real-time MachineTranslation (MT) system translating news captionsfrom English into Cantonese (Nyberg and Mita-mura, 1997).
We describe an efficient multi-levelchart parser that was designed to handle the kindof language used in our domain within a time thatallows for a real-time automatic translation.
In or-der to achieve high parsing speed, we divided anexisting English grammar into multiple levels.
Theparser proceeds in stages.
At each stage, rules cor-responding to only one level are used.
A constituentpruning step is added between levels to insure thatconstituents not likely to be part of the final parseare removed.
This results in a significant parse timeand ambiguity reduction.
Since the domain is un-restricted, out-of-coverage s ntences are to be ex-pected and the parser might not produce a singleanalysis panning the whole input.
Thus, the set offinal constituents has to be extracted from the chart.Despite the incomplete parsing strategy and the rad-ical pruning, the initial evaluation results how thatthe loss of parsing accuracy is acceptable.
The pars-ing time favorable compares with a Tomita parserand a chart parser parsing time when run on thesame grammar and lexicon.The outline of the paper is as follows.
In Section 2we describe the syntactic and semantic haracteris-tics of the input domain.
Section 3 provides a shortsummary of previous published research.
Section4 gives an overview of requirements on the parsingalgorithm posed by our application.
Section 5 de-scribes how the grammar was partitioned into lev-els.
Section 6 describes the constituent pruning al-gorithm that we used.
In Section 7 we present hemethod for extracting final constituents from thechart.
Section 8 presents the results of an initialevaluation.
Finally, we close with future research inSection 9.2 Capt ion ing  domainTranslation of closed captions has been attemptedbefore.
(Popowich et al, 1997) describe a sys-tem that translates closed captions taken fromNorth American prime tlme TV.
In their approach,(Popowich et al, 1997) assume a shallow parsingmethod that proves effective in achieving broad sys-tem coverage required for translation of dosed cap-tions from, e.g., movies.
As reported by (Popowichet al, 1997), the shallow analysis performed by thesystem combined with the transfer-based translationstrategy results in a number of sentences that are un-derstandable only in conjunction with the broadcastimages.
The number of sentencesthat are translatedincorrectly is also significant.The parsing scheme described below was used ina pilot Machine Translation system for translationof news captions.
The following requirements wereposed: a) the translations should not be mislead-ing, b) they can be telegraphic since the input isoften in a telegraphic style, c) partial translationsare acceptable, d) if no correct translation can beproduced then it is preferable to not output any.7The closed captions were taken from a financial newssegment.
Although the language in this segment isnot strongly domain-restricted, it is centered aroundthe financial aspects of the described events, whichmakes certain tasks such as sense disambiguationfeasible.In order to address the translation quality prob-lems found by (Popowich et al, 1997), (Nyberg andMitamura, 1997) propose a multi-engine MT sys-tem architechture to handle the task of translatingclosed captions.
The parser described in this paper,was developed for the knowledge-based module (Ny-berg and Mitamura, 1992) in the system and it wasrequired to produce "deep" analyses with the levelof detail sufficient for creating interlingua.The stream of closed captions we worked withshows many interesting characteristics that influ-ence the design of the parsing algorithm and thewhole MT system.
In the paper, we consider onlythe issues that are related to the syntactic analysisof closed captions.
The stream contaln.q long sen-tences taken from interviews, short phrases makingthe program more lively, and telegraphic style sen-tences used to report the latest stock market news.It has unrestricted syntax in long descriptive sen-tences, which resembles written language, with somephenomena like omission of function words that arecharacteristic ofspoken language.
It is likely to con-tain words not present in the lexicon, such as com-pany names.
Although not considered here directly,a caption stream usually also contains ome typosand corrections made by the captioner.
It is how-ever different from, e.g., a speech recognizer outputsince the human captioner usually provides the cor-rect transcription and there is no ~mraediate n ed tomodel recognition errors.3 Partial~ robust and fast parsingThe kind of input described above requires robustparsing methods.
Since our goal was real-time trans-lation, the parsing module had to be very efficient.We concentrated on reducing the amount of workdone by the parser at the potential cost of loweringthe quality of the resulting analysis 1.
Similar meth-ods have been adopted elsewhere, although in mostcases the goal was a shallow parse.
(Abney, 1996) de-scribes a chunking approach based on cascades of fi-nite transducers in which the parser finds "islands ofcertainty" at multiple analysis levels.
Only maximalconstituents (in terms of length) found by transduc-ers at lower levels are the input to the transducersat higher levels, which results in constituent pruningand ambiguity containment, and low parsing times.
(Ciravegna nd Lavelli, 1997) introduce additionalIDealing with typos, out-of-coverage lexical item, etc, wasnot considered here, although the parser offers some robust-hess in skipping unknown words, see Section 7.controlling strategies to a chart parser.
(Ciravegnaand Lavelli, 1997) experiment with chunks and adda preprocessing step that uses a finite-state machineto identify potential chunks in the input.
The chartparser control strategy is augmented so that con-stituents found during parsing that align with chunkboundaries are processed with a higher priority inthe hope that they are more likely to become partof the final parse.
Constituents that do not alignwith chunk boundaries are assigned lower prioritiesand remain in the agenda.
The resulting algorithmcan avoid pruning of useful constituents that canhappen in Abney's cascading approach.Explicit pruning of constituents i another strat-egy that can be used to improve the efficiency ofa parser as shown, e.g., by (Rayner and Carter,1996).
(Rayner and Carter, 1996) experimentedwith a bottom-up parser with three levels: lexi-cal level, phrasal evel and full parsing level.
Con-stituents are pruned before the phrasal and full pars-ing levels using statistical scores based on how likelyis a constituent with a certain property to be partof the final solution.4 Requ i rements  on the algorithmGiven the characteristics of the input and previouslypublished results described above, we decided to de-sign a parsing strategy with the following character-istics:1. bottom-up: this allows for partial parses to beidentified;2. multi-level: combined with pruning providesadditional control over created constituents;3. pruning: constituents not likely to contribute tothe final parse should be removed early.In what follows we will describe some initial re-sults of experiments in applying a multi-level chartparser with a radical priming strategy to the cap-tioning domain.5 Introducing levels to the grCmmarThe parsing proceeds in multiple levels.
The gram-mar is split into groups of rules corresponding to thedifferent levels.
At a given level, only rules assignedto this level are allowed to fire.
They use as inputall the constituents the parser created so far (onlyinactive edges, all active edges are currently prunedbetween levels).
In a sense, this corresponds to pars-ing with multiple grammars, although the grammarpartitioning can be done once the grammar is com-plete and fine tuning is performed to achieve theright balance between ambiguity, output fragmen-tation and parsing speed.
This approach makes italso possible to test the improvement introduced bythe special processing scheme in comparison to some8other parsing scheme since the grammar is kept in ageneral form.The grammar that we are currently using hasbeen adapted from a general English grammar writ-ten for the Tomita parser.
The grammar doesnot follow any particular theoretical framework, al-though it has been strongly influenced by bothLFG (Bresnan, 1982) and HPSG (Polard and Sag,1994).
It consists currently of about 300 gen-eral rules with some general attachment prefer-ence constraints that have been fine tuned onvarious kinds of text, including news broadcastsand written texts.
One important characteris-tic of the grammar is the use of subcategor'Lza-tion information (at the syntactic level, with val-ues such as subject+object, subject+complement,subj ect+obj ect+oblique ("of''), etc.
), and somebroad semantic classification of adjuncts to helpprevent ambiguity.
The lexicalist character of thegrammar requires that subcategorization and sim-ple semantic information (temporal expressions, lo-cation expressions, etc) be present in the lexicon.The grammar coverage (Czuba et al, 1998) is quitebroad, but it is not sufficient, e.g., to cover sometypes of clause attachment that are present in longsentences found in the input.The changes that were introduced to the gram-mar, in comparison to the original version withoutlevels, concentrated on a simple addition of levelsto the rules and duplicating some rules at multiplelevels.
In the current grammar, the following levelshave been introduced:I. lexical level: lexicon lookup, idioms and fixedphrases;2. nominal phrases without adjuncts;3. verb phrases with obligatory complements;4. noun and verb phrases with adjuncts;5. clausal constituents; noun and verb phraseswith obligatory clausal complements;6. top level with preferences for attachment andwell-formedness of constituents (e.g., prefer fi-nite clauses, prefer ungapped constituents, etc).Although some motivation for the above partition-ing can be found on X-bar theory, we mostly usedour intuition for choosing the number of levels anddeciding how to assign rules to different levels.
Theseare parameters of the algorithm and they can betuned in further experiments.
See the next sectionfor examples of rules that were added to multiplelevels.6 Const i tuent  p run ingWe will refer to constituents that were created usinggrammar ules with the nonterminal <X> on theirLHS as <X> constituents.
Two constituents will besimilar if they were created using rules with the samenonterminal on the LHS.
E.g., two <NP> constituentsare similar, regardless oftheir positions and spannedinput.The pruning phase was added to the usual chartparsing algorithm in a way that makes it invisibleto the grammar writer.
The pruning algorithm iscalled on the chart whenever no more rules are al-lowed to fire at the current level and no active arcscan be extended.
In the initial implementation, thepruning algorithm was based on a simple subsump-tion relation: only the maximal(i.e., covering thelongest number of input tokens) constituents froma set of similar constituents remain in the chart andare added to the agenda for the next level.
E.g., ifthe chart contained two <NP> constituents, only theone spanning more input would be retained.Although the original pruning strategy resultedin many reasonable parses, we noticed a few generalproblems.
The parser is very sensitive to wronglycreated long constituents.
This means that thegrammar has to be relatively tight for a successfulapplication with the described parser since no globaloptimization is performed.
However, this also meansthat if in a particular context the parser builds a con-stituent C that is not correct in this context but therules that were used to build the constituent cannotbe removed from the grammar since they are usefulin general, the pruning step will wrongly remove all,potentially correct, similar constituents subsumedbyC.We observed this kind of behavior in practice andat least two cases can be distinguished.
Some con-stituents are added to the chart early in the analysisand they form bigger constituents as the analysisprogresses.
Consequently, if a similar constituent iscreated, the original constituent will be pruned andwill not be available at higher levels.
This behav-ior can be observed, e.g., for the string that help, inwhich that is supposed to be a pronoun and help issupposed to have a verbal reading.
Since that helpis a well-formed <NP> constituent according to ourgrammar, it will be created and it will subsume thepronoun that.
This means that the pronoun that willbe pruned and it will not be available at a higherlevel that tries to create a correct parse incorporat-ing that as an object of a preceding verb and helpas the main verb.
In order to solve this problem weallowed some rules to be repeated at multiple lev-els.
The rules introducing pronouns were added attwo levels.
The rules involving verbal complementswere also introduced twice.
Since verbal phrases arecreated relatively late in the analysis, verbal comple-ments on, e.g., noun phrases are not available yet.Because of that, e.g., the rule that attaches the di-rect object o a verb is present twice in the grammar:9one version of it takes care of nominal objects with-out complements, the other one is specific for ob-jects with a verbal complements.
Since the secondversion of the rule contai~q a check for the presenceof a verbal complement, no work is repeated.The second case that we noticed involved largenoun phrases created as the result of applying therules for nominal apposition (e.g., the U.S. president,Bill Clinton) and coordination.
Since the parserdoes not use any semantic information, it is difficultto prevent such rules from applying in some wrongcontexts.
Examples include noun phrases at clauseboundaries as in the following sentence: BTM says itwill issue new shares to strengthen its capital base,BTM plans to raise 300 billion yen via the issue.In this case, an apposition its capital base, BTM iscreated and the phrases its capital base and BTMare pruned, preventing the parser from finding thecorrect analysis consisting of two finite clauses.In order to solve that problem, we introduced anoption of relaxing the pruning constraint.
Currently,such relaxing is allowed only for phrases containingappositions and coordination.
All constituents hatare subsumed by similar ones containing appositionor coordination are marked and they can never bepruned.
As a result, both its capital base and BTMremain in the chart and can be used to create therequired clauses at higher levels.The pruning algorithm that we implemented canbe potentially quite costly since it involves manycomparisons between constituents.
Although itsworst-case cost is quadratic, in practice the equiv-alence classes of similar constituents are small andthey are pruned quickly.
As a result, in our experi-ments the pr~mlng time was below 0.01% of the totalparse time.In addition to the actual removal of constituent,the function implementing the pruning algorithmperformed local ambiguity packing: it removes con-stituents that have the same feature structures as aconstituent already present in the chart and it cre-ates constituents with disjunction of feature struc-tures in case similar constituents spanning the samechunk of input but having different feature struc-tures are found.7 Ext rac t ion  f rom the  char tAfter the parser finished creating constituents atthehighest level, the final result has to be extracted fromthe chart.
Since the parser might not be able to pro-duce a single analysis panning the whole input, thebest sequence of constituents needs to be extracted.Currently, asimple best-first beam search throughthe chart is used to find a sequence (path) of con-stituents panning the whole input.
Paths are al-lowed to have gaps, i.e., they do not have to be con-tiguous, although we do not allow for overlappingconstituents.
The algorithm prefers horter paths.The length of a path is computed as a weighted sumof the lengths of constituents in the path.
We experi-mented with two different ways of assigning weightsand lengths to constituents.
In the first method,each constituent was assigned the length of 1 thatwas weighted by a factor depending of the "quality"of the constituent.
Paths can be extended by a gapspanning one input token at a time.
Such a gap isweighted with the factor of 3.
Constituents hat arecreated by rules with the nonterminal <OUTPUT> ontheir LHS are assumed to be of the highest qualityand they are weighted with the factor of 1.
All re-maining constituents can also be added to the pathand are weighted with the factor of 1.5.
So a pathconsisting of an <OUTPUT> constituent spanning in-put tokens 1 to 3, a gap spanning input tokens 4 and5, and a <Vl> constituent spanning input tokens 6to 10 would receive the length of 1 + 6 + 1.5 = 8.5.This algorithm shows a strong preference for con-tiguons paths and assigns lengths depending on thenumber of constituents in the path, ignoring theirlength.The second weighting scheme we tried was basedon the actual ength of constituents.
<OUTPUT> con-stituents were assigned their actual length multipliedby 1.
Other constituents had their actual lengthmultiplied by 1.5, and gaps were weighted with thefactor of 2.
The path described in the previous para-graph was thus assigned the length of 3 + 4 + 7.5= 14.5.Although the first weighting scheme seems rathercrude, it resulted in a very good performance bothin terms of the quality of paths found and the timerequired to find the best path.
The best-first searchwas implemented using a binary heap priority queuein an array, and the extraction time for the firstweighting scheme was below 5% of the total time re-quired for both parsing and extraction.
We also didnot notice any cases in which the returned path wasnot the optimal one given the constituents found bythe parser.
The second weighting scheme is morefine-grained and might turn out to be better on abigger corpus.
However, it required about 15 asmuch time to complete the search as the first scheme.Finally, in case of ambiguity, the first featurestructure returned by the parser was chosen.8 P re l iminary  resu l t sThe algorithm has been applied to a sample of 42sentences from a real TV broadcast.
The sentenceswere picked from a contiguous transcript that wascleaned up for captioner mistakes.
Since the parserwas designed for use in a real-time multi-engine MTsystem, we concentrated on sentences which werelikely to be translated by the knowledge-based partof the system.
Sentences that were likely to be10Sentence I, 17 tokens:\[\[We\]l\] \[\[Japan\] \[is \[unlikely to adopt \[any more stimulus spending measures\]\] soon \[despite \[that U.S. pressure.\]\]\]\]\]algorithmchartTomitamulti-level#arcs #constituents20410 6680891 191tim e (s)76.628.01.0Sentence  2, 49 tokens:\[\[Reform legislation\] \[is quite good\]\] \[because \[it \[\[puts up\] \[public money\]\] \[which\] \[\[financial institutions\] \[can get\]\]\[to protect \[depositors\]\] \[but only if\], \[\[\[they\] \[\[\[recycle\] \[or if you will,\] \[write off\]\] \[their bad loans\]\]\] \[and\] \[\[clean up\]\[their balance sheets\]\]\] [ [so\] \[they\] \[can start \[to loan \[money\]\] again.l\]algorithm #arcschart 13530Tomitamulti-level 3101#constituents time (s)4849 43.7- 22.0388 3.0Figure 1: Sample sentences with bracketing found the parsertranslated by a translation memory look-up, such asgreetings and short canned phrases, were not addedto the test corpus.
The resulting corpus consisted ofrelatively long sentences, with the average length of23.5 tokens (including punctuation).The parser was compared to a Tomita parser anda chart parser with local ambiguity packing.
All theparsers were implemented in lisp and used the sameunification package, which made the parsing resultseasily comparable.
They also used the same gram-mar and lexicon with about 8000 entries.
The gram-mar was preprocessed for the Tomita parser and thechart parser by removing the rules that were presentat multiple levels.
The chart parser was set up toquit after finding the first parse covering the wholeinput.
All times given below were obtained on thesame machine (SPARCStation 5).
Care was takento disable lisp garbage collection during parsing.As it was expected, the pruning strategy resultedin a significant reduction of execution time.
In Fig-ure 1 we present a few measurements to illustratethe time improvement for two example sentences.For the first one, all parsers produced a completeanalysis spanning the whole input.
For the secondone, due to the lack of grammar coverage, no singleanalysis can be found.
Figure 1 shows the bracketingthat the multi-level parser found.
It also producedcorrect feature structures for all the chunks that canbe used by an MT system.As can be seen from Table 1, the time improve-ment is significant.
The improvement in the numberof elements in the final chart is crucial for good per-formance of the extraction algorithm that choosesconstituents to be output by the parser in casethere is no single analysis spanning the whole in-put.
Also, the multilevel parser did not produce am-biguous feature-structures (ambiguity containment).The Tomita parser produced two packed f-structuresfor the first sentence that would have to be unpackedand disambiguated for further processing.
For thesecond sentence, the Tomita parser did not find anyfull parse.
The multilevel parser produced chunksthat are usable in an MT system and can be trans-lated giving at least partial translation to the user.The results on the whole test set were as follows.The Tomita parser needed 652 seconds to analyzeall the sentences.
It produced a complete analysisfor 31 sentences, returning no analysis for 11.
Thechart parser run till the first solution spanning thewhole input was found needed 937 seconds to ana-lyze the same 31 sentences, failing on the rest.
Inthe case of the Tomita parser, the average ambigu-ity level was 3.7 analyses per sentence.
The Tomitaparser produced an acceptable 2 parse for all the 31sentences it could analyze in full.
However, the ac-ceptable analysis would still have to be distilled fromall the parses it returned.
The time needed by thechart parser was prohibitively high for any attemptat extracting constituents in the cases when no sin-gle parse was found.
The total time required by themulti-level parser was 60.7 sec, 10.7 times less thanthe Tomita parser and 15.4 times less than the chartparser.
Figure 2 illustrates the parsing and extrac-tion time as function of sentence length.
Althoughclearly dependent on the syntactic omplexity of theinput sentence, the parsing time appears to be lin-early related to the input length.2An analysis with, e.g., wrong PP~attachment that couldbe potentially repaired using lexical selection rules duringtranslation, was marked as acceptable.110 .
3 5 ~ ~ ~0,3  .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
i .
.
.
.
.
.
.
.
: .
.
.
.
.
.
.
.
.OA5 10 15 ~ ~ ~ ~ ~ 45 50Sentenc~Jlen~h02..~~0.15Figure 2: Parse and extraction time as function of sentence l ngthSince the parser's ability to attach constituents islimited, it produced 114 chunks for all the input sen-tences, average 2.71 per sentence.
The chunks fullycovered the input sentences.
They were compared toa bracketing that was assigned by a human and cor-responded to linguistically motivated phrases.
Outof these, 11 corresponded to a wrong bracketing asjudged by a human.
This affected 8 sentences inthe corpus.
The remaining chunks were acceptable(as defined in the footnote on the previous page).Although an evaluation i  terms of precision/recallwould be possible, we have not done it for this paper.We believe that an end-to-end evaluation using, e.g.,an MT or Information Extraction system that wouldbe able to handle the parser output would be moremeaningful, since it would also be a way to evaluatethe effect of the large number of small chunks theparser produced.9 Future  researchThere are a number of research issues that we areplanning to address oon.
First, a more thoroughevaluation is required, as described in the previoussection.
We are currently looking for ways to per-form such an evaluation.
We are also looking intoreplacing the fixed pruning and constituent extrac-tion strategy with one learned from training data.We are also investigating ways of learning the num-ber of levels and grammar rule partitioning amonglevels.10 AcknowledgementsWe would like to thank Eric Nyberg and Teruko Mi-tamura for providing support for this research anduseful discussion, Alon Lavie for helpful commentsabout he research and earlier versions of the paper,and the anonymous NAACL Student Workshop re-viewers for their constructive comments.Re ferencesS.
Abney.
1991.
Parsing by chunks.
In Berwick, Ab-ney, and Tenny, editors, Principle-Based Parsing.Kluwer.S.
Abney.
1996.
Partial parsing via finite-state cas-cades.
In Proceedings of Workshop on RobustParsing, ESSLI-96.J.
Bresnan, editor.
1982.
The mental representationof 9rammatieal relations.
MIT Press.F.
Ciravegna and A. LaveUi.
1997.
Controllingbottom-up chart parser through text chunking.
InProceedings of IWPT'97.K.
Czuba, T. Mitamura, and E. Nyberg.
1998.
Canpractical interlinguas be used for difficult analysisproblems?
In Proceedings of AMTA-98 Workshopon Interlinguas.E.
Nyberg and T. Mitamura.
1992.
The kant sys-tem: Fast, accurate, high-quality translation inpractical domains.
In Proceedings of COLING-92.E.
Nyberg and T. Mitamura.
1997.
A real-time MTsystem for translating broadcast captions.
In Pro-ceedings of MT Summit V1.C.
Polard and I.
Sag.
1994.
Head-Driven PhraseStructure Grammar.
University of Chicago Press.F.
Popowich, D. Turcato, O. Laurens, andP.
McFetridge.
1997.
A lexicalist approach to thetranslation of coUoquial text.
In Proceedings ofTM1-97.M.
P~yner and D. Carter.
1996.
Fast parsing usingpruning and grammar specialization.
In Proceed-ings of A CL'96.12
