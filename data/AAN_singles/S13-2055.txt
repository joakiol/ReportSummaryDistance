Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 2: Seventh International Workshop on SemanticEvaluation (SemEval 2013), pages 333?340, Atlanta, Georgia, June 14-15, 2013. c?2013 Association for Computational LinguisticsAVAYA: Sentiment Analysis on Twitter with Self-Trainingand Polarity Lexicon ExpansionLee Becker, George Erhart, David Skiba and Valentine MatulaAvaya Labs Research1300 West 120th AvenueWestminster, CO 80234, USA{beckerl,gerhart,dskiba,matula}@avaya.comAbstractThis paper describes the systems submitted byAvaya Labs (AVAYA) to SemEval-2013 Task2 - Sentiment Analysis in Twitter.
For theconstrained conditions of both the messagepolarity classification and contextual polaritydisambiguation subtasks, our approach cen-ters on training high-dimensional, linear clas-sifiers with a combination of lexical and syn-tactic features.
The constrained message po-larity model is then used to tag nearly halfa million unlabeled tweets.
These automati-cally labeled data are used for two purposes:1) to discover prior polarities of words and2) to provide additional training examples forself-training.
Our systems performed compet-itively, placing in the top five for all subtasksand data conditions.
More importantly, theseresults show that expanding the polarity lexi-con and augmenting the training data with un-labeled tweets can yield improvements in pre-cision and recall in classifying the polarity ofnon-neutral messages and contexts.1 IntroductionThe past decade has witnessed a massive expansionin communication from long-form delivery suchas e-mail to short-form mechanisms such as mi-croblogging and short messaging service (SMS) textmessages.
Simultaneously businesses, media out-lets, and investors are increasingly relying on thesemessages as sources of real-time information andare increasingly turning to sentiment analysis to dis-cover product trends, identify customer preferences,and categorize users.
While a variety of corpora ex-ist for developing and evaluating sentiment classi-fiers for long-form texts such as product reviews,there are few such resources for evaluating senti-ment algorithms on microblogs and SMS texts.The organizers of SemEval-2013 task 2, have be-gun to address this resource deficiency by coordi-nating a shared evaluation task for Twitter sentimentanalysis.
In doing so they have assembled corporain support of the following two subtasks:Task A - Contextual Polarity Disambiguation?Given a message containing a marked in-stance of a word or phrase, determine whetherthat instance is positive, negative or neutral inthat context.
?Task B - Message Polarity Classification ?Givena message, classify whether the message isof positive, negative, or neutral sentiment.For messages conveying both a positive andnegative sentiment, whichever is the strongersentiment should be chosen.
?This paper describes the systems submitted byAvaya Labs for participation in subtasks A and B.Our goal for this evaluation was to investigate theusefulness of dependency parses, polarity lexicons,and unlabeled tweets for sentiment classification onshort messages.
In total we built four systems forSemEval-2013 task 2.
For task B we developed aconstrained model using supervised learning, andan unconstrained model that used semi-supervisedlearning in the form of self-training and polarity lex-icon expansion.
For task A the constrained sys-tem utilized supervised learning, while the uncon-strained model made use of the expanded lexicon333from task B.
Output from these systems were sub-mitted to all eight evaluation conditions.
For a com-plete description of the data, tasks, and conditions,please refer to Wilson et al(2013).
The remainderof this paper details the approaches, experiments andresults associated with each of these models.2 Related WorkOver the past few years sentiment analysis hasgrown from a nascent topic in natural language pro-cessing to a broad research area targeting a widerange of text genres and applications.
There isnow a significant body of work that spans topicsas diverse as document level sentiment classifica-tion (Pang and Lee, 2008), induction of word polar-ity lexicons (Hatzivassiloglou and McKeown, 1997;Turney, 2002; Esuli and Sebastiani, 2006; Moham-mad and Turney, 2011) and even election prediction(Tumasjan et al 2010).Efforts to train sentiment classifiers for Twittermessages have largely relied on using emoticonsand hashtags as proxies of the true polarity (Bar-bosa and Feng, 2010; Davidov et al 2010b; Pak andParoubek, 2010; Agarwal et al 2011; Kouloumpiset al 2011; Mohammad, 2012).
Classification ofword and phrase sentiment with respect to surround-ing context (Wilson et al 2005) has yet to be ex-plored for the less formal language often found inmicroblog and SMS text.
Semi-supervised learn-ing has been applied to polarity lexicon induction(Rao and Ravichandran, 2009), and sentiment clas-sification at the sentence level (Ta?ckstro?m and Mc-Donald, 2011) and document level (Sindhwani andMelville, 2008; He and Zhou, 2011); however tothe best of our knowledge self-training and othersemi-supervised learning has seen only minimal usein classifying Twitter texts (Davidov et al 2010a;Zhang et al 2012).3 System OverviewGiven our overarching goal of combining polaritylexicons, syntactic information and unlabeled data,our approach centered on first building strong con-strained models and then improving performanceby adding additional data and resources.
Forboth tasks, our data-constrained approach com-bined standard features for document classificationconj ?
conj ?conjunction?pobj ?
prep ?preposition?pcomp?
prepc ?preposition?prep|punct|cc?
?Table 1: Collapsed Dependency Transformation Ruleswith dependency parse and word polarity featuresinto a weighted linear classifier.
For our data-unconstrained models we used pointwise mutual in-formation for lexicon expansion in conjunction withself-training to increase the size of the feature space.4 Preprocessing and Text NormalizationOur systems were built with ClearTK (Ogren etal., 2008) a framework for developing NLP com-ponents built on top of Apache UIMA.
Our pre-processing pipeline utilized ClearTK?s wrappers forClearNLP?s (Choi and McCallum, 2013) tokenizer,lemmatizer, part-of-speech (POS) tagger, and de-pendency parser.
ClearNLP?s ability to retain emoti-cons and emoji as individual tokens made it espe-cially attractive for sentiment analysis.
POS tagswere mapped from Penn Treebank-style tags to thesimplified, Twitter-oriented tags introduced by Gim-pel et al(2011).
Dependency graphs output byClearNLP were also transformed to the StanfordCollapsed dependencies representation (de Marneffeand Manning, 2012) using our own transformationrules (table 1).
Input normalization consisted solelyof replacing all usernames and URLs with commonplaceholders.5 Sentiment ResourcesA variety of our classifier features rely on manuallytagged sentiment lexicons and word lists.
In partic-ular we make use of the MPQA Subjectivity Lexi-con (Wiebe et al 2005) as well as manually-creatednegation and emoticon dictionaries1.
The negationword list consisting of negation words such as noand not.
Because tokenization splits contractions,the list includes the sub-word token n?t as well asthe apostrophe-less version of 12 contractions (e.g.cant, wont, etc .
.
.
).
To support emoticon-specificfeatures we created a dictionary, which paired 183emoticons with either a positive or negative polarity.1http://leebecker.com/resources/semeval-20133346 Message Polarity Classification6.1 FeaturesPolarized Bag-of-Words Features: Instead of ex-tracting raw bag-of words (BOW), we opted to in-tegrate negation directly into the word representa-tions following the approaches used by Das andChen (2001) and Pang et al(2002).
All wordsbetween a negation word and the first punctuationmark after the negation word were suffixed witha NOT tag ?
essentially doubling the number ofBOW features.
We extended this polarized BOWparadigm to include not only the raw word formsbut all of the following combinations: raw word, rawword+PTB POS tag, raw word+simplified POS tag,lemma+simplified POS tag.Word Polarity Features: Using a subjectivity lex-icon, we extracted features for the number of posi-tive, negative, and neutral words as well as the netpolarity based on these counts.
Individual word po-larities were inverted if the word had a child depen-dency relation with a negation (neg) label.
Con-strained models use the MPQA lexicon, while un-constrained models use an expanded lexicon that isdescribed in section 6.2.Emoticon Features: Similar to the word polarityfeatures, we computed features for the number ofpositive, negative, and neutral emoticons, and thenet emoticon polarity score.Microblogging Features: As noted by Kouloumpiset al(2011), the emotional intensity of words in so-cial media messages is often emphasized by changesto the word form such as capitalization, charac-ter repetition, and emphasis characters (asterisks,dashes).
To capture this intuition we compute fea-tures for the number of fully-capitalized words,words with characters repeated more than 3 times(e.g.
booooo), and words surround by asterisks ordashes (e.g.
*yay*).
We also created a binary fea-ture to indicate the presence of a winning score orwinning record within the target span (e.g.
Oh yeah#Nuggets 15-0).Part-of-Speech Tag Features: Counts of the PennTreebank POS tags provide a rough measure of thecontent of the message.Syntactic Dependency Features: We extracteddependency pair features using both standard andcollapsed dependency parse graphs.
Extractedhead/child relations include: raw word/raw word,lemma/lemma, lemma/simplified POS tag, simpli-fied POS tag/lemma.
If the head node of the relationhas a child negation dependency, the pair?s relationlabel is prefixed with a NEG tag.6.2 Expanding the Polarity LexiconUnseen words pose a recurring challenge for bothmachine learning and dictionary-based approachesto sentiment analysis.
This problem is even moreprevalent in social media and SMS messages wheretext lengths are often limited to 140 characters orless.
To expand our word polarity lexicon we adopta framework similar to the one introduced by Turney(2002).
Turney?s unsupervised approach centered oncomputing pointwise mutual information (PMI) be-tween highly polar seed words and bigram phrasesextracted from a corpus of product reviews.Instead of relying solely on seed words for po-larity, we use the constrained version of the mes-sage polarity classifier to tag a corpus of approxi-mately 475,000 unlabeled, English language tweets.These tweets were collected over the period fromNovember 2012 to February 2013.
To reduce thenumber of noisy instances and to obtain a more bal-anced distribution of sentiment labels, we eliminatedall tweets with classifier confidence scores below0.9, 0.7, and 0.8 for positive, negative and neutralinstances respectively.
Applying the threshold, re-duced the tweet count to 180,419 tweets (50,789positive, 59,029 negative, 70,601 neutral).
This fil-tered set of automatically labeled tweets was usedto accumulate co-occurrence statistics between thewords in the tweets and their corresponding senti-ment labels.
These statistics are then used to com-pute word-sentiment PMI (equation 1), which isthe joint probability of a word and sentiment co-occurring divided by the probability of each of theevents occurring independently.
A word?s net po-larity is computed as the signum (sgn) of the differ-ence between a its positive and negative PMI values(equation 2).
It should be noted that polarities weredeliberately limited to values of {-1, 0, +1} to ensureconsistency with the existing MPQA lexicon, and todampen the bias of any single word.335PMI(word, sentiment) = log2p(word, sentiment)p(word)p(sentiment)(1)polarity(word) = sgn(PMI(word, positive)?PMI(word, negative))(2)Words with fewer than 10 occurrences, wordswith neutral polarities, numbers, single characters,and punctuation were then removed from this PMI-derived polarity dictionary.
Lastly, this dictionarywas merged with the dictionary created from theMPQA lexicon yielding a final polarity dictionarywith 11,740 entries.
In cases where an entry existedin both dictionaries, the MPQA polarity value wasretained.
This final polarity dictionary was used bythe unconstrained models for task A and B.6.3 Model Parameters and TrainingConstrained Model: Models were trained us-ing the LIBLINEAR classification library (Fan etal., 2008).
L2 regularized logistic regression waschosen over other LIBLINEAR loss functions be-cause it not only gave improved performance onthe development set but also produced calibratedoutcomes for confidence thresholding.
Trainingdata for the constrained model consisted of all9829 examples from the training (8175 exam-ples) and development (1654 examples) set re-leased for SemEval 2013.
Cost and label-specificcost weight parameters were selected via exper-imentation on the development set to maximizethe average positive and negative F1 values.
Thec values ranged over {0.1, 0.5, 1, 2, 5, 10, 20, 100}and the label weights wpolarity ranged over{0.1, 1, 2, 5, 10, 20, 25, 50, 100}.
Final parametersfor the constrained model were cost c = 1 andweights wpositive = 1, wnegative = 25, andwneutral = 1.Unconstrained Model: In addition to using the ex-panded polarity dictionary described in 6.2 for fea-ture extraction, the unconstrained model also makesuse of automatically labeled tweets for self-training(Scudder, 1965).
In contrast to preparation of the ex-panded polarity dictionary, the self-training placedno threshold on the examples.
Combining the self-labeled tweets, with the official training and devel-opment set yielded a new training set consistingof 485,112 examples.
Because the self-labeled in-stances were predominantly tagged neutral, the LI-BLINEAR cost parameters were adjusted to heav-ily discount neutral while emphasizing positive andneutral instances.
The size and cost of trainingthis model prevented extensive parameter tuning andinstead were chosen based on experience with theconstrained model and to maximize recall on pos-itive and negative items.
Final parameters for theunconstrained model were cost c = 1 and cate-gory weights wpositive = 2, wnegative = 5, andwneutral = 0.1.7 Contextual Polarity Disambiguation7.1 FeaturesThe same base set of features used for message po-larity classification were used for the contextual po-larity classification, with the exception of the syn-tactic dependency features.
To better express the in-context and out-of-context relation these additionalfeature classes were added:Scoped Dependency Features: Because this taskfocuses on a smaller context within the message,collapsed dependencies are less useful as the com-pression may cross over context boundaries.
In-stead the standard syntactic dependency features de-scribed above were modified to account for their re-lation to the context.
All governing relations for thewords contained within the contact were extracted.Relations wholly contained within the boundaries ofthe context were prefixed with an IN tag, whereasthose that crossed outside of the context were pre-fixed with an OUT tag.
Additionally counts of INand OUT relations were included as features.Dependency Path Features: Like the single de-pendency arcs, a dependency path can provide addi-tional information about the syntactic and semanticrole of the context in the sentence.
Our path fea-tures consisted of two varieties: 1) POS-path and2) Sentiment-POS-path.
The POS-path consisted ofthe PTB POS tags and dependency relation labelsfor all nodes between the head of the context and theroot node of the parent sentence.
The Sentiment-POS-path follows the same path but omits the de-pendency relation labels, uses the simplified POStags and appends word polarities (POS/NEG/NTR)to the POS tags along the path.336SystemPositive Negative Neutral Favg RankP R F P R F P R F +/-Tweet NRC-Canada (top) 0.814 0.667 0.733 0.697 0.604 0.647 0.677 0.826 0.744 0.690 1AVAYA-Unconstrained 0.751 0.655 0.700 0.608 0.557 0.582 0.665 0.768 0.713 0.641 5AVAYA-Constrained 0.791 0.580 0.669 0.593 0.509 0.548 0.636 0.832 0.721 0.608 12Mean of submissions 0.687 0.591 0.626 0.491 0.456 0.450 0.612 0.663 0.615 0.538 -SMSNRC-Canada (top) 0.731 0.730 0.730 0.554 0.754 0.639 0.852 0.753 0.799 0.685 1AVAYA-Constrained 0.630 0.667 0.648 0.526 0.581 0.553 0.802 0.756 0.778 0.600 4AVAYA-Unconstrained 0.609 0.659 0.633 0.494 0.637 0.557 0.814 0.710 0.759 0.595 5Mean of submissions 0.512 0.620 0.546 0.462 0.518 0.456 0.754 0.578 0.627 0.501 -Table 2: Message Polarity Classification (Task B) ResultsSystemPositive Negative Neutral Favg RankP R F P R F P R F +/-Tweet NRC-Canada (top) 0.889 0.932 0.910 0.866 0.871 0.869 0.455 0.063 0.110 0.889 1AVAYA-Unconstrained 0.892 0.905 0.898 0.834 0.865 0.849 0.539 0.219 0.311 0.874 2AVAYA-Constrained 0.882 0.911 0.896 0.844 0.843 0.843 0.493 0.225 0.309 0.870 3Mean of submissions 0.837 0.745 0.773 0.745 0.656 0.677 0.159 0.240 0.115 0.725 -SMSGUMLTLT (top) 0.814 0.924 0.865 0.908 0.896 0.902 0.286 0.050 0.086 0.884 1AVAYA-Unconstrained 0.815 0.871 0.842 0.853 0.896 0.874 0.448 0.082 0.138 0.858 3AVAYA-Constrained 0.777 0.875 0.823 0.859 0.852 0.856 0.364 0.076 0.125 0.839 4Mean of submissions 0.734 0.722 0.710 0.807 0.663 0.698 0.144 0.184 0.099 0.704 -Table 3: Contextual Polarity Disambiguation (Task A) ResultsFor example given the bold-faced context in thesentence:@User Criminals killed Sadat, and in theprocess they killed Egypt.
.
.
they destroyedthe future of young & old Egyptians..the extracted POS-path feature would be:{NNP} dobj <{VBD} conj <{VBD}ccomp <{VBD} root <{TOP}while the Sentiment-POS path would be:{?/pos}{V/neg}{V/neg}{V/neg}{TOP}.Paths with depth greater than 4 dependency rela-tions were truncated to reduce feature sparsity.
Inaddition to these detailed path features, we includetwo binary features to indicate if any part of the pathcontains subject or object relations.7.2 Model Parameters and TrainingLike with message polarity classification, the con-textual polarity disambiguation systems rely on LI-BLINEAR?s L2 regularized logistic regression formodel training.
Both constrained and unconstrainedmodels use identical parameters of cost c = 1and weights wpositive = 1, wnegative = 2, andwneutral = 1.
They vary only in the choice of polar-ity lexicon.
The constrained model uses the MPQAsubjectivity lexicon, while the unconstrained modeluses the expanded dictionary derived via computa-tion of PMI, which ultimately differentiates thesemodels through the variation in the sentiment pathand word polarity features.8 Experiments and ResultsIn this section we report results for the series of Sen-timent Analysis in Twitter tasks at SemEval 2013.Please refer to refer to Wilson et al(2013) for theexact details about the corpora, evaluation condi-tions, and methodology.We submitted polarity output for the Message Po-larity Classification (task B) and the Contextual Po-larity Disambiguation (task A).
For each task wesubmitted system output from our constrained andunconstrained models.
As stated above, the con-strained models made use of only the training datareleased for the task, whereas the unconstrainedmodels trained on additional tweets.
Each subtaskhad two test sets one comprised of tweets and theother comprised of SMS messages.
Final task 2337S G Message / Context1 + / Going to Helsinki tomorrow or on the day after tomorrow,yay!2 / + Eric Decker catches his second TD pass from Manning.
This puts Broncos up 31-7 with 14:54 left in the 4th.3 - / So, crashed a wedding reception and Andy Lee?s bro was in the bridal party.
How?d you spend your Saturdaynight?
#longstory4 - + Aiyo... Dun worry la, they?ll let u change one...
Anyway, sleep early, nite nite...5 + - Sori I haven?t done anything for today?s meeting.. pls pardon me.
Cya guys later at 10am.6 + - these PSSA?s are just gonna be the icing to another horrible monday.
#fmlll #ihateschoolTable 4: Example Classification Errors: S=System, G=Gold, +=positive, ?=negative, /=neutral.
Bold-faced textindicates the span for contextual polarities.evaluation is based on the average positive and neg-ative F-score.
Task B results are listed in table 2,and task A results are shown in table 3.
For compar-ison these tables also include the top-ranked systemin each category as well as the mean scores acrossall submissions.9 Error AnalysisTo better understand our systems?
limitations wemanually inspected misclassified output.
Table 4lists errors representative of the common issues un-covered in our error analysis.Though some degree of noise is expected in senti-ment analysis, we found several instances of annota-tion error or ambiguity where it could be argued thatthe system was actually correct.
The message in #1was annotated as neutral, whereas the presence ofthe word ?yay?
suggests an overall positive polarity.The text in #2 could be interpreted as positive, nega-tive or neutral depending on the author?s disposition.Unseen vocabulary and unexpected usages werethe largest category of error.
For example in #3?crashed?
means to attend without an invitation in-stead of the more negative meaning associated withcar accidents and airplane failures.
Although POSfeatures can disambiguate word senses, in this casemore sophisticated features for word sense disam-biguation could help.
While the degradation inperformance between the Tweet and SMS test setsmight be explained by differences in medium, er-rors like those found in #4 and #5 suggest that thismay have more to do with the dialectal differencesbetween the predominantly American and BritishEnglish found in the Tweet test set and the Collo-quial Singaporean English (aka Singlish) found inthe SMS test set.
Error #6 illustrates both how hash-tags composed of common words can easily becomea problem when assigning a polarity to a short con-text.
Hashtag segmentation presents one possiblepath to reducing this source of error.10 Conclusions and Future WorkThe results and rankings reported in section 8 sug-gest that our systems were competitive in assign-ing sentiment across the varied tasks and data con-ditions.
We performed particularly well in dis-ambiguating contextual polarities finishing secondoverall on the Tweet test set.
We hypothesize thisperformance is largely due to the expanded vocabu-lary obtained via unlabeled data and the richer syn-tactic context captured with dependency path repre-sentations.Looking forward, we expect that term recall andunseen vocabulary will continue to be key chal-lenges for sentiment analysis on social media.
Whilelarger amounts of data should assist in that pursuit,we would like to explore how a more iterative ap-proach to self-training and lexicon expansion mayprovide a less noisy path to attaining such recall.11 AcknowledgmentsWe would like to thank the organizers of SemEval2013 and the Sentiment Analysis in Twitter task fortheir time and energy.
We also would like to ex-press our appreciation to the anonymous reviewersfor their helpful feedback and suggestions.ReferencesApoorv Agarwal, Boyi Xie, Ilia Vovsha, Owen Rambow,and Rebecca Passonneau.
2011.
Sentiment analysisof twitter data.
In Proceedings of the Workshop onLanguage in Social Media (LSM 2011).Luciano Barbosa and Junlan Feng.
2010.
Robust senti-ment detection on twitter from biased and noisy data.338In Proceedings of the 23rd International Conferenceon Computational Linguistics, COLING ?10, pages36?44, Stroudsburg, PA, USA.
Association for Com-putational Linguistics.Jinho D. Choi and Andrew McCallum.
2013.
Transition-based dependency parsing with selectional branching.In Proceedings of the 51st Annual Meeting of the As-sociation for Computational Linguistics (ACL?13).Sanjiv Das and Mike Chen.
2001.
Yahoo!
for ama-zon: extracting market sentiment from stock messageboards.
In Proceedings of the 8th Asia Pacific FinanceAssociation Annual Conference.Dmitry Davidov, Oren Tsur, and Ari Rappaport.
2010a.Semi-supervised recognition of sarcastic sentences intwitter and amazon.
In Proceedings of the Four-teenth Conference on Computational Natural Lan-guage Learning.Dmitry Davidov, Oren Tsur, and Ari Rappoport.
2010b.Enhanced sentiment learning using twitter hashtagsand smileys.
In Coling 2010, pages 241?249.Marie-Catherine de Marneffe and Christopher D. Man-ning, 2012.
Stanford typed dependencies manual.Stanford University, v2.0.4 edition, November.Andrea Esuli and Fabrizio Sebastiani.
2006.
SENTI-WORDNET: A Publicly Available Lexical Resourcefor Opinion Mining.
In Proceedings of the 5thConference on Language Resources and Evaluation(LREC?06).Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, Xiang-RuiWang, and Chih-Jen Lin.
2008.
LIBLINEAR: A Li-brary for Large Linear Classication.
Journal of Ma-chine Learning Research, 9:1871?1874.Kevin Gimpel, Nathan Schneider, Brendan O?Connor,Dipanjan Das, Daniel Mills, Jacob Eisenstein, MichaelHeilman, Dani Yogatama, Jeffrey Flanigan, andNoah A. Smith.
2011.
Part-of-speech tagging for twit-ter: Annotation, features, and experiments.
In Pro-ceedings of the 49th Annual Meeting of the Associa-tion for Computational Linguistics: Human LanguageTechnologies ACL:HLT 2011.Vasileios Hatzivassiloglou and Kathleen R. McKeown.1997.
Predicting the semantic orientation of adjec-tives.
In Proceedings of the 35th Annual Meeting ofthe Association for Computational Linguistics (ACL1997).Yulan He and Deyu Zhou.
2011.
Self-training fromlabeled features for sentiment analysis.
InformationProcessing and Management, 47(4):606?616.Efthymios Kouloumpis, Theresa Wilson, and JohannaMoore.
2011.
Twitter Sentiment Analysis: The Goodthe Bad and the OMG!
In Proceedings of the Fifth In-ternational AAAI Conference on Weblogs and SocialMedia (ICWSM 2011).Saif M. Mohammad and Peter D. Turney.
2011.
Crowd-sourcing a word-emotion association lexicon.
Compu-tational Intelligence, 59(000).Saif M. Mohammad.
2012.
#emotional tweets.
In Pro-ceedings of the First Joint Conference on Lexical andComputational Semantics (*SEM).Philip V. Ogren, Philipp G. Wetzler, and Steven Bethard.2008.
ClearTK: A UIMA toolkit for statistical naturallanguage processing.
In Towards Enhanced Interoper-ability for Large HLT Systems: UIMA for NLP work-shop at Language Resources and Evaluation Confer-ence (LREC ?08), 5.Alexander Pak and Patrick Paroubek.
2010.
Twit-ter as a corpus for sentiment analysis and opinionmining.
In Nicoletta Calzolari (Conference Chair),Khalid Choukri, Bente Maegaard, Joseph Mariani,Jan Odijk, Stelios Piperidis, Mike Rosner, and DanielTapias, editors, Proceedings of the Seventh Interna-tional Conference on Language Resources and Eval-uation (LREC?10).Bo Pang and Lillian Lee.
2008.
Opinion mining andsentiment analysis.
Foundations and Trends in Infor-mation Retrieval, 2(1-2):1?135.Bo Pang, Lillian Lee, and Shivakumar Vaithyanathan.2002.
Thumbs up?
Sentiment Classification UsingMachine Learning Techniques.
In Proceedings of theConference on Empirical Methods in Natural Lan-guage Processing (EMNLP 2002).Delip Rao and Deepak Ravichandran.
2009.
Semi-supervised polarity lexicon induction.
In Proceedingsof the 12th Conference of the European Chapter of theACL (EACL 2009).H.
J. Scudder.
1965.
Probability of error of some adap-tive pattern-recognition machine.
IEEE Transactionson Information Theory, 11:363?371.Vikas Sindhwani and Prem Melville.
2008.
Document-word co-regularization for semi-supervised sentimentanalysis.
In Proceedings of the 2008 Eighth IEEE In-ternational Conference on Data Mining, ICDM ?08,pages 1025?1030.Oscar Ta?ckstro?m and Ryan McDonald.
2011.
Semi-supervised latent variable models for sentence-levelsentiment analysis.
In Proceedings of the 49th AnnualMeeting of the Association for Computational Linguis-tics (ACL 2011).Andranik Tumasjan, Timm O. Sprenger, Philipp G. Sand-ner, and Isabell M. Welpe.
2010.
Predicting electionswith twitter what 140 characters reveal about politi-cal sentiment.
In Proceedings of the Fourth Interna-tional AAAI Conference on Weblogs and Social Media(ICWSM 2010).Peter Turney.
2002.
Thumbs up or thumbs down?
se-mantic orientation applied to unsupervised classifica-tion of reviews.
In Proceedings of the 40th Annual339Meeting of the Association for Computational Linguis-tics (ACL 2002).Janyce Wiebe, Theresa Wilson, and Claire Cardie.
2005.Annotating expressions of opinions and emotionsin language.
Language Resources and Evaluation,39:165?210.Theresa Wilson, Janyce Wiebe, and Paul Hoffman.2005.
Recognizing contextual polarity in phrase-level sentiment analysis.
In Proceedings of HumanLanguage Technology Conference and Conference onEmpirical Methods in Natural Language Processing(HLT/EMNLP).Theresa Wilson, Zornitsa Kozareva, Preslav Nakov, AlanRitter, Sara Rosenthal, and Veselin Stoyanov.
2013.SemEval-2013 task 2: Sentiment analysis in twitter.
InProceedings of the 7th International Workshop on Se-mantic Evaluation.
Association for Computation Lin-guistics.Xiuzhen Zhang, Yun Zhou, James Bailey, and Kota-giri Ramamohanarao.
2012.
Sentiment analysisby augmenting expectation maximisation with lexi-cal knowledge.
Proceedings of the 13th InternationalConference on Web Information Systems Engineering(WISE2012).340
