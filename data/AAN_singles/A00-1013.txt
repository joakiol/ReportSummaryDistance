DP: A Detector for Presuppositions in survey questionsKatja WIEMER-HASTINGSPsychology Department / Institute for IntelligentSystemsUniversity of MemphisMemphis, TN 38152kwiemer @ latte.memphis.eduPeter WIEMER-HASTINGSHuman Communication Research CentreUniversity of Edinburgh2 Buccleuch PlaceEdinburgh EH8 9LW, UKpeterwh@cogsci.ed.ac.ukSonya RAJAN, Art GRAESSER, Roger KREUZ, & Ashish KARNAVATInstitute for Intelligent Systems, University of Memphis, Memphis, TN 38152sonyarajan@hotmail.com, graesser@memphis.edu, rkreuz@memphis.edu, akarnavat@hotmail.comAbstractThis paper describes and evaluates a detectorof presuppositions (DP) for survey questions.Incorrect presuppositions can make itdifficult to answer a question correctly.Since they can be difficult to detect, DP is auseful tool for questionnaire designer.
DPperforms well using local characteristics ofpresuppositions.
It reports the presuppositionto the survey methodologist who candetermine whether the presupposition isvalid.IntroductionPresuppositions are propositions that take someinformation as given, or as "the logicalassumptions underlying utterances" (Dijkstra &de Smedt, 1996, p. 255; for a general overview,see McCawley, 1981).
Presupposed informationincludes state of affairs, such as being married;events., such as a graduation; possessions, uch asa house, children, knowledge about something;and others.
For example, the question, "when didyou graduate from college", presupposes theevent that the respondent did in fact graduatefrom college.
The answer options may be rangesof years, such as "between 1970 and 1980".Someone who has never attended college caneither not respond at all, or give a random (andfalse) reply.
Thus, incorrect presuppositionscause two problems.
First, the question isdifficult o answer.
Second, assuming that peoplefeel obliged to answer them anyway, theiranswers present false information.
This biasessurvey statistics, or, in an extreme case, makesthem useless.The detector for presuppositions (DP) is part of thecomputer tool QUAID (Graesser, Wiemer-Hastings, Kreuz, Wiemer-Hastings & Marquis, inpress), which helps survey methodologists designquestions that are easy to process.
DP detects apresupposition and reports it to the surveymethodologist, who can examine if thepresupposition is correct.
QUAID is acomputerized QUEST questionnaire valuationaid.
It is based on QUEST (Graesser & Franklin,1990), a computational model of the cognitiveprocesses underlying human question answering.QUAID critiques questions with respect tounfamiliar technical terms, vague terms, workingmemory overload, complex syntax, incorrectpresuppositions, and unclear question purpose orcategory.
These problems are a subset of potentialproblems that have been identified by Graesser,Bommareddy, Swamer, and Golding (1996; seealso Graesser, Kennedy, Wiemer-Hastings &Ottati, 1999).QUAID performs reliably on the first five problemcategories.
In comparison to these five problems,presupposition detection is even more challenging.For unfamiliar technical terms, for example,QUAID reports words with frequencies below acertain threshold.
Such an elegant solution isimpossible for presuppositions.
Their forms varywidely across presupposition types.
Therefore,their detection requires a complex set of rules,carefully tuned to identify a variety ofpresupposition problems.
DP prints out the90presuppositions of a question, and relies on thesurvey methodologist to make the final decisionwhether the presuppositions are valid.1 How to detect presuppositionsWe conducted a content analysis of questionswith presupposition problems to construct a listof indicators for presuppositions.
22 questionscontaining problematic presuppositions wereselected from a corpus of 550 questions, takenfrom questionnaires provided by the U.S. CensusBureau.
The 22 questions were identified basedon ratings by three human expert raters.
It mayseem that this problem is infrequent, but then,these questions are part of commonly usedquestionnaires that have been designed andrevised very thoughtfully.Additionally, we randomly selected a contrastquestion sample of 22 questions ratedunproblematic with regard to incorrectpresuppositions by all three raters.
Examples (1)and (2) are questions rated as problematic by atleast two raters; examples (3) and (4) presentquestions that do not contain presuppositions.
(1) Is that the same place you USUALLY gowhen you need routine or preventive care, such asa physical examination or check up?
(2) How much do your parents or parent knowabout your close friends' parents?
(3) From date to December 31, did you take oneor more trips or outings in the United States, of atleast one mile, for the PRIMARY purpose ofobserving, photographing, orfeeding wildlife?
(4) Are you now on full-time active duty with thearmed forces?Example (1) presupposes the habit of makinguse of routine / preventive care; (2)presupposes that the respondent has closefriends.As stated above, incorrect presuppositions areinfrequent in well-designed questionnaires.
Forexample, questions about details of somebody'smarriage are usually preceded by a questionestablishing the person's marital status.In spite of this, providing feedback aboutpresuppositions to the survey methodologist isuseful.
Importantly, QUAID is designed to aid inthe design process.
Consider a survey on health-related issues.
In the context of this topic, asurvey methodologist may be interested in howmany days of work a person missed because ofillness, but not think about whether the personactually has a job.
Upon entering the question"how many days of work did you miss last yearbecause of illness" into the QUAID tool, DPwould report that the question presupposesemployment.
The survey methodologist couldthen insert a question about employment.Second, there are subtle presuppositions that maygo undetected even by a skilled survey designer.These are presuppositions about things that arelikely (but not necessarily) true.
For example, aquestion may inquire about a person's closefriends (presupposing close friends) or someone'sstandard place for preventive care (presupposingthe habit of making use of preventive care).
DPdoes not know which presuppositions are likely tobe valid or invalid, and is therefore more likely todetect such subtle incorrect presuppositions than ahuman expert.1.1 The presupposition detector (DP)We constructed a set of presupposition detectionrules based on the content analysis.
The rules usea wide range of linguistic information about theinput sentences, including particular words (suchas "why"), part of speech categories (e.g., wh-pronoun), and complex syntactic subtrees (such asa quantification clause, followed by a nounphrase).1.1.1 The syntactic analysis componentWe used Eric Brill's rule-based word tagger (1992,1994a, 1994b), the de facto state of the art taggingsystem, to break the questions down into part-of-speech categories.
Brill's tagger produces a singlelexical category for each word in a sentence byfirst assigning tags based on the frequency ofoccurrence of the word in that category, and thenapplying a set of context-based re-tagging rules.The tagged text was then passed on to Abney'sSCOL/CASS system (1996a, 1996b), an extremebottom-up parser.
It is designed to avoidambiguity problems by applying rammar rules ona level-by-level basis.
Each level contains rulesthat will only fire if they are correct with highprobability.
Once the parse moves on to a higherlevel, it will not attempt to apply lower-level rules.In this way, the parser identifies chunks ofinformation, which it can be reasonably certain are91connected, even when it cannot create a completeparse of a sentence.1.1.2 The presupposition i dicatorsThe indicators for presuppositions were testedagainst questions rated as "unproblematic" toeliminate items that failed to discriminatequestions with versus without presuppositions.We constructed a second list of indicators thatdetect questions containing no presuppositions.All indicators are listed in Table 1.
These listsare certainly far from complete, but they present agood basis for evaluating of  how wellpresuppositions can be detected by an NLPsystem.
These rules were integrated into adecision tree structure, as illustrated in Figure 1.Table 1: Indicators of absence or presencepresuppositionsFirst word(s)Presupposition No presuppositionWhen VP Initial or followingWhat time comma:Who VP - is thereWhy - are thereHow muchHow many Does / do NP have ...How often etc.
Will NP have ...How VP Has / Have NP ...Where V NP Is / are NP ...Keywords usually everPossessives: anymine, yours, anybodyNP's anythingwhile whetherIndexicals: ifthis, these, such could, wouldSpecific V infinitiveconstructions when NPofYESIAre indicators present hat question \[does not contain presuppositon?
INo//Are indicators present hat questioncontains a presupposition?Is indicator reliable?JFigure 1 : The DP decision structure tree921.2 Classifying presuppositionsDifferent types of presuppositions can bedistinguished based on particular indicators.Examples for presupposition types, such asevents or possessions, were mentioned above.Table 2 presents an exhaustive overview ofpresupposition types identified in our analysis.Note that some indicators can point to more thanone type of presupposition.Table 2 : Classification of presupposition based onindicators.
In the right column, expressions inparentheses identify the presupposed unit.Indicator"how often" ...VP"how" aux NP VP"while"... VP"where"... VP"why"... VPPresupposition type: Thequestion presupposes...an action (V)"usually"... VP"how often","frequently", etc.a habit (V)"how many" NP"where is" NPIndexicals:"this" / "that" NP"these" / "those" NP"such a(n)" NPan entity: object, state, orperson (NP)a shared referent orcommonground (NP)"how much" NP ..."how much does" NP"know""how many" NP ...Possessive pronounsApostrophe 's': NP'sa possession (NP);exception list: NP's that can bepresupposed (name, age, etc.
)"why" S a state of affairs, fact, orassertion (S)VP infinitive an intention / a goal (infinitive /"why" VP NP NP VP)"who" VP"When" VP..."when" NP VPan a~ent (A person who VP)an event (VP)DP reports when a presupposition is present, andit also indicates the type of presupposition that ismade (e.g., a common ground presupposition orthe presupposition f a habit) in order to point thequestion designer to the potential presuppositionerror.
DP uses the expressions in the rightcolumn in Table 2, selected in accordance withthe indicators, and fills them into the brackets inits output (see Figure 1).
For example, given thequestion "How old is your child?
", DP woulddetect the possessive pronoun "your", andaccordingly respond: "It looks like you arepresupposing a possession (child).
Make sure thatthe presupposition is correct by consulting theprevious questions.
"2 EvaluationIn this section, we report summary statistics forthe human ratings of our test questions and themeasures we computed based on these ratings toevaluate DP's performance.2.1 Human ratingsWe used human ratings as the standard againstwhich to evaluate the performance of DP.
Threeraters rated about 90 questions from 12questionnaires provided by the Census Bureau.DP currently does not use context.
To have a fairtest of its performance, the questions werepresented to the human raters out of context, andthey were instructed to rate them as isolatedquestions.
Ratings were made on a four-pointscale, indicating whether the question containedno presupposition (1), probably contained nopresupposition (2), probably contained apresupposition (3), or definitely contained apresupposition (4).
We transformed the ratingsinto Boolean ratings by combining ratings of 1 and2 ("no problem") versus ratings of 3 and 4("problem").
We obtained very similar results foranalyses of the ratings based on the four-point andthe Boolean scale.
For simplicity, we just reportthe results for the Boolean scale.2.2 Agreement among the ratersWe evaluated the agreement among the raters withthree measures: correlations, Cohen's kappa, andpercent agreement.
Correlations were significantonly between two raters (r = 0.
41); thecorrelations of these two with the third raterproduced non-significant correlations, indicatingthat the third rater may have used a differentstrategy.
The kappa scores, similarly, weresignificant only for two raters (_k_ = 0.36).
In termsof percent agreement, he raters with correlatedratings agreed in 67% of the cases.
Thepercentages of agreement with rater 3 were 57%and 56%, respectively.DP ratings were significantly correlated with theratings provided by the two human raters who93agreed well (_r = 0.32 and 0.31), resulting inagreement of ratings in 63% and 66% of thequestions.
In other words, the agreement ofratings provided by the system and by two humanraters is comparable to the highest agreement rateachieved between the human raters.Some of the human ratings divergedsubstantially.
Therefore, we computed tworestrictive measures based on the ratings toevaluate the performance of DP.
Both scores areBoolean.
The first score is "lenient"; it reports apresupposition only if at least two raters report apresupposition for the question (rating of 3 or 4).We call this measure P~j, a majority-basedpresupposition count.
The second score is strict.It reports a presupposition only if all three ratersreport a presupposition.
This measure is calledPcomp, a presupposition count based on completeagreement.
It results in fewer detectedpresuppositions overall: Pcomp reportspresuppositions for 29 of the questions (33%),whereas P~j reports 57 (64%).2.3 Evaluation of the DPDP ratings were significantly correlated only withPcomp (0.35).
DP and P~o~ ratings were inagreement for 67% of the questions.
Table 3 listshit and false alarm rates for DP, separately for P~jand P~omp.
The hit rate indicates how many of thepresuppositions identified by the human ratingswere detected by DP.
The false alarm rateindicates how often DP reported a presuppositionwhen the human raters did not.
The measures lookbetter with respect to the complete agreementcriterion, P~omp-Table 3 further lists recall and precision scores.The recall rate indicates how manypresuppositions DP detects out of thepresuppositions reported by the human ratingcriterion (computed as hits, divided by the sum ofhits and misses).
The precision score (computedas hits, divided by the sum of hits and falsealarms) measures how many presuppositionsreported by DP are actually present, as reported bythe human ratings.Table 3: Performance measures for DP with respect to hits, false alarms, and misses.Hit rate False alarm rate Recall Precision d'P~j 0.54 0.34 0.66 0,74 0.50Pcomo 0.72 0.35 0.72 0,50 0.95All measures, except for precision, lookcomparable or better in relation to Pco~,,including d', which measures the actual power ofDP to discriminate questions with and withoutpresuppositions.
Of course, picking a criterionwith better matches does not improve thesystem's performance in itself.3 An updated version of DPBased on the first results, we made a fewmodifications and then reevaluated DP.
Inparticular, we added items to the possessionexception list based on the new corpus and madesome of the no-presupposition rules morespecific.
As a more drastic change, we updatedthe decision tree structure so that presuppositionindicators overrule indicators againstpresuppositions, increasing the number ofreported presuppositions for cases of conflictingindicators:If there is evidence for a problem, report "Problem"Elseif evidence against problem, report "No problem"else, report "Probably not a problem"Separate analyses show that the modification ofthe decision tree accounts for most of theperformance improvement.3.1 ResultsTable 4 lists the performance measures for theupdated DP.
Hit and recall rate increased, but sodid the false alarm rate, resulting in a lowerprecision score.
The d' score of the updatedsystem with respect o Pcomp (1.3) is substantiallybetter.
The recall rate for this setting is perfect,i.e., DP did not miss any presuppositions.
Sincesurvey methodologists will decide whether thepresupposition is really a problem, a higher falsealarm rate is preferable to missing outpresupposition cases.
Thus, the updated DP is animprovement over the first version.R/t 94Table 4: Performance measures for the updated DP with respect to hits, false alarms, and misses.Hit rate False alarm rate Recall Precision d'Pmai 0.75 0.44 0.84 0.75 0.8P~o,~p 0.90 0.52 1.00 0.46 1.3ConclusionDP can detect presuppositions, and can therebyreliably help a survey methodologist to eliminateincorrect presuppositions.
The results for DPwith respect o Pco~p are comparable to, and insome cases even better than, the results for theother five categories.
This is a very good result,since most of the five problems allow for "easy"and "elegant" solutions, whereas DP needs to beadjusted to a variety of problems.It is interesting that the performance of DP looksso much better when compared to the completeagreement score, Pcomp than when compared toP~j.
Recall that Pcomp only reports apresupposition if all the raters report one.
Thehigh agreement of the raters in these cases canpresumably be explained by the salience of thepresupposition problem.
This indicates that DPmakes use of reliable indicators for itsperformance.
Good agreement with the othermeasure, Pmaj, would suggest that DP additionallyreports presuppositions i  cases where humans donot agree that a presupposition is present.
Thehigher agreement with the stricter measure is thusa good result.DP currently works like the other modules ofQUA\]D: it reports potential problems, but leavesit to the survey methodologist to decide whetherto act upon the feedback.
As such, DP is asubstantial addition to QUA\]D. A futurechallenge is to turn DP into a DIP (detector ofincorrect presuppositions), that is, to reduce thenumber of reported presuppositions to thoselikely to be incorrect.
DP currently evaluates allquestions independent of context, resulting infrequent detections.
For example, 20 questionsabout "this person" may follow one question thatestablishes the referent.
High-frequencyrepetitive presupposition reports could easily getannoying.Is a DIP system feasible?
At present, it isdifficult for NLP systems to use information fromcontext in the evaluation of a statement.
What isrequired to solve this problem is a mechanism thatdetermines whether a presupposed entity (anobject, an activity, an assertion, etc.)
has beenestablished as applicable in the previous discourse(e.g., in preceding questions).The Construction Integration (CI) model byKintsch (1998) provides a good example for howsuch reference ambiguity can be resolved.
CI usesa semantic network that represents an entity in thediscourse focus (such as "this person") throughhigher activations of its links to other conceptnodes.
Perhaps models such as the CI model canbe integrated into the QUAID model to performcontext analyses, in combination with tools likeLatent Semantic Analysis (LSA, Landauer &Dumais, 1997), which represents text units asvectors in a high-dimensional semantic space.LSA measures the semantic similarity of text units(such as questions) by computing vector cosines.This feature may make LSA a useful tool in thedetection of a previous question that establishes apresupposed ntity in a later question.However, questionnaires differ from connecteddiscourse, such as coherent stories, in aspects thatmake the present problem rather more difficult.Most importantly, the referent for "this person"may have been established in question umber 1,and the current question containing thepresupposition "this person" is question umber52.
A DIP system would have to handle a flexibleamount of context, because the distance betweenquestions establishing the correctness of apresupposition a d a question building up on it canvary.
On the one hand, one could limit theconsidered context to, say, three questions and riskmissing the critical question.
On the other hand, itis computationally expensive to keep the completeprevious context in the systems "workingmemory" to evaluate the few presuppositionswhich may refer back over a large number ofquestions.
Solving this problem will likely requirecomparing a variety of different settings.Q~ 95AcknowledgementsThis work was partially supported by the CensusBureau (43-YA-BC-802930) and by a grant fromthe National Science Foundation (SBR 9720314and SBR 9977969).
We wish to acknowledgethree colleagues for rating the questions in ourevaluation text corpus, and our collaboratorSusan Goldman as well as two anonymousreviewers for helpful comments.ReferencesAbney, S. (1996a).
Partial parsing via finite-statecascades.
In Proceedings of the ESSLLI '96 RobustParsing Workshop.Abney, S. (1996b).
Methods and statistical linguistics.In J. Klavans & P. Resnik (Eds.
), The BalancingAct.
Cambridge, MA: MIT PressBrill, E. (1992).
A simple rule-based part of speechtagger.
In Proceedings of the Third Conference onApplied Natural Language Processing.
ACL.Brill, E. (1993).
A corpus-based approach to languagelearning.
Ph.D. thesis, University of Pennsylvania,Philadelphia, PA.Brill, E. (1994).
Some advances in rule-based part ofspeech tagging.
In Proceedings of the TwelfthNational Conference on Articial Intelligence.
AAAIPress.Dijkstra, T., & de Smedt, K. (1996).
Computationalpsycholinguistics.
AI and connectionist models ofhuman language processing.
London: Taylor &Francis.Graesser, A. C., Bommareddy, S., Swamer, S., &Golding, J.
(1996).
Integrating questionnaire designwith a cognitive computational model of humanquestion answering.
In N. Schwarz & S.
Sudman(Eds.
), Answering questions: Methods ofdetermining cognitive and communicative processesin survey research (pp.
343-175).
San Francisco,CA: Jossey-Bass.Graesser, A.C., & Franklin, S.P.
(1990).
QUEST: Acognitive model of question answering.
DiscourseProcesses, 13, 279-304.Graesser, A.C., Kennedy, T., Wiemer-Hastings, P., &Ottati, V. (1999).
The use of computationalcognitive models to improve questions on surveysand questionnaires.
In M. Sirken, D. Herrrnann, S.Schechter, N. Schwarz, J. Tanur, & R.
Tourangeau(Eds.
), Cognition and Survey Research (pp.
199-216).
New York: John Wiley & Sons.Graesser, A.C., Wiemer-Hastings, K., Kreuz, R.,Wiemer-Hastings, P., & Marquis, K. (in press).QUAID: A questionnaire evaluation aid for surveymethodologists.
Behavior Research Methods,Instruments, & Computers.Kintsch, W. (1998).
Comprehension.
A paradigm forcognition.
Cambridge, UK: Cambridge UniversityPress.Landauer, T.K., & Dumais, S.T.
(1997).
A solution toPlato's problem: The latent semantic analysis theoryof acquisition, induction, and representation ofknowledge.
Psychological Review, 104, 211-240.McCawley, J.D.
(1981).
Everything that linguists havealways wanted to know about logic.
Chicago:University of Chicago Press.QI~ 96
