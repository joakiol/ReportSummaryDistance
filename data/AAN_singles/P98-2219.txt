Learning Optimal Dialogue Strategies:A Case Study of a Spoken Dialogue Agent for EmailMarilyn A. Walkerwalker @ research.att.comATT Labs Research180 Park Ave.Florham Park, NJ 07932Jeanne C. Fromerjeannie@ai.mit.eduMIT AI Lab545 Technology SquareCambridge, MA, 02139Shrikanth Narayananshri @ research.att.comATT Labs Research180 Park Ave.Florham Park, NJ 07932AbstractThis paper describes a novel method by which a dia-logue agent can learn to choose an optimal dialoguestrategy.
While it is widely agreed that dialoguestrategies hould be formulated in terms of com-municative intentions, there has been little work onautomatically optimizing an agent's choices whenthere are multiple ways to realize a communica-tive intention.
Our method is based on a combina-tion of learning algorithms and empirical evaluationtechniques.
The learning component of our methodis based on algorithms for reinforcement learning,such as dynamic programming and Q-learning.
Theempirical component uses the PARADISE evalua-tion framework (Walker et al, 1997) to identify theimportant performance factors and to provide theperformance function needed by the learning algo-rithm.
We illustrate our method with a dialogueagent named ELVIS (EmaiL Voice Interactive Sys-tem), that supports access to email over the phone.We show how ELVIS can learn to choose amongalternate strategies for agent initiative, for readingmessages, and for summarizing email folders.1 IntroductionThis paper describes a novel method by which a dia-logue agent can learn to choose an optimal dialoguestrategy.
The main problem for dialogue agentsis deciding what information to communicate o ahearer and how and when to communicate it.
Forexample, consider one of the strategy choices facedby a spoken dialogue agent that accesses email byphone.
When multiple messages match the user'squery, e.g.
Read my messages from Kim, an emailagent must choose among multiple response strate-gies.
The agent might choose the Read-First strat-egy in DI:(D1) A: In the messages from Kim, there's 1 messageabout "Interviewing Antonio" and 1 messageabout "Meeting Today:' The first message istitled, "Interviewing Antonio:' It says, 'Td liketo interview him.
I could also go along to lunch.Kim:'D1 involves summarizing all the messages fromKim, and then taking the initiative to read the firstone.
Alternate strategies are the Read-Summary-Only strategy in D2, where the agent provides infor-mation that allows users to refine their selection cri-teria, and the Read-Choice-Prompt strategy in D3,where the agent explicitly tells the user what to sayin order to refine the selection:(D2) A: In the messages from Kim, there's 1 messageabout "Interviewing Antonio" and 1 message about"Meeting Today:'(D3) A: In the messages from Kim, there's 1 messageabout "Interviewing Antonio" and 1 message about"Meeting Today:' To hear the messages, ay, "In-terviewing Antonio" or "Meeting.
"Decision theoretic planning can be applied to theproblem of choosing among strategies, by associ-ating a utility U with each strategy (action) choiceand by positing that agents should adhere to theMaximum Expected Utility Principle (Keeney andRaiffa, 1976; Russell and Norvig, 1995),Maximum Expected Utility Principle:An optimal action is one that maximizesthe expected utility of outcome states.An agent acts optimally by choosing a strategya in state Si that maximizes U(Si).
But how arethe utility values U(Si) for each dialogue state Siderived?Several reinforcement learning algorithms basedon dynamic programming specify a way to calcu-late U(Si) in terms of the utility of a successor stateSj (Bellman, 1957; Watkins, 1989; Sutton, 1991;Barto et al, 1995).
Thus if we know the utility for1345the final state of the dialogue, we can calculate theutilities for all the earlier states.
However, until re-cently there as been no way of determining a per-formance function for assigning a utility to the finalstate of a dialogue.This paper presents a method based on dynamicprogramming by which dialogue agents can learnto optimize their choice of dialogue strategies.
Wedraw on the recently proposed PARADISE evalua-tion framework (Walker et al, 1997) to identify theimportant performance factors and to provide a per-formance function for calculating the utility of thefinal state of a dialogue.
We illustrate our methodwith a dialogue agent named ELVIS (EmaiL VoiceInteractive System), that supports access to emailover the phone.
We test alternate strategies for agentinitiative, for reading messages, and for summariz-ing email folders.
We report results from modelinga corpus of 232 spoken dialogues in which ELVISconversed with human users to carry out a set ofemail tasks.2 Method  for Learn ing  to Opt imizeD ia logue Strategy Select ionOur method for learning to optimize dialogue strat-egy selection combines the application of PAR-ADISE to empirical data (Walker et al, 1997), withalgorithms for learning optimal strategy choices.PARADISE provides an empirical method for de-riving a performance function that calculates over-all agent performance as a linear combination of anumber of simpler metrics.
Our learning methodconsists of the following sequence of steps:?
Implement a spoken dialogue agent for a particulardomain.?
Implement multiple dialogue strategies and designthe agent so that strategies are selected randomly orunder experimenter control.?
Define a set of dialogue tasks for the domain, andtheir information exchange requirements.
Repre-sent hese tasks as attribute-value matrices to facil-itate calculating task success.?
Collect experimental dialogues in which a numberof human users converse with the agent o do thetasks.?
For each experimental dialogue:- Log the history of the state-strategy choicesfor each dialogue.
Use this to estimate a statetransition model.- Log a range of quantitative and qualitativecost measures for each dialogue, either auto-matically or with hand-tagging.- Collect user satisfaction reports for each dia-logue.?
Use multivariate linear regression with user satis-faction as the dependent variable and task successand the cost measures as independent variables todetermine a performance equation.?
Apply the derived performance equation to each di-alogue to determine the utility of the final state ofthe dialogue.?
Use reinforcement learning to propagate he utilityof the final state back to states Si where strategychoices were made to determine which action max-imizes U(Si).These steps consist of those for deriving aperfor-mance function (Section 3), and for using the de-rived performance function as feedback to the agentwith a learning algorithm (Section 4).3 Using PARADISE to Derive aPerformance Funct ion3.1 ELVIS Spoken Dialogue SystemELVIS is implemented using a general-purposeplatform for spoken dialogue agents (Kamm et al,1997).
The platform consists of a speech recognizerthat supports barge-in so that the user can interruptthe agent when it is speaking.
It also provides anaudio server for both voice recordings and text-to-speech ('I~I'S), an interface between the computerrunning ELVIS and the telephone network, a mod-ule for application specific functions, and modulesfor specifying the application grammars and the dia-logue manager.
Our experiments are based on mod-ifications to the dialogue manager as described be-low.The dialogue manager is based on a state ma-chine.
Each state specifies transitions to other statesand the conditions that license these transitions,as well as a grammar for what the user can say.State definitions also include the specification ofagent prompts in terms of templates, with variablesthat are instantiated each time the state is entered.Prompts include: (1) an initial prompt, which theagent says upon entering the state (this may includea response to the user's current request); (2) a helpprompt which the agent says if the user says help;(3) multiple rejection prompts which the agent saysif the speech recognizer confidence is too low tocontinue without more user input; (4) multiple time-out prompts which the agent produces if the userdoesn't say anything.Each of these specifications i affected by theagent' s dialogue strategy.
An agent' s dialogue strat-egy is implemented asa combination ofthe prompts1346that are played to the user and the state transitionsthat the agent makes in response to the user' sutter-ance.
In particular, alternative prompts can be spec-ified for all types of prompts (initial, help, rejectionand timeout) to provide alternate dialogue strategiesin each state.
We implemented alternate strategiesfor reading messages, for initiative and for summa-rization.The alternate strategies for reading messagesare Read-First, Read-Summarize-Only and Read-Choice-Prompt as in dialogues D1, D2, and D3.The alternate strategies for initiative are a system-initiative (SI) strategy and a mixed-initiative (MI)strategy.
In the SI strategy, the system maintainscontrol of the dialogue and the user must specifyone query parameter ata time, as in D4.
(D4) A: Hi, Elvis here.
You have 5 new and 0 unreadmessages inyour inbox.
Say Read, Summarize, orsay Help for more options.U: Read.A: Select by Sender, Subject or in order of arrival?U: Sender,A: Which Sender?U: Kim.A: In the messages from Kim, there's 1 messageabout "Interviewing Antonio".
.
....In the MI strategy, the system never tells the userwhat to do, and the user can take control of the dia-logue and specify exactly what they want to do in asingle utterance, as in D5.
(D5) A: Hi, Elvis here.
I' ve got your mail.U: Read me my messages from Kim.We also implemented three alternate summariza-tion strategies as responses to a user equest to Sum-marize my messages.
The basis for these alternatesis ELVIS's decision about which message attributesto mention in the summary.
The Summarize-Choicestrategy in D6 asks the user to specify which of therelevant attributes to summarize by.
(D6) A: Summarize by subject, by sender, or both?U: Subject.A: In your top level inbox, there's 1 message about"Lunch" 1 message about "Interviewing Antonio".1 message about "Call Me Tomorrow," 1messageabout "Evaluation Group Meeting," and 1 messageabout "Agent Personality:'The Summarize-Both strategy in D7 uses all at-tributes in the summary:(DT) A: In your top level inbox, from Kim, there's 1message about "Lunch:' From Michael, there's 1message about "Evaluation group meeting."
FromNoah, there's 1 message about "Call Me Tomor-row" and 1 message about "Interviewing Antonio?
'And from Owen, there's 1 message about "AgentPersonality?
'In the Summarize-System strategy in D8, ELVISsummarizes by subject or by sender based on thecurrent context.
For instance, if the user is in the toplevel inbox, ELVIS will summarize by sender, but ifthe user is situated in a folder containing messagesfrom Noah, ELVIS will summarize by subject, as asummary by sender would provide no new informa-tion.
(D8) A: In your top level inbox, there's 1 messagefrom Kim, 2 messages from Noah, 1 message fromMichael, and 1 message from Owen.Transitions between states are driven by theuser' sconversational behavior, such as whether s/hesays anything and what s/he says, the semantic in-terpretation fthe user' sutterances, and the settingsof the agent' s dialogue strategy parameters.3.2 Experimental DesignExperimental dialogues were collected via two ex-periments in which users (AT&T summer internsand MIT graduate students) interacted with ELVISto complete three representative application tasksthat required them to access email messages in threedifferent email inboxes.
In the second experiment,users participated in a tutorial dialogue before do-ing the three tasks.
The first experiment varied ini-tiative strategies and the second experiment variedthe presentation strategies for reading messages andsummarizing folders.
In order to have adequate datafor learning, the agent must explore the space ofstrategy combinations and collect enough samplesof each combination.
In the second experiment, weparameterized the agent so that each user interactedwith three different versions of ELVIS, one for eachtask.
These experiments resulted in a corpus of 108dialogues testing the initiative strategies, and a cor-pus of 124 dialogues testing the presentation strate-gies.Each of the three tasks were performed in se-quence, and each task consisted of two scenarios.Following PARADISE, the agent and the user hadto exchange information about criteria for selectingmessages and information within the message bodyin each scenario.
Scenario 1.1 is typical.?
1.1: You are working at home in the morning andplan to go directly to a meeting when you go into1347work.
Kim said she would send you a messagetelling you where and when the meeting is.
Findout the Meeting Time and the Meeting Place.Scenario 1.1 is represented in terms of the at-tribute value matrix (AVM) in Table 1.
Successfulcompletion of a scenario requires that all attribute-values must be exchanged (Walker et al, 1997).
TheAVM representation for all six scenarios is similarto Table 1, and is independent of ELVIS's dialoguestrategy.attribute actual valueSelection Criteria Kim V MeetingEmail.attl 10:30Email.att2 2D516Table 1: Attribute value matrix instantiation, Keyfor Email Scenario 1.13.3 Data CollectionThree different methods are used to collect he mea-sures for applying the PARADISE framework andthe data for learning: (1) All of the dialogues arerecorded; (2) The dialogue manager logs the agent'sdialogue behavior and a number of other measuresdiscussed below; (3) Users fill out web page formsafter each task (task success and user satisfactionmeasures).
Measures are in boldface below.The dialogue recordings are used to transcribe theuser's utterances to derive performance measuresfor speech recognition, tocheck the timing of the in-teraction, to check whether users barged in on agentutterances (Barge In), and to calculate the elapsedtime of the interaction (ET).For each state, the system logs which dialoguestrategy the agent selects.
In addition, the num-ber of timeout prompts (Timeout Prompts), Rec-ognizer Rejections, and the times the user saidHelp (Help Requests) are logged.
The number ofSystem Turns and the number of User Turns arecalculated on the basis of this data.
In addition,the recognition result for the user's utterance is ex-tracted from the recognizer and logged.
The tran-scriptions are used in combination with the loggedrecognition result to calculate a concept accuracymeasure for each utteranceJ Mean concept accu-racy is then calculated over the whole dialogue and1For example, the utterance R ad my messages from Kimcontains two concepts, the read function, and the sender:kimselection criterion.
If the system understood nly that he usersaid Read, concept accuracy would be .5.used as a Mean Recognition Score MRS for the di-alogue.The web page forms are the basis for calculat-ing Task Success and User Satisfaction measures.Users reported their perceptions as to whether theyhad completed the task (Comp), 2 and filled in anAVM with the information that they had acquiredfrom the agent, e.g.
the values for Email.attl andEmail.att2 in Table 1.
The AVM matrix supportscalculating Task Success objectively by using theKappa statistic to compare the information in theAVM that the users filled in with an AVM key suchas that in Table 1 (Walker et al, 1997).In order to calculate User Satisfaction, users wereasked to evaluate the agent's performance with auser satisfaction survey.
The data from the surveyresulted in user satisfaction values that range from 0to 33.
See (Walker et al, 1998) for more details.3.4 Deriving a Performance FunctionOverall, the results howed that users could success-fully complete the tasks with all versions of ELVIS.Most users completed each task in about 5 minutesand average ,~ over all subjects and tasks was .82.However, there were differences between strategies;as an example see Table 2.Measure SYSTEM (SI) MIXED (MI)KappaCompUser TurnsSystem TurnsElapsed Time (ET)MeanRecog (MRS)Time OutsHelp RequestsBarge InsRecognizer Rejects.81.8325.9428.18328.59 s.882.24.705.2.98.83.7817.5921.74289.43 s.724.15.943.51.67User Satisfaction 26.6 23.7Table 2: Performance measure means per dialoguefor Initiative StrategiesPARADISE provides a way to calculate dialogueagent performance as a linear combination of anumber of simpler metrics that can be directly mea-sured such as those in Table 2.
Performance for any(sub)dialogue D is defined by the following equa-tion:nPerformance= (a ,  A/'(,~))- ~_,wi*  A/'(ci)i=12 Yes, No responses are converted to 1,0.1348where o~ is a weight on n, ci are the cost functions,which are weighted by wl, and .Af is a Z score nor-malization function (Walker et al, 1997; Cohen,1995).
The Z score normalization function ensuresthat, when the weights c~ and wi are solved for, thatthe magnitude of the weights reflect he magnitudeof the contribution of that factor to performance.The performance function is derived through mul-tivariate linear regression with User Satisfaction asthe dependent variable and all the other measures asindependent variables (Walker et al, 1997).
See Ta-ble 2.
In the ELVIS data, an initial regression overthe measures in Table 2 suggests that Comp, MRSand ET are the only significant contributors to UserSatisfaction.
A second regression including onlythese factors results in the following equation:Performance = .21 ?
Comp+.47 .
MRS -.
15.
ETwith Comp (t=2.58, p =.01), MRS (t =5.75, p=.0001) and ET (t=-l.8, p=.07) significant predic-tors, accounting for 38% of the variance in R-Squared (F (3,104)=21.2, p <.0001).
The mag-nitude of the coefficients in this equation demon-strates the performance of the speech recognizer(MRS) is the most important predictor, followed byusers' perception of Task Success (Comp) and ef-ficiency (ET).
In the next section, we show how touse this derived performance equation to computethe utility of the final state of the dialogue.4 Apply ing Q- learning to ELVISExperimental DataThe basic idea is to apply the performance func-tion to the measures logged for each dialogue Di,thereby replacing a range of measures with a singleperformance value 19i.
Given the performance val-ues Pi, any of a number of automatic learning algo-rithms can be used to determine which sequence ofaction choices (dialogue strategies) maximize util-ity, by using/~ as the utility for the final state of thedialogue Di.
Possible algorithms include GeneticAlgorithms, Q-learning, TD-Leaming, and Adap-tive Dynamic Programming (Russell and Norvig,1995).
Here we use Q-learning to illustrate themethod (Watkins, 1989).
See (Fromer, 1998) forexperiments using alternative algorithms.The utility of doing action a in state Si, U(a, Si)(its Q-value), can be calculated terms of the utilityof a successor state S i, by obeying .the followingrecursive quation:M ~ u(a,s ) = R(Sd+ F,Jwhere R(Si) is a reward associated with being instate Si, a is a strategy from a finite set of strate-gies A that are admissable in state Si, and M~j isthe probability of reaching state Sj if strategy a isselected in state Si.In the experiments reported here, the reward asso-ciated with each state, R(SI), is zero.
3 In addition,since reliable a priori prediction of a user action in aparticular state is not possible (for example the usermay say Help or the speech recognizer may fail tounderstand the user), the state transition model M/~is estimated from the logged state-strategy historyfor the dialogue.The utility values can be estimated to within a de-sired threshold using Value Iteration, which updatesthe estimate of U(a, Si), based on updated utilityestimates for neighboring states, so that the equa-tion above becomes:Un+l(a, Sd = R(Sd + ~ M~ m?xUn(a',Sj)3where Un(a, Si) is the utility estimate for doinga in state Si after n iterations.
Value Iterationstops when the difference between Un(a, Si) andUn+l (a, Si) is below a threshold, and utility valueshave been associated with states where strategy se-lections were made.
After experimenting with var-ious threshholds, we used a threshold of 5% of theperformance range of the dialogues.The result of applying Q-learning to ELVIS datafor the initiative strategies i illustrated in Figure 1.The figure plots utility estimates for SI and MI overtime.
It is clear that the SI strategy is better becauseit has a higher utility: at the end of 108 trainingsessions (dialogues), the utility of SI is estimatedat .249 and the utility of MI is estimated at -0.174.TYPE STRATEGY UTILITYRead Read-First .21Read-Choice-Prompt .07Read-Summarize-Only .08Summarize Summarize-System .162Summarize-Choice -0.03Summarize-Both .09Table 3: Utilities for Presentation Strategy Choicesafter 124 Training SessionsThe SI and MI strategies affect the whole dia-logue; the presentation strategies apply locally anda See (Fromer, 1998) for experiments in which local rewardsare nonzero.1349O.O.E0.40.2-0.2-OA-g.6-0.8-1Utilities f~" SI and MI over Training Sessions- ~ oTraining Instarces (Dialogues)i100 120Figure 1: Results of applying Q-learning to System-Initiative (SI) and Mixed-Initiative (MI) Strategiesfor 108 ELVIS Dialoguescan be actived in different states of the dialogue.
Weexamined the variation in a strategy' s utility at eachphase of the task, by representing the task as havingthree phases: no scenarios completed, one scenariocompleted and both scenarios completed.
Table 3reports utilities for the use of a strategy after onescenario was completed.
The policy implied by theutilities at other phases of the task are the same.
See(Fromer, 1998) for more detail.The Read-First strategy in D1 has the best per-formance of the read strategies.
This strategy takesthe initiative to read a message, which might re-sult in messages being read that the user wasn't in-terested in.
However since the user can barge-inon system utterances, perhaps little is lost by tak-ing the initiative to start reading a message.
After124 training sessions, the best summarize strategyis Summarize-System, which automatically selectswhich attributes to summarize by, and so does notincur the cost of asking the user to specify these at-tributes.
However, the utilities for the Summarize-Choice strategy have not completely converged after124 trials.5 Conclusions and Future WorkThis paper illustrates a novel technique by whichan agent can learn to choose an optimal dialoguestrategy.
We illustrate our technique with ELVIS, anagent hat supports access to email by phone, withstrategies for initiative, and for reading and sum-marizing messages.
We show that ELVIS can learnthat the System-Initiative strategy has higher utilitythan the Mixed-Initiative strategy, that Read-First isthe best read strategy, and that Summarize-Systemis the best summary strategy.Here, our method was illustrated by evaluatingstrategies for managing initiative and for messagepresentation.
However there are numerous dia-logue strategies that an agent might use, e.g.
togather information, handle errors, or manage the di-alogue interaction (Chu-Carroll and Carberry, 1995;Danieli and Gerbino, 1995; Hovy, 1993; McK-eown, 1985; Moore and Paris, 1989).
Previouswork in natural anguage generation has proposedheuristics to determine an agent's choice of dialoguestrategy, based on factors such as discourse focus,medium, style, and the content of previous expla-nations (McKeown, 1985; Moore and Paris, 1989;Maybury, 1991; Hovy, 1993).
It should be possi-ble to test experimentally whether an agent can au-tomatically learn these heuristics ince the method-ology we propose is general, and could be appliedto any dialogue strategy choice that an agent mightmake.Previous work has also proposed that an agent'schoice of dialogue strategy can be treated as astochastic optimization problem (Walker, 1993;Biermann and Long, 1996; Levin and Pieraccini,1997).
However, to our knowledge, these meth-ods have not previously been applied to interactionswith real users.
The lack of an appropriate perfor-mance function has been a critical methodologicallimitation.We use the PARADISE framework (Walker etal., 1997) to derive an empirically motivated per-formance function, that combines both subjectiveuser preferences and objective system performancemeasures into a single function.
It would have beenimpossible to predict a prior{ which dialogue fac-tors influence the usability of a dialogue agent, andto what degree.
Our performance equation showsthat both dialogue quality and efficiency measurescontribute to agent performance, but that dialoguequality measures have a greater influence.
Further-more, in contrast to assuming an a priori model, weuse the dialogues from real user-system interactionsto provide realistic estimates of M/~, the state tran-sition model used by the learning algorithm.
It isimpossible to predict a priori the transition frequen-cies, given the imperfect nature of spoken languageunderstanding, and the unpredictability of user be-1350havior.The use of this method introduces several openissues.
First, the results of the learning algorithmare dependent on the representation f the statespace.
In many reinforcement learning problems(e.g.
backgammon), the state space is pre-defined.In spoken dialogue systems, the system designersconstruct the state space and decide what state vari-ables need to be monitored.
Our initial results ug-gest that the state representation that the agent usesto interact with the user may not be the optimal staterepresentation for learning.
See (Fromer, 1998).Second, in advance of actually running learning ex-periments, it is not clear how much experience anagent will need to determine which strategy is bet-ter.
Figure 1 shows that it took no more than 50dialogue samples for the algorithm to show the dif-ferences in convergence trends when learning aboutinitiative strategies.
However, it appears that moredata is needed to learn to distinguish between thesummarization strategies.
Third, our experimentaldata is based on short-term interactions with noviceusers, but we might expect hat users of an emailagent would engage in many interactions with thesame agent, and that preferences for agent interac-tion strategies could change over time with user ex-pertise.
This means that the performance functionmight change over time.
Finally, the learning algo-rithm that we report here is an off-line algorithm,i.e.
the agent collects aset of dialogues and then de-cides on an optimal strategy as a result.
In contrast,it should be possible for the agent o learn on-line,during the course of a dialogue, if the performancefunction could be automatically calculated (or ap-proximated).
We are exploring these issues in on-going work.6 AcknowledgementsG.
Di Fabbrizio, D. Hindle, J. Hirschberg, C.Kamm, and D. Litman provided assistance with thisresearch or paper.ReferencesA.G.
Barto, S. J. Bradtke, and S. P. Singh.
1995.
Learn-ing to act using real-time dynamic programming.
Ar-tificial Intelligence Journal, 72(I-2): 81-138.R.
E. Bellman.
1957.
Dynamic Programming.
PrincetonUniversity Press, Princeton, N.J.A.
W. Biermann and Philip M. Long.
1996.
The compo-sition of messages inspeech-graphics interactive sys-tems.
In Proc.
of the 1996 International Symposiumon Spoken Dialogue, pp.
97-100.J.
Chu-Carroll and S. Carberry.
1995.
Response genera-tion in collaborative negotiation.
In Proc.
of the 33rdAnnual Meeting of the ACL, pp.
136-143.E R. Cohen.
1995.
Empirical Methods for Artificial In-telligence.
MIT Press, Boston.M.
Danieli and E. Gerbino.
1995.
Metrics for evaluatingdialogue strategies in a spoken language system.
InProc.
of the 1995 AAAI Spring Symposium on Empir-ical Methods in Discourse, pages 34-39.J.
C. Fromer.
1998.
Learning optimal discourse strate-gies in a spoken dialogue system.
Technical ReportForthcoming, MIT AI Lab M.S.
Thesis.E.
H. Hovy.
1993.
Automated iscourse generationusing discourse structure relations.
Artificial Intelli-gence Journal, 63:341-385.C.
Kamm, S. Narayanan, D. Dutton, and R. Ritenour.1997.
Evaluating spoken dialog systems for telecom-munication services.
In EUROSPEECH 97.R.
Keeney and H. Raiffa.
1976.
Decisions with MultipleObjectives: Preferences and Value Tradeoffs.
JohnWiley and Sons.E.
Levin and R. Pieraccini.
1997.
A stochastic modelof computer-human interaction for learning dialoguestrategies.
InEUROSPEECH 97.M.T.
Maybury.
1991.
Planning multi-media explana-tions using communicative acts.
In Proc.
of the NinthNational Conf.
on Artificial Intelligence, pages 61-66.K.
R. McKeown.
1985.
Discourse strategies for gen-erating natural language text.
Artificial Intelligence,27( 1): 1-42, September.J.
D. Moore and C. L. Paris.
1989.
Planning text foradvisory dialogues.
In Proc.
27th Annual Meeting oftheACL.S.
Russell and R Norvig.
1995.
Artificial Intelligence: AModern Approach.
Prentice Hall, N.J.R.
S. Sutton.
1991.
Planning by incremental dynamicprogramming.
In Proc.
Ninth Conf.
on MachineLearning, pages 353-357.
Morgan-Kaufmann.M.
A. Walker, D. Litman, C. Kamm, and A. Abella.1997.
PARADISE: A general framework for evalu-ating spoken dialogue agents.
In Proc.
of the 35th An-nual Meeting of the ACL, pp.
271-280.M.
Walker, J. Fromer, G. Di Fabbrizio, C. Mestel, and D.Hindle.
1998.
What can I say: Evaluating a spokenlanguage interface to email.
In Proc.
of the Conf.
onComputer Human Interaction ( CH198).M.
A. Walker.
1993.
InformationalRedundancy and Re-source Bounds in Dialogue.
Ph.D. thesis, Universityof Pennsylvania.C.
J. Watkins.
1989.
Models of Delayed ReinforcementLearning.
Ph.D. thesis, Cambridge University.1351
