Term-list Translationusing Mono-l ingual Word Co-occurrence Vectors*Genich i ro  Kiku iNTT In format ion  and Communicat ion  Systems Labs.1-1 Hikar inooka,  Yokosuka-Shi ,  Kanagawa,  Japane-mai l :  k iku i@is l .nt t .co.
jpAbst ractA term-list is a list of content words that charac-terize a consistent ext or a concept.
This paperpresents a new method for translating a term-list byusing a corpus in the target language.
The methodfirst retrieves alternative translations for each inputword from a bilingual dictionary.
It then determinesthe most 'coherent' combination of alternative trans-lations, where the coherence of a set of words isdefined as the proximity among multi-dimensionalvectors produced from the words on the basis ofco-occurrence statistics.
The method was appliedto term-lists extracted from newspaper articles andachieved 81% translation accuracy for ambiguouswords (i.e., words with multiple translations).1 In t roduct ionA list of content words, called a term-list, is widelyused as a compact representation f documents in in-formation retrieval and other document processing.Automatic translation of term-lists enables this pro-cessing to be cross-linguistic.
This paper presents anew method for translating term-lists by using co-occurrence statistics in the target language.Although there is little study on automatic trans-lation of term-lists, related studies are found in thearea of target word selection (for content words) inconventional full-text machine translation (MT).Approaches for target word selection can be clas-sifted into two types.
The first type, which has beenadopted in many commercial MT systems, is basedon hand assembled disambiguation rules, and/or dic-tionaries.
The problem with this approach is thatcreating these rules requires much cost and that theyare usually domain-dependent 1The second type, called the statistics-based ap-proach, learns disambiguation k owledge from largecorpora.
Brown et al presented an algorithm that* This  research was done when the author  was at Centerfor the Study of Language and Information(CSLI) ,  StanfordUniversity.1In fact, this is part ly shown by the fact that  many  MTsystems have subst i tutable  domain-dependent  (or "user" ) dic-t ionaries .relies on translation probabilities estimated fromlarge bilingual corpora (Brown et al, 1990)(Brownet al, 1991).
Dagan and Itai (1994) and Tanaka andIwasaki (1996) proposed algorithms for selecting tar-get words by using word co-occurrence statistics inthe target language corpora.
The latter algorithmsusing mono-lingual corpora are particularly impor-tant because, at present, we cannot always get asufficient amount of bilingual or parallel corpora.Our method is closely related to (Tanaka andIwasaki, 1996) from the viewpoint that they bothrely on mono-lingual corpora only and do not re-quire any syntactic analysis.
The difference is thatour method uses "coherence scores", which can cap-ture associative relations between two words whichdo not co-occur in the training corpus.This paper is organized as follows, Section 2 de-scribes the overall translation process.
Section 3presents a disambiguation algorithm, which is thecore part of our translation method.
Section 4 and5 give experimental results and discussion.2 Term-list TranslationOur term-list translation method consists of twosteps called Dictionary Lookup and Disambiguation.1.
Dictionary Lookup:For each word in the given term-list, all the al-ternative translations are retrieved from a bilin-gual dictionary.A translation candidate is defined as a combi-nation of one translation for each input word.For example, if the input term-list consists oftwo words, say wl and w~, and their transla-tion include wll for wl and w23 for w2, then(w11, w23) is a translation candidate.
If wl andw~ have two and three alternatives respectivelythen there are 6 possible translation candidates.2.
Disambiguation:In this step, all possible translation candidatesare ranked according to a measure that reflectsthe 'coherence' of each candidate.
The topranked candidate is the translated term-list.670In the following sections we concentrate on thedisambiguation step.3 Disambiguation AlgorithmThe underlying hypothesis of our disambiguationmethod is that a plausible combination of transla-tion alternatives will be semantically coherent.In order to find the most coherent combinationof words, we map words onto points in a multidi-mensional vector space where the 'proximity' of twovectors represents he level of coherence of the corre-sponding two words.
The coherence of n words canbe defined as the order of spatial 'concentration' ofthe vectors.The rest of this section formalizes this idea.3.1 Co-occur rence  Vector  Space: WORDSPACEWe employed a multi-dimensional vector space,called WORD SPACE (Schuetze, 1997) for defin-ing the coherence of words.
The starting point ofWORD SPACE is to represent a word with an n-dimensional vector whose i-th element is how manytimes the word wi occurs close to the word.
Forsimplicity, we consider w~ and wj to occur close incontext if and only if they appear within an m-worddistance (i.e., the words occur within a window ofm-word length), where m is a predetermined natu-ral number.Table 1 shows an artificial example of co-occurrence statistics.
The table shows that theword ginko (bank, where people deposit money) co-occurred with shikin (fund) 483 times and with hashi(bridge) 31 times.
Thus the co-occurrence vectorof ginko (money bank) contains 483 as its 89th ele-ment and 31 as its 468th element.
In short, a wordis mapped onto the row vector of the co-occurrencetable (matrix).Table 1: An example of co-occurrence statistics.col.
no.word(Eng.
)89 .
.
.
468... shikin ... hashi(fund) (bridge)ginko(bank:money)teibo(bank:fiver).
.
.
.
.
.
483 31120Using this word representation, we define theproximity, proz, of two vectors, ~, b, as the cosineof the angle between them, given as follows.= g)/( I  II D'I) (1)If two vectors have high proximity then the corre-sponding two words occur in similar context, and inour terms, are coherent.This simple definition, however, has problems,namely its high-dimensionality and sparseness ofdata.
In order to solve these problems, the originalco-occurrence vector space is converted into a con-densed low dimensional real-valued matrix by usingSVD (Singular Value Decomposition).
For example,a 20000-by-1000 matrix can be reduced to a 20000-by-100 matrix.
The resulting vector space is theWORD SPACE 23.2 Coherence  of  WordsWe define the coherence of words in terms of a geo-metric relationship between the corresponding wordvectors.As shown above, two vectors with high proximityare coherent with respect o their associative prop-erties.
We have extended this notion to n-words.That is, if a group of vectors are concentrated, thenthe corresponding words are defined to be coherent.Conversely, if vectors are scattered, the correspond-ing words are in-coherent.
In this paper, the concen-tration of vectors is measured by the average prox-imity from their centroid vector.Formally, for a given word set W, its coherencecoh(W) is defined as follows:1eoh(W) - I W I y~ prox(~(w),~(W)) (2)wEWe(w)  = (3)wEW\ [WI  = the number of  words inW (4)3.3  Disambiguat lon  ProcedureOur disambiguation procedure is simply selectingthe combination of translation alternatives that hasthe largest cob(W) defined above.
The current im-plementation exhaustively calculates the coherencescore for each combination of translation alterna-tives, then selects the combination with the highestscore.3.4  ExampleSuppose the given term-list consists of bank andriver.
Our method first retrieves translation alter-natives from the bilingual dictionary.
Let the dictio-nary contain following translations.2The WORD SPACE method is closely related to La-tent Semantic Indexing (LSI)(Deerwester t al., 1990), wheredocument-by-word matrices are processed by SVD instead ofword-by-word matrices.
The difference between these two isdiscussed in (Schuetze and Pedersen, 1997).671source translationsbank --~ ginko (bank:money),teibo(bank:river)interest --+ rishi (interest:money),kyoumi(interest :feeling)Combining these translation alternatives yieldsfour translation candidates:(ginko, risoku), (ginko, kyoumi),(teibo, risoku), (teibo, kyoumi).Then the coherence score is calculated for eachcandidate.Table 2 shows scores calculated with the co-occurrence data used in the translation experiment(see.
Section 4.4.2).
The combination of ginko(bank:money) and risoku(interest:money) has thehighest score.
This is consistent with our intuition.Table 2: An example of scoresrank candidate score (coh)1 (ginko, risoku) 0.9302 (teibo, kyoumi) 0.8973 (ginko, kyoumi) 0.8394 (teibo, risoku) 0.8214 Exper imentsWe conducted two types of experiments: re-translation experiments and translation experi-ments.
Each experiment includes comparisonagainst he baseline algorithm, which is a unigram-based translation algorithm.
This section presentsthe two types of experiments, plus the baseline al-gorithm, followed by experimental results.4.1 Two Types  of  Exper iments4.1.1 Trans la t ion  Exper imentIn the translation experiment, erm-lists in one lan-guage, e.g., English, were translated into anotherlanguage, e.g., in Japanese.
In this experiment, hu-mans judged the correctness of outputs.4.1.2 Re- t rans la t ion  Exper imentAlthough the translation experiment recreates realapplications, it requires human judgment 3.
Thuswe decided to conduct another type of experiment,called a re-translation experiment.
This experimenttranslates given term-lists (e.g., in English) into asecond language (e.g., Japanese) and maps themback onto the source language (e.g., in this case, En-glish).
Thus the correct ranslation of a term list, inthe most strict sense, is the original term-list itself.3 If a bi l ingual paral lel  corpus is available, then correspond-ing translat ions could be used for correct results.This experiment uses two bilingual dictionaries: aforward dictionary and a backward ictionary.In this experiment, a word in the given term-list(e.g.
in English) is first mapped to another lan-guage (e.g., Japanese) by using the forward dictio-nary.
Each translated word is then mapped backinto original language by referring to the backwarddictionary.
The union of the translations from thebackward ictionary are the translation alternativesto be disambiguated.4.2 Baseline Algor i thmThe baseline algorithm against which our methodwas compared employs unigram probabilities for dis-ambiguation.
For each word in the given term-list,this algorithm chooses the translation alternativewith the highest unigram probability in the targetlanguage.
Note that each word is translated inde-pendently.4.3 Exper imenta l  DataThe source and the target languages of the trans-lation experiments were English and Japanese re-spectively.
The re-translation experiments were con-ducted for English term-lists using Japanese as thesecond language.The Japanese-to-English dictionary was EDICT(Breen, 1995) andthe English-to-Japanese dictionary was an inversionof the Japanese-to-English dictionary.The co-occurrence statistics were extracted fromthe 1994 New York Times (420MB) for Englishand 1990 Nikkei Shinbun (Japanese newspaper)(150MB) for Japanese.
The domains of these textsrange from business to sports.
Note that 400 articleswere randomly separated from the former corpus asthe test set.The initial size of each co-occurrence matrix was20000-by-1000, where rows and columns correspondto the 20,000 and 1000 most frequent words in thecorpus 4.
Each initial matrix was then reduced by us-ing SVD into a matrix of 20000-by-100 using SVD-PACKC(Berry et al, 1993).Term-lists for the experiments were automaticallygenerated from texts, where a term-list of a docu-ment consists of the topmost n words ranked by theirtf-idf scores 5.
The relation between the length n ofterm-list and the disambiguation accuracy was alsotested.We prepared two test sets of term-lists: those ex-tracted from the 400 articles from the New YorkTimes mentioned above, and those extracted from4 Stopwords are ignored.5The tf-idf score of a word w in a text is tfwlog(N-~),where tfwis the occurrence of w in the text, N is the num-ber of documents in the collection, and Nw is the number ofdocuments contain ing w.672articles in Reuters(Reuters, 1997), called Test-NYT,and Test-REU, respectively.4.4 Resul ts4.4.1 re - t rans la t ion  exper imentThe proposed method was applied to several setsof term-lists of different length.
Results are shownin Table 3.
In this table and the following tables,"ambiguous" and "success" correspond to the totalnumber of ambiguous words, not term-lists, and thenumber of words that were successfully translated 6.The best results were obtained when the length ofterm-lists was 4 or 6.
In general, the longer a term-list becomes, the more information it has.
However,a long term-list ends to be less coherent (i.e., con-tain different opics).
As far as our experiments areconcerned, 4 or 6 was the point of compromise.Table 3: Result of Re-translation for Test-NYTlength success/ambiguous (rate)2 98/141 (69.5%)4 240/329 (72.9%)6 410/555 (73.8%)8 559/777 (71.9%)10 691/981 (70.4%)12 813/1165 (69.8%)Then we compared our method against he base-line algorithm that was trained on the same set ofarticles used to create the co-occurrence matrix forour algorithm (i.e., New York Times).
Both are ap-plied to term-lists of length 6 made from test-NYT.The results are shown in Table 4.
Although the ab-solute value of the success rate is not satisfactory,our method significantly outperforms the baselinealgorithm.Table 4: Result of Re-translation for Test-NYTMethod success/ambiguous (rate)baseline 236/555 (42.5%)proposed 410/555 (73.8%)We, then, applied the same method with the sameparameters (i.e., cooccurence and unigram data) toTest-REU.
As shown in Table 5, our method id bet-ter than the baseline algorithm although the successrate is lower than the previous result.Table 5: Result of re-translation for Test-REUMethod success/ambiguous (rate)baseline 162/565 (28.7%)proposed 351/565 (62.1%)6If 100 term-l ists were processed and each term-list con-tains 2 ambiguous words, then the "total" becomes 200.Table 6: Result of Translation for Test-NYTMethod success/ambiguous (rate)baseline 74/125 (72.6%)proposed 101/125 (80.8%)4.4.2 t rans la t ion  exper imentThe translation experiment from English toJapanese was carried out on Test-NYT.
The trainingcorpus for both proposed and baseline methods wasthe Nikkei corpus described above.
Outputs werecompared against the "correct data" which weremanually created by removing incorrect alternativesfrom all possible alternatives.
If all the translationalternatives in the bilingual dictionary were judgedto  be correct, then we counted this word as unam-biguous.The accuracy of our method and baseline algo-rithm are shown on Table6.The accuracy of our method was 80.8%, about 8points higher than that of the baseline method.
Thisshows our method is effective in improving trans-lation accuracy when syntactic information is notavailable.
In this experiment, 57% of input wordswere unambiguous.
Thus the success rates for entirewords were 91.8% (proposed) and 82.6% (baseline).4.5 Er ror  AnalysisThe following are two major failure reasons relevantto our method 7The first reason is that alternatives were seman-tically too similar to be discriminated.
For ex-ample, "share" has at least two Japanese trans-lations: "shea"(market share) and "kabu" (stock ).Both translations frequently occur in the same con-text in business articles, and moreover these twowords sometimes co-occur in the same text.
Thus,it is very difficult to discriminate them.
In this case,the task is difficult also for humans unless the origi-nal text is presented.The second reason is more complicated.
Sometranslation alternatives are polysemous in the targetlanguage.
If a polysemous word has a very generalmeaning that co-occurs with various words, then thisword is more likely to be chosen.
This is because thecorresponding vector has "average" value for eachdimension and, thus, has high proximity with thecentroid vector of multiple words.For example, alternative translations of "stock ~'includes two words: "kabu" (company share) and"dashz" (liquid used for food).
The second trans-lation "dashz" is also a conjugation form of theJapanese verb "dasff', which means "put out" and"start".
In this case, the word, "dash,", has a cer-7Other reasons came from errors in pre-processing includ-ing 1) ignoring compound words, 2) incorrect handl ing of cap-italized words etc.673tain amount of proximity because of the meaningirrelevant to the source word, e.g., stock.This problem was pointed out by (Dagan and Itai,1994) and they suggested two solutions 1) increas-ing the size of the (mono-lingual) training corporaor 2) using bilingual corpora.
Another possible solu-tion is to resolve semantic ambiguities of the trainingcorpora by using a mono-lingual disambiguation al-gorithm (e.g., (?))
before making the co-occurrencematrix.5 Related WorkDagan and Itai (1994) proposed a method for choos-ing target words using mono-lingual corpora.
It firstlocates pairs of words in dependency relations (e.g.,verb-object, modifier-noun, etc.
), then for each pair,it chooses the most plausible combination of trans-lation alternatives.
The plausibility of a word-pair ismeasured by its co-occurence probability estimatedfrom corpora in the target language.One major difference is that their method re-lies on co-occurrence statistics between tightly andlocally related (i.e., syntactically dependent) wordpairs, whereas ours relies on associative proper-ties of loosely and more globally related (i.e., co-occurring within a certain distance) word groups.Although the former statistics could provide moreaccurate information for disambiguation, it requireshuge amounts of data to cover inputs (the datasparseness problem).Another difference, which also relates to the datasparseness problem, is that their method uses "row"co-occurrence statistics, whereas ours uses statisticsconverted with SVD.
The converted matrix has theadvantage that it represents the co-occurrence r la-tionship between two words that share similar con-texts but do not co-occur in the same text s. SVDconversion may, however, weaken co-occurrence r -lations which actually exist in the corpus.Tanaka and Iwasaki (1996) also proposed amethod for choosing translations that solely relies onco-occurrence statistics in the target language.
Themain difference with our approach lies in the plau-sibility measure of a translation candidate.
Insteadof using a "coherence score", their method employsproximity, or inverse distance, between the two co-occurrence matrices: one from the corpus (in thetarget language) and the other from the translationcandidate.
The distance measure of two matricesgiven in the paper is the sum of the absolute dis-tance of each corresponding element.
This defini-tion seems to lead the measure to be insensitive tothe candidate when the co-occurrence matrix is filledwith large numbers.s"Second order co-occurrence".
See (Schuetze, 1997)6 Conc lud ing  RemarksIn this paper, we have presented a method for trans-lating term-lists using mono-lingual corpora.The proposed method is evaluated by translationand re-translation experiments and showed a trans-lation accuracy of 82% for term-lists extracted fromarticles ranging from business to sports.We are planning to apply the proposed method tocross-linguistic nformation retrieval (CLIR).
Sincethe method does not rely on syntactic analysis, itis applicable to translating users' queries as well astranslating term-lists extracted from documents.A future issue is further evaluation of the pro-posed method using more data and various criteriaincluding overall performance of an application sys-tem (e.g., CLIR).AcknowledgmentI am grateful to members of the Infomap project atCSLI, Stanford for their kind support and discus-sions.
In particular I would like to thank StanleyPeters and Raymond Flournoy.Re ferencesM.W.
Berry, T. Do, G. O'Brien, V. Krishna,and S. Varadhan.
1993.
SVDPACKC USER'SGUIDE.
Tech.
Rep. CS-93-194, University ofTen-nessee, Knoxville, TN,.J.W.
Breen.
1995.
EDICT, Freeware, Japanese.to-English Dictionary.P.
Brown, J. Cocke, V. Della Pietra, F. Jelinek, R.L.Mercer, and P. C. Roosin.
1990.
A statisticalapproach to language translation.
ComputationalLinguistics, 16(2).P.
Brown, V. Della Pietra, and R.L.
Mercer.
1991.Word sense disambiguation using statisical meth-ods.
In Proceedings of ACL-91.I.
Dagan and A. Itai.
1994.
Word sense disambigua-tion using a second language monolingual corpus.Computational Linguistics.S.
Deerwester, S.T.
Dumais, and R. Harshman.1990.
Indexing by latent semantic analysis.
Jour-nal of American Society for Information Science.Reuters.
1997.
Reuters-21578, Distribution 1.0.available at http://www.research.att.com/~lewis.H.
Schuetze and Jan O. Pedersen.
1997.
Acooccurrence-based thesaurus and two applica-tions to information retrieval.
Information Pro-cessing ~ Management.H.
Schuetze.
1997.
Ambiguity Resolution in Lan-guage Learning.
CSLI.K.
Tanaka and H. Iwasaki.
1996.
Extraction of lexi-cal translations from non-aligned corpora.
In Pro-ceedings of COLING-96.674
