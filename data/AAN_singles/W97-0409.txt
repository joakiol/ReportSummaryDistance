Interactive Speech Translationin the DIPLOMAT ProjectRobert Frederking, Alexander Rudnicky, and Christopher Hogan{re f ,  air, chogan}?cs, cmu.
eduLanguage Technologies Inst i tuteCarnegie Mellon UniversityP i t tsburgh,  PA 15213Abst rac tThe DIPLOMAT rapid-deploymentspeech translation system is intended toallow naive users to communicate acrossa language barrier, without strong do-main restrictions, despite the error-prone nature of current speech andtranslation technologies.
Achieving thisambitious goal depends in large parton allowing the users to interactivelycorrect recognition and translation er-rors.
We briefly present the Multi-Engine Machine Translation (MEMT)architecture, describing how it is well-suited for such an application.
We thendescribe our incorporation of interac-tive error correction throughout the sys-tem design.
We have already developeda working bidirectional Serbo-CroatianEnglish system, and are currently de-veloping Haitian-Creole ~ English andKorean ~ English versions.1 In t roduct ionThe DIPLOMAT project is designed to explorethe feasibility of creating rapid-deployment, wear-able bi-directional speech translation systems.
By"rapid-deployment", we mean being able to de-velop an MT system that performs initial trans-lations at a useful level of quality between a newlanguage and English within a matter of days orweeks, with continual, graceful improvement toa good level of quality over a period of months.The speech understanding component used is theSPHINX II HMM-based speaker-independent con-tinuous peech recognition system (Huang el al.,1992; Ravishankar, 1996), with techniques forrapidly developing acoustic and language modelsfor new languages (Rudnicky, 1995).
The ma-chine translation (MT) technology is the Multi-Engine Machine Translation (MEMT) architec-ture (Frederking and Nirenburg, 1994), describedfurther below.
The speech synthes!s component isa newly-developed concatenative system (Lenzo,1997) based on variable-sized compositional units.This use of subword concatenation is especiallyimportant, since it is the only currently avail-able method for rapidly bringing up synthesis fora new language.
DIPLOMAT thus involves re-search in MT, speech understanding and synthe-sis, interface design, as well as wearable computersystems.
While beginning our investigations intonew semi-automatic techniques for both speechand MT knowledge-base d velopment, we have al-ready produced an initial bidirectional system forSerbo-Croatian ~ English speech translation inless than a month, and are currently developingHaitian-Creole ~ English and Korean ~ Englishsystems.A major concern in the design of theDIPLOMAT system has been to cope with theerror-prone nature of both current speech under-standing and MT technology, to produce an ap-plication that is usable by non-translators with asmall amount of training.
We attempt o achievethis primarily through user interaction: whereverfeasible, the user is presented with intermediateresults, and allowed to correct them.
In this pa-per, we will briefly describe the machine trans-lation architecture used in DIPLOMAT (showinghow it is well-suited for interactive user correc-tion), describe our approach to rapid-deploymentspeech recognition and then discuss our approachto interactive user correction of errors in the over-all system.2 Mu l t i -Eng ine  Mach ineTrans la t ionDifferent MT technologies exhibit differentstrengths and weaknesses.
Technologies uch asKnowledge-Based MT (KBMT) can provide high-quality, fully-automated translations in narrow,well-defined omains (Mitamura el al., 1991; Far-well and Wilks, 1991).
Other technologies such aslexical-transfer MT (Nirenburg et al, 1995; Fred-erking and Brown, 1996; MacDonald, 1963), andExample-Based MT (EBMT) (Brown, 1996; Na-61gao, 1984: Sato and Nagao, 1990) provide lower-quality general-purpose translations, unless theyare incorporated into human-assisted MT systems(Frederking et al, 1993; Melby, 1983), but can beused in non-domain-restricted ranslation applica-tions.
Moreover, these technologies differ not justin the quality of their translations, and level ofdomain-dependence, but also along other dimen-sions, such as types of errors they make, requireddevelopment time, cost of development, and abil-ity to easily make use of any available on-linecorpora, such as electronic dictionaries or onlinebilingual parallel texts.The Multi-Engine Machine Translation(MEMT) architecture (Frederking and Nirenburg,1994) makes it possible to exploit the differencesbetween MT technologies.
As shown in Figure 1,MEMT feeds an input text to several MT enginesin parallel, with each engine employing a differ-ent MT technology 1.
Each engine attempts totranslate the entire input text, segmenting eachsentence in whatever manner is most appropri-ate for its technology, and putting the resultingtranslated output segments into a shared chartdata structure (Kay, 1967; Winograd, 1983) af-ter giving each segment a score indicating the en-gine's internal assessment of the quality of theoutput segment.
These output (target language)segments are indexed in the chart based on thepositions of the corresponding input (source lan-guage) segments.
Thus the chart contains multi-ple, possibly overlapping, alternative translations.Since the scores produced by the engines are esti-mates of variable accuracy, we use statistical lan-guage modelling techniques adapted from speechrecognition research to select the best overall setof outputs (Brown and Frederking, 1995; Frederk-ing, 1994).
These selection techniques attempt oproduce the best overall result, taking the proba-bility of transitions between segments into accountas well as modifying the quality scores of individ-ual segments.Differences in the development times and costsof different .technologies can be exploited to en-able MT systems to be rapidly deployed for newlanguages (Frederking and Brown, 1996).
If par-allel corpora are available for a new language pair,the EBMT engine can provide translations for anew language in a matter of hours.
Knowledge-bases for lexical-transfer MT can be developed ina matter of days or weeks; those for structural-transfer MT may take months or years.
Thehigher-quality, higher-investment KBMT-style en-gine typically requires over a year to bring on-line.
The use of the MEMT architecture allowsthe improvement of initial MT engines and the1 Morphological analysis, part-of-speech tagging,and possibly other text enhancements can be sharedby the engines.addition of new engines to occur within an un-changing framework.
The only change that theuser sees is that the quality of translation im-proves over time.
This allows interfaces to re-main stable, preventing any need for retrainingof users, or redesign of inter-operating software.The EBMT and Lexical-Transfer-based MT trans-lation engines used in DIPLOMAT are describedelsewhere (Frederking and Brown, 1996).For the purposes of this paper, the most impor-tant aspects of the MEMT architecture are:?
the initially deployed versions are quite error-prone, although generally a correct translationis among the available choices, and?
the unchosen alternative translations are stillavailable in the chart structure after scoring bythe target language model.3 Speech recognition for novellanguagesContemporary speech recognition systems derivetheir power from corpus-based statistical model-ing, both at the acoustic and language levels.
Sta-tistical modeling, of course, presupposes that suf-ficiently large corpora are available for training.It is in the nature of the DIPLOMAT system thatsuch corpora, particularly acoustic ones, are notimmediately available for processing.
As for theMT component, he emphasis is on rapidly acquir-ing an initial capability in a novel language, thenbeing able to incrementally improve performanceas more data and time are available.
We haveadopted for the speech component a combinationof approaches which, although they rely on partic-ipation by native informants, also make extensiveuse of pre-existing acoustic and text resources.Building a speech recognition system for a tar-get domain or language requires models at threelevels (assuming that a basic processing infras-tructure for training and decoding is already inplace): acoustic, lexical and language.We have explored two strategies for acousticmodeling.
Assimilation makes use of existingacoustic models from a language that has a largephonetic overlap with the target language.
Thisallows us to rapidly put a recognition capabilityin place and was the strategy used for our Serbo-Croatian ~ English system.
We were able toachieve good recognition performance for vocabu-laries of up to 733 words using this technique.
Ofcourse, such overlaps cannot be relied upon andin any case will not produce recognition perfor-mance that approaches that possible with appro-priate training.
Nevertheless it does suggest hatuseful recognition performance for a large set oflanguages can be achieved given a carefully chosenset of core languages that can serve as a source of62Source TargetLanguage Language0 ?
# _ , Morphological  !_.d |Ana lyzerTransfer-Based MTUser  In ter faceExample-Based MT Stat i s t i ca l  ModellerKnowledge-Based "L............................__I",'"'~ Expanmon slot f .....Figure 1: S t ructure  of  MEMT architectureacoustic models for a cluster of phonetically simi-lar languages.The selective collection approach presupposesa preparation interval prior to deployment andcan be a follow-on to a system based on assim-ilation.
This is being developed in the contextof our Haitian-Creole and Korean systems.
Thegoal is to carry out a limited acoustic data collec-tion effort using materials that have been explic-itly constructed to yield a rich phonetic samplingfor the target language.
We do this by first com-puting phonetic statistics for the language usingavailable text materials, then designing a record-ing script that exhaustively samples all diphonesobserved in the available text sample.
Such scriptsrun from several hundred to around a thousandutterances for the languages we have examined.While the effectiveness of this approach dependson the quality (and quantity) of the text samplethat can be obtained, we believe it produces ap-propriate data for our modeling purposes.Lexical modeling is based on creating pronunci-ations from orthography and involves a variety oftechniques familiar from speech synthesis, includ-ing letter-to-sound rules, phonological rules andexception lists.
The goal of our lexical modelingapproach is to create an acceptable-quality pro-nouncing dictionary that can be variously usedfor acoustic training, decoding and synthesis.
Wework with an informant o map out the pronun-ciation system for the target language and makeuse of supporting published information (thoughwe have found such to be misleading on occasion).System vocabulary is derived from the text mate-rials assembled for acoustic modeling, as well asscenarios from the target domain (for example,interviews focussed on mine field mapping or in-telligence screening).Finally, due to the goals of our project, lan-guage modeling is necessarily based on small cor-pora.
We make use of materials derived from do-main scenarios and from general sources such asnewspapers ( canned and OCRed), text in the tar-get language available on the Internet and trans-lations of select documents.
Due to the smallamounts of readily available data (on the order of50k words for the languages we have worked with),standard language modeling tools are difficult touse, as they presuppose the availability of cor-pora that are several orders of magnitude larger.Nevertheless we have been successful in creatingstandard backoff trigram models from very smallcorpora.
Our technique involves the use of highdiscounts and appears to provide useful constraintwithout corresponding fragility in the face of novelmaterial.63In combination, these techniques allow us tocreate working recognition systems in very shortperiods of time and provide a path for evolution-ary improvement of recognition capability.
Theyclearly are not of the quality that would beexpected if conventional procedures were used,but nevertheless are sufficient for providing cross-language communication capability in limited-domain speech translation.4 User Interface DesignAs indicated above, our approach to coping witherror-prone speech translation is to allow user cor-rection wherever feasible.
While we would like asmuch user interaction as possible, it is also im-portant not to overwhelm the user with eitherinformation or decisions.
This requires a carefulbalance, which we are trying to achieve throughearly user testing.
We have carried out initial test-ing using local naive subjects (e.g., drama majorsand construction workers), and intend to test withactual end users once specific ones are identified.The primary potential use for DIPLOMATidentified so far is to allow English-speaking sol-diers on peace-keeping missions to interview localresidents.
While one could conceivably train theinterviewer to use a restricted vocabulary, the in-terviewee's responses are much more difficult tocontrol or predict.
An initial system has beendeveloped to run on a pair of laptop comput-ers, with each speaker using a graphical user in-terface (GUI) on the laptop's screen (see Figure2).
Feedback from initial demonstrations made itclear that, while we could expect the interviewerto have roughly eight hours of training, we neededto design the system to work with a totally naiveinterviewee, who had never used a computer be-fore.
We responded to this requirement by de-veloping an asymmetric interface, where any nec-essary complex operations were moved to the in-terviewer's ide.
The interviewee's GUI is nowextremely simple, and a touch screen has beenadded, so that the interviewee is not required totype or use the pointer.
In addition, the inter-viewer's GUI controls the state of the interviewee'sGUI.
The speech recognition system continuouslylistens, thus the participants do not need to phys-ically indicate their intention of speaking.A typical exchange consists of recognizingthe interviewer's poken utterance, translatingit to the target language, backtranslating it toEnglish 2, then displaying and synthesizing the(possibly corrected) translation.
The intervie-wee's response is recognized, translated to En-2We realize that backtranslation is also an error-prone process, but it at least provides ome evidenceas to whether the translation was correct o someonewho does not speak the target language at all.glish, and backtranslated.
The (possibly cor-rected) backtranslation is then shown to the inter-viewee for confirmation.
The interviewer eceivesa graphic indication of whether the backtransla-tion was accepted or not.
(The actual communi-cation process is quite flexible, but this is a normalscenario.
)In order to achieve such communication, theusers currently can interact with DIPLOMAT inthe following ways:?
Speech displayed as text: After any speechrecognition step, the best overall hypothesis isdisplayed as text on the screen.
The user canhighlight an incorrect portion using the touch-screen, and respeak or type it.?
Conf i rmat ion requests: After any speechrecognition or machine translation step, the useris offered an accept/reject button to indicatewhether this is "what they said".
For MT, back-translations provide the user with an ability tojudge whether they were interpreted correctly.?
Interact ive chart  editing: As mentionedabove, the MEMT technology produces as out-put a chart structure, similar to the word hy-pothesis lattices in speech systems.
After anyMT step, the interviewer is able to edit thebest overall hypothesis for either the forward orbackward translation using a popup-menu-basededitor, as in our earlier Pangloss text MT system(Frederking et al, 1993).
The editor allows theinterviewer to easily view and select alternativetranslations for any segment of the translation.Editing the forward translation, causes an auto-matic reworking of the backtranslation.
Editingthe backtranslation allows the interviewer to rec-ognize correct forward translations despite rrorsin the backtranslation; if the backtranslation canbe edited into correctness, the forward transla-tion was probably correct.Since a major goal of DIPLOMAT is rapid-deployment to new languages, the GUI uses theUNICODE multilingual character encoding stan-dard.
This will not always suffice, however; a ma-jor challenge for handling Haitian-Creole is that55% of the Haitian population is illiterate.
Wewill have to develop an all-speech version of theinterviewee-side interface.
As we have done withprevious interface designs, we will carry out usertests early in its development to ascertain whetherour intuitions on the usability of this version arecorrect.5 ConclusionWe have presented here the DIPLOMAT speechtranslation system, with particular emphasis onthe user interaction mechanisms employed to copewith error-prone speech and MT processes.
Weexpect that, after additional tuning based on fur-ther informal user studies, an interviewer witheight hours of training should be able to use the64Figure 2: Screen Shot of User Interfaces: Interviewer (left) and Interviewee (right)DIPLOMAT system to successfully interview sub-jects with no training or previous computer expe-rience.
We hope to have actual user trials of eitherthe Serbo-Croatian or the Haitian-Creole systemin the near future, possibly this summer.ReferencesRalf Brown.
1996.
Example-Based MachineTranslation in the Pangloss System.
In Pro-ceedings of the 16th International Conferenceon Computational Linguistics (COLING-96).Ralf Brown and Robert Frederking.
1995.
Apply-ing Statistical English Language Modeling toSymbolic Machine Translation.
In Proceedingsof the Sixth International Conference on The-oretical and Methodological Issues in MachineTranslation (TMI-95), pages 221-239.David Farwell and Yorick Wilks.
1991.
Ultra: AMulti-lingual Machine Translator.
In Proceed-ings of Machine Translation Summit IlI, Wash-ington, DC, July.Robert Frederking.
1994.
Statistical Lan-guage Models for Symbolic MT.
Presented atthe Language Engineering on the InformationHighway Workshop, Santorini, Greece, Septem-ber.
Refereed.Robert Frederking, D. Grannes, P. Cousseau, andS.
Nirenburg.
1993.
An MAT Tool and Its Ef-fectiveness.
In Proceedings of the DARPA Hu-man Language Technology Workshop, Prince-ton, NJ.Robert Frederking and Ralf Brown.
1996.
ThePangloss-Lite Machine Translation System.
InProceedings of the Conference of the Associa-tion for Machine Translation in the Americas(AMTA).Robert Frederking and Sergei Nirenburg.
1994.Three Heads are Better than One.
In Proceed-ings of the fourth Conference on Applied Natu-ral Language Processing (ANLP-94), Stuttgart,Germany.Xuedong Huang, Fileno Alleva, Hsiao-Wuen Hon,Mei-Yuh Hwang, Ronald Rosenfeld.
1992.
TheSPHINX-II Speech Recognition System: AnOverview.
Carnegie Mellon University Com-puter Science Technical Report CMU-CS-92-112.Martin Kay.
1967.
Experiments with a powerfulparser.
In Proceedings of the 2nd InternationalCOLING, August.Kevin Lenzo.
1997.
Personal Communication.R.
R. MacDonald.
1963.
General report 1952-1963 (Georgetown University Occasional Pa-pers in Machine Translation, no.
30), Washing-ton, DC.A.
K. Melby.
1983.
Computer-assisted translationsystems: the standard esign and a multi-leveldesign.
Conference on Applied Natural Lan-guage Processing, Santa Monica, February.Teruko Mitamura, Eric Nyberg, Jaime Carbonell.1991.
Interlingua Translation System for Multi-Lingual Document Production.
In Proceedingsof Machine Translation Summit III, Washing-ton, DC, July.M.
Nagao.
1984.
A framework of a mechani-cal translation between Japanese and Englishby analogy principle.
In: A. Elithorn andR.
Banerji (eds.)
Artificial and Human Intel-ligence.
NATO Publications.Sergei Nirenburg.
1995.
The Pangloss MarkIII Machine Translation System.
Joint Tech-nical Report, Computing Research Laboratory(New Mexico State University), Center for Ma-chine Translation (Carnegie Mellon University),65Information Sciences Institute (University ofSouthern (~alifornia).
Issued as CMU technicalreport CMU-CMT-95-145.Mosur Ravishankar.
1996.
Efficient Algorithmsfor Speech Recognition.
Ph.D. Thesis.
CarnegieMellon University.Alex Rudnieky.
1995.
Language modeling withlimited domain data.
In Proceedings of theARPA Workshop on Spoken Language Technol-ogy.
San Mateo: Morgan Kaufmann, 66-69.S.
Sato and M. Nagao.
1990.
Towards memorybased translation.
In Proceedings of COLING-90, Helsinki, Finland.Terry Winograd.
1983.
Language as a CognitiveProcess.
Volume 1: Syntax.
Addison-Wesley.66
