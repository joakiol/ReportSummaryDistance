Proceedings of the 6th Workshop on Statistical Machine Translation, pages 440?446,Edinburgh, Scotland, UK, July 30?31, 2011. c?2011 Association for Computational LinguisticsThe LIGA (LIG/LIA) Machine Translation System for WMT 2011Marion Potet1, Raphae?l Rubino2, Benjamin Lecouteux1, Ste?phane Huet2,Herve?
Blanchon1, Laurent Besacier1 and Fabrice Lefe`vre21UJF-Grenoble1, UPMF-Grenoble2LIG UMR 5217Grenoble, F-38041, FranceFirstName.LastName@imag.fr2Universite?
d?AvignonLIA-CERIAvignon, F-84911, FranceFirstName.LastName@univ-avignon.frAbstractWe describe our system for the news com-mentary translation task of WMT 2011.
Thesubmitted run for the French-English directionis a combination of two MOSES-based sys-tems developed at LIG and LIA laboratories.We report experiments to improve over thestandard phrase-based model using statisticalpost-edition, information retrieval methods tosubsample out-of-domain parallel corpora andROVER to combine n-best list of hypothesesoutput by different systems.1 IntroductionThis year, LIG and LIA have combined their effortsto produce a joint submission to WMT 2011 for theFrench-English translation task.
Each group startedby developing its own solution whilst sharing re-sources (corpora as provided by the organizers butalso aligned data etc) and acquired knowledge (cur-rent parameters, effect of the size of n-grams, etc.
)with the other.
Both LIG and LIA systems are stan-dard phrase-based translation systems based on theMOSES toolkit with appropriate carefully-tuned se-tups.
The final LIGA submission is a combinationof the two systems.We summarize in Section 2 the resources usedand the main characteristics of the systems.
Sec-tions 3 and 4 describe the specificities and reportexperiments of resp.
the LIG and the LIA system.Section 5 presents the combination of n-best listshypotheses generated by both systems.
Finally, weconclude in Section 6.2 System overview2.1 Used dataGlobally, our system1 was built using all the Frenchand English data supplied for the workshop?s sharedtranslation task, apart from the Gigaword monolin-gual corpora released by the LDC.
Table 1 sums upthe used data and introduces designations that wefollow in the remainder of this paper to refer to cor-pora.
Four corpora were used to build translationmodels: news-c, euro, UN and giga, while threeothers are employed to train monolingual languagemodels (LMs).
Three bilingual corpora were de-voted to model tuning: test09 was used for the de-velopment of the two seed systems (LIG and LIA),whereas test08 and testcomb08 were used to tune theweights for system combination.
test10 was finallyput aside to compare internally our methods.2.2 LIG and LIA system characteristicsBoth LIG and LIA systems are phrase-based trans-lation models.
All the data were first tokenized withthe tokenizer provided for the workshop.
Kneser-Ney discounted LMs were built from monolingualcorpora using the SRILM toolkit (Stolcke, 2002),while bilingual corpora were aligned at the word-level using GIZA++ (Och and Ney, 2003) or itsmulti-threaded version MGIZA++ (Gao and Vogel,2008) for the large corpora UN and giga.
Phrasetable and lexicalized reordering models were builtwith MOSES (Koehn et al, 2007).
Finally, 14 fea-tures were used in the phrase-based models:1When not specified otherwise ?our?
system refers to theLIGA system.440CORPORA DESIGNATION SIZE (SENTENCES)English-French Bilingual trainingNews Commentary v6 news-c 116 kEuroparl v6 euro 1.8 MUnited Nation corpus UN 12 M109 corpus giga 23 MEnglish Monolingual trainingNews Commentary v6 mono-news-c 181 kShuffled News Crawl corpus (from 2007 to 2011) news-s 25 MEuroparl v6 mono-euro 1.8 MDevelopmentnewstest2008 test08 2,051newssyscomb2009 testcomb09 502newstest2009 test09 2,525Testnewstest2010 test10 2,489Table 1: Used corpora?
5 translation model scores,?
1 distance-based reordering score,?
6 lexicalized reordering score,?
1 LM score and?
1 word penalty score.The score weights were optimized on the test09 cor-pus according to the BLEU score with the MERTmethod (Och, 2003).
The experiments led specifi-cally with either LIG or LIA system are respectivelydescribed in Sections 3 and 4.
Unless otherwiseindicated, all the evaluations were performed usingcase-insensitive BLEU and were computed with themteval-v13a.pl script provided by NIST.
Ta-ble 2 summarizes the differences between the finalconfiguration of the systems.3 The LIG machine translation systemLIG participated for the second time to the WMTshared news translation task for the French-Englishlanguage pair.3.1 Pre-processingTraining data were first lowercased with the PERLscript provided for the campaign.
They were alsoprocessed in order to normalize a special Frenchform (named euphonious ?t?)
as described in (Potetet al, 2010).The baseline system was built using a 4-gram LMtrained on the monolingual corpora provided lastyear and translation models trained on news-c andeuro (Table 3, System 1).
A significant improve-ment in terms of BLEU is obtained when taking intoaccount a third corpus, UN, to build translation mod-els (System 2).
The next section describes the LMsthat were trained using the monolingual data pro-vided this year.3.2 Language model trainingTarget LMs are standard 4-gram models trainedon the provided monolingual corpus (mono-news-c,mono-euro and news-s).
We decided to test two dif-ferent n-gram cut-off settings.
The fist set has lowcut-offs: 1-2-3-3 (respectively for 1-gram, 2-gram,3-gram and 4-gram counts), whereas the second one(LM2) is more aggressive: 1-5-7-7.
Experiment re-sults (Table 3, Systems 3 and 4) show that resortingto LM2 leads to an improvement of BLEU with re-spect to LM1.
LM2 was therefore used in the sub-sequent experiments.441FEATURES LIG SYSTEM LIA SYSTEMPre-processing Text lowercased Text truecasedNormalization of French euphonious?t?Reaccentuation of French words start-ing with a capital letterLM Training on mono-news-c, news-s andmono-euroTraining on mono-news-c and news-s4-gram models 5-gram modelsTranslation modelTraining on news-c, euro and UN Training on 10 M sentence pairs se-lected in news-c, euro, UN and gigaPhrase table filteringUse of -monotone-at-punctuation op-tionTable 2: Distinct features between final configurations retained for the LIG and LIA systems3.3 Translation model trainingTranslation models were trained from the parallelcorpora news-c, euro and UN.
Data were alignedat the word-level and then used to build standardphrase-based translation models.
We filtered the ob-tained phrase table using the method described in(Johnson et al, 2007).
Since this technique drasti-cally reduces the size of the phrase table, while notdegrading (and even slightly improving) the resultson the development and test corpora (System 6), wedecided to employ filtered phrase tables in the finalconfiguration of the LIG system.3.4 TuningFor decoding, the system uses a log-linear com-bination of translation model scores with the LMlog-probability.
We prevent phrase reordering overpunctuation using the MOSES option -monotone-at-punctuation.
As the system can be beforehand tunedby adjusting the log-linear combination weights ona development corpus, we used the MERT method(System 5).
Optimizing weights according to BLEUleads to an improvement with respect to the sys-tem with MOSES default value weights (System 5vs System 4).3.5 Post-processingWe also investigated the interest of a statisticalpost-editor (SPE) to improve translation hypotheses.About 9,000 sentences extracted from the news do-main test corpora of the 2007?2009 WMT transla-tion tasks were automatically translated by a sys-tem very similar to that described in (Potet et al,2010), then manually post-edited.
Manual correc-tions of translations were performed by means of thecrowd-sourcing platform AMAZON MECHANICALTURK2 ($0.15/sent.).
These collected data makea parallel corpus whose source part is MT outputand target part is the human post-edited version ofMT output.
This are used to train a phrase-basedSMT (with Moses without the tuning step) that au-tomatically post-edit the MT output.
That aims atlearning how to correct translation hypotheses.
Sys-tem 7 obtained when post-processing MT 1-best out-put shows a slight improvement.
However, SPE wasnot used in the final LIG system since we lackedtime to apply SPE on the N-best hypotheses for thedevelopment and test corpora (the N-best being nec-essary for combination of LIG and LIA systems).Ths LIGA submission is thus a constrained one.3.6 RecasingWe trained a phrase-based recaser model on thenews-s corpus using the provided MOSES scriptsand applied it to uppercase translation outputs.
Acommon and expected loss of around 1.5 case-sensitive BLEU points was observed on the test cor-pus (news10) after applying this recaser (System 7)with respect to the score case-insensitive BLEU pre-viously measured.2http://www.mturk.com/mturk/welcome442?
SYSTEM DESCRIPTION BLEU SCOREtest09 test101 Training: euro+news-c 24.89 26.012 Training: euro+news-c+UN 25.44 26.433 2 + LM1 24.81 27.194 2 + LM2 25.37 27.255 4 + MERT on test09 26.83 27.536 5 + phrase-table filtering 27.09 27.647 6 + SPE 27.53 27.748 6 + recaser 24.95 26.07Table 3: Incremental improvement of the LIG system interms of case-insensitive BLEU (%), except for line 8where case-sensitive BLEU (%) are reported4 The LIA machine translation systemThis section describes the particularities of the MTsystem which was built at the LIA for its first partic-ipation to WMT.4.1 System descriptionThe available corpora were pre-processed usingan in-house script that normalizes quotes, dashes,spaces and ligatures.
We also reaccentuated Frenchwords starting with a capital letter.
We significantlycleaned up the crawled parallel giga corpus, keeping19.3 M of the original 22.5 M sentence pairs.
For ex-ample, sentence pairs with numerous numbers, non-alphanumeric characters or words starting with cap-ital letters were removed.
The whole training ma-terial is truecased, meaning that the words occur-ing after a strong punctuation mark were lowercasedwhen they belonged to a dictionary of common all-lowercased forms; the others were left unchanged.The training of a 5-gram English LM was re-strained to the news corpora mono-news-c and news-s that we consider large enough to ignore other data.In order to reduce the size of the LM, we first limitedthe vocabulary of our model to a 1 M word vocabu-lary taking the most frequent words in the news cor-pora.
We also resorted to cut-offs to discard infre-quent n-grams (2-2-3-5 thresholds on 2- to 5-gramcounts) and uses the SRILM option prune, whichallowed us to train the LM on large data with 32 GbRAM.Our translation models are phrase-based models(PBMs) built with MOSES with the following non-default settings:?
maximum sentence length of 80 words,?
limit on the number of phrase translationsloaded for each phrase fixed to 30.Weights of LM, phrase table and lexicalized re-ordering model scores were optimized on the devel-opment corpus thanks to the MERT algorithm.Besides the size of used data, we experimentedwith two advanced features made available forMOSES.
Firstly, we filtered phrase tables using thedefault setting -l a+e -n 30.
This dramaticallyreduced phrase tables by dividing their size by afactor of 5 but did not improve our best configu-ration from the BLEU score perspective (Table 4,line 1); the method was therefore not kept in theLIA system.
Secondly, we introduced reorderingconstraints in order to consider quoted material asa block.
This method is particularly useful when ci-tations included in sentences have to be translated.Two configurations were tested: zone markups in-clusion around quotes and wall markups inclusionwithin zone markups.
However, the measured gainswere finally too marginal to include the method inthe final system.4.2 Parallel corpus subsamplingAs the only news parallel corpus provided for theworkshop contains 116 k sentence pairs, we mustresort to parallel out-of-domain corpora in order tobuild reliable translation models.
Information re-trieval (IR) methods have been used in the past tosubsample parallel corpora.
For example, Hilde-brand et al (2005) used sentences belonging to thedevelopment and test corpora as queries to select thek most similar source sentences in an indexed paral-lel corpus.
The retrieved sentence pairs constituteda training corpus for the translation models.The RALI submission for WMT10 proposed asimilar approach that builds queries from the mono-lingual news corpus in order to select sentence pairsstylistically close to the news domain (Huet et al,2010).
This method has the major interest that itdoes not require to build a new training parallelcorpus for each news data set to translate.
Fol-lowing the best configuration tested in (Huet et al,4432010), we index the three out-of-domain corpora us-ing LEMUR3, and build queries from English news-ssentences where stop words are removed.
The 10 topsentence pairs retrieved per query are selected andadded to the new training corpus if they are not re-dundant with a sentence pair already collected.
Theprocess is repeated until the training parallel cor-pus reaches a threshold over the number of retrievedpairs.Table 4 reports BLEU scores obtained with theLIA system using the in-domain corpus news-c andvarious amounts of out-of-domain data.
MERT wasre-run for each set of training data.
The first fourlines display results obtained with the same num-ber of sentence pairs, which corresponds to thesize of news-c appended to euro.
The experimentsshow that using euro instead of the first sentences ofUN and giga significantly improves BLEU scores,which indicates the better adequacy of euro with re-spect to the test10 corpus.
The use of the IR methodto select sentences from euro, UN and giga leads toa similar BLEU score to the one obtained with euro.The increase of the collected pairs up to 3 M pairsgenerates a significant improvement of 0.9 BLEUpoint.
A further rise of the amount of collectedpairs does not introduce a major gain since retriev-ing 10 M sentence pairs only augments BLEU from29.1 to 29.3.
This last configuration which leads tothe best BLEU was used to build the final LIA sys-tem.
Let us note that 2 M, 3 M and 15 M querieswere required to respectively obtain 3 M, 5 M and10 M sentence pairs because of the removal of re-dundant sentences in the increased corpus.For a matter of comparison, a system was alsobuilt taking into account all the training material,i.e.
37 M sentence pairs4.
This last system is out-performed by our best system built with IR and hasfinally close performance to the one obtained withnews-c+euro relatively to the quantity of used data.5 The system combinationSystem combination is based on the 500-best out-puts generated by the LIA and the LIG systems.3www.lemurproject.org4For this experiment, the data were split into three partsto build independent alignment models: news-c+euro, UN andgiga, and they were joined afterwards to build translation mod-els.USED PARALLEL CORPORA FILTERINGwithout withnews-c + euro (1.77 M) 28.1 28.0news-c + 1.77 M of UN 27.2 -news-c + 1.77 M of giga 27.1 -news-c + 1.77 M with IR 28.2 -news-c + 3 M with IR 29.1 29.0news-c + 5 M with IR 28.8 -news-c + 10 M with IR 29.3 29.2All data 28.9 29.0Table 4: BLEU (%) on test10 measured with the LIAsystem using different training parallel corporaThey both used the MOSES option distinct, en-suring that the hypotheses produced for a given sen-tence are different inside an N-best list.
Each N-bestlist is associated with a set of 14 scores and com-bined in several steps.The first step takes as input lowercased 500-bestlists, since preliminary experiments have shown abetter behavior using only lowercased output (withcased output, combination presents some degrada-tions).
The score combination weights are opti-mized on the development corpus, in order to max-imize the BLEU score at the sentence level whenN-best lists are reordered according to the 14 avail-able scores.
To this end, we resorted to the SRILMnbest-optimize tool to do a simplex-basedAmoeba search (Press et al, 1988) on the error func-tion with multiple restarts to avoid local minima.Once the optimized feature weights are com-puted independently for each system, N-best listsare turned into confusion networks (Mangu et al,2000).
The 14 features are used to compute poste-riors relatively to all the hypotheses in the N-bestlist.
Confusion networks are computed for each sen-tence and for each system.
In Table 5 we presentthe ROVER (Fiscus, 1997) results for the LIA andLIG confusion networks (LIA CNC and LIG CNC).Then, both confusion networks computed for eachsentence are merged into a single one.
A ROVERis applied on the combined confusion network andgenerates a lowercased 1-best.The final step aims at producing cased hypothe-ses.
The LIA system built from truecased corporaachieved significantly higher performance than the444LIG LIA LIG CNC LIA CNC LIG+LIAcase-insensitive test10 27.6 29.3 28.1 29.4 29.7BLEU test11 28.5 29.4 28.5 29.3 29.9case-sensitive test10 26.1 28.4 27.0 28.4 28.7BLEU test11 26.9 28.4 27.5 28.4 28.8Table 5: Performance measured before and after combining systemsLIG system trained on lowercased corpora (Table 5,two last lines).
In order to get an improvement whencombining the outputs, we had to adopt the follow-ing strategy.
The 500-best truecased outputs of theLIA system are first merged in a word graph (andnot a mesh lattice).
Then, the lowercased 1-bestpreviously obtained with ROVER is aligned with thegraph in order to find the closest existing path, whichis equivalent to matching an oracle with the graph.This method allows for several benefits.
The newhypothesis is based on a ?true?
decoding pass gener-ated by a truecased system and discarded marginalhypotheses.
Moreover, the selected path offers abetter BLEU score than the initial hypothesis withand without case.
This method is better than the onewhich consists of applying the LIG recaser (section3.6) on the combined (un-cased) hypothesis.The new recased one-best hypothesis is then usedas the final submission for WMT.
Our combinationapproach improves on test11 the best single sys-tem by 0.5 case-insensitive BLEU point and by 0.4case-sensitive BLEU (Table 5).
However, it also in-troduces some mistakes by duplicating in particularsome segments.
We plan to apply rules at the seg-ment level in order to reduce these artifacts.6 ConclusionThis paper presented two statistical machine trans-lation systems developed at different sites usingMOSES and the combination of these systems.
TheLIGA submission presented this year was rankedamong the best MT system for the French-Englishdirection.
This campaign was the first shot for LIAand the second for LIG.
Beside following the tradi-tional pipeline for building a phrase-based transla-tion system, each individual system led to specificworks: LIG worked on using SPE as post-treatment,LIA focused on extracting useful data from large-sized corpora.
And their combination implied to ad-dress the interesting issue of matching results fromsystems with different casing approaches.WMT is a great opportunity to chase after perfor-mance and joining our efforts has allowed to saveconsiderable amount of time for data preparationand tuning choices (even when final decisions weredifferent among systems), yet obtaining very com-petitive results.
This year, our goal was to developstate-of-the-art systems so as to investigate new ap-proaches for related topics such as translation withhuman-in-the-loop or multilingual interaction sys-tems (e.g.
vocal telephone information-query di-alogue systems in multiple languages or languageportability of such systems).ReferencesJonathan G. Fiscus.
1997.
A post-processing system toyield reduced word error rates:recognizer output vot-ing error reduction (ROVER).
In Proceedings of theIEEE Workshop on Automatic Speech Recognition andUnderstanding, pages 347?354, Santa Barbara, CA,USA.Qin Gao and Stephan Vogel.
2008.
Parallel implemen-tations of word alignment tool.
In Proceedings of theACL Workshop: Software Engineering, Testing, andQuality Assurance for Natural Language Processing,pages 49?57, Columbus, OH, USA.Almut Silja Hildebrand, Matthias Eck, Stephan Vogel,and Alex Waibel.
2005.
Adaptation of the translationmodel for statistical machine translation based on in-formation retrieval.
In Proceedings of the 10th confer-ence of the European Association for Machine Trans-lation (EAMT), Budapest, Hungary.Ste?phane Huet, Julien Bourdaillet, Alexandre Patry, andPhilippe Langlais.
2010.
The RALI machine trans-lation system for WMT 2010.
In Proceedings of theACL Joint 5th Workshop on Statistical Machine Trans-lation and Metrics (WMT), Uppsala, Sweden.Howard Johnson, Joel Martin, George Foster, and Roland445Kuhn.
2007.
Improving translation quality by dis-carding most of the phrasetable.
In Proceedings ofthe Joint Conference on Empirical Methods in Nat-ural Language Processing and Computational Natu-ral Language Learning (EMNLP-CoNLL), pages 967?975, Prague, Czech Republic, jun.Philipp Koehn, Hieu Hoang, Alexandra Birch, ChrisCallison-Burch, Marcello Federico, Nicola Bertoldi,Brooke Cowan, Wade Shen, Christine Moran, RichardZens, Chris Dyer, Ondrej Bojar, Alexandra Con-stantin, and Evan Herbst.
2007.
Moses: Open sourcetoolkit for statistical machine translation.
In Proceed-ings of the 45th Annual Meeting of the Associationfor Computational Linguistics (ACL), Companion Vol-ume, pages 177?180, Prague, Czech Republic, June.Lidia Mangu, Eric Brill, and Andreas Stolcke.
2000.Finding consensus in speech recognition: Word errorminimization and other applications of confusion net-works.
Computer Speech and Language, 14(4):373?400.Franz Josef Och and Hermann Ney.
2003.
A system-atic comparison of various statistical alignment mod-els.
Computational Linguistics, 29(1):19?51.Franz Josef Och.
2003.
Minimum error rate trainingin statistical machine translation.
In Proceedings ofthe 41st Annual Meeting on Association for Computa-tional Linguistics (ACL), Sapporo, Japan.Marion Potet, Laurent Besacier, and Herve?
Blanchon.2010.
The LIG machine translation for WMT 2010.In Proceedings of the ACL Joint 5th Workshop on Sta-tistical Machine Translation and Metrics (WMT), Up-psala, Sweden.William H. Press, Brian P. Flannery, Saul A. Teukolsky,and William T. Vetterling.
1988.
Numerical Recipesin C: The Art of Scientific Computing.
CambridgeUniversity Press.Andreas Stolcke.
2002.
SRILM ?
an extensible lan-guage modeling toolkit.
In Proceedings of the 7th In-ternational Conference on Spoken Language Process-ing (ICSLP), Denver, CO, USA.446
