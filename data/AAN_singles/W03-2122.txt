A Method for Forming Mutual Beliefs for Communication throughHuman-robot Multi-modal InteractionNaoto IwahashiSony Computer Science Labs.Tokyo, Japaniwahashi@csl.sony.co.jpAbstractThis paper describes a method of multi-modal language processing that reflectsexperiences shared by people and robots.Through incremental online optimizationin the process of interaction, the user andthe robot form mutual beliefs representedby a stochastic model.
Based on these mu-tual beliefs, the robot can interpret evenfragmental and ambiguous utterances, andcan act and generate utterances appropri-ate for a given situation.1 IntroductionThe process of human communication is based oncertain beliefs shared by those who are communi-cating.
Language is one such mutual belief, and itis used to convey meaning based on its relevanceto other mutual beliefs (Sperber and Wilson, 1995).These mutual beliefs are formed through interac-tion with the environment and other people, and themeaning of utterances is embedded in such sharedexperiences.If those communicating want to logically con-vince each other that proposition p is a mutual belief,they must prove that the infinitely nested proposition?They have information that they have informationthat .
.
.
that they have information that p?
also holds.However, in reality, all we can do is assume, basedon a few clues, that our beliefs are identical to thoseof other people we are talking to.
That is, it cannever be guaranteed that our beliefs are identical tothose of other people.The processes of utterance generation and un-derstanding rely on a system of mutual beliefs as-sumed by each person, and this system changes au-tonomously and recursively through these processes.The listener interprets utterances based on their rel-evance to their system of assumed mutual beliefs.The listener also receives information for updatingtheir system of assumed mutual beliefs through thisprocess.
In addition, the speaker can receive simi-lar information through the response of the listener.Through utterances, people simultaneously send andreceive information about one another?s system ofassumed mutual beliefs.
In this sense, we can saythat a mutual belief system assumed by one per-son couples with mutual belief systems assumed byother people they are communicating with.To enable humans and robots to communicatewith one another in a physical environment theway people do, spoken-language processing meth-ods must emphasize mutual understanding, and theymust have a mechanism that would enable the mu-tual belief systems couple with one another.
More-over, language, perception, and behavior have tobe processed integratively in order for humans androbots to physically share their environment as thebasis for the formation of common experiences andin order for linguistic and nonlinguistic beliefs tocombine in the process of human-robot interaction.Previous language processing methods, which arecharacterized by fixed language knowledge, do notsatisfy these requirements because they cannot dy-namically reflect experiences in the communicationprocess in a real environment.I have been working on methods for forming lin-guistic beliefs, such as beliefs about phonemes, lexi-con, and grammar, based on common perceptual ex-Figure 1: Interaction between a user and a robotperiences between people and robots (for further de-tail, see (Iwahashi, 2001; Iwahashi, 2003)).
This pa-per describes a method that enables robots to learna system of mutual beliefs including nonlinguisticones through multi-modal language interaction withpeople.
The learning is based on incremental on-line optimization, and it uses information from rawspeech and visual observations as well as behavioralreinforcement, which is integrated in a probabilisticframework.Theoretical research (Clark, 1996) and its com-putational modelling (Traum, 1994) focused on theformation of mutual beliefs that is a direct target ofcommunication, and aimed at representing the for-mation of mutual beliefs as a procedure- and rule-driven process.
In contrast, this study focuses on asystem of mutual beliefs that is used in the process ofutterance generation and understanding in a physicalenvironment, and aims at representing the formationof this system by a mathematical model of couplingsystems.2 Task for Forming Mutual BeliefsThe task for forming mutual beliefs was set up asfollows.
A robot was sat at a table so that the robotand the user sitting at the table could see and movethe objects on the table (Fig.
1) The user and therobot initially shared certain basic linguistic beliefs,including a lexicon with a small number of items anda simple grammar, and the robot could understandsome utterances 1.
The user asked the robot to movean object by making an utterance and a gesture, andthe robot acted in response.
If the robot respondedincorrectly, the user slapped the robot?s hand.
The1No function words were included in the lexicon.Figure 2: A scene during which utterances weremade and understoodrobot also asked the user to move an object, and theuser acted in response.
Mutual beliefs were formedincrementally, online, through such interaction.Figure 2 shows an example of utterance genera-tion and understanding using mutual beliefs.
In thescene shown in Fig.
2, the object on the left, Kermit,has just been put on the table.If the user in the figure wants to ask the robot tomove Kermit onto the box, he may say ?Kermit boxmove-onto?.
In this situation, if the user assumesthat the robot shares the belief that the object movedin the previous action is likely to be the next targetfor movement and the belief that the box is likelyto be something for the object to be moved onto,he might just say ?move-onto?.
To understand thisfragmental utterance, the robot has to have similarbeliefs.
Inversely, when the robot wants to ask theuser to do something, mutual beliefs are used in thesame way.3 Algorithm for Learning a System ofMutual Beliefs3.1 SettingThe robot has an arm with a hand, and a stereocamera unit.
A close-talk microphone is used forspeech input, and the speech is represented by atime-sequence of Mel-scale cepstrum coefficients.Visual observation oi of object i is representedby using such features as color (three-dimensional:L*a*b* parameters), size (one-dimensional), andshape (two-dimensional).
Trajectory u of the ob-ject?s motion is represented by a time-sequence of itspositions.
A touch sensor is attached to the robot?shand.3.2 Representation of a system of mutualbeliefsIn the algorithm I developed, the system of mutualbeliefs consists of two parts: 1) a decision func-tion, composed of a set of beliefs with values rep-resenting the degree of confidence that each beliefis shared by the robot and the user, and 2) a globalconfidence function, which represents the degree ofconfidence for the decision function.
The beliefs Iused are those concerning lexicon, grammar, behav-ioral context, and motion-object relationship.
Thedegree of confidence for each belief is representedby a scalar value.
The beliefs are represented bystochastic models as follows:Lexicon LThe lexicon is represented by a set of pairs, eachwith probability density function (pdf) p(s|ci) offeature s of a spoken word and a pdf for representingthe image concept of lexical item ci, i = 1, .
.
.
, N .Two types of image concepts are used.
One is a con-cept for the static observation of an object.
Con-ditional pdf p(o|c) of feature o of an object givenlexical item c is represented by a Gaussian pdf.
Theother is a concept for the movement of an object.The concept is viewed as the process of change inthe relationship between the trajector and the land-mark.
Here, given lexical item c, position ot,p oftrajector object t, and position ol,p of landmark ob-ject l, conditional pdf p(u|ot,p, ol,p, c) of trajectory uis represented by a hidden Markov Model (HMM).Pdf p(s|c) of the features of a spoken word is alsorepresented by an HMM.Grammar GI assume that an utterance can be understood basedon its conceptual structure z, which has three at-tributes: landmark, trajector, and motion, each ofwhich contains certain elements of the utterance.Grammar G is represented by a set of occurrenceprobabilities for the orders of these attributes in anutterance and a statistical bigram model of the lexi-cal items for each of the three attributes.Effect of behavioral context B1(i, q;H)Effect of behavioral context represents the belief thatthe current utterance refers to object i, given behav-ioral context q. q includes information on whetherobject i was a trajector or a landmark in the previ-ous action and whether the user?s current gesture isreferring to object i.
This belief is represented by aparameter set H = {hc, hg, hp}, and it takes hc asits value if object i was involved in the previous ac-tion, hg if the object is being held, hp if it is beingpointed to, and 0 in all other cases.Motion-object relationship B2(ot,f , ol,f , WM ;R)Motion-object relationship represents the belief thatin the motion corresponing to lexical item WM , fea-ture ot,f of object t and feature ol,f of object l aretypical for a trajector and a landmark, respectively.This belief is represented by a conditional multivari-ate Gaussian pdf, p(ot,f , ol,f |WM ;R), where R isits parameter set.3.3 Decision functionThe beliefs described above are organized and as-signed confidence values to obtain the decision func-tion used in the process of utterance generation andunderstanding.
This decision function is written as?
(s, a, O, q, L,G,R,H,?
)= maxl,z(?1log p(s|z; L,G) [Speech]+?2log p(u|ot,p, ol,p, WM ; L) [Motion]+?2(log p(ot,f |WT ; L) + log p(ol,f |WL; L))[Object]+?3log p(ot,f , ol,f , |WM ; R)[Motion-Object Relationship]+?4(B1(t, q; H) +B1(l, q; H)))[Behavioral Context]where ?
= {?1, .
.
.
, ?4} is a set of confidence valuesfor beliefs corresponding to the speech, motion, ob-ject, motion-object relationship, and behavioral con-text; a denotes the action, and it is represented by apair (t, u) of trajector object t and trajectory u of itsmovement; O denotes the scene, which includes thepositions and features of all the objects in the scene;and WT and WL denotes the sequences of the lexi-cal items in the utterances for the trajector and land-mark, respectively.
Given O, q, L, G, R, H , and?, the corresponding action, a?
= (u?, t?
), understoodto be the meaning of utterance s is determined bymaximizing the decision function asa?
= argmaxa?
(s, a, O, q, L,G,R,H,?
).3.4 Global confidence functionGlobal confidence function f outputs an estimate ofthe probability that the robot?s utterance s will becorrectly understood by the user, and it is written asf(x) =1?arctan(x?
?1?2)+ 0.5,where ?1and ?2are the parameters of this function.A margin in the value of the output of the decisionfunction in the process of generating an utterance isused for input x of this function.
Margin d obtainedin the process of generating utterance s that meansaction a in scene O under behavioral context q isdefined asd(s, a, O, q, L,G,R, H,?
)= minA=a(?
(s, a, O, q, L, G, R, H,?)??
(s, A, O, q, L, G,R, H,?
)).We can easily see that a large margin increases theprobability of the robot being understood correctlyby the user.
If there is a high probability of therobot?s utterances being understood correctly evenwhen the margin is small, we can say that the robot?sbeliefs are consistent with those of the user.
Whenthe robot asks for action a in scene O under behav-ioral context q, the robot generates utterance s?
soas to make the value of the output of f as close aspossible to value of parameter ?, which representsthe taget probability of the robot?s utterance beingunderstood correctly.
This utterance can be repre-sented ass?
= argmins(f(d(s, a, O, q, L,G,R,H,?))?
?
).The robot can increase the chance of being under-stood correctly by using more words.
On the otherhand, if the robot can predict correct understandingwith a sufficiently high probability, the robot canmanage with a fragmental utterance using a smallnumber of words.3.5 LearningThe decision function and the global confidencefunction are learned separately in the utterance un-derstanding and utterance generation processes, re-spectively.The decision function is learned incrementally,online, through a sequence of episodes each ofwhich consists of the following steps.
1) Throughan utterance and a gesture, the user asks the robot tomove an object.
2) The robot acts on its understand-ing of the utterance.
3) If the robot acts correctly, theprocess is terminated.
Otherwise, the user slaps therobot?s hand.
4) The robot acts in a different way.
5)If the robot acts incorrectly, the user slaps the robot?shand.When the robot acts correctly in the first or sec-ond trial in an episode, the robot associates utterances, action a, scene O, and behavioral context q witheach other, and makes these associations a learningsample.
Then the robot adapts the values of parame-ter set R for the belief about the motion-object rela-tionship, parameter set H for the belief about the ef-fect of the behavioral context, and a set of weightingparameters, ?.
R is learned by using the Bayesianlearning method.
H and ?
are learned based on theminimum error criterion (Juang and Katagiri, 1992).Lexicon L and grammar G are given beforehand andare fixed.
When the ith sample (si, ai, Oi, qi) is ob-tained based on this process of association, Hi and?i are adapted to minimize the probability of mis-understanding based on the minimum error criterionasi?j=i?Kwi?j g(d (sj , aj, Oj, qj, L, G, Ri, Hi,?i))?
min,where g(x) is ?x if x < 0 and 0 otherwise, andK and wi?j represent the number of latest samplesused in the learning process and the weights for eachsample, respectively.Global confidence function f is learned incremen-tally, online, through a sequence of episodes whichconsist of the following steps.
1) The robot gener-ates an utterance to ask the user to move an object.2) The user acts according to their understandingof the robot?s utterance.
3) The robot determineswhether the user?s action is correct or not.In each episode, the robot generates an utterancethat makes the value of the output of global confi-dence function f as close to ?
as possible.
After eachepisode, the value of margin d in the utterance gen-eration process is associated with information aboutwhether the utterance was understood correctly ornot, and this sample of associations is used for learn-ing.
The learning is done so as to approximate theprobability that an utterance will be understood cor-rectly by minimizing the weighted sum of squarederrors in the latest episodes.
After the ith episode,parameters ?1and ?2are adapted as[?1,i, ?2,i] ?
(1?
?
)[?1,i?1, ?2,i?1] + ?[?
?1,i, ??2,i],where(?
?1,i, ?
?2,i)= arg min?1,?2i?j=i?Kwi?j(f(dj; ?1, ?2)?
ej)2,where ei is 1 if the user?s understanding is correctand 0 if it is not, and ?
is the value that determinesthe learning speed.4 Experiments4.1 ConditionsThe lexicon used in the experiments included elevenitems for the static image and six items to describethe motions.
Eleven stuffed toys and four boxeswere used as objects in the interaction between auser and a robot.
In the learning process, the inter-action was simulated by using software.4.2 Learning of decision functionSequence X of quadruplets (si, ai, Oi, qi) consistingof the user?s utterance si, scene Oi, behavioral con-text qi, and action ai that the user wanted to ask therobot to perform (i = 1, .
.
.
, nd), was used for theinteraction.
At the beginning of the sequence, thesentences were relatively complete (e.g., ?green ker-mit red box move-onto?).
Then the length of the sen-tences was gradually reduced (e.g., ?move-onto?
).R could be estimated with high accuracy duringthe episodes in which relatively complete utteranceswere understood correctly.
H and ?
could be effec-tively estimated based on the estimation of R whenfragmental utterances were given.
Figure 3 shows32 64 12800.20.40.60.8Episode?1?2?3Confidenceweights1.01.21.41.6?
h4   cFigure 3: Changes in the confidence values32 64 128020406080EpisodeErrorrate[%]w/o learningwith learningFigure 4: The change in decision error ratechanges in the values of ?1, ?2, ?3, and ?4hc.
Wecan see that each value was adapted according tothe ambiguity of a given sentence.
Figure 4 showsthe decision error (misunderstanding) rates obtainedduring the course of the interaction, along with theerror rates obtained in the same data, X , by keepingthe values of the parameters of the decision functionfixed to their initial values.Examples of actions generated as a result of cor-rect understanding are shown together with the out-put log probabilities from the weighted beliefs inFigs.
5 (a) and (b), along with the second and thirdaction candidates, which led to incorrect actions.
Wecan see that each nonlinguistic belief was used ap-propriately in understanding the utterances.
Beliefs(a)" Move-onto "previouscorrect incorrectSpeech Motion BehavioralContextMotion-ObjectRelationship1st(Correct)2nd(Incorrect)(b)" Grover Small Kermit Jump-over "correctpreviousincorrectSpeech Motion BehavioralcontextMotion-objectrelationshipObject1st(Correct)3rd(Incorrect)Figure 5: Examples of fragmental utterances under-stood correctly by the robotabout the behavioral context were more effective inFig.
5 (a), while in Fig.
5 (b), beliefs about the objectconcepts were more effective than other nonlinguis-tic beliefs in leading to the correct understanding.This learning process is described in greater detailin (Miyata et al, 2001)4.3 Learning of the global confidence functionIn the experiments with the learning of the globalconfidence function, The robot?s utterances were ex-pressed through text on a display instead of oralspeech, and they included one word describing themotion and either no words or one to several wordsdescribing the trajector and landmark objects or justthe trajector object.A sequence of triplets (a, O, q) consisting ofscene O, behavioral context q, and action a that therobot needed to ask the user to perform, was used forthe interaction.
In each episode, the robot generatedan utterance to bring the global confidence functionas close to 0.75 as possible.The changes in f(d) are shown in Fig.
5 (a),where three lines are drawn for d0.5 = f?1(0.5),d0.75 = f?1(0.75), and d0.9 = f?1(0.9) in orderto make the shape of f(d) easily recognizable.
Thechanges in the number of words used to describe theobjects in each utterance are shown in Fig.
5 (b),along with the changes obtained in the case whenf(d) was not learned, which are shown for com-parison.
The initial values were set at d0.9 = 161,d0.75 = 120, and d0.5 = 100, which means that alarge margin was necessary for an utterance to beunderstood correctly.
Note that when all the valuesare close to 0, the slope in the middle of f is steep,and the robot makes a decision that a small marginis enough for its utterances to be understood cor-rectly.
After the learning began, these values rapidlyapproached 0, and the number of words decreased.The slope became temporarily smooth at around the15th episode.
Then, the number of words becametoo small, which sometimes lead to misunderstand-ing.
Finally, the slope became steep again at aroundthe 35th episode.5 DiscussionThe above experiments illustrate the importance ofmisunderstanding and clarification, i.e.
error and re-pair, in the formation of mutual beliefs between peo-ple and machines.
In the learning period for utter-ance understanding by the robot, the values of theparameters of the decision function changed signif-icantly when the robot acted incorrectly in the firsttrial, and correctly in the second trial.
In the learn-ing period for utterance generation by the robot,in the experiment in which the target value of theglobal confidence function was set to 0.95, whichwas larger than 0.75 and closer to 1, the global con-fidence function was not properly estimated becausealmost all utterances were understood correctly (Theresults of this experiment are not presented in de-(a)-40-200204060800 10 20 30 40 50 60 70EpisodeMargind 90%75%50%(b)12340 10 20 30 40 50 60 70EpisodeNumber of wordsw/o learningwith learningFigure 6: Changes in the global confidence function(a) and the number of words needed to describe theobjects in each utterance (b)tail in this paper).
These results show that occa-sional errors enhance the formation of mutual be-liefs in both the utterance generation and utteranceunderstanding processes.
This implies that in or-der to obtain information about mutual beliefs, boththe robot and the user must face the risk of not be-ing understood correctly.
The importance of errorand repair to learning in general has been seen as anexploration-exploitation trade-off in the area of re-inforcement learning by machines (e.g.
(Dayan andSejnowski, 1996)).The experimental results showed that the robotcould learn its system of the beliefs the robot as-sumed the user had.
Because the user came to un-derstand the robot?s fragmental and ambiguous ut-terances, the user and the robot must have sharedsimilar beliefs, and must have been aware of that.It would be interesting to investigate by experimentthe dynamics of sharing beliefs between a user anda robot.6 Related Works(Winograd, 1972) and (Shapiro et al, 2000) ex-plored the grounding of the meanings of utterancesin conversation onto the physical world by usinglogic, but the researchers did not investigate the pro-cessing of information from the real physical world.
(Matsui et al, 2000) focused on enabling robots towork in the real world, and integrated language withinformation from robot?s sensors by using patternrecognition.
(Inamura et al, 2000) investigated anautonomous mobile robot that controlled its actionsand conversations with a user based on a Bayesiannetwork.
The use of Bayesian networks in the in-terpretation and generation of dialogue was also in-vestigated by (Lemon et al, 2002).
In (Singh etal., 2000), the learning of dialogue strategies us-ing reinforcement learning was investigated.
Someof these works looked at beliefs ?held by?
the ma-chines themselves, but none focused on the for-mation of mutual beliefs between humans and ma-chines through interaction, based on common expe-riences.7 ConclusionThe presented method enables the formation of mu-tual beliefs between people and robots through in-teraction in physical environments, and it facilitatesthe process of human-machine communication.
Inthe future, I want to focus on the generalization oflearning of mutual beliefs and the learning of dia-logue control.ReferencesH.
Clark.
1996.
Using Language.
Cambridge UniversityPress.P.
Dayan and T. J. Sejnowski.
1996.
ExplorationBonuses and Dual Control.
Machine Learning, 25:5?22.T.
Inamura, M. Inaba and H. Inoue.
2000.
IntegrationModel of Learning Mechanism and Dialogue Strategybased on Stochastic Experience Representation us-ing Bayesian Network.
Proceedings of InternationalWorkshop on Robot and Human Interactive Communi-cation, 27?29.N.
Iwahashi.
2001.
Language Acquisition by Robots.The Institute of Electronics, Information, and Commu-nication Engineers Technical Report SP2001-96.N.
Iwahashi.
2003.
Language Acquisition by Robots:Towards New Paradigm of Language Processing.Journal of Japanese Society for Artificial Intelligence,18(1):49-58.B.-H. Juang and S. Katagiri.
1992.
DiscriminativeLearning for Minimum Error Classification.
IEEETransactions on Signal Processing, 40(12):3043?3054.O.
Lemon, P. Parikh and S. Peters.
2002.
ProbabilisticDialogue Management.
Proceedings of Third SIGdialWorkshop on Discourse and Dialogue, 125?128.T.
Matsui, H. Asoh, J. Fry, Y. Motomura, F. Asano, TKurita and N. Otsu.
1999.
Integrated Natural Spo-ken Dialogue System of Jijo-2 Mobile Robot for OfficeServices.
Proceedings of 15th National Conference onArtificial Intelligence.A.
Miyata, N. Iwahashi and A. Kurematsu.
2001.
Mutualbelief forming by robots based on the process of utter-ance comprehension.
Technical Report of The Insti-tute of Electronics, Information, and CommunicationEngineers, SP2001-98.C.
S. Shapiro, H. O. Ismail, and J. F. Santore.
2000.
OurDinner with Cassie.
Working Notes for AAAI 2000Spring Symposium on Natural Dialogues with Prac-tical Robotic Devices, 57-61.S.
Singh, M. Kearns, D. J. Litman and M. A. Malker.2000.
Empirical Evaluation of a Reinforce LearningSpoken Dialogue System.
Proc.
16th National Con-ference on Artificial Intelligence, 645?651.D.
Sperber and D. Wilson.
1995.
Relevance (2nd Edi-tion).
Blackwell.D.
R. Traum.
1994.
A computationaltheory of groundingin natural language conversation.
Unpublished doc-toral dissertation, University of Rochester.T.
Winograd.
1972.
Understanding Natural Language.Academic Press New York.
