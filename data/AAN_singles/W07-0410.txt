Proceedings of SSST, NAACL-HLT 2007 / AMTA Workshop on Syntax and Structure in Statistical Translation, pages 72?79,Rochester, New York, April 2007. c?2007 Association for Computational LinguisticsA Walk on the Other Side:Adding Statistical Components to a Transfer-Based Translation SystemAriadna Font Llitj?sCarnegie Mellon University5000 Forbes Ave.Pittsburgh, PA, 15213aria@cs.cmu.eduStephan VogelCarnegie Mellon University5000 Forbes Ave.Pittsburgh, PA, 15213vogel+@cs.cmu.eduAbstractThis paper seeks to complement the cur-rent trend of adding more structure to Sta-tistical Machine Translation systems, byexploring the opposite direction: addingstatistical components to a Transfer-BasedMT system.
Initial results on the BTECdata show significant improvement ac-cording to three automatic evaluationmetrics (BLEU, NIST and METEOR).1 IntroductionIn recent years the machine translation researchcommunity has seen a remarkable paradigm shift.It is not the first one, but it has been a very dra-matic one: statistical machine translation has takenthe center stage.
Conferences like ACL or HLT arevirtually flooded with papers on various flavors ofSMT.
In international machine translation evalua-tion like NIST (NIST MT Evaluation), TC-Star(TC-STAR Evaluation) or IWSLT (IWSLT 2006)evaluations, most participating systems are SMTsystems, with a few Example-Based systems sprin-kled in.
Rule-Based systems seem to have for themost part disappeared.
There may be many reasonsfor this paradigm shift.
One obvious reason is thecomparable ease, which with data-driven systemscan be built once some parallel data is available.Another reason is that the performance of statisti-cal translation systems has dramatically improvedover the last 5 to 10 years.Does this mean that work on grammar-basedsystems should be stopped?
Should all the insightinto the structure of languages be neglected?
Thismight be too drastic a reaction.
Actually, now thatSMT has reached some maturity, we see severalattempts to integrate more structure into these sys-tems, ranging from simple hierarchical alignmentmodels (Wu 1997, Chiang 2005) to syntax-basedstatistical systems (Yamada and Knight 2001,Zollmann and Venugopal 2006).
What can tradi-tional Rule-Based translation systems learn fromthese approaches?
And would it not make sense towork from both sides towards that common goal:structurally rich statistical translation models.
Inthis paper we study some enhancements for aTransfer-Based translation system, using tech-niques and even components developed for statisti-cal machine translation.
While the core engineremains virtually untouched, additional features areadded to re-score the n-best list generated by thetransfer engine.
Statistical alignment techniquesare used to lower the burden in building a lexiconfor a new domain.
Minimum error rate training isused to optimize the system.
We show that thisleads to significant improvements in performance.2 A Transfer-Based Translation System2.1 The Lexicon and GrammarIn our Rule-Based MT (RBMT) system, translationrules include parsing, transfer, and generation in-formation, similar to the modified transfer ap-proach used in the early Metal system (Hutchinsand Somers, 1992).The initial lexicon (479 entries) and grammar(40 rules) used in our experiments were manuallywritten to cover the syntactic structures and thevocabulary of the first 400 sentences of theAVENUE Elicitation Corpus (Probst et al2001).The Elicitation Corpus contains sets of minimalpairs in English and it was designed to cover a va-riety of linguistic phenomena.
Building these twolanguage-dependent components took a computa-tional linguist 2-3 months.
Figures 1 and 2 show72examples of a translation rules in the grammar andthe lexicon.
{S,4}S::S : [NP VP] -> [NP VP]( (X1::Y1)  (X2::Y2)(x0 = x2)((y2 subj) = -)((y1 case) = nom)((y1 agr) = (x1 agr))((y2 tense) = (x2 tense))((y2 agr pers) = (y1 agr pers))((y2 agr num) = (y1 agr num)) )Figure 1: English?Spanish translation rule withagreement constraints for subject (NP) and verb(VP).V::V |: ["prefer"] -> ["prefiero"]((X1::Y1)((x0 form) = prefer)((x0 tense) = pres)((y0 agr pers) = 1)((y0 agr num) = sg))Figure 2: English?Spanish lexical entry for theverb ?prefer?.2.2 Refined MT SystemThe original grammar and lexicon were automati-cally improved with an Automatic Rule Refiner,guided by a few bilingual speaker corrections(Font Llitj?s & Ridmann 2007).
In this approach,automatic refinements only affect the target lan-guage side of translation rules, namely transfer andgeneration information.The refined MT system used in our experimentsis the result of adding 30 agreement constraints tothe grammar rules, which makes the grammartighter (leading to an increase in precision), as wellas adding three new rules to cover new syntacticstructures and five lexical entries for new sensesand forms of existing words (leading to an increasein recall).2.3 The Transfer EngineThe Transfer Engine, or Xfer engine for short,combines the translation grammar and lexicon inorder to produce translations of a source languagesentence into a target language.
The Xfer engineincorporates the three main processes involved inTransfer-based MT: parsing of the source languageinput, transfer of the parsed constituents of thesource sentence to their corresponding structuredconstituents on the target language side, and gen-eration of the target sentence.The currently implemented algorithm is similarto bottom-up chart parsing as described for exam-ple in Allen (1995).
A chart is first populated withall constituent structures that were created in thecourse of parsing the source language sentencewith the source-side portion of the transfer gram-mar.
Transfer and generation are applied to eachconstituent entry.
The transfer rules associatedwith each entry in the chart are used in order todetermine the corresponding constituent structureon the target language side.
At the word level, lexi-cal transfer rules are used in order to get the differ-ent lexical choices.Often, no parse for the entire source sentencecan be found.
Partial parses are concatenated se-quentially to generate complete translations.In the current version of the Xfer system, theoutput can be a first-best translation or a n-best list,which can be used for additional n-best list rescor-ing.
The alternatives arise from lexical ambiguityand multiple synonymous choices for lexical itemsin the dictionary, but also from syntactic ambiguityand multiple competing hypotheses from thegrammar.For our experiments, we used version 3 of theXfer engine.
An older version of the Xfer engine isdescribed in detail in Peterson (2002).2.4 Ranking TranslationsThe Xfer engine can generate multiple translations.This requires a quality score to be assigned to allthe alternatives.
Based on these scores, the 1-besttranslation will be selected by the system.Fragmentation PenaltyIn the original Xfer system the only score used torank translation alternatives was a heuristic frag-mentation penalty.
The fragmentation penalty isessentially the number of different chunks (rules orlexical entries not embedded in another rule) thatspan the whole translation.
The intuition behindthis score is that the more partial parses are neces-sary to span the entire sentence the less likely theresulting translation will be a good one.N-gram LMThe fragmentation feature is rather weak.
It doesnot distinguish between words which are morelikely to be seen in the target language and wordswhich are less likely to be used.
To generate sen-73tences which are not only grammatically correct,but also use words and word sequences that aremore natural and more common, data-driven ma-chine translation systems use a n-gram languagemodel.
To get the same benefit in the Xfer system,an n-gram LM has been integrated with the engine.This has the advantage that in the case of prun-ing, the LM score can be used to avoid pruninggood hypotheses, in addition to re re-rank the finaltranslations.For our experiments, a suffix array languagemodel based on the SALM toolkit (Zhang & Vo-gel, 2006) is used.Length ModelTo adjust for the length of the translations gener-ated by the system, the difference between thenumber of words generated and the expected num-ber of words is added as a very simple feature.
Theexpected length is calculated by multiplying thesource sentence length by the ratio of the numberof target and source words in the training corpus.The effect of this feature is to balance globally thelength of the translations.2.5 PruningTo deal with the combinatorial explosion duringthe parsing/translation process, pruning has to beapplied.
Only the n top-ranking hypotheses arekept in each cell of the chart.
The ranking of thesepartial translations is based on their languagemodel score, which at this time is only an ap-proximation, as the true history has not been seenand cannot be taken into account.3 Building a Xfer System for a New Do-mainA major bottleneck in developing a RBMT systemfor a new translation task (a new language pair or anew domain) is writing the grammar and buildingthe lexicon.
Automatic grammar induction usingstatistical alignments has been studied in (Probst2005).Here, we start with an existing grammar andaugment the baseline lexicon with entries to coverthe new domain.
We explore semi-automatic lexi-con generation for fast adaptation to the travel do-main (Section 3.2).3.1 Test Data: BTEC CorpusFor initial evaluation on unseen data, we selectedthe Basic Travel Expression Corpus (BTEC)(Takezawa et al 2002), which has been used in theevaluation campaigns in connection with the Inter-national Workshop on Spoken Language Transla-tion (IWSLT 2006).
Besides still being currentlyused to build real systems (Shimizu et al 2006;Nakamura, et al 2006), this corpus contains rela-tively simple sentences that are comparable to theones initially corrected by users, and which arecovered by the baseline manual grammar.As our test set, we used 506 English sentencesfor which two sets of Spanish reference transla-tions were available.
Table 1 shows corpus statis-tics for the BTEC data.Data  EnglishSentences Pairs 123,416Sentence Length   7.3Word Tokens  903,525TrainWord Types  12,578Sentence Pairs 506Word Tokens 3,764Word Types 776BTECTestCoverage Test 756 (97%)Table 1: Corpus Statistics for the BTEC corpus3.2 Semi-Automatic Generation of theTransfer LexiconThe Transfer-Based system relies on a lexicon thatcontains POS, gender and number agreement,among other linguistic features.
To adjust the sys-tem quickly to a new task, we decided to leveragefrom statistical alignment models to generate wordand phrase alignments as candidates for the trans-fer lexicon.In the first step, we trained statistical lexiconsusing the well-known IBM1 word alignmentmodel: one for the directions Spanish to English,and one for the direction English to Spanish.
Asmulti-word entries, are often needed ([valuables]?
[objetos de valor], [reception desk] ?
[recep-ci?n], [air conditioner]?
[aire acondicionado]), weused phrase alignment techniques to create transla-tion candidates for words and 2-word phrases.
Thephrase alignment also generates multi-word trans-lations for single source words.
With reasonablytight pruning, a manageable phrase translation ta-74ble was generated.
This first step took about 5hours.The next step, manually cleaning the translationtable, annotating them with parts-of-speech, andwith agreement and tense constraints, was initiallyrestricted to those items that overlapped with thevocabulary of our development test set, and tooktwo days.The statistically generated lexicon comprises1,248 lexical entries, whereas the initial manuallexicon contained 479 lexical entries.
For ourBTEC experiments, we combined both lexicons.3.3 Xfer Results with No RankingTo determine how the Xfer system would performonly on the basis of the lexicon and grammar, weran one translation experiment in which no lan-guage model was used.
This experiment was alsointended to see if the refined grammar would leadto better translations.
We took the first-best transla-tion output by the system without using any statis-tical components to rank alternative translations.System METEOR BLEU NISTBaseline 0.5666 0.2745 5.88Refined 0.5676 0.2559 5.62Table 2: Automatic metric scores for a purelyRule-Based MT System.Table 2 shows that, in this crude setting, differ-ent automatic metrics do not agree on the transla-tion accuracy of both systems.
On one hand,METEOR (Lavie et al 2004), which has beenshown to correlate well with human judgments(Snover et al 2006), indicates that the refined sys-tem outperforms the baseline system (as measuredby the latest version v0.5.1,).
On the other hand,both BLEU (Papineni et al, 2002) and NIST(Doddington 2002) scores are higher for the base-line system (mteval-v11b.pl).However, human inspection revealed that the re-fined grammar is able to augment the n-best listwith correct translations that the baseline systemwas not able to generate.
This suggests that theseresults reflect poor re-ranking and not n-best listquality.
In the next section, we describe an oracleexperiment to measure n-best list quality of bothsystems.3.4 Oracle ExperimentOracle scores provide an upper-bound in perform-ance.
For the BTEC test set, we approximated ahuman oracle by calculating automatic metricscores for METEOR and for BLEU and NIST.Given 100-best lists for each source languagesentence, we selected the best translation hypothe-sis for each automatic metric separately.These scores reflect the fact that automatic re-finements are able to feed the n-best list with bettertranslations, as evulated by comparison againsthuman reference translations.
Even with a small setof independent user corrections, the refined systemshows potential improved translation quality asindicated by higher scores for all three automaticevaluation metrics in Table 3.System METEOR BLEU NISTBaseline 0.6863 0.4068 7.42Refined 0.6954 0.4215 7.51Table 3: Automatic metric oracle scores based on a100-best listMoreover, oracle scores provide the margin thatwe can gain when improving on the re-ranking ofthe n-best list produced by the Xfer engine.3.5 Xfer Results with Initial RankingAs expected, when the Xfer system is run in com-bination with a LM1 as well as the fragmentationpenalty, automatic metric scores for the 1-best hy-pothesis are significantly higher (Table 4), thanwhen just using the first translation output by theXfer system alone (Table 2).System METEOR BLEU NISTBaseline 0.6176 0.3425 6.53Refined 0.6222 0.3513 6.56Table 4: Automatic metric scores for 1-best de-coder hypothesis.These results are lower than the oracle scores forboth the baseline and the refined system (Table 3),which is also to be expected.
However, the impor-tant thing to notice from these results is that, like inthe oracle case, the refined system consistentlyoutperforms the baseline MT system for all threeautomatic metrics.1 The Suffix Array Language Model (SALM) was built usingthe 123,416 Spanish sentences from the training data.75The difference between the baseline and the re-fined system in terms of 1-best scores is slightlysmaller than the difference between oracle scores,which means that the decoder can not fully lever-age the improvements made in the grammar.
Thisindicates that the decoder fails to select the besttranslation in most cases.4 Adding Statistical Components to a Re-RankerThe information used in the Xfer system to rankalternative translations is limited.
Essentially, it isthe n-gram LM, which is the most important com-ponent, a simple sentence length model, and thefragmentation score, which measures if a com-pletely spanning parse could be found or if thetranslation is glued together from partial parses.Given an n-best list of translations for each sourcesentence, we can apply additional models to re-rank these n-best list, hopefully pushing more goodtranslations into the first rank.
We studied the ef-fect of adding different features to the n-best lists:lexical features and rule (type) probability features.4.1 Word-To-Word ProbabilitiesIn SMT systems, rescoring with an IBM1 model-like word alignment score has become a standardfeature.
We use two word-to-word lexicons (Eng-lish?Spanish and Spanish?English) to calculatesentence translation probabilities, based on word-to-word probabilities:?
?= )|(1)|( jiI sepJseP       Eq.1and:?
?= )|(1)|( ijJ espIesP       Eq.2Here, we denote the English words with e, theSpanish words with s, the sentence lengths aregiven by I and J.
In the IBM1 alignment model,the position alignment is a uniform distribution p( i| j ) = 1/I for Spanish to English and p( j | i ) = 1/Jfor English to Spanish.
For Spanish to English, wehave the additional factor of (1/I)J, i.e.
longertranslations get a smaller probability, and for En-Sp we have (1/J)I, which again gives a bias to-wards shorter translations.
To compensate for thisbias, we use probabilities normalized to the sen-tence length.
Table 5 shows that adding the lexicalprobabilities improves the 1-best translation score.However, there is no significant difference whenusing different normalization of the lexicon prob-abilities.
The length bias introduced by differentlexicon features can be balanced by the decoder?slength feature.BLEU NISTRefined 0.3513 6.56+Lex Prob 0.3755 6.88Table 5: Comparing 1-best scores with scoresresult of rescoring the n-best list with lexical fea-tures.4.2 Rule ProbabilitiesThe Xfer MT system can display the derivationtree showing the rules applied during translation.This allows rescoring the translations with ruleprobabilities.
However, there is no annotated cor-pus from which the rule probabilities could be es-timated.
As an approximation to such a trainingcorpus, we decided to run the Xfer system over thetraining data and to generate n-best lists with trans-lations and translation trees.
Overall, about 6 mil-lion parse trees were generated.
Using this data toestimate rule probabilities is definitely not ideal, asthe translation on the training data are far from per-fect, especially as not all the vocabulary has so farbeen added to the Xfer lexicon.
By averaging overall n-best translations a reasonable smoothing is tobe expected.We used this information in three ways.
We es-timated conditional probabilities rule r given rule-type R, i.e.
the distribution over different VP rulesor NP rules.
For each derivation D the overallprobability was then calculated as:?= )|()( RrpDP                             Eq.
3As an alternative, we just build n-gram languagemodels, one on the rule level and on the rule typelevel:?
?
?= )...|()( 1rrrpDP n                      Eq.
4?
?
?= )...|()( 1RRRpDP n                   Eq.
5Overall, 1,685 different rules and 19 rule typeswere seen in the training data.
For models 2 and 3,we used the suffix array LM once again to allowfor arbitrary long histories.
Even though it oftenbacks-off to 3-gram, 2-gram or even unigram prob-abilities.76In Table 6, we can see the effect of adding theseLMs as additional features to the system and run-ning MER training.BLEU NISTRefined 0.3513 6.56Lex.
Prob.
0.3755 6.88Cond.
Prob.
0.3728 6.81Rule LM 0.3717 6.74Rule Type LM 0.3736 6.78Table 6: BLEU scores when rescoring the n-best list with different rule probability features (aswell as the n-gram LM).5 MER TrainingLike in SMT systems, in the Xfer engine transla-tions are ranked to their total cost, which is aweighted linear combination of the individualcosts.
When adding more features to the translationsystem, a careful balancing of the individual con-tributions can make a significant difference.
How-ever, with each feature added, manually tuning thesystem becomes less and less practical, and auto-matic optimization becomes necessary.Different optimization techniques are available,like the Simplex algorithm or the special MinimumError Training as described in (Och 2003).
InMinimum Error Rate (MER) training, the n-bestlist generated by the translation system is used tofind feature weight, thereby re-ranking the n-bestlist.
This improves the match between the 1-besttranslation and given reference translations.
Opti-mization can use any metric as objective function.Typically, systems are tuned towards high BLEUor high NIST scores, more recently also towardsMETEOR or TER (Snover et al 2006).We used a MER training module (Venugopal),originally developed for an SMT system, to runMER training on the n-best lists generated by theXfer system.
This implementation allows for opti-mization towards BLEU and NIST mteval metrics.5.1 ResultsIn Table 7, we summarize some of the results fromdifferent n-best list rescoring experiments.
Usingonly the Xfer engine, without language model,gives a very low score, as the selection is basedonly on the fragmentation score.Adding the n-gram language model gives a hugeimprovement.
Adding additional features leads tomore then 2 BLEU points improvement.
However,there is not much difference when using differentfeature combinations.
It seems that the rather smallsize of the n-best list is a limiting factor.When setting the optimal weights in the Xferengine for the LM and fragmentation penaltyscores obtained from MER training, both the base-line and the refined system get higher scores, notonly according to BLEU, which was used as theobjective function, but also according to METEORand NIST automatic evaluation metrics (Table 8).System + Statistical Components 1-bestRule Based Xfer 0.2559+ Stat.
Comp.
Xfer + LM + Frag 0.3513POS LM 0.3180Rule Probabilities (Prob.)
0.2593LM + Rule Type LM 0.3736LM + Frag/Len + Rule Type LM 0.3737LM + POS + Rule LM 0.3744LM + Frag + Rule Type LM + Cond.
Rule Prob.
0.3743LM + Len + Rule Type LM + Cond.
Rule Prob.
0.3745LM + POS + Rule LM + Cond.
Rule Prob.
0.3741LM + Frag + Len + Rule Type LM + Rule Prob.
0.3746OptimizingweightswithMER trainingLM + Frag + Len + POS + Rule LM + Rule Prob.
0.3741Table 7:  BLEU scores for the Refined MT System as the weights for the different statistical componentsdescribed in Section 2.4 and 4 are optimized with MER Training.77Moreover, the difference between the Baselineand the Refined system after MER training is sta-tistically significant2, whereas this was not the casefor the initial ranking results (Table 4).Table 8: Automatic metric scores for 1-best de-coder hypothesis, after LM and Fragmentationweights have been optimized.Table 9 shows a few examples from the BTEC cor-pus with 1-best translations output by the RefinedMT system before (No Optimization) and after(With Optimization) MER training, given LM andFragmentation penalty scores.
From these exam-ples, it can be observed that re-ranking improvesafter optimizing the LM and fragmentationweights.
In particular, order issues get resolved(examples 1, 2 and 4), which result in correct de-terminer agreement (1 and 2); determiner insertion(3); correct verb form (5 and 7) and omission ofincorrect pronouns (6 and 7).6 ConclusionStarting from a Transfer-Based translation system,we explored techniques currently used in statisticaltranslation systems to rapidly adapt to a new do-main and to improve its performance.
Using wordand phrase alignment techniques allowed us toquickly augment the transfer lexicon.
Adding astatistical language model is crucial in selectinggood translations from the n-best lists generated bythe Xfer engine.
Adding additional features, suchas word-to-word probabilities and rule (type) prob-abilities, further improves performance.While this information would ideally be used inthe parsing and transfer steps of the translation sys-tem, our initial experiments were targeted at usingthis in an n-best list rescoring setup.
As rule prob-abilities were estimated from noisy training data,these models are far from optimal.To facilitate the experiments with the Xfer sys-tem, especially when adding more and more fea-tures, we added a Minimum Error Rate training2 According to the standard paired two-tailed t-Test, the de-coder METEOR scores with optimized weights are statisti-cally significant, with a p value of 0.0051.component.
Having such a component will defi-nitely boost the development of the Xfer engine.We see statistically significant improvementsover the baseline system when using optimizedweights for the word-level language model and thefragmentation score.System METEOR BLEU NISTBaseline 0.6184 0.3609 6.68Refined 0.6231 0.3780  6.79  1 Source: where is the boarding gate ?NO: d?nde est?
el embarque puerta ?WO: d?nde est?
la puerta embarque ?2 Src: where is the bus stop for city hall ?NO: d?nde est?
el autob?s parada para ayuntamiento ?WO: d?nde est?
la parada autob?s para ayuntamiento ?3 Src: i would like a twin room with a bath please .NO: me gustar?a habitaci?n una cama doble con unba?o por favor .WO: me gustar?a una habitaci?n cama doble con unba?o por favor .4 Src:  i would like to buy some duty-free items .NO: me gustar?a  comprar algunos duty-free productos.WO: me gustar?a  comprar algunos art?culos duty-free .5 Src: does he speak japanese ?NO: ?l hablar a japon?s ?WO: habla japon?s ?6 Src: it is just round the corner .NO: lo es simplemente a la vuelta de la esquina .WO: es simplemente a la vuelta de la esquina .7  Src: do you sell duty-free items ?NO: te venden art?culos duty-free ?WO: vend?is art?culos duty-free ?Table 9: 1-best translations from the BTEC test setoutput by the Refined MT system before and afterMER training.
NO stands for No Optimization ofLM and Fragmentation weights, and WO standsfor With Optimization of weights.7 Future WorkUsing rule probabilities has shown to be a promis-ing extension to the current Xfer system.
We planto improve these models by selecting the oraclebest translations from the n-best list generated onthe training data.
This will reduce the noise in thetraining stage.
Ultimately, the rule probabilitiesshould be applied not as an n-best list rescoringstep, but directly in the Xfer engine decoder.Analyzing the translation results, one importantshortcoming became obvious.
Currently the trans-lation lexicon only covers about 88% of the wordsthat appear in the reference translations.
This se-verely limits as to what kind of BLEU score wecan achieve.
When we generated the phrasal lexi-con from the BTEC training data, we deliberately78chose to only include few alternatives, mainly tolimit the manual labor when adding POS and con-straint.
We expect that the Xfer system will sig-nificantly benefit from further expanding thelexicon.ReferencesAllen, J.
1995.
Natural Language Understanding.
Sec-ond Edition ed.
Benjamin Cummings.Chiang, D. 2005.
A hierarchical phrase-based modelfor statistical machine translation.
In Proceedings ofthe 43rd Annual Meeting of the Association forComputational Linguistics (ACL), Ann Arbor, USA.Doddington G. 2002.
Automatic evaluation of machinetranslation quality using n-gram co-occurrence sta-tistics.
In Proc.
of the HLT 2002, San Diego, USA.Hutchins, W. J., and H. L. Somers.
1992.
An Introduc-tion to Machine Translation.
London: AcademicPress.Font Llitj?s, A. and W. Ridmann.
2007 The InnerWorks of an Automatic Rule Refiner for MachineTranslation.
METIS-II Workshop, Leuven, Belgium.IWSLT 2006: http://www.slt.atr.jp/IWSLT2006/Lavie, A., K. Sagae and S. Jayaraman.
2004.
The Sig-nificance of Recall in Automatic Metrics for MTEvaluation.
AMTA, Washington DC, USA.Nakamura, S., K. Markov, H. Nakaiwa, G. Kikui, H.Kawai, T. Jitsuhiro, J. Zhang, H. Yamamoto, E.Sumita, and S. Yamamoto.
2006.
The ATR multilin-gual speech-to-speech translation system.
IEEETrans.
on Audio, Speech, and Language Processing,14, No.2:365?376.NIST MT Evaluations:http://www.nist.gov/speech/tests/mt/Och, F. J.
2003.
Minimum error rate training in statisti-cal machine translation.
In Proc.
of the 41st AnnualMeeting of the Association for Computational Lin-guistics (ACL), Sapporo, Japan.Papineni, K, S. Roukos, T. Ward, and W. Zhu.
2002.BLEU: a method for automatic evaluation of ma-chine translation.
In Proc.
of the 40th ACL, Phila-delphia, USA.Peterson, E. 2002.
Adapting a transfer engine for rapidmachine translation development.
M.S.
Thesis,Georgetown University.Probst, K., Brown, R., Carbonell, J., Lavie, A. Levin,and L., Peterson, E., 2001.
Design and Implementa-tion of Controlled Elicitation for Machine Transla-tion of Low density Languages.
Proceedings of theMT2001 workshop at MT Summit, Santiago deCompostela, Spain.SALM Toolkit:http://projectile.is.cs.cmu.edu/research/public/tools/salm/salm.htmShimizu T., Y. Ashikari, E. Sumita, H. Kashioka and  S.Nakamura.
2006.
Development of client-serverspeech translation system on a multi-lingual speechcommunication platform.
IWSLT, Kyoto, Japan.Snover, M; B. Dorr, R. Schwartz, L. Micciulla, 2006.Targeted Human Annotation.
AMTA, Boston, USA.Takezawa, T, E. Sumita, F. Sugaya, H. Yamamoto, andS.
Yamamoto, 2002.
Toward a Broad-Coverage Bi-lingual Corpus for Speech Translation of TravelConversations in the Real World.
In Proceedings of3rd LREC, Las Palmas, Spain.TC-STAR Evaluations: http://www.tc-star.org/Venugopal, A.:  MER Training Toolkit.http://www.cs.cmu.edu/~ashishv/mer.htmlWu, D. 1997.
Stochastic inversion transduction gram-mars and bilingual parsing of parallel corpora.Computational Linguistics, 23:377?404.Yamada, Kenji and Kevin Knight.
2001.
A syntax-basedstatistical translation model.
In Proceedings of the39th Annual Meeting of the ACL, Toulouse, France.Zhang, Y and S. Vogel.
2006.
Suffix Array and its Ap-plications in Empirical Natural Language Process-ing,.
Technical Report CMU-LTI-06-010, PittsburghPA, USA.Zhang, Y, A. S. Hildebrand and S. Vogel.
2006.
Dis-tributed Language Modeling for N-best List Re-ranking.
Empirical Methods in Natural LanguageProcessing (EMNLP), Sydney, Australia.Zollmann A. and A. Venugopal.
2006.
Syntax Aug-mented Machine Translation via Chart Parsing.
InProc.
of NAACL 2006 - Workshop on StatisticalMachine Translation, New York, USA.79
