Proceedings of the 8th International Conference on Computational Semantics, pages 61?72,Tilburg, January 2009. c?2009 International Conference on Computational SemanticsUnderstanding Mental States in Natural LanguageWei ChenLanguage Technologies Institute, Carnegie Mellon UniversityPittsburgh, PA 15213, USAweichen@cs.cmu.eduAbstractUnderstanding mental states in narratives is an important aspectof human language comprehension.
By ?mental states?
we refer tobeliefs, states of knowledge, points of view, and suppositions, all ofwhich may change over time.
In this paper, we propose an approachfor automatically extracting and understanding multiple mental statesin stories.
Our model consists of two parts: (1) a parser that takesan English sentence and translates it to some semantic operations; (2)a mental-state inference engine that reads in the semantic operationsand produces a situation model that represents the meaning of thesentence.
We present the performance of the system on a corpus ofchildren stories containing both fictional and non-fictional texts.1 IntroductionNatural language involves statements that carry distinct world-views.
Byworld-views we refer to states of belief, supposition, intention, advice, per-ceived reality, as well as situations expressed by tenses in natural languagesuch as past, present and future.
In this paper, we call these world-views?mental states?.
Mental states are common phenomena.
They span variousdomains of natural language.
Sentence (1a) and sentence (1b) are examplesdrawn from two different domains: online news articles and fairy tales.
(1) a.
The police believe the thieves were trying to steal a solar panelfrom Sarah?s tin roof.11Excerpt from BBC online news: http://news.bbc.co.uk/1/hi/world/africa/7609872.stm61b.
She (little red-cap) was surprised to find the cottage-door standingopen.2Both of these two sentences involve multiple mental states, which may benested in one another.
Sentence (1a) involves the police?s belief and theintention of the thieves in the police?s belief; sentence (1b) contains littlered-cap?s old belief and her updated belief, both of which may be differentfrom the reality.
Since the information in mental states is rich and oftenimportant, we need some processing technique to extract that informationand understand it.There are two problems in extracting and understanding mental states.First, extracting mental states from text requires recognizing linguistic pat-terns of mental states.
Related problems such as subjectivity recognitionhave been studied intensively in the natural language processing commu-nity.
The problem covers various aspects of the ?private state frame?
[10],including recognizing private states, the sources of private states, the in-tensity and types of attitudes, among many others.
Second, mental statesextracted from natural language need to be encoded in some representationform and can be retrieved for further inference.
There exists many systemsthat implements nested evolving beliefs (e.g.
[1]), but they generally lackedthe ability to draw inference directly from natural language.In this paper, we propose an approach to extract and represent mentalstates based on different mental contexts such as one?s belief, intention andsupposition.
Our approach utilizes the mental spaces theory [4] in cognitivelinguistics.
Our goal for the mental state extraction step is to identify space-builders that establish new mental contexts (or spaces) or the ones that referto an existing mental context.
The main body of our space-builders consistsof agent and psych-term pairs such as ?the police believe?
(refers to thepolice?s belief context) and ?little red-cap was surprised?
(refers to multiplebelief contexts of little red-cap)3.
Different objects and propositions are thenbundled in these mental contexts.
In the mental state understanding step,the mental contexts are instantiated and maintained in a context network,where inference rules are applied within and across those contexts.The rest of this paper is organized as follows.
Section 2 provides ahigh-level overview of our implemented mental state understanding system.2Excerpt from ?The Little Red-Cap?
in Margaret Hunt?s translation of the GrimmsFairy Tales.
In some other translations, the story is also called ?Little Red Riding Hood?.3Fauconnier (1985) covers a much broader set of space-builders including prepositionalphrases (?from her point of view?
), connectives (?if ... then ...?
), and subject-verb com-binations (?she thinks?).
Our current system only deals with the last category.62Figure 1: Overview of the mental state understanding systemSimple examples will be presented to demonstrate the input and outputof the system.
Section 3 and section 4 introduces our parser for mentalstate extraction and the inference engine for mental state representation,respectively.
Section 5 discusses evaluation results on fictional and non-fictional children stories.2 System Overview2.1 System ComponentsSimilar to many story comprehension systems (e.g.
[8]), our system con-sists of two parts.
A parser combines several natural language processingcomponents to extract useful information from raw text.
The informationis then integrated and filled into psych-term-argument templates to gener-ate an intermediate semantic form called mental operation.
The inferenceengine translates mental operations into a situation model represented by asemantic network.
During this process, it fills in non-literal semantic infor-mation and maintains the situation model by a set of inference rules.
Figure1 shows the general structure of the system.2.2 Example OutputTo provide a general idea of what the system does, we temporarily treat itas a black box and use concrete examples to demonstrate its function.
The63input to the system is a piece of raw English text.
The system processesone sentence at a time and outputs a situation model which represents themental states of the characters appeared in the story.Figure 2(a) and 2(b) show the semantic networks generated by our sys-tem given input sentences (1a) and (1b), respectively.
As shown in thefigure, our semantic representation of mental states is a set of mental con-texts attached to different characters.
Any inference or retrieval is done withrespect to a specific context.
Figure 2(a) shows a structured representationof the nested mental states in sentence (1a).
We read the representation as:In the police?s belief, there is the thieves?
intention, in which the thieves tryto steal a solar panel from Sarah?s tin roof.
Figure 2(b) shows a situationwhere little red-cap?s mental image of the reality changes.
(a) The semantic representation of (1a).
(b) The semantic representation of (1b).Figure 2: The output models.
Arrows with big solid heads represent ?sub-context?
relation.
Arrows with dotted lines represent ?in-context?
relation.Arrows with solid lines represent ?property?
relation.
Double-headed arrowsrepresent ?equals?
relation.According to these models, our system can generate and answer yes-no ques-tions like4:(2) a.
Question: Do the police believe that the thieves were trying to steala solar panel from Sarah?s tin roof?Answer: Yes.b.
Question: In reality, were the thieves trying to steal a solar panelfrom Sarah?s tin roof?4The questions are generated in the form of Scone language (a language used in theScone knowledge-base), not English.
We translate Scone language into English for thepurpose of illustration.64Answer: Not sure5.c.
Question: Before little red-cap was surprised, did she think thatthe cottage door was open?Answer: No.d.
Question: Does little red-cap think the cottage door is open now?Answer: Yes.3 Integrated ParserThe parsing procedure consists of three stages: pre-processing, psych-term-argument parsing, and statement building.3.1 Pre-processingThe pre-processing component consists of a sentence detector, a tokenizer,a chunker, and an anaphora annotator.
We use the OpenNLP6toolkit toperform the first three tasks.
Then we apply an in-house anaphora annota-tor to annotate pronouns in the text.
The anaphora annotator implementsa modified version of Strube?s S-List and his coreference algorithm [9].
TheS-List is a datastructure that maintains a list of sorted candidate entities foranaphora resolution.
Following Strube, a set of ranking constraints is usedfor modeling readers?
attentions.
The constraints include the reader-old andreader-new discourse entitie labels (?brand new?, ?unused?, and ?evoked?
).Our features used for anaphora resolution include number/gender agree-ments, binding constraints, types of nouns (proper or common) and partialstring matching.
Our implementation is mostly developed on children sto-ries, in which there are many instances where the gender information fora proper noun is missing.
For example, the program does not know littlered-cap?s gender the first time it sees the name in the story.
But we allowthat information to be filled in as the annotator reads the text.
Also, somecommon features such as animacy and named entity class are not includedin our feature set because they do not adapt well to this kind of narratives.3.2 Psych-term-argument ParsingThe goal for the psych-term-argument parsing process is to produce flatmental operations (as opposed to structured mental operations which will be5?Not sure?
means ?neither ?yes?
nor ?no?
?.6http://opennlp.sourceforge.net/65generated by the statement builder).
After pre-processing, the ASSERT [7]semantic role labeler is used to annotate each sentence with PropBank [6] ar-gument labels.
The most frequently used argument labels include TARGET(for psych-terms), ARG0 (for agents), ARG1 (for patients or propositions),ARG2 (for patients or propositions when ARG1 is absent), ARG-MOD (formodal verbs) and ARG-NEG (for negations).Although every sentence is processed, the system only looks for thosethat contain the pre-defined psych-terms.
The set of psych-terms are cho-sen from a larger set of mental expressions drawn automatically from theWordNet alng the synset (sets of synonyms) links.
The seed words usedfor collecting mental expressions from the WordNet contains 6 mental verbs:?think?, ?want?, ?pretend?, ?confess?, ?surprise?
and ?realize?.
For eachmental expression returned by WordNet, we restrict the next search depthto 3.
Using this method, WordNet returns 238 different verbs and phrases,among which we choose 42 psych-terms that are relatively less ambiguousfor our initial system development.
The 42 psych-terms also include modalverbs such as ?will?
and ?must?.
These psych-terms are matched to thetext annotated by ASSERT.
Parses like (3a) and (3b) are then extracted7.
(3) a.
0: [ARG0 The police] [TARGET believe ] [ARG1 the thieves weretrying to steal a solar panel from Sarah ?s tin roof]b.
0: The police believe [ARG0 the thieves] were [TARGET trying ][ARG1 to steal a solar panel from Sarah ?s tin roof]A set of grammar rules are used to map the target verbs and their argumentsto mental operations like (4a) and (4b).
At the implementation level, themental operations are Lisp functions to be evaluated by the mental stateinference engine.
(4) a.
(new-single-modal {The police} ?
( (new-statement {the thieves weretrying to steal a solar panel from Sarah ?s tin roof})) {belief})b.
(new-single-modal {the thieves} ?
((new-statement {to steal a solarpanel from Sarah ?s tin roof})) {intention})3.3 Building Statement StructuresThe psych-term-argument parsing components generate a set of mental op-erations.
However, not all of these operations ought to be evaluated by the7We have 2 statement checking rules to correct possible ASSERT output errors onpropositions.66inference engine.
In general, mental operations generated from complemen-tizer phrases should not be evaluated.
Instead, they should be passed totheir parent operations as proposition arguments.
For example, mental op-eration (4b) comes from the complemetizer phrase ?the thieves were tryingto steal a solar panel from Sarah?s tin roof?.
If this operation is evaluatedin the inference engine, we will not get the same semantic representation asin Figure 2.
Instead, the system will erroneously judge the proposition ?thethieves were trying to steal a solar panel from Sarah?s tin roof?
to be truein reality, while the correct representation is to make it true in the police?sbelief.
To avoid such errors, (4b) is passed to its parent mental operation(4a) and made an argument of (4a), which ensures that (4b) will be evalu-ated under the police?s belief.
The product of this process is a structuredmental operation (5).
And finally, only (5) is sent to the inference engine.In summary, the goal of the statement builder is to build and evaluate thecorrect mental operations.
(5)(new-single-modal {The police} ?
( (new-single-modal {the thieves}?
((new-statement {to steal a solar panel from Sarah ?s tin roof})){intention})) {belief})To build a structured mental operation, each candidate mental operationis stored in a list.
A topological sort is performed based on the complemen-tizer phrase relationships among different mental operations.
Such relationsare found through argument span check.
That is, if some arguments of oper-ation 1 are all found in operation 2?s proposition argument, we will assumeoperation 1 is operation 2?s child operation.
After the topological sort, thehead of the list stores the element mental operations which do not havechildren operations, and the tail of the list stores the mental operation thatcovers the whole sentence.
Then the operations are fed into their parentoperations one by one along the list.
Finally, those operations that have noparents are sent to the inference engine.4 Mental State Inference EngineIn this section, we briefly explain the structure and mechanisms of the mentalstate inference engine.674.1 Context Activation MechanismThe mental state inference engine is built on top of the Scone knowledge-base (KB) system.
Scone is designed to be a practical KB with emphasis onits efficiency of the most commonly used operations for search and inference.Regarding these goals, Scone provides default reasoning with exceptions.
Aswe have shown, Scone can be viewed as a semantic network representation,with nodes representing entities and links representing relations or state-ments tied to these entities.
At a higher level, the types in Scone may beviewed as frames or descriptions.
A multi-context and context activationmechanism is designed into Scone using the marker-passing algorithm [3].In this paper, a context is used to represent the state of mental attitudes.
InScone, the context nodes are treated as the other nodes in that they are alsotied into their own inheritance hierarchy using ?sub-context?
links.
How-ever, the ?sub-context?
relation represented by inheritance is a mechanicalone.
The relation between the two contexts is neither ?is-a?
nor ?part-of?,but something more like ?is a clone of, modulo explicit changes?.
Contextscan also be used to represent the state of a changing world at different times.Each of the contexts represents a mental attitude at a specific time; it beginsas a clone of the state in the previous time-step, and then we can add orremove a few items if necessary.4.2 Mental Context RepresentationIn general, the mental context model tracks the changes of the mental state[2].
At each time point, it builds a mental context network that representsnested mental states.
The input to the model is a list of mental contextoperations extracted from text.
Each of the operations corresponds to onepsych-term.
The complex semantics of a psych-term is factored into a setof atomic operations on single mental contexts through semantic decom-position.
These contexts are organized into a hierarchical structure, whichconstitutes a simplified representation of human memory.The semantics of psych-terms are projected onto the context networkthrough semantic decomposition8.
For example, one sense of the word ?pre-tend?
can be represented as ?X is not present in reality and person P1?sbelief, but P1 wants person P2 to believe it?
(these semantic definitionsare restricted to mental contexts).
This example involves several contexts:the reality, the belief of P1, the intention of P1, the belief of P2 under theintention of P1, as well as the before context and the after context of ?pre-8Recent work on verb entailment can be found in [5].68tend?.
Note that the mental operations are higher order, so there can beother psych-terms (e.g.
?want?)
embedded the definition of ?pretend?.Mental operations update the mental context network in two aspects.First, they build a context inheritance chain which represents the evolutionof a single mental context at different time steps.
For example, in the ?LittleRed-Cap?
story, little red-cap has different belief contexts at different time.By default, each of the newly updated versions of one?s belief would inheritfrom his/her most recent belief.
Second, the mental operations are usedto build a hierarchical context structure which organizes multiple types ofmental context according to events and agents.
Figure 2 illustrates threebasic aspects of the context structure:1.
By default, the mental contexts inherit from a general knowledge con-text which represents the general background knowledge of a story.2.
Mental contexts can be organized by events.
A typical mental eventhas an agent whose mental contexts are changed as an effect of theevent.
Each psych-term would be mapped to one of the mental events.When we retrieve an event, a set of related contexts of that event wouldalso be retrieved.3.
The mental contexts are environment-sensitive.
For example, thethieves?
real intention can be different from the police?s belief of theirintention.In our representation, different instances of the mental contexts areorganized in a dynamic context structure.
We could then constrain thebehaviors of different mental contexts under different mental events usinginter-contextual rules.
Once a mental event (e.g.
?little red-cap was sur-prised?)
triggers, the related mental contexts would check and modify theirown structures and contents based on the new information.
Usually thisself-adjustment can be achieved by a realization of a difference between theexternal world and the belief, assumption or expectation.
According to this,newly updated mental contexts would be constructed.5 Evaluation ResultsWe use 513 children stories from Project LISTEN?s9reading tutor storydatabase for system evaluation.
This corpus contains 213 fictional articles9http://www.cs.cmu.edu/?listen69and 300 non-fictional (or informational) articles.
From these, our system ex-tracts 1181 mental state expressions in fictions and 413 in non-fictions.
Af-ter the parsing stage, 60.80% of the fictional and 62.71% of the non-fictionalmental state expressions are fully parsed (i.e.
there is no empty argumentsfor the mental operations) and sent to the inference engine.
After processingof the mental operations, the system automatically generates 1437 yes-noquestions and answers for fictional texts, and 518 for non-fictional texts.The question-answer pairs are generated by traversing newly visited mentalcontexts and statements/objects bundled in those contexts immediately af-ter each mental operation is evaluated.
These questions and answers are allin similar forms as the examples demonstrated in section 2.2.We do a careful evaluation on 431 questions for fictional texts and 155for non-fictional text that are randomly selected from the question-answerpool.
Since both the questions and answers come from the situation modelstored in the system, an error occurred in either the question or the answercounts for an incorrect example.Table 1 shows the error rates of each error category for both fictionaland non-fictional stories.
The second column (?argument error?)
gives thepercentage of incorrect question-answer pairs caused by wrong argumentsin the mental operations.
This type of error comes from a misinterpreta-tion of the ASSERT output.
For example, we assume that ARG1 indicatesa proposition for psych-terms if it is not a noun phrase.
But ?she [TAR-GET wanted ] [ARG1 so much]?
is an exception of this assumption.
Thethird column (?statement error?)
gives the percentage of incorrect examples(usually questions) caused by incomplete or unnecessary statements in themental contexts.
This type of errors results from inaccurate ASSERT out-puts that are not captured by our statement checking rules.
Space-buildererror (the fourth column) refers to the cases in which the mental contextsare not correctly constructed (this results in incorrect answers).
This usu-ally happens when there is a mental space that has been neglected by oursystem.
For example, when given sentence ?Dig for it if you want the gold?,our system would only look at ?you want the gold?
and treat it as a validstatement, without noticing that it is embedded in an if-clause which in-dicates another mental space.
Ambiguity error (the fifth column) refers toerrors cause by the lexical ambiguity of psych-terms (e.g.
the word ?will?
in-dicates future tense in ?Pilly will go to school tomorrow?, but not in ?Somesharks will eat just about anything?).
Negation error (the sixth column)occurs when there is a negation that has been neglected (this results in anincorrect answer).
For example ?It will do him no good, neither will it helpanybody else?
means ?It will not help anybody else?.
But this negation is70Table 1: Evaluation ResultsArgument Statement Space- Ambiguity Negation TotalErrors Errors Builder Errors Errors ErrorsErrorsFictions 6.26% 8.12% 8.35% 4.64% 4.64% 26.68%Non-Fics 1.29% 2.58% 7.10% 7.74% 1.29% 19.35%not captured by our system.
Note that since the five types of errors listed inTable 1 are not mutually exclusive, the error rates in each row of the tabledo not sum up to the total error rate of question-answer pairs generated bythe system.
Anaphora error has been counted separately since this type oferror alone has a significant effect on system performance, and the anaphoraannotator is relatively independent with our task compared to the other in-house components.
During evaluation, we only apply anaphora resolutionto pronouns (the word ?it?
is not counted).
In the same question-answerpool, we observe the anaphora error rate of 19.49% on fictions and 17.42%on non-fictions.6 ConclusionThis paper presents an implemented system that understands mental statesexpressed in narratives.
The system extracts and processes mental statesby mapping psych-terms and their arguments to mental contexts stored ina situation model.
The system is evaluated by automatically generatedquestion-answer pairs.
Future directions include extending the system to abroader set of psych-terms and patterns to cover more mental states fromnatural language.
Meanwhile, we will explore the two-way interaction be-tween the parser and the inference engine to refine the overall processingaccuracy.7 AcknowledgementsThe author would like to thank Scott Fahlman and Jack Mostow for manyhelpful discussions.
The research reported here was supported in part bythe Institute of Education Sciences, U.S. Department of Education, throughGrant R305B070458 to Carnegie Mellon University.
The opinions expressed71are those of the authors and do not necessarily represent the views of theInstitute.References[1] Afzal Ballim and Yorick Wilks.
Beliefs, stereotypes and dynamic agentmodeling.
User Modeling and User-Adapted Interaction, pages 33?65,1991.
[2] Wei Chen and Scott E. Fahlman.
Modelling mental context and theirinteractions.
In AAAI Fall Symposium on Biologically Inspired Cogni-tive Architectures, 2008.
[3] Scott E. Fahlman.
Marker-passing inference in the scone knowledge-based system.
In KSEM?06.
Springer-Verlag, 2006.
[4] Gilles Fauconnier.
Mental Spaces: Aspects of Meaning Construction.MIT Press, Cambridge, MA, USA, 1985.
[5] Lauri Karttunen.
Word play.
Computational Linguistics, 33(4):443?467, 2007.
[6] Martha Palmer, Daniel Gildea, and Paul Kingsbury.
The propositionbank: An annotated corpus of semantic roles.
Computational Linguis-tics, 31(1):71?106, 2005.
[7] Sameer Pradhan, Kadri Hacioglu, Valerie Krugler, Wayne Ward,James H. Martin, and Daniel Jurafsky.
Support vector learning for se-mantic argument classification.
Machine Learning, 60(1-3):11?39, 2005.
[8] C. K. Riesbeck and R. C. Schank.
Comprehension by computer:Expectation-based analysis of sentences in context.
Technical Re-port 78, Yale Computer Science Department, 1976.
[9] Michael Strube.
Never look back: an alternative to centering.
InProceedings of the 17th international conference on Computational lin-guistics, pages 1251?1257, Morristown, NJ, USA, 1998.
Association forComputational Linguistics.
[10] Janyce Wiebe, Theresa Wilson, and Claire Cardie.
Annotating expres-sions of opinions and emotions in language.
In Language Resources andEvaluation, volume 39, pages 165?210, 2005.72
