Proceedings of the 8th Workshop on Language Technology for Cultural Heritage, Social Sciences, and Humanities (LaTeCH) @ EACL 2014, pages 71?79,Gothenburg, Sweden, April 26 2014.c?2014 Association for Computational LinguisticsSocial and Semantic Diversity:Socio-semantic Representation of a Scientific CorpusElisa OmodeiLATTICE and ISC-PIFCNRS & ENS & U. Sorbonne Nouvelle1 rue Mauriece Arnoux92120 Montrouge Franceelisa.omodei@ens.frYufan GuoUniversity of WashingtonComputer ScienceEngineeringBox 352350 Seattle, WA 98195-2350yufanguo@cs.washington.eduJean-Philippe CointetINRA Sens and ISC-PIFCit?e Descartes, 5 boulevard Descartes77454 Marne-la-Vall?ee Cedex France75013 Paris Francejphcoi@yahoo.frThierry PoibeauLATTICECNRS & ENS & U. Sorbonne Nouvelle1 rue Mauriece Arnoux92120 Montrouge Francethierry.poibeau@ens.frAbstractWe propose a new method to extract key-words from texts and categorize thesekeywords according to their informationalvalue, derived from the analysis of the ar-gumentative goal of the sentences they ap-pear in.
The method is applied to the ACLAnthology corpus, containing papers onthe computational linguistic domain pub-lished between 1980 and 2008.
We showthat our approach allows to highlight inter-esting facts concerning the evolution of thetopics and methods used in computationallinguistics.1 IntroductionBig data makes it possible to observe in vivo thedynamics of a large number of different domains.It is particularly the case in the scientific field,where researchers produce a prolific literature butalso other kinds of data like numbers, figures, im-ages and so on.
For a number of domains, largescientific archives are now available over severaldecades.This is for example the case for computationallinguistics.
The ACL Anthology contains morethan 24,500 papers, for the most part in PDF for-mat.
The oldest ones date back to 1965 (first edi-tion of the COLING conference) but it is mostlyafter 1980 that data are available in large volumesso that they can be exploited in evolution studies.The volume of data increases over time, whichmeans there is a wide diversity in the number ofpapers available depending on the given period oftime.
There are similar archives for different do-mains like, e.g.
physics (the APS database pro-vided by the American Physical Society) or thebio-medical domain (with Medline).These scientific archives have already givenbirth to a large number of different pieces of work.Collaboration networks have for example been au-tomatically extracted so as to study the topologyof the domain (Girvan and Newman, 2002) orits morphogenesis (Guimera et al., 2005).
Ref-erencing has also been the subject of numerousstudies on inter-citation (Garfield, 1972) and co-citation (Small, 1973).
Other variables can betaken into account like the nationality of the au-thors, the projects they are involved in or the re-search institutions they belong to, but it is the anal-ysis of the textual content (mostly titles, abstractsand keywords provided with the papers) that haveattracted the most part of the research in the areasince the seminal work of Callon (Callon et al.,1986; Callon et al., 1991).In this paper, our goal is to investigate the evo-lution of the field of computational linguistics,which means that text will play a crucial role.
Tex-tual analysis is then mixed with the study of indi-vidual trajectories in the semantic space: our goalis to propose possible avenues for the study of thedynamics of innovation in the computational lin-71guistics domain.The ACL Anthology has been the subject ofseveral studies in 2012, for the 50 years of theACL.
More specifically, a workshop called ?Re-discovering 50 Years of Discoveries?
was orga-nized to examine 50 years of research in NLP(but, for the reasons given above, the workshopmostly focused on the evolution of the domainsince 1980).
This workshop was also an oppor-tunity to study a large scientific collection with re-cent NLP techniques and see how these techniquescan be applied to study the dynamics of a scientificdomain.The analysis of this kind of data is generallybased on the extraction of key information (au-thors, keywords) and the discovery of their rela-tionships.
The data can be represented as a graph,therefore graph algorithmics can be used to studythe topology and the evolution of the graph of col-laborations or the graph of linked authors.
It isthus possible to observe the evolution of the do-main, check some hypotheses or common assump-tions about this evolution and provide a strong em-pirical basis to epistemology studies.The paper ?Towards a computational History ofthe ACL: 1980-2008?
is very relevant from thispoint of view (Anderson et al., 2012).
The au-thors try to determine the evolution of the mainsub-domains of research within NLP since 1980and they obtain very interesting results.
For ex-ample, they show the influence of the Americanevaluation campaigns on the domain: when a USagency sponsored a sub-domain of NLP, one canobserve a quick concentration effect since a widenumber of research groups suddenly concentratedtheir efforts on the topic; when no evaluation cam-paign was organized, research was much morewidespread across the different sub-domains ofNLP.
Even if this is partially predictable, it wasnot obvious to be able to show this in a collectionof papers as large as the ACL Anthology.Our study has been profoundly influenced bythe study by Anderson et al.
However, our goalhere is to characterize automatically the keywordsbased on the information they carry.
We will thuscombine keyword extraction with text zoning soas to categorize the keywords depending on theircontext of use.The rest of the paper is organized as follows.We first present an analysis of the structure of ab-stracts so as to better characterize their content bymixing keyword extraction with text zoning.
Weshow how these techniques can be applied to theACL Anthology in order to examine specific facts,more specifically concerning the evolution of thetechniques used in the computational linguisticsdomain.2 A Text Zoning Analysis of the ACLAnthologyThe study of the evolution of topics in large cor-pora is usually done through keyword extraction.This is also our goal, but we would like to be ableto better characterize these keywords and make adifference, for example, between keywords refer-ring to concepts and keywords referring to meth-ods.
Hence, the context of these keywords seemshighly important.
Consequently, we propose touse Text Zoning that can provide an accurate char-acterization of the argumentative goal of each sen-tence in a scientific abstract.2.1 Previous workThe first important contributions in text zoning areprobably the experiments by S. Teufel who pro-posed to categorize sentences in scientific papers(and more specifically, in the NLP domain) ac-cording to different categories (Teufel, 1999) likeBKG: General scientific background, AIM: State-ments of the particular aim of the current paper orCTR: Contrastive or comparative statements aboutother work.
This task is called Rhetorical zoningor Argumentative zoning since the goal is to iden-tify the rhetoric or argumentative role of each sen-tence of the text.The initial work of Teufel was based on themanual annotation of 80 papers representing thedifferent areas of NLP (the corpus was made ofpapers published within the ACL conferences orComputational Linguistics).
A classifier was thentrained on this manually annotated corpus.
Theauthor reported interesting results despite ?a 20%diference between [the] system and human perfor-mance?
(Teufel and Moens, 2002).
The learningmethod used a Naive Bayesian model since moresophisticated methods tested by the author did notobtain better results.
Teufel in subsequent publica-tions showed that the technique can be used to pro-duce high quality summaries (Teufel and Moens,2002) or precisely characterize the different cita-tions in a paper (Ritchie et al., 2008).The seminal work of Teufel has since then given72rise to different kinds of works, on the one handto refine the annotation method, and on the otherhand to check its applicability to different scien-tific domains.
Concerning the first point, researchhas focused on the identification of relevant fea-tures for classification, on the evaluation of dif-ferent learning algorithms for the task and moreimportantly on the reduction of the volume of textto be annotated.
Concerning the second point, itis mostly the biological and bio-medical domainsthat have attracted attention, since scientists inthese domains often have to access the literature?vertically?
(i.e.
experts may need to have accessto all the methods and protocols that have beenused in a specific domain) (Mizuta et al., 2006;Tbahriti et al., 2006).Guo has since developed a similar trend of re-search to extend the initial work of Teufel (Guoet al., 2011; Guo et al., 2013): she has tested alarge list of features to analyze the zones, evalu-ated different learning algorithms for the task andproposed new methods to decrease the number oftexts to be annotated.
The features used for learn-ing are of three categories: i) positional (locationof the sentence inside the paper), ii) lexical (words,classes of words, bigrams, etc.
are taken into con-sideration) and iii) syntactic (the different syntac-tic relations as well as the class of words appear-ing in subject or object positions are taken into ac-count).
The analysis is thus based on more fea-tures than in Teufel?s initial work and requires aparser.2.2 Application to the ACL Anthology corpusIn our experiment, we only used the abstracts ofthe papers.
Our hypothesis is that abstracts con-tain enough information and are redundant enoughto study the evolution of the domain.
Taking intoconsideration the full text would probably give toomany details and thus introduce noise in the anal-ysis.The annotation scheme includes five differentcategories, which are the following: OBJEC-TIVE (objectives of the paper), METHOD (meth-ods used in the paper), RESULTS (main results),CONCLUSION (conclusion of the paper), BACK-GROUND (general context), as in (Reichart andKorhonen, 2012).
These categories are also closeto those of (Mizuta et al., 2006; Guo et al., 2011;Guo et al., 2013) and have been adapted to ab-stracts (as opposed to full text1).
It seems relevantto take into consideration an annotation schemethat has already been used by various authors sothat the results are easy to compare to others.Around one hundred abstracts from the ACLAnthology have then been manually annotated us-ing this scheme (?500 sentences; ACL abstractsare generally quite short since most of them arerelated to conference papers).
The selection of theabstracts has been done using stratified samplingover time and journals, so as to obtain a represen-tative corpus (papers must be related to differentperiods of time and different sub-areas of the do-main).
The annotation has been done accordingto the annotation guideline defined by Y. Guo, es-pecially for long sentences when more than onecategory could be applied (preferences are definedto solve complex cases2).The algorithm defined by (Guo et al., 2011) isthen adapted to our corpus.
The analysis is basedon positional, lexical and syntactic features, as ex-plained above.
No domain specific informationwas added, which makes the whole process easyto reproduce.
As for parsing, we used the C&Cparser (James Curran and Stephen Clark and JohanBos, 2007).
All the implementation details can befound in (Guo et al., 2011), especially concerningannotation and the learning algorithm.
As a result,each sentence is associated with a tag correspond-ing to one of the zones defined in the annotationscheme.2.3 Results and DiscussionIn order to evaluate the text zoning task, a num-ber of abstracts were chosen randomly (?300 sen-tences that do not overlap with the training set).CONCLUSION represented less than 3% of thesentences and was then dropped for the rest ofthe analysis.
The four remaining zones are un-equaly represented: 18.05 % of the sentences re-fer to BACKGROUND, 14.35% to OBJECTIVE,14.81 % to RESULT and 52.77 % to METHOD.Just by looking at these numbers, one can see how1The categories used in (Teufel, 1999) were not relevantsince this model focused on full text papers, with a specialemphasis on the novelty of the author?s work and the attitudetowards other people?s work, which is not the case here.2The task is to assign the sentence only a single category.The choice of the category should be made according to thefollowing priority list: Conclusion > Objective > Result >Method> Background.
The only exception is that when 75%or more of the sentence belongs to a less preferred category,then that category will be assigned to the sentence.73Table 1: Result of the text zoning analysis (preci-sion)Category PrecisionObjective 83,87 %Background 81,25 %Method 71,05 %Results 82,05 %Figure 1: An abstract annotated with text zoninginformation.
Categories are indicated in bold face.Most of errors in Korean morphological analysis andPOS ( Part-of-Speech ) tagging are caused by unknownmorphemes .
BACKGROUNDThis paper presents a generalized unknown morphemehandling method with POSTAG(POStech TAGger )which is a statistical/rule based hybrid POS taggingsystem .
OBJECTIVEThe generalized unknown morpheme guessing is basedon a combination of a morpheme pattern dictionarywhich encodes general lexical patterns of Koreanmorphemes with a posteriori syllable tri-gram estimation.
METHODThe syllable tri-grams help to calculate lexical proba-bilities of the unknown morphemes and are utilized tosearch the best tagging result .
METHODIn our scheme , we can guess the POS?s of unknownmorphemes regardless of their numbers and positionsin an eojeol , which was not possible before in Koreantagging systems .
RESULTSIn a series of experiments using three different domaincorpora , we can achieve 97% tagging accuracy regard-less of many unknown morphemes in test corpora .RESULTSmethodological issues are important for the do-main.We then calculate for each of the categories, thepercentage of sentences that received the right la-bel, which allows us to calculate precision.
Theresults are given in table 1.These results are similar to the state of the art(Guo et al., 2011), which is positive taking intoconsideration the small number of sentences an-notated for training.
The diversity of the featuresused makes it easy to transfer the technique fromone domain to the other without any heavy anno-tation phase.
Results are slightly worse for theMETHOD category, probably because this cate-gory is more diverse and thus more difficult to rec-ognize.
The fact that NLP terms can refer either toobjectives or to methods also contributes render-ing the recognition of this category more difficult.Figure 1 shows an abstract annotated by the textzoning module (the paper is (Lee et al., 2002): ithas been chosen randomly between those contain-ing the different types of zones).
One categoryis associated with each sentence but this is some-times problematic: for example the fact that a hy-brid method is used is mentioned in a sentence thatis globally tagged as OBJECTIVE by the system.However, sentences tagged as METHOD containrelevant keywords like lexical pattern or tri-gramestimation, which makes it possible to infer thatthe approach is hybrid.
One can also spot someproblems with digitization, which are typical ofthis corpus: the ACL Anthology contains automat-ically converted files to PDF, which means textsare not perfect and may contain some digitizationerrors.3 Contribution to the Study of theEvolution ACL AnthologyAs said above, we are largely inspired by (Ander-son et al., 2012).
We think the ACL Anthologyis typical since it contains papers spanning overmore than 30 years: it is thus interesting to use itas a way to study the main evolutions of the com-putational linguistics domain.
The method can ofcourse also be applied to other scientific corpora.3.1 Keyword extraction and characterizationThe first step consists in identifying the main key-words of the domain.
We then want to more pre-cisely categorize these keywords so as to identifythe ones specifically referring to methods for ex-ample.
From this perspective, keywords appear-ing in the METHOD sections are thus particularlyinteresting for us.
However, one major problem isthat there is no clear-cut difference between goalsand methods in NLP since most systems are madeof different layers and require various NLP tech-niques.
For example, a semantic analyzer may usea part-of-speech tagger and a parser, which meansNLP tools can appear as part of the method.Keyword extraction aims at automatically ex-tracting relevant keywords from a collection oftexts.
A popular approach consists in first extract-ing typical sequences of tags that are then filteredaccording to specific criteria (these criteria can in-clude the use of external resources but they aremore generally based on scores mixing frequencyand specificity (Bourigault and Jacquemin, 1999;Frantzi and Ananiadou, 2000)).
In this study, wevoluntarily used a minimal approach for keywordextraction and filtering since we want to keep most74Table 2: Most specific keywords found in the METHOD sections.MethodsCategory Method N-gramsMachine learningBayesian methods baesyanVector Space model space model, vector space, cosineGenetic algorithms genetic algorithmsHMM hidden markov models, markov modelCRF conditional random fieldsSVM support vector machinesMaxEnt maximum entropy model, maximum entropy approach, maximum entropyClustering clustering algorithm, clustering method, word clusters, classification problemSpeech & Mach.
Trans.Language models large-vocabulary, n-gram language model, ViterbiParallel Corpora parallel corpus, bilingual corpus, phrase pairs, source and target languages, sentence pairs, word pairs,source sentenceAlignment phrase alignment, alignment algorithm, alignment models, ibm model, phrase translation, translationcandidates, sentence alignmentNLP MethodsPOS tagging part-of-speech tagger, part-of-speech tagsMorphology two-level morphology, morphological analyzer, morphological rulesFST finite-state transducers, regular expressions, state automata, rule-based approachSyntax syntactic categories, syntactic patterns, extraction patternsDependency parsing dependency parser, dependency graphs, prague dependency, dependency treebank, derivation trees, parsetreesParsing grammar rules, parser output, parsing process, parsed sentences, transfer rulesSemantics logical forms, inference rules, generative lexicon, lexical rules, lexico-syntactic, predicate argumentApplicationsIE and IR entity recognition, answer candidates, temporal information, web search, query expansion, google, userqueries, keywords, query terms, term recognitionDiscourse generation component, dialogue acts, centering theory, lexical chains, resolution algorithm, generationprocess, discourse model, lexical choiceSegmentation machine transliteration, phonological rules, segmentation algorithm, word boundariesWords and ResourceLexical knowledge bases lexical knowledge base, semantic network, machine readable dictionaries, eurowordnet, lexical entries,dictionary entries, lexical units, representation structures, lookupWord similarity word associations, mutual information, semantic relationships, word similarity, semantic similarity,semeval-2007, word co-occurrence, synonymyCorpora brown corpus, dialogue corpus, annotation scheme, tagged corpusEvaluation Evaluation score, gold standard, evaluation measures, estimation methodCalculation & complexity Software tool development, polynomial time, software tools, series of experiments, system architecture, runtime,programming languageConstraints relaxation, constraint satisfaction, semantic constraintsof the information for the subsequent text zoningphase.
We thus used NLTK for part-of-speech tag-ging and from this result extracted the most com-mon noun phrases.
We used a pre-defined setof grammatical patterns to extract noun phrasesdefined as sequences of simple sequences (e.g.adjectives + nouns, ?phrase pairs?, ?dependencygraph?, etc.)
possibly connected to other such pat-terns through propositions to form longer phrases(e.g.
?series of experiments?).
Only the nounphrases appearing in more than 10 papers are keptfor subsequent processing.Candidate keywords are then ranked per zone,according to their specificity (the zone they arethe most specific of) .
Specificity corresponds tothe Kolmogorov-Smirnov test that quantifies a dis-tance between the empirical distribution functionsof two samples.
The test is calculated as follows:D = maxx|SN1(x)?
SN2(x)| (1)where SN1(x) et SN2(x) are the empirical distri-bution function of the two samples (that corre-spond in our case to the number of occurrencesof the keyword in a given zone, and to the totalnumber of occurrences of all the keywords in thesame zone, respectively) (Press et al., 2007).
Ahigh value of D for a given keyword means that itis highly specific of the considered zone.
At theopposite, a low value means that the keyword isspread over the different zones and not really spe-cific of any zone.The first keywords of each category are thencategorized by an expert of the domain.
For theMETHOD category, we obtain Table 2.
Logically,given our approach, the table does not contain allthe keywords relevant for the computational lin-guistics domain, but it contains the mots specificones according to the above approach.
One shouldthus not be surprised not to see all the keywordsused in the domain.3.2 Evolution of methods over timeThe automatic analysis of the corpus allows us totrack the main evolutions of the field over time.During the last 30 years, the methods used havechanged to a large extent, the most notable fact be-ing probably the generalization of machine learn-ing methods since the late 1990s.
This is outlinedby the fact that papers in the domain nowadaysnearly always include a section that describes anexperiment and some results.To confirm this hypothesis, we observe the rel-ative frequency of sentences tagged as RESULTSin the papers over time.
In the figure 3, we see thatthe curve increases almost linearly from the early1980s until the late 2000s.751980 1982 1984 1986 1988 1990 1992 1994 1996 1998 2000 2002 2004 2006 200800.10.20.30.40.50.60.70.80.91NLP MethodsSemanticsParsingDependency parsingSyntaxFSTMorphologyPOS taggingYearRelativeFrequency1980 1982 1984 1986 1988 1990 1992 1994 1996 1998 2000 2002 2004 2006 200800.10.20.30.40.50.60.70.80.91ApplicationsSegmentationDiscourseIE and IRYearRelativeFrequency1980 1982 1984 1986 1988 1990 1992 1994 1996 1998 2000 2002 2004 2006 200800.10.20.30.40.50.60.70.80.91Machine LearningClusteringMaxEntSVMCRFHMMGenetic algorithmsVector Space modelBayesian methodsYearRelativeFrequency1980 1982 1984 1986 1988 1990 1992 1994 1996 1998 2000 2002 2004 2006 200800.10.20.30.40.50.60.70.80.91Speech & machine translation specificAlignmentParallel CorporaLanguage modelsYearRelativeFrequency1980 1982 1984 1986 1988 1990 1992 1994 1996 1998 2000 2002 2004 2006 200800.10.20.30.40.50.60.70.80.91ResourcesCorporaWord similarityLexical knowledge basesYearRelativeFrequency1980 1982 1984 1986 1988 1990 1992 1994 1996 1998 2000 2002 2004 2006 200800.10.20.30.40.50.60.70.80.91Calculation & ComplexityConstraintsSoftwareYearRelativeFrequencyFigure 2: Evolution of the relative frequency of the different groups of methods over time.It is also possible to make more fine-grained ob-servations, for example to follow over time the dif-ferent kinds of methods under consideration.
Theresults are shown in figure 2.
Rule based methodsand manually crafted resources are used all overthe period, while machine learning based meth-ods are more and more successful after the late1990s.
This is not surprising since we know thatmachine learning is now highly popular within thefield.
However, symbolic methods are still used,sometimes in conjunction with learning methods.The two kinds of methods are thus more comple-mentary than antagonistic.One could observe details that should bechecked through a more thorough study.
We ob-serve for example the success of dependency pars-ing in the end of the 1980s (probably due to thesuccess of the Tree Adjoining Grammars at thetime) and the new popularity of this area of re-search in the early 2000s (dependency parsing hasbeen the subject of several evaluation campaignsin the 2000s, see for example for the CONLLshared tasks from 2006 to 2009).Different machine learning methods have beenpopular over time but each of them continues to beused after a first wave corresponding to their ini-tial success.
Hidden Markov Models and n-gramsare highly popular in the 1990s, probably thanksto the experiments made by Jelinek and his col-leagues, which will open the field of statistical ma-chine translation (Brown et al., 1990).
SVM andCRF have had a more recent success as everybodyknows.We are also interested in the distribution ofthese methods between papers and authors.
Fig-ure 4 shows the average number of keywords1980 1982 1984 1986 1988 1990 1992 1994 1996 1998 2000 2002 2004 2006 200800.050.10.150.20.25ResultsYearRelativeFrequencyFigure 3: Evolution of the relative frequency ofsentences tagged as RESULTS in the abstracts ofthe papersappearing in the METHOD section of the papersover time.
We see that this number regularly in-creases, especially during the 1980s, showing pos-sibly a gradually increasing complexity of the sys-tems under consideration.Lastly, figure 5 shows the number of authorswho are specialists of one or several methods.Most of the authors just mention one method intheir papers and, logically, the curves decrease,which means that there are few authors who arereally specialists of many methods.
This resultshould be confirmed by a larger scale study tak-ing into account a larger number of keywords butthe trend seems however interesting.3.3 The dynamics of the authors in themethod spaceOne could say that the results we have reported inthe previous section are not new but rather confirmsome already well known facts.
Our method al-lows to go one step further and try to answer more76Figure 4: Evolution of the number of keywordsrelated to methods over time.1 9 8 0 2 4 6 .55R15R95R85R05R25R4esultYariltrvs Fqalncr2rvyvnYaesultYariltrvs Fqalncr4rvyvnYaesultYariltrvs Fqalncr6rvyvnYaesultYariltrvs Fqalncr.rvyvnYaesultYariltrvs Fqalncr?rvyvnYaesultYariltrvs Fqalncr15rvyvnYaesultYariltrvs Fqalncr11rvyvnYaesultYariltrvs Fqalncr19rvyvnYaesultYariltrvs Fqalncr18rvyvnYa?s?
nYrt?r?nultca?YtvtYuqt?rt?rysultYaFigure 5: Proportion of authors specialized ina given number of methods (i.e.
mentioningfrequently the name of the method in the ab-stracts), for different categories of researchers.challenging questions.
How are new methods in-troduced in the field?
Are they mainly broughtby young researchers or is it mainly confirmed re-searchers who develop new techniques (or importthem from related fields)?
Are NLP experts spe-cialized in one field or in a wide variety of differ-ent fields?These questions are of course quite complex.Each individual has his own expertise and hisown history but we think that automatic meth-ods can provide some interesting trends over time.For example, (Anderson et al., 2012) show thatevaluation campaigns have played a central roleat certain periods of time, which does not meanof course that there was no independent researchoutside these campaigns at the time.
Our goalis thus to exhibit some tendencies that could beinterpreted or even make it possible to comparethe evolution of the computational linguistics fieldwith other fields.
Out tools provide some hypothe-ses that must of course be confirmed by further ob-servations and analysis.
We do not claim that theyprovide an exact and accurate view of the domain.Genetic algorithms HMMMorphologyCorpora SVMClusteringPOStaggingBayesian methodsMaxEnt CRFAlignmentLanguage modelsVector Space model00.10.20.30.40.50.60.70.80.91Fraction of pionners that are new to the fieldFraction of authors that enter the field in those yearsFigure 6: For each ?new method?, number of ?pi-oneers?
not having published any paper before(compared to the total number of new authors dur-ing the same period of time).For this study we only take into account authorswho have published at least 5 papers in the ACLAnthology, in order to take into consideration au-thors who have contributed to the domain during aperiod of time relevant for the study.
We consideras ?pioneers?
the authors of the first 25% of pa-pers in which a keyword referring to a method isintroduced (for example, the first papers where thekeywords support vector machine or SVM appear).We then calculate, among this set of authors, theones who can be considered as new authors, whichmeans people who have not published before inthe field.
Since there are every year a large numberof new authors (who use standard techniques) wecompare the ratio of new authors using new tech-niques with the number of authors using alreadyknown techniques over the considered period.
Re-sults are visible in figure 6.Results are variable depending on the methodunder consideration but some of them seem inter-esting.
Papers with the keyword Hidden MarkovModel in the 1990s seem to be largely writtenby new comers, probably by researchers havingtested this method in related fields before (andwe know that it was the case of Jelinek?s teamwho was largely involved in speech processing, adomain not so well represented in the ACL An-thology before the 1990s.
Of course, Jelinek andcolleague were confirmed and even highly estab-lished researchers already at the beginning of the1990s).
We observe a similar patten for geneticalgorithms but the number of authors is too lim-ited to say if the trend is really meaningful.
SVMalso seem to have been popularized by new com-ers but it is not the case of language models or ofthe vector space model.
A more thorough study isof course needed to confirm and better understand7700.20.40.60.810  0.2  0.4  0.6  0.8  1CumulativeDistributionFunctionFraction of total production of author already publishedFigure 7: Distribution function of the number ofpapers already published by ?pioneers?
when theyhave published their paper on the new method,compared to the total production of their career.these results.We then do a similar experiment to try to de-termine when, during their career, researchers usenew methods.
Practically, we examine at whatpoint of their career the authors who are character-ized as ?pioneers?
in our study (what refers to thefirst authors using a new method) have publishedthe papers containing new methods (for example,if an author is one of the first who employed thekeyword SVM, has he done this at the beginningof his career or later on?).
The result is visible infigure 7 and shows that 60% of pioneers had pub-lished less than a third of their scientific produc-tion when they use the new method.
We thus ob-serve a similar set of authors between the pioneersand researchers having published so far in relatedbut nevertheless different communities.
To con-firm this result, it would be useful to study otherdomains and other corpora (in computer science,linguistics, cognitive sciences) so as to get a betterpicture of the domain, but the task is then highlychallenging.One may want then to observe the diversity ofmethods employed in the domain, especially bythe set of people called ?pioneers?
in our study.Figure 8 shows in blue the number of methodsdetected for the pioneers and in red the number ofmethods used by all the authors.We see that pioneers, when taking into consid-eration the whole set of papers in the ACL An-thology, are using a larger number of methods.They are over represented among authors using 3methods and more.
This group of people also con-tribute to a larger number of sub-areas in the do-mains compared to the set of other authors.1 2 3 4 5 6 7 800.050.10.150.20.250.30.35Pioneers proportionTotal authors proportionNumber of methods per authorProportionof authorsFigure 8: Proportion of ?pioneers?
experts in agiven number of methods compared to all the otherauthors in the corpus.4 ConclusionWe have presented in this paper an analysis of theACL Anthology corpus.
Our analysis is based onthe identification of keywords which are catego-rized according to their informational status.
Cate-gorization is done according to a Text Zoning anal-ysis of the papers?
abstracts, which provides veryrelevant information for the study.
We have shownthat coupling keyword extraction with Text Zon-ing makes it possible to observe fine grained factsin the dynamics of a scientific domain.These tools only give pieces of information thatshould be confirmed by subsequent studies.
Itis necessary to go back to the texts themselves,consult domain experts and probably the largercontext to be able to get a really accurate pic-ture of the evolution of a scientific domain.
Thismulti-disciplinary research means that to collabo-rate with people from other fields is needed, espe-cially with the history of science and epistemol-ogy.
However, the platforms and the techniqueswe have described in this paper are now availableand can be re-used for other kinds of studies, mak-ing it possible to reproduce similar experimentsacross different domains.ReferencesAshton Anderson, Dan Jurafsky, and Daniel A. McFar-land.
2012.
Towards a computational history of theacl: 1980-2008.
In Proceedings of the ACL-2012Special Workshop on Rediscovering 50 Years of Dis-coveries, pages 13?21, Jeju Island, Core.
Associa-tion for Computational Linguistics.Didier Bourigault and Christian Jacquemin.
1999.Term extraction + term clustering: An integratedplatform for computer-aided terminology.
In Pro-ceedings of the Ninth Conference on European78Chapter of the Association for Computational Lin-guistics, EACL ?99, pages 15?22.Peter F. Brown, John Cocke, Stephen A. Della Pietra,Vincent J. Della Pietra, Fredrick Jelinek, John D.Lafferty, Robert L. Mercer, and Paul S. Roossin.1990.
A statistical approach to machine translation.Computational Linguistics, 16(2):79?85.Michel Callon, John Law, and Arie Rip.
1986.Mapping the dynamics of science and technology.McMillan, London.Michel Callon, Jean-Pierre Courtial, and Franc?oiseLaville.
1991.
Co-word analysis as a tool for de-scribing the network of interaction between basicand technological research: The case of polymerchemistry.
Scientometrics, 22(1):155?205.Katarina Frantzi and Sophia Ananiadou.
2000.
Au-tomatic recognition of multi-word terms:.
the C-value/NC-value method.
International Journal onDigital Libraries, 3(2):115?130.Eugene Garfield.
1972.
Citation Analysis as a Tool inJournal Evaluation.
Science, 178(4060):471?479.Michelle Girvan and Mark E J Newman.
2002.
Com-munity structure in social and biological networks.Proceedings of the National Academy of Sciences ofthe United States of America, 99:7821?7826.Roger Guimera, Brian Uzzi, Jarrett Spiro, and LuisA.
Nunes Amaral.
2005.
Team Assembly Mech-anisms Determine Collaboration Network Structureand Team Performance.
Science, 308(5722):697?702.Yufan Guo, Anna Korhonen, and Thierry Poibeau.2011.
A weakly-supervised approach to argumenta-tive zoning of scientific documents.
In Proceedingsof the 2011 Conference on Empirical Methods inNatural Language Processing, pages 273?283, Ed-inburgh.Yufan Guo, Roi Reichart, and Anna Korhonen.
2013.Improved information structure analysis of scien-tific documents through discourse and lexical con-straints.
In Proceedings of Human Language Tech-nologies: Conference of the North American Chap-ter of the Association of Computational Linguistics(HLT-NAACL), pages 928?937.James Curran and Stephen Clark and Johan Bos.2007.
Linguistically Motivated Large-Scale NLPwith C&C and Boxer.
In Proceedings of the 45thMeeting of the Association for Computation Linguis-tics (ACL), pages 33?36.Gary Geunbae Lee, Jong-Hyeok Lee, and Jeong-won Cha.
2002.
Syllable-pattern-based unknown-morpheme segmentation and estimation for hybridpart-of-speech tagging of korean.
ComputationalLinguistics, 28(1):53?70.Yoko Mizuta, Anna Korhonen, Tony Mullen, and NigelCollier.
2006.
Zone analysis in biology articles as abasis for information extraction.
International Jour-nal of Medical Informatics, 75(6):468?487.William H. Press, Saul A. Teukolsky, William T. Vet-terling, and Brian P. Flannery.
2007.
NumericalRecipes 3rd Edition: The Art of Scientific Comput-ing.
Cambridge University Press, New York, NY,USA, 3 edition.Roi Reichart and Anna Korhonen.
2012.
Docu-ment and corpus level inference for unsupervisedand transductive learning of information structure ofscientific documents.
In Proceedings of COLING(Posters), pages 995?1006, Mumbai.Anna Ritchie, Stephen Robertson, and Simone Teufel.2008.
Comparing citation contexts for informationretrieval.
In Proeedings of the 17th Conference onInformation and Knowledge Management (CIKM),pages 213?222, Napa Valley.Henry G Small.
1973.
Co-citation in the scientific lit-erature: A new measure of the relationship betweentwo documents.
Journal of American Society for In-formation Science, 24(4):265?269.Imad Tbahriti, Christine Chichester, Fr?ed?eriqueLisacek, and Patrick Ruch.
2006.
Using argumen-tation to retrieve articles with similar citations: Aninquiry into improving related articles search in themedline digital library.
I. J.
Medical Informatics,75(6):488?495.Simone Teufel and Marc Moens.
2002.
Summariz-ing scientific articles: Experiments with relevanceand rhetorical status.
Computational Linguistics,28(4):409?445.Simone Teufel.
1999.
Argumentative Zoning: Infor-mation Extraction from Scientific Articles.
Univer-sity of Edinburgh.79
