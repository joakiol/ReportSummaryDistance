Semantic Role Labeling: An Introduction tothe Special IssueLlu?
?s Ma`rquez?Universitat Polite`cnica de CatalunyaXavier Carreras?
?Massachusetts Institute of TechnologyKenneth C. Litkowski?CL ResearchSuzanne Stevenson?University of TorontoSemantic role labeling, the computational identification and labeling of arguments in text,has become a leading task in computational linguistics today.
Although the issues for thistask have been studied for decades, the availability of large resources and the development ofstatistical machine learning methods have heightened the amount of effort in this field.
Thisspecial issue presents selected and representative work in the field.
This overview describeslinguistic background of the problem, the movement from linguistic theories to computationalpractice, the major resources that are being used, an overview of steps taken in computationalsystems, and a description of the key issues and results in semantic role labeling (as revealed inseveral international evaluations).
We assess weaknesses in semantic role labeling and identifyimportant challenges facing the field.
Overall, the opportunities and the potential for usefulfurther research in semantic role labeling are considerable.1.
IntroductionThe sentence-level semantic analysis of text is concerned with the characterization ofevents, such as determining ?who?
did ?what?
to ?whom,?
?where,?
?when,?
and?how.?
The predicate of a clause (typically a verb) establishes ?what?
took place,and other sentence constituents express the participants in the event (such as ?who?
and?where?
), as well as further event properties (such as ?when?
and ?how?).
The primarytask of semantic role labeling (SRL) is to indicate exactly what semantic relations holdamong a predicate and its associated participants and properties, with these relations?
Departament de Llenguatges i Sistemes Informa`tics, Universitat Polite`cnica de Catalunya, Jordi GironaSalgado 1?3, 08034 Barcelona, Spain.
E-mail: lluism@lsi.upc.edu.??
Computer Science and Artificial Intelligence Laboratory (CSAIL), MIT, 32 Vassar St., Cambridge, MA02139, USA.
E-mail: carreras@csail.mit.edu.?
CL Research, 9208 Gue Road, Damascus, MD 20872 USA.
E-mail: ken@clres.com.?
Department of Computer Science, 6 King?s College Road, Toronto, ON M5S 3G4, Canada.E-mail: suzanne@cs.toronto.edu.?
2008 Association for Computational LinguisticsComputational Linguistics Volume 34, Number 2drawn from a pre-specified list of possible semantic roles for that predicate (or class ofpredicates).
In order to accomplish this, the role-bearing constituents in a clause mustbe identified and their correct semantic role labels assigned, as in:[The girl on the swing]Agent [whispered]Pred to [the boy beside her]RecipientTypical roles used in SRL are labels such as Agent, Patient, and Location for the entitiesparticipating in an event, and Temporal and Manner for the characterization of otheraspects of the event or participant relations.
This type of role labeling thus yields a first-level semantic representation of the text that indicates the basic event properties andrelations among relevant entities that are expressed in the sentence.Research has proceeded for decades on manually created lexicons, grammars, andother semantic resources (Hirst 1987; Pustejovsky 1995; Copestake and Flickinger 2000)in support of deep semantic analysis of language input, but such approaches have beenlabor-intensive and often restricted to narrow domains.
The 1990s saw a growth inthe development of statistical machine learning methods across the field of computa-tional linguistics, enabling systems to learn complex linguistic knowledge rather thanrequiring manual encoding.
These methods were shown to be effective in acquiringknowledge necessary for semantic interpretation, such as the properties of predicatesand the relations to their arguments?for example, learning subcategorization frames(Briscoe and Carroll 1997) or classifying verbs according to argument structure prop-erties (Merlo and Stevenson 2001; Schulte im Walde 2006).
Recently, medium-to-largecorpora have been manually annotated with semantic roles in FrameNet (Fillmore,Ruppenhofer, and Baker 2004), PropBank (Palmer, Gildea, and Kingsbury 2005), andNomBank (Meyers et al 2004), enabling the development of statistical approachesspecifically for SRL.With the advent of supporting resources, SRL has become a well-defined task witha substantial body of work and comparative evaluation (see, among others, Gildea andJurafsky [2002], Surdeanu et al [2003], Xue and Palmer [2004], Pradhan et al [2005a],the CoNLL Shared Task in 2004 and 2005, and Senseval-3 and SemEval-2007).
Theidentification of event frames may potentially benefit many natural language processing(NLP) applications, such as information extraction (Surdeanu et al 2003), questionanswering (Narayanan and Harabagiu 2004), summarization (Melli et al 2005), andmachine translation (Boas 2002).
Related work on classifying the semantic relations innoun phrases has also been encouraging for NLP tasks (Moldovan et al 2004; Rosarioand Hearst 2004).Although the use of SRL systems in real-world applications has thus far beenlimited, the outlook is promising for extending this type of analysis to many appli-cations requiring some level of semantic interpretation.
SRL represents an excellentframework with which to perform research on computational techniques for acquiringand exploiting semantic relations among the different components of a text.This special issue of Computational Linguistics presents several articles represent-ing the state-of-the-art in SRL, and this overview is intended to provide a broadercontext for that work.
First, we briefly discuss some of the linguistic views on se-mantic roles that have had the most influence on computational approaches to SRLand related NLP tasks.
Next, we show how the linguistic notions have influencedthe development of resources that support SRL.
We then provide an overview ofSRL methods and describe the state-of-the-art as well as current open problems in thefield.146Ma`rquez, Carreras, Litkowski, and Stevenson Semantic Role Labeling2.
Semantic Roles in LinguisticsSince the foundational work of Fillmore (1968), considerable linguistic research has beendevoted to the nature of semantic roles.
Although there is substantial agreement onmajor semantic roles, such as Agent and Theme, there is no consensus on a definitivelist of semantic roles, or even whether such a list exists.
Proposed lists range from alarge set of situation-specific roles, such as Suspect, Authorities, and Offense (Fillmore,Ruppenhofer, and Baker 2004), to a relatively small set of general roles, such as Agent,Theme, Location, and Goal (typically referred to as thematic roles, as in Jackendoff[1990]), to the set of two core roles, Proto-Agent and Proto-Theme, whose entailmentsdetermine the precise relation expressed (Dowty 1991).
This uncertainty within linguis-tic theory carries over into computational work on SRL, where there is much variabilityon the roles assumed in different resources.A major focus of work in the linguistics community is on the mapping between thepredicate?argument structure that determines the roles, and the syntactic realization ofthe recipients of those roles (Grimshaw 1990; Levin 1993; Levin and Rappaport Hovav2005).
Semantic role lists are generally viewed as inadequate for explaining the mor-phosyntactic behavior of argument expression, with argument realization dependenton a deeper lexical semantic representation of the components of the event that thepredicate describes.
Although much of the mapping from argument structure to syntaxis predictable, this mapping is not completely regular, nor entirely understood.
Animportant question for SRL, therefore, is the extent to which performance is degradedby the irregularities noted in linguistic studies of semantic roles.Nonetheless, sufficient regularity exists to provide the foundation for meaningfulgeneralizations.
Much research has focused on explaining the varied expression of verbarguments within syntactic positions (Levin 1993).
A major conclusion of that work isthat the patterns of syntactic alternation exhibit regularity that reflects an underlyingsemantic similarity among verbs, forming the basis for verb classes.
Such classes, andthe argument structure specifications for them, have proven useful in a number of NLPtasks (Habash, Dorr, and Traum 2003; Shi and Mihalcea 2005), including SRL (Swier andStevenson 2004), and have provided the foundation for the computational verb lexiconVerbNet (Kipper, Dang, and Palmer 2000).This approach to argument realization focuses on the relation of morphosyntacticbehavior to argument semantics, and typically leads to a general conceptualization ofsemantic roles.
In frame semantics (Fillmore 1976), on the other hand, a word activatesa frame of semantic knowledge that relates linguistic semantics to encyclopedic knowl-edge.
This effort has tended to focus on the delineation of situation-specific frames (e.g.,an Arrest frame) and correspondingly more specific semantic roles (e.g., Suspect andAuthorities) that codify the conceptual structure associated with lexical items (Fillmore,Ruppenhofer, and Baker 2004).
With a recognition that many lexical items could activateany such frame, this approach leads to lexical classes of a somewhat different naturethan those of Levin (1993).
Whereas lexical items in a Levin class are syntacticallyhomogeneous and share coarse semantic properties, items in a frame may syntacticallyvary somewhat but share fine-grained, real-world semantic properties.A further difference in these perspectives is the view of the roles themselves.
Indefining verb classes that capture argument structure similarities, Levin (1993) does notexplicitly draw on the notion of semantic role, instead basing the classes on behaviorthat is hypothesized to reflect the properties of those roles.
Other work also eschewsthe notion of a simple list of roles, instead postulating underlying semantic structurethat captures the relevant properties (Levin and Rappaport Hovav 1998).
Interestingly,147Computational Linguistics Volume 34, Number 2as described in Fillmore, Ruppenhofer, and Baker (2004), frame semantics also avoids apredefined list of roles, but for different reasons.
The set of semantic roles, called frameelements, are chosen for each frame, rather than being selected from a predefined listthat may not capture the relevant distinctions in that particular situation.
Clearly, tothe extent that disagreement persists on semantic role lists and the nature of the rolesthemselves, SRL may be working on a shifting target.These approaches also differ in the broad characterization of event participants(and their roles) as more or less essential to the predicate.
In the more syntactic-orientedapproaches, roles are typically divided into two categories: arguments, which cap-ture a core relation, and adjuncts, which are less central.
In frame semantics, the rolesare divided into core frame elements (e.g., Suspect, Authorities, Offense) and periph-eral or extra-thematic elements (e.g., Manner, Time, Place).
These distinctions carryover into SRL, where we see that systems generally perform better on the more centralarguments.Finally, although predicates are typically expressed as verbs, and thus much workin both linguistics and SRL focuses on them, some nouns and adjectives may be usedpredicatively, assigning their own roles to entities (as in the adjective phrase proud thatwe finished the paper, where the subordinate clause is a Theme argument of the adjectiveproud).
Frame semantics tends to include in a frame relevant non-verb lexical items,due to the emphasis on a common situation semantics.
In contrast, the morphosyntacticapproaches have focused on defining classes of verbs only, because they depend oncommon syntactic behavior that may not be apparent across syntactic categories.Interestingly, prepositions have a somewhat dual status with regard to role labeling.In languages like English, prepositions serve an important function in signaling the rela-tion of a participant to a verb.
For example, it is widely accepted that to in give the book toMary serves as a grammatical indicator of the Recipient role assigned by the verb, ratherthan as a role assigner itself.
In other situations, however, a preposition can be viewedas a role-assigning predicate in its own right.
Although some work in computationallinguistics is tackling the issue of the appropriate characterization of prepositions andtheir contribution to semantic role assignment (as we see subsequently), much workremains in order to fully integrate linguistic theories of prepositional function andsemantics into SRL.3.
From Linguistic Theory to Computational ResourcesThe linguistic approaches to semantic roles discussed previously have greatly influ-enced current work on SRL, leading to the creation of significant computational lexiconscapturing the foundational properties of predicate?argument relations.In the FrameNet project (Fillmore, Ruppenhofer, and Baker 2004), lexicographersdefine a frame to capture some semantic situation (e.g., Arrest), identify lexical itemsas belonging to the frame (e.g., apprehend and bust), and devise appropriate roles forthe frame (e.g., Suspect, Authorities, Offense).
They then select and annotate examplesentences from the British National Corpus and other sources to illustrate the range ofpossible assignments of roles to sentence constituents for each lexical item (at present,over 141,000 sentences have been annotated).FrameNet thus consists of both a computational lexicon and a role-annotated cor-pus.
The existence of such a corpus enabled Gildea and Jurafsky (2002) to develop thefirst statistical machine learning approach to SRL, using various lexical and syntacticfeatures such as phrase type and grammatical function calculated over the annotatedconstituents.
Although this research spurred the current wave of SRL work that has148Ma`rquez, Carreras, Litkowski, and Stevenson Semantic Role Labelingrefined and extended Gildea and Jurafsky?s approach, the FrameNet data has not beenused extensively.
One issue is that the corpus is not a representative sample of thelanguage, but rather consists of sentences chosen manually to illustrate the possiblerole assignments for a given lexical item.
Another issue is that the semantic roles aresituation-specific, rather than general roles like Agent, Theme, and Location that can beused across many situations and genres.The computational verb lexicon, VerbNet (Kipper, Dang, and Palmer 2000), insteadbuilds on Levin?s (1993) work on defining verb classes according to shared argument re-alization patterns.
VerbNet regularizes and extends the original Levin classes; moreover,each class is explicitly associated with argument realization specifications that state theconstituents that a verb can occur with and the role assigned to each.
The roles aremostly drawn from a small set (around 25) of general roles widely used in linguistictheory.
This lexicon has been an important resource in computational linguistics, butbecause of the lack of an associated role-annotated corpus, it has only been used directlyin SRL in an unsupervised setting (Swier and Stevenson 2004).Research on VerbNet inspired the development of the Proposition Bank (PropBank;Palmer, Gildea, and Kingsbury 2005), which has emerged as a primary resource forresearch in SRL (and used in four of the articles in this special issue).
PropBank ad-dresses some of the issues for SRL posed by the FrameNet data.
First, the PropBankproject has annotated the semantic roles for all verbs in the Penn Treebank corpus (theWall Street Journal [WSJ] news corpus).
This provides a representative sample of textwith role-annotations, in contrast to FrameNet?s reliance on manually selected, illus-trative sentences.
Importantly, PropBank?s composition allows for consideration of thestatistical patterns across natural text.
Although there is some concern about the limitedgenre of its newspaper text, this aspect has the advantage of allowing SRL systems tobenefit from the state-of-the-art syntactic parsers and other resources developed withthe WSJ TreeBank data.
Moreover, current work is extending the PropBank annotationto balanced corpora such as the Brown corpus.The lexical information associated with verbs in PropBank also differs significantlyfrom the situation-specific roles of FrameNet.
At the same time, the PropBank designersrecognize the difficulty of providing a small, predefined list of semantic roles that is suf-ficient for all verbs and predicate?argument relations, as in VerbNet.
PropBank insteadtakes a ?theory-neutral?
approach to the designation of core semantic roles.
Each verbhas a frameset listing its allowed role labelings in which the arguments are designatedby number (starting from 0).
Each numbered argument is provided with an English-language description specific to that verb.
Participants typically considered as adjunctsare given named argument roles, because there is more general agreement on suchmodifiers as Temporal or Manner applying consistently across verbs.
Different sensesfor a polysemous verb have different framesets; however, syntactic alternations whichpreserve meaning (as identified in Levin [1993]) are considered to be a single frameset.While the designations of Arg0 and Arg1 are intended to indicate the general roles ofAgent and Theme/Patient across verbs, other argument numbers do not consistentlycorrespond to general (non-verb-specific) semantic roles.Given the variability in the sets of roles used across the computational resources,an important issue is the extent to which different role sets affect the SRL task, as wellas subsequent use of the output in other NLP applications.
Gildea and Jurafsky (2002)initiated this type of investigation by exploring whether their results were dependenton the set of semantic roles they used.
To this end, they mapped the FrameNet frameelements into a set of abstract thematic roles (i.e., more general roles such as Agent,Theme, Location), and concluded that their system could use these thematic roles149Computational Linguistics Volume 34, Number 2without degradation.
Similar questions must be investigated in the context of PropBank,where the framesets for the verbs may have significant domain-specific meanings andarguments due to the dependence of the project on WSJ data.
Given the uncertainty inthe linguistic status of semantic role lists, and the lack of evidence about which typesof roles would be most useful in various NLP tasks, an important ongoing focus ofattention is the value of mapping between the role sets of the different resources (Swierand Stevenson 2005; Loper, Yi, and Palmer 2007; Yi, Loper, and Palmer 2007).We noted previously the somewhat special part that prepositions play in markingsemantic relations, in some sense mediating the role assignment of a verb to an argu-ment.
The resources noted earlier differ in their treatment of prepositions.
In VerbNet,for example, prepositions are listed explicitly as part of the syntactic context in whicha role is assigned (e.g., Agent V Prep(for) Recipient), but it is the NP object of the prep-osition that receives the semantic role.
In FrameNet and PropBank, on the other hand,the full prepositional phrase is considered as the frame element (the constituent re-ceiving the role).
Clearly, further work needs to proceed on how to best capture the in-teraction between verbs and prepositions in SRL.
This is especially complex giventhe high polysemy of prepositions, and work has proceeded on relating prepositiondisambiguation to role assignment (e.g., O?Hara and Wiebe 2003).
For such approachesto make meaningful progress, resources are needed that elaborate the senses of prepo-sitions and relate those senses to semantic roles.
In The Preposition Project (TPP;Litkowski and Hargraves 2005), a comprehensive, hierarchical characterization of thesemantic roles for all preposition senses in English is being developed.
TPP has sense-tagged more than 25,000 preposition instances in FrameNet sentences, allowing forcomprehensive investigation of the linking between preposition sense and semantic roleassignment.4.
Approaches to Automatic SRLThe work on SRL has included a broad spectrum of probabilistic and machine-learningapproaches to the task.
We focus here on supervised systems, because most SRL researchtakes an approach requiring training on role-annotated data.
We briefly survey the mainapproaches to automatic SRL, and the types of learning features used.4.1 SRL Step by StepGiven a sentence and a designated verb, the SRL task consists of identifying the bound-aries of the arguments of the verb predicate (argument identification) and labelingthem with semantic roles (argument classification).
The most common architecture forautomatic SRL consists of the following steps to achieve these subtasks.The first step in SRL typically consists of filtering (or pruning) the set of argu-ment candidates for a given predicate.
Because arguments may be a continuous ordiscontinuous sequence of words, any subsequence of words in the sentence is anargument candidate.
Exhaustive exploration of this space of candidates is not feasible,because it is both very large and imbalanced (i.e., the vast majority of candidates arenot actual arguments of the verb).
The simple heuristic rules of Xue and Palmer (2004)are commonly used to perform filtering because they greatly reduce the set of candidatearguments, while maintaining a very high recall.The second step consists of a local scoring of argument candidates by means ofa function that outputs probabilities (or confidence scores) for each of the possible150Ma`rquez, Carreras, Litkowski, and Stevenson Semantic Role Labelingrole labels, plus an extra ?no-argument?
label indicating that the candidate shouldnot be considered an argument in the solution.
In this step, candidates are usuallytreated independently of each other.
A crucial aspect in local scoring (see Section 4.2)is the representation of candidates with features, rather than the particular choice ofclassification algorithm.Argument identification and classification may be treated jointly or separately inthe local scoring step.
In the latter case, a pipeline of two subprocesses is typicallyapplied, first scoring between ?argument?
and ?no-argument?
labels, and then scoringthe particular argument labels.
Because argument identification is closely related tosyntax and argument classification is more a semantic issue, useful features for the twosubtasks may be very different?that is, a good feature for addressing recognition mayhurt classification and vice versa (Pradhan et al 2005a).The third step in SRL is to apply a joint scoring (or global scoring) in order tocombine the predictions of local scorers to produce a good structure of labeled argu-ments for the predicate.
In this step, dependencies among several arguments of the samepredicate can be exploited.
For instance, Punyakanok, Roth, and Yih (this issue) ensurethat a labeling satisfies a set of structural and SRL-dependent constraints (argumentsdo not overlap, core arguments do not repeat, etc.).
Also in this issue, Toutanova,Haghighi, and Manning apply re-ranking to select the best among a set of candidatecomplete solutions produced by a base SRL system.
Finally, probabilistic models havealso been applied to produce the structured output, for example, generative models(Thompson, Levy, and Manning 2003), sequence tagging with classifiers (Ma`rquez et al2005; Pradhan et al 2005b), and Conditional Random Fields on tree structures (Cohnand Blunsom 2005).
These approaches at a global level may demand considerable extracomputation, but current optimization techniques help solve them quite efficiently.Some variations in the three-step architecture are found.
Systems may bypass oneof the steps, by doing only local scoring, or skipping directly to joint scoring.
A fourthstep may consist of fixing common errors or enforcing coherence in the final solution.This postprocess usually consists of a set of hand-developed heuristic rules that aredependent on a particular architecture and corpus of application.An important consideration within this general SRL architecture is the combinationof systems and input annotations.
Most SRL systems include some kind of combi-nation to increase robustness, gain coverage, and reduce effects of parse errors.
Onemay combine: (1) the output of several independent SRL basic systems (Surdeanuet al 2007; Pradhan et al 2005b), or (2) several outputs from the same SRL systemobtained by changing input annotations or other internal parameters (Koomen et al2005; Toutanova, Haghighi, and Manning 2005).
The combination can be as simple asselecting the best among the set of complete candidate solutions, but usually consists ofcombining fragments of alternative solutions to construct the final output.
Finally, thecombination component may involve machine learning or not.
The gain in performancefrom the combination step is consistently between two and three F1 points.
However, acombination approach increases system complexity and penalizes efficiency.Several exceptions to this described architecture for SRL can be found in the lit-erature.
One approach entails joint labeling of all predicates of the sentence, insteadof proceeding one by one.
This opens the possibility of exploiting dependencies amongthe different verbs in the sentence.
However, the complexity may grow significantly, andresults so far are inconclusive (Carreras, Ma`rquez, and Chrupa?a 2004; Surdeanu et al2007).
Other promising approaches draw on dependency parsing rather than traditionalphrase structure parsing (Johansson and Nugues 2007), or combine parsing and SRLinto a single step of semantic parsing (Musillo and Merlo 2006).151Computational Linguistics Volume 34, Number 24.2 Feature EngineeringAs previously noted, devising the features with which to encode candidate argumentsis crucial for obtaining good results in the SRL task.
Given a verb and a candidate argu-ment (a syntactic phrase) to be classified in the local scoring step, three types of featuresare typically used: (1) features that characterize the candidate argument and its context;(2) features that characterize the verb predicate and its context; and (3) features that cap-ture the relation (either syntactic or semantic) between the candidate and the predicate.Gildea and Jurafsky (2002) presented a compact set of features across these threetypes, which has served as the core of most of the subsequent SRL work: (1) the phrasetype, headword, and governing category of the constituent; (2) the lemma, voice, andsubcategorization pattern of the verb; and (3) the left/right position of the constituentwith respect to the verb, and the category path between them.
Extensions to these fea-tures have been proposed in various directions.
Exploiting the ability of some machinelearning algorithms to work with very large feature spaces, some authors have largelyextended the representation of the constituent and its context, including among others:first and last words (and part-of-speech) in the constituent, bag-of-words, n-grams ofpart of speech, and sequence of top syntactic elements in the constituent.
Parent andsibling constituents in the tree may also be codified with all the previous structural andlexical features (Pradhan et al 2005a; Surdeanu et al 2007).
Other authors have designednew features with specific linguistic motivations.
For instance, Surdeanu et al (2003)generalized the concept of headword with the content word feature.
They also usednamed entity labels as features.
Xue and Palmer (2004) presented the syntactic framefeature, which captures the overall sentence structure using the verb predicate and theconstituent as pivots.
All these features resulted in a significant increase in performance.Finally, regarding the relation between the constituent and the predicate, severalvariants of Gildea and Jurafsky?s syntactic path have been proposed in the literature(e.g., generalizations to avoid sparsity, and adaptations to partial parsing).
Also, someattempts have been made at characterizing the semantic relation between the predicateand the constituent.
In Zapirain, Agirre, and Ma`rquez (2007) and Erk (2007), selectionalpreferences between predicate and headword of the constituent are explored to generatesemantic compatibility features.
Using conjunctions of several of the basic features isalso common practice.
This may be very relevant when the machine learning methodused is linear in the space of features.Joint scoring and combination components open the door to richer types of fea-tures, which may take into account global properties of the candidate solution plus de-pendencies among the different arguments.
The most remarkable work in this directionis the reranking approach by Toutanova, Haghighi, and Manning in this issue.
Whentraining the ranker to select the best candidate solution they codify pattern features asstrings containing the whole argument structure of the candidate.
Several variationsof this type of feature (with different degrees of generalization to avoid sparseness)allow them to significantly increase the performance of the base system.
Also related,Pradhan et al (2005b) and Surdeanu et al (2007) convert the confidence scores of severalbase SRL systems into features for training a final machine learning?based combinationsystem.
Surdeanu et al (2007) develop a broad spectrum of features, with sentence-based information, describing the role played by the candidate argument in everysolution proposed by the different base SRL systems.A completely different approach to feature engineering is the use of kernel meth-ods to implicitly exploit all kinds of substructures in the syntactic representation ofthe candidates.
This knowledge poor approach intends to take advantage of a massive152Ma`rquez, Carreras, Litkowski, and Stevenson Semantic Role Labelingquantity of features without the need for manual engineering of specialized features.This motivation might be relevant for fast system development and porting, especiallywhen specialized linguistic knowledge of the language of application is not available.The most studied approach consists of using some variants of the ?all subtrees kernel?applied to the sentence parse trees.
The work by Moschitti, Pighin, and Basili in thisissue is the main representative of this family.5.
Empirical Evaluations of SRL SystemsMany experimental studies have been conducted since the work of Gildea and Jurafsky(2002), including seven international evaluation tasks in ACL-related conferences andworkshops: the SIGNLL CoNLL shared tasks in 2004 and 2005 (Carreras and Ma`rquez2004, 2005), the SIGLEX Senseval-3 in 2004 (Litkowski 2004), and four tasks in theSIGLEX SemEval in 2007 (Pradhan et al 2007; Ma`rquez et al 2007; Baker, Ellsworth, andErk 2007; Litkowski and Hargraves 2007).
In the subsequent sections, we summarizetheir main features, results, and conclusions, although note that the scores are notdirectly comparable across different exercises, due to differences in scoring and in theexperimental methodologies.5.1 Task Definition and Evaluation MetricsThe standard experiment in automatic SRL can be defined as follows: Given a sentenceand a target predicate appearing in it, find the arguments of the predicate and labelthem with semantic roles.
A system is evaluated in terms of precision, recall, and F1 ofthe labeled arguments.
In evaluating a system, an argument is considered correct whenboth its boundaries and the semantic role label match a gold standard.
Performancecan be divided into two components: (1) the precision, recall, and F1 of unlabeledarguments, measuring the accuracy of the system at segmenting the sentence; and (2)the classification accuracy of assigning semantic roles to the arguments that have beencorrectly identified.
In calculating the metrics, the de facto standard is to give credit onlywhen a proposed argument perfectly matches an argument in the reference solution;nonetheless, variants that give some credit for partial matching also exist.5.2 Shared Task Experiments Using FrameNet, PropBank, and VerbNetTo date, most experimental work has made use of English data annotated either withPropBank or FrameNet semantic roles.The CoNLL shared tasks in 2004 and 2005 were based on PropBank (Carreras andMa`rquez 2004, 2005), which is the largest evaluation benchmark available today, andalso the most used by researchers?all articles in this special issue dealing with Englishuse this benchmark.
In the evaluation, the best systems obtained an F1 score of ?80%,and have achieved only minimal improvements since then.
The articles in this issue byPunyakanok, Roth, and Yih; Toutanova, Haghighi, and Manning; and Pradhan, Ward,and Martin describe such efforts.
An analysis of the outputs in CoNLL-2005 showedthat argument identification accounts for most of the errors: a system will recall ?81%of the correct unlabeled arguments, and ?95% of those will be assigned the correctsemantic role.
The analysis also showed that systems recognized core arguments betterthan adjuncts (with F1 scores from the high 60s to the high 80s for the former, but below60% for the latter).
Finally, it was also observed that, although systems performed better153Computational Linguistics Volume 34, Number 2on verbs appearing frequently in training, the best systems could recognize argumentsof unseen verbs with an F1 in the low 70s, not far from the overall performance.1SemEval-2007 included a task on semantic evaluation for English, combining wordsense disambiguation and SRL based on PropBank (Pradhan et al 2007).
Unlike theCoNLL tasks, this task concentrated on 50 selected verbs.
Interestingly, the data wasannotated using verb-independent roles using the PropBank/VerbNet mapping fromYi, Loper, and Palmer (2007).
The two participating systems could predict VerbNet rolesas accurately as PropBank verb-dependent roles.Experiments based on FrameNet usually concentrate on a selected list of frames.In Senseval-3, 40 frames were selected for an SRL task with the goal of replicatingGildea and Jurafsky (2002) and improving on them (Litkowski 2004).
Participants wereevaluated on assigning semantic roles to given arguments, with best F1 of 92%, and onthe task of segmenting and labeling arguments, with best F1 of 83%.SemEval-2007 also included an SRL task based on FrameNet (Baker, Ellsworth, andErk 2007).
It was much more complete, realistic, and difficult than its predecessor inSenseval-3.
The goal was to perform complete analysis of semantic roles on unseentexts, first determining the appropriate frames of predicates, and then determining theirarguments labeled with semantic roles.
It also involved creating a graph of the sentencerepresenting part of its semantics, by means of frames and labeled arguments.
Thetest data of this task consisted of novel manually-annotated documents, containing anumber of frames and roles not in the FrameNet lexicon.
Three teams submitted results,with precision percentages in the 60s, but recall percentages only in the 30s.To our knowledge, there is no evidence to date on the relative difficulty of assigningFrameNet or PropBank roles.5.3 Impact of Syntactic Processing in SRLSemantic roles are closely related to syntax, and, therefore, automatic SRL heavily relieson the syntactic structure of the sentence.
In PropBank, over 95% of the argumentsmatch with a single constituent of the parse tree.
If the output produced by a statisticalparser is used (e.g., Collins?s or Charniak?s) the exact matching is still over 90%.
More-over, some simple rules can be used to join constituents and fix a considerable portionof the mismatches (Toutanova, Haghighi, and Manning 2005).
Thus, it has become acommon practice to use full parse trees as the main source for solving SRL.The joint model presented in this issue by Toutanova, Haghighi, and Manningobtains an F1 at ?90% on the WSJ test of the CoNLL-2005 evaluation when using gold-standard trees; but with automatic syntactic analysis, its best result falls to ?80%.
Thisand other work consistently show that the drop in performance occurs in identifyingargument boundaries; when arguments are identified correctly with predicted parses,the accuracy of assigning semantic roles is similar to that with correct parses.A relevant question that has been addressed in experimental work concerns theuse of a partial parser instead of a parser that produces full WSJ trees.
In the CoNLL-2004 task, systems were restricted to the use of base syntactic phrases (i.e., chunks)and clauses, and the best results that could be obtained were just below 70%.
But thetraining set in that evaluation was about five times smaller than that of the 2005 task.Punyakanok, Roth, and Yih (this issue) and Surdeanu et al (2007) have shown that, in1 The analysis summarized here was presented in the oral session at CoNLL-2005.
The slides of the session,containing the results supporting this analysis, are available in the CoNLL-2005 shared task Web site.154Ma`rquez, Carreras, Litkowski, and Stevenson Semantic Role Labelingfact, a system working with partial parsing can do almost as well as a system workingwith full parses, with differences in F1 of only ?2?3 points.Currently, the top-performing systems on the CoNLL data make use of severaloutputs of syntactic parsers, as discussed in Section 4.
It is clear that many errors inSRL are caused by having incorrect syntactic constituents, as reported by Punyakanok,Roth, and Yih in this issue.
By using many parses, the recognition of semantic roles ismore robust to parsing errors.
Yet, it remains unanswered what is the most appropriatelevel of syntactic analysis needed in SRL.5.4 Generalization of SRL Systems to New DomainsPorting a system to a new domain, different than the domain used to develop and trainthe system, is a challenging question in NLP.
SRL is no exception, with the particulardifficulty that a predicate in a new domain may exhibit a behavior not contemplatedin the dictionary of frames at training time.
This difficulty was identified as a majorchallenge in the FrameNet-based task in SemEval-2007 (Baker, Ellsworth, and Erk 2007).In the CoNLL-2005 task, WSJ-trained systems were tested on three sections ofthe Brown corpus annotated by the PropBank team.
The performance of all systemsdropped dramatically: The best systems scored F1 below 70%, as opposed to figures at?80% when testing on WSJ data.
This is perhaps not surprising, taking into account thatthe pre-processing systems involved in the analysis (tagger and parser) also experienceda significant drop in performance.
The article in this issue by Pradhan, Ward, andMartin further investigates the robustness across text genres when porting a systemfrom WSJ to Brown.
Importantly, the authors claim that the loss in accuracy takes placein assigning the semantic roles, rather than in the identification of argument boundaries.5.5 SRL on Languages Other Than EnglishSemEval-2007 featured the first evaluation exercise of SRL systems for languages otherthan English, namely for Spanish and Catalan (Ma`rquez et al 2007).
The data was partof the CESS-ECE corpus, consisting of ?100K tokens for each language.
The semanticrole annotations are similar to PropBank, in that role labels are specific to each verb,but also include a verb-independent thematic role label similar to the scheme proposedin VerbNet.
The task consisted of assigning semantic class labels to target verbs, andidentifying and labeling arguments of such verbs, in both cases using gold-standardsyntax.
Only two teams participated, with best results at ?86% for disambiguatingpredicates, and at ?83% for labeling arguments.The work by Xue in this issue studies semantic role labeling for Chinese, using theChinese PropBank and NomBank corpora.
Apart from working also with nominalizedpredicates, this work constitutes the first comprehensive study on SRL for a languagedifferent from English.5.6 SRL with Other Parts-of-SpeechThe SemEval-2007 task on disambiguating prepositions (Litkowski and Hargraves 2007)used FrameNet sentences as the training and test data, with over 25,000 sentences forthe 34 most common English prepositions.
Although not overtly defined as semanticrole labeling, each instance was characterized with a semantic role name and also hadan associated FrameNet frame element.
Almost 80% of the prepositional phrases in theinstances were identified as core frame elements, and are likely to be closely associated155Computational Linguistics Volume 34, Number 2with arguments of the words to which they are attached.
The three participants used avariety of methods, with the top performing team using machine learning techniquessimilar to those in other semantic role labeling tasks.6.
Final RemarksTo date, SRL systems have been shown to perform reasonably well in some controlledexperiments, with F1 measures in the low 80s on standard test collections for English.Still, a number of important challenges exist for future research on SRL.
It remainsunclear what is the appropriate level of syntax needed to support robust analysis ofsemantic roles, and to what degree improved performance in SRL is constrained by thestate-of-the-art in tagging and parsing.
Beyond syntax, the relation of semantic roles toother semantic knowledge (such as WordNet, named entities, or even a catalogue offrames) has scarcely been addressed in the design of current SRL models.
A deeperunderstanding of these questions could help in developing methods that yield im-proved generalization, and that are less dependent on large quantities of role-annotatedtraining data.Indeed, the requirement of most SRL approaches for such training data, which isboth difficult and highly expensive to produce, is the major obstacle to the widespreadapplication of SRL across different genres and different languages.
Given the degrada-tion of performance when a supervised system is faced with unseen events or a testingcorpus different from training, this is a major impediment to increasing the applicationof SRL even within English, a language for which two major annotated corpora areavailable.
It is critical for the future of SRL that research broadens to include widerinvestigation of unsupervised and minimally supervised learning methods.In addition to these open research problems, there are also methodological issuesthat need to be addressed regarding how research is conducted and evaluated.
Sharedtask frameworks have been crucial in SRL development by supporting explicit compar-isons of approaches, but such benchmark testing can also overly focus research effortson small improvements in particular evaluation measures.
Improving the entire SRLapproach in a significant way may require more open-ended investigation and morequalitative analysis.AcknowledgmentsWe are grateful for the insightful commentsof two anonymous reviewers whose inputhelped us to improve the article.
This workwas supported by the Spanish Ministry ofEducation and Science (Ma`rquez); theCatalan Ministry of Innovation, Universitiesand Enterprise; and a grant from NTT, Agmt.Dtd.
6/21/1998 (Carreras); and NSERC ofCanada (Stevenson).ReferencesBaker, C., M. Ellsworth, and K. Erk.
2007.SemEval-2007 Task 19: Frame semanticstructure extraction.
In Proceedings of the4th International Workshop on SemanticEvaluations (SemEval-2007), pages 99?104,Prague, Czech Republic.Boas, H. C. 2002.
Bilingual framenetdictionaries for machine translation.In Proceedings of the Third InternationalConference on Language Resources andEvaluation (LREC), pages 1364?1371,Las Palmas de Gran Canaria, Spain.Briscoe, T. and J. Carroll.
1997.
Automaticextraction of subcategorization fromcorpora.
In Proceedings of the 5th ACLConference on Applied Natural LanguageProcessing (ANLP), pages 356?363,Washington, DC.Carreras, X. and L. Ma`rquez.
2004.Introduction to the CoNLL-2004 SharedTask: Semantic role labeling.
In Proceedingsof the Eighth Conference on ComputationalNatural Language Learning (CoNLL-2004),pages 89?97, Boston, MA.Carreras, X. and L. Ma`rquez.
2005.Introduction to the CoNLL-2005 Shared156Ma`rquez, Carreras, Litkowski, and Stevenson Semantic Role LabelingTask: Semantic role labeling.
In Proceedingsof the Ninth Conference on ComputationalNatural Language Learning (CoNLL-2005),pages 152?164, Ann Arbor, MI.Carreras, X., L. Ma`rquez, and G. Chrupa?a.2004.
Hierarchical recognition ofpropositional arguments with perceptrons.In Proceedings of the Eighth Conference onComputational Natural Language Learning(CoNLL-2004), pages 106?109, Boston, MA.Cohn, T. and P. Blunsom.
2005.
Semantic rolelabelling with tree conditional randomfields.
In Proceedings of the Ninth Conferenceon Computational Natural LanguageLearning (CoNLL-2005), pages 169?172,Ann Arbor, MI.Copestake, A. and D. Flickinger.
2000.
Anopen-source grammar developmentenvironment and broad-coverage Englishgrammar using HPSG.
In Proceedingsof the Second International Conference onLanguage Resources and Evaluation (LREC),pages 591?600, Athens, Greece.Dowty, D. 1991.
Thematic proto-roles andargument selection.
Language, 67:547?619.Erk, K. 2007.
A simple, similarity-basedmodel for selectional preferences.
InProceedings of the 45th Annual Meeting of theAssociation of Computational Linguistics,pages 216?223, Prague, Czech Republic.Fillmore, C. 1968.
The case for case.
InE.
Bach and R. T. Harms, editors, Universalsin Linguistic Theory.
Holt, Rinehart &Winston, New York, pages 1?88.Fillmore, C. J.
1976.
Frame semantics and thenature of language.
Annals of the New YorkAcademy of Sciences: Conference on the Originand Development of Language and Speech,280:20?32.Fillmore, C. J., J. Ruppenhofer, and C. F.Baker.
2004.
Framenet and representingthe link between semantic and syntacticrelations.
In Churen Huang and WinfriedLenders, editors, Frontiers in Linguistics,volume I of Language and LinguisticsMonograph Series B.
Institute of Linguistics,Academia Sinica, Taipei, pages 19?59.Gildea, D. and D. Jurafsky.
2002.
Automaticlabeling of semantic roles.
ComputationalLinguistics, 28(3):245?288.Grimshaw, J.
1990.
Argument Structure.
MITPress, Cambridge, MA.Habash, N., B. J. Dorr, and D. Traum.
2003.Hybrid natural language generation fromlexical conceptual structures.
MachineTranslation, 18(2):81?128.Hirst, G. 1987.
Semantic Interpretation andthe Resolution of Ambiguity.
CambridgeUniversity Press, Cambridge.Jackendoff, R. 1990.
Semantic Structures.
MITPress, Cambridge, MA.Johansson, R. and P. Nugues.
2007.
LTH:Semantic structure extraction usingnonprojective dependency trees.
InProceedings of the 4th InternationalWorkshop on Semantic Evaluations(SemEval-2007), pages 227?230, Prague,Czech Republic.Kipper, K., H. T. Dang, and M. Palmer.
2000.Class-based construction of a verb lexicon.In Proceedings of the 17th National Conferenceon Artificial Intelligence (AAAI-2000),Austin, TX.Koomen, P., V. Punyakanok, D. Roth, andW.
Yih.
2005.
Generalized inferencewith multiple semantic role labelingsystems.
In Proceedings of the NinthConference on Computational NaturalLanguage Learning (CoNLL-2005),pages 181?184, Ann Arbor, MI.Levin, B.
1993.
English Verb Classes andAlternations: A Preliminary Investigation.The University of Chicago Press,Chicago, IL.Levin, B. and M. Rappaport Hovav.
1998.Building verb meanings.
In M. Butt andW.
Geuder, editors, The Projection ofArguments: Lexical and CompositionalFactors.
CSLI Publications, Stanford, CA,pages 97?134.Levin, B. and M. Rappaport Hovav.
2005.Argument Realization.
CambridgeUniversity Press, Cambridge.Litkowski, K. C. 2004.
Senseval-3 task:Automatic labeling of semantic roles.
InProceedings of the 3rd International Workshopon the Evaluation of Systems for the SemanticAnalysis of Text (Senseval-3), pages 9?12,Barcelona, Spain.Litkowski, K. C. and O. Hargraves.
2005.The preposition project.
In Proceedingsof the ACL-SIGSEM Workshop on theLinguistic Dimensions of Prepositions andtheir Use in Computational LinguisticFormalisms and Applications, pages 171?179,Colchester, UK.Litkowski, K. C. and O. Hargraves.
2007.SemEval-2007 Task 06: Word-sensedisambiguation of prepositions.
InProceedings of the 4th International Workshopon Semantic Evaluations (SemEval-2007),pages 24?29, Prague, Czech Republic.Loper, E., S. Yi, and M. Palmer.
2007.Combining lexical resources: Mappingbetween PropBank and VerbNet.
InProceedings of the 7th International Workshopon Computational Semantics, pages 118?128,Tilburg, The Netherlands.157Computational Linguistics Volume 34, Number 2Ma`rquez, L., P. R. Comas, J. Gime?nez, andN.
Catala`.
2005.
Semantic role labelingas sequential tagging.
In Proceedingsof the Ninth Conference on ComputationalNatural Language Learning (CoNLL-2005),pages 193?196, Ann Arbor, MI.Ma`rquez, L., L. Villarejo, M.A.
Mart?
?, andM.
Taule?.
2007.
SemEval-2007 Task 09:Multilevel semantic annotation of Catalanand Spanish.
In Proceedings of the 4thInternational Workshop on SemanticEvaluations (SemEval-2007), pages 42?47,Prague, Czech Republic.Melli, G., Y. Wang, Y. Liu, M. M. Kashani,Z.
Shi, B. Gu, A. Sarkar, and F. Popowich.2005.
Description of SQUASH, the SFUquestion answering summary handlerfor the DUC-2005 Summarization Task.In Proceedings of the HLT/EMNLPDocument Understanding Workshop (DUC),Vancouver, Canada, available athttp://duc.nist.gov/pubs/2005papers/simonfraseru.sarkar.pdf.Merlo, P. and S. Stevenson.
2001.
Automaticverb classification based on statisticaldistributions of argument structure.Computational Linguistics, 27(3):373?408.Meyers, A., R. Reeves, C. Macleod,R.
Szekely, V. Zielinska, B.
Young, andR.
Grishman.
2004.
The NomBank Project:An interim report.
In Proceedings ofthe HLT-NAACL 2004 Workshop: Frontiersin Corpus Annotation, pages 24?31,Boston, MA.Moldovan, D., A. Badulescu, M. Tatu,D.
Antohe, and R. Girju.
2004.
Models forthe semantic classification of nounphrases.
In Proceedings of the HLT-NAACL2004 Workshop on Computational LexicalSemantics, pages 60?67, Boston, MA.Musillo, G. and P. Merlo.
2006.
Accurateparsing of the proposition bank.
InProceedings of the Human LanguageTechnology Conference of the NAACL,pages 101?104, New York, NY.Narayanan, S. and S. Harabagiu.
2004.Question answering based on semanticstructures.
In Proceedings of the 20thInternational Conference on ComputationalLinguistics (COLING), pages 693?701,Geneva, Switzerland.O?Hara, T. and J. Wiebe.
2003.
Prepositionsemantic classification via Penn Treebankand FrameNet.
In Proceedings of theSeventh Conference on ComputationalNatural Language Learning (CoNLL-2003),pages 79?86, Edmonton, Canada.Palmer, M., D. Gildea, and P. Kingsbury.2005.
The Proposition Bank: An annotatedcorpus of semantic roles.
ComputationalLinguistics, 31(1):71?105.Pradhan, S., K. Hacioglu, V. Krugler,W.
Ward, J. Martin, and D. Jurafsky.
2005a.Support vector learning for semanticargument classification.
Machine Learning,60(1):11?39.Pradhan, S., K. Hacioglu, W. Ward, J. H.Martin, and D. Jurafsky.
2005b.
Semanticrole chunking combining complementarysyntactic views.
In Proceedings of theNinth Conference on ComputationalNatural Language Learning (CoNLL-2005),pages 217?220, Ann Arbor, MI.Pradhan, S., E. Loper, D. Dligach, andM.
Palmer.
2007.
SemEval-2007 Task 17:English lexical sample, SRL and all words.In Proceedings of the 4th InternationalWorkshop on Semantic Evaluations(SemEval-2007), pages 87?92, Prague,Czech Republic.Pustejovsky, J.
1995.
The Generative Lexicon.MIT Press, Cambridge, MA.Rosario, B. and M. Hearst.
2004.
Classifyingsemantic relations in bioscience text.
InProceedings of the 42nd Annual Meeting of theAssociation for Computational Linguistics,pages 430?437, Barcelona, Spain.Schulte im Walde, S. 2006.
Experimentson the automatic induction of Germansemantic verb classes.
ComputationalLinguistics, 32(2):159?194.Shi, L. and R. Mihalcea.
2005.
Putting piecestogether: Combining FrameNet, VerbNetand WordNet for robust semantic parsing.In Computational Linguistics and IntelligentText Processing; Sixth InternationalConference, CICLing 2005, Proceedings,LNCS, vol 3406, pages 100?111, MexicoCity, Mexico.Surdeanu, M., S. Harabagiu, J. Williams,and P. Aarseth.
2003.
Usingpredicate-argument structures forinformation extraction.
In Proceedings of the41st Annual Meeting of the Association forComputational Linguistics, pages 8?15,Sapporo, Japan.Surdeanu, M., L. Ma`rquez, X. Carreras, andP.
R. Comas.
2007.
Combination strategiesfor semantic role labeling.
Journal ofArtificial Intelligence Research (JAIR),29:105?151.Swier, R. and S. Stevenson.
2004.Unsupervised semantic role labelling.
InProceedings of the Conference on EmpiricalMethods in Natural Language Processing(EMNLP), pages 95?102, Barcelona, Spain.Swier, R. and S. Stevenson.
2005.
Exploitinga verb lexicon in automatic semantic role158Ma`rquez, Carreras, Litkowski, and Stevenson Semantic Role Labelinglabelling.
In Proceedings of the HumanLanguage Technology Conference andConference on Empirical Methods in NaturalLanguage Processing (HLT/EMLNP), pages883?890, Vancouver, B.C., Canada.Thompson, C. A., R. Levy, and C. Manning.2003.
A generative model for semantic rolelabeling.
In Proceedings of the 14th EuropeanConference on Machine Learning (ECML),pages 397?408, Dubrovnik, Croatia.Toutanova, K., A. Haghighi, and C. Manning.2005.
Joint learning improves semantic rolelabeling.
In Proceedings of the 43rd AnnualMeeting of the Association for ComputationalLinguistics, pages 589?596, Ann Arbor, MI.Xue, N. and M. Palmer.
2004.
Calibratingfeatures for semantic role labeling.
InProceedings of the Conference on EmpiricalMethods in Natural Language Processing(EMNLP), pages 88?94, Barcelona, Spain.Yi, S., E. Loper, and M. Palmer.
2007.
Cansemantic roles generalize across corpora?In Human Language Technologies 2007:The Conference of the North AmericanChapter of the Association for ComputationalLinguistics; Proceedings of the MainConference, pages 548?555, Rochester, NY.Zapirain, B., E. Agirre, and L. Ma`rquez.
2007.UBC-UPC: Sequential SRL usingselectional preferences: an approach withmaximum entropy Markov models.
InProceedings of the 4th International Workshopon Semantic Evaluations (SemEval-2007),pages 354?357, Prague, Czech Republic.159
