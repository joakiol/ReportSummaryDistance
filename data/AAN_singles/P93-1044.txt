Guiding an HPSG Parser using Semantic and Pragmatic ExpectationsJ im SkonComputer and Information Science DepartmentThe Ohio State UniversityColumbus, OH 43210, USAInternet: skon@ cis.ohio-state.eduAbstract 1Efficient natural language generation has been successfullydemonstrated using highly compiled knowledge about speechacts and their related social actions.
A design and prototypeimplementation f a parser which utilizes this same pragmaticknowledge to efficiently guide parsing is presented.
Suchguidance is shown to prune the search space and thus avoidneedless processing of pragmatically unlikely constituentstructures.INTRODUCTIONThe use of purely syntactic knowledge during the parsephase of natural language understanding yields considerablelocal ambiguity (consideration of impossible subeonstituents)as well global ambiguity (construction f syntactically validparses not applicable to the socio-pragmatic context).This research investigates bringing socio-pragmaticknowledge to bear during the parse, while maintaining adomain independent grammar and parser.
The particulartechnique explored uses knowledge about he pragmatic contextto order the consideration of proposed parse constituents, thusguiding the parser to consider the best (wrt the expectations)solutions first.
Such a search may be classified as a best-first search.The theoretical models used to represent the pragmaticknowledge in this study are based on Halliday's SystemicGrammar and a model of the pragmatics of conversation.
Themodel used to represent the syntax and domain independentsemantic knowledge is HPSG - Head-driven Phrase StructureGrammar.BACKGROUNDPatten, Geis and Becker (1992) demonstrate theapplication of knowledge compilation to achieve the rapidgeneration ofnatural language.
Their mechanism is based onHalliday's ystemic networks, and on Geis' theory of thepragmatics of conversation.
A model of conversation usingprincipled compilation of pragmatic knowledge and otherlinguistic knowledge is used to permit the application ofpragmatic inference without expensive computation.
Apragmatic component is used to model social action, includingspeech acts, and utilize conventions of us.g involving suchfeatures of context such as politeness, ~e~gister, and stylisticfeatures.
These politeness features are critiqd}l to the account ofindirect speech acts.
This pragmatic knovCledge is compiledinto course-grained knowledge in the form of a classificationhierarchy.
A planner component uses knowledge aboutconditions which need to be satisfied (discourse goals) toproduce aset of pragmatic features which characterize a desiredutterance.
These features are mapped into the systemicl Research Funded by The Ohio State Center for CognitiveScience and The Ohio State Departments of Computer andInformation Science and Linguisticsgrammar (using compiled knowledge) which is then used torealize the actual utterance.The syntactic/semantic component used in this study is aparser based on the HPSG (Head Driven Phrase StructureGrammar) theory of grammar (Pollard and Sag, 1992).
HPSGmodels all linguistic constituents in terms of part/a/information structures called feature  structures.Linguistic signs incorporate simultaneous representation fphonological, syntactic, and semantic attributes ofgrammatical constituents.
HPSG is a lexiealized theory,with the lexical definitions, rather then phrase structure rules,specifying most configurational constraints.
Control (such assubcategorization, for example) is asserted by the use of HPSGconstraints - partially filled in feature structures called featuredescriptions, which constrain possible HPSG feature structuresby asserting specific attributes and/or labels.A HPSG based chart parser, under development a theauthor's university, was used for the implementation part ofthis study.FEATURE MAPPINGPlanning & generation of coherent "speech" in aconversation requires ome understanding of the "hearer's"perspective.
Thus the speaker naturally has some (limited)knowledge about possible responses from the hearer.
Thisknowledge can be given to the same planner used forgeneration, producing a partial set of pragmatic features orexpectations.
These pragmatic expectations can then bemapped into the systemic grammar, producing a set ofsemantic and syntactic expectations about what otherparticipants inthe conversation will say.The technique explored here is to bring such expectationsto bear during the parse process, guiding the parser to the mostlikely solution in a best-first manner.
It is thus necessary thatthe generated expectations be mapped into a form which can bedirectly compared with constituents proposed within the HPSGparse.Consider the sentence "Robin promised to come atnoon", with the following context:Sandy: "I guess we should get started, what time did theysay they would be here?
"Kim: "Robin promised to come at noon"A set of plausible partial expectations generated by thepragmatic and systemic omponents in anticipation of Kim'sresponse might be:((S) (UNMARKED-DECLARATIVE))((S SUBJECT) (PROPER))((S BETA) (NONFINITEPRED))((S PREDICATOR) (PROMISED))((S BETA TEMPORAL) (PP))((S BETA PREDICATOR) (ARRIVAL))In these xpectations the first list of each pair (e.g.
(S BETA))represents a functional role within the expected sentence.
The295second list in each pair are sets (in this case singleton) ofexpected features for the associated functional roles.
Theseexpected features assert expectations which are both semantic(e.g.
PROMISED) and syntactic (e.g.
((S BETATEMPORAL) (PP)) asserts both the existance and location ofa temporal adjunct PP).Note that in these expectations the temporal adjunct "atnoon" should modify the embedded clause "to come", as wouldbe expected in the specified context.Next consider the possible HPSG parses of the examplesentence.
Figures 1 and 2 below illustrate two semanticallydistinct parses generated by our HPSG parser.S Hs// H /  vPNP V V V PPRobin promised to come at noonFigure 1.S H/ / ,V-- vr \NP V V V PPRobin promised to come at noonFigure 2.Mapping expected features into HPSG constraints:Features generated from pragmatic expectations can bemapped into constraints on HPSG structures, tated in termsof feature descriptions.
Below are the HPSG featuredescriptions corresponding to the pragmatically generatedfeatures PP and UNMARKED-DECLARATIVE.PP = SYNSEMILOCICAT HEAD prep.
\[MARKING unmarked\]\]phrase cat 'Figure 3.UNMARKED-DECLARATIVE =FDTRSIHEAD-DTRISYNSEM v_EphraseLSU~-DTRISYNSEMILOCICATIH EAD __Figure 4.nounMapping expected funct ional  roles into HPSGconstituent structure:Pragmatic expectations are expected within certainfunctional roles, such a SUBJECT, PREDICATOR, BETA(the embedded clause) etc.
This structural information must beused to assert the constraints into the relevant HPSGsubstructures.
This mapping is not as straightforward as thefeature mapping technique, as the structure induced by thesystemic grammar is "flatter" than the structure produced byHPSG.Consider the following pragmatically generatedexpectation:((S TEMPORAL) (PP))Such an expectation may be realized by great variety ofHPSG structural realizations, e.g.:1.
Kim ran at noon2.
Kim could run home at noon3.
K.im could have been running home at noon4.
Kim ran east at noon.In these xamples modal verb operators (1-3) and multipleadjuncts (4) vary the actual structural depth of the temporal PPwithin the HPSG model.
Thus a given systemic role pathmay have numerous HI~G constituent path realizations.
Onepossible mapping technique is to generate constraintsexpressing all possible HPSG structural variants.
This,however would lead in many cases to a combinatorialexplosion of constraints.
The technique mployed by thisstudy was to add a new clause attribute to verbal HPSG signs,and use this attribute to embed within the signs a "clausallyflattened" structures.
Each HPSG verbal sign in the sameclause structure shares the same clausal value.
The clausevalue is a structure with labels for each systemic role, whereeach label points to the constituent which fills that role in thegiven verbal clause.
A clausal boundry is said to existbetween distinct clausal domains.
A clausal structure isillustrated in figure 5:~"~vP IF~IV I~ v\[r~" I \] V / -  / ~ % P \ [  \[~\] PR?bin promlised "E" H\[~ H/ /~pv\[N\] I I come at noon\[C F PREDICATOR V\[pr?mised\] \ ] \ ]  \ [ \ ]  LAUSE | SUBJECT NP\[Robin\]=- BETA VP\[to come at noon\]\]rEI~LAUSE r PREDICATOR V\[come\] LTEMPORAL PP\[atnoon\]\] \ ] \ ]  I=Figure 5.The current mapping on ly  considers the mapping of roleswithin verbal signs.
Similar role structures may exist forother constituent types, such as for noun phrase.
Thus far theverbal clause boundary definition has been adequate for otherphrasal structures.GUIDING THE HPSG PARSEThe guidance strategy employed is to evaluate allproposed edges (i.e.
complete and partially completeconstituents) against the expectations, ranking each based onthe relative similarity with the expectations.
These edges are296then placed in an agenda (a list of priority queues) andremoved from the agenda nd included in the partial parse in abest first order.Critical to the success of a best-first algorithm is theheuristic evaluation function used to order the proposedconstituents.The heuristic evaluation function:The heuristic evaluation function is based on three specifictypes of tests:I.
Role match - does a constituent match a role's set ofexpected features?II.
Role path match - is a constituent role path compatiblewith the roles of its children?III.
Clausal completeness - are all clausal roles expected forthis constituent present?Tests II and III above require that constituents underconsideration have roles already assigned to them.
Forexample, in the case of II, the test requires roles for both thenew constituent and the proposed aughters of the constituent.But since the parse strategy employeed is bottom-up, rolepaths cannot be anchored to a root, and thus fully known, untilparse completion.
The solution to this dilemma is tohypothesise a constituent's role using a process similar toabduction.
Two types of knowledge are exploited in thisprocess.
First, roles with features which subsume or areconsistant with a proposed constituent are considered goodcandidate roles.
Also, roles may also be inferred by projectingup from the roles already hypothesized for the children.
Byintersecting these two sources of role evidence, the list ofhypothesized roles can be refined (by ruling out roles withoutboth types of evidence).
In this manner the hypothesized rolesof later constituents can be refined from descendantconstituents.
In the case of roles projected from daughters,clausal boundary knowledge must be applied to correctly inferthe parent role.EVALUATION & TESTINGThe techniques described here have been used successfullyto guide the parsing of several sentences taken from realconversations.
The pragmatic and semantic knowledge alreadyexisted from Patten's research (Patten, 1992) to generate thesesentences.
A subset of this knowledge, judged to represent thepartial knowledge available to a listener, was used to generateexpectations in the form described above.The parser used in this study by default produced allpossible parses.
The modified version attempts to converge onthe "expected" parse first, and terminate.
For each sentencetested the parser converges on the correct parse first.
When theexpectations are modified to expect adifferent parse, a different(and correct) parse is found first.
The results in terms ofspeedup vary considerably depending on the level of ambiguitypresent in the sentence.
The most complex sentence parsedthus far exhibits considerable speedup.
When unguided, theparser produces 24 parses, and considers a total of 252 distinctconstituents.
In the guided case, the parser only considers 39constituents, and converges on the one "correct" parse first.Within the current esting environment, this guidence resultsin a greater then ten-fold speedup in terms of CPU time.SUMMARYPragmatic knowledge about language usage in routineconversational contexts can be highly compiled.
Thisknowledge can be used to produce semantic and syntacticexpectations about next turns in conversation, especially ofnext turns that are second members of adjacency pairs(Schegloff & Sacks 1973).
By mapping expected features intoHPSG constraints, and by augmenting HPSG sign structuresto model the role structure of systemic grammar, theseexpectations can be used as constraints on possible constituentstructures of a HPSG constituent.
Given this mapping, theexpectations may then be used to order the parse process,guiding the parse, and avoiding the consideration ofpragmatically unlikely constructions.
This process reduces thenumber of constituents considered uring parsing, reducingparse time and permitting the parser to correctly select theparse most like the pragmatic expectations,This solution closely follows a classical A.I.
searchtechnique called a best-first search.
The heuristic evaluationfunction used to classify the proposed constituents for bestfirst ordering uses inference similar to abductive reasoning.One benefit of this solution is that it retains themodularity of the syntactic and semantic omponents, notrequiring aspecialized grammar for each contextual domain.
Inadditional, as the coverage of the grammar increases, the searchspace will also increase, and thus possible benefits increase.Work is continuing on this study.
Currently the heuristicis being enhanced to consider the specificity of an expectationmatch, ordering those edges which match the most specificfeatures first.
In addition, work is in progress to extend thecoverage of the grammar and mapping to include theconversation domain utilized in Patten, Geis & Becker 1992.ReferencesGeis, Mike.
L. and Harlow, L. "Politeness Strategies inFrench and English: Implications for Second LanguageAcquisition"Mac Gregor, R., "LOOM Users Manual", University ofSouthern California, lnformations Sciences Institute,1991.Patten, Terry.
; Geis, Mike.
and Becker, Barbara., "Toward aTheory of Compilation for Natural-l_anguage G neration,"Computationallntelligence 8(1), 1992, pp 77-101.Pollard, Carl and Sag, Ivan A., "Head-Driven Phrase StructureGrammar", unpublished manuscript draft, 1992.Pollard, Carl.
and Sag, Ivan A., "Information-Based Syntaxand Semantics: Volume 1, Fundamentals", Center for theStudy of Language and Information, 1987.Schegloff, E.A.
and Sacks, H. Opening up closings.Semiotica, 7,4:289-387, 1973.Winograd, Terry.
1983.
"Language as a Cognitive Process",Addison-Wesley, Menlo Park, CA.297
