A Study of Chinese Lexical Analysis Based on Discriminative ModelsGuang-Lu Sun  Cheng-Jie Sun  Ke Sun and Xiao-Long WangIntelligent Technology & Natural Language Processing Laboratory, School of ComputerScience and Technology, Harbin Institute of Technology, 150001, Harbin, China{glsun, cjsun, ksun, wangxl}@insun.hit.edu.cnAbstractThis paper briefly describes our system inThe Fourth SIGHAN Bakeoff.Discriminative models including maximumentropy model and conditional randomfields are utilized in Chinese wordsegmentation and named entity recognitionwith different tag sets and features.Transformation-based learning model isused in part-of-speech tagging.
Evaluationshows that our system achieves theF-scores: 92.64% and 92.73% in NCCWord Segmentation close and open tests,89.11% in MSRA name entity recognitionopen test, 91.13% and 91.97% in PKUpart-of-speech tagging close and open tests.All the results get medium performanceson the bakeoff tracks.1 IntroductionLexical analysis is the basic step in naturallanguage processing.
It is prerequisite to manyfurther applications, such as question answersystem, information retrieval and machinetranslation.
Chinese lexical analysis chieflyconsists of word segmentation (WS), name entityrecognition (NER) and part-of-speech (POS)tagging.
Because Chinese does not have explicitword delimiters to mark word boundaries likeEnglish, WS is essential process for Chinese.
POStagging and NER are just like those of English.Our system participated in The Fourth SIGHANBakeoff which held in 2007.
Different approachesare applied to solve all the three tasks which areintegrated into a unified system (ITNLP-IsLex).For WS task, conditional random fields (CRF) areused.
For NER, maximum entropy model (MEM)is applied.
And transformation-based learning(TBL) algorithm is utilized to solve POS taggingproblem.
The reasons using different models arelisted in the rest sections of this paper.
We give abrief introduction to our system sequentially.Section 2 describes WS.
Section 3 and section 4introduce NER and POS tagging respectively.
Wegive some experimental results in section 5.
Finallywe draw some conclusions.2 Chinese word segmentationFor WS task, NCC corpus is chosen both in closetest and open test.2.1 Conditional random fieldsConditional random fields are undirected graphicalmodels defined by Lafferty (2001).
There are twoadvantages of CRF.
One is their great flexibility toincorporate various types of arbitrary,non-independent features of the input, the other istheir ability to overcome the label bias problem.Given the observation sequence X, on the basisof CRF, the conditional probability of the statesequence Y is:( ) (k k i-1 ik1p Y X = exp l f y , y ,X,iZ(X))?
??
??
??
(1)(k k i-1 iy Y k)Z(X)= exp l f y , y ,X,i??
??
??
??
?
(2)Z(x) is the normalization factor.
( )1, , ,k i if y y X i?is the universal definition of features in CRF.2.2 Word segmentation based on CRFInspired by Zhao (2006), the Chinese WS task isconsidered as a sequential labeling problem, i.e.,assigning a label to each character in a sentencegiven its contexts.
CRF model is adopted to dolabeling.6 tags are utilized in this work: B, B1, B2, I, E, S.The meaning of each tag is listed in Table 1.
The147Sixth SIGHAN Workshop on Chinese Language Processingraw training file format from NCC can be easily toconvert to this 6 tags format.?
?
?
?
?
?An example: /S /B /B1 /B2 /I /I?
?
?
?
?
?
?
?
?/I /I /I /E /B /E /B /E /S.Table 1 Tags of character-based labelingTag MeaningB The 1st character of a multi-character wordB1 The 2nd character of a multi-character wordB2 The 3rd character of a multi-character wordI Other than B, B1, B2 and last character in a multi-character wordE The last character of a multi-character wordS Single character wordThe contexts window size for each character is 5:C-2, C-1, C0, C1, and C2.
There are 10 featuretemplates used to generate features for CRF modelincluding uni-gram, bi-gram and tri-gram: C-2, C-1,C0, C1, C2, C-1C0, C0C1, C-2C-1C0, C-1C0C1, andC0C1C2.For the parameters in CRF model, we only dowork to choose cut-off value for features.
Ourexperiments show that the best performance can beachieved when cut-off value is set to 2.Maximum likelihood estimation and L-BFGSalgorithm is used to estimate the weight ofparameters in the training module.
Baum-Welchalgorithm is used to search the best sequence oftest data.For close test, we only used CRF to dosegmentation, no more post-processing, such astime and date finding, was done.
So theperformance could be further improved.For open test, we just use our NER system to tagthe output of our close segmentation result, no moreother resources were involved.3 Chinese name entity recognitionFor NER task, MSRA is chosen in open test.Chinese name dictionary, foreign name dictionary,Chinese place dictionary and organizationdictionary are used in the model.3.1 Maximum entropy modelMaximum entropy model is an exponentialmodel that offers the flexibility of integratingmultiple sources of knowledge into a model(Berger, 1996).
It focuses on the modeling oftagging sequence, replacing the modeling ofobservation sequence.Given the observations sequence X, on the basisof MEM, the conditional probability of the statesequence Y is:1( | ) exp ( , )( ) j jjp Y X f Y XZ X??
?= ???
??
??
(3)( ) exp ( , )j jY jZ X f??
?= ???
??
?
Y X ??
(4)Table 2 Feature templates of NERFeaturetemplate DescriptionCiThe word tokens in thewindowi =-2, -1, 0, 1, 2TiThe NE tagsi = -1CiCi-1The bigram of Cii = -1, 1PiThe POS tags of wordtokensi = -1, 0, 1P-1P1The combination of POStagsT-1C0The previous tag and thecurrent word tokenB Ci is Chinese family nameC Ci is part of Chinese first nameW Ci is Chinese whole nameF Ci is foreign nameS Ci is Chinese first nameW(Ci)O otherW(Ci-1)W(Ci)The bigram of W(Ci)i = -1, 1IsInOrgDict(C0)The current word token is inorganization dictionaryIsInPlaceDict(C0)The current word token is inplace dictionary148Sixth SIGHAN Workshop on Chinese Language ProcessingBeing Similar to the definition of CRF, Z(x) isthe normalization factor.
( ),jf Y X is the universaldefinition of features.3.2 Name entity recognition based on MEMFirstly, we use a segmentation tool to split bothtraining and test corpus into word-token-basedtexts.
Characters that are not in the dictionary arescattered in the texts.
NE tags using in the modelfollow the tags in training corpus.
Other wordtokens that do not belong to NE are tagged as O.Based on the segmented text, the context windowis also set as 5.
Inspired by Zhang?s (2006) work,there are 10 types of feature templates forgenerating features for NER model in Table 2.When training our ME Model, the bestperformance can be achieved when cut-off value isset to 1.Maximum likelihood estimation and GISalgorithm is used to estimate the weight ofparameters in the model.
The iteration time is 500.4 Chinese part-of-speech taggingFor POS tagging task, NCC corpus and PKUcorpus are chosen both in the close test and opentest.4.1 Transformation-based learningThe formalism of Transformation-based learning isfirst introduced in 1992.
It starts with the correctlytagged training corpus.
A baseline heuristic forinitial tag and a set of rule templates that specify thetransformation rules match the context of a word.By transformating the error initial tags to the correctones, a set of candidate rules are built to be theconditional pattern based on which thetransformation is applied.
Then, the candidate rulewhich has the best transformation effect is selectedand stored as the first transformation rules in theTBL model.
The training process is repeated untilno more candidate rule has the positive effect.
Theselected rules are stored in the learned rule sequencein turn for the purpose of template correctionlearning.4.2 Part-of-speech tagging based on TBLPOS tagging is a standard sequential labelingproblem.
CRF has some advantages to solve it.Because both corpora have relative many POS tags,our computational ability can not afford the CRFmodel in condition of these tags.
TBL model isutilized to replace with CRF.We compute the max probability of currentword?s POS tag in training corpus.
The POS tagwhich has max occurrence probability for eachword is used to tag its word token.
By this method,we got the initial POS tag for each word.The rule templates which are formed fromconjunctions of words match to particularcombinations in the histories of the currentposition.
40 types of rule templates are built usingthe patterns.
The cut-off value of thetransformation rules is set to 3 (Sun, 2007).For open test, our NER system is used to tag theoutput of our POS tagging result.
Parts of NE tagsare corrected.5 EvaluationFollowing the measurement approach adopted inSIGHAN, we measure the performance of the threetasks in terms of the precision (P), recall (R), andF-score (F).5.1 Word segmentation resultsTable 3 Word segmentation results on NCC corpusNCC close test open testR .9268 .9268Cr .00133447 .00133458P .926 .928Cp .00134119 .00132534F .9264 .9273Roov .6094 .6265Poov .4948 .5032Foov .5462 .5581Riv .9426 .9417Piv .9527 .9546Fiv .9476 9481The WS results are listed on the Table 3.
Someerrors could be caused by the annotationdifferences between the training data and test data.For example, ????
(A Zhen) was considered as awhole word in training data, while ????
(A Lan)was annotated as two separate word ???
(A) and???
(Lan) in the test data.
Some post-processingrules for English words, money unit andmorphology can improve the performance further,Following are such errors in our results: ?vid eo?,149Sixth SIGHAN Workshop on Chinese Language Processing??
??
(Japan yen), ??
?
?
??
(not threenot four).For open test, we hoped to use NER module toincrease the OOV recall.
But the NER moduledidn?t prompt the performance very much becauseit was trained by the MSRA NER data in Bakeoff3.The difference between two corpora may depressthe NER modules effect.
Also, the open test wasdone on the output of close test and all the errorswere passed.5.2 Name entity recognition resultsThe official results of our NER system on MSRAcorpus for open track are showed in Table 4.
As itshows, our system achieves a relatively high scoreon both PER and LOC task, but the performance ofORG is not so good, and the Avg1 performance isdecreased by it.
The reasons are: (1) The ORGsequences are often very long and our system isunable to deal with the long term, a MEMM orCRF model may perform better.
(2) The resourcefor LOC and ORG are much smaller than that ofPER.
More sophisticated features such like?W(Ci)?
may provide more useful information forthe system.Table 4 NER results on MSRA corpusMSRA P R FPER .9498 .9549 .9524LOC .9129 .9194 .9161ORG .8408 .7469 .7911Avg1 .9035 .8791 .89115.3 Part-of-speech tagging resultsWe evaluate our POS tagging model on the PKUcorpus for close and open track and NCC corpusfor close track based on TBL.
Table 5 is theofficial result of our system.
In PKU open test,NER is used to recognize name entity of text, so itsresult is better than that of close test.
The IV-Rresult is relative good, but the OOV-R is not sogood, which drops the total performance.
Thereasons lie in: (1) TBL model is not good attagging out of vocabulary words.
CRF model maybe a better selection if our computer can meet itshuge memory requirements.
(2) Our NER systemis trained by MSRA corpus.
It does not fit the PKUand NCC corpus.Table 5 POS results on PKU and NCC corpusCorpus Total-A IV-R OOV-R MT-RPKU closetest .9113 .9518 .2708 .8958PKU opentest .9197 .9512 .4222 .899NCC closetest .9277 .9664 .2329 .96 ConclusionsChinese lexical analysis system is built for theSIGHAN tracks which consists of Chinese wordsegmentation, name entity recognition andpart-of-speech tagging.
Conditional random fields,maximum entropy model and transformation-basedlearning model are utilized respectively.
Oursystem achieves the medium results in all the threetasks.ReferencesA.
Berger, S. A. Della Pietra and V. J. Della Pietra.
AMaximum Entropy Approach to Natural LanguageProcessing.
Computational Linguistics, 1996.
22(1),pages 39-71.G.
Sun, Y. Guan and X. Wang.
A Maximum EntropyChunking Model With N-fold Template Correction.Journal of Electronics, 2007.
24(5), pages 690-695.J.
Lafferty, A. McCallum and F. Pereira.
Conditionalrandom fields: Probabilistic models for segmentingand labeling sequence data.
In Proceedings ofICML-2001, Williams College, Massachusetts, USA.2001.
pages 282-289.S.
Zhang, Y. Qin, J. Wen, X. Wang.
WordSegmentation and Named Entity Recognition forSIGHAN Bakeoff3.
In Proceedings of the FifthSIGHAN Workshop on Chinese Language Processing.Sydney, Australia.
2006. pages 158?161.H.
Zhao, C. Huang, and M. Li.
An improved Chineseword segmentation system with conditional randomfield.
In Proceedings of the Fifth SIGHAN Workshopon Chinese Language Processing, Sydney, Australia.2006.
pages 162?165.150Sixth SIGHAN Workshop on Chinese Language Processing
