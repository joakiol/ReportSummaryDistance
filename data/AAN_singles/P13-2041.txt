Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 228?232,Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational LinguisticsUnsupervised joke generation from big dataSas?a Petrovic?School of InformaticsUniversity of Edinburghsasa.petrovic@ed.ac.ukDavid MatthewsSchool of InformaticsUniversity of Edinburghdave.matthews@ed.ac.ukAbstractHumor generation is a very hard problem.It is difficult to say exactly what makes ajoke funny, and solving this problem al-gorithmically is assumed to require deepsemantic understanding, as well as cul-tural and other contextual cues.
We departfrom previous work that tries to model thisknowledge using ad-hoc manually createddatabases and labeled training examples.Instead we present a model that uses largeamounts of unannotated data to generate Ilike my X like I like my Y, Z jokes, whereX, Y, and Z are variables to be filled in.This is, to the best of our knowledge, thefirst fully unsupervised humor generationsystem.
Our model significantly outper-forms a competitive baseline and gener-ates funny jokes 16% of the time, com-pared to 33% for human-generated jokes.1 IntroductionGenerating jokes is typically considered to be avery hard natural language problem, as it impliesa deep semantic and often cultural understandingof text.
We deal with generating a particular typeof joke ?
I like my X like I like my Y, Z ?
where Xand Y are nouns and Z is typically an attribute thatdescribes X and Y.
An example of such a joke isI like my men like I like my tea, hot and British ?these jokes are very popular online.While this particular type of joke is not interest-ing from a purely generational point of view (thesyntactic structure is fixed), the content selectionproblem is very challenging.
Indeed, most of theX, Y, and Z triples, when used in the context ofthis joke, will not be considered funny.
Thus, themain challenge in this work is to ?fill in?
the slotsin the joke template in a way that the whole phraseis considered funny.Unlike the previous work in humor generation,we do not rely on labeled training data or hand-coded rules, but instead on large quantities ofunannotated data.
We present a machine learningmodel that expresses our assumptions about whatmakes these types of jokes funny and show that byusing this fairly simple model and large quantitiesof data, we are able to generate jokes that are con-sidered funny by human raters in 16% of cases.The main contribution of this paper is, to thebest of our knowledge, the first fully unsupervisedjoke generation system.
We rely only on largequantities of unlabeled data, suggesting that gener-ating jokes does not always require deep semanticunderstanding, as usually thought.2 Related WorkRelated work on computational humor can be di-vided into two classes: humor recognition and hu-mor generation.
Humor recognition includes dou-ble entendre identification in the form of That?swhat she said jokes (Kiddon and Brun, 2011),sarcastic sentence identification (Davidov et al,2010), and one-liner joke recognition (Mihalceaand Strapparava, 2005).
All this previous workuses labeled training data.
Kiddon and Brun(2011) use a supervised classifier (SVM) trainedon 4,000 labeled examples, while Davidov et al(2010) and Mihalcea and Strapparava (2005) bothuse a small amount of training data followed by abootstrapping step to gather more.Examples of work on humor generation includedirty joke telling robots (Sjo?bergh and Araki,2008), a generative model of two-liner jokes (Lab-utov and Lipson, 2012), and a model of punningriddles (Binsted and Ritchie, 1994).
Again, all thiswork uses supervision in some form: Sjo?bergh andAraki (2008) use only human jokes collected fromvarious sources, Labutov and Lipson (2012) use asupervised approach to learn feasible circuits thatconnect two concepts in a semantic network, and228ZYX?1(Z)?
(Y, Z)?
(X, Z)?
(X, Y)?2(Z)Figure 1: Our model presented as a factor graph.Binsted and Ritchie (1994) have a set of six hard-coded rules for generating puns.3 Generating jokesWe generate jokes of the form I like my X like I likemy Y, Z, and we assume that X and Y are nouns,and that Z is an adjective.3.1 ModelOur model encodes four main assumptions aboutI like my jokes: i) a joke is funnier the more oftenthe attribute is used to describe both nouns, ii) ajoke is funnier the less common the attribute is, iii)a joke is funnier the more ambiguous the attributeis, and iv) a joke is funnier the more dissimilarthe two nouns are.
A graphical representation ofour model in the form of a factor graph is shownin Figure 1.
Variables, denoted by circles, and fac-tors, denoted by squares, define potential functionsinvolving the variables they are connected to.Assumption i) is the most straightforward, andis expressed through ?
(X,Z) and ?
(Y, Z) factors.Mathematically, this assumption is expressed as:?
(x, z) = p(x, z) = f(x, z)?x,z f(x, z), (1)where f(x, z)1 is a function that measures the co-occurrence between x and z.
In this work we sim-ply use frequency of co-occurrence of x and z insome large corpus, but other functions, e.g., TF-IDF weighted frequency, could also be used.
Thesame formula is used for ?
(Y,Z), only with dif-ferent variables.
Because this factor measures the1We use uppercase to denote random variables, and low-ercase to denote random variables taking on a specific value.similarity between nouns and attributes, we willalso refer to it as noun-attribute similarity.Assumption ii) says that jokes are funnier if theattribute used is less common.
For example, thereare a few attributes that are very common and canbe used to describe almost anything (e.g., new,free, good), but using them would probably leadto bad jokes.
We posit that the less common theattribute Z is, the more likely it is to lead to sur-prisal, which is known to contribute to the funni-ness of jokes.
We express this assumption in thefactor ?1(Z):?1(z) = 1/f(z) (2)where f(z) is the number of times attribute z ap-pears in some external corpus.
We will refer to thisfactor as attribute surprisal.Assumption iii) says that more ambiguous at-tributes lead to funnier jokes.
This is based on theobservation that the humor often stems from thefact that the attribute is used in one sense whendescribing noun x, and in a different sense whendescribing noun y.
This assumption is expressedin ?2(Z) as:?2(z) = 1/senses(z) (3)where senses(z) is the number of different sensesthat attribute z has.
Note that this does not exactlycapture the fact that z should be used in differentsenses for the different nouns, but it is a reason-able first approximation.
We refer to this factor asattribute ambiguity.Finally, assumption iv) says that dissimilarnouns lead to funnier jokes.
For example, if thetwo nouns are girls and boys, we could easily findmany attributes that both nouns share.
However,since the two nouns are very similar, the effect ofsurprisal would diminish as the observer would ex-pect us to find an attribute that can describe bothnouns well.
We therefore use ?
(X,Y ) to encour-age dissimilarity between the two nouns:?
(x, y) = 1/sim(x, y), (4)where sim is a similarity function that measureshow similar nouns x and y are.
We call this fac-tor noun dissimilarity.
There are many similar-ity functions proposed in the literature, see e.g.,Weeds et al (2004); we use the cosine betweenthe distributional representation of the nouns:sim(x, y) =?z p(z|x)p(z|y)?
?z p(z|x)2 ?
?z p(z|y)2(5)229Equation 5 computes the similarity between thenouns by representing them in the space of all at-tributes used to describe them, and then taking thecosine of the angle between the noun vectors inthis representation.To obtain the joint probability for an (x, y, z)triple we simply multiply all the factors and nor-malize over all the triples.4 DataFor estimating f(x, y) and f(z), we use Googlen-gram data (Michel et al, 2010), in particular theGoogle 2-grams.
We tag each word in the 2-gramswith the part-of-speech (POS) tag that correspondsto the most common POS tag associated with thatword in Wordnet (Fellbaum, 1998).
Once we havethe POS-tagged Google 2-gram data, we extractall (noun, adjective) pairs and use their counts toestimate both f(x, z) and f(y, z).
We discard 2-grams whose count in the Google data is less than1000.
After filtering we are left with 2 million(noun, adjective) pairs.
We estimate f(z) by sum-ming the counts of all Google 2-grams that con-tain that particular z.
We obtain senses(z) fromWordnet, which contains the number of senses forall common words.It is important to emphasize here that, while wedo use Wordnet in our work, our approach does notcrucially rely on it, and we use it to obtain onlyvery shallow information.
In particular, we useWordnet to obtain i) POS tags for Google 2-grams,and ii) number of senses for adjectives.
POS tag-ging could be easily done using any one of thereadily available POS taggers, but we chose thisapproach for its simplicity and speed.
The numberof different word senses for adjectives is harder toobtain without Wordnet, but this is only one of thefour factors in our model, and we do not dependcrucially on it.5 ExperimentsWe evaluate our model in two stages.
Firstly, usingautomatic evaluation with a set of jokes collectedfrom Twitter, and secondly, by comparing our ap-proach to human-generated jokes.5.1 InferenceAs the focus of this paper is on the model, not theinference methods, we use exact inference.
Whilethis is too expensive for estimating the true proba-bility of any (x, y, z) triple, it is feasible if we fixone of the nouns, i.e., if we deal with P (Y, Z|X =x).
Note that this is only a limitation of our infer-ence procedure, not the model, and future workwill look at other ways (e.g., Gibbs sampling) toperform inference.
However, generating Y andZ given X , such that the joke is funny, is still aformidable challenge that a lot of humans are notable to perform successfully (cf.
performance ofhuman-generated jokes in Table 2).5.2 Automatic evaluationIn the automatic evaluation we measure the effectof the different factors in the model, as laid out inSection 3.1.
We use two metrics for this evalua-tion.
The first is similar to log-likelihood, i.e., thelog of the probability that our model assigns to atriple.
However, because we do not compute it onall the data, just on the data that contains the Xsfrom our development set, it is not exactly equalto the log-likelihood.
It is a local approximationto log-likelihood, and we therefore dub it LOcalLog-likelihood, or LOL-likelihood for short.
Oursecond metric computes the rank of the human-generated jokes in the distribution of all possiblejokes sorted decreasingly by their LOL-likelihood.This Rank OF Likelihood (ROFL) is computedrelative to the number of all possible jokes, andlike LOL-likelihood is averaged over all the jokesin our development data.
One advantage of ROFLis that it is designed with the way we generatejokes in mind (cf.
Section 5.3), and thus more di-rectly measures the quality of generated jokes thanLOL-likelihood.
For measuring LOL-likelihoodand ROFL we use a set of 48 jokes randomly sam-pled from Twitter that fit the I like my X like I likemy Y, Z pattern.Table 1 shows the effect of the different fac-tors on the two metrics.
We use a model withonly noun-attribute similarity (factors ?
(X,Z)and ?
(Y, Z)) as the baseline.
We see that the sin-gle biggest improvement comes from the attributesurprisal factor, i.e., from using rarer attributes.The best combination of the factors, according toautomatic metrics, is using all factors except forthe noun similarity (Model 1), while using all thefactors is the second best combination (Model 2).5.3 Human evaluationThe main evaluation of our model is in terms ofhuman ratings, put simply: do humans find thejokes generated by our model funny?
We comparefour models: the two best models from Section 5.2230Model LOL-likelihood ROFLBaseline -225.3 0.1909Baseline + ?
(X,Y ) -227.1 0.2431Baseline + ?1(Z) -204.9 0.1467Baseline + ?2(Z) -224.6 0.1625Baseline + ?1(Z) + ?2(Z) (Model 1) -198.6 0.1002All factors (Model 2) -203.7 0.1267Table 1: Effect of different factors.
(one that uses all the factors (Model 2), and onethat uses all factors except for the noun dissimilar-ity (Model 1)), a baseline model that uses only thenoun-attribute similarity, and jokes generated byhumans, collected from Twitter.
We sample a fur-ther 32 jokes from Twitter, making sure that therewas no overlap with the development set.To generate a joke for a particular x we keep thetop n most probable jokes according to the model,renormalize their probabilities so they sum to one,and sample from this reduced distribution.
This al-lows our model to focus on the jokes that it consid-ers ?funny?.
In our experiments, we use n = 30,which ensures that we can still generate a varietyof jokes for any given x.In our experiments we showed five native En-glish speakers the jokes from all the systems in arandom, per rater, order.
The raters were askedto score each joke on a 3-point Likert scale: 1(funny), 2 (somewhat funny), and 3 (not funny).Naturally, the raters did not know which approacheach joke was coming from.
Our model was usedto sample Y and Z variables, given the same Xsused in the jokes collected from Twitter.Results are shown in Table 2.
The second col-umn shows the inter-rater agreement (Randolph,2005), and we can see that it is generally good, butthat it is lower on the set of human jokes.
We in-spected the human-generated jokes with high dis-agreement and found that the disagreement maybe partly explained by raters missing cultural ref-erences in the jokes (e.g., a sonic screwdriver isDoctor Who?s tool of choice, which might be loston those who are not familiar with the show).We do not explicitly model cultural references,and are thus less likely to generate such jokes,leading to higher agreement.
The third columnshows the mean joke score (lower is better), andwe can see that human-generated jokes were ratedthe funniest, jokes from the baseline model theleast funny, and that the model which uses all theModel ?
Mean % funny jokesHuman jokes 0.31 2.09 33.1Baseline 0.58 2.78 3.7Model 1 0.52 2.71 6.3Model 2 0.58 2.56 16.3Table 2: Comparison of different models on thetask of generating Y and Z given X.factors (Model 2) outperforms the model that wasbest according to the automatic evaluation (Model1).
Finally, the last column shows the percentageof jokes the raters scored as funny (i.e., the num-ber of funny scores divided by the total number ofscores).
This is a metric that we are ultimatelyinterested in ?
telling a joke that is somewhatfunny is not useful, and we should only rewardgenerating a joke that is found genuinely funnyby humans.
The last column shows that human-generated jokes are considered funnier than themachine-generated ones, but also that our modelwith all the factors does much better than the othertwo models.
Model 2 is significantly better thanthe baseline at p = 0.05 using a sign test, andhuman jokes are significantly better than all threemodels at p = 0.05 (because we were testing mul-tiple hypotheses, we employed Holm-Bonferronicorrection (Holm, 1979)).
In the end, our bestmodel generated jokes that were found funny byhumans in 16% of cases, compared to 33% ob-tained by human-generated jokes.Finally, we note that the funny jokes generatedby our system are not simply repeats of the humanjokes, but entirely new ones that we were not ableto find anywhere online.
Examples of the funnyjokes generated by Model 2 are shown in Table 3.6 ConclusionWe have presented a fully unsupervised humorgeneration system for generating jokes of the type231I like my relationships like I like my source, openI like my coffee like I like my war, coldI like my boys like I like my sectors, badTable 3: Example jokes generated by Model 2.I like my X like I like my Y, Z, where X, Y, and Z areslots to be filled in.
To the best of our knowledge,this is the first humor generation system that doesnot require any labeled data or hard-coded rules.We express our assumptions about what makes ajoke funny as a machine learning model and showthat by estimating its parameters on large quanti-ties of unlabeled data we can generate jokes thatare found funny by humans.
While our experi-ments show that human-generated jokes are fun-nier more of the time, our model significantly im-proves upon a non-trivial baseline, and we believethat the fact that humans found jokes generated byour model funny 16% of the time is encouraging.AcknowledgementsThe authors would like to thank the raters for theirhelp and patience in labeling the (often not sofunny) jokes.
We would also like to thank MichaElsner for this helpful comments.
Finally, wethank the inhabitants of offices 3.48 and 3.38 forputting up with our sniggering every Friday after-noon.ReferencesKim Binsted and Graeme Ritchie.
1994.
An imple-mented model of punning riddles.
In Proceedingsof the twelfth national conference on Artificial intel-ligence (vol.
1), AAAI ?94, pages 633?638, MenloPark, CA, USA.
American Association for ArtificialIntelligence.Dmitry Davidov, Oren Tsur, and Ari Rappoport.
2010.Semi-supervised recognition of sarcastic sentencesin twitter and amazon.
In Proceedings of the Four-teenth Conference on Computational Natural Lan-guage Learning, CoNLL ?10, pages 107?116.Christiane Fellbaum.
1998.
Wordnet: an electroniclexical database.
MIT Press.Sture Holm.
1979.
A simple sequentially rejectivemultiple test procedure.
Scandinavian journal ofstatistics, pages 65?70.Chloe?
Kiddon and Yuriy Brun.
2011.
That?s what shesaid: double entendre identification.
In Proceedingsof the 49th Annual Meeting of the ACL: Human Lan-guage Technologies: short papers - Volume 2, pages89?94.Igor Labutov and Hod Lipson.
2012.
Humor as cir-cuits in semantic networks.
In Proceedings of the50th Annual Meeting of the ACL (Volume 2: ShortPapers), pages 150?155, July.Jean-Baptiste Michel, Yuan Kui Shen, Aviva PresserAiden, Adrian Veres, Matthew K. Gray, TheGoogle Books Team, Joseph P. Pickett, Dale Hol-berg, Dan Clancy, Peter Norvig, Jon Orwant, StevenPinker, Martin A. Nowak, and Erez LiebermanAiden.
2010.
Quantitative analysis of culture usingmillions of digitized books.
Science.Rada Mihalcea and Carlo Strapparava.
2005.
Makingcomputers laugh: investigations in automatic humorrecognition.
In Proceedings of the conference onHuman Language Technology and EMNLP, pages531?538.Justus J. Randolph.
2005.
Free-marginal multi-rater kappa (multirater free): An alternative to fleissfixed- marginal multirater kappa.
In Joensuu Uni-versity Learning and Instruction Symposium.Jonas Sjo?bergh and Kenji Araki.
2008.
A com-plete and modestly funny system for generating andperforming japanese stand-up comedy.
In Coling2008: Companion volume: Posters, pages 111?114,Manchester, UK, August.
Coling 2008 OrganizingCommittee.Julie Weeds, David Weir, and Diana McCarthy.
2004.Characterising measures of lexical distributionalsimilarity.
In Proceedings of the 20th internationalconference on Computational Linguistics, COLING?04, Stroudsburg, PA, USA.
Association for Compu-tational Linguistics.232
