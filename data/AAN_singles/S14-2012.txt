Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 97?103,Dublin, Ireland, August 23-24, 2014.Alpage: Transition-based Semantic Graph Parsing with SyntacticFeaturesCorentin Ribeyre?
?Eric Villemonte de la Clergerie?Djam?
Seddah?
?Alpage, INRIA?Univ Paris Diderot, Sorbonne Paris Cit?Universit?
Paris Sorbonnefirstname.lastname@inria.frAbstractThis paper describes the systems deployedby the ALPAGE team to participate to theSemEval-2014 Task on Broad-CoverageSemantic Dependency Parsing.
We de-veloped two transition-based dependencyparsers with extended sets of actions tohandle non-planar acyclic graphs.
For theopen track, we worked over two orthog-onal axes ?
lexical and syntactic ?
in or-der to provide our models with lexical andsyntactic features such as word clusters,lemmas and tree fragments of differenttypes.1 IntroductionIn recent years, we have seen the emergenceof semantic parsing, relying on various tech-niques ranging from graph grammars (Chiang etal., 2013) to transitions-based dependency parsers(Sagae and Tsujii, 2008).
Assuming that obtain-ing predicate argument structures is a necessarygoal to move from syntax to accurate surface se-mantics, the question of the representation of suchstructures arises.
Regardless of the annotationscheme that should be used, one of the main is-sues of semantic representation is the constructionof graph structures, that are inherently harder togenerate than the classical tree structures.In that aspect, the shared task?s proposal (Oepenet al., 2014), to evaluate different syntactic-semantic schemes (Ivanova et al., 2012; Hajic etal., 2006; Miyao and Tsujii, 2004) could not ar-rive at a more timely moment when state-of-the-artsurface syntactic parsers regularly reach, or cross,a 90% labeled dependency recovery plateau for aThis work is licensed under a Creative Commons At-tribution 4.0 International Licence.
Page numbers and pro-ceedings footer are added by the organisers.
Licence details:http://creativecommons.org/licenses/by/4.0/.wide range of languages (Nivre et al., 2007a; Sed-dah et al., 2013).The two systems we present both extendtransition-based parsers in order to be able to gen-erate acyclic dependency graphs.
The first onefollows the standard greedy search mechanism of(Nivre et al., 2007b), while the second one fol-lows a slightly more global search strategy (Huangand Sagae, 2010; Goldberg et al., 2013) by rely-ing on dynamic programming techniques.
In addi-tion to building graphs directly, the main original-ity of our work lies in the use of different kinds ofsyntactic features, showing that using syntax forpure deep semantic parsing improves global per-formance by more than two points.Although not state-of-the-art, our systems per-form very honorably compared with other singlesystems in this shared task and pave quite an in-teresting way for further work.
In the remainderof this paper, we present the parsers and their ex-tensions for building graphs; we then present oursyntactic features and discuss our results.2 Systems DescriptionShift-reduce transition-based parsers essentiallyrely on configurations formed of a stack and abuffer, with stack transitions used to go from aconfiguration to the next one, until reaching a fi-nal configuration.
Following K?bler et al.
(2009),we define a configuration by c = (?, ?,A) where?
denotes a stack of words wi, ?
a buffer ofwords, and A a set of dependency arcs of the form(wi, r, wj), with withe head, wjthe dependent,and r a label in some set R.However, despite their overall similarities,transition-based systems may differ on many as-pects, such as the exact definition of the configura-tions, the set of transitions extracted from the con-figurations, the way the search space is explored(at parsing and training time), the set of features,the way the transition weights are learned and ap-97(?,wi|?,A) ` (?|wi, ?, A) (shift) BOTH(?|wj|wi, ?, A) ` (?|wi, ?, A ?
(wi, r, wj)) (left-reduce) S&T PARSER(?|wj|wi, ?, A) ` (?|wj, ?, A ?
(wj, r, wi)) (right-reduce) S&T PARSER(?|wj|wi, ?, A) ` (?|wj|wi, ?, A ?
(wi, r, wj)) (left-attach) BOTH(?|wj|wi, ?, A) ` (?|wj, wi|?,A ?
(wj, r, wi) (right-attach) BOTH(?|wi, ?, A) ` (?, ?,A) (pop0) BOTH(?|wj|wi, ?, A) ` (?|wi, ?, A) (pop1) DYALOG-SR(?|wj|wi, ?, A) ` (?|wi|wj, ?, A) (swap) DYALOG-SRFigure 1: An extended set of transitions for building dependency graphs.plied, etc.For various reasons, we started our experimentswith two rather different transition-based parsers,which have finally converged on several aspects.In particular, the main convergence concerns theset of transitions needed to parse the three pro-posed annotation schemes.
To be able to attachzero, one, or more heads to a word, it is necessaryto clearly dissociate the addition of a dependencyfrom the reduction of a word (i.e.
its removal fromthe stack).
Following Sagae and Tsujii (2008), asshown in Figure 1, beside the usual shift and re-duce transitions of the arc-standard strategy, weintroduced the new left and right attach actions foradding new dependencies (while keeping the de-pendent on the stack) and two reduce pop0 andpop1 actions to remove a word from the stack af-ter attachement of its dependents.
All transitionsadding an edge should also satisfy the conditionthat the new edge does not create a cycle or mul-tiple edges between the same pair of nodes.
It isworth noting that the pop actions may also be usedto remove words with no heads.2.1 Sagae & Tsujii?s DAG ParserOur first parsing system is a partial rewrite, withseveral extensions, of the Sagae and Tsujii (2008)DAG parser (henceforth S&T PARSER).
We mod-ified it to handle dependency graphs, in particu-lar non-governed words using pop0 transitions.This new transition removes the topmost stack el-ement when all its dependents have been attached(through attach or reduce transitions).
Thus, wecan handle partially connected graphs, since aword can be discarded when it has no incomingarc.We used two different learning algorithms:(i) the averaged perceptron because of its goodbalance between training time and performance(Daume, 2006), (ii) the logistic regression model(maximum entropy (Ratnaparkhi, 1997)).
For thelatter, we used the truncated gradient optimiza-tion (Langford et al., 2009), implemented in Clas-sias (Okazaki, 2009), in order to estimate the pa-rameters.
These algorithms have been used inter-changeably to test their performance in terms of F-score.
But the difference was negligeable in gen-eral.2.2 DYALOG-SROur second parsing system is DYALOG-SR(Villemonte De La Clergerie, 2013), which hasbeen developed to participate to the SPMRL?13shared task.
Coded on top of tabular logicprogramming system DYALOG, it implementsa transition-based parser relying on dynamicprogramming techniques, beams, and an aver-aged structured perceptron, following ideas from(Huang and Sagae, 2010; Goldberg et al., 2013).It was initially designed to follow an arc-standard parsing strategy, relying on shift andleft/right reduce transitions.
To deal with depen-dency graphs and non governed words, we firstadded the two attach transitions and the pop0transition.
But because there exist some overlapbetween the reduce and attach transitions leadingto some spurious ambiguities, we finally decidedto remove the left/right reduce transitions and tocomplete with the pop1 transition.
In order tohandle some cases of non-projectivty with mini-mal modifications of the system, we also addeda swap transition.
The parsing strategy is nowcloser to the arc-eager one, with an oracle sug-gesting to attach as soon as possible.2.3 Tree ApproximationsIn order to stack several dependency parsers, weneeded to transform our graphs into trees.
We re-port here the algorithms we used.The first one uses a simple strategy.
For nodeswith multiple incoming edges, we keep the longestincoming edge.
Singleton nodes (with no head)are attached with a _void_-labeled edge (bydecreasing priority) to the immediately adjacent98Word?1Lemma?1POS?1leftPOS?1rightPOS?1leftLabel?1rightLabel?1Word?2Lemma?2POS?2leftPOS?2rightPOS?2leftLabel?2rightLabel?2Word?3POS?3Word?1Lemma?1POS?1Word?2Lemma?2POS?2POS?3a d12d?11Table 1: Baseline features for S&T PARSER.node N , or the virtual root node (token 0).
Thisstrategy already improves over the baseline, pro-vided by the task organisers, on the PCEDT by 5points.The second algorithm tries to preserve moreedges: when it is possible, the deletion of a re-entrant edge is replaced by reversing its directionand changing its label l into <l.
We do this fornodes with no incoming edges by reversing thelongest edge only if this action does not create cy-cles.
The number of labels increases, but manymore edges are kept, leading to better results onDM and PAS corpora.3 Feature Engineering3.1 Closed TrackFor S&T PARSER we define Word?i(resp.Lemma?iand POS?i) as the word (resp.
lemmaand part-of-speech) at position i in the queue.
Thesame goes for ?i, which is the position i in thestack.
Let di,jbe the distance between Word?iand Word?j.
We also define d?i,j, the distance be-tween Word?iand Word?j.
In addition, we defineleftPOS?i(resp.
leftLabel?i) the part-of-speech(resp.
the label if any) of the word immediatelyat the left handside of ?i, and the same goes forrightPOS?i(resp.
rightLabel?i).
Finally, a is theprevious predicted action by the parser.
Table 1reports our baseline features.For DYALOG-SR we have the following lexi-cal features lex, lemma, cat, and morphosyn-tactic mstag.
They apply to next unread word(*I, say lemmaI), the three next lookaheadwords (*I2 to*I4), and (when present) to the3 stack elements (*0 to*2), their two leftmostand rightmost children (before b[01]*[012]and after a[01]*[012]).
We have dependencyfeatures such as the labels of the two leftmostand rightmost edges ([ab][01]label[012]),the left and right valency (number of depen-dency, [ab]v[012]) and domains (set of de-pendency labels, [ab]d[012]).
Finally, wehave 3 (discretized) distance features between thenext word and the stack elements (delta[01])and between the two topmost stack elements(delta01).
Most feature values are atomic (ei-ther numerical or symbolic), but they can also be(recursively) a list of values, for instance for themstag and domain features.
For dealing withgraphs, features were added about the incomingedges to the 3 topmost stack elements, similar tovalency (ngov[012]) and domain (gov[012]).For the PCEDT scheme, because of the high num-ber of dependency labels, the 30 most unfrequentones were replaced by a generic label when usedas feature value.Besides, for the PCEDT and DM corpora, staticand dynamic guiding features have been triedfor DYALOG-SR, provided by MATE (Bohnet,2010) (trained on versions of these corpora pro-jected to trees, using a 10-fold cross valida-tion).
The two static features mate_label andmate_distance are attached to each token h,indicating the label and the relative distance to itsgovernor d (if any).
At runtime, dynamic featuresare also added relative to the current configuration:if a semantic dependency (h, l, d) has been pre-dicted by MATE, and the topmost 2 stack elementsare either (h, d) or (d, h), a feature suggesting aleft or right attachment for l is added.We did the same for S&T PARSER, except thatwe used a simple but efficient hack: instead ofkeeping the labels predicted by our parser, we re-placed them by MATE predictions whenever it waspossible.3.2 Open TrackFor this track, we combined the previously de-scribed features (but the MATE-related ones) withvarious lexical and syntactic features, our intu-ition being that syntax and semantic are inter-dependent, and that syntactic features shouldtherefore help semantic parsing.
In particular, wehave considered the following bits of information.Unsupervized Brown clusters To reduce lexi-cal sparsity, we extracted 1,000 clusters from theBNC (Leech, 1992) preprocessed following Wag-ner et al.
(2007).
We extended them with capi-talization, digit features and 3 letters suffix signa-tures, leading to a vocabulary size reduced by half.Constituent tree fragments They were part ofthe companion data provided by the organizers.99They consist of fragments of the syntactic treesand can be used either as enhanced parts of speechor as features.Spinal elementary trees A full set of parses wasreconstructed from the tree fragments.
Then weextracted a spine grammar (Seddah, 2010), us-ing the head percolation table of the Bikel (2002)parser, slightly modified to avoid determiners to bemarked as head in some configurations.Predicted MATE dependencies Also providedin the companion data, they consist in the parsesbuilt by the MATE parsers, trained on the Stanforddependency version of the PTB.
We combined thelabels with a distance ?
= t ?
h where t is thetoken number and h the head number.Constituent head paths Inspired by Bj?rkelundet al.
(2013), we used the MATE dependencies toextract the shortest path between a token and itslexical head and included the path length (in termsof traversed nodes) as feature.Tree frag.
MATE labels+?
Spines trees Head PathsTrain 648 1305 637 27,670Dev 272 742 265 3,320Test 273 731 268 2,389Table 2: Syntactic features statistics.4 Results and DiscussionWe present here the results on section 21 (test set)1for both systems.
We report in Table 3, the differ-ent runs we submitted for the final evaluation ofthe shared task.
We also report improvements be-tween the two tracks.Both systems show relatively close F-measures,with correct results on every corpus.
If we com-pare the results more precisely, we observe that ingeneral, DYALOG-SR tends to behave better forthe unlabeled metrics.
Its main weakness is onMRS scheme, for both tracks.21Dev set results are available online athttp://goo.gl/w3XcpW.2The main and still unexplained problem of DYALOG-SR was that using larger beams has no impact, and often anegative one, when using the attach and pop transitions.
Ex-cept for PAS and PCEDT where a beam of size 4 workedbest for the open track, all other results were obtained forbeams of size 1.
This situation is in total contradiction withthe large impact of beam previously observed for the arc stan-dard strategy during the SPMRL?13 shared task and duringexperiments led on the French TreeBank (Abeill?
et al., 2003)(FTB).
Late experiments on the FTB using the attach andpop actions (but delaying attachments as long as possible) hasOn the other hand, it is worth noting that syn-tactic features greatly improve semantic parsing.In fact, we report in Figure 2(a) the improvementof the five most frequent labels and, in Figure 2(b),the five best improved labels with a frequency over0.5% in the training set, which represent 95% ofthe edges in the DM Corpus.
As we can see, syn-tactic information allow the systems to performbetter on coordination structures and to reduce am-biguity between modifiers and verbal arguments(such as the ARG3 label).We observed the same behaviour on the PAScorpus, which contains also predicate-argumentstructures.
For PCEDT, the results show that syn-tactic features give only small improvements, butthe corpus is harder because of a large set of labelsand is closer to syntactic structures than the twoothers.Of course, we only scratched the surface withour experiments and we plan to further investigatethe impact of syntactic information during seman-tic parsing.
We especially plan to explore the deepparsing of French, thanks to the recent release ofthe Deep Sequoia Treebank (Candito et al., 2014).5 ConclusionIn this paper, we presented our results on the task8 of the SemEval-2014 Task on Broad-CoverageSemantic Dependency Parsing.
Even though theresults do not reach state-of-the-art, they comparefavorably with other single systems and show thatsyntactic features can be efficiently used for se-mantic parsing.In future work, we will continue to investigatethis idea, by combining with more complex sys-tems and more efficient machine learning tech-niques, we are convinced that we can come closerto state of the art results.
and that syntax is the keyfor better semantic parsing.AcknowledgmentsWe warmly thank Kenji Sagae for making hisparser?s code available and kindly answering ourquestions.ReferencesAnne Abeill?, Lionel Cl?ment, and Fran?ois Toussenel.2003.
Building a Treebank for French.
In Treebanksconfirmed a problem with beams, even if less visible.
We arestill investigating why the use of the attach transitions and/orof the pop transitions seems to be incompatible with beams.100Closed trackPCEDT LF UFPEKING - BEST 76.28 89.19S&T PARSER b5 67.83 80.86DYALOG-SR b1 67.81 81.23DM (MRS)PEKING - BEST 89.40 90.82S&T PARSER b5 78.44 80.88DYALOG-SR b1 78.32 81.85PAS (ENJU)PEKING - BEST 92.04 93.13S&T PARSER b5 82.44 84.41DYALOG-SR b1 84.16 86.09Open trackPCEDT LF UFPRIBERAM - BEST 77.90 89.03S&T PARSER b5 69.20 +1.37 82.68 +1.86DYALOG-SR b4 69.58 +1.77 84.80 +3.77DM (MRS)PRIBERAM - BEST 89.16 90.32S&T PARSER b5 81.46 +3.02 83.68 +2.80DYALOG-SR b1 79.71 +1.39 81.97 +0.12PAS (ENJU)PRIBERAM - BEST 91.76 92.81S&T PARSER b5 84.97 +2.53 86.64 +2.23DYALOG-SR b4 85.58 +1.42 86.98 +0.87Table 3: Results on section 21 (test) of the PTB for closed and open track.60 70 80 90 100ARG1ARG2compoundBVpossF-score S&T PARSER (%)With SyntaxNo Syntax60 70 80 90 100ARG1ARG2compoundBVposs40.2%24.5%11.7%11.0%2.4%F-score DYALOG-SR (%)(a) the 5 most frequent labels20 40 60 80 100conj-and-capposlocARG3F-score S&T PARSER (%)With SyntaxNo Syntax20 40 60 80 100conj-and-capposlocARG30.6%2.1%0.8%1.5%1.3%F-score DYALOG-SR (%)(b) the 5 best improved labels (edges frequency above 0.5 % in the training set)Figure 2: Improvement with syntactic features for DM (test) corpus.
(numbers indicate edge frequency in training set)101: Building and Using Parsed Corpora, pages 165?188.
Springer.Daniel M. Bikel.
2002.
Design of a multi-lingual,parallel-processing statistical parsing engine.
InProceedings of the second international conferenceon Human Language Technology Research, pages178?182.
Morgan Kaufmann Publishers Inc. SanFrancisco, CA, USA.Anders Bj?rkelund, Ozlem Cetinoglu, Rich?rd Farkas,Thomas Mueller, and Wolfgang Seeker.
2013.
(re)ranking meets morphosyntax: State-of-the-artresults from the SPMRL 2013 shared task.
In Pro-ceedings of the Fourth Workshop on Statistical Pars-ing of Morphologically-Rich Languages, pages 135?145, Seattle, Washington, USA, October.Bernd Bohnet.
2010.
Very high accuracy and fast de-pendency parsing is not a contradiction.
In Proceed-ings of the 23rd International Conference on Com-putational Linguistics, COLING ?10, pages 89?97,Stroudsburg, PA, USA.Marie Candito, Guy Perrier, Bruno Guillaume,Corentin Ribeyre, Kar?n Fort, Djam?
Seddah, and?ric De La Clergerie.
2014.
Deep Syntax Anno-tation of the Sequoia French Treebank.
In Interna-tional Conference on Language Resources and Eval-uation (LREC), Reykjavik, Islande, May.David Chiang, Jacob Andreas, Daniel Bauer,Karl Moritz Hermann, Bevan Jones, and KevinKnight.
2013.
Parsing graphs with hyperedgereplacement grammars.
In Proceedings of the 51stMeeting of the ACL.Harold Charles Daume.
2006.
Practical structuredlearning techniques for natural language process-ing.
Ph.D. thesis, University of Southern California.Yoav Goldberg, Kai Zhao, and Liang Huang.
2013.Efficient implementation of beam-search incremen-tal parsers.
In Proceedings of the 51st Annual Meet-ing of the Association for Computational Linguistics(ACL), Sophia, Bulgaria, August.Jan Hajic, Jarmila Panevov?, Eva Hajicov?, PetrSgall, Petr Pajas, Jan ?tep?nek, Ji?r?
Havelka,Marie Mikulov?, Zdenek Zabokrtsk`y, andMagda ?evc?kov?
Raz?mov?.
2006.
Praguedependency treebank 2.0.
CD-ROM, LinguisticData Consortium, LDC Catalog No.
: LDC2006T01,Philadelphia, 98.Liang Huang and Kenji Sagae.
2010.
Dynamic pro-gramming for linear-time incremental parsing.
InProceedings of the 48th Annual Meeting of the Asso-ciation for Computational Linguistics, pages 1077?1086.
Association for Computational Linguistics.Angelina Ivanova, Stephan Oepen, Lilja ?vrelid, andDan Flickinger.
2012. Who did what to whom?
:A contrastive study of syntacto-semantic dependen-cies.
In Proceedings of the sixth linguistic annota-tion workshop, pages 2?11.Sandra K?bler, Ryan McDonald, and Joakim Nivre.2009.
Dependency Parsing.
Morgan and ClaypoolPublishers.John Langford, Lihong Li, and Tong Zhang.
2009.Sparse online learning via truncated gradient.
Jour-nal of Machine Learning Research, 10(777-801):65.Geoffrey Leech.
1992.
100 million words of English:the British National Corpus.
Language Research,28(1):1?13.Yusuke Miyao and Jun?ichi Tsujii.
2004.
DeepLinguistic Analysis for the Accurate Identificationof Predicate-Argument Relations.
In Proceedingsof the 18th International Conference on Compu-tational Linguistics (COLING 2004), pages 1392?1397, Geneva, Switzerland.Joakim Nivre, Johan Hall, Sandra K?bler, Ryan Mc-Donald, Jens Nilsson, Sebastian Riedel, and DenizYuret.
2007a.
The CoNLL 2007 shared task ondependency parsing.
In Proceedings of the CoNLLShared Task Session of EMNLP-CoNLL 2007, pages915?932, Prague, Czech Republic, June.Joakim Nivre, Johan Hall, Jens Nilsson, AtanasChanev, G?ls?en Eryi?git, Sandra K?bler, SvetoslavMarinov, and Erwin Marsi.
2007b.
MaltParser:A language-independent system for data-driven de-pendency parsing.
Natural Language Engineering,13(2):95?135.Stephan Oepen, Marco Kuhlmann, Yusuke Miyao,Daniel Zeman, Dan Flickinger, Jan Haji?c, AngelinaIvanova, and Yi Zhang.
2014.
SemEval 2014 Task8: Broad-coverage semantic dependency parsing.
InProceedings of the 8th International Workshop onSemantic Evaluation, Dublin, Ireland.Naoaki Okazaki.
2009.
Classias: A collection of ma-chine learning algorithms for classification.Adwait Ratnaparkhi.
1997.
A simple introduction tomaximum entropy models for natural language pro-cessing.
IRCS Technical Reports Series, page 81.Kenji Sagae and Jun?ichi Tsujii.
2008.
Shift-reducedependency DAG parsing.
In Proceedings of the22nd International Conference on ComputationalLinguistics (Coling 2008), pages 753?760, Manch-ester, UK, August.
Coling 2008 Organizing Com-mittee.Djam?
Seddah, Reut Tsarfaty, Sandra K?bler, MarieCandito, Jinho D. Choi, Rich?rd Farkas, Jen-nifer Foster, Iakes Goenaga, Koldo Gojenola Gal-letebeitia, Yoav Goldberg, Spence Green, NizarHabash, Marco Kuhlmann, Wolfgang Maier, JoakimNivre, Adam Przepi?rkowski, Ryan Roth, WolfgangSeeker, Yannick Versley, Veronika Vincze, MarcinWoli?nski, Alina Wr?blewska, and ?ric VillemonteDe La Clergerie.
2013.
Overview of the SPMRL2013 shared task: A cross-framework evaluation of102parsing morphologically rich languages.
In Pro-ceedings of the Fourth Workshop on Statistical Pars-ing of Morphologically-Rich Languages, pages 146?182, Seattle, Washington, USA, October.Djam?
Seddah.
2010.
Exploring the spinal-stigmodel for parsing french.
In Proceedings of theSeventh conference on International Language Re-sources and Evaluation (LREC?10), Valletta, Malta,may.
European Language Resources Association(ELRA).
?ric Villemonte De La Clergerie.
2013.
Exploringbeam-based shift-reduce dependency parsing withDyALog: Results from the SPMRL 2013 sharedtask.
In 4th Workshop on Statistical Parsing of Mor-phologically Rich Languages (SPMRL?2013), Seat-tle, ?tats-Unis.Joachim Wagner, Djam?
Seddah, Jennifer Foster, andJosef Van Genabith.
2007.
C-structures and F-structures for the British National Corpus.
In Pro-ceedings of the Twelfth International Lexical Func-tional Grammar Conference.
Citeseer.103
