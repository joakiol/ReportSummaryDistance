Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1013?1024,October 25-29, 2014, Doha, Qatar.c?2014 Association for Computational LinguisticsGreed is Good if Randomized: New Inference for Dependency ParsingYuan Zhang?, Tao Lei?, Regina Barzilay, and Tommi JaakkolaComputer Science and Artificial Intelligence LaboratoryMassachusetts Institute of Technology{yuanzh, taolei, regina, tommi}@csail.mit.eduAbstractDependency parsing with high-order fea-tures results in a provably hard decodingproblem.
A lot of work has gone intodeveloping powerful optimization meth-ods for solving these combinatorial prob-lems.
In contrast, we explore, analyze, anddemonstrate that a substantially simplerrandomized greedy inference algorithm al-ready suffices for near optimal parsing: a)we analytically quantify the number of lo-cal optima that the greedy method has toovercome in the context of first-order pars-ing; b) we show that, as a decoding algo-rithm, the greedy method surpasses dualdecomposition in second-order parsing; c)we empirically demonstrate that our ap-proach with up to third-order and globalfeatures outperforms the state-of-the-artdual decomposition and MCMC samplingmethods when evaluated on 14 languagesof non-projective CoNLL datasets.11 IntroductionDependency parsing is typically guided by param-eterized scoring functions that involve rich fea-tures exerting refined control over the choice ofparse trees.
As a consequence, finding the high-est scoring parse tree is a provably hard combina-torial inference problem (McDonald and Pereira,2006).
Much of the recent work on parsing hasfocused on solving these problems using powerfuloptimization techniques.
In this paper, we follow adifferent strategy, arguing that a much simpler in-ference strategy suffices.
In fact, we demonstratethat a randomized greedy method of inference sur-passes the state-of-the-art performance in depen-dency parsing.
?Both authors contributed equally.1Our code is available at https://github.com/taolei87/RBGParser.Our choice of a randomized greedy algorithmfor parsing follows from a successful track recordof such methods in other hard combinatorial prob-lems.
These conceptually simple and intuitivealgorithms have delivered competitive approxi-mations across a broad class of NP-hard prob-lems ranging from set cover (Hochbaum, 1982) toMAX-SAT (Resende et al., 1997).
Their successis predicated on the observation that most realiza-tions of problems are much easier to solve than theworst-cases.
A simpler algorithm will thereforesuffice in typical cases.
Evidence is accumulatingthat parsing problems may exhibit similar proper-ties.
For instance, methods such as dual decom-position offer certificates of optimality when thehighest scoring tree is found.
Across languages,dual decomposition has shown to lead to a cer-tificate of optimality for the vast majority of thesentences (Koo et al., 2010; Martins et al., 2011).These remarkable results suggest that, as a com-binatorial problem, parsing appears simpler thanits broader complexity class would suggest.
In-deed, we show that a simpler inference algorithmalready suffices for superior results.In this paper, we introduce a randomized greedyalgorithm that can be easily used with any richscoring function.
Starting with an initial treedrawn uniformly at random, the algorithm makesonly local myopic changes to the parse tree in anattempt to climb the objective function.
While asingle run of the hill-climbing algorithm may in-deed get stuck in a locally optimal solution, mul-tiple random restarts can help to overcome thisproblem.
The same algorithm is used both forlearning the parameters of the scoring function aswell as for parsing test sentences.The success of a randomized greedy algorithmis tied to the number of local maxima in the searchspace.
When the number is small, only a fewrestarts will suffice for the greedy algorithm tofind the highest scoring parse.
We provide an al-1013gorithm for explicitly counting the number of lo-cal optima in the context of first-order parsing,and demonstrate that the number is typically quitesmall.
Indeed, we find that a first-order parsertrained with exact inference or using our random-ized greedy algorithm delivers basically the sameperformance.We hypothesize that parsing with high-orderscoring functions exhibits similar properties.
Themain rationale is that, even in the presence of high-order features, the resulting scoring function re-mains first-order dominant.
The performance ofa simple arc-factored first-order parser is only afew percentage points behind higher-order parsers.The higher-order features in the scoring functionoffer additional refinement but only a few changesabove and beyond the first-order result.
As aconsequence, most of the arc choices are alreadydetermined by a much simpler, polynomial timeparser.We use dual decomposition to show that thegreedy method indeed succeeds as an inference al-gorithm even with higher-order scoring functions.In fact, with second-order features, regardless ofwhich method was used for training, the random-ized greedy method outperforms dual decomposi-tion by finding higher scoring trees.
For the sen-tences that dual decomposition is optimal (obtainsa certificate), the greedy method finds the samesolution in over 99% of the cases.
Our simpleinference algorithm is therefore likely to scale tohigher-order parsing and we demonstrate empiri-cally that this is indeed so.We validate our claim by evaluating the methodon the CoNLL dependency benchmark that com-prises treebanks from 14 languages.
Aver-aged across all languages, our method out-performs state-of-the-art parsers, including Tur-boParser (Martins et al., 2013) and our earliersampling-based parser (Zhang et al., 2014).
Onseven languages, we report the best published re-sults.
The method is not sensitive to initialization.In fact, drawing the initial tree uniformly at ran-dom results in the same performance as when ini-tialized from a trained first-order distribution.
Incontrast, sufficient randomization of the startingpoint is critical.
Only a small number of restartssuffices for finding (near) optimal parse trees.2 Related WorkFinding Optimal Structure in Parsing The useof rich-scoring functions in dependency parsinginevitably leads to the challenging combinatorialproblem of finding the maximizing parse.
In fact,McDonald and Pereira (2006) demonstrated thatthe task is provably NP-hard for non-projectivesecond-order parsing.
Not surprisingly, approx-imate inference has been at the center of pars-ing research.
Examples of these approaches in-clude easy-first parsing (Goldberg and Elhadad,2010), inexact search (Johansson and Nugues,2007; Zhang and Clark, 2008; Huang et al., 2012;Zhang et al., 2013), partial dynamic program-ming (Huang and Sagae, 2010) and dual decom-position (Koo et al., 2010; Martins et al., 2011).Our work is most closely related to the MCMCsampling-based approaches (Nakagawa, 2007;Zhang et al., 2014).
In our earlier work, we devel-oped a method that learns to take guided stochas-tic steps towards a high-scoring parse (Zhang etal., 2014).
In the heart of that technique are so-phisticated samplers for traversing the space oftrees.
In this paper, we demonstrate that a sub-stantially simpler approach that starts from a treedrawn from the uniform distribution and uses hill-climbing for parameter updates achieves similar orhigher performance.Another related greedy inference method hasbeen used for non-projective dependency pars-ing (McDonald and Pereira, 2006).
This methodrelies on hill-climbing to convert the highest scor-ing projective tree into its non-projective approxi-mation.
Our experiments demonstrate that whenhill-climbing is employed as a primary learningmechanism for high-order parsing, it exhibits dif-ferent properties: the distribution for initializationdoes not play a major role in the final outcome,while the use of restarts contributes significantlyto the quality of the resulting tree.Greedy Approximations for NP-hard ProblemsThere is an expansive body of research on greedyapproximations for NP-hard problems.
Examplesof NP-hard problems with successful greedy ap-proximations include the traveling saleman prob-lem problem (Held and Karp, 1970; Rego etal., 2011), the MAX-SAT problem (Mitchell etal., 1992; Resende et al., 1997) and vertexcover (Hochbaum, 1982).
While some greedymethods have poor worst-case complexity, many1014of them work remarkably well in practice.
Despitethe apparent simplicity of these algorithms, un-derstanding their properties is challenging: oftentheir ?theoretical analyses are negative and incon-clusive?
(Amenta and Ziegler, 1999; Spielman andTeng, 2001).
Identifying conditions under whichapproximations are provably optimal is an activearea of research in computer science theory (Du-mitrescu and T?oth, 2013; Jonsson et al., 2013).In NLP, randomized and greedy approximationshave been successfully used across multiple ap-plications, including machine translation and lan-guage modeling (Brown et al., 1993; Ravi andKnight, 2010; Daum?e III et al., 2009; Moore andQuirk, 2008; Deoras et al., 2011).
In this paper,we study the properties of these approximations inthe context of dependency parsing.3 Method3.1 PreliminariesLet x be a sentence and T (x) be the set of possi-ble dependency trees over the words in x.
We usey ?
T (x) to denote a dependency tree for x, andy(m) to specify the head (parent) of the modifierword indexed by m in tree y.
We also use m todenote the indexed word when there is no ambi-guity.
In addition, we define T (y,m) as the setof ?neighboring trees?
of y obtained by changingonly the head of the modifier, i.e.
y(m).The dependency trees are scored according toS(x, y) = ?
?
?
(x, y), where ?
is a vector of pa-rameters and ?
(x, y) is a sparse feature vector rep-resentation of tree y for sentence x.
In this work,?
(x, y) will include up to third-order features aswell as a range of global features commonly usedin re-ranking methods (Collins, 2000; Charniakand Johnson, 2005; Huang, 2008).The parameters ?
in the scoring function areestimated on the basis of a training set D ={(x?i, y?i)}Ni=1of sentences x?iand the correspond-ing gold (target) trees y?i.
We adopt a max-marginframework for this learning problem.
Specifically,we aim to find parameter values that score the goldtarget trees higher than others:?i ?
{1, ?
?
?
, N}, y ?
T (x?i),S(x?i, y?i) ?
S(x?i, y) + ?y?i?
y?1?
?iwhere ?i?
0 is the slack variable (non-zero valuesare penalized against) and ?y?i?
y?1is the ham-ming distance between the gold tree y?iand a can-didate parse y.In an online learning setup, parameters are up-dated successively after each sentence.
Each up-date still requires us to find the ?strongest viola-tion?, i.e., a candidate tree y?
that scores higherthan the gold tree y?i:y?
= argmaxy?T (x?i){S(x?i, y) + ?y ?
y?i?1}The parameters are then revised so as to selectagainst the offending y?.
Instead of a standardparameter update based on y?
as in perceptron,stochastic gradient descent, or passive-aggressiveupdates, our implementation follows Lei et al.
(2014) where the first-order parameters are brokenup into a tensor.
Each tensor component is updatedsuccessively in combination with the parameterscorresponding to MST features (McDonald et al.,2005) and higher-order features (when included).23.2 AlgorithmDuring training and testing, the key combinatorialproblem we must solve is that of decoding, i.e.,finding the highest scoring tree y?
?
T (x) for eachsentence x (or x?i).
In our notation,y?
= argmaxy?T (x?i){?
?
?
(x?i, y) + ?y ?
y?i?1} (train)y?
= argmaxy?T (x){?
?
?
(x, y)} (test)While the decoding problem with feature sets sim-ilar to ours has been shown to be NP-hard, manyapproximation algorithms work remarkably well.We commence with a motivating example.Locality and Parsing One possible reason forwhy greedy or other approximation algorithmswork well for dependency parsing is that typicalsentences and therefore the learned scoring func-tions S(x, y) = ?
?
?
(x, y) are primarily ?lo-cal?.
By this we mean that head-modifier deci-sions could be made largely without consideringthe surrounding structure (the context).
For exam-ple, in English an adjective and a determiner aretypically attached to the following noun.We demonstrate the degree of locality in de-pendency parsing by comparing a first-order tree-based parser to the parser that predicts each headword independently of others.
Note that the in-dependent prediction of dependency arcs does notnecessarily give rise to a tree.
The parameters of2We refer the readers to Lei et al.
(2014) for more detailsabout the tensor scoring function and the online update.1015Dataset Indp.
Pred Tree PredSlovene 83.7 84.2Arabic 79.0 79.2Japanese 93.4 93.7English 91.6 91.9Average 86.9 87.3Table 1: Head attachment accuracy of a first-orderlocal classifier (left) and a first-order structuralprediction model (right).
The two types of mod-els are trained using the same set of features.Input: parameter ?, sentence xOutput: dependency tree y?1: Randomly initialize tree y(0);2: t = 0;3: repeat4: list = bottom-up node list of y(t);5: for each word m in list do6: y(t+1)= argmaxy?T (y(t),m)S(x, y);7: t = t+ 1;8: end for9: until no change in this iteration10: return y?
= y(t);Figure 1: A randomized hill-climbing algorithmfor dependency parsing.the two parsers, the independent prediction anda tree-based parser, are trained separately withthe corresponding decoding algorithm but with thesame feature set.Table 1 shows that the accuracy of the inde-pendent prediction ranges from 79% to 93% onfour CoNLL datasets.
The results are on par withthe first-order structured prediction model.
Thisexperiment reinforces the conclusion in Liang etal.
(2008), where a local classifier was shownto achieve comparable accuracy to a sequentialmodel (e.g.
CRF) in POS tagging and named-entity recognition.Hill-Climbing with Random Restarts Webuild here on the motivating example and exploregreedy algorithms as generalizations of purely lo-cal decoding.
Greedy algorithms break the decod-ing problem into a sequence of simple local steps,each required to improve the solution.
In our case,simple local steps correspond to choosing the headfor each modifier word.We begin with a tree y(0), which can be a sam-ple drawn uniformly from T (x) (Wilson, 1996).Our greedy algorithm then updates y(t)to a bet-ter tree y(t+1)by revising the head of one modifierword while maintaining the constraint that the re-sulting structure is a tree.
The modifiers are con-sidered in the bottom-up order relative to the cur-rent tree (the word furthest from the root is consid-ered first).
We provide an analysis to motivate thisbottom-up update strategy in Section 4.1.
The al-gorithm continues until the score can no longer beimproved by changing the head of a single word.The resulting tree represents a locally optimal pre-diction relative to a single-arc greedy algorithm.Figure 1 gives the algorithm in pseudo-code.There are many possible variations of the sim-ple randomized greedy hill-climbing algorithm.First, the Wilson sampling algorithm (Wilson,1996) can be naturally extended to obtain i.i.d.samples from any first-order distributions.
There-fore, we could initialize the tree y(0)with a treefrom a first-order parser, or draw the initial treefrom a first-order distribution other than uniform.However, perhaps surprisingly, as we demon-strate later, little is lost with uniform initializa-tion.
Second, since a single run of randomizedhill-climbing is relatively cheap and runs are in-dependent to each other, it is easy to execute mul-tiple runs independently in parallel.
The final pre-dicted tree is then simply the highest scoring treeacross the multiple runs.
We demonstrate that onlya small number of parallel runs are necessary fornear optimal prediction.4 Analysis4.1 First-Order ParsingWe provide here a firmer basis for why the ran-domized greedy algorithm can be expected towork.
While the focus of the rest of the paperis on higher-order parsing, we limit ourselves inthis subsection to first-order parsing.
The reasonsfor this are threefold.
First, a simple greedy algo-rithm is already not guaranteed a priori to work inthe context of a first-order scoring function.
Theconclusions from this analysis are therefore likelyto carry over to higher-order parsing scenarios aswell.
Second, a first-order arc-factored scoringprovides us an easy way to ascertain when the ran-domized greedy algorithm indeed found the high-est scoring tree.
Finally, we are able to count the1016Dataset Average Len.# of local optima at percentile fraction of finding global optima (%)50% 70% 90% 0 <Len.?
15 Len.> 15Turkish 12.1 1 1 2 100 100Slovene 15.9 2 20 3647 100 98.1English 24.0 21 121 2443 100 99.3Arabic 36.8 2 35 >10000 100 99.1Table 2: The left part of the table shows the local optimum statistics of the first-order model.
Thesentences are sorted by the number of local optima.
Columns 3 to 5 show the number of local optima ofa sentence at different percentile of the sorted list.
For example, on English 50% of the sentences haveno more than 21 local optimum trees.
The right part shows the fraction of finding global optima using300 uniform restarts for each sentence.number of locally optimal solutions for a greedyalgorithm in the context of first-order parsing andcan therefore relate this property to the successrates of the algorithm.Reachability We begin by highlighting a basicproperty of trees, namely that single arc changessuffice for transforming any tree to any other treein a small number of steps while maintaining thateach intermediate structure is also a tree.
In thissense, a target tree is reachable from any start-ing point using only single arc changes.
Moreformally, let y be any starting tree and y?the de-sired target.
Let m1,m2, ?
?
?
,mnbe the bottom-up list of words (modifiers) corresponding to treey, where m1is the word furthest from the root.We can simply change each head y(mi) to that ofy?
(mi) in this order i = 1, .
.
.
, n. The bottom-uporder guarantees that no cycle is introduced withrespect to the remaining (yet unmodified) nodes ofy.
The fact that y?is a valid tree implies no cyclewill appear with respect to the already modifiednodes.Note that, according to this property, any treeis reachable from any starting point using only kmodifications, where k is the number of head dif-ferences, i.e.
k = |{m : y(m) 6= y?(m)}|.
Theresult also suggests that it may be helpful to per-form the greedy steps in the bottom-up order, asuggestion that we follow in our implementation.Broadly speaking, we have established thatthe greedy algorithm is not inherently limited byvirtue of its basic steps.
Of course, it is a differ-ent question whether the scoring function supportssuch local changes towards the correct target tree.Locally Optimal Trees While greedy algo-rithms are notoriously prone to getting stuck inlocally optimal solutions, we establish here thatFunction CountOptima(G = ?V,E?
)V = {w0, w1, ?
?
?
, wn} where w0is therootE = {eij?
R} are the arc scoresReturn: the number of local optima1: Let y(0) = ?
and y(i) = argmaxjeji;2: if y is a tree (no cycle) then return 1;3: Find a cycle C ?
V in y;4: count = 0;// contract the cycle5: create a vertex w?
;6: ?j /?
C : e?j= maxk?Cekj;7: for each vertex wi?
C do8: ?j /?
C : ej?= eji;9: V?= V ?
{w?}
\ C;10: E?= E ?
{e?j, ej?| ?j /?
C}11: count += CountOptima(G?= ?V?, E??
);12: end for13: return count;Figure 2: A recursive algorithm for counting lo-cal optima for a sentence with words w1, ?
?
?
, wn(first-order parsing).
The algorithm resembles theChu-Liu-Edmonds algorithm for finding the max-imum directed spanning tree (Chu and Liu, 1965).decoding with learned scoring functions involvesonly a small number of local optima.
In our case,a local optimum corresponds to a tree y where nosingle change of head y(m) results in a higherscoring tree.
Clearly, the highest scoring tree isalso a local optimum in this sense.
If there weremany such local optima, finding the one with thehighest score would be challenging for a greedyalgorithm, even with randomization.We begin with a worst case analysis and estab-1017DatasetTrained with Hill-Climbing (HC) Trained with Dual Decomposition (DD)%Cert (DD) sDD>sHCsDD=sHCsDD<sHC%Cert (DD) sDD>sHCsDD=sHCsDD<sHCTurkish 98.7 0.0 99.8 0.2 98.7 0.0 100.0 0.0Slovene 94.5 0.0 98.7 1.3 92.3 0.2 99.0 0.8English 94.5 0.3 98.7 1.0 94.6 0.5 98.7 0.8Arabic 78.8 3.4 93.9 2.7 75.3 4.7 88.4 6.9Table 3: Decoding quality comparison between hill-climbing (HC) and dual decomposition (DD).
Mod-els are trained either with HC (left) or DD (right).
sHCdenotes the score of the tree retrieved by HCand sDDgives the analogous score for DD.
The columns show the percentage of all test sentences forwhich one method succeeds in finding a higher or the same score.
?Cert?
column gives the percentageof sentences for which DD finds a certificate.lish a tight upper bound on the number of localoptima for a first-order scoring function.Theorem 1 For any first-order scoring functionthat factorizes into the sum of arc scores S(x, y) =?Sarc(y(m),m): (a) the number of locally op-timal trees is at most 2n?1for n words; (b) thisupper bound is tight.3While the number of possible dependency treesis (n + 1)n?1(Cayley?s formula), the number oflocal optima is at most 2n?1.
This is still too manyfor longer sentences, suggesting that, in the worstcase, a randomized greedy algorithm is unlikely tofind the highest scoring tree.
However, the scor-ing functions we learn for dependency parsing areconsiderably easier.Average Case Analysis In contrast to the worst-case analysis above, we will count here the actualnumber of local optima per sentence for a first-order scoring function learned from data with therandomized greedy algorithm.
Figure 2 providespseudo-code for our counting algorithm.
The al-gorithm is derived by tailoring the proof of Theo-rem 1 to each sentence.Table 2 shows the empirical number of locallyoptimal trees estimated by our algorithm across 4different languages.
Decoding with trained scor-ing functions in the average case is clearly sub-stantially easier than the worst case.
For exam-ple, on the English test set more than 70% of thesentences have at most 121 locally optimal trees.Since the average sentence length is 24, the dis-crepancy between the typical number (e.g., 121)and the worst case (224?1) is substantial.
As a re-sult, only a small number of restarts is likely tosuffice for finding optimal trees in practice.Optimal Decoding We can easily verifywhether the randomized greedy algorithm indeed3A proof sketch is given in Appendix.succeeds in finding the highest scoring trees witha learned first-order scoring function.
We haveestablished above that there are typically only asmall number of locally optimal trees.
We wouldtherefore expect the algorithm to work.
We showthe results in the second part of Table 2.
For shortsentences of length up to 15, our method finds theglobal optimum for all the test sentences.
Successrates remain high even for longer test sentences.4.2 Higher-Order ParsingExact decoding with high-order features is knownto be provably hard (McDonald et al., 2005).
Webegin our analysis here with a second-order (sib-ling/grandparent) model, and compare our ran-domized hill-climbing (HC) method to dual de-composition (DD), re-implementing Koo et al.(2010).
Table 3 compares decoding quality for thetwo methods across four languages.
Overall, in97.8% of the sentences, HC obtains the same scoreas DD, in 1.3% of the cases HC finds a higherscoring tree, and in 0.9% of cases DD results ina better tree.
The results follow the same patternregardless of which method was used to train thescoring function.
The average rate of certificatesfor DD was 92%.
In over 99% of these sentences,HC reaches the same optimum.We expect that these observations about the suc-cess of HC carry over to other high-order parsingmodels for several reasons.
First, a large num-ber of arcs are pruned in the initial stage, con-siderably reducing the search space and minimiz-ing the number of possible locally optimal trees.Second, many dependencies can be determinedalready with independent arc prediction (see ourmotivating example above), predictions that arereadily achieved with a greedy algorithm.
Finally,high-order features represent smaller refinements,i.e., suggest only a few changes above and be-yond the dominant first-order scores.
Greedy al-1018gorithms are therefore likely to be able to leverageat least some of this potential.
We demonstrate be-low that this is indeed so.Our methods are trained within the max-marginframework.
As a result, we are expected to findthe highest scoring competing tree for each train-ing sentence (the ?strongest violation?).
One mayquestion therefore whether possible sub-optimaldecoding for some training sentences (finding ?aviolation?
rather than the ?strongest violation?
)impacts the learned parser.
To this end, Huang etal.
(2012) have established that weaker violationsdo suffice for separable training sets.5 Experimental SetupDataset and Evaluation Measures We evalu-ate our model on CoNLL dependency treebanksfor 14 different languages (Buchholz and Marsi,2006; Surdeanu et al., 2008), using standard train-ing and testing splits.
We use part-of-speech tagsand the morphological information provided in thecorpus.
Following standard practice, we use Unla-beled Attachment Score (UAS) excluding punctu-ation (Koo et al., 2010; Martins et al., 2013) as theevaluation metric in all our experiments.Baselines We compare our model with the Tur-boParser (Martins et al., 2013) and our earliersampling-based parser (Zhang et al., 2014).
Forboth parsers, we directly compare with the re-cent published results on the CoNLL datasets.We also compare our parser against the best pub-lished results for the individual languages in ourdatasets.
This comparison set includes four ad-ditional parsers: Martins et al.
(2011), Koo et al.
(2010), Zhang et al.
(2013) and our tensor-basedparser (Lei et al., 2014).Features We use the same feature templates asin our prior work (Zhang et al., 2014; Lei et al.,2014)4.
Figure 3 shows the first- to third-orderfeature templates that we use in our model.
Forthe global features we use right-branching, coor-dination, PP attachment, span length, neighbors,valency and non-projective arcs features.Implementation Details Following standardpractices, we train our model using the passive-aggressive online learning algorithm (MIRA)and parameter averaging (Crammer et al., 2006;4We refer the readers to Zhang et al.
(2014) and Lei et al.
(2014) for the detailed definition of each feature template.arc!head bigram!
!h h m m+1h m consecutive sibling!h m s grandparent!g h mgrand-sibling!g h m stri-siblings!h m s t grand-grandparent!g h mggouter-sibling-grandchild!h m sgc h s gcminner-sibling-grandchild!Figure 3: First- to third-order features.Arabic Slovene English Chinese German?2?1012345 Len ?
15Len > 15Figure 4: Absolute UAS improvement of our fullmodel over the first-order model.
Sentences in thetest set are divided into 2 groups based on theirlengths.Collins, 2002).
By default we use an adaptivestrategy for running the hill-climbing algorithm?
for a given sentence we repeatedly run the al-gorithm in parallel5until the best tree does notchange for K = 300 consecutive restarts.
Foreach restart, by default we initialize the tree y(0)by sampling from the first-order distribution us-ing the current learned parameter values (and first-order scores).
We train our first-order and third-order model for 10 epochs and our full model for20 epochs for all languages, and report the averageperformance across three independent runs.6 ResultsComparison with the Baselines Table 4 sum-marizes the results of our model, along with thestate-of-the-art baselines.
On average across 14languages, our full model with the tensor com-ponent outperforms both TurboParser and thesampling-based parser.
The direct comparison5We use 8 threads in all the experiments.1019Our ModelExact 1stTurbo SamplingBest Published1st 3rd Fullw/o tensorFull (MA13) (ZL14)Arabic 78.98 79.95 79.38 80.24 79.22 79.64 80.12 81.12 (MS11)Bulgarian 92.15 93.38 93.69 93.72 92.24 93.10 93.30 94.02 (ZH13)Chinese 91.20 93.00 92.76 93.04 91.17 89.98 92.63 92.68 (LX14)Czech 87.65 90.11 90.34 90.77 87.82 90.32 91.04 91.04 (ZL14)Danish 90.50 91.43 91.66 91.86 90.56 91.48 91.80 92.00 (ZH13)Dutch 84.49 86.43 87.04 87.39 84.79 86.19 86.47 86.47 (ZL14)English 91.85 93.01 93.20 93.25 91.94 93.22 92.94 93.22 (MA13)German 90.52 91.91 92.64 92.67 90.54 92.41 92.07 92.41 (MA13)Japanese 93.78 93.80 93.35 93.56 93.74 93.52 93.42 93.74 (LX14)Portuguese 91.12 92.07 92.60 92.36 91.16 92.69 92.41 93.03 (KR10)Slovene 84.29 86.48 87.06 86.72 84.15 86.01 86.82 86.95 (MS11)Spanish 85.52 87.87 88.17 88.75 85.59 85.59 88.24 88.24 (ZL14)Swedish 89.89 91.17 91.35 91.08 89.78 91.14 90.71 91.62 (ZH13)Turkish 76.57 76.80 76.13 76.68 76.40 76.90 77.21 77.55 (KR10)Average 87.75 89.10 89.24 89.44 87.79 88.72 89.23 89.58Table 4: Results of our model and several state-of-the-art systems.
?Best Published UAS?
includes themost accurate parsers among Martins et al.
(2011), Martins et al.
(2013), Koo et al.
(2010), Zhang etal.
(2013), Lei et al.
(2014) and Zhang et al.
(2014).
For the third-order model, we use the feature setof TurboParser (Martins et al., 2013).
The full model combines features of our sampling-based parser(Zhang et al., 2014) and tensor features (Lei et al., 2014).DatasetMAP-1st Uniform Rnd-1stUAS Init.
UAS Init.
UAS Init.Slovene 85.2 80.1 86.7 13.7 86.7 34.2Arabic 78.8 75.1 79.7 12.4 80.2 32.8English 91.1 82.0 93.3 39.6 93.3 55.6Chinese 87.2 75.3 93.2 36.8 93.0 54.5Dutch 84.8 79.5 87.0 26.9 87.4 45.6Average 85.4 78.4 88.0 25.9 88.1 44.5Table 5: Comparison between different initializa-tion strategies: (a) MAP-1st: only the MAP treeof the first-order score; (b) Uniform: random treesare sampled from the uniform distribution; and(c) Rnd-1st: random trees are sampled from thefirst-order distribution.
For each method, the tableshows the average accuracy of the initial tree andthe final parsing accuracy.with TurboParser is achieved by restricting ourmodel to third order features which still outper-forms TurboParser (89.10% vs 88.72%).
To com-pare against the sampling-based parser, we em-ploy our model without the tensor component.
Thetwo models achieve a similar average performance(89.24% and 89.23% respectively).
Since relativeparsing performance depends on a target language,we also include comparison with the best pub-lished results.
The model achieves the best pub-lished results for seven languages.Another noteworthy comparison concerns first-order parsers.
As Table 4 shows, the exact and ap-proximate versions of the first-order parser deliveralmost identical performance.Impact of High-Order Features Table 4 showsthat the model can effectively utilize high-orderfeatures.
Comparing the average performance ofthe model variants, we see that the accuracy onthe benchmark languages consistently improveswhen higher-order features are added.
This char-acteristic of the randomized greedy parser is inline with findings about other state-of-the-art high-order parsers (Martins et al., 2013; Zhang et al.,2014).
Figure 4 breaks down these gains basedon the sentence length.
As expected, on most lan-guages high-order features are particularly helpfulwhen parsing longer sentences.Impact of Initialization and Restarts Table 5shows the impact of initialization on the modelperformance for several languages.
We considerthree strategies: the MAP estimate of the first-order score from the model, uniform sampling andsampling from the first-order distribution.
The ac-curacy of initial trees varies greatly, ranging from78.4% for the MAP estimate to 25.9% and 44.5%for the latter randomized strategies.
However, theresulting parsing accuracy is not determined bythe initial accuracy.
In fact, the two samplingstrategies result in almost identical parsing perfor-mance.
While the first-order MAP estimate givesthe best initial guess, the overall parsing accuracyof this method lags behind.
This result demon-strates the importance of restarts ?
in contrast tothe randomized strategies, the MAP initializationperforms only a single run of hill-climbing.1020Length ?
15 Length > 15Slovene 100 98.11English 100 99.12Table 6: Fractions (%) of the sentences that findthe best solution among 3,000 restarts within thefirst 300 restarts.0 200 400 600 800 10000.9940.9960.9981# RestartsScorelen?15len>15(a) Slovene0 200 400 600 800 10000.9940.9960.9981# RestartsScorelen?15len>15(b) EnglishFigure 5: Convergence analysis on Slovene andEnglish datasets.
The graph shows the normalizedscore of the output tree as a function of the numberof restarts.
The score of each sentence is normal-ized by the highest score obtained for this sentenceafter 3,000 restarts.
We only show the curves up to1,000 restarts because they all reach convergenceafter around 500 restarts.Convergence Properties Figure 5 shows thescore of the trees retrieved by our full model withrespect to the number of restarts, for short and longsentences in English and Slovene.
To facilitate thecomparison, we normalize the score of each sen-tence by the maximal score obtained for this sen-tence after 3,000 restarts.
Overall, most sentencesconverge quickly.
This view is also supported byTable 6 which shows the fraction of the sentencesthat converge within the first 300 restarts.
We cansee that all the short sentences (length up to 15)reach convergence within the allocated restarts.Perhaps surprisingly, more than 98% of the longsentences also converge within 300 restarts.Decoding Speed As the number of restarts im-pacts the parsing accuracy, we can trade perfor-mance for speed.
Figure 6 shows that the model2 4 6 8 10 12 14x 10?382848688Sec/TokUAS3rd?order ModelFull Model(a) Slovene2 4 6 8 10x 10?388909294Sec/TokUAS3rd?order ModelFull Model(b) EnglishFigure 6: Trade-off between performance andspeed on Slovene and English datasets.
The graphshows the accuracy as a function of decodingspeed measured in second per token.
Variations indecoding speed is achieved by changing the num-ber of restarts.achieves high performance with acceptable pars-ing speed.
While various system implementationissues such as programming language and com-putational platform complicate a direct compari-son with other parsing systems, our model deliv-ers parsing time roughly comparable to other state-of-the-art graph-based systems (for example, Tur-boParser and MST parser) and the sampling-basedparser.7 ConclusionsWe have shown that a simple, generally appli-cable randomized greedy algorithm for inferencesuffices to deliver state-of-the-art parsing perfor-mance.
We argued that the effectiveness of suchgreedy algorithms is contingent on having a smallnumber of local optima in the scoring function.
Byalgorithmically counting the number of locally op-timal solutions in the context of first-order parsing,we show that this number is indeed quite small.Moreover, we show that, as a decoding algorithm,the greedy method surpasses dual decompositionin second-order parsing.
Finally, we empiricallydemonstrate that our approach with up to third-order and global features outperforms the state-of-the-art parsers when evaluated on 14 languages of1021non-projective CoNLL datasets.AppendixWe provide here a more detailed justification forthe counting algorithm in Figure 2 and, by exten-sion, a proof sketch of Theorem 1.
The bulletsbelow follow the operation of the algorithm.?
Whenever independent selection of the headsresults in a valid tree, there is only 1 opti-mum (Lines 1&2 of the algorithm).
Other-wise there must be a cycle C in y (Line 3 ofthe algorithm)?
We claim that any locally optimal tree y?ofthe graph G = (V,E) must contain |C| ?
1arcs of the cycle C ?
V .
This can be shownby contradiction.
If y?contains less than|C| ?
1 arcs of C, then (a) we can constructa tree y?
?that contains |C| ?
1 arcs; (b) theheads in y?
?are strictly better than those iny?over the unused part of the cycle; (c) byreachability, there is a path y??
y?
?so y?cannot be a local optimum.?
Any locally optimal tree in G must select anarc inC and reassign it.
The rest of the |C|?1arcs will then result in a chain.?
By contracting cycle C we obtain a newgraph G?of size |G| ?
|C| + 1 (Lines 5-11of the algorithm).
Easy to verify that (notshown): any local optimum in G?is a localoptimum in G and vice versa.The theorem follows as a corollary of thesesteps.
To see this, let F (Gm) be the number oflocal optima in the graph of size m:F (Gm) ?
maxC?V (G)?iF (G(i)m?c+1)where G(i)m?c+1is the graph (of size m ?
c + 1)created by selecting the itharc in cycleC and con-tracting Gmaccordingly, and c = |C| is the sizeof the cycle.
Define?F (m) as the upper bound ofF (Gm) for any graph of size m. By the aboveformula, we know that?F (m) ?
max2?c<m?F (m?
c+ 1)?
cBy solving for?F (m) we get?F (m) ?
2m?2.
Sincem = n+1 for a sentence with n words, the upper-bound of local optima is 2n?1.To show the tightness, for any n > 0, createthe graph Gn+1with arc scores eij= eji= i forany 0 ?
i < j ?
n. Note that wn?
wn?1?wnforms the circle C of size 2, it can be shownby induction on n and F (Gn+1) that F (Gn+1) =F (Gn)?
2 = 2n?1.AcknowledgmentsThis research is developed in collaboration withthe Arabic Language Technologies (ALT) groupat Qatar Computing Research Institute (QCRI)within the IYAS project.
The authors acknowl-edge the support of the U.S. Army Research Of-fice under grant number W911NF-10-1-0533, andof the DARPA BOLT program.
We thank the MITNLP group and the ACL reviewers for their com-ments.
Any opinions, findings, conclusions, orrecommendations expressed in this paper are thoseof the authors, and do not necessarily reflect theviews of the funding organizations.ReferencesNina Amenta and G?unter Ziegler, 1999.
DeformedProducts and Maximal Shadows of Polytopes.
Con-temporary Mathematics.
American Mathematics So-ciety.Peter F. Brown, Vincent J Della Pietra, Stephen A DellaPietra, and Robert L. Mercer.
1993.
The mathemat-ics of statistical machine translation: Parameter esti-mation.
Computational linguistics, 19(2):263?311.Sabine Buchholz and Erwin Marsi.
2006.
CoNLL-Xshared task on multilingual dependency parsing.
InProceedings of the Tenth Conference on Computa-tional Natural Language Learning, CoNLL-X ?06.Association for Computational Linguistics.Eugene Charniak and Mark Johnson.
2005.
Coarse-to-fine n-best parsing and maxent discriminativereranking.
In Proceedings of the 43rd Annual Meet-ing on Association for Computational Linguistics,pages 173?180.
Association for Computational Lin-guistics.Yoeng-Jin Chu and Tseng-Hong Liu.
1965.
On theshortest arborescence of a directed graph.
ScientiaSinica, 14(10):1396.Michael Collins.
2000.
Discriminative reranking fornatural language parsing.
In Proceedings of theSeventeenth International Conference on MachineLearning, ICML ?00, pages 175?182.Michael Collins.
2002.
Discriminative training meth-ods for hidden markov models: Theory and exper-iments with perceptron algorithms.
In Proceedingsof the Conference on Empirical Methods in Natural1022Language Processing - Volume 10, EMNLP ?02.
As-sociation for Computational Linguistics.Koby Crammer, Ofer Dekel, Joseph Keshet, ShaiShalev-Shwartz, and Yoram Singer.
2006.
Onlinepassive-aggressive algorithms.
The Journal of Ma-chine Learning Research.Hal Daum?e III, John Langford, and Daniel Marcu.2009.
Search-based structured prediction.
Machinelearning, 75(3):297?325.Anoop Deoras, Tom?a?s Mikolov, and Kenneth Church.2011.
A fast re-scoring strategy to capture long dis-tance dependencies.
In Proceedings of the Confer-ence on Empirical Methods in Natural LanguageProcessing (EMNLP), pages 1116?1127.
Associa-tion for Computational Linguistics.Adrian Dumitrescu and Csaba D T?oth.
2013.
The trav-eling salesman problem for lines, balls and planes.In SODA, pages 828?843.
SIAM.Yoav Goldberg and Michael Elhadad.
2010.
An effi-cient algorithm for easy-first non-directional depen-dency parsing.
In Human Language Technologies:The 2010 Annual Conference of the North AmericanChapter of the Association for Computational Lin-guistics, pages 742?750.
Association for Computa-tional Linguistics.Michael Held and Richard M. Karp.
1970.
Thetraveling-salesman problem and minimum spanningtrees.
Operations Research, 18(6):1138?1162.Dorit S. Hochbaum.
1982.
Approximation algo-rithms for the set covering and vertex cover prob-lems.
SIAM Journal on Computing, 11(3):555?556.Liang Huang and Kenji Sagae.
2010.
Dynamic pro-gramming for linear-time incremental parsing.
InProceedings of the 48th Annual Meeting of the Asso-ciation for Computational Linguistics, pages 1077?1086.
Association for Computational Linguistics.Liang Huang, Suphan Fayong, and Yang Guo.
2012.Structured perceptron with inexact search.
In Pro-ceedings of the 2012 Conference of the North Amer-ican Chapter of the Association for ComputationalLinguistics: Human Language Technologies, pages142?151.
Association for Computational Linguis-tics.Liang Huang.
2008.
Forest reranking: Discriminativeparsing with non-local features.
In ACL, pages 586?594.Richard Johansson and Pierre Nugues.
2007.
Incre-mental dependency parsing using online learning.
InEMNLP-CoNLL, pages 1134?1138.Peter Jonsson, Victor Lagerkvist, Gustav Nordh, andBruno Zanuttini.
2013.
Complexity of sat problems,clone theory and the exponential time hypothesis.
InSODA, pages 1264?1277.
SIAM.Terry Koo, Alexander M. Rush, Michael Collins,Tommi Jaakkola, and David Sontag.
2010.
Dualdecomposition for parsing with non-projective headautomata.
In Proceedings of the 2010 Conference onEmpirical Methods in Natural Language Process-ing.
Association for Computational Linguistics.Tao Lei, Yu Xin, Yuan Zhang, Regina Barzilay, andTommi Jaakkola.
2014.
Low-rank tensors for scor-ing dependency structures.
In Proceedings of the52th Annual Meeting of the Association for Compu-tational Linguistics.
Association for ComputationalLinguistics.Percy Liang, Hal Daum?e III, and Dan Klein.
2008.Structure compilation: trading structure for features.In Proceedings of the 25th international conferenceon Machine learning, pages 592?599.
ACM.Andr?e F. T. Martins, Noah A. Smith, Pedro M. Q.Aguiar, and M?ario A. T. Figueiredo.
2011.
Dual de-composition with many overlapping components.
InProceedings of the Conference on Empirical Meth-ods in Natural Language Processing, EMNLP ?11.Association for Computational Linguistics.Andr?e F. T. Martins, Miguel B. Almeida, and Noah A.Smith.
2013.
Turning on the turbo: Fast third-ordernon-projective turbo parsers.
In Proceedings of the51th Annual Meeting of the Association for Compu-tational Linguistics.
Association for ComputationalLinguistics.Ryan McDonald and Fernando Pereira.
2006.
Onlinelearning of approximate dependency parsing algo-rithms.
In EACL.Ryan McDonald, Koby Crammer, and FernandoPereira.
2005.
Online large-margin training of de-pendency parsers.
In Proceedings of the 43rd An-nual Meeting of the Association for ComputationalLinguistics (ACL?05).David Mitchell, Bart Selman, and Hector Levesque.1992.
Hard and easy distributions of sat problems.In AAAI, volume 92, pages 459?465.
Citeseer.Robert C. Moore and Chris Quirk.
2008.
Randomrestarts in minimum error rate training for statis-tical machine translation.
In Proceedings of the22nd International Conference on ComputationalLinguistics-Volume 1, pages 585?592.
Associationfor Computational Linguistics.Tetsuji Nakagawa.
2007.
Multilingual dependencyparsing using global features.
In EMNLP-CoNLL,pages 952?956.Sujith Ravi and Kevin Knight.
2010.
Does giza++make search errors?
Computational Linguistics,36(3):295?302.C?esar Rego, Dorabela Gamboa, Fred Glover, and ColinOsterman.
2011.
Traveling salesman problemheuristics: leading methods, implementations andlatest advances.
European Journal of OperationalResearch, 211(3):427?441.1023Mauricio G. C. Resende, L. S. Pitsoulis, and P. M.Pardalos.
1997.
Approximate solution of weightedmax-sat problems using grasp.
Satisfiability prob-lems, 35:393?405.Daniel Spielman and Shang-Hua Teng.
2001.Smoothed analysis of algorithms: Why the simplexalgorithm usually takes polynomial time.
In Pro-ceedings of the thirty-third annual ACM symposiumon Theory of computing, pages 296?305.
ACM.Mihai Surdeanu, Richard Johansson, Adam Meyers,Llu?
?s M`arquez, and Joakim Nivre.
2008.
TheCoNLL-2008 shared task on joint parsing of syn-tactic and semantic dependencies.
In Proceedingsof the Twelfth Conference on Computational Natu-ral Language Learning, CoNLL ?08.
Association forComputational Linguistics.David B. Wilson.
1996.
Generating random spanningtrees more quickly than the cover time.
In Proceed-ings of the twenty-eighth annual ACM symposium onTheory of computing, pages 296?303.
ACM.Yue Zhang and Stephen Clark.
2008.
A tale oftwo parsers: investigating and combining graph-based and transition-based dependency parsing us-ing beam-search.
In Proceedings of the Conferenceon Empirical Methods in Natural Language Pro-cessing, pages 562?571.
Association for Computa-tional Linguistics.Hao Zhang, Liang Zhao, Kai Huang, and Ryan Mc-Donald.
2013.
Online learning for inexact hyper-graph search.
In Proceedings of EMNLP.Yuan Zhang, Tao Lei, Regina Barzilay, TommiJaakkola, and Amir Globerson.
2014.
Steps to ex-cellence: Simple inference with refined scoring ofdependency trees.
In Proceedings of the 52th An-nual Meeting of the Association for ComputationalLinguistics.
Association for Computational Linguis-tics.1024
