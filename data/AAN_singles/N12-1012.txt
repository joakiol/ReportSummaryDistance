2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 112?119,Montre?al, Canada, June 3-8, 2012. c?2012 Association for Computational LinguisticsHello, Who is Calling?
: Can Words Reveal the Social Nature ofConversations?Anthony Stark, Izhak Shafran and Jeffrey KayeCenter for Spoken Language Understanding, OHSU, Portland USA.
{starkan,shafrani,kaye}@ohsu.eduAbstractThis study aims to infer the social natureof conversations from their content automat-ically.
To place this work in context, our moti-vation stems from the need to understand howsocial disengagement affects cognitive declineor depression among older adults.
For thispurpose, we collected a comprehensive andnaturalistic corpus comprising of all the in-coming and outgoing telephone calls from 10subjects over the duration of a year.
As afirst step, we learned a binary classifier to fil-ter out business related conversation, achiev-ing an accuracy of about 85%.
This clas-sification task provides a convenient tool toprobe the nature of telephone conversations.We evaluated the utility of openings and clos-ing in differentiating personal calls, and findthat empirical results on a large corpus donot support the hypotheses by Schegloff andSacks that personal conversations are markedby unique closing structures.
For classifyingdifferent types of social relationships such asfamily vs other, we investigated features re-lated to language use (entropy), hand-crafteddictionary (LIWC) and topics learned usingunsupervised latent Dirichlet models (LDA).Our results show that the posteriors over top-ics from LDA provide consistently higher ac-curacy (60-81%) compared to LIWC or lan-guage use features in distinguishing differenttypes of conversations.1 IntroductionIn recent years, there has been a growing interestin analyzing text in informal interactions such asin Internet chat, newsgroups and twitter.
The em-phasis of most such research has been in estimatingnetwork structure (Kwak et al, 2010) and detectingtrending topics (Ritter et al, 2010), sentiments (Pakand Paroubek, 2010) and first stories (Petrovic?
et al,2010).
The focus has been on aggregating informa-tion from large number of users to analyze popula-tion level statistics.The study reported in this paper, in contrast, fo-cuses on understanding the social interactions of anindividual over long periods of time.
Our motiva-tion stems from the need to understand the factors ofsocial engagement that ameliorate the rate of cogni-tive decline and depression in older adults.
Since theearly work of Glass (1997) and colleagues, severalstudies on large cohorts over extended duration haveconfirmed that older adults with few social relation-ships are at an increased risk of suffering depressionand dementia.
The limited information available incurrently used coarse measures, often based on self-reports, have hindered epidemiologists from probingthe nature of this association further.While social engagement is typically multi-faceted, older adults, who are often less mobile, relyon telephone conversations to maintain their socialrelationships.
This is reflected in a recent surveyby Pew Research Center which reported that amongadults 65 years and older, nine in ten talk with familyor friends every day and more than 95% use land-line telephones for all or most of their calls (Tay-lor et al, June 29 2009).
Conveniently for us, tele-phone conversations present several advantages foranalysis.
Unlike many other forms of communica-tion, the interaction is restricted solely to an audio112channel, without recourse to gestures or facial ex-pressions.
While we do not discount the importanceof multi-modal communication, having a communi-cation channel restricted to a unimodal format doessignificantly simplify both collection and analysis.Furthermore, the use of a handset affords the oppor-tunity to capture naturalistic speech samples at rela-tively high signal-to-noise ratio.
Lastly, automaticspeech recognition (ASR) systems can now tran-scribe telephone conversations with sufficient accu-racy for useful automated analysis.Given the above premise, we focus our attentionon studying social interactions of older adults overland-line telephones.
To facilitate such a study, wecollected telephone conversations from several olderadults for approximately one year.
Note that ourcorpus is unlike the publicly available Switchboardand Fisher corpora, which contain conversations be-tween unfamiliar speakers discussing a topic from apre-determined list such as music, crime, air pollu-tion (Godfrey et al, 1992).
In contrast, the conversa-tions in our corpus are completely natural, coveringa wide range of topics, conversational partners andtypes of interactions.
Our corpus is also compre-hensive in that it includes all the outgoing/incomingcalls from subjects?
homes during the observationperiod.As a step toward understanding social networksand associated relationships, our first task was toclassify social and non-social (business) conversa-tions.
While reverse listing was useful to a certainextent, we were unable to find listing on up to 50%of the calls in our corpus due to lack of caller ID in-formation on many calls as well as unlisted numbers.Moreover, we cannot preclude the possibility that asocial conversation may occur on a business num-ber (e.g., a friend or a relative working in a businessestablishment) and vice versa.
Using the subset ofcalls for which we have reliable listing, we learneda supervised classifier and then employed the classi-fier to label the remaining calls for further analysis.The focus of this study was not so much on learn-ing a binary classifier, but using the resulting classi-fier as a tool to probe the nature of telephone con-versations as well as to test whether the scores ob-tained from it can serve as a proxy for degree ofsocial familiarity.
The classifier also affords us anopportunity to re-examine hypotheses proposed bySchegloff and Sacks (1974; 1968; 1973) about thestructure of openings and closing in business andpersonal conversations.
Within social conversation,we investigated the accuracy of identifying conver-sations with close friends and relatives from others.The rest of this paper is arranged as follows.
Afterdescribing the corpus and ASR system in Sections 2and 3, we probe the nature of telephone conversa-tions in Section 4.
We present direct binary classifi-cation experiments in Section 5 and lastly, we closewith a few remarks in Section 6.2 Corpus: Everyday TelephoneConversations Spanning a YearOur corpus consists of 12,067 digitized land-linetelephone conversations.
Recordings were takenfrom 10 volunteers, 79 years or older, over a pe-riod of approximately 12 months.
Subjects were allnative English speakers recruited from the USA.
Inaddition to the conversations, our corpus includesa rich set of meta-data, such as call direction (in-coming vs outgoing), time of call, duration andDTMF/caller ID when available.
At the end of thedata collection, for each subject, twenty telephonenumbers were identified corresponding to top tenmost frequent calls and top ten longest calls.
Sub-jects were asked to identify their relationship withthe speakers at these numbers as immediate family,near relatives, close friends, casual friends, strangersand business.For this initial study, we discard conversationswith less than 30 automatically transcribed words.This was done primarily to get rid of spurious and/ornoisy recordings related to device failure as wellas incorrectly dialed telephone numbers.
Moreover,short conversations are less likely to provide enoughsocial context to be useful.Of the 8,558 available conversations, 2,728 wereidentified as residential conversations and 1,095were identified as business conversations using re-verse listings from multiple sources; e.g.
phonedirectory lookup, exit interviews, internet lookup.This left 4,395 unlabeled records, for which the re-verse listing was either inconclusive or for which thephone number information was missing and/or im-properly recorded.1133 Automatic Speech RecognitionConversations in our corpus were automaticallytranscribed using an ASR system.
Our ASR sys-tem is structured after IBM?s conversation telephonysystem which gave the top performance in the mostrecent evaluation of speech recognition technologyfor telephony by National Institute of Standards andTechnology (Soltau et al, 2005).
The acoustic mod-els were trained on about 2000 hours of telephonespeech from Switchboard and Fisher corpora (God-frey et al, 1992).
The system has a vocabulary of47K and uses a trigram language model with about10M n-grams, estimated from a mix of transcriptsand web-harvested data.
Decoding is performedin three stages using speaker-independent models,vocal-tract normalized models and speaker-adaptedmodels.
The three sets of models are similar incomplexity with 4000 clustered pentaphone statesand 150K Gaussians with diagonal covariances.
Oursystem does not include discriminative training andperforms at a word error rate of about 24% on NISTRT Dev04 which is comparable to state of the artperformance for such systems.
The privacy require-ments in place for our corpus prohibit human lis-tening ?
precluding the transcriptions needed report-ing recognition accuracy.
However, while our cor-pus differs from Switchboard, we expect the perfor-mance of the 2000 hour recognizer to be relativelyclose to results on NIST benchmark.4 Nature of Telephone Conversations4.1 Classification ExperimentsAs mentioned earlier, we first learned a baseline bi-nary classifier to filter out business calls from res-idential calls.
Apart from using this as a tool toprobe the characteristics of social calls, it also helpsus to classify unlabeled calls and thus avoid discardhalf the corpus from subsequent analysis of socialnetwork and relationships.
Recall, the labels forthe calls were obtained using reverse lookup frommultiple sources.
We assume that the majority ofour training set reflect the true nature of the con-versations and expect to employ the classifier sub-sequently for correcting the errors arising when per-sonal conversations occur on business lines and viceversa.We learned a baseline SVM classifier using a bal-anced training set.
From the labeled records we cre-ated a balanced verification set containing 164,115words over 328 conversations.
The remainder wasused to create a balanced training set consisting of866,696 words over 1,862 conversations.
The SVMwas trained on 20-fold cross validation and evalu-ated on the verification set.
After experimentingwith different kernels, we found an RBF kernel tobe most effective, achieving an accuracy of 87.50%on the verification data.4.2 Can the Scores of the Binary ClassifierDifferentiate Types of Social Relationship?Since the SVM score has utility in measuring a con-versation on the social-business axis, we now exam-ine its usefulness in differentiating social ties.
Totest this, we computed SVM score statistics for allconversations with family and friends.
For compar-ison, we also computed the statistics for all conver-sations automatically tagged as residential as well asall conversations in the data.
Table 1 shows the av-erage family score is unambiguously higher than theaverage residential conversation (independent sam-ple t-test, p < 0.001).
This is an interesting re-sult since distinction of family conversations (fromgeneral social calls) never factored into the SVM.Rather, it appears to arise naturally as an extrap-olation from the more general residential/businessdiscriminator.
The friend sub-population exhibitedstatistics much closer to the general residential pop-ulation and its differences were not significant to anydegree.
The overlap between scores for conversa-tions with family and friends overlap significantly.Notably, the conversations with family have a sig-nificantly higher mean and a tighter variance thanwith other social ties.Table 1: SVM scores for phone number sub-categories.Category # Calls Mean score STDFamily 1162 1.12 0.50Friends 532 0.95 0.51Residential 2728 0.93 0.63Business 1095 -1.16 0.70Global 8558 0.46 0.961144.3 How Informative are Openings andClosings in Differentiating TelephoneConversations?Schegloff and Sacks assert openings (beginnings)and closings (ends) of telephone conversations havecertain identifiable structures (Sacks et al, 1974).For example, the structure of openings facilitate es-tablishing identity of the conversants and the pur-pose of their call (Schegloff, 1968).
Closings inpersonal conversations are likely to include a pre-closing signal that allows either party to mentionany unmentioned mentionables before conversationends (Schegloff and Sacks, 1973).Given the above assertions, we expect openingsand closings to be informative about the type of con-versations.
Using our classifier, we compare the ac-curacy of predicting the type from openings, clos-ings and random segments of the conversations.
Fordifferent lengths of the three types of segments, theobserved performance of the classifier is plotted inFigure 1.
The results for the random segment werecomputed by averaging over 100 trials.
Several im-portant results are immediately apparent.
Openingspossess much higher utility than closings.
This isconsistent with general intuition that the opening ex-change is expected to clarify the nature and topicof the call.
Closings were found to be only as in-formative as random segments from the conversa-tions.
This is contrary to what one might expectfrom Schegloff and Sack?s assertion that pre-closingdiffer significantly in personal telephone calls (Sche-gloff and Sacks, 1973).
Less intuitive is the fact thatincreasing the length of the opening segment doesnot improve performance.
Surprisingly, a 30-wordsegment from the opening appears to be sufficient toachieve high classification accuracy (87.20%).4.4 Data Sparsity or Inherent Ambiguity: Whyare Short Conversations difficult toClassify?Sparsity often has a deleterious effect on classifica-tion performance.
In our experiments, we noticedthat shorter conversations suffer from poor classifi-cation.
However, the results from the above sectionappear to contradict this assertion, as a 30-word win-dow can give very good performance.
This seems tosuggest short conversations suffer poor recognition30 50 100 250 500 1000051015202530Number of words sampledRes/biz classificationerrror(%)Word sample from startWord sample from endWord sample randomly takenFigure 1: Comparison of classification accuracy in pre-dicting the type of conversation from openings, closingsand random segments.
Error bars are one standard devia-tion.due to properties beyond the obvious sparsity effect.To test this, we investigated the differences in shortand long conversations in greater detail.
We sepa-rate calls into quintile groups based on word counts.However, we now calculate all features from a 30-word opening ?
eliminating effects directly relatedto size.
The results in Table 2 show that the abil-Table 2: Accuracy in predicting the type of conversationwhen they are truncated to 30-words of openings basedon conversation length quintiles.
The column, Res / Biz,split gives the label distributions for the quintiles.Orig.
Word Counts Split AccuracyQuintile #Words Res.
/ Biz.0-20 30-87 62.12 / 37.88 78.620-40 88-167 48.48 / 51.52 82.840-60 168-295 39.39 / 60.61 91.460-80 296-740 40.91 / 59.09 87.880-100 741+ 59.38 / 40.62 93.4ity to predict the type of conversation does not de-grade when long conversations are truncated.
Mean-while, the accuracy of classification drops for (orig-inally) short conversations.
There is a surprisinglysmall performance loss due to the artificial trunca-tion.
These observations suggest that the long andshort conversations are inherently different in na-ture, at least in their openings.We should point out that spurious recordingsin our corpus are concentrated in the low wordcount group ?
undoubtedly dropping their accura-cies.
However, the trend of improving accuracy per-sists well into the high word count ranges where spu-115rious records are rare.
Given this fact, it appears thatindividuals in our corpus are more careful in enun-ciating the reasons for calling if an extended phoneconversation is anticipated.4.5 Can Openings Help Predict RelativeLengths of Conversations?From the results presented so far, we know thatopenings are good predictors of the type of conver-sations yet to unfold.
We also know that there are in-herent language differences between short and longconversations.
So, it is natural to ask whether open-ings can predict relative lengths of conversations.To test this hypothesis, we bin conversations into5 groups or ranks based on their percentile lengths(word counts) ?
very short, short, moderate, longand very long durations, as in Table 2.
Using in-dependent features from the 30-word opening, weattempt to predict the relative rank of two conver-sations by learning a rank SVM (Joachims, 2006).We found the ranker to give 27% error rate, signifi-cantly lower (independent sample t-test, d.f.
?
1M,p<0.01) than the random chance of 40%.
Chancebaseline was determined using Monte Carlo simula-tion (1M random rankings) in conjunction with therank SVM evaluation (Joachims, 2006).Features from very short conversations may con-tain both openings and closings, i.e., both a hello anda goodbye, making them easier to rank.
To avoid thisconfounding factor, we also compute performanceafter discarding the shortest grouping of conversa-tions (< 88 words) to ensure closings are avoided inthe 30-word window.
The resulting classifier overshort, medium, long, very long conversations ranked30% of the pairs erroneously, somewhat better thanchance at 37%.
Though the performance gain overthe random ranker has shrunk considerably, there isstill some utility in using the opening of a conversa-tion to determine its ultimate duration.
However, itis clear predicting duration via conversation openingis a much more difficult task overall.5 Supervised Classification of Types ofSocial RelationshipsWhile the scores of the binary classifier providedstatistically significant differences between calls todifferent types of social relationships, they are notparticularly useful in classifying the calls with highaccuracy.
In this section, we investigate the perfor-mance of classifiers to differentiate the following bi-nary classes.?
Residential vs business?
Family vs all other?
Family vs other residential?
Familiar vs non-familiarFamiliar denotes calls to those numbers with whomsubject has conversed more than 5 times.
Recallthat the numbers corresponding to family memberswere identified by the subjects in a post-collectioninterview.
We learned binary classifier for the fourcases, a few of which were reported in our earlywork (Stark et al, 2011).
We investigated a vari-ety of features in these tasks.
A breakdown of thecorpus is give in Table 3.
Not all categories are mu-tually exclusive.
For example the majority of familyconversations also fall into the familiar and residen-tial categories.Table 3: Number of conversations per category.Category InstancesBiz.
1095Residential 2728Family 1111Res.
non-family 1462Familiar 3010All 85585.1 Lexical StatisticsSpeakers who share close social ties are likely toengage in conversations on a wide variety of top-ics and this is likely to reflect in the entropy of theirlanguage use.
We capture this aspect of languageuse by computing language entropy over the uni-gram word distribution for each conversation, i.e;H(d) = ?
?w p(w|d) log p(w|d), where p(w|d) isthe probability of word w given conversation d. Wealso included two other lexical statistics namely thespeaking rate and the word count (in log domain).Table 4 lists the utility of these language proper-ties for differentiating the four binary classes men-tioned earlier, where the p-value is computed usingtwo tailed independent sample t-test.116Table 4: T-statistics for different context groups.
Labels:a) Log-word count, b) speaking rate, c) language entropy.Asterisk denotes significance at p<0.0001.
Sample sizes(n) may be found in Table 3.Task d.f.
a) b) c)Res.
v. biz.
7646 1.9 10.1?
-1.9Family v. other 8556 16.3?
9.0?
13.4?Family v. other res.
2571 12.9?
5.1?
11.3?Familiar v. other 8556 10.4?
6.4?
9.3?For the most part, the significance tests conformwith preconceived ideas of language use over thetelephone.
It is shown that people talk longer,more rapidly and have wider range of language usewhen conversing with a familiar contact and/or fam-ily member.
Surprisingly, only the speaking rateshowed significant differences among the residen-tial/business categories, with business conversationsbeing conducted at a slower pace at least for the el-derly demographic in our corpus.5.2 Linguistic inquiry and Word CountWe investigated a hand-crafted dictionary of salientwords, called Linguistic Inquiry and Word Count(LIWC), employed in social psychology stud-ies (Pennebaker et al, 2003).
This dictionary groupwords into 64 categories such as pronouns, activ-ity words, positive emotion and health.
The cate-gories have significant overlap and a given word canmap to zero or more categories.
The clear benefitof LIWC is that the word categories have very clearand pre-labeled meanings.
They suffer from the ob-vious drawback that the words are labeled in isola-tion without taking their context into account.
Thetags are not chosen under any mathematical criteriaand so there are no guarantees the resultant featurewill be useful or optimal for classifying utterances.Table 5 lists the LIWC categories significant (p<0.001) to the different classes.
The listed terms aresorted according to their t-statistic, with early andlater terms more indicative of first and second classlabels respectively.5.3 Latent Dirichlet alocationUnsupervised clustering and feature selection canmake use of data for which we have no labels.
Forexample, in the case of business and residential la-bels, unlabeled data amounts to about 50% of ourcorpus.
Motivated by this consideration, we exam-ined unsupervised clustering using Latent DirichletAllocation (LDA) (Blei et al, 2003).LDA models a conversation as a bag of words.The model generates a conversation by: (a) sam-pling a topic distribution ?
for the conversation usinga per-conversation Dirichlet topic distribution with ahyper-parameter ?, (b) sampling a topic z for eachword in the conversation using a multinomial distri-bution using the topic mixture ?, and (c) samplingthe word from a per-topic multinomial word distri-bution with a hyper-parameter ?
(Blei et al, 2003).The number of topics are assumed to be given.
Theper-conversation topic distribution and the per-topicword distribution can be automatically estimated tomaximize the likelihood of training data.
The spar-sity of these two distributions can be controlled bytweaking ?
and ?
; lower values increase sparsity.For our experiments, we estimated a maximumlikelihood 30-topic LDA model from the corpus.Experimentally, we found best cross-validation re-sults were obtained when ?
and ?
were set to 0.01and 0.1 respectively.When peering into the topics learned by the LDAmethod, it did appear that topics were approximatelyseparated into contextual categories.
Most interest-ing, when the number of clusters are reduced to two,the LDA model managed to segment residential andbusiness conversations with relatively high accuracy(80%).
This suggests the LDA model was able toapproximately learn these classes in an unsupervisedmanner.Table 6 lists words strongly associated with thetwo topics and clearly the unsupervised cluster-ing appears to have automatically differentiated thebusiness-oriented calls from the rest.
On closer ex-amination, we found that most of the probabilitywas distributed in a limited number of words in thebusiness-oriented topic.
On the contrary, the proba-bility was more widely distributed among words inthe other cluster, reflecting the diversity of contentin personal calls.5.4 Classifying Types of Social RelationshipsThough t-tests are useful for ruling out insignificantrelationships, they are insufficient for quantifyingthe degree of separability ?
and thus, ultimately their117Table 5: LIWC categories found to be significant in classifying relationships, ranked according to their t-statistic.Relationship CategoriesRes.
v. biz.
I, Past, Self, Motion, Other, Insight, Eating, Pronoun, Down, Physcal, Excl, Space, Cogmech, Home,Sleep, Tentat, Assent, / Article, Optim, Fillers, Senses, Hear, We, Feel, Inhib, Incl, You, School, Money,Occup, Job, NumberFamily v. all Other, Past, Assent, Sleep, Insight, I, Pronoun, Cogmech, Tentat, Motion, Self / Affect, Optim, Certain,Future, School, Comm, Job, We, Preps, Incl, Occup, You, NumberFamily v. res.
Other, Past, Sleep, Pronoun, Tentat, Cogmech, Insight, Humans / Comm, We, Incl, You, Preps, NumberFamiliar v. other Other, Assent, Past, I, Leisure, Self, Insight / Fillers, Certain, Social, Posemo, We, Future, Affect, Incl,Comm, Achieve, School, You, Optim, Job, OccupTable 6: Per-topic word distribution learned using unsu-pervised clustering with LDA.
Words are sorted accord-ing to their posterior topic distribution.
Words with iden-tical distributions are sorted alphabetically.Topic 1 Topic 2Invalid, helpline, eligibility,transactions, promo-tional, representative,mastercard, touchtone,activation, nominating,receiver, voicemail, digit,representatives, Chrysler,ballots, staggering, refills,resented, classics, metro,represented, administer,transfers, reselling, recom-mendations, explanation,floral, exclusive, submit.Adorable, aeroplanes,Arlene, Astoria, baked,biscuits, bitches, blisters,bluegrass, bracelet, brains,bushes, calorie, casinos,Charlene, cheeses, chit,Chris, clam, clientele,cock, cookie, copying,crab, Davenport, debating,dementia, dictionary, dime,Disneyland, eek, Eileen,fascinated, follies, fry,gained.utility in discrimination.
To directly test discrimi-nation performance, we use support vector machineclassifiers.
Before performing classification, we pro-duce balanced datasets that have equal numbers ofconversations for each category.
Our primary moti-vation for artificially balancing the label distributionin each experiment is to provide a consistent base-line over which each classifier may be compared.We learn SVM classifiers with an RBF kernel us-ing 85% of data for development.
SVM parametersare tuned with 20-fold cross-validation on the dev-set.
The accuracies of the classifiers, measured on aheld out set, are reported in Table 7.We tested four feature vectors: 1) unigram fre-quencies, 2) surface language features (log wordcount, speaking rate, entropy), 3) the 64 dimensionLIWC frequency vector and 4) a 30-dimension vec-tor of LDA topic posterior log-probabilities.Table 7: SVM performance for the language features.
La-bels: a) unigram vector, b) lexical statistics, c) LIWC andd) LDA topic posterior log-probabilitiesTask 1-grams L.Stats LIWC LDARes.
v. biz.
84.95 67.61 78.70 81.03Family v. all 78.03 61.16 72.77 74.75Family v. res.
76.13 62.92 71.06 72.37Familiarity 69.17 60.92 64.20 69.56Overall, the plain unigram frequency vector pro-vided the best discrimination performance.
How-ever, this comes at significant training costs asthe unigram feature vector has a dimensionalityof approximately 20,000.
While the surface fea-tures did possess a degree of classification utility,there are clearly outclassed by the content-basedfeatures.
Furthermore, their integration into thecontent-features yielded only insignificant improve-ments to accuracy.
Finally, it is of interest to notethat the 30-topic LDA feature trained with ML cri-terion outperformed the 64-topic LIWC vector in allcases.6 ConclusionsThis paper studies a unique corpus of conversationaltelephone speech, a comprehensive and naturalis-tic sample of all the incoming and outgoing tele-phone calls from 10 older adults over the durationof one year.
Through empirical experiments weshow that the business calls can be separated fromsocial calls with accuracies as high as 85% usingstandard techniques.
Subgroups such as family canalso be differentiated automatically with accuraciesabove 74%.
When compared to language use (en-tropy) and hand-crafted dictionaries (LIWC), poste-118riors over topics computed using a latent Dirichletmodel provide superior performance.For the elderly demographic, openings of conver-sations were found to be more informative in clas-sifying conversation than closings or random seg-ments, when using automated transcripts.
The highaccuracy in classifying business from personal con-versations suggests potential applications in design-ing context user interface for smartphones to offericons related to work email, work calendar or Face-book apps.
In future work, we plan to examinesubject specific language use, turn taking and af-fect to further improve the classification of socialcalls (Shafran et al, 2003).7 AcknowledgementsThis research was supported in part by NIH Grants5K25AG033723-02 and P30 AG024978-05 andNSF Awards 1027834, 0958585 and 0905095.
Anyopinions, findings, conclusions or recommendationsexpressed in this publication are those of the authorsand do not reflect the views of the NIH or NSF.
Wethank Brian Kingsbury and IBM for making theirASR software tools available to us.
We are alsograteful to Nicole Larimer, Maider Lehr and Kather-ine Wild for their contributions to data collection.ReferencesDavid M. Blei, Andrew Y. Ng, and Michael I. Jordan.2003.
Latent dirichlet alocation.
J. Mach.
Learn.Res., 3:993?1022, March.T A Glass, C F Mendes de Leon, T E Seeman, and L FBerkman.
1997.
Beyond single indicators of socialnetworks: a lisrel analysis of social ties among the el-derly.
Soc Sci Med, 44(10):1503?1517.J.
Godfrey, E. Holliman, and J. McDaniel.
1992.SWITCHBOARD: Telephone speech corpus for re-search and development.
In IEEE International Con-ference on Acoustics, Speech, and Signal Processing,pages 517?520.T.
Joachims.
2006.
Training linear svms in lineartime.
In ACM Conference on Knowledge Discoveryand Data Mining.Haewoon Kwak, Changhyun Lee, Hosung Park, and SueMoon.
2010.
What is twitter, a social network ora news media?
In Proceedings of the 19th inter-national conference on World wide web, WWW ?10,pages 591?600, New York, NY, USA.
ACM.Alexander Pak and Patrick Paroubek.
2010.
Twit-ter as a corpus for sentiment analysis and opinionmining.
In Nicoletta Calzolari (Conference Chair),Khalid Choukri, Bente Maegaard, Joseph Mariani,Jan Odijk, Stelios Piperidis, Mike Rosner, and DanielTapias, editors, Proceedings of the Seventh Interna-tional Conference on Language Resources and Evalu-ation (LREC?10), Valletta, Malta, may.
European Lan-guage Resources Association (ELRA).J.
W. Pennebaker, M. R. Mehl, and K. G. Niederhoffer.2003.
Psychological aspects of natural language use:Our words, our selves.
Annual Review of Psychology,54(1):547?577.Sas?a Petrovic?, Miles Osborne, and Victor Lavrenko.2010.
Streaming first story detection with applica-tion to twitter.
In Human Language Technologies: The2010 Annual Conference of the North American Chap-ter of the Association for Computational Linguistics,HLT ?10, pages 181?189, Stroudsburg, PA, USA.
As-sociation for Computational Linguistics.Alan Ritter, Colin Cherry, and Bill Dolan.
2010.
Unsu-pervised modeling of twitter conversations.
In HumanLanguage Technologies: The 2010 Annual Conferenceof the North American Chapter of the Association forComputational Linguistics, HLT ?10, pages 172?180,Stroudsburg, PA, USA.
Association for ComputationalLinguistics.Harvey Sacks, Emanuel A. Schegloff, and Gail Jeffer-son.
1974.
A simplest systematics for the organizationof turn-taking for conversation language.
Language,50(4(1)):696?735.Emanuel A. Schegloff and Harvey Sacks.
1973.
Openingup closings.
Semiotica, 8:289?327.Emanuel A. Schegloff.
1968.
Sequencing in con-versational openings.
American Anthropologist,70(6):1075?1095.Izhak Shafran, Michael Riley, and Mehryar Mohri.
2003.Voice signatures.
In Proc.
IEEE Automatic SpeechRecognition and Understanding Workshop.H.
Soltau, B. Kingsbury, L. Mangu, D. Povey, G. Saon,and G. Zweig.
2005.
The IBM 2004 conversationaltelephony system for rich transcription.
In IEEE Inter-national Conference on Acoustics, Speech, and SignalProcessing, volume 1, pages 205?208.Anthony Stark, Izhak Shafran, and Jeffrey Kaye.
2011.Supervised and unsupervised feature selection for in-ferring social nature of telephone conversations fromtheir content.
In Proc.
IEEE Automatic Speech Recog-nition and Understanding Workshop.Paul Taylor, Rich Morin, Kim Parker, D?Vera Cohn,and Wendy Wang.
June 29, 2009.
Grow-ing old in America: Expectations vs. reality.http://pewsocialtrends.org/files/2010/10/Getting-Old-in-America.pdf.119
