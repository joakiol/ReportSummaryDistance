First Joint Conference on Lexical and Computational Semantics (*SEM), pages 514?518,Montre?al, Canada, June 7-8, 2012. c?2012 Association for Computational LinguisticsICT:A System Combination for Chinese Semantic Dependency ParsingHao Xiong and Qun LiuKey Lab.
of Intelligent Information ProcessingInstitute of Computing TechnologyChinese Academy of SciencesP.O.
Box 2704, Beijing 100190, China{xionghao, liuqun}@ict.ac.cnAbstractThe goal of semantic dependency parsing is tobuild dependency structure and label seman-tic relation between a head and its modifier.To attain this goal, we concentrate on obtain-ing better dependency structure to predict bet-ter semantic relations, and propose a methodto combine the results of three state-of-the-artdependency parsers.
Unfortunately, we madea mistake when we generate the final outputthat results in a lower score of 56.31% in termof Labeled Attachment Score (LAS), reportedby organizers.
After giving golden testing set,we fix the bug and rerun the evaluation script,this time we obtain the score of 62.8% whichis consistent with the results on developing set.We will report detailed experimental resultswith correct program as a comparison stan-dard for further research.1 IntroductionIn this year?s Semantic Evaluation Task, the organiz-ers hold a task for Chinese Semantic DependencyParsing.
The semantic dependency parsing (SDP)is a kind of dependency parsing.
It builds a depen-dency structure for a sentence and labels the seman-tic relation between a head and its modifier.
Thesemantic relations are different from syntactic rela-tions.
They are position independent, e.g., the pa-tient can be before or behind a predicate.
On theother hand, their grains are finer than syntactic re-lations, e.g., the syntactic subject can be agent orexperiencer.
Readers can refer to (Wanxiang Che,2012) for detailed introduction.Figure 1: The pipeline of our system, where we com-bine the results of three dependency parsers and use max-entropy classifier to predict the semantic relations.Different from most methods proposed inCoNLL-2008 1 and 2009 2, in which some re-searchers build a joint model to simultaneously gen-erate dependency structure and its syntactic relations(Surdeanu et al, 2008; Hajic?
et al, 2009), here,we first employ several parsers to generate depen-dency structure and then propose a method to com-bine their outputs.
After that, we label relation be-tween each head and its modifier via the traversalof this refined parse tree.
The reason why we usea pipeline model while not a joint model is thatthe number of semantic relations annotated by or-ganizers is more than 120 types, while in the for-mer task is only 21 types.
Compared to the formertask, the large number of types will obviously dropthe performance of classifier.
On the other hand, theperformance of syntactic dependency parsing is ap-proaching to perfect, intuitively, that better depen-dency structure does help to semantic parsing, thuswe can concentrate on improving the accuracy of de-pendency structure construction.The overall framework of our system is illustrated1http://www.yr-bcn.es/conll2008/2http://ufal.mff.cuni.cz/conll2009-st/514in figure 1, where three dependency parsers are em-ployed to generate the dependency structure, and amaximum entropy classifier is used to predict rela-tion for head and its modifier over combined parsetree.
Final experimental results show that our sys-tem achieves 80.45% in term of unlabeled attach-ment score (UAS), and 62.8 % in term of LAS.
Bothof them are higher than the baseline without usingsystem combinational techniques.In the following of this paper, we will demonstratethe detailed information of our system, and reportseveral experimental results.2 System DescriptionAs mentioned, we employ three single dependencyparsers to generate respect dependency structure.
Tofurther improve the accuracy of dependency struc-ture construction, we blend the syntactic outputs andfind a better dependency structure.
In the followings,we will first introduce the details of our strategy fordependency structure construction.2.1 ParsersWe implement three transition-based dependencyparsers with three different parsing algorithms:Nivre?s arc standard, Nivre?s arc eager (see Nivre(2004) for a comparison between the two Nivre al-gorithms), and Liang?s dynamic algorithm(Huangand Sagae, 2010).
We use these algorithms forseveral reasons: first, they are easy to implementand their reported performance are approaching tostate-of-the-art.
Second, their outputs are projective,which is consistent with given corpus.2.2 Parser CombinationWe use the similar method presented in Hall et al(2011) to advance the accuracy of parses.
The parsesof each sentence are combined into a weighted di-rected graph.
The left procedure is similar to tradi-tional graph-based dependency parsing except thatthe number of edges in our system is smaller sincewe reserve best edges predicted by three singleparsers.
We use the popular Chu-Liu-Edmonds al-gorithm (Chu and Liu, 1965; Edmonds et al, 1968)to find the maximum spanning tree (MST) of thenew constructed graph, which is considered as thefinal parse of the sentence.
Specifically, we use theparsing accuracy on developing set to represent theweight of graph edge.
Formally, the weight of graphedge is computed as follows,we =?p?PAccuracy(p) ?
I(e, p) (1)where the Accuracy(p) is the parsing score ofparse tree p whose value is the score of parsing accu-racy on developing set, and I(e, p) is an indicator, ifthere is such dependency in parse tree p, it returns 1,otherwise returns 0.
Since the value of Accuracy(p)ranges from 0 to 1, we doesn?t need to normalize itsvalue.Thus, the detailed procedure for dependencystructure construction is,?
Parsing each sentence using Nivre?s arc stan-dard, Nivre?s arc eager and Liang?s dynamic al-gorithm, respectively.?
Combining parses outputted by three parsersinto weighted directed graph, and representingits weight using equation 1.?
Using Chu-Liu-Edmonds algorithm to searchfinal parse for each sentence.2.3 Features for LabelingAfter given dependency structure, for each relationbetween head and its modifier, we extract 31 typesof features, which are typically exploited in syntac-tic dependency parsing, as our basic features.
Basedon these basic features, we also add a additional dis-tance metric for each features and obtain 31 types ofdistance incorporated features.
Besides that, we usegreedy hill climbing approach to select additional 29features to obtain better performance.
Table 1 showsthe basic features used in our system,And the table 2 gives the additional features.
Itis worth mentioning, that the distance is calculatedas the difference between the head and its modifier,which is different from the calculation reported bymost literatures.2.4 ClassifierWe use the classifier from Le Zhang?s MaximumEntropy Modeling Toolkit3 and use the L-BFGS3http://homepages.inf.ed.ac.uk/s0450736/maxent toolkit.html515FeaturesBasicmw:modifier?s wordmp:modifier?s POS taghw:head?s wordhp:head?s POS tagCombinationhw|hp,mw|mp,hw|mwhp|mp,hw|mp,hp|mwhw|hp|mwhw|hp|mphw|mw|mphp|mw|mphp|mp|mp-1hp|mp|mp+1hp|hp-1|mphp|hp+1|mphp|hp-1|mp-1hp|hp-1|mp+1hp|hp+1|mp-1hp|hp+1|mp+1hp-1|mp|mp-1hp-1|mp|mp+1hp+1|mp|mp-1hp+1|mp|mp+1hw|hp|mw|mphp|hp-1|mp|mp-1hp|hp+1|mp|mp+1hp|hp+1|mp|mp-1hp|hp-1|mp|mp+1Table 1: The basic features used in our system.
-1 and+1 indicate the one on the left and right of given word.parameter estimation algorithm with gaussian priorsmoothing(Chen and Rosenfeld, 1999).
We set thegaussian prior to 2 and train the model in 1000 iter-ations according to the previous experience.3 ExperimentsThe given corpus consists of 8301 sentencesfor training(TR), and 569 sentences for develop-ing(DE).
For tuning parameters, we just use TR por-tion, while for testing, we combine two parts andretrain the parser to obtain better results.
Surely, wealso give results of testing set trained on TR portionfor comparison.
In the following of this section, wewill report the detailed experimental results both onFeaturesDistance dist:basic features with distanceAdditionallmw:leftmost word of modifierrnw:rightnearest word of modifiergfw:grandfather of modifierlmp,rnp,gfplmw|lmp,rnw|rnp,lmw|rnwlmp|rnp,lmw|mw,lmp|mprnw|mw,rnp|mp,gfw|mwgfp|mp,gfw|hw,gfp|hpgfw|mw|gfp|mplmw|lmp|mw|mprnw|rnp|mw|mplmw|rnw|mw,lmp|rnp|mpgfw|hw|gfp|hpgfw|mw|hw,gfp|mp|hpgfw|mw|hw|gfp|mp|hplmw|rnw|lmp|rnp|mw|mplmw|rnw|lmp|rnpTable 2: The additional features used in our system.developing and testing set.3.1 Results on Developing SetWe first report the accuracy of dependency construc-tion on developing set using different parsing al-gorithms in table 3.
Note that, the features usedin our system are similar to that used in their pub-lished papers(Nivre, 2003; Nivre, 2004; Huang andSagae, 2010).
From table 3 we find that althoughPrecision (%)Nivre?s arc standard 78.86Nivre?s arc eager 79.11Liang?s dynamic 79.78System Combination 80.85Table 3: Syntactic precision of different parsers on devel-oping set.using simple method for combination over three sin-gle parsers, the system combination technique stillachieves 1.1 points improvement over the highestsingle system.
Since the Liang?s algorithm is a dy-namic algorithm, which enlarges the searching spacein decoding, while the former two Nivre?s arc al-516gorithms actually still are simple beam search al-gorithm, thus the Liang?s algorithm achieves betterperformance than Nivre?s two algorithm, which isconsistent with the experiments in Liang?s paper.To acknowledge that the better dependency struc-ture does help to semantic relation labeling, we fur-ther predict semantic relations on different depen-dency structures.
For comparison, we also report theperformance on golden structure.
Since our combi-Precision (%)Nivre?s arc standard 60.84Nivre?s arc eager 60.76Liang?s dynamic 61.43System Combination 62.92Golden Tree 76.63Table 4: LAS of semantic relations over different parseson developing set.national algorithm requires weight for each edges,we use the developing parsing accuracy 0.7886,0.7911, and 0.7978 as corresponding weights foreach single system.
Table 4 shows, that the pre-diction of semantic relation could benefit from theimprovement of dependency structure.
We also no-tice that even given the golden parse tree, the per-formance of relation labeling is still far from per-fect.
Two reasons could be explained for that: firstis the small size of supplied corpus, second is thatthe relation between head and its modifier is toofine-grained to distinguish for a classifier.
More-over, here we use golden segmentation for parsing,imagining that an automatic segmenter would fur-ther drop the accuracy both on syntactic and seman-tic parsing.3.2 Results on Testing SetSince there is a bug4 in our final results submittedto organizers, here, in order to confirm the improve-ment of our method and supply comparison standardfor further research, we reevaluate the correct outputand report its performance on different training set.Table 5 and table 6 give the results trained on dif-ferent corpus.
We can see that when increasing the4The bug is come from that when we converting the CoNLL-styled outputs generated by our combination system into plaintext.
While in developing stage, we directly used CoNLL-styledoutputs as our input, thus we didn?t realize this mistake.training size, the performance is slightly improved.Also, we find the results on testing set is consistentwith that on developing set, where best dependencystructure achieves the best performance.LAS (%) UAS(%)Nivre?s arc standard 60.38 78.19Nivre?s arc eager 60.78 78.62Liang?s dynamic 60.85 79.09System Combination 62.76 80.23Submitted Error Results 55.26 71.85Table 5: LAS and UAS on testing set trained on TR.LAS (%) UAS(%)Nivre?s arc standard 60.49 78.25Nivre?s arc eager 60.99 78.78Liang?s dynamic 61.29 79.59System Combination 62.80 80.45Submitted Error Results 56.31 73.20Table 6: LAS and UAS on testing set trained on TR andDE.4 ConclusionIn this paper, we demonstrate our system frameworkfor Chinese Semantic Dependency Parsing, and re-port the experiments with different configurations.We propose to use system combination to better thedependency structure construction, and then labelsemantic relations over refined parse tree.
Final ex-periments show that better syntactic parsing do helpto improve the accuracy of semantic relation predic-tion.AcknowledgmentsThe authors were supported by National ScienceFoundation of China, Contracts 90920004, andHigh-Technology R&D Program (863) Project No2011AA01A207 and 2012BAH39B03.
We thankHeng Yu for generating parse tree using Liang?s al-gorithm.
We thank organizers for their generoussupplied resources and arduous preparation.
We alsothank anonymous reviewers for their thoughtful sug-gestions.517ReferencesStanley F. Chen and Ronald Rosenfeld.
1999.
A gaussianprior for smoothing maximum entropy models.
Tech-nical report, CMU-CS-99-108.Y.J.
Chu and T.H.
Liu.
1965.
On the shortest arbores-cence of a directed graph.
Science Sinica, 14(1396-1400):270.J.
Edmonds, J. Edmonds, and J. Edmonds.
1968.
Opti-mum branchings.
National Bureau of standards.J.
Hajic?, M. Ciaramita, R. Johansson, D. Kawahara, M.A.Mart?
?, L. Ma`rquez, A. Meyers, J. Nivre, S. Pado?,J.
?Ste?pa?nek, et al 2009.
The conll-2009 sharedtask: Syntactic and semantic dependencies in multiplelanguages.
In Proceedings of the Thirteenth Confer-ence on Computational Natural Language Learning:Shared Task, pages 1?18.
Association for Computa-tional Linguistics.J.
Hall, J. Nilsson, and J. Nivre.
2011.
Single malt orblended?
a study in multilingual parser optimization.Trends in Parsing Technology, pages 19?33.L.
Huang and K. Sagae.
2010.
Dynamic programmingfor linear-time incremental parsing.
In Proceedings ofthe 48th Annual Meeting of the Association for Com-putational Linguistics, pages 1077?1086.
Associationfor Computational Linguistics.J.
Nivre.
2003.
An efficient algorithm for projective de-pendency parsing.
In Proceedings of the 8th Interna-tional Workshop on Parsing Technologies (IWPT.
Cite-seer.J.
Nivre.
2004.
Incrementality in deterministic depen-dency parsing.
In Proceedings of the Workshop on In-cremental Parsing: Bringing Engineering and Cogni-tion Together, pages 50?57.
Association for Computa-tional Linguistics.M.
Surdeanu, R. Johansson, A. Meyers, L. Ma`rquez, andJ.
Nivre.
2008.
The conll-2008 shared task on jointparsing of syntactic and semantic dependencies.
InProceedings of the Twelfth Conference on Computa-tional Natural Language Learning, pages 159?177.Association for Computational Linguistics.Ting Liu Wanxiang Che.
2012.
Semeval-2012 Task 5:Chinese Semantic Dependency Parsing.
In Proceed-ings of the 6th International Workshop on SemanticEvaluation (SemEval 2012).518
