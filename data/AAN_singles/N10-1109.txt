Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 721?724,Los Angeles, California, June 2010. c?2010 Association for Computational LinguisticsClassification of Prosodic Events using Quantized Contour ModelingAndrew RosenbergDepartment of Computer ScienceQueens College CUNY, New York, USAandrew@cs.qc.cuny.eduAbstractWe present Quantized Contour Modeling (QCM), aBayesian approach to the classification of acousticcontours.
We evaluate the performance of this tech-nique in the classification of prosodic events.
Wefind that, on BURNC, this technique can success-fully classify pitch accents with 63.99% accuracy(.4481 CER), and phrase ending tones with 72.91%accuracy.1 IntroductionIntonation can significantly vary the intended meaningof a spoken utterance.
In Standard American English,contrast is frequently indicated with an accent that hasa steeper pitch rise ?
?I went to the store (not the li-brary)?
?
than an accent that is used to indicate focus orintroduce new information ?
?I went to the store (beforegoing home)?
.
At phrase boundaries, rising pitch canindicate uncertainty or that the speaker is asking a ques-tion ?
?John likes Mary??
vs. ?John likes Mary?.
Auto-matically detecting prosodic events and classifying theirtype allows natural language understanding systems ac-cess to intonational information that would unavailable ifprocessing transcribed text alone.The ToBI standard of intonation (Silverman et al,1992) describes intonational contours as a sequenceof High and Low tones associated with two types ofprosodic events ?
pitch accents and phrase boundaries.The tones describe an inventory of types of prosodicevents.
In this work, we present Quantized Contour Mod-eling, a novel approach to the automatic classification ofprosodic event types.In Section 2, we describe related work on this task.
Wedescribe Quantized Contour Modeling in Section 3.
Ourmaterials are described in Section 4.
Experimental resultsare presented and discussed in Section 5.
We concludeand describe future directions for this work in Section 6.2 Related WorkFive types of pitch accents ?
pitch movements that corre-spond to perceived prominence of an associated word ?are defined in the ToBI standard(Silverman et al, 1992):H*, L*, L+H*, L*+H, H+!H*.
In addition to these five,high tones (H) can be produced in a compressed pitchrange indicated by (!H).
For the purposes of the experi-ments described in this paper, we collapse high (H) anddownstepped High (!H) tones into a single class leav-ing five accent types.
The ToBI standard describes twolevels of phrasing, intermediate phrases and intonationalphrases which are comprised of one or more intermedi-ate phrases.
Each intermediate phrase has an associatedphrase accent describing the pitch movement between theultimate pitch accent and the phrase boundary.
Phraseaccents can have High (H-), downstepped High (!H-) orlow (L-) tones.
Intonational phrase boundaries have anadditional boundary tone, to describe a final pitch move-ment.
These can be high (H%) or low (L%).
Intona-tional phrases have five possible phrase ending tone com-binations, L-L%, L-H%, H-L%, !H-L% and H-H%.
Insection 5.3, we describe experiments classifying thesephrase ending tones.The detection of pitch accents and phrase boundarieshas received significantly more research attention thanthe classification of accent types and phrase ending be-havior.
However, one technique that has been used ina number of research efforts is to simultaneously detectand classify pitch accent.
This is done by represent-ing pitch accent detection and classfication as a four-wayclassification task, where a token may be classified asUNACCENTED, HIGH, LOW, or DOWNSTEPPED.
BothRoss and Ostendorf (1996) and Sun (2002) used this ap-proach, reporting 72.4% and 77.0% accuracy respectivelywhen evaluated on a single speakers.
Levow also usedthis four-way classification for pitch accent detection andclassification under supervised (2005), and unsupervisedand semi-supervised learning approaches (2006).
Using721SVMs with only acoustic features, 81.3% accuracy at thesyllable level is achieved.
Using unsupervised spectralclustering, 78.4% accuracy is reported, while using thesemi-supervised technique, Laplacian SVMs, 81.5% ac-curacy is achieved.
Since these approaches simultane-ously evaluate the detection and classification of pitchaccents, direct comparison with this work is impossible.Ananthakrishnan and Narayanan (2008) used RFC(Taylor, 1994) and Tilt (Taylor, 2000) parameters alongwith word and part of speech language modeling to clas-sify pitch accents as H*, !H*, L+H* or L*.
When eval-uated on six BURNC speakers using leave-one-speaker-out cross-validation, accuracy of 56.4% was obtained.
Inthe same work, the authors were able to classify L-L%from L-H% phrase-final tones in the BURNC with 67.7%accuracy.
This performance was obtained using RFC F0parameterization and a language model trained over cat-egorical prosodic events.3 Quantized Contour ModelingIn this section, we present a modeling technique, Quan-tized Contour Modeling.
This technique quantizes the f0contour of a word in the time and pitch domains, generat-ing a low-dimensional representation of the contour.
Thepitch of the contour is linearly normalized to the range be-tween the minimum and maximum pitch in the contour,and quantized into N equally sized bins.
The time do-main is normalized to the range [0,1] and quantized intoM equally sized bins.
An example of such a quantiza-tion is presented in Figure 1 where N = 3 and M = 4.Using this quantized representation of a pitch contour, weFigure 1: Quantization with N=3 value and M=4 time bins.train a multinomial mixture model for each pitch accenttype.
Let the quantized contour be an M dimensionalvector C where C = (C1, C2, .
.
.
, CM ), where Ci ?
{0 .
.
.N ?
1}.
We indicate pitch (f0) contours by Cf0and intensity contours by CI .
We train a multinomialmodel p(type|Ci, i) for each time bin i ?
{0 .
.
.N ?
1}with Laplace (add-one) smoothing.
When using multi-nomial models, we quantize the mean of the pitch valuesassigned to a time bin.
We use these pitch accent typemodels to classify a contour using the Bayesian classi-fication function found in Equation 1.
This formulationassumes that the values at each time are conditionally in-dependent given the contour type.
Also, we can modifythe model incorporating a Markov hypothesis to includea sequential component by explicitly modeling the cur-rent and previous quantized values, as in Equation 2.
Weextend each of these models to model the energy contourshape simultaneously with the pitch contour.
The clas-sification technique allows for the number of pitch andenergy value quantization bins to be distinct.
However,in these experiments, we tie these, constraining them tobe equal.
The form of the classification functions usingthe energy contours are found in Figure 2.Standard shape modelingtype?
= argmaxtypep(type)MYip(Ci|type, i) (1)Sequential f0 modelingtype?
= argmaxtypep(type)MYip(Ci|Ci?1, type, i) (2)Standard f0 + I modelingtype?
= argmaxtypep(type)MYip(Cf0i , CIi |type, i) (3)Sequential f0 + I modelingtype?
= argmaxtypep(type)MYip(Cf0i , CIi |Cf0i?1, CIi , type, i)(4)Figure 2: Quantized contour modeling classification formulae.4 Materials and MethodsWe use two corpora that have been manually annotatedwith ToBI labels to evaluate the use of QCM in the clas-sification of prosodic events.
These two corpora are theBoston University Radio News Corpus (BURNC) (Os-tendorf et al, 1995) and the Boston Directions Corpus(BDC) (Nakatani et al, 1995).
The BURNC is a cor-pus of professionally read radio news data.
A 2.35 hour,29,578 word, subset from six speakers (three female andthree male) has been prosodically annotated.
The BDCis made up of elicited monologues spoken by four non-professional speakers, three male and one female.
TheBDC is divided into two subcorpora comprised of spon-taneous and read speech.
The 50 minutes of read speechcontain 10,831 words.
There are 60 minutes of annotatedspontaneous material containing 11,627 words.
Bothare spoken by the same four speakers.
In these experi-ments we evaluate these subcorpora separately, and referto them as BDC-spon and BDC-read, respectively.
Thedistribution of pitch accents and phrase-ending tones forthese three corpora can be found in Figure 3.722Corpus H* L+H* L* L*+H H+!H*BDC-read 78.24% 13.72% 5.97% 1.36% 0.71%BDC-spon 84.57% 6.32% 7.70% 0.68% 0.73%BURNC 69.99% 21.64% 3.67% 0.34% 4.37%Corpus L-L% L-H% H-L% !H-L% H-H%BDC-read 49.00% 35.62% 9.66% 4.29% 1.43%BDC-spon 29.45% 32.57% 30.96% 4.40% 2.61%BURNC 56.16% 38.38% 3.57% 0.68% 1.20%Figure 3: Distribution of prosodic event types in BURNC, BDC-read and BDC-spon corpora.In order to use QCM classification, we must firstidentify the region of an acoustic contour to quantify.Though there is evidence that acoustic evidence of promi-nence crosses the syllable boundary (Rosenberg andHirschberg, 2009), it is largely held that the acoustic ex-cursion corresponding to intonational prominence is cen-tered around a syllable.
To identify the region of analysisfor QCM, we identify the accent-bearing syllable fromthe manual prosodic annotation, and quantize the contourextracted from the syllable boundaries.
For the BURNCmaterial, forced alignment syllable boundaries are avail-able.
However, no forced-alignment phone informationis available for the BDC data.
Therefore we apply Villinget al?s (2004) envelope based pseudosyllabification rou-tine to identify candidate syllabic regions.
We use thepseudosyllable containing the accent annotation as the re-gion of analysis for the BDC material.
For classificationof phrase ending intonation, we use the final syllable (orpseudosyllable) in the phrase as the region of analysis.To be clear, the accent and phrase boundary locations arederived from manual annotations; the intonational tonesassociated with these events are classified using QCM.5 Prosodic Event Classification ResultsIn this section we present results applying QCM to theclassification of pitch accents and phrase ending intona-tion.
The work described in this section assumes thepresence of prosodic events is known a priori.
The ap-proaches described can be seen as operating on output ofan automatic prosodic event detection system.5.1 Combined Error RateAutomatic pitch accent classification poses an interest-ing problem.
Pitrelli, et al (Pitrelli et al, 1994) reporthuman agreement of only 64.1% on accent classifica-tion in the ToBI framework.
If downstepped variants ofaccents are collapsed with their non-downstapped formsthis agreement improves to 76.1%.
Second, pitch accentsare overwhelmingly H* in most labeled corpus, includ-ing the BDC and BURNC material used in this paper.This skewed class distribution leads to a very high base-line, at or above the rate of human agreement.
Becauseof this, we find accuracy an unreliable measure for evalu-ating the performance of this task.
Multiple solutions canhave similar accuracy, but radically different classifica-tion performance on minority classes.
We therefore pro-pose to use a different measure for the evaluation of pitchaccent type classification.
We define the Combined ErrorRate (CER) as the mean of the weighted rates of Type Iand Type II errors.
The combination of these measuresresults in an increased penalty for errors of the majorityclass while being more sensitive to minority class perfor-mance than accuracy.
Throughout this chapter, we willcontinue to report accuracy for comparison to other work,but consider CER to provide a more informative evalua-tion.
To avoid confusion, accuracy will be reported as apercentage (%) while CER will be reported as a decimal.CER = p(FP ) + p(FN)2 (5)The Type I error rate measures the false positive rate fora given class (cf.
Equation 6).p(FP ) =?ip(Ci)p(FPi) (6)We combine this measure with the Weighted Type II Er-ror Rate (cf.
Equation 7).
The Type II error rate measuresthe false negative rate for a given classp(FN) =?ip(Ci)p(FNi) (7)5.2 Pitch Accent ClassificationThe first step in applying Quantized Contour Modelingis to fix the desired quantization parameters.
We do thisby identifying a stratified 10% held out tuning set fromthe training data.
We evaluate quantization sizes rangingbetween 2 and 7 for both the time and value parameters,leading to 36 candidates.
Once we identify the best pa-rameterization on this tuning data, we run ten-fold crossvalidation on the remaining data to evaluate the perfor-mance of each modeling technique (cf.
Figure 2).The classification accuracy and CER for each modelis reported in Table 1 along with the number of time andvalue bins that were used.
We first observe that model-ing intensity information with f0 data does not improveclassification performance.
The alignment between pitchand intensity peaks have been shown to distinguish pitchaccent types (Rosenberg, 2009); this relationship is notsuccessfully captured by QCM.
Moreover, we find thatsequential modeling only leads to improvements in CERon BDC-read.
On all corpora, the classification accuracyis improved, with statistically insignificant (p > 0.05)reductions in CER.
This leads us to consider sequentialmodeling of pitch to be the best performing approach tothe classification of pitch accent using QCM.723Method BDC-read BDC-spon BURNCf0 46.51/.3860(5,3) 55.41/.4103(3,4) 47.56/.4444(4,4)Seq.
f0 73.17/.3667(6,7) 81.20/.4156 (7,5) 63.99/.4481(7,7)f0+I 37.53/.4094(3,3) 47.96/.4222(4,2) 48.36/.4472(2,2)Seq.
f0+I 74.08/.4032(7,3) 80.60/.4361(5,4) 66.97/.4530(6,5)Baseline 78.22/.0000 84.57/.0000 70.23/.0000Table 1: Accuracy (%), CER, time and value bins from QCM pitch accent type classification experiments.5.3 Phrase-ending Tone ClassificationAs in Section 5.2, we identify the best performing quanti-zation parameters on a stratified 10% tuning set, then run10-fold cross validation on the remaining data.
Resultsfrom QCM classification experiments classifying intona-tional phrase ending tone combinations ?
phrase accentand boundary tone ?
can be found in Table 2.
We findMethod BDC-read BDC-spon BURNCf0 48.21(3,6) 40.26(2,2) 70.36 (5,2)Seq.
f0 53.86(2,2) 43.80(4,4) 71.77 (6,2)f0+I 48.21(6,6) 38.28(6,6) 67.83(2,2)Seq.
f0+I 57.94(6,6) 46.61(6,5) 72.91(7,7)Baseline 49% 32% 55%Table 2: Accuracy (%), time and value bins from QCM phraseending tone classification experiments.that the simultaneous modeling of f0 and intensity con-sistently yields the best performance in the classificationof phrase ending tones.
These results all represent signif-icant improvement over the majority class baseline.
Theinteraction between pitch and intensity contours in theclassification of phrase-ending intonation has not beenthoroughly investigated and remains an open area for fu-ture research.6 Conclusion and Future WorkIn this paper we present a novel technique for the clas-sification of two dimensional contour data, QuantizedContour Modeling (QCM).
QCM operates by quantizingacoustic data into a pre-determined, fixed number of timeand value bins.
From this quantized data, a model of thevalue information is constructed for each time bin.
Thelikelihood of new data fitting these models is then per-formed using a Bayesian inference.We have applied QCM to the tasks of classifying pitchaccent types, and phrase-ending intonation.
The bestperforming parameterizations of QCM are able to clas-sify pitch accent types on BURNC with 63.99% accuracyand .4481 Combined Error Rate (CER).
QCM classifiesphrase ending tones on this corpus with 72.91% accuracy.These results do not represent the best performing ap-proaches to these tasks.
The best reported classificationof pitch accent types on BURNC is 59.95% accuracy and.422 CER, for phrase ending intonation 75.09% (Rosen-berg, 2009).
However, the classification of phrase endingintonation is accomplished by including QCM posteriorsin an SVM feature vector with other acoustic features.This technique may be applicable to classifying otherphenomena.
Here we have used ToBI tone classificationsas an intermediate representation of intonational phenom-ena.
QCM could be used to directly classify turn-takingbehavior, or dialog acts.
Also, previous work has lookedat using the same techniques to classify prosodic eventsand lexical tones in tonal languages such as MandarinChinese.
QCM could be directly applied to lexical tonemodeling; the only modification required would be a dif-ferent segmentation routine.ReferencesS.
Ananthakrishnan and S. Narayanan.
2008.
Fine-grainedpitch accent and boundary tone labeling with parametric f0features.
In ICASSP.G.-A.
Levow.
2005.
Context in multi-lingual tone and pitchaccent recognition.
In Interspeech.G.-A.
Levow.
2006.
Unsupervised and semi-supervised learn-ing of tone and pitch accent.
In HLT-NAACL.C.
Nakatani, J. Hirschberg, and B. Grosz.
1995.
Discoursestructure in spoken language: Studies on speech corpora.
InAAAI Spring Symposium on Empirical Methods in DiscourseInterpretation and Generation.M.
Ostendorf, P. Price, and S. Shattuck-Hufnagel.
1995.
Theboston university radio news corpus.
Technical Report ECS-95-001, Boston University, March.J.
Pitrelli, M. Beckman, and J. Hirschberg.
1994.
Evaluation ofprosodic transcription labeling reliability in the tobi frame-work.
In ICSLP.A.
Rosenberg and J. Hirschberg.
2009.
Detecting pitch accentsat the word, syllable and vowel level.
In HLT-NAACL.A.
Rosenberg.
2009.
Automatic Detection and Classificationof Prosodic Events.
Ph.D. thesis, Columbia University.K.
Ross and M. Ostendorf.
1996.
Prediction of abstractprosodic labels for speech synthesis.
Computer Speech &Language, 10(3):155?185.K.
Silverman, et al 1992.
Tobi: A standard for labeling englishprosody.
In ICSLP.X.
Sun.
2002.
Pitch accent predicting using ensemble machinelearning.
In ICSLP.P.
Taylor.
1994.
The rise/fall/connection model of intonation.Speech Commun., 15(1-2):169?186.P.
Taylor.
2000.
Analysis and synthesis of intonation using thetilt model.
Journal of the Acoustical Society of America.R.
Villing, et al 2004.
Automatic blind syllable segmentationfor continuous speech.
In ISSC.724
