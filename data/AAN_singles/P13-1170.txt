Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 1733?1743,Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational LinguisticsWhy-Question Answeringusing Intra- and Inter-Sentential Causal RelationsJong-Hoon Oh?
Kentaro Torisawa?
Chikara Hashimoto ?
Motoki Sano?Stijn De Saeger?
Kiyonori Ohtake?Information Analysis LaboratoryUniversal Communication Research InstituteNational Institute of Information and Communications Technology (NICT){?rovellia,?
torisawa,?
ch,?
msano,?stijn,?kiyonori.ohtake}@nict.go.jpAbstractIn this paper, we explore the utility ofintra- and inter-sentential causal relationsbetween terms or clauses as evidence foranswering why-questions.
To the best ofour knowledge, this is the first work thatuses both intra- and inter-sentential causalrelations for why-QA.
We also proposea method for assessing the appropriate-ness of causal relations as answers to agiven question using the semantic orienta-tion of excitation proposed by Hashimotoet al (2012).
By applying these ideasto Japanese why-QA, we improved preci-sion by 4.4% against all the questions inour test set over the current state-of-the-art system for Japanese why-QA.
In addi-tion, unlike the state-of-the-art system, oursystem could achieve very high precision(83.2%) for 25% of all the questions in thetest set by restricting its output to the con-fident answers only.1 Introduction?Why-question answering?
(why-QA) is a task toretrieve answers from a given text archive for awhy-question, such as ?Why are tsunamis gen-erated??
The answers are usually text fragmentsconsisting of one or more sentences.
Althoughmuch research exists on this task (Girju, 2003;Higashinaka and Isozaki, 2008; Verberne et al,2008; Verberne et al, 2011; Oh et al, 2012), itsperformance remains much lower than that of thestate-of-the-art factoid QA systems, such as IBM?sWatson (Ferrucci et al, 2010).In this work, we propose a quite straightfor-ward but novel approach for such difficult why-QA task.
Consider the sentence A1 in Table 1,which represents the causal relation between thecause, ?the ocean?s water mass ..., waves are gen-A1 [Tsunamis that can cause large coastal inundationare generated]effect because [the ocean?s watermass is displaced and, much like throwing a stoneinto a pond, waves are generated.
]causeA2 [Earthquake causes seismic waves which set upthe water in motion with a large force.
]causeThis causes [a tsunami.
]effectA3 [Tsunamis]effect are caused by [the sudden dis-placement of huge volumes of water.
]causeA4 [Tsunamis weaken as they pass throughforests]effect because [the hydraulic resistance ofthe trees diminish their energy.
]causeA5 [Automakers in Japan suspended production for anarray of vehicles]effect because [the magnitude 9earthquake and tsunami hit their country on Friday,March 11, 2011.
]causeTable 1: Examples of intra/inter-sentential causalrelations.
Cause and effect parts of each causal re-lation, marked with [..]cause and [..]effect, are con-nected by the underlined cue phrases for causality,such as because, this causes, and are caused by.erated,?
and its effect, ?Tsunamis ... are gener-ated.?
This is a good answer to the question, ?Whyare tsunamis generated?
?, since the effect part ismore or less equivalent to the (propositional) con-tent of the question.
Our method finds text frag-ments that include such causal relations with aneffect part that resembles a given question and pro-vides them as answers.Since this idea looks quite intuitive, many peo-ple would probably consider it as a solution towhy-QA.
However, to our surprise, we could notfind any previous work on why-QA that took thisapproach.
Some methods utilized the causal re-lations between terms as evidence for finding an-swers (i.e., matching a cause term with an answertext and its effect term with a question) (Girju,2003; Higashinaka and Isozaki, 2008).
Other ap-proaches utilized such clue terms for causality as?because?
as evidence for finding answers (Mu-rata et al, 2007).
However, these algorithms didnot check whether an answer candidate, i.e., a textfragment that may be provided as an answer, ex-plicitly contains a complex causal relation sen-1733tence with the effect part that resembles a ques-tion.
For example, A5 in Table 1 is an incorrect an-swer to ?Why are tsunamis generated?
?, but theseprevious approaches would probably choose it as aproper answer due to ?because?
and ?earthquake?
(i.e., a cause of tsunamis).
At least in our exper-imental setting, our approach outperformed thesesimpler causality-based QA systems.Perhaps this approach was previously deemedinfeasible due to two non-trivial technical chal-lenges.
The first challenge is to accurately iden-tify a wide range of causal relations like those inTable 1 in answer candidates.
To meet this chal-lenge, we developed a sequence labeling methodthat identifies not only intra-sentential causal re-lations, i.e., the causal relations between twoterms/phrases/clauses expressed in a single sen-tence (e.g., A1 in Table 1), but also the inter-sentential causal relations, which are the causalrelations between two terms/phrases/clauses ex-pressed in two adjacent sentences (e.g., A2) in agiven text fragment.The second challenge is assessing the appropri-ateness of each identified causal relation as an an-swer to a given question.
This is important sincethe causal relations identified in the answer candi-dates may have nothing to do with a given ques-tion.
In this case, we have to reject these causalrelations because they are inappropriate as an an-swer to the question.
When a single answer candi-date contains many causal relations, we also haveto select the appropriate ones.
Consider the causalrelations in A1?A4.
Those in A1?A3 are appro-priate answers to ?Why are tsunamis generated?
?,but not the one in A4.
To assess the appropri-ateness, the system must recognize textual entail-ment, i.e., ?tsunamis (are) generated?
in the ques-tion is entailed by all ?tsunamis are generated?
inA1, ?cause a tsunami?
in A2 and ?tsunamis arecaused?
in A3 but not by ?tsunamis weaken?
inA4.
This quite difficult task is currently beingstudied by many researchers in the RTE field (An-droutsopoulos and Malakasiotis, 2010; Dagan etal., 2010; Shima et al, 2011; Bentivogli et al,2011).
To meet this challenge, we developed arelatively simple method that can be seen as alightweight approximation for this difficult RTEtask, using excitation polarities (Hashimoto et al,2012).Through our experiments on Japanese why-QA,we show that a combination of the above methodscan improve why-QA accuracy.
In addition, ourproposed method can be successfully combinedwith other approaches to why-QA and can con-tribute to higher accuracy.
As a final result, we im-proved the precision by 4.4% against all the ques-tions in our test set over the current state-of-the-artsystem of Japanese why-QA (Oh et al, 2012).
Thedifference in the performance became much largerwhen we only compared the highly confident an-swers of each system.
When we made our sys-tem provide only its confident answers accordingto their confidence score given by our system, theprecision of these confident answers was 83.2%for 25% of all the questions in our test set.
In thesame setting, the precision of the state-of-the-artsystem (Oh et al, 2012) was only 62.4%.2 Related WorkAlthough there were many previous works on theacquisition of intra- and inter-sentential causal re-lations from texts (Khoo et al, 2000; Girju, 2003;Inui and Okumura, 2005; Chang and Choi, 2006;Torisawa, 2006; Blanco et al, 2008; De Saeger etal., 2009; De Saeger et al, 2011; Riaz and Girju,2010; Do et al, 2011; Radinsky et al, 2012), theirapplication to why-QA was limited to causal re-lations between terms (Girju, 2003; Higashinakaand Isozaki, 2008).As previous attempts to improve why-QA per-formance, such semantic knowledge as Word-Net synsets (Verberne et al, 2011), semanticword classes (Oh et al, 2012), sentiment analy-sis (Oh et al, 2012), and causal relations betweenterms (Girju, 2003; Higashinaka and Isozaki,2008) has been used.
These previous studies tookbasically bag-of-words approaches and used thesemantic knowledge to identify certain seman-tic associations using terms and n-grams.
Onthe other hand, our method explicitly identifiesintra- and inter-sentential causal relations betweenterms/phrases/clauses that have complex struc-tures and uses the identified relations to answera why-question.
In other words, our methodconsiders more complex linguistic structures thanthose used in the previous studies.
Note that ourmethod can complement the previous approaches.Through our experiments, we showed that it ispossible to achieve a higher precision by combin-ing our proposed method with bag-of-words ap-proaches considering semantic word classes andsentiment analysis in our previous work (Oh et al,1734Document?retrieval?from?Japanese?web?texts?Answer?candidate?extrac?onAnswer?candidate?extrac?on??from?the?retrieved?documents?Answer?re-??ranker?Answer?re-?
?rankingtop-n answer?candidates?by?answer?re-??ranking?Training?data?for?answer?re-?
?ranking?Training?data?for?causal?rela?on?recogni?on?Causal?rela?on?recogni?on?model?top-n answer?candidates?training?training?Why-??ques?on?recogni?on??recogni?on?
?Figure 1: System architecture2012).3 System ArchitectureWe first describe the system architecture ofour QA system before describing our proposedmethod.
It is composed of two components: an-swer candidate extraction and answer re-ranking(Fig.
1).
This architecture is basically the same asthat used in our previous work (Oh et al, 2012).We extended our previous work by introducingcausal relations recognized from answer candi-dates to the answer re-ranking.
The features usedin our previous work are very different from thosein this work, and we found that combining bothimproves accuracy.Answer candidate extraction: In our previouswork, we implemented the method of Murata etal.
(2007) for our answer candidate extractor.
Weretrieved documents from Japanese web texts us-ing Boolean AND and OR queries generated fromthe content words in why-questions.
Then we ex-tracted passages of five sentences from these re-trieved documents and ranked them with the rank-ing function proposed by Murata et al (2007).This method ranks a passage higher when it con-tains more query terms that are closer to each otherin the passage.
We used a set of clue terms, includ-ing the Japanese counterparts of cause and reason,as query terms for the ranking.
The top rankedpassages are regarded as answer candidates in theanswer re-ranking.
See Murata et al (2007) formore details.Answer re-ranking: Re-ranking the answercandidates is done by a supervised classifier(SVMs) (Vapnik, 1995).
In our previous work, weemployed three types of features for training there-ranker: morphosyntactic features (n-grams ofmorphemes and syntactic dependency chains), se-mantic word class features (semantic word classesobtained by automatic word clustering (Kazamaand Torisawa, 2008)) and sentiment polarity fea-tures (word and phrase polarities).
Here, we usedsemantic word classes and sentiment polarities foridentifying such semantic associations between awhy-question and its answer as ?if a disease?sname appears in a question, then answers that in-clude nutrient names are more likely to be correct?by semantic word classes and ?if something un-desirable happens, the reason is often also some-thing undesirable?
by sentiment polarities.
In thiswork, we propose causal relation features gener-ated from intra- and inter-sentential causal rela-tions in answer candidates and use them alongwith the features proposed in our previous workfor training our re-ranker.4 Causal Relations for Why-QAWe describe causal relation recognition in Sec-tion 4.1 and describe the features (of our re-ranker)generated from causal relations in Section 4.2.4.1 Causal Relation RecognitionWe restrict causal relations to those expressed bysuch cue phrases for causality as (the Japanesecounterparts of) because and as a result like inthe previous work (Khoo et al, 2000; Inui andOkumura, 2005) and recognize them in the fol-lowing two steps: extracting causal relation candi-dates and recognizing causal relations from thesecandidates.4.1.1 Extracting Causal Relation CandidatesWe identify cue phrases for causality in answercandidates using the regular expressions in Ta-ble 2.
Then, for each identified cue phrase, weextract three sentences as a causal relation candi-date, where one contains the cue phrase and theother two are the previous and next sentences inthe answer candidates.
When there is more thanone cue phrase in an answer candidate, we useall of them for extracting the causal relation can-didates, assuming that each of the cue phrases islinked to different causal relations.
We call a cuephrase used for extracting a causal relation candi-date a c-marker (causality marker) of the candi-date to distinguish it from the other cue phrases inthe same causal relation candidate.1735Regular expressions Examples(D|?)?
??
P?
??
(for),???
(for),????
(as a result),????
(for)??
??
(since or because of)??
(??|?)
????
(from the fact that),???
(by the fact that)(??|??)
C ???
(because),???
(It is be-cause)D?
RCT (P|C)+ ???
(the reason is), ???
(is the cause),??????
(fromthis reason)Table 2: Regular expressions for identifying cuephrases for causality.
D, P and C representdemonstratives (e.g., ??
(this) and ??
(that)),postpositions (including case markers such as ?
(nominative), ?
(genitive)), and copula (e.g., ??
(is) and ???
(is)) in Japanese, respectively.RCT, which represents Japanese terms meaningreason, cause, or thanks to, is defined as fol-lows: RCT = {??
(reason), ??
(cause), ??
(cause), ???
(cause), ???
(thanks to),??
(thanks to),??
(reason) }.4.1.2 Recognizing Causal RelationsNext, we recognize the spans of the cause and ef-fect parts of a causal relation linked to a c-marker.We regard this task as a sequence labeling problemand use Conditional Random Fields (CRFs) (Laf-ferty et al, 2001) as a machine learning frame-work.
In our task, CRFs take three sentencesof a causal relation candidate as input and gen-erate their cause-effect annotations with a set ofpossible cause-effect IOB labels, including Begin-Cause (B-C), Inside-Cause (I-C), Begin-Effect (B-E), Inside-Effect (I-E), and Outside (O).
Fig 2shows an example of such sequence labeling.
Al-though this example is about sequential labelingshown on English sentences for ease of explana-tion, it was actually done on Japanese sentences.We used the three types of feature sets in Table 3for training the CRFs, where j is in the range ofi?
4 ?
j ?
i+4 for current position i in a causalrelation candidate.Type FeaturesMorphological feature mj , mj+1j , posj , posj+1jSyntactic feature sj , sj+1j , bj , bj+1jC-marker feature (mj , cm), (mj+1j , cm)(sj , cm), (sj+1j , cm)Table 3: Features for training CRFs, wherexj+1j = xjxj+1Morphological features: mj and posj in Ta-ble 3 represent the jth morpheme and the POS tag.S1:?Earthquake?causes?seismic?waves?which?set?up?the?water?in?mo?on?with?a?large?force.?EOS?S2:?This?causes?a?tsunami.?EOS?S3:?EOA?S1?
Earthquake?
causes?
??
with?
a?
large?
force?
.?
EOS?IOB?
B-??C?
I-??C?
I-??C?
I-??C?
I-??C?
I-??C?
I-??C?
I-??C?
O?S2?
This?
causes?
a?
tsunami?
.?
EOS?IOB?
O?
O?
B-??E?
I-??E?
I-??E?
O?S3?
EOA?IOB?
O?CRFs?A?causal?rela?on?candidate?from?A2?Figure 2: Recognizing causal relations by se-quence labeling: Underlined text This causes rep-resents a c-marker, and EOS and EOA representend-of-sentence and end-of-answer candidates.???
???
????
????
????????
????
???
?????????water?
ice?
if?(it)?becomes?
its?volume?
because?(it)??increases?
an?iceberg?
water?
float?on?(water)?Subtree?informa?on?used?for?syntac?c?features??subtree?
subtree?
child?
child?
c-??marker?
subtree-??of-??parent?subtree-??of-??parent?parent?????????????[??????????????]cause???[?????????????]effect?(Because?[the?volume?of?the?water?increases?if?it?becomes?ice]cause,?[an?iceberg?floats?on?water]effect.)??????????root?c-?
?marker?node?Figure 3: Example of syntactic information relatedto a c-marker used for syntactic featuresWe use JUMAN1, a Japanese morphological ana-lyzer, for generating our morphological features.Syntactic features: The span of the causal rela-tions in a given causal relation candidate stronglydepends on the c-marker in the candidate.
Es-pecially for intra-sentential causal relations, theircause and effect parts often appear in the subtreesof the c-marker?s node or those of the c-marker?sparent node in a syntactic dependency tree struc-ture.
Fig.
3 shows an example that follows this ob-servation, where the c-marker node is representedin a hexagon and the other nodes are in a rectan-gle.
Note that each node in Fig.
3 is a word phrase(called a bunsetsu), which is the smallest unit ofsyntactic analysis in Japanese.
A bunsetsu is asyntactic constituent composed of a content wordand several function words such as postpositionsand case markers.
Syntactic dependency is repre-sented by an arrow in Fig.
3.
For example, thereis syntactic dependency from word phrase ?
?1 http://nlp.ist.i.kyoto-u.ac.jp/EN/index.php?JUMAN1736(water) to???
(if (it) becomes), i.e.,??
dep??????.
We encode this subtree information intosj , which is the syntactic information of a wordphrase to which the jth morpheme belongs.
sjonly has one of six values: 1) the c-marker?s node(c-marker), 2) the c-marker?s child node (child),3) the c-marker?s parent node (parent), 4) in the c-marker?s subtree but not the c-marker?s child node(subtree), 5) in the subtree of the c-marker?s par-ent node but not the c-marker?s node (subtree-of-parent) and 6) the others (others).
bj is the wordphrase information of the jth morpheme (mj) thatrepresents whether mj is in the beginning or in-side a word phrase.
For generating our syntacticfeatures, we use KNP2, a Japanese syntactic de-pendency parser.C-marker features: As our c-marker features,we use a pair composed of c-marker cm and oneof the following: mj , mj+1j , sj , or sj+1j .4.2 Causal Relation FeaturesWe use terms, partial trees (in a syntactic depen-dency tree structure), and the semantic orienta-tion of excitation (Hashimoto et al, 2012) to as-sess the appropriateness of each causal relation ob-tained by our causal relation recognizer as an an-swer to a given question.
Finding answers withterm matching and partial tree matching has beenused in the literature of question answering (Girju,2003; Narayanan and Harabagiu, 2004; Moschittiet al, 2007; Higashinaka and Isozaki, 2008; Ver-berne et al, 2008; Surdeanu et al, 2011; Verberneet al, 2011; Oh et al, 2012), while that with theexcitation polarity is proposed in this work.We use three types of features.
Each fea-ture type expresses the causal relations in an an-swer candidate that are determined to be appro-priate as answers to a given question by termmatching (tf1?tf4), partial tree matching (pf1?pf4) and excitation polarity matching (ef1?ef4).We call these causal relations used for generatingour causal relation features candidates of an ap-propriate causal relation in this section.
Note thatif one answer candidate has more than one candi-date of an appropriate causal relation found by onematching method, we generated features for eachappropriate candidate and merged all of them forthe answer candidate.2 http://nlp.ist.i.kyoto-u.ac.jp/EN/index.php?KNPType Descriptiontf1 word n-grams of causal relationstf2 word class version of tf1tf3 indicator for the existence of candidates of anappropriate causal relation identified by termmatching in an answer candidatetf4 number of matched terms in candidates of an ap-propriate causal relationpf1 syntactic dependency n-grams (n dependencychain) of causal relationspf2 word class version of pf1pf3 indicator for the existence of candidates of an ap-propriate causal relation identified by partial treematching in an answer candidatepf4 number of matched partial trees in candidates ofan appropriate causal relationef1 types of noun-polarity pairs shared by causal re-lations and the questionef2 ef1 coupled with each noun?s word classef3 indicator for the existence of candidates of an ap-propriate causal relation identified by excitationpolarity matching in an answer candidateef4 number of noun-polarity pairs shared by thequestion and the candidates of an appropriatecausal relationTable 4: Causal relation features: n in n-gramsis n = {2, 3} and n-grams in an effect part aredistinguished from those in a cause part.4.2.1 Term MatchingOur term matching method judges that a causal re-lation is a candidate of an appropriate causal rela-tion if its effect part contains at least one contentword (nouns, verbs, and adjectives) in the ques-tion.
For example, all the causal relations of A1?A4 in Table 1 are candidates of an appropriatecausal relation to the question, ?Why is a tsunamigenerated?
?, by term matching with question termtsunami.tf1?tf4 are generated from candidates of an ap-propriate causal relation identified by term match-ing.
The n-grams of tf1 and tf2 are restrictedto those containing at least one content word ina question.
We distinguish this matched wordfrom the other words by replacing it with QW, aspecial symbol representing a word in the ques-tion.
For example, word 3-gram ?this/cause/QW?is extracted from This causes tsunamis in A2 for?Why is a tsunami generated??
Further, we cre-ate a word class version of word n-grams by con-verting the words in these word n-grams into theircorresponding word class using the semantic wordclasses (500 classes for 5.5 million nouns) fromour previous work (Oh et al, 2012).
These wordclasses were created by applying the automaticword clustering method of Kazama and Torisawa(2008) to 600 million Japanese web pages.
Forexample, the word class version of word 3-gram1737?this/cause/QW?
is ?this/cause/QW,WCtsunami?,where WCtsunami represents the word class ofa tsunami.
tf3 is a binary feature that indi-cates the existence of candidates of an appropri-ate causal relation identified by term matching inan answer candidate.
tf4 represents the degreeof the relevance of the candidates of an appro-priate causal relation measured by the number ofmatched terms: one, two, and more than two.4.2.2 Partial Tree MatchingOur partial tree matching method judges a causalrelation as a candidate of an appropriate causal re-lation if its effect part contains at least one par-tial tree in a question, where the partial tree coversmore than one content word.
For example, onlythe causal relation A1 among A1?A4 is a can-didate of an appropriate causal relation for ques-tion ?Why are tsunamis generated??
by partialtree matching because only its effect part containspartial tree ?tsunamis dep???
(are) generated?
of thequestion.pf1?pf4 are generated from candidates of anappropriate causal relation identified by the par-tial tree matching.
The syntactic dependency n-grams in pf1 and pf2 are restricted to those thatcontain at least one content word in a question.
Wedistinguish this matched content word from theother content words in the n-gram by convertingit to QW, which represents a content word in thequestion.
For example, syntactic dependency 2-gram ?QW dep???
cause?
and its word class version?QW,WCtsunami dep???
cause?
are extracted fromTsunamis that can cause in A1.
pf3 is a binaryfeature that indicates whether an answer candidatecontains candidates of an appropriate causal rela-tion identified by partial tree matching.
pf4 rep-resents the degree of the relevance of the candi-date of an appropriate causal relation measured bythe number of matched partial trees: one, two, andmore than two.4.2.3 Excitation Polarity MatchingHashimoto et al (2012) proposed a semantic ori-entation called excitation polarities.
It classifiespredicates with their argument position (calledtemplates) into excitatory, inhibitory and neu-tral.
In the following, we denote a templateas ?
[argument position,predicate].?
According toHashimoto?s definition, excitatory templates im-ply that the function, effect, purpose, or the role ofan entity filling an argument position in the tem-plates is activated/enhanced.
On the contrary, in-hibitory templates imply that the effect, purposeor the role of an entity is deactivated/suppressed.Neutral templates are those that neither activatenor suppress the function of an argument.We assume that the meanings of a text canbe roughly captured by checking whether eachnoun in the text is activated or suppressed in thesense of the excitation polarity framework, wherethe activation and suppression of each entity (ornoun) can be detected by looking at the excita-tion polarities of the templates that are filled bythe entity.
For instance, effect part ?tsunamisthat can cause large coastal inundation are gen-erated?
of A1 roughly means that ?tsunamis?
areactivated and ?inundation?
is (or can be) acti-vated.
This activation/suppression configurationof the nouns is consistent with sentence ?tsunamisare caused?
in which ?tsunamis?
are activated.This consistency suggests that A1 is a good an-swer to question ?Why are tsunamis caused?
?, al-though the ?tsunamis?
are modified by differentpredicates; ?cause?
and ?generate.?
On the otherhand, effect part ?tsunamis weaken as they passthrough forests?
of A4 implies that ?tsunamis?are suppressed.
This suggests that A4 is nota good answer to ?Why are tsunamis caused?
?Note that the consistency checking between ac-tivation/suppression configurations of nouns3 intexts can be seen as a rough but lightweight ap-proximation of the recognition of textual entail-ments or paraphrases.Following the definition of excitation polarityin Hashimoto et al (2012), we manually classi-fied templates4 to each polarity type and obtained8,464 excitatory templates, such as [?, ???
]([subject, increase]) and [?, ????]
([sub-ject, improve]), 2,262 inhibitory templates, suchas [?, ??]
([object, prevent]) and [?, ??
]([subject, die]), and 7,230 neutral templates suchas [?, ???]
([object, consider]).
With thesetemplates, we obtain activation/suppression con-figurations (including neutral) for the nouns in thecausal relations in the answer candidates and ques-3 Because the activation/suppression configurations ofnouns come from an excitation polarity of templates, ?
[argu-ment position,predicate],?
the semantics of verbs in the tem-plates are implicitly considered in this consistency checking.4 Varga et al (2013) has used the same templates as ours,except they restricted their excitation/inhibitory templates tothose whose polarity is consistent with that given by the au-tomatic acquisition method of Hashimoto et al (2012).1738tions.Next, we assume that a causal relation is ap-propriate as an answer to a question if the effectpart of the causal relation and the question shareat least one common noun with the same polarity.More detailed information concerning the config-urations of all the nouns in all the candidates of anappropriate causal relation (including their causeparts) and the question are encoded into our fea-ture set ef1?ef4 in Table 4 and the final judgmentis done by our re-ranker.For generating ef1 and ef2, we classified all thenouns coupled with activation/suppression/neutralpolarities in a causal relation into three types:SAME (the question contains the same noun withthe same polarity), DiffPOL (the question con-tains the same noun with different polarity), andOTHER (the others).
ef1 indicates whether eachtype of noun-polarity pair exists in a causal rela-tion.
Note that the types for the effect and causeparts are represented in distinct features.
ef2 is thesame as ef1 except that the types are augmentedwith the word classes of the corresponding nouns.In other words, ef2 indicates whether each typeof noun-polarity pair exists in the causal relationfor each word class.
ef3 indicates the existence ofcandidates of an appropriate causal relation iden-tified by this matching scheme, and ef4 repre-sents the number of noun-polarity pairs shared bythe question and the candidates of an appropriatecausal relations (one, two, and more than two).5 ExperimentsWe experimented with causal relation recognitionand why-QA with our causal relation features.5.1 Data Set for Why-Question AnsweringFor our experiments, we used the same why-QAdata set as the one used in our previous work (Ohet al, 2012).
This why-QA data set is composedof 850 Japanese why-questions and their top-20answer candidates obtained by answer candidateextraction from 600 million Japanese web pages.Three annotators checked the top-20 answer can-didates of these 850 questions and the final judg-ment was made by their majority vote.
Their inter-rater agreement by Fleiss?
kappa reported in Oh etal.
(2012) was substantial (?
= 0.634).
Among the850 questions, 250 why-questions were extractedfrom the Japanese version of Yahoo!
Answers,and another 250 were created by annotators.
Inour previous work, we evaluated the system withthese 500 questions and their answer candidates astraining and test data in 10-fold cross-validation.The other 350 why-questions were manually builtfrom passages describing the causes or reasons ofevents/phenomena.
These questions and their an-swer candidates were used as additional trainingdata for testing subsamples in each fold during the10-fold cross-validation.
In our why-QA experi-ments, we evaluated our why-QA system with thesame settings.5.2 Data Set for Causal Relation RecognitionWe built a data set composed of manually anno-tated causal relations for evaluating our causal re-lation recognition.
As source data for this data set,we used the same 10-fold data that we used forevaluating our why-QA (500 questions and theiranswer candidates).
We extracted the causal re-lation candidates from the answer candidates ineach fold, and then our annotator (not an author)manually marked the span of the cause and effectparts of a causal relation for each causal relationcandidate, keeping in mind that the causal rela-tion must be expressed in terms of a c-marker ina given causal relation candidate.
Finally, we hada data set made of 16,051 causal relation candi-dates, 8,117 of which had a true causal relation;the number of intra- and inter-sentential causal re-lations were 7,120 and 997, respectively.Note that this data set can be partitioned into tenfolds by using the 10-fold partition of its sourcedata.
We performed 10-fold cross validation toevaluate our causal relation recognition with this10-fold data.5.3 Causal Relation RecognitionWe used CRF++5 for training our causal relationrecognizer.
In our evaluation, we judged a sys-tem?s output as correct if both spans of the causeand effect parts overlapped those in the gold stan-dard.
Evaluation was done by precision, recall,and F1.Precision Recall F1BASELINE 41.9 61.0 49.7INTRA-SENT 84.5 75.4 79.7INTER-SENT 80.2 52.6 63.6ALL 83.8 71.1 77.0Table 5: Results of causal relation recognition (%)Table 5 shows the result.
BASELINE represents5 http://code.google.com/p/crfpp/1739the result for our baseline system that recognizesa causal relation by simply taking the two phrasesadjacent to a c-marker (i.e., before and after) ascause and effect parts of the causal relation.
Weassumed that the system had an oracle for judgingcorrectly whether each phrase is a cause part or aneffect part.
In other words, we judged that a causalrelation recognized by BASELINE is correct if bothcause and effect parts in the gold standard are adja-cent to a c-marker.
INTRA-SENT and INTER-SENTrepresent the results for intra- and inter-sententialcausal relations and ALL represents the result forthe both causal relations by our method.
Fromthese results, we confirmed that our method rec-ognized both intra- and inter-sentential causal rela-tions with over 80% precision, and it significantlyoutperformed our baseline system in both preci-sion and recall rates.Precision Recall F1ALL-?MORPH?
80.8 66.4 72.9ALL-?SYNTACTIC?
82.9 67.0 74.1ALL-?C-MARKER?
76.3 51.4 61.4ALL 83.8 71.1 77.0Table 6: Ablation test results for causal relationrecognition (%)We also investigated the contribution of thethree types of features used in our causal rela-tion recognition to the performance.
We evalu-ated the performance when we removed one ofthe three types of features (ALL-?MORPH?, ALL-?SYNTACTIC?
and ALL-?C-MARKER?)
and com-pared the results in these settings with the onewhen all the feature sets were used (ALL).
Ta-ble 6 shows the result.
We confirmed that all thefeature sets improved the performance, and we gotthe best performance when using all of them.
Weused the causal relations obtained from the 10-foldcross validation for our why-QA experiments.5.4 Why-Question AnsweringWe performed why-QA experiments to confirmthe effectiveness of intra- and inter-sententialcausal relations in a why-QA task.
Inthis experiment, we compared five systems:four baseline systems (MURATA, OURCF, OHand OH+PREVCF) and our proposed method(PROPOSED).MURATA corresponds to our answer candidateextraction.OURCF uses a re-ranker trained with only ourcausal relation features.OH, which represents our previous work (Oh etal., 2012), has a re-ranker trained with mor-phosyntactic, semantic word class, and senti-ment polarity features.OH+PREVCF is a system with a re-rankertrained with the features used in OH and withthe causal relation feature proposed in Hi-gashinaka and Isozaki (2008).
The causal re-lation feature includes an indicator that deter-mines whether the causal relations betweentwo terms appear in a question-answer pair;cause in an answer and its effect in a question.We acquired the causal relation instances (be-tween terms) from 600 million Japanese webpages using the method of De Saeger et al(2009) and exploited the top-100,000 causalrelation instances in this system.PROPOSED has a re-ranker trained with ourcausal relation features as well as the threetypes of features proposed in Oh et al (2012).Comparison between OH and PROPOSED re-veals the contribution of our causal relationfeatures to why-QA.We used TinySVM6 with a linear kernelfor training the re-rankers in OURCF, OH,OH+PREVCF and PROPOSED.
Evaluation wasdone by P@1 (Precision of the top-answer) andMean Average Precision (MAP); they are the samemeasures used in Oh et al (2012).
P@1 measureshow many questions have a correct top-answercandidate.
MAP measures the overall quality ofthe top-20 answer candidates.
As mentioned inSection 5.1, we used 10-fold cross-validation withthe same setting as the one used in Oh et al (2012)for our experiments.P@1 MAPMURATA 22.2 27.0OURCF 27.8 31.4OH 37.4 39.1OH+PREVCF 37.4 38.9PROPOSED 41.8 41.0Table 7: Why-QA results (%)Table 7 shows the evaluation results.
Our pro-posed method outperformed the other four sys-tems and improved P@1 by 4.4% over OH, whichis the-state-of-the-art system for Japanese why-6 http://chasen.org/?taku/software/TinySVM/1740QA.
OURCF showed the performance improve-ment over MURATA.
Although this suggests theeffectiveness of our causal relation features, theoverall performance of OURCF was lower thanthat of OH.
OH+PREVCF outperformed neitherOH nor PROPOSED.
This suggests that our ap-proach is more effective than previous causality-based approaches (Girju, 2003; Higashinaka andIsozaki, 2008), at least in our setting.203040506070809010010  20  30  40  50  60  70  80  90  100Precision(%)% of questionsPROPOSEDOHOurCFFigure 4: Effect of causal relation features on thetop-answersWe also compared confident answers ofOURCF, OH, and PROPOSED by making each sys-tem provide only the k confident top-answers (fork questions) selected by their SVM scores givenby each system?s re-ranker.
This reduces the num-ber of questions that can be answered by a system,but the top-answers become more reliable as k de-creases.
Fig.
4 shows this result, where the x axisrepresents the percentage of questions (against allthe questions in our test set) whose top-answersare given by each system, and the y axis repre-sents the precision of the top-answers at a certainpoint on the x axis.
When both systems providedtop-answers for 25% of all the questions in our testset, our method achieved 83.2% precision, whichis much higher than OH?s (62.4%).
This exper-iment confirmed that our causal relation featureswere also effective in improving the quality of thehighly confident answers.However, the high precision by our method wasbound to confident answers for a small numberof questions, and the difference in the precisionbetween OH and PROPOSED in Fig.
4 becamesmaller as we considered more answers with lowerconfidence.
We think that one of the reasons is therelatively small coverage of the excitation polaritylexicon, a core resource in our excitation polaritymatching.
We are planning to enlarge the lexiconto deal with this problem.Next, we investigated the contribution of theintra- and inter-sentential causal relations to theperformance of our method.
We used only oneof the two types of causal relations for generatingcausal relation features (INTRA-SENT and INTER-SENT) for training our re-ranker and compared theresults in these settings with the one when bothwere used (ALL (PROPOSED)).
Table 8 showsthe result.
Both intra- and inter-sentential causalrelations contributed to the performance improve-ment.P@1 MAPINTER-SENT 39.0 39.7INTRA-SENT 40.4 40.5ALL (PROPOSED) 41.8 41.0Table 8: Results with/without intra- and inter-sentential causal relations (%)We also investigated the contributions of thethree types of causal relation features by ablationtests (Table 9).
When we do not use the fea-tures by excitation polarity matching (ALL-{ef1?ef4}), the performance is the worst.
This impliesthat the contribution of excitation polarity match-ing exceeds the other two.P@1 MAPALL-{tf1?tf4} 40.8 40.7ALL-{pf1?pf4} 41.0 40.9ALL-{ef1?ef4} 39.6 40.5ALL (PROPOSED) 41.8 41.0Table 9: Ablation test results for why-QA (%)6 ConclusionIn this paper, we explored the utility of intra- andinter-sentential causal relations for ranking answercandidates to why-questions.
We also proposed amethod for assessing the appropriateness of causalrelations as answers to a given question using thesemantic orientation of excitation.
Through ex-periments, we confirmed that these ideas are ef-fective for improving why-QA, and our proposedmethod achieved 41.8% P@1, which is 4.4% im-provement over the current state-of-the-art systemof Japanese why-QA.
We also showed that oursystem achieved 83.2% precision for its confidentanswers, when it only provided its confident an-swers for 25% of all the questions in our test set.1741ReferencesIon Androutsopoulos and Prodromos Malakasiotis.2010.
A survey of paraphrasing and textual entail-ment methods.
Journal of Artificial Intelligence Re-search (JAIR), 38(1):135?187.Luisa Bentivogli, Peter Clark, Ido Dagan, Hoa Dang,and Danilo Giampiccolo.
2011.
The seventh pascalrecognizing textual entailment challenge.
In Pro-ceedings of TAC.E.
Blanco, N. Castell, and Dan I. Moldovan.
2008.Causal relation extraction.
In Proceedings ofLREC?08.Du-Seong Chang and Key-Sun Choi.
2006.
Incremen-tal cue phrase learning and bootstrapping method forcausality extraction using cue phrase and word pairprobabilities.
Information Processing and Manage-ment, 42(3):662?678.Ido Dagan, Bill Dolan, Bernardo Magnini, and DanRoth.
2010.
Recognizing textual entailment: Ratio-nal, evaluation and approaches.
Natural LanguageEngineering, 16(1):1?17.Stijn De Saeger, Kentaro Torisawa, Jun?ichi Kazama,Kow Kuroda, and Masaki Murata.
2009.
Largescale relation acquisition using class dependent pat-terns.
In Proceedings of ICDM ?09, pages 764?769.Stijn De Saeger, Kentaro Torisawa, Masaaki Tsuchida,Jun?ichi Kazama, Chikara Hashimoto, Ichiro Ya-mada, Jong Hoon Oh, Istv?n Varga, and Yulan Yan.2011.
Relation acquisition using word classes andpartial patterns.
In Proceedings of EMNLP ?11,pages 825?835.Quang Xuan Do, Yee Seng Chan, and Dan Roth.
2011.Minimally supervised event causality identification.In Proceedings of EMNLP ?11, pages 294?303.David A. Ferrucci, Eric W. Brown, Jennifer Chu-Carroll, James Fan, David Gondek, Aditya Kalyan-pur, Adam Lally, J. William Murdock, Eric Nyberg,John M. Prager, Nico Schlaefer, and Christopher A.Welty.
2010.
Building Watson: An overview of theDeepQA project.
AI Magazine, 31(3):59?79.Roxana Girju.
2003.
Automatic detection of causalrelations for question answering.
In Proceedings ofthe ACL 2003 workshop on Multilingual summariza-tion and question answering, pages 76?83.Chikara Hashimoto, Kentaro Torisawa, Stijn DeSaeger, Jong-Hoon Oh, and Jun?ichi Kazama.
2012.Excitatory or inhibitory: A new semantic orientationextracts contradiction and causality from the web.
InProceedings of EMNLP-CoNLL ?12.Ryuichiro Higashinaka and Hideki Isozaki.
2008.Corpus-based question answering for why-questions.
In Proceedings of IJCNLP ?08, pages418?425.Takashi Inui and Manabu Okumura.
2005.
Investigat-ing the characteristics of causal relations in Japanesetext.
In In Annual Meeting of the Associationfor Computational Linguistics (ACL) Workshop onFrontiers in Corpus Annotations II: Pie in the Sky.Jun?ichi Kazama and Kentaro Torisawa.
2008.
In-ducing gazetteers for named entity recognition bylarge-scale clustering of dependency relations.
InProceedings of ACL-08: HLT, pages 407?415.Christopher S. G. Khoo, Syin Chan, and Yun Niu.2000.
Extracting causal knowledge from a medicaldatabase using graphical patterns.
In Proceedings ofACL ?00, pages 336?343.John Lafferty, Andrew McCallum, and FernandoPereira.
2001.
Conditional random fields: Prob-abilistic models for segmenting and labeling se-quence data.
In Proceedings of ICML ?01, pages282?289.Alessandro Moschitti, Silvia Quarteroni, RobertoBasili, and Suresh Manandhar.
2007.
Exploitingsyntactic and shallow semantic kernels for questionanswer classification.
In Proceedings of ACL ?07,pages 776?783.Masaki Murata, Sachiyo Tsukawaki, Toshiyuki Kana-maru, Qing Ma, and Hitoshi Isahara.
2007.
A sys-tem for answering non-factoid Japanese questionsby using passage retrieval weighted based on typeof answer.
In Proceedings of NTCIR-6.Srini Narayanan and Sanda Harabagiu.
2004.
Ques-tion answering based on semantic structures.
In Pro-ceedings of COLING ?04, pages 693?701.Jong-Hoon Oh, Kentaro Torisawa, Chikara Hashimoto,Takuya Kawada, Stijn De Saeger, Jun?ichi Kazama,and Yiou Wang.
2012.
Why question answeringusing sentiment analysis and word classes.
In Pro-ceedings of EMNLP-CoNLL ?12, pages 368?378.Kira Radinsky, Sagie Davidovich, and ShaulMarkovitch.
2012.
Learning causality fornews events prediction.
In Proceedings of WWW?12, pages 909?918.Mehwish Riaz and Roxana Girju.
2010.
Another lookat causality: Discovering scenario-specific contin-gency relationships with no supervision.
In ICSC?10, pages 361?368.Hideki Shima, Hiroshi Kanayama, Cheng wei Lee,Chuan jie Lin, Teruko Mitamura, Yusuke Miyao,Shuming Shi, and Koichi Takeda.
2011.
Overviewof NTCIR-9 RITE: Recognizing Inference in TExt.In Proceedings of NTCIR-9.Mihai Surdeanu, Massimiliano Ciaramita, and HugoZaragoza.
2011.
Learning to rank answers to non-factoid questions from web collections.
Computa-tional Linguistics, 37(2):351?383.1742Kentaro Torisawa.
2006.
Acquiring inference ruleswith temporal constraints by using japanese coordi-nated sentences and noun-verb co-occurrences.
InProceedings of HLT-NAACL ?06, pages 57?64.Vladimir N. Vapnik.
1995.
The nature of statisticallearning theory.
Springer-Verlag New York, Inc.,New York, NY, USA.Istvan Varga, Motoki Sano, Kentaro Torisawa, ChikaraHashimoto, Kiyonori Ohtake, Takao Kawai, Jong-Hoon Oh, and Stijn De Saeger.
2013.
Aid is outthere: Looking for help from tweets during a largescale disaster.
In Proceedings of ACL ?13.Suzan Verberne, Lou Boves, Nelleke Oostdijk, andPeter-Arno Coppen.
2008.
Using syntactic infor-mation for improving why-question answering.
InProceedings of COLING ?08, pages 953?960.Suzan Verberne, Lou Boves, and Wessel Kraaij.
2011.Bringing why-qa to web search.
In Proceedings ofECIR ?11, pages 491?496.1743
