Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational NaturalLanguage Learning, pages 984?994, Jeju Island, Korea, 12?14 July 2012. c?2012 Association for Computational LinguisticsProbabilistic Finite State Machines for Regression-based MT EvaluationMengqiu Wang and Christopher D. ManningComputer Science DepartmentStanford UniversityStanford, CA 94305 USA{mengqiu,manning}@cs.stanford.eduAbstractAccurate and robust metrics for automatic eval-uation are key to the development of statisticalmachine translation (MT) systems.
We firstintroduce a new regression model that uses aprobabilistic finite state machine (pFSM) tocompute weighted edit distance as predictionsof translation quality.
We also propose a novelpushdown automaton extension of the pFSMmodel for modeling word swapping and crossalignments that cannot be captured by stan-dard edit distance models.
Our models can eas-ily incorporate a rich set of linguistic features,and automatically learn their weights, elimi-nating the need for ad-hoc parameter tuning.Our methods achieve state-of-the-art correla-tion with human judgments on two differentprediction tasks across a diverse set of standardevaluations (NIST OpenMT06,08; WMT06-08).1 IntroductionResearch in automatic machine translation (MT) eval-uation metrics has been a key driving force behindthe recent advances of statistical machine transla-tion (SMT) systems.
The early seminal work onautomatic MT metrics (e.g., BLEU and NIST) islargely based on n-gram matches (Papineni et al2002; Doddington, 2002).
Despite their simplicity,these measures have shown good correlation with hu-man judgments, and enabled large-scale evaluationsacross many different MT systems, without incurringthe huge labor cost of human evaluation (Callison-Burch et al(2009; 2010; 2011), inter alia).
Recentstudies have also confirmed that tuning MT systemsagainst better MT metrics ?
using algorithms likeMERT (Och, 2003) ?
leads to better system perfor-mance (He and Way, 2009; Liu et al 2011).Later metrics that move beyond n-grams achievehigher accuracy and improved robustness from re-sources like WordNet synonyms (Miller et al 1990),and paraphrasing (Snover et al 2009; Denkowskiand Lavie, 2010).
But a common problem in thesemetrics is they typically resort to ad-hoc tuningmethods instead of principled approaches to incor-porate linguistic features.
Recent models use linearor SVM regression and train them against humanjudgments to automatic learn feature weights, andhave shown state-of-the-art correlation with humanjudgments (Kulesza and Shieber, 2004; Albrecht andHwa, 2007a; Albrecht and Hwa, 2007b; Sun et al2008; Pado et al 2009).
The drawback, however,is they rely on time-consuming preprocessing mod-ules to extract linguistic features (e.g., a full end-to-end textual entailment system was needed in Pado etal.
(2009)), which severely limits their practical use.Furthermore, these models employ a large numberof features (on the order of hundreds), and conse-quently make the model predictions opaque and hardto analyze.In this paper, we propose a simple yet powerfulprobabilistic Finite State Machine (pFSM) for thetask of MT evaluation.
It is built on the backbone ofweighted edit distance models, but learns to weightedit operations in a probabilistic regression frame-work.
One of the major contributions of this pa-per is a novel extension of the pFSM model into aprobabilistic Pushdown Automaton (pPDA), whichenhances traditional edit-distance models with theability to model phrase shift and word swapping.
Fur-thermore, we give a new log-linear parameterizationto the pFSM model, which allows it to easily incor-984porate rich linguistic features.
We experiment with aset of simple features based on labeled head-modifierdependency structure, in order to test the hypothesisthat modeling overall sentence structure can lead tomore accurate evaluation measures.We conducted extensive experiments on a di-verse set of standard evaluation data sets (NISTOpenMT06, 08; WMT06, 07, 08).
Our modelachieves or surpasses state-of-the-art results on alltest sets.2 pFSMs for MT RegressionWe start off by framing the problem of machine trans-lation evaluation in terms of weighted edit distancescalculated using probabilistic finite state machines(pFSMs).
A FSM defines a language by acceptinga string of input tokens in the language, and reject-ing those that are not.
A probabilistic FSM definesthe probability that a string is in a language, extend-ing on the concept of a FSM.
Commonly used mod-els such as HMMs, n-gram models, Markov Chainsand probabilistic finite state transducers all fall inthe broad family of pFSMs (Knight and Al-Onaizan,1998; Eisner, 2002; Kumar and Byrne, 2003; Vidalet al 2005).
Unlike all the other applications ofFSMs where tokens in the language are words, inour language tokens are edit operations.
A string oftokens that our pFSM accepts is an edit sequence thattransforms a reference translation (denoted as ref )into a system translation (sys).Our pFSM has a unique start and stop state, andone state per edit operation (i.e., Insert, Delete, Sub-stitution).
The probability of an edit sequence e isgenerated by the model is the product of the state tran-sition probabilities in the pFSM, formally describedas:w(e | s,r) =?|e|k=1 exp ?
?
f(ek?1,ek,s,r)Z(1)We featurize each of the state changes with a log-linear parameterization; f is a set of binary featurefunctions defined over pairs of neighboring states(by the Markov assumption) and the input sentences,and ?
are the associated feature weights; r and s areshorthand for ref and sys; Z is a partition function.In this basic pFSM model, the feature functions aresimply identity functions that emit the current state,and the state transition sequence of the previous stateand the current state.The feature weights are then automatically learnedby training a global regression model where sometranslational equivalence judgment score (e.g., hu-man assessment score, or HTER (Snover et al2006)) for each sys and ref translation pair is theregression target (y?).
We introduce a new regressionvariable y ?
R which is the log-sum of the unnormal-ized weights (Eqn.
(1)) of all edit sequences, formallyexpressed as:y = log ?e?
?e?|e?|?k=1exp ?
?
f(ek?1,ek,s,r) (2)e?
denotes a valid edit sequence.
Since the ?gold?edit sequence are not given at training or predictiontime, we treat the edit sequences as hidden variablesand sum them out.
The sum over an exponentialnumber of edit sequences in e?
is solved efficientlyusing a forward-backward style dynamic program.Any edit sequence that does not lead to a completetransformation of the translation pair has a probabilityof zero in our model.
Our regression target then seeksto minimize the least squares error with respect to y?,plus a L2-norm regularizer term parameterized by ?
:?
?
= min?
{?si,ri[y?i ?
(yi|si|+ |ri|+?
)]2 +???
?2}(3)The |si|+ |ri| is a length normalization term for theith training instance, and ?
is a scaling constant foradjusting to different scoring standards (e.g., 7-pointscale vs. 5-point scale), whose value is automaticallylearned.
At test time, y/(|s|+ |r|)+?
is computedas the predicted score.We replaced the standard substitution edit opera-tion with three new operations: Sword for same wordsubstitution, Slemma for same lemma substitution, andSpunc for same punctuation substitution.
In otherwords, all but the three matching-based substitutionsare disallowed.
The start state can transition into anyof the edit states with a constant unit cost, and eachedit state can transition into any other edit state ifand only if the edit operation involved is valid at thecurrent edit position (e.g., the model cannot transi-tion into Delete state if it is already at the end of ref ;similarly it cannot transition into Slemma unless the985Figure 1: This diagram illustrates an example translation pair in the Chinese-English portion of OpenMT08 data set(Doc:AFP CMN 20070703.0005, system09, sent 1).
The three rows below are the best state transition sequencesaccording to the three proposed models.
The corresponding alignments generated by the models (pFSM, pPDA,pPDA+f ) are shown with different styled lines, with later models in the order generating strictly more alignments thanearlier ones.
The gold human evaluation score is 6.5 (on a 7-point scale), and model predictions are: pPDA+f 5.5, pPDA4.3, pFSM 3.1, METEORR 3.2, TERR 2.8.lemma of the two words under edit in sys and refmatch).
When the end of both sentences are reached,the model transitions into the stop state and endsthe edit sequence.
The first row in Figure 1 startingwith pFSM shows a state transition sequence for anexample sys/ref translation pair.
1 There exists a one-to-one correspondence between substitution edits andword alignments.
Therefore this example state tran-sition sequence correctly generates an alignment forthe word 43 and people.It is helpful to compare with the TER met-ric (Snover et al 2006), which is based on the ideaof word error rate measured in terms of edit distance,to better understand the intuition behind our model.There are two major improvements in our model: 1)the edit operations in our model are weighted, asdefined by the feature functions and weights; 2) theweights are automatically learned, instead of beinguniform or manually set; and 3) we model state transi-tions, which can be understood as a bigram extensionof the unigram edit distance model used in TER.
Forexample, if in our learned model the feature for twoconsecutive Sword states has a positive weight, thenour model would favor consecutive same word sub-1It is safe to ignore the second and third row in Figure 1 fornow, their explanations are forthcoming in Section 2.2.stitutions, whereas in the TER model the order ofthe substitution does not matter.
The extended TER-plus (Snover et al 2009) metric addresses the firstproblem but not the other two.2.1 Soft-max InterpretationThere is also an alternative interpretation of the modelas a simple soft-max approximation that is very intu-itive and easy to understand.
For ease of illustration,we introduce a quantity Q(e | s,r) to be the score ofan edit sequence, defined simply as the sum of thedot product of feature values and feature weights:Q(e | s,r) =|e|?i=1?
?
f(ei?1,ei,s,r)For the regression task, the intuition is that we wanty to take on the score (Q) of the best edit sequence:y = maxe?e?Q(e | s,r)But since the max function is non-differentiable, wereplace it with a softmax:y = log ?e?e?exp?
??
?softmaxQ(e | s,r)Substituting in Q, we arrive at the same objectivefunction as (2).9862.2 Restricted pPDA ExtensionA shortcoming of edit distance models is that theycannot handle long-distance word swapping ?
apervasive phenomenon found in most natural lan-guages.
2 Edit operations in standard edit distancemodels need to obey strict incremental order in theiredit position, in order to admit efficient dynamic pro-gramming solutions.
The same limitation is sharedby our pFSM model, where the Markov assumptionis made based on the incremental order of edit po-sitions.
Although there is no known solution to thegeneral problem of computing edit distance wherelong-distance swapping is permitted (Dombb et al2010), approximate algorithms do exist.
We presenta simple but novel extension of the pFSM model to arestricted probabilistic pushdown automaton (pPDA),to capture non-nested word swapping within limiteddistance, which covers a majority of word swappingin observed in real data (Wu, 2010).A pPDA, in its simplest form, is a pFSM whereeach control state is equipped with a stack (Esparzaand Kucera, 2005).
The addition of stacks for eachtransition state endows the machine with memory,extending its expressiveness beyond that of context-free formalisms.
By construction, at any stage in anormal edit sequence, the pPDA model can ?jump?forward within a fixed distance (controlled by a maxdistance parameter) to a new edit position on eitherside of the sentence pair, and start a new edit subse-quence from there.
Assuming the jump was made onthe sys side, 3 the machine remembers its current editposition in sys as Jstart , and the destination positionon sys after the jump as Jlanding.We constrain our model so that the only edit op-erations that are allowed immediately following a?jump?
are from the set of substitution operations(e.g., Sword).
And after at least one substitutionhas been made, the device can now ?jump?
backto Jstart , remembering the current edit position asJend .
Another constraint here is that after the back-ward ?jump?, all edit operations are permitted exceptfor Insert, which cannot take place until at least one2The edit distance algorithm described in Cormen etal.
(2001) can only handle adjacent word swapping (transpo-sition), but not long-distance swapping.3Recall that we transform ref into sys, and thus on the sysside, we can only insert but not delete.
The argument appliesequally to the case where the jump was made on the other side.substitution has been made.
When the edit sequenceadvances to position Jlanding, the only operation al-lowed at that point is another ?jump?
forward opera-tion to position Jend , at which point we also clear allmemory about jump positions and reset.An intuitive explanation is that when pPDA makesthe first forward jump, a gap is left in sys that hasnot been edited yet.
It remembers where it left off,and comes back to it after some substitutions havebeen made to complete the edit sequence.
The sec-ond row in Figure 1 (starting with pPDA) illustratesan edit sequence in a pPDA model that involves three?jump?
operations, which are annotated and indexedby number 1-3 in the example.
?Jump 1?
creates anun-edited gap between word 43 and western, aftertwo substitutions, the model makes ?jump 2?
to goback and edit the gap.
The only edit permitted im-mediately after ?jump 2?
is deleting the comma inref, since inserting the word 43 in sys before any sub-stitution is disallowed.
Once the gap is completed,the model resumes at position Jend by making ?jump3?, and completes the jump sequence.
The ?jumps?allowed the model to align words such as western In-dia, in addition to the alignments of 43 people foundby the pFSM.In a general pPDA model without the limited dis-tance and non-nestedness jump constraints, therecould be recursive jump structures, which violatesthe finite state property that we are looking for.
Theconstraints we introduced upper-bounds possible re-ordering, and the resulting model is finite state.
Inpractice, we found that our extension gives a bigboost to model performance (cf.
Section 4.1), withonly a modest increase in computation time.
42.3 Parameter EstimationSince the least squares operator preserves convexity,and the inner log-sum-exponential function is con-vex, the resulting objective function is also convex.For parameter learning, we used the limited memoryquasi-newton method (Liu and Nocedal, 1989) tofind the optimal feature weights and scaling constantfor the objective.
We initialized ?
=~0, ?
= 0, and?
= 5.
We also threw away features occurring fewerthan five times in training corpus.
Two variants of the4The length of the longest edit sequence with jumps onlyincreased by 0.5 ?max(|s|, |r|) in the worst case, and on thewhole swapping is rare in comparison to basic edits.987forward-backward style dynamic programming algo-rithm were used for computing gradients in the pFSMand pPDA models, similar to other sequence modelssuch as HMMs and CRFs.
Details are omitted herefor brevity.3 Rich Linguistic FeaturesIn this section we will add new substitution opera-tions beyond those introduced in Section 2, to capturevarious linguistic phenomena.
These new substitu-tion operations correspond to new transition states inthe pPDA.3.1 SynonymsOur first set of features matches words that havesynonym relations according to WordNet (Miller etal., 1990).
Synonyms have been found to be veryuseful in METEOR and TERplus, and can be easilybuilt into our model as a new substitution operationSsyn.3.2 ParaphrasingNewer versions of METEOR and TERplus bothfound that inclusion of phrase-based matching greatlyimproves model robustness and accuracy (Denkowskiand Lavie, 2010; Snover et al 2009).
We add a sub-stitution operator (Spara) that matches words that areparaphrases.
To better take advantage of paraphraseinformation at the multi-word phrase level, we ex-tended our substitution operations to match longerphrases by adding one-to-many and many-to-manyn-gram block substitutions.
In preliminary experi-ments, we found that most of the gain came fromunigrams and bigrams, with little to no additionalgains from trigrams.
Therefore, we limited our ex-periments to bigram pFSM and pPDA models, andpruned the paraphrase table adopted from TERplus 5to unigrams and bigrams, resulting in 2.5 millionparaphrase pairs.3.3 Sentence StructureA problem that remains largely unaddressed by mostpopular MT evaluation metrics is the overall good-ness of the translated sentence?s structure (Liu et al2005; Owczarzak et al 2008).
Translations with5Available from www.umiacs.umd.edu/~snover/terp.good local n-gram coverage but horrible global syn-tactic ordering are not unusual in SMT outputs.
Suchtranslations usually score well with existing metricsbut poorly among human evaluators.In our model, when we detect consecutive bigramsubstitutions in the state transition, we examine thehead-modifier dependency between the two words oneach side of the sentence pair.
A feature is triggered ifand only if there is a head-modifier relation betweenthe two words on each side, the labeled dependencyon the two sides match, and it is one of subject, ob-ject or predicative relations.
We deliberately left outfeatures that model mismatches of dependency labels,because we found parsing output from translationsto be usually very poor.
Since parsing results aregenerally more reliable for more fluent translations,our hope is that by only modeling parse matches, ourmodel will be able to pick them up as positive signals,indicating good translation quality.4 ExperimentsThe goal of our experiments is to test both the ac-curacy and robustness of the proposed new models.We then show that modeling word swapping and richlinguistics features further improve our results.To better situate our work among past researchand to draw meaningful comparison, we use exactlythe same standard evaluation data sets and metricsas Pado et al(2009), which is currently the state-of-the-art result for regression-based MT evaluation.We consider four widely used MT metrics (BLEU,NIST, METEOR (v0.7), and TER) as our baselines.Since our models are trained to regress human eval-uation scores, to make a direct comparison in thesame regression setting, we also train a small lin-ear regression model for each baseline metric in thesame way as described in Pado et al(2009).
Theseregression models are strictly more powerful thanthe baseline metrics and show higher robustness andbetter correlation with human judgments.
6 We alsocompare our models with the state-of-the-art linearregression models reported in Pado et al(2009) that6The baseline metric (e.g., BLEU) computes its raw score bytaking the geometric mean of n-gram precision scores (1?
n?
4)scaled by a brevity penalty.
The regression model learns to com-bine these fine-grained scores more intelligently, by optimizingtheir weights to regress human judgments.
See Pado et al(2009)for more discussion.988combine features from multiple MT evaluation met-rics (MT), as well as rich linguistic features from atextual entailment system (RTE).In all of our experiments, each reference and sys-tem translation sentence pair is tokenized using thePenn Treebank (Marcus et al 1993) tokenizationscript, and lemmatized by the Porter Stemmer (Porter,1980).
For the overall sentence structure experi-ment, translations are additionally part-of-speechtagged with MXPOST tagger (Ratnaparkhi, 1996),and parsed withMSTParser (McDonald et al 2005) 7labeled dependency parser.
Statistical significancetests are performed using the paired bootstrap resam-pling method (Koehn, 2004).We divide our experiments into two sections, basedon two different prediction tasks ?
predicting abso-lute scores and predicting pairwise preference.4.1 Exp.
1: Predicting Absolute ScoresThe first task is to evaluate a system translationon a seven point Likert scale against a single ref-erence.
Higher scores indicate translations that arecloser to the meaning intended by the reference.
Hu-man ratings in the form of absolute scores are avail-able for standard evaluation data sets such as NISTOpenMT06,08.8 Since our model makes predictionsat the granularity of a whole sentence, we focus onsentence-level evaluation.
A metric?s goodness isjudged by how well it correlates with human judg-ments, and Spearman?s rank correlation (?)
is re-ported for all experiments in this section.We used the NIST OpenMT06 corpus for develop-ment purposes, and reserved the NIST OpenMT08corpus for post-development evaluation.
TheOpenMT06 data set contains 1,992 English trans-lations of Arabic newswire text from 8 MT systems.For development, we used a 2-fold cross-validationscheme with splits at the first 1,000 and last 992 sen-tences.
The OpenMT08 data set contains Englishtranslations of newswire text from three languages(Arabic has 2,769 pairs from 13 MT systems; Chi-nese has 1,815 pairs from 15; and Urdu has 1,519pairs, from 7).
We followed the same experimentalsetup as Pado et al(2009), using a ?round robin?training/testing scheme, i.e., we train a model on data7Trained on the entire Penn Treebank.8Available from http://www.nist.gov.from two languages, making predictions for the third.We also show results of models trained on the entireOpenMT08 data set and tested on OpenMT06.4.1.1 pFSM vs. pPDAData Set pFSM pPDAtr te n1 n2 j1 j2 j5 j10A+C U 54.6 54.8 55.6 55.0 55.3 55.3A+U C 59.9 59.8 58.0 61.4 63.8 64.0C+U A 61.2 61.2 60.2 59.9 60.4 60.2Table 1: pFSM vs. pPDA results for the round-robinapproach on OpenMT08 data set over three languages(A=Arabic, C=Chinese, U=Urdu).
Numbers in this tableare Spearman?s ?
for correlation between human assess-ment scores and model predictions; tr stands for trainingset, and te stands for test set.
nx means the model hasx-gram block edits.
jy means the model has jump distancelimit y.
The Best result for each test set row is highlightedin bold.The second and third columns under the pFSMlabel in Table 1 compares our bigram block edit ex-tension for the pFSM model.
Although we do notyet see a significant performance gain (or loss) fromadding block edits, they will enable longer paraphrasematches in later experiments.Columns 5 through 8 in Table 1 show experimentalresults validating the contribution of our pPDA ex-tension to the pFSM model (cf.
Section 2.2).
We cansee that the pPDA extension gave modest improve-ments on the Urdu test set, but at a small decreasein performance on the Arabic data.
However, forChinese, there is a substantial gain, particularly withjump distances of five or longer.
This trend is evenmore pronounced at the long jump distance of 10,consistent with the observation that Chinese-Englishtranslations exhibit much more medium and long dis-tance reordering than languages like Arabic (Birch etal., 2009).4.1.2 Evaluating Linguistic FeaturesExperimental results evaluating the benefits ofeach linguistic feature set are presented in Table 3.The first row is the pPDA model with jump distancelimit 5, without other additional features.
The nextthree rows are the results of adding each of the threefeature sets described in Section 3.Overall, we observed that only paraphrase match-ing features gave a significant boost to performance.989Data Set Our Metrics Baseline Metrics Combined Metricstrain test pFSM pPDA pPDA+f BLEUR NISTR TERR METR MTR RTER MT+RTERA+C U 54.6 55.3 57.2 49.9 49.5 50.1 49.1 50.1 54.5 55.6A+U C 59.9 63.8 65.8 53.9 53.1 50.3 61.1 57.3 58.0 62.7C+U A 61.2 60.4 59.8 52.5 50.4 54.5 60.1 55.2 59.9 61.1MT08 MT06 65.2 63.4 64.5 57.6 55.1 63.8 62.1 62.6 62.2 65.2Table 2: Overall Comparison: Results from OpenMT08 and OpenMT06 evaluation data sets.
The R (as in BLEUR)refers to the regression model trained for each baseline metric, same as Pado et al(2009).
The first three rows areround-robin train/test results over three languages on OpenMT08 (A=Arabic, C=Chinese, U=Urdu).
The last row areresults trained on entire OpenMT08 (A+C+U) and tested on OpenMT06.
Numbers in this table are Spearman?s rankcorrelation ?
between human assessment scores and model predictions.
The pPDA column describes our pPDA modelwith jump distance limit 5.
METR is shorthand for METEORR.
+f means the model includes synonyms, paraphraseand parsing features (cf.
Section 3).
Best results and scores that are not statistically significantly worse are highlightedin bold in each row.Urdu Chinese ArabicpPDA 55.3 63.8 60.4+Synonym 55.6 63.7 60.7+Tree 55.3 63.8 60.3+Paraphrase 57.1 65.4 60.0+Syn+Tree+Para 57.2 65.8 59.8Table 3: Results for OpenMT08 with linguistic features,using the same round robin scheme as in Table 1.
Numbersin this table are Spearman?s rank correlation ?
betweenhuman assessment scores and model predictions.
Bestresults on each test set are highlighted in bold.The row starting with pPDA+f in Figure 1 showsan example where adding paraphrase features allowpPDA+f to find more correct alignments and makebetter predictions than pPDA.No significant improvements from synonym anddependency tree matching features are evident fromthe results.
An examination of the feature statistics intraining data showed that the parse tree features havevery low occurrence counts.
On the Chinese+Urdutraining set, for example, the features for subject,object and predicative labeled dependency matchesfired only 55, 784 and 13 times, respectively.
As areference point for the scale of feature counts, the?same word?
match feature fired 875,375 times onthe same data set.
And our qualitative assessment ofthe labeled dependency parser outputs was that thequality is very poor on system translations.
For futurework, more elaborate parse feature engineering couldbe a promising direction, but is outside the scope ofour study.In combination, the joint feature set of synonym,paraphrase and parse tree features gave modest im-provements over the paraphrase feature alone on theChinese test set.4.1.3 Overall ComparisonResults of our proposed models compared againstthe baseline models described in Pado et al(2009)are shown in Table 2.
The pPDA+f model has accessto paraphrase information, which is not availableto the baselines, so it should not be directly com-pared with.
But the pFSM and pPDA models donot use any additional information other than wordsand lemmas, and thus make a fair comparison withthe baseline metrics.
9 We can see from the tablethat pFSM significantly outperforms all baselines onUrdu and Arabic, but trails behind METEORR onChinese by a small margin (1.2 point in Spearman?s?).
On Chinese data set, the pPDA extension givesresults significantly better than the best baseline met-rics for Chinese (2.7 better than METEORR).
Boththe pFSM and pPDA models also significantly outper-form the MTR linear regression model that combinesthe outputs of all four baselines, on all three sourcelanguages.
This demonstrates that our regressionmodel is more robust and accurate than a state-of-the-art system combination linear-regression model.Both pFSM and pPDA learned to assign a lower neg-ative feature weight for deletion than insertion (i.e.,it is bad to insert an unseen word into system trans-9METEORR actually has an unfair advantage in this compari-son, since it uses synonym information from WordNet; TERRon the other hand has a disadvantage because it does not uselemmas.
Lemma is added later in the TERplus extension (Snoveret al 2009).990lation, but worse if words from reference translationare deleted), which corresponds to the setting in ME-TEOR where recall is given more importance thanprecision (Banerjee and Lavie, 2005).The RTER and MT+RTER linear regression mod-els benefit from the rich linguistic features in thetextual entailment system?s output.
It has access toall the features in pPDA+f such as paraphrase and de-pendency parse relations, and many more (e.g., NormBank, part-of-speech, negation, antonyms).
However,our pPDA+f model rivals the performance of RTERand MT+RTER on Arabic (with no statistically sig-nificant difference from RTER), and greatly improveover these two models on Urdu and Chinese.
Mostnoticeably, pPDA+f is 7.7 points better than RTERon Chinese.Consistent with our earlier observation onOpenMT08 data set that the pPDA model performsslightly worse than the pFSM model on Arabic, thesame performance decrease is seen in OpenMT06data set, which is also Arabic-to-English.As shown earlier in Table 3, the combined set ofparaphrase, parsing and synonym features in pPDA+fhelps for Urdu and Chinese, but not for Arabic.
Herewe found that even though the pPDA+f model is stillworse than pFSM on OpenMT06 tests, it did give adecent improvement to pPDA model, closing up thegap with pFSM.Other than robustness and accuracy, simplicity isalso an important trait we seek in good MT met-rics.
Our models only have a few tens of features(instead of hundreds of features as found in RTERand MT+RTER), which makes interpretation of themodel?s prediction relatively easy.
On an importantpractical note, our model is much more lightweightthan the RTER or MTR system.
It runs at a muchfaster speed with a smaller memory footprint, hencepotentially useable in MERT training.4.2 Exp.
2: Predicting Pairwise PreferencesTo further test our model?s robustness, we evaluateit on WMT data sets with a different prediction taskin which metrics make pairwise preference judg-ments between translation systems.
The WMT06-08 data sets are much larger in comparison to theOpenMT06 and 08 data.
They contain MT outputs ofover 40 systems from five different source languages(French, German, Spanish, Czech, and Hungarian).The WMT06, 07 and 08 sets contains 10,159, 5,472and 6,856 sentence pairs, respectively.
We used por-tions of WMT 06 and 07 data sets 10 that are anno-tated with absolute scores on a five point scale fortraining, and the WMT08 data set annotated withpairwise preference for testing.To generate pairwise preference predictions, wefirst predict an absolute score for each system trans-lation, then compare the scores between each systempair, and give preference to the higher score.
Weadopt the sentence-level evaluation metric used inPado et al(2009), which measures the consistency(accuracy) of metric predictions with human prefer-ences.
The random baseline for this task on WMT08data set is 39.8%.
11Models WMT06 WMT07 WMT06+07pPDA+f 51.6 52.4 52.0BLEUR 49.7 49.5 49.6METEORR 51.4 51.4 51.5NISTR 50.0 50.3 50.2TERR 50.9 51.0 51.2MTR 50.8 51.5 51.5RTER 51.8 50.7 51.9MT+RTER 52.3 51.8 52.5Table 4: Pairwise preference prediction results on WMT08test set.
Each column shows a different training data set.Numbers in this table are model?s consistency with humanpairwise preference judgments.
Best result on each testset is highlighted in bold.Results are shown in Table 4.
Similar to the resultson OpenMT experiments, our model consistently out-performed BLEUR, METEORR, NISTR and TERR.Our model also gives better performance than theMTR ensemble model on all three tests; and ties withRTER in two out of the three tests but performs sig-nificantly better on the other test.
The MT+RTERensemble model is better on two tests, but worseon the other.
But overall the two systems are quitecomparable, with less than 0.6% accuracy difference.The results also show that our method is stable acrossdifferent training sets, with test accuracy differencesless than 0.4%.10Available from http://www.statmt.org.11The random baseline is not 50% for two reasons: (1) humanjudgments include contradictory and tie annotations; (2) tran-sitivity constraints need to be respected in total ordering.
Fordetails, see Pado et al(2009).9914.3 Qualitative AnalysisExample (1) shows a system and reference translationpair in the Chinese test portion of OpenMT08.
(1) REF: Two Jordanese sentenced1 for plotting2an attack3 on Americans4SYS: The name of Jordan plotting2 attacks3Americans4 were sentenced1 to deathHuman annotators give this example a score of 4.0,but TERR and METEORR both assigned erroneouslylow scores (1.0 and 2.2, respectively).
Words with thesame subscript index were aligned by pPDA model.This example exhibits a word swapping phenomenon,and our model was able to capture it correctly.
TERRclearly suffered from not being able to model wordswapping in this case.
It also missed out the wordpair attack and attacks due to the lack of lemma sup-port.
The reason why METEORR assigned such a lowscore for this example is because none of the matchedwords in the reference were adjacent to each other,causing a high fragmentation penalty.
The fragmenta-tion penalty term has two parameters that need to bemanually tuned, and has a high variance across exam-ples and data sets.
This example illustrates modelsthat require ad-hoc tuning tend not to be robust.
OurpPDA model (without linguistic feature) was ableto make a prediction of 3.7, much closer to humanjudgment.4.4 MetricsMATR10 and WMT12 ResultsAn earlier version of the pFSM model that wastrained on the OpenMT08 data set was submittedto the single reference sentence level track at Met-ricsMATR10 (Peterson and Przybocki, 2010) NISTevaluation.
Even though our system was not in themost ideal state at the time of the evaluation, 12 andwas trained on a small amount of data, the pFSMmodel still performed competitively against othermetrics.
Noticeably, we achieved second best resultsfor Human-targeted Translation Edit Rate (HTER)assessment, trailing behind TERplus with no statisti-cally significant difference.
On average, our systemmade 5th place among 15 different sites and 7th placeamong 25 different metrics, averaged across 9 assess-ment types.12Unfortunately the version we submitted in 2010 was plaguedwith a critical bug.
More general enhancements have been madeto the model since.We submitted the version of the pPDA+f modeltrained on the WMT07 dataset to the ?into English?segment-leval track of the WMT 2012 Shared Eval-uation Metrics Task (Callison-Burch et al 2012).Our model achieved the highest score (measured byKendall?s tau correlation) on all four language pairs(Fr-En, De-En, Es-En and Cs-En), and tied for thefirst place with METEOR v1.3 on average correla-tion.5 Related WorkFeatures and RepresentationOne of the findings in our experimentation is thatparaphrasing helps boosting model accuracy, andthe idea of using paraphrases in MT evaluation wasfirst proposed by Zhou et al(2006).
Several re-cent studies have introduced metrics over dependencyparses (Liu et al 2005; Owczarzak et al 2008; He etal., 2010), but their improvements over n-gram mod-els at the sentence level are not always consistent (Liuet al 2005; Peterson and Przybocki, 2010).
Otherthan string-based methods, recent work has exploredmore alternative representations for MT evaluation,such as network properties (Amancio et al 2011),semantic role structures (Lo and Wu, 2011), and thequality of word order (Birch and Osborne, 2011).ModelingThe idea of using extended edit distance models withblock movements was also explored in Leusch etal.
(2003).
However, their model is largely empiricaland not in a probabilistic learning setting.
The lineof work on probabilistic tree-edit distance modelsbears a strong connection to this work (McCallumet al 2005; Bernard et al 2008; Wang and Man-ning, 2010; Emms, 2012).
In particular, our pFSMmodel and the log-linear parameterization were in-spired by Wang and Manning (2010).
Another bodyof literature that is closely related to this work isFSM models for word alignment (Vogel et al 1996;Saers et al 2010; Berg-Kirkpatrick et al 2010).
Thestochastic Inversion Transduction Grammar in Saerset al(2010) for instance, is a pFSM with specialconstraints.
More recently, Saers and Wu (2011) fur-ther explored the connection between Linear Trans-duction Grammars and FSMs.
There is a close tie992between our pFSM model and the HMM model inBerg-Kirkpatrick et al(2010).
Both models adopteda log-linear parameterization for the state transitiondistribution, 13 but in their case the HMM model andthe pFSM arc weights are normalized locally, and theobjective is non-convex.6 ConclusionWe described a probabilistic finite state machinebased on string edits and a novel pushdown automa-ton extension for the task of machine translation eval-uation.
The models admit a rich set of linguisticfeatures, and are trained to learn feature weights auto-matically by optimizing a regression objective.
Theproposed models achieve state-of-the-art results ona wide range of standard evaluations, and are muchmore lightweight than previous regression models,making them suitable candidates to be used in MERTtraining.AcknowledgementsWe gratefully acknowledge the support of DefenseAdvanced Research Projects Agency (DARPA) Ma-chine Reading Program under Air Force ResearchLaboratory (AFRL) prime contract no.
FA8750-09-C-0181 and the support of the DARPA BroadOperational Language Translation (BOLT) programthrough IBM.
Any opinions, findings, and conclusionor recommendations expressed in this material arethose of the author(s) and do not necessarily reflectthe view of the DARPA, AFRL, or the US govern-ment.ReferencesJ.
Albrecht and R. Hwa.
2007a.
A re-examination ofmachine learning approaches for sentence-level MTevaluation.
In Proceedings of ACL.J.
Albrecht and R. Hwa.
2007b.
Regression for sentence-level MT evaluation with pseudo references.
In Pro-ceedings of ACL.D.R.
Amancio, M.G.V.
Nunes, O.N.
Oliveira Jr., T.A.S.Pardo, L. Antiqueira, and L. da F. Costa.
2011.
Usingmetrics from complex networks to evaluate machinetranslation.
Physica A, 390(1):131?142.S.
Banerjee and A. Lavie.
2005.
Meteor: An automaticmetric for MT evaluation with improved correlation13Similar parameterization was also used in much previouswork, such as Riezler et al(2000).with human judgments.
In Proceedings of ACL Work-shop on Intrinsic and Extrinsic Evaluation Measures.T.
Berg-Kirkpatrick, A. Bouchard-Cote, J. DeNero, andD.
Klein.
2010.
Painless unsupervised learning withfeatures.
In Proceedings of NAACL.M.
Bernard, L. Boyer, A. Habrard, and M. Sebban.
2008.Learning probabilistic models of tree edit distance.
Pat-tern Recognition, 41(8):2611?2629.A.
Birch and M. Osborne.
2011.
Reordering metrics forMT.
In Proceedings of ACL/HLT.A.
Birch, P. Blunsom, and M. Osborne.
2009.
A quantita-tive analysis of reordering phenomena.
In Proceedingsof WMT 09.C.
Callison-Burch, P. Koehn, C. Monz, and J. Schroeder.2009.
Findings of the 2009 Workshop on StatisticalMachine Translation.
In Proceedings of the FourthWorkshop on Statistical Machine Translation.C.
Callison-Burch, P. Koehn, C. Monz, K. Peterson,M.
Przybocki, and O. Zaidan.
2010.
Findings of the2010 joint workshop on Statistical Machine Translationand metrics for Machine Translation.
In Proceedingsof Joint WMT 10 and MetricsMatr Workshop at ACL.C.
Callison-Burch, P. Koehn, C. Monz, and O. Zaidan.2011.
Findings of the 2011 workshop on statistical ma-chine translation.
In Proceedings of the Sixth Workshopon Statistical Machine Translation.C.
Callison-Burch, P. Koehn, C. Monz, M. Post, R. Soricut,and L. Specia.
2012.
Findings of the 2012 workshopon Statistical Machine Translation.
In Proceedings ofSeventh Workshop on Statistical Machine Translationat NAACL.T.
Cormen, C. Leiserson, R. Rivest, and C. Stein.
2001.Introduction to Algorithms, Second Edition.
MIT Press.M.
Denkowski and A. Lavie.
2010.
Extending the ME-TEOR machine translation evaluation metric to thephrase level.
In Proceedings of HLT/NAACL.G.
Doddington.
2002.
Automatic evaluation of machinetranslation quality using n-gram cooccurrence statistics.In Proceedings of HLT.Y.
Dombb, O. Lipsky, B. Porat, E. Porat, and A. Tsur.2010.
The approximate swap and mismatch edit dis-tance.
Theoretical Computer Science, 411(43).J.
Eisner.
2002.
Parameter estimation for probabilisticfinite-state transducers.
In Proceedings of ACL.M.
Emms.
2012.
On stochastic tree distances and theirtraining via expectation-maximisation.
In Proceedingsof International Conference on Pattern RecognitionApplication and Methods.J.
Esparza and A. Kucera.
2005.
Quantitative analysisof probabilistic pushdown automata: Expectations andvariances.
In Proceedings of the 20th Annual IEEESymposium on Logic in Computer Science.993Y.
He and A.Way.
2009.
Improving the objective functionin minimum error rate training.
In Proceedings of MTSummit XII.Y.
He, J.
Du, A.
Way, and J. van Genabith.
2010.
TheDCU dependency-based metric inWMT-MetricsMATR2010.
In Proceedings of Joint WMT 10 and Metrics-Matr Workshop at ACL.K.
Knight and Y. Al-Onaizan.
1998.
Translation withfinite-state devices.
In Proceedings of AMTA.P.
Koehn.
2004.
Statistical significance tests for machinetranslation evaluation.
In Proceedings of EMNLP.A.
Kulesza and S. Shieber.
2004.
Robust machine trans-lation evaluation with entailment features.
In Proceed-ings of TMI.S.
Kumar and W. Byrne.
2003.
A weighted finite statetransducer implementation of the alignment templatemodel for statistical machine translation.
In Proceed-ings of HLT/NAACL.G.
Leusch, N. Ueffing, and H. Ney.
2003.
A novel string-to-string distance measure with applications to machinetranslation evaluation.
In Proceedings of MT Summit I.D.
C. Liu and J. Nocedal.
1989.
On the limited mem-ory BFGS method for large scale optimization.
Math.Programming, 45:503?528.D.
Liu, , and D. Gildea.
2005.
Syntactic features for eval-uation of machine translation.
In Proceedings of theACL Workshop on Intrinsic and Extrinsic EvaluationMeasures.C.
Liu, D. Dahlmeier, and H. Ng.
2011.
Better eval-uation metrics lead to better machine translation.
InProceedings of EMNLP.C.
Lo and D. Wu.
2011.
MEANT: An inexpensive, high-accuracy, semi-automatic metric for evaluating transla-tion utility based on semantic roles.
In Proceedings ofACL/HLT.M.
P. Marcus, M. A. Marcinkiewicz, and B. Santorini.1993.
Building a large annotated corpus of english: thePenn Treebank.
Computational Linguistics, 19(2):313?330.A.
McCallum, K. Bellare, and F. Pereira.
2005.
A condi-tional random field for discriminatively-trained finite-state string edit distance.
In Proceedings of UAI.R.
McDonald, K. Crammer, and F. Pereira.
2005.
On-line large-margin training of dependency parsers.
InProceedings of ACL.G.
A.Miller, R. Beckwith, C. Fellbaum, D. Gross, and K. J.Miller.
1990.
WordNet: an on-line lexical database.International Journal of Lexicography, 3(4).F.
Och.
2003.
Minimum error rate training in statisticalmachine translation.
In Proceedings of ACL.K.
Owczarzak, J. van Genabith, and A.
Way.
2008.
Evalu-ating machine translation with LFG dependencies.
Ma-chine Translation, 21(2):95?119.S.
Pado, M. Galley, D. Jurafsky, and C. D. Manning.
2009.A learning approach to improving sentence-level MTevaluation.
In Proceedings of ACL.K.
Papineni, S. Roukos, T. Ward, and W. Zhu.
2002.BLEU: a method for automatic evaluation of machinetranslation.
In Proceedings of ACL.K.
Peterson and M. Przybocki.
2010.
NIST 2010 metricsfor machine translation evaluation (MetricsMaTr10)official release of results.M.F.
Porter.
1980.
An algorithm for suffix stripping.Program, 14(3):130?137.A.
Ratnaparkhi.
1996.
A maximum entropy part-of-speech tagger.
In Proceedings of EMNLP.S.
Riezler, D. Prescher, J. Kuhn, and M. Johnson.
2000.Lexicalized stochastic modeling of constraint-basedgrammars using log-linear measures and em training.In Proceedings of ACL.M.
Saers and D. Wu.
2011.
Linear transduction grammarsand zipper finite-state transducers.
In Proceedings ofRecent Advances in Natural Language Processing.M.
Saers, J. Nivre, and D. Wu.
2010.
Word alignmentwith stochastic bracketing linear inversion transductiongrammar.
In Proceedings of NAACL.M.
Snover, B. Dorr, R. Schwartz, L. Micciulla, andJ.
Makhoul.
2006.
A study of translation edit ratewith targeted human annotation.
In Proceedings ofAMTA.M.
Snover, , N. Madnani, B. Dorr, and R. Schwartz.
2009.Fluency, adequacy, or HTER?
exploring different hu-man judgments with a tunable MT metric.
In Proceed-ings of WMT09 Workshop.S.
Sun, Y. Chen, and J. Li.
2008.
A re-examination onfeatures in regression based approach to automatic MTevaluation.
In Proceedings of ACL.E.
Vidal, F. Thollard, C. de la Higuera, F. Casacuberta,and R. C. Carrasco.
2005.
Probabilistic finite-state ma-chines part I. IEEE Transactions on Pattern Analysisand Machine Intelligence, 27(7):1013?1025.S.
Vogel, H. Ney, and C. Tillmann.
1996.
HMM-basedword alignment in statistical translation.
In Proceed-ings of COLING.M.
Wang and C.D.
Manning.
2010.
Probabilistic tree-edit models with structured latent variables for textualentailment and question answering.
In Proceedings ofCOLING.D.
Wu, 2010.
CRC Handbook of Natural Language Pro-cessing, chapter Alignment, pages 367?408.
CRCPress.L.
Zhou, C.Y.
Lin, and E. Hovy.
2006.
Re-evaluatingmachine translation results with paraphrase support.
InProceedings of EMNLP.994
