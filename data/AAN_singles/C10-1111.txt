Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 984?992,Beijing, August 2010Multi-Document Summarization viathe Minimum Dominating SetChao Shen and Tao LiSchool of Computing and Information SciencesFlorida Internation University{cshen001|taoli}@cs.fiu.eduAbstractMulti-document summarization hasbeen an important problem in infor-mation retrieval.
It aims to dis-till the most important informationfrom a set of documents to gener-ate a compressed summary.
Givena sentence graph generated from aset of documents where vertices repre-sent sentences and edges indicate thatthe corresponding vertices are simi-lar, the extracted summary can be de-scribed using the idea of graph dom-ination.
In this paper, we proposea new principled and versatile frame-work for multi-document summariza-tion using the minimum dominatingset.
We show that four well-knownsummarization tasks including generic,query-focused, update, and compara-tive summarization can be modeled asdifferent variations derived from theproposed framework.
Approximationalgorithms for performing summariza-tion are also proposed and empiricalexperiments are conducted to demon-strate the effectiveness of our proposedframework.1 IntroductionAs a fundamental and effective tool for docu-ment understanding and organization, multi-document summarization enables better infor-mation services by creating concise and infor-mative reports for a large collection of doc-uments.
Specifically, in multi-document sum-marization, given a set of documents as input,the goal is to produce a condensation (i.e.,a generated summary) of the content of theentire input set (Jurafsky and Martin, 2008).The generated summary can be generic whereit simply gives the important information con-tained in the input documents without anyparticular information needs or query/topic-focused where it is produced in response to auser query or related to a topic or concern thedevelopment of an event (Jurafsky and Mar-tin, 2008; Mani, 2001).Recently, new summarization tasks such asupdate summarization (Dang and Owczarzak,2008) and comparative summarization (Wanget al, 2009a) have also been proposed.
Up-date summarization aims to generate shortsummaries of recent documents to capturenew information different from earlier docu-ments and comparative summarization aimsto summarize the differences between compa-rable document groups.In this paper, we propose a new principledand versatile framework for multi-documentsummarization using the minimum dominat-ing set.
Many known summarization tasks in-cluding generic, query-focused, update, andcomparative summarization can be modeledas different variations derived from the pro-posed framework.
The framework provides anelegant basis to establish the connections be-tween various summarization tasks while high-lighting their differences.In our framework, a sentence graph is firstgenerated from the input documents wherevertices represent sentences and edges indicatethat the corresponding vertices are similar.
Anatural method for describing the extractedsummary is based on the idea of graph dom-ination (Wu and Li, 2001).
A dominating setof a graph is a subset of vertices such thatevery vertex in the graph is either in the sub-set or adjacent to a vertex in the subset; and984a minimum dominating set is a dominatingset with the minimum size.
The minimumdominating set of the sentence graph can benaturally used to describe the summary: itis representative since each sentence is eitherin the minimum dominating set or connectedto one sentence in the set; and it is withminimal redundancy since the set is of mini-mum size.
Approximation algorithms are pro-posed for performing summarization and em-pirical experiments are conducted to demon-strate the effectiveness of our proposed frame-work.
Though the dominating set problem hasbeen widely used in wireless networks, this pa-per is the first work on using it for modelingsentence extraction in document summariza-tion.The rest of the paper is organized as fol-lows.
In Section 2, we review the related workabout multi-document summarization and thedominating set.
After introducing the min-imum dominating set problem in graph the-ory in Section 3, we propose the minimumdominating set based framework for multi-document summarization and model the foursummarization tasks including generic, query-focused, update, and comparative summariza-tion in Section 4.
Section 5 presents the exper-imental results and analysis, and finally Sec-tion 6 concludes the paper.2 Related WorkGeneric Summarization For generic sum-marization, a saliency score is usually as-signed to each sentence and then the sen-tences are ranked according to the saliencyscore.
The scores are usually computed basedon a combination of statistical and linguisticfeatures.
MEAD (Radev et al, 2004) is animplementation of the centroid-based methodwhere the sentence scores are computed basedon sentence-level and inter-sentence features.SumBasic (Nenkova and Vanderwende, 2005)shows that the frequency of content wordsalone can also lead good summarization re-sults.
Graph-based methods (Erkan andRadev, 2004; Wan et al, 2007b) have alsobeen proposed to rank sentences or passagesbased on the PageRank algorithm or its vari-ants.Query-Focused Summarization Inquery-focused summarization, the informa-tion of the given topic or query should beincorporated into summarizers, and sentencessuiting the user?s declared information needshould be extracted.
Many methods forgeneric summarization can be extended toincorporate the query information (Saggionet al, 2003; Wei et al, 2008).
Wan et al(Wan et al, 2007a) make full use of boththe relationships among all the sentences inthe documents and relationship between thegiven query and the sentences by manifoldranking.
Probability models have also beenproposed with different assumptions on thegeneration process of the documents andthe queries (Daume?
III and Marcu, 2006;Haghighi and Vanderwende, 2009; Tang etal., 2009).Update Summarization and Compara-tive Summarization Update summariza-tion was introduced in Document Understand-ing Conference (DUC) 2007 (Dang, 2007) andwas a main task of the summarization track inText Analysis Conference (TAC) 2008 (Dangand Owczarzak, 2008).
It is required to sum-marize a set of documents under the assump-tion that the reader has already read andsummarized the first set of documents as themain summary.
To produce the update sum-mary, some strategies are required to avoid re-dundant information which has already beencovered by the main summary.
One of themost frequently used methods for remov-ing redundancy is Maximal Marginal Rele-vance(MMR) (Goldstein et al, 2000).
Com-parative document summarization is proposedby Wang et.
al.
(Wang et al, 2009a) tosummarize the differences between compara-ble document groups.
A sentence selectionapproach is proposed in (Wang et al, 2009a)to accurately discriminate the documents indifferent groups modeled by the conditionalentropy.985The Dominating Set Many approxima-tion algorithms have been developed for find-ing minimum dominating set for a givengraph (Guha and Khuller, 1998; Thai et al,2007).
Kann (Kann, 1992) shows that theminimum dominating set problem is equiv-alent to set cover problem, which is a well-known NP-hard problem.
Dominating set hasbeen widely used for clustering in wireless net-works (Chen and Liestman, 2002; Han andJia, 2007).
It has been used to find topicwords for hierarchical summarization (Lawrieet al, 2001), where a set of topic words is ex-tracted as a dominating set of word graph.
Inour work, we use the minimum dominating setto formalize the sentence extraction for docu-ment summarization.3 The Minimum Dominating SetProblemGiven a graph G =< V,E >, a dominatingset of G is a subset S of vertices with thefollowing property: each vertex of G is eitherin the dominating set S, or is adjacent to somevertices in S.Problem 3.1.
Given a graph G, the mini-mum dominating set problem (MDS) is to finda minimum size subset S of vertices, such thatS forms a dominating set.MDS is closely related to the set cover prob-lem (SC), a well-known NP-hard problem.Problem 3.2.
Given F , a finite collection{S1, S2, .
.
.
, Sn} of finite sets, the set coverproblem (SC) is to find the optimal solutionF ?
= arg minF ?
?F|F ?| s.t.?S?
?F ?S?
=?S?FS.Theorem 3.3.
There exists a pair of polyno-mial time reduction between MDS and SC.So, MDS is also NP-hard and it has beenshown that there are no approximate solutionswithin c log |V |, for some c > 0 (Feige, 1998;Raz and Safra, 1997).3.1 An Approximation AlgorithmA greedy approximation algorithm for the SCproblem is described in (Johnson, 1973).
Ba-sically, at each stage, the greedy algorithmchooses the set which contains the largestnumber of uncovered elements.Based on Theorem 3.3, we can obtain agreedy approximation algorithm for MDS.Starting from an empty set, if the current sub-set of vertices is not the dominating set, a newvertex which has the most number of the ad-jacent vertices that are not adjacent to anyvertex in the current set will be added.Proposition 3.4.
The greedy algorithm ap-proximates SC within 1 + ln s where s is thesize of the largest set.It was shown in (Johnson, 1973) that theapproximation factor for the greedy algorithmis no more thanH(s) , the s-th harmonic num-ber:H(s) =s?k=11k ?
ln s+ 1Corollary 3.5.
MDS has a approximation al-gorithm within 1 + ln?
where ?
is the maxi-mum degree of the graph.Corollary 3.5 follows directly from Theo-rem 3.3 and Proposition 3.4.4 The Summarization Framework4.1 Sentence Graph GenerationTo perform multi-document summarizationvia minimum dominating set, we need to firstconstruct a sentence graph in which each nodeis a sentence in the document collection.
Inour work, we represent the sentences as vec-tors based on tf-isf, and then obtain the cosinesimilarity for each pair of sentences.
If thesimilarity between a pair of sentences si andsj is above a given threshold ?, then there isan edge between si and sj .For generic summarization, we use all sen-tences for building the sentence graph.
Forquery-focused summarization, we only use thesentences containing at least one term in thequery.
In addition, when a query q is involved,we assign each node si a weight, w(si) =d(si, q) = 1 ?
cos(si, q), to indicate the dis-tance between the sentence and the query q.After building the sentence graph, we canformulate the summarization problem using986Generic Summary(a)Query-focused Summaryquery(b)Updated SummaryC1C2(c)Comparative SummaryComparative SummaryComparative SummaryC2C1C3(d)Figure 1: Graphical illustrations of multi-document summarization via the minimum domi-nating set.
(a): The minimum dominating set is extracted as the generic summary.
(b):Theminimum weighted dominating set is extracted as the query-based summary.
(c):Vertices inthe right rectangle represent the first document set C1, and ones in the left represent the sec-ond document set where update summary is generated.
(d):Each rectangle represents a groupof documents.
The vertices with rings are the dominating set for each group, while the solidvertices are the complementary dominating set, which is extracted as comparative summaries.the minimum dominating set.
A graphical il-lustration of the proposed framework is shownin Figure 1.4.2 Generic SummarizationGeneric summarization is to extract the mostrepresentative sentences to capture the impor-tant content of the input documents.
Withouttaking into account the length limitation ofthe summary, we can assume that the sum-mary should represent all the sentences in thedocument set (i.e., every sentence in the docu-ment set should either be extracted or be sim-ilar with one extracted sentence).
Meanwhile,a summary should also be as short as possi-ble.
Such summary of the input documentsunder the assumption is exactly the minimumdominating set of the sentence graph we con-structed from the input documents in Section4.1.
Therefore the summarization problemcan be formulated as the minimum dominat-ing set problem.However, usually there is a length restric-tion for generating the summary.
Moreover,the MDS is NP-hard as shown in Section 3.Therefore, it is straightforward to use a greedyapproximation algorithm to construct a subsetof the dominating set as the final summary.
Inthe greedy approach, at each stage, a sentencewhich is optimal according to the local crite-ria will be extracted.
Algorithm 1 describesAlgorithm 1 Algorithm for Generic Summariza-tionINPUT: G, WOUTPUT: S1: S = ?2: T = ?3: while L(S) < W and V (G)!
= S do4: for v ?
V (G)?
S do5: s(v) = |{ADJ(v) ?
T}|6: v?
= argmaxv s(v)7: S = S ?
{v?
}8: T = T ?ADJ(v?
)an approximation algorithm for generic sum-marization.
In Algorithm 1, G is the sen-tence graph, L(S) is the length of the sum-mary, W is the maximal length of the sum-mary, and ADJ(v) = {v?|(v?, v) ?
E(G)} isthe set of vertices which are adjacent to thevertex v. A graphical illustration of genericsummarization using the minimum dominat-ing set is shown in Figure 1(a).4.3 Query-Focused SummarizationLetting G be the sentence graph constructedin Section 4.1 and q be the query, the query-focused summarization can be modeled asD?
= argminD?G?s?D d(s, q) (1)s.t.
D is a dominating set of G.Note that d(s, q) can be viewed as the weightof vertex in G. Here the summary length isminimized implicitly, since if D?
?
D, then987?s?D?
d(s, q) ?
?s?D d(s, q).
The problemin Eq.
(1) is exactly a variant of the minimumdominating set problem, i.e., the minimumweighted dominating set problem (MWDS).Similar to MDS, MWDS can be reducedfrom the weighted version of the SC problem.In the weighted version of SC, each set has aweight and the sum of weights of selected setsneeds to be minimized.
To generate an ap-proximate solution for the weighted SC prob-lem, instead of choosing a set i maximizing|SET (i)|, a set i minimizing w(i)|SET (i)| is cho-sen, where SET (i) is composed of uncoveredelements in set i, and w(i) is the weight of seti.
The approximate solution has the same ap-proximation ratio as that for MDS, as statedby the following theorem (Chvatal, 1979).Theorem 4.1.
An approximate weighteddominating set can be generated with a size atmost 1+log?
?|OPT |, where ?
is the maximaldegree of the graph and OPT is the optimalweighted dominating set.Accordingly, from generic summarization toquery-focused summarization, we just need tomodify line 6 in Algorithm 1 tov?
= argminvw(v)s(v) , (2)where w(v) is the weight of vertex v. A graph-ical illustration of query-focused summariza-tion using the minimum dominating set isshown in Figure 1(b).4.4 Update SummarizationGive a query q and two sets of documents C1and C2, update summarization is to generatea summary of C2 based on q, given C1.
Firstly,summary of C1, referred as D1 can be gener-ated.
Then, to generate the update summaryof C2, referred as D2, we assume D1 and D2should represent all query related sentences inC2, and length of D2 should be minimized.Let G1 be the sentence graph for C1.
Firstwe use the method described in Section 4.3 toextract sentences from G1 to form D1.
Thenwe expand G1 to the whole graph G using thesecond set of documents C2.
G is then thegraph presentation of the document set in-cluding C1 and C2.
We can model the updatesummary of C2 asD?
= argminD2?s?D2 w(s) (3)s.t.
D2 ?D1 is a dominating set of G.Intuitively, we extract the smallest set of sen-tences that are closely related to the queryfrom C2 to complete the partial dominatingset of G generated from D1.
A graphical il-lustration of update summarization using theminimum dominating set is shown in Fig-ure 1(c).4.5 Comparative SummarizationComparative document summarization aimsto summarize the differences among compara-ble document groups.
The summary producedfor each group should emphasize its differencefrom other groups (Wang et al, 2009a).We extend our method for update sum-marization to generate the discriminant sum-mary for each group of documents.
Given Ngroups of documents C1, C2, .
.
.
, CN , we firstgenerate the sentence graphs G1, G2, .
.
.
, GN ,respectively.
To generate the summary forCi, 1 ?
i ?
N , we view Ci as the updateof all other groups.
To extract a new sen-tence, only the one connected with the largestnumber of sentences which have no represen-tatives in any groups will be extracted.
Wedenote the extracted set as the complemen-tary dominating set, since for each group weobtain a subset of vertices dominating thoseare not dominated by the dominating sets ofother groups.
To perform comparative sum-marization, we first extract the standard dom-inating sets for G1, .
.
.
, GN , respectively, de-noted as D1, .
.
.
, DN .
Then we extract theso-called complementary dominating set CDifor Gi by continuing adding vertices in Gi tofind the dominating set of ?1?j?NGj givenD1, .
.
.
,Di?1,Di+1, .
.
.
,DN .
A graphical il-lustration of comparative summarization isshown in Figure 1(d).988DUC04 DUC05 DUC06 TAC08 A TAC08 BType of Summarization Generic Topic-focused Topic-focused Topic-focused Update#topics NA 50 50 48 48#documents per topic 10 25-50 25 10 10Summary length 665 bytes 250 words 250 words 100 words 100 wordsTable 1: Brief description of the data set5 ExperimentsWe have conducted experiments on all foursummarization tasks and our proposed meth-ods based on the minimum dominating sethave outperformed many existing methods.For the generic, topic-focused and updatesummarization tasks, the experiments are per-form the DUC data sets using ROUGE-2 andROUGE-SU (Lin and Hovy, 2003) as evalua-tion measures.
For comparative summariza-tion, a case study as in (Wang et al, 2009a) isperformed.
Table 1 shows the characteristicsof the data sets.
We use DUC04 data set toevaluate our method for generic summariza-tion task and DUC05 and DUC06 data setsfor query-focused summarization task.
Thedata set for update summarization, (i.e.
themain task of TAC 2008 summarization track)consists of 48 topics and 20 newswire articlesfor each topic.
The 20 articles are groupedinto two clusters.
The task requires to pro-duce 2 summaries, including the initial sum-mary (TAC08 A) which is standard query-focused summarization and the update sum-mary (TAC08 B) under the assumption thatthe reader has already read the first 10 docu-ments.We apply a 5-fold cross-validation proce-dure to choose the threshold ?
used for gener-ating the sentence graph in our method.5.1 Generic SummarizationWe implement the following widely used orrecent published methods for generic summa-rization as the baseline systems to comparewith our proposed method (denoted as MDS).
(1) Centroid: The method applies MEAD al-gorithm (Radev et al, 2004) to extract sen-tences according to the following three pa-rameters: centroid value, positional value,and first-sentence overlap.
(2) LexPageR-ank: The method first constructs a sentenceconnectivity graph based on cosine similarityand then selects important sentences based onthe concept of eigenvector centrality (Erkanand Radev, 2004).
(3) BSTM: A Bayesiansentence-based topic model making use ofboth the term-document and term-sentenceassociations (Wang et al, 2009b).Our method outperforms the simple Cen-troid method and another graph-based Lex-PageRank, and its performance is close to theresults of the Bayesian sentence-based topicmodel and those of the best team in the DUCcompetition.
Note however that, like clus-tering or topic based methods, BSTM needsthe topic number as the input, which usuallyvaries by different summarization tasks and ishard to estimate.5.2 Query-Focused SummarizationWe compare our method (denoted as MWDS)described in Section 4.3 with some recentlypublished systems.
(1) TMR (Tang et al,2009): incorporates the query informationinto the topic model, and uses topic basedscore and term frequency to estimate the im-portance of the sentences.
(2) SNMF (Wanget al, 2008): calculates sentence-sentencesimilarities by sentence-level semantic analy-sis, clusters the sentences via symmetric non-negative matrix factorization, and extractsthe sentences based on the clustering result.
(3) Wiki (Nastase, 2008): uses Wikipediaas external knowledge to expand query andbuilds the connection between the query andthe sentences in documents.Table 3 presents the experimental compar-ison of query-focused summarization on thetwo datasets.
From Table 3, we observe thatour method is comparable with these systems.This is due to the good interpretation of thesummary extracted by our method, an ap-989DUC04ROUGE-2 ROUGE-SUDUC Best 0.09216 0.13233Centroid 0.07379 0.12511LexPageRank 0.08572 0.13097BSTM 0.09010 0.13218MDS 0.08934 0.13137Table 2: Results on generic summariza-tion.DUC05 DUC06ROUGE-2 ROUGE-SU ROUGE-2 ROUGE-SUDUC Best 0.0725 0.1316 0.09510 0.15470SNMF 0.06043 0.12298 0.08549 0.13981TMR 0.07147 0.13038 0.09132 0.15037Wiki 0.07074 0.13002 0.08091 0.14022MWDS 0.07311 0.13061 0.09296 0.14797Table 3: Results on query-focused summariza-tion.proximate minimal dominating set of the sen-tence graph.
On DUC05, our method achievesthe best result; and on DUC06, our methodoutperforms all other systems except the bestteam in DUC.
Note that our method basedon the minimum dominating set is much sim-pler than other systems.
Our method onlydepends on the distance to the query and hasonly one parameter (i.e., the threshold ?
ingenerating the sentence graph).0.0650.070.0750.080.0850.090.0950.05  0.1  0.15  0.2  0.25ROUGE-2Similarity threshold ?DUC 06DUC 05Figure 2: ROUGE-2 vs. threshold ?We also conduct experiments to empiricallyevaluate the sensitivity of the threshold ?.Figure 2 shows the ROUGE-2 curve of ourMWDS method on the two datasets when ?varies from 0.04 to 0.26.
When ?
is small,edges fail to represent the similarity of the sen-tences, while if ?
is too large, the graph willbe sparse.
As ?
is approximately in the rangeof 0.1?
0.17, ROUGE-2 value becomes stableand relatively high.5.3 Update SummarizationTable 5 presents the experimental results onupdate summarization.
In Table 5, ?TACBest?
and ?TAC Median?
represent the bestand median results from the participants ofTAC 2008 summarization track in the twotasks respectively according to the TAC 2008report (Dang and Owczarzak, 2008).
As seenfrom the results, the ROUGE scores of ourmethods are higher than the median results.The good results of the best team typicallycome from the fact that they utilize advancednatural language processing (NLP) techniquesto resolve pronouns and other anaphoric ex-pressions.
Although we can spend more effortson the preprocessing or language processingstep, our goal here is to demonstrate the ef-fectiveness of formalizing the update summa-rization problem using the minimum dominat-ing set and hence we do not utilize advancedNLP techniques for preprocessing.
The exper-imental results demonstrate that our simpleupdate summarization method based on theminimum dominating set can lead to compet-itive performance for update summarization.TAC08 A TAC08 BROUGE-2 ROUGE-SUROUGE-2 ROUGE-SUTAC Best 0.1114 0.14298 0.10108 0.13669TAC Median 0.08123 0.11975 0.06927 0.11046MWDS 0.09012 0.12094 0.08117 0.11728Table 5: Results on update summarization.5.4 Comparative SummarizationWe use the top six largest clusters of doc-uments from TDT2 corpora to compare thesummary generated by different comparativesummarization methods.
The topics of the sixdocument clusters are as follows: topic 1: IraqIssues; topic 2: Asia?s economic crisis; topic 3:Lewinsky scandal; topic 4: Nagano OlympicGames; topic 5: Nuclear Issues in Indian andPakistan; and topic 6: Jakarta Riot.
Fromeach of the topics, 30 documents are extracted990Topic Complementary Dominating Set Discriminative Sentence Selection Dominating Set1 ?
?
?
U.S. Secretary of StateMadeleine Albright arrives toconsult on the stand-off betweenthe United Nations and Iraq.the U.S. envoy to the UnitedNations, Bill Richardson, ?
?
?play down China?s refusal to sup-port threats of military forceagainst IraqThe United States and Britaindo not trust President Sad-dam and wants cdotswarningof serious consequences if Iraqviolates the accord.2 Thailand?s currency, thebaht, dropped through akey psychological level of ?
?
?amid a regional sell-off sparkedby escalating social unrest inIndonesia.Earlier, driven largely by the de-clining yen, South Korea?sstock market fell by ?
?
?
, whilethe Nikkei 225 benchmark in-dex dipped below 15,000 in themorning ?
?
?In the fourth quarter, IBMCorp.
earned $2.1 billion, up3.4 percent from $2 billion ayear earlier.3 ?
?
?
attorneys representing Pres-ident Clinton and MonicaLewinsky.The following night Isikoff ?
?
?
,where he directly followed therecitation of the top-10 list: ?Top10 White House Jobs ThatSound Dirty.
?In Washington, Ken Starr?sgrand jury continued its inves-tigation of theMonica Lewin-sky matter.4 Eight women and six men werenamed Saturday night as thefirst U.S. Olympic Snow-board Team as their sportgets set to make its debut inNagano, Japan.this tunnel is finland?s cross coun-try version of tokyo?s alpine skidome, and olympic skiers flockfrom russia, ?
?
?
, france and aus-tria this past summer to work outthe kinks ?
?
?If the skiers the men?s super-G and the women?s downhillon Saturday, they will be backon schedule.5 U.S. officials have announcedsanctions Washington will im-pose on India and Pakistanfor conducting nuclear tests.The sanctions would stop all for-eign aid except for humanitarianpurposes, ban military sales toIndia ?
?
?And Pakistan?s prime min-ister says his country will signthe U.N.?s comprehensiveban on nuclear tests if In-dia does, too.6 ?
?
?
remain in force aroundJakarta, and at the Parliamentbuilding where thousands ofstudents staged a sit-in Tues-day ?
?
?
.
?President Suharto has givenmuch to his country over thepast 30 years, raising Indone-sia?s standing in the world ?
?
?What were the students doingat the time you were there, andwhat was the reaction of thestudents to the troops?Table 4: A case study on comparative document summarization.
Some unimportant words are skipped due tothe space limit.
The bold font is used to annotate the phrases that are highly related with the topics, and italicfont is used to highlight the sentences that are not proper to be used in the summary.randomly to produce a one-sentence summary.For comparison purpose, we extract the sen-tence with the maximal degree as the base-line.
Note that the baseline can be thoughtas an approximation of the dominating setusing only one sentence.
Table 4 shows thesummaries generated by our method (comple-mentary dominating set (CDS)), discrimina-tive sentence selection (DSS) (Wang et al,2009a) and the baseline method.
Our CDSmethod can extract discriminative sentencesfor all the topics.
DSS can extract discrimina-tive sentences for all the topics except topic 4.Note that the sentence extracted by DSS fortopic 4 may be discriminative from other top-ics, but it is deviated from the topic NaganoOlympic Games.
In addition, DSS tends toselect long sentences which should not be pre-ferred for summarization purpose.
The base-line method may extract some general sen-tences, such as the sentence for topic 2 andtopic 6 in Table 4.6 ConclusionIn this paper, we propose a framework tomodel the multi-document summarization us-ing the minimum dominating set and showthat many well-known summarization taskscan be formulated using the proposed frame-work.
The proposed framework leads to sim-ple yet effective summarization methods.
Ex-perimental results show that our proposedmethods achieve good performance on severalmulti-document document tasks.7 AcknowledgementsThis work is supported by NSF grants IIS-0549280 and HRD-0833093.991ReferencesChen, Y.P.
and A.L.
Liestman.
2002.
Approximatingminimum size weakly-connected dominating sets forclustering mobile ad hoc networks.
In Proceedingsof International Symposium on Mobile Ad hoc Net-working & Computing.
ACM.Chvatal, V. 1979.
A greedy heuristic for the set-covering problem.
Mathematics of operations re-search, 4(3):233?235.Dang, H.T.
and K Owczarzak.
2008.
Overview of theTAC 2008 Update Summarization Task.
In Pro-ceedings of the Text Analysis Conference (TAC).Dang, H.T.
2007.
Overview of DUC 2007.
In Docu-ment Understanding Conference.Daume?
III, H. and D. Marcu.
2006.
Bayesian query-focused summarization.
In Proceedings of the ACL-COLING.Erkan, G. and D.R.
Radev.
2004.
Lexpagerank: Pres-tige in multi-document text summarization.
In Pro-ceedings of EMNLP.Feige, U.
1998.
A threshold of lnn for approximatingset cover.
Journal of the ACM (JACM), 45(4):634?652.Goldstein, J., V. Mittal, J. Carbonell, andM.
Kantrowitz.
2000.
Multi-document summariza-tion by sentence extraction.
In NAACL-ANLP 2000Workshop on Automatic summarization.Guha, S. and S. Khuller.
1998.
Approximation algo-rithms for connected dominating sets.
Algorithmica,20(4):374?387.Haghighi, A. and L. Vanderwende.
2009.
Exploringcontent models for multi-document summarization.In Proceedings of HLT-NAACL.Han, B. and W. Jia.
2007.
Clustering wireless adhoc networks with weakly connected dominatingset.
Journal of Parallel and Distributed Computing,67(6):727?737.Johnson, D.S.
1973.
Approximation algorithms forcombinatorial problems.
In Proceedings of STOC.Jurafsky, D. and J.H.
Martin.
2008.
Speech and lan-guage processing.
Prentice Hall New York.Kann, V. 1992.
On the approximability of NP-complete optimization problems.
PhD thesis, De-partment of Numerical Analysis and ComputingScience, Royal Institute of Technology, Stockholm.Lawrie, D., W.B.
Croft, and A. Rosenberg.
2001.Finding topic words for hierarchical summarization.In Proceedings of SIGIR.Lin, C.Y.
and E. Hovy.
2003.
Automatic evaluationof summaries using n-gram co-occurrence statistics.In Proceedings of HLT-NAACL.Mani, I.
2001.
Automatic summarization.
Computa-tional Linguistics, 28(2).Nastase, V. 2008.
Topic-driven multi-documentsummarization with encyclopedic knowledge andspreading activation.
In Proceedings of EMNLP.Nenkova, A. and L. Vanderwende.
2005.
The impactof frequency on summarization.
Microsoft Research,Redmond, Washington, Tech.
Rep. MSR-TR-2005-101.Radev, D.R., H. Jing, M.
Stys?, and D. Tam.
2004.Centroid-based summarization of multiple docu-ments.
Information Processing and Management,40(6):919?938.Raz, R. and S. Safra.
1997.
A sub-constant error-probability low-degree test, and a sub-constanterror-probability PCP characterization of NP.
InProceedings of STOC.Saggion, H., K. Bontcheva, and H. Cunningham.
2003.Robust generic and query-based summarisation.
InProceedings of EACL.Tang, J., L. Yao, and D. Chen.
2009.
Multi-topicbased Query-oriented Summarization.
In Proceed-ings of SDM.Thai, M.T., N. Zhang, R. Tiwari, and X. Xu.
2007.On approximation algorithms of k-connected m-dominating sets in disk graphs.
Theoretical Com-puter Science, 385(1-3):49?59.Wan, X., J. Yang, and J. Xiao.
2007a.
Manifold-ranking based topic-focused multi-document sum-marization.
In Proceedings of IJCAI.Wan, X., J. Yang, and J. Xiao.
2007b.
Towards aniterative reinforcement approach for simultaneousdocument summarization and keyword extraction.In Proceedings of ACL.Wang, D., T. Li, S. Zhu, and C. Ding.
2008.
Multi-document summarization via sentence-level seman-tic analysis and symmetric matrix factorization.
InProceedings of SIGIR.Wang, D., S. Zhu, T. Li, and Y. Gong.
2009a.
Com-parative document summarization via discrimina-tive sentence selection.
In Proceeding of CIKM.Wang, D., S. Zhu, T. Li, and Y. Gong.
2009b.Multi-document summarization using sentence-based topic models.
In Proceedings of the ACL-IJCNLP.Wei, F., W. Li, Q. Lu, and Y.
He.
2008.
Query-sensitive mutual reinforcement chain and its ap-plication in query-oriented multi-document summa-rization.
In Proceedings of SIGIR.Wu, J. and H. Li.
2001.
A dominating-set-based rout-ing scheme in ad hoc wireless networks.
Telecom-munication Systems, 18(1):13?36.992
