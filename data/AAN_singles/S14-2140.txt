Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 790?795,Dublin, Ireland, August 23-24, 2014.USF: Chunking for Aspect Term Identification & Polarity ClassificationCindi ThompsonUniversity of San Francisco2130 Fulton St, HR 240 San Francisco, CA 94117cathompson4@usfca.eduAbstractThis paper describes the systems submit-ted by the University of San Francisco(USF) to Semeval-2014 Task 4, AspectBased Sentiment Analysis (ABSA), whichprovides labeled data in two domains, lap-tops and restaurants.
For the constrainedcondition of both the aspect term extrac-tion and aspect term polarity tasks, we takea supervised machine learning approachusing a combination of lexical, syntactic,and baseline sentiment features.
Our ex-traction approach is inspired by a chunk-ing approach, based on its strong past re-sults on related tasks.
Our system per-formed slightly below average comparedto other submissions, possibly because weuse a simpler classification model thanprior work.
Our polarity labeling ap-proach uses two baseline hand-built sen-timent classifiers as features in additionto lexical and syntactic features, and per-formed in the top ten of other constrainedsystems on both domains.1 IntroductionAs stated in the call for participation for this Se-meval task, sentiment analysis focusing on overallpolarity of a document, sentence, or similar con-text has been well studied in recent years (Liu,2010; Pang and Lee, 2008; Tsytsarau and Pal-panas, 2012).
However, there is less prior workexamining finer levels of granularity associatedwith individual entities and their characteristicsor attributes, which the organizers for this taskcall aspects.
The aspect based sentiment analysisThis work is licenced under a Creative Commons At-tribution 4.0 International License.
Page numbers and pro-ceedings footer are added by the organizers.
License de-tails: http://creativecommons.org/licenses/by/4.0/task (ABSA) has the goal of identifying aspectsof stated or implied target entities and the senti-ment expressed towards each aspect.
This prob-lem has not been deeply studied in prior literaturedue to the lack, until now, of a large gold standarddataset.
This Semeval task has provided two suchdatasets, in the domains of laptops and restaurants.A full description of the task and data is presentedwith this volume (Pontiki et al., 2014).In this paper, we discuss our approach to thefirst two subtasks of the Semeval ABSA Task,those of aspect term extraction and aspect termpolarity.
In aspect term extraction the domain(restaurants or laptops) is known and the goal isto identify terms in a sentence that are featurescommonly associated with that domain, such asservice and staff in the case of restaurants or sizeand speed in the case of laptops.
In the polaritysubtask, the aspect terms for a given sentence arealready identified and the sentiment polarity (pos-itive, negative, conflict, or neutral) must be as-signed.We approach both subtasks using supervisedmachine learning with background knowledge ofsentiment lexicons and syntax included in our fea-ture set.
Our goal was to investigate whether tech-niques that have been successful in similar taskswould perform well on this newly created dataset.
We did not use additional corpus-based re-sources, so qualified for the constrained (versusunconstrained) version of the task.
The remain-der of the paper details related work, our approach,and experiments and the results we obtained.2 Related WorkWe divide related work into two areas: research re-lated to aspect and aspect term identification, andresearch related to sentiment classification for as-pect terms.
We note that aspects have also beencalled topics and features in prior work.
Un-til more recently, the community lacked a corpus790of gold-standard labeled data that focuses on as-pect terms, rather than more general expressionsof subjectivity or other private states.
Thus, earlywork focused on learning or identifying aspectsin an unsupervised (Hu and Liu, 2004) or semi-supervised setting (Moghaddam and Ester, 2010;Zhai et al., 2011).
The earliest work on aspectdetection focused on identifying frequently oc-curring noun phrases using information extraction(IE) techniques (Hu and Liu, 2004).
Unsupervisedtechniques include clustering (Fahrni and Klenner,2008; Popescu and Etzioni, 2005) and topic mod-els (Titov and McDonald, 2008).The benchmark corpus for sentiment analysisfrom Wiebe et al.
(2005) inspired much work onlearning subjective phrases in a supervised set-ting.
The nature of the data and annotation differfrom the data for this Semeval task, as it focuseson news articles and identifying an entire opinionphrase, including the source of the opinion, andonly recently added aspect annotations.
However,the techniques used by others to learn to extractthis data and the associated polarity inspired ourown approach.
These include extraction-like ap-proaches, usually using sequence modeling (Brecket al., 2007; Jin et al., 2009; Johansson andMoschitti, 2013; Li et al., 2010; Mitchell et al.,2013; Yang and Cardie, 2013) and semantic de-pendency or semantic parsing approaches (Kimand Hovy, 2006; Kobayashi et al., 2007; Wu etal., 2009) sometimes using background knowl-edge from sentiment lexicons (Zhang et al., 2009).The main differences between our approach andthat of Breck et al.
(2007) and Mitchell et al.
(2013) are the classifier used and some of the fea-tures; they both use CRFs versus our Maximumentropy classifier, and they used a wider range ofsyntactic and dictionary-based features.A second related corpus which includes moreaspect information is that developed by Kim andHovy (2006).
This corpus also focuses on newsarticles rather than reviews, but does expand thetypes of aspects identified.
The main focus of thatwork is on the identification, using FrameNet rolelabels, of the holder and target of an opinion, whilethe opinion itself is provided to the system.The restaurant reviews used in this Semeval taskare a 3000-plus sentence subset of those harvestedby Ganu et al.
(2009), plus newly annotated sen-tences used for test data.
The original corpus con-tains over 50,000 structured restaurant reviews in-cluding restaurant information and a star rating.The original star rating was not made available forthe Semeval tasks, and the aspect term annotationsand their associated sentiment were added for thistask; the original sentence-level sentiment annota-tions were not provided.
Most of the work explor-ing this corpus to date uses unsupervised (Brodyand Elhadad, 2010) or semi-supervised (Mukher-jee and Liu, 2012) approaches.As there has been an explosion of research insentiment classification, it is impossible to reviewall of the related work.
See Tsytsarau and Palpanas(2012) for a recent survey.
We will note that ourapproach follows a somewhat standard machinelearning approach inspired by that of Wilson et al.
(2005), but with a different feature set.
We didnot thoroughly explore as many classifiers as thiswork and others have done.
Finally, we note thatsome work has investigated the joint task of iden-tifying opinion phrases or targets simultaneouslywith polarity (Choi and Cardie, 2009; Johanssonand Moschitti, 2013; Mitchell et al., 2013).3 ApproachFor both subtasks, we take a supervised ma-chine learning approach, examining several classi-fiers and their variants, and converging on featuresets which performed best in small-scale cross-validation experiments.
After the official com-petition ended, we continued to examine differ-ent variants and discuss alternative approaches andtheir accuracy in the experimental results section.For all tasks we use the Maximum Entropy clas-sifier, ?iib?
variant from the Natural LanguageToolkit (NLTK) in Python (Bird et al., 2009).
Weexperimented with several other classifiers fromNLTK and found that Maximum Entropy per-formed best on a hold out set of data.
We had orig-inally planned to use a Conditional Random Field(CRF) model (Lafferty et al., 2001) because of itsstrong performance on similar tasks, but met withtime limitations when converting the data to theappropriate format (there is no CRF provided withNLTK at this time).
We had also planned to tryclassifiers from the scikit-learn toolkit (Pedregosaet al., 2011), but again met with time constraintsdue to the necessity to manually convert the fea-tures to a binary representation.We first preprocess the data using NLTK?s tok-enization and part-of-speech tagging modules andalign the results with the aspect terms in the data,791as detailed further below.
The sentiment lexiconwe use as the basis of all sentiment features dis-cussed below combines two standard lexicons (Liuet al., 2005; Wilson et al., 2005).3.1 Aspect Term ExtractionWhile it is difficult to give a precise definition ofaspect, it can be roughly thought of as a charac-teristic of a target concept such as a restaurant orlaptop.
Examples include the italicized terms inthe following:?
I liked the service and the staff, but not thefood.?
The hard disk is very noisy.We use a sequence labeling approach, whichcan also be thought of as a tagging or chunkingapproach, to identify the aspect terms in each sen-tence.
Specifically, and similar to Breck et al.
(2007) and Mitchell et al.
(2013), as the targetclass for each token, we use the IOB2 sequencelabeling scheme (Tjong et al., 2000), where theaspect terms are considered as the chunks to be la-beled.
Using this approach, each token is taggedas either Beginning an aspect term, being In an as-pect term, or being Outside an aspect term.
Wealso experimented with an IO labeling scheme asdiscussed in the experimental results section, inwhich each token is tagged as being either In orOutside an aspect term.
Here is an example of asentence with its IOB tags:?
The-O pizza-B is-O the-O best-O if-O you-Olike-O thin-B crusted-I pizza-I .-OOf course, unlike an HMM or CRF, a standardclassifier such as Maximum entropy does not la-bel entire sequences.
Therefore, each examplepresented to our classifier represents a single to-ken from the sentence being labeled, and the tar-get label is the IOB tag of that token.
Further,we present the tokens of a given sentence in orderfrom the first word in the sentence to the last.The features used for each token are derivedfrom the token, the prior token, and the next tokenin the sentence (thus using a three-token window).In addition, we include the IOB tag of the prior to-ken, using the gold standard at training time andthe classifier?s output at testing time, even if it isincorrect.
For each token we extract the word,its stem, its part-of-speech (POS) tag, its polar-ity from the sentiment dictionary, and whether theword is objective or subjective, from the same sen-timent dictionary.
We use dummy values for theprior and next words of the first and last token ina sentence, respectively.
All feature-value pairsare converted to binary features automatically byNLTK.Because we believed that the data would proveto be sparse and that new words would appear inthe testing data, we also include an unknown wordfeature, replacing the 50% least frequent wordsin the training data with the ?UNK?
token, anddoing the same for both these words and unseenwords in the test set.
However, we later found thatwe should have used cross-validation to supportour hypothesis, and that using the full vocabularywould have improved our results, as shown in theexperimental results section.3.2 PolarityIn the polarity subtask, the aspect terms are pro-vided, and the goal is to classify them as posi-tive, negative, conflict, or neutral.
In this case,we use a simple classification approach that in-cludes features of the aspect term and surround-ing tokens (again in a three-token window), andalso some simple baseline sentiment classificationfeatures.
First, we use similar features as for theaspect term extraction task, with changes to incor-porate the fact that aspect terms are occasionallyphrases, not single words.
In fact, we hypothesizethat features of the words before and after an as-pect phrase could be more useful than the wordsprior to and after a particular word in the phrase.Thus, instead of using features from the three-token window including the current token, we usefeatures from the words on each side of the as-pect phrase, and use the head of the aspect phraseand its features as the middle of the window.
Thisapproach is similar to that of Johansson and Mos-chitti (2013), who use features from the words be-fore and after opinion expressions.
In our case,these features are again the word, its POS tag, itssentiment polarity and objectivity, and its IOB tag.Note that in this case we use the IOB tag from allterms in the window, since the aspect term extrac-tion task is treated as a prerequisite to the polarityclassification task.In addition to these word-based features, weadd four higher-level features.
The first is an in-dicator of the number of aspect terms in the en-tire sentence, since this might indicate a more de-792tailed sentence, and we believe that more specificsentences might correlate with positive sentiment.The other three features are baselines connected tothe estimated sentiment of the sentence or phrase.First, we apply a hand-built sentence level senti-ment classifier that follows a now standard base-line approach (Zhu et al., 2009): using a senti-ment lexicon (Liu?s), it counts the number of posi-tive and negative sentiment words in the sentence,flipping polarity when negation words are encoun-tered, and discontinuing the polarity flip whenpunctuation is encountered.
This results in a ?highlevel sentiment?
feature consisting of the numberof positive sentiment words minus the number ofnegative sentiment words.Our other two sentiment features provide finergranularity information, based on the sentimentof the ?chunks?
in which an aspect term appears.First, we use the punctuation within the sentenceto divide it into punctuation-separated chunks.Then, we calculate the number of positive and neg-ative sentiment words within each chunk, againflipping polarity after the presence of a negationword.
The positive and negative counts associatedwith the chunk within which an aspect phrase ap-pears are then used as features when classifyingthe phrase.
We also experimented with using con-junctions (and, or, but, etc.)
as chunk boundaries,but preliminary results indicated that this resultedin reduced accuracy.4 Experimental Results & AnalysisIn this section we report our results and some ad-ditional analysis for the ABSA subtasks 1 and 2.Please refer to Pontiki et al.
(2014) for details onthe tasks, corpora, and evaluation criteria.
Wechose the constrained condition, which allows theuse of sentiment lexicons in addition to the train-ing data provided, but no additional data such asother reviews.Aspect term extraction is evaluated using Pre-cision, Recall, and F-measure on an unseen set ofsentences.
Table 1 shows our results1on both do-mains, the top results,2and the mean score of allconstrained submissions (21 entries).
Note that forRestaurants, COMMIT-P1WP3 had the best Preci-sion, at 0.909, but XRCE had the best F-measure,so we show their three scores.
Our results wereclose to the mean for both corpora and quite a1Rank averaged over P, R, and F for USF2We abbreviate IHS RD Belarus as Belarus.System P R F1 RankLapBelarus 0.848 0.665 0.746 1mean 0.760 0.503 0.562 11USF 0.754 0.404 0.526 14.7baseline 0.443 0.298 0.356RestXRCE 0.862 0.818 0.840 1mean 0.770 0.649 0.693 11USF 0.783 0.645 0.707 14.3baseline 0.525 0.428 0.472Table 1: Aspect Term Extraction Results, Con-strained.Approach P R F1LapFV-No-Snt 0.724 0.622 0.669Full Voc.
0.733 0.601 0.660Original 0.715 0.493 0.583IO 0.696 0.501 0.582RestFull Voc.
0.792 0.704 0.746FV-No-Snt 0.784 0.710 0.745Original 0.777 0.657 0.711IO 0.769 0.660 0.710Table 2: Aspect Term Extraction Cross-ValidationResults.bit above the lowest scoring submissions and thebaseline provided by the organizers; the latter isalso shown in the Table.After the submission deadline, we continued toexperiment with alternative approaches using 5-fold cross validation on the training set, shown inTable 2.
We found that using the full vocabularywas better than our original approach of only us-ing the top 50% occurring words, even with 28%unseen words in the restaurant test set and 21% inlaptops.
We also found that leaving out the polar-ity feature while using all vocabulary words (FV-No-Snt) improved our F-measure score to 0.669for laptops but reduced it slightly to 0.745 forrestaurants.
Finally, using IO versus IOB tag-ging did not influence the F-measure significantly.About 25% of the aspect terms in the restauranttraining set have length greater than one, and 37%of the laptop terms.Aspect term polarity is evaluated on accuracyover all labels: positive, negative, neutral, or con-flict.
Table 3 shows our results on both domains,the top results, the mean score of all constrainedsubmissions (24 entries for laptops, 28 for restau-rants), and the baseline accuracy.
In this case ourscores are above average in all cases.793System Acc RankLapNRC-Canada 0.705 1USF 0.645 6mean 0.604 12.5baseline 0.514RestDCU 0.810 1USF 0.732 9mean 0.702 14.5baseline 0.643Table 3: Aspect Term Polarity Results, Con-strained.5 ConclusionsIn conclusion, we show that a chunking approachto supervised learning works fairly well in theaspect term extraction task, and that local sen-tence features and a baseline sentiment classifierwork well for aspect term polarity classification.Our systems for both tasks performed reasonablywell considering the relatively simple classifica-tion techniques and features incorporated.
In fu-ture work, we plan to apply more sophisticatedclassifiers which have shown to be accurate on re-lated tasks, including CRFs and Support VectorMachines.
We also would like to experiment withvariants of the features used here, such as the ex-ploration of smaller or larger context windows, orthe usefulness of stemming compared to the orig-inal tokens.
We also believe that more sophisti-cated syntactic or semantic features, or topic mod-els, could improve results on one or both tasks.We thank the organizers for the provision of thisinteresting dataset.AcknowledgementsThe author thanks her spring 2014 research assis-tant, Hao Chen, for his help in preparing some ofthe code used in the experiments.ReferencesSteven Bird, Edward Loper, and Ewan Klein.2009.
Natural Language Processing with Python.O?Reilly Media Inc.Eric Breck, Yejin Choi, and Claire Cardie.
2007.
Iden-tifying expressions of opinion in context.
In Twen-tieth International Joint Conference on Artificial In-telligence, pages 2683?2688.Samuel Brody and Noemie Elhadad.
2010.
An unsu-pervised aspect-sentiment model for online reviews.In Human Language Technologies: The 2010 An-nual Conference of the North American Chapterof the Association for Computational Linguistics,pages 804?812.Yejin Choi and Claire Cardie.
2009.
Adapting a po-larity lexicon using integer linear programming fordomain-specific sentiment classification.
In Pro-ceedings of the 2009 Conference on Empirical Meth-ods in Natural Language Processing, pages 590?598.Angela Fahrni and Manfred Klenner.
2008.
Old wineor warm beer: Target-specific sentiment analysis ofadjectives.
In Proc.
of the Symposium on AffectiveLanguage in Human and Machine, AISB, pages 60?63.Gayatree Ganu, Noemie Elhadad, and Amelie Marian.2009.
Beyond the stars: Improving rating predic-tions using review text content.
In 12th Interna-tional Workshop on the Web and Databases, pages1?6.Minqing Hu and Bing Liu.
2004.
Mining and summa-rizing customer reviews.
In Proceedings of the tenthACM SIGKDD international conference on Knowl-edge discovery and data mining, pages 168?177.ACM.Wei Jin, Hung Hay Ho, and Rohini K Srihari.
2009.
Anovel lexicalized HMM-based learning frameworkfor web opinion mining.
In Proceedings of the26th Annual International Conference on MachineLearning, pages 465?472.Richard Johansson and Alessandro Moschitti.
2013.Relational features in fine-grained opinion analysis.Computational Linguistics, 39(3):473?509.Soo-min Kim and Eduard Hovy.
2006.
Extractingopinions, opinion holders, and topics expressed inonline news media text.
In Workshop on Sentimentand Subjectivity in Text, pages 1?8.Nozomi Kobayashi, Kentaro Inui, and Yuji Matsumoto.2007.
Extracting aspect-evaluation and aspect-ofrelations in opinion mining.
In EMNLP-CoNLL,pages 1065?1074.John Lafferty, Andrew McCallum, and FernandoPereira.
2001.
Conditional random fields: Prob-abilistic models for segmenting and labeling se-quence data.
In Proceedings of International Con-ference on Machine Learning, pages 282?289.Fangtao Li, Chao Han, Minlie Huang, and XiaoyanZhu.
2010.
Structure-aware review mining andsummarization.
In Proceedings of the 23rd Inter-national Conference on Computational Linguistics,pages 653?661.Bing Liu, Minqing Hu, and Junsheng Cheng.
2005.Opinion observer: Analyzing and comparing opin-ions on the web.
In Proceedings of the 14th Interna-tional World Wide Web conference, pages 342?351.ACM.794Bing Liu.
2010.
Sentiment analysis and subjectiv-ity.
In Handbook of Natural Language Processing2, pages 627?666.
CRC Press.Margaret Mitchell, Jacqueline Aguilar, Theresa Wil-son, and Benjamin Van Durme.
2013.
Open domaintargeted sentiment.
In EMNLP, pages 1643?1654.Samaneh Moghaddam and Martin Ester.
2010.
Opin-ion digger: An unsupervised opinion miner fromunstructured product reviews.
In Proceedings ofthe 19th ACM international conference on Informa-tion and knowledge management, pages 1825?1828.ACM.Arjun Mukherjee and Bing Liu.
2012.
Aspect extrac-tion through semi-supervised modeling.
In Proceed-ings of the 50th Annual Meeting of the Associationfor Computational Linguistics, pages 339?348.Bo Pang and Lillian Lee.
2008.
Opinion mining andsentiment analysis.
Foundations and Trends in In-formation Retrieval, 2(1-2):1?135.Fabian Pedregosa, Ga?el Varoquaux, Alexandre Gram-fort, Vincent Michel, Bertrand Thirion, OlivierGrisel, Mathieu Blondel, Peter Prettenhofer, RonWeiss, Vincent Dubourg, Jake Vanderplas, Alexan-dre Passos, David Cournapeau, Matthieu Brucher,Matthieu Perrot, and?Edouard Duchesnay.
2011.Scikit-learn: Machine learning in Python.
Journalof Machine Learning Research, 12:2825?2830.Maria Pontiki, Dimitrios Galanis, John Pavlopou-los, Harris Papageorgiou, Ion Androutsopoulos, andSuresh Manandhar.
2014.
Semeval-2014 task 4:Aspect based sentiment analysis.
In Proceedings ofthe 8th International Workshop on Semantic Evalu-ation (SemEval 2014).Ana-maria Popescu and Oren Etzioni.
2005.
Extract-ing product features and opinions from reviews.
InProceedings of Human Language Technology Con-ference and Conference on Empirical Methods inNatural Language Processing, pages 339?346.Ivan Titov and Ryan McDonald.
2008.
Modelingonline reviews with multi-grain topic models.
InProceeding of the 17th international conference onWorld Wide Web, pages 111?120, New York, NewYork, USA.
ACM.Erik F Tjong, Kim Sang, Walter Daelemans, RobKoeling, Yuval Krymolowski, Vasin Punyakanok,Dan Roth, Millers Yard, Mill Lane, and RamatGan.
2000.
Applying system combination to basenoun phrase identification.
In Proceedings of the18th conference on Computational linguistics, pages857?863.Mikalai Tsytsarau and Themis Palpanas.
2012.
Surveyon mining subjective data on the web.
Data Miningand Knowledge Discovery, 24(3):478?514.Janyce Wiebe, Theresa Wilson, and Claire Cardie.2005.
Annotating expressions of opinions and emo-tions in language.
Language Resources and Evalu-ation, 39(2-3):165?210.Theresa Wilson, Janyce Wiebe, and Paul Hoffmann.2005.
Recognizing contextual polarity in phrase-level sentiment analysis.
In Proceedings of the con-ference on Human Language Technology and Em-pirical Methods in Natural Language Processing,pages 347?354.Yuanbin Wu, Qi Zhang, Xuanjing Huang, and Lide Wu.2009.
Phrase dependency parsing for opinion min-ing.
In Proceedings of the 2009 Conference on Em-pirical Methods in Natural Language Processing,pages 1533?1541.Bishan Yang and Claire Cardie.
2013.
Joint inferencefor fine-grained opinion extraction.
In Proceedingsof ACL, pages 16550?1649.Zhongwu Zhai, Bing Liu, Hua Xu, and Peifa Jia.
2011.Clustering product features for opinion mining.
InProceedings of the fourth ACM international con-ference on Web search and data mining, pages 347?354, New York, New York, USA.
ACM.Qi Zhang, Yuanbin Wu, Tao Li, Mitsunori Ogihara,Joseph Johnson, and Xuanjing Huang.
2009.
Min-ing product reviews based on shallow dependencyparsing.
In Proceedings of the 32nd internationalACM SIGIR conference on Research and develop-ment in information retrieval, pages 726?727.
ACM.Jingbo Zhu, Muhua Zhu, Huizhen Wang, and Ben-jamin Tsou.
2009.
Aspect-based sentence segmen-tation for sentiment summarization.
In Proceedingsof the 1st international CIKM workshop on Topic-sentiment analysis for mass opinion, pages 65?72.ACM.795
