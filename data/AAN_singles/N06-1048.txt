Proceedings of the Human Language Technology Conference of the North American Chapter of the ACL, pages 375?382,New York, June 2006. c?2006 Association for Computational LinguisticsNuggeteer: Automatic Nugget-Based Evaluationusing Descriptions and JudgementsGregory Marton Alexey RadulInfolab Group, MIT CSAILCambridge, MA 02139{gremio,axch}@mit.eduAbstractThe TREC Definition and Relationshipquestions are evaluated on the basis of in-formation nuggets that may be containedin system responses.
Human evalua-tors provide informal descriptions of eachnugget, and judgements (assignments ofnuggets to responses) for each responsesubmitted by participants.
While humanevaluation is the most accurate way tocompare systems, approximate automaticevaluation becomes critical during systemdevelopment.We present Nuggeteer, a new automaticevaluation tool for nugget-based tasks.Like the first such tool, Pourpre, Nugge-teer uses words in common between can-didate answer and answer key to approx-imate human judgements.
Unlike Pour-pre, but like human assessors, Nuggeteercreates a judgement for each candidate-nugget pair, and can use existing judge-ments instead of guessing.
This cre-ates a more readily interpretable aggregatescore, and allows developers to track in-dividual nuggets through the variants oftheir system.
Nuggeteer is quantitativelycomparable in performance to Pourpre,and provides qualitatively better feedbackto developers.1 IntroductionThe TREC Definition and Relationship questionsare evaluated on the basis of information nuggets,abstract pieces of knowledge that, taken together,comprise an answer.
Nuggets are described infor-mally, with abbreviations, misspellings, etc., andeach is associated with an importance judgement:?vital?
or ?okay?.1 In some sense, nuggets are likeWordNet synsets, and their descriptions are likeglosses.
Responses may contain more than onenugget?when they contain more than one piece ofknowledge from the answer.
The median scores oftoday?s systems are frequently zero; most responsescontain no nuggets (Voorhees, 2005).Human assessors decide what nuggets make up ananswer based on some initial research and on poolsof top system responses for each question.
Answerkeys list, for each nugget, its id, importance, anddescription; two example answer keys are shownin Figures 1 and 2.
Assessors make binary deci-sions about each response, whether it contains eachnugget.
When multiple responses contain a nugget,the assessor gives credit only to the (subjectively)best response.Using the judgements of the assessors, the fi-nal score combines the recall of the available vi-tal nuggets, and the length (discounting whitespace)of the system response as a proxy for precision.Nuggets valued ?okay?
contribute to precision by in-creasing the length allowance, but do not contributeto recall.
The scoring formula is shown in Figure 3.1Nuggeteer implements the pyramid scoring system from(Lin and Demner-Fushman, 2006), designed to soften the dis-375Qid 87.8: ?other?
question for target Enrico Fermi1 vital belived in partical?s existence and named it neutrino2 vital Called the atomic Bomb an evil thing3 okay Achieved the first controlled nuclear chain reaction4 vital Designed and built the first nuclear reactor5 okay Concluded that the atmosphere was in no real danger before Trinity test6 okay co-developer of the atomic bomb7 okay pointed out that the galaxy is 100,000 light years acrossFigure 1: The ?answer key?
to an ?other?
question from 2005.The analyst is looking for links between Colombian businessmen and paramilitary forces.
Specif-ically, the analyst would like to know of evidence that business interests in Colombia are stillfunding the AUC paramilitary organization.1 vital Commander of the national paramilitary umbrella organization claimed his group enjoysgrowing support from local and international businesses2 vital Columbia?s Chief prosecutor said he had a list of businessmen who supported right-wingparamilitary squads and warned that financing outlawed groups is a criminal offense3 okay some landowners support AUC for protections services4 vital Rightist militias waging a dirty war against suspected leftists in Colombia enjoy growingsupport from private businessmen5 okay The AUC makes money by taxing Colombia?s drug trade6 okay The ACU is estimated to have 6000 combatants and has links to government security forces.7 okay Many ACU fighters are former government soldiersFigure 2: The ?answer key?
to a relationship question.Letr # of vital nuggets returned in a responsea # of okay nuggets returned in a responseR # of vital nuggets in the answer keyl # of non-whitespace characters in the entireanswer stringThen?recall?
R = r/R?allowance?
?
= 100 ?
(r + a)?precision?
P ={ 1 if l < ?1 ?
l?
?l otherwiseFinally, the F (?)
= (?2 + 1) ?
P ?R?2 ?
P + RFigure 3: Official definition of F-measure.Automatic evaluation of systems is highly desir-able.
Developers need to know whether one sys-tem performs better or worse than another.
Ideally,they would like to know which nuggets were lost orgained.
Because there is no exhaustive list of snip-pets from the document collection that contain eachnugget, an exact automatic solution is out of reach.Manual evaluation of system responses is too timeconsuming to be effective for a development cycle.The Qaviar system first described an approximateautomatic evaluation technique using keywords, andPourpre was the first publicly available implemen-tation for these nugget-based tasks.
(Breck et al,2000; Lin and Demner-Fushman, 2005).
Pourprecalculates an idf - or count-based, stemmed, unigramsimilarity between each nugget description and eachtinction between ?vital?
and ?okay?.376candidate system response.
If this similarity passes athreshold, then it uses this similarity to assign a par-tial value for recall and a partial length allowance,reflecting the uncertainty of the automatic judge-ment.
Importantly, it yields a ranking of systemsvery similar to the official ranking (See Table 2).Nuggeteer offers three important improvements:?
interpretability of the scores, as compared toofficial scores,?
use of known judgements for exact informationabout some responses, and?
information about individual nuggets, for de-tailed error analysis.Nuggeteer makes scores interpretable by makingbinary decisions about each nugget and each systemresponse, just as assessors do, and then calculatingthe final score in the usual way.
We will show thatNuggeteer?s absolute error is comparable to humanerror, and that the 95% confidence intervals Nugge-teer reports are correct around 95% of the time.Nuggeteer assumes that if a system response wasever judged by a human assessor to contain a partic-ular nugget, then other identical responses also con-tain that nugget.
When this is not true among the hu-man judgements, we claim it is due to annotator er-ror.
This assumption allows developers to add theirown judgements and have the responses they?ve ad-judicated scored ?exactly?
by Nuggeteer.These features empower developers to track notonly the numeric value of a change to their system,but also its effect on retrieval of each nugget.2 ApproachNuggeteer builds one binary classifier per nugget foreach question, based on n-grams (up to trigrams)in the description and optionally in any providedjudgement files.
The classifiers use a weight foreach n-gram, an informativeness measure for eachn-gram, and a threshold for accepting a response asbearing the nugget.2.1 N -gram weightThe idf -based weight for an n-gram w1...wn is thesum of unigram idf counts from the AQUAINTcorpus of English newspaper text, the corpus fromwhich responses for the TREC tasks are drawn.
Wedid not explore using n-gram idfs.
A tf componentis not meaningful because the data are so sparse.2.2 InformativenessLet G be the set of nuggets for some question.
Infor-mativeness of an n-gram for a nugget g is calculatedbased on how many other nuggets in that question(?
G) contain the n-gram.
Leti(g, w1...wn) ={ 1 if count(g, w1..wn) > 00 otherwise(1)where count(g, w1...wn) is the number of occur-rences of the n-gram in responses containing thenugget g.Then informativeness is:I(g, w1...wn) = 1 ??g?
?G i(g?, w1...wn)|G| (2)This captures the Bayesian intuition that the moreoutcomes a piece of evidence is associated with, theless confidence we can have in predicting the out-come based on that evidence.2.3 JudgementNuggeteer does not guess on responses which havebeen judged by a human to contain a nugget, or thosewhich have unambiguously judged not to, but as-signs the known judgement.2For unseen responses, we determine the n-gramrecall for each nugget g and candidate responsew1...wl by breaking the candidate into n-grams andfinding the sum of scores:Recall(g, w1...wl) = (3)n?1?k=0l?k?i=0W (g, wi...wi+k) ?
I(g, wi...wi+k)The candidate is considered to contain all nuggetswhose recall exceeds some threshold.
Put another2If a response was submitted, and no response from the samesystem was judged to contain a nugget, then the response is con-sidered to not contain the nugget.
We normalized whitespaceand case for matching previously seen responses.377way, we build an n-gram language model for eachnugget, and assign those nuggets whose predictedlikelihood exceeds a threshold.When several responses contain a nugget, Nugge-teer picks the first (instead of the best, as assessorscan) for purposes of scoring.2.4 Parameter EstimationWe explored a number of parameters in the scor-ing function: stemming, n-gram size, idf weightsvs.
count weights, and the effect of removing stop-words.
We tested all 24 combinations, and for eachexperiment, we cross-validated by leaving out onesubmitted system, or where possible, one submittinginstitution (to avoid training and testing on poten-tially very similar systems).3Each experiment was performed using a rangeof thresholds for Equation 3 above, and we se-lected the best performing threshold for each dataset.4 Because the threshold was selected after cross-validation, it is exposed to overtraining.
We used asingle global threshold to minimize this risk, but wehave no reason to think that the thresholds for differ-ent nuggets are related.Selecting thresholds as part of the training processcan maximize accuracy while eliminating overtrain-ing.
We therefore explored Bayesian models for au-tomatic threshold selection.
We model assignmentof nuggets to responses as caused by the scores ac-cording to a noisy threshold function, with separatefalse positive and false negative error rates.
We var-ied thresholds and error rates by entire dataset, byquestion, or by individual nugget, evaluating themusing Bayesian model selection.3 The DataFor our experiments, we used the definition ques-tions from TREC2003, the ?other?
questions fromTREC2004 and TREC2005, and the relation-ship questions from TREC2005.
(Voorhees, 2003;Voorhees, 2004; Voorhees, 2005) The distributionof nuggets and questions is shown for each data setin Table 1.
The number of nuggets by number of3For TREC2003 and TREC2004, the run-tags indicate thesubmitting institution.
For TREC2005 we did not run the non-anonymized data in time for this submission.
In the TREC2005Relationship task, RUN-1 was withdrawn.4Thresholds for Pourpre were also selected this way.00.050.10.150.20.250.30 2 4 6 8 10 12 14 16 18 20 22 24 26 28 !30D2003 / 54O2004 / 63O2005 / 72R2005 / 10Figure 4: Percents of nuggets, binned by the numberof systems that found each nugget.system responses assigned that nugget (difficulty ofnuggets, in a sense) is shown in Figure 4.
More thana quarter of relationship nuggets were not found byany system.
Among all data sets, many nuggets werefound in none or just a few responses.4 ResultsWe report correlation (R2), and Kendall?s ?b, follow-ing Lin and Demner-Fushman.
Nuggeteer?s scoresare in the same range as real system scores, so wealso report average root mean squared error from theofficial results.
We ?corrected?
the official judge-ments by assigning a nugget to a response if thatresponse was judged to contain that nugget in anyassessment for any system.4.1 Comparison with Pourpre(Lin et al, 2005) report Pourpre and Rouge perfor-mance with Pourpre optimal thresholds for TRECdefinition questions, as reproduced in Table 2.Nuggeteer?s results are shown in the last column.5Table 3 shows a comparison of Pourpre andNuggeteer?s correlations with official scores.
As ex-5We report only micro-averaged results, because we wish toemphasize the interpretability of Nuggeteer scores.
While thecorrelations of macro-averaged scores with official scores maybe higher (as seems to be the case for Pourpre), the actual val-ues of the micro-averaged scores are more interpretable becausethey include a variance.378#ques #vital #okay #n/q #sys #r/s #r/q/sD 2003: 50 207 210 9.3?
1.0 54 526?
180 10.5?
1.2O 2004: 64 234 346 10.1?
.7 63 870?
335 13.6?
0.9O 2005: 75 308 450 11.1?
.6 72 1277?
260a 17.0?
0.6aR 2005: 25 87 136 9.9?
1.6 10 379?
222b 15.2?
1.6ba excluding RUN-135: 410,080 responses 5468 ?
5320b excluding RUN-7: 6436 responses 257 ?
135Table 1: For each data set (D=?definition?, O=?other?, R=?relationship?
), the number of questions, thenumbers of vital and okay nuggets, the average total number of nuggets per question, the number of par-ticipating systems, the average number of responses per system, and the average number of responses perquestion over all systems.POURPRE ROUGE NUGGETEERRun micro, cnt macro, cnt micro, idf macro, idf default stop nostem, bigram,micro, idfD 2003 (?
= 3) 0.846 0.886 0.848 0.876 0.780 0.816 0.879D 2003 (?
= 5) 0.890 0.878 0.859 0.875 0.807 0.843 0.849O 2004 (?
= 3) 0.785 0.833 0.806 0.812 0.780 0.786 0.898O 2005 (?
= 3) 0.598 0.709 0.679 0.698 0.662 0.670 0.858R 2005 (?
= 3) 0.697 1Table 2: Kendall?s ?
correlation between rankings generated by POURPRE/ROUGE/NUGGETEER and offi-cial scores, for each data set (D=?definition?, O=?other?, R=?relationship?).
?=1 means same order, ?=-1means reverse order.
Pourpre and Rouge scores reproduced from (Lin and Demner-Fushman, 2005).POURPRE NUGGETEERRun R2 R2 ?mseD 2003 (?
= 3) 0.963 0.966 0.067D 2003 (?
= 5) 0.965 0.971 0.077O 2004 (?
= 3) 0.929 0.982 0.026O 2005 (?
= 3) 0.916 0.952 0.026R 2005 (?
= 3) 0.764 0.993 0.009Table 3: Correlation (R2) and Root Mean SquaredError (?mse) between scores generated by Pour-pre/Nuggeteer and official scores, for the same set-tings as the ?
comparison above.pected from the Kendall?s ?
comparisons, Pourpre?scorrelation is about the same or higher in 2003, butfares progressively worse in the subsequent tasks.To ensure that Pourpre scores correlated suf-ficiently with official scores, Lin and Demner-Fushman used the difference in official score be-tween runs whose ranks Pourpre had swapped, andshowed that the majority of swaps were betweenruns whose official scores were less than the 0.1apart, a threshold for assessor agreement reportedin (Voorhees, 2003).Nuggeteer scores are not only correlated with,but actually meant to approximate, the assessmentscores; thus we can use a stronger evaluation: rootmean squared error of Nuggeteer scores against of-ficial scores.
This estimates the average differencebetween the Nuggeteer score and the official score,and at 0.077, the estimate is below the 0.1 thresh-old.
This evaluation is meant to show that thescores are ?good enough?
for experimental evalua-tion, and in Section 4.4 we will substantiate Lin andDemner-Fushman?s observation that higher correla-tion scores may reflect overtraining rather than ac-tual improvement.Accordingly, rather than reporting the bestNuggeteer scores (Kendall?s ?
and R2) above, wefollow Pourpre?s lead in reporting a single variant(no stemming, bigrams) that performs well acrossthe data sets.
As with Pourpre?s evaluation, the par-379Figure 5: Scatter graph of official scores plot-ted against Nuggeteer scores (idf term weighting,no stemming, bigrams) for each data set (all F-measures have ?
= 3), with the Nuggeteer 95%confidence intervals on the score.
Across the fourdatasets, 6 systems (3%) have an official score out-side Nuggeteer?s 95% confidence interval.ticular thresholds for each year are experimentallyoptimized.
A scatter plot of Nuggeteer performanceon the definition tasks is shown in Figure 5.4.2 N -gram size and stemmingA hypothesis advanced with Pourpre is that bigrams,trigrams, and longer n-grams will primarily accountfor the fluency of an answer, rather than its semanticcontent, and thus not aid the scoring process.
Weincluded the option to use longer n-grams withinNuggeteer, and have found that using bigrams canyield very slightly better results than using uni-grams.
From inspection, bigrams sometimes capturenamed entity and grammatical order features.Experiments with Pourpre showed that stemminghurt slightly at peak performances.
Nuggeteer hasthe same tendency at all n-gram sizes.Figure 6 compares Kendall?s ?
over the possi-ble thresholds, n-gram lengths, and stemming.
Thechoice of threshold matters by far the most.4.3 Term weighting and stopwordsRemoving stopwords or giving unit weight to allterms rather than an idf -based weight made no sub-stantial difference in Nuggeteer?s performance.Figure 6: Fixed thresholds vs. Kendall?s ?
for uni-grams, bigrams, or trigrams averaged over the threeyears of definition data using F (?
= 3).Model log10 P (Data|Model)optimally biased coin -2780global threshold -2239per-question thresholds -1977per-nugget thresholds -1546per-nugget errors and thr.
-1595Table 4: The probabilities of the data given severalmodels: a baseline coin, three models of differentgranularity with globally specified false positive andnegative error rates, and a model with too many pa-rameters, where even the error rates have per-nuggetgranularity.
We select the most probable model, theper-nugget threshold model.4.4 ThresholdsWe experimented with Bayesian models for auto-matic threshold selection.
In the models, a systemresponse contains or does not contain each nuggetas a function of the response?s Nuggeteer score plusnoise.
Table 4 shows that, as expected, the best mod-els do not make assumptions about thresholds be-ing equal within a question or dataset.
It is interest-ing to note that Bayesian inference catches the over-parametrization of the model where error rates varyper-nugget as well.
In essence, we do not need thoseadditional parameters to explain the variation in thedata.The ?
of the best selection of parameters on the2003 data set using the model with one threshold per380nugget and global errors is 0.837 (?mse=0.037).We have indeed overtrained the best threshold forthis dataset (compare ?=0.879, ?mse=0.067 in Ta-bles 2 and 3), suggesting that the numeric differ-ences in Kendall?s Tau shown between the Nugge-teer, Pourpre, and Rouge systems are not indicativeof true performance.
The Bayesian model promisessettings free of overtraining, and thus more accuratejudgements in terms of?mse and individual nuggetclassification accuracy.4.5 Training on System ResponsesIntuitively, if a fact is expressed by a system re-sponse, then another response with similar n-gramsmay also contain the same fact.
To test this intuition,we tried expanding our judgement method (Equa-tion 3) to select the maximum judgement score fromamong those of the nugget description and each ofthe system responses judged to contain that nugget.Unfortunately, the assessors did not mark whichportion of a response expresses a nugget, so we alsofind spurious similarity, as shown in Figure 7.
The fi-nal results are not conclusively better or worse over-all, and the process is far more expensive.We are currently exploring the same extension formultiple ?nugget descriptions?
generated by manu-ally selecting the appropriate portions of system re-sponses containing each nugget.4.6 Judgment Precision and RecallBecause Nuggeteer makes a nugget classificationfor each system response, we can report precisionand recall on the nugget assignments.
Table 5shows Nuggeteer?s agreement rate with assessors onwhether each response contains a nugget.
64.7 Novel JudgementsApproximate evaluation will tend to undervalue newresults, simply because they may not have keywordoverlap with existing nugget descriptions.
We aretherefore creating tools to help developers manuallyassess their system outputs.As a proof of concept, we ran Nuggeteer on thebest 2005 ?other?
system (not giving Nuggeteer6Unlike human assessors, Nuggeteer is not able to pick the?best?
response containing a nugget if multiple responses haveit, and will instead pick the first, so these values are artifactuallylow.
However, 2005 results may be high because these resultsreflect anonymized runs.Data set best F(?
= 1) default F(?
= 1)2003 defn 0.68?
.01 0.66?
.022004 other 0.73?
.01 0.70?
.012005 other 0.87?
.01 0.86?
.012005 reln 0.75?
.04 0.72?
.05Table 5: Nuggeteer agreement with official judge-ments, under best settings for each year, and underthe default settings.the official judgements), and manualy corrected itsguesses.7 Assessment took about 6 hours, and ourjudgements had precision of 78% and recall of 90%,for F-measure 0.803?
0.065 (compare Table 5).
Theofficial score of .299 was still within the confidenceinterval, but now on the high side rather than thelow (.257?
.07), because we found the answers quitegood.
In fact, we were often tempted to add newnuggets!
We later learned that it was a manual run,produced by a student at the University of Maryland.5 DiscussionPourpre pioneered automatic nugget-based assess-ment for definition questions, and thus enabled arapid experimental cycle of system development.Nuggeteer improves on that functionality, and crit-ically adds:?
an interpretable score, comparable to officialscores, with near-human error rates,?
a reliable confidence interval on the estimatedscore,?
scoring known responses exactly,?
support for improving the accuracy of the scorethrough additional annotation, and?
a more robust training processWe have shown that Nuggeteer evaluates the def-inition and relationship tasks with comparable rankswap rates to Pourpre.
We explored the effects ofstemming, term weighting, n-gram size, stopwordremoval, and use of system responses for training,all with little effect.
We showed that previous meth-ods of selecting a threshold overtrained, and have7We used a low threshold to make the task mostly correctingand less searching.
This is clearly not how assessors shouldwork, but is expedient for developers.381question id 1901, response rank 2, response score 0.14response text: best american classical music bears its stamp: witnessaaron copland, whose "american-sounding" music was composed by a(the response was a sentence fragment)assigned nugget description: born brooklyn ny 1900bigram matches: ?american classical?, ?american-sounding music?, ?best american?, ?whoseamerican-sounding?, ?witness aaron?, ?copland whose?, ?stamp witness?, ...response containing the nugget: Even the best American classical music bears its stamp:witness Aaron Copland, whose ??American-sounding??
music was composed by aBrooklyn-born Jew of Russian lineage who studied in France and salted hisscores with jazz-derived syncopations, Mexican folk tunes and cowboy ballads.NYT19981210.0106Figure 7: This answer to the definition question on Aaron Copeland is assigned the nugget ?born brooklynny 1900?
at a recall score well above that of the background, despite containing none of those words.briefly described a promising way to select finer-grained thresholds automatically.Our experiences in using judgements of systemresponses point to the need for a better annotationof nugget content.
It is possible to give Nuggeteermultiple nugget descriptions for each nugget.
Man-ually extracting the relevant portions of correctly-judged system responses may not be an overly ardu-ous task, and may offer higher accuracy.
It would beideal if the community?including the assessors?were able to create and promulgate a gold-standardset of nugget descriptions for previous years.Nuggeteer currently supports evaluation for theTREC definition, ?other?, and relationship tasks, forthe AQUAINT opinion pilot 8, and is under devel-opment for the DARPA GALE task 9.6 AcknowledgementsWe would like to thank Jimmy Lin and DinaDemner-Fushman for valuable discussions, for Fig-ure 3, and Table 2, and for creating Pourpre.
Thanksto Ozlem Uzuner and Sue Felshin for valuable com-ments on earlier drafts of this paper and to BorisKatz for his inspiration and support.8http://www-24.nist.gov/projects/aquaint/opinion.html9http://www.darpa.mil/ipto/programs/galeReferencesEric J. Breck, John D. Burger, Lisa Ferro, LynetteHirschman, David House, Marc Light, and InderjeetMani.
2000.
How to evaluate your question answer-ing system every day ... and still get real work done.In Proceedings of the second international conferenceon Language Res ources and Evaluation (LREC2000).Jimmy Lin and Dina Demner-Fushman.
2005.
Automat-ically evaluating answers to definition questions.
InProceedings of HLT-EMNLP.Jimmy Lin and Dina Demner-Fushman.
2006.
Will pyra-mids built of nuggets topple over?
In Proceedings ofHLT-NAACL.Jimmy Lin, Eileen Abels, Dina Demner-Fushman, Dou-glas W. Oard, Philip Wu, and Yejun Wu.
2005.
Amenagerie of tracks at maryland: HARD, Enterprise,QA, and Genomics, oh my!
In Proceedings of TREC.Ellen Voorhees.
2003.
Overview of the TREC 2003question answering track.Ellen Voorhees.
2004.
Overview of the TREC 2004question answering track.Ellen Voorhees.
2005.
Overview of the TREC 2005question answering track.382
