Proceedings of ACL-08: HLT, Short Papers (Companion Volume), pages 49?52,Columbus, Ohio, USA, June 2008. c?2008 Association for Computational LinguisticsSimulating the Behaviour of Older versus Younger Userswhen Interacting with Spoken Dialogue SystemsKallirroi Georgila, Maria Wolters and Johanna D. MooreHuman Communication Research CentreUniversity of Edinburghkgeorgil|mwolters|jmoore@inf.ed.ac.ukAbstractIn this paper we build user simulations ofolder and younger adults using a corpus ofinteractions with a Wizard-of-Oz appointmentscheduling system.
We measure the quality ofthese models with standard metrics proposedin the literature.
Our results agree with predic-tions based on statistical analysis of the cor-pus and previous findings about the diversityof older people?s behaviour.
Furthermore, ourresults show that these metrics can be a goodpredictor of the behaviour of different types ofusers, which provides evidence for the validityof current user simulation evaluation metrics.1 IntroductionUsing machine learning to induce dialogue man-agement policies requires large amounts of trainingdata, and thus it is typically not feasible to buildsuch models solely with data from real users.
In-stead, data from real users is used to build simulatedusers (SUs), who then interact with the system asoften as needed.
In order to learn good policies, thebehaviour of the SUs needs to cover the range ofvariation seen in real users (Schatzmann et al, 2005;Georgila et al, 2006).
Furthermore, SUs are criticalfor evaluating candidate dialogue policies.To date, several techniques for building SUs havebeen investigated and metrics for evaluating theirquality have been proposed (Schatzmann et al,2005; Georgila et al, 2006).
However, to our knowl-edge, no one has tried to build user simulationsfor different populations of real users and measurewhether results from evaluating the quality of thosesimulations agree with what is known about thoseparticular types of real users, extracted from otherstudies of those populations.
This is presumably dueto the lack of corpora for different types of users.In this paper we focus on the behaviour of oldervs.
younger adults.
Most of the work to date on di-alogue systems focuses on young users.
However,as average life expectancy increases, it becomes in-creasingly important to design dialogue systems insuch a way that they can accommodate older peo-ple?s behaviour.
Older people are a user group withdistinct needs and abilities (Czaja and Lee, 2007)that present challenges for user modelling.
To ourknowledge no one so far has built statistical usersimulation models for older people.
The only sta-tistical spoken dialogue system for older people weare aware of is Nursebot, an early application of sta-tistical methods (POMDPs) within the context of amedication reminder system (Roy et al, 2000).In this study, we build SUs for both younger andolder adults using n-grams.
Our data comes from afully annotated corpus of 447 interactions of olderand younger users with a Wizard-of-Oz (WoZ) ap-pointment scheduling system (Georgila et al, 2008).We then evaluate these models using standard met-rics (Schatzmann et al, 2005; Georgila et al, 2006)and compare our findings with the results of statisti-cal corpus analysis.The novelty of our work lies in two areas.
First,to the best of our knowledge this is the first time thatstatistical SUs have been built for the increasinglyimportant population of older users.Secondly, a general (but as yet untested) assump-tion in this field is that current SUs are ?enough like?real users for training good policies, and that testingsystem performance in simulated dialogues is an ac-curate indication of how a system will perform withhuman users.
The validity of these assumptions is49a critically important open research question.
Cur-rently one of the standard methods for evaluatingthe quality of a SU is to run a user simulation ona real corpus and measure how often the action gen-erated by the SU agrees with the action observed inthe corpus (Schatzmann et al, 2005; Georgila et al,2006).
This method can certainly give us some in-sight into how strongly a SU resembles a real user,but the validity of the metrics used remains an openresearch problem.
In this paper, we take this a stepfurther.
We measure the quality of user simulationmodels for both older and younger users, and showthat these metrics are a good predictor of the be-haviour of those two user types.The structure of the paper is as follows: In sec-tion 2 we describe our data set.
In section 3 wediscuss the differences between older and youngerusers as measured in our corpus using standard sta-tistical techniques.
Then in section 4 we present ouruser simulations.
Finally in section 5 we present ourconclusions and propose future work.2 The CorpusThe dialogue corpus which our simulations arebased on was collected during a controlled experi-ment where we systematically varied: (1) the num-ber of options that users were presented with (oneoption, two options, four options); (2) the confirma-tion strategy employed (explicit confirmation, im-plicit confirmation, no confirmation).
The combina-tion of these 3?
3 design choices yielded 9 differentdialogue systems.Participants were asked to schedule a health careappointment with each of the 9 systems, yielding atotal of 9 dialogues per participant.
System utter-ances were generated using a simple template-basedalgorithm and synthesised using the speech synthe-sis system Cerevoice (Aylett et al, 2006), which hasbeen shown to be intelligible to older users (Wolterset al, 2007).
The human wizard took over the func-tion of the speech recognition, language understand-ing, and dialogue management components.Each dialogue corresponded to a fixed schema:First, users arranged to see a specific health care pro-fessional, then they arranged a specific half-day, andfinally, a specific half-hour time slot on that half-daywas agreed.
In a final step, the wizard confirmed theappointment.The full corpus consists of 447 dialogues; 3 di-alogues were not recorded.
A total of 50 partici-pants were recruited, of which 26 were older (50?85) and 24 were younger (20?30).
The older userscontributed 232 dialogues, the younger ones 215.Older and younger users were matched for level ofeducation and gender.All dialogues were transcribed orthographicallyand annotated with dialogue acts and dialogue con-text information.
Using a unique mapping, we as-sociate each dialogue act with a ?speech act, task?pair, where the speech act is task independent andthe task corresponds to the slot in focus (health pro-fessional, half-day or time slot).
For each dialogue,five measures of dialogue quality were recorded: ob-jective task completion, perceived task completion,appointment recall, length (in turns), and detaileduser satisfaction ratings.
A detailed description ofthe corpus design, statistics, and annotation schemeis provided in (Georgila et al, 2008).Our analysis of the corpus shows that there areclear differences in the way users interact with thesystems.
Since it is these differences that good usersimulations need to capture, the most relevant find-ings for the present study are summarised in the nextsection.3 Older vs.
Younger UsersSince the user simulations (see section 4) are basedmainly on dialogue act annotations, we will usespeech act statistics to illustrate some key differ-ences in behaviour between older and younger users.User speech acts were grouped into four categoriesthat are relevant to dialogue management: speechacts that result in grounding (ground), speech actsthat result in confirmations (confirm) (note, thiscategory overlaps with ground and occurs after thesystem has explicitly or implicitly attempted to con-firm the user?s response), speech acts that indicateuser initiative (init), and speech acts that indi-cate social interaction with the system (social).We also computed the average number of differentspeech act types used, the average number of speechact tokens, and the average token/type ratio per user.Results are given in Table 1.There are 28 distinct user speech acts (Georgila etal., 2008).
Older users not only produce more indi-vidual speech acts, they also use a far richer varietyof speech acts, on average 14 out of 28 as opposed to9 out of 28.
The token/type ratio remains the same,however.
Although the absolute frequency of confir-mation and grounding speech acts is approximately50Variable Older Younger Sig.# speech act types 14 9 ***# speech act tokens 126 73 ***Sp.
act tokens/types 8.7 8.5 n.s.# Confirm 31 30 n.s.% Confirm 28.3 41.5 ***# Ground 33 30 n.s.% Ground 29.4 41.7 ***# Social 26 5 ***% Social 17.9 5.3 ***# Init 15 3 ***% Init 9.0 3.4 **Table 1: Behaviour of older vs. younger users.
Numbersare summed over all dialogues and divided by the num-ber of users.
*: p<0.01, **: p<0.005, ***: p<0.001 orbetter.the same for younger and older users, the relativefrequency of these types of speech acts is far lowerfor older than for younger users, because older usersare far more likely to take initiative by providing ad-ditional information to the system and speech actsindicating social interaction.
Based on this analysisalone, we would predict that user simulations trainedon younger users only will not fare well when testedon older users, because the behaviour of older usersis richer and more complex.But do older and younger users constitute twoseparate groups, or are there older users that be-have like younger ones?
In the first case, we can-not use data from older people to create simulationsof younger users?
behaviour.
In the second case,data from older users might be sufficient to approx-imately cover the full range of behaviour we see inthe data.
The boxplots given in Fig.
1 indicate thatthe latter is in fact true.
Even though the meansdiffer considerably between the two groups, olderusers?
behaviour shows much greater variation thanthat of younger users.
For example, for user initia-tive, the main range of values seen for older usersincludes the majority of values observed for youngerusers.4 User SimulationsWe performed 5-fold cross validation ensuring thatthere was no overlap in speakers between differentfolds.
Each user utterance corresponds to a user ac-tion annotated as a list of ?speech act, task?
pairs.For example, the utterance ?I?d like to see the di-abetes nurse on Thursday morning?
could be an-notated as [(accept info, hp), (provide info, half-Figure 1: Relative frequency of (a) grounding and (b)user initiative.day)] or similarly, depending on the previous sys-tem prompt.
There are 389 distinct actions for olderpeople and 125 for younger people.
The actions ofthe younger people are a subset of the actions of theolder people.We built n-grams of system and user actions withn varying from 2 to 5.
Given a history of system anduser actions (n-1 actions) the SU generates an actionbased on a probability distribution learned from thetraining data (Georgila et al, 2006).
We tested fourvalues of n, 2, 3, 4, and 5.
For reasons of space, weonly report results from 3-grams because they sufferless from data sparsity than 4- and 5-grams and takeinto account larger contexts than 2-grams.
However,results are similar for all values of n.The actions generated by our SUs were comparedto the actions observed in the corpus using five met-rics proposed in the literature (Schatzmann et al,2005; Georgila et al, 2006): perplexity (PP), preci-sion, recall, expected precision and expected recall.While precision and recall are calculated based onthe most likely action at a given state, expected pre-cision and expected recall take into account all pos-sible user actions at a given state.
Details are givenin (Georgila et al, 2006).
In our cross-validationexperiments, we used three different sources for thetraining and test sets: data from older users (O), data51PP Prec Rec ExpPrec ExpRecO-O 18.1 42.8 39.8 56.0 49.4Y-O 19.6 34.2 25.1 53.4 40.7A-O 18.7 41.1 35.9 58.9 49.0O-Y 5.7 44.8 60.6 66.3 73.4Y-Y 3.7 50.5 54.1 73.1 70.4A-Y 3.8 45.8 58.5 70.5 73.0O-A 10.3 43.7 47.2 60.3 58.0Y-A 9.3 40.3 33.3 62.0 51.5A-A 9.3 43.2 43.4 63.9 57.9Table 2: Results for 3-grams and different combinationsof training and test data.
O: older users, Y: younger users,A: all users.from younger users (Y), and data from all users (A).Our results are summarised in Table 2.We find that models trained on younger users, buttested on older users (Y-O) perform worse than mod-els trained on older users / all users and tested onolder users (O-O, A-O).
Thus, models of the be-haviour of younger users cannot be used to simulateolder users.
In addition, models which are trainedon older users tend to generalise better to the wholedata set (O-A) than models trained only on youngerusers (Y-A).
These results are in line with our sta-tistical analysis, which showed that the behaviourof younger users appears to be a subset of the be-haviour of older users.
All results are statisticallysignificant at p<0.05 or better.5 ConclusionsIn this paper we built user simulations for olderand younger adults and evaluated them using stan-dard metrics.
Our results suggest that SUs trainedon older people may also cover the behaviour ofyounger users, but not vice versa.
This findingsupports the principle of ?inclusive design?
(Keatesand Clarkson, 2004): designers should consider awide range of users when developing a product forgeneral use.
Furthermore, our results agree withpredictions based on statistical analysis of our cor-pus.
They are also in line with findings of tests ofdeployed Interactive Voice Response systems withyounger and older users (Dulude, 2002), whichshow the diversity of older people?s behaviour.Therefore, we have shown that standard metrics forevaluating SUs are a good predictor of the behaviourof our two user types.
Overall, the metrics we usedyielded a clear and consistent picture.
Although ourresult needs to be verified on similar corpora, it hasan important implication for corpus design.
In orderto yield realistic models of user behaviour, we needto gather less data from students, and more data fromolder and middle-aged users.In our future work, we will perform more detailedstatistical analyses of user behaviour.
In particular,we will analyse the effect of dialogue strategies onbehaviour, experiment with different Bayesian net-work structures, and use the resulting user simula-tions to learn dialogue strategies for both older andyounger users as another way for testing the accu-racy of our user models and validating our results.AcknowledgementsThis research was supported by the Wellcome Trust VIPgrant and the Scottish Funding Council grant MATCH(HR04016).
We would like to thank Robert Logie andSarah MacPherson for contributing to the design of theoriginal experiment, Neil Mayo and Joe Eddy for codingthe Wizard-of-Oz interface, Vasilis Karaiskos and MattWatson for collecting the data, and Melissa Kronenthalfor transcribing the dialogues.ReferencesM.
Aylett, C. Pidcock, and M.E.
Fraser.
2006.
TheCerevoice Blizzard Entry 2006: A prototype databaseunit selection engine.
In Proc.
BLIZZARD Challenge.S.
Czaja and C. Lee.
2007.
The impact of aging on ac-cess to technology.
Universal Access in the Informa-tion Society (UAIS), 5:341?349.L.
Dulude.
2002.
Automated telephone answering sys-tems and aging.
Behaviour Information Technology,21:171?184.K.
Georgila, J. Henderson, and O.
Lemon.
2006.
Usersimulation for spoken dialogue systems: Learning andevaluation.
In Proc.
Interspeech/ICSLP.K.
Georgila, M. Wolters, V. Karaiskos, M. Kronenthal,R.
Logie, N. Mayo, J. Moore, and M. Watson.
2008.A fully annotated corpus for studying the effect of cog-nitive ageing on users?
interactions with spoken dia-logue systems.
In Proc.
LREC.S.
Keates and J. Clarkson.
2004.
Inclusive Design.Springer, London.N.
Roy, J. Pineau, and S. Thrun.
2000.
Spoken dialogmanagement for robots.
In Proc.
ACL.J.
Schatzmann, K. Georgila, and S. Young.
2005.
Quan-titative evaluation of user simulation techniques forspoken dialogue systems.
In Proc.
SIGdial.M.
Wolters, P. Campbell, C. DePlacido, A. Liddell, andD.
Owens.
2007.
Making synthetic speech accessi-ble to older people.
In Proc.
Sixth ISCA Workshop onSpeech Synthesis, Bonn, Germany.52
