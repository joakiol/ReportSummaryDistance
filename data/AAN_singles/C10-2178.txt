Coling 2010: Poster Volume, pages 1558?1566,Beijing, August 2010Interpreting Pointing Gestures and Spoken Requests ?
A Probabilistic,Salience-based ApproachIngrid Zukerman and Gideon Kowadlo and Patrick YeFaculty of Information TechnologyMonash UniversityIngrid.Zukerman@monash.edu,gkowadlo@gmail.com,ye.patrick@gmail.comAbstractWe present a probabilistic, salience-basedapproach to the interpretation of pointinggestures together with spoken utterances.Our mechanism models dependencies be-tween spatial and temporal aspects of ges-tures and features of utterances.
For ourevaluation, we collected a corpus of re-quests which optionally included point-ing.
Our results show that pointing infor-mation improves interpretation accuracy.1 IntroductionDORIS (Dialogue Oriented Roaming InteractiveSystem) is a spoken dialogue system designed fora household robot.
In (Zukerman et al, 2008),we described Scusi?
?
a spoken language in-terpretation module which considers multiple sub-interpretations at different levels of the interpreta-tion process, and estimates the probability of eachsub-interpretation at each level (Section 2).
Thisformalism is required for requests such as ?Getme the blue cup?
in the context of the scene de-picted in Figure 1, where possible candidates arethe three white cups, and the blue and purple tum-blers, but it is unclear which is the intended object,as none of the alternatives match the request per-fectly.In this paper, we integrate pointing gesturesinto Scusi?
?s probabilistic formalism.
We adopta salience-based approach, where we take into ac-count spatial and temporal information to estimatethe probability that a pointing gesture refers to anFigure 1: Experimental Setupobject.
To evaluate our formalism, we collected acorpus of requests where people were allowed topoint (Section 4).
Our results show that when peo-ple point, our mechanism yields significant im-provements in interpretation accuracy; and whenpointing was artificially added to utterances wherethe people did not point, its effect on interpretationaccuracy was reduced.This paper is organized as follows.
Section 2outlines the interpretation of a spoken request andthe estimation of the probability of an interpre-tation.
Section 3 describes how pointing affectsthis probability.
Our evaluation is detailed in Sec-tion 4.
Related research is discussed in Section 5,followed by concluding remarks.2 Interpreting Spoken RequestsHere we summarize our previous work on the in-terpretation of single-sentence requests (Makalicet al, 2008; Zukerman et al, 2008).1558Scusi?
processes spoken input in three stages:speech recognition, parsing and semantic inter-pretation.
First, Automatic Speech Recognition(ASR) software (Microsoft Speech SDK 5.3) gen-erates candidate hypotheses (texts) from a speechsignal.
The ASR produces up to 50 texts for aspoken utterance, where each text is associatedwith a probability.
In the parsing stage, the textsare considered in descending order of probability.Charniak?s probabilistic parser (ftp://ftp.cs.brown.edu/pub/nlparser/) is applied to eachtext, yielding up to 50 parse trees ?
each asso-ciated with a probability.During semantic interpretation, parse trees aresuccessively mapped into two representationsbased on Concept Graphs (Sowa, 1984).
FirstUninstantiated Concept Graphs (UCGs), and thenInstantiated Concept Graphs (ICGs).
UCGs areobtained from parse trees deterministically ?
oneparse tree generates one UCG.
A UCG representssyntactic information, where the concepts corre-spond to the words in the parent parse tree, andthe relations are derived from syntactic informa-tion in the parse tree and prepositions.
Each UCGcan generate many ICGs.
This is done by nomi-nating different instantiated concepts and relationsfrom the system?s knowledge base as potential re-alizations for each concept and relation in a UCG.Instantiated concepts are objects and actions in thedomain (e.g., mug01, mug02 and cup01 are pos-sible instantiations of the uninstantiated concept?mug?
), and instantiated relations are similar tosemantic role labels (Gildea and Jurafsky, 2002).The interpretation process continues until a pre-set number of sub-interpretations (including texts,parse trees, UCGs and ICGs) has been generatedor all options have been exhausted.Figure 2 illustrates a UCG and an ICG for therequest ?get the large red folder on the table?.
Theintrinsic features of an object (lexical item, colourand size) are stored in the UCG node for this ob-ject.
Structural features, which involve two ob-jects (e.g., ?folder-on-table?
), are represented assub-graphs of the UCG (and the ICG).2.1 Estimating the probability of an ICGScusi?
ranks candidate ICGs according to theirprobability of being the intended meaning of aPatientget01Onfolder02table01getobjectSIZE          LARGELEX folderCOLOUR  REDonLEX tableUtterance:UCGGet the large red folder on the tableICGFigure 2: UCG and ICG for a sample utterancespoken utterance.
Given a speech signal W and acontext C, the probability of an ICG I , Pr(I|W, C),is proportional to?
?Pr(T |W )?Pr(P |T )?Pr(U |P )?Pr(I|U, C) (1)where T , P and U denote text, parse tree andUCG respectively.
The summation is taken overall possible paths ?
= {P,U} from the speechwave to the ICG, because a UCG and an ICGcan have more than one ancestor.
As mentionedabove, the ASR and the parser return an esti-mate of Pr(T |W ) and Pr(P |T ) respectively; andPr(U |P ) = 1, since the process of generating aUCG from a parse tree is deterministic.
The es-timation of Pr(I|U, C) is described in (Zukermanet al, 2008).
Here we present the final equationobtained for Pr(I|U, C), and outline the ideas in-volved in its calculation.Pr(I|U, C)?
?k?IPr(u|k, C) Pr(k|kp, kgp) Pr(k|C)(2)where u is a node in UCG U , k is the correspond-ing instantiated node in ICG I , kp is k?s parentnode, and kgp is k?s grandparent node.
For exam-ple, On is the parent of table01, and folder02the grandparent in the ICG in Figure 2.?
Pr(u|k) is the ?match probability?
between thespecifications for node u in UCG U and the in-trinsic features of the corresponding node k inICG I , i.e., the probability that a speaker whointended a particular object k gave the specifi-cations in u.1559?
Pr(k|kp, kgp) represents the structural proba-bility of ICG I , where structural informationis simplified to node trigrams, e.g., whetherfolder02 is On table01.?
Pr(k|C) is the probability of a concept in lightof the context, which includes informationabout domain objects, actions and relations.Scusi?
handles three intrinsic features: lexicalitem, colour and size; and two structural features:ownership and several locative relations (e.g., on,under, near).
The match probability Pr(u|k) andthe structural probability Pr(k|kp, kgp) are esti-mated using a distance function between the re-quirements specified by the user and what is foundin reality ?
the closer the distance between thespecifications and reality, the higher the probabil-ity (for details see (Makalic et al, 2008)).3 Incorporating Pointing GesturesPointing affects the salience of objects and thelanguage used to refer to objects: objects in thetemporal and spatial vicinity of a pointing gestureare more salient than objects that are farther away,and pointing is often associated with demonstra-tive determiners.
Thus, the incorporation of point-ing into Scusi?
affects the following elements ofEquation 2 (Section 2.1).?
Pr(k|C) ?
the context-based probability of anobject (i.e., its salience) is affected by the timeof a pointing gesture and the space it encom-passes.
For instance, if the user says ?Get thecup?
in the context of the scene in Figure 1,pointing around the time s/he said ?cup?, thegesture most likely refers to an object that maybe called ?cup?.
Further, among the candidatecups in Figure 1, those closer to the ?pointingvector?
have a higher probability.1?
Pr(u|k, C) ?
when pointing, people often usedemonstrative determiners, e.g., ?get me thatcup?.
Also, people often use generic identifiersin conjunction with demonstrative determiners1At present, we assume that an utterance is associatedwith at most one pointing gesture, and that pointing pertainsto objects.
This assumption is supported by our user study(Section 4.1).to refer to unfamiliar objects, e.g., ?that thing?to refer to a vacuum tube (Figure 1).These probabilities are estimated in Sec-tions 3.1 and 3.2.
Our calculations are based oninformation returned by the gesture recognitionsystem described in (Li and Jarvis, 2009): gesturetype, time, probability and relevant parameters(e.g., a vector for a pointing gesture).
Since we fo-cus on pointing gestures, we convert the probabil-ities expected from Li and Jarvis?s system into theprobability of Pointing and that of Not Pointing,which comprises all other gestures and no gesture(these hypotheses are returned at the same time).23.1 Calculating salience from pointingWhen pointing is taken into account, the probabil-ity of object k is expressed as follows.Pr(k|C) = Pr(k|P, C) ?
Pr(P|C) + (3)Pr(k|?P, C) ?
Pr(?P|C)where P designates Pointing, Pr(P|C) and itscomplement are returned by the gesture recog-nition system, and Pr(k|?P, C) = 1N (N is thenumber of objects in the room, i.e., in the absenceof pointing, we assume that all the objects in theroom are equiprobable3).As indicated above, we posit that pointing isspatially correlated with an intended object, andtemporally correlated with a word referring to theintended object.
Hence, we separate Pointing intotwo components: spatial (s) and temporal (t), ob-taining ?Ps,Pt?.
ThusPr(k|P, C) = Pr(k,Pt,Ps, C)Pr(P, C) (4)= Pr(Pt|k,Ps, C) ?
Pr(k|Ps, C) ?
Pr(Ps|C)Pr(P|C)We assume that given k, Pt is conditionally in-dependent fromPs; and that Pr(Ps|C) = Pr(P|C),i.e., the spatial probability of a pointing gesture isthe probability returned by the gesture system forthe entire pointing hypothesis (time and space).This yieldsPr(k|P, C) = Pr(Pt|k, C) ?
Pr(k|Ps, C) (5)2Owing to timing limitations of the gesture recognitionsystem (Section 4), we simulate its output.3At present, we do not consider dialogue salience.1560vectorpointingkjpd(k,PLine) d(k,PLine)PLineuser(a) PointingpointingvectorOLinekjpd(j,OLine)d(j,OLine)user(b) OcclusionFigure 3: Spatial pointing and occlusionwhere Pr(k|Ps, C) and Pr(Pt|k, C) are estimatedas described in Section 3.1.1 and 3.1.2 respec-tively.
This equation is smoothed as follows (andincorporated into Equation 3) to take into accountobjects that are (spatially or temporally) excludedfrom the pointing gesture.Pr?
(k|P, C) = Pr(k|P, C) +1N1 +?Nj=1 Pr(kj |P, C)(6)3.1.1 Estimating Pr(k|Ps, C)Pr(k|Ps, C), the probability that the user in-tended object k when pointing to a location, is es-timated using a conic Gaussian density functionaround PLine, the Pointing Line created by ex-tending the pointing vector returned by the gestureidentification system (Figure 3(a)).4Pr(k|Ps, C) = ??k?2pi?Ps(pd)e?
d(k,PLine)22?2Ps(pd) (7)where ?
is a normalizing constant; ?Ps(pd) is thestandard deviation of the Gaussian cone as a func-tion of pd(k,PLine), the projected distance be-tween the user?s pointing hand and the projectionof object k on PLine; d(k,PLine) is the shortestdistance between the center of object k and PLine;and ?k is a factor that reduces the probability ofobject k if it is (partially) occluded (Figure 3(b)).The projected distance pd takes into accountthe imprecision of pointing actions ?
a problemthat is exacerbated by the uncertainty associatedwith sensing a pointing vector.
A small angular4Since this is a continuous density function, it does notdirectly yield a point probability.
Hence, it is normalized onthe basis of the largest possible returned value.error in the detected pointing vector yields a dis-crepancy in the distance between the pointing lineand candidate objects.
This discrepancy increasesas pd(k,PLine) increases.
To compensate for thissituation, we increase the variance of the Gaussiandistribution linearly with the projected distancefrom the user?s hand (we start with a small stan-dard deviation of ?0 = 5 mm at the user?s fingers,attributed to sensor error).
This allows farther ob-jects with a relatively high displacement from thepointing vector to be encompassed in a pointinggesture (e.g., the larger mug in Figure 3(a)), whilecloser objects with the same displacement are ex-cluded (e.g., the smaller mug).
This yields the fol-lowing equation for the variance.
?2Ps(pd) = ?20 +K ?
pd(k,PLine)where K = 2.5 mm is an empirically determinedincrease rate.The occlusion factor ?k reduces the probabilityof objects as they become more occluded.
We ap-proximate ?k by considering the objects that arecloser to the user than k, and estimating the extentto which these objects occlude k (Figure 3(b)).This estimate is a function of the position of theseobjects and their size ?
the larger an interveningobject, the lower the probability that the user ispointing at k. These factors are taken into accountas follows.Pr(j occl k)= ??2pi??(pd)e?
(d(j,OLine)?12 dimmin(j))22?2?
(pd)(8)where ?
is a normalizing constant; the numera-tor of the exponent represents the maximum dis-tance from the edge of object j to the line betweenthe user?s hand and object k, denoted Object Line(OLine); and?2?
(pd) = 12(?20 +K ?
pd(j,OLine))represents the variance of a cone from the user?shand to object k as a function of distance.
In orderto represent the idea that object j must be closeto the Object Line to occlude object k, we usehalf the variance of that used for the ?pointingcone?, which yields a thinner ?occlusion cone?
(Figure 3(b)).
?k is then estimated as 1 minus the1561maximum occlusion caused by the objects that arecloser to the user than k.?k=1?
max?j d(j,hand)<d(k,hand) {Pr(j occl k)} (9)3.1.2 Estimating Pr(Pt|k, C)Pr(Pt|k, C) is obtained as follows.Pr(Pt|k, C) =n?i=1Pr(Pt, k,Wi, C)Pr(k, C) (10)=n?i=1Pr(k|Pt, wi, C)?Pr(T (wi)|Pt, C)?Pr(Pt|C)Pr(k|C)where n is the number of nouns in the user?s utter-ance, and Wi = ?wi, T (wi)?
is a tuple comprisingthe ith noun and the mid point of the time when itwas uttered.We make the following assumptions.?
Pr(Pt|C) = 1, as all the gesture hypothesesare returned at the same time;?
given Pt, the timing of a word T (wi) is condi-tionally independent of C; and?
given wi, k is conditionally independent ofthe timing of the pointing gesture Pt, i.e.,Pr(k|Pt, wi, C) = Pr(k|wi, C).This probability is represented asPr(k|wi, C) = Pr(wi|k) ?
Pr(k|C)?Nj=1 {Pr(wi|kj) ?
Pr(kj |C)}where N is the number of objects.These assumptions yieldPr(Pt|k, C)=n?i=1Pr(wi|k) ?
Pr(T (wi)|Pt)?Nj=1 {Pr(wi|kj)?Pr(kj |C)}(11)where Pr(T (wi)|Pt), the probability of the timeof word wi given the time of the pointing gesture,is obtained from the following Gaussian time dis-tribution for pointing.Pr(T (wi)|Pt) = ??2pi?Pte?
(T(wi)?PTime)22?2Pt (12)where ?
is a normalizing constant, PTime is thetime of the gesture, and ?Pt is the standard de-viation of the Gaussian density function, which iscurrently set to 650 msec (based on our corpus).As in our previous work (Makalic et al,2008), we estimate Pr(wi|k) using the Leacockand Chodorow (1998) WordNet similarity metric.This metric also yields a match probability be-tween most objects and generic words like ?ob-ject, thing, here, there?, enabling us to handle re-quests such as ?Get that thing over there?.3.2 Calculating the probability of a referringexpressionAs mentioned in Section 2, the intrinsic featurespreviously considered in Scusi?
are lexical item,colour and size (Makalic et al, 2008).
Pointingaffects referring expressions in that people maypoint instead of generating complex descriptions,they may employ demonstrative determiners to-gether with generic terms such as ?thing?
(espe-cially when they are unfamiliar with the name ofan object), and they may use demonstrative pro-nouns.
The first two behaviours were exhibited inour user study (Section 4), but none of our trialparticipants used demonstrative pronouns.To incorporate pointing into the calculation ofPr(u|k, C), we add determiners to Scusi?
?s for-malism for intrinsic features, which yieldsPr(u|k, C) = Pr(ulex, udet, ucolr, usize|k, C)After adding weights for the intrinsic features(inspired by (Dale and Reiter, 1995)), and makingsome simplifying assumptions, we obtainPr(u|k, C) = (13)Pr(ulex|k, C)wlex ?
Pr(udet|k, C)wdet ?Pr(ucolr|k)wcolr ?
Pr(usize|ulex, k)wsizeThe estimation of Pr(ulex|k, C), Pr(ucolr|k) andPr(usize|ulex, k) is described in (Makalic et al,2008).
Here we focus on Pr(udet|k, C).3.2.1 Estimating Pr(udet|k, C)Pr(udet|k, C) is estimated as follows.Pr(udet|k, C) = Pr(k|udet, C) ?
Pr(udet|C)Pr(k|C) (14)= Pr(k|udet, C)Pr(k|C)[ Pr(udet|P, C) ?
Pr(P|C)+Pr(udet|?P, C) ?
Pr(?P|C)]1562where det = {def article, indef article, de-monstr this, demonstr that}; Pr(P|C) andPr(?P|C) are returned by the gesture system;Pr(udet|P, C) and Pr(udet|?P, C) are obtainedfrom our corpus; and for now we assume thatPr(k|udet, C) = Pr(k|C).5 This yieldsPr(udet|k, C) = Pr(udet|P, C) ?
Pr(P|C) + (15)Pr(udet|?P, C) ?
Pr(?P|C)4 EvaluationTo obtain a corpus, we conducted a user studywhereby we set up a room with labeled objects(Figure 1), and asked trial participants to request12 selected items from DORIS (the room included33 items in total, including distractors, and one ofthe authors pretended to be DORIS).
The objectswere selected and laid out in the room to reflecta variety of conditions, e.g., common and rareobjects (e.g., vacuum tube); unique, non-uniqueand similar objects (e.g., white cups); and objectsplaced near each other and far from each other.We divided our corpus of requests into twoparts: with and without pointing.
Scusi?
?s perfor-mance was tested on input obtained from the ASRand on textual input (perfect ASR).
We consid-ered two scenarios for each sub-corpus: Pointing,where our pointing mechanism was activated onthe basis of a simulated pointing gesture,6 and No-Pointing, where no pointing gesture was detected.This was done in order to test two hypotheses:(1) when people point, pointing information im-proves interpretation performance; and (2) whenthey do not point, even perfect pointing has littleeffect on interpretation performance.Scusi?
was set to generate at most 300 sub-interpretations in total (including texts, parsetrees, UCGs and ICGs) for each spoken request,and at most 200 sub-interpretations for each tex-tual request.
On average, Scusi?
takes 10 secondsto go from texts to ICGs.
An interpretation was5In the future, we will incorporate distance from the userto refine the probabilities of determiners.6At present, we assume accurate pointing and gesture de-tection, and precise information regarding the position of ob-jects.
In the near future, we will study the sensitivity of ourmechanism to pointing inaccuracies, and to errors in gesturedetection and scene analysis.deemed successful if it correctly represented thespeaker?s intention, which was encoded in one ormore Gold ICGs.
These ICGs were manually con-structed on the basis of the requested objects andthe participants?
utterances.
Multiple Gold ICGswere allowed if there were several suitable actionsand objects.4.1 The Corpus19 people participated in the trial, generating a to-tal of 276 requests, of which 136 involved point-ing gestures (3 participants were asked to repeatthe experiment after it became clear that they wererefraining from pointing, as they erroneously as-sumed they were not allowed to gesture).
We fil-tered out 64 requests, which included conceptsour system cannot yet handle, specifically ?theend of the table?, projective modifiers (e.g., ?be-hind/left?
), ordinals (?first/second?
), references togroups of things (e.g., ?six blue pens?
), and zero-and one-anaphora.
This yielded 212 requests, ofwhich 105 involved pointing gestures.In addition, the software we used has the fol-lowing limitations: the gesture recognition sys-tem (Li and Jarvis, 2009) requires users to holda gesture for 2 seconds, and the ASR system isspeaker dependent and cannot recognize certainwords (e.g., ?mug?, ?bowl?
and ?pen?).
To cir-cumvent these problems, each pointing gesturewas manually encoded into a time-stamped vec-tor; and one of the authors read slightly sanitizedversions of participants?
utterances into the ASR:?can you?, ?please?
and ?DORIS?
were omitted;long prepositional phrases were shortened (e.g.,?the thing with wires sticking out of it?
); andwords that were problematic for the ASR were re-placed (e.g., ?pencil?
was used instead of ?pen?
).There was some difference in the length of re-quests with and without pointing, but it wasn?t aspronounced as reported in (Johnston et al, 2002):requests with/without pointing had 5.84/6.27words on average.
ASR performance was worsefor the requests that had pointing, with the topASR interpretation being correct for only 46%of these requests, compared to 57.5% for the re-quests without pointing.
This difference may beattributed to the ASR having trouble with sen-tence constructs associated with pointing.
Overall1563% Gold ICGs in Avg adj rank % Not Avg adj rank % Nottop 1 top 3 (rank) found (rank) 20 found 20Sub-corpus without pointingText, Scusi?-NoPointing 89.7 93.5 4.39 (0.78) 0.9 1.18 (0.13) 4.7Text, Scusi?-Pointing 86.9 87.9 3.28 (1.89) 0.9 0.39 (0.35) 4.7ASR, Scusi?-NoPointing 81.3 85.0 4.67 (0.83) 7.5 1.24 (0.17) 12.1ASR, Scusi?-Pointing 79.4 81.3 5.00 (2.62) 5.6 0.46 (0.40) 12.1Sub-corpus with pointingText, Scusi?-NoPointing 84.8 89.5 3.54 (0.59) 4.8 1.48 (0.20) 9.5Text, Scusi?-Pointing 82.9 86.7 4.19 (1.63) 1.9 0.41 (0.29) 7.6ASR, Scusi?-NoPointing 76.2 82.9 7.93 (0.95) 10.5 1.79 (0.27) 15.2ASR, Scusi?-Pointing 73.3 81.0 8.65 (2.76) 8.6 0.68 (0.40) 14.3Table 1: Scusi?
?s interpretation performancethe ASR returned the correct interpretation, at anyrank, for 88% of the requests.4.2 ResultsTable 1 summarizes our results.
Column 1 dis-plays the test condition (sub-corpus with/withoutpointing, text/ASR, and with/without Scusi?
?spointing mechanism).
Columns 2-3 show the per-centage of utterances that had Gold ICGs whoseprobability was among the top 1 and top 3, e.g.,in the sub-corpus with pointing, when Scusi?-Pointing was run on text, 82.9% of the utter-ances had Gold ICGs with the highest probability(top 1).
The average adjusted rank (AR) and av-erage rank of the Gold ICG appear in Column 4.The rank of an ICG I is its position in a list sortedin descending order of probability (starting fromposition 0), such that all equiprobable ICGs aredeemed to have the same position.
The adjustedrank of an ICG I is the mean of the positions ofall ICGs that have the same probability as I .
Forexample, if we have 4 equiprobable ICGs in po-sitions 0-3, each has a rank of 0, but an adjustedrank of rbest+rworst2 = 1.5.
Column 5 shows thepercentage of utterances that didn?t yield a GoldICG.
Column 6 shows the average AR for inter-pretations with AR < 20 (and their average rank),and Column 7 shows the percentage of utterancesthat had AR ?
20 or were not found.
We dis-tinguish between Gold ICGs with ARs 0 to 19and total Gold ICGs that were found, because adialogue manager is likely to inspect the promis-ing options, i.e., those with AR < K (we assumeK = 20).
In addition, there is normally a trade-off between the number of Not Found Gold ICGsand average AR.
ICGs that are not found by oneapproach but are found by another approach typi-cally have a high (bad) rank when they are even-tually found (Zukerman et al, 2008).
Thus, an ap-proach that fails to find such ?difficult?
ICGs usu-ally yields a lower average AR than an approachthat finds these ICGs.
Capping the ARs of thefound Gold ICGs at 20 clarifies the trade-off be-tween average AR and Not Found.Our results show that, as expected, the mainrole of pointing is in referent disambiguation.This is evident from the significant reduction inaverage AR-20 (Column 6) for the pointing andno-pointing sub-corpora, under the text/ASR in-put conditions.
All the differences are statisticallysignificant with p < 0.01.7 Nonetheless, the im-provements in average AR-20 obtained by artifi-cially introduced pointing in the no-pointing sub-corpus are smaller for both text and ASR than theimprovements obtained with actual pointing.
Weposit that this smaller impact is due to the fact thatutterances without pointing are more descriptivethan those with pointing, hence benefitting lessfrom the disambiguating effect of pointing.The Pointing condition has a seemingly adverseeffect on the number of interpretations with topranks (Columns 2-3).
This is explained by the fact7The differences were calculated using a paired t-test forall the Gold ICGs that were found in both configurations.1564that all equiprobable interpretations have the samerank, which happens more often under the No-Pointing condition than under the Pointing con-dition (as pointing has a disambiguating effect).Finally, under all conditions, the rank of the re-quest at the 75%-ile is 0, which indicates cred-itable performance.
The larger number of NotFound Gold ICGs for the ASR condition is ex-pected, as the ASR failed to find 12% of the cor-rect texts on average, performing worse for thepointing sub-corpus.
The other Not Found GoldICGs were mainly due to parsing preferences, andmultiple parses for some utterances that had theword ?thing?
(which matched all objects).5 Related ResearchGesture recognition systems endeavour to detectthe gesture being made.
Common approaches in-clude Hidden Markov Models, e.g., (Nickel andStiefelhagen, 2003), and Finite State Machines,e.g., (Li and Jarvis, 2009).
Systems that focuson pointing also identify the target object, with-out recognizing the type of this object (Nickel andStiefelhagen, 2003; Li and Jarvis, 2009).Most of the research in gesture and speech in-tegration focuses on pointing gestures, employ-ing speech as the main input modality, and us-ing semantic fusion to combine spoken input withgesture.
Different approaches are used for ges-ture detection, e.g., vision (Stiefelhagen et al,2004; Brooks and Breazeal, 2006) and sensorglove (Corradini et al, 2002); and for languageinterpretation, e.g., dedicated grammars (Stiefel-hagen et al, 2004; Brooks and Breazeal, 2006)and keywords (Einstein and Christoudias, 2004).Fusion is variously implemented using heuristicsbased on temporal overlap (Bolt, 1980; Johnstonet al, 2002), querying a gesture-sensing modulewhen ambiguous referents are identified (Fransenet al, 2007), or unification to determine whichelements can be merged (Corradini et al, 2002;Stiefelhagen et al, 2004).
These are some-times combined with search techniques coupledwith penalties (Einstein and Christoudias, 2004;Brooks and Breazeal, 2006).
With the exceptionof Bolt?s system, these systems were tested on ut-terances that were quite short and constrained.Our approach integrates spatial and temporalaspects of gesture into our probabilistic formal-ism (Zukerman et al, 2008), focusing on the ef-fect of pointing on object salience.
Other salience-based approaches are described in (Einstein andChristoudias, 2004; Huls et al, 1995).
How-ever, they are not directly comparable with ourapproach, as they use salience to weigh the im-portance of factors pertaining to gesture-speechalignment, but there is no uncertainty associatedwith the visual salience resulting from pointing.Our use of a probabilistic parser enables usto handle more complex utterances than thoseconsidered by most speech-gesture systems (Sec-tion 2).
At the same time, we do not yet handlespeech disfluencies, which are currently handledby (Einstein and Christoudias, 2004; Stiefelhagenet al, 2004).
Also, at present we do not considerthe challenges pertaining to the real-time synchro-nization of the output of a gesture-sensing anda speech-recognition system (Stiefelhagen et al,2004; Brooks and Breazeal, 2006).6 Conclusion and Future WorkWe have extended Scusi?, our spoken language in-terpretation system, to incorporate pointing ges-tures.
Specifically, we have offered a formalismthat takes into account relationships between as-pects of gesture and spoken language to integrateinformation about pointing gestures into the es-timation of the probability of candidate interpre-tations of an utterance.
Our empirical evaluationshows that our formalism significantly improvesinterpretation accuracy.In the future, we propose to refine our modelof demonstrative determiners.
We also intend toperform sensitivity analysis regarding the accu-racy of the vision system, and that of the gesturerecognition system.
In addition, we will conductuser studies to gain insights with respect to con-ditions that influence the probability of pointing,e.g., type of object and its position relative to thespeaker.AcknowledgmentsThis research was supported in part by ARC grantDP0878195.
The authors thank R. Jarvis and D.Li for their help with the gesture system.1565ReferencesBolt, R.A. 1980.
?Put-that-there?
: voice and ges-ture at the graphics interface.
In Proceedings ofthe 7th Annual Conference on Computer Graphicsand Interactive Techniques, pages 262?270, Seattle,Washington.Brooks, A.G. and C. Breazeal.
2006.
Working withrobots and objects: Revisiting deictic reference forachieving spatial common ground.
In Proceedingsof the 1st ACM SIGCHI/SIGART Conference onHuman-robot Interaction, pages 297?304, Salt LakeCity, Utah.Corradini, A., R.M.
Wesson, and P.R.
Cohen.
2002.A Map-Based system using speech and 3D gesturesfor pervasive computing.
In ICMI?02 ?
Proceedingsof the 4th International Conference on MultimodalInterfaces, pages 191?196, Pittsburgh, Pennsylva-nia.Dale, R. and E. Reiter.
1995.
Computational in-terpretations of the Gricean maxims in the gener-ation of referring expressions.
Cognitive Science,18(2):233?263.Einstein, J. and C.M.
Christoudias.
2004.
A salience-based approach to gesture-speech alignment.
InProceedings of the Annual Conference of the NorthAmerican Chapter of the Association for Compu-tational Linguistics, pages 25?32, Boston, Mas-sachusetts.Fransen, B., V. Morariu, E. Martinson, S. Blis-ard, M. Marge, S. Thomas, A. Schultz, andD.
Perzanowski.
2007.
Using vision, acoustics, andnatural language for disambiguation.
In Proceed-ings of the ACM/IEEE International Conference onHuman-robot Interaction, pages 73?80, Washing-ton, DC.Gildea, D. and D. Jurafsky.
2002.
Automatic label-ing of semantic roles.
Computational Linguistics,28(3):245?288.Huls, C., W. Claassen, and E. Bos.
1995.
Automaticreferent resolution of deictic and anaphoric expres-sions.
Computational Linguistics, 21(1):59?79.Johnston, M., S. Bangalore, G. Vasireddy, A. Stent,P.
Ehlen, M. Walker, S. Whittaker, and P. Maloor.2002.
MATCH: an architecture for multimodal di-alogue systems.
In Proceedings of the 40th AnnualMeeting on Association for Computational Linguis-tics, pages 376?383, Philadelphia, Pennsylvania.Leacock, C. and M. Chodorow.
1998.
Combining lo-cal context and WordNet similarity forword senseidentification.
In Fellbaum, C., editor, WordNet: AnElectronic Lexical Database, pages 265?285.
MITPress.Li, Z. and R. Jarvis.
2009.
Real time hand gesturerecognition using a range camera.
In Proceedingsof the Australasian Conference on Robotics and Au-tomation, Sydney, Australia.Makalic, E., I. Zukerman, M. Niemann, andD.
Schmidt.
2008.
A probabilistic model for under-standing composite spoken descriptions.
In PRICAI2008 ?
Proceedings of the 10th Pacific Rim Interna-tional Conference on Artificial Intelligence, pages750?759, Hanoi, Vietnam.Nickel, K. and R. Stiefelhagen.
2003.
Pointinggesture recognition based on 3D-tracking of face,hands and head orientation.
In ICMI?03 ?
Pro-ceedings of the 5th International Conference onMultimodal Interfaces, pages 140?146, Vancouver,British Columbia.Sowa, J.F.
1984.
Conceptual Structures: InformationProcessing in Mind and Machine.
Addison-Wesley,Reading, MA.Stiefelhagen, R., C. Fugen, R. Gieselmann,H.
Holzapfel, K. Nickel, and A. Waibel.
2004.Natural human-robot interaction using speech, headpose and gestures.
In IROS 2004 ?
Proceedingsof the IEEE/RSJ International Conference onIntelligent Robots and Systems, volume 3, pages2422?2427, Sendai, Japan.Zukerman, I., E. Makalic, M. Niemann, and S. George.2008.
A probabilistic approach to the interpreta-tion of spoken utterances.
In PRICAI 2008 ?
Pro-ceedings of the 10th Pacific Rim International Con-ference on Artificial Intelligence, pages 581?592,Hanoi, Vietnam.1566
