Proceedings of SIGDIAL 2009: the 10th Annual Meeting of the Special Interest Group in Discourse and Dialogue, pages 152?155,Queen Mary University of London, September 2009. c?2009 Association for Computational LinguisticsDialog System for Mixed Initiative One-Turn Address Entry and ErrorRecoveryRajesh Balchandran, Leonid Rachevsky, Larry Sansone, Roberto SicconiIBM T J Watson Research Center, Yorktown Heights, NY 10598, USArajeshb,lrachevs,lsansone,rsicconi@us.ibm.comAbstractIn this demonstration we present a mixed-initiative dialog system for address recog-nition that lets users to specify a completeaddresses in a single sentence with ad-dress components spoken in their naturalsequence.
Users can also specify fewer ad-dress components in several ways, basedon their convenience.
The system extractsspecified address components, prompts formissing information, disambiguates itemsindependently or collectively all the whileguiding the user so as to obtain the de-sired valid address.
The language mod-eling and dialog management techniquesdeveloped for this purpose are also brieflydescribed.
Finally, several use cases withscreen shots are presented.
The combinedsystem yields very high task completionaccuracy on user tests.1 IntroductionIn recent years, speech recognition has been em-ployed for address input by voice for GPS nav-igation and similar applications.
Users are typi-cally directed to speak address components one ata time - first a state name, then city, street and fi-nally the house number - typically taking four ormore turns.
In this demonstration we present amixed-initiative dialog system that makes addressinput by voice more natural, so users can speakthe complete address (in normal order) (for e.g.
?Fifteen State Street Boston Massachusetts?
), in asingle turn.
They could also specify fewer addresscomponents as per their convenience, and the sys-tem would be expected to guide them to obtain acomplete and valid address.2 System DescriptionFigure 1 shows the high-level architecture andkey components of the system.
A programmableframework consisting of a system bus that con-nects various components (called plugins) formsthe core of the speech-dialog system.
Key compo-nents include plugins to connect to the ASR (Au-tomatic Speech Recognition) and TTS (Text-To-Speech ) engines, the GUI (Graphical User Inter-face), the Natural Language Processor and the Di-alog Manager.2.1 Speech Recognition and componentExtractionSpeech recognition is carried out using a statisti-cal Language Model (LM) with Embedded Gram-mars (Gillett and Ward, 1998) to represent NamedEntities such as city names, numbers etc.
This pro-vides flexibility for the user, while allowing for dy-namic content to be updated when required, sim-ply by swapping associated embedded grammars.For e.g., the grammar of street names could be up-dated based on the selected city.
The IBM Embed-ded Via Voice (EVV) (Sicconi et al, 2009) (Beranet al, 2004) ASR engine provides this functional-ity and is used in this system.In this system, a two-pass speech recognitiontechnique (Balchandran et al, 2009) is employed,based on multiple LMs where, the first pass is usedto accurately recognize some components, and thevalues of these components are used to dynam-ically update another LM which is used for thesecond pass to recognize remaining components.Specifically, the first LM is used to recognize thecity and state while the second is used to recognizethe street name and number.
The street names andoptionally the house number embedded grammars152Figure 1: System Architecturein the second LM are updated based on the city andstate recognized using the first LM.
This is carriedout transparent to the user - so the user perceivesfull address recognition in one step.2.2 Dialog managementA key part of this system is the dialog manage-ment component that handles incompletely spec-ified input, various types of ambiguities and er-ror conditions, all the while having an intelligentdialog with the user so as to correct these er-rors and obtain a valid address at the end.
Agoal oriented approach for managing the dialogthat does not require manual identification of allpossible scenarios was employed and is describedin (Balchandran et al, 2009).
The algorithm iter-atively tries to achieve the goal (getting valid val-ues for all address components), validating avail-able input components, and prompting for miss-ing input components, as defined by a priority or-der among components.
This algorithm was im-plemented on a state based, programmable dialogmanager as shown in Figure 1.3 ScenariosThe following scenarios illustrate different situa-tions that need to be handled by the dialog systemwhen processing addresses.3.1 Successful one-turn address recognitionFigure 2 shows the scenario where the user speaksa complete address in one sentence and the systemrecognizes it correctly.3.2 One-turn address with error correctionThe user speaks a complete address, but the sys-tem mis-recognizes the street name and number(Figure 3 (b)).
The user requests to ?go back?
andthe system re-prompts the user for the street nameand number.
User repeats the number in a differentway (Figure 3 (c)) and the system gets it correctly.3.3 Street and number around currentlocationIn addition to complete addresses, the languagemodels are built to include streets and numbersaround the ?current location?
of the car.
This datacould be periodically updated based on changingcar positions.
In this example, (Figure 4) the userjust specifies, ?15 Lake View Drive?
and the sys-tem defaults to the current city ?
Shelter Island,NY.3.4 Ambiguous cityIn this example, the user specifies an ambiguouscity name (Figure 5 (a)).
The system prompts theuser to disambiguate by selecting a state.
Oncethe user has done this, the system re-processes thestreet name and number to obtain the full addresswithout needing the user to specify it again.
Thesame concept is applied to other address compo-nents.ReferencesRajesh Balchandran, Leonid Rachevsky, and LarrySansone.
2009.
Language modeling and dia-log management for address recognition.
In Inter-speech.Toma?s Beran, Vladim?
?r Bergl, Radek Hampl, Pavel Kr-bec, Jan Sedivy?, Borivoj Tydlita?t, and Josef Vopicka.2004.
Embedded viavoice.
In TSD, pages 269?274.John Gillett and Wayne Ward.
1998.
A languagemodel combining trigrams and stochastic context-free grammars.
In in International Conferenceon Spoken Language Processing, volume 6, pages2319?2322.Roberto Sicconi, Kenneth White, and Harvey Ruback.2009.
Honda next generation speech user interface.In SAE World Congress, pages 2009?01?0518.153(a) User specifies address (b) System gets correct addressFigure 2: Successful one-turn address recognition(a) User specifies address (b) System makes mistake(c) User corrects erroneous components (d) System gets correct addressFigure 3: One-turn address recognition with error recovery154(a) User specifies street and number(b) System locates street and number around current location(c) System gets correct addressFigure 4: Street and number around current loca-tion (Shelter Island)(a) User specifies address with city which is ambiguous(b) User selects state and system combines previously speci-fied information to get complete address(c) System gets correct addressFigure 5: Ambiguous city example155
