An Improved Heuristic for Ellipsis Processing*Ralph M. WelschedelDepartment of Computer & Information SciencesUniversity of DelawareNewark, Delaware 19711and Norman K. SondheimerSoftware ResearchSperry Univac MS 2G3Blue Bell, Pennsylvania 19424I.
In t roduct ionRobust  response to e l l ips is  ( f ragmen-tary sentences)  is essent ia l  to acceptab lenatura l  language inter faces.
For in-stance, an exper iment  with the REL Engl i shquery system showed 10% e l l ip t ica l  input(Thompson, 1980).In Quirk, et al (1972), three typesof contextua l  e l l ips is  have been identi -fied:I. repet i t ion,  if the ut terance  is af ragment  of the prev ious sentence.2.
rep lacement ,  if the input rep laces  as t ructure  in the previous sentence.3.
expansion,  if the input adds a newtype of st ructure to those used in theprev ious  sentence.Ins tances  of the three types appear  inthe fo l lowing  example.Were you angry?a) I was.b) Fur ious.c) Probably.d) For a time.e) Very.f) I did not want to be.g) Yes terday  I was.
( repet i ion  withchange in person)( rep lacement)(expansion)(expansion)(expansion)(expansion)(expans ion  &repet i t ion)In add i t ion  to appear ing  as answers fol-lowing quest ions,  any of the three typescan appear  in quest ions  fo l lowing state-ments, s tatements  fo l lowing statements ,  orin the ut terances  of a s ingle speaker.This paper presents  a method of au-tomat ica l ly  in terpret ing  e l l ips is  based ond ia logue context.
Our method expands onp~evious work by a l lowing for expans ione l l ips is  and by a l lowing for all combina-t ions of s tatement fo l lowing quest ion,quest ion  fo l lowing statement,  quest ionfo l lowing quest ion,  etc.
*This material is based upon work partially sup-ported by the National Science Foundation underGrant No.
IST-8009673.2.
Re lated WorkSevera l  natura l  language systems(e.g., Bobrow et al, 1977; Hendr ix  etal., 1978; Kwasny  and Sondheimer ,  1979)inc lude heur i s t i cs  for rep lacement  andrepet i t ion  el l ips is ,  but not expans ionel l ips is .
One genera l  s t rategy has beento subst i tu te  f ragments  into the ana lys isof the previous input, e.g., subst i tu t ingparse trees of the e l l ip t i ca l  input intothe parse trees of the prev ious input inL IFER (Hendrix,  et al, 1978).
This onlyappl ies  to inputs of the same type, e.g.,repeated quest ions.A l len (1979) deals with some examplesof expans ion  e l l ips is ,  by f i t t ing a parsede l l ip t i ca l  input into a model of thespeaker ' s  plan.
This is s imi lar  to othermethods that interpret  f ragments  by plac-ing them into prepared f ie lds in frames orcase slots (Schank et al, 1980; Hayes andMouradian,  1980; Waltz, 1978).
This ap-proach seems most app l i cab le  to l imited-domain systems.3.
The Heur i s t i cThere are three aspects to our solu-tien: a mechan ism for repet i t ion  andrep lacement  e l l ips is ,  an extens ion  forinputs of d i f ferent  types, such as frag-mentary  answers  to quest ions,  and an ex-tens ion for expans ion  el l ips is .3.1 Repet i t ion  and Rep lacementAs noted above, repet i t ion  and re-p lacement  e l l ips is  can be viewed as sub-s t i tu t ion  in the previous form.
We haveimplemented this not ion in an augmentedt rans i t ion  network  (ATN) grammar inter-preter  with the assumpt ion  that the "pre-vious form" is the complete  ATN path thatparsed the previous input and that thelexical  items consumed along that path areassoc iated  with the arcs that consumedthem.
In e l l ips is  mode, the ATN inter-preter  executes the path using the e l l ipt -ical input in the fo l lowing way:85I.
Words from the e l l ip t ica l  input,i.e., the cur ren~ input, may be con-sumed along the path at any point.2.
Any arc requ i r ing  a word not foundin the current  input may bet raversed us ing the lexical  itemassoc ia ted  with the arc from theprev ious input.3.
However,  once the path consumes thef irst word from the e l l ip t i ca linput, all words from the e l l ip t i ca linput must be consumed before an arccan use a word from the prev iousinput.4.
T ravers ing  a PUSH arc may be accom ~p l ished ei ther  by fo l low ing  the sub-path of the previous input or byf ind ing any const i tuent  ef the re-qui red type in the current  input.The ent ire ATN can be used in thesecases.Suppose that the path for "Were youangry?"
is given by Table I. Squarebrackets  are used to ind icate subpathsresu l t ing  from PUSHes.
"..." ind icatestests and act ions  wh ich  are i r re levant  tethe current  d iscuss ion.01d LexicalState Arc ItemS (CAT COPULA ... (TO Sx)) "w--~'r~e"Sx (PUSH NP .
.
.
(TO Sy))\[NP (CAT PRO .
.
.
(TO NPa)) "you"NPa (POP ...) \]Sy (CAT ADJ ... (TO Sz)) "angry"Sz (POP .
.
.
)Table IAn ATN Path for "Were you Angry?
"An e l l ip t i ca l  input of "Was he?"
fol-lowing "Were you angry?"
could be under-steed by t ravers ing  all of the arcs as inTable I. Fo l low ing  point I above, "was"and "he" would be subst i tuted for "were"and "you".
Fo l low ing  point 3, in t ravers-ing the arc (CAT ADJ ... (TO Sz)) the lex-ical item "angry" from the previous inputwould be used.
Item 4 is i l lus t rated byan e l l ip t i ca l  input of "Was the old man?
";this is unders tood  by t ravers ing the arcsat the S level of Table I, but using theappropr ia te  path in the NP network toparse the old man3.2 T rans format ions  of the Prev ious FormWhi le the approach i l lus t ra ted  inSect ion  3.1 is usefu l  in a data base queryenv i ronment  where ~\ ] l ip t ica l  input typi-cal ly is a mod l f i ca t ion  of the prev iousquery, it does not account for e l l ip t ica ls tatements  fo l lowing quest ions,  e l l ip t i ca lquest ions  fo l lowing statements ,  etc.
Ourapproach to the prob lem is to write a setef t rans format ions  which map the parsepath of a quest ion  (e.g., Table I) into anexpected parse path for a dec la ra t iveresponse,  and the parse ~path for a de-c larat ive  into a path for an expectedquest ion,  etc.The le f t -hand side of a t rans forma-tion is a pat tern  which is matched againstthe ATN path of the previous ut terance.Pat tern  e lements  inc lude l i tera ls  refer-r ing te arcs, var iab les  which match a sin-gle arc or embedded path, var iab les  whichmatch zero or mere arcs, and sets ef al-ternat ives .
It is s t ra ight fo rward  to con-struct  a d i sc r iminat ion  net cor respond ingto all le f t -hand sides for e f f i c ient lyf ind ing what pat terns  match the ATN pathof the previous sentence.
The r ight -handside ef a t rans format ion  is a pat ternwhich const ructs  an expected path.
Theform of the pat tern  en the r ight -hand sideis a l ist of re ferences  to states,  arcs,and lexical  entr ies.
Such re ferences  canbe made through items matched on thele f t -hand side or by expl ic i t  const ruct ionef l i tera l  path e lements .Our technique is to rest r ic t  the map-ping such that any expected parse path isgenerated by app ly ing  only one t rans forma-tion and app ly ing it only once.
A spec ia lfeature of our t rans format iona l  system isthe automat ic  a l lowance  for d ia loguediexis.
An expected parse path for theanswer to "Were you angry?"
is g iven inTable 2.
Note in Table 2, "you" has be-come "I" and "were" has become "was"Old Lex ica lState Arc Item(PUSH NP ... (TO Sa))(CAT PRO ... (TO NPa))(PoP ...)(CAT COPULA .
.
.
(TO Sy))(CAT ADJ ... (TO Sz))(POP .
.
.
)S\[NP "I"NPa \]Sa "was "Sy "angry"SzTable 2Dec larat ive  for the expected answerfor "Were you angry?
".Us ing this path, the e l l ips is  in terpreterde'scribed in Sect ion 3.1 would unders tandthe e l l ipses in "a)" and "b)" below, inthe same way as "a')" and "b'i"a) I was.a') I was angry.b) ~y spouse was.b') My spouse was angry.863.3 Expans ionsA large class of expans ions  are sim-ple adjuncts ,  such as examples c, d, e,and g in sect ion  I.
We have handled thisby bu i ld ing  our e l l ips is  in terpreter  toa l low depar t ing  from the base path atdes ignated  states to consume an ad junctfrom the input str ing.
We mark states inthe grammar where ad juncts  can occur.
Foreach such state, we l ist a set of l inear( though poss ib ly  cycl ic)  paths, cal led"expans ion  paths".
Our in terpreter  asimp lemented  al lows depar tures  from thebase path at any state so marked in thegrammar;  it fo l lows expans ion  paths byconsuming words from the input str ing, andmust return to a state on the base form.Each of the examples  in c, d, e, and g ofsect ion  I can be handled by expans ionpaths only one arc long.
They are givenin Table 3.In i t ia lStateSyExpans ion  Path(PUSH ADVERB ... (TO S))P robab ly  (I was angry).
(PUSH PF  .
.
.
(To s))For a time (I was angry).
(PUS~ ~P(* this inc ludes a teatthat the NP is oneof time or place)?
.. (TO S))Yes terday  (I was angry).
(PUSH INTENSIF IER-ADVERB.
.
.
(TO Sy) )(I was) very (angry).Table 3Example  Expans ion  PathsSince this is an extens ion  to the e l l ips isin terpreter ,  combinat ions  of repet i t ion,rep lacement ,  and expans ion  can all be han-dled by the one mechanism.
For instance,in response to "Were you angry?
",  "Yester-day you were (angry)" would be treatedus ing the expans ion  and rep lacementmechan isms.~.
Spec ia l  Cases and L imi ta t ionsThe ideal model  of contextua l  el-l ipsis would cor rect ly  predict  what areappropr ia te  e l l ip t i ca l  forms in context,what their  in terpreta t ion  is, and whatforms are not mean ingfu l  in context .
Webel ieve this requires  s t ructura l  restr ic -tions, semant ic  constra ints ,  and a modelof the goals of the speaker.
Our heur is-tic does not meet these cr i ter ia  in anumber of cases.Only two c lasses of s t ruc tura l  con-s t ra ints  are captured.
One re lates thee l l ips is  to the prev ious  form as a combi-nat ion  of repet i t ion ,  rep lacement ,  andexpans ion.
The o~her const ra in t  is thatthe input must be consumed as a cont iguousstr ing.
This const ra in t  is v io lated,  forinstance,  in "I was (angry) yes terday"  asa response to "Were you angry?
"Never the less ,  the const ra in t  is computa-t iona l ly  useful ,  s ince a l low ing  arb i t ra rygaps in consuming  the e l l ip t i ca l  inputproduces  a very large space of cor rectin terpreta t ions .
A lud icrous  example isthe fo l lowing quest ion  and e l l ip t i ca lresponse:Has the boss given our mutual  fr iend araise?A fat raise.A l low ing  arb i t ra ry  gaps between the sub-str ings of the e l l ips is  a l lows an in-te rpreta t ion  such as "A (boss has givenour) fat ( fr iend a) raise.
"Whi le it may be poss ib le  to v iew allcontextua l  e l l ips is  as combinat ions  of theoperat ions  repet i t ion,  rep lacement ,  andexpans ion  appl ied  to something,  our modelmakes the strong assumpt ion  that theseoperat ions  may be viewed as app ly ing  to anATN path rather s t ra ight fo rward ly  re latedto the prev ious  ut terance.
Not all expan-s ions can be viewed that way, as example fin Sect ion  I i l lus t rates .
Also, answersof "No" require specia l  process ing;  thatresponse in answer  to "Were you angry"should not be in terpreted  as "No, I wasangry."
One should be able to account  forsuch examples  wi th in  the heur i s t i cdescr ibed  in this paper, perhaps by a l low-ing the t rans format ion  system descr ibed insect ion  3.2 to be complete ly  genera l  rath-er than st rongly  res t r i c ted  to one andonly one t rans format ion  app l i cat ion .
Row-ever, we propose hand l ing  such cases byspec ia l  purpose rules we are deve lop ing .These rules for the spec ia l  cases, plusthe mechan ism descr ibed in sect ion 3 to-gether  wil l  be formal ly  equ iva lent  inp red ic t ive  power to a grammar  for e l l ip t i -cal forms.Though the heur i s t i c  is independentof the ind iv idua l  grammar,  des ignat ingexpans ion  paths and t rans format ions  obvi-ously is not.
The grammar  may make thisan easy oz" d i f f i cu l t  task.
For instancein the grammar we are using, a subnetworkthat co l lects  all tense, aspect,  and mo-dal i ty  e lements  would s impl i fy  some of thet rans format ions  and expans ion  paths.~atura l ly ,  semant ics  must play animportant  part in e l l ips is  process ing.Cons ider  the ut terance pair below:87Did the bess have a martini  at lunch?Some wine.Though syntact ical ly  this could be inter-preted either as "Some wine (did have amartini  at lunch)", "(The boss did have)some wine (at lunch)", or "(The boss didhave a martini  at) some wine".
Semanticsshould prefer the second reading.
We aretesting our heurist ic using the RUS gram-mar (Bebrow, 1978) which has frequentcalls from the grammar requesting that thesemantic component decide whether to builda semantic interpretat ion for the partialparse found or to veto that partial parse.This should aid performance.~.
Summary and ConclusionThere are three aspects te oursolution: a mechanism for repetit ion andreplacement ell ipsis, an extension forinputs of different types, such as frag-mentary answers to questions, and an ex-tension for expansion ell ipsis.Our heurist ic deals with the threetypes of expansion ell ipsis as follows:Repet i t ion el l ipsis is processed by re-peating specific parts of a transformedprevious path using the same phrases as inthe transformed form ("I was angry").Replacement ell ipsis is processed by sub-st ituting the el l ipt ical  input for contig-uous const ituents on a transformed previ-ous path.
Expansion ell ipsis may be pro-cessed by taking special ly marked pathsthat detour from a given state in thatpath.
Combinat ions of the three types ofel l ipsis are represented by combinat ionsof the three var iat ions in a transformedprevious path.There are two contr ibut ions of thework.
First, our method allows for expan-sion ell ipsis.
Second, it accounts forcombinat ions of previous sentence form andell ided form, e.g., statement fol lowingquestion, question fol lowing statement,question fol lowing question.
Furthermore,the method works without any constraintson the ATN grammar.
The heurist ics carryover to formalisms similar to the ATN,such as context-free grammars and augment-ed phrase structure grammars.Our study of ell ipsis is part of amuch broader framework we are developingfor processing syntact ical ly  and/orsemantical ly i l l - formed input; seeWeischedel and Sondheimer (1981).ReferencesAllen, James F., "A Plan-Based Approach toSpeech Act Recognit ion,"  Ph.D. Thesis,Dept.
of 'Computer Science, Univers i ty  ofToronto, Toronto, Canada, 1979.Bobrew, D., R. Kaplan, M. Kay, D. Norman,H.
Thompson and T. Winograd, "GUS, AFrame-dr iven Dialog System", Art i f ic ia lIntel l igence, 8, (1977), 155-173.Bobrow, R., "The RUS System", in Researchin Natural  Language Understandin$,  by B.Webber and R. Bobrow, BBN Report No.
3878,Belt Beranek and Newman, Inc., Cambridge,MA, 1978.Hayes, P. and G. Mouradian, "FlexibleParsing", in Proc.
of the 18th AnnualMeet in~ of the Assoc.
for Cemp.
Ling.,Phi ladelphia,  June, 1980, 97-103.Hendrix, G., E. Sacerdoti,  D. Sagalowiczand J. Slocum, "Developing a NaturalLanguage Interface to Complex Data", ACMTrans.
on Database S~s., 3, 2, (1978--~,105-147.Kwasny, S. and N. Sondheimer, "Ungrammati-cality and Extragrammat ica l i ty  in NaturalLanguage Understanding Systems", in Proc.ef the 17th Annual Meeting of the Assoc.for Comp.
Lin~., San Diego, August, 1979,19-23.Quirk, R., S. Greenbaum, G. Leech and J.Svartvik, A Grammar of Centempory English,Seminar Press, New York, 1972.Schank, R., M. Lebowitz and L. Birnbaum,"An Integrated Understander",  AmericanJournal of Comp.
Ling., 6, I, (1980),13-30.Thompson, B. H., "Linguist ic Analysis of'Natural  Language Communicat ion with Com-puters", p~'oceedings o f  the EighthInternational  Conference on Computat ionaiLinguist ics, Tokyo, October, 1980,190-201.Waltz, D., "An English Language Quest ionAnswering System for a Large Relat ionalDatabase", Csmm.
ACM, 21, 7, (1978),526-559.Weischedel,  Ralph M. and Norman K. Son-dheimer, "A Framework for Processing Ill-Formed Input", Technical  Report, Dept.
ofComputer & Informatiou Sciences, Universi -ty of Delaware, Ne~ark, DE, 1981.Acknowledgement~luch credit is due to Amir Razi forhis programming assistance.88
