Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 359?367,Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational LinguisticsRecognizing Named Entities in TweetsXiaohua Liu ?
?, Shaodian Zhang?
?, Furu Wei ?, Ming Zhou ?
?School of Computer Science and TechnologyHarbin Institute of Technology, Harbin, 150001, China?Department of Computer Science and EngineeringShanghai Jiao Tong University, Shanghai, 200240, China?Microsoft Research AsiaBeijing, 100190, China?
{xiaoliu, fuwei, mingzhou}@microsoft.com?
zhangsd.sjtu@gmail.comAbstractThe challenges of Named Entities Recogni-tion (NER) for tweets lie in the insufficientinformation in a tweet and the unavailabil-ity of training data.
We propose to com-bine a K-Nearest Neighbors (KNN) classi-fier with a linear Conditional Random Fields(CRF) model under a semi-supervised learn-ing framework to tackle these challenges.
TheKNN based classifier conducts pre-labeling tocollect global coarse evidence across tweetswhile the CRF model conducts sequential la-beling to capture fine-grained information en-coded in a tweet.
The semi-supervised learn-ing plus the gazetteers alleviate the lack oftraining data.
Extensive experiments show theadvantages of our method over the baselinesas well as the effectiveness of KNN and semi-supervised learning.1 IntroductionNamed Entities Recognition (NER) is generally un-derstood as the task of identifying mentions of rigiddesignators from text belonging to named-entitytypes such as persons, organizations and locations(Nadeau and Sekine, 2007).
Proposed solutions toNER fall into three categories: 1) The rule-based(Krupka and Hausman, 1998); 2) the machine learn-ing based (Finkel and Manning, 2009; Singh et al,2010) ; and 3) hybrid methods (Jansche and Abney,2002).
With the availability of annotated corpora,such as ACE05, Enron (Minkov et al, 2005) and?
This work has been done while the author was visitingMicrosoft Research Asia.CoNLL03 (Tjong Kim Sang and DeMeulder, 2003),the data driven methods now become the dominatingmethods.However, current NER mainly focuses on for-mal text such as news articles (Mccallum and Li,2003; Etzioni et al, 2005).
Exceptions include stud-ies on informal text such as emails, blogs, clini-cal notes (Wang, 2009).
Because of the domainmismatch, current systems trained on non-tweetsperform poorly on tweets, a new genre of text,which are short, informal, ungrammatical and noiseprone.
For example, the average F1 of the Stan-ford NER (Finkel et al, 2005) , which is trainedon the CoNLL03 shared task data set and achievesstate-of-the-art performance on that task, drops from90.8% (Ratinov and Roth, 2009) to 45.8% on tweets.Thus, building a domain specific NER for tweetsis necessary, which requires a lot of annotated tweetsor rules.
However, manually creating them is tediousand prohibitively unaffordable.
Proposed solutionsto alleviate this issue include: 1) Domain adaption,which aims to reuse the knowledge of the source do-main in a target domain.
Two recent examples areWu et al (2009), which uses data that is informa-tive about the target domain and also easy to be la-beled to bridge the two domains, and Chiticariu etal.
(2010), which introduces a high-level rule lan-guage, called NERL, to build the general and do-main specific NER systems; and 2) semi-supervisedlearning, which aims to use the abundant unlabeleddata to compensate for the lack of annotated data.Suzuki and Isozaki (2008) is one such example.Another challenge is the limited information intweet.
Two factors contribute to this difficulty.
One359is the tweet?s informal nature, making conventionalfeatures such as part-of-speech (POS) and capital-ization not reliable.
The performance of currentNLP tools drops sharply on tweets.
For example,OpenNLP 1, the state-of-the-art POS tagger, getsonly an accuracy of 74.0% on our test data set.
Theother is the tweet?s short nature, leading to the ex-cessive abbreviations or shorthand in tweets, andthe availability of very limited context information.Tackling this challenge, ideally, requires adaptingrelated NLP tools to fit tweets, or normalizing tweetsto accommodate existing tools, both of which arehard tasks.We propose a novel NER system to address thesechallenges.
Firstly, a K-Nearest Neighbors (KNN)based classifier is adopted to conduct word levelclassification, leveraging the similar and recentlylabeled tweets.
Following the two-stage predic-tion aggregation methods (Krishnan and Manning,2006), such pre-labeled results, together with otherconventional features used by the state-of-the-artNER systems, are fed into a linear Conditional Ran-dom Fields (CRF) (Lafferty et al, 2001) model,which conducts fine-grained tweet level NER.
Fur-thermore, the KNN and CRF model are repeat-edly retrained with an incrementally augmentedtraining set, into which high confidently labeledtweets are added.
Indeed, it is the combination ofKNN and CRF under a semi-supervised learningframework that differentiates ours from the exist-ing.
Finally, following Lev Ratinov and Dan Roth(2009), 30 gazetteers are used, which cover com-mon names, countries, locations, temporal expres-sions, etc.
These gazetteers represent general knowl-edge across domains.
The underlying idea of ourmethod is to combine global evidence from KNNand the gazetteers with local contextual information,and to use common knowledge and unlabeled tweetsto make up for the lack of training data.12,245 tweets are manually annotated as the testdata set.
Experimental results show that our methodoutperforms the baselines.
It is also demonstratedthat integrating KNN classified results into the CRFmodel and semi-supervised learning considerablyboost the performance.Our contributions are summarized as follows.1http://sourceforge.net/projects/opennlp/1.
We propose to a novel method that combinesa KNN classifier with a conventional CRFbased labeler under a semi-supervised learningframework to combat the lack of information intweet and the unavailability of training data.2.
We evaluate our method on a human anno-tated data set, and show that our method outper-forms the baselines and that both the combina-tion with KNN and the semi-supervised learn-ing strategy are effective.The rest of our paper is organized as follows.
Inthe next section, we introduce related work.
In Sec-tion 3, we formally define the task and present thechallenges.
In Section 4, we detail our method.
InSection 5, we evaluate our method.
Finally, Section6 concludes our work.2 Related WorkRelated work can be roughly divided into three cat-egories: NER on tweets, NER on non-tweets (e.g.,news, bio-logical medicine, and clinical notes), andsemi-supervised learning for NER.2.1 NER on TweetsFinin et al (2010) use Amazons Mechanical Turkservice 2 and CrowdFlower 3 to annotate named en-tities in tweets and train a CRF model to evaluatethe effectiveness of human labeling.
In contrast, ourwork aims to build a system that can automaticallyidentify named entities in tweets.
To achieve this,a KNN classifier with a CRF model is combinedto leverage cross tweets information, and the semi-supervised learning is adopted to leverage unlabeledtweets.2.2 NER on Non-TweetsNER has been extensively studied on formal text,such as news, and various approaches have been pro-posed.
For example, Krupka and Hausman (1998)use manual rules to extract entities of predefinedtypes; Zhou and Ju (2002) adopt Hidden MarkovModels (HMM) while Finkel et al (2005) use CRFto train a sequential NE labeler, in which the BIO(meaning Beginning, the Inside and the Outside of2https://www.mturk.com/mturk/3http://crowdflower.com/360an entity, respectively) schema is applied.
Othermethods, such as classification based on MaximumEntropy models and sequential application of Per-ceptron or Winnow (Collins, 2002), are also prac-ticed.
The state-of-the-art system, e.g., the StanfordNER, can achieve an F1 score of over 92.0% on itstest set.Biomedical NER represents another line of activeresearch.
Machine learning based systems are com-monly used and outperform the rule based systems.A state-of-the-art biomedical NER system (Yoshidaand Tsujii, 2007) uses lexical features, orthographicfeatures, semantic features and syntactic features,such as part-of-speech (POS) and shallow parsing.A handful of work on other domains exists.
Forexample, Wang (2009) introduces NER on clinicalnotes.
A data set is manually annotated and a linearCRF model is trained, which achieves an F-score of81.48% on their test data set; Downey et al (2007)employ capitalization cues and n-gram statistics tolocate names of a variety of classes in web text;most recently, Chiticariu et al (2010) design and im-plement a high-level language NERL that is tunedto simplify the process of building, understanding,and customizing complex rule-based named-entityannotators for different domains.Ratinov and Roth (2009) systematically studythe challenges in NER, compare several solutionsand report some interesting findings.
For exam-ple, they show that a conditional model that doesnot consider interactions at the output level per-forms comparably to beam search or Viterbi, andthat the BILOU (Beginning, the Inside and the Lasttokens of multi-token chunks as well as Unit-lengthchunks) encoding scheme significantly outperformsthe BIO schema (Beginning, the Inside and Outsideof a chunk).In contrast to the above work, our study focuseson NER for tweets, a new genre of texts, which areshort, noise prone and ungrammatical.2.3 Semi-supervised Learning for NERSemi-supervised learning exploits both labeled andun-labeled data.
It proves useful when labeled datais scarce and hard to construct while unlabeled datais abundant and easy to access.Bootstrapping is a typical semi-supervised learn-ing method.
It iteratively adds data that has beenconfidently labeled but is also informative to itstraining set, which is used to re-train its model.
Jiangand Zhai (2007) propose a balanced bootstrappingalgorithm and successfully apply it to NER.
Theirmethod is based on instance re-weighting, whichallows the small amount of the bootstrapped train-ing sets to have an equal weight to the large sourcedomain training set.
Wu et al (2009) propose an-other bootstrapping algorithm that selects bridginginstances from an unlabeled target domain, whichare informative about the target domain and are alsoeasy to be correctly labeled.
We adopt bootstrappingas well, but use human labeled tweets as seeds.Another representative of semi-supervised learn-ing is learning a robust representation of the inputfrom unlabeled data.
Miller et al (2004) use wordclusters (Brown et al, 1992) learned from unla-beled text, resulting in a performance improvementof NER.
Guo et al (2009) introduce Latent Seman-tic Association (LSA) for NER.
In our pilot study ofNER for tweets, we adopt bag-of-words models torepresent a word in tweet, to concentrate our effortson combining global evidence with local informa-tion and semi-supervised learning.
We leave it toour future work to explore which is the best inputrepresentation for our task.3 Task DefinitionWe first introduce some background about tweets,then give a formal definition of the task.3.1 The TweetsA tweet is a short text message containing nomore than 140 characters in Twitter, the biggestmicro-blog service.
Here is an example oftweets: ?mycraftingworld: #Win Microsoft Of-fice 2010 Home and Student *2Winners* #Con-test from @office and @momtobedby8 #Giveawayhttp://bit.ly/bCsLOr ends 11/14?, where ?mycraft-ingworld?
is the name of the user who publishedthis tweet.
Words beginning with the ?#?
char-acter, like ?
?#Win?, ?#Contest?
and ?#Giveaway?,are hash tags, usually indicating the topics of thetweet; words starting with ?
@?, like ?
@office?and ?
@momtobedby8?, represent user names, and?http://bit.ly/bCsLOr?
is a shortened link.Twitter users are interested in named entities, such361Figure 1: Portion of different types of named entities intweets.
This is based on an investigation of 12,245 ran-domly sampled tweets, which are manually labeled.as person names, organization names and productnames, as evidenced by the abundant named entitiesin tweets.
According to our investigation on 12,245randomly sampled tweets that are manually labeled,about 46.8% have at least one named entity.
Figure1 shows the portion of named entities of differenttypes.3.2 The TaskGiven a tweet as input, our task is to identify both theboundary and the class of each mention of entities ofpredefined types.
We focus on four types of entitiesin our study, i.e., persons, organizations, products,and locations, which, according to our investigationas shown in Figure 1, account for 89.0% of all thenamed entities.Here is an example illustrating our task.The input is ?...Me without you is like aniphone without apps, Justin Bieber withouthis hair, Lady gaga without her telephone, itjust wouldn...?
The expected output is as fol-lows:?...Me without you is like an <PRODUCT>iphone</PRODUCT>without apps,<PERSON>Justin Bieber</PERSON>without hishair,<PERSON>Lady gaga</PERSON> withouther telephone, it just wouldn...?, meaning that?iphone?
is a product, while ?Justin Bieber?
and?Lady gaga?
are persons.4 Our MethodNow we present our solution to the challenging taskof NER for tweets.
An overview of our methodis first given, followed by detailed discussion of itscore components.4.1 Method OverviewNER task can be naturally divided into two sub-tasks, i.e., boundary detection and type classifica-tion.
Following the common practice , we adopta sequential labeling approach to jointly resolvethese sub-tasks, i.e., for each word in the inputtweet, a label is assigned to it, indicating both theboundary and entity type.
Inspired by Ratinov andRoth (2009), we use the BILOU schema.Algorithm 1 outlines our method, where: trainsand traink denote two machine learning processesto get the CRF labeler and the KNN classifier, re-spectively; reprw converts a word in a tweet into abag-of-words vector; the reprt function transformsa tweet into a feature matrix that is later fed into theCRF model; the knn function predicts the class ofa word; the update function applies the predictedclass by KNN to the inputted tweet; the crf functionconducts word level NE labeling;?
and ?
representthe minimum labeling confidence of KNN and CRF,respectively, which are experimentally set to 0.1 and0.001; N (1,000 in our work) denotes the maximumnumber of new accumulated training data.Our method, as illustrated in Algorithm 1, repeat-edly adds the new confidently labeled tweets to thetraining set 4 and retrains itself once the numberof new accumulated training data goes above thethreshold N .
Algorithm 1 also demonstrates onestriking characteristic of our method: A KNN clas-sifier is applied to determine the label of the currentword before the CRF model.
The labels of the wordsthat confidently assigned by the KNN classifier aretreated as visible variables for the CRF model.4.2 ModelOur model is hybrid in the sense that a KNN clas-sifier and a CRF model are sequentially applied tothe target tweet, with the goal that the KNN classi-fier captures global coarse evidence while the CRFmodel fine-grained information encoded in a singletweet and in the gazetteers.
Algorithm 2 outlines thetraining process of KNN, which records the labeledword vector for every type of label.Algorithm 3 describes how the KNN classifier4The training set ts has a maximum allowable number ofitems, which is 10,000 in our work.
Adding an item into it willcause the oldest one being removed if it is full.362Algorithm 1 NER for Tweets.Require: Tweet stream i; output stream o.Require: Training tweets ts; gazetteers ga.1: Initialize ls, the CRF labeler: ls = trains(ts).2: Initialize lk, the KNN classifier: lk = traink(ts).3: Initialize n, the # of new training tweets: n = 0.4: while Pop a tweet t from i and t ?= null do5: for Each word w ?
t do6: Get the feature vector w?
: w?
=reprw(w, t).7: Classify w?
with knn: (c, cf) =knn(lk, w?
).8: if cf > ?
then9: Pre-label: t = update(t, w, c).10: end if11: end for12: Get the feature vector t?
: t?
= reprt(t, ga).13: Label t?
with crf : (t, cf) = crf(ls, t?
).14: Put labeled result (t, cf) into o.15: if cf > ?
then16: Add labeled result t to ts , n = n + 1.17: end if18: if n > N then19: Retrain ls: ls = trains(ts).20: Retrain lk: lk = traink(ts).21: n = 0.22: end if23: end while24: return o.Algorithm 2 KNN Training.Require: Training tweets ts.1: Initialize the classifier lk:lk = ?.2: for Each tweet t ?
ts do3: for Each word,label pair (w, c) ?
t do4: Get the feature vector w?
: w?
=reprw(w, t).5: Add the w?
and c pair to the classifier: lk =lk ?
{(w?, c)}.6: end for7: end for8: return KNN classifier lk.predicts the label of the word.
In our work, K isexperimentally set to 20, which yields the best per-formance.Two desirable properties of KNN make it standout from its alternatives: 1) It can straightforwardlyincorporate evidence from new labeled tweets andretraining is fast; and 2) combining with a CRFAlgorithm 3 KNN predication.Require: KNN classifier lk ;word vector w?.1: Initialize nb, the neighbors of w?
: nb =neigbors(lk, w?
).2: Calculate the predicted class c?
: c?
=argmaxc?(w??
,c?
)?nb ?
(c, c?)
?
cos(w?, w??
).3: Calculate the labeling confidence cf : cf =?(w??
,c?
)?nb ?(c,c?)?cos(w?,w??)?(w??
,c?
)?nb cos(w?,w?? )
.4: return The predicted label c?
and its confidence cf .model, which is good at encoding the subtle interac-tions between words and their labels, compensatesfor KNN?s incapability to capture fine-grained evi-dence involving multiple decision points.The Linear CRF model is used as the fine model,with the following considerations: 1) It is well-studied and has been successfully used in state-of-the-art NER systems (Finkel et al, 2005; Wang,2009); 2) it can output the probability of a labelsequence, which can be used as the labeling con-fidence that is necessary for the semi-supervisedlearning framework.In our experiments, the CRF++ 5 toolkit is used totrain a linear CRF model.
We have written a Viterbidecoder that can incorporate partially observed la-bels to implement the crf function in Algorithm 1.4.3 FeaturesGiven a word in a tweet, the KNN classifier consid-ers a text window of size 5 with the word in the mid-dle (Zhang and Johnson, 2003), and extracts bag-of-word features from the window as features.
For eachword, our CRF model extracts similar features asWang (2009) and Ratinov and Roth (2009), namely,orthographic features, lexical features and gazetteersrelated features.
In our work, we use the gazetteersprovided by Ratinov and Roth (2009).Two points are worth noting here.
One is thatbefore feature extraction for either the KNN or theCRF, stop words are removed.
The stop wordsused here are mainly from a set of frequently-usedwords 6.
The other is that tweet meta data is normal-ized, that is, every link becomes *LINK* and every5http://crfpp.sourceforge.net/6http://www.textfixer.com/resources/common-english-words.txt363account name becomes *ACCOUNT*.
Hash tagsare treated as common words.4.4 DiscussionWe now discuss several design considerations re-lated to the performance of our method, i.e., addi-tional features, gazetteers and alternative models.Additional Features.
Features related to chunkingand parsing are not adopted in our final system, be-cause they give only a slight performance improve-ment while a lot of computing resources are requiredto extract such features.
The ineffectiveness of thesefeatures is linked to the noisy and informal nature oftweets.
Word class (Brown et al, 1992) features arenot used either, which prove to be unhelpful for oursystem.
We are interested in exploring other tweetrepresentations, which may fit our NER task, for ex-ample the LSA models (Guo et al, 2009).Gazetteers.
In our work, gazetteers prove to be sub-stantially useful, which is consistent with the obser-vation of Ratinov and Roth (2009).
However, thegazetteers used in our work contain noise, whichhurts the performance.
Moreover, they are static,directly from Ratinov and Roth (2009), thus witha relatively lower coverage, especially for personnames and product names in tweets.
We are devel-oping tools to clean the gazetteers.
In future, we planto feed the fresh entities correctly identified fromtweets back into the gazetteers.
The correctness ofan entity can rely on its frequency or other evidence.Alternative Models.
We have replaced KNN byother classifiers, such as those based on MaximumEntropy and Support Vector Machines, respectively.KNN consistently yields comparable performance,while enjoying a faster retraining speed.
Similarly,to study the effectiveness of the CRF model, it is re-placed by its alternations, such as the HMM labelerand a beam search plus a maximum entropy basedclassifier.
In contrast to what is reported by Ratinovand Roth (2009), it turns out that the CRF modelgives remarkably better results than its competitors.Note that all these evaluations are on the same train-ing and testing data sets as described in Section 5.1.5 ExperimentsIn this section, we evaluate our method on a man-ually annotated data set and show that our systemoutperforms the baselines.
The contributions of thecombination of KNN and CRF as well as the semi-supervised learning are studied, respectively.5.1 Data PreparationWe use the Twigg SDK 7 to crawl all tweetsfrom April 20th 2010 to April 25th 2010, then dropnon-English tweets and get about 11,371,389, fromwhich 15,800 tweets are randomly sampled, and arethen labeled by two independent annotators, so thatthe beginning and the end of each named entity aremarked with<TYPE> and</TYPE>, respectively.Here TYPE is PERSON, PRODUCT, ORGANIZA-TION or LOCATION.
3555 tweets are dropped be-cause of inconsistent annotation.
Finally we get12,245 tweets, forming the gold-standard data set.Figure 1 shows the portion of named entities of dif-ferent types.
On average, a named entity has 1.2words.
The gold-standard data set is evenly split intotwo parts: One for training and the other for testing.5.2 Evaluation MetricsFor every type of named entity, Precision (Pre.
), re-call (Rec.)
and F1 are used as the evaluation met-rics.
Precision is a measure of what percentage theoutput labels are correct, and recall tells us to whatpercentage the labels in the gold-standard data setare correctly labeled, while F1 is the harmonic meanof precision and recall.
For the overall performance,we use the average Precision, Recall and F1, wherethe weight of each name entity type is proportionalto the number of entities of that type.
These metricsare widely used by existing NER systems to evaluatetheir performance.5.3 BaselinesTwo systems are used as baselines: One is thedictionary look-up system based on the gazetteers;the other is the modified version of our systemwithout KNN and semi-supervised learning.
Here-after these two baselines are called NERDIC andNERBA, respectively.
The OpenNLP and the Stan-ford parser (Klein and Manning, 2003) are used toextract linguistic features for the baselines and ourmethod.7It is developed by the Bing social search team, and cur-rently is only internally available.364System Pre.
(%) Rec.
(%) F1(%)NERCB 81.6 78.8 80.2NERBA 83.6 68.6 75.4NERDIC 32.6 25.4 28.6Table 1: Overall experimental results.System Pre.
(%) Rec.
(%) F1(%)NERCB 78.4 74.5 76.4NERBA 83.6 68.4 75.2NERDIC 37.1 29.7 33.0Table 2: Experimental results on PERSON.5.4 Basic ResultsTable 1 shows the overall results for the baselinesand ours with the name NERCB .
Here our sys-tem is trained as described in Algorithm 1, combin-ing a KNN classifier and a CRF labeler, with semi-supervised learning enabled.
As can be seen fromTable 1, on the whole, our method significantly out-performs (with p < 0.001) the baselines.
Tables 2-5report the results on each entity type, indicating thatour method consistently yields better results on allentity types.5.5 Effects of KNN ClassifierTable 6 shows the performance of our methodwithout combining the KNN classifier, denoted byNERCB?KNN .
A drop in performance is observedthen.
We further check the confidently predicted la-bels of the KNN classifier, which account for about22.2% of all predications, and find that its F1 is ashigh as 80.2% while the baseline system based onthe CRF model achieves only an F1 of 75.4%.
Thislargely explains why the KNN classifier helps theCRF labeler.
The KNN classifier is replaced withits competitors, and only a slight difference in per-formance is observed.
We do observe that retrainingKNN is obviously faster.System Pre.
(%) Rec.
(%) F1(%)NERCB 81.3 65.4 72.5NERBA 82.5 58.4 68.4NERDIC 8.2 6.1 7.0Table 3: Experimental results on PRODUCT.System Pre.
(%) Rec.
(%) F1(%)NERCB 80.3 77.5 78.9NERBA 81.6 69.7 75.2NERDIC 30.2 30.0 30.1Table 4: Experimental results on LOCATION.System Pre.
(%) Rec.
(%) F1(%)NERCB 83.2 60.4 70.0NERBA 87.6 52.5 65.7NERDIC 54.5 11.8 19.4Table 5: Experimental results on ORGANIZATION.5.6 Effects of the CRF LabelerSimilarly, the CRF model is replaced by its alterna-tives.
As is opposite to the finding of Ratinov andRoth (2009), the CRF model gives remarkably bet-ter results, i.e., 2.1% higher in F1 than its best fol-lowers (with p < 0.001).
Table 7 shows the overallperformance of the CRF labeler with various featureset combinations, where Fo, Fl and Fg denote theorthographic features, the lexical features and thegazetteers related features, respectively.
It can beseen from Table 7 that the lexical and gazetteer re-lated features are helpful.
Other advanced featuressuch as chunking are also explored but with no sig-nificant improvement.5.7 Effects of Semi-supervised LearningTable 8 compares our method with its modified ver-sion without semi-supervised learning, suggestingthat semi-supervised learning considerably booststhe performance.
To get more details about self-training, we evenly divide the test data into 10 partsand feed them into our method sequentially; werecord the average F1 score on each part, as shownin Figure 2.5.8 Error AnalysisErrors made by our system on the test set fall intothree categories.
The first kind of error, accountingfor 35.5% of all errors, is largely related to slang ex-pressions and informal abbreviations.
For example,our method identifies ?Cali?, which actually means?California?, as a PERSON in the tweet ?i love Caliso much?.
In future, we can design a normalization365System Pre.
(%) Rec.
(%) F1(%)NERCB 81.6 78.8 80.2NERCB?KNN 82.6 74.8 78.5Table 6: Overall performance of our system with andwithout the KNN classifier, respectively.Features Pre.
(%) Rec.
(%) F1(%)Fo 71.3 42.8 53.5Fo + Fl 76.2 44.2 55.9Fo + Fg 80.5 66.2 72.7Fo + Fl + Fg 82.6 74.8 78.5Table 7: Overview performance of the CRF labeler (com-bined with KNN) with different feature sets.component to handle such slang expressions and in-formal abbreviations.The second kind of error, accounting for 37.2%of all errors, is mainly attributed to the data sparse-ness.
For example, for this tweet ?come to see jaxonsomeday?, our method mistakenly labels ?jaxon?as a LOCATION, which actually denotes a PER-SON.
This error is understandable somehow, sincethis tweet is one of the earliest tweets that mention?jaxon?, and at that time there was no strong evi-dence supporting that it represents a person.
Possi-ble solutions to these errors include continually en-riching the gazetteers and aggregating additional ex-ternal knowledge from other channels such as tradi-tional news.The last kind of error, which represents 27.3%of all errors, somehow links to the noise prone na-ture of tweets.
Consider this tweet ?wesley snipesws cought 4 nt payin tax coz ths celebz dnt take itcirus.
?, in which ?wesley snipes?
is not identifiedas a PERSON but simply ignored by our method,because this tweet is too noisy to provide effectivefeatures.
Tweet normalization technology seems apossible solution to alleviate this kind of error.Features Pre.
(%) Rec.
(%) F1(%)NERCB 81.6 78.8 80.2NER?CB 82.1 71.9 76.7Table 8: Performance of our system with and withoutsemi-supervised learning, respectively.Figure 2: F1 score on 10 test data sets sequentially fedinto the system, each with 600 instances.
Horizontal andvertical axes represent the sequential number of the testdata set and the averaged F1 score (%), respectively.6 Conclusions and Future workWe propose a novel NER system for tweets, whichcombines a KNN classifier with a CRF labeler undera semi-supervised learning framework.
The KNNclassifier collects global information across recentlylabeled tweets while the CRF labeler exploits infor-mation from a single tweet and from the gazetteers.A serials of experiments show the effectiveness ofour method, and particularly, show the positive ef-fects of KNN and semi-supervised learning.In future, we plan to further improve the per-formance of our method through two directions.Firstly, we hope to develop tweet normalizationtechnology to make tweets friendlier to the NERtask.
Secondly, we are interested in integratingnew entities from tweets or other channels into thegazetteers.AcknowledgmentsWe thank Long Jiang, Changning Huang, YunboCao, Dongdong Zhang, Zaiqing Nie for helpful dis-cussions, and the anonymous reviewers for theirvaluable comments.
We also thank Matt Callcut forhis careful proofreading of an early draft of this pa-per.ReferencesPeter F. Brown, Peter V. deSouza, Robert L. Mercer, Vin-cent J. Della Pietra, and Jenifer C. Lai.
1992.
Class-based n-gram models of natural language.
Comput.Linguist., 18:467?479.366Laura Chiticariu, Rajasekar Krishnamurthy, YunyaoLi, Frederick Reiss, and Shivakumar Vaithyanathan.2010.
Domain adaptation of rule-based annotatorsfor named-entity recognition tasks.
In EMNLP, pages1002?1012.Michael Collins.
2002.
Discriminative training methodsfor hidden markov models: theory and experimentswith perceptron algorithms.
In EMNLP, pages 1?8.Doug Downey, Matthew Broadhead, and Oren Etzioni.2007.
Locating Complex Named Entities in Web Text.In IJCAI.Oren Etzioni, Michael Cafarella, Doug Downey, Ana-Maria Popescu, Tal Shaked, Stephen Soderland,Daniel S. Weld, and Alexander Yates.
2005.
Unsu-pervised named-entity extraction from the web: an ex-perimental study.
Artif.
Intell., 165(1):91?134.Tim Finin, Will Murnane, Anand Karandikar, NicholasKeller, Justin Martineau, and Mark Dredze.
2010.Annotating named entities in twitter data with crowd-sourcing.
In CSLDAMT, pages 80?88.Jenny Rose Finkel and Christopher D. Manning.
2009.Nested named entity recognition.
In EMNLP, pages141?150.Jenny Rose Finkel, Trond Grenager, and ChristopherManning.
2005.
Incorporating non-local informationinto information extraction systems by gibbs sampling.In ACL, pages 363?370.Honglei Guo, Huijia Zhu, Zhili Guo, Xiaoxun Zhang,Xian Wu, and Zhong Su.
2009.
Domain adapta-tion with latent semantic association for named entityrecognition.
In NAACL, pages 281?289.Martin Jansche and Steven P. Abney.
2002.
Informa-tion extraction from voicemail transcripts.
In EMNLP,pages 320?327.Jing Jiang and ChengXiang Zhai.
2007.
Instance weight-ing for domain adaptation in nlp.
In ACL, pages 264?271.Dan Klein and Christopher D. Manning.
2003.
Accurateunlexicalized parsing.
In ACL, pages 423?430.Vijay Krishnan and Christopher D. Manning.
2006.
Aneffective two-stage model for exploiting non-local de-pendencies in named entity recognition.
In ACL, pages1121?1128.George R. Krupka and Kevin Hausman.
1998.
Isoquest:Description of the netowlTM extractor system as usedin muc-7.
In MUC-7.John D. Lafferty, AndrewMcCallum, and Fernando C. N.Pereira.
2001.
Conditional random fields: Probabilis-tic models for segmenting and labeling sequence data.In ICML, pages 282?289.Andrew Mccallum and Wei Li.
2003.
Early resultsfor named entity recognition with conditional randomfields, feature induction and web-enhanced lexicons.In HLT-NAACL, pages 188?191.Scott Miller, Jethran Guinness, and Alex Zamanian.2004.
Name tagging with word clusters and discrimi-native training.
In HLT-NAACL, pages 337?342.Einat Minkov, Richard C. Wang, and William W. Cohen.2005.
Extracting personal names from email: apply-ing named entity recognition to informal text.
In HLT,pages 443?450.David Nadeau and Satoshi Sekine.
2007.
A survey ofnamed entity recognition and classification.
Linguisti-cae Investigationes, 30:3?26.Lev Ratinov and Dan Roth.
2009.
Design challengesand misconceptions in named entity recognition.
InCoNLL, pages 147?155.Sameer Singh, Dustin Hillard, and Chris Leggetter.
2010.Minimally-supervised extraction of entities from textadvertisements.
In HLT-NAACL, pages 73?81.Jun Suzuki and Hideki Isozaki.
2008.
Semi-supervisedsequential labeling and segmentation using giga-wordscale unlabeled data.
In ACL, pages 665?673.Erik F. Tjong Kim Sang and Fien De Meulder.
2003.
In-troduction to the CoNLL-2003 shared task: language-independent named entity recognition.
In HLT-NAACL, pages 142?147.Yefeng Wang.
2009.
Annotating and recognising namedentities in clinical notes.
In ACL-IJCNLP, pages 18?26.Dan Wu, Wee Sun Lee, Nan Ye, and Hai Leong Chieu.2009.
Domain adaptive bootstrapping for named en-tity recognition.
In EMNLP, pages 1523?1532.Kazuhiro Yoshida and Jun?ichi Tsujii.
2007.
Rerankingfor biomedical named-entity recognition.
In BioNLP,pages 209?216.Tong Zhang and David Johnson.
2003.
A robust riskminimization based named entity recognition system.In HLT-NAACL, pages 204?207.GuoDong Zhou and Jian Su.
2002.
Named entity recog-nition using an hmm-based chunk tagger.
In ACL,pages 473?480.367
