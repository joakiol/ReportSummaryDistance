Proceedings of SIGDIAL 2010: the 11th Annual Meeting of the Special Interest Group on Discourse and Dialogue, pages 124?131,The University of Tokyo, September 24-25, 2010. c?2010 Association for Computational LinguisticsAdaptive Referring Expression Generation in Spoken Dialogue Systems:Evaluation with Real UsersSrinivasan JanarthanamSchool of InformaticsUniversity of Edinburghs.janarthanam@ed.ac.ukOliver LemonInteraction LabMathematics and Computer ScienceHeriot-Watt Universityo.lemon@hw.ac.ukAbstractWe present new results from a real-userevaluation of a data-driven approach tolearning user-adaptive referring expres-sion generation (REG) policies for spokendialogue systems.
Referring expressionscan be difficult to understand in techni-cal domains where users may not knowthe technical ?jargon?
names of the do-main entities.
In such cases, dialogue sys-tems must be able to model the user?s (lex-ical) domain knowledge and use appro-priate referring expressions.
We presenta reinforcement learning (RL) frameworkin which the system learns REG policieswhich can adapt to unknown users on-line.
For real users of such a system, weshow that in comparison to an adaptivehand-coded baseline policy, the learnedpolicy performs significantly better, witha 20.8% average increase in adaptation ac-curacy, 12.6% decrease in time taken, anda 15.1% increase in task completion rate.The learned policy also has a significantlybetter subjective rating from users.
This isbecause the learned policies adapt onlineto changing evidence about the user?s do-main expertise.
We also discuss the issueof evaluation in simulation versus evalua-tion with real users.1 IntroductionWe present new results from an evaluation withreal users, for a reinforcement learning (Suttonand Barto, 1998) framework to learn user-adaptivereferring expression generation policies from data-driven user simulations.
Such a policy allows thesystem to choose appropriate expressions to re-fer to domain entities in a dialogue setting.
Forinstance, in a technical support conversation, theJargon: Please plug one end of the broadbandcable into the broadband filter.Descriptive: Please plug one end of the thinwhite cable with grey ends into thesmall white box.Table 1: Referring expression examples for 2 enti-ties (from the corpus)system could choose to use more technical termswith an expert user, or to use more descriptive andgeneral expressions with novice users, and a mixof the two with intermediate users of various sorts(see examples in Table 1).In natural human-human conversations, dia-logue partners learn about each other and adapttheir language to suit their domain expertise (Is-sacs and Clark, 1987).
This kind of adaptationis called Alignment through AudienceDesign (Clark and Murphy, 1982; Bell, 1984).We assume that users are mostly unknown tothe system and therefore that a spoken dialoguesystem (SDS) must be capable of observing theuser?s dialogue behaviour, modelling his/her do-main knowledge, and adapting accordingly, justlike human interlocutors.
Therefore unlike sys-tems that use static user models, our system has todynamically model the user?s domain knowledgein order to adapt during the conversation.We present a corpus-driven framework forlearning a user-adaptive REG policy from a smallcorpus of non-adaptive human-machine interac-tion.
We show that the learned policy performsbetter than a simple hand-coded adaptive policyin terms of accuracy of adaptation, dialogue timeand task completion rate when evaluated with realusers in a wizarded study.In section 2, we present some of the relatedwork.
Section 3 and section 4 describe the dia-logue system framework and the user simulation124model.
In section 5, we present the training and insection 6, we present the evaluation for differentREG policies with real users.2 Related workRule-based and supervised learning approacheshave been proposed to learn and adapt duringconversations dynamically.
Such systems learnfrom a user at the start and later adapt to the do-main knowledge of the user.
However, they eitherrequire expensive expert knowledge resources tohand-code the inference rules (Cawsey, 1993) or alarge corpus of expert-layperson interaction fromwhich adaptive strategies can be learned and mod-elled, using methods such as Bayesian networks(Akiba and Tanaka, 1994).
In contrast, we presentan approach that learns in the absence of theseexpensive resources.
It is also not clear how su-pervised approaches choose between when to seekmore information and when to adapt.
In this study,we show that using reinforcement learning this de-cision is learned automatically.Reinforcement Learning (RL) has been suc-cessfully used for learning dialogue managementpolicies since (Levin et al, 1997).
The learnedpolicies allow the dialogue manager to optimallychoose appropriate dialogue acts such as instruc-tions, confirmation requests, and so on, underuncertain noise or other environment conditions.There have been recent efforts to learn infor-mation presentation and recommendation strate-gies using reinforcement learning (Hernandez etal., 2003; Rieser and Lemon, 2009; Rieser andLemon, 2010), and joint optimisation of DialogueManagement and NLG using hierarchical RL hasbeen proposed by (Lemon, 2010).
In addition,we present a framework to learn to choose appro-priate referring expressions based on a user?s do-main knowledge.
Following a proof-of-conceptstudy using a hand-coded rule-based user simu-lation (Janarthanam and Lemon, 2009c), we pre-viously showed that adaptive REG policies canbe learned using an RL framework with data-driven user simulations and that such policies per-form better than simple hand-coded policies (Ja-narthanam and Lemon, 2010).3 The Dialogue SystemIn this section, we describe the different modulesof the dialogue system.
The interaction betweenthe different modules is shown in figure 1 (inlearning mode).
The dialogue system presents theuser with instructions to setup a broadband con-nection at home.
In the Wizard of Oz setup, thesystem and the user interact using speech.
How-ever, in our machine learning setup, they interact atthe abstract level of dialogue actions and referringexpressions.
Our objective is to learn to choosethe appropriate referring expressions to refer to thedomain entities in the instructions.Figure 1: System User Interaction (learning)3.1 Dialogue ManagerThe dialogue manager identifies the next dialogueact (As,t where t denotes turn, s denotes system)to give to the user based on the dialogue man-agement policy pidm.
The dialogue managementis coded in the form of a finite state machine.
Inthis dialogue task, the system provides instructionsto either observe or manipulate the environment.When users ask for clarifications on referring ex-pressions, the system clarifies (provide clar) bygiving information to enable the user to associatethe expression with the intended referent.
Whenthe user responds in any other way, the instruc-tion is simply repeated.
The dialogue manageris also responsible for updating and managing thesystem state Ss,t (see section 3.2).
The system in-teracts with the user by passing both the systemaction As,t and the referring expressions RECs,t(see section 3.3).3.2 The dialogue stateThe dialogue state Ss,t is a set of variables thatrepresent the current state of the conversation.
Inour study, in addition to maintaining an overall di-alogue state, the system maintains a user modelUMs,t which records the initial domain knowl-edge of the user.
It is a dynamic model that starts125with a state where the system does not have anyknowledge about the user.
Since the model is up-dated according to the user?s behaviour, it may beinaccurate if the user?s behaviour is itself uncer-tain.
Hence, the user model used in this system isnot always an accurate model of the user?s knowl-edge and reflects a level of uncertainty about theuser.Each jargon referring expression x is repre-sented by a three-valued variable in the dialoguestate: user knows x.
The three values that eachvariable takes are yes, no, not sure.
The vari-ables are updated using a simple user model up-date algorithm after the user?s response each turn.Initially each variable is set to not sure.
If theuser responds to an instruction containing the re-ferring expression x with a clarification request,then user knows x is set to no.
Similarly, ifthe user responds with appropriate information tothe system?s instruction, the dialogue manager setsuser knows x is set to yes.
Only the user?s ini-tial knowledge is recorded.
This is based on theassumption that an estimate of the user?s initialknowledge helps to predict the user?s knowledgeof the rest of the referring expressions.3.3 REG moduleThe REG module is a part of the NLG modulewhose task is to identify the list of domain enti-ties to be referred to and to choose the appropriatereferring expression for each of the domain enti-ties for each given dialogue act.
In this study, wefocus only on the production of appropriate refer-ring expressions to refer to domain entities men-tioned in the dialogue act.
It chooses betweenthe two types of referring expressions - jargonand descriptive.
For example, the domain entitybroadband filter can be referred to using the jar-gon expression ?broadband filter?
or using the de-scriptive expression ?small white box?1.
Althoughadaptation is the primary goal, it should be notedthat in order to get an idea of the user the systemis dealing with, it needs to seek information usingjargon expressions.The REG module operates in two modes - learn-ing and evaluation.
In the learning mode, the REGmodule is the learning agent.
The REG modulelearns to associate dialogue states with optimal re-ferring expressions.
This is represented by a REG1We will use italicised forms to represent the domain enti-ties (e.g.
broadband filter) and double quotes to represent thereferring expressions (e.g.
?broadband filter?
).policy pireg : UMs,t ?
RECs,t, which mapsthe states of the dialogue (user model) to opti-mal referring expressions.
The referring expres-sion choices RECs,t is a set of pairs identifyingthe referent R and the type of expression T used inthe current system utterance.
For instance, the pair(broadband filter, desc) represents the descriptiveexpression ?small white box?.RECs,t = {(R1, T1), ..., (Rn, Tn)}In the evaluation mode, a trained REG policy in-teracts with unknown users.
It consults the learnedpolicy pireg to choose the referring expressionsbased on the current user model.4 User SimulationsIn this section, we present user simulation mod-els that simulate the dialogue behaviour of a realhuman user.
Several user simulation models havebeen proposed for use in reinforcement learningof dialogue policies (Georgila et al, 2005; Schatz-mann et al, 2006; Schatzmann et al, 2007; Ai andLitman, 2007).
However, they are suited only forlearning dialogue management policies, and notnatural language generation policies.
In particular,our model is the first to be sensitive to a system?schoices of referring expressions.
Earlier, we pre-sented a two-tier simulation trained on data pre-cisely for REG policy learning (Janarthanam andLemon, 2009a).
However, it is not suited for train-ing on small corpus like the one we have at ourdisposal.
In contrast to the earlier model, we nowcondition the clarification requests on the referentclass rather than the referent itself to handle thedata sparsity problem.4.1 Corpus-driven action selection modelThe user simulation (US) receives the systemaction As,t and its referring expression choicesRECs,t at each turn.
The US responds with a useraction Au,t (u denoting user).
This can either be aclarification request (cr) or an instruction response(ir).
The US produces a clarification request crbased on the class of the referent C(Ri), type ofthe referring expression Ti, and the current domainknowledge of the user for the referring expressionDKu,t(Ri, Ti).
Domain entities whose jargon ex-pressions raised clarification requests in the cor-pus were listed and those that had more than themean number of clarification requests were clas-sified as difficult and others as easy enti-ties (for example, power adaptor is easy - all126users understood this expression, broadband filteris difficult).
Clarification requests are pro-duced using the following model.P (Au,t = cr(Ri, Ti)|C(Ri), Ti, DKu,t(Ri, Ti))where (Ri, Ti) ?
RECs,tOne should note that the actual literal expres-sion is not used in the transaction.
Only the entitythat it is referring to (Ri) and its type (Ti) are used.However, the above model simulates the processof interpreting and resolving the expression andidentifying the domain entity of interest in the in-struction.
The user identification of the entity issignified when there is no clarification request pro-duced (i.e.
Au,t = none).
When no clarificationrequest is produced, the environment action EAu,tis generated using the following model.P (EAu,t|As,t) if Au,t!
= cr(Ri, Ti)Finally, the user action is an instruction re-sponse which is determined by the system ac-tion As,t.
Instruction responses can be ei-ther provide info, acknowledgement or otherbased on the system?s instruction.P (Au,t = ir|EAu,t, As,t)All the above models were trained on our cor-pus data using maximum likelihood estimation andsmoothed using a variant of Witten-Bell discount-ing.
The corpus contained dialogues betweena non-adaptive dialogue system and real users.According to the data, clarification requests aremuch more likely when jargon expressions areused to refer to the referents that belong to thedifficult class and which the user doesn?tknow about.
When the system uses expressionsthat the user knows, the user generally responds tothe instruction given by the system.4.2 User Domain knowledgeThe user domain knowledge is initially set to oneof several models at the start of every conver-sation.
The models range from novices to ex-perts which were identified from the corpus usingk-means clustering.
A novice user knows only?power adaptor?, an expert knows all the jargonexpressions and intermediate users know some.We assume that users can interpret the descriptiveexpressions and resolve their references.
There-fore, they are not explicitly represented.
We onlycode the user?s knowledge of jargon expressionsusing boolean variables representing whether theuser knows the expression or not.4.3 CorpusWe trained the action selection model on a smallcorpus of 12 non-adaptive dialogues between realusers and a dialogue system.
There were sixdialogues in which users interacted with a sys-tem using just jargon expressions and six with asystem using descriptive expressions.
For morediscussions on our user simulation models andthe corpus, please refer to (Janarthanam andLemon, 2009b; Janarthanam and Lemon, 2009a;Janarthanam and Lemon, 2010).5 TrainingThe REG module was trained (operated in learn-ing mode) using the above simulations to learnREG policies that select referring expressionsbased on the user expertise in the domain.
Inthis section, we discuss how to code the learningagent?s goals as reward.
We then discuss how thereward function is used to train the learning agent.5.1 Reward functionWe designed a reward function for the goal ofadapting to each user?s domain knowledge.
Wepresent the Adaptation Accuracy score (AA) thatcalculates how accurately the agent chose the ap-propriate expressions for each referent r, with re-spect to the user?s knowledge.
So, when the userknows the jargon expression for r, the appropri-ate expression to use is jargon, and if s/he doesn?tknow the jargon, a descriptive expression is appro-priate.
Although the user?s domain knowledge isdynamically changing due to learning, we base ap-propriateness on the initial state, because our ob-jective is to adapt to the initial state of the userDKu,initial.
However, in reality, designers mightwant their system to account for user?s chang-ing knowledge as well.
We calculate accuracyper referent RAr and then calculate the overallmean adaptation accuracy (AA) over all referentsas shown below.RAr = #(appropriate expressions(r))#(instances(r))AdaptationAccuracyAA = 1#(r)?rRAr5.2 LearningThe REG module was trained in learning mode us-ing the above reward function using the SHAR-SHA reinforcement learning algorithm (with lin-ear function approximation) (Shapiro and Langley,2002).
This is a hierarchical variant of SARSA,127which is an on-policy learning algorithm that up-dates the current behaviour policy (see (Suttonand Barto, 1998)).
The training produced approx.5000 dialogues.
The user simulation was cali-brated to produce three types of users: Novice,Intermediate and Expert, randomly but with equalprobability.Initially, the REG policy chooses randomly be-tween the referring expression types for each do-main entity in the system utterance, irrespectiveof the user model state.
Once the referring ex-pressions are chosen, the system presents the usersimulation with both the dialogue act and refer-ring expression choices.
The choice of referringexpression affects the user?s dialogue behaviour.For instance, choosing a jargon expression couldevoke a clarification request from the user, basedon which, the dialogue manager updates the inter-nal user model (UMs,t) with the new informationthat the user is ignorant of the particular expres-sion.
It should be noted that using a jargon expres-sion is an information seeking move which enablesthe REG module to estimate the user?s knowledgelevel.
The same process is repeated for every dia-logue instruction.
At the end of the dialogue, thesystem is rewarded based on its choices of refer-ring expressions.
If the system chooses jargon ex-pressions for novice users or descriptive expres-sions for expert users, penalties are incurred and ifthe system chooses REs appropriately, the rewardis high.
On the one hand, those actions that fetchmore reward are reinforced, and on the other hand,the agent tries out new state-action combinationsto explore the possibility of greater rewards.
Overtime, it stops exploring new state-action combina-tions and exploits those actions that contribute tohigher reward.
The REG module learns to choosethe appropriate referring expressions based on theuser model in order to maximize the overall adap-tation accuracy.
Figure 2 shows how the agentlearns using the data-driven (Learned DS) duringtraining.
It can be seen in the figure 2 that towardsthe end the curve plateaus, signifying that learninghas converged.6 EvaluationIn this section, we present the details of the eval-uation process, the baseline policy, the metricsused, and the results.
In a recent study, we eval-uated the learned policy and several hand-codedbaselines with simulated users and found thatFigure 2: Learning curve - Trainingthe Learned-DS policy produced higher adapta-tion accuracy than other policies (Janarthanam andLemon, 2010).
An interesting issue for researchin this area is to what extent evaluation results ob-tained in simulated environments transfer to eval-uations with real users (Lemon et al, 2006).6.1 Baseline systemIn order to compare the performance of the learnedpolicy with a baseline, a simple rule-based policywas built.
This baseline was chosen because it per-formed better in simulation, compared to a vari-ety of other baselines (Janarthanam and Lemon,2010).
It uses jargon for all referents by defaultand provides clarifications when requested.
It ex-ploits the user model in subsequent references af-ter the user?s knowledge of the expression hasbeen set to either yes or no.
Therefore, althoughit is a simple policy, it adapts to a certain extent(?locally?).
We refer to this policy as the ?Jargon-adapt?
policy.
It should be noted that this policywas built in the absence of expert domain knowl-edge and/or an expert-layperson corpus.6.2 ProcessWe evaluated the two policies with real users.36 university students from different backgrounds(e.g.
Arts, Humanities, Medicine and Engineer-ing) participated in the evaluation.
17 users weregiven a system with Jargon-adapt policy and 19users interacted with a system with Learned-DSpolicy.
Each user was given a pre-task recognitiontest to record his/her initial domain knowledge.The experimenter read out a list of technical termsand the user was asked to point out to the domainentities laid out in front of them.
They were then128given one of the two systems - learned or base-line, to interact with.
Following the system in-structions, they then attempted to set up the broad-band connection.
When the dialogue had ended,the user was given a post-task test where the recog-nition test was repeated and their responses wererecorded.
The user?s broadband connection setupwas manually examined for task completion (i.e.the percentage of correct connections that they hadmade in their final set-up).
The user was given thetask completion results and was then given a usersatisfaction questionnaire to evaluate the featuresof the system based on the conversation.All users interacted with a wizarded system em-ploying one of the two REG policies (see figure3).
The user?s responses were intercepted by a hu-man interpreter (or ?wizard?)
and were annotatedas dialogue acts, to which the automated dialoguemanager responded with a system dialogue action(the dialogue policy was fixed).
The wizards werenot aware of the policy used by the system.
Therespective policies chose only the referring expres-sions to generate the system utterance for the givendialogue action.
The system utterances were con-verted to speech by a speech synthesizer (Cere-proc) and were played to the user.Figure 3: Wizarded Dialogue System6.3 MetricsIn addition to the adaptation accuracy mentionedin section 5.1, we also measure other parame-ters from the conversation in order to show howlearned adaptive policies compare with other poli-cies on other dimensions.
We also measure thelearning effect on the users as (normalised) learn-ing gain (LG) produced by using unknown jargonexpressions.
This is calculated using the pre- andpost-test scores for the user domain knowledge(DKu) as follows.Metrics Jargon-adapt Learned-DSAA 63.91 84.72 **LG 0.59 0.61DT 7.86 6.98 *TC 84.7 99.8 *** Statistical significance (p < 0.05).
** Statistical significance (p < 0.001).Table 2: Evaluation with real usersLearning Gain LG = Post?Pre1?PreDialogue time (DT) is the actual time taken forthe user to complete the task.
We measured taskcompletion (TC) by examining the user?s broad-band setup after the task was completed (i.e.
thepercentage of correct connections that they hadmade in their final set-up).6.4 ResultsWe compare the performance of the two strategieson real users using objective parameters and sub-jective feedback scores.
Tests for statistical sig-nificance were done using Mann-Whitney test for2 independent samples (due to non-parametric na-ture of the data).Table 2 presents the mean accuracy of adap-tation (AA), learning gain (LG), dialogue time(DT), and task completion (TC), produced by thetwo strategies.
The Learned-DS strategy pro-duced more accurate adaptation than the Jargon-adapt strategy (p<0.001, U=9.0, r=-0.81).
Higheraccuracy of adaptation (AA) of the Learned-DSstrategy translates to less dialogue time (U=73.0,p<0.05, r=-0.46) and higher task completion(U=47.5, p<0.001, r=-0.72) than the Jargon-adaptpolicy.
However, there was no significant differ-ence in learning gain (LG).Table 3 presents how the users subjectivelyscored on a agreement scale of 1 to 4 (with 1meaning ?strongly disagree?
), different features ofthe system based on their conversations with thetwo different strategies.
Users?
feedback on dif-ferent features of the systems were not very differ-ent from each other.
However, users did feel thatit was easier to identify domain objects with theLearned-DS strategy than the Jargon-adapt strat-egy (U=104.0, p<0.05, r=-0.34).
To our knowl-edge, this is the first study to show a significantimprovement in real user ratings for a learned pol-icy in spoken dialogue systems (normally, objec-tive metrics show an improvement, but not subjec-129Feedback questions Jargon-adapt Learned-DSQ1.
Quality of voice 3.11 3.36Q2.
Had to ask too many questions 2.23 1.89Q3.
System adapted very well 3.41 3.58Q4.
Easy to identify objects 2.94 3.37 *Q5.
Right amount of dialogue time 3.23 3.26Q6.
Learned useful terms 2.94 3.05Q7.
Conversation was easy 3.17 3.42Q8.
Future use 3.22 3.47* Statistical significance (p < 0.05).Table 3: Real user feedbacktive scores (Lemon et al, 2006)).6.5 AnalysisThe results show that the Learned-DS strategy issignificantly better than the hand-coded Jargon-Adapt policy in terms of adaptation accuracy, di-alogue time, and task completion rate.
The ini-tial knowledge of the users (mean pre-task recog-nition score) of the two groups were not signifi-cantly different from each other (Jargon-adapt =7.33, Learned-DS = 7.45).
Hence there is no biason the user?s pre-task score towards any strategy.While the Learned-DS system adapts well to itsusers globally, the Jargon-adapt system adaptedonly locally.
This led to higher task completionrate and lower dialogue time.The Learned-DS strategy enabled the system toadapt using the dependencies that it learned dur-ing the training phase.
For instance, when the userasked for clarification on some referring expres-sions (e.g.
?ethernet cable?
), it used descriptiveexpressions for domain objects like ethernet lightand ethernet socket.
Such adaptation across ref-erents enabled the Learned-DS strategy to scorebetter than the Jargon-adapt strategy.
Since theagent starts the conversation with no knowledgeabout the user, it learned to use information seek-ing moves (use jargon) at appropriate moments,although they may be inappropriate.
But since itwas trained to maximize the adaptation accuracy,the agent also learned to restrict such moves andstart predicting the user?s domain knowledge assoon as possible.
By learning to trade-off betweeninformation-seeking and adaptation, the Learned-DS policy produced a higher adaptation with realusers with different domain knowledge levels.The users however did not generally rate thetwo policies differently.
However, they did rateit (significantly) easier to identify objects whenusing the learned policy.
For the other ratings,users seemed to be not able to recognize the nu-ances in the way the system adapted to them.
Theycould have been satisfied with the fact that the sys-tem adapted better (Q3).
This adaptation and thefact that the system offered help when the userswere confused in interpreting the technical terms,could have led the users to score the system well interms of future use (Q8), dialogue time (Q5), andease of conversation (Q7), but in common with ex-periments in dialogue management (Lemon et al,2006) it seems that users find it difficult to evaluatethese improvements subjectively.
The users weregiven only one of the two strategies and thereforewere not in a position to compare the two strate-gies and judge which one is better.
Results in table3 lead us to conclude that perhaps users need tocompare two or more strategies in order to judgethe strategies better.7 ConclusionWe presented new results from an evaluation withreal users.
In this study, we have shown that user-adaptive REG policies can be learned using an RLframework and data-driven user simulations.
Itlearned to trade off between adaptive moves andinformation seeking moves automatically to max-imize the overall adaptation accuracy.
The learnedpolicy started the conversation with informationseeking moves, learned a little about the user, andstarted adapting dynamically as the conversationprogressed.
We also showed that the learned pol-icy performs better than a reasonable hand-codedpolicy with real users in terms of accuracy of adap-tation, dialogue time, task completion, and a sub-jective evaluation.
Finally, this paper providesfurther evidence that evaluation results obtained130in simulated environments can transfer reliably toevaluations with real users (Lemon et al, 2006).Whether the learned policy would perform bet-ter than a hand-coded policy which was painstak-ingly crafted by a domain expert (or learned us-ing supervised methods from an expert-laypersoncorpus) is an interesting question that needs fur-ther exploration.
Also, it would also be interestingto make the learned policy account for the user?slearning behaviour and adapt accordingly.
We alsobelieve that this framework can be extended to in-clude other decisions in NLG besides REG (Deth-lefs and Cuayahuitl, 2010).AcknowledgementsThe research leading to these results has receivedfunding from the European Community?s SeventhFramework Programme (FP7/2007-2013) undergrant agreement no.
216594 (CLASSiC projectwww.classic-project.org) and from theEPSRC, project no.
EP/G069840/1.ReferencesH.
Ai and D. Litman.
2007.
Knowledge consistentuser simulations for dialog systems.
In Proceedingsof Interspeech 2007, Antwerp, Belgium.T.
Akiba and H. Tanaka.
1994.
A Bayesian approachfor User Modelling in Dialogue Systems.
In Pro-ceedings of the 15th conference on ComputationalLinguistics - Volume 2, Kyoto.A.
Bell.
1984.
Language style as audience design.Language in Society, 13(2):145?204.A.
Cawsey.
1993.
User Modelling in Interactive Ex-planations.
User Modeling and User-Adapted Inter-action, 3(3):221?247.H.
H. Clark and G. L. Murphy.
1982.
Audience de-sign in meaning and reference.
In J. F. LeNy andW.
Kintsch, editors, Language and comprehension.Amsterdam: North-Holland.N.
Dethlefs and H. Cuayahuitl.
2010.
Hierarchical Re-inforcement Learning for Adaptive Text Generation.In Proc.
INLG 2010.K.
Georgila, J. Henderson, and O.
Lemon.
2005.Learning User Simulations for Information StateUpdate Dialogue Systems.
In Proc of Eu-rospeech/Interspeech.F.
Hernandez, E. Gaudioso, and J. G. Boticario.
2003.A Multiagent Approach to Obtain Open and FlexibleUser Models in Adaptive Learning Communities.
InUser Modeling 2003, volume 2702/2003 of LNCS.Springer, Berlin / Heidelberg.E.
A. Issacs and H. H. Clark.
1987.
References inconversations between experts and novices.
Journalof Experimental Psychology: General, 116:26?37.S.
Janarthanam and O.
Lemon.
2009a.
A Two-tierUser Simulation Model for Reinforcement Learningof Adaptive Referring Expression Generation Poli-cies.
In Proc.
SigDial?09.S.
Janarthanam and O.
Lemon.
2009b.
A Wizard-of-Oz environment to study Referring Expression Gen-eration in a Situated Spoken Dialogue Task.
In Proc.ENLG?09.S.
Janarthanam and O.
Lemon.
2009c.
Learning Lexi-cal Alignment Policies for Generating Referring Ex-pressions for Spoken Dialogue Systems.
In Proc.ENLG?09.S.
Janarthanam and O.
Lemon.
2010.
Learning toAdapt to Unknown Users: Referring ExpressionGeneration in Spoken Dialogue Systems.
In Proc.ACL?10.O.
Lemon, Georgila.
K., and J. Henderson.
2006.Evaluating Effectiveness and Portability of Rein-forcement Learned Dialogue Strategies with realusers: the TALK TownInfo Evaluation.
InIEEE/ACL Spoken Language Technology.O.
Lemon.
2010.
Learning what to say and how to sayit: joint optimization of spoken dialogue manage-ment and Natural Language Generation.
ComputerSpeech and Language.
(to appear).E.
Levin, R. Pieraccini, and W. Eckert.
1997.
Learn-ing Dialogue Strategies within the Markov DecisionProcess Framework.
In Proc.
of ASRU97.V.
Rieser and O.
Lemon.
2009.
Natural LanguageGeneration as Planning Under Uncertainty for Spo-ken Dialogue Systems.
In Proc.
EACL?09.V.
Rieser and O.
Lemon.
2010.
Optimising informa-tion presentation for spoken dialogue systems.
InProc.
ACL.
(to appear).J.
Schatzmann, K. Weilhammer, M. N. Stuttle, and S. J.Young.
2006.
A Survey of Statistical User Sim-ulation Techniques for Reinforcement Learning ofDialogue Management Strategies.
Knowledge Engi-neering Review, pages 97?126.J.
Schatzmann, B. Thomson, K. Weilhammer, H. Ye,and S. J.
Young.
2007.
Agenda-based User Simula-tion for Bootstrapping a POMDP Dialogue System.In Proc of HLT/NAACL 2007.D.
Shapiro and P. Langley.
2002.
Separating skillsfrom preference: Using learning to program by re-ward.
In Proc.
ICML-02.R.
Sutton and A. Barto.
1998.
Reinforcement Learn-ing.
MIT Press.131
