Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 423?431,Suntec, Singapore, 2-7 August 2009. c?2009 ACL and AFNLPWho, What, When, Where, Why?Comparing Multiple Approaches to the Cross-Lingual 5W TaskKristen Parton*, Kathleen R. McKeown*, Bob Coyne*, Mona T. Diab*,Ralph Grishman?, Dilek Hakkani-T?r?, Mary Harper?, Heng Ji?, Wei Yun Ma*,Adam Meyers?, Sara Stolbach*, Ang Sun?, Gokhan Tur?, Wei Xu?
and Sibel Yaman?
*Columbia UniversityNew York, NY, USA{kristen, kathy,coyne, mdiab, ma,sara}@cs.columbia.edu?New York UniversityNew York, NY, USA{grishman, meyers,asun, xuwei}@cs.nyu.edu?International ComputerScience InstituteBerkeley, CA, USA{dilek, sibel}@icsi.berkeley.edu?Human Lang.
Tech.
Ctr.
ofExcellence, Johns Hopkinsand U. of Maryland,College Parkmharper@umd.edu?City University ofNew YorkNew York, NY, USAhengji@cs.qc.cuny.edu?SRI InternationalPalo Alto, CA, USAgokhan@speech.sri.comAbstractCross-lingual tasks are especially difficultdue to the compounding effect of errors inlanguage processing and errors in machinetranslation (MT).
In this paper, we present anerror analysis of a new cross-lingual task: the5W task, a sentence-level understanding taskwhich seeks to return the English 5W's (Who,What, When, Where and Why) correspondingto a Chinese sentence.
We analyze systemsthat we developed, identifying specific prob-lems in language processing and MT thatcause errors.
The best cross-lingual 5W sys-tem was still 19% worse than the best mono-lingual 5W system, which shows that MTsignificantly degrades sentence-level under-standing.
Neither source-language nor target-language analysis was able to circumventproblems in MT, although each approach hadadvantages relative to the other.
A detailederror analysis across multiple systems sug-gests directions for future research on theproblem.1 IntroductionIn our increasingly global world, it is ever morelikely for a mono-lingual speaker to require in-formation that is only available in a foreign lan-guage document.
Cross-lingual applications ad-dress this need by presenting information in thespeaker?s language even when it originally ap-peared in some other language, using machinetranslation (MT) in the process.
In this paper, wepresent an evaluation and error analysis of across-lingual application that we developed for agovernment-sponsored evaluation, the 5W task.The 5W task seeks to summarize the informa-tion in a natural language sentence by distilling itinto the answers to the 5W questions: Who,What, When, Where and Why.
To solve thisproblem, a number of different problems in NLPmust be addressed: predicate identification, ar-gument extraction, attachment disambiguation,location and time expression recognition, and(partial) semantic role labeling.
In this paper, weaddress the cross-lingual 5W task: given asource-language sentence, return the 5W?s trans-lated (comprehensibly) into the target language.Success in this task requires a synergy of suc-cessful MT and answer selection.The questions we address in this paper are:?
How much does machine translation (MT)degrade the performance of cross-lingual5W systems, as compared to monolingualperformance??
Is it better to do source-language analysisand then translate, or do target-languageanalysis on MT??
Which specific problems in languageprocessing and/or MT cause errors in 5Wanswers?In this evaluation, we compare several differ-ent approaches to the cross-lingual 5W task, twothat work on the target language (English) andone that works in the source language (Chinese).423A central question for many cross-lingual appli-cations is whether to process in the source lan-guage and then translate the result, or translatedocuments first and then process the translation.Depending on how errorful the translation is,results may be more accurate if models are de-veloped for the source language.
However, ifthere are more resources in the target language,then the translate-then-process approach may bemore appropriate.
We present a detailed analysis,both quantitative and qualitative, of how the ap-proaches differ in performance.We also compare system performance on hu-man translation (which we term reference trans-lations) and MT of the same data in order to de-termine how much MT degrades system per-formance.
Finally, we do an in-depth analysis ofthe errors in our 5W approaches, both on theNLP side and the MT side.
Our results provideexplanations for why different approaches suc-ceed, along with indications of where future ef-fort should be spent.2 Prior WorkThe cross-lingual 5W task is closely related tocross-lingual information retrieval and cross-lingual question answering (Wang and Oard2006; Mitamura et al 2008).
In these tasks, asystem is presented a query or question in thetarget language and asked to return documents oranswers from a corpus in the source language.Although MT may be used in solving this task, itis only used by the algorithms ?
the final evalua-tion is done in the source language.
However, inmany real-life situations, such as global business,international tourism, or intelligence work, usersmay not be able to read the source language.
Inthese cases, users must rely on MT to understandthe system response.
(Parton et al 2008) exam-ine the case of ?translingual?
information re-trieval, where evaluation is done on translatedresults in the target language.
In cross-lingualinformation extraction (Sudo et al 2004) theevaluation is also done on MT, but the goal is tolearn knowledge from a large corpus, rather thananalyzing individual sentences.The 5W task is also closely related to Seman-tic Role Labeling (SRL), which aims to effi-ciently and effectively derive semantic informa-tion from text.
SRL identifies predicates andtheir arguments in a sentence, and assigns rolesto each argument.
For example, in the sentence?I baked a cake yesterday.
?, the predicate?baked?
has three arguments.
?I?
is the subject ofthe predicate, ?a cake?
is the object and ?yester-day?
is a temporal argument.Since the release of large data resources anno-tated with relevant levels of semantic informa-tion, such as the FrameNet (Baker et al, 1998)and PropBank corpora (Kingsbury and Palmer,2003), efficient approaches to SRL have beendeveloped (Carreras and Marquez, 2005).
Mostapproaches to the problem of SRL follow theGildea and Jurafsky (2002) model.
First, for agiven predicate, the SRL system identifies itsarguments' boundaries.
Second, the Argumenttypes are classified depending on an adoptedlexical resource such as PropBank or FrameNet.Both steps are based on supervised learning overlabeled gold standard data.
A final step uses heu-ristics to resolve inconsistencies when applyingboth steps simultaneously to the test data.Since many of the SRL resources are English,most of the SRL systems to date have been forEnglish.
There has been work in other languagessuch as German and Chinese (Erk 2006; Sun2004; Xue and Palmer 2005).
The systems forthe other languages follow the successful modelsdevised for English, e.g.
(Gildea and Palmer,2002; Chen and Rambow, 2003; Moschitti, 2004;Xue and Palmer, 2004; Haghighi et al, 2005).3 The Chinese-English 5W Task3.1 5W Task DescriptionWe participated in the 5W task as part of theDARPA GALE (Global Autonomous LanguageExploitation) project.
The goal is to identify the5W?s (Who, What, When, Where and Why) for acomplete sentence.
The motivation for the 5Wtask is that, as their origin in journalism suggests,the 5W?s cover the key information nuggets in asentence.
If a system can isolate these pieces ofinformation successfully, then it can produce apr?cis of the basic meaning of the sentence.
Notethat this task differs from QA tasks, where?Who?
and ?What?
usually refer to definitiontype questions.
In this task, the 5W?s refer to se-mantic roles within a sentence, as defined in Ta-ble 1.In order to get al 5W?s for a sentence correct,a system must identify a top-level predicate, ex-tract the correct arguments, and resolve attach-ment ambiguity.
In the case of multiple top-levelpredicates, any of the top-level predicates may bechosen.
In the case of passive verbs, the Who isthe agent (often expressed as a ?by clause?, ornot stated), and the What should include the syn-tactic subject.424Answers are judged Correct1 if they identify acorrect null argument or correctly extract an ar-gument that is present in the sentence.
Answersare not penalized for including extra text, such asprepositional phrases or subordinate clauses,unless the extra text includes text from anotheranswer or text from another top-level predicate.In sentence 2a in Table 2, returning ?bought andcooked?
for the What would be Incorrect.
Simi-larly, returning ?bought the fish at the market?for the What would also be Incorrect, since itcontains the Where.
Answers may also be judgedPartial, meaning that only part of the answer wasreturned.
For example, if the What contains thepredicate but not the logical object, it is Partial.Since each sentence may have multiple correctsets of 5W?s, it is not straightforward to producea gold-standard corpus for automatic evaluation.One would have to specify answers for each pos-sible top-level predicate, as well as which partsof the sentence are optional and which are notallowed.
This also makes creating training datafor system development problematic.
For exam-ple, in Table 2, the sentence in 2a and 2b is thesame, but there are two possible sets of correctanswers.
Since we could not rely on a gold-standard corpus, we used manual annotation tojudge our 5W system, described in section 5.3.2 The Cross-Lingual 5W TaskIn the cross-lingual 5W task, a system is given asentence in the source language and asked toproduce the 5W?s in the target language.
In thistask, both machine translation (MT) and 5W ex-traction must succeed in order to produce correctanswers.
One motivation behind the cross-lingual5W task is MT evaluation.
Unlike word- orphrase-overlap measures such as BLEU, the 5Wevaluation takes into account ?concept?
or ?nug-get?
translation.
Of course, only the top-levelpredicate and arguments are evaluated, so it isnot a complete evaluation.
But it seeks to get atthe understandability of the MT output, ratherthan just n-gram overlap.Translation exacerbates the problem of auto-matically evaluating 5W systems.
Since transla-tion introduces paraphrase, rewording and sen-tence restructuring, the 5W?s may change fromone translation of a sentence to another transla-tion of the same sentence.
In some cases, rolesmay swap.
For example, in Table 2, sentences 1aand 1b could be valid translations of the same1The specific guidelines for determining correctnesswere formulated by BAE.Chinese sentence.
They contain the same infor-mation, but the 5W answers are different.
Also,translations may produce answers that are textu-ally similar to correct answers, but actually differin meaning.
These differences complicate proc-essing in the source followed by translation.Example: On Tuesday, President Obama met withFrench President Sarkozy in Paris to discuss theeconomic crisis.W Definition ExampleanswerWHO Logical subject of thetop-level predicate inWHAT, or null.PresidentObamaWHAT One of the top-levelpredicates in the sen-tence, and the predi-cate?s logical object.met withFrench Presi-dent SarkozyWHEN ARGM-TMP of thetop-level predicate inWHAT, or null.On TuesdayWHERE ARGM-LOC of thetop-level predicate inWHAT, or null.in ParisWHY ARGM-CAU of thetop-level predicate inWHAT, or null.to discuss theeconomic crisisTable 1.
Definition of the 5W task, and 5W answersfrom the example sentence above.4 5W SystemWe developed a 5W combination system thatwas based on five other 5W systems.
We se-lected four of these different systems for evalua-tion: the final combined system (which was oursubmission for the official evaluation), two sys-tems that did analysis in the target-language(English), and one system that did analysis in thesource language (Chinese).
In this section, wedescribe the individual systems that we evalu-ated, the combination strategy, the parsers thatwe tuned for the task, and the MT systems.Sentence WHO WHAT1a Mary bought a cakefrom Peter.Mary bought acake1b Peter sold Mary acake.Peter sold Mary2a I bought the fish atthe market yesterdayand cooked it today.I bought thefish[WHEN:yesterday]2b I bought the fish atthe market yesterdayand cooked it today.I cooked it[WHEN:today]Table 2.
Example 5W answers.4254.1 Latent Annotation ParserFor this work, we have re-implemented and en-hanced the Berkeley parser (Petrov and Klein2007) in several ways: (1) developed a newmethod to handle rare words in English and Chi-nese; (2) developed a new model of unknownChinese words based on characters in the word;(3) increased robustness by adding adaptivemodification of pruning thresholds and smooth-ing of word emission probabilities.
While theenhancements to the parser are important for ro-bustness and accuracy, it is even more importantto train grammars matched to the conditions ofuse.
For example, parsing a Chinese sentencecontaining full-width punctuation with a parsertrained on half-width punctuation reduces accu-racy by over 9% absolute F. In English, parsingaccuracy is seriously compromised by training agrammar with punctuation and case to processsentences without them.We developed grammars for English and Chi-nese trained specifically for each genre by sub-sampling from available treebanks (for English,WSJ, BN, Brown, Fisher, and Switchboard; forChinese, CTB5) and transforming them for aparticular genre (e.g., for informal speech, wereplaced symbolic expressions with verbal formsand remove punctuation and case) and by utiliz-ing a large amount of genre-matched self-labeledtraining parses.
Given these genre-specificparses, we extracted chunks and POS tags byscript.
We also trained grammars with a subset offunction tags annotated in the treebank that indi-cate case role information (e.g., SBJ, OBJ, LOC,MNR) in order to produce function tags.4.2 Individual 5W SystemsThe English systems were developed for themonolingual 5W task and not modified to handleMT.
They used hand-crafted rules on the outputof the latent annotation parser to extract the 5Ws.English-function used the function tags fromthe parser to map parser constituents to the 5Ws.First the Who, When, Where and Why were ex-tracted, and then the remaining pieces of the sen-tence were returned as the What.
The goal was tomake sure to return a complete What answer andavoid missing the object.English-LF, on the other hand, used a systemdeveloped over a period of eight years (Meyerset al 2001) to map from the parser?s syntacticconstituents into logical grammatical relations(GLARF), and then extracted the 5Ws from thelogical form.
As a back-up, it also extractedGLARF relations from another English-treebanktrained parser, the Charniak parser (Charniak2001).
After the parses were both converted tothe 5Ws, they were then merged, favoring thesystem that: recognized the passive, filled more5W slots or produced shorter 5W slots (provid-ing that the WHAT slot consisted of more thanjust the verb).
A third back-up method extracted5Ws from part-of-speech tag patterns.
UnlikeEnglish-function, English-LF explicitly tried toextract the shortest What possible, provided therewas a verb and a possible object, in order toavoid multiple predicates or other 5W answers.Chinese-align uses the latent annotationparser (trained for Chinese) to parse the Chinesesentences.
A dependency tree converter (Johans-son and Nuges 2007) was applied to the constitu-ent-based parse trees to obtain the dependencyrelations and determine top-level predicates.
Aset of hand-crafted dependency rules based onobservation of Chinese OntoNotes were used tomap from the Chinese function tags into Chinese5Ws.
Finally, Chinese-align used the alignmentsof three separate MT systems to translate the5Ws: a phrase-based system, a hierarchicalphrase-based system, and a syntax augmentedhierarchical phrase-based system.
Chinese-alignfaced a number of problems in using the align-ments, including the fact that the best MT did notalways have the best alignment.
Since the predi-cate is essential, it tried to detect when verbswere deleted in MT, and back-off to a differentMT system.
It also used strategies for findingand correcting noisy alignments, and for filteringWhen/Where answers from Who and What.4.3 Hybrid SystemA merging algorithm was learned based on a de-velopment test set.
The algorithm selected all5W?s from a single system, rather than trying tomerge W?s from different systems, since thepredicates may vary across systems.
For eachdocument genre (described in section 5.4), weranked the systems by performance on the devel-opment data.
We also experimented with a vari-ety of features (for instance, does ?What?
includea verb).
The best-performing features were usedin combination with the ranked list of prioritysystems to create a rule-based merger.4.4 MT SystemsThe MT Combination system used by both of theEnglish 5W systems combined up to nine sepa-rate MT systems.
System weights for combina-tion were optimized together with the language426model score and word penalty for a combinationof BLEU and TER (2*(1-BLEU) + TER).
Res-coring was applied after system combination us-ing large language models and lexical triggermodels.
Of the nine systems, six were phrased-based systems (one of these used chunk-levelreordering of the Chinese, one used word sensedisambiguation, and one used unsupervised Chi-nese word segmentation), two were hierarchicalphrase-based systems, one was a string-to-dependency system, one was syntax-augmented,and one was a combination of two other systems.Bleu scores on the government supplied test setin December 2008 were 35.2 for formal text,29.2 for informal text, 33.2 for formal speech,and 27.6 for informal speech.
More details maybe found in (Matusov et al 2009).5 Methods5.1 5W SystemsFor the purposes of this evaluation2, we com-pared the output of 4 systems: English-Function,English-LF, Chinese-align, and the combinedsystem.
Each English system was also run onreference translations of the Chinese sentence.So for each sentence in the evaluation corpus,there were 6 systems that each provided 5Ws.5.2 5W Answer AnnotationFor each 5W output, annotators were presentedwith the reference translation, the MT version,and the 5W answers.
The 5W system nameswere hidden from the annotators.
Annotators hadto select ?Correct?, ?Partial?
or ?Incorrect?
foreach W. For answers that were Partial or Incor-rect, annotators had to further specify the sourceof the error based on several categories (de-scribed in section 6).
All three annotators werenative English speakers who were not systemdevelopers for any of the 5W systems that werebeing evaluated (to avoid biased grading, or as-signing more blame to the MT system).
None ofthe annotators knew Chinese, so all of the judg-ments were based on the reference translations.After one round of annotation, we measuredinter-annotator agreement on the Correct, Partial,or Incorrect judgment only.
The kappa value was0.42, which was lower than we expected.
An-other surprise was that the agreement was lower2Note that an official evaluation was also performed byDARPA and BAE.
This evaluation provides more fine-grained detail on error types and gives results for the differ-ent approaches.for When, Where and Why (?=0.31) than forWho or What (?=0.48).
We found that, in caseswhere a system would get both Who and Whatwrong, it was often ambiguous how the remain-ing W?s should be graded.
Consider the sentence:?He went to the store yesterday and cooked lasa-gna today.?
A system might return erroneousWho and What answers, and return Where as ?tothe store?
and When as ?today.?
Since Whereand When apply to different predicates, theycannot both be correct.
In order to be consistent,if a system returned erroneous Who and Whatanswers, we decided to mark the When, Whereand Why answers Incorrect by default.
We addedclarifications to the guidelines and discussed ar-eas of confusion, and then the annotators re-viewed and updated their judgments.After this round of annotating, ?=0.83 on theCorrect, Partial, Incorrect judgments.
The re-maining disagreements were genuinely ambigu-ous cases, where a sentence could be interpretedmultiple ways, or the MT could be understood invarious ways.
There was higher agreement on5W?s answers from the reference text comparedto MT text, since MT is inherently harder tojudge and some annotators were more flexiblethan others in grading garbled MT.5.3 5W Error AnnotationIn addition to judging the system answers by thetask guidelines, annotators were asked to providereason(s) an answer was wrong by selecting froma list of predefined errors.
Annotators were askedto use their best judgment to ?assign blame?
tothe 5W system, the MT, or both.
There were sixtypes of system errors and four types of MT er-rors, and the annotator could select any numberof errors.
(Errors are described further in section6.)
For instance, if the translation was correct,but the 5W system still failed, the blame wouldbe assigned to the system.
If the 5W systempicked an incorrectly translated argument (e.g.,?baked a moon?
instead of ?baked a cake?
), thenthe error would be assigned to the MT system.Annotators could also assign blame to both sys-tems, to indicate that they both made mistakes.Since this annotation task was a 10-way selec-tion, with multiple selections possible, there weresome disagreements.
However, if categorizedbroadly into 5W System errors only, MT errorsonly, and both 5W System and MT errors, thenthe annotators had a substantial level of agree-ment (?=0.75 for error type, on sentences whereboth annotators indicated an error).4275.4 5 W CorpusThe full evaluation corpus is 350 documents,roughly evenly divided between four genres:formal text (newswire), informal text (blogs andnewsgroups), formal speech (broadcast news)and informal speech (broadcast conversation).For this analysis, we randomly sampled docu-ments to judge from each of the genres.
Therewere 50 documents (249 sentences) that werejudged by a single annotator.
A subset of that set,with 22 documents and 103 sentences, wasjudged by two annotators.
In comparing the re-sults from one annotator to the results from bothannotators, we found substantial agreement.Therefore, we present results from the single an-notator so we can do a more in-depth analysis.Since each sentence had 5W?s, and there were 6systems that were compared, there were 7,500single-annotator judgments over 249 sentences.6 ResultsFigure 1 shows the cross-lingual performance(on MT) of all the systems for each 5W.
The bestmonolingual performance (on human transla-tions) is shown as a dashed line (% Correctonly).
If a system returned Incorrect answers forWho and What, then the other answers weremarked Incorrect (as explained in section 5.2).For the last 3W?s, the majority of errors were dueto this (details in Figure 1), so our error analysisfocuses on the Who and What questions.6.1 Monolingual 5W PerformanceTo establish a monolingual baseline, the Eng-lish 5W system was run on reference (human)translations of the Chinese text.
For each partialor incorrect answer, annotators could select oneor more of these reasons:?
Wrong predicate or multiple predicates.?
Answer contained another 5W answer.?
Passive handled wrong (WHO/WHAT).?
Answer missed.?
Argument attached to wrong predicate.Figure 1 shows the performance of the bestmonolingual system for each 5W as a dashedline.
The What question was the hardest, since itrequires two pieces of information (the predicateand object).
The When, Where and Why ques-tions were easier, since they were null most ofthe time.
(In English OntoNotes 2.0, 38% of sen-tences have a When, 15% of sentences have aWhere, and only 2.6% of sentences have a Why.
)The most common monolingual system error onthese three questions was a missed answer, ac-counting for all of the Where errors, all but oneWhy error and 71% of the When errors.
The re-maining When errors usually occurred when thesystem assumed the wrong sense for adverbs(such as ?then?
or ?just?
).Missing Other5WWrong/MultiplePredicatesWrongREF-func 37 29 22 7REF-LF 54 20 17 13MT-func 18 18 18 8MT-LF 26 19 10 11Chinese 23 17 14 8Hybrid 13 17 15 12Table 3.
Percentages of Who/What errors attributed toeach system error type.The top half of Table 3 shows the reasons at-tributed to the Who/What errors for the referencecorpus.
Since English-LF preferred shorter an-swers, it frequently missed answers or parts ofFigure 1.
System performance on each 5W.
?Partial?
indicates that part of the answer was missing.
Dashed linesshow the performance of the best monolingual system (% Correct on human translations).
For the last 3W?s, thepercent of answers that were Incorrect ?by default?
were: 30%, 24%, 27% and 22%, respectively, and 8% for thebest monolingual system60 60 56 6636 40 38 4256 59 59 64 6370 66 73 68 75 71 78192019140102030405060708090100Eng-funcEng-LFChineseHybridEng-funcEng-LFChineseHybridEng-funcEng-LFChineseHybridEng-funcEng-LFChineseHybridEng-funcEng-LFChineseHybridWHO WHAT WHEN WHERE WHYPartia lCorrect9075 8183 90Bestmono-lingual428answers.
English-LF also had more Partial an-swers on the What question: 66% Correct and12% Partial, versus 75% Correct and 1% Partialfor English-function.
On the other hand, English-function was more likely to return answers thatcontained incorrect extra information, such asanother 5W or a second predicate.6.2 Effect of MT on 5W PerformanceThe cross-lingual 5W task requires that systemsreturn intelligible responses that are semanticallyequivalent to the source sentence (or, in the caseof this evaluation, equivalent to the reference).As can be seen in Figure 1, MT degrades theperformance of the 5W systems significantly, forall question types, and for all systems.
Averagedover all questions, the best monolingual systemdoes 19% better than the best cross-lingual sys-tem.
Surprisingly, even though English-functionoutperformed English-LF on the reference data,English-LF does consistently better on MT.
Thisis likely due to its use of multiple back-off meth-ods when the parser failed.6.3 Source-Language vs. Target-LanguageThe Chinese system did slightly worse than ei-ther English system overall, but in the formaltext genre, it outperformed both English systems.Although the accuracies for the Chinese andEnglish systems are similar, the answers vary alot.
Nearly half (48%) of the answers can be an-swered correctly by both the English system andthe Chinese system.
But 22% of the time, theEnglish system returned the correct answer whenthe Chinese system did not.
Conversely, 10% ofthe answers were returned correctly by the Chi-nese system and not the English systems.
Thehybrid system described in section 4.2 attemptsto exploit these complementary advantages.After running the hybrid system, 61% of theanswers were from English-LF, 25% from Eng-lish-function, 7% from Chinese-align, and theremaining 7% were from the other Chinesemethods (not evaluated here).
The hybrid didbetter than its parent systems on all 5Ws, and thenumbers above indicate that further improvementis possible with a better combination strategy.6.4 Cross-Lingual 5W Error AnalysisFor each Partial or Incorrect answer, annotatorswere asked to select system errors, translationerrors, or both.
(Further analysis is necessary todistinguish between ASR errors and MT errors.
)The translation errors considered were:?
Word/phrase deleted.?
Word/phrase mistranslated.?
Word order mixed up.?
MT unreadable.Table 4 shows the translation reasons attrib-uted to the Who/What errors.
For all systems, theerrors were almost evenly divided between sys-tem-only, MT-only and both, although the Chi-nese system had a higher percentage of system-only errors.
The hybrid system was able to over-come many system errors (for example, in Table2, only 13% of the errors are due to missing an-swers), but still suffered from MT errors.Table 4.
Percentages of Who/What errors by eachsystem attributed to each translation error type.Mistranslation was the biggest translationproblem for all the systems.
Consider the firstexample in Figure 3.
Both English systems cor-rectly extracted the Who and the When, but forMistrans-lationDeletion WordOrderUnreadableMT-func 34 18 24 18MT-LF 29 22 21 14Chinese 32 17 9 13Hybrid 35 19 27 18MT: After several rounds of reminded, I was a little bitRef: After several hints, it began to come back to me.??????,?????????
?MT: The Guizhou province, within a certain bank robber, under the watchful eyes of a weak woman, and, with aknife stabbed the woman.Ref: I saw that in a bank in Guizhou Province, robbers seized a vulnerable young woman in front of a group ofonlookers and stabbed the woman with a knife.?????????,?????????,???????,??,???????
?MT: Woke up after it was discovered that the property is not more than eleven people do not even said that thememory of the receipt of the country into the country.Ref: Well, after waking up, he found everything was completely changed.
Apart from having additional elevengrandchildren, even the motherland as he recalled has changed from a socialist country to a capitalist country.?????????????,?????????,????????????????????????
?Figure 3 Example sentences that presented problems for the 5W systems.429What they returned ?was a little bit.?
This is thecorrect predicate for the sentence, but it does notmatch the meaning of the reference.
The Chinese5W system was able to select a better translation,and instead returned ?remember a little bit.
?Garbled word order was chosen for 21-24% ofthe target-language system Who/What errors, butonly 9% of the source-language systemWho/What errors.
The source-language wordorder problems tended to be local, within-phraseerrors (e.g., ?the dispute over frozen funds?
wastranslated as ?the freezing of disputes?).
The tar-get-language system word order problems wereoften long-distance problems.
For example, thesecond sentence in Figure 3 has many phrases incommon with the reference translation, but theoverall sentence makes no sense.
The watchfuleyes actually belong to a ?group of onlookers?(deleted).
Ideally, the robber would have?stabbed the woman?
?with a knife,?
rather thanvice versa.
Long-distance phrase movement is acommon problem in Chinese-English MT, andmany MT systems try to handle it (e.g., Wang etal.
2007).
By doing analysis in the source lan-guage, the Chinese 5W system is often able toavoid this problem ?
for example, it successfullyreturned ?robbers?
?grabbed a weak woman?
forthe Who/What of this sentence.Although we expected that the Chinese systemwould have fewer problems with MT deletion,since it could choose from three different MTversions, MT deletion was a problem for all sys-tems.
In looking more closely at the deletions,we noticed that over half of deletions were verbsthat were completely missing from the translatedsentence.
Since MT systems are tuned for word-based overlap measures (such as BLEU), verbdeletion is penalized equally as, for example,determiner deletion.
Intuitively, a verb deletiondestroys the central meaning of a sentence, whilea determiner is rarely necessary for comprehen-sion.
Other kinds of deletions included nounphrases, pronouns, named entities, negations andlonger connecting phrases.Deletion also affected When and Where.
De-leting particles such as ?in?
and ?when?
that in-dicate a location or temporal argument causedthe English systems to miss the argument.
Wordorder problems in MT also caused attachmentambiguity in When and Where.The ?unreadable?
category was an option oflast resort for very difficult MT sentences.
Thethird sentence in Figure 3 is an example whereASR and MT errors compounded to create anunparseable sentence.7 ConclusionsIn our evaluation of various 5W systems, we dis-covered several characteristics of the task.
TheWhat answer was the hardest for all systems,since it is difficult to include enough informationto cover the top-level predicate and object, with-out getting penalized for including too much.The challenge in the When, Where and Whyquestions is due to sparsity ?
these responsesoccur in much fewer sentences than Who andWhat, so systems most often missed these an-swers.
Since this was a new task, this firstevaluation showed clear issues on the languageanalysis side that can be improved in the future.The best cross-lingual 5W system was still19% worse than the best monolingual 5W sys-tem, which shows that MT significantly degradessentence-level understanding.
A serious problemin MT for systems was deletion.
Chinese con-stituents that were never translated caused seri-ous problems, even when individual systems hadstrategies to recover.
When the verb was deleted,no top level predicate could be found and then all5Ws were wrong.One of our main research questions waswhether to extract or translate first.
We hypothe-sized that doing source-language analysis wouldbe more accurate, given the noise in ChineseMT, but the systems performed about the same.This is probably because the English tools (logi-cal form extraction and parser) were more ma-ture and accurate than the Chinese tools.Although neither source-language nor target-language analysis was able to circumvent prob-lems in MT, each approach had advantages rela-tive to the other, since they did well on differentsets of sentences.
For example, Chinese-alignhad fewer problems with word order, and mostof those were due to local word-order problems.Since the source-language and target-languagesystems made different kinds of mistakes, wewere able to build a hybrid system that used therelative advantages of each system to outperformall systems.
The different types of mistakes madeby each system suggest features that can be usedto improve the combination system in the future.AcknowledgmentsThis work was supported in part by the DefenseAdvanced Research Projects Agency (DARPA)under contract number HR0011-06-C-0023.
Anyopinions, findings and conclusions or recom-mendations expressed in this material are theauthors' and do not necessarily reflect those ofthe sponsors.430ReferencesCollin F. Baker, Charles J. Fillmore, and John B.Lowe.
1998.
The Berkeley FrameNet project.
InCOLING-ACL '98: Proceedings of the Conference,held at the University of Montr?al, pages 86?90.Xavier Carreras and Llu?s M?rquez.
2005.
Introduc-tion to the CoNLL-2005 shared task: Semantic rolelabeling.
In Proceedings of the Ninth Conferenceon Computational Natural Language Learning(CoNLL-2005), pages 152?164.Eugene Charniak.
2001.
Immediate-head parsing forlanguage models.
In Proceedings of the 39th An-nual Meeting on Association For ComputationalLinguistics (Toulouse, France, July 06 - 11, 2001).John Chen and Owen Rambow.
2003.
Use of deeplinguistic features for the recognition and labelingof semantic arguments.
In Proceedings of the 2003Conference on Empirical Methods in Natural Lan-guage Processing, Sapporo, Japan.Katrin Erk and Sebastian Pado.
2006.
Shalmaneser ?a toolchain for shallow semantic parsing.
Proceed-ings of LREC.Daniel Gildea and Daniel Jurafsky.
2002.
Automaticlabeling of semantic roles.
Computational Linguis-tics, 28(3):245?288.Daniel Gildea and Martha Palmer.
2002.
The neces-sity of parsing for predicate argument recognition.In Proceedings of the 40th Annual Conference ofthe Association for Computational Linguistics(ACL-02), Philadelphia, PA, USA.Mary Harper and Zhongqiang Huang.
2009.
ChineseStatistical Parsing, chapter to appear.Aria Haghighi, Kristina Toutanova, and ChristopherManning.
2005.
A joint model for semantic role la-beling.
In Proceedings of the Ninth Conference onComputational Natural Language Learning(CoNLL-2005), pages 173?176.Paul Kingsbury and Martha Palmer.
2003.
Propbank:the next level of treebank.
In Proceedings of Tree-banks and Lexical Theories.Evgeny Matusov, Gregor Leusch, & Hermann Ney:Learning to combine machine translation systems.In: Cyril Goutte, Nicola Cancedda, Marc Dymet-man, & George Foster (eds.)
Learning machinetranslation.
(Cambridge, Mass.
: The MIT Press,2009); pp.257-276.Adam Meyers, Ralph Grishman, Michiko Kosaka andShubin Zhao.
2001.
Covering Treebanks withGLARF.
In Proceedings of the ACL 2001 Work-shop on Sharing Tools and Resources.
AnnualMeeting of the ACL.
Association for Computa-tional Linguistics, Morristown, NJ, 51-58.Teruko Mitamura, Eric Nyberg, Hideki Shima,Tsuneaki Kato, Tatsunori Mori, Chin-Yew Lin,Ruihua Song, Chuan-Jie Lin, Tetsuya Sakai,Donghong Ji, and Noriko Kando.
2008.
Overviewof the NTCIR-7 ACLIA Tasks: Advanced Cross-Lingual Information Access.
In Proceedings of theSeventh NTCIR Workshop Meeting.Alessandro Moschitti, Silvia Quarteroni, RobertoBasili, and Suresh Manandhar.
2007.
Exploitingsyntactic and shallow semantic kernels for questionanswer classification.
In Proceedings of the 45thAnnual Meeting of the Association of Computa-tional Linguistics, pages 776?783.Kristen Parton, Kathleen R. McKeown, James Allan,and Enrique Henestroza.
Simultaneous multilingualsearch for translingual information retrieval.
InProceedings of ACM 17th Conference on Informa-tion and Knowledge Management (CIKM), NapaValley, CA, 2008.Slav Petrov and Dan Klein.
2007.
Improved Inferencefor Unlexicalized Parsing.
North American Chapterof the Association for Computational Linguistics(HLT-NAACL 2007).Sudo, K., Sekine, S., and Grishman, R. 2004.
Cross-lingual information extraction system evaluation.In Proceedings of the 20th international Confer-ence on Computational Linguistics.Honglin Sun and Daniel Jurafsky.
2004.
Shallow Se-mantic Parsing of Chinese.
In Proceedings ofNAACL-HLT.Cynthia A. Thompson, Roger Levy, and ChristopherManning.
2003.
A generative model for semanticrole labeling.
In 14th European Conference on Ma-chine Learning.Nianwen Xue and Martha Palmer.
2004.
Calibratingfeatures for semantic role labeling.
In Dekang Linand Dekai Wu, editors, Proceedings of EMNLP2004, pages 88?94, Barcelona, Spain, July.
Asso-ciation for Computational Linguistics.Xue, Nianwen and Martha Palmer.
2005.
Automaticsemantic role labeling for Chinese verbs.
InPro-ceedings of the Nineteenth International Joint Con-ference on Artificial Intelligence, pages 1160-1165.Chao Wang, Michael Collins, and Philipp Koehn.2007.
Chinese Syntactic Reordering for StatisticalMachine Translation.
Proceedings of the 2007 JointConference on Empirical Methods in Natural Lan-guage Processing and Computational Natural Lan-guage Learning (EMNLP-CoNLL), 737-745.Jianqiang Wang and Douglas W. Oard, 2006.
"Com-bining Bidirectional Translation and Synonymy forCross-Language Information Retrieval," in 29thAnnual International ACM SIGIR Conference onResearch and Development in Information Re-trieval, pp.
202-209.431
