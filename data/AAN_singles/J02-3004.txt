c?
2002 Association for Computational LinguisticsThe Disambiguation of NominalizationsMaria Lapata?University of EdinburghThis article addresses the interpretation of nominalizations, a particular class of compound nounswhose head noun is derived from a verb and whose modifier is interpreted as an argument of thisverb.
Any attempt to automatically interpret nominalizations needs to take into account: (a) theselectional constraints imposed by the nominalized compound head, (b) the fact that the relationof the modifier and the head noun can be ambiguous, and (c) the fact that these constraints canbe easily overridden by contextual or pragmatic factors.
The interpretation of nominalizationsposes a further challenge for probabilistic approaches since the argument relations between a headand its modifier are not readily available in the corpus.
Even an approximation that maps thecompound head to its underlying verb provides insufficient evidence.
We present an approachthat treats the interpretation task as a disambiguation problem and show how we can ?re-create?the missing distributional evidence by exploiting partial parsing, smoothing techniques, andcontextual information.
We combine these distinct information sources using Ripper, a systemthat learns sets of rules from data, and achieve an accuracy of 86.1% (over a baseline of 61.5%)on the British National Corpus.1.
IntroductionThe automatic interpretation of compound nouns has been a long-standing problemfor natural language processing (NLP).
Compound nouns in English have three basicproperties that present difficulties for their interpretation: (a) the compounding processis extremely productive (this means that a hypothetical system would have to interpretpreviously unseen instances), (b) the semantic relationship between the compoundhead and its modifier is implicit (this means that it cannot be easily recovered fromsyntactic or morphological analysis), and (c) the interpretation can be influenced by avariety of contextual and pragmatic factors.A considerable amount of effort has gone into specifying the set of semantic rela-tions that hold between a compound head and its modifier (Levi 1978; Warren 1978;Finin 1980; Isabelle 1984).
Levi (1978), for example, distinguishes two types of com-pound nouns: (a) compounds consisting of two nouns that are related by one of ninerecoverably deletable predicates (e.g., cause relates onion tears, for relates pet spray;see the examples in (1)) and (b) nominalizations, that is, compounds whose heads arenouns derived from a verb and whose modifiers are interpreted as arguments of therelated verb (e.g., a car lover loves cars; see the examples in (2)?(4)).
The prenominalmodifier can be either a noun or an adjective (see the examples in (2)).
The nominal-ized verb can take a subject (see (3a)), a direct object (see (3b)) or a prepositional object(see (3c)).
(1) a. onion tears causeb.
vegetable soup have?
Division of Informatics, University of Edinburgh, 2 Buccleuch Place, Edinburgh EH8 9LW, UK.
E-mail:mlap@cogsci.ed.ac.uk358Computational Linguistics Volume 28, Number 3c.
music box maked.
steam iron usee.
pine tree bef.
night flight ing.
pet spray forh.
peanut butter fromi.
abortion problem about(2) a. parental refusal subjb.
cardiac massage objc.
heart massage objd.
sound synthesizer obj(3) a. child behavior subjb.
car lover objc.
soccer competition at|in(4) a. government promotion subj|objb.
satellite observation subj|objBesides Levi (1978), a fair number of researchers (Warren 1978; Finin 1980; Isabelle1984; Leonard 1984) agree that there is a limited number of regularly recurring relationsbetween a compound head and its modifier.
There is far less agreement when it comesto the type and number of these relations.
The relations vary from Levi?s (1978) recov-erably deletable predicates to Warren?s (1978) paraphrases and Finin?s (1980) role nom-inals.
Leonard (1984) proposes eight relations, and Warren (1978) proposes six basic re-lations, whereas the number of relations proposed by Finin (1980) is potentially infinite.The attempt to restrict the semantic relations between the compound head and itsmodifier to a prespecified number and type has been criticized by Downing (1977),who has shown (through a series of psycholinguistic experiments) that the underlyingrelations can be influenced by a variety of pragmatic factors and cannot therefore bepresumed to be easily enumerable.
Sparck Jones (1983, page 4) further notes ?thatobservations about the semantic relation holding between the compound head and itsmodifier can only be remarks about tendencies and not about absolutes.?
Consider,for instance, the compound onion tears (see (1a)).
The relationship cause is one ofthe possible interpretations the compound may receive.
One could easily imagine acontext in which the tears are for or about the onion.
Consider example1 (5a), takenfrom Downing (1977, page 818).
Here apple-juice seat refers to the situation in whichsomeone is instructed to sit in a seat in front of which a glass of apple juice has beenplaced.
Given this particular state of affairs, none of the relations in (1) can be usedto successfully interpret apple-juice seat.
Such considerations have led Selkirk (1982) to1 Unless stated otherwise the example sentences were taken from the British National Corpus and insome cases simplified for purposes of clarity.359Lapata The Disambiguation of Nominalizationsclaim that only nominalizations are amenable to linguistic characterization, leaving allother compounds to be explained by pragmatics or discourse.
A similar approach is putforward by Hobbs et al (1993) for all types of compounds, including nominalizations:any two nouns can be combined, and the relation between these nouns is entirelyunderspecified, to be resolved pragmatically.
(5) a.
A friend of mine was once instructed to sit in the apple-juice seat.b.
By the end of the 1920s, government promotion of agriculturaldevelopment in Niger was limited, consisting mainly of crop trials andmodel sheep and ostrich farms.Less controversy arises with regard to nominalizations, perhaps because of thesmall number of allowable relations.
Most approaches follow Levi (1978) in distin-guishing nominalizations as a separate class of compounds, the exception being Finin(1980), who claims that most compounds are nominalizations, even in cases in whichthe head noun is not morphologically derived from a verb (see the examples in (1)).Under Finin?s analysis the head book in the compound recipe book is a role nominal, thatis, a noun that refers to a particular thematic role of another concept.
This means thatbook refers to the object role of write, which is filled by recipe.
It is not clear, however,how the implicit verb is to be recovered or why write is more appropriate than read inthis example.Despite the small number of relations between the nominalized head and its mod-ifier, the interpretation of nominalizations can readily change in different contexts.
Insome cases, the relation of the modifier and the nominalized verb (e.g., subject orobject) can be predicted either from the subcategorization properties of the verb orfrom the semantics of the nominalization suffix of the head noun.
Consider (3a), forexample.
Here child can be only the subject of behavior, since the verb behave is intran-sitive.
In (3b) the agentive suffix -er of the head noun lover indicates that the modifiercar is the object of the verb love.
In other cases, the relation of the modifier and thehead noun is genuinely ambiguous.
Out of context the compounds government promo-tion and satellite observation (see example (4)) can receive either a subject or an objectinterpretation.
One might argue that the preferred analysis for government promotion is?government that is promoted by someone.?
This interpretation can be easily overrid-den in context, however, as shown in Example (5b): here it is the government that isdoing the promotion.The automatic interpretation of compound nouns poses a challenge for empiricalapproaches, since the relations between a head and its modifier are not readily avail-able in a corpus, and therefore they have to be somehow retrieved and approximated.Given the data sparseness and the parameter estimation difficulties, it is not surprisingthat a far greater number of symbolic than probabilistic solutions have been proposedfor the automatic interpretation of compound nouns.
With the exception of Wu (1993)and Lauer (1995), who use probabilistic models for compound noun interpretation (seeSection 7 for details), most algorithms rely on hand-crafted knowledge bases or dic-tionaries that contain detailed semantic information for each noun; a sequence of rulesexploit a knowledge base to choose the correct interpretation for a given compound(Finin 1980; McDonald 1982; Leonard 1984; Vanderwende 1994).In what follows we develop a probabilistic model for the interpretation of nominal-izations.
We focus on nominalizations whose prenominal modifier is either the under-lying subject or direct object of the verb corresponding to the nominalized compoundhead.
In other words, we focus on examples like (3a, 3b) and ignore for the moment360Computational Linguistics Volume 28, Number 3nominalizations whose heads correspond to verbs taking prepositional complements(see example (3c)).
Nominalizations are attractive from an empirical perspective: theamount of relations is small (i.e., subject or object, at least if one focuses on directobjects only) and fairly uncontroversial (see the discussion above).
Although the rela-tions are not attested in the corpus, they can be retrieved and approximated throughparsing.
The probabilistic interpretation of nominalizations can provide a lower boundfor the difficulty of the compound interpretation task: if we cannot interpret nominal-izations successfully, there is little hope for modeling more complex semantic relationsstochastically (see the examples in (1)).We present a probabilistic algorithm that treats the interpretation task as a dis-ambiguation problem.
Our approach relies on the simplifying assumption that therelation of the nominalized head and its modifier noun can be approximated by therelation of the latter and the verb from which the head is derived.
This approach worksinsofar as the verb-argument relations from which the nominalizations are derived areattested in the corpus.
We show that a large number of verb-argument configurationsdo not occur in the corpus, something that is perhaps not surprising considering theease with which novel compounds are created (Levi 1978).
We estimate the frequen-cies of unseen verb-argument pairs by experimenting with three types of smoothingtechniques proposed in the literature (back-off smoothing, class-based smoothing, anddistance-weighted averaging) and show that their combination achieves good perfor-mance.
Furthermore, we explore the contribution of context to the disambiguation taskand show that performance is increased by taking contextual features into account.Our best results are achieved by combining the predictions of our probabilistic modelwith contextual information.The remainder of this article is organized as follows: in Section 2 we presenta simple statistical model for the interpretation of nominalizations and describe theprocedure used to collect the data for our experiments.
Section 3 presents details onhow the parameters of the model were estimated and gives a brief overview on thesmoothing methods with which we experimented.
Section 4 describes the algorithmused for the interpretation of nominalizations, and Section 5 reports the results ofseveral experiments that achieve a combined accuracy of 86.1% on the British NationalCorpus (BNC).
Section 6 discusses the findings.
In Section 7 we review related work,and we conclude in Section 8.2.
The Model2.1 Guessing Argument RelationsAs explained in Section 1, nominalizations are compounds whose head noun is a nom-inalized verb and whose prenominal modifier is derived from either the underlyingsubject or the underlying object of that verb (Levi 1978).
Our goal, given a nominaliza-tion, is to develop a procedure for inferring whether the modifier stands in a subjector object relation with respect to the head noun.
In other words, we need to assignprobabilities to the two different relations (subj, obj).
For each relation rel we calculatethe simple expression P(rel | n1, n2) given in (6).P(rel | n1, n2) =f (n1, rel , n2)f (n1, n2)(6)Since we have a choice between two outcomes we will use a likelihood ratio tocompare the two relation probabilities (Mosteller and Wallace 1964; Hindle and Rooth1993).
In particular we will compute the log of the ratio of the probability P(obj | n1, n2)361Lapata The Disambiguation of Nominalizationsto the probability P(subj | n1, n2).
We will call this log-likelihood ratio the argumentrelation (RA) score.RA(rel , n1, n2) = log2P(obj | n1, n2)P(subj | n1, n2)(7)Notice, however, that we cannot read off f (n1, rel , n2) directly from the corpus.What we can obtain from a corpus (through parsing) is the number of times a noun isthe object or the subject of a given verb.
By making the simplifying assumption thatthe relation of the nominalized head and its modifier noun is the same as the relationbetween the latter and the verb from which the head is derived, we can rewrite (6) asfollows:P(rel | n1, n2) ?f (vn2 , rel , n1)?if (vn2 , rel i, n1)(8)where f (vn2 , rel , n1) is the frequency with which the modifier noun n1 is found in thecorpus as the subject or object of vn2 , the verb from which the head noun is derived.The sum?i f (vn2 , rel i, n1) is a normalization factor.2.2 Parameter Estimation2.2.1 Verb-Argument Tuples.
We estimated the parameters of the model outlinedin the previous section from a part-of-speech-tagged and lemmatized version of theBNC, a 100-million-word collection of samples of written and spoken language from awide range of sources designed to represent current British English (Burnard 1995).
Toestimate the term f (vn2 , rel , n1), the corpus was automatically parsed by Cass (Abney1996), a robust chunk parser designed for the shallow analysis of noisy text.
The mainfeature of Cass is its finite-state cascade technique.
A finite-state cascade is a sequenceof nonrecursive levels: phrases at one level are built on phrases at the previous levelwithout containing same-level or higher-level phrases.
We used the parser?s built-infunction to extract tuples of verb subjects and verb objects (see (9)).
(9) a. change situation subjb.
come off heroin objc.
deal with situation obj(10) a. isolated people subjb.
smile good subjThe tuples obtained from the parser?s output are an imperfect source of infor-mation about argument relations.
Bracketing errors, as well as errors in identifyingchunk categories accurately, result in tuples whose lexical items do not stand in averb-argument relationship.
For example, inspection of the original BNC sentencesfrom which (10a) and (10b) were derived revealed that the verb is missing from theformer and the noun is missing from the latter (see the sentences in (11)).
(11) a. Wenger found that more than half the childless old people in herstudy of rural Wales saw a relative, a sibling, niece, nephew or cousinat least once a week, though in inner city London there were moreisolated old people.b.
I smiled my best smile down the line.362Computational Linguistics Volume 28, Number 3Table 1Tuples extracted from the BNC.Tokens TypesRelation Parser Filtering Tuples Verbs Nounssubj 4,491,386 4,095,578 588,333 10,852 41,336obj 2,631,752 2,598,069 615,328 9,490 35,846Table 2Deverbal suffixes.Suffix Nominalization-er drink ?
drinker-or direct ?
director-ant disinfect ?
disinfectant-ee employ ?
employee-ation educate ?
education-ment arrange ?
arrangement-al refuse ?
refusal-ing hire ?
hiringTable 3Conversion.Verb ?
Nounrelease ?
releasearrest ?
arrestcompromise ?
compromiseattempt ?
attemptTo compile a comprehensive count of verb-argument relations, we tried to elimi-nate from the parser?s output tuples containing erroneous verbs and nouns like thosein (10).
We did this by matching the verbs contained in the tuples against a list of allwords tagged as verbs and nouns in the BNC.
Tuples containing words not included inthe list were discarded.
Furthermore, we discarded tuples containing verbs or nounsattested in a verb-argument relationship only once.
This resulted in 588,333 distinctverb-subject pairs and 615,328 distinct verb-object pairs (see Table 1, which containsinformation about the tuples extracted from the corpus before and after the filteringdescribed earlier in the paragraph).2.2.2 The Data.
So far we have been using the term nominalization to refer to two-wordcompounds whose head is derived from a verb.
Morphologically speaking, nominal-ization is a word formation process by which a noun is derived from a verb, usuallyby means of suffixation (Quirk et al 1985).
A list of deverbal suffixes (i.e., suffixes thatform nouns when attached to verb bases) is given in Table 2.
Nominalizations can alsobe created by conversion, the word formation process whereby ?an item is adaptedor converted to a new word-class without the addition of an affix?
(Quirk et al 1985,page 1009).
Examples of conversion are shown in Table 3.It is beyond the scope of the present study to develop an algorithm that auto-matically detects nominalizations in a corpus.
In the experiments described in thesubsequent sections compounds with deverbal heads were obtained as follows:1.
Two-word compound nouns were extracted from the BNC using aheuristic that looks for consecutive pairs of nouns that are neitherpreceded nor succeeded by a noun (Lauer 1995).2.
A dictionary of deverbal nouns was created using two sources:(a) nomlex (Macleod et al 1998), a dictionary of nominalizations363Lapata The Disambiguation of Nominalizationscontaining 827 lexical entries, and (b) celex (Burnage 1990), a generalmorphological dictionary that contains 5,111 nominalizations; bothdictionaries list the verbs from which the nouns are derived.
Sampledictionary entries are given in Tables 2 and 3.3.
Candidate nominalizations were obtained from the compounds acquiredfrom the BNC by selecting noun-noun sequences whose head (i.e.,rightmost noun) was one of the deverbal nouns contained in eithercelex or nomlex.
The procedure resulted in 172,797 potential types ofnominalizations.From these candidate nominalizations a random sample of 1,277 tokens was selected.The sample was manually inspected, and compounds with modifiers whose relationto the head noun was other than subject or object were discarded.
In particular nom-inalizations were discarded if: (a) the relation between the head and the modifierwas any of the semantic relations listed in (1) (e.g., cause, have, make); these com-pounds represented 28.0% of the sample; (b) the head was derived from verbs takingprepositional objects (see example (3c)); these nominalizations represented 9.2% of thesample.
After manual inspection the sample contained 796 nominalizations (62.8%of the initial sample).
These tokens were used for the experiments described in Sec-tion 5.2.2.3 Mapping.
To estimate the frequency, f (vn2 , rel , n1), the nominalized heads weremapped to their corresponding verbs.
Inspection of the frequencies of the verb-argu-ment tuples contained in our data (796 tokens) revealed that 480 verb-noun pairs(60.3%) had a verb-object frequency of zero in the corpus.
Similarly, 503 verb-nounpairs (63.2%) had a verb-subject frequency of zero.
Furthermore, a total of 373 tuples(46.9%) were not attested at all in the BNC either in a verb-object or verb-subjectrelation.
This finding is not entirely unexpected, considering that compounds are typ-ically used as a text compression device (Marsh 1984), that is, to pack meaning intoa minimal amount of linguistic structure.
If a nominalization is chosen over a moreelaborate structure (i.e., a sentence), then it is not surprising that some verb-argumentconfigurations will not occur in the corpus.
Furthermore, some nominalizations areconventionalized (e.g., business administration, health organization) and are therefore at-tested more frequently than their verb-subject or verb-object counterparts.We re-created the frequencies of unseen verb-argument pairs by experimentingwith three types of smoothing techniques proposed in the literature: back-off smooth-ing (Katz 1987), class-based smoothing (Resnik 1993; Lauer 1995), and distance-weighted averaging (Grishman and Sterling 1994; Dagan, Lee, and Pereira 1999).
Wepresent these three smoothing variants and their underlying assumptions in the fol-lowing section.3.
SmoothingSmoothing techniques have been used in a variety of statistical NLP applications as ameans of addressing data sparseness, an inherent problem for statistical methods thatrely on the relative frequencies of word combinations.
The problem arises when theprobability of word combinations that do not occur in the training data needs to beestimated.
The smoothing methods proposed in the literature (overviews are providedby Dagan, Lee, and Pereira (1999) and Lee (1999)) can be generally divided into threetypes: discounting (Katz 1987), class-based smoothing (Resnik 1993; Brown et al 1992;364Computational Linguistics Volume 28, Number 3Pereira, Tishby, and Lee 1993), and distance-weighted averaging (Grishman and Sterling1994; Dagan, Lee, and Pereira 1999).Discounting methods decrease the probability of previously seen events so that thetotal probability of observed word co-occurrences is less than one, leaving some prob-ability mass to be redistributed among unseen co-occurrences.
Class-based smoothingand distance-weighted averaging both rely on an intuitively simple idea: interworddependencies are modeled by relying on the corpus evidence available for words thatare similar to the words of interest.
The two approaches differ in the way they measureword similarity.
Distance-weighted averaging estimates word similarity from lexicalco-occurrence information; namely, it finds similar words by taking into account thelinguistic contexts in which they occur: two words are similar if they occur in similarcontexts.
In class-based smoothing, classes are used as the basis according to which theco-occurrence probability of unseen word combinations is estimated.
Classes can beinduced directly from the corpus using distributional clustering (Pereira, Tishby, andLee 1993; Brown et al 1992; Lee and Pereira 1999) or taken from a manually craftedtaxonomy (Resnik 1993).
In the latter case the taxonomy is used to provide a mappingfrom words to conceptual classes.Distance-weighted averaging differs from distributional clustering in that it doesnot explicitly cluster words.
Although both methods make use of the evidence ofwords similar to the words of interest, distributional clustering assigns to each worda probability distribution over clusters to which it may belong; co-occurrence proba-bilities can then be estimated on the basis of the average of the clusters to which thewords in the co-occurrence belong.
This means that word co-occurrences are modeledby taking general word clusters into account and that the same set of clusters is usedfor different co-occurrences.
Distance-weighted averaging does not explicitly creategeneral word clusters.
Instead, unseen co-occurrences are estimated by averaging theset of co-occurrences most similar to the target unseen co-occurrence, and a differ-ent set of similar neighbors (i.e., distributionally similar words) is used for differentco-occurrences.In language modeling, smoothing techniques are typically evaluated by showingthat a language model that uses smoothed estimates incurs a reduction in perplexityon test data over a model that does not employ smoothed estimates (Katz 1987).Dagan, Lee, and Pereira (1999) use perplexity to compare back-off smoothing againstdistance-weighted averaging methods within the context of language modeling forspeech recognition and show that the latter outperform the former.
They also comparedifferent distance-weighted averaging methods on a pseudoword disambiguation taskin which the language model decides which of two verbs v1 and v2 is more likely totake a noun n as its object.
The method being tested must reconstruct which of theunseen (v1, n) and (v2, n) is a valid verb-object combination.
The same task is used byLee and Pereira (1999) in a detailed comparison between distributional clustering anddistance-weighted averaging that demonstrates that the two methods yield comparableresults.In our experiments we re-created co-occurrence frequencies for unseen verb-sub-ject and verb-object pairs using three maximally different approaches: back-off smooth-ing, class-based smoothing using a predefined taxonomy, and distance-weighted av-eraging.
We preferred taxonomic class-based methods over distributional clusteringmainly because we wanted to compare directly methods that use distributional infor-mation inherent in the corpus without making external assumptions with regard tohow concepts and their similarity are represented with methods that quantify sim-ilarity relationships based on information present in a hand-crafted taxonomy.
Fur-thermore, as Lee and Pereira?s (1999) results indicate that distributional clustering365Lapata The Disambiguation of Nominalizationsand distance-weighted averaging obtain similar levels of performance, we restrictedourselves to the latter.We evaluated the contribution of the different smoothing methods on the nomi-nalization task by exploring how each method and their combination influences dis-ambiguation performance.
Sections 3.1?3.3 review discounting, class-based smooth-ing, and distance-weighted averaging.
Section 4 introduces an algorithm that usessmoothed verb-argument tuples to arrive at the interpretation of nominalizations.3.1 Back-Off SmoothingBack-off n-gram models were initially proposed by Katz (1987) for speech recognitionbut have also been successfully used to disambiguate the attachment site of struc-turally ambiguous prepositional phrases (Collins and Brooks 1995).
The main ideabehind back-off smoothing is to adjust maximum likelihood estimates like (8) so thatthe total probability of observed word co-occurrences is less than one, leaving someprobability mass to be redistributed among unseen co-occurrences.
In general the fre-quency of observed word sequences is discounted using the Good-Turing estimate (seeKatz (1987) and Church and Gale (1991) for details on Good-Turing estimation), andthe probability of unseen sequences is estimated by using lower-level conditional dis-tributions.
Assuming that the numerator f (vn2 , rel , n1) in (8) is zero we can approximateP(rel | n1, n2) by backing off to P(rel | n1):P(rel | n1, n2) = ?f (rel , n1)f (n1)(12)where ?
is a normalization constant that ensures that the probabilities sum to one.
Ifthe frequency f (rel , n1) is also zero, backing off continues by making use of P(rel).3.2 Class-Based SmoothingGenerally speaking, taxonomic class-based smoothing re-creates co-occurrence fre-quencies based on information provided by lexical resources such as WordNet (Milleret al 1990) or Roget?s publicly available thesaurus.
In the case of verb-argument tuples,we use taxonomic information to estimate the frequencies f (vn2 , rel , n1) by substitutingfor the word n1 occurring in an argument position the concept with which it is repre-sented in the taxonomy (Resnik 1993).
So f (vn2 , rel , n1) can be estimated by countingthe number of times the concept corresponding to n1 was observed as the argumentof the verb vn2 in the corpus.This would be a straightforward task if each word was always represented inthe taxonomy by a single concept or if we had a corpus of verb-argument tupleslabeled explicitly with taxonomic information.
Lacking such a corpus we need to takeinto consideration the fact that words in a taxonomy may belong to more than oneconceptual class: counts of verb-argument configurations are reconstructed for eachconceptual class by dividing the contribution from the argument by the number ofclasses to which it belongs (Resnik 1993; Lauer 1995):f (vn2 , rel , c) ?
?n?1?cf (vn2 , rel , n?1)|classes(n?1)|(13)where f (vn2 , rel , n?1) is the number of times the verb vn2 was observed with conceptc ?
classes(n?1) bearing the argument relation rel (i.e., subject or object) and |classes(n?1)|is the number of conceptual classes to which n?1 belongs.366Computational Linguistics Volume 28, Number 3Table 4Frequency estimation for group registration using WordNet.Verb Class f (vn2 ,obj, n1) f (vn2 ,subj, n1)register ?abstraction?
16.26 7.28register ?entity?
14.10 4.50register ?object?
8.02 1.56register ?set?
.65 .07register ?substance?
.70 .08Consider, for example, the tuple register group (derived from the compound groupregistration), which is not attested in the BNC.
The word group has two senses inWordNet and belongs to five conceptual classes (?abstraction?, ?entity?, ?object?,?set?, and ?substance?).
This means that the frequency f (vn2 , rel , c) will be constructedfor each of the five classes, as shown in Table 4.
Suppose now that we see the tupleregister patient in the corpus.
The word patient has two senses in WordNet and belongsto seven conceptual classes (?case?, ?person?, ?life form?, ?entity?, ?causal agent?,?sick person?, ?unfortunate?
), one of which is ?entity?.
This means that we willincrement the observed co-occurrence count of register and ?entity?
by 17 .
Since wedo not know which is the actual class of the noun group in the corpus, we weight thecontribution of each class by taking the average of the constructed frequencies for allfive classes:f (vn2 , rel , n1) =?c?classes(n1)?n?1?cf (vn2 ,rel ,n?1)|classes(n?1)||classes(n1)|(14)Following (14) the frequencies f (register , obj, group) and f (register , subj, group) are39.735 and13.495 , respectively.
Note that the estimation of the frequency f (vn2 , rel , n1) (seeequations (13) and (14)) crucially relies on the simplifying assumption that the argu-ment of a verb is distributed evenly across its conceptual classes.
This simplificationis necessary unless we have a corpus of verb-argument pairs labeled explicitly withtaxonomic information.
The task of finding the right class for representing the argu-ment of a given predicate is a research issue on its own (Clark and Weir 2001; Li andAbe 1998; Carroll and McCarthy 2000), and a detailed comparison between differentmethods for accomplishing this task is beyond the scope of the present study.3.3 Distance-Weighted AveragingDistance-weighted averaging induces classes of similar words from word co-occur-rences without making reference to a taxonomy.
Instead, it is based on the assumptionthat if a word w?1 is similar to word w1, then w?1 can provide information about thefrequency of unseen word pairs involving w1 (Dagan, Lee, and Pereira 1999).
A keyfeature of this type of smoothing is the function that measures distributional similarityfrom co-occurrence frequencies.Several measures of distributional similarity have been proposed in the literature(Dagan, Lee, and Pereira 1999; Lee 1999).
We used two measures, the Jensen-Shannondivergence and the confusion probability.
The choice of these two measures was mo-tivated by work described in Dagan, Lee, and Pereira (1999), in which the Jensen-Shannon divergence outperforms related similarity measures (such as the confusionprobability or the L1 norm) on a pseudodisambiguation task that uses verb-objectpairs.
The confusion probability has been used by several authors to smooth word co-367Lapata The Disambiguation of Nominalizationsoccurrence probabilities (Essen and Steinbiss 1992; Grishman and Sterling 1994) andshown to give promising performance.
Grishman and Sterling (1994) in particular em-ploy the confusion probability to re-create the frequencies of verb-noun co-occurrencesin which the noun is the object or the subject of the verb in question.
In the followingwe describe these two similarity measures and show how they can be used to re-createthe frequencies for unseen verb-argument tuples (for a more detailed description seeDagan, Lee, and Pereira (1999)).3.3.1 Confusion Probability.
The confusion probability PC is an estimate of the prob-ability that a word w1 can be substituted for a word w?1, in the sense of being foundin the same contexts.
In other words, the metric expresses how probable it is for wordw?1 to occur in contexts in which word w1 occurs.
A large confusion probability valueindicates that the two words w?1 and w1 appear in similar contexts.
PC is estimated asfollows:PC(w1 | w?1) =?sP(w1 | s)P(s | w?1) (15)where PC(w1 | w?1) is the probability that word w?1 occurs in the same contexts s as wordw1, averaged over these contexts.
Given a tuple of the form w1, rel , w2, we can eithertreat w1, rel as context and smooth over the noun w2 or rel , w2 as context and smoothover the verb w1.
We opted for the latter for two reasons.
Theoretically speaking, it isthe verb that imposes the semantic restrictions on its arguments and not vice versa.
Theidea that semantically similar verbs have similar subcategorizational and selectionalpatterns is by no means new and has been extensively argued for by Levin (1993).Computational efficiency considerations also favor an approach that treats rel , w2 ascontext: the nouns w2 outnumber the verbs w1 by a factor of four (see Table 1).
Whenverb-argument tuples are taken into consideration, (8) can be rewritten as follows:PC(w1 | w?1) =?rel ,w2P(w1 | rel , w2)P(rel , w2 | w?1)=?rel ,w2f (w1,rel ,w2)f (rel ,w2)f (w?1,rel ,w2)f (w?1)(16)The confusion probability can be computed efficiently, since it involves summationonly over the common contexts rel , w2.3.3.2 Jensen-Shannon Divergence.
The Jensen-Shannon divergence J is an informa-tion-theoretic measure.
It recasts the concept of distributional similarity into a mea-sure of the ?distance?
between two probability distributions.
The value of the Jensen-Shannon divergence ranges from zero for identical distributions to log 2 for maximallydifferent distributions.
J is defined as:J(w1, w?1) =12[D(w1???
?w1 + w?12)+ D(w?1???
?w1 + w?12)](17)D(w1?w?1) =?rel ,w2P(rel , w2 | w1) logP(rel , w2 | w1)P(rel , w2 | w?1)(18)where w1 is a shorthand for P(rel , w2 | w1) and w?1 for P(rel , w2 | w?1); D in (17) isthe Kullback-Leibler divergence, a measure of the dissimilarity between two proba-bility distributions (see equation (18)) and (w1 + w?1)/2 is a shorthand for the averagedistribution:12(P(rel , w2 | w1) + P(rel , w2 | w?1)) (19)368Computational Linguistics Volume 28, Number 3Given a set of nominalizations n1 n2:1. map the head noun n2 to the verb vn2 from which it is derived;2. retrieve frequencies f (vn2 ,obj, n1) and f (vn2 , subj, n1) from the BNC;3. if f (vn2 ,obj, n1) < k then re-create fs(vn2 ,obj, n1);4. if f (vn2 , subj, n1) < k then re-create fs(vn2 , subj, n1);5. calculate probabilities P(obj | n1, n2) and P(subj | n1, n2);6. compute RA(rel, n1, n2);7. if RA ?
j then n1 is the object of n2;8. else n1 is the subject of n2.Figure 1Disambiguation algorithm for nominalizations.Similarly to the confusion probability, the computation of J depends only on thecommon contexts rel , w2.
Recall that the Jensen-Shannon divergence is a dissimilaritymeasure.
The dissimilarity measure is transformed into a similarity measure using aweight function WJ(w, w?1):WJ(w1, w?1) = 10?
?J(w1,w?1) (20)The parameter ?
controls the relative influence of the neighbors (i.e., distributionallysimilar words) closest to w1: if ?
is high, only neighbors extremely close to w1 con-tribute to the estimate, whereas if ?
is low, distant neighbors also contribute to theestimate.We estimate the frequency of an unseen verb-argument tuple by taking into ac-count the similar w1s and the contexts in which they occur (Grishman and Sterling1994):fs(w1, rel , w2) =?w?1sim(w1, w?1)f (w?1, rel , w2) (21)where sim(w1, w?1) is a function of the similarity between w1 and w?1.
In our experimentsthe confusion probability PC(w1 | w?1) and the Jensen-Shannon divergence WJ(w1, w1?
)were substituted for sim(w1, w?1).4.
The Disambiguation AlgorithmThe disambiguation algorithm for nominalizations is summarized in Figure 1.
The al-gorithm uses verb-argument tuples to infer the relation holding between the modifierand its nominalized head.
When the co-occurrence frequency of the verb-argument re-lations is zero, verb-argument tuples are smoothed using one of the methods describedin Section 3.Once frequencies (either actual or reconstructed through smoothing) for verb-argu-ment relations have been obtained, the RA score determines the relation between thehead n1 and its modifier n2 (see Section 2).
The sign of the RA score indicates whichrelation, subject or object, is more likely: a positive RA score indicates an object relation,whereas a negative score indicates a subject relation.
Depending on the task and thedata at hand, we can require that an object or subject analysis be preferred only if RAexceeds a certain threshold j (see steps 7 and 8 in Figure 1).
We can also impose athreshold k on the type of verb-argument tuples we smooth.
If, for instance, we know369Lapata The Disambiguation of NominalizationsTable 5RA score for verb-argument tuples extracted from the BNC.Verb-noun f (vn2,obj, n1) f (vn2,subj, n1) RAadminister student 0 0 .96establish unit 22 1 .55promote government 3 10 ?1.73that the parser?s output is noisy, then we might choose to smooth not only unseen verb-argument pairs but also pairs with nonzero corpus frequencies (e.g., f (verbn2 , rel , n1)?
1; see steps 3 and 4 in Figure 1).Consider, for example, the compound student administration: its correspondingverb-noun configuration (e.g., administer student) is not attested in the BNC.
This isa case in which we need smoothed estimates for both f (vn2, obj, n1) and f (vn2, subj,n1).
The re-created frequencies using the class-based smoothing method described inSection 3.2 are 5.06 and 2.59, respectively, yielding an RA score of .96 (see Table 5),which means that it is more likely that student is the object of administration.
Considernow the compound unit establishment: here, we have very little evidence in the corpuswith respect to the verb-subject relation (see Table 5, where f (establish , subj, unit) = 1).Assuming we have set the threshold k to 2 (see steps 4 and 5 in Figure 1) we need onlyre-create the frequency for the subject relation (e.g., 14.99 using class-based smooth-ing).
The resulting RA score is again positive (see Table 5), which indicates that thereis a greater probability for unit to be the object of establishment than for it to be thesubject.
Finally, consider the compound government promotion: counts for both subjectand object relations are found in the BNC (see Table 5), in which case no smoothingis involved; we need only calculate the RA score (see step 6 in Figure 1), which isnegative, indicating that government is more likely to be the subject of promotion thanits object.5.
Experiments5.1 MethodologyThe algorithm described in the previous section and the smoothing variants wereevaluated on the task of disambiguating nominalizations.
As detailed above, theJensen-Shannon divergence and confusion probability measures are parameterized.This means that we need to establish empirically the best parameter values for thesize of the vocabulary (i.e., number of verbs used to find the nearest neighbors)and, for the Jensen-Shannon divergence, the effect of the ?
parameter.
Recall fromSection 2.2.2 that we obtained 796 nominalizations from the BNC.
From these, 596were used as training data for finding the optimal parameters for the two variants ofdistance-weighted averaging.
The 596 nominalizations were also used to find the op-timal thresholds for the interpretation algorithm.
The remaining 200 nominalizationswere retained as test data and also to evaluate whether human judges can reliablydisambiguate the argument relation between the nominalized head and its modifier(see Experiment 1).In Experiment 2 we investigate how the different smoothing techniques detailedin Section 3 influence the disambiguation task.
As far as class-based smoothing is con-cerned, we experiment with two concept hierarchies, Roget?s thesaurus and WordNet.Although no parameter tuning is necessary for class-based and back-off smoothing, we370Computational Linguistics Volume 28, Number 3maintain the train/test data distinction also for these methods to facilitate comparisonswith distance-weighted averaging.We also examine whether knowledge of the semantics of the suffix of the nominal-ized head can improve performance.
We run two versions of the algorithm presentedin Section 4: in one version the algorithm assumes no prior knowledge about the se-mantics of the nominalization suffix (see Figure 1); in the other version the algorithmestimates the probabilities P(obj | n1, n2) and P(subj | n1, n2) only for compounds withnominalization suffixes other than -er, -or, -ant, or -ee.
For compounds with suffixes -er,-or and -ant (e.g., datum holder, car collector, water disinfectant), the algorithm defaultsto an object interpretation, and it defaults to a subject analysis for compounds withthe suffix -ee (e.g., university employee).
Compounds with heads ending in these foursuffixes represented 13.6% of the compounds contained in the train set and 10.8% ofthe compounds in the test set.In Experiment 3 we explore how the combination of the different smoothing meth-ods influences disambiguation performance; we also consider context as an additionalpredictor of the argument relation of a deverbal head and its modifier and combinethese distinct information sources using Ripper (Cohen 1996), a machine learning sys-tem that induces sets of rules from preclassified examples.In what follows we briefly describe our study on assessing how well humansagree on disambiguating nominalizations.
This study establishes an upper bound forthe task against which our automatic methods will be compared.
Sections 5.3 and 5.4present our results on the disambiguation task.5.2 Experiment 1: AgreementTwo graduate students in linguistics decided whether modifiers were the subject orobject of a given nominalized head.
The judges were given a page of guidelines but noprior training.
The nominalizations were disambiguated in context: the judges weregiven the corpus sentence in which the nominalization occurred together with theprevious and following sentence.
We measured the judges?
agreement using the kappacoefficient (Siegel and Castellan 1988), which is the ratio of the proportion of times P(A)that k raters agree (corrected by chance agreement P(E)) to the maximum proportionof times the raters would agree (corrected for chance agreement):K =P(A)?
P(E)1 ?
P(E) (22)If there is a complete agreement among the raters, then K = 1, whereas if there isno agreement among the raters (other than the agreement that would be expected tooccur by chance), then K = 0.The judges?
agreement on the disambiguation task was K = .78 (N = 200, k = 2).This translates into a percentage agreement of 89.7%.
Although the Kappa coefficienthas a number of advantages over percentage agreement (e.g., it takes into account theexpected chance interrater agreement; see Carletta (1996) for details), we also reportpercentage agreement as it allows us to compare straightforwardly the human perfor-mance and the automatic methods described below, whose performance will also bereported in terms of percentage agreement.
Furthermore, percentage agreement estab-lishes an intuitive upper bound for the task (i.e., 89.7%), allowing us to interpret howwell our empirical models are doing in relation to humans.Finally, note that the level of agreement was good, given that the judges wereprovided with minimal instructions and no prior training.
Even though context wasprovided to aid the disambiguation task, however, the judges were not in complete371Lapata The Disambiguation of NominalizationsFigure 2Disambiguation accuracy as the number of similar neighbors (i.e., number of verbs over whichthe similarity function is calculated) is varied for PC and J.agreement.
This points to the intrinsic difficulty of the task at hand.
Argument re-lations and consequently selectional restrictions are influenced by several pragmaticfactors that may not be readily inferred from the immediate context (see Section 6 fordiscussion).5.3 Experiment 2: Comparison of Smoothing VariantsBefore reporting the results of the disambiguation task, we describe our initial ex-periments on finding the optimal parameter settings for the two distance-weightedaveraging smoothing methods.Figure 2 shows how performance on the disambiguation task varies with respectto the number and frequency of verbs over which the similarity function is calculated.The y-axis in Figure 2 shows how performance on the training set varies (for both PCand J divergence) when verb-argument pairs are selected for the 1,000 most frequentverbs in the corpus, the 2,000 most frequent verbs in the corpus, etc.
(x-axis).
The bestperformance for both similarity functions is achieved with the 2,000 most frequentverbs.
Furthermore, J and PC yield comparable performances (68.0% and 68.3%, re-spectively under that condition).
Another important observation is that performancedeteriorates less severely for PC than for J as the number of verbs increases: whenall verbs for which verb-argument tuples are extracted from the BNC are used, theaccuracy for PC is 66.9%, whereas the accuracy for J is 62.8%.
These results are perhapsunsurprising: verb-argument pairs with low-frequency verbs introduce noise due tothe errors inherent in the partial parser.
Table 6 shows the 10 closest words to theverb accept according to PC as the number of verbs is varied: the quality of the closestneighbors deteriorates with the inclusion of less frequent verbs.Finally, we analyzed the role of the parameter ?.
Recall that ?
appears in the weightfunction for the Jensen-Shannon divergence and controls the influence of the mostsimilar words: the contribution of the closest neighbors increases with a high valuefor ?.
Figure 3 shows how the value of ?
affects performance on the disambiguationtask when the similarity function is computed for the 1,000 and 2,000 most frequentverbs in the corpus.
It is clear that performance is low with high or very low ?
values372Computational Linguistics Volume 28, Number 3Table 6Ten closest words to verb accept for PC.Number of Most Frequent Verbs1,000 2,000 3,000 4,000 5,000 >5,000accept decline decline decline decline inclrefuse accept tender tender re-issued declinereject refuse accept abdicate co-manage re-issuedsubmit delegate table accept tender co-manageendorse reject disclaim table oversubscribe tenderapprove repudiate plate wangle backdate goodwillissue hitch shirk disclaim abdicate oversubscribeimplement shoulder refuse plate accept pre-arrangeacknowledge delegate proffer shirk table backdateincur ratify apportion disdain wangle abdicateFigure 3Disambiguation accuracy for J as ?
is varied for the 1,000 and 2,000 most frequent verbs in theBNC.
(e.g., ?
?
{2, 9}).
We chose to set the parameter ?
to five, and the results shown inFigure 2 have been produced for this value for all verb frequency classes.Table 7 shows how the three types of smoothing, back-off (B), class-based (usingWordNet (Wn) and Roget (Ro)), and distance-weighted averaging (using confusionprobability (PC) and the Jensen-Shannon divergence (J)), influence performance inpredicting the relation between a modifier and its nominalized head.
For the distance-weighted averaging methods we report the results obtained with the optimal param-eter settings (?
= 5; 2,000 most frequent verbs).
The results in Table 7 were obtainedwithout taking the semantics of the nominalization suffix (-er, -or, -ant, -ee) into account(see Section 5.1).Let us concentrate on the training set first.
The back-off method is outperformedby all other methods, although its performance is comparable to that of class-basedsmoothing using Roget?s thesaurus (63.1% and 65.1%, respectively).
Distance-weightedaveraging methods outperform concept-based methods, although not considerably(accuracy on the training set was 68.3% for PC and 68.0% for class-based smoothing373Lapata The Disambiguation of NominalizationsTable 7Disambiguation performance withoutnominalization suffixes.Methods Train (%) Test (%)D 59.0 ?
2.01 61.5 ?
3.50B 63.1 ?
1.98 69.6 ?
3.31PC 68.3 ?
1.90 75.8 ?
3.08J 68.0 ?
1.91 69.1 ?
3.33Wn 68.0 ?
1.91 72.7 ?
3.20Ro 65.1 ?
1.95 68.6 ?
3.34Table 8Disambiguation performance withnominalization suffixes.Methods Train (%) Test (%)D 59.0 ?
2.01 61.5 ?
3.50B 67.5 ?
1.92 69.6 ?
3.31PC 70.6 ?
1.87 76.3 ?
3.06J 69.0 ?
1.89 69.6 ?
3.31Wn 70.5 ?
1.87 74.2 ?
3.15Ro 67.5 ?
1.92 69.6 ?
3.31using WordNet).
Furthermore, the particular concept hierarchy used for class-basedsmoothing seems to have an effect on disambiguation performance: an increase ofapproximately 3.0% is obtained by using WordNet instead of Roget?s thesaurus.
Oneexplanation might be that Roget?s thesaurus is too coarse-grained a taxonomy forthe task at hand.
We used the chi-square statistic to examine whether the observedperformance is better than the simple default strategy of always choosing an objectrelation, which yields an accuracy of 59.0% in the training data (see D in Table 7).
Theproportion of nominalizations classified correctly was significantly greater than 59.0%(p < .01) for all methods but back-off (B) and Roget (Ro).Similar results are observed on the test set.
Again PC outperforms all other meth-ods, achieving an accuracy of 75.8% (see Table 7).
The portion of nominalizationsclassified correctly by PC is significantly greater than 61.5% (?2 = 9.37, p < .01), whichis the percentage of object relations in the test set.
The second-best method is class-based smoothing using WordNet (see Table 7).
WordNet?s performance is also signif-icantly better (?2 = 5.64, p < .05) than the baseline.
The back-off method, class-basedsmoothing using Roget?s thesaurus, and J yield comparable results (see Table 7).Table 8 shows how each method performs when knowledge about the semantics ofthe nominalization suffix is taken into account.
Recall that compounds with agentiveand passive suffixes (i.e., -er, -or, -ant, and -ee) represent 13.6% of the training data and10.8% of the test data.
A general observation is that knowledge of the semantics of thenominalization suffix does not dramatically influence accuracy.
Performance on the testdata increases 1.5% for Wn , 1.0% for Ro and 0.5% for distance-weighted averaging (seeJ and PC in Table 8).
We observe no increase in performance for back-off smoothing(see Tables 7 and 8).
These results suggest that the nominalization suffixes do notcontribute much additional information to the interpretation task, as their meaningcan be successfully retrieved from the corpus.An interesting question is the extent to which any of the different methods agreein their assignments of subject and object relations.
We investigated this by calculatingthe methods?
agreement on the training set using the Kappa coefficient.
We calculatedthe Kappa coefficient for all pairwise combinations of the five smoothing variants.The results are reported in Table 9.
The highest agreement is observed for PC and theclass-based smoothing using the WordNet taxonomy (K = .75).
Agreement between Jand PC as well as agreement between Wn and Ro is rather low (K = .53 and K = .46,respectively).
Note that generally low agreement is observed when B is paired withJ, PC, Wn, or Ro.
This is not entirely unexpected, given the assumptions underlyingthe different smoothing techniques.
Both class-based and distance-weighted averagingmethods recreate the frequency of unseen word combinations by relying on corpus374Computational Linguistics Volume 28, Number 3Table 9Agreement between smoothing methods.B J PC WnJ .31PC .26 .53Wn .01 .37 .75Ro .25 .26 .49 .46Table 10Performance at predicting argumentrelations.Train (%) Test (%)Methods subj obj subj objB 41.6 78.1 38.0 87.8PC 47.4 82.9 54.9 87.8J 34.7 91.2 35.2 88.6Wn 47.8 82.1 49.3 86.2Ro 50.6 74.4 46.5 81.3evidence for words that are distributionally similar to the words of interest.
In distance-weighted averaging smoothing, word similarity is estimated from lexical co-occurrenceinformation, whereas in taxonomic class-based smoothing, similarity emerges fromthe hierarchical organization of conceptual information.
Back-off smoothing, however,incorporates no notion of similarity: unseen sequences are estimated using not similarconditional distributions, but lower-level ones.
This also relates to the fact that B?sperformance is lower than Wn and PC (see Table 7), which suggests that smoothingmethods that incorporate linguistic hypotheses (i.e., the notion of similarity) performbetter than methods relying simply on co-occurrence distributions.
To summarize, theagreement values in Table 9 suggest that methods inducing similarity relationshipsfrom corpus co-occurrence statistics are not necessarily incompatible with methods thatquantify similarity using manually crafted taxonomies and that different smoothingtechniques may be appropriate for different tasks.Table 10 shows how the different methods compare for the task of predictingthe individual argument relations for the training and test sets.
A general observa-tion is that all methods are fairly good at predicting object relations.
Predicting sub-ject relations is considerably harder: no method exceeds an accuracy of 54.9% (seeTable 10).
One explanation for this is that selectional constraints imposed on sub-jects can be more easily overridden by pragmatic and contextual factors than thoseimposed on objects.
Furthermore, selectional constraints on subjects are normallyweaker than on objects.
J is particularly good at predicting object relations, whereasPC yields the best performance when it comes to predicting subject relations (seeTable 10).5.4 Experiment 3: Using Ripper to Disambiguate NominalizationsAn obvious question is whether a better performance can be achieved by combiningthe five smoothing variants, given that they seem to provide complementary infor-mation for predicting argument relations.
For example, Wn , Ro, and PC are relativelygood for the prediction of subject relations , whereas J is best for the prediction of ob-ject relations (see Table 10).
Furthermore, note that the probabilistic model introducedin Section 2 and the algorithm based on it (see Section 4) ignore contextual informa-tion that can provide important cues for disambiguating nominalizations.
Consider thenominalization government promotion in (23a), which was assigned an object (insteadof a subject) interpretation by all smoothing variants except Wn.
Contextual informa-tion could help assign the correct interpretation in cases in which the head of thecompound is followed by prepositions such as of (see (23a)) or into (see (23b)).375Lapata The Disambiguation of Nominalizations(23) a.
It was not felt necessary to take account of government promotion ofunionism.b.
But politicians are calling for the Republic?s Government to start aCourt inquiry into Ross?
alleged links with firms in Eire.In the following we first examine whether combination of the five smoothing vari-ants improves performance at predicting the argument relations for nominalizations(see Section 5.4.1).
We then proceed to study the influence of context on the inter-pretation task; we explore the contribution of context alone (see Section 5.4.2) and incombination with the different smoothing variants (see Section 5.4.3).
The differentinformation sources are combined using Ripper (Cohen 1996), a system that inducesclassification rules from a set of preclassified examples.
Ripper takes as input theclasses to be learned (in our case the classification is binary, i.e., subject or object), thenames and possible values of a set of features, and training data specifying the classand feature values for each training example.
In our experiments the features are thesmoothing variants and the tokens surrounding the nominalizations in question.
Thefeature vector in (24a) represents the individual predictions of B, Wn , Ro, J, and PCfor the interpretation of government promotion (see (23a)).
We encode the context sur-rounding nominalizations using two distinct representations: (a) parts of speech and(b) lemmas.
In both cases we encode the position of the tokens with respect to thenominalization in question.
The feature vector in (24b) consists of the nominalizationcourt inquiry (see (23b)), represented by its parts of speech (nn1 and nn1, respectively)and a context of five words to its right and five words to its left, also reduced to theirparts of speech.
In (24c) the same tokens are represented by their lemmas.
(24) a.
[obj, subj, obj, obj, obj]b.
[pos, nn0, to0, vvi, aj0, nn1, nn1, prp, pos, aj0, nn2, prp]c. [?s government to start a court inquiry into Ross ?s alleged link]Ripper is trained on vectors of values like the ones presented in (24) and out-puts a classification model for classifying future examples.
The model is learned usinggreedy search guided by an information gain metric and is expressed as an orderedset of if-then rules.
For our experiments Ripper was trained on the 596 nominaliza-tions on which the smoothing methods were compared and tested on the 200 unseennominalizations for which the interjudge agreement was previously calculated (seeSection 5.2).5.4.1 Combination of Smoothing Variants.
Table 11 shows Ripper?s performancewhen different combinations of smoothing variants (i.e., features) are used withouttaking context into account.
All results in Table 11 were obtained using the version ofthe interpretation algorithm that takes suffix semantics into account (see Section 5.3).As shown in Table 11, the combination of all five smoothing variants achieves a per-formance of 80.4%.2 Table 11 further reports the accuracy achieved when removing2 An anonymous reviewer pointed out that suffix information could be alternatively exploited byincluding the ending suffix of the nominalization head as an additional feature for the classificationtask.
The latter approach yields comparable performance to our original idea of defaulting to theargument structure denoted by the nominalization suffix.
When B, J, PC, Ro , and Wn are used asfeatures together with nominalization suffixes (-age, -ion, -ment, etc.
), Ripper?s performance is 79.9%?
1.65 on the training data and 80.3% ?
2.95% on the test data.376Computational Linguistics Volume 28, Number 3Table 11Disambiguation performance using thesmoothing variants as features.Features Train (%) Test (%)D 59.0 ?
2.01 61.5 ?
3.50B, J, PC, Ro, Wn 80.2 ?
1.63 80.4 ?
2.86B, J, PC, Wn 80.2 ?
1.68 80.4 ?
2.88B, J, PC, Ro 78.5 ?
1.68 79.9 ?
2.88B, J, Wn, Ro 80.7 ?
1.62 79.9 ?
2.88J, PC, Ro, Wn 80.7 ?
1.62 78.4 ?
2.96B, PC, Wn, Ro 79.8 ?
1.64 74.7 ?
3.13Table 12Ripper?s performance at predictingargument relations.Train (%) Test (%)Features subj obj subj objB, J, PC, Ro, Wn 66.5 89.7 73.2 84.6B, J, PC, Wn 66.5 89.7 73.2 84.6B, J, Wn, Ro 71.4 87.2 78.9 80.5B, J, PC, Ro 71.4 87.2 78.9 80.5J, PC, Ro, Wn 69.4 88.6 71.8 82.1B, PC, Wn, Ro 63.3 91.5 50.7 88.6a single feature.
Evaluation on subsets of features allows us to explore the contribu-tion of individual features to the classification task by comparing the subsets to thefull feature set.
We see that removal of Ro has no effect on the results, whereas re-moval of J produces a 5.7% performance decrease.
Removing Wn or PC yields thesame decrease in performance (i.e., 0.5%).
This is not surprising, since PC and Wntend to agree in their assignments of subject and object relations (see the methods?agreement in Table 9), and therefore their combination is not expected to be very in-formative.
Absence of J from the feature set yields the most dramatic performancedecrease.
This is not unexpected, given that J is the best predictor for object relationsand that PC and WordNet behave similarly with respect to their interpretation deci-sions.
In general we observe that the combination of smoothing variants outperformstheir individual performances (compare Tables 11 and 8).
Comparison of Ripper?sbest performance (80.4%) against the individual smoothing methods reveals a 10.8%accuracy increase over B, J, and Ro, a 4.1% increase over PC, and a 6.2% increaseover Wn .We further analyzed Ripper?s performance at predicting object and subject rela-tions.
This information is displayed in Table 12, in which we show how performancevaries on the full set of n size features (i.e., five) and each of its n?1 size subsets.
As canbe seen in Table 12, accuracy at predicting subject relations increases when smoothingvariants are combined (compare Tables 12 and 10).
In fact, combination of B, J, Wn ,and Ro (or B, J, PC, and Ro) performs best at predicting subject relations, achievingan increase of 24% over PC, the best individual predictor for subject relations (see Ta-ble 10).
In sum, our results show that combination of the different smoothing variants(using Ripper) achieves better results than each individual method.
Our overall perfor-mance (i.e., 80.4%) outperforms the default baseline significantly, by 18.9% (?2 = 17.33,p < .05) and is 9.3% lower than the upper bound established in our agreement study(see Section 5.2).
In what follows we first examine the independent contribution ofcontext to the disambiguation performance and then turn to its combination with ourfive smoothing variants.5.4.2 The Contribution of Context.
We evaluated the influence of context by varyingboth the position and the size of the window of tokens (i.e., lemmas or parts of speech)surrounding the nominalization.
We varied the window size parameter between oneand five words before and after the nominalization target.
We use the symbols l andr for left and right context, respectively, subscripts to denote the context encoding(i.e., lemmas or parts of speech), and numbers to express the size of the window377Lapata The Disambiguation of NominalizationsTable 13Disambiguation performance using rightcontext encoded as lemmas.Features Train (%) Test (%)D 59.0 ?
2.01 61.5 ?
3.50rl = 1 70.8 ?
1.86 68.0 ?
3.36rl = 2 70.1 ?
1.88 68.6 ?
3.34rl = 3 68.8 ?
1.90 67.5 ?
3.37rl = 4 68.8 ?
1.90 67.5 ?
3.37rl = 5 68.8 ?
1.90 67.5 ?
3.37Table 14Disambiguation performance using leftcontent encoded as lemmas.Features Train (%) Test (%)D 59.0 ?
2.01 61.5 ?
3.50ll = 1 66.9 ?
1.93 64.9 ?
3.43ll = 2 70.5 ?
1.87 67.5 ?
3.37ll = 3 70.6 ?
1.87 67.0 ?
3.83ll = 4 67.8 ?
1.92 65.5 ?
3.42ll = 5 65.3 ?
1.95 63.9 ?
3.46Table 15Disambiguation performance using rightcontext encoded as POS tags.Features Train (%) Test (%)D 59.0 ?
2.01 61.5 ?
3.50rp = 1 64.9 ?
1.96 65.5 ?
3.42rp = 2 65.8 ?
1.95 62.4 ?
3.49rp = 3 64.4 ?
1.96 63.4 ?
3.47rp = 4 65.3 ?
1.95 63.4 ?
3.47rp = 5 65.9 ?
1.94 62.9 ?
3.48Table 16Disambiguation performance using leftcontent encoded as POS tags.Features Train (%) Test (%)D 59.0 ?
2.01 61.5 ?
3.50lp = 1 63.9 ?
1.97 66.0 ?
3.41lp = 2 68.1 ?
1.91 64.4 ?
3.45lp = 3 67.1 ?
1.93 66.5 ?
3.40lp = 4 65.6 ?
1.95 65.0 ?
3.43lp = 5 66.6 ?
1.93 61.9 ?
3.50surrounding the candidate compound.
For example, ll = 5 represents a window offive tokens, encoded as lemmas, to the left of the candidate compound.Tables 13 and 14 show the influence of right and left context, respectively, repre-sented as lemmas.
The best peformances are achieved with a window of two wordsto the right or left of the candidate nominalization (see the features rl = 2 and ll = 2 inTables 13 and 14, respectively).
Combination of the best left and right features (rl = 2,ll = 2) does not increase the disambiguation performance (70.4% ?
1.86% on the train-ing and 66.5% ?
3.41% on the test data).
Note that the disambiguation performancesimply using contextual features is not considerably worse than the performance ofsome smoothing variants (see Table 7).
Contextual features encoded as lemmas out-perform part-of-speech (POS) tags, for which the best performance is achieved with awindow of one token to the right or a window of three tokens to the left of the can-didate nominalization (see Tables 15 and 16).
As in the case of lemmas, combinationof the best left and right features (rp = 1, lp = 3) does not yield better results (66.3% ?1.94% on the training data and 66.5% ?
3.40% on the test data).
The lower performanceof POS tags is not entirely unexpected: lemmas capture lexical dependencies that aresomewhat lost when a more general level of representation is introduced.
For example,Ripper assigns a subject interpretation when for immediately follows a nominalizationhead (e.g., staff requirement for reconnaissance).
This rule cannot be induced when for isrepresented by its part of speech (e.g., PRP), as there are a number of prepositionsthat can follow the nominalization head, but only a few of them provide cues for itsargument structure.Table 17 shows the performance of the best contextual features for the task ofpredicting the individual argument relations.
The contextual features are consistentlybetter at predicting object than subject relations.
This is not surprising, given that ob-378Computational Linguistics Volume 28, Number 3Table 17Performance at predicting argument relations using context.Train (%) Test (%)Methods subj obj subj objrl = 2 28.0 99.2 20.8 96.7ll = 2 36.2 94.1 13.8 97.5lp = 3 33.7 90.1 29.1 88.5rp = 1 22.6 94.1 20.8 91.8ject relations represent the majority in both the training and test data; furthermore,identifying superficial features that are good predictors for subject relations is a rel-atively hard task.
For example, even though Ripper identifies prepositions (e.g., of,to) following the nominalization head and certain frequent nominalization heads (e.g.,behavior) as indicators of subject relations, it has no means of guessing the transitivityof deverbal heads in the absense of syntactic cues.
Consider example (25a), in whichneither left nor right context is informative with regard to the fact that intervene isintransitive.Finally, there are some cases in which the syntactic cues can be misleading, asadjacency to the nominalization target does not necessarily indicate argument struc-ture.
This is shown in (25b), in which youth is classified as the subject of manager.Although on the surface youth manager at is analogous to nominalizations followedby of (e.g., government promotion of), the prepositional phrase at Wimbledon in (25b) issimply locative and not the argument of manager.
(25) a.
If the second reminder produces no result or the reply to eitherreminder seems to indicate the need for court intervention the matterwill be referred to a master or district judge.b.
He was youth manager at Wimbledon when I held a similar positionat Palace.5.4.3 Combination of Context with Smoothing Variants.
In this section we investi-gate whether the combination of surface contextual features with the predictions ofthe different smoothing methods has an effect on the disambiguation performance.Although context is good at predicting object relations, it performs poorly at guessingsubject relations (see Table 17).
We expect the combination of context with smoothingvariants (some of which, e.g., Wn , Ro, and PC, perform relatively well at the predictingsubject relations) to improve performance.
Recall that the probabilistic model intro-duced in Section 2.1 and the interpretation algorithm that makes use of it attempt theinterpretation of nominalizations without taking contextual cues into account.
Here,we examine how well the different smoothing variants perform in the presence ofcontextual information.
Table 18 shows Ripper?s performance when the best context(i.e., rl = 2) is combined with a single smoothing method and with all five variants.For the smoothing variants, we used the version of the interpretation algorithm thattakes suffix semantics into account (see Table 8).Comparison between Tables 8 and 18 reveals that the inclusion of context generallyincreases performance.
Combination of B with the best context yields a 6.7% increaseover B; an increase of 8.8% (over J) and 7.7% (over Ro) is observed when J and Roare combined with context, respectively.
No increase in performance is observed when379Lapata The Disambiguation of NominalizationsTable 18Disambiguation performance using context and smoothing variants.Methods Train (%) Test (%)D 59.0 ?
2.01 61.5 ?
3.50rl = 2, B 78.2 ?
1.69 76.3 ?
3.06rl = 2, PC 75.0 ?
1.78 76.3 ?
3.06rl = 2, J 81.5 ?
1.59 78.4 ?
2.96rl = 2, Wn 88.9 ?
1.29 86.1 ?
2.49rl = 2, Ro 78.5 ?
1.68 77.3 ?
3.00B, J, PC, Ro, Wn, rl = 2 84.4 ?
1.49 85.1 ?
2.57Table 19Argument relations using context and smoothing variants.Train (%) Test (%)Methods subj obj subj objrl = 2, B 69.9 83.6 61.3 85.7rl = 2, PC 63.9 82.2 54.9 88.6rl = 2, J 72.9 87.2 66.7 85.7rl = 2, Wn 87.3 90.0 74.7 93.3rl = 2, Ro 69.1 84.7 64.0 85.7B, J, PC, Ro, Wn, rl = 2 75.0 90.6 72.0 93.3context is combined with PC (see Table 18), whereas combination of Wn with contextyields a 11.9% increase over Wn alone.
Combining all five smoothing variants withcontext yields an increase of 4.7% over just the combination of B, J, PC, Ro, and Wn(see Table 12).
Our best performance (i.e., 86.1%) is achieved when Wn is combinedwith right context (rl = 2); this performance is significantly better than the simplestrategy of always defaulting to a subject classification, which yields an accuracy of61.5% (?2 = 30.64, p < .05), and only 3.6% lower than the upper bound of 89.7%.As shown in Table 19, the inclusion of context increases accuracy when it comes tothe prediction of subject relations (with the exception of PC, which is relatively goodat predicting subject relations, and therefore in that case the inclusion of context doesnot add much useful information).
The combination of Wn with rl = 2 achieves thehighest accuracy (87.3%) at predicting subject relations.6.
DiscussionWe have described an empirical approach for the automatic interpretation of nominal-izations.
We cast the interpretation task as a disambiguation problem and proposeda statistical model for inferring the argument relations holding between a deverbalhead and its modifier.
Our experiments revealed that the interpretation task suffersfrom data sparseness: even an approximation that maps the nominalized head to itsunderlying verb does not provide sufficient evidence for quantifying the argumentrelation of a modifier noun and its nominalized head.We showed how the argument relations (which are not readily available in the cor-pus) can be retrieved by using partial parsing and smoothing techniques that exploit380Computational Linguistics Volume 28, Number 3distributional and taxonomic information.
We compensated for the lack of sufficientdistributional information using either methods that directly recreate the frequenciesof word combinations or contextual features whose distribution in the corpus indi-rectly provides information about nominalizations.
We compared and contrasted avariety of smoothing approaches proposed in the literature and demonstrated thattheir combination yields satisfactory results for the demanding task of semantic dis-ambiguation.
We also explored the contribution of context and showed that it is use-ful for the disambiguation task.
Our approach is applicable to domain-independentunrestricted text and does not require the hand coding of semantic information.
Inthe following sections we discuss our results and their potential usefulness for NLPapplications.
We also address the limitations of our approach and sketch potentialextensions.6.1 The Interpretation of NominalizationsOur results indicate that a simple probabilistic model that uses smoothed counts (seethe interpretation algorithm in Section 4) yields a significant increase over the base-line without taking context into account.
Distance-weighted smoothing using PC andclass-based smoothing using WordNet achieve the best results (76.3% and 74.2%, re-spectively).
Combination of different smoothing methods (using Ripper) yields anoverall performance of 80.4%, again without taking context into consideration.
Con-text alone achieves a disambiguation performance of 68.6%, approximating the per-formance of some of the smoothing variants (see Tables 9 and 13).
This result suggeststhat simple features that can be easily retrieved and estimated from the corpus containenough information to capture generalizations about the behavior of nominalizations.As expected, the combination of smoothed probabilities with context outperforms theaccuracy of individual smoothing variants.
The combination of WordNet with a rightcontext of size two achieves an accuracy of 86.1%, compared to an upper bound forthe task (i.e., intersubject agreement) of 89.7%.
This is an important result consideringthe simplifications in the system and the sparse data problems encountered in theestimation of the model probabilities.
The second-best performance is achieved whenJ is combined with context (78.4%; see Table 18).
This result shows that informationinherent in the corpus can make up for the lack of distributional evidence and further-more that it is possible to extract semantic information from corpora (even if they arenot semantically annotated in any way) without recourse to pre-existing taxonomiessuch as WordNet.6.2 Limitations and ExtensionsTo a certain extent the difficulty of interpreting nominalizations is due to their contextdependence.
Although the approach presented in the previous sections takes immedi-ate context into account, it does so in a shallow manner, without having access to themeaning of the words surrounding the nominalization target, their syntactic depen-dencies, or the general discourse context within which the compound is embedded.Consider example (26a), in which the compound computer guidance receives a subjectinterpretation (e.g., the computer guides the chef).
Our approach cannot detect that thecomputer here is ascribed animate qualities and opts for the most likely interpretation(i.e., an object analysis).
In some cases the modifier stands in a metonymic relation toits head.
Consider the examples in sentences (26b, 26c), in which the nominalizationsindustry reception and market acceptance can be thought of as instances of the metonymicschema ?whole for part?
(Lakoff and Johnson 1980).
In example (26b) it is the indus-try as a whole that receives the guests rather than lasmo, which is one of its parts,381Lapata The Disambiguation of Nominalizationswhereas in (26c) the modifier market in market acceptance refers to the opinion leaders,who are part of the market.
(26) a.
Of course, none of this means that the equipment is taking anythingaway from the chef?s own individual skills which are irreplaceable.What it does ensure is that the chef has complete control over some ofthe most vital tools of his trade, with computer guidance as animportant aid.b.
The final evening saw more than 300 guests attend an industryreception, hosted by lasmo.c.
Marketers interested in the development and introduction of newproducts will be particularly interested in the attitude of opinionleaders to these products, for their general market acceptance canbe slowed down or speeded up by the views of such people.Consider now sentence (27a).
The nominalization student briefing is ambiguous,even though it is presented within its immediate context.
Taking more context into ac-count (see (27a)) does not provide enough disambiguation information either, althoughperhaps it introduces a slight bias in favor of an object interpretation (i.e., someone isbriefing the students).
For this particular example, we would have to know what thedocument within which student briefing occurs is about (i.e., a list of teaching guide-lines for university lecturers).
The sentences in (27) are taken from a document sectionentitled ?Work Experience?
that emphasizes the importance of work experience forstudents.
Given all this background information, it becomes apparent that it is not thestudents who are doing the briefing in (27b).
(27) a.
Explain to both students and organisations the role of work experiencein personal development and its part in the planned programme.b.
Provide comprehensive guidelines on the work experience whichincludes a student briefing, an employer briefing and a student workchecklist.The observation that discourse or pragmatic context may influence interpretationsis by no means new or particular to nominalisations.
Sparck Jones (1983) observes thata variety of factors can potentially influence the interpretation of compound nouns ingeneral.
These factors range from syntactic analysis (e.g., to arrive at an interpretationof the compound onion tears, it is necessary to identify that tears is a noun and not thethird-person singular of the verb tear) to semantic information (e.g., for interpretingonion tears, it is important to know that onions cannot be tears or that tears are notmade of onions) and pragmatic information.
Pragmatic inference may be called for incases in which syntactic or semantic information is straightforwardly supplied, evenwhere the local text context provides rich information bearing on the interpretation ofthe compound.
Copestake and Lascarides (1997) and Lascarides and Copestake (1998)make the same observation for a variety of constructions such as compound nouns,adjective-noun combinations and verb-argument relations.
Consider the sentences in(28)?(30).
The discourse in (28) favors the interpretation ?bag for cotton clothes?
forcotton bag over the more likely interpretation ?bag made of cotton.?
Although fastprogrammer is typically a programmer who programs fast, when the adjective-nouncombination is embedded in a context like (29a, 29b), the less likely meaning ?a382Computational Linguistics Volume 28, Number 3programmer who runs fast?
is triggered.
Finally, although it is more likely to enjoyreading a book rather than eating it, the context in (30) triggers the latter interpreta-tion.
(28) a. Mary sorted her clothes into various bags made from plastic.b.
She put her skirt into the cotton bag.
(29) a.
All the office personnel took part in the company sports day last week.b.
One of the programmers was a good athlete, but the other wasstruggling to finish the courses.c.
The fast programmer came first in the 100m.
(30) a.
My goat eats anything.b.
He really enjoyed your book.Pragmatic context may be particularly important for the interpretation of com-pound nouns.
Because compounds can be used as a text compression device (Marsh1984), it is plausible that pragmatic inference is required to supply the compound?s in-terpretation.
This observation is somewhat supported by our interannotator agreementexperiment (see Section 5.2).
Even though our participants were provided with somecontext, the agreement among them was not complete (they reached a K of .78, whenabsolute agreement is 1).
Although our approach takes explicit contextual informationinto account, it is agnostic to discourse or pragmatic information.
Encoding pragmaticinformation would involve considerable manual effort.
Furthermore, a hypotheticalstatistical learner that takes pragmatic information into account would have not onlyto deal with data sparseness but furthermore to detect cases in which conflicts arisebetween discourse information and the likelihood of a given interpretation.Our experiments focused on nominalizations derived from verbs specifically sub-categorizing for direct objects.
Although nominalizations whose verbs take prepo-sitional frames (e.g., oil painting, soccer competition) represent a small fraction of thenominalizations found in the corpus (9.2%), a more general approach would haveto take those verbs into account.
This task is harder than interpreting direct objects,since to estimate the frequency f (vn2 , rel , n1), one needs first to determine with somedegree of accuracy the attachment site of the prepositional phrase.
Taking into accountprepositional phrases and their attachment sites can also be useful for the interpreta-tion of compounds other than nominalizations.
Consider the compound noun pet sprayfrom (1).
Assuming that pet spray can be ?spray for pets,?
?spray in pets,?
?spray aboutpets,?
or ?spray from pets,?
we can derive the most likely interpretation by looking atwhich types of prepositional phrases (e.g., for pets, about pets) are most likely to attachto spray.
Note that in cases in which the expressions spray for pets and spray in pets arenot attested in the corpus, their respective co-occurrence frequencies can be re-createdusing the techniques presented in Section 3.Finally, the approach advocated here can be straightforwardly extended to nomi-nalizations with adjectival modifiers (e.g., parental refusal; see the examples in (2)).
Inmost cases the adjective in question is derived from a noun, and any inference processon the argument relations between the head noun and the adjectival modifier couldtake advantage of this information.383Lapata The Disambiguation of Nominalizations6.3 Relevance for NLP ApplicationsRobust semantic ambiguity resolution is challenging for current NLP systems.
Al-though general-purpose taxonomies like WordNet or Roget?s thesaurus are useful forcertain interpretation tasks, such resources are not exhaustive or generally available forlanguages other than English.
Furthermore, the compound noun interpretation taskinvolves acquiring semantic information that is linguistically implicit and thereforenot directly available in corpora or taxonomic resources.
Indeed, interpreting com-pound nouns is often analyzed in the linguistics literature in terms of (impractical)general-purpose reasoning with pragmatic information such as real-world knowledge(e.g., Hobbs et al 1993; see Section 7 for details).
We show that it is feasible to learnimplicit semantic information automatically from the corpus by utilizing linguisticallyprincipled approximations, surface syntactic cues, and (when available) taxonomicinformation.The interpretation of compound nouns is important for several NLP tasks, notablymachine translation.
Consider the nominalization satellite observation (taken from (4a)),which may mean ?observation by satellite?
or ?observation of satellites.?
To translatesatellite observation into Spanish, we have to work out whether satellite is the subject orobject of the verb observe.
In the first case satellite observation translates as observacio?npor satelite (observation by satellite), whereas in the latter it translates as observacio?n desatelites (observation of satellites).
In this case the implicit linguistic information hasto be retrieved and disambiguated to obtain a meaningful translation.
Informationretrieval is another relevant application in which again the underlying meaning mustbe rendered explicit.
Consider a search engine faced with the query cancer treatment.Presumably one would not like to obtain information about cancer or treatment ingeneral, but about methods or medicines that help treat cancer.
So knowledge aboutthe fact that cancer is the object of treatment could help rank relevant documents (i.e.,documents in which cancer is the object of the verb treat) before nonrelevant ones orrestrict the number of retrieved documents.7.
Related WorkIn this section we review previous work on the interpretation of compound nouns andcompare it to our own work.
Despite the differences among them, most approachesrequire large amounts of hand-crafted knowledge, place emphasis on the recovery ofrelations other than nominalizations (see the examples in (1)), contain no quantitativeevaluation (the exceptions are Leonard (1984), Vanderwende (1994), and Lauer (1995)),and generally assume that context dependence is either negligible or of little impact.Most symbolic approaches are limited to a specific domain because of the large effortinvolved in hand-coding semantic information and are distinguished in two maintypes: concept-based and rule-based.Under the concept-based approach, each noun in the compound is associated witha concept and various slots.
Compound interpretation reduces to slot filling, that is,evaluating how appropriate concepts are as fillers of particular slots.
A scoring sys-tem evaluates each possible interpretation and selects the highest-scoring analysis.Examples of the approach are Finin (1980) and McDonald (1982).
As no qualitativeevaluation is reported in these studies, it is difficult to assess how their methods per-form, although it is clear that considerable effort needs to be invested in the encodingof the appropriate semantic knowledge.Under the rule-based approach, interpretation is performed by sequential ruleapplication.
A fixed set of rules is applied in a fixed order, and the first rule that issemantically compatible with the nouns forming the compound results in the most384Computational Linguistics Volume 28, Number 3plausible interpretation.
The approach was introduced by Leonard (1984), was basedon a hand-crafted lexicon, and achieved an accuracy of 76.0% (on the training set).Vanderwende (1994) further developed a rule-based algorithm that does not rely ona hand-crafted lexicon but extracts the required semantic information from an on-linedictionary instead.
The system achieved an accuracy of 52.0%.A variant of the concept-based approach uses unification to constrain the seman-tic relations between nouns represented as feature structures.
Jones (1995) used atyped graph?based unification formalism and default inheritance to specify featuresfor nouns whose combination results in different interpretations.
Again no evaluationis reported, although Jones points out that ambiguity can be a problem, as all possibleinterpretations are produced for a given compound.
Wu (1993) provides a statisticalframework for the unification-based approach and develops an algorithm for approx-imating the probabilities of different possible interpretations using the maximum-entropy principle.
No evaluation of the algorithm?s performance is given.
The ap-proach remains knowledge intensive, however, as it requires manual construction ofthe feature structures.Lauer (1995) provides a probabilistic model of compound noun paraphrasing(e.g., state laws are ?the laws of the state,?
war story is ?a story about war?)
that assignsprobabilities to different paraphrases using a corpus in conjunction with Roget?s the-saurus.
Lauer does not address the interpretation of nominalizations or compoundswith hyponymic relations (see example (1e)) and takes into account only prepositionalparaphrases of compounds (e.g., of, for, in, at, etc.).
Lauer?s model makes predictionsabout the meaning of compound nouns on the basis of observations about preposi-tional phrases.
The model combines the probability of the modifier given a certainpreposition with the probability of the head given the same preposition and assumesthat these two probabilities are independent.Consider, for instance, the compound war story.
To derive the intended interpre-tation (i.e., ?story about war?
), the model takes into account the frequency of storyabout and about war.
For the modifier and head noun are substituted the concepts withwhich they are represented in Roget?s thesaurus, and the frequency of a concept and apreposition is calculated accordingly (see Section 3.2).
Lauer?s (1995) model achievesan accuracy of 47.0%.
The result is difficult to interpret, given that no experimentswith humans are performed and therefore the optimal performance on the task is un-known.
Lauer acknowledges that data sparseness can be a problem for the estimationof the model parameters and also that the assumption of independence between thehead and its modifier is unrealistic and leads to errors in some cases.Although it is generally acknowledged that context, both intra- and intersentential,may influence the interpretation task, contextual factors are typically ignored, with theexception of Hobbs et al (1993), who propose that the interpretation of a compoundcan be achieved via abductive inference.
To interpret a compound one must provethe logical form of its constituent parts from what is mutually known.
The amountof world knowledge required to work out what is mutually known, however, renderssuch an approach infeasible in practice.
Furthermore, Hobbs et al?s approach doesnot capture linguistic constraints on compound noun formation and as a result cannotpredict that a noun-noun sequence like cancer lung (under the interpretation ?cancerin the lung?)
is odd.Unlike previous work, we did not attempt to recover the semantic relations holdingbetween a head and its modifier (see (1)).
Instead, we focused on the less ambitioustask of interpreting nominalizations, that is, compounds whose heads are derivedfrom a verb and whose modifiers are interpreted as its arguments.
Similarly to Lauer(1995), we have proposed a simple probabilistic model that uses information about385Lapata The Disambiguation of Nominalizationsthe distributional properties of words and domain-independent symbolic knowledge(i.e., WordNet, Roget?s thesaurus).
Unlike Lauer, we have addressed the sparse-dataproblem by directly comparing and contrasting a variety of smoothing approaches pro-posed in the literature and have shown that these methods yield satisfactory resultsfor the demanding task of semantic disambiguation.
Furthermore, we have shownthat the combination of different sources of taxonomic and nontaxonomic information(using Ripper) is effective for tasks facing data sparseness.
In contrast to previousapproaches, we explored the effect of context on the interpretation task and showedthat its inclusion generally improves disambiguation performance.
We combined dif-ferent information sources (e.g., contextual features and smoothing variants) usingRipper.
Although the use of classifiers has been widespread in studies concerning dis-course segmentation (Passonneau and Litman 1997), the disambiguation of discoursecues (Siegel and McKeown 1994), the acquisition of lexical semantic classes (Merloand Stevenson 1999; Siegel 1999), the automatic identification of user corrections inspoken dialogue systems (Hirschberg, Litman, and Swerts 2001), and word sense dis-ambiguation (Pedersen 2001), the treatment of the interpretation of compound nounsas a classification task is, to our knowledge, novel.Our approach can be easily adapted to account for Lauer?s (1995) paraphras-ing task.
Instead of assuming that the probability of the compound modifier givena preposition is independent from the probability of the compound head given thesame preposition, a more straightforward model would take into account the jointprobability of the head, the preposition, and the modifier.
In cases in which a certainhead, preposition, and modifier combination is not attested in the corpus (e.g., storyabout war), the methodology put forward in Experiments 2 and 3 could be used tore-create its frequency (see also the discussion in Section 6).Unlike previous approaches, we provide an upper bound for the task.
Recall fromSection 5.2 that an experiment with humans was performed to evaluate whether thetask can be performed reliably.
In doing so we took context into account, and asa result we established a higher upper bound for the task than would have beenthe case if context was not taken into account.
Furthermore, it is not clear whethersubjects could arrive at consistent interpretations for nominalizations out of context.Downing?s (1977) experiments show that, when asked to interpret compounds out ofcontext, participants tend to come up with a variety of interpretations that are notalways compatible.
For example, for the compound bullet hole, the interpretations ?ahole made by a bullet,?
?a hole shaped like a bullet,?
?a fast-moving hole,?
?a holein which to hide bullets,?
and ?a hole into which to throw (bullet) casings?
wereprovided.8.
ConclusionsIn this article we presented work on the automatic interpretation of nominalizations(i.e., compounds whose heads are derived from a verb and whose modifiers are inter-preted as its arguments).
Nominalizations pose a challenge for empirical approaches,as the argument relations between a head and its modifier are not readily available in acorpus, and therefore they have to be somehow retrieved and approximated.
Approx-imating the nominalized head to its corresponding verb and estimating the frequencyof verb-noun relations instead of noun-noun relations accounts for only half of thenominalizations attested in the corpus.Our experiments revealed that data sparseness can be overcome by taking ad-vantage of smoothing methods and surface contextual information.
We have directlycompared and contrasted a variety of smoothing approaches proposed in the literature386Computational Linguistics Volume 28, Number 3and have shown that these methods yield satisfactory results for the demanding task ofsemantic disambiguation, especially when coupled with contextual information.
Ourexperiments have shown that contextual information that is easily obtainable froma corpus and computationally cheap is good at predicting object relations, whereasthe computationally more expensive smoothing variants are better at guessing subjectrelations.
Combination of context with smoothing variants yields better performanceover either context or smoothing alone.We combined different information sources (i.e., contextual features and smoothingvariants) using Ripper.
Although a considerable body of previous research has treatedseveral linguistic phenomena as classification tasks, the interpretation of compoundnouns has so far been based on the availability of symbolic knowledge.
We show thatthe application of probabilistic learning to the interpretation of compound nouns isnovel and promising.
Finally, our experiments revealed that information inherent inthe corpus can make up for the lack of distributional evidence by taking advantage ofsmoothing methods that rely simply on verb-argument tuples extracted from a largecorpus and surface contextual information without strictly presupposing the existenceof annotated data or taxonomic information.AcknowledgmentsThis work was supported by the AlexanderS.
Onassis Foundation and ESRC grantnumber R000237772 (Data IntensiveSemantics and Pragmatics).
Thanks to FrankKeller, Alex Lascarides, Scott McDonald,and three anonymous reviewers forvaluable comments.ReferencesAbney, Steve.
1996.
?Partial parsing viafinite-state cascades.?
In John Carroll,editor, Workshop on Robust Parsing,pages 8?15, Prague.
European SummerSchool in Logic, Language andInformation, Prague.Brown, Peter F., Vincent J. Della Pietra,Peter V. de Souza, and Robert L. Mercer.1992.
Class-based n-gram models ofnatural language.
Computational Linguistics18(4):467?479.Burnage, Gavin.
1990.
?Celex?A guide forusers.?
Technical Report, Centre forLexical Information, University ofNijmegan, Nijmegan, Netherlands.Burnard, Lou.
1995.
Users Guide for the BritishNational Corpus.
British National CorpusConsortium, Oxford UniversityComputing Service, Oxford.Carletta, Jean.
1996.
Agreement onclassification tasks: The Kappa statistic.Computational Linguistics 22(2):249?254.Carroll, John and Diana McCarthy.
2000.Word sense disambiguation usingautomatically acquired verbal preferences.Computers and the Humanities34(1/2):109?114.Church, Kenneth W. and William A. Gale.1991.
A comparison of the enhancedGood-Turing and deleted estimationmethods for estimating probabilities ofEnglish bigrams.
Computer Speech andLanguage 5:19?54.Clark, Stephen and David Weir.
2001.?Class-based probability estimation usinga semantic hierarchy.?
In Proceedings of theSecond North American Annual Meeting ofthe Association for Computational Linguistics,pages 95?102, Pittsburgh, Pennsylvania.Cohen, William W. 1996.
?Learning treesand rules with set-valued features.?
InProceedings of the 13th National Conference onArtificial Intelligence, pages 709?716,Portland, Oregon.
AAAI Press, MenloPark, California.Collins, Michael and James Brooks.
1995.?Prepositional phrase attachment througha backed-off model.?
In David Yarowskyand Kenneth W. Church, editors,Proceedings of the Third Workshop on VeryLarge Corpora, pages 27?38, Cambridge,Massachusetts.Copestake, Ann and Alex Lascarides.
1997.?Integrating symbolic and statisticalrepresentations: The lexicon pragmaticsinterface.?
In Proceedings of the 35th AnnualMeeting of the Association for ComputationalLinguistics, pages 136?143, Madrid, Spain.Dagan, Ido, Lilian Lee, and Fernando C. N.Pereira.
1999.
Similarity-based models ofword cooccurrence probabilities.
MachineLearning 34(1?3):43?69.Downing, Pamela.
1977.
On the creationand use of English compound nouns.Language 53(4):810?842.Essen, Ute and Volker Steinbiss.
1992.?Co-occurrence smoothing for stochastic387Lapata The Disambiguation of Nominalizationslanguage modeling.?
In Proceedings ofInternational Conference on Acoustics Speechand Signal Processing, volume 1,pages 161?164, San Francisco, California.Finin, Tim 1980.
?The semanticinterpretation of nominal compounds.?
InProceedings of First National Conference onArtificial Intelligence, pages 310?315,Stanford, California.
AAAI Press, MenloPark, California.Grishman, Ralph and John Sterling.
1994.?Generalizing automatically generatedselectional patterns.?
In Proceedings of the15th International Conference onComputational Linguistics, pages 742?747,Kyoto, Japan.Hindle, Donald and Mats Rooth.
1993.Structural ambiguity and lexical relations.Computational Linguistics 19(1):103?120.Hirschberg, Julia, Diane Litman, and MarcSwerts.
2001.
?Identifying user correctionsautomatically in spoken dialoguesystems.?
In Proceedings of the Second NorthAmerican Annual Meeting of the Associationfor Computational Linguistics,pages 208?215, Pittsburgh, Pennsylvania.Hobbs, Jerry R., Mark Stickel, DouglasAppelt, and Paul Martin.
1993.Interpretation as abduction.
Journal ofArtificial Intelligence 63(1?2):69?142.Isabelle, Pierre.
1984.
?Another look atnominal compounds.?
In Proceedings of the10th International Conference onComputational Linguistics and 22nd AnnualMeeting of the Association for ComputationalLinguistics, pages 509?516, Stanford,California.Jones, Bernard.
1995.
?Predicating nominalcompounds.?
In Proceedings of the 17thAnnual Conference of the Cognitive ScienceSociety, pages 130?135, Pittsburgh,Pennsylvania.Katz, Slava M. 1987.
Estimation ofprobabilities from sparse data for thelanguage model component of a speechrecognizer.
IEEE Transactions on AcousticsSpeech and Signal Processing 33(3):400?401.Lakoff, George and Mark Johnson.
1980.Metaphors We Live By.
University ofChicago Press, Chicago.Lascarides, Alex and Alex Copestake.
1998.Pragmatics and word meaning.
Journal ofLinguistics 34(2):387?414.Lauer, Mark.
1995.
Designing StatisticalLanguage Learners: Experiments onCompound Nouns.
Ph.D. dissertation,Macquarie University, Sydney, Australia.Lee, Lilian.
1999.
?Measures ofdistributional similarity.?
In Proceedings ofthe 37th Annual Meeting of the Association forComputational Linguistics, pages 25?32,College Park, Maryland.Lee, Lilian and Fernando Pereira.
1999.?Distributional similarity models:Clustering vs. nearest neighbors.?
InProceedings of the 37th Annual Meeting of theAssociation for Computational Linguistics,pages 33?40, College Park, Maryland.Leonard, Rosemary.
1984.
The Interpretationof English Noun Sequences on the Computer.North-Holland, Amsterdam.Levi, Judith N. 1978.
The Syntax andSemantics of Complex Nominals.
AcademicPress, New York.Levin, Beth.
1993.
English Verb Classes andAlternations: A Preliminary Investigation.University of Chicago Press, Chicago.Li, Hang and Naoki Abe.
1998.
Generalizingcase frames using a thesaurus and theMDL principle.
Computational Linguistics24(2):217?244.Macleod, Catherine, Ralph Grishman,Adam Meyers, Leslie Barrett, and RuthReeves.
1998.
?Nomlex: A lexicon ofnominalizations.?
In Proceedings of theEighth International Congress of the EuropeanAssociation for Lexicography, pages 187?193,Lie`ge, Belgium.Marsh, Elaine.
1984.
?A computationalanalysis of complex noun phrases inNavy messages.?
In Proceedings of the 10thInternational Conference on ComputationalLinguistics, pages 505?508, Stanford,California.McDonald, David.
1982.
UnderstandingNoun Compounds.
Ph.D. dissertation,Carnegie Mellon University, Pittsburgh,Pennsylvania.Merlo, Paola and Suzanne Stevenson.
1999.?Automatic verb classification usingdistributions of grammatical features.?
InProceedings of the Ninth Conference of theEuropean Chapter of the Association forComputational Linguistics, pages 45?51,Bergen, Norway.Miller, George A., Richard Beckwith,Christiane Fellbaum, Derek Gross, andKatherine J. Miller.
1990.
Introduction toWordNet: An on-line lexical database.International Journal of Lexicography3(4):235?244.Mosteller, Frederick and David L. Wallace.1964.
Inference and Disputed Authorship: TheFederalist.
Addison-Wesley, London.Passonneau, Rebecca J. and Diane J. Litman.1997.
Discourse segmentation by humanand automated means.
ComputationalLinguistics 23(1):103?140.Pedersen, Ted.
2001.
?A decision tree ofbigrams is an accurate predictor of wordsense.?
In Proceedings of the Second NorthAmerican Annual Meeting of the Association388Computational Linguistics Volume 28, Number 3for Computational Linguistics, pages 79?86,Pittsburgh, Pennsylvania.Pereira, Fernando, Naftali Tishby, andLillian Lee.
1993.
?Distributionalclustering of English words.?
InProceedings of the 31st Annual Meeting of theAssociation for Computational Linguistics,pages 183?190, Columbus, Ohio.Quirk, Randolph, Sidney Greenbaum,Geoffrey Leech, and Jan Svartvik.
1985.
AComprehensive Grammar of the EnglishLanguage.
Longman, London.Resnik, Philip Stuart.
1993.
Selection andInformation: A Class-Based Approach toLexical Relationships.
Ph.D. dissertation,University of Pennsylvania, Philadelphia.Selkirk, Elizabeth.
1982.
The Syntax of Words.MIT Press, Cambridge, Massachusetts.Siegel, Eric V. 1999.
?Corpus-based linguisticindicators for aspectual classification.?
InProceedings of the 37th Annual Meeting of theAssociation for Computational Linguistics,pages 112?119, College Park, Maryland.Siegel, Eric V. and Kathleen R. McKeown.1994.
?Emergent linguistic rules frominducing decision trees: Disambiguatingdiscourse clue words.?
In Proceedings of the12th National Conference on ArtificialIntelligence, pages 820?826, Seattle,Washington.
AAAI Press, Menlo Park,California.Siegel, Sidney and N. John Castellan.
1988.Nonparametric Statistics for the BehavioralSciences.
McGraw-Hill, New York.Sparck Jones, Karen.
1983.
?Compoundnoun interpretation problems.?
TechnicalReport 45, Computer Laboratory,Cambridge University, Cambridge,England.Vanderwende, Lucy.
1994.
?Algorithm forautomatic interpretation of nounsequences.?
In Proceedings of the 15thInternational Conference on ComputationalLinguistics, pages 782?788, Kyoto, Japan.Warren, Beatrice.
1978.
Semantic Patterns ofNoun-Noun Compounds.
Acta UniversitatisGothoburgensis, Go?teborg, Sweden.Wu, Dekai.
1993.
?Approximatingmaximum-entropy ratings for evidentialparsing and semantic interpretation.?
InProceedings of 13th International JointConference on Artificial Intelligence,pages 1290?1296, Chamberry, France.Morgan Kaufmann.
