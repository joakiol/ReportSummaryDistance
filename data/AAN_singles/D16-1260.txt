Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 2350?2354,Austin, Texas, November 1-5, 2016. c?2016 Association for Computational LinguisticsEncoding Temporal Information for Time-Aware Link PredictionTingsong Jiang, Tianyu Liu, Tao Ge, Lei Sha, Sujian Li, Baobao Chang and Zhifang SuiKey Laboratory of Computational Linguistics, Ministry of EducationSchool of Electronics Engineering and Computer Science, Peking UniversityCollaborative Innovation Center for Language Ability, Xuzhou 221009 China{tingsong,tianyu0421,taoge,shalei,lisujian,chbb,szf}@pku.edu.cnAbstractMost existing knowledge base (KB) embed-ding methods solely learn from time-unknownfact triples but neglect the temporal informa-tion in the knowledge base.
In this paper,we propose a novel time-aware KB embed-ding approach taking advantage of the hap-pening time of facts.
Specifically, we use tem-poral order constraints to model transforma-tion between time-sensitive relations and en-force the embeddings to be temporally consis-tent and more accurate.
We empirically eval-uate our approach in two tasks of link predic-tion and triple classification.
Experimental re-sults show that our method outperforms otherbaselines on the two tasks consistently.1 IntroductionKnowledge bases (KBs) such as Freebase (Bollackeret al, 2008) and YAGO (Fabian et al, 2007) play apivotal role in many NLP related applications.
KB-s consist of facts in the form of triplets (ei, r, ej),indicating that head entity ei and tail entity ej islinked by relation r. Although KBs are large, theyare far from complete.
Link prediction is to predictrelations between entities based on existing triplet-s, which can alleviate the incompleteness of cur-rent KBs.
Recently a promising approach for thistask called knowledge base embedding (Nickel etal., 2011; Bordes et al, 2011; Socher et al, 2013)aims to embed entities and relations into a continu-ous vector space while preserving certain informa-tion of the KB graph.
TransE (Bordes et al, 2013) isa typical model considering relation vector as trans-lating operations between head and tail vector, i.e.,ei + r ?
ej when (ei, r, ej) holds.Most existing KB embedding methods solelylearn from time-unknown facts but ignore the use-ful temporal information in the KB.
In fact, thereare many temporal facts (or events) in the KB, e.g.,(Obama, wasBornIn, Hawaii) happened at August4, 1961.
(Obama, presidentOf, USA) is true since2009.
Current KBs such as YAGO and Freebasestore such temporal information either directly orindirectly.
The happening time of time-sensitivefacts may indicate special temporal order of fact-s and time-sensitive relations.
For example, (Ein-stein, wasBornIn, Ulm) happened in 1879, (Einstein,wonPrize, Nobel Prize) happened in 1922, (Einstein,diedIn, Princeton) occurred in 1955.
We can inferthe temporal order of time-sensitive relations fromall such kinds of facts: wasBornIn ?
wonPrize ?diedIn.
Traditional KB embedding models such asTransE often confuse relations such as wasBornInand diedIn when predicting (person,?,location) be-cause TransE learns only from time-unknown factsand cannot distinguish relations with similar seman-tic meaning.
To make more accurate predictions, itis non-trivial for existing KB embedding methods toincorporate temporal order information.This paper mainly focuses on incorporating thetemporal order information and proposes a time-aware link prediction model.
A new temporal di-mension is added to fact triples, denoted as a quadru-ple: (ei, r, ej , tr), indicating the fact happened attime tr1.
To make the embedding space compati-1tr is the absolute beginning time when the fact is true, e.g.,?1961-08-04?
for (Obama, wasBornIn, Hawaii).2350ble with the observed triple in the fact dimension,relation vectors behave as translations between enti-ty vectors similarly as TransE models.
To incorpo-rate temporal order information between pair-wisetemporal facts, we assume that prior time-sensitiverelation vector can evolve into a subsequent time-sensitive relation vector through a temporal tran-sition.
For example, we have two temporal fact-s sharing the same head entity: (ei, ri, ej , t1) and(ei, rj , ek, t2) and the temporal order constraint t1<t2, i.e., ri happens before rj , then we propose theassumption that prior relation ri after temporal tran-sition should lie close to subsequent relation rj , i.e.,riM ?
rj , here matrix M captures the temporalorder information between relations.
In this way,both semantic and temporal information are embed-ded into a continuous vector space during learning.To the best of our knowledge, we are the first to con-sider such temporal information for KB embedding.We evaluate our approach on public availabledatasets and our method outperforms state-of-the-artmethods in the time-aware link prediction and tripleclassification tasks.2 Time-Aware KB EmbeddingTraditional KB embedding methods encode only ob-served fact triples but neglect temporal constraintsbetween time-sensitive entities and facts.
To dealwith this limitation, we introduce Time-Aware KBEmbedding which constrains the task by incorporat-ing temporal constraints.To consider the happening time of facts, we for-mulate a temporal order constraint as an optimiza-tion problem based on a manifold regularization ter-m.
Specially, temporal order of relations in time-sensitive facts should affect KB representation.
If riand rj share the same head entity ei, and ri occursbefore rj , then prior relation?s vector ri could evolveinto subsequent relation?s vector rj in the temporaldimension.To encode the transition between time-sensitiverelations, we define a transition matrix M ?
Rn?nbetween pair-wise temporal ordering relation pair(ri, rj).
Our optimization requires that positive tem-poral ordering relation pairs should have lower s-cores (energies) than negative pairs, so we define atemporal order score function asg(ri, rj) = ?riM?
rj?1, (1)which is expected to be a low score when the relationpair is in chronological order, and high otherwise.To make the embedding space compatible withthe observed triples, we make use of the triple set?
and follow the same strategy adopted in previousmethods such as TransE.f(ei, r, ej) = ?ei + r?
ej?1.
(2)For each candidate triple, it requires positive triplesto have lower scores than negative triples.The optimization is to minimize the joint scorefunction,L=?x+??
[ ?x????
[?1 + f(x+)?
f(x?)]++??y+?
?ei ,y???
?ei[?2 + g(y+)?
g(y?
)]+] (3)where x+ = (ei, ri, ej) ?
?
is the positive triple(quad), x?=(e?i, ri, e?j)???
is corresponding thenegative triple.
y+ = (ri, rj)?
?ei is the positiverelation ordering pair with respect to (ei, ri, ej , tx).It?s defined as?ei = {(ri, rj)|(ei, ri, ej , tx)???
,(ei, rj , ek, ty)???
, tx< ty},(4)where ri and rj share the same head entity ei, andy?
= (rj , ri) ?
?
?ei are the corresponding negativerelation order pairs by inverse.
In experiments, weenforce constrains as ?ei?2 ?
1, ?ri?2 ?
1, ?rj?
?1 and ?riM?2 ?
1.The first term in Equation (3) enforces the resul-tant embedding space compatible with all the ob-served triples, and the second term further requiresthe space to be temporally consistent and more accu-rate.
Hyperparameter ?
makes a trade-off betweenthe two cases.
Stochastic gradient descent (in mini-batch mode) is adopted to solve the minimizationproblem.3 ExperimentsWe adopt the same evaluation metrics for time-aware KB embedding in two tasks: link prediction(Bordes et al, 2013) and triple classification (Socheret al, 2013).2351Dataset #Rel #Ent #Train/#Valid/#Test #Trip.
#QuadYG15k 10 9513 13345/1320/1249 15914 15914YG36k 10 9513 29757/3252/3058 36067 15914Table 1: Statistics of data sets.3.1 DatasetsWe create two temporal datasets from YAGO2 (Hof-fart et al, 2013), consisting of time-sensitive facts.In YAGO2, MetaFacts contains all happening timefor facts.
DateFacts contains all creation time forentities.
First, to make a pure time-sensitive datasetwhere all facts have time annotations, we selectedthe subset of entities that have at least 2 mentions inMetaFacts and DateFacts.
This resulted in 15,914triples (quadruples) which were randomly split withthe ratio shown in Table 1.
This dataset is denotedYG15k.
Second, to make a mixed dataset, we createdYG36k where 50% facts have time annotations and50% do not.
We will release the data upon request.3.2 Link PredictionLink prediction is to complete the triple (h, r, t)when h, r or t is missing.3.2.1 Entity PredictionEvaluation protocol.
For each test triple withmissing head or tail entity, various methods are usedto compute the scores for all candidate entities andrank them in descending order.
We use two metric-s for our evaluation as in (Bordes et al, 2013): themean of correct entity ranks (Mean Rank) and theproportion of valid entities ranked in top-10 (Hit-s@10).
As mentioned in (Bordes et al, 2013), themetrics are desirable but flawed when a corruptedtriple exists in the KB.
As a countermeasure, we fil-ter out all these valid triples in the KB before rank-ing.
We name the first evaluation set as Raw and thesecond as Filter.Baseline methods.
For comparison, we select trans-lating methods such as TransE (Bordes et al, 2013),TransH (Wang et al, 2014b) and TransR (Lin et al,2015b) as our baselines.
We then use time-awareembedding based on these methods to obtain the cor-responding time-aware embedding models.
A modelwith time-aware embedding is denoted as ?tTransE?for example.Implementation details.
For all methods, we cre-ate 100 mini-batches on each data set.
The di-Mean Rank Hits@1 (%)Metric Raw Filter Raw FilterTransE 1.53 1.48 69.4 73.0tTransE 1.42 1.35 71.1 75.7TransH 1.51 1.37 70.5 72.2tTransH 1.38 1.30 74.6 76.9TransR 1.40 1.28 71.1 74.3tTransR 1.27 1.12 74.5 78.9Table 3: Evaluation results on relation prediction.mension of the embedding n is set in the range of{20,50,100}, the margin ?1 and ?2 are set in therange {1,2,4,10}.
The learning rate is set in therange {0.1, 0.01, 0.001}.
The regularization hy-perparameter ?
is tuned in {10?1,10?2,10?3,10?4}.The best configuration is determined according tothe mean rank in validation set.
The optimal config-urations are n=100,?1=?2=4,?=10?2, learning rateis 0.001 and taking `1?norm.Results.
Table 2 reports the results on the test set.From the results, we can see that time-aware em-bedding methods outperform all the baselines on al-l the data sets and with all the metrics.
The im-provements are usually quite significant.
The MeanRank drops by about 75%, and Hits@10 rises about19% to 30%.
This demonstrates the superiority andgenerality of our method.
When dealing with s-parse data YG15k, all the temporal information is u-tilized to model temporal associations and make theembeddings more accurate, so it obtains better im-provement than mixing the time-unknown triples inYG36k.3.2.2 Relation PredictionRelation prediction aims to predict relations giv-en two entities.
Evaluation results are shownin Table 3 on only YG15K due to limited s-pace, where we report Hits@1 instead of Hit-s@10.
Example prediction results for TransEand tTransE are compared in Table 4.
For ex-ample, when testing (Billy Hughes,?,London,1862),it?s easy for TransE to mix relations wasBornInand diedIn because they act similarly for a per-son and a place.
But known that (Billy Hughes,isAffiliatedTo, National Labor Party) happened in1916, and tTransE have learnt temporal order thatwasBornIn?isAffiliatedTo?diedIn, so the regular-ization term |rbornT ?
raffiliated| is smaller than|rdiedT?
raffiliated|, so correct answer wasBornInranks higher than diedIn.2352Dataset YG15k YG36kMetric MeanRank Hits@10(%) MeanRank Hits@10(%)Raw Filter Raw Filter Raw Filter Raw FilterTransE 990 971 26.6 29.5 179 163 65.7 75.6tTransE 235 233 35.4 36.1 60 55 76.1 82.8TransH 986 966 25.7 28.0 174 158 65.3 77.8tTransH 232 230 36.1 37.2 61 54 76.6 82.9TransR 976 955 29.5 30.2 175 153 68.3 80.1tTransR 228 221 37.3 38.4 55 46 79.5 84.2Table 2: Evaluation results on link prediction.Testing quad TransE predictions tTransE predictions(Billy Hughes,?,London,1862) diedIn,wasBornIn wasBornIn,diedIn(John Schoenherr,?,Caldecott Medal,1988) owns,hasWonPrize hasWonPrize,owns(John G. Thompson,?,University of Cambridge,1961) graduatedFrom,worksAt worksAt,graduatedFrom(Tommy Douglas,?,New Democratic Party,1961) isMarriedTo,isAffiliatedTo isAffiliatedTo,worksAtTable 4: Example results of relation prediction in descending order.
Correct predictions are in bold.3.3 Triple ClassificationTriple classification aims to judge whether an un-seen triple is correct or not.Evaluation protocol.
We follow the same evalua-tion protocol used in Socher et al (2013).
To createlabeled data for classification, for each triple in thetest and validation sets, we construct a correspond-ing negative triple by randomly corrupting the enti-ties.
To corrupt a position (head or tail), only entitiesthat have appeared in that position are allowed.
Dur-ing triple classification, a triple is predicted as posi-tive if the score is below a relation-specific threshold?r; otherwise as negative.
We report averaged accu-racy on the test sets.Implementation details.
We use the same hyperpa-rameter settings as in the link prediction task.
Therelation-specific threshold ?r is determined by max-imizing averaged accuracy on the validation sets.Results.
Table 5 reports the results on the test set-s.
The results indicate that time-aware embeddingoutperforms all the baselines consistently.
Temporalorder information may help to distinguish positiveand negative triples as different head entities mayhave different temporally associated relations.
If thetemporal order is the same with most facts, the reg-ularization term helps it get lower energies and viceversa.4 Related WorkMany models have been proposed for KB embed-ding (Nickel et al, 2011; Bordes et al, 2013; Socheret al, 2013).
External information is employed toimprove KB embedding such as text (Riedel et al,Datasets YG15K YG36KTransE 63.9 71.9tTransE 75.0 82.7TransH 63.4 72.1tTransH 75.1 82.3TransR 64.5 74.9tTransR 78.5 83.9Table 5: Evaluation results on triple classification (%).2013; Wang et al, 2014a; Zhao et al, 2015), enti-ty type and relationship domain (Guo et al, 2015;Chang et al, 2014), and relation path (Lin et al,2015a; Gu et al, 2015).
However, these methodssolely rely on triple facts but neglect temporal or-der constraints between facts.
Temporal informa-tion such as relation ordering in text has been ex-plored (Talukdar et al, 2012; Chambers et al, 2014;Bethard, 2013; Cassidy et al, 2014; Chambers etal., 2007; Chambers and Jurafsky, 2008).
This pa-per proposes a time-aware embedding approach thatemploys temporal order constraints to improve KBembedding.5 Conclusion and Future WorkIn this paper, we propose a general time-aware KBembedding, which incorporates creation time of en-tities and imposes temporal order constraints on thegeometric structure of the embedding space and en-force it to be temporally consistent and accurate.
Asfuture work: (1) We will incorporate the valid timeof facts.
(2) Some time-sensitive facts lack temporalinformation in YAGO2, we will mine such temporalinformation from texts.2353AcknowledgmentsThis research is supported by National Key BasicResearch Program of China (No.2014CB340504)and National Natural Science Foundation of China(No.61375074,61273318).
The contact author forthis paper is Baobao Chang and Zhifang Sui.ReferencesSteven Bethard.
2013.
Cleartk-timeml: A minimalist ap-proach to tempeval 2013.
In Second Joint Conferenceon Lexical and Computational Semantics (*SEM), Vol-ume 2: Proceedings of the Seventh International Work-shop on Semantic Evaluation (SemEval 2013), pages10?14.
Association for Computational Linguistics.Kurt Bollacker, Colin Evans, Praveen Paritosh, Tim S-turge, and Jamie Taylor.
2008.
Freebase: a collabo-ratively created graph database for structuring humanknowledge.
In Proceedings of the 2008 ACM SIG-MOD international conference on Management of da-ta, pages 1247?1250.
ACM.Antoine Bordes, Jason Weston, Ronan Collobert, andYoshua Bengio.
2011.
Learning structured embed-dings of knowledge bases.
In Conference on ArtificialIntelligence, number EPFL-CONF-192344.Antoine Bordes, Nicolas Usunier, Alberto Garcia-Duran,Jason Weston, and Oksana Yakhnenko.
2013.
Trans-lating embeddings for modeling multi-relational data.In Advances in Neural Information Processing System-s, pages 2787?2795.Taylor Cassidy, Bill McDowell, Nathanael Chambers,and Steven Bethard.
2014.
An annotation frameworkfor dense event ordering.
In ACL.Nathanael Chambers and Daniel Jurafsky.
2008.
Un-supervised learning of narrative event chains.
ACL,94305:789?797.Nathanael Chambers, Shan Wang, and Dan Jurafsky.2007.
Classifying temporal relations between events.In Proceedings of the 45th Annual Meeting of the A-CL on Interactive Poster and Demonstration Session-s, pages 173?176.
Association for Computational Lin-guistics.Nathanael Chambers, Taylor Cassidy, Bill McDowell,and Steven Bethard.
2014.
Dense event ordering witha multi-pass architecture.
Transactions of the Associ-ation for Computational Linguistics, 2:273?284.Kai-Wei Chang, Wen-tau Yih, Bishan Yang, and Christo-pher Meek.
2014.
Typed tensor decomposition ofknowledge bases for relation extraction.
In EMNLP,pages 1568?1579.MS Fabian, K Gjergji, and W Gerhard.
2007.
Ya-go: A core of semantic knowledge unifying wordnetand wikipedia.
In 16th International World Wide WebConference, WWW, pages 697?706.Kelvin Gu, John Miller, and Percy Liang.
2015.
Travers-ing knowledge graphs in vector space.
arXiv preprintarXiv:1506.01094.Shu Guo, Quan Wang, Bin Wang, Lihong Wang, andLi Guo.
2015.
Semantically smooth knowledge graphembedding.
In Proceedings of the 53rd Annual Meet-ing of the Association for Computational Linguisticsand the 7th International Joint Conference on NaturalLanguage Processing, pages 84?94.Johannes Hoffart, Fabian M Suchanek, Klaus Berberich,and Gerhard Weikum.
2013.
Yago2: A spatially andtemporally enhanced knowledge base from wikipedia.Artificial Intelligence, 194:28?61.Yankai Lin, Zhiyuan Liu, and Maosong Sun.
2015a.Modeling relation paths for representation learning ofknowledge bases.
arXiv preprint arXiv:1506.00379.Yankai Lin, Zhiyuan Liu, Maosong Sun, Yang Liu, andXuan Zhu.
2015b.
Learning entity and relation em-beddings for knowledge graph completion.Maximilian Nickel, Volker Tresp, and Hans-PeterKriegel.
2011.
A three-way model for collectivelearning on multi-relational data.
In Proceedings ofthe 28th international conference on machine learning(ICML-11), pages 809?816.Sebastian Riedel, Limin Yao, Andrew McCallum, andBenjamin M Marlin.
2013.
Relation extraction withmatrix factorization and universal schemas.Richard Socher, Danqi Chen, Christopher D Manning,and Andrew Ng.
2013.
Reasoning with neural ten-sor networks for knowledge base completion.
InAdvances in Neural Information Processing Systems,pages 926?934.Partha Pratim Talukdar, Derry Wijaya, and Tom Mitchel-l. 2012.
Acquiring temporal constraints between rela-tions.
In CIKM.Zhen Wang, Jianwen Zhang, Jianlin Feng, and ZhengChen.
2014a.
Knowledge graph and text jointly em-bedding.
In Proceedings of the 2014 Conference onEmpirical Methods in Natural Language Processing(EMNLP)., pages 1591?1601.Zhen Wang, Jianwen Zhang, Jianlin Feng, and ZhengChen.
2014b.
Knowledge graph embedding by trans-lating on hyperplanes.
In Proceedings of the Twenty-Eighth AAAI Conference on Artificial Intelligence,pages 1112?1119.Yu Zhao, Zhiyuan Liu, and Maosong Sun.
2015.
Rep-resentation learning for measuring entity relatednesswith rich information.
In Proceedings of the 24thInternational Conference on Artificial Intelligence,pages 1412?1418.
AAAI Press.2354
