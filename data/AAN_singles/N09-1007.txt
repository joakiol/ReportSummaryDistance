Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the ACL, pages 56?64,Boulder, Colorado, June 2009. c?2009 Association for Computational LinguisticsA Discriminative Latent Variable Chinese Segmenterwith Hybrid Word/Character InformationXu SunDepartment of Computer ScienceUniversity of Tokyosunxu@is.s.u-tokyo.ac.jpYaozhong ZhangDepartment of Computer ScienceUniversity of Tokyoyaozhong.zhang@is.s.u-tokyo.ac.jpTakuya MatsuzakiDepartment of Computer ScienceUniversity of Tokyomatuzaki@is.s.u-tokyo.ac.jpYoshimasa TsuruokaSchool of Computer ScienceUniversity of Manchesteryoshimasa.tsuruoka@manchester.ac.ukJun?ichi TsujiiDepartment of Computer Science, University of Tokyo, JapanSchool of Computer Science, University of Manchester, UKNational Centre for Text Mining, UKtsujii@is.s.u-tokyo.ac.jpAbstractConventional approaches to Chinese wordsegmentation treat the problem as a character-based tagging task.
Recently, semi-Markovmodels have been applied to the problem, in-corporating features based on complete words.In this paper, we propose an alternative, alatent variable model, which uses hybrid in-formation based on both word sequences andcharacter sequences.
We argue that the use oflatent variables can help capture long rangedependencies and improve the recall on seg-menting long words, e.g., named-entities.
Ex-perimental results show that this is indeed thecase.
With this improvement, evaluations onthe data of the second SIGHAN CWS bakeoffshow that our system is competitive with thebest ones in the literature.1 IntroductionFor most natural language processing tasks, wordsare the basic units to process.
Since Chinese sen-tences are written as continuous sequences of char-acters, segmenting a character sequence into a wordsequence is the first step for most Chinese process-ing applications.
In this paper, we study the prob-lem of Chinese word segmentation (CWS), whichaims to find these basic units (words1) for a givensentence in Chinese.Chinese character sequences are normally am-biguous, and out-of-vocabulary (OOV) words are amajor source of the ambiguity.
Typical examplesof OOV words include named entities (e.g., orga-nization names, person names, and location names).Those named entities may be very long, and a dif-ficult case occurs when a long word W (|W | ?
4)consists of some words which can be separate wordson their own; in such cases an automatic segmentermay split the OOV word into individual words.
Forexample,(Computer Committee of International Federation ofAutomatic Control) is one of the organization namesin the Microsoft Research corpus.
Its length is 13and it contains more than 6 individual words, but itshould be treated as a single word.
Proper recogni-tion of long OOV words are meaningful not only forword segmentation, but also for a variety of otherpurposes, e.g., full-text indexing.
However, as is il-lustrated, recognizing long words (without sacrific-ing the performance on short words) is challenging.Conventional approaches to Chinese word seg-mentation treat the problem as a character-based la-1Following previous work, in this paper, words can also referto multi-word expressions, including proper names, long namedentities, idioms, etc.56beling task (Xue, 2003).
Labels are assigned to eachcharacter in the sentence, indicating whether thecharacter xi is the start (Labeli = B), middle or endof a multi-character word (Labeli = C).
A popu-lar discriminative model that have been used for thistask is the conditional random fields (CRFs) (Laf-ferty et al, 2001), starting with the model of Penget al (2004).
In the Second International ChineseWord Segmentation Bakeoff (the second SIGHANCWS bakeoff) (Emerson, 2005), two of the highestscoring systems in the closed track competition werebased on a CRF model (Tseng et al, 2005; Asaharaet al, 2005).While the CRF model is quite effective comparedwith other models designed for CWS, it may be lim-ited by its restrictive independence assumptions onnon-adjacent labels.
Although the window can inprinciple be widened by increasing the Markov or-der, this may not be a practical solution, becausethe complexity of training and decoding a linear-chain CRF grows exponentially with the Markov or-der (Andrew, 2006).To address this difficulty, a choice is to relax theMarkov assumption by using the semi-Markov con-ditional random field model (semi-CRF) (Sarawagiand Cohen, 2004).
Despite the theoretical advan-tage of semi-CRFs over CRFs, however, some pre-vious studies (Andrew, 2006; Liang, 2005) explor-ing the use of a semi-CRF for Chinese word seg-mentation did not find significant gains over theCRF ones.
As discussed in Andrew (2006), the rea-son may be that despite the greater representationalpower of the semi-CRF, there are some valuable fea-tures that could be more naturally expressed in acharacter-based labeling model.
For example, ona CRF model, one might use the feature ?the cur-rent character xi is X and the current label Labeliis C?.
This feature may be helpful in CWS for gen-eralizing to new words.
For example, it may ruleout certain word boundaries if X were a characterthat normally occurs only as a suffix but that com-bines freely with some other basic forms to createnew words.
This type of features is slightly less nat-ural in a semi-CRF, since in that case local features?
(yi, yi+1, x) are defined on pairs of adjacent words.That is to say, information about which charactersare not on boundaries is only implicit.
Notably, ex-cept the hybrid Markov/semi-Markov system in An-drew (2006)2, no other studies using the semi-CRF(Sarawagi and Cohen, 2004; Liang, 2005; Daume?III and Marcu, 2005) experimented with features ofsegmenting non-boundaries.In this paper, instead of using semi-Markov mod-els, we describe an alternative, a latent variablemodel, to learn long range dependencies in Chi-nese word segmentation.
We use the discrimina-tive probabilistic latent variable models (DPLVMs)(Morency et al, 2007; Petrov and Klein, 2008),which use latent variables to carry additional infor-mation that may not be expressed by those originallabels, and therefore try to build more complicatedor longer dependencies.
This is especially meaning-ful in CWS, because the used labels are quite coarse:Label(y) ?
{B,C}, where B signifies beginning aword and C signifies the continuation of a word.3For example, by using DPLVM, the aforementionedfeature may turn to ?the current character xi is X ,Labeli = C, and LatentV ariablei = LV ?.
Thecurrent latent variable LV may strongly depend onthe previous one or many latent variables, and there-fore we can model the long range dependencieswhich may not be captured by those very coarse la-bels.
Also, since character and word informationhave their different advantages in CWS, in our latentvariable model, we use hybrid information based onboth character and word sequences.2 A Latent Variable Segmenter2.1 Discriminative Probabilistic LatentVariable ModelGiven data with latent structures, the task is tolearn a mapping between a sequence of observa-tions x = x1, x2, .
.
.
, xm and a sequence of labelsy = y1, y2, .
.
.
, ym.
Each yj is a class label for thej?th character of an input sequence, and is a mem-ber of a set Y of possible class labels.
For each se-quence, the model also assumes a sequence of latentvariables h = h1, h2, .
.
.
, hm, which is unobserv-able in training examples.The DPLVM is defined as follows (Morency et al,2The system was also used in Gao et al (2007), with animproved performance in CWS.3In practice, one may add a few extra labels based on lin-guistic intuitions (Xue, 2003).572007):P (y|x,?)
=?hP (y|h,x,?
)P (h|x,?
), (1)where ?
are the parameters of the model.
DPLVMscan be seen as a natural extension of CRF models,and CRF models can be seen as a special case ofDPLVMs that have only one latent variable for eachlabel.To make the training and inference efficient, themodel is restricted to have disjoint sets of latent vari-ables associated with each class label.
Each hj is amember in a set Hyj of possible latent variables forthe class label yj .
H is defined as the set of all pos-sible latent variables, i.e., the union of all Hyj sets.Since sequences which have any hj /?
Hyj will bydefinition have P (y|x,?)
= 0, the model can befurther defined4 as:P (y|x,?)
= ?h?Hy1?...
?HymP (h|x,?
), (2)where P (h|x,?)
is defined by the usual conditionalrandom field formulation:P (h|x,?)
= exp??f(h,x)?
?h exp?
?f(h,x), (3)in which f(h,x) is a feature vector.
Given a trainingset consisting of n labeled sequences, (xi,yi), fori = 1 .
.
.
n, parameter estimation is performed byoptimizing the objective function,L(?)
=n?i=1log P (yi|xi,?)
?
R(?).
(4)The first term of this equation is the conditional log-likelihood of the training data.
The second term isa regularizer that is used for reducing overfitting inparameter estimation.For decoding in the test stage, given a test se-quence x, we want to find the most probable labelsequence, y?:y?
= argmaxyP (y|x,??).
(5)For latent conditional models like DPLVMs, the bestlabel path y?
cannot directly be produced by the4It means that Eq.
2 is from Eq.
1 with additional definition.Viterbi algorithm because of the incorporation ofhidden states.
In this paper, we use a techniquebased on A?
search and dynamic programming de-scribed in Sun and Tsujii (2009), for producing themost probable label sequence y?
on DPLVM.In detail, an A?
search algorithm5 (Hart et al,1968) with a Viterbi heuristic function is adopted toproduce top-n latent paths, h1,h2, .
.
.hn.
In addi-tion, a forward-backward-style algorithm is used tocompute the exact probabilities of their correspond-ing label paths, y1,y2, .
.
.yn.
The model then triesto determine the optimal label path based on thetop-n statistics, without enumerating the remaininglow-probability paths, which could be exponentiallyenormous.The optimal label path y?
is ready when the fol-lowing ?exact-condition?
is achieved:P (y1|x,?)
?
(1 ?
?yk?LPnP (yk|x,?))
?
0, (6)where y1 is the most probable label sequence incurrent stage.
It is straightforward to prove thaty?
= y1, and further search is unnecessary.
Thisis because the remaining probability mass, 1 ?
?yk?LPn P (yk|x,?
), cannot beat the current op-timal label path in this case.
For more details of theinference, refer to Sun and Tsujii (2009).2.2 Hybrid Word/Character InformationWe divide our main features into two types:character-based features and word-based features.The character-based features are indicator functionsthat fire when the latent variable label takes somevalue and some predicate of the input (at a certainposition) corresponding to the label is satisfied.
Foreach latent variable label hi (the latent variable la-bel at position i), we use the predicate templates asfollows:?
Input characters/numbers/letters locating at po-sitions i ?
2, i ?
1, i, i + 1 and i + 2?
The character/number/letter bigrams locatingat positions i ?
2, i ?
1, i and i + 15A?
search and its variants, like beam-search, are widelyused in statistical machine translation.
Compared to othersearch techniques, an interesting point of A?
search is that itcan produce top-n results one-by-one in an efficient manner.58?
Whether xj and xj+1 are identical, for j = (i?2) .
.
.
(i + 1)?
Whether xj and xj+2 are identical, for j = (i?3) .
.
.
(i + 1)The latter two feature templates are designed to de-tect character or word reduplication, a morphologi-cal phenomenon that can influence word segmenta-tion in Chinese.The word-based features are indicator functionsthat fire when the local character sequence matchesa word or a word bigram.
A dictionary containingword and bigram information was collected from thetraining data.
For each latent variable label unigramhi, we use the set of predicate template checking forword-based features:?
The identity of the string xj .
.
.
xi, if it matchesa word A from the word-dictionary of trainingdata, with the constraint i?6 < j < i; multiplefeatures will be generated if there are multiplestrings satisfying the condition.?
The identity of the string xi .
.
.
xk, if it matchesa word A from the word-dictionary of trainingdata, with the constraint i < k < i+6; multiplefeatures could be generated.?
The identity of the word bigram (xj .
.
.
xi?1,xi .
.
.
xk), if it matches a word bigram in thebigram dictionary and satisfies the aforemen-tioned constraints on j and k; multiple featurescould be generated.?
The identity of the word bigram (xj .
.
.
xi,xi+1 .
.
.
xk), if it matches a word bigram in thebigram dictionary and satisfies the aforemen-tioned constraints on j and k; multiple featurescould be generated.All feature templates were instantiated with val-ues that occur in positive training examples.
Wefound that using low-frequency features that occuronly a few times in the training set improves perfor-mance on the development set.
We hence do not doany thresholding of the DPLVM features: we simplyuse all those generated features.The aforementioned word based features can in-corporate word information naturally.
In addition,following Wang et al (2006), we found using avery simple heuristic can further improve the seg-mentation quality slightly.
More specifically, twooperations, merge and split, are performed on theDPLVM/CRF outputs: if a bigram A B was not ob-served in the training data, but the merged one ABwas, then A B will be simply merged into AB; onthe other hand, if AB was not observed but A B ap-peared, then it will be split into A B.
We found thissimple heuristic on word information slightly im-proved the performance (e.g., for the PKU corpus,+0.2% on the F-score).3 ExperimentsWe used the data provided by the second Inter-national Chinese Word Segmentation Bakeoff totest our approaches described in the previous sec-tions.
The data contains three corpora from differentsources: Microsoft Research Asia (MSR), City Uni-versity of Hong Kong (CU), and Peking University(PKU).Since the purpose of this work is to evaluate theproposed latent variable model, we did not use ex-tra resources such as common surnames, lexicons,parts-of-speech, and semantics.
For the generationof word-based features, we extracted a word listfrom the training data as the vocabulary.Four metrics were used to evaluate segmentationresults: recall (R, the percentage of gold standardoutput words that are correctly segmented by the de-coder), precision (P , the percentage of words in thedecoder output that are segmented correctly), bal-anced F-score (F ) defined by 2PR/(P + R), recallof OOV words (R-oov).
For more detailed informa-tion on the corpora and these metrics, refer to Emer-son (2005).3.1 Training the DPLVM SegmenterWe implemented DPLVMs in C++ and optimizedthe system to cope with large scale problems, inwhich the feature dimension is beyond millions.
Weemploy the feature templates defined in Section 2.2,taking into account those 3,069,861 features for theMSR data, 2,634,384 features for the CU data, and1,989,561 features for the PKU data.As for numerical optimization, we performedgradient decent with the Limited-Memory BFGS59(L-BFGS)6 optimization technique (Nocedal andWright, 1999).
L-BFGS is a second-order Quasi-Newton method that numerically estimates the cur-vature from previous gradients and updates.
Withno requirement on specialized Hessian approxima-tion, L-BFGS can handle large-scale problems in anefficient manner.Since the objective function of the DPLVM modelis non-convex, we randomly initialized parametersfor the training.7 To reduce overfitting, we employedan L2 Gaussian weight prior8 (Chen and Rosen-feld, 1999).
During training, we varied the L2-regularization term (with values 10k, k from -3 to3), and finally set the value to 1.
We use 4 hiddenvariables per label for this task, compromising be-tween accuracy and efficiency.3.2 Comparison on Convergence SpeedFirst, we show a comparison of the convergencespeed between the objective function of DPLVMsand CRFs.
We apply the L-BFGS optimization algo-rithm to optimize the objective function of DPLVMand CRF models, making a comparison betweenthem.
We find that the number of iterations requiredfor the convergence of DPLVMs are fewer than forCRFs.
Figure 1 illustrates the convergence-speedcomparison on the MSR data.
The DPLVM modelarrives at the plateau of convergence in around 300iterations, with the penalized loss of 95K when#passes = 300; while CRFs require 900 iterations,with the penalized loss of 98K when #passes =900.However, we should note that the time cost of theDPLVM model in each iteration is around four timeshigher than the CRF model, because of the incorpo-ration of hidden variables.
In order to speed up the6For numerical optimization on latent variable models, wealso experimented the conjugate-gradient (CG) optimization al-gorithm and stochastic gradient decent algorithm (SGD).
Wefound the L-BFGS with L2 Gaussian regularization performsslightly better than the CG and the SGD.
Therefore, we adoptthe L-BFGS optimizer in this study.7For a non-convex objective function, different parame-ter initializations normally bring different optimization results.Therefore, to approach closer to the global optimal point, itis recommended to perform multiple experiments on DPLVMswith random initialization and then select a good start point.8We also tested the L-BFGS with L1 regularization, and wefound the L-BFGS with L2 regularization performs better inthis task.0300K600K900K1200K1500K1800K100  200  300  400  500  600  700  800  900Obj.Func.
ValueForward-Backward PassesDPLVMCRFFigure 1: The value of the penalized loss based on thenumber of iterations: DPLVMs vs. CRFs on the MSRdata.Style #W.T.
#Word #C.T.
#CharMSR S.C. 88K 2,368K 5K 4,050KCU T.C.
69K 1,455K 5K 2,403KPKU S.C. 55K 1,109K 5K 1,826KTable 1: Details of the corpora.
W.T.
represents wordtypes; C.T.
represents character types; S.C. representssimplified Chinese; T.C.
represents traditional Chinese.training speed of the DPLVM model in the future,one solution is to use the stochastic learning tech-nique9.
Another solution is to use a distributed ver-sion of L-BFGS to parallelize the batch training.4 Results and DiscussionSince the CRF model is one of the most successfulmodels in Chinese word segmentation, we comparedDPLVMs with CRFs.
We tried to make experimen-tal results comparable between DPLVMs and CRFmodels, and have therefore employed the same fea-ture set, optimizer and fine-tuning strategy betweenthe two.
We also compared DPLVMs with semi-CRFs and other successful systems reported in pre-vious work.4.1 Evaluation ResultsThree training and test corpora were used in the test,including the MSR Corpus, the CU Corpus, and the9We have tried stochastic gradient decent, as described pre-viously.
It is possible to try other stochastic learning methods,e.g., stochastic meta decent (Vishwanathan et al, 2006).60MSR data P R F R-oovDPLVM (*) 97.3 97.3 97.3 72.2CRF (*) 97.1 96.8 97.0 72.0semi-CRF (A06) N/A N/A 96.8 N/Asemi-CRF (G07) N/A N/A 97.2 N/ACRF (Z06-a) 96.5 96.3 96.4 71.4Z06-b 97.2 96.9 97.1 71.2ZC07 N/A N/A 97.2 N/ABest05 (T05) 96.2 96.6 96.4 71.7CU data P R F R-oovDPLVM (*) 94.7 94.4 94.6 68.8CRF (*) 94.3 93.9 94.1 65.8CRF (Z06-a) 95.0 94.2 94.6 73.6Z06-b 95.2 94.9 95.1 74.1ZC07 N/A N/A 95.1 N/ABest05 (T05) 94.1 94.6 94.3 69.8PKU data P R F R-oovDPLVM (*) 95.6 94.8 95.2 77.8CRF (*) 95.2 94.2 94.7 76.8CRF (Z06-a) 94.3 94.6 94.5 75.4Z06-b 94.7 95.5 95.1 74.8ZC07 N/A N/A 94.5 N/ABest05 (C05) 95.3 94.6 95.0 63.6Table 2: Results from DPLVMs, CRFs, semi-CRFs, andother systems.PKU Corpus (see Table 1 for details).
The resultsare shown in Table 2.
The results are grouped intothree sub-tables according to different corpora.
Eachrow represents a CWS model.
For each group, therows marked by ?
represent our models with hy-brid word/character information.
Best05 representsthe best system of the Second International ChineseWord Segmentation Bakeoff on the correspondingdata; A06 represents the semi-CRF model in An-drew (2006)10, which was also used in Gao et al(2007) (denoted as G07) with an improved perfor-mance; Z06-a and Z06-b represents the pure sub-word CRF model and the confidence-based com-bination of CRF and rule-based models, respec-tively (Zhang et al, 2006); ZC07 represents theword-based perceptron model in Zhang and Clark(2007); T05 represents the CRF model in Tseng etal.
(2005); C05 represents the system in Chen et al10It is a hybrid Markov/semi-Markov CRF model whichoutperforms conventional semi-CRF models (Andrew, 2006).However, in general, as discussed in Andrew (2006), it is essen-tially still a semi-CRF model.(2005).
The best F-score and recall of OOV wordsof each group is shown in bold.As is shown in the table, we achieved the bestF-score in two out of the three corpora.
We alsoachieved the best recall rate of OOV words on thosetwo corpora.
Both of the MSR and PKU Corpus usesimplified Chinese, while the CU Corpus uses thetraditional Chinese.On the MSR Corpus, the DPLVM model reducedmore than 10% error rate over the CRF model us-ing exactly the same feature set.
We also comparedour DPLVM model with the semi-CRF models inAndrew (2006) and Gao et al (2007), and demon-strate that the DPLVM model achieved slightly bet-ter performance than the semi-CRF models.
Andrew(2006) and Gao et al (2007) only reported the re-sults on the MSR Corpus.In summary, tests for the Second InternationalChinese Word Segmentation Bakeoff showed com-petitive results for our method compared with thebest results in the literature.
Our discriminative la-tent variable models achieved the best F-scores onthe MSR Corpus (97.3%) and PKU Corpus (95.2%);the latent variable models also achieved the best re-calls of OOV words over those two corpora.
We willanalyze the results by varying the word-length in thefollowing subsection.4.2 Effect on Long WordsOne motivation of using a latent variable model forCWS is to use latent variables to more adequatelylearn long range dependencies, as we argued in Sec-tion 1.
In the test data of the MSR Corpus, 19% ofthe words are longer than 3 characters; there are also8% in the CU Corpus and 11% in the PKU Corpus,respectively.
In the MSR Corpus, there are some ex-tremely long words (Length > 10), while the CUand PKU corpus do not contain such extreme cases.Figure 2 shows the recall rate on different groupsof words categorized by their lengths (the numberof characters).
As we expected, the DPLVM modelperforms much better on long words (Length ?
4)than the CRF model, which used exactly the samefeature set.
Compared with the CRF model, theDPLVM model exhibited almost the same level ofperformance on short words.
Both models havethe best performance on segmenting the words withthe length of two.
The performance of the CRF610204060801000  2  4  6  8  10  12  14Recall-MSR(%)Length of Word (MSR)DPLVMCRF0204060801000  2  4  6  8  10  12  14Recall-CU(%)Length of Word (CU)DPLVMCRF4050607080901000  2  4  6  8  10  12  14Recall-PKU(%)Length of Word (PKU)DPLVMCRFFigure 2: The recall rate on words grouped by the length.model deteriorates rapidly as the word length in-creases, which demonstrated the difficulty on mod-eling long range dependencies in CWS.
Comparedwith the CRF model, the DPLVM model performedquite well in dealing with long words, without sacri-ficing the performance on short words.
All in all, weconclude that the improvement of using the DPLVMmodel came from the improvement on modelinglong range dependencies in CWS.4.3 Error AnalysisTable 3 lists the major errors collected from the la-tent variable segmenter.
We examined the collectederrors and found that many of them can be groupedinto four types: over-generalization (the top row),errors on named entities (the following three rows),errors on idioms (the following three rows) and er-rors from inconsistency (the two rows at the bottom).Our system performed reasonably well on verycomplex OOV words, such as(Agricultural Bank of China,Gold Segmentation Segmenter Output//Co-allocated org.
names(Chen Yao) //(Chen Fei) //(Vasillis) //////// //Idioms// (propagandist)(desertification) //Table 3: Error analysis on the latent variable seg-menter.
The errors are grouped into four types: over-generalization, errors on named entities, errors on idiomsand errors from data-inconsistency.Shijiazhuang-city Branch, the second sales depart-ment) and (Scienceand Technology Commission of China, National In-stitution on Scientific Information Analysis).
How-ever, it sometimes over-generalized to long words.For example, as shown in the top row,(National Department of Environmental Protection)and (The Central Propaganda Department)are two organization names, but they are incorrectlymerged into a single word.As for the following three rows, (Chen Yao)and (Chen Fei) are person names.
They arewrongly segmented because we lack the features tocapture the information of person names (such use-ful knowledge, e.g., common surname list, are cur-rently not used in our system).
In the future, sucherrors may be solved by integrating open resourcesinto our system.
(Vasillis) is a transliter-ated foreign location name and is also wrongly seg-mented.For the corpora that considered 4 character idiomsas a word, our system successfully combined mostof new idioms together.
This differs greatly from theresults of CRFs.
However, there are still a numberof new idioms that failed to be correctly segmented,as listed from the fifth row to the seventh row.Finally, some errors are due to inconsistencies inthe gold segmentation.
For example, // (pro-pagandist) is two words, but a word with similar62structure, (theorist), is one word.
(desertification) is one word, but its synonym,// (desertification), is two words in the gold seg-mentation.5 Conclusion and Future WorkWe presented a latent variable model for Chineseword segmentation, which used hybrid informationbased on both word and character sequences.
Wediscussed that word and character information havedifferent advantages, and could be complementaryto each other.
Our model is an alternative to the ex-isting word based models and character based mod-els.We argued that using latent variables can bettercapture long range dependencies.
We performedexperiments and demonstrated that our model canindeed improve the segmentation accuracy on longwords.
With this improvement, tests on the dataof the Second International Chinese Word Segmen-tation Bakeoff show that our system is competitivewith the best in the literature.Since the latent variable model allows a widerange of features, so the future work will considerhow to integrate open resources into our system.
Thelatent variable model handles latent-dependenciesnaturally, and can be easily extended to other label-ing tasks.AcknowledgmentsWe thank Kun Yu, Galen Andrew and Xiaojun Linfor the enlightening discussions.
We also thank theanonymous reviewers who gave very helpful com-ments.
This work was partially supported by Grant-in-Aid for Specially Promoted Research (MEXT,Japan).ReferencesGalen Andrew.
2006.
A hybrid markov/semi-markovconditional random field for sequence segmentation.Proceedings of EMNLP?06, pages 465?472.Masayuki Asahara, Kenta Fukuoka, Ai Azuma, Chooi-Ling Goh, Yotaro Watanabe, Yuji Matsumoto, andTakahashi Tsuzuki.
2005.
Combination of machinelearning methods for optimum chinese word segmen-tation.
Proceedings of the fourth SIGHAN workshop,pages 134?137.Stanley F. Chen and Ronald Rosenfeld.
1999.
A gaussianprior for smoothing maximum entropy models.
Tech-nical Report CMU-CS-99-108, CMU.Aitao Chen, Yiping Zhou, Anne Zhang, and Gordon Sun.2005.
Unigram language model for chinese word seg-mentation.
Proceedings of the fourth SIGHAN work-shop.Hal Daume?
III and Daniel Marcu.
2005.
Learn-ing as search optimization: approximate large mar-gin methods for structured prediction.
Proceedings ofICML?05.Thomas Emerson.
2005.
The second international chi-nese word segmentation bakeoff.
Proceedings of thefourth SIGHAN workshop, pages 123?133.Jianfeng Gao, Galen Andrew, Mark Johnson, andKristina Toutanova.
2007.
A comparative study of pa-rameter estimation methods for statistical natural lan-guage processing.
In Proceedings of the 45th AnnualMeeting of the Association of Computational Linguis-tics (ACL?07), pages 824?831.P.E.
Hart, N.J. Nilsson, and B. Raphael.
1968.
A formalbasis for the heuristic determination of minimum costpath.
IEEE Trans.
On System Science and Cybernet-ics, SSC-4(2):100?107.John Lafferty, Andrew McCallum, and Fernando Pereira.2001.
Conditional random fields: Probabilistic modelsfor segmenting and labeling sequence data.
Proceed-ings of ICML?01, pages 282?289.Percy Liang.
2005.
Semi-supervised learning for naturallanguage.
Master?s thesis, Massachusetts Institute ofTechnology.Louis-Philippe Morency, Ariadna Quattoni, and TrevorDarrell.
2007.
Latent-dynamic discriminative mod-els for continuous gesture recognition.
Proceedings ofCVPR?07, pages 1?8.Jorge Nocedal and Stephen J. Wright.
1999.
Numericaloptimization.
Springer.F.
Peng and A. McCallum.
2004.
Chinese segmenta-tion and new word detection using conditional randomfields.
Proceedings of COLING?04.Slav Petrov and Dan Klein.
2008.
Discriminative log-linear grammars with latent variables.
Proceedings ofNIPS?08.Sunita Sarawagi and William Cohen.
2004.
Semi-markov conditional random fields for information ex-traction.
Proceedings of ICML?04.Xu Sun and Jun?ichi Tsujii.
2009.
Sequential labelingwith latent variables: An exact inference algorithm andits efficient approximation.
Proceedings of the 12thConference of the European Chapter of the Associa-tion for Computational Linguistics (EACL?09).Huihsin Tseng, Pichuan Chang, Galen Andrew, DanielJurafsky, and Christopher Manning.
2005.
A condi-tional random field word segmenter for sighan bakeoff632005.
Proceedings of the fourth SIGHAN workshop,pages 168?171.S.V.N.
Vishwanathan, Nicol N. Schraudolph, Mark W.Schmidt, and Kevin P. Murphy.
2006.
Acceleratedtraining of conditional random fields with stochasticmeta-descent.
Proceedings of ICML?06, pages 969?976.Xinhao Wang, Xiaojun Lin, Dianhai Yu, Hao Tian, andXihong Wu.
2006.
Chinese word segmentation withmaximum entropy and n-gram language model.
InProceedings of the fifth SIGHAN workshop, pages138?141, July.Nianwen Xue.
2003.
Chinese word segmentation ascharacter tagging.
International Journal of Computa-tional Linguistics and Chinese Language Processing,8(1).Yue Zhang and Stephen Clark.
2007.
Chinese segmen-tation with a word-based perceptron algorithm.
Pro-ceedings of ACL?07.Ruiqiang Zhang, Genichiro Kikui, and Eiichiro Sumita.2006.
Subword-based tagging by conditional randomfields for chinese word segmentation.
Proceedings ofHLT/NAACL?06 companion volume short papers.64
