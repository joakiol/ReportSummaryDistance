LEFT-CORNER PARSING AND PSYCHOLOGICAL PLAUSIBILITYPh i l ip  l~esnikDepar tment  of  Co l Imuter  and  in format ion  Sc ienceUn ivers i ty  of  Pennsy lvan ia ,  Ph i lade lph ia ,  PA  19104, USAresnik?linc.cis.upenn.eduAbst ractIt is well known that even extremely limited center-embedding causes people to have difficulty ill comprehen-sion, but that left- and right-branching constractions pro-duce no such effect.
If the difficulty in comprehension istaken to be a result of processing load, as is widely as-sumed, then measuring the processing load induced by aparsing strategy on these constructions may help determineits plausibility as a psychological model.
On this basis, ithas been ~rgued \[A J91, JL83\] that by identifying processingload with space utilization, we can rule out both top-downand bottom-up arsing as viable candidates for the humansentence processing mechanism, attd that left-corner pars-ing represents a plausible Mternative.Examining their arguments in detail, we find difficultieswith each presentation.
In this paper we revise the argu-ment and validate its central claim.
In so doing, we discoverthat the key distinction between the parsing methods i  notthe form of prediction (top-down vs. bottom-up vs. left-corner), but rather the ability to iastantiate the operationof composition.1 IntroductionOne of our most robust observations about language - -dating back at least to the seminal work of Miller andChomsky \[MC63\] - -  is that right- and left-branchingconstructions such as ( la) and (lb) seem to cause noparticular difficulty in processing, but that multiplycenter-embedded constructions such as (lc) are difficultto understand.a.
\[\[\[John's\] brother's\] eat\] despises rats.b.
This is \[the dog that chased \[the cat that bit\[the rat that ate tbe cheese\]\]\].c.
#\[The rat that \[the cat that lille dog\] chased\]bit\] ate the cheese.The standard explanation for this distinction is atight bound on space in the human sentence process-ing mechanism: center-embedded constructions requirethat the head noun phrase of each subject be stored un-til the processing of the embedded clause is completeand the corresponding verb is finally encountered)Alternative accounts have been proposed, most shar-ing the premise that the parser's capacity for recur-sion is limited by bounds on storage.
(See, for exmn-pie, \[Kim73\] and \[MI64\]; for opposing views and otherpointers to the literature see \[DJK+82\].
)The distinction between center-embedding andleft/right-branching has important implications forthose who wish to construct psychologically plausiblemodels of parsing.
Johnson-Laird \[JL83\] observes thatneither the top-down nor the bottom-up methods ofconstructing a parse tree fit the facts of (1), arid pro-poses instead the lesS-well-known alternative of left-corner parsing.
Abney mid Johnson \[AJgl\] discussa somewhat more general version of Johnson-Laird'sargument, introducing the abstract notion of a pars-ing sf~ntegy in order to characterize what is meant bybottom-up, top-down, and left-corner parsing.In this paper, we examine the argument as pre-sented by Abney and Johnson and by Johnson-Laird,and point out a central problem with each variation.We then present he argument in a form that remediesthose difficulties, and, in so doing, we identify a pre-viously underrated aspect of the discussion that turnsout to be of central importance.
In particular, we showthat  the psychological plausibility argument hinges onthe operation of composition and not left-corner pre-diction per se.2 Comparing Strategies2.1  Summary  o f  the  ArgumentFor expository purposes, we begin with tile discussionin \[AJ91\].
Abney and Johnson sesame, as we shall,that the hunmn sentence processing mechanism con-struets a parse tree, consisting of labelled nodes andarcs, incrementally over the course of interpreting anutterance, though tile global parse tree need never "ex-ist ill its entirety at any point."
They define a parsingIThis oh~rvatlon is by t to  rne~n~ lanttnaage specific, thoughin SOV langttages it is embedding on objects, not subjectl, thatcauses ditllctdty.ACRES DE COLING-92, NANTES, 23-28 Aour 1992 1 9 1 PROC.
oF COLING-92, NANTES, AUG. 23-28, 1992AFigure 1: A parse treestrategy to be "a way of enumerating the nodes andarcs of parse trees."
This is, in fact, a generalization ofthe concept of a traversal \[ASU86\].A top-down strategy is one in which each node isenumerated before any of its descendants are; a bottom-up strategy is one in which all the descendants of a nodeare enumerated before it is.
So, for example, a top-down strategy would enumerate the nodes of the tree inFigure 1 in the order ABCDEFGHI, and a bottom-upstrategy would enumerate them in the order CEFDB-HIGA.
In a left-corner strategy, for each node ~1, theleftmost child of T/is enumerated before r/, and the sib-lings of the leftmcet child are enumerated after r/.
Thestrategy takes its name from the fact that the first itemon the r ight-hand side of a context-free rule (its left cor-ner) is used to predict the parent node.
For example,having recognized constituent C in Figure 1, the parserpredicts an invocation of rule B --4 C D and introducesnode B.
The complete left-corner enumeration of thetree is CBEDFAHGI.Thus far, we have discussed only the order of enu-meration of nodes, and not ares.
Abney and John-son define as arc-eager any strategy that enumeratesthe arc between two nodes as soon as both nodes arepresent.
An are-standard strategy is one that enu-merates the connecting arc once either none or allof the subtree dominated by the child has been enu-merated.
For example, the arc-eager left-corner enu-meration of the tree in Figure 1 would introduce arc(B,D)  just  after node D was enumerated, while thearc-standard version of the left-corner strategy wouldfirst completely enumerate the subtree containing E,D, and F,  and then enumerate arc (B, D).In order to characterize the space requirements ofa parsing strategy, two more definitions are required.A node is said to be incomplete ither if its parenthas not yet been enumerated (in which case the parsermust store it until it can be attached to the parentnode), or if some child has not yet been enumerated(in which case the parser must store tire node untilits child can be attached).
The space requirement ofa parsing strategy, given a grammar,  is the maximumnumber of incomplete nodes at any point during tbeenumeration of any parse tree of the grammar.Having established this set of definitions, the goalis to decide which parsing strategies are psychologi-cally plausible, given the facts about the human pars-x / e | c ?/-,,,'?
z ~ n/ \v zleft-branching center-embedded right-branchingFigure 2: Branching slr~e~aresing mechanism as exemplified by (1).
The central claimis summarized in the following table:Strategy " - Spaxze required ' \]No#, ?
A~c, L~/t ce .
t~ I m~hLlTop-d .
.
.
.
.
ither o(,) O(u) I o(1) IBottom-up- either 0(1) O(n) \]JO(n)"Left .
.
.
.
.
.
.
.
tandard 0(1) O(n) O(n)Left .
.
.
.
.
.
.
.
.
get,  0(1) O(a) \ ]Q(1) JWhat  people do .. O(1) O(.)
I o(1) IThe table can be explained with reference to Fig-ure 2.
A top-down enumeration of the left-branchingtree clearly requires storage proportional to n, tileheight of the tree: at the point when Z is enumer-ated, each of A, B , .
.
.
,  X remains inemnplete becauseits rightmost child has not yet been encounteredfl Thesame holds true for the center-embedded structure: us-ing a top-down enumeration, each of A, C, D, .
.
.
,  X re-mains incomplete until the subtree it dominates hasbeen entirely enumerated.
In contrast, the top-downstrategy requires only constant space for tim right-branching structure: each of A, C .
.
.
.
, X becomes coru-plete as soon as its rightmost child is enumerated, sothe number of incomplete nodes at any time is at mosttwo.
We conclude that if the human sentence process-ing strategy were top-down, people would find increas-ing difficulty with both multiply left-branching andmultiply center-embedded constructions, but not withright-branching constructions.
The evidence xempli-fied by (1) suggests that this is not the case.A similar analysis holds for the bottom-up strategy.The left-branching structure requires only constantspace, since each of X , .
.
.
,  B ,A  becomes complete assoon as both children have been enumerated.
In con-trust, enumerations of the right-branching and center-embedded constructions require linear space, since ev-ery leftmest child remains incomplete until the subtreedominated by its right sibling has been entirely enu-merated.
The left-corner strategy with arc-standardenumeration behaves imilarly to the bottom-up strat-egy, since every parent node remains incomplete un-til the subtree dominated by its right sibling has been2Abney and Johngoa di*cuss space complexity with r?apectto  the length of the input string, not the height of the ptmmtree, but if we t~sttme the grammar in finitely ambigltotm thisdistinction is of no hnportaxtce.Ac I~ DE COLING-92.
NANTEs, 23-28 AOI\]T 1992 1 9 2 PROC.
OF COLING-92, NANTES, AUO.
23-28.
1992entirely enumerated.
If increased memory load is re-sponsible for increased processing difficulty, as we havebeen assuming, then both the bottom-up strategy andthe arc-standard left-corner strategy predict that peo-ple have more difficulty with right-branching than withleft-branching structures.
Our conclusion is the sameas for the top-down strategy: the asymmetry of theprediction is not supported by the evidence.On the other hand, arc-eager enumeration makes acritical difference to the left-corner strategy when ap-plied to the right-branching structure.
Recall that theleft-corner enumeration of nodes for this structure isBADC .... Notice that after node (7 has been enumer-ated, arc (A,C) is introduced immediately, and as aresult, node A is no longer incomplete.
In general,the arc-eager left-corner strategy will enumerate theright-branching structure with at most three nodes in-complete at any point.
~ktrthermore, as was the casefor the bottom-up strategy, the left-branching structurerequires constant space.
We see that only tile center-embedded structure requires increased storage as thedepth of embedding increases.
Thus of the four strate-gies, the arc-eager version of the left-corner strategyis the only one that makes predictions consistent withobserved behavior.2.2  Two Prob lemsUnder the assumptions made by Abney and Johnson,the discussion sketched out above does make a case fora left-corner strategy being more psychologically plait-sible than top-down or bottom-up strategies.
However,there are two difficulties with the argument as it is pre-sented.First, by abstracting away from parsing algorithmsand placing the focus on parsing strategies, Abney antiJohnson make it difficult to fairly compare space re-quirements across different methods of parsing.
With-out a formal characterization f the algorithms them-selves, it is not clear that their abstract notion of spaceutilization means the same thing in each case.
3~br example, consider the augmented transitionnetwork (ATN) in Figure 3, where the actions on tilearcs are as follows:II: npl ~ *I2: result ~ (S (npl *))13: dell ~ *14: result ~ (NP (dell *))I5: result ~ aI6: result ~ theUppercase are labels represent PUSH operations, andlowercase labels represent erminal symbols.
In thepseudolanguage used here for are actions, npl, dell,3\] am grateful to Stuart Shleber for this observation.a(t5)tl~ (16)Figure 3: Fragment of an ATNand resull are registers, the leftward arrow (+--) indi-cates an assigmnent statement, he pop arc transmitscontrol (aud tile contents of the ~esalt register) to thecalling subuetwork, and the asterisk (*) represents thevalue so transmitted (cf.
\[WooT0\]).
So, for instance,action I4 constructs an NP dominating the structurein the dd l  register on tile left, and, on the right, tilenoun structure received on retnrn froln a push to tileN subnetwork.Now~ tile ATN is perhaps one of the mo~t commonexamples of a parser operating in a top-down fashion.Yet according to the definitions proposed by Abneyand Jolmson, the enumeration performed by the ATNparser given above would seem to make it, an instanceof a bottom-up strategy.
For example, in parsing thenoun phrase the man, the ATN above wonld recognizetile determiner the, then the nonn man, and finally itwould build and return the structure \[,vthe man\] fromthe NP subnetwork.
The source of difficulty lies in thedecoupling of the parser's hypotheses from the struc-tures that it builds.
When the determiner the is en-countered, no parse-tree structures have been built, butthe mechanism controlling the ATN's computation hasstored the hypotheses that we were parsing an S, thatwe had entered the NP subnetwork, and that we hadsubsequently entered the DET subnetwork.
These cor-respond precisely to the nodes we expect to see enu-merated uring the course of a top-down strategy.One could, of course, choose in this case to identifythe space utilization of this parser with the hypothe-ses rather than the structures built.
Itowever, thatleaves the status of the structures themselves in ques-tion.
More to the point, re-characterizing tile storagerequirements of a particular algorithm is exactly thesort of manipulation that the abstract notion of pars-ing strategies should help us avoid.Tile second difficulty with Abney and Johnson's dis-cussion concerns the distinction between arc-eager andarc-standard strategies.
As they point out, for bothtop-down and bottom-up strategies, the two forms ofAcrEs i~ COL1NG-92, NANYES, 23-28 AOflr 1992 1 9 3 PROC.
OF COLING-92, NANTES, AUO.
23-28, 1992arc enumeration are indistinguishable.
In addition,left-corner parsing with arc-standard enumeration is,at least for the purposes of this discussion, virtuallyidentical to bottom-up arsing, having no distinguish-able effects either with respect o space utilization oreven with respect o the hypotheses that are proposed.4So it seems omewhat odd to introduce a distinction be-tween "eager" versus "standard" when it turns out todistinguish only one of six possible combinations (top-down/eager, top-down/standard, etc.).
The questionof exactly what "eager enumeration" does would seemto merit further attention.
We shall give it that atten-tion shortly, in Section 4.3 Compar ing  AutomataAbney and Johnson's argument is largely an indepen-dent account quite similar to one made earlier in \[JL83\].Here we present a brief summary of the argument aspresented there.
Johnson-Laird's presentation, thoughit encounters a difficulty of its own, turns out to com-plement Abney and Johnson's and to make clear howto solve the difficulties in both.Following the standard escription in the compilersliterature (see, e.g., \[ASU86\]), Johnson-Laird adoptsthe definition of a top-down parser as one that oper-ates by recursive descent: it begins with the start sym-bol of the grammar and successively rewrites tile left-most nonterminal until it reaches a terminal symbol orsymbols that can be matched against he input.
Pars-ing in this fashion, the parse tree is constructed topdown and from left to right.
A bottom-up arser buildsthe tree by working upward from the terminal symbolsin the input string, constructing each parent node af-ter all its children have been recognized.
A left.cornerparser recognizes the left-corner of a context-free rulebottom-up, and predicts the remaining symbols on theright-hand-side of the rule top-down.Johnson-Laird examines the psychological p ausibil-ity of parsers, not parsing strategies, but otherwise hisargument is very much the same as the discussion inthe previous section.
He concludes that the symme-try of human performance on left- and right-branchingstructures counts against he top-down mid bottom-upparsers, and that the left-corner p~trser is a viable alter-native that appears to be consistent with the evidence.He then provides a more formal characterizatiml of thevarious parsers by expressing each as a push-down au-tomaton (PDA).
Such a characterization immediately*Although top-down filtering can be added (see, e.g., \[PS87,p.
182D, Schabea (personal commttrdcation) points out that left-corner parsing with top-down ffltethtg iS e~entially the same a.sLR parsing.
Top-down filtering restricts the non-determinlsticchoices made by the parser, bat does not affect the bottom-upconstruction of the parse tree along a single computation path.remedies the first difficulty we found in \[AJ91\]: the for-mal specification of each parsing algorithm permits usto express pace utilization uniformly in terms of theautomaton's stack.The top-down and bottom-up automata behave x-actly as we would expect.
The stack of the bottom-up automaton never grows beyond a constant sizefor left-branching constructions, but is potentially un-bounded for center-embedded and right-branching con-structions.
The top-down automaton displays the op-posite behavior, the size of its stack size being boundedonly for right-branching constructions, sOf particular interest is Johnson-Laird's construe-tion of a PDA for left-corner parsiug, which we considerin more detail.
The stack alphabet for the left-cornerPDA includes not only terminal and non-terminal sym-bols from thc grau~nar, but also special symbols of tileform \[X Y\], where X mad Y are nonterminals.
The firstsymbol in such a pair represents tile top-down predic-tion of a node, and tile second a node that has beenencountered bottom-up.
The use of these pairs per-mits a straightforward combination of left-corner pre-diction, which is bottom-up, and top-down predictionand matching against he input in the style of a top-down automaton.tiere we consider an extremely simple left-cornerautomaton, constructed from a grammar having thefollowing productions:(1) S ~ NP VP(2) NP ~ John \] Mary(3) VP ~ V NP(4) v -~ nke~The rules of tlle automaton are as follows:\[ .
I 'Inpn't I Stac.k " I New top of staclf..I1 John ...2 Mary ...3 likes4 iynored X John5 ignored ... X Mary'6 ignored ... X likes'7 ignored ... \[X NP\]'-~  ic, .
.
.
.
a .
.
.
\ [x  v \ ]9' ignored :.. IX X\].
.
.
John... Mary.. .
likes,.. \[?.NP 1... \[x NPJ... Ix v3... \[x s) vv.
.
.
\ [XVB\]  SPThe top of the stack is at right, and rules 4-9 are ac-tually schemata for a set of rules in which X can bereplaced by each of tile nonterminals (S, NP, VP, andV).
Tile parser begins with S on top of the stack, anda string has been successfully recognized if the stack isempty and the input exhausted.5The a~mlysis bein~ stralghtforward, we omit the details here;for n complete discussion of the construction of PDAs for top-down and bottom-up pm~ing, see ~LP81, ?3.6\].ACRES DE COLING-92, NANTES, 23-28 AO~T 1992 1 9 4 PROc.
oF COLING-92, NANTES, AUO.
23-28, 1992/ s~ iI ivelV (NP iI \ ~/likes "Jr ......Figure 4: Distinguishing the top-down view of a nodeb'om the bottom-up viewRules 1-3 simply introduce texical items onto thestack as they are scanned.
Rules 4--6 represent bottom-up reductions according to the lexical productions ofthe grammar (productions (2) and (4)); for example,rule 4 states that if a constituent X has been predictedtop-down, and the word John is scanned, we continueseeking X top-down with the knowledge that we haveidentified an NP bottom-up.
Rules 7 and 8 implementleft-corner prediction: if the left-corner node of a rulehas been recognized bottom-up, then we hypothesizethe parent node in bottom-up fashion and also predictthe right siblings top-down.
For example, rule 8 statesthat if a V has been recognized bottom-ul) , we shouldhypothesize that a VP is being recognized and also pre-dict the remainder of the VP, namely an NP, top-down.Finally, rule 9 pops a symbol off the top of the stack ifwe have predicted a constituent X top-down and thensucceeded in finding it bottom-up.In examining the behavior of this automaton forthe sentence John likes Mary, a problem immediatelybecomes apparent.
The contents of the stack at eachstep during the parse are as follows:!
!
..... =1  Joha  VP  VP {VP V\] vp  VP\] ?, - S ~ IS NP\] IS Sl IS Sl IS S I ( I j  ) (a)  (4) {s|  (~) "r!
\] !
l i t  .
.
.
.
NP INP NP ?
.
\ [v t  r' vP \ ]  \ [VP VP\] \ [Vp  VP) Is sl IS sl s s I s s (6) (9) o (~)As the sentence --- a right-branching structure ---is recognized, we find that the stack is accumulatingsymbols of the form \[X X\].
It is clear that as the depthof right-branching increases, the number of stacked-upsymbols of this form will also increase, without upperbmmd.
Why is this happening?Let us distinguish between the top-down "view" ofa node and the bottom-up (left-corner) "view" of thatnode.
Figure 4 makes this distinction explicit: the VPpredicted top-down by the rule S --* NP VP is dis-tinct from the VP predicted in left-corner fashion usingVP --~ V NP.
These are, in fact, precisely the two VPsin the symbol \[VP VP\].
Now, enumerating the arc be-tween VP and S in the final parse tree is equivalent toidentifying these two views (dotted ellipses in the fig-ure).
As long as we have not identified the two views ofVP as the same node, the arc is not enumerated --- andthe parent S remains incomplete in the sense definedby Abney and Johnson, It is rule 9 in the automa-ton that effects this identification: popping \[VP VP\]amounts to recognizing that the top-down view andthe bottom-up view match.
Since the operation of theautomaton prevents the symbol from being popped un-til the bottoIn-up view has been completed, it is clearthat this automaton implements an arc-standard strat-egy rather than an arc-eager one.
Itence it is not sur-prisiug that the antomaton fails to support Johuson-Laird's argument: far from being bounded, the stack ofsuch automaton can grow without bound as the depthof right-brmlching increases.4 Arc -eager  Enumerat ion  asCompos i t ion4.1  An  Easy  F ix .
.
.To summarize thus far, \[AJ91\] and \[JL83\] present woforms of the same argument, but each presentation suf-fers from a central shortcoming.
Abney and Johnson,discussing parsing strategies rather than parsers, failto characterize top-down, bottom-up, and left-cornerparsing in a way that permits a fair comparison ofspace utilization.
Johnson-Laird, ibrmalizing parsers aspush-down automata, provides a characterization thatclearly defines the terms of the comparison, but his left-corner automaton lacks the properties needed to makethe argument succeed.Modifying the left-corner automaton so that it per-forms arc-eager enumeration is straightforward.
As dis-cussed toward the end of the previous ection, "attach-ment" of a node X to its pareut occurs when the symbolIX X\], representing the top-down and bottom-up viewsof that node, is removed from the stack.
In order toattach the node (i.e., enumerate the arc) eagerly, weshould pop the symbol as soon as it is introduced.
Forthe automaton in the previous ection, this amounts toaugmenting rule schema 8 with the ruleI \[ Input I St~k I New top o ts t~k  I\ [8 '1  ign?red l .
, .
\ [vPV\ ]  I , , .NP  \[and, in general, augmenting the rules of left-corner pre-diction so that symbols of the form \[X X\] are not in-troduced obligatorily.It is easy to show that the automaton, modified inthis fashion, requires only a finite stack for arbitrarilyACTES DE COLING-92, NANTES, 23-28 Aotrr 1992 1 9 $ Pgoc.
OF COLING-92, NAN'I~S, AUG. 23-28.
1992(A'--~B1 ...B~ .
)(A~ ?
BI...B~: 1(Bt--.
3'~)...(B~.
rk)Figure 5: Inferenre-ru& ,liar,ncter~:alion of bottom-up reduction step (left) ~d t,,p.down prediction step(,~ght).deep left- and right-l,ranehmg constructions, but re-quires increasing stack ~t,,trq" fi,r c,'nter-embedded con-structions as the depth - f  ,-mb~-dding increases.
Thuswe have succeeded in pr,-~enl|ng a complete version ofthe argument in \[AJ91\] and \[JL83\] in the sense that1.
top-down, bottom-up, and left-corner parsing arecharacterized in a formally precise way,2.
the chaxacterizations are abstract, in the sensethat the logic of the algorithms (in the form of non-deterministic push-down automata) is separatedfrom their control (namely the control of how theautomata's nondeterministic choices are made),3. the notion of space utilization (namely stack size)is the same for each case, permitting us to make afair comparison, and4.
the conclusion, as expected, is that top-down andbottom-up parsing both make incorrect predic-tions, but a form of left-corner parsing is consistentwith the apparent behavior of the human sentenceprocessing mechanism.4 .2  .
.
.and  i t s  Imp l i ca t ionsThe import of the "fix" in the previous ection is notsimply that the automaton can be made to display theappropriate behavior.
It is that the "arc-eager" enu-meration strategy is a different (and perhaps mislead-ing) description of a purser's ability to perform compo-sition on the structures that it is building.If we describe the parsers as sets of inference rulesrather than automata, s the inference permitting arc-eager enumeration i the left-corner parser turns outto be a rule of composition: A ~ c~ ?
B and B ~/3  .
7can be composed to form the dotted item A ~/3  .
3'.For instance, the effect of rule 8' is to predict VPV ?
NP  from V, and then immediately compose thisnew item with S --, NP .
VP.
Equivalently, the rulefirst predicts the VP structure in Figure 4 from the V(giving us \[VP VP\], corresponding to the two VP nodesthe figure), and then immediately identifies the lowerVP node with the upper one (which removes \[VP VP\]),leaving just an S structure that lacks an NP.STwo descriptior~ that are formally equivalent.In contrast, even if one were to add a rule of com-position to the inferential description of top-down andbottom-up arsers, it would have no effect.
Neither thetop-down nor the bottom-up arser ever introduces aconfiguration in which the A constituent and B con-stituent are both only partially completed (and thuscan be composed).
Instead, these parsers rewrite theentire right-hand side of a rule at once (see Figure 5).In order for a rule of composition to be relevant, it isnecessary that the parser introduce both the top-downview of a constituent (e.g.
B in A ---* ~ ?
B) and thebottom-up view of that constituent (e.g.
B in B ~ ft.3`)so that they may later be identified.
Unlike top-downand bottom-up arsers, a left-corner parser meets thiscriterion.By presenting a complete version of the argumentin \[AJ91\] and \[JL83\], we have essentially re-discoveredproposals made by Puhnan \[Pu185, Pul86\] and Thomp-son et al \[TDL91\].
Both propose parsers with left-corner prediction and a composition operation added.Pulman motivates his purser's design on grounds ofpsychological plausibility, though he does not presenta complete version of the argument discussed here.Thompson et al are motivated by issues in parallelparsing.
In addition, we should note that Johnson-Laird introduces a parser with a composition-like oper-ation later in his discussion, though outside the contextof a formal comparison among parsing methods.Abney (personal communication) points out that,though psychologically plausible in terms of the spaceutilization argument we have discussed, the automa-ton presented here may nonetheless fail to be plausiblebecause of its behavior with regard to local ambigu-ity.
If we opt to compose whenever possible (e.g., al-ways preferring rule 8' to rule 8 when X = VP), whichseems natural, then left-recursive structures will leadto counterintuitive r sults - -  for example, in process-ing (2), the automaton will prefer to attach the NP thecat as the object of the verb, rather than waiting forthe full NP the cat's dinner.2 John prepared \[\[tlm eat\]'s dinner\].More generally, as Abney and Johnson discuss, thereis a tradeoff between storage, which is conserved bystrategies that perform attachment "eagerly," and am-biguity, which is avoided by deferring attachment untilmore information is present o resolve it.
On the basisof the observations we have made here, it appears thatthis tradeoff is expressed most naturally not in termsof a comparison between different parsing strategies,but rather in terms of the criteria for when to invoke acomposition operation that is available to the parser.ACTES DE COLING-92, NANTES, 23-28 Ao~Yr 1992 1 9 6 PROC.
OF COLING -92, NANTES, AUO.
23 -28, 19925 Conc lus ionsIn this paper, we have considered a space-utilizationargument concerning the psychological plausibility ofdifferent parsing methods.
Both \[AJ91\] and \[JL83\]make the same basic claim, namely that top-downand bottom-up parsing lead to incorrect predictionsof asymmetry in human processing - -  predictions thatcan be avoided by utilizing a left-corner strategy.
Wehave demonstrated difficulties with both of their for-mulations and presented a more precise account.
Inso doing, we have found that composition, rather thanleft-corner prediction per se, plays the central role indistinguishing parsing methods.In making the argument, we were forced to aban-don the abstract characterization f parsing methodsin terms of strategies, and return to defining parsersin terms of their realizations as automata.
This hasthe unfortunate consequence of tying the argument tocontext-free gramnrars, losing tire attractive fornralism-independent quality evoked in \[AJ91\].Since context:free grammars are no longer generallyconsidered likely models for natural language in thegeneral case \[Shi85\], one wonders how the discussionhere might be extended to parsing within more power-ful grannnatical frameworks.
It is interesting to notethe relationship between the style of left-corner parsingdescribed here and one such framework, combinatorycategorial grammar (CCG) \[Ste90\].
Composition is anintegral part of CCG, as is the notion of type-raising,which resembles left-corner prediction.
7 The operationof a left-corner parser with composition can fairly bedescribed as being in the style of CCG, but retain-ing the context-free base.
Since one attractive featureof CCG is its inherent left-to-right, word-by-word in-crementality, it is perhaps not surprising to find thatparsers of CCG tend naturally to meet the criteria forpsychological p ausibility discussed bere.CCG is one instance of a general class knownas the mildly context-sensitive grammar formalisms\[JVSW88\].
We are currently investigating a generaliza-tion of the argument presented here to other formalismswithin that class.AcknowledgementsThis research was supported by tile following grants:ARO DAAL 03-89-C-0031, DARPA N00014-9O-J-1863,NSF IRl 90-16592, and Ben Franklin 91S.3078C-1.
I wouldlike to thank Steve Abney, Mark Johnson, Aravind Joshi,Yves Schabes, Stuart Shieber, and members of tile CLIFFgroup at Penn for their helpfifl discussion and criticism,rFor example, NP can be type-ralsed to S/(S\NP), whid~roughly corresponds to S ~ NP .
VP.References\[AJgl\] Steven Abney and Mark Johnson.
Memory re-quirements and local ambiguities for parttiagstrategies.
Journal of Psycholinguistic Research,20(3):233--250, 1991.\[ASU86\] Alfred Aho, Ravi Sethi, and Jeffrey Ullmu.Compilers: Principles, Techniques, and Toe,t.Addison Wesley, 1986.\[DJK+82\] A. DeRoeck, R. Johnson, M. King, M.net, G. Sampson, and N. Varile.
A myth aboutceutre-embedding.
Lingua, 58:327-340, 1981.\[JL83\] Philip N. Johnson-Laird.
Mental Models.
Har-vard University Press, 1983.\[JVSW88\] A. K. Joshi, K. Vijay-Shanker, and D. J,Weir.
The convergence of mildly context-sensitive grammatical formalisms.
In P. Selhtand T. Wasow, editors, Processing of Lingutsl,cStructure.
MIT Press, Cambridge, MA, 1988.J.
Kimball.
Seven principles of surface-structureparsing in natural language.
Cognition, 2:15-47,1973.Itarry Lewis and Christos Papadimitrion.
Ele.ments of the Theory of Computation.
Prentice-Itall, 1981.George Miller and Noam Chomsky.
Finitarymodels of language users.
In R. Luce, R. Bush,and E. Galanter, editors, Handbook of Math.ematical Psycholcfy , Volume 2.
John Wiley,1963.G.
A. Miller and S. Isard.
Free recall of self-embedded English sentences.
Information andControl, 7:292--303, 1964.l"ernando C. N. Pereira and Stuart M. Shieber.Pralog and Natural Language Analysis.
Cen-.ter for the Study of Language and Information,1987.Stephen Pulman.
A parser that doesn't.
In Pro-ceedings of the 2nd European ACL, pages 128-135, 1985.Stephen Pulman.
Grammars, parsers, and mem-ory limitations.
Language and Cognitive Pro.cesses, 1(3):197-225, 1986.S.
M. Shieber.
Evidence against he context-freeness of naturM language.
Linguistics andPhilosophy, 8:333-343, 1985.Mark Steedman.
Gapping as constituent coordi-nation.
Linguistics and Philosophy, 13:207-263,April 1990.H.
Thompson, M. Dixon, and J. Lumping.Compose-reduce parsing.
In Proceedings o.f the29th Atmual Meetit,y of the ACL, pages 87-97,June 1991.William A.
Woods.
Transition network gram-mars for natural anguage analysis.
Commu.nications of the ACM, 13(10):591-606, October1970.\[I(im73\]\[LP81\]\[MC63\]\[MI84\]\[ps871\[Pu185\]\[Pul86\]\[Shi85\]\[Ste9O\]\[TI)L911\[WooT0\]ACTES DE COLING-92, NAN'NS, 23-28 Aotrr 1992 1 9 7 PROC.
OF COLING-92, NAI~t'ES, AUO.
23-28, 1992
